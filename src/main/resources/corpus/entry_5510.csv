2019,The Randomized Midpoint Method for Log-Concave Sampling,Sampling from log-concave distributions is a well researched problem
that has many applications in statistics and machine learning. We
study the distributions of the form $p^{*}\propto\exp(-f(x))$  where
$f:\mathbb{R}^{d}\rightarrow\mathbb{R}$ has an $L$-Lipschitz gradient
and is $m$-strongly convex. In our paper  we propose a Markov chain
Monte Carlo (MCMC) algorithm based on the underdamped Langevin diffusion
(ULD). It can achieve $\epsilon\cdot D$ error (in 2-Wasserstein distance)
in $\tilde{O}\left(\kappa^{7/6}/\epsilon^{1/3}+\kappa/\epsilon^{2/3}\right)$
steps  where $D\overset{\mathrm{def}}{=}\sqrt{\frac{d}{m}}$ is the effective diameter
of the problem and $\kappa\overset{\mathrm{def}}{=}\frac{L}{m}$ is the condition number. Our algorithm performs significantly faster than the previously best
known algorithm for solving this problem  which requires $\tilde{O}\left(\kappa^{1.5}/\epsilon\right)$
steps \cite{chen2019optimal dalalyan2018sampling}. Moreover  our
algorithm can be easily parallelized to require only $O(\kappa\log\frac{1}{\epsilon})$
parallel steps. 

To solve the sampling problem  we propose a new framework to discretize
stochastic differential equations. We apply this framework to discretize
and simulate ULD  which converges to the target distribution $p^{*}$.
The framework can be used to solve not only the log-concave sampling
problem  but any problem that involves simulating (stochastic) differential
equations.,The Randomized Midpoint Method for Log-Concave

Sampling

Ruoqi Shen

University of Washington

shenr3@cs.washington.edu

University of Washington and Microsoft Research

Yin Tat Lee

yintat@uw.edu

Abstract

Sampling from log-concave distributions is a well researched problem that has
many applications in statistics and machine learning. We study the distributions
of the form p∗ ∝ exp(−f (x))  where f : Rd → R has an L-Lipschitz gradi-
ent and is m-strongly convex. In our paper  we propose a Markov chain Monte
Carlo (MCMC) algorithm based on the underdamped Langevin diffusion (ULD).

It can achieve  · D error (in 2-Wasserstein distance) in ˜O(cid:0)κ7/6/1/3 + κ/2/3(cid:1)
ously best known algorithm for solving this problem  which requires ˜O(cid:0)κ1.5/(cid:1)

steps  where D def=
m is
the condition number. Our algorithm performs signiﬁcantly faster than the previ-

m is the effective diameter of the problem and κ def= L

(cid:113) d

 ) parallel steps.

steps [7  15]. Moreover  our algorithm can be easily parallelized to require only
O(κ log 1
To solve the sampling problem  we propose a new framework to discretize stochas-
tic differential equations. We apply this framework to discretize and simulate
ULD  which converges to the target distribution p∗. The framework can be used
to solve not only the log-concave sampling problem  but any problem that involves
simulating (stochastic) differential equations.

1

Introduction

In this paper  we study the problem of sampling from a high-dimensional log-concave distribution.
This problem is central in statistics  machine learning and theoretical computer science  with appli-
cations such as Bayesian estimation [1]  volume computation [55] and bandit optimization [54]. In
a seminal 1989 result  Dyer  Frieze and Kannan [23] ﬁrst presented a polynomial-time algorithm
(for an equivalent problem) that takes ˜O(d23 log 1
 ) steps on any d dimensional log-concave distri-
bution to achieve target accuracy . After three decades of research in Markov chain Monte Carlo
(MCMC) and convex geometry [34  2  22  35  27  37  11  30  31  43]  results have been improved
to ˜O(d4 log 1
 ) steps for general log-concave distributions and slightly better for distributions given
 steps are necessary even for a special case of log-concave
in a certain form. Unfortunately  d log 1
sampling  i.e.  convex optimization [3]. To avoid this lower bound  there has been a recent surge of
interest in obtaining a faster algorithm via assuming some properties on the distribution.
We call a distribution log-concave if its density is proportional to e−f (x) with a convex function f.
For the standard assumption that f is m-strongly convex with an L-Lipschitz gradient (see Section
3.1)  the current best algorithms have at least a linear d or 1/ dependence or a large dependence on
the condition number κ def= L
m. In this paper  we present an algorithm with no dependence on d and
a much smaller dependence on κ and  than shown in previous research. Moreover  our algorithm
is the ﬁrst algorithm with better than 1/ dependence that is not Metropolis-adjusted and does not
make any extra assumption  such as high-order smoothness [41  42  6  45].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

To explain our main result  we note that this problem has an effective diameter D def=
because the distance between the minimizer x∗ of f and a random point y ∼ e−f satisﬁes
Ey∼e−f(cid:107)x∗ − y(cid:107)2 ≤ d
m[19]. Therefore  a natural problem deﬁnition1 is to ﬁnd a random x that
makes the Wasserstein distance small:

m

(cid:113) d

W2(x  y) ≤  · D.

(1)

√

This choice of distance is also common in previous papers [19  20  10  41  29  42  6].
For  = 1  we can simply output the minimizer x∗ of f as the “random” point. We ﬁrst consider the
question how quickly we can ﬁnd a random point satisfying  = 1
2. For convex optimization under
the same assumption  it takes
κ iterations via acceleration methods or d iterations via cutting plane
methods  and these results are tight. For sampling  the current fastest algorithms take either ˜O(κ1.5)
steps [7  15] or ˜O(d4) steps [36]. Although there is no rigorous lower bound for this problem  it
is believed that min(κ  d2) is the natural barrier.2 This paper presents an algorithm that takes only
˜O(κ7/6) steps  much closer to the natural barrier of κ for the high-dimensional regime.
For general 0 <  < 1  our algorithm takes ˜O(κ7/6/1/3 + κ/2/3) steps  which is almost linear in κ
and sub-linear in  . It has signiﬁcantly better dependence on both κ and  than previous algorithms.
(See the detailed comparison in Table 1.) Moreover  if we query gradient ∇f at multiple points in
parallel in each step  we can improve the number to O(κ log 1

 ) steps.

1.1 Contributions

We propose a new framework to discretize stochastic differential equations (SDEs)  which is a cru-
cial step of log-sampling algorithms. Since our techniques can also be applied to ordinary differen-
tial equations (ODEs)  we focus on the following ODE here:

dx
dt

= F (x(t)).

There are two main frameworks to discretize a differential equation. One is the Taylor expansion 
2 + ··· . Our paper uses the second framework 
which approximates x(t) by x(0) + x(cid:48)(0)t + x(cid:48)(cid:48)(0) t2
called the collocation method. This method uses the fact that the differential equation is equivalent
to the integral equation x = T (x)  where T maps continuous functions to continuous functions:

T (x)(t) = x(0) +

F (x(s)) ds for all t ≥ 0.

(cid:90) t

0

Since x is a ﬁxed point of T   we can approximate x by computing T (T (··· (T (x0))··· )) for some
approximate initial function x0. Algorithmically  two key questions are how to: (1) show when and
how quickly T iterations converge  and (2) compute the integration. The convergence rate of T was
shown by the Picard–Lindelöf Theorem in the 1890s [32  48] and was key to achieving O(κ1.75)

and O(cid:0)κ1.5(cid:1) in the previous papers [29  7]. To approximate the integration  one standard approach

is to approximate

(cid:90) t

F (x(s)) ds ∼(cid:88)

wiF (x(si))

0

i

1Previous papers addressing this problem deﬁned  as W2(x  e−f ) ≤ . This deﬁnition is not scale invariant 
i.e.  the number of steps changes when we scale f. In comparison  our deﬁnition yields results that are invariant
under: (1) the scaling of f  namely  replacing f (x) by αf (x) for α > 0  and (2) the tensor power of f 
i f (xi). Our new deﬁnition of  also clariﬁes deﬁnitions in previous
m /)  and
m /) steps  respectively. Our new deﬁnition shows that these different dependences on d and m all

namely  replacing f (x) by g(x) def= (cid:80)
˜O(κ1.5(cid:113) d

(cid:113) d
m /)2)  ˜O(κ2(cid:113) d

research. Under the prior deﬁnition of   the algorithms [19  10  7] take ˜O(κ2(

relate to their dependence on .

√
2The corresponding optimization problem takes at least min(

κ  d) steps [3]. If we represent each point
√
the optimization algorithm visited by a vertex and each step the algorithm takes by an edge  then the existing
√
lower bound in fact shows that this graph has a diameter of at least min(
κ  d). Since a random walk on a
κ  d)2 steps.
graph of diameter D takes D2 to mix  a random walk on the graph takes at least min(

2

Algorithm

Hit-and-Run[36]

Langevin Diffusion[19  13]

Underdamped Langevin

Diffusion [10]

Underdamped Langevin

Diffusion2

[15]

High-Order Langevin

Diffusion[45]

Metropolis-Adjusted Langevin

Algorithm[21]

Hamiltonian Monte Carlo with

Euler Method [41]

Hamiltonian Monte Carlo with

Collocation Method [29]

Hamiltonian Monte Carlo with

Collocation Method 2 [7]
Underdamped Langevin

Diffusion with Randomized
Midpoint Method (This Paper)

Warm Start

˜O(cid:0)d3 log( 1
 )(cid:1)

Cold Start

˜O(cid:0)d4 log( 1
 )(cid:1)

(cid:16)(cid:16)

˜O

κd + κ1.5

˜O(cid:0)κ19/4/1/2 + κ13/3/2/3(cid:1)
(cid:17)

√

 )(cid:1)
˜O(cid:0)(cid:0)κd2 + κ1.5d1.5(cid:1) log( 1

d

log( 1
 )

# Step

˜O(cid:0)κ2/2(cid:1)
˜O(cid:0)κ2/(cid:1)
˜O(cid:0)κ1.5/ + κ2(cid:1)
(cid:17)
˜O(cid:0)κ6.5/(cid:1)
˜O(cid:0)κ1.75/(cid:1)
˜O(cid:0)κ1.5/(cid:1)

˜O(cid:0)κ7/6/1/3 + κ/2/3(cid:1)

Table 1: Summary of iteration complexity. Except for Hit-and-Run  each step involves O(1)-
gradient computation. Hit-and-Run takes ˜O(1) function value computations in each step. [15  45]
assume the hessian of f is Lipschitz  which is stronger than our assumption.

for some carefully chosen wi and si. The key drawback of this approach is its introduction of a
deterministic error  which accumulates linearly to the number of steps. Since we expect to take at
least κ-many iterations  the approximation error must be κ times smaller than the target accuracy.
In this paper  we improve upon the collocation method for sampling by developing a new algorithm 
called the randomized midpoint method  that yields three distinct beneﬁts:

1. We generalize ﬁxed point iteration to stochastic differential equations and hence avoid the

cost of reducing SDEs to ODEs  as was done in [29].

2. We greatly reduce the error accumulation by simply approximating(cid:82) t

0 F (x(s))ds by t ·

F (x(s)) where s is randomly chosen from 0 to t uniformly.

3. We show that two iterations of T sufﬁce to achieve the best theoretical guarantee.

Although we discuss only strongly convex functions with a Lipschitz gradient  we believe our frame-
work can be applied to other classes of functions  as well. By designing suitable unbiased estimators
of integrals  researchers can easily use our approach to obtain faster algorithms for solving SDEs
that are unrelated to sampling problems.

1.2 Paper Organization

Section 2 provides background information on solving the log-concave sampling problem  while
Section 3 introduces our notations and assumptions about the function f. We introduce our algo-
rithm in Section 4  where we present the main result of our paper. We show our proofs in appen-
dices: Appendix A–how we simulate the Brownian motion; Appendix B–important properties of
ULD and the Brownian motion; Appendix C– bounds for the discretization error of our algorithm;
Appendix D–a bound on the average value of (cid:107)∇f (xn)(cid:107) and (cid:107)vn(cid:107) in our algorithm  which is useful
for bounding the discretization error; Appendix E–proofs for the main result of our paper; Appendix
F–additional proofs on how to parallelize our algorithm.

3

2 Background

Many different algorithms have been proposed to solve the log-concave sampling problem. The gen-
eral approach uses a MCMC-based algorithm that often includes two steps. The ﬁrst step involves
the choice of a Markov process with a stationary distribution equal or close to the target distribu-
tion. The second step is discretizing the process and simulating it until the distribution of the points
generated is sufﬁciently close to the target distribution.

2.1 Choosing the Markov Process

One commonly used Markov process is the Langevin diffusion (LD) [52  25  18]. LD evolves
according to the SDE

√

dx(t) = −∇f (x(t)) dt +

(cid:17)

2 dBt 

(cid:16) κ2

(2)
where Bt is the standard Brownian motion. Under the assumption that f is L-smooth and m-strongly
m as the condition number  [19  13  8] show that algorithms based
convex (see Section 3.1) with κ = L
on LD can achieve less than  error in ˜O
steps. Other related works include LD with stochastic
gradient [14  57  50  6] and LD in the non-convex setting [50  9].
One important breakthrough introduced the Hamiltonian Monte Carlo (HMC)  originally proposed
in [28]. In this process  SDE (2) is approximated by a piece-wise curve  where each piece is governed
by an ODE called the Hamiltonian dynamics. The Hamiltonian dynamics maintains a velocity v in
2 (cid:107)v(cid:107)2 . HMC
addition to a position x and conserves the value of the Hamiltonian H(x  v) = f (x)+ 1
has been widely studied in [46  40  41  42  29  7  31]. The works [7  15] show that algorithms based
on HMC can achieve less than  error in ˜O

(cid:16) κ1.5

steps.

(cid:17)

2



√

The underdamped Langevin diffusion (ULD) can be viewed as a version of HMC that replaces
multiple ODEs with one SDE; it has been studied in [10  24  15]. ULD follows the SDE:

(cid:17)

u dBt 

(cid:16) κ2

dv(t) = −2v(t) dt − u∇f (x(t)) dt + 2
L. [10] shows that even a basic discretization of ULD has a fast convergence rate that
where u = 1
can achieve less than  error in ˜O
steps. Recently  it was shown that ULD can be viewed as
an accelerated gradient descent for sampling [39]. This suggests that ULD might be one of the right
dynamic for sampling in the same way as the accelerated gradient descent method is appropriate for
convex optimization. For this reason  our paper focuses on how to discretize ULD. We note that our
framework can be applied to both LD and HMC to improve on previous results for these dynamics
as well.

dx(t) = v(t) dt 

(3)



2.2 Discretizing the Process

To simulate the random process mentioned  previous works usually apply the Euler method [10  19]
or the Leapfrog method [41  42] to discretize the SDEs or the ODEs. In Section 4.2  we introduce a
2-step ﬁxed point iteration method to solve general differential equations. We apply this method to
ULD and signiﬁcantly reduce the discretization error compared to existing methods. In particular 
ULD can achieve less than  error in ˜O
steps. Table 1 summarizes the number of
steps needed by previous algorithms versus our algorithm. Moreover  with slightly more effort  our

(cid:17)
algorithm can be parallelized so that it needs only O(cid:0)κ log 1

(cid:1) parallel steps.

(cid:16) κ7/6

1/3 + κ

2/3



On top of the discretization method  one can use a Metropolis-Hastings accept-reject step to ensure
that the post-discretization random process results in a stationary distribution equal to the target
distribution [4  35  53  44  33  36  38]. [36] gives the current best algorithm for arbitrary log-concave
distribution. Originally proposed in [52  53]  the Metropolis Adjusted Langevin Algorithm (MALA)
[51  26  49  5  56  47] applies the Metropolis-Hastings accept-reject step to the Langevin diffusion.
[21] shows MALA can achieve  error in total variation distance in ˜O
steps for β-warm start. Unlike other algorithms that have a
O(1) dependence on   MALA depends
logarithmically on . However  β usually depends exponentially on the dimension d  which results

(cid:16) β

(cid:16)(cid:16)

(cid:17)(cid:17)

κd + κ1.5

(cid:17)

√

log

d

1



4

in a Ω(d1.5) dependence in total. Since this paper focuses on achieving a dimension independent
result  we do not discuss how to combine our process with a Metropolis-Hastings step in this paper.
Finally  we note that all results–including ours–can be improved if we assume that f has bounded
higher-order derivatives. To ensure a fair comparison in Table 1  we only include results that only
assume f is strongly convex and has a Lipschitz gradient.

3 Notations and Deﬁnitions
For any function f  we use ˜O(f ) to denote the class O (f ) · logO(1)(f ). For vector v ∈ Rd  we use
(cid:107)v(cid:107) to denote the Euclidean norm of v.

3.1 Assumptions on f
We assume that the function f is a twice continuously differentiable function from Rd to R that has
an L-Lipschitz continuous gradient and is m-strongly convex. That is  there exist positive constants
L and m such that for all x  y ∈ Rd 

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107)   and f (y) ≥ f (x) + (cid:104)∇f (x)  y − x(cid:105) +

(cid:107)x − y(cid:107)2 .

m
2

It is easy to show that these inequalities are equivalent to mId (cid:22) ∇2f (x) (cid:22) LId  where Id is the
m be the condition number. We assume that we have access
identity matrix of dimension d. Let κ = L
to an oracle that  given a point x ∈ Rd  can return the gradient of f at point x  ∇f (x).

3.2 Wasserstein Distance

The pth Wasserstein distance between two probability measures µ and ν is deﬁned as

(cid:18)

(cid:19)1/p

Wp (µ  ν) =

inf

(X Y )∈C(µ ν)

E [(cid:107)X − Y (cid:107)p]

 

where C (µ  ν) is the set of all couplings of µ and ν. In this paper  for any 0 <  < 1  we study the
number of steps needed so that the W2 distance between the distribution of the point our algorithms
generate and the target distribution is smaller than  · D.

4 Algorithms and Results

4.1 Underdamped Langevin Diffusion (ULD)

(cid:16)−f (x) + L(cid:107)v(cid:107)2 /2
(cid:17)

ULD is a random process that evolves according to (3). Our paper studies (3) with u = 1
L.
Under mild conditions  it can be shown that the stationary distribution of (3) is proportional to
. Then  the marginal distribution of x is proportional to exp (−f (x)) . It
exp
can also be shown that the solution to (3) has a contraction property [10  24]  shown in the following
lemma.
Lemma 1 (Theorem 5 of [10]). Let (x0  v0) and (y0  w0) be two arbitrary points in Rd × Rd. Let
(xt  vt) and (yt  wt) be the exact solutions of the underdamped Langevin diffusion after time t. If
(xt  vt) and (yt  wt) are coupled through a shared Brownian motion  then 

E(cid:104)(cid:107)xt − yt(cid:107)2 + (cid:107)(xt + vt) − (yt + wt)(cid:107)2(cid:105) ≤ e− t

κ E(cid:104)(cid:107)x0 − y0(cid:107)2 + (cid:107)(x0 + v0) − (y0 + w0)(cid:107)2(cid:105)

.

This contraction bound can be very useful for showing the convergence of the continuous process
(3). In our algorithm  we discretize the continuous process to implement it; therefore we need to
use this contraction bound together with a discretization error bound to show the guarantee of our
algorithm. In Section 4.2  we show how we discretize (3).

5

Algorithm 1 Randomized Midpoint Method for ULD
1: Procedure RandomMidpoint(x0  v0  N  h)
2: For n = 0  ...  N − 1
3:
4:

Randomly sample α uniformly from [0  1].
W (n)
Generate Gaussian random variable

(cid:16)
2 u(cid:0)αh − 1
(cid:0)1 − e−2αh(cid:1) vn − 1
2 uh(cid:0)1 − e−2(h−αh)(cid:1)∇f (xn+ 1
(cid:0)1 − e−2h(cid:1) vn − 1

2 (1 − e−2αh)(cid:1)∇f (xn) +

(cid:17) ∈ R3d as in Appendix A

1
uW (n)
.

uW (n)

  W (n)

  W (n)

√

√

) +

2

1

2

3

.

2

5:

6:

2

= xn + 1
xn+ 1
2
xn+1 = xn + 1
2
vn+1 = vne−2h − uhe−2(h−αh)∇f (xn+ 1

√

) + 2

2

uW (n)

3

.

7:
8: end for
9: end procedure

4.2 Randomized Midpoint Method

Our step size for each iteration is h. In iteration n of our algorithm  to simulate (3)  we need to
approximate the solution to SDE (3) at time h  (x∗
n(h))  with initial value  (xn  vn). The
simplest way to do so is to use the Euler method:

n(h)  v∗

vn(h) = (1 − 2h)vn − uh∇f (xn) + 2

uhζ 

xn(h) = xn + hvn 

√

(cid:90) t

(cid:16)
1 − e−2(t−s)(cid:17)∇f (x∗

where ζ ∈ Rd is drawn from the standard normal distribution. This discretization was considered in
[20  13] due to its simplicity.
As discussed in Section 1.1  we improve the accuracy by studying the integral formulation of (3):

1 − e−2t

(cid:18)(cid:90) t

vn − u
2

x∗
n(t) = xn +
2
n(t) = vne−2t − u
v∗
[10] considered the same integral formulation and used ∇f (xn) to approximate ∇f (x∗
t ∈ [0  h] to get the following algorithm:

e−2(t−s)∇f (x∗

e−2(t−s) dBs.

n(s)) ds +

n(s)) ds

(cid:90) t

(cid:19)

+ 2

√

u

u

0

0

0

0

(cid:16)

1 − e−2(h−s)(cid:17)∇f (xn) ds +

√

u

(cid:90) h

(cid:16)

1 − e−2(h−s)(cid:17)

1 − e−2h

dBs 

(4)

n(t)) for

dBs 

(cid:90) t

(cid:16)

1 − e−2(t−s)(cid:17)

√

(cid:90) h

0

ˆxn(h) = xn +

2
ˆvn(h) = vne−2h − u

vn − u
2

(cid:32)(cid:90) h

(cid:33)

(cid:90) h

e−2(h−s)∇f (xn) ds

0

0

√

u

+ 2

0

e−2(h−s) dBs.

However  this approximation method can still generate a relatively large error. Our paper proposes
a new method  the randomized midpoint method  to solve (4)  which yields a more accurate approx-
imation and signiﬁcantly reduces the total runtime of the algorithm.

We ﬁrst need to identify an accurate estimator of the integral (cid:82) h
point from [0  h]. Then  h(cid:0)1 − e−2(h−αh)(cid:1)∇f (x∗
(cid:0)1 − e−2(h−s)(cid:1)∇f (x∗
(cid:82) h

n(s)) ds.
To do so  we sample a random number α uniformly from [0  1] so that αh gives a random
n(αh)) is an accurate estimator of the integral

(cid:0)1 − e−2(h−s)(cid:1)∇f (x∗

0

n(s)) ds. We can further show that this estimator is unbiased.
to denote our approximation of x∗
n(αh). To approximate x∗

n(αh)  we use

0
For brevity  we use xn+ 1
equation (4) again:

2

1 − e−2αh

2

vn− u
2

(cid:90) αh
1 − e−2(αh−s)(cid:17)∇f (xn)ds+
(cid:16)
1 − e−2(h−αh)(cid:17)∇f (xn+ 1
(cid:16)

) +

h

0

2

√

u

√

u

(cid:90) αh
1 − e−2(αh−s)(cid:17)
(cid:16)
(cid:90) h
(cid:16)
1 − e−2(h−s)(cid:17)

0

dBs 

dBs.

0

n(h)) can be approximated as
1 − e−2h

vn − u
2

2

= xn+

2

xn+ 1
Then  (x∗

n(h)  v∗

xn+1 = xn +

6

(cid:90) h

√

vn − u
2

vn+1 = vne−2h − uhe−2(h−αh)∇f (xn+ 1
(cid:16)
Note that we can view (4) as the ﬁxed point of the operator T   x∗
T (x)(t) = xn +

1 − e−2(t−s)(cid:17)∇f (x(s)) ds +

1 − e−2t

e−2(h−s) dBs.

(cid:90) t

) + 2

u

0

2

n = T (x∗
n)  where for all t 
√

1 − e−2(t−s)(cid:17)

(cid:16)

(cid:90) t

0

0

2

u

dBs.
(5)
Then  our randomized algorithm is essentially approximating T (T (xn)). Under the assumption f
is twice differentiable  we show that two iterations sufﬁce to achieve the best theoretical guarantee 
but we suspect more iterations might be useful if f has higher order derivatives. As emphasized in
Section 1.1  the way we obtain our algorithm forms a general framework that can be applied to other
SDEs.

In Lemma 5  we show that the stochastic terms W1 = (cid:82) αh
(cid:0)1 − e−2(h−s)(cid:1) dBs  and W3 =(cid:82) h
(cid:82) h

0 e−2(h−s) dBs conditional on the choice of α follow a multi-
0
dimensional Gaussian distribution and therefore can be easily sampled. The steps mentioned above
are summarized in Algorithm 1. Using this randomized midpoint method  we can solve (4) much
more accurately than previous works. We show that the discretization error satisﬁes:
Lemma 2. For each iteration n of Algorithm 1  let Eα be the expectation taken over the random
choice of α in iteration n. Let E be the expectation taken over other randomness in iteration n.
Let (x∗
n(t))t∈[0 h] be the solution of the exact underdamped Langevin diffusion starting from
  vn and xn+1. Assume that h ≤ 1
(xn  vn) coupled through a shared Brownian motion with xn+ 1
and u = 1

(cid:0)1 − e−2(αh−s)(cid:1) dBs  W2 =

n(t)  v∗

20

0

2

L . Then  xn+1 and vn+1 of Algorithm 1 satisfy
E(cid:107)Eαxn+1 − x∗
E(cid:107)xn+1 − x∗
E(cid:107)Eαvn+1 − v∗
E(cid:107)vn+1 − v∗

n(h)(cid:107)2 ≤ O
n(h)(cid:107)2 ≤ O
n(h)(cid:107)2 ≤ O
n(h)(cid:107)2 ≤ O

 

(cid:16)
(cid:16)
(cid:16)
(cid:16)

h10 (cid:107)vn(cid:107)2 + u2h12 (cid:107)∇f (xn)(cid:107)2 + udh11(cid:17)
h6 (cid:107)vn(cid:107)2 + u2h4 (cid:107)∇f (xn)(cid:107)2 + udh7(cid:17)
h8 (cid:107)vn(cid:107)2 + u2h10 (cid:107)∇f (xn)(cid:107)2 + udh9(cid:17)
h4 (cid:107)vn(cid:107)2 + u2h4 (cid:107)∇f (xn)(cid:107)2 + udh5(cid:17)
(cid:18)

L

 

 

In Appendix D  we show that the average value of (cid:107)vn(cid:107)2 is of order ˜O(cid:0) d

.

(cid:1); that of (cid:107)∇f (xn)(cid:107)2 is of
(cid:19)

h4(cid:113) d

and

L

order ˜O (Ld). Then  Lemma 2 shows that the bias of the discretization is of order ˜O

(cid:18)

h2(cid:113) d

(cid:19)

L

the standard deviation is of order ˜O
  which implies the error is larger when h is larger.
However  by Lemma 1  in order for the algorithm to converge in a small number of steps  we need
to avoid choosing an h that is too small. Therefore  it is important to choose the largest possible
h that can still make the algorithm converge. By Lemma 1  it is sufﬁcient to run our algorithm
  and the

(cid:1) iterations. Then  the bias will cumulate to ˜O
(cid:18)
(cid:19)
h2(cid:113) d
L ·(cid:112) κ

(cid:19)
h4(cid:113) d
(cid:18)
h1.5(cid:113) d

standard deviation will cumulate to ˜O

(cid:18)
h3(cid:113) dκ

for ˜O(cid:0) κ

. Thus  in order to make

L · κ

(cid:18)

(cid:19)

(cid:19)

= ˜O

= ˜O

m

m

h

h

h

(cid:16)

κ1/6   2/3(cid:17)(cid:17)
(cid:16) 1/3

the W2 distance less than ˜O



  we show in Theorem 3 that it is enough to choose h to be

min

. This choice of h yields the main result of our paper  which is stated in

˜Θ
Theorem 3. (See Appendix E for the full proof.)
Theorem 3 (Main Result). Let f be a function such that 0 ≺ m · Id (cid:22) ∇2f (x) (cid:22) L · Id for all
x ∈ Rd. Let Y be a random point drawn from the density proportional to e−f . Let the starting
point x0 be the point that minimizes f (x) and v0 = 0. For any 0 <  < 1  if we set the step size
of Algorithm 1 as h = C min
  for some small constant C

(cid:1)(cid:17)

(cid:16) 1/3
h log(cid:0) 20

κ1/6 log

−1/6(cid:0) 1
(cid:1) ≤ ˜O

2



(cid:1)   2/3 log
(cid:16) κ7/6

−1/3(cid:0) 1
(cid:17)

1/3 + κ

2/3


iterations  then Algorithm 1 after

and run the algorithm for N = 2κ

(cid:18)

(cid:19)

(cid:113) d

m

7

Algorithm 2 Randomized Midpoint Method for ULD (Parallel)
1: Procedure RandomMidpoint_P(x0  v0  N  h  R)
2: For n = 0  ...  N − 1
3:
4:

Randomly sample α1  ...  αR uniformly from(cid:2)0  1

  W (n)

W (n)

1 1   ...  W (n)

1 R  W (n)

R

2

3

(cid:3)  ... (cid:2) R−1
R   1(cid:3).

(cid:3) (cid:2) 1
(cid:17) ∈ R(R+2)d similar to Appendix A

R   2

R

(cid:16)
(cid:0)1 − e−2αih(cid:1) vn
(cid:104)(cid:82) min(jδ αih)
(cid:0)1 − e−2h(cid:1) vn − 1

(j−1)δ

Generate Gaussian r.v.
x(0 i)
n = xn for i = 1  ...  R.
For k = 1  ...  K − 1  i = 1  ...  R

2 u(cid:80)i

x(k i)
n = xn + 1
2
− 1
end for
xn+1 = xn + 1
2

5:
6:
7:
8:
9:
10:
11:
12: end for
13: end procedure

vn+1 = vne−2h − u(cid:80)R

j=1

(cid:0)1 − e−2(αih−s)(cid:1) ds · ∇f (x(k−1 j)
2 u(cid:80)R

i=1 δ(cid:0)1 − e−2(h−αih)(cid:1)∇f (x(K−1 i)

√

+

)

n

n

(cid:105)

uW (n)
1 i

√

i=1 δe−2(h−αih)∇f (x(K−1 i)

n

) + 2

uW (n)

.

3

√

uW (n)

2

 

) +

N iterations can generate a random point X such that W2(X  Y ) ≤ 
iteration of Algorithm 1 involves computing ∇f exactly twice.

4.3 A More General Algorithm

(cid:113) d

m . Furthermore  each

Now we show how our algorithm can be parallelized. The algorithm studied in this section can be
viewed as a more general version of Algorithm 1. Instead of choosing one random point from [0  h] 
we divide the time interval [0  h] into R pieces  each of length δ = h
R   and choose one random point

from each piece. That is  we randomly choose α1  α2  ...  αR uniformly from(cid:2)0  1
(cid:2) R−1
R   1(cid:3). As in Algorithm 1  to approximate (x∗
(cid:16)

(cid:3) (cid:2) 1
(cid:3)  ... 
1 − e−2(h−s)(cid:17)

n(h))  we use

1 − e−2h

(cid:90) h

R(cid:88)

R   2

(cid:16)

√

˜x = xn +

n(αih)) +

u

δ

R

R

dBs 

0

n(h)  v∗

1 − e−2(h−αih)(cid:17)∇f (x∗
(cid:90) h

√

vn − u
2

i=1

2

R(cid:88)

˜v = vne−2h − u

δe−2(h−αih)∇f (x∗(αih)) + 2

u

e−2(h−s) dBs 

i=1

0

n(h)  v∗
which gives an unbiased estimator of (x∗
n(αih) for
n is the ﬁxed point of the operator T deﬁned in (5). To
i = 1  ..  R. We know that the solution x∗
solve the ﬁxed point of T   we can use the ﬁxed point iteration method  which applies the operator
T multiple times on some initial point. By the Banach ﬁxed point theorem  the resulting points can
converge to the ﬁxed point of T . Instead of applying T   which involves computing an integral  we

apply the operator ˜T   which approximates T   on X =(cid:0)x(1)  ...  x(R)(cid:1)  

n(h)). The next step is to approximate x∗

(cid:34)(cid:90) min(jδ αih)

i(cid:88)

u

(j−1)δ

j=1

(cid:16)

1 − e−2(αih−s)(cid:17)

(cid:35)

ds · ∇f (x(j))

(cid:0)1 − e−2αih(cid:1) vn − 1
˜T (X)i = xn +
(cid:90) αih
1 − e−2(αih−s)(cid:17)
(cid:16)

√

1
2

+

u

2

dBs.

0

n

= xn for j = 1  ...  R. Then  we apply ˜T for K times and get
We set the initial points to x(0 j)
(x(K 1)  ...  x(K R)) = ˜T ◦K(x(0 1)  ...  x(0 R)). The preceding steps are summarized in Algorithm
2. It is easy to see Algorithm 1 is a special case of Algorithm 2 with R = 1 and K = 2.
This algorithm can be parallelized since we can compute ˜T (x(k 1)  ...  x(k R))j for each j parallelly.
It can be shown that it is sufﬁcient to choose K to depend logarithmically on κ and . Similar to
Algorithm 1  we can show that Algorithm 2 has the guarantee that the bias of the discretization
is of order ˜O
(Appendix F). Then 

and the standard deviation is of order ˜O

(cid:113) d

(cid:113) d

(cid:18)

(cid:19)

(cid:18)

(cid:19)

h2
R

L

h4
R

L

8

Figure 1: Error of random walks with different choice of step size.

(cid:18)

(cid:113) dκ

m

h3
R

(cid:19)

  and

(cid:18)

(cid:113) d
(cid:18)

h4
R

L · κ

(cid:19)
(cid:113) d
(cid:113) d

h

m

= ˜O

(cid:19)

the total standard deviation would be ˜O

= ˜O

h1.5
R

. By choosing R =

summing from ˜O(cid:0) κ
(cid:16)√
the algorithm needs only O(cid:0) κ

(cid:17)

κ


˜Θ

h

h

h2
R

(cid:18)

(cid:1) iterations  the total bias would be ˜O
(cid:19)
(cid:113) d
L ·(cid:112) κ
(cid:1) = O(κ log 1
(cid:113) d
(cid:16)√

h log 1



  it is enough to choose h to be a constant to achieve less than 

m error  which shows that
 ) parallel steps. Appendix F gives a partial proof
of the guarantee of Algorithm 2. The other part of the proof is similar to that in Algorithm 1  so we
omit it here.
Theorem 4. Let f be a function such that 0 ≺ m · Id (cid:22) ∇2f (x) (cid:22) L · Id for all x ∈ Rd. Let Y
be a random point drawn from the density proportional to e−f . Algorithm 2 can generate a random
point X such that W2(X  Y ) ≤ 
 ) parallel steps. Furthermore  each iteration of
Algorithm 2 involves computing ˜Θ

m in O(κ log 1
of ∇fs.

(cid:17)

κ


5 Numerical Experiments

In this section  we compare the algorithm from our paper  randomized midpoint method  with the one
from [10]. We test the algorithms on the liver-disorders dataset and the breast-cancer dataset from
UCL machine learning [17]. In both datasets  we observe a set of independent samples {xi  yi}m
i=1 
where yi is the label  xi is the feature and m is the number of samples. We sample from the target
distribution p∗(θ) ∝ exp (−f (θ))   where
(cid:107)θ(cid:107)2 +

log(cid:0)exp(cid:0)−yixT

i θ(cid:1) + 1(cid:1)  

m(cid:88)

f (θ) =

λ
2

1
m

i=1

for regularization parameters λ. We set λ to be 10−2 in our experiments. Figure 1 shows the error
of randomized midpoint method and the algorithm from [10] with different step size h. The error
is measured by the (cid:96)2 distance to the true solution of (3) at time N = 5000  a time much greater
than the mixing time of (3) for both datasets. Our results show that the  dependence analysis of our
algorithm and that of [10] are both tight. However  we note that the logistic function is inﬁnitely
differentiable  so there are methods of higher orders for this objective such as the standard midpoint
method and Runge–Kutta methods.

References
[1] Christophe Andrieu  Nando De Freitas  Arnaud Doucet  and Michael I Jordan. An introduction

to MCMC for machine learning. Machine Learning  50(1-2):5–43  2003.

[2] David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions.
In Proceedings of the Twenty-Third Annual ACM Symposium on Theory of Computing  pages
156–163. ACM  1991.

9

[3] David Yudin Arkadii Nemirovsky. Problem complexity and method efﬁciency in optimization.

Wiley-Interscience Series in Discrete Mathematics. John Wiley & Sons  1983.

[4] Claude JP Bélisle  H Edwin Romeijn  and Robert L Smith. Hit-and-Run algorithms for gener-

ating multivariate distributions. Mathematics of Operations Research  18(2):255–266  1993.

[5] Nawaf Bou-Rabee and Martin Hairer. Nonasymptotic mixing of the MALA algorithm. IMA

Journal of Numerical Analysis  33(1):80–110  03 2012.

[6] Niladri S Chatterji  Nicolas Flammarion  Yi-An Ma  Peter L Bartlett  and Michael I Jor-
dan. On the theory of variance reduction for stochastic gradient Monte Carlo. arXiv preprint
arXiv:1802.05431  2018.

[7] Zongchen Chen and Santosh S Vempala. Optimal convergence rate of Hamiltonian Monte

Carlo for strongly logconcave distributions. arXiv preprint arXiv:1905.02313  2019.

[8] Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv

preprint arXiv:1705.09048  2017.

[9] Xiang Cheng  Niladri S Chatterji  Yasin Abbasi-Yadkori  Peter L Bartlett  and Michael I Jor-
dan. Sharp convergence rates for Langevin dynamics in the nonconvex setting. arXiv preprint
arXiv:1805.01648  2018.

[10] Xiang Cheng  Niladri S Chatterji  Peter L Bartlett  and Michael I Jordan. Underdamped

Langevin MCMC: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663  2017.

[11] Benjamin Cousins and Santosh Vempala. Bypassing KLS: Gaussian cooling and a cubic vol-
ume algorithm. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of
Computing  STOC ’15  pages 539–548  New York  NY  USA  2015. ACM.

[12] Arnak S Dalalyan. Further and stronger analogy between sampling and optimization: Langevin

Monte Carlo and gradient descent. arXiv preprint arXiv:1704.04752  2017.

[13] Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-
concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
79(3):651–676  2017.

[14] Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the Langevin Monte

Carlo with inaccurate gradient. Stochastic Processes and Their Applications  2019.

[15] Arnak S Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using

kinetic Langevin diffusions. arXiv preprint arXiv:1807.09382  2018.

[16] Joseph Leo Doob. Stochastic Processes  volume 101. New York  Wiley  1953.

[17] Dheeru Dua and Casey Graff. UCI machine learning repository  2017.

[18] Alain Durmus  Szymon Majewski  and Blazej Miasojedow. Analysis of langevin monte carlo

via convex optimization. Journal of Machine Learning Research  20(73):1–46  2019.

[19] Alain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted

Langevin algorithm. arXiv preprint arXiv:1605.01559  2016.

[20] Alain Durmus and Eric Moulines. Nonasymptotic convergence analysis for the unadjusted

Langevin algorithm. The Annals of Applied Probability  27(3):1551–1587  2017.

[21] Raaz Dwivedi  Yuansi Chen  Martin J Wainwright  and Bin Yu. Log-concave sampling:

Metropolis-Hastings algorithms are fast! arXiv preprint arXiv:1801.02309  2018.

[22] Martin Dyer and Alan Frieze. Computing the volume of convex bodies: a case where random-

ness provably helps. Probabilistic Combinatorics and Its Applications  44:123–170  1991.

[23] Martin Dyer  Alan Frieze  and Ravi Kannan. A random polynomial-time algorithm for ap-

proximating the volume of convex bodies. Journal of the ACM (JACM)  38(1):1–17  1991.

10

[24] Andreas Eberle  Arnaud Guillin  and Raphael Zimmer. Couplings and quantitative contraction

rates for Langevin dynamics. arXiv preprint arXiv:1703.01617  2017.

[25] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in Rˆd.

In 29th IEEE Conference on Decision and Control  pages 220–221 vol.1  Dec 1990.

[26] Søren Fiig Jarner and Ernst Hansen. Geometric ergodicity of Metropolis algorithms. Stochastic

Processes and Their Applications  85(2):341–361  2000.

[27] Ravi Kannan  Laszlo Lovasz  and Miklos Simonovits. Random walks and an o*(n5) volume

algorithm for convex bodies. Random Structures & Algorithms  11(1):1–50  1997.

[28] Hendrik Anthony Kramers. Brownian motion in a ﬁeld of force and the diffusion model of

chemical reactions. Physica  7(4):284–304  1940.

[29] Yin Tat Lee  Zhao Song  and Santosh S Vempala. Algorithmic theory of ODEs and sampling

from well-conditioned logconcave densities. arXiv preprint arXiv:1812.06243  2018.

[30] Yin Tat Lee and Santosh S. Vempala. Geodesic walks in polytopes. In Proceedings of the 49th
Annual ACM SIGACT Symposium on Theory of Computing  STOC 2017  pages 927–940  New
York  NY  USA  2017. ACM.

[31] Yin Tat Lee and Santosh S. Vempala. Convergence rate of riemannian Hamiltonian Monte
In Proceedings of the 50th Annual ACM
Carlo and faster polytope volume computation.
SIGACT Symposium on Theory of Computing  STOC 2018  pages 1115–1121  New York 
NY  USA  2018. ACM.

[32] Ernest Lindelof. Sur lapplication de la methode des approximations successives aux equations
differentielles ordinaires du premier ordre. Comptes rendus hebdomadaires des seances de
lAcademie des sciences  116(3):454–457  1894.

[33] László Lovász. Hit-and-Run mixes fast. Mathematical Programming  86(3):443–461  1999.

[34] László Lovász and Miklós Simonovits. The mixing rate of Markov chains  an isoperimetric in-
equality  and computing the volume. In Proceedings.  31st Annual Symposium on Foundations
of Computer Science  pages 346–354. IEEE  1990.

[35] László Lovász and Miklós Simonovits. Random walks in a convex body and an improved

volume algorithm. Random structures & algorithms  4(4):359–412  1993.

[36] László Lovász and Santosh Vempala. Hit-and-Run from a corner. SIAM Journal on Computing 

35(4):985–1005  2006.

[37] László Lovász and Santosh Vempala. Simulated annealing in convex bodies and an o*(n4)

volume algorithm. Journal of Computer and System Sciences  72(2):392–417  2006.

[38] László Lovász and Santosh Vempala. The geometry of logconcave functions and sampling

algorithms. Random Structures & Algorithms  30(3):307–358  2007.

[39] Yi-An Ma  Niladri Chatterji  Xiang Cheng  Nicolas Flammarion  Peter Bartlett  and
Is there an analog of nesterov acceleration for MCMC? arXiv preprint

Michael I. Jordan.
arXiv:1902.00996  2019.

[40] Yi-An Ma  Tianqi Chen  and Emily Fox. A complete recipe for stochastic gradient MCMC. In

Advances in Neural Information Processing Systems  pages 2917–2925  2015.

[41] Oren Mangoubi and Aaron Smith. Rapid mixing of Hamiltonian Monte Carlo on strongly

log-concave distributions. arXiv preprint arXiv:1708.07114  2017.

[42] Oren Mangoubi and Nisheeth Vishnoi. Dimensionally tight bounds for second-order Hamilto-
nian Monte Carlo. In Advances in Neural Information Processing Systems  pages 6027–6037 
2018.

11

[43] Oren Mangoubi and Nisheeth K. Vishnoi. Faster algorithms for polytope rounding  sampling 

and volume computation via a sublinear "ball walk”  2019.

[44] Kerrie L Mengersen and Richard L Tweedie. Rates of convergence of the Hastings and

Metropolis algorithms. The Annals of Statistics  24(1):101–121  1996.

[45] Wenlong Mou  Yi-An Ma  Martin J Wainwright  Peter L Bartlett  and Michael I Jor-
dan. High-order langevin diffusion yields an accelerated mcmc algorithm. arXiv preprint
arXiv:1908.10859  2019.

[46] Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte

cCarlo  2(11):2  2011.

[47] Marcelo Pereyra. Proximal Markov chain Monte Carlo algorithms. Statistics and Computing 

26(4):745–760  Jul 2016.

[48] Emile Picard. Sur les methodes dapproximations successives dans la theorie des equations

differentielles. American Journal of Mathematics  pages 87–100  1898.

[49] Natesh S Pillai  Andrew M Stuart  and Alexandre H Thiéry. Optimal scaling and diffusion
limits for the Langevin algorithm in high dimensions. The Annals of Applied Probability 
22(6):2320–2356  2012.

[50] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via stochas-
tic gradient Langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849 
2017.

[51] Gareth O. Roberts and Jeffrey S. Rosenthal. Optimal scaling of discrete approximations to

Langevin diffusions. Journal of the Royal Statistical Society: Series B  60:255–268  1997.

[52] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions

and their discrete approximations. Bernoulli  2(4):341–363  1996.

[53] Gareth O Roberts and Richard L Tweedie. Geometric convergence and central limit theorems

for multidimensional Hastings and Metropolis algorithms. Biometrika  83(1):95–110  1996.

[54] Daniel J Russo  Benjamin Van Roy  et al. A tutorial on thompson sampling. Foundations and

Trends R(cid:13) in Machine Learning  11(1):1–96  2018.

[55] Santosh S Vempala. Recent progress and open problems in algorithmic convex geometry. In
IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer
Science (FSTTCS 2010). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik  2010.

[56] Tatiana Xifara  Chris Sherlock  Samuel Livingstone  Simon Byrne  and Mark Girolami.
Langevin diffusions and the Metropolis-adjusted Langevin algorithm. Statistics & Probability
Letters  91:14–19  2014.

[57] Yuchen Zhang  Percy Liang  and Moses Charikar. A hitting time analysis of stochastic gradient

Langevin dynamics. arXiv preprint arXiv:1702.05575  2017.

12

,Hu Ding
Ronald Berezney
Jinhui Xu
Pritish Mohapatra
C.V. Jawahar
M. Pawan Kumar
Yunchen Pu
Zhe Gan
Ricardo Henao
Xin Yuan
Chunyuan Li
Andrew Stevens
Lawrence Carin
Ruoqi Shen
Yin Tat Lee