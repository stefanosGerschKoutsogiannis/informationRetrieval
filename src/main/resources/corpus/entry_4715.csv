2019,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach,Many generative models have to combat missing modes. The conventional wisdom to this end is by reducing through training a statistical distance (such as f -divergence) between the generated distribution and provided data distribution. But this is more of a heuristic than a guarantee. The statistical distance measures a global  but not local  similarity between two distributions. Even if it is small  it does not imply a plausible mode coverage. Rethinking this problem from a game-theoretic perspective  we show that a complete mode coverage is firmly attainable. If a generative model can approximate a data distribution moderately well under a global statistical distance measure  then we will be able to find a mixture of generators that collectively covers every data point and thus every mode  with a lower-bounded generation probability. Constructing the generator mixture has a connection to the multiplicative weights update rule  upon which we propose our algorithm. We prove that our algorithm guarantees complete mode coverage. And our experiments on real and synthetic datasets confirm better mode coverage over recent approaches  ones that also use generator mixtures but rely on global statistical distances.,Rethinking Generative Mode Coverage:

A Pointwise Guaranteed Approach

Peilin Zhong⇤

Yuchen Mo⇤

Chang Xiao⇤
Columbia University

Pengyu Chen

Changxi Zheng

{peilin  chang  cxz}@cs.columbia.edu
{yuchen.mo  pengyu.chen}@columbia.edu

Abstract

Many generative models have to combat missing modes. The conventional wis-
dom to this end is by reducing through training a statistical distance (such as
f-divergence) between the generated distribution and provided data distribution.
But this is more of a heuristic than a guarantee. The statistical distance measures
a global  but not local  similarity between two distributions. Even if it is small 
it does not imply a plausible mode coverage. Rethinking this problem from a
game-theoretic perspective  we show that a complete mode coverage is ﬁrmly
attainable. If a generative model can approximate a data distribution moderately
well under a global statistical distance measure  then we will be able to ﬁnd a
mixture of generators that collectively covers every data point and thus every mode 
with a lower-bounded generation probability. Constructing the generator mixture
has a connection to the multiplicative weights update rule  upon which we propose
our algorithm. We prove that our algorithm guarantees complete mode coverage.
And our experiments on real and synthetic datasets conﬁrm better mode coverage
over recent approaches  ones that also use generator mixtures but rely on global
statistical distances.

1

Introduction

A major pillar of machine learning  the generative approach aims at learning a data distribution from
a provided training dataset. While strikingly successful  many generative models suffer from missing
modes. Even after a painstaking training process  the generated samples represent only a limited
subset of the modes in the target data distribution  yielding a much lower entropy distribution.
Behind the missing mode problem is the conventional wisdom of training a generative model.
Formulated as an optimization problem  the training process reduces a statistical distance between the
generated distribution and the target data distribution. The statistical distance  such as f-divergence
or Wasserstein distance  is often a global measure.
It evaluates an integral of the discrepancy
between two distributions over the data space (or a summation over a discrete dataset). In practice 
reducing the global statistical distance to a perfect zero is virtually a mission impossible. Yet a
small statistical distance does not certify the generator complete mode coverage. The generator may
neglect underrepresented modes—ones that are less frequent in data space—in exchange for better
matching the distribution of well represented modes  thereby lowering the statistical distance. In
short  a global statistical distance is not ideal for promoting mode coverage (see Figure 1 for a 1D
motivating example and later Figure 2 for examples of a few classic generative models).
This inherent limitation is evident in various types of generative models (see Appendix A for the
analysis of a few classic generative models). Particularly in generative adversarial networks (GANs) 
mode collapse has been known as a prominent issue. Despite a number of recent improvements
toward alleviating it [1  2  3  4  5  6]  none of them offers a complete mode coverage. In fact  even the

⇤equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

fundamental question remains unanswered: what precisely does a complete mode coverage mean?
After all  the deﬁnition of “modes” in a dataset is rather vague  depending on what speciﬁc distance
metric is used for clustering data items (as discussed and illustrated in [4]).
We introduce an explicit notion of complete mode
coverage  by switching from the global statistical
distance to local pointwise coverage: provided a
target data distribution P with a probability density
p(x) at each point x of the data space X   we claim
that a generator G has a complete mode coverage of
P if the generator’s probability g(x) for generating
x is pointwise lower bounded  that is 
g(x)  · p(x) 8x 2X  

Figure 1: Motivating example. Consider a
1D target distribution P with three modes  i.e.  a
mixture of three Gaussians  P = 0.9·N (0  1)+
0.05 · N (10  1) + 0.05 · N (10  1) (solid or-
ange curve). If we learn this distribution using
a single Gaussian Q (black dashed curve). The
statistical distance between the two is small:
DTV(Q k P )  0.1 and DKL(Q k P )  0.16.
The probability of drawing samples from the
side modes (in [14 6] and [6  14]) of the tar-
get distribution P is Prx⇠P [6 | x| 14] ⇡
0.1  but the probability of generating samples
from Q in the same intervals is Prx⇠Q[6 
|x| 14] ⇡ 109. The side modes are missed!

(1)
for a reasonably large relaxation constant 2
(0  1). This notion of mode coverage ensures that
every point x in the data space X will be generated
by G with a ﬁnite and lower-bounded probability
g(x). Thereby  in contrast to the generator trained
by reducing a global statistical distance (recall Fig-
ure 1)  no mode will have an arbitrarily small gener-
ation probability  and thus no mode will be missed.
Meanwhile  our mode coverage notion (1) stays
compatible with the conventional heuristic toward
reducing a global statistical distance  as the satisfac-
tion of (1) implies that the total variation distance
between P and G is upper bounded by 1  (see a proof in Appendix C).
At ﬁrst sight  the pointwise condition (1) seems more stringent than reducing a global statistical
distance  and pursuing it might require a new formulation of generative models. Perhaps somewhat
surprisingly  a rethink from a game-theoretic perspective reveal that this notion of mode coverage is
viable without formulating any new models. Indeed  a mixture of existing generative models (such as
GANs) sufﬁces. In this work  we provide an algorithm for constructing the generator mixture and a
theoretical analysis showing the guarantee of our mode coverage notion (1).

1.1 A Game-Theoretic Analysis
Before delving into our algorithm  we offer an intuitive view of why our mode coverage notion (1)
is attainable through a game-theoretic lens. Consider a two-player game between Alice and Bob:
given a target data distribution P and a family G of generators2  Alice chooses a generator G 2G  
and Bob chooses a data point x 2X . If the probability density g(x) of Alice’s G generating Bob’s
choice of x satisﬁes g(x)  1
4 p(x)  the game produces a value v(G  x) = 1  otherwise it produces
v(G  x) = 0. Here 1/4 is used purposely as an example to concretize our intuition. Alice’s goal is to
maximize the game value  while Bob’s goal is to minimize the game value.
Now  consider two situations. In the ﬁrst situation  Bob ﬁrst chooses a mixed strategy  that is  a
distribution Q over X . Then  Alice chooses the best generator G 2G according to Bob’s distribution
Q. When the game starts  Bob samples a point x using his choice of distribution Q. Together
with Alice’s choice G  the game produces a value. Since x is now a random variable over Q  the
expected game value is maxG2G E
[v(G  x)]. In the second situation  Alice ﬁrst chooses a mixed
x⇠Q
strategy  that is  a distribution RG of generators over G. Then  given Alice’s choice RG  Bob chooses
the best data point x 2X . When the game starts  Alice samples a generator G from the chosen
distribution RG. Together with Bob’s choice of x  the game produces a value  and the expected value
is minx2X EG⇠RG
According to von Neumann’s minimax theorem [7  8]  Bob’s optimal expected value in the ﬁrst
situation must be the same as Alice’s optimal value in the second situation:
[v(G  x)].

[v(G  x)].

(2)

min
Q

max
G2G

E
x⇠Q

[v(G  x)] = max
RG

min
x2X

E

G⇠RG

2An example of the generator family is the GANs. The deﬁnition will be made clear later in this paper.

2

target distributiongenerated distribution0614-6-14With this equality realized  our agenda in the rest of the analysis is as follows. First  we show a lower
bound of the left-hand side of (2)  and then we use the right-hand side to reach the lower-bound
of g(x) as in (1)  for Alice’s generator G. To this end  we need to depart off from the current
game-theoretic analysis and discuss the properties of existing generative models for a moment.
Existing generative models such as GANs [9  1  10] aim to reproduce arbitrary data distributions.
While it remains intractable to have the generated distribution match exactly the data distribution 
the approximations are often plausible. One reason behind the plausible performance is that the data
space encountered in practice is “natural” and restricted—all English sentences or all natural object
images or all images on a manifold—but not a space of arbitrary data. Therefore  it is reasonable to
expect the generators in G (e.g.  all GANs) to meet the following requirement3 (without conﬂicting
the no-free-lunch theorem [11]): for any distribution Q over a natural data space X encountered
in practice  there exists a generator G 2G such that the total variation distance between G and Q
2RX |q(x)  g(x)| dx    where q(·) and g(·) are the
is upper bounded by a constant   that is  1
probability densities on Q and the generated samples of G  respectively. Again as a concrete example 
we use  = 0.1. With this property in mind  we now go back to our game-theoretic analysis.
Back to the ﬁrst situation described above. Once Bob’s distribution Q (over X ) and Alice’s generator
G are identiﬁed  then given a target distribution P over X and an x drawn by Bob from Q  the
probability of having Alice’s G cover P (i.e.  g(x)  1
4 p(x)) at x is lower bounded. In our current
example  we have the following lower bound:

Pr
x⇠Q

[g(x)  1/4 · p(x)]  0.4.

(3)

Here 0.4 is related to the total variation distance bound (i.e.   = 0.1) between G and Q  and this
lower bound value is derived in Appendix D. Next  notice that on the left-hand side of (2)  the
expected value  Ex⇠Q[v(G  x)]  is equivalent to the probability in (3). Thus  we have

min
Q

max
G2G

E
x⇠Q

[v(G  x)]  0.4.

(4)

Because of the equality in (2)  this is also the lower bound of its right-hand side  from which we know
that there exists a distribution RG of generators such that for any x 2X   we have

(5)

E

G⇠RG

[v(G  x)] = Pr
G⇠RG

[g(x)  1/4 · p(x)]  0.4.

This expression shows that for any x 2X   if we draw a generator G from RG  then with a probability
at least 0.4  G’s generation probability density satisﬁes g(x)  1
4 p(x). Thus  we can think RG as a
“collective” generator G⇤  or a mixture of generators. When generating a sample x  we ﬁrst choose
a generator G according to RG and then sample an x using G. The overall probability g⇤(x) of
generating x satisﬁes g⇤(x) > 0.1p(x)—precisely the pointwise lower bound that we pose in (1).
Takeaway from the analysis. This analysis reveals that a complete mode coverage is ﬁrmly viable.
Yet it offers no recipe on how to construct the mixture of generators and their distribution RG using
existing generative models. Interestingly  as pointed out by Arora et al. [12]  a constructive version
of von Neumann’s minimax theorem is related to the general idea of multiplicative weights update.
Therefore  our key contributions in this work are i) the design of a multiplicative weights update
algorithm (in Sec. 3) to construct a generator mixture  and ii) a theoretical analysis showing that
our generator mixture indeed obtains the pointwise data coverage (1). In fact  we only need a small
number of generators to construct the mixture (i.e.  it is easy to train)  and the distribution RG for
using the mixture is as simple as a uniform distribution (i.e.  it is easy to use).

2 Related Work

There exists a rich set of works improving classic generative models for alleviating missing modes 
especially in the framework of GANs  by altering objective functions [13  14  15  10  16  17] 
changing training methods [18  19]  modifying neural network architectures [2  20  21  22  23]  or
regularizing latent space distributions [4  24]. The general philosophy behind these improvements is
to reduce the statistical distance between the generated distribution and target distribution by making

3This requirement is weaker than the mainstream goal of generative models  which all aim to approximate a
target data distribution as closely as possible. Here we only require the approximation error is upper bounded.

3

the models easier to train. Despite their technical differences  their optimization goals are all toward
reducing a global statistical distance.
The idea of constructing a mixture of generators has been explored  with two ways of construction.
In the ﬁrst way  a set of generators are trained simultaneously. For example  Locatello et al. [25]
used multiple generators  each responsible for sampling a subset of data points decided in a k-means
clustering fashion. Other methods focus on the use of multiple GANs [26  27  28]. The theoretical
intuition behind these approaches is by viewing a GAN as a two-player game and extending it to
reach a Nash equilibrium with a mixture of generators [26]. In contrast  our method does not depend
speciﬁcally on GANs  and our game-theoretic view is fundamentally different (recall Sec. 1.1).
Another way of training a mixture of generators takes a sequential approach. This is related to boosting
algorithms in machine learning. Grnarova et al. [29] viewed the problem of training GANs as ﬁnding
a mixed strategy in a zero-sum game  and used the Follow-the-Regularized-Leader algorithm [30]
for training a mixture of generators iteratively. Inspired by AdaBoost [31]  other approaches train a
“weak” generator that ﬁts a reweighted data distribution in each iteration  and all iterations together
form an additive mixture of generators [32  33] or a multiplicative mixture of generators [34].
Our method can be also viewed as a boosting strategy. From this perspective  the most related is
AdaGAN [33]  while signiﬁcant differences exist. Theoretically  AdaGAN (and other boosting-like
algorithms) is based on the assumption that the reweighted data distribution in each iteration becomes
progressively easier to learn. It requires a generator in each iteration to have a statistical distance
to the reweighted distribution smaller than the previous iteration. As we will discuss in Sec. 5 
this assumption is not always feasible. We have no such assumption. Our method can use a weak
generator in each iteration. If the generator is more expressive  the theoretical lower bound of our
pointwise coverage becomes larger (i.e.  a larger in (1)). Algorithmically  our reweighting scheme
is simple and different from AdaGAN  only doubling the weights or leaving them unchanged in each
iteration. Also  in our mixture of generators  they are treated uniformly  and no mixture weights are
needed  whereas AdaGAN needs a set of weights that are heuristically chosen.
To summarize  in stark contrast to all prior methods  our approach is rooted in a different philosophy
of training generative models. Rather than striving for reducing a global statistical distance  our
method revolves around an explicit notion of complete mode coverage as deﬁned in (1). Unlike other
boosting algorithms  our algorithm of constructing the mixture of generators guarantees complete
mode coverage  and this guarantee is theoretically proved.

3 Algorithm
A mixture of generators. Provided a target distribution P on a data domain X   we train a mixture
of generators to pursue pointwise mode coverage (1). Let G⇤ = {G1  . . .   GT} denote the resulting
mixture of T generators. Each of them (Gt  t = 1...T ) may use any existing generative model such
as GANs. Existing methods that also rely on a mixture of generators associate each generator a
nonuniform weight ↵t and choose a generator for producing a sample randomly based on the weights.
Often  these weights are chosen heuristically  e.g.  in AdaGAN [33]. Our mixture is conceptually and
computationally simpler. Each generator is treated equally. When using G⇤ to generate a sample  we
ﬁrst choose a generator Gi uniformly at random  and then use Gi to generate the sample.
Algorithm overview. Our algorithm of training G⇤ can be understood as a speciﬁc rule design in
the framework of multiplicative weights update [12]. Outlined in Algorithm 1  it runs iteratively.
In each iteration  a generator Gt is trained using an updated data distribution Pt (see Line 6-7 of
Algorithm 1). The intuition here is simple: if in certain data domain regions the current generator fails
to cover the target distribution sufﬁciently well  then we update the data distribution to emphasize
those regions for the next round of generator training (see Line 9 of Algorithm 1). In this way  each
generator can focus on the data distribution in individual data regions. Collectively  they are able to
cover the distribution over the entire data domain  and thus guarantee pointwise data coverage.
Training. Each iteration of our algorithm trains an individual generator Gt  for which many existing
generative models  such as GANs [9]  can be used. The only prerequisite is that Gt needs to be
trained to approximate the data distribution Pt moderately well. This requirement arises from our
game-theoretic analysis (Sec. 1.1)  wherein the total variation distance between Gt’s distribution and
Pt needs to be upper bounded. Later in our theoretical analysis (Sec. 4)  we will formally state this
requirement  which  in practice  is easily satisﬁed by most existing generative models.

4

Algorithm 1 Constructing a mixture of generators
1: Parameters: T   a positive integer number of generators  and  2 (0  1)  a covering threshold.
2: Input: a target distribution P on a data domain X .
3: For each x 2X   initialize its weight w1(x) = p(x).
4: for t = 1 ! T do
5:
6:
7:
8:
9:
10: end for
11: Output: a mixture of generators G⇤ = {G1  . . .   GT}.

Construct a distribution Pt over X as follows:
For every x 2X   normalize the probability density pt(x) = wt(x)
Train a generative model Gt on the distribution Pt.
Estimate generated density gt(x) for every x 2X .
For each x 2X   if gt(x) < ·p(x)  set wt+1(x) = 2·wt(x). Otherwise  set wt+1(x) = wt(x).

  where Wt =RX

wt(x)dx.

Wt

Estimation of generated probability density.
In Line 8 of Algorithm 1  we need to estimate the
probability gt(x) of the current generator sampling a data point x. Our estimation follows the idea
of adversarial training  similar to AdaGAN [33]. First  we train a discriminator Dt to distinguish
between samples from Pt and samples from Gt. The optimization objective of Dt is deﬁned as

max
Dt

E
x⇠Pt

[log Dt(x)] + E
x⇠Gt

[log(1  Dt(x))].

=

< 

p(x)Wt

Unlike AdaGAN [33]  here Pt is the currently updated data distribution  not the original target
distribution  and Gt is the generator trained in the current round  not a mixture of generators in all
past rounds. As pointed out previously [35  33]  once Dt is optimized  we have Dt(x) =
for all x 2X   and equivalently gt(x)
testing the data coverage)  we rewrite the condition gt(x) < · p(x) as
Dt(x)  1◆ wt(x)

Dt(x)  1. Using this property in Line 9 of Algorithm 1 (for

=✓ 1

pt(x) = 1

gt(x)
p(x)

gt(x)
pt(x)

pt(x)
p(x)

pt(x)

pt(x)+gt(x)

where the second equality utilize the evaluation of pt(x) in Line 6 (i.e.  pt(x) = wt(x)/Wt).
Note that if the generators Gt are GANs  then the discriminator of each Gt can be reused as Dt here.
Reusing Dt introduces no additional computation. In contrast  AdaGAN [33] always has to train an
additional discriminator Dt in each round using the mixture of generators of all past rounds.
Working with empirical dataset.
In practice  the true data distribution P is often unknown when
i=1 is given. Instead  the empirical dataset is considered as n i.i.d.
an empirical dataset X = {xi}n
samples drawn from P . According to the Glivenko-Cantelli theorem [36]  the uniform distribution
over n i.i.d. samples from P will converge to P as n approaches to inﬁnity. Therefore  provided the
empirical dataset  we do not need to know the probability density p(x) of P   as every sample xi 2 X
is considered to have a ﬁnite and uniform probability measure. An empirical version of Algorithm 1
and more explanation are presented in the supplementary document (Algorithm 2 and Appendix B).

4 Theoretical Analysis
We now provide a theoretical understanding of our algorithm  showing that the pointwise data
coverage (1) is indeed obtained. Our analysis also sheds some light on how to choose the parameters
of Algorithm 1.

4.1 Preliminaries
We ﬁrst clarify a few notational conventions and introduce two new theoretical notions for our
subsequent analysis. Our analysis is in continuous setting; results on discrete datasets follow directly.
Notation. Formally  we consider a d-dimensional measurable space (X  B(X ))  where X is the
d-dimensional data space  and B(X ) is the Borel -algebra over X to enable probability measure. We
use a capital letter (e.g.  P ) to denote a probability measure on this space. When there is no ambiguity 
we also refer them as probability distributions (or distributions). For any subset S2B (X )  the
probability of S under P is P (S) := Prx⇠P [x 2S ]. We use G to denote a generator. When there is

5

no ambiguity  G also denotes the distribution of its generated samples. All distributions are assumed
absolutely continuous. Their probability density functions (i.e.  the derivative with respect to the
Lebesgue measure) are referred by their corresponding lowercase letters (e.g.  p(·)  q(·)  and g(·)).
Moreover  we use [n] to denote the set {1  2  ...  n}  N>0 for the set of all positive integers  and 1(E)
for the indicator function whose value is 1 if the event E happens  and 0 otherwise.
f-divergence. Widely used in objective functions of training generative models  f-divergence is a
statistical distance between two distributions. Let P and Q be two distributions over X . Provided a
convex function f on (0 1) such that f (1) = 0  f-divergence of Q from P is deﬁned as Df (Q k
f⇣ q(x)
p(x)⌘ p(x)dx. Various choices of f lead to some commonly used f-divergence metrics
P ) :=RX
such as total variation distance DTV  Kullback-Leibler divergence DKL  Hellinger distance DH  and
Jensen-Shannon divergence DJS [35  37]. Among them  total variation distance is upper bounded
by many other f-divergences. For instance  DTV(Q k P ) is upper bounded byq 1
2 DKL(Q k P ) 
p2DH(Q k P )  andp2DJS(Q k P )  respectively. Thus  if two distributions are close under those
f-divergence measures  so are they under total variation distance. For this reason  our theoretical
analysis is based on the total variation distance.
-cover and (  )-cover. We introduce two new notions for analyzing our algorithm. The ﬁrst is
the notion of -cover. Given a data distribution P over X and a value  2 (0  1]  if a generator G
satisﬁes g(x)   · p(x) at a data point x 2X   we say that x is -covered by G under distribution P .
Using this notion  the pointwise mode coverage (1) states that x is -covered by G under distribution
P for all x 2X . We also extend this notion to a measurable subset S2B (X ): we say that S is
-covered by G under distribution P if G(S)   · P (S) is satisﬁed.
Next  consider another distribution Q over X . We say that G can (  )-cover (P  Q)  if the following
condition holds:
(6)

[x is -covered by G under distributionP ]  .

Pr
x⇠Q

For instance  using this notation  Equation (3) in our game-theoretic analysis states that G can
(0.25  0.4)-cover (P  Q).

4.2 Guarantee of Pointwise Data Coverage
In each iteration of Algorithm 1  we expect the generator Gt to approximate the given data distribution
Pt sufﬁciently well. We now formalize this expectation and understand its implication. Our intuition
is that by ﬁnding a property similar to (3)  we should be able to establish a pointwise coverage lower
bound in a way similar to our analysis in Sec. 1.1. Such a property is given by the following lemma
(and proved in Appendix E.1).
Lemma 1. Consider two distributions  P and Q  over the data space X   and a generator G producing
samples in X . For any    2 (0  1]  if DT V (G k Q)    then G can (  1  2  )-cover (P  Q).
Intuitively  when G and Q are identiﬁed   is set. If  is reduced  then more data points in X can
be -covered by G under P . Thus  the probability deﬁned in (6) becomes larger  as reﬂected by the
increasing 1  2  . On the other hand  consider a ﬁxed . As the discrepancy between G and Q
becomes larger   increases. Then  sampling an x according to Q will have a smaller chance to land
at a point that is -covered by G under P   as reﬂected by the decreasing 1  2  .
Next  we consider Algorithm 1 and identify a sufﬁcient condition under which the output mixture of
generators G⇤ covers every data point with a lower-bounded guarantee (i.e.  our goal (1)). Simply
speaking  this sufﬁcient condition is as follows: in each round t  the generator Gt is trained such that
given an x drawn from distribution Pt  the probability of x being -covered by Gt under P is also
lower bounded. A formal statement is given in the next lemma (proved in Appendix E.2).
Lemma 2. Recall that T 2 N>0 and  2 (0  1) are the input parameters of Algorithm 1. For any
" 2 [0  1) and any measurable subset S2B (X ) whose probability measure satisﬁes P (S)  1/2⌘T
with some ⌘ 2 (0  1)  if in every round t 2 [T ]  Gt can (  1  ")-cover (P  Pt)  then the resulting
mixture of generators G⇤ can (1  "/ln 2  ⌘)-cover S under distribution P .
This lemma is about lower-bounded coverage of a measurable subset S  not a point x 2X . At ﬁrst
sight  it is not of the exact form in (1) (i.e.  pointwise -coverage). This is because formally speaking
it makes no sense to talk about covering probability at a single point (whose measure is zero). But as

6

T approaches to 1  S that satisﬁes P (S)  1/2⌘T can also approach to a point (and ⌘ approaches
to zero). Thus  Lemma 2 provides a condition for pointwise lower-bounded coverage in the limiting
sense. In practice  the provided dataset is always discrete  and the probability measure at each discrete
data point is ﬁnite. Then  Lemma 2 is indeed a sufﬁcient condition for pointwise lower-bounded
coverage.
From Lemma 1  we see that the condition posed by Lemma 2 is indeed satisﬁed by our algorithm 
and combing both lemmas yields our ﬁnal theorem (proved in Appendix E.3).
Theorem 1. Recall that T 2 N>0 and  2 (0  1) are the input parameters of Algorithm 1. For
any measurable subset S2B (X ) whose probability measure satisﬁes P (S)  1/2⌘T with some
⌘ 2 (0  1)  if in every round t 2 [T ]  DTV(Gt k Pt)    then the resulting mixture of generators G⇤
can (1  ( + 2)/ ln 2  ⌘)-cover S under distribution P .
In practice  existing generative models (such as GANs) can approximate Pt sufﬁciently well  and
thus DTV(Gt k Pt)   is always satisﬁed for some . According to Theorem 1  a pointwise
lower-bounded coverage can be obtained by our Algorithm 1. If we choose to use a more expressive
generative model (e.g.  a GAN with a stronger network architecture)  then Gt can better ﬁt Pt in each
round  yielding a smaller  used in Theorem 1. Consequently  the pointwise lower bound of the data
coverage becomes larger  and effectively the coefﬁcient in (1) becomes larger.

Insights from the Analysis

4.3
  ⌘    and T in Theorem 1.
In Theorem 1   depends on the expressive power of the generators
being used. It is therefore determined once the generator class G is chosen. But ⌘ can be directly set
by the user and a smaller ⌘ demands a larger T to ensure P (S)  1/2⌘T is satisﬁed. Once  and ⌘ is
determined  we can choose the best  by maximizing the coverage bound (i.e.  (1(+2)/ ln 2⌘))
in Theorem 1. For example  if   0.1 ⌘  0.01  then  ⇡ 1/4 would optimize the coverage bound
(see Appendix E.4 for more details)  and in this case the coefﬁcient in (1) is at least 1/30.
Theorem 1 also sets the tone for the training cost. As explained in Appendix E.4  given a training
dataset of size n  the size of the generator mixture  T   needs to be at most O(log n). This theoretical
bound is consistent with our experimental results presented in Sec. 5. In practice  only a small number
of generators are needed.
Estimated density function gt. The analysis in Sec. 4.2 assumes that the generated probability
density gt of the generator Gt in each round is known  while in practice we have to estimate gt by
training a discriminator Dt (recall Section 3). Fortunately  only mild assumptions in terms of the
quality of Dt are needed to retain the pointwise lower-bounded coverage. Roughly speaking  Dt
needs to meet two conditions: 1) In each round t  only a fraction of the covered data points (i.e.  those
with gt(x)   · p(x)) is falsely classiﬁed by Dt and doubled their weights. 2) In each round t  if the
weight of a data point x is not doubled based on the estimation of Dt(x)  then there is a good chance
that x is truly covered by Gt (i.e.  gt(x)   · p(x)). A detailed and formal discussion is presented in
Appendix E.5. In short  our estimation of gt would not deteriorate the efﬁcacy of the algorithm  as
also conﬁrmed in our experiments.
Generalization. An intriguing question for all generative models is their generalization perfor-
mance: how well can a generator trained on an empirical distribution (with a ﬁnite number of
data samples) generate samples that follow the true data distribution? While the generalization
performance has been long studied for supervised classiﬁcation  generalization of generative models
remains a widely open theoretical question. We propose a notion of generalization for our method 
and provide a preliminary theoretical analysis. All the details are presented in Appendix E.6.

5 Experiments
We now present our major experimental results  while referring to Appendix F for network details
and more results. We show that our mixture of generators is able to cover all the modes in various
synthetic and real datasets  while existing methods always have some modes missed.
Previous works on generative models used the Inception Score [1] or the Fréchet Inception Dis-
tance [18] as their evaluation metric. But we do not use them  because they are both global measures 
not reﬂecting mode coverage in local regions [38]. Moreover  these metrics are designed to measure
the quality of generated images  which is orthogonal to our goal. For example  one can always use a
more expressive GAN in each iteration of our algorithm to obtain better image quality and thus better
inception scores.

7

(a)(b)(c)(d)(e)(f)Figure2:Generativemodelsonsyntheticdataset.(a)Thedatasetconsistsoftwomodes:onemajormodeasanexpandingsinecurve(y=xsin4x⇡)andaminormodeasaGaussianlocatedat(10 0)(highlightedintherebbox).(b-f)Weshowcolor-codeddistributionsofgeneratedsamplesfrom(b)EM (c)GAN (d)AdaGAN (e)VAE and(f)ourmethod(i.e. amixtureofGANs).Onlyourmethodisabletocoverthesecondmode(highlightedinthegreenbox;zoomintoview).SincethephenomenonofmissingmodesisparticularlyprominentinGANs ourexperimentsemphasizeonthemodecoverageperformanceofGANsandcompareourmethod(usingamixtureofGANs)withDCGAN[39] MGAN[27] andAdaGAN.ThelattertwoalsousemultipleGANstoimprovemodecoverage althoughtheydonotaimforthesamemodecoveragenotionasours.Overview.Weﬁrstoutlineallourexperiments includingthosepresentedinAppendixF.i)Wecompareourmethodwithanumberofclassicgenerativemodelsonasyntheticdataset.ii)InAppendixF.3 wealsocompareourmethodwithAdaGAN[33]onothersyntheticdatasetsaswellasstackedMNISTdataset becausebothareboostingalgorithmsaimingatimprovingmodecoverage.iii)WefurthercompareourmethodwithasinglelargeDCGAN AdaGAN andMGANontheFashion-MNISTdataset[40]mixedwithaverysmallportionofMNISTdataset[41].Variousgenerativemodelsonsyntheticdataset.AsweshowinAppendixA manygenerativemodels suchasexpectation-maximization(EM)methods VAEs andGANs allrelyonaglobalstatisticaldistanceintheirtraining.Wethereforetesttheirmodecoverageandcomparewithours.WeconstructonR2asyntheticdatasetwithtwomodes.Theﬁrstmodeconsistsofdatapointswhosex-coordinateisuniformlysampledbyxi⇠[10 10]andthey-coordinateisyi=xisin4xi⇡.ThesecondmodehasdatapointsformingaGaussianat(0 10).Thetotalnumberofdatapointsintheﬁrstmodeis400⇥ofthesecond.AsshowninFigure2 generativemodelsincludeEM GAN VAE andAdaGAN[33]allfailtocoverthesecondmode.Ourmethod incontrast capturesbothmodes.WerunKDEtoestimatethelikelihoodofourgeneratedsamplesonoursyntheticdataexperiments(usingKDEbandwidth=0.1).WecomputeL=1/NPiPmodel(xi) wherexiisasampleintheminormode.Fortheminormode ourmethodhasameanloglikelihoodof-1.28 whileAdaGANhasonly-967.64(almostnosamplesfromAdaGAN).“1”sFrequencyAvgProb.DCGAN130.14⇥1040.49MGANcollapsed--AdaGAN600.67⇥1040.45Ourmethod2893.2⇥1040.68Table1:Ratiosofgeneratedimagesclassiﬁedas“1”.Wegenerate9⇥105imagesfromeachmethod.Thesecondcolumnindicatesthenumbersofsam-plesbeingclassiﬁedas“1” andthethirdcolumnindicatestheratio.Inthefourthcolumn weaveragethepredictionprobabilitiesoverallgeneratedimagesthatareclassiﬁedas“1”.Fashion-MNISTandpartialMNIST.OurnextexperimentistochallengedifferentGANmodelswitharealdatasetthathasseparatedandunbalancedmodes.Thisdatasetcon-sistsoftheentiretrainingdatasetofFashion-MNIST(with60kimages)mixedwithran-domlysampled100MNISTimageslabeledas“1”.Thesizeofgeneratormixtureisal-wayssettobe30forAdaGAN MGANandourmethod andallgeneratorssharethesamenetworkstructure.Additionally whencom-paringwithasingleDCGAN weensurethattheDCGAN’stotalnumberofparametersiscomparabletothetotalnumberofparametersofthe30generatorsinAdaGAN MGAN andours.Toevaluatetheresults wetrainan11-classclassiﬁertodistinguishthe10classesinFashion-MNISTandoneclassinMNIST(i.e. “1”).First wecheckhowmanysamplesfromeachmethodareclassiﬁedas“1”.ThetestsetupandresultsareshowninTable1anditscaption.Theresultssuggestthatourmethodcangeneratemore“1”sampleswithhigherpredictionconﬁdence.NotethatMGANhasastrongmodecollapseandfailstoproduce“1”samples.WhileDCGANandAdaGANgeneratesomesamplesthatareclassiﬁedas“1” inspectingthegeneratedimagesrevealsthatthosesamplesareallvisuallyfarfrom“1”s butincorrectlyclassiﬁedbythepre-trainedclassiﬁer(seeFigure3).Incontrast ourmethodisabletogeneratesamplescloseto“1”.Wealsonotethatourmethodcanproducehigher-qualityimagesiftheunderlyinggenerativemodelsineachroundbecomestronger.8AdaGANMGANDCGANOur ApproachFigure3:Mostconﬁdent“1”samples.Hereweshowsamplesthataregeneratedbyeachtestedmethodsandalsoclassiﬁedbythepre-trainedclassiﬁermostconﬁdentlyas“1”images(i.e. top10intermsoftheclassiﬁedprobability).Samplesofourmethodarevisuallymuchcloserto“1”.0.60.50.40.30.20.10.001234567890.60.50.40.30.20.10.001234567890.60.50.40.30.20.10.001234567890.60.50.40.30.20.10.00123456789AdaGANMGANDCGANOursClassFrequencyFrequencyFrequencyFrequencyClassClassClassFigure5:Distributionofgeneratedsamples.Trainingsamplesaredrawnuniformlyfromeachclass.ButgeneratedsamplesbyAdaGANandMGANareconsiderablynonuniform whilethosefromDCGANandourmethodaremoreuniform.Thisexperimentsuggeststhattheconventionalheuristicofreducingastatisticaldistancemightnotmerititsuseintraininggenerativemodels.0.0050.001601020304050RatioIterationsFigure4:Weightratioof“1”s.Wecalculatetheratioofthetotalweightsoftrainingimageslabeledby“1”tothetotalweightsofalltrainingimagesineachround andplotherehowtheratiochangeswithrespecttotheiterationsinouralgorithm.Anotherremarkablefeatureisobservedinouralgorithm.Ineachroundofourtrainingalgorithm wecalculatethetotalweight¯wtofprovidedtrain-ingsamplesclassiﬁedas“1”aswellasthetotalweightWtofalltrainingsamples.Whenplottingtheratio¯wt/Wtchangingwithrespecttothenum-berofrounds(Figure4) interestingly wefoundthatthisratiohasamaximumvalueataround0.005inthisexample.Weconjecturethatinthetrainingdatasetiftheratioof“1”imagesamongalltrainingimagesisaround1/200 thenasinglegeneratormaylearnandgenerate“1”images(theminoritymode).Toverifythisconjecture wetrainedaGAN(withthesamenetworkstructure)onanothertrainingdatasetwith60ktrainingimagesfromFashion-MNISTmixedwith300MNIST“1”images.Wethenusethetrainedgeneratortosample100kimages.Asaresult Inafractionof4.2⇥104 thoseimagesareclassiﬁedas“1”.Figure8inAppendixFshowssomeofthoseimages.Thisresultconﬁrmsourconjectureandsuggeststhat¯wt/Wtmaybeusedasameasureofmodebiasinadataset.Lastly inFigure5 weshowthegenerateddistributionoverthe10Fashion-MNISTclassesfromeachtestedmethod.Weneglecttheclass“1” asMGANfailstogeneratethem.ThegeneratedsamplesofAdaGANandMGANishighlynonuniform thoughinthetrainingdataset the10classesofimagesareuniformlydistributed.OurmethodandDCGANproducemoreuniformsamples.Thissuggeststhatalthoughothergenerativemodels(suchasAdaGANandMGAN)aimtoreduceaglobalstatisticaldistance thegeneratedsamplesmaynoteasilymatchtheempiricaldistribution—inthiscase auniformdistribution.Ourmethod whilenotaimingforreducingthestatisticaldistanceintheﬁrstplace matchesthetargetempiricaldistributionplausibly asabyproduct.6ConclusionWehavepresentedanalgorithmthatiterativelytrainsamixtureofgenerators drivenbyanexplicitnotionofcompletemodecoverage.Withthisnotionfordesigninggenerativemodels ourworkposesanalternativegoal onethatdiffersfromtheconventionaltrainingphilosophy:insteadofreducingaglobalstatisticaldistancebetweenthetargetdistributionandgenerateddistribution oneonlyneedstomakethedistancemildlysmallbutnothavetoreduceittowardaperfectzero andourmethodisabletoboostthegenerativemodelwiththeoreticallyguaranteedmodecoverage.Acknowledgments.ThisworkwassupportedinpartbytheNationalScienceFoundation(CAREER-1453101 1816041 1910839 1703925 1421161 1714818 1617955 1740833) SimonsFoundation(#491119toAlexandrAndoni) GoogleResearchAward aGooglePhDFellowship aSnapResearchFellowship aColumbiaSEASCKGSBFellowship andSoftBankGroup.9References
[1] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems 
pages 2234–2242  2016.

[2] Luke Metz  Ben Poole  David Pfau  and Jascha Sohl-Dickstein. Unrolled generative adversarial

networks. arXiv preprint arXiv:1611.02163  2016.

[3] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research  15(1):1929–1958  2014.

[4] Chang Xiao  Peilin Zhong  and Changxi Zheng. Bourgan: Generative networks with metric
embeddings. In Advances in Neural Information Processing Systems  pages 2269–2280  2018.

[5] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems  pages 2172–2180  2016.

[6] Samuel R Bowman  Luke Vilnis  Oriol Vinyals  Andrew Dai  Rafal Jozefowicz  and Samy
Bengio. Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL
Conference on Computational Natural Language Learning  pages 10–21  2016.

[7] J v Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen  100(1):295–320 

1928.

[8] Ding-Zhu Du and Panos M Pardalos. Minimax and applications  volume 4. Springer Science &

Business Media  2013.

[9] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[10] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein gan. arXiv preprint

arXiv:1701.07875  2017.

[11] David H Wolpert  William G Macready  et al. No free lunch theorems for optimization. IEEE

transactions on evolutionary computation  1(1):67–82  1997.

[12] Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a

meta-algorithm and applications. Theory of Computing  8(1):121–164  2012.

[13] Tong Che  Yanran Li  Athul Paul Jacob  Yoshua Bengio  and Wenjie Li. Mode regularized

generative adversarial networks. arXiv preprint arXiv:1612.02136  2016.

[14] Junbo Zhao  Michael Mathieu  and Yann LeCun. Energy-based generative adversarial network.

arXiv preprint arXiv:1609.03126  2016.

[15] Xudong Mao  Qing Li  Haoran Xie  Raymond YK Lau  Zhen Wang  and Stephen Paul Smolley.
Least squares generative adversarial networks. In 2017 IEEE International Conference on
Computer Vision (ICCV)  pages 2813–2821. IEEE  2017.

[16] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems 
pages 5769–5779  2017.

[17] Yunus Saatci and Andrew G Wilson. Bayesian gan.

processing systems  pages 3622–3631  2017.

In Advances in neural information

[18] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems  pages 6626–6637  2017.

10

[19] Andrew Brock  Jeff Donahue  and Karen Simonyan. Large scale gan training for high ﬁdelity

natural image synthesis. arXiv preprint arXiv:1809.11096  2018.

[20] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin Ar-
jovsky  and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704 
2016.

[21] Zinan Lin  Ashish Khetan  Giulia Fanti  and Sewoong Oh. Pacgan: The power of two samples

in generative adversarial networks. arXiv preprint arXiv:1712.04086  2017.

[22] Akash Srivastava  Lazar Valkoz  Chris Russell  Michael U Gutmann  and Charles Sutton.
Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in
Neural Information Processing Systems  pages 3310–3320  2017.

[23] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive growing of gans for

improved quality  stability  and variation. arXiv preprint arXiv:1710.10196  2017.

[24] Chongxuan Li  Max Welling  Jun Zhu  and Bo Zhang. Graphical generative adversarial networks.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors 
Advances in Neural Information Processing Systems 31  pages 6072–6083. Curran Associates 
Inc.  2018.

[25] Francesco Locatello  Damien Vincent  Ilya Tolstikhin  Gunnar Rätsch  Sylvain Gelly  and Bern-
hard Schölkopf. Clustering meets implicit generative models. arXiv preprint arXiv:1804.11130 
2018.

[26] Sanjeev Arora  Rong Ge  Yingyu Liang  Tengyu Ma  and Yi Zhang. Generalization and

equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573  2017.

[27] Quan Hoang  Tu Dinh Nguyen  Trung Le  and Dinh Phung. MGAN: Training generative adver-
sarial nets with multiple generators. In International Conference on Learning Representations 
2018.

[28] David Keetae Park  Seungjoo Yoo  Hyojin Bahng  Jaegul Choo  and Noseong Park. Megan:
Mixture of experts of generative adversarial networks for multimodal image generation. arXiv
preprint arXiv:1805.02481  2018.

[29] Paulina Grnarova  Kﬁr Y Levy  Aurelien Lucchi  Thomas Hofmann  and Andreas Krause. An
online learning approach to generative adversarial networks. arXiv preprint arXiv:1706.03269 
2017.

[30] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R in

Optimization  2(3-4):157–325  2016.

[31] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of computer and system sciences  55(1):119–139  1997.
[32] Yaxing Wang  Lichao Zhang  and Joost van de Weijer. Ensembles of generative adversarial

networks. arXiv preprint arXiv:1612.00991  2016.

[33] Ilya O Tolstikhin  Sylvain Gelly  Olivier Bousquet  Carl-Johann Simon-Gabriel  and Bernhard
Schölkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing
Systems  pages 5430–5439  2017.

[34] Aditya Grover and Stefano Ermon. Boosted generative models.

Conference on Artiﬁcial Intelligence  2018.

In Thirty-Second AAAI

[35] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  pages 271–279  2016.

[36] Francesco Paolo Cantelli. Sulla determinazione empirica delle leggi di probabilita. Giorn. Ist.

Ital. Attuari  4(421-424)  1933.

[37] Shun-ichi Amari. Information geometry and its applications. Springer  2016.

11

[38] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973 

2018.

[39] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.
[40] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747  2017.

[41] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[42] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[43] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

12

,Peilin Zhong
Yuchen Mo
Chang Xiao
Pengyu Chen
Changxi Zheng