2019,On the Power and Limitations of Random Features for Understanding Neural Networks,Recently  a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization  gradient-based methods will implicitly leave some components of the network relatively unchanged  so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact  fixing these \emph{explicitly} leads to the well-known approach of learning with random features (e.g. \citep{rahimi2008random rahimi2009weighted}). In other words  these techniques imply that we can successfully learn with neural networks  whenever we can successfully learn with random features. In this paper  we formalize the link between existing results and random features  and argue that despite the impressive positive results  random feature approaches are also inherently limited in what they can explain. In particular  we prove that random features cannot be used to learn \emph{even a single ReLU neuron} (over standard Gaussian inputs in $\reals^d$ and $\text{poly}(d)$ weights)  unless the network size (or magnitude of its weights) is exponentially large in $d$. Since a single neuron \emph{is} known to be learnable with gradient-based methods  we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof  using a random features technique  that one-hidden-layer neural networks can learn low-degree polynomials.,On the Power and Limitations of Random Features

for Understanding Neural Networks

Gilad Yehudai

Ohad Shamir

Weizmann Institute of Science

{gilad.yehudai ohad.shamir}@weizmann.ac.il

Abstract

Recently  a spate of papers have provided positive theoretical results for training
over-parameterized neural networks (where the network size is larger than what
is needed to achieve low error). The key insight is that with sufﬁcient over-
parameterization  gradient-based methods will implicitly leave some components
of the network relatively unchanged  so the optimization dynamics will behave as if
those components are essentially ﬁxed at their initial random values. In fact  ﬁxing
these explicitly leads to the well-known approach of learning with random features
(e.g. [27  29]). In other words  these techniques imply that we can successfully
learn with neural networks  whenever we can successfully learn with random
features. In this paper  we formalize the link between existing results and random
features  and argue that despite the impressive positive results  random feature
approaches are also inherently limited in what they can explain. In particular  we
prove that random features cannot be used to learn even a single ReLU neuron
(over standard Gaussian inputs in Rd and poly(d) weights)  unless the network size
(or magnitude of its weights) is exponentially large in d. Since a single neuron
is known to be learnable with gradient-based methods  we conclude that we are
still far from a satisfying general explanation for the empirical success of neural
networks. For completeness we also provide a simple self-contained proof  using
a random features technique  that one-hidden-layer neural networks can learn
low-degree polynomials.

1

Introduction

Deep learning  in the form of artiﬁcial neural networks  has seen a dramatic resurgence in popularity
in recent years. This is mainly due to impressive performance gains on various difﬁcult learning
problems  in ﬁelds such as computer vision  natural language processing and many others. Despite the
practical success of neural networks  our theoretical understanding of them is still very incomplete.
A key aspect of modern networks is that they tend to be very large  usually with many more parameters
than the size of the training data: In fact  so many that in principle  they can simply memorize all
the training examples (as shown in the inﬂuential work of Zhang et al. [40]). The fact that such
huge  over-parameterized networks are still able to learn and generalize is one of the big mysteries
concerning deep learning. A current leading hypothesis is that over-parameterization makes the
optimization landscape more benign  and encourages standard gradient-based training methods to ﬁnd
weight conﬁgurations that ﬁt the training data as well as generalize (even though there might be many
other conﬁgurations which ﬁt the training data without any generalization). However  pinpointing the
exact mechanism by which over-parameterization helps is still an open problem.
Recently  a spate of papers (such as [4  11  14  2  23  15  9  3  1]) provided positive results for training
and learning with over-parameterized neural networks. Although they differ in details  they are
all based on the following striking observation: When the networks are sufﬁciently large  standard

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

gradient-based methods change certain components of the network (such as the weights of a certain
layer) very slowly  so that if we run these methods for a bounded number of iterations  they might as
well be ﬁxed. To give a concrete example  consider one-hidden-layer neural networks  which can be
written as a linear combination of r neurons

N (x) =

uiσ((cid:104)wi  x(cid:105) + bi)  

(1)

r(cid:88)

i=1

using weights {ui  wi  bi}r
i=1 and an activation function σ. When r is sufﬁciently large  and with
standard random initializations  it can be shown that gradient descent will leave the weights wi  bi
in the ﬁrst layer nearly unchanged (at least initially). As a result  the dynamics of gradient descent
will resemble those where {wi  bi} are ﬁxed at random initial values – namely  where we learn a
linear predictor (parameterized by u1  . . .   ur) over a set of r random features of the form x (cid:55)→
σ((cid:104)wi  x(cid:105) + bi) (for some random choice of wi  bi). For such linear predictors  it is not difﬁcult to
show that they will converge quickly to an optimal predictor (over the span of the random features).
This leads to learning guarantees with respect to hypothesis classes which can be captured well
by such random features: For example  most papers focus (explicitly or implicitly) on multivariate
polynomials with certain constraints on their degree or the magnitude of their coefﬁcients. We discuss
these results in more detail (and demonstrate their close connection to random features) in Section 2.
Taken together  these results are a signiﬁcant and insightful advance in our understanding of neural
networks: They rigorously establish that sufﬁcient over-parameterization allows us to learn compli-
cated functions  while solving a non-convex optimization problem. However  it is important to realize
that this approach can only explain learnability of hypothesis classes which can already be learned
using random features. Considering the one-hidden-layer example above  this corresponds to learning
linear predictors over a ﬁxed representation (chosen obliviously and randomly at initialization). Thus 
it does not capture any element of representation learning  which appears to lend much of the power
of modern neural networks.
In this paper we show that there are inherent limitations on what predictors can be captured with
random features  and as a result  on what can be provably learned with neural networks using the
techniques described earlier. We consider features of the form fi : Rd → R which are chosen
(randomly or deterministically) and then ﬁxed. The fi’s can be arbitrary functions  including
multilayered neural networks  as long as their norm (suitably deﬁned) is not exponential in the input
i=1 uifi(x) we cannot efﬁciently approximate even a
single ReLU neuron:
Theorem 1.1 (Informal version of Theorem 4.8). Let F be a family of function on Rd  where for every
f ∈ F  Ex∼N (0 I)[f (x)2] is less than exponential in the dimension d  and let D be a distribution
over tuples (f1  . . .   fr) of functions from F. Then there exist a weight vector w∗ ∈ Rd (cid:107)w∗(cid:107) = d2
and bias term b∗ ∈ R such that w.h.p over the choice of functions from F  if:

dimension. We show that using N (x) =(cid:80)r

(cid:104)

(N (x) − [(cid:104)w∗  x(cid:105) + b∗]+)2(cid:105) ≤ 1

50

Ex∼N (0 I)

(where [z]+ = max{0  z} is the ReLU function and x has a standard Gaussian distribution) then

r · max

|ui| ≥ exp(Ω(d)).

i

In other words  either the number of neurons r or the magnitude of the weights (or both) must be
exponential in the dimension d  which generally implies exponential time to learn the target function
(see Remark 4.5). Moreover  if the features can be written as functions operating on a random linear
transformation over the data  then the same result holds for any choice of w∗. In more details:
Theorem 1.2 (Informal version of Theorem 4.2). Let F be a family of functions where each f ∈ F
can be written as x (cid:55)→ f (W x) where W is a random matrix whose rows are sampled uniformly at
random from the unit sphere. Then the result of theorem 3.1 holds for any w∗ ∈ Rd (cid:107)w∗(cid:107) = d2.
Both theorems apply for a large family of functions  particular examples include two layered neural
networks (Theorm 1.2) and "linearized" neural tangent kernel (Theorem 1.1  also see Jacot et al.
[20]). These results imply that the random features approach cannot fully explain polynomial-time
learnability of neural networks  even with respect to data generated by an extremely simple neural
network  composed of a single neuron. This is despite the fact that single ReLU neurons are easily

2

learnable with gradient-based methods (e.g.  [26]  [34]  also see Section 4 for further details). The
point we want to make here is that the random feature approach  as a theory for explaining the success
of neural networks  cannot explain even the fact that single neurons are learnable.
For completeness we also provide a simple  self-contained analysis  showing how over-parameterized 
one-hidden-layer networks can provably learn polynomials with bounded degrees and coefﬁcients 
using standard stochastic gradient descent with standard initialization.
We emphasize that there is no contradiction between our positive and negative results: In the positive
result on learning polynomials  the required size of the network is exponential in the degree of the
polynomial  and low-degree polynomials cannot express even a single ReLU neuron if its weights are
large enough.
Overall  we argue that although the random feature approach captures important aspects of training
neural networks  it is by no means the whole story  and we are still quite far from a satisfying general
explanation for the empirical success of neural networks.

Related Work

machine in the 1950’s). These involve learning predictors of the form x (cid:55)→(cid:80)r

The recent literature on the theory of deep learning is too large to be thoroughly described here.
Instead  we survey here some of the works most directly relevant to the themes of our paper. In
Section 2  we provide a more technical explanation on the connection of recent results to random
features.
The Power of Over-Parameterization. The fact that over-parameterized networks are easier to train
was empirically observed  for example  in [25]  and was used in several contexts to show positive
results for learning and training neural networks. For example  it is known that adding more neurons
makes the optimization landscape more benign (e.g.  [30  36  37  31  10  35])  or allows them to learn
in various settings (e.g.  besides the papers mentioned in the introduction  [8  24  7  39  13]).
Random Features. The technique of random features was proposed and formally analyzed in
[27  28  29]  originally as a computationally-efﬁcient alternative to kernel methods (although as
a heuristic  it can be traced back to the “random connections” feature of Rosenblatt’s Perceptron
i=1 uiψi(x)  where
ψi are random non-linear functions. The training involves only tuning of the ui weights. Thus  the
learning problem is as computationally easy as training linear predictors  but with the advantage
that the resulting predictor is non-linear  and in fact  if r is large enough  can capture arbitrarily
complex functions. The power of random features to express certain classes of functions has been
studied in past years (for example [5  28  21  38]). However  in our paper we also consider negative
rather than positive results for such features. [5] also discusses the limitation of approximating
functions with a bounded number of such features  but in a different setting than ours (worst-case
approximation of a large function class using a ﬁxed set of features  rather than inapproximability of
a ﬁxed target function  and not in the context of single neurons). Less directly related  [41] studied
learning neural networks using kernel methods  which can be seen as learning a linear predictor over
a ﬁxed non-linear mapping. However  the algorithm is not based on training neural networks with
standard gradient-based methods. In a very recent work (and following the initial dissemination
of our paper)  Ghorbani et al. [17] studied the representation capabilities of random features  and
showed that in high dimensions random features are not good at ﬁtting high degree polynomials.

Notation

Denote by U(cid:0)[a  b]d(cid:1) the d-dimensional uniform distribution over the rectangle [a  b]d  and by

N (0  Σ) the multivariate Gaussian distribution with covariance matrix Σ. For T ∈ N let [T ] =
{1  2  . . .   T}  and for a vector w ∈ Rd we denote by (cid:107)w(cid:107) the L2 norm. We denote the ReLU
function by [x]+ = max{0  x}.

2 Analysis of Neural Networks as Random Features

In many previous works  a key element in the analysis of neural networks is to build a reduction from
neural networks to random features. In this reduction  usually by choosing appropriate learning rate
and initialization  close to the initialization gradient descent is optimizing neural network similarly to

3

the way it would optimize random features. Thus  it is enough to analyze the optimization process of
random features  and deduce that if random features can achieve good performance  the same holds
for neural networks. Here we survey some of these works and how they can actually be viewed as
random features.

2.1 Optimization with Coupling  Fixing the Output Layer

One approach is to ﬁx the output layer and do optimization only on the inner layers. Most works
that use this method (e.g. [23]  [15]  [3]  [9]  [2]  [1]) also use the method of "coupling" and the
popular ReLU activation. This method uses the following observation: a ReLU neuron can be viewed
as a linear predictor multiplied by a threshold function  that is: [(cid:104)w  x(cid:105)]+ = (cid:104)w  x(cid:105)1(cid:104)w x(cid:105)≥0. The
coupling method informally states that after doing gradient descent with appropriate learning rate
and a limited number of iterations  the amount of neurons that change the sign of (cid:104)w  x(cid:105) (for x in
the data) is small. Thus  it is enough to analyze a linear network over random features of the form:
x (cid:55)→ (cid:104)w  x(cid:105)1(cid:104)w(0) x(cid:105)≥0 where w(0) are randomly chosen.
For example  a one-hidden-layer neural network where the activation σ is the ReLU function can be
written as

i σ((cid:104)w(t)
u(t)

i

  x(cid:105)) =

i (cid:104)w(t)
u(t)

i

  x(cid:105)1(cid:104)w(t)

i

 x(cid:105)≥0.

Using the coupling method  after doing gradient descent  the amount of neurons that change sign  i.e.
the sign of (cid:104)w(t)
  x(cid:105) changes  is small. As a result  using the homogeneity of the ReLU function  the
following network can actually be analyzed:

i

i (cid:104)w(t)
u(t)

i

  x(cid:105)1(cid:104)w(0)

i

 x(cid:105)≥0 =

(cid:104)u(t)

i

· w(t)

i

  x(cid:105)1(cid:104)w(0)

i

 x(cid:105)≥0 

i

i

where w(0)
are randomly chosen. This is just analyzing a linear predictor with random features of
the form x (cid:55)→ xj 1(cid:104)w(0)
 x(cid:105)≥0. Note that the homogeneity of the ReLU function is used in order to
show that ﬁxing the output layer does not change the network’s expressiveness. This is not true in
terms of optimization  as optimizing both the inner layers and the output layers may help the network
converge faster  and to ﬁnd a predictor which has better generalization properties. Thus  the challenge
in this approach is to ﬁnd functions or distributions that can be approximated with this kind of random
features network  using a polynomial number of features.

2.2 Optimization on all the Layers

A second approach in the literature (e.g. Andoni et al. [4]  Daniely et al. [12]  Du et al. [14]) is to
perform optimization on all the layers of the network  choose a "good" learning rate and bound the
number of iterations such that the inner layers stay close to their initialization. For example  in the
setting of a one-hidden-layer network  for every  > 0  a learning rate η and number of iterations
T are chosen  such that after running gradient descent with these parameters  there is an iteration
1 ≤ t ≤ T such that:

r(cid:88)

i=1

r(cid:88)

i=1

r(cid:88)

i=1

r(cid:88)

i=1

Hence  it is enough to analyze a linear predictor over a set of random features:

(cid:13)(cid:13)(cid:13)U (t)σ(W (t)x) − U (t)σ(W (0)x)
(cid:13)(cid:13)(cid:13) ≤ .
  x(cid:105)(cid:17)
(cid:16)(cid:104)w(0)
(cid:16)

r(cid:88)

W (0)x

(cid:17)

=

u(t)
i σ

i

U (t)σ

 

where σ is not necessarily the ReLU function. Again  the difﬁculty here is ﬁnding the functions that
can be approximated in this form  where r (the amount of neurons) is only polynomial in the relevant
parameters.

i=1

3 Over-Parameterized Neural Networks Learn Polynomials

For completeness we provide a simple  self-contained analysis  showing how over-parameterized 
one-hidden-layer networks can provably learn polynomials with bounded degrees and coefﬁcients 
using standard stochastic gradient descent with standard initialization. In more details:

4

Theorem 3.1 (Informal). Given any distribution D over labeled data (x  y)  where x ∈ Rd  y ∈
{−1  +1}  any  > 0  and almost any multivariate polynomial P (x) with degree at most k and
coefﬁcients of magnitude at most α  if we take a one-hidden-layer neural network N (x) with analytic
activation functions which are L − Lipschitz  and with at least

(cid:18) 1



(cid:18) 1

(cid:19)

δ

(cid:19)

r > poly

  log

  dk2

  αk  L

neurons  and run poly(r) many iterations of stochastic gradient descent on i.i.d. examples  then with
probability at least 1 − δ over the random initialization  it holds that
E[LD(N (x))] ≤ LD(P (x)) +   

where LD is the expected hinge loss and the expectation is over the random choice of examples.
For a formal statement and full proof see Appendix C. We emphasize that although our analysis
improves on previous ones in certain aspects (discussed in more detail in Appendix A)  it is not
fundamentally novel: Our goal is mostly to present a transparent and self-contained result using the
approach developed in previous papers  focusing on clarity rather than generality. In comparison 
some of the related results assume that the output layer is ﬁxed (although in practice all layers are
optimized); some focus on training error rather than population risk; some do not quantitatively
characterize the class of polynomials learned by the network; and some consider different networks
architectures or optimization methods. For an in-depth review of previous works and how they differ
from the above result see Appendix A.

4 Limitations of Random Features

Having discussed and shown positive results for learning using (essentially) random features  we turn
to discuss the limitations of this approach.
Concretely  we will consider in this section data (x  y)  where x ∈ Rd is drawn from a standard
Gaussian on Rd  and there exists some single ground-truth neuron which generates the target values
y: Namely  y = σ((cid:104)w∗  x(cid:105) + b∗) for some ﬁxed w∗  b∗. We also consider the squared loss l(ˆy  y) =
(ˆy − y)2  so the expected loss we wish to minimize takes the form
uifi(x) − σ((cid:104)w∗  x(cid:105) + b∗)

(cid:32) r(cid:88)

(cid:33)2

Ex

(2)

i=1

where fi are the random features. Importantly  when r = 1  σ is the ReLU function  and f1(x) =
σ((cid:104)w  x(cid:105) + b) (that is  we train a single neuron to learn a single target neuron)  this problem is quite
tractable with standard gradient-based methods (see  e.g.  [26]  [34]). In this section  we ask whether
this positive result – that single target neurons can be learned – can be explained by the random
features approach. Speciﬁcally  we consider the case where the function fi are arbitrary functions
chosen obliviously of the target neuron (e.g. multilayered neural networks at a standard random
initialization)  and ask what conditions on r and ui are required to minimize Eq. (2). Our main
results (Theorem 4.2 and Theorem 4.8) show that either one of them has to be exponential in the
dimension d  as long as the sizes of w∗  b∗ are allowed to be polynomial in d. Since networks with
exponentially-many neurons or exponentially-sized weights are generally not efﬁciently trainable  we
conclude that an approach based on random-features cannot explain why learning single neurons is
tractable in practice. In Theorem 4.2  we show the result for any choice of w∗ with some ﬁxed norm 
but require the feature functions to have a certain structure (which is satisﬁed by neural networks). In
Theorem 4.8  we drop this requirement  but then the result only holds for a particular w∗.
To simplify the notation in this section  we consider functions on x as elements of the L2(Rd) space
weighted by a standard Gaussian measureSpeciﬁcally  for a function f : Rd → R we denote

(cid:2)f 2(x)(cid:3) = cd

(cid:90)

(cid:107)f (·)(cid:107)2 := Ex∼N (0 I)
Rd
(cid:104)f (·)  g(·)(cid:105) := Ex∼N (0 I)[f (x)g(x)] = cd

(cid:90)

Rd

f 2(x)e

−(cid:107)x(cid:107)2

2 dx 

f (x)g(x)e

−(cid:107)x(cid:107)2

2 dx 

is a normalization term. For example  Eq. (2) can also be written as

(cid:17)d

(cid:16) 1√

2π

where cd =

(cid:107)(cid:80)r

i=1 uifi(·) − σ((cid:104)w∗ ·(cid:105) + b∗)(cid:107)2.

5

4.1 Warm up: Linear predictors

Before stating our main results  let us consider a particularly simple case  where σ is the identity 
and our goal is to learn a linear predictor x (cid:55)→ (cid:104)w∗  x(cid:105) with (cid:107)w∗(cid:107) = 1. We will show that already
in this case  there is a signiﬁcant cost to pay for using random features. The main result in the next
subsection can be seen as an elaboration of this idea.
In this setting  ﬁnding a good linear predictor  namely minimizing (cid:107)(cid:104)w ·(cid:105) − (cid:104)w∗ ·(cid:105)(cid:107) is easy: It is a
convex optimization problem  and is easily solved using standard gradient-based methods. Suppose

now that we are given random features w1  . . .   wr ∼ N(cid:0)0  1
(cid:1) and want to ﬁnd u1  . . .   ur ∈ R
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ .

ui(cid:104)wi ·(cid:105) − (cid:104)w∗ ·(cid:105)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) r(cid:88)

i=1

such that:

d Id

(3)

The following proposition shows that with high probability  Eq. (3) cannot hold unless r = Ω(d).
This shows that even for linear predictors  there is a price to pay for using a combination of random
features  instead of learning the linear predictor directly.
Proposition 4.1. Let w∗ be some unit vector in Rd  and suppose that we pick random features
w1  . . .   wr i.i.d. from any spherically symmetric distribution in Rd. If r ≤ d
2   then with probability
at least 1 − exp(−cd) (for some universal constant c > 0)  for any choice of weights u1  . . .   ur  it

holds that (cid:107)(cid:80)r

i=1 ui(cid:104)wi ·(cid:105) − (cid:104)w∗ ·(cid:105)(cid:107)2 ≥ 1
4 .

The full proof appears in Appendix B  but the intuition is quite simple: With random features  we
are forced to learn a linear predictor in the span of w1  . . .   wr  which is a random r-dimensional
subspace of Rd. Since this subspace is chosen obliviously of w∗  and r ≤ d
2   it is very likely that w∗
is not close to this subspace (namely  the component of w∗ orthogonal to this subspace is large)  and
therefore we cannot approximate this target linear predictor very well.

4.2 Features Based on Random Linear Transformations

Having discussed the linear case  let us return to the case of a non-linear neuron. Speciﬁcally  we
will show that even a single ReLU neuron cannot be approximated by a very large class of random
feature predictors  unless the amount of neurons in the network is exponential in the dimension  or
the coefﬁcients of the linear combination are exponential in the dimension. In more details:
Theorem 4.2. There exists a universal constant c > 0 such that the following holds. Let d > 40 
k ∈ N  and let F be a family of functions from Rk to R. Also  let W ∈ Rk×d be a random matrix
whose rows are sampled uniformly at random from the unit sphere. Suppose that fW (x) := f (W x)
satisﬁes (cid:107)fW (·)(cid:107) ≤ exp(d/40) for any realization of W and for all f ∈ F. Then for every w∗ ∈ Rd
with (cid:107)w∗(cid:107) = d2  there exists b∗ ∈ R with |b∗| ≤ 6d3 + 1  such that for any f1  . . . fr ∈ F  w.p
> 1 − exp(−cd) over sampling W   if

(cid:32) r(cid:88)

(cid:33)2 ≤ 1

 

50

Ex∼N (0 I)

then

uifi(W x) − [(cid:104)w∗  x(cid:105) + b∗]+

i=1

r · max

i

|ui| ≥ 1

48d2 exp (cd) .

Note that the theorem allows any “random” feature which can be written as a composition of some
function f (chosen randomly or not)  and a random linear transformation.
Example 4.3. As a special case of Theorem 4.2  consider two layer neural networks with activation
σ : R → R and without a bias term. Denote by Wi the i-th row of W   then two layer neural networks
can be viewed as a linear combination of functions of the form:
fi(W x) = σ((cid:104)Wi  x(cid:105)) 

where if each Wi is bounded and the norm of the activation σ is bounded  then the norm of fi(W x)
is also bounded. This example can be extended to multi-layer neurons of any depth as long as the
ﬁrst layer performs a random linear transformation.

6

Remark 4.4. The requirement that the rows of W are sampled uniformly from the unit sphere can be
easily relaxed so that the rows of W have a spherically symmetrical distribution and are bounded
w.h.p. These kind of distributions includes  among others  bounded distributions and standard
Gaussian distributions. However it would require a more careful analysis on the bound of the
functions fW   as they might be only bounded w.h.p.
Remark 4.5. As opposed to the linear case  here we also have a restriction on maxi |ui|. We
conjecture that it is possible to remove this dependence and leave it to future work. With that said 
existing analyses of stochastic gradient descent  even for convex functions  imply that the required
number of iterations scales polynomially with the norm of the target solution (e.g.  Hazan et al. [19]) 
which would mean exponentially many iterations in our case. Moreover  practically speaking  such
huge coefﬁcients can cause overﬂow when running SGD on a computer with standard ﬂoating point
formats.

To prove the theorem  we will use the following proposition  which implies that functions of the
form x (cid:55)→ ψ((cid:104)w  x(cid:105)) for a certain sine-like ψ and "random" w are nearly uncorrelated with any ﬁxed
function.
Proposition 4.6. Let d ∈ N  where d > 40  and let a = 6d2 + 1. Deﬁne the following function:

a(cid:88)

ψ(x) = [x + a]+ +

2[x + a − 2n]+(−1)n − 1 

Then ψ : R → R satisﬁes the following:

n=1

1. It is a periodic odd function on the interval [−a  a]
2. For every w∗ ∈ Rd with (cid:107)w∗(cid:107) = d  (cid:107)ψ((cid:104)w∗ ·(cid:105))(cid:107)2 ≥ d.
3. For every f ∈ L2(Rd)  we have Ew

(cid:0)(cid:104)f (·)  ψ ((cid:104)w ·(cid:105))(cid:105)2(cid:1) ≤ 20(cid:107)f(cid:107)2 · exp(−cd)  where w

is sampled uniformly from {w : (cid:107)w(cid:107) = d}  and c > 0 is a universal constant.

Items 1 and 2 follow by a straightforward calculation  where in item 2 we also used the fact that x
has a symmetric distribution. Item 3 relies on a claim from [33]  which shows that periodic functions
of the form x (cid:55)→ ψ((cid:104)w  x(cid:105)) for a random w with sufﬁciently large norm have low correlation with
any ﬁxed function. The full proof can be found in Appendix B.
At a high level  the proof of Theorem 4.2 proceeds as follows: If we choose and ﬁx {fi}r
i=1 and
W   then any linear combination of random features fi(W x) with small weights will be nearly
uncorrelated with ψ((cid:104)w∗  x(cid:105))  in expectation over w∗ . But  we know that ψ((cid:104)w∗  x(cid:105)) can be written
as a linear combination of ReLU neurons  so there must be some ReLU neuron which will be
nearly uncorrelated with any linear combination of the random features (and as a result  cannot be
well-approximated by them). Finally  by a symmetry argument  we can actually ﬁx w∗ arbitrarily and
the result still holds. We now turn to provide the formal proof:
Proof of Theorem 4.2. Take ψ(x) from Proposition 4.6 and denote for w ∈ Rd  ψw(x) = ψ((cid:104)w  x(cid:105)) :
Rd → R. If we sample w∗ uniformly from {w : (cid:107)w(cid:107) = d}  then for all f ∈ F:
Ew∗ [|(cid:104)fW   ψw∗(cid:105)|] ≤ 20(cid:107)fW(cid:107)2 exp(−c(cid:48)d) ≤ exp(−cd) 

where c is a universal constant that depends only on the constant c(cid:48) from Proposition 4.6. Hence also:

Ew∗ [EW [|(cid:104)fW   ψw∗(cid:105)|]] ≤ exp(−cd)

We now show that EW [|(cid:104)fW   ψw∗(cid:105)|] doesn’t depend on w∗. Fix w∗  then any w ∈ Rd with (cid:107)w(cid:107) = d
can be written as M w∗ for some orthogonal matrix M. Now:
EW [|(cid:104)fW   ψw∗(cid:105)|] EW [Ex [|f (W x) · ψ((cid:104)w∗  x(cid:105))|]] = EW [Ex
= EW [Ex [|f (W x) · ψ((cid:104)M w∗  x(cid:105))|]] = EW [|(cid:104)fW   ψM w∗(cid:105)|]
where we used the fact that both x and the rows of W have a spherically symmetric distribution.
Therefore  for all w∗ with (cid:107)w∗(cid:107) = d:

(cid:2)(cid:12)(cid:12)f (W M T M x) · ψ((cid:104)M w∗  M x(cid:105))(cid:12)(cid:12)(cid:3)]

EW [|(cid:104)fW   ψw∗(cid:105)|] ≤ exp(−cd)

(4)

7

Using Markov’s inequality and dividing c by a factor of 2  we get w.p > 1− exp (−cd) over sampling
of W   |(cid:104)fW   ψw∗(cid:105)| ≤ exp(−cd). Finally  if we pick f1  . . .   fr ∈ F  using the union bound we get
that w.p > 1 − r exp (−cd) over sampling of W :

We can write ψ(x) =(cid:80)a

∀i ∈ {1  . . .   r}  |(cid:104)fiW   ψw∗(cid:105)| ≤ exp(−cd)
j=1 aj[x + cj]+ − 1  where a = 6d2 + 1 and |aj| ≤ 2  cj ∈ [−a  a] for
j (x) = [(cid:104)w∗  x(cid:105) + cj]+. Assume that for

j = 1  . . . a. Let w∗ ∈ Rd with (cid:107)w∗(cid:107) = d  and denote f∗
every j we can ﬁnd uj ∈ Rr and f1  . . .   fr ∈ F such that
is distributed as above  otherwise there is a ReLU neuron that cannot be represented by a linear
combination of random features  which is what we want to prove.. Let f0(W x) = b0 the bias term of
the output layer of the network  then also:

i fiW − f∗

i=1 uj

j

(cid:13)(cid:13)(cid:13)(cid:80)r
2 = Ex

(cid:32) r(cid:88)

(cid:13)(cid:13)(cid:13)2 ≤   where W
(cid:33)2

(cid:101)uifi(W x) − ψ((cid:104)w∗  x(cid:105))

j (x) − 1

uj
i aj

i=0

j=1

Ex

≤


 r(cid:88)
 a(cid:88)
a(cid:88)
(cid:16)(cid:80)a
where(cid:101)ui =
(cid:32) r(cid:88)

|aj|2 ≤ 24d2

j=1 uj

Ex

j=1

i aj

i=0

≥d − 2 max

i

j=1

ajf∗

 fi(W x) − a(cid:88)
(cid:17)
(cid:101)uifi(W x) − ψ((cid:104)w∗  x(cid:105))
|(cid:101)ui| r(cid:88)

i=1

w.p > 1 − r exp (−cd) over the distribution of W that:

|(cid:104)fiW   ψw∗(cid:105)| ≥ d − 2 max

. On the other hand using Eq. (4) and item 2 from Proposition 4.6 we get

(cid:33)2 ≥ (cid:107)ψw∗(cid:107)2 − 2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:104) r(cid:88)
|(cid:101)ui|r exp (−cd)

i=0

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:101)uifiW   ψw∗(cid:105)

i=0

(5)

(cid:33)2 ≤ 

(cid:33)2 ≤ 1

 

50

≥d − 4a max

|uj
i|r exp (−cd)  

(6)
where the last inequality is true for all j = 1  . . .   a. Combining Eq. (5) and Eq. (6)  for all w∗ ∈ Rd
with (cid:107)w∗(cid:107) = d there is b∗ ∈ [−a  a] such that w.p > 1 − r exp (−cd) over the sampling of W   if
there is u ∈ Rr that:

i

uifi(W x) − [(cid:104)w∗  x(cid:105) + b∗]+

(7)

Ex

(cid:32) r(cid:88)
then r maxi |ui| ≥(cid:0)1 − 24d) 1
(cid:32) r(cid:88)

Ex

i=1

i=1

24d exp (cd).

Lastly  by multiplying both sides of Eq. (7) by d  using the homogeneity of ReLU and setting  = 1
50d
we get that for every ˆw∗ ∈ Rd with (cid:107) ˆw∗(cid:107) = d2 there is ˆb∗ ∈ [−6d3 − 1  6d3 + 1] such that w.p
> 1 − r exp (−cd) over the sampling of W   if there is ˆu ∈ Rr such that

ˆuifi(W x) − [(cid:104) ˆw∗  x(cid:105) + ˆb∗]+

48d2 exp (cd).

then r maxi |ˆui| ≥ 1
Remark 4.7. It is possible to trade-off between the norm of w∗ and the required error. This
can be done by altering the proof of Theorem 4.2  where instead of multiplying both sides of
Eq. (7) by d we could have multiplied both sides by α
d . This way the following is proved: With
the same assumptions as in Theorem 4.2  for all w∗ ∈ Rd with (cid:107)w∗(cid:107) = α there is b∗ ∈ R with

(cid:1) and all f1  . . .   fr ∈ F  w.h.p over sampling of W   if
(cid:1)2(cid:105) ≤   then r · maxi |ui| ≥ (1 − 5d2

|b∗| ≤ 6αd + 1 such that for all  ∈(cid:0)0  α
(cid:104)(cid:0)(cid:80)r

i=1 uifi(W x) − [(cid:104)w∗  x(cid:105) + b∗]+

8α exp (c3d) for

α ) 1

5d2

Ex
a universal constant c3.

8

4.3 General Features
In the previous subsection  we assumed that our features have a structure of the form x (cid:55)→ f (W x)
for a random matrix W . We now turn to a more general case  where we are given features of any
kind without any assumptions on their internal structure  as long as they are sampled from some ﬁxed
distribution. We show that even in this setting  such features cannot capture single ReLU neurons in
the worst-case (at the cost of proving this for some target weight vector w∗  instead of any w∗).
Theorem 4.8. There exists a universal constant c such that the following holds. Let d > 40  and let
F be a family of functions from Rd to R  such that (cid:107)f(cid:107) ≤ exp(d/40) for all f ∈ F. Also  for some
r ∈ N  let D be an arbitrary distribution over tuples (f1  . . . fr) of functions from F. Then there
exists w∗ ∈ Rd with (cid:107)w∗(cid:107) = d2  and b∗ ∈ R with |b∗| ≤ 6d3 + 1  such that with probability at least
1 − r exp(−cd) over sampling f1  . . .   fr  if

(cid:32) r(cid:88)

(cid:33)2 ≤ 1

 

50

Ex∼N (0 I)

then

uifi(x) − [(cid:104)w∗  x(cid:105) + b∗]+

i=1

r · max

i

|ui| ≥ 1

48d2 exp (cd) .

The proof is similar to the proof of Theorem 4.2. The main difference is that we do not have any
assumptions on the distribution of the random features (as the assumption on the distribution on
W in Theorem 4.2)  hence we can only show that there exists some ReLU neuron that cannot be
well-approximated. On the other hand  we have almost no restrictions on the random features  e.g.
they can be multi-layered neural network of any architecture and with any random initialization.
Example 4.9. Besides generalizing the setting of the previous subsection  Theorem 4.8 also captures
the setting of "linearized" neural tangent kernel (see Jacot et al. [20] and Subsection 2.1)  where we
consider a linear combination of functions of the form:

(8)
where wi are randomaly chosen and ai are being optimized using gradient based methods. This is
because each such function is a weighted sum of the features fi j(x) = σ(cid:48)((cid:104)wi  x(cid:105))xj for 1 ≤ j ≤ d
and wi are randomly initialized.

σ(cid:48)((cid:104)wi  x(cid:105))(cid:104)ai  x(cid:105) 

Acknowledgements

This research is supported in part by European Research Council (ERC) grant 754705. We thank
Yuanzhi Li for some helpful comments on the previous version of this paper.

References
[1] Z. Allen-Zhu and Y. Li. Can SGD learn recurrent neural networks with provable generalization?

arXiv preprint arXiv:1902.01028  2019.

[2] Z. Allen-Zhu  Y. Li  and Y. Liang. Learning and generalization in overparameterized neural

networks  going beyond two layers. arXiv preprint arXiv:1811.04918  2018.

[3] Z. Allen-Zhu  Y. Li  and Z. Song. A convergence theory for deep learning via over-

parameterization. arXiv preprint arXiv:1811.03962  2018.

[4] A. Andoni  R. Panigrahy  G. Valiant  and L. Zhang. Learning polynomials with neural networks.

In International Conference on Machine Learning  pages 1908–1916  2014.

[5] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE

Transactions on Information theory  39(3):930–945  1993.

[6] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[7] A. Brutzkus and A. Globerson. Over-parameterization improves generalization in the xor

detection problem. arXiv preprint arXiv:1810.03037  2018.

9

[8] A. Brutzkus  A. Globerson  E. Malach  and S. Shalev-Shwartz. Sgd learns over-parameterized
networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174 
2017.

[9] Y. Cao and Q. Gu. A generalization theory of gradient descent for learning over-parameterized

deep ReLU networks. arXiv preprint arXiv:1902.01384  2019.

[10] L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in neural information processing systems  pages
3040–3050  2018.

[11] A. Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural

Information Processing Systems  pages 2422–2430  2017.

[12] A. Daniely  R. Frostig  and Y. Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems  pages 2253–2261  2016.

[13] S. S. Du and J. D. Lee. On the power of over-parametrization in neural networks with quadratic

activation. arXiv preprint arXiv:1803.01206  2018.

[14] S. S. Du  J. D. Lee  H. Li  L. Wang  and X. Zhai. Gradient descent ﬁnds global minima of deep

neural networks. arXiv preprint arXiv:1811.03804  2018.

[15] S. S. Du  X. Zhai  B. Poczos  and A. Singh. Gradient descent provably optimizes over-

parameterized neural networks. arXiv preprint arXiv:1810.02054  2018.

[16] B. R. Gelbaum  J. G. De Lamadrid  et al. Bases of tensor products of banach spaces. Paciﬁc

Journal of Mathematics  11(4):1281–1286  1961.

[17] B. Ghorbani  S. Mei  T. Misiakiewicz  and A. Montanari. Linearized two-layers neural networks

in high dimension. arXiv preprint arXiv:1904.12191  2019.

[18] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence
and statistics  pages 249–256  2010.

[19] E. Hazan et al. Introduction to online convex optimization. Foundations and Trends R(cid:13) in

Optimization  2(3-4):157–325  2016.

[20] A. Jacot  F. Gabriel  and C. Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in neural information processing systems  pages 8571–8580 
2018.

[21] J. M. Klusowski and A. R. Barron. Approximation by combinations of ReLU and squared
ReLU ridge functions with l1 and l0 controls. IEEE Transactions on Information Theory  64
(12):7649–7656  2018.

[22] M. Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical

Soc.  2001.

[23] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent
on structured data. In Advances in Neural Information Processing Systems  pages 8168–8177 
2018.

[24] Y. Li  T. Ma  and H. Zhang. Algorithmic regularization in over-parameterized matrix sensing

and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203  2017.

[25] R. Livni  S. Shalev-Shwartz  and O. Shamir. On the computational efﬁciency of training neural

networks. In Advances in Neural Information Processing Systems  pages 855–863  2014.

[26] S. Mei  Y. Bai  and A. Montanari. The landscape of empirical risk for non-convex losses. arXiv

preprint arXiv:1607.06534  2016.

10

[27] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems  pages 1177–1184  2008.

[28] A. Rahimi and B. Recht. Uniform approximation of functions with random bases. In 2008
46th Annual Allerton Conference on Communication  Control  and Computing  pages 555–561.
IEEE  2008.

[29] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in neural information processing systems  pages
1313–1320  2009.

[30] I. Safran and O. Shamir. On the quality of the initial basin in overspeciﬁed neural networks. In

International Conference on Machine Learning  pages 774–782  2016.

[31] I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks.

arXiv preprint arXiv:1712.08968  2017.

[32] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[33] O. Shamir. Distribution-speciﬁc hardness of learning neural networks. The Journal of Machine

Learning Research  19(1):1135–1163  2018.

[34] M. Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information

Processing Systems  pages 2007–2017  2017.

[35] M. Soltanolkotabi  A. Javanmard  and J. D. Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory  65(2):742–769  2019.

[36] D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

[37] D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer

neural networks. arXiv preprint arXiv:1702.05777  2017.

[38] Y. Sun  A. Gilbert  and A. Tewari. Random ReLU features: Universality  approximation  and

composition. arXiv preprint arXiv:1810.04374  2018.

[39] G. Wang  G. B. Giannakis  and J. Chen. Learning relu networks on linearly separable data:

Algorithm  optimality  and generalization. arXiv preprint arXiv:1808.04685  2018.

[40] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires

rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

[41] Y. Zhang  J. D. Lee  and M. I. Jordan. l1-regularized neural networks are improperly learnable
in polynomial time. In International Conference on Machine Learning  pages 993–1001  2016.

11

,Gilad Yehudai
Ohad Shamir