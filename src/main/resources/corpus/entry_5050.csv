2010,Slice sampling covariance hyperparameters of latent Gaussian models,The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However  with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.,Slice sampling covariance hyperparameters

of latent Gaussian models

Iain Murray

School of Informatics

University of Edinburgh

Ryan Prescott Adams
Dept. Computer Science

University of Toronto

Abstract

The Gaussian process (GP) is a popular way to specify dependencies be-
tween random variables in a probabilistic model. In the Bayesian framework
the covariance structure can be speciﬁed using unknown hyperparameters.
Integrating over these hyperparameters considers diﬀerent possible expla-
nations for the data when making predictions. This integration is often per-
formed using Markov chain Monte Carlo (MCMC) sampling. However  with
non-Gaussian observations standard hyperparameter sampling approaches
require careful tuning and may converge slowly. In this paper we present
a slice sampling approach that requires little tuning while mixing well in
both strong- and weak-data regimes.

1 Introduction

Many probabilistic models incorporate multivariate Gaussian distributions to explain de-
pendencies between variables. Gaussian process (GP) models and generalized linear mixed
models are common examples. For non-Gaussian observation models  inferring the parame-
ters that specify the covariance structure can be diﬃcult. Existing computational methods
can be split into two complementary classes: deterministic approximations and Monte Carlo
simulation. This work presents a method to make the sampling approach easier to apply.

In recent work Murray et al. [1] developed a slice sampling [2] variant  elliptical slice sam-
pling  for updating strongly coupled a-priori Gaussian variates given non-Gaussian obser-
vations. Previously  Agarwal and Gelfand [3] demonstrated the utility of slice sampling for
updating covariance parameters  conventionally called hyperparameters  with a Gaussian
observation model  and questioned the possibility of slice sampling in more general settings.
In this work we develop a new slice sampler for updating covariance hyperparameters. Our
method uses a robust representation that should work well on a wide variety of problems 
has very few technical requirements  little need for tuning and so should be easy to apply.

1.1 Latent Gaussian models

We consider generative models of data that depend on a vector of latent variables f that are
Gaussian distributed with covariance Σθ set by unknown hyperparameters θ. These models
are common in the machine learning Gaussian process literature [e.g. 4] and throughout the
statistical sciences. We use standard notation for a Gaussian distribution with mean m and
covariance Σ 

(1)
and use f ∼ N (m  Σ) to indicate that f is drawn from a distribution with the density in (1).

N (f ; m  Σ) ≡ |2πΣ|−1/2 exp(cid:0)− 1

2 (f−m)(cid:62)Σ−1(f−m)(cid:1) 

1

(a) Prior draws

(b) Lengthscale given f

Figure 1: (a) Shows draws from the prior over f using three diﬀerent lengthscales in the squared
exponential covariance (2). (b) Shows the posteriors over log-lengthscale for these three draws.

The generic form of the generative models we consider is summarized by

covariance hyperparameters θ ∼ ph 

latent variables f ∼ N (0  Σθ) 

and a conditional likelihood P (data|f ) = L(f ).

The methods discussed in this paper apply to covariances Σθ that are arbitrary positive
deﬁnite functions parameterized by θ. However  our experiments focus on the popular case
where the covariance is associated with N input vectors {xn}N
n=1 through the squared-
exponential kernel 
(xd i−xd j )2

(cid:17)

(cid:80)D

(2)

(cid:16)− 1

2

(Σθ)ij = k(xi  xj) = σ2

f exp
f  {(cid:96)d}}. Here σ2

with hyperparameters θ ={σ2
f is the ‘signal variance’ controlling the overall
scale of the latent variables f . The (cid:96)d give characteristic lengthscales for converting the
distances between inputs into covariances between the corresponding latent values f .

d=1

(cid:96)2
d

 

For non-Gaussian likelihoods we wish to sample from the joint posterior over unknowns 

(3)
We would like to avoid implementing new code or tuning algorithms for diﬀerent covariances
Σθ and conditional likelihood functions L(f ).

P (f   θ|data) = 1

Z L(f )N (f ; 0  Σθ) ph(θ) .

invariant if π(z(cid:48)) =(cid:82) T (z(cid:48) ← z) π(z) dz. A standard way to sample from the joint poste-

2 Markov chain inference
A Markov chain transition operator T (z(cid:48) ← z) deﬁnes a conditional distribution on a new
position z(cid:48) given an initial position z. The operator is said to leave a target distribution π
rior (3) is to alternately simulate transition operators that leave its conditionals  P (f |data  θ)
and P (θ| f )  invariant. Under fairly mild conditions the Markov chain will equilibrate to-
wards the target distribution [e.g. 5].

Recent work has focused on transition operators for updating the latent variables f given
data and a ﬁxed covariance Σθ [6  1]. Updates to the hyperparameters for ﬁxed latent
variables f need to leave the conditional posterior 

(4)
invariant. The simplest algorithm for this is the Metropolis–Hastings operator  see Algo-
rithm 1. Other possibilities include slice sampling [2] and Hamiltonian Monte Carlo [7  8].

P (θ|f ) ∝ N (f ; 0  Σθ) ph(θ) 

Alternately ﬁxing the unknowns f and θ is appealing from an implementation standpoint.
However  the resulting Markov chain can be very slow in exploring the joint posterior distri-
bution. Figure 1a shows latent vector samples using squared-exponential covariances with
diﬀerent lengthscales. These samples are highly informative about the lengthscale hyperpa-
rameter that was used  especially for short lengthscales. The sharpness of P (θ| f )  Figure 1b 
dramatically limits the amount that any Markov chain can update the hyperparameters θ
for ﬁxed latent values f .

2

00.20.40.60.81−2−1.5−1−0.500.51Input Space  xLatent values  f  l = 0.1l = 0.5l = 210−210−110010100.020.040.060.080.1lengthscale  lp(log l | f)  l = 0.1l = 0.5l = 2Algorithm 1 M–H transition for ﬁxed f

Input: Current f and hyperparameters θ;
proposal dist. q; covariance function Σ().

Output: Next hyperparameters
1: Propose: θ(cid:48) ∼ q(θ(cid:48) ; θ)
2: Draw u ∼ Uniform(0  1)
3: if u <
4:
5: else
6:

N (f ;0 Σθ(cid:48) ) ph(θ(cid:48)) q(θ ; θ(cid:48))
N (f ;0 Σθ) ph(θ) q(θ(cid:48); θ)

return θ(cid:48)

return θ

(cid:46) Accept new state

(cid:46) Keep current state

Algorithm 2 M–H transition for ﬁxed ν
Input: Current state θ  f ; proposal dist. q;
covariance function Σ(); likelihood L().

Σθ

Output: Next θ  f
1: Solve for N (0  I) variate: ν = L−1
f
2: Propose θ(cid:48) ∼ q(θ(cid:48) ; θ)
3: Compute implied values: f(cid:48) = LΣθ(cid:48) ν
4: Draw u ∼ Uniform(0  1)
L(f(cid:48)) ph(θ(cid:48)) q(θ ; θ(cid:48))
5: if u <
L(f ) ph(θ) q(θ(cid:48); θ)
6:
7: else
8:

return θ(cid:48)  f(cid:48)

return θ  f

(cid:46) Accept new state

(cid:46) Keep current state

2.1 Whitening the prior

Often the conditional likelihood is quite weak; this is why strong prior smoothing as-
sumptions are often introduced in latent Gaussian models.
In the extreme limit in
i.e. L is constant  the target distribution is the prior model 
which there is no data 
P (f   θ) = N (f ; 0  Σθ) ph(θ). Sampling from the prior should be easy  but alternately ﬁx-
ing f and θ does not work well because they are strongly coupled. One strategy is to
reparameterize the model so that the unknown variables are independent under the prior.

Independent random variables can be identiﬁed from a commonly-used generative procedure
for the multivariate Gaussian distribution. A vector of independent normals  ν  is drawn
independently of the hyperparameters and then deterministically transformed:

ν ∼ N (0  I) 

Σθ

= Σθ.

f = LΣθ ν 

where LΣθ

(5)
Notation: Throughout this paper LC will be any user-chosen square root of covariance
matrix C. While any matrix square root can be used  the lower-diagonal Cholesky decom-
position is often the most convenient. We would reserve C 1/2 for the principal square root 
because other square roots do not behave like powers: for example  chol(C)−1 (cid:54)= chol(C−1).
We can choose to update the hyperparameters θ for ﬁxed ν instead of ﬁxed f . As the
original latent variables f are deterministically linked to the hyperparameters θ in (5)  these
updates will actually change both θ and f . The samples in Figure 1a resulted from using
the same whitened variable ν with diﬀerent hyperparameters. They follow the same general
trend  but vary over the lengthscales used to construct them.

L(cid:62)

The posterior over hyperparameters for ﬁxed ν is apparent by applying Bayes rule to the
generative procedure in (5)  or one can laboriously obtain it by changing variables in (3):

P (θ| ν  data) ∝ P (θ  ν  data) = P (θ  f = LΣθ ν  data)|LΣθ| ∝ ··· ∝ L(f (θ  ν)) ph(θ).

(6)

Algorithm 2 is the Metropolis–Hastings operator for this distribution. The acceptance rule
now depends on the latent variables through the conditional likelihood L(f ) instead of the
prior N (f ; 0  Σθ) and these variables are automatically updated to respect the prior. In the
no-data limit  new hyperparameters proposed from the prior are always accepted.

3 Surrogate data model

Neither of the previous two algorithms are ideal for statistical applications  which is illus-
trated in Figure 2. Algorithm 2 is ideal in the “weak data” limit where the latent variables f
are distributed according to the prior. In the example  the likelihoods are too restrictive for
Algorithm 2’s proposal to be acceptable. In the “strong data” limit  where the latent vari-
ables f are ﬁxed by the likelihood L  Algorithm 1 would be ideal. However  the likelihood
terms in the example are not so strong that the prior can be ignored.

For regression problems with Gaussian noise the latent variables can be marginalised out an-
alytically  allowing hyperparameters to be accepted or rejected according to their marginal
posterior P (θ|data).
If latent variables are required they can be sampled directly from
the conditional posterior P (f |θ  data). To build a method that applies to non-Gaussian
likelihoods  we create an auxiliary variable model that introduces surrogate Gaussian ob-
servations that will guide joint proposals of the hyperparameters and latent variables.

3

Figure 2: A regression problem with Gaussian observations illustrated by 2σ gray bars. The
current state of the sampler has a short lengthscale hyperparameter ((cid:96) = 0.3); a longer lengthscale
((cid:96) = 1.5) is being proposed. The current latent variables do not lie on a straight enough line for the
long lengthscale to be plausible. Whitening the prior (Section 2.1) updates the latent variables to
a straighter line  but ignores the observations. A proposal using surrogate data (Section 3  with Sθ
set to the observation noise) sets the latent variables to a draw that is plausible for the proposed
lengthscale while being close to the current state.

P (g| f   θ) = N (g; f   Sθ).

We augment the latent Gaussian model with auxiliary variables  g  a noisy version of the
true latent variables:

(7)
For now Sθ is an arbitrary free parameter that could be set by hand to either a ﬁxed
value or a value that depends on the current hyperparameters θ. We will discuss how to
automatically set the auxiliary noise covariance Sθ in Section 3.2.
The original model  f ∼ N (0  Σθ) and (7) deﬁne a joint auxiliary distribution P (f   g| θ)
given the hyperparameters. It is possible to sample from this distribution in the opposite
order  by ﬁrst drawing the auxiliary values from their marginal distribution

and then sampling the model’s latent values conditioned on the auxiliary values from

P (g| θ) = N (g; 0  Σθ +Sθ) 

P (f | g  θ) = N (f ; mθ g  Rθ)  where some standard manipulations give:

θ +S−1

Rθ = (Σ−1
mθ g = Σθ(Σθ +Sθ)−1g = RθS−1
θ g.

θ )−1 = Σθ−Σθ(Σθ +Sθ)−1Σθ = Sθ−Sθ(Sθ +Σθ)−1Sθ 

(8)

(9)

That is  under the auxiliary model the latent variables of interest are drawn from their
posterior given the surrogate data g. Again we can describe the sampling process via a
draw from a spherical Gaussian:

η ∼ N (0  I) 

f = LRθ η + mθ g 

where LRθ L(cid:62)

Rθ

= Rθ.

(10)

We then condition on the “whitened” variables η and the surrogate data g while updating
the hyperparameters θ. The implied latent variables f (θ  η  g) will remain a plausible draw
from the surrogate posterior for the current hyperparameters. This is illustrated in Figure 2.

We can leave the joint distribution (3) invariant by updating the following conditional
distribution derived from the above generative model:

P (θ|η  g  data) ∝ P (θ  η  g  data) ∝ L(cid:0)f (θ  η  g)(cid:1)N (g; 0  Σθ +Sθ) ph(θ).

(11)

The Metropolis–Hastings Algorithm 3 contains a ratio of these terms in the acceptance rule.

3.1 Slice sampling
The Metropolis–Hastings algorithms discussed so far have a proposal distribution q(θ(cid:48); θ)
that must be set and tuned. The eﬃciency of the algorithms depend crucially on careful
choice of the scale σ of the proposal distribution. Slice sampling [2] is a family of adaptive
search procedures that are much more robust to the choice of scale parameter.

4

00.20.40.60.81−2−1.5−1−0.500.5Input Space  xObservations  y  current state fwhitened prior proposalsurrogate data proposalAlgorithm 3 Surrogate data M–H

Algorithm 4 Surrogate data slice sampling

θ  f ; prop. dist. q; model of Sec. 3.

Rθ

η = L−1

(f − mθ g)

Input:
Output: Next θ  f
1: Draw surrogate data: g ∼ N (f   Sθ)
2: Compute implied latent variates:
3: Propose θ(cid:48) ∼ q(θ(cid:48) ; θ)
4: Compute function f(cid:48) = LRθ(cid:48) η + mθ(cid:48) g
5: Draw u ∼ Uniform(0  1)
L(f(cid:48)) N (g;0 Σθ(cid:48)+Sθ(cid:48) ) ph(θ(cid:48)) q(θ ; θ(cid:48))
6: if u <
L(f ) N (g;0 Σθ+Sθ) ph(θ) q(θ(cid:48) ; θ)
7:
8: else
9:

return θ(cid:48)  f(cid:48)

return θ  f

(cid:46) Accept new state

(cid:46) Keep current state

Input:
θ  f ; scale σ; model of Sec. 3.
Output: Next f   θ
1: Draw surrogate data: g ∼ N (f   Sθ)
2: Compute implied latent variates:

(f − mθ g)

η = L−1
v ∼ Uniform(0  σ)  θmin = θ−v  θmax = θmin +σ

Rθ

y = uL(f )N (g; 0  Σθ +Sθ) ph(θ)

3: Randomly center a bracket:
4: Draw u ∼ Uniform(0  1)
5: Determine threshold:
6: Draw proposal: θ(cid:48) ∼ Uniform(θmin  θmax)
7: Compute function f(cid:48) = LRθ(cid:48) η + mθ(cid:48) g
8: if L(f(cid:48))N (g; 0  Σθ(cid:48) +Sθ(cid:48)) ph(θ(cid:48)) > y
9:
10: else if θ(cid:48) < θ
11:
12: else
13:
14: goto 6

return f(cid:48)  θ(cid:48)
Shrink bracket minimum: θmin = θ(cid:48)
Shrink bracket maximum: θmax = θ(cid:48)

Algorithm 4 applies one possible slice sampling algorithm to a scalar hyperparameter θ in
the surrogate data model of this section. It has a free parameter σ  the scale of the initial
proposal distribution. However  careful tuning of this parameter is not required. If the initial
scale is set to a large value  such as the width of the prior  then the width of the proposals will
shrink to an acceptable range exponentially quickly. Stepping-out procedures [2] could be
used to adapt initial scales that are too small. We assume that axis-aligned hyperparameter
moves will be eﬀective  although reparameterizations could improve performance [e.g. 9].

3.2 The auxiliary noise covariance Sθ
The surrogate data g and noise covariance Sθ deﬁne a pseudo-posterior distribution that
softly speciﬁes a plausible region within which the latent variables f are updated. The noise
covariance determines the size of this region. The ﬁrst two baseline algorithms of Section 2
result from limiting cases of Sθ = αI: 1) if α = 0 the surrogate data and the current latent
variables are equal and the acceptance ratio reduces to that of Algorithm 1. 2) as α →∞
the observations are uninformative about the current state and the pseudo-posterior tends
to the prior. In the limit  the acceptance ratio reduces to that of Algorithm 2. One could
choose α based on preliminary runs  but such tuning would be burdensome.

For likelihood terms that factorize  L(f ) =(cid:81)

i Li(fi)  we can measure how much the likeli-

hood restricts each variable individually:

P (fi|Li  θ) ∝ Li(fi) N (fi; 0  (Σθ)ii).

(12)
A Gaussian can be ﬁtted by moment matching or a Laplace approximation (matching sec-
ond derivatives at the mode). Such ﬁts  or close approximations  are often possible analyti-
cally and can always be performed numerically as the distribution is only one-dimensional.
Given a Gaussian ﬁt to the site-posterior (12) with variance vi  we can set the auxil-
iary noise to a level that would result in the same posterior variance at that site alone:
−1)−1. (Any negative (Sθ)ii must be thresholded.) The moment match-
(Sθ)ii = (v−1
ing procedure is a grossly simpliﬁed ﬁrst step of “assumed density ﬁltering” or “expectation
propagation” [10]  which are too expensive for our use in the inner-loop of a Markov chain.

i −(Σθ)ii

4 Related work
We have discussed samplers that jointly update strongly-coupled latent variables and hy-
perparameters. The hyperparameters can move further in joint moves than their narrow
conditional posteriors (e.g.  Figure 1b) would allow. A generic way of jointly sampling real-
valued variables is Hamiltonian/Hybrid Monte Carlo (HMC) [7  8]. However  this method
is cumbersome to implement and tune  and using HMC to jointly update latent variables
and hyperparameters in hierarchical models does not itself seem to improve sampling [11].

Christensen et al. [9] have also proposed a robust representation for sampling in latent
Gaussian models. They use an approximation to the target posterior distribution to con-

5

struct a reparameterization where the unknown variables are close to independent. The
approximation replaces the likelihood with a Gaussian form proportional to N (f ; ˆf   Λ(ˆf )):

ˆf = argmaxf L(f ) 

Λij(ˆf ) = ∂2 log L(f )

∂fi ∂fj

 

(13)

where Λ is often diagonal  or it was suggested one would only take the diagonal part.
This Taylor approximation looks like a Laplace approximation  except that the likelihood
function is not a probability density in f . This likelihood ﬁt results in an approximate

Gaussian posterior N (f ; mθ g=ˆf   Rθ) as found in (9)  with noise Sθ = Λ(ˆf )−1 and data g =ˆf .
(f − mθ ˆf ).
ω∼N (0  I)  f = LRθ ω + mθ ˆf   suggests using the reparameterization ω = L−1

Thinking of the current latent variables as a draw from this approximate posterior 

Rθ

We can then ﬁx the new variables and update the hyperparameters under
P (θ| ω  data) ∝ L(f (ω  θ))N (f (ω  θ); 0  Σθ) ph(θ)|LRθ| .

(14)

When the likelihood is Gaussian  the reparameterized variables ω are independent of each
other and the hyperparameters. The hope is that approximating non-Gaussian likelihoods
will result in nearly-independent parameterizations on which Markov chains will mix rapidly.

Taylor expanding some common log-likelihoods around the maximum is not well deﬁned 
for example approximating probit or logistic likelihoods for binary classiﬁcation  or Pois-
son observations with zero counts. These Taylor expansions could be seen as giving ﬂat or
undeﬁned Gaussian approximations that do not reweight the prior. When all of the like-
lihood terms are ﬂat the reparameterization approach reduces to that of Section 2.1. The
alternative Sθ auxiliary covariances that we have proposed could be used instead.

Rθ

The surrogate data samplers of Section 3 can also be viewed as using reparameterizations 
(f − mθ g) as an arbitrary random reparameterization for making pro-
by treating η = L−1
posals. A proposal density q(η(cid:48)  θ(cid:48); η  θ) in the reparameterized space must be multiplied by
the Jacobian |L−1
Rθ(cid:48)| to give a proposal density in the original parameterization. The proba-
bility of proposing the reparameterization must also be included in the Metropolis–Hastings
acceptance probability:

min

1 

−1
P (θ(cid:48) f(cid:48) | data)·P (g| f(cid:48) Sθ(cid:48) )·q(θ;θ(cid:48)) |L
Rθ
−1
θ(cid:48) |
P (θ f | data)·P (g| f  Sθ)·q(θ(cid:48);θ) |L
R

|

.

(15)

(cid:12)(cid:12)(cid:12)ˆf

(cid:19)

(cid:18)

A few lines of linear algebra conﬁrms that  as it must do  the same acceptance ratio results
as before. Alternatively  substituting (3) into (15) shows that the acceptance probability
is very similar to that obtained by applying Metropolis–Hastings to (14) as proposed by
Christensen et al. [9]. The diﬀerences are that the new latent variables f(cid:48) are computed
using diﬀerent pseudo-posterior means and the surrogate data method has an extra term
for the random  rather than ﬁxed  choice of reparameterization.

The surrogate data sampler is easier to implement than the previous reparameterization
work because the surrogate posterior is centred around the current latent variables. This
means that 1) no point estimate  such as the maximum likelihood ˆf   is required. 2) picking
the noise covariance Sθ poorly may still produce a workable method  whereas a ﬁxed repa-
rameterized can work badly if the true posterior distribution is in the tails of the Gaussian
approximation. Christensen et al. [9] pointed out that centering the approximate Gaus-
sian likelihood in their reparameterization around the current state is tempting  but that
computing the Jacobian of the transformation is then intractable. By construction  the
surrogate data model centers the reparameterization near to the current state.

5 Experiments

We empirically compare the performance of the various approaches to GP hyperparameter
sampling on four data sets: one regression  one classiﬁcation  and two Cox process inference
problems. Further details are in the rest of this section  with full code as supplementary
material. The results are summarized in Figure 3 followed by a discussion section.

6

In each of the experimental conﬁgurations  we ran ten independent chains with diﬀerent
random seeds  burning in for 1000 iterations and sampling for 5000 iterations. We quantify
the mixing of the chain by estimating the eﬀective number of samples of the complete
data likelihood trace using R-CODA [12]  and compare that with three cost metrics: the
number of hyperparameter settings considered (each requiring a small number of covariance
decompositions with O(n3) time complexity)  the number of likelihood evaluations  and the
total elapsed time on a single core of an Intel Xeon 3GHz CPU.

The experiments are designed to test the mixing of hyperparameters θ while sampling from
the joint posterior (3). All of the discussed approaches except Algorithm 1 update the latent
variables f as a side-eﬀect. However  further transition operators for the latent variables for
ﬁxed hyperparameters are required. In Algorithm 2 the “whitened” variables ν remain ﬁxed;
the latent variables and hyperparameters are constrained to satisfy f = LΣθ ν. The surrogate
data samplers are ergodic: the full joint posterior distribution will eventually be explored.
However  each update changes the hyperparameters and requires expensive computations
involving covariances. After computing the covariances for one set of hyperparameters  it
makes sense to apply several cheap updates to the latent variables. For every method we
applied ten updates of elliptical slice sampling [1] to the latent variables f between each
hyperparameter update. One could also consider applying elliptical slice sampling to a
reparameterized representation  for simplicity of comparison we do not. Independently of
our work Titsias [13] has used surrogate data like reparameterizations to update latent
variables for ﬁxed hyperparameters.

Methods We implemented six methods for updating Gaussian covariance hyperparame-
ters. Each method used the same slice sampler  as in Algorithm 4  applied to the following
model representations. ﬁxed: ﬁxing the latent function f [14]. prior-white: whitening
with the prior. surr-site: using surrogate data with the noise level set to match the site
posterior (12). We used Laplace approximations for the Poisson likelihood. For classiﬁ-
cation problems we used moment matching  because Laplace approximations do not work
well [15]. surr-taylor: using surrogate data with noise variance set via Taylor expansion of
the log-likelihood (13). Inﬁnite variances were truncated to a large value. post-taylor and
post-site: as for the surr- methods but a ﬁxed reparameterization based on a posterior
approximation (14).

Binary Classiﬁcation (Ionosphere) We evaluated four diﬀerent methods for perform-
ing binary GP classiﬁcation: fixed  prior-white  surr-site and post-site. We applied
these methods to the Ionosphere dataset [16]  using 200 training data and 34 dimensions.
We used a logistic likelihood with zero-mean prior  inferring lengthscales as well as sig-
nal variance. The -taylor methods reduce to other methods or don’t apply because the
maximum of the log-likelihood is at plus or minus inﬁnity.

Gaussian Regression (Synthetic) When the observations have Gaussian noise the
post-taylor reparameterization of Christensen et al. [9] makes the hyperparameters and
latent variables exactly independent. The random centering of the surrogate data model will
be less eﬀective. We used a Gaussian regression problem to assess how much worse the sur-
rogate data method is compared to an ideal reparameterization. The synthetic data set had
200 input points in 10-D drawn uniformly within a unit hypercube. The GP had zero mean 
unit signal variance and its ten lengthscales in (2) drawn from Uniform(0 
10). Observation
noise had variance 0.09. We applied the fixed  prior-white  surr-site/surr-taylor 
and post-site/post-taylor methods. For Gaussian likelihoods the -site and -taylor
methods coincide: the auxiliary noise matches the observation noise (Sθ = 0.09 I).

√

Cox process inference We tested all six methods on an inhomogeneous Poisson process
with a Gaussian process prior for the log-rate. We sampled the hyperparameters in (2) and
a mean oﬀset to the log-rate. The model was applied to two point process datasets: 1) a
record of mining disasters [17] with 191 events in 112 bins of 365 days. 2) 195 redwood tree
locations in a region scaled to the unit square [18] split into 25×25 = 625 bins. The results
for the mining problem were initially highly variable. As the mining experiments were also
the quickest we re-ran each chain for 20 000 iterations.

7

Figure 3: The results of experimental comparisons of six MCMC methods for GP hyperparameter
inference on four data sets. Each ﬁgure shows four groups of bars (one for each experiment) and the
vertical axis shows the eﬀective number of samples of the complete data likelihood per unit cost.
The costs are per likelihood evaluation (left)  per covariance construction (center)  and per second
(right). Means and standard errors for 10 runs are shown. Each group of bars has been rescaled for
readability: the number beneath each group gives the eﬀective samples for the surr-site method 
which always has bars of height 1. Bars are missing where methods are inapplicable (see text).

6 Discussion

On the Ionosphere classiﬁcation problem both of the -site methods worked much better
than the two baselines. We slightly prefer surr-site as it involves less problem-speciﬁc
derivations than post-site.

On the synthetic test the post- and surr- methods perform very similarly. We had expected
the existing post- method to have an advantage of perhaps up to 2–3×  but that was not
realized on this particular dataset. The post- methods had a slight time advantage  but
this is down to implementation details and is not notable.

On the mining problem the Poisson likelihoods are often close to Gaussian  so the exist-
ing post-taylor approximation works well  as do all of our new proposed methods. The
Gaussian approximations to the Poisson likelihood ﬁt most poorly to sites with zero counts.
The redwood dataset discretizes two-dimensional space  leading to a large number of bins.
The majority of these bins have zero counts  many more than the mining dataset. Taylor
expanding the likelihood gives no likelihood contribution for bins with zero counts  so it
is unsurprising that post-taylor performs similarly to prior-white. While surr-taylor
works better  the best results here come from using approximations to the site-posterior (12).
For unreasonably ﬁne discretizations the results can be diﬀerent again: the site- reparam-
eterizations do not always work well.

Our empirical investigation used slice sampling because it is easy to implement and use.
However  all of the representations we discuss could be combined with any other MCMC
method  such as [19] recently used for Cox processes. The new surrogate data and post-site
representations oﬀer state-of-the-art performance and are the ﬁrst such advanced methods
to be applicable to Gaussian process classiﬁcation.

An important message from our results is that ﬁxing the latent variables and updating
hyperparameters according to the conditional posterior — as commonly used by GP practi-
tioners — can work exceedingly poorly. Even the simple reparameterization of “whitening
the prior” discussed in Section 2.1 works much better on problems where smoothness is
important in the posterior. Even if site approximations are diﬃcult and the more ad-
vanced methods presented are inapplicable  the simple whitening reparameterization should
be given serious consideration when performing MCMC inference of hyperparameters.

Acknowledgements

We thank an anonymous reviewer for useful comments. This work was supported in part
by the IST Programme of the European Community  under the PASCAL2 Network of
Excellence  IST-2007-216886. This publication only reﬂects the authors’ views. RPA is a
junior fellow of the Canadian Institute for Advanced Research.

8

ionospheresyntheticminingredwoods01234Effective samples per likelihood evaluationionospheresyntheticminingredwoods01234Effective samples per covariance construction  fixedprior−whitesurr−sitepost−sitesurr−taylorpost−taylorionospheresyntheticminingredwoods01234Effective samples per secondx1.6e−04   x3.3e−04   x4.3e−05   x4.8e−04   x2.9e−04   x1.1e−03   x7.4e−04   x3.7e−03   x7.7e−03   x5.4e−02   x1.2e−01   x1.5e−02   References

[1] Iain Murray  Ryan Prescott Adams  and David J.C. MacKay. Elliptical slice sampling.
Journal of Machine Learning Research: W&CP  9:541–548  2010. Proceedings of the
13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS).

[2] Radford M. Neal. Slice sampling. Annals of Statistics  31(3):705–767  2003.

[3] Deepak K. Agarwal and Alan E. Gelfand. Slice sampling for simulation based ﬁtting

of spatial data models. Statistics and Computing  15(1):61–69  2005.

[4] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for ma-

chine learning. MIT Press  2006.

[5] Luke Tierney. Markov chains for exploring posterior distributions. The Annals of

Statistics  22(4):1701–1728  1994.

[6] Michalis Titsias  Neil D Lawrence  and Magnus Rattray. Eﬃcient sampling for Gaussian
process inference using control variables. In Advances in Neural Information Processing
Systems 21  pages 1681–1688. MIT Press  2009.

[7] Simon Duane  A. D. Kennedy  Brian J. Pendleton  and Duncan Roweth. Hybrid Monte

Carlo. Physics Letters B  195(2):216–222  September 1987.

[8] Radford M. Neal. MCMC using Hamiltonian dynamics. To appear in the Handbook

of Markov Chain Monte Carlo  Chapman & Hall / CRC Press  2011.
http://www.cs.toronto.edu/~radford/ftp/ham-mcmc.pdf.

[9] Ole F. Christensen  Gareth O. Roberts  and Martin Sk˜ald. Robust Markov chain Monte
Carlo methods for spatial generalized linear mixed models. Journal of Computational
and Graphical Statistics  15(1):1–17  2006.

[10] Thomas Minka. Expectation propagation for approximate Bayesian inference. In Pro-
ceedings of the 17th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI) 
pages 362–369  2001. Corrected version available from
http://research.microsoft.com/~minka/papers/ep/.

[11] Kiam Choo. Learning hyperparameters for neural network models using Hamiltonian
dynamics. Master’s thesis  Department of Computer Science  University of Toronto 
2000. Available from http://www.cs.toronto.edu/~radford/ftp/kiam-thesis.ps.
[12] Mary Kathryn Cowles  Nicky Best  Karen Vines  and Martyn Plummer. R-CODA

0.10-5  2006. http://www-fis.iarc.fr/coda/.

[13] Michalis Titsias. Auxiliary sampling using imaginary data  2010. Unpublished.

[14] Radford M. Neal. Regression and classiﬁcation using Gaussian process priors. In J. M.

Bernardo et al.  editors  Bayesian Statistics 6  pages 475–501. OU Press  1999.

[15] Malte Kuss and Carl Edward Rasmussen. Assessing approximate inference for binary
Gaussian process classiﬁcation. Journal of Machine Learning Research  6:1679–1704 
2005.

[16] V. G. Sigillito  S. P. Wing  L. V. Hutton  and K. B. Baker. Classiﬁcation of radar
returns from the ionosphere using neural networks. Johns Hopkins APL Technical
Digest  10:262–266  1989.

[17] R. G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika  66

(1):191–193  1979.

[18] Brian D. Ripley. Modelling spatial patterns. Journal of the Royal Statistical Society 

Series B  39:172–212  1977.

[19] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian
Monte Carlo methods. Journal of the Royal Statistical Society. Series B (Methodologi-
cal)  2011. To appear.

9

,Magauiya Zhussip
Shakarim Soltanayev
Se Young Chun