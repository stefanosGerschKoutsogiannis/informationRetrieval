2013,Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.,Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these interactions when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal we analyze the properties of the kernel cross-spectral density operator induced by positive definite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions  showing improvement in terms of detection errors as well as the suitability of this approach for testing dependency in complex dynamical systems. Finally  we use this approach to characterize complex interactions in electrophysiological neural time series.,Statistical analysis of coupled time series with Kernel

Cross-Spectral Density operators.

MPI for Intelligent Systems and MPI for Biological Cybernetics  T¨ubingen  Germany

michel.besserve@tuebingen.mpg.de

Michel Besserve

Nikos K. Logothetis

MPI for Biological Cybernetics  T¨ubingen

nikos.logothetis@tuebingen.mpg.de

Bernhard Sch¨olkopf

MPI for Intelligent Systems  T¨ubingen

bs@tuebingen.mpg.de

Abstract

Many applications require the analysis of complex interactions between time se-
ries. These interactions can be non-linear and involve vector valued as well as
complex data structures such as graphs or strings. Here we provide a general
framework for the statistical analysis of these dependencies when random vari-
ables are sampled from stationary time-series of arbitrary objects. To achieve this
goal  we study the properties of the Kernel Cross-Spectral Density (KCSD) oper-
ator induced by positive deﬁnite kernels on arbitrary input domains. This frame-
work enables us to develop an independence test between time series  as well as a
similarity measure to compare different types of coupling. The performance of our
test is compared to the HSIC test using i.i.d. assumptions  showing improvements
in terms of detection errors  as well as the suitability of this approach for testing
dependency in complex dynamical systems. This similarity measure enables us to
identify different types of interactions in electrophysiological neural time series.

1

Introduction

Complex dynamical systems can often be observed by monitoring time series of one or more vari-
ables. Finding and characterizing dependencies between several of these time series is key to un-
derstand the underlying mechanisms of these systems. This problem can be addressed easily in
linear systems [4]  however non-linear systems are much more challenging. Whereas higher order
statistics can provide helpful tools in speciﬁc contexts [15]  and have been extensively used in sys-
tem identiﬁcation  causal inference and blind source separation (see for example [10  13  5]); it is
difﬁcult to derive a general approach with solid theoretical results accounting for a broad range of
interactions. Especially  studying the relationships between time series of arbitrary objects such as
texts or graphs within a general framework is largely unaddressed.
On the other hand  the dependency between independent identically distributed (i.i.d.) samples
of arbitrary objects can be studied elegantly in the framework of positive deﬁnite kernels [19]. It
relies on deﬁning cross-covariance operators between variables mapped implicitly to Reproducing
Kernel Hilbert Spaces (RKHS) [7]. It has been shown that when using a characteristic kernel for
the mapping [9]  the properties of RKHS operators are related to statistical independence between
input variables and allow testing for it in a principled way with the Hilbert-Schmidt Independence
Criterion (HSIC) test [11]. However  the suitability of this test relies heavily on the assumption
that i.i.d. samples of random variables are used. This assumption is obviously violated in any non-
trivial setting involving time series  and as a consequence trying to use HSIC in this context can
lead to incorrect conclusions. Zhang et al. established a framework in the context of Markov chains

1

[22]  showing that a structured HSIC test still provides good asymptotic properties for absolutely
regular processes. However  this methodology has not been assessed extensively in empirical time
series. Moreover  beyond the detection of interactions  it is important to be able to characterize the
nature of the coupling between time series. It was recently suggested that generalizing the concept
of cross-spectral density to Reproducible Kernel Hilbert Spaces (RKHS) could help formulate non-
linear dependency measures for time series [2]. However  no statistical assessment of this measure
has been established. In this paper  after recalling the concept of kernel spectral density operator 
we characterize its statistical properties. In particular  we deﬁne independence tests based on this
concept as well as a similarity measure to compare different types of couplings. We use these tests
in section 4 to compute the statistical dependencies between simulated time series of various types
of objects  as well as recordings of neural activity in the visual cortex of non-human primates. We
show that our technique reliably detects complex interactions and provides a characterization of
these interactions in the frequency domain.

2 Background and notations

to x = ki(.  x) ∈ Hi  such that ∀f ∈ Hi  f (x) = (cid:10)f  x(cid:11)

Random variables in Reproducing Kernel Hilbert Spaces
Let X1 and X2 be two (possibly non vectorial) input domains. Let k1(.  .) : X1 × X1 → C and
k2(.  .) : X2 × X2 → C be two positive deﬁnite kernels  associated to two separable Hilbert spaces
of functions  H1 and H2 respectively. For i ∈ {1  2}  they deﬁne a canonical mapping from x ∈ Xi
(see [19] for more details). In the
same way  this mapping can be extended to random variables  so that the random variable Xi ∈ Xi
is mapped to the random element Xi ∈ Hi. Statistical objects extending the classical mean and
covariance to random variables in the RKHS are deﬁned as follows:

Hi

• the Mean Element (see [1  3]): µi = E [Xi]  
• the Cross-covariance operator (see [6]): Cij = Cov [Xi  Xj] = E[Xi ⊗ X∗

f ⊗ g∗ =(cid:10)g  .(cid:11)f (following [3]). As a consequence  the cross-covariance can be seen as an operator

j ] − µi ⊗ µ∗
j  
where we use the tensor product notation f ⊗ g∗ to represent the rank one operator deﬁned by
in L(Hj Hi)  the Hilbert space of linear Hilbert-Schmidt operators from Hj to Hi (isomorphic to
Hi ⊗ H∗
j ). Interestingly  the link between Cij and covariance in the input domains is given by the
Hilbert-Schmidt scalar product

HS = Cov [fi(Xi)  fj(Xj)]  

∀(fi  fj) ∈ Hi ⊗ Hj

(cid:10)Cij  fi ⊗ f∗

j

(cid:11)

Moreover  the Hilbert-Schmidt norm of the operator in this space has been proved to be a measure of
independence between two random variables  whenever kernels are characteristic [11]. Extension of
this result has been provided in [22] for Markov chains. If the time series are assumed to be k-order
Markovian  then results of the classical HSIC can be generalized for a structured HSIC using uni-
versal kernels based on the state vectors (x1(t)  . . .   x1(t + k)  x2(t)  . . .   x2(t + k)). The statistical
performance of this methodology has not been studied extensively  in particular its sensitivity to the
dimension of the state vector. The following sections propose an alternative methodology.

Kernel Cross-Spectral Density operator
Consider a bivariate discrete time random process on X1 × X2 : {(X1(t)  X2(t))}t∈Z. We assume
stationarity of the process and thus use the following translation invariant notations for the mean
elements and cross-covariance operators:

EXi(t) = µi  Cov [Xi(t + τ )  Xj(t)] = Cij(τ )

The cross-spectral density operator was introduced for stationary signals in [2] based on second
order cumulants. Under mild assumptions  it is a Hilbert-Schmidt operator deﬁned for all normalized
frequencies ν ∈ [0 ; 1] as:

(cid:88)

k∈Z

S12(ν) =

(cid:88)

k∈Z

C12(k) exp(−k2πν) =

2

C12(k)z−k  for z = e2πiν.

This object summarizes all
the cross-spectral properties between the families of processes
{f (X1)}f∈H1 and {g(X2)}g∈H2 in the sense that the cross-spectrum between f (X1) and g(X2)
is given by Sf g
Density operator (KCSD).

12 (ν) = (cid:10)f  S12g(cid:11). We therefore refer to this object as the Kernel Cross-Spectral

3 Statistical properties of KCSD

Measuring independence with the KCSD

One interesting characteristic of the KCSD is given by the following theorem [2]:
Theorem 1. Assume the kernels k1 and k2 are characteristic [9]. The processes X1 and X2 are
pairwise independent (i.e. for all integers t and t’  X1(t) and X2(t(cid:48)) are independent)  if and only

if(cid:13)(cid:13)S12(ν)(cid:13)(cid:13)HS = 0  ∀ν ∈ [0   1].

While this theorem states that KCSD can be used to test pairwise independence between time series 
it does not imply independence between arbitrary sets of random variables taken from each time
series in general. However  if the joint probability distribution of the time series is encoded by a
Directed Acyclic Graph (DAG)  the following Theorem shows that independence in this broader
sense is achieved under mild assumptions.
Proposition 2. If the joint probability distribution of time series is encoded by a DAG with no
confounder under the Markov property and faithfulness assumption  pairwise independence between
time series implies the mutual independence relationship {X1(t)}t∈Z ⊥⊥ {X2(t)}t∈Z.
Proof. The proof uses the fact that the faithfulness and Markov property assumptions provide an
equivalence between the independence of two sets of random variables and the d-separation of the
corresponding sets of nodes in the DAG (see [17]). We start by assuming pairwise independence
between the time series.
For arbitrary times t and t(cid:48)  assume the DAG contains an arrow linking the nodes X1(t) and X2(t(cid:48)).
This is an unblocked path linking this two nodes; thus they are not d-separated. As a consequence of
faithfulness  X1(t) and X2(t(cid:48)) are not independent. Since this contradicts our initial assumptions 
there cannot exist any arrow between X1(t) and X2(t(cid:48)).
Since this holds for all t and t(cid:48)  there is no path linking the nodes of each time series and we have
{X1(t)}t∈Z ⊥⊥ {X2(t)}t∈Z according to the Markov property (any joint probability distribution on
the nodes will factorize in two terms  one for each time series).
As a consequence  the use of KCSD to test for independence is justiﬁed under the widely used
faithfulness and Markov assumptions of graphical models. As a comparison  the structured HSIC
proposed in [22] is theoretically able to capture all dependencies within the range of k samples by
assuming k-order Markovian time series.

Fourth order kernel cumulant operator

Statistical properties of KCSD require assumptions regarding the higher order statistics of the time
series. Analogously to covariance  higher order statistics can be generalized as operators in (tensor
products of) RKHSs. An important example in our setting is the joint quadricumulant (4th order
cumulant) (see [4]). We skip the general expression of this cumulant to focus on its simpliﬁed form
for four centered scalar random variables:

κ(X1  X2  X3  X4) = E[X1X2X3X4] − E[X1X2]E[X3X4] − E[X1X3]E[X2X4]

− E[X1X4]E[X2X3]

(1)
This object can be generalized to the case random variables mapped in two RKHSs. The quadricu-
mulant operator K1234 is a linear operator in the Hilbert space L(H1 ⊗ H∗
2)  such that

erties of this operator will be useful in the next sections due to the following lemma.
Lemma 3. [Property of the tensor quadricumulant] Let Xc
the Hilbert space H1 and Xc
deﬁned by Xc
1  Xc
3

(cid:11)  for arbitrary elements fi. The prop-
3 be centered random elements in
4 centered random elements in H2 (the centered random element is
(cid:11)
(cid:11) + Tr C1 3 Tr C2 4 +(cid:10)C1 4  C3 2

κ(f1(X1) f2(X2) f3(X3) f4(X4)) =(cid:10)f1⊗f∗
E(cid:2)(cid:10)Xc

(cid:11)
(cid:11)
2  Xc
i = Xj − µj)  then
H2

(cid:3) = TrK1234 +(cid:10)C1 2  C3 4

2  K1234f3⊗f∗

4

2 H1 ⊗ H∗

(cid:10)Xc

H1

2  Xc
4

1  Xc

3

In the case of two jointly stationary time series  we deﬁne the translation invariant quadricumulant
between the two stationary time series as:

K12(τ1  τ2  τ3) = K1234(X1(t + τ1)  X2(t + τ2)  X1(t + τ3)  X2(t))

Estimation with the Kernel Periodogram

In the following  we address the problem of estimating the properties of cross-spectral density oper-
ators from ﬁnite samples. The idea for doing this analytically is to select samples from a time-series
with a tapering window function w : R (cid:55)→ R with a support included in [0  1]. By scaling this
window according to wT (k) = w(k/T )  and multiplying it with the time series  T samples of the
sequence can be selected. The windowed periodogram estimate of the KCSD operator for T succes-
sive samples of the time series is

PT

1

T (cid:107)w(cid:107)2FT [Xc

1](ν)⊗FT [Xc
2](ν)∗ 
ˆ
12(ν) =
i (k) = Xi(k) − µi and (cid:107)w(cid:107)2 =

1

with Xc

w2(t)dt

0

1] =(cid:80)T

k=1wT (k)(Xc

where FT [Xc
1(k))z−k  for z = e2πiν  is the windowed Fourier transform of
the delayed time series in the RKHS. Properties of the windowed Fourier transform are related to
the regularity of the tapering window. In particular  we will chose a tapering window of bounded
variation. In such a case  the following lemma holds (see supplementary material for the proof).
Lemma 4. [A property of bounded variation functions] Let w be a bounded function of bounded

Using this assumption  the above periodogram estimate is asymptotically unbiased as shown in the
following theorem
k∈Z |k|(cid:107)C12(k)(cid:107)HS < +∞ 

t=−∞wT (t + k)w(t) −(cid:80)+∞

variation then for all k (cid:12)(cid:12)(cid:80)+∞
t=−∞wT (t)2(cid:12)(cid:12) ≤ C|k|
Theorem 5. Let w be a bounded function of bounded variation  if(cid:80)
(cid:12)(cid:12)Tr [K12(k  i  j)](cid:12)(cid:12) < +∞ 
(cid:80)
k∈Z |k| Tr(Cii(k)) < +∞ and(cid:80)
2(n)z−n(cid:1)∗

1(k)z−k(cid:1)⊗(cid:0)(cid:80)

12(ν) = S12(ν)  ν (cid:54)≡ 0

(k i j)∈Z3
E PT

Proof. By deﬁnition 

12(z) = 1

(mod 1/2)

PT

n∈Z zn−kwT (k)wT (n)Xc

n∈Z wT (n)Xc
1(k)⊗Xc

2(n)∗

= 1

= 1

T(cid:107)w(cid:107)2

T(cid:107)w(cid:107)2

T→+∞

then lim

k∈Z wT (k)Xc

(cid:0)(cid:80)
(cid:80)
k∈Z(cid:80)
δ∈Z z−δ(cid:80)
(cid:80)
(cid:80)
δ∈Z z−δ((cid:80)
)(cid:80)
= 1(cid:107)w(cid:107)2 ((cid:80)

n∈Z wT (n)2

T(cid:107)w(cid:107)2

T

Thus using Lemma 4 
E PT

12(z) = 1

T(cid:107)w(cid:107)2

n∈Z wT (n + δ)wT (n)Xc

1(n + δ)⊗Xc

2(n)∗   using δ = k − n.

n∈Z wT (n)2 + O(|δ|))C12(δ)
δ∈Z z−δC12(δ)+ 1

T O((cid:80)

δ∈Z |δ|(cid:13)(cid:13)C12(δ)(cid:13)(cid:13)HS) →

T→+∞ S12.

However  the squared Hilbert-Schmidt norm of PT
population KCSD squared norm according to the following theorem.
Theorem 6. Under the assumptions of Theorem 5  for ν (cid:54)≡ 0 (mod 1/2)

E(cid:13)(cid:13)PT
12(ν)(cid:13)(cid:13)2

HS =(cid:13)(cid:13)S12(ν)(cid:13)(cid:13)2

lim

T→+∞

HS + Tr(S11(ν)) Tr(S22(ν))

12(ν) is an asymptotically biased estimator of the

The proof of Theorem 5 is based on the decomposition in Lemma 3 and is provided in supplementary
information.
This estimate requires speciﬁc bias estimation techniques to develop an independence test  we will
call it the biased estimate of the KCSD squared norm. Having the KCSD deﬁned in an Hilbert space
also enables to deﬁne similarity between two KCSD operators  so that it is possible to compare
quantitatively whether different dynamical systems have similar couplings. The following theorem
shows how periodograms enable to estimate the scalar product between two KCSD operators  which
reﬂects their similarity.

4

Theorem 7. Assume assumptions of Theorem 5 hold for two independent samples of bivariate
time series{(X1(t)  X2(t))}t=... −1 0 1 ... and {(X3(t)  X4(t))}t=... −1 0 1 ...  mapped with the same
couple of reproducing kernels.

E(cid:10)PT

34(ν)(cid:11)

HS =(cid:10)S12(ν)  S34(ν)(cid:11)

Then

lim

T→+∞

12(ν)  PT

HS  ν (cid:54)≡ 0

(mod 1/2)

The proof of Theorem 7 is similar to the one of Theorem 6 provided as supplemental information.
Interestingly  this estimate of the scalar product between KCSD operators is unbiased. This comes
from the assumption that the two bivariate series are independent. This provides a new opportunity
to estimate the Hilbert-Schmidt norm as well  in case two independent samples of the same bivariate
series are available.
Corollary 8. Assume assumptions of Theorem 5 hold for
the bivariate time series
{(X1(t)  X2(t))}t∈Z and assume {( ˜X1(t)  ˜X2(t))}t∈Zan independent copy of the same time series 
providing the periodogram estimates PT

12(ν) and ˜PT

E(cid:10)PT

12(ν)(cid:11)

HS =(cid:13)(cid:13)S12(ν)(cid:13)(cid:13)2

12(ν)  respectively.
HS  ν (cid:54)≡ 0

Then

lim

T→+∞

12(ν)  ˜PT

(mod 1/2)

In many experimental settings  such as in neuroscience  it is possible to measure the same time series
in several independent trials. In such a case  corollary 8 states that estimating the Hilbert-Schmidt
norm of the KCSD without bias is possible using two intependent trials. We will call this estimate
the unbiased estimate of the KCSD squared norm.
These estimate can be computed efﬁciently for T equispaced frequency samples using the fast
Fourier transform of the centered kernel matrices of the two time series.
In general  the choice
of the kernel is a trade-off between the capacity to capture complex dependencies (a character-
istic kernel being better in this respect)  and the convergence rate of the estimate (simpler ker-
nels related to lower order statistics usually require less samples). Related theoretical analy-
sis can be found in [8  12]. Unless otherwise stated  the Gaussian RBF kernel with bandwidth
parameter σ  k(x  y) = exp((cid:107)x − y(cid:107)2 /2σ2)  will be used as a characteristic kernel for vec-
tor spaces. Let Kij denote the kernel matrix between the i-th and j-th time series (such that
(Kij)k l = k(xi(k)  xj(l)))  W the windowing matrix (such that (W)k l = wT (k)wT (l)) and
M be the centering matrix M = I − 1T 1T
T /T   then we can deﬁne the windowed centered kernel
matrices ˜Kij = (MKijM) ◦ W. Deﬁning the Discrete Fourier Transform matrix F  such that
√
(F)k l = exp(−i2πkl/T )/

ν=(0 1 ... (T−1))/T = (cid:107)w(cid:107)−4 diag(cid:0)F ˜K13F−1(cid:1) ◦ diag(cid:0)F−1 ˜K24F(cid:1) 

T   the estimated scalar product is

(cid:10)PT

12  PT
34

(cid:11)

which can be efﬁciently computed using the Fast Fourier Transform (◦ is the Hadamard product).
The biased and unbiased squared norm estimates can be trivially retrieved from the above expression.

Shufﬂing independence tests

According to Theorem 1  pairwise independence between time series requires the cross-spectral
density operator to be zero for all frequencies. We can thus test independence by testing whether
the Hilbert-Schmidt norm of the operator vanishes for each frequency. We rely on Theorem 6 and
Corollary 8 to compute biased and unbiased estimates of this norm. To achieve this  we generate
a distribution of the Hilbert-Schmidt norm statistics under the null hypothesis by cutting the time
interval in non-overlapping blocks and matching the blocks of each time series in pairs at random.
Due to the central limit theorem  for a sufﬁciently large number of time windows  the empirical
average of the statistics approaches a Gaussian distribution. We thus test whether the empirical
mean differs from the one under the null distribution using a t-statistic. To prevent false positive
resulting from multiple hypothesis testing  we control the Family-wise Error Rate (FWER) of the
tests performed for each frequency. Following [16]  we estimate a global maximum distribution on
the family of t-statistics across frequencies under the null hypothesis  and use the percentile of this
distribution to assess the signiﬁcance of the original t-statistics.

5

Figure 1: Results for the phase-amplitude coupling system. Top-left: example time course. Top-
middle: estimate of the KCSD squared norm with a linear kernel. Top-right: estimate of the KCSD
squared norm with an RBF kernel. Bottom-left: performance of the biased kcsd test as a function of
number of samples. Bottom-middle: performance of the unbiased kcsd test as a function of number
of samples. Bottom-right: Rate of type I and type II errors for several independence tests.

4 Experiments

In the following  we validate the performance of our test  called kcsd  on several datasets in the
biased and unbiased case. There is no general time series analysis tool in the literature to compare
with our approach on all these datasets. So our main source of comparison will be the HSIC test of
independence (assuming data is i.i.d.). This enables us  to compare both approaches using the same
kernels. For vector data  one can compare the performance of our approach with a linear dependency
measure: we do this by implementing our test using a linear kernel (instead of an RBF kernel)  and
we call it linear kscd. Finally  we use the alternative approach of structured HSIC [22] by cutting
the time series in time windows (using the same approach as our independence test) and considering
each of them as a single multivariate sample. This will be called block hsic. The bandwidth of the
HSIC methods is chosen proportional to the median norm of the sample points in the vector space.
The p-value for all independence tests will be set to 5%.

Phase amplitude coupling

We ﬁrst simulate a non-linear dependency between two time series by generating two oscillations at
frequencies f1 and f2   and introducing a modulation of the amplitude of the second oscillation by
the phase of the ﬁrst one. This is achieved using the following discrete time equations:

(cid:26) ϕ1(k + 1) = ϕ1(k) + .11(k) + 2πf1Ts

(cid:26) x1(k) =

cos(ϕ1(k))

ϕ2(k + 1) = ϕ2(k) + .12(k) + 2πf2Ts

x2(k) = (2 + C sin ϕ1(k)) cos(ϕ2(k))

Where the i are i.i.d normal. A simulation with f1 = 4Hz and f2 = 20Hz for a sampling fre-
quency 1/Ts=100Hz is plotted on Figure 1 (top-left panel). For the parameters of the periodogram 
we used a window length of 50 samples (.5 s). We used a Gaussian RBF kernel to compute non-
linear dependencies between the two time series after standardizing each of them (divide them by
their standard deviation). The top-middle and top-right panels of Figure 1 plot the mean and stan-
dard errors of the estimate of the squared Hilbert-Schmidt norm for this system (for C = .1) for a
linear and a Gaussian RBF kernel (with σ = 1) respectively. The bias of the ﬁrst estimate appears
clearly in both cases at the two power picks of the signals for the biased estimate. In the second
(unbiased) estimate  the spectrum exhibits a zero mean for all but one peak (at 4Hz for the RBF
kernel)  which corresponds to the expected frequency of non-linear interaction between the time
series. The observed negative values are also a direct consequence of the unbiased property of our
estimate (Corollary 8). The inﬂuence of the bandwidth parameter of the kernel was studied in the
case of weakly coupled time series (C = .4 ). The bottom left and middle panels of Figure 1 show

6

frequency (Hz)biasedunbiasedSquared norm estimate: linear kernelfrequency (Hz)biasedunbiasedSquared norm estimate: RBF kernel00.511.5−4−202420406080100block hsichsiclinear kcsdkcsd020406080100type I (C=0)type II (C=.4)type II (C=2)error rate (%)time (s)number of samplesnumber of dependencies detected (%)Detection probability for biased kcsdnumber of samples10210300.20.40.60.81Detection probability for unbiased kcsdnumber of dependencies detected (%)10210300.20.40.60.81linearrbfσ=.1.2.631.53.910051015202530−0.100.10.20.3051015202530−0.0500.050.10.15Figure 2: Markov chain dynamical system. Upper left: Markov transition probabilities  ﬂuctuating
between the values indicated in both graphs. Upper right: example of simulated time series. Bottom
left: the biased and unbiased KCSD norm estimates in the frequency domain. Bottom right: type I
and type II errors for hsic and kcsd tests

the inﬂuence of this parameter on the number of samples required to actually reject the null hypoth-
esis and detect the dependency for biased and unbiased estimates respectively. It was observed that
choosing an hyper-parameter close to the standard deviation of the signal (here 1.5) was an optimal
strategy  and that the test relying on the unbiased estimate outperformed the biased estimate. We
thus used the unbiased estimate in our subsequent analysis. The coupling parameter C was further
varied to test the performance of independence tests both in case the null hypothesis of indepen-
dence is true (C=0)  and when it should be rejected (C = .4 for weak coupling  C = 2 for strong
coupling). These two settings enable to quantify the type I and type II error of the tests  respectively.
The bottom-right panel of Figure 1 reports these errors for several independence tests. Showing the
superiority of our method especially for type II errors. In particular  methods based on HSIC fail to
detect weak dependencies in the time series.

Time varying Markov chain

We now illustrate the use of our test in an hybrid setting. We generate a symbolic time series x2
using the alphabet S = [1  2  3]  controlled by a scalar time series x1. The coupling is achieved by
modulating across time the transition probabilities of the Markov transition matrix generating the
symbolic time series x2 using the current value of the scalar time series x1 . This model is described
by the following equations with f1 = 1Hz.

(cid:40)

= ϕ1(k) + .11(k) + 2πf1Ts
=
p(x2(k + 1) = Si|x2(k) = Sj) =

ϕ1(k + 1)
x1(k + 1)

sin(ϕ1(k + 1))

Mij + ∆Mijx1(k)

Since x1 is bounded between -1 and 1  the Markov transition matrix ﬂuctuates across time between
two models represented Figure 2 (top-left panel). A model without these ﬂuctuations (∆M = 0)
was simulated as well to measure type I error. The time course of such an hybrid system is illustrated
on the top-right panel of the same ﬁgure. In order to measure the dependency between these two
time series  we use a k-spectrum kernel [14] for x2 and a RBF kernel for x1 . For the k-spectrum
kernel  we use k=2 (using k=1  i.e. counting occurrences of single symbols was less efﬁcient) and
we computed the kernel between words of 3 successive symbols of the time series. We used an RBF
kernel with σ = 1  decimated the signals by a factor 2 and signals were cut in time windows of 100
samples. The biased and unbiased estimates of the KCSD norm are represented at the bottom-left
of Figure 2 and show a clear peak at the modulating frequency (1Hz). The independence test results
shown at the bottom-right of Figure 2 illustrate again the superiority of KCSD for type II error 
whereas type I error stays in an acceptable range.

7

123.1.1.1.2.4.7.5.7.2123.1.1.1.2.6.01.3.7.89Transition probabilities01234−0.500.5state 1state 2state 3time (s)block hsichsickcsd020406080100error rate (%)type I errortype II errorbiasedunbiasedfrequency (Hz)KCSD norm estimate0.10.20.5125102005101520Figure 3: Left: Experimental setup of LFP recordings in anesthetized monkey during visual stim-
ulation with a movie. Right: Proportion of detected dependencies for the unbiased kcsd test of
interactions between Gamma band and wide band LFP for different kernels.

Neural data: local ﬁeld potentials from monkey visual cortex

We analyzed dependencies between local ﬁeld potential (LFP) time series recorded in the primary
visual cortex of one anesthetized monkey during visual stimulation by a commercial movie (see
Figure 3 for a scheme of the experiment). LFP activity reﬂects the non-linear interplay between a
large variety of underlying mechanisms. Here we investigate this interplay by extracting LFP activity
in two frequency bands within the same electrode and quantify the non-linear interactions between
them with our approach. LFPs were ﬁltered into two frequency bands: 1/ a wide band ranging from
1 to 100Hz which contains a rich variety of rhythms and 2/ a high gamma band ranging from 60 to
100Hz which as been shown to play a role in the processing of visual information.
Both of these time series were sampled at 1000Hz. Using non-overlapping time windows of 1s
points  we computed the Hilbert-Schmidt norm of the KCSD operator between gamma and large
band time series originating from the same electrode. We performed statistical testing for all fre-
quencies between 1 and 500Hz (using a Fourier transform on 2048 points). The results of the test
averaged over all recording sites is plotted on Figure 3. We observe a highly reliable detection of
interactions in the gamma band  using either a linear or non-linear kernel. This is due to the fact
that the Gamma band LFP is a ﬁltered version of the wide band LFP  making these signals highly
correlated in the Gamma band. However  in addition to this obvious linear dependency  we observe
signiﬁcant interactions in the lowest frequencies (0.5-2Hz) which can not be explained by linear in-
teraction (and is thus not detected by the linear kernel). This characteristic illustrates the non-linear
interaction between the high frequency gamma rhythm and other lower frequencies of the brain elec-
trical activity  which has been reported in other studies [21]. This also shows the interpretability of
our approach as a test of non-linear dependency in the frequency domain.

5 Conclusion

An independence test for time series based on the concept of Kernel Cross Spectral Density estima-
tion was introduced in this paper. It generalizes the linear approach based on the Fourier transform
in several respects. First  it allows quantiﬁcation of non-linear interactions for time series living
in vector spaces. Moreover  it can measure dependencies between more complex objects  includ-
ing sequences in an arbitrary alphabet  or graphs  as long as an appropriate positive deﬁnite kernel
can be deﬁned in the space of each time series. This paper provides asymptotic properties of the
KCSD estimates  as well as an efﬁcient approach to compute them on real data. The space of KCSD
operators constitutes a very general framework to analyze dependencies in multivariate and highly
structured dynamical systems. Following [13  18]  our independence test can further be combined
to recent developments in kernel time series prediction techniques [20] to deﬁne general and reliable
multivariate causal inference techniques.

Acknowledgments. MB is grateful to Dominik Janzing for fruitful discussions and advice.

8

References
[1] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.

Kluwer Academic Boston  2004.

[2] M. Besserve  D. Janzing  N. Logothetis  and B. Sch¨olkopf. Finding dependencies between frequencies
with the kernel cross-spectral density. In IEEE International Conference on Acoustics  Speech and Signal
Processing  pages 2080–2083  2011.

[3] G. Blanchard  O. Bousquet  and L. Zwald. Statistical properties of kernel principal component analysis.

Machine Learning  66(2-3):259–294  2007.

[4] D. Brillinger. Time series: data analysis and theory. Holt  Rinehart  and Winston  New York  1974.

[5] J.-F. Cardoso. High-order contrasts for independent component analysis. Neural computation  11(1):157–

192  1999.

[6] K. Fukumizu  F. Bach  and A. Gretton. Statistical convergence of kernel CCA. In Advances in Neural

Information Processing Systems 18  pages 387–394  2006.

[7] K. Fukumizu  F. Bach  and M. Jordan. Dimensionality reduction for supervised learning with reproducing

kernel Hilbert spaces. J. Mach. Learn. Res.  5:73–99  2004.

[8] K. Fukumizu  A. Gretton  G. R. Lanckriet  B. Sch¨olkopf  and B. K. Sriperumbudur. Kernel choice and
In Advances in Neural Information

classiﬁability for RKHS embeddings of probability distributions.
Processing Systems 21  pages 1750–1758  2009.

[9] K. Fukumizu  A. Gretton  X. Sun  and B. Sch¨olkopf. Kernel Measures of Conditional Dependence. In

Advances in Neural Information Processing Systems 20  pages 489–496  2008.

[10] G. B. Giannakis and J. M. Mendel.

Identiﬁcation of nonminimum phase systems using higher order

statistics. Acoustics  Speech and Signal Processing  IEEE Transactions on  37(3):360–377  1989.

[11] A. Gretton  K. Fukumizu  C. Teo  L. Song  B. Sch¨olkopf  and A. Smola. A kernel statistical test of

independence. In Advances in Neural Information Processing Systems 20  pages 585–592. 2008.

[12] A. Gretton  D. Sejdinovic  H. Strathmann  S. Balakrishnan  M. Pontil  K. Fukumizu  and B. K. Sripe-
rumbudur. Optimal kernel choice for large-scale two-sample tests. In Advances in Neural Information
Processing Systems 25  pages 1214–1222  2012.

[13] A. Hyv¨arinen  S. Shimizu  and P. O. Hoyer. Causal modelling combining instantaneous and lagged effects:
an identiﬁable model based on non-gaussianity. In Proceedings of the 25th International Conference on
Machine Learning  pages 424–431. ACM  2008.

[14] C. Leslie  E. Eskin  and W. Noble. The spectrum kernel: a string kernel for SVM protein classiﬁcation.

In Pac Symp Biocomput.  2002.

[15] C. Nikias and A. Petropulu. Higher-Order Spectra Analysis - A Non-linear Signal Processing Framework.

Prentice-Hall PTR  Englewood Cliffs  NJ  1993.

[16] D. Pantazis  T. Nichols  S. Baillet  and R. Leahy. A comparison of random ﬁeld theory and permutation

methods for the statistical analysis of MEG data. NeuroImage  25:383 – 394  2005.

[17] J. Pearl. Causality - Models  Reasoning  and Inference. Cambridge University Press  Cambridge  UK 

2000.

[18] J. Peters  D. Janzing  and B. Sch¨olkopf. Causal inference on time series using structural equation models.

In Advances in Neural Information Processing Systems 26  2013.

[19] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.

[20] V. Sindhwani  H. Q. Minh  and A. C. Lozano. Scalable matrix-valued kernel learning for high-dimensional
nonlinear multivariate regression and granger causality. In Proceedings of the 29th Conference on Uncer-
tainty in Artiﬁcial Intelligence  2013.

[21] K. Whittingstall and N. K. Logothetis. Frequency-band coupling in surface EEG reﬂects spiking activity

in monkey visual cortex. Neuron  64:281–9  2009.

[22] X. Zhang  L. Song  A. Gretton  and A. Smola. Kernel Measures of Independence for Non-IID Data. In

Advances in Neural Information Processing Systems 21  pages 1937–1944  2009.

9

,Michel Besserve
Nikos Logothetis
Bernhard Schölkopf
Pan Xu
Jian Ma
Quanquan Gu