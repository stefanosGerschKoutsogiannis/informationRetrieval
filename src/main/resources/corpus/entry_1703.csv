2013,Efficient Exploration and Value Function Generalization in Deterministic Systems,We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system  and as a solution propose optimistic constraint propagation (OCP)  an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within the given hypothesis class  OCP selects optimal actions over all but at most K episodes  where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis space  for  the special case where the hypothesis space is the span of pre-specified indicator functions over disjoint sets.,Efﬁcient Exploration and Value Function
Generalization in Deterministic Systems

Zheng Wen

Stanford University

zhengwen@stanford.edu

Benjamin Van Roy
Stanford University

bvr@stanford.edu

Abstract

We consider the problem of reinforcement learning over episodes of a ﬁnite-
horizon deterministic system and as a solution propose optimistic constraint prop-
agation (OCP)  an algorithm designed to synthesize efﬁcient exploration and
value function generalization. We establish that when the true value function Q⇤
lies within the hypothesis class Q  OCP selects optimal actions over all but at most
dimE[Q] episodes  where dimE denotes the eluder dimension. We establish fur-
ther efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does
not lie in Q  for the special case where Q is the span of pre-speciﬁed indicator
functions over disjoint sets.

1

Introduction

A growing body of work on efﬁcient reinforcement learning provides algorithms with guarantees
on sample and computational efﬁciency [13  6  2  22  4  9]. This literature highlights the point that
an effective exploration scheme is critical to the design of any efﬁcient reinforcement learning al-
gorithm. In particular  popular exploration schemes such as ✏-greedy  Boltzmann  and knowledge
gradient can require learning times that grow exponentially in the number of states and/or the plan-
ning horizon.
The aforementioned literature focuses on tabula rasa learning; that is  algorithms aim to learn with
little or no prior knowledge about transition probabilities and rewards. Such algorithms require
learning times that grow at least linearly with the number of states. Despite the valuable insights
that have been generated through their design and analysis  these algorithms are of limited practical
import because state spaces in most contexts of practical interest are enormous. There is a need for
algorithms that generalize from past experience in order to learn how to make effective decisions in
reasonable time.
There has been much work on reinforcement learning algorithms that generalize (see  e.g. 
[5  23  24  18] and references therein). Most of these algorithms do not come with statistical or
computational efﬁciency guarantees  though there are a few noteworthy exceptions  which we now
discuss. A number of results treat policy-based algorithms (see [10  3] and references therein)  in
which the goal is to select high-performers among a pre-speciﬁed collection of policies as learn-
ing progresses. Though interesting results have been produced in this line of work  each entails
quite restrictive assumptions or does not make strong guarantees. Another body of work focuses
on model-based algorithms. An algorithm is proposed in [12] that ﬁts a factored model to observed
data and makes decisions based on the ﬁtted model. The authors establish a sample complexity
bound that is polynomial in the number of model parameters rather than the number of states  but
the algorithm is computationally intractable because of the difﬁculty of solving factored MDPs. A
recent paper [14] proposes a novel algorithm for the case where the true environment is known to
belong to a ﬁnite or compact class of models  and shows that its sample complexity is polynomial
in the cardinality of the model class if the model class is ﬁnite  or the ✏-covering-number if the

1

model class is compact. Though this result is theoretically interesting  for most model classes of
interest  the ✏-covering-number is enormous since it typically grows exponentially in the number of
free parameters. Another recent paper [17] establishes a regret bound for an algorithm that applies
to problems with continuous state spaces and H¨older-continuous rewards and transition kernels.
Though the results represent an interesting contribution to the literature  a couple features of the
regret bound weaken its practical implications. First  regret grows linearly with the H¨older constant
of the transition kernel  which for most contexts of practical relevance grows exponentially in the
number of state variables. Second  the dependence on time becomes arbitrarily close to linear as the
dimension of the state space grows. Reinforcement learning in linear systems with quadratic cost
is treated in [1]. The method proposed is shown to realize regret that grows with the square root
of time. The result is interesting and the property is desirable  but to the best of our knowledge 
expressions derived for regret in the analysis exhibit an exponential dependence on the number of
state variables  and further  we are not aware of a computationally efﬁcient way of implementing the
proposed method. This work was extended by [8] to address linear systems with sparse structure.
Here  there are efﬁciency guarantees that scale gracefully with the number of state variables  but
only under sparsity and other technical assumptions.
The most popular approach to generalization in the applied reinforcement learning literature involves
ﬁtting parameterized value functions. Such approaches relate closely to supervised learning in that
they learn functions from state to value  though a difference is that value is inﬂuenced by action
and observed only through delayed feedback. One advantage over model learning approaches is
that  given a ﬁtted value function  decisions can be made without solving a potentially intractable
control problem. We see this as a promising direction  though there currently is a lack of theoretical
results that provide attractive bounds on learning time with value function generalization. A relevant
paper along this research line is [15]  which studies the efﬁcient reinforcement learning with value
function generalization in the KWIK framework (see [16])  and reduces the efﬁcient reinforcement
learning problem to the efﬁcient KWIK online regression problem. However  the authors do not
show how to solve the general KWIK online regression problem efﬁciently  and it is not even clear
whether this is possible. Thus  though the result of [15] is interesting  it does not provide a provably
efﬁcient algorithm.
An important challenge that remains is to couple exploration and value function generalization in
a provably effective way  and in particular  to establish sample and computational efﬁciency guar-
antees that scale gracefully with the planning horizon and model complexity. In this paper  we aim
to make progress in this direction. To start with a simple context  we restrict our attention to deter-
ministic systems that evolve over ﬁnite time horizons  and we consider episodic learning  in which
an agent repeatedly interacts with the same system. As a solution to the problem  we propose opti-
mistic constraint propagation (OCP)  a computationally efﬁcient reinforcement learning algorithm
designed to synthesize efﬁcient exploration and value function generalization. We establish that
when the true value function Q⇤ lies within the hypothesis class Q  OCP selects optimal actions
over all but at most dimE[Q] episodes. Here  dimE denotes the eluder dimension  which quantiﬁes
complexity of the hypothesis class. A corollary of this result is that regret is bounded by a function
that is constant over time and linear in the problem horizon and eluder dimension.
To put our aforementioned result in perspective  it is useful to relate it to other lines of work.
Consider ﬁrst the broad area of reinforcement learning algorithms that ﬁt value functions  such
as SARSA [19]. Even with the most commonly used sort of hypothesis class Q  which is made
up of linear combinations of ﬁxed basis functions  and even when the hypothesis class contains the
true value function Q⇤  there are no guarantees that these algorithms will efﬁciently learn to make
near-optimal decisions. On the other hand  our result implies that OCP attains near-optimal per-
formance in time that scales linearly with the number of basis functions. Now consider the more
specialized context of a deterministic linear system with quadratic cost and a ﬁnite time horizon.
The analysis of [1] can be leveraged to produce regret bounds that scale exponentially in the number
of state variables. On the other hand  using a hypothesis space Q consisting of quadratic functions
of state-action pairs  the results of this paper show that OCP behaves near optimally within time that
scales quadratically in the number of state and action variables.
We also establish efﬁciency and asymptotic performance guarantees that apply to agnostic reinforce-
ment learning  where Q⇤ does not necessarily lie in Q. In particular  we consider the case where Q
is the span of pre-speciﬁed indicator functions over disjoint sets. Our results here add to the litera-
ture on agnostic reinforcement learning with such a hypothesis class [21  25  7  26]. Prior work in

2

this area has produced interesting algorithms and insights  as well as bounds on performance loss
associated with potential limits of convergence  but no convergence or efﬁciency guarantees.

2 Reinforcement Learning in Deterministic Systems

In this paper  we consider an episodic reinforcement learning (RL) problem in which an agent repeat-
edly interacts with a discrete-time ﬁnite-horizon deterministic system  and refer to each interaction
as an episode. The system is identiﬁed by a sextuple M = (S A  H  F  R  S)  where S is the state
space  A is the action space  H is the horizon  F is a system function  R is a reward function and
S is a sequence of states. If action a 2A is selected while the system is in state x 2S at period
t = 0  1 ···   H  1  a reward of Rt(x  a) is realized; furthermore  if t < H  1  the state transitions
to Ft(x  a). Each episode terminates at period H  1  and then a new episode begins. The initial
state of episode j is the jth element of S.
To represent the history of actions and observations over multiple episodes  we will often index
variables by both episode and period. For example  xj t and aj t denote the state and action at
period t of episode j  where j = 0  1 ··· and t = 0  1 ···   H  1. To count the total number of
steps since the agent started learning  we say period t in episode j is time jH + t.
A (deterministic) policy µ = (µ0  . . .   µH1) is a sequence of functions  each mapping S to A.
For each policy µ  deﬁne a value function V µ
⌧ =t R⌧ (x⌧   a⌧ )  where xt = x  x⌧ +1 =
F⌧ (x⌧   a⌧ )  and a⌧ = µ⌧ (x⌧ ). The optimal value function is deﬁned by V ⇤t (x) = supµ V µ
t (x). A
policy µ⇤ is said to be optimal if V µ⇤ = V ⇤. Throughout this paper  we will restrict attention to
systems M = (S A  H  F  R  S) that admit optimal policies. Note that this restriction incurs no
loss of generality when the action space is ﬁnite.
It is also useful to deﬁne an action-contingent optimal value function: Q⇤t (x  a) = Rt(x  a) +
V ⇤t+1(Ft(x  a)) for t < H  1  and Q⇤H1(x  a) = RH1(x  a). Then  a policy µ⇤ is optimal if
µ⇤t (x) 2 arg maxa2A Q⇤t (x  a) for all (x  t).
A reinforcement learning algorithm generates each action aj t based on observations made up to the
tth period of the jth episode  including all states  actions  and rewards observed in previous episodes
and earlier in the current episode  as well as the state space S  action space A  horizon H  and possi-
ble prior information. In each episode  the algorithm realizes reward R(j) =PH1
t=0 Rt (xj t  aj t).
Note that R(j)  V ⇤0 (xj 0) for each jth episode. One way to quantify performance of a reinforce-
ment learning algorithm is in terms of the number of episodes JL for which R(j) < V ⇤0 (xj 0)  ✏ 
where ✏  0 is a pre-speciﬁed performance loss threshold. If the reward function R is bounded 
with |Rt(x  a)| R for all (x  a  t)  then this also implies a bound on regret over episodes ex-
perienced prior to time T   deﬁned by Regret(T ) = PbT /Hc1
(V ⇤0 (xj 0)  R(j)). In particular 
Regret(T )  2RHJL + ✏bT /Hc.

t (x) = PH1

j=0

3 Optimistic Constraint Propagation

At a high level  our reinforcement learning algorithm – optimistic constraint propagation (OCP) –
selects actions based on the optimism in the face of uncertainty principle and based on observed
rewards and state transitions propagates constraints backwards through time. Speciﬁcally  it takes
as input the state space S  the action space A  the horizon H  and a hypothesis class Q of candi-
dates for Q⇤. The algorithm maintains a sequence of subsets of Q and a sequence of scalar “upper
bounds”  which summarize constraints that past experience suggests for ruling out hypotheses. Each
constraint in this sequence is speciﬁed by a state x 2S   an action a 2A   a period t = 0  . . .   H  1 
and an interval [L  U ] ✓ <  and takes the form {Q 2Q : L  Qt(x  a)  U}. The upper bound
of the constraint is U. Given a sequence C = (C1  . . .  C|C|) of such constraints and upper bounds
U = (U1  . . .  U|C|)  a set QC is deﬁned constructively by Algorithm 1. Note that if the constraints
do not conﬂict then QC = C1 \···\C |C|. When constraints do conﬂict  priority is assigned ﬁrst
based on upper bound  with smaller upper bound preferred  and then  in the event of ties in upper
bound  based on position in the sequence  with more recent experience preferred.

3

Algorithm 1 Constraint Selection
Require: Q  C

QC Q   u minU
while u  1 do
for ⌧ = |C| to 1 do
QC Q C \C ⌧

if U⌧ = u and QC \C ⌧ 6= ? then
end if
end for
if {u0 2U : u0 > u} = ? then
end if
u min{u0 2U : u0 > u}

return QC

end while

OCP  presented below as Algorithm 2  at each time t computes for the current state xj t and each
action a the greatest state-action value Qt(xj t  a) among functions in QC and selects an action that
attains the maximum. In other words  an action is chosen based on the most optimistic feasible out-
come subject to constraints. The subsequent reward and state transition give rise to a new constraint
that is used to update C. Note that the update of C is postponed until one episode is completed.
Algorithm 2 Optimistic Constraint Propagation
Require: S  A  H  Q

Initialize C ?
for episode j = 0  1 ··· do

Set C0 C
for period t = 0  1 ···   H  1 do

Apply aj t 2 arg maxa2A supQ2QC
if t < H  1 then
Uj t supQ2QC
Lj t inf Q2QC
Uj t Rt(xj t  aj t)
Lj t Rt(xj t  aj t)

else

Qt(xj t  a)

(Rt(xj t  aj t) + supa2A Qt+1 (xj t+1  a))
(Rt(xj t  aj t) + supa2A Qt+1 (xj t+1  a))

end if
C0 C 0 _ {Q 2Q : Lj t  Qt(xj t  aj t)  Uj t}

end for
Update C C 0

end for

Note that if Q⇤ 2Q then each constraint appended to C does not rule out Q⇤  and therefore  the
sequence of sets QC generated as the algorithm progresses is decreasing and contains Q⇤ in its
intersection. In the agnostic case  where Q⇤ may not lie in Q  new constraints can be inconsistent
with previous constraints  in which case selected previous constraints are relaxed as determined by
Algorithm 1.
Let us brieﬂy discuss several contexts of practical relevance and/or theoretical interest in which OCP
can be applied.

• Finite state/action tabula rasa case. With ﬁnite state and action spaces  Q⇤ can be repre-
sented as a vector  and without special prior knowledge  it is natural to let Q = <|S|·|A|·H.
• Polytopic prior constraints. Consider the aforementioned example  but suppose that we
have prior knowledge that Q⇤ lies in a particular polytope. Then we can let Q be that
polytope and again apply OCP.
• Linear systems with quadratic cost (LQ). In this classical control model  if S = <n 
A = <m  and R is a positive semideﬁnite quadratic  then for each t  Q⇤t is known to be a

4

positive semideﬁnite quadratic  and it is natural to let Q = QH
positive semideﬁnite quadratics.

0 with Q0 denoting the set of
• Finite hypothesis class. Consider a context when we have prior knowledge that Q⇤ can
be well approximated by some element in a ﬁnite hypothesis class. Then we can let Q be
that ﬁnite hypothesis class and apply OCP. This scenario is of particular interest from the
perspective of learning theory. Note that this context entails agnostic learning  which is
accommodated by OCP.

• Linear combination of features.
It is often effective to hand-select a set of features
1  . . .   K  each mapping S⇥A to <  and  then for each t  aiming to compute weights
✓(t) 2 <K so that Pk ✓(t)
k k approximates Q⇤t without knowing for sure that Q⇤t lies
in the span of the features. To apply OCP here  we would let Q = QH
0 with Q0 =
span(1  . . .   K). Note that this context also entails agnostic learning.
• Sigmoid. If it is known that rewards are only received upon transitioning to the terminal
state and take values between 0 and 1  it might be appropriate to use a variation of the
aforementioned feature based model that applies a sigmoidal function to the linear combi-
0 with Q0 =  (Pk ✓kk(·)) : ✓ 2 <K  
nation. In particular  we could have Q = QH
where (z) = ez/(1 + ez).

It is worth mentioning that OCP  as we have deﬁned it  assumes that an action a maximizing
Qt(xj t  a) exists in each iteration. It is not difﬁcult to modify the algorithm so that it
supQ2QC
addresses cases where this is not true. But we have not presented the more general form of OCP in
order to avoid complicating this short paper.

4 Sample Efﬁciency of Optimistic Constraint Propagation

We now establish results concerning the sample efﬁciency of OCP. Our results bound the time it
takes OCP to learn  and this must depend on the complexity of the hypothesis class. As such  we
begin by deﬁning the eluder dimension  as introduced in [20]  which is the notion of complexity we
will use.

4.1 Eluder Dimension
Let Z = {(x  a  t) : x 2S   a 2A   t = 0  . . .   H  1} be the set of all state-action-period triples 
and let Q denote a nonempty set of functions mapping Z to <. For all (x  a  t) 2Z and ˜Z✓Z  
(x  a  t) is said to be dependent on ˜Z with respect to Q if any pair of functions Q  ˜Q 2Q that are
equal on ˜Z are equal at (x  a  t). Further  (x  a  t) is said to be independent of ˜Z with respect to Q
if (x  a  t) is not dependent on ˜Z with respect to Q.
The eluder dimension dimE[Q] of Q is the length of the longest sequence of elements in Z such that
every element is independent of its predecessors. Note that dimE[Q] can be zero or inﬁnity  and it
is straightforward to show that if Q1 ✓Q 2 then dimE[Q1]  dimE[Q2]. Based on results of [20] 
we can characterize the eluder dimensions of various hypothesis classes presented in the previous
section.

dimE[Q] = d.
ics with domain <m+n and Q = QH

• Finite state/action tabula rasa case. If Q = <|S|·|A|·H  then dimE[Q] = |S| · |A| · H.
• Polytopic prior constraints.
If Q is a polytope of dimension d in <|S|·|A|·H  then
• Linear systems with quadratic cost (LQ). If Q0 is the set of positive semideﬁnite quadrat-
• Finite hypothesis space. If |Q| < 1  then dimE[Q] |Q| 1.
• Linear combination of features.
• Sigmoid. If Q = QH

0 with Q0 = span(1  . . .   K)  then
0 with Q0 = (Pk ✓kk(·)) : ✓ 2 <K   then dimE[Q]  KH.

0   then dimE[Q] = (m + n + 1)(m + n)H/2.

dimE[Q]  KH.

If Q = QH

5

4.2 Learning with a Coherent Hypothesis Class

We now present results that apply when OCP is presented with a coherent hypothesis class; that is 
where Q⇤ 2Q . Our ﬁrst result establishes that OCP can deliver less than optimal performance in
no more than dimE[Q] episodes.
Theorem 1 For any system M = (S A  H  F  R  S)  if OCP is applied with Q⇤ 2Q   then |{j :
R(j) < V ⇤0 (xj 0)}|  dimE[Q].
This theorem follows from an “exploration-exploitation lemma”  which asserts that in each episode 
OCP either delivers optimal reward (exploitation) or introduces a constraint that reduces the eluder
dimension of the hypothesis class by one (exploration). Consequently  OCP will experience sub-
optimal performance in at most dimE[Q] episodes. A complete proof is provided in the appendix.
An immediate corollary bounds regret.

Corollary 1 For any R  any system M = (S A  H  F  R  S) with sup(x a t) |Rt(x  a)| R  and
any T   if OCP is applied with Q⇤ 2Q   then Regret(T )  2RHdimE[Q].
Note the regret bound in Corollary 1 does not depend on time T   thus  it is an O (1) bound. Further-
more  this regret bound is linear in R  H and dimE[Q]  and does not directly depend on |S| or |A|.
The following results demonstrate that the bounds of the above theorem and corollary are sharp.

Theorem 2 For any reinforcement learning algorithm that takes as input a state space  an action
space  a horizon  and a hypothesis class  there exists a system M = (S A  H  F  R  S) and a
hypothesis class Q3 Q⇤ such that |{j : R(j) < V ⇤0 (xj 0)}|  dimE[Q].
Theorem 3 For any R  0 and any reinforcement learning algorithm that takes as input a
state space  an action space  a horizon  and a hypothesis class  there exists a system M =
(S A  H  F  R  S) with sup(x a t) |Rt(x  a)| R and a hypothesis class Q3 Q⇤ such that
supT Regret(T )  2RHdimE[Q].
A constructive proof of these lower bounds is provided in the appendix. Following our discussion
in previous sections  we discuss several interesting contexts in which the agent knows a coherent
hypothesis class Q with ﬁnite eluder dimension.

• Finite state/action tabula rasa case. If we apply OCP in this case  then it will deliver sub-
optimal performance in at most |S|·|A|·H episodes. Furthermore  if sup(x a t) |Rt(x  a)|
R  then for any T   Regret(T )  2R|S||A|H 2.
• Polytopic prior constraints. If we apply OCP in this case  then it will deliver sub-optimal
performance in at most d episodes. Furthermore  if sup(x a t) |Rt(x  a)| R  then for any
T   Regret(T )  2RHd.
• Linear systems with quadratic cost (LQ). If we apply OCP in this case  then it will deliver
sub-optimal performance in at most (m + n + 1)(m + n)H/2 episodes.
• Finite hypothesis class case. Assume that the agent has prior knowledge that Q⇤ 2Q  
where Q is a ﬁnite hypothesis class. If we apply OCP in this case  then it will deliver sub-
optimal performance in at most |Q|1 episodes. Furthermore  if sup(x a t) |Rt(x  a)| R 
then for any T   Regret(T )  2RH [|Q|  1].

4.3 Agnostic Learning

As we have discussed in Section 3  OCP can also be applied in agnostic learning cases  where Q⇤
may not lie in Q. For such cases  the performance of OCP should depend on not only the complexity
of Q  but also the distance between Q and Q⇤. We now present results when OCP is applied in a
special agnostic learning case  where Q is the span of pre-speciﬁed indicator functions over disjoint
subsets. We henceforth refer to this case as the state aggregation case.

6

Speciﬁcally  we assume that for any t = 0  1 ···   H  1  the state-action space at period t  Zt =
{(x  a  t) : x 2S   a 2A}   can be partitioned into Kt disjoint subsets Zt 1 Zt 2 ···  Zt Kt  and
use t k to denote the indicator function for partition Zt k (i.e. t k(x  a  t) = 1 if (x  a  t) 2Z t k 
and t k(x  a  t) = 0 otherwise). We deﬁne K =PH1
Note that dimE[Q] = K. We deﬁne the distance between Q⇤ and the hypothesis class Q as

Q = span0 1  0 2 ···   0 K0  1 1 ···   H1 KH1 .
(x a t)|Qt(x  a)  Q⇤t (x  a)|.
Q2QkQ  Q⇤k1 = min
sup
Q2Q

t=0 Kt  and Q as

(4.1)

(4.2)

⇢ = min

The following result establishes that with Q and ⇢ deﬁned above  the performance loss of OCP is
larger than 2⇢H(H + 1) in at most K episodes.
Theorem 4 For any system M = (S A  H  F  R  S)  if OCP is applied with Q deﬁned in Eqn(4.1) 
then

where K is the number of partitions and ⇢ is deﬁned in Eqn(4.2).

|{j : R(j) < V ⇤0 (xj 0)  2⇢H(H + 1)}|  K 

Similar to Theorem 1  this theorem also follows from an “exploration-exploitation lemma”  which
asserts that in each episode  OCP either delivers near-optimal reward (exploitation)  or approxi-
mately determines Q⇤t (x  a)’s for all the (x  a  t)’s in a disjoint subset (exploration). A complete
proof for Theorem 4 is provided in the appendix. An immediate corollary bounds regret.
Corollary 2 For any R  0  any system M = (S A  H  F  R  S) with sup(x a t) |Rt(x  a)| R 
and any time T   if OCP is applied with Q deﬁned in Eqn(4.1)  then Regret(T )  2RKH + 2⇢(H +
1)T   where K is the number of partitions and ⇢ is deﬁned in Eqn(4.2).

Note that the regret bound in Corollary 2 is O (T )  and the coefﬁcient of the linear term is 2⇢(H +1).
Consequently  if Q⇤ is close to Q  then the regret will increase slowly with T . Furthermore  the
regret bound in Corollary 2 does not directly depend on |S| or |A|.
We further notice that the threshold performance loss in Theorem 4 is O⇢H 2. The following
proposition provides a condition under which the performance loss in one episode is O (⇢H).
Proposition 1 For any episode j  if 8t = 0  1 ···   H  1 

QC ✓{ Q 2Q : Lj t  Qt(xj t  aj t)  Uj t}  

then we have V ⇤0 (xj 0)  R(j)  6⇢H = O (⇢H).
That is  if all the new constraints in an episode are redundant  then the performance loss in that
episode is O (⇢H). Note that if the condition for Proposition 1 holds in an episode  then QC will
not be modiﬁed at the end of that episode. Furthermore  if the system has a ﬁxed initial state and
the condition for Proposition 1 holds in one episode  then it will hold in all the subsequent episodes 
and consequently  the performance losses in all the subsequent episodes are O (⇢H).

5 Computational Efﬁciency of Optimistic Constraint Propagation

We now brieﬂy discuss the computational complexity of OCP. As typical in the complexity analysis
of optimization algorithms  we assume that basic operations include the arithmetic operations  com-
parisons  and assignment  and measure computational complexity in terms of the number of basic
operations (henceforth referred to as operations) per period.
First  it is worth pointing out that for a general hypothesis class Q and general action space A  the
per period computations of OCP can intractable. This is because:

• Computing supQ2QC
timization problems.

Qt(xj t  a)  Uj t and Lj t requires solving a possibly intractable op-

7

• Selecting an action that maximizes supQ2QC

Qt(xj t  a) can be intractable.

Further  the number of constraints in C  and with it the number of operations per period  can grow
over time.
However  if |A| is tractably small and Q has some special structures (e.g. Q is a ﬁnite set or a
linear subspace or  more generally a polytope)  then by discarding some “redundant” constraints in
C  OCP with a variant of Algorithm 1 will be computationally efﬁcient  and the sample efﬁciency
results developed in Section 4 will still hold. Due to space limitations  we only discuss the scenario
where Q is a polytope of dimension d. Note that the ﬁnite state/action tabula rasa case  the linear-
quadratic case  and the case with linear combinations of disjoint indicator functions are all special
cases of this scenario.
Speciﬁcally  if Q is a polytope of dimension d (i.e.  within a d-dimensional subspace)  then any
Q 2Q can be represented by a weight vector ✓ 2 <d  and Q can be characterized by a set of linear
inequalities of ✓. Furthermore  the new constraints of the form Lj t  Qt(xj t  aj t)  Uj t are also
linear inequalities of ✓. Hence  in each episode  QC is characterized by a polyhedron in <d  and
Qt(xj t  a)  Uj t and Lj t can be computed by solving linear programming (LP) problems.
supQ2QC
If we assume that all the encountered numerical values can be represented with B bits  and LPs
are solved by Karmarkar’s algorithm [11]  then the following proposition bounds the computational
complexity.

Proposition 2 If Q is a polytope of dimension d  each numerical value in the problem data
or observed in the course of learning can be represented with B bits  and OCP uses Kar-
then the computational complexity of OCP is
markar’s algorithm to solve linear programs 
O[|A| + |C|]|C|d4.5B operations per period.
The proof of Proposition 2 is provided in the appendix. Notice that the computational complexity
is polynomial in d  B  |C| and |A|  and thus  OCP will be computationally efﬁcient if all these
parameters are tractably small. Note that the bound in Proposition 2 is a worst-case bound  and
the O(d4.5) term is incurred by the need to solve LPs. For some special cases  the computational
complexity is much less. For instance  in the state aggregation case  the computational complexity
is O (|C| + |A| + d) operations per period.
As we have discussed above  one can ensure that |C| remains bounded by using variants of Algo-
rithm 1 that discard the redundant constraints and/or update QC more efﬁciently. Speciﬁcally  it is
straightforward to design such constraint selection algorithms if Q is a coherent hypothesis class  or
if Q is the span of pre-speciﬁed indicator functions over disjoint sets. Furthermore  if the notion of
redundant constraints is properly deﬁned  the sample efﬁciency results derived in Section 4 will still
hold.

6 Conclusion

We have proposed a novel reinforcement learning algorithm  called optimistic constraint propagation
(OCP)  that synthesizes efﬁcient exploration and value function generalization for reinforcement
learning in deterministic systems. We have shown that OCP is sample efﬁcient if Q⇤ lies in the given
hypothesis class  or if the given hypothesis class is the span of pre-speciﬁed indicator functions over
disjoint sets.
It is worth pointing out that for more general reinforcement learning problems  how to design prov-
ably sample efﬁcient algorithms with value function generalization is currently still open. For in-
stance  it is not clear how to establish such algorithms for the general agnostic learning case dis-
cussed in this paper  as well as for reinforcement learning in MDPs. One interesting direction for
future research is to extend OCP  or a variant of it  to these two problems.

References
[1] Yasin Abbasi-Yadkori and Csaba Szepesv´ari. Regret bounds for the adaptive control of linear
quadratic systems. Journal of Machine Learning Research - Proceedings Track  19:1–26  2011.

8

[2] Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforce-

ment learning. In NIPS  pages 49–56  2006.

[3] Mohammad Gheshlaghi Azar  Alessandro Lazaric  and Emma Brunskill. Regret bounds for

reinforcement learning with policy advice. CoRR  abs/1305.1027  2013.

[4] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforce-
In Proceedings of the 25th Conference on

ment learning in weakly communicating MDPs.
Uncertainty in Artiﬁcial Intelligence (UAI2009)  pages 35–42  June 2009.

[5] Dimitri P. Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc 

September 1996.

[6] Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm
for near-optimal reinforcement learning. Journal of Machine Learning Research  3:213–231 
2002.

[7] Geoffrey Gordon. Online ﬁtted reinforcement learning. In Advances in Neural Information

Processing Systems 8  pages 1052–1058. MIT Press  1995.

[8] Morteza Ibrahimi  Adel Javanmard  and Benjamin Van Roy. Efﬁcient reinforcement learning

for high dimensional linear quadratic systems. In NIPS  2012.

[9] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[10] Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis  University

College London  2003.

[11] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. Combina-

torica  4(4):373–396  1984.

[12] Michael J. Kearns and Daphne Koller. Efﬁcient reinforcement learning in factored MDPs. In

IJCAI  pages 740–747  1999.

[13] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial

time. Machine Learning  49(2-3):209–232  2002.

[14] Tor Lattimore  Marcus Hutter  and Peter Sunehag. The sample-complexity of general rein-

forcement learning. In ICML  2013.

[15] Lihong Li and Michael Littman. Reducing reinforcement learning to kwik online regression.

Annals of Mathematics and Artiﬁcial Intelligence  2010.

[16] Lihong Li  Michael L. Littman  and Thomas J. Walsh. Knows what it knows: a framework for

self-aware learning. In ICML  pages 568–575  2008.

[17] Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous rein-

forcement learning. In NIPS  2012.

[18] Warren Powell and Ilya Ryzhov. Optimal Learning. John Wiley and Sons  2011.
[19] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technical

report  1994.

[20] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. CoRR 

abs/1301.2609  2013.

[21] Satinder P. Singh  Tommi Jaakkola  and Michael I. Jordan. Reinforcement learning with soft

state aggregation. In NIPS  pages 361–368  1994.

[22] Er L. Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L. Littman. PAC model-
free reinforcement learning. In Proceedings of the 23rd international conference on Machine
learning  pages 881–888  2006.

[23] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press 

March 1998.

[24] Csaba Szepesv´ari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artiﬁcial

Intelligence and Machine Learning. Morgan & Claypool Publishers  2010.

[25] John N. Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic

programming. Machine Learning  22(1-3):59–94  1996.

[26] Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggre-

gation. Math. Oper. Res.  31(2):234–244  2006.

9

,Zheng Wen
Benjamin Van Roy
Cynthia Dwork
Vitaly Feldman
Moritz Hardt
Toni Pitassi
Omer Reingold
Aaron Roth
Zelda Mariet
Suvrit Sra