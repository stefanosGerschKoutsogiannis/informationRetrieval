2018,Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization,We consider the minimization of submodular functions subject to ordering constraints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures  with ordering constraints corresponding to first-order stochastic dominance.  We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th  first  or higher order oracles;  these algorithms also lead to improvements without isotonic constraints. Finally    our experiments  show that non-convex loss functions can be much more robust to outliers for isotonic regression  while still being solvable in polynomial time.,Efﬁcient Algorithms for Non-convex Isotonic
Regression through Submodular Optimization

Francis Bach

Inria

Département d’Informatique de l’Ecole Normale Supérieure

PSL Research University  Paris  France

francis.bach@ens.fr

Abstract

We consider the minimization of submodular functions subject to ordering con-
straints. We show that this potentially non-convex optimization problem can be cast
as a convex optimization problem on a space of uni-dimensional measures  with
ordering constraints corresponding to ﬁrst-order stochastic dominance. We propose
new discretization schemes that lead to simple and efﬁcient algorithms based on
zero-th  ﬁrst  or higher order oracles; these algorithms also lead to improvements
without isotonic constraints. Finally  our experiments show that non-convex loss
functions can be much more robust to outliers for isotonic regression  while still
being solvable in polynomial time.

1

Introduction

Shape constraints such as ordering constraints appear everywhere in estimation problems in machine
learning  signal processing and statistics. They typically correspond to prior knowledge  and are
imposed for the interpretability of models  or to allow non-parametric estimation with improved
convergence rates [16  8]. In this paper  we focus on imposing ordering constraints into an estimation
problem  a setting typically referred to as isotonic regression [4  26  22]  and we aim to generalize
the set of problems for which efﬁcient (i.e.  polynomial-time) algorithms exist.
We thus focus on the following optimization problem:

min
x2[0 1]n

H(x) such that 8(i  j) 2 E  xi > xj 

(1)

where E ⇢{ 1  . . .   n}2 represents the set of constraints  which form a directed acyclic graph. For
simplicity  we restrict x to the set [0  1]n  but our results extend to general products of (potentially
unbounded) intervals.
As convex constraints  isotonic constraints are well-adapted to estimation problems formulated as
convex optimization problems where H is convex  such as for linear supervised learning problems 
with many efﬁcient algorithms for separable convex problems [4  26  22  30]  which can thus be used
as inner loops in more general convex problems by using projected gradient methods (see  e.g.  [3]).
In this paper  we show that another form of structure can be leveraged. We will assume that H is
submodular  which is equivalent  when twice continuously differentiable  to having nonpositive cross
second-order derivatives. This notably includes all (potentially non convex) separable functions (i.e. 
sums of functions that depend on single variables)  but also many other examples (see Section 2).
Minimizing submodular functions on continuous domains has been recently shown to be equivalent
to a convex optimization problem on a space of uni-dimensional measures [2]  and given that the
functions x 7! (xjxi)+ are submodular for any > 0  it is natural that by using  tending to +1 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

we recover as well a convex optimization problem; the main contribution of this paper is to provide a
simple framework based on stochastic dominance  for which we design efﬁcient algorithms which
are based on simple oracles on the function H (typically access to function values and derivatives).
In order to obtain such algorithms  we go signiﬁcantly beyond [2] by introducing novel discretization
algorithms that also provide improvements without any isotonic constraints.
More precisely  we make the following contributions:

– We show in Section 3 that minimizing a submodular function with isotonic constraints can be
cast as a convex optimization problem on a space of uni-dimensional measures  with isotonic
constraints corresponding to ﬁrst-order stochastic dominance.

– On top of the naive discretization schemes presented in Section 4  we propose in Section 5 new
discretization schemes that lead to simple and efﬁcient algorithms based on zero-th  ﬁrst  or
higher order oracles. They go from requiring O(1/"3) = O(1/"2+1) function evaluations to
reach a precision "  to O(1/"5/2) = O(1/"2+1/2) and O(1/"7/3) = O(1/"2+1/3).

– Our experiments in Section 6 show that non-convex loss functions can be much more robust to

outliers for isotonic regression.

2 Submodular Analysis in Continuous Domains

In this section  we review the framework of [2] that shows how to minimize submodular functions
using convex optimization.

8(x  y) 2 [0  1]n ⇥ [0  1]n  H(x) + H(y) > H(min{x  y}) + H(max{x  y}) 

Deﬁnition. Throughout this paper  we consider a continuous function H : [0  1]n ! R. The
function H is said to be submodular if and only if [21  29]:
(2)
where the min and max operations are applied component-wise. If H is continuously twice differen-
tiable  then this is equivalent to @2H
@xi@xj
The cone of submodular functions on [0  1]n is invariant by marginal strictly increasing transfor-
mations  and includes all functions that depend on a single variable (which play the role of linear
functions for convex functions)  which we refer to as separable functions.

(x) 6 0 for any i 6= j and x 2 [0  1]n [29].

Examples. The classical examples are: (a) any separable function  (b) convex functions of the
difference of two components  (c) concave functions of a positive linear combination  (d) negative
log densities of multivariate totally positive distributions [17]. See Section 6 for a concrete example.

Extension on a space of measures. We consider the convex set P([0  1]) of Radon probability
measures [24] on [0  1]  which is the closure (for the weak topology) of the convex hull of all Dirac
measures. In order to get an extension  we look for a function deﬁned on the set of products of
probability measures µ 2 P([0  1])n  such that if all µi  i = 1  . . .   n  are Dirac measures at points
xi 2 [0  1]  then we have a function value equal to H(x1  . . .   xn). Note that P([0  1])n is different
from P([0  1]n)  which is the set of probability measures on [0  1]n.
For a probability distribution µi 2 P([0  1]) deﬁned on [0  1]  we can deﬁne the (reversed) cumulative
: [0  1] ! [0  1] as Fµi(xi) = µi[xi  1]. This is a non-increasing
distribution function Fµi
left-continuous function from [0  1] to [0  1]  such that Fµi(0) = 1 and Fµi(1) = µi({1}). See
illustrations in the left plot of Figure 1.
We can then deﬁne the “inverse” cumulative function from [0  1] to [0  1] as F 1
µi (ti) = sup{xi 2
[0  1]  Fµi(xi) > ti}. The function F 1
is non-increasing and right-continuous  and such that
µi (1) = min supp(µi) and F 1
F 1
The extension from [0  1]n to the set of product probability measures is obtained by considering a
single threshold t applied to all n cumulative distribution functions  that is:

µi (0) = 1. Moreover  we have Fµi(xi) > ti   F 1

µi (ti) > xi.

µi

8µ 2 P([0  1])n  h(µ1  . . .   µn) =Z 1

0

2

H⇥F 1

µ1 (t)  . . .   F 1

µn (t)⇤dt.

(3)

2

1.5

1

0.5

 

F

µ
−1

F

µ

µ

3

2

1

 

F

ν

F

µ

ν

µ

0

 

0

0.5

1

0

 

0

0.2

0.4

0.6

0.8

1

Figure 1: Left: cumulative and inverse cumulative distribution functions with the corresponding
density (with respect to the Lebesgue measure). Right: cumulative functions for two distributions µ
and ⌫ such that µ < ⌫.

⇥F 1

µ1 (t)  . . .   F 1

We have the following properties when H is submodular: (a) it is an extension  that is  if for all i  µi
is a Dirac at xi  then h(µ) = H(x); (b) it is convex; (c) minimizing h on P([0  1])n and minimizing
H on [0  1]n is equivalent; moreover  the minimal values are equal and µ is a minimizer if and only if

µn (t)⇤ is a minimizer of H for almost all t2 [0  1]. Thus  submodular minimization

is equivalent to a convex optimization problem in a space of uni-dimensional measures.
Note that the extension is deﬁned on all tuples of measures µ = (µ1  . . .   µn) but it can equivalently
be deﬁned through non-increasing functions from [0  1] to [0  1]  e.g.  the representation in terms of
cumulative distribution functions Fµi deﬁned above (this representation will be used in Section 4
where algorithms based on the discretization of the equivalent obtained convex problem are discussed).

3

Isotonic Constraints and Stochastic Dominance

In this paper  we consider the following problem:

inf

x2[0 1]n

H(x) such that 8(i  j) 2 E  xi > xj 

(4)

where E is the edge set of a directed acyclic graph on {1  . . .   n} and H is submodular. We denote
by X ⇢ Rn (not necessarily a subset of [0  1]n) the set of x 2 Rn satisfying the isotonic constraints.
In order to deﬁne an extension in a space of measures  we consider a speciﬁc order on measures
on [0  1]  namely ﬁrst-order stochastic dominance [20]  deﬁned as follows.
Given two distributions µ and ⌫ on [0  1]  with (inverse) cumulative distribution functions Fµ and F⌫ 
we have µ < ⌫  if and only if 8x 2 [0  1]  Fµ(x) > F⌫(x)  or equivalently  8t 2 [0  1]  F 1
µ (t) >
(t). As shown in the right plot of Figure 1  the densities may still overlap. An equivalent
F 1
characterization [19  9] is the existence of a joint distribution on a vector (X  X0) 2 R2 with
marginals µ(x) and ⌫(x0) and such that X > X0 almost surely1. We now prove the main proposition
of the paper:

⌫

Proposition 1 We consider the convex minimization problem:

inf

µ2P([0 1])n

h(µ) such that 8(i  j) 2 E  µi < µj.

(5)

µ (t) is a minimizer of H of Eq. (4) for almost all t 2 [0  1].

Problems in Eq. (4) and Eq. (5) have the same objective values. Moreover  µ is a minimizer of Eq. (5)
if and only if F 1
Proof We denote by M the set of µ 2 P([0  1])n satisfying the stochastic ordering constraints. For
any x 2 [0  1]n that satisﬁes the constraints in Eq. (4)  i.e.  x 2 X \ [0  1]n  the associated Dirac
measures satisfy the constraint in Eq. (5). Therefore  the objective value M of Eq. (4) is greater or
equal to the one M0 of Eq. (5). Given a minimizer µ for the convex problem in Eq. (5)  we have:
M > M0 = h(µ) =R 1
0 M dt = M. This shows the proposition by

µn (t)⇤dt >R 1

studying the equality cases above.

0 H⇥F 1

µ1 (t)  . . .   F 1

distributed in [0  1].

1Such a joint distribution may be built as the distribution of (F 1

µ (T )  F 1

⌫

(T ))  where T is uniformly

3

Alternatively  we could add the penalty term P(i j)2ER +1
(Fµj (z)  Fµi(z))+dz  which corre-
sponds to the unconstrained minimization of H(x) + P(i j)2E(xj  xi)+. For > 0 big enough2 
this is equivalent to the problem above  but with a submodular function which has a large Lipschitz
constant (and is thus harder to optimize with the iterative methods presented below).

1

4 Discretization algorithms

Prop. 1 shows that the isotonic regression problem with a submodular cost can be cast as a convex
optimization problem; however  this is achieved in a space of measures  which cannot be handled
directly computationally in polynomial time. Following [2]  we consider a polynomial time and
space discretization scheme of each interval [0  1] (and not of [0  1]n)  but we propose in Section 5
a signiﬁcant improvement that allows to reduce the number of discrete points signiﬁcantly. All
pseudo-codes for the algorithms are available in Appendix B.

4.1 Review of submodular optimization in discrete domains
All our algorithms will end up minimizing approximately a submodular function F on {0  . . .   k1}n 
that is  which satisﬁes Eq. (2). Isotonic constraints will be added in Section 4.2.
Following [2]  this can be formulated as minimizing a convex function f# on the set of ⇢ 2
[0  1]n⇥(k1) so that for each i 2{ 1  . . .   n}  (⇢ij)j2{1 ... k1} is a non-increasing sequence (we
denote by S this set of constraints) corresponding to the cumulative distribution function. For any
feasible ⇢  a subgradient of f# may be computed by sorting all n(k  1) elements of the matrix ⇢ and
computing at most n(k  1) values of F . An approximate minimizer of F (which exactly inherits
approximation properties from the approximate optimality of ⇢) is then obtained by selecting the
minimum value of F in the computation of the subgradient. Projected subgradient methods can
then be used  and if F is the largest absolute difference in values of F when a single variable is

changed by ±1  we obtain an "-minimizer (for function values) after t iterations  with " 6 nkF /pt.

The projection step is composed of n simple separable quadratic isotonic regressions with chain
constraints in dimension k  which can be solved easily in O(nk) using the pool-adjacent-violator
algorithm [4]. Computing a subgradient requires a sorting operation  which is thus O(nk log(nk)).
See more details in [2].
Alternatively  we can minimize the strongly-convex f#(⇢) + 1
F on the set of ⇢ 2 Rn⇥(k1) so
that for each i  (⇢ij)j is a non-increasing sequence  that is  ⇢ 2 S (the constraints that ⇢ij 2 [0  1]
are dropped). We then get a minimizer z of F by looking for all i 2{ 1  . . .   n} at the largest
j 2{ 1  . . .   k  1} such that ⇢ij > 0. We take then zi = j (and if no such j exists  zi = 0). A gap
of " in the problem above  leads to a gap of p"nk for the original problem (see more details in [2]).
The subgradient method in the primal  or Frank-Wolfe algorithm in the dual may be used for this
problem. We obtain an "-minimizer (for function values) after t iterations  with " 6 F /t  which
leads for the original submodular minimization problem to the same optimality guarantees as above 
but with a faster algorithm in practice. See the detailed computations and comparisons in [2].

2k⇢k2

4.2 Naive discretization scheme

i

Following [2]  we simply discretize [0  1] by selecting the k values
2k   for i 2{ 0  . . .   k 
1}. If the function H : [0  1]n is L1-Lipschitz-continuous with respect to the `1-norm  that is
|H(x)  H(x0)| 6 L1kx  x0k1  the function F is (L1/k)-Lipschitz-continuous with respect to the
`1-norm (and thus we have F 6 L1/k above). Moreover  if F is minimized up to "  H is optimized
up to " + nL1/k.
In order to take into account the isotonic constraints  we simply minimize with respect to ⇢ 2
[0  1]n⇥(k1) \ S  with the additional constraint that for all j 2{ 1  . . .   k  1}  8(a  b) 2 E 
⇢a j > ⇢b j. This corresponds to additional contraints T ⇢ Rn⇥(k1).

k1 or 2i+1

2A short calculation shows that when H is differentiable  the ﬁrst order-optimality condition (which is only
necessary here) implies that if  is strictly larger than n times the largest possible partial ﬁrst-order derivative
of H  the isotonic constraints have to be satisﬁed.

4

Following Section 4.1  we can either choose to solve the convex problem min⇢2[0 1]n⇥k\S\T f#(⇢) 
or the strongly-convex problem min⇢2S\T f#(⇢) + 1
F . In the two situations  after t iterations 
that is tnk accesses to values of H  we get a constrained minimizer of H with approximation
guarantee nL1/k + nL1/pt. Thus in order to get a precision "  it sufﬁces to select k > 2nL1/" and
t > 4n2L2
1/"3 accesses to function values of H  which is the same
as obtained in [2] (except for an extra factor of n due to a different deﬁnition of L1).

1/"2  leading to an overall 8n4L3

2k⇢k2

4.3

Improved behavior for smooth functions

i

k1 for i 2{ 0  . . .   k  1}  and we assume that all ﬁrst-order
We consider the discretization points
(resp. second-order) partial derivatives are bounded by L1 (resp. L2
2). In the reasoning above  we may
upper-bound the inﬁmum of the discrete function in a ﬁner way  going from inf x2X H(x) + nL1/k
to inf x2X H(x) + 1
2/k2 (by doing a Taylor expansion around the global optimum  where the
ﬁrst-order terms are always zero  either because the partial derivative is zero or the deviation is zero).
We now select k > nL2/p"  leading to a number of accesses to H that scales as 4n4L2
1L2/"5/2. We
thus gain a factor p" with the exact same algorithm  but different assumptions.

2 n2L2

4.4 Algorithms for isotonic problem

Compared to plain submodular minimization where we need to project onto S  we need to take into
account the extra isotonic constraints  i.e.  ⇢ 2 T  and thus use more complex orthogonal projections.
Orthogonal projections. We now require the orthogonal projections on S\ T or [0  1]n⇥k \ S\ T 
which are themselves isotonic regression problems with nk variables. If there are m original isotonic
constraints in Eq. (4)  the number of isotonic constraints for the projection step is O(nk + mk)  which
is typically O(mk) if m > n  which we now assume. Thus  we can use existing parametric max-ﬂow
algorithms which can solve these in O(nmk2 log(nk)) [13] or in O(nmk2 log(n2k/m)) [11]. See
in Appendix A a description of the reformulation of isotonic regression as a parametric max-ﬂow
problem  and the link with minimum cut. Following [7  Prop. 5.3]  we incorporate the [0  1] box
constraints  by ﬁrst ignoring them and thus by projecting onto the regular isotonic constraints  and
then thresholding the result through x ! max{min{x  1}  0}.
Alternatively  we can explicitly consider a sequence of max-ﬂow problems (with at most log(1/") of
these  where " is the required precision) [28  15]. Finally  we may consider (approximate) alternate
projection algorithms such as Dykstra’s algorithm and its accelerated variants [6]  since the set S is
easy to project to  while  in some cases  such as chain isotonic constraints for the original problem  T
is easy to project to.
Finally  we could also use algorithms dedicated to special structures for isotonic regression (see [27]) 
in particular when our original set of isotonic constraints in Eq. (4) is a chain  and the orthogonal
projection corresponds to a two-dimensional grid [26]. In our experiments  we use a standard
max-ﬂow code [5] and the usual divide-and-conquer algorithms [28  15] for parametric max-ﬂow.

1

Separable problems. The function f# from Section 4.2 is then a linear function of the form
f#(⇢) = tr w>⇢  and then  a single max-ﬂow algorithm can be used.
For these separable problerms  the alternative strongly-convex problem of minimizing f#(⇢) + 1
2k⇢k2
F
becomes the one of minimizing min⇢2S\T
F   which is simply the problem of projecting
on the intersection of two convex sets  for which an accelerated Dykstra algorithm may be used [6] 
with convergence rate in O(1/t2) after t iterations. Each step is O(kn) for projecting onto S  while
this is k parametric network ﬂows with n variables and m constraints for projecting onto T  in
O(knm log n) for the general case and O(kn) for chains and rooted trees [4  30].
In our experiments in Section 6  we show that Dykstra’s algorithm converges quickly for separable
problems. Note that when the underlying losses are convex3  then Dykstra converges in a single
iteration. Indeed  in this situation  the sequences (wij)j are non-increasing and isotonic regression
3This is a situation where direct algorithms such as the ones by [22] are much more efﬁcient than our

2k⇢ + wk2

discretization schemes.

5

along a direction preserves decreasingness in the other direction  which implies that after two alternate
projections  the algorithm has converged to the optimal solution.
Alternatively  for the non-strongly convex formulation  this is a single network ﬂow problem with
n(k  1) nodes  and mk constraints  in thus O(nmk2 log(nk)) [25]. When E corresponds to a chain 
then this is a 2-dimensional-grid with an algorithm in O(n2k2) [26]. For a precision "  and thus k
proportional to n/" with the assumptions of Section 4.2  this makes a number of function calls for H 
equal to O(kn) = O(n2/") and a running-time complexity of O(n3m/"2 · log(n2/"))—for smooth
functions  as shown in Section 4.3  we get k proportional to n/p" and thus an improved behavior.

5

Improved discretization algorithms

We now consider a different discretization scheme that can take advantage of access to higher-order
derivatives. We divide [0  1] into k disjoint pieces A0 = [0  1
k   1].
This deﬁnes a new function ˜H : {0  . . .   k 1}n ! R deﬁned only for elements z 2{ 0  . . .   k 1}n
that satisfy the isotonic constraint  i.e.  z 2{ 0  . . .   k  1}n \ X:

k )  . . .   Ak1 = [ k1

k )  A1 = [ 1

k   2

(6)

i=1 Azi

x2Qn

˜H(z) = min

H(x) such that 8(i  j) 2 E  xi > xj.
The function ˜H(z) is equal to +1 if z does not satisfy the isotonic constraints.
Proposition 2 The function ˜H is submodular  and minimizing ˜H(z) for z 2{ 0  . . .   k  1}n such
that 8(i  j) 2 E  zi > zj is equivalent to minimizing Eq. (4).
Proof We consider z and z0 that satisfy the isotonic constraints  with minimizers x and x0 in the deﬁ-
nition in Eq. (6). We have H(z) + H(z0) = H(x) + H(x0) > H(min{x  x0}) + H(max{x  x0}) >
H(min{z  z0}) + H(max{z  z0}). Thus it is submodular on the sub-lattice {0  . . .   k  1}n \ X.
Note that in order to minimize ˜H  we need to make sure that we only access H for elements z that
satisfy the isotonic constraints  that is ⇢ 2 S \ T (which our algorithms impose).
5.1 Approximation from high-order smoothness

1

k

r=1P|↵|=r

i=1 Azi )\X Hq(x| z+1/2

The main idea behind our discretization scheme is to use high-order smoothness to approximate for
any required z  the function value ˜H(z). If we assume that H is q-times differentiable  with uniform
bounds Lr
r on all r-th order derivatives  then  the (q1)-th order Taylor expansion of H around y is
equal to Hq(x|y) = H(y) +Pq1
↵! (x  y)↵H (↵)(y)  where ↵ 2 Nn and |↵| is the sum
of elements  (x  y)↵ is the vector with components (xi  yi)↵i  ↵! the products of all factorials of
elements of ↵  and H (↵)(y) is the partial derivative of H with order ↵i for each i.
We thus approximate ˜H(z)  for any z that satisﬁes the isotonic constraint (i.e.  z 2 X)  by ˆH(z) =
). We have for any z  | ˜H(z)  ˆH(z)| 6 (nLq/2k)q/q!. Moreover 
minx2(Qn
when moving a single element of z by one  the maximal deviation is L1/k + 2(nLq/2k)q/q!.
If ˆH is submodular  then the same reasoning as in Section 4.2 leads to an approximate er-
ror of (nk/pt)L1/k + 2(nLq/2k)q/q! after t iterations  on top of (nLq/2k)q/q!  thus  with
1/"2 and k > (q!"/2)1/qnLq/2 (assuming " small enough such that t > 16n2k2)  this
t > 16n2L2
1Lq/"2+1/q). We thus get
leads to a number of accesses to the (q1)-th order oracle equal to O(n4L2
an improvement in the power of "  which tend to "2 for inﬁnitely smooth problems. Note that when
q = 1 we recover the same rate as in Section 4.3 (with the same assumptions but a slightly different
algorithm).
However  unless q = 1  the function ˆH(z) is not submodular  and we cannot apply directly the
bounds for convex optimization of the extension. We show in Appendix D that the bound still holds
for q > 1 by using the special structure of the convex problem.
What remains unknown is the computation of ˆH which requires to minimize polynomials on a small
cube. We can always use the generic algorithms from Section 4.2 for this  which do not access extra

6

function values but can be slow. For quadratic functions  we can use a convex relaxation which is
not tight but already allows strong improvements with much faster local steps  and which we now
present. See the pseudo-code in Appendix B. In any case  using expansions of higher order is only
practically useful in situations where function evaluations are expensive.

5.2 Quadratic problems
In this section  we consider the minimization of a quadratic submodular function H(x) = 1
2 x>Ax +
c>x (thus with all off-diagonal elements of A non-negative) on [0  1]n  subject to isotonic constraints
xi > xj for all (i  j) 2 E. This is the sub-problem required in Section 5.1 when using second-order
Taylor expansions.
It could be solved iteratively (and approximately) with the algorithm from Section 4.2; in this
section  we consider a semideﬁnite relaxation which is tight for certain problems (A positive semi-
deﬁnite  c non-positive  or A with non-positive diagonal elements)  but not in general (we have found
counter-examples but it is most often tight).
The relaxation is based on considering the set of (Y  y) 2 Rn⇥n ⇥ Rn such that there exists
x 2 [0  1]n \ X with Y = xx> and y = x. Our problem is thus equivalent to minimizing
2 tr AY + c>y such that (Y  y) is in the convex-hull Y of this set  which is NP-hard to characterize
1
in polynomial time [10]. However  following ideas from [18]  we can ﬁnd a simple relaxation by
considering the following constraints: (a) for all i 6= j  Yii Yij yi
1! is positive semi-deﬁnite  (b)
for all i 6= j  Yij 6 inf{yi  yj}  which corresponds to xixj 6 inf{xi  xj} for any x 2 [0  1]n  (c)
i 6 xi  and (d) for all (i  j) 2 E  yi > yj  Yii > Yjj 
for all i  Yii 6 yi  which corresponds to x2
Yij > max{Yjj  yj  yi + Yii} and Yij 6 max{Yii  yi  yj + Yjj}  which corresponds to xi > xj 
i   and xi(1  xj) > xj(1  xj). This
i > x2
x2
leads to a semi-deﬁnite program which provides a lower-bound on the optimal value of the problem.
See Appendix E for a proof of tightness for special cases and a counter-example for the tightness in
general.

j  xi(1  xi) 6 xi(1  xj)  xixj 6 x2

j  xixj > x2

Yij Yjj yj
yi

yj

6 Experiments

We consider experiments aiming at (a) showing that the new possibility of minimizing submodular
functions with isotonic constraints brings new possibilities and (b) that the new discretization
algorithms are faster than the naive one.

1

Robust isotonic regression. Given some z 2 Rn  we consider a separable function H(x) =
nPn
i=1 G(xi  zi) with various possibilities for G: (a) the square loss G(t) = 1
2 t2  (b) the absolute
2 log1 + t2/2  which is the negative log-
loss G(t) = |t| and (c) a logarithmic loss G(t) = 2
density of a Student distribution and non-convex. The non-convexity of the cost function and the fact
that is has vanishing derivatives for large values make it a good candidate for robust estimation [12].
The ﬁrst two losses may be dealt with methods for separable convex isotonic regression [22  30]  but
the non-convex loss can only dealt with exactly by the new optimization routine that we present—
majorization-minimization algorithms [14] based on the concavity of G as a function of t2 can be
used with such non-convex losses  but as shown below  they converge to bad local optima.
For simplicity  we consider chain constraints 1 > x1 > x2 > ··· > xn > 0. We consider two
set-ups: (a) a separable set-up where maximum ﬂow algorithms can be used directly (with n = 200) 
and (b) a general submodular set-up (with n = 25 and n = 200)  where we add a smoothness penalty
2Pn1
i=1 (xi  xi+1)2  which is submodular (but not separable).
which is the sum of terms of the form 
Data generation. We generate the data z 2 Rn  with n = 200  as follows: we ﬁrst generate a
simple decreasing function of i 2{ 1  . . .   n} (here an afﬁne function); we then perturb this ground
truth by (a) adding some independent noise and (b) corrupting the data by changing a random subset
of the n values by the application of another function which is increasing (see Figure 2  left). This
is an adversarial perturbation  while the independent noise is not adversarial; the presence of the
adversarial noise makes the problem harder as the proportion of corrupted data increases.

7

s
t
i
f
 

−
 
s
n
o

i
t

a
v
r
e
s
b
o

Prop. of corrupted data = 50 %
0.7

 

0.6

0.5

0.4

 

0

100

index i

200

square
absolute
logarithm

)
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
m

(

0
1

g
o

l

 

square
absolute
logarithm 0.01
logarithm 0.001

0

−1

−2

 

0
1
proportion of outliers

0.5

Figure 2: Left: robust isotonic regression with decreasing constraints  with 50% of corrupted data
(observation in pink crosses  and results of isotonic regression with various losses in red  blue and
black); the dashed black line corresponds to majorization-minimization algorithm started from the
observations. Right: robustness of various losses to the proportion of corrupted data. The two
logarithm-based losses are used with two values of  (0.01 and 0.001); the dashed line corresponds to
the majorization-minimization algorithm (with no convergence guarantees and worse performance).

Optimization of separable problems with maximum ﬂow algorithms. We solve the discretized
version by a single maximum-ﬂow problem of size nk. We compare the various losses for k = 1000
on data which is along a decreasing line (plus noise)  but corrupted (i.e.  replaced for a certain
proportion) by data along an increasing line. See an example in the left plot of Figure 2 for 50% of
corrupted data. We see that the square loss is highly non robust  while the (still convex) absolute loss
is slightly more robust  and the robust non-convex loss still approximates the decreasing function
correctly with 50% of corrupted data when optimized globally  while the method with no guarantee
(based on majorization-minimization  dashed line) does not converge to an acceptable solution. In
Appendix C  we show additional examples where it is robust up to 75% of corruption.
In the right plot of Figure 2  we also show the robustness to an increasing proportion of outliers
(for the same type of data as for the left plot)  by plotting the mean-squared error in log-scale and
averaged over 20 replications. Overall  this shows the beneﬁts of non-convex isotonic regression with
guaranteed global optimization  even for large proportions of corrupted data.

Optimization of separable problems with pool-adjacent violator (PAV) algorithm. As shown
in Section 4.2  discretized separable submodular optimization corresponds to the orthogonal projection
of a matrix into the intersection of chain isotonic constraints in each row  and isotonic constraints
in each column equal to the original set of isotonic constraints (in these simulations  these are also
chain constraints). This can be done by Dykstra’s alternating projection algorithm or its accelerated
version [6]  for which each projection step can be performed with the PAV algorithm because each of
them corresponds to chain constraints.
In the left plot of Figure 3  we show the difference in function values (in log-scale) for various
discretization levels (deﬁned by the integer k spaced by 1/4 in base-10 logarithm)  as as function of
the number of iterations (averaged over 20 replications). For large k (small difference of function
values)  we see a spacing between the ends of the plots of approximatively 1/2  highlighting the
dependence in 1/k2 of the ﬁnal error with discretization k  which our analysis in Section 4.3 suggests.

Effect of the discretization for separable problems.
In order to highlight the effect of discretiza-
tion and its interplay with differentiability properties of the function to minimize  we consider in
the middle plot of Figure 3  the distance in function values after full optimization of the discrete
submodular function for various values of k. We see that for the simple smooth function (quadratic
loss)  we have a decay in 1/k2  while for the simple non smooth function (absolute loss)  we have a
ﬁnal decay in 1/k)  a predicted by our analysis. For the logarithm-based loss  whose smoothness
constant depends on   when  is large  it behaves like a smooth function immediately  while for 
smaller  k needs to be large enough to reach that behavior.

Non-separable problems. We consider adding a smoothness penalty to add the prior knowledge
that values should be decreasing and close. In Appendix C  we show the effect of adding a smoothness
prior (for n = 200): it leads to better estimation. In the right plot of Figure 3  we show the effect
of various discretization schemes (for n = 25)  from order 0 (naive discretization)  to order 1 and 2

8

Dykstra

Effect of discretization
0

 

)

)

 

H
∆
(

g
o

l

0
1

−3

−4

−5

−6

−7
0

 

H
∆
(

g
o

0
1

l

30

−2

−4

 

−6
1

10

20

Number of iterations

square
absolute
logarithm 0.1
logarithm 0.001

)

 

H
∆
(

g
o

0
1

l

3

2
log

(k)

10

0

−2

−4

 

−6
1

 

order 0
order 1
order 2

2

log

1.5
10

(k)

Figure 3: Dykstra’s projection algorithms for separable problems  with several values of k  spaced
with 1/4 in base-10 logarithm  from 101 to 103.5. Dykstra in dashed and accelerated Dykstra in
plain. Middle: effect of discretization value k for various loss functions for separable problems (the
logarithm-based loss is considered with two values of    = 0.1 and  = 0.001). Right: effect of
discretization k on non-separable problems.

(our new schemes based on Taylor expansions from Section 5.1)  and we plot the difference in
function values after 50 steps of subgradient descent: in each plot  the quantity H is equal to
H(x⇤k)  H⇤  where x⇤k is an approximate minimizer of the discretized problem with k values and
H⇤ the minimum of H (taking into account the isotonic constraints). As outlined in our analysis  the
ﬁrst-order scheme does not help because our function has bounded Hessians  while the second-order
does so signiﬁcantly.

7 Conclusion

In this paper  we have shown how submodularity could be leveraged to obtain polynomial-time
algorithms for isotonic regressions with a submodular cost  based on convex optimization in a space
of measures—although based on convexity arguments  our algorithms apply to all separable non-
convex functions. The ﬁnal algorithms are based on discretization  with a new scheme that also
provides improvements based on smoothness (also without isotonic constraints). Our framework is
worth extending in the following directions: (a) we currently consider a ﬁxed discretization  it would
be advantageous to consider adaptive schemes  potentially improving the dependence on the number
of variables n and the precision "; (b) other shape constraints can be consider in a similar submodular
framework  such as xixj > 0 for certain pairs (i  j); (c) a direct convex formulation without
discretization could probably be found for quadratic programming with submodular costs (which are
potentially non-convex but solvable in polynomial time); (d) a statistical study of isotonic regression
with adversarial corruption could now rely on formulations with polynomial-time algorithms.

Acknowledgements

We acknowledge support the European Research Council (grant SEQUOIA 724063).

References

[1] F. Bach. Learning with Submodular Functions: A Convex Optimization Perspective  volume 6

of Foundations and Trends in Machine Learning. NOW  2013.

[2] F. Bach. Submodular functions: from discrete to continuous domains. Mathematical Program-

ming  2018.

[3] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc Belmont  2016. 3rd edition.
[4] M. J. Best and N. Chakravarti. Active set algorithms for isotonic regression: a unifying

framework. Mathematical Programming  47(1):425–439  1990.

[5] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for
energy minimization in vision. IEEE Transactions on Pattern Analysis and Machine Intelligence 
26(9):1124–1137  2004.

9

[6] A. Chambolle and T. Pock. A remark on accelerated block coordinate descent for computing the
proximity operators of a sum of convex functions. SMAI-Journal of computational mathematics 
1:29—54  2015.

[7] Xi Chen  Qihang Lin  and Bodhisattva Sen. On degrees of freedom of projection estimators
with applications to multivariate shape restricted regression. Technical Report 1509.01877 
arXiv  2015.

[8] Y. Chen and R. J. Samworth. Generalized additive and index models with shape constraints.

Journal of the Royal Statistical Society Series B  78(4):729–754  2016.

[9] D. Dentcheva and A. Ruszczy´nski. Semi-inﬁnite probabilistic optimization: ﬁrst-order stochastic

dominance constraint. Optimization  53(5-6):583–601  2004.

[10] M. M. Deza and M. Laurent. Geometry of Cuts and Metrics  volume 15. Springer  2009.
[11] G. Gallo  M. D. Grigoriadis  and R. E. Tarjan. A fast parametric maximum ﬂow algorithm and

applications. SIAM Journal on Computing  18(1):30–55  1989.

[12] Frank R. Hampel  Elvezio M. Ronchetti  Peter J. Rousseeuw  and Werner A. Stahel. Robust
Statistics: the Approach Based on Inﬂuence Functions  volume 196. John Wiley & Sons  2011.
[13] D. S. Hochbaum. The pseudoﬂow algorithm: A new algorithm for the maximum-ﬂow problem.

Operations Research  56(4):992–1009  2008.

[14] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician  58(1):30–

37  2004.

[15] S. Jegelka  F. Bach  and S. Sra. Reﬂection methods for user-friendly submodular optimization.

In Advances in Neural Information Processing Systems (NIPS)  2013.

[16] S. M. Kakade  V. Kanade  O. Shamir  and A. Kalai. Efﬁcient learning of generalized linear and
single index models with isotonic regression. In Advances in Neural Information Processing
Systems (NIPS)  2011.

[17] S. Karlin and Y. Rinott. Classes of orderings of measures and related correlation inequalities.
i. multivariate totally positive distributions. Journal of Multivariate Analysis  10(4):467–498 
1980.

[18] S. Kim and M. Kojima. Exact solutions of some nonconvex quadratic optimization problems
via SDP and SOCP relaxations. Comp. Optimization and Applications  26(2):143–154  2003.
[19] E. L. Lehmann. Ordered families of distributions. The Annals of Mathematical Statistics 

26(3):399–419  1955.

[20] H. Levy. Stochastic dominance and expected utility: survey and analysis. Management science 

38(4):555–593  1992.

[21] G. G. Lorentz. An inequality for rearrangements. Am. Math. Monthly  60(3):176–179  1953.
[22] R. Luss and S. Rosset. Generalized isotonic regression. Journal of Computational and Graphical

Statistics  23(1):192–210  2014.

[23] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer  2004.
[24] W. Rudin. Real and complex analysis. McGraw-Hill  1986.
[25] D. D. Sleator and R. E. Tarjan. A data structure for dynamic trees. Journal of Computer and

System Sciences  26(3):362–391  1983.

[26] J. Spouge  H. Wan  and W. J. Wilbur. Least squares isotonic regression in two dimensions.

Journal of Optimization Theory and Applications  117(3):585–605  2003.

[27] Q. F. Stout. Isotonic regression via partitioning. Algorithmica  66(1):93–112  2013.
[28] R. Tarjan  J. Ward  B. Zhang  Y. Zhou  and J. Mao. Balancing applied to maximum network

ﬂow problems. In European Symposium on Algorithms  pages 612–623. Springer  2006.

[29] D. M. Topkis. Minimizing a submodular function on a lattice. Operations Research  26(2):305–

321  1978.

[30] Y.-L. Yu and E. P. Xing. Exact algorithms for isotonic regression and related. In Journal of

Physics: Conference Series  volume 699. IOP Publishing  2016.

10

,Francis Bach