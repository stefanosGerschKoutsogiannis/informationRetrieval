2014,On the Computational Efficiency of Training Neural Networks,It is well-known that neural networks are computationally hard to train. On the other hand  in practice  modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU)  over-specification (i.e.  train networks which are larger than needed)  and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results  some of them yield new provably efficient and practical algorithms for training neural networks.,On the Computational Efﬁciency of Training Neural

Networks

Roi Livni

The Hebrew University
roi.livni@mail.huji.ac.il

Shai Shalev-Shwartz
The Hebrew University

shais@cs.huji.ac.il

Ohad Shamir

Weizmann Institute of Science
ohad.shamir@weizmann.ac.il

Abstract

It is well-known that neural networks are computationally hard to train. On the
other hand  in practice  modern day neural networks are trained efﬁciently us-
ing SGD and a variety of tricks that include different activation functions (e.g.
ReLU)  over-speciﬁcation (i.e.  train networks which are larger than needed)  and
regularization. In this paper we revisit the computational complexity of training
neural networks from a modern perspective. We provide both positive and neg-
ative results  some of them yield new provably efﬁcient and practical algorithms
for training certain types of neural networks.

1

Introduction

One of the most signiﬁcant recent developments in machine learning has been the resurgence of
“deep learning”  usually in the form of artiﬁcial neural networks. A combination of algorithmic
advancements  as well as increasing computational power and data size  has led to a breakthrough
in the effectiveness of neural networks  and they have been used to obtain very impressive practical
performance on a variety of domains (a few recent examples include [17  16  24  10  7]).
A neural network can be described by a (directed acyclic) graph  where each vertex in the graph cor-
responds to a neuron and each edge is associated with a weight. Each neuron calculates a weighted
sum of the outputs of neurons which are connected to it (and possibly adds a bias term). It then
passes the resulting number through an activation function σ : R → R and outputs the resulting
number. We focus on feed-forward neural networks  where the neurons are arranged in layers  in
which the output of each layer forms the input of the next layer. Intuitively  the input goes through
several transformations  with higher-level concepts derived from lower-level ones. The depth of the
network is the number of layers and the size of the network is the total number of neurons.
From the perspective of statistical learning theory  by specifying a neural network architecture (i.e.
the underlying graph and the activation function) we obtain a hypothesis class  namely  the set of all
prediction rules obtained by using the same network architecture while changing the weights of the
network. Learning the class involves ﬁnding a speciﬁc set of weights  based on training examples 
which yields a predictor that has good performance on future examples. When studying a hypothesis
class we are usually concerned with three questions:

1. Sample complexity: how many examples are required to learn the class.
2. Expressiveness: what type of functions can be expressed by predictors in the class.
3. Training time: how much computation time is required to learn the class.

For simplicity  let us ﬁrst consider neural networks with a threshold activation function (i.e. σ(z) =
1 if z > 0 and 0 otherwise)  over the boolean input space  {0  1}d  and with a single output in
{0  1}. The sample complexity of such neural networks is well understood [3]. It is known that the
VC dimension grows linearly with the number of edges (up to log factors). It is also easy to see that
no matter what the activation function is  as long as we represent each weight of the network using

1

a constant number of bits  the VC dimension is bounded by a constant times the number of edges.
This implies that empirical risk minimization - or ﬁnding weights with small average loss over the
training data - can be an effective learning strategy from a statistical point of view.
As to the expressiveness of such networks  it is easy to see that neural networks of depth 2 and
sufﬁcient size can express all functions from {0  1}d to {0  1}. However  it is also possible to show
that for this to happen  the size of the network must be exponential in d (e.g. [19  Chapter 20]).
Which functions can we express using a network of polynomial size? The theorem below shows that
all boolean functions that can be calculated in time O(T (d))  can also be expressed by a network of
depth O(T (d)) and size O(T (d)2).
Theorem 1. Let T : N → N and for every d  let Fd be the set of functions that can be implemented
by a Turing machine using at most T (d) operations. Then there exist constants b  c ∈ R+ such that
for every d  there is a network architecture of depth c T (d) + b  size of (c T (d) + b)2  and threshold
activation function  such that the resulting hypotesis class contains Fd.

The proof of the theorem follows directly from the relation between the time complexity of programs
and their circuit complexity (see  e.g.  [22])  and the fact that we can simulate the standard boolean
gates using a ﬁxed number of neurons.
We see that from the statistical perspective  neural networks form an excellent hypothesis class; On
one hand  for every runtime T (d)  by using depth of O(T (d)) we contain all predictors that can be
run in time at most T (d). On the other hand  the sample complexity of the resulting class depends
polynomially on T (d).
The main caveat of neural networks is the training time. Existing theoretical results are mostly
negative  showing that successfully learning with these networks is computationally hard in the worst
case. For example  neural networks of depth 2 contain the class of intersection of halfspaces (where
the number of halfspaces is the number of neurons in the hidden layer). By reduction to k-coloring 
it has been shown that ﬁnding the weights that best ﬁt the training set is NP-hard ([9]). [6] has
shown that even ﬁnding weights that result in close-to-minimal empirical error is computationally
infeasible. These hardness results focus on proper learning  where the goal is to ﬁnd a nearly-optimal
predictor with a ﬁxed network architecture A. However  if our goal is to ﬁnd a good predictor  there
is no reason to limit ourselves to predictors with one particular architecture. Instead  we can try 
for example  to ﬁnd a network with a different architecture A(cid:48)  which is almost as good as the
best network with architecture A. This is an example of the powerful concept of improper learning 
which has often proved useful in circumventing computational hardness results. Unfortunately  there
are hardness results showing that even with improper learning  and even if the data is generated
exactly from a small  depth-2 neural network  there are no efﬁcient algorithms which can ﬁnd a
predictor that performs well on test data. In particular  [15] and [12] have shown this in the case of
learning intersections of halfspaces  using cryptographic and average case complexity assumptions.
On a related note  [4] recently showed positive results on learning from data generated by a neural
network of a certain architecture and randomly connected weights. However  the assumptions used
are strong and unlikely to hold in practice.
Despite this theoretical pessimism  in practice  modern-day neural networks are trained successfully
in many learning problems. There are several tricks that enable successful training:
• Changing the activation function: The threshold activation function  σ(a) = 1a>0  has zero
derivative almost everywhere. Therefore  we cannot apply gradient-based methods with this ac-
tivation function. To circumvent this problem  we can consider other activation functions. Most
widely known is a sigmoidal activation  e.g. σ(a) = 1
1+ea   which forms a smooth approxima-
tion of the threshold function. Another recent popular activation function is the rectiﬁed linear
unit (ReLU) function  σ(a) = max{0  a}. Note that subtracting a shifted ReLU from a ReLU
yields an approximation of the threshold function  so by doubling the number of neurons we can
approximate a network with threshold activation by a network with ReLU activation.

• Over-speciﬁcation: It was empirically observed that it is easier to train networks which are larger

than needed. Indeed  we empirically demonstrate this phenomenon in Sec. 5.

• Regularization: It was empirically observed that regularizing the weights of the network speeds

up the convergence (e.g. [16]).

2

The goal of this paper is to revisit and re-raise the question of neural network’s computational efﬁ-
ciency  from a modern perspective. This is a challenging topic  and we do not pretend to give any
deﬁnite answers. However  we provide several results  both positive and negative. Most of them are
new  although a few appeared in the literature in other contexts. Our contributions are as follows:
• We make a simple observation that for sufﬁciently over-speciﬁed networks  global optima are
ubiquitous and in general computationally easy to ﬁnd. Although this holds only for extremely
large networks which will overﬁt  it can be seen as an indication that the computational hard-
ness of learning does decrease with the amount of over-speciﬁcation. This is also demonstrated
empirically in Sec. 5.
• Motivated by the idea of changing the activation function  we consider the quadratic activation
function  σ(a) = a2. Networks with the quadratic activation compute polynomial functions of
the input in Rd  hence we call them polynomial networks. Our main ﬁndings for such networks
are as follows:
– Networks with quadratic activation are as expressive as networks with threshold activation.
– Constant depth networks with quadratic activation can be learned in polynomial time.
– Sigmoidal networks of depth 2  and with (cid:96)1 regularization  can be approximated by polynomial
networks of depth O(log log(1/)). It follows that sigmoidal networks with (cid:96)1 regularization
can be learned in polynomial time as well.

– The aforementioned positive results are interesting theoretically  but lead to impractical algo-
rithms. We provide a practical  provably correct  algorithm for training depth-2 polynomial
networks. While such networks can also be learned using a linearization trick  our algorithm is
more efﬁcient and returns networks whose size does not depend on the data dimension. Our al-
gorithm follows a forward greedy selection procedure  where each step of the greedy selection
procedure builds a new neuron by solving an eigenvalue problem.

– We generalize the above algorithm to depth-3  in which each forward greedy step involves an
efﬁcient approximate solution to a tensor approximation problem. The algorithm can learn a
rich sub-class of depth-3 polynomial networks.

– We describe some experimental evidence  showing that our practical algorithm is competitive

with state-of-the-art neural network training methods for depth-2 networks.

2 Sufﬁciently Over-Speciﬁed Networks Are Easy to Train

We begin by considering the idea of over-speciﬁcation  and make an observation that for sufﬁciently
over-speciﬁed networks  the optimization problem associated with training them is generally quite
easy to solve  and that global optima are in a sense ubiquitous. As an interesting contrast  note that
for very small networks (such as a single neuron with a non-convex activation function)  the associ-
ated optimization problem is generally hard  and can exhibit exponentially many local (non-global)
minima [5]. We emphasize that our observation only holds for extremely large networks  which will
overﬁt in any reasonable scenario  but it does point to a possible spectrum where computational cost
decreases with the amount of over-speciﬁcation.
To present the result  let X ∈ Rd m be a matrix of m training examples in Rd. We can think of the
network as composed of two mappings. The ﬁrst maps X into a matrix Z ∈ Rn m  where n is the
number of neurons whose outputs are connected to the output layer. The second mapping is a linear
mapping Z (cid:55)→ W Z  where W ∈ Ro n  that maps Z to the o neurons in the output layer. Finally 
there is a loss function (cid:96) : Ro m → R  which we’ll assume to be convex  that assesses the quality of
the prediction on the entire data (and will of course depend on the m labels). Let V denote all the
weights that affect the mapping from X to Z  and denote by f (V ) the function that maps V to Z.
The optimization problem associated with learning the network is therefore minW V (cid:96)(W f (V )).
The function (cid:96)(W f (V )) is generally non-convex  and may have local minima. However  if n ≥ m 
then it is reasonable to assume that Rank(f (V )) = m with large probability (under some random
choice of V )  due to the non-linear nature of the function computed by neural networks1. In that
case  we can simply ﬁx V and solve minW (cid:96)(W f (V ))  which is computationally tractable as (cid:96) is
1For example  consider the function computed by the ﬁrst layer  X (cid:55)→ σ(VdX)  where σ is a sigmoid

function. Since σ is non-linear  the columns of σ(VdX) will not be linearly dependent in general.

3

assumed to be convex. Since f (V ) has full rank  the solution of this problem corresponds to a global
optima of (cid:96)  and hence to a global optima of the original optimization problem. Thus  for sufﬁciently
large networks  ﬁnding global optima is generally easy  and they are in a sense ubiquitous.

3 The Hardness of Learning Neural Networks

We now review several known hardness results and apply them to our learning setting. For simplic-
ity  throughout most of this section we focus on the PAC model in the binary classiﬁcation case  over
the Boolean cube  in the realizable case  and with a ﬁxed target accuracy.2
Fix some   δ ∈ (0  1). For every dimension d  let the input space be Xd = {0  1}d and let H be a
hypothesis class of functions from Xd to {±1}. We often omit the subscript d when it is clear from
context. A learning algorithm A has access to an oracle that samples x according to an unknown
distribution D over X and returns (x  f∗(x))  where f∗ is some unknown target hypothesis in H.
The objective of the algorithm is to return a classiﬁer f : X → {±1}  such that with probability of
at least 1 − δ 

Px∼D [f (x) (cid:54)= f∗(x)] ≤ .

We say that A is efﬁcient if it runs in time poly(d) and the function it returns can also be evaluated
on a new instance in time poly(d). If there is such A  we say that H is efﬁciently learnable.
In the context of neural networks  every network architecture deﬁnes a hypothesis class  Nt n σ 
that contains all target functions f that can be implemented using a neural network with t layers  n
neurons (excluding input neurons)  and an activation function σ. The immediate question is which
Nt n σ are efﬁciently learnable. We will ﬁrst address this question for the threshold activation func-
tion  σ0 1(z) = 1 if z > 0 and 0 otherwise.
Observing that depth-2 networks with the threshold activation function can implement intersections
of halfspaces  we will rely on the following hardness results  due to [15].
Theorem 2 (Theorem 1.2 in [15]). Let X = {±1}d  let

(cid:0)w(cid:62)x − b − 1/2(cid:1) : b ∈ N  w ∈ Nd |b| + (cid:107)w(cid:107)1 ≤ poly(d)(cid:9)  

H a =(cid:8)x → σ0 1

k = {x → h1(x) ∧ h2(x) ∧ . . . ∧ hk(x) : ∀i  hi ∈ H a}  where k = dρ for some constant

and let H a
ρ > 0. Then under a certain cryptographic assumption  H a

k is not efﬁciently learnable.

σ0 1 ((cid:80)

Under a different complexity assumption  [12] showed a similar result even for k = ω(1).
As mentioned before  neural networks of depth ≥ 2 and with the σ0 1 activation function can
express intersections of halfspaces: For example  the ﬁrst layer consists of k neurons comput-
ing the k halfspaces  and the second layer computes their conjunction by the mapping x (cid:55)→
i xi − k + 1/2). Trivially  if some class H is not efﬁciently learnable  then any class con-
taining it is also not efﬁciently learnable. We thus obtain the following corollary:
Corollary 1. For every t ≥ 2  n = ω(1)  the class Nt n σ0 1 is not efﬁciently learnable (under the
complexity assumption given in [12]).

What happens when we change the activation function? In particular  two widely used activation
functions for neural networks are the sigmoidal activation function  σsig(z) = 1/(1 + exp(−z)) 
and the rectiﬁed linear unit (ReLU) activation function  σrelu(z) = max{z  0}.
As a ﬁrst observation  note that for |z| (cid:29) 1 we have that σsig(z) ≈ σ0 1(z). Our data domain is
the discrete Boolean cube  hence if we allow the weights of the network to be arbitrarily large  then
Nt n σ0 1 ⊆ Nt n σsig. Similarly  the function σrelu(z)−σrelu(z−1) equals σ0 1(z) for every |z| ≥ 1.
As a result  without restricting the weights  we can simulate each threshold activated neuron by two
ReLU activated neurons  which implies that Nt n σ0 1 ⊆ Nt 2n σrelu. Hence  Corollary 1 applies to
both sigmoidal networks and ReLU networks as well  as long as we do not regularize the weights of
the network.

2While we focus on the realizable case (i.e.  there exists f∗ ∈ H that provides perfect predictions)  with a
ﬁxed accuracy () and conﬁdence (δ)  since we are dealing with hardness results  the results trivially apply to
the agnostic case and to learning with arbitrarily small accuracy and conﬁdence parameters.

4

What happens when we do regularize the weights? Let Nt n σ L be all target functions that can be
implemented using a neural network of depth t  size n  activation function σ  and when we restrict
the input weights of each neuron to be (cid:107)w(cid:107)1 + |b| ≤ L.
One may argue that in many real world distributions  the difference between the two classes  Nt n σ L
and Nt n σ0 1 is small. Roughly speaking  when the distribution density is low around the decision
boundary of neurons (similarly to separation with margin assumptions)  then sigmoidal neurons will
be able to effectively simulate threshold activated neurons.
In practice  the sigmoid and ReLU activation functions are advantageous over the threshold activa-
tion function  since they can be trained using gradient based methods. Can these empirical successes
be turned into formal guarantees? Unfortunately  a closer examination of Thm. 2 demonstrates that
if L = Ω(d) then learning N2 n σsig L and N2 n σrelu L is still hard. Formally  to apply these net-
works to binary classiﬁcation  we follow a standard deﬁnition of learning with a margin assumption:
We assume that the learner receives examples of the form (x  sign(f∗(x))) where f∗ is a real-valued
function that comes from the hypothesis class  and we further assume that |f∗(x)| ≥ 1. Even under
this margin assumption  we have the following:
Corollary 2. For every t ≥ 2  n = ω(1)  L = Ω(d)  the classes Nt n σsig L and Nt n σrelu L are not
efﬁciently learnable (under the complexity assumption given in [12]).

A proof is provided in the appendix. What happens when L is much smaller? Later on in the paper
we will show positive results for L being a constant and the depth being ﬁxed. These results will be
obtained using polynomial networks  which we study in the next section.

4 Polynomial Networks

In the previous section we have shown several strong negative results for learning neural networks
with the threshold  sigmoidal  and ReLU activation functions. One way to circumvent these hardness
results is by considering another activation function. Maybe the simplest non-linear function is
the squared function  σ2(x) = x2. We call networks that use this activation function polynomial
networks  since they compute polynomial functions of their inputs. As in the previous section  we
denote by Nt n σ2 L the class of functions that can be implemented using a neural network of depth
t  size n  squared activation function  and a bound L on the (cid:96)1 norm of the input weights of each
neuron. Whenever we do not specify L we refer to polynomial networks with unbounded weights.
Below we study the expressiveness and computational complexity of polynomial networks. We
note that algorithms for efﬁciently learning (real-valued) sparse or low-degree polynomials has been
studied in several previous works (e.g. [13  14  8  2  1]). However  these rely on strong distributional
assumptions  such as the data instances having a uniform or log-concave distribution  while we are
interested in a distribution-free setting.

4.1 Expressiveness

We ﬁrst show that  similarly to networks with threshold activation  polynomial networks of polyno-
mial size can express all functions that can be implemented efﬁciently using a Turing machine.
Theorem 3 (Polynomial networks can express Turing Machines). Let Fd and T be as in Thm. 1.
Then there exist constants b  c ∈ R+ such that for every d  the class Nt n σ2 L  with t =
c T (d) log(T (d)) + b  n = t2  and L = b  contains Fd.
The proof of the theorem relies on the result of [18] and is given in the appendix.
Another relevant expressiveness result  which we will use later  shows that polynomial networks can
approximate networks with sigmoidal activation functions:
Theorem 4. Fix 0 <  < 1  L ≥ 3 and t ∈ N. There are Bt ∈ ˜O(log(tL + L log 1
Bn ∈ ˜O(tL + L log 1
that sup(cid:107)x(cid:107)∞<1 (cid:107)f (x) − g(x)(cid:107)∞ ≤ .
The proof relies on an approximation of the sigmoid function based on Chebyshev polynomials  as
was done in [21]  and is given in the appendix.

 )) and
 ) such that for every f ∈ Nt n σsig L there is a function g ∈ NtBt nBn σ2  such

5

4.2 Training Time

We now turn to the computational complexity of learning polynomial networks. We ﬁrst show that
it is hard to learn polynomial networks of depth Ω(log(d)).
Indeed  by combining Thm. 4 and
Corollary 2 we obtain the following:
Corollary 3. The class Nt n σ2  where t = Ω(log(d)) and n = Ω(d)  is not efﬁciently learnable.
On the ﬂip side  constant-depth polynomial networks can be learned in polynomial time  using a
simple linearization trick. Speciﬁcally  the class of polynomial networks of constant depth t is
contained in the class of multivariate polynomials of total degree at most s = 2t. This class can
be represented as a ds-dimensional linear space  where each vector is the coefﬁcient vector of some
such polynomial. Therefore  the class of polynomial networks of depth t can be learned in time
)  by mapping each instance vector x ∈ Rd to all of its monomials  and learning a linear
poly(d2t
predictor on top of this representation (which can be done efﬁciently in the realizable case  or when
a convex loss function is used). In particular  if t is a constant then so is 2t and therefore polynomial
networks of constant depth are efﬁciently learnable. Another way to learn this class is using support
vector machines with polynomial kernels.
An interesting application of this observation is that depth-2 sigmoidal networks are efﬁciently learn-
able with sufﬁcient regularization  as formalized in the result below. This contrasts with corollary 2 
which provides a hardness result without regularization.
Theorem 5. The class N2 n σsig L can be learned  to accuracy   in time poly(T ) where T =
(1/) · O(d4L ln(11L2+1)).
The idea of the proof is as follows. Suppose that we obtain data from some f ∈ N2 n σsig L. Based
on Thm. 4  there is g ∈ N2Bt nBn σ2 that approximates f to some ﬁxed accuracy 0 = 0.5  where Bt
and Bn are as deﬁned in Thm. 4 for t = 2. Now we can learn N2Bt nBn σ2 by considering the class
of all polynomials of total degree 22Bt  and applying the linearization technique discussed above.
Since f is assumed to separate the data with margin 1 (i.e. y = sign(f∗(x)) |f∗(x)| ≥ 1|)  then g
separates the data with margin 0.5  which is enough for establishing accuracy  in sample and time
that depends polynomially on 1/.

4.3 Learning 2-layer and 3-layer Polynomial Networks

While interesting theoretically  the above results are not very practical  since the time and sample
complexity grow very fast with the depth of the network.3 In this section we describe practical 
provably correct  algorithms for the special case of depth-2 and depth-3 polynomial networks  with
some additional constraints. Although such networks can be learned in polynomial time via explicit
linearization (as described in section 4.2)  the runtime and resulting network size scales quadratically
(for depth-2) or cubically (for depth-3) with the data dimension d. In contrast  our algorithms and
guarantees have a much milder dependence on d.
We ﬁrst consider 2 layer polynomial networks  of the following form:

(cid:40)

P2 k =

x (cid:55)→ b + w(cid:62)

0 x +

αi(w(cid:62)

k(cid:88)

(cid:41)
i x)2 : ∀i ≥ 1 |αi| ≤ 1 (cid:107)wi(cid:107)2 = 1

.

i=1

This networks corresponds to one hidden layer containing r neurons with the squared activation
function  where we restrict the input weights of all neurons in the network to have bounded (cid:96)2 norm 
and where we also allow a direct linear dependency between the input layer and the output layer.
We’ll describe an efﬁcient algorithm for learning this class  which is based on the GECO algorithm
for convex optimization with low-rank constraints [20].

3If one uses SVM with polynomial kernels  the time and sample complexity may be small under margin
assumptions in a feature space corresponding to a given kernel. Note  however  that large margin in that space
is very different than the assumption we make here  namely  that there is a network with a small number of
hidden neurons that works well on the data.

6

The goal of the algorithm is to ﬁnd f that minimizes the objective

m(cid:88)

i=1

R(f ) =

1
m

(cid:96)(f (xi)  yi) 

(1)

where (cid:96) : R × R → R is a loss function. We’ll assume that (cid:96) is β-smooth and convex.
The basic idea of the algorithm is to gradually add hidden neurons to the hidden layer  in a greedy
manner  so as to decrease the loss function over the data. To do so  deﬁne V = {x (cid:55)→ (w(cid:62)x)2 :
(cid:107)w(cid:107)2 = 1} the set of functions that can be implemented by hidden neurons. Then every f ∈ P2 r
is an afﬁne function plus a weighted sum of functions from V. The algorithm starts with f being
the minimizer of R over all afﬁne functions. Then at each greedy step  we search for g ∈ V that
minimizes a ﬁrst order approximation of R(f + ηg):

m(cid:88)

i=1

R(f + ηg) ≈ R(f ) + η

1
m

(cid:96)(cid:48)(f (xi)  yi)g(xi)  

(2)

i

m

m

(cid:80)m
i=1 (cid:96)(cid:48)(f (xi)  yi)xix(cid:62)

i

(cid:1) w . The vector w that minimizes this
(cid:1).

be rewritten as R(f ) + η w(cid:62)(cid:0) 1
expression (for positive η) is the leading eigenvector of the matrix(cid:0) 1

where (cid:96)(cid:48) is the derivative of (cid:96) w.r.t. its ﬁrst argument. Observe that for every g ∈ V there is some w
with (cid:107)w(cid:107)2 = 1 for which g(x) = (w(cid:62)x)2 = w(cid:62)xx(cid:62)w. Hence  the right-hand side of Eq. (2) can
(cid:80)m
i=1 (cid:96)(cid:48)(f (xi)  yi)xix(cid:62)
We add this vector as a hidden neuron to the network.4 Finally  we minimize R w.r.t. the weights
from the hidden layer to the output layer (namely  w.r.t. the weights αi).
The following theorem  which follows directly from Theorem 1 of [20]  provides convergence guar-
antee for GECO. Observe that the theorem gives guarantee for learning P2 k if we allow to output
an over-speciﬁed network.
Theorem 6. Fix some  > 0. Assume that the loss function is convex and β-smooth. Then if
iterations  it outputs a network f ∈ N2 r σ2 for which
the GECO Algorithm is run for r > 2βk2
R(f ) ≤ minf∗∈P2 k R(f∗) + .

(cid:110)
x (cid:55)→(cid:81)i
We next consider a hypothesis class consisting of third degree polynomials  which is a subset of
3-layer polynomial networks (see Lemma 1 in the appendix) . The hidden neurons will be functions
i=1 αigi(x) : ∀i  |αi| ≤ 1  gi ∈ V(cid:111)
from the class: V = ∪3
. The hypothesis
class we consider is P3 k =
The basic idea of the algorithm is the same as for 2-layer networks. However  while in the 2-layer
case we could implement efﬁciently each greedy step by solving an eigenvalue problem  we now
face the following tensor approximation problem at each greedy step:

(cid:111)
j x) : ∀j  (cid:107)wj(cid:107)2 = 1

i=1Vi where Vi =

(cid:110)
x (cid:55)→(cid:80)k

j=1(w(cid:62)

.

max
g∈V3

1
m

(cid:96)(cid:48)(f (xi)  yi)g(xi) =

max

(cid:107)w(cid:107)=1 (cid:107)u(cid:107)=1 (cid:107)v(cid:107)=1

1
m

(cid:96)(cid:48)(f (xi)  yi)(w(cid:62)xi)(u(cid:62)xi)(v(cid:62)xi) .

While this is in general a hard optimization problem  we can approximate it – and luckily  an approx-
imate greedy step sufﬁces for success of the greedy procedure. This procedure is given in Figure 1 
and is again based on an approximate eigenvector computation. A guarantee for the quality of ap-
proximation is given in the appendix  and this leads to the following theorem  whose proof is given
in the appendix.
Theorem 7. Fix some δ   > 0. Assume that the loss function is convex and β-smooth. Then if the
GECO Algorithm is run for r > 4dβk2
(1−τ )2 iterations  where each iteration relies on the approximation
procedure given in Fig. 1  then with probability (1−δ)r  it outputs a network f ∈ N3 5r σ2 for which
R(f ) ≤ minf∗∈P3 k R(f∗) + .

4It is also possible to ﬁnd an approximate solution to the eigenvalue problem and still retain the performance
guarantees (see [20]). Since an approximate eigenvalue can be found in time O(d) using the power method  we
obtain the runtime of GECO depends linearly on d.

7

m(cid:88)

i=1

m(cid:88)

i=1

Input: {xi}m
Output: A 1−τ√

i=1 ∈ Rd α ∈ Rm  τ δ

approximate solution to
(cid:62)
αi(w

d
F (w  u  v) =

(cid:88)

i

max

xi)(u

xi)(v
(cid:107)w(cid:107) (cid:107)u(cid:107) (cid:107)v(cid:107)=1
Pick randomly w1  . . .   ws iid according to N (0  Id).
For t = 1  . . .   2d log 1
δ
i αi(w(cid:62)

i and set ut  vt s.t:

t Avt) ≥ (1 − τ ) max(cid:107)u(cid:107) (cid:107)v(cid:107)=1 T r(u(cid:62)Av).
Return w  u  v the maximizers of maxi≤s F (wi  ui  ui).

Let A =(cid:80)
wt ← wt(cid:107)wt(cid:107)
T r(u(cid:62)

t xi)xix(cid:62)

(cid:62)

(cid:62)

xi)

Figure 1: Approximate tensor maximization.

5 Experiments

0.1

r
o
r
r
E

0

0.2

0.8

GECO

6 · 10−2

9 · 10−2

7 · 10−2

5 · 10−2

8 · 10−2

SGD ReLU
SGD Squared

To demonstrate the practicality of GECO to train neural networks for real world problems  we con-
sidered a pedestrian detection problem as follows. We collected 200k training examples of image
patches of size 88x40 pixels containing either pedestrians (positive examples) or hard negative ex-
amples (containing images that were classiﬁed as pedestrians by applying a simple linear classiﬁer in
a sliding window manner). See a few examples of images above. We used half of the examples as a
training set and the other half as a test set. We calculated HoG
features ([11]) from the images5. We then trained  using GECO 
a depth-2 polynomial network on the resulting features. We
used 40 neurons in the hidden layer. For comparison we trained
the same network architecture (i.e. 40 hidden neurons with a
squared activation function) by SGD. We also trained a similar
network (40 hidden neurons again) with the ReLU activation
function. For the SGD implementation we tried the following
tricks to speed up the convergence: heuristics for initialization
of the weights  learning rate rules  mini-batches  Nesterov’s mo-
mentum (as explained in [23])  and dropout. The test errors of
SGD as a function of the number of iterations are depicted on
the top plot of the Figure on the side. We also mark the perfor-
mance of GECO as a straight line (since it doesn’t involve SGD
iterations). As can be seen  the error of GECO is slightly bet-
ter than SGD. It should be also noted that we had to perform a
very large number of SGD iterations to obtain a good solution 
while the runtime of GECO was much faster. This indicates that
GECO may be a valid alternative approach to SGD for training
depth-2 networks. It is also apparent that the squared activation
function is slightly better than the ReLU function for this task.
The second plot of the side ﬁgure demonstrates the beneﬁt of
over-speciﬁcation for SGD. We generated random examples in R150 and passed them through a
random depth-2 network that contains 60 hidden neurons with the ReLU activation function. We
then tried to ﬁt a new network to this data with over-speciﬁcation factors of 1  2  4  8 (e.g.  over-
speciﬁcation factor of 4 means that we used 60 · 4 = 240 hidden neurons). As can be clearly seen 
SGD converges much faster when we over-specify the network.
Acknowledgements: This research is supported by Intel (ICRI-CI). OS was also supported by
an ISF grant (No. 425/13)  and a Marie-Curie Career Integration Grant. SSS and RL were also
supported by the MOS center of Knowledge for AI and ML (No. 3-9243). RL is a recipient of the
Google Europe Fellowship in Learning Theory  and this research is supported in part by this Google
Fellowship. We thank Itay Safran for spotting a mistake in a previous version of Sec. 2 and to James
Martens for helpful discussions.

0.4
0.6
iterations

0.6
0.4
#iterations

1
·105

1
·105

4

3

2

1

0

0

0.2

E
S
M

1
2
4
8

0.8

5Using the Matlab implementation provided in http://www.mathworks.com/matlabcentral/

fileexchange/33863-histograms-of-oriented-gradients.

8

References
[1] A. Andoni  R. Panigrahy  G. Valiant  and L. Zhang. Learning polynomials with neural net-

works. In ICML  2014.

[2] A. Andoni  R. Panigrahy  G. Valiant  and L. Zhang. Learning sparse polynomial functions. In

SODA  2014.

[3] M. Anthony and P. Bartlett. Neural Network Learning - Theoretical Foundations. Cambridge

University Press  2002.

[4] S. Arora  A. Bhaskara  R. Ge  and T. Ma. Provable bounds for learning some deep representa-

tions. arXiv preprint arXiv:1310.6343  2013.

[5] P. Auer  M. Herbster  and M. Warmuth. Exponentially many local minima for single neurons.

In NIPS  1996.

[6] P. L. Bartlett and S. Ben-David. Hardness results for neural network approximation problems.

Theor. Comput. Sci.  284(1):53–66  2002.

[7] Y. Bengio  A. Courville  and P. Vincent. Representation learning: A review and new per-
spectives. IEEE Transactions on Pattern Analysis and Machine Intelligence  35:1798–1828 
2013.

[8] E. Blais  R. O’Donnell  and K. Wimmer. Polynomial regression under arbitrary product distri-

butions. Machine Learning  80(2-3):273–294  2010.

[9] A. Blum and R. Rivest. Training a 3-node neural network is np-complete. Neural Networks 

5(1):117–127  1992.

[10] G. Dahl  T. Sainath  and G. Hinton. Improving deep neural networks for lvcsr using rectiﬁed

linear units and dropout. In ICASSP  2013.

[11] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR  2005.
[12] A. Daniely  N. Linial  and S. Shalev-Shwartz. From average case complexity to improper

learning complexity. In FOCS  2014.

[13] A. Kalai  A. Klivans  Y. Mansour  and R. Servedio. Agnostically learning halfspaces. SIAM J.

Comput.  37(6):1777–1805  2008.

[14] A. Kalai  A. Samorodnitsky  and S.-H. Teng. Learning and smoothed analysis. In FOCS  2009.
[15] A. Klivans and A. Sherstov. Cryptographic hardness for learning intersections of halfspaces.

In FOCS  2006.

[16] A. Krizhevsky  I. Sutskever  and G. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS  2012.

[17] Q. V. Le  M.-A. Ranzato  R. Monga  M. Devin  G. Corrado  K. Chen  J. Dean  and A. Y. Ng.

Building high-level features using large scale unsupervised learning. In ICML  2012.

[18] N. Pippenger and M. Fischer. Relations among complexity measures. Journal of the ACM

(JACM)  26(2):361–381  1979.

[19] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  2014.

[20] S. Shalev-Shwartz  A. Gonen  and O. Shamir. Large-scale convex minimization with a low-

rank constraint. In ICML  2011.

[21] S. Shalev-Shwartz  O. Shamir  and K. Sridharan. Learning kernel-based halfspaces with the

0-1 loss. SIAM Journal on Computing  40(6):1623–1646  2011.

[22] M. Sipser. Introduction to the Theory of Computation. Thomson Course Technology  2006.
[23] I. Sutskever  J. Martens  G. Dahl  and G. Hinton. On the importance of initialization and

momentum in deep learning. In ICML  2013.

[24] M. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. arXiv

preprint arXiv:1311.2901  2013.

9

,Roi Livni
Shai Shalev-Shwartz
Ohad Shamir