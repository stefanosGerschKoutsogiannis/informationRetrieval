2016,Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning,In PU learning  a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing  it sometimes outperforms PN learning (i.e.  ordinary supervised learning). Hitherto  neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper  we theoretically compare PU (and NU) learning against PN learning based on the upper bounds on estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning  and we prove that  in terms of the upper bounds  either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly.,Theoretical Comparisons of Positive-Unlabeled
Learning against Positive-Negative Learning

Gang Niu1 Marthinus C. du Plessis1 Tomoya Sakai1 Yao Ma3 Masashi Sugiyama2 1

{ gang@ms.  christo@ms.  sakai@ms.  yao@ms.  sugi@ }k.u-tokyo.ac.jp

1The University of Tokyo  Japan

2RIKEN  Japan 3Boston University  USA

Abstract

In PU learning  a binary classiﬁer is trained from positive (P) and unlabeled (U) data
without negative (N) data. Although N data is missing  it sometimes outperforms
PN learning (i.e.  ordinary supervised learning). Hitherto  neither theoretical nor
experimental analysis has been given to explain this phenomenon. In this paper 
we theoretically compare PU (and NU) learning against PN learning based on the
upper bounds on estimation errors. We ﬁnd simple conditions when PU and NU
learning are likely to outperform PN learning  and we prove that  in terms of the
upper bounds  either PU or NU learning (depending on the class-prior probability
and the sizes of P and N data) given inﬁnite U data will improve on PN learning.
Our theoretical ﬁndings well agree with the experimental results on artiﬁcial and
benchmark data even when the experimental setup does not match the theoretical
assumptions exactly.

1

Introduction

Positive-unlabeled (PU) learning  where a binary classiﬁer is trained from P and U data  has drawn
considerable attention recently [1  2  3  4  5  6  7  8]. It is appealing to not only the academia but also
the industry  since for example the click-through data automatically collected in search engines are
highly PU due to position biases [9  10  11]. Although PU learning uses no negative (N) data  it is
sometimes even better than PN learning (i.e.  ordinary supervised learning  perhaps with class-prior
change [12]) in practice. Nevertheless  there is neither theoretical nor experimental analysis for this
phenomenon  and it is still an open problem when PU learning is likely to outperform PN learning.
We clarify this question in this paper.

Problem settings For PU learning  there are two problem settings based on one sample (OS) and
two samples (TS) of data respectively. More speciﬁcally  let X ∈ Rd and Y ∈ {±1} (d ∈ N) be the
input and output random variables and equipped with an underlying joint density p(x  y). In OS [3] 
a set of U data is sampled from the marginal density p(x). Then if a data point x is P  this P label is
observed with probability c  and x remains U with probability 1 − c; if x is N  this N label is never
observed  and x remains U with probability 1. In TS [4]  a set of P data is drawn from the positive
marginal density p(x | Y = +1) and a set of U data is drawn from p(x). Denote by n+ and nu the
sizes of P and U data. As two random variables  they are fully independent in TS  and they satisfy
n+/(n+ + nu) ≈ cπ in OS where π = p(Y = +1) is the class-prior probability. Therefore  TS is
slightly more general than OS  and we will focus on TS problem settings.
Similarly  consider TS problem settings of PN and NU learning  where a set of N data (of size n−) is
sampled from p(x | Y = −1) independently of the P/U data. For PN learning  if we enforce that
n+/(n+ + n−) ≈ π when sampling the data  it will be ordinary supervised learning; otherwise  it is
supervised learning with class-prior change  a.k.a. prior probability shift [12].

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In [7]  a cost-sensitive formulation for PU learning was proposed  and its risk estimator was proven
unbiased if the surrogate loss is non-convex and satisﬁes a symmetric condition. Therefore  we can
naturally compare empirical risk minimizers in PU and NU learning against that in PN learning.

Contributions We establish risk bounds of three risk minimizers in PN  PU and NU learning for
comparisons in a ﬂavor of statistical learning theory [13  14]. For each minimizer  we ﬁrstly derive
a uniform deviation bound from the risk estimator to the risk using Rademacher complexities (see 
e.g.  [15  16  17  18])  and secondly obtain an estimation error bound. Thirdly  if the surrogate loss
is classiﬁcation-calibrated [19]  an excess risk bound is an immediate corollary. In [7]  there was a
generalization error bound similar to our uniform deviation bound for PU learning. However  it is
based on a tricky decomposition of the risk  where surrogate losses for risk minimization and risk
analysis are different and labels of U data are needed for risk evaluation  so that no further bound is
implied. On the other hand  ours utilizes the same surrogate loss for risk minimization and analysis
and requires no label of U data for risk evaluation  so that an estimation error bound is possible.
Our main results can be summarized as follows. Denote by ˆgpn  ˆgpu and ˆgnu the risk minimizers in
PN  PU and NU learning. Under a mild assumption on the function class and data distributions 
• Finite-sample case: The estimation error bound of ˆgpu is tighter than that of ˆgpn whenever
√
√
nu < (1 − π)/
n−  and so is the bound of ˆgnu tighter than that of ˆgpn if
√
√
π/
(1 − π)/
n+.
nu < π/
• Asymptotic case: Either the limit of bounds of ˆgpu or that of ˆgnu (depending on π  n+ and
n−) will improve on that of ˆgpn  if n+  n− → ∞ in the same order and nu → ∞ faster in
order than n+ and n−.

√
n− + 1/

n+ + 1/

√

Notice that both results rely on only the constant π and variables n+  n− and nu; they are simple and
independent of the speciﬁc forms of the function class and/or the data distributions. The asymptotic
case is from the ﬁnite-sample case that is based on theoretical comparisons of the aforementioned
upper bounds on the estimation errors of ˆgpn  ˆgpu and ˆgnu. To the best of our knowledge  this is the
ﬁrst work that compares PU learning against PN learning.
Throughout the paper  we assume that the class-prior probability π is known. In practice  it can be
effectively estimated from P  N and U data [20  21  22] or only P and U data [23  24].

Organization The rest of this paper is organized as follows. Unbiased estimators are reviewed in
Section 2. Then in Section 3 we present our theoretical comparisons based on risk bounds. Finally
experiments are discussed in Section 4.

2 Unbiased estimators to the risk
For convenience  denote by p+(x) = p(x | Y = +1) and p−(x) = p(x | Y = −1) partial marginal
densities. Recall that instead of data sampled from p(x  y)  we consider three sets of data X+  X−
and Xu which are drawn from three marginal densities p+(x)  p−(x) and p(x) independently.
Let g : Rd → R be a real-valued decision function for binary classiﬁcation and (cid:96) : R × {±1} → R
be a Lipschitz-continuous loss function. Denote by

R+(g) = E+[(cid:96)(g(X)  +1)]  R−(g) = E−[(cid:96)(g(X) −1)]

partial risks  where E±[·] = EX∼p± [·]. Then the risk of g w.r.t. (cid:96) under p(x  y) is given by

(1)
In PN learning  by approximating R(g) based on Eq. (1)  we can get an empirical risk estimator as

R(g) = E(X Y )[(cid:96)(g(X)  Y )] = πR+(g) + (1 − π)R−(g).

(cid:98)Rpn(g) = π

n+

(cid:80)

(cid:80)
xj∈X− (cid:96)(g(xj) −1).

√

xi∈X+

(cid:96)(g(xi)  +1) + 1−π

√
n+ + 1/

For any ﬁxed g  (cid:98)Rpn(g) is an unbiased and consistent estimator to R(g) and its convergence rate is

n−) according to the central limit theorem [25]  where Op denotes the
of order Op(1/
order in probability.
In PU learning  X− is not available and then R−(g) cannot be directly estimated. However  [7] has
shown that we can estimate R(g) without any bias if (cid:96) satisﬁes the following symmetric condition:
(2)

(cid:96)(t  +1) + (cid:96)(t −1) = 1.

n−

2

(cid:98)Rpu(g) = −π + 2π

(cid:80)

Speciﬁcally  let Ru −(g) = EX [(cid:96)(g(X) −1)] = πE+[(cid:96)(g(X) −1)] + (1 − π)R−(g) be a risk that
U data are regarded as N data. Given Eq. (2)  we have E+[(cid:96)(g(X) −1)] = 1 − R+(g)  and hence
(3)

R(g) = 2πR+(g) + Ru −(g) − π.

By approximating R(g) based on (3) using X+ and Xu  we can obtain

n+

xi∈X+

(cid:96)(g(xi)  +1) + 1
nu

Although (cid:98)Rpu(g) regards Xu as N data and aims at separating X+ and Xu if being minimized  it is

√
an unbiased and consistent estimator to R(g) with a convergence rate Op(1/
nu) [25].
Similarly  in NU learning R+(g) cannot be directly estimated. Let Ru +(g) = EX [(cid:96)(g(X)  +1)] =
πR+(g) + (1 − π)E−[(cid:96)(g(X)  +1)]. Given Eq. (2)  E−[(cid:96)(g(X)  +1)] = 1 − R−(g)  and

√
n+ + 1/

xj∈Xu

(cid:96)(g(xj) −1).

R(g) = Ru +(g) + 2(1 − π)R−(g) − (1 − π).

(4)

(cid:80)

By approximating R(g) based on (4) using Xu and X−  we can obtain

(cid:98)Rnu(g) = −(1 − π) + 1

nu

(cid:80)

xi∈Xu

(cid:96)(g(xi)  +1) + 2(1−π)

n−

(cid:80)
xj∈X− (cid:96)(g(xj) −1).

On the loss function In order to train g by minimizing these estimators  it remains to specify the
loss (cid:96). The zero-one loss (cid:96)01(t  y) = (1 − sign(ty))/2 satisﬁes (2) but is non-smooth. [7] proposed
to use a scaled ramp loss as the surrogate loss for (cid:96)01 in PU learning:
(cid:96)sr(t  y) = max{0  min{1  (1 − ty)/2}} 

instead of the popular hinge loss that does not satisfy (2). Let I(g) = E(X Y )[(cid:96)01(g(X)  Y )] be the
risk of g w.r.t. (cid:96)01 under p(x  y). Then  (cid:96)sr is neither an upper bound of (cid:96)01 so that I(g) ≤ R(g) is
not guaranteed  nor a convex loss so that it gets more difﬁcult to know whether (cid:96)sr is classiﬁcation-
calibrated or not [19].1 If it is  we are able to control the excess risk w.r.t. (cid:96)01 by that w.r.t. (cid:96). Here
we prove the classiﬁcation calibration of (cid:96)sr  and consequently it is a safe surrogate loss for (cid:96)01.
Theorem 1. The scaled ramp loss (cid:96)sr is classiﬁcation-calibrated (see Appendix A for the proof).

3 Theoretical comparisons based on risk bounds
be the optimal decision function in G  ˆgpn = arg ming∈G (cid:98)Rpn(g)  ˆgpu = arg ming∈G (cid:98)Rpu(g)  and
When learning is involved  suppose we are given a function class G  and let g∗ = arg ming∈G R(g)
ˆgnu = arg ming∈G (cid:98)Rnu(g) be arbitrary global minimizers to three risk estimators. Furthermore  let

R∗ = inf g R(g) and I∗ = inf g I(g) denote the Bayes risks w.r.t. (cid:96) and (cid:96)01  where the inﬁmum of g
is over all measurable functions.
In this section  we derive and compare risk bounds of three risk minimizers ˆgpn  ˆgpu and ˆgnu under
the following mild assumption on G  p(x)  p+(x) and p−(x): There is a constant CG > 0 such that
(5)

Rn q(G) ≤ CG/

√

n

for any marginal density q(x) ∈ {p(x)  p+(x)  p−(x)}  where

Rn q(G) = EX∼qnEσ

(cid:2)supg∈G 1

n

(cid:80)

xi∈X σig(xi)(cid:3)

is the Rademacher complexity of G for the sampling of size n from q(x) (that is  X = {x1  . . .   xn}
and σ = {σ1  . . .   σn}  with each xi drawn from q(x) and each σi as a Rademacher variable) [18].
A special case is covered  namely  sets of hyperplanes with bounded normals and feature maps:

G = {g(x) = (cid:104)w  φ(x)(cid:105)H | (cid:107)w(cid:107)H ≤ Cw (cid:107)φ(x)(cid:107)H ≤ Cφ} 

(6)
where H is a Hilbert space with an inner product (cid:104)· ·(cid:105)H  w ∈ H is a normal vector  φ : Rd → H is a
feature map  and Cw > 0 and Cφ > 0 are constants [26].

1A loss function (cid:96) is classiﬁcation-calibrated if and only if there is a convex  invertible and nondecreasing

transformation ψ(cid:96) with ψ(cid:96)(0) = 0  such that ψ(cid:96)(I(g) − inf g I(g)) ≤ R(g) − inf g R(g) [19].

3

3.1 Risk bounds

Let L(cid:96) be the Lipschitz constant of (cid:96) in its ﬁrst parameter. To begin with  we establish the learning
guarantee of ˆgpu (the proof can be found in Appendix A).
Theorem 2. Assume (2). For any δ > 0  with probability at least 1 − δ 2

R(ˆgpu) − R(g∗) ≤ 8πL(cid:96)Rn+ p+(G) + 4L(cid:96)Rnu p(G) + 2π

(7)
where Rn+ p+(G) and Rnu p(G) are the Rademacher complexities of G for the sampling of size n+
from p+(x) and the sampling of size nu from p(x). Moreover  if (cid:96) is a classiﬁcation-calibrated loss 
there exists nondecreasing ϕ with ϕ(0) = 0  such that with probability at least 1 − δ 
I(ˆgpu)−I∗ ≤ ϕ

R(g∗)−R∗+8πL(cid:96)Rn+ p+(G)+4L(cid:96)Rnu p(G)+2π

(cid:113) 2 ln(4/δ)

. (8)

(cid:16)

(cid:17)

+

+

n+

 

n+

nu

nu

(cid:113) 2 ln(4/δ)
(cid:113) 2 ln(4/δ)

(cid:113) 2 ln(4/δ)

In Theorem 2  R(ˆgpu) and I(ˆgpu) are w.r.t. p(x  y)  though ˆgpu is trained from two samples following
p+(x) and p(x). We can see that (7) is an upper bound of the estimation error of ˆgpu w.r.t. (cid:96)  whose
right-hand side (RHS) is small if G is small; (8) is an upper bound of the excess risk of ˆgpu w.r.t. (cid:96)01 
whose RHS also involves the approximation error of G (i.e.  R(g∗) − R∗) that is small if G is large.
√
When G is ﬁxed and satisﬁes (5)  we have Rn+ p+(G) = O(1/
nu) 
and then
√
in Op(1/
√
those complexities of G vanish slower in order than O(1/

R(ˆgpu) − R(g∗) → 0 
√
nu). On the other hand  when the size of G grows with n+ and nu properly 

√
n+) and Rnu p(G) = O(1/

I(ˆgpu) − I∗ → ϕ(R(g∗) − R∗)

nu) but we may have

n+) and O(1/

n+ + 1/

√

R(ˆgpu) − R(g∗) → 0 
√

I(ˆgpu) − I∗ → 0 

n+ + 1/

nu) due to the growth of G.

√
which means ˆgpu approaches the Bayes classiﬁer if (cid:96) is a classiﬁcation-calibrated loss  in an order
slower than Op(1/
Similarly  we can derive the learning guarantees of ˆgpn and ˆgnu for comparisons. We will just focus
on estimation error bounds  because excess risk bounds are their immediate corollaries.
Theorem 3. Assume (2). For any δ > 0  with probability at least 1 − δ 
R(ˆgpn) − R(g∗) ≤ 4πL(cid:96)Rn+ p+(G) + 4(1 − π)L(cid:96)Rn− p− (G) + π
where Rn− p−(G) is the Rademacher complexity of G for the sampling of size n− from p−(x).
Theorem 4. Assume (2). For any δ > 0  with probability at least 1 − δ 
R(ˆgnu)−R(g∗) ≤ 4L(cid:96)Rnu p(G)+8(1−π)L(cid:96)Rn− p− (G)+
In order to compare the bounds  we simplify (9)  (7) and (10) using Eq. (5). To this end  we deﬁne

f (δ) = 4L(cid:96)CG +(cid:112)2 ln(4/δ). For the special case of G deﬁned in (6)  deﬁne f (δ) accordingly as
f (δ) = 4L(cid:96)CwCφ +(cid:112)2 ln(4/δ).

Corollary 5. The estimation error bounds below hold separately with probability at least 1 − δ:

(cid:113) 2 ln(4/δ)

(cid:113) 2 ln(4/δ)

(cid:113) 2 ln(4/δ)

(cid:113) 2 ln(4/δ)

+2(1−π)

+ (1 − π)

. (10)

 
(9)

n−

n−

n+

nu

√
√
n−} 
R(ˆgpn) − R(g∗) ≤ f (δ) · {π/
n+ + (1 − π)/
√
√
R(ˆgpu) − R(g∗) ≤ f (δ) · {2π/
nu} 
n+ + 1/
√
√
R(ˆgnu) − R(g∗) ≤ f (δ) · {1/
nu + 2(1 − π)/

n−}.

(11)
(12)
(13)

3.2 Finite-sample comparisons

Note that three risk minimizers ˆgpn  ˆgpu and ˆgnu work in similar problem settings and their bounds
in Corollary 5 are proven using exactly the same proof technique. Then  the differences in bounds
reﬂect the intrinsic differences between risk minimizers. Let us compare those bounds. Deﬁne

αpu pn =(cid:0)π/
αnu pn =(cid:0)(1 − π)/

√

√
n+ + 1/

nu

√

n− + 1/

(cid:1) /(cid:0)(1 − π)/
(cid:1) /(cid:0)π/

√

nu

√
√

n−(cid:1)  
(cid:1) .

n+

(14)
(15)

Eqs. (14) and (15) constitute our ﬁrst main result.

2Here  the probability is over repeated sampling of data for training ˆgpu  while in Lemma 8  it will be for

evaluating (cid:98)Rpu(g).

4

Table 1: Properties of αpu pn and αnu pn.

no speciﬁcation

sizes are proportional

ρpn = π/(1 − π)

mono. inc. mono. dec. mono. inc. mono. dec. mono. inc.

αpu pn
αnu pn

π  n−
n+

n+  nu
π  n−  nu

π  ρpu
ρpn  ρnu

ρpn
π

ρpu
ρnu

2(cid:112)ρpu +
2(cid:112)ρnu +

minimum
√
√

ρpu
ρnu

Theorem 6 (Finite-sample comparisons). Assume (5) is satisﬁed. Then the estimation error bound
of ˆgpu in (12) is tighter than that of ˆgpn in (11) if and only if αpu pn < 1; also  the estimation error
bound of ˆgnu in (13) is tighter than that of ˆgpn if and only if αnu pn < 1.
Proof. Fix π  n+  n− and nu  and then denote by Vpn  Vpu and Vnu the values of the RHSs of (11) 
(12) and (13). In fact  the deﬁnitions of αpu pn and αnu pn in (14) and (15) came from

√
Vpu − πf (δ)/
√
Vpn − πf (δ)/

n+
n+

  αnu pn =

√
Vnu − (1 − π)f (δ)/
n−
√
Vpn − (1 − π)f (δ)/
n−

.

αpu pn =

As a consequence  compared with Vpn  Vpu is smaller and (12) is tighter if and only if αpu pn < 1 
and Vnu is smaller and (13) is tighter if and only if αnu pn < 1.

We analyze some properties of αpu pn before going to our second main result. The most important
property is that it relies on π  n+  n− and nu only; it is independent of G  p(x  y)  p(x)  p+(x) and
p−(x) as long as (5) is satisﬁed. Next  αpu pn is obviously a monotonic function of π  n+  n− and
nu. Furthermore  it is unbounded no matter if π is ﬁxed or not. Properties of αnu pn are similar  as
summarized in Table 1.
Implications of the monotonicity of αpu pn are given as follows. Intuitively  when other factors are
ﬁxed  larger nu or n− improves ˆgpu or ˆgpn respectively. However  it is complicated why αpu pn is
monotonically decreasing with n+ and increasing with π. The weights of the empirical average of

X+ is 2π in (cid:98)Rpu(g) and π in (cid:98)Rpn(g)  as in (cid:98)Rpu(g) it also joins the estimation of (1 − π)R−(g). It
makes X+ more important for (cid:98)Rpu(g)  and thus larger n+ improves ˆgpu more than ˆgpn. Moreover 
(1 − π)R−(g) is directly estimated in (cid:98)Rpn(g) and the concentration Op((1 − π)/
π is larger  whereas it is indirectly estimated through Ru −(g) − π(1 − R+(g)) in (cid:98)Rpu(g) and the

n−) is better if

nu) is worse if π is larger. As a result  when the sample sizes are

√
concentration Op(π/
ﬁxed ˆgpu is more (or less) favorable as π decreases (or increases).
A natural question is what the monotonicity of αpu pn would be if we enforce n+  n− and nu to be
proportional. To answer this question  we assume n+/n− = ρpn  n+/nu = ρpu and n−/nu = ρnu
where ρpn  ρpu and ρnu are certain constants  then (14) and (15) can be rewritten as
√
ρnu)/(π/

ρpn)  αnu pn = (1 − π +

√
ρpu)/((1 − π)

√
n+ + 1/

αpu pn = (π +

ρpn).

√

√

√

As shown in Table 1  αpu pn is now increasing with ρpu and decreasing with ρpn. It is because  for
instance  when ρpn is ﬁxed and ρpu increases  nu is meant to decrease relatively to n+ and n−.
Finally  the properties will dramatically change if we enforce ρpn = π/(1 − π) that approximately
holds in ordinary supervised learning. Under this constraint  we have

αpu pn = (π +

ρpu)/(cid:112)π(1 − π) ≥ 2(cid:112)ρpu +

√

ρpu 

√
√

√

where the equality is achieved at ¯π =
ρpu + 1). Here  αpu pn decreases with π if π < ¯π
and increases with π if π > ¯π  though it is not convex in π. Only if nu is sufﬁciently larger than n+
(e.g.  ρpu < 0.04)  could αpu pn < 1 be possible and ˆgpu have a tighter estimation error bound.

ρpu/(2

3.3 Asymptotic comparisons
In practice  we may ﬁnd that ˆgpu is worse than ˆgpn and αpu pn > 1 given X+  X− and Xu. This is
probably the consequence especially when nu is not sufﬁciently larger than n+ and n−. Should we
then try to collect much more U data or just give up PU learning? Moreover  if we are able to have as
many U data as possible  is there any solution that would be provably better than PN learning?

5

We answer these questions by asymptotic comparisons. Notice that each pair of (n+  nu) yields a
value of the RHS of (12)  each (n+  n−) yields a value of the RHS of (11)  and consequently each
triple of (n+  n−  nu) determines a value of αpu pn. Deﬁne the limits of αpu pn and αnu pn as

α∗
pu pn = limn+ n− nu→∞ αpu pn  α∗

nu pn = limn+ n− nu→∞ αnu pn.

pu pn and α∗

Recall that n+  n− and nu are independent  and we need two conditions for the existence of α∗
nu pn: n+ → ∞ and n− → ∞ in the same order and nu → ∞ faster in order than them. It is
and α∗
a bit stricter than what is necessary  but is consistent with a practical assumption: P and N data are
roughly equally expensive  whereas U data are much cheaper than P and N data. Intuitively  since
αpu pn and αnu pn measure relative qualities of the estimation error bounds of ˆgpu and ˆgnu against
that of ˆgpn  α∗
nu pn measure relative qualities of the limits of those bounds accordingly.
In order to illustrate properties of α∗
nu pn  assume only nu approaches inﬁnity while n+
and n− stay ﬁnite  so that α∗
n+) and α∗
n−).
nu pn < 1 unless n+/n− = π2/(1 − π)2.
Thus  α∗
In principle  this exception should be exceptionally rare since n+/n− is a rational number whereas
π2/(1 − π)2 is a real number. This argument constitutes our second main result.
Theorem 7 (Asymptotic comparisons). Assume (5) and one set of conditions below are satisﬁed:

√
n−/((1 − π)
pu pn < 1 or α∗

nu pn = 1  which implies α∗

√
nu pn = (1 − π)

(a) n+ < ∞  n− < ∞ and nu → ∞. In this case  let α∗ = (π
(b) 0 < limn+ n−→∞ n+/n− < ∞ and limn+ n− nu→∞(n+ + n−)/nu = 0. In this case  let

√
n−)/((1 − π)

pu pn and α∗

pu pnα∗

pu pn = π

n+/(π

n+);

√

√

α∗ = π/((1 − π)(cid:112)ρ∗

pu pn

√

pn) where ρ∗

pn = limn+ n−→∞ n+/n−.

pu pn and α∗

pu pn < 1)
nu pn < 1) if α∗ > 1. The

pu pn in both cases. The proof of case (a) has been given as an illustration

Then  either the limit of estimation error bounds of ˆgpu will improve on that of ˆgpn (i.e.  α∗
if α∗ < 1  or the limit of bounds of ˆgnu will improve on that of ˆgpn (i.e.  α∗
pn = π2/(1 − π)2 in (b).
only exception is n+/n− = π2/(1 − π)2 in (a) or ρ∗
Proof. Note that α∗ = α∗
of the properties of α∗
As a result  when we ﬁnd that ˆgpu is worse than ˆgpn and αpu pn > 1  we should look at α∗ deﬁned in
Theorem 7. If α∗ < 1  ˆgpu is promising and we should collect more U data; if α∗ > 1 otherwise 
we should give up ˆgpu  but instead ˆgnu is promising and we should collect more U data as well. In
addition  the gap between α∗ and one indicates how many U data would be sufﬁcient. If the gap is
signiﬁcant  slightly more U data may be enough; if the gap is slight  signiﬁcantly more U data may
be necessary. In practice  however  U data are cheaper but not free  and we cannot have as many U
data as possible. Therefore  ˆgpn is still of practical importance given limited budgets.

nu pn. The proof of case (b) is analogous.

3.4 Remarks

the risk R(g):
Lemma 8. For any δ > 0  with probability at least 1 − δ 

Theorem 2 relies on a fundamental lemma of the uniform deviation from the risk estimator (cid:98)Rpu(g) to
supg∈G |(cid:98)Rpu(g) − R(g)| ≤ 4πL(cid:96)Rn+ p+(G) + 2L(cid:96)Rnu p(G) + 2π
In Lemma 8  R(g) is w.r.t. p(x  y)  though (cid:98)Rpu(g) is w.r.t. p+(x) and p(x). Rademacher complexities

are also w.r.t. p+(x) and p(x)  and they can be bounded easily for G deﬁned in Eq. (6).
Theorems 6 and 7 rely on (5). Thanks to it  we can simplify Theorems 2  3 and 4. In fact  (5) holds
for not only the special case of G deﬁned in (6)  but also the vast majority of discriminative models in
machine learning that are nonlinear in parameters such as decision trees (cf. Theorem 17 in [16]) and
feedforward neural networks (cf. Theorem 18 in [16]).
Theorem 2 in [7] is a similar bound of the same order as our Lemma 8. That theorem is based on a
tricky decomposition of the risk

(cid:113) ln(4/δ)

(cid:113) ln(4/δ)

2n+

2nu

+

.

E(X Y )[(cid:96)(g(X)  Y )] = πE+[˜(cid:96)(g(X)  +1)] + E(X Y )[˜(cid:96)(g(X)  Y )] 

where the surrogate loss ˜(cid:96)(t  y) = (2/(y + 3))(cid:96)(t  y) is not (cid:96) for risk minimization and labels of Xu
are needed for risk evaluation  so that no further bound is implied. Lemma 8 uses the same (cid:96) as risk

minimization and requires no label of Xu for evaluating (cid:98)Rpu(g)  so that it can serve as the stepping

stone to our estimation error bound in Theorem 2.

6

(a) Theo. (nu var.)

(b) Expe. (nu var.)

(c) Theo. (π var.)

(d) Expe. (π var.)

Figure 1: Theoretical and experimental results based on artiﬁcial data.

4 Experiments

In this section  we experimentally validate our theoretical ﬁndings.
Artiﬁcial data Here  X+  X− and Xu are in R2 and drawn from three marginal densities

√

√
p+(x) = N (+12/

2  I2) 

p−(x) = N (−12/

2  I2) 

p(x) = πp+(x) + (1 − π)p−(x) 

where N (µ  Σ) is the normal distribution with mean µ and covariance Σ  12 and I2 are the all-one
vector and identity matrix of size 2. The test set contains one million data drawn from p(x  y).
The model g(x) = (cid:104)w  x(cid:105) + b where w ∈ R2  b ∈ R and the scaled ramp loss (cid:96)sr are employed. In
addition  an (cid:96)2-regularization is added with the regularization parameter ﬁxed to 10−3  and there is
no hard constraint on (cid:107)w(cid:107)2 or (cid:107)x(cid:107)2 as in Eq. (6). The solver for minimizing three regularized risk
estimators comes from [7] (refer also to [27  28] for the optimization technique).
The results are reported in Figure 1. In (a)(b)  n+ = 45  n− = 5  π = 0.5  and nu varies from 5 to
200; in (c)(d)  n+ = 45  n− = 5  nu = 100  and π varies from 0.05 to 0.95. Speciﬁcally  (a) shows
αpu pn and αnu pn as functions of nu  and (c) shows them as functions of π. For the experimental
results  ˆgpn  ˆgpu and ˆgnu were trained based on 100 random samplings for every nu in (b) and π in
(d)  and means with standard errors of the misclassiﬁcation rates are shown  as (cid:96)sr is classiﬁcation-
calibrated. Note that the empirical misclassiﬁcation rates are essentially the risks w.r.t. (cid:96)01 as there
were one million test data  and the ﬂuctuations are attributed to the non-convex nature of (cid:96)sr. Also 
the curve of ˆgpn is not a ﬂat line in (b)  since its training data at every nu were exactly same as the
training data of ˆgpu and ˆgnu for fair experimental comparisons.
In Figure 1  the theoretical and experimental results are highly consistent. The red and blue curves
intersect at nearly the same positions in (a)(b) and in (c)(d)  even though the risk minimizers in the
experiments were locally optimal and regularized  making our estimation error bounds inexact.

Benchmark data Table 2 summarizes the speciﬁcation of benchmarks  which were downloaded
from many sources including the IDA benchmark repository [29]  the UCI machine learning reposi-
tory  the semi-supervised learning book [30]  and the European ESPRIT 5516 project.3 In Table 2 
three rows describe the number of features  the number of data  and the ratio of P data according to
the true class labels. Given a random sampling of X+  X− and Xu  the test set has all the remaining
data if they are less than 104  or else drawn uniformly from the remaining data of size 104.
For benchmark data  the linear model for the artiﬁcial data is not enough  and its kernel version is
employed. Consider training ˆgpu for example. Given a random sampling  g(x) = (cid:104)w  φ(x)(cid:105) + b is
used where w ∈ Rn++nu  b ∈ R and φ : Rd → Rn++nu is the empirical kernel map [26] based on
X+ and Xu for the Gaussian kernel. The kernel width and the regularization parameter are selected
by ﬁve-fold cross-validation for each risk minimizer and each random sampling.

3See http://www.raetschlab.org/Members/raetsch/benchmark/ for IDA  http://archive.ics.
uci.edu/ml/ for UCI  http://olivier.chapelle.cc/ssl-book/ for the SSL book and https://www.
elen.ucl.ac.be/neural-nets/Research/Projects/ELENA/ for the ELENA project.

Table 2: Speciﬁcation of benchmark datasets.

banana
2
5300
.448

phoneme magic
10
19020
.648

5
5404
.293

image
18
2086
.570

german
20
1000
.300

dim
size
P ratio

7

twonorm waveform spambase
57
4597
.394

21
5000
.329

20
7400
.500

coil2
241
1500
.500

050100150200nu024681012  pu;pn nu;pn =1050100150200nu20253035Misclassification rate (%)^gpu^gnu^gpn00.20.40.60.81:100101102 00.20.40.60.81:5101520253035Misclassification rate (%)(a) Theo.

(b) banana

(c) phoneme

(d) magic

(e) image

(f) german

(g) twonorm

(h) waveform

(i) spambase

(j) coil2

Figure 2: Experimental results based on benchmark data by varying nu.

(a) Theo.

(b) banana

(c) phoneme

(d) magic

(e) image

(f) german

(g) twonorm

(h) waveform

(i) spambase

(j) coil2

Figure 3: Experimental results based on benchmark data by varying π.

The results by varying nu and π are reported in Figures 2 and 3 respectively. Similarly to Figure 1 
in Figure 2  n+ = 25  n− = 5  π = 0.5  and nu varies from 10 to 300  while in Figure 3  n+ = 25 
n− = 5  nu = 200  and π varies from 0.05 to 0.95. Figures 2(a) and 3(a) depict αpu pn and αnu pn
as functions of nu and π  and all the remaining subﬁgures depict means with standard errors of the
misclassiﬁcation rates based on 100 random samplings for every nu and π.
The theoretical and experimental results based on benchmarks are still highly consistent. However 
unlike in Figure 1(b)  in Figure 2 only the errors of ˆgpu decrease with nu  and the errors of ˆgnu just
ﬂuctuate randomly. This may be because benchmark data are more difﬁcult than artiﬁcial data and
hence n− = 5 is not sufﬁciently informative for ˆgnu even when nu = 300. On the other hand  we
can see that Figures 3(a) and 1(c) look alike  and so do all the remaining subﬁgures in Figure 3 and
Figure 1(d). Nevertheless  three intersections in Figure 3(a) are closer than those in Figure 1(c)  as
nu = 200 in Figure 3(a) and nu = 100 in Figure 1(c). The three intersections will become a single
one if nu = ∞. By observing the experimental results  three curves in Figure 3 are also closer than
those in Figure 1(d) when π ≥ 0.6  which demonstrates the validity of our theoretical ﬁndings.

5 Conclusions

In this paper  we studied a fundamental problem in PU learning  namely  when PU learning is likely
to outperform PN learning. Estimation error bounds of the risk minimizers were established in PN 
PU and NU learning. We found that under the very mild assumption (5): The PU (or NU) bound is
tighter than the PN bound  if αpu pn in (14) (or αnu pn in (15)) is smaller than one (cf. Theorem 6);
either the limit of αpu pn or that of αnu pn will be smaller than one  if the size of U data increases
faster in order than the sizes of P and N data (cf. Theorem 7). We validated our theoretical ﬁndings
experimentally using one artiﬁcial data and nine benchmark data.

Acknowledgments

GN was supported by the JST CREST program and Microsoft Research Asia. MCdP  YM  and MS
were supported by the JST CREST program. TS was supported by JSPS KAKENHI 15J09111.

8

050100150200250300nu02468  pu;pn nu;pn =1050100150200250300nu30354045Misclassification rate (%)^gpu^gnu^gpn050100150200250300nu30354045Misclassification rate (%)050100150200250300nu354045Misclassification rate (%)050100150200250300nu30354045Misclassification rate (%)050100150200250300nu4042444648Misclassification rate (%)050100150200250300nu354045Misclassification rate (%)050100150200250300nu20253035Misclassification rate (%)050100150200250300nu30354045Misclassification rate (%)050100150200250300nu36384042444648Misclassification rate (%)00.20.40.60.81:100101102  pu;pn nu;pn =100.20.40.60.81:1020304050Misclassification rate (%)^gpu^gnu^gpn00.20.40.60.81:1020304050Misclassification rate (%)00.20.40.60.81:1020304050Misclassification rate (%)00.20.40.60.81:10203040Misclassification rate (%)00.20.40.60.81:1020304050Misclassification rate (%)00.20.40.60.81:1020304050Misclassification rate (%)00.20.40.60.81:10203040Misclassification rate (%)00.20.40.60.81:1020304050Misclassification rate (%)00.20.40.60.81:1020304050Misclassification rate (%)References
[1] F. Denis. PAC learning from positive statistical queries. In ALT  1998.
[2] F. Letouzey  F. Denis  and R. Gilleron. Learning from positive and unlabeled examples. In ALT  2000.
[3] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In KDD  2008.
[4] G. Ward  T. Hastie  S. Barry  J. Elith  and J. Leathwick. Presence-only data and the EM algorithm.

Biometrics  65(2):554–563  2009.

[5] C. Scott and G. Blanchard. Novelty detection: Unlabeled data deﬁnitely help. In AISTATS  2009.
[6] G. Blanchard  G. Lee  and C. Scott. Semi-supervised novelty detection. Journal of Machine Learning

Research  11:2973–3009  2010.

[7] M. C. du Plessis  G. Niu  and M. Sugiyama. Analysis of learning from positive and unlabeled data. In

NIPS  2014.

[8] M. C. du Plessis  G. Niu  and M. Sugiyama. Convex formulation for learning from positive and unlabeled

data. In ICML  2015.

[9] G. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past

observations. In SIGIR  2008.

[10] N. Craswell  O. Zoeter  M. Taylor  and B. Ramsey. An experimental comparison of click position-bias

models. In WSDM  2008.

[11] O. Chapelle and Y. Zhang. A dynamic Bayesian network click model for web search ranking. In WWW 

2009.

[12] J. Quiñonero-Candela  M. Sugiyama  A. Schwaighofer  and N. D. Lawrence. Dataset Shift in Machine

Learning. MIT Press  2009.

[13] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons  1998.
[14] O. Bousquet  S. Boucheron  and G. Lugosi. Introduction to statistical learning theory. In O. Bousquet 
U. von Luxburg  and G. Rätsch  editors  Advanced Lectures on Machine Learning  pages 169–207. Springer 
2004.

[15] V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information

Theory  47(5):1902–1914  2001.

[16] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3:463–482  2002.

[17] R. Meir and T. Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine

Learning Research  4:839–860  2003.

[18] M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of Machine Learning. MIT Press  2012.
[19] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  classiﬁcation  and risk bounds. Journal of the

American Statistical Association  101(473):138–156  2006.

[20] M. Saerens  P. Latinne  and C. Decaestecker. Adjusting the outputs of a classiﬁer to new a priori

probabilities: A simple procedure. Neural Computation  14(1):21–41  2002.

[21] M. C. du Plessis and M. Sugiyama. Semi-supervised learning of class balance under class-prior change by

distribution matching. In ICML  2012.

[22] A. Iyer  S. Nath  and S. Sarawagi. Maximum mean discrepancy for class ratio estimation: Convergence

bounds and kernel selection. In ICML  2014.

[23] M. C. du Plessis  G. Niu  and M. Sugiyama. Class-prior estimation for learning from positive and unlabeled

data. In ACML  2015.

[24] H. G. Ramaswamy  C. Scott  and A. Tewari. Mixture proportion estimation via kernel embedding of

distributions. In ICML  2016.

[25] K.-L. Chung. A Course in Probability Theory. Academic Press  1968.
[26] B. Schölkopf and A. Smola. Learning with Kernels. MIT Press  2001.
[27] R. Collobert  F. Sinz  J. Weston  and L. Bottou. Trading convexity for scalability. In ICML  2006.
[28] A. L. Yuille and A. Rangarajan. The concave-convex procedure (CCCP). In NIPS  2001.
[29] G. Rätsch  T. Onoda  and K. R. Müller. Soft margins for AdaBoost. Machine learning  42(3):287–320 

2001.

[30] O. Chapelle  B. Schölkopf  and A. Zien  editors. Semi-Supervised Learning. MIT Press  2006.
[31] C. McDiarmid. On the method of bounded differences. In J. Siemons  editor  Surveys in Combinatorics 

pages 148–188. Cambridge University Press  1989.

[32] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer  1991.

9

,Jian Zhang
Alex Schwing
Raquel Urtasun
Gang Niu
Marthinus Christoffel du Plessis
Tomoya Sakai
Masashi Sugiyama
Hoda Heidari
Claudio Ferrari
Krishna Gummadi
Andreas Krause