2015,Fast Classification Rates for High-dimensional Gaussian Generative Models,We consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate Gaussian distributions. We focus on the setting where the covariance matrices for the two conditional distributions are the same. The corresponding generative model classifier  derived via the Bayes rule  also called Linear Discriminant Analysis  has been shown to behave poorly in high-dimensional settings. We present a novel analysis of the classification error of any linear discriminant approach given conditional Gaussian models. This allows us to compare the generative model classifier  other recently proposed discriminative approaches that directly learn the discriminant function  and then finally logistic regression which is another classical discriminative model classifier. As we show  under a natural sparsity assumption  and letting $s$ denote the sparsity of the Bayes classifier  $p$ the number of covariates  and $n$ the number of samples  the simple ($\ell_1$-regularized) logistic regression classifier achieves the fast misclassification error rates of $O\left(\frac{s \log p}{n}\right)$  which is much better than the other approaches  which are either inconsistent under high-dimensional settings  or achieve a slower rate of $O\left(\sqrt{\frac{s \log p}{n}}\right)$.,Fast Classiﬁcation Rates for High-dimensional

Gaussian Generative Models

Tianyang Li

Adarsh Prasad

Department of Computer Science  UT Austin

{lty adarsh pradeepr}@cs.utexas.edu

Pradeep Ravikumar

Abstract

We consider the problem of binary classiﬁcation when the covariates conditioned
on the each of the response values follow multivariate Gaussian distributions. We
focus on the setting where the covariance matrices for the two conditional dis-
tributions are the same. The corresponding generative model classiﬁer  derived
via the Bayes rule  also called Linear Discriminant Analysis  has been shown to
behave poorly in high-dimensional settings. We present a novel analysis of the
classiﬁcation error of any linear discriminant approach given conditional Gaussian
models. This allows us to compare the generative model classiﬁer  other recently
proposed discriminative approaches that directly learn the discriminant function 
and then ﬁnally logistic regression which is another classical discriminative model
classiﬁer. As we show  under a natural sparsity assumption  and letting s denote
the sparsity of the Bayes classiﬁer  p the number of covariates  and n the num-
ber of samples  the simple ((cid:96)1-regularized) logistic regression classiﬁer achieves
the fast misclassiﬁcation error rates of O
  which is much better than the
other approaches  which are either inconsistent under high-dimensional settings 
or achieve a slower rate of O

(cid:18)(cid:113) s log p

(cid:16) s log p

(cid:17)

n

(cid:19)

.

n

Introduction

1
We consider the problem of classiﬁcation of a binary response given p covariates. A popular class of
approaches are statistical decision-theoretic: given a classiﬁcation evaluation metric  they then opti-
mize a surrogate evaluation metric that is computationally tractable  and yet have strong guarantees
on sample complexity  namely  number of observations required for some bound on the expected
classiﬁcation evaluation metric. These guarantees and methods have been developed largely for the
zero-one evaluation metric  and extending these to general evaluation metrics is an area of active
research. Another class of classiﬁcation methods are relatively evaluation metric agnostic  which
is an important desideratum in modern settings  where the evaluation metric for an application is
typically less clear: these are based on learning statistical models over the response and covariates 
and can be categorized into two classes. The ﬁrst are so-called generative models  where we specify
conditional distributions of the covariates conditioned on the response  and then use the Bayes rule
to derive the conditional distribution of the response given the covariates. The second are the so-
called discriminative models  where we directly specify the conditional distribution of the response
given the covariates.
In the classical ﬁxed p setting  we have now have a good understanding of the performance of
the classiﬁcation approaches above. For generative and discriminative modeling based approaches 
consider the speciﬁc case of Naive Bayes generative models and logistic regression discriminative
models (which form a so-called generative-discriminative pair1)  Ng and Jordan [27] provided qual-

1In such a so-called generative-discriminative pair  the discriminative model has the same form as that of
the conditional distribution of the response given the covariates speciﬁed by the Bayes rule given the generative
model

1

itative consistency analyses  and showed that under small sample settings  the generative model
classiﬁers converge at a faster rate to their population error rate compared to the discriminative
model classiﬁers  though the population error rate of the discriminative model classiﬁers could be
potentially lower than that of the generative model classiﬁers due to weaker model assumptions.
But if the generative model assumption holds  then generative model classiﬁers seem preferable to
discriminative model classiﬁers.
In this paper  we investigate whether this conventional wisdom holds even under high-dimensional
settings. We focus on the simple generative model where the response is binary  and the covariates
conditioned on each of the response values  follows a conditional multivariate Gaussian distribution.
We also assume that the two covariance matrices of the two conditional Gaussian distributions are
the same. The corresponding generative model classiﬁer  derived via the Bayes rule  is known in
the statistics literature as the Linear Discriminant Analysis (LDA) classiﬁer [21]. Under classical
settings where p (cid:28) n  the misclassiﬁcation error rate of this classiﬁer has been shown to converge to
that of the Bayes classiﬁer. However  in a high-dimensional setting  where the number of covariates
p could scale with the number of samples n  this performance of the LDA classiﬁer breaks down. In
particular  Bickel and Levina [3] show that when p/n → ∞  then the LDA classiﬁer could converge
to an error rate of 0.5  that of random chance. What should one then do  when we are even allowed
this generative model assumption  and when p > n?
Bickel and Levina [3] suggest the use of a Naive Bayes or conditional independence assumption 
which in the conditional Gaussian context  assumes the covariance matrices to be diagonal. As they
showed  the corresponding Naive Bayes LDA classiﬁer does have misclassiﬁcation error rate that is
better than chance  but it is asymptotically biased: it converges to an error rate that is strictly larger
than that of the Bayes classiﬁer when the Naive Bayes conditional independence assumption does
not hold. Bickel and Levina [3] also considered a weakening of the Naive Bayes rule  by assuming
that the covariance matrix is weakly sparse  and an ellipsoidal constraint on the means  showed
that an estimator that leverages these structural constraints converges to the Bayes risk at a rate of
O(log(n)/nγ)  where 0 < γ < 1 depends on the mean and covariance structural assumptions. A
caveat is that these covariance sparsity assumptions might not hold in practice. Similar caveats apply
to the related works on feature annealed independence rules [14]  nearest shrunken centroids [29 
30]  as well as those . Moreover  even when the assumptions hold  they do not yield the “fast” rates
of O(1/n).
An alternative approach is to directly impose sparsity on the linear discriminant [28  7]  which is
weaker than the covariance sparsity assumptions (though [28] impose these in addition). [28  7]
then proposed new estimators that leveraged these assumptions  but while they were able to show
convergence to the Bayes risk  they were only able to show a slower rate of O

(cid:18)(cid:113) s log p

(cid:19)

.

n

It is instructive at this juncture to look at recent results on classiﬁcation error rates from the machine
learning community. A key notion of importance here is whether the two classes are separable:
√
which can be understood as requiring that the classiﬁcation error of the Bayes classiﬁer is 0. Classi-
n) for any classiﬁer when two classes are non-separable 
cal learning theory gives a rate of O(1/
and it shown that this is also minimax [12]  with the note that this is relatively distribution agnostic 
since it assumes very little on the underlying distributions. When the two classes are non-separable 
√
only rates slower than Ω(1/n) are known. Another key notion is a “low-noise condition” [25]  under
which certain classiﬁers can be shown to attain a rate faster than o(1/
n)  albeit not at the O(1/n)
rate unless the two classes are separable. Speciﬁcally  let α denote a constant such that

(cid:16)

(cid:17)

P (|P(Y = 1|X) − 1/2| ≤ t) ≤ O (tα)  

(1)
holds when t → 0. This is said to be a low-noise assumption  since as α → +∞  the two classes
start becoming separable  that is  the Bayes risk approaches zero. Under this low-noise assumption 
[23]. Note that this is always slower than O( 1
known rates for excess 0-1 risk is O
n )
when α < +∞.
There has been a surge of recent results on high-dimensional statistical statistical analyses of M-
estimators [26  9  1]. These however are largely focused on parameter error bounds  empirical and
population log-likelihood  and sparsistency. In this paper however  we are interested in analyzing
the zero-one classiﬁcation error under high-dimensional sampling regimes. One could stitch these
recent results to obtain some error bounds: use bounds on the excess log-likelihood  and use trans-

1+α
2+α

( 1
n )

2

forms from [2]  to convert excess log-likelihood bounds to get bounds on 0-1 classiﬁcation error 
however  the resulting bounds are very loose  and in particular  do not yield the fast rates that we
seek.
In this paper  we leverage the closed form expression for the zero-one classiﬁcation error for our
generative model  and directly analyse it to give faster rates for any linear discriminant method.
Our analyses show that  assuming a sparse linear discriminant in addition  the simple (cid:96)1-regularized
logistic regression classiﬁer achieves near optimal fast rates of O
  even without requiring
that the two classes be separable.

(cid:16) s log p

(cid:17)

n

2 Problem Setup
We consider the problem of high dimensional binary classiﬁcation under the following generative
model. Let Y ∈ {0  1} denote a binary response variable  and let X = (X1  . . .   Xp) ∈ Rp denote
a set of p covariates. For technical simplicity  we assume Pr[Y = 1] = Pr[Y = 0] = 1
2  however
our analysis easily extends to the more general case when Pr[Y = 1]  Pr[Y = 0] ∈ [δ0  1 − δ0]  for
2. We assume that X|Y ∼ N (µY   ΣY )  i.e. conditioned on a response  the
some constant 0 < δ0 < 1
covariate follows a multivariate Gaussian distribution. We assume we are given n training samples
{(X (1)  Y (1))  (X (2)  Y (2))  . . .   (X (n)  Y (n))} drawn i.i.d. from the conditional Gaussian model
above.
For any classiﬁer  C : Rp → {1  0}  the 0-1 risk or simply the classiﬁcation error is given by
R0−1(C) = EX Y [(cid:96)0−1(C(X)  Y )]  where (cid:96)0−1(C(x)  y) = 1(C(x) (cid:54)= y) is the 0-1 loss. It can
also be simply written as R(C) = Pr[C(X) (cid:54)= Y ]. The classiﬁer attaining the lowest classiﬁcation
error is known as the Bayes classiﬁer  which we will denote by C∗. Under the generative model
assumption above  the Bayes classiﬁer can be derived simply as C∗(X) = 1(log Pr[Y =1|X]
Pr[Y =0|X] > 0) 
so that given sample X  it would be classiﬁed as 1 if Pr[Y =1|X]
Pr[Y =0|X] > 1  and as 0 otherwise. We denote
the error of the Bayes classiﬁer R∗ = R(C∗).
When Σ1 = Σ0 = Σ 

Pr[Y = 1|X]
Pr[Y = 0|X]

log

= (µ1 − µ0)T Σ−1X +

1
2

(−µT

1 Σ−1µ1 + µT

0 Σ−1µ0)

(2)

and we denote this quantity as w∗T X + b∗ where
w∗ = Σ−1(µ1 − µ0)  b∗ =

−µT

1 Σ−1µ1 + µT

0 Σ−1µ0

 

2

so that the Bayes classiﬁer can be written as: C∗(x) = 1(w∗T x + b∗ > 0).
For any trained classiﬁer ˆC we are interested in bounding the excess risk deﬁned as R( ˆC) − R∗.
The generative approach to training a classiﬁer is to estimate estimate Σ−1and δ from data  and
then plug the estimates into Equation 2 to construct the classiﬁer. This classiﬁer is known as the
linear discriminant analysis (LDA) classiﬁer  whose theoretical properties have been well-studied
in classical ﬁxed p setting. The discriminative approach to training is to estimate Pr[Y =1|X]
Pr[Y =0|X] directly
from samples.
2.1 Assumptions.
We assume that mean is bounded i.e. µ1  µ0 ∈ {µ ∈ Rp : (cid:107)µ(cid:107)2 ≤ Bµ}  where Bµ is a constant
which doesn’t scale with p. We assume that the covariance matrix Σ is non-degenerate i.e. all
eigenvalues of Σ are in [Bλmin  Bλmax]. Additionally we assume (µ1 − µ0)T Σ−1(µ1 − µ0) ≥ Bs 
which gives a lower bound on the Bayes classiﬁer’s classiﬁcation error R∗ ≥ 1 − Φ( 1
2 Bs) > 0.
Note that this assumption is different from the deﬁnition of separable classes in [11] and the low
noise condition in [25]  and the two classes are still not separable because R∗ > 0.

2.1.1 Sparsity Assumption.
Motivated by [7]  we assume that Σ−1(µ1 − µ0) is sparse  and there at most s non-zero entries.
Cai and Liu [7] extensively discuss and show that such a sparsity assumption  is much weaker than
assuming either Σ−1 and (µ1 − µ0) to be individually sparse. We refer the reader to [7] for an
elaborate discussion.

3

2.2 Generative Classiﬁers
Generative techniques work by estimating Σ−1 and (µ1 − µ0) from data and plugging them into
Equation 2. In high-dimensions  simple estimation techniques do not perform well when p (cid:29) n 
the sample estimate for the covariance matrix ˆΣ is singular; using the generalized inverse of the
sample covariance matrix makes the estimator highly biased and unstable. Numerous alternative
approaches have been proposed by imposing structural conditions on Σ or Σ−1 and δ to ensure that
they can be estimated consistently. Some early work based on nearest shrunken centroids [29  30] 
feature annealed independence rules [14]  and Naive Bayes [4] imposed independence assumptions
on Σ  which are often violated in real-world applications. [4  13] impose more complex structural
assumptions on the covariance matrix and suggest more complicated thresholding techniques. Most
commonly  Σ−1 and δ are assumed to be sparse and then some thresholding techniques are used to
estimate them consistently [17  28].
2.3 Discriminative Classiﬁers.
Recently  more direct techniques have been proposed to solve the sparse LDA problem. Let ˆΣ
and ˆµd be consistent estimators of Σ and µ = µ1+µ0
. Fan et al. [15] proposed the Regularized
Optimal Afﬁne Discriminant (ROAD) approach which minimizes wT Σw with wT µ restricted to be
a constant value and an (cid:96)1-penalty of w.

2

wROAD = argmin
wT ˆµ=1
||w||1≤c

wT ˆΣw

(3)

Kolar and Liu [22] provided theoretical insights into the ROAD estimator by analysing its consis-
tency for variable selection. Cai and Liu [7] proposed another variant called linear programming
discriminant (LPD) which tries to make w close to the Bayes rules linear term Σ−1(µ1 − µ0) in the
(cid:96)∞ norm. This can be cast as a linear programming problem related to the Dantzig selector.[8].

(4)

||w||1
wLPD = argmin
s.t.|| ˆΣw − ˆµ||∞ ≤ λn

w

Mai et al. [24]proposed another version of the sparse linear discriminant analysis based on an equiv-
alent least square formulation of the LDA  where they solve an (cid:96)1-regularized least squares problem
to produce a consistent classiﬁer.
All the techniques above either do not have ﬁnite sample convergence rates  or the 0-1 risk converged
at a slow rate of O

(cid:18)(cid:113) s log p

(cid:19)

.

n

(cid:18) wT µ1 + b

(cid:19)

√

wT Σw

(cid:18)

(cid:19)

In this paper  we ﬁrst provide an analysis of classiﬁcation error rates for any classiﬁer with a linear
discriminant function  and then follow this analysis by investigating the performance of generative
and discriminative classiﬁers for conditional Gaussian model.
3 Classiﬁers with Sparse Linear Discriminants
We ﬁrst analyze any classiﬁer with a linear discriminant function  of the form: C(x) = 1(wT x+b >
0). We ﬁrst note that the 0-1 classiﬁcation error of any such classiﬁer is available in closed-form as

R(w  b) = 1 − 1
2

Φ

− 1
2

Φ

√

− wT µ0 + b
wT Σw

 

(5)

which can be shown by noting that wT X + b is a univariate normal random variable when condi-
tioned on the label Y .
Next  we relate the 0-1 classiﬁction error above to that of the Bayes classiﬁer. Recall the earlier
notation of the Bayes classiﬁer as C∗(x) = 1(xT w∗ + b∗ > 0). The following theorem is a key
result of the paper that shows that for any linear discriminant classiﬁer whose linear discriminant
parameters are close to that of the Bayes classiﬁer  the excess 0-1 risk is bounded only by second
order terms of the difference. Note that this theorem will enable fast classiﬁcation rates if we obtain
fast rates for the parameter error.
Theorem 1. Let w = w∗ + ∆  b = b∗ + Ω  and ∆ → 0  Ω → 0  then we have
2 + Ω2).

R(w = w∗ + ∆  b = b∗ + Ω) − R(w∗  b∗) = O((cid:107)∆(cid:107)2

(6)

4

Proof. Denote the quantity S∗ = (cid:112)(µ1 − µ0)T Σ−1(µ1 − µ0) 

then we have µT
√

1 w∗+b∗
w∗ T Σw∗ =

2 S∗  we have
) − Φ(
S∗))|

1
2

(7)

−µT
1 w∗−b∗
√
w∗ T Σw∗ = 1

2 S∗. Using (5) and the Taylor series expansion of Φ(·) around 1
0 w − b
−µT
√
S∗)) + (Φ(
wT Σw
−µT
0 w − b
√
wT Σw

1
2
S∗)2 + K3(

µT
√
1 w + b
wT Σw
µT
√
1 w + b
wT Σw

|R(w  b) − R(w∗  b∗)| =
≤K1| (µ1 − µ0)T w

− S∗| + K2(

) − Φ(

− 1
2

wT Σw

|(Φ(

√

1
2

S∗)2

− 1
2

√

√

wT Σw −
2 w  ∆(cid:48)(cid:48) = Σ 1

where K1  K2  K3 > 0 are constants because the ﬁrst and second order derivatives of Φ(·) are
bounded.
First note that |
Denote w(cid:48)(cid:48) = Σ 1
Taylor series expansion)
(µ1 − µ0)T w
√
wT Σw
1 + a(cid:48)(cid:48)T ∆(cid:48)(cid:48)

w∗T Σw∗| = O((cid:107)∆(cid:107)2)  because (cid:107)w∗(cid:107)2 is bounded.
2 ∆  w(cid:48)(cid:48)∗

w(cid:48)(cid:48)T w(cid:48)(cid:48) −(cid:112)

2 (µ1 − µ0)  we have (by the binomial

a(cid:48)(cid:48) T a(cid:48)(cid:48) −(cid:113)

a(cid:48)(cid:48) T a(cid:48)(cid:48) + ∆(cid:48)(cid:48)T ∆(cid:48)(cid:48)
a(cid:48)(cid:48)T a(cid:48)(cid:48)

2 w∗ a(cid:48)(cid:48) = Σ− 1

a(cid:48)(cid:48)T w(cid:48)(cid:48)
√

− S∗ =

a(cid:48)(cid:48)T a(cid:48)(cid:48)

= Σ 1

(8)

=

1 + 2 a(cid:48)(cid:48) T ∆(cid:48)(cid:48)
√
w(cid:48)(cid:48)T w(cid:48)(cid:48)
a(cid:48)(cid:48)T a(cid:48)(cid:48)

= O(

(cid:107)∆(cid:48)(cid:48)(cid:107)2
√
a(cid:48)(cid:48)T a(cid:48)(cid:48) )

2

Note that w(cid:48)(cid:48) → a(cid:48)(cid:48)  ∆(cid:48)(cid:48) → 0  (cid:107)∆(cid:107)2 = Θ((cid:107)∆(cid:48)(cid:48)(cid:107)2)  and S∗ is lower bouned  we have | (µ1−µ0)T w
S∗| = O((cid:107)∆(cid:107)2
2).
Next we bound | µT
√

− 1

wT Σw

√

−

1 w+b
wT Σw

S∗| = | (µT

2 S∗|:
√
1 w∗ + b∗)(

√

√

w∗T Σw∗ −

√

√
wT Σw) +
w∗T Σw∗

wT Σw

w∗T Σw∗(µT

1 ∆ + Ω)

|

(9)

| µT
√
1 w + b
wT Σw

− 1
2

(cid:113)(cid:107)∆(cid:107)2

= O(

2 + Ω2)

where we use the fact that |µT
Similarly |−µT
√
Combing the above bounds we get the desired result.

2 S∗| = O((cid:112)(cid:107)∆(cid:107)2

1 w∗ + b∗| and S∗ are bounded.

0 w−b
wT Σw

2 + Ω2).

− 1

(cid:27)

(cid:88)

(cid:26) 1

n

4 Logistic Regression Classiﬁer
In this section  we show that the simple (cid:96)1 regularized logistic regression classiﬁer attains fast clas-
siﬁcation error rates.
Speciﬁcally  we are interested in the M-estimator [21] below:

( ˆw  ˆb) = arg min
w b

(Y (i)(wT X (i) + b) + log(1 + exp(wT X (i) + b))) + λ((cid:107)w(cid:107)1 + |b|)

 

(10)
which maximizes the penalized log-likelihood of the logistic regression model  which also corre-
sponds to the conditional probability of the response given the covariates P(Y |X) for the conditional
Gaussian model.
Note that here we penalize the intercept term b as well. Although the intercept term usually is not
penalized (e.g. [19])  some packages (e.g. [16]) penalize the intercept term. Our analysis show that
penalizing the intercept term does not degrade the performance of the classiﬁer.
In [2  31] it is shown that minimizing the expected risk of the logistic loss also minimizes the
classiﬁcation error for the corresponding linear classiﬁer. (cid:96)1 regularized logistic regression is a
popular classiﬁcation method in many settings [18  5]. Several commonly used packages ([19  16])
have been developed for (cid:96)1 regularized logistic regression. And recent works ([20  10]) have been
on scaling regularized logistic regression to ultra-high dimensions and large number of samples.

5

4.1 Analysis

(11)

s log p).

(12)

(cid:21)

1

(cid:21)
(cid:20)Ip − 1
(cid:21)(cid:2)− 1
(cid:113)

where A =

2 (µ1 + µ0)

0
2 (µ1 + µ0)

(cid:20)− 1
1(cid:3)
1(cid:3)  and we can see that the singular

2 (µ1 + µ0)T

  and

1

1

+

We ﬁrst show that (cid:96)1 regularized logistic regression estimator above converges to the Bayes classiﬁer
parameters using techniques. Next we use the theorem from the previous section to argue that since
estimated parameter ˆw  ˆb is close to the Bayes classiﬁer’s parameter w∗  b∗  the excess risk of the
classiﬁer using estimated parameter is tightly bounded as well.
For the ﬁrst step  we ﬁrst show a restricted eigenvalue condition for X(cid:48) = (X  1) where X are our
2N (µ0  Σ). Note
2N (µ1  Σ)+ 1
covariates  that comes from a mixture of two Gaussian distributions 1
that X(cid:48) is not zero centered  which is different from existing scenarios ([26]  [6]  etc.) that assume
covariates are zero centered. And we denote w(cid:48) = (w  b)  S(cid:48) = {i : w(cid:48)∗
i (cid:54)= 0}  and s(cid:48) = |S(cid:48)| ≤ s+1.
Lemma 1. With probability 1 − δ  ∀v(cid:48) ∈ A(cid:48) ⊆ {v(cid:48) ∈ Rp+1 : (cid:107)v(cid:48)(cid:107)2 = 1}  for some constants
κ1  κ2  κ3 > 0 we have

√

(cid:107)X(cid:48)v(cid:48)(cid:107)2 ≥ κ1

n − κ2w(A(cid:48)) − κ3

1
n

(cid:114)

log

1
δ

where w(A(cid:48)) = Eg(cid:48)∼N (0 Ip+1)[supa(cid:48)∈A(cid:48) g(cid:48)T a(cid:48)] is the Gaussian width of A(cid:48).
In the special case when A(cid:48) = {v(cid:48) : (cid:107)v(cid:48)

√
S(cid:48)(cid:107)1 (cid:107)v(cid:48)(cid:107)2 = 1}  we have w(A(cid:48)) = O(

¯S(cid:48)(cid:107)1 ≤ 3(cid:107)v(cid:48)

Proof. First note that X(cid:48) is sub-Gaussian with bounded parameter and

(cid:20)Σ + 1

1 + µ0µT
0 )

1
2 (µ1 + µ0)

Note that AΣ(cid:48)AT =

A−1 =

1
2 (µ1 + µ0)

(cid:20)Ip

0

(cid:20)Ip

1

Σ(cid:48) = E[

1

1
n

(cid:20)Σ + 1
2 (µ1µT
X(cid:48)T X(cid:48)] =
2 (µT
4 (µ1 − µ0)T (µ1 − µ0)
(cid:21)
(cid:20) 1
(cid:21)

. Notice that AAT =

(cid:20)Ip

1 + µT
0 )
0
1
0
0

(cid:21)
(cid:21)

0

0

(cid:21)(cid:2) 1

1

0

+

0
0

2 (µ1 + µ0)

and A−1A−T =
2 (µ1 + µ0)T
1√
values of A and A−1 are lower bounded by
µ. Let λ1
1(cid:107)2 = 1) the corresponding eigenvector. From the
1 ((cid:107)u(cid:48)
be the minimum eigenvalue of Σ(cid:48)  and u(cid:48)
1  so we know that the minimum eigenvalue of Σ(cid:48) is lower
expression AΣAT A−T u(cid:48)
bounded. Similarly the largest eigenvalue of Σ(cid:48) is upper bounded. Then the desired result follows
the proof of Theorem 10 in [1]. Although the proof of Theorem 10 in [1] is for zero-centered random
variables  the proof remains valid for non zero-centered random variables.
√
S(cid:48)(cid:107)1 (cid:107)v(cid:48)(cid:107)2 = 1}  [9] gives w(A(cid:48)) = O(
When A(cid:48) = {v(cid:48) : (cid:107)v(cid:48)

and upper bounded by

1 = λ1Au(cid:48)

¯S(cid:48)(cid:107)1 ≤ 3(cid:107)v(cid:48)

s log p).

2 + B2

2+B2
µ

Having established a restricted eigenvalue result in Lemma 1  next we use the result in [26] for pa-
rameter recovery in generalized linear models (GLMs) to show that (cid:96)1 regularized logistic regression
can recover the Bayes classiﬁer parameters.
Lemma 2. When the number of samples n (cid:29) s(cid:48) log p  and choose λ = c0
c0  then we have

n for some constant

(cid:113) log p

with probability at least 1 − O( 1

(cid:107)w∗ − ˆw(cid:107)2
pc1 + 1

2 + (b∗ − ˆb)2 = O(
nc2 )  where c1  c2 > 0 are constants.

n

)

s(cid:48) log p

(13)

Proof. Following the proof of Lemma 1  we see that the conditions (GLM1) and (GLM2) in [26]
are satisﬁed. Following the proof of Proposition 2 and Corollary 5 in [26]  we have the desired
result. Although the proof of Proposition 2 and Corollary 5 in [26] is for zero-centered random
variables  the proof remains valid for non zero-centered random variables.

Combining Lemma 2 and Theorem 1 we have the following theorem which gives a fast rate for the
excess 0-1 risk of a classiﬁer trained using (cid:96)1 regularized logistic regression.

6

(cid:113) log p

Theorem 2. With probability at least 1 − O( 1
set λ = c0

n for some constant c0 the Lasso estimate ˆw  ˆb in (10) satisﬁes

pc1 + 1

nc2 ) where c1  c2 > 0 are constants  when we

R( ˆw  ˆb) − R(w∗  b∗) = O(

s log p

n

)

(14)

Proof. This follows from Lemma 2 and Theorem 1.

5 Other Linear Discriminant Classiﬁers

In this section  we provide convergence results for the 0-1 risk for other linear discriminant classiﬁers
discussed in Section 2.3.

(cid:21)

0p−s

(cid:20) 1s

(cid:20) 1s

and µ0 = − M0√

Naive Bayes We compare the discriminative approach using (cid:96)1 logistic regression to the genera-
tive approach using naive Bayes. For illustration purposes we conside the case where Σ = Ip  µ1 =
. where 0 < B1 ≤ M1  M0 ≤ B2 are unknown but bounded
M1√
s

0p−s
constants. In this case w∗ = M1+M0√
0 ). Using naive Bayes we es-
timate ˆw = ¯µ1 − ¯µ0  where ¯µ1 =
Y (i)=0 X (i).
Thus with high probability  we have (cid:107) ˆw − w∗(cid:107)2
n )  using Theorem 1 we get a slower rate
than the bound given in Theorem 2 for discriminative classiﬁcation using (cid:96)1 regularized logistic
regression.

1 + M 2
Y (i)=1 X (i) and ¯µ0 =
2 = O( p

(cid:21)
(cid:20) 1s
(cid:21)
1(cid:80) 1(Y (i)=1 )(cid:80)

1(cid:80) 1(Y (i)=0 )(cid:80)

and b∗ = 1

2 (−M 2

s

0p−s

s

LPD [7] LPD uses a linear programming similar to the Dantzig selector.
Lemma 3 (Cai and Liu [7]  Theorem 4). Let λn = C
n with C being a sufﬁciently large
constant. Let n > log p  let ∆ = (µ1 − µ0)T Σ−1(µ1 − µ0) > c1 for some constant c1 > 0  and
let wLPD be obtained as in Equation 4  then with probability greater than 1 − O(p−1)  we have

(cid:113) s log p

n ).

R(wLPD)

R∗ − 1 = O(

SLDA [28] SLDA uses thresholded estimate for Σ and µ1 − µ0. We state a simpler version.
Lemma 4 ([28]  Theorem 3). Assume that Σ and µ1 − µ0 are sparse  then we have R(wSLDA)
O(max(( s log p
non-zero entries in Σ  and α1  α2 ∈ (0  1

− 1 =
)α2) with high probability  where s = (cid:107)µ1 − µ0(cid:107)0  S is the number of

n )α1  ( S log p

2 ) are constants.

R∗

n

ROAD [15] ROAD minimizes wT Σw with wT µ restricted to be a constant value and an (cid:96)1-penalty
of w.
Lemma 5 (Fan et al. [15]  Theorem 1). Assume that with high probability  || ˆΣ−Σ||∞ = O(
and ||ˆµ−µ||∞ = O(
we have R(wROAD) − R∗ = O(

n )
n )  and let wROAD be obtained as in Equation 3  then with high probability 

(cid:113) s log p

(cid:113) log p

n ).

(cid:113) log p

6 Experiments

(cid:113) s log p

In this section we describe experiments which illustrate the rates for excess 0-1 risk given in Theorem
2. In our experiments we use Glmnet [19] where we set the option to penalize the intercept term
along with all other parameters. Glmnet is popular package for (cid:96)1 regularized logistic regression
using coordinate descent methods.

For illustration purposes in all simulations we use Σ = Ip  µ1 = 1p + 1√

  µ0 = 1p −

(cid:20) 1s

(cid:21)

s

0p−s

To illustrate our bound in Theorem 2  we consider three different scenarios. In Figure 1a

7

(cid:21)

(cid:20) 1s

0p−s

1√
s

(a) Only varying p.

(b) Only varying s.

(c) Dependence of excess 0-1 risk
on n.

Figure 1: Simulations for different Gaussian classiﬁcation problems showing the dependence of
classiﬁcation error on different quantities. All experiments plotted the average of 20 trials. In all
experiments we set the regularization parameter λ =

(cid:113) log p

n .

we vary p while keeping s  (µ1 − µ0)T Σ−1(µ1 − µ0) constant. Figure 1a shows for different p how
the classiﬁcation error changes with increasing n. In Figure 1a we show the relationship between
the classiﬁcation error and the quantity n
log p. This ﬁgure agrees with our result on excess 0-1 risk’s
dependence on p. In Figure 1b we vary s while keeping p  (µ1 − µ0)T Σ−1(µ1 − µ0) constant.
Figure 1b shows for different s how the classiﬁcation error changes with increasing n. In Figure 1a
we show the relationship between the classiﬁcation error and the quantity n
s . This ﬁgure agrees with
our result on excess 0-1 risk’s dependence on s. In Figure 1c we show how R( ˆw  ˆb) − R(w∗  b∗)
changes with respect to 1
n in one instance Gaussian classiﬁcation. We can see that the excess 0-1
risk achieves the fast rate and agrees with our bound.

Acknowledgements

We acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803  IIS-
1320894  IIS-1447574  and DMS-1264033  and NIH via R01 GM117594-01 as part of the Joint
DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical
Sciences.

References
[1] Arindam Banerjee  Sheng Chen  Farideh Fazayeli  and Vidyashankar Sivakumar. Estimation with norm

regularization. In Advances in Neural Information Processing Systems  pages 1556–1564  2014.

[2] Peter L Bartlett  Michael I Jordan  and Jon D McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[3] Peter J Bickel and Elizaveta Levina. Some theory for Fisher’s linear discriminant function  ‘naive Bayes’ 
and some alternatives when there are many more variables than observations. Bernoulli  pages 989–1010 
2004.

[4] Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics 

pages 2577–2604  2008.

[5] C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer 

2006. ISBN 9780387310732.

[6] Peter B¨uhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods  theory and appli-

cations. Springer Science & Business Media  2011.

[7] Tony Cai and Weidong Liu. A direct estimation approach to sparse linear discriminant analysis. Journal

of the American Statistical Association  106(496)  2011.

[8] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is much larger

than n. The Annals of Statistics  pages 2313–2351  2007.

[9] Venkat Chandrasekaran  Benjamin Recht  Pablo A Parrilo  and Alan S Willsky. The convex geometry of

linear inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

8

01002003004000.20.250.30.350.40.450.5n/log(p)classification error p=100p=400p=160001002003004000.20.250.30.350.40.450.5n/sclassification error s=5s=10s=15s=2000.0050.010.01500.050.10.150.20.251/nexcess 0−1 risk[10] Weizhu Chen  Zhenghao Wang  and Jingren Zhou. Large-scale L-BFGS using MapReduce. In Advances

in Neural Information Processing Systems  pages 1332–1340  2014.

[11] L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer New York 

1996.

[12] Luc Devroye. A probabilistic theory of pattern recognition  volume 31. Springer Science & Business

Media  1996.

[13] David Donoho and Jiashun Jin. Higher criticism thresholding: Optimal feature selection when useful
features are rare and weak. Proceedings of the National Academy of Sciences  105(39):14790–14795 
2008.

[14] Jianqing Fan and Yingying Fan. High dimensional classiﬁcation using features annealed independence

rules. Annals of statistics  36(6):2605  2008.

[15] Jianqing Fan  Yang Feng  and Xin Tong. A road to classiﬁcation in high dimensional space: the regular-
ized optimal afﬁne discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodol-
ogy)  74(4):745–771  2012.

[16] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. LIBLINEAR: A

library for large linear classiﬁcation. The Journal of Machine Learning Research  9:1871–1874  2008.

[17] Yingying Fan  Jiashun Jin  Zhigang Yao  et al. Optimal classiﬁcation in sparse gaussian graphic model.

The Annals of Statistics  41(5):2537–2571  2013.

[18] Manuel Fern´andez-Delgado  Eva Cernadas  Sen´en Barro  and Dinani Amorim. Do we need hundreds of
classiﬁers to solve real world classiﬁcation problems? The Journal of Machine Learning Research  15
(1):3133–3181  2014.

[19] Jerome Friedman  Trevor Hastie  and Rob Tibshirani. Regularization paths for generalized linear models

via coordinate descent. Journal of statistical software  33(1):1  2010.

[20] Siddharth Gopal and Yiming Yang. Distributed training of Large-scale Logistic models. In Proceedings

of the 30th International Conference on Machine Learning (ICML-13)  pages 289–297  2013.

[21] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning: Data Mining  Inference 

and Prediction. Springer  2009.

[22] Mladen Kolar and Han Liu. Feature selection in high-dimensional classiﬁcation. In Proceedings of the

30th International Conference on Machine Learning (ICML-13)  pages 329–337  2013.

[23] Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Prob-
lems: Ecole dEt´e de Probabilit´es de Saint-Flour XXXVIII-2008  volume 2033. Springer Science & Busi-
ness Media  2011.

[24] Qing Mai  Hui Zou  and Ming Yuan. A direct approach to sparse discriminant analysis in ultra-high

dimensions. Biometrika  page asr066  2012.

[25] Enno Mammen  Alexandre B Tsybakov  et al. Smooth discrimination analysis. The Annals of Statistics 

27(6):1808–1829  1999.

[26] Sahand Negahban  Bin Yu  Martin J Wainwright  and Pradeep K Ravikumar. A uniﬁed framework for
high-dimensional analysis of M-estimators with decomposable regularizers. In Advances in Neural In-
formation Processing Systems  pages 1348–1356  2009.

[27] Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classiﬁers: A comparison of
In Advances in Neural Information Processing Systems 14 (NIPS

logistic regression and naive bayes.
2001)  2001.

[28] Jun Shao  Yazhen Wang  Xinwei Deng  Sijian Wang  et al. Sparse linear discriminant analysis by thresh-

olding for high dimensional data. The Annals of Statistics  39(2):1241–1265  2011.

[29] Robert Tibshirani  Trevor Hastie  Balasubramanian Narasimhan  and Gilbert Chu. Diagnosis of multiple
cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences 
99(10):6567–6572  2002.

[30] Sijian Wang and Ji Zhu. Improved centroids estimation for the nearest shrunken centroid classiﬁer. Bioin-

formatics  23(8):972–979  2007.

[31] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk mini-

mization. Annals of Statistics  pages 56–85  2004.

9

,Tianyang Li
Adarsh Prasad
Pradeep Ravikumar
Josip Djolonga
Andreas Krause