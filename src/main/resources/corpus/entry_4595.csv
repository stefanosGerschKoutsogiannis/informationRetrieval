2014,Recovery of Coherent Data via Low-Rank Dictionary Pursuit,The recently established RPCA method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality  RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed  its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows  the coherence of data keeps increasing  and accordingly  the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e.  data with high coherence) could be alleviated by Low-Rank Representation (LRR)~\cite{tpami_2013_lrr}  provided that the dictionary in LRR is configured appropriately. More precisely  we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Experiments on randomly generated matrices and real motion sequences verify our claims. See the full paper at arXiv:1404.4032.,Recovery of Coherent Data via Low-Rank

Dictionary Pursuit

Guangcan Liu

Ping Li

Department of Statistics and Biostatistics

Department of Statistics and Biostatistics

Department of Computer Science

Department of Computer Science

Rutgers University

Piscataway  NJ 08854  USA
gcliu@rutgers.edu

Rutgers University

Piscataway  NJ 08854  USA
pingli@rutgers.edu

Abstract

The recently established RPCA [4] method provides a convenient way to restore
low-rank matrices from grossly corrupted observations. While elegant in theory
and powerful in reality  RPCA is not an ultimate solution to the low-rank ma-
trix recovery problem. Indeed  its performance may not be perfect even when
data are strictly low-rank. This is because RPCA ignores clustering structures of
the data which are ubiquitous in applications. As the number of cluster grows 
the coherence of data keeps increasing  and accordingly  the recovery perfor-
mance of RPCA degrades. We show that the challenges raised by coherent data
(i.e.  data with high coherence) could be alleviated by Low-Rank Representation
(LRR) [13]  provided that the dictionary in LRR is conﬁgured appropriately. More
precisely  we mathematically prove that if the dictionary itself is low-rank then
LRR is immune to the coherence parameter which increases with the underlying
cluster number. This provides an elementary principle for dealing with coherent
data and naturally leads to a practical algorithm for obtaining proper dictionaries
in unsupervised environments. Experiments on randomly generated matrices and
real motion sequences verify our claims. See the full paper at arXiv:1404.4032.

1 Introduction

Nowadays our data are often high-dimensional  massive and full of gross errors (e.g.  corruptions 
outliers and missing measurements). In the presence of gross errors  the classical Principal Com-
ponent Analysis (PCA) method  which is probably the most widely used tool for data analysis and
dimensionality reduction  becomes brittle — A single gross error could render the estimate produced
by PCA arbitrarily far from the desired estimate. As a consequence  it is crucial to develop new sta-
tistical tools for robustifying PCA. A variety of methods have been proposed and explored in the
literature over several decades  e.g.  [2  3  4  8  9  10  11  12  24  13  16  19  25]. One of the most ex-
citing methods is probably the so-called RPCA (Robust Principal Component Analysis) method [4] 
which was built upon the exploration of the following low-rank matrix recovery problem:
Problem 1 (Low-Rank Matrix Recovery) Suppose we have a data matrix X ∈ Rm×n and we
know it can be decomposed as
(1.1)
where L0 ∈ Rm×n is a low-rank matrix each column of which is a data point drawn from some
low-dimensional subspace  and S0 ∈ Rm×n is a sparse matrix supported on Ω ⊆ {1 ···   m} ×
{1 ···   n}. Except these mild restrictions  both components are arbitrary. The rank of L0 is un-
known  the support set Ω (i.e.  the locations of the nonzero entries of S0) and its cardinality (i.e. 
the amount of the nonzero entries of S0) are unknown either. In particular  the magnitudes of the
nonzero entries in S0 may be arbitrarily large. Given X  can we recover both L0 and S0  in a
scalable and exact fashion?

X = L0 + S0 

1

cluster 1

cluster 2

Figure 1: Exemplifying the extra structures of low-rank data. Each entry of the data matrix is a grade
that a user assigns to a movie. It is often the case that such data are low-rank  as there exist wide
correlations among the grades that different users assign to the same movie. Also  such data could
own some clustering structure  since the preferences of the same type of users are more similar to
each other than to those with different gender  personality  culture and education background. In
summary  such data (1) are often low-rank and (2) exhibit some clustering structure.

The theory of RPCA tells us that  very generally  when the low-rank matrix L0 is meanwhile inco-
herent (i.e.  with low coherence)  both the low-rank and the sparse matrices can be exactly recovered
by using the following convex  potentially scalable program:

min
L S kLk∗ + λkSk1 

s.t. X = L + S 

(1.2)

where k · k∗ is the nuclear norm [7] of a matrix  k · k1 denotes the ℓ1 norm of a matrix seen as
a long vector  and λ > 0 is a parameter. Besides of its elegance in theory  RPCA also has good
empirical performance in many practical areas  e.g.  image processing [26]  computer vision [18] 
radar imaging [1]  magnetic resonance imaging [17]  etc.

While complete in theory and powerful in reality  RPCA cannot be an ultimate solution to the low-
rank matrix recovery Problem 1. Indeed  the method might not produce perfect recovery even when
L0 is strictly low-rank. This is because RPCA captures only the low-rankness property  which
however is not the only property of our data  but essentially ignores the extra structures (beyond
low-rankness) widely existing in data: Given the low-rankness constraint that the data points (i.e. 
columns vectors of L0) locate on a low-dimensional subspace  it is unnecessary for the data points
to locate on the subspace uniformly at random and it is quite normal that the data may have some
extra structures  which specify in more detail how the data points locate on the subspace. Figure 1
demonstrates a typical example of extra structures; that is  the clustering structures which are ubiq-
uitous in modern applications. Whenever the data are exhibiting some clustering structures  RPCA
is no longer a method of perfection. Because  as will be shown in this paper  while the rank of L0 is
ﬁxed and the underlying cluster number goes large  the coherence of L0 keeps heightening and thus 
arguably  the performance of RPCA drops.

To better handle coherent data (i.e.  the cases where L0 has large coherence parameters)  a seem-
ingly straightforward idea is to avoid the coherence parameters of L0. However  as explained in [4] 
the coherence parameters are indeed necessary (if there is no additional condition assumed on the
data). This paper shall further indicate that the coherence parameters are related in nature to some
extra structures intrinsically existing in L0 and therefore cannot be discarded simply. Interestingly 
we show that it is possible to avoid the coherence parameters by using some additional conditions 
which are easy to obey in supervised environment and can also be approximately achieved in un-
supervised environment. Our study is based on the following convex program termed Low-Rank
Representation (LRR) [13]:

min
Z S kZk∗ + λkSk1 

s.t. X = AZ + S 

(1.3)

where A ∈ Rm×d is a size-d dictionary matrix constructed in advance1  and λ > 0 is a parameter. In
order for LRR to avoid the coherence parameters which increase with the cluster number underlying

1It is not crucial to determine the exact value of d. Suppose Z ∗ is the optimal solution with respect to Z.
Then LRR uses AZ ∗ to restore L0. LRR falls back to RPCA when A = I (identity matrix). Furthermore  it can
be proved that the recovery produced by LRR is the same as RPCA whenever the dictionary A is orthogonal.

2

L0  we prove that it is sufﬁcient to construct in advance a dictionary A which is low-rank by itself.
This gives a generic prescription to defend the possible infections raised by coherent data  providing
an elementary criteria for learning the dictionary matrix A. Subsequently  we propose a simple and
effective algorithm that utilizes the output of RPCA to construct the dictionary in LRR. Our exten-
sive experiments demonstrated on randomly generated matrices and motion data show promising
results. In summary  the contributions of this paper include the following:

⋄ For the ﬁrst time  this paper studies the problem of recovering low-rank  and coherent (or
less incoherent as equal) matrices from their corrupted versions. We investigate the physical
regime where coherent data arise. For example  the widely existing clustering structures
may lead to coherent data. We prove some basic theories for resolving the problem  and
also establish a practical algorithm that outperforms RPCA in our experimental study.

⋄ Our studies help reveal the physical meaning of coherence  which is now standard and
widely used in various literatures  e.g.  [2  3  4  25  15]. We show that the coherence
parameters are not “assumptions” for a proof  but rather some excellent quantities that
relate in nature to the extra structures (beyond low-rankness) intrinsically existing in L0.
⋄ This paper provides insights regarding the LRR model proposed by [13]. While the special
case of A = X has been extensively studied  the LRR model (1.3) with general dictionaries
is not fully understood yet. We show that LRR (1.3) equipped with proper dictionaries
could well handle coherent data.

⋄ The idea of replacing L with AZ is essentially related to the spirit of matrix factorization
which has been explored for long  e.g.  [20  23]. In that sense  the explorations of this paper
help to understand why factorization techniques are useful.

2 Summary of Main Notations

Capital letters such as M are used to represent matrices  and accordingly  [M ]ij denotes its (i  j)th
entry. Letters U   V   Ω and their variants (complements  subscripts  etc.) are reserved for left singular
vectors  right singular vectors and support set  respectively. We shall abuse the notation U (resp. V )
to denote the linear space spanned by the columns of U (resp. V )  i.e.  the column space (resp. row
space). The projection onto the column space U   is denoted by PU and given by PU (M ) = U U T M  
and similarly for the row space PV (M ) = M V V T . We shall also abuse the notation Ω to denote
the linear space of matrices supported on Ω. Then PΩ and PΩ⊥ respectively denote the projections
onto Ω and Ωc such that PΩ + PΩ⊥ = I  where I is the identity operator. The symbol (·)+ denotes
the Moore-Penrose pseudoinverse of a matrix: M + = VM Σ−1
M for a matrix M with Singular
Value Decomposition (SVD)2 UM ΣM V T
M .
Six different matrix norms are used in this paper. The ﬁrst three norms are functions of the singular
values: 1) The operator norm (i.e.  the largest singular value) denoted by kMk  2) the Frobenius
norm (i.e.  square root of the sum of squared singular values) denoted by kMkF   and 3) the nuclear
norm (i.e.  the sum of singular values) denoted by kMk∗. The other three are the ℓ1  ℓ∞ (i.e. 
sup-norm) and ℓ2 ∞ norms of a matrix: kMk1 = Pi j |[M ]ij|  kMk∞ = maxi j{|[M ]ij|} and
kMk2 ∞ = maxj{qPi[M ]2
The Greek letter µ and its variants (e.g.  subscripts and superscripts) are reserved for the coherence
parameters of a matrix. We shall also reserve two lower case letters  m and n  to respectively denote
the data dimension and the number of data points  and we use the following two symbols throughout
this paper:

ij}  respectively.

M U T

n1 = max(m  n)

and n2 = min(m  n).

3 On the Recovery of Coherent Data

In this section  we shall ﬁrstly investigate the physical regime that raises coherent (or less incoher-
ent) data  and then discuss the problem of recovering coherent data from corrupted observations 
providing some basic principles and an algorithm for resolving the problem.

2In this paper  SVD always refers to skinny SVD. For a rank-r matrix M ∈ Rm×n  its SVD is of the form

UM ΣM V T

M   with UM ∈ Rm×r  ΣM ∈ Rr×r and VM ∈ Rn×r.

3

3.1 Coherence Parameters and Their Properties

As the rank function cannot fully capture all characteristics of L0  it is necessary to deﬁne some
quantities to measure the effects of various extra structures (beyond low-rankness) such as the clus-
tering structure as demonstrated in Figure 1. The coherence parameters deﬁned in [3  4] are excellent
exemplars of such quantities.

3.1.1 Coherence Parameters: µ1  µ2  µ3
0   some important properties can
For an m × n matrix L0 with rank r0 and SVD L0 = U0Σ0V T
be characterized by three coherence parameters  denoted as µ1  µ2 and µ3  respectively. The ﬁrst
coherence parameter  1 ≤ µ1(L0) ≤ m  which characterizes the column space identiﬁed by U0  is
deﬁned as

µ1(L0) =

m
r0

1≤i≤mkU T
max

0 eik2
2 

(3.4)

where ei denotes the ith standard basis. The second coherence parameter  1 ≤ µ2(L0) ≤ n  which
characterizes the row space identiﬁed by V0  is deﬁned as

µ2(L0) =

n
r0

1≤j≤n kV T
max

0 ejk2
2.

(3.5)

The third coherence parameter  1 ≤ µ3(L0) ≤ mn  which characterizes the joint space identiﬁed
by U0V T

0   is deﬁned as

µ3(L0) =

mn
r0

(kU0V T

0 k∞)2 =

mn
r0

max
i j

(|hU T

0 ei  V T

0 eji|)2.

(3.6)

The analysis in RPCA [4] merges the above three parameters into a single one: µ(L0) =
max{µ1(L0)  µ2(L0)  µ3(L0)}. As will be seen later  the behaviors of those three coherence pa-
rameters are different from each other  and hence it is more adequate to consider them individually.

3.1.2 µ2-phenomenon
According to the analysis in [4]  the success condition (regarding L0) of RPCA is

rank (L0) ≤

crn2

µ(L0)(log n1)2  

(3.7)

where µ(L0) = max{µ1(L0)  µ2(L0)  µ3(L0)} and cr > 0 is some numerical constant. Thus 
RPCA will be less successful when the coherence parameters are considerably larger. In this subsec-
tion  we shall show that the widely existing clustering structure can enlarge the coherence parameters
and  accordingly  downgrades the performance of RPCA.

Given the restriction that rank (L0) = r0  the data points (i.e.  column vectors of L0) are unneces-
sarily sampled from a r0-dimensional subspace uniformly at random. A more realistic interpretation
is to consider the data points as samples from the union of k number of subspaces (i.e.  clusters) 
and the sum of those multiple subspaces together has a dimension r0. That is to say  there are
multiple “small” subspaces inside one r0-dimensional “large” subspace  as exempliﬁed in Figure 1.
Whenever the low-rank matrix L0 is meanwhile exhibiting such clustering behaviors  the second
coherence parameter µ2(L0) (and so µ3(L0)) will increase with the number of clusters underlying
L0  as shown in Figure 2. When the coherence is heightening  (3.7) suggests that the performance
of RPCA will drop  as veriﬁed in Figure 2(d). Note here that the variation of µ3 is mainly due
to the variation of the row space  which is characterized by µ2. We call the phenomena shown in
Figure 2(b)∼(d) as the “µ2-phenomenon”. Readers can also refer to the full paper to see why the
second coherence parameter increases with the cluster number underlying L0.
Interestingly  one may have noticed that µ1 is invariant to the variation of the clustering number  as
can be seen from Figure 2(a). This is because the clustering behavior of the data points can only
affect the row space  while µ1 is deﬁned on the column space. Yet  if the row vectors of L0 also
own some clustering structure  µ1 could be large as well. Such kind of data can exist widely in text
documents and we leave this as future work.

4

(a)

(b)

1.5

1

1

µ

0.5

0

4

3

2

1

0

2

µ

1 2 5 10 20 50

#cluster

60

40

3

µ

(c)

(d)

0.3

0.2

0.1

r
o
r
r
e
 
r
e
v
o
c
e
r

1 2 5 10 20 50

#cluster

0

1

2

5 10 20 50

#cluster

20

0

1 2 5 10 20 50

#cluster

Figure 2: Exploring the inﬂuence of the cluster number  using randomly generated matrices. The
size and rank of L0 are ﬁxed to be 500 × 500 and 100  respectively. The underlying cluster number
varies from 1 to 50. For the recovery experiments  S0 is ﬁxed as a sparse matrix with 13% nonzero
entries. (a) The ﬁrst coherence parameter µ1(L0) vs cluster number. (b) µ2(L0) vs cluster number.
(c) µ3(L0) vs cluster number.
(d) Recover error (produced by RPCA) vs cluster number. The
numbers shown in these ﬁgure are averaged from 100 random trials. The recover error is computed
as k ˆL0 − L0kF /kL0kF   where ˆL0 denotes an estimate of L0.
3.2 Avoiding µ2 by LRR

The µ2-phenomenon implies that the second coherence parameter µ2 is related in nature to some
intrinsic structures of L0 and thus cannot be eschewed without using additional conditions. In the
following  we shall ﬁgure out under what conditions the second coherence parameter µ2 (and µ3)
can be avoided such that LRR could well handle coherent data.
Main Result: We show that  when the dictionary A itself is low-rank  LRR is able to avoid µ2.
Namely  the following theorem is proved without using µ2. See the full paper for a detailed proof.
Theorem 1 (Noiseless) Let A ∈ Rm×d with SVD A = UAΣAV T
A be a column-wisely unit-normed
(i.e.  kAeik2 = 1 ∀i) dictionary matrix which satisﬁes PUA(U0) = U0 (i.e.  U0 is a subspace of
UA). For any 0 < ǫ < 0.5 and some numerical constant ca > 1  if

rank (L0) ≤ rank (A) ≤
then with probability at least 1 − n−10
1/√n1 is unique and exact  in a sense that

1

ǫ2n2

caµ1(A) log n1

and |Ω| ≤ (0.5 − ǫ)mn 

(3.8)

  the optimal solution to the LRR problem (1.3) with λ =

and S ∗ = S0 

Z ∗ = A+L0
where (Z ∗  S ∗) is the optimal solution to (1.3).
It is worth noting that the restriction rank (L0) ≤ O(n2/ log n1) is looser than that of PRCA3  which
requires rank (L0) ≤ O(n2/(log n1)2). The requirement of column-wisely unit-normed dictionary
(i.e.  kAeik2 = 1 ∀i) is purely for complying the parameter estimate of λ = 1/√n1  which is
consistent with RPCA. The condition PUA(U0) = U0  i.e.  U0 is a subspace of UA  is indispensable
if we ask for exact recovery  because PUA (U0) = U0 is implied by the equality AZ ∗ = L0. This
necessary condition  together with the low-rankness condition  provides an elementary criterion for
learning the dictionary matrix A in LRR. Figure 3 presents an example  which further conﬁrms our
main result; that is  LRR is able to avoid µ2 as long as U0 ⊂ UA and A is low-rank. It is also
worth noting that it is unnecessary for A to satisfy UA = U0  and that LRR is actually tolerant to the
“errors” possibly existing in the dictionary.

The program (1.3) is designed for the case where the uncorrupted observations are noiseless. In
reality this assumption is often not true and all entries of X can be contaminated by a small amount
of noises  i.e.  X = L0 + S0 + N   where N is a matrix of dense Gaussian noises. In this case  the
formula of LRR (1.3) need be modiﬁed to

min
Z S kZk∗ + λkSk1 

s.t.

kX − AZ − SkF ≤ ε 

(3.9)

3In terms of exact recovery  O(n2/ log n1) is probably the “ﬁnest” bound one could accomplish in theory.

5

X

AZ*

S*

0.2

0.1

r
o
r
r
e

 
r
e
v
o
c
e
r

0
1 5 10 15 20

rank(A)

Figure 3: Exemplifying that LRR can void µ2. In this experiment  L0 is a 200 × 200 rank-1 matrix
with one column being 1 (i.e.  a vector of all ones) and everything else being zero. Thus  µ1(L0) = 1
and µ2(L0) = 200. The dictionary is set as A = [1  W ]  where W is a 200 × p random Gaussian
matrix (with varying p). As long as rank (A) = p + 1 ≤ 10  LRR with λ = 0.08 can exactly recover
L0 from a grossly corrupted observation matrix X.

where ε is a parameter that measures the noise level of data.
In the experiments of this paper 
we consistently set ε = 10−6kXkF . In the presence of dense noises  the latent matrices  L0 and
S0  cannot be exactly restored. Yet we have the following theorem to guarantee the near recovery
property of the solution produced by the program (3.9):
Theorem 2 (Noisy) Suppose kX − L0 − S0kF ≤ ε. Let A ∈ Rm×d with SVD A = UAΣAV T
A be a
column-wisely unit-normed dictionary matrix which satisﬁes PUA(U0) = U0 (i.e.  U0 is a subspace
of UA). For any 0 < ǫ < 0.35 and some numerical constant ca > 1  if

(3.10)
rank (L0) ≤ rank (A) ≤
caµ1(A) log n1
  any solution (Z ∗  S ∗) to (3.9) with λ = 1/√n1 gives a near
then with probability at least 1− n−10
recovery to (L0  S0)  in a sense that kAZ ∗ − L0kF ≤ 8√mnε and kS ∗ − S0kF ≤ (8√mn + 2)ε.

and |Ω| ≤ (0.35 − ǫ)mn 

ǫ2n2

1

3.3 An Unsupervised Algorithm for Matrix Recovery

To handle coherent (equivalently  less incoherent) data  Theorem 1 suggests that the dictionary ma-
trix A should be low-rank and satisfy U0 ⊂ UA. In certain supervised environment  this might not be
difﬁcult as one could potentially use clear  well processed training data to construct the dictionary. In
an unsupervised environment  however  it will be challenging to identify a low-rank dictionary that
obeys U0 ⊂ UA. Note that U0 ⊂ UA can be viewed as supervision information (if A is low-rank).
In this paper  we will introduce a heuristic algorithm that can work distinctly better than RPCA in
an unsupervised environment. As can be seen from (3.7)  RPCA is actually not brittle with respect
to coherent data (although its performance is depressed). Based on this observation  we propose
a simple algorithm  as summarized in Algorithm 1  to achieve a solid improvement over RPCA.
Our idea is straightforward: We ﬁrst obtain an estimate of L0 by using RPCA and then utilize the
estimate to construct the dictionary matrix A in LRR. The post-processing steps (Step 2 and Step 3)
that slightly modify the solution of RPCA is to encourage well-conditioned dictionary  which is the
circumstance favoring LRR.

Whenever the recovery produced by RPCA is already exact  the claim in Theorem 1 gives that the
recovery produced by our Algorithm 1 is exact as well. That is to say  in terms of exactly recovering
L0 from a given X  the success probability of our Algorithm 1 is greater than or equal to that of
RPCA. From the computational perspective  Algorithm 1 does not really double the work of RPCA 
although there are two convex programs in our algorithm. In fact  according to our simulations 
usually the computational time of Algorithm 1 is merely about 1.2 times as much as RPCA. The
reason is that  as has been explored by [13]  the complexity of solving the LRR problem (1.3) is
O(n2rA) (assuming m = n)  which is much lower than that of RPCA (which requires O(n3))
provided that the obtained dictionary matrix A is fairly low-rank (i.e.  rA is small).
One may have noticed that the procedure of Algorithm 1 could be made iterative  i.e.  one can
consider ˆAZ ∗ as a new estimate of L0 and use it to further update the dictionary matrix A  and so
on. Nevertheless  we empirically ﬁnd that such an iterative procedure often converges within two
iterations. Hence  for the sake of simplicity  we do not consider iterative strategies in this paper.

6

Algorithm 1 Matrix Recovery

input: Observed data matrix X ∈ Rm×n.
adjustable parameter: λ.
1. Solve for ˆL0 by optimizing the RPCA problem (1.2) with λ = 1/√n1.
2. Estimate the rank of ˆL0 by

where σ1  σ2 ···   σn2 are the singular values of ˆL0.
3. Form ˜L0 by using the rank-ˆr0 approximation of ˆL0. That is 

ˆr0 = #{i : σi > 10−3σ1} 

˜L0 = arg min

L kL − ˆL0k2

F   s.t. rank (L) ≤ ˆr0 

which is solved by SVD.
4. Construct a dictionary ˆA from ˜L0 by normalizing the column vectors of ˜L0:

[ ˆA]: i =

[ ˜L0]: i
k[ ˜L0]: ik2
where [·]: i denotes the ith column of a matrix.
5. Solve for Z ∗ by optimizing the LRR problem (1.3) with A = ˆA and λ = 1/√n1.
output: ˆAZ ∗.

  i = 1 ···   n 

4 Experiments

4.1 Results on Randomly Generated Matrices

We ﬁrst verify the effectiveness of our Algorithm 1 on randomly generated matrices. We generate
a collection of 200 × 1000 data matrices according to the model of X = PΩ⊥(L0) + PΩ(S0):
Ω is a support set chosen at random; L0 is created by sampling 200 data points from each of 5
randomly generated subspaces; S0 consists of random values from Bernoulli ±1. The dimension of
each subspace varies from 1 to 20 with step size 1  and thus the rank of L0 varies from 5 to 100 with
step size 5. The fraction |Ω|/(mn) varies from 2.5% to 50% with step size 2.5%. For each pair of
rank and support size (r0 |Ω|)  we run 10 trials  resulting in a total of 4000 (20 × 20 × 10) trials.

RPCA

Algorithm 1

)

%

(
 
n
o

i
t

p
u
r
r
o
c

42

32

22

12

2

)

%

(
 

n
o

i
t
p
u
r
r
o
c

0.1 0.2 0.3 0.4 0.5

)/n
rank(L
0
2

42

32

22

12

2

)

%

(
 

n
o

i
t
p
u
r
r
o
c

50

40

30

20

10

 

 

RPCA
Algorithm 1

0.1 0.2 0.3 0.4 0.5

)/n
rank(L
0
2

0.1 0.2 0.3 0.4 0.5

)/n
rank(L
0
2

Figure 4: Algorithm 1 vs RPCA for the task of recovering randomly generated matrices  both using
λ = 1/√n1. A curve shown in the third subﬁgure is the boundary for a method to be successful
— The recovery is successful for any pair (r0/n2 |Ω|/(mn)) that locates below the curve. Here  a
success means k ˆL0 − L0kF < 0.05kL0kF   where ˆL0 denotes an estimate of L0.
Figure 4 compares our Algorithm 1 to RPCA  both using λ = 1/√n1. It can be seen that  using the
learned dictionary matrix  Algorithm 1 works distinctly better than RPCA. In fact  the success area
(i.e.  the area of the white region) of our algorithm is 47% wider than that of RPCA! We should also
mention that it is possible for RPCA to be exactly successful on coherent (or less incoherent) data 
provided that the rank of L0 is low enough and/or S0 is sparse enough. Our algorithm in general
improves RPCA when L0 is moderately low-rank and/or S0 is moderately sparse.

7

4.2 Results on Corrupted Motion Sequences
We now present our experiment with 11 additional sequences attached to the Hopkins155 [21]
database. In those sequences  about 10% of the entries in the data matrix of trajectories are un-
observed (i.e.  missed) due to vision occlusion. We replace each missed entry with a number from
Bernoulli ±1  resulting in a collection of corrupted trajectory matrices for evaluating the effective-
ness of matrix recovery algorithms. We perform subspace clustering on both the corrupted trajectory
matrices and the recovered versions  and use the clustering error rates produced by existing subspace
clustering methods as the evaluation metrics. We consider three state-of-the-art subspace clustering
methods: Shape Interaction Matrix (SIM) [5]  Low-Rank Representation with A = X [14] (which
is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) [6].

Table 1: Clustering error rates (%) on 11 corrupted motion sequences.

SIM

RPCA + SIM

Algorithm 1 + SIM

LRRx

RPCA + LRRx

Algorithm 1 + LRRx

SSC

RPCA + SSC

Algorithm 1 + SSC

Mean Median Maximum Minimum Std.
11.74
29.19
16.23
14.82
12.95
8.74
17.10
21.38
10.70
15.63
10.59
7.09
18.46
22.81
16.17
9.50
5.74
8.52

12.45
0.97
0.23
0.58
0.20
0.22
1.55
0.61
0.20

27.77
8.38
3.09
22.00
3.05
3.06
20.78
2.13
1.85

45.82
45.78
42.61
56.96
46.25
32.33
58.24
50.32
27.84

Time (sec.)

0.07
9.96
11.64
1.80
10.75
12.11
3.18
12.51
13.11

Table 1 shows the error rates of various algorithms. Without the preprocessing of matrix recovery 
all the subspace clustering methods fail to accurately categorize the trajectories of motion objects 
producing error rates higher than 20%. This illustrates that it is important for motion segmentation
to correct the gross corruptions possibly existing in the data matrix of trajectories. By using RPCA
(λ = 1/√n1) to correct the corruptions  the clustering performances of all considered methods are
improved dramatically. For example  the error rate of SSC is reduced from 22.81% to 9.50%. By
choosing an appropriate dictionary for LRR (λ = 1/√n1)  the error rates can be reduced again 
from 9.50% to 5.74%  which is a 40% relative improvement. These results verify the effectiveness
of our dictionary learning strategy in realistic environments.

5 Conclusion and Future Work

We have studied the problem of disentangling the low-rank and sparse components in a given data
matrix. Whenever the low-rank component exhibits clustering structures  the state-of-the-art RPCA
method could be less successful. This is because RPCA prefers incoherent data  which however may
be inconsistent with data in the real world. When the number of clusters becomes large  the second
and third coherence parameters enlarge and hence the performance of RPCA could be depressed. We
have showed that the challenges arising from coherent (equivalently  less incoherent) data could be
effectively alleviated by learning a suitable dictionary under the LRR framework. Namely  when the
dictionary matrix is low-rank and contains information about the ground truth matrix  LRR can be
immune to the coherence parameters that increase with the underlying cluster number. Furthermore 
we have established a practical algorithm that outperforms RPCA in our extensive experiments.

The problem of recovering coherent data essentially concerns the robustness issues of the General-
ized PCA (GPCA) [22] problem. Although the classic GPCA problem has been explored for several
decades  robust GPCA is new and has not been well studied. The approach proposed in this pa-
per is in a sense preliminary  and it is possible to develop other effective methods for learning the
dictionary matrix in LRR and for handling coherent data. We leave these as future work.

Acknowledgement

Guangcan Liu was a Postdoctoral Researcher supported by NSF-DMS0808864  NSF-SES1131848 
NSF-EAGER1249316  AFOSR-FA9550-13-1-0137  and ONR-N00014-13-1-0764. Ping Li is also
partially supported by NSF-III1360971 and NSF-BIGDATA1419210.

8

References
[1] Liliana Borcea  Thomas Callaghan  and George Papanicolaou. Synthetic aperture radar imaging and

motion estimation via robust principle component analysis. Arxiv  2012.

[2] Emmanuel Cand`es and Yaniv Plan. Matrix completion with noise. In IEEE Proceeding  volume 98  pages

925–936  2010.

[3] Emmanuel Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics  9(6):717–772  2009.

[4] Emmanuel J. Cand`es  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis?

Journal of the ACM  58(3):1–37  2011.

[5] Joao Costeira and Takeo Kanade. A multibody factorization method for independently moving objects.

International Journal of Computer Vision  29(3):159–179  1998.

[6] E. Elhamifar and R. Vidal. Sparse subspace clustering. In IEEE Conference on Computer Vision and

Pattern Recognition  volume 2  pages 2790–2797  2009.

[7] M. Fazel. Matrix rank minimization with applications. PhD thesis  2002.
[8] Martin Fischler and Robert Bolles. Random sample consensus: A paradigm for model ﬁtting with ap-
plications to image analysis and automated cartography. Communications of the ACM  24(6):381–395 
1981.

[9] R. Gnanadesikan and J. R. Kettenring. Robust estimates  residuals  and outlier detection with multire-

sponse data. Biometrics  28(1):81–124  1972.

[10] D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis.

Information Theory  57(3):1548–1566  2011.

IEEE Transactions on

[11] Qifa Ke and Takeo Kanade. Robust l1 norm factorization in the presence of outliers and missing data
by alternative convex programming. In IEEE Conference on Computer Vision and Pattern Recognition 
pages 739–746  2005.

[12] Fernando De la Torre and Michael J. Black. A framework for robust subspace learning. International

Journal of Computer Vision  54(1-3):117–142  2003.

[13] Guangcan Liu  Zhouchen Lin  Shuicheng Yan  Ju Sun  Yong Yu  and Yi Ma. Robust recovery of subspace
structures by low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence 
35(1):171–184  2013.

[14] Guangcan Liu  Zhouchen Lin  and Yong Yu. Robust subspace segmentation by low-rank representation.

In International Conference on Machine Learning  pages 663–670  2010.

[15] Guangcan Liu  Huan Xu  and Shuicheng Yan. Exact subspace segmentation and outlier detection by
low-rank representation. Journal of Machine Learning Research - Proceedings Track  22:703–711  2012.
[16] Rahul Mazumder  Trevor Hastie  and Robert Tibshirani. Spectral regularization algorithms for learning

large incomplete matrices. Journal of Machine Learning Research  11:2287–2322  2010.

[17] Ricardo Otazo  Emmanuel Cand`es  and Daniel K. Sodickson. Low-rank and sparse matrix decomposition

for accelerated dynamic mri with separation of background and dynamic components. Arxiv  2012.

[18] YiGang Peng  Arvind Ganesh  John Wright  Wenli Xu  and Yi Ma. Rasl: Robust alignment by sparse
and low-rank decomposition for linearly correlated images. IEEE Transactions on Pattern Analysis and
Machine Intelligence  34(11):2233–2246  2012.

[19] Mahdi Soltanolkotabi  Ehsan Elhamifar  and Emmanuel Candes.

arXiv:1301.2603  2013.

Robust subspace clustering.

[20] Nathan Srebro and Tommi Jaakkola. Generalization error bounds for collaborative prediction with low-

rank matrices. In Neural Information Processing Systems  pages 5–27  2005.

[21] Roberto Tron and Rene Vidal. A benchmark for the comparison of 3-d motion segmentation algorithms.

In IEEE Conference on Computer Vision and Pattern Recognition  pages 1–8  2007.

[22] Rene Vidal  Yi Ma  and S. Sastry. Generalized Principal Component Analysis. Springer Verlag  2012.
[23] Markus Weimer  Alexandros Karatzoglou  Quoc V. Le  and Alex J. Smola. Coﬁ rank - maximum margin

matrix factorization for collaborative ranking. In Neural Information Processing Systems  2007.

[24] Huan Xu  Constantine Caramanis  and Shie Mannor. Outlier-robust pca: The high-dimensional case.

IEEE Transactions on Information Theory  59(1):546–572  2013.

[25] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust pca via outlier pursuit. In Neural Infor-

mation Processing Systems  2010.

[26] Zhengdong Zhang  Arvind Ganesh  Xiao Liang  and Yi Ma. Tilt: Transform invariant low-rank textures.

International Journal of Computer Vision  99(1):1–24  2012.

9

,Guangcan Liu
Ping Li