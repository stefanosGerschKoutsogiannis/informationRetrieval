2013,Structured Learning via Logistic Regression,A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages  and iterate between updates to each. This paper observes that if the inference problem is “smoothed” through the addition of entropy terms  for fixed messages  the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems  each training example has a bias term determined by the current set of messages. Based on this insight  the structured energy function can be extended from linear factors to any function class where an “oracle” exists to minimize a logistic loss.,Structured Learning via Logistic Regression

NICTA and The Australian National University

Justin Domke

justin.domke@nicta.com.au

Abstract

A successful approach to structured learning is to write the learning objective as
a joint function of linear parameters and inference messages  and iterate between
updates to each. This paper observes that if the inference problem is “smoothed”
through the addition of entropy terms  for ﬁxed messages  the learning objective
reduces to a traditional (non-structured) logistic regression problem with respect
to parameters. In these logistic regression problems  each training example has a
bias term determined by the current set of messages. Based on this insight  the
structured energy function can be extended from linear factors to any function
class where an “oracle” exists to minimize a logistic loss.

1 Introduction

The structured learning problem is to ﬁnd a function F (x  y) to map from inputs x to outputs as
y∗ = arg maxy F (x  y). F is chosen to optimize a loss function deﬁned on these outputs. A
major challenge is that evaluating the loss for a given function F requires solving the inference
optimization to ﬁnd the highest-scoring output y for each exemplar  which is NP-hard in general.
A standard solution to this is to write the loss function using an LP-relaxation of the inference
problem  meaning an upper-bound on the true loss. The learning problem can then be phrased as a
joint optimization of parameters and inference variables  which can be solved  e.g.  by alternating
message-passing updates to inference variables with gradient descent updates to parameters [16  9].
Previous work has mostly focused on linear energy functions F (x  y) = wT Φ(x  y)  where a vector

of weights w is adjusted in learning  and Φ(x  y) = !α Φ(x  yα) decomposes over subsets of

variables yα. While linear weights are often useful in practice [23  16  9  3  17  12  5]  it is also
common to make use of non-linear classiﬁers. This is typically done by training a classiﬁer (e.g.
ensembles of trees [20  8  25  13  24  18  19] or multi-layer perceptrons [10  21]) to predict each
variable independently. Linear edge interaction weights are then learned  with unary classiﬁers
either held ﬁxed [20  8  25  13  24  10] or used essentially as “features” with linear weights re-
adjusted [18].

This paper allows the more general form F (x  y) =!α fα(x  yα). The learning problem is to select

fα from some set of functions Fα. Here  following previous work [15]  we add entropy smoothing
to the LP-relaxation of the inference problem. Again  this leads to phrasing the learning problem as a
joint optimization of learning and inference variables  alternating between message-passing updates
to inference variables and optimization of the functions fα. The major result is that minimization of
the loss over fα ∈ Fα can be re-formulated as a logistic regression problem  with a “bias” vector
added to each example reﬂecting the current messages incoming to factor α. No assumptions are
needed on the sets of functions Fα  beyond assuming that an algorithm exists to optimize the logistic
loss on a given dataset over all fα ∈ Fα
We experimentally test the results of varying Fα to be the set of linear functions  multi-layer per-
ceptrons  or boosted decision trees. Results verify the beneﬁts of training ﬂexible function classes
in terms of joint prediction accuracy.

1

2 Structured Prediction

The structured prediction problem can be written as seeking a function h that will predict an output
y from an input x. Most commonly  it can be written in the form
wT Φ(x  y) 

h(x; w) = arg max

(1)

y

where Φ is a ﬁxed function of both x and y. The maximum takes place over all conﬁgurations of the
discrete vector y. It is further assumed that Φ decomposes into a sum of functions evaluated over
subsets of variables yα as

Φ(x  y) =!α

Φα(x  yα).

The learning problem is to adjust set of linear weights w. This paper considers the structured learning
problem in a more general setting  directly handling nonlinear function classes. We generalize the
function h to

where the energy F again decomposes as

h(x; F ) = arg max

y

F (x  y) 

F (x  y) =!α

fα(x  yα).

The learning problem now becomes to select {fα ∈ Fα} for some set of functions Fα. This reduces
to the previous case when fα(x  yα) = wT Φα(x  yα) is a linear function. Here  we do not make any
assumption on the class of functions Fα other than assuming that there exists an algorithm to ﬁnd
the best function fα ∈ Fα in terms of the logistic regression loss (Section 6).

3 Loss Functions

Given a dataset (x1  y1)  ...  (xN   yN )  we wish to select the energy F to minimize the empirical risk

for some loss function l. Absent computational concerns  a standard choice would be the slack-
rescaled loss [22]

R(F ) =!k

l(xk  yk; F ) 

(2)

l0(xk  yk; F ) = max

y

F (xk  y) − F (xk  yk) + ∆(yk  y) 

(3)

where ∆(yk  y) is some measure of discrepancy. We assume that ∆ is a function that decomposes

over α  (i.e. that ∆(yk  y) ="α ∆α(yk

In Eq. 3  the maximum ranges over all possible discrete labelings y  which is in NP-hard in general.
If this inference problem must be solved approximately  there is strong motivation [6] for using
relaxations of the maximization in Eq. 1  since this yields an upper-bound on the loss. A common
solution [16  14  6] is to use a linear relaxation1

α  yα)). Our experiments use the Hamming distance.

l1(xk  yk; F ) = max
µ∈M

F (xk  µ) − F (xk  yk) + ∆(yk  µ) 

(4)

where the local polytope M is deﬁned as the set of local pseudomarginals that are normalized  and
agree when marginalized over other neighboring regions 

M = {µ|µαβ(yβ) = µβ(yβ) ∀β ⊂ α  !yα

µα(yα) = 1 ∀α  µα(yα) ≥ 1 ∀α  yα}.

Here  µαβ(yβ) = "yα\β

µα(yα) is µα marginalized out over some region β contained in α. It is
easy to show that l1 ≥ l0  since the two would be equivalent if µ were restricted to binary values 
and hence the maximization in l1 takes place over a larger set [6]. We also deﬁne

θk
F (yα) = fα(xk  yα) + ∆α(yk

α  yα) 

(5)

1Here  F and ∆ are slightly generalized to allow arguments of pseudomarginals  as F (xk  µ) =

!α !yα

f(xk  yα)µ(yα) and ∆(yk  µ) = !α !yα

∆α(yk

α  yα)µ(yα).

2

which gives the equivalent representation of l1 as l1(xk  yk; F ) = −F (xk  yk) + maxµ∈M θk
F · µ.
The maximization in l1 is of a linear objective under linear constraints  and is thus a linear program
(LP)  solvable in polynomial time using a generic LP solver. In practice  however  it is preferable to
use custom solvers based on message-passing that exploit the sparsity of the problem.
Here  we make a further approximation to the loss  replacing the inference problem of maxµ∈M θ ·µ

with the “smoothed” problem maxµ∈M θ · µ + !α H(µα)  where H(µα) is the entropy of the

marginals µα. This approximation has been considered by Meshi et al. [15] who show that local
message-passing can have a guaranteed convergence rate  and by Hazan and Urtasun [9] who use it
for learning. The relaxed loss is

l(xk  yk; F ) = −F (xk  yk) + max

µ∈M"θk

F · µ + #α

H(µα)$ .

(6)

Since the entropy is positive  this is clearly a further upper-bound on the “unsmoothed” loss  i.e.
l1 ≤ l. Moreover  we can bound the looseness of this approximation as in the following theorem 
proved in the appendix. A similar result was previously given [15] bounding the difference of the
objective obtained by inference with and without entropy smoothing.
Theorem 1. l and l1 are bounded by (where |yα| is the number of conﬁgurations of yα)

l1(x  y  F ) ≤ l(x  y  F ) ≤ l1(x  y  F ) + Hmax  Hmax =#α

log |yα|.

4 Overview

Now  the learning problem is to select the functions fα composing F to minimize R as deﬁned in
Eq. 2. The major challenge is that evaluating R(F ) requires performing inference. Speciﬁcally  if
we deﬁne

then we have that

A(θ) = max
µ∈M

θ · µ + #α

H(µα) 

(7)

min
F

R(F ) = min

F #k %−F (xk  yk) + A(θk
F )& .

Since A(θ) contains a maximization  this is a saddle-point problem. Inspired by previous work
[16  9]  our solution (Section 5) is to introduce a vector of “messages” λ to write A in the dual form

which leads to phrasing learning as the joint minimization

A(θ) = min

A(λ  θ) 

λ

min
F

min

{λk}#k ’−F (xk  yk) + A(λk  θk
F )( .

We propose to solve this through an alternating optimization of F and {λk}. For ﬁxed F   message-
passing can be used to perform coordinate ascent updates to all the messages λk (Section 5). These
updates are trivially parallelized with respect to k. However  the problem remains  for ﬁxed mes-
sages  how to optimize the functions fα composing F . Section 7 observes that this problem can be
re-formulated into a (non-structured) logistic regression problem  with “bias” terms added to each
example that reﬂect the current messages into factor α.

5 Inference

In order to evaluate the loss  it is necessary to solve the maximization in Eq. 6. For a given θ 
consider doing inference over µ  that is  in solving the maximization in Eq. 7. Standard Lagrangian
duality theory gives the following dual representation for A(θ) in terms of “messages” λα(xβ) from
a region α to a subregion β ⊂ α  a variant of the representation of Heskes [11].

3

Algorithm 1 Reducing structured learning to logistic regression.
For all k  α  initialize λk(yα) ← 0.
Repeat until convergence:
1. For all k  for all α  set the bias term to

α  yα) + #β⊂α

λk

α(yβ) −#γ⊃α

λk

γ(yα)
 .

bk
α(yα) ←

1

 
∆(yk
#k=1&’fα(xk  yk

K

2. For all α  solve the logistic regression problem

fα ← arg max
fα∈Fα

α) + bk

α(yk

α)( − log#yα

exp’fα(xk  yα) + bk

α(yα)() .

3. For all k  for all α  form updated parameters as

θk(yα) ← fα(xk  yα) + ∆(yk

α  yα).

4. For all k  perform a ﬁxed number of message-passing iterations to update λk using θk. (Eq. 10)

Theorem 2. A(θ) can be represented in the dual form A(θ) = minλ A(λ  θ)  where

A(λ  θ) = max
µ∈N

θ · µ + #α

H(µα) +#α #β⊂α#xβ

λα(xβ) (µαβ(yβ) − µβ(yβ))  

(8)

µα(yα) = 1  µα(yα) ≥ 0} is the set of locally normalized pseudomarginals.

and N = {µ|*yα

Moreover  for a ﬁxed λ  the maximizing µ is given by

µα(yα) =

 
θ(yα) + #β⊂α
where Zα is a normalizing constant to ensure that*yα

exp


1
Zα

1

λα(yβ) −#γ⊃α

µα(yα) = 1.

λγ(yα)



  

(9)

Thus  for any set of messages λ  there is an easily-evaluated upper-bound A(λ  θ) ≥ A(θ)  and when
A(λ  θ) is minimized with respect to λ  this bound is tight. The standard approach to performing
the minimization over λ is essentially block-coordinate descent. There are variants  depending on
the size of the “block” that is updated. In our experiments  we use blocks consisting of the set of all
messages λα(yν) for all regions α containing ν. When the graph only contains regions for single
variables and pairs  this is a “star update” of all the messages from pairs that contain a variable i. It
can be shown [11  15] that the update is

λ$
α(yν) ← λα(yν) +



1 + Nν

(log µν(yν) + #α!⊃ν

log µα!(yν)) −  log µα(yν) 

(10)

for all α ⊃ ν  where Nν = |{α|α ⊃ ν}|. Meshi et al. [15] show that with greedy or randomized
selection of blocks to update  O( 1

δ ) iterations are sufﬁcient to converge within error δ.

6 Logistic Regression

Logistic regression is traditionally understood as deﬁning a conditional distribution p(y|x; W ) =
exp ((W x)y) /Z(x) where W is a matrix that maps the input features x to a vector of mar-
gins W x.
likelihood training problem

the maximum conditional

is easy to show that

It

maxW *k log p(yk|xk; W ) is equivalent to

max

W #k &(W xk)yk − log#y

exp(W xk)y) .

4

Here  we generalize this in two ways. First  rather than taking the mapping from features x to the
margin for label y as the y-th component of W x  we take it as f (x  y) for some function f in a set
of function F. (This reduces to the linear case when f (x  y) = (W x)y.) Secondly  we assume that
there is a pre-determined “bias” vector bk associated with each training example. This yields the
learning problem

max

f ∈F !k "#f (xk  yk) + bk(yk)$ − log!y

exp#f (xk  y) + bk(y)$%  

(11)

Aside from linear logistic regression  one can see decision trees  multi-layer perceptrons  and
boosted ensembles under an appropriate loss as solving Eq. 11 for different sets of functions F
(albeit possibly to a local maximum).

7 Training

Recall that the learning problem is to select the functions fα ∈ Fα so as to minimize the empirical
F )]. At ﬁrst blush  this appears challenging  since evaluating
A(θ) requires solving a message-passing optimization. However  we can use the dual representation
of A from Theorem 2 to represent minF R(F ) in the form

risk R(F ) = &k[−F (xk  yk) + A(θk

min
F

min

{λk}!k ’−F (xk  yk) + A(λk  θk
F )( .

(12)

To optimize Eq. 12  we alternating between optimization of messages {λk} and energy functions
{fα}. Optimization with respect to λk for ﬁxed F decomposes into minimizing A(λk  θk
F ) indepen-
dently for each yk  which can be done by running message-passing updates as in Section 5 using the
parameter vector θk
F . Thus  the rest of this section is concerned with how to optimize with respect
to F for ﬁxed messages. Below  we will use a slight generalization of a standard result [1  p. 93].
Lemma 3. The conjugate of the entropy is the “log-sum-exp” function. Formally 

max

x:xT 1=1 x≥0

θ · x − ρ!i

xi log xi = ρ log!i

exp

θi
ρ

.

Theorem 4. If f ∗

α is the minimizer of Eq 12 for ﬁxed messages λ  then

f ∗
α =  arg max

α) + bk

α(yk

where the set of biases are deﬁned as

fα !k "#fα(xk  yk
 
∆(yk

bk
α(yα) =

1

α)$ − log!yα

exp#fα(xk  yα) + bk

α(yα)$%  

α  yα) + !β⊂α

λα(yβ) −!γ⊃α

λγ(yα)
 .

(13)

(14)

Proof. Substituting A(λ  θ) from Eq. 8 and θk from Eq. 5 gives that

A(λk  θk

F ) = max

µ∈N !α !yα #fα(xk  yα) + ∆α(yk

H(µα)

α  yα)$ µ(yα) + !α
+!α !β⊂α!xβ

λk
α(xβ) (µαβ(yβ) − µβ(yβ)) .

Using the deﬁnition of bk from Eq. 14 above  this simpliﬁes into

A(λk  θk

F ) =!α

max

µα∈Nα-!yα

(fα(x  yα) + bα(yα)) µα(yα) + H(µα).  

5

Denoising

Fi \ Fij Zero Const. Linear Boost. MLP
.502
.502
.034
.007
.008

Zero
Const.
Linear
Boost.
MLP

.502
.502
.059
.015
.015

.502
.502
.444
.444
.445

.502
.502
.077
.034
.032

.511
.510
.049
.009
.009

Horses

Fi \ Fij Zero Const. Linear Boost. MLP
.245
.245
.156
.086
.081

Zero
Const.
Linear
Boost.
MLP

.244
.244
.154
.084
.080

.246
.246
.185
.103
.096

.246
.246
.185
.098
.094

.247
.247
.168
.092
.087

Table 1: Univariate Test Error Rates (Train Errors in Appendix)

marginals. Applying Lemma 3 to the inner maximization gives the closed-form expression

µα(yα) = 1  µα(yα) ≥ 0} enforces that µα is a locally normalized set of

where Nα = {µα|!yα

A(λk  θk

F ) ="α

 log"yα

exp# 1



fα(x  yα) + bα(yα)$ .

Thus  minimizing Eq. 12 with respect to F is equivalent to ﬁnding (for all α)

arg max

fα "k %fα(xk  yk
fα "k % 1





α) −  log"yα
α) − log"yα

exp# 1
exp# 1

= arg max

fα(xk  yk

α(yα)$&
α(yα)$&

fα(x  yα) + bk

f (xk  yα) + bk



Observing that adding a bias term doesn’t change the maximizing fα  and using the fact that
arg max g( 1

 ·) =  arg max g(·) gives the result.

The ﬁnal learning algorithm is summarized as Alg. 1. Sometimes  the local classiﬁer fα will
depend on the input x only through some “local features” φα. The above framework accomodates
this situation if the set Fα is considered to select these local features.
In practice  one will often wish to constrain that some of the functions fα are the same.
This is done by taking the sum in Eq.
13 not just over all data k  but also over all fac-
tors α that should be so constrained. For example  it is common to model image segmen-

are functions mapping local features to local energies. In this case  u would be selected to max-

tation problems using a 4-connected grid with an energy like F (x  y) = !i u(φi  yi) +
!ij v(φij  yi  yj)  where φi/φij are univariate/pairwise features determined by x  and u and v
i (yi))*  and analogous expres-
imize!k!i’(u(φk

sion exists for v. This is the framework used in the following experiments.

i )) − log!yi

exp(u(φk

i   yi) + bk

i (yk

i   yk

i ) + bk

8 Experiments

These experiments consider three different function classes:
linear  boosted decision trees  and
multi-layer perceptrons. To maximize Eq. 11 under linear functions f (x  y) = (W x)y  we sim-
ply compute the gradient with respect to W and use batch L-BFGS. For a multi-layer perceptron 
we ﬁt the function f (x  y) = (W σ(U x))y using stochastic gradient descent with momentum2 on
mini-batches of size 1000  using a step size of .25 for univariate classiﬁers and .05 for pairwise.
Boosted decision trees use stochastic gradient boosting [7]: the gradient of the logistic loss is com-
puted for each exemplar  and a regression tree is induced to ﬁt this (one tree for each class). To
control overﬁtting  each leaf node must contain at least 5% of the data. Then  an optimization ad-
justs the values of leaf nodes to optimize the logistic loss. Finally  the tree values are multiplied by

2At each time  the new step is a combination of .1 times the new gradient plus .9 times the old step.

6

400

200

i

f

0

−200

 

−400
0

100

50

y

y

=1
i
=2
i

0.2

0.4

φ
i

0.6

0.8

y

y

y

y

=(1 1)
ij
=(1 2)
ij
=(2 1)
ij
=(2 2)
ij

 

400

200

i

f

0

1

 

−200

 

−400
0

100

50

 

y
=1
i
y
=2
i

400

200

i

f

0

 

y
=1
i
y
=2
i

1

 

−200

 

−400
0

100

50

0.2

0.4

φ
i

0.6

0.8

y
=(1 1)
ij
y
=(1 2)
ij
y
=(2 1)
ij
y
=(2 2)
ij

1

 

0.2

0.4

φ
i

0.6

0.8

y
=(1 1)
ij
y
=(1 2)
ij
y
=(2 1)
ij
y
=(2 2)
ij

j
i

f

0

j
i

f

0

j
i

f

0

−50

 

−100
0

−50

 

−100
0

0.6

0.8

1

0.2

0.4

φ
ij

Linear

0.2

0.4

φ
ij

0.6

0.8

1

Boosting

−50

 

−100
0

0.6

0.8

1

0.2

0.4

φ
ij

MLP

Figure 1: The univariate (top) and pairwise (bottom) energy functions learned on denoising data.
Each column shows the result of training both univariate and pairwise terms with one function class.

Linear

Denoising

Boosting

MLP Fi \ Fij

Linear

Horses
Boosting

MLP Fi \ Fij

0.2

r
o
r
r

E

0.1

0
0.2

r
o
r
r

E

0.1

0
0.2

r
o
r
r

E

0.1

0
0

r
a
e
n
i
L

g
n
i
t
s
o
o
B

0.2

r
o
r
r

E

0.1

0
0.2

r
o
r
r

E

0.1

0
0.2

P
L
M

r
o
r
r

E

0.1

0
0

r
a
e
n
i
L

g
n
i
t
s
o
o
B

P
L
M

10
Iteration

20

0

10
Iteration

20

0

10
Iteration

20

20
Iteration

40

0

20
Iteration

40

0

20
Iteration

40

Figure 2: Dashed/Solid lines show univariate train/test error rates as a function of learning iterations
for varying univariate (rows) and pairwise (columns) classiﬁers.

Input

True

Denoising

Linear Boosting MLP

Input

True

Horses
Linear Boosting MLP

Figure 3: Example Predictions on Test Images (More in Appendix)

7

i = yk

j   then φk

!= yk

i = 1  φk

ij is sampled from [0  .8] while if yk
i

.25 and added to the ensemble. For reference  we also consider the “zero” classiﬁer  and a “constant”
classiﬁer that ignores the input– equivalent to a linear classiﬁer with a single constant feature.
All examples use  = 0.1. Each learning iteration consists of updating fi  performing 25 iterations
of message passing  updating fij   and then performing another 25 iterations of message-passing.
The ﬁrst dataset is a synthetic binary denoising dataset  intended for the purpose of visualization. To
create an example  an image is generated with each pixel random in [0  1]. To generate y  this image
is convolved with a Gaussian with standard deviation 10 and rounded to {0  1}. Next  if yi = 0  φk
i
is sampled uniformly from [0  .9]  while if yk
i is sampled from [.1  1]. Finally  for a pair
(i  j)  if yk
j φij is sampled from [.2  1]. A
constant feature is also added to both φk
There are 16 100 × 100 images each training and testing. Test errors for each classiﬁer combination
are in Table 1  learning curves are in Fig. 2  and example results in Fig. 3. The nonlinear classiﬁers
result in both lower asymptotic training and testing errors and faster convergence rates. Boosting
converges particularly quickly. Finally  because there is only a single input feature for univariate
and pairwise terms  the resulting functions are plotted in Fig. 1.
Second  as a more realistic example  we use the Weizmann horses dataset. We use 42 univariate
features f k
i consisting of a constant (1) the RBG values of the pixel (3)  the vertical and horizontal
position (2) and a histogram of gradients [2] (36). There are three edge features  consisting of a
constant  the l2 distance of the RBG vectors for the two pixels  and the output of a Sobel edge ﬁlter.
Results are show in Table 1 and Figures 2 and 3. Again  we see beneﬁts in using nonlinear classiﬁers 
both in convergence rate and asymptotic error.

i and φk
ij.

9 Discussion

This paper observes that in the structured learning setting  the optimization with respect to energy
can be formulated as a logistic regression problem for each factor  “biased” by the current messages.
Thus  it is possible to use any function class where an “oracle” exists to optimize a logistic loss.
Besides the possibility of using more general classes of energies  another advantage of the proposed
method is the “software engineering” beneﬁt of having the algorithm for ﬁtting the energy modular-
ized from the rest of the learning procedure. The ability to easily deﬁne new energy functions for
individual problems could have practical impact.
Future work could consider convergence rates of the overall learning optimization  systematically
investigate the choice of   or consider more general entropy approximations  such as the Bethe
approximation used with loopy belief propagation.
In related work  Hazan and Urtasun [9] use a linear energy  and alternate between updating all in-
ference variables and a gradient descent update to parameters  using an entropy-smoothed inference
objective. Meshi et al. [16] also use a linear energy  with a stochastic algorithm updating inference
variables and taking a stochastic gradient step on parameters for one exemplar at a time  with a pure
LP-relaxation of inference. The proposed method iterates between updating all inference variables
and performing a full optimization of the energy. This is a “batch” algorithm in the sense of mak-
ing repeated passes over the data  and so is expected to be slower than an online method for large
datasets. In practice  however  inference is easily parallelized over the data  and the majority of
computational time is spent in the logistic regression subproblems. A stochastic solver can easily be
used for these  as was done for MLPs above  giving a partially stochastic learning method.
Another related work is Gradient Tree Boosting [4] in which to train a CRF  the functional gradient
of the conditional likelihood is computed  and a regression tree is induced. This is iterated to produce
an ensemble. The main limitation is the assumption that inference can be solved exactly. It appears
possible to extend this to inexact inference  where the tree is induced to improve a dual bound  but
this has not been done so far. Experimentally  however  simply inducing a tree on the loss gradient
leads to much slower learning if the leaf nodes are not modiﬁed to optimize the logistic loss. Thus 
it is likely that such a strategy would still beneﬁt from using the logistic regression reformulation.

8

References

[1] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[2] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR  2005.
[3] Chaitanya Desai  Deva Ramanan  and Charless C. Fowlkes. Discriminative models for multi-class object

layout. International Journal of Computer Vision  95(1):1–12  2011.

[4] Thomas G. Dietterich  Adam Ashenfelter  and Yaroslav Bulatov. Training conditional random ﬁelds via

gradient tree boosting. In ICML  2004.

[5] Justin Domke. Learning graphical model parameters with approximate marginal inference. PAMI 

35(10):2454–2467  2013.

[6] Thomas Finley and Thorsten Joachims. Training structural svms when exact inference is intractable. In

ICML  2008.

[7] Jerome H. Friedman. Stochastic gradient boosting. Computational Statistics and Data Analysis  38:367–

378  1999.

[8] Stephen Gould  Jim Rodgers  David Cohen  Gal Elidan  and Daphne Koller. Multi-class segmentation

with relative location prior. IJCV  80(3):300–316  2008.

[9] Tamir Hazan and Raquel Urtasun. Efﬁcient learning of structured predictors in general graphical models.

CoRR  abs/1210.2346  2012.

[10] Xuming He  Richard S. Zemel  and Miguel Á. Carreira-Perpiñán. Multiscale conditional random ﬁelds

for image labeling. In CVPR  2004.

[11] Tom Heskes. Convexity arguments for efﬁcient minimization of the bethe and kikuchi free energies. J.

Artif. Intell. Res. (JAIR)  26:153–190  2006.

[12] Sanjiv Kumar and Martial Hebert. Discriminative ﬁelds for modeling spatial dependencies in natural

images. In NIPS  2003.

[13] Lubor Ladicky  Christopher Russell  Pushmeet Kohli  and Philip H. S. Torr. Associative hierarchical

CRFs for object class image segmentation. In ICCV  2009.

[14] André F. T. Martins  Noah A. Smith  and Eric P. Xing. Polyhedral outer approximations with application

to natural language parsing. In ICML  2009.

[15] Ofer Meshi  Tommi Jaakkola  and Amir Globerson. Convergence rate analysis of MAP coordinate mini-

mization algorithms. In NIPS. 2012.

[16] Ofer Meshi  David Sontag  Tommi Jaakkola  and Amir Globerson. Learning efﬁciently with approximate

inference via dual losses. In ICML  2010.

[17] Sebastian Nowozin  Peter V. Gehler  and Christoph H. Lampert. On parameter learning in CRF-based

approaches to object class image segmentation. In ECCV  2010.

[18] Sebastian Nowozin  Carsten Rother  Shai Bagon  Toby Sharp  Bangpeng Yao  and Pushmeet Kohli. De-

cision tree ﬁelds. In ICCV  2011.

[19] Florian Schroff  Antonio Criminisi  and Andrew Zisserman. Object class segmentation using random

forests. In BMVC  2008.

[20] Jamie Shotton  John M. Winn  Carsten Rother  and Antonio Criminisi. Textonboost for image understand-
ing: Multi-class object recognition and segmentation by jointly modeling texture  layout  and context.
IJCV  81(1):2–23  2009.

[21] Nathan Silberman and Rob Fergus. Indoor scene segmentation using a structured light sensor. In ICCV

Workshops  2011.

[22] Benjamin Taskar  Carlos Guestrin  and Daphne Koller. Max-margin markov networks. In NIPS  2003.
[23] Jakob J. Verbeek and Bill Triggs. Scene segmentation with crfs learned from partially labeled images. In

NIPS  2007.

[24] John M. Winn and Jamie Shotton. The layout consistent random ﬁeld for recognizing and segmenting

partially occluded objects. In CVPR  2006.

[25] Jianxiong Xiao and Long Quan. Multiple view semantic segmentation for street view images. In ICCV 

2009.

9

,Justin Domke