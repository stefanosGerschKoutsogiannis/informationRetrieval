2012,Multi-Stage Multi-Task Feature Learning,Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem  which is usually suboptimal  due to its looseness for approximating an $\ell_0$-type regularizer. In this paper  we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem  we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover  we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.,Multi-Stage Multi-Task Feature Learning∗

yPinghua Gong 

zJieping Ye 

yChangshui Zhang

yState Key Laboratory on Intelligent Technology and Systems

Tsinghua National Laboratory for Information Science and Technology (TNList)

Department of Automation  Tsinghua University  Beijing 100084  China

zComputer Science and Engineering  Center for Evolutionary Medicine and Informatics

The Biodesign Institute  Arizona State University  Tempe  AZ 85287  USA

y{gph08@mails  zcs@mail}.tsinghua.edu.cn 

zjieping.ye@asu.edu

Abstract

Multi-task sparse feature learning aims to improve the generalization performance
by exploiting the shared features among tasks. It has been successfully applied to
many applications including computer vision and biomedical informatics. Most
of the existing multi-task sparse feature learning algorithms are formulated as
a convex sparse regularization problem  which is usually suboptimal  due to its
looseness for approximating an ℓ0-type regularizer. In this paper  we propose a
non-convex formulation for multi-task sparse feature learning based on a novel
regularizer. To solve the non-convex optimization problem  we propose a Multi-
Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover  we present
a detailed theoretical analysis showing that MSMTFL achieves a better parameter
estimation error bound than the convex formulation. Empirical studies on both
synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in
comparison with the state of the art multi-task sparse feature learning algorithms.

1 Introduction

Multi-task learning (MTL) exploits the relationships among multiple related tasks to improve the
generalization performance. It has been applied successfully to many applications such as speech
classiﬁcation [16]  handwritten character recognition [14  17] and medical diagnosis [2]. One com-
mon assumption in multi-task learning is that all tasks should share some common structures in-
cluding the prior or parameters of Bayesian models [18  21  24]  a similarity metric matrix [16]  a
classiﬁcation weight vector [6]  a low rank subspace [4  13] and a common set of shared features
[1  8  10  11  12  14  20].
In this paper  we focus on multi-task feature learning  in which we learn the features speciﬁc to
each task as well as the common features shared among tasks. Although many multi-task feature
learning algorithms have been proposed  most of them assume that the relevant features are shared
by all tasks. This is too restrictive in real-world applications [9]. To overcome this limitation  Jalali
et al. (2010) [9] proposed an ℓ1 + ℓ1 1 regularized formulation  called dirty model  to leverage
the common features shared among tasks. The dirty model allows a certain feature to be shared
by some tasks but not all tasks. Jalali et al. (2010) also presented a theoretical analysis under the
incoherence condition [5  15] which is more restrictive than RIP [3  27]. The ℓ1 + ℓ1 1 regularizer
is a convex relaxation for the ℓ0-type one  which  however  is too loose to well approximate the
ℓ0-type regularizer and usually achieves suboptimal performance (requiring restrictive conditions or
obtaining a suboptimal error bound) [23  26  27]. To remedy the shortcoming  we propose to use a
non-convex regularizer for multi-task feature learning in this paper.

(cid:3)

This work was completed when the ﬁrst author visited Arizona State University.

1

Contributions: We propose to employ a capped-ℓ1 ℓ1 regularized formulation (non-convex) to
learn the features speciﬁc to each task as well as the common features shared among tasks. To
solve the non-convex optimization problem  we propose a Multi-Stage Multi-Task Feature Learning
(MSMTFL) algorithm  using the concave duality [26]. Although the MSMTFL algorithm may not
obtain a globally optimal solution  we theoretically show that this solution achieves good perfor-
mance. Speciﬁcally  we present a detailed theoretical analysis on the parameter estimation error
bound for the MSMTFL algorithm. Our analysis shows that  under the sparse eigenvalue condition
which is weaker than the incoherence condition in Jalali et al. (2010) [9]  MSMTFL improves the
error bound during the multi-stage iteration  i.e.  the error bound at the current iteration improves
the one at the last iteration. Empirical studies on both synthetic and real-world data sets demonstrate
the effectiveness of the MSMTFL algorithm in comparison with the state of the art algorithms.
Notations: Scalars and vectors are denoted by lower case letters and bold face lower case let-
ters  respectively. Matrices and sets are denoted by capital letters and calligraphic capital let-
ters  respectively. The ℓ1 norm  Euclidean norm  ℓ1 norm and Frobenius norm are denoted by
∥ · ∥1  ∥ · ∥  ∥ · ∥1 and ∥ · ∥F   respectively.
| · | denotes the absolute value of a scalar or the
number of elements in a set  depending on the context. We deﬁne the ℓp q norm of a matrix X as
∥X∥p q =
. We deﬁne Nn as {1 ···   n} and N (µ  σ2) as a normal dis-
tribution with mean µ and variance σ2. For a d×m matrix W and sets Ii ⊆ Nd×{i} I ⊆ Nd×Nd 
we let wIi be a d × 1 vector with the j-th entry being wji  if (j  i) ∈ Ii  and 0  otherwise. We also
let WI be a d × m matrix with the (j  i)-th entry being wji  if (j  i) ∈ I  and 0  otherwise.

)p)1/p

|xij|q)1/q

(∑

∑

(

(

i

j

2 The Proposed Formulation
Assume we are given m learning tasks associated with training data {(X1  y1) ···   (Xm  ym)} 
where Xi ∈ Rni(cid:2)d is the data matrix of the i-th task with each row as a sample; yi ∈ Rni is
the response of the i-th task; d is the data dimensionality; ni is the number of samples for the i-th
task. We consider learning a weight matrix W = [w1 ···   wm] ∈ Rd(cid:2)m consisting of the weight
vectors for m linear predictive models: yi ≈ fi(Xi) = Xiwi  i ∈ Nm. In this paper  we propose a
non-convex multi-task feature learning formulation to learn these m models simultaneously  based
on the capped-ℓ1 ℓ1 regularization. Speciﬁcally  we ﬁrst impose the ℓ1 penalty on each row of W  
obtaining a column vector. Then  we impose the capped-ℓ1 penalty [26  27] on that vector. Formally 
we formulate our proposed model as follows:

l(W ) + λ

d∑

min
W

min

j=1

)  

(∥wj∥1  θ
∑

(1)

where l(W ) is an empirical loss function of W ; λ (> 0) is a parameter balancing the empirical loss
and the regularization; θ (> 0) is a thresholding parameter; wj is the j-th row of the matrix W . In
this paper  we focus on the quadratic loss function: l(W ) =

∥Xiwi − yi∥2.

1

m
i=1

mni

Algorithm 1: MSMTFL: Multi-Stage Multi-Task Feature Learning

1 Initialize λ(0)
j = λ;
2 for ℓ = 1  2 ··· do
3

Let ˆW (ℓ) be a solution of the following problem:

l(W ) +

d∑

j=1

 .

λ(ℓ(cid:0)1)

j

∥wj∥1

(2)

min

W2Rd(cid:2)m

j = λI(∥( ˆw(ℓ))j∥1 < θ) (j = 1 ···   d)  where ( ˆw(ℓ))j is the j-th row of ˆW (ℓ) and I(·)

Let λ(ℓ)
denotes the {0  1} valued indicator function.

4 end

Intuitively  due to the capped-ℓ1  ℓ1 penalty  the optimal solution of Eq. (1) denoted as W ⋆ has many
zero rows. For a nonzero row (w⋆)k  some entries may be zero  due to the ℓ1-norm imposed on each

2

row of W . Thus  under the formulation in Eq. (1)  a certain feature can be shared by some tasks
but not all the tasks. Therefore  the proposed formulation can leverage the common features shared
among tasks.
The formulation in Eq. (1) is non-convex and is difﬁcult to solve. To this end  we propose a Multi-
Stage Multi-Task Feature Learning (MSMTFL) algorithm (see Algorithm 1). Note that if we termi-
nate the algorithm with ℓ = 1  the MSMTFL algorithm is equivalent to the ℓ1 regularized multi-task
feature learning algorithm (Lasso). Thus  the solution obtained by MSMTFL can be considered
as a reﬁnement of that of Lasso. Although Algorithm 1 may not ﬁnd a globally optimal solution 
the solution has good performance. Speciﬁcally  we will theoretically show that the solution ob-
tained by Algorithm 1 improves the performance of the parameter estimation error bound during
the multi-stage iteration. Moreover  empirical studies also demonstrate the effectiveness of our pro-
posed MSMTFL algorithm. We provide more details about intuitive interpretations  convergence
analysis and reproducibility discussions of the proposed algorithm in the full version [7].

3 Theoretical Analysis

(

In this section  we theoretically analyze the parameter estimation performance of the solution ob-
tained by the MSMTFL algorithm. To simplify the notations in the theoretical analysis  we assume
that the number of samples for all the tasks are the same. However  our theoretical analysis can be
easily extended to the case where the tasks have different sample sizes.
We ﬁrst present a sub-Gaussian noise assumption which is very common in the analysis of sparse
regularization literature [23  25  26  27].
Assumption 1 Let ¯W = [ ¯w1 ···   ¯wm] ∈ Rd(cid:2)m be the underlying sparse weight matrix and
yi = Xi ¯wi + δi  Eyi = Xi ¯wi  where δi ∈ Rn is a random vector with all entries δji (j ∈ Nn  i ∈
Nm) being independent sub-Gaussians: there exists σ > 0 such that ∀j ∈ Nn  i ∈ Nm  t ∈ R:
Eδji exp(tδji) ≤ exp
)
Remark 1 We call the random variable satisfying the condition in Assumption 1 sub-Gaussian 
∫ 1
since its moment generating function is upper bounded by that of the zero mean Gaussian ran-
dom variable. That is  if a normal random variable x ∼ N (0  σ2)  then we have E exp(tx) =
− x2
(cid:0)1 exp(tx)
dx =
exp(σ2t2/2) ≥ Eδji exp(tδji).
Remark 2 Based on the Hoeffding’s Lemma  for any random variable x ∈ [a  b] and Ex = 0  we
have E(exp(tx)) ≤ exp
. Therefore  both zero mean Gaussian and zero mean bounded
random variables are sub-Guassians. Thus  the sub-Gaussian noise assumption is more general
than the Gaussian noise assumption which is commonly used in the literature [9  11].

(−(x − σ2t)2/(2σ2)

(
(

dx = exp(σ2t2/2)

∫ 1

σ2t2/2

.

(cid:0)1 1p

2πσ

t2(b(cid:0)a)2

)

1p

2πσ

exp

)

exp

)

8

2σ2

We next introduce the following sparse eigenvalue concept which is also common in the analysis of
sparse regularization literature [22  23  25  26  27].
Deﬁnition 1 Given 1 ≤ k ≤ d  we deﬁne

{∥Xiw∥2
}
{∥Xiw∥2
}
n∥w∥2 : ∥w∥0 ≤ k
n∥w∥2 : ∥w∥0 ≤ k

ρ+
i (k) = sup
w

(cid:0)
ρ
i (k) = inf
w

  ρ+

max(k) = max
i2Nm

ρ+
i (k) 

  ρ

(cid:0)
min(k) = min
i2Nm

(cid:0)
ρ
i (k).

(cid:0)
Remark 3 ρ+
i (k)) is in fact the maximum (minimum) eigenvalue of (Xi)TS (Xi)S /n  where
S is a set satisfying |S| ≤ k and (Xi)S is a submatrix composed of the columns of Xi indexed by
S. In the MTL setting  we need to exploit the relations of ρ+

(cid:0)
i (k)) among multiple tasks.

i (k) (ρ

i (k) (ρ

We present our parameter estimation error bound on MSMTFL in the following theorem:

3

Theorem 1 Let Assumption 1 hold. Deﬁne ¯Fi = {(j  i) : ¯wji ̸= 0} and ¯F = ∪i2Nm
as the number of nonzero rows of ¯W . We assume that

¯Fi. Denote ¯r

where s is some integer satisfying s ≥ ¯r. If we choose λ and θ such that for some s ≥ ¯r:

ρ+
i (s)

∀(j  i) ∈ ¯F ∥ ¯wj∥1 ≥ 2θ
√

(cid:0)
ρ
i (2¯r + 2s)

≤ 1 +

s
2¯r

 

and

2ρ+

max(1) ln(2dm/η)

 

λ ≥ 12σ
θ ≥

n

 

√

11mλ

(cid:0)
min(2¯r + s)
ρ
√

(3)

(4)

(5)

(6)

(7)

then the following parameter estimation error bound holds with probability larger than 1 − η:
ρ+
max(¯r)(7.4¯r + 2.7 ln(2/η))/n

39.5mσ

∥ ˆW (ℓ) − ¯W∥2 1 ≤ 0.8ℓ/2 9.1mλ

¯r

(cid:0)
ρ
min(2¯r + s)

+

(cid:0)
ρ
min(2¯r + s)

 

where ˆW (ℓ) is a solution of Eq. (2).
Remark 4 Eq. (3) assumes that the ℓ1-norm of each nonzero row of ¯W is away from zero. This
requires the true nonzero coefﬁcients should be large enough  in order to distinguish them from
the noise. Eq. (4) is called the sparse eigenvalue condition [27]  which requires the eigenvalue ratio
(cid:0)
ρ+
i (s) to grow sub-linearly with respect to s. Such a condition is very common in the analysis
i (s)/ρ
of sparse regularization [22  25] and it is slightly weaker than the RIP condition [3  27].

(

√

)

)

(√

Remark 5 When ℓ = 1 (corresponds to Lasso)  the ﬁrst term of the right-hand side of Eq. (7)
dominates the error bound in the order of

(8)
since λ satisﬁes the condition in Eq. (5). Note that the ﬁrst term of the right-hand side of Eq. (7)
shrinks exponentially as ℓ increases. When ℓ is sufﬁciently large in the order of O(ln(m
¯r/n) +
ln ln(dm))  this term tends to zero and we obtain the following parameter estimation error bound:
(9)

∥ ˆW Lasso − ¯W∥2 1 = O
(

∥ ˆW (ℓ) − ¯W∥2 1 = O

¯r/n + ln(1/η)/n

¯r ln(dm/η)/n

√

√

)

m

m

.

 

m

)

(

√

¯r ln(dm/η)/n

Jalali et al. (2010) [9] gave an ℓ1 1-norm error bound ∥ ˆW Dirty− ¯W∥1 1 = O
ln(dm/η)/n
as well as a sign consistency result between ˆW and ¯W . A direct comparison between these two
bounds is difﬁcult due to the use of different norms. On the other hand  the worst-case estimate of
the ℓ2 1-norm error bound of the algorithm in Jalali et al. (2010) [9] is in the same order with Eq. (8) 
that is: ∥ ˆW Dirty − ¯W∥2 1 = O
. When dm is large and the ground truth has
a large number of sparse rows (i.e.  ¯r is a small constant)  the bound in Eq. (9) is signiﬁcantly better
than the ones for the Lasso and Dirty model.
Remark 6 Jalali et al. (2010) [9] presented an ℓ1 1-norm parameter estimation error bound and
hence a sign consistency result can be obtained. The results are derived under the incoherence
condition which is more restrictive than the RIP condition and hence more restrictive than the sparse
eigenvalue condition in Eq. (4). From the viewpoint of the parameter estimation error  our proposed
algorithm can achieve a better bound under weaker conditions. Please refer to [19  25  27] for
more details about the incoherence condition  the RIP condition  the sparse eigenvalue condition
and their relationships.
Remark 7 The capped-ℓ1 regularized formulation in Zhang (2010) [26] is a special case of our for-
mulation when m = 1. However  extending the analysis from the single task to the multi-task setting
is nontrivial. Different from previous work on multi-stage sparse learning which focuses on a single
task [26  27]  we study a more general multi-stage framework in the multi-task setting. We need to
(cid:0)
exploit the relationship among tasks  by using the relations of sparse eigenvalues ρ+
i (k))
and treating the ℓ1-norm on each row of the weight matrix as a whole for consideration. Moreover 
we simultaneously exploit the relations of each column and each row of the matrix.

i (k) (ρ

4

4 Proof Sketch

We ﬁrst provide several important lemmas (please refer to the full version [7] or supplementary
materials for detailed proofs) and then complete the proof of Theorem 1 based on these lemmas.
Lemma 1 Let ¯Υ = [¯ϵ1 ···   ¯ϵm] with ¯ϵi = [¯ϵ1i ···   ¯ϵdi]T = 1
i (Xi ¯wi − yi) (i ∈ Nm). Deﬁne
¯H ⊇ ¯F such that (j  i) ∈ ¯H (∀i ∈ Nm)  provided there exists (j  g) ∈ ¯F ( ¯H is a set consisting of
the indices of all entries in the nonzero rows of ¯W ). Under the conditions of Assumption 1 and the
notations of Theorem 1  the followings hold with probability larger than 1 − η:

n X T

√

∥ ¯Υ∥1 1 ≤ σ
∥ ¯Υ (cid:22)H∥2

F

≤ mσ2ρ+

2ρ+

max(1) ln(2dm/η)

 

n

max(¯r)(7.4¯r + 2.7 ln(2/η))/n.

(10)

(11)

Lemma 1 gives bounds on the residual correlation ( ¯Υ) with respect to ¯W . We note that Eq. (10) and
Eq. (11) are closely related to the assumption on λ in Eq. (5) and the second term of the right-hand
side of Eq. (7) (error bound)  respectively. This lemma provides a fundamental basis for the proof
of Theorem 1.
Lemma 2 Use the notations of Lemma 1 and consider Gi ⊆ Nd × {i} such that ¯Fi ∩ Gi = ∅ (i ∈
Nm). Let ˆW = ˆW (ℓ) be a solution of Eq. (2) and ∆ ˆW = ˆW − ¯W . Denote ˆλi = ˆλ(ℓ(cid:0)1)
[λ(ℓ(cid:0)1)
maxi

 ···   λ(ℓ(cid:0)1)
ˆλ0i. If 2∥¯ϵi∥1 < ˆλGi  then the following inequality holds at any stage ℓ ≥ 1:

]T . Let ˆλGi = min(j i)2Gi

ˆλGi and ˆλ0i = maxj

ˆλji  ˆλG = mini2Gi

=
ˆλji  ˆλ0 =

d

1

i

m∑

∑

i=1

(j i)2Gi

| ˆw(ℓ)

ji

| ≤ 2∥ ¯Υ∥1 1 + ˆλ0
ˆλG − 2∥ ¯Υ∥1 1

m∑

∑

i=1

(j i)2Gc

i

|∆ ˆw(ℓ)

ji

|.

Gi  ¯F = ∪i2Nm

Denote G = ∪i2Nm
¯Fi and notice that ¯F ∩ G = ∅ ⇒ ∆ ˆW (ℓ) = ˆW (ℓ). Lemma 2
says that ∥∆ ˆW (ℓ)G ∥1 1 = ∥ ˆW (ℓ)G ∥1 1 is upper bounded in terms of ∥∆ ˆW (ℓ)Gc ∥1 1  which indicates that
the error of the estimated coefﬁcients locating outside of ¯F should be small enough. This provides
an intuitive explanation why the parameter estimation error of our algorithm can be small.
Lemma 3 Using the notations of Lemma 2  we denote G = G(ℓ) = ¯Hc ∩ {(j  i) : ˆλ(ℓ(cid:0)1)
∪i2Nm
largest s coefﬁcients (in absolute value) of ˆwGi  Ii = Gc
)
(
Then  the following inequalities hold at any stage ℓ ≥ 1:
4∥ ¯ΥGc
(cid:0)
min(2¯r + s)

= λ} =
Gi with ¯H being deﬁned as in Lemma 1 and Gi ⊆ Nd × {i}. Let Ji be the indices of the
¯Fi.

∑
(j i)2 (cid:22)F (ˆλ(ℓ(cid:0)1)

Ii and ¯F = ∪i2Nm

∪ Ji  I = ∪i2Nm

)√

∥2
F +

√

1 + 1.5

(

(12)

8m

2(cid:22)r
s

)2

ρ

ji

ji

(ℓ)

 

i

∥∆ ˆW (ℓ)∥2 1 ≤
∥∆ ˆW (ℓ)∥2 1 ≤ 9.1mλ

√

¯r

(cid:0)
ρ
min(2¯r + s)

.

(13)

Lemma 3 is established based on Lemma 2  by considering the relationship between Eq. (5) and
Eq. (10)  and the speciﬁc deﬁnition of G = G(ℓ). Eq. (12) provides a parameter estimation error
bound in terms of ℓ2 1-norm by ∥ ¯ΥGc
(see the deﬁnition
of ˆλji (ˆλ(ℓ(cid:0)1)
) in Lemma 2). This is the result directly used in the proof of Theorem 1. Eq. (13)
states that the error bound is upper bounded in terms of λ  the right-hand side of which constitutes
the shrinkage part of the error bound in Eq. (7).

∥2
F and the regularization parameters ˆλ(ℓ(cid:0)1)

ji

ji

(ℓ)

Lemma 4 Let ˆλji = λI
in Lemma 1. Then under the condition of Eq. (3)  we have:

 ∀i ∈ Nm with some ˆW ∈ Rd(cid:2)m. ¯H ⊇ ¯F is deﬁned
≤ mλ2∥ ¯W (cid:22)H − ˆW (cid:22)H∥2

2 1/θ2.

ˆλ2
ji

(∥ ˆwj∥1 < θ  j ∈ Nd
∑
∑

)

≤

ˆλ2
ji

(j i)2 (cid:22)F

(j i)2 (cid:22)H

5

(ℓ)

(ℓ)

(ℓ)

Gc

\ ¯H| ≤ mθ

(cid:0)2∥ ˆW (ℓ(cid:0)1)

where the last inequality follows from ∀(j  i) ∈ Gc
√
)2(
n (cid:22)H − ¯WGc
¯wj∥2
)
(cid:13)(cid:13)(cid:13)2

(
(cid:13)(cid:13)(cid:13) ˆW (ℓ(cid:0)1) − ¯W

2 1 = ∥∆ ˆW (ℓ)∥2

4u + (37/36)mλ2θ

1/θ2 ≥ 1 ⇒ |Gc
(
∥ ˆW (ℓ) − ¯W∥2
(cid:13)(cid:13)(cid:13) ˆW (0) − ¯W

≤ 78m

(cid:0)
min(2¯r + s))2

≤ 0.8ℓ

≤ 8m

(cid:13)(cid:13)(cid:13)2

1 + 1.5

312mu

(cid:0)2

≤

2(cid:22)r
s

(ρ

+

2 1

2 1

(ℓ)

1 − 0.8ℓ
1 − 0.8

(ℓ)

(ℓ)

(ℓ)

Gc

n (cid:22)H∥2
2 1 

(15)
1/θ2 = ∥( ˆw(ℓ(cid:0)1))j −

\ ¯H|∥ ¯Υ∥21 1
(cid:0)2∥ ˆW (ℓ(cid:0)1)
n (cid:22)H − ¯WGc
\ ¯H ∥( ˆw(ℓ(cid:0)1))j∥2
∑
n (cid:22)H∥2
2 1. According to Eq. (12)  we have:
∥2
4∥ ¯ΥGc
(j i)2 (cid:22)F (ˆλ(ℓ(cid:0)1)
F +
(cid:0)
min(2¯r + s))2

)
(cid:13)(cid:13)(cid:13) ˆW (ℓ(cid:0)1) − ¯W

312mu

+ 0.8

(ρ

)2

ji

(ℓ)

(cid:13)(cid:13)(cid:13)2

2 1

(cid:0)
min(2¯r + s))2
(ρ
≤ 0.8ℓ
9.12m2λ2¯r
(cid:0)
min(2¯r + s))2

(ρ

1560mu

∑

Lemma 4 establishes an upper bound of
2 1  which is critical for
building the recursive relationship between ∥ ˆW (ℓ) − ¯W∥2 1 and ∥ ˆW (ℓ(cid:0)1) − ¯W∥2 1 in the proof of
Theorem 1. This recursive relation is crucial for the shrinkage part of the error bound in Eq. (7).

(j i)2 (cid:22)F ˆλ2

ji by ∥ ¯W (cid:22)H − ˆW (cid:22)H∥2

4.1 Proof of Theorem 1

Proof For notational simplicity  we denote the right-hand side of Eq. (11) as:

Based on ¯H ⊆ Gc
∥ ¯ΥGc
≤ u + λ2|Gc

(ℓ)

(ℓ)

u = mσ2ρ+

max(¯r)(7.4¯r + 2.7 ln(2/η))/n.

(14)
(ℓ)  Lemma 1 and Eq. (5)  the followings hold with probability larger than 1 − η:
∥2
F = ∥ ¯Υ (cid:22)H∥2

F

n (cid:22)H∥2

F + ∥ ¯ΥGc

≤ u + |Gc
\ ¯H|/144 ≤ u + (1/144)mλ2θ

(ℓ)

2 1

(ρ

(cid:0)
min(2¯r + s))2

(cid:0)
min(2¯r + s))2
In the above derivation  the ﬁrst inequality is due to Eq. (12); the second inequality is due to the
assumption s ≥ ¯r in Theorem 1  Eq. (15) and Lemma 4; the third inequality is due to Eq. (6); the
last inequality follows from Eq. (13) and 1 − 0.8ℓ ≤ 1 (ℓ ≥ 1). Thus  following the inequality
√
a + b ≤ √

a +

√

(ρ

+

.

b (∀a  b ≥ 0)  we obtain:
∥ ˆW (ℓ) − ¯W∥2 1 ≤ 0.8ℓ/2 9.1mλ

√

¯r

(cid:0)
ρ
min(2¯r + s)

+

√
39.5
(cid:0)
ρ
min(2¯r + s)

mu

.

(cid:3)

Substituting Eq. (14) into the above inequality  we verify Theorem 1.

5 Experiments

We compare our proposed MSMTFL algorithm with three competing multi-task feature learning
algorithms: ℓ1-norm multi-task feature learning algorithm (Lasso)  ℓ1 2-norm multi-task feature
learning algorithm (L1 2) [14] and dirty model multi-task feature learning algorithm (DirtyMTL)
[9]. In our experiments  we employ the quadratic loss function for all the compared algorithms.

5.1 Synthetic Data Experiments

We generate synthetic data by setting the number of tasks as m and each task has n samples which
are of dimensionality d; each element of the data matrix Xi ∈ Rn(cid:2)d (i ∈ Nm) for the i-th task is
sampled i.i.d. from the Gaussian distribution N (0  1) and we then normalize all columns to length 1;
each entry of the underlying true weight ¯W ∈ Rd(cid:2)m is sampled i.i.d. from the uniform distribution
in the interval [−10  10]; we randomly set 90% rows of ¯W as zero vectors and 80% elements of
the remaining nonzero entries as zeros; each entry of the noise δi ∈ Rn is sampled i.i.d. from the
Gaussian distribution N (0  σ2); the responses are computed as yi = Xi ¯wi + δi (i ∈ Nm).
We ﬁrst report the averaged parameter estimation error ∥ ˆW− ¯W∥2 1 vs. Stage (ℓ) plots for MSMTFL
(Figure 1). We observe that the error decreases as ℓ increases  which shows the advantage of our pro-
posed algorithm over Lasso. This is consistent with the theoretical result in Theorem 1. Moreover 
the parameter estimation error decreases quickly and converges in a few stages.

6

We then report the averaged parameter estimation error ∥ ˆW − ¯W∥2 1 in comparison with four al-
gorithms in different parameter settings (Figure 2). For a fair comparison  we compare the smallest
estimation errors of the four algorithms in all the parameter settings [25  26]. As expected  the pa-
rameter estimation error of the MSMTFL algorithm is the smallest among the four algorithms. This
empirical result demonstrates the effectiveness of the MSMTFL algorithm. We also have the follow-
ing observations: (a) When λ is large enough  all four algorithms tend to have the same parameter
estimation error. This is reasonable  because the solutions ˆW ’s obtained by the four algorithms are
all zero matrices  when λ is very large. (b) The performance of the MSMTFL algorithm is similar
for different θ’s  when λ exceeds a certain value.

Figure 1: Averaged parameter estimation error ∥ ˆW − ¯W∥2 1 vs. Stage (ℓ) plots for MSMTFL on
the synthetic data set (averaged over 10 runs). Here we set λ = α
ln(dm)/n  θ = 50mλ. Note
that ℓ = 1 corresponds to Lasso; the results show the stage-wise improvement over Lasso.

√

Figure 2: Averaged parameter estimation error ∥ ˆW − ¯W∥2 1 vs. λ plots on the synthetic data set
(averaged over 10 runs). MSMTFL has the smallest parameter estimation error among the four al-
gorithms. Both DirtyMTL and MSMTFL have two parameters; we set λs/λb = 1  0.5  0.2  0.1
for DirtyMTL (1/m ≤ λs/λb ≤ 1 was adopted in Jalali et al. (2010) [9]) and θ/λ =
50m  10m  2m  0.4m for MSMTFL.

5.2 Real-World Data Experiments

We conduct experiments on two real-world data sets: MRI and Isolet data sets. (1) The MRI data
set is collected from the ANDI database  which contains 675 patients’ MRI data preprocessed using
FreeSurfer1. The MRI data include 306 features and the response (target) is the Mini Mental State
Examination (MMSE) score coming from 6 different time points: M06  M12  M18  M24  M36  and
M48. We remove the samples which fail the MRI quality controls and have missing entries. Thus 
we have 6 tasks with each task corresponding to a time point and the sample sizes corresponding to
6 tasks are 648  642  293  569  389 and 87  respectively. (2) The Isolet data set2 is collected from
150 speakers who speak the name of each English letter of the alphabet twice. Thus  there are 52
samples from each speaker. The speakers are grouped into 5 subsets which respectively include 30
similar speakers  and the subsets are named Isolet1  Isolet2  Isolet3  Isolet4  and Isolet5. Thus  we
naturally have 5 tasks with each task corresponding to a subset. The 5 tasks respectively have 1560 
1560  1560  1558  and 1559 samples (Three samples are historically missing)  where each sample
includes 617 features and the response is the English letter label (1-26).
In the experiments  we treat the MMSE and letter labels as the regression values for the MRI data
set and the Isolet data set  respectively. For both data sets  we randomly extract the training samples
from each task with different training ratios (15%  20% and 25%) and use the rest of samples to form
the test set. We evaluate the four multi-task feature learning algorithms in terms of normalized mean
squared error (nMSE) and averaged means squared error (aMSE)  which are commonly used in

1www.loni.ucla.edu/ADNI/
2www.zjucadcg.cn/dengcai/Data/data.html

7

246810020406080100120StageParamter estimation error (L2 1)m=15 n=40 d=250 σ=0.01 α=5e−005α=0.0001α=0.0002α=0.0005246810050100150200StageParamter estimation error (L2 1)m=20 n=30 d=200 σ=0.005 α=5e−005α=0.0001α=0.0002α=0.0005246810020406080100StageParamter estimation error (L2 1)m=10 n=60 d=300 σ=0.001 α=5e−005α=0.0001α=0.0002α=0.000510−5100100101102103λParamter estimation error (L2 1)m=15 n=40 d=250 σ=0.01 10−610−410−2100100101102103λParamter estimation error (L2 1)m=20 n=30 d=200 σ=0.005 10−610−410−210010−1100101102103λParamter estimation error (L2 1)m=10 n=60 d=300 σ=0.001 LassoL1 2DirtyMTL(1λ)DirtyMTL(0.5λ)DirtyMTL(0.2λ)DirtyMTL(0.1λ)MSMTFL(50mλ)MSMTFL(10mλ)MSMTFL(2mλ)MSMTFL(0.4mλ)Table 1: Comparison of four multi-task feature learning algorithms on the MRI data set in terms of
averaged nMSE and aMSE (standard deviation)  which are averaged over 10 random splittings.

measure

traning ratio

nMSE

aMSE

0.15
0.20
0.25
0.15
0.20
0.25

Lasso

0.6651(0.0280)
0.6254(0.0212)
0.6105(0.0186)
0.0189(0.0008)
0.0179(0.0006)
0.0172(0.0009)

L1 2

0.6633(0.0470)
0.6489(0.0275)
0.6577(0.0194)
0.0187(0.0010)
0.0184(0.0005)
0.0183(0.0006)

DirtyMTL

0.6224(0.0265)
0.6140(0.0185)
0.6136(0.0180)
0.0172(0.0006)
0.0171(0.0005)
0.0167(0.0008)

MSMTFL

0.5539(0.0154)
0.5542(0.0139)
0.5507(0.0142)
0.0159(0.0004)
0.0161(0.0004)
0.0157(0.0006)

multi-task learning problems [28  29]. For each training ratio  both nMSE and aMSE are averaged
over 10 random splittings of training and test sets and the standard deviation is also shown. All
parameters of the four algorithms are tuned via 3-fold cross validation.

Figure 3: Averaged test error (nMSE and aMSE) vs. training ratio plots on the Isolet data set. The
results are averaged over 10 random splittings.

Table 1 and Figure 3 show the experimental results in terms of averaged nMSE (aMSE) and the
standard deviation. From these results  we observe that: (a) Our proposed MSMTFL algorithm out-
performs all the competing feature learning algorithms on both data sets  with the smallest regression
errors (nMSE and aMSE) as well as the smallest standard deviations. (b) On the MRI data set  the
MSMTFL algorithm performs well even in the case of a small training ratio. The performance for
the 15% training ratio is comparable to that for the 25% training ratio. (c) On the Isolet data set 
when the training ratio increases from 15% to 25%  the performance of the MSMTFL algorithm
increases and the superiority of the MSMTFL algorithm over the other three algorithms is more
signiﬁcant. Our results demonstrate the effectiveness of the proposed algorithm.

6 Conclusions

In this paper  we propose a non-convex multi-task feature learning formulation based on the capped-
ℓ1 ℓ1 regularization. The proposed formulation learns the speciﬁc features of each task as well as the
common features shared among tasks. We propose to solve the non-convex optimization problem
by employing a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm  using concave
duality. We also present a detailed theoretical analysis in terms of the parameter estimation error
bound for the MSMTFL algorithm. The analysis shows that our MSMTFL algorithm achieves good
performance under the sparse eigenvalue condition  which is weaker than the incoherence condition.
Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our
proposed MSMTFL algorithm in comparison with the state of the art multi-task feature learning
algorithms. In our future work  we will focus on a general non-convex regularization framework for
multi-task feature learning settings (involving different loss functions and non-convex regularization
terms) and derive theoretical bounds.

Acknowledgements

This work is supported in part by 973 Program (2013CB329503)  NSFC (Grant No. 91120301 
60835002 and 61075004)  NIH (R01 LM010730) and NSF (IIS-0953662  CCF-1025177).

8

0.150.20.250.50.550.60.650.7Training RationMSE LassoL1 2DirtyMTLMSMTFL0.150.20.250.120.130.140.150.160.17Training RatioaMSE LassoL1 2DirtyMTLMSMTFLReferences
[1] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning 

73(3):243–272  2008.

[2] J. Bi  T. Xiong  S. Yu  M. Dundar  and R. Rao. An improved multi-task learning approach with applica-
tions in medical diagnosis. Machine Learning and Knowledge Discovery in Databases  pages 117–132 
2008.

[3] E. Candes and T. Tao. Decoding by linear programming.

51(12):4203–4215  2005.

IEEE Transactions on Information Theory 

[4] J. Chen  J. Liu  and J. Ye. Learning incoherent sparse and low-rank patterns from multiple tasks.

SIGKDD  pages 1179–1188  2010.

In

[5] D. Donoho  M. Elad  and V. Temlyakov. Stable recovery of sparse overcomplete representations in the

presence of noise. IEEE Transactions on Information Theory  52(1):6–18  2006.

[6] T. Evgeniou and M. Pontil. Regularized multi–task learning. In SIGKDD  pages 109–117  2004.
[7] P. Gong  J. Ye  and C. Zhang. Multi-stage multi-task feature learning. arXiv:1210.5806  2012.
[8] P. Gong  J. Ye  and C. Zhang. Robust multi-task feature learning. In SIGKDD  pages 895–903  2012.
[9] A. Jalali  P. Ravikumar  S. Sanghavi  and C. Ruan. A dirty model for multi-task learning. In NIPS  pages

964–972  2010.

[10] S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In ICML 

pages 543–550  2009.

[11] K. Lounici  M. Pontil  A. Tsybakov  and S. Van De Geer. Taking advantage of sparsity in multi-task

learning. In COLT  pages 73–82  2009.

[12] S. Negahban and M. Wainwright. Joint support recovery under high-dimensional scaling: Beneﬁts and

perils of ℓ1;1-regularization. In NIPS  pages 1161–1168  2008.

[13] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional

scaling. The Annals of Statistics  39(2):1069–1097  2011.

[14] G. Obozinski  B. Taskar  and M. Jordan. Multi-task feature selection. Statistics Department  UC Berkeley 

Tech. Rep  2006.

[15] G. Obozinski  M. Wainwright  and M. Jordan. Support union recovery in high-dimensional multivariate

regression. Annals of statistics  39(1):1–47  2011.

[16] S. Parameswaran and K. Weinberger. Large margin multi-task metric learning. In NIPS  pages 1867–

1875  2010.

[17] N. Quadrianto  A. Smola  T. Caetano  S. Vishwanathan  and J. Petterson. Multitask learning without label

correspondences. In NIPS  pages 1957–1965  2010.

[18] A. Schwaighofer  V. Tresp  and K. Yu. Learning gaussian process kernels via hierarchical bayes. In NIPS 

pages 1209–1216  2005.

[19] S. Van De Geer and P. B¨uhlmann. On the conditions used to prove oracle results for the lasso. Electronic

Journal of Statistics  3:1360–1392  2009.

[20] X. Yang  S. Kim  and E. Xing. Heterogeneous multitask learning with joint sparsity constraints. In NIPS 

pages 2151–2159  2009.

[21] K. Yu  V. Tresp  and A. Schwaighofer. Learning gaussian processes from multiple tasks. In ICML  pages

1012–1019  2005.

[22] C. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regression.

The Annals of Statistics  36(4):1567–1594  2008.

[23] C. Zhang and T. Zhang. A general theory of concave regularization for high dimensional sparse estimation

problems. Statistical Science  2012.

[24] J. Zhang  Z. Ghahramani  and Y. Yang. Learning multiple related tasks using latent independent compo-

nent analysis. In NIPS  pages 1585–1592  2006.

[25] T. Zhang. Some sharp performance bounds for least squares regression with ℓ1 regularization. The Annals

of Statistics  37:2109–2144  2009.

[26] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. JMLR  11:1081–1107 

2010.

[27] T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli  2012.
[28] Y. Zhang and D. Yeung. Multi-task learning using generalized t process. In AISTATS  2010.
[29] J. Zhou  J. Chen  and J. Ye. Clustered multi-task learning via alternating structure optimization. In NIPS 

pages 702–710  2011.

9

,Odalric-Ambrym Maillard
Timothy Mann
Shie Mannor