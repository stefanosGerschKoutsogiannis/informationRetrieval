2018,A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization,We analyze stochastic gradient algorithms for optimizing nonconvex  nonsmooth finite-sum problems. In particular  the objective function is given by the summation of a differentiable (possibly nonconvex) component  together with a possibly non-differentiable but convex component.
We propose a proximal stochastic gradient algorithm based on variance reduction  called ProxSVRG+.
Our main contribution lies in the analysis of ProxSVRG+.
It recovers several existing convergence results and improves/generalizes them (in terms of the number of stochastic gradient oracle calls and proximal oracle calls).
In particular  ProxSVRG+ generalizes the best results given by the SCSG algorithm  recently proposed by [Lei et al.  NIPS'17] for the smooth nonconvex case.
ProxSVRG+ is also more straightforward than SCSG and yields simpler analysis.
Moreover  ProxSVRG+ outperforms the deterministic proximal gradient descent (ProxGD) for a wide range of minibatch sizes  which partially solves an open problem proposed in [Reddi et al.  NIPS'16].
Also  ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi et al.  NIPS'16].
Moreover  for nonconvex functions satisfied Polyak-\L{}ojasiewicz condition  we prove that ProxSVRG+ achieves a global linear convergence rate without restart unlike ProxSVRG.
Thus  it can \emph{automatically} switch to the faster linear convergence in some regions as long as the objective function satisfies the PL condition locally in these regions.
Finally  we conduct several experiments and the experimental results are consistent with the theoretical results.,A Simple Proximal Stochastic Gradient Method for

Nonsmooth Nonconvex Optimization

Zhize Li

IIIS  Tsinghua University

Jian Li

IIIS  Tsinghua University

zz-li14@mails.tsinghua.edu.cn

lijian83@mail.tsinghua.edu.cn

Abstract

We analyze stochastic gradient algorithms for optimizing nonconvex  nonsmooth
ﬁnite-sum problems. In particular  the objective function is given by the summation
of a differentiable (possibly nonconvex) component  together with a possibly non-
differentiable but convex component. We propose a proximal stochastic gradient
algorithm based on variance reduction  called ProxSVRG+. Our main contribution
lies in the analysis of ProxSVRG+. It recovers several existing convergence results
and improves/generalizes them (in terms of the number of stochastic gradient
oracle calls and proximal oracle calls). In particular  ProxSVRG+ generalizes
the best results given by the SCSG algorithm  recently proposed by [Lei et al. 
2017] for the smooth nonconvex case. ProxSVRG+ is also more straightforward
than SCSG and yields simpler analysis. Moreover  ProxSVRG+ outperforms the
deterministic proximal gradient descent (ProxGD) for a wide range of minibatch
sizes  which partially solves an open problem proposed in [Reddi et al.  2016].
Also  ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi
et al.  2016]. Moreover  for nonconvex functions satisﬁed Polyak-Łojasiewicz
condition  we prove that ProxSVRG+ achieves a global linear convergence rate
without restart unlike ProxSVRG. Thus  it can automatically switch to the faster
linear convergence in some regions as long as the objective function satisﬁes the
PL condition locally in these regions. Finally  we conduct several experiments and
the experimental results are consistent with the theoretical results.

1

Introduction

(cid:80)n

In this paper  we consider nonsmooth nonconvex ﬁnite-sum optimization problems of the form

min

x

Φ(x) := f (x) + h(x) 

(1)

where f (x) := 1
i=1 fi(x) and each fi(x) is possibly nonconvex with a Lipschitz continuous
gradient  while h(x) is nonsmooth but convex (e.g.  l1 norm (cid:107)x(cid:107)1 or indicator function IC(x) for
n
some convex set C). We assume that the proximal operator of h(x) can be computed efﬁciently.
This above optimization problem is fundamental to many machine learning problems  ranging from
convex optimization such as Lasso  SVM to highly nonconvex problem such as optimizing deep
neural networks. There has been extensive research when f (x) is convex (see e.g.  [25  7  15  1]).
In particular  if fis are strongly-convex  Xiao and Zhang [25] proposed the Prox-SVRG algorithm 
which achieves a linear convergence rate  based on the well-known variance reduction technique
SVRG developed in [12]. In recent years  due to the increasing popularity of deep learning  the
nonconvex case has attracted signiﬁcant attention. See e.g.  [9  3  23  17] for results on the smooth
nonconvex case (i.e.  h(x) ≡ 0). Very recently  Zhou et al. [27] proposed an algorithm with stochastic
 ) [3].

gradient complexity (cid:101)O( 1

For the more general nonsmooth nonconvex case  the research is still somewhat limited.

 )  improving the previous results O( 1

5/3 ) [17] and O( n2/3

3/2 ∧ n1/2

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Recently  for the nonsmooth nonconvex case  Reddi et al. [24] provided two algorithms called
ProxSVRG and ProxSAGA  which are based on the well-known variance reduction techniques SVRG
and SAGA [12  7]. Also  we would like to mention that Aravkin and Davis [5] considered the case
when h can be nonconvex in a more general context of robust optimization. Before that  Ghadimi
et al. [10] analyzed the deterministic proximal gradient method (i.e.  computing the full-gradient in
every iteration) for nonconvex nonsmooth problems. Here we denote it as ProxGD. Ghadimi et al.
[10] also considered the stochastic case (here we denote it as ProxSGD). However  ProxSGD requires
the batch sizes being a large number (i.e.  Ω(1/)) or increasing with the iteration number t. Note
that ProxSGD may reduce to deterministic ProxGD after some iterations due to the increasing batch
sizes. Note that from the perspectives of both computational efﬁciency and statistical generalization 
always computing full-gradient (GD or ProxGD) may not be desirable for large-scale machine
learning problems. A reasonable minibatch size is also desirable in practice  since the computation
of minibatch stochastic gradients can be implemented in parallel. In fact  practitioners typically use
moderate minibatch sizes  often ranging from something like 16 or 32 to a few hundreds (sometimes
to a few thousands  see e.g.  [11]).1 Hence  it is important to study the convergence in moderate and
constant minibatch size regime.
Reddi et al. [24] provided the ﬁrst non-asymptotic convergence rates for ProxSVRG with minibatch
size at most O(n2/3)  for the nonsmooth nonconvex problems. However  their convergence bounds
(using constant or moderate size minibatches) are worse than the deterministic ProxGD in terms of the
number of proximal oracle calls. Note that their algorithms (i.e.  ProxSVRG/SAGA) outperform the
ProxGD only if they use quite large minibatch size b = O(n2/3). Note that in a typical application 
the number of training data is n = 106 ∼ 109  and n2/3 = 104 ∼ 106. Hence  O(n2/3) is a quite
large minibatch size. Finally  they presented an important open problem of developing stochastic
methods with provably better performance than ProxGD with constant minibatch size.
Our Contribution: In this paper  we propose a very straightforward algorithm called ProxSVRG+
to solve the nonsmooth nonconvex problem (1). Our main technical contribution lies in the new
convergence analysis of ProxSVRG+  which has notable difference from that of ProxSVRG [24].
We list our results in Table 1–3  and Figure 1–2. Our convergence results are stated in terms of the
number of stochastic ﬁrst-order oracle (SFO) calls and proximal oracle (PO) calls (see Deﬁnition 2).
We would like to highlight the following results yielded by our new analysis:
1) ProxSVRG+ is

bn) times faster than ProxGD in terms of #SFO when b ≤ n2/3 (resp.
b ≤ 1/2/3)  and n/b times faster than ProxGD when b > n2/3 (resp. b > 1/2/3). Note that #PO
= O(1/) for both ProxSVRG+ and ProxGD. Obviously  for any super constant b  ProxSVRG+
is strictly better than ProxGD. Hence  we partially answer the open question (i.e. developing
stochastic methods with provably better performance than ProxGD with constant minibatch size b)
proposed in [24]. Also  ProxSVRG+ matches the best result achieved by ProxSVRG at b = n2/3 
and ProxSVRG+ is strictly better for smaller b (using less PO calls). See Figure 1 for an overview.
2) Assuming that the variance of the stochastic gradient is bounded  i.e. online/stochastic setting 
ProvSVRG+ generalizes the best result achieved by SCSG  recently proposed by Lei et al. [17]
for the smooth nonconvex case  i.e.  h(x) ≡ 0 in form (1) (see Table 1  the 5th row). ProxSVRG+
is more straightforward than SCSG and yields simpler proof. Our results also match the results of
Natasha1.5 proposed by Allen-Zhu [2] very recently  in terms of #SFO  if there is no additional
assumption (see Footnote 2 for details). In terms of #PO  our algorithm outperforms Natasha1.5.
We also note that SCSG [17] and ProxSVRG [24] achieved their best convergence results with
b = 1 and b = n2/3 respectively  while ProxSVRG+ achieves the best result with b = 1/2/3 (see
Figure 1)  which is a moderate minibatch size (which is not too small for parallelism and not too
large for better generalization). In our experiments  the best b for ProxSVRG and ProxSVRG+ in
the MNIST experiments is 4096 and 256  respectively (see the second row of Figure 4).

√

√

b (resp.

3) For the nonconvex functions satisfying Polyak-Łojasiewicz condition [22]  we prove that Prox-
SVRG+ achieves a global linear convergence rate without restart  while Reddi et al. [24] used
PL-SVRG to restart ProxSVRG O(log(1/)) times to obtain the linear convergence rate. More-
over  ProxSVRG+ also improves ProxGD and ProxSVRG/SAGA  and generalizes the results of
SCSG in this case (see Table 3). Also see the remarks after Theorem 2 for more details.
1In fact  some studies argued that smaller minibatch sizes in SGD are very useful for generalization (e.g. 
[14]). Although generalization is not the focus of the present paper  it provides further motivation for studying
the moderate minibatch size regime.

2

Algorithms
ProxGD [10]
(full gradient)
ProxSGD [10]

ProxSVRG/SAGA [24]

SCSG [17]

(smooth nonconvex 
i.e.  h(x) ≡ 0 in (1))

Natasha1.5 [2]

ProxSVRG+
(this paper)

O

O

b

√


O(b/)

O(cid:0) n
+ n(cid:1)
(cid:1)2/3(cid:17)
(cid:16) b1/3
(cid:0)n ∧ 1
O(cid:0) n
(cid:1)
(cid:16)(cid:0)n ∧ 1

O(1/5/3) 2
+ b


(cid:1) 1

√

√



b





+ b






b

(cid:17)

O(1/)

O(cid:0) n

b3/2

(cid:1)

–

σ = O(1) 
b ≥ 1/
b ≤ n2/3

NA

σ = O(1)

O(1/5/3)

σ = O(1)

O(1/)

–

O(1/)

σ = O(1)

Table 1: Comparison of the SFO and PO complexity

Stochastic ﬁrst-order

oracle (SFO)

Proximal oracle Additional
condition

(PO)

O(n/)

O(1/)

The ∧ denotes the minimum and b denotes the minibatch size. The deﬁnitions of SFO
and PO are given in Deﬁnition 2  σ (in the last column) is deﬁned in Assumption 1.

Table 2: Some recommended minibatch sizes b

Algorithm Minibatch

ProxSVRG+

b = 1

b = 1
2/3

b = n2/3

b = n

SFO

O(n/)
O(1/2)
2/3 + 1

O(cid:0) n
O(cid:0) 1
O(cid:0) n2/3

(cid:1)
(cid:1)

5/3



5/3

O(n/)

PO

O(1/)
O(1/)

(cid:1) O(1/)

O(1/)

O(1/)

O(1/)

Cond.

–

σ = O(1)

–

σ = O(1) 
n > 1/

–
–

Notes

Same as ProxGD
Same as ProxSGD
Better than ProxGD 

does not need σ = O(1)
Better than ProxGD and

ProxSVRG/SAGA 

same as SCSG (in SFO)

Same as

ProxSVRG/SAGA
Same as ProxGD

Figure 2: PO complexity in terms of minibatch b

Figure 1: SFO complexity in terms of minibatch b 3

is O( 1

2Natasha 1.5 used an additional parameter  called strongly nonconvex parameter(cid:101)L ((cid:101)L ≤ L) and #SFO in [2]
5/3 ). If(cid:101)L is much smaller than L  the bound is better. Without any additional assumption  the
3/2 + (cid:101)L1/3
default value of(cid:101)L is L. The result listed in the table is the(cid:101)L = L case. Besides  one can verify that #PO of

Natasha1.5 is the same as its #SFO.
3Note that the curve of ProxSGD overlaps with ProxSVRG+ for b ≥ 1/  and the curve of ProxSVRG/SAGA
overlaps with ProxSVRG+ for b ≤ n2/3 in Figure 1. We did not plot Natasha 1.5 since it did not consider the
minibatch case  i.e.  b ≡ 1 in Natasha 1.5.

3

bSFO15/3nn2/3nProxGD1ProxSVRG/SAGA1/1/2/3ProxSVRG+12n2/3SCSG(σ=O(1))(σ=O(1))(b≤n2/3)ProxSVRG+ProxSGD(b≥1/ σ=O(1))bPO1nn2/3nProxSVRG+ProxSGD(b≥1/)ProxGD1ProxSVRG/SAGA(b≤n2/3)2 Preliminaries
We assume that fi(x) in (1) has an L-Lipschitz continuous gradient for all i ∈ [n]  i.e.  there is a
constant L such that
(2)
where (cid:107) · (cid:107) denotes the Eculidean norm (cid:107) · (cid:107)2. Note that fi(x) does not need to be convex. We also
assume that the nonsmooth convex function h(x) in (1) is well structured  i.e.  the following proximal
operator on h can be computed efﬁciently:

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107) 

(cid:16)

(cid:107)y − x(cid:107)2(cid:17)

.

1
2η

h(y) +

proxηh(x) := arg min
y∈Rd

(3)
For convex problems  one typically uses the optimality gap Φ(x)−Φ(x∗) as the convergence criterion
(see e.g.  [21]). But for general nonconvex problems  one typically uses the gradient norm as the
convergence criterion. E.g.  for smooth nonconvex problems (i.e.  h(x) ≡ 0)  Ghadimi and Lan [9] 
Reddi et al. [23] and Lei et al. [17] used (cid:107)∇Φ(x)(cid:107)2 (i.e.  (cid:107)∇f (x)(cid:107)2) to measure the convergence
results. In order to analyze the convergence results for nonsmooth nonconvex problems  we need to
deﬁne the gradient mapping as follows (as in [10  24]):
x − proxηh
(cid:16)
We often use an equivalent but useful form of proxηh

(cid:0)x − η∇f (x)(cid:1)(cid:17)
(cid:0)x − η∇f (x)(cid:1) as follows:

(cid:107)y − x(cid:107)2 + (cid:104)∇f (x)  y(cid:105)(cid:17)

(cid:0)x − η∇f (x)(cid:1) = arg min

Gη(x) :=

(cid:16)

1
η

(4)

(5)

.

.

proxηh

h(y) +

y∈Rd

1
2η

η∇f (x)(cid:1)  then Gη(x) := 1

Note that if h(x) is a constant function (in particular  zero)  this gradient mapping reduces to the
ordinary gradient: Gη(x) = ∇Φ(x) = ∇f (x). In this paper  we use the gradient mapping Gη(x) as
the convergence criterion (same as [10  24]).
Deﬁnition 1 ˆx is called an -accurate solution for problem (1) if E[(cid:107)Gη(ˆx)(cid:107)2] ≤   where ˆx denotes
the point returned by a stochastic algorithm.
Note that the metric Gη(x) has already normalized the step-size η  i.e.  it is independent of different
algorithms. Also it is indeed a convergence metric for Φ(x) = f (x) + h(x). Let x+ := proxηh
η(cid:107)x − x+(cid:107) = (cid:107)∇f (x) + ∂h(x+)(cid:107) ≤   then
(cid:107)∂Φ(x+)(cid:107) = (cid:107)∇f (x+) + ∂h(x+)(cid:107) ≤ L(cid:107)x − x+(cid:107) + (cid:107)∇f (x) + ∂h(x+)(cid:107) ≤ Lη +  = O().
Thus the next iteration point x+ is an -approximate stationary solution for the objection function
Φ(x) = f (x) + h(x).
To measure the efﬁciency of a stochastic algorithm  we use the following oracle complexity.
Deﬁnition 2 (1) Stochastic ﬁrst-order oracle (SFO): given a point x  SFO outputs a stochastic

(cid:0)x − x+(cid:1). If (cid:107)Gη(x)(cid:107) = 1

(cid:0)x −

η

gradient ∇fi(x) such that Ei∼[n][∇fi(x)] = ∇f (x).

(2) Proximal oracle (PO): given a point x  PO outputs the result of the proximal projection proxηh(x)

(see (3)).

Sometimes  the following assumption on the variance of the stochastic gradients is needed (see the
last column “additional condition" in Table 1). Such an assumption is necessary if one wants the
convergence result to be independent of n. People also denote this case as the online/stochastic
setting  in which the full gradient is not available (see e.g.  [2  16]).
Assumption 1 For ∀x  E[(cid:107)∇fi(x) − ∇f (x)(cid:107)2] ≤ σ2  where σ > 0 is a constant and ∇fi(x) is a
stochastic gradient.

3 Nonconvex ProxSVRG+ Algorithm

In this section  we propose a proximal stochastic gradient algorithm called ProxSVRG+  which is
very straightforward (similar to nonconvex ProxSVRG [24] and convex Prox-SVRG [25]). The
details are described in Algorithm 1. We call B the batch size and b the minibatch size.

4

Algorithm 1 Nonconvex ProxSVRG+
Input: initial point x0  batch size B  minibatch size b  epoch length m  step size η

1: (cid:101)x0 = x0

0 =(cid:101)xs−1
(cid:80)

j∈IB

(cid:80)

xs
gs = 1
B
for t = 1  2  . . .   m do

∇fj((cid:101)xs−1) 4
(cid:0)∇fi(xs
t−1 − ηvs

2: for s = 1  2  . . .   S do
3:
4:
5:
6:
7:
8:
9:
10: end for
Output: ˆx chosen uniformly from {xs

t−1 = 1
vs
i∈Ib
b
t = proxηh(xs
xs

(cid:101)xs = xs

end for
m

t−1) − ∇fi((cid:101)xs−1)(cid:1) + gs

t−1) (call PO once)

t−1}t∈[m] s∈[S]

Compared with Prox-SVRG  ProxSVRG [24] analyzed the nonconvex functions while Prox-SVRG
[25] only analyzed the convex functions. The major difference of our ProxSVRG+ is that we avoid
the computation of the full gradient at the beginning of each epoch  i.e.  B may not equal to n (see
Line 4 of Algorithm 1) while ProxSVRG and Prox-SVRG used B = n. Note that even if we choose
B = n  our analysis is more stronger than ProxSVRG [24]. Also  our ProxSVRG+ shows that
the “stochastically controlled” trick of SCSG [17] (i.e.  the length of each epoch is a geometrically
distributed random variable) is not really necessary for achieving the desired bound.5 As a result 
our straightforward ProxSVRG+ generalizes the result of SCSG to the more general nonsmooth
nonconvex case and yields simpler analysis.

4 Convergence Results

Now  we present the main theorem for our ProxSVRG+ which corresponds to the last two rows in
Table 1 and give some remarks.

Theorem 1 Let step size η = 1
b  where b denotes the minibatch size. Then ˆx returned
by Algorithm 1 is an -accurate solution for problem (1) (i.e.  E[(cid:107)Gη(ˆx)(cid:107)2] ≤ ). We distinguish the
following two cases:

6L and m =

√

1) We let batch size B = n. The number of SFO calls is at most

2) Under Assumption 1  we let batch size B = min{6σ2/  n}. The number of SFO calls is at most

36L(cid:0)Φ(x0) − Φ(x∗)(cid:1)(cid:16) B
36L(cid:0)Φ(x0) − Φ(x∗)(cid:1)(cid:16) B

√

√

+





b

(cid:17)

b


+

b

(cid:17)

b


= O

(cid:0)Φ(x0) − Φ(x∗)(cid:1) = O

36L



(cid:17)

.

b


+

b

(cid:1) 1

√



b


+

b

(cid:17)

 

= O

√



(cid:16) n
(cid:16)(cid:0)n ∧ 1
(cid:19)
(cid:18) 1



.



where ∧ denotes the minimum.

In both cases  the number of PO calls equals to the total number of iterations T   which is at most

Remark: The proof for Theorem 1 is notably different from that of ProxSVRG [24]. Reddi et al. [24]
used a Lyapunov function Rs+1

) + ct(cid:107)xs+1

t = Φ(xs+1

t

t −(cid:101)xS(cid:107)2 and showed that Rs decreases by
(cid:80)n
j=1 ∇fj((cid:101)xs−1) = ∇f ((cid:101)xs−1))

4If B = n  ProxSVRG+ is almost the same as ProxSVRG (i.e.  gs = 1
n

except some detailed parameter settings (e.g.  step-size  epoch length).

5A similar observation was also made in Natasha1.5 [2]. However  Natasha1.5 divides each epoch into
multiple sub-epochs and randomly chooses the iteration point at the end of each sub-epoch. In our ProxSVRG+ 
the length of an epoch is deterministic and it directly uses the last iteration point at the end of each epoch.

5

the accumulated gradient mapping(cid:80)m
Φ(xs) decreases by(cid:80)m

t=1 (cid:107)Gη(xs

t=1 (cid:107)Gη(xs

t )(cid:107)2 in epoch s. In our proof  we directly show that
t )(cid:107)2 using a different analysis. This is made possible by tightening
the inequalities using Young’s inequality and Lemma 2 (which gives the relation between the variance
of stochastic gradient estimator and the inner product of the gradient difference and point difference).
Also  our convergence result holds for any minibatch size b ∈ [1  n] unlike ProxSVRG b ≤ n2/3 (see
Fig. 1). Moreover  ProxSVRG+ uses much less proximal oracle calls than ProxSVRG (see Fig. 2).
For the online/stochastic Case 2)  we avoid the computation of the full gradient at the beginning of
each epoch  i.e.  B (cid:54)= n. Then  we use the similar idea in SCSG [17] to bound the variance term 
but we do not need the “stochastically controlled” trick of SCSG (as we discussed in Section 3) to
achieve the desired convergence bound which yields a much simpler analysis for our ProxSVRG+.
We defer the proof of Theorem 1 to Appendix A.1. Also  similar convergence results for other choices
of epoch length m (cid:54)=

b are provided in Appendix A.2.

√

5 Convergence Under PL Condition
In this section  we provide the global linear convergence rate for nonconvex functions under the
Polyak-Łojasiewicz (PL) condition [22]. The original form of PL condition is
∃µ > 0  such that (cid:107)∇f (x)(cid:107)2 ≥ 2µ(f (x) − f∗)  ∀x 

(6)
where f∗ denotes the (global) optimal function value. It is worth noting that f satisﬁes PL condition
when f is µ-strongly convex. Moreover  Karimi et al. [13] showed that PL condition is weaker than
many conditions (e.g.  strong convexity (SC)  restricted strong convexity (RSC) and weak strong
convexity (WSC) [20]). Also  if f is convex  PL condition is equivalent to the error bounds (EB) and
quadratic growth (QG) condition [19  4]. Note that PL condition implies that every stationary point
is a global minimum  but it does not imply there is a unique minimum unlike the strongly convex
condition.
Due to the nonsmooth term h(x) in problem (1)  we use the gradient mapping (see (4)) to deﬁne a
more general form of PL condition as follows:

∃µ > 0  such that (cid:107)Gη(x)(cid:107)2 ≥ 2µ(Φ(x) − Φ∗)  ∀x.

(7)
Recall that if h(x) is a constant function  the gradient mapping reduces to Gη(x) = ∇Φ(x) = ∇f (x).
Our PL condition is different from the one used in [13  24]. See the Remark (3) after Theorem 2.
Further Motivation: In many cases  although the loss function is generally nonconvex  the local
region near a local minimum may satisfy the PL condition. In fact  there are some recent studies
showing the strong convexity in the neighborhood of the ground truth solution in some simple neural
networks [26  8]. Such results provide further motivation for studying the PL condition. Moreover 
we argue that our ProxSVRG+ is particularly desirable in this case since it ﬁrst converges sublinearly
O(1/) (according to Theorem 1) then automatically converges linearly O(log 1/) (according to
Theorem 2) in the regions as long as the loss function satisﬁes the PL condition locally in these
regions. We list the convergence results in Table 3 (also see the remarks after Theorem 2).

Table 3: Under the PL condition with parameter µ

Algorithms
ProxGD [13]
(full gradient)

ProxSVRG/SAGA

[24]

(smooth nonconvex 

SCSG [17]
i.e.  h(x) ≡ 0)

(cid:16) b

1
3
µ

O

ProxSVRG+
(this paper)

Stochastic ﬁrst-order

oracle (SFO)
µ log 1
O( n
 )



 + n log 1

(cid:1)
 +(cid:0)n ∧ 1
(cid:1)

 + b

µ log 1



µ

log 1

b

√
µ

O(cid:0) n
(cid:1) 2
(cid:0)n ∧ 1
O(cid:0) n
(cid:16)(cid:0)n ∧ 1
(cid:1) 1

√
µ

µ

b

√
µ

3 log 1

log 1

(cid:17)

(cid:1) log 1
(cid:17)



Proximal oracle

(PO)
µ log 1
 )

O( 1

O(cid:0) n

µb3/2 log 1



(cid:1)

Addi.

condition

–

b ≤ n2/3

NA

σ = O(1)

O( 1

µ log 1
 )

–

σ = O(1)
The notation ∧ denotes the minimum. Similar to Table 2  ProxSVRG+ is better than ProxGD and

O

 + b

µ log 1

log 1

µ log 1
 )

O( 1

µ

b



ProxSVRG/SAGA  and generalizes the SCSG by choosing different minibatch size b.

6

Similar to Theorem 1  we provide the convergence result of ProxSVRG+ (Algorithm 1) under PL-
condition in the following Theorem 2. Note that under PL condition (i.e. (7) holds)  ProxSVRG+ can

to [24]  we assume the condition number L/µ >
n for simplicity. Otherwise  one can choose
different step size η which is similar to the case where we deal with other choices of epoch length m
(see Appendix A.2).

directly use the ﬁnal iteration(cid:101)xS as the output point instead of the randomly chosen one ˆx. Similar
6L and b denote the minibatch size. Then the ﬁnal iteration point(cid:101)xS
in Algorithm 1 satisﬁes E[Φ((cid:101)xS) − Φ∗] ≤  under PL condition. We distinguish the following two

Theorem 2 Let step size η = 1

√

cases:

1) We let batch size B = n. The number of SFO calls is bounded by

b
µ
2) Under Assumption 1  we let batch size B = min{ 6σ2

√
µ

log

1


+

O

b

µ   n}. The number of SFO calls is bounded

(cid:16) n

(cid:16)(cid:0)n ∧ 1

µ

(cid:17)

.

log

1


(cid:17)

 

1


+

b
µ

log

1


log

b

(cid:1) 1

√
µ

(cid:16) 1

µ

(cid:17)

.

log

1


by

O
where ∧ denotes the minimum.

O

Remark:

In both cases  the number of PO calls equals to the total number of iterations T which is bounded by

µ log 1

(1) We show that ProxSVRG+ directly obtains a global linear convergence rate without restart by a
nontrivial proof. Note that Reddi et al. [24] used PL-SVRG/SAGA to restart ProxSVRG/SAGA
O(log(1/)) times to obtain the linear convergence rate under PL condition.
Moreover  similar to Table 2  if we choose b = 1 or n for ProxSVRG+  then its convergence result
 )  which is the same as ProxGD [13]. If we choose b = n2/3 for ProxSVRG+  then
is O( n
the convergence result is O( n2/3
 )  the same as the best result achieved by ProxSVRG/SAGA
[24]. If we choose b = 1/(µ)2/3 (assuming 1/(µ) < n) for ProxSVRG+  then its convergence
result is O(
 ) which generalizes the best result of SCSG [17] to the more general
nonsmooth nonconvex case and is better than ProxGD and ProxSVRG/SAGA. Also note that our
ProxSVRG+ uses much less proximal oracle calls than ProxSVRG/SAGA if b < n2/3.

µ5/32/3 log 1

µ log 1

1

(2) Another beneﬁt of ProxSVRG+ is that it can automatically switch to the faster linear convergence
rate in some regions as long as the loss function satisﬁes the PL condition locally in these regions.
This is impossible for ProxSVRG [24] since it needs to be restarted many times.

(3) We want to point out that [13  24] used the following form of PL condition:
∃µ > 0  such that Dh(x  α) ≥ 2µ(Φ(x) − Φ∗)  ∀x 

(cid:8)(cid:104)∇f (x)  y− x(cid:105) + α

2 (cid:107)y− x(cid:107)2 + h(y)− h(x)(cid:9). Our PL condition

(8)

where Dh(x  α) := −2α miny
is arguably more natural. In fact  one can show that if α = 1/η  our new PL condition (7) implies
(8). For a direct comparison with prior results  we also provide the proof of the same result of
Theorem 2 using the previous PL condition (8) in the appendix.

The proofs of Theorem 2 under PL form (7) and (8) are provided in Appendix B.1 and B.2  respectively.
Recently  Csiba and Richtárik [6] proposed a novel weakly PL condition. The (strongly) PL condition
(7) or (8) serves as a generalization of strong convexity as we discussed in the beginning of this
section. One can achieve linear convergence under (7) or (8). However  the weakly PL condition [6]
may be considered as a generalization of (weak) convexity. Although one only achieves the sublinear
convergence under this condition  it is still interesting to ﬁgure out similar (sublinear) convergence
(for ProxSVRG+  ProxSVRG  etc.) under their weakly PL condition.

7

6 Experiments

In this section  we present the experimental results. We compare the nonconvex ProxSVRG+ with
nonconvex ProxGD  ProxSGD [10]  ProxSVRG [24]. We conduct the experiments using the non-
negative principal component analysis (NN-PCA) problem (same as [24]). In general  NN-PCA is
NP-hard. Speciﬁcally  the optimization problem for a given set of samples (i.e.  {zi}n

i=1) is:

(cid:17)
Note that (9) can be written in the form (1)  where f (x) =(cid:80)n

xT(cid:16) n(cid:88)

− 1
2

min

(cid:107)x(cid:107)≤1 x≥0

zizT
i

i=1

x.

i=1 fi(x) =(cid:80)n

i=1 − 1

(9)

2 (xT zi)2 and
h(x) = IC(x) where set C = {x ∈ Rd|(cid:107)x(cid:107) ≤ 1  x ≥ 0}. We conduct the experiment on the
standard MNIST and ‘a9a’ datasets. 6 The experimental results on both datasets (corresponding to
the ﬁrst row and second row in Figure 3–5) are almost the same.
The samples from each dataset are normalized  i.e.  (cid:107)zi(cid:107) = 1 for all i ∈ n. The parameters of
the algorithms are chosen as follows: L can be precomputed from the data samples {zi}n
i=1 in the
same way as in [18]. The step sizes η for different algorithms are set to be the ones used in their
convergence results: For ProxGD  it is η = 1/L (see Corollary 1 in [10]); for ProxSGD  η = 1/(2L)
(see Corollary 3 in [10]); for ProxSVRG  η = b3/2/(3Ln) (see Theorem 6 in [24]). The step size
for our ProxSVRG+ is 1/(6L) (see our Theorem 1). We did not further tune the step sizes. The
batch size B (in Line 4 of Algorithm 1) is equal to n/5 (i.e.  20% data samples). We also considered
B = n/10  the performance among these algorithms are similar to the case B = n/5. In practice 
one can tune the step size η and parameter B.
Regarding the comparison among these algorithms  we use the number of SFO calls (see Deﬁnition
2) to evaluate them since the number of PO calls of them are the same except ProxSVRG (which is
already clearly demonstrated by Figure 2). Note that we amortize the batch size (n or B in Line 4 of
Algorithm 1) into the inner loops  so that the curves in the ﬁgures are smoother  i.e.  the number of
SFO calls for each iteration (inner loop) of ProxSVRG and ProxSVRG+ is counted as b + n/m and
b + B/m  respectively. Note that ProxGD uses n SFO calls (full gradient) in each iteration.

Figure 3: Comparison among algorithms with different minibatch size b

In Figure 3  we compare the performance of these four algorithms as we vary the minibatch size b.
In particular  the ﬁrst column (b = 4) shows that ProxSVRG+ and ProxSVRG perform similar to
ProxSGD and ProxGD respectively  which is quite consistent with the theoretical results (Figure
1). Then  ProxSVRG+ and ProxSVRG both get better as b increases. Note that our ProxSVRG+
performs better than ProxGD  ProxSGD and ProxSVRG.

6The datasets can be downloaded from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/

datasets/

8

0123456#SFO/n70006000500040003000200010000Function valuea9a (b=4)ProxGDProxSGDProxSVRGProxSVRG+ 0123456#SFO/n70006000500040003000200010000Function valuea9a (b=64)ProxGDProxSGDProxSVRGProxSVRG+ 0123456#SFO/n70006000500040003000200010000Function valuea9a (b=256)ProxGDProxSGDProxSVRGProxSVRG+ 0123456#SFO/n12000100008000600040002000Function valueMNIST (b=4)ProxGDProxSGDProxSVRGProxSVRG+ 0123456#SFO/n12000100008000600040002000Function valueMNIST (b=64)ProxGDProxSGDProxSVRGProxSVRG+ 0123456#SFO/n12000100008000600040002000Function valueMNIST (b=256)ProxGDProxSGDProxSVRGProxSVRG+ Figure 4: ProxSVRG+ and ProxSVRG under different b

Figure 5: Under the best b

Figure 4 demonstrates that our ProxSVRG+ prefers smaller minibatch sizes than ProxSVRG (see
the curves with dots). Then  in Figure 5  we compare the algorithms with their corresponding best
minibatch size b.
In conclusion  the experimental results are quite consistent with the theoretical results  i.e.  different
algorithms favor different minibatch sizes (see Figure 1). Concretely  our ProxSVRG+ achieves its
best performance with a moderate minibatch size b = 256 unlike ProxSVRG with b = 2048/4096.
Besides  choosing b = 64 is already good enough for ProxSVRG+ by comparing the second column
and last column of Figure 3  however ProxSVRG is only as good as ProxSGD with such a minibatch
size. Moreover  ProxSVRG+ uses much less proximal oracle calls than ProxSVRG if b < n2/3 (see
Figure 2). Note that small minibatch size also usually provides better generalization in practice. Thus 
we argue that our ProxSVRG+ might be more attractive in certain applications due to its moderate
minibatch size.

7 Conclusion

√

√

In this paper  we propose a simple proximal stochastic method called ProxSVRG+ for nonsmooth
nonconvex optimization. We prove that ProxSVRG+ improves/generalizes several well-known
convergence results (e.g.  ProxGD  ProxSGD  ProxSVRG/SAGA and SCSG) by choosing proper
minibatch sizes. In particular  ProxSVRG+ is
bn if n > 1/) times faster than ProxGD 
which partially answers the open problem (i.e.  developing stochastic methods with provably better
performance than ProxGD with constant minibatch size b) proposed in [24]. Also  ProxSVRG+
generalizes the results of SCSG [17] to this nonsmooth nonconvex case  and it is more straightforward
than SCSG and yields simpler analysis. Moreover  for nonconvex functions satisfying Polyak-
Łojasiewicz condition  we prove that ProxSVRG+ achieves the global linear convergence rate without
restart. As a result  ProxSVRG+ can automatically switch to the faster linear convergence rate (i.e. 
O(log 1/)) from sublinear convergence rate (i.e.  O(1/)) in some regions (e.g.  the neighborhood of
a local minimum) as long as the objective function satisﬁes the PL condition locally in these regions.
This is impossible for ProxSVRG [24] since it needs to be restarted O(log 1/) times.

b (or

Acknowledgments

This research is supported in part by the National Basic Research Program of China Grant
2015CB358700  the National Natural Science Foundation of China Grant 61772297  61632016 
61761146003  and a grant from Microsoft Research Asia. The authors would like to thank Rong Ge
(Duke)  Xiangliang Zhang (KAUST) and the anonymous reviewers for their useful suggestions.

9

0.00.51.01.52.02.53.0#SFO/n7000600050004000300020001000Function valuea9a (ProxSVRG+)b=1b=16b=64b=256b=512b=1024b=2048b=4096b=8192b=163840.00.51.01.52.02.53.0#SFO/n7000600050004000300020001000Function valuea9a (ProxSVRG)b=1b=16b=64b=256b=512b=1024b=2048b=4096b=8192b=163840123456#SFO/n70006000500040003000200010000Function valuea9aProxGDProxSGDProxSVRG (b=2048)ProxSVRG+ (b=256)0.00.51.01.52.02.53.0#SFO/n12000100008000600040002000Function valueMNIST (ProxSVRG+)b=1b=16b=64b=256b=512b=1024b=2048b=4096b=8192b=163840.00.51.01.52.02.53.0#SFO/n12000100008000600040002000Function valueMNIST (ProxSVRG)b=1b=16b=64b=256b=512b=1024b=2048b=4096b=8192b=163840123456#SFO/n12000100008000600040002000Function valueMNISTProxGDProxSGDProxSVRG (b=4096)ProxSVRG+ (b=256)References
[1] Zeyuan Allen-Zhu. Katyusha: the ﬁrst direct acceleration of stochastic gradient methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
1200–1205. ACM  2017.

[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint

arXiv:1708.08694  2017.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[4] Mihai Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM

Journal on Optimization  10(4):1116–1135  2000.

[5] Aleksandr Aravkin and Damek Davis. A smart stochastic algorithm for nonconvex optimization

with applications to robust machine learning. arXiv preprint arXiv:1610.01101  2016.

[6] Dominik Csiba and Peter Richtárik. Global convergence of arbitrary-block gradient methods

for generalized polyak-łojasiewicz functions. arXiv preprint arXiv:1709.03014  2017.

[7] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems  pages 1646–1654  2014.

[8] Haoyu Fu  Yuejie Chi  and Yingbin Liang. Local geometry of one-hidden-layer neural networks

for logistic regression. arXiv preprint arXiv:1802.06463  2018.

[9] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[10] Saeed Ghadimi  Guanghui Lan  and Hongchao Zhang. Mini-batch stochastic approximation
methods for nonconvex stochastic composite optimization. Mathematical Programming  155(1-
2):267–305  2016.

[11] Priya Goyal  Piotr Dollár  Ross Girshick  Pieter Noordhuis  Lukasz Wesolowski  Aapo Kyrola 
Andrew Tulloch  Yangqing Jia  and Kaiming He. Accurate  large minibatch sgd: training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677  2017.

[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[13] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases  pages 795–811. Springer  2016.

[14] Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
arXiv preprint arXiv:1609.04836  2016.

[15] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv

preprint arXiv:1507.02000  2015.

[16] Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic

optimization. SIAM Journal on Optimization  28(4):2753–2782  2018.

[17] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via scsg methods. In Advances in Neural Information Processing Systems  pages 2345–2355 
2017.

[18] Qunwei Li  Yi Zhou  Yingbin Liang  and Pramod K Varshney. Convergence analysis of proximal
gradient with momentum for nonconvex optimization. In International Conference on Machine
Learning  pages 2111–2119  2017.

10

[19] Zhi-Quan Luo and Paul Tseng. Error bounds and convergence analysis of feasible descent

methods: a general approach. Annals of Operations Research  46(1):157–178  1993.

[20] Ion Necoara  Yurii Nesterov  and Francois Glineur. Linear convergence of ﬁrst order methods

for non-strongly convex optimization. arXiv preprint arXiv:1504.06298  2015.

[21] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  2004.

[22] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychisli-

tel’noi Matematiki i Matematicheskoi Fiziki  3(4):643–653  1963.

[23] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabás Póczos  and Alex Smola. Stochastic
variance reduction for nonconvex optimization. In International conference on machine learning 
pages 314–323  2016.

[24] Sashank J Reddi  Suvrit Sra  Barnabás Póczos  and Alexander J Smola. Proximal stochastic
methods for nonsmooth nonconvex ﬁnite-sum optimization. In Advances in Neural Information
Processing Systems  pages 1145–1153  2016.

[25] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[26] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery

guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175  2017.

[27] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic nested variance reduction for nonconvex

optimization. arXiv preprint arXiv:1806.07811  2018.

11

,Zhize Li
Jian Li