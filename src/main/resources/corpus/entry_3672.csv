2018,NEON2: Finding Local Minima via First-Order Oracles,We propose a reduction for non-convex optimization that can (1) turn an stationary-point finding algorithm into an local-minimum finding one  and (2) replace the Hessian-vector product computations with only gradient computations. It works both in the stochastic and the deterministic settings  without hurting the algorithm's performance.

As applications  our reduction turns Natasha2 into a first-order method without hurting its theoretical performance. It also converts SGD  GD  SCSG  and SVRG into algorithms finding approximate local minima  outperforming some best known results.,NEON2: Finding Local Minima

via First-Order Oracles

Zeyuan Allen-Zhu∗
Microsoft Research AI
Redmond  WA 98052

zeyuan@csail.mit.edu

Yuanzhi Li∗

Stanford University
Stanford  CA 94305

yuanzhil@stanford.edu

Abstract

We propose a reduction for non-convex optimization that can (1) turn an
stationary-point ﬁnding algorithm into an local-minimum ﬁnding one  and (2) re-
place the Hessian-vector product computations with only gradient computations.
It works both in the stochastic and the deterministic settings  without hurting the
algorithm’s performance.
As applications  our reduction turns Natasha2 into a ﬁrst-order method with-
out hurting its theoretical performance. It also converts SGD  GD  SCSG  and
SVRG into algorithms ﬁnding approximate local minima  outperforming some
best known results.

n(cid:88)

i=1

f (x) =

1
n

fi(x)

Introduction

1
Nonconvex optimization has become increasingly popular due its ability to capture modern machine
learning tasks in large scale. For instance  training neural nets corresponds to minimizing a function

over x ∈ Rd that is non-convex  where each training sample i corresponds to one loss function
fi(·) in the summation. This average structure allows one to perform stochastic gradient descent
(SGD) which uses a random ∇fi(x) —corresponding to computing backpropagation once— to
approximate ∇f (x) and performs descent updates.
Motivated by such large-scale machine learning applications  we wish to design faster ﬁrst-order
non-convex optimization methods that outperform the performance of gradient descent  both in the
online and ofﬂine settings. In this paper  we say an algorithm is online if its complexity is indepen-
dent of n (so n can be inﬁnite)  and ofﬂine otherwise. In recently years  researchers across different
communities have gathered together to tackle this challenging question. By far  known theoretical
approaches mostly fall into one of the following two categories.
First-order methods for stationary points.
In analyzing ﬁrst-order methods  we denote by gra-
dient complexity T the number of computations of ∇fi(x). To achieve an ε-approximate stationary
point —namely  a point x with (cid:107)∇f (x)(cid:107) ≤ ε— it is a folklore that gradient descent (GD) is ofﬂine

(cid:1)  while stochastic gradient decent (SGD) is online and needs T ∝ O(cid:0) 1

and needs T ∝ O(cid:0) n
In recent years  the ofﬂine complexity has been improved to T ∝ O(cid:0) n2/3
24]  and the online complexity has been improved to T ∝ O(cid:0) 1

Both of them rely on the so-called variance-reduction technique  originally discovered for convex
problems [12  17  27  29].

ε10/3

(cid:1).
(cid:1) by the SVRG method [4 
(cid:1) by the SCSG method [19].

ε4

ε2

ε2

∗Authors sorted in alphabetical order. We acknowledge a parallel work of Xu and Yang [31] (which ap-
peared online a few days before us)  and have adopted their algorithm name Neon and called our new algorithm
Neon2. Our algorithms are very different from theirs  and give better theoretical performance. The full version
of this paper can be found on https://arxiv.org/abs/1711.06673.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

(a)

(b)

(c)

Figure 1: Neon vs Neon2 for ﬁnding (ε  δ)-approximate local minima. We emphasize that Neon2 and Neon try

to tackle the same problem  but are different algorithms.

Both algorithms SVRG and SCSG are only capable of ﬁnding approximate stationary points  which
may not necessarily be approximate local minima and are arguably bad solutions for deep neural
nets [10  11  15]. Thus 

can we turn stationary-point ﬁnding algorithms into local-minimum ﬁnding ones?

Hessian-vector methods for local minima. Using information about the Hessian  one can ﬁnd
ε-approximate local minima —namely  a point x with (cid:107)∇f (x)(cid:107) ≤ ε and also ∇2f (x) (cid:23) −ε1/CI.2
In 2006  Nesterov and Polyak [21] showed that one can ﬁnd an ε-approximate in O( 1
ε1.5 ) iterations 
but each iteration requires an (ofﬂine) computation as heavy as inverting the matrix ∇2f (x).
To ﬁx this issue  researchers propose to study the so-called “Hessian-free” methods that  in addition
to gradient computations  also compute Hessian-vector products. That is  instead of using the full
matrix ∇2fi(x) or ∇2f (x)  these methods also compute ∇2fi(x)·v for indices i and vectors v.3 For
Hessian-free methods  we denote by gradient complexity T the number of computations of ∇fi(x)
plus that of ∇2fi(x) · v. The hope of using Hessian-vector products is to improve the complexity T
as a function of ε.
Such improvement was ﬁrst shown possible independently by [1  8] for the ofﬂine setting  with

(cid:1) so is better than that of gradient descent. In the online setting  the

complexity T ∝(cid:0) n
ﬁrst improvement was by Natasha2 which gives complexity T ∝(cid:0) 1

(cid:1) [2].

ε1.5 + n3/4

ε1.75

Unfortunately  it is argued by some researchers that Hessian-vector products are not general enough
and may not be as simple to implement as evaluating gradients [9]. Therefore 

ε3.25

can we turn Hessian-free methods into ﬁrst-order ones  without hurting their performance?

1.1 From Hessian-Vector Products to First-Order Methods
Recall by deﬁnition of derivative we have

∇2fi(x) · v = limq→0{∇fi(x+qv)−∇fi(x)

} .

q

q

∇fi(x+qv)−∇fi(x)

for some small q > 0?

Given any Hessian-free method  at least at a high level  can we replace every occurrence of ∇2fi(x)·
v with w =
Note the error introduced in this approximation is (cid:107)∇2fi(x)·v−w(cid:107) ∝ q(cid:107)v(cid:107)2. However  the original
algorithm might not be stable to adversarial noise  thus  an (inverse) exponentially small q might be
required. One of our main contributions is to show how to implement these algorithms stably  so we
can convert Hessian-free methods into ﬁrst-order ones with an (inverse) polynomially small q.
In this paper  we demonstrate this idea by converting negative-curvature-search (NC-search) subrou-
tines into ﬁrst-order processes. NC-search is a key subroutine used in state-of-the-art Hessian-free
methods that have rigorous proofs [1  2  8]. It solves the following simple task:

negative-curvature search (NC-search)

given point x0  decide if ∇2f (x0) (cid:23) −δI or ﬁnd a unit vector v such that v(cid:62)∇2f (x0)v ≤ − δ
2.
2We say A (cid:23) −δI if all the eigenvalues of A are no smaller than −δ. In this high-level introduction  we
3Hessian-free methods are useful because when fi(·) is explicitly given  computing its gradient is in the

focus only on the case when δ = ε1/C for some constant C.

same complexity as computing its Hessian-vector product [23] [28]  using backpropagation.

2

T=δ-5T=δ-3ε-2T=ε-4T=δ-7Neon2+SGDNeon+SGDεε2/3ε4/7ε1/2ε1/4δε-4ε-5TT=δ-5T=δ-3ε-2T=ε-3.33T=δ-6Neon2+SCSGNeon+SCSGεε2/3ε4/9ε1/2ε1/4δε-4ε-5ε-3.33TT=δ-5T=δ-1ε-3T=ε-3.25T=δ-6Neon2+Natasha2Neon+Natashaεε3/4ε3/5ε1/2ε1/4δε-3.25ε-3.75ε-3.6ε-5T1

√

Online Setting.

(cid:101)O(1/δ2) computations of Hessian-vector products. This is ﬁrst proved by Allen-Zhu and Li [7] and

In the online setting  NC-search can be solved by Oja’s algorithm [22] which costs

ﬁrst applied to NC-search in Natasha2 [2]).
In this paper  we propose a method Neon2online which solves the NC-search problem via only
stochastic ﬁrst-order updates. That is  starting from x1 = x0 + ξ where ξ is some random per-
turbation  we keep updating xt+1 = xt − η(∇fi(xt) − ∇fi(x0)). In the end  the vector xT − x0
gives us enough information about the negative curvature.

Theorem 1 (informal). Our Neon2online algorithm solves NC-search using (cid:101)O(1/δ2) stochastic
This complexity (cid:101)O(1/δ2) matches that of Oja’s algorithm  and is information-theoretically optimal
that proposed this approach. However  Neon needs (cid:101)O(1/δ3) stochastic gradients  because it uses

(up to log factors)  see the lower bound in [7].
The independent work Neon by Xu and Yang [31] is actually the ﬁrst recorded theoretical result

gradients  without Hessian-vector product computations.

In this paper  we convert (a variant of) Lanscoz’s method into a ﬁrst-order one:

full gradient descent to ﬁnd NC (on a sub-sampled objective) inspired by power method and [16];
instead  Neon2online uses stochastic gradients and is based on the recent result of Oja’s algorithm [7].
Plugging Neon2online into Natasha2 [2]  we achieve the following corollary (see Figure 1(c)):
Theorem 2 (informal). Neon2online turns Natasha2 into a stochastic ﬁrst-order method  without
ε3.25 +

hurting its performance. That is  it ﬁnds an (ε  δ)-approximate local minimum in T = (cid:101)O(cid:0) 1
(cid:1) stochastic gradient computations  without Hessian-vector product computations.

ε3δ + 1
δ5
(We say x is an (ε  δ)-approximate local minimum if (cid:107)∇f (x)(cid:107) ≤ ε and ∇2f (x) (cid:23) −δI.)
Ofﬂine Deterministic Setting. There are a number of ways to solve the NC-search problem in the

Their approach is inspired by [16]  but our Neon2det is based on Chebyshev approximation theory.
By putting Neon2det and Neon2ﬁnite into the CDHS method of Carmon et al. [8]  we have
Theorem 4 (informal). Neon2det turns CDHS into a ﬁrst-order method without hurting its perfor-

ofﬂine setting using Hessian-vector products. Most notably  power method uses (cid:101)O(n/δ) computa-
tions of Hessian-vector products  and Lanscoz method [18] uses (cid:101)O(n/
Theorem 3 (informal). Our Neon2det algorithm solves NC-search using (cid:101)O(1/
(or equivalently (cid:101)O(n/
The independent work Neon [31] also applies to the ofﬂine setting  and needs (cid:101)O(1/δ) full gradients.
(cid:1) full gradient computations.
mance: it ﬁnds an (ε  δ)-approximate local minimum in (cid:101)O(cid:0) 1
invert [13] method  using (cid:101)O(n + n3/4/
Theorem 5 (informal). Neon2ﬁnite algorithm solves NC-search using (cid:101)O(n + n3/4/
mance: it ﬁnds an (ε  δ)-approximate local minimum in T = (cid:101)O(cid:0) n

gradients.
Putting Neon2ﬁnite into the (ﬁnite-sum version of) CDHS method [8]  we have4
Theorem 6 (informal). Neon2ﬁnite turns CDHS into a ﬁrst-order method without hurting its perfor-

1.1.1 Ofﬂine Finite-Sum Setting
Recall one can also solve the NC-search problem in the ofﬂine setting by the (ﬁnite-sum) shift-and-
δ) computations of Hessian-vector products. We refer to

this method as “ﬁnite-sum SI”  and also convert it into a ﬁrst-order method.

(cid:1) stochastic

δ) stochastic gradients).

gradient computations.
Remark 1.1. All the cited works in Section 1.1 requires the objective to have (1) Lipschitz-
continuous Hessian and (2) Lipschitz-continuous gradient. One can argue that (1) and (2) are both
necessary for ﬁnding approximate local minima  but if only ﬁnding approximate stationary points 
then only (2) is necessary. We shall formally discuss our assumptions in Section 2.

ε1.5 + n

δ3 + n3/4

ε1.75 + n3/4

δ3.5

√

δ) stochastic

√

δ) full gradients

√

δ) computations.

ε1.75 + 1
δ3.5

√

4The original paper of CDHS only stated their algorithm in the deterministic setting  but is easily veriﬁable

to work in the ﬁnite-sum setting  see discussions in [1].

3

gradient complexity T

Hessian-vector

products

(cid:1)
(cid:1)

no

no

no

no

no

no

no

(cid:1)
(cid:1)

variance
bound
needed

Lip.

smooth
needed

2nd-order
smooth

no

needed

needed

needed

needed

needed

needed

needed

needed

needed

needed

needed

no

needed

needed

needed

needed

needed

needed

needed

needed

needed

needed

no

no

needed

needed

needed

needed

needed

needed

↓ ofﬂine methods ↓

no

no

no

no

needed

no

no

needed

no

no

no

no

no

no
no

no

no

no

needed

no

needed

needed

needed

needed

needed

no

needed
needed

needed
needed

needed

needed

needed

needed

needed

needed

stationary

algorithm

SGD (folklore)
perturbed SGD

local minima

Neon+SGD

stationary

local minima

Neon2+SGD

SCSG
Neon+SCSG

Neon2+SCSG

Natasha2

local minima

Neon+Natasha2

Neon2+Natasha2

stationary

GD (folklore [20])
perturbed GD

local minima

Neon2+GD

stationary

SVRG

local minima

Reddi et al.

Neon2+SVRG

stationary

“convex until guilty”

local minima

FastCubic
CDHS

Neon2+CDHS

ε2δ3 + 1
δ6

ε2δ3 + 1
δ5

ε3δ + 1
δ5

ε3δ + 1
δ6

(cid:1)
(cid:1)
(cid:1)

ε4
ε4 + poly(d)
δ16

ε4 + 1

ε2δ3 + 1
δ5

(cid:1)

ε4 + 1
δ7

ε3.25 + 1

ε3.25 + 1

ε10/3
ε10/3 + 1
ε10/3 + 1

O(cid:0) 1
(cid:1)
[14] (cid:101)O(cid:0) poly(d)
[31] (cid:101)O(cid:0) 1
(cid:101)O(cid:0) 1
(cid:1)
[19] O(cid:0) 1
[31] O(cid:0) 1
O(cid:0) 1
[2] (cid:101)O(cid:0) 1
[31] (cid:101)O(cid:0) 1
(cid:101)O(cid:0) 1
O(cid:0) n
(cid:1)
(cid:1)
[16] (cid:101)O(cid:0) n
(cid:1)
(cid:101)O(cid:0) n
[4] O(cid:0) n2/3
ε2 + n(cid:1)
[25] (cid:101)O(cid:0) n2/3
(cid:1)
[9] (cid:101)O(cid:0) n
(cid:101)O(cid:0) n
(cid:1)
(cid:101)O(cid:0) n
(cid:1)

ε2
ε2 + n
δ4

ε1.5 + n

ε1.5 + n

ε2 + n
δ3.5

[1]
[8]

n3/4
δ3.5

n3/4
δ3.5

[24]

ε1.75

(cid:1)

ε2 + n

δ3 + n3/4

δ3.5

δ3 + n3/4

ε1.75 +

δ3 + n3/4

ε1.75 +

ε3.25 + 1

ε3δ + 1
δ5

↑ online methods ↑

Table 1: Complexity for ﬁnding (cid:107)∇f (x)(cid:107) ≤ ε and ∇2f (x) (cid:23) −δI. Following tradition  in these complexity
bounds  we assume variance and smoothness parameters as constants  and only show the dependency
on n  d  ε.

Remark 1. Variance bounds is needed for online methods (ﬁrst half of the table).
Remark 2. Lipschitz smoothness is needed for ﬁnding even approximate stationary points.
Remark 3. Second-order Lipschitz smoothness is needed for ﬁnding approximate local minima.

1.2 From Stationary Points to Local Minima
Given any ﬁrst-order method that ﬁnds stationary points (such as GD  SGD  SVRG or SCSG)  we
can hope for using the NC-search routine to identify whether or not its output x satisﬁes ∇2f (x) (cid:23)
−δI. If so  then automatically x becomes an (ε  δ)-approximate local minima so we can terminate.
If not  we can go into its negative curvature direction to further decrease the objective.
In the independent work of Xu and Yang [31]  they applied their Neon method for NC-search  and
thus turned SGD and SCSG into ﬁrst-order methods ﬁnding approximate local minima. In this paper 
we use Neon2 instead. We show the following theorem:
Theorem 7 (informal). To ﬁnd an (ε  δ)-approximate local minima 

4

0 log(100d)

 

(cid:5) for sufﬁciently large constant C0
(cid:5) ξ is Gaussian random vector with norm σ := (100d)−3C0 η2δ3

L2

xt+1 ← xt − η (∇fi(xt) − ∇fi(x0)) where i ∈R [n].
if (cid:107)xt+1 − x0(cid:107)2 ≥ r then return v = xs−x0
(cid:107)xs−x0(cid:107)2

for a uniformly random s ∈ [t].

(cid:5) r := (100d)C0 σ

δ

Algorithm 1 Neon2online
weak (f  x0  δ)
0 L2 log(100d)  T ← C2
1: η ←
C2
2: ξ ← σ ξ(cid:48)
(cid:107)ξ(cid:48)(cid:107)2
3: x1 ← x0 + ξ.
4: for t ← 1 to T do
5:
6:

where ξ(cid:48) ∼ N (0  I).

ηδ

7: end for
8: return v = ⊥;

(a) Neon2+SGD needs T = (cid:101)O(cid:0) 1
(b) Neon2+SCSG needs T = (cid:101)O(cid:0) 1
(c) Neon2+GD needs T = (cid:101)O(cid:0) n
(d) Neon2+SVRG needs T = (cid:101)O(cid:0) n2/3

ε2 + n
δ3.5

(cid:1) stochastic gradients;
(cid:1) stochastic gradients; and
(cid:1) full gradients).
(cid:1) stochastic gradients.

(cid:1) (so (cid:101)O(cid:0) 1

ε2δ3 + 1
δ5
ε2 + 1
δ3.5

δ3 + n3/4

δ3.5

ε2δ3 + 1
δ5

ε4 + 1
ε10/3 + 1

ε2 + n

1.3 Roadmap

We introduce notions and formalize the problem in Section 2. We introduce Neon2 in the online 
deterministic  and SVRG settings respectively in Section 3  Section 4 and Section 5. We apply
Neon2 to SGD  GD  Natasha2  CDHS  SCSG and SVRG in Section 6. Most of the proofs are in the
appendix.
2 Preliminaries
Throughout this paper  we denote by (cid:107) · (cid:107) the Euclidean norm. We use i ∈R [n] to denote that i
is generated from [n] = {1  2  . . .   n} uniformly at random. We denote by I[event] the indicator
function of probabilistic events.
We denote by (cid:107)A(cid:107)2 the spectral norm of matrix A. For symmetric matrices A and B  we write
A (cid:23) B to indicate that A − B is positive semideﬁnite (PSD). Therefore  A (cid:23) −σI if and only if
all eigenvalues of A are no less than −σ. We denote by λmin(A) and λmax(A) the minimum and
maximum eigenvalue of a symmetric matrix A.
Deﬁnition 2.1. For a function f : Rd → R 
• f is L-Lipschitz smooth (or L-smooth for short) if ∀x  y ∈ Rd  (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107).
• f is second-order L2-Lipschitz smooth (or L2-second-order smooth for short) if

∀x  y ∈ Rd  (cid:107)∇2f (x) − ∇2f (y)(cid:107)2 ≤ L2(cid:107)x − y(cid:107).

2.1 Problem and Assumptions

Throughout the paper we study

(cid:110)

(cid:80)n

(cid:111)

(2.1)
where both f (·) and each fi(·) can be nonconvex. We wish to ﬁnd (ε  δ)-approximate local minima
which are points x satisfying

f (x) := 1
n

i=1 fi(x)

minx∈Rd

(cid:107)∇f (x)(cid:107) ≤ ε

and ∇2f (x) (cid:23) −δI .

We need the following three assumptions
• Each fi(x) is L-Lipschitz smooth.
• Each fi(x) is second-order L2-Lipschitz smooth.
• Stochastic gradients have bounded variance: ∀x ∈ Rd :
(This assumption is needed only for online algorithms.)

5

Ei∈R[n] (cid:107)∇f (x) − ∇fi(x)(cid:107)2 ≤ V .

(cid:80)n
(cid:5) for boosting conﬁdence of Neon2online
i=1 fi(x)  vector x0  negative curvature δ > 0  conﬁdence p ∈ (0  1].
(cid:5) boost the conﬁdence

weak

Algorithm 2 Neon2online(f  x0  δ  p)
Input: Function f (x) = 1
1: for j = 1  2 ··· Θ(log 1/p) do
n
vj ← Neon2online
weak (f  x0  δ);
2:
if vj (cid:54)= ⊥ then
3:
4:
5:
6:
7:
8:
9: end for
10: return v = ⊥.

m ← Θ( L2 log 1/p
Draw i1  . . .   im ∈R [n].
zj = 1
if zj ≤ −3δ/4 return v = vj

end if

m(cid:107)v(cid:48)(cid:107)2

δ2

2

)  v(cid:48) ← Θ( δ

j=1(v(cid:48))(cid:62)(cid:0)∇fij (x0 + v(cid:48)) − ∇fij (x0)(cid:1)
(cid:80)m

)v.

L2

3 Neon2 in the Online Setting
We propose Neon2online as the online version of Neon2.
It repeatedly invokes Neon2online
weak
Algorithm 1  whose goal is to solve the NC-search problem with conﬁdence 2/3 only;
weak repeatedly for log(1/p) times to boost the conﬁdence to 1 − p.
Neon2online invokes Neon2online
We prove the following theorem:
Theorem 1 (Neon2online). Let f (x) = 1
order smooth. For every point x0 ∈ Rd  every δ ∈ (0  L]  every p ∈ (0  1)  the output
n

i=1 fi(x) where each fi is L-smooth and L2-second-

(cid:80)n

in
then

v = Neon2online(f  x0  δ  p)

satisﬁes that  with probability at least 1 − p:
1. If v = ⊥  then ∇2f (x0) (cid:23) −δI.
2. If v (cid:54)= ⊥  then (cid:107)v(cid:107)2 = 1 and v(cid:62)∇2f (x0)v ≤ − δ
2 .

Moreover  the total number of stochastic gradient evaluations O(cid:0) log2(d/p)L2

(cid:1).

δ2

The proof of Theorem 1 immediately follows from Lemma 3.1 and Lemma 3.2 below.
Lemma 3.1 (Neon2online
weak ).
weak (f  x0  δ)
satisﬁes If λmin(∇2f (x0)) ≤ −δ  then with probability at least 2/3  v (cid:54)= ⊥ and v(cid:62)∇2f (x0)v ≤
− 3
4 δ.

In the same setting as Theorem 1  the output v = Neon2online

Proof sketch of Lemma 3.1. We explain why Neon2online
weak works as follows. Starting from a ran-
domly perturbed point x1 = x0 + ξ  it keeps updating xt+1 ← xt − η (∇fi(xt) − ∇fi(x0)) for
some random index i ∈ [n]  and stops either when T iterations are reached  or when (cid:107)xt+1−x0(cid:107)2 >
r. Therefore  we have (cid:107)xt − x0(cid:107)2 ≤ r throughout the iterations  and thus can approximate
∇2fi(x0)(xt − x0) using ∇fi(xt) − ∇fi(x0)  up to error O(r2). This is a small term based on
our choice of r.

Ignoring the error term  our updates look like xt+1 − x0 = (cid:0)I − η∇2fi(x0)(cid:1)(xt − x0). This
(cid:1) iterations  where λ =
ysis of Oja’s algorithm [7]  one can conclude that after T1 = Θ(cid:0) log r

is exactly the same as Oja’s algorithm [22] which is known to approximately compute the min-
imum eigenvector of ∇2f (x0) = 1
i=1 fi(x0). Using the recent optimal convergence anal-
max{0 −λmin(∇2f (x0))}  then we not only have that (cid:107)xt+1 − x0(cid:107)2 is blown up  but also it aligns
well with the minimum eigenvector of ∇2f (x0). In other words  if λ ≥ δ  then the algorithm must
stop before T .
Finally  one has to carefully argue that the error does not blow up in this iterative process. We defer
(cid:3)
the proof details to Appendix B.3.

(cid:80)n

σ
ηλ

n

Our Lemma 3.2 below tells us we can verify if the output v of Neon2online
additive δ
this procedure as Neon2online in Algorithm 2.

weak is indeed correct (up to
4)  so we can boost the success probability to 1− p. For completeness’ sake  we summarize

6

Lemma 3.2 (veriﬁcation). In the same setting as Theorem 1  let vectors x  v ∈ Rd. If i1  . . .   im ∈R
[n] and deﬁne

Then  if (cid:107)v(cid:107) ≤ δ

8L2

and m = Θ( L2 log 1/p

z = 1
m

(cid:80)m
j=1 v(cid:62)(∇fij (x + v) − ∇fij (x))
(cid:12)(cid:12)(cid:12) z(cid:107)v(cid:107)2

)  with probability at least 1 − p 
− v(cid:62)∇2f (x)v

(cid:12)(cid:12)(cid:12) ≤ δ

(cid:107)v(cid:107)2

4 .

δ2

2

2

The simple proof of Lemma 3.2 can be found in Section B.4.
4 Neon2 in the Deterministic Setting

.

√
δ

√
L

1 log(d/p)

Algorithm 3 Neon2det(f  x0  δ  p)
Input: A function f  vector x0  negative curvature target δ > 0  failure probability p ∈ (0  1].
1: T ← C2
2: ξ ← Gaussian random vector with norm σ;
3: x1 ← x0 + ξ. y1 ← ξ  y0 ← 0
4: for t ← 1 to T do
yt+1 = 2M(yt) − yt−1;
5:
xt+1 = x0 + yt+1 − M(yt).
6:
if (cid:107)xt+1 − x0(cid:107)2 ≥ r then return xt+1−x0
7:
(cid:107)xt+1−x0(cid:107)2
8: end for
9: return ⊥.

L (∇f (x0 + y) − ∇f (x0)) +(cid:0)1 − 3δ

(cid:5) σ := (d/p)−2C1

(cid:5) M(y) := − 1

(cid:5) for sufﬁciently large constant C1.

(cid:5) r := (d/p)C1 σ

(cid:1) y

T 4L2

4L

.

δ

We propose Neon2det formally in Algorithm 3 and prove:
Theorem 3 (Neon2det). Let f (x) be a function that is L-smooth and L2-second-order smooth. For
every point x0 ∈ Rd  every δ > 0  every p ∈ (0  1]  the output v = Neon2det(f  x0  δ  p) satisﬁes
that  with probability at least 1 − p:
1. If v = ⊥  then ∇2f (x0) (cid:23) −δI.
2. If v (cid:54)= ⊥  then (cid:107)v(cid:107)2 = 1 and v(cid:62)∇2f (x0)v ≤ − δ
2 .

Moreover  the total number full gradient evaluations is O(cid:0) log2(d/p)
L∇2f (x0) +(cid:0)1 − 3δ
(cid:1) I. We immediately notice that
4   L(cid:3) are mapped to the eigenvalues of M in [−1  1]  and
• all eigenvalues of ∇2f (x0) in(cid:2)−3δ
(cid:1)  if we compute xT +1 = x0 + MT ξ for some random vector ξ 
Therefore  as long as T ≥ (cid:101)Ω(cid:0) L
(cid:1) if
The ﬁrst issue is that  the degree T of this matrix polynomial MT can be reduced to T =(cid:101)Ω(cid:0)√

by the theory of power method  xT +1 − x0 must be a negative-curvature direction of ∇2f (x0) with
value ≤ 1

Proof sketch of Theorem 3. We explain the high-level intuition of Neon2det and the proof of
Theorem 3 as follows. Deﬁne M = − 1

• any eigenvalue of ∇2f (x0) smaller than −δ is mapped to eigenvalue of M greater than 1 + δ
4L.

2 δ. There are two issues with this approach.

(cid:1).

√

√

4L

L

δ

δ

L√
δ

the so-called Chebyshev polynomial is used.

Claim 4.1. Let Tt(x) be the t-th Chebyshev polynomial of the ﬁrst kind  deﬁned as:

T0(x) := 1 

T1(x) := x 
then Tt(x) satisﬁes (see Trefethen [30]):

(cid:26) cos(n arccos(x)) ∈ [−1  1]
(cid:2)(cid:0)x − √
+(cid:0)x +

x2 − 1(cid:1)n

1
2

Tt(x) =

x2 − 1(cid:1)n(cid:3)

√

Tn+1(x) := 2x · Tn(x) − Tn−1(x)

if x ∈ [−1  1];
if x > 1.

√

Since Tt(x) stays between [−1  1] when x ∈ [−1  1]  and grows to ≈ (1 +
can use TT (M) in replacement of MT . Then  any eigenvalue of M that is above 1 + δ

x2 − 1)t for x ≥ 1  we
4L shall grow

7

in a speed like (1 +(cid:112)δ/L)T   so it sufﬁces to choose T ≥(cid:101)Ω(cid:0)√

applying the power method  so in Neon2det we wish to compute xt+1 ≈ x0 + Tt (M) ξ.
The second issue is that  since we cannot compute Hessian-vector products  we have to use the
gradient difference to approximate it; that is  we can only use M(y) to approximate My where

L√
σ

(cid:1). This is quadratically faster than
(cid:18)

(cid:19)

M(y) := − 1
L

(∇f (x0 + y) − ∇f (x0)) +

1 − 3δ
4L

y .

How does error propagate if we compute Tt (M) ξ by replacing M with M? Note that this is a very
non-trivial question  because the coefﬁcients of the polynomial Tt(x) is as large as 2O(t).
It turns out  the way that error propagates depends on how the Chebyshev polynomial is calculated.
If the so-called backward recurrence formula is used  namely 

y0 = 0 

y1 = ξ 

yt = 2M(yt−1) − yt−2

and setting xT +1 = x0 + yT +1 − M(yT )  then this xT +1 is sufﬁciently close to the exact value
x0 + Tt (M) ξ. This is known as the stability theory of computing Chebyshev polynomials  and is
(cid:3)
proved in [6]. We defer all the proof details to Appendix C.2.

5 Neon2 in the Finite-Sum Setting
Let us recall how the shift-and-invert (SI) approach [26] solves the minimum eigenvector problem.
Given matrix A = ∇2f (x0) ∈ Rd×d and suppose its eigenvalues are −L ≤ λ1 ≤ ··· ≤ λd ≤ L.
At a high level  the SI approach
• chooses λ = δ − λ1  5
• deﬁnes positive deﬁnite matrix B = (λI + A)−1  and
• applies power method for a logarithmic number of rounds to B to ﬁnd its approximate maximum
One can show that this unit vector v satisﬁes λ1 ≤ v(cid:62)Av ≤ λ1 + O(δ) [13].
To apply power method to B  one needs to compute matrix inversion By = (λI + A)−1y for arbi-
trary vectors y ∈ Rd. The stability of SI ensures that it sufﬁces to compute By to some sufﬁciently
high accuracy.7
One efﬁcient way to compute By to such high accuracy is by expressing A in a ﬁnite-sum form
and then adopt convex optimization [13]. We call this approach ﬁnite-sum SI. Consider a convex
quadratic function that is of a ﬁnite sum of non-convex functions:

eigenvector v.6

g(z) :=

1
2

z(cid:62)(λI + A)z + y(cid:62)z =

1
n

z(cid:62)(λI + ∇2fi(x0))z + y(cid:62)z

gi(z) .

(cid:17)

n(cid:88)

i=1

=:

1
n

n(cid:88)

(cid:16) 1

2

i=1

momentum  and ﬁnds z using (cid:101)O(n + n3/4(cid:112)L/δ) computations of stochastic gradients.8 Whenever

Now  computing By is equivalent to minimizing g(z)  and one can use a stochastic ﬁrst-order
method to minimize it.
One such method is KatyushaX  which directly accelerates the so-called SVRG method using
a stochastic gradient ∇gi(z) = (λI + ∇2fi(x0))z + y is needed at some point z ∈ Rd for some
random i ∈ [n]  instead of evaluating it exactly (which require a Hessian-vector product)  we use
∇fi(x0 + z) − ∇fi(x0) to approximate ∇2fi(x0) · z. We call this method Neon2ﬁnite.

5The precise SI approach needs to binary search λ because λ1 is unknown.
6More precisely  applying power method for O(log(d/p)) rounds  one can ﬁnd a unit vector v such that
10 λmax(B) with probability at least 1 − p. One can also prove that this vector v satisﬁes λ1 ≤
7More precisely  if sufﬁces to compute w ∈ Rd so that (cid:107)w − By(cid:107) ≤ ε(cid:107)y(cid:107)  in a time complexity that

v(cid:62)Bv ≥ 9
v(cid:62)Av ≤ λ1 + O(δ).

polynomially depends on log 1

ε [5  13].

8Shalev-Shwartz [29] ﬁrst discovered that one can apply SVRG to minimize sum-of-nonconvex functions.
It was also observed that applying APPA/Catalyst reductions to SVRG one can achieve accelerated convergence
rates [13  29]  and this approach is commonly known as AccSVRG. However  AccSVRG requires some careful
parameter tuning of its inner loops  and thus is a logarithmic-factor slower than KatyushaX and also less
practical [3].

8

Of course  one needs to show that KatyushaX is stable to noise. Using similar techniques as the
previous two sections  one can show that the error term is proportional to O((cid:107)z(cid:107)2
2)  and thus as long
as we bound the norm of z is bounded (just like we did in the previous two sections)  this should not
affect the performance of the algorithm. We decide to ignore the detailed theoretical proof of this
result  because it will complicate this paper.
Theorem 5 (Neon2ﬁnite).
i=1 fi(x) where each fi is L-smooth and L2-
second-order smooth. For every point x0 ∈ Rd  every δ > 0  every p ∈ (0  1]  the output
v = Neon2ﬁnite(f  x0  δ  p) satisﬁes that  with probability at least 1 − p:
1. If v = ⊥  then ∇2f (x0) (cid:23) −δI.
2. If v (cid:54)= ⊥  then (cid:107)v(cid:107)2 = 1 and v(cid:62)∇2f (x0)v ≤ − δ
2 .

Moreover  the total number stochastic gradient evaluations is (cid:101)O(cid:0)n + n3/4

(cid:1)  where the (cid:101)O notion

Let f (x) = 1
n

(cid:80)n

√
L√

δ

hides logarithmic factors in d  1/p and L/δ.
6 Applications of Neon2
We show how Neon2 can be applied to existing algorithms such as SGD  GD  SCSG  SVRG  Natasha2 
CDHS. Unfortunately  we are unaware of a generic statement for applying Neon2 to any algorithm.
Therefore  we have to prove them individually.9
Throughout this section  we assume that some starting vector x0 ∈ Rd and upper bound ∆f is given
to the algorithm  and it satisﬁes f (x0)− minx{f (x)} ≤ ∆f . This is only for the purpose of proving
theoretical bounds. Since ∆f only appears in specifying the number of iterations  in practice  one
can run enough number of iterations and then halt the algorithm  without knowing ∆f .
6.1 Applying Neon2 to SGD and GD
To apply Neon2 to turn SGD into an algorithm ﬁnding approximate local minima  we propose the
following process Neon2+SGD (see Algorithm 4). In each iteration t  it ﬁrst applies SGD with mini-
ε2 ) (see Line 4). Then  if SGD ﬁnds a point with small gradient  we apply Neon2online
batch size O( 1
to decide if it has a negative curvature  if so  then we move in the direction of the negative curvature
(see Line 10). We have the following theorem:
Theorem 7a. With probability at least 1 − p  Neon2+SGD outputs an (ε  δ)-approximate local

(cid:1) + L2
minimum in gradient complexity T = (cid:101)O
Corollary 6.1. Treating ∆f  V  L  L2 as constants  we have T = (cid:101)O(cid:0) 1

ε2 + 1)(cid:0) L2

δ3 + L∆f
2∆f

L2

2∆f
δ3

(cid:1).

(cid:16)

ε4 + 1

ε2δ3 + 1
δ5

(cid:17)

( V

ε2

δ2

.

One can similarly (and more easily) give an algorithm Neon2+GD  which is the same as Neon2+SGD
except that the mini-batch SGD is replaced with a full gradient descent  and the use of Neon2online
is replaced with Neon2det. We have the following theorem:
Theorem 7c. With probability at least 1 − p  Neon2+GD outputs an (ε  δ)-approximate local mini-

mum using (cid:101)O

(cid:16) L∆f

(cid:17)

ε2 + L1/2

δ1/2

L2

2∆f
δ3

full gradient computations.

We only prove Theorem 7a in Appendix D and the proof of Theorem 7c is only simpler.
6.2 Other Applications
Due to space limitation  we defer the applications to Natasha2  CDHS  and SCSG to Appendix A.
At a high level  the applications to Natasha2 and CDHS are trivial because NC-search was already
a subroutine required by both algorithms  so one can directly replace them with Neon2 of this pa-
per. The application to SCSG is less non-trivial  because one has to additionally take care of some
probabilistic behavior from SCSG.
Acknowledgements
We would like to thank Tianbao Yang and Yi Xu for helpful feedbacks on this manuscript. This
work was done when Yuanzhi Li was a summer intern at Microsoft Research in 2017.

9This is because stationary-point ﬁnding algorithms have somewhat different guarantees. For instance  in
mini-batch SGD we have f (xt) − E[f (xt+1)] ≥ Ω((cid:107)∇f (xt)(cid:107)2) but in SCSG we have f (xt) − E[f (xt+1)] ≥
Ω(E[(cid:107)∇f (xt+1)(cid:107)2]).

9

References
[1] Naman Agarwal  Zeyuan Allen-Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Finding

Approximate Local Minima for Nonconvex Optimization in Linear Time. In STOC  2017.

[2] Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD.

2018.

In NeurIPS 

[3] Zeyuan Allen-Zhu. Katyusha X: Practical Momentum Method for Stochastic Sum-of-

Nonconvex Optimization. In ICML  2018.

[4] Zeyuan Allen-Zhu and Elad Hazan. Variance Reduction for Faster Non-Convex Optimization.

In ICML  2016.

[5] Zeyuan Allen-Zhu and Yuanzhi Li. LazySVD: Even Faster SVD Decomposition Yet Without

Agonizing Pain. In NeurIPS  2016.

[6] Zeyuan Allen-Zhu and Yuanzhi Li. Faster Principal Component Regression and Stable Matrix

Chebyshev Approximation. In ICML  2017.

[7] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the Compressed Leader: Faster Online Learning of

Eigenvectors and Faster MMWU. In ICML  2017.

[8] Yair Carmon  John C. Duchi  Oliver Hinder  and Aaron Sidford. Accelerated Methods for

Non-Convex Optimization. ArXiv e-prints  abs/1611.00756  November 2016.

[9] Yair Carmon  Oliver Hinder  John C. Duchi  and Aaron Sidford. ”Convex Until Proven Guilty”:
Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions. In ICML  2017.

[10] Anna Choromanska  Mikael Henaff  Michael Mathieu  G´erard Ben Arous  and Yann LeCun.

The loss surfaces of multilayer networks. In AISTATS  2015.

[11] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In NeurIPS  pages 2933–2941  2014.

[12] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A Fast Incremental Gradient

Method With Support for Non-Strongly Convex Composite Objectives. In NeurIPS  2014.

[13] Dan Garber  Elad Hazan  Chi Jin  Sham M. Kakade  Cameron Musco  Praneeth Netrapalli 
and Aaron Sidford. Robust shift-and-invert preconditioning: Faster and more sample efﬁcient
algorithms for eigenvector computation. In ICML  2016.

[14] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online
stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference
on Learning Theory  COLT 2015  2015.

[15] I. J. Goodfellow  O. Vinyals  and A. M. Saxe. Qualitatively characterizing neural network

optimization problems. ArXiv e-prints  December 2014.

[16] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to Escape

Saddle Points Efﬁciently. In ICML  2017.

[17] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems  NeurIPS 2013  pages
315–323  2013.

[18] Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear
differential and integral operators. Journal of Research of the National Bureau of Standards 
45(4)  1950.

[19] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Nonconvex Finite-Sum Optimization

Via SCSG Methods. In NeurIPS  2017.

[20] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course  vol-

ume I. Kluwer Academic Publishers  2004. ISBN 1402075537.

10

[21] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1):177–205  2006.

[22] Erkki Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathemat-

ical biology  15(3):267–273  1982.

[23] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation  6(1):

147–160  1994.

[24] Sashank J. Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic

variance reduction for nonconvex optimization. In ICML  2016.

[25] Sashank J Reddi  Manzil Zaheer  Suvrit Sra  Barnabas Poczos  Francis Bach  Ruslan Salakhut-
dinov  and Alexander J Smola. A generic approach for escaping saddle points. ArXiv e-prints 
abs/1709.01434  September 2017.

[26] Youcef Saad. Numerical methods for large eigenvalue problems. Manchester University Press 

1992.

[27] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. ArXiv e-prints  abs/1309.2388  September 2013.

[28] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.

Neural computation  14(7):1723–1738  2002.

[29] Shai Shalev-Shwartz. SDCA without Duality  Regularization  and Individual Convexity. In

ICML  2016.

[30] Lloyd N. Trefethen. Approximation Theory and Approximation Practice. SIAM  2013.

[31] Yi Xu and Tianbao Yang. First-order Stochastic Algorithms for Escaping From Saddle Points

in Almost Linear Time. ArXiv e-prints  abs/1711.01944  November 2017.

11

,Zeyuan Allen-Zhu
Yuanzhi Li