2018,GIANT: Globally Improved Approximate Newton Method for Distributed Optimization,For distributed computing environment  we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration  each worker locally finds an Approximate NewTon (ANT) direction  which is sent to the main driver. The main driver  then  averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically  we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further  and in sharp contrast with many existing distributed Newton-type methods  as well as popular first-order methods  a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and  empirically  demonstrate the superior performance of GIANT.,GIANT: Globally Improved Approximate Newton

Method for Distributed Optimization

Peng Xu

Stanford University

pengxu@stanford.edu

Shusen Wang

Stevens Institute of Technology
shusen.wang@stevens.edu

Farbod Roosta-Khorasani
University of Queensland
fred.roosta@uq.edu.au

Michael W. Mahoney

University of California at Berkeley
mmahoney@stat.berkeley.edu

Abstract

For distributed computing environment  we consider the empirical risk minimiza-
tion problem and propose a distributed and communication-efﬁcient Newton-type
optimization method. At every iteration  each worker locally ﬁnds an Approximate
NewTon (ANT) direction  which is sent to the main driver. The main driver  then 
averages all the ANT directions received from workers to form a Globally Improved
ANT (GIANT) direction. GIANT is highly communication efﬁcient and naturally
exploits the trade-offs between local computations and global communications
in that more local computations result in fewer overall rounds of communica-
tions. Theoretically  we show that GIANT enjoys an improved convergence rate as
compared with ﬁrst-order methods and existing distributed Newton-type methods.
Further  and in sharp contrast with many existing distributed Newton-type methods 
as well as popular ﬁrst-order methods  a highly advantageous practical feature
of GIANT is that it only involves one tuning parameter. We conduct large-scale
experiments on a computer cluster and  empirically  demonstrate the superior
performance of GIANT.

1

Introduction

The large-scale nature of many modern “big-data” problems  arising routinely in science  engineering 
ﬁnancial markets  Internet and social media  etc.  poses signiﬁcant computational as well as storage
challenges for machine learning procedures. For example  the scale of data gathered in many
applications nowadays typically exceeds the memory capacity of a single machine  which  in turn 
makes learning from data ever more challenging. In this light  several modern parallel (or distributed)
computing architectures  e.g.  MapReduce [4]  Apache Spark [44  19]  GraphLab [14]  and Parameter
Server [11]  have been designed to operate on and learn from data at massive scales. Despite the
fact that  when compared to a single machine  distributed systems tremendously reduce the storage
and (local) computational costs  the inevitable cost of communications across the network can often
be the bottleneck of distributed computations. As a result  designing methods which can strike an
appropriate balance between the cost of computations and that of communications are increasingly
desired.
The desire to reduce communication costs is even more pronounced in the federated learning
framework [8  9  1  18  37]. Similarly to typical settings of distributed computing  federated learning
assumes data are distributed over a network across nodes that enjoy reasonable computational
resources  e.g.  mobile phones  wearable devices  and smart homes. However  the network has

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

severely limited bandwidth and high latency. As a result  it is imperative to reduce the communications
between the center and a node or between two nodes. In such settings  the preferred methods are those
which can perform expensive local computations with the aim of reducing the overall communications
across the network.
Optimization algorithms designed for distributed setting are abundantly found in the literature. First-
order methods  i.e  those that rely solely on gradient information  are often embarrassingly parallel
and easy to implement. Examples of such methods include distributed variants of stochastic gradient
descent (SGD) [17  27  47]  accelerated SGD [35]  variance reduction SGD [10  28]  stochastic
coordinate descent methods [5  13  20  31] and dual coordinate ascent algorithms [30  43  46].
The common denominator in all of these methods is that they signiﬁcantly reduce the amount of
local computation. But this blessing comes with an inevitable curse that they  in turn  may require
a far greater number of iterations and hence  incur more communications overall. Indeed  as a
result of their highly iterative nature  many of these ﬁrst-order methods require several rounds of
communications and  potentially  synchronizations in every iteration  and they must do so for many
iterations. In a computer cluster  due to limitations on the network’s bandwidth and latency and
software system overhead  communications across the nodes can oftentimes be the critical bottleneck
for the distributed optimization. Such overheads are increasingly exacerbated by the growing number
of compute nodes in the network  limiting the scalability of any distributed optimization method that
requires many communication-intensive iterations.
To remedy such drawbacks of high number of iter-
ations for distributed optimization  communication-
efﬁcient second-order methods  i.e.  those that 
in addition to the gradient 
incorporate curva-
ture information  have also been recently consid-
ered [16  36  29  45  7  15  38]; see also Section 1.1.
The common feature in all of these methods is that
they intend to increase the local computations with
the aim of reducing the overall iterations  and hence 
lowering the communications.
In other words 
these methods are designed to perform as much
local computation as possible before making any
communications across the network. Pursuing similar objectives  in this paper  we propose a Globally
Improved Approximate NewTon (GIANT) method and establish its improved theoretical convergence
properties as compared with other similar second-order methods. We also showcase the superior
empirical performance of GIANT through several numerical experiments.
The rest of this paper is organized as follows. Section 1.1 brieﬂy reviews prior works most closely
related to this paper. Section 1.2 gives a summary of our main contributions. The formal description
of the distributed empirical risk minimization problem is given in Section 2  followed by the derivation
of various steps of GIANT in Section 3. Section 4 presents the theoretical guarantees. The most
commonly used notation is listed in Table 1. Due to the page limit  the readers can refer to the long
version [41]; Section 5 provides a summary of our experiments. The proofs can be found in the long
version [41].

Deﬁnition
total number of samples
number of features (attributes)
number of partitions
objective function
regularization parameter
the variable at iteration t
the variable that minimizes f
some condition number

Table 1: Commonly used notation.

n
d
m
f
γ
wt
w(cid:63)
κ

Notation

Table 2: The number of communications (proportional to the number of iterations) required for the
ridge regression problem. Here κ is the condition number of the Hessian matrix  µ is the matrix
coherence  and ˜O conceals constants (analogous to µ) and logarithmic factors.

Method

GIANT [this work]

DiSCO [45]
DANE [36]
AIDE [29]
CoCoA [38]

AGD

log(n/µdm)

#Iterations

n3/4 + κ1/2m1/4

t = O(cid:16) log(dκ/E)
t = ˜O(cid:16) dκ1/2m3/4
t = ˜O(cid:16) κ2m
t = ˜O(cid:16) κ1/2m1/4
t = O(cid:16)(cid:0)n + 1
t = O(cid:16)

(cid:17)
(cid:17)
(cid:1) log nE
(cid:17)

n1/4
n log 1E

n1/4

γ

log 1E

(cid:17)
(cid:17)

κ1/2 log dE

2

(cid:17)

log 1E

Metric

(cid:107)wt − w(cid:63)(cid:107)2 ≤ E
f (wt) − f (w(cid:63)) ≤ E
f (wt) − f (w(cid:63)) ≤ E
f (wt) − f (w(cid:63)) ≤ E
f (wt) − f (w(cid:63)) ≤ E
(cid:107)wt − w(cid:63)(cid:107)2 ≤ E

1.1 Related Work

1

2 + γ(cid:107)w(cid:107)2

Among the existing distributed second-order optimization methods  the most notably are DANE [36] 
AIDE [29]  and DiSCO [45]. Another similar method is CoCoA [7  15  38]  which is analogous to
second-order methods in that it involves sub-problems which are local quadratic approximations
to the dual objective function. However  despite the fact that CoCoA makes use of the smoothness
condition  it does not exploit any explicit second-order information.
We can evaluate the theoretical properties the above-mentioned methods in light of comparison
with optimal ﬁrst-order methods  i.e.  accelerated gradient descent (AGD) methods [22  23]. It is
because AGD methods are mostly embarrassingly parallel and can be regarded as the baseline for
distributed optimization. Recall that AGD methods  being optimal in worst-case analysis sense [21] 

are guaranteed to convergence to E-precision in O(√κ log 1E ) iterations [23]  where κ can be thought

n(cid:107)Xw− y(cid:107)2

of as the condition number of the problem. Each iteration of AGD has two rounds of communications—
broadcast or aggregation of a vector.
In Table 2  we compare the communication costs with other methods for the ridge regression
2.1 The communication cost of GIANT has a mere logarithmic
problem: minw
dependence on the condition number κ; in contrast  the other methods have at least a square root
dependence on κ. Even if κ is assumed to be small  say κ = O(√n)  which was made by [45] 
GIANT’s bound is better than the compared methods regarding the dependence on the number of
partitions  m.
Our GIANT method is motivated
by the subsampled Newton method
[33  42  25]. Later on  we realized
that a similar idea has been pro-
posed by DANE [36]; GIANT and
DANE are identical for quadratic
programming; they are different for
the general convex problems. Nev-
ertheless  we show better conver-
gence bounds than DANE  even for
quadratic programming. Our im-
provement over DANE is obtained
by better bounds the Hessian matrix
approximation and better analysis
of convex optimization.
GIANT also bears a resemblance
to FADL [16]  but we show better
convergence bounds. Mahajan et
al. [16] has conducted comprehen-
sive empirical comparisons among
many distributed computing meth-
ods and concluded that the local
quadratic approximation  which is
very similar to GIANT  is the ﬁnal
method which they recommended.

Figure 1: One iteration of GIANT. Here X and y are respec-
tively the features and lables; Xi and yi denotes the blocks of
X and y  respectively. Each one-to-all operation is a Broad-
cast and each all-to-one operation is a Reduce.

1.2 Contributions

In this paper  we consider the problem of empirical risk minimization involving smooth and strongly
convex objective function (which is the same setting considered in prior works of DANE  AIDE  and
DiSCO). In this context  we propose a Globally Improved Approximate NewTon (GIANT) method
and establish its theoretical and empirical properties as follows.
• For quadratic objectives  we establish global convergence of GIANT. To attain a ﬁxed precision 
the number of iterations of GIANT (which is proportional to the communication complexity) has

1As for general convex problems  it is very hard to present the comparison in an easily understanding way.

This is why we do not compare the convergence for the general convex optimization.

3

Approximate	Newton	(ANT)	DirectionGlobally	Improved	Approximate	Newton	(GIANT)	Direction!"#"$% "⋯!(#($% (⋯!)#)$% )!"#"*+% "⋯!(#(*+% (⋯!)#)*+% )gt=Mwt+mXi=1gt iwt+1=wt1mmXi=1˜pt i7˜pt=1mmXi=1˜pt iwt+1=wt˜pt24˜pt=1mmXi=1˜pt iwt+1=wt˜pt24gt=wt+mXi=1gt igt=mXi=1gt i2a mere logarithmic dependence on the condition number. In contrast  the prior works have at least
square root dependence. In fact  for quadratic problems  GIANT and DANE [36] can be shown to be
identical. In this light  for such problems  our work improves upon the convergence of DANE.
• For more general problems  GIANT has linear-quadratic convergence in the vicinity of the optimal
solution  which we refer to as “local convergence”.2 The advantage of GIANT mainly manifests in
big-data regimes where there are many data points available. In other words  when the number of
data points is much larger than the number of features  the theoretical convergence of GIANT enjoys
signiﬁcant improvement over other similar methods.
• In addition to theoretical features  GIANT also exhibits desirable practical advantages. For example 
in sharp contrast with many existing distributed Newton-type methods  as well as popular ﬁrst-order
methods  GIANT only involves one tuning parameter  i.e.  the maximal iterations of its sub-problem
solvers  which makes GIANT easy to implement in practice. Furthermore  our experiments on a
computer cluster show that GIANT consistently outperforms AGD  L-BFGS  and DANE.

2 Problem Formulation

In this paper  we consider the distributed variant of empirical risk minimization  a supervised-
learning problem arising very often in machine learning and data analysis [34]. More speciﬁcally 
let x1 ···   xn ∈ Rd be the input feature vectors and y1 ···   yn ∈ R be the corresponding response.
The goal of supervised learning is to compute a model from the training data  which can be achieved
by minimizing an empirical risk function  i.e. 

w∈Rd (cid:26)f (w) (cid:44) 1

min

n

(cid:96)j(wT xj) +

n(cid:88)j=1

2(cid:27) 
γ
2(cid:107)w(cid:107)2

(1)

where (cid:96)j : R (cid:55)→ R is convex  twice differentiable  and smooth. We further assume that f is strongly
convex  which in turn  implies the uniqueness of the minimizer of (1)  denoted throughout the text
by w(cid:63). Note that yj is implicitly captured by (cid:96)j. Examples of the loss function  (cid:96)j  appearing in (1)
include

linear regression:
logistic regression:

(cid:96)j(zj) = 1
(cid:96)j(zj) = log(1 + e

2 (zj − yj)2 

−zj yj ).

Suppose the n feature vectors and loss functions (x1  (cid:96)1) ···   (xn  (cid:96)n) are partitioned among m
worker machines. Let s (cid:44) n/m be the local sample size. Our theories require s > d; nevertheless 
GIANT empirically works well for s < d.
We consider solving (1) in the regimes where n (cid:29) d. We assume that the data points  {xi}n
i=1 are
partitioned among m machines  with possible overlaps  such that the number of local data is larger
than d. Otherwise  if n (cid:28) d  we can consider the dual problem and partition features. If the dual
problem is also decomposable  smooth  strongly convex  and unconstrained  e.g.  ridge regression 
then our approach directly applies.

3 Algorithm Description

In this section  we present the algorithm derivation and complexity analysis. GIANT is a central-
ized and synchronous method; one iteration of GIANT is depicted in Figure 1. The key idea of
GIANT is avoiding forming of the exact Hessian matrices Ht ∈ Rd×d in order to avoid expensive
communications.

2The second-order methods typically have the local convergence issue. Global convergence of GIANT can
be trivially established by following [32]  however  the convergence rate is not very interesting  as it is worse
than the ﬁrst-order methods.

4

3.1 Gradient and Hessian

GIANT iterations require the exact gradient  which in the t-th iteration  can be written as

gt = ∇f (wt) =

1
n

n(cid:88)j=1

(cid:48)
j(wT
(cid:96)

t xj) xj + γwt ∈ Rd.

(2)

The gradient  gt can be computed  embarrassingly  in parallel. The driver Broadcasts wt to all
the worker machines. Each machine then uses its own {(xj  (cid:96)j)} to compute its local gradient.
Subsequently  the driver performs a Reduce operation to sum up the local gradients and get gt. The
per-iteration communication complexity is ˜O(d) words  where ˜O hides the dependence on m (which
can be m or log m  depending on the network structure).
More speciﬁcally  in the t-th iteration  the Hessian matrix at wt ∈ Rd can be written as

Ht = ∇2f (wt) =

1
n

n(cid:88)j=1

(cid:48)(cid:48)
j (wT
(cid:96)

t xj) · xjxT

j + γId.

(3)

To compute the exact Hessian  the driver must aggregate the m local Hessian matrices (each of size
d × d) by one Reduce operation  which has ˜O(d2) communication complexity and is obviously
impractical when d is thousands. The Hessian approximation developed in this paper has a mere
˜O(d) communication complexity which is the same to the ﬁrst-order methods.
3.2 Approximate NewTon (ANT) Directions
j=1.3 Let Ji be
Assume each worker machine locally holds s random samples drawn from {(xj  (cid:96)j)}n
the set containing the indices of the samples held by the i-th machine  and s = |Ji| denote its size.
Each worker machine can use its local samples to form a local Hessian matrix

1

s (cid:88)j∈Ji

(cid:101)Ht i =

(cid:48)(cid:48)
j (wT
(cid:96)

t xj) · xjxT

j + γId.

To reduce the computational cost  we opt to compute the ANT direction by the conjugate gradient

−1
Clearly  E[(cid:101)Ht i] = Ht. We deﬁne the Approximate NewTon (ANT) direction by ˜pt i = (cid:101)H
t i gt.
The cost of computing the ANT direction ˜pt i in this way  involves O(sd2) time to form the d × d
dense matrix (cid:101)Ht i and O(d3) to invert it.
(CG) method [24]. Let aj =(cid:113)(cid:96)(cid:48)(cid:48)
(4)
and At i ∈ Rs×d contain the rows of At indexed by the set Ji. Using the matrix notation  we can
write the local Hessian matrix as

t xj) · xj ∈ Rd 
1 ;··· ; aT

n ] ∈ Rn×d 

At = [aT

j (wT

s AT

t iAt i + γId.

(cid:101)Ht i = 1
t iAt i + γId(cid:1) p = gt
(cid:0) 1

Employing CG  it is unnecessary to explicitly form (cid:101)Ht i. Indeed  one can simply approximately solve
round of GIANT  the local computational cost of a worker machine is O(cid:0)q · nnz(At i)(cid:1)  where q is

in a “Hessian-free” manner  i.e.  by employing only Hessian-vector products in CG iterations. In each

the number of CG iterations speciﬁed by the users and typically set to tens.

s AT

(6)

(5)

3If the samples themselves are i.i.d. drawn from some distribution  then a data-independent partition is

equivalent to uniform sampling. Otherwise  the system can Shufﬂe the data.

5

1
m

m(cid:88)i=1

−1

1
m

m(cid:88)i=1 (cid:101)H

3.3 Globally Improved ANT (GIANT) Direction

Using random matrix concentration  we can show that for sufﬁciently large s  the local Hessian matrix

ANT (GIANT) direction is deﬁned as

(cid:101)Ht i is a spectral approximation to Ht. Now let ˜pt i be an ANT direction. The Globally Improved

−1
t gt.

(7)

˜pt =

˜pt i =

Hessian Ht is the arithmetic mean deﬁned as Ht (cid:44) 1
is  the “information” is spread-out rather than concentrated to a small fraction of samples  then
the harmonic mean and the arithmetic mean are very close to each other  and thereby the GIANT

t i gt = (cid:101)H
m(cid:80)m
Interestingly  here (cid:101)Ht is the harmonic mean deﬁned as (cid:101)Ht (cid:44) ( 1
i=1 (cid:101)H
m(cid:80)m
i=1 (cid:101)Ht i. If the data is incoherent  that
direction ˜pt = (cid:101)H−1gt very well approximates the true Newton direction H−1gt. This is the intuition
The motivation of using the harmonic mean  (cid:101)Ht  to approximate the arithmetic mean (the true Hessian
matrix)  Ht  is the communication cost. Computing the arithmetic mean Ht (cid:44) 1
i=1 (cid:101)Ht i would
require the communication of d × d matrices which is very expensive. In contrast  computing ˜pt
merely requires the communication of d-dimensional vectors.

−1
t i )−1  whereas the true

of our global improvement.

m(cid:80)m

3.4 Time and Communication Complexities

For each worker machine  the per-iteration time complexity is O(sdq)  where s is the local sample
size and q is the number of CG iterations for (approximately) solving (6). (See Proposition 5 for the
setting of q.) If the feature matrix X ∈ Rn×d has a sparsity of  = nnz(X)/(nd) < 1  the expected
per-iteration time complexity is then O(sdq).
Each iteration of GIANT has four rounds of communications: two Broadcast for sending and two
Reduce for aggregating some d-dimensional vector. If the communication is in a tree fashion  the
per-iteration communication complexity is then ˜O(d) words  where ˜O hides the factor involving
m which can be m or log m. In contrast  the naive Newton’s method has ˜O(d2) communication
complexity  because the system sends and receives d × d Hessian matrices.

4 Theoretical Analysis

In this section  we formally present the convergence guarantees of GIANT. Section 4.1 focuses on
quadratic loss and treats the global convergence of GIANT. This is then followed by local convergence
properties of GIANT for more general non-quadratic loss in Section 4.2. For the results of Sections 4.1
and 4.2  we require that the local linear system to obtain the local Newton direction is solved exactly.
Section 4.3 then relaxes this requirement to allow for inexactness in the solution  and establishes
similar convergence rates as those of exact variants.
For our analysis here  we frequently make use of the notion of matrix row coherence  deﬁned as
follows. Such a notation has been used in compressed sensing [3]  matrix completion [2]  and
randomized linear algebra [6  40  39].
Deﬁnition 1 (Coherence). Let A ∈ Rn×d be any matrix and U ∈ Rn×d be its column orthonormal
bases. The row coherence of A is µ(A) = n
Remark 1. Our work assumes At ∈ Rn×d  which is deﬁned in (4)  is incoherent  namely µ(At) is
small. The prior works  DANE  AIDE  and DiSCO  did not use the notation of incoherence; instead 
j is upper bounded for all j ∈ [n] and wt ∈ Rd  where
they assume ∇2
aj ∈ Rd is the j-th row of At. Such an assumption is different from but has similar implication as
our incoherence assumption; under either of the two assumptions  it can be shown that the Hessian
matrix can be approximated using a subset of samples selected uniformly at random.

wlj(wT xj)|w=wt = ajaT

d maxj (cid:107)uj(cid:107)2

d ].
2 ∈ [1  n

6

4.1 Quadratic Loss
In this section  we consider a special case of (1) with (cid:96)i(z) = (z − yi)2/2  i.e.  the quadratic
optimization problems:
(8)
f (w) = 1
The Hessian matrix is given as ∇2f (w) = 1
n XT X + γId  which does not depend on w. Theorem 1
describes the convergence of the error in the iterates  i.e.  ∆t (cid:44) wt − w(cid:63).
Theorem 1. Let µ be the row coherence of X ∈ Rn×d and m be the number of partitions. Assume
the local sample size satisﬁes s ≥ 3µd
for some η  δ ∈ (0  1). It holds with probability 1 − δ
that

2n(cid:13)(cid:13)Xw − y(cid:13)(cid:13)2

2(cid:107)w(cid:107)2
2.

2 + γ

δ

η2 log md
(cid:107)∆t(cid:107)2 ≤ αt √κ(cid:107)∆0(cid:107)2 

n XT X + γId.

where α = η√
Remark 2. The theorem can be interpreted in the this way. Assume the total number of samples  n 
is at least 3µdm log(md). Then

m + η2 and κ is the condition number of ∇2f (w) = 1
+(cid:113) 3µd log(md/δ)

(cid:107)∆t(cid:107)2 ≤ (cid:16) 3µdm log(md/δ)

holds with probability at least 1 − δ.
If the total number of samples  n  is substantially bigger than µdm  then GIANT converges in a very
small number of iterations. Furthermore  to reach a ﬁxed precision  say (cid:107)∆t(cid:107)2 ≤ E  the number of
iterations  t  has a mere logarithmic dependence on the condition number  κ.

(cid:17)t √κ(cid:107)∆0(cid:107)2

n

n

4.2 General Smooth Loss

For more general (not necessarily quadratic) but smooth loss  GIANT has linear-quadratic local
convergence  which is formally stated in Theorem 2 and Corollary 3. Let H(cid:63) = ∇2f (w(cid:63)) and
Ht = ∇2f (wt). For this general case  we assume the Hessian is L-Lipschitz  which is a standard
assumption in analyzing second-order methods.
Assumption 1. The Hessian matrix is L-Lipschitz continuous  i.e. (cid:13)(cid:13)∇2f (w) − ∇2f (w(cid:48))(cid:13)(cid:13)2 ≤
L(cid:107)w − w(cid:48)
(cid:107)2  for all w and w(cid:48).
Theorem 2 establishes the linear-quadratic convergence of ∆t (cid:44) wt − w(cid:63). We remind that At ∈
Rn×d is deﬁned in (4) (thus AT
t At + γId = Ht). Note that  unlike Section 4.1  the coherence of At 
denote µt  changes with iterations.
Theorem 2. Let µt ∈ [1  n/d] be the coherence of At and m be the number of partitions. Assume
the local sample size satisﬁes st ≥ 3µtd
for some η  δ ∈ (0  1). Under Assumption 1  it holds
with probability 1 − δ that

log md
δ

η2

(cid:13)(cid:13)∆t+1(cid:13)(cid:13)2 ≤ max(cid:110)α(cid:113) σmax(Ht)

σmin(Ht)(cid:13)(cid:13)∆t(cid:13)(cid:13)2 

2L

2(cid:111) 
σmin(Ht)(cid:13)(cid:13)∆t(cid:13)(cid:13)2

m + η2.

where α = η√
Remark 3. The standard Newton’s method is well known to have local quadratic convergence; the
quadratic term in Theorem 2 is the same as Newton’s method. The quadratic term is caused by the
non-quadritic objective function. The linear term arises from the Hessian approximation. For large
sample size  s  equivalently  small η  the linear term is small.
Note that in Theorem 2 the convergence depends on the condition numbers of the Hessian at every
point. Due to the Lipschitz assumption on the Hessian  it is easy to see that the condition number of
the Hessian in a neighborhood of w(cid:63) is close to κ(H(cid:63)). This simple observation implies Corollary 3 
in which the dependence of the local convergence of GIANT on iterations via Ht is removed.
Assumption 2. Assume wt is close to w(cid:63) in that (cid:107)∆t(cid:107)2 ≤ 3
L · σmin(H(cid:63))  where L is deﬁned in
Assumption 1.
Corollary 3. Under the same setting as Theorem 2 and Assumption 2  it holds with probability 1 − δ
that

where κ is the condition number of the Hessian matrix at w(cid:63).

(cid:13)(cid:13)∆t+1(cid:13)(cid:13)2 ≤ max(cid:110)2α√κ(cid:13)(cid:13)∆t(cid:13)(cid:13)2 

3L

2(cid:111) 
σmin(H(cid:63))(cid:13)(cid:13)∆t(cid:13)(cid:13)2

7

4.3

Inexact Solutions to Local Sub-Problems

In the t-th iteration  the i-th worker locally computes ˜pt i by solving (cid:101)Ht ip = gt  where (cid:101)Ht i is
the i-th local Hessian matrix deﬁned in (5). In high-dimensional problems  say d ≥ 104  the exact
formation of (cid:101)Ht i ∈ Rd×d and its inversion are impractical. Instead  we could employ iterative linear
system solvers  such as CG  to inexactly solve the arising linear system in (6). Let ˜p(cid:48)
t i be an inexact
solution which is close to ˜pt i (cid:44) (cid:101)H
(cid:13)(cid:13)(cid:13)(cid:101)H1/2
t i − ˜pt i(cid:1)(cid:13)(cid:13)(cid:13)2 ≤
t i (cid:0)˜p
m(cid:80)m
i=1 ˜p(cid:48)
for some 0 ∈ (0  1). GIANT then takes ˜p(cid:48)
t i as the approximate Newton direction in
η√
lieu of ˜pt. In this case  as long as 0 is of the same order as
m + η2  the convergence rate of such
inexact variant of GIANT remains similar to the exact algorithm in which the local linear system is
solved exactly. Theorem 4 makes convergence properties of inexact GIANT more explicit.
Theorem 4. Suppose inexact local solution to (6)  denote ˜p(cid:48)

−1
t i gt  in the sense that
0

2(cid:13)(cid:13)(cid:13)(cid:101)H1/2
t i ˜pt i(cid:13)(cid:13)(cid:13)2

t i  satisﬁes (9). Then Theorems 1 and 2

t = 1

(9)

(cid:48)

 

and Corollary 3 all continue to hold with α =(cid:0) η√

m + η2(cid:1) + 0.

Proposition 5 gives conditions to guarantee (9)  which is  in turn  required for Theorem 4.
Proposition 5. To compute an inexact local Newton direction from the sub-problem (6)  suppose
each worker performs

q = log 8
2

0 (cid:14) log

√
√
˜κt−1 ≈

˜κt+1

√

κt−1
2

log 8
2
0

and Ht. Then requirement (9) is satisﬁed.

iterations of CG  initialized at zero  where ˜κt and κt are  respectiely  the condition number of (cid:101)Ht i

5 A Summary of the Empirical Study

Due to the page limit  the experiments are not included in this paper; please refer to the long
version [41] for the experiments. The Apache Spark code is available at https://github.com/
wangshusen/SparkGiant.git. Here we brieﬂy describe our results.
We implement GIANT  Accelerated Gradient Descent (AGD) [23]  Limited memory BFGS (L-
BFGS) [12]  and Distributed Approximate NEwton (DANE) [36] in Scala and Apache Spark [44].
We empirically study the (cid:96)2-regularized logistic regression problem (which satisﬁes our assumptions):

min

w

1
n

n(cid:88)j=1

log(cid:0)1 + exp(−yjxT

j w)(cid:1) +

γ
2(cid:107)w(cid:107)2
2 

(10)

We conduct large-scale experiments on the Cori Supercomputer maintained by NERSC  a Cray XC40
system with 1632 compute nodes  each of which has two 2.3GHz 16-core Haswell processors and
128GB of DRAM. We use up to 375 nodes (12 000 CPU cores).
To apply logistic regression  we use three binary classiﬁcation datasets: MNIST8M (digit “4” versus
“9”  thus n = 2M and d = 784)  Covtype (n = 581K and d = 54)  and Epsilon (n = 500K and
d = 2K)  which are available at the LIBSVM website. We randomly hold 80% for training and the
rest for test. To increase the size of the data  we generate 104 random Fourier features [26] and use
them in lieu of the original features in the logistic regression problem.
For the four methods  we use different settings of the parameters and report the best convergence
curve; we do not count the cost of parameter tuning. (This actually favors AGD and DANE because
they have more tuning parameters than GIANT and L-BFGS.) Using the same amount of wall-clock
time  GIANT consistently converges faster than AGD  DANE  and L-BFGS in terms of both
training objective value and test classiﬁcation error (see the ﬁgures in [41]).
m to be larger than d. But in practice  GIANT
Our theory requires the local sample size s = n
converges even if s is smaller than d. In this set of experiments  we set m = 89  and thus s is about
half of d. Nevetheless  GIANT converges in all of our experiments. Our empirical may imply that the
theoretical sample complexity can be potentially improved.

8

We further use data augmentation (i.e.  adding random noise to the feature vectors) to increase n to
5 and 25 times larger. In this way  the feature matrices are all dense  and the largest feature matrix
we use is about 1TB. As we increase both n and the number of compute nodes  the advantage of
GIANT further increases  which means GIANT is more scalable than the compared methods. It is
because as we increase the number of samples and the number of nodes by the same factor  the local
computation remains the same  but the communication and synchronization costs increase  which
favors the communication-efﬁcient methods; see the ﬁgures and explanations in [41].

6 Conclusions and Future Work

We have proposed GIANT  a practical Newton-type method  for empirical risk minimization in
distributed computing environments. In comparison to similar methods  GIANT has three desirable
advantages. First  GIANT is guaranteed to converge to high precision in a small number of iterations 
provided that the number of training samples  n  is sufﬁciently large  relative to dm  where d is
the number of features and m is the number of partitions. Second  GIANT is very communication
efﬁcient in that each iteration requires four or six rounds of communications  each with a complexity
of merely ˜O(d). Third  in contrast to all other alternates  GIANT is easy to use  as it involves tuning
one parameter. Empirical studies also showed the superior performance of GIANT as compared
several other methods.
GIANT has been developed only for unconstrained problems with smooth and strongly convex
objective function. However  we believe that similar ideas can be naturally extended to projected
Newton for constrained problems  proximal Newton for non-smooth regularization  and trust-region
method for nonconvex problems. However  strong convergence bounds of the extensions appear
nontrivial and will be left for future work.

Acknowledgement

We thank Kimon Fountoulakis  Alex Gittens  Jey Kottalam  Zirui Liu  Hao Ren  Sathiya Selvaraj  Ze-
bang Shen  and Haishan Ye for their helpful suggestions. The four authors would like to acknowledge
ARO  DARPA  Cray  and NSF for providing partial support of this work. Farbod Roosta-Khorasani
was partially supported by the Australian Research Council through a Discovery Early Career
Researcher Award (DE180100923).

References
[1] Keith Bonawitz  Vladimir Ivanov  Ben Kreuter  Antonio Marcedone  H Brendan McMahan  Sarvar Patel 
Daniel Ramage  Aaron Segal  and Karn Seth. Practical secure aggregation for privacy preserving machine
learning. IACR Cryptology ePrint Archive  2017:281  2017.

[2] Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational mathematics  9(6):717  2009.

[3] Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal

encoding strategies? IEEE Transactions on Information Theory  52(12):5406–5425  2006.

[4] Jeffrey Dean and Sanjay Ghemawat. MapReduce: simpliﬁed data processing on large clusters. Communi-

cations of the ACM  51(1):107–113  2008.

[5] Olivier Fercoq and Peter Richtárik. Optimization in high dimensions via accelerated  parallel  and proximal

coordinate descent. SIAM Review  58(4):739–771  2016.

[6] Alex Gittens and Michael W Mahoney. Revisiting the Nyström method for improved large-scale machine

learning. Journal of Machine Learning Research  17(1):3977–4041  2016.

[7] Martin Jaggi  Virginia Smith  Martin Takác  Jonathan Terhorst  Sanjay Krishnan  Thomas Hofmann  and
Michael I Jordan. Communication-efﬁcient distributed dual coordinate ascent. In Advances in Neural
Information Processing Systems (NIPS)  2014.

[8] Jakub Konecn`y  H Brendan McMahan  Daniel Ramage  and Peter Richtárik. Federated optimization:

distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527  2016.

[9] Jakub Konecn`y  H Brendan McMahan  Felix X Yu  Peter Richtárik  Ananda Theertha Suresh  and
Dave Bacon. Federated learning: strategies for improving communication efﬁciency. arXiv preprint
arXiv:1610.05492  2016.

[10] Jason D Lee  Qihang Lin  Tengyu Ma  and Tianbao Yang. Distributed stochastic variance reduced gradient

methods and a lower bound for communication complexity. arXiv preprint arXiv:1507.07595  2015.

9

[11] Mu Li  David G Andersen  Jun Woo Park  Alexander J Smola  Amr Ahmed  Vanja Josifovski  James Long 
Eugene J Shekita  and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In
USENIX Symposium on Operating Systems Design and Implementation (OSDI)  2014.

[12] Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.

Mathematical programming  45(1-3):503–528  1989.

[13] Ji Liu  Stephen J Wright  Christopher Ré  Victor Bittorf  and Srikrishna Sridhar. An asynchronous parallel
stochastic coordinate descent algorithm. Journal of Machine Learning Research  16(285-322):1–5  2015.
[14] Yucheng Low  Danny Bickson  Joseph Gonzalez  Carlos Guestrin  Aapo Kyrola  and Joseph M. Hellerstein.
Distributed GraphLab: A framework for machine learning and data mining in the cloud. Proceedings of
the VLDB Endowment  2012.

[15] Chenxin Ma  Virginia Smith  Martin Jaggi  Michael Jordan  Peter Richtarik  and Martin Takac. Adding
vs. averaging in distributed primal-dual optimization. In International Conference on Machine Learning
(ICML)  2015.

[16] Dhruv Mahajan  Nikunj Agrawal  S Sathiya Keerthi  S Sundararajan  and Léon Bottou. An efﬁ-
cient distributed learning algorithm based on effective local functional approximations. arXiv preprint
arXiv:1310.8418  2013.

[17] Dhruv Mahajan  S Sathiya Keerthi  S Sundararajan  and Léon Bottou. A parallel SGD method with strong

convergence. arXiv preprint arXiv:1311.0636  2013.

[18] Brendan McMahan  Eider Moore  Daniel Ramage  Seth Hampson  and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS)  2017.

[19] Xiangrui Meng  Joseph Bradley  Burak Yavuz  Evan Sparks  Shivaram Venkataraman  Davies Liu  Jeremy
Freeman  DB Tsai  Manish Amde  Sean Owen  et al. MLlib: machine learning in Apache Spark. Journal
of Machine Learning Research  17(34):1–7  2016.

[20] Ion Necoara and Dragos Clipici. Parallel random coordinate descent method for composite minimization:

Convergence analysis and error bounds. SIAM Journal on Optimization  26(1):197–226  2016.

[21] A.S. Nemirovskii and D.B. Yudin. Problem Complexity and Method Efﬁciency in Optimization. A
[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In

Wiley-Interscience publication. Wiley  1983.

Soviet Mathematics Doklady  volume 27  pages 372–376  1983.

[23] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Science

& Business Media  2013.

[24] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media  2006.
[25] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with

linear-quadratic convergence. SIAM Journal on Optimization  27(1):205–245  2017.

[26] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural

Information Processing Systems (NIPS)  2007.

[27] Benjamin Recht  Christopher Re  Stephen Wright  and Feng Niu. Hogwild: a lock-free approach to
parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems (NIPS) 
2011.

[28] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alexander J Smola. On variance
reduction in stochastic gradient descent and its asynchronous variants. In Advances in Neural Information
Processing Systems (NIPS). 2015.

[29] Sashank J Reddi  Jakub Konecn`y  Peter Richtárik  Barnabás Póczós  and Alex Smola. AIDE: fast and

communication efﬁcient distributed optimization. arXiv preprint arXiv:1608.06879  2016.

[30] Peter Richtárik and Martin Takác. Distributed coordinate descent method for learning with big data.

Journal of Machine Learning Research  17(1):2657–2681  2016.

[31] Peter Richtárik and Martin Takávc. Parallel coordinate descent methods for big data optimization. Mathe-

matical Programming  156(1-2):433–484  2016.

[32] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled Newton methods I: globally convergent

algorithms. arXiv preprint arXiv:1601.04737  2016.

[33] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled Newton methods II: Local convergence

rates. arXiv preprint arXiv:1601.04738  2016.

[34] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algorithms.

Cambridge University Press  2014.

[35] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In Annual Allerton

Conference on Communication  Control  and Computing  2014.

[36] Ohad Shamir  Nati Srebro  and Tong Zhang. Communication-efﬁcient distributed optimization using an

approximate Newton-type method. In International conference on machine learning (ICML)  2014.

10

[37] Virginia Smith  Chao-Kai Chiang  Maziar Sanjabi  and Ameet Talwalkar. Federated multi-task learning.

arXiv preprint arXiv:1705.10467  2017.

[38] Virginia Smith  Simone Forte  Chenxin Ma  Martin Takac  Michael I Jordan  and Martin Jaggi. CoCoA: A
general framework for communication-efﬁcient distributed optimization. arXiv preprint arXiv:1611.02189 
2016.

[39] Shusen Wang  Alex Gittens  and Michael W. Mahoney. Sketched ridge regression: Optimization perspective 
statistical perspective  and model averaging. In International Conference on Machine Learning (ICML) 
2017.

[40] Shusen Wang  Luo Luo  and Zhihua Zhang. SPSD matrix approximation vis column selection: Theories 

algorithms  and extensions. Journal of Machine Learning Research  17(49):1–49  2016.

[41] Shusen Wang  Farbod Roosta-Khorasani  Peng Xu  and Michael W. Mahoney. GIANT: Globally improved

approximate Newton method for distributed optimization. arXiv:1709.03528  2018.

[42] Peng Xu  Jiyan Yang  Farbod Roosta-Khorasani  Christopher Ré  and Michael W Mahoney. Sub-sampled
Newton methods with non-uniform sampling. In Advances in Neural Information Processing Systems
(NIPS)  2016.

[43] Tianbao Yang. Trading computation for communication: distributed stochastic dual coordinate ascent. In

Advances in Neural Information Processing Systems (NIPS)  2013.

[44] Matei Zaharia  Mosharaf Chowdhury  Michael J Franklin  Scott Shenker  and Ion Stoica. Spark: Cluster

computing with working sets. HotCloud  10(10-10):95  2010.

[45] Yuchen Zhang and Xiao Lin. DiSCO: distributed optimization for self-concordant empirical loss. In

International Conference on Machine Learning (ICML)  2015.

[46] Shun Zheng  Fen Xia  Wei Xu  and Tong Zhang. A general distributed dual coordinate optimization

framework for regularized loss minimization. arXiv preprint arXiv:1604.03763  2016.

[47] Martin Zinkevich  Markus Weimer  Lihong Li  and Alex J Smola. Parallelized stochastic gradient descent.

In Advances in Neural Information Processing Systems (NIPS)  2010.

11

,David Pfau
Nicholas Bartlett
Frank Wood
Sewoong Oh
Kiran Thekumparampil
Shusen Wang
Fred Roosta
Peng Xu
Michael Mahoney
Wei-Da Chen
Shan-Hung (Brandon) Wu