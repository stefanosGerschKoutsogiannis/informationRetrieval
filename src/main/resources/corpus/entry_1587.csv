2009,Distribution-Calibrated Hierarchical Classification,While many advances have already been made on the topic of hierarchical classi-  ﬁcation learning  we take a step back and examine how a hierarchical classiﬁca-  tion problem should be formally deﬁned. We pay particular attention to the fact  that many arbitrary decisions go into the design of the the label taxonomy that  is provided with the training data  and that this taxonomy is often unbalanced.  We correct this problem by using the data distribution to calibrate the hierarchical  classiﬁcation loss function. This distribution-based correction must be done with  care  to avoid introducing unmanagable statstical dependencies into the learning  problem. This leads us off the beaten path of binomial-type estimation and into  the uncharted waters of geometric-type estimation. We present a new calibrated  deﬁnition of statistical risk for hierarchical classiﬁcation  an unbiased geometric  estimator for this risk  and a new algorithmic reduction from hierarchical classiﬁ-  cation to cost-sensitive classiﬁcation.,Distribution-Calibrated Hierarchical Classiﬁcation

Ofer Dekel

Microsoft Research

One Microsoft Way  Redmond  WA 98052  USA

oferd@microsoft.com

Abstract

While many advances have already been made in hierarchical classiﬁcation learn-
ing  we take a step back and examine how a hierarchical classiﬁcation problem
should be formally deﬁned. We pay particular attention to the fact that many ar-
bitrary decisions go into the design of the label taxonomy that is given with the
training data. Moreover  many hand-designed taxonomies are unbalanced and
misrepresent the class structure in the underlying data distribution. We attempt
to correct these problems by using the data distribution itself to calibrate the hi-
erarchical classiﬁcation loss function. This distribution-based correction must be
done with care  to avoid introducing unmanageable statistical dependencies into
the learning problem. This leads us off the beaten path of binomial-type estima-
tion and into the unfamiliar waters of geometric-type estimation. In this paper 
we present a new calibrated deﬁnition of statistical risk for hierarchical classiﬁ-
cation  an unbiased estimator for this risk  and a new algorithmic reduction from
hierarchical classiﬁcation to cost-sensitive classiﬁcation.

1 Introduction

Multiclass classiﬁcation is the task of assigning labels from a predeﬁned label-set to instances in a
given domain. For example  consider the task of assigning a topic to each document in a corpus.
If a training set of labeled documents is available  then a multiclass classiﬁer can be trained using
a supervised machine learning algorithm. Often  large label-sets can be organized in a taxonomy.
Examples of popular label taxonomies are the ODP taxonomy of web pages [2]  the gene ontology
[6]  and the LCC ontology of book topics [1]. A taxonomy is a hierarchical structure over labels 
where some labels deﬁne very general concepts  and other labels deﬁne more speciﬁc specializations
of those general concepts. A taxonomy of document topics could include the labels MUSIC  CLAS-
SICAL MUSIC  and POPULAR MUSIC  where the last two are special cases of the ﬁrst. Some label
taxonomies form trees (each label has a single parent) while others form directed acyclic graphs.
When a label taxonomy is given alongside a training set  the multiclass classiﬁcation problem is
often called a hierarchical classiﬁcation problem. The label taxonomy deﬁnes a structure over the
multiclass problem  and this structure should be used both in the formal deﬁnition of the hierarchical
classiﬁcation problem  and in the design of learning algorithms to solve this problem.

Most hierarchical classiﬁcation learning algorithms treat the taxonomy as an indisputable deﬁnitive
model of the world  never questioning its accuracy. However  most taxonomies are authored by
human editors and subjective matters of style and taste play a major role in their design. Many
arbitrary decisions go into the design of a taxonomy  and when multiple editors are involved  these
arbitrary decisions are made inconsistently. Figure 1 shows two versions of a simple taxonomy  both
equally reasonable; choosing between them is a matter of personal preference. Arbitrary decisions
that go into the taxonomy design can have a signiﬁcant inﬂuence on the outcome of the learning
algorithm [19]. Ideally  we want learning algorithms that are immune to the arbitrariness in the
taxonomy.

1

The arbitrary factor in popular label taxonomies is a well-known phenomenon. [17] gives the ex-
ample of the Library of Congress Classiﬁcation system (LCC)  a widely adopted and constantly
updated taxonomy of “all knowledge”  which includes the category WORLD HISTORY and four of
its direct subcategories: ASIA  AFRICA  NETHERLANDS  and BALKAN PENINSULA. There is a clear
imbalance between the the level of granularity of ASIA versus its sibling BALKAN PENINSULA.
The Dewey Decimal Classiﬁcation (DDC)  another widely accepted taxonomy of “all knowledge” 
deﬁnes ten main classes  each has exactly ten subclasses  and each of those again has exactly ten sub-
classes. The rigid choice of a decimal fan-out is an arbitrary one  and stems from an aesthetic ideal
rather than a notion of informativeness. Incidentally  the ten subclasses of RELIGION in the DDC
include six categories about Christianity and the additional category OTHER RELIGIONS  demon-
strating the editor’s clear subjective predilection for Christianity. The ODP taxonomy of web-page
topics is optimized for navigability rather than informativeness  and is therefore very ﬂat and often
unbalanced. As a result  two of the direct children of the label GAMES are VIDEO GAMES (with
over 42  000 websites listed) and PAPER AND PENCIL GAMES (with only 32 websites). These ex-
amples are not intended to show that these useful taxonomies are ﬂawed  they merely demonstrate
the arbitrary subjective aspect of their design.

Our goal is to deﬁne the problem such that it is invariant to many of these subjective and arbitrary
design choices  while still exploiting much of the available information. Some older approaches to
hierarchical classiﬁcation do not use the taxonomy in the deﬁnition of the classiﬁcation problem
[12  13  18  9  16]. Namely  these approaches consider all classiﬁcation mistakes to be equally
bad  and use the taxonomy only to the extent that it reduces computational complexity and the
number of classiﬁcation mistakes. More recent approaches [3  8  5  4] exploit the label taxonomy
more thoroughly  by using it to induce a hierarchy-dependent loss function  which captures the
intuitive idea that not all classiﬁcation mistakes are equally bad: incorrectly classifying a document
as CLASSICAL MUSIC when its true topic is actually JAZZ is not nearly as bad as classifying that
document as COMPUTER HARDWARE. When this interpretation of the taxonomy can be made 
ignoring it is effectively wasting a valuable signal in the problem input. For example  [8] deﬁne the
loss of predicting a label u when the correct label is y as the number of edges along the path between
the two labels in the taxonomy graph.

Additionally  a taxonomy provides a very natural framework for balancing the tradeoff between
speciﬁcity and accuracy in classiﬁcation. Ideally  we would like our classiﬁer to assign the most
speciﬁc label possible to an instance  and the loss function should reward it adequately for doing
so. However  when a speciﬁc label cannot be assigned with sufﬁciently high conﬁdence  it is often
better to fall-back on a more general correct label than it is to assign an incorrect speciﬁc label. For
example  classifying a document on JAZZ as the broader topic MUSIC is better than classifying it as
the more speciﬁc yet incorrect topic COUNTRY MUSIC. A hierarchical classiﬁcation problem should
be deﬁned in a way that penalizes both over-conﬁdence and under-conﬁdence in a balanced way.

The graph-distance based loss function introduced by [8] captures both of the ideas mentioned
above  but it is very sensitive to arbitrary choices that go into the taxonomy design. Once again
consider the example in Fig. 1: each hierarchy would induce a different graph-distance  which
would lead to a different outcome of the learning algorithm. We can make the difference between
the two outcomes arbitrarily large by making some regions of the taxonomy very deep and other
regions very ﬂat. Additionally  we note that the simple graph-distance based loss works best when
the taxonomy is balanced  namely  when all of the splits in the taxonomy convey roughly the same
amount of information. For example  in the taxonomy of Fig. 1  the children of CLASSICAL MU-
SIC are VIVALDI and NON-VIVALDI  where the vast majority of classical music falls in the latter.
If the correct label is NON-VIVALDI and our classiﬁer predicts the more general label CLASSICAL
MUSIC  the loss should be small  since the two labels are essentially equivalent. On the other hand 
if the correct label is VIVALDI then predicting CLASSICAL MUSIC should incur a larger loss  since
important detail was excluded. A simple graph-distance based loss will penalize both errors equally.

On one hand  we want to use the hierarchy to deﬁne the problem. On the other hand  we don’t want
arbitrary choices and unbalanced splits in the taxonomy to have a signiﬁcant effect on the outcome.
Can we have our cake and eat it too? Our proposed solution is to leave the taxonomy structure
as-is  and to stick with a graph-distance based loss  but to introduce non-uniform edge weights.
Namely  the loss of predicting u when the true label is y is deﬁned as the sum of edge-weights
along the shortest path from u to y. We use the underlying distribution over labels to set the edge

2

Figure 1: Two equally-reasonable label taxonomies. Note the subjective decision to include/exclude
the label ROCK  and note the unbalanced split of CLASSICAL to the small class VIVALDI and the
much larger class NON-VIVALDI.

weights in a way that adds balance to the taxonomy and compensates for certain arbitrary design
choices. Speciﬁcally  we set edge weights using the information-theoretic notion of conditional self-
information [7]. The weight of an edge between a label u and its parent u′ is the log-probability of
observing the label u given that the example is also labeled by u′.
Others [19] have previously tried to use the training data to “ﬁx” the hierarchy  as a preprocessing
step to classiﬁcation. However  it is unclear whether it is statistically permissible to reuse the training
data twice: once to ﬁx the hierarchy and then again in the actual learning procedure. The problem
is that the preprocessing step may introduce strong statistical dependencies into our problem. These
dependencies could prove detrimental to our learning algorithm  which expects to see a set of inde-
pendent examples. The key to our approach is that we can estimate our distribution-dependent loss
using the same data used to deﬁne it  without introducing any signiﬁcant bias. It turns out that to
accomplish this  we must deviate from the prevalent binomial-type estimation scheme that currently
dominates machine learning and turn to a more peculiar geometric-distribution-type estimator. A
binomial-type estimator essentially counts things (such as mistakes)  while a geometric-type esti-
mator measures the amount of time that passes before something occurs. Geometric-type estimators
have the interesting property that they might occasionally fail  which we investigate in detail below.
Moreover  we show how to control the variance of our estimate without adding bias. Since em-
pirical estimation is the basis of supervised machine learning  we can now extrapolate hierarchical
learning algorithms from our unbiased estimation technique. Speciﬁcally  we present a reduction
from hierarchical classiﬁcation to cost-sensitive multiclass classiﬁcation  which is based on our new
geometric-type estimator.

This paper is organized as follows. We formally set the problem in Sec. 2 and present our new
distribution-dependent loss function in Sec. 3. In Sec. 4 we discuss how to control the variance of
our empirical estimates  which is a critical step towards the learning algorithm described in Sec. 5.
We conclude with a discussion in Sec. 6. We omit technical proofs due to space constraints.

2 Problem Setting

We now deﬁne our problem more formally. Let X be an instance space and let T be a taxonomy of
labels. For simplicity  we focus on tree hierarchies. T is formally deﬁned as the pair (U  π)  where
U is a ﬁnite set of labels and π is the function that speciﬁes the parent of each label in U. U contains
both general labels and speciﬁc labels. Speciﬁcally  we assume that U contains the special label
ALL  and that all other labels in U are special cases of ALL. π : U → U is a function that deﬁnes
the structure of the taxonomy by assigning a parent π(u) to each label u ∈ U. Semantically  π(u) is
a more general label than u that contains u as a special case. In other words  we can say that “u is
a speciﬁc type of π(u)”. For completeness  we deﬁne π(ALL) = ALL. The n’th generation parent
function πn : U → U is deﬁned by recursively applying π to itself n times. Formally

πn(u) = π(π(. . . π
}

{z

(u) . . .)) .

For completeness  deﬁne π0 as the identity function over U. T is acyclic  namely  for all u 6= ALL
and for all n ≥ 1 it holds that πn(u) 6= u. The ancestor function π⋆  maps each label to its set of
ancestors  and is deﬁned as π⋆(u) = S∞
n=0{πn(u)}. In other words  π⋆(u) includes u  its parent  its
parent’s parent  and so on. We assume that T is connected and speciﬁcally that ALL is an ancestor

|

n

3

of all labels  meaning that ALL ∈ π⋆(u) for all u ∈ U. The inverse of the ancestor function is the
descendent function τ  which maps u ∈ U to the subset {u′ ∈ U : u ∈ π⋆(u′)}. In other words 
u is a descendent of u′ if and only if u′ is an ancestor of u. Graphically  we can depict T as a
rooted tree: U deﬁnes the tree nodes  ALL is the root  and {(cid:0)u  π(u)(cid:1) : u ∈ U \ ALL} is the set of
edges. In this graphical representation  τ (u) includes the nodes in the subtree rooted at u. Using this
representation  we deﬁne the graph distance between any two labels d(u  u′) as the number of edges
along the path between u and u′ in the tree. The lowest common ancestor function λ : U × U → U
maps any pair of labels to their lowest common ancestor in the taxonomy  where “lowest” is in the
sense of tree depth. Formally  λ(u  u′) = πj(u) where j = min{i : πi(u) ∈ π⋆(u′)}. In words 
λ(u  u′) is the closest ancestor of u that is also an ancestor if u′. It is straightforward to verify that
λ(u  u′) = λ(u′  u). The leaves of a taxonomy are the labels that are not parents of any other labels.
We denote the set of leaves by Y and note that Y ⊂ U.
Now  let D be a distribution on the product space X × Y. In other words  D is a joint distribution
over instances and their corresponding labels. Note that we assume that the labels that occur in the
distribution are always leaves of the taxonomy T . This assumption can be made without loss of
generality: if this is not the case then we can always add a leaf to each interior node  and relabel
all of the examples accordingly. More formally  for each label u ∈ U \ Y  we add a new node y to
U with π(y) = u  and whenever we sample (x  u) from D then we replace it with (x  y). Initially 
we do not know anything about D  other than the fact that it is supported on X × Y. We sample m
independent points from D  to obtain the sample S = {(xi  yi)}m
A classiﬁer is a function f : X → U that assigns a label to each instance of X . Note that a classiﬁer
is allowed to predict any label in U  even though it knows that only leaf labels are ever observed
in the real world. We feel that this property captures a fundamental characteristic of hierarchical
classiﬁcation: although the truth is always speciﬁc  a good hierarchical classiﬁer will fall-back to a
more general label when it cannot conﬁdently give a speciﬁc prediction. The quality of f is measured
using a loss function ℓ : U × Y → R+. For any instance-label pair (x  y)  the loss ℓ(f (x)  y) should
be interpreted as the penalty associated with predicting the label f (x) when the true label is y. We
require ℓ to be weakly monotonic  in the following sense: if u′ lies along the path from u to y then
ℓ(u′  y) ≤ ℓ(u  y). Although the error indicator function  ℓ(u  y) = 1u6=y satisﬁes our requirements 
it is not what we have in mind. Another fundamental characteristic of hierarchical classiﬁcation
problems is that not all prediction errors are equally bad  and the deﬁnition of the loss should reﬂect
this. More speciﬁcally  if u′ lies along the path from u to y and u is not semantically equivalent to
u′  we actually expect that ℓ(u′  y) < ℓ(u  y).

i=1.

3 A Distribution-Calibrated Loss for Hierarchical Classiﬁcation

As mentioned above  we want to calibrate the hierarchical classiﬁcation loss function using the
distribution D  through its empirical proxy S. In other words  we want D to differentiate between
informative splits in the taxonomy and redundant ones. We follow [8] in using graph-distance to
deﬁne the loss function  but instead of setting all of the edge weights to 1  we deﬁne edge weights
using D.
For each y ∈ Y  let p(y) be the marginal probability of the label y in the distribution D. For
each u ∈ U  deﬁne p(u) = Py∈Y∩τ (u) p(y). In words  for any u ∈ U  p(u) is the probability of
observing any descendent of u. We assume henceforth that p(u) > 0 for all u ∈ U. With these
deﬁnitions handy  deﬁne the weight of the edge between u and π(u) as log(cid:0)p(π(u))/p(u)(cid:1). This
weight is essentially the deﬁnition of conditional self information from information theory [7].
The nice thing about this deﬁnition is that the weighted graph-distance between labels u and y
telescopes between u and λ(u  y) and between u and λ(u  y)  and becomes

ℓ(u  y) = 2 log(cid:0)p(λ(u  y))(cid:1) − log(cid:0)p(u)(cid:1) − log(cid:0)p(y)(cid:1) .

(1)

Since this loss function depends only on u  y  and λ(u  y)  and their frequencies according to D  it
is completely invariant to the the number of labels along the path from u or y. It is also invariant
to inconsistent degrees of ﬂatness of the taxonomy in different regions. Finally  it is even invariant
to the addition or subtraction of new leaves or entire subtrees  so long as the marginal distributions
p(u)  p(y)  and p(λ(u  y)) remain unchanged. This loss also balances uneven splits in the taxonomy.

4

Recalling the example in Fig. 1 where CLASSICAL is split into VIVALDI and NON-VIVALDI  the edge
to the former will have a very high weight  whereas the edge to the latter will have a weight close to
zero.
Now  deﬁne the risk of a classiﬁer h as R(f ) = E(X Y )∼D[ℓ(f (X)  Y )]  the expected loss over
examples sampled from D. Our goal is to obtain a classiﬁer with a small risk. However  before we
tackle the problem of ﬁnding a low risk classiﬁer  we address the intermediate task of estimating the
risk of a given classiﬁer f using the sample S. The solution is not straightforward since we cannot
even compute the loss on an individual example  ℓ(f (xi)  yi)  as this requires knowledge of D. A
naive way to estimate ℓ(f (xi)  yi) using the sample S is to ﬁrst estimate each p(y) by Pm
i=1 1yi=y 
and to plug these values into the deﬁnition of ℓ. This estimator tends to suffer from a strong bias 
due to the non-linearity of the logarithm  and is considered to be unreliable1. Instead  we want an
unbiased estimator.

First  we write the deﬁnition of risk more explicitly using the deﬁnition of the loss function in Eq. (1).
Deﬁne q(f  u) = Pr(f (X) = u)  the probability that f outputs u when X is drawn according to
the marginal distribution of D over X . Also deﬁne r(f  u) = Pr(λ(f (X)  Y ) = u)  the probability
that the lowest common ancestor of f (X) and Y is u  when (X  Y ) is drawn from D. R(f ) can be
rewritten as

R(f ) = X

u∈U

(cid:0)2r(f  u) − q(f  u)(cid:1) log(p(u)) − X

y∈Y

p(y) log(cid:0)p(y)(cid:1) .

(2)

Notice that the second term in the deﬁnition of risk is a constant  independent of f . This constant
is simply H(Y )  the Shannon entropy [7] of the label distribution. Our ultimate goal is to compare
the risk values of different classiﬁers and to choose the best one  so we don’t really care about this
constant  and we can discard it henceforth. From here on  we focus on estimating the augmented
risk ¯R(f ) = R(f ) − H(Y ).
The main building block of our estimator is the estimation technique presented in [14]. Assume for
a moment that the sample S is inﬁnite. Recall that the harmonic number hn is deﬁned as Pn
1
i  
with h0 = 0. Deﬁne the random variables Ai and Bi as follows

i=1

Ai = min{j ∈ N : yi+j ∈ τ (f (xi))} − 1
Bi = min(cid:8)j ∈ N : yi+j ∈ τ(cid:0)λ(f (xi)  yi)(cid:1)(cid:9) − 1

For example  A1 + 2 is the index of the ﬁrst example after (x1  y1) whose label is contained in
the subtree rooted at f (x1)  and B1 + 2 is the index of the ﬁrst example after (x1  y1) whose
label is contained in the subtree rooted at λ(f (x1)  y1). Note that Bi ≤ Ai  since λ(u  y) is  by
deﬁnition  an ancestor of u  so y′ ∈ τ (u) implies y′ ∈ τ (λ(u  y)). Next  deﬁne the random variable
L1 = hA1 − 2hB1.
Theorem 1. L1 is an unbiased estimator of ¯R(f ).

Proof. We have that

∞

E(cid:2)L1 (cid:12)(cid:12) f (X1) = u  Y1 = y(cid:3) = p(u)
Using the fact that for any α ∈ [0  1) it holds that P∞
u  Y1 = y] = − log(cid:0)p(u)(cid:1) + 2 log(cid:0)p(λ(u  y))(cid:1). Therefore 

hj(cid:0)1 − p(u)(cid:1)j

j=0

X

− 2p(cid:0)λ(u  y)(cid:1)

∞

X

j=0

hj(cid:0)1 − p(λ(u  y))(cid:1)j

.

n=0 hnαn = − log(1−α)

1−α we get  E[L1|f (X1) =

E[L1] = Pu∈U Py∈Y Pr(f (X) = u  Y = y) E[L1|f (X1) = u  Y1 = y]

= Pu∈U (cid:0)2r(f  u) − q(f  u)(cid:1) log(cid:0)p(u)(cid:1) = ¯R(f ) .

We now recall that our sample S is actually of ﬁnite size m. The problem that now occurs is that
A1 and B1 are not well deﬁned when f (X1) does not appear anywhere in Y2  . . .   Ym. When this
happens  we say that the estimator L1 fails. If f outputs a label u with p(u) = 0 then L1 will fail

1The interested reader is referred to the extensive literature on the closely related problem of estimating the

entropy of a distribution from a ﬁnite sample.

5

with probability 1. On the other hand  the probability of failure is negligible when m is large enough 
and when f does not output labels with tiny probabilities. Formally  let β(f ) = minu:q(f u)>0 p(u)
be the smallest probability of any label that f outputs.
Theorem 2. The probability of failure is at most e−(m−1)β(f ).

The estimator E[L1|no-fail] is no longer an unbiased estimator of ¯R(f )  but the bias is small. Specif-
ically  since we are after a classiﬁer f with a small risk  we prove an upper-bound on ¯R(f ).

Theorem 3. It holds that E(cid:2)L1(cid:12)(cid:12)no-fail(cid:3) ≥ ¯R(f ) − (m−1)e−β(f )(m−1)

β2(f )

.

For example  with β = 0.01 and m = 2500  the bias term in Thm. 3 is less than 0.0004. With
m = 5000 it is already less than 10−14.

4 Decreasing the Variance of the Estimator

− 2hBSi

Say that we have k classiﬁers and we want to choose the best one. The estimator L1 suffers from
an unnecessarily high variance because it typically uses a short preﬁx of the sample S and wastes
the remaining examples. To reliably compare k empirical risk estimates  we need to reduce the
variance of each estimator. The exact value of Var(L1) depends on the distributions p  q  and r in a
non-trivial way  but we can give a simple upper-bound on Var(L1) in terms of β(f ).
Theorem 4. Var(L1) ≤ −9 log(cid:0)β(f )(cid:1) + 9 log2 (cid:0)β(f )(cid:1).
We reduce the variance of the estimator by repeating the estimation multiple times  without reusing
any sample points. Formally  deﬁne S1 = 1  and deﬁne for all i ≥ 2 the random variables Si =
. In words: the ﬁrst estimator L1 starts at S1 = 1
Si−1 + ASi−1 + 2  and Li = hASi
and uses A1 + 2 examples  namely  the examples 1  . . .   (A1 + 2). Now  S2 = A1 + 3 is the ﬁrst
untouched example in the sequence. The second estimator  L2 starts at example S2 and uses AS2 + 2
examples  namely  the examples S2  . . .   (S2 + AS2 + 1)  and so on. If we had an inﬁnite sample and
chose some threshold t  the random variables L1  . . .   Lt would all be unbiased estimators of ¯R(f ) 
i=1 Li would also be an unbiased estimate of ¯R(f ).
and therefore the aggregate estimator L = 1
Since L1  . . .   Lt are also independent  the variance of the aggregate estimator would be 1
t Var(L1).
In the ﬁnite-sample case  aggregating multiple estimators is not as straightforward. Again  the event
where the estimation fails introduces a small bias. Additionally  the number of independent estima-
tions that ﬁt in a sample of ﬁxed size m is itself a random variable T . Moreover  the value of T
depends on the value of the risk estimators. In other words  if L1  L2  . . . take large values then T
will take a small value. The precise deﬁnition of T should be handled with care  to ensure that the
individual estimators remain independent and that the aggregate estimator maintains a small bias.
For example  the ﬁrst thing that comes to mind is to set T to be the largest number t such that
St ≤ m - this is a bad idea. To see why  note that if T = 2 and A1 = m − 4 then we know with
certainty that AS2 = 0. This clearly demonstrates a strong statistical dependence between L1  L2
and T   which both interferes with the variance reduction and introduces a bias. Instead  we deﬁne T
as follows: choose a positive integer l ≤ m and set T using the last l examples in S  as follows  set

t Pt

T = min {t ∈ N : St+1 ≥ m − l} .

(3)

In words  we think of the last l examples in S as the “landing strip” of our procedure: we keep
jumping forward in the sequence of samples  from S1 to S2  to S3  and so on  until the ﬁrst time we
land on the landing strip. Our new failure scenario occurs when our last jump overshoots the strip 
and no Si falls on any one of the last l examples. If L does not fail  deﬁne the aggregate estimator as
L = PT
Theorem 5. The probability of failure of the estimator L is at most e−lβ(f ).

i=1 Li. Note that we are summing Li rather than averaging them; we explain this later on.

We now prove that our deﬁnition of T indeed decreases the variance without adding bias. We give a
simpliﬁed version of the analysis  assuming that S is inﬁnite  and assuming that the limit m is merely
a recommendation. In other words  T is still deﬁned as before  but estimation never fails  even in the
rare case where ST + AST + 1 > m (the index of the last example used in the estimation exceeds
the predeﬁned limit m). We note that a very similar theorem can be stated in the ﬁnite-sample case 

6

INPUTS: a training set S = {(xi  yi)}m
1 for i = 1  . . .   m

i=1  a label taxonomy T .

2

3

4

5

6

generate random permutation ψ : {1  . . .   (m − 1)} → {1  . . .   (i − 1)  (i + 1)  . . .   m}.
for u = 1  . . .   d
a = −1 + minnj ∈ {1  . . .   (m − 1)} : yψ(j) ∈ τ (u)o
b = −1 + minnj ∈ {1  . . .   (m − 1)} : yψ(j) ∈ τ(cid:0)λ(u  yi)(cid:1)o

b+1 + 1

b+2 + · · · + 1

M (i  u) = 1

a

OUTPUT: M

Figure 2: A reduction from hierarchical multiclass to cost-sensitive multiclass.

at the price of a signiﬁcantly more complicated analysis. The complication stems from the fact that
we are estimating the risk of k classiﬁers simultaneously  and the failure of one estimator depends
on the values of the other estimators. We allow ourselves to ignore failures because they occur with
such small probability  and because they introduce an insigniﬁcant bias.
Theorem 6. Assuming that S is inﬁnite  but T is still deﬁned as in Eq. (3)  it holds that E(cid:2)L] =
E(cid:2)T ] ¯R(f ) and Var(L) ≤ E[T ]σ2  where σ2 = Var(cid:0)Li).
The proof follows from variations on Wald’s theorem [15].
Recall that we have k competing classiﬁers  f1  . . .   fk  and we want to choose one with a small
risk. We overload our notation to support multiple concurrent estimations  and deﬁne T (fj) as the
stopping time (previously deﬁned as T in Eq. (3)) of the estimation process for ¯R(fj). Also let
Li(fj) be the i’th unbiased estimator of ¯R(fj). To conduct a fair comparison of the k classiﬁers 
we redeﬁne T = minj=1 ... k T (fj)  and let L(fj) = PT
i=1 Li(fj). In other words  we aggregate
the same number of estimators for each classiﬁer. We then choose the classiﬁer with the smallest
risk estimate  arg min L(Fj). Theorem 6 still holds for each individual classiﬁer because the new
deﬁnition of T remains a stopping time for each of the individual estimation processes. Although
we may not know the exact value of E[T ]  it is just a number that we can use to reason about the bias
and the variance of L. We note that ﬁnding j that minimizes L(fj) is equivalent to ﬁnding j that
minimizes L(fj)/E[T ]. The latter  according to Thm. 6  is an unbiased estimate of ¯R(f ). Moreover 
the variance of each L(fj)/E[T ] is Var (L(fj)/E[T ]) = σ2/E[T ]  so the effective variance of our
unbiased estimate decreases like 1/E[T ]  which is what we would expect. Using the one-tailed
Chebyshev inequality [11]  we get that for any ǫ > 0  Pr(cid:0) ¯R(fj) ≥ L(fj) + ǫ(cid:1) < σ2/(σ2+E[T ]ǫ2).
The bound holds uniformly for all k classiﬁers with probability kσ2/(σ2 + E[T ]ǫ2) (using the union
bound). The variance of the estimation depends on E[T ]  and we expect E[T ] to grow linearly with
m. For example we can prove the following crude lower-bound.
Theorem 7. E[T ] ≥ (m − l)/c  where c = k + Pk
j=1 1/β(fj).
5 Reducing Hierarchical Classiﬁcation to Cost-Sensitive Classiﬁcation

In this section  we propose a method for learning low-risk hierarchical classiﬁers  using our new
deﬁnition of risk. More precisely  we describe a reduction from hierarchical classiﬁcation to cost-
sensitive multiclass classiﬁcation. The appeal of this approach is the abundance of existing cost-
sensitive learning algorithms. This reduction is itself an algorithm whose input is a training set of m
examples and a taxonomy over d labels  and whose output is a d × m matrix of non-negative reals 
denoted by M . Entry M (i  j) is the cost of classifying example i with label j. This cost matrix  and
the original training set  are given to a cost-aware multiclass learning algorithm  which attempts to
ﬁnd a classiﬁer f with a small empirical loss Pm

i=1 M (i  f (xi)).

7

For example  a common approach to multiclass problems is to train a model fu : X → R for each
label u ∈ U and to deﬁne the classiﬁer f (x) = arg maxu∈U fu(x). An SVM-ﬂavored way to train
a cost sensitive classiﬁer is to assume that the functions fu live in a Hilbert space  and to minimize

d

X

u=1

kfuk2 + C

m

X

i=1

hM (i  u) + fu(xi) − fyi(xi)i+

 

X

u6=yi

(4)

where C > 0 is a parameter and [α]+ = max{0  α}. The ﬁrst term is a regularizer and the second is
an empirical loss  justiﬁed by the fact that M (i  f (xi)) ≤ Pu6=yi (cid:2)M (i  u) + fu(xi) − fyi(xi)(cid:3)+
.
Coming back to the reduction algorithm  we generate M using the procedure outlined in Fig. 2.
Based on the analysis of the previous sections  it is easy to see that  for all i  M (i  f (xi)) is an
unbiased estimator of the risk ¯R(f ). This holds even if ψ (as deﬁned in Fig. 2) is a ﬁxed function 
m P M (i  f (xi)) is also an unbiased
because the training set is assumed to be i.i.d. Therefore  1
estimator of ¯R(f ). The cost-sensitive learning algorithm will try to minimize this empirical esti-
mate. The purpose of the random permutation at each step is to hopefully decrease the variance
of the overall estimate  by decreasing the dependencies between the different individual estimators.
We profess that a rigorous analysis of the variance of this estimator is missing from this work. Ide-
m P M (i  f (xi)) is
ally  we would like to show that  with high probability  the empirical estimate 1
ǫ-close to its expectation of ¯R(f )  uniformly for all classiﬁers f in our function class. This is a
challenging problem due to the complex dependencies in the estimator.

The learning algorithm used to solve this problem can (and should) use the hierarchical structure to
guide its search for a good classiﬁer. Our reduction to an unstructured cost-sensitive problem should
not be misinterpreted as a recommendation not to use the structure in the learning process. For
example  following [10  8]  we could augment the SVM approach described in Eq. (4) by replacing
the unstructured regularizerPd
u=1 kfuk2 with the structured regularizerPd
u=1 kfu−fπ(u)k2  where
[8] showed signiﬁcant gains on hierarchical problems using this
π(u) is the parent label of u.
regularizer.

6 Discussion

We started by taking a step back from the typical setup of a hierarchical classiﬁcation machine
learning problem. As a consequence  our focus was on the fundamental aspects of the hierarchical
problem deﬁnition  rather than on the equally important algorithmic issues. Our discussion was
restricted to the simplistic model of single-label hierarchical classiﬁcation with single-linked tax-
onomies  and our ﬁrst goal going forward is to relax these assumptions.

We point out that many of the theorems proven in this paper depend on the value of β(f )  which
is deﬁned as minu:q(u)>0 p(u). Speciﬁcally  if f occasionally outputs a very rare label  then β(f )
is tiny and much of our analysis breaks down. This provides a strong indication that an empirical
estimate of β(f ) would make a good regularization term in a hierarchical learning scheme. In other
words  we should deter the learning algorithm from choosing a classiﬁer that predicts very rare
labels. As mentioned in the introduction  the label taxonomy provides the perfect mechanism for
backing off and predicting a more common and less risky ancestor of that label.

We believe that our work is signiﬁcant in the broader context of structured learning. Most structured
learning algorithms blindly trust the structure that they are given  and arbitrary design choices are
likely to appear in many types of structured learning. The idea of using the data distribution to
calibrate  correct  and balance the side-information extends to other structured learning scenarios.
The geometric-type estimation procedure outlined in this paper may play an important role in those
settings as well.

Acknowledgment

The author would like to thank Paul Bennett for his suggestion of the loss function for its information
theoretic properties  reduction to a tree-weighted distance  and ability to capture other desirable
characteristics of hierarchical loss functions like weak monotonicity. The author also thanks Ohad
Shamir  Chris Burges  and Yael Dekel for helpful discussions.

8

References

[1] The Library of Congress Classiﬁcation. http://www.loc.gov/aba/cataloging/classiﬁcation/.
[2] The Open Directory Project. http://www.dmoz.org/about.html.
[3] L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines.

In 13th ACM Conference on Information and Knowledge Management  2004.

[4] N. Cesa-Bianchi  C. Gentile  and L. Zaniboni. Hierarchical classiﬁcation: combining bayes
with svm. In Proceedings of the 23rd International Conference on Machine Learning  2006.
[5] N. Cesa-Bianchi  C. Gentile  and L. Zaniboni. Incremental algorithms for hierarchical classi-

ﬁcation. Journal of Machine Learning Research  7:31–54  2007.

[6] The Gene Ontology Consortium. Gene ontology: tool for the uniﬁcation of biology. Nature

Genetics  25:25–29  2000.

[7] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley  1991.
[8] O. Dekel  J. Keshet  and Y. Singer. Large margin hierarchical classiﬁcation. In Proceedings of

the Twenty-First International Conference on Machine Learning  2004.

[9] S. T. Dumais and H. Chen. Hierarchical classiﬁcation of Web content.

SIGIR-00  pages 256–263  2000.

In Proceedings of

[10] T. Evgeniou  C.Micchelli  and M. Pontil. Learning multiple tasks with kernel methods. Journal

of Machine Learning Research  6:615–637  2005.

[11] W. Feller. An Introduction to Probability and its Applications  volume 2. John Wiley and Sons 

second edition  1970.

[12] D. Koller and M. Sahami. Hierarchically classifying docuemnts using very few words.

In
Machine Learning: Proceedings of the Fourteenth International Conference  pages 171–178 
1997.

[13] A. K. McCallum  R. Rosenfeld  T. M. Mitchell  and A. Y. Ng. Improving text classiﬁcation by

shrinkage in a hierarchy of classes. In Proceedings of ICML-98  pages 359–367  1998.

[14] S. Montgomery-Smith and T. Schurmann. Unbiased estimators for entropy and class number.
[15] S.M. Ross and E.A. Pekoz. A second course in probability theory. 2007.
[16] E. Ruiz and P. Srinivasan. Hierarchical text categorization using neural networks. Information

Retrieval  5(1):87–118  2002.

[17] C. Shirky. Ontology is overrated: Categories  links  and tags. In O’Reilly Media Emerging

Technology Conference  2005.

[18] A. S. Weigend  E. D. Wiener  and J. O. Pedersen. Exploiting hierarchy in text categorization.

Information Retrieval  1(3):193–216  1999.

[19] J. Zhang  L. Tang  and H. Liu. Automatically adjusting content taxonomies for hierarchical

classiﬁcation. In Proceedings of the Fourth Workshop on Text Mining  SDM06  2006.

9

,Kejun Huang
Xiao Fu
Nikolaos Sidiropoulos