2019,Understanding the Role of Momentum in Stochastic Gradient Methods,The use of momentum in stochastic gradient methods has become a widespread practice in machine learning. Different variants of momentum  including heavy-ball momentum  Nesterov's accelerated gradient (NAG)  and quasi-hyperbolic momentum (QHM)  have demonstrated success on various tasks. Despite these empirical successes  there is a lack of clear understanding of how the momentum parameters affect convergence and various performance measures of different algorithms. In this paper  we use the general formulation of QHM to give a unified analysis of several popular algorithms  covering their asymptotic convergence conditions  stability regions  and properties of their stationary distributions. In addition  by combining the results on convergence rates and stationary distributions  we obtain sometimes counter-intuitive practical guidelines for setting the learning rate and momentum parameters.,Understanding the Role of Momentum in

Stochastic Gradient Methods

Igor Gitman

Hunter Lang

Pengchuan Zhang

Lin Xiao

Microsoft Research AI

Redmond  WA 98052  USA

{igor.gitman  hunter.lang  penzhan  lin.xiao}@microsoft.com

Abstract

The use of momentum in stochastic gradient methods has become a widespread
practice in machine learning. Diﬀerent variants of momentum  including heavy-
ball momentum  Nesterov’s accelerated gradient (NAG)  and quasi-hyperbolic
momentum (QHM)  have demonstrated success on various tasks. Despite these
empirical successes  there is a lack of clear understanding of how the momentum
parameters aﬀect convergence and various performance measures of diﬀerent
algorithms. In this paper  we use the general formulation of QHM to give a uniﬁed
analysis of several popular algorithms  covering their asymptotic convergence
conditions  stability regions  and properties of their stationary distributions. In
addition  by combining the results on convergence rates and stationary distributions 
we obtain sometimes counter-intuitive practical guidelines for setting the learning
rate and momentum parameters.

Introduction

1
Stochastic gradient methods have become extremely popular in machine learning for solving stochastic
optimization problems of the form

(1)

(cid:2) f(x  ζ)(cid:3) 

minimize

x∈Rn

F(x) (cid:44) Eζ

where ζ is a random variable representing data sampled from some (unknown) probability distribution 
x ∈ Rn represents the parameters of a machine learning model (e.g.  the weight matrices in a neural
network)  and f is a loss function associated with the model parameters and any sample ζ. Many
variants of the stochastic gradient methods can be written in the form of

xk+1 = xk − αk dk 

(2)
where dk is a (stochastic) search direction and αk > 0 is the step size or learning rate. The classical
stochastic gradient descent (SGD) [31] method uses dk = ∇x f(xk  ζ k)  where ζ k is a random sample
collected at step k. For the ease of notation  we use gk to denote ∇x f(xk  ζ k) throughout this paper.
There is a vast literature on modiﬁcations of SGD that aim to improve its theoretical and empirical
performance. The most common such modiﬁcation is the addition of a momentum term  which
sets the search direction dk as the combination of the current stochastic gradient gk and past search
directions. For example  the stochastic variant of Polyak’s heavy ball method [26] uses

(3)
where βk ∈ [0  1). We call the combination of (2) and (3) the Stochastic Heavy Ball (SHB) method.
Gupal and Bazhenov [9] studied a “normalized” version of SHB  where

dk = gk + βk dk−1
 

dk = (1 − βk)gk + βk dk−1

.

(4)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

dk = ∇x f(cid:0)xk − αk βk dk−1

  ζ k(cid:1) + βk dk−1

In the context of modern deep learning  Sutskever et al. [34] proposed to use a stochastic variant of
Nesterov’s accelerated gradient (NAG) method  where

.

(5)
The number of variations on momentum has kept growing in recent years; see  e.g.  Synthesized
Nesterov Variants (SNV) [17]  Triple Momentum [36]  Robust Momentum [3]  PID Control-based
methods [1]  Accelerated SGD (AccSGD) [12]  and Quasi-Hyperbolic Momentum (QHM) [18].
Despite various empirical successes reported for these diﬀerent methods  there is a lack of clear
understanding of how the diﬀerent forms of momentum and their associated parameters aﬀect
convergence properties of the algorithms and other performance measures  such as ﬁnal loss value.
For example  Sutskever et al. [34] show that momentum is critical to obtaining good performance in
deep learning. But using diﬀerent parametrizations  Ma and Yarats [18] claim that momentum may
have little practical eﬀect. In order to clear up this confusion  several recent works [see  e.g.  40  1  18]
have aimed to develop and analyze general frameworks that capture many diﬀerent momentum
methods as special cases.
In this paper  we focus on a class of algorithms captured by the general form of QHM [18]:

dk = (1 − βk)gk + βk dk−1
 
xk+1 = xk − αk

(cid:2)(1 − νk)gk + νk dk(cid:3)  

(6)
where the parameter νk ∈ [0  1] interpolates between SGD (νk = 0) and (normalized) SHB (νk = 1).
When the parameters αk  βk and νk are held constant (thus the subscript k can be omitted) and
ν = β  it recovers a normalized variant of NAG with an additional coeﬃcient 1 − βk on the stochastic
gradient term in (5) (see Appendix A). In addition  Ma and Yarats [18] show that diﬀerent settings
of αk  βk and νk recover other variants such as AccSGD [12]  Robust Momentum [3]  and Triple
Momentum [36]. They also show that it is equivalent to SNV [17] and special cases of PID Control
(either PI or PD) [1]. However  there is little theoretical analysis of QHM in general. In this paper  we
take advantage of its general formulation to derive a uniﬁed set of analytic results that help us better
understand the role of momentum in stochastic gradient methods.

1.1 Contributions and outline
Our theoretical results on the QHM model (6) cover three diﬀerent aspects: asymptotic convergence
with probability one  stability region and local convergence rates  and characterizations of the
stationary distribution of {xk} under constant parameters α  β  and ν. Speciﬁcally:

• In Section 3  we show that for minimizing smooth nonconvex functions  QHM converges almost
surely as βk → 0 for arbitrary values of νk. And more surprisingly  we show that QHM converges
as νk βk → 1 (which requires both νk → 1 and βk → 1) as long as νk βk → 1 slow enough  as
compared with the speed of αk → 0.
• In Section 4  we consider local convergence behaviors of QHM for ﬁxed parameters α  β  and ν.
In particular  we derive joint conditions on (α  β  ν) that ensure local stability (or convergence
when there is no stochastic noise in the gradient approximations) of the algorithm near a strict
local minimum. We also characterize the local convergence rate within the stability region.
• In Section 5  we investigate the stationary distribution of {xk} generated by the QHM dynamics
around a local minimum (using a simple quadratic model with noise). We derive the dependence
of the stationary variance on (α  β  ν) up to the second-order Taylor expansion in α. These results
reveal interesting eﬀects of β and ν that cannot be seen from ﬁrst-order expansions.

Our asymptotic convergence results in Section 3 give strong guarantees for the convergence of QHM
with diminishing learning rates under diﬀerent regimes (βk → 0 and βk → 1). However  as with
most asymptotic results  they provide limited guidance on how to set the parameters in practice for
fast convergence. Our results in Sections 4 and 5 complement the asymptotic results by providing
principled guidelines for tuning these parameters. For example  one of the most eﬀective schemes
used in deep learning practice is called “constant and drop”  where constant parameters (α  β  ν) are
used to train the model for a long period until it reaches a stationary state and then the learning rate α
is dropped by a constant factor for reﬁned training. Each stage of the constant-and-drop scheme runs
variants of QHM with constant parameters  and their choices dictate the overall performance of the
algorithm. In Section 6  by combining our results in Sections 4 and 5  we obtain new and  in some
cases  counter-intuitive insight into how to set these parameters in practice.

2

2

k=0 α

convergence is(cid:80)∞

k=0 αk = ∞ and(cid:80)∞

2 Related work
Asymptotic convergence There exist many classical results concerning the asymptotic convergence
of the stochastic gradient methods [see  e.g. 37  28  14  and references therein]. For the classical SGD
method without momentum  i.e.  (2) with dk = gk  a well-known general condition for asymptotic
k < ∞. In general  we will always need αk → 0 to counteract
the eﬀect of noise. But interestingly  the conditions on βk are much less restricted. For normalized
SHB  Polyak [27] and Kaniovski [11] studied its asymptotic convergence properties in the regime
of αk → 0 and βk → 0  while Gupal and Bazhenov [9] investigated asymptotic convergence in the
regime of αk → 0 and βk → 1  both for convex optimization problems. More recently  Gadat et al.
[7] extended asymptotic convergence analysis for the normalized SHB update to smooth nonconvex
functions for βk → 1. In this work we generalize the classical SGD and SHB results to the case of
QHM for smooth nonconvex functions.
Local convergence rate The stability region and local convergence rate of the deterministic gradient
descent and heavy ball algorithms were established by Boris Polyak for the case of convex functions
near a strict twice-diﬀerentiable local minimum [29  26]. For this class of functions heavy ball method
is optimal in terms of the local convergence rate [21]. However  it might fail to converge globally for
the general strongly convex twice-diﬀerentiable functions [17] and is no longer optimal for the class
of smooth convex functions. For the latter case  Nesterov’s accelerated gradient was shown to attain
the optimal global convergence rate [22  23]. In this paper we extend the results of Polyak [26] on
local convergence to the more general QHM algorithm.
Stationary analysis The limit behavior analysis of SGD algorithms with momentum and constant
step size was used in various applications. [25  39  15] establish suﬃcient conditions on detecting
whether iterates reach stationarity and use them in combination with statistical tests to automatically
change learning rate during training. [6  4] prove many properties of limiting behavior of SGD with
constant step size by using tools from Markov chain theory. Our results are most closely related
to the work of Mandt et al. [19] who use stationary analysis of SGD with momentum to perform
approximate Bayesian inference. In fact  our Theorem 4 extends their results to the case of QHM and
our Theorem 5 establishes more precise relations (to the second order in α)  revealing interesting
dependence on the parameters β and ν which cannot be seen from the ﬁrst order equations.

3 Asymptotic convergence
In this section  we generalize the classical asymptotic results to provide conditions under which
QHM converges almost surely to a stationary point for smooth nonconvex functions. Throughout this
section  "a.s." refers to "almost surely". We need to make the following assumptions.
Assumption A. The following conditions hold for F deﬁned in (1) and the stochastic gradient oracle:
1. F is diﬀerentiable and ∇F is Lipschitz continuous  i.e.  there is a constant L such that

(cid:107)∇F(x) − ∇F(y)(cid:107) ≤ L(cid:107)x − y(cid:107) 

x  y ∈ Rn.

2. F is bounded below and (cid:107)∇F(x)(cid:107) is bounded above  i.e.  there exist F∗ and G such that

F(x) ≥ F∗ 

(cid:107)∇F(x)(cid:107) ≤ G 

x ∈ Rn.

3. For k = 0  1  2  . . .  the stochastic gradient gk = ∇F(xk) + ξ k  where the random noise ξ k satisﬁes

(cid:2)(cid:107)ξ k(cid:107)2(cid:3) ≤ C a.s.

Ek[ξ k] = 0 

Ek

  . . .   xk−1
0

where Ek[·] denotes expectation conditioned on {x0
  g

  xk}  and C is a constant.
Note that Assumption A.3 allows the distribution of ξ k to depend on xk  and we simply require the
second moment to be conditionally bounded uniformly in k. The assumption (cid:107)∇F(x)(cid:107) ≤ G can be
removed if we assume a bounded domain for x. However  this will complicate the proof by requiring
special treatment (e.g.  using the machinery of gradient mapping [24]) when {xk} converges to the
boundary of the domain. Here we assume this condition to simplify the analysis.
By convergence to a stationary point  we mean that the sequence {xk} satisﬁes the condition

  gk−1

k→∞ (cid:107)∇F(xk)(cid:107) = 0
lim inf

a.s.

(7)

3

Intuitively  as βk → 0  regardless of νk  the QHM dynamics become more like SGD  so there should
be no issue with convergence. The following theorem  which generalizes the analysis technique of
Ruszczyński and Syski [33] to QHM  shows formally that this is indeed the case:
Theorem 1. Let F satisfy Assumption A. Additionally  assume 0 ≤ νk ≤ 1 and the sequences {αk}
and {βk} satisfy the following conditions:

αk = ∞ 

2

k < ∞ 

α

lim
k→∞ βk = 0 

¯β (cid:44) sup

βk < 1.

k

Then the sequence {xk} generated by the QHM algorithm (6) satisﬁes (7). Moreover  we have

∞(cid:88)

k=0

∞(cid:88)

k=0

F(xk) =

lim sup
k→∞

lim sup

k→∞  (cid:107)∇F(x k)(cid:107)→0

F(xk)

a.s.

(8)

∞(cid:88)

∞(cid:88)

∞(cid:88)

More surprisingly  however  one can actually send νk βk → 1 as long as νk βk → 1 slow enough 
although we require a stronger condition on the noise ξ. We extend the technique of Gupal and
Bazhenov [9] to show asymptotic convergence of QHM for minimizing smooth nonconvex functions.
Theorem 2. Let F satisfy assumption A  and additionally assume that ||ξ k||2
< C almost surely  i.e. 
the noise ξ is a.s. bounded. Let the sequences {αk}  {βk}  and {νk} satisfy the following conditions:

2
α
k

< ∞ 

αk = ∞ 

(1 − νk βk)2

< ∞ 

lim
k→∞ βk = 1.

k=0

k=0

k=0

1 − νk βk
Then the sequence {xk} generated by Algorithm (6) satisﬁes (7).
The conditions in Theorem 2 can be satisﬁed by  for example  taking αk = k−ω and (1 − νk βk) = k−c
for 1+c2 < ω ≤ 1 and 1
2 < c < 1. We should note that  even though setting νk βk → 1 is somewhat
unusual in practice  we think the result of Theorem 2 is interesting from both theoretical and practical
points of view. From the theoretical side  this result shows that it is possible to always be increasing
the amount of momentum (in the limit when νk βk = 1  we are not using the fresh gradient information
at all) and still obtain convergence for smooth functions. From the practical point of view  our
Theorem 5 in Section 5 shows that for a ﬁxed α  increasing νk βk might lead to smaller stationary
distribution size  which may give better empirical results.
Also  note that when νk = βk  Theorems 1 and 2 give asymptotic convergence guarantees for the
common practical variant of NAG  which have not appeared in the literature before. However  we
should mention that the bounded noise assumption of Theorem 2 (i.e.
< C a.s.) is quite
restrictive. In fact  Ruszczyński and Syski [32] prove a similar result for SGM with a more general
noise condition  and their technique may extend to QHM  but bounded noise greatly simpliﬁes the
derivations. We provide the proofs of Theorems 1 and 2 in Appendix B.
The results in this section indicate that both βk → 0 and νk βk → 1 are admissible from the perspective
of asymptotic convergence. However  they give limited guidance on how to choose momentum
parameters in practice  where non-asymptotic behaviors are of main concern.
In the next two
sections  we study local convergence and stationary behaviors of QHM with constant learning rate
and momentum parameters; our analysis provides new insights that could be very useful in practice.

||ξ k||2

4 Stability region and local convergence rate
Let the sequence {xk} be generated by the QHM algorithm (6) with constant parameters αk = α 
βk = β and νk = ν. In this case  xk does not converge to any local minimum in the asymptotic sense 
but its distribution may converge to a stationary distribution around a local minimum. Since the
objective function F is smooth  we can approximate F around a strict local minimum x∗ by a convex
quadratic function. Since ∇F(x∗) = 0  we have
F(x) ≈ F(x∗) + 1

2(x − x∗)T∇2F(x∗)(x − x∗)  

where the Hessian ∇2F(x∗) is positive deﬁnite. Therefore  for the ease of analysis  we focus on convex
quadratic functions of the form F(x) = (1/2)(x − x∗)T A(x − x∗)  where A is positive deﬁnite (and we
can set x∗ = 0 without loss of generality). In addition  we assume

gk = ∇F(xk) + ξ k = A(x − x∗) + ξ k 

(9)

4

where the noise ξ k satisﬁes Assumption A.3 and in addition  ξ k is independent of xk for all k ≥ 0.
Mandt et al. [19] observe that this independence assumption often holds approximately when the
dynamics of SHB are approaching stationarity around a local minimum.
Under the above assumptions  the behaviors of QHM can be described by a linear dynamical system
driven by i.i.d. noise. More speciﬁcally  let zk = [dk−1; xk − x∗] ∈ R2n be an augmented state vector 
then the dynamics of (6) can be written as (see Appendix E for details)

where T and S are functions of (α  β  ν) and A:
(1 − β)A

(cid:20) βI

(cid:21)

zk+1 = T zk + Sξ k 

T =

−αν βI

I − α(1 − ν β)A

 

S =

(cid:20)

(1 − β)I
−α(1 − ν β)I

(cid:21)

.

(10)

(11)

It is well-known that the linear system (10) is stable if and only if the spectral radius of T  denoted by
ρ(T)  is less than 1. When ρ(T) < 1  the dynamics of (10) is the superposition of two components:
• A deterministic part described by the dynamics zk+1 = T zk with initial condition z0 = [0; x0]
(we always take d−1 = 0). This part asymptotically decays to zero.
• An auto-regressive stochastic process (10) driven by {ξ k} with zero initial condition z0 = [0; 0].
Roughly speaking  ρ(T) determines how fast the dynamics converge from an arbitrary initial point
x0 to the stationary distribution  while properties of the stationary distribution (such as its variance
and auto-correlations) depends on the full spectrum of the matrix T as well as S. Both aspects have
important implications for the practical performance of QHM on stochastic optimization problems.
Often there are trade-oﬀs that we have to make in choosing the parameters α  β and ν to balance the
transient convergence behavior and stationary distribution properties.
In the rest of this section  we focus on the deterministic dynamics zk+1 = T zk to derive the conditions
on (α  β  ν) that ensure ρ(T) < 1 and characterize the convergence rate. Let λi(A) for i = 1  . . .   n
denote the eigenvalues of A (they are all real and positive). In addition  we deﬁne

µ = min
i=1 ... n

λi(A) 

L = max
i=1 ... n

λi(A) 

κ = L/µ 

κ − 1)/(√

where κ is the condition number. The local convergence rate for strictly convex quadratic functions
is well studied for the case of gradient descent (ν = 0) and heavy ball (ν = 1) [26]. In fact  heavy
ball achieves the best possible convergence rate of (√
κ + 1)[23]. Thus  it is immediately
clear that the optimal convergence rate of QHM will be the same and will be achieved with ν = 1.
However  there are no results in the literature characterizing how the optimal rate or optimal parameters
change as a function of ν. Our next result establishes the convergence region and dependence of the
convergence rate on the parameters α  β  and ν. We present the result for quadratic functions  but it
can be generalized to any L-smooth and µ-strongly convex functions  assuming the initial point x0 is
close enough to the optimal point x∗ (see Theorem 6 in Appendix C).
Theorem 3. Let’s denote θ = {α  β  ν} 1. For any function F(x) = xT Ax + bT x + c that satisﬁes
0 < µ ≤ λi(A) ≤ L for all i = 1  . . .   n and any x0  ∃{k}  with k ≥ 0  such that the deterministic
QHM algorithm zk+1 = T zk satisﬁes

(cid:13)(cid:13)xk − x∗(cid:13)(cid:13) ≤ (R(θ  µ  L) + k)k(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)  

x F(x)  limk→∞ k = 0 and R(θ  µ  L) = ρ(T)  which can be characterized as

where x∗ = arg min

R(θ  µ  L) = max {r(θ  µ)  r(θ  L)}   where

0.5(cid:16)(cid:112)C1(λ)2 − 4C2(λ) + C1(λ)(cid:17)
0.5(cid:16)(cid:112)C1(λ)2 − 4C2(λ) − C1(λ)(cid:17)
(cid:112)C2(λ)



r(θ  λ) =

C1(λ  θ) = 1 − αλ + αλν β + β  
C2(λ  θ) = β(1 − αλ + αλν) .

if C1(λ) ≥ 0  C1(λ)2 − 4C2(λ) ≥ 0  
if C1(λ) < 0  C1(λ)2 − 4C2(λ) ≥ 0  
if C1(λ)2 − 4C2(λ) < 0  

To ensure R(θ  µ  L) < 1  the parameters α  β  ν must satisfy the following constraints:

0 < α <

2(1 + β)

L(1 + β(1 − 2ν)) 

0 ≤ β < 1 

0 ≤ ν ≤ 1 .

In addition  the optimal rate depends only on κ: minθ R(θ  µ  L) is a function of only κ.

5

(12)

(13)

(a)

(b)

(c)

(d)

Figure 1: Plots (a)  (b) show the dependence of the optimal α  β and convergence rate as a function
of ν. We can see that both rate and optimal β are decreasing functions of ν. Plots (c)  (d) show
the dependence of optimal α  ν and rate on β. We can see that there are three phases in which the
dependence is quite diﬀerent. Also note that in all presented cases  changing ν required changing α
in the same way (they increase and decrease together).

The conditions in (13) characterize the stability region of QHM. Note that when ν = 0 we have
the classical result for gradient descent: α < 2/L; when ν = 1  the condition matches that of the
normalized heavy ball: α < 2(1 + β)/(L(1 − β)).
The equations (12) deﬁne the convergence rate for any ﬁxed values of the parameters α  β  ν. While it
does not give a simple analytic form  it allows us to conduct easy numerical investigations. To gain
more intuition into the eﬀect that momentum parameters ν and β have on the convergence rate  we
study how the optimal ν changes as a function of β and vice versa. To ﬁnd the optimal parameters and
rate  we solve the corresponding optimization problem numerically (using the procedure described in
Appendix D). For each pair {β  ν} we set α to the optimal value in order to remove its eﬀect. These
plots are presented in Figure 1.
A natural way to think about the interplay between parameters α  β and ν is in terms of the total
“amount of momentum”. Intuitively  it should be controlled by the product of ν × β. This intuition
helps explain Figure 1 (a)  (b)  which show the dependence of the optimal β as a function of ν for
diﬀerent values of κ. We can see that for bigger values of ν we need to use smaller values of β  since
increasing each one of them increases the “amount of momentum” in QHM. However  the same
intuition fails when considering ν as a function of β (and β is big enough)  as shown in Figure 1
(c)  (d). In this case there are 3 regimes of diﬀerent behavior. In the ﬁrst regime  since β is small 
the amount of momentum is not enough for the problem and thus the optimal ν is always 1. In this
phase we also need to increase α when increasing β (it is typical to use larger learning rate when the
momentum coeﬃcient is bigger). The second phase begins when we reach the optimal value of β
(rate is minimal) and  after that  the amount of momentum becomes too big and we need to decrease ν
and α. However  somewhat surprisingly  there is a third phase  when β becomes big enough we need
to start increasing ν and α again. Thus we can see that it’s not just the product of ν β that governs the
behavior of QHM  but a more complicated function.
Finally  based on our analytic and numerical investigations  we conjecture that the optimal convergence
rate is a monotonically decreasing function of ν (if α and β are chosen optimally for each ν). While
we can’t prove this statement2  we verify this conjecture numerically in Appendix D. The code of all
of our experiments is available at https://github.com/Kipok/understanding-momentum.

5 Stationary analysis

In this section  we study the stationary behavior of QHM with constant parameters α  β and ν. Again
we only consider quadratic functions for the same reasons as outlined in the beginning of Section 4.
In other words  we focus on the linear dynamics of (10) driven by the noise ξ k as k → ∞ (where
the deterministic part depending on x0 dies out). Under the assumptions of Section 4 we have the

following result on the covariance matrix deﬁned as Σx (cid:44) limk→∞ E(cid:2)xk(xk)T(cid:3).

1We drop the dependence of some functions on θ for brevity.
2In fact  we hypothesise that R∗(ν  κ) might not have analytical formula  since it is possible to show that the

optimization problem over α and β is equivalent to the system of highly non-linear equations.

6

0.00.51.0ν0.000.250.500.751.00κ=10R(θ κ)β∗(ν)α∗(ν)0.00.51.0ν0.000.250.500.751.00κ=1000R(θ κ)β∗(ν)α∗(ν)0.00.51.0β0.000.250.500.751.00κ=10R(θ κ)ν∗(β)α∗(β)0.00.51.0β0.000.250.500.751.00κ=1000R(θ κ)ν∗(β)α∗(β)(a) Mean loss = 0.11

(b) Mean loss = 0.01

(c) Mean loss = 0.06

(d) Mean loss = 0.15

(e) Mean loss = 0.86

(f) Mean loss = 0.06

(g) Mean loss = 0.44

(h) Mean loss = 0.68

Figure 2: Changes in the shape and size of stationary distribution changes with respect to α  β  and
ν on a 2-dimensional quadratic problem. Each picture shows the last 5000 iterates of QHM on a
contour plot. The ﬁrst picture of each row is a reference and other pictures should be compared to it.
The second pictures show how the stationary distribution changes when we decrease α. The third and
fourth show the dependence on β and ν  respectively. We can see that as expected  moving α → 0
and β → 1 always decreases the achievable loss. However  the dependence on ν is more complicated 
and for some values of α and β increasing ν increases the loss (top row)  while for other values the
dependence is reversed (bottom row). Note the scale change between top and bottom plots.

E[ξ] = 0 and covariance matrix E(cid:2)ξξT(cid:3) = Σξ. Also  suppose the parameters α  β  ν satisfy (13).

Theorem 4. Suppose F(x) = 1
2 xT Ax  where A is symmetric positive deﬁnite matrix. The stochastic
gradients satisfy gk = ∇F(xk) + ξ  where ξ is a random vector independent of xk with zero mean
Then the QHM algorithm (6)  equivalently (10) in this case  converges to a stationary distribution
satisfying
(14)
When ν = 1  this result matches the known formula for the stationary distribution of unnormalized
SHB [19] with reparametrization of α → α/(1 − β). Note that Theorem 4 shows that for the
normalized version of the algorithm  the stationary distribution’s covariance does not depend on β (or
ν) to the ﬁrst order in α. In order to explore such dependence  we need to expand the dependence
on α to the second order. In that case  we are not able to obtain a matrix equation  but can get the
following relation for tr(AΣx).
Theorem 5. Under the conditions of Theorem 4  we have

AΣx + Σx A = αAΣξ + O(α

2) .

tr(AΣx) =

α

2 tr(Σξ) +

2
α
4

1 +

2ν β
1 − β

− 1

tr(AΣξ) + O(α

3) .

(15)

(cid:18)

(cid:21)(cid:19)

(cid:20) 2ν β

1 + β

We note that tr(AΣx) is twice the mean value of F(x) when the dynamics have reached stationarity 
so the right-hand side of (15) is approximately the “achievable loss” given the values of α  β and ν. It
is interesting to consider several special cases:
• ν = 0 (SGD): tr(AΣx) = α2 tr(Σξ) + α
2
• ν = 1 (SHB): tr(AΣx) = α2 tr(Σξ) + α
• ν = β (NAG): tr(AΣx) = α2 tr(Σξ) + α

(cid:17) tr(AΣξ) + O(α

4 tr(AΣξ) + O(α
3).
1−β
1+β tr(AΣξ) + O(α

(cid:16)1 − 2β

3) .

2
4

2
4

3).

2(1+2β)
1+β

From the expressions for SHB and NAG  it might be beneﬁcial to move β to 1 during training in
order to make the achievable loss smaller. While moving β to 1 is somewhat counter-intuitive  we

7

1214161812141618α=1.0 β=0.9 ν=0.71214161812141618α=0.1 β=0.9 ν=0.71214161812141618α=1.0 β=0.99 ν=0.71214161812141618α=1.0 β=0.9 ν=1.0101520255101520α=4.0 β=0.8 ν=0.7101520255101520α=0.4 β=0.8 ν=0.7101520255101520α=4.0 β=0.95 ν=0.7101520255101520α=4.0 β=0.8 ν=1.0Figure 3: These pictures show dependence of the average ﬁnal loss (depicted with color: whiter is
smaller) on the parameters of QHM algorithm for diﬀerent problems. The top row shows results for a
synthetic 2-dimensional quadratic problem  where all the assumptions of Theorem 5 are satisﬁed.
The red curve indicates the boundary of convergence region (algorithm diverges below it). In this
case  we start the algorithm directly at the optimal value to measure the size of stationary distribution
and ignore convergence rate. We can see that as predicted by theory  smaller α and bigger β make
the ﬁnal loss smaller. The bottom row shows results of the same experiments repeated for logistic
regression on MNIST and ResNet-18 on the CIFAR-10 dataset. We can see that while the assumptions
of Theorem 5 are no longer valid  QHM still shows similar qualitative behavior.

proved in Section 3 that QHM still converges asymptotically in this regime  assuming ν also goes to
1 and ν β converges to 1 “slower” than α converges to 0. However  since we only consider Taylor
expansion in α  there is no guarantee that the approximation remains accurate when ν and β converge
to 1 (see Appendix G for evaluation of this approximation error). In order to precisely investigate the
dependence on β and ν  it is necessary to further extend our results by considering Taylor expansion
with respect to them as well  especially in terms of 1 − β. We leave this for future work.
Figure 2 shows a visualization of the QHM stationary distribution on a 2-dimensional quadratic
problem. We can see that our prediction about the dependence on α and β holds in this case. However 
the dependence on ν is more complicated: the top and bottom rows of Figure 2 show opposite behavior.
Comparing this experiment with our analysis of the convergence rate (Figure 1) we can see another
conﬁrmation that for big values of β  increasing ν can  in a sense  decrease the “amount of momentum”
in the system. Next  we evaluate the average ﬁnal loss for a large grid of parameters α  β and ν on
three problems: a 2-dimensional quadratic function (where all of our assumptions are satisﬁed) 
logistic regression on the MNIST [16] dataset (where the quadratic assumption is approximately
satisﬁed  but gradient noise comes from mini-batches) and ResNet-18 [10] on CIFAR-10 [13] (where
all of our assumptions are likely violated). Figure 3 shows the results of this experiment. We can
indeed see that β → 1 and α → 0 make the ﬁnal loss smaller in all cases. The dependence on ν is
less clear  but we can see that for large values of β it is approximately quadratic  with a minimum at
some ν < 1. Thus from this point of view ν (cid:44) 1 helps when β is big enough  which might be one of
the reasons for the empirical success of the QHM algorithm. Notice that the empirical dependence on
ν is qualitatively the same as predicted by formula (15)  but with optimal value shifted closer to 1.
See Appendix F for details.

6 Some practical implications and guidelines
In this section  we present some practical implications and guidelines for setting learning rate and
momentum parameters in practical machine learning applications. In particular  we consider the
question of how to set the optimal parameters in each stage of the popular constant-and-drop scheme
for deep learning. We argue that in order to answer this question  it is necessary to consider both

8

0.51.01.5α0.00.20.40.60.81.0βQuadraticfunction(ν=1.0)0.000.120.240.360.480.600.720.840.960.51.01.5α0.00.20.40.60.81.0βQuadraticfunction(ν=0.7)0.000.060.120.180.240.300.360.420.480.20.40.60.81.0ν0.00.20.40.60.81.0βQuadraticfunction(α=0.3)0.000.060.120.180.240.300.360.420.48051015α0.20.40.60.81.0βLRonMNIST(ν=1.0)0.150.601.051.501.952.402.853.303.750102030α0.20.40.60.81.0νLRonMNIST(β=0.95)0.01.22.43.64.86.07.28.49.62468α0.00.20.40.60.81.0βResNet-18onCIFAR-10(ν=1.0)0.270.330.390.450.510.570.630.690.750.81(a)

(b)

(c)

Figure 4:
(a) This plot shows a trade-oﬀ between stationary distribution size (ﬁnal loss) and
convergence rate on a simple 2-dimensional quadratic problem. Algorithms that converge faster 
typically will converge to a higher ﬁnal loss. (b) This plot illustrates the regime where there is no
trade-oﬀ between stationary distribution size and convergence rate. Larger values of α don’t change
the convergence rate  while making ﬁnal loss signiﬁcantly higher. To make plots (a) and (b) smoother 
we plot the average value of the loss for each 100 iterations on y-axis. (c) This plot shows that the
same behavior can also be observed in training deep neural networks. For all plots β = 0.9  ν = 1.0.
All of the presented results depend continuously on the algorithm’s parameters (e.g. the transition
between behaviours shown in (a) and (b) is smooth).

convergence rate and stationary distribution perspectives. There is typically a trade-oﬀ between
obtaining a fast rate and a small stationary distribution. You can see an illustration of this trade-oﬀ
in Figure 4 (a). Interestingly  by combining stationary analysis of Section 5 and results for the
convergence rate (3)  we can ﬁnd certain regimes of parameters α  β  and ν where the ﬁnal loss and
the convergence speed do not compete with each other.
One of the most important of these regimes happens in the case of the SHB algorithm (ν = 1). In that
case  we can see that when C2
β and does
not depend on α. Thus  as long as this inequality is satisﬁed  we can set α as small as possible and it
would not harm the convergence rate  but will decrease the size of stationary distribution. To get the
best possible convergence rate  we  in fact  have to set α and β in such a way that this inequality will
turn into equality and thus there will be only a single value of α that could be used. However  as long
as β is not exactly at the optimal value  there is going to be some freedom in choosing α and it should
be used to decrease the size of stationary distribution. From this point of view  the optimal value

β(cid:1)(cid:17)  which will be smaller then the largest possible α for convergence

2(l) ≤ 0  l ∈ {µ  L}  the convergence rate equals √

1(l) − C2

as long as κ > 2 and β is set close to 1 (see proof of Theorem 3 for more details). This guideline
contradicts some typical advice to set α as big as possible while algorithm still converges3. The
reﬁned guideline for the constant-and-drop scheme would be to set α as small as possible until the
convergence noticeably slows down. You can see an illustration of this behavior on a simple quadratic
problem (Figure 4 (b))  as well as for ResNet-18 on CIFAR-10 (Figure 4 (c)). Such regimes of no
trade-oﬀ can be identiﬁed for β and ν as well.

of α =(cid:0)1 − √

β(cid:1)(cid:14)(cid:16)

µ(cid:0)1 +

√

7 Conclusion

Using the general formulation of QHM  we have derived a uniﬁed set of new analytic results that give
us better understanding of the role of momentum in stochastic gradient methods. Our results cover
several diﬀerent aspects: asymptotic convergence  stability region and local convergence rate  and
characterizations of stationary distribution. We show that it is important to consider these diﬀerent
aspects together to understand the key trade-oﬀs in tuning the learning rate and momentum parameters
for better performance in practice. On the other hand  we note that the obtained guidelines are mainly
for stochastic optimization  meaning the minimization of the training loss. There is evidence that
diﬀerent heuristics and guidelines may be necessary for achieving better generalization performance
in machine learning  but this topic is beyond the scope of our current paper.

3For example [8] says “So set β as close to 1 as you can  and then ﬁnd the highest α which still converges.

Being at the knife’s edge of divergence  like in gradient descent  is a good place to be.”

9

05000100001500020000iteration10−2101104averagelossQuadraticfunctionα=0.005α=0.02α=0.10200040006000iteration100102104averagelossQuadraticfunctionα=1.0α=2.0α=3.0050010001500iteration246traininglossResNet-18onCIFAR-10α=2.0α=5.0α=8.5References
[1] Wangpeng An  Haoqian Wang  Qingyun Sun  Jun Xu  Qionghai Dai  and Lei Zhang. A pid
controller approach for stochastic optimization of deep networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 8522–8531  2018.

[2] Yoshua Bengio  Nicolas Boulanger-Lewandowski  and Razvan Pascanu. Advances in optimizing
recurrent networks. In 2013 IEEE International Conference on Acoustics  Speech and Signal
Processing  pages 8624–8628. IEEE  2013.

[3] Saman Cyrus  Bin Hu  Bryan Van Scoy  and Laurent Lessard. A robust accelerated optimization
algorithm for strongly convex functions. In 2018 Annual American Control Conference (ACC) 
pages 1376–1381. IEEE  2018.

[4] Aymeric Dieuleveut  Alain Durmus  and Francis Bach. Bridging the gap between constant step

size stochastic gradient descent and markov chains. arXiv preprint arXiv:1707.06386  2017.

[5] Yu M Ermoliev. On the stochastic quasi-gradient method and stochastic quasi-feyer sequences.

Kibernetika  2:72–83  1969.

[6] Mark Iosifovich Freidlin and Alexander D Wentzell. Random perturbations.

perturbations of dynamical systems  pages 15–43. Springer  1998.

In Random

[7] Sébastien Gadat  Fabien Panloup  Soﬁane Saadane  et al. Stochastic heavy ball. Electronic

Journal of Statistics  12(1):461–529  2018.

[8] Gabriel Goh. Why momentum really works. Distill  2017. doi: 10.23915/distill.00006. URL

http://distill.pub/2017/momentum.

[9] A. M. Gupal and L. T. Bazhenov. A stochastic analog of the conjugate gradient method.

Cybernetics  8(1):138–140  1972.

[10] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[11] Yu. M. Kaniovski. Behaviour in the limit of iterations of the stochastic two-step method. USSR

Computational Mathematics and Mathematical Physics  23(1):8–13  1983.

[12] Rahul Kidambi  Praneeth Netrapalli  Prateek Jain  and Sham Kakade. On the insuﬃciency
of existing momentum schemes for stochastic optimization. In 2018 Information Theory and
Applications Workshop (ITA)  pages 1–9. IEEE  2018.

[13] Alex Krizhevsky and Geoﬀrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[14] Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and

Applications. Springer  2nd edition  2003.

[15] Hunter Lang  Pengchuan Zhang  and Lin Xiao. Statistical adaptive stochastic approximation.

2019.

[16] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ 

1998.

[17] Laurent Lessard  Benjamin Recht  and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization  26(1):57–95 
2016.

[18] Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for deep learning. In

International Conference on Learning Representations  2019.

[19] Stephan Mandt  Matthew D Hoﬀman  and David M Blei. Stochastic gradient descent as
approximate bayesian inference. The Journal of Machine Learning Research  18(1):4873–4907 
2017.

10

[20] Paul-André Meyer. Martingales and stochastic integrals I  volume 284 of Lecture notes in

mathematics. Springer-Verlag  1972.

[21] Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method

eﬃciency in optimization. 1983.

[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate

o(1/k2). In Soviet Math. Dokl  volume 27  1983.

[23] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Kluwer Academic

Publishers  2004.

[24] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming  140(1):125–161  2013.

[25] Georg Ch. Pﬂug. On the determination of the step size in stochastic quasigradient methods.
Collaborative Paper CP-83-025  International Institute for Applied Systems Analysis (IIASA) 
Laxenburg  Austria  1983.

[26] Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics  4(5):1–17  1964.

[27] Boris T. Polyak. Comparison of the rates of convergence of one-step and multi-step optimization

algorithms in the presence of noise. Engineering Cybernetics  15:6–10  1977.

[28] Boris T Polyak. Introduction to optimization. optimization software. Inc.  Publications Division 

New York  1  1987.

[29] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi

Matematiki i Matematicheskoi Fiziki  3(4):643–653  1963.

[30] Benjamin Recht. Cs726-lyapunov analysis and the heavy ball method. 2010.
[31] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics  pages 400–407  1951.

[32] Andrzej Ruszczyński and Wojciech Syski. Stochastic approximation method with gradient
IEEE Transactions on Automatic Control  28(12):

averaging for unconstrained problems.
1097–1105  1983.

[33] Andrzej Ruszczyński and Wojciech Syski. Stochastic approximation algorithm with gradient
averaging and on-line stepsize rules. In J. Gertler and L. Keviczky  editors  Proceedings of 9th
IFAC World Congress  pages 1023–1027  Budapest  Hungary  1984.

[34] Ilya Sutskever  James Martens  George Dahl  and Geoﬀrey Hinton. On the importance of
initialization and momentum in deep learning. In Sanjoy Dasgupta and David McAllester 
editors  Proceedings of the 30th International Conference on Machine Learning  volume 28 of
Proceedings of Machine Learning Research  pages 1139–1147  Atlanta  Georgia  USA  17–19
Jun 2013. PMLR.

[35] Ole Tange et al. Gnu parallel-the command-line power tool. The USENIX Magazine  36(1):

42–47  2011.

[36] Bryan Van Scoy  Randy A Freeman  and Kevin M Lynch. The fastest known globally convergent
ﬁrst-order method for minimizing strongly convex functions. IEEE Control Systems Letters  2
(1):49–54  2017.

[37] M. T. Wasan. Stochastic Approximation. Cambridge University Press  1969.
[38] David Williams. Probability with Martingales. Cambridge University Press  1991.
[39] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint

arXiv:1810.00004  2018.

[40] Tianbao Yang  Qihang Lin  and Zhe Li. Uniﬁed convergence analysis of stochastic momentum

methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257  2016.

11

,Igor Gitman
Hunter Lang
Pengchuan Zhang
Lin Xiao