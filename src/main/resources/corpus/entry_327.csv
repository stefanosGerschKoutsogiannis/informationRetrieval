2017,Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex,In this paper we study the well-known greedy coordinate descent (GCD)  algorithm to solve $\ell_1$-regularized problems and improve GCD by the two popular strategies: Nesterov's acceleration and stochastic optimization. Firstly  we propose a new rule for greedy selection based on an $\ell_1$-norm square approximation  which is nontrivial to solve but convex; then an efficient algorithm called ``SOft ThreshOlding PrOjection (SOTOPO)'' is proposed to exactly solve the $\ell_1$-regularized $\ell_1$-norm square approximation problem  which is induced by the new rule.  Based on the new rule and the SOTOPO algorithm  the Nesterov's acceleration and stochastic optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD) has the optimal convergence rate $O(\sqrt{1/\epsilon})$;  meanwhile  it reduces the iteration complexity of greedy selection up to a factor of sample size. Both theoretically and empirically  we show that ASGCD has better performance for high-dimensional and dense problems with sparse solution.,Accelerated Stochastic Greedy Coordinate Descent by

Soft Thresholding Projection onto Simplex

Chaobing Song  Shaobo Cui  Yong Jiang  Shu-Tao Xia

Tsinghua University

{songcb16 shaobocui16}@mails.tsinghua.edu.cn

{jiangy  xiast}@sz.tsinghua.edu.cn ⇤

Abstract

In this paper we study the well-known greedy coordinate descent (GCD) algorithm
to solve `1-regularized problems and improve GCD by the two popular strategies:
Nesterov’s acceleration and stochastic optimization. Firstly  based on an `1-norm
square approximation  we propose a new rule for greedy selection which is non-
trivial to solve but convex; then an efﬁcient algorithm called “SOft ThreshOlding
PrOjection (SOTOPO)” is proposed to exactly solve an `1-regularized `1-norm
square approximation problem  which is induced by the new rule. Based on the
new rule and the SOTOPO algorithm  the Nesterov’s acceleration and stochastic
optimization strategies are then successfully applied to the GCD algorithm. The re-
sulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD)

has the optimal convergence rate O(p1/✏); meanwhile  it reduces the iteration

complexity of greedy selection up to a factor of sample size. Both theoretically and
empirically  we show that ASGCD has better performance for high-dimensional
and dense problems with sparse solutions.

1

Introduction

In large-scale convex optimization  ﬁrst-order methods are widely used due to their cheap iteration
cost. In order to improve the convergence rate and reduce the iteration cost further  two important
strategies are used in ﬁrst-order methods: Nesterov’s acceleration and stochastic optimization.
Nesterov’s acceleration is referred to the technique that uses some algebra trick to accelerate ﬁrst-
order algorithms; while stochastic optimization is referred to the method that samples one training
example or one dual coordinate at random from the training data in each iteration. Assume the
objective function F (x) is convex and smooth. Let F ⇤ = minx2Rd F (x) be the optimal value. In
order to ﬁnd an approximate solution x that satisﬁes F (x)  F ⇤  ✏  the vanilla gradient descent
method needs O(1/✏) iterations. While after applying the Nesterov’s acceleration scheme [18] 
the resulted accelerated full gradient method (AFG) [18] only needs O(p1/✏) iterations  which is
by a factor of the sample size [17] and obtain the optimal convergence rate O(p1/✏) by Nesterov’s

optimal for ﬁrst-order algorithms [18]. Meanwhile  assume F (x) is also a ﬁnite sum of n sample
convex functions. By sampling one training example  the resulted stochastic gradient descent (SGD)
and its variants [15  25  1] can reduce the iteration complexity by a factor of the sample size. As an
alternative of SGD  randomized coordinate descent (RCD) can also reduce the iteration complexity

acceleration [16  14]. The development of gradient descent and RCD raises an interesting problem:
can the Nesterov’s acceleration and stochastic optimization strategies be used to improve other
existing ﬁrst-order algorithms?

⇤This work is supported by the National Natural Science Foundation of China under grant Nos. 61771273 

61371078.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In this paper  we answer this question partly by studying coordinate descent with Gauss-Southwell
selection  i.e.  greedy coordinate descent (GCD). GCD is widely used for solving sparse optimization
problems in machine learning [24  11  19]. If an optimization problem has a sparse solution  it is
more suitable than its counterpart RCD. However  the theoretical convergence rate is still O(1/✏).
Meanwhile if the iteration complexity is comparable  GCD will be preferable than RCD [19]. However
in the general case  in order to do exact Gauss-Southwell selection  computing the full gradient
beforehand is necessary  which causes GCD has much higher iteration complexity than RCD. To be
concrete  in this paper we consider the well-known nonsmooth `1-regularized problem:

def

def
=

def

1
n

(1)

min

2 (bj  aT

= f (x) + kxk1

x2RdnF (x)

fj(x) + kxk1o 

nXj=1
nPn
j=1 fj(x) is a smooth convex function that is
where   0 is a regularization parameter  f (x) = 1
a ﬁnite average of n smooth convex function fj(x). Given samples {(a1  b1)  (a2  b2)  . . .   (an  bn)}
with aj 2 Rd (j 2 [n]
j x  bj)  then (1) is an `1-regularized
= {1  2  . . .   n})  if each fj(x) = fj(aT
empirical risk minimization (`1-ERM) problem. For example  if bj 2 R and fj(x) = 1
j x)2 
j x))  `1-regularized logistic regression
(1) is Lasso; if bj 2 {1  1} and fj(x) = log(1 + exp(bjaT
is obtained.
In the above nonsmooth case  the Gauss-Southwell rule has 3 different variants [19  24]: GS-s  GS-r
and GS-q. The GCD algorithm with all the 3 rules can be viewed as the following procedure: in
each iteration based on a quadratic approximation of f (x) in (1)  one minimizes a surrogate objective
function under the constraint that the direction vector used for update has at most 1 nonzero entry.
The resulted problems under the 3 rules are easy to solve but are nonconvex due to the cardinality
constraint of direction vector. While when using Nesterov’s acceleration scheme  convexity is needed

for the derivation of the optimal convergence rate O(p1/✏) [18]. Therefore  it is impossible to

accelerate GCD by the Nesterov’s acceleration scheme under the 3 existing rules.
In this paper  we propose a novel variant of Gauss-Southwell rule by using an `1-norm square
approximation of f (x) rather than quadratic approximation. The new rule involves an `1-regularized
`1-norm square approximation problem  which is nontrivial to solve but is convex. To exactly
solve the challenging problem  we propose an efﬁcient SOft ThreshOlding PrOjection (SOTOPO)
algorithm. The SOTOPO algorithm has O(d + |Q| log |Q|) cost  where it is often the case |Q|⌧ d.
The complexity result O(d + |Q| log |Q|) is better than O(d log d) of its counterpart SOPOPO [20] 
which is an Euclidean projection method.
Then based on the new rule and SOTOPO  we accelerate GCD to attain the optimal convergence rate
O(p1/✏) by combing a delicately selected mirror descent step. Meanwhile  we show that it is not

necessary to compute full gradient beforehand: sampling one training example and computing a noisy
gradient rather than full gradient is enough to perform greedy selection. This stochastic optimization
technique reduces the iteration complexity of greedy selection by a factor of the sample size. The
ﬁnal result is an accelerated stochastic greedy coordinate descent (ASGCD) algorithm.
Assume x⇤ is an optimal solution of (1). Assume that each fj(x)(for all j 2 [n]) is Lp-smooth w.r.t.
k·k p (p = 1  2)  i.e.  for all x  y 2 Rd 

2

krfj(x)  rfj(y)kq  Lpkx  ykp 

(2)

where if p = 1  then q = 1; if p = 2  then q = 2.
In order to ﬁnd an x that satisﬁes F (x)  F (x⇤)  ✏  ASGCD needs O⇣pCL1kx⇤k1

(16))  where C is a function of d that varies slowly over d and is upper bounded by log2(d). For
high-dimensional and dense problems with sparse solutions  ASGCD has better performance than the
state of the art. Experiments demonstrate the theoretical result.
Notations: Let [d] denote the set {1  2  . . .   d}. Let R+ denote the set of nonnegative real number. For
x 2 Rd  let kxkp = (Pd
p (1  p < 1) denote the `p-norm and kxk1 = maxi2[d] |xi|
denote the `1-norm of x. For a vector x  let dim(x) denote the dimension of x; let xi denote the i-th
element of x. For a gradient vector rf (x)  let rif (x) denote the i-th element of rf (x). For a set
S  let |S| denote the cardinality of S. Denote the simplex 4d = {✓ 2 Rd

⌘ iterations (see

i=1 ✓i = 1}.

i=1 |xi|p)

p✏

1

+ :Pd

2 The SOTOPO algorithm

The proposed SOTOPO algorithm aims to solve the proposed new rule  i.e.  minimize the following
`1-regularized `1-norm square approximation problem 

˜h

def
= arg min

g2Rd ⇢hrf (x)  gi +

1
2⌘kgk2

1 + kx + gk1  

(3)

def
= x + ˜h 

˜x

(4)
where x denotes the current iteration  ⌘ a step size  g the variable to optimize  ˜h the director vector for
update and ˜x the next iteration. The number of nonzero entries of ˜h denotes how many coordinates
will be updated in this iteration. Unlike the quadratic approximation used in GS-s  GS-r and GS-q
rules  in the new rule the coordinate(s) to update is implicitly selected by the sparsity-inducing
property of the `1-norm square kgk2
1 rather than using the cardinality constraint kgk0  1 (i.e.  g
has at most 1 nonzero element) [19  24]. By [8  §9.4.2]  when the nonsmooth term kx + gk1 in (1)
does not exist  the minimizer of the `1-norm square approximation (i.e.  `1-norm steepest descent)
is equivalent to GCD. When kx + gk1 exists  generally  there may be one or more coordinates to
update in this new rule. Because of the sparsity-inducing property of kgk2
1 and kx + gk1  both the
direction vector ˜h and the iterative solution ˜x are sparse. In addition  (3) is an unconstrained problem
and thus is feasible.

2.1 A variational reformulation and its properties
(3) involves the nonseparable  nonsmooth term kgk2
1 and the nonsmooth term kx + gk1. Because
there are two nonsmooth terms  it seems difﬁcult to solve (3) directly. While by the variational
in [5] 2  in Lemma 1  it is shown that we can transform the
identity kgk2
original nonseparable and nonsmooth problem into a separable and smooth optimization problem on
a simplex.
Lemma 1. By deﬁning

1 = inf ✓24dPd

g2
i
✓i

i=1

J(g  ✓)

def

= hrf (x)  gi +

1
2⌘

˜g(✓)
˜✓

def
= arg ming2Rd J(g  ✓) 
def
= arg inf ✓24d J(✓) 

dXi=1

g2
i
✓i

+ kx + gk1 

J(✓)

def
= J(˜g(✓) ✓ ) 

(5)

(6)

(7)

where ˜g(✓) is a vector function. Then the minimization problem to ﬁnd ˜h in (3) is equivalent to the
problem (7) to ﬁnd ˜✓ with the relation ˜h = ˜g(˜✓). Meanwhile  ˜g(✓) and J(✓) in (6) are both coordinate
separable with the expressions

def

8i 2 [d]  ˜gi(✓) = ˜gi(✓i)
J(✓) =

= sign(xi  ✓i⌘rif (x)) · max{0 |xi  ✓i⌘rif (x)| ✓i⌘} xi  (8)
(9)

def

Ji(✓i)  where Ji(✓i)

= rif (x) · ˜gi(✓i) +

+ |xi + ˜gi(✓i)|.

1
2⌘

dXi=1

˜g2
i (✓i)
✓i

dXi=1

In Lemma 1  (8) is obtained by the iterative soft thresholding operator [7]. By Lemma 1  we can
reformulate (3) into the problem (5)  which is about two parameters g and ✓. Then by the joint
convexity  we swap the optimization order of g and ✓. Fixing ✓ and optimizing with respect to (w.r.t.)
g  we can get a closed form of ˜g(✓)  which is a vector function about ✓. Substituting ˜g(✓) into J(g  ✓) 
we get the problem (7) about ✓. Finally  the optimal solution ˜h in (3) can be obtained by ˜h = ˜g(˜✓).
The explicit expression of each Ji(✓i) can be given by substituting (8) into (9). Because ✓ 2 4d  we
have for all i 2 [d]  0  ✓i  1. In the following Lemma 2  it is observed that the derivate J0i(✓i) can
be a constant or have a piecewise structure  which is the key to deduce the SOTOPO algorithm.

2The inﬁma can be replaced by minimization if the convention “0/0 = 0” is used.

3

def
=

and ri2

  then J0i(✓i) belongs to one of the 4 cases 

Lemma 2. Assume that for all i 2 [d]  J0i(0) and J0i(1) have been computed. Denote ri1
|xi|p2⌘J 0i(0)
|xi|p2⌘J 0i(1)
(case a) : J0i(✓i) = 0 
(case c) : J0i(✓i) =(J0i(0) 
 x2

(case b) : J0i(✓i) = J0i(0) < 0 

0  ✓i  ri1
ri1 <✓ i  1

0  ✓i  1 

0  ✓i  1 
0  ✓i  ri1
ri1 <✓ i < ri2
ri2  ✓i  1

(case d) : J0i(✓i) =8><>:

Although the formulation of J0i(✓i) is complicated  by summarizing the property of the 4 cases in
Lemma 2  we have Corollary 1.
Corollary 1. For all i 2 [d] and 0  ✓i  1  if the derivate J0i(✓i) is not always 0  then J0i(✓i) is a
non-decreasing  continuous function with value always less than 0.
Corollary 1 shows that except the trivial (case a)  for all i 2 [d]  whichever J0i(✓i) belong to (case b) 
(case c) or case (d)  they all share the same group of properties  which makes a consistent iterative
procedure possible for all the cases. The different formulations in the four cases mainly have impact
about the stopping criteria of SOTOPO.

J0i(0) 
 x2
2⌘✓2
i
J0i(1) 

i

i

2⌘✓2
i

 

 

 

def
=

.

2.2 The property of the optimal solution
The Lagrangian of the problem (7) is

L(✓    ⇣ )

def

= J(✓) + ⇣ dXi=1

✓i  1⌘  h⇣ ✓i 

(10)

where  2 R is a Lagrange multiplier and ⇣ 2 Rd
Due to the coordinate separable property of J(✓) in (9)  it follows that @J (✓)
@✓i
KKT condition of (10) can be written as

+ is a vector of non-negative Lagrange multipliers.
= J0i(✓i). Then the

dXi=1

8i 2 [d] 

J0i(✓i) +   ⇣i = 0 ⇣

i✓i = 0 

and

✓i = 1.

(11)

def

By reformulating the KKT condition (11)  we have Lemma 3.
Lemma 3. If (˜  ˜✓  ˜⇣) is a stationary point of (10)  then ˜✓ is an optimal solution of (7). Meanwhile 
denote S

= {i : ˜✓i > 0} and T
= {j : ˜✓j = 0}  then the KKT condition can be formulated as
Pi2S
˜✓i = 1;
for all j 2 T 
for all i 2 S 

˜✓j = 0;
˜ = J0i(˜✓i)  maxj2T J0j(0).

(12)

def

By Lemma 3  if the set S in Lemma 3 is known beforehand  then we can compute ˜✓ by simply
applying the equations in (12). Therefore ﬁnding the optimal solution ˜✓ is equivalent to ﬁnding the
set of the nonzero elements of ˜✓.

8<:

2.3 The soft thresholding projection algorithm
In Lemma 3  for each i 2 [d] with ˜✓i > 0  it is shown that the negative derivate J0i(˜✓i) is equal to a
single variable ˜. Therefore  a much simpler problem can be obtained if we know the coordinates of
these positive elements. At ﬁrst glance  it seems difﬁcult to identify these coordinates  because the
number of potential subsets of coordinates is clearly exponential on the dimension d. However  the
property clariﬁed by Lemma 2 enables an efﬁcient procedure for identifying the nonzero elements of
˜✓. Lemma 4 is a key tool in deriving the procedure for identifying the non-zero elements of ˜✓.
Lemma 4 (Nonzero element identiﬁcation). Let ˜✓ be an optimal solution of (7). Let s and t be two
coordinates such that J0s(0) < J0t(0). If ˜✓s = 0  then ˜✓t must be 0 as well; equivalently  if ˜✓t > 0 
then ˜✓s must be greater than 0 as well.

4

def

= rJ(0) such that ui1  ui2 ··· uid  where {i1  i2  . . .   id}
Lemma 4 shows that if we sort u
is a permutation of [d]  then the set S in Lemma 3 is of the form {i1  i2  . . .   i%}  where 1  %  d.
If % is obtained  then we can use the fact that for all j 2 [%] 
%Xj=1

to compute ˜. Therefore  by Lemma 4  we can efﬁciently identify the nonzero elements of the optimal
solution ˜✓ after a sort operation  which costs O(d log d). However based on Lemmas 2 and 3  the sort
cost O(d log d) can be further reduced by the following Lemma 5.
Lemma 5 (Efﬁcient identiﬁcation). Assume ˜✓ and S are given in Lemma 3. Then for all i 2 S 

J0ij (˜✓ij ) = ˜

˜✓ij = 1

(13)

and

(14)

J0i(0)  max

j2[d]{J0j(1)}.

By Lemma 5  before ordering u  we can ﬁlter out all the coordinates i’s that satisfy J0i(0) <
maxj2[d] J0j(1). Based on Lemmas 4 and 5  we propose the SOft ThreshOlding PrOjection
(SOTOPO) algorithm in Alg. 1 to efﬁciently obtain an optimal solution ˜✓. In the step 1  by Lemma 5 
we ﬁnd the quantity vm  im and Q. In the step 2  by Lemma 4  we sort the elements {J0i(0)| i 2 Q}.
In the step 3  because S in Lemma 3 is of the form {i1  i2  . . .   i%}  we search the quantity ⇢ from
1 to |Q| + 1 until a stopping criteria is met. In Alg. 1  the number of nonzero elements of ˜✓ is ⇢ or
⇢  1. In the step 4  we compute the ˜ in Lemma 3 according to the conditions. In the step 5  the
optimal ˜✓ and the corresponding ˜h  ˜x are given.

Algorithm 1 ˜x =SOTOPO(rf (x)  x  ⌘ )

1. Find

(vm  im)

def

= (maxi2[d]{J0i(1)}  arg maxi2[d]{J0i(1)})  Q

def

= {i 2 [d]| J0i(0) > vm}.
(0)  where

2. Sort {J0i(0)| i 2 Q} such that J0i1(0)  J0i2(0) ···  J0i|Q|

{i1  i2  . . .   i|Q|} is a permutation of the elements in Q. Denote
i|Q|+1
v

= (J0i1(0) J0i2(0)  . . .  J0i|Q|

(0)  vm) 

and

def

def
= im  v|Q|+1

def
= vm.

3. For j 2 [|Q| + 1]  denote Rj = {ik|k 2 [j]}. Search from 1 to |Q| + 1 to ﬁnd the quantity

= minj 2 [|Q| + 1]| J0ij (0) = J0ij (1) or Xl2Rj |xl| p2⌘vj or j = |Q| + 1 .

def

⇢

4. The ˜ in Lemma 3 is given by

/(2⌘) 

if Pl2R⇢1 |xl| p2⌘v⇢;

otherwise.

v⇢ 

˜ =(⇣Pl2R⇢1 |xl|⌘2
(˜✓l  ˜hl  ˜xl) =8><>:
 |xl|p2⌘˜  xl  0 
1 Pk2 R⇢\{i⇢}

(0  0  xl) 

5. Then the ˜✓ in Lemma 3 and its corresponding ˜h  ˜x in (3) and (4) are obtained by
if l 2 R⇢\{i⇢};
if l = i⇢;
if l 2 [d]\R⇢.

˜✓k  ˜gl(˜✓l)  xl + ˜gl(˜✓l) 

In Theorem 1  we give the main result about the SOTOPO algorithm.
Theorem 1. The SOTOPO algorihtm in Alg. 1 can get the exact minimizer ˜h  ˜x of the `1-regularized
`1-norm square approximation problem in (3) and (4).
The SOTOPO algorithm seems complicated but is indeed efﬁcient. The dominant operations in Alg.
1 are steps 1 and 2 with the total cost O(d + |Q| log |Q|). To show the effect of the complexity
reduction by Lemma 5  we give the following fact.

5

Proposition 1. For the optimization problem deﬁned in (5)-(7)  where  is the regularization param-
eter of the original problem (1)  we have that

⌘

0  max

i2[d](s2J0i(0)

Assume vm is deﬁned in the step 1 of Alg. 1. By Proposition 1  for all i 2 Q 

j2[d]8<:
⌘ 9=;  2.
s2J0j(1)
)  max
j2[d]8<:
⌘ 9=;
s2J0j(1)
)  max
+ 2 =r 2vm
Therefore at least the coordinates j’s that satisfyq2J0j (0)
>q 2vm

k2[d](s2J0k(0)

s2J0i(0)

⌘

 max

⌘

⌘

⌘

Q. In practice  it can considerably reduce the sort complexity.
Remark 1. SOTOPO can be viewed as an extension of the SOPOPO algorithm [20] by changing the
objective function from Euclidean distance to a more general function J(✓) in (9). It should be noted
that Lemma 5 does not have a counterpart in the case that the objective function is Euclidean distance
[20]. In addition  some extension of randomized median ﬁnding algorithm [12] with linear time in
our setting is also deserved to research. Due to the limited space  it is left for further discussion.

(15)

+ 2 

⌘ + 2 will be not contained in

3 The ASGCD algorithm

Now we can come back to our motivation  i.e.  accelerating GCD to obtain the optimal convergence
rate O(1/p✏) by Nesterov’s acceleration and reducing the complexity of greedy selection by stochas-
tic optimization. The main idea is that although like any (block) coordinate descent algorithm  the
proposed new rule  i.e.  minimizing the problem in (3)  performs update on one or several coordinates 
it is a generalized proximal gradient descent problem based on `1-norm. Therefore this rule can be
applied into the existing Nesterov’s acceleration and stochastic optimization framework “Katyusha”
[1] if it can be solved efﬁciently. The ﬁnal result is the accelerated stochastic greedy coordinate
descent (ASGCD) algorithm  which is described in Alg. 2.

Algorithm 2 ASGCD

 = log(d)  1 p(log(d)  1)2  1;

;

2
1+


p = 1 +   q = p
p1   C = d
z0 = y0 = ˜x0 = #0 = 0;
⌧2 = 1
b e ⌘ =
for s = 0  1  2  . . .   S  1  do
s+4  ↵ s = ⌘

2   m = d n

(1+2 nb

1
b(n1) )L1

;

⌧1 sC ;

1. ⌧1 s = 2
2. µs = rf (˜xs);
3. for l = 0  1  . . .   m  1  do

(a) k = (sm) + l;
(b) randomly sample a mini batch B of size b from {1  2  . . .   n} with equal probability;
(c) xk+1 = ⌧1 szk + ⌧2 ˜xs + (1  ⌧1 s  ⌧2)yk;
bPj2B(rfj(xk+1)  rfj(˜xs));
(d) ˜rk+1 = µs + 1
(e) yk+1 =SOTOPO( ˜rk+1  xk+1  ⌘ );
(f) (zk+1 # k+1) = pCOMID( ˜rk+1 # k  q  ↵ s);
end for

4. ˜xs+1 = 1

l=1 ysm+l;

mPm

end for
Output: ˜xS

6

Algorithm 3 (˜x  ˜#) = pCOMID(g  #  q    ↵ )

1. 8i 2 [d]  ˜#i = sign(#i  ↵gi) · max{0 |#i  ↵gi| ↵};
2. 8i 2 [d]  ˜xi = sign( ˜#i)|˜✓i|q1
3. Output: ˜x  ˜#.

k ˜#kq2

;

q

4

2m

p✏

1

S2

2
1+


(1+2 nb

1 + 2(b)

1
b(n1) )L1

In Alg. 2  the gradient descent step 3(e) is solved by the proposed SOTOPO algorithm  while the
mirror descent step 3(f ) is solved by the COMID algorithm with p-norm divergence [13  Sec. 7.2].
We denote the mirror descent step as pCOMID in Alg. 3. All other parts are standard steps in the
Katyusha framework except some parameter settings. For example  instead of the custom setting
p = 1 + 1/log(d) [21  13]  a particular choice p = 1 +  ( is deﬁned in Alg. 2) is used to minimize
. C varies slowly over d and is upper bounded by log2(d). Meanwhile  ↵k+1 depends
the C = d
on the extra constant C. Furthermore  the step size ⌘ =
is used  where L1 is deﬁned
in (2). Finally  unlike [1  Alg. 2]  we let the batch size b as an algorithm parameter to cover both the
stochastic case b < n and the deterministic case b = n. To the best of our knowledge  the existing
GCD algorithms are deterministic  therefore by setting b = n  we can compare with the existing
GCD algorithms better.
Based on the efﬁcient SOTOPO algorithm  ASGCD has nearly the same iteration complexity with
the standard form [1  Alg. 2] of Katyusha. Meanwhile we have the following convergence rate.
Theorem 2. If each fj(x)(j 2 [n]) is convex  L1-smooth in (2) and x⇤ is an optimum of the
`1-regularized problem (1)  then ASGCD satisﬁes
◆   (16)
b(n1)  S  b  m and C are given in Alg. 2. In other words  ASGCD achieves an

C◆ L1kx⇤k2
✏-additive error (i.e.  E[F (˜xS)]  F (x⇤)  ✏ ) using at most O⇣pCL1kx⇤k1

1 = O✓ CL1kx⇤k2
⌘ iterations.

(S + 3)2✓1 +

E[F (˜xS)]  F (x⇤) 

where (b) = nb

pL1kxk1 log d

O⇣pL2kx⇤k2p✏

In Table 1  we give the convergence rate of the existing algorithms and ASGCD to solve the `1-
regularized problem (1).
In the ﬁrst column  “Acc” and “Non-Acc” denote the corresponding
algorithms are Nesterov’s accelerated or not respectively  “Primal” and “Dual” denote the corre-
sponding algorithms solves the primal problem (1) and its regularized dual problem [22] respectively 
`2-norm and `1-norm denote the theoretical guarantee is based on `2-norm and `1-norm respectively.
In terms of `2-norm based guarantee  Katyusha and APPROX give the state of the art convergence rate

⌘. In terms of `1-norm based guarantee  GCD gives the state of the art convergence rate

O( L1kxk2
)  which is only applicable for the smooth case  = 0 in (1). When > 0  the generalized
GS-r  GS-s and GS-q rules generally have worse theoretical guarantee than GCD [19]. While the
bound of ASGCD in this paper is O(
)  which can be viewed as an accelerated version
p✏
of the `1-norm based guarantee O( L1kxk2
). Meanwhile  because the bound depends on kx⇤k1 rather
than kx⇤k2 and on L1 rather than L2 (L1 and L2 are deﬁned in (2))  for the `1-ERM problem  if the
samples are high-dimensional  dense and the regularization parameter  is relatively large  then it is
possible that L1 ⌧ L2 (in the extreme case  L2 = dL1 [11]) and kx⇤k1 ⇡ kx⇤k2. In this case  the
`1-norm based guarantee O(
) of ASGCD is better than the `2-norm based guarantee
⌘ of Katyusha and APPROX. Finally  whether the log d factor in the bound of ASGCD
O⇣pL2kx⇤k2p✏
(which also appears in the COMID [13] analysis) is necessary deserves further research.
Remark 2. When the batch size b = n  ASGCD is a deterministic algorithm. In this case  we can use
a better smooth constant T1 that satisﬁes krf (x)  rf (y)k1  T1kx  yk1 rather than L1 [1].
Remark 3. The necessity of computing the full gradient beforehand is the main bottleneck of GCD
in applications [19]. There exists some work [11] to avoid the computation of full gradient by
performing some approximate greedy selection. While the method in [11] needs preprocessing 

pL1kxk1 log d

1

✏

1

✏

p✏

7

Table 1: Convergence rate on `1-regularized empirical risk minimization problems. (For GCD  the
convergence rate is applied for  = 0. )

ALGORITHM TYPE
NON-ACC  PRIMAL  `2-NORM
ACC  PRIMAL  `2-NORM
ACC 
DUAL 
`2-NORM

PAPER

SAGA [10]

KATYUSHA [1]
ACC-SDCA [23]

SPDC [26]
APCG [16]

APPROX [14]

NON-ACC  PRIMAL  `1-NORM
ACC  PRIMAL  `1-NORM

GCD [3]

ASGCD (THIS PAPER)

2

1

✏

p✏

CONVERGENCE RATE

⌘
O⇣ L2kx⇤k2
O⇣ pL2kx⇤k2
⌘
✏ )⌘
O⇣ pL2kx⇤k2
⌘
O⇣ L1kx⇤k2
O⇣ pL1kx⇤k1 log d
⌘

log( 1

✏

p✏

p✏

incoherence condition for dataset and is somewhat complicated. Contrary to [11]  the proposed
ASGCD algorithm reduces the complexity of greedy selection by a factor up to n in terms of the
amortized cost by simply applying the existing stochastic variance reduction framework.

4 Experiments

In this section  we use numerical experiments to demonstrate the theoretical results in Section 3
and show the empirical performance of ASGCD with batch size b = 1 and its deterministic version
with b = n (In Fig. 1 they are denoted as ASGCD (b = 1) and ASGCD (b = n) respectively). In
addition  following the claim to using data access rather than CPU time [21] and the recent SGD
and RCD literature [15  16  1]  we use the data access  i.e.  the number of times the algorithm
accesses the data matrix  to measure the algorithm performance. To show the effect of Nesterov’s
acceleration  we compare ASGCD (b = n) with the non-accelerated greedy coordinate descent
with GS-q rule  i.e.  coordinate gradient descent (CGD) [24]. To show the effect of both Nesterov’s
acceleration and stochastic optimization strategies  we compare ASGCD (b = 1) with Katyusha
[1  Alg. 2]. To show the effect of the proposed new rule in Section 2  which is based on `1-norm
square approximation  we compare ASGCD (b = n) with the `2-norm based proximal accelerated
full gradient (AFG) implemented by the linear coupling framework [4]. Meanwhile  as a benchmark
of stochastic optimization for the problems with ﬁnite-sum structure  we also show the performance
of proximal stochastic variance reduced gradient (SVRG) [25]. In addition  based on [1] and our
experiments  we ﬁnd that “Katyusha” [1  Alg. 2] has the best empirical performance in general for
the `1-regularized problem (1). Therefore other well-known state-of-art algorithms  such as APCG
[16] and accelerated SDCA [23]  are not included in the experiments.
The datasets are obtained from LIBSVM data [9] and summarized in Table 2. All the algorithms are
used to solve the following lasso problem

min

x2Rd{f (x) + kxk1 =

1
2nkb  Axk2

2 + kxk1}

(17)

on the 3 datasets  where A = (a1  a2  . . .   an)T = (h1  h2  . . .   hd) 2 Rn⇥d with each aj 2 Rd
representing a sample vector and hi 2 Rn representing a feature vector  b 2 Rn is the prediction
vector.

Table 2: Characteristics of three real datasets.

DATASET NAME
LEUKEMIA
GISETTE
MNIST

# SAMPLES n

# FEATURES d

38
6000
60000

7129
5000
780

For ASGCD (b = 1) and Katyusha [1  Alg. 2]  we can use the tight smooth constant L1 =
2 respectively in their implementation. While for AS-
maxj2[n] i2[d] |a2

j i| and L2 = maxj2[n] kajk2

8



Leu

Gisette

Mnist

CGD
AFG
ASGCD (b=n)
SVRG
Katyusha
ASGCD (b=1)

0

1

2

3

4

Number of Passes

5
×10 4

102

106

s
s
o

l
 

g
o
L

s
s
o
L

 

g
o
L

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

-20

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

-20

0

200

400

600

s
s
o
L

 

g
o
L

s
s
o
L

 

g
o
L

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

-20

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

-20

800

1000

1200
Number of Passes

1400

1600

1800

2000

0

200

400

600

s
s
o
L

 

g
o
L

s
s
o
L

 

g
o
L

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

-20

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

-20

800

1000

1200
Number of Passes

1400

1600

1800

2000

0

1000

2000

3000

5000

4000
6000
Number of Passes

7000

8000

9000 10000

0

1

2

3

4

5

6

7

8

Number of Passes

9

10
×104

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

Number of Passes

5
×104

Figure 1: Comparing AGCD (b = 1) and ASGCD (b = n) with CGD  SVRG  AFG and Katyusha on
Lasso.

GCD (b = n) and AFG  the better smooth constant T1 = maxi2[d] khik2
are used re-
spectively. The learning rate of CGD and SVRG are tuned in {106  105  104  103  102  101}.

and T2 = kAk2

n

n

2

Table 3: Factor rates of for the 6 cases


102
106

LEU

(0.85  1.33)
(1.45  2.27)

GISETTE
(0.88  0.74)
(3.51  2.94)

MNIST

(5.85  3.02)
(5.84  3.02)

We use  = 106 and  = 102 in the experiments. In addition  for each case (Dataset  )  AFG is
used to ﬁnd an optimum x⇤ with enough accuracy.
The performance of the 6 algorithms is plotted in Fig. 1. We use Log loss log(F (xk) F (x⇤)) in the
y-axis. x-axis denotes the number that the algorithm access the data matrix A. For example  ASGCD
(b = n) accesses A once in each iteration  while ASGCD (b = 1) accesses A twice in an entire outer
pT2kx⇤k2 ⌘
iteration. For each case (Dataset  )  we compute the rate (r1  r2) = ⇣pCL1kx⇤k1
pCT1kx⇤k1
in Table 3. First  because of the acceleration effect  ASGCD (b = n) are always better than the
non-accelerated CGD algorithm; second  by comparing ASGCD(b = 1) with Katyusha and ASGCD
(b = n) with AFG  we ﬁnd that for the cases (Leu  102)  (Leu  106) and (Gisette  102)  ASGCD
(b = 1) dominates Katyusha [1  Alg.2] and ASGCD (b = n) dominates AFG. While the theoretical
analysis in Section 3 shows that if r1 is relatively small such as around 1  then ASGCD (b = 1)
will be better than [1  Alg.2]. For the other 3 cases  [1  Alg.2] and AFG are better. The consistency
between Table 3 and Fig. 1 demonstrates the theoretical analysis.

pL2kx⇤k2

 

References
[1] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. ArXiv e-prints 

abs/1603.05953  2016.

[2] Zeyuan Allen-Zhu  Zhenyu Liao  and Lorenzo Orecchia. Spectral sparsiﬁcation and regret minimization
beyond matrix multiplicative updates. In Proceedings of the Forty-Seventh Annual ACM on Symposium on
Theory of Computing  pages 237–245. ACM  2015.

[3] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear Coupling: An Ultimate Uniﬁcation of Gradient and

Mirror Descent. ArXiv e-prints  abs/1407.1537  July 2014.

[4] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate uniﬁcation of gradient and mirror

descent. ArXiv e-prints  abs/1407.1537  July 2014.

9

[5] Francis Bach  Rodolphe Jenatton  Julien Mairal  Guillaume Obozinski  et al. Optimization with sparsity-

inducing penalties. Foundations and Trends R in Machine Learning  4(1):1–106  2012.

[6] Keith Ball  Eric A Carlen  and Elliott H Lieb. Sharp uniform convexity and smoothness inequalities for

trace norms. Inventiones mathematicae  115(1):463–482  1994.

[7] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM journal on imaging sciences  2(1):183–202  2009.

[8] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press  2004.

[9] Chih-Chung Chang. Libsvm: Introduction and benchmarks. http://www. csie. ntn. edu. tw/˜ cjlin/libsvm 

2000.

[10] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in Neural Information Processing
Systems  pages 1646–1654  2014.

[11] Inderjit S Dhillon  Pradeep K Ravikumar  and Ambuj Tewari. Nearest neighbor based greedy coordinate

descent. In Advances in Neural Information Processing Systems  pages 2160–2168  2011.

[12] John Duchi  Shai Shalev-Shwartz  Yoram Singer  and Tushar Chandra. Efﬁcient projections onto the l
1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine
learning  pages 272–279. ACM  2008.

[13] John C Duchi  Shai Shalev-Shwartz  Yoram Singer  and Ambuj Tewari. Composite objective mirror descent.

In COLT  pages 14–26  2010.

[14] Olivier Fercoq and Peter Richtárik. Accelerated  parallel  and proximal coordinate descent. SIAM Journal

on Optimization  25(4):1997–2023  2015.

[15] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In Advances in Neural Information Processing Systems  pages 315–323  2013.

[16] Qihang Lin  Zhaosong Lu  and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances

in Neural Information Processing Systems  pages 3059–3067  2014.

[17] Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization  22(2):341–362  2012.

[18] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Science

& Business Media  2013.

[19] Julie Nutini  Mark Schmidt  Issam H Laradji  Michael Friedlander  and Hoyt Koepke. Coordinate
descent converges faster with the gauss-southwell rule than random selection. In Proceedings of the 32nd
International Conference on Machine Learning (ICML-15)  pages 1632–1641  2015.

[20] Shai Shalev-Shwartz and Yoram Singer. Efﬁcient learning of label ranking by soft projections onto

polyhedra. Journal of Machine Learning Research  7(Jul):1567–1599  2006.

[21] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for l1-regularized loss minimization. Journal

of Machine Learning Research  12(Jun):1865–1892  2011.

[22] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research  14(Feb):567–599  2013.

[23] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regular-

ized loss minimization. In ICML  pages 64–72  2014.

[24] Paul Tseng and Sangwoon Yun. A coordinate gradient descent method for nonsmooth separable minimiza-

tion. Mathematical Programming  117(1):387–423  2009.

[25] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.

SIAM Journal on Optimization  24(4):2057–2075  2014.

[26] Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical risk
minimization. In Proceedings of the 32nd International Conference on Machine Learning  volume 951 
page 2015  2015.

[27] Shuai Zheng and James T Kwok. Fast-and-light stochastic admm.

In The 25th International Joint

Conference on Artiﬁcial Intelligence (IJCAI-16)  New York City  NY  USA  2016.

10

,Chaobing Song
Shaobo Cui
Yong Jiang
Shu-Tao Xia
Alan Malek
Peter Bartlett
Philip Paquette
Yuchen Lu
SETON STEVEN BOCCO
Max Smith
Satya O.-G.
Jonathan Kummerfeld
Joelle Pineau
Satinder Singh
Aaron Courville