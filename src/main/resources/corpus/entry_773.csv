2011,Beating SGD: Learning SVMs in Sublinear Time,We present an optimization approach for linear SVMs based on a stochastic primal-dual approach  where the primal step is akin to an importance-weighted SGD  and the dual step is a stochastic update on the importance weights.  This yields an optimization method with a sublinear dependence on the training set size  and the first method for learning linear SVMs with runtime less then the size of the training set required for learning!,Beating SGD: Learning SVMs in Sublinear Time

Tomer Koren
Elad Hazan
Technion  Israel Institute of Technology

{ehazan@ie tomerk@cs}.technion.ac.il

Haifa  Israel 32000

Nathan Srebro

Toyota Technological Institute

Chicago  Illinois 60637

nati@ttic.edu

Abstract

We present an optimization approach for linear SVMs based on a stochastic
primal-dual approach  where the primal step is akin to an importance-weighted
SGD  and the dual step is a stochastic update on the importance weights. This
yields an optimization method with a sublinear dependence on the training set
size  and the ﬁrst method for learning linear SVMs with runtime less then the size
of the training set required for learning!

1

Introduction

Stochastic approximation (online) approaches  such as stochastic gradient descent and stochastic
dual averaging  have become the optimization method of choice for many learning problems  includ-
ing linear SVMs. This is not surprising  since such methods yield optimal generalization guarantees
with only a single pass over the data. They therefore in a sense have optimal  unbeatable runtime:
from a learning (generalization) point of view  in a “data laden” setting [2  13]  the runtime to get to
a desired generalization goal is the same as the size of the data set required to do so. Their runtime
is therefore equal (up to a small constant factor) to the runtime required to just read the data.
In this paper we show  for the ﬁrst time  how to beat this unbeatable runtime  and present a method
that  in a certain relevant regime of high dimensionality  relatively low noise and accuracy propor-
tional to the noise level  learns in runtime less then the size of the minimal training set size required
for generalization. The key here  is that unlike online methods that consider an entire training vector
at each iteration  our method accesses single features (coordinates) of training vectors. Our compu-
tational model is thus that of random access to a desired coordinate of a desired training vector (as is
standard for sublinear time algorithms)  and our main computational cost are these feature accesses.
Our method can also be understood in the framework of “budgeted learning” [5] where the cost
is explicitly the cost of observing features (but unlike  e.g. [8]  we do not have differential costs
for different features)  and gives the ﬁrst non-trivial guarantee in this setting (i.e. ﬁrst theoretical
guarantee on the number of feature accesses that is less then simply observing entire feature vectors).
We emphasize that our method is not online in nature  and we do require repeated access to training
examples  but the resulting runtime (as well as the overall number of features accessed) is less (in
some regimes) then for any online algorithms that considers entire training vectors. Also  unlike
recent work by Cesa-Bianchi et al. [3]  we are not constrained to only a few features from every
vector  and can ask for however many we need (with the aim of minimizing the overall runtime  and
thus the overall number of feature accesses)  and so we obtain an overall number of feature accesses
which is better then with SGD  unlike Cesa-Bianchi et al.  which aim at not being too much worse
then full-information SGD.
As discussed in Section 3  our method is a primal-dual method  where both the primal and dual steps
are stochastic. The primal steps can be viewed as importance-weighted stochastic gradient descent 
and the dual step as a stochastic update on the importance weighting  informed by the current pri-
mal solution. This approach builds on the work of [4] that presented a sublinear time algorithm for
approximating the margin of a linearly separable data set. Here  we extend that work to the more rel-

1

evant noisy (non-separable) setting  and show how it can be applied to a learning problem  yielding
generalization runtime better then SGD. The extension to the non-separable setting is not straight-
forward and requires re-writing the SVM objective  and applying additional relaxation techniques
borrowed from [10].

2 The SVM Optimization Problem
We consider training a linear binary SVM based on a training set of n labeled points {xi  yi}i=1...n 
xi ∈ Rd  yi ∈ {±1}  with the data normalized such that (cid:107)xi(cid:107) ≤ 1. A predictor is speciﬁed by
(cid:80)n
w ∈ Rd and a bias b ∈ R. In training  we wish to minimize the empirical error  measured in terms
i=1[1 − y((cid:104)w  xi(cid:105) + b)]+   and the norm of w. Since
of the average hinge loss ˆRhinge(w  b) = 1
n
we do not typically know a-priori how to balance the norm with the error  this is best described as
an unconstrained bi-criteria optimization problem:

w∈Rd b∈R (cid:107)w(cid:107)   ˆRhinge(w  b)

min

(1)

A common approach to ﬁnding Pareto optimal points of (1) is to scalarize the objective as:

min

w∈Rd b∈R

ˆRhinge(w  b) +

(2)
where the multiplier λ ≥ 0 controls the trade-off between the two objectives. However  in order
to apply our framework  we need to consider a different parametrization of the Pareto optimal set
(the “regularization path”): instead of minimizing a trade-off between the norm and the error  we
maximize the margin (equivalent to minimizing the norm) subject to a constraint on the error. This
allows us to write the objective (the margin) as a minimum over all training points—a form we will
later exploit. Speciﬁcally  we introduce slack variables and consider the optimization problem:

(cid:107)w(cid:107)2

λ
2

n(cid:88)

i=1

max

w∈Rd  b∈R  0≤ξi

min
i∈[n]

yi((cid:104)w  xi(cid:105) + b) + ξi

s.t.

(cid:107)w(cid:107) ≤ 1 and

ξi ≤ nν

(3)

where the parameter ν controls the trade-off between desiring a large margin (low norm) and small
error (low slack)  and parameterizes solutions along the regularization path. This is formalized by
the following Lemma  which also gives guarantees for ε-sub-optimal solutions of (3):
Lemma 2.1. For any w (cid:54)= 0 b ∈ R consider problem (3) with ν = ˆRhinge(w  b)/(cid:107)w(cid:107). Let wε  bε  ξε
be an ε-suboptimal solution to this problem with value γε  and consider the rescaled solution ˜w =
wε/γε  ˜b = bε/γε. Then:

(cid:107) ˜w(cid:107) ≤

1

1 − (cid:107)w(cid:107) ε

(cid:107)w(cid:107)  

and

ˆRhinge( ˜w) ≤

1

1 − (cid:107)w(cid:107) ε

ˆRhinge(w).

That is  solving (3) exactly (to within ε = 0) yields Pareto optimal solutions of (1)  and all such
solutions (i.e. the entire regularization path) can be obtained by varying ν. When (3) is only solved
approximately  we obtain a Pareto sub-optimal point  as quantiﬁed by Lemma 2.1.
Before proceeding  we also note that any solution of (1) that classiﬁes at least some positive and
negative points within the desired margin must have (cid:107)w(cid:107) ≥ 1 and so in Lemma 2.1 we will only
need to consider 0 ≤ ν ≤ 1. In terms of (3)  this means that we could restrict 0 ≤ ξi ≤ 2 without
affecting the optimal solution.

3 Overview: Primal-Dual Algorithms and Our Approach

The CHW framework

The method of [4] applies to saddle-point problems of the form

z∈K min
max
i∈[n]

ci(z).

2

(4)

where ci(z) are concave functions of z over some set K ⊆ Rd. The method is a stochastic primal-
dual method  where the dual solution can be viewed as importance weighting over the n terms ci(z).
To better understand this view  consider the equivalent problem:

n(cid:88)

i=1

z∈K min
max
p∈∆n

pici(z)

(5)

where ∆n = {p ∈ Rn | pi ≥ 0 (cid:107)p(cid:107)1 = 1} is the probability simplex. The method maintains
and (stochastically) improves both a primal solution (in our case  a predictor w ∈ Rd) and a dual
solution  which is a distribution p over [n]. Roughly speaking  the distribution p is used to focus in
on the terms actually affecting the minimum. Each iteration of the method proceeds as follows:

1. Stochastic primal update:

2. Stochastic dual update:

(a) A term i ∈ [n] is chosen according to the distribution p  in time O(n).
(b) The primal variable z is updated according to the gradient of the ci(z)  via an online
low-regret update. This update is in fact a Stochastic Gradient Descent (SGD) step on
the objective of (5)  as explained in section 4. Since we use only a single term ci(z) 
this can be usually done in time O(d).

(a) We obtain a stochastic estimate of ci(z)  for each i ∈ [n]. We would like to use an
estimator that has a bounded variance  and can be computed in O(1) time per term 
i.e. in overall O(n) time. When the ci’s are linear functions  this can be achieved using
a form of (cid:96)2-sampling for estimating an inner-product in Rd.

(b) The distribution p is updated toward those terms with low estimated values of ci(z).
This is accomplished using a variant of the Multiplicative Updates (MW) framework
for online optimization over the simplex (see for example [1])  adapted to our case in
which the updates are based on random variables with bounded variance. This can be
done in time O(n).

Evidently  the overall runtime per iteration is O(n + d).
In addition  the regret bounds on the
updates of z and p can be used to bound the number of iterations required to reach an ε-suboptimal
solution. Hence  the CHW approach is particularly effective when this regret bound has a favorable
dependence on d and n. As we note below  this is not the case in our application  and we shall need
some additional machinery to proceed.

The PST framework

problem maxz∈K(cid:80)n

The Plotkin-Shmoys-Tardos framework [10] is a deterministic primal-dual method  originally pro-
posed for approximately solving certain types of linear programs known as “fractional packing and
covering” problems. The same idea  however  applies also to saddle-point problems of the form (5).
In each iteration of this method  the primal variable z is updated by solving the “simple” optimization
i=1 pici(z) (where p is now ﬁxed)  while the dual variable p is again updated
using a MW step (note that we do not use an estimation for ci(z) here  but rather the exact value).
These iterations yield convergence to the optimum of (5)  and the regret bound of the MW updates
is used to derive a convergence rate guarantee.
Since each iteration of the framework relies on the entire set of functions ci  it is reasonable to apply
it only on relatively small-sized problems. Indeed  in our application we shall use this method for
the update of the slack variables ξ and the bias term b  for which the implied cost is only O(n) time.

Our hybrid approach

The saddle-point formulation (3) of SVM from section 2 suggests that the SVM optimization prob-
lem can be efﬁciently approximated using primal-dual methods  and speciﬁcally using the CHW
framework. Indeed  taking z = (w  b  ξ) and K = Bd × [−1  1] × Ξν where Bd ⊆ Rd is the Eu-
clidean unit ball and Ξν = {ξ ∈ Rn | ∀i 0 ≤ ξi ≤ 2  (cid:107)ξ(cid:107)1 ≤ νn}   we cast the problem into the
form (4). However  as already pointed out  a na¨ıve application of the CHW framework yields in this
case a rather slow convergence rate. Informally speaking  this is because our set K is “too large”
and thus the involved regret grows too quickly.
In this work  we propose a novel hybrid approach for tackling problems such as (3)  that combines
the ideas of the CHW and PST frameworks. Speciﬁcally  we suggest using a SGD-like low-regret

3

update for the variable w  while updating the variables ξ and b via a PST-like step; the dual update
of our method is similar to that of CHW. Consequently  our algorithm enjoys the beneﬁts of both
methods  each in its respective domain  and avoids the problem originating from the “size” of K.
We defer the detailed description of the method to the following section.

4 Algorithm and Analysis

In this section we present and analyze our algorithm  which we call SIMBA (stands for “Sublinear
IMportance-sampling Bi-stochastic Algorithm”). The algorithm is a sublinear-time approximation
algorithm for problem (3)  which as shown in section 2 is a reformulation of the standard soft-
margin SVM problem. For the simplicity of presentation  we omit the bias term for now (i.e.  ﬁx
b = 0 in (3)) and later explain how adding such bias to our framework is almost immediate and does
not affect the analysis. This allows us to ignore the labels yi  by setting xi ← −xi for any i with
yi = −1.
Let us begin the presentation with some additional notation. To avoid confusion  we use the notation
v(i) to refer to the i’th coordinate of a vector v. We also use the shorthand v2 to denote the vector
for which v2(i) = (v(i))2 for all i. The n-vector whose entries are all 1 is denoted as 1n. Finally 
we stack the training instances xi as the rows of a matrix X ∈ Rn×d  although we treat each xi as a
column vector.

Algorithm 1 SVM-SIMBA
1: Input: ε > 0  0 ≤ ν ≤ 1  and X ∈ Rn×d with xi ∈ Bd for i ∈ [n].

2: Let T ← 1002ε−2 log n  η ←(cid:112)log(n)/T and u1 ← 0  q1 ← 1n

2T   ξt ← arg maxξ∈Ξν (p(cid:62)
t ξ)

3: for t = 1 to T do
Choose it ← i with probability pt(i)
√
4:
Let ut ← ut−1 + xit/
5:
6: wt ← ut/ max{1 (cid:107)ut(cid:107)}
Choose jt ← j with probability wt(j)2/(cid:107)wt(cid:107)2.
7:
for i = 1 to n do
8:
9:
10:
11:
end for
12:
pt ← qt/(cid:107)qt(cid:107)1
13:
14: end for
15: return ¯w = 1
T

˜vt(i) ← xi(jt)(cid:107)wt(cid:107)2/wt(jt) + ξt(i)
vt(i) ← clip(˜vt(i)  1/η)
qt+1(i) ← qt(i)(1 − ηvt(i) + η2vt(i)2)

(cid:80)

(cid:80)

t wt  ¯ξ = 1

T

t ξt

The pseudo-code of the SIMBA algorithm is given in ﬁgure 1. In the primal part (lines 4 through 6) 
the vector ut is updated by adding an instance xi  randomly chosen according to the distribution pt.
This is a version of SGD applied on the function p(cid:62)
t (Xw + ξt)  whose gradient with respect to w is
p(cid:62)
t X; by the sampling procedure of it  the vector xit is an unbiased estimator of this gradient. The
vector ut is then projected onto the unit ball  to obtain wt. On the other hand  the primal variable ξt
t ξ with respect to ξ ∈ Ξν. This is an instance of the PST
is updated by a complete optimization of p(cid:62)
framework  described in section 3. Note that  by the structure of Ξν  this update can be accomplished
using a simple greedy algorithm that sets ξt(i) = 2 corresponding to the largest entries pt(i) of pt 
until a total mass of νn is reached  and puts ξt(i) = 0 elsewhere; this can be implemented in O(n)
time using standard selection algorithms.
In the dual part (lines 7 through 13)  the algorithm ﬁrst updates the vector qt using the jt column of X
t /(cid:107)wt(cid:107)2. This
and the value of wt(jt)  where jt is randomly selected according to the distribution w2
is a variant of the MW framework (see deﬁnition 4.1 below) applied on the function p(cid:62)(Xwt + ξt);
the vector ˜v serves as an estimator of Xwt + ξt  the gradient with respect to p. We note  however 
that the algorithm uses a clipped version v of the estimator ˜v; see line 10  where we use the notation
clip(z  C) = max(min(z  C) −C) for z  C ∈ R. This  in fact  makes v a biased estimator of the
gradient. As we show in the analysis  while the clipping operation is crucial to the stability of the
algorithm  the resulting slight bias is not harmful.
Before stating the main theorem  we describe in detail the MW algorithm we use for the dual update.

4

Deﬁnition 4.1 (Variance MW algorithm). Consider a sequence of vectors v1  . . .   vT ∈ Rn and a
parameter η > 0. The Variance Multiplicative Weights (Variance MW) algorithm is as follows. Let
w1 ← 1n  and for t ≥ 1 
pt ← wt/(cid:107)wt(cid:107)1  

wt+1(i) ← wt(i)(1 − ηvt(i) + η2vt(i)2).

and

(6)

The following lemma establishes a regret bound for the Variance MW algorithm.
Lemma 4.2 (Variance MW Lemma). The Variance MW algorithm satisﬁes

(cid:88)

t∈[T ]

(cid:88)

t∈[T ]

t vt ≤ min
p(cid:62)
i∈[n]

max{vt(i) −1/η} +

log n

η

+ η

p(cid:62)
t v2
t .

(cid:88)

t∈[T ]

(cid:80)
t∈[T ] p(cid:62)

We now state the main theorem. Due to space limitations  we only give here a sketch of the proof.
Theorem 4.3 (Main). The SIMBA algorithm above returns an ε-approximate solution to formula-
tion (3) with probability at least 1/2. It can be implemented to run in time ˜O(ε−2(n + d)).

Proof (sketch). The main idea of the proof is to establish lower and upper bounds on the average
t (Xwt + ξt). Then  combining these bounds we are able to relate the
objective value 1
T
value of the output solution ( ¯w  ¯ξ) to the value of the optimum of (3). In the following  we let
(w∗  ξ∗) be the optimal solution of (3) and denote the value of this optimum by γ∗.

For the lower bound  we consider the primal part of the algorithm. Noting that(cid:80)
(cid:80)
t∈[T ] p(cid:62)

t∈[T ] p(cid:62)
bounding the regret of the SGD update  we obtain the lower bound (with probability ≥ 1 − O( 1

t ξt ≥
t ξ∗ (which follows from the PST step) and employing a standard regret guarantee for
n )):

t (Xwt + ξt) ≥ γ∗ − O
p(cid:62)

(cid:16)(cid:113) log n

(cid:17)

.

T

(cid:88)

t∈[T ]

1
T

(cid:88)

t∈[T ]

1
T

(cid:88)

t∈[T ]

[x(cid:62)

For the upper bound  we examine the dual part of the algorithm. Applying lemma 4.2 for bounding
the regret of the MW update  we get the following upper bound (with probability > 3

4 − O( 1

n )):

t (Xwt + ξt) ≤ 1
p(cid:62)
T

min
i∈[n]

i wt + ξt(i)] + O

i ¯w + ¯ξ(i)] ≥ γ∗ − O((cid:112)log(n)/T ) with

T

.

2  and using our choice for T the claim follows.

Relating the two bounds we conclude that mini∈[n] [x(cid:62)
probability ≥ 1
Finally  we note the runtime. The algorithm makes T = O(ε−2 log n) iterations. In each iteration 
the update of the vectors wt and pt takes O(d) and O(n) time respectively  while ξt can be computed
in O(n) time as explained above. The overall runtime is therefore ˜O(ε−2(n + d)).

(cid:16)(cid:113) log n

(cid:17)

Incorporating a bias term We return to the optimization problem (3) presented in section 2  and
show how the bias term b can be integrated into our algorithm. Unlike with SGD-based approaches 
including the bias term in our framework is straightforward. The only modiﬁcation required to
our algorithm as presented in ﬁgure 1 occurs in lines 5 and 9  where the vector ξt is referred. For
additionally maintaining a bias bt  we change the optimization over ξ in line 5 to a joint optimization
over both ξ and b:

(ξt  bt) ← argmax

t (ξ + b · y)
p(cid:62)

ξ∈Ξν   b∈[−1 1]

while returning the average bias ¯b = (cid:80)

and use the computed bt for the dual update  in line 9: ˜vt(i) ← xi(jt)(cid:107)wt(cid:107)2/wt(jt) + ξt(i) + yibt 
t∈[T ] bt/T in the output of the algorithm. Notice that we
still assume that the labels yi were subsumed into the instances xi  as in section 4. The update of ξt
is thus unchanged and can be carried out as described in section 4. The update of bt  on the other
hand  admits a simple  closed-form formula: bt = sign(p(cid:62)
t y). Evidently  the running time of each
iteration remains O(n + d)  as before. The adaptation of the analysis to this case  which involves
only a change of constants  is technical and straightforward.

5

The sparse case We conclude the section with a short discussion of the common situation in which
the instances are sparse  that is  each instance contains very few non-zero entries. In this case  we can
implement algorithm 1 so that each iteration takes ˜O(α(n + d))  where α is the overall data sparsity
ratio. Implementing the vector updates is straightforward  using a data representation similar to
[12]. In order to implement the sampling operations in time O(log n) and O(log d)  we maintain
a tree over the points and coordinates  with internal nodes caching the combined (unnormalized)
probability mass of their descendants.

5 Runtime Analysis for Learning

In Section 4 we saw how to obtain an ε-approximate solution to the optimization problem (3) in time
˜O(ε−2(n + d)). Combining this with Lemma 2.1  we see that for any Pareto optimal point w∗ of
(1) with (cid:107)w∗(cid:107) = B and ˆRhinge(w∗) = ˆR∗  the runtime required for our method to ﬁnd a predictor
with (cid:107)w(cid:107) ≤ 2B and ˆRhinge(w) ≤ ˆR∗ + ˆδ is

(cid:18)

˜O

B2(n + d)

(cid:18) ˆR∗ + ˆδ

(cid:19)2(cid:19)

.

ˆδ

This guarantee is rather different from guarantee for other SVM optimization approaches. E.g. us-
ing a stochastic gradient descent (SGD) approach  we could ﬁnd a predictor with (cid:107)w(cid:107) ≤ B and
ˆRhinge(w) ≤ ˆR∗ + ˆδ in time O(B2d/ˆδ2). Compared with SGD  we only ensure a constant factor ap-
proximation to the norm  and our runtime does depend on the training set size n  but the dependence
on ˆδ is more favorable. This makes it difﬁcult to compare the guarantees and suggests a different
form of comparison is needed. Following [13]  instead of comparing the runtime to achieve a certain
optimization accuracy on the empirical optimization problem  we analyze the runtime to achieve a
desired generalization performance.
Recall that our true learning objective is to ﬁnd a predictor with low generalization error Rerr(w) =
Pr(x y)(y (cid:104)w  x(cid:105) ≤ 0) where x  y are distributed according to some unknown source distribution 
and the training set is drawn i.i.d. from this distribution. We assume that there exists some (un-
known) predictor w∗ that has norm (cid:107)w∗(cid:107) ≤ B and low expected hinge loss R∗ = Rhinge(w∗) =
E [[1 − y (cid:104)w∗  x(cid:105)]+]  and analyze the runtime to ﬁnd a predictor w with generalization error
Rerr(w) ≤ R∗ + δ.
In order to understand the runtime from this perspective  we must consider the required sample size
to obtain generalization to within δ  as well as the required suboptimality for (cid:107)w(cid:107) and ˆRhinge(w).
The standard SVMs analysis calls for a sample size of n = O(B2/δ2). But since  as we will see 
our analysis will be sensitive to the value of R∗  we will consider a more reﬁned generalization
guarantee which gives a better rate when R∗ is small relative to δ. Following Theorem 5 of [14]
(and recalling that the hinge-loss is an upper bound on margin violations)  we have that with high
probability over a sample of size n  for all predictors w:

(7)

(8)

(9)

Rerr(w) ≤ ˆRhinge(w) + O

This implies that a training set of size

n = ˜O

(cid:115)

+

(cid:107)w(cid:107)2
(cid:18) B2

n

δ

· R∗ + δ

δ

 .

(cid:107)w(cid:107)2 ˆRhinge(w)

n

(cid:19)

is enough for generalization to within δ. We will be mostly concerned here with the regime where
either R∗ is small and we seek generalization to within δ = Ω(R∗)—a typical regime in learning.
This is always the case in the realizable setting  where R∗ = 0  but includes also the non-realizable
setting  as long as the desired estimation error δ is not much smaller then the unavoidable error R∗.
In any case  in such a regime In that case  the second factor in (9) is of order one.
In fact  an online approach 1 can ﬁnd a predictor with Rerr(w) ≤ R∗ + δ with a single pass over
n = ˜O(B2/δ · (δ + R∗)/δ) training points. Since each step takes O(d) time (essentially the time

1The Perceptron rule  which amounts to SGD on Rhinge(w)  ignoring correctly classiﬁed points [7  3].

6

required to read the training point)  the overall runtime is:
d · R∗ + δ

O

(cid:18) B2

δ

(cid:19)

δ

.

(10)

Returning to our approach  approximating the norm to within a factor of two is ﬁne  as it only effects
the required sample size  and hence the runtime by a constant factor. In particular  in order to ensure
Rerr(w) ≤ R∗ + δ it is enough to have (cid:107)w(cid:107) ≤ 2B  optimize the empirical hinge loss to within
ˆδ = δ/2  and use a sample size as speciﬁed in (9) (where we actually consider a radius of 2B and
require generalization to within δ/4  but this is subsumed in the constant factors). Plugging this into
the runtime analysis (7) yields:
Corollary 5.1. For any B ≥ 1 and δ > 0  with high probability over a training set of size  n =
˜O(B2/δ · (δ + R∗)/δ)  Algorithm 1 outputs a predictor w with Rerr(w) ≤ R∗ + δ in time

(cid:32)(cid:18)

˜O

B2d +

B4
δ

· δ + R∗

δ

(cid:18) δ + R∗

(cid:19)2(cid:33)

(cid:19)

·

δ

where R∗ = inf(cid:107)w∗(cid:107)≤B Rhinge(w∗).
Let us compare the above runtime to the online runtime (10)  focusing on the regime where R∗ is
δ = O(1)  and ignoring the logarithmic factors hidden in the ˜O(·)
small and δ = Ω(R∗) and so R∗+δ
notation in Corollary 5.1. To do so  we will ﬁrst rewrite the runtime in Corollary 5.1 as:

(cid:32)

˜O

d · R∗ + δ

δ

B2
δ

· (R∗ + δ) +

B2 ·

B2
δ

.

(11)

(cid:18) R∗ + δ

(cid:19)3(cid:33)

δ

δ

)2.

In order to compare the runtimes  we must consider the relative magnitudes of the dimensionality d
and the norm B. Recall that using a norm-regularized approach  such as SVM  makes sense only
when d (cid:29) B2. Otherwise  the low dimensionality would guarantee us good generalization  and we
wouldn’t gain anything from regularizing the norm. And so  at least when R∗+δ
δ = O(1)  the ﬁrst
term in (11) is the dominant term and we should compare it with (10). More generally  we will see
an improvement as long as d (cid:29) B2( R∗+δ
Now  the ﬁrst term in (11) is more directly comparable to the online runtime (10)  and is always
smaller by a factor of (R∗ + δ) ≤ 1. This factor  then  is the improvement over the online approach 
or more generally  over any approach which considers entire sample vectors (as opposed to individ-
ual features). We see  then  that our proposed approach can yield a signiﬁcant reduction in runtime
when the resulting error rate is small. Taking into account the hidden logarithmic factors  we get an
improvement as long as (R∗ + δ) = O(1/ log(B2/δ)).
Returning to the form of the runtime in Corollary 5.1  we can also understand the runtime as fol-
lows: Initially  a runtime of O(B2d) is required in order for the estimates of w and p to start being
reasonable. However  this runtime does not depend on the desired error (as long as δ = Ω(R∗) 
including when R∗ = 0)  and after this initial runtime investment  once w and p are “reasonable” 
we can continue decreasing the error toward R∗ with runtime that depends only on the norm  but is
independent of the dimensionality.

6 Experiments

In this section we present preliminary experimental results  that demonstrate situations in which
our approach has an advantage over SGD-based methods. To this end  we choose to compare the
performance of our algorithm to that of the state-of-the-art Pegasos algorithm [12]  a popular SGD
variant for solving SVM. The experiments were performed with two standard  large-scale data sets:
• The news20 data set of [9] that has 1 355 191 features and 19 996 examples. We split the
• The real vs. simulated data set of McCallum  with 20 958 features and 72 309 examples.
We split the data set into a training set of 20 000 examples and a test set of 52 309 examples.
We implemented the SIMBA algorithm exactly as in Section 4  with a single modiﬁcation: we used

a time-adaptive learning rate ηt =(cid:112)log(n)/t and a similarly an adaptive SGD step-size (in line 5) 

data set into a training set of 8 000 examples and a test set of 11 996 examples.

7

r
o
r
r
e

t
s
e
t

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

SIMBA(ν = 1 · 10−3)
Pegasos (λ = 1.25 · 10−4)

SIMBA(ν = 5 · 10−5)
Pegasos (λ = 5 · 10−5)

0.55

0.5

0.45

0.4

0.35

0.3

0.25

r
o
r
r
e

t
s
e
t

107

108

109

109

1010

1011

feature accesses

feature accesses

Figure 1: The test error  averaged over 10 repetitions  vs.
the number of feature accesses  on the real vs.
simulated (left) and news20 (right) data sets. The error bars depict one standard-deviation of the measurements.

instead of leaving them constant. While this version of the algorithm is more convenient to work
with  we found that in practice its performance is almost equivalent to that of the original algorithm.
In both experiments  we tuned the tradeoff parameter of each algorithm (i.e.  ν and λ) so as to
obtain the lowest possible error over the test set. Note that our algorithm assumes random access to
features (as opposed to instances)  thus it is not meaningful to compare the test error as a function of
the number of iterations of each algorithm. Instead  and according to our computational model  we
compare the test error as a function of the number of feature accesses of each algorithm. The results 
averaged over 10 repetitions  are presented in ﬁgure 1 along with the parameters we used. As can be
seen from the graphs  on both data sets our algorithm obtains the same test error as Pegasos achieves
at the optimum  using about 100 times less feature accesses.

7 Summary

Building on ideas ﬁrst introduced by [4]  we present a stochastic-primal-stochastic-dual approach
that solves a non-separable linear SVM optimization problem in sublinear time  and yields a learning
method that  in a certain regime  beats SGD and runs in less time than the size of the training set
required for learning. We also showed some encouraging preliminary experiments  and we expect
further work can yield signiﬁcant gains  either by improving our method  or by borrowing from the
ideas and innovations introduced  including:

stochastic step.

• Using importance weighting  and stochastically updating the importance weights in a dual
• Explicitly introducing the slack variables (which are not typically represented in primal
SGD approaches). This allows us to differentiate between an accounted-for margin mis-
takes  and a constraint violation where we did not yet assign enough “slack” and want to
focus our attention on. This differs from heuristic importance weighting approaches for
stochastic learning  which tend to focus on all samples with a non-zero loss gradient.

• Employing the PST methodology when the standard low-regret tools fail to apply.

We believe that our ideas and framework can also be applied to more complex situations where much
computational effort is currently being spent  including highly multiclass and structured SVMs 
latent SVMs [6]  and situations where features are very expensive to calculate  but can be calculated
on-demand. The ideas can also be extended to kernels  either through linearization [11]  using an
implicit linearization as in [4]  or through a representation approach. Beyond SVMs  the framework
can apply more broadly  whenever we have a low-regret method for the primal problem  and a
sampling procedure for the dual updates. E.g. we expect the approach to be successful for (cid:96)1-
regularized problems  and are working on this direction.

Acknowledgments This work was supported in part by the IST Programme of the European Com-
munity  under the PASCAL2 Network of Excellence  IST-2007-216886. This publication only re-
ﬂects the authors’ views.

8

References
[1] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta algorithm

and applications. Manuscript  2005.

[2] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. Advances in neural informa-

tion processing systems  20:161–168  2008.

[3] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning

algorithms. Information Theory  IEEE Transactions on  50(9):2050–2057  2004.

[4] K.L. Clarkson  E. Hazan  and D.P. Woodruff. Sublinear optimization for machine learning.
In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science  pages 449–457.
IEEE  2010.

[5] K. Deng  C. Bourke  S. Scott  J. Sunderman  and Y. Zheng. Bandit-based algorithms for
budgeted learning. In Data Mining  2007. ICDM 2007. Seventh IEEE International Conference
on  pages 463–468. IEEE  2007.

[6] P. Felzenszwalb  D. Mcallester  and D. Ramanan. A discriminatively trained  multiscale  de-
In In IEEE Conference on Computer Vision and Pattern Recognition

formable part model.
(CVPR-2008  2008.

[7] C. Gentile. The robustness of the p-norm algorithms. Machine Learning  53(3):265–299  2003.

[8] A. Kapoor and R. Greiner. Learning and classifying under hard budgets. Machine Learning:

ECML 2005  pages 170–181  2005.

[9] S.S. Keerthi and D. DeCoste. A modiﬁed ﬁnite newton method for fast solution of large scale

linear SVMs. Journal of Machine Learning Research  6(1):341  2006.

[10] S.A. Plotkin  D.B. Shmoys  and ´E. Tardos. Fast approximation algorithms for fractional pack-
ing and covering problems. In Proceedings of the 32nd annual symposium on Foundations of
computer science  pages 495–504. IEEE Computer Society  1991.

[11] A. Rahimi and B. Recht. Random features for large-scale kernel machines. Advances in neural

information processing systems  20:1177–1184  2008.

[12] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for SVM. In Proceedings of the 24th international conference on Machine learning  pages
807–814. ACM  2007.

[13] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size.
In Proceedings of the 25th international conference on Machine learning  pages 928–935 
2008.

[14] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates. In Advances in

Neural Information Processing Systems 23  pages 2199–2207. 2010.

9

,Shinichi Nakajima
Akiko Takeda
S. Derin Babacan
Masashi Sugiyama
Ichiro Takeuchi