2019,Convergence of Adversarial Training in Overparametrized Neural Networks,Neural networks are vulnerable to adversarial examples  i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classified by the network. Adversarial training \cite{madry2017towards}  a heuristic form of robust optimization that alternates between minimization and maximization steps  has proven to be among the most successful methods to train networks to be robust against a pre-defined family of perturbations. This paper provides a partial answer to the success of adversarial training  by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within $\epsilon$ of the optimal robust loss. Then we show that the optimal robust loss is also close to zero  hence adversarial training finds a robust classifier. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK)  combined with motivation from online-learning when the maximization is solved by a heuristic  and the expressiveness of the NTK kernel in the $\ell_\infty$-norm. In addition  we also prove that robust interpolation requires more model capacity  supporting the evidence that adversarial training requires wider networks.,Convergence of Adversarial Training in

Overparametrized Neural Networks

Ruiqi Gao1 ∗ Tianle Cai1 ∗ Haochuan Li2 Liwei Wang3 Cho-Jui Hsieh4

Jason D. Lee5

1School of Mathematical Sciences  Peking University

2Department of EECS  Massachusetts Institute of Technology

3Key Laboratory of Machine Perception  MOE  School of EECS  Peking University

4Department of Computer Science  University of California  Los Angeles

5Department of Electrical Engineering  Princeton University

Abstract

Neural networks are vulnerable to adversarial examples  i.e. inputs that are imper-
ceptibly perturbed from natural data and yet incorrectly classiﬁed by the network.
Adversarial training [31]  a heuristic form of robust optimization that alternates
between minimization and maximization steps  has proven to be among the most
successful methods to train networks to be robust against a pre-deﬁned family of
perturbations. This paper provides a partial answer to the success of adversarial
training  by showing that it converges to a network where the surrogate loss with
respect to the the attack algorithm is within  of the optimal robust loss. Then we
show that the optimal robust loss is also close to zero  hence adversarial training
ﬁnds a robust classiﬁer. The analysis technique leverages recent work on the
analysis of neural networks via Neural Tangent Kernel (NTK)  combined with mo-
tivation from online-learning when the maximization is solved by a heuristic  and
the expressiveness of the NTK kernel in the (cid:96)∞-norm. In addition  we also prove
that robust interpolation requires more model capacity  supporting the evidence
that adversarial training requires wider networks.

1

Introduction

Recent studies have demonstrated that neural network models  despite achieving human-level per-
formance on many important tasks  are not robust to adversarial examples—a small and human
imperceptible input perturbation can easily change the prediction label [44  22]. This phenomenon
brings out security concerns when deploying neural network models to real world systems [20]. In
the past few years  many defense algorithms have been developed [23  43  30  28  39] to improve the
network’s robustness  but most of them are still vulnerable under stronger attacks  as reported in [3].
Among current defense methods  adversarial training [31] has become one of the most successful
methods to train robust neural networks.
To obtain a robust network  we need to consider the “robust loss” instead of a regular loss. The robust
loss is deﬁned as the maximal loss within a neighborhood around the input of each sample  and
minimizing the robust loss under empirical distribution leads to a min-max optimization problem.
Adversarial training [31] is a way to minimize the robust loss. At each iteration  it (approximately)
solves the inner maximization problem by an attack algorithm A to get an adversarial sample  and
then runs a (stochastic) gradient-descent update to minimize the loss on the adversarial samples.
Although adversarial training has been widely used in practice and hugely improves the robustness
of neural networks in many applications  its convergence properties are still unknown. It is unclear

∗Joint ﬁrst author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

whether a network with small robust error exists and whether adversarial training is able to converge
to a solution with minimal adversarial train loss.
In this paper  we study the convergence of adversarial training algorithms and try to answer the above
questions on over-parameterized neural networks. We consider width-m neural networks both for the
setting of deep networks with H layers  and two-layer networks for some additional analysis. Our
contributions are summarized below.

• For an H-layer deep network with ReLU activations  and an arbitrary attack algorithm 
when the width m is large enough  we show that projected gradient descent converges to
a network where the surrogate loss with respect to the attack A is within  of the optimal
robust loss (Theorem 4.1). The required width is polynomial in the depth and the input
dimension.

• For a two-layer network with smooth activations  we provide a proof of convergence  where

the projection step is not required in the algorithm (Theorem 5.1).

• We then consider the expressivity of neural networks w.r.t. robust loss (or robust interpo-
lation). We show when the width m is sufﬁciently large  the neural network can achieve
optimal robust loss ; see Theorems 5.2 and C.1 for the precise statement. By combining
the expressivity result and the previous bound of the loss over the optimal robust loss  we
show that adversarial training ﬁnds networks of small robust training loss (Corollary 5.1 and
Corollary C.1).

• We show that the VC-Dimension of the model class which can robustly interpolate any
n samples is lower bounded by Ω(nd) where d is the dimension. In contrast  there are
neural net architectures that can interpolate n samples with only O(n) parameters and
VC-Dimension at most O(n log n). Therefore  the capacity required for robust learning is
higher.

2 Related Work

Attack and Defense Adversarial examples are inputs that are slightly perturbed from a natural
sample and yet incorrectly classiﬁed by the model. An adversarial example can be generated by
maximizing the loss function within an -ball around a natural sample. Thus  generating adversarial
examples can be viewed as solving a constrained optimization problem and can be (approximately)
solved by a projected gradient descent (PGD) method [31]. Some other techniques have also been
proposed in the literature including L-BFGS [44]  FGSM [22]  iterative FGSM [26] and C&W
attack [12]  where they differ from each other by the distance measurements  loss function or
optimization algorithms. There are also studies on adversarial attacks with limited information about
the target model. For instance  [13  24  8] considered the black-box setting where the model is hidden
but the attacker can make queries and get the corresponding outputs of the model.
Improving the robustness of neural networks against adversarial attacks  also known as defense  has
been recognized as an important and unsolved problem in machine learning. Various kinds of defense
methods have been proposed [23  43  30  28  39]  but many of them are based on obfuscated gradients
which does not really improve robustness under stronger attacks [3]. As an exception  [3] reported
that the adversarial training method developed in [31] is the only defense that works even under
carefully designed attacks.

Adversarial Training Adversarial training is one of the ﬁrst defense ideas proposed in earlier
papers [22]. The main idea is to add adversarial examples into the training set to improve the
robustness. However  earlier work usually only adds adversarial example once or only few times
during the training phase. Recently  [31] showed that adversarial training can be viewed as solving
a min-max optimization problem where the training algorithm aims to minimize the robust loss 
deﬁned as the maximal loss within a certain -ball around each training sample. Based on this
formulation  a clean adversarial training procedure based on PGD-attack has been developed and
achieved state-of-the-art results even under strong attacks. This also motivates some recent research
on gaining theoretical understanding of robust error [9  40]. Also  adversarial training suffers
from slow training time since it runs several steps of attacks within one update  and several recent
works are trying to resolve this issue [41  53]. From the theoretical perspective  a recent work [46]

2

considers to quantitatively evaluate the convergence quality of adversarial examples found in the
inner maximization and therefore ensure robustness. [51] consider generalization upper and lower
bounds for robust generalization. [29] improves the robust generalization by data augmentation with
GAN. [21] considers to reduce the optimization of min-max problem to online learning setting and
use their results to analyze the convergence of GAN. In this paper  our analysis for adversarial is
quite general and is not restricted to any speciﬁc kind of attack algorithm.

Global convergence of Gradient Descent Recent works on the over-parametrization of neural
networks prove that when the width greatly exceeds the sample size  gradient descent converges to a
global minimizer from random initialization [27  18  19  1  55]. The key idea in the earlier literature
is to show that the Jacobian w.r.t. parameters has minimum singular value lower bounded  and thus
there is a global minimum near every random initialization  with high probability. However for
the robust loss  the maximization cannot be evaluated and the Jacobian is not necessarily full rank.
For the surrogate loss  the heuristic attack algorithm may not even be continuous and so the same
arguments cannot be utilized.

Certiﬁed Defense and Robustness Veriﬁcation In contrast to attack algorithms  neural network
veriﬁcation methods [48  47  54  42  14  38] tries to ﬁnd upper bounds of the robust loss and provide
certiﬁed robustness measurements. Equipped with these veriﬁcation methods for computing upper
bounds of robust error  one can then apply adversarial training to get a network with certiﬁed
robustness. Our analysis in Section 4 can also be extended to certiﬁed adversarial training.

3 Preliminaries

3.1 Notations
Let [n] = {1  2  . . .   n}. We use N (0  I) to denote the standard Gaussian distribution. For a vector
v  we use (cid:107)v(cid:107)2 to denote the Euclidean norm. For a matrix A we use (cid:107)A(cid:107)F to denote the Frobenius
norm and (cid:107)A(cid:107)2 to denote the spectral norm. We use (cid:104)· ·(cid:105) to denote the standard Euclidean inner
product between two vectors  matrices  or tensors. We let O(·)  Θ(·) and Ω (·) denote standard Big-O 
Big-Theta and Big-Omega notations that suppress multiplicative constants.

3.2 Deep Neural Networks

Here we give the deﬁnition of our deep fully-connected neural networks. For the convenience of
proof  we use the same architecture as deﬁned in [1].2 Formally  we consider a neural network of the
following form.
Let x ∈ Rd be the input  the fully-connected neural network is deﬁned as follows: A ∈ Rm×d
is the ﬁrst weight matrix  W(h) ∈ Rm×m is the weight matrix at the h-th layer for h ∈ [H] 
a ∈ Rm×1 is the output layer  and σ(·) is the ReLU activation function.3 The parameters
are W = (vec{A}(cid:62)  vec{W(1)}(cid:62) ···   vec{W(H)}(cid:62)  a(cid:62))(cid:62). However  without loss of gen-
erality  during training we will ﬁx A and a once initialized  so later we will refer to W as
W = (vec{W(1)}(cid:62) ···   vec{W(H)}(cid:62))(cid:62). The prediction function is deﬁned recursively:

x(0) = Ax
x(h) = W(h)x(h−1) 
x(h) = σ

x(h)(cid:17)

(cid:16)

h ∈ [H]

 

h ∈ [H]

(1)

f (W  x) = a(cid:62)x(H) 

where x(h) and x(h) are the feature vectors before and after the activation function  respectively.
Sometimes we also denote x(0) = x(0).

2We only consider the setting when the network output is scalar. However  it is not hard to extend out results

to the setting of vector outputs.

our analysis to rectangular weight matrices.

3We assume intermediate layers are square matrices of size m for simplicity. It is not difﬁcult to generalize

3

We use the following initialization scheme: Each entry in A and W(h) for h ∈ [H] follows the
i.i.d. Gaussian distribution N (0  2
m )  and each entry in a follows the i.i.d. Gaussian distribution
(cid:80)n
N (0  1). As we mentioned  we only train on W(h) for h ∈ [H] and ﬁx a and A. For a training
i=1  the loss function is denoted (cid:96) : (R  R) (cid:55)→ R  and the (non-robust) training loss is
set {xi  yi}n
L(W) = 1
n
Assumption 3.1 (Assumption on the Loss Function). The loss (cid:96)(f (W  x)  y) is Lipschitz  smooth 
convex in f (W  x) and satisﬁes (cid:96)(y  y) = 0.

i=1 (cid:96)(f (W  xi)  yi). We make the following assumption on the loss function:

3.3 Perturbation and the Surrogate Loss Function

The goal of adversarial training is to make the model robust in a neighbor of each datum. We ﬁrst
introduce the deﬁnition of the perturbation set function to determine the perturbation at each point.
Deﬁnition 3.1 (Perturbation Set). Let the input space be X ⊂ Rd. The perturbation set function
is B : X → P(X )  where P(X ) is the power set of X . At each data point x  B(x) gives the
perturbation set on which we would like to guarantee robustness. For example  a commonly used
perturbation set is B(x) = {x(cid:48) : (cid:107)x(cid:48) − x(cid:107)2 ≤ δ}. Given a dataset {xi  yi}n
i=1  we say that the
perturbation set is compatible with the dataset if B(xi) ∩ B(xj) (cid:54)= φ implies yi = yj. In the rest of
the paper  we will always assume that B is compatible with the given data.
Given a perturbation set  we are now ready to deﬁne the perturbation function that maps a data point
to another point inside its perturbation set. We note that the perturbation function can be quite general
including the identity function and any adversarial attack4. Formally  we give the following deﬁnition.
Deﬁnition 3.2 (Perturbation Function). A perturbation function is deﬁned as a function A : W ×
Rd → Rd  where W is the parameter space. Given the parameter W of the neural network (1) 
A(W  x) maps x ∈ Rd to some x(cid:48) ∈ B(x) where B(x) refers to the perturbation set deﬁned in
Deﬁnition 3.1.

Without loss of generality  throughout Section 4 and 5  we will restrict our input x as well as the
perturbation set B(x) within the surface of the unit ball S = {x ∈ Rd : (cid:107)x(cid:107)2 = 1}.
With the deﬁnition of perturbation function  we can now deﬁne a large family of loss functions on the
training set {xi  yi}n
i=1. We will show this deﬁnition covers the standard loss used in empirical risk
minimization and the robust loss used in adversarial training.
Deﬁnition 3.3 (Surrogate Loss Function). Given a perturbation function A deﬁned in Deﬁnition 3.2 
the current parameter W of a neural network f  and a training set {xi  yi}n
i=1  we deﬁne the
surrogate loss LA(W) on the training set as

n(cid:88)

i=1

LA(W) =

1
n

(cid:96)(f (W A(W  xi))  yi).

It can be easily observed that the standard training loss L(W) is a special case of surrogate loss
function when A is the identity. The goal of adversarial training is to minimize the robust loss  i.e.
the surrogate loss when A is the strongest possible attack. The formal deﬁnition is as follows:
Deﬁnition 3.4 (Robust Loss Function). The robust loss function is deﬁned as

where

L∗(W) := LA∗ (W)

A∗(W  xi) = argmax
i∈B(xi)
x(cid:48)

(cid:96)(f (W  x(cid:48)

i)  yi).

4 Convergence Results of Adversarial Training
We consider optimizing the surrogate loss LA with the perturbation function A(W  x) deﬁned in
Deﬁnition 3.2  which is what adversarial training does given any attack algorithm A. In this section 

4It is also not hard to extend our analysis to perturbation functions involving randomness.

4

we will prove that for a neural network with sufﬁcient width  starting from the initialization W0 
after certain steps of projected gradient descent within a convex set B(R)  the loss LA is provably
upper-bounded by the best minimax robust loss in this set
L∗(W) 

min

(cid:26)

W∈B(R)

(cid:13)(cid:13)(cid:13)W(h) − W(h)

0

(cid:13)(cid:13)(cid:13)F

(cid:27)

W :

B(R) =

(2)
Denote PB(R) as the Euclidean projection to the convex set B(R). Denote the parameter W after
the t-th iteration as Wt  and similarly W(h)
. For each step in adversarial training  projected gradient
descent takes an update

.

t

≤ R√
m

  h ∈ [H]

where

i=1

(cid:17)

Vt+1 = Wt − α∇WLA(Wt) 
Wt+1 = PB(R)(Vt+1) 

(cid:17)

(cid:17)

(cid:16)

where

∇WLA(W) =

l(cid:48) (f (W A(W  xi))  yi)∇Wf (W A(W  xi)) 

n(cid:88)
∂f   the gradient ∇Wf is with respect to the ﬁrst argument W.

1
n
and the derivative (cid:96)(cid:48) stands for ∂(cid:96)
Speciﬁcally  we have the following theorem.
Theorem 4.1 (Convergence of Projected Gradient Descent for Optimizing Surrogate Loss). Given
 > 0  suppose R = Ω(1)  and m ≥ max
. Let the loss function satisfy
Assumption 3.1.5 If we run projected gradient descent based on the convex constraint set B(R) with
steps  then with high probability we have
(3)

stepsize α = O(cid:0) 

= Ω
LA(Wt) − L∗(W∗) ≤  

(cid:16) R9H 16
(cid:16) R2H 2
(cid:17)

t=1 ···  T
where W∗ = arg minW∈B(R) L∗(W).
Remark. Recall that LA(W) is the loss suffered with respect to the perturbation function A. This
means  for example  if the adversary uses the projected gradient ascent algorithm  then the theorem
guarantees that projected gradient ascent cannot successfully attack the learned network. The
stronger the attack algorithm is during training  the stronger the guaranteed surrogate loss becomes.
Remark. The value of R depends on the approximation capability of the network  i.e. the greater R
is  the less L∗(W∗) will be  thus affecting the overall bound on mint LA(Wt). We will elaborate on
this in the next section  where we show that for R independent of m there exists a network of small
adversarial training error.

(cid:1) for T = Θ

(cid:16) R2

  Θ(d2)

mα

min

Θ

7

mH 2

2

4.1 Proof Sketch

Our proof idea utilizes the same high-level intuition as [1  27  18  55  10  11] that near the initialization
the network is linear. However  unlike these earlier works  the surrogate loss neither smooth  nor
semi-smooth so there is no Polyak gradient domination phenomenon to allow for the global geometric
contraction of gradient descent. In fact due to the the generality of perturbation function A allowed 
the surrogate loss is not differentiable or even continuous in W  and so the standard analysis cannot
be applied. Our analysis utilizes two key observations. First the network f (W A(W  x)) is still
smooth w.r.t. the ﬁrst argument6  and is close to linear in the ﬁrst argument near initialization  which
is shown by directly bounding the Hessian w.r.t. W. Second  the perturbation function A can be
treated as an adversary providing a worst-case loss function (cid:96)A(f  y) as done in online learning.
However  online learning typically assumes the sequence of losses is convex  which is not the case
here. We make a careful decoupling of the contribution to non-convexity from the ﬁrst argument
and the worst-case contribution from the perturbation function  and then we can prove that gradient
descent succeeds in minimizing the surrogate loss. The full proof is in Appendix A.

5We actually didn’t use the assumption (cid:96)(y  y) = 0 in the proof  so common loss functions like the cross-
entropy loss works in this theorem. Also  with some slight modiﬁcations  it is possible to prove for other loss
functions including the square loss.

6It is not jointly smooth in W  which is part of the subtlety of the analysis.

5

5 Adversarial Training Finds Robust Classiﬁer

Motivated by the optimization result in Theorem 4.1  we hope to show that there is indeed a robust
classiﬁer in B(R). To show this  we utilize the connection between neural networks and their induced
Reproducing Kernel Hilbert Space (RKHS) via viewing networks near initialization as a random
feature scheme [15  16  25  2]. Since we only need to show the existence of a network architecture
that robustly ﬁts the training data in B(R) and neural networks are at least as expressive as their
induced kernels  we may prove this via the RKHS connection. The strategy is to ﬁrst show the
existence of a robust classiﬁer in the RKHS  and then show that a sufﬁciently wide network can
approximate the kernel via random feature analysis. The approximation results of this section will be 
in general  exponential in dimension dependence due to the known issue of d-dimensional functions
having exponentially large RKHS norm [4]  so only offer qualitative guidance on existence of robust
classiﬁers.
Since deep networks contain two-layer networks as a sub-network  and we are concerned with
expressivity  we focus on the local expressivity of two-layer networks. We write the standard
two-layer network in the suggestive way7 (where the width m is an even number)

m/2(cid:88)

m/2(cid:88)

  

f (W  x) =

1√
m

arσ(w(cid:62)

r x) +

a(cid:48)
rσ( ¯w(cid:62)

r x)

r=1

r=1

(4)

for r = 1 ···   m

and initialize as wr ∼ N (0  Id) i.i.d.
2   and ¯wr is set to be equal to wr 
r = −ar. Similarly  we deﬁne the set B(R) =
ar is randomly drawn from {1 −1} and a(cid:48)
{W : (cid:107)W − W0(cid:107)F ≤ R}8 for W = (w1 ···   wm/2  ¯w1 ···   ¯wm/2)  W0 being the initialization
of W  and ﬁx all ar after initialization.
To make things cleaner  we will use a smooth activation function σ(·) throughout this section9 
formally stated as follows.
Assumption 5.1 (Smoothness of Activation Function). The activation function σ(·) is smooth  that
is  there exists an absolute constant C > 0 such that for any z  z(cid:48) ∈ R

|σ(cid:48)(z) − σ(cid:48)(z(cid:48))| ≤ C|z − z(cid:48)|.

Prior to proving the approximation results  we would like to ﬁrst provide a version of convergence
theorem similar to Theorem 4.1  but for this two-layer setting. It is encouraged that the reader can
read Appendix B for the proof of the following Theorem 5.1 ﬁrst  since it is relatively cleaner than
that of the deep setting but the proof logic is analogous.
Theorem 5.1 (Convergence of Gradient Descent without Projection for Optimizing Surrogate Loss
for Two-layer Networks). Suppose the loss function satisﬁes Assumption 3.1 and the activation
function satisﬁes Assumption 5.1. With high probability  using the two-layer network deﬁned above 
for any  > 0  if we run gradient descent with step size α = O ()  and if m = Ω

(cid:16) R4

  we have

(cid:17)

2

LA(Wt) − L∗(W∗) ≤  

(5)

min

t=1 ···  T

√
α ).

m

where W∗ = minW∈B(R) L∗(W) and T = Θ(
Remark. Compared to Theorem 4.1  we do not need the projection step for this two-layer theorem.
We believe using a smooth activation function can also eliminate the need of the projection step in the
deep setting from a technical perspective  and from a practical sense we conjecture that the projection
step is not needed anyway.

Now we’re ready to proceed to the approximation results  i.e. proving that L∗(W∗) is also small 
and combined with Equation (5) we can give an absolute bound on mint LA(Wt). For the reader’s
convenience  we ﬁrst introduce the Neural Tangent Kernel (NTK) [25] w.r.t. our two-layer network.

7This makes f (W  x) = 0 at initialization  which helps eliminate some unnecessary technical nuisance.
8Note that we have taken out the term 1√

m explicitly in the network expression for convenience  so in this

section there is a difference of scaling by a factor of

m from the W used in the previous section.

9Similar approximation results also hold for other activation functions like ReLU.

√

6

Deﬁnition 5.1 (NTK [25]). The NTK with activation function σ (·) and initialization distribution
w ∼ N (0  Id) is deﬁned as Kσ(x  y) = Ew∼N (0 Id)(cid:104)xσ(cid:48)(w(cid:62)x)  yσ(cid:48)(w(cid:62)y)(cid:105).
For a given kernel K  there is a reproducing kernel Hilbert space (RKHS) introduced by K. We
denote it as H(K). We refer the readers to [36] for an introduction of the theory of RKHS.
We formally make the following assumption on the universality of NTK.
Assumption 5.2 (Existence of Robust Classiﬁer in NTK). For any  > 0  there exists f ∈ H(Kσ) 
such that |f (x(cid:48)
Also  we make an additional assumption on the activation function σ(·):
Assumption 5.3 (Lipschitz Property of Activation Function). The activation function σ(·) satisﬁes
|σ(cid:48)(z)| ≤ C ∀z ∈ R for some constant C.

i) − yi| ≤   for every i ∈ [n] and x(cid:48)

i ∈ B(xi).

Under these assumptions  by applying the strategy of approximating the inﬁnite situation by ﬁnite
sum of random features  we can get the following theorem:
Theorem 5.2 (Existence of Robust Classiﬁer near Initialization). Given data set D = {(xi  yi)}n
and a compatible perturbation set function B with xi and its allowed perturbations taking value on
S  for the two-layer network deﬁned in (4)  if Assumption 3.1  5.1  5.2  5.3 hold  then for any   δ > 0 
there exists RD B  such that when the width m satisﬁes m = Ω
  with probability at least
0.99 over the initialization there exists W such that

(cid:16) R4D B 

(cid:17)

i=1

2

L∗(W) ≤  and W ∈ B(RD B ).

Combining Theorem 5.1 and 5.2 we ﬁnally know that
Corollary 5.1 (Adversarial Training Finds a Network of Small Robust Training Loss). Given data
set on the unit sphere equipped with a compatible perturbation set function and an associated
perturbation function A  which also takes value on the unit sphere. Suppose Assumption 3.1  5.1  5.2 
5.3 are satisﬁed. Then there exists a RD B  which only depends on dataset D  perturbation B and
)  if we run gradient
  such that for any 2-layer fully connected network with width m = Ω(
descent with stepsize α = O () for T = Θ(

R4D B 

R2D B 

2

α ) steps  then with probability 0.99 
LA(Wt) ≤ .

min

t=1 ···  T

(6)

Remark 5.1. We point out that Assumption 5.2 is rather general and can be veriﬁed for a large class
of activation functions by showing their induced kernel is universal as done in [32]. Also  here we use
an implicit expression of the radius BD B   but the dependence on  can be calculated under speciﬁc
activation function with or without the smoothness assumptions. As an example  using quadratic
ReLU as activation function  we solve the explicit dependency on  in Appendix C.2 that doesn’t rely
on Assumption 5.2.

Therefore  adversarial training is guaranteed to ﬁnd a robust classiﬁer under a given attack algorithm
when the network width is sufﬁciently large.

6 Capacity Requirement of Robustness

In this section  we will show that in order to achieve adversarially robust interpolation (which is
formally deﬁned below)  one needs more capacity than just normal interpolation. In fact  empirical
evidence have already shown that to reliably withstand strong adversarial attacks  networks require a
signiﬁcantly larger capacity than for correctly classifying benign examples only [31]. This implies 
in some sense  that using a neural network with larger width is necessary.
Let Sδ = {(x1 ···   xn) ∈ (Rd)n : (cid:107)xi − xj(cid:107)2 > 2δ} and Bδ(x) = {x(cid:48) : (cid:107)x(cid:48) − x(cid:107)2 ≤ δ}  where
δ is a constant. We consider datasets in Sδ and use Bδ as the perturbation set function in this section.
We begin with the deﬁnition of the interpolation class and the robust interpolation class.

7

Deﬁnition 6.1 (Interpolation class). We say that a function class F of functions f : Rd → {1 −1}is
an n-interpolation class10  if the following is satisﬁed:

∀(x1 ···   xn) ∈ Sδ ∀(y1 ···   yn) ∈ {±1}n 
∃f ∈ F  s.t. f (xi) = yi ∀i ∈ [n].

Deﬁnition 6.2 (Robust interpolation class). We say that a function class F is an n-robust interpolation
class  if the following is satisﬁed:

∀(x1 ···   xn) ∈ Sδ ∀(y1 ···   yn) ∈ {±1}n 
i ∈ Bδ(xi) ∀i ∈ [n].
∃f ∈ F  s.t.f (x(cid:48)

i) = yi ∀x(cid:48)

We will use the VC-Dimension of a function class F to measure its complexity. In fact  as shown in
[6] (Equation(2))  for neural networks there is a tight connection between the number of parameters
W   the number of layers H and their VC-Dimension

Ω(HW log(W/H)) ≤ VC-Dimension ≤ O(HW log W ).

In addition  combining with the results in [52] (Theorem 3) which shows the existence of a 4-layer
neural network with O(n) parameters that can interpolate any n data points  i.e. an n-interpolation
class  we have that an n-interpolation class can be realized by a ﬁxed depth neural network with
VC-Dimension upper bound

VC-Dimension ≤ O(n log n).

(7)
For a general hypothesis class F  we can evidently see that when F is an n-interpolation class  F
has VC-Dimension at least n. For a neural network that is an n-interpolation class  without further
architectural constraints  this lower bound of its VC-dimension is tight up to logarithmic factors as
indicated in Equation (7). However  we show that for a robust-interpolation class we will have a
much larger VC-Dimension lower bound:
Theorem 6.1. If F is an n-robust interpolation class  then we have the following lower bound on
the VC-Dimension of F

VC-Dimension ≥ Ω(nd) 

(8)

where d is the dimension of the input space.

For neural networks  Equation (8) shows that any architecture that is an n-robust interpolation
class should have VC-Dimension at least Ω(nd). Compared with Equation (7) which shows an
n-interpolation class can be realized by a network architecture with VC-Dimension O(n log n)  we
can conclude that robust interpolation by neural networks needs more capacity  so increasing the
width of neural network is indeed in some sense necessary.

7 Discussion on Limitations and Future Directions

This work provides a theoretical analysis of the empirically successful adversarial training algorithm
in the training of robust neural networks. Our main results indicate that adversarial training will
ﬁnd a network of low robust surrogate loss  even when the maximization is computed via a heuristic
algorithm such as projected gradient ascent. However  there are still some limitations with our current
theory  and we also feel our results can lead to several thought-provoking future work  which is
discussed as follows.
Removal of projection. It is also natural to ask whether the projection step can be removed  as it is
empirically unnecessary and also unnecessary for our two-layer analysis. We believe using smooth
activations might resolve this issue from a technical perspective  although practically it seems the
projection step in the algorithm is unnecessary in any case.
Generalizing to different attacks. Firstly  our current guarantee of the surrogate loss is based on the
same perturbation function as that used during training. It is natural to ask that whether we can ensure
10Here we let the classiﬁcation output be ±1  and a usual classiﬁer f outputting a number in R can be treated

as sign(f ) here.

8

the surrogate loss is low with respect to a larger family of perturbation functions than that used during
training.
Exploiting structures of network and data. Same as the recent proof of convergence on overparame-
terized networks in the non-robust setting  our analysis fails to further incorporate useful network
structures apart from being sufﬁciently wide  and as a result increasing depth can only hurt the bound.
It would be interesting to provide ﬁner analysis based on additional assumptions on the alignment of
the network structure and data distribution.
Improving the approximation bound. On the expressivity side  the current argument utilizes that a
neural net restricted to a local region can approximate its induced RKHS. Although the RKHS is
universal  they do not avoid the curse of dimensionality (see Appendix C.2). However  we believe in
reality  the required radius of region R to achieve robust approximation is not as large as the theorem
demands. So an interesting question is whether the robust expressivity of neural networks can adapt
to structures such as low latent dimension of the data mechanism [17  50]  thereby reducing the
approximation bound.
Capacity requirement of robustness and robust generalization. Apart from this paper  there are other
works supporting the need for capacity including the perspective of network width [31]  depth [49]
and computational complexity [35]. It is argued in [51] that robust generalization is also harder
using Rademacher complexity. In fact  it appears empirically that robust generalization is even
harder than robust training. It is observed that increasing the capacity  though beniﬁting the dacay of
training loss  has much less effect on robust generalization. There are also other factors behind robust
generalization  like the number of training data [40]. The questions about robust generalization  as
well as to what extent capacity inﬂunces it  are still subject to much debate.
The above are several interesting directions of further improvement to our current result. In fact 
many of these questions are largely unanswered even for neural nets in the non-robust setting  so we
leave them to future work.

8 Acknowlegements

We acknowlegde useful discussions with Siyu Chen  Di He  Runtian Zhai  and Xiyu Zhai. RG and
TC are partially supported by the elite undergraduate training program of School of Mathematical
Sciences in Peking University. LW acknowledges support by Natioanl Key R&D Program of China
(no. 2018YFB1402600)  BJNSF (L172037). JDL acknowledges support of the ARO under MURI
Award W911NF-11-1-0303  the Sloan Research Fellowship  and NSF CCF #1900145.

References
[1] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via

over-parameterization. arXiv preprint arXiv:1811.03962  2018.

[2] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584  2019.

[3] Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense of

security: Circumventing defenses to adversarial examples. In ICML  2018.

[4] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal

of Machine Learning Research  18(1):629–681  2017.

[5] Francis Bach. On the equivalence between kernel quadrature rules and random feature expan-

sions. The Journal of Machine Learning Research  18(1):714–751  2017.

[6] Peter L Bartlett  Nick Harvey  Christopher Liaw  and Abbas Mehrabian. Nearly-tight vc-
dimension and pseudodimension bounds for piecewise linear neural networks. Journal of
Machine Learning Research  20(63):1–17  2019.

9

[7] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint

arXiv:1905.12173  2019.

[8] Wieland Brendel  Jonas Rauber  and Matthias Bethge. Decision-based adversarial attacks:
Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248 
2017.

[9] Sébastien Bubeck  Eric Price  and Ilya Razenshteyn. Adversarial examples from computational

constraints. arXiv preprint arXiv:1805.10204  2018.

[10] Tianle Cai  Ruiqi Gao  Jikai Hou  Siyu Chen  Dong Wang  Di He  Zhihua Zhang  and Liwei
Wang. A gram-gauss-newton method learning overparameterized deep neural networks for
regression problems. arXiv preprint arXiv:1905.11675  2019.

[11] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide

and deep neural networks. arXiv preprint arXiv:1905.13210  2019.

[12] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In

2017 IEEE Symposium on Security and Privacy (SP)  pages 39–57. IEEE  2017.

[13] Pin-Yu Chen  Huan Zhang  Yash Sharma  Jinfeng Yi  and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security  pages 15–26.
ACM  2017.

[14] Jeremy M Cohen  Elan Rosenfeld  and J Zico Kolter. Certiﬁed adversarial robustness via

randomized smoothing. arXiv preprint arXiv:1902.02918  2019.

[15] Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural

Information Processing Systems  pages 2422–2430  2017.

[16] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems  pages 2253–2261  2016.

[17] Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with

quadratic activation. arXiv preprint arXiv:1803.01206  2018.

[18] Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds

global minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018.

[19] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054  2018.

[20] Kevin Eykholt  Ivan Evtimov  Earlence Fernandes  Bo Li  Amir Rahmati  Chaowei Xiao  Atul
Prakash  Tadayoshi Kohno  and Dawn Song. Robust physical-world attacks on deep learning
models. arXiv preprint arXiv:1707.08945  2017.

[21] Alon Gonen and Elad Hazan. Learning in non-convex games with an optimization oracle. arXiv

preprint arXiv:1810.07362  2018.

[22] Ian Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations  2015.

[23] Chuan Guo  Mayank Rana  Moustapha Cisse  and Laurens van der Maaten. Countering

adversarial images using input transformations. arXiv preprint arXiv:1711.00117  2017.

[24] Andrew Ilyas  Logan Engstrom  Anish Athalye  and Jessy Lin. Black-box adversarial attacks
with limited queries and information. In International Conference on Machine Learning  pages
2142–2151  2018.

10

[25] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and

generalization in neural networks. arXiv preprint arXiv:1806.07572  2018.

[26] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial machine learning at scale.

arXiv preprint arXiv:1611.01236  2016.

[27] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic

gradient descent on structured data. arXiv preprint arXiv:1808.01204  2018.

[28] Xuanqing Liu  Minhao Cheng  Huan Zhang  and Cho-Jui Hsieh. Towards robust neural
networks via random self-ensemble. In European Conference on Computer Vision  pages
381–397. Springer  2018.

[29] Xuanqing Liu and Cho-Jui Hsieh. Rob-gan: Generator  discriminator  and adversarial attacker.

In CVPR  2019.

[30] Xingjun Ma  Bo Li  Yisen Wang  Sarah M Erfani  Sudanthi Wijewickrema  Michael E Houle 
Grant Schoenebeck  Dawn Song  and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. arXiv preprint arXiv:1801.02613  2018.

[31] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 
2017.

[32] Charles A Micchelli  Yuesheng Xu  and Haizhang Zhang. Universal kernels. Journal of

Machine Learning Research  7(Dec):2651–2667  2006.

[33] Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for learning with

drifting distributions. In Algorithmic Learning Theory  pages 124–138. Springer  2012.

[34] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of Machine Learning.

MIT Press  2018.

[35] Preetum Nakkiran. Adversarial robustness may be at odds with simplicity. arXiv preprint

arXiv:1901.00532  2019.

[36] Vern I Paulsen and Mrinal Raghupathi. An introduction to the theory of reproducing kernel

Hilbert spaces  volume 152. Cambridge University Press  2016.

[37] Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases. In
2008 46th Annual Allerton Conference on Communication  Control  and Computing  pages
555–561. IEEE  2008.

[38] Hadi Salman  Greg Yang  Huan Zhang  Cho-Jui Hsieh  and Pengchuan Zhang. A convex relax-
ation barrier to tight robust veriﬁcation of neural networks. arXiv preprint arXiv:1902.08722 
2019.

[39] Pouya Samangouei  Maya Kabkab  and Rama Chellappa. Defense-GAN: Protecting classiﬁers
against adversarial attacks using generative models. arXiv preprint arXiv:1805.06605  2018.

[40] Ludwig Schmidt  Shibani Santurkar  Dimitris Tsipras  Kunal Talwar  and Aleksander Madry.
Adversarially robust generalization requires more data. In Advances in Neural Information
Processing Systems  pages 5014–5026  2018.

[41] Ali Shafahi  Mahyar Najibi  Amin Ghiasi  Zheng Xu  John Dickerson  Christoph Studer 
Larry S Davis  Gavin Taylor  and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843  2019.

[42] Gagandeep Singh  Timon Gehr  Matthew Mirman  Markus Püschel  and Martin Vechev. Fast
and effective robustness certiﬁcation. In Advances in Neural Information Processing Systems 
pages 10802–10813  2018.

11

[43] Yang Song  Taesup Kim  Sebastian Nowozin  Stefano Ermon  and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766  2017.

[44] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[45] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[46] Yisen Wang  Xingjun Ma  James Bailey  Jinfeng Yi  Bowen Zhou  and Quanquan Gu. On the
convergence and robustness of adversarial training. In International Conference on Machine
Learning  pages 6586–6595  2019.

[47] Tsui-Wei Weng  Huan Zhang  Hongge Chen  Zhao Song  Cho-Jui Hsieh  Luca Daniel  Duane
Boning  and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for relu networks.
In International Conference on Machine Learning  pages 5273–5282  2018.

[48] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning  pages 5283–5292 
2018.

[49] Cihang Xie and Alan Yuille. Intriguing properties of adversarial training. arXiv preprint

arXiv:1906.03787  2019.

[50] Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks.

arXiv preprint arXiv:1802.03620  2018.

[51] Dong Yin  Kannan Ramchandran  and Peter Bartlett. Rademacher complexity for adversarially

robust generalization. arXiv preprint arXiv:1810.11914  2018.

[52] Chulhee Yun  Suvrit Sra  and Ali Jadbabaie. Finite sample expressive power of small-width

relu networks. arXiv preprint arXiv:1810.07770  2018.

[53] Dinghuai Zhang  Tianyuan Zhang  Yiping Lu  Zhanxing Zhu  and Bin Dong. You only propagate
once: Painless adversarial training using maximal principle. arXiv preprint arXiv:1905.00877 
2019.

[54] Huan Zhang  Tsui-Wei Weng  Pin-Yu Chen  Cho-Jui Hsieh  and Luca Daniel. Efﬁcient neural
network robustness certiﬁcation with general activation functions. In Advances in Neural
Information Processing Systems  pages 4939–4948  2018.

[55] Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent optimizes

over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888  2018.

12

,Ruiqi Gao
Tianle Cai
Haochuan Li
Cho-Jui Hsieh
Liwei Wang
Jason Lee