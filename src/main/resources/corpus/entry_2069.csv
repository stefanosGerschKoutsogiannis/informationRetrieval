2019,Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes,The developments of Rademacher complexity and PAC-Bayesian theory have been largely independent. One exception is the PAC-Bayes theorem of Kakade  Sridharan  and Tewari (2008)  which is established via Rademacher complexity theory by viewing Gibbs classifiers as linear operators. The goal of this paper is to extend this bridge between Rademacher complexity and state-of-the-art PAC-Bayesian theory. We first demonstrate that one can match the fast rate of Catoni's PAC-Bayes bounds (Catoni  2007) using shifted Rademacher processes (Wegkamp  2003; Lecué and Mitchell  2012; Zhivotovskiy and Hanneke  2018). We then derive a new fast-rate PAC-Bayes bound in terms of the "flatness" of the empirical risk surface on which the posterior concentrates. Our analysis establishes a new framework for deriving fast-rate PAC-Bayes bounds and yields new insights on PAC-Bayesian theory.,Fast-rate PAC-Bayes Generalization Bounds via

Shifted Rademacher Processes

Department of Statistical Sciences

Department of Computer Science

Shengyang Sun∗

University of Toronto 

Vector Institute

ssy@cs.toronto.edu

Jun Yang∗

University of Toronto 

Vector Institute

jun@utstat.toronto.edu

Daniel M. Roy

Department of Statistical Sciences

University of Toronto 

Vector Institute

droy@utstat.toronto.edu

Abstract

The developments of Rademacher complexity and PAC-Bayesian theory have
been largely independent. One exception is the PAC-Bayes theorem of Kakade 
Sridharan  and Tewari [21]  which is established via Rademacher complexity the-
ory by viewing Gibbs classiﬁers as linear operators. The goal of this paper is
to extend this bridge between Rademacher complexity and state-of-the-art PAC-
Bayesian theory. We ﬁrst demonstrate that one can match the fast rate of Catoni’s
PAC-Bayes bounds [8] using shifted Rademacher processes [27  43  44]. We then
derive a new fast-rate PAC-Bayes bound in terms of the “ﬂatness” of the empirical
risk surface on which the posterior concentrates. Our analysis establishes a new
framework for deriving fast-rate PAC-Bayes bounds and yields new insights on
PAC-Bayesian theory.

1

Introduction

PAC-Bayes theory [33  38] was developed to provide probably approximately correct (PAC) guar-
antees for supervised learning algorithms whose outputs can be expressed as a weighted majority
vote. Its uses have expanded considerably since [3  6  14  17  18  28  39  40]. See [12  25  32]
for gentle introductions. Indeed  there has been a surge of interest and work in PAC-Bayes the-
ory and its application to large-scale neural networks  especially towards studying generalization in
overparametrized neural networks trained by variants of gradient descent [9–11  30  36  37].
PAC-Bayes bounds are one of several tools available for the study of the generalization and risk
properties of learning algorithms. One advantage of the PAC-Bayes framework is its ease of use: one
can obtain high-probability risk bounds for arbitrary (“posterior”) Gibbs classiﬁers provided one can
compute or bound relative entropies with respect to some ﬁxed (“prior”) Gibbs classiﬁer. Another
tool for studying generalization is Rademacher complexity  a distribution-dependent complexity
measure for classes of real-valued functions [4  5  23  29  34  44].
The literature on PAC-Bayes bounds and bounds based on Rademacher complexity are essentially
disjoint. One point of contact is the work of Kakade  Sridharan  and Tewari [21]  which builds the
ﬁrst bridge between PAC-Bayes theory and Rademacher complexity. By viewing Gibbs classiﬁers as

∗These authors contributed equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

linear operators and relative entropy as a strictly convex regularizer  they were able to use their gen-
eral Rademacher complexity bounds on strictly convex linear classes to develop a slightly sharper
version of McAllester’s PAC-Bayes bound [33]. This result offers new insight on PAC-Bayes the-
ory  including potential roles for data-dependent complexity estimates and stability. However  even
within the PAC-Bayes community  this result is relatively unknown.
√
While the PAC-Bayes bound established by Kakade  Sridharan  and Tewari
improves on
m rate  where m denotes the number of data
McAllester’s bound  it still converges at a slow 1/
used to form the empirical risk estimate. This observation raises the question of whether one can
match state-of-the-art PAC-Bayes bounds via a Rademacher-process argument. In particular  can
one match Catoni’s bound [8  Thm. 1.2.6]  which can obtain a fast 1/m rate of convergence?
There is an extensive literature on the problem of obtaining fast 1/m rates of convergence for the gen-
eralization error of (approximate) empirical risk minimization (ERM). Available approaches include
the use of local Rademacher complexity [4  22]  shifted empirical processes [27]  offset Rademacher
complexities [29]  and local empirical entropy [44]. See also [16  19  20  26  31  35] and [13] for an
extensive survey. To date  these techniques have not been connected to PAC-Bayesian theory  which
presents the opportunity to obtain new PAC-Bayes theory for ERM.

1.1 Contributions

In this paper  we extend the bridge between Rademacher process theory and PAC-Bayes theory by
constructing new bounds using Rademacher process techniques. Among our contributions:

i) We show how to recover Catoni’s fast-rate PAC-Bayes bound [8]  up to constants  using tail
bounds on shifted Rademacher processes  which are special cases of shifted empirical processes
[27  43  44]; See Section 3.

ii) We derive a new fast-rate PAC-Bayes bound  building on our shifted-Rademacher-process ap-
proach. This bound is determined by the “ﬂatness” of the empirical risk surface on which the
posterior Gibbs classiﬁer concentrates. The notion of “ﬂatness” is inspired by the proposal
by Dziugaite and Roy [9] to formalize the empirical connection between “ﬂat minima” and
generalization using PAC-Bayes bounds; See Section 4.

iii) More generally  we introduce a new approach to derive fast-rate PAC-Bayes bounds and  in

turn  offer new insight on PAC-Bayesian theory.

2 Background

Let D be an unknown distribution over a space Z of labeled examples  and let H be a hypothesis
class. Relative to a binary loss function (cid:96) : H × Z → {0 1}  we deﬁne the associated loss class
F := {(cid:96)(h ·) : h ∈ H } of functions from Z → {0 1}  each associated to one or more hypotheses.
Let LD ( f ) := Ez∼D f (z) denote the expected loss  i.e.  risk  of every hypothesis associated to f . Let
S = (z1 ···  zm) ∼ D m be a sequence of i.i.d. random variables. Let
i=1 f (zi) denote
the empirical risk of every hypothesis associated to f .
We will be primarily interested in Gibbs classiﬁers  i.e.  distributions P on F which are interpreted
as randomized classiﬁers that classify each new example according to a hypothesis drawn indepen-
dently from P. (It is more common to work with distributions over H   but these lead to looser
results.) For a Gibbs classiﬁer P and labeled example z ∈ Z   let EP f (z) = E f∼P[ f (z)] be the ex-
pected loss P suffers when labeling z. For Gibbs classiﬁers  the (expected) risk is deﬁned to be
LD (P) := E f∼PLD ( f ) = Ez∼DEP f (z). The (expected) empirical risk is
ˆLS(P) := E f∼P ˆLS( f ) =
m ∑m
1

i=1 EP f (zi).

ˆLS( f ) = 1

m ∑m

2.1 PAC-Bayes

The PAC-Bayes framework [33] provides data-dependent generalization guarantees for Gibbs clas-
siﬁers. Each bound is speciﬁed in terms of a Gibbs classiﬁer P called the prior  as it must be
independent of the training sample. The bound then holds for all posterior distributions  i.e.  Gibbs
classiﬁers that may be deﬁned in terms of the training sample.

2

Theorem 2.1 (PAC-Bayes [33]). For any prior distribution P over F   for any δ ∈ (0 1)  with
probability at least 1− δ over draws of training data S ∼ D m  for all distributions Q over F  

LD (Q) ≤ ˆLS(Q) +

KL (Q||P) + log m
δ

2(m− 1)

.

(1)

(cid:115)

Note in Theorem 2.1  the generalization bound scales as O(m− 1
PAC-Bayesian bound  in which the generalization bound scales as O(m−1).
Theorem 2.2 (Fast-Rate PAC-Bayes [8  Thm 1.2.6]). For any prior distribution P over F   for any
δ ∈ (0 1) and C > 0  with probability at least 1− δ over draws of training data S ∼ D m  for all
distributions Q over F  

2 ). Catoni [8] presents a fast rate

LD (Q) ≤ 1

1− e−C

C ˆLS(Q) +

KL (Q||P) + log 1
δ

m

.

(2)

(cid:34)

(cid:35)

Because the constant C/(1− e−C) > 1 holds for any C > 0  the generalization bound in Theorem 2.2
will always be bounded below by the empirical risk. Usually for a well-trained distribution Q over
training set  the empirical risk ˆLS(Q) is small  therefore the generalization bound is dominated by
the KL term. Compared to the standard PAC-Bayes bound in Theorem 2.1  where the KL term
decreases at a rate O(m− 1
2 )  the KL term of Catoni’s bound decreases at a rate O(m−1). For this
reason  we say that Catoni’s bound achieves a fast rate of convergence. Note that fast-rate bounds
can lead to much tighter bounds. Of course  C/(1− e−C) → 1 as C → 0  but  in that limit  the
constants ignored in the asymptotic rate O(m−1) degrade. (See [28] for more discussion.)

2.2 Rademacher Viewpoint

ˆLS(Q)) as the inner product (cid:104)dQ/dP  LD (·)(cid:105) (resp.  (cid:104)dQ/dP 

(cid:104)g h(cid:105) =(cid:82) g( f )h( f )P(d f ). The key observation of Kakade  Sridharan  and Tewari is that one can

Fix a prior Gibbs classiﬁer P on F . Then  for measurable functions g h  consider the inner product
ˆLS(·)(cid:105)) between
view LD (Q) (resp. 
the posterior Q  represented by its Radon–Nikodym derivative with P  and the risk (resp.  empir-
ical risk)  viewed as measurable function on F . Thus  Gibbs classiﬁers can be viewed as linear
predictors. Using their distribution-independent bounds on the Rademacher complexity of certain
classes of linear predictors  Kakade  Sridharan  and Tewari [21] derive a PAC-Bayes bound similar
to Theorem 2.1. We refer to this as the “Rademacher viewpoint” on PAC-Bayes.
We now summarize their argument in more detail. Let Q(κ) := {Q : KL (Q||P) ≤ κ}. One
can follow the classical steps for controlling the generalization error uniformly over Q(κ) us-
ing Rademacher complexity. Their ﬁrst step is to connect supQ∈Q(κ)
ES supQ∈Q(κ)
In particular  with probability at least 1− δ  

(cid:2)LD (Q)− ˆLS(Q)(cid:3) to
(cid:2)LD (Q)− ˆLS(Q)(cid:3) by the bounded difference inequality (McDiarmid’s inequality).
(cid:114)log(1/δ )
(cid:35)

Then they apply a symmetrization argument to obtain an upper bound in terms of Rademacher
complexity [5]. In particular  recalling that S = (z1 ···  zm) is our training data 

(cid:2)LD (Q)− ˆLS(Q)(cid:3) ≤ ES

(cid:2)LD (Q)− ˆLS(Q)(cid:3) +

Q∈Q(κ)

Q∈Q(κ)

sup

sup

(3)

.

m

sup

Q∈Q(κ)

(4)
where {εi} are i.i.d. Rademacher random variables  i.e.  P(εi = +1) = P(εi = −1) = 1/2. Their last
step is to bound the Rademacher complexity ESE supQ∈Q(κ)
as the Rademacher complexity of a linear class with a (strongly) convex constraint [21]. According

i=1 εiEQ f (zi)(cid:3)  which can be seen
to [21]  the Rademacher complexity in Eq. (4) is of order(cid:112)κ/m  which eventually leads to a term
of order(cid:112)KL (Q||P) /m after applying a union bound argument on κ.

Q∈Q(κ)

m ∑m

sup

i=1

 

(cid:2)LD (Q)− ˆLS(Q)(cid:3) ≤ 2ESE

m

∑

1
m

εiEQ f (zi)

(cid:34)
(cid:2) 1

ES

In the end  using the above arguments and their sharp bounds on the Rademacher and Gaussian
complexities of (constrained) linear classes [21  Thm. 1]  Kakade  Sridharan  and Tewari obtain the

3

following PAC-Bayes bound [21  Cor. 8]: for every prior P over F   with probability at least 1− δ
over draws of training data S ∼ D m  for all distribution Q over F  

(cid:114)max{KL (Q||P)  2}
Note that this PAC-Bayes bound has a slow rate of(cid:112)1/m  but it slightly improves the rate in the
term(cid:112)log(m/δ )/m of McAllester’s bound [33] to(cid:112)log(1/δ )/m.

(cid:114)log(1/δ )

LD (Q) ≤ ˆLS(Q) + 4.5

(5)

m

+

.

m

Since McAllester’s bound is far from the state-of-art in PAC-Bayesian theory  this raises the ques-
tion whether one can extend the “Rademacher viewpoint” of PAC-Bayes to derive more advanced
bounds  such as one matching the fast rate of Catoni’s bound.

3 Extending the Rademacher Viewpoint

m ∑m

i=1 ε(cid:48)

There are at least two difﬁculties in the “Rademacher viewpoint” that prevent fast rates. First  if we
connect the generalization error to Rademacher complexity using the bounded difference inequality 

a slow rate term(cid:112)log(1/δ )/m will appear. Second  as is shown by Kakade  Sridharan  and Tewari
a slow rate of order O((cid:112)KL (Q||P) /m). Therefore  in order to derive fast rate PAC-Bayes bounds 

[21]  the standard Rademacher complexity of (constraint) linear classes leads to an upper bound with

i f (zi)} f∈F where the variables {ε(cid:48)

rate term of(cid:112)log(1/δ )/m. It remains to bound the deviation of shifted Rademacher processes to

we need to extend the “Rademacher viewpoint”.
In order to obtain fast rates  we work with so-called shifted Rademacher processes  i.e.  processes
of the form { 1
i} are independent from S  i.i.d.  and take
two values with equal probability. (These shifted Rademacher variables  {ε(cid:48)
i}  are not necessarily
zero mean. When they take values in {±1}  we obtain a standard Rademacher process.) Shifted
Rademacher processes are examples of shifted empirical processes [27  43  44].
Recall that Rademacher complexity is the expected value of the supremum of Rademacher processes
over a class [5]. In order to get a fast rate  we connect the tail probabilities of the supremum of the
generalization error to the tail probabilities of shifted Rademacher processes via a symmetrization-
in-deviation argument instead of the symmetrization-in-expectation argument. The key is that we
can avoid using the bounded difference inequality by bounding the deviation. This removes the slow
get a fast rate bound of order O(KL (Q||P) /m).
In the following  we demonstrate how the extended “Rademacher viewpoint” via shifted
Rademacher processes can be applied to derive a fast rate PAC-Bayes bound that matches the fast
rate of Catoni’s bound. Note that  since C/(1− e−C) > 1 for ﬁxed C > 0 in Catoni’s bound in Eq. (2) 
we can write C/(1− e−C) = 1 + c for some constant c > 0. Furthermore  note that our goal in this
section is not to derive new PAC-Bayes bounds. Therefore  we do not make attempts to optimize the
constants.
Proposition 3.1 (Matching Catoni’s Fast Rate via Shifted Rademacher Processes). For any given
c > 0 and prior P over F   there exists constants C1  C2  and C3 such that  with probability at least
1− δ   for all distributions Q over F

LD (Q) ≤(1 + c) ˆLS(Q) +C1

KL (Q||P)

m

+C2

log 1
δ
m +C3

1
m .

(6)

Outline of the proof. We wish to emphasize two key differences from traditional machinery for de-
riving Rademacher-complexity-based generalization bounds. The complete proof is given in Ap-
pendix A.1.
Fix P and let Q(κ) := {Q : KL (Q||P) ≤ κ} be deﬁned as in Section 2.2. Rather than control
supQ∈Q(κ)
Rademacher complexity  we bound the tail/deviation of supQ∈Q(κ)
avoiding the use of the bounded differences inequality altogether. In particular  we can obtain fast
rates by bounding the tail in terms of tail of supremum of shifted Rademacher processes [27  43 
44].

(cid:2)LD (Q)− ˆLS(Q)(cid:3) in terms of its expectation via the bounded difference inequality and
(cid:2)LD (Q)− (1 + c) ˆLS(Q)(cid:3)  thus

4

Deﬁne Gκ := {EQ f (·) : Q ∈ Q(κ)} and  by an abuse of notation  let LD (g) denote Ez∼D [g(z)].
Then we can write supQ∈Q(κ)
start from bounding the tail probability PS
(cid:32)
c > c2 > 0  let c(cid:48) = c−c2
1+c2

(cid:2)LD (g)− (1 + c) ˆLS(g)(cid:3). We
LD (g)− (1 + c) ˆLS(g) ≥ t(cid:1). For ﬁxed constants
(cid:33)
(cid:34)1 + c(cid:48)

(cid:0)supg∈Gκ
(cid:32)

(cid:2)LD (Q)− (1 + c) ˆLS(Q)(cid:3) as supg∈Gκ
(cid:18)
εi − c(cid:48)
2 + c(cid:48)

2(1+c2). Then  by [44  Cor. 1]  we have

LD (g)− (1 + c) ˆLS(g) ≥ t

and t(cid:48) = t

≤ 4PS ε

≥ t(cid:48)
2

(cid:33)

(cid:19)

g(zi)

. (7)

(cid:35)

∑

PS

m

m

2

sup
g∈Gκ

i=1

i} are i.i.d. “shifted” Rademacher random variables with

2+c(cid:48)   one can see that {ε(cid:48)

i := εi − c(cid:48)
(cid:20) 1
2+c(cid:48) . For any g ∈ Gκ  there exists Q ∈ Q(κ) such that
ε(cid:48)
iEQ f (zi) = EQ

ε(cid:48)
i g(zi) =

m

m

1
m

∑

i=1

1
m

∑

i=1

(cid:21)

m

∑

i=1

m

ε(cid:48)
i f (zi)

 

(8)

sup
g∈Gκ
Letting ε(cid:48)
mean − c(cid:48)

which can be viewed as a linear function of Q. Further  it can be veriﬁed that the set Q(κ) is
iEQ f (zi) is a convex optimization problem. By
(strongly) convex. Therefore  supQ∈Q(κ)
duality [7  Chp. 5]  and  in this particular case  the Legendre transform of Kullback–Leibler diver-
gence (see  e.g.  [18])  we have

i=1 ε(cid:48)

m ∑m
1

(cid:34)

(cid:32)

(cid:33)(cid:35)(cid:41)

1
m

sup
g∈Gκ

m

∑

i=1

ε(cid:48)
i g(zi) = sup
Q∈Q(κ)

1
m

m

∑

i=1

ε(cid:48)
iEQ f (zi) = inf
λ >0

κ
λ +

1
λ

logEP

exp

λ
m

m

∑

i=1

ε(cid:48)
i f (zi)

.

(9)

(cid:32)

Combining the shifted symmetrization in deviation in Eq. (7) and the dual problem in Eq. (9) 
Markov’s inequality yields  for every λ > 0 
EQ[LD ( f )− (1 + c) ˆLS( f )] ≥ t

≤ 4eκ− λt(cid:48)

2+c(cid:48) ESEεEP

ε(cid:48)
i f (zi)

(10)

exp

sup

PS

m

.

(cid:33)(cid:35)

(cid:33)

(cid:32)

(cid:34)

(cid:40)

Q∈Q(κ)

λ
m

∑

i=1

We then exploit the shifted property of ε(cid:48)
obtain fast rates. In particular  we show that  so long as k ≥ logcosh(λ /m)

 

i to bound the expectation term on the right-hand side and

(cid:34)

(cid:32)

λ
m

m

∑

i=1

(cid:33)(cid:35)

λ /m

EPESE

exp

(εi − k) f (zi)

≤ 1.

(11)

In our case  k = c(cid:48)
2+c(cid:48)   which leads to constraints relating λ   c  and c2. In particular  when c = 0 
the required condition for the above result  k ≥ logcosh(λ /m)
  does not hold. Therefore  this approach
obtains fast rates only if c > 0  i.e.  if we shift. Combing Eqs. (10) and (11)  there exists a constant
C(cid:48)  depending only on c  c2 and δ   such that  with probability at least 1− δ  

λ /m

EQ[LD ( f )− (1 + c) ˆLS( f )] ≤ C(cid:48)

m (κ + log(4/δ )).

sup

Q∈Q(κ)

(12)

Finally  we may apply the same union-bound argument as in the proof of [21  Cor. 7] in order to
cover all possible values of κ. This completes the proof.

4 New Fast Rate PAC-Bayes Bound based on “Flatness”

The extended “ Rademacher viewpoint” of PAC-Bayes provides a new approach for deriving fast-
rate PAC-Bayes bounds. In this section  we demonstrate the use of shifted Rademacher processes to
derive a new fast-rate PAC-Bayes bound using a notion of “ﬂatness”. This notion is inspired by the
proposal by Dziugaite and Roy [9] to formalize the empirical connection between “ﬂat minima” and
generalization using PAC-Bayes bounds  and  in particular  posterior distributions which concentrate
in these “ﬂat minima”.
Deﬁnition 4.1 (Notion of “Flatness”). For given h ∈ [0 1]  the “h-ﬂatness” of Q (w.r.t. S) is

1
m

m

∑

i=1

EQ[ f (zi)− (1 + h)EQ f (zi)]2.

(13)

5

One way to understand this new notion is to observe that  under zero–one loss  h-ﬂatness can be
written as the difference between the empirical risk and the quadratic empirical risk:

1
m

m

∑

i=1

EQ[ f (zi)− (1 + h)EQ f (zi)]2 = ˆLS(Q)− 1− h2

m

m

∑

i=1

(EQ f (zi))2.

(14)

Note that  for [0 1]-valued (bounded) loss  equality is replaced by an inequality: the r.h.s. is an upper
bound of the l.h.s.
Remark 4.2. To see that optimizing h-ﬂatness prefers “ﬂat minima”  consider the following simpli-
ﬁed case: Call a posterior Q “completely ﬂat” if f = g on S a.s.  when f  g ∼ Q. It can be veriﬁed
that  if the posterior is “completely ﬂat”  then under the zero–one loss  the “h-ﬂatness” is h2 ˆLS(Q).
That is  given a “completely ﬂat” posterior  the “h-ﬂatness” goes to zero as h → 0. For h > 0  the
“h-ﬂatness” is zero when Q is “completely ﬂat” and ˆLS(Q) = 0.
(cid:47)
The following PAC-Bayes theorem establishes favorable bounds for h-ﬂat posteriors:
Theorem 4.3 (Fast Rate PAC-Bayes using “Flatness”). For any given c > 0 and h ∈ (0 1)  with
probability at least 1−δ over random draws of training set S ∼ D m  for all distributions Q over F  

(cid:20)

3KL (Q||P) + log

 

(15)

(cid:21)
1
δ + 5

LD (Q) ≤ ˆLS(Q) +

c
m

where C = 2h4c

1+16h2c .

m

∑

i=1

EQ[ f (zi)− (1 + h)EQ f (zi)]2 +

4
Cm

This bound can be tighter than Catoni’s bound under certain conditions. We delay the comparison
with Catoni’s bound to Section 4.1. We now give an outline of the proof of Theorem 4.3  high-
lighting the technical differences from the proof of Proposition 3.1. The complete proof is given in
Appendix A.2.

Outline of the proof of Theorem 4.3. By Eq. (14)  we can write

EQLD ( f )− ˆLS(Q)− c
m

m

∑

i=1

= LD (Q)− (1 + c) ˆLS(Q) +

EQ[ f (zi)− (1 + h)EQ f (zi)]2

c(1− h2)

m

(EQ f (zi))2.

m

∑

i=1

(16)

m ∑m

There are at least two new challenges compared with the proof of Proposition 3.1. First  the
shifted symmetrization in Eq. (7) cannot be applied because of the existence of the quadratic term
c(1−h2)
i=1(EQ f (zi))2. This means we need to derive a new shifted symmetrization involving the
quadratic term. Second  the quadratic term c(1−h2)
i=1(EQ f (zi))2 cannot be seen as a linear func-
tion of Q. Therefore  some technical arguments are required in order to apply the Legendre transform
of Kullback–Leibler divergence.
First  we derive a new shifted symmetrization which involves quadratic terms. The proof is inspired
by an argument due to Zhivotovskiy and Hanneke [44]. The result extends [44  Cor. 1]  which
is recovered as a special case when h = 1. For κ > 0  recall that we have deﬁned Q(κ) = {Q :
KL (Q||P) ≤ κ} and Gκ = {EQ f (·) : Q ∈ Q(κ)}. Then for any g ∈ Gκ  there exists a Q ∈ Q(κ)
such that g = EQ f (·). We can ﬁrst show a tail bound that for any given c2 > 0 and g ∈ Gκ  if
t ≥ (1+c2)(1+c2h2)

m ∑m

  then

(cid:16)LD (g)− (1 + c2) ˆLS(g) + c2(1− h2) ˆLS(g2) ≥ t

(cid:17) ≤ 1

PS

(17)
m} ∈ D m. For c > c2  by taking
Then  consider another independent random data set S(cid:48) = {z(cid:48)
the difference of LD (g)− (1 + c) ˆLS(g) + c(1− h2) ˆLS(g2) and LD (g)− (1 + c2) ˆLS(cid:48)(g) + c2(1−

1  . . .  z(cid:48)

2

2

.

mc2h2

6

h2) ˆLS(cid:48) (g2) and using Eq. (17)  we obtain

LD (g)− (1 + c) ˆLS(g) + c(1− h2) ˆLS(g2) ≥ t

(cid:33)

PS

1
4
≤ 1
2

sup
g∈Gκ

(cid:32)

PS S(cid:48)

sup
g∈Gκ

(1 +

c + c2

2

(cid:32)

(cid:32)

(18)

(cid:33)

.

(19)

(20)

(cid:33)

≥ t
4

 

(21)

(1 + c2) ˆLS(cid:48)(g)− c2(1− h2) ˆLS(cid:48)(g2)− (1 + c) ˆLS(g) + c(1− h2) ˆLS(g2) ≥ t
2

Now by writing (1 + c2) ˆLS(cid:48)(g)− c2(1− h2) ˆLS(cid:48) (g2)− (1 + c) ˆLS(g) + c(1− h2) ˆLS(g2) as

2

2

ˆLS

)(cid:0) ˆLS(cid:48)(g)− ˆLS(g)(cid:1)− c + c2
− c− c2

(1− h2)(cid:0) ˆLS(cid:48)(g2)− ˆLS(g2)(cid:1)
ˆLS(cid:48)(cid:0)g− (1− h2)g2(cid:1)  
(cid:0)g− (1− h2)g2(cid:1)− c− c2
(cid:33)
(cid:0)g− (1− h2)g2(cid:1)(cid:35)
(cid:0)(cid:0)1 + c(cid:48)(cid:1)g(zi)− c(cid:48)(1− h2)g2(zi)(cid:1)− c(cid:48)(cid:48)

ˆLS

2

one can apply the symmetrization argument to get

LD (g)− (1 + c) ˆLS(g) + c(1− h2) ˆLS(g2) ≥ t

PS

1
4

sup
g∈Gκ

(cid:32)

(cid:34)

m

∑

1
m

≤ PS 

i=1

εi
sup
g∈Gκ
2  c(cid:48)(cid:48) = c−c2
where c(cid:48) = c+c2
involving a quadratic term.
Recalling the deﬁnition of Gκ  we have

2 . Therefore  we have derived the new shifted symmetrization in deviation

1
m

sup
g∈Gκ

m

∑

i=1

(cid:0)(cid:0)1 + c(cid:48)(cid:1)g(zi)− c(cid:48)(1− h2)g(zi)2(cid:1)− c(cid:48)(cid:48)

(cid:0)1 + c(cid:48)(cid:1)− c(cid:48)(cid:48)]EQ f (zi)−(cid:2)εic(cid:48) − c(cid:48)(cid:48)(cid:3) (1− h2)[EQ f (zi)]2.

ˆLS(g− (1− h2)g2)

m

∑

[εi

εi

1
m

i=1

Q∈Q(κ)

= sup

(22)
Note that there are two shifted Rademacher random variables εi (1 + c(cid:48))− c(cid:48)(cid:48) and εic(cid:48) − c(cid:48)(cid:48)  which
not only involve a shift term −c(cid:48)(cid:48) but also scale terms (1 + c(cid:48)) and c(cid:48)  respectively. Furthermore  the
term [EQ f (zi)]2 cannot be seen as a linear function of Q. This prevents the use of the key argument
in [21] to formulate an upper bound using Rademacher complexities of constrained linear classes by
considering the generalization error as a linear function of Q.
In order to sidestep this obstruction  deﬁne  := {εi}m
i=1 and suppose ˆQ( z) achieves
the supremum above. (If the supremum cannot be achieved  one can use a carefully chosen sequence
of { ˆQi( z)} to prove the same statement as the supremum can be approximated arbitrarily closely.)
The following inequality then holds:

i=1 z := {zi}m

(cid:0)1 + c(cid:48)(cid:1)− c(cid:48)(cid:48)]EQ f (zi)−(cid:2)εic(cid:48) − c(cid:48)(cid:48)(cid:3) (1− h2)[EQ f (zi)]2
(cid:0)1 + c(cid:48)(cid:1)− c(cid:48)(cid:48)]EQ f (zi)−(cid:2)εic(cid:48) − c(cid:48)(cid:48)(cid:3) (1− h2)EQ f (zi)E ˆQ( z) f (zi).

[εi

sup

1
m
Q∈Q(κ)
≤ sup
Q∈Q(κ)

m

∑
i=1
1
m

[εi

m

∑

i=1

(23)

:= εic(cid:48) − c(cid:48)(cid:48) = εi

To see this  note that  on the one hand  if we plug in Q = ˆQ( z) the inequality is tight; on the other
hand  by deﬁnition  Q = ˆQ( z) already achieves the supremum of the l.h.s. Note that the r.h.s. can
be seen as a linear function of Q  because ˆQ( z) is a random variable which does not depend on Q.
2 − c1−c2
Let ε(cid:48)(cid:48)
. Then by keeping the term ˆQ( z)  one can apply the convex
i
2
conjugate of relative entropy to get
(cid:34)

LD (Q)− (1 + c) ˆLS(Q) +

(EQ f (zi))2 ≥ t

c(1− h2)

(cid:32)

(cid:34)

(cid:35)

∑

c1+c2

sup

m

P

m

(cid:105)(cid:33)(cid:35)

i=1

(cid:104)(cid:0)εi + ε(cid:48)(cid:48)

i

(cid:1)− ε(cid:48)(cid:48)

i (1− h2)E ˆQ( z) f (zi)

.

(24)

Q∈Q(κ)
≤ 4exp

(cid:19)

(cid:18)
κ − λt
4

ESEEP

exp

λ
m

m

∑

i=1

f (zi)

7

Therefore  the problem turns to bounding the expectation of a function involving shifted Rademacher
processes. Although the expectation looks quite complicated since it involves two scaled and shifted
Rademacher variables as well as the unknown ˆQ( z)  fortunately  we are able to show that  for any
random variables Yi ∈ [0 1]  we have
λ
m

f (zi)(cid:2)(cid:0)εi + ε(cid:48)(cid:48)

(cid:1)− ε(cid:48)(cid:48)

i (1− h2)Yi

(cid:3)(cid:33)(cid:35)

ESEEP

≤ 1 

(cid:32)

(cid:34)

(25)

i

exp

m

∑

i=1

if h ∈ (0 1] 1 > h2c > c2 > 0 and 0 < λ
2(1+h2c)(1+c2). This result removes the term ˆQ( z)
by letting Yi = E ˆQ( z) f (zi). Finally  we combine different values of κ by a union bound argument
similar to the proof of Proposition 3.1 to complete the proof.

m < C =

h2c−c2

4.1 Comparison with Catoni’s Bound

m ∑m

m ∑m

i=1 EQ[ f (zi)− (1 + h)EQ f (zi)]2 is very small yet

As we have shown in Proposition 3.1  using shifted Rademacher processes  we can match Catoni’s
fast-rate PAC-Bayesian bound (Theorem 2.2) up to constants. We have also presented a new fast-
rate PAC-Bayes bound based on "ﬂatness". Although both our bound and Catoni’s bound show fast
O(m−1) rates of convergence  our bound can exploit ﬂatness in the posterior distribution.
In particular  our PAC-Bayes bound based on ﬂatness (Eq. (15)) can be much tighter than
Catoni’s bound (Eq. (6)) when the posterior is chosen to concentrate on a “ﬂat minimum” where
ˆLS(Q) is nonzero. It can be veriﬁed that the
c
m ∑m
i=1 EQ[ f (zi)− (1 + h)EQ f (zi)]2 in Eq. (15) is smaller than the excess empirical
“ﬂatness” term c
risk term c ˆLS(Q) when 1−h2
i=1(EQ f (zi))2 is greater than 0  which is precisely when the empirical
risk is greater than zero. (See Eq. (14).)
Based on this observation  we expect our bound to be tighter for sufﬁcient ﬂat posteriors  nonzero
empirical risk  and sufﬁcient training data. In order to see this  note that Catoni’s bound has the
m (KL(Q(cid:107)P) + log 1
form (1 + cc) ˆLSQ +
Cc
δ )  while our bound based on Eq. (14) can be written (1 +
cr) ˆLSQ− cr(1−h2)
i=1(EQ f (zi))2 + Cr
m ∑m
δ +1). Here cc cr inﬂate the empirical risk
and Cc  Cr are constants. Let Tm be cr(1−h2)
i=1(EQ f (zi))2. Note that cc and cr must be ﬁxed before
seeing the data. Assuming we equate the inﬂation of the empirical risk  i.e.  cc = cr  the proposed
bound is tighter than Catoni’s bound provided m > 1
Tm
converges to a positive number (a reasonable assumption)  then our proposed bound will be tighter
for sufﬁciently many samples. If we assume cc (cid:54)= cr  our bound can still be tighter than Catoni’s
bound under more involved conditions.

(cid:0)(Cr − Cc)(cid:0)KL(Q(cid:107)P) + log 1

m (KL(Q(cid:107)P) +log 1
m ∑m

(cid:1). If Tm

(cid:1) + Cr

δ

5 Related Work

There is a large literature on obtaining fast 1/m convergence rates for generalization error and excess
risk using Rademacher processes and their generalizations [4  22  27  29  44]. As far as we know 
this literature does not connect with the PAC-Bayesian literature. There do exist  however  PAC-
Bayesian analyses for speciﬁc learning algorithms that achieve fast rates [2  15  24]. These speciﬁc
analyses do not lead to general PAC-Bayes bounds  like those produced by Catoni [8].
Our new PAC-Bayes bound based on ﬂatness bears a superﬁcial resemblance to a number of bounds
in the literature. However  our notion of ﬂatness is not related to the variance of the randomized
classiﬁer caused by the randomness of the observed data. Therefore  our new bound is fundamentally
different from existing PAC-Bayes bounds based on this type of variance [15  24  41].
For example  Tolstikhin and Seldin [41  Thm. 4] presents a generalization bound based on
the “empirical variance”  which is distinct from our "ﬂatness". The “empirical variance” is
i=1 EQ[ f (zi) − EQ f (zi)]2. Note that
EQ
it is possible for ﬂatness to be zero  even when empirical variance is large.
To the best of our knowledge  the closest work to ours in the literature is that by Audibert [2].
The bound given in [2  Thm. 6.1] uses a notion similar to our “ﬂatness”. The bound is  however 
not comparable with ours for several reasons: First  [2  Theorem 6.1] holds only for the particular

i=1 f (zi)]2  while our “ﬂatness” is 1

i=1[ f (zi) − 1

m ∑m
1

m ∑m

m ∑m

8

algorithm proposed by Audibert  and so it is not a general PAC-Bayes bound like ours. Second 
our notion of “ﬂatness” is empirical  while the “ﬂatness” term in [2  Theorem 6.1] is deﬁned by
an expectation over the data distribution  which is often presumed unknown. Finally  the proof
techniques used to establish [2  Theorem 6.1] are specialized to the proposed algorithm and not
based on the use of Rademacher processes. Our proof techniques via shifted Rademacher processes
provides a blueprint for other approaches to deriving fast-rate PAC-Bayes bounds.
Grünwald and Mehta [17] establish new excess risk bounds in terms of a novel complexity measure
based on “luckiness” functions. In the setting of randomized classiﬁers  particular choices of luck-
iness functions can be related to PAC-Bayesian notions of complexity based on “priors”. Indeed 
in this setting  their complexity measure can be bounded in terms of a KL divergence  as in PAC-
Bayesian bounds. In a setting with deterministic classiﬁers  the authors show that their complexity
measure can be bounded in terms of Rademacher complexity. Thus  while their framework connects
with both PAC-Bayesian and Rademacher-complexity bounds  it is not immediately clear whether
it produces direct connections  as we have accomplished here. It is certainly interesting to consider
whether our bounds can be achieved (or surpassed) by an appropriate use of their framework.

6 Conclusion

In this paper we exploit the connections between modern PAC-Bayesian theory and Rademacher
complexities. Using shifted Rademacher processes [27  43  44]  we derive a novel fast-rate PAC-
Bayes bound that depends on the empirical "ﬂatness" of the posterior. Our work provides new
insights on PAC-Bayesian theory and opens up new avenues for developing stronger bounds.
It is worth highlighting some potentially interesting directions that may be worth further investiga-
tion:
We have “rederived” Catoni’s bound via shifted Rademacher processes  up to constants. It is inter-
esting to ask whether the Rademacher approach can dominate the direct PAC-Bayes bound. In the
other direction  we have not derived our ﬂatness bound via a direct PAC-Bayes approach. Whether
this is possible and what it achieves might shed light on the relative strengths of these two distinct
approaches to PAC-Bayes bounds. It may also be interesting to pursue PAC-Bayes bounds via some
adaptation of Talagrand’s concentration inequalities [42  Ch.3].
We have derived PAC-Bayes bounds for zero–one loss. While the extension to bounded loss is
straightforward  the problem of extending our approach to unbounded loss relates to a growing
body of work on this problem within the PAC-Bayesian framework. (See  for example  [1] and the
references therein). Whether the Rademacher perspective is helpful or not in this regard is not clear
at this point.
There has been a surge of interest in PAC-Bayes bounds and their application to the study of general-
ization in large-scale neural networks. One promising direction is to consider Rademacher-process
techniques may aid in the development of PAC-Bayesian analyses of speciﬁc algorithms [2  15 
24]  especially in the case when the algorithms are related to large-scale neural networks trained by
stochastic gradient descent [30  36  37].
It would be interesting to perform a careful empirical study of our ﬂatness bound in the context of
large-scale neural networks  in the vein of the work of Dziugaite and Roy [9]. Preliminary work
suggests that the posteriors found by PAC-Bayes bound optimization are not ﬂat in our sense. After
some investigation  we believe the reason is that optimizing the PAC-Bayes bound results in under-
ﬁtting  due in part to the distribution-independent prior. It would be interesting to compare various
PAC-Bayes bounds under strict constraints on the empirical risk.

Acknowledgments

We would like to also thank Peter Bartlett  Gintare Karolina Dziugaite  Roger Grosse  Yasaman
Mahdaviyeh  Zacharie Naulet  and Sasha Rakhlin for helpful discussions.
In particular  the au-
thors would like to thank Sasha Rakhlin for introducing us to the work of Kakade  Sridharan  and
Tewari [21]. The work beneﬁtted also from constructive feedback from anonymous referees. JY was
supported by an Alexander Graham Bell Canada Graduate Scholarship (NSERC CGS D)  Ontario
Graduate Scholarship (OGS)  and Queen Elizabeth II Graduate Scholarship in Science and Technol-
ogy (QEII-GSST). SS was supported by a Borealis AI Global Fellowship Award  Connaught New

9

Researcher Award  and Connaught Fellowship. DMR was supported by an NSERC Discovery Grant
and Ontario Early Researcher Award.

References

[1] P. Alquier and B. Guedj. “Simpler PAC-Bayesian bounds for hostile data”. Machine Learning

[2]

[3]

107.5 (2018)  pp. 887–902.
J.-Y. Audibert. “Fast learning rates in statistical inference through aggregation”. The Annals
of Statistics 37.4 (2009)  pp. 1591–1646.
J.-Y. Audibert and O. Bousquet. “Combining PAC-Bayesian and Generic Chaining Bounds”.
Journal of Machine Learning Research 8 (2007)  pp. 863–889.

[4] P. L. Bartlett  O. Bousquet  and S. Mendelson. “Local Rademacher Complexities”. The Annals

of Statistics 33.4 (2005)  pp. 1497–1537.

[5] P. L. Bartlett and S. Mendelson. “Rademacher and Gaussian complexities: Risk bounds and

structural results”. Journal of Machine Learning Research 3 (2002)  pp. 463–482.

[6] L. Bégin  P. Germain  F. Laviolette  and J.-F. Roy. “PAC-Bayesian bounds based on the Rényi

divergence”. In: Artiﬁcial Intelligence and Statistics. 2016  pp. 435–444.

[7] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[8] O. Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical
Learning. Vol. 56. Lecture Notes – Monograph Series. Institute of Mathematical Statistics 
2007.

[9] G. K. Dziugaite and D. M. Roy. “Computing Nonvacuous Generalization Bounds for Deep
(Stochastic) Neural Networks with Many More Parameters than Training Data”. In: Proceed-
ings of the 33rd Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2017.

[10] G. K. Dziugaite and D. M. Roy. “Data-dependent PAC-Bayes priors via differential privacy”.

In: Advances in Neural Information Processing Systems. 2018  pp. 8430–8441.

[11] G. K. Dziugaite and D. M. Roy. “Entropy-SGD optimizes the prior of a PAC-Bayes bound:
Generalization properties of Entropy-SGD and data-dependent priors”. In: International Con-
ference on Machine Learning. 2018  pp. 1376–1385.

[12] T. van Erven. PAC-Bayes Mini-tutorial: A Continuous Union Bound. 2014. arXiv: 1405 .

1580.

[13] T. van Erven  P. D. Grünwald  N. A. Mehta  M. D. Reid  and R. C. Williamson. “Fast Rates in
Statistical and Online Learning”. Journal of Machine Learning Research 16 (2015)  pp. 1793–
1861.

[14] P. Germain  F. Bach  A. Lacoste  and S. Lacoste-Julien. “PAC-Bayesian theory meets
Bayesian inference”. In: Advances in Neural Information Processing Systems. 2016 
pp. 1884–1892.

[15] P. Germain  A. Lacasse  F. Laviolette  M. Marchand  and J.-F. Roy. “Risk bounds for the ma-
jority vote: From a PAC-Bayesian analysis to a learning algorithm”. The Journal of Machine
Learning Research 16.1 (2015)  pp. 787–860.

[16] E. Giné and V. Koltchinskii. “Concentration inequalities and asymptotic results for ratio type

empirical processes”. The Annals of Probability 34.3 (2006)  pp. 1143–1216.

[17] P. D. Grünwald and N. A. Mehta. “A tight excess risk bound via a uniﬁed PAC-Bayesian–
Rademacher–Shtarkov–MDL complexity”. In: Algorithmic Learning Theory. 2019  pp. 433–
465. arXiv: 1710.07732.

[18] B. Guedj. A primer on PAC-Bayesian learning. 2019. arXiv: 1901.05353.
[19] S. Hanneke. “Reﬁned error bounds for several learning algorithms”. The Journal of Machine

Learning Research 17.1 (2016)  pp. 4667–4721.

[20] S. Hanneke and L. Yang. “Minimax analysis of active learning”. The Journal of Machine

Learning Research 16.1 (2015)  pp. 3487–3602.

10

[21] S. M. Kakade  K. Sridharan  and A. Tewari. “On the complexity of linear prediction: risk
bounds  margin bounds  and regularization”. In: Advances in Neural Information Processing
Systems. 2008  pp. 793–800.

[22] V. Koltchinskii. “Local Rademacher complexities and oracle inequalities in risk minimiza-

tion”. The Annals of Statistics 34.6 (2006)  pp. 2593–2656.

[23] V. Koltchinskii and D. Panchenko. “Empirical margin distributions and bounding the gener-

alization error of combined classiﬁers”. The Annals of Statistics 30.1 (2002)  pp. 1–50.

[24] A. Lacasse  F. Laviolette  M. Marchand  P. Germain  and N. Usunier. “PAC-Bayes bounds for
the risk of the majority vote and the variance of the Gibbs classiﬁer”. In: Advances in Neural
Information Processing Systems. 2007  pp. 769–776.
J. Langford. “Tutorial on practical prediction theory for classiﬁcation”. Journal of Machine
Learning Research 6.Mar (2005)  pp. 273–306.

[25]

[26] G. Lecué and S. Mendelson. Learning subgaussian classes: Upper and minimax bounds.

2013. arXiv: 1305.4825.

[27] G. Lecué and C. Mitchell. “Oracle inequalities for cross-validation type procedures”. Elec-

tronic Journal of Statistics 6 (2012)  pp. 1803–1837.

[28] G. Lever  F. Laviolette  and J. Shawe-Taylor. “Tighter PAC-Bayes bounds through

distribution-dependent priors”. Theoretical Computer Science 473 (2013)  pp. 4–28.

[29] T. Liang  A. Rakhlin  and K. Sridharan. “Learning with square loss: Localization through
offset Rademacher complexity”. In: Conference on Learning Theory. 2015  pp. 1260–1285.
[30] B. London. “A PAC-Bayesian analysis of randomized learning with application to stochastic
gradient descent”. In: Advances in Neural Information Processing Systems. 2017  pp. 2931–
2940.

[31] P. Massart and É. Nédélec. “Risk bounds for statistical learning”. The Annals of Statistics

34.5 (2006)  pp. 2326–2366.

[32] D. A. McAllester. A PAC-Bayesian Tutorial with A Dropout Bound. 2013. arXiv: 1307.2118.
[33] D. A. McAllester. “PAC-Bayesian Model Averaging”. In: Conference on Learning Theory.

1999  pp. 164–170.

[34] S. Mendelson. “Learning without concentration”. In: Conference on Learning Theory. 2014 

pp. 25–39.

[35] S. Mendelson. ““Local” vs. “global” parameters–breaking the Gaussian complexity barrier”.

The Annals of Statistics 45.5 (2017)  pp. 1835–1862.

[36] B. Neyshabur  S. Bhojanapalli  D. McAllester  and N. Srebro. “Exploring generalization in
deep learning”. In: Advances in Neural Information Processing Systems. 2017  pp. 5947–
5956.

[37] B. Neyshabur  S. Bhojanapalli  and N. Srebro. A PAC-Bayesian approach to spectrally-

normalized margin bounds for neural networks. 2017. arXiv: 1707.09564.
J. Shawe-Taylor and R. C. Williamson. “A PAC Analysis of a Bayesian Estimator”. In: Con-
ference on Learning Theory. 1997  pp. 2–9.

[38]

[39] S. L. Smith and Q. V. Le. A Bayesian perspective on generalization and stochastic gradient

descent. 2017. arXiv: 1710.06451.

[40] N. Thiemann  C. Igel  O. Wintenberger  and Y. Seldin. A strongly quasiconvex PAC-Bayesian

bound. 2016. arXiv: 1608.05610.
I. O. Tolstikhin and Y. Seldin. “PAC-Bayes-empirical-Bernstein inequality”. In: Advances in
Neural Information Processing Systems. 2013  pp. 109–117.

[41]

[42] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge

University Press  2019.

[43] M. Wegkamp. “Model selection in nonparametric regression”. The Annals of Statistics 31.1

(2003)  pp. 252–273.

[44] N. Zhivotovskiy and S. Hanneke. “Localization of VC classes: Beyond local Rademacher

complexities”. Theoretical Computer Science 742 (2018)  pp. 27–49.

11

,Jun Yang
Shengyang Sun
Daniel Roy