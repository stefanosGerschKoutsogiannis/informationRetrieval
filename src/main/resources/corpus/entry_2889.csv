2019,Learning Data Manipulation for Augmentation and Weighting,Manipulating data  such as weighting data examples or augmenting with new instances  has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work  we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL)  and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the ``data reward'' function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network  and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.,Learning Data Manipulation for Augmentation and

Weighting

Zhiting Hu1 2∗  Bowen Tan1∗  Ruslan Salakhutdinov1  Tom Mitchell1  Eric P. Xing1 2

{zhitingh btan2 rsalakhu tom.mitchell}@cs.cmu.edu  eric.xing@petuum.com

1Carnegie Mellon University  2Petuum Inc.

Abstract

Manipulating data  such as weighting data examples or augmenting with new
instances  has been increasingly used to improve model training. Previous work
has studied various rule- or learning-based approaches designed for speciﬁc types
of data manipulation. In this work  we propose a new method that supports learning
different manipulation schemes with the same gradient-based algorithm. Our
approach builds upon a recent connection of supervised learning and reinforcement
learning (RL)  and adapts an off-the-shelf reward learning algorithm from RL for
joint data manipulation learning and model training. Different parameterization
of the “data reward” function instantiates different manipulation schemes. We
showcase data augmentation that learns a text transformation network  and data
weighting that dynamically adapts the data sample importance. Experiments show
the resulting algorithms signiﬁcantly improve the image and text classiﬁcation
performance in low data regime and class-imbalance problems.

1

Introduction

The performance of machines often crucially depend on the amount and quality of the data used for
training. It has become increasingly ubiquitous to manipulate data to improve learning  especially in
low data regime or in presence of low-quality datasets (e.g.  imbalanced labels). For example  data
augmentation applies label-preserving transformations on original data points to expand the data size;
data weighting assigns an importance weight to each instance to adapt its effect on learning; and data
synthesis generates entire artiﬁcial examples. Different types of manipulation can be suitable for
different application settings.
Common data manipulation methods are usually designed manually  e.g.  augmenting by ﬂipping
an image or replacing a word with synonyms  and weighting with inverse class frequency or loss
values [10  32]. Recent work has studied automated approaches  such as learning the composition of
augmentation operators with reinforcement learning [38  5]  deriving sample weights adaptively from
a validation set via meta learning [39]  or learning a weighting network by inducing a curriculum [21].
These learning-based approaches have alleviated the engineering burden and produced impressive
results. However  the algorithms are usually designed speciﬁcally for certain types of manipulation
(e.g.  either augmentation or weighting) and thus have limited application scope in practice.
In this work  we propose a new approach that enables learning for different manipulation schemes
with the same single algorithm. Our approach draws inspiration from the recent work [46] that
shows equivalence between the data in supervised learning and the reward function in reinforcement
learning. We thus adapt an off-the-shelf reward learning algorithm [52] to the supervised setting
for automated data manipulation. The marriage of the two paradigms results in a simple yet general
algorithm  where various manipulation schemes are reduced to different parameterization of the

∗equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

data reward. Free parameters of manipulation are learned jointly with the target model through
efﬁcient gradient descent on validation examples. We demonstrate instantiations of the approach for
automatically ﬁne-tuning an augmentation network and learning data weights  respectively.
We conduct extensive experiments on text and image classiﬁcation in challenging situations of very
limited data and imbalanced labels. Both augmentation and weighting by our approach signiﬁcantly
improve over strong base models  even though the models are initialized with large-scale pretrained
networks such as BERT [7] for text and ResNet [14] for images. Our approach  besides its generality 
also outperforms a variety of dedicated rule- and learning-based methods for either augmentation
or weighting  respectively. Lastly  we observe that the two types of manipulation tend to excel in
different contexts: augmentation shows superiority over weighting with a small amount of data
available  while weighting is better at addressing class imbalance problems.
The way we derive the manipulation algorithm represents a general means of problem solving through
algorithm extrapolation between learning paradigms  which we discuss more in section 6.

2 Related Work

Rich types of data manipulation have been increasingly used in modern machine learning pipelines.
Previous work each has typically focused on a particular manipulation type. Data augmentation
that perturbs examples without changing the labels is widely used especially in vision [44  26] and
speech [24  36] domains. Common heuristic-based methods on images include cropping  mirroring 
rotation [26]  and so forth. Recent work has developed automated augmentation approaches [5  38 
28  37  47]. Xie et al. [50] additionally use large-scale unlabeled data. Cubuk et al. [5]  Ratner
et al. [38] learn to induce the composition of data transformation operators. Instead of treating
data augmentation as a policy in reinforcement learning [5]  we formulate manipulation as a reward
function and use efﬁcient stochastic gradient descent to learn the manipulation parameters. Text
data augmentation has also achieved impressive success  such as contextual augmentation [25  49] 
back-translation [42]  and manual approaches [48  2]. In addition to perturbing the input text as in
classiﬁcation tasks  text generation problems expose opportunities to adding noise also in the output
text  such as [35  51]. Recent work [46] shows output nosing in sequence generation can be treated as
an intermediate approach in between supervised learning and reinforcement learning  and developed
a new sequence learning algorithm that interpolates between the spectrum of existing algorithms. We
instantiate our approach for text contextual augmentation as in [25  49]  but enhance the previous
work by additionally ﬁne-tuning the augmentation network jointly with the target model.
Data weighting has been used in various algorithms  such as AdaBoost [10]  self-paced learning [27] 
hard-example mining [43]  and others [4  22]. These algorithms largely deﬁne sample weights based
on training loss. Recent work [21  8] learns a separate network to predict sample weights. Of
particular relevance to our work is [39] which induces sample weights using a validation set. The data
weighting mechanism instantiated by our framework has a key difference in that samples weights are
treated as parameters that are updated iteratively  instead of re-estimated from scratch at each step.
We show improved performance of our approach. Besides  our data manipulation approach is derived
based on a different perspective of reward learning  instead of meta-learning as in [39].
Another popular type of data manipulation involves data synthesis  which creates entire artiﬁcial
samples from scratch. GAN-based approaches have achieved impressive results for synthesizing
conditional image data [3  34]. In the text domain  controllable text generation [17] presents a
way of co-training the data generator and classiﬁer in a cyclic manner within a joint VAE [23] and
wake-sleep [15] framework. It is interesting to explore the instantiation of the present approach for
adaptive data synthesis in the future.

3 Background

We ﬁrst present the relevant work upon which our automated data manipulation is built. This section
also establishes the notations used throughout the paper.
Let x denote the input and y the output. For example  in text classiﬁcation  x can be a sentence and y
is the sentence label. Denote the model of interest as pθ(y|x)  where θ is the model parameters to be

2

learned. In supervised setting  given a set of training examples D = {(x∗  y∗)}  we learn the model
by maximizing the data log-likelihood.

Equivalence between Data and Reward The recent work [46] introduced a unifying perspective
of reformulating maximum likelihood supervised learning as a special instance of a policy optimiza-
tion framework. In this perspective  data examples providing supervision signals are equivalent to a
specialized reward function. Since the original framework [46] was derived for sequence generation
problems  here we present a slightly adapted formulation for our context of data manipulation.
To connect the maximum likelihood supervised learning with policy optimization  consider the
model pθ(y|x) as a policy that takes “action” y given the “state” x. Let R(x  y|D) ∈ R denote
a reward function  and p(x) be the empirical data distribution which is known given D. Further
assume a variational distribution q(x  y) that factorizes as q(x  y) = p(x)q(y|x). A variational
policy optimization objective is then written as:

L(q  θ) = Eq(x y) [R(x  y|D)] − αKL(cid:0)q(x  y)(cid:107)p(x)pθ(y|x)(cid:1) + βH(q) 

(1)
where KL(·(cid:107)·) is the Kullback–Leibler divergence; H(·) is the Shannon entropy; and α  β > 0 are
balancing weights. The objective is in the same form with the RL-as-inference formalism of policy
optimization [e.g.  6  29  1]. Intuitively  the objective maximizes the expected reward under q  and
enforces the model pθ to stay close to q  with a maximum entropy regularization over q. The problem
is solved with an EM procedure that optimizes q and θ alternatingly:

(cid:26) α log p(x)pθ(y|x) + R(x  y|D)

(cid:27)

/ Z 

(2)

E-step:

q

(cid:48)

M-step: θ

(x  y) = exp
(cid:48)

= arg maxθ

(cid:2) log pθ(y|x)(cid:3) 

α + β

Eq(cid:48)(x y)

where Z is the normalization term. With the established framework  it is easy to show that the above
optimization procedure reduces to maximum likelihood learning by taking α → 0  β = 1  and the
reward function:

(cid:26) 1

Rδ(x  y|D) =

if (x  y) ∈ D

−∞ otherwise.

(3)

That is  a sample (x  y) receives a unit reward only when it matches a training example in the
dataset  while the reward is negative inﬁnite in all other cases. To make the equivalence to maximum
likelihood learning clearer  note that the above M-step now reduces to

(4)
where the joint distribution p(x) exp{Rδ}/Z equals the empirical data distribution  which means the
M-step is in fact maximizing the data log-likelihood of the model pθ.

= arg maxθ

θ

Ep(x) exp{Rδ}/Z

(cid:48)

(cid:2) log pθ(y|x)(cid:3) 

Gradient-based Reward Learning There is a rich line of research on learning the reward in
reinforcement learning. Of particular interest to this work is [52] which learns a parametric intrinsic
reward that additively transforms the original task reward (a.k.a extrinsic reward) to improve the
policy optimization. For consistency of notations with above  formally  let pθ(y|x) be a policy where
y is an action and x is a state. Let Rin
φ be the intrinsic reward with parameters φ. In each iteration 
the policy parameter θ is updated to maximize the joint rewards  through:

(cid:48)

= θ + γ∇θLex+in(θ  φ) 

θ

(5)
where Lex+in is the expectation of the sum of extrinsic and intrinsic rewards; and γ is the step size.
The equation shows θ(cid:48) depends on φ  thus we can write as θ(cid:48) = θ(cid:48)(φ).
The next step is to optimize the intrinsic reward parameters φ. Recall that the ultimate measure of
the performance of a policy is the value of extrinsic reward it achieves. Therefore  a good intrinsic
reward is supposed to  when the policy is trained with it  increase the eventual extrinsic reward. The
update to φ is then written as:

(6)
That is  we want the expected extrinsic reward Lex(θ(cid:48)) of the new policy θ(cid:48) to be maximized. Since
θ(cid:48) is a function of φ  we can directly backpropagate the gradient through θ(cid:48) to φ.

(φ)).

(cid:48)
φ

= φ + γ∇φLex(θ

(cid:48)

3

Algorithm 1 Joint Learning of Model and Data Manipulation
Input: The target model pθ(y|x)

The data manipulation function Rφ(x  y|D)
Training set D  validation set Dv

1: Initialize model parameter θ and manipulation parameter φ
2: repeat
3:

Optimize θ on D enriched with data manipulation
through Eq.(7)
Optimize φ by maximizing data log-likelihood on Dv
through Eq.(8)
5: until convergence
Output: Learned model pθ∗ (y|x) and manipulation Rφ∗ (y  x|D)

4:

Figure 1: Algorithm Computation.
Blue arrows denote learning model θ.
Red arrows denote learning manipula-
tion φ. Solid arrows denote forward
pass. Dashed arrows denote backward
pass and parameter updates.

4 Learning Data Manipulation

4.1 Method

Parameterizing Data Manipulation We now develop our approach of learning data manipulation 
through a novel marriage of supervised learning and the above reward learning. Speciﬁcally  from
the policy optimization perspective  due to the δ-function reward (Eq.3)  the standard maximum
likelihood learning is restricted to use only the exact training examples D in a uniform way. A natural
idea of enabling data manipulation is to relax the strong restrictions of the δ-function reward and
instead use a relaxed reward Rφ(x  y|D) with parameters φ. The relaxed reward can be parameterized
in various ways  resulting in different types of manipulation. For example  when a sample (x  y)
matches a data instance  instead of returning constant 1 by Rδ  the new Rφ can return varying reward
values depending on the matched instance  resulting in a data weighting scheme. Alternatively  Rφ
can return a valid reward even when x matches a data example only in part  or (x  y) is an entire new
sample not in D  which in effect makes data augmentation and data synthesis  respectively  in which
cases φ is either a data transformer or a generator. In the next section  we demonstrate two particular
parameterizations for data augmentation and weighting  respectively.
We thus have shown that the diverse types of manipulation all boil down to a parameterized data
reward Rφ. Such an concise  uniform formulation of data manipulation has the advantage that  once
we devise a method of learning the manipulation parameters φ  the resulting algorithm can directly
be applied to automate any manipulation type. We present a learning algorithm next.

Learning Manipulation Parameters To learn the parameters φ in the manipulation reward
Rφ(x  y|D)  we could in principle adopt any off-the-shelf reward learning algorithm in the lit-
erature. In this work  we draw inspiration from the above gradient-based reward learning (section 3)
due to its simplicity and efﬁciency. Brieﬂy  the objective of φ is to maximize the ultimate measure
of the performance of model pθ(y|x)  which  in the context of supervised learning  is the model
performance on a held-out validation set.
The algorithm optimizes θ and φ alternatingly  corresponding to Eq.(5) and Eq.(6)  respectively.
More concretely  in each iteration  we ﬁrst update the model parameters θ in analogue to Eq.(5)
which optimizes intrinsic reward-enriched objective. Here  we optimize the log-likelihood of the
training set enriched with data manipulation. That is  we replace Rδ with Rφ in Eq.(4)  and obtain
the augmented M-step:

= arg maxθ

(7)
By noticing that the new θ(cid:48) depends on φ  we can write θ(cid:48) as a function of φ  namely  θ(cid:48) = θ(cid:48)(φ). The
practical implementation of the above update depends on the actual parameterization of manipulation
Rφ  which we discuss in more details in the next section.

Ep(x) exp{Rφ(x y|D)}/Z

(cid:48)

θ

(cid:2) log pθ(y|x)(cid:3).

4

Train Data!ValData!"ℒ(% ')%′(')Model%ℒ(%′('))''′ManipulationThe next step is to optimize φ in terms of the model validation performance  in analogue to Eq.(6).
Formally  let Dv be the validation set of data examples. The update is then:

(cid:48)
φ

= arg maxφ
= arg maxφ

Ep(x) exp{Rδ (x y|Dv )}/Z
E(x y)∼Dv

(cid:2) log pθ(cid:48) (y|x)(cid:3) 

(cid:2) log pθ(cid:48) (y|x)(cid:3)

(8)

where  since θ(cid:48) is a function of φ  the gradient is backpropagated to φ through θ(cid:48)(φ). Taking data
weighting for example where φ is the training sample weights (more details in section 4.2)  the update
is to optimize the weights of training samples so that the model performs best on the validation set.
The resulting algorithm is summarized in Algorithm 1. Figure 1 illustrates the computation ﬂow.
Learning the manipulation parameters effectively uses a held-out validation set. We show in our
experiments that a very small set of validation examples (e.g.  2 labels per class) is enough to
signiﬁcantly improve the model performance in low data regime.
It is worth noting that some previous work has also leveraged validation examples  such as learning
data augmentation with policy gradient [5] or inducing data weights with meta-learning [39]. Our
approach is inspired from a distinct paradigm of (intrinsic) reward learning. In contrast to [5] that
treats data augmentation as a policy  we instead formulate manipulation as a reward function and
enable efﬁcient stochastic gradient updates. Our approach is also more broadly applicable to diverse
data manipulation types than [39  5].

4.2

Instantiations: Augmentation & Weighting

As a case study  we show two parameterizations of Rφ which instantiate distinct data manipulation
schemes. The ﬁrst example learns augmentation for text data  a domain that has been less studied in
the literature compared to vision and speech [25  12]. The second instance focuses on automated data
weighting  which is applicable to any data domains.

Fine-tuning Text Augmentation
The recent work [25  49] developed a novel contextual augmentation approach for text data  in which
a powerful pretrained language model (LM)  such as BERT [7]  is used to generate substitutions
of words in a sentence. Speciﬁcally  given an observed sentence x∗  the method ﬁrst randomly
masks out a few words. The masked sentence is then fed to BERT which ﬁlls the masked positions
with new words. To preserve the original sentence class  the BERT LM is retroﬁtted as a label-
conditional model  and trained on the task training examples. The resulting model is then ﬁxed and
used to augment data during the training of target model. We denote the augmentation distribution as
gφ0 (x|x∗  y∗)  where φ0 is the ﬁxed BERT LM parameters.
The above process has two drawbacks. First  the LM is ﬁxed after ﬁtting to the task data. In the
subsequent phase of training the target model  the LM augments data without knowing the state of
the target model  which can lead to sub-optimal results. Second  in the cases where the task dataset
is small  the LM can be insufﬁciently trained for preserving the labels faithfully  resulting in noisy
augmented samples.
To address the difﬁculties  it is beneﬁcial to apply the proposed learning data manipulation algorithm
to additionally ﬁne-tune the LM jointly with target model training. As discussed in section 4  this
reduces to properly parameterizing the data reward function:

(cid:26) 1

φ (x  y|D) =
Raug

−∞ otherwise.

if x ∼ gφ(x|x∗  y)  (x∗  y) ∈ D

That is  a sample (x  y) receives a unit reward when y is the true label and x is the augmented sample
by the LM (instead of the exact original data x∗). Plugging the reward into Eq.(7)  we obtain the
data-augmented update for the model parameters:

Ex∼gφ(x|x∗ y)  (x∗ y)∼D(cid:2) log pθ(y|x)(cid:3).

(cid:48)

θ

= arg maxθ

That is  we pick an example from the training set  and use the LM to create augmented samples 
which are then used to update the target model. Regarding the update of augmentation parameters φ
(Eq.8)  since text samples are discrete  to enable efﬁcient gradient propagation through θ(cid:48) to φ  we
use a gumbel-softmax approximation [20] to x when sampling substitution words from the LM.

5

(9)

(10)

Learning Data Weights
We now demonstrate the instantiation of data weighting. We aim to assign an importance weight to
each training example to adapt its effect on model training. We automate the process by learning the
data weights. This is achieved by parameterizing Rφ as:

(cid:26) φi

φ (x  y|D) =
Rw

−∞ otherwise 

if (x  y) = (x∗

i   y∗

i )  (x∗

i   y∗

i ) ∈ D

(11)

(12)

where φi ∈ R is the weight associated with the ith example. Plugging Rw
weighted update for the model θ:

φ into Eq.(7)  we obtain the

(cid:48)

θ

= arg maxθ
= arg maxθ

E(x∗
E(x∗

i  y∗
i  y∗

(cid:2) log pθ(y
i )∼D(cid:2)softmax(φi) log pθ(y

i )∈D  i∼softmax(φi)

i )(cid:3)
i )(cid:3)

i |x
∗
∗
i |x
∗
∗

In practice  when minibatch stochastic optimization is used  we approximate the weighted sampling
by taking the softmax over the weights of only the minibatch examples. The data weights φ are
updated with Eq.(8). It is worth noting that the previous work [39] similarly derives data weights
based on their gradient directions on a validation set. Our algorithm differs in that the data weights are
parameters maintained and updated throughout the training  instead of re-estimated from scratch in
each iteration. Experiments show the parametric treatment achieves superior performance in various
settings. There are alternative parameterizations of Rφ other than Eq.(11). For example  replacing φi
in Eq.(11) with log φi in effect changes the softmax normalization in Eq.(12) to linear normalization 
which is used in [39].

5 Experiments

We empirically validate the proposed data manipulation approach through extensive experiments on
learning augmentation and weighting. We study both text and image classiﬁcation  in two difﬁcult
settings of low data regime and imbalanced labels1.

5.1 Experimental Setup

Base Models. We choose strong pretrained networks as our base models for both text and image
classiﬁcation. Speciﬁcally  on text data  we use the BERT (base  uncased) model [7]; while on
image data  we use ResNet-34 [14] pretrained on ImageNet. We show that  even with the large-
scale pretraining  data manipulation can still be very helpful to boost the model performance on
downstream tasks. Since our approach uses validation sets for manipulation parameter learning  for a
fair comparison with the base model  we train the base model in two ways. The ﬁrst is to train the
model on the training sets as usual and select the best step using the validation sets; the second is to
train on the merged training and validation sets for a ﬁxed number of steps. The step number is set to
the average number of steps selected in the ﬁrst method. We report the results of both methods.
Comparison Methods. We compare our approach with a variety of previous methods that were
designed for speciﬁc manipulation schemes: (1) For text data augmentation  we compare with the
latest model-based augmentation [49] which uses a ﬁxed conditional BERT language model for
word substitution (section 4.2). As with base models  we also tried ﬁtting the augmentatin model to
both the training data and the joint training-validation data  and did not observe signiﬁcant difference.
Following [49]  we also study a conventional approach that replaces words with their synonyms
using WordNet [33]. (2) For data weighting  we compare with the state-of-the-art approach [39]
that dynamically re-estimates sample weights in each iteration based on the validation set gradient
directions. We follow [39] and also evaluate the commonly-used proportion method that weights
data by inverse class frequency.
Training.
For both the BERT classiﬁer and the augmentation model (which is also based on
BERT)  we use Adam optimization with an initial learning rate of 4e-5. For ResNets  we use SGD
optimization with a learning rate of 1e-3. For text data augmentation  we augment each minibatch
by generating two or three samples for each data points (each with 1  2 or 3 substitutions)  and use
both the samples and the original data to train the model. For data weighting  to avoid exploding
value  we update the weight of each data point in a minibatch by decaying the previous weight value

1Code available at https://github.com/tanyuqian/learning-data-manipulation

6

Model
Base model: BERT [7]
Base model + val-data

Augment

Weight

Fixed augmentation [49]
Ours: Fine-tuned augmentation
Ren et al. [39]
Ours

SST-5 (40+2)
33.32 ± 4.04
35.86 ± 3.03
Synonym 32.45 ± 4.59
34.84 ± 2.76
37.03 ± 2.05
36.09 ± 2.26
36.51 ± 2.54

IMDB (40+5)
63.55 ± 5.35
63.65 ± 3.32
62.68 ± 3.94
63.65 ± 3.21
65.62 ± 3.32
63.01 ± 3.33
64.78 ± 2.72

TREC (40+5)
88.25 ± 2.81
88.42 ± 4.90
88.26 ± 2.76
88.28 ± 4.50
89.15 ± 2.41
88.60 ± 2.85
89.01 ± 2.39

Table 1: Accuracy of Data Manipulation on Text Classiﬁcation. All results are averaged over 15 runs
± one standard deviation. The numbers in parentheses next to the dataset names indicate the size of
the datasets. For example  (40+2) denotes 40 training instances and 2 validation instances per class.

Pretrained
37.69 ± 3.03
38.09 ± 1.87
38.02 ± 2.14
38.95 ± 2.03

Model
Base model: ResNet-34
Base model + val-data
Ren et al. [39]
Ours

Not-Pretrained
22.98 ± 2.81
23.42 ± 1.47
23.44 ± 1.63
24.92 ± 1.57
Table 2: Accuracy of Data Weighting on Image Clas-
siﬁcation. The small subset of CIFAR10 used here has
40 training instances and 2 validation instances for each
class. The “pretrained” column is the results by initial-
izing the ResNet-34 [14] base model with ImageNet-
pretrained weights. In contrast  “Not-Pretrained” de-
notes the base model is randomly initialized. Since
every class has the same number of examples  the
proportion-based weighting degenerates to base model
training and thus is omitted here.

Figure 2: Words predicted with the high-
est probabilities by the augmentation LM.
Two tokens “striking” and “grey” are
masked for substitution. The boxes in re-
spective colors list the predicted words af-
ter training epoch 1 and 3  respectively.
E.g.  “stunning” is the most probable sub-
stitution for “striking” in epoch 1.

with a factor of 0.1 and then adding the gradient. All experiments were implemented with PyTorch
(pytorch.org) and were performed on a Linux machine with 4 GTX 1080Ti GPUs and 64GB RAM.
All reported results are averaged over 15 runs ± one standard deviation.

5.2 Low Data Regime

We study the problem where only very few labeled examples for each class are available. Both
of our augmentation and weighting boost base model performance  and are superior to respective
comparison methods. We also observe that augmentation performs better than weighting in the
low-data setting.

Setup For text classiﬁcation  we use the popular benchmark datasets  including SST-5 for 5-class
sentence sentiment [45]  IMDB for binary movie review sentiment [31]  and TREC for 6-class
question types [30]. We subsample a small training set on each task by randomly picking 40 instances
for each class. We further create small validation sets  i.e.  2 instances per class for SST-5  and 5
instances per class for IMDB and TREC  respectively. The reason we use slightly more validation
examples on IMDB and TREC is that the model can easily achieve 100% validation accuracy if the
validation sets are too small. Thus  the SST-5 task has 210 labeled examples in total  while IMDB has
90 labels and TREC has 270. Such extremely small datasets pose signiﬁcant challenges for learning
deep neural networks. Since the manipulation parameters are trained using the small validation sets 
to avoid possible overﬁtting we restrict the training to small number (e.g.  5 or 10) of epochs. For
image classiﬁcation  we similarly create a small subset of the CIFAR10 data  which includes 40
instances per class for training  and 2 instances per class for validation.

7

(cid:7)(cid:5)(cid:8)(cid:7)(cid:2)(cid:1)Although visually strikingand slickly s sstaged  it’s also cold  grey  antiseptic a a and emotionally desiccated.(cid:6)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:1)negative stunningblandfantasticdazzlinglivelysharpcharmingheroismdemandingrevealingtaboodarknegativemisleadingmessybittergoofyslowtrivialdry(cid:7)(cid:13)(cid:12)(cid:6)(cid:9)(cid:1)(cid:2)(cid:7)(cid:13)(cid:12)(cid:6)(cid:9)(cid:1)(cid:3)(cid:7)(cid:13)(cid:12)(cid:6)(cid:9)(cid:1)(cid:2)(cid:7)(cid:13)(cid:12)(cid:6)(cid:9)(cid:1)(cid:3)(cid:9)(cid:10)(cid:8)(cid:9)(cid:7)(cid:14)(cid:13)(cid:14)(cid:12)(cid:5)(cid:4)(cid:5)(cid:10)(cid:11)(cid:10)(cid:15)(cid:16)Model
Base model: BERT [7]
Base model + val-data
Proportion
Ren et al. [39]
Ours

20 : 1000
54.91 ± 5.98
52.58 ± 4.58
57.42 ± 7.91
74.61 ± 3.54
75.08 ± 4.98

50 : 1000
67.73 ± 9.20
55.90 ± 4.18
71.14 ± 6.71
76.89 ± 5.07
79.35 ± 2.59

100 : 1000
75.04 ± 4.51
68.21 ± 5.28
76.14 ± 5.8
80.73 ± 2.19
81.82 ± 1.88

Table 3: Accuracy of Data Weighting on Imbalanced SST-2. The ﬁrst row shows the number of
training examples in each of the two classes.

Model
Base model: ResNet [14]
Base model + val-data
Proportion
Ren et al. [39]
Ours

20 : 1000
72.20 ± 4.70
64.66 ± 4.81
72.29 ± 5.67
74.35 ± 6.37
75.32 ± 6.36

50 : 1000
81.65 ± 2.93
69.51 ± 2.90
81.49 ± 3.83
82.25 ± 2.08
83.11 ± 2.08

100 : 1000
86.42 ± 3.15
79.38 ± 2.92
84.26 ± 4.58
86.54 ± 2.69
86.99 ± 3.47

Table 4: Accuracy of Data Weighting on Imbalanced CIFAR10. The ﬁrst row shows the number of
training examples in each of the two classes.

Results Table 1 shows the manipulation results on text classiﬁcation. For data augmentation  our
approach signiﬁcantly improves over the base model on all the three datasets. Besides  compared to
both the conventional synonym substitution and the approach that keeps the augmentation network
ﬁxed  our adaptive method that ﬁne-tunes the augmentation network jointly with model training
achieves superior results. Indeed  the heuristic-based synonym approach can sometimes harm the
model performance (e.g.  SST-5 and IMDB)  as also observed in previous work [49  25]. This
can be because the heuristic rules do not ﬁt the task or datasets well. In contrast  learning-based
augmentation has the advantage of adaptively generating useful samples to improve model training.
Table 1 also shows the data weighting results. Our weight learning consistently improves over the base
model and the latest weighting method [39]. In particular  instead of re-estimating sample weights
from scratch in each iteration [39]  our approach treats the weights as manipulation parameters
maintained throughout the training. We speculate that the parametric treatment can adapt weights
more smoothly and provide historical information  which is beneﬁcial in the small-data context.
It is interesting to see from Table 1 that our augmentation method consistently outperforms the
weighting method  showing that data augmentation can be a more suitable technique than data
weighting for manipulating small-size data. Our approach provides the generality to instantiate
diverse manipulation types and learn with the same single procedure.
To investigate the augmentation model and how the ﬁne-tuning affects the augmentation results  we
show in Figure 2 the top-5 most probable word substitutions predicted by the augmentation model
for two masked tokens  respectively. Comparing the results of epoch 1 and epoch 3  we can see
the augmentation model evolves and dynamically adjusts the augmentation behavior as the training
proceeds. Through ﬁne-tuning  the model seems to make substitutions that are more coherent with
the conditioning label and relevant to the original words (e.g.  replacing the word “striking” with
“bland” in epoch 1 v.s. “charming” in epoch 3).
Table 2 shows the data weighting results on image classiﬁcation. We evaluate two settings with the
ResNet-34 base model being initialized randomly or with pretrained weights  respectively. Our data
weighting consistently improves over the base model and [39] regardless of the initialization.

5.3

Imbalanced Labels

We next study a different problem setting where the training data of different classes are imbalanced.
We show the data weighting approach greatly improves the classiﬁcation performance. It is also
observed that  the LM data augmentation approach  which performs well in the low-data setting  fails
on the class-imbalance problems.

8

Setup Though the methods are broadly applicable to multi-way classiﬁcation problems  here
we only study binary classiﬁcation tasks for simplicity. For text classiﬁcation  we use the SST-2
sentiment analysis benchmark [45]; while for image  we select class 1 and 2 from CIFAR10 for binary
classiﬁcation. We use the same processing on both datasets to build the class-imbalance setting.
Speciﬁcally  we randomly select 1 000 training instances of class 2  and vary the number of class-1
instances in {20  50  100}. For each dataset  we use 10 validation examples in each class. Trained
models are evaluated on the full binary-class test set.

Results Table 3 shows the classiﬁcation results on SST-2 with varying imbalance ratios. We can
see our data weighting performs best across all settings. In particular  the improvement over the
base model increases as the data gets more imbalanced  ranging from around 6 accuracy points on
100:1000 to over 20 accuracy points on 20:1000. Our method is again consistently better than [39] 
validating that the parametric treatment is beneﬁcial. The proportion-based data weighting provides
only limited improvement  showing the advantage of adaptive data weighting. The base model trained
on the joint training-validation data for ﬁxed steps fails to perform well  partly due to the lack of a
proper mechanism for selecting steps.
Table 4 shows the results on imbalanced CIFAR10 classiﬁcation. Similarly  our method outperforms
other comparison approaches. In contrast  the ﬁxed proportion-based method sometimes harms the
performance as in the 50:1000 and 100:1000 settings.
We also tested the text augmentation LM on the SST-2 imbalanced data. Interestingly  the augmen-
tation tends to hinder model training and yields accuracy of around 50% (random guess). This is
because the augmentation LM is ﬁrst ﬁt to the imbalanced data  which makes label preservation
inaccurate and introduces lots of noise during augmentation. Though a more carefully designed
augmentation mechanism can potentially help with imbalanced classiﬁcation (e.g.  augmenting only
the rare classes)  the above observation further shows that the varying data manipulation schemes
have different applicable scopes. Our approach is thus favorable as the single algorithm can be
instantiated to learn different schemes.

6 Discussions: Algorithm Extrapolation between Learning Paradigms

Conclusions. We have developed a new method of learning different data manipulation schemes with
the same single algorithm. Different manipulation schemes reduce to just different parameterization
of the data reward function. The manipulation parameters are trained jointly with the target model
parameters. We instantiate the algorithm for data augmentation and weighting  and show improved
performance over strong base models and previous manipulation methods. We are excited to explore
more types of manipulations such as data synthesis  and in particular study the combination of
different manipulation schemes.
The proposed method builds upon the connections between supervised learning and reinforcement
learning (RL) [46] through which we extrapolate an off-the-shelf reward learning algorithm in the RL
literature to the supervised setting. The way we obtained the manipulation algorithm represents a
general means of innovating problem solutions based on unifying formalisms of different learning
paradigms. Speciﬁcally  a unifying formalism not only offers new understandings of the seemingly
distinct paradigms  but also allows us to systematically apply solutions to problems in one paradigm
to similar problems in another. Previous work along this line has made fruitful results in other
domains. For example  an extended formulation of [46] that connects RL and posterior regularization
(PR) [11  16] has enabled to similarly export a reward learning algorithm to the context of PR
for learning structured knowledge [18]. By establishing a uniform abstration of GANs [13] and
VAEs [23]  Hu et al. [19] exchange techniques between the two families and get improved generative
modeling. Other work in the similar spirit includes [40  41  9  etc].
By extrapolating algorithms between paradigms  one can go beyond crafting new algorithms from
scratch as in most existing studies  which often requires deep expertise and yields unique solutions in
a dedicated context. Instead  innovation becomes easier by importing rich ideas from other paradigms 
and is repeatable as a new algorithm can be methodically extrapolated to multiple different contexts.

9

References
[1] A. Abdolmaleki  J. T. Springenberg  Y. Tassa  R. Munos  N. Heess  and M. Riedmiller. Maximum a

posteriori policy optimisation. In ICLR  2018.

[2] J. Andreas  M. Rohrbach  T. Darrell  and D. Klein. Learning to compose neural networks for question

answering. arXiv preprint arXiv:1601.01705  2016.

[3] S. Baluja and I. Fischer. Adversarial transformation networks: Learning to generate adversarial examples.

arXiv preprint arXiv:1703.09387  2017.

[4] H.-S. Chang  E. Learned-Miller  and A. McCallum. Active bias: Training more accurate neural networks

by emphasizing high variance samples. In NeurIPS  pages 1002–1012  2017.

[5] E. D. Cubuk  B. Zoph  D. Mane  V. Vasudevan  and Q. V. Le. Autoaugment: Learning augmentation

policies from data. In CVPR  2019.

[6] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural Computa-

tion  9(2):271–278  1997.

[7] J. Devlin  M.-W. Chang  K. Lee  and K. Toutanova. BERT: Pre-training of deep bidirectional transformers

for language understanding. In NAACL  2019.

[8] Y. Fan  F. Tian  T. Qin  X.-Y. Li  and T.-Y. Liu. Learning to teach. In ICLR  2018.

[9] C. Finn  P. Christiano  P. Abbeel  and S. Levine. A connection between generative adversarial networks 

inverse reinforcement learning  and energy-based models. arXiv preprint arXiv:1611.03852  2016.

[10] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to

boosting. Journal of computer and system sciences  55(1):119–139  1997.

[11] K. Ganchev  J. Gillenwater  B. Taskar  et al. Posterior regularization for structured latent variable models.

Journal of Machine Learning Research  11(Jul):2001–2049  2010.

[12] P. K. B. Giridhara  M. Chinmaya  R. K. M. Venkataramana  S. S. Bukhari  and A. Dengel. A study of

various text augmentation techniques for relation classiﬁcation in free text. In ICPRAM  2019.

[13] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In NIPS  pages 2672–2680  2014.

[14] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778  2016.

[15] G. E. Hinton  P. Dayan  B. J. Frey  and R. M. Neal. The" wake-sleep" algorithm for unsupervised neural

networks. Science  268(5214):1158  1995.

[16] Z. Hu  X. Ma  Z. Liu  E. Hovy  and E. Xing. Harnessing deep neural networks with logic rules. In ACL 

2016.

[17] Z. Hu  Z. Yang  X. Liang  R. Salakhutdinov  and E. P. Xing. Toward controlled generation of text. In

ICML  2017.

[18] Z. Hu  Z. Yang  R. Salakhutdinov  X. Liang  L. Qin  H. Dong  and E. Xing. Deep generative models with

learnable knowledge constraints. In NIPS  2018.

[19] Z. Hu  Z. Yang  R. Salakhutdinov  and E. P. Xing. On unifying deep generative models. In ICLR  2018.

[20] E. Jang  S. Gu  and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint

arXiv:1611.01144  2016.

[21] L. Jiang  Z. Zhou  T. Leung  L.-J. Li  and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very

deep neural networks on corrupted labels. In ICML  2018.

[22] A. Katharopoulos and F. Fleuret. Not all samples are created equal: Deep learning with importance

sampling. In ICML  2018.

[23] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114  2013.

[24] T. Ko  V. Peddinti  D. Povey  and S. Khudanpur. Audio augmentation for speech recognition.

INTERSPEECH  2015.

In

10

[25] S. Kobayashi. Contextual augmentation: Data augmentation by words with paradigmatic relations. In

NAACL  2018.

[26] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in neural information processing systems  pages 1097–1105  2012.

[27] M. P. Kumar  B. Packer  and D. Koller. Self-paced learning for latent variable models. In NeurIPS  pages

1189–1197  2010.

[28] J. Lemley  S. Bazrafkan  and P. Corcoran. Smart augmentation learning an optimal data augmentation

strategy. IEEE Access  5:5858–5869  2017.

[29] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv

preprint arXiv:1805.00909  2018.

[30] X. Li and D. Roth. Learning question classiﬁers. In Proceedings of the 19th international conference on

Computational linguistics  pages 1–7  2002.

[31] A. L. Maas  R. E. Daly  P. T. Pham  D. Huang  A. Y. Ng  and C. Potts. Learning word vectors for sentiment
analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics:
Human language technologies-volume 1  pages 142–150. Association for Computational Linguistics  2011.

[32] T. Malisiewicz  A. Gupta  A. A. Efros  et al. Ensemble of exemplar-SVMs for object detection and beyond.

In ICCV  volume 1  page 6. Citeseer  2011.

[33] G. A. Miller. WordNet: a lexical database for English. Communications of the ACM  38(11):39–41  1995.

[34] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784  2014.

[35] M. Norouzi  S. Bengio  N. Jaitly  M. Schuster  Y. Wu  D. Schuurmans  et al. Reward augmented maximum
likelihood for neural structured prediction. In Advances In Neural Information Processing Systems  pages
1723–1731  2016.

[36] D. S. Park  W. Chan  Y. Zhang  C.-C. Chiu  B. Zoph  E. D. Cubuk  and Q. V. Le. Specaugment: A simple

data augmentation method for automatic speech recognition. arXiv preprint arXiv 1904.08779.

[37] X. Peng  Z. Tang  F. Yang  R. S. Feris  and D. Metaxas. Jointly optimize data augmentation and network

training: Adversarial data augmentation in human pose estimation. In CVPR  2018.

[38] A. J. Ratner  H. Ehrenberg  Z. Hussain  J. Dunnmon  and C. Ré. Learning to compose domain-speciﬁc

transformations for data augmentation. In NeurIPS  2017.

[39] M. Ren  W. Zeng  B. Yang  and R. Urtasun. Learning to reweight examples for robust deep learning. In

ICML  2018.

[40] S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural computation  1999.

[41] R. Samdani  M.-W. Chang  and D. Roth. Uniﬁed expectation maximization. In ACL  2012.

[42] R. Sennrich  B. Haddow  and A. Birch. Improving neural machine translation models with monolingual

data. In ACL  2016.

[43] A. Shrivastava  A. Gupta  and R. Girshick. Training region-based object detectors with online hard example

mining. In CVPR  pages 761–769  2016.

[44] P. Y. Simard  Y. A. LeCun  J. S. Denker  and B. Victorri. Transformation invariance in pattern recogni-
tion—tangent distance and tangent propagation. In Neural networks: tricks of the trade  pages 239–274.
Springer  1998.

[45] R. Socher  A. Perelygin  J. Wu  J. Chuang  C. D. Manning  A. Ng  and C. Potts. Recursive deep models for

semantic compositionality over a sentiment treebank. In EMNLP  pages 1631–1642  2013.

[46] B. Tan  Z. Hu  Z. Yang  R. Salakhutdinov  and E. Xing. Connecting the dots between MLE and RL for

sequence generation. arXiv preprint arXiv:1811.09740  2018.

[47] T. Tran  T. Pham  G. Carneiro  L. Palmer  and I. Reid. A Bayesian data augmentation approach for learning

deep models. In NeurIPS  pages 2797–2806  2017.

[48] J. W. Wei and K. Zou. EDA: Easy data augmentation techniques for boosting performance on text

classiﬁcation tasks. arXiv preprint arXiv:1901.11196  2019.

11

[49] X. Wu  S. Lv  L. Zang  J. Han  and S. Hu. Conditional BERT contextual augmentation. arXiv preprint

arXiv:1812.06705  2018.

[50] Q. Xie  Z. Dai  E. Hovy  M.-T. Luong  and L. Q. V. Unsupervised data augmentation. arXiv preprint arXiv

1904.12848  2019.

[51] Z. Xie  S. I. Wang  J. Li  D. Lévy  A. Nie  D. Jurafsky  and A. Y. Ng. Data noising as smoothing in neural

network language models. In ICLR  2017.

[52] Z. Zheng  J. Oh  and S. Singh. On learning intrinsic rewards for policy gradient methods. In NeurIPS 

2018.

12

,Zhiting Hu
Bowen Tan
Russ Salakhutdinov
Tom Mitchell
Eric Xing