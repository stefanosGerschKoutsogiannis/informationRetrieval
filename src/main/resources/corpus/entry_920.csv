2018,Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models,The inference of the causal relationship between a pair of observed variables is a fundamental problem in science  and most existing approaches are based on one single causal model. In practice  however  observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors  which renders causal analysis results obtained by a single model skeptical. In this paper  we generalize the Additive Noise Model (ANM) to a mixture model  which consists of a finite number of ANMs  and provide the condition of its causal identifiability. To conduct model estimation  we propose Gaussian Process Partially Observable Model (GPPOM)  and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach.,Causal Inference and Mechanism Clustering of A

Mixture of Additive Noise Models

Shoubo Hu∗  Zhitang Chen†  Vahid Partovi Nia†  Laiwan Chan∗  Yanhui Geng‡

∗The Chinese University of Hong Kong; †Huawei Noah’s Ark Lab;

‡Huawei Montréal Research Center
∗{sbhu  lwchan}@cse.cuhk.edu.hk

†‡{chenzhitang2  vahid.partovinia  geng.yanhui}@huawei.com

Abstract

The inference of the causal relationship between a pair of observed variables is a
fundamental problem in science  and most existing approaches are based on one
single causal model. In practice  however  observations are often collected from
multiple sources with heterogeneous causal models due to certain uncontrollable
factors  which renders causal analysis results obtained by a single model skeptical.
In this paper  we generalize the Additive Noise Model (ANM) to a mixture model 
which consists of a ﬁnite number of ANMs  and provide the condition of its causal
identiﬁability. To conduct model estimation  we propose Gaussian Process Partially
Observable Model (GPPOM)  and incorporate independence enforcement into it
to learn latent parameter associated with each observation. Causal inference and
clustering according to the underlying generating mechanisms of the mixture model
are addressed in this work. Experiments on synthetic and real data demonstrate the
effectiveness of our proposed approach.

1

Introduction

Understanding the data-generating mechanism (g.m.) has been a main theme of causal inference. To
infer the causal direction between two random variables (r.v.s) X and Y using passive observations 
most existing approaches ﬁrst model the relation between them using a functional model with
certain assumptions [18  6  21  8]. Then a certain asymmetric property (usually termed cause-effect
asymmetry)  which only holds in the causal direction  is derived to conduct inference. For example 
the additive noise model (ANM) [6] represents the effect as a function of the cause with an additive
independent noise: Y = f (X) + . It is shown in [6] that there is no model of the form X = g(Y ) + ˜
that admits an ANM in the anticausal direction for most combinations (f  p(X)  p()).
Similar to ANM  most causal inference approaches based on functional models  such as LiNGAM
[18]  PNL [21]  and IGCI [9]  assume a single causal model for all observations. However  there is
no such a guarantee in practice  and it could be very common that the observations are generated by a
mixture of causal models due to different data sources or data collection under different conditions 
rendering existing single-causal-model based approaches inapplicable in many problems (e.g. Fig.
1). Recently  an approach was proposed for inferring the causal direction of mixtures of ANMs with
discrete variables [12]. However  the inference of such mixture models with continuous variables
remains a challenging problem and is not yet well studied.
Another question regarding mixture models addressed in this paper is how one could reveal causal
knowledge in clustering tasks. Speciﬁcally  we aim at ﬁnding clusters consistent with the causal g.m.s
of a mixture model  which is usually vital in the preliminary phase of many research. For example in
the analysis of air data (see section 4.2 for detail)  discovering knowledge from air data combined
from several different regions (i.e. mechanisms in causal perspective) is much more difﬁcult than

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a)

(b)

(c)

Figure 1: Example illustrating the failure of ANM on the inference of a mixture of ANMs (a)
the distribution of data generated from M1 : Y = X 2 +  (red) and M2 : Y = X 5 +  (blue) 
where X ∼ U (0  1) (x-axis) and  ∼ U (−0.1  0.1) ; (b) Conditional p(Y |X = 0.2); (c) Conditional
p(Y |X = 0.6). It is obvious that when the data is generated from a mixture of ANMs  the consistency
of conditionals is likely to be violated which leads to the failure of ANM.

from data of each region separately. Most existing clustering algorithms are weak for this perspective
as they typically deﬁne similarity between observations in the form of distances in some spaces or
manifolds. Most of them neglect the relation among r.v.s within a feature vector (observation)  and
only use those feature dimensions to calculate an overall distance metric as the clustering criterion.
In this paper  we focus on analyzing observations generated by a mixture of ANMs of two r.v.s and
try to answer two questions: 1) causal inference: how can we infer the causal direction between the
two r.v.s? 2) mechanism clustering: how can we cluster the observations generated from the same
g.m. together? To answer these questions  ﬁrst as the main result of this paper  we show that the
causal direction of the mixture of ANMs is identiﬁable in most cases  and we propose a variant of
GP-LVM [10] named Gaussian Process Partially Observable Model (GPPOM) for model estimation 
based on which we further develop the algorithms for causal inference and mechanism clustering.
The rest of the paper is organized as follows:
in section 2  we formalize the model  show its
identiﬁability and elaborate mechanism clustering; in section 3  model estimation method is proposed;
we present experiments on synthetic and real world data in section 4 and conclude in section 5.

2 ANM Mixture Model

2.1 Model deﬁnition

Each observation is assumed to be generated from an
ANM and the entire data set is generated by a ﬁnite num-
ber of related ANMs. They are called the ANM Mixture
Model (ANM-MM) and formally deﬁned as:
Deﬁnition 1 (ANM Mixture Model). An ANM Mixture
Model is a set of causal models of the same causal direc-
tion between two continuous r.v.s X and Y . All causal
models share the same form given by the following ANM:

Y = f (X; θ) +  

(1)

X

fn

yn

θ

n

N

β

Figure 2: ANM Mixture Model

(cid:80)C
c=1 ac1θc (·)  where ac > 0 (cid:80)C

where X denotes the cause  Y denotes the effect  f is a
nonlinear function parameterized by θ and the noise  ⊥⊥ X. The differences between causal
models in an ANM-MM stem only from different values of function parameter θ. In ANM-MM  θ is
assumed to be drawn from a discrete distribution on a ﬁnite set Θ = {θ1 ···   θC}  i.e. θ ∼ pθ(θ) =
c=1 ac = 1 and 1θc (·) is the indicator function of a single value θc.
Obviously in ANM-MM  all observations are generated by a set of g.m.s  which share the same
function form (f) but differ in parameter values (θ). This model is inspired by commonly encountered
cases where the data-generating process is slightly different in each independent trial due to the
inﬂuence of certain external factors that one can hardly control. In addition  these factors are usually

2

believed to be independent of the observed variables. The data-generating process of ANM-MM can
be represented by a directed graph in Fig. 2.

2.2 Causal inference: identiﬁability of ANM-MM
Let X be the cause and Y be the effect (X → Y ) without loss of generality. As most recently
proposed causal inference approaches  following postulate  which was originally proposed in [1]  is
adopted in the analysis of ANM-MM.
Postulate 1 (Independence of input and function). If X → Y   the distribution of X and the function
f mapping X to Y are independent since they correspond to independent mechanisms of nature.

In a general perspective  postulate 1 essentially claims the independence between the cause (X) and
mechanism mapping the cause to effect [9]. In ANM-MM  we interpret the independence between
the cause and mechanism in an intuitive way: θ  as the function parameter  captures all variability
of mechanisms f so it should be independent of the cause X according to postulate 1. Based on
the independence between X and θ  cause-effect asymmetry could be derived to infer the causal
direction.
Since ANM-MM consists of a set of ANMs  the identiﬁability result of ANM-MM can be a simple
corollary of that in [6] when the number of ANMs (C) is equal and there is a one-to-one correspon-
dence between mechanisms in the forward and backward ANM-MM. In this case the condition of
ANM-MM being unidentiﬁable is to fulﬁll C ordinary differential equations given in [6] simultane-
ously which can hardly happen in a generic case. However  C in ANM-MM in both directions may
not necessarily be equal and there may also exist many-to-one correspondence between ANMs in
both directions. In this case  the identiﬁability result can not be derived as a simple corollary of [6].
To analyze the identiﬁability result of ANM-MM  we ﬁrst derive lemma 1 to ﬁnd the condition of
existence of many-to-one correspondence (which is a generalization of the condition given in [6]) 
then conclude the identiﬁability result of ANM-MM (theorem 1) based on the condition in lemma 1.
The condition that there exists one backward ANM for a forward ANM-MM is:
Lemma 1. Let X → Y and they follow an ANM-MM. If there exists a backward ANM in the
anti-causal direction  i.e.

X = g(Y ) + ˜ 

the cause distribution (pX)  the noise distribution (p)  the nonlinear function (f) and its parameter
distribution (pθ) should jointly fulﬁll the following ordinary differential equation (ODE)

ξ(cid:48)(cid:48)(cid:48) − G(X  Y )
H(X  Y )

ξ(cid:48)(cid:48) =

G(X  Y )V (X  Y )

U (X  Y )

− H(X  Y ) 

(2)

where ξ := log pX  and the deﬁnitions of G(X  Y )  H(X  Y )  V (X  Y ) and U (X  Y ) are provided
in supplementary due to the page limitation.

direction by p(X  Y ) =(cid:80)C
c=1 p(Y |X  θc)pX (X)pθ(θc) = pX (X)(cid:80)C
(cid:16) ∂2π/∂X∂Y
ward ANM. Since p(X  Y ) should be the same  by substituting p(X  Y ) = pX (X)(cid:80)C
(cid:17)
(cid:16) ∂2π/∂X∂Y

Sketch of proof. Since X and Y follow an ANM-MM  their joint density is factorized in the causal
c=1 acp(Y − f (X; θc)). If
there exists a backward ANM in the anti-causal direction  i.e. X = g(Y )+˜  then p(X  Y ) = p˜(X−
= 0 holds  where π = log [p˜(X − g(Y ))pY (Y )]  in the back-
g(Y ))pY (Y ) and ∂
∂X
c=1 acp(Y −

∂2π/∂X 2

(cid:17)

f (X; θc)) into ∂
∂X

∂2π/∂X 2

= 0  the condition shown in (2) is obtained.

The proof of lemma 1 follows the idea of the identiﬁability of ANM in [6] and is provided in
the supplementary. Since the condition that one backward ANM exists for an forward ANM-MM
(mixture of ANMs) is more restrictive than that for a single forward ANM  which is the identiﬁability
in [6]  lemma 1 indicates that a backward ANM is unlikely to exist in the anticausal direction if 1) X
and Y follow an ANM-MM; 2) postulate 1 holds. Based on lemma 1  it is reasonable to hypothesize
that a stronger result  which is justiﬁed in theorem 1  is valid  i.e. if the g.m. follows an ANM-MM 
then it is almost impossible to have a backward ANM-MM in the anticausal direction.
Theorem 1. Let X → Y and they follow an ANM-MM. If there exists a backward ANM-MM 

X = g(Y ; ω) + ˜ 

3

where ω ∼ pω(ω) =(cid:80) ˜C

˜c=1 b˜c1ω˜c(·)  b˜c > 0 (cid:80) ˜C

then (pX  p  f  pθ) should fulﬁll ˜C ordinary differential equations similar to (2)  i.e. 

˜c=1 b˜c = 1 and ˜ ⊥⊥ Y   in the anticausal direction 

ξ(cid:48)(cid:48)(cid:48) − G(˜c)(X  Y )
H (˜c)(X  Y )

ξ(cid:48)(cid:48) =

G(˜c)(X  Y )V (˜c)(X  Y )

U (˜c)(X  Y )

− H (˜c)(X  Y )  ˜c = 1  2 ···   ˜C 

(3)

where ξ := log pX  G(˜c)(X  Y )  H (˜c)(X  Y )  U (˜c)(X  Y ) and V (˜c)(X  Y ) are deﬁned similarly to
those in lemma 1.

Proof. Assume that there exists ANM-MM in both directions. Then there exists a non overlapping
n=1 = D1 ∪ ··· ∪ D ˜C such that in each data block D˜c 
partition of the entire data D := {(xn  yn)}N
there is an ANM-MM in the causal direction Y = f (X; θ) +   where θ ∼ p(˜c)
θ (θ) is a discrete
distribution on a ﬁnite set Θ(˜c) ⊆ Θ  and an ANM in the anti-causal direction X = g(Y ; ω =
ω˜c) + ˜. According to lemma 1  for each data block  to ensure the existence of an ANM-MM in the
causal direction and an ANM in the anti-causal direction  (pX  p  f  pθ) should fulﬁll an ordinary
differential equation in the form of (2). Then the existence of backward ANM-MM requires ˜C
ordinary differential equations to be fulﬁlled simultaneously which yields (3).

Then the causal direction in ANM-MM can be inferred by investigating the independence between
the hypothetical cause and the corresponding function parameter. According to theorem 1  if they
are independent in the causal direction  then it is highly likely they are dependent in the anticausal
direction. Therefore in practice  the inferred direction is the one that shows more evidence of
independence between them.

2.3 Mechanism clustering of ANM-MM

In ANM-MM  θ  which represents function parameters  can be directly used to identify different g.m.s
since each parameter value corresponds to one mechanism. In other words  observations generated by
the same g.m. would have the same θ if the imposed statistical model is identiﬁable with respect to θ.
Denote the parameter associated with each observation (xn  yn) by θn  we suppose a more practical
inherent clustering structure behind hidden θn. Formally  there is a grouping indicator of integers
z ∈ {1  . . .   C}N that assign each θn to one of the C clusters  through the nth element of z  e.g.
θn belongs to cluster c if [z]n = c  c ∈ {1  . . .   C}. Following ANM-MM  we may assume each
θn belong to one of C components and each component follows N (µc  σ2). A likelihood-based
clustering scheme suggests minimizing −(cid:96) jointly with respect to all means and z

(cid:26) 1√

N(cid:89)

C(cid:89)

n=1

c=1

(cid:18)

(cid:96)(M  z) = log

exp

2πσ

− 1
2σ2 (θn − µc)2

(cid:19)(cid:27)1([z]n=c)

 

where M = {µc}C
σ2 and minimize −(cid:96) using coordinate descent iteratively

c=1 and 1(·) is the indicator function. To simplify further let’s ignore the known

ˆM | z = arg min

ˆz | M = arg min

c=1

{n|[z]n=c}

(θn − µc)2

(cid:88)
(cid:88)

C(cid:88)
C(cid:88)
(cid:80){n|[z]n=c} θn  where nc is the size of the cth cluster

(θn − µc)2.

c=1

{n|[z]n=c}

(4)

(5)

M

z

nc =(cid:80)N

The minimizer of (4) is the mean ˆµc = 1
nc

n=1 1([z]n = c). The minimizer of (5) is group assignment through minimum Euclidean
distance. Therefore  iterating between (4) and (5) coincides with applying k-means algorithm on
all θn and the goal of ﬁnding clusters consistent with the g.m.s for data from ANM-MM can be
achieved by ﬁrstly estimating parameters associated with each observation and then conducting
k-means directly on parameters.

4

3 ANM-MM Estimation by GPPOM

We propose Gaussian process partially observable model (GPPOM) and incorporate Hilbert-Schmidt
independence criterion (HSIC) [4] enforcement into GPPOM to estimate the model parameter θ.
Then we summarize algorithms for causal inference and mechanism clustering of ANM-MM.

3.1 Preliminaries

2 tr

(cid:16)

2 ln (|K|)− 1

2 ln(2π)− D

K−1YYT(cid:17)

Dual PPCA. Dual PPCA [11] is a latent variable model in which maximum likelihood solution
for the latent variables is found by marginalizing out the parameters. Given a set of N centered
D-dimensional data Y = [y1  . . .   yN ]T   dual PPCA learns the q-dimensional latent representation
xn associated with each observation yn. The relation between xn and yn in dual PPCA is yn =
Wxn + n  where the matrix W speciﬁes the linear relation between yn and xn and noise n ∼
N (0  β−1I). Then by placing a standard Gaussian prior on each row of W  one obtains the marginal
likelihood of all observations and the objective function of dual PPCA is the log-likelihood L =
− DN
  where K = XXT + β−1I and X = [x1  . . .   xN ]T .
GP-LVM. GP-LVM [10] generalizes dual PPCA to cases of nonlinear relation between yn and xn
by mapping latent representations in X to a feature space  i.e. Φ = [φ(x1)  . . .   φ(xN )]T   where φ(·)
denotes the canonical feature map. Then K = ΦΦT + β−1I and ΦΦT can be computed using kernel
trick. GP-LVM can also be interpreted as a new class of models which consists of D independent
Gaussian processes [19] mapping from a latent space to an observed data space [10].
HSIC. HSIC [4]  which is based on reproducing kernel Hilbert space (RKHS) theory  is widely used
to measure the dependence between r.v.s. Let D := {(xn  yn)}N
n=1 be a sample of size N draw
independently and identically distributed according to P (X  Y )  HSIC answers the query whether
X ⊥⊥ Y . Formally  denote by F and G RKHSs with universal kernel k  l on the compact domains
X and Y  HSIC is the measure deﬁned as HSIC(P (X  Y ) F G) := (cid:107)Cxy(cid:107)2
HS  which is essentially
the squared Hilbert Schmidt norm [4] of the cross-covariance operator Cxy from RKHS G to F [3].
It is proved in [4] that  under conditions speciﬁed in [5]  HSIC(P (X  Y ) F G) = 0 if and only if
X ⊥⊥ Y . In practice  a biased empirical estimator of HSIC based on the sample D is often adopted:

where [K]ij = k(xi  xj)  [L]ij = l(yi  yj)  H = I − 1

N

(cid:126)1(cid:126)1T   and (cid:126)1 is a N × 1 vector of ones.

HSICb(D) =

1
N 2 tr (KHLH)  

(6)

3.2 Gaussian process partially observable model

Partially observable dual PPCA. Dual PPCA is not directly applicable to model ANM-MM since:
1) part of the r.v. that maps to the effect is visible (i.e. X); 2) the relation (i.e. f) is nonlinear; 3) r.v.s
that contribute to the effect should be independent (X ⊥⊥ θ) in ANM-MM. To tackle 1)  a latent r.v.
θ is brought in dual PPCA.
Denote the observed effect by Y = [y1  . . .   yN ]T   observed cause by X = [x1  . . .   xN ]T   the
matrix collecting function parameters associated with each observation by Θ = [θ1  . . .   θN ]T and
the r.v. that contribute to the effect by ˜X = [X  θ]. Similar to dual PPCA  the relation between the
latent representation and the observation is given by

yn = ˜W ˜xn + n  n = 1  . . .   N

(cid:3)T   ˜W is the matrix speciﬁes the relation between yn and ˜xn  n ∼

where ˜xn = (cid:2)xT
(cid:81)D
N (0  β−1I) is the additive noise. Then by placing a standard Gaussian prior on ˜W  i.e. p( ˜W) =
i=1 N ( ˜wi :|0  I)  where ˜wi : is the ith row of the matrix ˜W  the log-likelihood of the observations
is given by

n   θT
n

L(Θ|X  Y  β) = − DN
2

(7)
where ˜K = ˜X ˜XT + β−1I = [X  Θ] [X  Θ]T + β−1I = XXT + ΘΘT + β−1I is the covariance
matrix after bringing in θ.

ln(2π) − D
2

ln

tr

2

(cid:16)| ˜K|(cid:17) − 1

(cid:16) ˜K−1YYT(cid:17)

 

5

General nonlinear cases (GPPOM). Similar
to the generalization from dual PPCA to GP-
LVM  the dual PPCA with observable X and
latent θ can be easily generalized to nonlin-
ear cases. Denote the feature map by φ(·)
and Φ = [φ( ˜x1)  . . .   φ( ˜xN )]T   then the co-
variance matrix is given by ˜K = ΦΦT +
β−1I.
The entries of ΦΦT can be com-
puted using kernel trick given a selected ker-
nel k(· ·).
In this paper  we adopt the ra-
dial basis function (RBF) kernel  which reads
 
k(xi  xj) = exp
where γd  for d = 1  . . .   Dx  are free param-
eters and Dx is the dimension of the input. As
a result of adopting RBF kernel  the covariance
matrix ˜K in (7) can be computed as

d=1 γd(xid − xjd)2(cid:17)

(cid:16)−(cid:80)Dx

˜K = ΦΦT + β−1I = KX ◦ Kθ + β−1I 

input

Algorithm 1: Causal Inference
:D = {(xn  yn)}N
observations of two r.v.s;
λ - parameter of independence

n=1 - the set of

output :The causal direction

1 Standardize observations of each r.v.;
2 Initialize β and kernel parameters;
3 Optimize (8) in both directions  denote the
the value of HSIC term by HSICX→Y and
HSICY →X  respectively;

4 if HSICX→Y < HSICY →X then
The causal direction is X → Y ;
5
6 else if HSICX→Y > HSICY →X then
The causal direction is Y → X;
7
8 else
9
10 end

No decision made.

where ◦ denotes the Hadamard product  the
entries on ith row and jth column of KX and Kθ are given by [KX ]ij = k(xi  xj) and
[Kθ]ij = k(θi  θj)  respectively. After the nonlinear generalization  the relation between Y and ˜X
reads Y = f ( ˜X) +  = f (X  θ) + . This variant of GP-LVM with partially observable latent space
is named GPPOM in this paper. Like GP-LVM  ˜X is mapped to Y by the same set of Gaussian
processes in GPPOM so the differences in the g.m.s is captured by θn  the latent representations
associated with each observation.

3.3 Model estimation by independence enforcement

Both dual PPCA and GP-LVM ﬁnds the latent representations through log-likelihood maximization
using scaled conjugate gradient [14]. However  the θ can not be found by directly conducting
likelihood maximization since the ANM-MM requires additionally the independence between X and
θ. To this end  we include HSIC [4] in the objective. By incorporating HSIC term into the negative
log-likelihood of GPPOM  the optimization objective reads

J (Θ) = arg min

[−L(Θ|X  Y  Ω) + λ log HSICb(X  Θ)] 

(8)

arg min

Θ Ω

Θ Ω

where λ is the parameter which controls the importance of the HSIC term and Ω is the set of all hyper
parameters including β and all kernel parameters γd  d = 1  . . .   Dx.
To ﬁnd Θ  we resort to the gradient descant methods. The gradient of the objective J with respect to
latent points in Θ is given by

(9)
The ﬁrst part on the right hand side of (9)  which is the gradient of J with respect to the kernel matrix
Kθ  can be computed as

∂ [Θ]ij

= tr

.

(cid:35)

(cid:34)(cid:18) ∂J

∂J
∂ [Θ]ij

(cid:19)T ∂Kθ
(cid:20)(cid:16) ˜K−1YYT ˜K−1 − D ˜K−1(cid:17)T(cid:0)KX ◦ Jij(cid:1)(cid:21)

∂Kθ

+ λ

tr (KX HKθH))

1

HKX H 

(10)

∂J
∂Kθ

= − tr

where Jij is the single-entry matrix  1 at (i  j) and 0 elsewhere and H = I − 1
(cid:126)1(cid:126)1T . Combining
∂L
= ∂k(θm θn)
 
∂Kθ
through the chain rule  all latent points in Θ can be optimized. With Θ  one can conduct causal
inference and mechanism clustering of ANM-MM. The detailed steps are given in Algorithm 1 and 2.

  whose entry on the mth row and nth column is given by ∂[KΘ]mn
∂[Θ]ij

with ∂Kθ
∂[Θ]ij

∂[Θ]ij

N

4 Experiments

6

Figure 3: Accuracy (y-axis) versus sample size (x-axis) on Y = f (X; θc) +  with different
mechanisms. (a) f1  (b) f2  (c) f3  (d) f4.

In this section  experimental results on both
synthetic and real data are given to show the
performance of ANM-MM on causal inference
and mechanism clustering tasks. The Python
code of ANM-MM is available online at https:
//github.com/amber0309/ANM-MM.

4.1 Synthetic data

Algorithm 2: Mechanism clustering

input

n=1 - the set of

:D = {(xn  yn)}N
observations of two r.v.s;
λ - parameter of independence;
C - Number of clusters

output :The cluster labels

1 Standardize observations of each r.v.;
2 Initialize β and kernel parameters;
3 Find Θ by optimizing (8) in causal

direction;

4 Apply k-means on θn  n = 1  . . .   N;
5 return the cluster labels.

In experiments of causal inference  ANM-MM
is compared with ANM [6]  PNL [21]  IGCI
[8]  ECP [20] and LiNGAM [18]. The results
are evaluated using accuracy  which is the per-
centage of correct causal direction estimation
of 50 independent experiments. Note that ANM-MM was applied using different parameter
λ ∈ {0.001  0.01  0.1  1  10} and IGCI was applied using different reference measures and esti-
mators. Their highest accuracy is reported.
In experiments of clustering  ANM-MM is compared with well-known k-means [13] (similarity-
based) on both raw data (k-means) and its PCA component (PCA-km)  Gaussian mixture clustering
(GMM) [16] (model-based)  spectral clustering (SpeClu) [17] (spectral graph theory-based) and
DBSCAN [2] (density-based). Clustering performance is evaluated using average adjusted Rand
index [7] (avgARI)  which is the mean ARI over 100 experiments. High ARI (∈ [−1  1]) indicates
good match between the clustering results and the ground truth. Sample size (N) is 100 in all
synthetic clustering experiments. Clustering results are visualized in the supplementary1.
Different g.m.s and sample sizes. We examine the performance on different g.m.s (f) and sample
sizes (N). The mechanisms adopted are the following elementary functions: 1) f1 =
1.5+θcX 2 ; 2)
f2 = 2 × X θc−0.25; 3) f3 = exp(−θcX); 4) f4 = tanh(θcX). We tested sample size N = 50  100
and 200 for each mechanism. Given f and N  the cause X is sampled from a uniform distribution
U (0  1) and then mapped to the effect by Y = f (X; θc) +   c ∈ {1  2}  where θ1 ∼ U (1  1.1) 
θ2 ∼ U (3  3.1) and  ∼ N (0  0.052). Each mechanism generates half of the observations.
Causal Inference. The results are shown in Fig. 3. ANM-MM and ECP outperforms others based on
a single causal model  which is consistent with our anticipation. Compared with ECP  ANM-MM
shows slight advantages in 3 out of 4 settings. Clustering. The avgARI values are summarized in (i)
of Table 1. ANM-MM signiﬁcantly outperforms other approaches in all mechanism settings.
Different number of g.m.s.2 We examine the performance on different number of g.m.s (C in
Deﬁnition 1). θ1  θ2 and  are the same as in previous experiments. In the setting of three mechanisms 
θ3 ∼ U (0.5  0.6). In the setting of four  θ3 ∼ U (0.5  0.6) and θ4 ∼ U (2  2.1). Again  the numbers
of observations from each mechanism are the same.

1

1The results of PCA-km are not visualized since they are similar to and worse than those of k-means.
2From this part on  g.m. is ﬁxed to be f3.

7

Table 1: avgARI of synthetic clustering experiments

avgARI

f1

ANM-MM 0.393
0.014
k-means
0.013
PCA-km
0.015
GMM
SpeClu
0.003
DBSCAN 0.055

(i) f

f2

0.660
0.039
0.037
0.340
0.129
0.265

f3

0.777
0.046
0.044
0.073
0.295
0.342

(ii) C

(iii) σ

(iv) a1

f4

0.682
0.046
0.048
0.208
0.192
0.358

3

0.610
0.194
0.056
0.237
0.285
0.257

4

0.447
0.165
0.041
0.202
0.175
0.106

0.01
0.798
0.049
0.047
0.191
0.595
0.527

0.1
0.608
0.042
0.040
0.025
0.048
0.110

0.25
0.604
0.047
0.052
0.048
0.044
0.521

0.75
0.867
0.013
0.014
0.381
-0.008
0.718

(a)

(b)

(c)

Figure 4: Accuracy (y-axis) versus (a) number of mechanisms; (b) noise standard deviation; (c)
mixing proportion; on f3 with N = 100.

Causal Inference. The results are given in Fig. 4a which shows decreasing trend for all approaches.
However  ANM-MM keeps 100% when the number of mechanisms increases from 2 to 3. Clustering.
The avgARI values are given in (ii) and (i)f3 of Table 1. The performance of different approaches
show different trends which is probably due to the clustering principle they are based on. Although
ANM-MM is heavily inﬂuenced by C  its performance is still much better than others.
Different noise standard deviations. We ex-
amine the performance on different noise stan-
dard deviations σ. θ1  θ2 are the same as in the
ﬁrst part of experiments. Three different cases
where σ = 0.01  0.05 and 0.1 are tested.
Causal Inference. The results are given in Fig.
4b. The change in σ in this range does not signif-
icantly inﬂuence the performance of most causal
inference approaches. ANM-MM keeps 100%
accuracy for all choice of σ. Clustering. The
avgARI values are given in (iii) and (i)f3 of
Table 1. As our anticipation  the clustering re-
sults heavily rely on σ and all approaches show
a decreasing trend in avgARI as σ increases.
However  ANM-MM is the most robust against
large σ.
Different mixing proportions. We examine the performance on different mixing proportions (ac in
Deﬁnition 1). θ1  θ2 and σ are the same as in the ﬁrst part of experiments. Cases where a1 = 0.25  0.5
and 0.75 (corresponding a2 = 0.75  0.5 and 0.25) are tested.
Causal Inference. The results on different a1 are given in Fig. 4c. Approaches based on a single
causal model are sensitive to the change in a1 whereas ECP and ANM-MM are more robust and
outperform others. Clustering. The avgARI values of experiments on different a1 are given in (iv) and
(i)f3 of Table 1. The results of comparing approaches are signiﬁcantly affected by a1 and ANM-MM
shows best robustness against the change in a1.

Figure 5: Accuracy on real cause-effect pairs.

8

(a) Ground truth

(b) ANM-MM

(c) k-means

(d) GMM

(e) SpeClu

(f) DBSCAN

Figure 6: Ground truth and clustering results of different approaches on BAFU air data.

4.2 Real data

Causal inference on Tüebingen cause-effect pairs. We evaluate the causal inference performance
of ANM-MM on real world benchmark cause-effect pairs3 [15]. Nine out of 41 data sets are excluded
in our experiment because either they consists of multivariate or categorical data (pair 47  52  53  54 
55  70  71  101 and 105) or the estimated latent representations are extremely close4 (pair 12 and
17). Fifty independent experiments are repeated for each pair  and the percentage of correct inference
of different approaches are recorded. Then average percentage of pairs from the same data set is
computed as the accuracy of the corresponding data set. In each independent experiment  different
inference approaches are applied on 90 points randomly sampled from raw data without replacement.
The results are summarized in Fig. 5 with blue solid line indicating median accuracy and red dashed
line indicating mean accuracy. It shows that the performance of ANM-MM is satisfactory  with
highest median accuracy of about 82%. IGCI also performs quite well  especially in terms of median 
followed by PNL.
Clustering on BAFU air data. We evaluate the clustering performance of ANM-MM on real air data
obtained online5. This data consists of daily mean values of ozone (µg/m3) and temperature (◦) of
2009 from two distinct locations in Switzerland. In our experiment  we regard the data as generating
from two mechanisms (each corresponds to a location). The clustering results are visualized in Fig. 6.
The ARI values of ANM-MM is 0.503  whereas k-means  GMM  spectral clustering and DBSCAN
could only obtain ARI of -0.001  0.003  0.078 and 0.003  respectively. ANM-MM is the only one
that could reveal the property related to the location of the data g.m..

5 Conclusion

In this paper  we extend the ANM to a more general model (ANM-MM) in which there are a ﬁnite
number of ANMs of the same function form and differ only in parameter values. The condition of
identiﬁability of ANM-MM is analyzed. To estimate ANM-MM  we adopt the GP-LVM framework
and propose a variant of it called GPPOM to ﬁnd the optimized latent representations and further
conduct causal inference and mechanism clustering. Results on both synthetic and real world data
verify the effectiveness of our proposed approach.

3https://webdav.tuebingen.mpg.de/cause-effect/.
4close in the sense that |θi − θj| < 0.001.
5https://www.bafu.admin.ch/bafu/en/home/topics/air.html

9

Acknowledgments

This work is partially supported by the Hong Kong Research Grants Council.

References
[1] Daniusis  P.  Janzing  D.  Mooij  J.  Zscheischler  J.  Steudel  B.  Zhang  K.  and Schölkopf  B.

(2012). Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475.

[2] Ester  M.  Kriegel  H.-P.  Sander  J.  and Xu  X. (1996). A density-based algorithm for discovering
clusters a density-based algorithm for discovering clusters in large spatial databases with noise. In
Proceedings of the Second International Conference on Knowledge Discovery and Data Mining 
KDD’96  pages 226–231. AAAI Press.

[3] Fukumizu  K.  Bach  F. R.  and Jordan  M. I. (2004). Dimensionality reduction for supervised
learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research  5(Jan):73–
99.

[4] Gretton  A.  Bousquet  O.  Smola  A.  and Schölkopf  B. (2005a). Measuring statistical depen-
dence with hilbert-schmidt norms. In International conference on algorithmic learning theory 
pages 63–77. Springer.

[5] Gretton  A.  Smola  A. J.  Bousquet  O.  Herbrich  R.  Belitski  A.  Augath  M.  Murayama 
Y.  Pauls  J.  Schölkopf  B.  and Logothetis  N. K. (2005b). Kernel constrained covariance for
dependence measurement. In AISTATS  volume 10  pages 112–119.

[6] Hoyer  P. O.  Janzing  D.  Mooij  J. M.  Peters  J.  and Schölkopf  B. (2009). Nonlinear causal
discovery with additive noise models. In Advances in neural information processing systems 
pages 689–696.

[7] Hubert  L. and Arabie  P. (1985). Comparing partitions. Journal of classiﬁcation  2(1):193–218.

[8] Janzing  D.  Mooij  J.  Zhang  K.  Lemeire  J.  Zscheischler  J.  Daniušis  P.  Steudel  B.  and
Schölkopf  B. (2012). Information-geometric approach to inferring causal directions. Artiﬁcial
Intelligence  182:1–31.

[9] Janzing  D. and Scholkopf  B. (2010). Causal inference using the algorithmic markov condition.

IEEE Transactions on Information Theory  56(10):5168–5194.

[10] Lawrence  N. (2005). Probabilistic non-linear principal component analysis with gaussian

process latent variable models. Journal of machine learning research  6(Nov):1783–1816.

[11] Lawrence  N. D. (2004). Gaussian process latent variable models for visualisation of high

dimensional data. In Advances in neural information processing systems  pages 329–336.

[12] Liu  F. and Chan  L. (2016). Causal discovery on discrete data with extensions to mixture model.

ACM Transactions on Intelligent Systems and Technology (TIST)  7(2):21.

[13] MacQueen  J. (1967). Some methods for classiﬁcation and analysis of multivariate observations.
In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 
Volume 1: Statistics  pages 281–297  Berkeley  Calif. University of California Press.

[14] Møller  M. F. (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural

networks  6(4):525–533.

[15] Mooij  J. M.  Peters  J.  Janzing  D.  Zscheischler  J.  and Schölkopf  B. (2016). Distinguishing
cause from effect using observational data: methods and benchmarks. The Journal of Machine
Learning Research  17(1):1103–1204.

[16] Rasmussen  C. E. (2000). The inﬁnite gaussian mixture model. In Advances in neural informa-

tion processing systems  pages 554–560.

[17] Shi  J. and Malik  J. (2000). Normalized cuts and image segmentation. IEEE Transactions on

pattern analysis and machine intelligence  22(8):888–905.

10

[18] Shimizu  S.  Hoyer  P. O.  Hyvärinen  A.  and Kerminen  A. (2006). A linear non-gaussian
acyclic model for causal discovery. Journal of Machine Learning Research  7(Oct):2003–2030.

[19] Williams  C. K. (1998). Prediction with gaussian processes: From linear regression to linear

prediction and beyond. In Learning in graphical models  pages 599–621. Springer.

[20] Zhang  K.  Huang  B.  Zhang  J.  Schölkopf  B.  and Glymour  C. (2015). Discovery and

visualization of nonstationary causal models. arXiv preprint arXiv:1509.08056.

[21] Zhang  K. and Hyvärinen  A. (2009). On the identiﬁability of the post-nonlinear causal model. In
Proceedings of the twenty-ﬁfth conference on uncertainty in artiﬁcial intelligence  pages 647–655.
AUAI Press.

11

,Tomer Koren
Roi Livni
Shoubo Hu
Zhitang Chen
Vahid Partovi Nia
Laiwan CHAN
Yanhui Geng