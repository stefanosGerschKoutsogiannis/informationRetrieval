2013,Stochastic blockmodel approximation of a graphon: Theory and consistent estimation,Given a convergent sequence of graphs  there exists a limit object called the graphon from which random graphs are generated. This nonparametric perspective of random graphs opens the door to study graphs beyond the traditional parametric models  but at the same time also poses the challenging question of how to estimate the graphon underlying observed graphs. In this paper  we propose a computationally efficient algorithm to estimate a graphon from a set of observed graphs generated from it. We show that  by approximating the graphon with stochastic block models  the graphon can be consistently estimated  that is  the estimation error vanishes as the size of the graph approaches infinity.,Stochastic blockmodel approximation of a graphon:

Theory and consistent estimation

Edoardo M. Airoldi

Dept. Statistics

Harvard University

Thiago B. Costa

Stanley H. Chan

SEAS  and Dept. Statistics

SEAS  and Dept. Statistics

Harvard University

Harvard University

Abstract

Non-parametric approaches for analyzing network data based on exchangeable
graph models (ExGM) have recently gained interest. The key object that deﬁnes
an ExGM is often referred to as a graphon. This non-parametric perspective on
network modeling poses challenging questions on how to make inference on the
graphon underlying observed network data. In this paper  we propose a computa-
tionally efﬁcient procedure to estimate a graphon from a set of observed networks
generated from it. This procedure is based on a stochastic blockmodel approxi-
mation (SBA) of the graphon. We show that  by approximating the graphon with
a stochastic block model  the graphon can be consistently estimated  that is  the
estimation error vanishes as the size of the graph approaches inﬁnity.

1 Introduction

Revealing hidden structures of a graph is the heart of many data analysis problems. From the well-
known small-world network to the recent large-scale data collected from online service providers
such as Wikipedia  Twitter and Facebook  there is always a momentum in seeking better and more
informative representations of the graphs [1  14  29  3  26  12]. In this paper  we develop a new com-
putational tool to study one type of non-parametric representations which recently draws signiﬁcant
attentions from the community [4  19  5  30  23].

The root of the non-parametric model discussed in this paper is in the theory of exchangeable ran-
dom arrays [2  15  16]  and it is presented in [11] as a link connecting de Finetti’s work on partial
exchangeability and graph limits [20  6]. In a nutshell  the theory predicts that every convergent
sequence of graphs (Gn) has a limit object that preserves many local and global properties of the
graphs in the sequence. This limit object  which is called a graphon  can be represented by mea-
surable functions w : [0  1]2 → [0  1]  in a way that any w′ obtained from measure preserving
transformations of w describes the same graphon.
Graphons are usually seen as kernel functions for random network models [18]. To construct an
n-vertex random graph G(n  w) for a given w  we ﬁrst assign a random label ui ∼ Uniform[0  1] to
each vertex i ∈ {1  . . .   n}  and connect any two vertices i and j with probability w(ui  uj)  i.e. 

Pr (G[i  j] = 1 | ui  uj) = w(ui  uj) 

i  j = 1  . . .   n 

(1)

where G[i  j] denotes the (i  j)th entry of the adjacency matrix representing a particular realization
of G(n  w) (See Figure 1). As an example  we note that the stochastic block-model is the case where
w(x  y) is a piecewise constant function.
The problem of interest is deﬁned as follows: Given a sequence of 2T observed directed graphs

G1  . . .   G2T   can we make an estimate bw of w  such that bw → w with high probability as n → ∞?

This question has been loosely attempted in the literature  but none of which has a complete solution.
For example  Lloyd et al. [19] proposed a Bayesian estimator without a consistency proof; Choi and

1

w

w(ui  uj)

G2T

×

uj

ui

(ui  uj)

G1

[Left] Given a graphon w :

samples ui  uj from
Figure 1:
Uniform[0 1] and assign Gt[i  j] = 1 with probability w(ui  uj)  for t = 1  . . .   2T .
[Middle]
Heat map of a graphon w. [Right] A random graph generated by the graphon shown in the middle.
Rows and columns of the graph are ordered by increasing ui  instead of i for better visualization.

[0  1]2 → [0  1]  we draw i.i.d.

Wolfe [9] studied the consistency properties  but did not provide algorithms to estimate the graphon.
To the best of our knowledge  the only method that estimates graphons consistently  besides ours  is
USVT [8]. However  our algorithm has better complexity and outperforms USVT in our simulations.
More recently  other groups have begun exploring approaches related to ours [28  24].

The proposed approximation procedure requires w to be piecewise Lipschitz. The basic idea is to

approximate w by a two-dimensional step function bw with diminishing intervals as n increases.The

proposed method is called the stochastic blockmodel approximation (SBA) algorithm  as the idea of
using a two-dimensional step function for approximation is equivalent to using the stochastic block
models [10  22  13  7  25]. The SBA algorithm is deﬁned up to permutations of the nodes  so the
estimated graphon is not canonical. However  this does not affect the consistency properties of the
SBA algorithm  as the consistency is measured w.r.t. the graphon that generates the graphs.

2 Stochastic blockmodel approximation: Procedure

In this section we present the proposed SBA algorithm and discuss its basic properties.

2.1 Assumptions on graphons

We assume that w is piecewise Lipschitz  i.e.  there exists a sequence of non-overlaping intervals
Ik = [αk−1  αk] deﬁned by 0 = α0 < . . . < αK = 1  and a constant L > 0 such that  for any
(x1  y1) and (x2  y2) ∈ Iij = Ii × Ij 

|w(x1  y1) − w(x2  y2)| ≤ L (|x1 − x2| + |y1 − y2|) .

For generality we assume w to be asymmetric i.e.  w(u  v) 6= w(v  u)  so that symmetric graphons
can be considered as a special case. Consequently  a random graph G(n  w) generated by w is
directed  i.e.  G[i  j] 6= G[j  i].
2.2 Similarity of graphon slices

The intuition of the proposed SBA algorithm is that if the graphon is smooth  neighboring cross-
sections of the graphon should be similar. In other words  if two labels ui and uj are close i.e. 
|ui − uj| ≈ 0  then the difference between the row slices |w(ui ·)− w(uj ·)| and the column slices
|w(·  ui) − w(·  uj)| should also be small. To measure the similarity between two labels using the
graphon slices  we deﬁne the following distance

dij =

1

2(cid:18)Z 1

0

[w(x  ui) − w(x  uj )]2 dx +Z 1

0

[w(ui  y) − w(uj   y)]2 dy(cid:19) .

(2)

2

 
 
Thus  dij is small only if both row and column slices of the graphon are similar.
The usage of dij for graphon estimation will be discussed in the next subsection. But before
we proceed  it should be noted that in practice dij has to be estimated from the observed graphs

1

1

ij =

ij =

w(x  ui)w(x  uj )dx

and

w(ui  y)w(uj   y)dy 

cij =Z 1

0

rij =Z 1

0

tors can be easily obtained. To this end  we let

Gt2 [k  j]  
Gt2 [j  k] .

and express dij as dij = 1
we consider the following estimators for cij and rij:

G1  . . .   G2T . To derive an estimator bdij of dij  it is helpful to express dij in a way that the estima-
2h(cii−cij−cji +cjj )+(rii−rij−rji +rjj )i. Inspecting this expression 
T 2 X1≤t1≤T
bck
T 2 X1≤t1≤T
brk
2" 1
S Xk∈S(cid:8)(cid:0)brk
ii −brk

respectively. Summing all possible k’s yields an estimator bdij that looks similar to dij:
jj(cid:1)(cid:9)#  
where S = {1  . . .   n}\{i  j} is the set of summation indices.
The motivation of deﬁning the estimators in (3) and (4) is that a row of the adjacency matrix G[i ·]
is fully characterized by the corresponding row of the graphon w(ui ·). Thus the expected value of
T (cid:16)P1≤t1≤T Gt1 [i ·](cid:17) is w(ui ·)  and hence 1
ij is an estimator for rij. To theoretically
justify this intuition  we will show in Section 3 that bdij is indeed a good estimator: it is not only
unbiased  but is also concentrated round dij for large n. Furthermore  we will show that it is possible
to use a random subset of S instead of {1  . . .   n}\{i  j} to achieve the same asymptotic behavior.
As a result  the estimation of dij can be performed locally in a neighborhood of i and j  instead of
all n vertices.

Gt1 [k  i] XT <t2≤2T
Gt1 [i  k] XT <t2≤2T
jj(cid:1) +(cid:0)bck

Here  the superscript k can be interpreted as the dummy variables x and y in deﬁning cij and rij 

ii −bck

ij −bck

ji +bck

ij −brk

ji +brk

SPk∈Sbrk

1

bdij =

(3)

(4)

(5)

1

2.3 Blocking the vertices

The similarity metric bdij discussed above suggests one simple method to approximate w by a piece-
wise constant function bw (i.e.  a stochastic block-model). Given G1  . . .   G2T   we can cluster the
(unknown) labels {u1  . . .   un} into K blocks bB1  . . .   bBK using a procedure described below. Once
the blocks bB1  . . .   bBK are deﬁned  we can then determine bw(ui  uj) by computing the empirical
frequency of edges that are present across blocks bBi and bBj:

(G1[ix  jy] + G2[ix  jy] + . . . + G2T [ix  jy])  

(6)

1

bw(ui  uj) =

|bBi||bBj| Xix∈ bBi Xjy∈ bBj

1
2T

where bBi is the block containing ui so that summing Gt[x  y] over x ∈ bBi and y ∈ bBj yields an
estimate of the expected number of edges linking block bBi and bBj.
To cluster the unknown labels {u1  . . .   un} we propose a greedy approach as shown in Algorithm
1. Starting with Ω = {u1  . . .   un}  we randomly pick a node ip and call it the pivot. Then for all
other vertices iv ∈ Ω\{ip}  we compute the distancebdip iv and check whetherbdip iv < ∆2 for some
precision parameter ∆ > 0. If bdip iv < ∆2  then we assign iv to the same block as ip. Therefore 
after scanning through Ω once  a block bB1 = {ip  iv1  iv2   . . .} will be deﬁned. By updating Ω as
Ω ← Ω\bB1  the process repeats until Ω = ∅.

3

The proposed greedy algorithm is only a local solution in a sense that it does not return the globally
optimal clusters. However  as will be shown in Section 3  although the clustering algorithm is not

globally optimal  the estimated graphon bw is still guaranteed to be a consistent estimate of the true

graphon w as n → ∞. Since the greedy algorithm is numerically efﬁcient  it serves as a practical
computational tool to estimate w.

2.4 Main algorithm

Algorithm 1 Stochastic blockmodel approximation

Input: A set of observed graphs G1  . . .   G2T and the precision parameter ∆.

Output: Estimated stochastic blocks bB1  . . .   bBK.
Initialize: Ω = {1  . . .   n}  and k = 1.
while Ω 6= ∅ do
Randomly choose a vertex ip from Ω and assign it as the pivot for bBk: bBk ← ip.
for Every other vertices iv ∈ Ω\{ip} do
Compute the distance estimate bdip iv .
If bdip iv ≤ ∆2  then assign iv as a member of bBk: bBk ← iv.
Update Ω: Ω ← Ω\bBk.
Update counter: k ← k + 1.

end while

end for

Algorithm 1 illustrates the pseudo-code for the proposed stochastic block-model approximation.
The complexity of this algorithm is O(T SKn)  where T is half the number of observations  S is
the size of the neighborhood  K is the number of blocks and n is number of vertices of the graph.

3 Stochastic blockmodel approximation: Theory of estimation

In this section we present the theoretical aspects of the proposed SBA algorithm. We will ﬁrst

discuss the properties of the estimator bdij  and then show the consistency of the estimated graphon
bw. Details of the proofs can be found in the supplementary material.
3.1 Concentration analysis of bdij
Our ﬁrst theorem below shows that the proposed estimator bdij is both unbiased  and is concentrated
Theorem 1. The estimator bdij for dij is unbiased  i.e.  E[bdij ] = dij. Further  for any ǫ > 0 

around its expected value dij.

32/T +8ǫ/3  

(7)

where S is the size of the neighborhood S  and 2T is the number of observations.
Proof. Here we only highlight the important steps to present the intuition. The basic idea of the
ij and show that it is unbiased. To this end  we use the

proof is to zoom-in a microscopic term ofbrk

fact that Gt1 [i  k] and Gt2 [j  k] are conditionally independent on uk to show
E[Gt1 [i  k]Gt2[j  k] | uk] = Pr[Gt1 [i  k] = 1  Gt2[j  k] = 1 | uk]

Prh(cid:12)(cid:12)(cid:12)bdij − dij(cid:12)(cid:12)(cid:12) > ǫi ≤ 8e− Sǫ2

(a)

= Pr[Gt1 [i  k] = 1 | uk] Pr[Gt2 [j  k] = 1 | uk]
= w(ui  uk)w(uj   uk) 

ij ] =
ij | uk]] = rij. The concentration inequality follows from a similar idea to bound the variance
ij and apply Bernstein’s inequality.

ij | uk] = w(ui  uk)w(uj   uk)  and by iterated expectation we have E[brk

which then implies E[brk
E[E[brk
ofbrk

4

That Gt1[i  k] and Gt2[j  k] are conditionally independent on uk is a critical fact for the success of
the proposed algorithm. It also explains why at least 2 independently observed graphs are necessary 
for otherwise we cannot separate the probability in the second equality above marked with (a).

3.2 Choosing the number of blocks

The performance of the Algorithm 1 is sensitive to the number of blocks it deﬁnes. On the one hand 
it is desirable to have more blocks so that the graphon can be ﬁnely approximated. But on the other
hand  if the number of blocks is too large then each block will contain only few vertices. This is bad
because in order to estimate the value on each block  a sufﬁcient number of vertices in each block is
required. The trade-off between these two cases is controlled by the precision parameter ∆: a large
∆ generates few large clusters  while small ∆ generates many small clusters. A precise relationship
between the ∆ and K  the number of blocks generated the algorithm  is given in Theorem 2.
Theorem 2. Let ∆ be the accuracy parameter and K be the number of blocks estimated by Algo-
rithm 1  then

Pr"K >

∆ # ≤ 8n2e−
QL√2

S∆4

128/T +16∆2/3  

(8)

where L is the Lipschitz constant and Q is the number of Lipschitz blocks in w.

In practice  we estimate ∆ using a cross-validation scheme to ﬁnd the optimal 2D histogram bin
width [27]. The idea is to test a sequence of potential values of ∆ and seek the one that minimizes
the cross validation risk  deﬁned as

(9)

2

h(n − 1) −

n + 1
h(n − 1)

bJ(∆) =

j  

KXj=1bp2

for a sequence of ∆’s do

Algorithm 2 Cross Validation
Input: Graphs G1  . . .   G2T .

wherebpj = |bBj|/n and h = 1/K. Algorithm 2 details the proposed cross-validation scheme.
Output: Blocks bB1  . . .   bBK  and optimal ∆.
Estimate blocks bB1  . . .   bBK from G1  . . .   G2T . [Algorithm 1]
Computebpj = |bBj|/n  for j = 1  . . .   K.
h(n−1)PK
Compute bJ(∆) = 2
j=1bp2
Pick the ∆ with minimum bJ(∆)  and the corresponding bB1  . . .   bBK.
3.3 Consistency of bw
The goal of our next theorem is to show that bw is a consistent estimate of w  i.e.  bw → w as n → ∞.

To begin with  let us ﬁrst recall two commonly used metric:
Deﬁnition 1. The mean squared error (MSE) and mean absolute error (MAE) are deﬁned as

h(n−1) − n+1

j   with h = 1/K.

end for

1
n2

MSE(bw) =
MAE(bw) =

nXiv =1
nXjv =1
(w(uiv   ujv ) − bw(uiv   ujv ))2
nXiv =1
nXjv =1
|w(uiv   ujv ) − bw(uiv   ujv )| .
Theorem 3. If S ∈ Θ(n) and ∆ ∈ ω(cid:18)(cid:16) log(n)
4(cid:19) ∩ o(1)  then
n (cid:17) 1
E[MAE(bw)] = 0

E[MSE(bw)] = 0.

lim
n→∞

lim
n→∞

1
n2

and

5

Proof. The details of the proof can be found in the supplementary material . Here we only outline
the key steps to present the intuition of the theorem. The goal of Theorem 3 is to show convergence

1

1

1
2T

w(ui  uj) =

w(uix   ujx) 

bw(ui  uj) =

(G1[ix  jy] + G2[ix  jy] + . . . + G2T [ix  jy])  

of |bw(ui  uj) − w(ui  uj)|. The idea is to consider the following two quantities:

|bBi||bBj| Xix∈ bBi Xjx∈ bBj
|bBi||bBj| Xix∈ bBi Xjy∈ bBj
so that if we can bound |w(ui  uj) − w(ui  uj)| and |w(ui  uj) − bw(ui  uj)|  then consequently
|bw(ui  uj) − w(ui  uj)| can also be bounded.
The bound for the ﬁrst term |w(ui  uj) − w(ui  uj)| is shown in Lemma 1: By Algorithm 1  any
vertex iv ∈ bBi is guaranteed to be within a distance ∆ from the pivot of bBi. Since w(ui  uj) is an
average over bBi and bBj  by Theorem 1 a probability bound involving ∆ can be obtained.
The bound for the second term |w(ui  uj)−bw(ui  uj)| is shown in Lemma 2. Different from Lemma

1  here we need to consider two possible situations: either the intermediate estimate w(ui  uj) is
close to the ground truth w(ui  uj)  or w(ui  uj) is far from the ground truth w(ui  uj). This ac-
counts for the sum in Lemma 2. Individual bounds are derived based on Lemma 1 and Theorem 1.

Combining Lemma 1 and Lemma 2  we can then bound the error and show convergence.

Lemma 1. For any iv ∈ bBi and jv ∈ bBj 
Prh|w(ui  uj) − w(uiv   ujv )| > 8∆1/2L1/4i ≤ 32|bBi||bBj|e−
Lemma 2. For any iv ∈ bBi and jv ∈ bBj 
Prh|bwij − wij| > 8∆1/2L1/4i ≤ 2e−256(T| bBi| | bBj|√L∆) + 32|bBi|2|bBj|2e−
The condition S ∈ Θ(n) is necessary to make Theorem 3 valid  because if S is independent of n 
the right hand sides of (10) and (11) cannot approach 0 even if n → ∞. The condition on ∆ is also
important as it forces the numerators and denominators in the exponentials of (10) and (11) to be
well behaved.

32/T +8∆2/3) .

32/T +8∆2/3 .

(10)

(11)

S∆4

S∆4

4 Experiments

In this section we evaluate the proposed SBA algorithm by showing some empirical results. For the
purpose of comparison  we consider (i) the universal singular value thresholding (USVT) [8]; (ii)
the largest-gap algorithm (LG) [7]; (iii) matrix completion from few entries (OptSpace) [17].

4.1 Estimating stochastic blockmodels

Accuracy as a function of growing graph size. Our ﬁrst experiment is to evaluate the proposed
SBA algorithm for estimating stochastic blockmodels. For this purpose  we generate (arbitrarily) a
graphon

(12)

w =

0.8 0.9 0.4 0.5
0.1 0.6 0.3 0.2
0.3 0.2 0.8 0.3
0.4 0.1 0.2 0.9

  

which represents a piecewise constant function with 4 × 4 equi-space blocks.
Since USVT and LG use only one observed graph whereas the proposed SBA require at least 2
observations  in order to make the comparison fair  we use half of the nodes for SBA by generating
two independent n

2 observed graphs. For USVT and LG  we use one n × n observed graph.

Figure 2(a) shows the asymptotic behavior of the algorithms when n grows. Figure 2(b) shows the
estimation error of SBA algorithm as T grows for graphs of size 200 vertices.

2 × n

6

−0.5

−1

)
E
A
M

(
0
1
g
o
l

−1.5

−2

−2.5

 
−3
0

 

Proposed

 

−2

−2.1

−2.2

−2.3

−2.4

−2.5

−2.6

−2.7

−2.8

−2.9

)
E
A
M

(
0
1
g
o
l

Proposed
Largest Gap
OptSpace
USVT

200

400

n

600

800

1000

(a) Growing graph size  n

 
−3
0

5

10

15

20
2T

25

30

35

40

(b) Growing no. observations  2T

Figure 2: (a) MAE reduces as graph size grows. For the fairness of the amount of data that can be
used  we use n
2 × 2 observations for SBA  and n × n × 1 observation for USVT [8] and LG
[7]. (b) MAE of the proposed SBA algorithm reduces when more observations T is available. Both
plots are averaged over 100 independent trials.

2 × n

Accuracy as a function of growing number of blocks. Our second experiment is to evaluate the
performance of the algorithms as K  the number of blocks  increases. To this end  we consider a
sequence of K  and for each K we generate a graphon w of K × K blocks. Each entry of the
block is a random number generated from Uniform[0  1]. Same as the previous experiment  we ﬁx
n = 200 and T = 1. The experiment is repeated over 100 trials so that in every trial a different
graphon is generated. The result shown in Figure 3(a) indicates that while estimation error increases
as K grows  the proposed SBA algorithm still attains the lowest MAE for all K.

)
E
A
M

(
0
1
g
o
l

−0.7

−0.8

−0.9

−1

−1.1

−1.2

−1.3

 
−1.4
0

 

Proposed
Largest Gap
USVT

15

20

5

10
K

(a) Growing no. blocks  K

)
E
A
M

(
0
1
g
o
l

−0.6

−0.7

−0.8

−0.9

−1

−1.1

−1.2

−1.3

−1.4

−1.5

 
−1.6
0

 

Proposed
Largest Gap
OptSpace
USVT

5

10

15

20

% missing links

(b) Missing links

Figure 3: (a) As K increases  MAE of all three algorithm increases but SBA still attains the lowest
MAE. Here  we use n
2 × 2 observations for SBA  and n × n × 1 observation for USVT [8] and
LG [7]. (b) Estimation of graphon in the presence of missing links: As the amount of missing links
increases  estimation error also increases.

2 × n

4.2 Estimation with missing edges

Our next experiment is to evaluate the performance of proposed SBA algorithm when there are
missing edges in the observed graph. To model missing edges  we construct an n × n binary matrix
M with probability Pr[M [i  j] = 0] = ξ  where 0 ≤ ξ ≤ 1 deﬁnes the percentage of missing
edges. Given ξ  2T matrices are generated with missing edges  and the observed graphs are deﬁned
as M1 ⊙ G1  . . .   M2T ⊙ G2T   where ⊙ denotes the element-wise multiplication. The goal is to

study how well SBA can reconstruct the graphon bw in the presence of missing links.

7

The modiﬁcation of the proposed SBA algorithm for the case missing links is minimal: when com-

puting (6)  instead of averaging over all ix ∈ bBi and jy ∈ bBj  we only average ix ∈ bBi and jy ∈ bBj

that are not masked out by all M′s. Figure 3(b) shows the result of average over 100 independent
trials. Here  we consider the graphon given in (12)  with n = 200 and T = 1. It is evident that SBA
outperforms its counterparts at a lower rate of missing links.

4.3 Estimating continuous graphons

Our ﬁnal experiment is to evaluate the proposed SBA algorithm in estimating continuous graphons.
Here  we consider two of the graphons reported in [8]:

w1(u  v) =

1

1 + exp{−50(u2 + v2)}

 

and w2(u  v) = uv 

where u  v ∈ [0  1]. Here  w2 can be considered as a special case of the Eigenmodel [13] or latent
feature relational model [21].

The results in Figure 4 shows that while both algorithms have improved estimates when n grows  the
performance depends on which of w1 and w2 that we are studying. This suggests that in practice the
choice of the algorithm should depend on the expected structure of the graphon to be estimated: If the
graph generated by the graphon demonstrates some low-rank properties  then USVT is likely to be
a better option. For more structured or complex graphons the proposed procedure is recommended.

)
E
A
M

(
0
1
g
o
l

−2.9

−2.95

−3

−3.05

−3.1

−3.15

 
−3.2
0

 

Proposed
USVT

200

400

n

600

800

1000

(a) graphon w1

)
E
A
M

(
0
1
g
o
l

−0.6

−0.8

−1

−1.2

−1.4

−1.6

−1.8

 
−2
0

 

Proposed
USVT

200

400

n

600

800

1000

(b) graphon w2

Figure 4: Comparison between SBA and USVT in estimating two continuous graphons w1 and w2.
Evidently  SBA performs better for w1 (high-rank) and worse for w2 (low-rank).

5 Concluding remarks

We presented a new computational tool for estimating graphons. The proposed algorithm approx-
imates the continuous graphon by a stochastic block-model  in which the ﬁrst step is to cluster
the unknown vertex labels into blocks by using an empirical estimate of the distance between two
graphon slices  and the second step is to build an empirical histogram to estimate the graphon. Com-
plete consistency analysis of the algorithm is derived. The algorithm was evaluated experimentally 
and we found that the algorithm is effective in estimating block structured graphons.

Implementation of the SBA algorithm is available online at https://github.com/airoldilab/SBA.
Acknowledgments. EMA is partially supported by NSF CAREER award IIS-1149662  ARO MURI
award W911NF-11-1-0036  and an Alfred P. Sloan Research Fellowship. SHC is partially supported
by a Croucher Foundation Post-Doctoral Research Fellowship.

References
[1] E.M. Airoldi  D.M. Blei  S.E. Fienberg  and E.P. Xing. Mixed-membership stochastic blockmodels.

Journal of Machine Learning Research  9:1981–2014  2008.

8

[2] D.J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multi-

variate Analysis  11:581–598  1981.

[3] H. Azari and E. M. Airoldi. Graphlet decomposition of a weighted network. Journal of Machine Learning

Research  W&CP  22:54–63  2012.

[4] P.J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other mod-

ularities. Proc. Natl. Acad. Sci. USA  106:21068–21073  2009.

[5] P.J. Bickel  A. Chen  and E. Levina. The method of moments and degree distributions for network models.

Annals of Statistics  39(5):2280–2301  2011.

[6] C. Borgs  J. Chayes  L. Lov´asz  V. T. S´os  B. Szegedy  and K. Vesztergombi. Graph limits and parameter

testing. In Proc. ACM Symposium on Theory of Computing  pages 261–270  2006.

[7] A. Channarond  J. Daudin  and S. Robin. Classiﬁcation and estimation in the Stochastic Blockmodel

based on the empirical degrees. Electronic Journal of Statistics  6:2574–2601  2012.

[8] S. Chatterjee. Matrix estimation by universal singular value thresholding. ArXiv:1212.1247. 2012.
[9] D.S. Choi and P.J. Wolfe. Co-clustering separately exchangeable network data. ArXiv:1212.4093. 2012.
[10] D.S. Choi  P.J. Wolfe  and E.M. Airoldi. Stochastic blockmodels with a growing number of classes.

Biometrika  99:273–284  2012.

[11] P. Diaconis and S. Janson. Graph limits and exchangeable random graphs. Rendiconti di Matematica e

delle sue Applicazioni  Series VII  pages 33–61  2008.

[12] A. Goldenberg  A.X. Zheng  S.E. Fienberg  and E.M. Airoldi. A survey of statistical network models.

Foundations and Trends in Machine Learning  2:129–233  2009.

[13] P.D. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data.

Information Processing Systems (NIPS)  volume 20  pages 657–664  2008.

In Neural

[14] P.D. Hoff  A.E. Raftery  and M.S. Handcock. Latent space approaches to social network analysis. Journal

of the American Statistical Association  97(460):1090–1098  2002.

[15] D.N. Hoover. Relations on probability spaces and arrays of random variables. Preprint  Institute for

Advanced Study  Princeton  NJ  1979.

[16] O. Kallenberg. On the representation theorem for exchangeable arrays. Journal of Multivariate Analysis 

30(1):137–154  1989.

[17] R.H. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries. IEEE Trans. Information

Theory  56:2980–2998  Jun. 2010.

[18] N.D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent vari-

able models. Journal of Machine Learning Research  6:1783–1816  2005.

[19] J.R. Lloyd  P. Orbanz  Z. Ghahramani  and D.M. Roy. Random function priors for exchangeable arrays
with applications to graphs and relational data. In Neural Information Processing Systems (NIPS)  2012.
[20] L. Lov´asz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory  Series B 

96:933–957  2006.

[21] K.T. Miller  T.L. Grifﬁths  and M.I. Jordan. Nonparametric latent fature models for link prediction. In

Neural Information Processing Systems (NIPS)  2009.

[22] K. Nowicki and T.A. Snijders. Estimation and prediction of stochastic block structures. Journal of

American Statistical Association  96:1077–1087  2001.

[23] P. Orbanz and D.M. Roy. Bayesian models of graphs  arrays and other exchangeable random structures 

2013. Unpublished manuscript.

[24] P.Latouche and S. Robin. Bayesian model averaging of stochastic block models to estimate the graphon
function and motif frequencies in a w-graph model. ArXiv:1310.6150  October 2013. Unpublished
manuscript.

[25] K. Rohe  S. Chatterjee  and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel.

Annals of Statistics  39(4):1878–1915  2011.

[26] M. Tang  D.L. Sussman  and C.E. Priebe. Universally consistent vertex classiﬁcation for latent positions

graphs. Annals of Statistics  2013. In press.

[27] L. Wasserman. All of Nonparametric Statistics. Springer  2005.
[28] P.J. Wolfe and S.C. Olhede. Nonparametric graphon estimation. ArXiv:1309.5936  September 2013.

Unpublished manuscript.

[29] Z. Xu  F. Yan  and Y. Qi. Inﬁnite Tucker decomposition: nonparametric Bayesian models for multiway

data analysis. In Proc. Intl. Conf. Machine Learning (ICML)  2012.

[30] Y. Zhao  E. Levina  and J. Zhu. Community extraction for social networks. In Proc. Natl. Acad. Sci. USA 

volume 108  pages 7321–7326  2011.

9

,Edo Airoldi
Thiago Costa
Stanley Chan