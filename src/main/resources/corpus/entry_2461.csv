2011,High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions,We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efficient threshold-based algorithm   for structure estimation based known as  conditional mutual information test. This simple local algorithm    requires only low-order statistics of the data and decides    whether  two nodes   are neighbors in the unknown graph. Under some transparent assumptions  we establish that the proposed algorithm is structurally consistent (or sparsistent)  when the number of samples scales as n= Omega(J_{min}^{-4} log p)  where p is the number of nodes and J_{min} is the minimum edge potential.  We also prove novel non-asymptotic necessary conditions for graphical model selection.,High-Dimensional Graphical Model Selection:

Tractable Graph Families and Necessary Conditions

Anima Anandkumar

Dept. of EECS 

Univ. of California
Irvine  CA  92697

a.anandkumar@uci.edu

Vincent Y.F. Tan

Dept. of ECE 

Univ. of Wisconsin
Madison  WI  53706.
vtan@wisc.edu

Alan S. Willsky
Dept. of EECS

Massachusetts Inst. of Technology 

Cambridge  MA  02139.
willsky@mit.edu

Abstract

We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples
from the model. We propose an efﬁcient threshold-based algorithm for structure estimation based
on conditional mutual information thresholding. This simple local algorithm requires only low-
order statistics of the data and decides whether two nodes are neighbors in the unknown graph.
We identify graph families for which the proposed algorithm has low sample and computational
complexities. Under some transparent assumptions  we establish that the proposed algorithm is
structurally consistent (or sparsistent) when the number of samples scales as n = Ω(J −4
min log p) 
where p is the number of nodes and Jmin is the minimum edge potential. We also develop novel
non-asymptotic techniques for obtaining necessary conditions for graphical model selection.

Keywords: Graphical model selection  high-dimensional learning  local-separation property  necessary conditions 
typical sets  Fano’s inequality.

1 Introduction

The formalism of probabilistic graphical models can be employed to represent dependencies among a large set of
random variables in the form of a graph [1]. An important challenge in the study of graphical models is to learn
the unknown graph using samples drawn from the graphical model. The general structure estimation problem is
NP-hard [2]. In the high-dimensional regime  structure estimation is even more difﬁcult since the number of available
observations is typically much smaller than the number of dimensions (or variables). One of the goals is to characterize
tractable model classes for which consistent graphical model selection can be guaranteed with low computational and
sample complexities.

The seminal work by Chow and Liu [3] proposed an efﬁcient algorithm for maximum-likelihood structure estimation
in tree-structured graphical models by reducing the problem to a maximum weight spanning tree problem. A more
recent approach for efﬁcient structure estimation is based on convex-relaxation [4–6]. The success of such methods
typically requires certain “incoherence” conditions to hold. However  these conditions are NP-hard to verify for
general graphical models.

We adopt an alternative paradigm in this paper and instead analyze a simple local algorithm which requires only
low-order statistics of the data and makes decisions on whether two nodes are neighbors in the unknown graph. We
characterize the class of Ising and Gaussian graphical models for which we can guarantee efﬁcient and consistent
structure estimation using this simple algorithm. The class of graphs is based on a local-separation property and
includes many well-known random graph families  including locally-tree like graphs such as large girth graphs  the
Erd˝os-R´enyi random graphs [7] and power-law graphs [8]  as well as graphs with short cycles such as bounded-degree
graphs  and small-world graphs [9]. These graphs are especially relevant in modeling social networks [10  11].

1

1.1 Summary of Results

We propose an algorithm for structure estimation  termed as conditional mutual information thresholding (CMIT) 
which computes the minimum empirical conditional mutual information of a given node pair over conditioning sets
of bounded cardinality η. If the minimum exceeds a given threshold (depending on the number of samples n and the
number of nodes p)  the node pair is declared as an edge. This test has a low computational complexity of O(pη+2)
and requires only low-order statistics (up to order η + 2) when η is small. The parameter η is an upper bound on the
size of local vertex-separators in the graph  and is small for many common graph families  as discussed earlier. We
establish that under a set of mild and transparent assumptions  structure learning is consistent in high-dimensions for
CMIT when the number of samples scales as n = Ω(J −4
min log p)  for a p-node graph  where Jmin is the minimum
(absolute) edge-potential in the model.

We also develop novel techniques to obtain necessary conditions for consistent structure estimation of Erd˝os-R´enyi
random graphs. We obtain non-asymptotic bounds on the number of samples n in terms of the expected degree and
the number of nodes of the model. The techniques employed are information-theoretic in nature and combine the use
of Fano’s inequality and the so-called asymptotic equipartition property.

Our results have many ramiﬁcations: we explicitly characterize the tradeoff between various graph parameters such
as the maximum degree  girth and the strength of edge potentials for efﬁcient and consistent structure estimation. We
draw connections between structure learning and the statistical physical properties of the model: learning is fundamen-
tally related to the absence of long-range dependencies in the model  i.e.  the regime of correlation decay. The notion
of correlation decay on Ising models has been extensively characterized [12]  but its connections to structure learning
have only been explored in a few recent works (e.g.  [13]). This work establishes that consistent structure learning is
feasible under a slightly weaker condition than the usual notion of correlation decay for a rich class of graphs. More-
over  we show that the Gaussian analog of correlation decay is the so-called walk-summability condition [14]. This is
a somewhat unexpected and surprising connection since walk-summability is a condition to characterize the perfor-
mance of inference algorithms such as loopy belief propagation (LBP). Our work demonstrates that both successful
inference and learning hinge on similar properties of the Gaussian graphical model.

2 Preliminaries

2.1 Graphical Models

A p-dimensional graphical model is a family of p-dimensional multivariate distributions Markov on some undirected
graph G=(V  E) [1]. Each node in the graph i ∈ V is associated to a random variable Xi taking values in a set X . We
consider both discrete (in particular Ising) models where X is a ﬁnite set and Gaussian models where X = R. The set
of edges E captures the set of conditional-independence relationships among the random variables. More speciﬁcally 
the vector of random variables X := (X1  . . .   Xp) with joint distribution P satisﬁes the global Markov property with
respect to a graph G  if for all disjoint sets A  B ⊂ V   we have

P (xA  xB|xS) = P (xA|xS)P (xB|xS).

(1)

where set S is a separator1between A and B. The Hammersley-Clifford theorem states that under the positivity
condition  given by P (x) > 0 for all x ∈ X p [1]  the model P satisﬁes the global Markov property according to a
graph G if and only if it factorizes according to the cliques of G.
We consider the class of Ising models  i.e.  binary pairwise models which factorize according to the edges of the graph.
More precisely  the probability mass function (pmf) of an Ising model is

P (x) ∝ exp(cid:20) 1

2

xT JGx + hT x(cid:21)   x ∈ {−1  1}p.

For Gaussian graphical models  the probability density function (pdf) is of the form 

f (x) ∝ exp(cid:20)−

xT JGx + hT x(cid:21)   x ∈ Rp.

1
2

(2)

(3)

1A set S ⊂ V is a separator of sets A and B if the removal of nodes in S separates A and B into distinct components.

2

In both the cases  the matrix JG is called the potential or information matrix and h  the potential vector. For both Ising
and Gaussian models  the sparsity pattern of the matrix JG corresponds to that of the graph G  i.e.  JG(i  j) = 0 if and
only if (i  j) /∈ G.
We assume that the potentials are uniformly bounded above and below as:

Jmin ≤ |JG(i  j)| ≤ Jmax 

∀ (i  j) ∈ G.

(4)

Our results on structure learning depend on Jmin and Jmax  which is fairly natural – intuitively  models with edge
potentials which are “too small” or “too large” are harder to learn than those with comparable potentials  i.e.  homoge-
nous models.

Notice that the conventional parameterizations for the Ising models in (2) and the Gaussian models in (3) are slightly
different. Without loss of generality  for Ising model  we assume that J(i  i) = 0 for all i ∈ V . On the other hand  in
the Gaussian setting  we assume that the diagonal elements of the inverse covariance (or information) matrix JG are
normalized to unity (J(i  i) = 1  i ∈ V )  and that JG can be decomposed as JG = I − RG  where RG is the matrix
of partial correlation coefﬁcients [14].

We consider the problem of structure learning  which involves the estimation of the edge set of the graph G given n
i.i.d. samples X1  . . .   Xn drawn either from the Ising model in (2) or the Gaussian model in (3). We consider the
high-dimensional regime  where both p and n grow simultaneously; typically  the growth of p is much faster than that
of n.

2.2 Tractable Graph Families

We consider the class of graphical models Markov on a graph Gp belonging to some ensemble G(p) of graphs with
p nodes. We emphasize that in our formulation the graph ensemble G(p) can either be deterministic or random
– in the latter  we also specify a probability measure over the set of graphs in G(p).
In the random setting  we
use the term almost every (a.e.) graph G ∼ G(p) satisﬁes a certain property Q (for example  connectedness) if
limp→∞ P [Gp satisﬁes Q] = 1. In other words  the property Q holds asymptotically almost surely2 (a.a.s.) with
respect to the random graph ensemble G(p). Intuitively  this means that graphs that have a vanishing probability of
occurrence as p → ∞ are ignored. Our conditions and theoretical guarantees will be based on this notion for random
graph ensembles.
We now characterize the ensemble of graphs amenable for consistent structure estimation. For γ ∈ N  let Bγ(i; G)
denote the set of vertices within distance γ from node i with respect to graph G. Let Hγ i := G(Bγ (i; G)) de-
note the subgraph of G spanned by Bγ(i; G)  but in addition  we retain the nodes not in Bγ(i; G) (and remove the
corresponding edges).

Deﬁnition 1 (γ-Local Separator) Given a graph G  a γ-local separator Sγ(i  j) between i and j  for (i  j) /∈ G  is a
minimal vertex separator3 with respect to the subgraph Hγ i. The parameter γ is referred to as the path threshold for
local separation.

In other words  the γ-local separator Sγ(i  j) separates nodes i and j with respect to paths in G of length at most γ.
We now characterize the ensemble of graphs based on the size of local separators.

Deﬁnition 2 ((η  γ)-Local Separation Property) An ensemble of graphs G(p; η  γ) satisﬁes (η  γ)-local separation
property if for a.e. Gp ∈ G(p; η  γ) 

(5)

(i j) /∈Gp |Sγ(i  j)| ≤ η.
max

In Section 3  we propose an efﬁcient algorithm for graphical model selection when the underlying graph belongs to a
graph ensemble G(p; η  γ) with sparse local node separators (i.e.  with small η). Below we provide examples of three
graph families which satisfy (5) for small η.

2Note that the term a.a.s. does not apply to deterministic graph ensembles G(p) where no randomness is assumed  and in this

setting  we assume that the property Q holds for every graph in the ensemble.

3A minimal separator is a separator of smallest cardinality.

3

(Example 1) Bounded Degree: Any (deterministic or random) ensemble of degree-bounded graphs GDeg(p  ∆)
satisﬁes the (η  γ)-local separation property with η = ∆ and every γ ∈ N. Thus  our algorithm consistently recovers
graphs with small (bounded) degrees (∆ = O(1)). This case was considered previously in several works  e.g. [15 16].
(Example 2) Bounded Local Paths: The (η  γ)-local separation property also holds when there are at most η paths
of length at most γ in G between any two nodes (henceforth  termed as the (η  γ)-local paths property). In other
words  there are at most η − 1 overlapping4 cycles of length smaller than 2γ. Thus  a graph with girth g (length of
the shortest cycle) satisﬁes the (η  γ)-local separation property with η = 1 and γ = g. For example  the bipartite
Ramanujan graph [17  p. 107] and the random Cayley graphs [18] have large girths. The girth condition can be
weakened to allow for a small number of short cycles  while not allowing for overlapping cycles. Such graphs are
termed as locally tree-like. For instance  the ensemble of Erd˝os-R´enyi graphs GER(p  c/p)  where an edge between
any node pair appears with a probability c/p  independent of other node pairs  is locally tree-like. It can be shown
that GER(p  c/p) satisﬁes (η  γ)-local separation property with η = 2 and γ ≤ log p
4 log c a.a.s. Similar observations apply
for the more general scale-free or power-law graphs [8  19]. Along similar lines  the ensemble of ∆-random regular
graphs  denoted by GReg(p  ∆)  which is the uniform ensemble of regular graphs with degree ∆ has no overlapping
cycles of length at most Θ(log∆−1 p) a.a.s. [20  Lemma 1].
(Example 3) Small-World Graphs: The class of hybrid graphs or augmented graphs [8  Ch. 12] consist of graphs
which are the union of two graphs: a “local” graph having short cycles and a “global” graph having small average
distances between nodes. Since the hybrid graph is the union of these local and global graphs  it simultaneously has
large degrees and short cycles. The simplest model GWatts(p  d  c/p)  ﬁrst studied by Watts and Strogatz [9]  consists
of the union of a d-dimensional grid and an Erd˝os-R´enyi random graph with parameter c. One can check that a.e.
graph G ∼ GWatts(p  d  c/p) satisﬁes (η  γ)-local separation property in (5)  with η = d + 2 and γ ≤ log p
4 log c . Similar
observations apply for more general hybrid graphs studied in [8  Ch. 12].

3 Method and Guarantees

3.1 Assumptions

(A1) Scaling Requirements: We consider the asymptotic setting where both the number of variables (nodes) p
and the number of samples n go to inﬁnity. We assume that the parameters (n  p  Jmin) scale in the following
fashion:5

(6)
We require that the number of nodes p → ∞ to exploit the local separation properties of the class of graphs
under consideration.
(A2a) Strict Walk-summability for Gaussian Models: The Gaussian graphical model Markov on almost every

min log p).

n = ω(J −4

Gp ∼ G(p) is α-walk summable  i.e. 

(7)
where α is a constant (i.e.  is not a function of p)  RGp := [|RGp (i  j)|] is the entry-wise absolute value of
the partial correlation matrix RGp. In addition  k·k denotes the spectral norm  which for symmetric matrices 
is given by the maximum absolute eigenvalue.
(A2b) Bounded Potentials for Ising Models: The Ising model Markov on a.e. Gp ∼ G(p) has its maximum

kRGpk ≤ α < 1 

absolute potential below a threshold J ∗. More precisely 

Furthermore  the ratio α in (8) is not a function of p. See [21  22] for an explicit characterization of J ∗ for
speciﬁc graph ensembles.

α :=

tanh Jmax
tanh J ∗ < 1.

(8)

(A3) Local-Separation Property: We assume that the ensemble of graphs G(p; η  γ) satisﬁes the (η  γ)-local

separation property with η  γ ∈ N satisfying:

4Two cycles are said to overlap if they have common vertices.
5The notations ω(·)  Ω(·)  o(·) and O(·) refer to asymptotics as the number of variables p → ∞.

η = O(1)  Jminα−γ = eω(1) 

(9)

4

where α is given by (7) for Gaussian models and by (8) for Ising models.6 We can weaken the second re-
quirement in (9) as Jminα−γ = ω(1) for deterministic graph families (rather than random graph ensembles).
(A4) Edge Potentials: The edge potentials {Ji j  (i  j) ∈ G} of the Ising model are assumed to be generically
drawn from [−Jmax −Jmin] ∪ [Jmin  Jmax]  i.e.  our results hold except for a set of Lebesgue measure zero.
We also characterize speciﬁc classes of models where this assumption can be removed and we allow for all
choices of edge potentials. See [21  22] for details.

The above assumptions are very general and hold for a rich class of models. Assumption (A1) stipulates the scaling
requirements of number of samples for consistent structure estimation. Assumption (A2) and (A4) impose constraints
on the model parameters. Assumption (A3) requires the local-separation property described in Section 2.2 with the
path threshold γ satisfying (9). We provide examples of graphs where the above assumptions are met.
Gaussian Models on Girth-bounded Graphs: Consider the ensemble of graphs GDeg Girth(p; ∆  g) with maximum
degree ∆ and girth g. We now derive a relationship between ∆ and g  for the above assumptions to hold. It can be
established that for the walk-summability condition in (A2a) to hold for Gaussian models  we require that Jmax =
O(1/∆). When the minimum edge potential achieves this bound (Jmin = Θ(1/∆))  a sufﬁcient condition for (A3) to
hold is given by

(10)
In (10)  we notice a natural tradeoff between the girth and the maximum degree of the graph ensemble for successful
estimation under our framework: graphs with large degrees can be learned efﬁciently if their girths are large. Indeed  in
the extreme case of trees which have inﬁnite girth  in accordance with (10)  there is no constraint on the node degrees
for consistent graphical model selection and recall that the Chow-Liu algorithm [3] is an efﬁcient method for model
selection on tree-structured graphical models.

∆αg = o(1).

Note that the condition in (10) allows for the maximum degree bound ∆ to grow with the number of nodes as long as
the girth g also grows appropriately. For example  if the maximum degree scales as ∆ = O(poly(log p)) and the girth
scales as g = O(log log p)  then (10) is satisﬁed. This implies that graphs with fairly large degrees and short cycles
can be recovered successfully consistently using the algorithm in Section 3.2.
Gaussian Models on Erd˝os-R´enyi and Small-World Graphs: We can also conclude that a.e. Erd˝os-R´enyi graph
G ∼ GER(p  c/p) satisﬁes (9) with η = 2 when c = O(poly(log p)) under the best possible scaling for Jmin subject
to the walk-summability constraint in (7). Similarly  the small-world ensemble GWatts(p  d  c/p) satisﬁes (9) with
η = d + 2  when d = O(1) and c = O(poly(log p)).
Ising Models: For Ising models  the best possible scaling of the minimum edge potential Jmin is when Jmin = Θ(J ∗) 
for the threshold J ∗ deﬁned in (8). For the ensemble of graphs GDeg Girth(p; ∆  g) with degree ∆ and girth g  we can
establish that J ∗ = Θ(1/∆). When the minimum edge potential achieves the threshold  i.e.  Jmin = Θ(1/∆)  we
end up with a similar requirement as in (10) for Gaussian models. Similarly  for both the Erd˝os-R´enyi graph ensemble
GER(p  c/p) and small-world ensemble GWatts(p  d  c/p)  we can establish that the threshold J ∗ = Θ(1/c)  and thus 
the observations made for the Gaussian setting hold for the Ising model as well.

3.2 Conditional Mutual Information Threshold Test

Our structure learning procedure is known as the Conditional Mutual Information Threshold Test (CMIT). Let
CMIT(xn; ξn p  η) be the output edge set from CMIT given n i.i.d. samples xn  a threshold ξn p and a constant
η ∈ N. The conditional mutual information test proceeds as follows: one computes the empirical conditional mutual
information7 for each node pair (i  j) ∈ V 2 and ﬁnds the conditioning set which achieves the minimum  over all
subsets of cardinality at most η 
(11)

min

S⊂V \{i j} |S|≤ηbI(Xi; Xj|XS) 

where bI(Xi; Xj|XS) denotes the empirical conditional mutual information of Xi and Xj given XS. If the above
minimum value exceeds the given threshold ξn p  then the node pair is declared as an edge. Recall that the conditional
mutual information I(Xi; Xj|XS) = 0 iff given XS  the random variables Xi and Xj are conditionally independent.

6We say that two sequences f (p)  g(p) satisfy f (p) = eω(g(p))  if
7The empirical conditional mutual information is obtained by ﬁrst computing the empirical distribution and then computing its

→ ∞ as p → ∞.

g(p) log p

f (p)

conditional mutual information.

5

Thus  (11) seeks to identify non-neighbors  i.e.  node pairs which can be separated in the unknown graph G. However 
since we constrain the conditioning set |S| ≤ η in (11)  the optimal conditioning set may not form an exact separator.
Despite this restriction  we establish that the above test can correctly classify the edges and non-neighbors using a
suitable threshold ξn p subject to the assumptions (A1)–(A4). The threshold ξn p is chosen as a function of the number
of nodes p  the number of samples n  and the minimum edge potential Jmin as follows:

ξn p = O(J 2

min)  ξn p = ω(α2γ)  ξn p = Ω(cid:18) log p
n (cid:19)  

where γ is the path-threshold in (5) for (η  γ)-local separation to hold and α is given by (7) and (8). The computational
complexity of the CMIT algorithm is O(pη+2). Thus the algorithm is computationally efﬁcient for small η. Moreover 
the algorithm only uses statistics of order η + 2 in contrast to the convex-relaxation approaches [4–6] which typically
use higher-order statistics.

(12)

(13)

Theorem 1 (Structural consistency of CMIT) Assume that (A1)-(A4) hold. Given a Gaussian graphical model or
an Ising model Markov on a graph Gp ∼ G(p; η  γ)  CMIT(xn; ξn p  η) is structurally consistent. In other words 

lim

n p→∞

P [CMIT ({xn}; ξn p  η) 6= Gp] = 0.

Consistency guarantee The CMIT algorithm consistently recovers the structure of the graphical models with prob-
ability tending to one and the probability measure in (4) is with respect to both the graph and the samples.

Sample-complexity The sample complexity of the CMIT scales as Ω(J −4
min log p) and is favorable when the mini-
mum edge potential Jmin is large. This is intuitive since the edges have stronger potentials when Jmin is large. On the
other hand  Jmin cannot be arbitrarily large due to the assumption (A2). The minimum sample complexity is attained
when Jmin achieves this upper bound.
It can be established that for both Gaussian and Ising models Markov on a degree-bounded graph ensemble
GDeg(p  ∆) with maximum degree ∆ and satisfying assumption (A3)  the minimum sample complexity is given by
n = Ω(∆4 log p) i.e.  when Jmin = Θ(1/∆).
We can have improved guarantees for the Erd˝os-R´enyi random graphs GER(p  c/p).
In the Gaussian setting  the
minimum sample complexity can be improved to n = Ω(∆2 log p)  i.e.  when Jmin = Θ(1/√∆) where the maximum
degree scales as ∆ = Θ(log p log c) [7].
On the other hand  for Ising models  the minimum sample complexity can be further improved to n = Ω(c4 log p) 
i.e.  when Jmin = Θ(J ∗) = Θ(1/c). Note that c/2 is the expected degree of the GER(p  c/p) ensemble. Speciﬁcally 
when the Erd˝os-R´enyi random graphs have a bounded average degree (c = O(1))  we can obtain a minimum sample
complexity of n = Ω(log p) for structure estimation of Ising models. Recall that the sample complexity of learning
tree models is Ω(log p) [23]. Thus  the complexity of learning sparse Erd˝os-R´enyi random graphs is akin to learning
trees in certain parameter regimes.
The sample complexity of structure estimation can be improved to n = Ω(J −2
min log p) by employing empirical condi-
tional covariances for Gaussian models and empirical conditional variation distances in place of empirical conditional
mutual information. However  to present a uniﬁed framework for Gaussian and Ising models  we present the CMIT
here. See [21  22] for details.

Comparison with convex-relaxation approaches We now compare our approach for structure learning with
convex-relaxation methods. The work by Ravikumar et al. in [5] employs an `1-penalized likelihood estimator and un-
der the so-called incoherence conditions  the sample complexity is n = Ω((∆2 + J −2
min) log p). Our sample complexity
(using conditional covariances) n = Ω(J −2
min log p) is the same in terms of Jmin  while there is no explicit dependence
on the maximum degree ∆. Similarly  we match the neighborhood-based regression method by Meinshausen and
Buhlmann in [24] under more transparent conditions.
For structure estimation of Ising models  the work in [6] considers `1-penalized logistic regression which has a sample
complexity of n = Ω(∆3 log p) for a degree-bounded ensemble GDeg(p  ∆) satisfying certain “incoherence” condi-
tions. The sample complexity of CMIT  given by n = Ω(∆4 log p)  is slightly worse  while the modiﬁed algorithm
described previously has a sample complexity of n = Ω(∆2 log p)  for general degree-bounded ensembles. Addition-
ally  under the CMIT algorithm  we can guarantee an improved sample complexity of n = Ω(c4 log p) for Erd˝os-R´enyi

6

random graphs GER(p  c/p) and small-world graphs GWatts(p  d  c/p)  since the average degree c/2 is typically much
smaller than the maximum degree ∆. Moreover  note that  the incoherence conditions stated in [6] are NP-hard to
establish for general models since they involve the partition function of the model. In contrast  our conditions are
transparent and relate to the statistical-physical properties of the model. Moreover  our algorithm is local and requires
only low-order statistics  while the method in [6] requires full-order statistics.

Proof Outline We ﬁrst analyze the scenario when exact statistics are available. (i) We establish that for any two
non-neighbors (i  j) /∈ G  the minimum conditional mutual information in (11) (based on exact statistics) does not
exceed the threshold ξn p. (ii) Similarly  we also establish that the conditional mutual information in (11) exceeds the
threshold ξn p for all neighbors (i  j) ∈ G. (iii) We then extend these results to empirical versions using concentration
bounds. See [21  22] for details.

The main challenge in our proof is step (i). To this end  we analyze the conditional mutual information when the
conditioning set is a local separator between i and j and establish that it decays as p → ∞. The techniques involved to
establish this for Ising and Gaussian models are different: for Ising models  we employ the self-avoiding walk (SAW)
tree construction [25]. For Gaussian models  we use the techniques from walk-sum analysis [14].

4 Necessary Conditions for Model Selection

In the previous sections  we proposed and analyzed efﬁcient algorithms for learning the structure of graphical models.
We now derive the necessary conditions for consistent structure learning. We focus on the ensemble of Erd˝os-R´enyi
graphs GER(p  c/p).
For the class of degree-bounded graphs GDeg(p  ∆)  necessary conditions on sample complexity have been character-
ized previously [26] by considering a certain (restricted) set of ensembles. However  a na¨ıve application of such bounds
(based on Fano’s inequality [27  Ch. 2]) turns out to be too weak for the class of Erd˝os-R´enyi graphs GER(p  c/p).
We provide novel necessary conditions for structure learning of Erd˝os-R´enyi graphs. Our techniques may also be
applicable to other classes of random graphs.
Recall that a graph G is drawn from the ensemble of Erd˝os-R´enyi graphs GER(p  c/p). Given n i.i.d. samples Xn :=

to derive tight necessary conditions on the number of samples n (as a function of average degree c/2 and number of
nodes p) so that the probability of error P (p)
Again  note that the probability measure P is with respect to both the Erd˝os-R´enyi graph and the samples.

(X1  . . .   Xn) ∈ (X p)n  the task is to estimate G from Xn. Denote the estimated graph as bG := bG(Xn). It is desired
:= P (bG(Xn) 6= G) → 0 as the number of nodes p tends to inﬁnity.
Discrete Graphical Models Let Hb(q) := −q log2 q − (1 − q) log2(1 − q) be the binary entropy function. For
the Ising model  or more generally any discrete model where each random variable Xi ∈ X = {1  . . .  |X|}  we can
demonstrate the following:

e

Theorem 2 (Weak Converse for Discrete Models) For a discrete graphical model Markov on G ∼ GER(p  c/p)  if
P (p)
e → 0  it is necessary for n to satisfy
n ≥

2(cid:19)Hb(cid:18) c

p(cid:19) ≥

(14)

1

p log2 |X|(cid:18)p

c log2 p
2 log2 |X|

.

The above bound does not involve any asymptotic notation and shows transparently  how n has to depend on p  c
and |X| for consistent structure learning. Note that if the cardinality of the random variables |X| is large  then the
necessary sample complexity is small  which makes intuitive sense from a source-coding perspective. Moreover  the
above bound states that more samples are required as the average degree c/2 increases. Our bound involves only the
average degree c/2 and not the maximum degree of the graph  which is typically much larger than c [7].

Gaussian Graphical Models We now turn out attention to the Gaussian analogue of Theorem 2 under a similar
setup. We assume that the α-walk-summability condition in assumption (A2a) holds. We are then able to demonstrate
the following:

7

Theorem 3 (Weak Converse for Gaussian Models) For an α-walk summable Gaussian graphical model Markov on
G ∼ GER(p  c/p) as p → ∞  if P (p)

e → 0  we require

2

1−α + 1(cid:17)i(cid:18)p

2(cid:19)Hb(cid:18) c

p(cid:19) ≥

n ≥

p log2h2πe(cid:16) 1

c log2 p

log2h2πe(cid:16) 1

1−α + 1(cid:17)i .

(15)

As with Theorem 2  the above bound does not involve any asymptotic notation and similar intuitions hold as before.
There is a natural logarithmic dependence on p and a linear dependence on the average degree parameter c. Finally  the
dependence on α can be explained as follows: any α-walk-summable model is also β-walk-summable for all β > α.
Thus  the class of β-walk-summable models contains the class of α-walk-summable models. This results in a looser
bound in (15) for large α.

Analysis tools Our analysis tools are information-theoretic in nature. A common tool to derive necessary conditions
is to resort to Fano’s inequality [27  Ch. 2]  which (lower) bounds the probability of error P (p)
as a function of the
conditional entropy H(G|Xn) and the size of the set of all graphs with p nodes. However  a na¨ıve application of
Fano’s inequality results in a trivial lower bound as the set of all graphs  which can be realized by GER(p  c/p) is “too
large”.

e

To ameliorate this problem  we focus our attention on the typical graphs for applying Fano’s inequality and not all
graphs. The set of typical graphs has a small cardinality but high probability when p is large. The novelty of our proof
lies in our use of both typicality as well as Fano’s inequality to derive necessary conditions for structure learning. We
can show that (i) the probability of the typical set tends to one as p → ∞  (ii) the graphs in the typical set are almost
uniformly distributed (the asymptotic equipartition property)  (iii) the cardinality of the typical set is small relative to
the set of all graphs. These properties are used to prove Theorems 2 and 3.

5 Conclusion

In this paper  we adopted a novel and a uniﬁed paradigm for graphical model selection. We presented a simple local
algorithm for structure estimation with low computational and sample complexities under a set of mild and transparent
conditions. This algorithm succeeds on a wide range of graph ensembles such as the Erd˝os-R´enyi ensemble  small-
world networks etc. We also employed novel information-theoretic techniques for establishing necessary conditions
for graphical model selection.

Acknowledgement

The ﬁrst author is supported by the setup funds at UCI and in part by the AFOSR under Grant FA9550-10-1-0310  the
second author is supported by A*STAR  Singapore and the third author is supported in part by AFOSR under Grant
FA9550-08-1-1080.

References

[1] S. Lauritzen  Graphical models: Clarendon Press. Clarendon Press  1996.
[2] D. Karger and N. Srebro  “Learning Markov Networks: Maximum Bounded Tree-width Graphs ” in Proc. of

ACM-SIAM symposium on Discrete algorithms  2001  pp. 392–401.

[3] C. Chow and C. Liu  “Approximating Discrete Probability Distributions with Dependence Trees ” IEEE Tran. on

Information Theory  vol. 14  no. 3  pp. 462–467  1968.

[4] A. d’Aspremont  O. Banerjee  and L. El Ghaoui  “First-order methods for sparse covariance selection ” SIAM. J.

Matrix Anal. & Appl.  vol. 30  no. 56  2008.

[5] P. Ravikumar  M. Wainwright  G. Raskutti  and B. Yu  “High-dimensional covariance estimation by minimizing

`1-penalized log-determinant divergence ” Arxiv preprint arXiv:0811.3628  2008.

[6] P. Ravikumar  M. Wainwright  and J. Lafferty  “High-dimensional Ising Model Selection Using l1-Regularized

Logistic Regression ” Annals of Statistics  2008.

8

[7] B. Bollob´as  Random Graphs. Academic Press  1985.
[8] F. Chung and L. Lu  Complex graphs and network. Amer. Mathematical Society  2006.
[9] D. Watts and S. Strogatz  “Collective dynamics of small-worldnetworks ” Nature  vol. 393  no. 6684  pp. 440–

442  1998.

[10] M. Newman  D. Watts  and S. Strogatz  “Random graph models of social networks ” Proc. of the National

Academy of Sciences of the United States of America  vol. 99  no. Suppl 1  2002.

[11] R. Albert and A. Barab´asi  “Statistical mechanics of complex networks ” Reviews of modern physics  vol. 74 

no. 1  p. 47  2002.

[12] H. Georgii  Gibbs Measures and Phase Transitions. Walter de Gruyter  1988.
[13] J. Bento and A. Montanari  “Which Graphical Models are Difﬁcult to Learn?” in Proc. of Neural Information

Processing Systems (NIPS)  Vancouver  Canada  Dec. 2009.

[14] D. Malioutov  J. Johnson  and A. Willsky  “Walk-Sums and Belief Propagation in Gaussian Graphical Models ”

J. of Machine Learning Research  vol. 7  pp. 2031–2064  2006.

[15] G. Bresler  E. Mossel  and A. Sly  “Reconstruction of Markov Random Fields from Samples: Some Observations
and Algorithms ” in Intl. workshop APPROX Approximation  Randomization and Combinatorial Optimization.
Springer  2008  pp. 343–356.

[16] P. Netrapalli  S. Banerjee  S. Sanghavi  and S. Shakkottai  “Greedy Learning of Markov Network Structure  ” in

Proc. of Allerton Conf. on Communication  Control and Computing  Monticello  USA  Sept. 2010.

[17] F. Chung  Spectral graph theory. Amer Mathematical Society  1997.
[18] A. Gamburd  S. Hoory  M. Shahshahani  A. Shalev  and B. Virag  “On the girth of random cayley graphs ”

Random Structures & Algorithms  vol. 35  no. 1  pp. 100–117  2009.

[19] S. Dommers  C. Giardin`a  and R. van der Hofstad  “Ising models on power-law random graphs ” Journal of

Statistical Physics  pp. 1–23  2010.

[20] B. McKay  N. Wormald  and B. Wysocka  “Short cycles in random regular graphs ” The Electronic Journal of

Combinatorics  vol. 11  no. R66  p. 1  2004.

[21] A. Anandkumar  V. Y. F. Tan  and A. S. Willsky  “High-Dimensional Structure Learning of Ising Models:

Tractable Graph Families ” Preprint  Available on ArXiv 1107.1736  June 2011.

[22] ——  “High-Dimensional Gaussian Graphical Model Selection: Tractable Graph Families ” Preprint  ArXiv

1107.1270  June 2011.

[23] V. Tan  A. Anandkumar  and A. Willsky  “Learning Markov Forest Models: Analysis of Error Rates ” J. of

Machine Learning Research  vol. 12  pp. 1617–1653  May 2011.

[24] N. Meinshausen and P. Buehlmann  “High Dimensional Graphs and Variable Selection With the Lasso ” Annals

of Statistics  vol. 34  no. 3  pp. 1436–1462  2006.

[25] D. Weitz  “Counting independent sets up to the tree threshold ” in Proc. of ACM symp. on Theory of computing 

2006  pp. 140–149.

[26] W. Wang  M. Wainwright  and K. Ramchandran  “Information-theoretic bounds on model selection for Gaussian
Markov random ﬁelds ” in IEEE International Symposium on Information Theory Proceedings (ISIT)  Austin 
Tx  June 2010.

[27] T. Cover and J. Thomas  Elements of Information Theory.

John Wiley & Sons  Inc.  2006.

9

,Ilya Tolstikhin
Bharath Sriperumbudur
Bernhard Schölkopf