2019,Exact Rate-Distortion in Autoencoders via Echo Noise,Compression is at the heart of effective representation learning. However  lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability  and the limitations this imposes on learning are largely unexplored.  Further  the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general.  We introduce a new noise channel  Echo noise  that admits a simple  exact expression for mutual information for arbitrary input distributions.  The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions.  With its complex encoding mechanism and exact rate regularization  Echo leads to improved bounds on log-likelihood and dominates beta-VAEs across the achievable range of rate-distortion trade-offs. Further  we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations.,Exact Rate-Distortion in Autoencoders via Echo Noise

Rob Brekelmans  Daniel Moyer  Aram Galstyan  Greg Ver Steeg

Information Sciences Institute

University of Southern California

brekelma  moyerd@usc.edu; galstyan  gregv@isi.edu

Marina del Rey  CA 90292

Abstract

Compression is at the heart of effective representation learning. However  lossy
compression is typically achieved through simple parametric models like Gaussian
noise to preserve analytic tractability  and the limitations this imposes on learning
are largely unexplored. Further  the Gaussian prior assumptions in models such as
variational autoencoders (VAEs) provide only an upper bound on the compression
rate in general. We introduce a new noise channel  Echo noise  that admits a simple 
exact expression for mutual information for arbitrary input distributions. The noise
is constructed in a data-driven fashion that does not require restrictive distributional
assumptions. With its complex encoding mechanism and exact rate regularization 
Echo leads to improved bounds on log-likelihood and dominates -VAEs across the
achievable range of rate-distortion trade-offs. Further  we show that Echo noise can
outperform ﬂow-based methods without the need to train additional distributional
transformations.

1

Introduction

Rate-distortion theory provides an organizing principle for representation learning that is enshrined
in machine learning as the Information Bottleneck principle [39]. The goal is to compress input
random variables X into a representation Z with mutual information rate I(X; Z)  while minimizing
a distortion measure that captures our ability to use the representation for a task. For the rate to be
restricted  some information must be lost through noise. Despite the use of increasingly complex
encoding functions via neural networks  simple noise models like Gaussians still dominate the
literature because of their analytic tractability. Unfortunately  the effect of these assumptions on the
quality of learned representations is not well understood.
The Variational Autoencoding (VAE) framework [21  36] has provided the basis for a number of
recent developments in representation learning [1  10  11  18  20  41]. While VAEs were originally
motivated as performing posterior inference under a generative model  several recent works have
viewed the Evidence Lower Bound objective as corresponding to an unsupervised rate-distortion
problem [1  3  35]. From this perspective  reconstruction of the input provides the distortion measure 
while the KL divergence between encoder and prior gives an upper bound on the information rate
that depends heavily on the choice of prior [3  37  40].
In this work  we deconstruct this interpretation of VAEs and their extensions. Do the restrictive
assumptions of the Gaussian noise model limit the quality of VAE representations? Does forcing
the latent space to be independent and Gaussian constrain the expressivity of our models? We ﬁnd
evidence to support both claims  showing that a powerful noise model can achieve more efﬁcient
lossy compression and that relaxing prior or marginal assumptions can lead to better bounds on both
the information rate and log-likelihood.
The main contribution of this paper is the introduction of the Echo noise channel  a powerful  data-
driven improvement over Gaussian channels whose compression rate can be precisely expressed for

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Echo
Noise

I(Z; X) = 3 bits

<latexit sha1_base64="Gp2rws88UxFJYVH0peEDqMvePWM=">AAACAXicbVDLSgNBEJyNrxhfq14EL4NBiJewq4KCCEEveotgHpgsYXYySYbMY5mZFcMSL/6KFw+KePUvvPk3TpI9aGJBQ1HVTXdXGDGqjed9O5m5+YXFpexybmV1bX3D3dyqahkrTCpYMqnqIdKEUUEqhhpG6pEiiIeM1ML+5civ3ROlqRS3ZhCRgKOuoB2KkbFSy925LtydwfoBPIdHsMlD+ZDAkBo9bLl5r+iNAWeJn5I8SFFuuV/NtsQxJ8JghrRu+F5kggQpQzEjw1wz1iRCuI+6pGGpQJzoIBl/MIT7VmnDjlS2hIFj9fdEgrjWAx7aTo5MT097I/E/rxGbzmmQUBHFhgg8WdSJGTQSjuKAbaoINmxgCcKK2lsh7iGFsLGh5WwI/vTLs6R6WPS9on9znC9dpHFkwS7YAwXggxNQAlegDCoAg0fwDF7Bm/PkvDjvzsekNeOkM9vgD5zPH10AlO4=</latexit>
<latexit sha1_base64="Gp2rws88UxFJYVH0peEDqMvePWM=">AAACAXicbVDLSgNBEJyNrxhfq14EL4NBiJewq4KCCEEveotgHpgsYXYySYbMY5mZFcMSL/6KFw+KePUvvPk3TpI9aGJBQ1HVTXdXGDGqjed9O5m5+YXFpexybmV1bX3D3dyqahkrTCpYMqnqIdKEUUEqhhpG6pEiiIeM1ML+5civ3ROlqRS3ZhCRgKOuoB2KkbFSy925LtydwfoBPIdHsMlD+ZDAkBo9bLl5r+iNAWeJn5I8SFFuuV/NtsQxJ8JghrRu+F5kggQpQzEjw1wz1iRCuI+6pGGpQJzoIBl/MIT7VmnDjlS2hIFj9fdEgrjWAx7aTo5MT097I/E/rxGbzmmQUBHFhgg8WdSJGTQSjuKAbaoINmxgCcKK2lsh7iGFsLGh5WwI/vTLs6R6WPS9on9znC9dpHFkwS7YAwXggxNQAlegDCoAg0fwDF7Bm/PkvDjvzsekNeOkM9vgD5zPH10AlO4=</latexit>
<latexit sha1_base64="Gp2rws88UxFJYVH0peEDqMvePWM=">AAACAXicbVDLSgNBEJyNrxhfq14EL4NBiJewq4KCCEEveotgHpgsYXYySYbMY5mZFcMSL/6KFw+KePUvvPk3TpI9aGJBQ1HVTXdXGDGqjed9O5m5+YXFpexybmV1bX3D3dyqahkrTCpYMqnqIdKEUUEqhhpG6pEiiIeM1ML+5civ3ROlqRS3ZhCRgKOuoB2KkbFSy925LtydwfoBPIdHsMlD+ZDAkBo9bLl5r+iNAWeJn5I8SFFuuV/NtsQxJ8JghrRu+F5kggQpQzEjw1wz1iRCuI+6pGGpQJzoIBl/MIT7VmnDjlS2hIFj9fdEgrjWAx7aTo5MT097I/E/rxGbzmmQUBHFhgg8WdSJGTQSjuKAbaoINmxgCcKK2lsh7iGFsLGh5WwI/vTLs6R6WPS9on9znC9dpHFkwS7YAwXggxNQAlegDCoAg0fwDF7Bm/PkvDjvzsekNeOkM9vgD5zPH10AlO4=</latexit>
<latexit sha1_base64="Gp2rws88UxFJYVH0peEDqMvePWM=">AAACAXicbVDLSgNBEJyNrxhfq14EL4NBiJewq4KCCEEveotgHpgsYXYySYbMY5mZFcMSL/6KFw+KePUvvPk3TpI9aGJBQ1HVTXdXGDGqjed9O5m5+YXFpexybmV1bX3D3dyqahkrTCpYMqnqIdKEUUEqhhpG6pEiiIeM1ML+5civ3ROlqRS3ZhCRgKOuoB2KkbFSy925LtydwfoBPIdHsMlD+ZDAkBo9bLl5r+iNAWeJn5I8SFFuuV/NtsQxJ8JghrRu+F5kggQpQzEjw1wz1iRCuI+6pGGpQJzoIBl/MIT7VmnDjlS2hIFj9fdEgrjWAx7aTo5MT097I/E/rxGbzmmQUBHFhgg8WdSJGTQSjuKAbaoINmxgCcKK2lsh7iGFsLGh5WwI/vTLs6R6WPS9on9znC9dpHFkwS7YAwXggxNQAlegDCoAg0fwDF7Bm/PkvDjvzsekNeOkM9vgD5zPH10AlO4=</latexit>

+

Gaussian

Noise

=

=

q(x)

<latexit sha1_base64="2WgeRWylqqkwyLD6JK8it3eUrVI=">AAAB83icbVDLSgMxFL3xWeur6tJNsAh1U2ZE0GXRjcsK9gGdoWTSTBuayYxJRixDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJIJr4zjfaGV1bX1js7RV3t7Z3duvHBy2dZwqylo0FrHqBkQzwSVrGW4E6yaKkSgQrBOMb3K/88iU5rG8N5OE+REZSh5ySoyVvIeaFxEzCkL8dNavVJ26MwNeJm5BqlCg2a98eYOYphGThgqidc91EuNnRBlOBZuWvVSzhNAxGbKepZJETPvZLPMUn1plgMNY2ScNnqm/NzISaT2JAjuZJ9SLXi7+5/VSE175GZdJapik80NhKrCJcV4AHnDFqBETSwhV3GbFdEQUocbWVLYluItfXibt87rr1N27i2rjuqijBMdwAjVw4RIacAtNaAGFBJ7hFd5Qil7QO/qYj66gYucI/gB9/gArk5Ea</latexit>
<latexit sha1_base64="2WgeRWylqqkwyLD6JK8it3eUrVI=">AAAB83icbVDLSgMxFL3xWeur6tJNsAh1U2ZE0GXRjcsK9gGdoWTSTBuayYxJRixDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJIJr4zjfaGV1bX1js7RV3t7Z3duvHBy2dZwqylo0FrHqBkQzwSVrGW4E6yaKkSgQrBOMb3K/88iU5rG8N5OE+REZSh5ySoyVvIeaFxEzCkL8dNavVJ26MwNeJm5BqlCg2a98eYOYphGThgqidc91EuNnRBlOBZuWvVSzhNAxGbKepZJETPvZLPMUn1plgMNY2ScNnqm/NzISaT2JAjuZJ9SLXi7+5/VSE175GZdJapik80NhKrCJcV4AHnDFqBETSwhV3GbFdEQUocbWVLYluItfXibt87rr1N27i2rjuqijBMdwAjVw4RIacAtNaAGFBJ7hFd5Qil7QO/qYj66gYucI/gB9/gArk5Ea</latexit>
<latexit sha1_base64="2WgeRWylqqkwyLD6JK8it3eUrVI=">AAAB83icbVDLSgMxFL3xWeur6tJNsAh1U2ZE0GXRjcsK9gGdoWTSTBuayYxJRixDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJIJr4zjfaGV1bX1js7RV3t7Z3duvHBy2dZwqylo0FrHqBkQzwSVrGW4E6yaKkSgQrBOMb3K/88iU5rG8N5OE+REZSh5ySoyVvIeaFxEzCkL8dNavVJ26MwNeJm5BqlCg2a98eYOYphGThgqidc91EuNnRBlOBZuWvVSzhNAxGbKepZJETPvZLPMUn1plgMNY2ScNnqm/NzISaT2JAjuZJ9SLXi7+5/VSE175GZdJapik80NhKrCJcV4AHnDFqBETSwhV3GbFdEQUocbWVLYluItfXibt87rr1N27i2rjuqijBMdwAjVw4RIacAtNaAGFBJ7hFd5Qil7QO/qYj66gYucI/gB9/gArk5Ea</latexit>
<latexit sha1_base64="2WgeRWylqqkwyLD6JK8it3eUrVI=">AAAB83icbVDLSgMxFL3xWeur6tJNsAh1U2ZE0GXRjcsK9gGdoWTSTBuayYxJRixDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJIJr4zjfaGV1bX1js7RV3t7Z3duvHBy2dZwqylo0FrHqBkQzwSVrGW4E6yaKkSgQrBOMb3K/88iU5rG8N5OE+REZSh5ySoyVvIeaFxEzCkL8dNavVJ26MwNeJm5BqlCg2a98eYOYphGThgqidc91EuNnRBlOBZuWvVSzhNAxGbKepZJETPvZLPMUn1plgMNY2ScNnqm/NzISaT2JAjuZJ9SLXi7+5/VSE175GZdJapik80NhKrCJcV4AHnDFqBETSwhV3GbFdEQUocbWVLYluItfXibt87rr1N27i2rjuqijBMdwAjVw4RIacAtNaAGFBJ7hFd5Qil7QO/qYj66gYucI/gB9/gArk5Ea</latexit>

Input distribution

Noisy channel

<latexit sha1_base64="3sTAvva5UF6O0WoigMynSTaSlHA=">AAACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQm7CrgoJN0Ea7COaB2SXMTmaTITM768ysGJYUNv6KjYUitn6EnX/jJNlCEw9cOJxzL/feE8SMKu0431ZuYXFpeSW/Wlhb39jcsrd3GkokEpM6FkzIVoAUYTQidU01I61YEsQDRprB4GLsN++JVFREN3oYE5+jXkRDipE2UscuXpVvz2DrAHqM3MEj6PFAPKQwoFqNOnbJqTgTwHniZqQEMtQ69pfXFTjhJNKYIaXarhNrP0VSU8zIqOAlisQID1CPtA2NECfKTydPjOC+UbowFNJUpOFE/T2RIq7UkAemkyPdV7PeWPzPayc6PPVTGsWJJhGeLgoTBrWA40Rgl0qCNRsagrCk5laI+0girE1uBROCO/vyPGkcVlyn4l4fl6rnWRx5UAR7oAxccAKq4BLUQB1g8AiewSt4s56sF+vd+pi25qxsZhf8gfX5AwAflm0=</latexit>
<latexit sha1_base64="3sTAvva5UF6O0WoigMynSTaSlHA=">AAACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQm7CrgoJN0Ea7COaB2SXMTmaTITM768ysGJYUNv6KjYUitn6EnX/jJNlCEw9cOJxzL/feE8SMKu0431ZuYXFpeSW/Wlhb39jcsrd3GkokEpM6FkzIVoAUYTQidU01I61YEsQDRprB4GLsN++JVFREN3oYE5+jXkRDipE2UscuXpVvz2DrAHqM3MEj6PFAPKQwoFqNOnbJqTgTwHniZqQEMtQ69pfXFTjhJNKYIaXarhNrP0VSU8zIqOAlisQID1CPtA2NECfKTydPjOC+UbowFNJUpOFE/T2RIq7UkAemkyPdV7PeWPzPayc6PPVTGsWJJhGeLgoTBrWA40Rgl0qCNRsagrCk5laI+0girE1uBROCO/vyPGkcVlyn4l4fl6rnWRx5UAR7oAxccAKq4BLUQB1g8AiewSt4s56sF+vd+pi25qxsZhf8gfX5AwAflm0=</latexit>
<latexit sha1_base64="3sTAvva5UF6O0WoigMynSTaSlHA=">AAACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQm7CrgoJN0Ea7COaB2SXMTmaTITM768ysGJYUNv6KjYUitn6EnX/jJNlCEw9cOJxzL/feE8SMKu0431ZuYXFpeSW/Wlhb39jcsrd3GkokEpM6FkzIVoAUYTQidU01I61YEsQDRprB4GLsN++JVFREN3oYE5+jXkRDipE2UscuXpVvz2DrAHqM3MEj6PFAPKQwoFqNOnbJqTgTwHniZqQEMtQ69pfXFTjhJNKYIaXarhNrP0VSU8zIqOAlisQID1CPtA2NECfKTydPjOC+UbowFNJUpOFE/T2RIq7UkAemkyPdV7PeWPzPayc6PPVTGsWJJhGeLgoTBrWA40Rgl0qCNRsagrCk5laI+0girE1uBROCO/vyPGkcVlyn4l4fl6rnWRx5UAR7oAxccAKq4BLUQB1g8AiewSt4s56sF+vd+pi25qxsZhf8gfX5AwAflm0=</latexit>
<latexit sha1_base64="3sTAvva5UF6O0WoigMynSTaSlHA=">AAACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQm7CrgoJN0Ea7COaB2SXMTmaTITM768ysGJYUNv6KjYUitn6EnX/jJNlCEw9cOJxzL/feE8SMKu0431ZuYXFpeSW/Wlhb39jcsrd3GkokEpM6FkzIVoAUYTQidU01I61YEsQDRprB4GLsN++JVFREN3oYE5+jXkRDipE2UscuXpVvz2DrAHqM3MEj6PFAPKQwoFqNOnbJqTgTwHniZqQEMtQ69pfXFTjhJNKYIaXarhNrP0VSU8zIqOAlisQID1CPtA2NECfKTydPjOC+UbowFNJUpOFE/T2RIq7UkAemkyPdV7PeWPzPayc6PPVTGsWJJhGeLgoTBrWA40Rgl0qCNRsagrCk5laI+0girE1uBROCO/vyPGkcVlyn4l4fl6rnWRx5UAR7oAxccAKq4BLUQB1g8AiewSt4s56sF+vd+pi25qxsZhf8gfX5AwAflm0=</latexit>

I(Z; X)  3 bits
q(z|x)

<latexit sha1_base64="uApyO09SM/XQ7i69yuHc0XZ0D7M=">AAACAXicbZDLSsNAFIZPvNZ6i7oR3AwWoW5KIoIui25cVrAXaEOZTCft0MkkzkzEWuvGV3HjQhG3voU738ZJG0Fbfxj4+M85zDm/H3OmtON8WXPzC4tLy7mV/Ora+samvbVdU1EiCa2SiEey4WNFORO0qpnmtBFLikOf07rfP0/r9RsqFYvElR7E1AtxV7CAEayN1bZ3r4utEOueH6A7dI9++PawbReckjMWmgU3gwJkqrTtz1YnIklIhSYcK9V0nVh7Qyw1I5yO8q1E0RiTPu7SpkGBQ6q84fiCETowTgcFkTRPaDR2f08McajUIPRNZ7qhmq6l5n+1ZqKDU2/IRJxoKsjkoyDhSEcojQN1mKRE84EBTCQzuyLSwxITbULLmxDc6ZNnoXZUcp2Se3lcKJ9lceRgD/ahCC6cQBkuoAJVIPAAT/ACr9aj9Wy9We+T1jkrm9mBP7I+vgHxQpXn</latexit>
<latexit sha1_base64="uApyO09SM/XQ7i69yuHc0XZ0D7M=">AAACAXicbZDLSsNAFIZPvNZ6i7oR3AwWoW5KIoIui25cVrAXaEOZTCft0MkkzkzEWuvGV3HjQhG3voU738ZJG0Fbfxj4+M85zDm/H3OmtON8WXPzC4tLy7mV/Ora+samvbVdU1EiCa2SiEey4WNFORO0qpnmtBFLikOf07rfP0/r9RsqFYvElR7E1AtxV7CAEayN1bZ3r4utEOueH6A7dI9++PawbReckjMWmgU3gwJkqrTtz1YnIklIhSYcK9V0nVh7Qyw1I5yO8q1E0RiTPu7SpkGBQ6q84fiCETowTgcFkTRPaDR2f08McajUIPRNZ7qhmq6l5n+1ZqKDU2/IRJxoKsjkoyDhSEcojQN1mKRE84EBTCQzuyLSwxITbULLmxDc6ZNnoXZUcp2Se3lcKJ9lceRgD/ahCC6cQBkuoAJVIPAAT/ACr9aj9Wy9We+T1jkrm9mBP7I+vgHxQpXn</latexit>
<latexit sha1_base64="uApyO09SM/XQ7i69yuHc0XZ0D7M=">AAACAXicbZDLSsNAFIZPvNZ6i7oR3AwWoW5KIoIui25cVrAXaEOZTCft0MkkzkzEWuvGV3HjQhG3voU738ZJG0Fbfxj4+M85zDm/H3OmtON8WXPzC4tLy7mV/Ora+samvbVdU1EiCa2SiEey4WNFORO0qpnmtBFLikOf07rfP0/r9RsqFYvElR7E1AtxV7CAEayN1bZ3r4utEOueH6A7dI9++PawbReckjMWmgU3gwJkqrTtz1YnIklIhSYcK9V0nVh7Qyw1I5yO8q1E0RiTPu7SpkGBQ6q84fiCETowTgcFkTRPaDR2f08McajUIPRNZ7qhmq6l5n+1ZqKDU2/IRJxoKsjkoyDhSEcojQN1mKRE84EBTCQzuyLSwxITbULLmxDc6ZNnoXZUcp2Se3lcKJ9lceRgD/ahCC6cQBkuoAJVIPAAT/ACr9aj9Wy9We+T1jkrm9mBP7I+vgHxQpXn</latexit>
<latexit sha1_base64="uApyO09SM/XQ7i69yuHc0XZ0D7M=">AAACAXicbZDLSsNAFIZPvNZ6i7oR3AwWoW5KIoIui25cVrAXaEOZTCft0MkkzkzEWuvGV3HjQhG3voU738ZJG0Fbfxj4+M85zDm/H3OmtON8WXPzC4tLy7mV/Ora+samvbVdU1EiCa2SiEey4WNFORO0qpnmtBFLikOf07rfP0/r9RsqFYvElR7E1AtxV7CAEayN1bZ3r4utEOueH6A7dI9++PawbReckjMWmgU3gwJkqrTtz1YnIklIhSYcK9V0nVh7Qyw1I5yO8q1E0RiTPu7SpkGBQ6q84fiCETowTgcFkTRPaDR2f08McajUIPRNZ7qhmq6l5n+1ZqKDU2/IRJxoKsjkoyDhSEcojQN1mKRE84EBTCQzuyLSwxITbULLmxDc6ZNnoXZUcp2Se3lcKJ9lceRgD/ahCC6cQBkuoAJVIPAAT/ACr9aj9Wy9We+T1jkrm9mBP7I+vgHxQpXn</latexit>

q(z)

<latexit sha1_base64="SWh8fvZHV8L7wc02qVZJ+1cbL5U=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHqpsyIoMuiG5cV7AM6Q8mkmTY0yYxJRqhDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJpxp47rfzsrq2vrGZmmrvL2zu7dfOThs6zhVhLZIzGPVDbGmnEnaMsxw2k0UxSLktBOOb3K/80iVZrG8N5OEBgIPJYsYwcZK/kPNF9iMwgg9nfUrVbfuzoCWiVeQKhRo9itf/iAmqaDSEI617nluYoIMK8MIp9Oyn2qaYDLGQ9qzVGJBdZDNMk/RqVUGKIqVfdKgmfp7I8NC64kI7WSeUC96ufif10tNdBVkTCapoZLMD0UpRyZGeQFowBQlhk8swUQxmxWREVaYGFtT2ZbgLX55mbTP655b9+4uqo3roo4SHMMJ1MCDS2jALTShBQQSeIZXeHNS58V5dz7moytOsXMEf+B8/gAunZEc</latexit>
<latexit sha1_base64="SWh8fvZHV8L7wc02qVZJ+1cbL5U=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHqpsyIoMuiG5cV7AM6Q8mkmTY0yYxJRqhDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJpxp47rfzsrq2vrGZmmrvL2zu7dfOThs6zhVhLZIzGPVDbGmnEnaMsxw2k0UxSLktBOOb3K/80iVZrG8N5OEBgIPJYsYwcZK/kPNF9iMwgg9nfUrVbfuzoCWiVeQKhRo9itf/iAmqaDSEI617nluYoIMK8MIp9Oyn2qaYDLGQ9qzVGJBdZDNMk/RqVUGKIqVfdKgmfp7I8NC64kI7WSeUC96ufif10tNdBVkTCapoZLMD0UpRyZGeQFowBQlhk8swUQxmxWREVaYGFtT2ZbgLX55mbTP655b9+4uqo3roo4SHMMJ1MCDS2jALTShBQQSeIZXeHNS58V5dz7moytOsXMEf+B8/gAunZEc</latexit>
<latexit sha1_base64="SWh8fvZHV8L7wc02qVZJ+1cbL5U=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHqpsyIoMuiG5cV7AM6Q8mkmTY0yYxJRqhDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJpxp47rfzsrq2vrGZmmrvL2zu7dfOThs6zhVhLZIzGPVDbGmnEnaMsxw2k0UxSLktBOOb3K/80iVZrG8N5OEBgIPJYsYwcZK/kPNF9iMwgg9nfUrVbfuzoCWiVeQKhRo9itf/iAmqaDSEI617nluYoIMK8MIp9Oyn2qaYDLGQ9qzVGJBdZDNMk/RqVUGKIqVfdKgmfp7I8NC64kI7WSeUC96ufif10tNdBVkTCapoZLMD0UpRyZGeQFowBQlhk8swUQxmxWREVaYGFtT2ZbgLX55mbTP655b9+4uqo3roo4SHMMJ1MCDS2jALTShBQQSeIZXeHNS58V5dz7moytOsXMEf+B8/gAunZEc</latexit>
<latexit sha1_base64="SWh8fvZHV8L7wc02qVZJ+1cbL5U=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHqpsyIoMuiG5cV7AM6Q8mkmTY0yYxJRqhDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJpxp47rfzsrq2vrGZmmrvL2zu7dfOThs6zhVhLZIzGPVDbGmnEnaMsxw2k0UxSLktBOOb3K/80iVZrG8N5OEBgIPJYsYwcZK/kPNF9iMwgg9nfUrVbfuzoCWiVeQKhRo9itf/iAmqaDSEI617nluYoIMK8MIp9Oyn2qaYDLGQ9qzVGJBdZDNMk/RqVUGKIqVfdKgmfp7I8NC64kI7WSeUC96ufif10tNdBVkTCapoZLMD0UpRyZGeQFowBQlhk8swUQxmxWREVaYGFtT2ZbgLX55mbTP655b9+4uqo3roo4SHMMJ1MCDS2jALTShBQQSeIZXeHNS58V5dz7moytOsXMEf+B8/gAunZEc</latexit>

Encoder distribution

Figure 1: For a noisy channel characterized by z = x + s✏  we compare drawing the noise  ✏  from a
Gaussian distribution (as in VAEs) or an Echo distribution.

arbitrary input distributions. Echo noise is constructed from the empirical distribution of its inputs 
allowing its variation to reﬂect that of the source (see Fig. 1). We leverage this relationship to derive
an analytic form for mutual information that avoids distributional assumptions on either the noise
or the encoding marginal. Further  the Echo channel avoids the need to specify a prior  and instead
implicitly uses the optimal prior in the Evidence Lower Bound. This marginal distribution is neither
Gaussian nor independent in general.
After introducing the Echo noise channel and an exact characterization of its information rate in Sec. 2 
we proceed to interpret Variational Autoencoders from an encoding perspective in Sec. 3. We formally
deﬁne our rate-distortion objective in Sec. 3.1  and draw connections with recent related works in
Sec. 4. Finally  we report log likelihood results  visualize the space of compression-reconstruction
trade-offs  and evaluate disentanglement in Echo representations in Sec. 5.

2 Echo Noise

To avoid learning representations that memorize the data  we would like to constrain the mutual
information between the input X and the representation Z. Since we have freedom to choose how to
encode the data  we can design a noise model that facilitates calculating this generally intractable
quantity.
The Echo noise channel has a shift-and-scale form that mirrors the reparameterization trick in VAEs.
Referring to the observed data distribution as q(x)  with z 2 Rdz   x 2 Rdx  we can deﬁne the
stochastic encoder q(z|x) using:

z = f (x) + S(x)✏

(1)

For brevity  we omit the subscripts that indicate that the functions f : Rdx ! Rdz and matrix function
S : Rdx ! Rdz ⇥ Rdz depend on neural networks parameterized by . All that remains to specify the
encoder is to ﬁx the distribution of the noise variable  q(✏). For VAEs  the noise is typically chosen to
be Gaussian  ✏ ⇠N (0  Idz ). 1
With the goal of calculating mutual information  we will need to compare the marginal entropy H(Z) 
which integrates over samples x  and the conditional entropy H(Z|X)  whose stochasticity is only
due to the noise for deterministic f (x) and S(x). The choice of noise will affect both quantities  and
our approach is to relate them by enforcing an equivalence between the distributions q(z) and q(✏).
Since q(z) =R q(z|x)q(x)dx is deﬁned in terms of the source  we can also imagine constructing
the noise in a data-driven way. For instance  we could draw ✏ = f (x0)  x0 iid⇠q(x) in an effort to make
the noise match the channel output. However  this changes the distribution of Z and the noise would
need to be updated to continue resembling the output.

1Our approach is also easily adapted to multiplicative noise  such as in Achille and Soatto [1].

2

Instead  by iteratively applying Eq. 1  we can guarantee that the noise and marginal distributions
match in the limit. Using superscripts to indicate iid samples x` iid⇠q(x)  we draw ✏ according to:

✏ = f (x0) + S(x0)✓f (x1) + S(x1)⇣f (x2) + S(x2)...

= f (x0) + S(x0)f (x1) + S(x0)S(x1)f (x2)...

(2)

Echo noise is thus constructed using an inﬁnite sum over attenuated “echoes” of the transformed data
samples. This can be written more compactly as follows.
Deﬁnition: Echo Noise The Echo noise distribution E(f (x)  S(x)  q(x)) is deﬁned for functions
f  S  and probability density function q over x 2 Rdx  by sampling according to the following
procedure.

✏ =

1X`=0 `Y`0=1

S(x`0)! f (x`) 

x` iid⇠q(x)

(3)

Although the noise distribution may be complex  it has the interesting property that it exactly matches
the eventual output marginal q(z).
Lemma 2.1 (Echo noise matches channel output). If ✏ ⇠ Echo(f (x)  S(x)  q(x)) and z = f (x) +
S(x)✏  then z has the same distribution as ✏.
We can observe this relationship by simply re-labeling the sample indices in the expanded expression
for the noise in Eq. 2. In particular  the training example that we condition on in Eq. 1 corresponds to
the ﬁrst sample x0 in a draw from the noise. This equivalence is the key insight leading to an exact
expression for the mutual information:
Theorem 2.2 (Echo Information). For any source distribution q(x)  and a noisy channel deﬁned by
Eq. 1 that satisﬁes 2.3  the mutual information is as follows:

I(X; Z) = Ex log | det S(x)|

(4)

Proof. We start by expanding the deﬁnition of mutual information in terms of entropies. Since f (x)
and S(x) are deterministic  we treat them as constants after conditioning on X = x. The stochasticity
underlying H(Z|X = x) is thus only due to the random variable ✏.

I(X; Z) = H(Z)  H(Z|X)

= H(Z)  Ex H(f (x) + S(x)E | X = x)
= H(Z)  Ex H(S(x)E | X = x)
= H(Z)  H(E)  Ex log | det S(x)|
= Ex log | det S(X)|

We have used the translation invariance of differential entropy in the third line  and the scaling
property in the fourth line [12]. The entropy terms cancel as a result of Lemma 2.1.

In this work  we consider only diagonal S(x) ⌘ diag(s1(x)  . . .   sdz (x)) as is typical for VAEs  so
that the determinant in Eq. 4 simpliﬁes as I(X; Z) = Pj Ex log |sj(x)| =Pj I(X; Zj).

Finally  we note that the noise distribution q(✏) is only deﬁned implicitly through a sampling procedure.
For this to be meaningful  we must ensure that the inﬁnite sum converges.
Lemma 2.3. The inﬁnite sum in Eq. 3 converges  and thus Echo noise sampling is well-behaved  if
8x  9M s.t. |f (x)| M and ⇢(S(x)) < 1  where ⇢ is the spectral radius.
In App. B  we discuss several implementation choices to guarantee that these conditions are met
and that Echo noise can be accurately sampled using a ﬁnite number of terms. This is particularly
difﬁcult in the high noise  low information regime  as zero mutual information (sj(x)) = 18 x  j)
would imply an inﬁnite amount of noise. To avoid this issue and ensure precise sampling  we clip
the magnitude of sj(x) so that  for a given M and number of samples  the sum of remainder terms
is guaranteed to be within machine precision. This imposes a lower bound on the achievable rate
across the Echo channel  which depends on the number of terms considered and can be tuned by the
practitioner.

3

2.1 Properties of Echo Noise
We can visualize applying Echo noise to a complex input distribution in Fig. 1  using the identity
transformation f (x) = x and constant noise scaling sj(x) = .5. Here  we directly observe the
equivalence of the noise and output distributions. Further  the data-driven nature of the Echo channel
means it can leverage the structure in the (transformed) input to destroy information in a more targeted
way than spherical Gaussian noise.
In particular  Echo’s ability to add noise that is correlated across dimensions distinguishes it from
common diagonal noise models. It is important to note that the noise still reﬂects the dependence
in f (x) even when S(x) is diagonal. In fact  we show in App. C that T C(Z) = T C(Z|X) for the
diagonal case  where total correlation measures the divergence from independence  e.g. T C(Z|X) =
DKL[q(z|x)||Q q(zj|x)] [43].
In the setting of learned f (x) and S(x)  notice that the noise depends on the parameters. This means
that training gradients are propagated through ✏  unlike traditional VAEs where q(✏) is ﬁxed. This
may be a factor in improved performance: data samples are used as both signal and noise in different
parts of the optimization  leading to a more efﬁcient use of data.
Finally  the Echo channel fulﬁlls several of the desirable properties that often motivate Gaussian
noise and prior assumptions. Eqs. 1 and 3 deﬁne a simple sampling procedure that only requires a
supply of iid samples from the input distribution. It is easy to sample both the noise and conditional
distributions for the purposes of evaluating expectations  while Echo also provides a natural way
to sample from the true encoding marginal q(z) via its equivalence with q(✏). While we cannot
evaluate the density of a given z under q(z|x) or q(z)  as might be useful in importance sampling
[8]  we can characterize their relationship on average using the mutual information in Eq. 4. These
ingredients make Echo noise useful for learning representations within the autoencoder framework.

3 Lossy Compression in VAEs

Variational Autoencoders (VAEs) [21  36] seek to maximize the log-likelihood of data under a
latent factor generative model deﬁned by p✓(x  z) = p(z)p✓(x|z)  where ✓ represents parameters
of the generative model decoder and p(z) is the prior distribution over latent variables. However 
maximum likelihood is intractable in general due to the difﬁcult integral over Z  log p✓(x) =
logR p(z)p✓(x|z)dz.
To avoid this problem  VAEs introduce a variational distribution  q(z|x)  which encodes the input
data q(x) and approximates the generative model posterior p✓(z|x). This leads to the tractable
(average) Evidence Lower Bound (ELBO) on likelihood:

Eq log p✓(x)  Eq log p✓(x)  DKL[q(z|x)||p✓(z|x)]
= Eq log p✓(x|z)  DKL[q(z|x)||p(z)]

(5)

The connection between VAEs and rate-distortion theory can be seen using a decomposition of the
KL divergence term from Hoffman and Johnson [19].

DKL[q(z|x)||p(z)] = DKL[q(z|x)||q(z)] + DKL[q(z)||p(z)]

 DKL[q(z|x)||q(z))] = Iq(X; Z)

(6)

This decomposition lends insight into the orthogonal goals of the ELBO regularization term. The
mutual information Iq(X; Z) encourages lossy compression of the data into a latent code  while the
marginal divergence enforces consistency with the prior. The non-negativity of the KL divergence
implies that each of these terms detracts from our likelihood bound.
Similarly  we observe that DKL[q(z|x)||p(z)] gives an upper bound on the mutual information  with
a gap of DKL[q(z)||p(z)]. From this perspective  a static Gaussian prior can be seen a particular
and possibly loose marginal approximation [3  14  37]. The true encoding marginal q(z) provides
the unique  optimal choice of prior and leads to a tighter bound on the likelihood:

Eq log p✓(x)  Eq log p✓(x|z)  Iq(X; Z)

(7)

Our exact expression for the mutual information over an Echo channel provides the ﬁrst general
method to directly optimize this objective. This corresponds to adaptively setting p(z) equal to q(z)

4

throughout training  so that Eq. 7 can be seen as bounding the likelihood under the generative model

p(x) =R q(z)p✓(x|z)dx.

3.1 Rate-Distortion Objective

While the VAE is motivated as performing amortized inference of the latent variables in a generative
model  the prior is rarely leveraged to encode domain-speciﬁc structure. Further  we have shown that
enforcing prior consistency can detract from likelihood bounds.
We instead follow Alemi et al. [3] in advocating that representation learning be motivated from
an encoding perspective using rate-distortion theory. In particular  we choose reconstruction under
the generative model as the distortion measure d(x  z) =  log p✓(x|z)  and study the following
optimization problem:

max
✓ 

Eq log p✓(x|z)  Iq(X; Z)

(8)

While this resembles the -VAE objective of Higgins et al. [18]  we highlight two notable distinctions.
First  treating Iq(X; Z) rather than the upper bound DKL[q(z|x)||p(z)] avoids the need to specify a
prior and facilitates a direct interpretation in terms of lossy compression. Further  the  parameter is
naturally interpreted as a Lagrange multiplier enforcing a constraint on Iq(X; Z). The special choice
of  = 1 gives a bound on log-likelihood according to Eq. 7  which we use to compare results across
methods in Sec. 5. We direct the reader to App. A for a more formal treatment of rate-distortion.

4 Related Work

Rate-Distortion Theory: A number of recent works have made connections between the Evidence
Lower Bound objective and rate-distortion theory [1  3  25  35]  with the average distortion corre-
sponding to the cross entropy reconstruction loss as above.. In particular  Alemi et al. [3] consider the
following upper and lower bounds on the mutual information Iq(X; Z):

H  D = Hq(X) + Eq log p✓(x|z)  Iq(X; Z)  DKL[q(z|x)||r(z)] = R

With the data entropy as a constant  minimizing the cross entropy distortion corresponds to the
variational information maximization lower bound of Barber and Agakov [4]. The upper bound
matches the decomposition in Eq. 6 for the generalized choice of marginal r(z). Several recent works
have also considered ‘learned priors’ or ﬂow-based density estimators [3  11  40] that seek to reduce
the marginal divergence by approximating q(z) (see below). Using this upper bound on the rate
term  Alemi et al. [3] and Rezende and Viola [35] obtain objectives similar to Eq. 8.
Existing models are usually trained with a static  [3  18] or a heuristic annealing schedule [6  9] 
which implicitly correspond to constant constraints (see App.A). However  setting target values
for either the rate or distortion remains an interesting direction for future discussion. Rezende and
Viola [35] view the distortion as an intuitive quantity to specify in practice  while Zhao et al. [46]
train a separate model to provide constraint values. As both works show  specifying a constant and
optimizing the Lagrange multiplier  with gradient descent can lead to improved performance.
Mutual Information in Unsupervised Learning: A number of recent works have argued that the
maximum likelihood objective may be insufﬁcient to guarantee useful representations of data [3  45].
In particular  when paired with powerful decoders that can match the data distribution  VAEs may
learn to completely ignore the latent code [6  11].
To rectify these issues  a commonly proposed solution has been to add terms to the objective function
that maximize  minimize  or constrain the mutual information between data and representation
[3  7  32  45  46]. However  justiﬁcations for these approaches have varied and numerous methods
have been employed for estimating the mutual information. These include sampling [32]  indirect
optimization via other divergences [45]  mixture entropy estimation [23]  learned mixtures [40] 
autoregressive density estimation [3]  and a dual form of the KL divergence [5]. Poole et al. [33]
provide a thorough review and analysis of variational upper and lower bounds on mutual information 
although recent results have shown limits on our ability to construct high conﬁdence estimators
directly from samples [29]. Echo notably avoids this limitation by providing an analytic expression
for the rate whenever the representation is sampled according to Eq. 3.

5

Table 1: Test Log Likelihood Bounds

Binary MNIST

Omniglot

Fashion MNIST

Method
Rate Dist
Echo
26.4 62.4
VAE
26.2 63.6
InfoVAE
26.0 64.0
VAE-MAF
26.1 63.7
VAE-Vamp
26.3 63.0
IAF-Prior
26.5 63.5
IAF+MMD 26.3 63.6
26.4 63.6
IAF-MAF
IAF-Vamp
26.4 62.8

-ELBO 
88.8
.18
.18
89.8
.14
90.0
.15
89.8
.19
89.3
90.0
.13
.15
90.1
.18
89.9
89.2
.18

Rate Dist
30.2 84.4
30.5 86.5
30.3 87.3
30.5 86.4
30.8 84.3
30.5 86.7
30.7 86.4
30.6 86.5
30.4 85.0

-ELBO 
114.6
.30
.44
117.0
.51
117.6
.31
116.9
.28
115.1
117.2
.36
.28
117.1
.24
117.1
115.4
.20

Rate Dist
16.6 218.3
15.7 219.3
15.6 219.5
15.7 219.3
15.9 218.5
15.8 219.1
15.7 219.2
15.8 219.1
16.0 218.3

-ELBO 
234.9
.21
.10
235.0
.10
235.1
.14
234.9
.08
234.4
234.9
.10
.13
234.9
.14
234.9
234.3
.16

Params
(·106)
1.40
1.40
1.40
3.12
1.99
3.12
3.12
4.84
3.71

Among the approaches above   the InfoVAE model of Zhao et al. [45] provides a potentially interesting
comparison with our method. The objective adds a parameter  to more heavily regularize the marginal
divergence and a parameter ↵ to control mutual information. However  since DKL[q(z)||p(z)] is
intractable  the Maximum Mean Discrepancy (MMD) [16] between the encoding outputs and a
standard Gaussian is used as a proxy. For the choice of  = 1000 (as in the original paper) and ↵ = 0
(no information preference)  the objective simpliﬁes to:

LInfoVAE = LELBO  999 DMMD[q(z)||p(z)]

The sizeable MMD penalty encourages q(z) ⇡ p(z)  so that DKL[q(z|x)||p(z)] ⇡
DKL[q(z|x)||q(z)] = Iq(X; Z). Thus  the KL divergence term in the ELBO should more closely
reﬂect a mutual information regularizer  facilitating comparison with the rate in Echo models.
Flow models  which evaluate densities on simple distributions such as Gaussians but apply complex
transformations with tractable Jacobians  are another prominent recent development in unsuper-
vised learning [15  22  31  34]. Flows can be used both as an encoding mechanism and marginal
approximation for our purposes. In particular  Inverse Autoregressive Flow [22] can be seen as
transforming the output of a Gaussian noise channel into an approximate posterior sample using a
stack of autoregressive networks. Masked Autoregressive Flow [31] models a similar transformation
with computational tradeoffs suited for density estimation  mapping latent samples to high probability
under a Gaussian base distribution to approximate q(z).
Finally  the VampPrior [40] may also be used as a marginal approximation  modeling q(z) using
a mixture distribution 1
backpropagation.

KPk q(z|uk) evaluated on a set of ‘pseudo-inputs’ uk 2 Rdx learned by

5 Results

In this section  we would ideally like to quantify the impact of three key elements of the Echo
approach: a data-driven noise model  exact rate regularization throughout training  and a ﬂexible
marginal distribution. In App. D.2  we observe that the dimension-wise marginals learned by Echo
appear Gaussian despite our lack of explicit constraints. However  the joint marginal over q(z) (or
equivalently q(✏)) may still have a complex dependence structure  which is not penalized for deviating
from independence or Gaussianity. We calculate a second-order approximation of total correlation in
App. C to conﬁrm that this noise is indeed dependent across dimensions.

5.1 ELBO Results

We proceed to analyse the log-likelihood performance of relevant models on three image datasets:
static Binary MNIST [38]  Omniglot [24] as adapted by Burda et al. [8]  and Fashion MNIST (fM-
NIST) [44]. All models are trained with 32 latent variables using the same convolutional architecture
as in Alemi et al. [3] except with ReLU activations. We trained using Adam optimization for 200
epochs  with an initial learning rate of 0.0003 decaying linearly to 0 over the last 100 epochs.

6

Figure 2: Binary MNIST R-D and Visualization

Figure 3: Omniglot R-D and Visualization

Table 1 shows negative test ELBO values  with the rate column reported as the appropriate upper
bound for comparison methods. Results are averaged from ten runs of each model after removing the
highest and lowest outliers. We compare Echo against diagonal Gaussian noise and IAF encoders 
each with four marginal approximations: a Gaussian prior with and without the MMD penalty (e.g.
IAF-Prior  IAF+MMD)  MAF [31]  and VampPrior [40]. Note that VAE is still used to denote the
Gaussian encoder when paired with a different marginal (e.g. VAE-Vamp).
We ﬁnd that the Echo noise autoencoder obtains improved likelihood bounds on Binary MNIST and
Omniglot  with competitive results on fMNIST. We emphasize that Echo achieves this performance
with signiﬁcantly fewer parameters than comparison methods. IAF and MAF each require training
an additional autoregressive model with size similar to the original network  while the VampPrior
uses 750 learned pseudoinputs of the same dimension as the data. Although Echo involves special
computation to construct the noise for each training example  it has the same number of parameters
as a standard VAE and runs in approximately the same wall clock time.
We observe only minor differences based on the choice of encoding mechanism  which is somewhat
surprising given the additional expressivity of the IAF transformation. The beneﬁt of the ﬂow
transformations may be more readily observed on more difﬁcult datasets or with more advanced
architecture tuning [22].
We do ﬁnd that a more complex marginal approximation can help performance. Although we see
minimal gains from the MMD penalty and MAF marginal  the VampPrior bridges much of the
performance gap with Echo noise. Recall that a learned prior can help ensure a tight rate bound while
providing ﬂexibility to learn a more complex marginal (in this case  a mixture model). However  the
relative contribution of these effects is difﬁcult to decouple. Echo instead provides both an exact rate
and an adaptive prior by directly linking the choice of encoder and marginal.

5.2 Rate Distortion Curves

Moving beyond the special case of  = 1  rate-distortion theory provides the practitioner with an
entire space of compression-relevance tradeoffs corresponding to constraints on the rate. We plot
R-D curves for Binary MNIST in Fig. 2  Omniglot in Fig. 3  and Fashion MNIST in App. D.1. We
also show model reconstructions at several points along the curve  with the output averaged over 10
encoding samples to observe how stochasticity in the latent space is translated through the decoder.

7

Table 2: Disentanglement Scores

Figure 4: Echo  = 0  = 1

Independent Ground Truth
Factor

MIG

Dependent Ground Truth
MIG

Factor

 = 1
 = 4
 = 8

Echo VAE
0.83 0.65
0.78 0.65
0.75 0.69
0.83 0.65
 = 0
0.78 0.72
 = 20
0.79 0.73
 = 50
 = 100 0.77 0.70

Echo VAE
0.16 0.07
0.18 0.10
0.18 0.13
0.16 0.07
0.30 0.17
0.30 0.18
0.29 0.18

Echo VAE
0.70 0.60
0.67 0.60
0.56 0.56
0.70 0.60
0.65 0.60
0.58 0.53
0.49 0.53

Echo VAE
0.11 0.08
0.11 0.07
0.06 0.06
0.10 0.08
0.16 0.07
0.16 0.07
0.09 0.08

These visualizations are organized to compare models with similar rates  which we emphasize may
occur at different values of  for different methods depending on the shape of their respective curves.
The Echo rate-distortion curve indeed exhibits several notable differences with comparison methods.
We ﬁrst note that Echo performance begins to drop off as we approach the lower limit on achievable
rate  which is shown with a dashed vertical line and ensures that the rate calculation accurately reﬂects
the noise for a ﬁnite number of samples (see App.B). In this regime  the sigmoids parameterizing
sj(x) are saturated for much of training  and unused dimensions still count against the objective since
we cannot achieve zero rate. We reiterate that this low rate limit may be adjusted by considering more
terms in the inﬁnite sum or decreasing the number of latent factors.
At low rates  our models maintain only high level features of the input image  and the blurred average
reconstructions reﬂect that different samples can lead to semantically different generations. On both
datasets  Echo gives qualitatively different output variation than Gaussian noise at low rate and similar
distortion. Intermediate-rate models still reﬂect some of this sample diversity  particularly on the
more difﬁcult Omniglot dataset.
For very high capacity models  we observe that Echo slightly extends its gains on both datasets  with
three to ﬁve nats lower distortion than comparison methods at the same rates. Intuitively  a more
complex encoding marginal may be harder to match to a (learned) prior  loosening the upper bound
on mutual information. The Echo approach can be particularly useful in this regime  as it avoids
explicitly constructing the marginal while still providing exact rate regularization.

5.3 Disentangled Representations
Signiﬁcant recent attention has been devoted to learning disentangled representations of data  which
reﬂect the true generative factors of variation in the data [10  27] and may be useful for downstream
tasks [26  42]. While prevailing deﬁnitions and metrics for disentanglement have recently been
challenged [26]  existing methods often rely on the inductive bias of independent ground truth factors 
either via total correlation (TC) regularization [10  20]  or by using higher  to more strongly penalize
the KL divergence to an independent prior [9  18]. Since Echo does not assume a factorized encoder
or marginal  we investigate whether it can better preserve disentanglement when the ground truth
factors are not independent.
To evaluate the quality of Echo noise representations  we compare against VAE models with diag-
onal Gaussian noise and priors  and consider the effects of increasing  or adding independence
regularization with parameter  [10  20]:

L = Eq log p✓(x|z)  Iq(X; Z)  T C (Z)

TC regularization is implemented as in [20]  where a discriminator is trained to distinguish samples
from q(z) andQ q(zj). We keep  = 1 when modifying . Note that enforcing marginal indepen-
dence will also limit the dependence in the noise learned by Echo  since T C(Z|X) and T C(Z) are
linked as described in Sec. 2.1.

8

We calculate disentanglement scores on the dSprites dataset [28]  where the ground truth factors of
shape  scale  x-y position  and rotation are known and sampled independently across the dataset. To
induce dependence in the ground truth factors  we downsample the dataset by partitioning each factor
into 4 bins and randomly excluding pairwise combinations of bins with probability 0.15. This leads
to a dataset of 15% of the original size  with a total correlation of 1.49 nats in the generative factors.
We use both the implementation and experimental setup of Locatello et al. [26] and average scores
over ten runs of each method.
Table 2 reports FactorVAE [20] and Mutual Information Gap [10] scores for both independent and
dependent ground truth factors. We ﬁnd that Echo provides superior disentanglement scores to VAEs
across the board  although the relative improvement does not increase in the case of dependent
latent factors. On the full dataset  independence regularization improves the MIG score for Echo and
both scores for VAE  but may guide both models toward more entangled representations when this
inductive bias does not match the ground truth. Finally  we note that increasing  need not improve
disentanglement for Echo noise  since we have relaxed assumptions of independence in both the
encoder and marginal. Higher  actually appear to hurt disentanglement scores on the dependent
dataset for both methods.
In Figure 4  we visualize an Echo model that has successfully learned to disentangle position and
scale  but not rotation  on the full dSprites dataset. Each row represents a single latent dimension  and
each column shows mean f (x) values as a function of the respective ground truth factors. Note that
the ﬁrst column shows a heatmap in the x-y plane  while the orange  blue  and green lines indicate
ellipse  square  and heart  respectively (see [10]). In general  we observed that Echo models achieved
their highest MIG scores on position  scale  and shape  with rotation often entangled across two or
more dimensions.

6 Conclusion

VAEs can be interpreted as performing a rate-distortion optimization  but may be handicapped by their
weak compression mechanism  independent Gaussian marginal assumptions  and upper bound on rate.
We introduced a new type of channel  Echo noise  that provides a more ﬂexible  data-driven approach
to constructing noise and admits an exact expression for mutual information. Our results demonstrate
that using Echo noise in autoencoders can lead to better bounds on log-likelihood  favorable trade-offs
between compression and reconstruction  and more disentangled representations.
The Echo channel can be substituted for Gaussian noise in most scenarios where VAEs are used  with
similar runtime and the same number of parameters. Echo should also translate to other rate-distortion
problems via the choice of distortion measure  including supervised learning with the traditional
Information Bottleneck method [2  39] and invariant representation learning as in [30]. Exploring
further settings where mutual information provides meaningful regularization for neural network
representations remains an exciting avenue for future work.

References
[1] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations

through noisy computation. arXiv preprint arXiv:1611.01353  2016.

[2] Alexander Alemi  Ian Fischer  Joshua Dillon  and Kevin Murphy. Deep variational information

bottleneck. arXiv preprint arXiv:1612.00410  2016.

[3] Alexander Alemi  Ben Poole  Ian Fischer  Joshua Dillon  Rif A Saurous  and Kevin Murphy.
Fixing a broken elbo. In International Conference on Machine Learning  pages 159–168  2018.
[4] David Barber and Felix V Agakov. The im algorithm: a variational approach to information

maximization. In Advances in neural information processing systems  page None  2003.

[5] Ishmael Belghazi  Sai Rajeswar  Aristide Baratin  R Devon Hjelm  and Aaron Courville. Mine:

mutual information neural estimation. arXiv preprint arXiv:1801.04062  2018.

[6] Samuel R. Bowman  Luke Vilnis  Oriol Vinyals  Andrew M. Dai  Rafal Józefowicz  and Samy
Bengio. Generating sentences from a continuous space. CoRR  abs/1511.06349  2015. URL
http://arxiv.org/abs/1511.06349.

9

[7] DT Braithwaite and W Bastiaan Kleijn. Bounded information rate variational autoencoders.

arXiv preprint arXiv:1807.07306  2018.

[8] Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov. Importance weighted autoencoders.

arXiv preprint arXiv:1509.00519  2015.

[9] Christopher P Burgess  Irina Higgins  Arka Pal  Loic Matthey  Nick Watters  Guillaume Des-
jardins  and Alexander Lerchner. Understanding disentangling in beta-vae. arXiv preprint
arXiv:1804.03599  2018.

[10] Tian Qi Chen  Xuechen Li  Roger Grosse  and David Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems 
2018.

[11] Xi Chen  Diederik P Kingma  Tim Salimans  Yan Duan  Prafulla Dhariwal  John Schulman  Ilya
Sutskever  and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731 
2016.

[12] Thomas M Cover and Joy A Thomas. Elements of information theory. Wiley-Interscience 

2006.

[13] Joshua V Dillon  Ian Langmore  Dustin Tran  Eugene Brevdo  Srinivas Vasudevan  Dave Moore 
Brian Patton  Alex Alemi  Matt Hoffman  and Rif A Saurous. Tensorﬂow distributions. arXiv
preprint arXiv:1711.10604  2017.

[14] Shuyang Gao  Rob Brekelmans  Greg Ver Steeg  and Aram Galstyan. Auto-encoding total

correlation explanation. AISTATS  2019.

[15] Mathieu Germain  Karol Gregor  Iain Murray  and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning  pages 881–889 
2015.

[16] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773 
2012.

[17] Virgil Grifﬁth and Christof Koch. Quantifying synergistic mutual information. In Guided

Self-Organization: Inception  pages 159–190. Springer  2014.

[18] Irina Higgins  Loic Matthey  Arka Pal  Matthew Botvinick Shakir Mohamed Christo-
pher Burgess  Xavier Glorot  and Alexander Lerchner. "beta-vae: Learning basic visual concepts
with a constrained variational framework.". In Proceedings of the International Conference on
Learning Representations (ICLR)  2017.

[19] Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the
variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference 
NIPS  2016.

[20] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983 

2018.

[21] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[22] Diederik P Kingma  Tim Salimans  Rafal Jozefowicz  Xi Chen  Ilya Sutskever  and Max
Welling. Improved variational inference with inverse autoregressive ﬂow. In Advances in Neural
Information Processing Systems  pages 4743–4751  2016.

[23] Artemy Kolchinsky  Brendan D Tracey  and David H Wolpert. Nonlinear information bottleneck.

arXiv preprint arXiv:1705.02436  2017.

[24] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept

learning through probabilistic program induction. Science  350(6266):1332–1338  2015.

10

[25] Luis A. Lastras-Montano. Information theoretic lower bounds on negative log likelihood. 2018.

URL https://openreview.net/forum?id=rkemqsC9Fm.

[26] Francesco Locatello  Stefan Bauer  Mario Lucic  Gunnar Raetsch  Sylvain Gelly  Bernhard
Schölkopf  and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In International Conference on Machine Learning  pages
4114–4124  2019.

[27] Emile Mathieu  Tom Rainforth  N Siddharth  and Yee Whye Teh. Disentangling disentanglement

in variational autoencoders. In International Conference on Machine Learning  2019.

[28] Loic Matthey  Irina Higgins  Demis Hassabis  and Alexander Lerchner. dsprites: Disentangle-

ment testing sprites dataset. https://github.com/deepmind/dsprites-dataset/  2017.

[29] David McAllester and Karl Statos. Formal limitations on the measurement of mutual information.

arXiv preprint arXiv:1811.04251  2018.

[30] Daniel Moyer  Shuyang Gao  Rob Brekelmans  Aram Galstyan  and Greg Ver Steeg. Invariant
representations without adversarial training. In Advances in Neural Information Processing
Systems  pages 9084–9093  2018.

[31] George Papamakarios  Iain Murray  and Theo Pavlakou. Masked autoregressive ﬂow for density
estimation. In Advances in Neural Information Processing Systems  pages 2338–2347  2017.

[32] Mary Phuong  Max Welling  Nate Kushman  Ryota Tomioka  and Sebastian Nowozin. The
mutual autoencoder: Controlling information in latent code representations. 2018. URL
https://openreview.net/forum?id=HkbmWqxCZ.

[33] Ben Poole  Sherjil Ozair  Aaron Van Den Oord  Alex Alemi  and George Tucker. On variational
In International Conference on Machine Learning  pages

bounds of mutual information.
5171–5180  2019.

[34] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows.

International Conference on Machine Learning  pages 1530–1538  2015.

In

[35] Danilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597 

2018.

[36] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In International Conference on Machine
Learning  pages 1278–1286  2014.

[37] Mihaela Rosca  Balaji Lakshminarayanan  and Shakir Mohamed. Distribution matching in

variational inference. arXiv preprint arXiv:1802.06847  2018.

[38] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th international conference on Machine learning  pages 872–879. ACM 
2008.

[39] Naftali Tishby  Fernando C Pereira  and William Bialek. The information bottleneck method.

arXiv preprint physics/0004057  2000.

[40] Jakub M Tomczak and Max Welling. Vae with a vampprior. AIStats 2018  2017. URL

arXivpreprintarXiv:1705.07120.

[41] Michael Tschannen  Olivier Bachem  and Mario Lucic. Recent advances in autoencoder-based

representation learning. arXiv preprint arXiv:1812.05069  2018.

[42] Sjoerd van Steenkiste  Francesco Locatello  Jürgen Schmidhuber  and Olivier Bachem.
arXiv preprint

Are disentangled representations helpful for abstract visual reasoning?
arXiv:1905.12506  2019.

[43] Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of

research and development  4(1):66–82  1960.

11

[44] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms. 2017.

[45] Shengjia Zhao  Jiaming Song  and Stefano Ermon. Infovae: Information maximizing varia-
tional autoencoders. CoRR  abs/1706.02262  2017. URL http://arxiv.org/abs/1706.
02262.

[46] Shengjia Zhao  Jiaming Song  and Stefano Ermon. The information autoencoding family: A
lagrangian perspective on latent variable generative models. CoRR  abs/1806.06514  2018. URL
http://arxiv.org/abs/1806.06514.

12

,Rob Brekelmans
Daniel Moyer
Aram Galstyan
Greg Ver Steeg