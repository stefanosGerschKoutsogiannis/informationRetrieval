2010,Variational bounds for mixed-data factor analysis,We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function.  In the special case of fully observed binary data  the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters  which is the approach used by exponential family PCA and other related matrix-factorization methods.  A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers  as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.,Variational Bounds for Mixed-Data Factor Analysis

Mohammad Emtiyaz Khan
University of British Columbia
Vancouver  BC  Canada V6T 1Z4

emtiyaz@cs.ubc.ca

Benjamin M. Marlin

University of British Columbia
Vancouver  BC  Canada V6T 1Z4

bmarlin@cs.ubc.ca

Guillaume Bouchard

Xerox Research Center Europe

38240 Meylan  France

guillaume.bouchard@xerox.com

Kevin P. Murphy

University of British Columbia
Vancouver  BC  Canada V6T 1Z4

murphyk@cs.ubc.ca

Abstract

We propose a new variational EM algorithm for ﬁtting factor analysis models
with mixed continuous and categorical observations. The algorithm is based on a
simple quadratic bound to the log-sum-exp function. In the special case of fully
observed binary data  the bound we propose is signiﬁcantly faster than previous
variational methods. We show that EM is signiﬁcantly more robust in the presence
of missing data compared to treating the latent factors as parameters  which is the
approach used by exponential family PCA and other related matrix-factorization
methods. A further beneﬁt of the variational approach is that it can easily be
extended to the case of mixtures of factor analyzers  as we show. We present
results on synthetic and real data sets demonstrating several desirable properties
of our proposed method.

1

Introduction

Continuous latent factor models  such as factor analysis (FA) and probabilistic principal components
analysis (PPCA)  are very commonly used density models for continuous-valued data. They have
many applications including latent factor discovery  dimensionality reduction  and missing data im-
putation. The factor analysis model asserts that a low-dimensional continuous latent factor zn ∈ RL
underlies each high-dimensional observed data vector yn ∈ RD. Standard factor analysis models
assume the prior on the latent factor has the form p(zn) = N (zn|0  I)  while the likelihood has the
form p(yn|zn) = N (yn|Wzn + µ  Σ). W is the D × L factor loading matrix  µ is an offset term 
and Σ is a D × D diagonal matrix specifying the marginal noise variances. If we set Σ = σ2I and
require W to be orthogonal  we recover probabilistic principal components analysis (PPCA). Such
models can be easily ﬁt using the expectation-maximization (EM) algorithm [Row97  TB99].
The FA model can be extended to other members of the exponential family by requiring that the
natural (canonical) parameters have the form Wzn + µ [WK01  CDS02  MHG08  LT10]. This
is the unsupervised version of a generalized linear model (GLM)  and is extremely useful since it
allows for non-trivial dependencies between data variables with mixed types.
The principal difﬁculty with the general FA model is computational tractability  both at training
and test time. A problem arises because the Gaussian prior on p(zn) is not conjugate to the likeli-
hood except when yn also has a Gaussian distribution (the standard FA model). There are several
approaches one can take to this problem. The simplest is to approximate the posterior p(zn|yn)
using a point estimate  which is equivalent to viewing the latent variables as parameters and esti-
mating them by maximum likelihood. This approach is known as exponential family PCA (ePCA)

1

Graphical Model:

Notation:

Mixture indicator variable
Latent factor vector
Continuous data vector
Discrete data variable
Factor loading matrices
Offset vectors
Continuous noise covariance
Mixture prior parameter
# data cases
# latent dimensions
# mixture components
# continuous variables
# discrete variables
# classes per discrete variable

dk

qn
zn
yC
n
yD
nd
k   WD
WC
dk
µC
k   µD
ΣC
k
π
N
L
K
Dc
Dd
Md + 1

Figure 1: The generalized mixture of factor analyzers model for discrete and continuous data.

[CDS02]. We refer to it as the “MM” approach to ﬁtting the general FA model since we maximize
over zn in the E-step  as well as W in the M-step. The main drawback of the MM approach is that
it ignores posterior uncertainty in zn  which can result in over-ﬁtting unless the model is carefully
regularized [WCS08]. This is a particular concern when we have missing data.
The opposite end of the model estimation spectrum is to integrate out both zn and W using Markov
chain Monte Carlo methods. This approach has recently been studied under the name “Bayesian
exponential family PCA” [MHG08] using a Hamiltonian Monte Carlo (HMC) sampling approach.
We will refer to this as the “SS” approach to indicate that we are integrating out both zn and W by
sampling. The SS approach preserves posterior uncertainty about zn (unlike the MM approach) and
is robust to missing data  but can have a signiﬁcantly higher computational cost.
In this work  we study a variational EM model ﬁtting approach that preserves posterior uncertainty
about zn  is robust to missing data  and is more computationally efﬁcient than SS. We refer to this
as the “VM” approach to indicate that we integrate over zn in the E-step after applying a variational
bound  and maximize over W in the M-step. We focus on the case of continuous (Gaussian) and
categorical data. Our main contribution is the development of variational EM algorithms for factor
analysis and mixtures of factor analyzers based on a simple quadratic lower bound to the multinomial
likelihood (which subsumes the Bernoulli case) [Boh92]. This bound results in an EM iteration that
is computationally more efﬁcient than the bound previously proposed by Jaakkola for binary PCA
when the training data is fully observed [JJ96]  but is less tight. The proposed bound has advantages
relative to other previously introduced bounds  as we discuss in the following sections.

2 The Generalized Mixture of Factor Analyzers Model

In this section  we describe a model for mixed continuous and discrete data that we call the gen-
eralized mixture of factor analyzers model. This model has two important special cases: mixture
models and factor analysis  both for mixed continuous and discrete data. We use the general model
as well as both special cases in subsequent experiments. In this work  we focus on Gaussian dis-
tributed continuous data and multinomially distributed discrete data. The graphical model is given
in Figure 1 while the probabilistic model is given in Equations 1 to 4. We begin with a description
of the the general model and then highlight the two special cases.
We let n ∈ {1 . . . N} index data cases  d ∈ {1 . . . Dd} index discrete data dimensions and k ∈
{1 . . . K} index mixture components. Superscripts C and D indicate variables associated with
n ∈ RDc denote the continuous data vector and
continuous and discrete data respectively. We let yC

2

ΣCkµCkWCkkYCnYDndqnznπλzµDdkWDdkkndλwnd ∈ {1 . . . M + 1} denote the dth discrete data variable.1 We use a 1-of-(M + 1) encoding for the
yD
nd = m is represented by a (M + 1)-dimensional vector yD
discrete variables where a variable yD
nd
in which m’th element is set to 1  and all remaining elements equal 0. We denote the complete data

vector by yn =(cid:2)yC

(cid:3).

n   yD

n1  . . .   yD

nDd

The generative process begins by sampling a state of the mixture indicator variable qn for each data
case n from a K-state multinomial distribution with parameters π. Simultaneously  a length L latent
factor vector zn ∈ RL is sampled from a zero-mean Gaussian distribution with precision parameter
λz. Both steps are given in Equation 1. The natural parameters of the distribution over the data
variables is obtained by passing the latent factor vector zn through a linear function deﬁned by a
factor loading matrix and an offset term  both of which depend on the setting of the mixture indicator
variable qn.

p(zn  qn|θ) = N (zn|0  λ−1
n |WC

p(yn|zn  qn = k  θ) = N (yC

z IL)M(qn|π)

k zn + µC

k )
k   ΣC

ηndk = WD
Sm(η) = exp[ηm − lse(η)]

dkzn + µD

dk

M +1X

lse(η) = log[

exp(ηm)]

DdY

d=1

M(yD

nd|S(ηndk))

(1)

(2)

(3)
(4)

(5)

m=1

k   µD

1k  µD

dkzn + µD

k   WD

1k  WD

2k  . . .   WD

k ∈ RDc and µD

k   and each discrete data variable yD

n is Gaussian distributed with mean WC

dk ∈ RM +1×L  while the offsets are µC

parameter vector α such thatP

k zn +
Assuming that qn = k  the continuous data vector yC
k and covariance ΣC
nd is multinomially distributed with natural
µC
dk  as seen in Equation 2. Here  N (·|m  V) denotes a Gaussian
parameters ηndk = WD
distribution with mean m and covariance V  while M(·|α) denotes a multinomial distribution with
i αi = 1 and αi ≥ 0. For the discrete data variables  the natural
parameter vector is converted into the standard mean parameter vector through the softmax function
S(η) = [S1(η)  . . .  SM +1(η)]  where Sm(η) is deﬁned in Equation 4. The softmax function
Sm(η) is itself deﬁned in terms of the log-sum-exp (LSE) function  which we give in Equation 5.
k ∈ RDc×L and
We note that the factor loading matrices for the kth mixture component are WC
dk ∈ RM +1. We deﬁne the en-
WD
Ddk] and
semble of factor loading matrices and offsets to be Wk = [WC
Ddk]  respectively. The complete set of parameters for this model is
µk = [µC
2k  . . .   µD
thus θ = {W1:K  µ1:K  ΣC
1:K  π  λz}. To complete the model speciﬁcation  we must specify the
prior on these parameters. For each row of each factor loading matrix Wk  we use a Gaussian prior
of the form N (0  λ−1
As mentioned at the start of this section  this general model has two important special cases: general-
ized factor analysis and mixture models for mixed continuous and discrete data. The factor analysis
model is obtained by using one mixture component and at least one latent factor (K = 1  L > 1).
The mixture model is obtained by using no latent factors and at least one mixture component
(K > 1  L = 0). In the mixture model case where L = 0  the distribution is modeled through
the offset parameters µk only. We will compare these three models in Section 5.
Before concluding this section  we point out one key difference between the current model and
other latent factor models for discrete data like multinomial PCA [BJ04] and latent Dirichlet allo-
cation (LDA) [BNJ03]. In our model  the natural parameters for discrete data are deﬁned on a low-
dimensional linear subspace and are mapped to the mean parameters via the softmax function. In
multinomial PCA and LDA  the mean parameters are instead directly deﬁned on a low-dimensional
linear subspace. The latter approach can also be extended to the mixed-data case [BDdF+03]. How-
ever  model ﬁtting is even more computationally challenging than in our approach.
In fact  the
bounds we propose can be used in this alternative setting  but we leave this to future work.

w I). We use vague conjugate priors for the remaining parameters.

1Note that we assume all the discrete data variables have the same number of states  namely M + 1  for

notational simplicity only. In the general case  the dth discrete variable has Md + 1 states.

3

3 Variational Bounds for Model Fitting

In the standard expectation-maximization (EM) algorithm for mixtures of factor analyzers  the E-
step consists of marginalizing over the complete-data log likelihood with respect to the posterior
over the mixture indicator variable qn and latent factors zn. The M-step consists of maximizing
the expected complete log likelihood with respect to the parameters θ.
In the case of Gaussian
observations  this posterior is available in closed form because of conjugacy. Introduction of discrete
observations  however  makes it intractable to compute the posterior as the likelihood for these
observations is not conjugate to the Gaussian prior on the latent factors.
To overcome these problems  we propose to use a quadratic bound on the LSE function. This allows
us to obtain closed form updates for both the E and M steps. We use the quadratic bound described
in [Boh92]. In rest of the paper  we will refer to it as the “Bohning bound”. For simplicity  we
describe the bound only for one discrete measurement with K = 1 and µk = 0 in order to suppress
the n  k and d subscripts. To ensure identiﬁability  we assume that the last element of η is zero (this
can be enforced by setting the last row of W to zero).
The key idea behind the Bohning bound is to take a second order Taylor series expansion of the LSE
function around a point ψ. An upper bound to the LSE function is found by replacing the Hessian
matrix H(ψ)  which appears in the second order term  with a ﬁxed matrix A such that A− H(ψ) is
positive deﬁnite for all ψ [Boh92]. Bohning gives one such matrix A  which we deﬁne below. The
expansion point ψ is a free variational parameter that must be optimized.

lse(η) ≤ 1
2 ηT Aη − bT
1
[IM − 1M 1T
2

A =
bψ = Aψ − S(ψ)
cψ =

ψ η + cψ

M /(M + 1)]

(6)

(7)

(8)

1
2 ψT Aψ − S(ψ)T ψ + lse(ψ)

(9)
ψ ∈ RM is the vector of variational parameters  IM is the identity matrix of size M × M and 1M
is a vector of ones of length M. By substituting this bound in to the log-likelihood  completing
the square and exponentiating  we obtain the Gaussian lower bound described below. We obtain a
Gaussian-like “pseudo” observation ˜yψ corresponding to the discrete observation yD.

p(yD|z  W) ≥ h(ψ)N (˜yψ|Wz  A−1)

˜yψ = A−1(bψ + yD)

h(ψ) = |2πA−1| 1

ψA˜yψ − cψ
˜yT

2 exp(cid:2)1

2

(cid:3)

(10)
(11)

(12)

We use this result to obtain a lower bound for each mixed data vector yn. We will suppress the
ψ subscripts  which differ for each data point n and each discrete variable d for clarity. Let ˜yn =
n   ˜y1 n  . . .   ˜yDd n] be the data vector for a given n and ψ. It is straightforward to show that this
[yC
observation gives the following lower bound on the joint likelihood 
p(˜yn|zn) = N (˜yn| ˜Wzn  ˜Σ) 
1   . . .   A−1
)
Given this pseudo observation  the computation of the posterior means mn and covariances Vn is
similar to the Gaussian FA model as seen below. This result can be generalized to the mixture case in
a straightforward way. The M-step is the same as in mixtures of Gaussian factor analyzers [GH96].

˜W =(cid:2)WC  WD

˜Σ = diag(ΣC  A−1

1   . . .   wD
Dd

(cid:3)  

Dd

Vn = ( ˜WT ˜Σ

−1 ˜W + λzIL)−1  mn = Vn ˜WT ˜Σ

(13)
The only question remaining is how to obtain the value of ψ. By maximizing the lower bound  one
can show that the optimal value is ψn = ˜Wmn. This follows from the fact that the Bohning bound
is tight for lse(η) when ψ = η  and that the curvature is independent of η [Boh92]. We iterate this
update until convergence. In practice  we ﬁnd that the method usually converges in ﬁve or fewer
iterations.
The most attractive feature of the bound described above is its computational efﬁciency. To see
this  note that the posterior covariance Vn does not in fact depend on n if the data vector is fully

−1˜yn

4

observed  since A is a constant matrix. Consequently we need only invert Vn once outside the EM
loop instead of N times  once for each data point. We will see in the next section that the other
existing quadratic bounds do not have this property. To derive the overall computational cost of our
EM algorithm  let us deﬁne the total dimension of ˜yn to be D and assume K = 1. Computing Vn
takes O(L3 + L2D) time  and computing each mn takes O(L2 + LD) time. So the total cost of one
E-step is O(L3 + L2D + N I(L2 + LD))  where I is the number of variational updates. If there is
missing data  Vn will change across data cases  so the total cost will be O(N I(L3 + L2D)).

3.1 Comparison with Other Bounding Methods

2 ξ + log(1 + eξ)  λξ = 1
2ξ (

1

2

In the binary case  the Bohning bound reduces to the following: log(1 + eη) ≤ 1
2 Aη2 − bψη + cψ 
2 Aψ2 − (1 + e−ψ)−1ψ + log(1 + eψ). It is
where A = 1/4  bψ = Aψ − (1 + e−ψ)−1  and cψ = 1
interesting to compare this bound to Jaakkola’s bound [JJ96] used in [Tip98  YT04]. This bound can
also be written in the quadratic form: log(1 + eη) ≤ 1
˜Aξη2 −˜bξη + ˜cξ  where ˜Aξ = 2λξ  ˜bξ = − 1
2 
˜cξ = −λξξ2 − 1
1+e−ξ − 1
2).
Although the Jaakkola bound is tighter than the Bohning bound  it has higher computational com-
plexity. The reason is that the ˜Aξ parameter depends on ξ and hence on n  which means we need to
compute a different posterior covariance matrix for each n. Consequently  the cost of an E-step is
O(N I(L3 + L2D))  even if there is no missing data (note the L3 term inside the N I loop).
To explore the speed vs accuracy trade-off  we use the synthetic binary data described in [MHG08]
with N = 600  D = 16  and 10% missing data. We learn a binary FA model with L = 10  λz = 1 
and λw = 0. We learn on the observed entries in the data matrix and compute the mean squared error
(MSE) on the held out missing entries as in [MHG08]. We average the results over 20 repetitions
of the experiment. We see in Figure 2 (top left) that the Jaakkola bound gives a lower MSE than
Bohning’s bound in less time on this data. Next  we consider the case where the training data is fully
observed using a modiﬁed version of the data generating procedure described in [MHG08]. We vary
D from 16 to 128 while setting L = 0.25D and N = 10D. We sample L different binary prototypes
at random  assign each data case to a prototype  and add 10% random binary noise. We measure the
average time per iteration over 40 iterations of each method. Figure 2 (bottom left) shows that the
Bohning bound exhibits much better scalability per iteration than the Jaakkola bound in this regime.
The speed issue becomes more serious when combining binary variables with categorical variables.
Firstly  there is no direct extension of the Jaakkola bound to the general categorical case. Hence 
to combine categorical variables with binary variables  we can use the Jaakkola bound for binary
and the Bohning for the rest. However  this is not computationally efﬁcient as we need to compute
the posterior covariance for each data point because of the Jaakkola bound. For computational
simplicity  we use Bohning’s bound for both binary and categorical data.
Various other bounds and approximations to the multinomial likelihood also exist; however  they
are all more computationally intensive  and do not give an efﬁcient variational algorithm. To the
best of our knowledge these methods have not been applied to the FA model  but we describe them
brieﬂy for completeness. An extension of the Jaakkola bound to the multinomial case was given in
[Bou07]. However  this tends to be less accurate than the Bohning bound. Another approach [BL06]
j=1 exp(ηj))−log ν−1  where
ν is a variational parameter. This bound does not give closed form updates for the E and M steps so
a numerical optimizer needs to be used (see [BL06] for details).
Instead of using a bound  an alternative approach is to apply a quadratic approximation derived
from a Taylor series expansion of the LSE function [AX07]. This provides a tighter approximation
that could perform better than a bound  but one cannot make convergence guarantees when using
it inside of EM. In practice we found this alternative approach to be very slow on the datasets that
we consider. In view of its speed and simplicity  we will only consider the Bohning method for the
remainder of the paper.

is to use the concavity of the log function to write lse(η) ≤ ν(1+PM

5

Figure 2: Top left: accuracy vs speed of variational EM with the Bohning bound (FA-VM)  Jaakkola bound
(FA-VJM) and HMC (FA-SS) on synthetic binary data. Bottom left: Time per iteration of EM with Bohning
bound and Jaakkola bound as we vary D. Right: MSE vs λw for FA-MM  FA-VM  and FA-SS on synthetic
Gaussian data. We show results on the test and training sets  for 10% and 50% missing data.

4 Alternative Estimation Approaches

In this section  we discuss several alternative methods for ﬁtting the generalized FA model in the
case K = 1  which we compare to the VM method. We defer comparisons of FA to mixture models
to Section 5.

4.1 Maximize-Maximize (MM) Method
The simplest approach to ﬁt the FA model is to maximize log p(Y  Z  W|λw  λz) with respect to
Z and W  the matrix of latent factor values and the factor loading matrix. It is straightforward to
compute the gradient of the log posterior and apply a generic optimizer (we use the limited-memory
quasi-newton method). Alternatively  one can use coordinate descent [CDS02]. We set the hyper-
parameters λw and λz by cross validation. To handle missing data  we simply evaluate the gradients
by only summing over the observed entries of Y. At test time  consider a data vector consisting
of missing and observed components  y∗ = [y∗m  y∗o]. To ﬁll in the missing entries  we compute
ˆz∗ = arg max p(z∗  y∗o| ˆW) and use it with ˆθ to predict y∗m.
The MM approach is simple and widely applicable  but these beneﬁts come at the expense of ignor-
ing the posterior variance of Z [WCS08]. This has negative consequences for the method in terms
of sensitivity to the parameters λw and λz. To illustrate this effect  we generate a continuous dataset
using D = 10  L = 5  and N = 200 data cases by sampling from the FA model. We set λw = 1 
λz = 1  and σc = 0.1. We standardize each data dimension to have unit variance and zero mean.
We consider the case of 10% and 50% missing data. We evaluate the sensitivity of the methods to
the setting of the posterior precision parameter λw by varying it over the range 10−2 to 102. We ﬁx
λz = 1  since this is the standard assumption when ﬁtting FA models. We run the methods on a
random 50/50 train/test split. We train on the observed entries in the training set  and then compute
MSE on the missing entries in the training and test sets. We average the results over 20 repetitions
of the experiment.
Figure 2 (top right) shows that the test MSE of the MM method is extremely sensitive to the prior
precision λw. We can see that this sensitivity increases as a function of the missing data rate.
We hypothesize that this is a result of the MM method ignoring the posterior uncertainty in Z.

6

1001011020.050.10.15Accuracy vs SpeedMSETime (s)  FA−VMFA−VJMFA−SS10−210010210−1100101Test Sensitivity: 10% Mis.MSEPrior Strength (λW)  FA−VMFA−MMFA−SS10−210010210−1100101Test Sensitivity: 50% Mis.MSEPrior Strength (λW)16326412800.511.522.5ScalabilityTime per Iteration (s)Data Dimension (D)  FA−VMFA−VJM10−210010210−310−210−1100Train Sensitivity: 10% Mis.Train MSEPrior Strength (λW)10−210010210−310−210−1100Train Sensitivity: 50% Mis.Train MSEPrior Strength (λW)This is supported by looking at the MSE on the training set  Figure 2 (bottom right). We see that
the MM method overﬁts when λw is small. Consequently  MM requires a careful discrete search
over the values of λw  which is slow  since the quality of each such value must be estimated by
cross-validation. By contrast  the VM method takes the posterior uncertainty about Z into account 
resulting in almost no sensitivity to λw over this range. Henceforth we set λw = 0 for VM  meaning
we are performing (approximate) maximum likelihood parameter estimation.

4.2 Sample-Sample (SS) Method

An alternative to the MM approach is to sample both Z and W from their posteriors using Hamil-
tonian Monte Carlo (HMC) [MHG08]. We call this the “SS” method  since we sample both Z and
W. HMC leverages the fact that we can compute the gradient of the log posterior in closed form.
However  it has several important parameters that must be set including the step size  the momentum
distribution  the number of leapfrog steps  etc.
To handle missing data  we can simply evaluate the gradients by only summing over the observed
entries of Y. We do not need to impute the missing entries on the training set. At test time  we
have a collection of samples of W. For each sample of W and each test case  we sample a set of z 
and compute an averaged prediction for ym. In Figure 2 (right)  we see that SS is insensitive to λw 
just like VM  since it also models posterior uncertainty in Z (note that the absolute MSE values are
higher for SS than VM since for continuous data  VM corresponds to EM with an exact posterior).
However  in Figure 2 (top left)  we see that SS can be much slower than VM. In the remainder of
the paper we focus on deterministic ﬁtting methods only.

5 Experiments on Real Data

In this section  we evaluate the performance of our model on real data with mixed continuous and
discrete variables. We consider the following three cases of our model: (1) a model with latent
factors but no mixtures (FA) (2) a model with mixtures but no latent factors (Mix) and (3) the
general mixture of factor analyzers model (MixFA). To learn the FA model  we consider the FA-
MM and FA-VM approaches. For the Mix model  we use the standard EM algorithm. In the Mix
model  continuous variables can be modeled with either a diagonal or a full covariance matrix. We
refer to these two variants as Mix-Diag and Mix-Full. For MixFA model  we use the VM approach.
This gives us ﬁve methods: FA-MM  FA-VM  MixFA  Mix-Full and Mix-Diag.
We consider three real datasets of different sizes (see the table in Figure 3).2 For each dataset  we use
70% for training  10% for validation and 20% for testing. We consider 20 splits for each dataset. We
use the validation set to determine the number of latent factors and the number of mixtures (ranges
shown in the table) with imputation error (described below) as our performance objective. For the
FA-MM method  we set the values of the regularization parameters λz and λw by cross validation.
We use the range {0.01  0.1  1  10  100} for both λz and λw . As VM is robust to the setting of these
parameters  we set λz = 1 and λw = 0.
One way to assess the performance of a generative model is to see how well it can impute missing
data. We do this by randomly introducing missing values in the test data with a missing data rate
of 0.3. For continuous variables  we compute the imputation MSE averaged over all the missing
values (these variables are standardized beforehand). For discrete variables  we report the cross-
entropy (averaged over missing values) deﬁned as yT log ˆp  where ˆpm is the estimated probability
that y = m and y uses the one-of-(M + 1) encoding.
These errors are shown in Figure 3 along with the running time for ASES dataset in the bottom
right subﬁgure. We see that FA-VM consistently performs better than FA-MM for all the datasets.
Moreover  because of the need for cross-validation  FA-MM takes more time than FA-VM. We also
see that the Mix model  although faster  performs worse than FA-VM. Finally  as expected  MixFA
generally performs slightly better than FA  but takes longer to run.

2Adult and Auto are available in UCI repository  while ASES dataset is a subset of Asia-Europe Survey

from www.icpsr.umich.edu

7

Dataset Details

Auto

Adult

ASES

45222
5
27
4
31
4  15 
31
1  5 
10  20

16815
42
156
0
156
20  40 
60  80
1  10  20 
30  40

N
Dd

P Md

Dc
D

L

K

392
3
21
5
26
5  13 
26
1  5 
10  20

Figure 3: Left: the table shows the details of each dataset used. Here D = Dc +P Md is the total size of

the data vector. L and K are the ranges of number of latent factors and mixture components used for cross
validation. Note that the maximum value of L is D  as required by the FA model. Right: the ﬁgure shows the
imputation error for each dataset for continuous and discrete variables. The bottom right subﬁgure shows the
timing comparison for the ASES dataset.

6 Discussion and Future Work

In this work we have proposed a new variational EM algorithm for ﬁtting factor analysis models
with mixed data. The algorithm is based on the Bohning bound  a simple quadratic bound to the
log-sum-exp function. In the special case of fully observed binary data  the Bohning bound itera-
tion is theoretically faster than Jaakkola’s bound iteration and we have demonstrated this advantage
empirically. More importantly  the Bohning bound also easily extends to the categorical case. This
enables  for the ﬁrst time  an efﬁcient variational method for ﬁtting a factor analysis model to mixed
continuous  binary  and categorical observations.
In comparison to the maximize-maximize (MM) method  which forms the basis of ePCA and other
matrix factorization methods  our variational EM method accounts for posterior uncertainty in the
latent factors  leading to reduced sensitivity to hyper parameters. This has important practical con-
sequences as the MM method requires extensive cross validation while our approach does not.
We have compared a range of models and algorithms in terms of imputation performance on real
data. This analysis shows that the cost of the cross validation search for MM is higher than the cost
of ﬁtting the FA model using our method. It also shows that standard alternatives to FA  such as
ﬁnite mixture models  do not perform as well as FA. Finally  we show that the MixFA model can
yield a performance improvement over a single FA model  although at a higher computational cost.
We note that the quadratic bound that we study can be used in a variety of other models  such as
linear-Gaussian state-space models with categorical observations [SH03]. It might be an interesting
alternative to a Laplace approximation to the posterior  which is used in [KPBSK10  RMC09]. The
bound might also be useful in the context of the correlated topic model [BL06  AX07]  where similar
variational EM methods have been applied.
In the Bayesian statistics literature  it is common to use latent factor models combined with a pro-
bit observation model; this allows one to perform inference for the latent states using efﬁcient
auxiliary-variable MCMC techniques (see e.g.  [HSC09  Dun07]). Additionally  the recently pro-
posed Riemannian Manifold Hamiltonian Monte Carlo sampler [GCC09] may signiﬁcantly speed-
up sampling-based approaches for mixed-data factor analysis models. We leave a comparison to
these approaches to future work.

Acknowledgments

We would like to thank the reviewers for their helpful coments. This work was completed in part at
the Xerox Research Center Europe and was supported by the Paciﬁc Institute for the Mathematical
Sciences and the Killam Trusts at the University of British Columbia.

8

0.40.50.6Error DiscreteAuto0.20.30.4Error ContinuousFA−MMFA−VMMixFAMix−FullMix−Diag0.40.50.6Error DiscreteAdult0.80.91Error ContinuousFA−MMFA−VMMixFAMix−FullMix−Diag0.40.5Error DiscreteASES100102104Time in secFA−MMFA−VMMixFAMix−FullMix−DiagReferences
[AX07]

A. Ahmed and E. Xing. On tight approximate inference of the logistic-normal topic
admixture model. In AI/Statistics  2007.

[BJ04]
[BL06]
[BNJ03]

[BDdF+03] Kobus Barnard  Pinar Duygulu  Nando de Freitas  David Forsyth  David Blei  and
Michael I. Jordan. Matching words and pictures. J. of Machine Learning Research 
3:1107–1135  2003.
W. Buntine and A. Jakulin. Applying Discrete PCA in Data Analysis. In UAI  2004.
D. Blei and J. Lafferty. Correlated topic models. In NIPS  2006.
D. Blei  A. Ng  and M. Jordan. Latent dirichlet allocation. J. of Machine Learning
Research  3:993–1022  2003.
D. Bohning. Multinomial logistic regression algorithm. Annals of the Inst. of Statistical
Math.  44:197–200  1992.
G. Bouchard. Efﬁcient bounds for the softmax and applications to approximate infer-
ence in hybrid models. In NIPS 2007 Workshop on Approximate Inference in Hybrid
Models  2007.

[Boh92]

[Bou07]

[CDS02] M. Collins  S. Dasgupta  and R. E. Schapire. A generalization of principal components

analysis to the exponential family. In NIPS-14  2002.
D. Dunson. Bayesian methods for latent trait modelling of longitudinal data. Stat.
Methods Med. Res.  16(5):399–415  Oct 2007.

[GCC09] M. Girolami  B. Calderhead  and S.A. Chin. Riemannian manifold hamiltonian monte

carlo. Arxiv preprint arXiv:0907.1100  2009.
Z. Ghahramani and G. Hinton. The EM algorithm for mixtures of factor analyzers.
Technical report  Dept. of Comp. Sci.  Uni. Toronto  1996.
P. R. Hahn  J. Scott  and C. Carvahlo. Sparse Factor-Analytic Probit Models. Technical
report  Duke  2009.
T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression
problems and their extensions. In AI/Statistics  1996.

[KPBSK10] S. Koyama  L. Perez-Bolde  C. Shalizi  and R. Kass. Approximate methods for state-

space models. Technical report  CMU  2010.
J. Li and D. Tao. Simple exponential family PCA. In AI/Statistics  2010.
S. Mohamed  K. Heller  and Z. Ghahramani. Bayesian Exponential Family PCA. In
NIPS  2008.
H. Rue  S. Martino  and N. Chopin. Approximate Bayesian Inference for Latent Gaus-
sian Models Using Integrated Nested Laplace Approximations. J. of Royal Stat. Soc.
Series B  71:319–392  2009.
S. Roweis. EM algorithms for PCA and SPCA. In NIPS  1997.
V. Siivola and A. Honkela. A state-space method for language modeling. In Proc.
IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)  pages
548–553  2003.
M. Tipping and C. Bishop. Probabilistic principal component analysis. J. of Royal
Stat. Soc. Series B  21(3):611–622  1999.
M. Tipping. Probabilistic visualization of high-dimensional binary data.
1998.

In NIPS 

[Dun07]

[GH96]

[HSC09]

[JJ96]

[LT10]
[MHG08]

[RMC09]

[Row97]
[SH03]

[TB99]

[Tip98]

[WK01]

[YT04]

[WCS08] Max Welling  Chaitanya Chemudugunta  and Nathan Sutter. Deterministic latent vari-

able models and their pitfalls. In Intl. Conf. on Data Mining  2008.
Michel Wedel and Wagner Kamakura. Factor analysis with (mixed) observed and
latent variables in the exponential family. Psychometrika  66(4):515–530  December
2001.
K. Yu and V. Tresp. Heterogenous data fusion via a probabilistic latent-variable model.
In Organic and Pervasive Computing (ARCS 2004)  2004.

9

,Antoine Wehenkel
Gilles Louppe