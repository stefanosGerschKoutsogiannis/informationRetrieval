2016,A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing,We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank $k$  our algorithm converges linearly  achieves $O(\epsilon)$ recovery error after retrieving $O(k^{3}d\log(1/\epsilon))$ training instances  consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence  endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM  our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems  such as inductive matrix completion and phase retrieval.,A Non-convex One-Pass Framework for Generalized
Factorization Machine and Rank-One Matrix Sensing

Ming Lin

University of Michigan
linmin@umich.edu

Abstract

Jieping Ye

University of Michigan

jpye@umich.edu

We develop an efﬁcient alternating framework for learning a generalized version of
Factorization Machine (gFM) on steaming data with provable guarantees. When
the instances are sampled from d dimensional random Gaussian vectors and the
target second order coefﬁcient matrix in gFM is of rank k  our algorithm converges
linearly  achieves O() recovery error after retrieving O(k3d log(1/)) training
instances  consumes O(kd) memory in one-pass of dataset and only requires matrix-
vector product operations in each iteration. The key ingredient of our framework is
a construction of an estimation sequence endowed with a so-called Conditionally
Independent RIP condition (CI-RIP). As special cases of gFM  our framework can
be applied to symmetric or asymmetric rank-one matrix sensing problems  such as
inductive matrix completion and phase retrieval.

1

Introduction

Linear models are one of the foundations of modern machine learning due to their strong learning
guarantees and efﬁcient solvers [Koltchinskii  2011]. Conventionally linear models only consider the
ﬁrst order information of the input feature which limits their capacity in non-linear problems. Among
various efforts extending linear models to the non-linear domain  the Factorization Machine [Rendle 
2010] (FM) captures the second order information by modeling the pairwise feature interaction in
regression under low-rank constraints. FMs have been found successful in many applications  such as
recommendation systems [Rendle et al.  2011] and text retrieval [Hong et al.  2013]. In this paper  we
consider a generalized version of FM called gFM which removes several redundant constraints in
the original FM such as positive semi-deﬁnite and zero-diagonal  leading to a more general model
without sacriﬁcing its learning ability. From theoretical side  the gFM includes rank-one matrix
sensing [Zhong et al.  2015  Chen et al.  2015  Cai and Zhang  2015  Kueng et al.  2014] as a special
case  where the latter one has been studied widely in context such as inductive matrix completion
[Jain and Dhillon  2013] and phase retrieval [Candes et al.  2011].
Despite of the popularity of FMs in industry  there is rare theoretical study of learning guarantees for
FMs. One of the main challenges in developing a provable FM algorithm is to handle its symmetric
rank-one matrix sensing operator. For conventional matrix sensing problems where the matrix sensing
operator is RIP  there are several alternating methods with provable guarantees [Hardt  2013  Jain
et al.  2013  Hardt and Wootters  2014  Zhao et al.  2015a b]. However  for a symmetric rank-one
matrix sensing operator  the RIP condition doesn’t hold trivially which turns out to be the main
difﬁculty in designing efﬁcient provable FM solvers.
In rank-one matrix sensing  when the sensing operator is asymmetric  the problem is also known as
inductive matrix completion which can be solved via alternating minimization with a global linear
convergence rate [Jain and Dhillon  2013  Zhong et al.  2015]. For symmetric rank-one matrix sensing
operators  we are not aware of any efﬁcient solver by the time of writing this paper. In a special case
when the target matrix is of rank one  the problem is called “phase retrieval” whose convex solver

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

is ﬁrst proposed by Candes et al. [2011] then alternating methods are provided in [Lee et al.  2013 
Netrapalli et al.  2013]. While the target matrix is of rank k > 1   only convex methods minimizing
the trace norm have been proposed recently  which are computationally expensive [Kueng et al.  2014 
Cai and Zhang  2015  Chen et al.  2015  Davenport and Romberg  2016].
Despite of the above fundamental challenges  extending rank-one matrix sensing algorithm to gFM
itself is difﬁcult. Please refer to Section 2.1 for an in-depth discussion. The main difﬁculty is due to
the ﬁrst order term in the gFM formulation  which cannot be trivially converted to a standard matrix
sensing problem.
In this paper  we develop a uniﬁed theoretical framework and an efﬁcient solver for generalized
Factorization Machine and its special cases such as rank-one matrix sensing  either symmetric or
asymmetric. The key ingredient is to show that the sensing operator in gFM satisﬁes a so-called
Conditionally Independent RIP condition (CI-RIP  see Deﬁnition 2) . Then we can construct an
estimation sequence via noisy power iteration [Hardt and Price  2013]. Unlike previous approaches 
our method does not require alternating minimization or choosing the step-size as in alternating
gradient descent. The proposed method works on steaming data  converges linearly and has O(kd)
space complexity for a d-dimension rank-k gFM model. The solver achieves O() recovery error
after retrieving O(k3d log(1/)) training instances.
The remainder of this paper is organized as following. In Section 2  we introduce necessary notation
and background of gFM. Subsection 2.1 investigates several fundamental challenges in depth. Section
3 presents our algorithm  called One-Pass gFM  followed by its theoretical guarantees. Our analysis
framework is presented in Section 4. Section 5 concludes this paper.

2 Generalized Factorization Machine (gFM)

In this section  we ﬁrst introduce necessary notation and background of FM and its generalized
version gFM. Then in Subsection 2.1  we reveal the connection between gFM and rank-one matrix
sensing followed by several fundamental challenges encountered when applying frameworks of
rank-one matrix sensing to gFM.
The FM predicts the labels of instances by not only their features but also high order interactions
between features. In the following  we focus on the second order FM due to its popularity. Suppose
we are given N training instances xi ∈ Rd independently and identically (I.I.D.) sampled from
the standard Gaussian distribution and so are their associated labels yi ∈ R. Denote the feature
matrix X = [x1  x2 ···   xn] ∈ Rd×n and the label vector y = [y1  y2 ···   yn](cid:62) ∈ Rn . In second
order FM  yi is assumed to be generated from a target vector w∗ ∈ Rd and a target rank k matrix
M∗ ∈ Rd×d satisfying

yi =xi

(cid:62)w∗ + xi

(cid:62)M∗xi + ξi

(1)

where ξi is a random subgaussian noise with proxy variance ξ2 .
to write Eq.
[(cid:104)A1  M(cid:105)  (cid:104)A2  M(cid:105)  ···  (cid:104)An  M(cid:105)](cid:62) where Ai = xixi

It is often more convenient
(1) in matrix form. Denote the linear operator A : Rd×d → Rn as A(M ) (cid:44)

(cid:62) . Then Eq. (1) has a compact form:

y = X(cid:62)w∗+A(M∗) + ξ .

(cid:62)M xi where the left/right design vectors (xi and xi

(2)
The FM model given by Eq. (2) consists of two components: the ﬁrst order component X(cid:62)w∗ and
the second order component A(M∗). The component A(M∗) is a symmetric rank-one Gaussian
measurement since Ai(M ) = xi
(cid:62)) are identical.
The original FM requires that M∗ should be positive semi-deﬁnite and the diagonal elements of M∗
should be zero. However our analysis shows that both constraints are redundant for learning Eq. 2.
Therefore in this paper we consider a generalized version of FM which we call gFM where M∗ is
only required to be symmetric and low rank. To make the recovery of M∗ well deﬁned  it is necessary
to assume M∗ to be symmetric. Indeed for any asymmetric matrix M∗  there is always a symmetric
matrix M∗
sym) thus the symmetric constraint does
not affect the model. Another standard assumption in rank-one matrix sensing is that the rank of M∗
should be no more than k for k (cid:28) d. When w∗ = 0  gFM is equal to the symmetric rank-one matrix
sensing problem. Recent researches have proposed several convex programming methods based on
the trace norm minimization to recover M∗ with a sampling complexity on order of O(k3d) [Candes

sym = (M∗ + M∗(cid:62))/2 such that A(M∗) = A(M∗

2

et al.  2011  Cai and Zhang  2015  Kueng et al.  2014  Chen et al.  2015  Zhong et al.  2015]. Some
authors also call gFM as second order polynomial network [Blondel et al.  2016].
When d is much larger than k  the convex programming on the trace norm or nuclear norm of M∗
becomes difﬁcult since M∗ can be a d × d dense matrix. Although modern convex solvers can scale
to large d with reasonable computational cost  a more popular strategy to efﬁciently estimate w∗
and M∗ is to decompose M∗ as U V (cid:62) for some U  V ∈ Rd×k  then alternatively update w  U  V to
minimize the empirical loss function

min
w U V

1
2N

(cid:107)y − X(cid:62)w − A(U V (cid:62))(cid:107)2
2 .

(3)

The loss function in Eq. (3) is non-convex. It is even unclear whether an estimator of the optimal
solution {w∗  M∗} of Eq. (3) with a polynomial time complexity exists or not.
In our analysis  we denote M + O() as a matrix M plus a perturbation matrix whose spectral
norm is bounded by . We use (cid:107) · (cid:107)2   (cid:107) · (cid:107)F   (cid:107) · (cid:107)∗ to denote the matrix spectral norm  Frobenius
norm and nuclear norm respectively. To abbreviate the high probability bound  we denote C =
polylog(d  n  T  1/η) to be a constant polynomial logarithmic in {d  n  T  1/η}. The eigenvalue
decomposition of M∗ is M∗ = U∗Λ∗U∗(cid:62) where U∗ ∈ Rd×k is the top-k eigenvectors of M∗
k) are the corresponding eigenvalues sorted by |λi| ≥ |λi+1|. Let
and Λ∗ = diag(λ∗
i | denote the singular value of M∗ and σi{M} be the i-th largest singular value of M. U∗
i = |λ∗
σ∗
⊥
denotes an matrix whose columns are the orthogonal basis of the complementary subspace of U∗.

2 ···   λ∗

1  λ∗

i

(M ) = ui

gFM and Rank-One Matrix Sensing

2.1
When w∗ = 0 in Eq. (1)  the gFM becomes the symmetric rank-one matrix sensing problem.
While the recovery ability of rank-one matrix sensing is somehow provable recently despite of the
computational issue  it is not the case for gFM. It is therefore important to discuss the differences
between gFM and rank-one matrix sensing to give us a better understanding of the fundamental
barriers in developing provable gFM algorithm.
In the rank-one matrix sensing problem  a relaxed setting is to assume that the sensing operator is
asymmetric  which is deﬁned by Aasy
(cid:62)M vi where ui and vi are independent random
vectors. Under this setting  the recovery ability of alternating methods is provable [Jain and Dhillon 
2013]. However  existing analyses cannot be generalized to their symmetric counterpart  since ui
and vi are not allowed to be dependent in these frameworks. For example  the sensing operator
Aasy(·) is unbiased ( EAasy(·) = 0) but the symmetric sensing operator is clearly not [Cai and
Zhang  2015]. Therefore  the asymmetric setting oversimpliﬁes the problem and loses important
structure information which is critical to gFM.
As for the symmetric rank-one matrix sensing operator  the state-of-the-art estimator is based on the
trace norm convex optimization [Tropp  2014  Chen et al.  2015  Cai and Zhang  2015]  which is
computationally expensive. When w∗ (cid:54)= 0  the gFM has an extra perturbation term X(cid:62)w∗ . This
ﬁrst order perturbation term turns out to be a fundamental challenge in theoretical analysis. One might
attempt to merge w∗ into M∗ in order to convert gFM as a rank (k + 1) matrix sensing problem. For
example  one may extend the feature ˆxi (cid:44) [xi  1](cid:62) and the matrix ˆM∗ = [M∗; w∗(cid:62)] ∈ R(d+1)×d.
However  after this simple extension  the sensing operator becomes ˆA(M∗) = ˆxi
(cid:62) ˆM∗xi. It is no
longer symmetric. The left/right design vector is neither independent nor identical. Especially  not
all dimensions of ˆxi are random variables. According to the above discussion  the conditions to
guarantee the success of rank-one matrix sensing do not hold after feature extension and all the
mentioned analyses cannot be directly applied.

3 One-Pass gFM

In this section  we present the proposed algorithm  called One-Pass gFM followed by its theoretical
guarantees. We will focus on the intuition of our algorithm. A rigorous theoretical analysis is
presented in the next section.
The One-Pass gFM is a mini-batch algorithm. In each mini-batch  it processes n training instances
and then alternatively updates parameters. The iteration will continue until T mini-batch updates.

3

Algorithm 1 One-Pass gFM
Require: The mini-batch size n  number of total mini-batch update T   training instances X =

[x1  x2 ··· xnT}  y = [y1  y2 ···   ynT ](cid:62)  desired rank k ≥ 1.

h(t)
2

(cid:44) 1

n 1(cid:62)(y − A(M (t)) − X (t)(cid:62)w(t))   h(t)

Ensure: w(T )  U (T )  V (T ).
1: Deﬁne M (t) (cid:44) (U (t)V (t)(cid:62) + V (t)U (t)(cid:62))/2   H (t)
(cid:44) 1
1 − 1
2: Initialize: w(0) = 0  V (0) = 0. U (0) = SVD(H (0)
3: for t = 1  2 ···   T do
4:

vectors.

3

1

(cid:44) 1

2nA(cid:48)(y − A(M (t)) − X (t)(cid:62)w(t))  

n X (t)(y − A(M (t)) − X (t)(cid:62)w(t)) .

2 h(0)

2 I  k)  that is  the top-k left singular

Retrieve n training instances X (t) = [x(t−1)n+1 ···   x(t−1)n+n] . Deﬁne A(M ) (cid:44)
(cid:62)M X (t)
[X (t)
i=1.
]n
ˆU (t) = (H (t−1)
− 1
Orthogonalize ˆU (t) via QR decomposition: U (t) = QR

I + M (t−1)(cid:62))U (t−1) .

(cid:16) ˆU (t)(cid:17)

2 h(t−1)

.

1

i

i

2

5:
6:
7: w(t) = h(t−1)
+ w(t−1) .
V (t) = (H (t−1)
2 h(t−1)
− 1
8:
9: end for
10: Output: w(T )  U (T )  V (T ) .

2

1

3

I + M (t−1))U (t)

2nA(cid:48)(y) − 1

Since gFM deals with a non-convex learning problem  the conventional gradient descent framework
hardly works to show the global convergence. Instead  our method is based on a construction
Intuitively  when w∗ = 0  we will show in the next section that
of an estimation sequence.
nA(cid:48)A(M ) ≈ 2M + tr(M )I and tr(M ) ≈ 1
n 1(cid:62)A(M ). Since y ≈ A(M∗)  we can estimate
1
n 1(cid:62)yI. But this simple construction cannot generate a convergent estimation
M∗ via 1
sequence since the perturbation terms in the above approximate equalities cannot be reduced along
iterations. To overcome this problem  we replace A(M∗) with A(M∗ − M (t)) in our construction.
Then the perturbation terms will be on order of O((cid:107)M∗ − M (t)(cid:107)2). When w∗ (cid:54)= 0  we can apply a
similar trick to construct its estimation sequence via the second and the third order moments of X.
Algorithm 1 gives a step-by-step description of our algorithm1.
In Algorithm 1  we only need to store w(t) ∈ Rd  U (t)  V (t) ∈ Rd×k. Therefore the space com-
plexity is O(d + kd). The auxiliary variables M (t)  H (t)
can be implicitly presented
by w(t)  U (t)  V (t). In each mini-batch updating  we only need matrix-vector product operations
which can be efﬁciently implemented on many computation architectures. We use truncated SVD
to initialize gFM  a standard initialization step in matrix sensing. We do not require this step to
be computed exactly but up to an accuracy of O(δ) where δ is the RIP constant. The QR step on
line 6 requires O(k2d) operations. Compared with SVD which requires O(kd2) operations  the QR
step is much more efﬁcient when d (cid:29) k. Algorithm 1 retrieves instances streamingly  a favorable
behavior on systems with high speed cache. Finally  we export w(T )  U (T )  V (T ) as our estimation
of w∗ ≈ w(T ) and M∗ ≈ U (T )V (T )(cid:62).
Our main theoretical result is presented in the following theorem  which gives the convergence rate
of recovery and sampling complexity of gFM when M∗ is low rank and the noise ξ = 0.
Theorem 1. Suppose xi’s are independently sampled from the standard Gaussian distribution. M∗
is a rank k matrix. The noise ξ = 0. Then with a probability at least 1 − η  there exists a constant C
and a constant δ < 1 such that

2   h(t)

1   h(t)

3

provided n ≥ C(4

√

(cid:107)w∗ − w(t)(cid:107)2 + (cid:107)M∗ − M (t)(cid:107)2 ≤δt((cid:107)w∗(cid:107)2 + (cid:107)M∗(cid:107)2)
1/σ∗
5σ∗

k + 3)2k3d/δ2  δ ≤

k+3)σ∗
√

√
5σ∗
(4
1 +3σ∗
5σ∗

1 /σ∗
k+4

5(cid:107)w∗(cid:107)2

√
4

.

k

2

Theorem 1 shows that {w(t)  M (t)} will converge to {w∗  M∗} linearly. The convergence rate is
n). A small δ will result in a fast convergence rate
controlled by δ  whose value is on order of O(1/

√

1Implementation is available from https://minglin-home.github.io/

4

but a large sampling complexity. To reduce the sampling complexity  a large δ is preferred. The largest
allowed δ is bounded by O(1/((cid:107)M∗(cid:107)2 + (cid:107)w∗(cid:107)2)). The sampling complexity is O((σ∗
k)2k3d).
If M∗ is not well conditioned  it is possible to remove (σ∗
k)2 in the sampling complexity by a
procedure called “soft-deﬂation” [Jain et al.  2013  Hardt and Wootters  2014]. By theorem 1  gFM
achieves  recovery error after retrieving nT = O(k3d log (((cid:107)w∗(cid:107)2 + (cid:107)M∗(cid:107)2)/)) instances.
The noisy case where M∗ is not exactly low rank and ξ > 0 is more intricate therefore we postpone
it to Subsection 4.1. The main conclusion is similar to the noise-free case Theorem 1 under a small
noise assumption.

1/σ∗

1/σ∗

4 Theoretical Analysis

In this section  we give the sketch of our proof of Theorem 1. Omitted details are postponed to
appendix.

From high level  our proof constructs an estimation sequence {(cid:101)w(t) (cid:102)M (t)  t} such that t → 0 and
(cid:107)w∗ − (cid:101)w(t)(cid:107)2 + (cid:107)M∗ −(cid:102)M (t)(cid:107)2 ≤ t . In conventional matrix sensing  this construction is possible

when the sensing matrix satisﬁes the Restricted Isometric Property (RIP) [Candès and Recht  2009]:
Deﬁnition 2 ((cid:96)2-norm RIP). A sensing operator A is (cid:96)2-norm δk-RIP if for any rank k matrix M 

(1 − δk)(cid:107)M(cid:107)2

F ≤ 1
n

(cid:107)A(M )(cid:107)2

2 ≤ (1 + δk)(cid:107)M(cid:107)2
F .

When A is (cid:96)2-norm δk-RIP for any rank k matrix M  A(cid:48)A is nearly isometric [Jain et al.  2012]  which
implies (cid:107)M − A(cid:48)A(M )/n(cid:107)2 ≤ δ. Then we can construct our estimation sequence as following:

(cid:102)M (t) =

1
n

A(cid:48)A(M∗ −(cid:102)M (t−1)) +(cid:102)M (t−1)   (cid:101)w(t) = (I − 1

XX(cid:62))(w∗ − (cid:101)w(t−1)) + (cid:101)w(t−1) .

n

However  in gFM and symmetric rank-one matrix sensing  the (cid:96)2-norm RIP condition cannot be
satisﬁed with high probability [Cai and Zhang  2015]. To establish an RIP-like condition for rank-one
matrix sensing  several variants have been proposed  such as the (cid:96)2/(cid:96)1-RIP condition [Cai and Zhang 
2015  Chen et al.  2015]. The essential idea of these variants is to replace the (cid:96)2-norm (cid:107)A(M )(cid:107)2 with
(cid:96)1-norm (cid:107)A(M )(cid:107)1 then a similar norm inequality can be established for all low rank matrix again.
However  even using these (cid:96)1-norm RIP variants  we are still unable to design an efﬁcient alternating
algorithm. All these (cid:96)1-norm RIP variants have to deal with trace norm programming problems. In
fact  it is impossible to construct an estimation sequence based on (cid:96)1-norm RIP because we require
(cid:96)2-norm bound on A(cid:48)A during the construction.
A key ingredient of our framework is to propose a novel (cid:96)2-norm RIP condition to overcome the
above difﬁculty. The main technique reason for the failure of conventional (cid:96)2-norm RIP is that it
tries to bound A(cid:48)A(M ) over all rank k matrices. This is too aggressive to be successful in rank-one
matrix sensing. Regarding to our estimation sequence  what we really need is to make the RIP hold
for current low rank matrix M (t). Once we update our estimation M (t+1)  we can regenerate a new
sensing operator independent of M (t) to avoid bounding A(cid:48)A over all rank k matrices. To this end 
we propose the Conditionally Independent RIP (CI-RIP) condition.
Deﬁnition 3 (CI-RIP). A matrix sensing operator A is Conditionally Independent RIP with constant
δk  if for a ﬁxed rank k matrix M  A is sampled independently regarding to M and satisﬁes

(cid:107)(I − 1
n

A(cid:48)A)M(cid:107)2

2 ≤ δk .

(4)

An (cid:96)2-norm or (cid:96)1-norm RIP sensing operator is naturally CI-RIP but the reverse is not true. In CI-RIP 
A is no longer a ﬁxed but random sensing operator independent of M. In one-pass algorithm  this is
achievable if we always retrieve new instances to construct A in one mini-batch updating. Usually
Eq. (4) doesn’t hold in a batch method since M (t+1) depends on A(M (t)).
An asymmetric rank-one matrix sensing operator is clearly CI-RIP due to the independency between
left/right design vectors. But a symmetric rank-one matrix sensing operator is not CI-RIP. In fact it is
a biased estimator since E(x(cid:62)M x) = tr(M ) . To this end  we propose a shifted version of CI-RIP
for symmetric rank-one matrix sensing operator in the following theorem. This theorem is the key
tool in our analysis.

5

Theorem 4 (Shifted CI-RIP). Suppose xi are independent standard random Gaussian vectors  M is
a ﬁxed symmetric rank k matrix independent of xi and w is a ﬁxed vector. Then with a probability at
least 1 − η  provided n ≥ Ck3d/δ2  

(cid:107) 1
2n

A(cid:48)A(M ) − 1
2

tr(M )I − M(cid:107)2 ≤ δ(cid:107)M(cid:107)2 .

RIP constant δ = O((cid:112)k3d/n) . In gFM  we choose M = M∗ − M (t) therefore M is of rank 3k .

2nA(cid:48)A(M ) is nearly isometric after shifting by its expectation 1

Theorem 4 shows that 1

2 tr(M )I. The

n 1(cid:62)A(M )) − tr(M )| ≤ δ(cid:107)M(cid:107)2 provided n ≥ Ck/δ2 .
n 1(cid:62)X(cid:62)w| ≤ (cid:107)w(cid:107)2δ provided n ≥ C/δ2 .
nA(cid:48)(X(cid:62)w)(cid:107)2 ≤ (cid:107)w(cid:107)2δ provided n ≥ Cd/δ2 .
n X(cid:62)A(M )(cid:107)2 ≤ (cid:107)M(cid:107)2δ provided n ≥ Ck2d/δ2 .

Under the same settings of Theorem 4  suppose that d ≥ C then the following lemmas hold true with
a probability at least 1 − η for ﬁxed w and M .
Lemma 5. | 1
Lemma 6. | 1
Lemma 7. (cid:107) 1
Lemma 8. (cid:107) 1
Lemma 9. (cid:107)I − 1
Equipping with the above lemmas  we construct our estimation sequence as following.
Lemma 10. Let M (t)  H (t)
(cid:107)M∗ − M (t)(cid:107)2 . Then with a probability at least 1 − η  provided n ≥ Ck3d/δ2  

3 be deﬁned as in Algorithm 1. Deﬁne t = (cid:107)w∗ − w(t)(cid:107)2 +

n XX(cid:62)(cid:107)2 ≤ δ provided n ≥ Cd/δ2 .

2   h(t)

1   h(t)

1 =M∗ − M (t) + tr(M∗ − M (t))I + O(δt)   h(t)
H (t)
3 =w∗ − w(t) + O(δt) .
h(t)

2 = tr(M∗ − M (t)) + O(δt)

1 −h(t)

2 I +M (t) → M∗ and h(t)

Suppose by construction  t → 0 when t → ∞. Then H (t)
3 +w(t) →
w∗ and then the proof of Theorem 1 is completed. In the following we only need to show that Lemma
10 constructs an estimation sequence with t = O(δt) → 0. To this end  we need a few things from
matrix perturbation theory.
By Theorem 1  U (t) will converge to U∗ up to column order perturbation. We use the largest
canonical angle to measure the subspace distance spanned by U (t) and U∗  which is denoted as
θt = θ(U (t)  U∗). For any matrix U  it is well known [Zhu and Knyazev  2013] that
sin θ(U  U∗) = (cid:107)U∗
(cid:62)U (U∗(cid:62)U )−1(cid:107)2 .
⊥
The last tangent equality allows us to bound the canonical angle after QR decomposition. Suppose
U (t)R = ˆU (t) in the QR step of Algorithm 1  we have

(cid:62)U(cid:107)2  cos θ(U  U∗) = σk{U∗(cid:62)U}  tan θ(U  U∗) = (cid:107)U∗
⊥

tan θ( ˆU (t)  U∗) = (cid:107)U∗
⊥
= (cid:107)U∗
⊥

(cid:62) ˆU (t)(U∗(cid:62) ˆU (t))−1(cid:107)2 = (cid:107)U∗
⊥
(cid:62)U (t)(U∗(cid:62)U (t))−1(cid:107)2 = tan θ(U (t)  U∗) .

(cid:62)U (t)R(U∗(cid:62)U (t)R)−1(cid:107)2

Therefore  it is more convenient to measure the subspace distance by tangent function.
To show t → 0  we recursively deﬁne the following variables:

αt (cid:44) tan θt  βt (cid:44) (cid:107)w∗ − w(t)(cid:107)2  γt (cid:44) (cid:107)M∗ − M (t)(cid:107)2  t (cid:44) βt + γt .

The following lemma derives the recursive inequalities regarding to {αt  βt  γt} .
√
Lemma 11. Under the same settings of Theorem 1  suppose αt ≤ 2  δt ≤ 4
5σ∗

k  then

√

6

√

αt+1 ≤ 4

5δσ∗−1

k

(βt + γt)  βt+1 ≤ δ(βt + γt)  γt+1 ≤ αt+1(cid:107)M∗(cid:107)2 + 2δ(βt + γt) .

n) is small enough  {αt  βt  γt} will converge
In Lemma 11  when we choose n such that δ = O(1/
to zero. The only question is the initial value {α0  β0  γ0}. According to the initialization step of
gFM  β0 ≤ (cid:107)w∗(cid:107)2 and γ0 ≤ (cid:107)M∗(cid:107)2 . To bound α0   we need the following lemma which directly
follows Wely’s and Wedin’s theorems [Stewart and Sun  1990].

√
4

5(β0 + γ0)δ/σ∗

k ≤ 2 ⇔ δ ≤

√
2

σ∗
k
5(σ∗
1 + β0)

.

√
k/0 = 4

5σ∗

k/(σ∗

√
1 + β0) ⇒ δ ≤ 4

5σ∗

k/t .

√

 

4

σ∗
5σ∗
1 + 3σ∗

k

k

√

σ∗
k
5(σ∗
1 + β0)

 

 

2

σ∗
1 + β0)

k

8(σ∗

(cid:41)

√
To ensure the condition δt ≤ 4
5σ∗

δ ≤ 4

√

5σ∗
k 

In summary  when

δ ≤ min

(cid:40)

⇐δ ≤

√
4

5σ∗

4

√

σ∗
k
5(σ∗
1 + β0)
σ∗
√
1 + 3σ∗

k
k + 4

.

5β0
√
t = [(4

The i-th singular value of M is σi. Suppose that  ≤ σk−σk+1

Lemma 12. Denote U and(cid:101)U as the top-k left singular vectors of M and(cid:102)M = M +O() respectively.
between U and (cid:101)U  denoted as θ(U (cid:101)U )  is bounded by sin θ(U (cid:101)U ) ≤ 2/(σk − σk+1) .
According to Lemma 12  when 2δ((cid:107)w∗(cid:107)2 + (cid:107)M∗(cid:107)2) ≤ σ∗
k/4  we have sin θ0 ≤ 4δ((cid:107)w∗(cid:107)2 +
(cid:107)M∗(cid:107)2)/σ∗
√
Proof of Theorem 1. Suppose that at step t  αt ≤ 2  δt ≤ 4

k. Therefore  α0 ≤ 2 provided δ ≤ σ∗

k/[8((cid:107)w∗(cid:107)2 + (cid:107)M∗(cid:107)2 )] .

. Then the largest canonical angle

5σ∗
βt+1 + γt+1 ≤βt+1 + αt+1(cid:107)M∗(cid:107)2 + 2δ(βt + γt) ≤ δt + 4

t(cid:107)M∗(cid:107)2 + 2δt

k  from Lemma 11 

√

4

5δσ∗−1

k

√
=(4

5σ∗

1/σ∗

k + 3)δt .

Therefore 

√
αt+1 ≤ 4
√
Clearly we need (4
√
4
guaranteed by

σ∗
1 +3σ∗
5σ∗

k

k

√
t = βt + γt ≤ [(4

5σ∗

1/σ∗
(βt + γt) ≤ 4
k + 3)δ < 1 to ensure convergence  which is guaranteed by δ <
. To ensure the recursive inequality holds for any t  we require αt+1 ≤ 2  which is

k + 3)δ]t(β0 + γ0)
√
5σ∗
[(4

5δσ∗−1
5σ∗
1/σ∗

k + 3)δ]t(β0 + γ0) .

5δσ∗−1

1/σ∗

√

k

k

we have

5σ∗
1/σ∗
k + 3)δ]t(σ∗
√
1/σ∗
5σ∗
To simplify the result  replace δ with δ1 = (4

1 + γ0) .

k + 3)δ. The proof is completed.

⊥Λ∗

⊥U∗
⊥

(cid:62) where Λ∗

k = U∗Λ∗U∗(cid:62) to be the best rank k approximation of M∗ and M∗

4.1 Noisy Case
In this subsection  we analyze the performance of gFM under noisy setting. Suppose that M∗ is no
⊥ = diag(λk+1 ···   λd) is the residual
longer low rank  M∗ = U∗Λ∗U∗(cid:62) + U∗
⊥ = M∗−M∗
spectrum. Denote M∗
k .
The additive noise ξi’s are independently sampled from subgaussian with proxy variance ξ.
First we generalize the above theorems and lemmas to noisy case.
Lemma 13. Suppose that in Eq. (1) xi’s are independent standard random Gaussian vectors. M is
⊥ (cid:54)= 0 and ξ > 0. Then provided n ≥ Ck3d/δ2  with a probability at least
a ﬁxed rank k matrix. M∗
1 − η 
(cid:107) 1
A(cid:48)A(M∗ − M ) − 1
2n
2
1(cid:62)A(M∗ − M ) − tr(M∗
| 1
n
X(cid:62)A(M∗ − M )(cid:107)2 ≤ δ(cid:107)M∗
(cid:107) 1
n
A(cid:48)(X(cid:62)w)(cid:107)2 ≤ δ(cid:107)w(cid:107)2  (cid:107) 1
(cid:107) 1
n
n

k − M )I − (M∗
tr(M∗
k − M )| ≤ δ(cid:107)M∗
√
k − M(cid:107)2 + Cσ∗
k+1d2/
1(cid:62)X(cid:62)w(cid:107)2 ≤ δ(cid:107)w(cid:107)2 .

k − M )(cid:107)2 ≤ δ(cid:107)M∗
k − M(cid:107)2 + Cσ∗

k − M(cid:107)2 + Cσ∗

√
k+1d2/

√
k+1d2/

n (5)

(6)

(8)

(7)

n

n

7

Deﬁne γt = (cid:107)M∗
n ≥ Ck3d/δ2 

k − M (t)(cid:107)2 similar to the noise-free case. According to Lemma 13  when ξ = 0  for

√
k+1d2/

n)

n)

1
2

tr(M∗

k − M (t) +

√
k+1d2/
n) .

k − M (t))I + O(δt + Cσ∗

1 =M∗
H (t)
2 =tr(M∗ − M (t)) + O(δt + Cσ∗
h(t)
√
3 =w∗ − w(t) + O(δt + Cσ∗
h(t)
k+1d2/
√
Deﬁne r = Cσ∗
√
k+1d2/
r + O(ξ/
inequalities regarding to the recovery error is constructed in Lemma 14.
Lemma 14. Under the same settings of Lemma 13  deﬁne ρ (cid:44) 2σ∗
k+1/(σ∗
√
any step i  0 ≤ i ≤ t   αi ≤ 2 . When provided 4
5(δt + r) ≤ σ∗
k − σ∗
αt+1 ≤ραt +

δt +

√
4
5
σ∗
k + σ∗

k+1

√
4
5
σ∗
k + σ∗

k+1

k+1). Suppose that at

k + σ∗
k+1 

r   βt+1 ≤ δt + r   γt+1 ≤ αt+1(cid:107)M∗(cid:107)2 + 2δt + 2r .

If ξ > 0  it is easy to check that the perturbation becomes ˆr =
n) . Therefore we uniformly use r to present the perturbation term. The recursive

n.

The solution to the recursive inequalities in Lemma 14 is non-trivial. Comparing to the inequalities
in Lemma 11  αt+1 is bounded by αt in noisy case. Therefore  if we simply follow Lemma 11 to
construct recursive inequality about t   we will quickly be overloaded by recursive expansion terms.
The key construction of our solution is to bound the term αt + 8
k+1)δt . The solution
is given in the following theorem.
Theorem 15. Deﬁne constants
k + σ∗

k+1)   q = (1 + ρ)/2 .

k+1)   b = 3 + 4

k + σ∗

5/(σ∗

5/(σ∗

5σ∗

√

√

√
c =4
Then for any t ≥ 0 

(cid:18)

1/(σ∗
k + σ∗
(cid:19)

αt + 2cδt ≤qt

2 − (1 + ρ)cr

1 − q

+

(1 + ρ)cr
1 − q

.

provided

√

4

δ ≤ min{ 1 − ρ
5(cid:0)4 + 2c(σ∗
 
4ρσ∗
1c
k − σ∗

}   (2 + c(σ∗

ρ
2b

k+1)(cid:1) δ0 + 4

k − σ∗

5(cid:0)4 + (σ∗

√

k+1))δ0 + r ≤ (σ∗

k+1)(cid:1) r ≤ (σ∗

k − σ∗

k − σ∗

k+1)
k − σ∗

k+1)2 .

(9)

(10)

Theorem 15 gives the convergence rate of gFM under noisy settings. We bound αt + 2cδt as the
index of recovery error  whose convergence rate is linear. The convergence rate is controlled by q  a
k . The ﬁnal recovery error is bounded by O(r/(1 − q)) .
constant depends on the eigen gap σ∗
Eq. (10) is the small noise condition to ensure the noisy recovery is possible. Generally speaking 
learning a d× d matrix with O(d) samples is an ill-conditioned problem when the target matrix is full
rank. The small noise condition given by Eq. (10) essentially says that M∗ can be slightly deviated
from low rank manifold and the noise shouldn’t be too large to blur the spectrum of M∗. When the
noise is large  Eq. (10) will be satisﬁed with n = O(d2) which is the information-theoretical lower
bound for recovering a full rank matrix.

k+1/σ∗

5 Conclusion

In this paper  we propose a provable efﬁcient algorithm to solve generalized Factorization Machine
(gFM) and rank-one matrix sensing. Our method is based on an one-pass alternating updating
framework. The proposed algorithm is able to learn gFM within O(kd) memory on steaming data 
has linear convergence rate and only requires matrix-vector product implementation. The algorithm
takes no more than O(k3d log (1/)) instances to achieve O() recovery error.

Acknowledgments

This work was supported in part by research grants from NIH (RF1AG051710) and NSF (III-1421057
and III-1421100).

8

References
Mathieu Blondel  Masakazu Ishihata  Akinori Fujino  and Naonori Ueda. Polynomial Networks and Factorization

Machines: New Insights and Efﬁcient Training Algorithms. pages 850–858  2016.

T. Tony Cai and Anru Zhang. ROP: Matrix recovery via rank-one projections. The Annals of Statistics  43(1):

102–138  2015.

Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics  9(6):717–772  2009.

Emmanuel J. Candes  Yonina Eldar  Thomas Strohmer  and Vlad Voroninski. Phase Retrieval via Matrix

Completion. arXiv:1109.0573  2011.

Yuxin Chen  Yuejie Chi  and Andrea J. Goldsmith. Exact and stable covariance estimation from quadratic

sampling via convex programming. Information Theory  IEEE Transactions on  61(7):4034–4059  2015.

Mark A. Davenport and Justin Romberg. An overview of low-rank matrix recovery from incomplete observations.

arXiv:1601.06422  2016.

Moritz Hardt. Understanding Alternating Minimization for Matrix Completion. arXiv:1312.0925  2013.

Moritz Hardt and Eric Price. The Noisy Power Method: A Meta Algorithm with Applications. arXiv:1311.2495 

2013.

Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. arXiv:1407.4070  2014.

Liangjie Hong  Aziz S. Doumith  and Brian D. Davison. Co-factorization Machines: Modeling User Interests

and Predicting Individual Decisions in Twitter. In WSDM  pages 557–566  2013.

Prateek Jain and Inderjit S. Dhillon. Provable inductive matrix completion. arXiv:1306.0626  2013.

Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank Matrix Completion using Alternating Mini-

mization. arXiv:1212.0467  2012.

Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank Matrix Completion Using Alternating

Minimization. In STOC  pages 665–674  2013.

V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems  volume

2033. Springer  2011.

Richard Kueng  Holger Rauhut  and Ulrich Terstiege. Low rank matrix recovery from rank one measurements.

arXiv:1410.6913  2014.

Kiryung Lee  Yihong Wu  and Yoram Bresler. Near Optimal Compressed Sensing of Sparse Rank-One Matrices

via Sparse Power Factorization. arXiv:1312.0525  2013.

Praneeth Netrapalli  Prateek Jain  and Sujay Sanghavi. Phase Retrieval using Alternating Minimization.

arXiv:1306.0160  2013.

Steffen Rendle. Factorization machines. In ICDM  pages 995–1000  2010.

Steffen Rendle  Zeno Gantner  Christoph Freudenthaler  and Lars Schmidt-Thieme. Fast Context-aware

Recommendations with Factorization Machines. In SIGIR  pages 635–644  2011.

G. W. Stewart and Ji-guang Sun. Matrix Perturbation Theory. Academic Press  1990.

Joel A. Tropp. Convex recovery of a structured signal from independent random linear measurements.

arXiv:1405.1102  2014.

Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. arXiv:1501.01571  2015.

Tuo Zhao  Zhaoran Wang  and Han Liu. Nonconvex Low Rank Matrix Factorization via Inexact First Order

Oracle. 2015a.

Tuo Zhao  Zhaoran Wang  and Han Liu. A Nonconvex Optimization Framework for Low Rank Matrix Estimation.

In NIPS  pages 559–567  2015b.

Kai Zhong  Prateek Jain  and Inderjit S. Dhillon. Efﬁcient matrix sensing using rank-1 gaussian measurements.

In Algorithmic Learning Theory  pages 3–18  2015.

Peizhen Zhu and Andrew V. Knyazev. Angles between subspaces and their tangents. Journal of Numerical

Mathematics  21(4)  2013.

9

,Hastagiri Vanchinathan
Gábor Bartók
Andreas Krause
Ming Lin
Jieping Ye
Isabel Valera
Adish Singla
Manuel Gomez Rodriguez