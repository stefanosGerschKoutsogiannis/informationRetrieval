2013,Factorized Asymptotic Bayesian Inference for Latent Feature Models,This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models~(LFMs). FAB inference has not been applicable to models  including LFMs  without a specific condition on the Hesqsian matrix of a complete log-likelihood  which is required to derive a factorized information criterion''~(FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models.  FAB/LFMs have several desirable properties (e.g.  automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection  prediction  and computational efficiency.",Factorized Asymptotic Bayesian Inference

for Latent Feature Models

Kohei Hayashi∗†

∗National Institute of Informatics

†JST  ERATO  Kawarabayashi Large Graph Project

Ryohei Fujimaki

NEC Laboratories America

rfujimaki@nec-labs.com

kohei-h@nii.ac.jp

Abstract

This paper extends factorized asymptotic Bayesian (FAB) inference for latent fea-
ture models (LFMs). FAB inference has not been applicable to models  includ-
ing LFMs  without a speciﬁc condition on the Hessian matrix of a complete log-
likelihood  which is required to derive a “factorized information criterion” (FIC).
Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs
has the same form as those of mixture models. FAB/LFMs have several desir-
able properties (e.g.  automatic hidden states selection and parameter identiﬁabil-
ity) and empirically perform better than state-of-the-art Indian Buffet processes in
terms of model selection  prediction  and computational efﬁciency.

1 Introduction

Factorized asymptotic Bayesian (FAB) inference is a recently-developed Bayesian approximation
inference method for model selection of latent variable models [5  6]. FAB inference maximizes
a computationally tractable lower bound of a “factorized information criterion” (FIC) which con-
verges to a marginal log-likelihood for a large sample limit. In application with respect to mixture
models (MMs) and hidden Markov models  previous work has shown that FAB inference achieves
as good or even better model selection accuracy as state-of-the-art non-parametric Bayesian (NPB)
methods and variational Bayesian (VB) methods with less computational cost. One of the interesting
characteristics of FAB inference is that it estimates both models (e.g.  the number of mixed compo-
nents for MMs) and parameter values without priors (i.e.  it asymptotically ignores priors)  and it
does not have a hand-tunable hyper-parameter. With respect to the trade-off between controllability
and automation  FAB inference places more importance on automation.
Although FAB inference is a promising model selection method  as yet it has only been applicable to
models satisfying a speciﬁc condition that the Hessian matrix of a complete log-likelihood (i.e.  of a
log-likelihood over both observed and latent variables) must be block diagonal  with only a part of
the observed samples contributing individual sub-blocks. Such models include basic latent variable
models as MMs [6]. The application of FAB inference to more advanced models that do not satisfy
the condition remains to be accomplished.
This paper extends an FAB framework to latent feature models (LFMs) [9  17]. Model selection for
LFMs (i.e.  determination of the dimensionality of latent features) has been addressed by NBP and
VB methods [10  3]. Although they have shown promising performance in such applications as link
prediction [16]  their high computational costs restrict their applications to large-scale data.
Our asymptotic analysis of the Hessian matrix of the log-likelihood shows that FICs for LFMs have
the same form as those for MMs  despite the fact that LFMs do not satisfy the condition explained
above (see Lemma 1). Eventually  as FAB/MMs  FAB/LFMs offer several desirable properties  such
as FIC convergence to a marginal log-likelihood  automatic hidden states selection  and monotonic
increase in the lower FIC bound through iterative optimization. Further we conduct two analysis in

1

Section 3: 1) we relate FAB E-steps to a convex concave procedure (CCCP) [29]. Inspired by this
analysis  we propose a shrinkage acceleration method which drastically reduces computational cost
in practice  and 2) we show that FAB/LFMs have parameter identiﬁability. This analysis offers a
natural guide to the merging post-processing of latent features. Rigorous proofs and assumptions
with respect to the main results are given in the supplementary materials.
Notation In this paper  we denote the (i  j)-th element  the i-th row vector  and the j-th column
vector of A by aij  ai  and a·j  respectively.

}

{∑

1.1 Related Work
FIC for MMs Suppose we have N × D observed data X and N × K latent variables Z. FIC
considers the following alternative representation of the marginal log-likelihood:
log p(X|M) = max
p(X  Z|P)p(P|M)dP  (1)
∑
where q(Z) is a variational distribution on Z; M and P are a model and its parameter  respec-
In the case of MMs  log p(X  Z|P) can be factorized into log p(Z) and log p(X|Z) =
tively.
k log pk(X|z·k)  where pk is the k-th observation distribution (we here omit parameters for
notational simplicity.) We can then approximate p(X  Z|M) by individually applying Laplace’s
method [28] to log p(Z) and log pk(X|z·k):

  p(X  Z|M) =

p(X  Z|M)

q(Z) log

∫

q(Z)

Z

q

(2π)DZ /2

N DZ /2 det|FZ|1/2

p(X  Z|M) ≈ p(X  Z| ˆP)
(2)
∑
where ˆP is the maximum likelihood estimator (MLE) of p(X  Z|P).1 DZ and Dk are the param-
eter dimensionalities of p(Z) and pk(X|z·k)  respectively. FZ and Fk are −∇∇ log p(Z)| ˆP /N
[
and −∇∇ log pk(X|z·k)| ˆP /(
n znk)  respectively. Under conditions for asymptotic ignoring of
log det|FZ| and log det|Fk|  substituting Eq.(2) into (1) gives the FIC for MMs as follows:

]

k=1

(

 

∑

∑

FICMM ≡ max

Eq

q

log p(X  Z| ˆP) − DZ
2

log N −

Dk
2

k

log

znk

n

K∏

∑
n znk)Dk/2 det|Fk|1/2

(2π)Dk/2

+ H(q) 

(3)

∑

n znk)  which
where H(q) is the entropy of q(Z). The most important term in FICMM (3) is log(
offers such theoretically desirable properties for FAB inference as automatic shrinkage of irrelevant
latent variables and parameter identiﬁability [6].
∑
Direct optimization of FICMM is difﬁcult because: (i) evaluation of Eq[log
n znk] is computa-
tionally infeasible  and (ii) the MLE is not available in principle. Instead  FAB optimizes a tractable
lower bound of an FIC [6]. For (i)  since − log
∑
n znk is a convex function  its linear approximation
at N ˜πk > 0 yields the lower bound:

∑

∑

∑

∑

(

)

[

]

znk

(4)
where 0 < ˜πk ≤ 1 is a linearization parameter. For (ii)  since  from the deﬁnition of the MLE  the
inequality log p(X  Z| ˆP) ≥ log p(X  Z|P) holds for any P  we optimize P along with q. Alternat-
ing maximization of the lower bound with respect to q  P  and ˜π guarantees a monotonic increase
in the FIC lower bound [6].

log N ˜πk +

Eq[znk]/N − ˜πk

≥ −

Dk
2

Dk
2

Eq

−

log

˜πk

n

n

k

k

 

Inﬁnite LFMs and Indian Buffet Process The IBP [10  11] is a nonparametric prior over inﬁ-
nite LFMs. It enables us to express an inﬁnite number of latent features  and making it possible to
adjust model complexity on the basis of observations. Inﬁnite IBPs have still been actively stud-
ied in terms of both applications (e.g.  link prediction [16]) and model representations (e.g.  latent
attribute models [19]). Since naive Gibbs sampling requires unrealistic computational cost  accel-
eration algorithms such as accelerated sampling [2] and VB [3] have been developed. Reed and
Ghahramani [22] have recently proposed an efﬁcient MAP estimation framework of an IBP model
via submodular optimization  which is referred to as maximum-expectation IBP (MEIBP). As simi-
lar to FIC  “MAD-Bayes” [1] considers asymptotics of MMs and LFMs  but it is based on a limiting
case that the noise variance goes to zero  which yields a prior-derived regularization term.

1While p(X|P) is a non-regular model  P (X  Z|P) is a regular model (i.e.  the Fisher information is non-

singular at the ML estimator ) and Fk and FZ have their inversions at ˆP.

2

2 FIC and FAB Algorithm for LFMs
LFMs assume underlying relationships for X with binary features Z ∈ {0  1}N×K and linear bases
W ∈ RD×K such that  for n = 1  . . .   N 

>

xn = Wzn + b + εn 

(5)
where εn ∼ N (0  Λ
−1) is the Gaussian noise having the diagonal precision matrix Λ ≡ diag(λ) 
and b ∈ RD is a bias term. For later convenience  we deﬁne the centered observation ¯X = X −
. Z follows a Bernoulli prior distribution znk ∼ Bern(πk) with a mean parameter πk. The
1b
parameter set P is deﬁned as P ≡ {W  b  λ  π}. Also  we denote parameters with respect to the
d-th dimension as θd = (wd  bd  λd). Similarly with other FAB frameworks  the log-priors of P are
assumed to be constant with respect to N  i.e.  limN→∞ log p(P|M)
In the case of MMs  we implicitly use the fact that: A1) parameters of pk(X|z·k) are mutually inde-
pendent for k = 1  . . .   K (in other words  ∇∇ log p(X|Z) is block diagonal having K blocks)  and
A2) the number of observations which contribute ∇∇ log pk(X|z·k) is
n znk. These conditions
naturally yield the FAB regularization term log
n znk by the Laplace approximation of MMs (2).
However  since θd is shared by all latent features in LFMs  A1 and A2 are not satisﬁed. In the next
section  we address this issue and derive FIC for LFMs.

∑

∑

= 0

N

2.1 FICs for LFMs

∑

∑

k

The following lemma plays the most important role in our derivation of FICs for LFMs.
Lemma 1. Let F(d) be the Hessian matrix of the negated log-likelihood with respect to θd  i.e. 
−∇∇ log p(x·d|Z  θd). Under some mild assumptions (see the supplementary materials)  the fol-
lowing equality holds:

log

+ Op(1).

n znk
N

log det|F(d)| =
∑
n znk term naturally appears in log det|F(d)| without A1 and A2.
An important fact is that the log
Lemma 1 induces the following theorem  which states an asymptotic approximation of a marginal
complete log-likelihood  log p(X  Z|M).
Theorem 2. If Lemma 1 holds and the joint marginal log-likelihood is bounded for a sufﬁciently
large N  it can be asymptotically approximated as:
log p(X  Z|M) = J(Z  ˆP) + Op(1) 
J(Z P) ≡ log p(X  Z|P) − |P| − DK

∑

∑

(6)

(7)

(8)

log

znk.

log N − D
2

2

k

n

∑

It is worth noting that  if we evaluate the model complexity of θd (log det|F(d)|) by N  i.e. 
∑
if we apply Laplace’s method without Lemma 1  Eq. (7) falls into Bayesian Information Crite-
rion [23]  which tells us that the model complexity relevant to θd increases not O(K log N ) but
O(
By substituting approximation (7) into Eq. (1)  we obtain the FIC of the LFM as follows:

n znk).

k log

FICLFM ≡ max

Eq[J(Z  ˆP)] + H(q).

q

(9)

It is interesting that FICLFM (9) and FICMM (3) have exactly the same representation despite the
fact that LFMs do not satisfy A1 and A2. This indicates the wide applicability of FICs and suggests
that FIC representation of approximated marginal log-likelihoods is feasible not only for MMs but
also for more general (discrete) latent variable models.
Since the asymptotic constant terms of Eq. (7) are not affected by the expectation of q(Z)  the
difference between the FIC and the marginal log-likelihood is asymptotically constant; in other
words  the distance between log p(X|M)/N and FICLFM/N is asymptotically small.
Corollary 3. For N → ∞  log p(X|M) = FICLFM + Op(1) holds.

3

2.2 FAB/LFM Algorithm

∏

As with the case of MMs (3)  FICLFM is not available in practice  and we employ the lower bound-
ing techniques (i) and (ii). For LFMs  we further introduce a mean-ﬁled approximation on Z  i.e.  we
k ˜q(znk|µnk)  where ˜q(z|µ) is a Bernoulli
restrict the class of q(zn) to a factorized form: q(zn) =
distribution with a mean parameter µ = Eq[z]. Rather than this approximation’s making the FIC
lower bound looser (the equality (1) no longer holds)  the variational distribution has a closed-form
solution. Note that this approximation does not cause signiﬁcant performance degradation in VB
contexts [20  25]. The VB-extension of IBP [3] also uses this factorized assumption.
By applying (i)  (ii)  and the mean-ﬁeld approximation  we obtain the lower bound: L(q P  ˜π) =

Eq [log p(X|Z  Θ) + log p(Z|π) + RHS of (4)] − 2D + K

2

log N +

H(q(zn)).

n

(10)

∑

An FAB algorithm alternatingly maximizes L(q P  ˜π) with respect to {{µn
} P  ˜π}. Notice that
the algorithm described below monotonically increases L in every single step  and therefore we are
guaranteed to obtain a local maximum. This monotonic increase in L gives us a natural stopping
condition with a tolerance δ: if (Lt − Lt−1)/N < δ then stop the algorithm  where we denote the
value of L at the t-th iteration by Lt.

l6=k µnlw·l − 1

FAB E-step In the FAB E-step  we update µn in a way similar to that with the variational mean-
ﬁeld inference in a restricted Boltzmann machine [20]. Taking the gradient of L with respect to µn
and setting it to zero yields the following ﬁxed-point equations:

·kΛ(¯xn −∑
∑

µnk = g (cnk + η(πk) − D/2N ˜πk)
>
−1 is the sigmoid function  cnk = w

− µ2
n).
  which originated in the log

(11)
where g(x) = (1 + exp(−x))
2 w·k) 
is a natural parameter of the prior of z·k. Update equation (11) is a form of
and η(πk) = log πk
1−πk
coordinate descent  and every update is guaranteed to increase the lower bound [25]. After several
iterations of Eq. (11) over k = 1  . . .   K  we are able to obtain a local maximum of Eq[zn] = µn
>
and Eq[znz
n ] = µnµ
One unique term in Eq. (11) is − D
n znk term in Eq. (8). In
updating µnk (11)  the smaller ˜πk (or equivalent to πk by Eq. (12)) is  the smaller µnk is. And a
smaller µnk is likely to induce a smaller ˜πk (see Eq. (12)). This results in the shrinking of irrelevant
features  and therefore FAB/LFMs are capable of automatically selecting feature dimensionality
K. This regularization effect is induced independently of prior (i.e.  asymptotic ignorance of prior)
and is known as “model induced regularization” which is caused by Bayesian marginalization in
singular models [18]. Notice that Eq. (11) offers another shrinking effect  by means of η(πk)  which
is a prior-based regularization. We empirically show that the latter shrinking effect is too weak to
mitigate over-ﬁtting and the FAB algorithm achieves faster convergence  with respect to N  to the
true model (see Section 4.) Note that if we only use the effect of η(πk) (i.e. setting D/2N ˜πk = 0) 
then update equation (11) is equivalent to that of variational EM.

>
n + diag(µn

2N ˜πk

FAB M-step The FAB M-step is equivalent to the M-step in the EM algorithm of LFMs; the
solutions of W  Λ and b are given as in closed form and is exactly the same as those of PPCA [24]
(see the supplementary materials.) For ˜π and π  we obtain the following solutions:

∑

πk = ˜πk =

µnk/N.

n

(12)

Shrinkage step As we have explained  in principle  the FAB regularization term D
in Eq. (11)
2N ˜πk
∑
automatically eliminates irrelevant latent features. While the elimination does not change the value
of Eq[log(X|Z P)]  removing them from the model increases L due to a decrease in model com-
∑
plexity. We eliminate shrunken features after FAB E-step in terms of that LFMs approximate X by
n µnk/N = 0  the k-th feature does not affect to the approximation
n µnk/N = 1  wk can be seen as a
>
·l + 1w

>
>
. When
k µ·kw
·k + 1b
>
>
l6=k z·lw
l z·lw
·l )  and we simply remove it. When
·l =
l z·lw

>
·k)  and we update bnew = b + wk and then remove it.

(
bias (

l6=k z·lw

∑

∑

∑

∑

∑

>
·l =

4

}

})

Algorithm 1 The FAB algorithm for LFMs.
1: Initialize {µn
2: while Convergence do
Update P
3:
accelerateShrinkage({µn
4:
for k = 1  . . .   K do
5:
Update {µnk} by Eq. (11)
6:
end for
7:
Shrink unnecessary latent features
8:
if (Lt − Lt−1)/N < δ then
9:
10:
11:

{{µ
0} ← merge({µn
}  W
0
) = dim(W) then Con-
if dim(W
verge
} ← {µ
else {µn
0
n
end if

}  W ← W

}  W)

0
n

0

}

ck ← ( ¯X−∑

12:
13:
14: end while
Algorithm 2 accelerateShrinkage
input {µn
1: for k = 1  . . .   K do
2:
3:
4:
5:
6:
7: end for

− 1
>
·l
for t = 1  . . .   Tshrink do
Update {µnk} by Eq. (11)
Update π and ˜π by Eq. (12)
end for

l6=k µ·lw

2 1w

>
·k)Λw·k

Figure 1: Time evolution of K (top) and L/N
(bottom) in FAB with and without shrinkage ac-
celeration (D = 50 and K = 5). Different lines
represent different random starts.

∑

∑

This model shrinkage also works to avoid the ill-conditioning of the FIC; if there are latent fea-
n µnk/N = 1)  the
tures that are never activated (
FIC will no longer be an approximation of the marginal log-likelihood. Algorithm 1 summa-
rizes whole procedures with respect to the FAB/LFMs. Note that details regarding sub-routines
accelerateShrinkage() and merge() are explained in Section 3.

n µnk/N = 0) or always activated (

3 Analysis and Reﬁnements

∑

CCCP Interpretation and Shrinkage Acceleration Here we interpret the alternating updates
of µ and ˜π as a convex concave procedure (CCCP) [29] and consider to eliminate irrelevant
features in early steps to reduce computational cost. By substituting an optimality condition
˜πk =

n µnk/N (12) into the lower bound  we obtain

∑

∑

(∑

)

L(q) = − D
2

log

µnk +

(cn + η)

k

n

n

>

µn + H(q)

+ const.

(13)

The ﬁrst and second terms are convex and concave with respect to µnk  respectively. The CCCP
solves Eq.(13) by iteratively linearizing the ﬁrst term around µt−1
nk . By setting the derivative of the
“linearized” objective to be zero  we obtain the CCCP update as follows:

(
cnk + η(πk) − D

∑

2

n

)

µt−1

nk

.

(14)

µt

nk = g

∑
n µt−1

nk into account  Eq.(14) is equivalent to Eq.(11).

By taking N ˜πk =
This new view of the FAB optimization gives us an important insight to accelerate the algorithm.
By considering the FAB optimization as the alternating maximization in terms of P and µ (˜π is
removed)  it is natural to take multiple CCCP steps (14). Such multiple CCCP steps in each FAB-
EM step is expected to accelerate the shrinkage effect discussed in the previous section because the

5

Elapsed time (sec)Estimated K1020304050100200300400Elapsed time (sec)FIC lower bound / N−100−90−80−70−60−50−40−30100200300400AccelerationOnOff#Iteration204080160∑

regularization in terms of −D/2(
n µnk) causes the effect. Eventually  it is expected to reduce the
total computational cost since we may be able to remove irrelevant latent features in earlier iterations.
We summarize the whole routine of accelerateShrinkage() based on the CCCP in Algorithm 2.
Note that  in practice  we update π along with ˜π for further acceleration of the shrinkage. We
empirically conﬁrmed that Algorithm 2 signiﬁcantly reduced computational costs (see Section 4 and
Figure 1.) Further discussion of this this update (an exponentiated gradient descent interpretation)
can be found in the supplementary materials.

Identiﬁability and Merge Post-processing Parameter identiﬁability is an important theoretical
aspect in learning algorithms for latent variable models. It has been known [26  27] that general-
ization error signiﬁcantly worsens if the mapping between parameters and functions is not one-to-
one (i.e.  is non-identiﬁable.) Let us consider the LFM case of K = 2. If w·1 = w·2  then any
combination of µn1 and µn2 = 2µ − µn1 will have the same representation: Eq[Ex[¯xnd|θd]] =
∑
wd1(µn1 + µn2) = 2wd1µ  and therefore the MLE is non-identiﬁable.
The following theorem shows that FAB inference resolves such non-identiﬁability in LFMs.
∑
∑
Theorem 4. Let P∗ and q
∗ be stationary points of L such that 0 <
n µ
1  . . .   K and |¯x
| < ∞ for k = 1  . . .   K  n = 1  . . .   N. Then  w
∗
∗
>
·k = w
n Λ
∗
condition of
nk/N =
n µ

∗
nk/N < 1 for k =
∗
·l is a sufﬁcient

∗
nl/N.
n µ

∗
·k

w

For the ill-conditioned situation described above  the FAB algorithm has a unique solution that
balances the sizes of latent features. In large sample limit  both FAB and EM reach the same ML
value. The point is  for LFMs  ML solutions are not unique and EM is likely to choose large-K-
solutions because of this non-identiﬁability issue. On the other hands  FAB prefers to small-K ML
solutions on the basis of the regularizer. In addition  Theorem 4 gives us an important insight about
post-processing of latent features. If w
)] is equivalent without
relation to µnk and µnl  while model complexity is smaller if we only have one latent feature.
∗
∗
Therefore  if w
·k and
·k = w
∗
∗
·l
·k = µ
. In practice  we search for such overlapping features on the basis of a Euclidean
µ
∗
distance matrix of W
·k for k = 1  . . .   K and merge them if the lower bound increases after
the post-processing. We empirically found that a few merging operations were likely to occur in real
world data sets. The algorithm of merge() is summarized in the supplementary materials.

·l  merging these two latent features increases L  i.e.  w
∗

·l  then Eq[log p(X  Z|M∗
∗

∗
·k = 2w

∗
·k = w

and w

∗
·k+µ

2

∗

4 Experiments

We have evaluated FAB/LFMs in terms of computational speed  model selection accuracy  and pre-
diction performance with respect to missing values. We compared FAB inference and the variational
EM algorithm (see Section 2.2) with an IBP that utilized fast Gibbs sampling [2]  a VB [3] having a
ﬁnite K  and MEIBP [22]. IBP and MEIBP select a model which maximizes posterior probability.
For VB  we performed inference with K = 2  . . .   D and selected the model having the highest free
energy. EM selects K using the shrinkage effect of η as we have explained in Section 2.2.
All the methods were implemented in Matlab (for IBP  VB  and MEIBP  we used original codes
released by the authors)  and the computational performance were fairly compared. For FAB and
} were randomly
EM  we set δ = 10
and uniformly initialized by 0 and 1; the initial number of latent features was set to min(N  D) as
√
well as MEIBP. Since the softwares of IBP  VB  and MEIBP did not learn the standard deviation
λ in FAB)  we ﬁxed it to 1 for artiﬁcial simulations  which is the true standard
of the noise (1/
deviation of toy data  and 0.75 for real data by following the original papers [2  22]. We set other
parameters with software default values. For example  α  a hyperparameter of IBP  was set to 3 
which might cause overestimation of K. As common preprocessing  we normalized X (i.e.  the
sample variance is 1) in all experiments.

−4 (this was not sensitive) and Tshrink = 100 (FAB only); {µn

Artiﬁcial Simulations We ﬁrst conducted artiﬁcial simulations with fully-observed synthetic data
generated by model (5) having a ﬁxed λk = 1 and πk = 0.5. Figure 1 shows the results of a com-
parison between FAB with and without shrinkage acceleration.2 Clearly  our shrinkage acceleration

2We also investigated the effect of merge post-processing  but none was observed in this small example.

6

Figure 2: Comparative evaluation of the artiﬁcial simulations in terms of N v.s. elapsed time (left)
and selected K (right). Each error-bar shows the standard deviation over 10 trials (D = 30).

Figure 3: Learned Ws in block data.

signiﬁcantly reduced computational cost by eliminating irrelevant features in the early steps  while
both algorithms achieved roughly the same objective value L and model selection performance at
the convergence. Figure 2 shows the results of a comparison between FAB (with acceleration) and
the other methods. While MEIBP was much faster than FAB in terms of elapsed computational time 
FAB achieved the most accurate estimation of K  especially for large N.

Block Data We next demonstrate performance of FAB/LFMs in terms of learning features. We
used the block data  a synthetic data originally used in [10]. Observations were generated by
combining four distinct patterns (i.e.  K = 4  see Figure 3) with Gaussian noise  on 6 by 6 pixels
(i.e.  D = 36). We prepared the results of N = 2000 samples with the noise standard deviation
0.3 and no missing values (more results can be found in the supplementary materials.) Figure 3
compares estimated features of each method on early learning phase (at the 5th iteration) and after
the convergence (the result displayed is the example which has the median log-likelihood over 10
trials.) Note that  we omitted MEIPB since we observed that its parameter setting was very sensitive
for this data. While EM and IBP retain irrelevant features  FAB successfully extracts the true patterns
without irrelevant features.

Real World Data We ﬁnally evaluated predictive performance by using the real data sets described
in Table 1. We randomly removed 30% of data with 5 different random seeds and treated them as
missing values  and we measured predictive and training log-likelihood (PLL and TLL) for them.
Table 1 summarizes the results with respect to elapsed computational time (hours)  selected K 
PLL  and TLL. Note that  for cases when the computational time for a method exceeded 50 hours 
we stopped the program after that iteration.3 Since MEIBP is the method for non-negative data  we
omitted the results of those containing negative values. Also  since MEIBP did not ﬁnish the ﬁrst
iteration within 50 hours for yaleB and USPS data  we set the initial K as 100. FAB consistently
achieved good predictive performance (higher PLL) with low computational cost. Although MEIBP
performed faster than FAB with appropriately set the initial value of K (i.e.  yaleB and USPS) 
PLLs of FAB were much better than those of MEIBP. In terms of K  FAB typically achieved a
more compact and better model representation than the others (smaller K). Another important
observation is that FAB have much smaller differences between TLL and PLL than the others. This
suggests that FAB’s unique regularization worked well for mitigating over-ﬁtting. For the large
sample data sets (EEG  Piano  USPS)  PLLs of FAB and EM were competitive with one another;

3We totally omitted VB because of its long computational time.

7

NElapsed time (sec)100.5101101.5102102.5103True K=5100250500100020001010025050010002000fabemibpmeibpvbNEstimated K510152025305100250500100020001010025050010002000Table 1: Results on real-world data sets. The best result (e.g.  the smallest K in model selection)
and those not signiﬁcantly worse than it are highlighted in boldface. We used a one-side t-test with
95% conﬁdence.
*We exclude the results of MEIBP for yaleB and USPS from the t-test because of the
different experimental settings (initial K was smaller than the others. See the body text for details.)

K

PLL

Data
Sonar [4]
208 × 49

Libras [4]
360 × 90

Auslan [14]
16180 × 22

EEG [12]
120576 × 32

Piano [21]
57931 × 161

yaleB [7]
2414 × 1024

USPS [13]
110000 × 256

∗

∗

TLL

Method Time (h)
4.4 ± 1.1 −1.25 ± 0.02 −1.14 ± 0.03
FAB < 0.01
48.8 ± 0.5 −4.04 ± 0.46 −0.08 ± 0.07
EM < 0.01
69.6 ± 4.8 −4.48 ± 0.15
0.13 ± 0.02
IBP
3.3
45.4 ± 1.7 −18.10 ± 1.90 −15.60 ± 1.80
MEIBP < 0.01
FAB < 0.01 19.0 ± 0.7 −0.63 ± 0.03 −0.42 ± 0.03
75.6 ± 8.6 −0.68 ± 0.11
0.76 ± 0.24
EM
0.01
0.13 ± 0.01
36.4 ± 1.1 −0.18 ± 0.01
IBP
4.8
40.8 ± 1.3 −11.30 ± 2.00 −10.70 ± 1.80
MEIBP
0.05
6.0 ± 0.7 −1.34 ± 0.15 −0.92 ± 0.02
FAB
0.04
−1.79 ± 0.27 −0.78 ± 0.02
22 ± 0
EM
0.2
73 ± 5
0.08 ± 0.01
−4.54 ± 0.08
IBP
50.2
N/A
MEIBP
N/A
1.6 11.2 ± 1.6 −0.93 ± 0.02 −0.76 ± 0.04
FAB
32 ± 0
−0.88 ± 0.09 −0.59 ± 0.01
EM
3.7
46.4 ± 4.4 −3.16 ± 0.03 −0.26 ± 0.05
IBP
53.0
N/A
MEIBP
58.0 ± 3.5 −0.83 ± 0.01 −0.63 ± 0.02
FAB
19.4
50.1 158.6 ± 3.4 −0.82 ± 0.02 −0.45 ± 0.01
EM
89.6 ± 4.2 −1.83 ± 0.02 −0.84 ± 0.05
IBP
55.8
14.3 48.4 ± 3.2 −7.14 ± 0.52 −6.90 ± 0.50
MEIBP
2.2 77.2 ± 7.9 −0.37 ± 0.02 −0.29 ± 0.03
FAB
929 ± 20
0.80 ± 0.27
EM
50.9
94.2 ± 7.5 −0.54 ± 0.02 −0.35 ± 0.02
IBP
51.7
69.8 ± 2.7 −1.18 ± 0.02 −1.12 ± 0.02
MEIBP
7.2
11.2 110.2 ± 5.1 −0.96 ± 0.01 −0.64 ± 0.02
FAB
256 ± 0
−1.06 ± 0.01 −0.36 ± 0.01
EM
45.7
61.6 181.0 ± 4.8 −2.59 ± 0.08 −0.76 ± 0.01
IBP
22.0 ± 2.7 −1.35 ± 0.03 −1.31 ± 0.03
MEIBP
1.9

−4.60 ± 1.20

N/A

N/A

N/A

N/A

N/A

this is reasonable  for large N  both of them ideally achieve the maximum likelihood while FAB
achieved much smaller K (see identiﬁability discussion in Section 3). In small N scenarios  on the
other hand  FIC approximation would be not accurate  and FAB would perform worse than NPBs
(while we observed such case only in Libras.)

5 Summary

We have considered here an FAB framework for LFMs that offers fully automated model selection 
i.e.  selecting the number of latent features. While LFMs do not satisfy the assumptions that naturally
induce FIC/FAB on MMs  we have shown that they have the same “degree” of model complexity as
the approximated marginal log-likelihood  and we have derived FIC/FAB in a form similar to that
for MMs. In addition  our proposed accelerating mechanism for shrinking models drastically re-
duces total computational time. Experimental comparisons of FAB inference with existing methods 
including state-of-the-art IBP methods  have demonstrated the superiority of FAB/LFM.

Acknowledgments

The authors would like to thank Finale Doshi-Velez for providing Piano and EEG data sets. This
work was supported by JSPS KAKENHI Grant Number 25880028.

8

References
[1] T. Broderick  B. Kulis  and M. I. Jordan. MAD-Bayes: MAP-based Asymptotic Derivations from Bayes.

In ICML  2013.

process. In AISTATS  2009.

[2] F. Doshi-Velez and Z. Ghahramani. Accelerated sampling for the indian buffet process. In ICML  2009.
[3] F. Doshi-Velez  K. T. Miller  J. Van Gael  and Y. W. Teh. Variational inference for the Indian buffet

[4] A. Frank and A. Asuncion. UCI machine learning repository  2010.
[5] R. Fujimaki and K. Hayashi. Factorized asymptotic bayesian hidden markov model. In ICML  2012.
[6] R. Fujimaki and S. Morinaga. Factorized asymptotic bayesian inference for mixture modeling. In AIS-

TATS  2012.

[7] A. S. Georghiades  P. N. Belhumeur  and D. J. Kriegman. From few to many: Illumination cone models
for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine
Intelligence  23:643–660  2001.

[8] Z. Ghahramani. Factorial learning and the EM algorithm. In NIPS  1995.
[9] Z. Ghahramani  T. L. Grifﬁths  and P. Sollich. Bayesian nonparametric latent feature models (with dis-

cussion). In 8th Valencia International Meeting on Bayesian Statistics  2006.

[10] T. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the indian buffet process  2005.
[11] T. L. Grifﬁths and Z. Ghahramani. The indian buffet process: An introduction and review. JMLR 

12:1185–1224  2011.

[12] U. Hoffmann  G. Garcia  J. M. Vesin  K. Diserens  and T. Ebrahimi. A boosting approach to p300
In International IEEE EMBS Conference on

detection with application to brain-computer interfaces.
Neural Engineering  pages 97–100. 2005.

[13] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis

and Machine Intelligence  16(5):550–554  1994.

[14] M. W. Kadous. Temporal Classiﬁcation: Extending the Classiﬁcation Paradigm to Multivariate Time
Series. PhD thesis  School of Computer Science & Engineering  University of New South Wales  2002.
[15] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.

Information and Computation  132(1):1–63  1997.

[16] K. Miller  T. Grifﬁths  and M. Jordan. Nonparametric latent feature models for link prediction. In NIPS 

[17] K. T. Miller. Bayesian Nonparametric Latent Feature Models. PhD thesis  University of California 

[18] S. Nakajima  M. Sugiyama  and D. Babacan. On bayesian PCA: Automatic dimensionality selection and

[19] K. Palla  D. A. Knowles  and Z. Ghahramani. An inﬁnite latent attribute model for network data.

In

[20] C. Peterson and J. Anderson. A mean ﬁeld theory learning algorithm for neural networks. Complex

[21] G. E. Poliner and D. P. W. Ellis. A discriminative model for polyphonic piano transcription. EURASIP

Journal of Advances in Signal Processing  2007(1):154  2007.

[22] C. Reed and Z. Ghahramani. Scaling the indian buffet process via submodular maximization. In ICML 

2009.

Berkeley  2011.

analytic solution. In ICML  2011.

ICML  2012.

systems  1:995–1019  1987.

[23] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics  6(2):461–464  1978.
[24] M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical

Society. Series B  61(3):611–622  1999.

[25] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

[26] S. Watanabe. Algebraic analysis for nonidentiﬁable learning machines. Neural Computation  13(4):899–

[27] S. Watanabe. Algebraic Geometry and Statistical Learning Theory (Cambridge Monographs on Applied

and Computational Mathematics). Cambridge University Press  2009.

[28] R. Wong. Asymptotic Approximation of Integrals (Classics in Applied Mathematics). SIAM  2001.
[29] A. L. Yuille and A. Rangarajan. The Concave-Convex procedure. Neural Computation  15(4):915–936 

2013.

933  2001.

2003.

[30] R. S. Zemel and G. E. Hinton. Learning population codes by minimizing description length. Neural

Computation  7(3):11–18  1994.

9

,Kohei Hayashi
Ryohei Fujimaki
Yining Wang
Hsiao-Yu Tung
Alexander Smola
Anima Anandkumar