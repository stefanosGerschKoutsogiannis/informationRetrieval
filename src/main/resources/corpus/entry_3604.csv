2008,Multi-task Gaussian Process Learning of Robot Inverse Dynamics,The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneficial to be able to learn this function for adaptive control. A given robot manipulator will often need to be controlled while holding different loads in its end effector  giving rise to a multi-task learning problem. We show how the structure of the inverse dynamics problem gives rise to a multi-task Gaussian process prior over functions  where the inter-task similarity depends on the underlying dynamic parameters. Experiments demonstrate that this multi-task formulation generally improves performance over either learning only on single tasks or pooling the data over all tasks.,Multi-task Gaussian Process Learning of Robot

Inverse Dynamics

Kian Ming A. Chai
School of Informatics  University of Edinburgh  10 Crichton Street  Edinburgh EH8 9AB  UK

Christopher K. I. Williams

Stefan Klanke

Sethu Vijayakumar

{k.m.a.chai  c.k.i.williams  s.klanke  sethu.vijayakumar}@ed.ac.uk

Abstract

The inverse dynamics problem for a robotic manipulator is to compute the torques
needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able
to learn this function for adaptive control. A robotic manipulator will often need
to be controlled while holding different loads in its end effector  giving rise to a
multi-task learning problem. By placing independent Gaussian process priors over
the latent functions of the inverse dynamics  we obtain a multi-task Gaussian pro-
cess prior for handling multiple loads  where the inter-task similarity depends on
the underlying inertial parameters. Experiments demonstrate that this multi-task
formulation is effective in sharing information among the various loads  and gen-
erally improves performance over either learning only on single tasks or pooling
the data over all tasks.

1 Introduction

The inverse dynamics problem for a robotic manipulator is to compute the torques τ needed at the
joints to drive it along a given trajectory  i.e. the motion speciﬁed by the joint angles q(t)  velocities
˙q(t) and accelerations ¨q(t)  through time t. Analytical models for the inverse dynamics τ (q  ˙q  ¨q)
are often infeasible  for example due to uncertainty in the physical parameters of the robot  or the
difﬁculty of modelling friction. This leads to the need to learn the inverse dynamics.

A given robotic manipulator will often need to be controlled while holding different loads in its end
effector. We refer to different loadings as different contexts. The inverse dynamics functions depend
on the different contexts. A simple approach is to learn a different mapping for each context  but
it is more attractive if one can exploit commonality in these related tasks to improve performance 
i.e. to carry out multi-task learning (MTL) [1  2]. The aim of this paper is to show how this can be
carried out for the inverse dynamics problem using a multi-task Gaussian process (GP) framework.

In §2 we discuss the relevant theory for the problem. Details of how we optimize the hyperparam-
eters of the multi-task GP are given in §3  and model selection is described in §4. Relationships to
other work are discussed in §5  and the experimental setup and results are given in §6.

2 Theory

We ﬁrst describe the relationship of inverse dynamics functions among contexts in §2.1. In §2.2 we
review the multi-task GP regression model proposed in [3]  and in §2.3 we describe how to derive a
multi-task GP model for the inverse-dynamics problem.

2.1 Linear relationship of inverse dynamics between contexts

Suppose we have a robotic manipulator consisting of J joints  and a set of M loads. Figure 1 illus-
trates a six-jointed manipulator  with joint j connecting links j −1 and j. We wish to learn the inverse

Joint 1
Waist

Joint 2
Shoulder

Base

Joint 3
Elbow

Joint 5
Wrist Bend

q3

Joint 6
Flange

Joint 4

Wrist rotation

Figure 1: Schematic of the PUMA 560 without
the end-effector (to be connected to joint 6).

m = 1 . . . M







1
πm
J  1
· · ·
πm

J  10

τ m
j

· · ·

· · ·· · ·

yjJ  10

hj

j = 1 . . . J

yjJ  1

Figure 2: A schematic diagram on how the dif-
ferent functions are related. A plate repeats its
contents over the speciﬁed range.

dynamics model of the manipulator for the mth context  i.e. when it handles the mth load in its end-
effector connected to the last link. We denote this by τ m(x) ∈ RJ   with x def= (qT  ˙qT  ¨qT)T ∈ R3J .
It can be shown that the required torque for the jth joint can be written as [4]

j (x) = PJ
τ m

(1)
j ′ ∈ R10 is the vector of inertial parameters1
where the yjj ′’s are vector-valued functions of x  and πm
of the j ′th joint when manipulating the mth load. The inertial parameters for a joint depend on the
physical characteristics of its corresponding link (e.g. mass) and are independent of x.

yjj ′ : R3J 7→ R10 

jj ′(x)πm

j ′=j yT

j ′

When  as in our case  the loads are rigidly attached to the end effector  each load may be considered
as part of the last link  and thus modiﬁes the inertia parameters for the last link only [5]. The
parameters for the other links remain unchanged since the parameters are local to the links and their
frames. Denoting the common inertial parameters of the j ′th link by π•

j ′  we can write
(2)
j ′=j yT
jj ′(x)π•
j ′ .
Deﬁne ˜yj(x) def= (hj(x)  (yjJ (x))T)T and ˜πm def= (1  (πm
j (x) = ˜yj(x)T ˜πm. Note
that the ˜yjs are shared among the contexts  while the ˜πms are shared among the J links  as illustrated
in Figure 2. This decomposition is not unique  since given a non-singular square 11×11 matrix Aj 
setting zj(x) def= A−T

where hj(x) def= PJ −1
J )T)T  then τ m

def= Aj ˜πm  we also have

j (x) = hj(x) + yT
τ m

j ˜yj(x) and ρm

jJ (x)πm
J  

j

(3)
Hence the vector of parameters ˜πγ is identiﬁable only up to a linear combination. Note that in
general the matrix Aj may vary across the joints.

j Aj ˜πm = zj(x)Tρm
j .

j (x) = ˜yj(x)TA−1
τ m

2.2 Multi-task GP regression model

We give a brief summary of the multi-task Gaussian process (GP) regression model described in [3].
This model learns M related functions {f m}M
m=1 by placing a zero mean GP prior which directly
induces correlations between tasks. Let tm be the observation of the mth function at x. Then the
model is given by

hf m(x)f m′

(4)
where kx is a covariance function over inputs  K f is a positive semi-deﬁnite (p.s.d) matrix of inter-
task similarities  and σ2

m is the noise variance for the mth task.

tm ∼ N (f m(x)  σ2

mm′kx(x  x′)

= K f

m) 

(x′)i def

2.3 Multi-task GP model for multiple contexts

We now show that the multi-task GP model can be used for inferring inverse dynamics for multiple
contexts. We begin by placing independent zero mean GP priors on all the component functions of
z1(·)  . . .   zJ (·). Let α be an index into the elements of the vector function zj(·)  then our prior is
(5)

hzjα(x)zj ′α′ (x′)i = δjj ′ δαα′ kx

j (x  x′).

1We may also formulate our model using the more general vector of dynamic parameters which includes
also the friction parameters  motor inertia etc. However  these additional parameters are independent of the
load  and so can be absorbed into the function hj in eq. 2.

hτ m
def= (ρ1

j (·) is given by

j (x)τ m′
j | · · · |ρM

In addition to independence speciﬁed by the Kronecker delta functions δ··  this model also imposes
the constraint that all component functions for a given joint j share the same covariance function
j (·  ·). With this prior over the zjs  the Gaussian process prior for τ m
kx
j (x  x′) 

(6)
j )mm′  the
j deﬁnes the similarity
j is the rank of Pj  and is upper bounded by min(M   11) 

j ′ (x′)i = δjj ′ (K ρ
where we have set Pj
j ) and K ρ
def= P T
(m  m′)th entry of the positive semi-deﬁnite matrix K ρ
between different contexts. The rank of K ρ
reﬂecting the fact that there are at most 11 underlying latent functions (see Figure 2).
j (x) may be modelled with
Let tm
j   sharing the vari-
tm
j . . . ≡ σM
j (x) ∼ N (τ m
ance parameters among the contexts. This completes the correspondence with the multi-task GP
model in eq. 4. Note  however  that in this case we have J multi-task GP models  one for each joint.
This model is a simple and convenient one where the prior  likelihood and posterior factorize over
joints. Hence inference and hyperparameter learning can be done separately for each joint.

j )mm′ kx
j Pj  so that (ρm
j . Notice that K ρ

j (x). The deviations from τ m
j ≡ σ2

j (x) be the observed value of τ m

j )2)  though in practice we let σj

j (x)  (σm

j )Tρm′

j = (K ρ

def= σ1

j

Making predictions As in [3]  inference in our model can be done by using the standard GP
formulae for the mean and variance of the predictive distribution with the covariance function given
in eq. 6 together with the normal noise model. The observations over all contexts for a given joint j
will be used to make the predictions. For the case of complete data (where there are observations at
the same set of x-values for all contexts) one can exploit the Kronecker-product structure [3  eq. 2].

2.3.1 The relationship among task similarity matrices

Let ˜Π def= ( ˜π1| · · · | ˜πM ). Recall that ˜πm is an 11 dimensional vector. However  if the different loads
in the end effector do not explore the full space (e.g. if some of the inertial parameters are constant
over all loads)  then it can happen that s def= rank( ˜Π) ≤ min(M   11).
It is worthwhile to investigate the relationship between K ρ
j and K ρ
ρm
j
so that rank(K ρ
exact values may differ. This observation will be useful for model selection in §4.

j ′  j 6= j ′. Recall from eq. 3 that
j Aj ˜Π 
j s have the same rank for all joints  although their

def= Aj ˜πm  where Aj is a full-rank square matrix. This gives Pj = Aj ˜Π and K ρ

j ) = rank( ˜Π). Therefore the K ρ

j = ˜ΠTAT

3 Learning the hyperparameters — a staged optimization heuristic

In this section  we drop the joint index j for the sake of brevity and clarity. The following applies
separately for each joint. Let tm be the vector of nm observed torques at the joint for context m 
and X m be the corresponding 3J ×nm design matrix. Further  let X be the 3J ×N design matrix
of distinct x-conﬁgurations observed over all M contexts. Given this data  we wish to optimize the
marginal likelihood L(θx  K ρ  σ2) def= p({tm}M
m=1|X  θx  K ρ  σ2)  where θx are the parameters of
kx. As pointed out in [3]  one may approach this either using general gradient-based optimization 
or using expectation-maximization. In this paper  the former is used.
In general  the objective function L(θx  K ρ  σ2) will have multiple modes  and it is a difﬁcult prob-
lem of how to locate the best mode. We propose a staged strategy during optimization to help
localize the search region. This is outlined below  with details given in the subsections that follow.

Require: Starting positions θx

0  K ρ

0   σ2

0  and rank r.

{All arg max operations are understood to ﬁnd only the local maximum.}

1: Starting from θx
2: Calculate K1
3: Starting from θx

0 and σ2

0  ﬁnd (θx

1  σ2
ρ based on details in §3.2.
0  ﬁnd (θx

1   and σ2

1  K ρ

1) = arg maxθx σ2 L(θx  K ρ

0   σ2).

ans  K ρ

ans  σ2

ans) = arg maxθx K ρ σ2 L(θx  K ρ  σ2).

The optimization order reﬂects the relative importance of the different constituents of the model.
The most important is kx  hence the estimation of θx begins in step 1; the least important is σ2 
hence its estimation from the initial value σ2
0 is in step 3. For our application  we ﬁnd that this
strategy works better than one which simultaneously optimizes for all the parameters.

3.1 The initial choice of K ρ

The choice of K ρ
0 is important  since it affects the search very early on. Reasonable values that
admit ready interpretations are the matrix of ones 11T and the identity matrix I. For K ρ
0 = 11T 
we initially assume the contexts to be indistinguishable from each other; while for K ρ
0 = I  we
initially assume the contexts to be independent given the kernel parameters  which is a multi-task
learning model that has been previously explored  e.g. [6]. These two are at the opposite extremes
in the spectrum of inter-context/task correlation  and we believe the merit of each will be application
dependent. Since these two models have the same number of free parameters  we select the one with
the higher likelihood as the starting point for the search in step 2. However  we note that in some
applications there may be reasons to prefer one over the other.

3.2 Computation of K ρ

1 in step 2

1 and σ2

1  we wish to estimate a K ρ

Given estimates θx
in step 3. Here we give the sequence of considerations that leads to a formula for computing K ρ
1 .
1 for kx. Let T be an N×M matrix which
Let K x
corresponds to the true values of the torque function τ m(xi) for m = 1  . . .   M and i = 1  . . .   N.
Then as per the EM step discussed in [3  eq. 4]  we have

1 be the covariance matrix for all pairs in X  using θx

1 from which the likelihood can be optimized

EM = N −1 (cid:10)T T(K x
K ρ

1 )−1T (cid:11)˜θ0

≃ N −1 hT iT
˜θ0

(K x

1 )−1 hT i˜θ0

 

(7)

1  K ρ

0   σ2

0   so that the rank of K ρ

where the expectations are taken w.r.t a GP with parameters ˜θ0 = (θx
1)  and the (i  m)th
entry of hT i˜θ0
is the mean of τ m(xi) with this GP. The approximation neglects the GP’s variance;
this is justiﬁable since the current aim is to obtain a starting estimate of K ρ for a search procedure.
There are two weaknesses with eq. 7 that we shall address. The ﬁrst is that the rank of hT i˜θ0
is
EM is similarly upper bounded.2 This property
upper bounded by that of K ρ
is undesirable  particularly when K ρ
with the
corresponding observed value tm(xi) wherever it is available  and call the resultant matrix Taug.
The second weakness is that with the commonly used covariance functions  K x
1 will typically have
rapidly decaying eigenvalues [7  §4.3.1]. To overcome this  we regularize its inversion by adding η2I
to the diagonal of K x
augTaug)/(M N ) 
so that tr(K ρ
Finally  the required K ρ
rently achieved by computing the eigen-decomposition of K ρ
vectors/values; it could also be implemented using an incomplete Cholesky decomposition.

aug by constraining it to have rank r. This is cur-
aug and keeping only the top r eigen-

0 = 11T . We ameliorate this by replacing hτ m(xi)i˜θ0

aug(K x
1 were the zero matrix.

1 + η2I)−1Taug. We set η2 to tr(T T

1 is obtained from K ρ

aug) = M if K x

aug = N −1T T

1 to give K ρ

3.3

Incorporating a novel task

Above we have assumed that data from all contexts is available at training time. However  we may
encounter a new context for which we have not seen much data. In this case we ﬁx θx and σ2 while
extending K ρ by an extra row and column for the new context  and it is only this new border which
needs to be learned by maximising the marginal likelihood. Note that as K ρ is p.s.d this means
learning only at most M new parameters  or fewer if we exploit the rank-constraint property of K ρ.

4 Model selection

j in the model is important  since it reﬂects on the rank s of ˜Π. In our
The choice of the rank r of K ρ
model  r is not a hyperparameter to be optimized. Thus to infer its value we rely on an information
criterion to select the most parsimonious correct model. Here  we use the Bayesian Information
Criterion (BIC)  but the use of Akaike or Hannan-Quinn criteria is similar.
Let Ljr be the likelihood for each joint at optimized hyperparameters θx
is constrained to have rank r; let nm

j   when K ρ
j be the number of observations for the jth joint in the mth

j   and σ2

j   K ρ

j

2This is not due to our approximation; indeed  it can be shown that the rank of K ρ

EM is upper bounded by

that of K ρ

0 even if the exact EM update in eq. 7 has been used.

context  and n def= Pj m nm
θx
j . Since the likelihood of the model factorizes over joints  we have

j be the total number of observations; and let dj be the dimensionality of

BIC(r) = −2PJ

j=1 log Ljr + (cid:16)PJ

j=1 dj + J

2 r(2M + 1 − r) + J(cid:17) log n 

(8)

where r(2M + 1 − r)/2 is the number of parameters needed to deﬁne an incomplete Cholesky
decomposition of rank r for an M ×M matrix. For selecting the appropriate rank of the K ρ
j s  we
compute and compare BIC(r) for different values of r.

5 Relationships to other work

We consider related work ﬁrst with regard to the inverse dynamics problem  and then to multi-task
learning with Gaussian processes.

Learning methods for the single-context inverse dynamics problem can be found in e.g. [8]  where
the locally weighted projection regression (LWPR) method is used. Gaussian process methods for
the same problem have also been shown to be effective [7  §2.5; 9]. The LWPR method has been
extended to the multi-context situation by Petkos and Vijayakumar [5]. If the inertial parameters
J s are known for at least 11 contexts then the estimated torque functions can be used to estimate
πm
the underlying yjj ′s using linear regression  and prediction in a novel context (with limited training
data) will depend on estimating the inertial parameters for that context. Assuming the original
estimated torque functions are imperfect  having more than 11 models for distinct known inertial
parameters will improve load estimation. If the inertial parameters are unknown  the novel torque
function can still be represented as a linear combination of a set of 11 linearly independent torque
functions  and so one can estimate the inverse dynamics in a novel context by linear regression on
those estimated functions. In contrast to the known case  however  no more than 11 models can be
used [5  §V]. Another difference between known and unknown parameters is that in the former case
the resulting πm

J s are interpretable  while in the latter there is ambiguity due to the Ajs in eq. 3.

Comparing our approach with [5]  we note that: (a) their approach does not exploit the knowledge
that the torque functions for the different contexts are known to share latent functions as in eq. 2 
and thus it may be useful to learn the M inverse dynamics models jointly. This is expected to be
particularly advantageous when the data for each task explores rather different portions of x-space;
(b) rather than relying on least-squares methods (which assume equal error variances everywhere) 
our fully probabilistic model will propagate uncertainties (co-variances for jointly Gaussian models)
automatically; and (c) eq. 6 shows that we do not need to be limited to exactly 11 reference contexts 
either fewer or more than 11 can be used. On the other hand  using the LWPR methods will generally
give rise to better computational scaling for large data-sets (although see approximate GP methods
in [7  ch. 8])  and are perhaps less complex than the method in this paper.

Earlier work on multiple model learning such as Multiple Model Switching and Tuning (MMST)
[10] uses an inverse dynamics model and a controller for each context  switching among the models
to the one producing the most accurate predictions. The models are linear-in-the-parameters with
known non-linear regressor functions of x  and the number of models are assumed known. MMST
involves very little dynamics learning  estimating only the linear parameters of the models. A closely
related approach is Modular Selection and Identiﬁcation for Control (MOSAIC) [11]  which uses
inverse dynamics models for control and forward dynamics models for context identiﬁcation. How-
ever  MOSAIC was developed and tested on linear dynamics models without the insights into how
eq. 1 may be used across contexts for more efﬁcient and robust learning and control.

Early references to general multi-task learning are [1] and [2]. There has been a lot of work in recent
years on MTL with e.g. neural networks  Dirichlet processes  Gaussian processes and support vector
machines. Some previous models using GPs are summarized in [3]. An important related work is the
semiparametric latent factor model [12] which has a number of latent processes which are linearly
combined to produce observable functions as in eq. 3. However  in our model all the latent functions
share a common covariance function  which reduces the number of free parameters and should thus
help to reduce over-ﬁtting. Also we note that the regression experiments by Teh et al. [12  §4] used
a forward dynamics problem on a four-jointed robot arm for a single context  with an artiﬁcial linear
mixing of the four target joint accelerations to produce six response variables. In contrast  we have
shown how linear mixing arises naturally in a multi-context inverse dynamics situation. In relation

p3

p4

p2

p1

0.5

z/m
0.3

0.2
y/m

0
−0.2

0.3

0.4

0.7
Figure 3: The four paths p1  p2  p3  p4. The
robot base is located at (0  0  0).

x/m

0.6

0.5

Table 1: The trajectories at which the training
samples for each load are acquired. All loads
have training samples from the common trajec-
tory (p2  s3). For the multiple-contexts setting 
c15  and hence (p4  s4)  is not used for training.

s1
c1
c6
c11
c2

p1
p2
p3
p4

s2
c7
c12
c3
c8

s3
c13

c1 · · · c15

c4
c9

s4
c14
c5
c10
c15∗

Table 2: The average nMSEs of the predictions by LR and sGP  for joint 3 and for both kinds of test
sets. Training set sizes given in the second row. The nMSEs are averaged over loads c1 . . . c15.

average nMSE for the interpm sets
4000
20
6×10−4
3×10−9

1004
6×10−4
2×10−8

7×10−4
2×10−7

1×10−1
1×10−2

170

average nMSE for the extrapm sets
4000
20
2×10−1
3×10−3

1004
2×10−1
4×10−3

2×10−1
3×10−2

5×10−1
1×10−1

170

LR
sGP

to work by Bonilla et al. [3] described in section 2.2  we note that the factorization between inter-task
similarity K f and a common covariance function kx is an assumption there  while we have shown
that such decomposition is inherent in our application.

6 Experiments

Data We investigate the effectiveness of our model with the Puma 560 (Figure 1)  which has
J = 6 degrees of freedom. We learn the inverse dynamic models of this robot manipulating M = 15
different loads c1  . . .   c15 through four different ﬁgure-of-eight paths at four different speeds. The
data for our experiments is obtained using a realistic simulation package [13]  which models both
Coulomb and viscous frictional forces. Figure 3 shows the paths p1  . . .   p4 which are placed at
0.35m  0.45m  0.55m and 0.65m along the x-axis  at 0.36m  0.40m  0.44m and 0.48m along the
z-axis  and rotated about the z-axis by −10◦  0◦  10◦ and 20◦. There are four speeds s1  . . .   s4 
ﬁnishing a path in 20s  15s  10s and 5s respectively.
In general  loads can have very different
physical characteristics; in our case  this is done by representing each load as a cuboid with differing
dimensions and mass  and attaching each load rigidly to a random point at the end-effector. The
masses range evenly from 0.2kg for c1 to 3.0kg for c15; details of the other parameters are omitted
due to space constraints.
For each load cm  4000 data points are sampled at regular intervals along the path for each path-speed
(trajectory) combination (p·  s·). Each sample is the pair (t  x)  where t ∈ RJ are the observed
torques at the joints  and x ∈ R3J are the joint angles  velocities and accelerations. This set of data
is partitioned into train and test sets in the manner described below.

Acquiring training data combinatorially by sampling for every possible load-trajectory pair may be
prohibitively expensive. One may imagine  however  that training data for the handling of a load can
be obtained along a ﬁxed reference trajectory Tr for calibration purposes  and also along a trajectory
typical for that load  say Tm for the mth load. Thus  for each load  2000 random training samples
are acquired at a common reference trajectory Tr = (p2  s3)  and an additional 2000 random training
samples are acquired at a trajectory unique to each load; Table 1 gives the combinations. Therefore
each load has a training set of 4000 samples  but acquired only on two different trajectories.
Following [14]  two kinds of test sets are used to assess our models for (a) control along a repeated
trajectory (which is of practical interest in industry)  and (b) control along arbitrary trajectories
(which is of general interest to roboticists). The test for (a) assesses the accuracy of torque predic-
tions for staying within the trajectories that were used for training. In this case  the test set for load
cm  denoted by interpm for interpolation  consists of the rest of the samples from Tr and Tm that are
not used for training. The test for (b) assesses the accuracy also for extrapolation to trajectories not

sampled for training. The test set for this  denoted by extrapm  consists of all the samples that are
not training samples for cm.
In addition  we consider a data-poor scenario  and investigate the quality of the models using ran-
domly selected subsets of the training data. The sizes of these subsets range from 20 to 4000.

Results comparing GP with linear regression We ﬁrst compare learning the inverse dynamics
with Bayesian linear regression (LR) to learning with single-task Gaussian processes (sGP). For each
context and each joint  we train a LR model and a sGP model with the corresponding training data
separately. For LR  the covariates are (x  sgn( ˙q)  1)  where sgn(·) is the component-wise signum
of its arguments; regression coefﬁcients β and noise variance σ2 are given a broad normal-inverse-
gamma prior p(β  σ2) ≡ N (β|0  σ2 · 108I)IG(σ2|1  1)  though note that the mean predictions do
not depend on the parameters of the inverse-gamma prior on σ2. The covariance function of each
sGP model is a sum of an inhomogeneous linear kernel on (x  sgn( ˙q))  a squared exponential kernel
on x  and an independent noise component [7  §4.2]  with the ﬁrst two using the automatic relevance
determination parameterization [7  §5.1]. The hyperparameters of sGP are initialized by giving equal
weightings among the covariates and among the components of the covariance function  and then
learnt by optimizing the marginal likelihood independently for each context and each joint.
The trained LR and sGP models are used to predict torques for the interpm and extrapm data sets. For
each test set  the normalized mean square error (nMSE) of the predictions are computed  by dividing
the MSE by the variance of the test data. The nMSEs are then averaged over the 15 contexts for
the interpm and extrapm tests. Table 2 shows how the averages for joint 3 vary with the number
of training samples. Similar relative results are obtained for the other joints. The results show that
sGP outperforms LR for both the test cases. As one would expect  the errors of LR level-off early
at around 200 training samples  while the quality of predictions by sGP continues to improve with
training sample size  especially so for the interpm sets. Both sGP and LR do reasonably well on the
interpm sets  but not so well on the extrapm sets. This suggests that learning from multiple contexts
which have training data from different parts of the trajectory space will be advantageous.

Results for multi-task GP We now investigate the merit of using MTL  using the training data
tabulated in Table 1 for loads c1  . . .   c14. We use n to denote the number of observed torques for
each joint totalled across the 14 contexts. Note that trajectory (p4  s4) is entirely unobserved during
learning  but is included in the extrapm sets. We learn the hyperparameters of a multi-task GP model
(mGP) for each joint by optimizing the marginal likelihood for all training data (accumulated across
contexts) for that joint  as discussed in §3  using the same kernel and parameterization as for the
sGP. This is done for ranks 2  4  5  6  8 and 10. Finally  a common rank r for all the joints is chosen
using the selection criterion given in §4. We denote the selected set of mGP models by mGP-BIC.

In addition to comparing with sGP  we also compare mGP-BIC with two other na¨ıve schemes: (a)
denoted by iGP  a collection of independent GPs for the contexts  but sharing kernel parameters of
j among the contexts; and (b) denoted by pGP  a single GP for each joint that learns by pooling
kx
all training data from all the contexts. The iGP and pGP models can be seen as restrictions of the
multi-task GP model  restricting K ρ
j to the identity matrix I and the matrix of ones 11T respectively.
As discussed in §3  the hyperparameters for the mGPs are initialized to either those of pGP or those
of iGP during optimization  choosing the one with the higher marginal likelihood. For our data 
we ﬁnd that the choice is mostly iGP; pGP is only chosen for the case of joint 1 and n < 532. In
addition  the chosen ranks based on the BIC are r = 4 for all cases of n  except for n = 476 and
n = 1820 when r = 5 is selected instead.
Figure 4 gives results of sGP  iGP  pGP and mGP-BIC for both the interpm and extrapm test sets 
and for joints 1 and 4. Plots for the other joints are omitted due to space constraints  but they are
qualitatively similar to the plots for joint 4. The plots are the average nMSEs over the 14 contexts
against n. The vertical scales of the plots indicate that extrapolation is at least an order of magnitude
harder than interpolation. Since the training data are subsets selected independently for the different
values of n  the plots reﬂect the underlying variability in sampling. Nevertheless  we can see that
mGP-BIC performs favorably in almost all the cases  and especially so for the extrapolation task.
For joint 1  we see a close match between the predictive performances of mGP-BIC and pGP  with
mGP-BIC slightly better than pGP for the interpolation task. This is due to the limited variation
among observed torques for this joint across the different contexts for the range of end-effector

×10−5
5

4

3

2

1

×10−4
2

1.5

1

0.5

×10−4
4

3

2

1

×10−2
2

1.5

1

0.5

0
280
1820
(a) joint 1  interpm tests

896

532

0
280
1820
(b) joint 1  extrapm tests

532

896

0
280
1820
(c) joint 4  interpm tests

532

896

0
280
1820
(d) joint 4  extrapm tests

896

532

Figure 4: Average nMSEs of sGP (
) against n (on log2
scale). Ticks on the x-axes represent speciﬁed values of n. The vertical scales of the plots varies. A
value above the upper limit of its vertical range is plotted with a nominal value near the top instead.

) and mGP-BIC (

)  pGP (

)  iGP (

movements investigated here. Therefore it is not surprising that pGP produces good predictions
for joint 1. For the other joints  iGP is usually the next best after mGP-BIC. In particular  iGP is
better than sGP  showing that (in this case) combining all the data to estimate the parameters of a
single common covariance function is better than separating the data to estimate the parameters of
14 covariance functions.

7 Summary

We have shown how the structure of the multiple-context inverse dynamics problem maps onto a
multi-task GP prior as given in eq. 6  how the corresponding marginal likelihood can be optimized
effectively  and how the rank of the K ρ
j s can be chosen. We have demonstrated experimentally
that the results of the multi-task GP method (mGP) are generally superior to sGP  iGP and pGP.
Therefore it is advantageous to learn inverse dynamics models jointly using mGP-BIC  especially
when each context/task explores different portions of the data space  a common case in dynamics
learning. In future work we would like to investigate if coupling learning over joints is beneﬁcial.

Acknowledgments

We thank Sam Roweis for suggesting pGP as a baseline. This work is supported in part by the EU
PASCAL2 ICT Programme  and in part by the EU FP6 SENSOPAC project grant to SV and SK.
KMAC would also like to thank DSO NL for ﬁnancial support.

References
[1] R. Caruana. Multitask Learning. Machine Learning  28(1)  July 1997.
[2] S. Thrun and L. Pratt  editors. Learning to Learn. Kluwer Academic Publishers  1998.
[3] E. Bonilla  K. M. A. Chai  and C. K. I. Williams. Multi-task Gaussian Process Prediction. NIPS 20  2008.
[4] L. Sciavicco and B. Siciliano. Modelling and Control of Robot Manipulators. Springer  2000.
[5] G. Petkos and S. Vijayakumar. Load estimation and control using learned dynamics models. IROS  2007.
[6] T. P. Minka and R. W. Picard. Learning How to Learn is Learning with Point Sets  1997. URL http:

//research.microsoft.com/˜minka/papers/point-sets.html. revised 1999.

[7] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press  2006.
[8] S. Vijayakumar and S. Schaal. LWPR: An O(n) Algorithm for Incremental Real Time Learning in High

[9] D. Nguyen-Tuong  J. Peters  and M. Seeger. Computed torque control with nonparametric regression

Dimensional Space. ICML 2000  2000.

models. ACC 2008  2008.

[10] M. Kemal Cılız and K. S. Narendra. Adaptive control of robotic manipulators using multiple models and

[11] M. Haruno  D. M. Wolpert  and M. Kawato. MOSAIC Model for Sensorimotor Learning and Control.

switching. Int. J. Rob. Res.  15(6):592–610  1996.

Neural Comp.  13(10):2201–2220  2001.

[12] Y. W. Teh  M. Seeger  and M. I. Jordan. Semiparametric latent factor models. 10th AISTATS  2005.
[13] P. I. Corke. A robotics toolbox for MATLAB. IEEE Rob. and Auto. Magazine  3(1):24–32  1996.
[14] E. Burdet and A. Codourey. Evaluation of parametric and nonparametric nonlinear adaptive controllers.

Robotica  16(1):59–73  1998.

,Christopher Meek
Marina Meila