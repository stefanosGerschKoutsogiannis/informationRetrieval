2014,Parallel Feature Selection Inspired by Group Testing,This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory  under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features  for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets  for which our method exhibits competitive performance in terms of running time and accuracy. Moreover  it also yields substantial speedup when used as a pre-processing step for most other existing methods.,Parallel Feature Selection inspired by Group Testing

Yingbo Zhou∗

Utkarsh Porwal∗

CSE Department
SUNY at Buffalo

{yingbozh  utkarshp}@buffalo.edu

Ce Zhang

CS Department

University of Wisconsin-Madison

czhang@cs.wisc.edu

Hung Ngo

CSE Department
SUNY at Buffalo

XuanLong Nguyen
EECS Department

University of Michigan

hungngo@buffalo.edu

xuanlong@umich.edu

Christopher R´e
CS Department

Stanford University

chrismre@cs.stanford.edu

Venu Govindaraju
CSE Department
SUNY at Buffalo

govind@buffalo.edu

Abstract

This paper presents a parallel feature selection method for classiﬁcation that scales
up to very high dimensions and large data sizes. Our original method is inspired
by group testing theory  under which the feature selection procedure consists of a
collection of randomized tests to be performed in parallel. Each test corresponds
to a subset of features  for which a scoring function may be applied to measure
the relevance of the features in a classiﬁcation task. We develop a general the-
ory providing sufﬁcient conditions under which true features are guaranteed to
be correctly identiﬁed. Superior performance of our method is demonstrated on
a challenging relation extraction task from a very large data set that have both
redundant features and sample size in the order of millions. We present compre-
hensive comparisons with state-of-the-art feature selection methods on a range of
data sets  for which our method exhibits competitive performance in terms of run-
ning time and accuracy. Moreover  it also yields substantial speedup when used
as a pre-processing step for most other existing methods.

Introduction

1
Feature selection (FS) is a fundamental and classic problem in machine learning [10  4  12]. In
classiﬁcation  FS is the following problem: Given a universe U of possible features  identify a
subset of features F ⊆ U such that using the features in F one can build a model to best predict
the target class. The set F not only inﬂuences the model’s accuracy  its computational cost  but also
the ability of an analyst to understand the resulting model. In applications  such as gene selection
from micro-array data [10  4]  text categorization [3]  and ﬁnance [22]  U may contain hundreds of
thousands of features from which one wants to select only a small handful for F .
While the overall goal is to have an FS method that is both computationally efﬁcient and statistically
sound  natural formulations of the FS problem are known to be NP-hard [2]. For large scale data 
scalability is a crucial criterion  because FS often serves not as an end but a means to other sophis-
ticated subsequent learning. In reality  practitioners often resort to heuristic methods  which can
broadly be categorized into three types: wrapper  embedded  and ﬁlter [10  4  12]. In the wrapper
method  a classiﬁer is used as a black-box to test on any subset of features. In ﬁlter methods no
classiﬁer is used; instead  features are selected based on generic statistical properties of the (labeled)

∗denotes equal contribution

1

data such as mutual information and entropy. Embedded methods have built in mechanisms for FS
as an integral part of the classiﬁer training. Devising a mathematically rigorous framework to ex-
plain and justify FS heuristics is an emerging research area. Recently Brown et al. [4] considered
common FS heuristics using a formulation based on conditional likelihood maximization.
The primary contribution of this paper is a new framework for parallelizable feature selection  which
is inspired by the theory of group testing. By exploiting parallelism in our test design we obtain a
FS method that is easily scalable to millions of features and samples or more  while preserving
useful statistical properties in terms of classiﬁcation accuracy  stability and robustness. Recall that
group testing is a combinatorial search paradigm [7] in which one wants to identify a small subset of
“positive items” from a large universe of possible items. In the original application  items are blood
samples of WWII draftees and an item is positive if it is infected with syphilis. Testing individual
blood sample is very expensive; the group testing approach is to distribute samples into pools in
a smart way. If a pool is tested negative  then all samples in the pool are negative. On the other
hand  if a pool is tested positive then at least one sample in the pool is positive. We can think of
the FS problem in the group testing framework: there is a presumably small  unknown subset F of
relevant features in a large universe of N features. Both FS and group testing algorithms perform
the same basic operation: apply a “test” to a subset T of the underlying universe; this test produces
a score  s(T )  that is designed to measure the quality of the features T (or return positive/negative
in the group testing case). From the collection of test scores the relevant features are supposed to
be identiﬁed. Most existing FS algorithms can be thought of as sequential instantiations in this
framework1: we select the set T to test based on the scores of previous tests. For example  let X =
(X1  . . .   XN ) be a collection of features (variables) and Y be the class label. In the joint mutual
information (JMI) method [25]  the feature set T is grown sequentially by adding one feature at each
iteration. The next feature’s score  s(Xk)  is deﬁned relative to the set of features already selected in
Xj∈T I(Xk  Xj; Y ). As each such scoring operation takes a non-negligible amount

T : s(Xk) =(cid:80)

of time  a sequential method may take a long time to complete.
A key insight is that group testing needs not be done sequentially. With a good pooling design  all
the tests can be performed in parallel in which we determine the pooling design without knowing
any pool’s test outcome. From the vector of test outcomes  one can identify exactly the collection
of positive blood samples. Parallel group testing  commonly called non-adaptive group testing
(NAGT) is a natural paradigm and has found numerous applications in many areas of mathematics 
computer Science  and biology [18]. It is natural to wonder whether a “parallel” FS scheme can be
designed for machine learning in the same way NAGT was possible: all feature sets T are speciﬁed
in advance  without knowing the scores of any other tests  and from the ﬁnal collection of scores the
features are identiﬁed. This paper initiates a mathematical investigation of this possibility.
At a high level  our parallel feature selection (PFS) scheme has three inter-related components: (1)
the test design indicates the collection of subsets of features to be tested  (2) the scoring function
s : 2[N ] → R that assigns a score to each test  and (3) the feature identiﬁcation algorithm that
identiﬁes the ﬁnal selected feature set from the test scores. The design space is thus very large. Every
combination of the three components leads to a new PFS scheme.2 We argue that PFS schemes are
preferred over sequential FS for two reasons:

1. scalability  the tests in a PFS scheme can be performed in parallel  and thus the scheme can

be scaled to large datasets using standard parallel computing techniques  and

2. stability  errors in individual trials do not affect PFS methods as dramatically as sequential
methods. In fact  we will show in this paper that increasing the number of tests improves
the accuracy of our PFS scheme.

We propose and study one such PFS approach. We show that our approach has comparable (and
sometimes better) empirical quality compared to previous heuristic approaches while providing
sound statistical guarantees and substantially improved scalability.
Our technical contributions We propose a simple approach for the ﬁrst and the third components
of a PFS scheme. For the second component  we prove a sufﬁcient condition on the scoring function
under which the feature identiﬁcation algorithm we propose is guaranteed to identify exactly the set

1A notable exception is the MIM method  which is easily parallelizable and can be regarded as a special

implementation of our framework

2It is important to emphasize that this PFS framework is applicable to both ﬁlter and wrapper approaches.

In the wrapper approach  the score s(T ) might be the training error of some classiﬁer  for instance.

2

of original (true) features. In particular  we introduce a notion called C-separability  which roughly
indicates the strength of the scoring function in separating a relevant feature from an irrelevant
feature. We show that when s is C-separable and we can estimate s  we are able to guarantee exact
recovery of the right set of features with high probability. Moreover  when C > 0  the number of
tests can be asymptotically logarithmic in the number of features in U.
In theory  we provide sufﬁcient conditions (a Na¨ıve Bayes assumption) according to which one can
obtain separable scoring functions  including the KL divergence and mutual information (MI). In
practice  we demonstrate that MI is separable even when the sufﬁcient condition does not hold 
and moreover  on generated synthetic data sets  our method is shown recover exactly the relevant
features. We proceed to provide a comprehensive evaluation of our method on a range of real-world
data sets of both large and small sizes. It is the large scale data sets where our method exhibits
superior performance.
In particular  for a huge relation extraction data set (TAC-KBP) that has
millions redundant features and samples  we outperform all existing methods in accuracy and time 
in addition to generating plausible features (in fact  many competing methods could not ﬁnish the
execution). For the more familiar NIPS 2013 FS Challenge data  our method is also competitive
(best or second-best) on the two largest data sets. Since our method hinges on the accuracy of score
functions  which is difﬁcult achieve for small data  our performance is more modest in this regime
(staying in the middle of the pack in terms of classiﬁcation accuracy). Nonetheless  we show that our
method can be used as a preprocessing step for other FS methods to eliminate a large portion of the
feature space  thereby providing substantial computational speedups while retaining the accuracy of
those methods.

2 Parallel Feature Selection
The general setting Let N be the total number of input features. For each subset T ⊆ [N ] :=
{1  . . .   N}  there is a score s(T ) normalized to be in [0  1] that assesses the “quality” of features in
T . We select a collection of t tests  each of which is a subset T ⊆ [N ] such that from the scores
of all tests we can identify the unknown subset F of d relevant variables that are most important
to the classiﬁcation task. We encode the collection of t tests with a binary matrix A = (aij) of
dimension t × N  where aij = 1 iff feature j belongs to test i. Corresponding to each row i of A is
a “test score” si = s({j | aij = 1}) ∈ [0  1]. Specifying A is called test design  identifying F from
the score vector (si)i∈[t] is the job of the feature identiﬁcation algorithm. The scheme is inherently
parallel because all the tests must be speciﬁed in advance and executed in parallel; then the features
are selected from all the test outcomes.

Test design and feature identiﬁcation Our test design and feature identiﬁcation algorithms are
extremely simple. We construct the test matrix A randomly by putting a feature in the test with
probability p (to be chosen later). Then  from the test scores we rank the features and select d
top-ranked features. The ranking function is deﬁned as follows. Given a t × N test matrix A  let
aj denote its jth column. The dot-product (cid:104)aj  s(cid:105) is the total score of all the tests that feature j
participates in. We deﬁne ρ(j) = (cid:104)aj  s(cid:105) to be the rank of feature j with respect to the test matrix
A and the score function s.

The scoring function The crucial piece stiching together the entire scheme is the scoring func-
tion. The following theorem explains why the above test design and feature identiﬁcation strategy
make sense  as long as one can choose a scoring function s that satisﬁes a natural separability prop-
erty. Intuitively  separable scoring functions require that adding more hidden features into a test set
increase its score.
Deﬁnition 2.1 (Separable scoring function). Let C ≥ 0 be a real number. The score function
s : 2[N ] → [0  1] is said to be C-separable if the following property holds: for every f ∈ F and
˜f /∈ F   and for every T ⊆ [N ] − {f  ˜f}  we have s(T ∪ {f}) − s(T ∪ { ˜f}) ≥ C.

In words  with a separable scoring function adding a relevant feature should be better than adding
an irrelevant feature to a given subset T of features. Due to space limination  the proofs of the
following theorem  propositions  and corollaries can be found in the supplementary materials. The
essence of the idea is that  when s can separate relevant features from irrelevant features  with high
probability a relevant feature will be ranked higher than an irrelevant feature. Hoeffding’s inequality
is then used to bound the number of tests.

3

Theorem 2.2. Let A be the random t × N test matrix obtained by setting each entry to be 1 with
probability p ∈ [0  1] and 0 with probability 1 − p. If the scoring function s is C-separable  then the
expected rank of a feature in F is at least the expected rank of a feature not in F .
Furthermore  if C > 0  then for any δ ∈ (0  1)  with probability at least 1 − δ every feature in F
has rank higher than every feature not in F   provided that the number of tests t satisﬁes

(cid:18) d(N − d)

(cid:19)

t ≥

2

C 2p2(1 − p)2 log

δ

.

(1)

By setting p = 1/2 in the above theorem  we obtain the following. It is quite remarkable that 
assuming we can estimate the scores accurately  we only need about O(log N ) tests to identify F .
Corollary 2.3. Let C > 0 be a constant such that there is a C-separable scoring function s. Let
d = |F|  where F is the set of hidden features. Let δ ∈ (0  1) be an arbitrary constant. Then  there
is a distribution of t × N test matrices A with t = O(log(d(N − d)/δ)) such that  by selecting a
test matrix randomly from the distribution  the d top-ranked features are exactly the hidden features
with probability at least 1 − δ.
Of course  in reality estimating the scores accurately is a very difﬁcult problem  both statistically
and computationally  depending on what the scoring function is. We elaborate more on this point
below. But ﬁrst  we show that separable scoring functions exist  under certain assumption about the
underlying distribution.
Sufﬁcient conditions for separable scoring functions We demonstrate the existence of separable
scoring functions given some sufﬁcient conditions on the data. In practice  loss functions such as
classiﬁcation error and other surrogate losses may be used as scoring functions. For binary classiﬁ-
cation  information-theoretic quantities such as Kullback-Leibler divergence  Hellinger distance and
the total variation — all of which special cases of f-divergences [5  1] — may also be considered.
For multi-class classiﬁcation  mutual information (MI) is a popular choice.
The data pairs (X  Y ) are assumed to be iid samples from a joint distribution P (X  Y ). The fol-
lowing result shows that under the so-called “naive Bayes” condition  i.e.  all components of random
vector X are conditionally independent given label variable Y   the Kullback-Leibler distance is a
separable scoring function in a binary classiﬁcation setting:
Proposition 2.4. Consider the binary classiﬁcation setting  i.e.  Y ∈ {0  1} and assume that the
naive Bayes condition holds. Deﬁne score function to be the Kullback-Leibler divergence:

s(T ) := KL(P (X T|Y = 0)||P (X T|Y = 1)).

Then s is a separable scoring function. Moreover  s is C-separable  where C := minf∈F s(f ).
Proposition 2.5. Consider the multi-class classiﬁcation setting  and assume that the naive Bayes
condition holds. Moreover  for any pair f ∈ F and ˜f /∈ F   the following holds for any T ⊆
[N ] − {f  ˜f}

I(Xf ; Y ) − I(Xf ; X T ) ≥ I(X ˜f ; Y ) − I(X ˜f ; X T ).

Then  the MI function s(T ) := I(XT ; Y ) is a separable scoring function.
We note the naturalness of the condition so required  as quantity I(Xf ; Y ) − I(Xf ; XT ) may be
viewed as the relevance of feature f with respect to the label Y   subtracted by the redundancy with
other existing features T . If we assume further that X ˜f is independent of both X T and the label Y  
and there is a positive constant C such that I(Xf ; Y ) − I(Xf ; XT ) ≥ C for any f ∈ F   then s(T )
is obviously a C-separable scoring function. It should be noted that the naive Bayes conditions are
sufﬁcient  but not necessary for a scoring function to be C-separable.
Separable scoring functions for ﬁlters and wrappers.
In practice  information-based scoring
functions need to be estimated from the data. Consistent estimators of scoring functions such as KL
divergence (more generally f-divergences) and MI are available (e.g.  [20]). This provides the theo-
retical support for applying our test technique to ﬁlter methods: when the number of training data is
sufﬁciently large  a consistent estimate of a separable scoring function must also be a separable scor-
ing function. On the other hand  a wrapper method uses a classiﬁcation algorithm’s performance as
a scoring function for testing. Therefore  the choice of the underlying (surrogate) loss function plays
a critical role. The following result provides the existence of loss functions which induce separable
scoring functions for the wrapper method:

4

P (X T|Y = 1). Assume that an f-divergence of the form: s(T ) = (cid:82) φ(dP T

0 := P (X T|Y = 0)  P T
1 )dP T

0 /dP T

0 and P T

Proposition 2.6. Consider the binary classiﬁcation setting  and let P T
1 :=
1 is a
separable scoring function for some convex function φ : R+ → R. Then there exists a surrogate
loss function l : R × R → R+ under which the minimum l-risk: Rl(T ) := inf g E [l(Y  g(X T ))] is
also a separable scoring function. Here the inﬁmum is taken over all measurable classiﬁer functions
g acting on feature input X T   E denotes expectation with respect to the joint distribution of X T
and Y .
This result follows from Theorem 1 of [19]  who established a precise correspondence between f-
divergences deﬁned by convex φ and equivalent classes of surrogate losses l. As a consequence 
if the Hellinger distance between P T
1 is separable  then the wrapper method using the
Adaboost classiﬁer corresponds to a separable scoring function. Similarly  a separable Kullback-
Leibler divergence implies that of a logistic regression based wrapper; while a separable variational
distance implies that of a SVM based wrapper.
3 Experimental results
3.1 Synthetic experiments
In this section  we synthetically illustrate that separable scoring functions exist and our PFS frame-
work is sound beyond the Na¨ıve Bayes assumption (NBA). We ﬁrst show that MI is C-separable for
large C even when the NBA is violated. The NBA was only needed in Propositions 2.4 and 2.5 in
order for the proofs to go through. Then  we show that our framework recovers exactly the relevant
features for two common classes of input distributions.
We generate 1  000 data points from two sep-
arated 2-D Gaussians with the same covari-
ance matrix but different means  one centered
at (−2 −2) and the other at (2  2). We start
with the identity covariance matrix  and gradu-
ally change the off diagonal element to −0.999 
representing highly correlated features. Then 
we add 1 000 dimensional zero mean Gaussian
noise with the same covariance matrix  where
the diagonal is 1 and the off-diagonal elements
increases from 0 gradually to 0.999. We then
calculate the MI between two features and the
class label  and the two features are selected in
three settings: 1) the two genuine dimensions;
2) one of the genuine feature and one from the
noisy dimensions; 3) two random pair from the
noisy dimensions. The MI that we get from
these three conditions is shown in Figure 1. It is clear from this ﬁgure MI is a separable scoring
function  despite the fact that the NBA is violated.
We also synthetically evaluated our entire PFS idea  using two multinomials and two Gaussians to
generate two binary classiﬁcation task data. Our PFS scheme is able to capture exactly the relevant
features in most cases. Details are in the supplementary material section due to lack of space.
3.2 Real-world data experiment results
This section evaluates our approach in terms of accuracy  scalability  and robustness accross a range
of real-world data sets: small  medium  and large. We will show that our PFS scheme works very
well on medium and large data sets; because  as was shown in Section 3.1  with sufﬁcient data to
estimate test scores  we expect our method to work well in terms of accuracy. On the small datasets 
our approach is only competitive and does not dominate existing approaches  due to the lack of data
to estimate scores well. However  we show that we can still use our PFS scheme as a pre-processing
step to ﬁlter down the number of dimensions; this step reduces the dimensionality  helps speed up
existing FS methods from 3-5 times while keeps their accuracies.
3.2.1 The data sets and competing methods
Large: TAC-KBP is a large data set with the number of samples and dimensions in the millions3;
its domain is on relation extraction from natural language text. Medium: GISETTE and MADE-

Figure 1:
Illustration of MI as a separable scor-
ing function for the case of statistically dependent
features. The top left point shows the scores for
the 1st setting; the middle points shows the scores
for the 2nd setting; and the bottom points shows
the scores for the 3rd setting.

3http://nlp.cs.qc.cuny.edu/kbp/2010/

5

LON are two largest data sets from the NIPS 2003 feature selection challenge4  with the number of
dimensions in the thousands. Small: Colon  Leukemia  Lymph  NCI9  and Lung are chosen from
the small Micro-array datasets [6]  along with the UCI datasets5. These sets typically have a few
hundreds to a few thousands variables  with only tens of data samples.
We compared our method with various baseline methods including mutual
information
maximization[14] (MIM)  maximum relevancy minimum redundancy[21] (MRMR)  conditional
mutual information maximization[9] (CMIM)  joint mutual information[25] (JMI)  double input
symmetrical relevance[16] (DISR)  conditional infomax feature extraction[15] (CIFE)  interaction
capping[11] (ICAP)  fast correlation based ﬁlter[26] (FCBF)  local learning based feature selection
[23] (LOGO)  and feature generating machine [24] (FGM).
3.2.2 Accuracy

(a) Precision/Recall of different
Figure 2: Result from different methods on TAC-KBP dataset.
methods; (b) Top-5 keywords appearing in the Top-20 features selected by our method. Dotted lines
in (a) are FGM (or MIM) with our approach as pre-processing step.
Accuracy results on large data set. As shown in Figure 2(a)  our method dominates both MIM
and FGM. Given the same precision  our method achieves 2-14× higher recall than FGM  and 1.2-
2.4× higher recall than MIM. Other competitors do not ﬁnish execution in 12 hours. We compare the
top-features produced by our method and MIM  and ﬁnd that our method is able to extract features
that are strong indicators only when they are combined with other features  while MIM  which tests
features individually  ignores this type of combination. We then validate that the features selected
by our method makes intuitive sense. For each relation  we select the top-20 features and report the
keyword in these features.6 As shown in Figure 2(b)  these top-features selected by our method are
good indicators of each relation. We also observe that using our approach as the pre-processing step
improves the quality of FGM signiﬁcantly. In Figure 2(a) (the broken lines)  we run FGM (MIM)
on the top-10K features produced by our approach. We see that running FGM with pre-processing
achieves up to 10× higher recall given the same precision than running FGM on all 1M features.
Accuracy results on medium data sets Since the focus of the evaluation is to analyze the efﬁcacy
of feature selection approaches  we employed the same strategy as Brown et al.[4] i.e.
the ﬁnal
classiﬁcation is done using k-nearest neighbor classiﬁer with k ﬁxed to three  and applied Euclidean
distance7.
We denote our method by Fk (and Wk)  where F denotes ﬁlter (and W denotes wrapper method).
k denotes the number of tests (i.e. let N be the dimension of data  then the total number of tests is
kN). We bin each dimension of the data into ﬁve equal distanced bins when the data is real valued 
otherwise the data is not processed8. MI is used as the scoring function for ﬁlter method  and log-
likelihood is used for scoring the wrapper method. The wrapper we used is logistic regression9.
For GISETTE we select up to 500 features and for MADELON we select up to 100 features. To get
the test results  we use the features according to the smallest validation error for each method  and
the results on test set are illustrated in table 4.

4http://www.nipsfsc.ecs.soton.ac.uk/datasets/
5http://archive.ics.uci.edu/ml/
6Following the syntax used by Mintz et al. [17]  if a feature has the form [⇑poss wif e ⇓prop of ]  we report

the keyword as wife in Figure 2(b).

7The classiﬁer for FGM is linear support vector machine (SVM)  since it optimized for the SVM criteria.
8For SVM based method  the real valued data is not processed  and all data is normalized to have unit length.
9The logistic regressor used in wrapper is only to get the testing scores  the ﬁnal classiﬁcation scheme is

still k-NN.

6

0 0.5 1 0 0.1 0.2 Precision Recall Spouse MemberOf TopMember wife leader head pictures member executive husband rebels chairman married commander general widower iraq leader president (b) Keywords in top features (a) P/R-Curve Ours FGM MIM Table 1: Test set balanced error rate (%) from different methods on NIPS datasets
Datasets
Ours
(W10)
2.89
10.50

Best
Perf.
2.15
GISETTE
MADELON 10.61

Ours
(F3)
4.85
22.61

3rd Best Median
Perf.
3.86
25.92

Perf.
3.09
12.33

2nd Best

Perf.
3.06
11.28

Ours
(W3)
2.72
10.17

Ours
(F10)
4.69
18.39

Accuracy results on the small data sets. As expected  due to the lack of data to estimate scores 
our accuracy performance is average for this data set. Numbers can be found in the supplementary
materials. However  as suggested by theorem A.3 (in supplementary materials)  our method can also
be used as a preprocessing step for other feature selection method to eliminate a large portion of the
features. In this case  we use the ﬁlter methods to ﬁlter out e + 0.1 of the input features  where e is
the desired proportion of the features that one wants to reserve.

(a)

(b)

Figure 3: Result from real world datasets: a) curve showing the ratio between the errors of various
methods applied on original data and on ﬁltered data  where a large portion of the dimension is
ﬁltered out (value larger than one indicates performance improvement); b) the speed up we get by
applying our method as a pre-processing method on various methods across different datasets  the
ﬂat dashed line indicates the location where the speed up is one.
Using our method as preprocessing step achieves 3-5 times speedup as compare to the time spend
by original methods that take multiple passes through the datasets  and keeps or improves the per-
formance in most of the cases (see ﬁgure 3 a and b). The actual running time can be found in
supplementary materials.
3.2.3 Scalability

Figure 4: Scalability Experiment of Our Approach

We validate that our method is able to run on large-scale data set efﬁciently  and the ability to take
advantage of parallelism is the key to its scalability.
Experiment Setup Given the TAC-KBP data set  we report the execution time by varying the
degree of parallelism  number of features  and number of examples. We ﬁrst produce a series of
data sets by sub-sampling the original data set with different number examples ({104  105  106})
and number of features ({104  105  106}). We also try different degree of parallelism by running
our approach using a single thread  4-threads on a 4-core CPU  32 threads on a single 8-CPU (4-
core/CPU) machine  and multiple machines available in the national Open Science Grid (OSG).
For each combination of number of features  number of examples  and degree of parallelism  we
estimate the throughput as the number of tests that we can run in 1 second  and estimate the total
running time accordingly. We also ran our largest data set (106 rows and 106 columns) on OSG and
report the actual run time.
Degree of Parallelism Figure 4(a) reports the (estimated) run time on the largest data set (106
rows and 106 columns) with different degree of parallelism. We ﬁrst observe that running our

7

360	  3600	  36000	  360000	  3600000	  1	  10	  100	  1000	  360	  3600	  36000	  360000	  3600000	  10000	  100000	  1000000	  360	  3600	  36000	  360000	  3600000	  10000	  100000	  1000000	  Time (seconds) (a) # Cores (b) # Features (c) # Examples OSG runtime Single Thread Single 4-Core CPU Single 8-CPU Machine approach requires non-trivial amount of computational resources–if we only use a single thread  we
need about 400 hours to ﬁnish our approach. However  the running time of our approach decreases
linearly with the number of cores that we used. If we run our approach on a single machine with 32
cores  it ﬁnishes in just 11 hours. This linear speed-up behavior allows our approach to scale to very
large data set–when we run our approach on the national Open Science Grid  we observed that our
approach is able to ﬁnish in 2.2 hours (0.7 hours for actual execution  and 1.5 hours for scheduling
overhead).
The Impact of Number of Features and Number of Examples Figure 4(b c) report the run time
with different number of features and number of examples  respectively. In Figure 4(b)  we ﬁx the
number of examples to be 105  and vary the number of features  and in Figure 4(c)  we ﬁx the number
of features to be 106 and vary the number of examples. We see that as the number of features or the
number of examples increase  our approach uses more time; however  the running time never grows
super-linearly. This behavior implies the potential of our approach to scale to even larger data sets.
3.2.4 Stability and robustness
Our method exhibits several robustness properties. In particular  the proof of Theorem 2.2 suggests
that as the number of tests are increased the performance also improves. Therefore  in this section
we empirically evaluate this observation. We picked four datasets: KRVSKP  Landset  Splice and
Waveform from the UCI datasets and both NIPS datasets.

(a)

(b)

(c)

(d)

Figure 5: Change of performance with respect of number of tests on several UCI datasets with (a)
ﬁlter and (b) wrapper methods; and (c) GISETTE and (d) MADELON datasets.

The trend is pretty clear as can be observed from ﬁgure 5. The performance of both wrapper and
ﬁlter methods improves as we increase the number of tests  which can be attributed to the increase of
robustness against inferior estimates for the test scores as the number of tests increases. In addition 
apart from MADELON dataset  the performance converges fast  normally around k = 10 ∼ 15.
Additional stability experiments can be found in the supplementary materials  where we evaluate
ours and other methods in terms of consistency index.
Acknowledgements
CR acknowledges the support of DARPA XDATA Program under No. FA8750-12-2-0335 and
DEFT Program under No. FA8750-13-2-0039  DARPAs MEMEX program  the NSF CAREER
Award under No. IIS-1353606 and EarthCube Award under No. ACI-1343760  the Sloan Research
Fellowship  the ONR under awards No. N000141210041 and No. N000141310129  the Moore
Foundation  American Family Insurance  Google  and Toshiba. HN is supported by NSF grants
CNF-1409551  CCF-1319402  and CNF-1409551. XN is supported in part by NSF grants CCF-
1115769  ACI 1342076   NSF CAREER award under DMS-1351362  and CNS-1409303. Any
opinions  ﬁndings  and conclusions or recommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of DARPA  AFRL  NSF  ONR  or the U.S. govern-
ment.
References
[1] S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from another.

J. Royal Stat. Soc. Series B  28:131–142  1966.

[2] Edoardo Amaldi and Viggo Kann. On the approximability of minimizing nonzero variables or unsatisﬁed

relations in linear systems  1997.

[3] Ron Bekkerman  Ran El-Yaniv  Naftali Tishby  and Yoad Winter. Distributional word clusters vs. words

for text categorization. J. Mach. Learn. Res.  3:1183–1208  March 2003.

8

[4] Gavin Brown  Adam Pocock  Ming-Jie Zhao  and Mikel Luj´an. Conditional likelihood maximisation: A

unifying framework for information theoretic feature selection. JMLR  13:27–66  2012.

[5] I. Csisz´ar. Information-type measures of difference of probability distributions and indirect observation.

Studia Sci. Math. Hungar  2:299–318  1967.

[6] C. H. Q. Ding and H. Peng. Minimum redundancy feature selection from microarray gene expression

data. J. Bioinformatics and Computational Biology  pages 185–206  2005.

[7] Ding-Zhu Du and Frank K. Hwang. Combinatorial group testing and its applications  volume 12 of Series

on Applied Mathematics. World Scientiﬁc Publishing Co. Inc.  River Edge  NJ  second edition  2000.

[8] Devdatt P. Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of randomized

algorithms. Cambridge University Press  Cambridge  2009.

[9] Francois Fleuret and Isabelle Guyon. Fast binary feature selection with conditional mutual information.

Journal of Machine Learning Research  5:1531–1555  2004.

[10] Isabelle Guyon and Andr´e Elisseeff. An introduction to variable and feature selection. J. Mach. Learn.

Res.  3:1157–1182  March 2003.

[11] A. Jakulin and I. Bratko. Machine learning based on attribute interactions: Ph.D. dissertation. 2005.

[12] Ron Kohavi and George H. John. Wrappers for feature subset selection. Artif. Intell.  97(1-2):273–324 

December 1997.

[13] Ludmila I. Kuncheva. A stability index for feature selection. In Artiﬁcial Intelligence and Applications 

pages 421–427  2007.

[14] David D. Lewis. Feature selection and feature extraction for text categorization. In In Proceedings of

Speech and Natural Language Workshop  pages 212–217. Morgan Kaufmann  1992.

[15] Dahua Lin and Xiaoou Tang. Conditional infomax learning: An integrated framework for feature extrac-

tion and fusion. In ECCV (1)  pages 68–82  2006.

[16] P. E. Meyer and G. Bontempi. On the use of variable complementarity for feature selection in cancer

classiﬁcation. In Proceedings of EvoWorkshop  pages 91–102. Springer-Verlag  2006.

[17] Mike Mintz  Steven Bills  Rion Snow  and Daniel Jurafsky. Distant supervision for relation extraction

without labeled data. In ACL/IJCNLP  pages 1003–1011  2009.

[18] Hung Q. Ngo  Ely Porat  and Atri Rudra. Efﬁciently decodable compressed sensing by list-recoverable

codes and recursion. In Proceedings of STACS  volume 14  pages 230–241  2012.

[19] X. Nguyen  M. J. Wainwright  and M. I. Jordan. On surrogate losses and f-divergences. Annals of

Statistics  37(2):876–904  2009.

[20] X. Nguyen  M. J. Wainwright  and M. I. Jordan. Estimating divergence functionals and the likelihood

ratio by by convex risk minimization. IEEE Trans. on Information Theory  56(11):5847–5861  2010.

[21] H. Peng  F. Long  and C. Ding. Feature selection based on mutual information: criteria of max-

dependency  max-relevance  and min-redundancy. IEEE Transactions on PAMI  27:1226–1238  2005.

[22] Herv´e Stoppiglia  G´erard Dreyfus  R´emi Dubois  and Yacine Oussar. Ranking a random feature for

variable and feature selection. J. Mach. Learn. Res.  3:1399–1414  March 2003.

[23] Y. Sun  S. Todorovic  and S. Goodison. Local-learning-based feature selection for high-dimensional data
analysis. Pattern Analysis and Machine Intelligence  IEEE Transactions on  32(9):1610–1626  Sept 2010.

[24] Mingkui Tan  Li Wang  and Ivor W. Tsang. Learning sparse svm for feature selection on very high

dimensional datasets. In ICML  pages 1047–1054  2010.

[25] Howard Hua Yang and John E. Moody. Data visualization and feature selection: New algorithms for

nongaussian data. In NIPS  pages 687–702  1999.

[26] Lei Yu and Huan Liu. Efﬁcient feature selection via analysis of relevance and redundancy. Journal of

Machine Learning Research  5:1205–1224  2004.

[27] Ce Zhang  Feng Niu  Christopher R´e  and Jude W. Shavlik. Big data versus the crowd: Looking for

relationships in all the right places. In ACL (1)  pages 825–834  2012.

9

,Yingbo Zhou
Ce Zhang
Hung Ngo
XuanLong Nguyen
Christopher Ré
Venu Govindaraju
Łukasz Kaiser
Samy Bengio