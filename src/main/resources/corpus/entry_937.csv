2018,Chaining Mutual Information and Tightening Generalization Bounds,Bounding the generalization error of learning algorithms has a long history  which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses  (ii) exploiting the dependence between the algorithm’s input and output. Progress on the first point was made with the chaining method  originating from the work of Kolmogorov  and used in the VC-dimension bound. More recently  progress on the second point was made with the mutual information method by Russo and Zou ’15. Yet  these two methods are currently disjoint. In this paper  we introduce a technique to combine chaining and mutual information methods  to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary  we tighten Dudley’s inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability.,Chaining Mutual Information and Tightening

Generalization Bounds

Amir R. Asadi1∗ Emmanuel Abbe1 2

Sergio Verdú

1Princeton University

2EPFL

Abstract

Bounding the generalization error of learning algorithms has a long history  which
yet falls short in explaining various generalization successes including those of deep
learning. Two important difﬁculties are (i) exploiting the dependencies between the
hypotheses  (ii) exploiting the dependence between the algorithm’s input and output.
Progress on the ﬁrst point was made with the chaining method  originating from
the work of Kolmogorov  and used in the VC-dimension bound. More recently 
progress on the second point was made with the mutual information method by
Russo and Zou ’15. Yet  these two methods are currently disjoint. In this paper  we
introduce a technique to combine the chaining and mutual information methods  to
obtain a generalization bound that is both algorithm-dependent and that exploits the
dependencies between the hypotheses. We provide an example in which our bound
signiﬁcantly outperforms both the chaining and the mutual information bounds. As
a corollary  we tighten Dudley’s inequality when the learning algorithm chooses its
output from a small subset of hypotheses with high probability.

1

Introduction

1.1 Motivation

Understanding the generalization phenomenon in machine learning has been a central question for
many years and revived in recent years with the success and mystery of deep learning: why do neural
nets generalize well  although they operate in a classically overparametrized setting? In particular 
classical generalization bounds do not explain this phenomenon (see e.g. [1]  [2]). Even simpler
instances of successful machine learning problems and algorithms are not explained satisfactorily
with current generalization bounds  e.g. [2]. This paper aims at deriving tighter generalization bounds
for learning algorithms by combining ideas from information theory and from high dimensional
probability.
Generalization bounds have evolved throughout the years  starting from the basic union bound over
the hypothesis set  the reﬁned union bound  Rademacher complexity  chaining and VC-dimension
[3]  [4]; and algorithm-dependent bounds such as PAC-Bayesian bounds [5]  uniform stability [6] 
compression bounds [7]  and recently  the mutual information bound [8].
We highlight two pitfalls among the key limitations of current bounds:

A. Ignoring the dependencies between the hypotheses. Consider the following example (which
we refer to as Example I): an algorithm observes G2 = (G1  G2)  where G1 and G2 are two
independent standard normal random variables; the hypothesis set H = {ht : t ∈ T} consists of
functions ht(G2) (cid:44) (cid:104)t  G2(cid:105)  where T (cid:44) {t ∈ R2 : (cid:107)t(cid:107)2 = 1}. Suppose the algorithm is designed

∗Corresponding author: aasadi@princeton.edu

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

random variables  the expected bias of the algorithm is E(cid:2)maxt∈T ht(G2)(cid:3). Moreover  since H

to choose the hypothesis which achieves maxt∈T ht(G2). Since ht(G2)  t ∈ T are all zero mean

consists of an uncountable number of hypotheses  the union bound (or equivalently the maximal
inequality) over the hypothesis set gives a vacuous bound. However  the fact is that we are not dealing
with inﬁnite number of independent random variables: the random variables ht(G2) and hs(G2) are
actually quite dependent on each other when t and s are close.
To exploit the dependencies  the powerful technique of chaining has been developed in high dimen-
sional probability in order to obtain uniform bounds on random processes  and has proven successful
in a variety of problems including statistical learning. More speciﬁcally  chaining is the method for
proving the tightest generalization bound using VC-dimension [9]  [10]. Originating from the work of
Kolmogorov in 1934 (see [9  p. 149]) and later developed by Dudley  Fernique  Talagrand and many
others [11]  the basic idea of chaining is to ﬁrst describe the dependencies between the hypotheses by
a metric d on the set T   then to discretize T and to approximate the maximal value (maxt∈T ht(G2))
by approximating the maxima over successively reﬁned ﬁnite discretizations  using union bounds
at each step  and by introducing the notion of -nets and covering numbers [12]. For instance  with

this method  one can prove the ﬁnite upper bound E(cid:2)maxt∈T ht(G2)(cid:3) ≤ 19.0353. Even for many

examples of ﬁnite hypothesis sets  chaining is known to give far tighter bounds than the union bound
[9]. Next we state a fundamental result which is based on the chaining method. For a metric space
(T  d)  let N (T  d  ) denote the covering number of (T  d) at scale . For the deﬁnitions of -net and
covering number  see Deﬁnition 8 in Section C of the supplementary material  and for the deﬁnition
of seperable subgaussian processes see Deﬁnitions 1 and 2.
Theorem 1 (Dudley). [13]. Assume that {Xt}t∈T is a separable subgaussian process on the bounded
metric space (T  d). Then

log N (T  d  2−k).

(1)

(cid:20)

E

(cid:21)

≤ 6

Xt

sup
t∈T

2−k(cid:113)

(cid:88)

k∈Z

Note that PAC-Bayesian bounds  compression bounds and bounds based on uniform stability also
do not exploit the dependencies between the hypotheses as they are not based on any metric on the
hypothesis set.

B. Ignoring the dependence between the algorithm input (data) and output. Generalization
bounds based on Rademacher complexity2and VC-dimension only depend on the hypothesis set and
not on the algorithm  effectively rendering them too pessimistic for practical algorithms. Recent
experimental ﬁndings in [1] have shown that in the over-parameterized regime of deep neural nets 
such complexity measures give vacuous bounds for the generalization error. A possible explanation
for that vacuousness is as follows: if H = {ht : t ∈ T} denotes the hypothesis set and for every
t ∈ T   Xt denotes the generalization error of hypothesis ht and W denotes the index of the chosen
hypothesis by the algorithm  then to upper bound the expected generalization error E[XW ]  one uses

(cid:20)

(cid:21)

E[XW ] ≤ E

Xt

 

sup
t∈T

and aims at upper bounding E [supt∈T Xt] with these bounds  hence giving a uniform bound over the
generalization errors of the entire hypothesis set. However  all we need to control is the generalization
error of the speciﬁc hypothesis W selected by the algorithm. That expected generalization error of
W can be much smaller than the right side of (2) (see also [14]). In other words  such bounds are not
taking into account the input-output relation of the algorithm  and uniform bounding seems to be
too stringent for these applications. Consider the following example (which we refer to as Example
II): let X1  X2  ...  Xn be standard normal random variables and assume that the algorithm output is
index W . Therefore the expected bias of the algorithm is E[XW ] and the goal is to upper bound it.
By the maximal inequality (or equivalently the union bound)  we have

(cid:20)

E

sup
1≤i≤n

Xi

(cid:21)

≤(cid:112)2 log n 

2Here we are referring to the Rademacher average of the entire hypothesis set. There exist other notions of
Rademacher averages which are used in algorithm-dependent bounds  such as in local Rademacher complexities
[15].

2

(2)

(3)

|E[XW ]| ≤(cid:112)2σ2I(W ;{Xt}t∈T ).
E[XW ] ≤(cid:112)2I(W ; X1  ...  Xn).

(4)

(6)

(7)

where (3) is asymptotically tight if Xi  i = 1  2  ...  n are independent (see [12  Chapter 2]). But
what if the algorithm is always more likely to choose W among a small subset of {1  2  ...  n}? Then
E[XW ] could be much smaller than the right side of (3)  as the chances of having an outlier value
is smaller. Or  if the choice of W is not dependent on the data  then E[XW ] = E[E[XW|W ]] = 0.
Interestingly  to explain this phenomenon and to obtain tighter upper bounds on E[XW ] an important
information theoretic measure appears: the mutual information. This was originally proposed in the
key paper of Russo and Zou [8] and then generalized in [16]  [17]  and in [18] for inﬁnite number of
hypotheses:
Theorem 2. [8][18] Let {Xt}t∈T be a random process and T an arbitrary set. Assume that Xt is
σ2-subgaussian and E[Xt] = 0 for every t ∈ T   and let W be a random variable taking values on T .
Then

In Example II  instead of using (2) and (3)  one can have the tighter upper bound

(5)
For example  if the algorithm chooses W among {1  2  ... (cid:100)log n(cid:101)} with probability 1 − o(1)  then
(5) implies

E[XW ] ≤(cid:112)2 ((1 − o(1)) log(log n) + o(1) log(n − log n) + 1 (cid:28)(cid:112)2 log n.

However  this method does not give a ﬁnite bound for Example I  since

I(cid:0)argmaxt∈T ht(G2);{ht(G2)}t∈T

(cid:1) = ∞.

Similarly  as discussed in [19]  the mutual information bound for perturbed SGD or any iterative
algorithm which adds degenerate noise in each iteration blows up  and information-theoretic strategies
for analyzing generalization error of such algorithms have not been reported.

1.2 This paper

By combining the ideas of the chaining method and the mutual information method  in this paper
we obtain a chained mutual information bound on the expected generalization error which takes
into account the dependencies between the hypotheses as well as the dependence between output
and input of the algorithm. When applied to the two aforementioned simple examples (Examples
I and II)  our bound yields the better bound between the classical chaining and classical mutual
information bounds. More importantly  we provide examples for which our bound outperforms
both of the previous bounds signiﬁcantly: in Example 1 we provide a family of cases where the
chaining method gives a relatively large constant  the mutual information bound blows up  but our
bound tends towards zero. We also discuss how our new bound gives a possible direction to explain
the phenomenon described in [19] (see Remark 3)  and to exploit regularization properties of some
algorithms (see Section 4).

1.3 Further related literature

In [20]  the mutual information between the input and the output of binary classiﬁcation learning
algorithms is used to obtain high probability generalization bounds.
PAC-Bayesian bounds are another type of algorithm-dependent bounds which are concerned with
ﬁnding high probability generalization bounds for randomized classiﬁers [5]. These bounds deﬁne a
hierarchy over the hypothesis set by using a prior distribution on that set [4]. As discussed in [20] 
there is a connection and similarity between PAC-Bayesian bounds and the mutual information bound 
both using the variational representation of relative entropy in their proofs. In [21] and [22]  the
authors combine the ideas of PAC-Bayesian bounds with generic chaining and create high probability
bounds for randomized classiﬁers. Their use of an auxiliary sample set and the notion of average
distance between partitions makes their bounds conceptually different from our work. However  their
bounds have the advantage to exploit the variance of the hypotheses and to give high probability
results.
In the probability theory literature  Fernique [23] gives upper and lower bounds on the expected bias
of an algorithm (or a selection rule) which chooses its output from a Gaussian process  by using a

3

chaining argument while taking into account the marginal distribution of the algorithm output. We
further utilize the dependence between the algorithm input and output and the stochasticity of the
algorithm  and we give results for more general processes. However  we only obtain upper bounds in
this paper.

1.4 Notation
In the framework of supervised statistical learning  X is the instances domain  Y is the labels domain
and Z = X × Y denotes the examples domain. Furthermore  H = {hw : w ∈ W} is the hypothesis
set where the hypotheses are indexed by an index set W  and there is a nonnegative loss function
(cid:96) : H × Z → R+. A learning algorithm receives the training set S = (Z1  Z2  ...  Zn) of n examples
with i.i.d. random elements drawn from Z with distribution µ. Then it picks an element hW ∈ H as
the output hypothesis according to a random transformation PW|S (thus  we are allowing randomized
algorithms). For any w ∈ W  let

(8)
denote the statistical (or population) risk of hypothesis hw. For a given training set S  the empirical
risk of hypothesis hw is deﬁned as

Lµ(w) (cid:44) E[(cid:96)(hw  Z)]  Z ∼ µ

n(cid:88)

i=1

LS(w) (cid:44) 1
n

(cid:96)(hw  Zi) 

(9)

and the generalization error of hypothesis hw (dependent on the training set) is deﬁned as

gen(w) (cid:44) Lµ(w) − LS(w).

(10)
Averaging with respect to the joint distribution PS W = µ⊗nPW|S  we denote the expected general-
ization error and the expected absolute value of generalization error by
gen(µ  PW|S) (cid:44) E[Lµ(W ) − LS(W )] 
gen+(µ  PW|S) (cid:44) E[|Lµ(W ) − LS(W )|] 

(11)

(12)

and

respectively. Our purpose is to ﬁnd upper bounds on gen(µ  PW|S) and gen+(µ  PW|S).
Let XN (cid:44) {Xi : i ∈ N} denote a random process indexed by the elements of the set N . Let 0 denote
the identically zero function. In this paper  all logarithms are in natural base and all information
theoretic measures are in nats. H(X) denotes the Shannon entropy of a discrete random variable X 
and h(Y ) denotes the differential entropy of an absolutely continuous random variable Y .

2 Main results
Assume that {Xt}t∈T is a random process with index set T . In the chaining method  we impose a
metric d on T which describes the dependencies between the random variables. The widely used
subgaussian processes capture this notion and they arise in many applications:
Deﬁnition 1 (Subgaussian process). The random process {Xt}t∈T on the metric space (T  d) is
called subgaussian if E[Xt] = 0 for all t ∈ T and
2 λ2d2(t s)

(13)
For example  based on the Azuma–Hoeffding inequality  {gen(w)}w∈W is a subgaussian process
with the metric

eλ(Xt−Xs)(cid:105) ≤ e

t  s ∈ T  λ ≥ 0.

E(cid:104)

for all

1

d(gen(w)  gen(v)) (cid:44) (cid:107)(cid:96)(hw ·) − (cid:96)(hv ·)(cid:107)∞

√

n

 

(14)

regardless of the choice of distribution µ on Z.
The following is a technical assumption which holds in almost all cases of interest:
Deﬁnition 2 (Separable process). The random process {Xt}t∈T is called separable if there is a
countable set T0 ⊆ T such that Xt ∈ lim s→t
xs means
s∈T0
that there is a sequence (sn) in T0 such that sn → t and xsn → x.

Xs for all t ∈ T a.s.  where x ∈ lim s→t
s∈T0

4

For example  if t → Xt is continuous a.s.  then Xt is a separable process [9].
Our main results rely on the notion of increasing sequence of -partitions of the metric space (T  d):
Deﬁnition 3 (Increasing sequence of -partitions). We call a partition P = {A1  A2  ...  Am} of the
set T an -partition of the metric space (T  d) if for all i = 1  2  ...  m  Ai can be contained within a
ball of radius . A sequence of partitions {Pk}∞
k=m of a set T is called an increasing sequence if for
all k ≥ m and each A ∈ Pk+1  there exists B ∈ Pk such that A ⊆ B. For any such sequence and
any t ∈ T   let [t]k denote the unique set A ∈ Pk such that t ∈ A.
Assume now that (T  d) is a bounded metric space  and let k1(T ) be an integer such that
2−(k1(T )−1) ≥ diam(T ). We have the following upper bounds on gen(µ  PW|S) and gen+(µ  PW|S)
based on the mutual information between the training set S and the discretized output of the learning
algorithm  where each of these mutual information terms is multiplied by an exponentially decreasing
weight 2−k  in which the exponent measures how ﬁnely the output W of the learning algorithm is
discretized:
Theorem 3. Assume that {gen(w)}w∈W is a separable subgaussian process on the bounded metric
space (W  d). Let {Pk}∞
k=k1(W) be an increasing sequence of partitions of W  where for each
k ≥ k1(W)  Pk is a 2−k-partition of (W  d).
(a)

√
gen(µ  PW|S) ≤ 3

2

∞(cid:88)

k=k1(W)

2−k(cid:112)I([W ]k; S) 
2−k(cid:112)I([W ]k; S) + log 2.

(b) If 0 ∈ {(cid:96)(hw ·) : w ∈ W}  then

√
gen+(µ  PW|S) ≤ 3

2

∞(cid:88)

k=k1(W)

(15)

(16)

(17)

(18)

Remark 1. Based on the general deﬁnition of mutual information with partitions ([24  p. 252])  we
have I(W ; S) = supk I([W ]k; S) therefore I([W ]k; S) → I(W ; S) as k → ∞.
Theorem 3 is stated in the context of statistical learning. The more general counterpart in the context
of random processes is:
Theorem 4. Assume that {Xt}t∈T is a separable subgaussian process on the bounded metric space
(T  d). Let {Pk}∞
k=k1(T ) be an increasing sequence of partitions of T   where for each k ≥ k1(T ) 
Pk is a 2−k-partition of (T  d).
(a)

√
E[XW ] ≤ 3

2

∞(cid:88)

(b) For any arbitrary t0 ∈ T  

E[|XW − Xt0|] ≤ 3

k=k1(T )

∞(cid:88)

√

2

k=k1(T )

2−k(cid:112)I([W ]k; XT ).
2−k(cid:112)I([W ]k; XT ) + log 2.

Note that in Theorem 4 if we let T (cid:44) W and Xw (cid:44) gen(w) for all w ∈ W  then for each k ≥ k1(T ) 
due to the Markov chain

XT = {gen(w)}w∈W ↔ S ↔ W ↔ [W ]k 

(19)
and the data processing inequality  we have I([W ]k; XT ) ≤ I([W ]k; S). Therefore Theorem
3 follows from Theorem 4. The proof of Theorem 4 and the etymology of “chaining mutual
information" is given in Section 3.

5

Remark 2. For random processes other than subgaussian processes  where the tail of increments
are controlled by a function ψ  similar results can be derived from Theorem 12 in Section D of the
supplementary material.

Both Theorem 3 and Theorem 4 capture the dependencies between the hypotheses by utilizing
a metric d  and they are algorithm-dependent as the mutual information between the algorithm’s
discretized output and its input appears in their bounds. Now  to demonstrate the power of Theorem 4
and to compare it with the existing results in the literature  consider the following example:
Example 1. Let T be an arbitrary subset of Rn  and Gn (cid:44) (G1  ...  Gn) ∼ N (0  In) be a standard
normal random vector in Rn. The canonical Gaussian process is deﬁned as {Xt}t∈T   where

Xt (cid:44) (cid:104)t  Gn(cid:105) for all t ∈ T.

(20)
Note that {Xt}t∈T is a subgaussian process on the metric space (T  d)  where d is the Euclidean
distance.
Consider a canonical Gaussian process where n = 2 and T (cid:44) {t ∈ R2 : (cid:107)t(cid:107)2 = 1}. The process
{Xt}t∈T can be reparameterized according to the phase of each point t ∈ T : the random variable
Xt can also be denoted as Xφ  where φ ∈ [0  2π) is the phase of t. In other words  φ is the unique
number in [0  2π) such that t = (sin φ  cos φ). Henceforth  we will assume the indices are in the
phase form.
Let the relation between the input XT of an algorithm and its output W be as

argmaxφ∈[0 2π)Xφ

(21)
where the noise Z is independent from XT   and has an atom with probability mass  on 0  and 1 − 
probability is uniformly distributed on (−π  π). Note that since Z has a singular (degenerate) part 
h(Z) = −∞.
Due to symmetry  W has uniform distribution over [0  2π). But we have

W (cid:44)(cid:16)

(cid:17) ⊕ Z (mod 2π) 

argmaxφ∈[0 2π)Xφ ⊕ Z

(cid:17)

(cid:12)(cid:12)(cid:12)XT

I(W ; XT ) = h(W ) − h(W|XT )

(cid:16)

= log 2π − h
= log 2π − h(Z|XT )
= log 2π − h(Z)
= ∞.

(22)

(23)

(24)
(25)
(26)

Hence the upper bound on E[XW ] due to the mutual information method (Theorem 2) blows up:

(27)
Note that 2−(−2) ≥ diam(T ) = 2. Therefore let k1(T ) ← −1 and for all integers k ≥ −1  deﬁne

E[XW ] ≤(cid:112)2I(W ; XT ) = ∞.
(cid:20) 2π
2k+2   2 × 2π

(cid:19)

2k+2

  ... 

(cid:19)

 

(cid:20)(cid:0)2k+2 − 1(cid:1) 2π

(cid:26)(cid:20)

Pk (cid:44)

0 

2π
2k+2

2k+2   2π

.

(28)

(cid:19)(cid:27)

k=−1 is an increasing sequence of partitions of T . Furthermore  for each k ≥ −1 
2k+2 < 21−k. Thus each Pk is a 2−k-partition of

It is clear that {Pk}∞
the length of the arc of each set in Pk is δk (cid:44) 2π
(T  d) and |Pk| = 2k+2 (see Figure 1).
Now by using the classical chaining method (Theorem 1) to upper bound E[XW ] by upper bounding
E[supφ∈[0 2π) Xφ] and ignoring the algorithm  we get

(cid:34)

(cid:35)
2−k(cid:112)

Xφ

E[XW ] ≤ E
√
≤ 3

2

φ∈[0 2π)

sup

∞(cid:88)

k=−1
= 19.0352...3

6

log 2k+2

(29)

(30)

(31)

Figure 1: Depiction of T P−1 P0 and P1 in the R2 plane. (The three partitions are magniﬁed for
clarity.)

On the other hand  for every k ≥ −1 we have

I([W ]k; XT ) = H([W ]k) − H ([W ]k|XT )

= log 2k+2 − H
= log 2k+2 − H

argmaxφ∈[0 2π)Xφ
1 − 
2k+2  

1 − 
2k+2   . . .  

 +

1 − 
2k+2

(cid:17) ⊕ Z

(cid:17)

(cid:12)(cid:12)(cid:12)XT
(cid:105)
(cid:19)

k

.

(cid:16)(cid:104)(cid:16)
(cid:18)

Therefore  based on the chained mutual information method (Theorem 4)  we have

√
E[XW ] ≤ 3

2

√
= 3

2

2−k(cid:112)I([W ]k; XT )
(cid:115)

2−k

log 2k+2 − H

∞(cid:88)
∞(cid:88)

k=−1

k=−1

(cid:18)

 +

(cid:19)

1 − 
2k+2  

1 − 
2k+2   . . .  

1 − 
2k+2

(32)

(33)

(34)

(35)

(36)

Numerical values of the right side of (36) for different values of  are given in Table 1 (CMI bound).
Note that indeed I([W ]k; XT ) → I(W ; XT ) = ∞ as k → ∞. However  the slow rate of that
convergence and the existence of the 2−k term makes the sum not only ﬁnite  but very small. In fact 
as  → 0  the right side of (36) tends to 0 as well.

It is interesting to notice that for this toy example  the exact values of E(cid:104)
can be computed. As supφ∈[0 2π) Xφ has a Rayleigh distribution  we have E(cid:104)
(cid:112) π
out  and we have E[XW ] = (cid:112) π

and E[XW ]
=
2 = 1.253... . Since the noise Z is independent from XT   the effect of its continuous part cancels

supφ∈[0 2π) Xφ

supφ∈[0 2π) Xφ

(cid:105)

(cid:105)

2 . See Table 1.

3The exact value of the bound in Theorem 1 is slightly smaller  since with our partitions we are using a rough
approximate for the covering numbers. For example  at scale 2−(−1)  the covering number is 1  while we have
used partition P−1 with |P−1| = 2 sets.

7

2(cid:112)I(W ; XT )



Chaining bound

CMI bound

E[XW ]

Table 1: E[XW ] and its upper bounds

1
20

∞

1
30

∞

1
40

∞

1
50

∞

1

100

∞

1

200

∞

1

400

∞

19.0352
1.1013
0.0626

19.0352
0.7507
0.0417

19.0352
0.5709
0.0313

19.0352
0.4612
0.0250

19.0352
0.2364
0.0125

19.0352
0.1204
0.0062

19.0352
0.0610
0.0031

Remark 3. Notice that in Example 1 there exists an independent additive noise term Z which has a
degenerate part  causing the mutual information bound to blow up. Similarly  as discussed in [19] 
the mutual information bound for perturbed SGD or any iterative algorithm which adds degenerate
noise in each iteration blows up. Example 1 illustrates that combining the mutual information method
with the chaining method as in our bound could give tight generalization bounds for such algorithms
as well.
Remark 4. It is clear that having degenerate noise is not necessary to observe that the chained
mutual information bound is tighter than the mutual information bound; this is just an extreme case
for which the mutual information bound blows up. For instance  in Example 1  one can replace Z
with a sequence of continuous random variables which converge to Z in distribution.

3 Proof outline

Here we provide an outline of the proof of Theorem 4. As noted in Section 2  Theorem 3 follows
from Theorem 4.
For an arbitrary k ≥ k1(T )  consider Pk = {A1  A2  ...  Am}. Since Pk is a 2−k-partition of
(T  d)  by deﬁnition there exists a set (or a multiset) Nk (cid:44) {a1  a2  ...  am} ⊆ T and a mapping
πNk : T → Nk such that πNk (t) = ai if t ∈ Ai  and further d (t  πNk (t)) ≤ 2−k  for all
i = 1  2  ...  m. Therefore Nk is a 2−k-net and πNk is its associated mapping. It is also clear that for
an arbitrary t0 ∈ T   Nk0
(cid:44) {t0} is a 2−(k1(T )−1)-net. Note that for any integer n ≥ k1(T ) we can
write

Since by the deﬁnition of subgaussian processes the process is centered  we have E[Xt0 ] = 0. Thus

n(cid:88)

(cid:16)

k=k1(T )

XW = Xt0 +

XπNk (W ) − XπNk−1 (W )

E[XW ] − E(cid:2)XW − XπNn (W )

(cid:3) =

n(cid:88)

E(cid:104)

k=k1(T )

(cid:17)

(cid:1) .
+(cid:0)XW − XπNn (W )
(cid:105)

XπNk (W ) − XπNk−1 (W )

.

(37)

(38)

For every k ≥ k1(T )  {XπNk (t) − XπNk−1 (t)}t∈T is a subgaussian process with at most |Nk||Nk−1|
distinct terms  hence a ﬁnite process. Based on the triangle inequality 

d(cid:0)πNk (t)  πNk−1 (t)(cid:1) ≤ d (t  πNk (t)) + d(cid:0)t  πNk−1(t)(cid:1)

(cid:110)
XπNk (t) − XπNk−1 (t)

Note that knowing the value of (cid:0)πNk (W )  πNk−1(W )(cid:1) is enough to determine which one
(cid:0)πNk (W )  πNk−1(W )(cid:1) is playing the role of the random index  and since XπNk (t) − XπNk−1 (t)
is d2(cid:0)πNk (t)  πNk−1(t)(cid:1)-subgaussian  based on Theorem 2  an application of data processing in-
n(cid:88)

is chosen according to W . Therefore

equality and by summation  we obtain

of the random variables

≤ 3 × 2−k.

n(cid:88)

2 × 2−k(cid:113)

√
3

I(πNk (W )  πNk−1(W ); XT ). (40)

XπNk (W ) − XπNk−1 (W )

(cid:111)

t∈T

(cid:105) ≤

E(cid:104)

(39)

k=k1(T )

k=k1(T )

8

Notice the chain of mutual information terms in the right side of (40). Since {Pk}∞
k=k1(T ) is an
increasing sequence of partitions  for any t ∈ T   knowing Nk(t) will uniquely determine Nk−1(t).
Therefore

I(cid:0)πNk (W )  πNk−1(W ); XT

(cid:1) = I (πNk (W ); XT )

= I ([W ]k; XT ) .

(41)
(42)

The rest of the proof follows from the deﬁnition of separable processes (Deﬁnition 2). For more
details  see proof of Theorem 11 in Section D of the supplementary material.

4 Additional result: small subset property

We adjusted the conservative chaining method in random processes theory to learning problems by
taking into account information about the algorithm  with the chained mutual information method. In
this section  we state a result in which such information could make the bounds much tighter.
It is known that for linear models  the stochastic gradient descent (SGD) algorithm always converges
to a solution with small norm [1]. Inspired by this observation  we tighten Dudley’s inequality
(Theorem 1)  given the following regularization property: the output W of an algorithm  with high
probability  chooses a hypothesis from a subset of the hypothesis set with small covering numbers:
Theorem 5 (Small subset property). Assume that {Xt}t∈T is a separable subgaussian process on
the bounded metric space (T  d). Let {T1  T2} be a partition of T and assume that W is a random
variable taking values on T with P[W ∈ T1] = α. Then we have

∞(cid:88)

2−k(cid:113)

E[XW ] ≤6

α log N (T1  d  2−k) + (1 − α) log N (T2  d  2−k) + H(α).

(43)

k=k1(T )

Proof of Theorem 5 appears in Section D of the supplementary material. Note that the right side of
(43) becomes much smaller than Dudley’s bound when α is close to 1 and the covering numbers of
T1 (the small subset) are much smaller than the covering numbers of T2.
Remark 5. One can upper bound the right side of (43) by replacing N (T2  d  2−k) with N (T  d  2−k).
This is particularly useful when bounding the latter is easier than the former.

5 Conclusion

We combined ideas from information theory and from high dimensional probability to obtain a
generalization bound that takes into account both the dependencies between the hypotheses and the
dependence between the input and the output of a learning algorithm. We showed on an example
that our chained mutual information bound signiﬁcantly outperforms previous bounds and gets close
to the true generalization error. Under a natural regularization property of the learning algorithm 
we provided a corollary of our bound which tightens Dudley’s inequality; i.e. when the learning
algorithm chooses its output from a small subset of hypotheses with high probability.

6 Acknowledgments

We gratefully acknowledge discussions with Ramon van Handel on the topic of chaining. This work
was partly supported by the NSF CAREER Award CCF-1552131.

References
[1] C. Zhang  S. Bengio  M. Hardt  B. Recht and O. Vinyals. Understanding deep learning requires
rethinking generalization. In International Conference on Learning Representations (ICLR) 
Apr. 2017.

[2] M. Belkin  S. Ma  and S. Mandal. To understand deep learning we need to understand kernel

learning. arXiv preprint arXiv:1802.01396  2018.

9

[3] O. Bousquet  S. Boucheron  and G. Lugosi. Introduction to statistical learning theory. In

Advanced Lectures on Machine Learning  pages 169–207. Springer  2004.

[4] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  2014.

[5] D. A. McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  1999.

[6] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning

Research  2(Mar):499–526  2002.

[7] N. Littlestone and M. Warmuth. Relating data compression and learnability. Technical report 

University of California  Santa Cruz  1986.

[8] D. Russo and J. Zou. How much does your data exploration overﬁt? controlling bias via

information usage. arXiv preprint arXiv:1511.05219  2015.

[9] R. van Handel. Probability in high dimension.

[Online]. Available: https: // www.

princeton. edu/ ~rvan/ APC550. pdf   Dec. 21 2016.

[10] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press  2018.

[11] M. Talagrand. Upper and Lower Bounds for Stochastic Processes: Modern Methods and

Classical Problems  volume 60. Springer Science & Business Media  2014.

[12] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities: A Nonasymptotic Theory

of Independence. Oxford University Press  2013.

[13] R. M. Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian

processes. Journal of Functional Analysis  1(3):290–330  1967.

[14] K. Kawaguchi  L. P. Kaelbling and Y. Bengio. Generalization in deep learning. arXiv preprint

arXiv:1710.05468  2017.

[15] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. The Annals of

Statistics  33(4):1497–1537  2005.

[16] J. Jiao  Y. Han and T. Weissman. Dependence measures bounding the exploration bias for
general measurements. In Proc. of IEEE Symposium on Information Theory (ISIT)  pages
1475–1479  Aachen  Germany  June 2017.

[17] J. Jiao  Y. Han and T. Weissman. Generalizations of maximal inequalities to arbitrary selection

rules. arXiv preprint arXiv:1708.09041  2017.

[18] A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning
algorithms. In Advances in Neural Information Processing Systems (NIPS)  pages 2524–2533 
Dec. 2017.

[19] A. Pensia  V. Jog and P. Loh. Generalization error bounds for noisy  iterative algorithms. arXiv

preprint arXiv:1801.04295  12 Jan 2018.

[20] R. Bassily  S. Moran  I. Nachum  J. Shafer and A. Yehudayoff. Learners that leak little

information. arXiv preprint arXiv:1710.05233  2017.

[21] J. Audibert and O. Bousquet. PAC-Bayesian generic chaining. In Advances in Neural Informa-

tion Processing Systems (NIPS)  pages 1125–1132  2004.

[22] J. Audibert and O. Bousquet. Combining PAC-Bayesian and generic chaining bounds. Journal

of Machine Learning Research  8(Apr):863–889  2007.

[23] X. Fernique. Evaluations de processus Gaussiens composes. In Probability in Banach Spaces 

pages 67–83. Springer  1976.

[24] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons  2012.

10

,Amir Asadi
Emmanuel Abbe
Sergio Verdu