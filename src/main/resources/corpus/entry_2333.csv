2008,Goal-directed decision making in prefrontal cortex: a computational framework,Research in animal learning and behavioral neuroscience has distinguished between two forms of action control: a habit-based form  which relies on stored action values  and a goal-directed form  which forecasts and compares action outcomes based on a model of the environment. While habit-based control has been the subject of extensive computational research  the computational principles underlying goal-directed control in animals have so far received less attention. In the present paper  we advance a computational framework for goal-directed control in animals and humans. We take three empirically motivated points as founding premises: (1) Neurons in dorsolateral prefrontal cortex represent action policies  (2) Neurons in orbitofrontal cortex represent rewards  and (3) Neural computation  across domains  can be appropriately understood as performing structured probabilistic inference. On a purely computational level  the resulting account relates closely to previous work using Bayesian inference to solve Markov decision problems  but extends this work by introducing a new algorithm  which provably converges on optimal plans. On a cognitive and neuroscientific level  the theory provides a unifying framework for several different forms of goal-directed action selection  placing emphasis on a novel form  within which orbitofrontal reward representations directly drive policy selection.,Goal-directed  decision making  in  prefrontal

cortex: A  computational framework

                         Matthew Botvinick   
            Princeton Neuroscience Institute and                        Computer Science Department
      Department of Psychology  Princeton University                  Princeton University
                        Princeton  NJ 08540
                    matthewb@princeton.edu

                         Princeton  NJ 08540
                          an@princeton.edu

                                  James An

Abstract

Research  in  animal  learning  and  behavioral  neuroscience  has  distinguished
between  two  forms  of  action  control:  a  habit-based  form   which  relies  on
stored  action  values   and  a  goal-directed  form   which  forecasts  and
compares  action  outcomes  based  on  a  model  of  the  environment.    While
habit-based  control  has  been  the  subject  of  extensive  computational
research   the  computational  principles  underlying  goal-directed  control  in
animals  have  so  far  received  less  attention.    In  the  present  paper   we
advance  a  computational  framework  for  goal-directed  control  in  animals
and  humans.    We  take  three  empirically  motivated  points  as  founding
premises:  (1)  Neurons  in  dorsolateral  prefrontal  cortex  represent  action
policies   (2)  Neurons  in  orbitofrontal  cortex  represent  rewards   and  (3)
Neural  computation   across  domains   can  be  appropriately  understood  as
performing  structured  probabilistic  inference.    On  a  purely  computational
level   the resulting  account  relates closely  to  previous  work using  Bayesian
inference  to  solve  Markov  decision  problems   but  extends  this  work  by
introducing  a  new  algorithm   which  provably  converges  on  optimal  plans.
On  a  cognitive  and  neuroscientific  level   the  theory  provides  a  unifying
framework  for  several  different  forms  of  goal-directed  action  selection 
placing  emphasis  on  a  novel  form   within  which  orbitofrontal  reward
representations directly drive policy selection.

1

G o a l - d i re c t e d   a c t i o n   c o n t ro l

In  the  study  of  human  and  animal  behavior   it  is  a  long-standing  idea  that  reward-based
decision  making  may  rely  on  two  qualitatively  different  mechanisms.    In  habit-based
decision  making   stimuli  elicit  reflex-like  responses   shaped  by  past  reinforcement  [1].    In
goal-directed or purposive  decision making  on the other hand  actions are selected based on
a prospective consideration of possible outcomes and future lines of action [2]. Over the past
twenty  years  or  so   the  attention  of  cognitive  neuroscientists  and  computationally  minded
psychologists  has  tended  to  focus  on  habit-based  control   due  in  large  part  to  interest  in
potential  links  between  dopaminergic  function  and  temporal-difference  algorithms  for
reinforcement  learning.    However   a  resurgence  of  interest  in  purposive  action  selection  is
now  being  driven  by  innovations  in  animal  behavior  research   which  have  yielded  powerful
new  behavioral  assays  [3]   and  revealed  specific  effects  of  focal  neural  damage  on  goal-
directed behavior [4].

In  discussing  some  of  the  relevant  data   Daw   Niv  and  Dayan  [5]  recently  pointed  out  the
close  relationship  between  purposive  decision  making   as  understood  in  the  behavioral
sciences   and  model-based  methods  for  the  solution  of  Markov  decision  problems  (MDPs) 
where  action  policies  are  derived  from  a  joint  analysis  of  a  transition  function  (a  mapping

from  states  and  actions  to  outcomes)  and  a  reward  function  (a  mapping  from  states  to
rewards).    Beyond  this  important  insight   little  work  has  yet  been  done  to  characterize  the
computations  underlying  goal-directed  action  selection  (though  see  [6   7]).    As  discussed
below  a great deal of evidence indicates that purposive action selection depends critically on
a particular region of the brain  the prefrontal cortex.  However  it is currently a critical  and
quite open  question what the relevant computations within this part of the brain might be.

Of  course   the basic  computational  problem of  formulating  an optimal  policy  given a  model
of  an  MDP  has  been  extensively  studied   and  there  is  no  shortage  of  algorithms  one  might
consider  as  potentially  relevant  to  prefrontal  function  (e.g.   value  iteration   policy  iteration 
backward  induction   linear  programming   and  others).    However   from  a  cognitive  and
neuroscientific perspective  there  is one approach  to solving  MDPs that it  seems particularly
appealing to  consider.  In particular   several researchers have  suggested  methods for solving
MDPs  through  probabilistic  inference  [8-12].    The  interest  of  this  idea   in  the  present
context   derives  from  a  recent  movement  toward  framing  human  and  animal  information
processing   as  well  as  the  underlying  neural  computations  
in  terms  of  structured
probabilistic  inference  [13   14].    Given  this  perspective   it  is  inviting  to  consider  whether
goal-directed  action  selection   and  the  neural  mechanisms  that  underlie  it   might  be
understood in those same terms.

One challenge in investigating this  possibility is that previous research furnishes  no ‘off-the-
shelf’  algorithm  for  solving  MDPs  through  probabilistic  inference  that  both  provably  yields
optimal  policies  and  aligns  with  what  is  known  about  action  selection  in  the  brain.    We
endeavor  here  to start  filling  in  that  gap.   In  the  following  section   we introduce  an  account
of  how  goal-directed  action  selection  can  be  performed  based  on  probabilisitic  inference 
within  a  network  whose  components  map  grossly  onto  specific  brain  structures.    As  part  of
this  account   we  introduce  a  new  algorithm  for  solving  MDPs  through  Bayesian  inference 
along  with  a  convergence  proof.    We  then  present  results  from  a  set  of  simulations
illustrating  how  the  framework  would  account  for  a  variety  of  behavioral  phenomena    that
are thought to involve purposive  action selection.

2

C o m p u t a t i o n a l   m o d e l

As  noted  earlier   the  prefrontal  cortex  (PFC)  is  believed  to  play  a  pivotal  role  in  purposive
behavior.  This  is  indicated  by  a  broad  association  between  prefrontal  lesions  and
impairments  in  goal-directed  action  in  both  humans  (see  [15])  and  animals  [4].    Single-unit
recording  and  other  data  suggest  that  different  sectors  of  PFC  make  distinct  contributions.
In  particular   neurons  in  dorsolateral  prefrontal  cortex  (DLPFC)  appear  to  encode  task-
specific  mappings  from  stimuli  to  responses  (e.g.   [16]):  “task  representations ”  in  the
language  of  psychology   or  “policies”  in  the  language  of  dynamic  programming.    Although
there  is  some  understanding  of  how  policy  representations  in  DLPFC  may  guide  action
execution  [15]   little  is  yet  known  about  how  these  representations  are  themselves  selected.
Our  most  basic  proposal  is  that  DLPFC  policy  representations  are  selected  in  a  prospective 
model-based  fashion   leveraging  information  about  action-outcome  contingencies  (i.e.   the
transition function) and about the incentive value associated with specific outcomes or states
(the  reward  function).    There  is  extensive  evidence  to  suggest  that  state-reward  associations
are  represented  in  another  area  of  the  PFC   the  orbitofrontal  cortex  (OFC)  [17   18].    As  for
the  transition function   although it  is  clear that  the  brain contains  detailed representations  of
action-outcome  associations  [19]   their  anatomical  localization  is  not  yet  entirely  clear.
However   some  evidence  suggests  that  the  enviromental  effects  of  simple  actions  may  be
represented  in  inferior fronto-parietal  cortex  [20]   and there  is  also  evidence suggesting  that
medial temporal structures may be important in forecasting action outcomes [21].

As  detailed  in  the  next  section   our  model  assumes  that  policy  representations  in  DLPFC 
reward  representations  in  OFC   and  representations  of  states  and  actions  in  other  brain
regions   are  coordinated  within  a  network  structure  that  represents  their  causal  or  statistical
interdependencies  and that policy selection occurs  within this network  through a process of
probabilistic inference.

2 . 1  

A r c h i t e c t u r e

The implementation takes the form of a directed graphical model [22]  with the layout shown
in  Figure  1.    Each  node  represents  a  discrete  random  variable.    State  variables  (s) 

representing  the  set  of  m  possible  world  states   serve  the  role  played  by  parietal  and  medial
temporal  cortices  in  representing  action  outcomes.  Action  variables  (a)  representing  the  set
of  available  actions   play  the  role
of  high-level  cortical  motor  areas
involved  in  the  programming  of
action  sequences.  Policy  variables
((cid:1))   each  repre-senting  the  set  of
all 
policies
associated  with  a  specific  state 
capture  the  representational  role
of  DLPFC. 
  Local  and  global
utility  variables   described  further
below   capture  the  role  of  OFC  in
representing  incentive  value.    A
separate set of nodes is included for each discrete time-step up to the planning horizon.

   Fig 1. Left: Single-step decision. Right: Sequential decision.
   Each time-slice includes a set of m policy nodes.

deterministic 

The  conditional  probabilities  associated  with  each  variable  are  represented  in  tabular  form.
State probabilities are based on the state and action variables in the preceding time-step  and
thus  encode  the  transition  function.    Action  probabilities depend  on  the  current  state  and  its
associated  policy  variable.    Utilities  depend  only  on  the  current  state.    Rather  than
representing reward magnitude as a continuous variable  we adopt an approach introduced by
[23]   representing  reward  through  the  posterior  probability  of  a  binary  variable  (u).    States
associated  with  large  positive  reward  raise  p(u)  (i.e  p(u=1|s))  near  to  one;  states  associated
with large negative rewards reduce  p(u) to near zero.   In the simulations reported below  we
used a simple linear transformation to map from scalar reward values to p(u):

p u si
(

) =

)

1
2

(cid:2)
(cid:6)
(cid:3)

R si(
rmax

(cid:4)
(cid:7) 
+1
(cid:5)

      

rmax (cid:1) max j R s j(

)

     (1)

In situations involving sequential actions  expected returns from different time-steps must be
integrated  into  a  global  representation  of  expected  value.    In  order  to  accomplish  this   we
employ a technique proposed by [8]  introducing a “global” utility variable (uG).  Like u  this
is a binary random variable  but associated with a posterior probability determined as:1

p uG(

) =

1
N

(cid:1)

p(ui )

     (2)

i

                                                    
where  N is  the number  of u  nodes.   The network  as whole  embodies a  generative model  for
instrumental  action.    The  basic  idea  is  to  use  this  model  as  a  substrate  for  probabilistic
inference   in  order  to  arrive  at  optimal  policies.    There  are  three  general  methods  for
accomplishing  this   which  correspond  three  forms  of  query.    First   a  desired  outcome  state
can  be  identified   by  treating  one  of  the  state  variables  (as  well  as  the  initial  state  variable)
as  observed  (see  [9]  for  an  application  of  this  approach).    Second   the  expected  return  for
specific plans can be evaluated and compared by conditioning on specific sets of values over
the  policy  nodes  (see  [5   21]).    However   our  focus  here  is  on  a  less  obvious  possibility 
which is to condition directly on the utility variable uG   as explained next.

2 . 2

P o l i c y   s e l e c t i o n   b y   p r o b a b i l i s t i c   i n f e r e n c e :   a n   i t e r a t i v e   a l g o r i t h m

Cooper  [23]  introduced  the  idea  of  inferring  optimal  decisions  in  influence  diagrams  by
treating  utility  nodes into  binary  random  variables and  then  conditioning  on these  variables.
Although this technique has been adopted in some more recent work [9  12]  we are aware of
no  application that  guarantees  optimal decisions   in the  expected-reward  sense  in  multi-step
tasks.    We  introduce  here  a  simple  algorithm  that  does  furnish  such  a  guarantee.    The
procedure  is  as  follows:  (1)  Initialize  the  policy  nodes  with  any  set  of  non-deterministic
priors. (2) Treating the initial state and uG as observed variables (uG = 1) 2 use standard belief
                                                  
1 Note that temporal discounting can be incorporated into the framework through minimal
modifications to Equation 2.
2 In the single-action situation  where there is only one  u node  it is this variable that is treated as
observed (u = 1).

propagation  (or  a  comparable  algorithm)  to  infer  the  posterior  distributions  over  all  policy
nodes.    (3)  Set  the  prior  distributions  over  the  policy  nodes  to  the  values  (posteriors)
obtained  in  step 2.   (4)  Go to  step  2.   The  next two  sections present  proofs of  monotonicity
and convergence for this algorithm.

2 . 2 . 1 M o n o t o n i c i t y

We show first that  at each policy node  the probability associated with the optimal policy will rise
on every iteration. Define (cid:1)* as follows:

                                        p uG (cid:2)
+ is the current set of probability distributions at all policy nodes on subsequent time-steps.
where (cid:1)
(Note that we assume here  for simplicity  that there is a unique optimal policy.) The objective is
to establish that:

)  (cid:4) (cid:3)(cid:2) (cid:1) (cid:2)

) > p uG

         (3)

(cid:3)(cid:2)  (cid:2)+

(

(

* (cid:2)+

*

                                                     

*
p (cid:1)t
(

*

) > p (cid:1)t (cid:2)1

(

)

         (4)

where t indexes processing iterations.  The dynamics of the network entail that

                                                    p (cid:1)t(
where (cid:1) represents any value (i.e.  policy) of the decision node being considered.  Substituting this
into (4) gives

) = p (cid:1)t (cid:2)1 uG

         (5)

(

)

                                                  
From  this  point  on  the  focus  is  on  a  single  iteration   which  permits  us  to  omit  the  relevant
subscripts.   Applying Bayes’ law to (6) yields

        (6)

(
p (cid:1)t (cid:2)1

* uG

*

) > p (cid:1)t (cid:2)1

(

)

p uG (cid:2)*
(
(cid:1)

) p (cid:2)*
(
)
p (cid:2)(
p uG (cid:2)(
)
)

> p (cid:2)*

(

)

                                                
Canceling  and bringing the denominator up  this becomes

(cid:2)

                                             
Rewriting the left hand side  we obtain

p uG (cid:2)*
(

) >

(cid:1)

p uG (cid:2)(

) p (cid:2)(
)

(cid:2)

                                         
Subtracting and further rearranging:

(cid:2)

(cid:1)

p uG (cid:2)*
(

) p (cid:2)(
)

>

(cid:1)

(cid:2)

p uG (cid:2)(

) p (cid:2)(
)

                                           
(

p uG (cid:3)*
(

) (cid:4) p uG (cid:3)*

(cid:6)
(cid:7)

(cid:8)
(cid:9) p (cid:3)*
)

(

                         

p uG (cid:2)*
(

) (cid:3) p uG (cid:2)(
)

(cid:6)
(cid:7) p (cid:2)(
)

> 0

(cid:1)

(cid:4)
(cid:5)

(cid:2)

                                         

(cid:6)
(cid:7)

p uG (cid:3)*
(

(cid:2)
(cid:5)(cid:3) (cid:1)(cid:3)*

) (cid:4) p uG

(cid:5)(cid:3)(
)

(cid:8)
(cid:9) p

(cid:5)(cid:3)(
)

> 0

(cid:6)
(cid:7)

p uG (cid:3)*
(

) +

(cid:2)
(cid:5)(cid:3) (cid:1)(cid:3)*
(cid:8)
) (cid:4) p uG
(cid:9) p
)
(cid:5)(cid:3)(

(cid:5)(cid:3)(
)

> 0

         (7)

         (8)

         (9)

       (10)

       (11)

       (12)

Note that this last inequality (12) follows from the definition of (cid:1)*.

+.  In particular  the policy (cid:1)* will only be part
Remark:  Of course  the identity of (cid:1)* depends on (cid:1)
+  is  optimal.   Fortunately   this  requirement  is
of  a  globally  optimal  plan  if  the  set  of  choices (cid:1)
guaranteed  to  be  met   as  long  as  no  upper  bound  is  placed  on  the  number  of  processing  cycles.
Recalling  that  we  are  considering  only  finite-horizon  problems   note  that  for  policies  leading  to
+  is  empty.   Thus (cid:1)*  at  the  relevant  policy  nodes  is  fixed   and  is
states  with  no  successors  (cid:1)
guaranteed to be part of the optimal policy.  The proof above shows that (cid:1)* will continuously rise.
Once  it  reaches  a  maximum   (cid:1)*  at  immediately  preceding  decisions  will  perforce  fit  with  the
globally optimal policy.  The process works backward  in the fashion of backward induction.

2 . 2 . 2 C o n v e r g e n c e

Continuing with the same notation  we show now that

                                                 
Note that  if we apply Bayes’ law recursively 

limt (cid:3)(cid:1) pt (cid:2)
(

* uG

) = 1

       (13)

) =

(
p uG (cid:1)(cid:3)

) pt (cid:1)(cid:3)

(

)

pi uG(

)

=

2

(
p uG (cid:1)(cid:3)
pi uG(

)
(
) pt (cid:2)1 uG(

pt (cid:2)1 (cid:1)(cid:3)
)

)

=

3

)

(
p uG (cid:1)(cid:3)
pt uG(

) pt (cid:2)1 uG(

(

pt (cid:2)2 (cid:1)(cid:3)
)
) pt (cid:2)2 uG(

…

)

       (14)

pt (cid:1)(cid:3) uG

(

            
Thus 

p1 (cid:1)(cid:2) uG

(

) =

(
p uG (cid:1)(cid:2)

) p1 (cid:1)(cid:2)

(

)

p1 uG(

)

p2 (cid:1)(cid:2) uG

(

) =

 

 

2

(
p uG (cid:1)(cid:2)
p2 uG(

)
(
) p1 uG(

p1 (cid:1)(cid:2)
)

)

 

 

p3 (cid:1)(cid:2) uG

(

) =

and so forth.  Thus  what we wish to prove is

3

(
p uG (cid:1)(cid:2)
p3 uG(

)
) p2 uG(

p1 (cid:1)(cid:2)
)
(
) p1 uG(

                                                 
or  rearranging 

(cid:1)

p1 (cid:3)*

(

)

= 1

(cid:1)

)
pt uG(

)

p uG (cid:3)*
(

(cid:1)

(cid:2)

t =1

(cid:2)

t =1

pt uG(
)
(
p uG (cid:3)(cid:4)

)

= p1 (cid:3)(cid:4)

(

).

(15)

 

)

      (16)

       (17)

       (18)

…        (19)

                                                
Note that  given the stipulated relationship between p((cid:1)) on each processing iteration and p((cid:1) | uG)
on the previous iteration 

pt uG(

) =

(cid:1)

(cid:2)

p uG (cid:2)(
)

pt (cid:2)(

) =

(cid:1)

(cid:2)

p uG (cid:2)(
)

pt (cid:3)1 (cid:2) uG

(

) =

(cid:1)

(cid:2)

pt (cid:3)1 (cid:2)(

)

p uG (cid:2)(

)2
pt (cid:3)1 uG(

)

                   

=
                                 
With this in mind  we can rewrite the left hand side product in (17) as follows:

) pt (cid:3)3 uG(

…

=

)

)

pt (cid:3)1 (cid:2)(
)

p uG (cid:2)(

(cid:1)
pt (cid:3)1 uG(

)3
) pt (cid:3) 2 uG(

(cid:2)

(cid:1)
pt (cid:3)1 uG(

p uG (cid:2)(

)4
) pt (cid:3)2 uG(

(cid:2)

pt (cid:3)1 (cid:2)(
)

p1 uG(
)
(
p uG (cid:2)(cid:4)

)

(cid:3)

p uG (cid:2)(
)

2

p1 (cid:2)(
)

(cid:1)

(cid:2)

(
p uG (cid:2)(cid:4)

) p1 uG(

)

(cid:3)

p uG (cid:2)(
)

3

p1 (cid:2)(
)

(cid:1)

(cid:2)

(cid:3)

p uG (cid:2)(
)

4

p1 (cid:2)(
)

(cid:1)

(cid:2)

         
Note  that   given  (18)   the  numerator  in  each  factor  of  (19)  cancels  with  the  denominator  in  the
subsequent factor  leaving only p(uG|(cid:1)*) in that denominator. The expression can thus be rewritten
as

(
p uG (cid:2)(cid:4)

) p1 uG(

) p2 uG(

)

(
p uG (cid:2)(cid:4)

) p1 uG(

) p2 uG(

) p3 uG(

)

1

                    

(
p uG (cid:2)(cid:4)

1

(
p uG (cid:2)(cid:4)

)

(cid:3)

1

(
p uG (cid:2)(cid:4)

)

(cid:3)

)

(cid:1)

(cid:2)

(cid:3)

p uG (cid:2)(
)

4

p1 (cid:2)(
)

(
p uG (cid:2)(cid:4)

)

)(cid:1)
p uG (cid:3)(
.
(cid:1) p1 (cid:3)(
)
(
)
p uG (cid:3)(cid:4)

=

(cid:2)

(cid:3)

…

 

       (20)

The objective is then to show that the above equals p((cid:1)*).   It proceeds directly from the definition
of (cid:1)* that  for all (cid:1) other than (cid:1)* 

p uG (cid:1)(
)
(
)
p uG (cid:1)(cid:2)

< 1

       (21)

                                                        
Thus   all  but  one  of  the  terms  in  the  sum  above  approach  zero   and  the  remaining  term  equals
p1((cid:1)*).  Thus 

                                            

(cid:2)

(cid:3)

)(cid:1)
p uG (cid:3)(
)
(
p uG (cid:3)(cid:5)

(cid:1)

p1 (cid:3)(

) = p1 (cid:3)(cid:5)

(

)

       (22)

3

S i m u l a t i o n s

3 . 1

B i n a r y   c h o i c e

We  begin  with  a  simulation  of  a  simple  incentive  choice  situation.    Here   an  animal  faces
two  levers.    Pressing  the  left  lever  reliably  yields  a  preferred  food  (r  =  2)   the  right  a  less
preferred food (r = 1).  Representing these contingencies in a network structured as in Fig. 1
(left)  and  employing  the  iterative  algorithm  described  in  section  2.2  yields  the  results  in
Figure  2A.    Shown  here  are  the  posterior  probabilities  for  the  policies  press  left  and  press
right   along  with  the  marginal  value  of  p(u  =  1)  under  these  posteriors  (labeled  EV  for
expected  value).    The  dashed  horizontal  line  indicates  the  expected  value  for  the  optimal
plan  to which the model obviously converges.

A  key  empirical  assay  for  purposive  behavior  involves  outcome  devaluation.  Here   actions
yielding a previously valued outcome are abandoned after the incentive value of the outcome
is reduced   for example by pairing with an aversive event (e.g.  [4]).  To simulate this within
the  binary  choice  scenario  just  described   we  reduced  to  zero  the  reward  value  of  the  food
yielded  by  the  left  lever  (fL)   by  making  the  appropriate  change  to  p(u|fL).    This  yielded  a
reversal in lever choice (Fig. 2B).

Another  signature  of  purposive  actions  is  that  they  are  abandoned  when  their  causal
connection  with  rewarding  outcomes  is  removed  (contingency  degradation   see  [4]).    We
simulated  this  by  starting  with  the  model  from  Fig.  2A  and  changing  conditional
probabilities  at  s  for  t=2  to  reflect a  decoupling  of  the left  action  from  the  fL  outcome.    The
resulting behavior is shown in Fig. 2C.

 

Fig 2. Simulation results  binary choice.

3 . 2

S t o c h a s t i c   o u t c o m e s

A  critical  aspect  of  the  present  modeling  paradigm  is  that  it  yields  reward-maximizing
choices  in  stochastic  domains   a  property  that  distinguishes  it  from  some  other  recent
approaches  using  graphical  models  to  do  planning  (e.g.   [9]).    To  illustrate   we  used  the
architecture  in  Figure  1  (left)  to  simulate  a  choice  between  two  fair  coins.    A  ‘left’  coin
yields  $1  for  heads   $0  for  tails;  a  ‘right’  coin  $2  for  heads  but  for  tails  a  $3  loss.    As
illustrated in Fig. 2D  the model maximizes expected value by opting for the left coin.

Fig 3. Simulation results  two-step sequential choice.

3 . 3

S e q u e n t i a l   d e c i s i o n

Here  we  adopt the  two-step T-maze  scenario used  by [24]  (Fig. 3A).   Representing  the task
contingencies  in  a  graphical  model  based  on  the  template  from  Fig  1  (right)   and  using  the
reward  values  indicated  in  Fig.  3A   yields  the  choice  behavior  shown  in  Figure  3B.
Following  [24]   a  shift  in  motivational  state  from  hunger  to  thirst  can  be  represented  in  the

graphical  model  by  changing  the  reward  function  (R(cheese)  =  2   R(X)  =  0   R(water)  =  4 
R(carrots)  =  1).    Imposing  this  change  at  the  level  of  the  u  variables  yields  the  choice
behavior  shown  in  Fig.  3C.    The  model  can  also  be  used  to  simulate  effort-based  decision.
Starting  with  the  scenario  in  Fig.  2A   we  simulated  the  insertion  of  an  effort-demanding
scalable  barrier  at  S2  (R(S2)  =  -2)  by  making  appropriate  changes  p(u|s).    The  resulting
behavior is shown in Fig. 3D.

A  famous  empirical  demonstration  of  purposive  control  involves  detour  behavior.  Using  a
maze like the  one shown  in Fig. 4A   with a  food reward  placed at  s5   Tolman [2]  found that
rats reacted to a barrier at location A by taking the upper route  but to a barrier at B by taking
the  longer  lower  route.    We  simulated  this  experiment  by  representing  the  corresponding
transition  and  reward  functions  in  a  graphical  model  of  the  form  shown  in  Fig.  1  (right) 3
representing  the  insertion  of  barriers  by  appropriate changes  to  the  transition  function.    The
resulting choice behavior at the critical juncture s2 is shown in Fig. 4.

Fig 4. Simulation results  detour behavior. B: No barrier. C: Barrier at A. D: Barrier at B.

Another  classic  empirical  demonstration  involves  latent
learning.   Blodgett  [25]  allowed rats  to explore  the maze
shown  in  Fig.  5.    Later  insertion  of  a  food  reward  at  s13
was  followed  immediately  by  dramatic  reductions  in  the
running  time   reflecting  a  reduction  in  entries  into  blind
alleys.    We  simulated  this  effect  in  a  model  based  on  the
template  in  Fig.  1  (right)   representing  the  maze  layout
via  an  appropriate  transition  function.    In  the  absence  of
a  reward  at  s12   random  choices  occurred  at  each
intersection.    However   setting  R(s13)  =  1  resulted  in  the
set of choices indicated by the heavier arrows in Fig. 5.

4

R e l a t i o n   t o   p re v i o u s   w o r k

         Fig 5. Latent learning.

Initial  proposals  for  how  to  solve  decision  problems  through  probabilistic  inference  in
graphical  models   including  the  idea  of  encoding  reward  as  the  posterior  probability  of  a
random  utility  variable   were  put  forth  by  Cooper  [23].    Related  ideas  were  presented  by
Shachter  and  Peot  [12]   including  the  use  of  nodes  that  integrate  information  from  multiple
utility nodes.  More recently  Attias [11] and Verma and Rao [9] have used graphical models
to  solve  shortest-path  problems   leveraging  probabilistic  representations  of  rewards   though
not  in  a  way  that  guaranteed  convergence  on  optimal  (reward  maximizing)  plans.    More
closely related to the present research is work by Toussaint and Storkey [10]  employing  the
EM algorithm.  The iterative approach we have introduced here has a certain resemblance to
the EM procedure  which becomes evident if one views the policy variables in our models as
parameters  on  the  mapping  from  states  to  actions.    It  seems  possible  that  there  may  be  a
formal equivalence between the algorithm we have proposed and the one reported by [10].

As a cognitive and neuroscientific proposal  the present work bears a close relation to recent
work  by  Hasselmo  [6]   addressing  the  prefrontal  computations  underlying  goal-directed
action  selection  (see  also  [7]).    The  present  efforts  are  tied  more  closely  to  normative
principles  of  decision-making   whereas the  work  in  [6]  is tied  more  closely  to the  details  of
neural  circuitry.    In  this  respect   the  two  approaches  may  prove  complementary   and  it  will
be interesting to further consider their interrelations.
                                                  
3 In this simulation and the next  the set of states associated with each state node was limited to the
set of reachable states for the relevant time-step  assuming an initial state of s1.

A c k n o w l e d g m e n t s

Thanks to  Andrew Ledvina   David  Blei  Yael  Niv  Nathaniel  Daw  and  Francisco Pereira  for
useful comments.

R e f e r e n c e s

[1] Hull  C.L.  Principles of Behavior. 1943  New York: Appleton-Century.

[2] Tolman  E.C.  Purposive Behavior in Animals and Men. 1932  New York: Century.

[3]  Dickinson   A.   Actions  and  habits:  the  development  of  behavioral  autonomy.  Philosophical
Transactions of the Royal Society (London)  Series B  1985. 308: p. 67-78.

[4]  Balleine   B.W.  and  A.  Dickinson   Goal-directed  instrumental  action:  contingency  and  incentive
learning and their cortical substrates. Neuropharmacology  1998. 37: p. 407-419.

[5]  Daw   N.D.   Y.  Niv   and  P.  Dayan   Uncertainty-based  competition  between  prefrontal  and  striatal
systems for behavioral control. Nature Neuroscience  2005. 8: p. 1704-1711.

[6]  Hasselmo   M.E.   A  model  of  prefrontal  cortical  mechanisms  for  goal-directed  behavior.  Journal  of
Cognitive Neuroscience  2005. 17: p. 1115-1129.

[7]  Schmajuk   N.A.  and  A.D.  Thieme   Purposive  behavior  and  cognitive  mapping.    A  neural  network
model. Biological Cybernetics  1992. 67: p. 165-174.

[8]  Tatman   J.A.  and  R.D.  Shachter   Dynamic  programming  and 
Transactions on Systems  Man and Cybernetics  1990. 20: p. 365-379.

influence  diagrams.  IEEE

[9]  Verma   D.  and  R.P.N.  Rao.  Planning  and  acting  in  uncertain  enviroments  using  probabilistic
inference. in IEEE/RSJ International Conference on Intelligent R obots and Systems. 2006.

[10]  Toussaint   M.  and  A.  Storkey.  Probabilistic  inference  for  solving  discrete  and  continuous  state
markov  decision  processes.  in  Proceedings  of  the  23rd  International  Conference  on  Machine
Learning. 2006. Pittsburgh  PA.

[11]  Attias   H.  Planning  by  probabilistic  inference.  in  Proceedings  of  the  9th  Int.  Workshop  on
Artificial Intelligence and Statistics. 2003.

[12]  Shachter   R.D.  and  M.A.  Peot.  Decision  making  using  probabilistic  inference  methods.  in
Uncertainty  in  artificial  intelligence:  Proceedings  of  the  Eighth  Conference  (1992).  1992.  Stanford
University: M. Kaufmann.

[13]  Chater   N.   J.B.  Tenenbaum   and  A.  Yuille   Probabilistic  models  of  cognition:  conceptual
foundations. Trends in Cognitive Sciences  2006. 10(7): p. 287-291.

[14] Doya  K.  et al.  eds. The Bayesian Brain: Probabilistic  Approaches to Neural Coding. 2006  MIT
Press: Cambridge  MA.

[15]  Miller   E.K.  and  J.D.  Cohen   An  integrative  theory  of  prefrontal  cortex  function.  Annual  Review
of Neuroscience  2001. 24:  p. 167-202.

[16]  Asaad   W.F.   G.  Rainer   and  E.K.  Miller   Task-specific  neural  activity  in  the  primate  prefrontal
cortex. Journal of  Neurophysiology  2000. 84: p. 451-459.

[17] Rolls  E.T.  The functions of the orbitofrontal cortex. Brain and Cognition  2004. 55: p. 11-29.

[18]  Padoa-Schioppa   C.  and  J.A.  Assad   Neurons  in  the  orbitofrontal  cortex  encode  economic  value.
Nature  2006. 441: p. 223-226.

[19]  Gopnik   A.   et  al.   A  theory  of  causal  learning  in  children:  causal  maps  and  Bayes  nets.
Psychological Review  2004. 111: p. 1-31.

[20]  Hamilton   A.F.d.C.  and  S.T.  Grafton   Action  outcomes  are  represented  in  human  inferior
frontoparietal cortex. Cerebral Cortex  2008. 18: p. 1160-1168.

[21]  Johnson   A.   M.A.A.  van  der  Meer   and  D.A.  Redish   Integrating  hippocampus  and  striatum  in
decision-making. Current Opinion in Neurobiology  2008. 17: p. 692-697.

[22] Jensen  F.V.  Bayesian Networks and Decision Graphs. 2001  New York: Springer Verlag.

[23]  Cooper   G.F.  A  method  for  using  belief  networks  as  influence  diagrams.  in  Fourth  Workshop  on
Uncertainty in Artificial Intelligence. 1988. University of Minnesota  Minneapolis.

[24]  Niv   Y.   D.  Joel   and  P.  Dayan   A  normative  perspective  on  motivation.  Trends  in  Cognitive
Sciences  2006. 10: p. 375-381.

[25]  Blodgett   H.C.   The  effect  of  the  introduction  of  reward  upon  the  maze  performance  of  rats.
University of California Publications in Psychology  1929. 4: p. 113-134.

,Anna Choromanska
John Langford
Pan Ji
Tong Zhang
Hongdong Li
Mathieu Salzmann
Ian Reid
Guangrun Wang
jiefeng peng
Ping Luo
Xinjiang Wang
Liang Lin