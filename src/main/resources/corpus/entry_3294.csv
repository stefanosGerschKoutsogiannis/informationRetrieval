2011,Lower Bounds for Passive and Active Learning,We develop unified information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander's capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of "disagreement coefficient." For passive learning  our lower bounds match the upper bounds of Gine and Koltchinskii up to constants and generalize analogous results of Massart and Nedelec. For active learning  we provide first known lower bounds based on the capacity function rather than the disagreement coefficient.,Lower Bounds for Passive and Active Learning

Maxim Raginsky∗

Coordinated Science Laboratory

University of Illinois at Urbana-Champaign

Alexander Rakhlin

Department of Statistics
University of Pennsylvania

Abstract

We develop uniﬁed information-theoretic machinery for deriving lower bounds
for passive and active learning schemes. Our bounds involve the so-called Alexan-
der’s capacity function. The supremum of this function has been recently redis-
covered by Hanneke in the context of active learning under the name of “disagree-
ment coefﬁcient.” For passive learning  our lower bounds match the upper bounds
of Gin´e and Koltchinskii up to constants and generalize analogous results of Mas-
sart and N´ed´elec. For active learning  we provide ﬁrst known lower bounds based
on the capacity function rather than the disagreement coefﬁcient.

1

Introduction

Not all Vapnik-Chervonenkis classes are created equal. This was observed by Massart and N´ed´elec
[24]  who showed that  when it comes to binary classiﬁcation rates on a sample of size n under a
margin condition  some classes admit rates of the order 1/n while others only (log n)/n. The latter
classes were called “rich” in [24]. As noted by Gin´e and Koltchinskii [15]  the ﬁne complexity
notion that deﬁnes this “richness” is in fact embodied in Alexander’s capacity function.1 Somewhat
surprisingly  the supremum of this function (called the disagreement coefﬁcient by Hanneke [19])
plays a key role in risk bounds for active learning. The contribution of this paper is twofold. First  we
prove lower bounds for passive learning based on Alexander’s capacity function  matching the upper
bounds of [15] up to constants. Second  we prove lower bounds for the number of label requests in
active learning in terms of the capacity function. Our proof techniques are information-theoretic in
nature and provide a uniﬁed tool to study active and passive learning within the same framework.
Active and passive learning. Let (X  A) be an arbitrary measurable space. Let (X  Y ) be a random
variable taking values in X × {0  1} according to an unknown distribution P = Π ⊗ PY |X  where
Π denotes the marginal distribution of X. Here  X is an instance (or a feature  a predictor variable)
and Y is a binary response (or a label). Classical results in statistical learning assume availability of
an i.i.d. sample {(Xi  Yi)}n
i=1 from P . In this framework  the learner is passive and has no control
on how this sample is chosen. The classical setting is well studied  and the following question has
recently received attention: do we gain anything if data are obtained sequentially  and the learner
is allowed to modify the design distribution Π of the predictor variable before receiving the next
pair (Xi  Yi)? That is  can the learner actively use the information obtained so far to facilitate faster
learning?
Two paradigms often appear in the literature: (i) the design distribution is a Dirac delta function
at some xi that depends on (xi−1  Y i−1)  or (ii) the design distribution is a restriction of the orig-
inal distribution to some measurable set. There is rich literature on both approaches  and we only
mention a few results here. The paradigm (i) is closely related to learning with membership queries
[21]  generalized binary search [25]  and coding with noiseless feedback [6]. The goal is to actively
choose the next xi so that the observed Yi ∼ PY |X=xi is sufﬁciently “informative” for the clas-
siﬁcation task. In this paradigm  the sample no longer provides information about the distribution

∗Afﬁliation until January  2012: Department of Electrical and Computer Engineering  Duke University.
1To be precise  the capacity function depends on the underlying probability distribution.

1

Π (see [7] for further discussion and references). The setting (ii) is often called selective sampling
[9  13  8]  although the term active learning is also used. In this paradigm  the aim is to sequentially
choose subsets Di ⊆ X based on the observations prior to the ith example  such that the label Yi
is requested only if Xi ∈ Di. The sequence {Xi}n
i=1 is assumed to be i.i.d.  and so  form the view
point of the learner  the Xi is sampled from the conditional distribution Π(·|Di).
In recent years  several interesting algorithms for active learning and selective sampling have ap-
peared in the literature  most notably: the A2 algorithm of Balcan et al. [4]  which explicitly main-
tains Di as a “disagreement” set of a “version space”; the empirical risk minimization (ERM) based
algorithm of Dasgupta et al. [11]  which maintains the set Di implicitly through synthetic and real
examples; and the importance-weighted active learning algorithm of Beygelzimer et al. [5]  which
constructs the design distribution through careful reweighting in the feature space. An insightful
analysis has been carried out by Hanneke [20  19]  who distilled the role of the so-called disagree-
ment coefﬁcient in governing the performance of several of these active learning algorithms. Finally 
Koltchinskii [23] analyzed active learning procedures using localized Rademacher complexities and
Alexander’s capacity function  which we discuss next.
Alexander’s capacity function. Let F denote a class of candidate classiﬁers  where a classiﬁer is a
measurable function f : X → {0  1}. Suppose the VC dimension of F is ﬁnite: VC-dim(F) = d.
The loss (or risk) of f is its probability of error  RP (f) (cid:44) EP [1{f (X)(cid:54)=Y }] = P (f(X) (cid:54)= Y ).
It is well known that the risk is globally minimized by the Bayes classiﬁer f∗ = f∗
P   deﬁned by
f∗(x) (cid:44) 1{2η(x)≥1}  where η(x) (cid:44) E[Y |X = x] is the regression function. Deﬁne the margin as
h (cid:44) inf x∈X |2η(x) − 1|. If h > 0  we say the problem satisﬁes Massart’s noise condition. We
deﬁne the excess risk of a classiﬁer f by EP (f) (cid:44) RP (f) − RP (f∗)  so that EP (f) ≥ 0  with
equality if and only if f = f∗ Π-a.s. Given ε ∈ (0  1]  deﬁne

Fε(f∗) (cid:44) {f ∈ F : Π(f(X) (cid:54)= f∗(X)) ≤ ε}  
Dε(f∗) (cid:44) {x ∈ X : ∃f ∈ Fε(f∗) s.t. f(x) (cid:54)= f∗(x)}

The set Fε consists of all classiﬁers f ∈ F that are ε-close to f∗ in the L1(Π) sense  while the set
Dε consists of all points x ∈ X   for which there exists a classiﬁer f ∈ Fε that disagrees with the
Bayes classiﬁer f∗ at x. The Alexander’s capacity function [15] is deﬁned as

τ(ε) (cid:44) Π(Dε(f∗))/ε 

(1)
that is  τ(ε) measures the relative size (in terms of Π) of the disagreement region Dε compared to ε.
Clearly  τ(ε) is always bounded above by 1/ε; however  in some cases τ(ε) ≤ τ0 with τ0 < ∞.
The function τ was originally introduced by Alexander [1  2] in the context of exponential in-
equalities for empirical processes indexed by VC classes of functions  and Gin´e and Koltchin-
skii [15] generalized Alexander’s results.
In particular  they proved (see [15  p. 1213]) that 

for a VC-class of binary-valued functions with VC-dim(F) = d  the ERM solution (cid:98)fn =

arg minf∈F 1
n

i=1 1{f (Xi)(cid:54)=Yi} under Massart’s noise condition satisﬁes

(cid:80)n

EP ((cid:98)fn) ≤ C

(cid:20) d

nh

(cid:18) d

(cid:19)

(cid:21)

+ s
nh

log τ

nh2

(2)
with probability at least 1 − Ks−1e−s/K for some constants C  K and any s > 0. The upper bound
(2) suggests the importance of the Alexander’s capacity function for passive learning  leaving open
the question of necessity. Our ﬁrst contribution is a lower bound which matches the upper bound (2)
up to constant  showing that  in fact  dependence on the capacity is unavoidable.
Recently  Koltchinskii [23] made an important connection between Hanneke’s disagreement coefﬁ-
cient and Alexander’s capacity function. Under Massart’s noise condition  Koltchinskii showed (see
[23  Corollary 1]) that  for achieving an excess loss of ε with conﬁdence 1−δ  the number of queries
issued by his active learning algorithm is bounded above by

C

τ0 log(1/ε)

h2

[d log τ0 + log(1/δ) + log log(1/ε) + log log(1/h)]  

(3)

where τ0 = supε∈(0 1] τ(ε) is Hanneke’s disagreement coefﬁcient. Similar bounds based on the
disagreement coefﬁcient have appeared in [19  20  11]. The second contribution of this paper is a
lower bound on the expected number of queries based on Alexander’s capacity τ(ε).

2

Comparison to known lower bounds. For passive learning  Massart and N´ed´elec [24] proved
two lower bounds which  in fact  correspond to τ(ε) = 1/ε and τ(ε) = τ0  the two endpoints on
the complexity scale for the capacity function. Without the capacity function at hand  the authors
emphasize that “rich” VC classes yield a larger lower bound. Our Theorem 1 below gives a uniﬁed
construction for all possible complexities τ(ε).
In the PAC framework  the lower bound Ω(d/ε + (1/ε) log(1/δ)) goes back to [12]. It follows
from our results that in the noisy version of the problem (h (cid:54)= 1)  the lower bound is in fact
Ω((d/ε) log(1/ε) + (1/ε) log(1/δ)) for classes with τ(ε) = Ω(1/ε).
For active learning  Castro and Nowak [7] derived lower bounds  but without the disagreement
coefﬁcient and under a Tsybakov-type noise condition. This setting is out of the scope of this paper.
Hanneke [19] proved a lower bound on the number of label requests speciﬁcally for the A2 algorithm
in terms of the disagreement coefﬁcient. In contrast  lower bounds of Theorem 2 are valid for any
algorithm and are in terms of Alexander’s capacity function. Finally  a result by K¨a¨ari¨ainen [22]
(strengthened by [5]) gives a lower bound of Ω(ν2/ε2) where ν = inf f∈F EP (f). A closer look
at the construction of the lower bound reveals that it is achieved by considering a speciﬁc margin
h = ε/ν. Such an analysis is somewhat unsatisfying  as we would like to keep h as a free parameter 
not necessarily coupled with the desired accuracy ε. This point of view is put forth by Massart and
N´ed´elec [24  p. 2329]  who argue for a non-asymptotic analysis where all the parameters of the
problem are made explicit. We also feel that this gives a better understanding of the problem.

2 Setup and main results
We suppose that the instance space X is a countably inﬁnite set. Also  log(·) ≡ loge(·) throughout.
Deﬁnition 1. Given a VC function class F and a margin parameter h ∈ [0  1]  let C(F  h) denote
the class of all conditional probability distributions PY |X of Y ∈ {0  1} given X ∈ X   such that:
(a) the Bayes classiﬁer f∗ ∈ F  and (b) the corresponding regression function satisﬁes the Massart
condition with margin h > 0.
Let P(X ) denote the space of all probability measures on X . We now introduce Alexander’s ca-
pacity function (1) into the picture. Whenever we need to specify explicitly the dependence of τ(ε)
on f∗ and Π  we will write τ(ε; f∗  Π). We also denote by T the set of all admissible capacity
functions τ : (0  1] → R+  i.e.  τ ∈ T if and only if there exist some f∗ ∈ F and Π ∈ P(X )  such
that τ(ε) = τ(ε; f∗  Π) for all ε ∈ (0  1]. Without loss of generality  we assume τ(ε) ≥ 2.
Deﬁnition 2. Given some Π ∈ P(X ) and a pair (F  h) as in Def. 1  we let P(Π F  h) denote the set
of all joint distributions of (X  Y ) ∈ X × {0  1} of the form Π ⊗ PY |X  such that PY |X ∈ C(F  h).
Moreover  given an admissible function τ ∈ T and some ε ∈ (0  1]  we let P(Π F  h  τ  ε) denote
the subset of P(Π F  h)  such that τ(ε; f∗  Π) = τ(ε).

Finally  we specify the type of learning schemes we will be dealing with.
Deﬁnition 3. An n-step learning scheme S consists of the following objects: n conditional proba-
bility distributions Π(t)

Xt|X t−1 Y t−1  t = 1  . . .   n  and a mapping ψ : X n × {0  1}n → F.

This deﬁnition covers the passive case if we let
Xt|X t−1 Y t−1(·|xt−1  yt−1) = Π(·) 
Π(t)

∀(xt−1  yt−1) ∈ X t−1 × {0  1}t−1

as well as the active case  in which Π(t)
Xt|X t−1 Y t−1 is the user-controlled design distribution for
the feature at time t given all currently available information. The learning process takes place
sequentially as follows: At each time step t = 1  . . .   n  a random feature Xt is drawn accord-
t=1 are collected  the learner computes the candidate classiﬁer (cid:98)fn = ψ(X n  Y n).
X t−1 Y t−1(·|X t−1  Y t−1)  and then a label Yt is drawn given Xt. After the n samples
ing to Π(t)
{(Xt  Yt)}n
To quantify the performance of such a scheme  we need the concept of an induced measure  which
generalizes the set-up of [14]. Speciﬁcally  given some P = Π ⊗ PY |X ∈ P(Π F  h)  deﬁne the

3

following probability measure on X n × {0  1}n:

n(cid:89)

t=1

PS(xn  yn) =

PY |X(yt|xt)Π(t)

Xt|X t−1 Y t−1(xt|xt−1  yt−1).

Deﬁnition 4. Let Q be a subset of P(Π F  h). Given an accuracy parameter ε ∈ (0  1) and a
conﬁdence parameter δ ∈ (0  1)  an n-step learning scheme S is said to (ε  δ)-learn Q if

PS(cid:16)

EP ((cid:98)fn) ≥ εh

(cid:17) ≤ δ.

sup
P∈Q

(4)

Remark 1. Leaving the precision as εh makes the exposition a bit cleaner in light of the fact that 
under Massart’s noise condition with margin h  EP (f) ≥ h(cid:107)f − f∗
P (X))
(cf. Massart and N´ed´elec [24  p. 2352]).

P(cid:107)L1(Π) = hΠ(f(X) (cid:54)= f∗

With these preliminaries out of the way  we can state the main results of this paper:
Theorem 1 (Lower bounds for passive learning). Given any τ ∈ T   any sufﬁciently large d ∈ N and
any ε ∈ (0  1]  there exist a probability measure Π ∈ P(X ) and a VC class F with VC-dim(F) = d
with the following properties:
(1) Fix any K > 1 and δ ∈ (0  1/2). If there exists an n-step passive learning scheme that (ε/2  δ)-
learns P(Π F  h  τ  ε) for some h ∈ (0  1 − K−1]  then

(cid:19)

log 1
δ
Kεh2

(cid:18)(1 − δ)d log τ(ε)
(cid:18)(1 − δ)d

Kεh2

+

(cid:19)

n = Ω

.

(5)

(2) If there exists an n-step passive learning scheme that (ε/2  δ)-learns P(Π F  1  τ  ε)  then

n = Ω

(6)
Theorem 2 (Lower bounds for active learning). Given any τ ∈ T   any sufﬁciently large d ∈ N and
any ε ∈ (0  1]  there exist a probability measure Π ∈ P(X ) and a VC class F with VC-dim(F) = d
with the following property: Fix any K > 1 and any δ ∈ (0  1/2). If there exists an n-step active
learning scheme that (ε/2  δ)-learns P(Π F  h  τ  ε) for some h ∈ (0  1 − K−1]  then

ε

.

(cid:18)(1 − δ)d log τ(ε)

Kh2

n = Ω

+

τ(ε) log 1
δ

Kh2

(cid:19)

.

(7)

Remark 2. The lower bound in (6) is well-known and goes back to [12]. We mention it because it
naturally arises from our construction. In fact  there is a smooth transition between (5) and (6)  with
the extra log τ(ε) factor disappearing as h approaches 1. As for the active learning lower bound  we
conjecture that d log τ(ε) is  in fact  optimal  and the extra factor of τ0 in dτ0 log τ0 log(1/ε) in (3)
arises from the use of a passive learning algorithm as a black box.

The remainder of the paper is organized as follows: Section 3 describes the required information-
theoretic tools  which are then used in Section 4 to prove Theorems 1 and 2. The proofs of a number
of technical lemmas can be found in the Supplementary Material.

Information-theoretic framework

3
Let P and Q be two probability distributions on a common measurable space W. Given a convex
function φ : [0 ∞) → R such that φ(1) = 0  the φ-divergence2 between P and Q [3  10] is given by

(cid:90)

(cid:19)

(cid:18) dP/dµ

dQ/dµ

Dφ(P(cid:107)Q) (cid:44)

dQ
dµ

φ

W

dµ 

(8)

where µ is an arbitrary σ-ﬁnite measure that dominates both P and Q.3 For the special case of
W = {0  1}  when P and Q are the distributions of a Bernoulli(p) and a Bernoulli(q) random

2We deviate from the standard term “f-divergence” since f is already reserved for a generic classiﬁer.
3For instance  one can always take µ = P + Q. It it easy to show that the value of Dφ(P(cid:107)Q) in (8) does not

depend on the choice of the dominating measure.

4

variable  we will denote their φ-divergence by

(cid:18) p

q

(cid:19)

+ (1 − q) · φ

(cid:19)

(cid:18)1 − p

1 − q

.

(9)

dφ(p(cid:107)q) = q · φ

Two particular choices of φ are of interest: φ(u) = u log u  which gives the ordinary Kullback–
Leibler (KL) divergence D(P(cid:107)Q)  and φ(u) = − log u  which gives the reverse KL divergence
D(Q(cid:107)P)  which we will denote by Dre(P(cid:107)Q). We will write d(·(cid:107)·) for the binary KL divergence.
Our approach makes fundamental use of the data processing inequality that holds for any φ-
divergence [10]: if P and Q are two possible probability distributions for a random variable W ∈ W
and if PZ|W is a conditional probability distribution of some other random variable Z given W   then
(10)

Dφ(PZ(cid:107)QZ) ≤ Dφ(P(cid:107)Q) 

where PZ (resp.  QZ) is the marginal distribution of Z when W has distribution P (resp.  Q).
Consider now an arbitrary n-step learning scheme S. Let us ﬁx a ﬁnite set {f1  . . .   fN} ⊂ F and
assume that to each m ∈ [N] we can associate a probability measure P m = Π⊗P m
Y |X ∈ P(Π F  h)
with the Bayes classiﬁer f∗

= fm. For each m ∈ [N]  let us deﬁne the induced measure

Y |X(yt|xt)Π(t)
P m

Xt|X t−1 Y t−1(xt|xt−1  yt−1).

(11)

PS m(xn  yn) (cid:44) n(cid:89)

Pm

t=1

Moreover  given any probability distribution π over [N]  let PS π(m  xn  yn) (cid:44) π(m)PS m(xn  yn).
In other words  PS π is the joint distribution of (M  X n  Y n) ∈ [N] × X n × {0  1}n  under which
M ∼ π and P(X n  Y n|M = m) = PS m(X n  Y n).
The ﬁrst ingredient in our approach is standard [27  14  24]. Let {f1  . . .   fN} be an arbitrary 2ε-
packing subset of F (that is  (cid:107)fi − fj(cid:107)L1(Π) > 2ε for all i (cid:54)= j). Suppose that S satisﬁes (4) on
some Q that contains {P 1  . . .   P N}. Now consider

(cid:99)M ≡ (cid:99)M(X n  Y n) (cid:44) arg min
Lemma 1. With the above deﬁnitions  PS π((cid:99)M (cid:54)= M) ≤ δ.

Then the following lemma is easily proved using triangle inequality:

(cid:107)(cid:98)fn − fm(cid:107)L1(Π).

1≤m≤N

(12)

The second ingredient of our approach is an application of the data processing inequality (10) with a
judicious choice of φ. Let W (cid:44) (M  X n  Y n)  let M be uniformly distributed over [N]  π(m) = 1
for all m ∈ [N]  and let P be the induced measure PS π. Then we have the following lemma (see
also [17  16]):
Lemma 2. Consider any probability measure Q for W   under which M is distributed according to
π and independent of (X n  Y n). Let the divergence-generating function φ be such that the mapping
p (cid:55)→ dφ(p(cid:107)q) is nondecreasing on the interval [q  1]. Then  assuming that δ ≤ 1 − 1
N  

N

(cid:18)

(cid:19)

(cid:19)

(cid:18) N δ

N − 1

Dφ(P(cid:107)Q) ≥ 1
N

· φ (N(1 − δ)) +

1 − 1
N

· φ

.

(13)

Proof. Deﬁne the indicator random variable Z = 1{cM =M}. Then P(Z = 1) ≥ 1 − δ by Lemma 1.
On the other hand  since Q can be factored as Q(m  xn  yn) = 1

N(cid:88)

m=1

Q(M = m (cid:99)M = m) =

N(cid:88)

(cid:88)

m=1

xn yn

1
N

N

QX n Y n(xn  yn)  we have
QX n Y n(xn  yn)1{cM (xn yn)=m} =

1
N

.

Q(Z = 1) =

Therefore 

Dφ(P(cid:107)Q) ≥ Dφ(PZ(cid:107)QZ) = dφ(P(Z = 1)(cid:107)Q(Z = 1)) ≥ dφ(1 − δ(cid:107)1/N) 

where the ﬁrst step is by the data processing inequality (10)  the second is due to the fact that Z is
binary  and the third is by the assumed monotonicity property of φ. Using (9)  we arrive at (13).

Next  we need to choose the divergence-generating function φ and the auxiliary distribution Q.

5

if φ(u) behaves like − log u for small u  then the lower bounds will be of the form Ω(cid:0)log 1
the marginals PM ≡ π and PX n Y n ≡ N−1(cid:80)N

Choice of φ.
Inspection of the right-hand side of (13) suggests that the usual Ω(log N) lower
bounds [14  27  24] can be obtained if φ(u) behaves like u log u for large u. On the other hand 
These observations naturally lead to the respective choices φ(u) = u log u and φ(u) = − log u 
corresponding to the KL divergence D(P(cid:107)Q) and the reverse KL divergence Dre(P(cid:107)Q) = D(Q(cid:107)P).
Choice of Q. One obvious choice of Q satisfying the conditions of the lemma is the product of
PS m: Q = PM ⊗ PX n Y n. With this Q and

(cid:1).

δ

m=1
φ(u) = u log u  the left-hand side of (13) is given by

D(P(cid:107)Q) = D(PM X n Y n(cid:107)PM ⊗ PX n Y n) = I(M; X n  Y n) 

(14)
where I(M; X n  Y n) is the mutual information between M and (X n  Y n) with joint distribution P.
On the other hand  it is not hard to show that the right-hand side of (13) can be lower-bounded by
(1 − δ) log N − log 2. Combining with (14)  we get

I(M; X n  Y n) ≥ (1 − δ) log N − log 2 

(cid:19)

(cid:18)

which is (a commonly used variant of) the well-known Fano’s inequality [14  Lemma 4.1]  [18 
p. 1250]  [27  p. 1571]. The same steps  but with φ(u) = − log u  lead to the bound
− log 2 

L(M; X n  Y n) ≥

log

log

1
δ

− log 2 ≥ 1
2

1
δ

1 − 1
N

where L(M; X n  Y n) (cid:44) Dre(PM X n Y n(cid:107)PM ⊗ PX n Y n) is the so-called lautum information be-
tween M and (X n  Y n) [26]  and the second inequality holds whenever N ≥ 2.
However  it is often more convenient to choose Q as follows. Fix an arbitrary conditional distribution
QY |X of Y ∈ {0  1} given X ∈ X . Given a learning scheme S  deﬁne the probability measure

QY |X(yt|xt)Π(t)
QS(xn  yn) for all m ∈ [N].

t=1

Xt|X t−1 Y t−1(xt|xt−1  yt−1)

and let Q(m  xn  yn) = 1
Lemma 3. For each xn ∈ X n and y ∈ X   let N(y|xn) (cid:44) |{1 ≤ t ≤ n : xt = y}|. Then

N

QS(xn  yn) (cid:44) n(cid:89)

D(P(cid:107)Q) =

Dre(P(cid:107)Q) =

1
N

1
N

N(cid:88)
N(cid:88)

m=1

(cid:88)
(cid:88)

x∈X

m=1

x∈X

(15)

(16)

(17)

(18)

D(P m

Y |X(·|x)(cid:107)QY |X(·|x))EPS m [N(x|X n)] ;

Dre(P m

Y |X(·|x)(cid:107)QY |X(·|x))EQ [N(x|X n)] .

(cid:104)

(cid:105)
Y |X(·|X)(cid:107)QY |X(·|X))

 

Dre(P M

Moreover  if the scheme S is passive  then Eq. (17) becomes

Dre(P(cid:107)Q) = n · EXEM
and the same holds for Dre replaced by D.

tance dH(β  β(cid:48)) (cid:44)(cid:80)k

4 Proofs of Theorems 1 and 2
Combinatorial preliminaries. Given k ∈ N  onsider the k-dimensional Boolean cube {0  1}k =
{β = (β1  . . .   βk) : βi ∈ {0  1}  i ∈ [k]}. For any two β  β(cid:48) ∈ {0  1}k  deﬁne their Hamming dis-
i}. The Hamming weight of any β ∈ {0  1}k is the number of its
nonzero coordinates. For k > d  let {0  1}k
d denote the subset of {0  1}k consisting of all binary
strings with Hamming weight d. We are interested in large separated and well-balanced subsets of
{0  1}k
Lemma 4. Suppose that d is even and k > 2d. Then  for d sufﬁciently large  there exists a set
Mk d ⊂ {0  1}k
6d ; (ii) dH(β  β(cid:48)) > d for
any two distinct β  β(cid:48) ∈ M(2)

d with the following properties: (i) log |Mk d| ≥ d

d. To that end  we will use the following lemma:

i=1 1{βi(cid:54)=β(cid:48)

4 log k

βj ≤ 3d
2k

(19)

(cid:88)
k d ; (iii) for any j ∈ [k] 

1

≤

d
2k

|Mk d|

β∈Mk d

6

Proof of Theorem 1. Without loss of generality  we take X = N. Let k = dτ(ε) (we increase ε if
necessary to ensure that k ∈ N)  and consider the probability measure Π that puts mass ε/d on each
x = 1 through x = k and the remaining mass 1 − ετ(ε) on x = k + 1. (Recall that τ(ε) ≤ 1/ε.)
Let F be the class of indicator functions of all subsets of X with cardinality d. Then VC-dim(F) =
d. We will focus on a particular subclass F(cid:48) of F. For each β ∈ {0  1}k
d  deﬁne fβ : X → {0  1}
by fβ(x) = βx if x ∈ [k] and 0 otherwise  and take F(cid:48) = {fβ : β ∈ {0  1}k
d}. For p ∈ [0  1]  let νp
denote the probability distribution of a Bernoulli(p) random variable. Now  to each fβ ∈ F(cid:48) let us
associate the following conditional probability measure P β

Y |X(y|x) =(cid:2)ν(1+h)/2(y)βx + ν(1−h)/2(y)(1 − βx)(cid:3) 1{x∈[k]} + 1{y=0}1{x(cid:54)∈[k]}

Y |X:

P β

It is easy to see that each P β

Y |X belongs to C(F  h). Moreover  for any two fβ  fβ(cid:48) ∈ F we have

(cid:107)fβ − fβ(cid:48)(cid:107)L1(Π) = Π(fβ(X) (cid:54)= fβ(cid:48)(X)) = ε
d

1{βi(cid:54)=β(cid:48)

i} ≡ ε

d

dH(β  β(cid:48)).

k(cid:88)

i=1

d  the probability measure P β = Π ⊗ P β

Hence  for each choice of f∗ = fβ∗ ∈ F we have Fε(fβ∗) = {fβ : dH(β  β∗) ≤ d}. This implies
that Dε(fβ∗) = [k]  and therefore τ(ε; fβ∗   Π) = Π([k])/ε = τ(ε). We have thus established that 
Y |X is an element of P(Π F  h  τ  ε).
for each β ∈ {0  1}k
d be the set described in Lemma 4  and let G (cid:44) {fβ : β ∈ Mk d}. Then
Finally  let Mk d ⊂ {0  1}k
for any two distinct β  β(cid:48) ∈ Mk d we have (cid:107)fβ − fβ(cid:48)(cid:107)L1(Π) = ε
d dH(β  β(cid:48)) > ε. Hence  G is a
ε-packing of F(cid:48) in the L1(Π)-norm.
Now we are in a position to apply the lemmas of Section 3. Let {β(1)  . . .   β(N )}  N = |Mk d| 
be a ﬁxed enumeration of the elements of Mk d. For each m ∈ [N]  let us denote by P m
Y |X the
Y |X on X × {0  1}  and by
conditional probability measure P β(m)
fm ∈ G the corresponding Bayes classiﬁer. Now consider any n-step passive learning scheme that
(ε/2  δ)-learns P(Π F  h  τ  ε)  and deﬁne the probability measure P on [N] × X n × {0  1}n by
P(m  xn  yn) = 1
In addition  for
every γ ∈ (0  1) deﬁne the auxiliary measure Qγ on [N] × X n × {0  1}n by Qγ(m  xn  yn) =

PS m(xn  yn)  where PS m is constructed according to (11).

Y |X   by P m the measure Π ⊗ P m

N

γ(xn  yn)  where QS
QS

1
N

γ is constructed according to (15) with

Y |X(y|x) (cid:44) νγ(y)1{x∈[k]} + 1{y=0}1{x(cid:54)∈[k]}.
Qγ

Applying Lemma 2 with φ(u) = u log u  we can write

D(P(cid:107)Qγ) ≥ (1 − δ) log N − log 2 ≥ (1 − δ)d

4

log k
6d

− log 2

Next we apply Lemma 3. Deﬁning η = 1+h
2

and using the easily proved fact that

D(P m

Y |X(·|x)(cid:107)Qγ

Y |X(·|x)) = [d(η(cid:107)γ) − d(1 − η(cid:107)γ)] fm(x) + d(1 − η(cid:107)γ)1{x∈[k]} 

we get

D(P(cid:107)Qγ) = nε [d(η(cid:107)γ) + (τ(ε) − 1)d(1 − η(cid:107)γ)] .

Therefore  combining Eqs. (20) and (21) and using the fact that k = dτ(ε)  we obtain

(20)

(21)

n ≥

(1 − δ)d log τ (ε)

6 − log 16

4ε [d(η(cid:107)γ) + (τ(ε) − 1)d(1 − η(cid:107)γ)]  

(22)
This bound is valid for all h ∈ (0  1]  and the optimal choice of γ for a given h can be calculated in
closed form: γ∗(h) = 1−h
τ (ε) . We now turn to the reverse KL divergence. First  suppose that
h (cid:54)= 1. Lemma 2 gives Dre(P(cid:107)Q1−η) ≥ (1/2) log(1/δ) − log 2. On the other hand  using the fact
that

2 + h

∀γ ∈ (0  1)

Dre(P m

Y |X(·|x)(cid:107)Q1−η

Y |X(·|x)) = d(η(cid:107)1 − η)fm(x)

(23)

7

and applying Eq. (18)  we can write

Dre(P(cid:107)Q1−η) = nε · d(η(cid:107)1 − η) = nε · h log

1 + h
1 − h

.

(24)

We conclude that

δ − log 2
2 log 1
εh log 1+h
1−h

.

n ≥ 1
For h = 1  we get the vacuous bound n ≥ 0.
Now we consider the two cases of Theorem 1.
1−h ≤ Kh2 for all
(1) For a ﬁxed K > 1  it follows from the inequality log u ≤ u − 1 that h log 1+h
(2) For h = 1  we use (22) with the optimal setting γ∗(1) = 1/τ(ε)  which gives (6). The transition

h ∈ (0  1 − K−1]. Choosing γ = 1−h
between h = 1 and h (cid:54)= 1 is smooth and determined by γ∗(h) = 1−h

and using Eqs. (22) and (25)  we obtain (5).

(25)

2

2 + h

τ (ε).

N

Proof of Theorem 2. We work with the same construction as in the proof of Theorem 1. First  let
PS m. and Q = π ⊗ QX n Y n  where π is the uniform distribution on [N].
QX n Y n (cid:44) 1
Then  by convexity 
D(P(cid:107)Q) ≤ 1
N 2

Y |X(·|x)(cid:107)P m(cid:48)

(cid:34) n(cid:88)

≤ n max

Y |X(·|x))

Y |X(Yt|Xt)
P m
Y |X(Yt|Xt)
P m(cid:48)

m m(cid:48)∈[N ]

max
x∈[k]

D(P m

(cid:35)

EP

log

m m(cid:48)=1

t=1

(cid:80)N
N(cid:88)

m=1

.

which is upper bounded by nh log 1+h
obtain

1−h. Applying Lemma 2 with φ(u) = u log u  we therefore
n ≥ (1 − δ)d log k
k(cid:88)
N(cid:88)
(b)= d(η(cid:107)1 − η)

6d − log 16
4h log 1+h
1−h
Next  consider the auxiliary measure Q1−η with η = 1+h
2 . Then
Y |X(·|x)(cid:107)Q1−η
Y |X(·|x))EQ1−η[N(x|X n)]
N(cid:88)
k(cid:88)
(cid:33)
(cid:32)
k(cid:88)
(cid:33)
(cid:32)
k(cid:88)

EQ1−η[N(x|X n)]

Dre(P(cid:107)Q1−η) (a)=

= d(η(cid:107)1 − η)

x=1
1
N

Dre(P m

1
N

(26)

M =1

m=1

m=1

x=1

x=1

N

fm(x)

fm(x)EQ1−η[N(x|X n)]
N(cid:88)
N(cid:88)
(cid:34) k(cid:88)

β(m)
x

(cid:35)

N(x|X n)

m=1

EQ1−η[N(x|X n)]

(c)= d(η(cid:107)1 − η)

1
N

(d)≤ 3

2τ(ε) h log

(e)≤ 3n

2τ(ε) h log

x=1
1 + h
1 − h
1 + h
1 − h

 

EQ1−η

x=1

where (a) is by Lemma 3  (b) is by (23)  (c) is by deﬁnition of {fm}  (d) is by the balance condi-
x∈X N(x|X n) = n.
Applying Lemma 2 with φ(u) = − log u  we get

tion (19) satisﬁed by Mk d  and (e) is by the fact that(cid:80)k
δ − log 4(cid:1)

x=1 N(x|X n) ≤ (cid:80)

n ≥ τ(ε)(cid:0)log 1

3h log 1+h
1−h
1−h ≤ Kh2 for h ∈ (0  1 − K−1]  we get (7).
Combining (26) and (27) and using the bound h log 1+h

(27)

8

References
[1] K.S. Alexander. Rates of growth and sample moduli for weighted empirical processes indexed by sets.

Probability Theory and Related Fields  75(3):379–423  1987.

[2] K.S. Alexander. The central limit theorem for weighted empirical processes indexed by sets. Journal of

Multivariate Analysis  22(2):313–339  1987.

[3] S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from another.

J. Roy. Stat. Soc. Ser. B  28:131–142  1966.

[4] M.-F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. In ICML ’06: Proceedings of
the 23rd international conference on Machine learning  pages 65–72  New York  NY  USA  2006. ACM.
[5] A. Beygelzimer  S. Dasgupta  and J. Langford. Importance weighted active learning. In ICML. ACM

New York  NY  USA  2009.

[6] M.V. Burnashev and K.S. Zigangirov. An interval estimation problem for controlled observations. Prob-

lemy Peredachi Informatsii  10(3):51–61  1974.

[7] R. M. Castro and R. D. Nowak. Minimax bounds for active learning.

54(5):2339–2353  2008.

IEEE Trans. Inform. Theory 

[8] G. Cavallanti  N. Cesa-Bianchi  and C. Gentile. Linear classiﬁcation and selective sampling under low

noise conditions. Advances in Neural Information Processing Systems  21  2009.

[9] D. Cohn  L. Atlas  and R. Ladner. Improving generalization with active learning. Machine Learning 

15(2):201–221  1994.

[10] I. Csisz´ar. Information-type measures of difference of probability distributions and indirect observations.

Studia Sci. Math. Hungar.  2:299–318  1967.

[11] S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In Advances in

neural information processing systems  volume 20  page 2  2007.

[12] A. Ehrenfeucht  D. Haussler  M. Kearns  and L. Valiant. A general lower bound on the number of exam-

ples needed for learning. Information and Computation  82(3):247–261  1989.

[13] Y. Freund  H.S. Seung  E. Shamir  and N. Tishby. Selective sampling using the query by committee

algorithm. Machine Learning  28(2):133–168  1997.

[14] C. Gentile and D. P. Helmbold. Improved lower bounds for learning from noisy examples: an information-

theoretic approach. Inform. Comput.  166:133–155  2001.

[15] E. Gin´e and V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type empirical

processes. Ann. Statist.  34(3):1143–1216  2006.

[16] A. Guntuboyina. Lower bounds for the minimax risk using f-divergences  and applications. IEEE Trans.

Inf. Theory  57(4):2386–2399  2011.

[17] A. A. Gushchin. On Fano’s lemma and similar inequalities for the minimax risk. Theory of Probability

and Mathematical Statistics  pages 29–42  2003.

[18] T. S. Han and S. Verd´u. Generalizing the Fano inequality. IEEE Trans. Inf. Theory  40(4):1247–1251 

1994.

[19] S. Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th

international conference on Machine learning  page 360. ACM  2007.

[20] S. Hanneke. Rates of convergence in active learning. Ann. Statist.  39(1):333–361  2011.
[21] T. Heged˝us. Generalized teaching dimensions and the query complexity of learning. In COLT ’95  pages

108–117  New York  NY  USA  1995. ACM.

[22] M. K¨a¨ari¨ainen. Active learning in the non-realizable case. In ALT  pages 63–77  2006.
[23] V. Koltchinskii. Rademacher complexities and bounding the excess risk of active learning. J. Machine

Learn. Res.  11:2457–2485  2010.

[24] P. Massart and ´E. N´ed´elec. Risk bounds for statistical learning. Ann. Statist.  34(5):2326–2366  2006.
[25] R. D. Nowak. The geometry of generalized binary search. Preprint  October 2009.
[26] D. P. Palomar and S. Verd´u. Lautum information. IEEE Trans. Inform. Theory  54(3):964–975  March

2008.

[27] Y. Yang and A. Barron.

Information-theoretic determination of minimax rates of convergence. Ann.

Statist.  27(5):1564–1599  1999.

9

,Yining Wang
Yu-Xiang Wang
Aarti Singh