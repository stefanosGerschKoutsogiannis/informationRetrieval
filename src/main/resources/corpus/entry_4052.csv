2015,Online Learning with Gaussian Payoffs and Side Observations,We consider a sequential learning problem with Gaussian payoffs and side information: after selecting an action $i$  the learner receives information about the payoff of every action $j$ in the form of Gaussian observations whose mean is the same as the mean payoff  but the variance depends on the pair $(i j)$ (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups  including the recently introduced graph-structured feedback case. For the first time in the literature  we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm  which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to  logarithmic factors).,Online Learning with Gaussian Payoffs and Side

Observations

Yifan Wu1

Andr´as Gy¨orgy2

Csaba Szepesv´ari1

1Dept. of Computing Science

University of Alberta

{ywu12 szepesva}@ualberta.ca

2Dept. of Electrical and Electronic Engineering

Imperial College London

a.gyorgy@imperial.ac.uk

Abstract

We consider a sequential learning problem with Gaussian payoffs and side ob-
servations: after selecting an action i  the learner receives information about the
payoff of every action j in the form of Gaussian observations whose mean is the
same as the mean payoff  but the variance depends on the pair (i  j) (and may be
inﬁnite). The setup allows a more reﬁned information transfer from one action to
another than previous partial monitoring setups  including the recently introduced
graph-structured feedback case. For the ﬁrst time in the literature  we provide
non-asymptotic problem-dependent lower bounds on the regret of any algorithm 
which recover existing asymptotic problem-dependent lower bounds and ﬁnite-
time minimax lower bounds available in the literature. We also provide algorithms
that achieve the problem-dependent lower bound (up to some universal constant
factor) or the minimax lower bounds (up to logarithmic factors).

1

Introduction

Online learning in stochastic environments is a sequential decision problem where in each time step
a learner chooses an action from a given ﬁnite set  observes some random feedback and receives
a random payoff. Several feedback models have been considered in the literature: The simplest is
the full information case where the learner observes the payoff of all possible actions at the end
of every round. A popular setup is the case of bandit feedback  where the learner only observes
its own payoff and receives no information about the payoff of other actions [1]. Recently  several
papers considered a more reﬁned setup  called graph-structured feedback  that interpolates between
the full-information and the bandit case: here the feedback structure is described by a (possibly
directed) graph  and choosing an action reveals the payoff of all actions that are connected to the
selected one  including the chosen action itself. This problem  motivated for example by social
networks  has been studied extensively in both the adversarial [2  3  4  5] and the stochastic cases
[6  7]. However  most algorithms presented heavily depend on the self-observability assumption 
that is  that the payoff of the selected action can be observed. Removing this self-loop assumption
leads to the so-called partial monitoring case [5]. In the absolutely general partial monitoring setup
the learner receives some general feedback that depends on its choice (and the environment)  with
some arbitrary (but known) dependence [8  9]. While the partial monitoring setup covers all other
problems  its analysis has concentrated on the ﬁnite case where both the set of actions and the set
of feedback signals are ﬁnite [8  9]  which is in contrast to the standard full information and bandit
settings where the feedback is typically assumed to be real-valued. To our knowledge there are only
a few exceptions to this case: in [5]  graph-structured feedback is considered without the self-loop
assumption  while continuous action spaces are considered in [10] and [11] with special feedback
structure (linear and censored observations  resp.).
In this paper we consider a generalization of the graph-structured feedback model that can also be
viewed as a general partial monitoring model with real-valued feedback. We assume that selecting

1

an action i the learner can observe a random variable Xij for each action j whose mean is the same
as the payoff of j  but its variance σ2
ij depends on the pair (i  j). For simplicity  throughout the paper
we assume that all the payoffs and the Xij are Gaussian. While in the graph-structured feedback
case one either has observation on an action or not  but the observation always gives the same amount
ij  the information can be
of information  our model is more reﬁned: Depending on the value of σ2
ij = ∞  trying action i gives no information about action j.
of different quality. For example  if σ2
ij < ∞  the value of the information depends on the time horizon T of the
In general  for any σ2
problem: when σ2
ij is large relative to T (and the payoff differences of the actions) essentially no
information is received  while a small variance results in useful observations.
After deﬁning the problem formally in Section 2  we provide non-asymptotic problem-dependent
lower bounds in Section 3  which depend on the distribution of the observations through their mean
payoffs and variances. To our knowledge  these are the ﬁrst such bounds presented for any stochas-
tic partial monitoring problem beyond the full-information setting: previous work either presented
asymptotic problem-dependent lower bounds (e.g.  [12  7])  or ﬁnite-time minimax bounds (e.g. 
[9  3  5]). Our bounds can recover all previous bounds up to some universal constant factors not de-
pending on the problem.
In Section 4  we present two algorithms with ﬁnite-time performance
guarantees for the case of graph-structured feedback without the self-observability assumption.
While due to their complicated forms it is hard to compare our ﬁnite-time upper and lower bounds 
we show that our ﬁrst algorithm achieves the asymptotic problem-dependent lower bound up to

problem-independent multiplicative factors. Regarding the minimax regret  the hardness ((cid:101)Θ(T 1/2)
or(cid:101)Θ(T 2/3) regret1) of partial monitoring problems is characterized by their global/local observabil-

ity property [9] or  in case of the graph-structured feedback model  by their strong/weak observabil-
ity property [5]. In the same section we present another algorithm that achieves the minimax regret
(up to logarithmic factors) under both strong and weak observability  and achieves an O(log3/2 T )
problem-dependent regret. Earlier results for the stochastic graph-structured feedback problems
[6  7] provided only asymptotic problem-dependent lower bounds and performance bounds that did
not match the asymptotic lower bounds or the minimax rate up to constant factors. A related combi-
natorial partial monitoring problem with linear feedback was considered in [10]  where the presented

algorithm was shown to satisfy both an (cid:101)O(T 2/3) minimax bound and a logarithmic problem depen-

√
dent bound. However  the dependence on the problem structure in that paper is not optimal  and  in
particular  the paper does not achieve the O(
T ) minimax bound for easy problems. Finally  we
draw conclusions and consider some interesting future directions in Section 5. Proofs can be found
in the long version of this paper [13].

2 Problem Formulation
Formally  we consider an online learning problem with Gaussian payoffs and side observations:
Suppose a learner has to choose from K actions in every round. When choosing an action  the
learner receives a random payoff and also some side observations corresponding to other actions.
More precisely  each action i ∈ [K] = {1  . . .   K} is associated with some parameter θi  and
the payoff Yt i to action i in round t is normally distributed random variable with mean θi and
variance σ2
ii  while the learner observes a K-dimensional Gaussian random vector Xt i whose jth
ij (we assume 0 ≤ σij ≤ ∞)
coordinate is a normal random variable with mean θj and variance σ2
and the coordinates of Xt i are independent of each other. We assume the following: (i) the random
variables (Xt  Yt)t are independent for all t; (ii) the parameter vector θ is unknown to the learner but
ij)i j∈[K] is known in advance; (iii) θ ∈ [0  D]K for some D > 0; (iv)
the variance matrix Σ = (σ2
mini∈[K] σij ≤ σ < ∞ for all j ∈ [K]  that is  the expected payoff of each action can be observed.
The goal of the learner is to maximize its payoff or  in other words  minimize the expected regret

θi − T(cid:88)

t=1

RT = T max
i∈[K]

E [Yt it]

where it is the action selected by the learner in round t. Note that the problem encompasses several
common feedback models considered in online learning (modulo the Gaussian assumption)  and
makes it possible to examine more delicate observation structures:

1Tilde denotes order up to logarithmic factors.

2

Full information: σij = σj < ∞ for all i  j ∈ [K].
Bandit: σii < ∞ and σij = ∞ for all i (cid:54)= j ∈ [K].
Partial monitoring with feedback graphs [5]: Each action i ∈ [K] is associated with an observa-

tion set Si ⊂ [K] such that σij = σj < ∞ if j ∈ Si and σij = ∞ otherwise.

We will call the uniform variance version of these problems when all the ﬁnite σij are equal to some
σ ≥ 0. Some interesting features of the problem can be seen when considering the generalized full
information case   when all entries of Σ are ﬁnite. In this case  the greedy algorithm  which estimates
the payoff of each action by the average of the corresponding observed samples and selects the one
with the highest average  achieves at most a constant regret for any time horizon T .2 On the other
hand  the constant can be quite large: in particular  when the variance of some observations are
large relative to the gaps dj = maxi θi − θj  the situation is rather similar to a partial monitoring
setup for a smaller  ﬁnite time horizon. In this paper we are going to analyze this problem and
present algorithms and lower bounds that are able to “interpolate” between these cases and capture
the characteristics of the different regimes.

2.1 Notation

T = {c ∈ NK : ci ≥ 0  (cid:80)

N

0  (cid:80)

i∈[K] ci = T} and let N (T ) ∈ C
N
Deﬁne C
T denote the number of
T = {c ∈ RK : ci ≥
R
plays over all actions taken by some algorithm in T rounds. Also let C
i∈[K] ci = T}. We will consider environments with different expected payoff vectors θ ∈ Θ 
but the variance matrix Σ will be ﬁxed. Therefore  an environment can be speciﬁed by θ; oftentimes 
we will explicitly denote the dependence of different quantities on θ: The probability and expectation
functionals under environment θ will be denoted by Pr (·; θ) and E [·; θ]  respectively. Furthermore 
(cid:80)
let ij(θ) be the jth best action (ties are broken arbitrarily  i.e.  θi1 ≥ θi2 ≥ ··· ≥ θiK ) and deﬁne
di(θ) = θi1(θ) − θi for any i ∈ [K]. Then the expected regret under environment θ is RT (θ) =
E [Ni(T ); θ] di(θ). For any action i ∈ [K]  let Si = {j ∈ [K] : σij < ∞} denote the set of
actions whose parameter θj is observable by choosing action i. Throughout the paper  log denotes
the natural logarithm and ∆n denotes the n-dimensional simplex for any positive integer n.

i∈[K]

3 Lower Bounds

The aim of this section is to derive generic  problem-dependent lower bounds to the regret  which
are also able to provide minimax lower bounds. The hardness in deriving such bounds is that for any
ﬁxed θ and Σ  the dumb algorithm that always selects i1(θ) achieves zero regret (obviously  the re-
gret of this algorithm is linear for any θ(cid:48) with i1(θ) (cid:54)= i1(θ(cid:48)))  so in general it is not possible to give a
lower bound for a single instance. When deriving asymptotic lower bounds  this is circumvented by
only considering consistent algorithms whose regret is sub-polynomial for any problem [12]. How-
ever  this asymptotic notion of consistency is not applicable to ﬁnite-horizon problems. Therefore 
following ideas of [14]  for any problem we create a family of related problems (by perturbing the
mean payoffs) such that if the regret of an algorithm is “too small” in one of the problems than it
will be “large” in another one  while it still depends on the original problem parameters (note that
deriving minimax bounds usually only involves perturbing certain special “worst-case” problems).
As a warm-up  and to show the reader what form of a lower bound can be expected  ﬁrst we present
an asymptotic lower bound for the uniform-variance version of the problem of partial monitoring
with feedback graphs. The result presented below is an easy consequence of [12]  hence its proof
is omitted. An algorithm is said to be consistent if supθ∈Θ RT (θ) = o(T γ) for every γ > 0. Now
assume for simplicity that there is a unique optimal action in environment θ  that is  θi1(θ) > θi for
all i (cid:54)= i1 and let

c ∈ [0 ∞)K :

(cid:88)

i:j∈Si

Cθ =

ci ≥ 2σ2
d2
j (θ)

for all j (cid:54)= i1(θ)  

ci ≥ 2σ2

d2
i2(θ)(θ)

i:i1(θ)∈Si

(cid:88)

 .

2To see this  notice that the error of identifying the optimal action decays exponentially with the number of

rounds.

3

Then  for any consistent algorithm and for any θ with θi1(θ) > θi2(θ) 

lim inf
T→∞

RT (θ)
log T

≥ inf
c∈Cθ

(cid:104)c  d(θ)(cid:105) .

(1)

Note that the right hand side of (1) is 0 for any generalized full information problem (recall that
the expected regret is bounded by a constant for such problems)  but it is a ﬁnite positive number
for other problems. Similar bounds have been provided in [6  7] for graph-structured feedback with
self-observability (under non-Gaussian assumptions on the payoffs).
In the following we derive
ﬁnite time lower bounds that are also able to replicate this result.

3.1 A General Finite Time Lower Bound
First we derive a general lower bound. For any θ  θ(cid:48) ∈ Θ and q ∈ ∆|C

N

T |  deﬁne f (θ  q  θ(cid:48)) as

(cid:88)

|

a∈C

N
T

q(cid:48)(a)(cid:104)a  d(θ(cid:48))(cid:105)

≤ (cid:88)

i∈[K]

Ii(θ  θ(cid:48))

  

q(a)ai

(cid:88)

a∈C

N
T

q(a) log

q(a)
q(cid:48)(a)

f (θ  q  θ(cid:48)) = inf
N
|C
q(cid:48)∈∆
T

such that (cid:88)
KL(Xt i(θ); Xt i(θ(cid:48))) = (cid:80)K

a∈C

N
T

where Ii(θ  θ(cid:48)) is the KL-divergence between Xt i(θ) and Xt i(θ(cid:48))  given by Ii(θ  θ(cid:48)) =
ij. Clearly  f (θ  q  θ(cid:48)) is a lower bound on RT (θ(cid:48))
for any algorithm for which the distribution of N (T ) is q. The intuition behind the allowed values
of q(cid:48) is that we want q(cid:48) to be as similar to q as the environments θ and θ(cid:48) look like for the algorithm
(through the feedback (Xt it)t). Now deﬁne

j=1(θj − θ(cid:48)

j)2/2σ2

g(θ  c) = inf
|C
q∈∆

N
T

|

sup
θ(cid:48)∈Θ

f (θ  q  θ(cid:48)) 

such that (cid:88)

a∈C

N
T

q(a)a = c ∈ C

R
T .

g(θ  c) is a lower bound of the worst-case regret of any algorithm with E [N (T ); θ] = c. Finally  for
any x > 0  deﬁne

b(θ  x) = inf

c∈Cθ x

(cid:104)c  d(θ)(cid:105)

where Cθ x = {c ∈ C

T ; g(θ  c) ≤ x}.
R

Here Cθ B contains all the possible values of E [N (T ); θ] that can be achieved by some algorithm
whose lower bound g on the worst-case regret is smaller than x. These deﬁnitions give rise to the
following theorem:
Theorem 1. Given any B > 0  for any algorithm such that supθ(cid:48)∈Θ RT (θ(cid:48)) ≤ B  we have  for any
environment θ ∈ Θ  RT (θ) ≥ b(θ  B).
Remark 2. If B is picked as the minimax value of the problem given the observation structure Σ 
the theorem states that for any minimax optimal algorithm the expected regret for a certain θ is lower
bounded by b(θ  B).

3.2 A Relaxed Lower Bound

Now we introduce a relaxed but more interpretable version of the ﬁnite-time lower bound of Theo-
rem 1  which can be shown to match the asymptotic lower bound (1). The idea of deriving the lower
bound is the following: instead of ensuring that the algorithm performs well in the most adversarial
environment θ(cid:48)  we consider a set of “bad” environments and make sure that the algorithm performs
well on them  where each “bad” environment θ(cid:48) is the most adversarial one by only perturbing one
coordinate θi of θ.
However  in order to get meaningful ﬁnite-time lower bounds  we need to perturb θ more carefully
than in the case of asymptotic lower bounds. The reason for this is that for any sub-optimal action
i  if θi is very close to θi1(θ)  then E [Ni(T ); θ] is not necessarily small for a good algorithm for
θ. If it is small  one can increase θi to obtain an environment θ(cid:48) where i is the best action and the
algorithm performs bad; otherwise  when E [Ni(T ); θ] is large  we need to decrease θi to make the

4

i − θi1(θ) arbitrarily small as in asymptotic lower-bound arguments  because when θ(cid:48)

is small  large E(cid:2)Ni1(θ); θ(cid:48)(cid:3)  and not necessarily large E [Ni(T ); θ(cid:48)]  may also lead to low ﬁnite-time

algorithm perform badly in θ(cid:48). Moreover  when perturbing θi to be better than θi1(θ)  we cannot
i − θi1(θ)
make θ(cid:48)
regret in θ(cid:48). In the following we make this argument precise to obtain an interpretable lower bound.
3.2.1 Formulation

R
T that contains the set of “reasonable” values for E [N (T ); θ].

We start with deﬁning a subset of C
For any θ ∈ Θ and B > 0  let

c ∈ C

R
T :

K(cid:88)

j=1

cj
σ2
ji

C(cid:48)
θ B =

≥ mi(θ  B) for all i ∈ [K]



where mi  the minimum sample size required to distinguish between θi and its worst-case perturba-
tion  is deﬁned as follows: For i (cid:54)= i1  if θi1 = D 3 then mi(θ  B) = 0. Otherwise let

mi +(θ  B) =

max

∈(di(θ) D−θi]

1

2 log T (−di(θ))

8B

 

mi −(θ  B) = max
∈(0 θi]

1

2 log T (+di(θ))

8B

 

and let i + and i − denote the value of  achieving the maximum in mi + and mi −  respectively.
Then  deﬁne

mi(θ  B) =

min{mi +(θ  B)  mi −(θ  B)}

if di(θ) ≥ 4B/T ;
if di(θ) < 4B/T .

(cid:26)mi +(θ  B)

For i = i1  then mi1(θ  B) = 0 if θi2(θ) = 0  else the deﬁnitions for i (cid:54)= i1 change by replacing
di(θ) with di2(θ)(θ) (and switching the + and − indices):
∈(di2(θ)(θ) θi1(θ)]

mi1(θ) −(θ  B) =

T (−di2(θ)(θ))

1
2 log

max

8B

 

mi1(θ) +(θ  B) =

max

∈(0 D−θi1(θ)]

1
2 log

T (+di2(θ)(θ))

8B

where i1(θ) − and i1(θ) + are the maximizers for  in the above expressions. Then  deﬁne
if di2(θ)(θ) ≥ 4B/T ;
if di2(θ)(θ) < 4B/T .

mi1(θ)(θ  B) =

Note that i + and i − can be expressed in closed form using the Lambert W : R → R function
satisfying W (x)eW (x) = x: for any i (cid:54)= i1(θ) 
√
D − θi   8

i + = min

/T + di(θ)

(cid:41)

√

eB

16

 

(cid:26)mi1(θ) −(θ  B)
min(cid:8)mi1(θ) +(θ  B)  mi1(θ) −(θ  B)(cid:9)
(cid:19)
(cid:18) di(θ)T
(cid:19)

(cid:40)
(cid:40)

eBe

(cid:18)

W

√
θi   8

W

eBe

√

− di(θ)T
eB

16

/T − di(θ)

(2)

 

i − = min

(cid:41)

and similar results hold for i = i1  as well.
Now we can give the main result of this section  a simpliﬁed version of Theorem 1:
Corollary 3. Given B > 0  for any algorithm such that supλ∈Θ RT (λ) ≤ B  we have  for any
environment θ ∈ Θ  RT (θ) ≥ b(cid:48)(θ  B) = minc∈C(cid:48)

(cid:104)c  d(θ)(cid:105).

θ B

Next we compare this bound to existing lower bounds.

3.2.2 Comparison to the Asymptotic Lower Bound (1)

Now we will show that our ﬁnite time lower bound in Corollary 3 matches the asymptotic lower
bound in (1) up to some constants. Pick B = αT β for some α > 0 and 0 < β < 1. For sim-
plicity  we only consider θ which is “away from” the boundary of Θ (so that the minima in (2) are

3Recall that θi ∈ [0  D].

5

di(θ)

2W (di(θ)T 1−β /(16α

achieved by the second terms) and has a unique optimal action. Then  for i (cid:54)= i1(θ)  it is easy
log T (i +−di(θ))
to show that i + =
for large enough T . Then  using the fact that log x − log log x ≤ W (x) ≤ log x for x ≥ e 
it follows that limT→∞ mi(θ  B)/ log T = (1 − β)/d2
i (θ)  and similarly we can show that
limT→∞ mi1(θ)(θ  B)/ log T = (1 − β)/d2
Cθ  under the as-
sumptions of (1)  as T → ∞. This implies that Corollary 3 matches the asymptotic lower bound of
(1) up to a factor of (1 − β)/2.

e)) + di(θ) by (2) and mi(θ  B) = 1
2
i +

i2(θ)(θ). Thus  C(cid:48)

θ B → (1−β) log T

√

8B

2

3.2.3 Comparison to Minimax Bounds

Now we will show that our θ-dependent ﬁnite-time lower bound reproduces the minimax regret
bounds of [2] and [5]  except for the generalized full information case.
The minimax bounds depend on the following notion of observability: An action i is strongly ob-
servable if either i ∈ Si or [K] \ {i} ⊂ {j : i ∈ Sj}. i is weakly observable if it is not strongly
observable but there exists j such that i ∈ Sj (note that we already assumed the latter condition for
all i). Let W(Σ) be the set of all weakly observable actions. Σ is said to be strongly observable if
W(Σ) = ∅. Σ is weakly observable if W(Σ) (cid:54)= ∅.
Next we will deﬁne two key qualities introduced by [2] and [5] that characterize the hardness of a
problem instance with feedback structure Σ: A set A ⊂ [K] is called an independent set if for any
i ∈ A  Si ∩ A ⊂ {i}. The independence number κ(Σ) is deﬁned as the cardinality of the largest
independent set. For any pair of subsets A  A(cid:48) ⊂ [K]  A is said to be dominating A(cid:48) if for any j ∈ A(cid:48)
there exists i ∈ A such that j ∈ Si. The weak domination number ρ(Σ) is deﬁned as the cardinality
of the smallest set that dominates W(Σ).
Corollary 4. Assume that σij = ∞ for some i  j ∈ [K]  that is  we are not in the generalized full
information case. Then 

(i) if Σ is strongly observable  with B = ασ(cid:112)κ(Σ)T for some α > 0  we have

√

supθ∈Θ b(cid:48)(θ  B) ≥ σ

κ(Σ)T

64eα

for T ≥ 64e2α2σ2κ(Σ)3/D2.

(ii) If Σ is weakly observable  with B = α(ρ(Σ)D)1/3(σT )2/3 log

have supθ∈Θ b(cid:48)(θ  B) ≥ (ρ(Σ)D)1/3(σT )2/3 log−2/3 K

.

51200e2α2
√
Remark 5. In Corollary 4  picking α = 1
73 for weakly
8
observable Σ gives formal minimax lower bounds: (i) If Σ is strongly observable  for any algorithm
we have supθ∈Θ RT (θ) ≥ σ
for T ≥ eσ2κ(Σ)3/D2. (ii) If Σ is weakly observable  for any
algorithm we have supθ∈Θ RT (θ) ≥ (ρ(Σ)D)1/3(σT )2/3

e for strongly observable Σ and α = 1

√
κ(Σ)T
8

√

.

e

73 log2/3 K

−2/3 K for some α > 0  we

4 Algorithms

In this section we present two algorithms and their ﬁnite-time analysis for the uniform variance
version of our problem (where σij is either σ or ∞). The upper bound for the ﬁrst algorithm matches
the asymptotic lower bound in (1) up to constants. The second algorithm achieves the minimax lower
bounds of Corollary 4 up to logarithmic factors  as well as O(log3/2 T ) problem-dependent regret.
In the problem-dependent upper bounds of both algorithms  we assume that the optimal action is
unique  that is  di2(θ)(θ) > 0.

4.1 An Asymptotically Optimal Algorithm

ni(t) =(cid:80)t−1
empirical estimate of θi based on the ﬁrst ni(t) observations. Let Ni(t) =(cid:80)t−1

(cid:104)c  d(θ)(cid:105); note that increasing ci1(θ)(θ) does not change the value of
Let c(θ) = argminc∈Cθ
(cid:104)c  d(θ)(cid:105) (since di1(θ)(θ) = 0)  so we take the minimum value of ci1(θ)(θ) in this deﬁnition. Let
I{i ∈ Sis} be the number of observations for action i before round t and ˆθt i be the
I{is = i} be the
number of plays for action i before round t. Note that this deﬁnition of Ni(t) is different from that
in the previous sections since it excludes round t.

s=1

s=1

6

else

then

4α log t ∈ Cˆθt
if N (t)
Play it = i1(ˆθt).
Set ne(t + 1) = ne(t).

Algorithm 1
1: Inputs: Σ  α  β : N → [0 ∞).
2: For t = 1  ...  K  observe each action i at least

once by playing it such that t ∈ Sit.
3: Set exploration count ne(K + 1) = 0.
4: for t = K + 1  K + 2  ... do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
end if
15:
16: end for
Theorem 6. For any θ ∈ Θ   > 0  α > 2 and any non-decreasing β(n) that satisﬁes 0 ≤ β(n) ≤
n/2 and β(m + n) ≤ β(m) + β(n) for m  n ∈ N 

Our ﬁrst algorithm is presented in Algo-
rithm 1. The main idea  coming from
[15]  is that by forcing exploration over
all actions  the solution c(θ) of the lin-
ear program can be well approximated
while paying a constant price. This solves
the main difﬁculty that  without getting
enough observations on each action  we
may not have good enough estimates for
d(θ) and c(θ). One advantage of our algo-
rithm compared to that of [15] is that we
use a nondecreasing  sublinear exploration
schedule β(n) (β : N → [0 ∞)) instead
of a constant rate β(n) = βn. This re-
solves the problem that  to achieve asymp-
totically optimal performance  some pa-
rameter of the algorithm needs to be cho-
sen according to dmin(θ) as in [15]. The
expected regret of Algorithm 1 is upper
bounded as follows:

Play it such that argmini∈[K] ni(t) ∈ Sit.
Play it such that Ni(t) < ci(ˆθt)4α log t.

end if
Set ne(t + 1) = ne(t) + 1.

if mini∈[K] ni(t) < β(ne(t))/K then

RT (θ) ≤(cid:0)2K + 2 + 4K/(α − 2)(cid:1)dmax(θ) + 4Kdmax(θ)

(cid:16) − β(s)2

T(cid:88)

(cid:17)

else

(cid:16)

(cid:88)

(cid:17)
j − θj| ≤  for all j ∈ [K]}.

ci(θ  ) + K

i∈[K]

2Kσ2

s=0

exp

(cid:88)

i∈[K]

+ 2dmax(θ)β

4α log T

+ 4α log T

ci(θ  )di(θ) .

where ci(θ  ) = sup{ci(θ(cid:48)) : |θ(cid:48)
Further specifying β(n) and using the continuity of c(θ) around θ  it immediately follows that Al-
gorithm 1 achieves asymptotically optimal performance:
Corollary 7. Suppose the conditions of Theorem 6 hold. Assume  furthermore  that β(n) satisﬁes
< ∞ for any  > 0  then for any θ such that c(θ) is unique 

β(n) = o(n) and(cid:80)∞

(cid:16)− β(s)2

(cid:17)

s=0 exp

2Kσ2

lim sup
T→∞

RT (θ)/ log T ≤ 4α inf
c∈C(θ)

(cid:104)c  d(θ)(cid:105) .

Note that any β(n) = anb with a ∈ (0  1
2 ]  b ∈ (0  1) satisﬁes the requirements in Theorem 6 and
Corollary 7. Also note that the algorithms presented in [6  7] do not achieve this asymptotic bound.

j:i∈Sj

j:i∈Sj

4.2 A Minimax Optimal Algorithm

For any A  A(cid:48) ⊂ [K] 

c(A  A(cid:48)) = argmaxc∈∆|A| mini∈A(cid:48)(cid:80)
mini∈A(cid:48)(cid:80)

(cid:113) 2 log(8K2r3/δ)

Next we present an algorithm achieving the minimax bounds.

where ni(r) =(cid:80)r−1

let
cj (ties are broken arbitrarily) and m(A  A(cid:48)) =
cj(A  A(cid:48)). For any A ⊂ [K] and |A| ≥ 2  let AS = {i ∈ A : ∃j ∈ A  i ∈ Sj}
s=1 is i and ˆθr i

and AW = A− AS. Furthermore  let gr i(δ) = σ
be the empirical estimate of θi based on ﬁrst ni(r) observations (i.e.  the average of the samples).
The algorithm is presented in Algorithm 2. It follows a successive elimination process: it explores all
possibly optimal actions (called “good actions” later) based on some conﬁdence intervals until only
one action remains. While doing exploration  the algorithm ﬁrst tries to explore the good actions
by only using good ones. However  due to weak observability  some good actions might have to be
explored by actions that have already been eliminated. To control this exploration-exploitation trade
off  we use a sublinear function γ to control the exploration of weakly observable actions.
In the following we present high-probability bounds on the performance of the algorithm  so  with a
slight abuse of notation  RT (θ) will denote the regret without expectation in the rest of this section.

ni(r)

7

Algorithm 2
1: Inputs: Σ  δ.
2: Set t1 = 0  A1 = [K].
3: for r = 1  2  ... do
4:

r

r

else

s (cid:54)=∅ m([K]   AW

ni(r) < mini∈AS

Let αr = min1≤s≤r AW
s = ∅ for all 1 ≤ s ≤ r.)
AW
(cid:54)= ∅ and mini∈AW
if AW
Set cr = c([K]   AW
r ).
Set cr = c(Ar  AS
r ).

5:
6:
7:
8:
end if
9:
Play ir = (cid:100)cr · (cid:107)cr(cid:107)0(cid:101) and set tr+1 ← tr + (cid:107)ir(cid:107)1.
10:
Ar+1 ← {i ∈ Ar : ˆθr+1 i + gr+1 i(δ) ≥ maxj∈Ar
11:
if |Ar+1| = 1 then
12:
13:
end if
14:
15: end for
Theorem 8. For any δ ∈ (0  1) and any θ ∈ Θ 

Play the only action in the remaining rounds.

s ) and γ(r) = (σαrtr/D)2/3. (Deﬁne αr = 1 if

ni(r) and mini∈AW

r

r

ni(r) < γ(r) then

ˆθr+1 j − gr+1 j(δ)}.

RT (θ) ≤ (ρ(Σ)D)1/3(σT )2/3 · 7(cid:112)6 log(2KT /δ) + 125σ2K 3/D + 13K 3D

with probability at least 1 − δ if Σ is weakly observable  while

(cid:114)

RT (θ) ≤ 2KD + 80σ

κ(Σ)T · 6 log K log

2KT

δ

with probability at least 1 − δ if Σ is strongly observable.
Theorem 9 (Problem-dependent upper bound). For any δ ∈ (0  1) and any θ ∈ Θ such that the
optimal action is unique  with probability at least 1 − δ 

RT (θ) ≤ 1603ρ(Σ)Dσ2

+ 15(cid:0)ρ(Σ)Dσ2(cid:1)1/3(cid:0)125σ2/D2 + 10(cid:1) K 2 (log(2KT /δ))1/2 .

(log(2KT /δ))3/2 + 14K 3D + 125σ2K 3/D

d2
min(θ)

Remark 10. Picking δ = 1/T gives an O(log3/2 T ) upper bound on the expected regret.
Remark 11. Note that Algortihm 2 is similar to the UCB-LP algorithm of [7]  which admits a bet-
ter problem-dependent upper bound (although does not achieve it with optimal problem-dependent
constants)  but it does not achieve the minimax bound even under strong observability.

5 Conclusions and Open Problems

We considered a novel partial-monitoring setup with Gaussian side observations  which generalizes
the recently introduced setting of graph-structured feedback  allowing ﬁner quantiﬁcation of the
observed information from one action to another. We provided non-asymptotic problem-dependent
lower bounds that imply existing asymptotic problem-dependent and non-asymptotic minimax lower
bounds (up to some constant factors) beyond the full information case. We also provided an algo-
rithm that achieves the asymptotic problem-dependent lower bound (up to some universal constants)
and another algorithm that achieves the minimax bounds under both weak and strong observability.
However  we think this is just the beginning. For example  we currently have no algorithm that
achieves both the problem dependent and the minimax lower bounds at the same time. Also  our
upper bounds only correspond to the graph-structured feedback case. It is of great interest to go
beyond the weak/strong observability in characterizing the hardness of the problem  and provide
algorithms that can adapt to any correspondence between the mean payoffs and the variances (the
hardness is that one needs to identify suboptimal actions with good information/cost trade-off).

Acknowledgments This work was supported by the Alberta Innovates Technology Futures
through the Alberta Ingenuity Centre for Machine Learning (AICML) and NSERC. During this
work  A. Gy¨orgy was with the Department of Computing Science  University of Alberta.

8

References
[1] S´ebatien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochas-
tic multi-armed bandit problems. Foundations and Trends in Machine Learning  5(1):1–122 
2012.

[2] Shie Mannor and Ohad Shamir. From bandits to experts: on the value of side-observations. In

Advances in Neural Information Processing Systems 24 (NIPS)  pages 684–692  2011.

[3] Noga Alon  Nicolo Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. From bandits to ex-
perts: A tale of domination and independence. In Advances in Neural Information Processing
Systems 26 (NIPS)  pages 1610–1618  2013.

[4] Tom´aˇs Koc´ak  Gergely Neu  Michal Valko  and R´emi Munos. Efﬁcient learning by implicit
In Advances in Neural Information

exploration in bandit problems with side observations.
Processing Systems 27 (NIPS)  pages 613–621  2014.

[5] Noga Alon  Nicol`o Cesa-Bianchi  Ofer Dekel  and Tomer Koren. Online learning with feed-
In Proceedings of The 28th Conference on Learning Theory

back graphs: beyond bandits.
(COLT)  pages 23–35  2015.

[6] St´ephane Caron  Branislav Kveton  Marc Lelarge  and Smriti Bhagat. Leveraging side observa-
tions in stochastic bandits. In Proceedings of the 28th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI)  pages 142–151  2012.

[7] Swapna Buccapatnam  Atilla Eryilmaz  and Ness B. Shroff. Stochastic bandits with side ob-

servations on networks. SIGMETRICS Perform. Eval. Rev.  42(1):289–300  June 2014.

[8] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  Learning  and Games. Cambridge Uni-

versity Press  Cambridge  2006.

[9] G´abor Bart´ok  Dean P. Foster  D´avid P´al  Alexander Rakhlin  and Csaba Szepesv´ari. Par-
tial monitoring – classiﬁcation  regret bounds  and algorithms. Mathematics of Operations
Research  39:967–997  2014.

[10] Tian Lin  Bruno Abrahao  Robert Kleinberg  John Lui  and Wei Chen. Combinatorial par-
In Proceedings of the 31st

tial monitoring game with linear feedback and its applications.
International Conference on Machine Learning (ICML)  pages 901–909  2014.

[11] Tor Lattimore  Andr´as Gy¨orgy  and Csaba Szepesv´ari. On learning the optimal waiting time.
In Peter Auer  Alexander Clark  Thomas Zeugmann  and Sandra Zilles  editors  Algorith-
mic Learning Theory  volume 8776 of Lecture Notes in Computer Science  pages 200–214.
Springer International Publishing  2014.

[12] Todd L. Graves and Tze Leung Lai. Asymptotically efﬁcient adaptive choice of control laws
incontrolled markov chains. SIAM Journal on Control and Optimization  35(3):715–743  1997.
[13] Yifan Wu  Andr´as Gy¨orgy  and Csaba Szepesv´ari. Online learning with Gaussian payoffs and

side observations. arXiv preprint arXiv:1510.08108  2015.

[14] Lihong Li  R´emi Munos  and Csaba Szepesv´ari. Toward minimax off-policy value estima-
tion. In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS)  pages 608–616  2015.

[15] Stefan Magureanu  Richard Combes  and Alexandre Proutiere. Lipschitz bandits: Regret lower
bounds and optimal algorithms. In Proceedings of The 27th Conference on Learning Theory
(COLT)  pages 975–999  2014.

[16] Emilie Kaufmann  Olivier Capp´e  and Aur´elien Garivier. On the complexity of best arm iden-
tiﬁcation in multi-armed bandit models. The Journal of Machine Learning Research  2015. (to
appear).

[17] Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and op-
timal algorithms. In Proceedings of the 31st International Conference on Machine Learning
(ICML)  pages 521–529  2014.

9

,Yifan Wu
András György
Csaba Szepesvari
Prateek Jain
Nikhil Rao
Inderjit Dhillon