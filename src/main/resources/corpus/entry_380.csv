2017,Adaptive Accelerated Gradient Converging Method under H\"{o}lderian Error Bound Condition,Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity  namely a quadratic growth condition (QGC). However  the faster convergence of restarting APG method relies on the potentially unknown constant  in QGC to appropriately restart APG  which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods  i.e.  leveraging  the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC  namely the H\"{o}lderian error bound (HEB) condition.   {\it The key technique} for our development is a novel synthesis of  {\it adaptive regularization and a conditional restarting scheme}  which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore  we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic  PG's convergence speed is essentially $o(\frac{1}{t})$  where $t$ is the total number of iterations; (ii) if the objective function consists of an $\ell_1$  $\ell_\infty$  $\ell_{1 \infty}$  or huber  norm regularization and a convex smooth piecewise quadratic loss (e.g.  square loss  squared hinge loss and huber loss)  the proposed algorithm is parameter-free and enjoys a {\it faster linear convergence} than PG without any other assumptions  (e.g.  restricted eigen-value condition).   It is notable that  our linear convergence results for the aforementioned problems  are global instead of local.  To the best of our knowledge  these improved results are first shown in this work.,Adaptive Accelerated Gradient Converging Method

under Hölderian Error Bound Condition

Mingrui Liu  Tianbao Yang

Department of Computer Science

The University of Iowa  Iowa City  IA 52242
mingrui-liu  tianbao-yang@uiowa.edu

Abstract

Recent studies have shown that proximal gradient (PG) method and accelerated
gradient method (APG) with restarting can enjoy a linear convergence under a
weaker condition than strong convexity  namely a quadratic growth condition
(QGC). However  the faster convergence of restarting APG method relies on
the potentially unknown constant in QGC to appropriately restart APG  which
restricts its applicability. We address this issue by developing a novel adaptive
gradient converging methods  i.e.  leveraging the magnitude of proximal gradient
as a criterion for restart and termination. Our analysis extends to a much more
general condition beyond the QGC  namely the Hölderian error bound (HEB)
condition. The key technique for our development is a novel synthesis of adaptive
regularization and a conditional restarting scheme  which extends previous work
focusing on strongly convex problems to a much broader family of problems.
Furthermore  we demonstrate that our results have important implication and
applications in machine learning: (i) if the objective function is coercive and semi-
algebraic  PG’s convergence speed is essentially o( 1
t )  where t is the total number
of iterations; (ii) if the objective function consists of an (cid:96)1  (cid:96)∞  (cid:96)1 ∞  or huber
norm regularization and a convex smooth piecewise quadratic loss (e.g.  square
loss  squared hinge loss and huber loss)  the proposed algorithm is parameter-free
and enjoys a faster linear convergence than PG without any other assumptions
(e.g.  restricted eigen-value condition). It is notable that our linear convergence
results for the aforementioned problems are global instead of local. To the best of
our knowledge  these improved results are ﬁrst shown in this work.

1

Introduction

We consider the following smooth composite optimization:

F (x) (cid:44) f (x) + g(x) 

min
x∈Rd

(1)

where g(x) is a proper lower semi-continuous convex function and f (x) is a continuously dif-
ferentiable convex function  whose gradient is L-Lipschitz continuous. The above problem has
been studied extensively in literature and many algorithms have been developed with convergence
guarantee. In particular  by employing the proximal mapping associated with g(x)  i.e. 

Pηg(u) = arg min
x∈Rd

(cid:107)x − u(cid:107)2

2 + ηg(x) 

1
2

(2)

proximal gradient (PG) and accelerated proximal gradient (APG) methods have been developed
) 1 iteration complexities for ﬁnding an -optimal solution.
for solving (1) with O(1/) and O(1/

√

1For the moment  we neglect the constant factor.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Table 1: Summary of iteration complexities in this work under the HEB condition with θ ∈ (0  1/2] 

where G(x) denotes the proximal gradient  C(1/α) = max(1/α  log(1/)) and (cid:101)O(·) suppresses
F (x) − F∗ ≤  O(cid:0)c2LC(cid:0)

a logarithmic term. If θ > 1/2  all algorithms can converge with ﬁnite steps of proximal mapping.
rAPG stands for restarting APG. ∗ mark results available for certain subclasses of problems.

LC(cid:16) 1

rAPG
√
c

(cid:17)(cid:17)

adaAGC

(cid:1)(cid:1)

algo.

(cid:16)

PG

O

1/2−θ

(cid:18)

(cid:19)(cid:19)

1

2(1−θ)

√
LC

c

1

1−2θ
2(1−θ)



*

(cid:18)

(cid:101)O

Yes
No

1−2θ

1

(cid:18)

(cid:19)(cid:19)

1

1−θ LC

c

1
1−2θ
1−θ



(cid:18)

O

No
No

–

Yes
Yes

(cid:107)G(x)(cid:107)2 ≤ 

requires θ
requires c

When either f (x) or g(x) is strongly convex  both PG and APG can enjoy a linear convergence  i.e. 
the iteration complexity is improved to be O(log(1/)).
Recently  a wave of studies try to generalize the linear convergence to problems without strong
convexity but under certain structured condition of the objective function or more generally a
quadratic growth condition [8  32  21  23  7  31  3  15  9  29  4  24  26  25]. Earlier work along the
line dates back to [12  13  14]. An example of the structured condition is such that f (x) = h(Ax)
where h(·) is strongly convex function and ∇h(x) is Lipschitz continuous on any compact set  and
g(x) is a polyhedral function. Under such a structured condition  a local error bound condition can
be established [12  13  14]  which renders an asymptotic (local) linear convergence for the proximal
gradient method. A quadratic growth condition (QGC) prescribes that the objective function satisﬁes
for any x ∈ Rd 2: α
2 ≤ F (x) − F (x∗)  where x∗ denotes a closest point to x in the
optimal set. Under such a quadratic growth condition  several recent studies have established the
linear convergence of PG  APG and many other algorithms (e.g.  coordinate descent methods) [3 
15  4  9  29]. A notable result is that PG enjoys an iteration complexity of O( L
α log(1/)) without
knowing the value of α  while a restarting version of APG studied in [15] enjoys an improved iteration
complexity of O(
α log(1/)) hinging on the value of α to appropriately restart APG periodically.
Other equivalent conditions or more restricted conditions are also considered in several studies to
show the linear convergence of (proximal) gradient method and other methods [9  15  29  30].
In this paper  we extend this line of work to a more general error bound condition  i.e.  the Hölderian
error bound (HEB) condition on a compact sublevel set Sξ = {x ∈ Rd : F (x) − F (x∗) ≤ ξ}: there
exists θ ∈ (0  1] and 0 < c < ∞ such that

2 (cid:107)x − x∗(cid:107)2
(cid:113) L

Note that when θ = 1/2 and c =(cid:112)1/α  the HEB reduces to the QGC. In the sequel  we will refer to

(cid:107)x − x∗(cid:107)2 ≤ c(F (x) − F (x∗))θ  ∀x ∈ Sξ.

(3)

C = Lc2 as condition number of the problem. It is worth mentioning that Bolte et al. [3] considered
the same condition or an equivalent Kurdyka - Łojasiewicz inequality but they only focused on
descent methods that bear a sufﬁcient decrease condition for each update consequentially excluding
APG. In addition  they do not provide explicit iteration complexity under the general HEB condition.
As a warm-up and motivation  we will ﬁrst present a straightforward analysis to show that PG
is automatically adaptive and APG can be made adaptive to the HEB by restarting.
In par-
ticular if F (x) satisﬁes a HEB condition on the initial sublevel set  PG has an iteration com-
 ))) 3  and restarting APG enjoys an iteration complexity of
plexity of O(max( C
 ))) for the convergence of objective value  where C = Lc2 is the condition
O(max(
number. These two results resemble but generalize recent works that establish linear convergence of
PG and restarting APG under the QGC - a special case of HEB. Although enjoying faster convergence 
restarting APG has a critical caveat: it requires the knowledge of constant c in HEB to restart APG 
which is usually difﬁcult to compute or estimate. In this paper  we make nontrivial contributions to

1−2θ   C log( 1

√
1/2−θ  

C log( 1

√

C

2It can be relaxed to a ﬁxed domain as done in this work.
3When θ > 1/2  all algorithms can converge in ﬁnite steps.

2

obtain faster convergence of the proximal gradient’s norm under the HEB condition by developing an
adaptive accelerated gradient converging method.
The main results of this paper are summarized in Table 1. The contributions of this paper are: (i)
we extend the analysis of PG and restarting APG under the quadratic growth condition to more
general HEB condition  and establish the adaptive iteration complexities of both algorithms; (ii)
to enjoy faster convergence of restarting APG and to eliminate the algorithmic dependence on
the unknown parameter c  we propose and analyze an adaptive accelerated gradient converging
(adaAGC) method. The developed algorithms and theory have important implication and applications
in machine learning. Firstly  if the considered objective function is also coercive and semi-algebraic
(e.g.  a norm regularized problem in machine learning with a semi-algebraic loss function)  then PG’s
convergence speed is essentially o(1/t) instead of O(1/t)  where t is the total number of iterations.
Secondly  for solving (cid:96)1  (cid:96)∞ or (cid:96)1 ∞ regularized smooth loss minimization problems including
least-squares loss  squared hinge loss and huber loss  the proposed adaAGC method enjoys a linear
convergence and a square root dependence on the “condition" number. In contrast to previous work 
the proposed algorithm is parameter free and does not rely on any restricted conditions (e.g.  the
restricted eigen-value conditions).

0≤|α|≤r λαxα  where λα ∈ R and xα = xα1

r ∈ N such that h(x) = (cid:80)
|α| =(cid:80)d

2 Notations and Preliminaries
In this section  we present some notations and preliminaries. In the sequel  we let (cid:107)·(cid:107)p (p ≥ 1) denote
the p-norm of a vector. A function g(x) : Rd → (−∞ ∞] is a proper function if g(x) < +∞ for at
least one x. g(x) is lower semi-continuous at a point x0 if lim inf x→x0 g(x) = g(x0). A function
F (x) is coercive if and only if F (x) → ∞ as (cid:107)x(cid:107)2 → ∞. We will also refer to semi-algebraic set and
semi-algebraic function several times in the paper  which are standard concepts in mathematics [2].
Due to limit of space  we present the deﬁnitions in the supplement.
Denote by N the set of all positive integers. A function h(x) is a real polynomial if there exists
d   αj ∈ N ∪ {0} 
j=1 αj and r is referred to as the degree of h(x). A continuous function f (x) is said to be a
piecewise convex polynomial if there exist ﬁnitely many polyhedra P1  . . .   Pk with ∪k
j=1Pj = Rn
such that the restriction of f on each Pj is a convex polynomial. Let fj be the restriction of f on Pj.
The degree of a piecewise convex polynomial function f denoted by deg(f ) is the maximum of the
degree of each fj. If deg(f ) = 2  the function is referred to as a piecewise convex quadratic function.
Note that a piecewise convex polynomial function is not necessarily a convex function [10].
A function f (x) is L-smooth w.r.t (cid:107) · (cid:107)2 if it is differentiable and has a Lipschitz continuous gradient
with the Lipschitz constant L  i.e.  (cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ L(cid:107)x − y(cid:107)2 ∀x  y. Let ∂g(x) denote the
subdifferential of g at x. Denote by (cid:107)∂g(x)(cid:107)2 = minu∈∂g(x) (cid:107)u(cid:107)2. A function g(x) is α-strongly
convex w.r.t (cid:107) · (cid:107)2 if it satisﬁes for any u ∈ ∂g(y) such that g(x) ≥ g(y) + u(cid:62)(x − y) + α
2 (cid:107)x −
y(cid:107)2
Denote by η > 0 a positive scalar  and let Pηg be the proximal mapping associated with ηg(·) deﬁned
in (2). Given an objective function F (x) = f (x) + g(x)  where f (x) is L-smooth and convex  g(x)
is a simple non-smooth function which is closed and convex  deﬁne a proximal gradient Gη(x) as:

2 ∀x  y.

1 . . . xαd

Gη(x) =

1
η

(x − x+

η )  where x+

η = Pηg(x − η∇f (x)).

When g(x) = 0  we have Gη(x) = ∇f (x)  i.e.  the proximal gradient is the gradient. It is known that
x is an optimal solution iff Gη(x) = 0. If η = 1/L  for simplicity we denote by G(x) = G1/L(x)
and x+ = Pg/L(x − ∇f (x)/L). Let F∗ denote the optimal objective value to minx∈Rd F (x) and
Ω∗ denote the optimal set. Denote by Sξ = {x : F (x) − F∗ ≤ ξ} the ξ-sublevel set of F (x). Let
D(x  Ω) = miny∈Ω (cid:107)x − y(cid:107)2.
The proximal gradient (PG) method solves the problem (1) by the update

(4)
with η ≤ 1/L starting from some initial solution x1 ∈ Rd. It can be shown that PG has an iteration
complexity of O( LD(x1 Ω∗)2
). Nevertheless  accelerated proximal gradient (APG) converges faster
than PG. There are many variants of APG in literature [22] including the well-known FISTA [1]. The



xt+1 = Pηg(xt − η∇f (xt)) 

3

Algorithm 1: ADG
x0 ∈ Ω  A0 = 0  v0 = x0
for t = 0  . . .   T do

At+a = 2 1+αAt

L

Find at+1 from quadratic equation a2
Set At+1 = At + at+1
xt + at+1
Set yt = At
At+1
At+1
Compute xt+1 = Pg/L(yt − ∇f (yt)/L)
Compute vt+1 = arg minx

vt

(cid:80)t+1
τ =1 aτ∇f (xτ )(cid:62)x + At+1g(x) + 1

2(cid:107)x − x0(cid:107)2

2

simplest variant adopts the following update

yt = xt + βt(xt − xt−1)  xt+1 = Pηg(yt − η∇f (yt)) 

√
√
LD(x1 Ω∗)
√
L−√
√
√

where η ≤ 1/L and βt is an appropriate sequence (e.g. βt = t−1
t+2 ). APG enjoys an iteration
) [22]. Furthermore  if f (x) is both L-smooth and α-strongly convex 
complexity of O(
one can set βt =
and deduce a linear convergence [16  11] with a better dependence on the
condition number than that of PG. If g(x) is α-strongly convex and f (x) is L-smooth  Nesterov [17]
proposed a different variant based on dual averaging  which is referred to accelerated dual gradient
(ADG) method and will be useful for our development. The key steps are presented in Algorithm 1.


α
α

L+

2.1 Hölderian error bound (HEB) condition

Deﬁnition 1 (Hölderian error bound (HEB)). A function F (x) is said to satisfy a HEB condition on
the ξ-sublevel set if there exist θ ∈ (0  1] and 0 < c < ∞ such that for any x ∈ Sξ

dist(x  Ω∗) ≤ c(F (x) − F∗)θ.

(5)

The HEB condition is closely related to the Łojasiewicz inequality or more generally Kurdyka-
Łojasiewicz (KL) inequality in real algebraic geometry. It has been shown that when functions are
semi-algebraic and continuous  the above inequality is known to hold on any compact set [3]. We
refer the readers to [3] for more discussions on HEB and KL inequalities.
In the remainder of this section  we will review some previous results to demonstrate that HEB is a
generic condition that holds for a broad family of problems of interest. The following proposition
states that any proper  coercive  convex  lower-semicontinuous and semi-algebraic functions satisfy
the HEB condition.
Proposition 1. [3] Let F (x) be a proper  coercive  convex  lower semicontinuous and semi-algebraic
function. Then there exists θ ∈ (0  1] and 0 < c < ∞ such that F (x) satisﬁes the HEB on any
ξ-sublevel set.

Example: Most optimization problems in machine learning with an objective that consists of an
empirical loss that is semi-algebraic (e.g.  hinge loss  squared hinge loss  absolute loss  square loss)
and a norm regularization (cid:107) · (cid:107)p (p ≥ 1 is a rational) or a norm constraint are proper  coercive  lower
semicontinuous and semi-algebraic functions.
Next two propositions exhibit the value θ for piecewise convex quadratic functions and piecewise
convex polynomial functions.
Proposition 2. [10] Let F (x) be a piecewise convex quadratic function on Rd. Suppose F (x) is
convex. Then for any ξ > 0  there exists 0 < c < ∞ such that D(x  Ω∗) ≤ c(F (x) − F∗)1/2 ∀x ∈
Sξ.
Many problems in machine learning are piecewise convex quadratic functions  which will be discussed
more in Section 5.
Proposition 3. [10] Let F (x) be a piecewise convex polynomial function on Rd. Suppose
F (x) is convex. Then for any ξ > 0  there exists c > 0 such that D(x  Ω∗) ≤ c(F (x) −
F∗)

(deg(F )−1)d+1  ∀x ∈ Sξ.

1

4

Algorithm 2: restarting APG (rAPG)
Input: the number of stages K and x0 ∈ Ω
for k = 1  . . .   K do

1 = xk−1 and xk

Set yk
for τ = 1  . . .   tk do

1 = xk−1

Update xk
Update yk

τ +1 = Pg/L(yk
τ +1 + τ
τ +1 = xk
tk+1 and update tk

Let xk = xk

τ − ∇f (yk
τ +3 (xk

τ )/L)
τ +1 − xk
τ )

Output: xK
Indeed  for a polyhedral constrained convex polynomial  we can have a tighter result  as shown below.
Proposition 4. [27] Let F (x) be a convex polynomial function on Rd with degree m. If P ⊂ Rd is
a polyhedral set  then the problem minx∈P F (x) admits a global error bound: ∀x ∈ P there exists
0 < c < ∞ such that

D(x  Ω∗) ≤ c

(F (x) − F∗) + (F (x) − F∗)

1
m

.

(6)

(cid:104)

(cid:105)

From the global error bound (6)  one can easily derive the HEB condition (3). As an example  an (cid:96)1
constrained (cid:96)p norm regression below [19] satisﬁes the HEB condition (3) with θ = 1
p:

min
(cid:107)x(cid:107)1≤s

F (x) (cid:44) 1
n

i x − bi)p 
(a(cid:62)

p ∈ 2N.

(7)

n(cid:88)

i=1

h(u) =(cid:80)

Many previous papers have considered a family of structured smooth composite functions F (x) =
h(Ax) + g(x)  where g(x) is a polyhedral function and h(·) is a smooth and strongly convex function
on any compact set. Suppose the optimal set of the above problem is non-empty and compact (e.g. 
the function is coercive) so is the sublevel set Sξ  and it can been shown that such a function satisﬁes
HEB with θ = 1/2 on any sublevel set Sξ [15  Theorem 10]. Examples of h(u) include logistic loss

i log(1 + exp(−ui)) and square loss h(u) = (cid:107)u(cid:107)2
2.
2(cid:107)x(cid:107)2

Finally  we note that there exist problems that admit HEB with θ > 1/2. A trivial example is given by
p with p ∈ [1  2)  which satisﬁes HEB with θ = 1/p ∈ (1/2  1]. An interesting
F (x) = 1
non-trivial family of problems is that f (x) = 0 and g(x) is a piece-wise linear functions according
to Proposition 3. PG or APG applied to such family of problems is closely related to proximal point
algorithm [20]. Explorations of such algorithmic connection is not the focus of this paper.

2 +(cid:107)x(cid:107)p

3 PG and restarting APG under HEB

As a warm-up and motivation of the major contribution presented in next section  we present a
convergence result of PG and a restarting APG under the HEB condition. The analysis is mostly
straightforward and is included in the supplement. We ﬁrst present a result of PG using the update (4).
Theorem 1. Suppose F (x0) − F∗ ≤ 0 and F (x) satisﬁes HEB on S0. The iteration complexity
of PG with option I (which returns the last solution  see the supplementary material) for achieving
F (xt) − F∗ ≤  is O(c2L2θ−1
Next  we show that APG can be made adaptive to HEB by periodically restarting given c and θ. This
is similar to [15] under the QGC. The steps of restarting APG (rAPG) are presented in Algorithm 2 
where we employ the simplest variant of APG.
Theorem 2. Suppose F (x0) − F∗ ≤ 0 and F (x) satisﬁes HEB on S0. By running Algorithm 2
 (cid:101) and tk = (cid:100)2c
with K = (cid:100)log2
k−1 (cid:101)  we have F (xK) − F∗ ≤ . The iteration complexity
√
L1/2−θ
of rAPG is O(c

√
) if θ > 1/2  and if θ ≤ 1/2 it is O(max{ c

) if θ > 1/2  and is O(max{ c2L

 )}) if θ ≤ 1/2.

1−2θ   c2L log( 0

Lθ−1/2

L log( 0

√

0

0

√
1/2−θ   c

L

 )}).

0

From Algorithm 2  we can see that rAPG requires the knowledge of c besides θ to restart APG.
However  for many problems of interest  the value of c is unknown  which makes rAPG impractical.
To address this issue  we propose to use the magnitude of the proximal gradient as a measure for
restart and termination. It is worth mentioning the difference between the development in this
paper and previous studies. Previous work [16  11] have considered strongly convex optimization

5

problems where the strong convexity parameter is unknown  where they also use the magnitude of
the proximal gradient as a measure for restart and termination. However  in order to achieve faster
convergence under the HEB condition without the strong convexity  we have to introduce a novel
technique of adaptive regularization that adapts to the HEB. With a novel synthesis of the adaptive
regularization and a conditional restarting that searchs for the c  we are able to develop practical
adaptive accelerated gradient methods. We also notice a recent work [6] that proposed unconditional
restarted accelerated gradient methods under QGC. Their restart of APG/FISTA does not involve
evaluation of the gradient or the objective value but rather depends on a restarting frequency parameter
and a convex combination parameter for computing the restarting solution  which can be set based on
a rough estimate of the strong convexity parameter. As a result  their linear convergence (established
for distance of solutions to the optimal set) heavily depends on the rough estimate of the strong
convexity parameter.
Before diving into the details of the proposed algorithm  we will ﬁrst present a variant of PG as a
baseline for comparison motivated by [18] for smooth problems  which enjoys a faster convergence
than the vanilla PG in terms of the proximal gradient’s norm. The idea is to return a solution
that achieves the minimum magnitude of the proximal gradient  i.e.  min1≤τ≤t (cid:107)G(xτ )(cid:107)2. The
convergence of min1≤τ≤t (cid:107)G(xτ )(cid:107)2 under HEB is presented in the following theorem.
Theorem 3. Suppose F (x0) − F∗ ≤ 0 and F (x) satisﬁes HEB on S0. The iteration complexity of
PG (option II  which returns the solution with historically minimal proximal gradient  see the supple-
mentary material) for achieving min1≤τ≤t (cid:107)G(xτ )(cid:107)2 ≤   is O(c
 )}) if
θ ≤ 1/2  and is O(c2L2θ−1

1−θ L max{1/

1−2θ
1−θ   log( 0

) if θ > 1/2.

1

0

The ﬁnal theorem in this section summarizes an o(1/t) convergence result of PG for minimizing
a proper  coercive  convex  lower semicontinuous and semi-algebraic function  which could be
interesting of its own.
Theorem 4. Let F (x) be a proper  coercive  convex  lower semicontinuous and semi-algebraic
functions. Then PG (with option I and option II) converges at a speed of o(1/t) for F (x) − F∗ and
G(x)  respectively  where t is the total number of iterations.

Remark: This can be easily proved by combining Proposition 1 and Theorems 1  3.

4 Adaptive Accelerated Gradient Converging Methods

We ﬁrst present a key lemma for our development that serves the foundation of the adaptive regular-
ization and conditional restarting.
Lemma 1. Assume F (x) satisﬁes HEB for any x ∈ Sξ with θ ∈ (0  1]. If θ ∈ (0  1/2]  then for
any x ∈ Sξ  we have D(x  Ω∗) ≤ 2
. If θ ∈ (1/2  1]  then for any

1−θ (cid:107)G(x)(cid:107) θ
1−θ
2

x ∈ Sξ  we have D(x  Ω∗) ≤(cid:0) 2

L + 2c2ξ2θ−1(cid:1)(cid:107)G(x)(cid:107)2.

L(cid:107)G(x)(cid:107)2 + c

1−θ 2

1

θ

A building block of the proposed algorithm is to solve a problem of the following style by employing
the Algorithm 1 (i.e.  Nesterov’s ADG):

(cid:107)x − x0(cid:107)2

δ
2

Fδ(x) = F (x) +

(8)
2(cid:107)x−
2. A key result for our development of conditional restarting is the following theorem for each

which consists of a L-smooth function f (x) and a δ-strongly convex function gδ(x) = g(x) + δ
x0(cid:107)2
call of Algorithm 1 for solving the above problem.
Theorem 5. By running the Algorithm 1 for minimizing f (x) + gδ(x) with an initial solution x0 

2 = f (x) + g(x) +

(cid:107)x − x0(cid:107)2
2 

δ
2

for t ≥(cid:113) L

(cid:1) we have

2δ log(cid:0) L
(cid:107)G(xt+1)(cid:107)2 ≤(cid:112)L(L + δ)(cid:107)x0 − x∗(cid:107)2

δ

(cid:104)

1 +(cid:112)δ/(2L)

(cid:105)−t

√

+ 2

2δ(cid:107)x0 − x∗(cid:107)2.

where x∗ is any optimal solution to the original problem.

Finally  we present the proposed adaptive accelerated gradient converging (adaAGC) method for
solving the smooth composite optimization in Algorithm 3 and prove the main theorem of this section.

6

Algorithm 3: adaAGC for solving (1)
Input: x0 ∈ Ω and c0 and γ > 1
Let ce = c0 and ε0 = (cid:107)G(x0)(cid:107)2
for k = 1  . . .   K do
for s = 1  . . .   do

Let δk be given in (9) and gδk (x) = g(x) + δk
A0 = 0  v0 = xk−1  xk
for t = 0  . . . do

0 = xk−1

2 (cid:107)x − xk−1(cid:107)2

2

L

a2
At+a = 2 1+δkAt

Let at+1 be the root of
Set At+1 = At + at+1
Set yt = At
At+1
Compute xk
Compute vt+1 = arg minx
if (cid:107)G(xk

t + at+1
xk
At+1

t+1)(cid:107)2 ≤ εk−1/2 then

t+1 = Pgδk /L(yt − ∇f (yt)/L)
2(cid:107)x − xk−1(cid:107)2

vt

1

let xk = xk
t+1 and εk = εk−1/2
break the enclosing two for loops

2 +(cid:80)t+1

// step S1

if τ = (cid:100)(cid:113) 2L

δk

√

τ =1 aτ∇f (xk

τ )(cid:62)x + At+1gδk (x)

log

L(L+δk)

δk

(cid:101) then

// condition (*)

let ce = γce and break the enclosing for loop

// step S2

Output: xK

The adaAGC runs with multiple stages (k = 1  . . .   K). We start with an initial guess c0 of the
parameter c in the HEB. With the current guess ce of c  at the k-th stage adaAGC employs ADG to
solve a problem of (8) with an adaptive regularization parameter δk being
if θ ∈ (0  1/2]
if θ ∈ (1/2  1]

 min

1−2θ
1−θ
k−1
16c1/(1−θ)

(cid:32)
(cid:16) L

(cid:33)

(cid:17)

δk =

L
32  

min

(9)

1−θ

2

ε

θ

e

32  

1
e2θ−1

0

32c2

(cid:16)√

The condition (*) speciﬁes the condition for restarting with an increased value of ce. When the ﬂow
enters step S2 before step S1 for each s  it means that the current guess ce is not sufﬁciently large
according to Theorem 5 and Lemma 1  then we increase ce and repeat the same process (next iteration
for s). We refer to this machinery as conditional restarting. We present the main result of this section
in the following theorem.
Theorem 6. Suppose F (x0)− F∗ ≤ 0  F (x) satisﬁes HEB on S0 and c0 ≤ c. Let ε0 = (cid:107)G(x0)(cid:107)2 
having (cid:107)G(xK)(cid:107)2 ≤  is (cid:101)O
K = (cid:100)log2( ε0
 )(cid:101)  p = (1 − 2θ)/(1 − θ) for θ ∈ (0  1/2]. The iteration complexity of Algorithm 3 for
if θ ∈ (1/2  1]  where (cid:101)O(·) suppresses a log term depending on c  c0  L  γ.
Lcθ−1/2
We sketch the idea of the proof here: for each k  we can bound the number of cycles (indexd by s in the
number of iterations across all stages is bounded by(cid:80)K
algorithm) in order to enter step S1 denoted by sk. We can bound sk ≤ logγ(c/c0) + 1 and then total
(cid:101).
Before ending this section  we would like to remark that if the smoothness parameter L is unknown 
one can also employ the backtracking technique pairing with each update to search for L [17].

if θ ∈ (0  1/2]  and (cid:101)O(
k=1 sktk where tk = (cid:100)(cid:113) 2L

p/2   log(ε0/)

2(1−θ) max( 1

L(L+δk)

(cid:17)

√

√

log

Lc

δk

δk

)

0

1

4.1 Convergence of Objective Gap

In this subsection  we show that the convergence of the proximal gradient also implies the convergence
of the objective gap F (x)− F∗ for certain subclasses of the general problems that we have considered.
Our ﬁrst result applies to the case when F (x) satisﬁes the HEB with θ ∈ (0  1) and the nonsmooth
part g(x) is absent  i.e.  F (x) = f (x).
In this case  we can establish the convergence of the
objective gap  since the objective gap can be bounded by a function of the magnitude of gradient 

7

0

1

2

√

(cid:16)√

Lcθ−1/2

i.e.  f (x) − f∗ ≤ c1/(1−θ)(cid:107)∇f (x)(cid:107)1/(1−θ)
easily prove the following result.
complexity of Algorithm 3 for having F (xK) − F (x∗) ≤  is (cid:101)O
Theorem 7. Assume F (x) = f (x) and the same conditions in Theorem 6 hold. The iteration
θ ∈ (0  1/2]  and (cid:101)O(
if

(cid:17)
) if θ ∈ (1/2  1)  where (cid:101)O(·) suppresses a log term depending on

(c.f. the proof of Lemma 2 in the supplement). One can

c  c0  L  γ.
Remark Note that the above iteration complexity of adaAGC is the same as that of rAPG (shown in
Table 1)  where the later is established under the knowledge of c.
Our second result applies to a subclass of the general problems where either g(x) or f (x) is µ-strongly
convex or F (x) = f (x) + g(x)  where f (x) = h(Ax) with h(·) being a strongly convex function
and g(x) is the indicator function of a polyhedral set Ω = {x : Cx ≤ b}. Examples include square
loss minimization under an (cid:96)1 or (cid:96)∞ constraint [15  Theorem 8]. It has been shown that in the last
case  for any x ∈ dom(F )  there exists µ > 0 such that

1/2−θ   log(ε0/)

Lc max(

µ
2

(cid:107)x − x∗(cid:107)2
2 

f (x∗) ≥ f (x) + ∇f (x)(cid:62)(x∗ − x) +

(10)
where x∗ is the closest optimal solution to x  and the HEB condition of F (x) with θ = 1/2 and
F (x+) − F∗ ≤ O(1/µ)(cid:107)G(x)(cid:107)2
Theorem 8. Assume f (x) or g(x) is µ-strongly convex  or f (x) = h(Ax) and g(x) is the in-
dicator function of a polyhedral set such that (10) holds for some µ > 0  and other conditions
K) − F (x∗) ≤  is
in Theorem 6 hold. The iteration complexity of Algorithm 3 for having F (x+

c = (cid:112)2/µ holds [15  Theorem 1]. In the three cases mentioned above  we can establish that
(cid:16)(cid:112)L/µ log(ε0/

(cid:17)
  where (cid:101)O(·) suppresses a log term depending on µ  c0  L  γ.

2  where x+ = Pg/L(x − ∇f (x)/L)  and the following result.

(cid:101)O

√

µ)

5 Applications and Experiments

i=1

1
n

min
x∈Rd

n(cid:88)

In this section  we present some applications of our theorems and algorithms in machine learning. In
particular  we consider the regularized problems with a smooth loss:
(cid:96)(x(cid:62)ai  bi) + λR(x) 

(cid:96)∞ norm (cid:107)x(cid:107)∞  or a huber norm [28]  or the (cid:96)1 p norm(cid:80)K

(11)
where (ai  bi)  i = 1  . . .   n denote a set of training examples  R(x) could be the (cid:96)1 norm (cid:107)x(cid:107)1  the
k=1 (cid:107)xk(cid:107)p  where k is the k-th component
vector of x. Next  we present several results about the HEB condition to cover a broad family of loss
functions that enjoy the faster convergence of adaAGC.
Corollary 1. Assume the loss function (cid:96)(z  b) is nonnegative  convex  smooth and piecewise quadratic 
then the problems in (11) with (cid:96)1 norm  (cid:96)∞ norm  Huber norm and (cid:96)1 ∞ norm regularization satisfy
the HEB condition with θ = 1/2 on any sublevel set Sξ with ξ > 0. Hence adaAGC has a global
linear convergence in terms of the proximal gradient’s norm and a square root dependence on the
condition number.

Remark: The above corollary follows directly from Proposition 2 and Theorem 6. If the loss function
is a logistic loss and the regularizer is a polyhedral function (e.g.  (cid:96)1  (cid:96)∞ and (cid:96)1 ∞ norm)  we can
prove the same result. Examples of convex  smooth and piecewise convex quadratic loss functions
include: square loss: (cid:96)(z  b) = (z − b)2 for b ∈ R; squared hinge loss: (cid:96)(z  b) = max(0  1 − bz)2
for b ∈ {1 −1}; and huber loss: (cid:96)(z  b) = ρ(|z − b| − ρ
2 ) if |z − b| > ρ  and (cid:96)(z  b) = (z − b)2/2 if
|z − b| ≤ ρ  for b ∈ R.

Experimental Results We conduct some experiments to demonstrate the effectiveness of adaAGC
for solving problems of type (1). Speciﬁcally  we compare adaAGC  PG with option II that returns
the solution with historically minimal proximal gradient  FISTA  unconditional restarting FISTA
(urFISTA) [6] for optimizing the squared hinge loss (classiﬁcation)  square loss (regression)  huber
loss (with ρ = 1) (regression) with (cid:96)1 and (cid:96)∞ regularization  which are cases of (11)  and we also
consider the (cid:96)1 constrained (cid:96)p norm regression (7) with varying p. We use three datasets from
the LibSVM website [5]  which are splice (n = 1000  d = 60) for classiﬁcation  and bodyfat

8

PG

FISTA
urFISTA
adaAGC

2040
1289
1666
1410

2040
1289
2371
1410

2040
1289
2601
1410

 = 10−7

2040
1289
3480
1410

3514
5526
1674
2382

3724
5526
2379
2382

3724
5526
2605
2382

3724
5526
3488
2382

Table 2: squared hinge loss with (cid:96)1 norm (left) and (cid:96)∞ norm (right) regularization on splice data

Algorithm  = 10−4

 = 10−5

 = 10−6

 = 10−4

 = 10−5

 = 10−6

 = 10−7

FISTA > adaAGC > PG > urFISTA

adaAGC > urFISTA > PG > FISTA

Table 3: square loss with (cid:96)1 norm (left) and (cid:96)∞ norm (right) regularization on cpusmall data
 = 10−7
Algorithm  = 10−4
109298
210874
6781
20082
43601
18278
13632
9571

 = 10−4
139505
6610
18276
9881

 = 10−6
210874
20082
35169
13632

 = 10−5
159908
16387
26706
12623

 = 10−7
170915
23779
43603
13575

 = 10−5
204120
16418
26704
13033

 = 10−6
170915
23779
35173
13575

FISTA
urFISTA
adaAGC

PG

adaAGC > FISTA > urFISTA > PG

adaAGC > FISTA > urFISTA > PG

Table 4: (cid:96)1 regularized huber loss (left) and (cid:96)1 constrained square loss (right) on bodyfat data

PG

Algorithm  = 10−4
258723
6630
6855
16976

FISTA
urFISTA
adaAGC

 = 10−5
423181
25020
12662
16980

 = 10−6
602043
74416
17994
23844

 = 10−7
681488
124261
23933
25697

 = 10−4
1006880
15805
138359
23054

 = 10−5
1768482
66319
235081
33818

 = 10−6
2530085
180977
331203
44582

 = 10−7
2632578
181176
426341
48127

urFISTA > adaAGC > FISTA > PG
Table 5: (cid:96)1 constrained (cid:96)p norm regression on bodyfat data ( = 10−3)

adaAGC> FISTA > urFISTA > PG

Algorithm

PG

adaAGC

p = 2

250869 (1)
8710 (1)

p = 4

979401 (3.90)
17494 (2.0)

p = 6

1559753 (6.22)
22481 (2.58)

p = 8

4015665 (16.00)

33081 (3.80)

n  and the parameter s in (7) is set to s = 100.

(n = 252  d = 14)  cpusmall (n = 8192  d = 12) for regression. For problems covered by (11)  we
ﬁx λ = 1
We use the backtracking in PG  adaAGC and FISTA to search for the smoothness parameter. In
adaAGC  we set c0 = 2  γ = 2 for the (cid:96)1 constrained (cid:96)p norm regression and c0 = 10  γ = 2 for the
rest problems. For fairness  for urFISTA and adaAGC  we use the same initial estimate of unknown
parameter (i.e.  c). Each algorithm starts at the same initial point  which is set to be zero  and we stop
each algorithm when the norm of its proximal gradient is less than a prescribed threshold  and report
the total number of proximal mappings. The results are presented in the Tables 2–5. It indicates
that adaAGC converges faster than PG and FISTA (except for solving squared hinge loss with (cid:96)1
norm regularization) when  is very small  which is consistent with the theoretical results. Note that
urFISTA sometimes has better performance than adaAGC but is worse than adaAGC in most cases. It
is notable that for some problems (see Table 2) the number of proximal mappings is the same value
for achieving different precision . This is because that value is the minimum number of proximal
mappings such that the magnitude of the proximal gradient suddenly becomes zero. In Table 5  the
numbers in parenthesis indicate the increasing factor in the number of proximal mappings compared
to the base case p = 2  which show that increasing factors of adaAGC are approximately the square
root of that of PG and thus are consistent with our theory.
6 Conclusions
In this paper  we have considered smooth composite optimization problems under a general Hölderian
error bound condition. We have established adaptive iteration complexity to the Hölderian error
bound condition of proximal gradient and accelerated proximal gradient methods. To eliminate the
dependence on the unknown parameter in the error bound condition and enjoy the faster convergence
of accelerated proximal gradient method  we have developed a novel parameter-free adaptive ac-
celerated gradient converging method using the magnitude of the (proximal) gradient as a measure
for restart and termination. We have also considered a broad family of norm regularized problems
in machine learning and showed faster convergence of the proposed adaptive accelerated gradient
converging method.

Acknowledgments We thank the anonymous reviewers for their helpful comments. M. Liu and T.
Yang are partially supported by National Science Foundation (IIS-1463988  IIS-1545995).

9

References
[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Img. Sci.  2:183–202  2009.

[2] E. Bierstone and P. D. Milman. Semianalytic and subanalytic sets. Publications Mathématiques

de l’Institut des Hautes Études Scientiﬁques  67(1):5–42  1988.

[3] J. Bolte  T. P. Nguyen  J. Peypouquet  and B. Suter. From error bounds to the complexity of

ﬁrst-order descent methods for convex functions. CoRR  abs/1510.08234  2015.

[4] D. Drusvyatskiy and A. S. Lewis. Error bounds  quadratic growth  and linear convergence of

proximal methods. arXiv:1602.06661  2016.

[5] R.-E. Fan and C.-J. Lin. Libsvm data: Classiﬁcation  regression and multi-label. URL:

http://www. csie. ntu. edu. tw/cjlin/libsvmtools/datasets  2011.

[6] O. Fercoq and Z. Qu. Restarting accelerated gradient methods with a rough strong convexity

estimate. arXiv preprint arXiv:1609.07358  2016.

[7] P. Gong and J. Ye. Linear convergence of variance-reduced projected stochastic gradient without

strong convexity. CoRR  abs/1406.1102  2014.

[8] K. Hou  Z. Zhou  A. M. So  and Z. Luo. On the linear convergence of the proximal gradient
method for trace norm regularization. In Advances in Neural Information Processing Systems
(NIPS)  pages 710–718  2013.

[9] H. Karimi  J. Nutini  and M. W. Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge
Discovery in Databases - European Conference (ECML-PKDD)  pages 795–811  2016.

[10] G. Li. Global error bounds for piecewise convex polynomials. Math. Program.  137(1-2):37–64 

2013.

[11] Q. Lin and L. Xiao. An adaptive accelerated proximal gradient method and its homotopy
In Proceedings of the International Conference on

continuation for sparse optimization.
Machine Learning  (ICML)  pages 73–81  2014.

[12] Z.-Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differen-

tiable minization. Journal of Optimization Theory and Applications  72(1):7–35  1992.

[13] Z.-Q. Luo and P. Tseng. On the linear convergence of descent methods for convex essenially

smooth minization. SIAM Journal on Control and Optimization  30(2):408–425  1992.

[14] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a

general approach. Annals of Operations Research  46:157–178  1993.

[15] I. Necoara  Y. Nesterov  and F. Glineur. Linear convergence of ﬁrst order methods for non-

strongly convex optimization. CoRR  abs/1504.06298  2015.

[16] Y. Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.

Kluwer Academic Publ.  2004.

[17] Y. Nesterov. Gradient methods for minimizing composite objective function. Core discussion
papers  Universite catholique de Louvain  Center for Operations Research and Econometrics
(CORE)  2007.

[18] Y. Nesterov. How to make the gradients small. Optima 88  2012.

[19] H. Nyquist. The optimal lp norm estimator in linear regression models. Communications in

Statistics - Theory and Methods  12(21):2511–2524  1983.

[20] R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM J. on Control

and Optimization  14  1976.

10

[21] A. M. So. Non-asymptotic convergence analysis of inexact gradient methods for machine

learning without strong convexity. CoRR  abs/1309.0113  2013.

[22] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted

to SIAM Journal on Optimization  2008.

[23] P. Wang and C. Lin. Iteration complexity of feasible descent methods for convex optimization.

Journal of Machine Learning Research  15(1):1523–1548  2014.

[24] Y. Xu  Q. Lin  and T. Yang. Stochastic convex optimization: Faster local growth implies faster
global convergence. In International Conference on Machine Learning  pages 3821–3830 
2017.

[25] Y. Xu  Y. Yan  Q. Lin  and T. Yang. Homotopy smoothing for non-smooth problems with lower
complexity than O(1/). In Advances In Neural Information Processing Systems 29 (NIPS) 
pages 1208–1216  2016.

[26] T. Yang and Q. Lin. Rsg: Beating subgradient method without smoothness and strong convexity.

CoRR  abs/1512.03107  2016.

[27] W. H. Yang. Error bounds for convex polynomials. SIAM Journal on Optimization  19(4):1633–

1647  2009.

[28] O. Zadorozhnyi  G. Benecke  S. Mandt  T. Scheffer  and M. Kloft. Huber-norm regularization
for linear prediction models. In Machine Learning and Knowledge Discovery in Databases -
European Conference (ECML-PKDD)  pages 714–730  2016.

[29] H. Zhang. New analysis of linear convergence of gradient-type methods via unifying error

bound conditions. CoRR  abs/1606.00269  2016.

[30] H. Zhang. The restricted strong convexity revisited: analysis of equivalence to error bound and

quadratic growth. Optimization Letters  pages 1–17  2016.

[31] Z. Zhou and A. M. So. A uniﬁed approach to error bounds for structured convex optimization

problems. CoRR  abs/1512.03518  2015.

[32] Z. Zhou  Q. Zhang  and A. M. So. L1p-norm regularization: Error bounds and convergence
rate analysis of ﬁrst-order methods. In Proceedings of the 32nd International Conference on
Machine Learning  (ICML)  pages 1501–1510  2015.

11

,Igor Mordatch
Kendall Lowrey
Galen Andrew
Zoran Popovic
Emanuel Todorov
Mingrui Liu
Tianbao Yang