2019,k-Means Clustering of Lines for Big Data,The input to the \emph{$k$-mean for lines} problem is a set $L$ of $n$ lines in $\mathbb{R}^d$  and the goal is to compute
a set of $k$ centers (points) in $\mathbb{R}^d$ that minimizes the sum of squared distances over every line in $L$ and its nearest center. This is a straightforward generalization of the $k$-mean problem where the input is a set of $n$ points instead of lines.

We suggest the first PTAS that computes a $(1+\epsilon)$-approximation to this problem in time $O(n \log n)$ for any constant approximation error $\epsilon \in (0  1)$  and constant integers $k  d \geq 1$. This is by proving that there is always a weighted subset (called coreset) of $dk^{O(k)}\log (n)/\epsilon^2$ lines in $L$ that approximates the sum of squared distances from $L$ to \emph{any} given set of $k$ points. 

Using traditional merge-and-reduce technique  this coreset implies results for a streaming set (possibly infinite) of lines to $M$ machines in one pass (e.g. cloud) using memory  update time and communication that is near-logarithmic in $n$  as well as deletion of any line but using linear space. These results generalized for other distance functions such as $k$-median (sum of distances) or ignoring farthest $m$ lines from the given centers to handle outliers.

Experimental results on 10 machines on Amazon EC2 cloud show that the algorithm performs well in practice.
Open source code for all the algorithms and experiments is also provided.,k-Means Clustering of Lines for Big Data

Department of Computer Science

Department of Computer Science

Yair Marom

University of Haifa

Haifa  Israel

Dan Feldman

University of Haifa

Haifa  Israel

yairmrm@gmail.com

dannyf.post@gmail.com

Abstract

The input to the k-mean for lines problem is a set L of n lines in Rd  and the goal
is to compute a set of k centers (points) in Rd that minimizes the sum of squared
distances over every line in L and its nearest center. This is a straightforward
generalization of the k-mean problem where the input is a set of n points instead
of lines.
We suggest the Ô¨Årst PTAS that computes a (1 + Œµ)-approximation to this problem
in time O(n log n) for any constant approximation error Œµ ‚àà (0  1)  and constant
integers k  d ‚â• 1. This is by proving that there is always a weighted subset (called
coreset) of dkO(k) log(n)/Œµ2 lines in L that approximates the sum of squared
distances from L to any given set of k points.
Using traditional merge-and-reduce technique  this coreset implies results for a
streaming set (possibly inÔ¨Ånite) of lines to M machines in one pass (e.g. cloud)
using memory  update time and communication that is near-logarithmic in n  as
well as deletion of any line but using linear space. These results generalized for
other distance functions such as k-median (sum of distances) or ignoring farthest
m lines from the given centers to handle outliers.
Experimental results on 10 machines on Amazon EC2 cloud show that the
algorithm performs well in practice. Open source code for all the algorithms and
experiments is also provided.

1

Introduction

1.1 Background

Clustering is the task of partitioning the input set to subsets  where items in the same subset (cluster)
are similar to each other  compared to items in other clusters. There are many different clustering
techniques  but arguably the most common in both industry and academy is the k-mean problem 
where the input is a set P of n points in Rd  and the goal is to compute a set C of k centers (points) in
Rd  that minimizes the sum of squared distances over each point p ‚àà P to its nearest center in C  i.e.

C ‚àà arg min

C(cid:48)‚äÜRd |C(cid:48)|=k

c(cid:48)‚ààC(cid:48) (cid:107)p ‚àí c(cid:48)(cid:107)2 .

min

(cid:88)

p‚ààP

A very common heuristics to solve this problem is the Lloyd‚Äôs algorithm [3  22]  that is similar to the
EM-Algorithm that is described in Section 5 in supplementary material [2]. We consider a natural
generalization of this k-mean problem  where the input set P of n points is replaced by a set L of n
lines in Rd; See Fig. 2. Here  the distance from a line to a center c is the closest Euclidean distance
to c over all the points on the line. Since we only assume the ‚Äúweak" triangle inequality between

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

points  our solution can easily be generalized to sum of distances to the power of any constant z ‚â• 1
as explained e.g. in [7  6].
Motivation for solving the k-line mean problem arises in many different Ô¨Åelds  when there is some
missing entry in all or some of the input vectors  or incomplete information such as a missing sensor.
For example  a common problem in Computer Vision is to compute the position of a point or k points
in the world  based on their projections on a set of n 2D-images  which turn into n lines via the
pinhole camera model; See Fig 1  and [18  27] for surveys.
In Data Science and matrix approximation theory  every missing entry turns a point (database‚Äôs
record) into a line by considering all the possible values for the missing entry. E.g.  n points on the
plane from k-mean clusters would turn into n horizontal/vertical lines that intersect "around" the
k-mean centers. The resulting problem is then k-mean for n lines [24  25]. One can consider also an
applications to semi-supervised learning - k-mean for mixed points and lines. This problem arises
when lines are unlabeled points (last axis is a label) and we want to add a label to the farthest lines
from the points.

Figure 1: Application of k-line mean for computer vision. Given a drone (or any other rigid body) that
is captured by n cameras - our goal is to locate the 3 dimensional position of the drone in space by
identifying k = 4 Ô¨Åxed known markers/features on the drone. Each point on each image corresponds
to a line that passes through it and the pin-hole of the camera. Without noise  all the lines intersect at
the same point (marker in R3). Otherwise their 1-mean is a natural approximation.

1.2 Related Work

The k-mean problem and its variance has been researched in numerous papers over the recent decades 
especially in the machine learning community   see [16  20  28  5] and references therein. There
are also many results regarding projective clustering  when the k centers are replaced by lines or
j-dimensional subspaces instead of points.
However  signiÔ¨Åcantly less results are known for the case of clustering subspaces  or even lines. A
possible reason might be to the fact that the triangle inequality or its weaker version holds for a
pair of points but not for lines  even in the planar case: two parallel lines can have an arbitrarily
large distance from each other  but still intersect with a third line simultaneously. Gao  Langebreg
and Schulman [12] used Helly‚Äôs theorem [8] (intersection of convex sets) to introduce the "k-center
problem" for lines  that aims to cover a collection of lines by the smallest k balls in R3.
Langebreg and Schulman [11] addressed the 1-convex sets center problem that aims to compute a
ball that intersects a given set of n convex sets such as lines and ‚àÜ-afÔ¨Åne subspaces. This type of
non-clustering problems (k = 1) is easier since it admits a convex optimization problem instead of a
clustering non-convex problem.
Unlike the case of numerous theoretical papers that study the k-mean problem for points  we did not
Ô¨Ånd any provable solution for the case of k-line mean problem or even an efÔ¨Åcient PTAS (polynomial
approximation scheme). There are very few related results that we give here. A solution for the
special case of d = 2 and sum of distances was considered in [23]. Lee and Schulman [17] studied
the generalization of the k-center problem (maximum over the n distances  instead of their sum)  for
the case where the input is a set of n afÔ¨Åne subspaces  each of dimension ‚àÜ. In this case  the size of

2

Figure 2: Problem Statement demonstration for the planar case. The input is a set L = {(cid:96)1  . . .   (cid:96)6}
of n = 6 lines in R2  and our goal is to Ô¨Ånd the k = 2 points (centers) p1  p2 ‚àà R2 that minimize the
sum of Euclidean distances from each center to its nearest line in L. See Section 2.1 deÔ¨Ånition of the
distance function D.

the coreset is exponential in d  which was proved to be unavoidable even for a coreset of 1-center
(single point) for this type of covering problems. The corresponding covering problems can then be
solved using traditional computational geometry techniques.
Table 1 summarizes the above results.

Problem

1-line center in Rd
2-line centers in R2
3-line centers in R2
1-center for convex-
sets in Rd
k-line median in R3
k-line median in R2
1-‚àÜ-Ô¨Çats center in Rd
2-‚àÜ-Ô¨Çats centers in Rd
3-‚àÜ-Ô¨Çats centers in Rd
k-‚àÜ-Ô¨Çats centers in Rd
k-line median in Rd

(cid:16)

O

Running Time
nd(1/Œµ)O(1)

O (n log (1/Œµ) (d + log n))
nd log(1/Œµ) + n log2(n) log(1/Œµ)

Œµ

iterative  unbounded

O(cid:0)n‚àÜ+1d(1/Œµ)O(1)(cid:1)
(cid:16) log n
(cid:17)O(k)
(cid:1)
O(cid:0) nd‚àÜ
O(cid:0)dn2 log n(cid:1)

log ‚àÜ
Œµ

n

Œµ

Œµ2

(cid:17)

Approx.
Factor
1 + Œµ

2 + Œµ

2 + Œµ

1 + Œµ

unbounded

1 + Œµ

O(cid:0)‚àÜ1/4(cid:1)

1 + Œµ

2O(‚àÜ(1+1/Œµ2))nd

O(cid:0)d3n log(n)k log k + (d/Œµ)2(cid:1) + ndkO(k)

2O(‚àÜk log k(1+1/Œµ2))nd

1 + Œµ

1 + Œµ

1 + Œµ

Paper
[11]
[12]

[12]

[11]
[21]

[23]
[17]
[17]
[17]
[17]
Our

Table 1: Summary of related results for k centers of n points in Rd. The dimension of the quarried
subspaces is denoted by ‚àÜ and the error rate is by Œµ.

1.3 Main Contribution
Our main technical result is an algorithm that gets a set L of n lines in Rd  an integer k ‚â• 1 
and computes an Œµ-coreset (see DeÔ¨Ånition 2.7) of size dkO(k) log(n)/Œµ2 for L and every error
parameter Œµ > 0  in a near-linear running time in the number of data lines n  and polynomial in the
dimensionality d and the number k of desired centers; See Theorem 2.8 for details and exact bounds.
Using this coreset with a merge-and-reduce technique  we achieve the following results:

1. An algorithm that  during one pass  maintains and outputs a (1 + Œµ)-approximation to the

k-line mean of the lines seen so far; See DeÔ¨Ånition 2.4.

3

‚Ñì1‚Ñì2‚Ñì3‚Ñì4‚Ñì5‚Ñì6ùëù1ùëù2ùê∑(‚Ñì5 ùëù1)2. A streaming algorithm that computes an (1+Œµ)-approximation for the k-line mean of a set L
of lines that may be distributed (partitioned) among M machines  where each machine needs
to send only d3kO(k) log2 n input lines to the main server at the end of its computation.

3. Experimental results on 10 machines on Amazon EC2 Cloud [9] show that the algorithm

performs well in practice  boost the performance of existing EM-heuristic [19].

Why the coreset is exponential in k? The worst case size of our coreset is polynomial in log n but
exponential in k. Still  it is tight: such lower bounds are known for weighted centers [14] which is a
special case of coreset for lines  as explained in [10]. Fortunately  it seems that it is only a theoretical
worst-case bound for a very tailored and artiÔ¨Åcial synthetic example. Our experiments imply that
the error on real-world data sets is far better than our theorem predicts using the same importance
distribution but for a smaller sample size. In addition  we can partition the n-line set in the hope that
each subset would be served by at most k points.

2 Problem Statement and Theoretical Result

2.1 Preliminaries and Problem Statement
For an integer n ‚â• 1 we deÔ¨Åne [n] = {1  . . .   n}. From here and in the rest of the paper  we
assume that we are given a function D : Rd √ó Rd ‚Üí R and a constant œÅ > 0 such that D(a  b) ‚â§
œÅ(D(a  c) + D(c  b)) for every a  b  c ‚àà Rd.
DeÔ¨Ånition 2.1 (weighted set) A weighted set of lines is a pair L(cid:48) = (L  w) where L is a set of lines
in Rd  and w : L ‚Üí (0 ‚àû) is a function that maps every (cid:96) ‚àà L to w((cid:96)) ‚â• 0  called the weight of (cid:96).
A weighted set (L  1) where 1 is the weight function w : L ‚Üí {1} that assigns w((cid:96)) = 1 for every
(cid:96) ‚àà L may be denoted by L for short.
DeÔ¨Ånition 2.2 (distance) The Euclidean distance between a pair of points is denoted by the function
D : Rd √ó Rd ‚Üí [0 ‚àû)  s.t. for every x  y ‚àà Rd we have D(x  y) = (cid:107)x ‚àí y(cid:107)2. For every set
X ‚äÜ Rd and a point x ‚àà Rd  we deÔ¨Åne the distance from X to x by D(X  x) = inf q‚ààX D(q  x)  and
for every set Y ‚äÜ Rd  we denote the distance from X to Y by D(X  Y ) = inf (x y)‚ààX√óY D(x  y).
DeÔ¨Ånition 2.3 (cost) For every set P ‚äÜ Rd of k points and a weighted set of lines L(cid:48) = (L  w) in

Rd  we denote the sum of weighted distances from L to P by cost(L(cid:48)  P ) =(cid:80)

(cid:96)‚ààL w((cid:96))D((cid:96)  P ).

A natural generalization of the k-mean problem is to replace the input set of points P by a set L of n
lines in Rd.
DeÔ¨Ånition 2.4 (k-mean for lines) Let L(cid:48) = (L(cid:48)  w) be a weighted set of lines in Rd and k ‚â• 1 be
an integer. A set P ‚àó ‚äÜ Rd is a k-mean of L(cid:48) if it minimizes cost(L(cid:48)  P ) over every set P of k points
in Rd.

2.2 Theoretical Results
We begin with our Ô¨Årst result - the Ô¨Årst bicriteria approximation for the general k-mean of lines in Rd 
for any integers k  d ‚â• 1.
DeÔ¨Ånition 2.5 (Œ±  Œ≤-approximation) Let L be a Ô¨Ånite set of lines in Rd  k ‚â• 1 be an integer and
P ‚àó ‚äÜ Rd be a k-mean of L; See DeÔ¨Ånition 2.4. Then  for every Œ±  Œ≤ ‚â• 0  a set B ‚äÜ Rd of kŒ≤ points
is called (Œ±  Œ≤)-approximation of L  if

cost(L  B) ‚â§ Œ± ¬∑ cost(L  P ‚àó).

If Œ≤ = 1 then B is called an Œ±-approximation of L.   if Œ± = Œ≤ = 1 then B is called the optimal
solution.
Theorem 2.6 Let L be a set of n lines in Rd  k ‚â• 1 be an integer  Œ¥ ‚àà (0  1) and

(cid:18)

m ‚â• c

(cid:18) 1

(cid:19)(cid:19)

 

Œ¥

dk log2 k + log2

4

for a sufÔ¨Åciently large constant c > 1 that can be determined from the proof. Let B be the output set
of a call to BI-CRITERIA-APPROXIMATION(L  m); See Algorithm 2. Then 

|B| ‚àà O (log n (dk log k + log(1/Œ¥)))

(1)

and with probability at least 1 ‚àí Œ¥ 

Moreover  B can by computed in O(cid:0)nd2k log k + m2 log n(cid:1) time.

cost(L  B) ‚â§ 4œÅ2 ¬∑ min

P‚äÜRd |P|=k

cost(L  P ).

Coreset is a problem dependent data summarization. The deÔ¨Ånition of coreset is not consistent among
papers. In this paper  the input is usually a set of lines in Rd  but for the streaming case we compute
coreset for union of (weighted) coresets and thus weights will be needed. We use the following
deÔ¨Ånition of Feldman and KÔ¨År [15].
DeÔ¨Ånition 2.7 (Œµ-coreset [15]) For an approximation error Œµ > 0  the weighted set S(cid:48) = (S  u) is
called an Œµ-coreset for the query space (P (cid:48)  Y  f  loss)  if S ‚äÜ P  u : S ‚Üí [0 ‚àû)  and for every
y ‚àà Y we have

(1 ‚àí Œµ)floss(P (cid:48)  y) ‚â§ floss(S(cid:48)  y) ‚â§ (1 + Œµ)floss(P (cid:48)  y).

Theorem 2.8 (coreset for k-line mean) Let L be a set of n lines in Rd  k ‚â• 1 be an integer  Œµ  Œ¥ ‚àà
(0  1) and m > 1 be an integer such that

m ‚â• cd2k log2(k) log2(n) + log(1/Œ¥)

for some universal constant c > 0  and Qk = (cid:8)B ‚äÜ Rd | |B| = k(cid:9). Let (S  u) be the output of

a call to CORESET(L  k  m); see Algorithm 4. Then  with probability at least 1 ‚àí Œ¥  (S  u) is an
Œµ-coreset for the query space (L Qk  D (cid:107)¬∑(cid:107)1). Moreover  (S  u) can be computed in time

Œµ2

 

O(cid:0)d3n log(n)k log k + (d/Œµ)2(cid:1) + ndkO(k).

Using KÔ¨År and Feldman coreset on streaming framework [15]  we enable to boost performance and
instead of performing the coreset calculation on the whole data in one piece - we performed the
action on batches we read one after the other (the streaming version)  and its validity can be seen in
the carried out experiments in Section 4.

3 Algorithms and Technique

We present here the Ô¨Årst bi-criteria solution for the k-line mean problem  where Alg. 1 is a sub-
procedure that is being called during the running of Alg. 2.

Algorithm 1: CENTROID-SET(L)

A Ô¨Ånite set L of n lines in Rd.
A set G ‚äÜ Rd of O(n2) points.

Input:
Output:
1 for every (cid:96) ‚àà L do
2
3

for every (cid:96)(cid:48) ‚àà L \ (cid:96) do

Compute q((cid:96)  (cid:96)(cid:48)) ‚àà arg minx‚àà(cid:96) D((cid:96)(cid:48)  x)

// the closest point on (cid:96) to (cid:96)(cid:48).

Q((cid:96)) := {q((cid:96)  (cid:96)(cid:48)) | (cid:96)(cid:48) ‚àà L \ {(cid:96)}}

G :=(cid:83)

4
5
6 return G

(cid:96)‚ààL Q((cid:96))

Ties broken arbitrarily.

Overview of Algorithm 2. The input to the algorithm is a set L consist of n lines in Rd and a
positive integer m ‚â• 1. In each iteration of the algorithm it picks a small uniform sample S of the
input in Line 3  compute their centroid-set G using a call to Algorithm 1 in Line 4  and add them to
the output set B in Line 5. Then  in Line 6  a constant fraction of the closest lines to G is removed

5

1 X := L  B := ‚àÖ
2 while |X| > 100 do
3

from the input set L. The algorithm then continues recursively for the next iteration  but only on the
remaining set of lines until almost no more lines remain. The output is the resulting set B.

Algorithm 2: BI-CRITERIA-APPROXIMATION(L  m)

Input:
Output:

A set L of n lines in Rd  and an integer m ‚â• 1.
A set B ‚äÜ Rd which is  with probability at least 1/2  an (Œ±  Œ≤)-approximation

for the k-mean of L  where Œ± ‚àà O(1) and Œ≤ = O(cid:0)m2 log n(cid:1).

// See Definition 2.5 and Theorem 2.6.

Pick a sample S of |S| ‚â• m lines  where each line (cid:96) ‚àà S is sampled i.i.d and uniformly
at random from X.
G := CENTROID-SET(S)
B := B ‚à™ G
X(cid:48) := the closest 7|X| /11 lines in X to G. Ties broken arbitrarily.
X := X \ X(cid:48)

4
5
6
7
8 return B
Overview of Algorithm 3. The input is a set L of lines in Rd  a point b ‚àà Rd and an integer
k ‚â• 1 for the number of desired centers. This procedure is called from Algorithm 4  where b is an
approximation to the 1-mean of L. The output function s maps every line (cid:96) ‚àà L to [0 ‚àû)  and is
being used during Algorithm 4 execution. We deÔ¨Åne in Line 1 a unit sphere Sd‚àí1 that is centered
around b. Next  in Line 3 for each line (cid:96) ‚àà L we deÔ¨Åne (cid:96)(cid:48) ‚äÜ Rd to be the translation of (cid:96) to b. In Lines
4‚Äì5  we replace every line (cid:96)(cid:48) with one of its two intersections with Sd‚àí1  and deÔ¨Åne the union of these
points to be the set Q. In Line 6 we call the sub-procedure WEIGHTED-CENTERS-SENSITIVITY
that is described in [10]. This procedure returns the sensitivities of the query space of k-weighted
centers queries on Q. As stated in Lemma 32 in supplementary material  the total sensitivities of this
coreset is kO(k) log n. Finally  in Line 7  we convert the output sensitivity s(p) of each point p in Q
to the output sensitivity s((cid:96)) of the corresponding line (cid:96) in L.

Algorithm 3: LINES-SENSITIVITY(L  b  k)

Input:
Output:

1 Sd‚àí1 :=(cid:8)x ‚àà Rd | (cid:107)x ‚àí b(cid:107) = 1(cid:9) // the unit sphere that is centered at b.

A set L of n lines in Rd  a point b ‚àà Rd and integer k ‚â• 1.
A (sensitivity) function s : L ‚Üí [0 ‚àû).

(cid:96)(cid:48) := the line {x ‚àí b | x ‚àà (cid:96)} that is parallel to (cid:96) and intersects b // see Fig. 3.
p((cid:96)(cid:48)) := an arbitrary point in the pair (cid:96)(cid:48) ‚à© Sd‚àí1

2 for (cid:96) ‚àà L do
3
4
5 Q := Q{p((cid:96)(cid:48)) | (cid:96) ‚àà L}
6 u := WEIGHTED-CENTERS-SENSITIVITY(Q  2k) // see algorithm overview.
7 Set s : L ‚Üí [0 ‚àû) such that for every (cid:96) ‚àà L

8 return s

s((cid:96)) := u (p((cid:96)(cid:48))) .

Overview of Algorithm 4. The algorithm gets a set L of lines in Rd  an integer k ‚â• 1 for the number
of desired centers and a positive integer m ‚â• 1 for the coreset size  and returns an Œµ-coreset for L;
See DeÔ¨Ånition 2.7. In Line 2 a small set B that approximate the k-mean of L is computed via a call
to BI-CRITERIA-APPROXIMATION. In Line 3 the lines are clustered according to their nearest point
in B  and in Line 5 the lines sensitivities in the cluster Lb are computed for each center b ‚àà B. In the
second "for" loop in Lines 8‚Äì10 we set the sensitivity of each line to be the sum of the scaled distance
of the line to its nearest center b (translation)  and the sensitivity sb that measure its importance with
respect to its direction (rotation). Here  scaled distance means that the distance is divided by the sum
of distances over all the lines in L. The If statement in Line 7 is used to avoid division by zero. In
Line 12 we pick a random sample S from L  where the probability of choosing a line (cid:96) is proportional
to its sensitivity s((cid:96)). In Line 13 we assign a weight to each line  that is inverse proportional to the
probability of sampling it. The resulting weighted set (S  u) is returned in Line 14.

6

Figure 3: Example of running Alg. 3 in the d = 2 dimensional case. On the left  we start with a set
L = {(cid:96)1  . . .   (cid:96)5} of lines and a single center b. Next  we translate each line onto b and stretch the
unit sphere S around it. Finally  we replace each line with one of its two intersections with S and
achieve the set Q = {p((cid:96)1)  . . .   p((cid:96)5)}.

Algorithm 4: CORESET(L  k  m)

Input:
Output:

A Ô¨Ånite set L of lines in Rd  number k ‚â• 1 of centers and the coreset size m ‚â• 1.
A weighted set (‚Äúcoreset‚Äù) (S  u) that satisÔ¨Åes Theorem 2.8.

1 j := cdk log2 k  where c is a sufÔ¨Åcient large constant c > 0 that can be determined from the

proof of Theorem 2.6.
2 B := BI-CRITERIA-APPROXIMATION (L  j) // see Algorithm 2
3 Compute a partition {Lb | b ‚àà B} of L such that Lb is the set (cluster) of lines that are
4 for every b ‚àà B do
5

closest to the point b ‚àà B. Ties broken arbitrarily.

sb := LINES-SENSITIVITY(Lb  b  k)
// the sensitivity of each line (cid:96) ‚àà Lb that was translated onto b;

see Algorithm 3

6 for every b ‚àà B and (cid:96) ‚àà Lb do

+ 2 ¬∑ sb((cid:96))

7

8

9
10

if cost(L  B) > 0 then
D((cid:96)  b)

s((cid:96)) :=

cost(L  B)

else

s((cid:96)) := sb((cid:96))

s((cid:96))(cid:80)

prob((cid:96)) :=

(cid:96)(cid:48)‚ààL s((cid:96)(cid:48))

11
12 Pick a sample S of at least m lines from L  where each line (cid:96) ‚àà L is sampled i.i.d. with
13 Set u : S ‚Üí [0 ‚àû) such that for every (cid:96) ‚àà S

probability prob((cid:96)).

u((cid:96)) :=

1

|S| prob((cid:96))

.

14 return (S  u)

4 Experimental Results

Following motivation to narrow the gap between the theoretical and practical Ô¨Åelds  experiments took
a dominant place during research.
Software. We implemented our coreset construction from Algorithm 4 and its sub-procedures in
Python V. 3.6. We make use of the MKL package [26] to improve its performance  but it is not
necessary in order to run it. The source code can be found in [1].
Data Sets. We evaluate our system on two types of data sets: synthetic data generated with carefully
controlled parameters  and a real data of roads map from the "Open Street Map" Dataset [13] and
"SimpleHome XCS7 1002 WHT Security Camera" from the the "UCI Machine Learning Repository"
Dataset [4]. The roads dataset [13] contains n = 10  000 roads in China from the "Open Street Map"
dataset (Fig. 4 plot (a))  each road is represented as a 2-dimensional segment that was stretched into

7

ùëè‚Ñì1‚Ñì2‚Ñì3‚Ñì4‚Ñì5ùëè‚Ñì1‚Ñì2‚Ñì3‚Ñì4‚Ñì5ùëù(‚Ñì2)ùëù(‚Ñì1)ùëù(‚Ñì3)ùëù(‚Ñì4)ùëù(‚Ñì5)ùëèan inÔ¨Ånite line on a plane. Synthetic data of n = 10  000 lines was generated as well (Fig. 4 plot (b)).
Experiments on ofÔ¨Çine data analysis. In Plot (a) in Graph 4  when the sample size was m = 700
lines out of 10 000 given lines  the coreset error and variance were 1.86 and 0.16  respectively  that
is an error of Œµ = 0.86  for a sample size of m = (cid:100)602/Œµ(cid:101) lines. On the other hand  the error and
variance of the competitor algorithm with the same sample size were 2.62 and 0.26. This implies that
our coreset is more accurate and stable than RANSAC  and that our mathematically provable constant
approximation algorithm for k-line mean works better than a standard EM algorithm also in practice.
Experiments on streaming data analysis. Plot (c) in Graph 4 demonstrates the size of the merge-
and-reduce streaming coreset tree during the streaming  which is logarithmic in the number of lines
we streamed so far. In Plot (d) in Graph 4  we can see how the coreset construction running time
decreases linearly as the number of machines in the machines cluster increases (parameters are
written in the chart‚Äôs title)  where coreset construction was measured 3 different times on 3 different
number of centers. Note that the decrease rate is almost linear in the cluster‚Äôs machines number and
not exactly  due to overhead of communications and I/O.

Figure 4: Experiment Results. Graphs (a) and (b) reÔ¨Çects the error decreasing rate in compare to an
increasing size of sample by coreset and uniform sampling. Graph (c) shows how the amount of data
required by the merge-and-reduce coreset tree is logarithmic in the number of lines read so far in a
stream. Graph (d) demonstrate how the coreset construction time decreases linearly in the number of
machines in Amazon EC2 cluster  where coreset samples were taken by different number of centers 
preserving the invariant.

5 Conclusions and Future Work

This paper purposes a deterministic algorithm that computes an Œµ-coreset of size near-logarithmic
in the input. Moreover  we suggest a streaming algorithm that computes a (1 + Œµ)-approximation
for the k-means of a set of lines that is distributed among M machines  where each machine needs
to send only near-logarithmic number of input lines to the main server for its computations. Other
future work will consider an input of j-dimensional afÔ¨Åne sub-spaces in Rd (here input of lines is a
private case of j = 1)  where the motivation is multiple missing entries completion.

8

References
[1] Github. https://github.com/YairMarom/k_lines_means  2019.

[2] Supplementary material. https://arxiv.org/abs/1903.06904  2019.

[3] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms  pages
1027‚Äì1035. Society for Industrial and Applied Mathematics  2007.

[4] Arthur Asuncion and David Newman. Uci machine learning repository  2007.

[5] Francis R Bach and Michael I Jordan. Learning spectral clustering. In Advances in neural

information processing systems  pages 305‚Äì312  2004.

[6] Olivier Bachem  Mario Lucic  and Silvio Lattanzi. One-shot coresets: The case of k-clustering.

arXiv preprint arXiv:1711.09649  2017.

[7] Moses Charikar  Sudipto Guha  √âva Tardos  and David B Shmoys. A constant-factor ap-
proximation algorithm for the k-median problem. Journal of Computer and System Sciences 
65(1):129‚Äì149  2002.

[8] Ludwig Danzer. " helly‚Äôs theorem and its relatives " in convexity. In Proc. Symp. Pure Math. 

volume 7  pages 101‚Äì180. Amer. Math. Soc.  1963.

[9] Amazon EC2. Amazon ec2. https://aws.amazon.com/ec2/  2010.

[10] Dan Feldman and Leonard J Schulman. Data reduction for weighted and outlier-resistant
clustering. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete
Algorithms  pages 1343‚Äì1354. Society for Industrial and Applied Mathematics  2012.

[11] Jie Gao  Michael Langberg  and Leonard J Schulman. Analysis of incomplete data and an
intrinsic-dimension helly theorem. Discrete & Computational Geometry  40(4):537‚Äì560  2008.

[12] Jie Gao  Michael Langberg  and Leonard J Schulman. Clustering lines in high-dimensional
space: ClassiÔ¨Åcation of incomplete data. ACM Transactions on Algorithms (TALG)  7(1):8 
2010.

[13] Mordechai Haklay and Patrick Weber. Openstreetmap: User-generated street maps. Ieee Pervas

Comput  7(4):12‚Äì18  2008.

[14] Sariel Har-Peled. Coresets for discrete integration and clustering. In International Conference on
Foundations of Software Technology and Theoretical Computer Science  pages 33‚Äì44. Springer 
2006.

[15] KÔ¨År and Feldman. Coresets for big data learning of gaussian mixture models of any shape.

2018.

[16] Andreas Krause  Pietro Perona  and Ryan G Gomes. Discriminative clustering by regularized
In Advances in neural information processing systems  pages

information maximization.
775‚Äì783  2010.

[17] Euiwoong Lee and Leonard J Schulman. Clustering afÔ¨Åne subspaces: hardness and algorithms.
In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms 
pages 810‚Äì827. SIAM  2013.

[18] Yufeng Liu  Rosemary Emery  Deepayan Chakrabarti  Wolfram Burgard  and Sebastian Thrun.
Using em to learn 3d models of indoor environments with mobile robots. In ICML  volume 1 
pages 329‚Äì336  2001.

[19] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory 

28(2):129‚Äì137  1982.

[20] Boaz Nadler  Stephane Lafon  Ioannis Kevrekidis  and Ronald R Coifman. Diffusion maps 
In Advances in neural

spectral clustering and eigenfunctions of fokker-planck operators.
information processing systems  pages 955‚Äì962  2006.

9

[21] Bj√∂rn Ommer and Jitendra Malik. Multi-scale object detection by clustering lines. In Computer

Vision  2009 IEEE 12th International Conference on  pages 484‚Äì491. IEEE  2009.

[22] Rafail Ostrovsky  Yuval Rabani  Leonard J Schulman  and Chaitanya Swamy. The effectiveness
of lloyd-type methods for the k-means problem. In Foundations of Computer Science  2006.
FOCS‚Äô06. 47th Annual IEEE Symposium on  pages 165‚Äì176. IEEE  2006.

[23] Tomer Perets. Clustering of lines. Open University of Israel  2011.

[24] Huamin Ren  Hong Pan  S√∏ren Ingvor Olsen  and Thomas B Moeslund. How does structured

sparsity work in abnormal event detection? In ICML‚Äô15 Workshop  2015.

[25] Jie Shen  Ping Li  and Huan Xu. Online low-rank subspace clustering by basis dictionary

pursuit. In International Conference on Machine Learning  pages 622‚Äì631  2016.

[26] Endong Wang  Qing Zhang  Bo Shen  Guangyong Zhang  Xiaowei Lu  Qing Wu  and Yajuan

Wang. High-performance computing on the intel xeon phi. Springer  5:2  2014.

[27] David Williams and Lawrence Carin. Analytical kernel matrix completion with incomplete
multi-view data. In Proceedings of the International Conference on Machine Learning (ICML)
Workshop on Learning with Multiple Views  pages 80‚Äì86  2005.

[28] Linli Xu  James Neufeld  Bryce Larson  and Dale Schuurmans. Maximum margin clustering.

In Advances in neural information processing systems  pages 1537‚Äì1544  2005.

10

,Yair Marom
Dan Feldman