2009,Positive Semidefinite Metric Learning with Boosting,The learning of appropriate distance metrics is a critical problem in classification. In this work  we propose a boosting-based technique  termed BoostMetric  for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint  but does not scale well. BoostMetric is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices.  BoostMetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement  does not require tuning  and can accommodate various types of constraints.  Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.,Positive Semideﬁnite Metric Learning with Boosting

Chunhua Shen†‡  Junae Kim†‡  Lei Wang‡  Anton van den Hengel¶

† NICTA Canberra Research Lab  Canberra  ACT 2601  Australia∗
‡ Australian National University  Canberra  ACT 0200  Australia

¶ The University of Adelaide  Adelaide  SA 5005  Australia

Abstract

The learning of appropriate distance metrics is a critical problem in image classiﬁ-
cation and retrieval. In this work  we propose a boosting-based technique  termed
BOOSTMETRIC  for learning a Mahalanobis distance metric. One of the primary
difﬁculties in learning such a metric is to ensure that the Mahalanobis matrix re-
mains positive semideﬁnite. Semideﬁnite programming is sometimes used to en-
force this constraint  but does not scale well. BOOSTMETRIC is instead based
on a key observation that any positive semideﬁnite matrix can be decomposed
into a linear positive combination of trace-one rank-one matrices. BOOSTMET-
RIC thus uses rank-one positive semideﬁnite matrices as weak learners within an
efﬁcient and scalable boosting-based learning process. The resulting method is
easy to implement  does not require tuning  and can accommodate various types
of constraints. Experiments on various datasets show that the proposed algorithm
compares favorably to those state-of-the-art methods in terms of classiﬁcation ac-
curacy and running time.

1

Introduction

It has been an extensively sought-after goal to learn an appropriate distance metric in image clas-
siﬁcation and retrieval problems using simple and efﬁcient algorithms [1–5]. Such distance metrics
are essential to the effectiveness of many critical algorithms such as k-nearest neighbor (kNN)  k-
means clustering  and kernel regression  for example. We show in this work how a Mahalanobis
metric is learned from proximity comparisons among triples of training data. Mahalanobis dis-
tance  a.k.a. Gaussian quadratic distance  is parameterized by a positive semideﬁnite (p.s.d.) matrix.
Therefore  typically methods for learning a Mahalanobis distance result in constrained semideﬁnite
programs. We discuss the problem setting as well as the difﬁculties for learning such a p.s.d. ma-
trix. If we let ai  i = 1  2 · · ·   represent a set of points in RD  the training data consist of a set of
constraints upon the relative distances between these points  SS = {(ai  aj  ak)|distij < distik} 
where distij measures the distance between ai and aj. We are interested in the case that dist
computes the Mahalanobis distance. The Mahalanobis distance between two vectors takes the form:

kai − ajkX = p(ai − aj)⊤X(ai − aj)  with X < 0  a p.s.d. matrix. It is equivalent to learn a pro-

jection matrix L and X = LL⊤. Constraints such as those above often arise when it is known that ai
and aj belong to the same class of data points while ai  ak belong to different classes. In some cases 
these comparison constraints are much easier to obtain than either the class labels or distances be-
tween data elements. For example  in video content retrieval  faces extracted from successive frames
at close locations can be safely assumed to belong to the same person  without requiring the indi-
vidual to be identiﬁed. In web search  the results returned by a search engine are ranked according
to the relevance  an ordering which allows a natural conversion into a set of constraints.

∗NICTA is funded through the Australian Government’s Backing Australia’s Ability initiative  in part

through the Australian Research Council.

The requirement of X being p.s.d. has led to the development of a number of methods for learning
a Mahalanobis distance which rely upon constrained semideﬁnite programing. This approach has a
number of limitations  however  which we now discuss with reference to the problem of learning a
p.s.d. matrix from a set of constraints upon pairwise-distance comparisons. Relevant work on this
topic includes [3–8] amongst others.

Xing et al [4] ﬁrstly proposed to learn a Mahalanobis metric for clustering using convex optimiza-
tion. The inputs are two sets: a similarity set and a dis-similarity set. The algorithm maximizes the
distance between points in the dis-similarity set under the constraint that the distance between points
in the similarity set is upper-bounded. Neighborhood component analysis (NCA) [6] and large mar-
gin nearest neighbor (LMNN) [7] learn a metric by maintaining consistency in data’s neighborhood
and keeping a large margin at the boundaries of different classes. It has been shown in [7] that
LMNN delivers the state-of-the-art performance among most distance metric learning algorithms.

The work of LMNN [7] and PSDBoost [9] has directly inspired our work. Instead of using hinge
loss in LMNN and PSDBoost  we use the exponential loss function in order to derive an AdaBoost-
like optimization procedure. Hence  despite similar purposes  our algorithm differs essentially in
the optimization. While the formulation of LMNN looks more similar to support vector machines
(SVM’s) and PSDBoost to LPBoost  our algorithm  termed BOOSTMETRIC  largely draws upon
AdaBoost [10].

In many cases  it is difﬁcult to ﬁnd a global optimum in the projection matrix L [6]. Reformulation-
linearization is a typical technique in convex optimization to relax and convexify the problem [11].
In metric learning  much existing work instead learns X = LL⊤ for seeking a global optimum  e.g. 
[4  7  12  8]. The price is heavy computation and poor scalability: it is not trivial to preserve the
semideﬁniteness of X during the course of learning. Standard approaches like interior point Newton
methods require the Hessian  which usually requires O(D4) resources (where D is the input dimen-
sion). It could be prohibitive for many real-world problems. Alternative projected (sub-)gradient is
adopted in [7  4  8]. The disadvantages of this algorithm are: (1) not easy to implement; (2) many
parameters involved; (3) slow convergence. PSDBoost [9] converts the particular semideﬁnite pro-
gram in metric learning into a sequence of linear programs (LP’s). At each iteration of PSDBoost  an
LP needs to be solved as in LPBoost  which scales around O(J 3.5) with J the number of iterations
(and therefore variables). As J increases  the scale of the LP becomes larger. Another problem is
that PSDBoost needs to store all the weak learners (the rank-one matrices) during the optimization.
When the input dimension D is large  the memory required is proportional to J D2  which can be
prohibitively huge at a late iteration J . Our proposed algorithm solves both of these problems.

Based on the observation from [9] that any positive semideﬁnite matrix can be decomposed into a
linear positive combination of trace-one rank-one matrices  we propose BOOSTMETRIC for learning
a p.s.d. matrix. The weak learner of BOOSTMETRIC is a rank-one p.s.d. matrix as in PSDBoost.
The proposed BOOSTMETRIC algorithm has the following desirable properties: (1) BOOSTMETRIC
is efﬁcient and scalable. Unlike most existing methods  no semideﬁnite programming is required.
At each iteration  only the largest eigenvalue and its corresponding eigenvector are needed.
(2)
BOOSTMETRIC can accommodate various types of constraints. We demonstrate learning a Maha-
lanobis metric by proximity comparison constraints. (3) Like AdaBoost  BOOSTMETRIC does not
have any parameter to tune. The user only needs to know when to stop. In contrast  both LMNN
and PSDBoost have parameters to cross validate. Also like AdaBoost it is easy to implement. No
sophisticated optimization techniques such as LP solvers are involved. Unlike PSDBoost  we do not
need to store all the weak learners. The efﬁcacy and efﬁciency of the proposed BOOSTMETRIC is
demonstrated on various datasets.

Throughout this paper  a matrix is denoted by a bold upper-case letter (X); a column vector is
denoted by a bold lower-case letter (x). The ith row of X is denoted by Xi: and the ith column
Xij Zij calculates the
inner product of two matrices. An element-wise inequality between two vectors like u ≤ v means
ui ≤ vi for all i. We use X < 0 to indicate that matrix X is positive semideﬁnite.

X:i. Tr(·) is the trace of a symmetric matrix and hX  Zi = Tr(XZ⊤) = Pij

2 Algorithms

2.1 Distance Metric Learning

As discussed  the Mahalanobis metric is equivalent to linearly transform the data by a projection
matrix L ∈ RD×d (usually D ≥ d) before calculating the standard Euclidean distance:

dist2

ij = kL⊤ai − L⊤ajk2

2 = (ai − aj)⊤LL⊤(ai − aj) = (ai − aj)⊤X(ai − aj).

(1)

Although one can learn L directly as many conventional approaches do  in this setting  non-convex
constraints are involved  which make the problem difﬁcult to solve. As we will show  in order to
convexify these conditions  a new variable X = LL⊤ is introduced instead. This technique has been
used widely in convex optimization and machine learning such as [12]. If X = I  it reduces to the
Euclidean distance. If X is diagonal  the problem corresponds to learning a metric in which the
different features are given different weights  a.k.a. feature weighting.

In the framework of large-margin learning  we want to maximize the distance between distij and
distik. That is  we wish to make dist2
ik = (ai −ak)⊤X(ai −ak)−(ai −aj)⊤X(ai −aj) as
large as possible under some regularization. To simplify notation  we rewrite the distance between
dist2

ij −dist2

ij and dist2

ik as dist2

ij − dist2

ik = hAr  Xi 

Ar = (ai − ak)(ai − ak)⊤ − (ai − aj)(ai − aj)⊤ 

(2)

r = 1  · · ·   |SS|. |SS| is the size of the set SS.

2.2 Learning with Exponential Loss

We derive a general algorithm for p.s.d. matrix learning with exponential loss. Assume that we want
to ﬁnd a p.s.d. matrix X < 0 such that a bunch of constraints

hAr  Xi > 0  r = 1  2  · · ·  

are satisﬁed as well as possible. These constraints need not be all strictly satisﬁed. We can deﬁne
the margin ρr = hAr  Xi  ∀r. By employing exponential loss  we want to optimize

min log(cid:0)P|SS|

r=1 exp −ρr(cid:1) + v Tr(X)

s.t. ρr = hAr  Xi  r = 1  · · ·   |SS|  X < 0.

(P0)

Note that: (1) We have worked on the logarithmic version of the sum of exponential loss. This
transform does not change the original optimization problem of sum of exponential loss because
the logarithmic function is strictly monotonically decreasing. (2) A regularization term Tr(X) has
been applied. Without this regularization  one can always multiply an arbitrarily large factor to X
to make the exponential loss approach zero in the case of all constraints being satisﬁed. This trace-
norm regularization may also lead to low-rank solutions. (3) An auxiliary variable ρr  r = 1  . . .
must be introduced for deriving a meaningful dual problem  as we show later.

We can decompose X into: X = PJ

So

j=1wj Zj  with wj ≥ 0  rank(Zj) = 1 and Tr(Zj) = 1  ∀j.

ρr = hAr  Xi = DAr PJ

j=1wjhAr  Zji = PJ
Here Hrj is a shorthand for Hrj = hAr  Zji. Clearly Tr(X) = PJ

j=1wj ZjE = PJ

j=1wj Hrj = Hr:w  ∀r.

(3)

j=1wj Tr(Zj) = 1⊤w.

2.3 The Lagrange Dual Problem

We now derive the Lagrange dual of the problem we are interested in. The original problem (P0)
now becomes

In order to derive its dual  we write its Lagrangian

r=1 exp −ρr(cid:1) + v1⊤w  s.t. ρr = Hr:w  r = 1  · · ·   |SS|  w ≥ 0.

min log(cid:0)P|SS|
L(w  ρ  u  p) = log(cid:0)P|SS|

r=1 exp −ρr(cid:1) + v1⊤w +P|SS|

r=1ur(ρr − Hr:w) − p⊤w 

(P1)

(4)

max

u

−P|SS|

r=1ur log ur  s.t. u ≥ 0  1⊤u = 1  and (5).

Weak and strong duality hold under mild conditions [11]. That means  one can usually solve one
problem from the other. The KKT conditions link the optimal between these two problems. In our
case  it is

  ∀r.

(6)

u⋆

r =

exp −ρ⋆
r
k=1 exp −ρ⋆

k

P|SS|

with p ≥ 0. Here u and p are Lagrange multipliers. The dual problem is obtained by ﬁnding the
saddle point of L; i.e.  supu p inf w ρ L.

L1

L2

inf
w ρ

L = inf
ρ

z
log(cid:0)P|SS|

r=1 exp −ρr(cid:1) + u⊤ρ + inf

w

}|

{

z
(v1⊤ −P|SS|

}|
r=1urHr: − p⊤)w = −P|SS|

{

r=1ur log ur.

The inﬁmum of L1 is found by setting its ﬁrst derivative to zero and we have:

L1 = (cid:26)−Prur log ur

−∞

inf
ρ

if u ≥ 0  1⊤u = 1 
otherwise.

The inﬁmum is Shannon entropy. L2 is linear in w  hence L2 must be 0. It leads to

The Lagrange dual problem of (P1) is an entropy maximization problem  which writes

P|SS|
r=1urHr: ≤ v1⊤.

(5)

(D1)

While it is possible to devise a totally-corrective column generation based optimization procedure
for solving our problem as the case of LPBoost [13]  we are more interested in considering one-at-
a-time coordinate-wise descent algorithms  as the case of AdaBoost [10]  which has the advantages:
(1) computationally efﬁcient and (2) parameter free. Let us start from some basic knowledge of
column generation because our coordinate descent strategy is inspired by column generation.

If we knew all the bases Zj(j = 1 . . . J) and hence the entire matrix H is known  then either the
primal (P1) or the dual (D1) could be trivially solved (at least in theory) because both are convex
optimization problems. We can solve them in polynomial time. Especially the primal problem is
convex minimization with simple nonnegativeness constraints. Off-the-shelf software like LBFGS-
B [14] can be used for this purpose. Unfortunately  in practice  we do not access all the bases: the
number of possible Z’s is inﬁnite. In convex optimization  column generation is a technique that is
designed for solving this difﬁculty.

Instead of directly solving the primal problem (P1)  we ﬁnd the most violated constraint in the dual
(D1) iteratively for the current solution and add this constraint to the optimization problem. For this
purpose  we need to solve

ˆZ = argmaxZnP|SS|

r=1ur(cid:10)Ar  Z(cid:11)  s.t. Z ∈ Ω1o .

(7)

Here Ω1 is the set of trace-one rank-one matrices. We discuss how to efﬁciently solve (7) later. Now
we move on to derive a coordinate descent optimization procedure.

2.4 Coordinate Descent Optimization

We show how an AdaBoost-like optimization procedure can be derived for our metric learning prob-
lem. As in AdaBoost  we need to solve for the primal variables wj given all the weak learners up to
iteration j.

Optimizing for wj
Since we are interested in the one-at-a-time coordinate-wise optimization  we
keep w1  w2  . . .   wj−1 ﬁxed when solving for wj . The cost function of the primal problem is (in
the following derivation  we drop those terms irrelevant to the variable wj )

Cp(wj) = log(cid:2)P|SS|

r=1 exp(−ρj−1

r

) · exp(−Hrjwj)(cid:3) + vwj.

Clearly  Cp is convex in wj and hence there is only one minimum that is also globally optimal. The
ﬁrst derivative of Cp w.r.t. wj vanishes at optimality  which results in

P|SS|
r=1(Hrj − v)uj−1

r

exp(−wj Hrj) = 0.

(8)

Algorithm 1 Bisection search for wj .

Input: An interval [wl  wu] known to contain the optimal value of wj and convergence tolerance ε > 0.
repeat

· wj = 0.5(wl + wu);
· if l.h.s. of (8) > 0 then

wl = wj ;

else

wu = wj .

until wu − wl < ε ;
Output: wj .

1

2

3

4

5

6

7

If Hrj is discrete  such as {+1  −1} in standard AdaBoost  we can obtain a close-form solution
similar to AdaBoost. Unfortunately in our case  Hrj can be any real value. We instead use bisection
to search for the optimal wj . The bisection method is one of the root-ﬁnding algorithms. It repeat-
edly divides an interval in half and then selects the subinterval in which a root exists. Bisection is a
simple and robust  although it is not the fastest algorithm for root-ﬁnding. Newton-type algorithms
are also applicable here. Algorithm 1 gives the bisection procedure. We have utilized the fact that
the l.h.s. of (8) must be positive at wl. Otherwise no solution can be found. When wj = 0  clearly
the l.h.s. of (8) is positive.

Updating u The rule for updating the dual variable u can be easily obtained from (6). At iteration
j  we have

derived from (6). So once wj is calculated  we can update u as

uj
r ∝ exp −ρj

r ∝ uj−1

r

exp(−Hrjwj)  andP|SS|

r=1uj

r = 1 

uj

r =

uj−1

r

exp(−Hrjwj)

z

  r = 1  . . .   |SS| 

(9)

where z is a normalization factor so thatP|SS|

r=1uj

r = 1. This is exactly the same as AdaBoost.

2.5 Base Learning Algorithm

In this section  we show that the optimization problem (7) can be exactly and efﬁciently solved using
eigenvalue-decomposition (EVD). From Z < 0 and rank(Z) = 1  we know that Z has the format:
Z = ξξ⊤  ξ ∈ RD; and Tr(Z) = 1 means kξk2 = 1. We have

By denoting

r=1ur(cid:10)Ar  Z(cid:11) = (cid:10)P|SS|
P|SS|

r=1urAr  Z(cid:11) = ξ⊤(cid:0)P|SS|
ˆA = P|SS|

r=1urAr 

r=1urAr(cid:1)ξ.

(10)
the base learning optimization equals: maxξ ξ⊤ ˆAξ  s.t. kξk2 = 1. It is clear that the largest
eigenvalue of ˆA  λmax( ˆA)  and its corresponding eigenvector ξ1 gives the solution to the above
problem. Note that ˆA is symmetric. Also see [9] for details.

λmax( ˆA) is also used as one of the stopping criteria of the algorithm. Form the condition (5) 
λmax( ˆA) < v means that we are not able to ﬁnd a new base matrix ˆZ that violates (5)—the algorithm
converges. We summarize our main algorithmic results in Algorithm 2.

3 Experiments

3.1 Classiﬁcation on Benchmark Datasets

We evaluate BOOSTMETRIC on 15 datasets of different sizes. Some of the datasets have very high
dimensional inputs. We use PCA to decrease the dimensionality before training on these datasets
(datasets 2-6). PCA pre-processing helps to eliminate noises and speed up computation. We have

1

2

3

4

5

6

7

Algorithm 2 Positive semideﬁnite matrix learning with boosting.

Input:

• Training set triplets (ai  aj   ak) ∈ SS; Compute Ar  r = 1  2  · · ·   using (2).

• J : maximum number of iterations;
• (optional) regularization parameter v; We may simply set v to a very small value  e.g.  10−7.

Initialize: u0
for j = 1  2  · · ·   J do

r = 1

|SS|   r = 1 · · · |SS|;

· Find a new base Zj by ﬁnding the largest eigenvalue (λmax( ˆA)) and its eigenvector of ˆA in (10);
· if λmax( ˆA) < v then
break (converged);

· Compute wj using Algorithm 1;
· Update u to obtain uj

r  r = 1  · · · |SS| using (9);

Output: The ﬁnal p.s.d. matrix X ∈ RD×D  X = PJ

j=1 wj Zj .

used USPS and MNIST handwritten digits  ORL face recognition datasets  Columbia University
Image Library (COIL20)1  and UCI machine learning datasets2 (datasets 7-13)  Twin Peaks and
Helix. The last two are artiﬁcial datasets3.

Experimental results are obtained by averaging over 10 runs (except USPS-1). We randomly split the
datasets for each run. We have used the same mechanism to generate training triplets as described
in [7]. Brieﬂy  for each training point ai  k nearest neighbors that have same labels as yi (targets) 
as well as k nearest neighbors that have different labels from yi (imposers) are found. We then
construct triplets from ai and its corresponding targets and imposers. For all the datasets  we have
set k = 3 except that k = 1 for datasets USPS-1  ORLFace-1 and ORLFace-2 due to their large
size. We have compared our method against a few methods: Xing et al [4]  RCA [5]  NCA [6]
and LMNN [7]. LMNN is one of the state-of-the-art according to recent studies such as [15]. Also
in Table 1  “Euclidean” is the baseline algorithm that uses the standard Euclidean distance. The
codes for these compared algorithms are downloaded from the corresponding authors’ websites. We
have released our codes for BOOSTMETRIC at [16]. Experiment setting for LMNN follows [7]. For
BOOSTMETRIC  we have set v = 10−7  the maximum number of iterations J = 500. As we can
see from Table 1  we can conclude: (1) BOOSTMETRIC consistently improves kNN classiﬁcation
using Euclidean distance on most datasets. So learning a Mahalanobis metric based upon the large
margin concept does lead to improvements in kNN classiﬁcation. (2) BOOSTMETRIC outperforms
other algorithms in most cases (on 11 out of 15 datasets). LMNN is the second best algorithm on
these 15 datasets statistically. LMNN’s results are consistent with those given in [7]. (3) Xing et
al [4] and NCA can only handle a few small datasets. In general they do not perform very well. A
good initialization is important for NCA because NCA’s cost function is non-convex and can only
ﬁnd a local optimum.

Inﬂuence of v Previously  we claim that our algorithm is parameter-free like AdaBoost. However 
we do have a parameter v in BOOSTMETRIC. Actually  AdaBoost simply set v = 0. The coordinate-
wise gradient descent optimization strategy of AdaBoost leads to an ℓ1-norm regularized maximum
margin classiﬁer [17]. It is shown that AdaBoost minimizes its loss criterion with an ℓ1 constraint on
the coefﬁcient vector. Given the similarity of the optimization of BOOSTMETRIC with AdaBoost 
we conjecture that BOOSTMETRIC has the same property. Here we empirically prove that as long
as v is sufﬁciently small  the ﬁnal performance is not affected by the value of v. We have set v from
10−8 to 10−4 and run BOOSTMETRIC on 3 UCI datasets. Table 2 reports the ﬁnal 3NN classiﬁcation
error with different v. The results are nearly identical.

Computational time As we discussed  one major issue in learning a Mahalanobis distance is
heavy computational cost because of the semideﬁniteness constraint.

1http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php
2http://archive.ics.uci.edu/ml/
3http://boosting.googlecode.com/files/dataset1.tar.bz2

Table 1: Test classiﬁcation error rates (%) of a 3-nearest neighbor classiﬁer on benchmark datasets. Results of
NCA and Xing et al [4] on large datasets are not available either because the algorithm does not converge or
due to the out-of-memory problem.

dataset

Euclidean

Xing et al [4] RCA

NCA

LMNN

BOOSTMETRIC

1 USPS-1
2 USPS-2
3 ORLFace-1
4 ORLFace-2
5 MNIST
6 COIL20
7
8 Wine
9 Bal
Iris

Letters

10
11 Vehicle
12 Breast-Cancer
13 Diabetes
14
15 Helix

Twin Peaks

5.18
3.56 (0.28)
3.33 (1.47)
5.33 (2.70)
4.11 (0.43)
0.19 (0.21)
5.74 (0.24)
26.23 (5.52)
18.13 (1.79)
2.22 (2.10)
30.47 (2.41)
3.28 (1.06)
27.43 (2.93)
1.13 (0.09)
0.60 (0.12)

10.38 (4.81)
11.12 (2.12)
2.22 (2.10)
28.66 (2.49)
3.63 (0.93)
27.87 (2.71)

32.71
5.57 (0.33)
5.75 (2.85)
4.42 (2.08)
4.31 (0.42)
0.32 (0.29)
5.06 (0.26)
2.26 (1.95)
19.47 (2.39)
3.11 (2.15)
21.42 (2.46)
3.82 (1.15)
26.48 (1.61)
1.02 (0.09)
0.61 (0.11)

3.92 (2.01)
3.75 (1.63)

27.36 (6.31)
4.81 (1.80)
2.89 (2.58)
22.61 (3.26)
4.31 (1.10)
27.61 (1.55)

7.51
2.18 (0.27)
6.67 (2.94)
2.83 (1.77)
4.19 (0.49)
2.41 (1.80)
4.34 (0.36)
5.47 (3.01)
11.87 (2.14)
2.89 (2.58)
22.57 (2.16)
3.19 (1.43)
26.78 (2.42)
0.98 (0.11)
0.61 (0.13)

2.96
1.99 (0.24)
2.00 (1.05)
3.00 (1.31)
4.09 (0.31)
0.02 (0.07)
3.54 (0.18)
2.64 (1.59)
8.93 (2.28)
2.89 (2.78)
19.17 (2.10)
2.45 (0.95)
25.04 (2.25)
0.14 (0.08)
0.58 (0.12)

Table 2: Test error (%) of a 3-nearest neighbor classiﬁer with different values of the parameter v. Each
experiment is run 10 times. We report the mean and variance. As expected  as long as v is sufﬁciently small  in
a wide range it almost does not affect the ﬁnal classiﬁcation performance.

v

10−8

10−7

10−6

10−5

10−4

Bal
B-Cancer
Diabetes

8.98 (2.59)
2.11 (0.69)
26.0 (1.33)

8.88 (2.52)
2.11 (0.69)
26.0 (1.33)

8.88 (2.52)
2.11 (0.69)
26.0 (1.33)

8.88 (2.52)
2.11 (0.69)
26.0 (1.34)

8.93 (2.52)
2.11 (0.69)
26.0 (1.46)

Our algorithm is generally fast. It involves matrix operations and an EVD for ﬁnding its largest
eigenvalue and its corresponding eigenvector. The time complexity of this EVD is O(D2) with
D the input dimensions. We compare our algorithm’s running time with LMNN in Fig. 1 on the
artiﬁcial dataset (concentric circles). We vary the input dimensions from 50 to 1000 and keep the
number of triplets ﬁxed to 250. Instead of using standard interior-point SDP solvers that do not scale
well  LMNN heuristically combines sub-gradient descent in both the matrices L and X. At each
iteration  X is projected back onto the p.s.d. cone using EVD. So a full EVD with time complexity
O(D3) is needed. Note that LMNN is much faster than SDP solvers like CSDP [18]. As seen from
Fig. 1  when the input dimensions are low  BOOSTMETRIC is comparable to LMNN. As expected 
when the input dimensions become high  BOOSTMETRIC is signiﬁcantly faster than LMNN. Note
that our implementation is in Matlab. Improvements are expected if implemented in C/C++.

3.2 Visual Object Categorization and Detection

The proposed BOOSTMETRIC and the LMNN are further compared on four classes of the Caltech-
101 object recognition database [19]  including Motorbikes (798 images)  Airplanes (800)  Faces
(435)  and Background-Google (520). For each image  a number of interest regions are identiﬁed
by the Harris-afﬁne detector [20] and the visual content in each region is characterized by the SIFT
descriptor [21]. The total number of local descriptors extracted from the images of the four classes

800

700

600

500

400

300

200

100

)
s
d
n
o
c
e
s
(
n
u
r
r
e
p
e 
m

i
t

U 
P
C

0
0

BoostMetric

LMNN

200

400
600
input dimensions

800

1000

Figure 1: Computation time of the proposed BOOSTMET-
RIC and the LMNN method versus the input data’s dimen-
sions on an artiﬁcial dataset. BOOSTMETRIC is faster than
LMNN with large input dimensions because at each iter-
ation BOOSTMETRIC only needs to calculate the largest
eigenvector and LMNN needs a full eigen-decomposition.

 
 
 
 
20

15

10

5

0

)

%

(
r
o
b
h
g
i
e
n
t
s
e
r
a
e
n
-
3
f
o
r 
o
r
r
e
t
s
e
T

Euclidean

LMNN

BoostMetric

5.5

5

4.5

4

3.5

3

2.5

)

%

(
r 
o
b
h
g
i
e
n
t 
s
e
r
a
e
n
-
3
f 
o
r
o
r
r
e
t 
s
e
T

dim.: 100D

200D

1000 2000 3000 4000 5000 6000 7000 8000 9000

Number of triplets

Figure 2: Test error (3-nearest neighbor) of BOOSTMETRIC on the Motorbikes vs. Airplanes datasets. The
second ﬁgure shows the test error against the number of training triplets with a 100-word codebook. Test error
of LMNN is 4.7% ± 0.5% with 8631 triplets for training  which is worse than BOOSTMETRIC. For Euclidean
distance  the error is much larger: 15% ± 1%.

are about 134  000  84  000  57  000  and 293  000  respectively. This experiment includes both ob-
ject categorization (Motorbikes vs. Airplanes) and object detection (Faces vs. Background-Google)
problems. To accumulate statistics  the images of two involved object classes are randomly split as
10 pairs of training/test subsets. Restricted to the images in a training subset (those in a test subset
are only used for test)  their local descriptors are clustered to form visual words by using k-means
clustering. Each image is then represented by a histogram containing the number of occurrences of
each visual word.

Motorbikes vs. Airplanes This experiment discriminates the images of a motorbike from those
of an airplane. In each of the 10 pairs of training/test subsets  there are 959 training images and
639 test images. Two visual codebooks of size 100 and 200 are used  respectively. With the result-
ing histograms  the proposed BOOSTMETRIC and the LMNN are learned on a training subset and
evaluated on the corresponding test subset. Their averaged classiﬁcation error rates are compared
in Fig. 2 (left). For both visual codebooks  the proposed BOOSTMETRIC achieves lower error rates
than the LMNN and the Euclidean distance  demonstrating its superior performance. We also apply
a linear SVM classiﬁer with its regularization parameter carefully tuned by 5-fold cross-validation.
Its error rates are 3.87% ± 0.69% and 3.00% ± 0.72% on the two visual codebooks  respectively. In
contrast  a 3NN with BOOSTMETRIC has error rates 3.63% ± 0.68% and 2.96% ± 0.59%. Hence 
the performance of the proposed BOOSTMETRIC is comparable to or even slightly better than the
SVM classiﬁer. Also  Fig. 2 (right) plots the test error of the BOOSTMETRIC against the number of
triplets for training. The general trend is that more triplets lead to smaller errors.

Faces vs. Background-Google This experiment uses the two object classes as a retrieval prob-
lem. The target of retrieval is the face images. The images in the class of Background-Google are
randomly collected from the Internet and they are used to represent the non-target class. BOOST-
METRIC is ﬁrst learned from a training subset and retrieval is conducted on the corresponding test
subset. In each of the 10 training/test subsets  there are 573 training images and 382 test images.
Again  two visual codebooks of size 100 and 200 are used. Each face image in a test subset is used
as a query  and its distances from other test images are calculated by BOOSTMETRIC  LMNN and
the Euclidean distance. For each metric  the precision of the retrieved top 5  10  15 and 20 images
are computed. The retrieval precision for each query are averaged on this test subset and then aver-
aged over the whole 10 test subsets. BOOSTMETRIC consistently attains the highest values  which
again veriﬁes its advantages over LMNN and the Euclidean distance. With a codebook size 200 
very similar results are obtained. See [16] for the experiment results.

4 Conclusion

We have presented a new algorithm  BOOSTMETRIC  to learn a positive semideﬁnite metric using
boosting techniques. We have generalized AdaBoost in the sense that the weak learner of BOOST-
METRIC is a matrix  rather than a classiﬁer. Our algorithm is simple and efﬁcient. Experiments
show its better performance over a few state-of-the-art existing metric learning methods. We are
currently combining the idea of on-line learning into BOOSTMETRIC to make it handle even larger
datasets.

 
 
 
 
 
 
 
References

[1] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Trans.

Pattern Anal. Mach. Intell.  18(6):607–616  1996.

[2] J. Yu  J. Amores  N. Sebe  P. Radeva  and Q. Tian. Distance learning for similarity estimation.

IEEE Trans. Pattern Anal. Mach. Intell.  30(3):451–462  2008.

[3] B. Jian and B. C. Vemuri. Metric learning using Iwasawa decomposition. In Proc. IEEE Int.

Conf. Comp. Vis.  pages 1–6  Rio de Janeiro  Brazil  2007. IEEE.

[4] E. Xing  A. Ng  M. Jordan  and S. Russell. Distance metric learning  with application to

clustering with side-information. In Proc. Adv. Neural Inf. Process. Syst. MIT Press  2002.

[5] A. Bar-Hillel  T. Hertz  N. Shental  and D. Weinshall. Learning a Mahalanobis metric from

equivalence constraints. J. Mach. Learn. Res.  6:937–965  2005.

[6] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood component anal-

ysis. In Proc. Adv. Neural Inf. Process. Syst. MIT Press  2004.

[7] K. Q. Weinberger  J. Blitzer  and L. K. Saul. Distance metric learning for large margin nearest

neighbor classiﬁcation. In Proc. Adv. Neural Inf. Process. Syst.  pages 1473–1480  2005.

[8] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Proc. Adv. Neural Inf.

Process. Syst.  2005.

[9] C. Shen  A. Welsh  and L. Wang. PSDBoost: Matrix-generation linear programming for pos-
itive semideﬁnite matrices learning. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou 
editors  Proc. Adv. Neural Inf. Process. Syst.  pages 1473–1480  Vancouver  Canada  2008.

[10] R. E. Schapire. Theoretical views of boosting and applications. In Proc. Int. Conf. Algorithmic

Learn. Theory  pages 13–25  London  UK  1999. Springer-Verlag.

[11] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

[12] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semideﬁnite

programming. Int. J. Comp. Vis.  70(1):77–90  2006.

[13] A. Demiriz  K.P. Bennett  and J. Shawe-Taylor. Linear programming boosting via column

generation. Mach. Learn.  46(1-3):225–254  2002.

[14] C. Zhu  R. H. Byrd  and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B  FORTRAN
routines for large scale bound constrained optimization. ACM Trans. Math. Softw.  23(4):550–
560  1997.

[15] L. Yang  R. Jin  L. Mummert  R. Sukthankar  A. Goode  B. Zheng  S. Hoi  and M. Satya-
narayanan. A boosting framework for visuality-preserving distance metric learning and its
application to medical image retrieval. IEEE Trans. Pattern Anal. Mach. Intell. IEEE com-
puter Society Digital Library  November 2008  http://doi.ieeecomputersociety.
org/10.1109/TPAMI.2008.273.

[16] http://code.google.com/p/boosting/.

[17] S. Rosset  J. Zhu  and T. Hastie. Boosting as a regularized path to a maximum margin classiﬁer.

J. Mach. Learn. Res.  5:941–973  2004.

[18] B. Borchers. CSDP  a C library for semideﬁnite programming. Optim. Methods and Softw. 

11(1):613–623  1999.

[19] L. Fei-Fei  R. Fergus  and P. Perona. One-shot learning of object categories.

IEEE Trans.

Pattern Anal. Mach. Intell.  28(4):594–611  April 2006.

[20] K. Mikolajczyk and C. Schmid. Scale & afﬁne invariant interest point detectors. Int. J. Comp.

Vis.  60(1):63–86  2004.

[21] D. G. Lowe. Distinctive image features from scale-invariant keypoints.

Int. J. Comp. Vis. 

60(2):91–110  2004.

,Yining Wang
Yu-Xiang Wang
Aarti Singh