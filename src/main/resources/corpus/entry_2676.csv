2018,Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models,Neuroscience studies of human decision-making abilities commonly involve
subjects completing a decision-making task while BOLD signals are
recorded using fMRI. Hypotheses are tested about which brain regions
mediate the effect of past experience  such as rewards  on future
actions. One standard approach to this is model-based fMRI data
analysis  in which a model is fitted to the behavioral data  i.e.  a
subject's choices  and then the neural data are parsed to find brain
regions whose BOLD signals are related to the model's internal
signals. However  the internal mechanics of such purely behavioral
models are not constrained by the neural data  and therefore might miss
or mischaracterize aspects of the brain. To address this limitation  we
introduce a new method using recurrent neural network models that are
flexible enough to be jointly fitted to the behavioral and neural
data. We trained a model so that its internal states were suitably
related to neural activity during the task  while at the same time its
output predicted the next action a subject would execute. We then used
the fitted model to create a novel visualization of the relationship
between the activity in brain regions at different times following a
reward and the choices the subject subsequently made. Finally  we
validated our method using a previously published dataset. We found that
the model was able to recover the underlying neural substrates that were
discovered by explicit model engineering in the previous work  and also
derived new results regarding the temporal pattern of brain activity.,Integrated accounts of behavioral and neuroimaging
data using ﬂexible recurrent neural network models

Amir Dezfouli12# Richard Morris3† Fabio Ramos3‡ Peter Dayan4§ Bernard W. Balleine1⇤

1UNSW Sydney 2Data61  CSIRO 3University of Sydney 4Gatsby Unit  UCL

#akdezfuli@gmail.com †richardumorris@gmail.com §p.dayan@ucl.ac.uk

‡fabio.ramos@sydney.edu.au ⇤bernard.balleine@unsw.edu.au

Abstract

Neuroscience studies of human decision-making abilities commonly involve sub-
jects completing a decision-making task while BOLD signals are recorded using
fMRI. Hypotheses are tested about which brain regions mediate the effect of past
experience  such as rewards  on future actions. One standard approach to this is
model-based fMRI data analysis  in which a model is ﬁtted to the behavioral data 
i.e.  a subject’s choices  and then the neural data are parsed to ﬁnd brain regions
whose BOLD signals are related to the model’s internal signals. However  the
internal mechanics of such purely behavioral models are not constrained by the
neural data  and therefore might miss or mischaracterize aspects of the brain. To
address this limitation  we introduce a new method using recurrent neural network
models that are ﬂexible enough to be jointly ﬁtted to the behavioral and neural data.
We trained a model so that its internal states were suitably related to neural activity
during the task  while at the same time its output predicted the next action a subject
would execute. We then used the ﬁtted model to create a novel visualization of
the relationship between the activity in brain regions at different times following
a reward and the choices the subject subsequently made. Finally  we validated
our method using a previously published dataset. We found that the model was
able to recover the underlying neural substrates that were discovered by explicit
model engineering in the previous work  and also derived new results regarding the
temporal pattern of brain activity.

1

Introduction

Decision-making circuitry in the brain enables humans and animals to learn from the consequences
of their past actions to adjust their future choices. The role of different brain regions in this circuitry
has been the subject of extensive research in the past [Gold and Shadlen  2007  Doya  2008]  with one
of the main challenges being that decisions – and thus the neural activity that causes them – are not
only affected by the immediate events in the task  but are also affected by a potentially long history
of previous inputs  such as rewards  actions and environmental cues. As an example  assume that
subjects make choices in a bandit task while their brain activity is recorded using fMRI  and we seek
to determine which brain regions are involved in reward processing. Key signals  such as reward
prediction errors  are not only determined by the current reward  but also a potentially extensive
history of past inputs. Thus  it is inadequate merely to ﬁnd brain regions showing marked BOLD
changes just in response to reward.
An inﬂuential approach to address the above problem has been to use model-based analysis of fMRI
data [e.g.  O’Doherty et al.  2007  Cohen et al.  2017]  which involves training a computational model
using behavioral data and then searching the brain for regions whose BOLD activity is related to the
internal signals and variables of the model. Examples include ﬁtting a reinforcement-learning model

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

to the choices of subjects (with learning-rates etc. as the model parameters) and then ﬁnding the
brain regions that are related to the estimated value of each action or other variables of interest [e.g. 
Daw et al.  2006]. One major challenge for this approach is that  even if the model produces actions
similar to the subjects  the variables and summary statistics that the brain explicitly tracks might not
transparently represent the ones the hypothetical model represents. In this case  either the relevant
signals in the brain will be missed in the analysis  or the model will have to be altered manually in
the hope that the new signals in the model resemble neural activity in the brain.
In contrast  here  we propose a new approach using a recurrent neural network as a type of model that
is sufﬁciently ﬂexible [Siegelmann and Sontag  1995] to represent the potentially complex neural
computations in the brain  while also closely matching subjects’ choice behavior. In this way  the
model learns to learn the task such that (a) its output matches subjects’ choices; and (b) its internal
mechanism tracks subjects’ brain activity. A model trained using this approach ideally provides an
end-to-end model of neural decision-making circuitry that does not beneﬁt from manual engineering 
but describes how past inputs are translated to future actions through a successive set of computations
occurring in different brain regions.
Having introduced the architecture of this recurrent neural network meta-learner  we show how
to interpret it by unrolling it over space and time to determine the role of each brain region at
each time slice in the path from reward processing to action selection. We show that experimental
results obtained using our method are consistent with the previous literature on the neural basis of
decision-making and provide novel insights into the temporal dynamics of reward processing in the
brain.

2 Related work

There are at least four types of previous approach. In type one  which includes model-based fMRI
analysis and some work on complex non-linear recurrent dynamical systems [Sussillo et al.  2015] 
the models are trained on the behavioral data and are only then applied to the neural data. By contrast 
we include neural data at the outset. In a second type recurrent neural networks are trained to perform
a task [e.g.  to maximize reward; Song et al.  2017]  but without the attention that we give to both the
psychological and neural data. A third type aims to uncover the dynamics of the interaction between
different brain regions by approximating the underlying neural activity (see Breakspear [2017] for
review). However  unlike our protocol  these models are not trained on behavioral data. A fourth
type relies on two separate models for the behavioral and neural data but  unlike model-based fMRI
analyses  the free parameters of the two models are jointly modeled and estimated  e.g.  by assuming
that they follow a joint distribution [Turner et al.  2013  Halpern et al.  2018]. Nevertheless  similar to
model-based fMRI  this approach requires manual model engineering and is limited by how well the
hypothesized behavioral model characterizes its underlying neural processes.

3 The model

3.1 Data
We consider a typical neuroscience study of decision-making processes in humans  in which the data
include the actions of a set of subjects while they are making choices and receiving rewards (DBEH)
in a decision-making task  while their brain activity in the form of fMRI images is recorded (DfMRI).
Behavioral data include the states of the environment (described by set S)  choices executed by the
subjects in each state  and the rewards they receive. At each time t 2 T i subject i observes state
t 2S as an input  calculates and then executes action ai
t (e.g.  presses a button on a computer
si
keyboard; ai
t (e.g.  a monetary reward;
t 2 <). The behavioral data can be described as 
ri

t 2A and A is a set of actions) and receives a reward ri

DBEH = {(si

ti) |i = 1...NSUBJ  ti 2 T i}.

ti  ai

ti  ri

(1)

The second component of the data is the recorded brain activity in the form of 3D images taken by
the scanner during the task. Each image can be divided into a set of voxels (NVOX voxels; e.g.  3mm
x 3mm x 3mm cubes)  each of which has an intensity (a scalar number) which represents the neural
activity of the corresponding brain region at the time of image acquisition by the scanner. Images

2

at1
rt1
st

rnn
(GRU)

W
.....

h1
t
h2
t
h3
t
. . .
. . .
hNcells
t

ut

yt

.....

P (at = eye)

P (at = hand)

LfMRI(⇥)

L(⇥)

LBEH(⇥)

Figure 1: Architecture of the model. The model has a RNN layer which consists of a set of GRU cells 
and receives previous actions  rewards and the current state of the environment as inputs (NCELLS is
the number of cells in the RNN layer). The outputs/states of the RNN layer (ht) are connected to a
middle layer (shown by red circles) with the same number of units as there are voxels (NVOX); the
outputs of the units ut are weighted sums over their inputs. Each component of ut is convolved
with the HRF signal and is compared to the bias-adjusted intensity of its corresponding voxel in
fMRI recordings (yt). Voxels are shown by the squares overlaying the brain  and three of them are
highlighted (in blue) as an example of how they are connected to the units in the middle layer. The
outputs of the GRU cells are also connected to a softmax layer (the green lines)  which outputs the
probability of selecting each action on the next trial (in this case  EYE and HAND are the available
actions). LBEH refers to the behavioral loss function and LfMRI refers to the fMRI loss function. The
ﬁnal loss function is denoted by L(⇥)  which is a weighted sum of the fMRI and the behavioral loss
functions. ⇥ contains all the parameters.

are acquired at times 0  TR  2TR  . . .   (NACQ  1)TR  where TR refers to the repetition time of the
scanner (time between image acquisitions)  and NACQ is the total number of images. Let yi v
t denote
the intensity of voxel v recorded at time t for subject i. The fMRI data will take the following form:

DfMRI = {yi v

t }  t = 0  TR  2TR  . . .   (NACQ  1)TR  i = 1 . . . NSUBJ  v = 1 . . . NVOX.

(2)

3.2 Network architecture
Actions taken by a subject at each point in time are affected by the history of previous rewards 
actions and states experienced by the subject. Aspects of this history are encoded in neural activity in
a persistent  albeit mutating  form  and enable subjects’ future choices to beneﬁt from past experience.
This process constitutes learning in the task; we aim to recover it by jointly modeling the behavioral
and neural data. We ﬁrst describe the network architecture and then explain how it can be interpreted
to answer the questions of interest.
RNN layer. The model (Figure 1) is a speciﬁc form of recurrent neural network (RNN). The recurrent
layer consists of a set of NCELLS GRU cells [Gated recurrent unit; Cho et al.  2014]; cell c outputs
its state hc
]>).
This state summarizes the past history of the inputs to the network and is updated as new inputs are
received according to a function that we denote by f 

t at time t. We deﬁne ht as the state of the whole RNN network (ht = [h1

t   . . .   hNCELLS

t

ht = f (at1  rt1  st  ht1;⇥)  

(3)
depending on parameters ⇥. We aim to train the parameters of this dynamical system to approximate
the underlying neural computations in the brain that translate previous inputs to future actions during
the task.
fMRI layer. To establish a correspondence between the underlying RNN and neural activity  one
training signal for ⇥ comes from requiring the activity of each voxel at each point in time to be
described as a (noisy) linear combination of GRU cell states (shown by the red connections in Figure 1).
We denote the weights of this linear combination as W 2 <NVOX⇥NCELLS  and ut as a vector of size
NVOX representing predicted neural activity at each voxel at time t. Thus 
(4)

ut = W ht.

3

For training the model  the predicted neural activity is compared with the actual activity recorded by
the scanner. However  neural activity is not instantly reﬂected in the intensity recorded by the scanner 
but is delayed according to the haemodynamic response function (HRF; Figure S3). To correct for
this delay  elements of ut are ﬁrst convolved with HRF [Henson and Friston  2007]  and after adding
a bias term b  are compared with the intensities of the corresponding voxels  to form the following
loss function 

LfMRI(⇥) =Xt kut ~ HRF + b  ytk2   t 2{ 0  TR  . . . (NACQ  1)TR} 

(5)

in which ⇥ is the model parameters (W and RNN parameters)  yt is a vector of size NVOX containing
the recorded activity of each voxel at time t. Symbol ~ is the convolution operator. The above
loss function can be thought of as the logarithm of a Gaussian likelihood function. Note that in this
case the convolution operator acts on the output of the network  and so this is not a conventional
convolutional neural network  in which convolutions act on the input.
Behavioral layer. To ensure that the RNN also captures the behavioral data  a second training signal
for ⇥ comes from requiring it to produce actions similar to those of humans. This is achieved by
connecting the output of the RNN network to a softmax layer in Figure 1 (shown by the green lines) 
in which the weights of the connections determine the inﬂuence of each cell on the probability of
selecting actions. Denoting by ⇡t(a) the predicted probability of taking action a at time t  we deﬁne
the behavioral loss function as:

LBEH(⇥) = Xt2T 0

log ⇡t(at) 

(6)

in which T 0 refers to the timesteps at which the subject was allowed to execute an action.
Training. We deﬁne the overall loss function as the weighted sum of the behavioral and fMRI loss
functions 

L(⇥) =

LBEH(⇥;Di) + LfMRI(⇥;Di) 

(7)

NSUBJXi=1

with parameter  determining the contribution of the fMRI loss function  and Di denoting the data of
subject i. Note the above loss function can be thought of as the logarithm of the multiplication of a
Gaussian likelihood function (for the fMRI part) – with  being related to the level of noise/variance
in the likelihood function – and a multinomial likelihood function (for the behavioral part).

Interpreting the model

3.3
We seek to understand how the inputs to the network (previous rewards  actions  states) affect future
actions through the medium of the brain’s neural activity. Although different methods have been
suggested for investigating the way the inputs to a neural network determine its outputs  the most
fundamental quantity is the gradient of the output with respect to the input  which represents how
much the output changes by changing the input (as used  for instance  by Simonyan et al. [2013] in
the context of an image classiﬁcation task).
Inspired by this  we deﬁned two differential quantities relating rewards  actions and brain activity to
each other. There are at least two ‘layers’ to this: off- and on-policy. In the off-policy setting  which
is conventionally studied in model-based imaging  there is a ﬁxed sequence of inputs  whose effects
on future predicted probabilities and neural activities we determine. In the on-policy setting  which is
used in settings such as approximate Bayesian computation [Sunnåker et al.  2013]  future choices 
and thus future inputs are also affected by past inputs. For the present  we consider the simpler 
off-policy setting. This allows us to look  for instance  at the brain regions involved in mediating the
effect of the reward that subject i actually received at  say  time t1 on the predicted probability of
the action that the subject actually executed at  say  time t2. For convenience  we drop notation for
the ﬁxed inputs for the subject; and indeed for the subject number (since we ﬁt a single model to the
whole group).
The ﬁrst measure represents the behavioral effects of reward on future actions  which can be calculated
as the gradient of the predicted probabilities of actions at each time t2 with respect to the input
received at time t1. For the case of binary choices  which are the focus of the current experiment  with

4

EYE and HAND as the two available actions in the task  we only needed to calculate the probability
for one of the actions. Let ⇡t2 denote the probability of taking action EYE at time t2. The effect of
reward at time t1 on the action at time t2 can be calculated as follows 

d⇡r
t1 t2 =

@⇡t2
@rt1

.

This is a straightforward application of backpropagation (calculated using automatic differentiation) 
noting again that we consider the inputs received by the network between t1 and t2 to be ﬁxed. d⇡r
t1 t2
can be thought as capturing how much the probability of taking action EYE at time t2 increases as the
results of increasing the magnitude of reward earned at time t1.
The second measure relates behavioral and fMRI data by exploiting the informational association
between the predicted neural activity ut and the state of RNN  ht. First  note that  at each time t  ht
is a Markov state for the RNN  in that given ht  RNN outputs after time t are independent of their past.
Thus  we can decompose:

@⇡t2
@rt1

=

NCELLSXk=1

@⇡t2
@hk
t

@hk
t
@rt1

 

for any

t 2{ t1 + 1 . . . t2} 

(8)

as the effect changing rt1 has on the predicted RNN state hk
t at time t  times the effect that a change
in hk
t has on the action probability ⇡t2 at t2. Now  consider the case that W >W is non-singular (note
that NVOX  NCELLS). This implies that there is a one-to-one mapping between the RNN state and
predicted neural activity:

ht = (W >W )1W >ut .

(9)

Thus  we can rewrite equation 8 in terms of the effect changing rt1 has on the predicted neural
activity uv
t implies about a change in ⇡t2  operating
implicitly via what the change in uv

t in each voxel at time t times what a change in uv

t tells us about a change in ht. We can write this as 

@⇡t2
@rt1

=

NVOXXv=1

@⇡t2
@uv
t

@uv
t
@rt1

  t = t1 + 1 . . . t2.

(10)

Note that this is a correlational relationship – the direction of causality is from ht to ut. Nevertheless
the individual terms in this sum:

@uv
t
@rt1

(11)

t /@rt1) 
t ). This

d⇡ur
v t1 t t2 =

  for any t 2{ t1 + 1 . . . t2} 

@⇡t2
@uv
t
combine the inﬂuence that voxel uv
t at time t receives from the reward at time t1 (which is @uv
with the covariation between the voxel activity and the action at time t2 (which is @⇡t2/@uv
quantiﬁes the intermediation of voxel uv
t between the reward at t1 and the action at time t2.
We make two remarks: (i) The joint ﬁtting of the model to both the behavioral and fMRI data was
important that  if there are behaviorally equivalent solutions  then the one that can ﬁt the neural data
should be chosen; and (ii) for equation 10 to hold it is necessary for the state of the network to be
fully determined by the neural activity (ut). This can hold in the case of GRU cells (provided that the
hidden units do not partition into separate behavioral and neural groups). In contrast  in LSTM cells
[Long short-term memory; Hochreiter and Schmidhuber  1997]  the cell states and cell outputs are
different and are both required to determine the outputs in the next time-step  and therefore in the
case of LSTM cells equation 10 does not hold.

4 Results

In this section we aim to show how the above measures can be used to study the neural substrates of
decision-making in the brain.

5

A

make choice

2.5 sec

delay

3.5 sec

outcome
revealed
1 sec

ITI

1-8 sec

A

Ve
Vh

SMA
(0  -12  78)

preSEF
(-6  9  60)

1C

 

0.5

 

1D
0.6
0.2

j

i

B

t

0.6

6

4

0

50

0

50

)
.

.

100

150

100

150

d
r
a
w
e
r
 
P

1
0.5

Figure 2: The task. Each trial started with the presentation of a screen (the left most square in the
ﬁgure) and the subjects had 2.5 seconds to make an eye saccade to the red target circle or press
a button with their right hand. After a delay (3.5 second) during which the screen showed only a
ﬁxation point  subjects received the outcome of their choice which indicated whether their choice was
rewarded. The next trial started after an inter-trial interval (ITI) that varied between 1 and 8 seconds.
Figure reprinted with permission from Wunderlich et al. [2009]. Copyright (2009) National Academy
of Sciences.

0 0.2
1
model choice prob.

time (choice trial)
choice eye
choice hand

model choice probability
subject choice

s
e
c
o
h
c
 
’
s
s

time (choice trial)

e
y
e
e
s
o
o
h
c
P

u
a
(
 
e

x = -6

B

a
m

2

i
t
s
E

n
o
C

 
t
s
a
r
t

4.2 Model settings

4.1 Task and subjects

Fig. 1.
Experimental Design and Behavior. (A) Subjects were presented with a
choice cue after which they had to respond within 2.5 s by performing a saccade
to the red target circle or a right handed button press. Once a response was
registered the screen was immediately cleared for a short delay and subsequently
the outcome was revealed (6 s after trial onset) indicating either receipt of reward
or no reward. Inter-trial-intervals varied between 1 and 8 s. (B) Example reward
probabilities for saccades and button presses as a function of the trial number.
The probability of being rewarded following choice of either the hand or eye
movement was varied across the experiment independently for each movement.
(C) Fitted model choice probability (red) and actual choice behavior (blue) shown
for a single subject. (D) Actual choice behavior versus model predicted choice
probability. Data are pooled across subjects  the regression slope is shown as a
line  vertical bars  SEM.

0
The data used here were previously published in Wunderlich et al. [2009]. The structure of the
decision-making task is shown in Figure 2. In each trial subjects had a choice between making
a saccade (EYE) or pressing a button (HAND). Choices were rewarded with varying probabilities
−2
across the experiment. There were four trial types in the task: (i) free-choice trials (150 trials)  in
which subjects could choose between EYE and HAND; (ii) forced-choice trials in which subjects
were instructed to choose EYE (50 trials) or (iii) HAND (50 trials); (iv) null trials in which no
reward was received irrespective of the action selected (50 trials). Forced-choices and null trails
were randomly inserted between the free-choice trials. The environment consisted of two actions
(EYE and HAND) and ﬁve states corresponding to the four trial types and one state when the choice
outcomes were shown (reward or no-reward). Actions and states are assumed to be coded using
one-hot representations. Since time was discretized (see below)  there were time points at which no
action was taken or no visual stimulus was shown  in which case states and actions were coded using
zero vectors.
The total number of subjects was NSUBJ = 22  and in total NACQ = 1136 images were acquired by
the scanner each containing NVOX = 63191 voxels. Therefore  the fMRI data can be summarized as
a tensor of size 22 ⇥ 1136 ⇥ 63191. Each subject made ⇠ 300 choices. See Supplementary Material
for the details of fMRI preprocessing and model settings.

To look for neural correlates of action values we had to estimate
the value of taking each action in every trial. We calculated the
action values using a computational reinforcement-learning (RL)
model in which the value of each action  Veye and Vhand  was
updated in proportion to a prediction error on each trial (see Table
S1 for a summary of how the different types of value signals relate
to the components of the experiment). The model also assumed
that action selection in every trial followed a soft-max probability
rule based on the difference of the estimated action values (8). To
test for the presence of action value signals in the brain we took the
model predicted trial-by-trial estimates of the two action values and
entered these into a regression analysis against the fMRI data. In
addition to a whole brain screening for the presence of action-value
signals  we specifically looked for them in areas known to be
involved in the planning of motor actions  including supplementary
motor cortex (18–21) and lateral parietal cortex (22  23). Given that
both of these areas have previously been shown to contain value-
related signals for movements in nonhuman primates  and that they
are closely interconnected with the area of motor cortex involved
in carrying out motor actions (24–26)  we considered these areas
prime candidates for containing action-value representations that
could then be used to guide action-based choices. It is important to
emphasize  however  that the tasks used in previous studies did not
make it possible to determine if the value signals identified were
chosen values or action values.

Figure 3(a b) shows two sets of off-policy simulations. In each simulation there are four choice states 
the times of which are shown by the vertical gray patches in the top panels. The red patch following
each grey ribbon shows the time at which the outcome was revealed following the choice. The ﬁrst
choice was rewarded (shown by ‘R’ in the graph)  but the rest were not. In panel (a) action HAND
was selected in all choice states whereas in panel (b) it was action EYE. Based on this  since in
panel (a) the reward was earned when HAND was selected  we expected that choice to decrease the
probability of selecting action EYE on the next choice. This is shown by the blue bars which illustrate
the gradient of the probability of selecting the EYE action at each subsequent choice with respect to
the amount of reward earned after the ﬁrst choice (d⇡r). For panel (b)  since the reward was earned
as a consequence of choosing EYE in the ﬁrst choice  we expected the reward to have a positive effect
on the probability of selecting EYE on the next trials  which is consistent with the graph.
Next we asked about the intermediation of each brain region between the reward earned after the ﬁrst
choice (t1) and the next choice (t2)  shown by the red arrow in Figure 3(a). To answer this question 

We also looked for areas that are involved in comparing the
action values to make a choice. Two areas of a priori interest were
the anterior cingulate cortex (ACC) and the dorsal striatum. ACC
has been previously implicated in action-based choice  both in the
context of a human imaging study reporting activity in this area
during a task involving choices between different actions compared
to a situation involving responses guided by instruction (27)  and in
a monkey lesion study where ACC lesions produced an impairment
in action-outcome based choice but not in mediating changes in
responses following errors (28). Dorsal striatum has been impli-
cated in both goal-directed and habitual instrumental responding
for reward in rodents (29  30). Moreover  human fMRI studies
reveal increased activity in both of these regions when subjects

All the methods were implemented in Tensorﬂow [Abadi et al.  2016] and gradients (for both
optimization and interpretation of the model) were calculated using automatic differentiation methods
available in this package. See Supplementary Material for the model settings.

4.3 From reward to action

6

z = +60

preSEF

Ve

Vh

Ve

Fig. 2. Action values. (A) Region of supplementary motor area showing cor-
relations with action values for hand movement (Vh/green) and a region of
pre-SEF showing correlations with action-values for eye movements (Ve/red).
T-maps are shown from a whole brain analysis thresholded at P ⬍ 0.001 uncor-
rected (see Fig. S1 for a version with color bars relating to t stats). (B) Average
effect sizes of Ve (red) and Vh (green) extracted from SEF and SMA. The effects
shown here were calculated from trials independent of those used to functionally
identify the ROI. Note that only Ve but not Vh modulate the signal in preSEF  and
that activity in SMA shows the opposite pattern. Vertical lines  SEM.

make choices to obtain reward compared to an otherwise analogous
situation in which the rewards are obtained without the need to
make a choice (31–34).

The most simple type of comparison process would be to
compute a difference between the two action values. We tested for
such a difference  but as we had no a priori hypothesis about the
directionality of the computation  we tested for both the difference
between the value of the action chosen and the value of action not
chosen (Vchosen ⫺ Vunchosen)  and one involving the opposite
difference (Vunchosen ⫺ Vchosen). As we found evidence for such an
action-value comparison signal in the brain  we then proposed a
simple computational model to provide a conceptual explanation as
to how such a signal could reflect the output of a computationally
plausible decision mechanism.

Results
RL Model Fits to Behavioral Choice Data. A comparison of the choice
probabilities predicted by the RL model and the soft-max proce-
dure to subjects’ actual behavior suggests that the model matches
subjects behavior well. Fig. 1C compares both variables for a typical
subject. Fig. 1D compares the predicted choice probability (binned)
against the actual choice probabilities for the group. A similar linear
regression analysis at the individual level generated an average R2
across subjects of 0.83 and regression coefficients that were signif-
icant at P ⬍ 0.001 in each subject.

Action Values. We found neural activity correlating with the action
values for making a hand movement in left supplementary motor
area (SMA; Fig. 2A and Table S2). A region of interest (ROI)
analysis showed that activity in this area satisfied the properties of
a hand action value: it was sensitive to the value of hand movements 
and it showed no response selectivity to the value of eye movements
(Fig. 2B). Activity in lateral parietal cortex  ACC  and right dorsal

we calculated d⇡ur for every voxel and every time-step between t1 and t2  and masked out the voxels
that were not in the top one percent. By focusing only on the 99th percentile of |d⇡ur|  we hoped to
limit our analysis to the circuitry known to be involved in decision-making. The resulting voxel maps
are shown in Figure 3(c) for the case of HAND action corresponding to the inputs shown in panel
(a)  and Figure 3(d) shows the time-course of changes in d⇡ur. See Figure S1(c d) for EYE action
corresponding to the inputs shown in panel (b).
The results show that  for each action  the top 1% of voxels contain three key cortical and subcortical
brain regions known to be critically involved in reward-processing and decision-making  i.e.  (i)
striatum (associative aStr; or ventral  vStr)  (ii) anterior cingulate cortex (ACC) and (iii) supplementary
motor area (SMA) [Rangel and Hare  2010  Wunderlich et al.  2009]. We ﬁrst note that these
anatomical regions are among the same anatomical regions that Wunderlich et al. [2009] also
identiﬁed as involved in decision-making in this task (see Figure S4 for the time course of changes in
d⇡ur for the voxel coordinates reported in Wunderlich et al. [Table S3; 2009]).
Secondly  we can see that not only are the identiﬁed regions consistent with the neural substrates
of decision-making based on previous work  but the temporal order of engagement of these regions
is also consistent with their functional role in decision-making. It has been argued that activity in
subregions of the striatum reﬂect reward prediction-errors [O’Doherty et al.  2004] and that these
errors serve to update action-values in the ACC [Dayan and Balleine  2002  Wunderlich et al.  2009 
Seo and Lee  2007  Walton et al.  2004]  which in turn must be compared in the SMA to determine
the best action before a decision can be made [Wunderlich et al.  2009]. Such prior work has argued
that these different decision-making signals are carried by separate regions in a corticostriatal loop 
which is assumed to participate in a time course of events leading to action-selection [Balleine and
O’Doherty  2010  Hare et al.  2011].
Here we show for the ﬁrst time the temporal dynamics between these critical regions in the striatum 
anterior cingulate cortex and motor areas leading to action-selection. Figure 3(d) shows the time
course of each region’s d⇡ur between the reward at 9.2 s (t1) and the next response at 12.8 s (t2).
Note that since we took the probability of taking the EYE action as the reference  negative values
of d⇡ur indicate a region’s role in selecting the HAND action. At reward receipt (9.2 s)  d⇡ur of the
ventral striatum begins below the zero baseline and then (negatively) peaks at 9.8 s  as it mediates
the effect of reward prediction-errors on the subsequent hand response. The value of d⇡ur for the
anterior cingulate then (negatively) peaks after 10.4 s  consistent with its role in updating action
values with the new errors before the next response. Finally d⇡ur for the large cluster in the motor
area (including the supplementary motor area) controlling motor responses such as the HAND action 
negatively peaks at the time of the action (12.8 s)  which marks the end of the decision process in the
current task.
As part of our supplementary material  Figure S1(d) shows the time dynamics between the striatum 
anterior cingulate and motor areas controlling EYE choices – corresponding to the inputs shown in
panel (b). Here positive values of d⇡ur indicate a region’s role in selecting the EYE action. At reward
receipt (9.2 s) the associative striatum is involved immediately in mediating the effect of reward on
the subsequent action-selection. Then at 11 s the involvement of the anterior cingulate peaks before
a region in the motor area nearest the supplementary eye ﬁeld peaks at the time of action (12.8 s).
In sum  changes in d⇡ur over this time period mirror those for the HAND action  and are consistent
with the hypothesized roles of these regions in the varying decision stages of the reward-learning task
used here.

5 Discussion

We have introduced a new neural architecture for investigating the neural substrates of decision-
making in the brain. Unlike previous methods  our approach does not require manual engineering and
is able to learn computational processes directly from the data. We further showed that the model can
be interpreted to uncover the temporal engagement of different brain regions in choice and reward
processing. Besides being used as a standalone analysis tool  this approach can inform model-based
fMRI analyses to investigate whether the model correctly tracks the brain’s internal mechanism. That
is  if a brain region is found to be important in the current analysis  but not using the model-based
fMRI analysis  this could mean that the model used to extract neural information is not representing
all of the relevant neural signals involved in decision-making and requires further modiﬁcation.

7

a) HAND action

to how such a signal could reflect the output of a computationally
plausible decision mechanism.

to how such a signal could reflect the output of a computationally
plausible decision mechanism.

related signals for movements in nonhuman primates  and that they
are closely interconnected with the area of motor cortex involved
in carrying out motor actions (24–26)  we considered these areas
prime candidates for containing action-value representations that
could then be used to guide action-based choices. It is important to
emphasize  however  that the tasks used in previous studies did not
make it possible to determine if the value signals identified were
chosen values or action values.

related signals for movements in nonhuman primates  and that they
are closely interconnected with the area of motor cortex involved
in carrying out motor actions (24–26)  we considered these areas
prime candidates for containing action-value representations that
could then be used to guide action-based choices. It is important to
emphasize  however  that the tasks used in previous studies did not
make it possible to determine if the value signals identified were
chosen values or action values.

Figure 3(a b) shows two sets of off-policy simulations. In each simulation there are four choice states 
the times of which are shown by the vertical gray patches in the top panels. The red patch following
each grey ribbon shows the time at which the outcome was revealed following the choice. The ﬁrst
choice was rewarded (shown by ‘R’ in the graph)  but the rest were not. In panel (a) action HAND
was selected in all choice states whereas in panel (b) it was action EYE. Based on this  since in
b) EYE action
panel (a) the reward was earned when HAND was selected  we expected that choice to decrease the
probability of selecting action EYE on the next choice. This is shown by the blue bars which illustrate
the gradient of the probability of selecting the EYE action at each subsequent choice with respect to
the amount of reward earned after the ﬁrst choice (d⇡r). For panel (b)  since the reward was earned
as a consequence of choosing EYE in the ﬁrst choice  we expected the reward to have a positive effect
on the probability of selecting EYE on the next trials  which is consistent with the graph.
Next we asked about the intermediation of each brain region between the reward earned after the ﬁrst
choice (t1) and the next choice (t2)  shown by the red arrow in Figure 3(a). To answer this question 

Figure 3(a b) shows two sets of off-policy simulations. In each simulation there are four choice states 
the times of which are shown by the vertical gray patches in the top panels. The red patch following
each grey ribbon shows the time at which the outcome was revealed following the choice. The ﬁrst
choice was rewarded (shown by ‘R’ in the graph)  but the rest were not. In panel (a) action HAND
was selected in all choice states whereas in panel (b) it was action EYE. Based on this  since in
panel (a) the reward was earned when HAND was selected  we expected that choice to decrease the
probability of selecting action EYE on the next choice. This is shown by the blue bars which illustrate
the gradient of the probability of selecting the EYE action at each subsequent choice with respect to
the amount of reward earned after the ﬁrst choice (d⇡r). For panel (b)  since the reward was earned
as a consequence of choosing EYE in the ﬁrst choice  we expected the reward to have a positive effect
on the probability of selecting EYE on the next trials  which is consistent with the graph.
Next we asked about the intermediation of each brain region between the reward earned after the ﬁrst
choice (t1) and the next choice (t2)  shown by the red arrow in Figure 3(a). To answer this question 

Results
RL Model Fits to Behavioral Choice Data. A comparison of the choice
probabilities predicted by the RL model and the soft-max proce-
dure to subjects’ actual behavior suggests that the model matches
0.15
subjects behavior well. Fig. 1C compares both variables for a typical
We also looked for areas that are involved in comparing the
0.10
subject. Fig. 1D compares the predicted choice probability (binned)
action values to make a choice. Two areas of a priori interest were
against the actual choice probabilities for the group. A similar linear
H
0.05
the anterior cingulate cortex (ACC) and the dorsal striatum. ACC
regression analysis at the individual level generated an average R2
0.00
has been previously implicated in action-based choice  both in the
across subjects of 0.83 and regression coefficients that were signif-
−0.05
context of a human imaging study reporting activity in this area
icant at P ⬍ 0.001 in each subject.
~26
time (s)
during a task involving choices between different actions compared
to a situation involving responses guided by instruction (27)  and in
Action Values. We found neural activity correlating with the action
c)
a monkey lesion study where ACC lesions produced an impairment
values for making a hand movement in left supplementary motor
in action-outcome based choice but not in mediating changes in
area (SMA; Fig. 2A and Table S2). A region of interest (ROI)
6
responses following errors (28). Dorsal striatum has been impli-
analysis showed that activity in this area satisfied the properties of
cated in both goal-directed and habitual instrumental responding
a hand action value: it was sensitive to the value of hand movements 
for reward in rodents (29  30). Moreover  human fMRI studies
and it showed no response selectivity to the value of eye movements
reveal increased activity in both of these regions when subjects
(Fig. 2B). Activity in lateral parietal cortex  ACC  and right dorsal

We also looked for areas that are involved in comparing the
action values to make a choice. Two areas of a priori interest were
the anterior cingulate cortex (ACC) and the dorsal striatum. ACC
has been previously implicated in action-based choice  both in the
context of a human imaging study reporting activity in this area
~26
during a task involving choices between different actions compared
to a situation involving responses guided by instruction (27)  and in
a monkey lesion study where ACC lesions produced an impairment
in action-outcome based choice but not in mediating changes in
10.4 s
responses following errors (28). Dorsal striatum has been impli-
cated in both goal-directed and habitual instrumental responding
for reward in rodents (29  30). Moreover  human fMRI studies
reveal increased activity in both of these regions when subjects

Results
RL Model Fits to Behavioral Choice Data. A comparison of the choice
probabilities predicted by the RL model and the soft-max proce-
dure to subjects’ actual behavior suggests that the model matches
subjects behavior well. Fig. 1C compares both variables for a typical
E
subject. Fig. 1D compares the predicted choice probability (binned)
against the actual choice probabilities for the group. A similar linear
regression analysis at the individual level generated an average R2
across subjects of 0.83 and regression coefficients that were signif-
icant at P ⬍ 0.001 in each subject.
time (s)
Action Values. We found neural activity correlating with the action
values for making a hand movement in left supplementary motor
area (SMA; Fig. 2A and Table S2). A region of interest (ROI)
analysis showed that activity in this area satisfied the properties of
a hand action value: it was sensitive to the value of hand movements 
and it showed no response selectivity to the value of eye movements
(Fig. 2B). Activity in lateral parietal cortex  ACC  and right dorsal

~0 ~3 ~6 ~9

~0 ~3 ~6 ~9

0.3
0.2
0.1
0.0

11.0 s

11.6 s

12.2 s

12.8 s

r
 
π
d

r
 
π
d

9.8 s

9.2 s

~13

~16

~19

~23

~13

~16

~19

~23

~29

~29

R

H

R

H

H

6

E

E

E

 

m
17200 兩 www.pnas.org兾cgi兾doi兾10.1073兾pnas.0901077106
u
t
a
i
r
t
s

m
u
t
a
i
r
t
s

e
h
t

r
o
f

f
o

i
t

s
e
u
l
a
v

n
o
i
t
c
a

e
m

e
h
t

e
h
t

s

s
e
t
a
i
d
e
m

.
)
2
t
(

 

 

n
i

.
)
s

k
s
a
t

e
h
t

e
r
o
f
e
b

d
r
a
w
e
r

n
i
n
w
o
h
s

n
o
d
r
a
w
e
r

d
n
n
17200 兩 www.pnas.org兾cgi兾doi兾10.1073兾pnas.0901077106
o
a
i
t
g
c
a
n
i
D
r
e
N
e
A
n
i
H
g
n
e
e
h
t

t
n
e
t
s
i
s
n
o
c

s
s
e
c
o
r
p

n
o
i
t
c
a

s
t
u
p
n
i

s
k
a
e
p

8
.
2
1
(

e
h
t

-
n
o
i
s
i
c
e
d

f
o

A

t

.

n
o
i
t
c
a

r
o
t
o
m
e
h
t
n
i

r
e
t
s
u
l
c

g
n
i
t
a
d
p
u

n
i

e
l
o
r

s
t
i

h
t
i

w

t
n
e
t
s
i
s
n
o
c

 
s

4
.
0
1

r
e
t
f
a

n
o
i
s
i
c
e
d

e
h
t

d
n
e

e
h
t

s
k
r
a
m
9.2
h
c
i
h
w

 
)
s

s
a

e
g
r
a
l

d)
40

e
h
t

r
o
f

h
c
u
s

s
e
s
n
o
p
s
e
r

0

r
u
r
⇡
o
d
t
o
y
m
l
l
-40
a
g
n
n
i
i
F
l
l
o
r
t
n
o
c

.
e
s
n
o
p
s
e
r

f
o

x10-5

g
n
i
n
r
a
e
l
-
d
r
a
w
e
r

e
h
t

f
o

s
e
g
a
t
s

n
o
i
s
i
c
e
d

e
r
a
d
n
a

 
n
o
i
t
c
a
D
N
A
H
e
h
t

r
o
f

e
s
o
h
t

10.4

g
n
i
y
r
a
v

vStr

e
h
t

r
o
r
r
i

m
d
o
i
r
e
p
e
m

e
t
a
l
u
g
n
i
c

r
o
i
r
e
t
n
a

e
h
t

f
o

t
n
e
m
e
v
l
o
v
n
i

e
h
t

f
o
t
c
e
f
f
e

e
h
t
g
n
i
t
a
i
d
e
m
n
i
y
l
e
t
a
i
d
e
m
m

i
d
e
v
l
o
v
n
i

f
o

e
m

i
t

e
h
t

t
a

s
k
a
e
p

d
l
e
ﬁ
e
y
e

y
r
a
t
n
e
m
e
l
p
p
u
s

E
Y
E

e
h
t

g
n
i
t
c
e
l
e
s

n
i

e
l
o
r

e
h
t
o
t
g
n
i
d
n
o
p
s
e
r
r
o
c
–
s
e
c
i
o
h
c

9.8

s
’
n
o
i
g
e
r

a

E
Y
E

n
e
e
w
t
e
b

s
c
i
m
a
n
y
d

e
m

i
t

e
h
t

s
w
o
h
s

)
d
(
1
S
e
r
u
g
i
F

11.0
ACC

t
a
h
T

d
e
s
a
b
-
l
e
d
o
m
m
r
o
f
n
i

n
a
c

h
c
a
o
r
p
p
a

s
i
h
t

 
l
o
o
t

.

m

s
i
n
a
h
c
e
m

l
a
n
r
e
t
n
i

s
’
n
i
a
r
b

e
h
t

s
k
c
a
r
t

s
i
s
y
l
a
n
a

y
l
t
c
e
r
r
o
c

d
e
s
a
b
-
l
e
d
o
m
e
h
t
g
n
i
s
u
t
o
n
t
u
b
 
s
i
s
y
l
a
n
a

t
n
e
r
r
u
c

g
n
i
t
n
e
s
e
r
p
e
r

t
o
n

s
i

n
o
i
t
a
m
r
o
f
n
i

l
a
r
u
e
n

t
c
a
r
t
x
e

o
t

d
e
s
u

.

n
o
i
t
a
c
ﬁ

i
d
o
m

r
e
h
t
r
u
f

s
e
r
i
u
q
e
r
d
n
a

g
n
i
k
a
m
-
n
o
i
s
i
c
e
d

e
h
t
n
i

12.2

n
a
c

l
e
d
o
m
e
h
t

t
a
h
t

d
e
w
o
h
s

r
e
h
t
r
u
f

e

W

.
a
t
a
d

e
h
t

m
o
r
f

d
r
a
w
e
r
d
n
a

e
c
i
o
h
c
n
i

s
n
o
i
g
e
r
n
i
a
r
b
t
n
e
r
e
f
f
i
d
f
o
t
n
e
m
e
g
a
g
n
e

s
e
t
a
r
t
s
b
u
s

l
a
r
u
e
n

e
h
t

g
n
i
t
a
g
i
t
s
e
v
n
i

r
o
f

l
a
u
n
a
m
e
r
i
u
q
e
r

t
o
n
s
e
o
d
h
c
a
o
r
p
p
a

11.6
PMC

r
u
o

Wunderlich et al.

12.8

7

e
h
t

n
i

s
w
o
h
s

8
.
2
1

t
a

e
s
n
o
p
s
e
r

t
x
e
n

e
h
t

d
n
a

e
v
i
t
a
g
e
n

 
e
c
n
e
r
e
f
e
r

e
h
t

s
a

n
o
i
t
c
a

)
1
t
(

s

2
.
9

t
a

E
Y
E

e
h
t

)
d
(
3

e
r
u
g
i

F

.
n
o
i
t
c
e
l
e
s
-
n
o
i
t
c
a

o
t

g
n
i
d
a
e
l

d
r
a
w
e
r

g
n
i
k
a
t

s
n
o
i
g
e
r

l
a
c
i
t
i
r
c

e
s
e
h
t

n
e
e
w
t
e
b

s
c
i
m
a
n
y
d

l
a
r
o
p
m
e
t

e
h
t

e
m

i
t

s
a
e
r
a

r
o
t
o
m
d
n
a

t
s
r
ﬁ
e
h
t

x
e
t
r
o
c

r
o
f

w
o
h
s

e
t
a
l
u
g
n
i
c

e
h
t

n
e
e
w
t
e
b

r
u
⇡
d

s
’
n
o
i
g
e
r

h
c
a
e

f
o

f
o

y
t
i
l
i
b
a
b
o
r
p

e
h
t

k
o
o
t

e
w
e
c
n
i
s

t
a
h
t

e
t
o
N

e
w
e
r
e
H

r
o
i
r
e
t
n
a

e
s
r
u
o
c

.
]
1
1
0
2

 
.
l
a

t
e

e
r
a
H

 

0
1
0
2

 

y
t
r
e
h
o
D
O

’

r
u
⇡
d

r
u
⇡
d

t
i

s
a

 
s
8
.
9
t
a

s
k
a
e
p
)
y
l
e
v
i
t
a
g
e
n
(
n
e
h
t
d
n
a

e
n
i
l
e
s
a
b
o
r
e
z

e
h
t

w
o
l
e
b
s
n
i
g
e
b
m
u
t
a
i
r
t
s

 
)
s
2

.

9
(

t
p
i
e
c
e
r
d
r
a
w
e
r

t

A

.

n
o
i
t
c
a
D
N
A
H
e
h
t
g
n
i
t
c
e
l
e
s
n
i

e
l
o
r

s
’
n
o
i
g
e
r

a

e
t
a
c
i
d
n
i

r
u
⇡
d
f
o

l
a
r
t
n
e
v

f
o

e
u
l
a
v

e
h
T

.
e
s
n
o
p
s
e
r

d
n
a
h

t
n
e
u
q
e
s
b
u
s

e
h
t

n
o

s
r
o
r
r
e
-
n
o
i
t
c
i
d
e
r
p

d
r
a
w
e
r

f
o

t
c
e
f
f
e

e
h
t

s

i
t

s
i

t
a

n
i

n
i

1
1

e
h
t

e
h
t

)
a
e
r
a

8
.
2
1
(

n
e
h
T

s
a
e
r
a

n
o
i
t
c
a

s
n
o
i
g
e
r

r
u
⇡
d
f
o

y
l
t
c
e
r
i
d

e
t
a
c
i
d
n
i

t
x
e
n
e
h
t

m
u
t
a
i
r
t
s

 
s
d
o
h
t
e
m

t
n
a
t
r
o
p
m

l
e
d
o
m
e
h
t

l
e
d
o
m
e
h
t

e
n
o
l
a
d
n
a
t
s

g
n
i
l
l
o
r
t
n
o
c

e
r
u
t
c
e
t
i
h
c
r
a

r
o
t
o
m
y
r
a
t
n
e
m
e
l
p
p
u
s

Figure 3: (a b). The graphs show the effect of reward on actions in terms of d⇡r. The choice states
(between EYE and HAND actions) are shown by the grey shaded area. In the left panel  action HAND
(shown by ‘H’) was selected and in the right panel action EYE (shown by ‘E’) was selected at all
of the choice states. The outcome of each choice (reward/no reward) was delivered in red shaded
area. The ﬁrst choice was rewarded  as shown by ‘R’ in the graph  but the other choices were not
followed by any reward. The blue bars show the effect of reward received after the ﬁrst choice on
the subsequent choices (d⇡r). (c). Voxel maps and the time-course of changes in d⇡ur in cortical
e
r
and subcortical brain regions between reward of the HAND action at 9.2 s and the response at 12.8 s
o
f
shown by the red arrow in panel (a). Voxels below the 99th percentile of voxels were masked to reveal
e
b
only the top one percent of voxels shown here. (d) The time courses of each region calculated from
s
r
the maximum voxel in that region at each time point (smoothed)  selected within an anatomical mask
o
r
from wfu_pickatlas. y-axis represents d⇡ur. ACC: anterior cingulate cortex; vStr: ventral striatum;
r
e
PMC: primary motor cortex. See Figure S1 for the voxel maps and time course changes relating to
w
the EYE action.
e
n
e
h
t
h
t
i

 
l
a
i
r
e
t
a
m
y
r
a
t
n
e
m
e
l
p
p
u
s

r
e
h
t
e
h
w
e
t
a
g
i
t
s
e
v
n
i

.
n
o
i
t
c
e
l
e
s
-
n
o
i
t
c
a

s
u
o
i
v
e
r
p
e
k
i
l
n
U

l
a
n
o
i
t
a
t
u
p
m
o
c

e
b
o
t
d
n
u
o
f

n
a
e
m
d
l
u
o
c

l
a
r
u
e
n
w
e
n

e
v
i
t
a
i
c
o
s
s
a

d
e
c
u
d
o
r
t
n
i

r
o
t
o
m
d
n
a

r
o
t
o
m
e
h
t

s
e
s
s
e
c
o
r
p

l
a
r
o
p
m
e
t

d
e
v
l
o
v
n
i

t
n
a
v
e
l
e
r

s
e
d
i
s
e
B

e
v
i
t
i
s
o
p

r
u
⇡
d
n
i

t
s
e
r
a
e
n

s
l
a
n
g
i
s

l
a
r
u
e
n

s
e
u
l
a
v

s
k
a
e
p

g
n
i
e
b

e
s
e
h
t

n
r
a
e
l

e
r
e
H

s
e
l
o
r

d
e
s
u

r
e
v
o

a
e
r
a

d
e
z
i
s
e
h
t
o
p
y
h

e
t
a
l
u
g
n
i
c

t
a
h
t

)
s
2

s
i
h
t

s
i
h
t

r
u
o

e
h
t

e
h
t

e
h
t

e
h
t

e
m

s
a

f
o

f
o

n
i

o
t

t
a

i
t

a

a

i

s
e
g
n
a
h
c

.

s
k
a
e
p

)
y
l
e
v
i
t
a
g
e
n
(

n
e
h
t

e
t
a
l
u
g
n
i
c

r
e
v
o
c
n
u
o
t
d
e
t
e
r
p
r
e
t
n
i

s
i
n
o
i
g
e
r
n
i
a
r
b
a

f
i

 
s
i

 
s
i
s
y
l
a
n
a

e
h
t

I

R
M

f

f
o
l
l
a

s
e
s
y
l
a
n
a

I

R
M

f

.
g
n
i
s
s
e
c
o
r
p

.
n
i
a
r
b
e
h
t
n
i
g
n
i
k
a
m

e
v
a
h

e

W

o
t

e
l
b
a

s
i

e
b

n
o
i
s
s
u
c
s
i
D
8

5

g
n
i
d
u
l
c
n
i
(

a
e
r
a

k
s
a
t

t
n
e
r
r
u
c

y
l
e
v
i
t
a
g
e
n

r
o
i
r
e
t
n
a

w
s
e
u
l
a
v

t
n
e
u
q
e
s
b
u
s

e
h
t

f
o

.
9
(

.
)
b
(

t
r
a
p

s
A

r
o
i
r
e
t
n
a

t
p
i
e
c
e
r

l
e
n
a
p

n
o
i
g
e
r

a

 

m
u
s
n
I

.
e
r
e
h

e
h
t

h
t
i

w

d
e
s
u

Acknowledgments
AD and BWB were supported by funding from UNSW Sydney and the National Health and Medical
Research Council of Australia GNT1079561. PD was funded by the Gatsby Charitable Foundation.
Part of this work was conducted whilst PD was at Uber Technologies. Neither body played a part in
its design  execution or communication. PD is afﬁliated with Max Planck Institute for Biological
Cybernetics  Tübingen  Germany (peter.dayan@tuebingen.mpg.de).

References
Joshua I Gold and Michael N Shadlen. The neural basis of decision making. Annual review of neuroscience  30 

2007.

Kenji Doya. Modulators of decision making. Nature neuroscience  11(4):410–6  apr 2008.

John P O’Doherty  Alan Hampton  and Hackjin Kim. Model-based fMRI and its application to reward learning

and decision making. Annals of the New York Academy of sciences  1104(1):35–53  2007.

Jonathan D Cohen  Nathaniel Daw  Barbara Engelhardt  Uri Hasson  Kai Li  Yael Niv  Kenneth A Norman 
Jonathan Pillow  Peter J Ramadge  Nicholas B Turk-Browne  and Others. Computational approaches to fMRI
analysis. Nature neuroscience  20(3):304  2017.

Nathaniel D Daw  John P O’Doherty  Peter Dayan  Ben Seymour  and Raymond J. Dolan. Cortical substrates for

exploratory decisions in humans. Nature  441(7095):876–9  jun 2006.

Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of computer

and system sciences  50(1):132–150  1995.

David Sussillo  Mark M. Churchland  Matthew T. Kaufman  and Krishna V. Shenoy. A neural network that ﬁnds
a naturalistic solution for the production of muscle activity. Nature Neuroscience  18(7):1025–1033  2015.

H. Francis Song  Guangyu R. Yang  and Xiao Jing Wang. Reward-based training of recurrent neural networks

for cognitive and value-based tasks. eLife  6:1–24  2017.

Michael Breakspear. Dynamic models of large-scale brain activity. Nature neuroscience  20(3):340  2017.

Brandon M Turner  Birte U Forstmann  Eric-Jan Wagenmakers  Scott D Brown  Per B Sederberg  and Mark
Steyvers. A Bayesian framework for simultaneously modeling neural and behavioral data. NeuroImage  72:
193–206  2013.

David Halpern  Shannon Tubridy  Hong Yu Wang  Camille Gasser  Pamela Osborn Popp  Lila Davachi  and
Todd M Gureckis. Knowledge Tracing Using the Brain. In Proceedings of the 11th International Conference
on Educational Data Mining  EDM  2018.

Kyunghyun Cho  Bart Van Merriënboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares  Holger Schwenk 
and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078  2014.

Richard Henson and Karl J Friston. CHAPTER 14 - Convolution Models for fMRI. In Karl Friston  John
Ashburner  Stefan Kiebel  Thomas Nichols  and William Penny  editors  Statistical Parametric Mapping 
pages 178–192. Academic Press  London  2007. ISBN 978-0-12-372560-8.

Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks: Visualising

image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034  2013.

Mikael Sunnåker  Alberto Giovanni Busetto  Elina Numminen  Jukka Corander  Matthieu Foll  and Christophe

Dessimoz. Approximate Bayesian Computation. PLOS Computational Biology  9(1):1–10  2013.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

Klaus Wunderlich  Antonio Rangel  and John P O’Doherty. Neural computations underlying action-based
decision making in the human brain. Proceedings of the National Academy of Sciences  106(40):17199–17204 
2009.

Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S Corrado  Andy
Davis  Jeffrey Dean  Matthieu Devin  and Others. Tensorﬂow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467  2016.

9

Antonio Rangel and Todd Hare. Neural computations associated with goal-directed choice. Current opinion in

neurobiology  20(2):262–270  2010.

John P O’Doherty  Peter Dayan  Johannes Schultz  Ralf Deichmann  Karl J Friston  and Raymond J. Dolan.
Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science  304(5669):452–4  apr
2004.

Peter Dayan and Bernard W Balleine. Reward  motivation  and reinforcement learning. Neuron  36(2):285–98 

2002. ISSN 0896-6273.

Hyojung Seo and Daeyeol Lee. Temporal ﬁltering of reward signals in the dorsal anterior cingulate cortex during

a mixed-strategy game. Journal of neuroscience  27(31):8366–8377  2007.

Mark E Walton  Joseph T Devlin  and Matthew F S Rushworth. Interactions between decision making and

performance monitoring within prefrontal cortex. Nature neuroscience  7(11):1259  2004.

Bernard W Balleine and John P O’Doherty. Human and rodent homologies in action control: corticostriatal

determinants of goal-directed and habitual action. Neuropsychopharmacology  35(1):48–69  jan 2010.

Todd A Hare  Wolfram Schultz  Colin F Camerer  John P O’Doherty  and Antonio Rangel. Transformation of
stimulus value signals into motor commands during simple choice. Proceedings of the National Academy of
Sciences  108(44):18120–18125  2011.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its

recent magnitude. COURSERA: Neural networks for machine learning  4(2):26–31  2012.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization.

arXiv:1412.6980  2014.

arXiv preprint

10

,Amir Dezfouli
Richard Morris
Fabio Ramos
Peter Dayan
Bernard Balleine