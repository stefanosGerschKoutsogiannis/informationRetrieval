2015,Fast and Memory Optimal Low-Rank Matrix Approximation,In this paper  we revisit the problem of constructing a near-optimal rank $k$ approximation of a matrix $M\in [0 1]^{m\times n}$ under the streaming data model where the columns of $M$ are revealed sequentially. We present SLA (Streaming Low-rank Approximation)  an algorithm that is asymptotically accurate  when $k s_{k+1} (M) = o(\sqrt{mn})$ where $s_{k+1}(M)$ is the $(k+1)$-th largest singular value of $M$. This means that its average mean-square error converges to 0 as $m$ and $n$ grow large (i.e.  $\|\hat{M}^{(k)}-M^{(k)} \|_F^2 = o(mn)$ with high probability  where $\hat{M}^{(k)}$ and $M^{(k)}$ denote the output of SLA and the optimal rank $k$ approximation of $M$  respectively). Our algorithm makes one pass on the data if the columns of $M$ are revealed in a random order  and two passes if the columns of $M$ arrive in an arbitrary order. To reduce its memory footprint and complexity  SLA uses random sparsification  and samples each entry of $M$ with a small probability $\delta$. In turn  SLA is memory optimal as its required memory space scales as $k(m+n)$  the dimension of its output. Furthermore  SLA is computationally efficient as it runs in $O(\delta  kmn)$ time (a constant number of operations is made for each observed entry of $M$)  which can be as small as $O(k\log(m)^4 n)$ for an appropriate choice of $\delta$ and if $n\ge m$.,Fast and Memory Optimal Low-Rank Matrix

Approximation

Se-Young Yun
MSR  Cambridge

seyoung.yun@inria.fr

Marc Lelarge ∗
Inria & ENS

marc.lelarge@ens.fr

Alexandre Proutiere †
KTH  EE School / ACL

alepro@kth.se

Abstract

In this paper  we revisit the problem of constructing a near-optimal rank k approx-
imation of a matrix M ∈ [0  1]m×n under the streaming data model where the
columns of M are revealed sequentially. We present SLA (Streaming Low-rank
√
Approximation)  an algorithm that is asymptotically accurate  when ksk+1(M ) =
mn) where sk+1(M ) is the (k + 1)-th largest singular value of M. This
o(
means that its average mean-square error converges to 0 as m and n grow large
(i.e.  (cid:107) ˆM (k)−M (k)(cid:107)2
F = o(mn) with high probability  where ˆM (k) and M (k) de-
note the output of SLA and the optimal rank k approximation of M  respectively).
Our algorithm makes one pass on the data if the columns of M are revealed in
a random order  and two passes if the columns of M arrive in an arbitrary order.
To reduce its memory footprint and complexity  SLA uses random sparsiﬁcation 
and samples each entry of M with a small probability δ. In turn  SLA is memory
optimal as its required memory space scales as k(m+n)  the dimension of its out-
put. Furthermore  SLA is computationally efﬁcient as it runs in O(δkmn) time (a
constant number of operations is made for each observed entry of M)  which can
be as small as O(k log(m)4n) for an appropriate choice of δ and if n ≥ m.

1

Introduction

We investigate the problem of constructing  in a memory and computationally efﬁcient man-
ner  an accurate estimate of the optimal rank k approximation M (k) of a large (m × n) matrix
M ∈ [0  1]m×n. This problem is fundamental in machine learning  and has naturally found nu-
merous applications in computer science. The optimal rank k approximation M (k) minimizes  over
all rank k matrices Z  the Frobenius norm (cid:107)M − Z(cid:107)F (and any norm that is invariant under rota-
tion) and can be computed by Singular Value Decomposition (SVD) of M in O(nm2) time (if we
assume that m ≤ n). For massive matrices M (i.e.  when m and n are very large)  this becomes
unacceptably slow. In addition  storing and manipulating M in memory may become difﬁcult. In
this paper  we design a memory and computationally efﬁcient algorithm  referred to as Streaming
Low-rank Approximation (SLA)  that computes a near-optimal rank k approximation ˆM (k). Under
mild assumptions on M  the SLA algorithm is asymptotically accurate in the sense that as m and n
grow large  its average mean-square error converges to 0  i.e.  (cid:107) ˆM (k) − M (k)(cid:107)2
F = o(mn) with high
probability (we interpret M (k) as the signal that we aim to recover form a noisy observation M).
To reduce its memory footprint and running time  the proposed algorithm combines random sparsi-
ﬁcation and the idea of the streaming data model. More precisely  each entry of M is revealed to
the algorithm with probability δ  called the sampling rate. Moreover  SLA observes and treats the
∗Work performed as part of MSR-INRIA joint research centre. M.L. acknowledges the support of the
†A. Proutiere’s research is supported by the ERC FSA grant  and the SSF ICT-Psi project.

French Agence Nationale de la Recherche (ANR) under reference ANR-11-JS02-005-01 (GAP project).

1

columns of M one after the other in a sequential manner. The sequence of observed columns may
be chosen uniformly at random in which case the algorithm requires one pass on M only  or can be
arbitrary in which case the algorithm needs two passes. SLA ﬁrst stores (cid:96) = 1/(δ log(m)) randomly
selected columns  and extracts via spectral decomposition an estimator of parts of the k top right
singular vectors of M. It then completes the estimator of these vectors by receiving and treating the
remain columns sequentially. SLA ﬁnally builds  from the estimated top k right singular vectors  the
linear projection onto the subspace generated by these vectors  and deduces an estimator of M (k).
The analysis of the performance of SLA is presented in Theorems 7  and 8. In summary:
when m ≤ n  log4(m)

m ≤ δ ≤ m−8/9  with probability 1 − kδ  the output ˆM (k) of SLA satisﬁes:

(cid:107)M (k) − ˆM (k)(cid:107)2

mn

F

= O

k2

k+1(M )

mn

+

log(m)√
δm

 

(1)

(cid:18)

(cid:18) s2

(cid:19)(cid:19)

where sk+1(M ) is the (k + 1)-th singular value of M. SLA requires O(kn) memory space  and if
δ ≥ log4(m)
m and k ≤ log6(m)  its time is O(δkmn). To ensure the asymptotic accuracy of SLA  the
√
mn). In the
upper-bound in (1) needs to converge to 0 which is true as soon as ksk+1(M ) = o(
case where M is seen as a noisy version of M (k)  this condition quantiﬁes the maximum amount of
noise allowed for our algorithm to be asymptotically accurate.
SLA is memory optimal  since any rank k approximation algorithm needs to at least store its output 
i.e.  k right and left singular vectors  and hence needs at least O(kn) memory space. Further observe
that among the class of algorithms sampling each entry of M at a given rate δ  SLA is computational
optimal  since it runs in O(δkmn) time (it does a constant number of operations per observed entry
if k = O(1)). In turn  to the best of our knowledge  SLA is both faster and more memory efﬁcient
than existing algorithms. SLA is the ﬁrst memory optimal and asymptotically accurate low rank
approximation algorithm.
The approach used to design SLA can be readily extended to devise memory and computationally
efﬁcient matrix completion algorithms. We present this extension in the supplementary material.
Notations. Throughout the paper  we use the following notations. For any m × n matrix A  we de-
note by A(cid:62) its transpose  and by A−1 its pseudo-inverse. We denote by s1(A) ≥ ··· ≥ sn∧m(A) ≥
0  the singular values of A. When matrices A and B have the same number of rows  [A  B] to denote
the matrix whose ﬁrst columns are those of A followed by those of B. A⊥ denotes an orthonormal
basis of the subspace perpendicular to the linear span of the columns of A. Aj  Ai  and Aij de-
note the j-th column of A  the i-th row of A  and the entry of A on the i-th line and j-th column 
respectively. For h ≤ l  Ah:l (resp. Ah:l) is the matrix obtained by extracting the columns (resp.
lines) h  . . .   l of A. For any ordered set B = {b1  . . .   bp} ⊂ {1  . . .   n}  A(B) refers to the matrix
composed by the ordered set B of columns of A. A(B) is deﬁned similarly (but for lines). For real
numbers a ≤ b  we deﬁne |A|b
a)ij = min(b  max(a  Aij)).
Finally  for any vector v  (cid:107)v(cid:107) denotes its Euclidean norm  whereas for any matrix A  (cid:107)A(cid:107)F denotes
its Frobenius norm  (cid:107)A(cid:107)2 its operator norm  and (cid:107)A(cid:107)∞ its (cid:96)∞-norm  i.e.  (cid:107)A(cid:107)∞ = maxi j |Aij|.

a the matrix with (i  j) entry equal to (|A|b

2 Related Work

Low-rank approximation algorithms have received a lot of attention over the last decade. There are
two types of error estimate for these algorithms: either the error is additive or relative.
To translate our bound (1) in an additive error is easy:

(cid:107)M − ˆM (k)(cid:107)F ≤ (cid:107)M − M (k)(cid:107)F + O

k

√
sk+1(M )

mn

+

log1/2 m
(δm)1/4

mn

.

(2)

(cid:32)

(cid:32)

(cid:33)√

(cid:33)

Sparsifying M to speed-up the computation of a low-rank approximation has been proposed in the
literature and the best additive error bounds have been obtained in [AM07]. When the sampling rate
δ satisﬁes δ ≥ log4 m

m   the authors show that with probability 1 − exp(− log4 m) 
(cid:107)M (k)(cid:107)1/2

(cid:107)M − ˜M (k)(cid:107)F ≤ (cid:107)M − M (k)(cid:107)F + O

(cid:18) k1/2n1/2

.

(3)

k1/4n1/4

(cid:19)

+

F

δ1/2

δ1/4

2

mn.

(cid:17)√

mn

(cid:16) 1

This performance guarantee is derived from Lemma 1.1 and Theorem 1.4 in [AM07]. To
compare (2) and (3)  note that our assumptions on the bounded entries of M ensures that:
s2
k+1(M )
In particular  we see that the worst case
nm which is always lower than the worst case bound for (2):

(cid:16) k1/2√
k and (cid:107)M (k)(cid:107)F ≤ (cid:107)M(cid:107)F ≤ √
(cid:17)1/2 √

≤ 1
bound for (3) is

+ k1/4
(δm)1/4

δm

k + log m√

δm
nm. When k = O(1)  our bound is only larger by a logarithmic term in m
k
compared to [AM07]. However  the algorithm proposed in [AM07] requires to store O(δmn) en-
tries of M whereas SLA needs O(n) memory space. Recall that log4 m ≤ δm ≤ m1/9 so that our
algorithm makes a signiﬁcant improvement on the memory requirement at a low price in the error
guarantee bounds. Although biased sampling algorithms can reduce the error  the algorithm have to
run leverage scores with multiple passes over data [BJS15]. In a recent work  [CW13] proposes a
time efﬁcient algorithm to compute a low-rank approximation of a sparse matrix. Combined with
[AM07]  we obtain an algorithm running in time O(δmn) + O(nk2 + k3) but with an increased
additive error term.
We can also compare our result to papers providing an estimate ˜M (k) of the optimal low-rank ap-
proximation of M with a relative error ε  i.e. such that (cid:107)M − ˜M (k)(cid:107)F ≤ (1 + ε)(cid:107)M − M (k)(cid:107)F . To
the best of our knowledge  [CW09] provides the best result in this setting. Theorem 4.4 in [CW09]
shows that provided the rank of M is at least 2(k + 1)  their algorithm outputs with probability 1− η
a rank-k matrix ˜M (k) with relative error ε using memory space O (k/ε log(1/η)(n + m)) (note that
in [CW09]  the authors use as unit of memory a bit whereas we use as unit of memory an entry of the
matrix so we removed a log mn factor in their expression to make fair comparisons). To compare
with our result  we can translate our bound (1) in a relative error  and we need to take:

√

mn

 .

k

ε = O

sk+1(M ) + log1/2 m
(δm)1/4
(cid:107)M − M (k)(cid:107)F

√
m + n) while sk+1(M ) = Θ(

First note that since M is assumed to be of rank at least 2(k + 1)  we have (cid:107)M − M (k)(cid:107)F ≥
sk+1(M ) > 0 and ε is well-deﬁned. Clearly  for our ε to tend to zero  we need (cid:107)M − M (k)(cid:107)F to
be not too small. For the scenario we have in mind  M is a noisy version of the signal M (k) so that
M −M (k) is the noise matrix. When every entry of M −M (k) is generated independently at random
√
with a constant variance  (cid:107)M − M (k)(cid:107)F = Θ(
n). In such a case 
we have ε = o(1) and we improve the memory requirement of [CW09] by a factor ε−1 log(kδ)−1.
[CW09] also considers a model where the full columns of M are revealed one after the other in an
arbitrary order  and proposes a one-pass algorithm to derive the rank-k approximation of M with the
same memory requirement. In this general setting  our algorithm is required to make two passes on
the data (and only one pass if the order of arrival of the column is random instead of arbitrary). The
running time of the algorithm scales as O(kmnε−1 log(kδ)−1) to project M onto kε−1 log(kδ)−1
dimensional random space. Thus  SLA improves the time again by a factor of ε−1 log(kδ)−1.
We could also think of using sketching and streaming PCA algorithms to estimate M (k). When the
columns arrive sequentially  these algorithms identify the left singular vectors using one-pass on the
matrix and then need a second pass on the data to estimate the right singular vectors. For example 
[Lib13] proposes a sketching algorithm that updates the p most frequent directions as columns are
observed. [GP14] shows that with O(km/ε) memory space (for p = k/ε)  this sketching algorithm
ﬁnds m × k matrix ˆU such that (cid:107)M − P ˆU M(cid:107)F ≤ (1 + ε)(cid:107)M − M (k)(cid:107)F   where P ˆU denotes
the projection matrix to the linear span of the columns of ˆU. The running time of the algorithm
is roughly O(kmnε−1)  which is much greater than that of SLA. Note also that to identify such
matrix ˆU in one pass on M  it is shown in [Woo14] that we have to use Ω(km/ε) memory space.
This result does not contradict the performance analysis of SLA  since the latter needs two passes
on M if the columns of M are observed in an arbitrary manner. Finally  note that the streaming
PCA algorithm proposed in [MCJ13] does not apply to our problem as this paper investigates a very
speciﬁc problem: the spiked covariance model where a column is randomly generated in an i.i.d.
manner.

3 Streaming Low-rank Approximation Algorithm

3

Algorithm 1 Streaming Low-rank Approximation (SLA)

1

Input: M  k  δ  and (cid:96) =
1. A(B1)  A(B2) ← independently sample entries of [M1  . . .   M(cid:96)] at rate δ
2. PCA for the ﬁrst (cid:96) columns: Q ← SPCA(A(B1)  k)
3. Trimming the rows and columns of A(B2):

δ log(m)

A(B2) ← set the entries of rows of A(B2) having more than two non-zero entries to 0
A(B2) ← set the entries of the columns of A(B2) having more than 10mδ non-zero entries to 0

4. W ← A(B2)Q
Remove A(B1)  A(B2)  and Q from the memory space
for t = (cid:96) + 1 to n do

5. ˆV (B1) ← (A(B1))(cid:62)W

7. At ← sample entries of Mt at rate δ
Remove At from the memory space

6. ˆI ← A(B1)

ˆV (B1)

8. ˆV t ← (At)(cid:62)W

9. ˆI ← ˆI + At ˆV t

end for
10. ˆR ← ﬁnd ˆR using the Gram-Schmidt process such that ˆV ˆR is an orthonormal matrix
11. ˆU ← 1
ˆδ
Output: ˆM (k) = | ˆU ˆV (cid:62)|1

ˆI ˆR ˆR(cid:62)

0

Algorithm 2 Spectral PCA (SPCA)

Input: C ∈ [0  1]m×(cid:96)  k
Ω ← (cid:96) × k Gaussian random matrix
Trimming: ¯C ← set the entries of the rows of C with more than 10 non-zero entries to 0
Φ ← ¯C(cid:62) ¯C − diag( ¯C(cid:62) ¯C)
Power Iteration: QR ← QR decomposition of Φ(cid:100)5 log((cid:96))(cid:101)Ω
Output: Q

1

In this section  we present the Streaming Low-rank Approximation (SLA) algorithm and analyze
its performance. SLA makes one pass on the matrix M  and is provided with the columns of M
one after the other in a streaming manner. The SVD of M is M = U ΣV (cid:62) where U and V are
(m× m) and (n× n) unitary matrices and Σ is the (m× n) matrix diag(s1(M )  . . . sn∧m(M )). We
assume (or impose by design of SLA) that the (cid:96) (speciﬁed below) ﬁrst observed columns of M are
chosen uniformly at random among all columns. An extension of SLA to scenarios where columns
are observed in an arbitrary order is presented in §3.5  but this extension requires two passes on M.
To be memory efﬁcient  SLA uses sampling. Each observed entry of M is erased (i.e.  set equal to
0) with probability 1 − δ  where δ > 0 is referred to as the sampling rate. The algorithm  whose
pseudo-code is presented in Algorithm 1  proceeds in three steps:
δ log(m) columns of M chosen uniformly at random. These
1. In the ﬁrst step  we observe (cid:96) =
columns form the matrix M(B) = U Σ(V (B))(cid:62)  where B denotes the ordered set of the indexes of
the (cid:96) ﬁrst observed columns. M(B) is sampled at rate δ. More precisely  we apply two independent
sampling procedures  where in each of them  every entry of M(B) is sampled at rate δ. The two
resulting independent random matrices A(B1)  and A(B2) are stored in memory. A(B1)  referred to
as A(B) to simplify the notations  is used in this ﬁrst step  whereas A(B2) will be used in subsequent
steps. Next through a spectral decomposition of A(B)  we derive a ((cid:96) × k) orthonormal matrix Q
such that the span of its column vectors approximates that of the column vectors of V (B)
1:k . The ﬁrst
step corresponds to Lines 1 and 2 in the pseudo-code of SLA.
2. In the second step  we complete the construction of our estimator of the top k right singular
vectors V1:k of M. Denote by ˆV the k × n matrix formed by these estimated vectors. We ﬁrst
compute the components of these vectors corresponding to the set of indexes B as ˆV (B) = A(cid:62)
(B1)W
with W = A(B2)Q. Then for t = (cid:96) + 1  . . .   n  after receiving the t-th column Mt of M  we set
ˆV t = A(cid:62)
t W   where At is obtained by sampling entries of Mt at rate δ. Hence after one pass on
M  we get ˆV = ˜A(cid:62)W   where ˜A = [A(B1)  A(cid:96)+1  . . .   An]. As it turns out  multiplying W by ˜A(cid:62)
ampliﬁes the useful signal contained in W   and yields an accurate approximation of the span of the

4

top k right singular vectors V1:k of M. The second step is presented in Lines 3  4  5  7 and 8 in SLA
pseudo-code.
3. In the last step  we deduce from ˆV a set of column vectors gathered in matrix ˆU such that ˆU(cid:62) ˆV
provides an accurate approximation of M (k). First  using the Gram-Schmidt process  we ﬁnd ˆR
δ A ˆV ˆR ˆR(cid:62) in a streaming manner as in
such that ˆV ˆR is an orthonormal matrix and compute ˆU = 1
δ A ˆV ˆR( ˆV ˆR)(cid:62) where ˆV ˆR( ˆV ˆR)(cid:62) approximates the projection matrix onto
Step 2. Then  ˆU ˆV (cid:62) = 1
the linear span of the top k right singular vectors of M. Thus  ˆU ˆV (cid:62) is close to M (k). This last step
is described in Lines 6  9  10 and 11 in SLA pseudo-code.
In the next subsections  we present in more details the rationale behind the three steps of SLA  and
provide a performance analysis of the algorithm.

3.1 Step 1. Estimating right-singular vectors of the ﬁrst batch of columns

The objective of the ﬁrst step is to estimate V (B)
1:k   those components of the top k right singular
vectors of M whose indexes are in the set B (remember that B is the set of indexes of the (cid:96) ﬁrst
observed columns). This estimator  denoted by Q  is obtained by applying the power method to
extract the top k right singular vector of M(B)  as described in Algorithm 2. In the design of this
algorithm and its performance analysis  we face two challenges: (i) we only have access to a sampled
version A(B) of M(B); and (ii) U Σ(V (B))(cid:62) is not the SVD of M(B) since the column vectors of
V (B)
1:k are not orthonormal in general (we keep the components of these vectors corresponding to the
set of indexes B). Hence  the top k right singular vectors of M(B) that we extract in Algorithm 2 do
not necessarily correspond to V (B)
1:k .
To address (i)  in Algorithm 2  we do not directly extract the top k right singular vectors of A(B).
We ﬁrst remove the rows of A(B) with too many non-zero entries (i.e.  too many observed entries
from M(B))  since these rows would perturb the SVD of A(B). Let us denote by ¯A the obtained
trimmed matrix. We then form the covariance matrix ¯A(cid:62) ¯A  and remove its diagonal entries to
obtain the matrix Φ = ¯A(cid:62) ¯A − diag( ¯A(cid:62) ¯A). Removing the diagonal entries is needed because of
the sampling procedure. Indeed  the diagonal entries of ¯A(cid:62) ¯A scale as δ  whereas its off-diagonal
entries scale as δ2. Hence  when δ is small  the diagonal entries would clearly become dominant in
the spectral decomposition. We ﬁnally apply the power method to Φ to obtain Q. In the analysis of
the performance of Algorithm 2  the following lemma will be instrumental  and provides an upper
bound of the gap between Φ and (M(B))(cid:62)M(B) using the matrix Bernstein inequality (Theorem 6.1
[Tro12]). All proofs are detailed in Appendix.
Lemma 1 If δ ≤ m− 8
9   with probability 1 − 1
some constant c1 > 1.

(cid:96)2   (cid:107)Φ − δ2(M(B))(cid:62)M(B)(cid:107)2 ≤ c1δ(cid:112)m(cid:96) log((cid:96))  for

To address (ii)  we ﬁrst establish in Lemma 2 that for an appropriate choice of (cid:96)  the column vectors
of V (B)
1:k are approximately orthonormal. This lemma is of independent interest  and relates the SVD
of a truncated matrix  here M(B)  to that of the initial matrix M. More precisely:
Lemma 2 If δ ≤ m−8/9  there exists a (cid:96)× k matrix ¯V (B) such that its column vectors are orthonor-
mal  and with probability 1 − exp(−m1/7)  for all i ≤ k satisfying that s2

(cid:107)(cid:112) n

1:i − ¯V (B)

1:i (cid:107)2 ≤ m− 1
3 .

(cid:96) V (B)

δ(cid:96)

i (M ) ≥ n

(cid:112)m(cid:96) log((cid:96)) 
(cid:112)m(cid:96) log((cid:96))). However 
(cid:112)m(cid:96) log((cid:96))  i ≤ k}.

when the corre-

δ(cid:96)

i

Note that as suggested by the above lemma  it might be impossible to recover V (B)
sponding singular value si(M ) is small (more precisely  when s2
the singular vectors corresponding to such small singular values generate very little error for low-
rank approximation. Thus  we are only interested in singular vectors whose singular values are
above the threshold ( n
δ(cid:96)
Now to analyze the performance of Algorithm 2 when applied to A(B)  we decompose Φ as Φ =
1:k(cid:48) )(cid:62) is a noise matrix. The
δ2(cid:96)
n

(cid:112)m(cid:96) log((cid:96)))1/2. Let k(cid:48) = max{i : s2

1:k(cid:48) )(cid:62) + Y   where Y = Φ − δ2(cid:96)

i (M ) ≥ n

i (M ) ≤ n

1:k(cid:48) (Σ1:k(cid:48)
¯V (B)

1:k(cid:48))2( ¯V (B)

1:k(cid:48) (Σ1:k(cid:48)
¯V (B)

1:k(cid:48))2( ¯V (B)

δ(cid:96)

n

5

following lemma quantiﬁes how noise may affect the performance of the power method  i.e.  it
provides an upper bound of the gap between Q and ¯V (B)
1:k(cid:48) as a function of the operator norm of the
noise matrix Y :
Lemma 3 With probability 1 − 1
1:i )(cid:62) · Q⊥(cid:107)2 ≤ 3(cid:107)Y (cid:107)2
i ≤ k(cid:48): (cid:107)( ¯V (B)

(cid:96)2   the output Q of SPCA when applied to A(B) satisﬁes for all
n si(M )2 .

δ2 (cid:96)

In the proof  we analyze the power iteration algorithm from results in [HMT11].
To complete the performance analysis of Algorithm 2  it remains to upper bound (cid:107)Y (cid:107)2. To this aim 
we decompose Y into three terms:

Y =(cid:0)Φ − δ2(M(B))(cid:62)M(B)

(cid:1) + δ2(M(B))(cid:62)(cid:0)I − U1:k(cid:48)U(cid:62)

1:k(cid:48)(cid:1) M(B)+
1:k(cid:48) )(cid:62)(cid:19)

1:k(cid:48))2( ¯V (B)

.

(cid:18)

δ2

(M(B))(cid:62)U1:k(cid:48)U(cid:62)

1:k(cid:48)M(B) − (cid:96)
n

1:k(cid:48) (Σ1:k(cid:48)
¯V (B)

The ﬁrst term can be controlled using Lemma 1  and the last term is upper bounded using Lemma
2. Finally  the second term corresponds to the error made by ignoring the singular vectors which
are not within the top k(cid:48). To estimate this term  we use the matrix Chernoff bound (Theorem 2.2 in
[Tro11])  and prove that:
Lemma 4 With probability 1 − exp(−m1/4)  (cid:107)(I − U1:k(cid:48)U(cid:62)
n s2

(cid:112)m(cid:96) log((cid:96)) +

1:k(cid:48))M(B)(cid:107)2

2 ≤ 2

k+1(M ).

δ

(cid:96)

In summary  combining the four above lemmas  we can establish that Q accurately estimates ¯V (B)
1:k :
Theorem 5 If δ ≤ m−8/9  with probability 1 − 3
(cid:96)2   the output Q of Algorithm 2 when applied to
A(B) satisﬁes for all i ≤ k: (cid:107)( ¯V (B)
  where
c1 is the constant from Lemma 1.

1:i )(cid:62) · Q⊥(cid:107)2 ≤ 3δ2(s2

2
3 n)+3(2+c1)δ n
(cid:96)
δ2s2

k+1(M )+2m

m(cid:96) log((cid:96))

i (M )

√

3.2 Step 2: Estimating the principal right singular vectors of M

(cid:96) V (B)

Q ≈(cid:112) n

1:k   and E[A(B2)] = δU Σ(V (B))(cid:62)  and hence E[W ] ≈ δ(cid:112) n
(cid:80)k
F ] = (cid:80)m
mn and sj(M(B)) ≈(cid:113) (cid:96)

In this step  we aim at estimating the top k right singular vectors V1:k  or at least at producing
k vectors whose linear span approximates that of V1:k. Towards this objective  we start from Q
derived in the previous step  and deﬁne the (m × k) matrix W = A(B2)Q. W is stored and kept in
memory for the remaining of the algorithm.
It is tempting to directly read from W the top k(cid:48) left singular vectors U1:k(cid:48). Indeed  we know that
1:k. However  the
(cid:96) U1:kΣ1:k
level of the noise in W is too important so as to accurately extract U1:k(cid:48). In turn  W can be written
as δU Σ(V (B))(cid:62)Q + Z  where Z = (A(B2) − δU Σ(V (B))(cid:62))Q partly captures the noise in W . It
√
is then easy to see that the level of the noise Z satisﬁes E[(cid:107)Z(cid:107)2] ≥ E[(cid:107)Z(cid:107)F /
δm).
ij] ≈ mkδ: this is
Indeed  ﬁrst observe that Z is of rank k. Then E[(cid:107)Z(cid:107)2
E[Z 2
due to the facts that (i) Q and A(B2) − δU Σ(V (B))(cid:62) are independent (since A(B1) and A(B2) are
2 = 1 for all j ≤ k  and (iii) the entries of A(B2) are independent with
independent)  (ii) (cid:107)Qj(cid:107)2
variance Θ(δ(1 − δ)). However  for all j ≤ k(cid:48)  the j-th singular value of δU Σ(V (B))(cid:62)Q scales as
n sj(M ) when j ≤ k(cid:48) from
O(δ
Lemma 2.
Instead  from W   A(B1) and the subsequent sampled arriving columns At  t > (cid:96)  we produce
a (n × k) matrix ˆV whose linear span approximates that of V1:k(cid:48). More precisely  we ﬁrst let
ˆV (B) = A(cid:62)
t W   where At is obtained
from the t-th observed column of M after sampling each of its entries at rate δ. Multiplying W by
˜A = [A(B1)  A(cid:96)+1  . . .   An] ampliﬁes the useful signal in W   so that ˆV = ˜A(cid:62)W constitutes a good
approximation of V1:k. To understand why  we can rewrite ˆV as follows:

(cid:113) δm
log(m) )  since sj(M ) ≤ √

(B1)W . Then for all t = (cid:96) + 1  . . .   n  we deﬁne ˆV t = A(cid:62)

m(cid:96)) = O(

k] = Ω(

√

√

j=1

i=1

ˆV = δ2M(cid:62)M(B)Q + δM(cid:62)(A(B2) − δM(B))Q + ( ˜A − δM )(cid:62)W.

6

j (M )

In the above equation  the ﬁrst term corresponds to the useful signal and the two remaining terms
constitute noise matrices. From Theorem 5  the linear span of columns of Q approximates that of
the columns of ¯V (B) and thus  for j ≤ k(cid:48)  sj(δ2M(cid:62)M(B)Q) ≈ δ2s2
The spectral norms of the noise matrices are bounded using random matrix arguments  and the fact
that (A(B2) − δM(B)) and ( ˜A − δM ) are zero-mean random matrices with independent entries. We
can show (see Lemma 14 given in the supplementary material) using the independence of A(B1)
and A(B2) that with high probability  (cid:107)δM(cid:62)(A(B2) − δM(B))Q(cid:107)2 = O(δ
mn). We may also

(cid:113) (cid:96)
n ≥ δ(cid:112)mn log((cid:96)).
establish that with high probability  (cid:107)( ˜A − δM )(cid:62)W(cid:107)2 = O(δ(cid:112)m(m + n)). This is a consequence
high probability  (cid:107) ˜A − δM(cid:107) = O((cid:112)δ(m + n)) and of the fact that due to the trimming process

of a result derived in [AM07] (quoted in Lemma 13 in the supplementary material) stating that with
presented in Line 3 in Algorithm 1  (cid:107)W(cid:107)2 = O(
δm). In summary  as soon as n scales at least
as m  the noise level becomes negligible  and the span of ˆV1:k(cid:48) provides an accurate approximation
of that of V1:k(cid:48). The above arguments are made precise and rigorous in the supplementary material.
The following theorem summarizes the accuracy of our estimator of V1:k.

√

√

Theorem 6 With log4(m)
probability 1 − kδ  (cid:107)V (cid:62)

m

≤ δ ≤ m− 8
i ( ˆV1:k)⊥(cid:107)2 ≤ c2

9 for all i ≤ k  there exists a constant c2 such that with

√

√

s2
k+1(M )+n log(m)
s2
i (M )

m/δ+m

n log(m)/δ

.

3.3 Step 3: Estimating the principal left singular vectors of M

δ

˜AP ˆV .

1:kV (cid:62)

1:k = M PV1:k  where PV1:k = V1:kV (cid:62)

In the last step  we estimate the principal left singular vectors of M to ﬁnally derive an estimator of
M (k)  the optimal rank-k approximation of M. The construction of this estimator is based on the
1:k is an (n × n) matrix
observation that M (k) = U1:kΣ1:k
representing the projection onto the linear span of the top k right singular vectors V1:k of M. Hence
to estimate M (k)  we try to approximate the matrix PV1:k. To this aim  we construct a (k× k) matrix
ˆR so that the column vectors of ˆV ˆR form an orthonormal basis whose span corresponds to that
of the column vectors of ˆV . This construction is achieved using Gram-Schmidt process. We then
approximate PV1:k by P ˆV = ˆV ˆR ˆR(cid:62) ˆV (cid:62)  and ﬁnally our estimator ˆM (k) of M (k) is 1
The construction of ˆM (k) can be made in a memory efﬁcient way accommodating for our streaming
model where the columns of M arrive one after the other  as described in the pseudo-code of SLA.
ˆV (B). Then  for t =
First  after constructing ˆV (B) in Step 2  we build the matrix ˆI = A(B1)
(cid:96) + 1  . . .   n  after constructing the t-th line ˆV t of ˆV   we update ˆI by adding to it the matrix At ˆV t 
so that after all columns of M are observed  ˆI = ˜A ˆV . Hence we can build an estimator ˆU of the
principal left singular vectors of M as ˆU = 1
δ
To quantify the estimation error of ˆM (k)  we decompose M (k) − ˆM (k) as: M (k) − ˆM (k) =
M (k)(I − P ˆV ) + (M (k) − M )P ˆV + (M − 1
˜A)P ˆV . The ﬁrst term of the r.h.s. of the above
ˆV⊥(cid:107) ≤ z =
for i ≤ k  we have si(M )2(cid:107)V (cid:62)
equation can be bounded using Theorem 6:
c2(s2
F ≤ z. The second term can be easily bounded observing that the matrix
(M (k) − M )P ˆV is of rank k: (cid:107)(M (k) − M )P ˆV (cid:107)2
2 =
ksk+1(M )2. The last term in the r.h.s. can be controlled as in the performance analysis of Step 2  and
observing that ( 1
= O(kδ(m + n)).
δ
It is then easy to remark that for the range of the parameter δ we are interested in  the upper bound z
of the ﬁrst term dominates the upper bound of the two other terms. Finally  we obtain the following
result (see the supplementary material for a complete proof):

k+1(M ) + n log(m)(cid:112)m/δ + m(cid:112)n log(m)/δ)  and hence we can conclude that for all i ≤ k 
(cid:13)(cid:13)si(M )UiV (cid:62)

i (I − P ˆV )(cid:13)(cid:13)2
˜A− M )P ˆV is of rank k: (cid:107)(cid:16) 1

F ≤ k(cid:107)(M (k) − M )P ˆV (cid:107)2
(cid:17)
˜A − M

ˆI ˆR ˆR(cid:62)  and ﬁnally obtain ˆM (k) = | ˆU ˆV (cid:62)|1
0.

2 ≤ k(cid:107)M (k) − M(cid:107)2

(cid:13)(cid:13)(cid:13) 1

˜A − M

P ˆV (cid:107)2

F ≤ k

(cid:13)(cid:13)(cid:13)2

2

δ

δ

δ

i

9   with probability 1 − kδ  the output of the SLA algorithm
m ≤ δ ≤ m− 8
Theorem 7 When log4(m)
0(cid:107)2
satisﬁes with constant c3: (cid:107)M (k)−[ ˆU ˆV (cid:62)]1

s2
mn + log(m)√
k+1(M )

= c3k2

mn

+

δn

.

F

δm

(cid:113) log(m)

(cid:19)

(cid:18)

7

Note that if log4(m)
an asymptotically accurate estimate of M (k) as soon as sk+1(M )2

9   then log(m)√

m ≤ δ ≤ m− 8

δm

= o(1). Hence if n ≥ m  the SLA algorithm provides

= o(1).

mn

3.4 Required Memory and Running Time

Required memory.
Lines 1-6 in SLA pseudo-code. A(B1) and A(B2) have O(δm(cid:96)) non-zero entries and we need
O(δm(cid:96) log m) bits to store the id of these entries. Similarly  the memory required to store Φ is
O(δ2m(cid:96)2 log((cid:96))). Storing Q further requires O((cid:96)k) memory. Finally  ˆV (B1) and ˆI computed in
Line 6 require O((cid:96)k) and O(km) memory space  respectively. Thus  when (cid:96) = 1
δ log m  this ﬁrst part
of the algorithm requires O(k(m + n)) memory.
Lines 7-9. Before we treat the remaining columns  A(B1)  A(B2)  and Q are removed from the mem-
ory. Using this released memory  when the t-th column arrives  we can store it  compute ˆV t and ˆI 
and remove the column to save memory. Therefore  we do not need additional memory to treat the
remaining columns.
Lines 10 and 11. From ˆI and ˆV   we compute ˆU. To this aim  the memory required is O(k(m + n)).
Running time.
From line 1 to 6. The SPCA algorithm requires O((cid:96)k(δ2m(cid:96) + k) log((cid:96))) ﬂoating-point operations to
compute Q. W   ˆV   and ˆI are inner products  and their computations require O(δkm(cid:96)) operations.
With (cid:96) =
δ log(m)  the number of operations to treat the ﬁrst (cid:96) columns is O((cid:96)k(δ2m(cid:96) + k) log((cid:96)) +
kδm(cid:96)) = O(km) + O( k2
From line 7 to 9. To compute ˆV t and ˆI when the t-th column arrives  we need O(δkm) operations.
Since there are n − (cid:96) remaining columns  the total number of operations is O(δkmn).
Lines 10 and 11 ˆR is computed from ˆV using the Gram-Schmidt process which requires O(k2m)
operations. We then compute ˆI ˆR ˆR(cid:62) using O(k2m) operations. Hence we conclude that:
In summary  we have shown that:
Theorem 8 The memory required to run the SLA algorithm is O(k(m + n)). Its running time is
O(δkmn + k2
Observe that when δ ≥ max( (log(m))4
k2m  and therefore  the running time of SLA is O(δkmn).

) and k ≤ (log(m))6  we have δkmn ≥ k2/δ ≥

δ + k2m).

  (log(m))2

δ ).

1

m

n

3.5 General Streaming Model

SLA is a one-pass low-rank approximation algorithm  but the set of the (cid:96) ﬁrst observed columns
of M needs to be chosen uniformly at random. We can readily extend SLA to deal with scenarios
where the columns of M can be observed in an arbitrary order. This extension requires two passes
on M  but otherwise performs exactly the same operations as SLA. In the ﬁrst pass  we extract a set
of (cid:96) columns chosen uniformly at random  and in the second pass  we deal with all other columns.
To extract (cid:96) randomly selected columns in the ﬁrst pass  we proceed as follows. Assume that when
the t-th column of M arrives  we have already extracted l columns. Then the t-th column is extracted
(cid:96)−l
with probability
n−t+1. This two-pass version of SLA enjoys the same performance guarantees as
those of SLA.

4 Conclusion

This paper revisited the low rank approximation problem. We proposed a streaming algorithm that
samples the data and produces a near optimal solution with a vanishing mean square error. The
algorithm uses a memory space scaling linearly with the ambient dimension of the matrix  i.e. the
memory required to store the output alone. Its running time scales as the number of sampled entries
of the input matrix. The algorithm is relatively simple  and in particular  does exploit elaborated
techniques (such as sparse embedding techniques) recently developed to reduce the memory re-
quirement and complexity of algorithms addressing various problems in linear algebra.

8

References
[AM07] Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approxi-

mations. Journal of the ACM (JACM)  54(2):9  2007.

[BJS15] Srinadh Bhojanapalli  Prateek Jain  and Sujay Sanghavi. Tighter low-rank approximation
via sampling the leveraged element. In Proceedings of the Twenty-Sixth Annual ACM-
SIAM Symposium on Discrete Algorithms  pages 902–920. SIAM  2015.

[CW09] Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming
model. In Proceedings of the forty-ﬁrst annual ACM symposium on Theory of computing 
pages 205–214. ACM  2009.

[CW13] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in
input sparsity time. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory
of computing  pages 81–90. ACM  2013.

[GP14] Mina Ghashami and Jeff M Phillips. Relative errors for deterministic low-rank matrix

approximations. In SODA  pages 707–717. SIAM  2014.

[Lib13]

[HMT11] Nathan Halko  Per-Gunnar Martinsson  and Joel A Tropp. Finding structure with ran-
domness: Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM review  53(2):217–288  2011.
Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM
SIGKDD international conference on Knowledge discovery and data mining  pages 581–
588. ACM  2013.
Ioannis Mitliagkas  Constantine Caramanis  and Prateek Jain. Memory limited  streaming
PCA. In Advances in Neural Information Processing Systems  2013.
Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform.
Advances in Adaptive Data Analysis  3(01n02):115–126  2011.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
Computational Mathematics  12(4):389–434  2012.

[MCJ13]

[Tro11]

[Tro12]

[Woo14] David Woodruff. Low rank approximation lower bounds in row-update streams.

Advances in Neural Information Processing Systems  pages 1781–1789  2014.

In

9

,Se-Young Yun
marc lelarge
Emilien Dupont
Arnaud Doucet
Yee Whye Teh