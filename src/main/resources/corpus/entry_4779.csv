2015,Robust Spectral Inference for Joint Stochastic Matrix Factorization,Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics  and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small  noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.,Robust Spectral Inference for Joint Stochastic Matrix

Factorization

Moontae Lee  David Bindel
Dept. of Computer Science

Cornell University
Ithaca  NY 14850

{moontae bindel}@cs.cornell.edu

David Mimno

Dept. of Information Science

Cornell University
Ithaca  NY 14850

mimno@cornell.edu

Abstract

Spectral inference provides fast algorithms and provable optimality for latent topic
analysis. But for real data these algorithms require additional ad-hoc heuristics 
and even then often produce unusable results. We explain this poor performance
by casting the problem of topic inference in the framework of Joint Stochastic
Matrix Factorization (JSMF) and showing that previous methods violate the theo-
retical conditions necessary for a good solution to exist. We then propose a novel
rectiﬁcation method that learns high quality topics and their interactions even on
small  noisy data. This method achieves results comparable to probabilistic tech-
niques in several domains while maintaining scalability and provable optimality.

1

Introduction

Summarizing large data sets using pairwise co-occurrence frequencies is a powerful tool for data
mining. Objects can often be better described by their relationships than their inherent char-
acteristics. Communities can be discovered from friendships [1]  song genres can be identiﬁed
from co-occurrence in playlists [2]  and neural word embeddings are factorizations of pairwise co-
occurrence information [3  4]. Recent Anchor Word algorithms [5  6] perform spectral inference on
co-occurrence statistics for inferring topic models [7  8]. Co-occurrence statistics can be calculated
using a single parallel pass through a training corpus. While these algorithms are fast  deterministic 
and provably guaranteed  they are sensitive to observation noise and small samples  often producing
effectively useless results on real documents that present no problems for probabilistic algorithms.
We cast this general problem
of learning overlapping latent
clusters as Joint-Stochastic Ma-
trix Factorization (JSMF)  a
subset of non-negative matrix
factorization that contains topic
modeling as a special case.
We explore the conditions nec-
essary for inference from co-
occurrence statistics and show
that the Anchor Words algo-
rithms necessarily violate such
conditions. Then we propose a rectiﬁed algorithm that matches the performance of probabilistic
inference—even on small and noisy datasets—without losing efﬁciency and provable guarantees.
Validating on both real and synthetic data  we demonstrate that our rectiﬁcation not only produces
better clusters  but also  unlike previous work  learns meaningful cluster interactions.

Figure 1: 2D visualizations show the low-quality convex hull
found by Anchor Words [6] (left) and a better convex hull (middle)
found by discovering anchor words on a rectiﬁed space (right).

1

-0.04-0.0200.020.040.060.08-0.04-0.03-0.02-0.0100.010.020.030.040.05Area = 0.000313-0.04-0.0200.020.040.060.08-0.04-0.03-0.02-0.0100.010.020.030.040.05Area = 0.002602-0.02-0.0100.010.020.03-0.015-0.01-0.00500.0050.010.0150.02Area = 0.000660Let the matrix C represent the co-occurrence of pairs drawn from N objects: Cij is the joint prob-
ability p(X1 = i  X2 = j) for a pair of objects i and j. Our goal is to discover K latent clus-
ters by approximately decomposing C ≈ BABT . B is the object-cluster matrix  in which each
column corresponds to a cluster and Bik = p(X = i|Z = k) is the probability of drawing an
object i conditioned on the object belonging to the cluster k; and A is the cluster-cluster matrix 
in which Akl = p(Z1 = k  Z2 = l) represents the joint probability of pairs of clusters. We
call the matrices C and A joint-stochastic (i.e.  C ∈ J S N   A ∈ J S K) due to their correspon-
dence to joint distributions; B is column-stochastic. Example applications are shown in Table 1.

Domain
Document

Table 1: JSMF applications  with anchor-word equivalents.

Anchor Word algorithms [5 
6] solve JSMF problems us-
ing a separability assumption:
each topic contains at
least
one “anchor” word that has
non-negligible probability ex-
clusively in that topic. The al-
gorithm uses the co-occurrence
patterns of the anchor words as a summary basis for the co-occurrence patterns of all other words.
The initial algorithm [5] is theoretically sound but unable to produce column-stochastic word-topic
matrix B due to unstable matrix inversions. A subsequent algorithm [6] ﬁxes negative entries in B 
but still produces large negative entries in the estimated topic-topic matrix A. As shown in Figure 3 
the proposed algorithm infers valid topic-topic interactions.

Image
Network
Legislature Member

Object
Word
Pixel
User

Cluster
Topic

Segment

Community
Party/Group

Playlist

Anchor Word

Pure Pixel

Representative

Partisan

Signature Song

Song

Genre

Basis

2 Requirements for Factorization

In this section we review the probabilistic and statistical structures of JSMF and then deﬁne geo-
metric structures of co-occurrence matrices required for successful factorization. C ∈ RN×N is a
joint-stochastic matrix constructed from M training examples  each of which contain some subset
of N objects. We wish to ﬁnd K (cid:28) N latent clusters by factorizing C into a column-stochastic
matrix B ∈ RN×K and a joint-stochastic matrix A ∈ RK×K  satisfying C ≈ BABT .
Probabilistic structure. Figure 2 shows the event
space of our model. The distribution A over pairs of clus-
ters is generated ﬁrst from a stochastic process with a hy-
perparameter α.
If the m-th training example contains
a total of nm objects  our model views the example as
consisting of all possible nm(nm − 1) pairs of objects.1
For each of these pairs  cluster assignments are sampled
from the selected distribution ((z1  z2) ∼ A). Then an
actual object pair is drawn with respect to the correspond-
ing cluster assignments (x1 ∼ Bz1  x2 ∼ Bz2). Note that
this process does not explain how each training example
is generated from a model  but shows how our model un-
derstands the objects in the training examples.
Following [5  6]  our model views B as a set of parameters rather than random variables.2 The
primary learning task is to estimate B; we then estimate A to recover the hyperparameter α. Due to
the conditional independence X1 ⊥ X2 | (Z1 or Z2)  the factorization C ≈ BABT is equivalent to

Figure 2: The JSMF event space differs
from LDA’s. JSMF deals only with pairwise
co-occurrence events and does not generate
observations/documents.

(cid:88)

(cid:88)

z1

z2

p(X1  X2|A; B) =

p(X1|Z1; B)p(Z1  Z2|A)p(X2|Z2; B).

Under the separability assumption  each cluster k has a basis object sk such that p(X = sk|Z =
k) > 0 and p(X = sk|Z (cid:54)= k) = 0. In matrix terms  we assume the submatrix of B comprised of
1Due to the bag-of-words assumption  every object can pair with any other object in that example  except
itself. One implication of our work is better understanding the self-co-occurrences  the diagonal entries in the
co-occurrence matrix.

2In LDA  each column of B is generated from a known distribution Bk ∼ Dir(β).

2

AαZ1Z2X1X2Bknm(nm−1)1≤m≤M1≤k≤K1the rows with indices S = {s1  . . .   sK} is diagonal. As these rows form a non-negative basis for
the row space of B  the assumption implies rank+(B) = K = rank(B).3 Providing identiﬁability
to the factorization  this assumption becomes crucial for inference of both B and A. Note that JSMF
factorization is unique up to column permutation  meaning that no speciﬁc ordering exists among
the discovered clusters  equivalent to probabilistic topic models (see the Appendix).

Statistical structure. Let f (α) be a (known) distribution of distributions from which a cluster
distribution is sampled for each training example. Saying Wm ∼ f (α)  we have M i.i.d samples
{W1  . . .   WM} which are not directly observable. Deﬁning the posterior cluster-cluster matrix
A∗
m]  Lemma 2.2 in [5] showed that4
M = 1
M

m=1 WmW T

(cid:80)M

Denote the posterior co-occurrence for the m-th training example by C∗
Then C∗

mBT   and C∗ = 1

m = BWmW T

∗

A

∗
M −→ A

m and the expectation A∗ = E[WmW T
as M −→ ∞.
(cid:33)

(cid:80)M
m=1 C∗
M(cid:88)

m. Thus

(cid:32)

M

∗

C

= B

1
M

m=1

(1)
m and all examples by C∗.

WmW T
m

BT = BA

∗
M BT .

(2)

(cid:88)

∈ DNN K and C∗
M(cid:88)

yT A

∗
M y =

1
M

m=1

(cid:88)

1
M

Denote the noisy observation for the m-th training example by Cm  and all examples by C. Let
W = [W1|...|WM ] be a matrix of topics. We will construct Cm so that E[C|W ] is an unbiased
estimator of C∗. Thus as M → ∞
(3)

= BA

BT .

∗

∗

C −→ E[C] = C

∗
M BT −→ BA

Geometric structure. Though the separability assumption allows us to identify B even from the
noisy observation C  we need to throughly investigate the structure of cluster interactions. This is
because it will eventually be related to how much useful information the co-occurrence between
corresponding anchor bases contains  enabling us to best use our training data. Say DNN n is the
set of n× n doubly non-negative matrices: entrywise non-negative and positive semideﬁnite (PSD).
Claim A∗
Proof Take any vector y ∈ RK. As A∗

M is deﬁned as a sum of outer-products 

∈ DNN N

M   A∗

yT WmW T

my =

(W T

my)T (W T

my) =

(non-negative) ≥ 0.

(4)

In addition  (A∗

M ∈ PSDK.

Thus A∗
M )kl = p(Z1 = k  Z2 = l) ≥ 0 for all k  l. Proving
A∗
∈ DNN K is analogous by the linearity of expectation. Relying on double non-negativity of
M   Equation (3) implies not only the low-rank structure of C∗  but also double non-negativity of
A∗
C∗ by a similar proof (see the Appendix).
The Anchor Word algorithms in [5  6] consider neither double non-negativity of cluster interactions
nor its implication on co-occurrence statistics. Indeed  the empirical co-occurrence matrices col-
lected from limited data are generally indeﬁnite and full-rank  whereas the posterior co-occurrences
must be positive semideﬁnite and low-rank. Our new approach will efﬁciently enforce double non-
negativity and low-rankness of the co-occurrence matrix C based on the geometric property of its
posterior behavior. We will later clarify how this process substantially improves the quality of the
clusters and their interactions by eliminating noises and restoring missing information.

3 Rectiﬁed Anchor Words Algorithm

In this section  we describe how to estimate the co-occurrence matrix C from the training data  and
how to rectify C so that it is low-rank and doubly non-negative. We then decompose the rectiﬁed
C(cid:48) in a way that preserves the doubly non-negative structure in the cluster interaction matrix.

3rank+(B) means the non-negative rank of the matrix B  whereas rank(B) means the usual rank.
4This convergence is not trivial while 1
M

(cid:80)M
m=1 Wm → E[Wm] as M → ∞ by the Central Limit Theorem.

3

Generating co-occurrence C. Let Hm be the vector of object counts for the m-th training exam-
ple  and let pm = BWm where Wm is the document’s latent topic distribution. Then Hm is assumed
m   and

to be a sample from a multinomial distribution Hm ∼ Multi(nm  pm) where nm =(cid:80)N

(cid:1). As in [6]  we

recall E[Hm] = nmpm = nmBWm and Cov(Hm) = nm
generate the co-occurrence for the m-th example by

(cid:0)diag(pm) − pmpT

i=1 H (i)

m

dm

dm

Cm =

HmH T

m − diag(Hm)
nm(nm − 1)

.

(5)

1
dm

diag(E[Hm]) = 1

The diagonal penalty in Eq. 5 cancels out the diagonal matrix term in the variance-covariance matrix 
making the estimator unbiased. Putting dm = nm(nm − 1)  that is E[Cm|Wm] = 1
E[HmH T
m] −
m)BT ≡ C∗
(E[Hm]E[Hm]T + Cov(Hm) − diag(E[Hm])) = B(WmW T
m.
Thus E[C|W ] = C∗ by the linearity of expectation.
Rectifying co-occurrence C. While C is an unbiased estimator for C∗ in our model  in reality the
two matrices often differ due to a mismatch between our model assumptions and the data5 or due
to error in estimation from limited data. The computed C is generally full-rank with many negative
eigenvalues  causing a large approximation error. As the posterior co-occurrence C∗ must be low-
rank  doubly non-negative  and joint-stochastic  we propose two rectiﬁcation methods: Diagonal
Completion (DC) and Alternating Projection (AP). DC modiﬁes only diagonal entries so that C
becomes low-rank  non-negative  and joint-stochastic; while AP enforces modiﬁes every entry and
enforces the same properties as well as positive semi-deﬁniteness. As our empirical results strongly
favor alternating projection  we defer the details of diagonal completion to the Appendix.
Based on the desired property of the posterior co-occurrence C∗  we seek to project our estimator
C onto the set of joint-stochastic  doubly non-negative  low rank matrices. Alternating projection
methods like Dykstra’s algorithm [9] allow us to project onto an intersection of ﬁnitely many convex
sets using projections onto each individual set in turn. In our setting  we consider the intersection
of three sets of symmetric N × N matrices: the elementwise non-negative matrices NN N   the
normalized matrices NORN whose entry sum is equal to 1  and the positive semi-deﬁnite matrices
with rank K  PSDN K. We project onto these three sets as follows:
ΠPSDNK (C) = U Λ+

KU T   ΠNORN (C) = C +

11T   ΠNN N (C) = max{C  0}.

(cid:80)

i j Cij

1 −

N 2

where C = U ΛU T is an eigendecomposition and Λ+
K is the matrix Λ modiﬁed so that all negative
eigenvalues and any but the K largest positive eigenvalues are set to zero. Truncated eigendecom-
positions can be computed efﬁciently  and the other projections are likewise efﬁcient. While NN N
and NORN are convex  PSDN K is not. However  [10] show that alternating projection with a
non-convex set still works under certain conditions  guaranteeing a local convergence. Thus iterat-
ing three projections in turn until the convergence rectiﬁes C to be in the desired space. We will
show how to satisfy such conditions and the convergence behavior in Section 5.

Selecting basis S. The ﬁrst step of the factorization is to select the subset S of objects that satisfy
the separability assumption. We want the K best rows of the row-normalized co-occurrence matrix
C so that all other rows lie nearly in the convex hull of the selected rows.
[6] use the Gram-
Schmidt process to select anchors  which computes pivoted QR decomposition  but did not utilize the
sparsity of C. To scale beyond small vocabularies  they use random projections that approximately
preserve (cid:96)2 distances between rows of C. For all experiments we use a new pivoted QR algorithm
(see the Appendix) that exploits sparsity instead of using random projections  and thus preserves
deterministic inference.6

Recovering object-cluster B. After ﬁnding the set of basis objects S  we can infer each entry of
B by Bayes’ rule as in [6]. Let {p(Z1 = k|X1 = i)}K
k=1 be the coefﬁcients that reconstruct the
i-th row of C in terms of the basis rows corresponding to S. Since Bik = p(X1 = i|Z1 = k) 

5There is no reason to expect real data to be generated from topics  much less exactly K latent topics.
6To effectively use random projections  it is necessary to either ﬁnd proper dimensions based on multiple

trials or perform low-dimensional random projection multiple times [25] and merge the resulting anchors.

4

we can use the corpus frequencies p(X1 = i) = (cid:80)

j Cij to estimate Bik ∝ p(Z1 = k|X1 =
i)p(X1 = i). Thus the main task for this step is to solve simplex-constrained QPs to infer a
set of such coefﬁcients for each object. We use an exponentiated gradient algorithm to solve the
problem similar to [6]. Note that this step can be efﬁciently done in parallel for each object.

failing to model

Recovering cluster-cluster A.
[6] recovered A by minimizing
(cid:107)C − BABT(cid:107)F ; but the inferred
A generally has many negative
entries 
the
probabilistic interaction between
topics. While we can further
project A onto the joint-stochastic
matrices 
this produces a large
approximation error.
We consider an alternate recovery
method that again leverages the
separability assumption. Let CSS be the submatrix whose rows and columns correspond to the
selected objects S  and let D be the diagonal submatrix BS∗ of rows of B corresponding to S. Then

Figure 3: The algorithm of [6] (ﬁrst panel) produces negative cluster
co-occurrence probabilities. A probabilistic reconstruction alone (this
paper & [5]  second panel) removes negative entries but has no off-
diagonals and does not sum to one. Trying after rectiﬁcation (this
paper  third panel) produces a valid joint stochastic matrix.

CSS = DADT = DAD =⇒ A = D

(6)
This approach efﬁciently recovers a cluster-cluster matrix A mostly based on the co-occrrurence
information between corresponding anchor basis  and produces no negative entries due to the sta-
bility of diagonal matrix inversion. Note that the principle submatrices of a PSD matrix are also
PSD; hence  if C ∈ PSDN then CSS  A ∈ PSDK. Thus  not only is the recovered A an unbiased
estimator for A∗
M ∈ DNN K after the rectiﬁcation.7

M   but also it is now doubly non-negative as A∗

−1CSSD

−1.

4 Experimental Results

Our Rectiﬁed Anchor Words algorithm with alternating projection ﬁxes many problems in the base-
line Anchor Words algorithm [6] while matching the performance of Gibbs sampling [11] and main-
taining spectral inference’s determinism and independence from corpus size. We evaluate direct
measurement of matrix quality as well as indicators of topic utility. We use two text datasets:
NIPS full papers and New York Times news articles.8 We eliminate a minimal list of 347 En-
glish stop words and prune rare words based on tf-idf scores and remove documents with fewer
than ﬁve tokens after vocabulary curation. We also prepare two non-textual item-selection datasets:
users’ movie reviews from the Movielens 10M Dataset 9 and music playlists from the complete
Yes.com dataset.10 We perform similar vocabulary curation and document tailoring  with the ex-
ception of frequent stop-object elimination. Playlists often contain the same songs multiple times 
but users are unlikely to review the same movies more than once  so we augment the movie dataset
so that each review contains 2 × (stars) number of movies based on the half-scaled rating in-
formation that varies from 0.5 stars to 5 stars. Statistics of our datasets are shown in Table 2.
We run DC 30 times for each experiment  randomly
permuting the order of objects and using the median
results to minimize the effect of different orderings.
We also run 150 iterations of AP alternating PSDN K 
NORN   and NN N in turn. For probabilistic Gibbs
sampling  we use the Mallet with the standard option
doing 1 000 iterations. All metrics are evaluated against
the original C  not against the rectiﬁed C(cid:48)  whereas we use B and A inferred from the rectiﬁed C(cid:48).
7We later realized that essentially same approach was previously tried in [5]  but it was not able to generate

Table 2: Statistics of four datasets.
Dataset
NIPS

M
1 348
269 325
63 041
14 653

NYTimes
Movies
Songs

380.5
204.9
142.8
119.2

N
5k
15k
10k
10k

Avg. Len

a valid topic-topic matrix as shown in the middle panel of Figure 3.

8https://archive.ics.uci.edu/ml/datasets/Bag+of+Words
9http://grouplens.org/datasets/movielens
10http://www.cs.cornell.edu/˜shuochen/lme

5

22.842-7.6870.629-2.723-12.888-7.68743.605-4.986-7.788-22.9300.629-4.98612.782-5.269-2.998-2.723-7.788-5.26919.237-3.267-12.888-22.930-2.998-3.26742.36745.0210.0000.0000.0000.0000.00043.0860.0000.0000.0000.0000.00052.8280.0000.0000.0000.0000.00017.5270.0000.0000.0000.0000.00076.1530.1140.0000.0020.0240.0040.0000.1150.0100.0070.0170.0020.0100.1620.0160.0120.0240.0070.0160.0720.0140.0040.0170.0120.0140.328−22.93−11.230.000.170.340.500.670.841.0023.46Qualitative results. Although [6] report comparable results to probabilistic algorithms for LDA 
the algorithm fails under many circumstances. The algorithm prefers rare and unusual anchor words
that form a poor basis  so topic clusters consist of the same high-frequency terms repeatedly  as
shown in the upper third of Table 3.
In contrast  our algorithm with AP rectiﬁcation success-
fully learns themes similar to the probabilistic algorithm. One can also verify that cluster inter-
actions given in the third panel of Figure 3 explain how the ﬁve topics correlate with each other.

Each panel

Table 3: Each line is a topic from NIPS (K = 5). Previous work
simply repeats the most frequent words in the corpus ﬁve times.

Similar to [12]  we visualize the
ﬁve anchor words
in the co-
occurrence space after 2D PCA
of C.
in Figure 1
shows a 2D embedding of
the
NIPS vocabulary as blue dots and
ﬁve selected anchor words in red.
The ﬁrst plot shows standard an-
chor words and the original co-
occurrence space. The second plot
shows anchor words selected from
the rectiﬁed space overlaid on the
original co-occurrence space. The
third plot shows the same anchor
words as the second plot overlaid
on the AP-rectiﬁed space. The rec-
tiﬁed anchor words provide better
coverage on both spaces  explain-
ing why we are able to achieve rea-
sonable topics even with K = 5.
Rectiﬁcation also produces better clusters in the non-textual movie dataset. Each cluster is notably
more genre-coherent and year-coherent than the clusters from the original algorithm. When K = 15 
for example  we verify a cluster of Walt Disney 2D Animations mostly from the 1990s and a cluster
of Fantasy movies represented by Lord of the Rings ﬁlms  similar to clusters found by probabilistic
Gibbs sampling. The Baseline algorithm [6] repeats Pulp Fiction and Silence of the Lambs 15 times.

Arora et al. 2013 (Baseline)
neuron layer hidden recognition signal cell noise
neuron layer hidden cell signal representation noise
neuron layer cell hidden signal noise dynamic
neuron layer cell hidden control signal noise
neuron layer hidden cell signal recognition noise
This paper (AP)
neuron circuit cell synaptic signal layer activity
control action dynamic optimal policy controller reinforcement
recognition layer hidden word speech image net
cell ﬁeld visual direction image motion object orientation
gaussian noise hidden approximation matrix bound examples
Probabilistic LDA (Gibbs)
neuron cell visual signal response ﬁeld activity
control action policy optimal reinforcement dynamic robot
recognition image object feature word speech features
hidden net layer dynamic neuron recurrent noise
gaussian approximation matrix bound component variables

(cid:80)K

i (cid:107)C i −

k p(Z1 = k|X1 = i)C Sk(cid:107)2

N
rectiﬁed matrix. AP reduces error in almost all cases and is more effective than DC. Although
we expect error to decrease as we increase the number of clusters K  reducing recovery error for
a ﬁxed K by choosing better anchors is extremely difﬁcult: no other subset selection algorithm
[13] decreased error by more than 0.001. A good matrix factorization should have small element-

Quantitative results. We measure the intrinsic quality of inference and summarization with re-
spect to the JSMF objectives as well as the extrinsic quality of resulting topics. Lines correspond to
four methods: ◦ Baseline for the algorithm in the previous work [6] without any rectiﬁcation  (cid:52) DC
for Diagonal Completion  (cid:3) AP for Alternating Projection  and (cid:5) Gibbs for Gibbs sampling.
(cid:0) 1
(cid:1) with respect to the original C matrix  not the
(cid:80)N
Anchor objects should form a good basis for the remaining objects. We measure Recovery error
wise Approximation error(cid:0)
(cid:1). DC and AP preserve more of the information in
k p(Z2 = k|Z1 = k)(cid:1) indicates lower correlation between clusters.12
diagonal Dominancy(cid:0) 1
(cid:80)K
Speciﬁcity(cid:0) 1
k KL (p(X|Z = k)(cid:107)p(X))(cid:1) measures how much each cluster is distinct from
(cid:80)K

the original matrix C than the Baseline method  especially when K is small.11 We expect non-
trivial interactions between clusters  even when we do not explicitly model them as in [14]. Greater

AP and Gibbs results are similar. We do not report held-out probability because we ﬁnd that relative
results are determined by user-deﬁned smoothing parameters [12  24].

K

the corpus distribution. When anchors produce a poor basis  the conditional distribution of clus-
11In the NYTimes corpus  10−2 is a large error: each element is around 10−9 due to the number of normal-

(cid:107)C − BABT(cid:107)F

K

ized entries.

12Dominancy in Songs corpus lacks any Baseline results at K > 10 because dominancy is undeﬁned if an
algorithm picks a song that occurs at most once in each playlist as a basis object. In this case  the original
construction of CSS  and hence of A  has a zero diagonal element  making dominancy NaN.

6

Figure 4: Experimental results on real dataset. The x-axis indicates logK where K varies by 5 up to 25 topics
and by 25 up to 100 or 150 topics. Whereas the Baseline algorithm largely fails with small K and does not infer
quality B and A even with large K  Alternating Projection (AP) not only ﬁnds better basis vectors (Recovery) 
but also shows stable and comparable behaviors to probabilistic inference (Gibbs) in every metric.

Inter-topic Dissimilarity
ters given objects becomes uniform  making p(X|Z) similar to p(X).
counts the average number of objects in each cluster that do not occur in any other cluster’s top
(cid:0) 1
(cid:80)K
20 objects. Our experiments validate that AP and Gibbs yield comparably speciﬁc and distinct
topics  while Baseline and DC simply repeat the corpus distribution as in Table 3. Coherence

(cid:1) penalizes topics that assign high probability (rank > 20) to

(cid:80)∈T opk

x1(cid:54)=x2 log D2(x1 x2)+

k

D1(x2)

K
words that do not occur together frequently. AP produces results close to Gibbs sampling  and
far from the Baseline and DC. While this metric correlates with human evaluation of clusters [15]
“worse” coherence can actually be better because the metric does not penalize repetition [12].
In semi-synthetic experiments [6] AP matches Gibbs sampling and outperforms the Baseline  but
the discrepancies in topic quality metrics are smaller than in the real experiments (see Appendix).
We speculate that semi-synthetic data is more “well-behaved” than real data  explaining why issues
were not recognized previously.

5 Analysis of Algorithm

Why does AP work? Before rectiﬁcation  diagonals of the empirical C matrix may be far from
correct. Bursty objects yield diagonal entries that are too large; extremely rare objects that occur
at most once per document yield zero diagonals. Rare objects are problematic in general: the cor-
responding rows in the C matrix are sparse and noisy  and these rows are likely to be selected by
the pivoted QR. Because rare objects are likely to be anchors  the matrix CSS is likely to be highly
diagonally dominant  and provides an uninformative picture of topic correlations. These problems
are exacerbated when K is small relative to the effective rank of C  so that an early choice of a poor
anchor precludes a better choice later on; and when the number of documents M is small  in which
case the empirical C is relatively sparse and is strongly affected by noise. To mitigate this issue 
[24] run exhaustive grid search to ﬁnd document frequency cutoffs to get informative anchors. As

7

llllllllllllllllllllllllllllllllllllllllllllllllRecoveryApproximationDominancySpecificityDissimilarityCoherence0.030.040.050.060.000.050.100.150.20.40.60.81.00123051015−320−280−240−200−160510152550751005101525507510051015255075100510152550751005101525507510051015255075100CategorylAPBaselineDCGibbsNipsllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllRecoveryApproximationDominancySpecificityDissimilarityCoherence0.050.100.150.200.250.000.050.100.150.200.20.40.60.81.0012351015−400−350−300510152550751001505101525507510015051015255075100150510152550751001505101525507510015051015255075100150CategorylAPBaselineDCGibbsNYTimesllllllllllllllllllllllllllllllllllllllllllllllllRecoveryApproximationDominancySpecificityDissimilarityCoherence0.040.060.080.1005100.250.500.751.0001234051015−240−210−180−150−120510152550751005101525507510051015255075100510152550751005101525507510051015255075100CategorylAPBaselineDCGibbsMoviesllllllllllllllllllllllllllllllllllllllllllRecoveryApproximationDominancySpecificityDissimilarityCoherence0.0500.0750.1000.1250.1500.0000.0050.0100.0150.40.60.81.001234505101520−700−500−300510152550751005101525507510051015255075100510152550751005101525507510051015255075100CategorylAPBaselineDCGibbsSongsmodel performance is inconsistent for different cutoffs and search requires cross-validation for each
case  it is nearly impossible to ﬁnd good heuristics for each dataset and number of topics.
Fortunately  a low-rank PSD matrix cannot have too many diagonally-dominant rows  since this vi-
olates the low rank property. Nor can it have diagonal entries that are small relative to off-diagonals 
since this violates positive semi-deﬁniteness. Because the anchor word assumption implies that
non-negative rank and ordinary rank are the same  the AP algorithm ideally does not remove the
information we wish to learn; rather  1) the low-rank projection in AP suppresses the inﬂuence of
small numbers of noisy rows associated with rare words which may not be well correlated with the
others  and 2) the PSD projection in AP recovers missing information in diagonals. (As illustrated
in the Dominancy panel of the Songs corpus in Figure 4  AP shows valid dominancies even after
K > 10 in contrast to the Baseline algorithm.)
Why does AP converge? AP enjoys local linear convergence [10] if 1) the initial C is near the
convergence point C(cid:48)  2) PSDN K is super-regular at C(cid:48)  and 3) strong regularity holds at C(cid:48). For
the ﬁrst condition  recall that we rectiﬁed C(cid:48) by pushing C toward C∗  which is the ideal convergence
point inside the intersection. Since C → C∗ as shown in (5)  C is close to C(cid:48) as desired.The prox-
regular sets13 are subsets of super-regular sets  so prox-regularity of PSDN K at C(cid:48) is sufﬁcient for
the second condition. For permutation invariant M ⊂ RN   the spectral set of symmetric matrices
is deﬁned as λ−1(M) = {X ∈ SN : (λ1(X)  . . .   λN (X)) ∈ M}  and λ−1(M) is prox-regular
if and only if M is prox-regular [16  Th. 2.4]. Let M be {x ∈ R+
n : |supp(x)| = K}. Since each
element in M has exactly K positive components and all others are zero  λ−1(M) = PSDN K. By
the deﬁnition of M and K < N  PM is locally unique almost everywhere  satisfying the second
condition almost surely. (As the intersection of the convex set PSDN and the smooth manifold of
rank K matrices  PSDN K is a smooth manifold almost everywhere.)
Checking the third condition a priori is challenging  but we expect noise in the empirical C to
prevent an irregular solution  following the argument of Numerical Example 9 in [10]. We expect
AP to converge locally linearly and we can verify local convergence of AP in practice. Empirically 
the ratio of average distances between two iterations are always ≤ 0.9794 on the NYTimes dataset
(see the Appendix)  and other datasets were similar. Note again that our rectiﬁed C(cid:48) is a result of
pushing the empirical C toward the ideal C∗. Because approximation factors of [6] are all computed
based on how far C and its co-occurrence shape could be distant from C∗’s  all provable guarantees
of [6] hold better with our rectiﬁed C(cid:48).

6 Related and Future Work

JSMF is a speciﬁc structure-preserving Non-negative Matrix Factorization (NMF) performing spec-
tral inference. [17  18] exploit a similar separable structure for NMF problmes. To tackle hyperspec-
tral unmixing problems  [19  20] assume pure pixels  a separability-equivalent in computer vision.
In more general NMF without such structures  RESCAL [21] studies tensorial extension of similar
factorization and SymNMF [22] infers BBT rather than BABT . For topic modeling  [23] performs
spectral inference on third moment tensor assuming topics are uncorrelated.
As the core of our algorithm is to rectify the input co-occurrence matrix  it can be combined with
several recent developments.
[24] proposes two regularization methods for recovering better B.
[12] nonlinearly projects co-occurrence to low-dimensional space via t-SNE and achieves better
anchors by ﬁnding the exact anchors in that space. [25] performs multiple random projections to
low-dimensional spaces and recovers approximate anchors efﬁciently by divide-and-conquer strat-
egy. In addition  our work also opens several promising research directions. How exactly do anchors
found in the rectiﬁed C(cid:48) form better bases than ones found in the original space C? Since now the
topic-topic matrix A is again doubly non-negative and joint-stochastic  can we learn super-topics in
a multi-layered hierarchical model by recursively applying JSMF to topic-topic co-occurrence A?

Acknowledgments

This research is supported by NSF grant HCC:Large-0910664. We thank Adrian Lewis for valuable
discussions on AP convergence.

13A set M is prox-regular if PM is locally unique.

8

References
[1] Alan Mislove  Bimal Viswanath  Krishna P. Gummadi  and Peter Druschel. You are who you know: In-
ferring user proﬁles in Online Social Networks. In Proceedings of the 3rd ACM International Conference
of Web Search and Data Mining (WSDM’10)  New York  NY  February 2010.

[2] Shuo Chen  J. Moore  D. Turnbull  and T. Joachims. Playlist prediction via metric embedding. In ACM

SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)  pages 714–722  2012.

[3] Jeffrey Pennington  Richard Socher  and Christopher D. Manning. GloVe: Global vectors for word repre-

sentation. In EMNLP  2014.

[4] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS  2014.
[5] S. Arora  R. Ge  and A. Moitra. Learning topic models – going beyond SVD. In FOCS  2012.
[6] Sanjeev Arora  Rong Ge  Yonatan Halpern  David Mimno  Ankur Moitra  David Sontag  Yichen Wu  and

Michael Zhu. A practical algorithm for topic modeling with provable guarantees. In ICML  2013.

[7] T. Hofmann. Probabilistic latent semantic analysis. In UAI  pages 289–296  1999.
[8] D. Blei  A. Ng  and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research  pages

993–1022  2003. Preliminary version in NIPS 2001.

[9] JamesP. Boyle and RichardL. Dykstra. A method for ﬁnding projections onto the intersection of convex
sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference  volume 37 of Lecture Notes
in Statistics  pages 28–47. Springer New York  1986.

[10] Adrian S. Lewis  D. R. Luke  and Jrme Malick. Local linear convergence for alternating and averaged

nonconvex projections. Foundations of Computational Mathematics  9:485–513  2009.

[11] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of

Sciences  101:5228–5235  2004.

[12] Moontae Lee and David Mimno. Low-dimensional embeddings for interpretable anchor-based topic
inference. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP)  pages 1319–1328. Association for Computational Linguistics  2014.

[13] Mary E Broadbent  Martin Brown  Kevin Penner  I Ipsen  and R Rehman. Subset selection algorithms:

Randomized vs. deterministic. SIAM Undergraduate Research Online  3:50–71  2010.

[14] D. Blei and J. Lafferty. A correlated topic model of science. Annals of Applied Statistics  pages 17–35 

2007.

[15] David Mimno  Hanna Wallach  Edmund Talley  Miriam Leenders  and Andrew McCallum. Optimizing

semantic coherence in topic models. In EMNLP  2011.

[16] A. Daniilidis  A. S. Lewis  J. Malick  and H. Sendov. Prox-regularity of spectral functions and spectral

sets. Journal of Convex Analysis  15(3):547–560  2008.

[17] Christian Thurau  Kristian Kersting  and Christian Bauckhage. Yes we can: simplex volume maximization

for descriptive web-scale matrix factorization. In CIKM’10  pages 1785–1788  2010.

[18] Abhishek Kumar  Vikas Sindhwani  and Prabhanjan Kambadur. Fast conical hull algorithms for near-

separable non-negative matrix factorization. CoRR  pages –1–1  2012.

[19] Jos M. P. Nascimento  Student Member  and Jos M. Bioucas Dias. Vertex component analysis: A fast
algorithm to unmix hyperspectral data. IEEE Transactions on Geoscience and Remote Sensing  pages
898–910  2005.

[20] C´ecile Gomez  H. Le Borgne  Pascal Allemand  Christophe Delacourt  and Patrick Ledru. N-FindR
method versus independent component analysis for lithological identiﬁcation in hyperspectral imagery.
International Journal of Remote Sensing  28(23):5315–5338  2007.

[21] Maximilian Nickel  Volker Tresp  and Hans-Peter Kriegel. A three-way model for collective learning on
multi-relational data. In Proceedings of the 28th International Conference on Machine Learning (ICML-
11)  ICML  pages 809–816. ACM  2011.

[22] Da Kuang  Haesun Park  and Chris H. Q. Ding. Symmetric nonnegative matrix factorization for graph

clustering. In SDM. SIAM / Omnipress  2012.

[23] Anima Anandkumar  Dean P. Foster  Daniel Hsu  Sham Kakade  and Yi-Kai Liu. A spectral algorithm
for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25: 26th Annual
Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December
3-6  2012  Lake Tahoe  Nevada  United States.  pages 926–934  2012.

[24] Thang Nguyen  Yuening Hu  and Jordan Boyd-Graber. Anchors regularized: Adding robustness and
extensibility to scalable topic-modeling algorithms. In Association for Computational Linguistics  2014.
[25] Tianyi Zhou  Jeff A Bilmes  and Carlos Guestrin. Divide-and-conquer learning by anchoring a conical

hull. In Advances in Neural Information Processing Systems 27  pages 1242–1250. 2014.

9

,Ramya Korlakai Vinayak
Samet Oymak
Babak Hassibi
Moontae Lee
David Bindel
David Mimno