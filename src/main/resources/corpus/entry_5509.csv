2013,The Randomized Dependence Coefficient,We introduce the Randomized Dependence Coefficient (RDC)  a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations  has low computational cost and is easy to implement: just five lines of R code  included at the end of the paper.,The Randomized Dependence Coefﬁcient

David Lopez-Paz  Philipp Hennig  Bernhard Sch¨olkopf

{dlopez phennig bs}@tue.mpg.de

Max Planck Institute for Intelligent Systems

Spemannstraße 38  T¨ubingen  Germany

Abstract

We introduce the Randomized Dependence Coefﬁcient (RDC)  a measure of non-
linear dependence between random variables of arbitrary dimension based on the
Hirschfeld-Gebelein-R´enyi Maximum Correlation Coefﬁcient. RDC is deﬁned in
terms of correlation of random non-linear copula projections; it is invariant with
respect to marginal distribution transformations  has low computational cost and
is easy to implement: just ﬁve lines of R code  included at the end of the paper.

1

Introduction

Measuring statistical dependence between random variables is a fundamental problem in statistics.
Commonly used measures of dependence  Pearson’s rho  Spearman’s rank or Kendall’s tau are com-
putationally efﬁcient and theoretically well understood  but consider only a limited class of asso-
ciation patterns  like linear or monotonically increasing functions. The development of non-linear
dependence measures is challenging because of the radically larger amount of possible association
patterns.
Despite these difﬁculties  many non-linear statistical dependence measures have been developed
recently. Examples include the Alternating Conditional Expectations or backﬁtting algorithm (ACE)
[2  9]  Kernel Canonical Correlation Analysis (KCCA) [1]  (Copula) Hilbert-Schmidt Independence
Criterion (CHSIC  HSIC) [6  5  15]  Distance or Brownian Correlation (dCor) [24  23] and the
Maximal Information Coefﬁcient (MIC) [18]. However  these methods exhibit high computational
demands (at least quadratic costs in the number of samples for KCCA  HSIC  CHSIC  dCor or
MIC)  are limited to measuring dependencies between scalar random variables (ACE  MIC) or can
be difﬁcult to implement (ACE  MIC).
This paper develops the Randomized Dependence Coefﬁcient (RDC)  an estimator of the Hirschfeld-
Gebelein-R´enyi Maximum Correlation Coefﬁcient (HGR) addressing the issues listed above. RDC
deﬁnes dependence between two random variables as the largest canonical correlation between ran-
dom non-linear projections of their respective empirical copula-transformations. RDC is invariant
to monotonically increasing transformations  operates on random variables of arbitrary dimension 
and has computational cost of O(n log n) with respect to the sample size. Moreover  it is easy to
implement: just ﬁve lines of R code  included in Appendix A.
The following Section reviews the classic work of Alfr´ed R´enyi [17]  who proposed seven desirable
fundamental properties of dependence measures  proved to be satisﬁed by the Hirschfeld-Gebelein-
R´enyi’s Maximum Correlation Coefﬁcient (HGR). Section 3 introduces the Randomized Depen-
dence Coefﬁcient as an estimator designed in the spirit of HGR  since HGR itself is computationally
intractable. Properties of RDC and its relationship to other non-linear dependence measures are
analysed in Section 4. Section 5 validates the empirical performance of RDC on a series of numeri-
cal experiments on both synthetic and real-world data.

1

2 Hirschfeld-Gebelein-R´enyi’s Maximum Correlation Coefﬁcient
In 1959 [17]  Alfr´ed R´enyi argued that a measure of dependence ρ∗ : X × Y → [0  1] between
random variables X ∈ X and Y ∈ Y should satisfy seven fundamental properties:

1. ρ∗(X  Y ) is deﬁned for any pair of non-constant random variables X and Y .
2. ρ∗(X  Y ) = ρ∗(Y  X)
3. 0 ≤ ρ∗(X  Y ) ≤ 1
4. ρ∗(X  Y ) = 0 iff X and Y are statistically independent.
5. For bijective Borel-measurable functions f  g : R → R  ρ∗(X  Y ) = ρ∗(f (X)  g(Y )).
6. ρ∗(X  Y ) = 1 if for Borel-measurable functions f or g  Y = f (X) or X = g(Y ).
7. If (X  Y ) ∼ N (µ  Σ)  then ρ∗(X  Y ) = |ρ(X  Y )|  where ρ is the correlation coefﬁcient.
R´enyi also showed the Hirschfeld-Gebelein-R´enyi Maximum Correlation Coefﬁcient (HGR) [3  17]
to satisfy all these properties. HGR was deﬁned by Gebelein in 1941 [3] as the supremum of Pear-
son’s correlation coefﬁcient ρ over all Borel-measurable functions f  g of ﬁnite variance:

hgr(X  Y ) = sup
f g

ρ(f (X)  g(Y )) 

(1)

Since the supremum in (1) is over an inﬁnite-dimensional space  HGR is not computable.
It is
an abstract concept  not a practical dependence measure. In the following we propose a scalable
estimator with the same structure as HGR: the Randomized Dependence Coefﬁcient.

3 Randomized Dependence Coefﬁcient

The Randomized Dependence Coefﬁcient (RDC) measures the dependence between random samples
X ∈ Rp×n and Y ∈ Rq×n as the largest canonical correlation between k randomly chosen non-
linear projections of their copula transformations. Before Section 3.4 deﬁnes this concept formally 
we describe the three necessary steps to construct the RDC statistic: copula-transformation of each
of the two random samples (Section 3.1)  projection of the copulas through k randomly chosen non-
linear maps (Section 3.2) and computation of the largest canonical correlation between the two sets
of non-linear random projections (Section 3.3). Figure 1 offers a sketch of this process.

Figure 1: RDC computation for a simple set of samples {(xi  yi)}100
i=1 drawn from a noisy circular
pattern: The samples are used to estimate the copula  then mapped with randomly drawn non-linear
functions. The RDC is the largest canonical correlation between these non-linear projections.

3.1 Estimation of Copula-Transformations

To achieve invariance with respect to transformations on marginal distributions (such as shifts or
rescalings)  we operate on the empirical copula transformation of the data [14  15]. Consider a ran-
dom vector X = (X1  . . .   Xd) with continuous marginal cumulative distribution functions (cdfs)
Pi  1 ≤ i ≤ d. Then the vector U = (U1  . . .   Ud) := P (X) = (P1(X1)  . . .   Pd(Xd))  known as
the copula transformation  has uniform marginals:

2

yxP(y)∼U[0 1]P(x)∼U[0 1]βTΦ(P(y))αTΦ(P(x))αTΦ(P(x))βTΦ(P(y))P(x)P(y)P(x)P(y)φ(wiP(x)+bi)φ(miP(y)+li)ρ≈0ρ≈0ρ≈1CCATheorem 1. (Probability Integral Transform [14]) For a random variable X with cdf P   the random
variable U := P (X) is uniformly distributed on [0  1].

The random variables U1  . . .   Ud are known as the observation ranks of X1  . . .   Xd. Crucially 
U preserves the dependence structure of the original random vector X  but ignores each of its d
marginal forms [14]. The joint distribution of U is known as the copula of X:
Theorem 2. (Sklar [20]) Let the random vector X = (X1  . . .   Xd) have continuous marginal cdfs
Pi  1 ≤ i ≤ d. Then  the joint cumulative distribution of X is uniquely expressed as:

where the distribution C is known as the copula of X.

P (X1  . . .   Xd) = C(P1(X1)  . . .   Pd(Xd)) 

A practical estimator of the univariate cdfs P1  . . .   Pd is the empirical cdf :

which gives rise to the empirical copula transformations of a multivariate sample:

Pn(x) :=

1
n

I(Xi ≤ x) 

n(cid:88)i=1

Pn(x) = [Pn 1(x1)  . . .   Pn d(xd)].

(2)

(3)

(4)

The Massart-Dvoretzky-Kiefer-Wolfowitz inequality [13] can be used to show that empirical copula
transformations converge fast to the true transformation as the sample size increases:
Theorem 3. (Convergence of the empirical copula  [15  Lemma 7]) Let X1  . . .   Xn be an i.i.d.
sample from a probability distribution over Rd with marginal cdf’s P1  . . .   Pd. Let P (X) be the
copula transformation and Pn(X) the empirical copula transformation. Then  for any  > 0:

Pr(cid:20) sup
x∈Rd (cid:107)P (x) − Pn(x)(cid:107)2 > (cid:21) ≤ 2d exp(cid:18)−

2n2

d (cid:19) .

(5)

Computing Pn(X) involves sorting the marginals of X ∈ Rd×n  thus O(dn log(n)) operations.
3.2 Generation of Random Non-Linear Projections

The second step of the RDC computation is to augment the empirical copula transformations with
non-linear projections  so that linear methods can subsequently be used to capture non-linear depen-
dencies on the original data. This is a classic idea also used in other areas  particularly in regression.
In an elegant result  Rahimi and Recht [16] proved that linear regression on random  non-linear
projections of the original feature space can generate high-performance regressors:
Theorem 4.

(Rahimi-Recht) Let p be a distribution on Ω and |φ(x; w)| ≤ 1. Let F =
i=1 drawn iid from some
i=1 αiφ(x; wi) minimizes the empirical

(cid:8) f (x) =(cid:82)Ω α(w)φ(x; w)dw(cid:12)(cid:12)|α(w)| ≤ Cp(w)(cid:9). Draw w1  . . .   wk iid from p. Further let
δ > 0  and c be some L-Lipschitz loss function  and consider data {xi  yi}n
arbitrary P (X  Y ). The α1  . . .   αk for which fk(x) =(cid:80)k
risk c(fk(x)  y) has a distance from the c-optimal estimator in F bounded by
EP [c(f (x)  y)] ≤ O(cid:32)(cid:18) 1
with probability at least 1 − 2δ.
Intuitively  Theorem 4 states that randomly selecting wi in(cid:80)k
i=1 αiφ(x; wi) instead of optimising
them causes only bounded error.
The choice of the non-linearities φ : R → R is the main and unavoidable assumption in RDC.
This choice is a well-known problem common to all non-linear regression methods and has been
studied extensively in the theory of regression as the selection of reproducing kernel Hilbert space
[19  §3.13]. The only way to favour one such family and distribution over another is to use prior
assumptions about which kind of distributions the method will typically have to analyse.

1

√k(cid:19) LC(cid:114)log

EP [c(fk(x)  y)] − min
f∈F

1

δ(cid:33)

+

√n

(6)

3

We use random features instead of the Nystr¨om method because of their smaller memory and com-
putation requirements [11]. In our experiments  we will use sinusoidal projections  φ(wT x + b) :=
sin(wT x + b). Arguments favouring this choice are that shift-invariant kernels are approximated
with these features when using the appropriate random parameter sampling distribution [16]  [4 
p. 208] [22  p. 24]  and that functions with absolutely integrable Fourier transforms are approxi-
mated with L2 error below O(1/√k) by k of these features [10].
Let the random parameters wi ∼ N (0  sI)  bi ∼ N (0  s). Choosing wi to be Normal is analogous
to the use of the Gaussian kernel for HSIC  CHSIC or KCCA [16]. Tuning s is analogous to selecting
the kernel width  that is  to regularize the non-linearity of the random projections.
Given a data collection X = (x1  . . .   xn)  we will denote by

Φ(X; k  s) :=

φ(wT

1 x1 + b1)

k x1 + bk)

...

··· φ(wT
...
··· φ(wT

...

T



φ(wT

1 xn + b1)

k xn + bk)

the k−th order random non-linear projection from X ∈ Rd×n to Φ(X; k  s) ∈ Rk×n. The com-
putational complexity of computing Φ(X; k  s) with naive matrix multiplications is O(kdn). How-
ever  recent techniques using fast Walsh-Hadamard transforms [11] allow computing these feature
expansions within a computational cost of O(k log(d)n) and O(k) storage.

(7)

(8)

3.3 Computation of Canonical Correlations

The ﬁnal step of RDC is to compute the linear combinations of the augmented empirical copula
transformations that have maximal correlation. Canonical Correlation Analysis (CCA  [7]) is the
calculation of pairs of basis vectors (α  β) such that the projections αT X and βT Y of two ran-
dom samples X ∈ Rp×n and Y ∈ Rq×n are maximally correlated. The correlations between the
projected (or canonical) random samples are referred to as canonical correlations. There exist up to
max(rank(X)  rank(Y )) of them. Canonical correlations ρ2 are the solutions to the eigenproblem:

0
C−1
yy Cyx

C−1
xx Cxy
0

(cid:18)

(cid:19)(cid:18) α

β (cid:19) = ρ2(cid:18) α
β (cid:19)  

where Cxy = cov(X  Y ) and the matrices Cxx and Cyy are assumed to be invertible. Therefore 
the largest canonical correlation ρ1 between X and Y is the supremum of the correlation coefﬁcients
over their linear projections  that is: ρ1(X  Y ) = supα β ρ(αT X  βT Y ).
When p  q (cid:28) n  the cost of CCA is dominated by the estimation of the matrices Cxx  Cyy and Cxy 
hence being O((p + q)2n) for two random variables of dimensions p and q  respectively.

3.4 Formal Deﬁnition or RDC
Given the random samples X ∈ Rp×n and Y ∈ Rq×n and the parameters k ∈ N+ and s ∈ R+  the
Randomized Dependence Coefﬁcient between X and Y is deﬁned as:
(9)

ρ(cid:0)αT Φ(P (X); k  s)  βT Φ(P (Y ); k  s)(cid:1) .

rdc(X  Y ; k  s) := sup
α β

4 Properties of RDC

Computational complexity:
In the typical setup (very large n  large p and q  small k) the compu-
tational complexity of RDC is dominated by the calculation of the copula-transformations. Hence 
we achieve a cost in terms of the sample size of O((p+q)n log n+kn log(pq)+k2n) ≈ O(n log n).
Ease of implementation: An implementation of RDC in R is included in the Appendix A.

Relationship to the HGR coefﬁcient:
It is tempting to wonder whether RDC is a consistent  or
even an efﬁcient estimator of the HGR coefﬁcient. However  a simple experiment shows that it is not
desirable to approximate HGR exactly on ﬁnite datasets: Consider p(X  Y ) = N (x; 0  1)N (y; 0  1)

4

which is independent  thus  by both R´enyi’s 4th and 7th properties  has hgr(X  Y ) = 0. How-
ever  for ﬁnitely many N samples from p(X  Y )  almost surely  values in both X and Y are
pairwise different and separated by a ﬁnite difference. So there exist continuous (thus Borel
measurable) functions f (X) and g(Y ) mapping both X and Y to the sorting ranks of Y   i.e.
f (xi) = g(yi) ∀(xi  yi) ∈ (X  Y ). Therefore  the ﬁnite-sample version of Equation (1) is con-
stant and equal to “1” for continuous random variables. Meaningful measures of dependence from
ﬁnite samples thus must rely on some form of regularization. RDC achieves this by approximating
the space of Borel measurable functions with the restricted function class F from Theorem 4:
Assume the optimal transformations f and g (Equation 1) to belong to the Reproducing Kernel
Hilbert Space F (Theorem 4)  with associated shift-invariant  positive semi-deﬁnite kernel function
k(x  x(cid:48)) = (cid:104)φ(x)  φ(x(cid:48))(cid:105)F ≤ 1. Then  with probability greater than 1 − 2δ:
√k(cid:19)(cid:114)log

hgr(X  Y ;F) − rdc(X  Y ; k) = O(cid:32)(cid:18)(cid:107)m(cid:107)F√n

δ(cid:33)  

(10)

LC

+

1

where m := ααT + ββT and n  k denote the sample size and number of random features. The
bound (10) is the sum of two errors. The error O(1/√n) is due to the convergence of CCA’s
largest eigenvalue in the ﬁnite sample size regime. This result [8  Theorem 6] is originally ob-
tained by posing CCA as a least squares regression on the product space induced by the feature map
ψ(x  y) = [φ(x)φ(x)T   φ(y)φ(y)T  √2φ(x)φ(y)T ]T . Because of approximating ψ with k ran-
dom features  an additional error O(1/√k) is introduced in the least squares regression [16  Lemma
3]. Therefore  an equivalence between RDC and KCCA is established if RDC uses an inﬁnite num-
ber of sinusoidal features  the random sampling distribution is set to the inverse Fourier transform
of the shift-invariant kernel used by KCCA and the copula-transformations are discarded. However 
when k ≥ n regularization is needed to avoid spurious perfect correlations  as discussed above.
Relationship to other estimators: Table 1 summarizes several state-of-the-art dependence mea-
sures showing  for each measure  whether it allows for general non-linear dependence estimation 
handles multidimensional random variables  is invariant with respect to changes in marginal distri-
butions  returns a statistic in [0  1]  satisfy R´enyi’s properties (Section 2)  and how many parameters
it requires. As parameters  we here count the kernel function for kernel methods  the basis function
and number of random features for RDC  the stopping tolerance for ACE and the search-grid size for
MIC  respectively. Finally  the table lists computational complexities with respect to sample size.
When using random features φ linear for some neighbourhood around zero (like sinusoids or sig-
moids)  RDC converges to Spearman’s rank correlation coefﬁcient as s → 0  for any k.

Table 1: Comparison between non-linear dependence measures.
Non-
Linear

Renyi’s
Properties

Marginal
Invariant

Vector
Inputs

Coeff.
∈ [0  1]

Name of
Coeff.

Pearson’s ρ
Spearman’s ρ
Kendall’s τ
CCA
KCCA [1]
ACE [2]
MIC [18]
dCor [24]
HSIC [5]
CHSIC [15]
RDC

×
×
×
×
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

×
×
×
(cid:88)
(cid:88)
×
×
(cid:88)
(cid:88)
(cid:88)
(cid:88)

×
(cid:88)
(cid:88)
×
×
×
×
×
×
(cid:88)
(cid:88)

×
×
×
×
×
(cid:88)
×
×
×
×
(cid:88)

# Par. Comp.
Cost
n
n log n
n log n
n
n3
n
n1.2
n2
n2
n2
n log n

0
0
0
0
1
1
1
1
1
1
2

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
×
×
(cid:88)

Testing for independence with RDC: Consider the hypothesis “the two sets of non-linear projec-
tions are mutually uncorrelated”. Under normality assumptions (or large sample sizes)  Bartlett’s ap-
k2  where ρ1  . . .   ρk are the

proximation [12] can be used to show(cid:0) 2k+3

2 − n(cid:1) log(cid:81)k

i=1(1−ρ2

i ) ∼ χ2

5

canonical correlations between Φ(P (X); k  s) and Φ(P (Y ); k  s). Alternatively  non-parametric
asymptotic distributions can be obtained from the spectrum of the inner products of the non-linear
random projection matrices [25  Theorem 3].

5 Experimental Results

We performed experiments on both synthetic and real-world data to validate the empirical perfor-
mance of RDC versus the non-linear dependence measures listed in Table 1. In some experiments
we do not compare against to KCCA because we were unable to ﬁnd a good set of hyperparameters.

Parameter selection: For RDC  the number of random features is set to k = 20 for both random
samples  since no signiﬁcant improvements were observed for larger values. The random feature
sampling parameter s is more crucial  and set as follows: when the marginals of u are standard

uniforms  w ∼ N (0  sI) and b ∼ N (0  s)  then V[wT u + b] = s(cid:0)1 + d
s to a linear scaling of the input variable dimensionality. In all our experiments s = 1
6d worked well.
The development of better methods to set the parameters of RDC is left as future work.
HSIC and CHSIC use Gaussian kernels k(z  z(cid:48)) = exp(−γ(cid:107)z− z(cid:48)
2) with γ−1 set to the euclidean
(cid:107)2
distance median of each sample [5]. MIC’s search-grid size is set to B(n) = n0.6 as recommended
by the authors [18]  although speed improvements are achieved when using lower values. ACE’s
tolerance is set to  = 0.01  default value in the R package acepack.

3(cid:1); therefore  we opt to set

5.1 Synthetic Data

Resistance to additive noise: We deﬁne the power of a dependence measure as its ability to
discern between dependent and independent samples that share equal marginal forms. In the spirit
of Simon and Tibshirani1  we conducted experiments to estimate the power of RDC as a measure
of non-linear dependence. We chose 8 bivariate association patterns  depicted inside little boxes in
Figure 3. For each of the 8 association patterns  500 repetitions of 500 samples were generated 
in which the input sample was uniformly distributed on the unit interval. Next  we regenerated
the input sample randomly  to generate independent versions of each sample with equal marginals.
Figure 3 shows the power for the discussed non-linear dependence measures as the variance of some
zero-mean Gaussian additive noise increases from 1/30 to 3. RDC shows worse performance in
the linear association pattern due to overﬁtting and in the step-function due to the smoothness prior
induced by the sinusoidal features  but has good performance in non-functional association patterns.

Running times: Table 2 shows running times for the considered non-linear dependence measures
on scalar  uniformly distributed  independent samples of sizes {103  . . .   106} when averaging over
100 runs. Single runs above ten minutes were cancelled. Pearson’s ρ  ACE  dCor  KCCA and MIC
are implemented in C  while RDC  HSIC and CHSIC are implemented as interpreted R code. KCCA
is approximated using incomplete Cholesky decompositions as described in [1].

Pearson’s ρ

Table 2: Average running times (in seconds) for dependence measures on versus sample sizes.
sample size
HSIC CHSIC MIC
0.3103
1 000
1.0983
10 000
27.630
100 000
1 000 000

ACE KCCA
0.402
0.0080
3.247
0.0782
0.5101
43.801
5.3830

RDC
0.0047
0.0557
0.3991
4.6253

0.0001
0.0002
0.0071
0.0914

0.3501
29.522

—
—

—
—
—

dCor
0.3417
59.587

—
—

—
—

—

Value of statistic in [0  1]: Figure 4 shows RDC  ACE  dCor  MIC  Pearson’s ρ  Spearman’s rank
and Kendall’s τ dependence estimates for 14 different associations of two scalar random samples.
RDC scores values close to one on all the proposed dependent associations  whilst scoring values
close to zero for the independent association  depicted last. When the associations are Gaussian (ﬁrst
row)  RDC scores values close to the Pearson’s correlation coefﬁcient (Section 2  7th property).

1http://www-stat.stanford.edu/˜tibs/reshef/comment.pdf

6

5.2 Feature Selection in Real-World Data

We performed greedy feature selection via dependence maximization [21] on eight real-world
datasets. More speciﬁcally  we attempted to construct the subset of features G ⊂ X that mini-
mizes the normalized mean squared regression error (NMSE) of a Gaussian process regressor. We
do so by selecting the feature x(i) maximizing dependence between the feature set Gi = {Gi−1  x(i)}
and the target variable y at each iteration i ∈ {1  . . . 10}  such that G0 = {∅} and x(i) /∈ Gi−1.
We considered 12 heterogeneous datasets  obtained from the UCI dataset repository2  the Gaus-
sian process web site Data3 and the Machine Learning data set repository4. Random training/test
partitions are computed to be disjoint and equal sized.
Since G can be multi-dimensional  we compare RDC to the non-linear methods dCor  HSIC and
CHSIC. Given their quadratic computational demands  dCor  HSIC and CHSIC use up to 1  000
points when measuring dependence; this constraint only applied on the sarcos and abalone
datasets. Results are averages of 20 random training/test partitions.

Figure 2: Feature selection experiments on real-world datasets.

Figure 2 summarizes the results for all datasets and algorithms as the number of selected features
increases. RDC performs best in most datasets  with much lower running time than its contenders.

6 Conclusion

We have presented the randomized dependence coefﬁcient  a lightweight non-linear measure of
dependence between multivariate random samples. Constructed as a ﬁnite-dimensional estimator in
the spirit of the Hirschfeld-Gebelein-R´enyi maximum correlation coefﬁcient  RDC performs well
empirically  is scalable to very large datasets  and is easy to adapt to concrete problems.
We thank fruitful discussions with Alberto Su´arez  Theofanis Karaletsos and David Reshef.

2http://www.ics.uci.edu/˜mlearn
3http://www.gaussianprocess.org/gpml/data/
4http://www.mldata.org

7

12345670.480.540.60abalone2468100.360.400.440.48automobile12345670.150.250.35autompg2468100.350.45breastdCorHSICCHSICRDC123456780.300.400.50calhousing2468100.30.50.70.9cpuact2468100.200.30crime2468100.250.35housing2468100.9650.980insurance2468100.350.400.45parkinson2468100.10.30.50.7sarcos2468100.700.80whitewineNumberoffeaturesGaussianProcessNMSEFigure 3: Power of discussed measures on several bivariate association patterns as noise increases.
Insets show the noise-free form of each association pattern.

Figure 4: RDC  ACE  dCor  MIC  Pearson’s ρ  Spearman’s rank and Kendall’s τ estimates (numbers
in tables above plots  in that order) for several bivariate association patterns.

A R Source Code

rdc <- function(x y k=20 s=1/6 f=sin) {

x <- cbind(apply(as.matrix(x) 2 function(u)rank(u)/length(u)) 1)
y <- cbind(apply(as.matrix(y) 2 function(u)rank(u)/length(u)) 1)
x <- s/ncol(x)*x%*%matrix(rnorm(ncol(x)*k) ncol(x))
y <- s/ncol(y)*y%*%matrix(rnorm(ncol(y)*k) ncol(y))
cancor(cbind(f(x) 1) cbind(f(y) 1))$cor[1]

}

8

0.00.40.8xvalspower.cor[typ ]xvalspower.cor[typ ]0.00.40.8xvalspower.cor[typ ]xvalspower.cor[typ ]cordCorMICACEHSICCHSICRDC0.00.40.8xvalspower.cor[typ ]xvalspower.cor[typ ]0204060801000.00.40.8xvalspower.cor[typ ]020406080100xvalspower.cor[typ ]NoiseLevelPower1.01.01.01.01.01.01.00.80.80.70.50.80.80.60.40.40.40.20.40.40.30.10.10.10.10.00.00.00.40.40.40.2-0.4-0.4-0.30.80.80.70.5-0.8-0.8-0.61.01.01.01.0-1.0-1.0-1.01.01.00.41.00.00.00.00.30.30.10.20.00.0-0.00.50.50.10.20.00.00.01.01.00.50.90.00.00.01.01.00.30.60.10.10.11.01.00.20.6-0.0-0.0-0.00.10.10.00.1-0.0-0.0-0.0References
[1] F. R. Bach and M. I. Jordan. Kernel independent component analysis. JMLR  3:1–48  2002.
[2] L. Breiman and J. H. Friedman. Estimating Optimal Transformations for Multiple Regression

and Correlation. Journal of the American Statistical Association  80(391):580–598  1985.

[3] H. Gebelein. Das statistische Problem der Korrelation als Variations- und Eigenwertproblem
und sein Zusammenhang mit der Ausgleichsrechnung. Zeitschrift f¨ur Angewandte Mathematik
und Mechanik  21(6):364–379  1941.

[4] I.I. Gihman and A.V. Skorohod. The Theory of Stochastic Processes  volume 1. Springer 

1974s.

[5] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Sch¨olkopf  and A. Smola. A kernel two-sample

test. JMLR  13:723–773  2012.

[6] A. Gretton  O. Bousquet  A. Smola  and B. Sch¨olkopf. Measuring statistical dependence with
Hilbert-Schmidt norms. In Proceedings of the 16th international conference on Algorithmic
Learning Theory  pages 63–77. Springer-Verlag  2005.

[7] W. K. H¨ardle and L. Simar. Applied Multivariate Statistical Analysis. Springer  2nd edition 

2007.

[8] D. Hardoon and J. Shawe-Taylor. Convergence analysis of kernel canonical correlation analy-

sis: theory and practice. Machine Learning  74(1):23–38  2009.

[9] T. Hastie and R. Tibshirani. Generalized additive models. Statistical Science  1:297–310  1986.
[10] L. K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence rates
for projection pursuit regression and neural network training. Annals of Statistics  20(1):608–
613  1992.

[11] Q. Le  T. Sarlos  and A. Smola. Fastfood – Approximating kernel expansions in loglinear time.

In ICML  2013.

[12] K. V. Mardia  J. T. Kent  and J. M. Bibby. Multivariate Analysis. Academic Press  1979.
[13] P. Massart. The tight constant in the Dvoretzky-Kiefer-wolfowitz inequality. The Annals of

Probability  18(3)  1990.

[14] R. Nelsen. An Introduction to Copulas. Springer Series in Statistics  2nd edition  2006.
[15] B. Poczos  Z. Ghahramani  and J. Schneider. Copula-based kernel dependency measures. In

ICML  2012.

[16] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization

with randomization in learning. NIPS  2008.

[17] A. R´enyi. On measures of dependence. Acta Mathematica Academiae Scientiarum Hungari-

cae  10:441–451  1959.

[18] D. N. Reshef  Y. A. Reshef  H. K. Finucane  S. R. Grossman  G. McVean  P. J. Turnbaugh 
E. S. Lander  M. Mitzenmacher  and P. C. Sabeti. Detecting novel associations in large data
sets. Science  334(6062):1518–1524  2011.

[19] B. Sch¨olkopf and A.J. Smola. Learning with Kernels. MIT Press  2002.
[20] A. Sklar. Fonctions de repartition `a n dimension set leurs marges. Publ. Inst. Statis. Univ.

Paris  8(1):229–231  1959.

[21] L. Song  A. Smola  A. Gretton  J. Bedo  and K. Borgwardt. Feature selection via dependence

maximization. JMLR  13:1393–1434  June 2012.

[22] M.L. Stein. Interpolation of Spatial Data. Springer  1999.
[23] G. J. Sz´ekely and M. L. Rizzo. Rejoinder: Brownian distance covariance. Annals of Applied

Statistics  3(4):1303–1308  2009.

[24] G. J. Sz´ekely  M. L. Rizzo  and N. K. Bakirov. Measuring and testing dependence by correla-

tion of distances. Annals of Statistics  35(6)  2007.

[25] K. Zhang  J. Peters  D. Janzing  and B.Sch¨olkopf. Kernel-based conditional independence test

and application in causal discovery. CoRR  abs/1202.3775  2012.

9

,David Lopez-Paz
Philipp Hennig
Bernhard Schölkopf