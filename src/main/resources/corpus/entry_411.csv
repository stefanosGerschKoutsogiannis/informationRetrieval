2018,Adaptive Online Learning in Dynamic Environments,In this paper  we study online convex optimization in dynamic environments  and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an $O(\sqrt{T}(1+P_T))$ dynamic regret  where $T$ is the number of iterations and $P_T$ is the path-length of the comparator sequence.  However  this result is unsatisfactory  as there exists a large gap from the $\Omega(\sqrt{T(1+P_T)})$ lower bound established in our paper. To address this limitation  we develop a novel online method  namely adaptive learning for dynamic environment (Ader)  which achieves an optimal $O(\sqrt{T(1+P_T)})$ dynamic regret. The basic idea is to maintain a set of experts  each attaining an optimal dynamic regret for a specific path-length  and combines them with an expert-tracking algorithm.  Furthermore  we propose an improved Ader based on the surrogate loss  and in this way the number of gradient evaluations per round is reduced from $O(\log T)$ to $1$. Finally  we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.,Adaptive Online Learning in Dynamic Environments

Lijun Zhang 

Shiyin Lu  Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology

Nanjing University  Nanjing 210023  China

{zhanglj  lusy  zhouzh}@lamda.nju.edu.cn

Abstract

In this paper  we study online convex optimization in dynamic environments  and
√
aim to bound the dynamic regret with respect to any sequence of comparators.
Existing work have shown that online gradient descent enjoys an O(
T (1 + PT ))
dynamic regret  where T is the number of iterations and PT is the path-length of
the comparator sequence. However  this result is unsatisfactory  as there exists

a large gap from the Ω((cid:112)T (1 + PT )) lower bound established in our paper. To
for dynamic environment (Ader)  which achieves an optimal O((cid:112)T (1 + PT ))

address this limitation  we develop a novel online method  namely adaptive learning

dynamic regret. The basic idea is to maintain a set of experts  each attaining an
optimal dynamic regret for a speciﬁc path-length  and combines them with an
expert-tracking algorithm. Furthermore  we propose an improved Ader based on
the surrogate loss  and in this way the number of gradient evaluations per round is
reduced from O(log T ) to 1. Finally  we extend Ader to the setting that a sequence
of dynamical models is available to characterize the comparators.

1

Introduction

Online convex optimization (OCO) has become a popular learning framework for modeling various
real-world problems  such as online routing  ad selection for search engines and spam ﬁltering [Hazan 
2016]. The protocol of OCO is as follows: At iteration t  the online learner chooses xt from a convex
set X . After the learner has committed to this choice  a convex cost function ft : X (cid:55)→ R is revealed.
Then  the learner suffers an instantaneous loss ft(xt)  and the goal is to minimize the cumulative loss
over T iterations. The standard performance measure of OCO is regret:

T(cid:88)

t=1

ft(xt) − min
x∈X

T(cid:88)

t=1

ft(x)

(1)

which is the cumulative loss of the learner minus that of the best constant point chosen in hindsight.
The notion of regret has been extensively studied  and there exist plenty of algorithms and theories
for minimizing regret [Shalev-Shwartz et al.  2007  Hazan et al.  2007  Srebro et al.  2010  Duchi
et al.  2011  Shalev-Shwartz  2011  Zhang et al.  2013]. However  when the environment is changing 
the traditional regret is no longer a suitable measure  since it compares the learner against a static
point. To address this limitation  recent advances in online learning have introduced an enhanced
measure—dynamic regret  which received considerable research interest over the years [Hall and
Willett  2013  Jadbabaie et al.  2015  Mokhtari et al.  2016  Yang et al.  2016  Zhang et al.  2017].
In the literature  there are two different forms of dynamic regret. The general one is introduced by
Zinkevich [2003]  who proposes to compare the cumulative loss of the learner against any sequence

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

of comparators

T(cid:88)

ft(xt) − T(cid:88)

R(u1  . . .   uT ) =

(2)
where u1  . . .   uT ∈ X . Instead of following the deﬁnition in (2)  most of existing studies on dynamic
regret consider a restricted form  in which the sequence of comparators consists of local minimizers
of online functions [Besbes et al.  2015]  i.e. 

ft(ut)

t=1

t=1

R(x∗

1  . . .   x∗

T ) =

ft(x∗

t ) =

T(cid:88)

t=1

ft(xt) − T(cid:88)

t=1

T(cid:88)

t=1

ft(xt) − T(cid:88)

t=1

min
x∈X ft(x)

(3)

1  . . .   x∗

1  . . .   x∗
1  . . .   x∗

T )  it does not mean the notion of R(x∗
T ) could be very loose for R(u1  . . .   uT ).

t ∈ argminx∈X ft(x) is a minimizer of ft(·) over domain X . Note that although
T ) is stronger  because

where x∗
R(u1  . . .   uT ) ≤ R(x∗
an upper bound for R(x∗
The general dynamic regret in (2) includes the static regret in (1) and the restricted dynamic regret
in (3) as special cases. Thus  minimizing the general dynamic regret can automatically adapt to
the nature of environments  stationary or dynamic. In contrast  the restricted dynamic regret is too
pessimistic  and unsuitable for stationary problems. For example  it is meaningless to the problem of
statistical machine learning  where ft’s are sampled independently from the same distribution. Due
to the random perturbation caused by sampling  the local minimizers could differ signiﬁcantly from
the global minimizer of the expected loss. In this case  minimizing (3) will lead to overﬁtting.
Because of its ﬂexibility  we focus on the general dynamic regret in this paper. Bounding the general
dynamic regret is very challenging  because we need to establish a universal guarantee that holds
for any sequence of comparators. By comparison  when bounding the restricted dynamic regret  we
only need to focus on the local minimizers. Till now  we have very limited knowledge on the general
dynamic regret. One result is given by Zinkevich [2003]  who demonstrates that online gradient
descent (OGD) achieves the following dynamic regret bound

(cid:16)√

(cid:17)

R(u1  . . .   uT ) = O

T (1 + PT )

(4)

where PT   deﬁned in (5)  is the path-length of u1  . . .   uT .
However  the linear dependence on PT in (4) is too loose  and there is a large gap between the upper

bound and the Ω((cid:112)T (1 + PT )) lower bound established in our paper. To address this limitation  we
attains an O((cid:112)T (1 + PT )) dynamic regret. Ader follows the framework of learning with expert

propose a novel online method  namely adaptive learning for dynamic environment (Ader)  which

advice [Cesa-Bianchi and Lugosi  2006]  and is inspired by the strategy of maintaining multiple
learning rates in MetaGrad [van Erven and Koolen  2016]. The basic idea is to run multiple OGD
algorithms in parallel  each with a different step size that is optimal for a speciﬁc path-length  and
combine them with an expert-tracking algorithm. While the basic version of Ader needs to query
the gradient O(log T ) times in each round  we develop an improved version based on surrogate loss
and reduce the number of gradient evaluations to 1. Finally  we provide extensions of Ader to the
case that a sequence of dynamical models is given  and obtain tighter bounds when the comparator
sequence follows the dynamical models closely.
The contributions of this paper are summarized below.

Ω((cid:112)T (1 + PT )).
an optimal O((cid:112)T (1 + PT )) upper bound.

• We establish the ﬁrst
lower bound for the general regret bound in (2)  which is
• We develop a serial of novel methods for minimizing the general dynamic regret  and prove
• Compared to existing work for the restricted dynamic regret in (3)  our result is universal in
• Our result is also adaptive because the upper bound depends on the path-length of the
comparator sequence  so it automatically becomes small when comparators change slowly.

the sense that the regret bound holds for any sequence of comparators.

2 Related Work

In this section  we provide a brief review of related work in online convex optimization.

2

2.1 Static Regret

√

√

In static setting  online gradient descent (OGD) achieves an O(
T ) regret bound for general convex
functions. If the online functions have additional curvature properties  then faster rates are attaina-
ble. For strongly convex functions  the regret bound of OGD becomes O(log T ) [Shalev-Shwartz
et al.  2007]. The O(
T ) and O(log T ) regret bounds  for convex and strongly convex functions
respectively  are known to be minimax optimal [Abernethy et al.  2008]. For exponentially concave
functions  Online Newton Step (ONS) enjoys an O(d log T ) regret  where d is the dimensionality
[Hazan et al.  2007]. When the online functions are both smooth and convex  the regret bound could
also be improved if the cumulative loss of the optimal prediction is small [Srebro et al.  2010].

2.2 Dynamic Regret

To the best of our knowledge  there are only two studies that investigate the general dynamic regret
[Zinkevich  2003  Hall and Willett  2013]. While it is impossible to achieve a sublinear dynamic
regret in general  we can bound the dynamic regret in terms of certain regularity of the comparator
sequence or the function sequence. Zinkevich [2003] introduces the path-length

PT (u1  . . .   uT ) =

(cid:107)ut − ut−1(cid:107)2

(5)

and provides an upper bound for OGD in (4). In a subsequent work  Hall and Willett [2013] propose
a variant of path-length

P (cid:48)
T (u1  . . .   uT ) =

(cid:107)ut+1 − Φt(ut)(cid:107)2

(6)

in which a sequence of dynamical models Φt(·) : X (cid:55)→ X is incorporated. Then  they develop a new
T (1 + P (cid:48)
T )) dynamic regret. When the
method  dynamic mirror descent  which achieves an O(
comparator sequence follows the dynamical models closely  P (cid:48)
T could be much smaller than PT   and
thus the upper bound of Hall and Willett [2013] could be tighter than that of Zinkevich [2003].
For the restricted dynamic regret  a powerful baseline  which simply plays the minimizer of previous
round  i.e.  xt+1 = argminx∈X ft(x)  attains an O(P ∗
T ) dynamic regret [Yang et al.  2016]  where

√

T(cid:88)

t=2

T(cid:88)

t=1

T(cid:88)

t=2

T(cid:88)

T(cid:88)

t=2

3

P ∗
T := PT (x∗

1  . . .   x∗

T ) =

(cid:107)x∗

t − x∗

t−1(cid:107)2.

OGD also achieves the O(P ∗
T ) dynamic regret  when the online functions are strongly convex and
smooth [Mokhtari et al.  2016]  or when they are convex and smooth and all the minimizers lie in
the interior of X [Yang et al.  2016]. Another regularity of the comparator sequence is the squared
path-length

S∗
T := ST (x∗

1  . . .   x∗
which could be smaller than the path-length P ∗
[2017] propose online multiple gradient descent  and establish an O(min(P ∗
(semi-)strongly convex and smooth functions.
In a recent work  Besbes et al. [2015] introduce the functional variation

t − x∗

t−1(cid:107)2

(cid:107)x∗

T ) =

t=2

2

T when local minimizers move slowly. Zhang et al.
T )) regret bound for

T   S∗

FT := F (f1  . . .   fT ) =

x∈X |ft(x) − ft−1(x)|

max

to measure the complexity of the function sequence. Under the assumption that an upper bound
VT ≥ FT is known beforehand  Besbes et al. [2015] develop a restarted online gradient descent  and

prove its dynamic regret is upper bounded by O(T 2/3(VT + 1)1/3) and O(log T(cid:112)T (VT + 1)) for

convex functions and strongly convex functions  respectively. One limitation of this work is that
the bounds are not adaptive because they depend on the upper bound VT . So  even when the actual
functional variation FT is small  the regret bounds do not become better.

One regularity that involves the gradient of functions is

T(cid:88)

DT =

(cid:107)∇ft(xt) − mt(cid:107)2

2

t=1

where m1  . . .   mT is a predictable sequence computable by the learner [Chiang et al.  2012  Rakhlin
and Sridharan  2013]. From the above discussions  we observe that there are different types of
regularities. As shown by Jadbabaie et al. [2015]  these regularities reﬂect distinct aspects of the online
problem  and are not comparable in general. To take advantage of the smaller regularity  Jadbabaie
et al. [2015] develop an adaptive method whose dynamic regret is on the order of
DT + 1 +
T }. However  it relies on the assumption that the learner

min{(cid:112)(DT + 1)P ∗

T   (DT + 1)1/3T 1/3F 1/3

√

can calculate each regularity online.

2.3 Adaptive Regret

Another way to deal with changing environments is to minimize the adaptive regret  which is deﬁned
as maximum static regret over any contiguous time interval [Hazan and Seshadhri  2007]. For convex
functions and exponentially concave functions  Hazan and Seshadhri [2007] have developed efﬁcient
T log3 T ) and O(d log2 T ) adaptive regrets  respectively. Later  the
algorithms that achieve O(
adaptive regret of convex functions is improved [Daniely et al.  2015  Jun et al.  2017]. The relation
between adaptive regret and restricted dynamic regret is investigated by Zhang et al. [2018b].

(cid:112)

3 Our Methods

We ﬁrst state assumptions about the online problem  then provide our motivations  including a lower
bound of the general dynamic regret  and ﬁnally present the proposed methods as well as their
theoretical guarantees. All the proofs can be found in the full paper [Zhang et al.  2018a].

3.1 Assumptions

Similar to previous studies in online learning  we introduce the following common assumptions.
Assumption 1 On domain X   the values of all functions belong to the range [a  a + c]  i.e. 

a ≤ ft(x) ≤ a + c  ∀x ∈ X   and t ∈ [T ].
Assumption 2 The gradients of all functions are bounded by G  i.e. 
x∈X (cid:107)∇ft(x)(cid:107)2 ≤ G  ∀t ∈ [T ].

max

(7)

Assumption 3 The domain X contains the origin 0  and its diameter is bounded by D  i.e. 

(8)
Note that Assumptions 2 and 3 imply Assumption 1 with any c ≥ GD. In the following  we assume
the values of G and D are known to the leaner.

x x(cid:48)∈X (cid:107)x − x(cid:48)(cid:107)2 ≤ D.

max

3.2 Motivations

According to Theorem 2 of Zinkevich [2003]  we have the following dynamic regret bound for online
gradient descent (OGD) with a constant step size.
Theorem 1 Consider the online gradient descent (OGD) with x1 ∈ X and

xt+1 = ΠX(cid:2)xt − η∇ft(xt)(cid:3)  ∀t ≥ 1

where ΠX [·] denotes the projection onto the nearest point in X . Under Assumptions 2 and 3  OGD
satisﬁes

T(cid:88)

ft(xt) − T(cid:88)

ft(ut) ≤ 7D2
4η
for any comparator sequence u1  . . .   uT ∈ X .

t=1

t=1

T(cid:88)

t=2

+

D
η

4

(cid:107)ut−1 − ut(cid:107)2 +

ηT G2

2

√

√
Thus  by choosing η = O(1/

universal. However  this upper bound is far from the Ω((cid:112)T (1 + PT )) lower bound indicated by the

T (1 + PT )) dynamic regret  that is

T )  OGD achieves an O(

theorem below.
Theorem 2 For any online algorithm and any τ ∈ [0  T D]  there exists a sequence of comparators
u1  . . .   uT satisfying Assumption 3 and a sequence of functions f1  . . .   fT satisfying Assumption 2 
such that

PT (u1  . . .   uT ) ≤ τ and R(u1  . . .   uT ) = Ω(cid:0)G(cid:112)T (D2 + Dτ )(cid:1).

a speciﬁc sequence ¯u1  . . .   ¯uT ∈ X whose path-length P T = (cid:80)T

Although there exist lower bounds for the restricted dynamic regret [Besbes et al.  2015  Yang et al. 
2016]  to the best of our knowledge  this is the ﬁrst lower bound for the general dynamic regret.
Let’s drop the universal property for the moment  and suppose we only want to compare against
t=2 (cid:107)¯ut − ¯ut−1(cid:107)2 is known
(1 + P T )/T )

beforehand. In this simple setting  we can tune the step size optimally as η∗ = O(
and obtain an improved O(
T (1 + P T )) dynamic regret bound  which matches the lower bound in
Theorem 2. Thus  when bounding the general dynamic regret  we face the following challenge: On
one hand  we want the regret bound to hold for any sequence of comparators  but on the other hand 
to get a tighter bound  we need to tune the step size for a speciﬁc path-length. In the next section  we
address this dilemma by running multiple OGD algorithms with different step sizes  and combining
them through a meta-algorithm.

(cid:113)

(cid:113)

3.3 The Basic Approach

Our proposed method  named as adaptive learning for dynamic environment (Ader)  is inspired by a
recent work for online learning with multiple types of functions—MetaGrad [van Erven and Koolen 
2016]. Ader maintains a set of experts  each attaining an optimal dynamic regret for a different
path-length  and chooses the best one using an expert-tracking algorithm.

Meta-algorithm Tracking the best expert is a well-studied problem [Herbster and Warmuth  1998] 
and our meta-algorithm  summarized in Algorithm 1  is built upon the exponentially weighted average
forecaster [Cesa-Bianchi and Lugosi  2006]. The inputs of the meta-algorithm are its own step size α 
and a set H of step sizes for experts. In Step 1  we active a set of experts {Eη|η ∈ H} by invoking
the expert-algorithm for each η ∈ H. In Step 2  we set the initial weight of each expert. Let ηi be the
i-th smallest step size in H. The weight of Eηi is chosen as

1
(9)
|H| .
In each round  the meta-algorithm receives a set of predictions {xη
t |η ∈ H} from all experts (Step 4) 
and outputs the weighted average (Step 5):

  and C = 1 +

i(i + 1)

1 =

wηi

C

(cid:88)

η∈H

xt =

wη

t xη

t

where wη
experts are updated according to the exponential weighting scheme (Step 7):

t is the weight assigned to expert Eη. After observing the loss function  the weights of

(cid:80)

wη

t+1 =

t )

t e−αft(xη
wη
µ∈H wµ
t ) to each expert Eη so that they can update their own

t e−αft(xµ

t )

.

In the last step  we send the gradient ∇ft(xη
predictions.

Expert-algorithm Experts are themselves algorithms  and our expert-algorithm  presented in
Algorithm 2  is the standard online gradient descent (OGD). Each expert is an instance of OGD  and
takes the step size η as its input. In Step 3 of Algorithm 2  each expert submits its prediction xη
t to
the meta-algorithm  and receives the gradient ∇ft(xη
t ) in Step 4. Then  in Step 5 it performs gradient
descent
t − η∇ft(xη

t+1 = ΠX(cid:2)xη

t )(cid:3)

xη

5

Algorithm 1 Ader: Meta-algorithm
Require: A step size α  and a set H containing step sizes for experts
1: Activate a set of experts {Eη|η ∈ H} by invoking Algorithm 2 for each step size η ∈ H
2: Sort step sizes in ascending order η1 ≤ η2 ≤ ··· ≤ ηN   and set wηi
3: for t = 1  . . .   T do
4:
5:

t from each expert Eη

Receive xη
Output

1 = C

i(i+1)

(cid:88)

η∈H

xt =

wη

t xη

t

6:
7:

Observe the loss function ft(·)
Update the weight of each expert by

wη

t+1 =

(cid:80)

t e−αft(xη
wη
µ∈H wµ

t e−αft(xµ

t )

t )

Send gradient ∇ft(xη

t ) to each expert Eη

8:
9: end for

1 be any point in X

Algorithm 2 Ader: Expert-algorithm
Require: The step size η
1: Let xη
2: for t = 1  . . .   T do
3:
4:
5:

Submit xη
Receive gradient ∇ft(xη

t to the meta-algorithm

t ) from the meta-algorithm

t+1 = ΠX(cid:2)xη

xη

t − η∇ft(xη

t )(cid:3)

6: end for

to get the prediction for the next round.
Next  we specify the parameter setting and our dynamic regret. The set H is constructed in the way
such that for any possible sequence of comparators  there exists a step size that is nearly optimal. To
control the size of H  we use a geometric series with ratio 2. The value of α is tuned such that the
upper bound is minimized. Speciﬁcally  we have the following theorem.
Theorem 3 Set

H =

ηi =

where N = (cid:100) 1
and 3  for any comparator sequence u1  . . .   uT ∈ X   our proposed Ader method satisﬁes

2 log2(1 + 4T /7)(cid:101) + 1  and α =(cid:112)8/(T c2) in Algorithm 1. Under Assumptions 1  2
ft(xt) − T(cid:88)

[1 + 2 ln(k + 1)]

T(cid:88)

√
c

(10)

G

(cid:40)

(cid:41)

2T

2i−1D

(cid:114) 7

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) i = 1  . . .   N
(cid:112)2T (7D2 + 4DPT ) +
(cid:16)(cid:112)T (1 + PT )
(cid:17)
(cid:19)(cid:23)
(cid:22) 1

(cid:18)

t=1

t=1

where

The order of the upper bound matches the Ω((cid:112)T (1 + PT )) lower bound in Theorem 2 exactly.

log2

+ 1.

k =

1 +

2

(11)

4PT
7D

ft(ut) ≤ 3G
4

=O

2T
4

3.4 An Improved Approach

The basic approach in Section 3.3 is simple  but it has an obvious limitation: From Steps 7 and 8
in Algorithm 1  we observe that the meta-algorithm needs to query the value and gradient of ft(·)

6

N times in each round  where N = O(log T ). In contrast  existing algorithms for minimizing static
regret  such as OGD  only query the gradient once per iteration. When the function is complex  the
evaluation of gradients or values could be expensive  and it is appealing to reduce the number of
queries in each round.

Surrogate Loss We introduce surrogate loss [van Erven and Koolen  2016] to replace the original
loss function. From the ﬁrst-order condition of convexity [Boyd and Vandenberghe  2004]  we have

ft(x) ≥ ft(xt) + (cid:104)∇ft(xt)  x − xt(cid:105)  ∀x ∈ X .

Then  we deﬁne the surrogate loss in the t-th iteration as

(cid:96)t(x) = (cid:104)∇ft(xt)  x − xt(cid:105)

and use it to update the prediction. Because

ft(xt) − ft(ut) ≤ (cid:96)t(xt) − (cid:96)t(ut) 

(12)

(13)

we conclude that the regret w.r.t. true losses ft’s is smaller than that w.r.t. surrogate losses (cid:96)t’s.
Thus  it is safe to replace ft with (cid:96)t. The new method  named as improved Ader  is summarized in
Algorithms 3 and 4.

Meta-algorithm The new meta-algorithm in Algorithm 3 differs from the old one in Algorithm 1
since Step 6. The new algorithm queries the gradient of ft(·) at xt  and then constructs the surrogate
loss (cid:96)t(·) in (12)  which is used in subsequent steps. In Step 8  the weights of experts are updated
based on (cid:96)t(·)  i.e. 

(cid:80)

t e−α(cid:96)t(xη
wη
µ∈H wµ

t e−α(cid:96)t(xµ

t )

.

t )

wη

t+1 =

In Step 9  the gradient of (cid:96)t(·) at xη

t is sent to each expert Eη. Because the surrogate loss is linear 
∇(cid:96)t(xη

t ) = ∇ft(xt)  ∀η ∈ H.

As a result  we only need to send the same ∇ft(xt) to all experts. From the above descriptions  it is
clear that the new algorithm only queries the gradient once in each iteration.

Expert-algorithm The new expert-algorithm in Algorithm 4 is almost the same as the previous
one in Algorithm 2. The only difference is that in Step 4  the expert receives the gradient ∇ft(xt) 
and uses it to perform gradient descent

t+1 = ΠX(cid:2)xη

t − η∇ft(xt)(cid:3)

xη

in Step 5.
We have the following theorem to bound the dynamic regret of the improved Ader.

Theorem 4 Use the construction of H in (10)  and set α =(cid:112)2/(T G2D2) in Algorithm 3. Under

Assumptions 2 and 3  for any comparator sequence u1  . . .   uT ∈ X   our improved Ader method
satisﬁes

T(cid:88)

ft(xt) − T(cid:88)

t=1

t=1

ft(ut) ≤ 3G
4

(cid:112)2T (7D2 + 4DPT ) +
(cid:16)(cid:112)T (1 + PT )
(cid:17)

=O

√

GD
2

2T

[1 + 2 ln(k + 1)]

where k is deﬁned in (11).

Similar to the basic approach  the improved Ader also achieves an O((cid:112)T (1 + PT )) dynamic regret 

that is universal and adaptive. The main advantage is that the improved Ader only needs to query the
gradient of the online function once in each iteration.

7

Algorithm 3 Improved Ader: Meta-algorithm
Require: A step size α  and a set H containing step sizes for experts
1: Activate a set of experts {Eη|η ∈ H} by invoking Algorithm 4 for each step size η ∈ H
2: Sort step sizes in ascending order η1 ≤ η2 ≤ ··· ≤ ηN   and set wηi
3: for t = 1  . . .   T do
4:
5:

t from each expert Eη

Receive xη
Output

1 = C

i(i+1)

(cid:88)

η∈H

xt =

wη

t xη

t

6:
7:
8:

Query the gradient of ft(·) at xt
Construct the surrogate loss (cid:96)t(·) in (12)
Update the weight of each expert by

(cid:80)

t e−α(cid:96)t(xη
wη
µ∈H wµ

t e−α(cid:96)t(xµ

t )

t )

wη

t+1 =

Send gradient ∇ft(xt) to each expert Eη

9:
10: end for

1 be any point in X

Algorithm 4 Improved Ader: Expert-algorithm
Require: The step size η
1: Let xη
2: for t = 1  . . .   T do
3:
4:
5:

Submit xη
Receive gradient ∇ft(xt) from the meta-algorithm

t to the meta-algorithm

t+1 = ΠX(cid:2)xη

t − η∇ft(xt)(cid:3)

xη

6: end for

3.5 Extensions

Following Hall and Willett [2013]  we consider the case that the learner is given a sequence of
dynamical models Φt(·) : X (cid:55)→ X   which can be used to characterize the comparators we are
interested in. Similar to Hall and Willett [2013]  we assume each Φt(·) is a contraction mapping.
Assumption 4 All the dynamical models are contraction mappings  i.e. 

(cid:107)Φt(x) − Φt(x(cid:48))(cid:107)2 ≤ (cid:107)x − x(cid:48)(cid:107)2 

(14)

for all t ∈ [T ]  and x  x(cid:48) ∈ X .
Then  we choose P (cid:48)
deviates from the given dynamics.

T in (6) as the regularity of a comparator sequence  which measures how much it

Algorithms For brevity  we only discuss how to incorporate the dynamical models into the basic
Ader in Section 3.3  and the extension to the improved version can be done in the same way. In fact 
we only need to modify the expert-algorithm  and the updated one is provided in Algorithm 5. To
utilize the dynamical model  after performing gradient descent  i.e. 

t+1 = ΠX(cid:2)xη

¯xη

t )(cid:3)

t − η∇ft(xη

in Step 5  we apply the dynamical model to the intermediate solution ¯xη

t+1  i.e. 

xη
t+1 = Φt(¯xη

t+1) 

and obtain the prediction for the next round. In the meta-algorithm (Algorithm 1)  we only need to
replace Algorithm 2 in Step 1 with Algorithm 5  and the rest is the same. The dynamic regret of the
new algorithm is given below.

8

1 be any point in X

Algorithm 5 Ader: Expert-algorithm with dynamical models
Require: The step size η  a sequence of dynamical models Φt(·)
1: Let xη
2: for t = 1  . . .   T do
3:
4:
5:

Submit xη
Receive gradient ∇ft(xη

t ) from the meta-algorithm

t to the meta-algorithm

t+1 = ΠX(cid:2)xη

¯xη

t − η∇ft(xη

t )(cid:3)

6:

7: end for

Theorem 5 Set

xη
t+1 = Φt(¯xη

t+1)

(cid:40)

H =

(cid:114) 1

T

(cid:41)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) i = 1  . . .   N

2i−1D

G

ηi =

Algorithm 1. Under Assumptions 1  2  3 and 4  for any comparator sequence u1  . . .   uT ∈ X   our
proposed Ader method satisﬁes

2 log2(1 + 2T )(cid:7) + 1  α =(cid:112)8/(T c2)  and use Algorithm 5 as the expert-algorithm in
where N =(cid:6) 1
ft(xt)− T(cid:88)
T(cid:88)
(cid:18)(cid:113)

ft(ut) ≤ 3G
2

T (D2 + 2DP (cid:48)

(cid:113)
(cid:19)

[1 + 2 ln(k + 1)]

√
c

2T
4

T ) +

(15)

t=1

t=1

=O

T (1 + P (cid:48)
T )

where

(cid:19)(cid:23)
Theorem 5 indicates our method achieves an O((cid:112)T (1 + P (cid:48)

(cid:22) 1

2P (cid:48)
T
D

(cid:18)

log2

k =

1 +

2

+ 1.

T (1 + P (cid:48)

√
T )) dynamic regret  improving the
T )) dynamic regret of Hall and Willett [2013] signiﬁcantly. Note that when Φt(·) is
O(
the identity map  we recover the result in Theorem 3. Thus  the upper bound in Theorem 5 is also
optimal.

4 Conclusion and Future Work

In this paper  we study the general form of dynamic regret  which compares the cumulative loss of the
online learner against an arbitrary sequence of comparators. To this end  we develop a novel method 
named as adaptive learning for dynamic environment (Ader). Theoretical analysis shows that Ader

achieves an optimal O((cid:112)T (1 + PT )) dynamic regret. When a sequence of dynamical models is
available  we extend Ader to incorporate this additional information  and obtain an O((cid:112)T (1 + P (cid:48)

dynamic regret.
In the future  we will investigate whether the curvature of functions  such as strong convexity and
smoothness  can be utilized to improve the dynamic regret bound. We note that in the setting of the
restricted dynamic regret  the curvature of functions indeed makes the upper bound tighter [Mokhtari
et al.  2016  Zhang et al.  2017]. But whether it improves the general dynamic regret remains an open
problem.

T ))

Acknowledgments

This work was partially supported by the National Key R&D Program of China (2018YFB1004300) 
NSFC (61603177)  JiangsuSF (BK20160658)  YESS (2017QNRC001)  and Microsoft Research
Asia.

9

References
J. Abernethy  P. L. Bartlett  A. Rakhlin  and A. Tewari. Optimal stragies and minimax lower bounds
for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory 
pages 415–423  2008.

O. Besbes  Y. Gur  and A. Zeevi. Non-stationary stochastic optimization. Operations Research  63

(5):1227–1244  2015.

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University Press 

2006.

C.-K. Chiang  T. Yang  C.-J. Lee  M. Mahdavi  C.-J. Lu  R. Jin  and S. Zhu. Online optimization with

gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory  2012.

A. Daniely  A. Gonen  and S. Shalev-Shwartz. Strongly adaptive online learning. In Proceedings of

the 32nd International Conference on Machine Learning  pages 1405–1411  2015.

J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12:2121–2159  2011.

E. C. Hall and R. M. Willett. Dynamical models and tracking regret in online convex programming.
In Proceedings of the 30th International Conference on Machine Learning  pages 579–587  2013.

E. Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization  2

(3-4):157–325  2016.

E. Hazan and C. Seshadhri. Adaptive algorithms for online decision problems. Electronic Colloquium

on Computational Complexity  88  2007.

E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimization.

Machine Learning  69(2-3):169–192  2007.

M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning  32(2):151–178  1998.

A. Jadbabaie  A. Rakhlin  S. Shahrampour  and K. Sridharan. Online optimization: Competing
with dynamic comparators. In Proceedings of the 18th International Conference on Artiﬁcial
Intelligence and Statistics  pages 398–406  2015.

K.-S. Jun  F. Orabona  S. Wright  and R. Willett. Improved strongly adaptive online learning using
coin betting. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and
Statistics  pages 943–951  2017.

A. Mokhtari  S. Shahrampour  A. Jadbabaie  and A. Ribeiro. Online optimization in dynamic
environments: Improved regret rates for strongly convex problems. In IEEE 55th Conference on
Decision and Control  pages 7195–7201  2016.

A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th

Conference on Learning Theory  pages 993–1019  2013.

S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in

Machine Learning  4(2):107–194  2011.

S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: primal estimated sub-gradient solver for SVM.
In Proceedings of the 24th International Conference on Machine Learning  pages 807–814  2007.

N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low-noise and fast rates. In Advances in Neural

Information Processing Systems 23  pages 2199–2207  2010.

T. van Erven and W. M. Koolen. Metagrad: Multiple learning rates in online learning. In Advances

in Neural Information Processing Systems 29  pages 3666–3674  2016.

10

T. Yang  L. Zhang  R. Jin  and J. Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of
online learning with true and noisy gradient. In Proceedings of the 33rd International Conference
on Machine Learning  pages 449–457  2016.

L. Zhang  J. Yi  R. Jin  M. Lin  and X. He. Online kernel learning with a near optimal sparsity bound.

In Proceedings of the 30th International Conference on Machine Learning  2013.

L. Zhang  T. Yang  J. Yi  R. Jin  and Z.-H. Zhou. Improved dynamic regret for non-degenerate

functions. In Advances in Neural Information Processing Systems 30  pages 732–741  2017.

L. Zhang  S. Lu  and Z.-H. Zhou. Adaptive online learning in dynamic environments. ArXiv e-prints 

arXiv:1810.10815  2018a.

L. Zhang  T. Yang  R. Jin  and Z.-H. Zhou. Dynamic regret of strongly adaptive methods.

Proceedings of the 35th International Conference on Machine Learning  2018b.

In

M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

Proceedings of the 20th International Conference on Machine Learning  pages 928–936  2003.

In

11

,Steve Esser
Rathinakumar Appuswamy
Paul Merolla
John Arthur
Dharmendra Modha
Qi Meng
Guolin Ke
Taifeng Wang
Wei Chen
Qiwei Ye
Zhi-Ming Ma
Tie-Yan Liu
Lijun Zhang
Shiyin Lu
Zhi-Hua Zhou
Max Simchowitz
Kevin Jamieson