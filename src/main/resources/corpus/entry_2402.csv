2015,HONOR: Hybrid Optimization for NOn-convex Regularized problems,Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However  due to the non-convexity and non-smoothness of the regularizer  how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper  we propose an efficient \underline{H}ybrid \underline{O}ptimization algorithm for \underline{NO}n convex \underline{R}egularized problems (HONOR). Specifically  we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence  while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2)  We establish a rigorous convergence analysis for HONOR  which shows that convergence is guaranteed even for non-convex problems  while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.,HONOR: Hybrid Optimization for NOn-convex

Regularized problems

Pinghua Gong

Jieping Ye

Univeristy of Michigan  Ann Arbor  MI 48109

Univeristy of Michigan  Ann Arbor  MI 48109

gongp@umich.edu

jpye@umich.edu

Abstract

Recent years have witnessed the superiority of non-convex sparse learning formu-
lations over their convex counterparts in both theory and practice. However  due
to the non-convexity and non-smoothness of the regularizer  how to efﬁciently
solve the non-convex optimization problem for large-scale data is still quite chal-
lenging. In this paper  we propose an efﬁcient Hybrid Optimization algorithm
for NOn-convex Regularized problems (HONOR). Speciﬁcally  we develop a hy-
brid scheme which effectively integrates a Quasi-Newton (QN) step and a Gra-
dient Descent (GD) step. Our contributions are as follows: (1) HONOR incor-
porates the second-order information to greatly speed up the convergence  while
it avoids solving a regularized quadratic programming and only involves matrix-
vector multiplications without explicitly forming the inverse Hessian matrix. (2)
We establish a rigorous convergence analysis for HONOR  which shows that con-
vergence is guaranteed even for non-convex problems  while it is typically chal-
lenging to analyze the convergence for non-convex problems. (3) We conduct
empirical studies on large-scale data sets and results demonstrate that HONOR
converges signiﬁcantly faster than state-of-the-art algorithms.

1 Introduction

Sparse learning with convex regularization has been successfully applied to a wide range of ap-
plications including marker genes identiﬁcation [19]  face recognition [22]  image restoration [2] 
text corpora understanding [9] and radar imaging [20]. However  it has been shown recently that
many convex sparse learning formulations are inferior to their non-convex counterparts in both the-
ory and practice [27  12  23  25  16  26  24  11]. Popular non-convex sparsity-inducing penalties
include Smoothly Clipped Absolute Deviation (SCAD) [10]  Log-Sum Penalty (LSP) [6] and Mini-
max Concave Penalty (MCP) [23]. Although non-convex sparse learning reveals its advantage over
the convex one  it remains a challenge to develop an efﬁcient algorithm to solve the non-convex
optimization problem especially for large-scale data.

DC programming [21] is a popular approach to solve non-convex problems whose objective func-
tions can be expressed as the difference of two convex functions. However  a potentially non-trivial
convex subproblem is required to solve at each iteration  which is not practical for large-scale prob-
lems. SparseNet [16] can solve a least squares problem with a non-convex penalty. At each step 
SparseNet solves a univariate subproblem with a non-convex penalty which admits a closed-form
solution. However  to establish the convergence analysis  the parameter of the non-convex penalty
is required to be restricted to some interval such that the univariate subproblem (with a non-convex
penalty) is convex. Moreover  it is quite challenging to extend SparseNet to non-convex problems
with a non-least-squares loss  as the univariate subproblem generally does not admit a closed-form
solution. The GIST algorithm [14] can solve a class of non-convex regularized problems by itera-
tively solving a possibly non-convex proximal operator problem  which in turn admits a closed-form
solution. However  GIST does not well exploit the second-order information. The DC-PN algorithm

1

[18] can incorporate the second-order information to solve non-convex regularized problems but it
requires to solve a non-trivial regularized quadratic subproblem at each iteration.

In this paper  we propose an efﬁcient Hybrid Optimization algorithm for NOn-convex Regularized
problems (HONOR)  which incorporates the second-order information to speed up the convergence.
HONOR adopts a hybrid optimization scheme which chooses either a Quasi-Newton (QN) step or
a Gradient Descent (GD) step per iteration mainly depending on whether an iterate has very small
components. If an iterate does not have any small component  the QN-step is adopted  which uses
L-BFGS to exploit the second-order information. The key advantage of the QN-step is that it does
not need to solve a regularized quadratic programming and only involves matrix-vector multipli-
cations without explicitly forming the inverse Hessian matrix. If an iterate has small components 
we switch to a GD-step. Our detailed theoretical analysis sheds light on the effect of such a hy-
brid scheme on the convergence of the algorithm. Speciﬁcally  we provide a rigorous convergence
analysis for HONOR  which shows that every limit point of the sequence generated by HONOR is
a Clarke critical point. It is worth noting that the convergence analysis for a non-convex problem
is typically much more challenging than the convex one  because many important properties for a
convex problem may not hold for non-convex problems. Empirical studies are also conducted on
large-scale data sets which include up to millions of samples and features; results demonstrate that
HONOR converges signiﬁcantly faster than state-of-the-art algorithms.

2 Non-convex Sparse Learning

We focus on the following non-convex regularized optimization problem:

min
x∈Rn

{f (x) = l(x) + r(x)}  

(1)

where we make the following assumptions throughout the paper:

(A1) l(x) is coercive  continuously differentiable and ∇l(x) is Lipschitz continuous with con-

stant L. Moreover  l(x) > −∞ for all x ∈ Rn.

(A2) r(x) = Pn

i=1 ρ(|xi|)  where ρ(t) is non-decreasing  continuously differentiable and con-
cave with respect to t in [0  ∞); ρ(0) = 0 and ρ′(0) 6= 0 with ρ′(t) = ∂ρ(t)/∂t denoting
the derivative of ρ(t) at the point t.

Remark 1 Assumption (A1) allows l(x) to be non-convex. Assumption (A2) implies that ρ(|xi|) is
generally non-convex with respect to xi and the only convex case is ρ(|xi|) = λ|xi| with λ > 0.
Moreover  ρ(|xi|) is continuously differentiable with respect to xi in (−∞  0) ∪ (0  ∞) and non-
6= 0  where
differentiable at xi = 0.
σ(xi) = 1  if xi > 0; σ(xi) = −1  if xi < 0 and σ(xi) = 0  otherwise. In addition  ρ′(0) > 0 must
hold (Otherwise ρ′(0) < 0 implies ρ(t) ≤ ρ(0) + ρ′(0)t < 0 for any t > 0  contradicting the fact
that ρ(t) is non-decreasing). It is also easy to show that  under the assumptions above  both l(x)
and r(x) are locally Lipschitz continuous. Thus  the Clarke subdifferential [7] is well-deﬁned.

In particular  ∂ρ(|xi|)/∂xi = σ(xi)ρ′(|xi|) for any xi

The commonly used least squares loss and the logistic regression loss satisfy the assumption (A1);
we can add a small term δkxk2 to make them coercive. The following popular non-convex regular-
izers satisfy the assumption (A2)  where λ > 0 and θ > 0 except that θ > 2 for SCAD.

• LSP: ρ(|xi|) = λ log(1 + |xi|/θ).

λ|xi| 
−x2

• SCAD: ρ(|xi|) =

• MCP: ρ(|xi|) =(cid:26) λ|xi| − x2

θλ2/2 

i /(2θ) 

i +2θλ|xi|−λ2

2(θ−1)

(θ + 1)λ2/2 

 

if |xi| ≤ λ 
if λ < |xi| ≤ θλ 
if |xi| > θλ.

if |xi| ≤ θλ 
if |xi| > θλ.

Due to the non-convexity and non-differentiability of problem (1)  the traditional subdifferential
concept for the convex optimization is not applicable here. Thus  we use the Clarke subdifferential
[7] to characterize the optimality of problem (1). We say ¯x is a Clarke critical point of problem (1) 
if 0 ∈ ∂of (¯x)  where ∂of (¯x) is the Clarke subdifferential of f (x) at x = ¯x. To be self-contained 

2

we brieﬂy review the Clarke subdifferential: for a locally Lipschitz continuous function f (x)  the
Clarke generalized directional derivative of f (x) at x = ¯x along the direction d is deﬁned as

f o(¯x; d) = lim sup
x→¯x α↓0

f (x + αd) − f (x)

α

.

Then  the Clarke subdifferential of f (x) at x = ¯x is deﬁned as

∂of (¯x) = {δ ∈ Rn : f o(¯x; d) ≥ dT δ  ∀d ∈ Rn}.

Interested readers may refer to Proposition 4 in the Supplement A for more properties about the
Clarke Subdifferential. We want to emphasize that some basic properties of the subdifferential of a
convex function may not hold for the Clarke Subdifferential of a non-convex function.

3 Proposed Optimization Algorithm: HONOR

Since each decomposable component function of the regularizer is only non-differentiable at the
origin  the objective function is differentiable  if the segment between any two consecutive iterates
do not cross any axis. This motivates us to design an algorithm which can keep the current iterate
in the same orthant of the previous iterate. Before we present the detailed HONOR algorithm  we
introduce two functions as follows:

Deﬁne a function π : Rn 7→ Rn with the i-th entry being:

πi(xi; yi) =(cid:26) xi 

0 

if σ(xi) = σ(yi) 
otherwise 

where y ∈ Rn (yi is the i-th entry of y) is the parameter of the function π; σ(·) is the sign function
deﬁned as follows: σ(xi) = 1  if xi > 0; σ(xi) = −1  if xi < 0 and σ(xi) = 0  otherwise.

Deﬁne the pseudo-gradient ⋄f (x) whose i-th entry is given by:

∇il(x) + ρ′(|xi|) 
∇il(x) − ρ′(|xi|) 
∇il(x) + ρ′(0) 
∇il(x) − ρ′(0) 
0 

if xi > 0 
if xi < 0 
if xi = 0  ∇il(x) + ρ′(0) < 0 
if xi = 0  ∇il(x) − ρ′(0) > 0 
otherwise 

⋄if (x) =


where ρ′(t) is the derivative of ρ(t) at the point t.

Remark 2 If r(x) is convex  ⋄f (x) is the minimum-norm sub-gradient of f (x) at x. Thus  − ⋄ f (x)
is a descent direction. However  ⋄f (x) is not even a sub-gradient of f (x) if r(x) is non-convex.
This indicates that some obvious concepts and properties for a convex problem may not hold in the
non-convex case. Thus  it is signiﬁcantly more challenging to develop and analyze algorithms for a
non-convex problem.

Interestingly  we can still show that vk = − ⋄ f (xk) is a descent direction at the point xk (refer
to Supplement D and replace pk = π(dk; vk) with vk). To utilize the second-order information 
we may perform the optimization along the direction dk = H kvk  where H k is a positive deﬁnite
matrix containing the second-order information. However  dk is not necessarily a descent direction.
To address this issue  we use the following slightly modiﬁed direction pk:

pk = π(dk; vk).

We can show that pk is a descent direction (proof is provided in Supplement D). Thus  we can
perform the optimization along the direction pk. Recall that we need to keep the current iterate in
the same orthant of the previous iterate. So the following iterative scheme is proposed:

where

xk(α) = π(xk + αpk; ξk) 

ξk

i =(cid:26) σ(xk

σ(vk

i ) 
i ) 

if xk
if xk

i 6= 0 
i = 0 

3

(2)

(3)

and α is a step size chosen by the following line search procedure: for constants α0 > 0  β  γ ∈
(0  1) and m = 0  1  · · ·   ﬁnd the smallest integer m with α = α0βm such that the following
inequality holds:

f (xk(α)) ≤ f (xk) − γα(vk)T dk.

(4)

However  only using the above iterative scheme may not guarantee the convergence. The main chal-
lenge is: if there exists a subsequence K such that {xk
i }K converges to zero  it is possible that for
sufﬁciently large k ∈ K  |xk
i | is arbitrarily small but never equal to zero (refer to the proof of The-
orem 1 for more details). To address this issue  we propose a hybrid optimization scheme. Speciﬁ-
cally  for a small constant ǫ > 0  if I k = {i ∈ {1  · · ·   n} : 0 < |xk
i < 0}
is not empty  we switch the iteration to the following gradient descent step (GD-step):

i | ≤ min(kvkk  ǫ)  xk

i vk

xk(α) = arg min

x

(cid:26)∇l(xk)T (x − xk) +

1
2α

kx − xkk2 + r(x)(cid:27)  

where α is a step size chosen by the following line search procedure: for constants α0 > 0  β  γ ∈
(0  1) and m = 0  1  · · ·   ﬁnd the smallest integer m with α = α0βm such that the following
inequality holds:

f (xk(α)) ≤ f (xk) −

γ
2α

kxk(α) − xkk2.

(5)

The detailed steps of the algorithm are presented in Algorithm 1.

Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1  3  4  17  13]. However 
HONOR is signiﬁcantly different from them: (1) The OWL-QN-type algorithms can only handle ℓ1-
regularized convex problems while HONOR is applicable to a class of non-convex problems beyond
ℓ1-regularized ones. (2) The convergence analyses of the OWL-QN-type algorithms heavily rely on
the convexity of the ℓ1-regularized problem. In contrast  the convergence analysis for HONOR is
applicable to non-convex cases beyond the convex ones  which is a non-trivial extension.

Algorithm 1: HONOR: Hybrid Optimization for NOn-convex Regularized problems

1 Initialize x0  H 0 and choose β  γ ∈ (0  1)  ǫ > 0  α0 > 0;
2 for k = 0 to maxiter do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Compute vk ← − ⋄ f (xk) and I k = {i ∈ {1  · · ·   n} : 0 < |xk
ǫk = min(kvkk  ǫ);
Initialize α ← α0;
if I k = ∅ then
(QN-step)
Compute dk ← H kvk with a positive deﬁnite matrix H k using L-BFGS;
Alignment: pk ← π(dk; vk);
while Eq. (4) is not satisﬁed do

i | ≤ ǫk  xk

i vk

i < 0}  where

α ← αβ; xk(α) ← π(xk + αpk; ξk);

end

else

(GD-step)
while Eq. (5) is not satisﬁed do

α ← αβ;

xk(α) ← arg minx(cid:8)∇l(xk)T (x − xk) + 1

2α kx − xkk2 + r(x)(cid:9);

end

end
xk+1 ← xk(α);
if some stopping criterion is satisﬁed then

stop and return xk+1;

end

22
23 end

4

4 Convergence Analysis

We ﬁrst present a few basic propositions and then provide the convergence theorem based on the
propositions; all proofs of the presented propositions are carefully handled due to the lack of con-
vexity. First of all  an optimality condition is presented (proof is provided in Supplement B)  which
will be directly used in the proof of Theorem 1.

Proposition 1 Let ¯x = limk∈K k→∞ xk  vk = − ⋄ f (xk) and ¯v = − ⋄ f (¯x)  where K is a
subsequence of {1  2  · · ·   k  k + 1  · · · }. If lim inf k∈K k→∞ |vk
i | = 0 for all i ∈ {1  · · ·   n}  then
¯v = 0 and ¯x is a Clarke critical point of problem (1).

We subsequently show that we have a Lipschitz-continuous-like inequality in the following propo-
sition (proof is provided in Supplement C)  which is crucial to prove the ﬁnal convergence theorem.

Proposition 2 Let vk = −⋄f (xk)  xk(α) = π(xk +αpk; ξk) and qk
with α > 0. Then under assumptions (A1) and (A2)  we have

α = 1

α (π(xk +αpk; ξk)− xk)

(i) ∇l(xk)T (xk(α) − xk) + r(xk(α)) − r(xk) ≤ −(vk)T (xk(α) − xk) 

(ii) f (xk(α)) ≤ f (xk) − α(vk)T qk

α +

α2L

2

kqk

αk2.

(6)

(7)

We next show that both line search criteria in the QN-step [Eq. (4)] and the GD-step [Eq. (5)] at any
iteration k is satisﬁed in a ﬁnite number of trials (proof is provided in Supplement D).

Proposition 3 At any iteration k of the HONOR algorithm  if xk is not a Clarke critical point of
problem (1)  then (a) for the QN-step  there exists an α ∈ [¯αk  α0] with 0 < ¯αk ≤ α0 such that the
line search criterion in Eq. (4) is satisﬁed; (b) for the GD-step  the line search criterion in Eq. (5) is
satisﬁed whenever α ≥ β min(α0  (1 − γ)/L). That is  both line search criteria at any iteration k
are satisﬁed in a ﬁnite number of trials.

We are now ready to provide the convergence proof for the HONOR algorithm:

Theorem 1 The sequence {xk} generated by the HONOR algorithm has at least a limit point and
every limit point of {xk} is a Clarke critical point of problem (1).

Proof It follows from Proposition 3 that both line search criteria in the QN-step [Eq. (4)] and the
GD-step [Eq. (5)] at each iteration can be satisﬁed in a ﬁnite number of trials. Let αk be the accepted
step size at iteration k. Then we have

f (xk) − f (xk+1) ≥ γαk(vk)T dk = γαk(vk)T H kvk (QN-step) 

or f (xk) − f (xk+1) ≥

γ
2αk kxk+1 − xkk2 ≥

γ
2α0

kxk+1 − xkk2 (GD-step).

(8)

(9)

Recall that H k is positive deﬁnite and γ > 0  αk > 0  which together with Eqs.(8)  (9) imply
that {f (xk)} is monotonically decreasing. Thus  {f (xk)} converges to a ﬁnite value ¯f   since f is
bounded from below (note that l(x) > −∞ and r(x) ≥ 0 for all x ∈ Rn). Due to the boundedness
of {xk} (see Proposition 7 in Supplement F)  the sequence {xk} generated by the HONOR algorithm
has at least a limit point ¯x. Since f is continuous  there exists a subsequence K of {1  2 · · ·   k  k +
1  · · · } such that

lim

k∈K k→∞

xk = ¯x 

lim
k→∞

f (xk) = lim

k∈K k→∞

f (xk) = ¯f = f (¯x).

(10)

(11)

In the following  we prove the theorem by contradiction. Assume that ¯x is not a Clarke critical point
of problem (1). Then by Proposition 1  there exists at least one i ∈ {1  · · ·   n} such that

lim inf

k∈K k→∞

|vk

i | > 0.

5

(12)

We next consider the following two cases:
(a) There exist a subsequence ˜K of K and an integer ˜k > 0 such that for all k ∈ ˜K  k ≥ ˜k  the
GD-step is adopted. Then for all k ∈ ˜K  k ≥ ˜k  we have

xk+1 = arg min

x

(cid:26)∇l(xk)T (x − xk) +

1

2αk kx − xkk2 + r(x)(cid:27) .

Thus  by the optimality condition of the above problem and properties of the Clarke subdifferential
(Proposition 4 in Supplement A)  we have

0 ∈ ∇l(xk) +

1
αk (xk+1 − xk) + ∂or(xk+1).

Taking limits with k ∈ ˜K for Eq. (9) and considering Eqs. (10)  (11)  we have

lim

k∈ ˜K k→∞

kxk+1 − xkk2 ≤ 0 ⇒ lim

k∈ ˜K k→∞

xk = lim

k∈ ˜K k→∞

xk+1 = ¯x.

(13)

(14)

Taking limits with k ∈ ˜K for Eq. (13) and considering Eq. (14)  αk ≥ β min(α0  (1 − γ)/L)
[Proposition 3] and ∂or(·) is upper-semicontinuous (upper-hemicontinuous) [8] (see Proposition 4
in the Supplement A)  we have

0 ∈ ∇l(¯x) + ∂or(¯x) = ∂of (¯x) 

which contradicts the assumption that ¯x is not a Clarke critical point of problem (1).
(b) There exists an integer ˆk > 0 such that for all k ∈ K  k ≥ ˆk  the QN-step is adopted. According
to Remark 7 (in Supplement F)  we know that the smallest eigenvalue of H k is uniformly bounded
from below by a positive constant  which together with Eq. (12) implies

lim inf

k∈K k→∞

(vk)T H kvk > 0.

Taking limits with k ∈ K for Eq. (8)  we have

lim

k∈K k→∞

γαk(vk)T H kvk ≤ 0 

which together with γ ∈ (0  1)  αk ∈ (0  α0] and Eq. (15) implies that

lim

k∈K k→∞

αk = 0.

(15)

(16)

Eq. (12) implies that there exist an integer ˇk > 0 and a constant ¯ǫ > 0 such that ǫk =
min(kvkk  ǫ) ≥ ¯ǫ for all k ∈ K  k ≥ ˇk. Notice that for all k ∈ K  k ≥ ˆk  the QN-step is
adopted. Thus  we obtain that I k = {i ∈ {1  · · ·   n} : 0 < |xk
i < 0} = ∅ for all
k ∈ K  k ≥ ˆk. We also notice that  if |xk
i | ≥ ¯ǫ  then there exists a constant ¯αi > 0 such that
xk
i (α) = πi(xk
i } is bounded (Proposition 8
in Supplement F). Therefore  we conclude that  for all k ∈ K  k ≥ ¯k = max(ˇk  ˆk) and for all
i ∈ {1  · · ·   n}  at least one of the following three cases must happen:

i for all α ∈ (0  ¯αi]  as {pk

i | ≤ ǫk  xk

i ) = xk

i + αpk

i + αpk

i ; ξk

i vk

i (α) = πi(xk

i = 0 ⇒ xk
xk
i | > ǫk ≥ ¯ǫ ⇒ xk
or |xk
i vk
or xk

i ≥ 0 ⇒ xk

i pk

i + αpk
i (α) = πi(xk

i ; ξk
i + αpk

i ) = xk
i ; ξk
i (α) = πi(xk

i + αpk
i ) = xk
i + αpk

i   ∀α > 0 
i + αpk
i ; ξk

i ) = xk

i ≥ 0 ⇒ xk

i   ∀α ∈ (0  ¯αi] 

i + αpk

i   ∀α > 0.

It follows that there exists a constant ¯α > 0 such that

qk

α =

1
α

(xk(α) − xk) = pk  ∀k ∈ K  k ≥ ¯k  α ∈ (0  ¯α].

Thus  considering |pk

i | = |πi(dk

i ; vk

i )| ≤ |dk

i | and vk

i pk

i ≥ vk

i dk

i for all i ∈ {1  · · ·   n}  we have

kqk
(vk)T qk

αk2 = kpkk2 ≤ kdkk2 = (vk)T (H k)2vk  ∀k ∈ K  k ≥ ¯k  α ∈ (0  ¯α] 

α = (vk)T pk ≥ (vk)T dk = (vk)T H kvk  ∀k ∈ K  k ≥ ¯k  α ∈ (0  ¯α].

(17)

(18)

(19)

6

According to Proposition 8 (in Supplement F)  we know that the largest eigenvalue of H k is uni-
formly bounded from above by some positive constant M . Thus  we have

(vk)T (H k)2vk ≤

2
αL

(vk)T H kvk −(cid:18) 2

αL

− M(cid:19) (vk)T H kvk  ∀k 

which together with Eqs. (18)  (19) and dk = H kvk implies

kqk

αk2 ≤

2
αL

(vk)T qk

α −(cid:18) 2

αL

− M(cid:19) (vk)T dk  ∀k ∈ K  k ≥ ¯k  α ∈ (0  ¯α].

(20)

Considering Eqs. (7)  (20)  we have

f (xk(α)) ≤ f (xk) − α(cid:18)1 −

αLM

2 (cid:19) (vk)T dk  ∀k ∈ K  k ≥ ¯k  α ∈ (0  ¯α] 

which together with (vk)T dk = (vk)T H kvk ≥ 0 implies that the line search criterion in the
QN-step [Eq. (4)] is satisﬁed if

1 −

αLM

2

≥ γ   0 < α ≤ α0 and 0 < α ≤ ¯α  ∀k ∈ K  k ≥ ¯k.

Considering the backtracking form of the line search in QN-step [Eq. (4)]  we conclude that the line
search criterion in the QN-step [Eq. (4)] is satisﬁed whenever

αk ≥ β min(min(¯α  α0)  2(1 − γ)/(LM )) > 0  ∀k ∈ K  k ≥ ¯k.

This leads to a contradiction with Eq. (16).

By (a) and (b)  we conclude that ¯x = limk∈K k→∞ xk is a Clarke critical point of problem (1). (cid:3)

5 Experiments

gistic regression problem1 by setting l(x) = 1/NPN

In this section  we evaluate the efﬁciency of HONOR on solving the non-convex regularized lo-
i x))  where ai ∈
Rn is the i-th sample associated with the label yi ∈ {1  −1}. Three non-convex regulariz-
ers (LSP  MCP and SCAD) are included in experiments  where the parameters are set as λ =
1/N and θ = 10−2λ (θ is set as 2 + 10−2λ for SCAD as it requires θ > 2). We com-
pare HONOR with the non-convex solver2 GIST [14] on three large-scale  high-dimensional
and sparse data sets which are summarized in Table 1. All data sets can be downloaded from
http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/.

i=1 log(1 + exp(−yiaT

datasets

♯ samples N

url

Table 1: Data set statistics.

All algorithms are implemented in Mat-
lab 2015a under a Linux operating sys-
tem and executed on an Intel Core
i7-4790 CPU (@3.6GHz) with 32GB
memory. We choose the starting points
x0 for the compared algorithms using the same random vector whose entries are i.i.d. sampled from
the standard Gaussian distribution. We terminate the compared algorithms if the relative change of
two consecutive objective function values is less than 10−5 or the number of iterations exceeds 1000
(HONOR) or 10000 (GIST). For HONOR  we set γ = 10−5  β = 0.5  α0 = 1 and the number of
unrolling steps in L-BFGS as m = 10. For GIST  we use the non-monotone line search in exper-
iments as it usually performs better than its monotone counterpart. To show how the convergence
behavior of HONOR varies over the parameter ǫ  we use three values: ǫ = 10−10  10−6  10−2.

kdd2010b
748 401

kdd2010a
510 302

2 396 130
3 231 961

dimensionality n

20 216 830

29 890 095

We report the objective function value (in log-scale) vs. CPU time (in seconds) plots in Figure 1.
We can observe from Figure 1 that: (1) If ǫ is set to a small value  the QN-step is adopted at almost
all steps in HONOR and HONOR converges signiﬁcantly faster than GIST for all three non-convex

1We do not include the term δkxk2 in the objective and ﬁnd that the proposed algorithm still works well.
2We do not involve SparseNet  DC programming and DC-PN in comparison  because (1) adapting
SparseNet to the logistic regression problem is challenging; (2) DC programming is shown to be much in-
ferior to GIST; (3) The objective function value of DC-PN is larger than GIST in most cases [18].

7

regularizers on all three data sets. This shows that using the second-order information greatly speeds
up the convergence. (2) When ǫ increases  the ratio of the GD-step adopted in HONOR increases.
Meanwhile  the convergence performance of HONOR generally degrades. In some cases  setting
a slightly larger ǫ and adopting a small number of GD steps even sligtly boosts the convergence
performance of HONOR (the green curves in the ﬁrst row). But setting ǫ to a very small value is
always safe to guarantee the fast convergence of HONOR. (3) When ǫ is large enough  the GD steps
dominate all iterations of HONOR and HONOR converge much slower. In this case  HONOR con-
verges even slower than GIST. The reason is that  at each iteration of HONOR  extra computational
cost is required in addition to the basic computation in the GD-step. Moreover  the non-monotone
line search is used in GIST while the monotone line search is adopted in the GD-step. (4) In some
cases (the ﬁrst row)  GIST is trapped in a local solution which has a much larger objective function
value than HONOR with a small ǫ. This implies that HONOR may have a potential of escaping
from high error plateau which often exists in high dimensional non-convex problems. These results
show the great potential of HONOR for solving large-scale non-convex sparse learning problems.

LSP (kdd2010a)

LSP (kdd2010b)

LSP (url)

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

101

100

100

10-1

10-2

100

10-1

10-2

0

200

600

400
800
CPU time (seconds)
MCP (kdd2010a)

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

101

100

1000

1200

0

200

400

600

800

1000

1200

CPU time (seconds)
MCP (kdd2010b)

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

100

10-1

10-2

10-3

0

500

1500

1000
2000
CPU time (seconds)
SCAD (kdd2010a)

2500

3000

0

500

1000

2000

2500
1500
CPU time (seconds)
SCAD (kdd2010b)

3000

3500

4000

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

100

10-1

10-2

10-3

0

500

1000
CPU time (seconds)

1500

2000

2500

0

1000

2000

3000
CPU time (seconds)

4000

101

100

10-1

100

10-1

10-2

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

l

)
e
a
c
s
 

d
e
g
g
o
l
(
 

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 

e
v
i
t
c
e
b
O

j

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

0

2000

4000

6000

8000

10000 12000 14000

CPU time (seconds)

MCP (url)

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

2.5

3
×104

HONOR(ǫ=1e-10)
HONOR(ǫ=1e-6)
HONOR(ǫ=1e-2)
GIST

1.5

1
CPU time (seconds)

2

SCAD (url)

10-3

0

0.5

100

10-1

10-2

10-3

0

0.5

1
2
CPU time (seconds)

1.5

2.5

×104

Figure 1: Objective function value (in log-scale) vs. CPU time (in seconds) plots for differ-
ent non-convex regularizers and different large-scale and high-dimensional data sets. The ra-
tios of the GD-step adopted in HONOR are: LSP (kdd2010a): 0%  1%  34%; LSP (kdd2010b):
0%  2%  27%; LSP (url): 0.1%  2%  35%; MCP (kdd2010a): 0%  88%  100%; MCP (kdd2010b):
0%  89%  100%; MCP (url): 0%  97%  100%; SCAD (kdd2010a): 0%  43%  100%; SCAD (2010b):
0%  32%  99.5%; SCAD (url): 0%  79%  100%.

6 Conclusions

In this paper  we propose an efﬁcient optimization algorithm called HONOR for solving non-convex
regularized sparse learning problems. HONOR incorporates the second-order information to speed
up the convergence in practice and uses a carefully designed hybrid optimization scheme to guaran-
tee the convergence in theory. Experiments are conducted on large-scale data sets and results show
that HONOR converges signiﬁcantly faster than state-of-the-art algorithms. In our future work  we
plan to develop parallel/distributed variants of HONOR to tackle much larger data sets.

Acknowledgements

This work is supported in part by research grants from NIH (R01 LM010730  U54 EB020403) and
NSF (IIS- 0953662  III-1539991  III-1539722).

8

References

[1] G. Andrew and J. Gao. Scalable training of ℓ1-regularized log-linear models. In ICML  pages 33–40 

2007.

[2] J. Bioucas-Dias and M. Figueiredo. A new TwIST: two-step iterative shrinkage/thresholding algorithms

for image restoration. IEEE Transactions on Image Processing  16(12):2992–3004  2007.

[3] R. H. Byrd  G. M. Chin  J. Nocedal  and F. Oztoprak. A family of second-order methods for convex
ℓ1-regularized optimization. Technical report  Industrial Engineering and Management Sciences  North-
western University  Evanston  IL  2012.

[4] R. H. Byrd  G. M. Chin  J. Nocedal  and Y. Wu. Sample size selection in optimization methods for

machine learning. Mathematical Programming  134(1):127–155  2012.

[5] R. H. Byrd  P. Lu  J. Nocedal  and C. Zhu. A limited memory algorithm for bound constrained optimiza-

tion. SIAM Journal on Scientiﬁc Computing  16(5):1190–1208  1995.

[6] E. Candes  M. Wakin  and S. Boyd. Enhancing sparsity by reweighted ℓ1 minimization. Journal of Fourier

Analysis and Applications  14(5):877–905  2008.

[7] F. Clarke. Optimization and Nonsmooth Analysis. John Wiley&Sons  New York  1983.

[8] J. Dutta. Generalized derivatives and nonsmooth optimization  a ﬁnite dimensional tour. Top  13(2):185–

279  2005.

[9] L. El Ghaoui  G. Li  V. Duong  V. Pham  A. Srivastava  and K. Bhaduri. Sparse machine learning methods

for understanding large text corpora. In CIDU  pages 159–173  2011.

[10] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal

of the American Statistical Association  96(456):1348–1360  2001.

[11] J. Fan  L. Xue  and H. Zou. Strong oracle optimality of folded concave penalized estimation. Annals of

Statistics  42(3):819  2014.

[12] G. Gasso  A. Rakotomamonjy  and S. Canu. Recovering sparse signals with a certain family of nonconvex

penalties and dc programming. IEEE Transactions on Signal Processing  57(12):4686–4698  2009.

[13] P. Gong and J. Ye. A modiﬁed orthant-wise limited memory quasi-newton method with convergence

analysis. In ICML  2015.

[14] P. Gong  C. Zhang  Z. Lu  J. Huang  and J. Ye. A general iterative shrinkage and thresholding algorithm

for non-convex regularized optimization problems. In ICML  volume 28  pages 37–45  2013.

[15] N. Jorge and J. Stephen. Numerical Optimization. Springer  1999.

[16] R. Mazumder  J. Friedman  and T. Hastie. Sparsenet: Coordinate descent with nonconvex penalties.

Journal of the American Statistical Association  106(495)  2011.

[17] P. Olsen  F. Oztoprak  J. Nocedal  and S. Rennie. Newton-like methods for sparse inverse covariance

estimation. In Advances in Neural Information Processing Systems (NIPS)  pages 764–772  2012.

[18] A. Rakotomamonjy  R. Flamary  and G. Gasso. Dc proximal newton for non-convex optimization prob-

lems. 2014.

[19] S. Shevade and S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic

regression. Bioinformatics  19(17):2246  2003.

[20] X. Tan  W. Roberts  J. Li  and P. Stoica. Sparse learning via iterative minimization with application to

mimo radar imaging. IEEE Transactions on Signal Processing  59(3):1088–1101  2011.

[21] P. Tao and L. An. The dc (difference of convex functions) programming and dca revisited with dc models
of real world nonconvex optimization problems. Annals of Operations Research  133(1-4):23–46  2005.

[22] J. Wright  A. Yang  A. Ganesh  S. Sastry  and Y. Ma. Robust face recognition via sparse representation.

IEEE Transactions on Pattern Analysis and Machine Intelligence  31(2):210–227  2008.

[23] C. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 

38(2):894–942  2010.

[24] C. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse estimation

problems. Statistical Science  27(4):576–593  2012.

[25] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. JMLR  11:1081–1107 

2010.

[26] T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli  2012.

[27] H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of

Statistics  36(4):1509  2008.

9

,Pinghua Gong
Jieping Ye
Kenji Kawaguchi
Sekitoshi Kanai
Yasuhiro Fujiwara
Sotetsu Iwamura