2013,Marginals-to-Models Reducibility,We consider a number of classical and new computational problems regarding marginal distributions  and inference in models specifying a full joint distribution. We prove general and efficient reductions between a number of these problems  which demonstrate that algorithmic progress in inference automatically yields progress for “pure data” problems. Our main technique involves formulating the problems as linear programs  and proving that the dual separation oracle for the Ellipsoid Method is provided by the target problem. This technique may be of independent interest in probabilistic inference.,Marginals-to-Models Reducibility

Tim Roughgarden
Stanford University

tim@cs.stanford.edu

Michael Kearns

University of Pennsylvania

mkearns@cis.upenn.edu

Abstract

We consider a number of classical and new computational problems regarding
marginal distributions  and inference in models specifying a full joint distribution.
We prove general and efﬁcient reductions between a number of these problems 
which demonstrate that algorithmic progress in inference automatically yields
progress for “pure data” problems. Our main technique involves formulating the
problems as linear programs  and proving that the dual separation oracle required
by the ellipsoid method is provided by the target problem. This technique may be
of independent interest in probabilistic inference.

1

Introduction

The movement between the speciﬁcation of “local” marginals and models for complete joint distri-
butions is ingrained in the language and methods of modern probabilistic inference. For instance 
in Bayesian networks  we begin with a (perhaps partial) speciﬁcation of local marginals or CPTs 
which then allows us to construct a graphical model for the full joint distribution. In turn  this allows
us to make inferences (perhaps conditioned on observed evidence) regarding marginals that were not
part of the original speciﬁcation.
In many applications  the speciﬁcation of marginals is derived from some combination of (noisy)
observed data and (imperfect) domain expertise. As such  even before the passage to models for the
full joint distribution  there are a number of basic computational questions we might wish to ask of
given marginals  such as whether they are consistent with any joint distribution  and if not  what the
nearest consistent marginals are. These can be viewed as questions about the “data”  as opposed to
inferences made in models derived from the data.
In this paper  we prove a number of general  polynomial time reductions between such problems
regarding data or marginals  and problems of inference in graphical models. By “general” we mean
the reductions are not restricted to particular classes of graphs or algorithmic approaches  but show
that any computational progress on the target problem immediately transfers to progress on the
source problem. For example  one of our main results establishes that the problem of determining
whether given marginals  whose induced graph (the “data graph”) falls within some class G  are
consistent with any joint distribution reduces to the problem of MAP inference in Markov networks
falling in the same class G. Thus  for instance  we immediately obtain that the tractability of MAP
inference in trees or tree-like graphs yields an efﬁcient algorithm for marginal consistency in tree
data graphs; and any future progress in MAP inference for other classes G will similarly transfer.
Conversely  our reductions also can be used to establish negative results. For instance  for any class
G for which we can prove the intractability of marginal consistency  we can immediately infer the
intractability of MAP inference as well.
There are a number of reasons to be interested in such problems regarding marginals. One  as
we have already suggested  is the fact that given marginals may not be consistent with any joint

1

Figure 1: Summary of main results. Arrows indicate that the source problem can be reduced to the target
problem for any class of graphs G  and in polynomial time. Our main results are the left-to-right arrows from
marginals-based problems to Markov net inference problems.

distribution  due to noisy observations or faulty domain intuitions 1 and we may wish to know this
before simply passing to a joint model that forces or assumes consistency. At the other extreme 
given marginals may be consistent with many joint distributions  with potentially very different
properties.2 Rather than simply selecting one of these consistent distributions in which to perform
inference (as would typically happen in the construction of a Markov or Bayes net)  we may wish to
reason over the entire class of consistent distributions  or optimize over it (for instance  choosing to
maximize or minimize independence).
We thus consider four natural algorithmic problems involving (partially) speciﬁed marginals:

marginals?

• CONSISTENCY: Is there any joint distribution consistent with given marginals?
• CLOSEST CONSISTENCY: What are the consistent marginals closest to given inconsistent
• SMALL SUPPORT: Of the consistent distributions with the closest marginals  can we com-
pute one with support size polynomial in the data (i.e.  number of given marginal values)?
• MAX ENTROPY: What is the maximum entropy distribution closest to given marginals?

The consistency problem has been studied before as the membership problem for the marginal poly-
tope (see Related Work); in the case of inconsistency  the closest consistency problem seeks the
minimal perturbation to the data necessary to recover coherence.
When there are many consistent distributions  which one should be singled out? While the maxi-
mum entropy distribution is a staple of probabilistic inference  it is not the only interesting answer.
For example  consider the three features “votes Republican”  “supports universal healthcare”  and
“supports tougher gun control”  and suppose the single marginals are 0.5  0.5  0.5. The maximum
entropy distribution is uniform over the 8 possibilities. We might expect reality to hew closer to a
small support distribution  perhaps even 50/50 over the two vectors 100 and 011. The small support
problem can be informally viewed as attempting to minimize independence or randomization  and
thus is a natural contrast to maximum entropy. It is also worth noting that small support distributions
arise naturally through the joint behavior of no-regret algorithms in game-theoretic settings [1].
We also consider two standard algorithmic inference problems on full joint distributions (models):

1For a simple example  consider three random variables for which each pairwise marginal speciﬁes that the
settings (0 1) and (1 0) each occurs with probability 1/2. The corresponding “data graph” is a triangle. This
requires that each variable always disagrees with the other two  which is impossible.

2 For example  consider random variables X  Y  Z. Suppose the pairwise marginals for X and Y and for Y
and Z specify that all four binary settings are equally likely. No pairwise marginals for X and Z are given  so
the data graph is a two-hop path. One consistent distribution ﬂips a fair coin independently for each variable;
but another ﬂips one coin for X  a second for Y   and sets Z = X. The former maximizes entropy while the
latter minimizes support size.

2

• MAP INFERENCE: What is the MAP joint assignment in a given Markov network?
• GENERALIZED PARTITION: What is the normalizing constant of a given Markov network 

possibly after conditioning on the value of one vertex or edge?

All six of these problems are parameterized by a class of graphs G — for the four marginals prob-
lems  this is the graph induced by the given pairwise marginals  while for the models problems  it is
the graph of the given Markov network. All of our reductions are of the form “for every class G  if
there is a polynomial-time algorithm for solving inference problem B for (model) graphs in G  then
there is a polynomial-time algorithm for marginals problem A for (marginal) graphs in G” — that
is  A reduces to B. Our main results  which are summarized in Figure 1  can be stated informally as
follows:

• CONSISTENCY reduces to MAP INFERENCE.
• CLOSEST CONSISTENCY reduces to MAP INFERENCE.
• SMALL SUPPORT reduces to MAP INFERENCE.
• MAX ENTROPY reduces to GENERALIZED PARTITION.3

While connections between some of these problems are known for speciﬁc classes of graphs —
most notably in trees  where all of these problems are tractable and rely on common underlying al-
gorithmic approaches such as dynamic programming — the novelty of our results is their generality 
showing that the above reductions hold for every class of graphs.
All of our reductions share a common and powerful technique:
the use of the ellipsoid method
for Linear Programming (LP)  with the key step being the articulation of an appropriate separation
oracle. The ﬁrst three problems we consider have a straightforward LP formulation which will
typically have a number of variables that is equal to the number of joint settings  and therefore
exponential in the number of variables; for the MAX ENTROPY problem  there is an analogous
convex program formulation. Since our goal is to run in time polynomial in the input length (the
number and size of given marginals)  the straightforward LP formulation will not sufﬁce. However 
by passing to the dual LP  we instead obtain an LP with only a polynomial number of variables  but
an exponential number of constraints that can be represented implicitly. For each of the reductions
above  we show that the required separation oracle for these implicit constraints is provided exactly
by the corresponding inference problem (MAP INFERENCE or GENERALIZED PARTITION). We
believe this technique may be of independent interest and have other applications in probabilistic
inference.
It is perhaps surprising that in the study of problems strictly addressing properties of given marginals
(which have received relatively little attention in the graphical models literature historically)  prob-
lems of inference in full joint models (which have received great attention) should arise so naturally
and generally. For the marginal problems  our reductions (via the ellipsoid method) effectively
create a series of “ﬁctitious” Markov networks such that the solutions to corresponding inference
problems (MAP INFERENCE and GENERALIZED PARTITION) indirectly lead to a solution to the
original marginal problems.
Related Work: The literature on graphical models and probabilistic inference is rife with connec-
tions between some of the problems we study here for speciﬁc classes of graphical models (such as
trees or otherwise sparse structures)  and under speciﬁc algorithmic approaches (such as dynamic
programming or message-passing algorithms more generally  and various forms of variational infer-
ence); see [2  3  4] for good overviews. In contrast  here we develop general and efﬁcient reductions
between marginal and inference problems that hold regardless of the graph structure or algorithmic
approach; we are not aware of prior efforts in this vein. Some of the problems we consider are also
either new or have been studied very little  such as CLOSEST CONSISTENCY and SMALL SUPPORT.
The CONSISTENCY problem has been studied before as the membership problem for the marginal
polytope. In particular  [8] shows that ﬁnding the MAP assignment for Markov random ﬁelds with
pairwise potentials can be cast as an integer linear program over the marginal polytope — that is 
algorithms for the CONSISTENCY problem are useful subroutines for inference. Our work is the

3The conceptual ideas in this reduction are well known. We include a formal treatment in the Appendix for

completeness and to provide an analogy with our other reductions  which are our more novel contributions.

3

ﬁrst to show a converse  that inference algorithms are useful subroutines for decision and optimiza-
tion problems for the marginal polytope. Furthermore  previous polynomial-time solutions to the
CONSISTENCY problem generally give a compact (polynomial-size) description of the marginal
polytope. Our approach dodges this ambitious requirement  in that it only needs a polynomial-time
separation oracle (which  for this problem  turns out to be MAP inference). As there are many
combinatorial optimization problems with no compact LP formulation that admit polynomial-time
ellipsoid-based algorithms — like non-bipartite matching  with its exponentially many odd cycle
inequalities — our approach provides a new way of identifying computationally tractable special
cases of problems concerning marginals.
The previous work that is perhaps most closely related in spirit to our interests are [5] and [6  7].
These works provide reductions of some form  but not ones that are both general (independent of
graph structure) and polynomial time. However  they do suggest both the possibility and interest in
such stronger reductions. The paper [5] discusses and provides heuristic reductions between MAP
INFERENCE and GENERALIZED PARTITION.
The work in [6  7] makes the point that maximizing entropy subject to an (approximate) consistency
condition yields a distribution that can be represented as a Markov network over the graph induced
by the original data or marginals. As far as we are aware  however  there has been essentially no for-
mal complexity analysis (i.e.  worst-case polynomial-time guarantees) for algorithms that compute
max-entropy distributions.4

2 Preliminaries

2.1 Problem Deﬁnitions

For clarity of exposition  we focus on the pairwise case in which every marginal involves at most
two variables.5 Denote the underlying random variables by X1  . . .   Xn  which we assume have
range [k] = {0  1  2  . . .   k}. The input is at most one real-valued single marginal value µis for
every variable i ∈ [n] and value s ∈ [k]  and at most one real-valued pairwise marginal value µijst
for every ordered variable pair i  j ∈ [n]×[n] with i < j and every pair s  t ∈ [k]. Note that we allow
a marginal to be only partially speciﬁed. The data graph induced by a set of marginals has one vertex
per random variable Xi  and an undirected edge (i  j) if and only if at least one of the given pairwise
marginal values involves the variables Xi and Xj. Let M1 and M2 denote the sets of indices (i  s)
and (i  j  s  t) of the given single and pairwise marginal values  and m = |M1| + |M2| the total
number of marginal values. Let A = [k]n denote the space of all possible variable assignments.
We say that the given marginals µ are consistent if there exists a (joint) probability distribution
consistent with all of them (i.e.  that induces the marginals µ).
With these basic deﬁnitions  we can now give formal deﬁnitions for the marginals problems we
consider. Let G denote an arbitrary class of undirected graphs.

• CONSISTENCY (G): Given marginals µ such that the induced data graph falls in G  are they

consistent?

• CLOSEST CONSISTENCY (G): Given (possibly inconsistent) marginals µ such that the
induced data graph falls in G  compute the consistent marginals ν minimizing ||ν − µ||1.
• SMALL SUPPORT (G): Given (consistent or inconsistent) marginals µ such that the in-
duced data graph falls in G  compute a distribution that has a polynomial-size support and
marginals ν that minimize ||ν − µ||1.
• MAX ENTROPY (G): Given (consistent or inconsistent) marginals µ such that the induced
data graph falls in G  compute the maximum entropy distribution that has marginals ν that
minimize ||ν − µ||1.

4There are two challenges to doing this. The ﬁrst  which has been addressed in previous work  is to circum-
vent the exponential number of decision variables via a separation oracle. The second  which does not seem to
have been previously addressed  is to bound the diameter of the search space (i.e.  the magnitude of the optimal
Lagrange variables). Proving this requires using special properties of the MAX ENTROPY problem  beyond
mere convexity. We adapt recent techniques of [13] to provide the necessary argument.

5All of our results generalize to the case of higher-order marginals in a straightforward manner.

4

It is important to emphasize that all of the problems above are “model-free”  in that we do not
assume that the marginals are consistent with  or generated by  any particular model (such as a
Markov network). They are simply given marginals  or “data”.
For each of these problems  our interest is in algorithms whose running time is polynomial in the
size of the input µ. The prospects for this depend strongly on the class G  with tractability generally
following for “nice” classes such as tree or tree-like graphs  and intractability for the most general
cases. Our contribution is in showing a strong connection between tractability for these marginals
problems and the following inference problems for any class G.

a posteriori (MAP) or most probable joint assignment.6

• MAP INFERENCE (G): Given a Markov network whose graph falls in G  ﬁnd the maximum
• GENERALIZED PARTITION: Given a Markov network whose graph falls in G  compute
the partition function or normalization constant for the full joint distribution  possibly after
conditioning on the value of a single vertex or edge.7

2.2 The Ellipsoid Method for Linear Programming

Our algorithms for the CONSISTENCY  CLOSEST CONSISTENCY  and SMALL SUPPORT problems
use linear programming. There are a number of algorithms that solve explictly described linear
programs in time polynomial in the description size. Our problems  however  pose an additional
challenge: the obvious linear programming formulation has size exponential in the parameters of
interest. To address this challenge  we turn to the ellipsoid method [9]  which can solve in poly-
nomial time linear programs that have an exponential number of implicitly described constraints 
provided there is a polynomial-time “separation oracle” for these constraints. The ellipsoid method
is discussed exhaustively in [10  11]; we record in this section the facts necessary for our results.
mx ≤ bm} denote the
Deﬁnition 2.1 (Separation Oracle) Let P = {x ∈ Rn : aT
feasible region of m linear constraints in n dimensions. A separation oracle for P is an algorithm
that takes as input a vector x ∈ Rn  and either (i) veriﬁes that x ∈ P; or (ii) returns a constraint i
such that at
ix > bi. A polynomial-time separation oracle runs in time polynomial in n  the maximum
description length of a single constraint  and the description length of the input x.

1 x ≤ b1  . . .   aT

1 x ≤ b1  . . .   aT

One obvious separation oracle is to simply check  given a candidate solution x  each of the m
constraints in turn. More interesting and relevant are constraint sets that have size exponential in the
dimension n but admit a polynomial-time separation oracle.
Theorem 2.2 (Convergence Guarantee of the Ellipsoid Method [9]) Suppose the set P = {x ∈
mx ≤ bm} admits a polynomial-time separation oracle and cT x is a linear
Rn : aT
objective function. Then  the ellipsoid method solves the optimization problem {max cT x : x ∈ P}
in time polynomial in n and the maximum description length of a single constraint or objective
function. The method correctly detects if P = ∅. Moreover  if P is non-empty and bounded  the
ellipsoid method returns a vertex of P.8
Theorem 2.2 provides a general reduction from a problem to an intuitively easier one: if the problem
of verifying membership in P can be solved in polynomial time  then the problem of optimizing an
arbitrary linear function over P can also be solved in polynomial time. This reduction is “many-to-
one ” meaning that the ellipsoid method invokes the separation oracle for P a large (but polynomial)
number of times  each with a different candidate point x. See Appendix A.1 for a high-level de-
scription of the ellipsoid method and [10  11] for a detailed treatment.
The ellipsoid method also applies to convex programming problems under some additional techni-
cal conditions. This is discussed in Appendix A.2 and applied to the MAX ENTROPY problem in
Appendix A.3.

6Formally  the input is a graph G = (V  E) with a log-potential log φi(s) and log φij(s  t) for each vertex
i ∈ V and edge (i  j) ∈ E  and each value s ∈ [k] = {0  1  2 . . .   k} and pair s  t ∈ [k] × [k] of values. The

MAP assignment maximizes P (a) :=(cid:81)
7Formally  given the log-potentials of a Markov network  compute(cid:80)
given i  s; or(cid:80)

i∈V φi(ai)(cid:81)

a : ai=s aj =t P (a) for a given i  j  s  t.

8A vertex is a point of P that satisﬁes with equality n linearly independent constraints.

(i j)∈E φij(ai  aj) over all assignments a ∈ [k]V .

a∈[k]n P (a); (cid:80)

a : ai=s P (a) for a

5

3 CONSISTENCY Reduces to MAP INFERENCE
The goal of this section is to reduce the CONSISTENCY problem for data graphs in the family G to
the MAP INFERENCE problem for networks in G.
Theorem 3.1 (Main Result 1) Let G be a set of graphs. If the the MAP INFERENCE (G) problem
can be solved in polynomial time  then the CONSISTENCY (G) problem can be solved in polynomial
time.

We begin with a straightforward linear programming formulation of the CONSISTENCY problem.

Lemma 3.2 (Linear Programming Formulation) An instance of the CONSISTENCY problem ad-
mits a consistent distribution if and only if the following linear program (P) has a solution:

0

(P ) max

p

subject to:

(cid:80)

(cid:80)

for all (i  s) ∈ M1
for all (i  j  s  t) ∈ M2

a∈A:ai=s pa = µis

a∈A:ai=s aj =t pa = µijst

(cid:80)

a∈A pa = 1
pa ≥ 0

for all a ∈ A.

Solving (P) using the ellipsoid method (Theorem 2.2)  or any other linear programming method 
requires time at least |A| = (k +1)n  the number of decision variables. This is generally exponential
in the size of the input  which is proportional to the number m of given marginal values.
A ray of hope is provided by the fact that the number of constraints of the linear program in
Lemma 3.2 is equal to the number of marginal values. With an eye toward applying the ellipsoid
method (Theorem 2.2)  we consider the dual linear program. We use the following notation. Given
a vector y indexed by M1 ∪ M2  we deﬁne

y(a) =

yis +

yijst

(i s)∈M1 : ai=s

(i j s t)∈M2 : ai=s aj =t

(cid:88)
(cid:88)

(cid:88)

(cid:88)

µisyis +

µijstyijst.

(i s)∈M1

(i j s t)∈M2

for each assignment a ∈ A  and
µT y =

(1)

(2)

Strong linear programming duality implies the following.

Lemma 3.3 (Dual Linear Programming Formulation) An instance of the CONSISTENCY prob-
lem admits a consistent distribution if and only if the optimal value of the following linear pro-
gram (D) is 0:

(D) max
y z
subject to:

µT y + z

y(a) + z ≤ 0
y  z unrestricted.

for all a ∈ A

The number of variables in (D) — one per constraint of the primal linear program — is polynomial
in the size of the CONSISTENCY input.
What use is the MAP INFERENCE problem for solving the CONSISTENCY problem? The next
lemma forges the connection.
Lemma 3.4 (Map Inference as a Separation Oracle) Let G be a set of graphs and suppose that
the MAP INFERENCE (G) problem can be solved in polynomial time. Consider an instance of the
CONSISTENCY problem with a data graph in G  and a candidate solution y  z to the corresponding

6

dual linear program (D). Then  there is a polynomial-time algorithm that checks whether or not
there is an assignment a ∈ A that satisﬁes

(cid:88)

(cid:88)

yis +

(i s)∈M1 : ai=s

(i j s t)∈M2 : ai=s aj =t

yijst > −z 

(3)

and produces such an assignment if one exists.

Proof: The key idea is to interpret y as the log-potentials of a Markov network. Precisely  construct a
Markov network N as follows. The vertex set V and edge set E correspond to the random variables
and edge set of the data graph of the CONSISTENCY instance. The potential function at a vertex i
is deﬁned as φi(s) = exp{yis} for each value s ∈ [k]. The potential function at an edge (i  j)
is deﬁned as φij(s  t) = exp{yijst} for (s  t) ∈ [k] × [k]. For a missing pair (i  s) /∈ M1 or 4-
tuple (i  j  s  t) /∈ M2  we deﬁne the corresponding potential value φi(s) or φij(st) to be 1. The
underlying graph of N is the same as the data graph of the given CONSISTENCY instance and hence
is a member of G.
In the distribution induced by N  the probability of an assignment a ∈ [k]n is  by deﬁnition  propor-

tional to (cid:89)



exp{yiai}

(cid:89)

 = exp{y(a)}.

exp{yijaiaj}

i∈V : (i ai)∈M1

(i j)∈E : (i j ai aj )∈M2

That is  the MAP assignment for the Markov network N is the assignment that maximizes the left-
hand size of (3).
Checking if some assignment a ∈ A satisﬁes (3) can thus be implemented as follows: compute the
MAP assignment a∗ for N — by assumption  and since the graph of N lies in G  this can be done
in polynomial time; return a∗ if it satisﬁes (3)  and otherwise conclude that no assignment a ∈ A
satisﬁes (3). (cid:4)

All of the ingredients for the proof of Theorem 3.1 are now in place.
Proof of Theorem 3.1: Assume that there is a polynomial-time algorithm for the MAP INFERENCE
(G) problem with the family G of graphs  and consider an instance of the CONSISTENCY problem
with data graph G ∈ G. Deciding whether or not this instance has a consistent distribution is equiv-
alent to solving the program (D) in Lemma 3.3. By Theorem 2.2  the ellipsoid method can be used
to solve (D) in polynomial time  provided the constraint set admits a polynomial-time separation
oracle. Lemma 3.4 shows that the relevant separation oracle is equivalent to computing the MAP
assignment of a Markov network with graph G ∈ G. By assumption  the latter problem can be
solved in polynomial time. (cid:4)

We deﬁned the CONSISTENCY problem as a decision problem  where the answer is “yes” or no.”
For instances that admit a consistent distribution  we can also ask for a succinct representation of a
distribution that witnesses the marginals’ consistency. We next strengthen Theorem 3.1 by showing
that for consistent instances  under the same hypothesis  we can compute a small-support consistent
distribution in polynomial time. See Figure 2 for the high-level description of the algorithm.
Theorem 3.5 (Small-Support Witnesses) Let G be a set of graphs. If the MAP INFERENCE (G)
problem can be solved in polynomial time  then for every consistent instance of the CONSISTENCY
(G) problem with m = |M1| + |M2| marginal values  a consistent distribution with support size at
most m + 1 can be computed in polynomial time.
Proof: Consider a consistent instance of CONSISTENCY with data graph G ∈ G. The algorithm
of Theorem 3.1 concludes by solving the dual linear program of Lemma 3.3 using the ellipsoid
method. This method runs for a polynomial number K of iterations  and each iteration generates
one new inequality. At termination  the algorithm has identiﬁed a “reduced dual linear program”  in
which a set of only K out of the original (k + 1)n constraints is sufﬁcient to prove the optimality of
its solution. By strong duality  the corresponding “reduced primal linear program ” obtained from
the linear program in Lemma 3.2 by retaining only the decision variables corresponding to the K

7

1. Solve the dual linear program (D) (Lemma 3.3) using the ellipsoid method (Theorem 2.2) 
using the given polynomial-time algorithm for MAP INFERENCE (G) to implement the
ellipsoid separation oracle (see Lemma 3.4).

2. If the dual (D) has a nonzero (and hence  unbounded) optimal objective function value 

then report “no consistent distributions” and halt.

3. Explicitly form the reduced primal linear program (P-red)  obtained from (P) by retaining
only the variables that correspond to the dual inequalities generated by the separation oracle
in Step 1.

4. Solve (P-red) using a polynomial-time linear programming algorithm that returns a vertex

solution  and return the result.

Figure 2: High-level description of the polynomial-time reduction from CONSISTENCY (G) to MAP INFER-
ENCE (G) (Steps 1 and 2) and postprocessing to extract a small-support distribution that witnesses consistent
marginals (Steps 3 and 4).

reduced dual constraints  has optimal objective function value 0. In particular  this reduced primal
linear program is feasible.
The reduced primal linear program has a polynomial number of variables and constraints  so it can be
solved by the ellipsoid method (or any other polynomial-time method) to obtain a feasible point p.
The point p is an explicit description of a consistent distribution with support size at most K. To
improve the support size upper bound from K to m + 1  recall from Theorem 2.2 that p is a vertex
of the feasible region  meaning it satisﬁes K linearly independent constraints of the reduced primal
linear program with equality. This linear program has at most one constraint for each of the m given
a∈A pa = 1  and non-negativity constraints.
Thus  at least K−m−1 of the constraints that p satisﬁes with equality are non-negativity constraints.
Equivalently  it has at most m + 1 strictly positive entries. (cid:4)

marginal values  at most one normalization constraint(cid:80)

4 CLOSEST CONSISTENCY  SMALL SUPPORT Reduce to MAP INFERENCE

This section considers the CLOSEST CONSISTENCY and SMALL SUPPORT problems. The input
to these problems is the same as in the CONSISTENCY problem — single marginal values µis for
(i  s) ∈ M1 and pairwise marginal values µijst for (i  j  s  t) ∈ M2. The goal is to compute sets
of marginals {νis}M1 and {νijst}M2 that are consistent and  subject to this constraint  minimize the
(cid:96)1 norm ||µ − ν||1 with respect to the given marginals. An algorithm for the CLOSEST CONSIS-
TENCY problem solves the CONSISTENCY problem as a special case  since a given set of marginals
is consistent if and only if the corresponding CLOSEST CONSISTENCY problem has optimal objec-
tive function value 0. Despite this greater generality  the CLOSEST CONSISTENCY problem also
reduces in polynomial time to the MAP INFERENCE problem  as does the still more general SMALL
SUPPORT problem.
If the MAP INFERENCE (G) problem
Theorem 4.1 (Main Result 2) Let G be a set of graphs.
can be solved in polynomial time  then the CLOSEST CONSISTENCY (G) problem can be solved in
polynomial time. Moreover  a distribution consistent with the optimal marginals with support size
at most 3m + 1 can be computed in polynomial time  where m = |M1| + |M2| denotes the number
of marginal values.
The formulation of the CLOSEST CONSISTENCY (G) problem has linear constraints — the same
as those in Lemma 3.2  except with the given marginals µ replaced by the computed consistent
marginals ν — but a nonlinear objective function ||µ − ν||1. We can simulate the absolute value
functions in the objective by adding a small number of variables and constraints. We provide details
and the proof of Theorem 4.1 in Appendix A.4.

8

References
[1] Nicolo Cesa-Bianchi and G´abor Lugosi. Prediction  Learning  and Games. Cambridge Uni-

versity Press  2006.

[2] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT

Press  2009.

[3] M.J. Wainwright and M.I. Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends in Machine Learning  1(1)  2008.

[4] S. Lauritzen. Graphical Models. Oxford University Press  1996.
[5] T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori pertur-

bations. Proceedings of the 29th International Conference on Machine Learning  2012.

[6] J. K. Johnson  V. Chandrasekaran  and A. S. Willsky. Learning markov structure by maximum
entropy relaxation. In 11th International Conference in Artiﬁcial Intelligence and Statistics
(AISTATS 2007)  2007.

[7] V. Chandrasekaran  J. K. Johnson  and A. S. Willsky. Maximum entropy relaxation for graph-
ical model selection given inconsistent statistics. In IEEE Statistical Signal Processing Work-
shop (SSP 2007)  2007.

[8] D. Sontag and T. Jaakkola. New outer bounds on the marginal polytope. In Neural Information

Processing Systems (NIPS)  2007.

[9] L. G. Khachiyan. A polynomial algorithm in linear programming. Soviet Mathematics Dok-

lady  20(1):191–194  1979.

[10] A. Ben-Tal and A. Nemirovski. Optimization iii. Lecture notes  2012.
[11] M. Gr¨otschel  L. Lov´asz  and A. Schrijver. Geometric Algorithms and Combinatorial Opti-

mization. Springer  1988. Second Edition  1993.

[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge  2004.
[13] M. Singh and N. Vishnoi. Entropy  optimization and counting. arXiv  (1304.8108)  2013.

9

,Tim Roughgarden
Michael Kearns
Daniele Calandriello
Alessandro Lazaric
Marcello Restelli