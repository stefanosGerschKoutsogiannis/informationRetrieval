2019,Learning and Generalization in Overparameterized Neural Networks  Going Beyond Two Layers,The fundamental learning theory behind neural networks remains largely open. What classes of functions can neural networks actually learn? Why doesn't the trained network overfit when it is overparameterized?

In this work  we prove that overparameterized neural networks can learn some notable concept classes  including two and three-layer networks with fewer parameters and smooth activations. Moreover  the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using polynomially many samples. The sample complexity can also be almost independent of the number of parameters in the network.

On the technique side  our analysis goes beyond the so-called NTK (neural tangent kernel) linearization of neural networks in prior works. We establish a new notion of quadratic approximation of the neural network  and connect it to the SGD theory of escaping saddle points.,Learning and Generalization in Overparameterized

Neural Networks  Going Beyond Two Layers∗

Zeyuan Allen-Zhu
Microsoft Research AI

zeyuan@csail.mit.edu

Yuanzhi Li

Carnegie Mellon University
yuanzhil@andrew.cmu.edu

Yingyu Liang

University of Wisconsin-Madison

yliang@cs.wisc.edu

Abstract

The fundamental learning theory behind neural networks remains largely open.
What classes of functions can neural networks actually learn? Why doesn’t the
trained network overﬁt when it is overparameterized?
In this work  we prove that overparameterized neural networks can learn some
notable concept classes  including two and three-layer networks with fewer pa-
rameters and smooth activations. Moreover  the learning can be simply done by
SGD (stochastic gradient descent) or its variants in polynomial time using poly-
nomially many samples. The sample complexity can also be almost independent
of the number of parameters in the network.
On the technique side  our analysis goes beyond the so-called NTK (neural tan-
gent kernel) linearization of neural networks in prior works. We establish a new
notion of quadratic approximation of the neural network  and connect it to the
SGD theory of escaping saddle points.

Introduction

1
Neural network learning has become a key machine learning approach and has achieved remarkable
success in a wide range of real-world domains  such as computer vision  speech recognition  and
game playing [25  26  30  41].
In contrast to the widely accepted empirical success  much less
theory is known. Despite a recent boost of theoretical studies  many questions remain largely open 
including fundamental ones about the optimization and generalization in learning neural networks.
One key challenge in analyzing neural networks is that the corresponding optimization is non-convex
and is theoretically hard in the general case [40  55]. This is in sharp contrast to the fact that simple
optimization algorithms like stochastic gradient descent (SGD) and its variants usually produce good
solutions in practice even on both training and test data. Therefore 

what functions can neural networks provably learn?

Another key challenge is that  in practice  neural networks are heavily overparameterized (e.g.  [53]):
the number of learnable parameters is much larger than the number of the training samples.
It
is observed that overparameterization empirically improves both optimization and generalization 
appearing to contradict traditional learning theory.2 Therefore 

why do overparameterized networks (found by those training algorithms) generalize?

∗Full version and future updates can be found on https://arxiv.org/abs/1811.04918.
2For example  Livni et al. [36] observed that on synthetic data generated from a target network  SGD con-
verges faster when the learned network has more parameters than the target. Perhaps more interestingly  Arora
et al. [6] found that overparameterized networks learned in practice can often be compressed to simpler ones
with much fewer parameters  without hurting their ability to generalize; however  directly learning such simpler
networks runs into worse results due to the optimization difﬁculty. We also have experiments in Figure 1(a).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 What Can Neural Networks Provably Learn?
Most existing works analyzing the learnability of neural networks [9  12  13  20  21  28  33  34  42 
43  47  49  50  56] make unrealistic assumptions about the data distribution (such as being random
Gaussian)  and/or make strong assumptions about the network (such as using linear activations).
Li and Liang [32] show that two-layer ReLU networks can learn classiﬁcation tasks when the data
come from mixtures of arbitrary but well-separated distributions.
A theorem without distributional assumptions on data is often more desirable. Indeed  how to obtain
a result that does not depend on the data distribution  but only on the concept class itself  lies in the
center of PAC-learning which is one of the foundations of machine learning theory [48]. Also 
studying non-linear activations is critical because otherwise one can only learn linear functions 
which can also be easily learned via linear models without neural networks.
Brutzkus et al. [14] prove that two-layer networks with ReLU activations can learn linearly-separable
data (and thus the class of linear functions) using just SGD. This is an (improper) PAC-learning
type of result because it makes no assumption on the data distribution. Andoni et al. [5] proves that
two-layer networks can learn polynomial functions of degree r over d-dimensional inputs in sample
complexity O(dr). Their learner networks use exponential activation functions  where in practice the
rectiﬁed linear unit (ReLU) activation has been the dominant choice across vastly different domains.
On a separate note  if one treats all but the last layer of neural networks as generating a random
feature map  then training only the last layer is a convex task  so one can learn the class of linear
functions in this implicit feature space [15  16]. This result implies low-degree polynomials and
compositional kernels can be learned by neural networks in polynomial time. Empirically  training
last layer greatly weakens the power of neural networks (see Figure 1).
Our Result. We prove that an important concept class that contains three-layer (resp. two-layer)
neural networks equipped with smooth activations can be efﬁciently learned by three-layer (resp.
two-layer) ReLU neural networks via SGD or its variants.
Speciﬁcally  suppose in aforementioned class the best network (called the target function or target
network) achieves a population risk OPT with respect to some convex loss function. We show that
one can learn up to population risk OPT + ε  using three-layer (resp. two-layer) ReLU networks of
size greater than a ﬁxed polynomial in the size of the target network  in 1/ε  and in the “complexity”
of the activation function used in the target network. Furthermore  the sample complexity is also
polynomial in these parameters  and only poly-logarithmic in the size of the learner ReLU network.
We stress here that this is agnostic PAC-learning because we allow the target function to have er-
ror (e.g.  OPT can be positive for regression)  and is improper learning because the concept class
consists of smaller neural networks comparing to the networks being trained.
Our Contributions. We believe our result gives further insights to the fundamental questions about
the learning theory of neural networks.

• To the best of our knowledge  this is the ﬁrst result showing that using hidden layers of neural
networks one can provably learn the concept class containing two (or even three) layer neural
networks with non-trivial activation functions.3

• Our three-layer result gives the ﬁrst theoretical proof that learning neural networks  even with
non-convex interactions across layers  can still be plausible. In contrast  in the two-layer case
the optimization landscape with overparameterization is almost convex [17  32]; and in previ-
ous studies on the multi-layer case  researchers have weakened the network by applying the so-
called NTK (neural tangent kernel) linearization to remove all non-convex interactions [4  27].
• To some extent we explain the reason why overparameterization improves testing accuracy:
with larger overparameterization  one can hope to learn better target functions with possibly
larger size  more complex activations  smaller risk OPT  and to a smaller error ε.

• We establish new tools to tackle the learning process of neural networks in general  which can
be useful for studying other network architectures and learning tasks. (E.g.  the new tools here

3In contrast  Daniely [15] focuses on training essentially only the last layer (and the hidden-layer movement
is negligible). After this paper has appeared online  Arora et al. [8] showed that neural networks can provably
learn two-layer networks with a slightly weaker class of smooth activation functions. Namely  the activation
functions that are either linear functions or even functions.

2

have allowed researchers to study also the learning of recurrent neural networks [2].)

Other Related Works. We acknowledge a different line of research using kernels as improper
learners to learn the concept class of neural networks [22  23  36  54]. This is very different from us
because we use “neural networks” as learners. In other words  we study the question of “what can
neural networks learn” but they study “what alternative methods can replace neural networks.”
There is also a line of work studying the relationship between neural networks and NTKs (neural
tangent kernels) [3  4  7  27  31  51]. These works study neural networks by considering their
“linearized approximations.” There is a known performance gap between the power of real neural
networks and the power of their linearized approximations. For instance  ResNet achieves 96%
test error on the CIFAR-10 data set but NTK (even with inﬁnite width) achieves 77% [7]. We also
illustrate this in Figure 1.

1.2 Why Do Overparameterized Networks Generalize?
Our result above assumes that the learner network is sufﬁciently overparameterized. So  why does it
generalize to the population risk and give small test error? More importantly  why does it generalize
with a number of samples that is (almost) independent of the number of parameters?
This question cannot be studied under the traditional VC-dimension learning theory since the VC
dimension grows with the number of parameters. Several works [6  11  24  39] explain generaliza-
tion by studying some other “complexity” of the learned networks. Most related to the discussion
here is [11] where the authors prove a generalization bound in the norms (of weight matrices) of
each layer  as opposed to the number of parameters. There are two main concerns with those results.
• Learnability = Trainability + Generalization. It is not clear from those results how a network
with both low “complexity” and small training loss can be found by the training method.
Therefore  they do not directly imply PAC-learnability for non-trivial concept classes (at least
for those concept classes studied by this paper).

• Their norms are “sparsity induced norms”: for the norm not to scale with the number of hidden
neurons m  essentially  it requires the number of neurons with non-zero weights not to scale
with m. This more or less reduces the problem to the non-overparameterized case.

At a high level  our generalization is made possible with the following sequence of conceptual steps.
• Good networks with small risks are plentiful: thanks to overparameterization  with high prob-
ability over random initialization  there exists a good network in the close neighborhood of
any point on the SGD training trajectory. (This corresponds to Section 6.2 and 6.3.)

• The optimization in overparameterized neural networks has benign properties: essentially
along the training trajectory  there is no second-order critical points for learning three-layer
networks  and no ﬁrst-order critical points for two-layer. (This corresponds to Section 6.4.)

• In the learned networks  information is also evenly distributed among neurons  by utilizing
either implicit or explicit regularization. This structure allows a new generalization bound that
is (almost) independent of the number of neurons. (This corresponds to Section 6.5 and 6.6 
and we also empirically verify it in Section 7.1.)

Since practical neural networks are typically overparameterized  we genuinely hope that our results
can provide theoretical insights to networks used in various applications.

1.3 Roadmap
In the main body of this paper  we introduce notations in Section 2  present our main results and
contributions for two and three-layer networks in Section 3 and 4  and conclude in Section 5.
For readers interested in our novel techniques  we present in Section 6 an 8-paged proof sketch of our
three-layer result. For readers more interested in the practical relevance  we give more experiments
in Section 7. In the appendix  we begin with mathematical preliminaries in Appendix A. Our full
three-layer proof is in Appendix C. Our two-layer proof is much easier and in Appendix B.

3

(a) N = 1000 and vary m

(b) m = 2000 and vary N

Figure 1: Performance comparison.

3layer/2layer stands for training (hidden weights) in three and
two-layer neural networks. (last) stands for conjugate kernel [15]  meaning training only the
output layer. (NTK) stands for neural tangent kernel [27] with ﬁnite width. We also implemented
other direct kernels such as [54] but they perform much worse.
Setup. We consider (cid:96)2 regression task on synthetic data where feature vectors x ∈ R4
are generated as normalized random Gaussian  and label
function
F ∗(x) = (sin(3x1) + sin(3x2) + sin(3x3) − 2)2 · cos(7x4). We use N training samples 
and SGD with mini-batch size 50 and best tune learning rates and weight decay parameters. See
Appendix 7 for our experiment setup  how we choose such target function  and more experiments.

is generated by target

function φ(z). Suppose φ(z) =(cid:80)∞
(C∗R)i +(cid:0)√
Cε(φ  R) :=(cid:80)∞

(cid:16)

2 Notations
σ(·) denotes the ReLU function σ(x) = max{x  0}. Given f : R → R and a vector x ∈ Rm  f (x)
denotes f (x) = (f (x1)  . . .   f (xm)). For a vector w  (cid:107)w(cid:107)p denote its p-th norm  and when clear
from the context  abbreviate (cid:107)w(cid:107) = (cid:107)w(cid:107)2. For a matrix W ∈ Rm×d  use Wi or sometimes wi to
denote the i-th row of W . The row (cid:96)p norm is (cid:107)W(cid:107)2 p :=
  the spectral norm is
(cid:107)W(cid:107)2  and the Frobenius norm is (cid:107)W(cid:107)F = (cid:107)W(cid:107)2 2. We say f : Rd → R is L-Lipschitz continuous
if |f (x) − f (y)| ≤ L(cid:107)x − y(cid:107)2; is L-Lipschitz smooth if (cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ L(cid:107)x − y(cid:107)2.
Function complexity. The following notion measures the complexity of any smooth activation

i∈[m] (cid:107)Wi(cid:107)p

(cid:17)1/p

(cid:16)(cid:80)

2

i=0 cizi. Given a non-negative R  the complexity

C∗R(cid:1)i(cid:17)|ci|  Cs(φ  R) := C∗(cid:80)∞

√
log(1/ε)

i

i=0

i=0(i + 1)1.75Ri|ci|
where C∗ is a sufﬁciently large constant (e.g.  104). Intuitively  Cs measures the sample complexity:
how many samples are required to learn φ correctly; while Cε bounds the network size: how much
over-parameterization is needed for the algorithm to efﬁciently learn φ up to ε error. It is always
true that Cs(φ  R) ≤ Cε(φ  R) ≤ Cs(φ  O(R)) × poly(1/ε).4 While for sin z  exp(z) or low degree
polynomials  Cs(φ  O(R)) and Cε(φ  R) only differ by o(1/ε).
Example 2.1. If φ(z) = ec·z − 1  φ(z) = sin(c · z)  φ(z) = cos(c · z) for constant c or φ(z) is
low degree polynomial  then Cε(φ  1) = o(1/ε) and Cs(φ  1) = O(1). If φ(z) = sigmoid(z) or
tanh(z)  we can truncate their Taylor series at degree Θ(log 1
ε ) to get ε approximation. One can
verify this gives Cε(φ  1) ≤ poly(1/ε) and Cs(φ  1) ≤ O(1).

3 Result for Two-Layer Networks
We consider learning some unknown distribution D of data points z = (x  y) ∈ Rd × Y  where x
is the input point and y is the label. Without loss of generality  assume (cid:107)x(cid:107)2 = 1 and xd = 1
2.5
Consider a loss function L : Rk × R → Y such that for every y ∈ Y  the function L(·  y) is non-
negative  convex  1-Lipschitz continuous and 1-Lipschitz smooth and L(0  y) ∈ [0  1]. This includes
both the cross-entropy loss and the (cid:96)2-regression loss (for bounded Y).

4Recall(cid:0)√
padding(cid:112)1 − (cid:107)x(cid:107)2

5 1

√
log(1/ε)
i

C∗(cid:1)i ≤ eO(log(1/ε)) = 1

poly(ε) for every i ≥ 1.

2 can always be padded to the last coordinate  and (cid:107)x(cid:107)2 = 1 can always be ensured from (cid:107)x(cid:107)2 ≤ 1 by

2. This assumption is for simplifying the presentation.

4

0.0630.251.4.16.64.Test errorm = number of hidden neurons3layer2layer3layer(last)2layer(last)3layer(NTK)0.0010.0040.0160.0630.251.4.16.64.Test errorN = number of samples3layer2layer3layer(last)2layer(last)3layer(NTK)Concept class and target function F ∗(x). Consider target functions F ∗ : Rd → Rk of

F ∗ = (f∗

1   . . .   f∗
k )

r iφi((cid:104)w∗
a∗
where each φi : R → R is inﬁnite-order smooth and the weights w∗
We assume for simplicity (cid:107)w∗

1 i  x(cid:105))(cid:104)w∗
1 i ∈ Rd  w∗
r i| ≤ 1.6 We denote by

2 i(cid:107)2 = 1 and |a∗

1 i(cid:107)2 = (cid:107)w∗

f∗
r (x) =

and

i=1

2 i  x(cid:105)
2 i ∈ Rd and a∗

(3.1)
r i ∈ R.

p(cid:88)

Cε(φ  R) := maxj∈[p]{Cε(φj  R)}

and Cs(φ  R) := maxj∈[p]{Cs(φj  R)}

the complexity of F ∗ and assume they are bounded.
In the agnostic PAC-learning language  our concept class consists of all functions F ∗ in the form
of (3.1) with complexity bounded by threshold C and parameter p bounded by threshold p0. Let
OPT = E[L(F ∗(x)  y)] be the population risk achieved by the best target function in this concept
class. Then  our goal is to learn this concept class with population risk OPT + ε using sample and
time complexity polynomial in C  p0 and 1/ε. In the remainder of this paper  to simplify notations 
we do not explicitly deﬁne this concept class parameterized by C and p. Instead  we equivalently
state our theorem with respect to any (unknown) target function F ∗ with speciﬁc parameters C and
p satisfying OPT = E[L(F ∗(x)  y)]. We assume OPT ∈ [0  1] for simplicity.
1 i  x(cid:105)) are special cases of (3.1) (by
Remark. Standard two-layer networks f∗
setting w∗
2 i = (0  . . .   0  1) and φi = φ). Our formulation (3.1) additionally captures combinations
of correlations between non-linear and linear measurements of different directions of x.
Learner network F (x; W ). Using a data set Z = {z1  . . .   zN} of N i.i.d. samples from D  we
train a network F = (f1 ···   fk) : Rd → Rk with

r (x) =(cid:80)p

r iφ((cid:104)w∗

i=1 a∗

ar iσ((cid:104)wi  x(cid:105) + bi) = a(cid:62)

i=1

fr(x) :=

(3.2)
where σ is the ReLU activation  W = (w1  . . .   wm) ∈ Rm×d is the hidden weight matrix  b ∈ Rm
is the bias vector  and ar ∈ Rm is the output weight vector. To simplify analysis  we only update W
and keep b and ar at initialization values. For such reason  we write the learner network as fr(x; W )
and F (x; W ). We sometimes use b(0) = b and a(0)
r = ar to emphasize they are randomly initialized.

Our goal is to learn a weight matrix W with population risk E(cid:2)L(F (x; W )  y)(cid:3) ≤ OPT + ε.

r σ(W x + b)

m(cid:88)

r

are i.i.d. random Gaussians from N (0  ε2

Learning Process. Let W (0) denote the initial value of the hidden weight matrix  and let W (0)+Wt
denote the value at time t. (Note that Wt is the matrix of increments.) The weights are initialized
with Gaussians and then W is updated by the vanilla SGD. More precisely 
• entries of W (0) and b(0) are i.i.d. random Gaussians from N (0  1/m) 
• entries of each a(0)

a) for some ﬁxed εa ∈ (0  1].7
At time t  SGD samples z = (x  y) ∼ Z and updates Wt+1 = Wt − η∇L(F (x; W (0) + Wt)  y).
3.1 Main Theorem
For notation simplicity  with high probability (or w.h.p.) means with probability 1 − e−c log2 m for a

sufﬁciently large constant c  and (cid:101)O hides factors of polylog(m).
Theorem 1 (two-layer). For every ε ∈(cid:0)0 
(cid:1)  there exists
such that for every m ≥ M0 and every N ≥ (cid:101)Ω(N0)  choosing εa = ε/(cid:101)Θ(1) for the initialization 
choosing learning rate η = (cid:101)Θ(cid:0) 1
(cid:1) and
T = (cid:101)Θ

(cid:18) (Cs(φ  1))2 · k3p2

M0 = poly(Cε(φ  1)  1/ε) and N0 = poly(Cs(φ  1)  1/ε)

(cid:19)

pkCs(φ 1)

εkm

1

 

ε2

6For general (cid:107)w∗

activation function φ(cid:48)(x) = φ(Bx). Our results then hold by replacing the complexity of φ with φ(cid:48).

7We shall choose εa = (cid:101)Θ(ε) in the proof due to technical reason. As we shall see in the three-layer case  if

r i| ≤ B  the scaling factor B can be absorbed into the

1 i(cid:107)2 ≤ B (cid:107)w∗

2 i(cid:107)2 ≤ B  |a∗

weight decay is used  one can relax this to εa = 1.

5

with high probability over the random initialization  SGD after T iteration satisﬁes

(cid:104) 1

T

(cid:80)T−1

t=0

Esgd

E(x y)∼DL(F (x; W (0) + Wt)  y)

(cid:105) ≤ OPT + ε.

Example 3.1. For functions such as φ(z) = ez  sin z  sigmoid(z)  tanh(z) or low degree polynomi-
als  using Example 2.1  our theorem indicates that for target networks with such activation functions 
we can learn them using two-layer ReLU networks with

size m =

poly(k  p)

poly(ε)

and sample complexity min{N  T} =

poly(k  p  log m)

ε2

We note sample complexity T is (almost) independent of m  the amount of overparametrization.

3.2 Our Interpretations
Overparameterization improves generalization. By increasing m  Theorem 1 supports more
target functions with possibly larger size  more complex activations  and smaller population risk
OPT. In other words  when m is ﬁxed  among the class of target functions whose complexities
are captured by m  SGD can learn the best function approximator of the data  with the smallest
population risk. This gives intuition how overparameterization improves test error  see Figure 1(a).
Large margin non-linear classiﬁer. Theorem 1 is a nonlinear analogue of the margin theory for
linear classiﬁers. The target function with a small population risk (and of bounded norm) can be
viewed as a “large margin non-linear classiﬁer.” In this view  Theorem 1 shows that assuming the
existence of such large-margin classiﬁer  SGD ﬁnds a good solution with sample complexity mostly
determined by the margin  instead of the dimension of the data.
Inductive bias. Recent works (e.g.  [4  32]) show that when the network is heavily overparame-
terized (that is  m is polynomial in the number of training samples) and no two training samples are
identical  then SGD can ﬁnd a global optimum with 0 classiﬁcation error (or ﬁnd a solution with
ε training loss) in polynomial time. This does not come with generalization  since it can even ﬁt
random labels. Our theorem  combined with [4]  conﬁrms the inductive bias of SGD for two-layer
networks: when the labels are random  SGD ﬁnds a network that memorizes the training data; when
the labels are (even only approximately) realizable by some target network  then SGD learns and
generalizes. This gives an explanation towards the well-known empirical observations of such in-
ductive bias (e.g.  [53]) in the two-layer setting  and is more general than Brutzkus et al. [14] in
which the target network is only linear.

4 Result for Three-Layer Networks
Concept class and target function F ∗(x). This time we consider more powerful target functions
F ∗ = (f∗

k ) of the form

a∗
r iΦi

1 i jφ1 j((cid:104)w∗
v∗

1 j  x(cid:105))

2 i jφ2 j((cid:104)w∗
v∗

2 j  x(cid:105))

(4.1)



1  ···   f∗
(cid:88)

f∗
r (x) :=

i∈[p1]

(cid:88)

j∈[p2]

(cid:88)

j∈[p2]

where each φ1 j  φ2 j  Φi : R → R is inﬁnite-order smooth  and the weights w∗
v∗
1 i  v∗
Let

1 j(cid:107)2 = (cid:107)w∗

2 j(cid:107)2 = (cid:107)v∗

r i ∈ R satisfy (cid:107)w∗

2 i ∈ Rp2 and a∗
Cε(φ  R) = maxj∈[p2] s∈[1 2]{Cε(φs j  R)} 
Cs(φ  R) = maxj∈[p2] s∈[1 2]{Cs(φs j  R)} 

1 i(cid:107)2 = (cid:107)v∗

1 i  w∗
2 i(cid:107)2 = 1 and |a∗
Cε(Φ  R) = maxj∈[p1]{Cε(Φj  R)}
Cs(Φ  R) = maxj∈[p1]{Cs(Φj  R)}

2 i ∈ Rd 
r i| ≤ 1.

to denote the complexity of the two layers  and assume they are bounded.
Our concept class contains measures of correlations between composite non-linear functions and
non-linear functions of the input  there are plenty of functions in this new concept class that may not
necessarily have small-complexity representation in the previous formulation (3.1)  and as we shall
see in Figure 1(a)  this is the critical advantage of using three-layer networks compared to two-
layer ones or their NTKs. The learnability of this correlation is due to the non-convex interactions
between hidden layers. As a comparison  [15] studies the regime where the changes in hidden layers
are negligible thus can not show how to learn this concept class with a three-layer network.

6

Remark 4.1. Standard three-layer networks
i∈[p1] a∗

f∗

r iΦi

r (x) =(cid:80)
(cid:16)(cid:80)

(cid:16)(cid:80)

(cid:17)

j∈[p2] v∗

i jφj((cid:104)w∗

j   x(cid:105))

(cid:17)(cid:16)(cid:80)

(cid:17)
2 j  x(cid:105))

are only special cases of (4.1). Also  even in the special case of Φi(z) = z  the target
2 i jφ2((cid:104)w∗

1 i jφ1((cid:104)w∗

1 j  x(cid:105))

j∈[p2] v∗

i∈[p1] a∗

f∗

r i

captures combinations of correlations of non-linear measurements in different directions of x.
Learner network F (x; W  V ). Our learners are three-layer networks F = (f1  . . .   fk) with

fr(x) =

ar iσ(ni(x) + b2 i) where each ni(x) =

vi jσ ((cid:104)wj  x(cid:105) + b1 j)

j∈[p2] v∗
(cid:88)

j∈[m1]

r (x) =(cid:80)
(cid:88)

i∈[m2]

The ﬁrst and second layers have m1 and m2 hidden neurons. Let W ∈ Rm1×d and V ∈ Rm2×m1
represent the weights of the ﬁrst and second hidden layers respectively  and b1 ∈ Rm1 and b2 ∈ Rm2
represent the corresponding bias vectors  ar ∈ Rm2 represent the output weight vector.
4.1 Learning Process
Again for simplicity  we only update W and V . The weights are randomly initialized as:

• entries of W (0) and b1 = b(0)
• entries of V (0) and b2 = b(0)
• entries of each ar = a(0)

2

1

r

are i.i.d. from N (0  1/m1) 
are i.i.d. from N (0  1/m2) 

are i.i.d. from N (0  ε2

a) for εa = 1.

As for the optimization algorithm  we use SGD with weight decay and an explicit regularizer.
For some λ ∈ (0  1]  we will use λF (x; W  V ) as the learner network  i.e.  linearly scale F down
by λ. This is equivalent to replacing W   V with
λV   since a ReLU network is positive
homogenous. The SGD will start with λ = 1 and slowly decrease it  similar to weight decay.8
We also use an explicit regularizer for some λw  λv > 0 with9
F + λw(cid:107)

√
λV ) := λv(cid:107)

λW(cid:107)4

λV (cid:107)2

λW  

2 4 .

λW 

√

√

√

√

√

R(

L2(λt−1; W (cid:48)  V (cid:48)) := L

Now  in each round t = 1  2  . . .   T   we use (noisy) SGD to minimize the following stochastic
objective for some ﬁxed λt−1:

(cid:16)
λt−1F(cid:0)x; W (0) + W ρ + ΣW (cid:48)  V (0) + V ρ + V (cid:48)Σ(cid:1)(cid:17)
+ R((cid:112)λt−1W (cid:48) (cid:112)λt−1V (cid:48))

(4.2)
Above  the objective is stochastic because (1) z ∼ Z is a random sample from the training set  (2)
W ρ and V ρ are two small perturbation random matrices with entries i.i.d. drawn from N (0  σ2
w)
v) respectively  and (3) Σ ∈ Rm1×m1 is a random diagonal matrix with diagonals i.i.d.
and N (0  σ2
uniformly drawn from {+1 −1}. We note that the use of W ρ and V ρ is standard for Gaussian
smoothing on the objective (and not needed in practice).10 The use of Σ may be reminiscent of the
Dropout technique [46] in practice which randomly masks out neurons  and can also be removed.11

8We illustrate the technical necessity of adding weight decay. During training  it is easy to add new infor-
mation to the current network  but hard to forget “false” information that is already in the network. Such false
information can be accumulated from randomness of SGD  non-convex landscapes  and so on. Thus  by scaling
down the network we can effectively forget false information.
9This (cid:107) · (cid:107)2 4 norm on W encourages weights to be more evenly distributed across neurons. It can be
2 2+α for any constant α > 0 for our theoretical purpose. We choose α = 2 for
simplicity  and observe that in practice  weights are automatically spread out due to data randomness  so this
explicit regularization may not be needed. See Section 7.1 for an experiment.

replaced with (cid:107)√

λt−1Wt−1(cid:107)2+α

10Similar to known non-convex literature [19] or smooth analysis  we introduce Gaussian perturbation W ρ
and V ρ for theoretical purpose and it is not needed in practice. Also  we apply noisy SGD which is the vanilla
SGD plus Gaussian perturbation  which again is needed in theory but believed unnecessary for practice [19].
11In the full paper we study two variants of SGD. This present version is the “second variant ” and the ﬁrst
variant L1(λt−1; W (cid:48)  V (cid:48)) is the same as (4.2) by removing Σ. Due to technical difﬁculty  the best sample
complexity we can prove for L1 is a bit higher.

7

Algorithm 1 SGD for three-layer networks (second variant (4.2))
Input: Data set Z  initialization W (0)  V (0)  step size η  number of inner steps Tw  σw  σv  λw  λv.

1: W0 = 0  V0 = 0  λ1 = 1  T = Θ(cid:0)η−1 log log(m1m2)

(cid:1).

ε0

2: for t = 1  2  . . .   T do
3:

Apply noisy SGD with step size η on the stochastic objective L2(λt−1; W  V ) for Tw steps; the starting
(cid:5) see Lemma A.9
point is W = Wt−1  V = Vt−1 and suppose it reaches Wt  Vt.
λt+1 = (1 − η)λt.
(cid:5) weight decay

4:
5: end for

6: Randomly sample (cid:98)Σ with diagonal entries i.i.d. uniform on {1 −1}
0) many noise matrices(cid:8)W ρ j  V ρ j(cid:9). Let
7: Randomly sample(cid:101)Θ(1/ε2
(cid:110)Ez∈Z L

j∗ = arg minj

λT F(cid:0)x; W (0) + W ρ j +(cid:98)ΣWT   V (0) + V ρ j + VT(cid:98)Σ(cid:1)(cid:17)(cid:111)
(cid:16)
+(cid:98)ΣWT   V (out)

+ VT(cid:98)Σ.

= V (0) + V ρ j∗

T

8: Output W (out)

T

= W (0) + W ρ j∗

Algorithm 1 presents the details. Speciﬁcally  in each round t  Algorithm 1 starts with weight ma-
trices Wt−1  Vt−1 and performs Tw iterations. In each iteration it goes in the negative direction of
the stochastic gradient ∇W (cid:48) V (cid:48)L2(λt; W (cid:48)  V (cid:48)). Let the ﬁnal matrices be Wt  Vt. At the end of this
round t  Algorithm 1 performs weight decay by setting λt = (1 − η)λt−1 for some η > 0.
4.2 Main Theorems
For notation simplicity  with high probability (or w.h.p.) means with probability 1 − e−c log2(m1m2)

and (cid:101)O hides factors of polylog(m1  m2).

Theorem 2 (three-layer  second variant). Consider Algorithm 1. For every constant γ ∈ (0  1/4] 
every ε0 ∈ (0  1/100]  every ε =

2Cs(Φ p2Cs(φ 1))Cs(φ 1)2   there exists

kp1p2

ε0

√

(cid:16)
(cid:17)
p2Cε(φ  1)) · Cε(φ  1) · √

p2Cε(φ  1)) 

Cε(Φ 

1
ε

(cid:17)2(cid:19)

p2p1k2

ε0

M = poly

(cid:18)(cid:16) Cε(Φ 

√

N ≥(cid:101)Ω

such that for every m2 = m1 = m ≥ M  and properly set λw  λv  σw  σv in Table 1  as long as

there is a choice η = 1/poly(m1  m2) and T = poly(m1  m2) such that with probability ≥ 99/100 

E(x y)∼DL(λT F (x; W (out)

T

  V (out)

T

)  y) ≤ (1 + γ)OPT + ε0.

4.3 Our Contributions
Our sample complexity N scales polynomially with the complexity of the target network  and is
(almost) independent of m  the amount of overparameterization. This itself can be quite surprising 
because recent results on neural network generalization [6  11  24  39] require N to be polynomial
in m. Furthermore  Theorem 2 shows three-layer networks can efﬁciently learn a bigger concept
class (4.1) comparing to what we know about two-layer networks (3.1).
From a practical standpoint  one can construct target functions of the form (4.1) that cannot be
(efﬁciently) approximated by any two-layer target function in (3.1). If data is generated according
to such functions  then it may be necessary to use three-layer networks as learners (see Figure 1).
From a theoretical standpoint  even in the special case of Φ(z) = z  our target function can cap-
ture correlations between non-linear measurements of the data (recall Remark 4.1). This means
p2Cε(φ  1))  so learning it is essentially in the same complexity as
Cε(Φ  Cε(φ  1)
learning each φs j. For example  a three-layer network can learn cos(100(cid:104)w∗
2  x(cid:105) up to
accuracy ε in complexity poly(1/ε)  while it is unclear how to do so using two-layer networks.
Technical Contributions. We highlight some technical contributions in the proof of Theorem 2.
In recent results on the training convergence of neural networks for more than two layers [3  4]  the
optimization process stays in a close neighborhood of the initialization so that  with heavy overpa-
rameterization  the network becomes “linearized” and the interactions across layers are negligible.
In our three-layer case  this means that the matrix W never interacts with V . They then argue that

1  x(cid:105)) · e100(cid:104)w∗

p2) ≈ O(

√

√

8

SGD simulates a neural tangent kernel so the learning process is almost convex [27]. In our analysis 
we directly tackle non-convex interactions between W and V   by studying a “quadratic approxima-
tion” of the network. (See Remark 6.1 for a mathematical comparison.) Our new proofs techniques
that could be useful for future theoretical applications.
Also  for the results [3  4] and our two-layer Theorem 1 to hold  it sufﬁces to analyze a regime where
the “sign pattern” of ReLUs can be replaced with that of the random initialization. (Recall σ(x) =
Ix≥0 · x and we call Ix≥0 the “sign pattern.”) In our three-layer analysis  the optimization process
has moved sufﬁciently away from initialization  so that the sign pattern change can signiﬁcantly
affect output. This brings in additional technical challenge because we have to tackle non-convex
interactions between W and V together with changing sign patterns.12
Comparison to Daniely [15]. Daniely [15] studies the learnability of multi-layer networks when
(essentially) only the output layer is trained  which reduces to a convex task. He shows that multi-
layer networks can learn a compositional kernel space  which implies two/three-layer networks can
efﬁciently learn low-degree polynomials. He did not derive the general sample/time complexity
bounds for more complex functions such as those in our concept classes (3.1) and (4.1)  but showed
that they are ﬁnite.
In contrast  our learnability result of concept class (4.1) is due to the non-convex interaction between
hidden layers. Since Daniely [15] studies the regime when the changes in hidden layers are negli-
gible  if three layer networks are used  to the best of our knowledge  their theorem cannot lead to
similar sample complexity bounds comparing to Theorem 2 by only training the last layer of a three-
layer network. Empirically  one can also observe that training hidden layers is better than training
the last layer (see Figure 1).

5 Conclusion and Discussion
We show by training the hidden layers of two-layer (resp.
three-layer) overparameterized neu-
ral networks  one can efﬁciently learn some important concept classes including two-layer (resp.
three-layer) networks equipped with smooth activation functions. Our result is in the agnostic PAC-
learning language thus is distribution-free. We believe our work opens up a new direction in both
algorithmic and generalization perspectives of overparameterized neural networks  and pushing for-
ward can possibly lead to more understanding about deep learning.
Our results apply to other more structured neural networks. As a concrete example  consider con-
volutional neural networks (CNN). Suppose the input is a two dimensional matrix x ∈ Rd×s which
can be viewed as d-dimensional vectors in s channels  then a convolutional layer on top of x is
deﬁned as follows. There are d(cid:48) ﬁxed subsets {S1  S2  . . .   Sd(cid:48)} of [d] each of size k(cid:48). The output
of the convolution layer is a matrix of size d(cid:48) × m  whose (i  j)-th entry is φ((cid:104)wj  xSi(cid:105))  where
xSi ∈ Rk(cid:48)×s is the submatrix of x with rows indexed by Si; wj ∈ Rk(cid:48)×s is the weight matrix of the
j-th channel; and φ is the activation function. Overparameterization then means a larger number of
channels m in our learned network comparing to the target. Our analysis can be adapted to show a
similar result for this type of networks.
One can also combine this paper with properties of recurrent neural networks (RNNs) [3] to derive
PAC-learning results for RNNs [2]  or use the existential tools of this paper to derive PAC-learning
results for three-layer residual networks (ResNet) [1]. The latter gives a provable separation between
neural networks and kernels in the efﬁcient PAC-learning regime.

Acknowledgements
This work was supported in part by FA9550-18-1-0166. Y. Liang would also like to acknowl-
edge that support for this research was provided by the Ofﬁce of the Vice Chancellor for Research
and Graduate Education at the University of Wisconsin-Madison with funding from the Wisconsin
Alumni Research Foundation.

12For instance  the number of sign changes can be m0.999 for the second hidden layer (see Lemma 6.5).
In this region  the network output can be affected by m0.499 since each neuron is of value roughly m−1/2.
Therefore  if after training we replace the sign pattern with random initialization  the output will be meaningless.

9

References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently  Going Beyond Ker-

nels? In NeurIPS  2019. Full version available at http://arxiv.org/abs/1905.10337.

[2] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD Learn Recurrent Neural Networks with Provable
Generalization? In NeurIPS  2019. Full version available at http://arxiv.org/abs/1902.
01028.

[3] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. On the convergence rate of training recurrent
neural networks. In NeurIPS  2019. Full version available at http://arxiv.org/abs/1810.
12065.

[4] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML  2019. Full version available at http://arxiv.org/abs/
1811.03962.

[5] Alexandr Andoni  Rina Panigrahy  Gregory Valiant  and Li Zhang. Learning polynomials with
neural networks. In International Conference on Machine Learning  pages 1908–1916  2014.

[6] Sanjeev Arora  Rong Ge  Behnam Neyshabur  and Yi Zhang. Stronger generalization bounds

for deep nets via a compression approach. arXiv preprint arXiv:1802.05296  2018.

[7] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  Ruslan Salakhutdinov  and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 
2019.

[8] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584  2019.

[9] Ainesh Bakshi  Rajesh Jayaram  and David P Woodruff. Learning two layer rectiﬁed neural

networks in polynomial time. arXiv preprint arXiv:1811.01885  2018.

[10] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[11] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems  pages 6241–6250 
2017.

[12] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer

neural network. arXiv preprint arXiv:1710.11241  2017.

[13] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with

gaussian inputs. arXiv preprint arXiv:1702.07966  2017.

[14] Alon Brutzkus  Amir Globerson  Eran Malach  and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations  2018.

[15] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural

Information Processing Systems  pages 2422–2430  2017.

[16] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural net-
works: The power of initialization and a dual view on expressivity. In Advances in Neural
Information Processing Systems (NIPS)  pages 2253–2261  2016.

[17] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably opti-

mizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054  2018.

[18] Ronen Eldan  Dan Mikulincer  and Alex Zhai. The clt in high dimensions: quantitative bounds

via martingale embedding. arXiv preprint arXiv:1806.09087  2018.

10

[19] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle pointsonline stochas-
In Conference on Learning Theory  pages 797–842 

tic gradient for tensor decomposition.
2015.

[20] Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with

landscape design. arXiv preprint arXiv:1711.00501  2017.

[21] Rong Ge  Rohith Kuditipudi  Zhize Li  and Xiang Wang. Learning two-layer neural networks

with symmetric inputs. In International Conference on Learning Representations  2019.

[22] Surbhi Goel and Adam Klivans. Learning neural networks with two nonlinear layers in poly-

nomial time. arXiv preprint arXiv:1709.06010v4  2018.

[23] Surbhi Goel  Varun Kanade  Adam Klivans  and Justin Thaler. Reliably learning the relu in

polynomial time. In Conference on Learning Theory  pages 1004–1042  2017.

[24] Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity

of neural networks. In Proceedings of the Conference on Learning Theory  2018.

[25] Alex Graves  Abdel-rahman Mohamed  and Geoffrey Hinton. Speech recognition with deep
In Acoustics  speech and signal processing (icassp)  2013 ieee

recurrent neural networks.
international conference on  pages 6645–6649. IEEE  2013.

[26] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion  pages 770–778  2016.

[27] Arthur Jacot  Franck Gabriel  and Cl´ement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks. In Advances in neural information processing systems 
pages 8571–8580  2018.

[28] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Informa-

tion Processing Systems  pages 586–594  2016.

[29] Robert Kleinberg  Yuanzhi Li  and Yang Yuan. An alternative view: When does sgd escape

local minima? arXiv preprint arXiv:1802.06175  2018.

[30] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[31] Jaehoon Lee  Lechao Xiao  Samuel S Schoenholz  Yasaman Bahri  Jascha Sohl-Dickstein  and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720  2019.

[32] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems 
2018.

[33] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu

activation. In Advances in Neural Information Processing Systems  pages 597–607  2017.

[34] Yuanzhi Li  Tengyu Ma  and Hongyang Zhang.

Algorithmic regularization in over-

parameterized matrix recovery. arXiv preprint arXiv:1712.09203  2017.

[35] Percy Liang. CS229T/STAT231: Statistical Learning Theory (Winter 2016). https://web.

stanford.edu/class/cs229t/notes.pdf  April 2016. accessed January 2019.

[36] Roi Livni  Shai Shalev-Shwartz  and Ohad Shamir. On the computational efﬁciency of training
In Advances in Neural Information Processing Systems  pages 855–863 

neural networks.
2014.

[37] Martin J. Wainwright. Basic tail and concentration bounds. https://www.stat.berkeley.
edu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf  2015. Online; accessed
Oct 2018.

11

[38] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International

Conference on Algorithmic Learning Theory  pages 3–17. Springer  2016.

[39] Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564  2017.

[40] Ohad Shamir. Distribution-speciﬁc hardness of learning neural networks. Journal of Machine

Learning Research  19(32)  2018.

[41] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van
Den Driessche  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanc-
tot  et al. Mastering the game of go with deep neural networks and tree search. nature  529
(7587):484  2016.

[42] Mahdi Soltanolkotabi  Adel Javanmard  and Jason D Lee. Theoretical insights into the
arXiv preprint

optimization landscape of over-parameterized shallow neural networks.
arXiv:1707.04926  2017.

[43] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guar-

antees for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

[44] Daniel Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. In Proceedings of the thirty-third annual ACM sym-
posium on Theory of computing  pages 296–305. ACM  2001.

[45] Karthik Sridharan. Machine Learning Theory (CS 6783). http://www.cs.cornell.edu/

courses/cs6783/2014fa/lec7.pdf  2014. accessed January 2019.

[46] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhut-
dinov. Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research  15(1):1929–1958  2014.

[47] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and
its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560 
2017.

[48] Leslie Valiant. A theory of the learnable. Communications of the ACM  1984.

[49] Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training

one-hidden-layer neural networks. arXiv preprint arXiv:1805.02677  2018.

[50] Bo Xie  Yingyu Liang  and Le Song. Diversity leads to generalization in neural networks.

arXiv preprint Arxiv:1611.03131  2016.

[51] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-
cess behavior  gradient independence  and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760  2019.

[52] Alex Zhai. A high-dimensional CLT in W2 distance with near optimal convergence rate. Prob-

ability Theory and Related Fields  170(3-4):821–845  2018.

[53] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understand-

ing deep learning requires rethinking generalization. In ICLR  2017. arXiv 1611.03530.

[54] Yuchen Zhang  Jason D Lee  and Michael I Jordan. l1-regularized neural networks are improp-
erly learnable in polynomial time. In International Conference on Machine Learning  pages
993–1001  2016.

[55] Yuchen Zhang  Jason Lee  Martin Wainwright  and Michael Jordan. On the learnability of
fully-connected neural networks. In Artiﬁcial Intelligence and Statistics  pages 83–91  2017.

[56] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery guar-

antees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175  2017.

12

,Søren Dahlgaard
Mathias Knudsen
Mikkel Thorup
Zeyuan Allen-Zhu
Yuanzhi Li
Yingyu Liang