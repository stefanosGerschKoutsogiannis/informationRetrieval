2019,Are deep ResNets provably better than linear predictors?,Recent results in the literature indicate that a residual network (ResNet) composed of a single residual block outperforms linear predictors  in the sense that all local minima in its optimization landscape are at least as good as the best linear predictor. However  these results are limited to a single residual block (i.e.  shallow ResNets)  instead of the deep ResNets composed of multiple residual blocks. We take a step towards extending this result to deep ResNets. We start by two motivating examples. First  we show that there exist datasets for which all local minima of a fully-connected ReLU network are no better than the best linear predictor  whereas a ResNet has strictly better local minima. Second  we show that even at the global minimum  the representation obtained from the residual block outputs of a 2-block ResNet do not necessarily improve monotonically over subsequent blocks  which highlights a fundamental difficulty in analyzing deep ResNets. Our main theorem on deep ResNets shows under simple geometric conditions that  any critical point in the optimization landscape is either (i) at least as good as the best linear predictor; or (ii) the Hessian at this critical point has a strictly negative eigenvalue. Notably  our theorem shows that a chain of multiple skip-connections can improve the optimization landscape  whereas existing results study direct skip-connections to the last hidden layer or output layer. Finally  we complement our results by showing benign properties of the "near-identity regions" of deep ResNets  showing depth-independent upper bounds for the risk attained at critical points as well as the Rademacher complexity.,Are deep ResNets provably
better than linear predictors?

Chulhee Yun

MIT

Cambridge  MA 02139
chulheey@mit.edu

Suvrit Sra

MIT

Cambridge  MA 02139

suvrit@mit.edu

Ali Jadbabaie

MIT

Cambridge  MA 02139
jadbabai@mit.edu

Abstract

Recent results in the literature indicate that a residual network (ResNet) composed
of a single residual block outperforms linear predictors  in the sense that all local
minima in its optimization landscape are at least as good as the best linear predictor.
However  these results are limited to a single residual block (i.e.  shallow ResNets) 
instead of the deep ResNets composed of multiple residual blocks. We take a
step towards extending this result to deep ResNets. We start by two motivating
examples. First  we show that there exist datasets for which all local minima of a
fully-connected ReLU network are no better than the best linear predictor  whereas
a ResNet has strictly better local minima. Second  we show that even at the global
minimum  the representation obtained from the residual block outputs of a 2-block
ResNet do not necessarily improve monotonically over subsequent blocks  which
highlights a fundamental difﬁculty in analyzing deep ResNets. Our main theorem
on deep ResNets shows under simple geometric conditions that  any critical point in
the optimization landscape is either (i) at least as good as the best linear predictor;
or (ii) the Hessian at this critical point has a strictly negative eigenvalue. Notably 
our theorem shows that a chain of multiple skip-connections can improve the
optimization landscape  whereas existing results study direct skip-connections
to the last hidden layer or output layer. Finally  we complement our results by
showing benign properties of the “near-identity regions” of deep ResNets  showing
depth-independent upper bounds for the risk attained at critical points as well as
the Rademacher complexity.

1

Introduction

Empirical success of deep neural network models has sparked a huge interest in the theory of deep
learning  but a concrete theoretical understanding of deep learning still remains elusive. From the
optimization point of view  the biggest mystery is why gradient-based methods ﬁnd close-to-global
solutions despite nonconvexity of the empirical risk.
There have been several attempts to explain this phenomenon by studying the loss surface of the risk.
The idea is to ﬁnd benign properties of the empirical or population risk that make optimization easier.
So far  the theoretical investigation as been mostly focused on vanilla fully-connected neural networks
[1  8  10  11  18  20  22–29  31]. For example  Kawaguchi [8] proved that “local minima are global
minima” property holds for squared error empirical risk of linear neural networks (i.e.  no nonlinear
activation function at hidden nodes). Other results on deep linear neural networks [10  27  29  31]
have extended [8]. However  it was later theoretically and empirically shown that “local minima are
global minima” property no longer holds in nonlinear neural networks [20  29] for general datasets
and activations.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Moving beyond fully-connected networks  there is an increasing body of analysis dedicated to
studying residual networks (ResNets). A ResNet [6  7] is a special type of neural network that gained
widespread popularity in practice. While fully-connected neural networks or convolutional neural
networks can be viewed as a composition of nonlinear layers x (cid:55)→ Φ(x)  a ResNet consists of a
series of residual blocks of the form x (cid:55)→ g(x + Φ(x))  where Φ(x) is some feedforward neural
network and g(·) is usually taken to be identity [7]. Given these identity skip-connections  the output
of a residual block is a feedforward network Φ(x) plus the input x itself  which is different from
fully-connected neural networks. The motivation for this architecture is to let the network learn only
the residual of the input.
ResNets are very popular in practice  and it has been argued that they have benign loss landscapes
that make optimization easier [12]. Recently  Shamir [21] showed that ResNets composed of a single
residual block have “good” local minima  in the sense that any local minimum in the loss surface
attains a risk value at least as good as the one attained by the best linear predictor. A subsequent
result [9] extended this result to non-scalar outputs  with weaker assumptions on the loss function.
However  these existing results are limited to a single residual block  instead of deep ResNets formed
by composing multiple residual blocks. In light of these results  a natural question arises: can these
single-block results be extended to multi-block ResNets?
There are also another line of works that consider network architectures with “skip-connections.”
Liang et al. [13  14] consider networks of the form x (cid:55)→ fS(x) + fD(x) where fS(x) is a “shortcut”
network with one or a few hidden nodes  and they show that under some conditions this shortcut
network eliminates spurious local minima. Nguyen et al. [19] consider skip-connections from hidden
nodes to the output layer  and show that if the number of skip-connections to output layer is greater
than or equal to the dataset size  the loss landscape has no spurious local valleys. However  skip-
connections in these results are all connections directly to output  so it remains unclear whether a
chain of multiple skip-connections can improve the loss landscape.
There is also another line of theoretical results studying what happens in the near-identity regions of
ResNets  i.e.  when the residual part Φ is “small” for all layers. Hardt and Ma [5] proved that for linear
ResNets x (cid:55)→ (I + AL)··· (I + A1)x  any critical point in the region {(cid:107)Al(cid:107) < 1 for all l} is a global
minimum. The authors also proved that any matrix R with positive determinant can be decomposed
into products of I + Al  where (cid:107)Al(cid:107) = O(1/L). Bartlett et al. [3] extended this result to nonlinear
function space  and showed similar expressive power and optimization properties of near-identity
regions; however  their results are on function spaces  so they don’t imply that the same properties
hold for parameter spaces. In addition  an empirical work by Zhang et al. [30] showed that initializing
ResNets in near-identity regions also leads to good empirical performance. For the residual part
Φ of each block  they initialize the last layer of Φ at zero  and scale the initialization of the other
layers by a factor inversely proportional to depth L. This means that each Φ at initialization is zero 
hence the network starts in the near-identity region. Their experiments demonstrate that ResNets
can be stably trained without batch normalization  and trained networks match the generalization
performance of the state-of-the-art models. These results thus suggest that understanding optimization
and generalization of ResNets in near-identity regions is a meaningful and important question.

1.1 Summary of contributions

This paper takes a step towards answering the questions above. In Section 3  we start with two
motivating examples showing the advantage of ResNets and the difﬁculty of deep ResNet analysis:
(cid:73) The ﬁrst example shows that there exists a family of datasets on which the squared error loss
attained by a fully-connected neural network is at best the linear least squares model  whereas a
ResNet attains a strictly better loss than the linear model. This highlights that the guarantee on
the risk value of local minima is indeed special to residual networks.

(cid:73) In the single-block case [21]  we have seen that the “representation” obtained at the residual block
output x + Φ(x) has an improved linear ﬁt compared to the raw input x. Then  in multi-block
ResNets  do the representations at residual block outputs improve monotonically over subsequent
blocks as we proceed to the output layer? The second example shows that it is not necessarily the
case; we give an example where the linear ﬁt with representations by the output of residual blocks
does not monotonically improve over blocks. This highlights the difﬁculty of ResNet analysis 
and shows that [21] cannot be directly extended to multi-block ResNets.

2

Using new techniques  Section 4 extends the results in [21] to deeper ResNets  under some simple
geometric conditions on the parameters.
(cid:73) We consider a deep ResNet model that subsumes [21] as a special case  under the same assump-
tions on the loss function. We prove that if two geometric conditions called “representation
coverage” and “parameter coverage” are satisﬁed  then a critical point of the loss surface satisﬁes
at least one of the following: 1) the risk value is no greater than the best linear predictor  2) the
Hessian at the critical point has a strictly negative eigenvalue. We also provide an architectural
sufﬁcient condition for the parameter coverage condition to hold.

Finally  Section 5 shows benign properties of deep ResNets in the near-identity regions  in both
optimization and generalization aspects. Speciﬁcally 
(cid:73) In the absence of the geometric conditions above  we prove an upper bound on the risk values at
critical points. The upper bound shows that if each residual block is close to identity  then the risk
values at its critical points are not too far from the risk value of the best linear model. Crucially 
we establish that the distortion over the linear model is independent of network size  as long as
each blocks are near-identity.

(cid:73) We provide an upper bound on the Rademacher complexity of deep ResNets. Again  we observe
that in the near-identity region  the upper bound is independent of network size  which is difﬁcult
to achieve for fully-connected networks [4].

2 Preliminaries

In this section  we brieﬂy introduce the ResNet architecture and summarize our notation.
Given positive integers a and b  where a < b  [a] denotes the set {1  2  . . .   a} and [a : b] denote
{a  a + 1  . . .   b − 1  b}. Given a vector x  (cid:107)x(cid:107) denotes its Euclidean norm. For a matrix M  by
(cid:107)M(cid:107) and (cid:107)M(cid:107)F we mean its spectral norm and Frobenius norm  respectively. Let λmin(M ) be the
minimum eigenvalue of a symmetric matrix M. Let col(M ) be the column space of a matrix M.
Let x ∈ Rdx be the input vector. We consider an L-block ResNet fθ(·) with a linear output layer:

h0(x) = x 
hl(x) = hl−1(x) + Φl
fθ(x) = wT hL(x).

θ(hl−1(x)) 

l = 1  . . .   L 

We use bold-cased symbols to denote network parameter vectors/matrices  and θ to denote the
collection of all parameters. As mentioned above  the output of l-th residual block is the input
hl−1(x) plus the output of the “residual part” Φl
θ(hl−1(x))  which is some feedforward neural
θ : Rdx (cid:55)→ Rdx considered will vary depending on the theorems.
network. The speciﬁc structure of Φl
After L such residual blocks  there is a linear fully-connected layer parametrized by w ∈ Rdx  and
the output of the ResNet is scalar-valued.
Using ResNets  we are interested in training the network under some distribution P of the input and
label pairs (x  y) ∼ P  with the goal of minimizing the loss (cid:96)(fθ(x); y). More concretely  the risk
function R(θ) we want to minimize is

R(θ) := E(x y)∼P [(cid:96)(fθ(x); y)]  

where (cid:96)(p; y) : R (cid:55)→ R is the loss function parametrized by y. If P is an empirical distribution by a
given set of training examples  this reduces to an empirical risk minimization problem. Let (cid:96)(cid:48)(·; y)
and (cid:96)(cid:48)(cid:48)(·; y) be ﬁrst and second derivatives of (cid:96)  whenever they exist.
We will state our results by comparing against the risk achieved by linear predictors. Thus  let Rlin
be the risk value achieved by the best linear predictor:

E(x y)∼P(cid:2)(cid:96)(tT x; y)(cid:3) .

Rlin := inf
t∈Rdx

3 Motivating examples

Before presenting the main theoretical results  we present two motivating examples. The ﬁrst one
shows the advantage of ResNets over fully-connected networks  and the next one highlights that deep
ResNets are difﬁcult to analyze and techniques from previous works cannot be directly applied.

3

Table 1: Lower bounds on R1(θ

∗
1)  if w∗

1 > 0

−b∗
1/w∗
(−∞  0)
[0  1)
[1  2)
[2  3)
[3  4)
[4  5)
[5 ∞)

1 in: Error by constant part Error by linear part Lower bound

0
0
1/12
4ρ2/9 + 2ρ/3 + 1/3
ρ2/2 + ρ/3 + 5/6
4ρ2/5 + 4ρ/3 + 5/3
ρ2 + 7ρ/3 + 35/12

8ρ2/15
8ρ2/15
7ρ2/15
ρ2/9
0
0
0

8ρ2/15
8ρ2/15
7ρ2/15 + 1/12
5ρ2/9 + 2ρ/3 + 1/3
ρ2/2 + ρ/3 + 5/6
4ρ2/5 + 4ρ/3 + 5/3
ρ2 + 7ρ/3 + 35/12

6(cid:88)

i=1

1
6

6(cid:88)

i=1

1
6

3.1 All local minima of fully-connected networks can be worse than a linear predictor

Although it is known that local minima of 1-block ResNets are at least as good as linear predictors 
can this property hold also for fully-connected networks? Can a local minimum of a fully-connected
network be strictly worse than a linear predictor? In fact  we present a simple example where all local
minima of a fully-connected network are at best as good as linear models  while a residual network
has strictly better local minima.
Consider the following dataset with six data points  where ρ > 0 is a ﬁxed constant:

X = [0 1

2

3 4 5]  

Y = [−ρ 1 − ρ 2 + ρ 3 − ρ 4 + ρ 5 + ρ] .

Let xi and yi be the i-th entry of X and Y   respectively. We consider two different neural networks:
f1(x; θ1) is a fully-connected network parametrized by θ1 = (w1  w2  b1  b2)  and f2(x; θ2) is a
ResNet parametrized by θ2 = (w  v  u  b  c)  deﬁned as

f1(x; θ1) = w2σ(w1x + b1) + b2 

f2(x; θ2) = w(x + vσ(ux + b)) + c 
where σ(t) = max{t  0} is ReLU activation. In this example  all parameters are scalars.
With these networks  our goal is to ﬁt the dataset under squared error loss. The empirical risk
functions we want to minimize are given by

R1(θ1) :=

∗
2) < Rlin.

(w2σ(w1xi + b1) + b2 − yi)2  R2(θ2) :=

1) ≥ Rlin  whereas there exists a local minimum θ
∗

Proposition 1. Consider the dataset X and Y as above. If ρ ≤(cid:112)5/4  then any local minimum

(w(xi + vσ(uxi + b)) + c− yi)2 
respectively. It is easy to check that the best empirical risk achieved by linear models x (cid:55)→ wx + b is
Rlin = 8ρ2/15. It follows from [21] that all local minima of R2(·) have risk values at most Rlin. For
this particular example  we show that the opposite holds for the fully-connected network  whereas for
the ResNet there exists a local minimum strictly better than Rlin.
1 of R1(·) satisﬁes R1(θ
∗
θ
R2(θ
Proof
The function f1(x; θ1) is piece-wise continuous  and consists of two pieces (unless w1 = 0
or w2 = 0). If w1 > 0  the function is linear for x ≥ −b1/w1 and constant for x ≤ −b1/w1. For any
∗
local minimum θ
1) is bounded from below by the risk achieved by ﬁtting
the linear piece and constant piece separately  without the restriction of continuity. This is because
we are removing the constraint that the function f1(·) has to be continuous.
∗
For example  if w∗
1 = 1.5  then its empirical risk R1(θ
1) is at least the error
attained by the best constant ﬁt of (x1  y1)  (x2  y2)  and the best linear ﬁt of (x3  y3)  . . .   (x6  y6).
For all possible values of −b∗
∗
1). It is easy
1 < 0
∗
can be proved similarly  and the case w∗
1) is a
constant function.
For the ResNet part  it sufﬁces to show that there is a point θ2 such that R2(θ2) < 8ρ2/15 
because then its global minimum will be strictly smaller than 8ρ2/15. Choose v = 0.5ρ 

to check that if ρ ≤(cid:112)5/4  all the lower bounds are no less than 8ρ2/15. The case where w∗

1/w∗
1  we summarize in Table 1 the lower bounds on R1(θ

1 > 0 and −b∗
1/w∗

1 = 0 is trivially worse than 8ρ2/15 because f1(x; θ

∗
1  the empirical risk R1(θ

2 of R2(·) such that
∗

4

2

u = 1  and b = −3. Given input X  the output of the residual block x (cid:55)→ x + vσ(ux + b) is
3 4 + 0.5ρ 5 + ρ] =: H. Using this  we choose w and c that linearly ﬁt H and Y .
[0 1
Using the optimal w and c  a straightforward calculation gives R2(θ2) = ρ2(12ρ2+82ρ+215)
21ρ2+156ρ+420   and it is

strictly smaller than 8ρ2/15 on ρ ∈ (0 (cid:112)5/4].

3.2 Representations by residual block outputs do not improve monotonically

Consider a 1-block ResNet. Given a dataset X and Y   the residual block transforms X into H  where
H is the collection of outputs of the residual block. Let err(X  Y ) be the minimum mean squared
error from ﬁtting X and Y with a linear least squares model. The result that a local minimum of a
1-block ResNet is better than a linear predictor can be stated in other words: the output of the residual
block produces a “better representation” of the data  so that err(H  Y ) ≤ err(X  Y ).
For a local minimum of a L-layer ResNet  our goal is to prove that err(HL  Y ) ≤ err(X  Y )  where
Hl  l ∈ [L] is the collection of output of l-th residual block. Seeing the improvement of representation
in 1-block case  it is tempting to conjecture that each residual block monotonically improves the
representation  i.e.  err(HL  Y ) ≤ err(HL−1  Y ) ≤ ··· ≤ err(H1  Y ) ≤ err(X  Y ). Our next
example shows that this monotonicity does not necessarily hold.
Consider a dataset X = [1

2]  and a 2-block ResNet

2.5 3] and Y = [1

3

h1(x) = x + v1σ(u1x + b1) 

h2(x) = h1(x) + v2σ(u2h1(x) + b2) 

f (x) = wh2(x) + c 

where σ denotes ReLU activation. We choose

v1 = 1  u1 = 1  b1 = −2  v2 = −4  u2 = 1  b2 = −3.5  w = 1  c = 0.

3

3

4] and H2 = [1

With these parameter values  we have H1 = [1
2]. It is evident that the
network output perfectly ﬁts the dataset  and err(H2  Y ) = 0. Indeed  the chosen set of parameters
is a global minimum of the squared loss empirical risk. Also  by a straightforward calculation
we get err(X  Y ) = 0.3205 and err(H1  Y ) = 0.3810  so err(H1  Y ) > err(X  Y ). This shows
that the conjecture err(H2  Y ) ≤ err(H1  Y ) ≤ err(X  Y ) is not true  and it also implies that an
induction-type approach showing err(H2  Y ) ≤ err(H1  Y ) and then err(H1  Y ) ≤ err(X  Y ) will
never be able to prove err(H2  Y ) ≤ err(X  Y ).
In fact  application of the proof techniques in [21] only shows that err(H2  Y ) ≤ err(H1  Y )  so
a comparison of err(H2  Y ) and err(X  Y ) does not follow. Further  our example shows that even
err(H1  Y ) > err(X  Y ) is possible  showing that theoretically proving err(H2  Y ) ≤ err(X  Y ) is
challenging even for L = 2. In the next section  we present results using new techniques to overcome
this difﬁculty and prove err(HL  Y ) ≤ err(X  Y ) under some geometric conditions.

4 Local minima of deep ResNets are better than linear predictors

Given the motivating examples  we now present our ﬁrst main result  which shows that under certain
geometric conditions  each critical point of ResNets has benign properties: either (i) it is as good as
the best linear predictor; or (ii) it is a strict saddle point.

4.1 Problem setup

We consider an L-block ResNet whose residual parts Φl
θ(t) = V lφl

θ(t) = V 1φ1

z(t)  and Φl

θ(·) are deﬁned as follows:
l = 2  . . .   L.
z(U lt) 

Φ1

z : Rml →
We collect all parameters into θ := (w  V 1  V 2  U 2  . . .   V L  U L  z). The functions φl
Rnl denote any arbitrary function parametrized by z that are differentiable almost everywhere. They
could be fully-connected ReLU networks  convolutional neural networks  or any combination of
such feed-forward architectures. We even allow different φl
z’s to share parameters in z. Note that
m1 = dx by the deﬁnition of the architecture. The matrices U l ∈ Rml×dx and V l ∈ Rdx×nl form
linear fully-connected layers. Note that if L = 1  the network boils down to x (cid:55)→ wT (x + V 1φ1
z(x)) 
which is exactly the architecture considered by Shamir [21]; we are considering a deeper extension of
the previous paper.
For this section  we make the following mild assumption on the loss function:

5

Assumption 4.1. The loss function (cid:96)(p; y) is a convex and twice differentiable function of p.

This assumption is the same as the one in [21]. It is satisﬁed by standard losses such as square error
loss and logistic loss.

4.2 Theorem statement and discussion

We now present our main theorem on ResNets. Theorem 2 outlines two geometric conditions under
which it shows that the critical points of deep ResNets have benign properties.
Theorem 2. Suppose Assumption 4.1 holds. Let
1  V ∗

:= (w∗  V ∗

L  z∗)

L  U∗

θ

∗

2  U∗
be any twice-differentiable critical point of R(·). If

• E(x y)∼P(cid:2)(cid:96)(cid:48)(cid:48)(fθ∗ (x); y)hL(x)hL(x)T(cid:3) is full-rank; and
• col(cid:0)(cid:2)(U∗

L)T(cid:3)(cid:1) (cid:40) Rdx 

2  . . .   V ∗

(U∗

···

2)T

then at least one of the following inequalities holds:

∗

• R(θ
) ≤ Rlin.
• λmin(∇2R(θ
∗

)) < 0.

∗

The proof of Theorem 2 is deferred to Appendix A. Theorem 2 shows that if the two geometric and
linear-algebraic conditions hold  then the risk function value for fθ∗ is at least as good as the best
∗ so that it is easy to escape
linear predictor  or there is a strict negative eigenvalue of the Hessian at θ
from this saddle point. A direct implication of these conditions is that if they continue to hold over the
optimization process  then with curvature sensitive algorithms we can ﬁnd a local minimum no worse
than the best linear predictor; notice that our result holds for general losses and data distributions.
As noted earlier  if L = 1  our ResNet reduces down to the one considered in [21]. In this case 
the second condition is always satisﬁed because it does not involve the ﬁrst residual block. In fact 
our proof reveals that in the L = 1 case  any critical point with w∗ (cid:54)= 0 satisﬁes R(θ
) ≤ Rlin
even without the ﬁrst condition  which recovers the key implication of [21  Theorem 1]. We again
emphasize that Theorem 2 extends the previous result.
Theorem 2 also implies something noteworthy about the role of skip-connections in general. Existing
results featuring beneﬁcial impacts of skip-connections or parellel shortcut networks on optimization
landscapes require direct connection to output [13  14  19] or the last hidden layer [21]. The
multi-block ResNet we consider in our paper is fundamentally different from other works; the skip-
connections connect input to output through a chain of multiple skip-connections. Our paper proves
that multiple skip-connections (as opposed to direct) can also improve the optimization landscape of
neural networks  as was observed empirically [12].
We now discuss the conditions. We call the ﬁrst condition the representation coverage condition 
because it requires that the representation hL(x) by the last residual block “covers” the full space

Rdx so that E(x y)∼P(cid:2)(cid:96)(cid:48)(cid:48)(fθ(x); y)hL(x)hL(x)T(cid:3) is full rank. Especially in cases where (cid:96) is strictly

2  . . .   U∗

convex  this condition is very mild and likely to hold in most cases.
The second condition is the parameter coverage condition. It requires that the subspace spanned by the
rows of U∗
L is not the full space Rdx. This condition means that the parameters U∗
2  . . .   U∗
L
do not cover the full feature space Rdx  so there is some information in the data/representation that
this network “misses ” which enables us to easily ﬁnd a direction to improve the parameters.
These conditions stipulate that if the data representation is “rich” enough but the parameters do not
cover the full space  then there is always a sufﬁcient room for improvement. We also note that there
l=2 ml < dx for our parameter coverage condition to always

is an architectural sufﬁcient condition(cid:80)L
Corollary 3. Suppose Assumption 4.1 holds. For a ResNet fθ(·) that satisﬁes(cid:80)L
E(x y)∼P(cid:2)(cid:96)(cid:48)(cid:48)(fθ∗ (x); y)hL(x)hL(x)T(cid:3) is full-rank.

∗
l=2 ml < dx  let θ
be a twice-differentiable critical point of R(·). Then  the conclusion of Theorem 2 holds as long as

hold  which yields the following noteworthy corollary:

6

Example. Consider a deep ResNet with very simple residual blocks: h (cid:55)→ h + vlσ(uT
l h)  where
vl  ul ∈ Rdx are vectors and σ is ReLU activation. Even this simple architecture is a universal
approximator [15]. Notice that Corollary 3 applies to this architecture as long as the depth L ≤ dx.
The reader may be wondering what happens if the coverage conditions are not satisﬁed. In particular 

if the parameter coverage condition is not satisﬁed  i.e.  col(cid:0)(cid:2)(U∗

L)T(cid:3)(cid:1) = Rdx  we

conjecture that since the parameters already cover the full feature space  the critical point should be of
“good” quality. However  we leave a weakening/removal of our geometric conditions to future work.

(U∗

···

2)T

5 Benign properties in near-identity regions of ResNets

This section studies near-identity regions in optimization and generalization aspects  and shows
interesting bounds that hold in near-identity regions. We ﬁrst show an upper bound on the risk value
at critical points  and show that the bound is Rlin plus a size-independent (i.e.  independent of depth
and width) constant if the Lipschitz constants of Φl
θ’s satisfy O(1/L). We then prove a Rademacher
complexity bound on ResNets  and show that the bound also becomes size-independent if Φl
θ is
O(1/L)-Lipschitz.

5.1 Upper bound on the risk value at critical points

Even without the geometric conditions in Section 4  can we prove an upper bound on the risk value
of critical points? We prove that for general architectures  the risk value of critical points can be
bounded above by Rlin plus an additive term. Surprisingly  if each residual block is close to identity 
this additive term becomes depth-independent.
θ(·) of ResNet can have any general feedforward architecture:
In this subsection  the residual parts Φl

Φl

θ(t) = φl

z(t) 

l = 1  . . .   L.

z(0) = 0.

z : Rdx (cid:55)→ Rdx:

z is ρl-Lipschitz  and φl

z(t) = V lσ(U lt)  where σ is ReLU activation. In this case 

The collection of all parameters is simply θ := (w  z). We make the following assumption on the
functions φl
Assumption 5.1. For any l ∈ [L]  the residual part φl
For example  this assumption holds for φl
ρl depends on the spectral norm of V l and U l.
We also make the following assumption on the loss function (cid:96):
Assumption 5.2. The loss function (cid:96)(p; y) is a convex differentiable function of p. We also assume
that (cid:96)(p; y) is µ-Lipschitz;  i.e.  |(cid:96)(cid:48)(p; y)| ≤ µ for all p.
Under these assumptions  we prove a bound on the risk value attained at critical points of ResNets.
∗ be any critical point of R(·). Let ˆt ∈ Rdx
Theorem 4. Suppose Assumptions 5.1 and 5.2 hold. Let θ

be any vector that attains the best linear ﬁt  i.e.  Rlin = E(x y)∼P(cid:2)(cid:96)(ˆtT x; y)(cid:3). Then  for any critical

(1 + ρl) − 1(cid:1)E(x y)∼P [(cid:107)x(cid:107)].
the bound could be way above Rlin. However  if ρl = O(1/L)  the term(cid:81)L
if ρl = o(1/L)  the term(cid:81)L

The proof can be found in Appendix B. Theorem 4 provides an upper bound on R(θ
) for critical
points  without any conditions as in Theorem 2. Of course  depending on the values of constants 
l=1(1 + ρl) is bounded
above by a constant  so the additive term in the upper bound becomes size-independent. Furthermore 
l=1(1 + ρl) → 1 as L → ∞  so the additive term in the upper bound
diminishes to zero as the network gets deeper. This result indicates that the near-identity region has a
good optimization landscape property that any critical point has a risk value that is not too far off
from Rlin.

) ≤ Rlin + µ(cid:107)ˆt(cid:107)(cid:0)(cid:89)L

∗ of R(·) 

point θ

∗

R(θ

l=1

∗

5.2 Radamacher complexity of ResNets

In this subsection  we consider ResNets with the following residual part:

Φl

θ(t) = V lσ(U lt) 

l = 1  . . .   L 

7

where σ is ReLU activation  V l ∈ Rdx×dl   U l ∈ Rdl×dx. For this architecture  we prove an upper
bound on empirical Rademacher complexity that is size-independent in the near-identity region.
Given a set S = (x1  . . .   xn) of n samples  and a class F of real-valued functions deﬁned on X  
the empirical Rademacher complexity or Rademacher averages of F restricted to S (denoted as
F|S) is deﬁned as

(cid:34)

(cid:98)Rn(F|S) = E1:n

n(cid:88)

i=1

1
n

sup
f∈F

(cid:35)

if (xi)

 

where i  i = 1  . . . n  are i.i.d. Rademacher random variables (i.e.  Bernoulli coin ﬂips with probabil-
ity 0.5 and outcome ±1).
We now state the main result  which proves an upper bound on the Rademacher averages of the class
of ResNet functions on a compact domain and norm-bounded parameters.
Theorem 5. Given a set S = (x1  . . .   xn)  suppose (cid:107)xi(cid:107) ≤ B for all i ∈ [n]. Deﬁne the function
class FL of L-block ResNet with parameter constraints as:

FL := {fθ : Rdx (cid:55)→ R | (cid:107)w(cid:107) ≤ 1  and (cid:107)V l(cid:107)F  (cid:107)U l(cid:107)F ≤ Ml for all l ∈ [L]}.

Then  the empirical Rademacher complexity satisﬁes

(cid:98)Rn(FL|S) ≤ B(cid:81)L

l=1(1 + 2M 2
l )

√

.

n

(cid:16)

B · 2L(cid:81)L

The proof of Theorem 5 is deferred to Appendix C. The proof technique used in Theorem 5 is
to “peel off” the blocks: we upper-bound the Rademacher complexity of a l-block ResNet with
(cid:17)
l . Consider a fully-connected network x (cid:55)→
that of a (l − 1)-block ResNet multiplied by 1 + 2M 2
W Lσ(W L−1 ··· σ(W 1x)··· )  where W l’s are weight matrices and σ is ReLU activation. The
same “peeling off” technique was used in [16]  which showed a bound of O
 
n
where Cl is the Frobenius norm bound of W l. As we can see  this bound has an exponential
dependence on depth L  which is difﬁcult to remove. Other results [2  17] reduced the dependence
down to polynomial  but it wasn’t until the work by Golowich et al. [4] that a size-independent bound
√
became known. However  their size-independent bound has worse dependence on n (O(1/n1/4))
than other bounds (O(1/
In contrast  Theorem 5 shows that for ResNets  the upper bound easily becomes size-independent as
long as Ml = O(1/
L)  which is surprising. Of course  for fully-connected networks  the upper
bound above can also be made size-independent by forcing Cl ≤ 1/2 for all l ∈ [L]. However  in
this case  the network becomes trivial  meaning that the output has to be very close to zero for any
input x. In case of ResNets  the difference is that the bound can be made size-independent even for
non-trivial networks.

√
l=1 Cl/

n)).

√

6 Conclusion

We investigated the question whether local minima of risk function of a deep ResNet are better than
linear predictors. We showed two motivating examples showing 1) the advantage of ResNets over
fully-connected networks  and 2) difﬁculty in analysis of deep ResNets. Then  we showed that under
geometric conditions  any critical point of the risk function of a deep ResNet has benign properties
that it is either better than linear predictors or the Hessian at the critical point has a strict negative
eigenvalue. We supplement the result by showing size-independent upper bounds on the risk value of
critical points as well as empirical Rademacher complexity for near-identity regions of deep ResNets.
We hope that this work becomes a stepping stone on deeper understanding of ResNets.

Acknowledgments

All the authors acknowledge support from DARPA Lagrange. Chulhee Yun also thanks Korea
Foundation for Advanced Studies for their support. Suvrit Sra also acknowledges support from an
NSF-CAREER grant and an Amazon Research Award.

8

References
[1] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from

examples without local minima. Neural networks  2(1):53–58  1989.

[2] P. L. Bartlett  D. J. Foster  and M. J. Telgarsky. Spectrally-normalized margin bounds for neural

networks. In Advances in Neural Information Processing Systems  pages 6240–6249  2017.

[3] P. L. Bartlett  S. N. Evans  and P. M. Long. Representing smooth functions as compositions
of near-identity functions with implications for deep network optimization. arXiv preprint
arXiv:1804.05012  2018.

[4] N. Golowich  A. Rakhlin  and O. Shamir. Size-independent sample complexity of neural

networks. arXiv preprint arXiv:1712.06541  2017.

[5] M. Hardt and T. Ma. Identity matters in deep learning. In International Conference on Learning

Representations  2017.

[6] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
770–778  2016.

[7] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. In European

Conference on Computer Vision  pages 630–645. Springer  2016.

[8] K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information

Processing Systems  pages 586–594  2016.

[9] K. Kawaguchi and Y. Bengio. Depth with nonlinearity creates no bad local minima in resnets.

arXiv preprint arXiv:1810.09038  2018.

[10] T. Laurent and J. Brecht. Deep linear networks with arbitrary loss: All local minima are global.

In International Conference on Machine Learning  pages 2908–2913  2018.

[11] T. Laurent and J. von Brecht. The multilinear structure of ReLU networks. arXiv preprint

arXiv:1712.10132  2017.

[12] H. Li  Z. Xu  G. Taylor  C. Studer  and T. Goldstein. Visualizing the loss landscape of neural

nets. In Advances in Neural Information Processing Systems  pages 6389–6399  2018.

[13] S. Liang  R. Sun  J. D. Lee  and R. Srikant. Adding one neuron can eliminate all bad local

minima. In Advances in Neural Information Processing Systems  pages 4355–4365  2018.

[14] S. Liang  R. Sun  Y. Li  and R. Srikant. Understanding the loss surface of neural networks for
binary classiﬁcation. In International Conference on Machine Learning  pages 2840–2849 
2018.

[15] H. Lin and S. Jegelka. ResNet with one-neuron hidden layers is a universal approximator. arXiv

preprint arXiv:1806.10909  2018.

[16] B. Neyshabur  R. Tomioka  and N. Srebro. Norm-based capacity control in neural networks. In

Conference on Learning Theory  pages 1376–1401  2015.

[17] B. Neyshabur  S. Bhojanapalli  D. McAllester  and N. Srebro. Exploring generalization in deep

learning. In Advances in Neural Information Processing Systems  pages 5947–5956  2017.

[18] Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proceedings of
the 34th International Conference on Machine Learning  volume 70  pages 2603–2612  2017.

[19] Q. Nguyen  M. C. Mukkamala  and M. Hein. On the loss landscape of a class of deep neural

networks with no bad local valleys. arXiv preprint arXiv:1809.10749  2018.

[20] I. Safran and O. Shamir. Spurious local minima are common in two-layer ReLU neural networks.

arXiv preprint arXiv:1712.08968  2017.

9

[21] O. Shamir. Are ResNets provably better than linear predictors? arXiv preprint arXiv:1804.06739 

2018.

[22] D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

[23] G. Swirszcz  W. M. Czarnecki  and R. Pascanu. Local minima in training of neural networks.

arXiv preprint arXiv:1611.06310  2016.

[24] C. Wu  J. Luo  and J. D. Lee. No spurious local minima in a two hidden unit ReLU network. In

International Conference on Learning Representations Workshop  2018.

[25] B. Xie  Y. Liang  and L. Song. Diverse neural network learns true target functions. arXiv

preprint arXiv:1611.03131  2016.

[26] X.-H. Yu and G.-A. Chen. On the local minima free condition of backpropagation learning.

IEEE Transactions on Neural Networks  6(5):1300–1303  1995.

[27] C. Yun  S. Sra  and A. Jadbabaie. Global optimality conditions for deep neural networks. In

International Conference on Learning Representations  2018.

[28] C. Yun  S. Sra  and A. Jadbabaie. Efﬁciently testing local optimality and escaping saddles for

ReLU networks. In International Conference on Learning Representations  2019.

[29] C. Yun  S. Sra  and A. Jadbabaie. Small nonlinearities in activation functions create bad local
minima in neural networks. In International Conference on Learning Representations  2019.

[30] H. Zhang  Y. N. Dauphin  and T. Ma. Fixup initialization: Residual learning without normaliza-

tion. In International Conference on Learning Representations (ICLR)  2019.

[31] Y. Zhou and Y. Liang. Critical points of neural networks: Analytical forms and landscape

properties. In International Conference on Learning Representations  2018.

10

,Chulhee Yun
Suvrit Sra
Ali Jadbabaie