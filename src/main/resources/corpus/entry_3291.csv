2018,Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames,Many applications of machine learning involve the analysis of large data frames -- matrices collecting heterogeneous measurements (binary  numerical  counts  etc.) across samples -- with missing values. Low-rank models  as studied by Udell et al. (2016)  are popular in this framework for tasks such as visualization  clustering and missing value imputation. Yet  available methods with statistical guarantees and efficient optimization do not allow explicit modeling of main additive effects such as row and column  or covariate effects. In this paper  we introduce a low-rank interaction and sparse additive effects (LORIS) model which combines matrix regression on a dictionary and low-rank design  to estimate main effects and interactions simultaneously. We provide statistical guarantees in the form of upper bounds on the estimation error of both components. Then  we introduce a mixed coordinate gradient descent (MCGD) method which provably converges sub-linearly to an optimal solution and is computationally efficient for large scale data sets. We show on simulated and survey data that the method has a clear advantage over current practices.,Low-rank Interaction with Sparse Additive Effects

Model for Large Data Frames

Geneviève Robin

Centre de Mathématiques Appliquées
École Polytechnique  XPOP  INRIA

91120 Palaiseau  France

genevieve.robin@polytechnique.edu

Hoi-To Wai

Department of SE&EM

The Chinese University of Hong Kong

Shatin  Hong Kong

htwai@se.cuhk.edu.hk

Julie Josse

Centre de Mathématiques Appliquées
École Polytechnique  XPOP  INRIA

91120 Palaiseau  France

julie.josse@polytechnique.edu

Olga Klopp

ESSEC Business School

CREST  ENSAE

95021 Cergy  France
klopp@essec.edu

Éric Moulines

Centre de Mathématiques Appliquées
École Polytechnique  XPOP  INRIA

91120 Palaiseau  France

eric.moulines@polytechnique.edu

Abstract

Many applications of machine learning involve the analysis of large data frames –
matrices collecting heterogeneous measurements (binary  numerical  counts  etc.)
across samples – with missing values. Low-rank models  as studied by Udell
et al. [27]  are popular in this framework for tasks such as visualization  clustering
and missing value imputation. Yet  available methods with statistical guarantees
and efﬁcient optimization do not allow explicit modeling of main additive effects
such as row and column  or covariate effects. In this paper  we introduce a low-
rank interaction and sparse additive effects (LORIS) model which combines
matrix regression on a dictionary and low-rank design  to estimate main effects
and interactions simultaneously. We provide statistical guarantees in the form of
upper bounds on the estimation error of both components. Then  we introduce a
mixed coordinate gradient descent (MCGD) method which provably converges
sub-linearly to an optimal solution and is computationally efﬁcient for large scale
data sets. We show on simulated and survey data that the method has a clear
advantage over current practices.

1

Introduction

Recently  a lot of effort has been devoted towards the efﬁcient analysis of large data frames  a term
coined by Udell et al. [27]. A data frame is a large table of heterogeneous data (binary  numerical 
counts) with missing entries  where each row represents an example and each column a feature. In
order to analyze them  a powerful technique is to use low-rank models that embed rows and columns
of data frames into low-dimensional spaces [15  25  27]  enabling effective data analytics such as
clustering  visualization and missing value imputation; see also [18] and the references therein.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Characterizing additive effects of side information – such as covariates  row or column effects –
simultaneously with low rank interactions is an important extension to plain low-rank models. For
example  in data frames obtained from recommender systems  user information and item characteris-
tics are known to inﬂuence the ratings in addition to interactions between users and items [7]. These
modiﬁcations to the low rank model have been advocated in the statistics literature  but they have
been implemented only for small data frames [1].
In the large-scale low-rank matrix estimation literature  available methods either do not take additive
effects into account [6  20  27  22  8]  or only handle the numerical data [12  11]. As a common
heuristics for preprocessing  prior work such as [20  27] remove the row and column means and
apply some normalization of the row and column variance. We show in numerical experiments this
apparently benign operation is not appropriate for large and heterogenous data frames  and can cause
severe impairments in the analysis.
The present work investigates a generalization of previous contributions in the analysis of data frames.
Our contributions can be summarized as follows.

Contributions We present a new framework that is statistically and computationally efﬁcient for
analyzing large and incomplete heterogeneous data frames.

• We describe in Section 2 the low-rank interaction with sparse additive effects (LORIS)
model  which combines matrix regression on a dictionary with low rank approximation. We
propose a convex doubly penalized quasi-maximum likelihood approach  where the rank
constraint is relaxed with a nuclear norm penalty  to estimate the regression coefﬁcients and
the low rank component simultaneously. We establish non-asymptotic upper bounds on the
estimation errors.

• We propose in Section 3 a Mixed Coordinate Gradient Descent (MCGD) method to solve
efﬁciently the LORIS estimation problem. It uses a mixed update strategy including a
proximal update for the sparse component and a conditional gradient (CG) for the low-rank
component. We show that the MCGD method converges to an -optimal solution in O(1/)
iterations. We also outline an extension to efﬁcient distributed implementation.

• We demonstrate in Section 4 the efﬁcacy of our method both in terms of estimation and

imputation quality on simulated and survey data examples.

Related work Our statistical model and analysis are related to prior work on low-rank plus sparse
matrix decomposition [28  3  4  13  17]; these papers provide statistical results for a particular case
where the loss function is quadratic and the sparse component is entry-wise sparse. In comparison 
the originality of the present work is two-fold. First  the sparsity pattern of the main effects is not
restricted to entry-wise sparsity. Second  the data ﬁtting term is not quadratic  but a heterogeneous
exponential family quasi log-likelihood. This new framework enables us to tackle many more data
sets combining heterogeneous data  main effects and interactions.
For the algorithmic development  our proposed method is related to the prior work such as [21  26  5 
11  29  14  24  9  19  2  10]. These are based on various ﬁrst-order optimization methods and shall be
reviewed in detail in Section 3. Among others  the MCGD method is mostly related to the recent
FW-T method by Mu et al. [24] that uses a mixed update rule to tackle a similar estimation problem.
There are two differences: ﬁrst  FW-T is focused on a quadratic loss which is a special case of the
statistical estimation problem that we analyze; second  the per-iteration complexity of MCGD is
lower as the update rules are simpler. Despite the simpliﬁcations  using a new proof technique  we
prove that the convergence rate of MCGD is strictly faster than FW-T.
Notations: For any m ∈ N  [m] := {1  ...  m}. The operator PΩ(·) : Rn×p → Rn×p is the
projection operator on the set of entries in Ω ⊂ [n] × [p]  and (·)+ : R → R+ is the projection
operator on the non-negative orthant (x)+ := max{0  x}. For matrices  we denote by (cid:107)·(cid:107)F the
Frobenius norm  (cid:107)·(cid:107)(cid:63) the nuclear norm  (cid:107)·(cid:107) the operator norm  and (cid:107)·(cid:107)∞ the entry-wise inﬁnity norm.
For vectors  we denote by (cid:107)·(cid:107)1 is the (cid:96)1-norm  (cid:107)·(cid:107)2 the Euclidean norm  (cid:107)·(cid:107)∞ the inﬁnity norm 
and (cid:107)·(cid:107)0 the number of non zero coefﬁcients. The binary operator (cid:104)X  Y (cid:105) denotes the Frobenius
inner product. A function f : Rq → R is said to be σ-smooth if f is continuously differentiable and
(cid:107)∇f (θ) − ∇f (θ(cid:48))(cid:107)2 ≤ σ(cid:107)θ − θ(cid:48)(cid:107)2 for all θ  θ(cid:48) ∈ Rq.

2

2 Problem Formulation

Heterogenous Data Model Let (Y  X) be a probability space equipped with a σ-ﬁnite measure µ.
The canonical exponential family distribution {Exph g(m)  m ∈ X} with base measure h : Y → R+ 
link function g : X → R  and scalar parameter  m ∈ X  has a density given by

fm(y) = h(y) exp (ym − g(m)) .

(1)

The exponential family is a ﬂexible framework to model different types of data. For example 
(Y = R  g(m) = m2σ2/2  h(y) = (2πσ2)−1/2 exp(−y2/2σ2)) yields a Gaussian distribution with
mean m and variance σ2 for numerical data; (Y = {0  1}  g(m) = log(1 + exp(m))  h(y) = 1)
yields a Bernoulli distribution with success probability 1/(1 + exp(−m)) for binary data;
(Y = N  g(m) = exp(am)  h(y) = 1/y!) where a ∈ R yields a Poisson distribution with intensity
exp(am) for count data. In these cases  the parameter space is X = R.
Let {(Yj  gj  hj)  j ∈ [p]} be a collection of observation spaces  base and link functions correspond-
ing to the column types of a data frame Y = [Yij](i j)∈[n]×[p] ∈ Yn
p . For each i ∈ [n]
and j ∈ [p]  we denote by M0
ij the target parameter minimizing the Kullback-Leibler divergence
between the distribution of Yij and the exponential family Exphj  gj   j ∈ [p]  given by

1 × . . . × Yn

M0

ij = arg max

m

EYij [log(hj(Yij)) + Yijm − gj(m)] .

(2)

We propose the following model to estimate M0 = [M0
effects and interactions.

ij](i j)∈[n]×[p] in the presence of additive

LOw-rank Interaction with Sparse additive effects (LORIS) model For every entry Yij  as-
sume a vector of covariates xij ∈ Rq is also available  e.g.  user information and item characteristics.
Denote xij(k)  k ∈ [q] the k-th component of xij and deﬁne the matrix X(k) = [xij(k)](i j)∈[n]×[p].
We introduce the following decomposition of the parameter matrix M0:

M0 =

α0

kX(k) + Θ0.

(3)

We call (3) the LORIS model  where α ∈ Rq is a sparse vector with unknown support modeling
additive effects and Θ0 ∈ Rn×p a low-rank matrix modeling the interactions.
In fact  LORIS is a generalization of robust matrix completion [3]  where the parameter matrix can
be decomposed as the sum of two matrices  one is low-rank and the other has some complementary
low-dimensional structure such as entry-wise or column-wise sparsity. Statistical recoverability
results in robust matrix estimation under a noiseless setting can be found in [28  3  4  13]; the additive
noise setting can be found in a recent work [17]. [23] also provide exact recovery results for more
general sparsity patterns.
Estimation Problem Denote Ω = {(i  j) ∈ [n] × [p] : Yij is observed} as the observation set.
For M ∈ Rn×p  L(M) is the negative log-likelihood of the observed data (Y  Ω) parameterized by
M. Up to an additive constant 

L(M) =

{−YijMij + gj(Mij)} .

q(cid:88)

k=1

For a > 0  we consider the following estimation problem:

(cid:33)

αkX(k) + Θ

+ λS (cid:107)α(cid:107)1 + λL (cid:107)Θ(cid:107)(cid:63) .

( ˆα  ˆΘ) ∈ argmin
(cid:107)α(cid:107)∞≤a
(cid:107)Θ(cid:107)∞≤a

L

We denote by ˆM =(cid:80)q

k=1 ˆαkX(k) + ˆΘ the estimated parameter matrix. The (cid:96)1 and nuclear norm
penalties are convex relaxations of the sparsity and low-rank constraints  and the regularization
parameters λS and λL serve as trade-offs between ﬁtting the data and enforcing sparsity of α and
controlling the "effective rank" of Θ.

(cid:88)
(cid:32) q(cid:88)

(i j)∈Ω

k=1

3

(4)

(5)

k=1 |X(k)ij| ≤ ν.

k (cid:54)= 0  (cid:104)Θ0  X(k)(cid:105) = 0.

Statistical Guarantees Here we establish convergence rates for the joint estimation of α0 and Θ0;
the proofs can be found in the supplementary material. Consider the following assumptions.

H1 (cid:13)(cid:13)Θ0(cid:13)(cid:13)∞ ≤ a (cid:13)(cid:13)α0(cid:13)(cid:13)∞ ≤ a and for all k ∈ [q] such that α0
[n] × [p] (cid:80)q
In particular  H2 guarantees that for all (Θ  α) satisfying H1  the matrix M =(cid:80)q

In particular  H1 guarantees the uniqueness of the decomposition in the LORIS model (3).
H2 For ν > 0  all k ∈ [q] and (i  j) ∈ [n] × [p]  X(k)ij ∈ [−1  1]. Furthermore for all (i  j) ∈

k=1 αkX(k) + Θ
satisﬁes (cid:107)M(cid:107)∞ ≤ (1 + ν)a. Let G be the q × q Gram matrix of the dictionary (X(1)  . . .   X(q))
deﬁned by G = [(cid:104)X(k)  X(l)(cid:105)](k l)∈[q]×[q].
H3 For κ > 0 and all α ∈ Rq  α(cid:62)Gα ≥ κ2 (cid:107)α(cid:107)2
2 .
Note we do not consider the case where the Gram matrix is singular  e.g.  q > np. For 0 < σ− ≤
σ+ < +∞ and 0 < γ < ∞ consider the following assumption on the link functions gj:
H4 The functions gj are twice differentiable  and for all x ∈ [−(1 + ν)a − γ  (1 + ν)a + γ] 

σ2− ≤ g(cid:48)(cid:48)

j (x) ≤ σ2

+  j ∈ [p].

H4 implies the data ﬁtting term L(M) is smooth and satisﬁes a restricted strong convexity property.
H5 For all (i  j) ∈ [n] × [p]  Yij is a sub-exponential random variable with scale and variance
+.
parameters 1/γ and σ2

If the random variables Yij are actually distributed according to an exponential family distribution of
the form (1)  then H4 implies H5.
H6 For (i  j) ∈ [n]× [p]  the events ωij = {(i  j) ∈ Ω} are independent with occurrence probability
πij. Furthermore  there exists 0 < π ≤ 1 such that for all (i  j) ∈ [n] × [p]  πij ≥ π.
H6 implies a data missing-at-random scenario where Yij is observed with probability at least π.
Theorem 1 Assume H1-6. Set

(cid:112)π max(n  p) log(n + p)  and λS = 24 max

λL = 2Cσ+

(6)

where C is a positive constant. Assume that max(n  p) ≥ 4σ2
2 exp(σ2

+/γ2 + 2σ2

(cid:107)X(k)(cid:107)1 log(n + p)/γ 

+/γ6 log2((cid:112)min(n  p)/(πγσ−)) +
+γa). Then  with probability at least 1 − 9(n + p)−1 
(cid:19)

s maxk (cid:107)X(k)(cid:107)1 log(n + p)

(cid:18) r max(n  p)

+ Dα 
s maxk (cid:107)X(k)(cid:107)1

κ2π

(7)

k

(cid:13)(cid:13) ˆα − α0(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13) ˆΘ − Θ0(cid:13)(cid:13)(cid:13)2

2

F

≤ C1
≤ C2

+

π

π

log(n + p) + DΘ.

In (7)  s := (cid:107)α0(cid:107)0  r := rank(Θ0). C1 and C2 are positive constants and Dα and DΘ are residuals
of lower order whose exact values are given in Appendix A.

The proof can be found in Appendix A. In Theorem 1  the rate obtained for α0 is the same as
the bound obtained in [17] in the special case of robust matrix completion. Examples satisfying
maxk (cid:107)X(k)(cid:107)1 /κ2 = O(1) include the case where the elements of the dictionary are matrices are
all zeros except a row or a column of one  (to model row and column effects) and the number of rows
n and columns p are of the same order; or when the covariates xij are categorical and the categories
are balanced  i.e.  the number of samples per category is of the same order.
The rate obtained for Θ0 is the sum of the standard low-rank matrix completion rate of order
r max(n  p)/π  e.g.  [16]  and of a term which boils down to sparse vector estimation rate as long
as maxk (cid:107)X(k)(cid:107)1 = O(1). Again  the latter can be satisﬁed by the special case of robust matrix
completion  for which our rates match the results of [17].

4

3 A Mixed Coordinate Gradient Descent Method for LORIS

This section introduces a mixed coordinate gradient descent (MCGD) method to solve the LORIS
estimation problem (5). We assume that a is sufﬁciently large such that the constraints (cid:107)α(cid:107)∞ ≤
a (cid:107)Θ(cid:107)∞ ≤ a are always inactive. To simplify notation  we denote the log-likelihood function as

L(α  Θ) := L ((cid:80)q

k=1 αkX(k) + Θ). We assume

H7 (a) L(α  Θ) is σΘ-smooth w.r.t. Θij for (i  j) ∈ Ω and (b) σα-smooth w.r.t. α; (c) the gradient
∇αL(α  Θ) is ˆσΘ-Lipschitz w.r.t. Θ. Moreover  the gradient ∇ΘL(α  Θ) is bounded as long as
α  Θ are bounded.

The above is implied by H4 for bounded (α  Θ). We consider the augmented objective function:

F (α  Θ  R) := L(α  Θ) + λS(cid:107)α(cid:107)1 + λLR .

(8)
For some RUB ≥ 0  if an optimal solution ( ˆα  ˆΘ) to (5) satisﬁes (cid:107) ˆΘ(cid:107)(cid:63) ≤ RUB  then any optimal
solution to the following problem

P(RUB) :

min

α∈Rq Θ∈Rn×p R∈R+

F (α  Θ  R) s.t. RUB ≥ R ≥ (cid:107)Θ(cid:107)(cid:63)  

(9)

will also be optimal to (5). For example  ( ˆα  ˆΘ  ˆR) with ˆR = (cid:107) ˆΘ(cid:107)(cid:63) is an optimal solution to (9). We
have deﬁned the problem as P(RUB) to emphasize its dependence on the upper bound RUB. Later we
shall describe a simple strategy to estimate RUB. We ﬁx the set Ξ ⊆ [n] × [p] where Ω ⊆ Ξ is the
target coordinate set for the low rank matrix ˆΘ that we are interested in.

Proposed Method A natural way to exploit structure in P(RUB) is to apply coordinate gradient
descent to update α and (Θ  R) separately. While the trace-norm constraint on (Θ  R) can be handled
by the conditional gradient (CG) method [14]  the (cid:96)1 norm penalization on α is more efﬁciently
tackled by the proximal gradient method in practice. In addition  we tighten the upper bound RUB
on-the-ﬂy as the algorithm proceeds. The MCGD method goes as follows. At the tth iteration  we are
given the previous iterate (α(t−1)  Θ(t−1)  R(t−1)) and the upper bound R(t)
UB is computed. The ﬁrst
block α is updated with a proximal gradient step:

(cid:0)α(t−1) − γ∇αL(α(t−1)  Θ(t−1))(cid:1)

(cid:0)α(t−1) − γ∇αL(α(t−1)  Θ(t−1))(cid:1) .

α(t) = proxγλS(cid:107)·(cid:107)1

= TγλS

(10)

In (10)  ∇αL(·) is the gradient of the log-likelihood function taken w.r.t. α  γ > 0 is a pre-deﬁned
step size parameter and Tλ(x) := sign(x) (cid:12) (x − λ1)+ is the component-wise soft thresholding
operator. Alternatively  we can exactly solve the problem

α(t) ∈ arg minα∈Rq F (α  Θ(t−1)  R(t−1))  

for which closed-form solution can be obtained in certain special cases (see below).
The second block (Θ  R) is updated with a CG step

(Θ(t)  R(t)) = (Θ(t−1)  R(t−1)) + βt( ˆΘ(t) − Θ(t−1)  ˆR(t) − R(t−1))  

where βt ∈ [0  1] is a step size to be deﬁned later. ( ˆΘ(t)  ˆR(t)) is a direction evaluated as
(cid:104)Z ∇ΘL(α(t)  Θ(t−1))(cid:105) + λ1R s.t. (cid:107)Z(cid:107)(cid:63) ≤ R ≤ R(t)
UB  

( ˆΘ(t)  ˆR(t)) ∈ arg min

Z R

(11)

(12)

(13)

and ∇ΘL(·) is the gradient of L(·) taken w.r.t. Θ. If (Θ(t−1)  R(t−1)) is feasible to P(R(t)
UB)  then
(Θ(t)  R(t)) must also be feasible to P(R(t)
UB). Furthermore  if we let u1  v1 be the top left and right
singular vectors of the gradient matrix ∇ΘL(α(t)  Θ(t−1)) and σ1(∇ΘL(α(t)  Θ(t−1))) be the top
singular value  then ( ˆΘ(t)  ˆR(t)) admits a simple closed form solution:

( ˆΘ(t)  ˆR(t)) =

(0  0) 
(−R(t)

UBu1v(cid:62)

1   R(t)

UB) 

if λL ≥ σ1(∇ΘL(α(t)  Θ(t−1)))  
if λL < σ1(∇ΘL(α(t)  Θ(t−1))) .

(14)

(cid:40)

5

Lastly  the step size βt is determined by:

(cid:110)

βt = min

1 

(cid:104)Θ(t−1) − ˆΘ(t) ∇ΘL(α(t)  Θ(t−1))(cid:105) + λL(R(t−1) − ˆR(t))

σΘ(cid:107)PΩ( ˆΘ(t) − Θ(t−1))(cid:107)2

F

(cid:111)

.

(15)

The step size strategy ensures decrease in the objective value between successive iterations. This is
essential for establishing convergence of the proposed method [cf. Theorem 2]. We remark that the
arithmetics in the MCGD method are not affected when we restrict the update of Θ(t) in (12) to the
entries in Ξ only. This is due to L(X) = L(PΩ(X)) and the CG update direction (13) only involves
the gradient of ∇ΘL(α(t)  Θ(t−1)) w.r.t. entries of Θ in Ω  where Ω ⊆ Ξ.

UB We describe a strategy for computing a valid upper bound

Computing the Upper Bound R(t)
UB for ˆR and (cid:107) ˆΘ(cid:107)(cid:63) during the updates in the MCGD method. Let us assume that:
R(t)
H8 For all Θ and α  we have L(α  Θ) ≥ 0.
The above can be enforced as the log-likelihood function is lower bounded [cf. H4]. From (5) and
using the above assumption  it is obvious that

F0(0  0) = L(0  0) ≥ L( ˆα  ˆΘ) + λS(cid:107) ˆα(cid:107)1 + λL(cid:107) ˆΘ(cid:107)(cid:63) ≥ λL(cid:107) ˆΘ(cid:107)(cid:63) 

(16)
L L(0 + fU (0)) is a valid upper bound to (cid:107) ˆΘ(cid:107)(cid:63); furthermore it can be tightened
and thus R0
as we progress in the MCGD method. In particular  observe that ( ˆα  ˆΘ  ˆR) with ˆR = (cid:107) ˆΘ(cid:107)(cid:63) is an
optimal solution to P(R0

UB := λ−1

UB)  we have

F (α  Θ  R) ≥ F ( ˆα  ˆΘ  ˆR) = L( ˆα  ˆΘ) + λS(cid:107) ˆα(cid:107)1 + λL ˆR ≥ λL ˆR.

(17)
UB)  λ−1
L F (α  Θ  R) is an upper bound to ˆR and
L F (α(t)  Θ(t−1)  R(t−1)) at iteration t  where
UB) and

UB := λ−1

UB ≥ R(t−1). That is  (α(t)  Θ(t−1)  R(t−1)) is feasible to both P(R(t)

). Lastly  we summarize the MCGD method in Algorithm 1.

UB

In other words  for all feasible (α  Θ  R) to P(R0
(cid:107) ˆΘ(cid:107)(cid:63). The above motivates us to select R(t)
we observe that R(t)
P(R(t−1)
Computation Complexity Consider the MCGD
method in Algorithm 1. Observe that line 3 re-
quires computing the gradient w.r.t. α which in-
volves |Ω|q Floating Points Operations (FLOPS)
and the soft thresholding operator involves O(q)
FLOPS. As the log-likelihood function L(·) is
evaluated element-wisely on Θ  evaluating the
objective value and the derivative w.r.t. Θ re-
quires O(|Ω|) FLOPS. As such  line 4 can be
evaluated in O(|Ω|) FLOPS and line 5 requires
O(|Ω| max{n  p} log(1/δ)) FLOPS where the
additional complexity is due to the top SVD
computation and δ is a preset accuracy level
of SVD computation. Lastly  line 6 requires
O(|Ξ|) FLOPS since we only need to update
the entries of Θ in Ξ [cf. see the remark after
(15)]. The overall per-iteration complexity is
O(|Ξ| + |Ω|(max{n  p} log(1/δ) + q)).

Algorithm 1 MCGD Method for (9).
1: Initialize: — Θ(0)  α(0)  R(0).

E.g. 

Θ(0)  α(0)  R(0) = (0  0  0).

2: for t = 1  2  . . .   T do
3:

4:

// Update for α //
Compute the proximal update using (10)
[or exact update via (11)] to obtain α(t).
// Update for (Θ  R) //
Compute the upper bound as R(t)
λ−1
L F (α(t)  Θ(t−1)  R(t−1)).

UB :=

5: Compute the update direction  ( ˆΘ(t)  ˆR(t)) 

using Eq. (14).

6: Compute the CG update using (12)  where

the step size βt is set as Eq. (15).

7: end for
8: Return: Θ(T )  α(T )  R(T ).

From the above  the per-iteration computation complexity of the MCGD method scales linearly with
the problem dimension max{n  p} and |Ω|. This is comparable to [24  9]  where the former focuses
only on the least square loss case. The following theorem  whose proof can be found in Appendix C 
shows that the MCGD method converges at a sublinear rate.

Theorem 2 Assume H7 and H8. Deﬁne the quantity

(cid:110) 24(Q(t))2

C(t) := max

 

24ˆσ2

Θ(Q(t))2
σΘ

γ

+ max{6R(t)

UB(λL + M (t))  24σΘ(R(t)

 

(18)

UB)2}(cid:111)

6

 

UB)2}(cid:111)
(cid:17)−1

1

(cid:16) 1

T(cid:88)

T

t=1

C(t)

(cid:17)

(cid:110) 24(Q(0))2

(cid:16) 1



:= λ−1

where Q(t)
:=
L F (α(t)  Θ(t−1)  R(t−1)). If we choose the step sizes as γ ≤ 1/σα and βt as in (15)  then
λ−1
(i) the above quantity is upper bounded as C(t) ≤ C for all t ≥ 1  where

S F (α(t)  Θ(t)  R(t))  M (t)

:= (cid:107)∇ΘL(α(t)  Θ(t−1))(cid:107)2 and R(t)

UB

C := max

 

24ˆσ2

Θ(Q(0))2
σΘ

γ

+ max{6R(0)

UB(λL + ¯M )  24σΘ(R(0)

(19)

such that ¯M is an upper bound to M (t)  and (ii) the MCGD method converges to an -optimal
solution to (5) in T iterations  i.e.  F0(α(T )  Θ(T )) − F0( ˆα  ˆΘ) ≤   where

T ≥ C(T )

−

1

F0(α(0)  Θ(0)) − F0( ˆα  ˆΘ)

with C(T ) :=

+

.

(20)

In particular  as C(T ) ≤ C  at most C(−1 − (F0(α(0)  Θ(0)) − F0( ˆα  ˆΘ))−1)+ iterations are
required for the MCGD method to reach an -optimal solution to (5).

Detailed Comparison to Prior Algorithms Previous contributions have focused on the special
case of (5) where q = np  the dictionary (X(1)  . . .   X(q)) is the canonical basis of Rn×p  and the
link functions are quadratic. In this particular case  (5) becomes the estimation problem solved in
sparse plus low-rank matrix decomposition. Popular examples are the alternating direction method of
multiplier [21  26] or the projected gradient method on a reformulated problem [5]. These methods
either require computing a complete SVD or knowing the optimal rank number of Θ a priori. When
n  p (cid:29) 1  it is computationally prohibitive to evaluate the complete SVD since each iteration would
require O(max{n2p  p2n}) FLOPS. Other related work rely on factorizing the low-rank component 
yielding nonconvex problems [11]; see also [29] and references therein.
Similar to the development of MCGD  a natural alternative is to apply algorithms based on the CG
(a.k.a. Frank-Wolfe) method [14]  whose iterations only require the computation of a top SVD. The
present work is closely related to the efforts in [24  9] which focused on the quadratic setting. Mu
et al. [24] combines the CG method with proximal update as a two-steps procedure; Garber et al. [9]
combines a CD method with CG updates on both the sparse and low-rank components. The work in
[9] is also related to [19  2] which combine CD with CG updates for solving constrained problems 
instead of penalized problems like (5). Sublinear convergence rates are proven for the above methods.
Finally  Fithian and Mazumder [8] also suggested to apply CD on (5)  yet the convergence properties
were not discussed.
In fact  when the MCGD’s result is specialized to the same setting as [24]  our worst-case bound on
iteration number computed with C match the bound in [24]. As shown in the supplementary material 
we have C(t) → C (cid:63)  where C (cid:63) depends on the optimal objective value of (9) and is smaller than
C. Since the quantity C(T ) in (20) is an average of {C(t)}T
t=1  this implies that the MCGD method
requires less number of iterations for convergence than that is required by [24]. Such reduction is
possible due to the on-the-ﬂy update for R(t)
UB. Moreover  our analysis in Theorem 2 holds when the
MCGD method is implemented with a few practical modiﬁcations.

Exact Partial Minimization for α Consider the special case of (5) where the link functions are
either quadratic or exponential and the dictionary matrices satisfy:

supp(X(k)) ∩ supp(X(k(cid:48))) = ∅  k (cid:54)= k(cid:48) and [X(k)]i j = ck  ∀ (i  j) ∈ supp(X(k)) .

(21)
In this case  the partial minimization (11) can be decoupled into q scalar optimizations involving
one coordinate of α  which can be solved in closed form. Note that this modiﬁcation to the MCGD
method is supported by Theorem 2 and the sublinear convergence rate holds. On the contrary  closed
form update of α is not supported by prior works such as [24  9  19  2].

Distributed MCGD Optimization Consider the case where the observed data entries are stored
across K workers  each of them communicating with a central server. It is natural to distribute the
MCGD optimization over these workers to ofﬂoad computation burden  or for privacy protection.
Formally  we divide Ω into K disjoint partitions such that Ω = Ω1 ∪···∪ ΩK and worker k holds Ωk.
k=1 Lk(α  Θ)  where Lk(α  Θ) is deﬁned by replacing the summation
over Ω with Ωk in (4). Clearly  when α and PΩk (Θ) are given to the kth worker  the worker will be

In this way  L(α  Θ) =(cid:80)K

7

singular vectors of the gradient matrix ∇ΘL(α  Θ) =(cid:80)K

able to evaluate the local loss function and its gradient.
As shown in Appendix D  the MCGD method can be easily extended to utilize distributed computation.
The proximal update in line 3 is replaced by the following procedure. First  the local gradients
computed by the workers are aggregated  then the soft thresholding operation is performed at
the central server. Meanwhile  as the CG update in line 5 essentially requires computing the top
k=1 ∇ΘLk(α PΩk (Θ))  the latter can be
implemented through a distributed version of the power method exploiting the decomposable structure
of the gradient  such as described in [30]. It only requires O(log(1/δ)) power iterations to compute
a top SVD solution of accuracy δ. Thus  for a sufﬁciently small δ > 0  the overall per-iteration
complexity of the distributed method at the tth iteration is reduced to O(|Ξ| + max{n  p} log(1/δ))
at the central server  and O(|Ωk|(max{n  p} log(1/δ) + q)) at the kth worker.

4 Numerical Experiments

Experimental Setup We ﬁrst generate the target parameter M0 according to the LORIS model in
(3). For the sparse additive effects component  we consider q = pn/5 where we set (X(k))ij = 1 if
j(n − 1) + i ∈ {5(k − 1) + 1  ...  5k}. This models a categorical variable containing n/5 categories.
Furthermore  the target sparse component α0 has a sparsity level of 10%. For the low-rank component 
the target parameter Θ0 is generated as a rank-4 matrix formed by the outer product of random
orthogonal vectors. Notice that due to the structure of sparse additive effects  the surveyed prior
methods [21  11  5] cannot be applied directly.

Gaussian Design To compare our framework to a reasonable benchmark  we focus on a homoge-
nous setting with numerical data modeled with the quadratic link function g(m) = m2. We set the
regularization parameters λS and λL to the theoretical values given in Theorem 1. We compare
our result with a common two-step procedure where the components αkj are ﬁrst estimated in
a preprocessing step as the means of the variables taken by group; then Θ is estimated using the
softImpute method proposed in [12]. The regularization parameter for [12] is set to the same value λL.
We compare the results in terms of estimation error and computing time in Table 1  after letting the
two methods converge to the same precision of 10−5. We observe the two methods perform equally
well in terms of estimating Θ. LORIS yields constant estimation errors of α0 as the dimension
increases and the support of α0 is kept constant  contrary to the two-step procedure for which the
estimation error of α0 increases with the dimension. As expected  the two-step method is faster for
small data sets  whereas for large data sizes LORIS is superior in computational time. The above
results are consistent with our theoretical ﬁndings.

(cid:13)(cid:13)(cid:13)Θ0 − ˆΘ

(cid:13)(cid:13)(cid:13)2

F

(cid:13)(cid:13)α0 − ˆα(cid:13)(cid:13)2

2

problem size (n × p)

time (secs)

150 × 30
1  500 × 300
15  000 × 300
15  000 × 3  000

LORIS
0.17
13.8
130.2
348

two-step LORIS
0.02
10.7
136.6
528

52
175.5
675
2.7 × 103

two-step
52
234
720
2.6 × 103

LORIS
1.8
0.95
0.95
2.34

two-step
3.0
17.1
16.2
180

Table 1: Comparison of proposed method with a two-step method in terms of computation time and
estimation error for increasing dimensions (averaged over 10 experiments).

Survey data To test the efﬁcacy of our framework with heterogeneous data  we examine a survey
conducted by the French National Institute of Statistics (Insee: http://www.insee.fr/) concerning
the hobbies of French people. The data set contains n = 8  403 individuals and p = 19 binary and
quantitative variables  indicating whether or not the person has been involved in different activities
(reading  ﬁshing  etc.)  the number of hours spent watching TV and the overall number of hobbies of
the individuals. Individuals are grouped by age category (15 − 25  25 − 35  etc.): this categorical
variable is used as a predictor of the survey responses in the subsequent experiment. We introduce
30% of missing values in the data set  and compare the imputation error of LORIS with a mixed data
model (using a quadratic loss for numeric columns  a logistic loss for binary columns and a Poisson
loss for counts) and LORIS with a Gaussian data model  with the imputation error of softImpute. The

8

Figure 2: Imputation error of LORIS with mixed data model and Gaussian data model  and softImpute
(10 replications) for categorical variables (left) and quantitative variables (right).

results are given in Figure 2 across 10 replications of the experiment  and show that  for this example 
both LORIS models improve on the baseline softImpute by a factor 2. We also observe that modeling
explicitly the binary variables leads to better imputation.
Finally  we apply LORIS with a mixed data model to the original data set. A subset of the resulting α
vector is given in Table 2. There is a coefﬁcient in αkj for every age category k and every variable j.
The coefﬁcients in Table 2 indicate that young individuals engage in activities such as music and sport
more than older people  and the opposite trend for collecting  knitting and ﬁshing. Some coefﬁcients
are set to zero  indicating the absence of effect of the age category on the variable. We also observe
that younger people engage overall in more activities than older people.

Age category Music
25-35
35-45
45-55
55-65
65-75
75-85

2.2
2.0
1.1
0
0
-0.1

Sport Collecting Mechanic Knitting
0.4
0.3
-0.8
-2.2
-2.1
-0.9
Table 2: Estimated age category effects (α).

-1.7
-2.3
-2.7
-1.0
-0.7
-0.1

-2.1
-2.7
-2.1
-1.9
-1.4
-0.6

0
0
0
0
-1.1
-0.5

Fishing Nb activities
-1.9
-2.3
-2.7
-1.6
-1.3
-0.6

10.0
13.0
13.8
8.8
5.5
2.2

Conclusion In this paper  we proposed a new framework for handling large data frames with
heterogeneous data and missing values which incorporates additive effects. It consists of a doubly
penalized quasi-maximum likelihood estimator and a new optimization algorithm to implement the
estimator. We examined both the statistical and computational efﬁciency of the framework and
derived worst case bounds of its performance. Future work includes the incorporation of qualitative
features with more than two categories and of missing values in the dictionary matrices.

5 Acknowledgement

The authors would like to thank for the useful comments from three anonymous reviewers. HTW’s
work was supported by the grant NSF CCF-BSF 1714672.

References
[1] A. Agresti. Categorical Data Analysis  3rd Edition. Wiley  2013.

[2] A. Beck  E. Pauwels  and S. Sabach. The cyclic block conditional gradient method for convex optimization

problems. SIAM Journal on Optimization  25(4):2024–2049  2015.

9

LORIS−mixedLORIS−gaussiansoftImpute30003500400045005000LORIS−mixedLORIS−gaussiansoftImpute20000300004000050000[3] E. J. Candès  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? J. ACM  58(3):11:1–11:37 
June 2011. ISSN 0004-5411. doi: 10.1145/1970392.1970395. URL http://doi.acm.org/10.1145/
1970392.1970395.

[4] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Rank-sparsity incoherence for matrix
decomposition. SIAM Journal on Optimization  21(2):572–596  2011. doi: 10.1137/090761793. URL
https://doi.org/10.1137/090761793.

[5] Y. Chen and M. J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees. CoRR  abs/1509.03025  2015.

[6] J. de Leeuw. Principal component analysis of binary data by iterated singular value decomposition.
Comput. Stat. Data Anal.  50(1):21–39  Jan. 2006. ISSN 0167-9473. doi: 10.1016/j.csda.2004.07.010.
URL http://dx.doi.org/10.1016/j.csda.2004.07.010.

[7] A. Feuerverger  Y. He  and S. Khatri. Statistical signiﬁcance of the netﬂix challenge. Statist. Sci.  27(2):

202–231  05 2012. doi: 10.1214/11-STS368. URL http://dx.doi.org/10.1214/11-STS368.

[8] W. Fithian and R. Mazumder. Flexible Low-Rank Statistical Modeling with Missing Data and Side

Information. Statistical Science  33(2):238–260  2018.

[9] D. Garber  S. Sabach  and A. Kaplan. Fast generalized conditional gradient method with applications to

matrix recovery problems. arXiv preprint arXiv:1802.05581  2018.

[10] G. Gidel  F. Pedregosa  and S. Lacoste-Julien. Frank-wolfe splitting via augmented lagrangian method. In
OPTML 2017: 10th NIPS Workshop on Optimization for Machine Learning (NIPS 2017)  page 21  2017.
URL http://opt-ml.org/papers/OPT2017_paper_21.pdf.

[11] Q. Gu  Z. W. Wang  and H. Liu. Low-rank and sparse structure pursuit via alternating minimization.
In A. Gretton and C. C. Robert  editors  Proceedings of the 19th International Conference on Artiﬁcial
Intelligence and Statistics  volume 51 of Proceedings of Machine Learning Research  pages 600–609 
Cadiz  Spain  09–11 May 2016. PMLR. URL http://proceedings.mlr.press/v51/gu16.html.

[12] T. Hastie  R. Mazumder  J. Lee  and R. Zadeh. Matrix Completion and Low-Rank SVD via Fast Alternating

Least Squares. The Journal of Machine Learning Research  16:3367–3402  jan 2015.

[13] D. Hsu  S. M. Kakade  and T. Zhang. Robust matrix decomposition with sparse corruptions. EEE

Transactions on Information Theory  57(11):7221–7234  2011.

[14] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1)  pages 427–435 

2013.

[15] H. A. L. Kiers. Simple structure in component analysis techniques for mixtures of qualitative and
ISSN 1860-0980. doi: 10.1007/

quantitative variables. Psychometrika  56(2):197–212  Jun 1991.
BF02294458. URL https://doi.org/10.1007/BF02294458.

[16] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli  20(1):282–303 

2014.

[17] O. Klopp  K. Lounici  and A. B. Tsybakov. Robust matrix completion. Probability Theory and Related
Fields  169(1):523–564  Oct 2017. doi: 10.1007/s00440-016-0736-y. URL https://doi.org/10.
1007/s00440-016-0736-y.

[18] N. K. Kumar and J. Schneider. Literature survey on low rank approximation of matrices. Linear and
Multilinear Algebra  65(11):2212–2244  2017. doi: 10.1080/03081087.2016.1267104. URL https:
//doi.org/10.1080/03081087.2016.1267104.

[19] S. Lacoste-Julien  M. Jaggi  M. Schmidt  and P. Pletscher. Block-coordinate frank-wolfe optimization
for structural svms. In Proceedings of the 30th International Conference on International Conference on
Machine Learning-Volume 28  pages I–53. JMLR. org  2013.

[20] A. J. Landgraf and Y. Lee. Generalized principal component analysis: Projection of saturated model

parameters. Technical report  The Ohio State University  Department of Statistics  06 2015.

[21] Z. Lin  R. Liu  and Z. Su. Linearized alternating direction method with adaptive penalty for low-rank

representation. In Advances in neural information processing systems  pages 612–620  2011.

[22] L. T. Liu  E. Dobriban  and A. Singer. e pca: High dimensional exponential family pca. Annals of Applied

Statistics  to appear  2018.

10

[23] M. Mardani  G. Mateos  and G. B. Giannakis. Recovery of low-rank plus compressed sparse matrices with
application to unveiling trafﬁc anomalies. IEEE Transactions on Information Theory  59(8):5186–5205 
Aug 2013. ISSN 0018-9448. doi: 10.1109/TIT.2013.2257913.

[24] C. Mu  Y. Zhang  J. Wright  and D. Goldfarb. Scalable robust matrix recovery: Frank–wolfe meets proximal
methods. SIAM Journal on Scientiﬁc Computing  38(5):A3291–A3317  2016. doi: 10.1137/15M101628X.
URL https://doi.org/10.1137/15M101628X.

[25] J. Pagès. Multiple factor analysis by example using R. Chapman and Hall/CRC  2014.

[26] M. Tao and X. Yuan. Recovering low-rank and sparse components of matrices from incomplete and noisy

observations. SIAM Journal on Optimization  21(1):57–81  2011.

[27] M. Udell  C. Horn  R. Zadeh  and S. Boyd. Generalized low rank models. Foundations and Trends
in Machine Learning  9(1)  2016. doi: 10.1561/2200000055. URL http://dx.doi.org/10.1561/
2200000055.

[28] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit.

In Proceedings of the 23rd
International Conference on Neural Information Processing Systems  NIPS’10  pages 2496–2504  USA 
2010. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id=2997046.2997174.

[29] X. Zhang  L. Wang  and Q. Gu. A uniﬁed framework for nonconvex low-rank plus sparse matrix
In International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2018  9-11
recovery.
April 2018  Playa Blanca  Lanzarote  Canary Islands  Spain  pages 1097–1107  2018. URL http:
//proceedings.mlr.press/v84/zhang18c.html.

[30] W. Zheng  A. Bellet  and P. Gallinari. A distributed frank-wolfe framework for learning low-rank matrices

with the trace norm. arXiv preprint arXiv:1712.07495  2017.

11

,Geneviève Robin
Hoi-To Wai
Julie Josse
Olga Klopp
Eric Moulines