2019,A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning,We propose and study a general framework for regularized Markov decision processes (MDPs) where the goal is to find an optimal policy that maximizes the expected discounted total reward plus a policy regularization term. 
The extant entropy-regularized MDPs can be cast into our framework. 
Moreover  under our framework  many regularization terms can bring multi-modality and sparsity  which are potentially useful in reinforcement learning. 
In particular  we present sufficient and necessary conditions that induce a sparse optimal policy. We also conduct a full mathematical analysis of the proposed regularized MDPs  including the optimality condition  performance error  and sparseness control. We provide a generic method to devise regularization forms and propose off-policy actor critic algorithms in complex environment settings. We empirically analyze the numerical properties of optimal policies and compare the performance of different sparse regularization forms in discrete and continuous environments.,A Regularized Approach to Sparse Optimal Policy in

Reinforcement Learning

Xiang Li∗

School of Mathematical Sciences

Peking University

Beijing  China

lx10077@pku.edu.cn

Wenhao Yang∗

Center for Data Science

Peking University

Beijing  China

yangwenhaosms@pku.edu.cn

National Engineering Lab for Big Data Analysis and Applications

Zhihua Zhang

School of Mathematical Sciences

Peking University

Beijing  China

zhzhang@math.pku.edu.cn

Abstract

We propose and study a general framework for regularized Markov decision pro-
cesses (MDPs) where the goal is to ﬁnd an optimal policy that maximizes the
expected discounted total reward plus a policy regularization term. The extant
entropy-regularized MDPs can be cast into our framework. Moreover  under our
framework  many regularization terms can bring multi-modality and sparsity  which
are potentially useful in reinforcement learning. In particular  we present sufﬁcient
and necessary conditions that induce a sparse optimal policy. We also conduct a full
mathematical analysis of the proposed regularized MDPs  including the optimality
condition  performance error  and sparseness control. We provide a generic method
to devise regularization forms and propose off-policy actor critic algorithms in
complex environment settings. We empirically analyze the numerical properties of
optimal policies and compare the performance of different sparse regularization
forms in discrete and continuous environments.

1

Introduction

Reinforcement learning (RL) aims to ﬁnd an optimal policy that maximizes the expected discounted
total reward in an MDP [4  36]. It’s not an easy task to solve the nonlinear Bellman equation [2]
greedily in a high-dimension action space or when function approximation (such as neural networks)
is used. Even if the optimal policy is obtained precisely  it is often the case the optimal policy is
deterministic. Deterministic policies are bad to cope with unexpected situations when its suggested
action is suddenly unavailable or forbidden. By contrast  a multi-modal policy assigns positive
probability mass to both optimal and near optimal actions and hence has multiple alternatives to
handle this case. For example  an autonomous vehicle aims to do path planning with a pair of departure
and destination as the state. When a suggested routine is unfortunately congested  an alternative
routine could be provided by a multi-modal policy  which can’t be provided by a deterministic policy
without evoking a new computation. Therefore  in a real-life application  we hope the optimal policy
to possess thee property of multi-modality.

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Entropy-regularized RL methods have been proposed to handle the issue. More speciﬁcally  an
entropy bonus term is added to the expected long-term returns. As a result  it not only softens the
non-linearity of the original Bellman equation but also forces the optimal policy to be stochastic 
which is desirable in problems where dealing with unexpected situations is crucial. In prior work  the
Shannon entropy is usually used. The optimal policy is of the form of softmax  which has been shown
can encourage exploration [8  40]. However  a softmax policy assigns a non-negligible probability
mass to all actions  including those really terrible and dismissible ones  which may result in an unsafe
policy. For RL problems with high dimensional action spaces  a sparse distribution is preferred in
modeling a policy function  because it implicitly does action ﬁltration  i.e.  weeds out suboptimal
actions and maintains near optimal actions. Thus  Lee et al. [19] proposed to use Tsallis entropy [39]
instead  giving rise to a sparse MDP where only few actions have non-zero probability at each state
in the optimal policy. Lee et al. [20] empirically showed that general Tsallis entropy2 also leads to a
sparse MDP. Moreover  the Tsallis regularized RL has a lower performance error  i.e.  the optimal
value of the Tsallis regularized RL is closer to the original optimal value than that of the Shannon
regularized RL.
The above discussions manifest that an entropy regularization characterizes the solution to the
corresponding regularized RL. From Neu et al. [28]  any entropy-regularized MDP can be viewed
as a regularized convex optimization problem where the entropy serves as the regularizer and the
decision variable is a stationary policy. Geist et al. [10] proposed a framework in which the MDP is
regularized by a general strongly concave function. It analyzes some variants of classic algorithms
under that framework but does not provide insight into the choice of regularizers. On the other hand  a
sparse optimal policy distribution is more favored in large action space RL problems. Prior work Lee
et al. [19]  Nachum et al. [27] obtains a sparse optimal policy by the Tsallis entropy regularization.
Considering the diversity and generality of regularization forms in convex optimization  it is natural
to ask whether other regularizations can lead to sparseness. The answer is that there does exist other
regularizers that induces sparsity.
In this paper  we propose a framework for regularized MDPs  where a general form of regularizers is
imposed on the expected discounted total reward. This framework includes the entropy regularized
MDP as a special case  implying certain regularizers can induce sparseness. We ﬁrst give the
optimality condition in regularized MDPs under the framework and then give necessary and sufﬁcient
conditions to show which kind of regularization can lead to a sparse optimal policy. Interestingly 
there are lots of regularization that can lead to the sparseness  and the degree of sparseness can be
controlled by the regularization coefﬁcient. Furthermore  we show that regularized MDPs have a
regularization-dependent performance error caused by the regularization term  which could guide
us to choose an effective regularization when it comes to dealing with problems with a continuous
action space. To solve regularized MDPs  we employ the idea of generalized policy iteration and
propose an off-policy actor-critic algorithm to ﬁgure out the performance of different regularizers.

2 Notation and preliminaries

∆X = {P :(cid:80)

Markov Decision Processes In reinforcement learning (RL) problems  the agent’s interaction with
the environment is often modeled as an Markov decision process (MDP). An MDP is deﬁned by
a tuple (S A  P  r  P0  γ)  where S is the state space and A the action space with |A| actions. We
use ∆X to denote the simplex on any set X   which is deﬁned as the set of distributions over X   i.e. 
x∈X P (x) = 1  P (x) ≥ 0}. The vertex set of ∆X is deﬁned as VX = {P ∈ ∆X :
∃ x ∈ X   s.t. P (x) = 1}. P : S × A → ∆S is the unknown state transition probability distribution
and r : S × A → [0  Rmax] is the bounded reward on each transition. P0 is the distribution of initial
state and γ ∈ [0  1) is the discount factor.
Optimality Condition of MDP
The goal of RL is to ﬁnd a stationary policy which maps from state space to a simplex over the actions
π : S → ∆A that maximizes the expected discounted total reward  i.e. 

γtr(st  at)

 

(1)

(cid:34) ∞(cid:88)

t=0

E

max

π

(cid:35)

(cid:12)(cid:12)(cid:12)π  P0

2The general Tsallis entropy is deﬁned with an additional real-valued parameter  called an entropic index.

Lee et al. [20] shows that when this entropic index in large enough  the optimal policy is sparse.

2

(cid:34) ∞(cid:88)

t=0

(cid:35)

(cid:12)(cid:12)(cid:12)π  P0

V π(s) = E

(cid:34) ∞(cid:88)

where s0 ∼ P0  at ∼ π(·|st)  and st+1 ∼ P(·|st  at). Given any policy π  its state value and Q-value
functions are deﬁned respectively as

(cid:35)
)(cid:3).
(cid:2)r(s  a) + γEs(cid:48)|s aV π(s
Any solution of the problem (1) is called an optimal policy and denoted by π∗. Optimal policies may
not be unique in an MDP  but the optimal state value is unique (denoted V ∗). Actually  V ∗ is the
(cid:2)r(s  a) + γEs(cid:48)|s aV (s
)(cid:3).
unique ﬁxed point of the Bellman operator T   i.e.  V ∗(s) = T V ∗(s) and
(cid:48)

γtr(st  at)|s0 = s  π

Qπ(s  a) = Ea∼π(·|s)

Ea∼π(·|s)

t=0

(cid:48)

 

T V (s) (cid:44) max

π

π∗ often is a deterministic policy which puts all probability mass on one action[31]. Actually  it can
be obtained as the greedy action w.r.t. the optimal Q-value function  i.e.  π∗(s) ∈ argmaxa Q∗(s  a)
3. The optimal Q-value can be obtained from the state value V ∗(s) by deﬁnition.
As a summary  any optimal policy π∗ and its optimal state value V ∗ and Q-value Q∗ satisfy the
following optimality condition for all states and actions 
∗
(s  a) = r(s  a) + γEs(cid:48)|s aV
∗
∗
V

(s) = max

(s  a)  π

(s  a).

∗
Q

∗
Q

∗
Q

(s) 

a

(s) ∈ argmax

a

3 Regularized MDPs

To obtain a sparse but multi-modal optimal policy  we impose a general regularization term to the
objective (1) and solve the following regularized MDP problem

E

max

π

γt(r(st  at) + λφ(π(at|st)))

 

(2)

Hφ(π) = Ea∼π(·|s)φ(π(a|s)) 

where φ(·) is a regularization function. Problem (2) can be seen as a RL problem in which the
reward function is the sum of the original reward function r(s  a) and a term φ(π(a|s)) that provides
regularization. If we take expectation to the regularization term φ(π(a|s))  it can be found that the
quantity
(3)
is entropy-like but not necessarily an entropy in our work. However  Problem (2) is not well-deﬁned
since arbitrary regularizers would be more of a hindrance than a help. In the following  we make
some assumptions about φ(·).
3.1 Assumption for regularizers
A regularizer φ(·) characterizes solutions to Problem (2). In order to make Problems (2) analyzable 
a basic assumption (Assumption 1) is prerequisite. Explanation and examples will be provided to
show that such an assumption is reasonable and not strict.

Assumption 1 The regularizer φ(x) is assumed to satisfy the following conditions on the interval
(0  1]: (1) Monotonicity: φ(x) is non-increasing; (2) Non-negativity: φ(1) = 0; (3) Differentiabil-
ity: φ(x) is differentiable; (4) Expected Concavity: xφ(x) is strictly concave.

The assumptions of monotonicity and non-negativity make the regularizer be an positive exploration
bonus. The bonus for choosing an action of high probability is less than that of choosing an action of
low probability. When the policy becomes deterministic  the bonus is forced to zero. The assumption
of differentiability facilitates theoretic analysis and beneﬁts practical implementation due to the widely
used automatic derivation in deep learning platforms. The last assumption of expected concavity
makes Hφ(π) a concave function w.r.t. π. Thus any solution to Eqn.(2) hardly lies in the vertex set of
3π∗ is not necessarily deterministic. If there are two actions a1  a2 that obtain the maximum of T V (s) for a

ﬁxed s ∈ S  one can show that the stochastic policy π(a1|s) = 1 − π(a2|s) = p ∈ [0  1] is also optimal.

3

φ)−1(x).

2 ]. Since these functions are simple  we term them basic functions.

φ(x) = φ(x) + xφ(cid:48)(x) is a strictly decreasing function on (0  1) and thus (f(cid:48)

the action simplex VA (where the policy is deterministic). As a byproduct  let fφ(x) = xφ(x). Then
its derivative f(cid:48)
φ)−1(x)
exists. For simplicity  we denote gφ(x) = (f(cid:48)
There are plenty of options for the regularizer φ(·) that satisfy Assumption 1. First  entropy can be
recovered by Hφ(π) with speciﬁc φ(·). For example  when φ(x) = − log x  the Shannon entropy is
q−1 (1−xq−1) with k > 0  the Tsallis entropy is recovered. Second  there
recovered; when φ(x) = k
are many instances that are not viewed as an entropy but can serve as a regularizer. We ﬁnd two
families of such functions  namely  the exponential function family q − xkqx with k ≥ 0  q ≥ 1 and
the trigonometric function family cos(θx) − cos(θ) and sin(θ) − sin(θx) both with hyper-parameter
θ ∈ (0  π
Apart from the basic functions mentioned earlier  we come up with a generic method to combine
different basic functions. Let F be the set of all functions satisfying Assumption 1. By Proposition 1 
the operations of positive addition and minimum can preserve the properties shared among F.
Therefore  the ﬁnite-time application of such operations still leads to an available regularizer.
Proposition 1 Given φ1  φ2 ∈ F  we have αφ1 + βφ2 ∈ F for all α  β ≥ 0 and min{φ1  φ2} ∈ F.
Here we only consider those differentiable min{φ1  φ2} in theoretical analysis  because the minimum
of any two functions in F may be non-differentiable on some points. For instance  given any q > 1 
the minimum of − log(x) and q(1 − x) has a unique non-differentiable point on (0  1).
3.2 Optimality and sparsity
Once the regularizer φ(·) is given  similar to non-regularized case  the (regularized) state value and
Q-value functions of any given policy π in a regularized MDP are deﬁned as

λ (s) = E(cid:104) +∞(cid:88)

V π

t=0

(cid:12)(cid:12)(cid:12)s0 = s  π

(cid:105)

 

γt(r(st  at) + λφ(π(at|st)))

(cid:48)
λ (s

).

λ(s  a) = r(s  a) + γEa∼π(·|s)Es(cid:48)|s aV π
Qπ

(4)
Any solution to Problem (2) is call the regularized optimal policy (denoted π∗
λ). The corresponding
λ and Q∗
regularized optimal state value and Q-value are also optimal and denoted by V ∗
λ respectively.
If the context is clear  we will omit the word regularized for simplicity. In this part  we aim to show
the optimality condition for the regularized MDPs (Theorem 1). The proof of Theorem 1 is given in
Appendix B.
Theorem 1 Any optimal policy π∗
following optimality condition for all states and actions:
∗
λ(s  a) = r(s  a) + γEs(cid:48)|s aV
Q
∗
λ(a|s) = max
∗
∗
λ(s) − λ
λ (s) = µ
V

(cid:18) µ∗
(cid:48)
∗
λ (s
λ(s) − Q∗
(cid:88)
∗
λ(a|s)2φ
φ)−1(x) is strictly decreasing and µ∗

λ and its optimal state value V ∗

λ(s) is a normalization term so that

λ and Q-value Q∗

∗
λ(a|s)) 

) 
λ(s  a)

λ satisfy the

(cid:26)

(cid:27)

(cid:19)

(cid:48)

(π

π

a

  0

 

(5)

π

gφ

λ

(cid:80)
a∈A π∗

where gφ(x) = (f(cid:48)
λ(a|s) = 1.

λ(s  a) are below the threshold µ∗

f(cid:48)
Theorem 1 shows how the regularization inﬂuences the optimality condition. Let f(cid:48)
φ(x)
for short. From (5)  it can be shown that the optimal policy π∗
λ assigns zero probability to the actions
whose Q-values Q∗
λ(s) − λf(cid:48)
φ(0) and assigns positive probability to
(cid:80)
near optimal actions in proportion to their Q-values (since gφ(x) is decreasing). The threshold involves
µ∗
λ(s) and f(cid:48)
φ(0). µ∗
λ(s) can be uniquely solved from the equation obtained by plugging Eqn.(5) into
a∈A π∗
λ(s) is
actually always a multivariate ﬁnite-valued function of {Q∗
φ(0)
can be inﬁnity  making the threshold out of function. To see this  if f(cid:48)
φ(0) = ∞  the threshold will be

λ(a|s) = 1. Note that the resulting equation only involves {Q∗

λ(s  a)}a∈A. Thus µ∗
λ(s  a)}a∈A. However  the value f(cid:48)

φ(0) (cid:44) lim
x→0+

4

−∞ and all actions will be assigned positive probability in any optimal policy. To characterize the
number of zero probability actions  we deﬁne a δ-sparse policy as Deﬁnition 1 shows. It is trivial that
1|A| ≤ δ ≤ 1. For instance  a deterministic optimal policy in non-regularized MDP is 1|A|-sparse.
Deﬁnition 1 A given policy π : S → ∆A is called δ-sparse if it satisﬁes:

|{(s  a) ∈ S × A|π(a|s) (cid:54)= 0}|

|S||A|

≤ δ.

(6)

If π(a|s) > 0 for all (s  a) ∈ S × A  we call it has no sparsity.
Theorem 2 If
sparse.

f(cid:48)
φ(x) = ∞ (or 0 (cid:54)∈ domf(cid:48)

lim
x→0+

φ)  the optimal policy of regularized MDP is not

f(cid:48)
φ(x) = k

f(cid:48)
φ(x) = lim

lim
x→0+

k

1−q (qxq−1 − 1) = ∞.

f(cid:48)
φ(x) = lim
x→0+

Theorem 2 provides us a criteria to determine whether a regularization could render its corresponding
regularized optimal policy the property of sparseness. To facilitate understanding  let us see two
examples. When φ(x) = − log(x)  we have that
x→0+− log(x) − 1 = ∞  which
implies that the optimal policy of Shannon entropy-regularized MDP does not have sparsity. When
q−1 (1 − xq−1) for q > 1 and λ is small enough  the corresponding optimal policy can
φ(x) = k
be spare if λ is small enough because lim
q−1. What’s more  the sparseness property
x→0+
of Tsallis entropy still keeps for 1 < q < ∞ and small λ  which is empirically proved true in
[20]. Additionally  when 0 < q < 1  the Tsallis entropy could no longer lead to sparseness due to
lim
x→0+
The sparseness property is ﬁrst discussed in [19] which shows the Tsallis entropy with k = 1
q = 2 can devise a sparse MDP. However  we point out that any φ(·) such that f(cid:48)
proper λ leads to a sparse MDP. Let G ⊆ F be the set that satisﬁes ∀φ ∈ G  0 ∈ domf(cid:48)
combination of any two regularizers belonging to G still belongs to G.
Proposition 2 Given φ1  φ2 ∈ G  we have that αφ1 + βφ2 ∈ G for all α  β ≥ 0. However  if φ1 ∈ G
but φ2 /∈ G  αφ1 + βφ2 /∈ G for any positive β.
It is easily checked that the two families (i.e.  exponential functions and trigonometric functions)
given in Section 3.1 can also induce a sparse MDP with a proper λ. For convenience  we prefer to
q−1 (1−xq−1) that deﬁnes the Tsallis entropy as a polynomial function 
term the function φ(x) = k
because when q > 1 it is a polynomial function of degree q−1. Additionally  these three basic
families of functions could be combined to construct more complex regularizers (Propositions 1  2).
Control the Sparsity of Optimal Policy. Theorem 2 shows 0 ∈ domf(cid:48)
φ is necessary but not
sufﬁcient for that the optimal policy π∗
λ is sparse. The sparsity of optimal policy is also controlled by
λ. Theorem 3 shows how the sparsity of optimal policy can be controlled by λ when f(cid:48)
φ(0) < ∞.
The proof is detailed in Appendix E.
Theorem 3 Let Q∗
the sparsity of the optimal policy π∗
sparsity. More speciﬁcally  π∗

φ(0) < ∞. When λ → 0 
λ shrinks to δ = 1|A| . When λ → ∞  the optimal policy has no

λ(s) be deﬁned in Theorem 1 and assume f(cid:48)

2 and
φ(0) < ∞ with a
φ. The positive

λ(s  a) and µ∗

λ(a|s) → 1|A| for all (s  a) ∈ S × A as λ → ∞.

3.3 Properties of regularized MDPs

In this section  we present some properties of regularized MDPs. We ﬁrst prove the uniqueness of the
optimal policy and value. Next  we give the bound of the performance error between π∗
λ (the optimal
policy obtained by a regularized MDP) and π∗ (the policy obtained by the original MDP). In the
proofs of this section  we need an additional assumption for regularizers. Assumption 2 is quite weak.
All the functions introduced in Section 3.1 satisfy it.

Assumption 2 The regularizer φ(·) satisﬁes fφ(0) (cid:44) lim
x→0+

xφ(x) = 0.

5

Generic Bellman Operator Tλ We deﬁne a new operator Tλ for regularized MDPs  which deﬁnes a
smoothed maximum. Given one state s ∈ S and current value function Vλ  Tλ is deﬁned as
(7)

(cid:88)

π(a|s) [Qλ(s  a)+λφ(π(a|s))]  

TλVλ(s) (cid:44) max

π

a

where Qλ(s  a) = r(s  a) + γEs(cid:48)|s aVλ(s(cid:48)) is Q-value function derived from one-step foreseeing
according to Vλ. By deﬁnition  Tλ maps Vλ(s) to its possible highest value which considers both
future discounted rewards and regularization term. We provide simple upper and lower bounds of Tλ
w.r.t. T   i.e. 
Theorem 4 Under Assumptions 1 and 2  for any value function V and s ∈ S  we have

T V (s) ≤ TλV (s) ≤ T V (s) + λφ(

).

(8)

1
|A|

λ

(cid:54)= V ∗

λ are both unique. We formally state the conclusion and give the proof in Appendix C.

The bound (8) shows that Tλ is a bounded and smooth approximation of T . When λ = 0  Tλ
degenerates to the Bellman operator T . Moreover  it can be proved that Tλ is a γ-contraction. By the
Banach ﬁxed point theorem [35]  V ∗
λ   the ﬁxed point of Tλ  is unique. As a result of Theorem 1  Q∗
and π∗
Performance Error Between V ∗
λ and V ∗ In general  V ∗
λ . But their difference is controlled
by both λ and φ(·). The behavior of φ(x) around the origin represents the regularization ability of
φ(x). Theorem 5 shows that when |A| is quite large (which means φ( 1|A|) is close to φ(0) due to
its continuity)  the closeness of φ(0) to 0 also determines their difference. As a result  the Tsallis
entropy regularized MDPs have always tighter error bounds than the Shannon entropy regularized
q−1 (1 − xq−1)(q > 1) is much lower
MDPs  because the value at the origin of the concave function k
than that of − log x  both function satisfying in Assumption 2. Our theory incorporates the result of
Lee et al. [19  20] which shows a similar performance error for (general) Tsallis entropy RL. The
proof of Theorem 5 is detailed in Appendix D.
λ and V ∗ can be bounded as
Theorem 5 Under Assumptions 1 and 2  the error between V ∗
1
|A|

λ
1 − γ

∗
λ − V

(cid:107)∞ ≤

(cid:107)V

φ(

).

∗

4 Regularized Actor-Critic

To solve the problem (2) in complex environments  we propose an off-policy algorithm Regularized
Actor-Critic (RAC)  which alternates between policy evaluation and policy improvement. In practice 
we apply neural networks to parameterize the Q-value and policy to increase expressive power. In
particular  we model the regularized Q-value function Qθ(s  a) and a tractable policy πψ(a|s). We use
Adam [17] to optimize ψ  θ. Actually  RAC is created by consulting the previous work SAC [13  14]
and making some necessary changes so that it is able to be agnostic to the form of regularization.
The goal for training regularized Q-value parameters is to minimize the general Bellman residual:

(9)
where D is the replay buffer used to eliminate the correlation of sampled trajectory data and y is the
target function deﬁned as follows

ˆED(Qθ(st  at) − y)2 

JQ(θ) =

1
2

y = r(st  at)+γ [Q¯θ(st+1  at+1)+λφ(πψ(at+1|st+1))] .

The target involves a target regularized Q-value function with parameters ¯θ that are updated in a
moving average fashion  which can stabilize the training process [24  13]. Thus the gradient of JQ(θ)
w.r.t. θ can be estimated by

ˆ∇JQ(θ) = ˆED∇θQθ(st  at) (Qθ(st  at)−y) .
For training policy parameters  we minimize the negative total reward:

Jπ(ψ) = ˆED(cid:2)Ea∼πψ(·|st) [−λφ(πψ(a|st)) − Qθ(st  φ(πψ(a|st)))](cid:3) .

(10)

6

RAC is formally described in Algorithm 1. The method alternates between data collection and
parameter updating. Trajectory data is collected by executing the current policy in environments and
then stored in a replay buffer. Parameters of the function approximators are updated by descending
along the stochastic gradients computed from the batch sampled from that replay buffer. The
method makes use of two Q-functions to overcome the positive bias incurred by overestimation of
Q-value  which is known to yield a poor performance [15  9]. Speciﬁcally  these two Q-functions
are parametrized by different parameters θi and are independently trained to minimize JQ(θi). The
minimum of these two Q-functions is used to compute the target value y which is involved in the
computation of ˆ∇JQ(θ) and ˆ∇Jπ(ψ).

5 Experiments

We investigate the performance of different reg-
ularizers among diverse environments. We ﬁrst
test basic and combined regularizers in two nu-
merical environments. Then we test basic reg-
ularizers in Atari discrete problems. In the end 
we explore the possible application in Mujoco
control environments.

5.1 Numerical results

Algorithm 1 Regularized Actor-Critic (RAC)

Input: θ1  θ2  ψ
Initialization: ¯θ1 ← θ1  ¯θ1 ← θ2 D ← ∅
for each iteration do
for each environment step do
sample action  at ∼ πψ(·|st)
receive reward rt ∼ rt(st  at)
receive next state st+1 from environment
{(st  at  rt  st+1)}
D ← D

(cid:83)

end for
for each gradient step do

end for

end for
Output: θ1  θ2  ψ

θi ← θi − ηQ ˆ∇JQ(θi) for i ∈ {1  2}
ψ ← ψ − ηπ ˆ∇Jπ(ψ)
¯θi ← τ θi + (1 − τ )¯θi for i ∈ {1  2}

The two discrete numerical environments we
consider include a simple random generated
MDP (S = 50  A = 10) and a Gridworld en-
vironment (S = 81  A = 4). Refer to Ap-
pendix H.1 for more detail settings.
Regularizers. Four basic regularizers include
shannon (− log x)  tsallis ( 1
2 (1 − x))  cos
2 x)) and exp (exp(1) − exp(x)). Propo-
(cos( π
sition 1 and 2 allow three combined regulariz-
ers: (1) min:
the minimum of tsallis and
shannon  i.e.  min{− log(x)  2(1 − x)}  (2) poly: the positive addition of two polynomial func-
tions  i.e.  1
2 (1 − x) + (1 − x2)  and (3) mix: the positive addition of tsallis and shannon  i.e. 
2 (1 − x).
− log(x) + 1
Sparsity and Convergence. From (a)(b) in Figure 1  when λ is extremely large  δ = 1 for all
regularizers. (c) shows how the probability of each action in the optimal policy at a given state
varies with λ (one curve represents one action). These results validate the Theorem 3. A reasonable
explanation is that large λ reduces the importance of discounted reward sum and makes Hφ(π)
dominate the loss  which forces the optimal policy to put probability mass evenly on all actions in
order to maximize Hφ(π). We regard the ability to defend the tendency towards converging to a
uniform distribution as sparseness power. From our additional experiments in Appendix H  cos has
the strongest sparseness power. (d) shows the convergence speed of RPI on different regularizers. It
also shows that (cid:107)V ∗
5.2 Atari results

λ(cid:107)∞ is bounded as Theorem 4 states.

− V π∗

Regularizers. We test four basic regularizers across four discrete control tasks from OpenAI Gym
benchmark [5]. All the training details are in Appendix H.2.
Performance. Figure 2 shows the score during training for RAC with four regularization forms with
best performance over λ = {0.01  0.1  1.0}. Except Breakout  Shannon performs worse than other
three regularizers. Cos performs best in Alien and Seaquest while tsallis performs best in Boxing
and exp performs quite normally. Appendix H.2 gives all the results with different λ and sensitive
analysis. In general  shannon is the most insensitive among others.

7

(a) Random MDP

(b) Gridworld

(c) cos: cos( π

2 x)

(d) Error of different regular-
izers on Random MDP

Figure 1: (a) and (b) show the results of the sparsity δ of optimal policies on Random MDP and
Gridworld. (c) shows the changing process of the probability of each action in optimal policy
regularized by cos( π

2 x) on Random MDP. (d) shows the (cid:96)∞-error between V ∗ and V π∗
λ.

(a) Alien

(b) Boxing

(c) Breakout

(d) Seaquest

Figure 2: Training curves on Atari games. Each entry in the legend is named with the rule
the regularization form + λ. The score is smoothed with 100 windows while the shaded
area is the one standard deviation.

5.3 Mujoco results

Regularizers. We explore basic regularizers across four continuous control tasks from OpenAI Gym
benchmark [5] with the MuJoCo simulator [38]. Unfortunately cos is quite unstable and prone
to gradient exploding problems in deep RL training process. We speculate it instableness roots in
numerical issues where the probability density function often diverges into inﬁnity. What’s more 
the periodicity of cos( π
2 x) makes the gradients vacillate and the algorithm hard to converge. All the
details of the following experiments are given in Appendix H.3.

(a) Ant-v2

(b) Walker-v2

(c) Hopper-v2

(d) HalfCheetah-v2

Figure 3: Training curves on continuous control benchmarks. Each curve is the average
of four experiments with different seeds. Each entry in the legend is named with the rule
the regularization form + λ. The score is smoothed with 30 windows while the shaded
area is the one standard deviation.

Performance. Figure 3 shows the total average return of rollouts during training for RAC with
three regularization forms and different regularization coefﬁcients ([0.01  0.1  1]). For each curve 
we train four different instances with different random seeds. Tsallis performs steadily better than
shannon given the same regularization coefﬁcient λ. Tsallis is also more stable since its shaded
area is thinner than shannon. Exp performs almost as good as tsallis in Ant-v2 and Hopper-v2 but
performs badly in the rest two environments. From the sensitivity analysis provided in Appendix H.3 
tsallis is less sensitive to λ than cos and shannon.

8

0246810RegularizationCoeﬃcients0.20.40.60.81.0Sparsitymin:min{−log(x) 2(1−x)}poly:12(1−x)+(1−x2)cos:cos(π2x)exp:exp(1)−exp(x)shannon:−log(x)mix:−log(x)+12(1−x)tsallis:12(1−x)0246810RegularizationCoeﬃcients0.50.60.70.80.91.0Sparsity0246810Regularization Coefficients0.00.10.20.30.40.5Optimal Policy Probability020406080100Iteration0.00.20.40.60.81.0ErrorBoundshannon:−log(x)tsallis:12(1−x)min:min{−log(x) 2(1−x)}exp:exp(1)−exp(x)cos:cos(π2x)poly:12(1−x)+(1−x2)mix:−log(x)+12(1−x)0246810Frames(M)025050075010001250150017502000Scorecosx-0.1expx-0.1shannon-0.1tsallis-1.00246810Frames(M)020406080100Scorecosx-1.0expx-0.1shannon-0.1tsallis-0.10246810Frames(M)0100200300400500600Scorecosx-0.1expx-0.01shannon-0.1tsallis-0.10246810Frames(M)020004000600080001000012000Scorecosx-0.01expx-0.1shannon-0.01tsallis-0.1050010001500200025003000Iterarion01000200030004000500060007000Average rewardexpx-0.01shannon-0.1tsallis-0.01050010001500200025003000Iterarion0100020003000400050006000Average rewardexpx-0.01shannon-0.1tsallis-0.0102004006008001000Iterarion050010001500200025003000Average rewardexpx-1shannon-1tsallis-1050010001500200025003000Iterarion02000400060008000100001200014000Average rewardexpx-0.01shannon-0.1tsallis-0.016 Conclusion

In this paper  we have proposed a uniﬁed framework for regularized reinforcement learning  which
includes entropy-regularized RL as a special case. Under this framework  the regularization function
characterizes the optimal policy and value of the corresponding regularized MDPs. We have shown
there are many regularization functions that can lead to a sparse but multi-modal optimal policy such
as trigonometric and exponential functions. We have speciﬁed a necessary and sufﬁcient condition
for these regularization functions that could lead to sparse optimal policies and how the sparsity is
controlled with λ. We have presented the logical and mathematical foundations of these properties
and also conducted the experimental results.

Acknowledgements

This work is sponsored by the Key Project of MOST of China (No. 2018AAA0101000)  by Beijing
Municipal Commission of Science and Technology under Grant No. 181100008918005  and by
Beijing Academy of Artiﬁcial Intelligence (BAAI).

References
[1] Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 243–252. JMLR. org  2017.

[2] R Bellmann. Dynamic programming princeton university press. Princeton  NJ  1957.

[3] Boris Belousov and Jan Peters. f-divergence constrained policy improvement. arXiv preprint

arXiv:1801.00056  2017.

[4] Dimitri P Bertsekas. Neuro-dynamic programming. In Encyclopedia of optimization  pages

2555–2560. Springer  2008.

[5] Greg Brockman  Vicki Cheung  Ludwig Pettersson  Jonas Schneider  John Schulman  Jie Tang 

and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540  2016.

[6] Bo Dai  Albert Shaw  Lihong Li  Lin Xiao  Niao He  Zhen Liu  Jianshu Chen  and Le Song.
Sbeed: Convergent reinforcement learning with nonlinear function approximation. In Interna-
tional Conference on Machine Learning  pages 1133–1142  2018.

[7] Amir M Farahmand  Mohammad Ghavamzadeh  Shie Mannor  and Csaba Szepesvári. Regular-
ized policy iteration. In Advances in Neural Information Processing Systems  pages 441–448 
2009.

[8] Roy Fox  Ari Pakman  and Naftali Tishby. Taming the noise in reinforcement learning via soft

updates. arXiv preprint arXiv:1512.08562  2015.

[9] Scott Fujimoto  Herke van Hoof  and Dave Meger. Addressing function approximation error in

actor-critic methods. arXiv preprint arXiv:1802.09477  2018.

[10] Matthieu Geist  Bruno Scherrer  and Olivier Pietquin. A theory of regularized markov decision

processes. arXiv preprint arXiv:1901.11275  2019.

[11] Jordi Grau-Moya  Felix Leibfried  and Peter Vrancx. Soft q-learning with mutual-information

regularization. 2018.

[12] Tuomas Haarnoja  Haoran Tang  Pieter Abbeel  and Sergey Levine. Reinforcement learning

with deep energy-based policies. arXiv preprint arXiv:1702.08165  2017.

[13] Tuomas Haarnoja  Aurick Zhou  Pieter Abbeel  and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290  2018.

9

[14] Tuomas Haarnoja  Aurick Zhou  Kristian Hartikainen  George Tucker  Sehoon Ha  Jie Tan 
Vikash Kumar  Henry Zhu  Abhishek Gupta  Pieter Abbeel  et al. Soft actor-critic algorithms
and applications. arXiv preprint arXiv:1812.05905  2018.

[15] Hado V Hasselt. Double q-learning. In Advances in Neural Information Processing Systems 

pages 2613–2621  2010.

[16] Jeffrey Johns  Christopher Painter-Wakeﬁeld  and Ronald Parr. Linear complementarity for
regularized policy evaluation and improvement. In Advances in neural information processing
systems  pages 1009–1017  2010.

[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[18] J Zico Kolter and Andrew Y Ng. Regularization and feature selection in least-squares temporal
difference learning. In Proceedings of the 26th annual international conference on machine
learning  pages 521–528. ACM  2009.

[19] Kyungjae Lee  Sungjoon Choi  and Songhwai Oh. Sparse markov decision processes with
causal sparse tsallis entropy regularization for reinforcement learning. IEEE Robotics and
Automation Letters  3(3):1466–1473  2018.

[20] Kyungjae Lee  Sungyub Kim  Sungbin Lim  Sungjoon Choi  and Songhwai Oh. Tsallis
reinforcement learning: A uniﬁed framework for maximum entropy reinforcement learning.
arXiv preprint arXiv:1902.00137  2019.

[21] Yang Liu  Prajit Ramachandran  Qiang Liu  and Jian Peng. Stein variational policy gradient.

arXiv preprint arXiv:1704.02399  2017.

[22] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored ap-
proximate curvature. In International conference on machine learning  pages 2408–2417 
2015.

[23] Amir Massoud Farahmand  Mohammad Ghavamzadeh  Csaba Szepesvári  and Shie Mannor.
Regularized ﬁtted q-iteration for planning in continuous-space markovian decision problems.
In American Control Conference  2009. ACC’09.  pages 725–730. IEEE  2009.

[24] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[25] Oﬁr Nachum  Mohammad Norouzi  Kelvin Xu  and Dale Schuurmans. Bridging the gap
between value and policy based reinforcement learning. In Advances in Neural Information
Processing Systems  pages 2775–2785  2017.

[26] Oﬁr Nachum  Mohammad Norouzi  Kelvin Xu  and Dale Schuurmans. Trust-pcl: An off-policy

trust region method for continuous control. arXiv preprint arXiv:1707.01891  2017.

[27] Oﬁr Nachum  Yinlam Chow  and Mohammad Ghavamzadeh. Path consistency learning in tsallis

entropy regularized mdps. arXiv preprint arXiv:1802.03501  2018.

[28] Gergely Neu  Anders Jonsson  and Vicenç Gómez. A uniﬁed view of entropy-regularized

markov decision processes. arXiv preprint arXiv:1705.07798  2017.

[29] Brendan O’Donoghue  Remi Munos  Koray Kavukcuoglu  and Volodymyr Mnih. Combining

policy gradient and q-learning. arXiv preprint arXiv:1611.01626  2016.

[30] Jan Peters  Katharina Mülling  and Yasemin Altun. Relative entropy policy search. In AAAI 

pages 1607–1612. Atlanta  2010.

[31] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons  2014.

10

[32] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889–1897 
2015.

[33] John Schulman  Xi Chen  and Pieter Abbeel. Equivalence between policy gradients and soft

q-learning. arXiv preprint arXiv:1704.06440  2017.

[34] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

[35] David Roger Smart. Fixed point theorems  volume 66. CUP Archive  1980.

[36] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

2018.

[37] Emanuel Todorov. Linearly-solvable markov decision problems.

information processing systems  pages 1369–1376  2007.

In Advances in neural

[38] Emanuel Todorov  Tom Erez  and Yuval Tassa. Mujoco: A physics engine for model-based
control. In Intelligent Robots and Systems (IROS)  2012 IEEE/RSJ International Conference on 
pages 5026–5033. IEEE  2012.

[39] Constantino Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of statistical

physics  52(1-2):479–487  1988.

[40] Peter Vamplew  Richard Dazeley  and Cameron Foale. Softmax exploration strategies for

multiobjective reinforcement learning. Neurocomputing  263:74–86  2017.

[41] Lieven Vandenberghe. The cvxopt linear and quadratic cone program solvers. Online:

http://cvxopt. org/documentation/coneprog. pdf  2010.

11

,Wenhao Yang
Xiang Li
Zhihua Zhang