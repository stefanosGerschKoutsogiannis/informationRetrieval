2019,Learning Nearest Neighbor Graphs from Noisy Distance Samples,We consider the problem of learning the nearest neighbor graph of a dataset of n items. The metric is unknown  but we can query an oracle to obtain a noisy estimate of the distance between any pair of items. This framework applies to problem domains where one wants to learn people's preferences from responses commonly modeled as noisy distance judgments. In this paper  we propose an active algorithm to find the graph with high probability and analyze its query complexity. In contrast to existing work that forces Euclidean structure  our method is valid for general metrics  assuming only symmetry and the triangle inequality. Furthermore  we demonstrate efficiency of our method empirically and theoretically  needing only O(n\log(n)\Delta^{-2}) queries in favorable settings  where \Delta^{-2} accounts for the effect of noise. Using crowd-sourced data collected for a subset of the UT~Zappos50K dataset  we apply our algorithm to learn which shoes people believe are most similar and show that it beats both an active baseline and ordinal embedding.,Learning Nearest Neighbor Graphs from Noisy

Distance Samples

Blake Mason ⇤

University of Wisconsin

Madison  WI 53706
bmason3@wisc.edu

Ardhendu Tripathy ⇤
University of Wisconsin

Madison  WI 53706

astripathy@wisc.edu

Robert Nowak

University of Wisconsin

Madison  WI 53706
rdnowak@wisc.edu

Abstract

We consider the problem of learning the nearest neighbor graph of a dataset of n
items. The metric is unknown  but we can query an oracle to obtain a noisy estimate
of the distance between any pair of items. This framework applies to problem
domains where one wants to learn people’s preferences from responses commonly
modeled as noisy distance judgments. In this paper  we propose an active algorithm
to ﬁnd the graph with high probability and analyze its query complexity. In contrast
to existing work that forces Euclidean structure  our method is valid for general
metrics  assuming only symmetry and the triangle inequality. Furthermore  we
demonstrate efﬁciency of our method empirically and theoretically  needing only
O(n log(n)2) queries in favorable settings  where 2 accounts for the effect
of noise. Using crowd-sourced data collected for a subset of the UT Zappos50K
dataset  we apply our algorithm to learn which shoes people believe are most
similar and show that it beats both an active baseline and ordinal embedding.

1

Introduction

In modern machine learning applications  we frequently seek to learn proximity/ similarity relation-
ships between a set of items given only noisy access to pairwise distances. For instance  practitioners
wishing to estimate internet topology frequently collect one-way-delay measurements to estimate the
distance between a pair of hosts [9]. Such measurements are affected by physical constraints as well as
server load  and are often noisy. Researchers studying movement in hospitals from WiFi localization
data likewise contend with noisy distance measurements due to both temporal variability and varying
signal strengths inside the building [4]. Additionally  human judgments are commonly modeled as
noisy distances [26  23]. As an example  Amazon Discover asks customers their preferences about
different products and uses this information to recommend new items it believes are similar based
on this feedback. We are often primarily interested in the closest or most similar item to a given
one– e.g.  the closest server  the closest doctor  the most similar product. The particular item of
interest may not be known a priori. Internet trafﬁc can ﬂuctuate  different patients may suddenly need
attention  and customers may be looking for different products. To handle this  we must learn the
closest/ most similar item for each item. This paper introduces the problem of learning the Nearest
Neighbor Graph that connects each item to its nearest neighbor from noisy distance measurements.
Problem Statement: Consider a set of n points X = {x1 ···   xn} in a metric space. The metric
is unknown  but we can query a stochastic oracle for an estimate of any pairwise distance. In as few
queries as possible  we seek to learn a nearest neighbor graph of X that is correct with probability
1    where each xi is a vertex and has a directed edge to its nearest neighbor xi⇤ 2X \ { xi}.
⇤Authors contributed equally to this paper and are listed alphabetically.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Related work
Nearest neighbor problems (from noiseless measurements) are well studied and we direct the reader to
[3] for a survey. [6  30  25] all provide theory and algorithms to learn the nearest neighbor graph which
apply in the noiseless regime. Note that the problem in the noiseless setting is very different. If noise
corrupts measurements  the methods from the noiseless setting can suffer persistent errors. There has
been recent interest in introducing noise via subsampling for a variety of distance problems [24  1  2] 
though the noise here is not actually part of the data but introduced for efﬁciency. In our algorithm 
we use the triangle inequality to get tighter estimates of noisy distances in a process equivalent to the
classical Floyd–Warshall [11  7]. This has strong connections to the metric repair literature [5  13]
where one seeks to alter a set of noisy distance measurements as little as possible to learn a metric
satisfying the standard axioms. [27] similarly uses the triangle inequality to bound unknown distances
in a related but noiseless setting. In the speciﬁc case of noisy distances corresponding to human
judgments  a number of algorithms have been proposed to handle related problems  most notably
Euclidean embedding techniques  e.g. [17  31  23]. To reduce the load on human subjects  several
attempts at an active method for learning Euclidean embeddings have been made but have only seen
limited success [20]. Among the culprits is the strict and often unrealistic modeling assumption that
the metric be Euclidean and low dimensional. In the particular case that the algorithm may query
triplets (e.g.  “is i or j closer to k?”) and receive noisy responses  [22] develop an interesting  passive
technique under general metrics for learning a relative neighborhood graph which is an undirected
relaxation of a nearest neighbor graph.

1.2 Main contributions
In this paper we introduce the problem of identifying the nearest neighbor graph from noisy distance
samples and propose ANNTri  an active algorithm  to solve it for general metrics. We empirically and
theoretically analyze its complexity to show improved performance over a passive and an active base-
line. In favorable settings  such as when the data forms clusters  ANNTri needs only O(n log(n)/2)
queries  where  accounts for the effect of noise. Furthermore  we show that ANNTri achieves
superior performance compared to methods which require much stronger assumptions. We highlight
two such examples. In Fig. 2c  for an embedding in R2  ANNTri outperforms the common technique
of triangulation that works by estimating each point’s distance to a set of anchors. In Fig. 3b  we
show that ANNTri likewise outperforms Euclidean embedding for predicting which images are most
similar from a set of similarity judgments collected on Amazon Mechanical Turk. The rest of the
paper is organized as follows. In Section 2  we further setup the problem. In Sections 3 and 4 we
present the algorithm and analyze its theoretical properties. In Section 5 we show ANNTri’s empirical
performance on both simulated and real data. In particular  we highlight its efﬁciency in learning
from human judgments.

Q(i  j)

di j + ⌘ 

yields a realization of

2 Problem setup and summary of our approach
We denote distances as di j where d : X⇥X! R0 is a distance function satisfying the standard
axioms and deﬁne xi⇤ := arg minx2X\{xi} d(xi  x). Though the distances are unknown  we are able
to draw independent samples of its true value according to a stochastic distance oracle  i.e. querying
(1)
where ⌘ is a zero-mean subGaussian random variable assumed to have scale parameter  = 1. We let
ˆdi j(t) denote the empirical mean of the values returned by Q(i  j) queries made until time t. The
number of Q(i  j) queries made until time t is denoted as Ti j(t). A possible approach to obtain the

2 pairs and report xi⇤(t) = arg minj6=i ˆdi j(t) for
nearest neighbor graph is to repeatedly query alln
all i 2 [n]. But since we only wish to learn xi⇤8i  if di k  di i⇤  we do not need to query Q(i  k)
as many times as Q(i  i⇤). To improve our query efﬁciency  we could instead adaptively sample to
focus queries on distances that we estimate are smaller. A simple adaptive method to ﬁnd the nearest
neighbor graph would be to iterate over x1  x2  . . .   xn and use a best-arm identiﬁcation algorithm
to ﬁnd xi⇤ in the ith round.1 However  this procedure treats each round independently  ignoring
properties of metric spaces that allow information to be shared between rounds.

1We could also proceed in a non-iterative manner  by adaptively choosing which amongn

next. However this has worse empirical performance and same theoretical guarantees as the in-order approach.

2 pairs to query

2

• Due to symmetry  for any i < j the queries Q(i  j) and Q(j  i) follow the same law  and we

can reuse values of Q(i  j) collected in the ith round while ﬁnding xj⇤ in the jth round.

• Using concentration bounds on di j and di k from samples from Q(i  j) and Q(i  k) collected
in the ith round  we can bound dj k via the triangle inequality. As a result  we may be able
to state xk 6= xj⇤ without even querying Q(j  k).

Our proposed algorithm ANNTri uses all the above ideas to ﬁnd the nearest neighbor graph of X . For
general X   the sample complexity of ANNTri contains a problem-dependent term that involves the
order in which the nearest neighbors are found. For an X consisting of sufﬁciently well separated
clusters  this order-dependence for the sample complexity does not exist.

3 Algorithm

Our proposed algorithm (Algorithm 1) ANNTri ﬁnds the nearest neighbor graph of X with probability
1  . It iterates over xj 2X in order of their subscript index and ﬁnds xj⇤ in the jth ‘round’. All
bounds  counts of samples  and empirical means are stored in n ⇥ n symmetric matrices in order
to share information between different rounds. We use Python array/Matlab notation to indicate
individual entries in the matrices  for e.g.  ˆd[i  j] = ˆdi j(t). The number of Q(i  j) queries made is
queried is stored in the (i  j)th entry of T . Matrices U and L record upper and lower conﬁdence
bounds on di j. U4 and L4 record the associated triangle inequality bounds. Symmetry is ensured
by updating the (j  i)th entry at the same time as the (i  j)th entry for each of the above matrices. In
the jth round  ANNTri ﬁnds the correct xj⇤ with probability 1 /n by calling SETri (Algorithm 2) 
a modiﬁcation of the successive elimination algorithm for best-arm identiﬁcation. In contrast to
standard successive elimination  at each time step SETri only samples those points in the active set
that have the fewest number of samples.

as n ⇥ n matrices where each entry is 1  NN as a length n array

for i = 1 to n do {ﬁnd tightest triangle bounds}

Algorithm 1 ANNTri
Require: n  procedure SETri (Alg. 2)  conﬁdence 
1: Initialize ˆd  T as n⇥ n matrices of zeros  U  U4 as n⇥ n matrices where each entry is 1  L  L4
2: for j = 1 to n do
3:
4:
i k   see (7)
5:
i k   see (8)
6:
7: NN[j] = SETri(j  ˆd  U  U4  L  L4  T ⇠ = /n)
8: return The nearest neighbor graph adjacency list NN

for all k 6= i do
Set U4[i  k]  U4[k  i]  min` U4`
Set L4[i  k]  L4[k  i] max` L4`

conﬁdence ⇠

Algorithm 2 SETri
Require: index j  callable oracle Q(· ·) (Eq. (1))  six n ⇥ n matrices: ˆd  U  U4  L  L4  T  
1: Initialize active set Aj { a 6= j : max{L[a  j]  L4[a  j]} < mink min{U [j  k]  U4[j  k]}}
2: while |Aj| > 1 do
for all i 2A j such that T [i  j] = mink2Aj T [i  k] do {only query points with fewest samples}
3:
Update ˆd[i  j]  ˆd[j  i] ( ˆd[i  j] · T [i  j] + Q(i  j))/(T [i  j] + 1)
4:
Update T [i  j]  T [j  i] T [i  j] + 1
5:
Update U [i  j]  U [j  i] ˆd[i  j] + C⇠(T [i  j])
6:
Update L[i  j]  L[j  i] ˆd[i  j]  C⇠(T [i  j])
7:
Update Aj { a 6= j : max{L[a  j]  L4[a  j]} < mink min{U [j  k]  U4[j  k]}}
8:
9: return The index i for which xi 2A j

3

3.1 Conﬁdence bounds on the distances
Using the subGaussian assumption on the noise random process  we can use Hoeffding’s inequality
and a union bound over time to get the following conﬁdence intervals on the distance dj k:

| ˆdj k(t)  dj k| s2

log(4n2(Tj k(t))2/)

Tj k(t)

=: C/n(Tj k(t)) 

(2)

which hold for all points xk 2X \ { xj} at all times t with probability 1  /n  i.e.

(3)
where Li j(t) := ˆdi j(t)  C/n(Ti j(t)) and Ui j(t) := ˆdi j(t) + C/n(Ti j(t)). [10] use the above
procedure to derive the following upper bound for the number of oracle queries used to ﬁnd xj⇤:

P(8t 2 N 8i 6= j  di j 2 [Li j(t)  Ui j(t)])  1  /n 

O0@Xk6=j

log(n2/(j k))

2

j k

1A  

(4)

where for any xk /2{ xj  xj⇤} the suboptimality gap j k := dj k  dj j⇤ characterizes how hard it
is to rule out xk from being the nearest neighbor. We also set j j⇤ := mink /2{j j⇤} j k. Note that
one can use tighter conﬁdence bounds as detailed in [12] and [18] to obtain sharper bounds on the
sample complexity of this subroutine.

3.2 Computing the triangle bounds and active set Aj(t)
Since Aj(·) is only computed within SETri  we abuse notation and use its argument t to indicate
the time counter private to SETri. Thus  the initial active set computed by SETri when called in
the jth round is denoted Aj(0). During the jth round  the active set Aj(t) contains all points that
have not been eliminated from being the nearest neighbor of xj at time t. In what follows  we add a
superscript 4 to denote a bound obtained via the triangle inequality  whose precise deﬁnitions are
given in Lemma 3.1. We deﬁne xj’s active set at time t as

Aj(t) := {a 6= j : max{La j(t)  L4a j(t)} < min

k

min{Uj k(t)  U4j k(t)}}.

(5)

Assuming L4a j(t) and U4j k(t) are valid lower and upper bounds on da j  dj k respectively  (5) states
that point xa is active if its lower bound is less than the minimum upper bound for dj k over all
choices of xk 6= xj. Next  for any (j  k) we construct triangle bounds L4  U4 on the distance dj k.
Intuitively  for some reals g  g0  h  h0  if di j 2 [g  g0] and di k 2 [h  h0] then dj k  g0 + h0  and
dj k | di j  di k| = max{di j  di k} min{di j  di k} (max{g  h} min{g0  h0})+
(6)
where (s)+:= max{s  0}. The lower bound can be seen as true by Fig. 7 in the Appendix. Lemma 3.1
uses these ideas to form upper and lower bounds on distances by the triangle inequality. Note that
this deﬁnition is inherently recursive as it may rely on past triangle inequality bounds to achieve the
tightest possible result. We denote a triangle inequality upper and lower bounds on dj k due to a point
i at time t as U4i
Lemma 3.1. For all k 6= 1  U41
min

1 k (t) = U41 k(t) = U1 k(t). For any i < j deﬁne
i j (t)} + min{Ui k(t)  U4i2

(min{Ui j(t)  U4i1

j k respectively.

j k and L4i

i k (t)}).

j k (t) :=

U4i

(7)

max{i1 i2}<i

For all k 6= 1  L41

1 k(t) = L41 k(t) = L1 k(t). For any i < j deﬁne

L4i

j k(t) :=

max

max{i1 i2 i3 i4}<i⇣ max{Li j(t)  L4i1
 min{Ui j(t)  U4i3

i j (t)  Li k(t)  L4i2

i k (t)}
i j (t)  Ui k(t)  U4i4

i k (t)}⌘+

 

(8)

where (s)+ := max{s  0}. If all the bounds obtained by SETri in rounds i < j are correct then
dj k 2⇥L4j k(t)  U4j k(t)⇤ 
U4i
j k (t).

where L4j k(t) := max

and U4j k(t) := min

j k(t)

L4i

i<j

i<j

4

j k and U4i

The proof is in Appendix B.1. ANNTri has access to two sources of bounds on distances: concentration
bounds and triangle inequality bounds  and as can be seen in Lemma 3.1  the former affects the
latter. Furthermore  triangle bounds are computed from other triangle bounds  leading to the recursive
deﬁnitions of L4i
j k . Because of these facts  triangle bounds are dependent on the order
in which ANNTri ﬁnds each nearest neighbor. These bounds can be computed using dynamic
programming and brute force search over all possible i1  i2  i3  i4 is not necessary. We note that the
above recursion is similar to the Floyd-Warshall algorithm for ﬁnding shortest paths between all pairs
of vertices in a weighted graph [11  7]. The results in [27] show that the triangle bounds obtained in
this manner have the minimum L1 norm between the upper and lower bound matrices.
Extension to k-Nearest Neighbor Graphs: All algorithms and theory in this paper can be extended
to the case of k-nearest neighbor graphs where one wishes to draw a directed edge from each point
to all of its k-nearest neighbors. To modify ANNTri  one can change the subroutine SETri to be a
variant of the KL-Racing algorithm by [21]  for instance. Racing style algorithms are the natural
extension of Successive-Elimination style algorithms to the top-k bandit setting. To achieve best
complexity  in this case  one would again want to sample the distances with the only minimum
number of calls to the oracle ﬁrst  as in Line 3 of SETri. To make a statement similar to Theorem 4.4 
i.e. to bound the complexity of learning k-NN graphs  it is necessary to alter the deﬁnition of the
events Aj k so as to certify that a point has been eliminated from being a k-nearest neighbor as
opposed to a 1-nearest neighbor in the current form. The suboptimality gaps j k (and therefore
Hj k) would be deﬁned differently for the k-nearest neighbor case leading to a different bound. A
similar statement as Theorem 4.6 can likewise be achieved as long as k < log(n) and one should

expect a complexity of O⇣kn log(n)2⌘ for an appropriately deﬁned 2.

4 Analysis

All omitted proofs of this section can be found in the Appendix Section B.
Theorem 4.1. ANNTri ﬁnds the nearest neighbor for each point in X with probability 1  .
4.1 A simpliﬁed algorithm

The following Lemma indicates which points must be eliminated initially in the jth round.
Lemma 4.2. If 9i : 2Ui j < Li k  then xk /2A j(0) for ANNTri.
Proof. 2Ui j < Li k () Ui j < Li k  Ui j  L4i
Next  we deﬁne ANNEasy  a simpliﬁed version of ANNTri that is more amenable to analysis. Here 
we say that xk is eliminated in the jth round of ANNEasy if i) k<j and 9i : Ui j < Lj k (symmetry
from past samples) or ii) 9i : 2Ui j < Li k (Lemma 4.2). Therefore  xj’s active set for ANNEasy is
(9)

Aj = {a6=j : La k  2Uj k 8k and La j < min

Uj k}.

j k

k

To deﬁne ANNEasy in code  we remove lines 3-6 of ANNTri (Algorithm 1)  and call a subroutine
SEEasy in place of SETri. SEEasy matches SETri (Algorithm 2) except that lines 1 and 8 are
replaced with (9) instead. We provide full pseudocode of both ANNEasy and SEEasy in the Appendix
A.1.1. Though ANNEasy is a simpliﬁcation for analysis  we note that it empirically captures much of
the same behavior of ANNTri. In the Appendix A.1.2 we provide an empirical comparison of the two.

4.2 Complexity of ANNEasy
We now turn our attention to account for the effect of the triangle inequality in ANNEasy.
Lemma 4.3. For any xk 2X if the following conditions hold for some i < j  then xk /2A j(0).

(10)

6C/n(1)  di k  2di j

and

{j  k}\ ([m<i{` : 2dm i < dm `}) = ;.

The ﬁrst condition characterizes which xk’s must satisfy the condition in Lemma 4.2 for the jth
round. The second guarantees that xk was sampled in the ith round  a necessary condition for forming
triangle bounds using xi.

5

C1

C3

C2

C1 [C 2
C2

C1

C3 [C 4
C4

C3

(a) Clustered data
Figure 1: Example datasets where triangle inequalities lead to provable gains.

(b) Hierarchical clusters

Theorem 4.4. Conditioned on the event that all conﬁdence bounds are valid at all times  ANNEasy
learns the nearest neighbor graph of X in the following number of calls to the distance oracle:

O0@
nXj=1Xk>j

1[Aj k]Hj k +Xk<j

1[Aj k](Hj k  1[Ak j ]Hk j)+1A .

(11)

2

j k

and 1[Aj k] := 1  if xk does not satisfy the triangle

In the above expression Hj k := log(n2/(j k))
inequality elimination conditions of (10) 8i < j  and 0 otherwise.
The expression in (11) can be understood as a sum over the complexity of each of the n rounds  as
speciﬁed by the outer sum. The complexity of each individual round is a sum of two terms. Consider
the jth round. The ﬁrst term bounds the number of calls to Q(j  k) for all k > j. In general Hj k
calls are necessary  unless a triangle inequality bound allows for elimination of k without sampling 
as given by 1[Aj k]. The second term bounds the number of calls to Q(j  k) for all k < j. It has
the same form as the ﬁrst term  except we must now use past samples we may already have via
symmetry of distances (provided the triangle inequality did not prevent us from querying Q(k  j) in
the previous round). The (·)+ operation prevents negative terms  since it may be the case that no
additional samples are necessary  even if we don’t use the triangle inequality for elimination.
In Theorem B.6  in the Appendix  we state the sample complexity when triangle inequality bounds
are ignored by ANNTri  and this upper bounds (11). Whether a point can be eliminated by the triangle
inequality depends both on the underlying distances and the order in which ANNTri ﬁnds each
nearest neighbor (c.f. Lemma 4.3). In general  this dependence on the order is necessary to ensure
that past samples exist and may be used to form upper and lower bounds. Furthermore  it is worth
noting that even without noise the triangle inequality may not always help. A simple example is any
arrangement of points such that 0 < r  dj k < 2r 8j  k. To see this  consider triangle bounds on
any distance dj k due to any xi  xi0 2 X\{xj  xk}. Then |di j  di k| r < 2r  di0 j + di0 k 8i  i0
so L4i j < U4j k 8i  j  k. Thus no triangle upper bounds separate from triangle lower bounds so no
elimination via the triangle inequality occurs. In such cases  it is necessary to sample all O(n2)
distances. However  in more favorable settings where data may be split into clusters  the sample
complexity can be much lower by using triangle inequality.
The order in which {xi⇤} are found follows their subscript index  which is randomly chosen and
ﬁxed before starting the algorithm. As described above  different orders in which {xi} are processed
can affect the query complexity of our algorithm. The best order that minimizes the total number of
queries made in general depends on the true distance values. Even if the oracle is noiseless  there are
datasets where the pair (i  j) with the smallest di j must be queried within the ﬁrst n queries in order
to identify the NN-graph using the minimum number of queries. Since this requirement cannot be
ensured by any algorithm that only has access to information via a distance oracle  it is not possible
to achieve the minimum number of queries in such examples.

4.3 Adaptive gains via the triangle inequality
We highlight two settings where ANNTri provably achieves sample complexity better than O(n2)
independent of the order of the rounds. Consider a dataset containing c clusters of n/c points each as
in Fig. 1a. Denote the mth cluster as Cm and suppose the distances between the points are such that
(12)

{xk : di k < 6C/n(1) + 2di j}✓C m 8i  j 2C m.

6

The above condition is ensured if the distance between any two points belonging to different clusters
is at least a (  n)-dependent constant plus twice the diameter of any cluster.
Theorem 4.5. Consider a dataset of pn clusters which satisfy the condition in (12). Then ANNEasy
learns the correct nearest neighbor graph of X with probability at least 1   in

(13)

O⇣n3/22⌘

n3/2Ppn

i=1Pj k2Ci

min) where 2

log(n2/(j k))2

j k is the average number of samples

min := minj k log(n2/(j k))2

queries where 2 := 1
distances between points in the same cluster.
By contrast  random sampling requires O(n22
j k 
2. In fact  the value in (11) can be even lower if unions of clusters also satisfy (12). In this case 
the triangle inequality can be used to separate groups of clusters. For example  in Fig. 1b  if C1 [C 2
and C3 [C 4 satisfy (12) along with C1 ···  C4  then the triangle inequality can separate C1 [C 2
and C3 [C 4. This process can be generalized to consider a dataset that can be split recursively into
subclusters following a binary tree of k levels. At each level  the clusters are assumed to satisfy (12).
We refer to such a dataset as hierarchical in (12).
Theorem 4.6. Consider a dataset X = [n/⌫
i=1Ci of n/⌫ clusters of size ⌫ = O(log(n)) that is
hierarchical in (12). Then ANNEasy learns the correct nearest neighbor graph of X with probability
at least 1   in
O⇣n log(n)2⌘
(14)

n⌫Pn/⌫

i=1Pj k2Ci

log(n2/(j k))2

j k is the average number of samples

queries where 2 := 1
distances between points in the same cluster.
Expression (14) matches known lower bounds of O(n log(n)) on the sample complexity for learning
the nearest neighbor graph from noiseless samples [30]  the additional penalty of 2 is due to the
effect of noise in the samples. An easy way to see the lower bound is to consider the fact that there are
O(nn1) unique nearest neighbor graphs so any algorithm will require O(log(nn1)) = O(n log(n))
bits of information to identify the correct one. In Appendix C  we state the sample complexity in the
average case  as opposed to the high probability statements above. The analog of the cluster condition
(12) there does not involve constants and is solely in terms of pairwise distances (c.f. (33)).

5 Experiments

Here we evaluate the performance of ANNTri on simulated and real data. To construct the tightest
possible conﬁdence bounds for SETri  we use the law of the iterated logarithm as in [18] with
parameters ✏ = 0.7 and  = 0.1. Our analysis bounds the number of queries made to the oracle.
We visualize the performance by tracking the empirical error rate with the number of queries made
per point. For a given point xi  we say that a method makes an error at the tth sample if it fails
to return xi⇤ as the nearest neighbor  that is  xi⇤ 6= arg minj ˆd[i  j]. Throughout  we will compare
ANNTri against random sampling. Additionally  to highlight the effect of the triangle inequality  we
will compare our method against the same active procedure  but ignoring triangle inequality bounds
(referred to as ANN in plots). All baselines may reuse samples via symmetry as well. We plot all
curves with 95% conﬁdence regions shaded.

5.1 Simulated Experiments

We test the effectiveness of our method  we generate an embedding of 10 clusters of 10 points spread
around a circle such that each cluster is separated by at least 10% of its diameter in R2 as in shown
in Fig. 2a. We consider Gaussian noise with  = 0.1. In Fig. 2b  we present average error rates of
ANNTri  ANN  and Random plotted on a log scale. ANNTri quickly learns xi⇤ and has lower error with
0 samples due to initial elimination by the triangle inequality. The error curves are averaged over
4000 repetitions. All rounds were capped at 105 samples for efﬁciency.

7

(a) Example embedding

(b) Error curves

(c) Comparison to triangulation

Figure 2: Comparison of ANNTri to ANN and Random for 10 clusters of 10 points separated by 10%
of their diameter with  = 0.1. ANNTri identiﬁes clusters of nearby points more easily.

5.1.1 Comparison to triangulation
An alternative way a practitioner may use to obtain the nearest neighbor graph might be to estimate
distances with respect to a few anchor points and then triangulate to learn the rest. [9] provide a
comprehensive example  and we summarize in Appendix A.2 for completeness. The triangulation
method is naïve for two reasons. First  it requires much stronger modeling assumptions than ANNTri—
namely that the metric is Euclidean and the points are in a low-dimensional of known dimension.
Forcing Euclidean structure can lead to unpredictable errors if the underlying metric might not be
Euclidean  such as in data from human judgments. Second  this procedure may be more noise
sensitive because it estimates squared distances. In the example in Section A.2  this leads to the
additive noise being sub-exponential rather than subGaussian. In Fig. 2c  we show that even in a
favorable setting where distances are truly sampled from a low-dimensional Euclidean embedding and
pairwise distances between anchors are known exactly  triangulation still performs poorly compared
to ANNTri. We consider the same 2-dimensional embedding of points as in Fig. 2a for a noise
variance of  = 1 and compare the ANNTri and triangulation for different numbers of samples.

5.2 Human judgment experiments
5.2.1 Setup
Here we consider the problem of learning from human judgments. For this experiment  we used a
set X of 85 images of shoes drawn from the UT Zappos50k dataset [32  33] and seek to learn which
shoes are most visually similar. To do this  we consider queries of the form “between i  j  and k 
which two are most similar?”. We show example queries in Figs. 5a and 5b in the Appendix. Each
query maps to a pair of triplet judgments of the form “is j or k more similar to i?”. For instance  if
i and j are chosen  then we may imply the judgments “i is more similar to j than to k” and “j is
more similar to i than to k”. We therefore construct these queries from a set of triplets collected from

participants on Mechanical Turk by [15]. The set contains multiple samples of all 8584

triples so that the probability of any triplet response can be estimated. We expect that i⇤ is most
commonly selected as being more similar to i than any third point k. We take distance to correspond
to the fraction of times that two images i  j are judged as being more similar to each other than a
different pair in a triplet query (i  j  k). Let Ej
i k be the event that the pair i  k are chosen as most
similar amongst i  j  and k. Accordingly  we deﬁne the ‘distance’ between images i and j as

2 unique

di j := Ek⇠Unif(X\{i j})E[1

i k|k]
Ej

Ej

i k|k] = P(Ej

where k is drawn uniformly from the remaining 83 images in X\{i  j}. For a ﬁxed value of k 
E[1

i k|k) = P(“i more similar to j than to k”)P(“j more similar to i than to k”).
where the probabilities are the empirical probabilities of the associated triplets in the dataset. This
distance is a quasi-metric on our dataset as it does not always satisfy the triangle inequality; but
satisﬁes it with a multiplicative constant: di j  1.47(di k + dj k) 8i  j  k. Relaxing metrics to
quasi-metrics has a rich history in the classical nearest neighbors literature [16  29  14]  and ANNTri
can be trivially modiﬁed to handle quasi-metrics. However  we empirically note that < 1% of the
distances violate the ordinary triangle inequality here so we ignore this point in our evaluation.

8

(a) Sample complexity gains

(b) Comparison to STE

Figure 3: Performance of ANNTri on the Zappos dataset. ANNTri achieves superior performance
over STE in identifying nearest neighbors and has 5  10x gains in sample efﬁciency over random.
5.2.2 Results
When ANNTri or any baseline queries Q(i  j) from the oracle  we randomly sample a third point
k 2 X\{i  j} and ﬂip a coin with probability P(Ej
i k). The resulting sample is an unbiased estimate
of the distance between i and j. In Fig. 3a  we compare the error rate averaged over 1000 trials
of ANNTri compared to Random and STE. We also plot associated gains in sample complexity by
ANNTri. In particular  we see gains of 5  10x over random sampling  and gains up to 16x relative
to ordinal embedding. ANNTri also shows 2x gains over ANN in sample complexity (see Fig. 6 in
Appendix).
Additionally  a standard way of learning from triplet data is to perform ordinal embedding. With a
learned embedding  the nearest neighbor graph may easily be computed. In Fig. 3b  we compare
ANNTri against the state of the art STE algorithm [31] for estimating Euclidean embeddings from
triplets  and select the embedding dimension of d = 16 via cross validation. To normalize the number
of samples  we ﬁrst perform ANNTri with a given max budget of samples and record the total number
needed. Then we select a random set of triplets of the same size and learn an embedding in R16 via
STE. We compare both methods on the fraction of nearest neighbors predicted correctly. On the x
axis  we show the total number of triplets given to each method. For small dataset sizes  there is
little difference  however  for larger dataset sizes  ANNTri signiﬁcantly outperforms STE. Given that
ANNTri is active  it is reasonable to wonder if STE would perform better with an actively sampled
dataset  such as [28]. Many of these methods are computationally intensive and lack empirical
support [20]  but we can embed using the full set of triplets to mitigate the effect of the subsampling
procedure. Doing so  STE achieves 52% error  within the conﬁdence bounds of the largest subsample
shown in Fig. 3b. In particular  more data and more carefully selected datasets  may not correct for
the bias induced by forcing Euclidean structure.
6 Conclusion

In this paper we solve the nearest neighbor graph problem by adaptively querying distances. Our
method makes no assumptions beyond standard metric properties and is empirically shown to achieve
sample complexity gains over passive sampling. In the case of clustered data  we show provable
gains and achieve optimal rates in favorable settings. One interesting avenue for future work would
be to specialize to the case of hyperbolic embeddings which naturally encode trees [8] and may be a
more ﬂexible way to describe hierarchical clusters as in Theorem 4.6. Implementations of ANNTri 
ANN  and RANDOM can be found alongside a demo and summary slides at https://github.com/
blakemas/nngraph.
Acknowledgments

The authors wish to thank Lalit Jain for many helpful discussions over the course of this work for
which the paper is better and the reviewers for their helpful suggestions. This work was partially
supported by AFOSR/AFRL grants FA8750-17-2-0262 and FA9550-18-1-0166.

9

References
[1] Vivek Bagaria  Govinda M Kamath  Vasilis Ntranos  Martin J Zhang  and David Tse. Medoids

in almost linear time via multi-armed bandits. arXiv preprint arXiv:1711.00817  2017.

[2] Vivek Bagaria  Govinda M Kamath  and David N Tse. Adaptive monte-carlo optimization.

arXiv preprint arXiv:1805.08321  2018.

[3] Nitin Bhatia et al. Survey of nearest neighbor techniques. arXiv preprint arXiv:1007.0085 

2010.

[4] Brandon M Booth  Tiantian Feng  Abhishek Jangalwa  and Shrikanth S Narayanan. Toward
robust interpretable human movement pattern analysis in a workplace setting. In ICASSP 2019-
2019 IEEE International Conference on Acoustics  Speech and Signal Processing (ICASSP) 
pages 7630–7634. IEEE  2019.

[5] Justin Brickell  Inderjit S Dhillon  Suvrit Sra  and Joel A Tropp. The metric nearness problem.

SIAM Journal on Matrix Analysis and Applications  30(1):375–396  2008.

[6] Kenneth L Clarkson. Fast algorithms for the all nearest neighbors problem. In 24th Annual

Symposium on Foundations of Computer Science (sfcs 1983)  pages 226–232. IEEE  1983.

[7] Thomas H Cormen  Charles E Leiserson  Ronald L Rivest  and Clifford Stein. Introduction to

algorithms. MIT press  2009.

[8] Andrej Cvetkovski and Mark Crovella. Hyperbolic embedding and routing for dynamic graphs.

In IEEE INFOCOM 2009  pages 1647–1655. IEEE  2009.

[9] Brian Eriksson  Paul Barford  Joel Sommers  and Robert Nowak. A learning-based approach
for ip geolocation. In International Conference on Passive and Active Network Measurement 
pages 171–180. Springer  2010.

[10] Eyal Even-Dar  Shie Mannor  and Yishay Mansour. Action elimination and stopping conditions
for the multi-armed bandit and reinforcement learning problems. Journal of machine learning
research  7(Jun):1079–1105  2006.

[11] Robert W Floyd. Algorithm 97: shortest path. Communications of the ACM  5(6):345  1962.

[12] Aurélien Garivier. Informational conﬁdence bounds for self-normalized averages and applica-

tions. In 2013 IEEE Information Theory Workshop (ITW)  pages 1–5. IEEE  2013.

[13] Anna C Gilbert and Lalit Jain. If it ain’t broke  don’t ﬁx it: Sparse metric repair. In 2017 55th
Annual Allerton Conference on Communication  Control  and Computing (Allerton)  pages
612–619. IEEE  2017.

[14] Navin Goyal  Yury Lifshits  and Hinrich Schütze. Disorder inequality: a combinatorial approach
to nearest neighbor search. In Proceedings of the 2008 International Conference on Web Search
and Data Mining  pages 25–32. ACM  2008.

[15] Eric Heim  Matthew Berger  Lee Seversky  and Milos Hauskrecht. Active perceptual similarity

modeling with auxiliary information. arXiv preprint arXiv:1511.02254  2015.

[16] Michael E Houle and Michael Nett. Rank-based similarity search: Reducing the dimensional
dependence. IEEE transactions on pattern analysis and machine intelligence  37(1):136–150 
2015.

[17] Lalit Jain  Kevin G Jamieson  and Rob Nowak. Finite sample prediction and recovery bounds for
ordinal embedding. In Advances In Neural Information Processing Systems  pages 2711–2719 
2016.

[18] K. Jamieson and R. Nowak. Best-arm identiﬁcation algorithms for multi-armed bandits in the
ﬁxed conﬁdence setting. In 2014 48th Annual Conference on Information Sciences and Systems
(CISS)  pages 1–6  March 2014. doi: 10.1109/CISS.2014.6814096.

10

[19] Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sebastien Bubeck. On ﬁnding the largest

mean among many. arXiv preprint arXiv:1306.3917  2013.

[20] Kevin G Jamieson  Lalit Jain  Chris Fernandez  Nicholas J Glattard  and Rob Nowak. Next: A
system for real-world development  evaluation  and application of active learning. In Advances
in Neural Information Processing Systems  pages 2656–2664  2015.

[21] Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset

selection. In Conference on Learning Theory  pages 228–251  2013.

[22] Matthäus Kleindessner and Ulrike Von Luxburg. Lens depth function and k-relative neigh-
borhood graph: versatile tools for ordinal data analysis. The Journal of Machine Learning
Research  18(1):1889–1940  2017.

[23] Joseph B Kruskal. Nonmetric multidimensional scaling: a numerical method. Psychometrika 

29(2):115–129  1964.

[24] Daniel LeJeune  Richard G Baraniuk  and Reinhard Heckel. Adaptive estimation for approxi-

mate k-nearest-neighbor computations. arXiv preprint arXiv:1902.09465  2019.

[25] Jagan Sankaranarayanan  Hanan Samet  and Amitabh Varshney. A fast all nearest neighbor
algorithm for applications involving large point-clouds. Computers & Graphics  31(2):157–174 
2007.

[26] Roger N Shepard. The analysis of proximities: multidimensional scaling with an unknown

distance function. i. Psychometrika  27(2):125–140  1962.

[27] Adish Singla  Sebastian Tschiatschek  and Andreas Krause. Actively learning hemimetrics with
applications to eliciting user preferences. In International Conference on Machine Learning 
pages 412–420  2016.

[28] Omer Tamuz  Ce Liu  Serge Belongie  Ohad Shamir  and Adam Tauman Kalai. Adaptively

learning the crowd kernel. arXiv preprint arXiv:1105.1033  2011.

[29] Dominique Tschopp  Suhas Diggavi  Payam Delgosha  and Soheil Mohajer. Randomized
In Advances in Neural Information Processing

algorithms for comparison-based search.
Systems  pages 2231–2239  2011.

[30] Pravin M Vaidya. Ano (n logn) algorithm for the all-nearest-neighbors problem. Discrete &

Computational Geometry  4(2):101–115  1989.

[31] Laurens Van Der Maaten and Kilian Weinberger. Stochastic triplet embedding. In 2012 IEEE
International Workshop on Machine Learning for Signal Processing  pages 1–6. IEEE  2012.
[32] A. Yu and K. Grauman. Fine-grained visual comparisons with local learning. In Computer

Vision and Pattern Recognition (CVPR)  Jun 2014.

[33] A. Yu and K. Grauman. Semantic jitter: Dense supervision for visual comparisons via synthetic

images. In International Conference on Computer Vision (ICCV)  Oct 2017.

11

,Blake Mason
Ardhendu Tripathy
Robert Nowak