2017,Near Optimal Sketching of Low-Rank Tensor Regression,We study the least squares regression problem $\min_{\Theta \in \RR^{p_1 \times \cdots \times p_D}} \| \cA(\Theta) -  b \|_2^2$  where $\Theta$ is a low-rank tensor  defined as $\Theta = \sum_{r=1}^{R} \theta_1^{(r)} \circ \cdots \circ \theta_D^{(r)}$  for vectors $\theta_d^{(r)} \in \mathbb{R}^{p_d}$ for all $r \in [R]$ and $d \in [D]$.    %$R$ is small compared with $p_1 \ldots p_D$    Here  $\circ$ denotes the outer product of vectors  and $\cA(\Theta)$ is a linear function on $\Theta$. This problem is motivated by the fact that the number of parameters in $\Theta$ is only $R \cdot \sum_{d=1}^D p_D$  which is significantly smaller than the $\prod_{d=1}^{D} p_d$ number of parameters in ordinary least squares regression. We consider the above CP decomposition model of tensors $\Theta$  as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on {\it sparse} random projections $\Phi \in \RR^{m \times n}$  with $m \ll n$  to reduce the problem to a much smaller problem $\min_{\Theta} \|\Phi \cA(\Theta) - \Phi b\|_2^2$  for which $\|\Phi \cA(\Theta) - \Phi b\|_2^2 = (1 \pm \varepsilon) \| \cA(\Theta) -  b \|_2^2$ holds simultaneously for all $\Theta$. We obtain a significantly smaller dimension and sparsity in the randomized linear mapping $\Phi$ than is possible for ordinary least squares regression. Finally  we give a number of numerical simulations supporting our theory.,Near Optimal Sketching of Low-Rank Tensor Regression

Jarvis Haupt1

jdhaupt@umn.edu

Xingguo Li1 2

lixx1661@umn.edu

David P. Woodruff 3

dwoodruf@cs.cmu.edu ⇤

1 University of Minnesota

2Georgia Tech

3Carnegie Mellon University

Abstract
We study the least squares regression problem

min

r=1 ✓(r)

1 ··· ✓(r)

⇥2Rp1⇥···⇥pD kA(⇥)  bk2
2 
where ⇥ is a low-rank tensor  deﬁned as ⇥= PR
D   for vectors
✓(r)
d 2 Rpd for all r 2 [R] and d 2 [D]. Here   denotes the outer product of
vectors  and A(⇥) is a linear function on ⇥. This problem is motivated by the fact
that the number of parameters in ⇥ is only R ·PD
d=1 pd  which is signiﬁcantly
smaller than theQD
d=1 pd number of parameters in ordinary least squares regression.
We consider the above CP decomposition model of tensors ⇥  as well as the
Tucker decomposition. For both models we show how to apply data dimensionality
reduction techniques based on sparse random projections  2 Rm⇥n  with m ⌧ n 
to reduce the problem to a much smaller problem min⇥ kA(⇥)bk2
2  for which
2 holds simultaneously for all ⇥. We obtain
kA(⇥) bk2
a signiﬁcantly smaller dimension and sparsity in the randomized linear mapping 
than is possible for ordinary least squares regression. Finally  we give a number of
numerical simulations supporting our theory.

2 = (1± ")kA(⇥) bk2

Introduction

1
For a sequence of D-way design tensors Ai 2 Rp1⇥···⇥pD  i 2 [n]   {1  . . .   n}  suppose we observe
noisy linear measurements of an unknown D-way tensor ⇥ 2 Rp1⇥···⇥pD  given by
(1)
where A(·) : Rp1⇥···⇥pD ! Rn is a linear function with Ai(⇥) = hAi  ⇥i = vec(Ai)>vec(⇥) for
all i 2 [n]  vec(X) is the vectorization of a tensor X  and z = [z1  . . .   zn]> corresponds to the
observation noise. Given the design tensors {Ai}n
i=1 and noisy observations b = [b1  . . .   bn]>  a
natural approach for estimating the parameter ⇥ is to use the Ordinary Least Square (OLS) estimation
for the tensor regression problem  i.e.  to solve

b = Ai(⇥) + z  b  z 2 Rn 

min

⇥2Rp1⇥···⇥pD kA(⇥)  bk2
2.

(2)

Tensor regression has been widely studied in the literature. Applications include computer vision
[8  19  34]  data mining [5]  multi-model ensembles [32]  neuroimaging analysis [15  36]  multitask
learning [21  31]  and multivariate spatial-temporal data analysis [1  11]. In these applications 
modeling the unknown parameters as a tensor is what is needed  as it allows for learning data that
has multi-directional relations  such as in climate prediction [33]  inherent structure learning with
multi-dimensional indices [21]  and hand movement trajectory decoding [34].

⇤The authors are listed in alphabetical order. Correspondence to: Xingguo Li <lixx1661@umn.edu>.
The authors acknowledge support from University of Minnesota Startup Funding and Doctoral Dissertation
Fellowship from University of Minnesota.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Due to the high dimensionality of tensor data  structured learning based on low-rank tensor de-
compositions  such as CANDECOMP/PARAFAC (CP) decomposition and Tucker decomposition
models [13  24]  have been proposed in order to obtain tractable tensor regression problems. As
discussed more below  requiring the unknown tensor to be low-rank signiﬁcantly reduces the number
of unknown parameters.
We consider low-rank tensor regression problems based on the CP decomposition and Tucker
decomposition models. For simplicity  we ﬁrst focus on the CP model  and later extend our analysis
to the Tucker model. Suppose that ⇥ admits a rank-R CP decomposition  that is 

⇥=

✓(r)
1 ··· ✓(r)
D  

(3)

RXr=1

(4)

(5)

where ✓(r)
reparameterize the set of low-rank tensors by its matrix slabs/factors:

d 2 Rpd for all r 2 [R]  d 2 [D]  and  is the outer product of vectors. For convenience  we
SD R  n[[⇥1  . . .   ⇥D]] | ⇥d = [✓(1)

] 2 Rpd⇥R  for all d 2 [D]o.

Then we can rewrite model (1) in a compact form

d   . . .  ✓ (R)

d

b = A(⇥D ··· ⇥1)1R + z 

where A = [vec(A1) ···   vec(An)]> 2 Rn⇥QD
d=1 pd is the matricization of all design tensors 
1R = [1  . . .   1] 2 RR is a vector of all 1s  ⌦ is the Kronecker product  and  is the Khatri-Rao
product. In addition  the OLS estimation for tensor regression (2) can be rewritten as the following
nonconvex problem in terms of low-rank tensor parameters [[⇥1  . . .   ⇥D]] 

min

2  where

#2SD R kA#  bk2
SD R  n(⇥D ··· ⇥1)1R 2 RQD

d=1 pd [[⇥1  . . .   ⇥D]] 2S D Ro.

The number of parameters for a general tensor ⇥ 2 Rp1⇥···⇥pD isQD
d=1 pd  which may be prohibitive
d=1. The beneﬁt of the low-rank tensor model (3) is that
for estimation even for small values of {pd}D
it dramatically reduces the degrees of freedom of the unknown tensor fromQD
d=1 pd 
where we are typically interested in the case when R  pd for all d 2 [D]. For example  a typical
MRI image has size 2563 ⇡ 1.7 ⇥ 107  while using the low-rank model with R = 10  we reduce the
number of unknown parameters to 256 ⇥ 3 ⇥ 10 ⇡ 8 ⇥ 103 ⌧ 107. This signiﬁcantly increases the
applicability of the tensor regression model in practice.
Nevertheless  solving the tensor regression problem (5) is still expensive in terms of both computation
and memory requirements  for typical settings  when n  R·PD
d=1 pd. In particular  the per iteration
complexity is at least linear in n for popular algorithms such as block alternating minimization and
block gradient descent [27  28]. In addition  in order to store A  it takes n ·QD
d=1 pd words of
memory. Both of these aspects are undesirable when n is large. This motivates us to consider data
dimensionality reduction techniques  also called sketching  for the tensor regression problem.
Instead of solving (5)  we consider the simple Sketched Ordinary Least Square (SOLS) problem:

d=1 pd to R ·PD

min

#2SD R kA#  bk2
2 

(6)

where  2 Rm⇥n is a random matrix (speciﬁed in Section 2). Importantly   will satisfy two
properties  namely (1) m ⌧ n so that we signiﬁcantly reduce the size of the problem  and (2)  will
be very sparse so that v can be computed very quickly for any v 2 Rn.
Naïvely applying existing analyses of sketching techniques for least squares regression requires
m =⌦( QD
d=1 pd)  which is prohibitive (for a survey  see  e.g.  [30]). In this paper  our main
contribution is to show that it is possible to use a sparse Johnson-Lindenstrauss transformation as
our sketching matrix for the CP model of low-rank tensor regression  with constant column sparsity
and dimension m = R ·PD
d=1 pd  up to poly-logarithmic (polylog) factors. Note that our dimension
matches the number of intrinsic parameters in the CP model. Further  we stress that we do not assume
anything about the tensor  such as orthogonal matrix slabs/factor  or incoherence; our dimensionality

2

reduction works for arbitrary tensors. We show  with the above sparsity and dimenion  that with
constant probability  simultaneously for all # 2S  D R 

kA#  bk2

2 = (1 ± ")kA#  bk2
2.

reduction for this problem  i.e.  dimensionality reduction better thanQD

This implies that any solution to (6) has the same cost as in (5) up to a (1 + ")-factor. In partic-
ular  by solving (6) we obtain a (1 + ")-approximation to (5). We note that our dimensionality
reduction technique is not tied to any particular algorithm; that is  if one runs any algorithm or
heuristic on the reduced (sketched) problem  obtaining an ↵-approximate solution #  then # is also a
(1+✏)↵-approximate solution to the original problem. Our result is the ﬁrst non-trivial dimensionality
d=1 pd  which is trivial by ig-
noring the low-rank structure of the tensor  and which achieves a relative error (1 + ")-approximation.
While it may be possible to apply dimensionality reduction methods directly in alternating minimiza-
tion methods for solving tensor regression  unlike our method  such methods do not have provable
guarantees and it is not clear how errors propagate across iterations. However  since we reduce the
original problem to a smaller version of itself with a provable guarantee  one could further apply
dimensionality reduction techniques as heuristics for alternating minimization on the smaller problem.
Our proof is based on a careful characterization of Talagrand’s functional for the parameter space
of low-rank tensors  providing a highly nontrivial analysis for what we consider to be a simple and
practical algorithm. One of the main difﬁculties is dealing with general  non-orthogonal tensors  for
which we are able to provide a careful re-parameterization in order to bound the so-called Finsler
metric; interestingly  for non-orthogonal tensors it is always possible to partially orthogonalize them 
and this partial orthogonalization turns out to sufﬁce for our analysis. We give precise details below.
We also provide numerical evaluations on both synthetic and real data to demonstrate the empirical
performance of our algorithm.
Notation. For scalars x  y 2 R  let x = (1 ± ")y if x 2 [(1  ")y  (1 + ")y]  x . (&)y if
x  ()c1y  poly(x) = xc2 and polylog(x  y) = (log x)c3 · (log y)c4 for some universal constants
c1  c2  c3  c4 > 0. We also use standard asymptotic notations O(·) and ⌦(·). Given a matrix
A 2 Rm⇥n  we denote kAk2 as the spectral norm  span(A) ✓ Rm as the subspace spanned by the
columns of A  max(A) and min(A) as the largest and smallest singular values of A  respectively 
and A = max(A)/min(A) as the condition number. We use nnz(A) to denote the number
of nonzero entries of A  and PA as the projection operator onto span(A). Given two matrices
A = [a1  . . .   an] 2 Rm⇥n and B = [b1  . . .   bq] 2 Rp⇥q  A⌦B = [a1⌦B  . . .   an⌦B] 2 Rmp⇥nq
denotes the Kronecker product  and AB = [a1⌦b1  . . .   an⌦bn] 2 Rmp⇥n denotes the Khatri-Rao
product with n = q. We let Bn ⇢ Rn be the unit sphere in Rn  i.e.  Bn = {x 2 Rn |k xk2 = 1} 
P(·) be the probability of an event  and E(·) denotes the expectation of a random variable. Without
further speciﬁcation  we denoteQ =QD
d=1. We further summarize the dimension
parameters for ease of reference. Given a tensor ⇥  D is the number of ways  pd is the dimension of
the d-th way for d 2 [D]. R is the rank of ⇥ for all ways under the CP decomposition  and Rd is the
rank of the d-th way under the Tucker decomposition for d 2 [D]. n is the number of observations
for tensor regression. m is the sketching dimension and s is the sparsity of each column in a sparse
Johnson-Lindenstrauss transformation.

d=1 andP =PD

2 Background

We start with a few important deﬁnitions.
Deﬁnition 1 (Oblivious Subspace Embedding). Suppose ⇧ is a distribution on m⇥ n matrices where
m is a function of parameters n  d  and ". Further  suppose that with probability at least 1    for
any ﬁxed n ⇥ d matrix A  a matrix  drawn from ⇧ has the property that kAxk2
2 = (1 ± ")kAxk2
simultaneously for all x 2X✓ Rd. Then ⇧ is an ("  ) oblivious subspace embedding (OSE) of X .
An OSE  preserves the norm of vectors in a certain set X after linear transformation by A. This is
widely studied as a key property for sketching based analyses (see [30] and the references therein).
We want to show an analogous property when X is parameterized by low-rank tensors.
Deﬁnition 2 (Leverage Scores). Given A 2 Rn⇥d  let Z 2 Rn⇥d have orthonormal columns that
span the column space of A. Then `2

2 is the i-th leverage score of A.

2

i (A) = ke>i Zk2

3

Leverage scores play an important role in randomized matrix algorithms [7  16  17]. Calculating
the leverage scores naïvely by orthogonalizing A requires O(nd2) time. It is shown in [3] that
the leverage scores of A can be approximated individually up to a constant multiplicative factor in
O(nnz(A) log n + poly(d)) time using sparse subspace embeddings. In our analysis  there will be a
very mild dependence on the maximum leverage score of A and the sparsity for the sketching matrix
. Note that we do not need to calculate the leverage scores.
Deﬁnition 3 (Talagrand’s Functional). Given a (semi-)metric ⇢ on Rn and a bounded set S⇢ Rn 
Talagrand’s 2-functional is

(7)

sup
x2S

{Sr}1r=0

2(S  ⇢) = inf

2r/2 · ⇢(x Sr) 

1Xr=0
where ⇢(x Sr) is a distance from x to Sr and the inﬁmum is taken over all collections {Sr}1r=0 such
that S0 ⇢S 1 ⇢ . . . ⇢S with |S0| = 1 and |Sr| 22r.
A closely related notion of the 2-functional is the Gausssian mean width: G(S) = Eg supx2Shg  xi 
where g ⇠N n(0  In). For any bounded S⇢ Rn  G(S) and 2(S k·k 2) differ multiplicatively by at
most a universal constant in Euclidean space [25]. Finding a tight upper bound on the 2-functional
for the parameter space of low-rank tensors is key to our analysis.
Deﬁnition 4 (Finsler Metric). Let E  E0 ⇢ Rn be p-dimensional subspaces. The Finsler metric of
E and E0 is ⇢Fin(E  E0) = kPE P E0k2  where PE is the projection onto the subspace E.
The Finsler metric is the semi-metric used in the 2-functional in our analysis. Note that ⇢Fin(E  E0) 
1 always holds for any E and E0 [23].
Deﬁnition 5 (Sparse Johnson-Lindenstrauss Transforms). Let ij be independent Rademacher
random variables  i.e.  P(ij = 1) = P(ij = 1) = 1/2  and let ij :⌦  !{ 0  1} be random
variables  independent of the ij  with the following properties:
(i) ij are negatively correlated for ﬁxed j  i.e.  for all 1  i1 < . . . < ik  m  we have
E⇣Qk
t=1 it j⌘ Qk
(ii) There are s =Pm
(iii) The vectors (ij)m
Then  2 Rm⇥n is a sparse Johnson-Lindenstrauss transform (SJLT) matrix if ij = 1ps ijij.
The SJLT has several beneﬁts [4  12  30]. First  the computation of x takes only O(nnz(x)) time
when s is a constant. Second  storing  takes only sn memory instead of mn  which is signiﬁcant
when s ⌧ m. This can often further be reduced by drawing the entries of  from a limited
independent family of random variables.
We will use an SJLT matrix as the sketching matrix  in our analysis and our goal will be to
show sufﬁcient conditions on the sketching dimension m and per-column sparsity s such that the
analogue of the OSE property holds for low-rank tensor regression. Speciﬁcally  we provide sufﬁcient
conditions for the SJLT matrix  2 Rm⇥n to preserve the cost of all solutions for tensor regression 
i.e.  bounds on m and s for which

t=1 E (it j) = s
mk;
i=1 ij nonzero ij for a ﬁxed j; and
i=1 are independent across j 2 [n].

E


sup

x2Tkxk2

2  1 <

"
10

 

(8)

where " is a given precision and T is a normalized space parameterized as the union of certain
subspaces of A  which will be further discussed in the following sections. Note that by linearity  it is
sufﬁcient to consider x with kxk2 = 1 in the above  which explains the form of (8). Moreover  by
Markov’s inequality  (8) implies that simultaneously for all # = vec(⇥) 2S D R  where ⇥ admits a
low-rank tensor decomposition  with probability at least 9/10  we have
(9)
which allows us to minimize the much smaller sketched problem to obtain parameters # which  when
plugged into the original objective function  provide a multiplicative (1 + ")-approximation.

2 = (1 ± ")kA#  bk2
2 

kA#  bk2

4

3 Dimensionality Reduction for CP Decomposition

1

\1   . . .   A✓(R)

D   where ✓(r)

d 2 Rpd

1 ··· ✓(r)

We start with the following notation. Given a tensor ⇥= PR
for all d 2 [D] and r 2 [R]  we ﬁx all but ✓(r)
\1o =hA✓(1)
An✓(r)
jD=1 ···Pp2
j2=1 A(jD ... j2)✓(i)

r=1 ✓(r)
for r 2 [R]  and denote
\1 i 2 Rn⇥Rp1 

where A✓(i)
d   and
A(jD ... j2) 2 Rn⇥p1 is a column submatrix of A indexed by jD 2 [pD]  . . .   j2 2 [p2]  i.e. 
A =⇥A(1 ... 1)  . . .   A(pD ... p2)⇤ 2 Rn⇥Q pd. The above parameterization allows us to view tensor
regression as preserving the norms of vectors in an inﬁnite union of subspaces  described in more
detail in the full version of our paper [10]. Then we rewrite the observation model (4) as
i> + z.

d jd is the jd-th entry of ✓(i)

\1o ·h✓(1)>1

\1 = PpD

1 + z = An✓(r)

✓(r)
D ⌦···⌦ ✓(r)

D jD ··· ✓(i)

2 j2  ✓(i)

A✓(r)

\1 · ✓(r)

b = A ·

RXr=1

1 + z =

RXr=1

. . .✓ (R)>1

r=1 ✓(r)

d   (r)

r=1 (r)

[18]. The following theorem provides sufﬁcient conditions to guarantee (1 + ")-approximation of the
objective for low-rank tensor regression under the CP decomposition model.

3.1 Main Result
The parameter space for the tensor regression problem (1) is a subspace of RQ pd  i.e.  SD R ⇢
RQ pd. Therefore  a naïve application of sketching requires m &Q pd/"2 in order for (9) to hold
Theorem 1. Suppose R  maxd pd/2 and maxi2[n] `2
d 2B pdo
T = [r2[R] d2[D]n A#A'
D ⌦···⌦ ✓(r)
and let  2 Rm⇥n be an SJLT matrix with column sparsity s. Then with probability at least 9/10  (9)
holds if m and s satisfy  respectively 

i (A)  1/(RPD
1  ' =PR

kA#A'k2# =PR

d=2 pd)2. Let
1  ✓ (r)

D ⌦···⌦ (r)

⌦(1)  up to logarithmic factors  we can guarantee (1 + ")-approximation of the objective. The
sketching complexity of m is nearly optimal compared with the number of free parameters for the CP

m & RX pd log⇣DRAX pd⌘ polylog(m  n)/"2 and s & log2⇣X pd⌘ polylog(m  n)/"2.
From Theorem 1  we have that for an SJLT matrix  2 Rm⇥n with m =⌦( RP pd) and s =
decomposition model  i.e.  R(P pd  D + 1)  up to logarithmic factors. Here wo do not make any
orthogonality assumption on the tensor factors ✓(r)
d   and show in our analysis that the general tensor
space T can be paramterized in terms of an orthogonal one if R  maxd pd/2 holds. The condition
R  maxd pd/2 is not restrictive in our setting  as we are interested in low-rank tensors with R  pd.
Note that we achieve a (1 + ")-approximation in objective function value for arbitrary tensors; if one
wants to achieve closeness of the underlying parameters one needs to impose further assumptions on
the model  such as the form of the noise distribution or structural properties of A [20  36].
Our maximum leverage score assumption is very mild and much weaker than the standard inco-
herence assumptions used for example  in matrix completion  which allow for uniform sampling
based approaches. For example  our assumption states that the maximum leverage score is at most

1/(RPD
d=2 pd)2. In the typical overconstrained case  n Q pd  and in order for uniform sampling
to provide a subspace embedding  one needs the maximum leverage score to be at most RP pd/n
(see  e.g.  Section 2.4 of [30])  which is much less than 1/(RPD
d=2 pd)2 when n is large  and so
uniform sampling fails in our setting. Moreover  it is also possible to apply a standard idea to ﬂatten
the leverage scores of a deterministic design A based on the Subsampled Randomized Hadamard
Transformation (SRHT) using the Walsh-Hadamard matrix [9  26]. Note that applying the SRHT to
an n ⇥ d matrix A only takes O(nd log n) time  which if A is dense  is the same amount of time one
needs just to read A (up to a log n factor). Further details are deferred to the full version of our paper
[10].

5

3.2 Proof Sketch of Our Analysis for a Basic Case

We provide a sketch of our analysis for the case when R = 1 and D = 2  i.e.  ⇥ is rank 1 matrix. The
analysis for more general cases is more involved  but with similar intuition. Details of the analyses
are deferred to the the full version of our paper  where we start with a proof for the most basic cases
and gradually build up the proof for the most general case.

Let Av = Pp2
i=1 A(i)vi  where A = [A(1)  . . .   A(p2)] 2 Rn⇥p2p1 with A(i) 2 Rn⇥p1 for all
i 2 [p2]  V =SfW {span[Av1  Av2]}  andfW = {v1  v2 2B p2 with hv1  v2i = 0}. We start with an
illustration that the set T can be reparameterized to the following set with respect to tensors with
orthogonal factors:

T = [E2V

{x 2 E |k xk2 = 1} .

=

=

Suppose hv1  v2i 6= 0. Let v2 = ↵v1 + z for some ↵   2 R and a unit vector z 2 Rp2  where
hv1  zi = 0. Then we have
Ax  Ay
kAx  Ayk2

Av1(u1  ↵u2)  Az(u2)
kAv1(u1  ↵u2)  Az(u2)k2

Av1u1  Av2u2
kAv1u1  Av2u2k2

which is equivalent to hv1  v2i = 0 by reparameterizing z as v2.
Based on known dimensionality reduction results [2  6] (see further details in the full version [10])  the
main quantities needed for bounding properties of  are the quantities ⇢V  2
2(V ⇢ Fin)  N (V ⇢ Fin " 0) 
andR "0
0 (log N (V ⇢ Fin  t))1/2 dt  where N (V ⇢ Fin  t) is the covering number of V under the Finsler
metric using balls of radius t and pV = supv1 v22Bp2  hv1 v2i=0 dim{span (Av1 v2)} 2p1. Bounding
these quantities for the space of low-rank tensors is new and is our main technical contribution. These
will be addressed separately as follows.
Part 1: Bound pV. Let Av1 v2 = [Av1  Av2]. It is straightforward that pV  2p1.
Part 2: Bound 2

2(V ⇢ Fin). By the deﬁnition of 2-functional in (7) for the Finsler metric  we have

 

2(V ⇢ Fin) = inf

{V k}1k=0

sup

Av1 v22V

2k/2 · ⇢Fin(Av1 v2 V k) 

1Xk=0

where V k is an "k-net of V  i e.  for any Av1 v2 2V there exist v1  v2 2B p2 with hv1  v2i = 0 
kv1  v1k2  ⌘k  and kv2  v2k2  ⌘k  such that Av1 v2 2 V k and ⇢Fin(Av1 v2  Av1 v2)  "k.
From Lemma 6  we have ⇢Fin(Av1 v2 V k)  2A⌘k for kv1  v1k2  ⌘k and kv2  v2k2 
⌘k. On the other hand  we have that ⇢Fin(Av1 v2 V k)  1 always holds. Therefore  we have
⇢Fin(Av1 v2 V k)  min{2A⌘k  1}. Let k0 be the smallest integer such that 2A⌘k0  1. Then

2k/2 +

k0Xk=0

1Xk=0

2(V ⇢ Fin) 

2k/2⇢Fin(Av1 v2 V k).

2k/2⇢Fin(Av1 v2 V k) 

1Xk=k0+1
Starting from ⌘0 = 1 and |V 0| = 1  for k  1  we have ⌘k < 1 and |V k| (3/⌘k)p2 [29]. Also from
the 2-functional  we require |V k| 22k  (3/⌘k)p2  which implies
1
⌘k0

k0Xk=0
k such that (3/⌘k+1)p2  22k+1. Then we have |V k+1| 22k+1.

For k > k0  we choose ⌘k+1 = ⌘2
By choosing k0 to be the smallest integer such that (3/⌘k0+1)p2  22k0+1 holds  we have
 2k0/2 .rp2 log

2k/2 · ⇢Fin(Av1 v2 V k) = 2k0/2 ·

2t/2 ·✓ 1
2◆2t

.rp2 log

2k0/2
p2  1

2k/2 =

1
⌘k0

.

(12)

(10)

(11)

.

1Xk=k0+1

1Xt=1

6

.

A
"0

this implies

2(V ⇢ Fin) . p2 log
2
0 [log N (V ⇢ Fin  t)]1/2dt. From our choice from Part 2  "0 2
"0⌘2p2. From direct integration 

Combining (10) – (12)  and choosing a small enough "0 such that "0  2A⌘k0  we have
Part 3: Bound N (V ⇢ Fin " 0) andR "0
(0  1) is a constant. Then it is straightforward that N (V ⇢ Fin " 0) ⇣ 3
[log N (V ⇢ Fin  t)]1/2dt."0rp2 log
"0⌘ · polylog(m  n)
"0⌘ · polylog(m  n)

Combining the results in Parts 1  2  and 3  we have that (9) holds if m and s satisfy  respectively

m & ⇣p2 log A
s & ⇣log2 1

0(p1 + p2) log 1

+ p1 + p2 log 1

Z "0

+ "2

1
"0

and

"2

"0

"0

0

.

.

"2

We ﬁnish the proof by taking "0 = 1/(p1 + p2).

4 Dimensionality Reduction for Tucker Decomposition

We start with a formal model description. Suppose ⇥ admits the following Tucker decomposition:

⇥=

R1Xr1=1

···

RDXrD=1

G(r1  . . .   rD) · ✓(r1)

1

··· ✓(rD)
D  

(13)

\1

\1

\1

rD=1 A✓(r1 ... rD )

rD=1 A✓(r1 ... rD )

D jD ··· ✓(r2)

j2=1 A(jD ... j2)✓(rD)

rD=1 G(r1  . . .   rD)✓(rD)

r2=1···PRD
1 + z = A⇢✓{rd}

2 Rpd for all rd 2 [Rd] and d 2 [D]. Let
2 j2 and

G(1  r2 . . .  rD) . . . PR2
D ⌦···⌦ ✓(r1)

where G 2 RR1⇥···⇥RD is the core tensor and ✓(rd)
=PpD
jD=1···Pp2
A✓(r1 ... rD )
A⇢✓{rd}
\1  =PR2
r2=1···PRD
Then the observation model (4) can be written as
r1=1···PRD
b = APR1
The following theorem provides sufﬁcient conditions to guarantee (1 + ")-approximation of the
objective function for low-rank tensor regression under the Tucker decomposition model.
Theorem 2. Suppose nnz(G)  maxd pd/2 and maxi2[n] `2
Let T = [r2[R] d2[D]n A#  A'
RDXrD=1
RDXrD=1
D ⌦···⌦ (r1)
G2(r1  . . .   rD) · (rD)

D ⌦···⌦ ✓(r1)
d 2B pdo
and  2 Rm⇥n be an SJLT matrix with column sparsity s. Then with probability at least 9/10  (9)
holds if m and s satisfy

\1 h✓(1)>1
i (A)  1/(PD
G1(r1  . . .   rD) · ✓(rD)

kA#  A'k2# =
R1Xr1=1

G(R1  r2 . . .  rD).
i> + z.

d=2 Rdpd + nnz(G))2.

R1Xr1=1

. . .✓ (R1)>1

m & C1 · log⇣C1DAR1pnnz(G)⌘ · polylog(m  n)/"2 and s & log2 C1 · polylog(m  n)/"2 
where C1 =P Rdpd + nnz(G).
From Theorem 2  we have that using an SJLT matrix  with m =⌦( P Rdpd + nnz(G)) and

s = ⌦(1)  up to logarithmic factors  we can guarantee (1+")-approximation of the objective function.

 ✓ (rd)

  (rd)

···

···

' =

 

1

1

d

d

7

The sketching complexity of m is near optimal compared with the number of free parameters for
the Tucker decomposition model  i.e. P Rdpd + nnz(G) P R2
d  up to logarithmic factors. Note
that nnz(G)  Q Rd  and thus the condition that nnz(G)  maxd pd/2 can be more restrictive
than R  maxd pd/2 in the CP model when nnz(G) > R. This is due to the fact that the Tucker
model is more “expressive” than the CP model for a tensor of the same dimensions. For example  if
R1 = ··· = RD = R  then the CP model (3) can be viewed as special case of the Tucker model (13)
by setting all off-diagonal entries of the core tensor G to be 0. Moreover  the conditions and results
in Theorem 2 are essentially of the same order as those in Theorem 1 when nnz(G) = R  which
indicates the tightness of our analysis.

5 Experiments
We study the performance of sketching for tensor regression through numerical experiments over
both synthetic and real data sets. For solving the OLS problem for tensor regression (2)  we use a
cyclic block-coordinate minimization algorithm based on a tensor toolbox [35]. Speciﬁcally  in a
cyclic manner for all d 2 [D]  we ﬁx all but one ⇥d of [[⇥1  . . .   ⇥D]] 2S D R and minimize the
resulting quadratic loss function (2) with respect to ⇥i  until the decrease of the objective is smaller
than a predeﬁned threshold ⌧. For SOLS  we use the same algorithm after multiplying A and b with
an SJLT matrix . All results are run on a supercomputer due to the large scale of the data. Note
that our result is not tied to any speciﬁc algorithm and we can use any algorithm that solves OLS for
low-rank tensors for solving SOLS for low-rank tensors.
For synthetic data  we generate the low-rank tensor ⇥ as follows. For each d 2 [D]  we generate R
random columns with N (0  1) entries to form non-orthogonal tensor factors ⇥d = [✓(1)
d   . . .  ✓ (R)
]
of [[⇥1  . . .   ⇥D]] 2S D R independently. We also generate R real scalars ↵1  . . .  ↵ R uniformly
and independently from [1  10]. Then ⇥ is formed by ⇥= PR
D . The n tensor
i=1 are generated independently with i.i.d. N (0  1) entries for 10% of the entries chosen
designs {Ai}n
uniformly at random  and the remaining entries are set to zero. We also generate the noise z to have
z) entries  and the generation of the SJLT matrix  follows Deﬁnition 5. For both OLS
i.i.d. N (0  2
and SOLS  we use random initializations for ⇥  i.e.  ⇥d has i.i.d. N (0  1) entries for all d 2 [D].
We compare OLS and SOLS for low-rank tensor regression under both the noiseless and noisy
scenarios. For the noiseless case  i.e.  z = 0  we choose R = 3  p1 = p2 = p3 = 100  m =
5 ⇥ R(p1 + p2 + p3) = 4500  and s = 200. Different values of n = 104  105  and 106 are chosen to
compare both statistical and computational performances of OLS and SOLS. For the noisy case  the
settings of all parameters are identical to those in the noiseless case  except that z = 1. We provide
a plot of the scaled objective versus the number of iterations for some random trials in Figure 1. The
2/n for OLS  where
scaled objective is set to be kA#t

2/n for SOLS and kA#t

1 ··· ✓(r)

SOLS  bk2

OLS  bk2

r=1 ↵r✓(r)

d

e
v
i
t
c
e
j
b
O

105

100

10-5

10-10

SOLS n1
SOLS n2
SOLS n3
OLS n1
OLS n2
OLS n3

106

104

102

100

e
v
i
t
c
e
j
b
O

SOLS n1
SOLS n2
SOLS n3
OLS n1
OLS n2
OLS n3

5

10
Iteration
(a) z = 0

15

20

25

5

10

15

20

Iteration
(b) z = 1

Figure 1: Comparison of SOLS and OLS on synthetic data. The vertical axis corresponds to the scaled
objectives kA#t
2/n for OLS  where #t is the update in the
t-th iteration. The horizontal axis corresponds to the number of iterations (passes of block-coordinate
minimization for all blocks). For both the noiseless case z = 0 and noisy case z = 1  we set
n1 = 104  n2 = 105  and n3 = 106 respectively.

2/n for SOLS and kA#t

SOLS  bk2

OLS  bk2

8

2/n and kA#SOLS  bk2

OLS are the updates in the t-th iterations of SOLS and OLS respectively. Note the we are
SOLS and #t
#t
2/n as the objective function for solving the SOLS problem  but looking at
using kA#SOLS  bk2
2/n for the solution of SOLS is ultimately what we are interested
the original objective kA#SOLS  bk2
2/n is very
in. However  we have that the gap between kA#SOLS  bk2
small in our results (< 1%). The number of iterations is the number of passes of block-coordinate
minimization for all blocks. We can see that OLS and SOLS require approximately the same number
of iterations for comparable decrease in objective function value. However  since the SOLS instance
has a much smaller size  its per iteration computational cost is much lower than that of OLS.
We further provide numerical results on the running time (CPU execution time) and the optimal
scaled objectives in Table 1. Using the same stopping criterion  we see that SOLS and OLS achieve
comparable objectives (within < 5% differences)  matching our theory. In terms of the running time 
SOLS is signiﬁcantly faster than OLS  especially when n is large compared to the sketching dimen
sion m. For example  when n = 106  SOLS is more than 200 times faster than OLS while achieving
a comparable objective function value with OLS. This matches with our theoretical results on the
computational cost of OLS versus SOLS. Note that here we suppose that the rank is known for our
simulation  which can be restrictive in practice. We observe that if we choose a moderately larger
rank than the true rank of the underlying model  then the results are similar to what we discussed
above. Smaller values of the rank result in a much deteriorated statistical performance for both OLS
and SOLS.
We also examine sketching of low-rank tensor regression on a real dataset of MRI images [22]. The
dataset consists of 56 frames of a human brain  each of which is of dimension 128 ⇥ 128 pixels  i.e. 
p1 = p2 = 128 and p3 = 56. The generation of design tensors {Ai}n
i=1 and linear measurements
b follows the same settings as for the synthetic data  with z = 0. We choose three values of
R = 3  5  10  and set m = 5 ⇥ R(p1 + p2 + p3). The sample size is set to n = 104 for all settings of
R. Analogous to the synthetic data  we provide numerical results for SOLS and OLS on the running
time (CPU execution time) and the optimal scaled objectives. The results are provided in Table 2.
Again  we have that SOLS is much faster than OLS and they achieve comparable optimal objectives 
under all settings of ranks.

Table 1: Comparison of SOLS and OLS on CPU execution time (in seconds) and the optimal scaled
objective over different choices of sample sizes and noise levels on synthetic data. The results are
averaged over 50 random trials  with both the mean values and standard deviations (in parentheses)
provided. Note that we terminate the program after the running time exceeds 3 ⇥ 104 seconds.

Variance of Noise

Sample Size

Time

Objective

OLS

SOLS

OLS

SOLS

n = 104
175.37
(65.784)
120.34
(35.711)
< 1010
(< 1010)
< 1010
(< 1010)

z = 0
n = 105
3683.9
(1496.7)
128.09
(37.293)
< 1010
(< 1010)
< 1010
(< 1010)

n = 106
> 3 ⇥ 104

(NA)
132.93
(38.649)
< 1010
(< 1010)
< 1010
(< 1010)

n = 104
168.62
(24.570)
121.71
(34.214)
0.9153
(0.0256)
0.9376
(0.0261)

z = 1
n = 105
2707.3
(897.14)
124.84
(33.774)
0.9341
(0.0213)
0.9817
(0.0242)

n = 106
> 3 ⇥ 104

(NA)
128.65
(32.863)
0.9425
(0.0172)
0.9901
(0.0256)

Table 2: Comparison of SOLS and OLS on CPU execution time (in seconds) and the optimal scaled
objective over different choices of ranks on the MRI data. The results are averaged over 10 random
trials  with both the mean values and standard deviations (in parentheses) provided.

Rank

Time

Objective

R = 3
2824.4
(768.08)
16.003
(0.1378)

OLS
R = 5
8137.2
(1616.3)
11.164
(0.1152)

R = 10
26851
(8320.1)
6.8679
(0.0471)

R = 3
196.31
(68.180)
17.047
(0.1561)

SOLS
R = 5
364.09
(145.79)
11.992
(0.1538)

R = 10
761.73
(356.76)
7.3968
(0.0975)

9

References
[1] Mohammad Taha Bahadori  Qi Rose Yu  and Yan Liu. Fast multivariate spatio-temporal analysis
via low-rank tensor learning. In Advances in Neural Information Processing Systems  pages
3491–3499  2014.

[2] Jean Bourgain  Sjoerd Dirksen  and Jelani Nelson. Toward a uniﬁed theory of sparse dimen-
sionality reduction in Euclidean space. Geometric and Functional Analysis  25(4):1009–1088 
2015.

[3] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input
sparsity time. In Proceedings of the 45th Annual ACM Symposium on Theory of Computing 
pages 81–90. ACM  2013.

[4] Anirban Dasgupta  Ravi Kumar  and Tamás Sarlós. A sparse Johnson–Lindenstrauss transform.
In Proceedings of the 42nd Annual ACM Symposium on Theory of Computing  pages 341–350.
ACM  2010.

[5] Lieven De Lathauwer  Bart De Moor  and Joos Vandewalle. A multilinear singular value
decomposition. SIAM Journal on Matrix Analysis and Applications  21(4):1253–1278  2000.

[6] Sjoerd Dirksen. Dimensionality reduction with subgaussian matrices: A uniﬁed theory. Foun-

dations of Computational Mathematics  pages 1–30  2015.

[7] Petros Drineas  Malik Magdon-Ismail  Michael W Mahoney  and David P Woodruff. Fast
approximation of matrix coherence and statistical leverage. Journal of Machine Learning
Research  13(Dec):3475–3506  2012.

[8] Weiwei Guo  Irene Kotsia  and Ioannis Patras. Tensor learning for regression. IEEE Transactions

on Image Processing  21(2):816–827  2012.

[9] Nathan Halko  Per-Gunnar Martinsson  and Joel A Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review 
53(2):217–288  2011.

[10] Jarvis Haupt  Xingguo Li  and David P Woodruff. Near optimal sketching of low-rank tensor

regression. arXiv preprint arXiv:1709.07093  2017.

[11] Peter D Hoff. Multilinear tensor regression for longitudinal relational data. The Annals of

Applied Statistics  9(3):1169  2015.

[12] Daniel M. Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. Journal of the

ACM  61(1):4:1–4:23  2014.

[13] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Review 

51(3):455–500  2009.

[14] Bingxiang Li  Wen Li  and Lubin Cui. New bounds for perturbation of the orthogonal projection.

Calcolo  50(1):69–78  2013.

[15] Xiaoshan Li  Hua Zhou  and Lexin Li. Tucker tensor regression and neuroimaging analysis.

arXiv preprint arXiv:1304.5637  2013.

[16] Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends R

in Machine Learning  3(2):123–224  2011.

[17] Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data

analysis. Proceedings of the National Academy of Sciences  106(3):697–702  2009.

[18] Jelani Nelson and Huy L Nguyen. Lower bounds for oblivious subspace embeddings. In
International Colloquium on Automata  Languages  and Programming  pages 883–894. Springer 
2014.

10

[19] Sung Won Park and Marios Savvides.

Individual kernel tensor-subspaces for robust face
recognition: A computationally efﬁcient tensor framework without requiring mode factorization.
IEEE Transactions on Systems  Man  and Cybernetics  Part B (Cybernetics)  37(5):1156–1166 
2007.

[20] Garvesh Raskutti and Ming Yuan. Convex regularization for high-dimensional tensor regression.

arXiv preprint arXiv:1512.01215  2015.

[21] Bernardino Romera-Paredes  Hane Aung  Nadia Bianchi-Berthouze  and Massimiliano Pontil.
Multilinear multitask learning. In Proceedings of the 30th International Conference on Machine
Learning  pages 1444–1452  2013.

[22] Antoine Rosset  Luca Spadola  and Osman Ratib. Osirix: an open-source software for navigating

in multidimensional DICOM images. Journal of Digital Imaging  17(3):205–216  2004.

[23] Zhongmin Shen. Lectures on Finsler geometry  volume 2001. World Scientiﬁc  2001.
[24] Nicholas Sidiropoulos  Lieven De Lathauwer  Xiao Fu  Kejun Huang  Evangelos Papalexakis 
and Christos Faloutsos. Tensor decomposition for signal processing and machine learning.
IEEE Transactions on Signal Processing  2017.

[25] Michel Talagrand. The generic chaining: upper and lower bounds of stochastic processes.

Springer Science &amp; Business Media  2006.

[26] Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances

in Adaptive Data Analysis  3(01n02):115–126  2011.

[27] Paul Tseng. Convergence of a block coordinate descent method for nondifferentiable minimiza-

tion. Journal of Optimization Theory and Applications  109(3):475–494  2001.

[28] Paul Tseng and Sangwoon Yun. A coordinate gradient descent method for nonsmooth separable

minimization. Mathematical Programming  117(1-2):387–423  2009.

[29] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[30] David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends R

in Theoretical Computer Science  10(1–2):1–157  2014.

[31] Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor

factorisation approach. arXiv preprint arXiv:1605.06391  2016.

[32] Rose Yu  Dehua Cheng  and Yan Liu. Accelerated online low-rank tensor learning for multivari-

ate spatio-temporal streams. In International Conference on Machine Learning  2015.

[33] Rose Yu and Yan Liu. Learning from multiway data: Simple and efﬁcient tensor regression. In

International Conference on Machine Learning  pages 373–381  2016.

[34] Qibin Zhao  Cesar F Caiafa  Danilo P Mandic  Zenas C Chao  Yasuo Nagasaka  Naotaka Fujii 
Liqing Zhang  and Andrzej Cichocki. Higher order partial least squares (hopls): a generalized
multilinear regression method. IEEE Transactions on Pattern Analysis and Machine Intelligence 
35(7):1660–1673  2013.

[35] Hua Zhou. Matlab TensorReg toolbox.

tensorreg/  2013.

http://hua-zhou.github.io/softwares/

[36] Hua Zhou  Lexin Li  and Hongtu Zhu. Tensor regression with applications in neuroimaging

data analysis. Journal of the American Statistical Association  108(502):540–552  2013.

11

,Xingguo Li
Jarvis Haupt
David Woodruff
Tavor Baharav
David Tse