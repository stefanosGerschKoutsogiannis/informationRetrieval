2014,Learning Mixtures of Ranking Models,This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a {\em Mallows Mixture Model}. Despite being widely studied  current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-$k$ prefix in both the rankings. Before this work  even the question of {\em identifiability} in the case of a mixture of two Mallows models was unresolved.,Learning Mixtures of Ranking Models∗

Pranjal Awasthi

Princeton University

pawashti@cs.princeton.edu

Avrim Blum

Carnegie Mellon University

avrim@cs.cmu.edu

Or Sheffet

Harvard University

osheffet@seas.harvard.edu

Aravindan Vijayaraghavan

New York University

vijayara@cims.nyu.edu

Abstract

This work concerns learning probabilistic models for ranking data in a heteroge-
neous population. The speciﬁc problem we study is learning the parameters of a
Mallows Mixture Model. Despite being widely studied  current heuristics for this
problem do not have theoretical guarantees and can get stuck in bad local optima.
We present the ﬁrst polynomial time algorithm which provably learns the param-
eters of a mixture of two Mallows models. A key component of our algorithm is
a novel use of tensor decomposition techniques to learn the top-k preﬁx in both
the rankings. Before this work  even the question of identiﬁability in the case of a
mixture of two Mallows models was unresolved.

1

Introduction

Probabilistic modeling of ranking data is an extensively studied problem with a rich body of past
work [1  2  3  4  5  6  7  8  9]. Ranking using such models has applications in a variety of areas
ranging from understanding user preferences in electoral systems and social choice theory  to more
modern learning tasks in online web search  crowd-sourcing and recommendation systems. Tradi-
tionally  models for generating ranking data consider a homogeneous group of users with a central
ranking (permutation) π∗ over a set of n elements or alternatives. (For instance  π∗ might corre-
spond to a “ground-truth ranking” over a set of movies.) Each individual user generates her own
ranking as a noisy version of this one central ranking and independently from other users. The most
popular ranking model of choice is the Mallows model [1]  where in addition to π∗ there is also a
scaling parameter φ ∈ (0  1). Each user picks her ranking π w.p. proportional to φdkt(π π∗) where
dkt(·) denotes the Kendall-Tau distance between permutations (see Section 2).1 We denote such a
model as Mn(φ  π∗).
The Mallows model and its generalizations have received much attention from the statistics  political
science and machine learning communities  relating this probabilistic model to the long-studied
work about voting and social choice [10  11]. From a machine learning perspective  the problem is
to ﬁnd the parameters of the model — the central permutation π∗ and the scaling parameter φ  using
independent samples from the distribution. There is a large body of work [4  6  5  7  12] providing
efﬁcient algorithms for learning the parameters of a Mallows model.

∗This work was supported in part by NSF grants CCF-1101215  CCF-1116892  the Simons Institute  and a
Simons Foundation Postdoctoral fellowhsip. Part of this work was performed while the 3rd author was at the
Simons Institute for the Theory of Computing at the University of California  Berkeley and the 4th author was
at CMU.

1In fact  it was shown [1] that this model is the result of the following simple (inefﬁcient) algorithm: rank
1+φ they agree with π∗ and with

(cid:1) pairs agree on a single ranking – output this ranking  otherwise resample.

every pair of elements randomly and independently s.t. with probability
probability φ

1+φ they don’t; if all(cid:0)n

2

1

1

1

1  φ(cid:48)

1  π(cid:48)

1  w(cid:48)

2  φ(cid:48)

1

φ1(1−φ1)  

φ2(1−φ2)   −1).

In many scenarios  however  the population is heterogeneous with multiple groups of people  each
with their own central ranking [2]. For instance  when ranking movies  the population may be di-
vided into two groups corresponding to men and women; with men ranking movies with one under-
lying central permutation  and women ranking movies with another underlying central permutation.
This naturally motivates the problem of learning a mixture of multiple Mallows models for rankings 
a problem that has received signiﬁcant attention [8  13  3  4]. Heuristics like the EM algorithm have
been applied to learn the model parameters of a mixture of Mallows models [8]. The problem has
also been studied under distributional assumptions over the parameters  e.g. weights derived from
a Dirichlet distribution [13]. However  unlike the case of a single Mallows model  algorithms with
provable guarantees have remained elusive for this problem.
In this work we give the ﬁrst polynomial time algorithm that provably learns a mixture of two
Mallows models. The input to our algorithm consists of i.i.d random rankings (samples)  with
each ranking drawn with probability w1 from a Mallows model Mn(φ1  π1)  and with probability
w2(= 1 − w1) from a different model Mn(φ2  π2).
Informal Theorem. Given sufﬁciently many i.i.d samples drawn from a mixture of two Mallows
models  we can learn the central permutations π1  π2 exactly and parameters φ1  φ2  w1  w2 up to
-accuracy in time poly(n  (min{w1  w2})−1 
It is worth mentioning that  to the best of our knowledge  prior to this work even the question of iden-
tiﬁability was unresolved for a mixture of two Mallows models; given inﬁnitely many i.i.d. samples
generated from a mixture of two distinct Mallow models with parameters {w1  φ1  π1  w2  φ2  π2}
(with π1 (cid:54)= π2 or φ1 (cid:54)= φ2)  could there be a different set of parameters {w(cid:48)
2}
2  π(cid:48)
which explains the data just as well. Our result shows that this is not the case and the mixture is
uniquely identiﬁable given polynomially many samples.
Intuition and a Na¨ıve First Attempt. It is evident that having access to sufﬁciently many random
samples allows one to learn a single Mallows model. Let the elements in the permutations be denoted
as {e1  e2  . . .   en}. In a single Mallows model  the probability of element ei going to position j (for
j ∈ [n]) drops off exponentially as one goes farther from the true position of ei [12]. So by assigning
each ei the most frequent position in our sample  we can ﬁnd the central ranking π∗.
The above mentioned intuition suggests the following clustering based approach to learn a mixture
of two Mallows models — look at the distribution of the positions where element ei appears. If the
distribution has 2 clearly separated “peaks” then they will correspond to the positions of ei in the
central permutations. Now  dividing the samples according to ei being ranked in a high or a low
position is likely to give us two pure (or almost pure) subsamples  each one coming from a single
Mallows model. We can then learn the individual models separately. More generally  this strategy
works when the two underlying permutations π1 and π2 are far apart which can be formulated as
a separation condition.2 Indeed  the above-mentioned intuition works only under strong separator
conditions: otherwise  the observation regarding the distribution of positions of element ei is no
longer true 3. For example  if π1 ranks ei in position k and π2 ranks ei in position k + 2  it is likely
that the most frequent position of ei is k + 1  which differs from ei’s position in either permutations!
Handling arbitrary permutations. Learning mixture models under no separation requirements is
a challenging task. To the best of our knowledge  the only polynomial time algorithm known is
for the case of a mixture of a constant number of Gaussians [17  18]. Other works  like the recent
developments that use tensor based methods for learning mixture models without distance-based
separation condition [19  20  21] still require non-degeneracy conditions and/or work for speciﬁc
sub cases (e.g. spherical Gaussians).
These sophisticated tensor methods form a key component in our algorithm for learning a mixture
of two Mallows models. This is non-trivial as learning over rankings poses challenges which are
not present in other widely studied problems such as mixture of Gaussians. For the case of Gaus-
sians  spectral techniques have been extremely successful [22  16  19  21]. Such techniques rely on
estimating the covariances and higher order moments in terms of the model parameters to detect
structure and dependencies. On the other hand  in the mixture of Mallows models problem there is

can be roughly stated as (cid:107)π1 − π2(cid:107)∞ = ˜Ω(cid:0)(min{w1  w2})−1 · (min{log(1/φ1)  log(1/φ2)}))−1(cid:1).

2Identifying a permutation π over n elements with a n-dimensional vector (π(i))i  this separation condition

3Much like how other mixture models are solvable under separation conditions  see [14  15  16].

2

no “natural” notion of a second/third moment. A key contribution of our work is deﬁning analogous
notions of moments which can be represented succinctly in terms of the model parameters. As we
later show  this allows us to use tensor based techniques to get a good starting solution.
Overview of Techniques. One key difﬁculty in arguing about the Mallows model is the lack of
closed form expressions for basic propositions like “the probability that the i-th element of π∗ is
ranked in position j.” Our ﬁrst observation is that the distribution of a given element appearing at
the top  i.e. the ﬁrst position  behaves nicely. Given an element e whose rank in the central ranking
π∗ is i  the probability that a ranking sampled from a Mallows model ranks e as the ﬁrst element is
∝ φi−1. A length n vector consisting of these probabilities is what we deﬁne as the ﬁrst moment
vector of the Mallows model. Clearly by sorting the coordinate of the ﬁrst moment vector  one can
recover the underlying central permutation and estimate φ. Going a step further  consider any two
elements which are in positions i  j respectively in π∗. We show that the probability that a ranking
sampled from a Mallows model ranks {i  j} in (any of the 2! possible ordering of) the ﬁrst two
positions is ∝ f (φ)φi+j−2. We call the n × n matrix of these probabilities as the second moment
matrix of the model (analogous to the covariance matrix). Similarly  we deﬁne the 3rd moment
tensor as the probability that any 3 elements appear in positions {1  2  3}. We show in the next
section that in the case of a mixture of two Mallows models  the 3rd moment tensor deﬁned this way
has a rank-2 decomposition  with each rank-1 term corresponds to the ﬁrst moment vector of each of
two Mallows models. This motivates us to use tensor-based techniques to estimate the ﬁrst moment
vectors of the two Mallows models  thus learning the models’ parameters.
The above mentioned strategy would work if one had access to inﬁnitely many samples from the
mixture model. But notice that the probabilities in the ﬁrst-moment vectors decay exponentially  so
by using polynomially many samples we can only recover a preﬁx of length ∼ log1/φ n from both
rankings. This forms the ﬁrst part of our algorithm which outputs good estimates of the mixture
weights  scaling parameters φ1  φ2 and preﬁxes of a certain size from both the rankings. Armed
with w1  w2 and these two preﬁxes we next proceed to recover the full permutations π1 and π2.
In order to do this  we take two new fresh batches of samples. On the ﬁrst batch  we estimate
the probability that element e appears in position j for all e and j. On the second batch  which is
noticeably larger than the ﬁrst  we estimate the probability that e appears in position j conditioned
on a carefully chosen element e∗ appearing as the ﬁrst element. We show that this conditioning is
almost equivalent to sampling from the same mixture model but with rescaled weights w(cid:48)
1 and w(cid:48)
2.
The two estimations allow us to set a system of two linear equations in two variables: f (1) (e → j) –
the probability of element e appearing in position j in π1  and f (2) (e → j) — the same probability
for π2. Solving this linear system we ﬁnd the position of e in each permutation.
The above description contains most of the core ideas involved in the algorithm. We need two
additional components. First  notice that the 3rd moment tensor is not well deﬁned for triplets
(i  j  k)  when i  j  k are not all distinct and hence cannot be estimated from sampled data. To get
around this barrier we consider a random partition of our element-set into 3 disjoint subsets. The
actual tensor we work with consists only of triplets (i  j  k) where the indices belong to different
partitions. Secondly  we have to handle the case where tensor based-technique fails  i.e. when the
3rd moment tensor isn’t full-rank. This is a degenerate case. Typically  tensor based approaches for
other problems cannot handle such degenerate cases. However  in the case of the Mallows mixture
model  we show that such a degenerate case provides a lot of useful information about the problem.
In particular  it must hold that φ1 (cid:39) φ2  and π1 and π2 are fairly close — one is almost a cyclic
shift of the other. To show this we use a characterization of the when the tensor decomposition is
unique (for tensors of rank 2)  and we handle such degenerate cases separately. Altogether  we ﬁnd
the mixture model’s parameters with no non-degeneracy conditions.
Lower bound under the pairwise access model. Given that a single Mallows model can be learned
using only pairwise comparisons  a very restricted access to each sample  it is natural to ask  “Is it
possible to learn a mixture of Mallows models from pairwise queries?”. This next example shows
that we cannot hope to do this even for a mixture of two Mallows models. Fix some φ and π and
assume our sample is taken using mixing weights of w1 = w2 = 1
2 from the two Mallows models
Mn(φ  π) and Mn(φ  rev(π))  where rev(π) indicates the reverse permutation (the ﬁrst element of
π is the last of rev(π)  the second is the next-to-last  etc.) . Consider two elements  e and e(cid:48). Using
only pairwise comparisons  we have that it is just as likely to rank e > e(cid:48) as it is to rank e(cid:48) > e and
so this case cannot be learned regardless of the sample size.

3

3-wise queries. We would also like to stress that our algorithm does not need full access to the
sampled rankings and instead will work with access to certain 3-wise queries. Observe that the ﬁrst
part of our algorithm  where we recover the top elements in each of the two central permutations 
only uses access to the top 3 elements in each sample. In that sense  we replace the pairwise query
“do you prefer e to e(cid:48)?” with a 3-wise query: “what are your top 3 choices?” Furthermore  the
second part of the algorithm (where we solve a set of 2 linear equations) can be altered to support
3-wise queries of the (admittedly  somewhat unnatural) form “if e∗ is your top choice  do you prefer
e to e(cid:48)?” For ease of exposition  we will assume full-access to the sampled rankings.
Future Directions. Several interesting directions come out of this work. A natural next step is to
generalize our results to learn a mixture of k Mallows models for k > 2. We believe that most
of these techniques can be extended to design algorithms that take poly(n  1/)k time. It would
also be interesting to get algorithms for learning a mixture of k Mallows models which run in time
poly(k  n)  perhaps in an appropriate smoothed analysis setting [23] or under other non-degeneracy
assumptions. Perhaps  more importantly  our result indicates that tensor based methods which have
been very popular for learning problems  might also be a powerful tool for tackling ranking-related
problems in the ﬁelds of machine learning  voting and social choice.
Organization. In Section 2 we give the formal deﬁnition of the Mallow model and of the problem
statement  as well as some useful facts about the Mallow model. Our algorithm and its numerous
subroutines are detailed in Section 3. In Section 4 we experimentally compare our algorithm with a
popular EM based approach for the problem. The complete details of our algorithms and proofs are
included in the supplementary material.

2 Notations and Properties of the Mallows Model
Let Un = {e1  e2  . . .   en} be a set of n distinct elements. We represent permutations over the
elements in Un through their indices [n]. (E.g.  π = (n  n − 1  . . .   1) represents the permutation
(en  en−1  . . .   e1).) Let posπ(ei) = π−1(i) refer to the position of ei in the permutation π. We
omit the subscript π when the permutation π is clear from context. For any two permutations π  π(cid:48)
we denote dkt(π  π(cid:48)) as the Kendall-Tau distance [24] between them (number of pairwise inversions
between π  π(cid:48)). Given some φ ∈ (0  1) we denote Zi(φ) = 1−φi
1−φ   and partition function Z[n](φ) =

π φdkt(π π0) =(cid:81)n
(cid:80)

i=1 Zi(φ) (see Section 6 in the supplementary material).

Deﬁnition 2.1. [Mallows model (Mn(φ  π0)).] Given a permutation π0 on [n] and a parameter
φ ∈ (0  1) 4  a Mallows model is a permutation generation process that returns permutation π w.p.

Pr (π) = φdkt(π π0)/Z[n](φ)

In Section 6 we show many useful properties of the Mallows model which we use repeatedly
throughout this work. We believe that they provide an insight to Mallows model  and we advise
the reader to go through them. We proceed with the main deﬁnition.
Deﬁnition 2.2. [Mallows Mixture model w1Mn(φ1  π1) ⊕ w2Mn(φ2  π2).] Given parameters
w1  w2 ∈ (0  1) s.t. w1 + w2 = 1  parameters φ1  φ2 ∈ (0  1) and two permutations π1  π2  we call
a mixture of two Mallows models to be the process that with probability w1 generates a permutation
from M (φ1  π1) and with probability w2 generates a permutation from M (φ2  π2).
Our next deﬁnition is crucial for our application of tensor decomposition techniques.
Deﬁnition 2.3. [Representative vectors.] The representative vector of a Mallows model is a vector
where for every i ∈ [n]  the ith-coordinate is φposπ(ei)−1/Zn.
The expression φposπ(ei)−1/Zn is precisely the probability that a permutation generated by a model
Mn(φ  π) ranks element ei at the ﬁrst position (proof deferred to the supplementary material).
Given that our focus is on learning a mixture of two Mallows models Mn(φ1  π1) and Mn(φ2  π2) 
we denote x as the representative vector of the ﬁrst model  and y as the representative vector of the
latter. Note that retrieving the vectors x and y exactly implies that we can learn the permutations π1
and π2 and the values of φ1  φ2.

4It is also common to parameterize using β ∈ R+ where φ = e−β. For small β we have (1 − φ) ≈ β.

4

A tensor T ∈ Rn1×n2×n3 has a rank-r decomposition if T can be expressed as(cid:80)

Finally  let f (i → j) be the probability that element ei goes to position j according to mixture
model. Similarly f (1) (i → j) be the corresponding probabilities according to Mallows model M1
and M2 respectively. Hence  f (i → j) = w1f (1) (i → j) + w2f (2) (i → j).
Tensors: Given two vectors u ∈ Rn1  v ∈ Rn2  we deﬁne u⊗v ∈ Rn1×n2 as the matrix uvT . Given
also z ∈ Rn3 then u⊗ v⊗ z denotes the 3-tensor (of rank- 1) whose (i  j  k)-th coordinate is uivjzk.
i∈[r] ui ⊗ vi ⊗ zi
where ui ∈ Rn1  vi ∈ Rn2  zi ∈ Rn3. Given two vectors u  v ∈ Rn  we use (u; v) to denote the
n × 2 matrix that is obtained with u and v as columns.
We now deﬁne ﬁrst  second and third order statistics (frequencies) that serve as our proxies for the
ﬁrst  second and third order moments.
Deﬁnition 2.4. [Moments] Given a Mallows mixture model  we denote for every i  j  k ∈ [n]
• Pi = Pr (pos (ei) = 1) is the probability that element ei is ranked at the ﬁrst position
• Pij = Pr (pos ({ei  ej}) = {1  2})  is the probability that ei  ej are ranked at the ﬁrst two

positions (in any order)

• Pijk = Pr (pos ({ei  ej  ek}) = {1  2  3}) is the probability that ei  ej  ek are ranked at

the ﬁrst three positions (in any order).

For convenience  let P represent the set of quantities (Pi  Pij  Pijk)1≤i<j<k≤n. These can be esti-
mated up to any inverse polynomial accuracy using only polynomial samples. The following simple 
yet crucial lemma relates P to the vectors x and y  and demonstrates why these statistics and repre-
sentative vectors are ideal for tensor decomposition.
Lemma 2.5. Given a mixture w1M (φ1  π1) ⊕ w2M (φ2  π2) let x  y and P be as deﬁned above.

1. For any i it holds that Pi = w1xi + w2yi.

2. Denote c2(φ) = Zn(φ)
Zn−1(φ)

w2c2(φ2)yiyj.

1+φ

φ . Then for any i (cid:54)= j it holds that Pij = w1c2(φ1)xixj +

3. Denote c3(φ) =

Pijk = w1c3(φ1)xixjxk + w2c3(φ2)yiyjyk.

Zn−1(φ)Zn−2(φ)

φ3

Z2

n(φ)

1+2φ+2φ2+φ3

. Then for any distinct i  j  k it holds that

Clearly  if i = j then Pij = 0  and if i  j  k are not all distinct then Pijk = 0.
In addition  in Lemma 13.2 in the supplementary material we prove the bounds c2(φ) = O(1/φ)
and c3(φ) = O(φ−3).
Partitioning Indices: Given a partition of [n] into Sa  Sb  Sc  let x(a)  y(a) be the representative
vectors x  y restricted to the indices (rows) in Sa (similarly for Sb  Sc). Then the 3-tensor

T (abc) ≡ (Pijk)i∈Sa j∈Sb k∈Sc = w1c3(φ1)x(a) ⊗ x(b) ⊗ x(c) + w2c3(φ2)y(a) ⊗ y(b) ⊗ y(c).

This tensor has a rank-2 decomposition  with one rank-1 term for each Mallows model. Finally for
convenience we deﬁne the matrix M = (x; y)  and similarly deﬁne the matrices Ma = (x(a); y(a)) 
Mb = (x(b); y(b))  Mc = (x(c); y(c)).
Error Dependency and Error Polynomials. Our algorithm gives an estimate of the parameters
w  φ that we learn in the ﬁrst stage  and we use these estimates to ﬁgure out the entire central rankings
in the second stage. The following lemma essentially allows us to assume instead of estimations  we
have access to the true values of w and φ.
Lemma 2.6. For every δ > 0 there exists a function f (n  φ  δ) s.t. for every n  φ and ˆφ satisfying
|φ− ˆφ| <

f (n φ δ) we have that the total-variation distance satisﬁes (cid:107)M (φ  π)−M(cid:16) ˆφ  π

(cid:17)(cid:107)TV ≤ δ.

δ

For the ease of presentation  we do not optimize constants or polynomial factors in all parameters.
In our analysis  we show how our algorithm is robust (in a polynomial sense) to errors in various
statistics  to prove that we can learn with polynomial samples. However  the simpliﬁcation when
there are no errors (inﬁnite samples) still carries many of the main ideas in the algorithm — this in
fact shows the identiﬁability of the model  which was not known previously.

5

3 Algorithm Overview

Algorithm 1 LEARN MIXTURES OF TWO MALLOWS MODELS  Input: a set S of N samples from
w1M (φ1  π1) ⊕ w2M (φ2  π2)  Accuracy parameters   2.
1. Let (cid:98)P be the empirical estimate of P on samples in S.
(a) Partition [n] randomly into Sa  Sb and Sc. Let T (abc) =(cid:0)(cid:98)Pijk

2. Repeat O(log n) times:

(cid:1)

.

i∈Sa j∈Sb k∈Sc

(b) Run TENSOR-DECOMP from [25  26  23] to get a decomposition of T (abc) = u(a) ⊗ u(b) ⊗
(c) If min{σ2(u(a); v(a))  σ2(u(b); v(b))  σ2(u(c); v(c))} > 2

u(c) + v(a) ⊗ v(b) ⊗ v(c).

(In the non-degenerate case these matrices are far from being rank-1 matrices in the sense that
their least singular value is bounded away from 0.)

i. Obtain parameter estimates ((cid:98)w1 (cid:98)w2 (cid:98)φ1 (cid:98)φ2 and preﬁxes of the central rankings π1
from INFER-TOP-K((cid:98)P   M(cid:48)
ii. Use RECOVER-REST to ﬁnd the full central rankings(cid:98)π1 (cid:98)π2.
Return SUCCESS and output ((cid:98)w1 (cid:98)w2 (cid:98)φ1 (cid:98)φ2 (cid:98)π1 (cid:98)π2).

i = (u(i); v(i)) for i ∈ {a  b  c}.

c)  with M(cid:48)

a  M(cid:48)

b  M(cid:48)

(cid:48)  π2

(cid:48))

3. Run HANDLE DEGENERATE CASES ((cid:98)P ).

1

  1
w2

max

n 

1

(cid:17)

1

φ1(1−φ1)  

(cid:16)

min{ 0}  

φ2(1−φ2)   1
w1

. Then  given any 0 <  < 0  suitably small 2 = poly( 1

Our algorithm (Algorithm 1) has two main components. First we invoke a decomposition algo-
rithm [25  26  23] over the tensor T (abc)  and retrieve approximations of the two Mallows models’
representative vectors which in turn allow us to approximate the weight parameters w1  w2  scale
parameters φ1  φ2  and the top few elements in each central ranking. We then use the inferred pa-
rameters to recover the entire rankings π1 and π2. Should the tensor-decomposition fail  we invoke
a special procedure to handle such degenerate cases. Our algorithm has the following guarantee.
Theorem 3.1. Let w1M (φ1  π1) ⊕ w2M (φ2  π2) be a mixture of two Mallows models and let
wmin = min{w1  w2} and φmax = max{φ1  φ2} and similarly φmin = min{φ1  φ2}. Denote
min(1−φmax)10
0 = w2
n     φmin  wmin)
16n22φ2
i.i.d samples from the mixture model 
and N = poly
Algorithm 1 recovers  in poly-time and with probability ≥ 1 − n−3  the model’s parameters with
w1  w2  φ1  φ2 recovered up to -accuracy.
Next we detail the various subroutines of the algorithm  and give an overview of the analysis for
each subroutine. The full analysis is given in the supplementary material.
The TENSOR-DECOMP Procedure. This procedure is a straight-forward invocation of the al-
gorithm detailed in [25  26  23]. This algorithm uses spectral methods to retrieve the two vec-
tors generating the rank-2 tensor T (abc). This technique works when all factor matrices Ma =
(x(a); y(a))  Mb = (x(b); y(b))  Mc = (x(c); y(c)) are well-conditioned. We note that any algorithm
that decomposes non-symmetric tensors which have well-conditioned factor matrices  can be used
as a black box.
Lemma 3.2 (Full rank case). In the conditions of Theorem 3.1  suppose our algorithm picks
some partition Sa  Sb  Sc such that the matrices Ma  Mb  Mc are all well-conditioned — i.e. have
σ2(Ma)  σ2(Mb)  σ2(Mc) ≥ (cid:48)
n     2  w1  w2) then with high probability  Algorithm
TENSORDECOMP of [25] ﬁnds M(cid:48)
c = (u(c); v(c)) such
that for any τ ∈ {a  b  c}  we have u(τ ) = ατ x(τ ) + z(τ )
2 ; with
1 (cid:107) (cid:107)z(τ )
(cid:107)z(τ )
The INFER-TOP-K procedure. This procedure uses the output of the tensor-decomposition to
retrieve the weights  φ’s and the representative vectors. In order to convert u(a)  u(b)  u(c) into an
approximation of x(a)  x(b)  x(c) (and similarly with v(a)  v(b)  v(c) and y(a)  y(b)  y(c))  we need to
ﬁnd a good approximation of the scalars αa  αb  αc. This is done by solving a certain linear system.
ﬁrst elements of π1 — we sort the coordinates of x  setting π(cid:48)

This also allows us to estimate (cid:98)w1 (cid:98)w2. Given our approximation of x  it is easy to ﬁnd φ1 and the top

n     2  wmin) and  σ2(M(cid:48)

τ ) > 2 for τ ∈ {a  b  c}.

1 to be the ﬁrst elements in the sorted

a = (u(a); v(a))  M(cid:48)

b = (u(b); v(b))  M(cid:48)

and v(τ ) = βτ y(τ ) + z(τ )

2 (cid:107) ≤ poly( 1

2 ≥ poly( 1

1

6

vector  and φ1 as the ratio between any two adjacent entries in the sorted vector. We refer the reader
to Section 8 in the supplementary material for full details. The RECOVER-REST procedure. The
algorithm for recovering the remaining entries of the central permutations (Algorithm 2) is more
involved.
Algorithm 2 RECOVER-REST  Input: a set S of N samples from w1M (φ1  π1)⊕ w2M (φ2  π2) 
parameters ˆw1  ˆw2  ˆφ1  ˆφ2 and initial permutations ˆπ1  ˆπ2  and accuracy parameter .

1. For elements in ˆπ1 and ˆπ2  compute representative vectors ˆx and ˆy using estimates ˆφ1 and ˆφ2.
2. Let | ˆπ1| = r1  | ˆπ2| = r2 and wlog r1 ≥ r2.
If there exists an element ei such that posˆπ1
case)  then:
Let S1 be the subsample with ei ranked in the ﬁrst position.
(a) Learn a single Mallows model on S1 to ﬁnd ˆπ1. Given ˆπ1 use dynamic programming to ﬁnd ˆπ2
3. Let ei∗ be the ﬁrst element in ˆπ1 having its probabilities of appearing in ﬁrst place in π1 and π2 differ
1. Let S1 be the subsample with ei∗

(ei) < r2/2 (or in the symmetric

(ei) > r1 and posˆπ2

2 = 1 − ˆw(cid:48)

(cid:17)−1

and ˆw(cid:48)

(cid:16)

1 + ˆw2
ˆw1

ˆy(ei∗ )
ˆx(ei∗ )

by at least . Deﬁne ˆw(cid:48)
1 =
ranked at the ﬁrst position.

4. For each ei that doesn’t appear in either ˆπ1 or ˆπ2 and any possible position j it might belong to

(a) Use S to estimate ˆfi j = Pr (ei goes to position j)  and S1 to estimate ˆf (i → j|ei∗ → 1) =

Pr (ei goes to position j|ei∗ (cid:55)→ 1).

(b) Solve the system

(1)
(2)
5. To complete ˆπ1 assign each ei to position arg maxj{f (1) (i → j)}. Similarly complete ˆπ2 using

ˆf (i → j) = ˆw1f (1) (i → j) + ˆw2f (2) (i → j)
2f (2) (i → j)
(cid:48)

ˆf (i → j|ei∗ → 1) = ˆw

1f (1) (i → j) + ˆw
(cid:48)

f (2) (i → j). Return the two permutations.

Algorithm 2 ﬁrst attempts to ﬁnd a pivot — an element ei which appears at a fairly high rank in
one permutation  yet does not appear in the other preﬁx ˆπ2. Let Eei be the event that a permutation
ranks ei at the ﬁrst position. As ei is a pivot  then PrM1 (Eei) is noticeable whereas PrM2 (Eei )
is negligible. Hence  conditioning on ei appearing at the ﬁrst position leaves us with a subsample in
which all sampled rankings are generated from the ﬁrst model. This subsample allows us to easily
retrieve the rest of π1. Given π1  the rest of π2 can be recovered using a dynamic programming
procedure. Refer to the supplementary material for details.
The more interesting case is when no such pivot exists  i.e.  when the two preﬁxes of π1 and π2
contain almost the same elements. Yet  since we invoke RECOVER-REST after successfully calling
TENSOR-DECOMP   it must hold that the distance between the obtained representative vectors ˆx and
ˆy is noticeably large. Hence some element ei∗ satisﬁes |ˆx(ei∗ ) − ˆy(ei∗ )| >   and we proceed by
setting up a linear system. To ﬁnd the complete rankings  we measure appropriate statistics to set
up a system of linear equations to calculate f (1) (i → j) and f (2) (i → j) up to inverse polynomial
ranking of M1.

accuracy. The largest of these values(cid:8)f (1) (i → j)(cid:9) corresponds to the position of ei in the central
To compute the values(cid:8)f (r) (i → j)(cid:9)

r=1 2 we consider f (1) (i → j|ei∗ → 1) – the probability that
ei is ranked at the jth position conditioned on the element ei∗ ranking ﬁrst according to M1 (and
resp. for M2). Using w(cid:48)

1 and w(cid:48)

Pr (ei → j|ei∗ → 1) = w(cid:48)

2 as in Algorithm 2  it holds that
1f (1) (i → j|ei∗ → 1) + w(cid:48)

2f (2) (i → j|ei∗ → 1) .

We need to relate f (r) (i → j|ei∗ → 1) to f (r) (i → j).
Indeed Lemma 10.1 shows that
Pr (ei → j|ei∗ → 1) is an almost linear equations in the two unknowns. We show that if ei∗ is
ranked above ei in the central permutation  then for some small δ it holds that

2f (2) (i → j) ± δ
We refer the reader to Section 10 in the supplementary material for full details.

Pr (ei → j|ei∗ → 1) = w(cid:48)

1f (1) (i → j) + w(cid:48)

7

The HANDLE-DEGENERATE-CASES procedure. We call a mixture model w1M (φ1  π1) ⊕
w2M (φ2  π2) degenerate if the parameters of the two Mallows models are equal  and the edit dis-
tance between the preﬁxes of the two central rankings is at most two i.e.  by changing the positions
of at most two elements in π1 we retrieve π2. We show that unless w1M (φ1  π1)⊕w2M (φ2  π2) is
degenerate  a random partition (Sa  Sb  Sc) is likely to satisfy the requirements of Lemma 3.2 (and
TENSOR-DECOMP will be successful). Hence  if TENSOR-DECOMP repeatedly fail  we deduce our
model is indeed degenerate. To show this  we characterize the uniqueness of decompositions of rank
2  along with some very useful properties of random partitions. In such degenerate cases  we ﬁnd
the two preﬁxes and then remove the elements in the preﬁxes from U  and recurse on the remaining
elements. We refer the reader to Section 9 in the supplementary material for full details.

4 Experiments
Goal. The main contribution of our paper is devising an algorithm that provably learns any mixture
of two Mallows models. But could it be the case that the previously existing heuristics  even though
they are unproven  still perform well in practice? We compare our algorithm to existing techniques 
to see if  and under what settings our algorithm outperforms them.
Baseline. We compare our algorithm to the popular EM based algorithm of [5]  seeing as EM based
heuristics are the most popular way to learn a mixture of Mallows models. The EM algorithm starts
with a random guess for the two central permutations. At iteration t  EM maintains a guess as to
the two Mallows models that generated the sample. First (expectation step) the algorithm assigns a
weight to each ranking in our sample  where the weight of a ranking reﬂects the probability that it
was generated from the ﬁrst or the second of the current Mallows models. Then (the maximization
step) the algorithm updates its guess of the models’ parameters based on a local search – minimizing
the average distance to the weighted rankings in our sample. We comment that we implemented
only the version of our algorithm that handles non-degenerate cases (more interesting case). In our
experiment the two Mallows models had parameters φ1 (cid:54)= φ2  so our setting was never degenerate.
Setting. We ran both the algorithms on synthetic data comprising of rankings of size n = 10. The
weights were sampled u.a.r from [0  1]  and the φ-parameters were sampled by sampling ln(1/φ)

(cid:1) we generated the two central rankings π1 and π2 to

be within distance d in the following manner. π1 was always ﬁxed as (1  2  3  . . .   10). To describe
π2  observe that it sufﬁces to note the number of inversion between 1 and elements 2  3  ...  10; the
number of inversions between 2 and 3  4  ...  10 and so on. So we picked u.a.r a non-negative integral
solution to x1 + . . . + xn = d which yields a feasible permutation and let π2 be the permutation that
it details. Using these models’ parameters  we generated N = 5 · 106 random samples.
Evaluation Metric and Results. For each value of d  we ran both algorithms 20 times and counted
the fraction of times on which they returned the true rankings that generated the sample. The results
of the experiment for rankings of size n = 10 are in Table 1. Clearly  the closer the two centrals
rankings are to one another  the worst EM performs. On the other hand  our algorithm is able to
recover the true rankings even at very close distances. As the rankings get slightly farther  our algo-
rithm recovers the true rankings all the time. We comment that similar performance was observed
for other values of n as well. We also comment that our algorithm’s runtime was reasonable (less
than 10 minutes on a 8-cores Intel x86 64 computer). Surprisingly  our implementation of the EM
algorithm typically took much longer to run — due to the fact that it simply did not converge.

u.a.r from [0  5]. For d ranging from 0 to(cid:0)n

2

success rate of EM success rate of our algorithm

distance between rankings

0
2
4
8
16
24
30
35
40
45

0%
0%
0%
10%
30%
30%
60%
60%
80%
60%

10%
10%
40%
70%
60 %
100%
100%
100%
100%
100%

Table 1: Results of our experiment.

8

References
[1] C. L. Mallows. Non-null ranking models i. Biometrika  44(1-2)  1957.
[2] John I. Marden. Analyzing and Modeling Rank Data. Chapman & Hall  1995.
[3] Guy Lebanon and John Lafferty. Cranking: Combining rankings using conditional probability models on

permutations. In ICML  2002.

[4] Thomas Brendan Murphy and Donal Martin. Mixtures of distance-based models for ranking data. Com-

putational Statistics and Data Analysis  41  2003.

[5] Marina Meila  Kapil Phadnis  Arthur Patterson  and Jeff Bilmes. Consensus ranking under the exponential

model. Technical report  UAI  2007.

[6] Ludwig M. Busse  Peter Orbanz  and Joachim M. Buhmann. Cluster analysis of heterogeneous rank data.

In ICML  ICML ’07  2007.

[7] Bhushan Mandhani and Marina Meila. Tractable search for learning exponential models of rankings.

Journal of Machine Learning Research - Proceedings Track  5  2009.

[8] Tyler Lu and Craig Boutilier. Learning mallows models with pairwise preferences. In ICML  2011.
[9] Joel Oren  Yuval Filmus  and Craig Boutilier. Efﬁcient vote elicitation under candidate uncertainty. JCAI 

2013.

[10] H Peyton Young. Condorcet’s theory of voting. The American Political Science Review  1988.
[11] Persi Diaconis. Group representations in probability and statistics. Institute of Mathematical Statistics 

1988.

[12] Mark Braverman and Elchanan Mossel. Sorting from noisy information. CoRR  abs/0910.1191  2009.
[13] Marina Meila and Harr Chen. Dirichlet process mixtures of generalized mallows models. In UAI  2010.
[14] Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS  1999.
[15] Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In STOC  2001.
[16] Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In COLT 

2005.

[17] Adam Tauman Kalai  Ankur Moitra  and Gregory Valiant. Efﬁciently learning mixtures of two gaussians.

In STOC  STOC ’10  2010.

[18] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations

of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  2010.

[19] Anima Anandkumar  Rong Ge  Daniel Hsu  Sham M. Kakade  and Matus Telgarsky. Tensor decomposi-

tions for learning latent variable models. CoRR  abs/1210.7559  2012.

[20] Animashree Anandkumar  Daniel Hsu  and Sham M. Kakade. A method of moments for mixture models

and hidden markov models. In COLT  2012.

[21] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: moment methods and

spectral decompositions. In ITCS  ITCS ’13  2013.

[22] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. J. Comput. Syst.

Sci.  68(4)  2004.

[23] Aditya Bhaskara  Moses Charikar  Ankur Moitra  and Aravindan Vijayaraghavan. Smoothed analysis of

tensor decompositions. In Symposium on the Theory of Computing (STOC)  2014.

[24] M. G. Kendall. Biometrika  30(1/2)  1938.
[25] Aditya Bhaskara  Moses Charikar  and Aravindan Vijayaraghavan. Uniqueness of tensor decompositions

with applications to polynomial identiﬁability. CoRR  abs/1304.8087  2013.

[26] Naveen Goyal  Santosh Vempala  and Ying Xiao. Fourier pca. In Symposium on the Theory of Computing

(STOC)  2014.

[27] R.P. Stanley. Enumerative Combinatorics. Number v. 1 in Cambridge studies in advanced mathematics.

Cambridge University Press  2002.

9

,Haim Avron
Vikas Sindhwani
David Woodruff
Pranjal Awasthi
Avrim Blum
Or Sheffet
Aravindan Vijayaraghavan