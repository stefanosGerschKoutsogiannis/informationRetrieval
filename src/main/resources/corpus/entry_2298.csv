2008,Generative and Discriminative Learning with Unknown Labeling Bias,We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data  when the true class proportions are unknown. For the generative case  we derive an entropy-based weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case  we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data  an extreme case of label bias since there is no absence data. On a benchmark dataset  we find that entropy-based weighting offers an improvement over constant estimates of class proportions  consistently reducing log loss on unbiased test data.,Generative and Discriminative Learning with

Unknown Labeling Bias

Miroslav Dud´ık

Carnegie Mellon University

5000 Forbes Ave  Pittsburgh  PA 15213

Steven J. Phillips

AT&T Labs − Research

180 Park Ave  Florham Park  NJ 07932

mdudik@cmu.edu

phillips@research.att.com

Abstract

We apply robust Bayesian decision theory to improve both generative and discrim-
inative learners under bias in class proportions in labeled training data  when the
true class proportions are unknown. For the generative case  we derive an entropy-
based weighting that maximizes expected log likelihood under the worst-case true
class proportions. For the discriminative case  we derive a multinomial logistic
model that minimizes worst-case conditional log loss. We apply our theory to the
modeling of species geographic distributions from presence data  an extreme case
of labeling bias since there is no absence data. On a benchmark dataset  we ﬁnd
that entropy-based weighting offers an improvement over constant estimates of
class proportions  consistently reducing log loss on unbiased test data.

1 Introduction

In many real-world classiﬁcation problems  it is not equally easy or affordable to verify membership
in different classes. Thus  class proportions in labeled data may signiﬁcantly differ from true class
proportions. In an extreme case  labeled data for an entire class might be missing (for example 
negative experimental results are typically not published). A naively trained learner may perform
poorly on test data that is not similarly afﬂicted by labeling bias. Several techniques address labeling
bias in the context of cost-sensitive learning and learning from imbalanced data [5  11  2]. If the
labeling bias is known or can be estimated  and all classes appear in the training set  a model trained
on biased data can be corrected by reweighting [5]. When the labeling bias is unknown  a model is
often selected using threshold-independent analysis such as ROC curves [11]. A good ROC curve 
however  does not guarantee a low loss on test data. Here  we are concerned with situations when
the labeling bias is unknown and some classes may be missing  but we have access to unlabeled
data. We want to construct models that in addition to good ROC-based performance  also yield
low test loss. We will be concerned with minimizing joint and conditional log loss  or equivalently 
maximizing joint and conditional log likelihood.

Our work is motivated by the application of modeling species’ geographic distributions from occur-
rence data. The data consists of a set of locations within some region (for example  the Australian
wet tropics) where a species (such as the golden bowerbird) was observed  and a set of features such
as precipitation and temperature  describing environmental conditions at each location. Species dis-
tribution modeling suffers from extreme imbalance in training data: we often only have information
about species presence (positive examples)  but no information about species absence (negative ex-
amples). We do  however  have unlabeled data  obtained either by randomly sampling locations
from the region [4]  or pooling presence data for several species collected with similar methods to
yield a representative sample of locations which biologists have surveyed [13].

Previous statistical methods for species distribution modeling can be divided into three main ap-
proaches. The ﬁrst interprets all unlabeled data as examples of species absence and learns a rule

to discriminate them from presences [19  4]. The second embeds a discriminative learner in the
EM algorithm in order to infer presences and absences in unlabeled data; this explicitly requires
knowledge of true class probabilities [17]. The third models the presences alone  which is known in
machine learning as one-class estimation [14  7]. When using the ﬁrst approach  the training data is
commonly reweighted so that positive and negative examples have the same weight [4]; this models
a quantity monotonically related to conditional probability of presence [13]  with the relationship
depending on true class probabilities. If we use y to denote the binary variable indicating presence
and x to denote a location on the map  then the ﬁrst two approaches yield models of conditional
probability p(y = 1|x)  given estimates of true class probabilities. On the other hand  the main in-
stantiation of the third approach  maximum entropy density estimation (maxent) [14] yields a model
of the distribution p(x|y = 1). To convert this to an estimate of p(y = 1|x) (as is usually required 
and necessary for measuring conditional log loss on which we focus here) again requires knowledge
of the class probabilities p(y = 1) and p(y = 0). Thus  existing discriminative approaches (the ﬁrst
and second) as well as generative approaches (the third) require estimates of true class probabilities.

We apply robust Bayesian decision theory  which is closely related to the maximum entropy prin-
ciple [6]  to derive conditional probability estimates p(y | x) that perform well under a wide range
of test distributions. Our approach can be used to derive robust estimates of class probabilities p(y)
which are then used to reweight discriminative models or to convert generative models into discrimi-
native ones. We present a treatment for the general multiclass problem  but our experiments focus on
one-class estimation and species distribution modeling in particular. Using an extensive evaluation
on real-world data  we show improvement in both generative and discriminative techniques.

Throughout this paper we assume that the difﬁculty of uncovering the true class label depends on the
class label y alone  but is independent of the example x. Even though this assumption is simplistic 
we will see that our approach yields signiﬁcant improvements. A related set of techniques estimates
and corrects for the bias in sample selection  also known as covariate shift [9  16  18  1  13]. When
the bias can be decomposed into an estimable and inestimable part  the right approach might be to
use a combination of techniques presented in this paper and those for sample-selection bias.

2 Robust Bayesian Estimation with Unknown Class Probabilities

Our goal is to estimate an unknown conditional distribution π(y | x)  where x ∈ X is an example
and y ∈ Y is a label. The input consists of labeled examples (x1  y1)  . . .   (xm  ym) and unlabeled
examples xm+1  . . .   xM . Each example x is described by a set of features fj : X → R  indexed
by j ∈ J. For simplicity  we assume that sets X  Y  and J are ﬁnite  but we would like to allow the
space X and the set of features J to be very large.

In species distribution modeling from occurrence data  the space X corresponds to locations on the
map  features are various functions derived from the environmental variables  and the set Y contains
two classes: presence (y = 1) and absence (y = 0) for a particular species. Labeled examples are
presences of the species  e.g.  recorded presence locations of the golden bowerbird  while unlabeled
examples are locations that have been surveyed by biologists  but neither presence nor absence was
recorded. The unlabeled examples can be obtained as presence locations of species observed by a
similar protocol  for example other birds [13].

We posit a joint density π(x  y) and assume that examples are generated by the following process.
First  a pair (x  y) is chosen according to π. We always get to see the example x  but the label y is
revealed with an unknown probability that depends on y and is independent of x. This means that
we have access to independent samples from π(x) and from π(x| y)  but no information about π(y).
In our example  species presence is revealed with an unknown ﬁxed probability whereas absence is
revealed with probability zero (i.e.  never revealed).

2.1 Robust Bayesian Estimation  Maximum Entropy  and Logistic Regression

Robust Bayesian decision theory formulates an estimation problem as a zero-sum game between a
decision maker and nature [6]. In our case  the decision maker chooses an estimate p(x  y) while
nature selects a joint density π(x  y). Using the available data  the decision maker forms a set P in
which he believes nature’s choice lies  and tries to minimize worst-case loss under nature’s choice.
In this paper we are interested in minimizing the worst-case log loss relative to a ﬁxed default

estimate ν (equivalently  maximizing the worst-case log likelihood ratio)

min
p∈∆

max
π∈P

Eπ(cid:20)ln(cid:18) p(X  Y )

ν(X  Y )(cid:19)(cid:21) .

Here  ∆ is the simplex of joint densities and Eπ is a shorthand for EX Y ∼π. The default density ν
represents any prior information we have about π; if we have no prior information  ν is typically the
uniform density.

Gr¨unwald and Dawid [6] show that the robust Bayesian problem (Eq. 1) is often equivalent to the
minimum relative entropy problem

min
p∈P

RE(pk ν)  

where RE(pk q) = Ep[ln(p(X  Y )/q(X  Y )] is relative entropy or Kullback-Leibler divergence
and measures discrepancy between distributions p and q. The formulation intuitively says that we
should choose the density p which is closest to ν while respecting constraints P. When ν is uniform 
minimizing relative entropy is equivalent to maximizing entropy H(p) = Ep[− ln p(X  Y )]. Hence 
the approach is mainly referred to as maximum entropy [10] or maxent for short. The next theorem
outlines the equivalence of robust Bayes and maxent for the case considered in this paper. It is a
special case of Theorem 6.4 of [6].
Theorem 1 (Equivalence of maxent and robust Bayes). Let X × Y be a ﬁnite sample space  ν
a density on X × Y and P ⊆ ∆ a closed convex set containing at least one density absolutely
continuous w.r.t. ν . Then Eqs. (1) and (2) have the same optimizers.

For the case without labeling bias  the set P is usually described in terms of equality constraints
on moments of the joint distribution (feature expectations). Speciﬁcally  feature expectations with
respect to p are required to equal their empirical averages. When features are functions of x  but the
goal is to discriminate among classes y  it is natural to consider a derived set of features which are
versions of fj(x) active solely in individual classes y (see for instance [8]). If we were to estimate the
distribution of the golden bowerbird from presence-absence data then moment equality constraints
require that the joint model p(x  y) match the average altitude of presence locations as well as the
average altitude of absence locations (both weighted by their respective training proportions).

When the number of samples is too small or the number of features too large then equality con-
straints lead to overﬁtting because the true distribution does not match empirical averages exactly.
Overﬁtting is alleviated by relaxing the constraints so that feature expectations are only required to
lie within a certain distance of sample averages [3].

The solution of Eq. (2) with equality or relaxed constraints can be shown to lie in an exponential
family parameterized by λ = hλyiy∈Y  λy ∈ RJ  and containing densities

The optimizer of Eq. (2) is the unique density which minimizes the empirical log loss

qλ(x  y) ∝ ν(x  y)eλy·f (x) .

ln qλ(xi  yi)

(3)

1

m Xi≤m

possibly with an additional ℓ1-regularization term accounting for slacks in equality constraints. (See
[3] for a proof.)

In addition to constraints on moments of the joint distribution  it is possible to introduce constraints
on marginals of p. The most common implementations of maxent impose marginal constraints
p(x) = ˜πlab(x) where ˜πlab is the empirical distribution over labeled examples. The solution then
takes form qλ(x  y) = ˜πlab(x)qλ(y | x) where qλ(y | x) is the multinomial logistic model

(1)

(2)

As before  the maxent solution is the unique density of this form which minimizes the empirical log
loss (Eq. 3). The minimization of Eq. (3) is equivalent to the minimization of conditional log loss

qλ(y | x) ∝ ν(y | x)eλy·f (x) .

1

m Xi≤m

− ln qλ(yi | xi) .

Hence  this approach corresponds to logistic regression. Since it only models the labeling process
π(y | x)  but not the sample generation π(x)  it is known as discriminative training.
The case with equality constraints p(y) = ˜πlab(y) has been analyzed for example by [8]. The
solution has the form qλ(x  y) = ˜πlab(y)qλ(x| y) with

qλ(x| y) ∝ ν(x| y)eλy·f (x) .

Log loss can be minimized for each class separately  i.e.  each λy is the maximum likelihood esti-
mate (possibly with regularization) of π(x| y). The joint estimate qλ(x  y) can be used to derive the
conditional distribution qλ(y | x). Since this approach estimates the sample generating distributions
of individual classes  it is known as generative training. Naive Bayes is a special case of generative
training when ν(x| y) =Qj νj(fj(x)| y).

The two approaches presented in this paper can be viewed as generalizations of generative and
discriminative training with two additional components: availability of unlabeled examples and lack
of information about class probabilities. The former will inﬂuence the choice of the default ν  the
latter the form of constraints P.

2.2 Generative Training: Entropy-weighted Maxent

When the number of labeled and unlabeled examples is sufﬁciently large  it is reasonable to assume
that the empirical distribution ˜π(x) over all examples (labeled and unlabeled) is a faithful repre-
sentation of π(x). Thus  we consider defaults with ν(x) = ˜π(x)  shown to work well in species
distribution modeling [13]. For simplicity  we assume that ν(y | x) does not depend on x and focus
on ν(x  y) = ˜π(x)ν(y). Other options are possible. For example  when the number of examples is
small  ˜π(x) might be replaced by an estimate of π(x). The distribution ν(y) can be chosen uniform
across y  but if some classes are known to be rarer than others then a non-uniform estimate will
perform better. In Section 3  we analyze the impact of this choice.

Ep[fj(X)| y] − ˜µy

j

∀j : (cid:12)(cid:12)

X} where py

j = β ˜σy

j /√my where β is a single tuning constant  ˜σy

j is the empirical average of fj among labeled examples in class y and βy

Constraints on moments of the joint distribution  such as those in the previous section  will misspec-
ify true moments in the presence of labeling bias. However  as discussed earlier  labeled examples
from each class y approximate conditional distributions π(x| y). Thus  instead of constraining joint
expectations  we constrain conditional expectations Ep[fj(X)| y]. In general  we consider robust
Bayes and maxent problems with the set P of the form P = {p ∈ ∆ : py
X ∈ Py
X denotes
the |X|-dimensional vector of conditional probabilities p(x| y) and Py
X expresses the constraints on
py
X. For example  relaxed constraints for class y are expressed as
j(cid:12)(cid:12) ≤ βy

(4)
where ˜µy
j are estimates of
deviations of averages from true expectations. Similar to [14]  we use standard-error-like deviation
estimates βy
j is the empirical standard devia-
tion of fj among labeled examples in class y  and my is the number of labeled examples in class y.
When my equals 0  we choose βy
The next theorem and the following corollary show that robust Bayes (and also maxent) with the
constraint set P of the form above yield estimators similar to generative training. In addition to the
notation py
X for conditional densities  we use the notation pY and pX to denote vectors of marginal
probabilities p(y) and p(x)  respectively. For example  the empirical distribution over examples is
denoted ˜πX.
Theorem 2. Let Py
X ∈ Py
X}.
If P contains at least one density absolutely continuous w.r.t. ν then robust Bayes and maxent over
P are equivalent. The solution ˆp has the form ˆp(y)ˆp(x| y) where class-conditional densities ˆpy
minimize RE(py
(5)

X  y ∈ Y be closed convex sets of densities over X and P = {p ∈ ∆ : py

j = ∞ and thus leave feature expectations unconstrained.

X k ˜πX) among py

X ∈ Py

X and

X

ˆp(y) ∝ ν(y)e−RE( ˆpy

X k ˜πX) .

Proof. It is not too difﬁcult to verify that the set P is a closed convex set of joint densities  so
the equivalence of robust Bayes and maxent follows from Theorem 1. To prove the remainder  we
rewrite the maxent objective as

RE(pk ν) = RE(pY k νY) +Xy

p(y)RE(py

X k ˜πX) .

Maxent problem is then equivalent to

min

pY hRE(pY k νY) +Xy

p(y) min
X∈Py
py

X

RE(py

X k ˜πX)i

= min

= min

pY " Xy
pY "Xy

p(y) ln  p(y)
p(y) ln 

ν(y)!! + Xy
X k ˜πX)!#

p(y)
ν(y)e−RE( ˆpy

= const. + min
pY

RE(pY k ˆpY) .

p(y)RE(ˆpy

X k ˜πX)!#

Since RE(pk q) is minimized for p = q  we indeed obtain that for the minimizing p  pY = ˆpY.
Theorem 2 generalizes to the case when in addition to constraining py
X  we also constrain
pY to lie in a closed convex set PY. The solution then takes form p(y)ˆp(x| y) with ˆp(x| y) as
in the theorem  but with p(y) minimizing RE(pY k ˆpY) subject to pY ∈ PY. Unlike generative
training without labeling bias  the class-conditional densities in the theorem above inﬂuence class
probabilities. When sets Py
X are speciﬁed using constraints of Eq. (4) then ˆp has a form derived from
regularized maximum likelihood estimates in an exponential family (see  e.g.  [3]):
Corollary 3. If sets Py
maxent are equivalent. The class-conditional densities ˆp(x| y) of the solution take form

X are speciﬁed by inequality constraints of Eq. (4) then robust Bayes and

X to lie in Py

and solve single-class regularized maximum likelihood problems

ˆλ
qλ(x| y) ∝ ˜π(x)e

y

·f (x)

(6)

(7)

min

λy n Xi:yi=y(cid:2)− ln qλ(xi | y)(cid:3) + myXj∈J

βj|λy

j|o .

One-class Estimation.
In one-class estimation problems  there are two classes (0 and 1)  but we
only have access to labeled examples from one class (e.g.  class 1). In species distribution modeling 
we only have access to presence records of the species. Based on labeled examples  we derive a set
of constraints on p(x| y = 1)  but leave p(x| y = 0) unconstrained. By Theorem 2  ˆp(x| y = 1)
then solves the single-class maximum entropy problem  we write ˆp(x| y = 1) = ˆpME(x)  and
ˆp(x| y = 0) = ˜π(x). Assume without loss of generality that examples x1  . . .   xM are distinct (but
allow them to have identical feature vectors). Thus  ˜π(x) = 1/M on examples and zero elsewhere 
and RE(ˆpME k ˜πX) = −H(ˆpME) + ln M. Plugging these into Theorem 2  we can derive the condi-
tional estimate ˆp(y = 1| x) across all unlabeled examples x:

ˆp(y = 1| x) =

ν(y = 0) + ν(y = 1)ˆpME(x)eH( ˆpME)

ν(y = 1)ˆpME(x)eH( ˆpME)

.

(8)

If constraints on p(x| y = 1) are chosen as in Corollary 3 then ˆpME is exponential and Eq. (8) thus
describes a logistic model. This model has the same coefﬁcients as ˆpME  with the intercept chosen
so that “typical” examples x under ˆpME (examples with log probability close to the expected log
probability) yield predictions close to the default.

2.3 Discriminative Training: Class-robust Logistic Regression

Similar to the previous section  we consider ν(x  y) = ˜π(x)ν(y). The set of constraints P will
now also include equality constraints on p(x). Since ˜πlab(x) misspeciﬁes the marginal  we use
p(x) = ˜π(x). Next theorem is an analog of Corollary 3 for discriminative training. It follows from
a combination of Theorem 1 and duality of maxent with maximum likelihood [3]. A complete proof
will appear in the extended version of this paper.
Theorem 4. Assume that sets Py
∆ : py
equivalent. For the solution ˆp  ˆp(x) = ˜π(x) and ˆp(y | x) takes form
λy·f (x)−λy· ˜µy+Pj

X are speciﬁed by inequality constraints of Eq. (4). Let P = {p ∈
X and pX = ˜πX}. If the set P is non-empty then robust Bayes and maxent over P are

qλ(y | x) ∝ ν(y)e

X ∈ Py

j |λy
βy
j |

(9)

and solves the regularized “logistic regression” problem

ji) .

min

λ ( 1

j − ˜µy

j )λy

(10)

j its class-conditional feature expectations.

¯π(y)Xj∈Jhβy
j(cid:12)(cid:12)λy
j(cid:12)(cid:12) + (¯µy

M Xi≤MXy∈Yh−¯π(y | xi) ln qλ(y | xi)i +Xy∈Y
where ¯π is an arbitrary feasible point  ¯π ∈ P  and ¯µy
We put logistic regression in quotes  because the model described by Eq. (9) is not the usual logistic
model; however  once the parameters λy are ﬁxed  Eq. (9) simply determines a logistic model with a
special form of the intercept. Note that the second term of Eq. (10) is indeed a regularization  albeit
possibly an asymmetric one  since any feasible ¯π will have |¯µy
j . Since ¯π(x) = ˜π(x) 
¯π is speciﬁed solely by ¯π(y | x) and thus can be viewed as a tentative imputation of labels across
all examples. We remark that the value of the objective of Eq. (10) does not depend on the choice
of ¯π  because a different choice of ¯π (inﬂuencing the ﬁrst term) yields a different set of means ¯µy
j
(inﬂuencing the second term) and these differences cancel out. To provide a more concrete example
and some intuition about Eq. (10)  we now consider one-class estimation.

j − ˜µy

j| ≤ βy

One-class estimation. A natural choice of ¯π is the “pseudo-empirical” distribution which views
all unlabeled examples as negatives. Pseudo-empirical means of class 1 match empirical averages of
class 1 exactly  whereas pseudo-empirical means of class 0 can be arbitrary because they are uncon-
strained. The lack of constraints on class 0 forces the corresponding λy to equal zero. The objective
can thus be formulated solely using λy for the class 1; therefore  we will omit the superscript y.
Eq. (10) after multiplying by M then becomes

min

λ (Xi≤m(cid:2)− ln qλ(y = 1| xi)(cid:3) + Xm<i≤M(cid:2)− ln qλ(y = 0| xi)(cid:3) + mXj∈J

βj|λj|) .

Thus the objective of class-robust logistic regression is the same as of regularized logistic regression
discriminating positives from unlabeled examples.

3 Experiments

We evaluate our techniques using a large real-world dataset containing 226 species from 6 regions
of the world  produced by the “Testing alternative methodologies for modeling species’ ecological
niches and predicting geographic distributions” Working Group at the National Center for Ecological
Analysis and Synthesis (NCEAS). The training set contains presence-only data from unplanned
surveys or incidental records  including those from museums and herbariums. The test set contains
presence-absence data from rigorously planned independent surveys (i.e.  without labeling bias).
The regions are described by 11–13 environmental variables  with 20–54 species per region  2–5822
training presences per species (median of 57)  and 102–19120 test points (presences and absences);
for details see [4]. As unlabeled examples we use presences of species captured by similar methods 
known as “target group”  with the groups as in [13].

We evaluate both entropy-weighted maxent and class-robust logistic regression while varying the
default estimate ν(y = 1)  referred to as default species prevalence by analogy with p(y = 1)  which
is called species prevalence. Entropy-weighted maxent solutions for different default prevalences are
derived by Eq. (8) from the same one-class estimate ˆpME. Class-robust logistic regression requires
separate optimization for each default prevalence.
We calculate ˆpME using the Maxent package [15] with features spanning the space of piecewise linear
splines (of each environmental variable separately) and a tuned value of β (see [12] for the details
on features and tuning). Class-robust logistic models are calculated by a boosting-like algorithm
SUMMET [3] with the same set of features and the same value β as the maxent runs.
For comparison  we also evaluate default-weighted maxent  using class probabilities p(y) = ν(y)
instead of Eq. (5)  and two “oracle” methods based on class probabilities in the test data: constant
Bernoulli prediction p(y | x) = π(y)  and oracle-weighted maxent  using p(y) = π(y) instead of
Eq. (5). Note that the constant Bernoulli prediction has no discrimination power (its AUC is 0.5)
even though it matches class probabilities perfectly.

0.2

0.15

0.1

0.05

s
s
o

l
 

g
o

l
 
t
s
e

t
 

e
g
a
r
e
v
a

species with

test prev. 0.00−0.04

species with

test prev. 0.04−0.15

species with

test prev. 0.15−0.70

all species

0.35

0.3

0.25

0.65

0.6

0.55

0.4

0.35

0.3

0

0.2

0.4

0

0.2

0.4

0.2
default prevalence

0.4

0.6

0

0.2

0.4

0.6

maxent weighted by default prevalence
maxent weighted by default*exp{−RE}

Bernoulli according to test prevalence (oracle setting)
maxent weighted by test prevalence (oracle setting)

l

s
e
u
a
v
 
.
v
e
r
p

s
s
o

l
 
t
s
e

t
 

i

 

n
e
v
g
g
n
v
e
h
c
a

i

i

 
t
l

u
a

f

e
d

 
f
o
 
e
g
n
a
r

species with
species with
species with

test prev. 0.00−0.04
test prev. 0.00−0.04
test prev. 0.00−0.04

species with
species with
species with

test prev. 0.04−0.15
test prev. 0.04−0.15
test prev. 0.04−0.15

species with
species with
species with

test prev. 0.15−0.70
test prev. 0.15−0.70
test prev. 0.15−0.70

all species
all species
all species

0.4

0.2

0
0.05

0.1

0.15

0.2

0.4

0.2

0

0.25

0.3

0.35

0.4

0.2

0

0.55

0.6

0.65

0.4

0.2

0

test log loss

0.3

0.35

0.4

maxent weighted by default prevalence
maxent weighted by default*exp{−RE}
BRT reweighted by default prevalence
BRT reweighted by default*exp{−RE}

 

class−robust logistic regression

Bernoulli according to test prevalence (oracle setting)
maxent weighted by test prevalence (oracle setting)

Figure 1: Comparison of reweighting schemes. Top: Test log loss averaged over species with given
values of test prevalence  for varying default prevalence. Bottom: For each value of test log loss  we
determine the range of default prevalence values that achieve it.

To test entropy-weighting as a general method for estimating class probabilities  we also evalu-
ate boosted regression trees (BRT)  which have the highest predictive accuracy along with maxent
among species distribution modeling techniques [4]. In this application  BRT is used to construct a
logistic model discriminating positive examples from unlabeled examples. Recent work [17] uses a
more principled approach where unknown labels are ﬁtted by an EM algorithm  but our preliminary
runs had too low AUC values  so they are excluded from our comparison. We train BRT using the
R package gbm on datasets weighted so that the total weight of positives is equal to the total weight
of unlabeled examples  and then apply Elkan’s reweighting scheme [5]. Speciﬁcally  the BRT result
ˆpBRT(y | x) is transformed to
p(y = 1| x) =

p(y = 1)ˆpBRT(y = 1| x)

p(y = 1)ˆpBRT(y = 1| x) + p(y = 0)ˆpBRT(y = 0| x)

for two choices of p(y): default  p(y) = ν(y)  and entropy-based (using ˆpME).
All three techniques yield state-of-the-art discrimination (see [13]) measured by the average AUC:
maxent achieves AUC of 0.7583; class-robust logistic regression 0.7451–0.7568; BRT 0.7545. Un-
like maxent and BRT estimates  class-robust logistic estimates are not monotonically related  so they
yield different AUC for different default prevalence. However  log loss performance varies broadly
according to the reweighting scheme.
In the top portion of Fig. 1  we focus on maxent. Naive
weighting by default prevalence yields sharp peaks in performance around the best default preva-
lence. Entropy-based weighting yields broader peaks  so it is less sensitive to the default prevalence.
The improvement diminishes as the true prevalence increases  but entropy-based weighting is never
more sensitive. Thanks to smaller sensitivity  entropy-based weighting outperforms naive weight-
ing when a single default needs to be chosen for all species (the rightmost plot). Note that the
optimal default values are higher for entropy-based weighting  because in one-class estimation the
entropy-based prevalence is always smaller than default (unless the estimate ˆpME is uniform).
Improved sensitivity is demonstrated more clearly in the bottom portion of Fig. 1  now also including
BRT and class-robust logistic regression. We see that BRT and maxent results are fairly similar  with
BRT performing overall slightly better than maxent. Note that entropy-reweighted BRT relies both
on BRT and maxent for its performance. A striking observation is the poor performance of class-
robust logistic regression for species with larger prevalence values; it merits further investigation.

4 Conclusion and Discussion

To correct for unknown labeling bias in training data  we used robust Bayesian decision theory and
developed generative and discriminative approaches that optimize log loss under worst-case true
class proportions. We found that our approaches improve test performance on a benchmark dataset
for species distribution modeling  a one-class application with extreme labeling bias.

Acknowledgments. We would like to thank all of those who provided data used here: A. Ford 
CSIRO Atherton  Australia; M. Peck and G. Peck  Royal Ontario Museum; M. Cadman  Bird Stud-
ies Canada  Canadian Wildlife Service of Environment Canada; the National Vegetation Survey
Databank and the Allan Herbarium  New Zealand; Missouri Botanical Garden  especially R. Magill
and T. Consiglio; and T. Wohlgemuth and U. Braendi  WSL Switzerland.

References
[1] Bickel  S.  M. Br¨uckner  and T. Scheffer (2007). Discriminative learning for differing training and test

distributions. In Proc. 24th Int. Conf. Machine Learning  pp. 161–168.

[2] Chawla  N. V.  N. Japkowicz  and A. Kołcz (2004). Editorial: special issue on learning from imbalanced

data sets. SIGKDD Explorations 6(1)  1–6.

[3] Dud´ık  M.  S. J. Phillips  and R. E. Schapire (2007). Maximum entropy density estimation with generalized
regularization and an application to species distribution modeling. J. Machine Learning Res. 8  1217–1260.

[4] Elith  J.  C. H. Graham  et al. (2006). Novel methods improve prediction of species’ distributions from

occurrence data. Ecography 29(2)  129–151.

[5] Elkan  C. (2001). The foundations of cost-sensitive learning. In Proc. 17th Int. Joint Conf. on Artiﬁcial

Intelligence  pp. 973–978.

[6] Gr¨unwald  P. D. and A. P. Dawid (2004). Game theory  maximum entropy  minimum discrepancy  and

robust Bayesian decision theory. Ann. Stat. 32(4)  1367–1433.

[7] Guo  Q.  M. Kelly  and C. H. Graham (2005). Support vector machines for predicting distribution of Sudden

Oak Death in California. Ecol. Model. 182  75–90.

[8] Haffner  P.  S. Phillips  and R. Schapire (2005). Efﬁcient multiclass implementations of L1-regularized

maximum entropy. E-print arXiv:cs/0506101.

[9] Heckman  J. J. (1979). Sample selection bias as a speciﬁcation error. Econometrica 47(1)  153–161.

[10] Jaynes  E. T. (1957). Information theory and statistical mechanics. Phys. Rev. 106(4)  620–630.

[11] Maloof  M. (2003). Learning when data sets are imbalanced and costs are unequal and unknown. In Proc.

ICML’03 Workshop on Learning from Imbalanced Data Sets.

[12] Phillips  S. J. and M. Dud´ık (2008). Modeling of species distributions with Maxent: new extensions and a

comprehensive evaluation. Ecography 31(2)  161–175.

[13] Phillips  S. J.  M. Dud´ık  J. Elith  C. H. Graham  A. Lehmann  J. Leathwick  and S. Ferrier. Sample
selection bias and presence-only models of species distributions: Implications for selection of background
and pseudo-absences. Ecol. Appl. To appear.

[14] Phillips  S. J.  M. Dud´ık  and R. E. Schapire (2004). A maximum entropy approach to species distribution

modeling. In Proc. 21st Int. Conf. Machine Learning  pp. 655–662. ACM Press.

[15] Phillips  S. J.  M. Dud´ık  and R. E. Schapire (2007). Maxent software for species habitat modeling. http://

www.cs.princeton.edu/∼schapire/maxent.

[16] Shimodaira  H. (2000). Improving predictive inference under covariate shift by weighting the log-likelihood

function. J. Stat. Plan. Infer. 90(2)  227–244.

[17] Ward  G.  T. Hastie  S. Barry  J. Elith  and J. Leathwick (2008). Presence-only data and the EM algorithm.

Biometrics. In press.

[18] Zadrozny  B. (2004). Learning and evaluating classiﬁers under sample selection bias. In Proc. 21st Int.

Conf. Machine Learning  pp. 903–910. ACM Press.

[19] Zaniewski  A. E.  A. Lehmann  and J. M. Overton (2002). Predicting species spatial distributions using

presence-only data: A case study of native New Zealand ferns. Ecol. Model. 157  261–280.

,Nicolò Cesa-Bianchi
Ofer Dekel
Ohad Shamir
Ming Liang
Xiaolin Hu
Bo Zhang
Luca Ambrogioni
Max Hinne
Marcel Van Gerven
Eric Maris