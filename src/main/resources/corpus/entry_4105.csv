2015,Optimal Linear Estimation under Unknown Nonlinear Transform,Linear regression studies the problem of estimating a model parameter $\beta^* \in \R^p$  from $n$ observations $\{(y_i x_i)\}_{i=1}^n$ from linear model $y_i = \langle \x_i \beta^* \rangle + \epsilon_i$. We consider a significant generalization in which the relationship between $\langle x_i \beta^* \rangle$ and $y_i$ is noisy  quantized to a single bit  potentially nonlinear  noninvertible  as well as unknown. This model is known as the single-index model in statistics  and  among other things  it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover $\beta^*$ in settings (i.e.  classes of link function $f$) where previous algorithms fail. In general  our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\langle x_i \beta^* \rangle$. We also consider the high dimensional setting where $\beta^*$ is sparse  and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \gg n$. For a broad class of link functions between $\langle x_i \beta^* \rangle$ and $y_i$  we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.,Optimal Linear Estimation under Unknown

Nonlinear Transform

Xinyang Yi

The University of Texas at Austin

yixy@utexas.edu

Zhaoran Wang

Princeton University

zhaoran@princeton.edu

Constantine Caramanis

The University of Texas at Austin
constantine@utexas.edu

Han Liu

Princeton University

hanliu@princeton.edu

Abstract

Linear regression studies the problem of estimating a model parameter β∗ ∈ Rp 
from n observations {(yi  xi)}n
i=1 from linear model yi = (cid:104)xi  β∗(cid:105) + i. We
consider a signiﬁcant generalization in which the relationship between (cid:104)xi  β∗(cid:105)
and yi is noisy  quantized to a single bit  potentially nonlinear  noninvertible  as
well as unknown. This model is known as the single-index model in statistics  and 
among other things  it represents a signiﬁcant generalization of one-bit compressed
sensing. We propose a novel spectral-based estimation procedure and show that
we can recover β∗ in settings (i.e.  classes of link function f) where previous
algorithms fail. In general  our algorithm requires only very mild restrictions on the
(unknown) functional relationship between yi and (cid:104)xi  β∗(cid:105). We also consider the
high dimensional setting where β∗ is sparse  and introduce a two-stage nonconvex
framework that addresses estimation challenges in high dimensional regimes where
p (cid:29) n. For a broad class of link functions between (cid:104)xi  β∗(cid:105) and yi  we establish
minimax lower bounds that demonstrate the optimality of our estimators in both
the classical and high dimensional regimes.

1

Introduction

P(Y = 1|X = x) =

We consider a generalization of the one-bit quantized regression problem  where we seek to recover
the regression coefﬁcient β∗ ∈ Rp from one-bit measurements. Speciﬁcally  suppose that X is a
random vector in Rp and Y is a binary random variable taking values in {−1  1}. We assume the
conditional distribution of Y given X takes the form
1
2

(1.1)
where f : R → [−1  1] is called the link function. We aim to estimate β∗ from n i.i.d. observations
{(yi  xi)}n
i=1 of the pair (Y  X). In particular  we assume the link function f is unknown. Without
any loss of generality  we take β∗ to be on the unit sphere Sp−1 since its magnitude can always be
incorporated into the link function f.
The model in (1.1) is simple but general. Under speciﬁc choices of the link function f  (1.1) immedi-
ately leads to many practical models in machine learning and signal processing  including logistic
regression and one-bit compressed sensing. In the settings where the link function is assumed to
be known  a popular estimation procedure is to calculate an estimator that minimizes a certain loss

f ((cid:104)x  β∗(cid:105)) +

1
2

 

1

function. However  for particular link functions  this approach involves minimizing a nonconvex
objective function for which the global minimizer is in general intractable to obtain. Furthermore  it
is difﬁcult or even impossible to know the link function in practice  and a poor choice of link function
may result in inaccurate parameter estimation and high prediction error. We take a more general
approach  and in particular  target the setting where f is unknown. We propose an algorithm that can
estimate the parameter β∗ in the absence of prior knowledge on the link function f. As our results
make precise  our algorithm succeeds as long as the function f satisﬁes a single moment condition.
As we demonstrate  this moment condition is only a mild restriction on f. In particular  our methods
and theory are widely applicable even to the settings where f is non-smooth  e.g.  f (z) = sign(z)  or
noninvertible  e.g.  f (z) = sin(z).
In particular  as we show in §2  our restrictions on f are sufﬁciently ﬂexible so that our results provide
a uniﬁed framework that encompasses a broad range of problems  including logistic regression 
one-bit compressed sensing  one-bit phase retrieval as well as their robust extensions. We use these
important examples to illustrate our results  and discuss them at several points throughout the paper.
Main contributions. The key conceptual contribution of this work is a novel use of the method of
moments. Rather than considering moments of the covariate  X  and the response variable  Y   we
look at moments of differences of covariates  and differences of response variables. Such a simple yet
critical observation enables everything that follows and leads to our spectral-based procedure.
We also make two theoretical contributions. First  we simultaneously establish the statistical and
computational rates of convergence of the proposed spectral algorithm. We consider both the low
dimensional setting where the number of samples exceeds the dimension and the high dimensional
setting where the dimensionality may (greatly) exceed the number of samples. In both these settings 
our proposed algorithm achieves the same statistical rate of convergence as that of linear regression
applied on data generated by the linear model without quantization. Second  we provide minimax
lower bounds for the statistical rate of convergence  and thereby establish the optimality of our
procedure within a broad model class. In the low dimensional setting  our results obtain the optimal
rate with the optimal sample complexity. In the high dimensional setting  our algorithm requires
estimating a sparse eigenvector  and thus our sample complexity coincides with what is believed to
be the best achievable via polynomial time methods [2]; the error rate itself  however  is information-
theoretically optimal. We discuss this further in §3.4.
Related works. Our model in (1.1) is close to the single-index model (SIM) in statistics. In the SIM 
we assume that the response-covariate pair (Y  X) is determined by

Y = f ((cid:104)X  β∗(cid:105)) + W

(1.2)
with unknown link function f and noise W . Our setting is a special case of this  as we restrict Y
to be a binary random variable. The single index model is a classical topic  and therefore there is
extensive literature – too much to exhaustively review it. We therefore outline the pieces of work most
relevant to our setting and our results. For estimating β∗ in (1.2)  a feasible approach is M-estimation
[8  9  12]  in which the unknown link function f is jointly estimated using nonparametric estimators.
Although these M-estimators have been shown to be consistent  they are not computationally efﬁcient
since they involve solving a nonconvex optimization problem. Another approach to estimate β∗ is
named the average derivative estimator (ADE; [24]). Further improvements of ADE are considered
in [13  22]. ADE and its related methods require that the link function f is at least differentiable  and
thus excludes important models such as one-bit compressed sensing with f (z) = sign(z). Beyond
estimating β∗  the works in [15  16] focus on iteratively estimating a function f and vector β that
are good for prediction  and they attempt to control the generalization error. Their algorithms are
based on isotonic regression  and are therefore only applicable when the link function is monotonic
and satisﬁes Lipschitz constraints. The work discussed above focuses on the low dimensional setting
where p (cid:28) n. Another related line of works is sufﬁcient dimension reduction  where the goal is to
ﬁnd a subspace U of the input space such that the response Y only depends on the projection U(cid:62)X.
Single-index model and our problem can be regarded as special cases of this problem as we are
primarily interested in recovering a one-dimensional subspace. Due to space limit  we refer readers to
the long version of this paper for a detailed survey [29].

2

In the high dimensional regime with p (cid:29) n and β∗ has some structure (for us this means sparsity) 
we note there exists some recent progress [1] on estimating f via PAC Bayesian methods. In the
special case when f is linear function  sparse linear regression has attracted extensive study over
the years. The recent work by Plan et al. [21] is closest to our setting. They consider the setting of
normal covariates  X ∼ N (0  Ip)  and they propose a marginal regression estimator for estimating
β∗  that  like our approach  requires no prior knowledge about f. Their proposed algorithm relies
on the assumption that Ez∼N (0 1)
even. As we will describe below  our algorithm is based on a novel moment-based estimator  and
avoids requiring such a condition  thus allowing us to handle even link functions under a very mild
moment restriction  which we describe in detail below. Generally  the work in [21] requires different
conditions  and thus beyond the discussion above  is not directly comparable to the work here. In
cases where both approaches apply  the results are minimax optimal.

(cid:2)zf (z)(cid:3) (cid:54)= 0  and hence cannot work for link functions that are

2 Example models

1

1−pe

pe

In this section  we discuss several popular (and important) models in machine learning and signal
processing that fall into our general model (1.1) under speciﬁc link functions. Variants of these models
have been studied extensively in the recent literature. These examples trace through the paper  and we
use them to illustrate the details of our algorithms and results.
Logistic regression. In logistic regression (LR)  we assume that P(Y = 1|X = x) =
1+exp (−(cid:104)x β∗(cid:105)−ζ)  where ζ is the intercept. The link function corresponds to f (z) = exp (z+ζ)−1
exp (z+ζ)+1 .
One robust variant of LR is called ﬂipped logistic regression  where we assume that the labels
Y generated from standard LR model are ﬂipped with probability pe  i.e.  P(Y = 1|X = x) =
1+exp ((cid:104)x β∗(cid:105)+ζ). This reduces to the standard LR model when pe = 0. For
1+exp (−(cid:104)x β∗(cid:105)−ζ) +
ﬂipped LR  the link function f can be written as
exp (z + ζ) − 1
exp (z + ζ) + 1

(2.1)
Flipped LR has been studied by [19  25]. In both papers  estimating β∗ is based on minimizing some
surrogate loss function involving a certain tuning parameter connected to pe. However  pe is unknown
in practice. In contrast to their approaches  our method does not hinge on the unknown parameter pe.
Our approach has the same formulation for both standard and ﬂipped LR  thus uniﬁes the two models.
One-bit compressed sensing. One-bit compressed sensing (CS) aims at recovering sparse signals
from quantized linear measurements (see e.g.  [11  20]). In detail  we deﬁne B0(s  p) := {β ∈ Rp :
| supp(β)| ≤ s} as the set of sparse vectors in Rp with at most s nonzero elements. We assume
(Y  X) ∈ {−1  1} × Rp satisﬁes

+ 2pe · 1 − exp (z + ζ)

1 + exp (z + ζ)

f (z) =

.

Y = sign((cid:104)X  β∗(cid:105)) 

1√
2πσ

(cid:90) ∞

(2.2)
where β∗ ∈ B0(s  p). In this paper  we also consider its robust version with noise   i.e.  Y =
sign((cid:104)X  β∗(cid:105) + ). Assuming  ∼ N (0  σ2)  the link function f of robust 1-bit CS thus corresponds
to

e−(u−z)2/2σ2

du − 1.

0

f (z) = 2

(2.3)
Note that (2.2) also corresponds to the probit regression model without the sparse constraint on β∗.
Throughout the paper  we do not distinguish between the two model names. Model (2.2) is referred
to as one-bit compressed sensing even in the case where β∗ is not sparse.
One-bit phase retrieval. The goal of phase retrieval (e.g.  [5]) is to recover signals based on linear
measurements with phase information erased  i.e.  pair (Y  X) ∈ R × Rp is determined by equation
Y = |(cid:104)X  β∗(cid:105)|. Analogous to one-bit compressed sensing  we consider a new model named one-bit
phase retrieval where the linear measurement with phase information erased is quantized to one bit.
In detail  pair (Y  X) ∈ {−1  1} × Rp is linked through Y = sign(|(cid:104)X  β∗(cid:105)| − θ)  where θ is the
quantization threshold. Compared with one-bit compressed sensing  this problem is more difﬁcult
because Y only depends on β∗ through the magnitude of (cid:104)X  β∗(cid:105) instead of the value of (cid:104)X  β∗(cid:105).
Also  it is more difﬁcult than the original phase retrieval problem due to the additional quantization.

3

Using our general model  The link function thus corresponds to

f (z) = sign(|z| − θ).

It is worth noting that  unlike previous models  here f is neither odd nor monotonic.

(2.4)

3 Main results
We now turn to our algorithms for estimating β∗ in both low and high dimensional settings. We ﬁrst
introduce a second moment estimator based on pairwise differences. We prove that the eigenstructure
of the constructed second moment estimator encodes the information of β∗. We then propose
algorithms to estimate β∗ based upon this second moment estimator. In the high dimensional setting
where β∗ is sparse  computing the top eigenvector of our pairwise-difference matrix reduces to
computing a sparse eigenvector. Beyond algorithms  we discuss minimax lower bound in §3.5. We
present simulation results in §3.6

3.1 Conditions for success

We now introduce several key quantities  which allow us to state precisely the conditions required for
the success of our algorithm.
Deﬁnition 3.1. For any (unknown) link function  f  deﬁne the quantity φ(f ) as follows:

where µ0  µ1 and µ2 are given by

1 − µ0µ2 + µ2
0.

φ(f ) := µ2

µk := E(cid:2)f (Z)Z k(cid:3) 

k = 0  1  2 . . .  

where Z ∼ N (0  1).
As we discuss in detail below  the key condition for success of our algorithm is φ(f ) (cid:54)= 0. As we
show below  this is a relatively mild condition  and in particular  it is satisﬁed by the three examples
introduced in §2. For odd and monotonic f  φ(f ) > 0 unless f (z) = 0 for all z in which case no
algorithm is able to recover β∗. For even f  we have µ1 = 0. Thus φ(f ) (cid:54)= 0 if and only if µ0 (cid:54)= µ2.

3.2 Second moment estimator
We describe a novel moment estimator that enables our algorithm. Let {(yi  xi)}n
i=1 be the n i.i.d.
observations of (Y  X). Assuming without loss of generality that n is even  we consider the following
key transformation

∆yi := y2i − y2i−1  ∆xi := x2i − x2i−1 

for i = 1  2  ...  n/2. Our procedure is based on the following second moment

(3.1)

(3.2)

(3.3)

(3.4)

n/2(cid:88)

i=1

M :=

2
n

∆y2

i ∆xi∆x(cid:62)

i ∈ Rp×p.

The intuition behind this second moment is as follows. By (1.1)  the variation of X along the direction
β∗ has the largest impact on the variation of (cid:104)X  β∗(cid:105). Thus  the variation of Y directly depends
on the variation of X along β∗. Consequently  {(∆yi  ∆xi)}n/2
i=1 encodes the information of such a
dependency relationship. In the following  we make this intuition more rigorous by analyzing the
eigenstructure of E(M) and its relationship with β∗.
Lemma 3.2. For β∗ ∈ Sp−1  we assume that (Y  X) ∈ {−1  1} × Rp satisﬁes (1.1). For X ∼
N (0  Ip)  we have

E(M) = 4φ(f ) · β∗β∗(cid:62) + 4(1 − µ2

0) · Ip 

(3.5)

where µ0 and φ(f ) are deﬁned in (3.2) and (3.1).
Lemma 3.2 proves that β∗ is the leading eigenvector of E(M) as long as the eigengap φ(f ) is positive.
If instead we have φ(f ) < 0  we can use a related moment estimator which has analogous properties.

4

(cid:80)n/2
i=1(y2i + y2i−1)2∆xi∆x(cid:62)

To this end  deﬁne M(cid:48) := 2
similar result for M(cid:48) as stated below.
Corollary 3.3. Under the setting of Lemma 3.2 

n

E(M(cid:48)) = −4φ(f ) · β∗β∗(cid:62) + 4(1 + µ2

0) · Ip.

i . In parallel to Lemma 3.2  we have a

Corollary 3.3 therefore shows that when φ(f ) < 0  we can construct another second moment estimator
M(cid:48) such that β∗ is the leading eigenvector of E(M(cid:48)). As discussed above  this is precisely the setting
for one-bit phase retrieval when the quantization threshold in (3.1) satisﬁes θ < θm. For simplicity of
the discussion  hereafter we assume that φ(f ) > 0 and focus on the second moment estimator M
deﬁned in (3.4).
A natural question to ask is whether φ(f ) (cid:54)= 0 holds for speciﬁc models. The following lemma
demonstrates exactly this  for the example models introduced in §2.
Lemma 3.4. (a) Consider the ﬂipped logistic regression where f is given in (2.1). By setting the
intercept to be ζ = 0  we have φ(f ) (cid:38) (1− 2pe)2. (b) For robust one-bit compressed sensing where f
is given in (2.3). We have φ(f ) (cid:38) min
. (c) For one-bit phase retrieval where
f is given in (2.4). For Z ∼ N (0  1)  we let θm be the median of |Z|  i.e.  P(|Z| ≥ θm) = 1/2. We
have |φ(f )| (cid:38) θ|θ − θm| exp(−θ2) and sign[φ(f )] = sign(θ − θm). We thus obtain φ(f ) > 0 for
θ > θm.

(cid:26)(cid:16) 1−σ2

  C(cid:48)σ4

(cid:17)2

(cid:27)

(1+σ3)2

1+σ2

(cid:20)

(cid:124)

γ :=

0

+ 1

0

(cid:21)(cid:30)

2 

and ξ :=

φ(f ) + 1 − µ2

3.3 Low dimensional recovery
We consider estimating β∗ in the classical (low dimensional) setting where p (cid:28) n. Based on the
second moment estimator M deﬁned in (3.4)  estimating β∗ amounts to solving a noisy eigenvalue
problem. We solve this by a simple iterative algorithm: provided an initial vector β0 ∈ Sp−1 (which
may be chosen at random) we perform power iterations as shown in Algorithm 1.
Theorem 3.5. We assume X ∼ N (0  Ip) and (Y  X) follows (1.1). Let {(yi  xi)}n
i=1 be n i.i.d.
samples of response input pair (Y  X). For any link function f in (1.1) with µ0  φ(f ) deﬁned in (3.2)
and (3.1)  and φ(f ) > 01. We let
1 − µ2

(3.6)
There exist constant Ci such that when n ≥ C1p/ξ2  for Algorithm 1  we have that with probability
at least 1 − 2 exp(−C2p) 

(cid:13)(cid:13)βt − β∗(cid:13)(cid:13)2 ≤ C3 · φ(f ) + 1 − µ2
(cid:125)
(cid:123)(cid:122)
Here α =(cid:10)β0 (cid:98)β(cid:11)  where (cid:98)β is the ﬁrst leading eigenvector of M.
and optimization error terms in (3.7) are of the same order  we have(cid:13)(cid:13)βTmax − β∗(cid:13)(cid:13)2

Note that by (3.6) we have γ ∈ (0  1). Thus  the optimization error term in (3.7) decreases at
a geometric rate to zero as t increases. For Tmax sufﬁciently large such that the statistical error

γφ(f ) + (γ − 1)(1 − µ2
0)

(1 + γ)(cid:2)φ(f ) + 1 − µ2

(cid:46) (cid:112)p/n.

(cid:114) p
(cid:125)

This statistical rate of convergence matches the rate of estimating a p-dimensional vector in linear
regression without any quantization  and will later be shown to be optimal. This result shows that the
lack of prior knowledge on the link function and the information loss from quantization do not keep
our procedure from obtaining the optimal statistical rate.

for t = 1  . . .   Tmax.

1 − α2
α2

(cid:123)(cid:122)

Optimization Error

Statistical Error

(cid:114)
(cid:124)

(cid:3) .

0

· γt

 

0

·

+

n

(3.7)

φ(f )

3.4 High dimensional recovery
Next we consider the high dimensional setting where p (cid:29) n and β∗ is sparse  i.e.  β∗ ∈ Sp−1 ∩
B0(s  p) with s being support size. Although this high dimensional estimation problem is closely

1Recall that we have an analogous treatment and thus results for φ(f ) < 0.

5

related to the well-studied sparse PCA problem  the existing works [4  6  17  23  27  28  31  32] on
sparse PCA do not provide a direct solution to our problem. In particular  they either lack statistical
guarantees on the convergence rate of the obtained estimator [6  23  28] or rely on the properties of
the sample covariance matrix of Gaussian data [4  17]  which are violated by the second moment
estimator deﬁned in (3.4). For the sample covariance matrix of sub-Gaussian data  [27] prove that the

convex relaxation proposed by [7] achieves a suboptimal s(cid:112)log p/n rate of convergence. Yuan and
Zhang [31] propose the truncated power method  and show that it attains the optimal(cid:112)s log p/n rate
regularization parameter ρ  sparsity level(cid:98)s.

Algorithm 1 Low dimensional recovery
Input {(yi  xi)}n
i=1  number of iterations Tmax
1: Second moment estimation: Construct M
2: Initialization: Choose a random vector β0 ∈

Algorithm 2 Sparse recovery
Input {(yi  xi)}n

1: Second moment estimation: Construct M

from samples according to (3.4).

from samples according to (3.4).

i=1  number of iterations Tmax 

φ(f )3

Statistical Error

Optimization Error

(3.11)

with high probability. Here κ is deﬁned in (3.9).

The ﬁrst term on the right-hand side of (3.11) is the statistical error while the second term gives the
optimization error. Note that the optimization error decays at a geometric rate since κ < 1. For Tmax

6

Sn−1

βt ← M · βt−1
βt ← βt/(cid:107)βt(cid:107)2

3: For t = 1  2  . . .   Tmax do
4:
5:
6: end For
Output βTmax

2: Initialization:
3: Π0 ← argmin
Π∈Rp×p

{−(cid:104)M  Π(cid:105) + ρ(cid:107)Π(cid:107)1 1

| Tr(Π) = 1  0 (cid:22) Π (cid:22) I} (3.8)

βt ← βt/(cid:107)βt(cid:107)2

β0 ← ﬁrst leading eigenvector of Π0
β0 ← β0/(cid:107)β0(cid:107)2

β0 ← trunc(β0 (cid:98)s)
βt ← trunc(M · βt−1 (cid:98)s)

4:
5:
6:
7: For t = 1  2  . . .   Tmax do
8:
9:
10: end For
Output βTmax

locally; that is  it exhibits this rate of convergence
only in a neighborhood of the true solution where
(cid:104)β0  β∗(cid:105) > C where C > 0 is some constant. It
is well understood that for a random initialization
on Sp−1  such a condition fails with probability
going to one as p → ∞.
Instead  we propose a two-stage procedure for estimating β∗ in our setting. In the ﬁrst stage  we adapt
the convex relaxation proposed by [27] and use it as an initialization step  in order to obtain a good
enough initial point satisfying the condition (cid:104)β0  β∗(cid:105) > C. The convex optimization problem can be
easily solved by the alternating direction method of multipliers (ADMM) algorithm (see [3  27] for
details). Then we adapt the truncated power method. This procedure is illustrated in Algorithm 2. In
particular  we deﬁne truncation operator trunc(· ·) as [trunc(β  s)]j = 1(j ∈ S)βj  where S is the
index set corresponding to the top s largest |βj|. The initialization phase of our algorithm requires
O(s2 log p) samples (see below for more precise details) to succeed. As work in [2] suggests  it is
unlikely that a polynomial time algorithm can avoid such dependence. However  once we are near the

solution  as we show  this two-step procedure achieves the optimal error rate of(cid:112)s log p/n.

κ :=(cid:2)4(1 − µ2

0) + 3φ(f )(cid:3) < 1 
nmin := C · s2 log p · φ(f )2 · min(cid:8)κ(1 − κ1/2)/2  κ/8(cid:9)(cid:14)(cid:2)(1 − µ2

0) + φ(f )(cid:3)(cid:14)(cid:2)4(1 − µ2

0)(cid:3)(cid:112)log p/n with a sufﬁciently large constant C  where φ(f ) and µ0
Suppose ρ = C(cid:2)φ(f )+(1−µ2
are speciﬁed in (3.2) and (3.5). Meanwhile  assume the sparsity parameter(cid:98)s in Algorithm 2 is set to
be(cid:98)s = C(cid:48)(cid:48) max(cid:8)(cid:6)1/(κ−1/2−1)2(cid:7)  1(cid:9)·s∗. For n ≥ nmin with nmin deﬁned in (3.10)  we have
min(cid:8)(1 − κ1/2)/2  1/8(cid:9)
(cid:125)

0)(cid:3) 5
(cid:2)φ(f ) + (1 − µ2
(cid:123)(cid:122)

+ κt ·(cid:113)
(cid:124)

(cid:107)βt − β∗(cid:107)2 ≤ C ·

0) + φ(f )(cid:3)2

and the minimum sample size be

Theorem 3.6. Let

s log p

n

(cid:125)

2 (1 − µ2

0) 1

2

(cid:114)

·

.

(3.10)

(cid:123)(cid:122)

(3.9)

(cid:124)

sufﬁciently large  we have

(cid:13)(cid:13)βTmax − β∗(cid:13)(cid:13)2

(cid:46)(cid:112)s log p/n.

In the sequel  we show that the right-hand side gives the optimal statistical rate of convergence for a
broad model class under the high dimensional setting with p (cid:29) n.

Let X n

f := {(yi  xi)}n

|f (z) − f (z(cid:48))| ≤ L|z − z(cid:48)| 

3.5 Minimax lower bound
We establish the minimax lower bound for estimating β∗ in the model deﬁned in (1.1). In the sequel
we deﬁne the family of link functions that are Lipschitz continuous and are bounded away from ±1.
Formally  for any m ∈ (0  1) and L > 0  we deﬁne

F(m  L) :=(cid:8)f : |f (z)| ≤ 1 − m 
(3.12)
satisﬁes (1.1) with link function f. Correspondingly  we denote the estimator of β∗ ∈ B to be (cid:98)β(X n
i=1 be the n i.i.d. realizations of (Y  X)  where X follows N (0  Ip) and Y
f ) 
In the above deﬁnition  we not only take the inﬁmum over all possible estimators (cid:98)β  but also all

for all z  z(cid:48) ∈ R(cid:9).
f ) − β∗(cid:13)(cid:13)2.

where B is the domain of β∗. We deﬁne the minimax risk for estimating β∗ as

possible link functions in F(m  L). For a ﬁxed f  our formulation recovers the standard deﬁnition
of minimax risk [30]. By taking the inﬁmum over all link functions  our formulation characterizes
the minimax lower bound under the least challenging f in F(m  L). In the sequel we prove that our
procedure attains such a minimax lower bound for the least challenging f given any unknown link
function in F(m  L). That is to say  even when f is unknown  our estimation procedure is as accurate
as in the setting where we are provided the least challenging f  and the achieved accuracy is not
improvable due to the information-theoretic limit. The following theorem establishes the minimax
lower bound in the high dimensional setting.

Theorem 3.7. Let B =Sp−1∩B0(s  p). We assume that n > m(1−m)/(2L2)2·(cid:2)Cs log(p/s)/2−log 2(cid:3).

R(n  m  L B) := inf

E(cid:13)(cid:13)(cid:98)β(X n

f∈F (m L)

inf(cid:98)β(X n

f )

sup
β∗∈B

(3.13)

For any s ∈ (0  p/4]  the minimax risk deﬁned in (3.13) satisﬁes

(cid:112)m(1 − m)

(cid:114)

·

s log(p/s)

R(n  m  L B) ≥ C(cid:48) ·

n

L

.
Here C and C(cid:48) are absolute constants  while m and L are deﬁned in (3.12).
Theorem 3.7 establishes the minimax optimality of the statistical rate attained by our procedure for

p(cid:29) n and s-sparse β∗. In particular  for arbitrary f ∈ F(m  L) ∩ {f : φ(f ) > 0}  the estimator (cid:98)β
attained by Algorithm 2 is minimax-optimal in the sense that its(cid:112)s log p/n rate of convergence is
one can show the best possible convergence rate is Ω((cid:112)m(1 − m)p/n/L) by setting s = p/4 in
f (z) = sign(z). In fact  for noiseless one-bit compressed sensing  the(cid:112)s log p/n rate is not optimal.

Theorem 3.7.
It is worth to note that our lower bound becomes trivial for m = 0  i.e.  there exists some z such
that |f (z)| = 1. One example is the noiseless one-bit compressed sensing for which we have

not improvable  even when the information on the link function f is available. For general β∗ ∈ Rp 

For example  the Jacques et al. [14] provide an algorithm (with exponential running time) that achieves
rate s log p/n. Understanding such a rate transition phenomenon for link functions with zero margin 
i.e.  m = 0 in (3.12)  is an interesting future direction.

3.6 Numerical results

We now turn to the numerical results that support our theory. For the three models introduced in §2 
we apply Algorithm 1 and Algorithm 2 to do parameter estimation in the classic and high dimensional
regimes. Our simulations are based on synthetic data. For classic recovery  β∗ is randomly chosen
j = s−1/21(j ∈ S) for all j ∈ [p]  where S is a random
from Sp−1; for sparse recovery  we set β∗
index subset of [p] with size s. In Figure 1  as predicted by Theorem 3.5  we observe that the same

7

(cid:112)p/n leads to nearly identical estimation error. Figure 2 demonstrates similar results for the predicted
rate(cid:112)s log p/n of sparse recovery and thus validates Theorem 3.6.

(c) One-bit Phase Retrieval
(a) Flipped Logistic Regression
Figure 1: Estimation error of low dimensional recovery. (a) pe = 0.1. (b) δ2 = 0.1. (c) θ = 1.

(b) One-bit Compressed Sensing

(a) Flipped Logistic Regression

(b) One-bit Compressed Sensing

(c) One-bit Phase Retrieval

Figure 2: Estimation error of sparse recovery. (a) pe = 0.1. (b) δ2 = 0.1. (c) θ = 1.

4 Discussion

Sample complexity. In high dimensional regime  while our algorithm achieves optimal convergence
rate  the sample complexity we need is Ω(s2 log p). The natural question is whether it can be reduced
to O(s log p). We note that breaking the barrier s2 log p is challenging. Consider a simpler problem
sparse phase retrieval where yi = |(cid:104)xi  β∗(cid:105)|  with a fairly extensive body of literature  the state-of-
the-art efﬁcient algorithms (i.e.  with polynomial running time) for recovering sparse β∗ requires
sample complexity Ω(s2 log p) [10]. It remains open to show whether it’s possible to do consistent
sparse recovery with O(s log p) samples by any polynomial time algorithms.

Acknowledgment

XY and CC would like to acknowledge NSF grants 1056028  1302435 and 1116955. This research
was also partially supported by the U.S. Department of Transportation through the Data-Supported
Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center. HL is
grateful for the support of NSF CAREER Award DMS1454377  NSF IIS1408910  NSF IIS1332109 
NIH R01MH102339  NIH R01GM083084  and NIH R01HG06841. ZW was partially supported by
MSR PhD fellowship while this work was done.

References

[1] A L Q U I E R   P. and B I A U   G . (2013). Sparse single-index model. Journal of Machine Learning

Research  14 243–280.

[2] B E R T H E T  Q . and R I G O L L E T  P. (2013). Complexity theoretic lower bounds for sparse principal

component detection. In Conference on Learning Theory.

[3] B O Y D   S .  PA R I K H   N .  C H U   E .  P E L E AT O   B . and E C K S T E I N   J . (2011). Distributed
optimization and statistical learning via the alternating direction method of multipliers. Foundations and
Trends R(cid:13) in Machine Learning  3 1–122.

8

pp/n0.050.10.150.2EstimationError0.20.30.40.50.60.7p=10p=20p=40pp/n0.050.10.150.2EstimationError0.20.30.40.5p=10p=20p=40pp/n0.050.10.150.2EstimationError0.20.30.40.50.60.7p=10p=20p=40pslogp/n0.10.20.30.4EstimationError0.20.40.60.811.2p=100 s=5p=100 s=10p=200 s=5p=200 s=10pslogp/n0.10.20.30.4EstimationError00.20.40.60.8p=100 s=5p=100 s=10p=200 s=5p=200 s=10pslogp/n0.10.150.20.250.3EstimationError00.20.40.60.81p=100 s=5p=100 s=10p=200 s=5p=200 s=10[4] C A I   T. T.  M A   Z . and W U   Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. Annals

of Statistics  41 3074–3110.

[5] C A N D È S   E . J .  E L D A R   Y. C .  S T R O H M E R   T. and V O R O N I N S K I   V. (2013). Phase retrieval

via matrix completion. SIAM Journal on Imaging Sciences  6 199–225.

[6] D ’ A S P R E M O N T  A .  B A C H   F. and E L G H A O U I   L . (2008). Optimal solutions for sparse principal

component analysis. Journal of Machine Learning Research  9 1269–1294.

[7] D ’ A S P R E M O N T  A .  E L G H A O U I   L .  J O R D A N   M . I . and L A N C K R I E T  G . R . (2007). A

direct formulation for sparse PCA using semideﬁnite programming. SIAM Review 434–448.

[8] D E L E C R O I X   M .  H R I S TA C H E   M . and PAT I L E A   V. (2000). Optimal smoothing in semiparametric
index approximation of regression functions. Tech. rep.  Interdisciplinary Research Project: Quantiﬁcation
and Simulation of Economic Processes.

[9] D E L E C R O I X   M .  H R I S TA C H E   M . and PAT I L E A   V. (2006). On semiparametric M-estimation in

single-index regression. Journal of Statistical Planning and Inference  136 730–769.

[10] E L D A R   Y. C . and M E N D E L S O N   S . (2014). Phase retrieval: Stability and recovery guarantees.

Applied and Computational Harmonic Analysis  36 473–494.

[11] G O P I   S .  N E T R A PA L L I   P.  JA I N   P. and N O R I   A . (2013). One-bit compressed sensing: Provable

support and vector recovery. In International Conference on Machine Learning.

[12] H A R D L E   W.  H A L L   P. and I C H I M U R A   H . (1993). Optimal smoothing in single-index models.

Annals of Statistics  21 157–178.

[13] H R I S TA C H E   M .  J U D I T S K Y  A . and S P O K O I N Y  V. (2001). Direct estimation of the index

coefﬁcient in a single-index model. Annals of Statistics  29 pp. 595–623.

[14] JA C Q U E S   L .  L A S K A   J . N .  B O U F O U N O S   P. T. and B A R A N I U K   R . G . (2011). Robust 1-bit

compressive sensing via binary stable embeddings of sparse vectors. arXiv preprint arXiv:1104.3160.

[15] K A K A D E   S . M .  K A N A D E   V.  S H A M I R   O . and K A L A I   A . (2011). Efﬁcient learning of
generalized linear and single index models with isotonic regression. In Advances in Neural Information
Processing Systems.

[16] K A L A I   A . T. and S A S T RY  R . (2009). The isotron algorithm: High-dimensional isotonic regression.

In Conference on Learning Theory.

[17] M A   Z . (2013). Sparse principal component analysis and iterative thresholding. The Annals of Statistics 

41 772–801.

[18] M A S S A R T  P. and P I C A R D   J . (2007). Concentration inequalities and model selection  vol. 1896.

Springer.

[19] N ATA R A J A N   N .  D H I L L O N   I .  R AV I K U M A R   P. and T E WA R I   A . (2013). Learning with noisy

labels. In Advances in Neural Information Processing Systems.

[20] P L A N   Y. and V E R S H Y N I N   R . (2013). One-bit compressed sensing by linear programming. Commu-

nications on Pure and Applied Mathematics  66 1275–1297.

[21] P L A N   Y.  V E R S H Y N I N   R . and Y U D O V I N A   E . (2014). High-dimensional estimation with

geometric constraints. arXiv preprint arXiv:1404.3749.

[22] P O W E L L   J . L .  S T O C K   J . H . and S T O K E R   T. M . (1989). Semiparametric estimation of index

coefﬁcients. Econometrica  57 pp. 1403–1430.

[23] S H E N   H . and H U A N G   J . (2008). Sparse principal component analysis via regularized low rank matrix

approximation. Journal of Multivariate Analysis  99 1015–1034.

[24] S T O K E R   T. M . (1986). Consistent estimation of scaled coefﬁcients. Econometrica  54 pp. 1461–1481.
[25] T I B S H I R A N I   J . and M A N N I N G   C . D . (2013). Robust logistic regression using shift parameters.

arXiv preprint arXiv:1305.4987.

[26] V E R S H Y N I N   R . (2010).

Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027.

[27] V U   V. Q .  C H O   J .  L E I   J . and R O H E   K . (2013). Fantope projection and selection: A near-optimal

convex relaxation of sparse PCA. In Advances in Neural Information Processing Systems.

[28] W I T T E N   D .  T I B S H I R A N I   R . and H A S T I E   T. (2009). A penalized matrix decomposition  with
applications to sparse principal components and canonical correlation analysis. Biostatistics  10 515–534.
[29] Y I   X .  WA N G   Z .  C A R A M A N I S   C . and L I U   H . (2015). Optimal linear estimation under unknown

nonlinear transform. arXiv preprint arXiv:1505.03257.

[30] Y U   B . (1997). Assouad  Fano  and Le Cam. In Festschrift for Lucien Le Cam. Springer  423–435.
[31] Y U A N   X . - T. and Z H A N G   T. (2013). Truncated power method for sparse eigenvalue problems.

Journal of Machine Learning Research  14 899–925.

[32] Z O U   H .  H A S T I E   T. and T I B S H I R A N I   R . (2006). Sparse principal component analysis. Journal

of Computational and Graphical Statistics  15 265–286.

9

,Xinyang Yi
Zhaoran Wang
Constantine Caramanis
Han Liu