2019,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling,Motivated by the many real-world applications of reinforcement learning (RL) that require safe-policy iterations  we consider the problem of off-policy evaluation (OPE) --- the problem of  evaluating a new policy using the historical data obtained by different behavior policies --- under the model of nonstationary episodic Markov Decision Processes (MDP) with a long horizon and a large action space. Existing importance sampling (IS) methods often suffer from large variance that depends exponentially on the RL horizon $H$. To solve this problem  we consider a marginalized importance sampling (MIS) estimator that recursively estimates the state marginal distribution for the target policy at every step. 
MIS achieves a mean-squared error of 
$$
\frac{1}{n} \sum_{t=1}^H\mathbb{E}_{\mu}\left[\frac{d_t^\pi(s_t)^2}{d_t^\mu(s_t)^2} \Var_{\mu}\left[\frac{\pi_t(a_t|s_t)}{\mu_t(a_t|s_t)}\big( V_{t+1}^\pi(s_{t+1}) + r_t\big) \middle| s_t\right]\right]   + \tilde{O}(n^{-1.5})
$$
where $\mu$ and $\pi$ are the logging and target policies  $d_t^{\mu}(s_t)$ and $d_t^{\pi}(s_t)$ are the marginal distribution of the state at $t$th step  $H$ is the horizon  $n$ is the sample size and $V_{t+1}^\pi$ is the value function of the MDP under $\pi$. The result matches the Cramer-Rao lower bound in [Jiang and Li  2016] up to a multiplicative factor of $H$. To the best of our knowledge  this is the first OPE estimation error bound with a polynomial dependence on $H$. Besides theory  we show empirical superiority of our method in time-varying  partially observable  and long-horizon RL environments.,Towards Optimal Off-Policy Evaluation for
Reinforcement Learning with Marginalized

Importance Sampling

Tengyang Xieâˆ—

Dept. of Computer Science

UIUC

Urbana  IL 61801

tx10@illinois.edu

Yifei Ma

AWS AI Labs

Amazon.com Services  Inc.
East Palo Alto  CA 94303

yifeim@amazon.com

Abstract

Yu-Xiang Wang

Dept. of Computer Science 

UC Santa Barbara

Santa Barbara  CA 93106
yuxiangw@cs.ucsb.edu

Motivated by the many real-world applications of reinforcement learning (RL) that
require safe-policy iterations  we consider the problem of off-policy evaluation
(OPE) â€” the problem of evaluating a new policy using the historical data ob-
tained by different behavior policies â€” under the model of nonstationary episodic
Markov Decision Processes (MDP) with a long horizon and a large action space.
Existing importance sampling (IS) methods often suffer from large variance that
depends exponentially on the RL horizon H. To solve this problem  we consider
a marginalized importance sampling (MIS) estimator that recursively estimates
the state marginal distribution for the target policy at every step. MIS achieves a
mean-squared error of

EÂµ

t=1

t (st)2
t (st)2 VarÂµ
dÂµ

t+1(st+1) + rt

+ ËœO(nâˆ’1.5)

(cid:20) dÏ€

(cid:88)H

1
n

(cid:20) Ï€t(at|st)

Âµt(at|st)

(cid:0)V Ï€

(cid:21)(cid:21)

(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)st

where Âµ and Ï€ are the logging and target policies  dÂµ
t (st) are the
marginal distribution of the state at tth step  H is the horizon  n is the sample
size and V Ï€
t+1 is the value function of the MDP under Ï€. The result matches the
Cramer-Rao lower bound in Jiang and Li [2016] up to a multiplicative factor of H.
To the best of our knowledge  this is the ï¬rst OPE estimation error bound with a
polynomial dependence on H. Besides theory  we show empirical superiority of our
method in time-varying  partially observable  and long-horizon RL environments.

t (st) and dÏ€

1

Introduction

The problem of off-policy evaluation (OPE)  which predicts the performance of a policy with data only
sampled by a behavior policy [Sutton and Barto  1998]  is crucial for using reinforcement learning
(RL) algorithms responsibly in many real-world applications. In many settings where RL algorithms
have already been deployed  e.g.  targeted advertising and marketing [Bottou et al.  2013; Tang et al. 
2013; Chapelle et al.  2015; Theocharous et al.  2015; Thomas et al.  2017] or medical treatments
[Murphy et al.  2001; Ernst et al.  2006; Raghu et al.  2017]  online policy evaluation is usually
expensive  risky  or even unethical. Also  using a bad policy in these applications is dangerous and
could lead to severe consequences. Solving OPE is often the starting point in many RL applications.
To tackle the problem of OPE  the idea of importance sampling (IS) corrects the mismatch in the
distributions under the behavior policy and target policy. It also provides typically unbiased or
âˆ—The research was partially conducted when TX was visiting YW and YM at Amazon AWS AI Labs during

his internship in Summer 2018.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

strongly consistent estimators [Precup et al.  2000]. IS-based off-policy evaluation methods have
also seen lots of interest recently especially for short-horizon problems  including contextual bandits
[Murphy et al.  2001; Hirano et al.  2003; DudÃ­k et al.  2011; Wang et al.  2017]. However  the
variance of IS-based approaches [Precup et al.  2000; Thomas et al.  2015; Jiang and Li  2016;
Thomas and Brunskill  2016; Guo et al.  2017; Farajtabar et al.  2018] tends to be too high to provide
informative results  for long-horizon problems [Mandel et al.  2014]  since the variance of the product
of importance weights may grow exponentially as the horizon goes long. There are also model-based
approaches for solving OPE problems [Liu et al.  2018b; Gottesman et al.  2019]  where the value of
the target policy is estimated directly using the approximated MDP.
Given this high-variance issue  it is necessary to ï¬nd an IS-based approach without relying heavily
on the cumulative product of importance weights from the whole trajectories. While the beneï¬t of
cumulative products is to allow unbiased estimation even without any state observability assumptions 
reweighing the entire trajectories may not be necessary if some intermediate states are directly
observable. For the latter  based on Markov independence assumptions  we can aggregate all
trajectories that share the same state transition patterns to directly estimate the state distribution shifts
after the change of policies from the behavioral to the target. We call this approach marginalized
importance sampling (MIS)  because it computes the marginal state distribution shifts at every single
step  instead of the product of policy weights.
Related work [Liu et al.  2018a] tackles the high variance issue due to the cumulative product of
importance weights. They apply importance sampling on the average visitation distribution of state-
action pairs  based on an estimation of the mixed state distribution. Hallak and Mannor [2017] and
Gelada and Bellemare [2019] also leverage the same fact in time-invariant MDPs  where they use the
stationary ratio of state-action pairs to replace the trajectory weights. However  these methods may
not directly work in ï¬nite-horizon MDPs  where the state distributions may not mix.
In contrast to the prior work  the ï¬rst goal of our paper is to study the sample complexity and
optimality of the marginalized approach. Speciï¬cally  we provide the ï¬rst ï¬nite sample error bound
on the mean-square error for our MIS off-policy evaluation estimator under the episodic tabular MDP
setting (with potentially continuous action space). Our MSE bound is the exact calculation up to low
order terms. Comparing to the Cramer-Rao lower bound established in [Jiang and Li  2016  Theorem
3] for DAG-MDP  our bound is larger by at most a factor of H and we have good reasons to believe
that this additional factor is required for any OPE estimators in this setting.
In addition to the theoretical results  we empirically evaluate our estimator against a number of strong
baselines from prior work in a number of time-invariant/time-varying  fully observable/partially
observable  and long-horizon environments. Our approach can also be used in most of OPE estimators
that leverage IS-based estimators  such as doubly robust [Jiang and Li  2016]  MAGIC [Thomas and
Brunskill  2016]  MRDR [Farajtabar et al.  2018] under mild assumptions (Markov assumption).
Here is a road map for the rest of the paper. Section 2 provides the preliminaries of the problem of
off-policy evaluation. In Section 3  we offer the design of our marginalized estimator  and we study
its information-theoretical optimality in Section 4. We present the empirical results in a number of
RL tasks in Section 5. At last  Section 6 concludes the paper.

2 Problem formulation

Symbols and notations. We consider the problem of off-policy evaluation for a ï¬nite horizon 
nonstationary  episodic MDP  which is a tuple deï¬ned by M = (S A  T  r  H)  where S is the state
space  A is the action space  Tt : SÃ—AÃ—S â†’ [0  1] is the transition function with Tt(s(cid:48)|s  a) deï¬ned
by probability of achieving state s(cid:48) after taking action a in state s at time t  and rt : S Ã— A Ã— S â†’ R
is the expected reward function with rt(s  a  s(cid:48)) deï¬ned by the mean of immediate received reward
after taking action a in state s and transitioning into s(cid:48)  and H denotes the ï¬nite horizon. We use P[E]
to denote the probability of an event E and p(x) the p.m.f. (or pdf) of the random variable X taking
value x. E[Â·] and E[Â·|E] denotes the expectation and conditional expectation given E  respectively.
Let Âµ  Ï€ : S â†’ PA be policies which output a distribution of actions given an observed state. We
call Âµ the behavioral policy and Ï€ the target policy. For notation convenience we denote Âµ(at|st)
and Ï€(at|st) the p.m.f of actions given state at time t. The expectation operators in this paper will
either be indexed with Ï€ or Âµ  which denotes that all random variables coming from roll-outs from

2

atâˆ’1

t (st) and dÏ€

the speciï¬ed policy. Moreover  we denote dÂµ
t (st) the induced state distribution at time t.
When t = 1  the initial distributions are identical dÂµ
t (st) are
functions of not just the policies themselves but also the unknown underlying transition dynamics 
i.e.  for Ï€ (and similarly Âµ)  recursively deï¬ne
t (st|stâˆ’1)dÏ€
(cid:88)
P Ï€
t (st|stâˆ’1) =

Tt(st|stâˆ’1  atâˆ’1)Ï€(atâˆ’1|stâˆ’1).

1 = d1. For t > 1  dÂµ

t (st) and dÏ€

tâˆ’1(stâˆ’1) 

(cid:88)

dÏ€
t (st) =

where P Ï€

1 = dÏ€

(2.1)

stâˆ’1

t+1 t(s(cid:48)|s) = (cid:80)

t

  a(i)
t

  a(i)
t

i j âˆˆ RSÃ—S âˆ€j < i as the state-transition probability from step j to step i un-
a Pt+1 t(s(cid:48)|s  a)Ï€t(a|s) =

We denote P Ï€
der a sequence of actions taken by Ï€. Note that P Ï€
Tt+1(s(cid:48)|s  Ï€t(s)).
t ) âˆˆ S Ã— A Ã— R for time index
Behavior policy Âµ is used to collect data in the form of (s(i)
  r(i)
t
t = 1  . . .   H and episode index i = 1  ...  n. Target policy Ï€ is what we are interested to evaluate.
Also  let D to denote the historical data  which contains n episode trajectories in total. We also deï¬ne
Dh = {(s(i)
Throughout the paper  probability distributions are often used in their vector or matrix form. For
t without an input is interpreted as a vector in a S-dimensional probability simplex and
instance  dÏ€
t .
i j is then a stochastic transition matrix. This allows us to write (2.1) concisely as dÏ€
P Ï€
t+1 tdÏ€
Also note that while st  at  rt are usually used to denote ï¬xed elements in set S A and R  in
some cases we also overload them to denote generic random variables s(1)
. For exam-
ple  EÏ€[rt] = EÏ€[r(1)
dÏ€(st  at  st+1)rt(st  at  st+1) and VarÏ€[rt(st  at  st+1)] =
VarÏ€[rt(s(1)
  s(1)

t ) : i âˆˆ [n]  t â‰¤ h} to be roll-in realization of n trajectories up to step h.
  r(i)

t
t+1)]. The distinctions will be clear in each context.

Problem setup. The problem of off-policy evaluation is about ï¬nding an estimator(cid:98)vÏ€ : (S Ã— A Ã—

R)HÃ—n â†’ R that makes use of the data collected by running Âµ to estimate

] = (cid:80)

t+1 = P Ï€

st at st+1

  a(1)

  a(1)

  r(1)

t

t

t

t

t

H(cid:88)

(cid:88)

(cid:88)

(cid:88)

Ï€(at|st)

Tt(st+1|st  at)rt(st  at  st+1) 

st

at

t=1

st+1

vÏ€ =

dÏ€
t (st)

(2.2)
where we assume knowledge about Âµ(a|s) and Ï€(a|s) for all (s  a) âˆˆ S Ã— A  but do not observe
rt(st  at  st+1) for any actions other than a noisy version of it the evaluated actions. Nor do we
t (st)âˆ€t > 1 implied by the change of policies. Nonetheless  our goal
observe the state distributions dÏ€
is to ï¬nd an estimator to minimize the mean-square error (MSE): MSE(Ï€  Âµ  M ) = EÂµ[(Ë†vÏ€ âˆ’ vÏ€)2] 
using the observed data and the known action probabilities. Different from previous studies  we focus
on the case where S is sufï¬ciently small but S2A is too large for a reasonable sample size. In other
words  this is a setting where we do not have enough data points to estimate the state-action-state
transition dynamics  but we do observe the states and can estimate the distribution of the states after
the change of policies  which is our main strategy.

Assumptions: We list the technical assumptions we need and provide necessary justiï¬cation.

A1. âˆƒRmax  Ïƒ < +âˆ such that 0 â‰¤ E[rt|st  at  st+1] â‰¤ Rmax  Var[rt|st  at  st+1] â‰¤ Ïƒ2 for

all t  st  at.

A2. Behavior policy Âµ obeys that dm := mint st dÂµ
A3. Bounded weights: Ï„s := maxt st

t (st) > 0 âˆ€t  st such that dÏ€
t (st) > 0.
Ï€(at|st)
Âµ(at|st) < +âˆ.

t (st) < +âˆ and Ï„a := maxt st at

dÏ€
t (st)
dÂµ

Assumption A1 is assumed without loss of generality. The Ïƒ bound is required even for on-policy
evaluation and the assumption on the non-negativity and Rmax can always be obtained by shifting and
rescaling the problem. Assumption A2 is necessary for any consistent off-policy evaluation estimator.
Assumption A3 is also necessary for discrete state and actions  as otherwise the second moments of
the importance weight would be unbounded. For continuous actions  Ï„a < +âˆ is stronger than we
need and should be considered a simplifying assumption for the clarity of our presentation. Finally 
we comment that the dependence in the parameter dm  Ï„s  Ï„a do not occur in the leading O(1/n)
term of our MSE bound  but only in simpliï¬ed results after relaxation.

3

3 Marginalized Importance Sampling Estimators for OPE

In this section  we present the design of marginalized IS estimators for OPE. For small action spaces 
we may directly build models by the estimated transition function Tt(st|stâˆ’1  atâˆ’1) and the reward
function rt(st  at  st+1) from empirical data. However  the models may be inaccurate in large action
spaces  where not all actions are frequently visited. Function approximation in the models may cause
additional biases from covariate shifts due to the change of policies. Standard importance sampling
estimators (including the doubly robust versions)[DudÃ­k et al.  2011; Jiang and Li  2016] avoid the
need to estimate the modelâ€™s dynamics but rather directly approximating the expected reward:

n(cid:88)

H(cid:88)

(cid:34) h(cid:89)

i=1

h=1

t=1

(cid:98)vÏ€

IS =

1
n

(cid:35)

Ï€(a(i)
t
Âµ(a(i)
t

|s(i)
t )
|s(i)
t )

r(i)
h .

To adjust for the differences in the policy  importance weights are used and it can be shown that this
is an unbiased estimator of vÏ€ (See more detailed discussion of IS and the doubly robust version
in Appendix C). The main issue of this approach  when applying to the episodic MDP with large
action space is that the variance of the importance weights grows exponentially in H [Liu et al. 
2018a]  which makes the sample complexity exponentially worse than the model-based approaches 
when they are applicable. We address this problem by proposing an alternative way of estimating
the importance weights which achieves the same sample complexity as the model-based approaches
while allowing us to achieve the same ï¬‚exibility and interpretability as the IS estimator that does not
explicitly require estimating the state-action dynamics Tt. We propose the Marginalized Importance
Sampling (MIS) estimator:

H(cid:88)

t=1

(cid:98)dÏ€
(cid:98)dÂµ

n(cid:88)
(cid:80)

i=1

1
n

MIS =

t (s(i)
t ).

t (s(i)
t )
t (s(i)
t )

(cid:98)rÏ€
(cid:98)vÏ€
t   (cid:98)dÂµ â†’ dÂµ
Clearly  if (cid:98)dÏ€ â†’ dÏ€
t â†’ EÏ€[Rt(st  at)|st]  then(cid:98)vÏ€
t  (cid:98)rÏ€
It turns out that if we take (cid:98)dÂµ
t (st) = 0 whenever nst = 0  then (3.1) is equivalent to(cid:80)H
t (st)/(cid:98)dÂµ
(cid:98)dÏ€
direct plug-in estimator of (2.2). It remains to specify (cid:98)dÏ€
(cid:98)dÏ€
t (cid:98)dÏ€
t = (cid:98)P Ï€
tâˆ’1  where (cid:98)P Ï€
n(cid:88)
and(cid:98)rÏ€

tâˆ’1|stâˆ’1)
tâˆ’1|stâˆ’1)

t (st|stâˆ’1) =

recursively using

Ï€(a(i)
Âµ(a(i)

t (st) := 1
n

n(cid:88)

t 1(s(i)
r(i)

t = st) 

MIS â†’ vÏ€.

i 1(s(i)

t (st) =

nstâˆ’1

i=1

1

1
nst

i=1

Ï€(a(i)
t
Âµ(a(i)
t

|st)
|st)

t = st) â€” the empirical mean â€” and deï¬ne

(cid:80)
st (cid:98)dÏ€
t (st)(cid:98)rÏ€(st) â€“ the
t (st) and (cid:98)rÏ€(st). (cid:98)dÏ€

t (st) is estimated

t=1

1((s(i)

tâˆ’1  s(i)

t ) = (stâˆ’1  st));

(3.1)

(3.2)

where nsÏ„ is the empirical visitation frequency to state sÏ„ at time Ï„. Note that our estimator of rÏ€
t (st)
is the standard IS estimators we use in bandits [Li et al.  2015]  which are shown to be optimal (up to
a universal constant) when A is large [Wang et al.  2017].
The advantage of MIS over the naive IS estimator is that the variance of the importance weight need
not depend exponentially in H. A major theoretical contribution of this paper is to formalize this
argument by characterizing the dependence on Ï€  Âµ as well as parameters of the MDP M. Note that
MIS estimator does not dominate the IS estimator. In the more general setting when the state is given
by the entire history of observations  Jiang and Li [2016] establishes that no estimators can achieve
polynomial dependence in H. We give a concrete example later (Example 1) about how IS estimator
suffers from the â€œcurse of horizonâ€ [Liu et al.  2018a]. MIS estimator can be thought of as one that
exploits the state-observability while retaining properties of the IS estimators to tackle the problem of
large action space. As we illustrate in the experiments  MIS estimator can be modiï¬ed to naturally
handle partially observed states  e.g.  when s is only observed every other step.
Finally  when available  model-based approaches can be combined into importance-weighted methods
[Jiang and Li  2016; Thomas and Brunskill  2016]. We defer discussions about these extensions in
Appendix C to stay focused on the scenarios where model-based approaches are not applicable.

4

4 Theoretical Analysis of the MIS Estimator

Motivated by the challenge of curse of horizon with naive IS estimators  similar to [Liu et al.  2018a] 
we show that the sample complexity of our MIS estimator reduces to O(H 3). To the best of our
knowledge  this is ï¬rst sample complexity guarantee under this setting  which also matches the
Cramer-Rao lower bound for DAG-MDP [Jiang and Li  2016] as n â†’ âˆ up to a factor of H.
Example 1 (Curse of horizon). Assume a MDP with i.i.d. state transition models over time and
assume that Ï€t
is bounded from both sides for all t. Suppose the reward is a constant 1 only
Âµt

shown at the last step  such that naive IS becomes(cid:98)vÏ€
trajectory  (cid:81)H
Central Limit Theorem (cid:80)H
(cid:0)âˆ’HElog  HVlog
asymptotically follows LogNormal(cid:0)âˆ’HElog  HVlog
(cid:1) 
whose variance is exponential in horizon:(cid:0)exp (HVlog) âˆ’ 1(cid:1). On the other hand  MIS estimates the

]. By
asymptotically follows a normal distribution with parameters

(cid:104)(cid:80)H
(cid:1). In other words (cid:81)H

] and Vlog = Var[log Ï€t
Âµt

; let Elog = E[log Ï€t

(cid:20)(cid:81)H

state distributions recursively  yielding variance that is polynomial in horizon and small OPE errors.

t=1 log Ï€t
Âµt

t=1 log Ï€t
Âµt

(cid:80)n

. For every

|s(i)
t )
|s(i)
t )

IS = 1
n

Ï€(a(i)
Âµ(a(i)

= exp

(cid:21)

(cid:105)

Ï€t
Âµt

Ï€t
Âµt

t=1

t=1

t=1

i=1

Âµt

t

t

We now formalize the sample complexity bound in Theorem 4.1.
Theorem 4.1. Let the value function under Ï€ be deï¬ned as follows:

(cid:34) H(cid:88)

t=h

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s(1)

h (sh) := EÏ€
V Ï€

rt(s(1)

t

  a(1)

t

  s(1)

t+1)

h = sh

âˆˆ [0  Vmax]  âˆ€h âˆˆ {1  2  ...  H}.

For the simplicity of the statement  deï¬ne boundary conditions: r0(s0) â‰¡ 0  Ïƒ0(s0  a0) â‰¡ 0  dÏ€
1  Ï€(a0|s0)
the number of episodes n obeys that

H+1 â‰¡ 0. Moreover  let Ï„a := maxt st at

Ï€(at|st)
Âµ(at|st) and Ï„s := maxt st

Âµ(a0|s0) â‰¡ 1 and V Ï€

0 (s0) â‰¡
0 (s0)
dÂµ
dÏ€
t (st)
t (st) . If
dÂµ

n > max

(cid:26) 16 log n
for all t = 2  ...  H  then the our estimator(cid:98)vÏ€
(cid:88)
H(cid:88)
E[(P(cid:98)vÏ€
(cid:115)
(cid:32)

MIS âˆ’ vÏ€)2] â‰¤ 1
n

h(sh)2
dÏ€
dÂµ
h(sh)

mint st dÂµ

h=0

sh

 

t (st)

VarÂµ

4tÏ„aÏ„s
mint st max{dÏ€
(cid:34)

Ï€(a(1)
Âµ(a(1)

(cid:33)

h |sh)
h |sh)
19Ï„ 2

+

(cid:27)

t (st)  dÂµ

t (st)}

MIS with an additional clipping step obeys that

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s(1)

(V Ï€

h+1(s(1)

h+1) + r(1)
h )

h = sh

Â·

1 +

16 log n
n mint st dÂµ

t (st)

a Ï„ 2

s SH 2(Ïƒ2 + R2

max + V 2

max)

.

n2

Corollary 1. In the familiar setting when Vmax = HRmax  then the same conditions in Theorem 4.1
implies that:

E[(P(cid:98)vÏ€

MIS âˆ’ vÏ€)2] â‰¤ 4
n

Ï„aÏ„s(HÏƒ2 + H 3R2

max).

lates the MSE of a clipped version of our estimator(cid:98)vMIS

We make a few remarks about the results in Theorem 4.1.
Dependence on S  A and the weights. The leading term in the variance bound very precisely calcu-
1 modulo a (1 + O(nâˆ’1/2)) multiplicative
factor and an O(1/n2) additive factor. Speciï¬cally  our bound does not explicitly depend on S and A
but instead on how similar Ï€ and Âµ are. This allows the method to handle the case when the action
space is continuous. The dependence on Ï„a  Ï„s only appear in the low-order terms  while the leading
term depends only on the second moments of the importance weights.
Dependence on H. In general  our sample complexity upper bound is proportional to H 3  as
Corollary 1 indicates. Our bound reveals that in several cases it is possible to achieve a smaller

1The clipping step to [0  HRmax] or [0  Vmax] should not be alarming. It is required only for technical
reasons  and the clipped estimator is a valid estimator to begin with. Since the true policy value must be within
the range  the clipping step is only going to improve the MSE.

5

exponent on H for speciï¬c triplets of (M  Ï€  Âµ). For instance  when Ï€ â‰ˆ Âµ  such that Ï„a  Ï„s =
1 + O(1/H)  the variance bound gives O((V 2
max + HÏƒ2)/n)  which
matches the MSE bound (up to a constant) of the simple-averaging estimator that knows Ï€ = Âµ
a-priori. (See Remark 3 in the Appendix for more details). If Vmax is a constant that does not depend
on H (this is often the case in games when there is a ï¬xed reward at the end)  then the sample
complexity is only O(H).
Optimality. Comparing to the Cramer-Rao lower bound of the Theorem 3 in [Jiang and Li  2016] 
which we paraphrase below

max + HÏƒ2)/n) or O((H 2R2

the MSE of our estimator is asymptotically bigger by an additive factor of

(cid:12)(cid:12)(cid:12)s(1)

(cid:105)

(cid:104)

1
n

H(cid:88)

h=1

sh

ah

(cid:88)

dÏ€
h(sh)2
dÂµ
h(sh)

(cid:88)
(cid:88)
H(cid:88)
h(sh  ah) := E(cid:2)(V Ï€

1
n

h=1

Ï€h(ah|sh)2
Âµh(ah|sh)

h(sh)2
dÏ€
dÂµ
h(sh)

sh
h+1(s(1)

Var

h+1(s(1)
V Ï€

h+1) + r(1)

h

h = sh  a(1)

h = ah

 

(4.1)

(cid:34)
h )(cid:12)(cid:12)s(1)

VarÂµ

Ï€h(a(1)
Âµh(a(1)

h |sh)
h |sh)
h = sh  a(1)

QÏ€

h(sh  a(1)
h )

(cid:35)
(cid:3) is the standard Q-function

(4.2)

 

h+1) + r(1)

h = ah

where QÏ€
the MDP. The gap is signiï¬cant as the CR lower bound (4.1) itself only has a worst-case bound of
H 2Ï„sÏ„a/n 2  while (4.2) is proportional to H 3Ï„sÏ„a/n. This implies that our estimator is optimal up
to a factor of H. See Remark 4 for more details in the appendix.
It is an intriguing open question whether this additional factor of H can be removed. Our conjecture is
that the answer is negative and what we established in Theorem 4.1 matches the correct information-
theoretic limit for any methods in the cases when the action space A is continuous (or signiï¬cantly
larger than n). This conjecture is consistent with an existing lower bound in the simpler contextual
bandits setting  where Wang et al. [2017] established that a variance of expectation term analogous to
the one above cannot be removed  and no estimators can asymptotically attain the CR lower bound
for all problems in the large state/action space setting.

4.1 Proof Sketch

Recall that (3.1) is equivalent to(cid:80)H
sampling and(cid:98)dÏ€

In this section  we brieï¬‚y describe the main technical components in the proof of Theorem 4.1. More
detailed arguments are deferred to the full proof in Appendix B.

t (st) is recursively estimated using(cid:98)dÏ€

(cid:80)
st (cid:98)dÏ€
t (st)(cid:98)rÏ€(st)  where(cid:98)rÏ€(st) is estimated with importance
tâˆ’1(stâˆ’1) and the importance sampling estimator
t (st|stâˆ’1) under Ï€. While the MIS estimator is easy to state  it is not

of the transition matrix P Ï€
straightforward to analyze. We highlight three challenges below.

t=1

1. Dependent data and complex estimator: While the episodes are independent  the data within
each episode are not. Each time step of our MIS estimator uses the data from all episodes
up to that time step.

2. An annoying bias: There is a non-zero probability that some states st at time t is not visited
at all in all n episodes. This creates a bias in the estimator of Ë†dÏ€
h for all time h > t. While
the probability of this happening is extremely small  conditioning on the high probability
event breaks the natural conditional independences  which makes it hard to analyze.

3. Error propagation: The recursive estimator Ë†dÏ€

t is affected by all estimation errors in earlier
time steps. Naive calculation of the error with a constant slack in each step can lead to a
â€œsnowballâ€ effect that causes an exponential blow-up.

All these issues require delicate handling because otherwise the MSE calculation will not be tight.
Our solutions are as follows.
Deï¬ning the appropriate ï¬ltration. The ï¬rst observation is that we need to have a convenient
representation of the data. Instead of considering the n episodes as independent trajectories  it is
more useful to think of them all together as a Markov chain of multi-dimensional observations

2This is somewhat surprising as each of the H summands in the expression can be as large as H 2.

6

(cid:88)

stâˆ’1

ËœdÏ€
t (st) =

ËœP Ï€(st|stâˆ’1) ËœdÏ€

tâˆ’1(stâˆ’1).

(cid:110)

(cid:111)n

s(i)
1:t  a(i)

of n state  action  reward triplets. Speciï¬cally  we deï¬ne the â€œcumulativeâ€ data up to time t by
Datat :=
. Also  we observe that the state of the Markov chain at time t can
be summarized by nst â€” the number of times state st is visited.

Fictitious estimator technique. We address the bias issue by deï¬ning a ï¬ctitious estimator(cid:101)vÏ€. The

1:tâˆ’1  r(i)

1:tâˆ’1

i=1

t and Ë†rÏ€

t   the ï¬ctitious version of these estimators

ï¬ctitious estimator is constructed by  instead of Ë†dÏ€
ËœdÏ€
t and ËœrÏ€

t is constructed recursively using

t   where ËœdÏ€

VarÂµ

Ï€(a(1)
Âµ(a(1)

h |sh)
h |sh)

(V Ï€

h+1(s(1)

h+1) + r(1)
h )

h = sh

.

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s(1)

The key difference is that whenever nst < EÂµnst (1 âˆ’ Î´) for some 0 < Î´ < 1  we assign
ËœP Ï€(st+1|st) = P Ï€(st+1|st) and ËœrÏ€(st) = EÏ€[rt|st] â€” the true values of interest. This ensures that
the ï¬ctitious estimator is always unbiased (see Lemma B.2). Note that this ï¬ctitious estimator cannot
be implemented in practice. It is used as a purely theoretical construct that simpliï¬es the analysis of
the (biased) MIS estimator. In Lemma B.1  we show that the ËœvÏ€ and Ë†vÏ€ are exponentially close to
each other.
Disentangling the dependency by backwards peeling. The ï¬ctitious estimator technique reduces
the problem of calculating the MSE of the MIS estimator to a variance analysis of the ï¬ctitious
estimator. By recursively applying the law of total variance backwards that peels one item at a time
from Datat  we establish an exact linear decomposition of the variance of the ï¬ctitious estimator
(Lemma B.3):

Var[(cid:101)vÏ€] =

(cid:34)(cid:101)dÏ€

E

H(cid:88)

(cid:88)

h=0

sh

h(sh)2
nsh

1

(cid:18)
nsh â‰¥ ndÂµ

h(sh)
(1 âˆ’ Î´)âˆ’1

(cid:19)(cid:35)

(cid:34)

E

(cid:18)

(cid:34)(cid:101)dÏ€

Observe that the value function V Ï€
t shows up naturally. This novel decomposition can be thought of
as a generalization of the celebrated Bellman-equation of variance [Sobel  1982] in the off-policy 
episodic MDP setting with a ï¬nite sample and can be of independent interest.
Characterizing the error propagation in ËœdÏ€
distribution estimation as follows

(cid:19)(cid:35)
which reduces the problem to bounding Var[(cid:101)dÏ€
at most linearly in h: Var[(cid:101)dÏ€
Cov((cid:101)dÏ€
h(sh)] â‰¤ 2(1âˆ’Î´)âˆ’1hdÏ€
Finally  Theorem 4.1 is established by appropriately choosing Î´ = O((cid:112)log n/n mint st dÂµ

h(sh)]. We show (in Theorem B.1) that instead of an
exponential blow-up as will a concentration-inequality based argument imply  the variance increases
. The proof uses a novel decomposition of
h) (Lemma B.5)  which is derived using a similar backwards peeling argument as before.

h(sh). Lastly  we bound the error term in the state
â‰¤ (1 âˆ’ Î´)âˆ’1

nsh â‰¥ ndÂµ

h(sh)
(1 âˆ’ Î´)âˆ’1

h(sh)2
dÂµ
h(sh)

(cid:18) dÏ€

h(sh)2
nsh

(cid:104)(cid:101)dÏ€

(cid:105)(cid:19)

h(sh)

+ Var

t (st)).

h(sh)

n

1

n

 

Due to space limits  we can only highlight a few key elements of the proof. We invite the readers to
check out a more detailed exposition in Appendix B.

5 Experiments

Throughout this section  we present the empirical results to illustrate the comparison among different
estimators. We demonstrate the effectiveness of our proposed marginalized estimator by comparing it
with different classic estimators on several domains.
The methods we compare in this section are: direct method (DM)  importance sampling (IS) 
weighted importance sampling (WIS)  importance sampling with stationary state distribution (SSD-
IS)  and marginalized importance sampling (MIS). DM uses the model-based approach to estimate
Tt(st|stâˆ’1  atâˆ’1)  rt(st  at) by enumerating all tuples of (stâˆ’1  atâˆ’1  st)  IS is the step-wise impor-
tance sampling method  WIS uses the step-wise weighted (self-normalized) importance sampling
method  SSD-IS denotes the method of importance sampling with stationary state distribution pro-
posed by [Liu et al.  2018a]3  and MIS is our proposed marginalized method. Note that our MIS

3Our implementation of SSD-IS for the discrete state case is described in Appendix D.3.

7

is different: we normalize the estimate (cid:98)dÏ€

also uses the trick of self-normalization to obtain better performance  but the MIS normalization
t to the probability simplex  whereas WIS normalizes the
importance weights. We provide further results by comparing doubly robust estimator  weighted
doubly robust estimator  and our estimators in Appendix D. We use logarithmic scales in all ï¬gures
and include 95% conï¬dence intervals as error bars from 128 runs. Our metric is the relative root
mean squared error (Relative-RMSE)  which is the ratio of RMSE and the true value vÏ€.

Time-invariant MDPs We ï¬rst test our methods on the
standard ModelWin and ModelFail models with time-
invariant MDPs  ï¬rst introduced by Thomas and Brunskill
[2016]. The ModelWin domain simulates a fully observ-
able MDP  depicted in Figure 1(a). On the other hand 
the ModelFail domain (Figure 1(b)) simulates a partially
observable MDP  where the agent can only tell the differ-
ence between s1 and the â€œotherâ€ unobservable states. A
detailed description of these two domains can be found in
Appendix D. For both problems  the target policy Ï€ is to
always select a1 and a2 with probabilities 0.2 and 0.8  respectively  and the behavior policy Âµ is a
uniform policy.
We provide two types of experiments to show the properties of our marginalized approach. The ï¬rst
kind is with different numbers of episodes  where we use a ï¬xed horizon H = 50. The second kind
is with different horizons  where we use a ï¬xed number of episodes n = 1024. We use MIS only
with observable states and the partial trajectories between them. Details about applying MIS with
partial observability can be found in Appendix C. While this approach is general in more complex
applications  for ModelFail  the agent always visits s1 at every other step and we can simply replace
Ï€(a(i)
Âµ(a(i)

(a) ModelWin
(b) ModelFail
Figure 1: MDPs of OPE domains.

for t = 2Ï„ âˆ’ 1 in (3.2).

with Ï€(a(i)
Âµ(a(i)

Ï€(a(i)
Âµ(a(i)

2Ï„âˆ’1|s(i)
2Ï„âˆ’1|s(i)

2Ï„ |s(i)
2Ï„ |s(i)

2Ï„ =?)
2Ï„ =?)

2Ï„âˆ’1)
2Ï„âˆ’1)

|s(i)
t )
|s(i)
t )

t

t

(a) ModelWin with differ-
ent number of episodes n.
Figure 2: Results on Time-invariant MDPs. MIS matches DM on ModelWin and outperforms IS/WIS
on ModelFail  both of which are the best existing methods on their respective domains.

(c) ModelFail with differ-
ent number of episodes n.

(b) ModelWin with differ-
ent horizon H.

(d) ModelFail with differ-
ent horizon H.

Figure 2 shows the results in the time-invariant ModelWin MDP and ModelFail MDP. The results
clearly demonstrate that MIS maintains a polynomial dependence on H and matches the best
alternatives such as DM in Figure 2(b) and IS at the beginning of Figure 2(d). Notably  the IS
in Figure 2(d) reï¬‚ects a bias-variance trade-off  that its RMSE is smaller at short horizons due to
unbiasedness yet larger at long horizons due to high variance.

Time-varying  non-mixing MDPs with continuous actions. We also test our approach in simu-
lated MDP environments where the states are binary  the actions are continuous between [0 1] and
the state transition models are time-varying with a ï¬nite horizon H. The agent starts at State 1. At
every step  the environment samples a random parameter p âˆˆ [0.5/H  0.5 âˆ’ 0.5/H]. Any agent in
State 1 will transition to State 0 if and only if it samples an action between [p âˆ’ 0.5/H  p + 0.5/H].
On the other hand  State 0 is a sinking state. The agent collects rewards at State 0 in the latter half of
the steps (t â‰¥ H/2). Thus  the agent wants to transition to State 0  but the transition probability is
inversely proportional to the horizon H for uniform action policies. We pick the behavior policy to
be uniform on [0  1] and the target policy to be uniform on [0  0.5] with 95% total probability and 5%
chance uniformly distributed on [0.5  1].

8

S1S2S3!1!2p1-pr=1r=-1r=1r=-11-ppS1??!1!2r=1r=-1p1-p1-ppDMISWISSSD-ISMIS101102103Number of Episodes  n101100101102Relative RMSE101102103Horizon  H102101100Relative RMSE101102103Number of Episodes  n101100101Relative RMSE101102103Horizon  H102101100Relative RMSEFigure 3(a) shows the asymptotic convergence
rates of RMSE with respect to the number of
episodes  given ï¬xed horizon H = 64. MIS
âˆš
converges at a O(1/
n) rate from the very be-
ginning. In comparison  neither IS or MIS has
entered their asymptotic nâˆ’1/2 regime yet with
n â‰¤ 4  096. SSD-IS does not improve as n gets
larger  because the stationary state distribution (a
point mass on State 0) is not a good approxima-
tion of the average probability of visiting State
0 for t âˆˆ [H/2  H]. We exclude DM because it
requires additional model assumptions to apply to continuous action spaces.
âˆš
Figure 3(b) shows the Relative RMSE dependency in H  ï¬xing the number of episodes n = 1024. We
see that as H gets larger  the Relative RMSE scales as O(
H) for MIS and stays roughly constant
for SSD-IS. Since the true reward vÏ€ âˆ H  the result matches the worst-case bound of a O(H 3)
MSE in Corollary 1. SSD-IS saves a factor of H in variance  as it marginalizes over the H steps  but
introduces a large bias as we have seen in Figure 3(a). IS and WIS worked better for small H  but
quickly deteriorates as H increases. Together with Figure 3(a)  we may conclude that In conclusion 
MIS is the only method  among the alternatives in this example  that produces a consistent estimator
with low variance.

Figure 3: Time-varying MDPs

(a) Dependency on n

(b) Dependency on H

Mountain Car. Finally  we benchmark our estimator on the
Mountain Car domain [Singh and Sutton  1996]  where an
under-powered car drives up a steep valley by â€œswingingâ€ on
both sides to gradually build up potential energy. To construct
the stochastic behavior policy Âµ and stochastic evaluated policy
Ï€  we ï¬rst compute the optimal Q-function using Q-learning
and use its softmax policy of the optimal Q-function as eval-
uated policy Ï€ (with the temperature of 1). For the behavior
policy Âµ  we also use the softmax policy of the optimal Q-
function but set the temperature to 1.25. Note that this is a
ï¬nite-horizon MDP with continuous state. We apply MIS by
discretizing the state space as in [Jiang and Li  2016].
The results  shown in Figure 4  demonstrate the effectiveness of
our approach in a common benchmark control task  where the
ability to evaluate under long horizons is required for success.
Note that Mountain Car is an episodic environment with a absorbing state  so it is not a setting
that SSD-IS is designed for. We include the the detailed description on the experimental setup and
discussion on the results in Appendix D.

Figure 4: Mountain Car with differ-
ent number of episodes.

6 Conclusions

In this paper  we propose a marginalized importance sampling (MIS) method for the problem of
off-policy evaluation in reinforcement learning. Our approach gets rid of the burden of horizon
by using an estimated marginal state distribution of the target policy at every step instead of the
cumulative product of importance weights.
Comparing to the pioneering work of Liu et al. [2018a] that uses a similar philosophy  this paper
focuses on the ï¬nite state episodic setting with an potentially inï¬nite action space. We proved the
ï¬rst ï¬nite sample error bound for such estimators with polynomial dependence in all parameters. The
error bound is tight in that it matches the asymptotic variance of a ï¬ctitious estimator that has access
to oracle information up to a low-order additive factor. Moreover  it is within a factor of O(H) of the
Cramer-Rao lower bound of this problem in [Jiang and Li  2016]. We conjecture that this additional
factor of H is required for any estimators in the inï¬nite action setting.
Our experiments demonstrate that the MIS estimator is effective in practice as it achieves substantially
better performance than existing approaches in a number of benchmarks.

9

ISWISMISSSD-IS102103Number of Episodes  n102101100Relative RMSE101102103Horizon  H102101100Relative RMSEDMISWISSSD-ISMIS:2-0741584/08n#0 9;0#$Acknowledgement

The authors thank Yu Bai  Murali Narayanaswamy  Lin F. Yang  Nan Jiang  Phil Thomas  Ying Yang
for helpful discussion and Amazon internal review committee for the feedback on an early version of
the paper. We also acknowledge the NeurIPS area chair  anonymous reviewers for helpful comments
and Ming Yin for carefully proofreading the paper.
YW was supported by a start-up grant from UCSB CS department  NSF-OAC 1934641 and a gift
from AWS ML Research Award.

References
Bottou  L.  Peters  J.  QuiÃ±onero-Candela  J.  Charles  D. X.  Chickering  D. M.  Portugaly  E.  Ray  D. 
Simard  P.  and Snelson  E. (2013). Counterfactual reasoning and learning systems: The example
of computational advertising. The Journal of Machine Learning Research  14(1):3207â€“3260.

Chapelle  O.  Manavoglu  E.  and Rosales  R. (2015). Simple and scalable response prediction for

display advertising. ACM Transactions on Intelligent Systems and Technology (TIST)  5(4):61.

Chernoff  H. et al. (1952). A measure of asymptotic efï¬ciency for tests of a hypothesis based on the

sum of observations. The Annals of Mathematical Statistics  23(4):493â€“507.

DudÃ­k  M.  Langford  J.  and Li  L. (2011). Doubly robust policy evaluation and learning.

International Conference on Machine Learning  pages 1097â€“1104. Omnipress.

In

Ernst  D.  Stan  G.-B.  Goncalves  J.  and Wehenkel  L. (2006). Clinical data based optimal sti
strategies for hiv: a reinforcement learning approach. In Decision and Control  2006 45th IEEE
Conference on  pages 667â€“672. IEEE.

Farajtabar  M.  Chow  Y.  and Ghavamzadeh  M. (2018). More robust doubly robust off-policy
evaluation. In International Conference on Machine Learning (ICML-18)  volume 80  pages
1447â€“1456  StockholmsmÃ¤ssan  Stockholm Sweden. PMLR.

Gelada  C. and Bellemare  M. G. (2019). Off-policy deep reinforcement learning by bootstrapping
the covariate shift. In AAAI Conference on Artiï¬cial Intelligence (AAAI-19)  volume 33  pages
3647â€“3655.

Gottesman  O.  Liu  Y.  Sussex  S.  Brunskill  E.  and Doshi-Velez  F. (2019). Combining parametric
and nonparametric models for off-policy evaluation. In International Conference on Machine
Learning (ICML-19).

Guo  Z.  Thomas  P. S.  and Brunskill  E. (2017). Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems
(NIPS-17)  pages 2492â€“2501.

Hallak  A. and Mannor  S. (2017). Consistent on-line off-policy evaluation.
Conference on Machine Learning (ICML-17)  pages 1372â€“1383. JMLR. org.

In International

Hirano  K.  Imbens  G. W.  and Ridder  G. (2003). Efï¬cient estimation of average treatment effects

using the estimated propensity score. Econometrica  71(4):1161â€“1189.

Jiang  N. and Li  L. (2016). Doubly robust off-policy value evaluation for reinforcement learning. In

International Conference on Machine Learning (ICML-16)  pages 652â€“661. JMLR. org.

Li  L.  Munos  R.  and Szepesvari  C. (2015). Toward minimax off-policy value estimation. In

Artiï¬cial Intelligence and Statistics (AISTATS-15)  pages 608â€“616.

Liu  Q.  Li  L.  Tang  Z.  and Zhou  D. (2018a). Breaking the curse of horizon: Inï¬nite-horizon
off-policy estimation. In Advances in Neural Information Processing Systems (NeurIPS-18)  pages
5361â€“5371.

Liu  Y.  Gottesman  O.  Raghu  A.  Komorowski  M.  Faisal  A. A.  Doshi-Velez  F.  and Brunskill  E.
(2018b). Representation balancing mdps for off-policy policy evaluation. In Advances in Neural
Information Processing Systems (NeurIPS-18)  pages 2649â€“2658.

10

Mandel  T.  Liu  Y.-E.  Levine  S.  Brunskill  E.  and Popovic  Z. (2014). Ofï¬‚ine policy evaluation
across representations with applications to educational games. In International conference on
Autonomous agents and multi-agent systems  pages 1077â€“1084. International Foundation for
Autonomous Agents and Multiagent Systems.

Murphy  S. A.  van der Laan  M. J.  Robins  J. M.  and Group  C. P. P. R. (2001). Marginal mean
models for dynamic regimes. Journal of the American Statistical Association  96(456):1410â€“1423.

Precup  D.  Sutton  R. S.  and Singh  S. P. (2000). Eligibility traces for off-policy policy evaluation.
In International Conference on Machine Learning (ICML-00)  pages 759â€“766. Morgan Kaufmann
Publishers Inc.

Raghu  A.  Komorowski  M.  Celi  L. A.  Szolovits  P.  and Ghassemi  M. (2017). Continuous
state-space models for optimal sepsis treatment: a deep reinforcement learning approach. In
Machine Learning for Healthcare Conference  pages 147â€“163.

Singh  S. P. and Sutton  R. S. (1996). Reinforcement learning with replacing eligibility traces.

Machine learning  22(1-3):123â€“158.

Sobel  M. J. (1982). The variance of discounted markov decision processes. Journal of Applied

Probability  19(4):794â€“802.

Sutton  R. S. and Barto  A. G. (1998). Reinforcement learning: An introduction  volume 1. MIT

press Cambridge.

Tang  L.  Rosales  R.  Singh  A.  and Agarwal  D. (2013). Automatic ad format selection via
contextual bandits. In ACM International Conference on Information & Knowledge Management
(CIKM-13)  pages 1587â€“1594. ACM.

Theocharous  G.  Thomas  P. S.  and Ghavamzadeh  M. (2015). Personalized ad recommendation
systems for life-time value optimization with guarantees. In International Joint Conferences on
Artiï¬cial Intelligence (IJCAI-15)  pages 1806â€“1812.

Thomas  P. and Brunskill  E. (2016). Data-efï¬cient off-policy policy evaluation for reinforcement

learning. In International Conference on Machine Learning (ICML-16)  pages 2139â€“2148.

Thomas  P. S. (2015). Safe reinforcement learning. PhD thesis  University of Massachusetts Amherst.

Thomas  P. S.  Theocharous  G.  and Ghavamzadeh  M. (2015). High-conï¬dence off-policy evaluation.

In AAAI Conference on Artiï¬cial Intelligence (AAAI-15)  pages 3000â€“3006.

Thomas  P. S.  Theocharous  G.  Ghavamzadeh  M.  Durugkar  I.  and Brunskill  E. (2017). Predictive
off-policy policy evaluation for nonstationary decision problems  with applications to digital
marketing. In AAAI Conference on Artiï¬cial Intelligence (AAAI-17)  pages 4740â€“4745.

Wang  Y.-X.  Agarwal  A.  and DudÄ±k  M. (2017). Optimal and adaptive off-policy evaluation in
contextual bandits. In International Conference on Machine Learning (ICML-17)  pages 3589â€“
3597.

11

,Dandan Guo
Bo Chen
Hao Zhang
Mingyuan Zhou
Tengyang Xie
Yifei Ma
Yu-Xiang Wang