2019,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling,Motivated by the many real-world applications of reinforcement learning (RL) that require safe-policy iterations  we consider the problem of off-policy evaluation (OPE) --- the problem of  evaluating a new policy using the historical data obtained by different behavior policies --- under the model of nonstationary episodic Markov Decision Processes (MDP) with a long horizon and a large action space. Existing importance sampling (IS) methods often suffer from large variance that depends exponentially on the RL horizon $H$. To solve this problem  we consider a marginalized importance sampling (MIS) estimator that recursively estimates the state marginal distribution for the target policy at every step. 
MIS achieves a mean-squared error of 
$$
\frac{1}{n} \sum_{t=1}^H\mathbb{E}_{\mu}\left[\frac{d_t^\pi(s_t)^2}{d_t^\mu(s_t)^2} \Var_{\mu}\left[\frac{\pi_t(a_t|s_t)}{\mu_t(a_t|s_t)}\big( V_{t+1}^\pi(s_{t+1}) + r_t\big) \middle| s_t\right]\right]   + \tilde{O}(n^{-1.5})
$$
where $\mu$ and $\pi$ are the logging and target policies  $d_t^{\mu}(s_t)$ and $d_t^{\pi}(s_t)$ are the marginal distribution of the state at $t$th step  $H$ is the horizon  $n$ is the sample size and $V_{t+1}^\pi$ is the value function of the MDP under $\pi$. The result matches the Cramer-Rao lower bound in [Jiang and Li  2016] up to a multiplicative factor of $H$. To the best of our knowledge  this is the first OPE estimation error bound with a polynomial dependence on $H$. Besides theory  we show empirical superiority of our method in time-varying  partially observable  and long-horizon RL environments.,Towards Optimal Off-Policy Evaluation for
Reinforcement Learning with Marginalized

Importance Sampling

Tengyang Xie∗

Dept. of Computer Science

UIUC

Urbana  IL 61801

tx10@illinois.edu

Yifei Ma

AWS AI Labs

Amazon.com Services  Inc.
East Palo Alto  CA 94303

yifeim@amazon.com

Abstract

Yu-Xiang Wang

Dept. of Computer Science 

UC Santa Barbara

Santa Barbara  CA 93106
yuxiangw@cs.ucsb.edu

Motivated by the many real-world applications of reinforcement learning (RL) that
require safe-policy iterations  we consider the problem of off-policy evaluation
(OPE) — the problem of evaluating a new policy using the historical data ob-
tained by different behavior policies — under the model of nonstationary episodic
Markov Decision Processes (MDP) with a long horizon and a large action space.
Existing importance sampling (IS) methods often suffer from large variance that
depends exponentially on the RL horizon H. To solve this problem  we consider
a marginalized importance sampling (MIS) estimator that recursively estimates
the state marginal distribution for the target policy at every step. MIS achieves a
mean-squared error of

Eµ

t=1

t (st)2
t (st)2 Varµ
dµ

t+1(st+1) + rt

+ ˜O(n−1.5)

(cid:20) dπ

(cid:88)H

1
n

(cid:20) πt(at|st)

µt(at|st)

(cid:0)V π

(cid:21)(cid:21)

(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)st

where µ and π are the logging and target policies  dµ
t (st) are the
marginal distribution of the state at tth step  H is the horizon  n is the sample
size and V π
t+1 is the value function of the MDP under π. The result matches the
Cramer-Rao lower bound in Jiang and Li [2016] up to a multiplicative factor of H.
To the best of our knowledge  this is the ﬁrst OPE estimation error bound with a
polynomial dependence on H. Besides theory  we show empirical superiority of our
method in time-varying  partially observable  and long-horizon RL environments.

t (st) and dπ

1

Introduction

The problem of off-policy evaluation (OPE)  which predicts the performance of a policy with data only
sampled by a behavior policy [Sutton and Barto  1998]  is crucial for using reinforcement learning
(RL) algorithms responsibly in many real-world applications. In many settings where RL algorithms
have already been deployed  e.g.  targeted advertising and marketing [Bottou et al.  2013; Tang et al. 
2013; Chapelle et al.  2015; Theocharous et al.  2015; Thomas et al.  2017] or medical treatments
[Murphy et al.  2001; Ernst et al.  2006; Raghu et al.  2017]  online policy evaluation is usually
expensive  risky  or even unethical. Also  using a bad policy in these applications is dangerous and
could lead to severe consequences. Solving OPE is often the starting point in many RL applications.
To tackle the problem of OPE  the idea of importance sampling (IS) corrects the mismatch in the
distributions under the behavior policy and target policy. It also provides typically unbiased or
∗The research was partially conducted when TX was visiting YW and YM at Amazon AWS AI Labs during

his internship in Summer 2018.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

strongly consistent estimators [Precup et al.  2000]. IS-based off-policy evaluation methods have
also seen lots of interest recently especially for short-horizon problems  including contextual bandits
[Murphy et al.  2001; Hirano et al.  2003; Dudík et al.  2011; Wang et al.  2017]. However  the
variance of IS-based approaches [Precup et al.  2000; Thomas et al.  2015; Jiang and Li  2016;
Thomas and Brunskill  2016; Guo et al.  2017; Farajtabar et al.  2018] tends to be too high to provide
informative results  for long-horizon problems [Mandel et al.  2014]  since the variance of the product
of importance weights may grow exponentially as the horizon goes long. There are also model-based
approaches for solving OPE problems [Liu et al.  2018b; Gottesman et al.  2019]  where the value of
the target policy is estimated directly using the approximated MDP.
Given this high-variance issue  it is necessary to ﬁnd an IS-based approach without relying heavily
on the cumulative product of importance weights from the whole trajectories. While the beneﬁt of
cumulative products is to allow unbiased estimation even without any state observability assumptions 
reweighing the entire trajectories may not be necessary if some intermediate states are directly
observable. For the latter  based on Markov independence assumptions  we can aggregate all
trajectories that share the same state transition patterns to directly estimate the state distribution shifts
after the change of policies from the behavioral to the target. We call this approach marginalized
importance sampling (MIS)  because it computes the marginal state distribution shifts at every single
step  instead of the product of policy weights.
Related work [Liu et al.  2018a] tackles the high variance issue due to the cumulative product of
importance weights. They apply importance sampling on the average visitation distribution of state-
action pairs  based on an estimation of the mixed state distribution. Hallak and Mannor [2017] and
Gelada and Bellemare [2019] also leverage the same fact in time-invariant MDPs  where they use the
stationary ratio of state-action pairs to replace the trajectory weights. However  these methods may
not directly work in ﬁnite-horizon MDPs  where the state distributions may not mix.
In contrast to the prior work  the ﬁrst goal of our paper is to study the sample complexity and
optimality of the marginalized approach. Speciﬁcally  we provide the ﬁrst ﬁnite sample error bound
on the mean-square error for our MIS off-policy evaluation estimator under the episodic tabular MDP
setting (with potentially continuous action space). Our MSE bound is the exact calculation up to low
order terms. Comparing to the Cramer-Rao lower bound established in [Jiang and Li  2016  Theorem
3] for DAG-MDP  our bound is larger by at most a factor of H and we have good reasons to believe
that this additional factor is required for any OPE estimators in this setting.
In addition to the theoretical results  we empirically evaluate our estimator against a number of strong
baselines from prior work in a number of time-invariant/time-varying  fully observable/partially
observable  and long-horizon environments. Our approach can also be used in most of OPE estimators
that leverage IS-based estimators  such as doubly robust [Jiang and Li  2016]  MAGIC [Thomas and
Brunskill  2016]  MRDR [Farajtabar et al.  2018] under mild assumptions (Markov assumption).
Here is a road map for the rest of the paper. Section 2 provides the preliminaries of the problem of
off-policy evaluation. In Section 3  we offer the design of our marginalized estimator  and we study
its information-theoretical optimality in Section 4. We present the empirical results in a number of
RL tasks in Section 5. At last  Section 6 concludes the paper.

2 Problem formulation

Symbols and notations. We consider the problem of off-policy evaluation for a ﬁnite horizon 
nonstationary  episodic MDP  which is a tuple deﬁned by M = (S A  T  r  H)  where S is the state
space  A is the action space  Tt : S×A×S → [0  1] is the transition function with Tt(s(cid:48)|s  a) deﬁned
by probability of achieving state s(cid:48) after taking action a in state s at time t  and rt : S × A × S → R
is the expected reward function with rt(s  a  s(cid:48)) deﬁned by the mean of immediate received reward
after taking action a in state s and transitioning into s(cid:48)  and H denotes the ﬁnite horizon. We use P[E]
to denote the probability of an event E and p(x) the p.m.f. (or pdf) of the random variable X taking
value x. E[·] and E[·|E] denotes the expectation and conditional expectation given E  respectively.
Let µ  π : S → PA be policies which output a distribution of actions given an observed state. We
call µ the behavioral policy and π the target policy. For notation convenience we denote µ(at|st)
and π(at|st) the p.m.f of actions given state at time t. The expectation operators in this paper will
either be indexed with π or µ  which denotes that all random variables coming from roll-outs from

2

at−1

t (st) and dπ

the speciﬁed policy. Moreover  we denote dµ
t (st) the induced state distribution at time t.
When t = 1  the initial distributions are identical dµ
t (st) are
functions of not just the policies themselves but also the unknown underlying transition dynamics 
i.e.  for π (and similarly µ)  recursively deﬁne
t (st|st−1)dπ
(cid:88)
P π
t (st|st−1) =

Tt(st|st−1  at−1)π(at−1|st−1).

1 = d1. For t > 1  dµ

t (st) and dπ

t−1(st−1) 

(cid:88)

dπ
t (st) =

where P π

1 = dπ

(2.1)

st−1

t+1 t(s(cid:48)|s) = (cid:80)

t

  a(i)
t

  a(i)
t

i j ∈ RS×S ∀j < i as the state-transition probability from step j to step i un-
a Pt+1 t(s(cid:48)|s  a)πt(a|s) =

We denote P π
der a sequence of actions taken by π. Note that P π
Tt+1(s(cid:48)|s  πt(s)).
t ) ∈ S × A × R for time index
Behavior policy µ is used to collect data in the form of (s(i)
  r(i)
t
t = 1  . . .   H and episode index i = 1  ...  n. Target policy π is what we are interested to evaluate.
Also  let D to denote the historical data  which contains n episode trajectories in total. We also deﬁne
Dh = {(s(i)
Throughout the paper  probability distributions are often used in their vector or matrix form. For
t without an input is interpreted as a vector in a S-dimensional probability simplex and
instance  dπ
t .
i j is then a stochastic transition matrix. This allows us to write (2.1) concisely as dπ
P π
t+1 tdπ
Also note that while st  at  rt are usually used to denote ﬁxed elements in set S A and R  in
some cases we also overload them to denote generic random variables s(1)
. For exam-
ple  Eπ[rt] = Eπ[r(1)
dπ(st  at  st+1)rt(st  at  st+1) and Varπ[rt(st  at  st+1)] =
Varπ[rt(s(1)
  s(1)

t ) : i ∈ [n]  t ≤ h} to be roll-in realization of n trajectories up to step h.
  r(i)

t
t+1)]. The distinctions will be clear in each context.

Problem setup. The problem of off-policy evaluation is about ﬁnding an estimator(cid:98)vπ : (S × A ×

R)H×n → R that makes use of the data collected by running µ to estimate

] = (cid:80)

t+1 = P π

st at st+1

  a(1)

  a(1)

  r(1)

t

t

t

t

t

H(cid:88)

(cid:88)

(cid:88)

(cid:88)

π(at|st)

Tt(st+1|st  at)rt(st  at  st+1) 

st

at

t=1

st+1

vπ =

dπ
t (st)

(2.2)
where we assume knowledge about µ(a|s) and π(a|s) for all (s  a) ∈ S × A  but do not observe
rt(st  at  st+1) for any actions other than a noisy version of it the evaluated actions. Nor do we
t (st)∀t > 1 implied by the change of policies. Nonetheless  our goal
observe the state distributions dπ
is to ﬁnd an estimator to minimize the mean-square error (MSE): MSE(π  µ  M ) = Eµ[(ˆvπ − vπ)2] 
using the observed data and the known action probabilities. Different from previous studies  we focus
on the case where S is sufﬁciently small but S2A is too large for a reasonable sample size. In other
words  this is a setting where we do not have enough data points to estimate the state-action-state
transition dynamics  but we do observe the states and can estimate the distribution of the states after
the change of policies  which is our main strategy.

Assumptions: We list the technical assumptions we need and provide necessary justiﬁcation.

A1. ∃Rmax  σ < +∞ such that 0 ≤ E[rt|st  at  st+1] ≤ Rmax  Var[rt|st  at  st+1] ≤ σ2 for

all t  st  at.

A2. Behavior policy µ obeys that dm := mint st dµ
A3. Bounded weights: τs := maxt st

t (st) > 0 ∀t  st such that dπ
t (st) > 0.
π(at|st)
µ(at|st) < +∞.

t (st) < +∞ and τa := maxt st at

dπ
t (st)
dµ

Assumption A1 is assumed without loss of generality. The σ bound is required even for on-policy
evaluation and the assumption on the non-negativity and Rmax can always be obtained by shifting and
rescaling the problem. Assumption A2 is necessary for any consistent off-policy evaluation estimator.
Assumption A3 is also necessary for discrete state and actions  as otherwise the second moments of
the importance weight would be unbounded. For continuous actions  τa < +∞ is stronger than we
need and should be considered a simplifying assumption for the clarity of our presentation. Finally 
we comment that the dependence in the parameter dm  τs  τa do not occur in the leading O(1/n)
term of our MSE bound  but only in simpliﬁed results after relaxation.

3

3 Marginalized Importance Sampling Estimators for OPE

In this section  we present the design of marginalized IS estimators for OPE. For small action spaces 
we may directly build models by the estimated transition function Tt(st|st−1  at−1) and the reward
function rt(st  at  st+1) from empirical data. However  the models may be inaccurate in large action
spaces  where not all actions are frequently visited. Function approximation in the models may cause
additional biases from covariate shifts due to the change of policies. Standard importance sampling
estimators (including the doubly robust versions)[Dudík et al.  2011; Jiang and Li  2016] avoid the
need to estimate the model’s dynamics but rather directly approximating the expected reward:

n(cid:88)

H(cid:88)

(cid:34) h(cid:89)

i=1

h=1

t=1

(cid:98)vπ

IS =

1
n

(cid:35)

π(a(i)
t
µ(a(i)
t

|s(i)
t )
|s(i)
t )

r(i)
h .

To adjust for the differences in the policy  importance weights are used and it can be shown that this
is an unbiased estimator of vπ (See more detailed discussion of IS and the doubly robust version
in Appendix C). The main issue of this approach  when applying to the episodic MDP with large
action space is that the variance of the importance weights grows exponentially in H [Liu et al. 
2018a]  which makes the sample complexity exponentially worse than the model-based approaches 
when they are applicable. We address this problem by proposing an alternative way of estimating
the importance weights which achieves the same sample complexity as the model-based approaches
while allowing us to achieve the same ﬂexibility and interpretability as the IS estimator that does not
explicitly require estimating the state-action dynamics Tt. We propose the Marginalized Importance
Sampling (MIS) estimator:

H(cid:88)

t=1

(cid:98)dπ
(cid:98)dµ

n(cid:88)
(cid:80)

i=1

1
n

MIS =

t (s(i)
t ).

t (s(i)
t )
t (s(i)
t )

(cid:98)rπ
(cid:98)vπ
t   (cid:98)dµ → dµ
Clearly  if (cid:98)dπ → dπ
t → Eπ[Rt(st  at)|st]  then(cid:98)vπ
t  (cid:98)rπ
It turns out that if we take (cid:98)dµ
t (st) = 0 whenever nst = 0  then (3.1) is equivalent to(cid:80)H
t (st)/(cid:98)dµ
(cid:98)dπ
direct plug-in estimator of (2.2). It remains to specify (cid:98)dπ
(cid:98)dπ
t (cid:98)dπ
t = (cid:98)P π
t−1  where (cid:98)P π
n(cid:88)
and(cid:98)rπ

t−1|st−1)
t−1|st−1)

t (st|st−1) =

recursively using

π(a(i)
µ(a(i)

t (st) := 1
n

n(cid:88)

t 1(s(i)
r(i)

t = st) 

MIS → vπ.

i 1(s(i)

t (st) =

nst−1

i=1

1

1
nst

i=1

π(a(i)
t
µ(a(i)
t

|st)
|st)

t = st) — the empirical mean — and deﬁne

(cid:80)
st (cid:98)dπ
t (st)(cid:98)rπ(st) – the
t (st) and (cid:98)rπ(st). (cid:98)dπ

t (st) is estimated

t=1

1((s(i)

t−1  s(i)

t ) = (st−1  st));

(3.1)

(3.2)

where nsτ is the empirical visitation frequency to state sτ at time τ. Note that our estimator of rπ
t (st)
is the standard IS estimators we use in bandits [Li et al.  2015]  which are shown to be optimal (up to
a universal constant) when A is large [Wang et al.  2017].
The advantage of MIS over the naive IS estimator is that the variance of the importance weight need
not depend exponentially in H. A major theoretical contribution of this paper is to formalize this
argument by characterizing the dependence on π  µ as well as parameters of the MDP M. Note that
MIS estimator does not dominate the IS estimator. In the more general setting when the state is given
by the entire history of observations  Jiang and Li [2016] establishes that no estimators can achieve
polynomial dependence in H. We give a concrete example later (Example 1) about how IS estimator
suffers from the “curse of horizon” [Liu et al.  2018a]. MIS estimator can be thought of as one that
exploits the state-observability while retaining properties of the IS estimators to tackle the problem of
large action space. As we illustrate in the experiments  MIS estimator can be modiﬁed to naturally
handle partially observed states  e.g.  when s is only observed every other step.
Finally  when available  model-based approaches can be combined into importance-weighted methods
[Jiang and Li  2016; Thomas and Brunskill  2016]. We defer discussions about these extensions in
Appendix C to stay focused on the scenarios where model-based approaches are not applicable.

4

4 Theoretical Analysis of the MIS Estimator

Motivated by the challenge of curse of horizon with naive IS estimators  similar to [Liu et al.  2018a] 
we show that the sample complexity of our MIS estimator reduces to O(H 3). To the best of our
knowledge  this is ﬁrst sample complexity guarantee under this setting  which also matches the
Cramer-Rao lower bound for DAG-MDP [Jiang and Li  2016] as n → ∞ up to a factor of H.
Example 1 (Curse of horizon). Assume a MDP with i.i.d. state transition models over time and
assume that πt
is bounded from both sides for all t. Suppose the reward is a constant 1 only
µt

shown at the last step  such that naive IS becomes(cid:98)vπ
trajectory  (cid:81)H
Central Limit Theorem (cid:80)H
(cid:0)−HElog  HVlog
asymptotically follows LogNormal(cid:0)−HElog  HVlog
(cid:1) 
whose variance is exponential in horizon:(cid:0)exp (HVlog) − 1(cid:1). On the other hand  MIS estimates the

]. By
asymptotically follows a normal distribution with parameters

(cid:104)(cid:80)H
(cid:1). In other words (cid:81)H

] and Vlog = Var[log πt
µt

; let Elog = E[log πt

(cid:20)(cid:81)H

state distributions recursively  yielding variance that is polynomial in horizon and small OPE errors.

t=1 log πt
µt

t=1 log πt
µt

(cid:80)n

. For every

|s(i)
t )
|s(i)
t )

IS = 1
n

π(a(i)
µ(a(i)

= exp

(cid:21)

(cid:105)

πt
µt

πt
µt

t=1

t=1

t=1

i=1

µt

t

t

We now formalize the sample complexity bound in Theorem 4.1.
Theorem 4.1. Let the value function under π be deﬁned as follows:

(cid:34) H(cid:88)

t=h

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s(1)

h (sh) := Eπ
V π

rt(s(1)

t

  a(1)

t

  s(1)

t+1)

h = sh

∈ [0  Vmax]  ∀h ∈ {1  2  ...  H}.

For the simplicity of the statement  deﬁne boundary conditions: r0(s0) ≡ 0  σ0(s0  a0) ≡ 0  dπ
1  π(a0|s0)
the number of episodes n obeys that

H+1 ≡ 0. Moreover  let τa := maxt st at

π(at|st)
µ(at|st) and τs := maxt st

µ(a0|s0) ≡ 1 and V π

0 (s0) ≡
0 (s0)
dµ
dπ
t (st)
t (st) . If
dµ

n > max

(cid:26) 16 log n
for all t = 2  ...  H  then the our estimator(cid:98)vπ
(cid:88)
H(cid:88)
E[(P(cid:98)vπ
(cid:115)
(cid:32)

MIS − vπ)2] ≤ 1
n

h(sh)2
dπ
dµ
h(sh)

mint st dµ

h=0

sh

 

t (st)

Varµ

4tτaτs
mint st max{dπ
(cid:34)

π(a(1)
µ(a(1)

(cid:33)

h |sh)
h |sh)
19τ 2

+

(cid:27)

t (st)  dµ

t (st)}

MIS with an additional clipping step obeys that

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s(1)

(V π

h+1(s(1)

h+1) + r(1)
h )

h = sh

·

1 +

16 log n
n mint st dµ

t (st)

a τ 2

s SH 2(σ2 + R2

max + V 2

max)

.

n2

Corollary 1. In the familiar setting when Vmax = HRmax  then the same conditions in Theorem 4.1
implies that:

E[(P(cid:98)vπ

MIS − vπ)2] ≤ 4
n

τaτs(Hσ2 + H 3R2

max).

lates the MSE of a clipped version of our estimator(cid:98)vMIS

We make a few remarks about the results in Theorem 4.1.
Dependence on S  A and the weights. The leading term in the variance bound very precisely calcu-
1 modulo a (1 + O(n−1/2)) multiplicative
factor and an O(1/n2) additive factor. Speciﬁcally  our bound does not explicitly depend on S and A
but instead on how similar π and µ are. This allows the method to handle the case when the action
space is continuous. The dependence on τa  τs only appear in the low-order terms  while the leading
term depends only on the second moments of the importance weights.
Dependence on H. In general  our sample complexity upper bound is proportional to H 3  as
Corollary 1 indicates. Our bound reveals that in several cases it is possible to achieve a smaller

1The clipping step to [0  HRmax] or [0  Vmax] should not be alarming. It is required only for technical
reasons  and the clipped estimator is a valid estimator to begin with. Since the true policy value must be within
the range  the clipping step is only going to improve the MSE.

5

exponent on H for speciﬁc triplets of (M  π  µ). For instance  when π ≈ µ  such that τa  τs =
1 + O(1/H)  the variance bound gives O((V 2
max + Hσ2)/n)  which
matches the MSE bound (up to a constant) of the simple-averaging estimator that knows π = µ
a-priori. (See Remark 3 in the Appendix for more details). If Vmax is a constant that does not depend
on H (this is often the case in games when there is a ﬁxed reward at the end)  then the sample
complexity is only O(H).
Optimality. Comparing to the Cramer-Rao lower bound of the Theorem 3 in [Jiang and Li  2016] 
which we paraphrase below

max + Hσ2)/n) or O((H 2R2

the MSE of our estimator is asymptotically bigger by an additive factor of

(cid:12)(cid:12)(cid:12)s(1)

(cid:105)

(cid:104)

1
n

H(cid:88)

h=1

sh

ah

(cid:88)

dπ
h(sh)2
dµ
h(sh)

(cid:88)
(cid:88)
H(cid:88)
h(sh  ah) := E(cid:2)(V π

1
n

h=1

πh(ah|sh)2
µh(ah|sh)

h(sh)2
dπ
dµ
h(sh)

sh
h+1(s(1)

Var

h+1(s(1)
V π

h+1) + r(1)

h

h = sh  a(1)

h = ah

 

(4.1)

(cid:34)
h )(cid:12)(cid:12)s(1)

Varµ

πh(a(1)
µh(a(1)

h |sh)
h |sh)
h = sh  a(1)

Qπ

h(sh  a(1)
h )

(cid:35)
(cid:3) is the standard Q-function

(4.2)

 

h+1) + r(1)

h = ah

where Qπ
the MDP. The gap is signiﬁcant as the CR lower bound (4.1) itself only has a worst-case bound of
H 2τsτa/n 2  while (4.2) is proportional to H 3τsτa/n. This implies that our estimator is optimal up
to a factor of H. See Remark 4 for more details in the appendix.
It is an intriguing open question whether this additional factor of H can be removed. Our conjecture is
that the answer is negative and what we established in Theorem 4.1 matches the correct information-
theoretic limit for any methods in the cases when the action space A is continuous (or signiﬁcantly
larger than n). This conjecture is consistent with an existing lower bound in the simpler contextual
bandits setting  where Wang et al. [2017] established that a variance of expectation term analogous to
the one above cannot be removed  and no estimators can asymptotically attain the CR lower bound
for all problems in the large state/action space setting.

4.1 Proof Sketch

Recall that (3.1) is equivalent to(cid:80)H
sampling and(cid:98)dπ

In this section  we brieﬂy describe the main technical components in the proof of Theorem 4.1. More
detailed arguments are deferred to the full proof in Appendix B.

t (st) is recursively estimated using(cid:98)dπ

(cid:80)
st (cid:98)dπ
t (st)(cid:98)rπ(st)  where(cid:98)rπ(st) is estimated with importance
t−1(st−1) and the importance sampling estimator
t (st|st−1) under π. While the MIS estimator is easy to state  it is not

of the transition matrix P π
straightforward to analyze. We highlight three challenges below.

t=1

1. Dependent data and complex estimator: While the episodes are independent  the data within
each episode are not. Each time step of our MIS estimator uses the data from all episodes
up to that time step.

2. An annoying bias: There is a non-zero probability that some states st at time t is not visited
at all in all n episodes. This creates a bias in the estimator of ˆdπ
h for all time h > t. While
the probability of this happening is extremely small  conditioning on the high probability
event breaks the natural conditional independences  which makes it hard to analyze.

3. Error propagation: The recursive estimator ˆdπ

t is affected by all estimation errors in earlier
time steps. Naive calculation of the error with a constant slack in each step can lead to a
“snowball” effect that causes an exponential blow-up.

All these issues require delicate handling because otherwise the MSE calculation will not be tight.
Our solutions are as follows.
Deﬁning the appropriate ﬁltration. The ﬁrst observation is that we need to have a convenient
representation of the data. Instead of considering the n episodes as independent trajectories  it is
more useful to think of them all together as a Markov chain of multi-dimensional observations

2This is somewhat surprising as each of the H summands in the expression can be as large as H 2.

6

(cid:88)

st−1

˜dπ
t (st) =

˜P π(st|st−1) ˜dπ

t−1(st−1).

(cid:110)

(cid:111)n

s(i)
1:t  a(i)

of n state  action  reward triplets. Speciﬁcally  we deﬁne the “cumulative” data up to time t by
Datat :=
. Also  we observe that the state of the Markov chain at time t can
be summarized by nst — the number of times state st is visited.

Fictitious estimator technique. We address the bias issue by deﬁning a ﬁctitious estimator(cid:101)vπ. The

1:t−1  r(i)

1:t−1

i=1

t and ˆrπ

t   the ﬁctitious version of these estimators

ﬁctitious estimator is constructed by  instead of ˆdπ
˜dπ
t and ˜rπ

t is constructed recursively using

t   where ˜dπ

Varµ

π(a(1)
µ(a(1)

h |sh)
h |sh)

(V π

h+1(s(1)

h+1) + r(1)
h )

h = sh

.

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s(1)

The key difference is that whenever nst < Eµnst (1 − δ) for some 0 < δ < 1  we assign
˜P π(st+1|st) = P π(st+1|st) and ˜rπ(st) = Eπ[rt|st] — the true values of interest. This ensures that
the ﬁctitious estimator is always unbiased (see Lemma B.2). Note that this ﬁctitious estimator cannot
be implemented in practice. It is used as a purely theoretical construct that simpliﬁes the analysis of
the (biased) MIS estimator. In Lemma B.1  we show that the ˜vπ and ˆvπ are exponentially close to
each other.
Disentangling the dependency by backwards peeling. The ﬁctitious estimator technique reduces
the problem of calculating the MSE of the MIS estimator to a variance analysis of the ﬁctitious
estimator. By recursively applying the law of total variance backwards that peels one item at a time
from Datat  we establish an exact linear decomposition of the variance of the ﬁctitious estimator
(Lemma B.3):

Var[(cid:101)vπ] =

(cid:34)(cid:101)dπ

E

H(cid:88)

(cid:88)

h=0

sh

h(sh)2
nsh

1

(cid:18)
nsh ≥ ndµ

h(sh)
(1 − δ)−1

(cid:19)(cid:35)

(cid:34)

E

(cid:18)

(cid:34)(cid:101)dπ

Observe that the value function V π
t shows up naturally. This novel decomposition can be thought of
as a generalization of the celebrated Bellman-equation of variance [Sobel  1982] in the off-policy 
episodic MDP setting with a ﬁnite sample and can be of independent interest.
Characterizing the error propagation in ˜dπ
distribution estimation as follows

(cid:19)(cid:35)
which reduces the problem to bounding Var[(cid:101)dπ
at most linearly in h: Var[(cid:101)dπ
Cov((cid:101)dπ
h(sh)] ≤ 2(1−δ)−1hdπ
Finally  Theorem 4.1 is established by appropriately choosing δ = O((cid:112)log n/n mint st dµ

h(sh)]. We show (in Theorem B.1) that instead of an
exponential blow-up as will a concentration-inequality based argument imply  the variance increases
. The proof uses a novel decomposition of
h) (Lemma B.5)  which is derived using a similar backwards peeling argument as before.

h(sh). Lastly  we bound the error term in the state
≤ (1 − δ)−1

nsh ≥ ndµ

h(sh)
(1 − δ)−1

h(sh)2
dµ
h(sh)

(cid:18) dπ

h(sh)2
nsh

(cid:104)(cid:101)dπ

(cid:105)(cid:19)

h(sh)

+ Var

t (st)).

h(sh)

n

1

n

 

Due to space limits  we can only highlight a few key elements of the proof. We invite the readers to
check out a more detailed exposition in Appendix B.

5 Experiments

Throughout this section  we present the empirical results to illustrate the comparison among different
estimators. We demonstrate the effectiveness of our proposed marginalized estimator by comparing it
with different classic estimators on several domains.
The methods we compare in this section are: direct method (DM)  importance sampling (IS) 
weighted importance sampling (WIS)  importance sampling with stationary state distribution (SSD-
IS)  and marginalized importance sampling (MIS). DM uses the model-based approach to estimate
Tt(st|st−1  at−1)  rt(st  at) by enumerating all tuples of (st−1  at−1  st)  IS is the step-wise impor-
tance sampling method  WIS uses the step-wise weighted (self-normalized) importance sampling
method  SSD-IS denotes the method of importance sampling with stationary state distribution pro-
posed by [Liu et al.  2018a]3  and MIS is our proposed marginalized method. Note that our MIS

3Our implementation of SSD-IS for the discrete state case is described in Appendix D.3.

7

is different: we normalize the estimate (cid:98)dπ

also uses the trick of self-normalization to obtain better performance  but the MIS normalization
t to the probability simplex  whereas WIS normalizes the
importance weights. We provide further results by comparing doubly robust estimator  weighted
doubly robust estimator  and our estimators in Appendix D. We use logarithmic scales in all ﬁgures
and include 95% conﬁdence intervals as error bars from 128 runs. Our metric is the relative root
mean squared error (Relative-RMSE)  which is the ratio of RMSE and the true value vπ.

Time-invariant MDPs We ﬁrst test our methods on the
standard ModelWin and ModelFail models with time-
invariant MDPs  ﬁrst introduced by Thomas and Brunskill
[2016]. The ModelWin domain simulates a fully observ-
able MDP  depicted in Figure 1(a). On the other hand 
the ModelFail domain (Figure 1(b)) simulates a partially
observable MDP  where the agent can only tell the differ-
ence between s1 and the “other” unobservable states. A
detailed description of these two domains can be found in
Appendix D. For both problems  the target policy π is to
always select a1 and a2 with probabilities 0.2 and 0.8  respectively  and the behavior policy µ is a
uniform policy.
We provide two types of experiments to show the properties of our marginalized approach. The ﬁrst
kind is with different numbers of episodes  where we use a ﬁxed horizon H = 50. The second kind
is with different horizons  where we use a ﬁxed number of episodes n = 1024. We use MIS only
with observable states and the partial trajectories between them. Details about applying MIS with
partial observability can be found in Appendix C. While this approach is general in more complex
applications  for ModelFail  the agent always visits s1 at every other step and we can simply replace
π(a(i)
µ(a(i)

(a) ModelWin
(b) ModelFail
Figure 1: MDPs of OPE domains.

for t = 2τ − 1 in (3.2).

with π(a(i)
µ(a(i)

π(a(i)
µ(a(i)

2τ−1|s(i)
2τ−1|s(i)

2τ |s(i)
2τ |s(i)

2τ =?)
2τ =?)

2τ−1)
2τ−1)

|s(i)
t )
|s(i)
t )

t

t

(a) ModelWin with differ-
ent number of episodes n.
Figure 2: Results on Time-invariant MDPs. MIS matches DM on ModelWin and outperforms IS/WIS
on ModelFail  both of which are the best existing methods on their respective domains.

(c) ModelFail with differ-
ent number of episodes n.

(b) ModelWin with differ-
ent horizon H.

(d) ModelFail with differ-
ent horizon H.

Figure 2 shows the results in the time-invariant ModelWin MDP and ModelFail MDP. The results
clearly demonstrate that MIS maintains a polynomial dependence on H and matches the best
alternatives such as DM in Figure 2(b) and IS at the beginning of Figure 2(d). Notably  the IS
in Figure 2(d) reﬂects a bias-variance trade-off  that its RMSE is smaller at short horizons due to
unbiasedness yet larger at long horizons due to high variance.

Time-varying  non-mixing MDPs with continuous actions. We also test our approach in simu-
lated MDP environments where the states are binary  the actions are continuous between [0 1] and
the state transition models are time-varying with a ﬁnite horizon H. The agent starts at State 1. At
every step  the environment samples a random parameter p ∈ [0.5/H  0.5 − 0.5/H]. Any agent in
State 1 will transition to State 0 if and only if it samples an action between [p − 0.5/H  p + 0.5/H].
On the other hand  State 0 is a sinking state. The agent collects rewards at State 0 in the latter half of
the steps (t ≥ H/2). Thus  the agent wants to transition to State 0  but the transition probability is
inversely proportional to the horizon H for uniform action policies. We pick the behavior policy to
be uniform on [0  1] and the target policy to be uniform on [0  0.5] with 95% total probability and 5%
chance uniformly distributed on [0.5  1].

8

S1S2S3!1!2p1-pr=1r=-1r=1r=-11-ppS1??!1!2r=1r=-1p1-p1-ppDMISWISSSD-ISMIS101102103Number of Episodes  n101100101102Relative RMSE101102103Horizon  H102101100Relative RMSE101102103Number of Episodes  n101100101Relative RMSE101102103Horizon  H102101100Relative RMSEFigure 3(a) shows the asymptotic convergence
rates of RMSE with respect to the number of
episodes  given ﬁxed horizon H = 64. MIS
√
converges at a O(1/
n) rate from the very be-
ginning. In comparison  neither IS or MIS has
entered their asymptotic n−1/2 regime yet with
n ≤ 4  096. SSD-IS does not improve as n gets
larger  because the stationary state distribution (a
point mass on State 0) is not a good approxima-
tion of the average probability of visiting State
0 for t ∈ [H/2  H]. We exclude DM because it
requires additional model assumptions to apply to continuous action spaces.
√
Figure 3(b) shows the Relative RMSE dependency in H  ﬁxing the number of episodes n = 1024. We
see that as H gets larger  the Relative RMSE scales as O(
H) for MIS and stays roughly constant
for SSD-IS. Since the true reward vπ ∝ H  the result matches the worst-case bound of a O(H 3)
MSE in Corollary 1. SSD-IS saves a factor of H in variance  as it marginalizes over the H steps  but
introduces a large bias as we have seen in Figure 3(a). IS and WIS worked better for small H  but
quickly deteriorates as H increases. Together with Figure 3(a)  we may conclude that In conclusion 
MIS is the only method  among the alternatives in this example  that produces a consistent estimator
with low variance.

Figure 3: Time-varying MDPs

(a) Dependency on n

(b) Dependency on H

Mountain Car. Finally  we benchmark our estimator on the
Mountain Car domain [Singh and Sutton  1996]  where an
under-powered car drives up a steep valley by “swinging” on
both sides to gradually build up potential energy. To construct
the stochastic behavior policy µ and stochastic evaluated policy
π  we ﬁrst compute the optimal Q-function using Q-learning
and use its softmax policy of the optimal Q-function as eval-
uated policy π (with the temperature of 1). For the behavior
policy µ  we also use the softmax policy of the optimal Q-
function but set the temperature to 1.25. Note that this is a
ﬁnite-horizon MDP with continuous state. We apply MIS by
discretizing the state space as in [Jiang and Li  2016].
The results  shown in Figure 4  demonstrate the effectiveness of
our approach in a common benchmark control task  where the
ability to evaluate under long horizons is required for success.
Note that Mountain Car is an episodic environment with a absorbing state  so it is not a setting
that SSD-IS is designed for. We include the the detailed description on the experimental setup and
discussion on the results in Appendix D.

Figure 4: Mountain Car with differ-
ent number of episodes.

6 Conclusions

In this paper  we propose a marginalized importance sampling (MIS) method for the problem of
off-policy evaluation in reinforcement learning. Our approach gets rid of the burden of horizon
by using an estimated marginal state distribution of the target policy at every step instead of the
cumulative product of importance weights.
Comparing to the pioneering work of Liu et al. [2018a] that uses a similar philosophy  this paper
focuses on the ﬁnite state episodic setting with an potentially inﬁnite action space. We proved the
ﬁrst ﬁnite sample error bound for such estimators with polynomial dependence in all parameters. The
error bound is tight in that it matches the asymptotic variance of a ﬁctitious estimator that has access
to oracle information up to a low-order additive factor. Moreover  it is within a factor of O(H) of the
Cramer-Rao lower bound of this problem in [Jiang and Li  2016]. We conjecture that this additional
factor of H is required for any estimators in the inﬁnite action setting.
Our experiments demonstrate that the MIS estimator is effective in practice as it achieves substantially
better performance than existing approaches in a number of benchmarks.

9

ISWISMISSSD-IS102103Number of Episodes  n102101100Relative RMSE101102103Horizon  H102101100Relative RMSEDMISWISSSD-ISMIS:2-0741584/08n#0 9;0#$Acknowledgement

The authors thank Yu Bai  Murali Narayanaswamy  Lin F. Yang  Nan Jiang  Phil Thomas  Ying Yang
for helpful discussion and Amazon internal review committee for the feedback on an early version of
the paper. We also acknowledge the NeurIPS area chair  anonymous reviewers for helpful comments
and Ming Yin for carefully proofreading the paper.
YW was supported by a start-up grant from UCSB CS department  NSF-OAC 1934641 and a gift
from AWS ML Research Award.

References
Bottou  L.  Peters  J.  Quiñonero-Candela  J.  Charles  D. X.  Chickering  D. M.  Portugaly  E.  Ray  D. 
Simard  P.  and Snelson  E. (2013). Counterfactual reasoning and learning systems: The example
of computational advertising. The Journal of Machine Learning Research  14(1):3207–3260.

Chapelle  O.  Manavoglu  E.  and Rosales  R. (2015). Simple and scalable response prediction for

display advertising. ACM Transactions on Intelligent Systems and Technology (TIST)  5(4):61.

Chernoff  H. et al. (1952). A measure of asymptotic efﬁciency for tests of a hypothesis based on the

sum of observations. The Annals of Mathematical Statistics  23(4):493–507.

Dudík  M.  Langford  J.  and Li  L. (2011). Doubly robust policy evaluation and learning.

International Conference on Machine Learning  pages 1097–1104. Omnipress.

In

Ernst  D.  Stan  G.-B.  Goncalves  J.  and Wehenkel  L. (2006). Clinical data based optimal sti
strategies for hiv: a reinforcement learning approach. In Decision and Control  2006 45th IEEE
Conference on  pages 667–672. IEEE.

Farajtabar  M.  Chow  Y.  and Ghavamzadeh  M. (2018). More robust doubly robust off-policy
evaluation. In International Conference on Machine Learning (ICML-18)  volume 80  pages
1447–1456  Stockholmsmässan  Stockholm Sweden. PMLR.

Gelada  C. and Bellemare  M. G. (2019). Off-policy deep reinforcement learning by bootstrapping
the covariate shift. In AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  volume 33  pages
3647–3655.

Gottesman  O.  Liu  Y.  Sussex  S.  Brunskill  E.  and Doshi-Velez  F. (2019). Combining parametric
and nonparametric models for off-policy evaluation. In International Conference on Machine
Learning (ICML-19).

Guo  Z.  Thomas  P. S.  and Brunskill  E. (2017). Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems
(NIPS-17)  pages 2492–2501.

Hallak  A. and Mannor  S. (2017). Consistent on-line off-policy evaluation.
Conference on Machine Learning (ICML-17)  pages 1372–1383. JMLR. org.

In International

Hirano  K.  Imbens  G. W.  and Ridder  G. (2003). Efﬁcient estimation of average treatment effects

using the estimated propensity score. Econometrica  71(4):1161–1189.

Jiang  N. and Li  L. (2016). Doubly robust off-policy value evaluation for reinforcement learning. In

International Conference on Machine Learning (ICML-16)  pages 652–661. JMLR. org.

Li  L.  Munos  R.  and Szepesvari  C. (2015). Toward minimax off-policy value estimation. In

Artiﬁcial Intelligence and Statistics (AISTATS-15)  pages 608–616.

Liu  Q.  Li  L.  Tang  Z.  and Zhou  D. (2018a). Breaking the curse of horizon: Inﬁnite-horizon
off-policy estimation. In Advances in Neural Information Processing Systems (NeurIPS-18)  pages
5361–5371.

Liu  Y.  Gottesman  O.  Raghu  A.  Komorowski  M.  Faisal  A. A.  Doshi-Velez  F.  and Brunskill  E.
(2018b). Representation balancing mdps for off-policy policy evaluation. In Advances in Neural
Information Processing Systems (NeurIPS-18)  pages 2649–2658.

10

Mandel  T.  Liu  Y.-E.  Levine  S.  Brunskill  E.  and Popovic  Z. (2014). Ofﬂine policy evaluation
across representations with applications to educational games. In International conference on
Autonomous agents and multi-agent systems  pages 1077–1084. International Foundation for
Autonomous Agents and Multiagent Systems.

Murphy  S. A.  van der Laan  M. J.  Robins  J. M.  and Group  C. P. P. R. (2001). Marginal mean
models for dynamic regimes. Journal of the American Statistical Association  96(456):1410–1423.

Precup  D.  Sutton  R. S.  and Singh  S. P. (2000). Eligibility traces for off-policy policy evaluation.
In International Conference on Machine Learning (ICML-00)  pages 759–766. Morgan Kaufmann
Publishers Inc.

Raghu  A.  Komorowski  M.  Celi  L. A.  Szolovits  P.  and Ghassemi  M. (2017). Continuous
state-space models for optimal sepsis treatment: a deep reinforcement learning approach. In
Machine Learning for Healthcare Conference  pages 147–163.

Singh  S. P. and Sutton  R. S. (1996). Reinforcement learning with replacing eligibility traces.

Machine learning  22(1-3):123–158.

Sobel  M. J. (1982). The variance of discounted markov decision processes. Journal of Applied

Probability  19(4):794–802.

Sutton  R. S. and Barto  A. G. (1998). Reinforcement learning: An introduction  volume 1. MIT

press Cambridge.

Tang  L.  Rosales  R.  Singh  A.  and Agarwal  D. (2013). Automatic ad format selection via
contextual bandits. In ACM International Conference on Information & Knowledge Management
(CIKM-13)  pages 1587–1594. ACM.

Theocharous  G.  Thomas  P. S.  and Ghavamzadeh  M. (2015). Personalized ad recommendation
systems for life-time value optimization with guarantees. In International Joint Conferences on
Artiﬁcial Intelligence (IJCAI-15)  pages 1806–1812.

Thomas  P. and Brunskill  E. (2016). Data-efﬁcient off-policy policy evaluation for reinforcement

learning. In International Conference on Machine Learning (ICML-16)  pages 2139–2148.

Thomas  P. S. (2015). Safe reinforcement learning. PhD thesis  University of Massachusetts Amherst.

Thomas  P. S.  Theocharous  G.  and Ghavamzadeh  M. (2015). High-conﬁdence off-policy evaluation.

In AAAI Conference on Artiﬁcial Intelligence (AAAI-15)  pages 3000–3006.

Thomas  P. S.  Theocharous  G.  Ghavamzadeh  M.  Durugkar  I.  and Brunskill  E. (2017). Predictive
off-policy policy evaluation for nonstationary decision problems  with applications to digital
marketing. In AAAI Conference on Artiﬁcial Intelligence (AAAI-17)  pages 4740–4745.

Wang  Y.-X.  Agarwal  A.  and Dudık  M. (2017). Optimal and adaptive off-policy evaluation in
contextual bandits. In International Conference on Machine Learning (ICML-17)  pages 3589–
3597.

11

,Dandan Guo
Bo Chen
Hao Zhang
Mingyuan Zhou
Tengyang Xie
Yifei Ma
Yu-Xiang Wang