2018,A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem,Bandit learning is characterized by the tension between long-term exploration and short-term exploitation.  However  as has recently been noted  in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction  lending  and sequential drug trials)  exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings  one might like to run a ``greedy'' algorithm  which always makes the optimal decision for the individuals at hand --- but doing this can result in a catastrophic failure to learn. In this paper  we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm.

We give a smoothed analysis  showing that even when contexts may be chosen by an adversary  small perturbations of the adversary's choices suffice for the algorithm to achieve ``no regret''  perhaps (depending on the specifics of the setting) with a constant amount of initial training data.  This suggests that in slightly perturbed environments  exploration and exploitation need not be in conflict in the linear setting.,A Smoothed Analysis of the Greedy Algorithm for the

Linear Contextual Bandit Problem

Sampath Kannan

University of Pennsylvania

Jamie Morgenstern

Georgia Tech

Aaron Roth

University of Pennsylvania

Bo Waggoner

Microsoft Research  NYC

Zhiwei Steven Wu

University of Minnesota

Abstract

Bandit learning is characterized by the tension between long-term exploration and
short-term exploitation. However  as has recently been noted  in settings in which
the choices of the learning algorithm correspond to important decisions about
individual people (such as criminal recidivism prediction  lending  and sequential
drug trials)  exploration corresponds to explicitly sacriﬁcing the well-being of one
individual for the potential future beneﬁt of others. In such settings  one might like
to run a “greedy” algorithm  which always makes the optimal decision for the indi-
viduals at hand — but doing this can result in a catastrophic failure to learn. In this
paper  we consider the linear contextual bandit problem and revisit the performance
of the greedy algorithm. We give a smoothed analysis  showing that even when
contexts may be chosen by an adversary  small perturbations of the adversary’s
choices sufﬁce for the algorithm to achieve “no regret”  perhaps (depending on
the speciﬁcs of the setting) with a constant amount of initial training data. This
suggests that in slightly perturbed environments  exploration and exploitation need
not be in conﬂict in the linear setting.1

1

Introduction

Learning algorithms often need to operate in partial feedback settings (also known as bandit settings) 
in which the decisions of the algorithm determine the data that it observes. Many real-world
application domains of machine learning have this ﬂavor. Predictive policing algorithms [Rudin 
2013] deploy police ofﬁcers and receive feedback about crimes committed and observed in areas the
algorithm chose to deploy ofﬁcers. Lending algorithms [Byrnes  2016] observe whether individuals
who were granted loans pay them back  but do not get to observe counterfactuals: would an individual
not granted a loan have repaid such a loan? Algorithms which inform bail and parole decisions
[Barry-Jester et al.  2015] observe whether individuals who are released go on to recidivate  but do
not get to observe whether individuals who remain incarcerated would have committed crimes had
they been released. Algorithms assigning drugs to patients in clinical trials do not get to observe the
effects of the drugs that were not assigned to particular patients.
Learning in partial feedback settings faces the well-understood tension between exploration and
exploitation. In order to perform well  the algorithms need at some point to exploit the information
they have gathered and make the best decisions they can. But they also need to explore: to make
decisions which do not seem optimal according to the algorithm’s current point-predictions  in order
to gather more information about less explored portions of the decision space.

1The full version of this paper is available at https://arxiv.org/abs/1801.03423.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

However  in practice  decision systems often do not explicitly explore  for a number of reasons.
Exploration is important for maximizing long-run performance  but decision makers might be myopic
— more interested in their short-term reward. In other situations  the decisions made at each round
affect the lives of individuals  and explicit exploration might be objectionable on its face: it can be
considered immoral to harm an individual today (explicitly sacriﬁcing present utility) for a potential
beneﬁt to future individuals (long-term learning rates) [Bird et al.  2016  Bastani et al.  2017]. For
example  in a medical trial  it may be repugnant to knowingly assign a patient a drug that is thought to
be sub-optimal (or even dangerous) given the current state of knowledge  simply to increase statistical
certainty. In a parole scenario  we may not want to release a criminal that we estimate is at high risk
for committing violent crime.
On the other hand  a lack of exploration can lead to a catastrophic failure to learn  which is highly
undesirable – and which can also lead to unfairness. A lack of exploration (and a corresponding
failure to correctly learn about crime statistics) has been blamed as a source of “unfairness” in
predictive policing algorithms [Ensign et al.  2017]. In this paper  we seek to quantify how costly
we should expect a lack of exploration to be when the instances are not entirely worst-case. In other
words: is myopia a friction that we should generically expect to quickly be overcome  or is it really a
long-term obstacle to learning? Empirical evaluation shows that greedy algorithms often do well —
even outperforming algorithms with explicit exploration [Bietti et al.  2018]. Our work provides a
theoretical explanation for this phenomenon.

1.1 Our Results

i] = βi · xt

We study the linear contextual bandits problem  which informally  represents the following learning
scenario which takes place over a sequence of rounds t (formal deﬁnitions appear in Section 2). At
each round t  the learner must make a decision amongst k choices  which are represented by contexts
i ∈ Rd. If the learner chooses action it at round t  he observes a reward rt
it — but does not observe
xt
the rewards corresponding to choices not taken. The rewards are stochastic  and their expectations are
governed by unknown linear functions of the contexts. For an unknown set of parameters βi ∈ Rd 
E[rt
i. We consider two variants of the problem: in one (the single parameter setting)  all of
the rewards are governed by the same linear function: β1 = . . . = βk = β. In the other (the multiple
parameter setting)  the parameter vectors βi for each choice can be distinct. Normally  these two
settings are equivalent to one another (up to a factor of k in the problem dimension) — but as we
show  in our case  they have distinct properties2. The single-parameter setting can model  for example 
the choice of which of some subset of individuals should participate in a particular clinical trial. The
multi-parameter setting can model  for example  the risk of criminal recidivism amongst different
individuals who come from different backgrounds  when observable features correlate differently to
crime risk amongst different groups of individuals.
We study the greedy algorithm  which trains least-squares estimates ˆβt
tions  and at each round  picks the arm with the highest predicted reward: it = arg maxi
the single parameter setting  greedy simply maintains a single estimate ˆβt.
It is well known that the greedy algorithm does not obtain any non-trivial worst-case regret bound.
We give a smoothed analysis which shows that the worst case is brittle  however. Speciﬁcally  we
consider a model in which the contexts xt
i are chosen at each round by an adaptive adversary  but are
then perturbed by independent Gaussian perturbations in each coordinate  with standard deviation σ.
We show that under smoothed analysis  there is a qualitative distinction between the single parameter
and multiple parameter settings:

i on the current set of observa-
i. In

i · xt
ˆβt

1. In the single parameter setting (Section 4)  the greedy algorithm with high probability

(cid:16)√

(cid:17)

obtains regret bounded by ˜O

T d
σ2

over T rounds.

2. In the multiple parameter setting (Section 5)  the greedy algorithm requires a “warm start” –
that is  to start with a small number of observations for each action – to obtain non-trivial
regret bounds  even when facing a perturbed adversary. We show that if the warm start
provides for each arm a small number of examples (depending polynomially on ﬁxed
parameters of the instance  like 1/σ  d  k  and 1/(mini ||βi||))  that may themselves be
2To convert a multi-parameter problem to single-parameter  concatenate the parameter vectors βi ∈ Rd into
i into kd dimensions with zeros in all irrelevant kd − d coordinates.

a single vector β ∈ Rkd  and lift contexts xt

2

(cid:16)√

(cid:17)

T k
σ2

chosen by an adversary and perturbed  then with high probability greedy obtains regret
˜O
. Moreover  this warm start is necessary: we give lower bounds showing that if
the greedy algorithm is not initialized with a number of examples n that grows polynomially
with both 1/σ and with 1/ mini ||βi||  then there are simple ﬁxed instances that force the
algorithm to have regret growing linearly with T   with constant probability. (See Section 6
for a formal statement of the lower bounds.)

Our results extend beyond this particular perturbed adversary: we give general conditions on the
distribution over contexts which imply our regret bounds. All missing proofs can be found in the full
version.

1.2 Related Work

The most closely related piece of work (from which we take direct inspiration) is Bastani et al. [2017] 
who  in a stochastic setting  give conditions on the sampling distribution over contexts that causes
the greedy algorithm to have diminishing regret in a closely related but incomparable version of
the two-armed linear contextual bandits problem3. The conditions on the context distribution given
in that work are restrictive  however. They imply  for example  that every linear policy (and in
particular the optimal policy) will choose each action with constant probability bounded away from
zero. When translated to our perturbed adversarial setting  the distributional conditions of Bastani
et al. [2017] do not imply regret bounds that are sub-exponential in either the perturbation magnitude
σ or the dimension d of the problem. There is also strong empirical evidence that exploration free
algorithms perform well on real datasets: [Bietti et al.  2018]. Our work can be viewed as providing
an explanation of this phenomenon. Finally  building on our work  [Raghavan et al.  2018] use the
same diversity condition that we introduce in this paper to show a stronger result in a more restrictive
setting. They show that in the single parameter setting  when one further assumes that 1) the linear
parameter is drawn from a Bayesian prior that is not too concentrated  2) the contexts are drawn
i.i.d. from a ﬁxed distribution and then perturbed  and 3) that the algorithm is allowed to make its
decisions in “batches” of polylog(d  t)/σ2 many rounds  then the greedy algorithm is essentially
instance optimal in terms of Bayesian regret  and moreover  that its regret grows at a rate of O(T 1/3)
in the worst case. In contrast  we make substantially weaker assumptions (the parameter vector and
contexts can be worst case  we need not be in the single parameter setting  and we don’t need batches) 
but prove a worse regret bound of O(T 1/2)  without a guarantee of instance optimality.
A large literature focuses on designing no-regret algorithms for contextual bandit problems (e.g.
Li et al. [2010]  Agarwal et al. [2014]  Li et al. [2011])  particularly for linear contextual bandits
(e.g. [Chu et al.  2011  Abbasi-Yadkori et al.  2011]). Some of these (e.g. Syrgkanis et al. [2016])
use “follow the perturbed leader” style algorithms  which invite a natural comparison to our setting.
However  the phenomenon we are exploiting is quite different. It is very important in our setting that
the perturbations are added by nature  and if the perturbations were instead added by our algorithm
(against worst-case contexts)  the regret guarantee would cease to hold. To see this  note that against
worst-case adversaries  the single parameter and multiple parameter settings are equivalent to one
another — but in our smoothed setting  we prove a qualitative separation.
We defer further related work  including work on smoothed analysis and algorithmic fairness  to the
full version.

2 Model and Preliminaries

We now introduce the notation and deﬁnitions we use for this work. For a vector x  (cid:107)x(cid:107) represents its
Euclidean norm. We consider two variants of the k-arm linear contextual bandits problem. The ﬁrst
setting has a single d-dimensional parameter vector β which governs rewards for all contexts x ∈ Rd;
the second has k distinct parameter vectors βi ∈ Rd governing the rewards for different arms.

3Bastani et al. [2017] assume a single context at each round  shared between two actions. We consider each

action as parameterized by its own context  and k can be arbitrary.

3

T(cid:88)

(cid:16)

(cid:17)

βi · xt

i − βit · xt

it

.

1  . . .   xt

i ∈ Rd is treated as a row vector unless
In rounds t  contexts xt
otherwise noted. The learner chooses an arm it ∈ {1  . . .   k}  and obtains s2-subgaussian4 reward
rt whose mean satisﬁes E [rt] = β · xt
it in the
multi-parameter setting. The regret of a sequence of actions and contexts of length T is (again  in the
single parameter setting all βi = β):

it in the single parameter setting and E [rt] = βit · xt

k  are presented  where xt

Regret = Regret(x1  i1  . . .   xT   iT ) =

max

i

t=1

We next formalize the history or transcript of an algorithm on a sequence of contexts. A history entry

is a member of H =(cid:0)Rd(cid:1)k × {1  . . .   k} × R. A history is a list of history entries  i.e. a member of
contexts: A : H∗ →(cid:0)Rd(cid:1)k. We denote the output of the adversary by (µ1  µ2  . . .   µk)5 We assume

H∗. Given a history H ∈ HT   entry t is denoted ht = (x1  . . .   xk  it  rt
Formally  an adaptive adversary A is a (possibly randomized) algorithm that maps a history to k
that (cid:107)µi(cid:107) ≤ 1 always. Next we deﬁne the notion of a perturbed adversary  which encompasses both
stages of the context-generation process.
Deﬁnition 1 (Perturbed Adversary). For any adversary A  the σ-perturbed adversary Aσ is deﬁned
by the following process. In round t:

it).

1 + et

1  . . .   et

1  . . .   xt

1  . . .   µt

k) = (µt

k = A(H t−1).
1  . . .   µt

1. Given history H t−1 ∈ Ht−1  let µt
2. Perturbations et
3. Output the list of contexts (xt

k are drawn independently from N (0  σ2I).
k).
k + et
We deﬁne a perturbed adversary to be R-bounded if with probability 1  (cid:107)xt
i(cid:107) ≤ R for all i and t and
all histories. We call perturbations (r  δ)-centrally bounded if  for each history  and ﬁxed unit vectors
w1  . . .   wk (possibly all equal)  we have with probability 1 − δ that maxi=1 ... k wi · et
We can interpret the output of a perturbed adversary as being a mild perturbation of the (unperturbed)
adaptive adversary when the magnitude of the perturbations is smaller than the magnitude of the
original context choices µi themselves. Said another way  we can think of the perturbations as
being mild when they do not substantially increase the norms of the contexts with probability at
least 1 − δ. This will be the case throughout the run of the algorithm (via a union bound over T )
when σ ≤ ˜O(1/
d). We refer to this case as the “low perturbation regime”. We view it as the
most interesting case because otherwise  the perturbations tend to be large enough to overwhelm the
adversarial choices and the problem becomes easier. Here we focus on presenting results for the low
perturbation regime  leaving the rest to the full version.

i ≤ r.

√

3 Proof Approach and Key Conditions

Our goal will be to show that the greedy algorithm achieves no regret against any perturbed adversary
in both the single-parameter and multiple-parameter settings. The key idea is to show that the
distribution on contexts generated by perturbed adversaries satisfy certain conditions which sufﬁce to
prove a regret bound. The conditions we work with are related to (but substantially weaker than) the
conditions shown to be sufﬁcient for a no regret guarantee by Bastani et al. [2017].
The ﬁrst key condition  diversity of contexts  considers the positive semideﬁnite d × d matrix E [x
x]
for a context x  and asks for a lower bound on its minimum eigenvalue. This implies the distribution
over x has non-trivial variance in all directions  which is necessary for the least squares estimator
to converge to the underlying parameter β. It implies that observations of β · x convey information
about β in all directions.
However  we only observe the rewards for contexts x conditioned on Greedy selecting them: we
see a biased (conditional) distribution on x. Thus we need the diversity condition to hold on these
conditional distributions.
Condition 1 (Diversity). Let e ∼ D on Rd and let r  λ0 ≥ 0. We call D (r  λ0)-diverse if for all ˆβ 
all µ with (cid:107)µ(cid:107) ≤ 1  and all ˆb ≤ r(cid:107) ˆβ(cid:107)  for x = µ + e:

4A random variable Y with mean µ is s2-subgaussian if E(cid:104)

et(Y −µ)(cid:105) ≤ et2s2/2 for all t.

(cid:124)

5The notation is chosen since µi will be the mean around which the perturbed context is drawn.

4

(cid:18)

E
e∼D

(cid:104)

x

(cid:124)

x

(cid:105)(cid:19)
(cid:12)(cid:12)(cid:12) ˆβ · e ≥ ˆb

λmin

≥ λ0.

In the single parameter setting  diversity will imply a regret guarantee: when any arm is pulled  the
context-reward pair gives useful information about all components of the (single) parameter β. In the
multiple parameter setting  diversity will sufﬁce to guarantee that the learner’s estimate of arm i’s
parameter vector converges to βi as a function of the number of times arm i is pulled; but alone it
does not cause arm i to be pulled sufﬁciently often (even in rounds where i is the best alternative 
when failing to pull it will cause our algorithm to suffer regret).
Thus the multiple parameter setting will require a second key condition  margins. Margins will imply
that conditioned on an arm being optimal on a given round  there is a non-trivial probability (over the
randomness in the contexts) that Greedy perceives it to be optimal based on current estimates { ˆβt
i} 
so long as the current estimates achieve at least some constant baseline accuracy. A small initial
training set can guarantee that initial estimates achieve constant error  and so the margin condition
implies that Greedy will continue to explore arms with a frequency that is proportional to the number
of rounds for which they are optimal; then diversity implies that estimates of those arms’ parameters
will improve quickly (without promising anything about arms that are rarely optimal – and hence
inconsequential for regret).
Condition 2 (Conditional Margins). Let e ∼ D and let r  α  γ ≥ 0. We say D has (r  α  γ) margins
if for all β (cid:54)= 0 and b ≤ r(cid:107)β(cid:107) 

P [β · e > b + α(cid:107)β(cid:107) | β · e ≥ b] ≥ γ.

So  on rounds for which arm i has largest expected reward  with probability at least γ its expected
reward is largest by at least some margin (α(cid:107)β(cid:107)). If Greedy has sufﬁciently accurate estimates { ˆβt
i} 
this implies that Greedy will pull arm i. We say a perturbed adversary satisﬁes the diversity and
margin conditions if the distributions of et
i are independent and satisfy these conditions for all i  t.
We will show the diversity condition implies no-regret in single-parameter settings  and the diver-
sity and margin conditions imply no-regret in multi-parameter settings. We further show that the
perturbation distribution N (0  σ2I) satisﬁes these conditions. We note that our choice of Gaussian
perturbations was convenient and natural but not necessary (other perturbation distributions also
satisfy our conditions  implying similar results for those perturbations).

Complications: extreme perturbation realizations. When the realizations of the Gaussian per-
turbations have extremely large magnitude  the diversity and margin conditions will not hold6. This
is potentially problematic  because the probabilistic conditioning in both conditions increases the
likelihood that the perturbations will be large. This is the role of the parameter r in both conditions:
to provide a reasonable upper bound on the threshold that a perturbation variable should not exceed.
exceed. In the succeeding sections  we will use conditions we call “good” to formalize the intuition
that this is unlikely to happen often  when the perturbations satisfy a centrally-bounded condition.

4 Single Parameter Setting

We deﬁne the “Greedy Algorithm” as the algorithm which myopically pulls the “best” arm at each
round according to the predictions of the classic least-squares estimator. Let X t denote the (t− 1)× d
design matrix at time t  in which each row t(cid:48) is some observed context xt(cid:48)
was
selected at round t(cid:48) < t. The corresponding vector of rewards is denoted yt = (r1
it−1). The
(cid:124). At each round t  Greedy ﬁrst computes
transposes of a matrix Z and vector z are denoted Z
the least-squares estimator based on the historical contexts and rewards: ˆβt ∈ arg minβ ||X tβ−yt||2
2 
and then greedily selects the arm with the highest estimated reward: it = arg maxi
i. We defer
the formal description of the algorithm to the full version.

it(cid:48) where arm it(cid:48)
i1  . . .   rt−1

(cid:124) and z

ˆβt · xt

“Reasonable” rounds. As discussed in Section 2  the diversity condition will only hold when an
i are not too large; we formalize these “good” situations here. Fix a round t 
arm’s perturbations et
6E.g. for margins  consider the one-dimensional case: a lower truncated Gaussian tightly concentrates on its

minimal support value.

5

the current Greedy hypothesis ˆβt  and any choices of the adversary µt
k conditioned on the
entire history up to round t. Now each value ˆβtxt
i is a random variable  and Greedy
selects the arm corresponding to the largest realized value. In particular  we deﬁne the “threshold”
for Greedy to pull i as follows.
Deﬁnition 2. Fix a round t  Greedy’s hypothesis ˆβt  and the adversary’s choices µt
deﬁne ˆct

i is r-(cid:100)good if ˆct

j. We say a realization of ˆct

i + r(cid:107) ˆβt(cid:107).

i ≤ ˆβt · µt

i = ˆβtµt

i + ˆβtet

1  . . .   µt

1  . . .   µt

ˆβt · xt

k. We

i := maxj(cid:54)=i

The “hat” on (cid:100)good corresponds to those on ˆct

analogous conditions without the hats. Notice that ˆct
perturbations et
determined by the perturbations et
too large for arm i to be selected.

j for j (cid:54)= i  and Greedy pulls i if and only if ˆβtxt

i and ˆβt. In the multiple parameter setting we will use
i is a random variable that depends on all the
i is r-good is
i need not be

i is r-(cid:100)good  then et

i ≥ ˆct
i(cid:48) of all arms i(cid:48) (cid:54)= i. Intuitively  if ˆct

i. The event that ˆct

4.1 Regret framework for perturbed adversaries

We ﬁrst observe an upper-bound on Greedy’s regret as a function of the distance between ˆβt and
the true model β. This allows us to focus on the diversity condition  which will guarantee that this
distance shrinks. Let i∗(t) = arg maxi β · xt
Lemma 4.1. Suppose for all i  t that (cid:107)xt
i(cid:107) ≤ R. In the single-parameter setting  for any tmin ∈ [T ] 
we have:

i  the optimal arm at time t.

Regret(x1  i1  . . .   xT   iT ) ≤ 2Rtmin + 2R

T(cid:88)

(cid:13)(cid:13)(cid:13)β − ˆβt(cid:13)(cid:13)(cid:13) .

t=tmin

To apply Lemma 4.1  we need to show that estimates ˆβt → β quickly. The key idea is that if the
input contexts are “diverse” enough (captured formally by Deﬁnition 1)  we will be able to infer
β. Lemma 4.2 shows ˆβt approaches β at a rate governed by the minimum eigenvalue of the design
matrix.
i(cid:107) ≤ R and recall
Lemma 4.2. Fix a round t and let Z t = (X t)
that rewards are s2-subgaussian. Then with probability 1 − δ over the randomness in rewards  we
have

X t. Suppose all contexts satisfy (cid:107)xt

(cid:112)2tdRs2 ln(td/δ)

(cid:124)

Observe that the matrix Z t =(cid:80)

(cid:107)β − ˆβt(cid:107) ≤

(cid:124)

λmin (Z t)

.

t(cid:48)≤t (xt(cid:48)
i )

xt(cid:48)
i . The next step is to show that λmin(Z t) grows at a
rate of Θ(t) with high probability  which will imply via Lemma 4.2 that (cid:107)β − ˆβt(cid:107) ≤ O(1/
t) 
ﬁxing all other parameters. This is proven in the following key result  Lemma 4.3. The proof uses a
concentration result for the minimum eigenvalue to show that λmin(Z t) grows at a rate Θ(t) with high
probability. This relies crucially on the (r  λ0) diversity condition  which intuitively lower-bounds the
expected increase in λmin(Z t) at each round. The details are more complicated  as this increase only
i; we show this happens with constant probability

holds when Greedy’s choice of i has an r-(cid:100)good ˆct

for an (r  1/2)-centrally bounded adversary.
Lemma 4.3. For Greedy in the single parameter setting with an R-bounded  (r  1/2)-
centrally bounded  (r  λ0)-diverse adversary  we have with probability 1 − δ that for all t ≥
max{0  20R2

λ0dδ )}  we have λmin(Z t) ≥ tλ0
4 .

ln( 20R2

√

λ0

Combining these results gives a bound on the regret of Greedy against general perturbed adversaries.

The Gaussian  σ-perturbed adversary. We need to show that our σ-perturbed adversary satisﬁes
the diversity condition (and another technical condition that we defer to the supplementary materials).
For the diversity condition  we show that the diversity parameter λ can be lower bounded by the
variance of a single-dimensional truncated Gaussian  then analyze this variance using tight Gaussian
tail bounds. Our proof makes use of the careful choice of truncations of A(cid:48)
σ using a different
orthonormal change of basis each round  which maintains the perturbation’s Gaussian distribution
but allows the form of the conditioning to be much simpliﬁed. Finally  we arrive at the main result
for this section:

6

Theorem 4.1. In the single parameter setting against the σ-perturbed adversary Aσ  ﬁx any choice
of parameters such that σ ≤
(the low perturbation regime) and d ≤ eO(s2T ). With
probability at least 1 − δ  Greedy has

2d ln(T kd/δ)

√

2

1

(cid:32)(cid:112)T ds2 ln(T d/δ) ln(k)

(cid:33)

σ2

Regret ≤ O

where d is the dimension of contexts  k is the number of arms  rewards are s2-subgaussian  and in all
cases O(·) hides an absolute constant.

5 Multiple Parameter Setting

In the multi-parameter setting  we cannot hope for the greedy algorithm to achieve vanishing regret
without any initial information  as it never learns about parameters of arms it does not pull (formalized
in a lower bound in Section 6). If  however  Greedy receives a small amount of initial information in
the form of a constant number of n samples (xi  ri) for each arm i  perturbations will imply vanishing
regret. We refer to this as an n-sample “warm start” to Greedy. (See the full version for a formal
description of the algorithm.)
For this setting  we show that the diversity and margin conditions together on a generic bounded
adversary imply low regret. We then leverage this to give regret bounds for the Gaussian adversary
Aσ. As discussed in Section 3  the key idea is as follows. Analogous to the single parameter setting 
the diversity condition implies that additional datapoints we collect for an arm improve the accuracy
of its estimate ˆβt
i . Meanwhile  the margin condition implies that for sufﬁciently accurate estimates 
when an arm is optimal (βixt
i is largest)  the perturbations have a good chance of causing Greedy to
pull that arm ( ˆβt
i is largest). Thus  the initial data sample kickstarts Greedy with reasonably accurate
estimates  causing it to regularly pull optimal arms and accrue more data points  thus becoming more
accurate.

i xt

Notation and preliminaries. Recall that it is the arm pulled by Greedy at round t  i.e. it =
i · xt
i. Similarly let i∗(t) be the optimal arm at round t  i.e. i∗(t) = arg maxi βi · xt
ˆβt
i. Let
arg maxi
ni(t) be the number of times arm i is pulled prior to round t  including the warm start (so ni(1) will
i = {t : i∗(t) = i}  the rounds where i is pulled and is
be nonzero). Let Si = {t : it = i} and let S∗
i is a threshold that i must exceed to be pulled by Greedy  and the r-(cid:100)good condition
optimal respectively.

Recall that ˆct
captures cases where this can happen without the perturbation et
condition formally for the multiple parameter case. We also need a similar threshold ct
exceed to be the optimal arm  and an analogous r-good condition.
Deﬁnition 3. Fix a round t  the current Greedy hypotheses ˆβt
µt
1  . . .   µt
outcome of ˆct
outcome of ct

j · xt
ˆβt
i := maxj(cid:54)=i
i + r(cid:107) ˆβt
i ≤ ˆβt
i · µt
i ≤ βi · µt
i + r(cid:107)βi(cid:107).

1  . . .   ˆβt
i(cid:107). Similarly  deﬁne ct

j  a random variable depending on {et

i is r-(cid:100)good if ˆct

i is r-good if ct

k  and choices of an adversary
j : j (cid:54)= i}. Say an
i := maxj(cid:54)=i βj · xt
j and say an

i being too large. We now deﬁne this
i that i must

k. Deﬁne ˆct

5.1 Regret framework for perturbed adversaries

Similarly to Lemma 4.1  here the regret of Greedy shrinks as each ˆβt
identical  but in this case  we prove this for each arm i ∈ [k].

Lemma 5.1. In the multiple parameter setting  the regret of Greedy is bounded by(cid:80)k
 .
(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)βi − ˆβt

(cid:13)(cid:13)(cid:13)βi − ˆβt

(cid:88)

(cid:32)(cid:88)

Regreti(T ) = R

(cid:13)(cid:13)(cid:13)(cid:33)

with

+ R

i

i

t∈Si

t∈S∗

i

i → βi. The proof is essentially

i=1 Regreti(T )

As in the single parameter setting  the diversity condition implies that with enough observations ni(t)
for arm i  we have (cid:107)βi − ˆβt
). We omit the details as they are analogous to the single
parameter case  and move on to the margin condition.

i(cid:107) ≤ O(

1√

ni(t)

7

We wish to capture the beneﬁts of the margin condition  i.e. that arms which are often optimal are also
actually pulled often by Greedy. The ﬁrst step is to leverage the margin condition to argue that when
i is r-good)  it is optimal by a signiﬁcant margin (α(cid:107)βi(cid:107)) with a signiﬁcant
arm i is optimal (and ct
probability (γ). Combining this with accurate initial estimates implies that it will actually be pulled
by Greedy.
Lemma 5.2. Suppose the perturbed adversary is R-bounded and has (r  α  γ) margins for some
r ≤ R. Consider any round t where for all j we have (cid:107)βj − ˆβt

j(cid:107) ≤ α minj(cid:48) (cid:107)βj(cid:48)(cid:107)

. Then

2R

P(cid:2)it = i(cid:12)(cid:12) i∗(t) = i  ct

i is r-good(cid:3) ≥ γ.

Recall that Si  S∗
i are respectively the set of rounds in which it = i (Greedy pulls arm i) and i∗(t) = i
(arm i is optimal)  respectively. The following key result leverages the margin condition to argue that 
if i is optimal for a signiﬁcant number of rounds  then it is pulled by Greedy. This is vital to a good
regret bound because it shows that ni(t)  the number of samples from arm i  is steadily increasing in
t if i is often optimal  which we know from the diversity condition implies that the estimate ˆβt
i is
converging.
Lemma 5.3. Consider an R-bounded perturbed adversary with (r  α  γ) margins and assume
for all i and t. With probability at least 1 − δ  we have for all natural
(cid:107)βi − ˆβt
numbers N  |{t ∈ S∗
δ times
before being pulled by Greedy.

δ . That is  arm i can be optimal at most 5

i : ni(t) = N}| ≤ 5

i(cid:107) ≤ α minj (cid:107)βj(cid:107)

γ ln 2

γ ln 2

2R

The Gaussian  σ-perturbed adversary At a high level  all that remains to complete our analysis
is to prove that our perturbed adversary Aσ produces distributions that satisfy our margin condition.
There are some complications that make the details of this argument slightly circuitous  that we defer
to the full version in the supplement (we ﬁrst prove this result for an adversary that uses a truncated
Gaussian distribution  and hence always produces bounded contexts  and then use this to argue that
our actual adversary also has the properties that we need). Once this is proven  we obtain the main
result of the multiple parameter setting. In particular  in the small-perturbation regime  a constant-size
√
warm start (i.e. independent of T   as long as σ is small) sufﬁces to initialize Greedy such that  with
high probability  it can obtain ˜O(
Theorem 5.1. In the multiple parameter setting  against the σ-perturbed adversary Aσ  with a warm
start of size

T ) regret.

for any setting of parameters such that σ ≤

  Greedy satisﬁes with probability 1 − δ

 ds2 ln(
(cid:32)√

3

dks2

δσ minj (cid:107)βj(cid:107)2 )

σ12 minj (cid:107)βj(cid:107)2
√

1

d ln(2T kd/δ)
T kds2(ln T kd

δ )3/2

σ2

  
(cid:33)

n ≥ Ω

Regret ≤ O

where d is the dimension of the contexts  k is the number of arms  rewards are s2-subgaussian  and
in all cases O(·)  Ω(·) hide absolute constants.

6 Lower Bounds for the Multi-Parameter Setting

Finally  in this section  we show that our results for the multi-parameter setting are qualitatively tight.
Namely  Greedy can be forced to suffer linear regret in the multi-parameter setting unless it is given
σ   the perturbation parameter  and 1/ mini ||β||i  the
a “warm start” that scales polynomially with 1
norm of the smallest parameter vector. This shows the polynomial dependencies on these parameters
in our upper bound cannot be removed  and in particular  prove a qualitative separation between the
multi-parameter setting and the single parameter setting (in which a warm start is not required). Both
of our lower bounds are in the fully stochastic setting – i.e. they are based on instances in which
contexts are drawn from a ﬁxed distribution  and do not require that we make use of an adaptive
adversary. First  we focus on the perturbation parameter σ.

8

Theorem 6.1. Suppose greedy is given a warm start of size n ≤
setting. Then  there exists an instance for which Greedy incurs regret Ω( ρ√
in its ﬁrst ρ rounds.
Remark 1. Theorem 6.1 implies for T < exp( 1

σ )  either

100σ2 ln

• n = Ω(cid:0)poly(cid:0) 1

(cid:1)(cid:1)  or

• Greedy suffers linear regret.

σ

(cid:18)

(cid:19)

1

ρ

in the σ-perturbed
n ) with constant probability

100

n

ρ

100

100n ln

(cid:17)

(cid:16) 1√

n   σ2)  for σ =

The lower bound instance is simple: one-dimensional  with two arms and model parameters β1 =
√
β2 = 1.
In each round (including the warm start) the unperturbed contexts are µ1 = 1 and
µ2 = 1− 1/
2 are drawn independently from the Gaussian
distributions N (1  σ2) and N (1 − 1√
. We show the estimators after

n  and so the perturbed contexts xt

(cid:114) 1

1 and xt

the warm start have additive error Ω

constant probability  arm 1 will only be pulled ˜O(cid:0)n2/3(cid:1) rounds. So  with constant probability greedy

with a constant probability  and when this is true  with

will pull arm 2 nearly every round  even though arm 1 will be better in a constant fraction of rounds.
We now turn our attention to showing that the warm start must also grow with 1/ mini ||βi||. Infor-
mally  the instance we use to show this lower bound has unperturbed contexts µt
i = 1 for both arms
and all rounds  and β1 = 8  β2 = 10. We show again that the warm start of size n yields  with
constant probability  estimators with error ci√
n  causing Greedy to choose arm 2 rather than arm 1 for
a large number of rounds. When 2 is not pulled too many times  with constant probability its estimate
Theorem 6.2. Let  = mini |βi|  σ < 1(cid:114)
remains small and continues to be passed over in favor of arm 1.
of size n ≤ 1

δ < 2n1/3. Suppose Greedy is given a warm start

2 . Then  some instances cause Greedy to incur regret

and T

ln

T
δ

(cid:18)

(cid:18)

(cid:19)(cid:19)

.

R(T ) = Ω



e

1

18σ2 − n

2
3

Remark 2. Observe again that this implies that Greedy can be forced to incur linear regret if its warm
start size does not grow with 1/ mini ||βi|| for exponentially many rounds.

References
Yasin Abbasi-Yadkori  Dávid Pál  and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems  pages 2312–2320  2011.

Alekh Agarwal  Daniel Hsu  Satyen Kale  John Langford  Lihong Li  and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning  pages 1638–1646  2014.

Anna Maria Barry-Jester  Ben Casselman  and Dana Goldstein. The new science of sentencing. The
Marshall Project  August 8 2015. URL https://www.themarshallproject.org/2015/08/
04/the-new-science-of-sentencing. Retrieved 4/28/2016.

H. Bastani  M. Bayati  and K. Khosravi. Exploiting the Natural Exploration In Contextual Bandits.

ArXiv e-prints  April 2017.

A. Bietti  A. Agarwal  and J. Langford. Practical Evaluation and Optimization of Contextual Bandit

Algorithms. ArXiv e-prints  February 2018.

Sarah Bird  Solon Barocas  Kate Crawford  Fernando Diaz  and Hanna Wallach. Exploring or
exploiting? social and ethical implications of autonomous experimentation. Workshop on Fairness 
Accountability  and Transparency in Machine Learning  2016.

Nanette Byrnes. Artiﬁcial intolerance. MIT Technology Review  March 28 2016. URL https://
www.technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016.

9

Wei Chu  Lihong Li  Lev Reyzin  and Robert E Schapire. Contextual bandits with linear payoff
functions. In International Conference on Artiﬁcial Intelligence and Statistics  pages 208–214 
2011.

Danielle Ensign  Sorelle A. Friedler  Scott Neville  Carlos Eduardo Scheidegger  and Suresh Venkata-
subramanian. Runaway feedback loops in predictive policing. Workshop on Fairness  Accountabil-
ity  and Transparency in Machine Learning  2017.

Lihong Li  Wei Chu  John Langford  and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web  pages 661–670. ACM  2010.

Lihong Li  Wei Chu  John Langford  and Xuanhui Wang. Unbiased ofﬂine evaluation of contextual-
bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM interna-
tional conference on Web search and data mining  pages 297–306. ACM  2011.

Manish Raghavan  Aleksandrs Slivkins  Jennifer Wortman Vaughan  and Zhiwei Steven Wu. The
externalities of exploration and how data diversity helps exploitation. In Proceedings of the 31st
Conference on Learning Theory  COLT 2018  2018.

Cynthia Rudin.

Predictive policing using machine learning to detect patterns of crime.
Wired Magazine  August 2013.
URL http://www.wired.com/insights/2013/08/
predictive-policing-using-machine-learning-to-detect-\patterns-of-crime/.
Retrieved 4/28/2016.

Vasilis Syrgkanis  Akshay Krishnamurthy  and Robert Schapire. Efﬁcient algorithms for adversarial
contextual learning. In International Conference on Machine Learning  pages 2159–2168  2016.

10

,Ozan Irsoy
Claire Cardie
Sampath Kannan
Jamie Morgenstern
Aaron Roth
Bo Waggoner
Zhiwei  Steven Wu