2019,Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm,We present a novel algorithm to estimate the barycenter of arbitrary probability distributions with respect to the Sinkhorn divergence. Based on a Frank-Wolfe optimization strategy  our approach proceeds by populating the support of the barycenter incrementally  without requiring any pre-allocation. We consider discrete as well as continuous distributions  proving convergence rates of the proposed algorithm in both settings. Key elements of our analysis are a new result showing that the Sinkhorn divergence on compact domains has Lipschitz continuous gradient with respect to the Total Variation and a characterization of the sample complexity of Sinkhorn potentials. Experiments validate the effectiveness of our method in practice.,Sinkhorn Barycenters with Free Support via

Frank-Wolfe Algorithm

Giulia Luise1  Saverio Salzo2  Massimiliano Pontil1 2  Carlo Ciliberto3

g.luise.16@ucl.ac.uk  saverio.salzo@iit.it  m.pontil@cs.ucl.ac.uk c.ciliberto@ic.ac.uk

1 Department of Computer Science  University College London  UK

2 CSML  Istituto Italiano di Tecnologia  Genova  Italy

3 Department of Electrical and Electronic Engineering  Imperial College London  UK

Abstract

We present a novel algorithm to estimate the barycenter of arbitrary probability
distributions with respect to the Sinkhorn divergence. Based on a Frank-Wolfe
optimization strategy  our approach proceeds by populating the support of the
barycenter incrementally  without requiring any pre-allocation. We consider dis-
crete as well as continuous distributions  proving convergence rates of the proposed
algorithm in both settings. Key elements of our analysis are a new result show-
ing that the Sinkhorn divergence on compact domains has Lipschitz continuous
gradient with respect to the Total Variation and a characterization of the sample
complexity of Sinkhorn potentials. Experiments validate the effectiveness of our
method in practice.

1

Introduction

Aggregating and summarizing collections of probability measures is a key task in several machine
learning scenarios. Depending on the metric adopted  the properties of the resulting average (or
barycenter) of a family of probability measures vary signiﬁcantly. By design  optimal transport
metrics are better suited at capturing the geometry of the distribution than Euclidean distance or
f-divergences [14]. In particular  Wasserstein barycenters have been successfully used in settings
such as texture mixing [40]  Bayesian inference [49]  imaging [26]  or model ensemble [18].
The notion of barycenter in Wasserstein space was ﬁrst introduced by [2] and then investigated
from the computational perspective for the original Wasserstein distance [12  50  54] as well as its
entropic regularizations (e.g. Sinkhorn) [6  14  20]. Two main challenges in this regard are: i) how to
efﬁciently identify the support of the candidate barycenter and ii) how to deal with continuous (or
inﬁnitely supported) probability measures. The ﬁrst problem is typically addressed by either ﬁxing
the support of the barycenter a-priori [20  50] or by adopting an alternating minimization procedure
to iteratively optimize the support point locations and their weights [12  14]. While ﬁxed-support
methods enjoy better theoretical guarantees  free-support algorithms are more memory efﬁcient and
practicable in high dimensional settings. The problem of dealing with continuous distributions has
been mainly approached by adopting stochastic optimization methods to minimize the barycenter
functional [12  20  50].
In this work we propose a novel method to compute the barycenter of a set of probability distributions
with respect to the Sinkhorn divergence [25] that does not require to ﬁx the support beforehand.
We address both the cases of discrete and continuous probability measures. In contrast to previous
free-support methods  our algorithm does not perform an alternate minimization between support and
weights. Instead  we adopt a Frank-Wolfe (FW) procedure to populate the support by incrementally
adding new points and updating their weights at each iteration  similarly to kernel herding strategies
[5]. We prove the convergence of the proposed optimization scheme for both ﬁnitely and inﬁnitely

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

supported distribution settings. A central result to our analysis is the characterization of regularity
properties of Sinkhorn potentials (i.e.  the dual solutions of the Sinkhorn divergence problem)  which
extends recent work in [21  23]. We empirically evaluate the performance of the proposed algorithm.
Contributions. The analysis of the proposed algorithm hinges on the following contributions: i) we
show that the gradient of the Sinkhorn divergence is Lipschitz continuous on the space of probability
measures with respect to the Total Variation. This grants us convergence of the barycenter algorithm
in ﬁnite settings. ii) We characterize the sample complexity of Sinkhorn potentials of two empirical
distributions sampled from arbitrary probability measures. This latter result is interesting on its
own but it also enables us to iii) design a concrete optimization scheme to approximately solve the
barycenter problem for arbitrary probability measures with convergence guarantees. iv) A byproduct
of our analysis is the generalization of the FW algorithm to settings where the objective functional
is deﬁned only on a set with empty interior  which is the case for Sinkhorn divergence barycenter
problem.
The rest of the paper is organized as follows: Sec. 2 reviews standard notions of optimal transport
theory. Sec. 3 introduces the barycenter functional  and analyses the Lipschitz continuity of its
gradient. Sec. 4 describes the implementation of our algorithm and Sec. 5 studies its convergence
rates. Finally  Sec. 6 evaluates the proposed methods empirically and Sec. 7 provides concluding
remarks.

2 Background

The aim of this section is to recall deﬁnitions and properties of Optimal Transport theory with entropic
regularization. Throughout the work  we consider a compact set X⇢ Rd and a symmetric cost
function c : X⇥X! R. We set D := supx y2X c(x  y) and denote by M+
1 (X ) the space of
probability measures on X (positive Radon measures with mass 1). For any ↵   2M +
1 (X )  the
Optimal Transport problem with entropic regularization is deﬁned as follow [13  24  38]

OT"(↵  ) = min

⇡2⇧(↵ ) ZX 2

c(x  y) d⇡(x  y) + "KL(⇡|↵ ⌦ ) "

 0

(1)

where KL(⇡|↵ ⌦ ) is the Kullback-Leibler divergence between the candidate transport plan ⇡ and
the product distribution ↵ ⌦   and ⇧(↵  ) = {⇡ 2M 1
+(X 2) : P1#⇡ = ↵  P2#⇡ = }  with
the projector onto the i-th component and # the push-forward operator. The case
Pi : X⇥X!X
" = 0 corresponds to the classic Optimal Transport problem introduced by Kantorovich [29]. In
particular  if c = k·  ·kp for p 2 [1 1)  then OT0 is the well-known p-Wasserstein distance [52].
Let "> 0. Then  the dual problem of (1)  in the sense of Fenchel-Rockafellar  is (see [10  21])

OT"(↵  ) = max

u v2C(X )Z u(x) d↵(x) +Z v(y) d(y)  "Z e

u(x)+v(y)c(x y)

"

d↵(x)d(y) 

(2)

where C(X ) denotes the space of real-valued continuous functions on X   endowed with k·k1. Let
µ 2M +

1 (X ). We denote by Tµ : C(X ) !C (X ) the map such that  for any w 2C (X ) 

Tµ(w) : x 7! " logZ e

w(y)c(x y)

"

dµ(y).

(3)

The ﬁrst order optimality conditions for (2) are (see [21] or Appendix B.2)

u = T(v) ↵- a.e.

and

v = T↵(u) - a.e.

(4)
Pairs (u  v) satisfying (4) exist [30] and are referred to as Sinkhorn potentials. They are unique (↵  )
- a.e. up to an additive constant  i.e.  (u + t  v  t) is also a solution for any t 2 R. In line with
[21  23] it will be useful in the following to assume (u  v) to be the Sinkhorn potentials such that: i)
u(xo) = 0 for an arbitrary anchor point xo 2X and ii) (4) is satisﬁed pointwise on the entire domain
X . Then  u is a ﬁxed point of the map T↵ = T  T↵ (analogously for v). This suggests a ﬁxed
point iteration approach to minimize (2)  yielding the well-known Sinkhorn-Knopp algorithm which
has been shown to converge linearly in C(X ) [30  41]. See also Thm. B.10 for a precise statement.
We recall a key result characterizing the differentiability of OT" in terms of the Sinkhorn potentials
that will be useful in the following.

2

Proposition 1 (Prop 2 in [21]). Let rOT" : M+

1 (X )2 !C (X )2 be such that  8↵   2M 1
u = T(v)  v = T↵(u) on X  
u(xo) = 0.

+(X )

with

rOT"(↵  ) = (u  v) 

(5)
1 (X )  the directional derivative of
(6)

OT0"(↵  ; µ  ⌫) = hrOT"(↵  )  (µ  ⌫)i = hu  µi + hv  ⌫i  

Then  OT" is directionally differentiable and  8↵  ↵0   0 2M +
OT" at (↵  ) along the feasible direction (µ  ⌫) = (↵0  ↵  0  ) is
where hw  ⇢i =R w(x) d⇢(x) denotes the canonical pairing between the spaces C(X ) and M(X ).
Note that rOT" is not a gradient in the standard sense. In particular note that the directional derivative
in (6) is not deﬁned for any pair of signed measures  but only along feasible directions (↵0↵  0).
Sinkhorn Divergence. The fast convergence of Sinkhorn-Knopp algorithm makes OT" (with "> 0)
preferable to OT0 from a computational perspective [13]. However  when "> 0 the entropic
regularization introduces a bias in the optimal transport problem  since in general OT"(µ  µ) 6= 0. To
compensate for this bias  [25] introduced the Sinkhorn divergence
1
2

(↵  ) 7! OT"(↵  ) 

1 (X ) ⇥M +

OT"(↵  ↵) 

1 (X ) ! R 

S" : M+

OT"(   ) 

1
2

(7)

which was shown in [21] to be nonnegative  biconvex and to metrize the convergence in law under
mild assumptions. We characterize the gradient of S"(·  ) for a ﬁxed  2M +
1 (X )  which will be
key to derive our optimization algorithm for computing Sinkhorn barycenters.
Remark 2. Let r1OT" : M+
1 (X )2 !C (X ) denote the ﬁrst component of rOT" (informally the
component u of the Sinkhorn potentials (u  v)). Then  it follows from Prop. 1 and the deﬁnition
of Sinkhorn divergence (7) that for any  2M +
1 (X ) ! R is
directionally differentiable and admits gradient

1 (X ) the function S"(·  ) : M+

r[S"(·  )] : M+

1 (X ) !C (X )

↵ 7! r1OT"(↵  ) 

1
2r1OT"(↵  ↵) = u  p 

(8)

with u = T↵(u) and p = T↵↵(p) the Sinkhorn potentials of OT"(↵  ) and OT"(↵  ↵) respectively
which are zero at xo.

We refer to Appendix C for an in-depth analysis of the directional differentiability properties of the
Sinkorn divergence.

3 Sinkhorn barycenters with Frank-Wolfe

Given 1  . . . m 2M +
goal of this paper is to solve the following Sinkhorn barycenter problem

1 (X ) and !1  . . .  ! m  0 a set of weights such thatPm

j=1 !j = 1  the main

(9)

with

1 (X )

B"(↵) 

B"(↵) =

!j S"(↵  j).

min
↵2M+

mXj=1
+(X ) has empty interior in the space of
Although the objective functional B" is convex  its domain M1
ﬁnite signed measure M(X ). Hence standard notions of Fréchet or Gâteaux differentiability do not
apply. This  in principle causes some difﬁculties in devising optimization methods. To circumvent
this issue  in this work we adopt the Frank-Wolfe (FW) algorithm. Indeed  one key advantage of
this method is that it is formulated in terms of directional derivatives along feasible directions (i.e. 
directions that locally remain inside the constraint set). Building upon [15  16  19]  which study the
algorithm in Banach spaces  we show that the “weak” notion of directional differentiability of S"
(and hence of B") in Remark 2 is sufﬁcient to carry out the convergence analysis. While full details
are provided in Appendix A  below we give an overview of the main result.
Frank-Wolfe in dual Banach spaces. Let W be a real Banach space with topological dual W⇤
and let D⇢W ⇤ be a nonempty  convex  closed and bounded set. For any w 2W ⇤ denote by
FD(w) = R+(D w) the set of feasible direction of D at w (namely s = t(w0  w) with w0 2D
and t > 0). Let G : D! R be a convex function and assume that there exists a map rG : D!W
(not necessarily unique) such that hrG(w)  si = G0(w; s) for every s 2F D(w). In Alg. 1 we present

3

Algorithm 1 FRANK-WOLFE IN DUAL BANACH SPACES
Input: initial w0 2D   precision (k)k2N 2 RN
For k = 0  1  . . .

++  such that k(k + 2) is nondecreasing.

Take zk+1 such that G0(wk  zk+1  wk)  minz2D G0(wk  z  wk) + k
wk+1 = wk + 2

2

k+2 (zk+1  wk)

a method to minimize G. The algorithm is structurally equivalent to the standard FW [19  27] and
accounts for possible inaccuracies when computing the conditional gradient (i.e. solving the FW
inner minimization). This will be key in Sec. 5 when studying the barycenter problem for j with
inﬁnite support. The following result (see proof in Appendix A) shows that under the additional
assumption that rG is Lipschitz-continuous and with sufﬁciently fast decay of the errors  the above
procedure converges in value to the minimum of G with rate O(1/k). Here diam(D) denotes the
diameter of D with respect to the dual norm.
Theorem 3. Under the assumptions above  suppose in addition that rG is L-Lipschitz continuous
with L > 0. Let (wk)k2N and (k)k2N be deﬁned according to Alg. 1. Then  for every integer k  1 
(10)

2

G(wk)  min
w2D

G(w) 

L diam(D)2 + k.

k + 2

Frank-Wolfe Sinkhorn barycenters. We show that the barycenter problem (9) satisﬁes the setting
and hypotheses of Thm. 3 and can be thus approached via Alg. 1.
Optimization domain. Let W = C(X )  with dual W⇤ = M(X ). The constraint set D = M+
convex  closed  and bounded.
Objective functional. The objective functional G = B" : M+
since it is a convex combination of S"(·  j)  with j = 1 . . . m. The gradient rB" : M+
is rB" =Pm
Lipschitz continuity of the gradient. This is the most critical condition and it is studied in the following
theorem.
Theorem 4. The gradient rOT" deﬁned in Prop. 1 is Lipschitz continuous. In particular  the ﬁrst
component r1OT" is 2"e3D/"-Lipschitz continuous  i.e.  for every ↵  ↵0   0 2M +

1 (X ) is
1 (X ) ! R  deﬁned in (9)  is convex
1 (X ) !C (X )

j=1 !j rS"(·  j)  where rS"(·  j) is given in Remark 2.

1 (X ) 

ku  u0k1 = kr1OT"(↵  )  r1OT"(↵0  0)k1  2"e3D/" (k↵  ↵0kT V + k  0kT V ) 

(11)
where D = supx y2X c(x  y)  u = T↵(u)  u0 = T0 ↵0(u0)  and u(xo) = u0(xo) = 0. Moreover  it
follows from (8) that rS"(·  ) is 6"e3D/"-Lipschitz continuous. The same holds for rB".
Thm. 4 is one of the main contributions of this paper. It can be rephrased by saying that the operator
that maps a pair of distributions to their Sinkhorn potentials is Lipschitz continuous. This result is
signiﬁcantly deeper than the one given in [20  Lemma 1]  which establishes the Lipschitz continuity
of the gradient in the semidiscrete case. The proof (given in Appendix D) relies on non-trivial tools
from Perron-Frobenius theory for Hilbert’s metric [32]  which is a well-established framework to
study Sinkhorn potentials [38]. We believe this result is interesting not only for the application of
FW to the Sinkhorn barycenter problem  but also for further understanding regularity properties of
entropic optimal transport.

4 Algorithm: practical Sinkhorn barycenters

According to Sec. 3  FW is a valid approach to tackle the barycenter problem (9). Here we describe
how to implement in practice the abstract procedure of Alg. 1 to obtain a sequence of distributions
(↵k)k2N minimizing B". A main challenge in this sense resides in ﬁnding a minimizing feasible
direction for B0"(↵k; µ  ↵k) = hrB"(↵k)  µ  ↵ki. According to Remark 2  this amounts to solve
(12)

where

ujk  pk = rS"[(·  j)](↵k) 

!j hujk  pk  µi

µk+1 2 argmin
1 (X )

µ2M+

mXj=1

4

Algorithm 2 SINKHORN BARYCENTER

Input: j = (Yj  bj) with Yj 2 Rd⇥nj   bj 2 Rnj  ! j > 0 for j = 1  . . .   m  x0 2 Rd  "> 0  K 2 N.
Initialize: ↵0 = (X0  a0) with X0 = x0  a0 = 1.
For k = 0  1  . . .   K  1

p = SINKHORNKNOPP(↵k ↵ k " )
p(·) = SINKHORNGRADIENT(Xk  ak  p)
For j = 1  . . . m
vj = SINKHORNKNOPP(↵k  j " )
uj(·) = SINKHORNGRADIENT(Yj  bj  vj)

Let ' : x 7!Pm

j=1 !j uj(x)  p(x)
xk+1 = MINIMIZE(')
Xk+1 = [Xk  xk+1] and ak+1 = 1
↵k+1 = (Xk+1  ak+1)

k+2 [k ak  2]

Return: ↵K

with pk = r1OT"(↵k ↵ k) not depending on j. In general (12) would entail a minimization over
the set of all probability distributions on X . However  since the objective functional is linear in µ
and M+
1 (X ) is a weakly-⇤ compact convex set  we can apply Bauer maximum principle (see e.g. 
[3  Thm. 7.69]). Hence  solutions are achieved at the extreme points of the optimization domain.
These correspond to Dirac’s deltas in the case of M+
1 (X )
the Dirac’s delta centered at x 2X . We have hw  xi = w(x) for every w 2C (X ). Hence (12) is
equivalent to

1 (X ) [11  p. 108]. Denote by x 2M +

µk+1 = xk+1

with

xk+1 2 argmin
x2X

mXj=1

!jujk(x)  pk(x).

(13)

Once the new support point xk+1 has been obtained  the update in Alg. 1 corresponds to

2

k + 2

k

k + 2

2

k + 2

↵k +

xk+1.

↵k+1 = ↵k +

(xk+1  ↵k) =

(14)
If FW is initialized with a Dirac’s delta ↵0 = x0 for some x0 2X   then every further iterate ↵k
will have at most k + 1 support points. According to (13)  the inner optimization for FW consists in
j=1 !jujk(x)  pk(x) over X . In practice  having access to
minimizing the functional x 7!Pm

such functional poses already a challenge  since it requires computing the Sinkhorn potentials ujk
and pk  which are inﬁnite dimensional objects. Below we discuss how to estimate these potentials
when the j have ﬁnite support. We then address the general setting.
Computing r1OT" for probability distributions with ﬁnite support. Let ↵   2M +
1 (X )  where
 =Pn
i=1 nonnegative weights summing up to 1. It is useful to identify 
with the pair (Y  b)  where Y 2 Rd⇥n is the matrix with i-th column equal to yi. Let (u  v) 2C (X )2
be the pair of Sinkhorn potentials associated to ↵ and  in Prop. 1  recall that u = T(v). Denote by
v 2 Rn the evaluation vector of the Sinkhorn potential v  with i-th entry vi = v(yi). According to
the deﬁnition of T in (3)  for any x 2X

i=1 biyi and b = (bi)n

[r1OT"(↵  )](x) = u(x) = [T(v)](x) = " log

e(vic(x yi))/" bi 

(15)

nXi=1

since the integral T(v) reduces to a sum over the support of . Hence  the gradient of OT" (i.e.
the potential u)  is uniquely characterized in terms of the ﬁnite dimensional vector v collecting the
values of the potential v on the support of  . We refer as SINKHORNGRADIENT to the routine which

associates to each triplet (Y  b  v) the map x 7! " logPn
Sinkhorn barycenters: ﬁnite case. Alg. 2 summarizes FW applied to the barycenter problem (9)
when the j’s have ﬁnite support. Starting from a Dirac’s delta ↵0 = x0  at each iteration k 2 N the
algorithm proceeds by: i) ﬁnding the corresponding evaluation vectors vj’s and p of the Sinkhorn
potentials for OT"(↵k  j) and OT"(↵k ↵ k) respectively  via the routine SINKHORNKNOPP (see
[13  21] or Alg. B.2). This is possible since both j and ↵k have ﬁnite support and therefore the

i=1 e(vic(x yi))/" bi.

5

problem of approximating the evaluation vectors vj and p reduces to an optimization problem over
ﬁnite vector spaces that can be efﬁciently solved [13]; ii) obtain the gradients uj = r1OT"(↵k  j)
and p = r1OT"(↵k ↵ k) via SINKHORNGRADIENT; iii) minimize ' : x 7!Pn
j=1 !j uj(x)  p(x)
over X to ﬁnd a new point xk+1 (we comment on this meta-routine MINIMIZE below); iv) ﬁnally
update the support and weights of ↵k according to (14) to obtain the new iterate ↵k+1.
A key feature of Alg. 2 is that the support of the candidate barycenter is updated incrementally
by adding at most one point at each iteration  a procedure similar in ﬂavor to the kernel herding
strategy in [5  31] and conditional gradient for sparse inverse problem [8  9]. This contrasts with
previous methods for barycenter estimation [6  14  20  50]  which require the support set  or at least
its cardinality  to be ﬁxed beforehand. However  indentifying the new support point requires solving
the nonconvex problem (13)  a task addressed by the meta-routine MINIMIZE. This problem is
typically smooth (e.g.  a linear combination of Gaussians when c(x  y) = kx  yk2) and ﬁrst or
second order nonlinear optimization methods can be adopted to ﬁnd stationary points. We note that
all free-support methods in the literature for barycenter estimation are also affected by nonconvexity
since they typically require solving a biconvex problem (alternating minimization between support
points and weights) which is not jointly convex [12  14]. We conclude by observing that if we restrict
to the setting of [20  50] with ﬁxed ﬁnite support set  then MINIMIZE can be solved exactly by
evaluating the functional in (13) on each candidate support point.
Sinkhorn barycenters: general case. When the j’s have inﬁnite support  it is not possible to apply
Sinkhorn-Knopp in practice. In line with [23  50]  we can randomly sample empirical distributions
ˆj = 1
i=1 xij from each j and apply Sinkhorn-Knopp to (↵k  ˆj) in Alg. 1 rather than to the
ideal pair (↵k  j). This strategy is motivated by [21  Prop 13]  where it was shown that Sinkhorn
potentials vary continuously with the input measures. However  it opens two questions: i) whether
this approach is theoretically justiﬁed (consistency) and ii) how many points should we sample from
each j to ensure convergence (rates). We answer these questions in Thm. 7 in the next section.

nPn

5 Convergence analysis

We ﬁnally address the convergence of FW applied to both the ﬁnite and inﬁnite settings discussed in
Sec. 4. We begin by considering the ﬁnite setting.
Theorem 5. Suppose that 1  . . . m 2M +
Alg. 2 applied to (9). Then 

1 (X ) have ﬁnite support and let ↵k be the k-th iterate of

B"(↵k)  min
↵2M+

1 (X )

B"(↵) 

48 "e 3D/"

k + 2

.

(16)

The result follows by the convergence result of FW in Thm. 3 applied with the Lipschitz constant
from Thm. 4  and recalling that diam(M+
1 (X )) = 2 with respect to the Total Variation. Note that
Thm. 5 assumes SINKHORNKNOPP and MINIMIZE in Alg. 2 to yield exact solutions. In Appendix D
we extend of Alg. 2 and Thm. 5 which account for approximation errors in the above routines.
General setting. As mentioned in Sec. 4  when the j’s are not ﬁnitely supported we adopt a
sampling approach. More precisely we propose to replace in Alg. 2 the ideal Sinkhorn potentials of
the pairs (↵  j) with those of (↵  ˆj)  where each ˆj is an empirical measure randomly sampled from
j. In other words we are performing the FW algorithm with a (possibly rough) approximation of
the correct gradient of B". According to Thm. 3  FW allows errors in the gradient estimation (which
are captured into the precision k in the statement). To this end  the following result quantiﬁes the
approximation error between r1OT"(·  ) and r1OT"(·  ˆ) in terms of the sample size of ˆ.
Theorem 6 (Sample Complexity of Sinkhorn Potentials). Suppose that c 2C s+1(X⇥X ) with
s > d/2. Then  there exists a constant r = r(X   c  d) such that for any ↵   2M +
1 (X ) and any
empirical measure ˆ of a set of n points independently sampled from   we have  for every ⌧ 2 (0  1]
(17)

ku  unk1 = kr1OT"(↵  )  r1OT"(↵  ˆ)k1 

8" re3D/" log 3
⌧

pn

with probability at least 1  ⌧  where u = T↵(u)  un = T ˆ↵(un) and u(xo) = un(xo) = 0.

6

Fig. 1: Barycenter of nested ellipses

Fig. 2: Barycenters of Gaussians (see text)

The result in Thm. 6 is of central importance in this work. We point out that it cannot be obtained by
means of the Lipschitz continuity of r1OT" in Thm. 4  since empirical measures do not converge in
k·kT V to their target distribution [17]. Instead  the proof consists in considering the weaker Maximum
Mean Discrepancy (MMD) metric associated to a universal kernel [46]  which metrizes the topology
of the convergence in law of M+
1 (X ) [47]. Empirical measures converge in MMD metric to their
target distribution [46]. Therefore  by proving the Lipschitz continuity of r1OT" with respect to
MMD (see Prop. E.5) we are able to conclude that (17) holds. This latter result relies on regularity
properties of Sinkhorn potentials  which have been recently shown [23  Thm.2] to be uniformly
bounded in Sobolev spaces under the additional assumption c 2C s+1(X⇥X ). For sufﬁciently large
s  the Sobolev norm is in duality with the MMD [35] and allows us to derive the required Lipschitz
continuity. We conclude noting that while [23] studied the sample complexity of the Sinkhorn
divergence  Thm. 6 is a sample complexity result for Sinkhorn potentials. In this sense  we observe
that the constants appearing in the bound are tightly related to those in [23  Thm.3] and have similar
behavior with respect to ". We can now study the convergence of FW in continuous settings.
Theorem 7. Suppose that c 2C s+1(X⇥X ) with s > d/2. Let n 2 N and ˆ1  . . .   ˆm be empirical
distributions with n support points  each independently sampled from 1  . . .   m. Let ↵k be the k-th
iterate of Alg. 2 applied to ˆ1  . . .   ˆm. Then for any ⌧ 2 (0  1]  the following holds with probability
larger than 1  ⌧

64¯r"e3D/" log 3m
⌧

min(k pn)

B"(↵k)  min
↵2M+

1 (X )

B"(↵) 

.

(18)

The proof is shown in Appendix E. A consequence of Thm. 7 is that the accuracy of FW depends
simultaneously on the number of iterations and the sample size used in the approximation of the
gradients: by choosing n = k2 we recover the O(1/k) rate of the ﬁnite setting  while for n = k we
have a rate of O(k1/2)  which is reminiscent of typical sample complexity results  highlighting the
statistical nature of the problem.
Remark 8 (Incremental Sampling). The above strategy requires sampling the empirical distributions
for 1  . . .   m beforehand. A natural question is whether it is be possible to do this incrementally 
sampling new points and updating ˆj accordingly  as the number of FW iterations increase. To this
end  one can perform an intersection bound and see that this strategy is still consistent  but the bound
in Thm. 7 worsens the logarithmic term  which becomes log(3mk/⌧ ).

6 Experiments

In this section we show the performance of our method in a range of experiments. Additional
experiments are provided in the supplementary material. Code has been made publicly available1.
Discrete measures: barycenter of nested ellipses. We compute the barycenter of 30 randomly
generated nested ellipses on a 50 ⇥ 50 grid similarly to [14]. We interpret each image as a probability
distribution in 2D. The cost matrix is given by the squared Euclidean distances between pixels. Fig. 1
reports 8 samples of the input ellipses and the barycenter obtained with Alg. 2. It shows qualitatively
that our approach captures key geometric properties of the input measures.

1 https://github.com/GiulsLu/Sinkhorn-Barycenters

7

Fig. 3: Matching of a 140x140 image. 5000 FW iterations Fig. 4: MNIST k-means (20 centers)

Continuous measures: barycenter of Gaussians. We compute the barycenter of 5 Gaussian
distributions N (mi  Ci) i = 1  . . .   5 in R2  with mean mi 2 R2 and covariance Ci randomly
generated. We apply Alg. 2 to empirical measures obtained by sampling n = 500 points from
each N (mi  Ci)  i = 1  . . .   5. Since the (Wasserstein) barycenter of Gaussian distributions can be
estimated accurately (see [2])  in Fig. 2 we report both the output of our method (as a scatter plot) and
the true Wasserstein barycenter (as level sets of its density). We observe that our estimator recovers
both the mean and covariance of the target barycenter. See the supplementary material for additional
experiments also in the case of mixtures of Gaussians.
Image “compression” via distribution matching. Similarly to [12]  we test Alg. 2 in the special
case of computing the “barycenter” of a single measure  2M 1
+(X ). While the solution of
this problem is the distribution  itself  we can interpret the intermediate iterates ↵k of Alg. 2 as
compressed version of the original measure. In this sense k would represent the level of compression
since ↵k is supported on at most k points. Fig. 3 (Right) reports iteration k = 5000 of Alg. 2 applied
to the 140 ⇥ 140 image in Fig. 3 (Left) interpreted as a probability measure  in 2D. We note that the
number of points in the support is ⇠ 3900: indeed  Alg. 2 selects the most relevant support points
points multiple times to accumulate the right amount of mass on each of them (darker color = higher
weight). This shows that FW tends to greedily search for the most relevant support points  prioritizing
those with higher weight.
k-means on MNIST digits. We tested our algorithm on a k-means clustering experiment. We
consider a subset of 500 random images from the MNIST dataset. Each image is suitably normalized
to be interpreted as a probability distribution on the grid of 28⇥ 28 pixels with values scaled between
0 and 1. We initialize 20 centroids according to the k-means++ strategy [4]. Fig. 4 depicts the 20
centroids obtained by performing k-means with Alg. 2. We see that the structure of the digits is
successfully detected  recovering also minor details (e.g. note the difference between the 2 centroids).
Real data: Sinkhorn propagation of weather data. We consider the problem of Sinkhorn propa-
gation similar to the one in [45]. The goal is to predict the distribution of missing measurements for
weather stations in the state of Texas  US by “propagating” measurements from neighboring stations

in the network. The problem can be formulated as minimizing the functionalP(v u)2V !uvS"(⇢v ⇢ u)
over the set {⇢v 2M +
1 (R2)|v 2V 0} with: V0 ⇢V the subset of stations with missing measurements 
G = (V E) the whole graph of the stations network  !uv a weight inversely proportional to the
geographical distance between two vertices/stations u  v 2V . The variable ⇢v 2M +
1 (R2) denotes
the distribution of measurements at station v of daily temperature and atmospheric pressure over one
year. This is a generalization of the barycenter problem (9) (see also [38]).
From the total |V| = 115  we randomly select 10%  20% or 30% to be available stations  and
use Alg. 2 to propagate their measurements to the remaining “missing” ones. We compare our
approach (FW) with the Dirichlet (DR) baseline in [45] in terms of the error d(CT   ˆC) between the
covariance matrix CT of the groundtruth distribution and that of the predicted one. Here d(A  B) =
klog(A1/2BA1/2)k is the geodesic distance on the cone of positive deﬁnite matrices. The average
prediction errors are: 2.07 (FW)  2.24 (DR) for 10%  1.47 (FW)  1.89(DR) for 20% and 1.3 (FW) 
1.6 (DR) for 30%. Fig. 5 qualitatively reports the improvement = d(CT   CDR)  d(CT   CF W )
of our method on individual stations: a higher color intensity corresponds to a wider gap in our
favor between prediction errors  from light green ( ⇠ 0) to red ( ⇠ 2). Our approach tends to
propagate the distributions to missing locations with higher accuracy.

8

Fig. 5: From Left to Right: propagation of weather data with 10%  20% and 30% stations with
available measurements (see text).

7 Conclusion

We proposed a Frank-Wolfe-based algorithm to ﬁnd the Sinkhorn barycenter of probability distribu-
tions with either ﬁnitely or inﬁnitely many support points. Our algorithm belongs to the family of
barycenter methods with free support since it adaptively identiﬁes support points rather than ﬁxing
them a-priori. In the ﬁnite settings  we were able to guarantee convergence of the proposed algorithm
by proving the Lipschitz continuity of gradient of the barycenter functional in the Total Variation
sense. Then  by studying the sample complexity of Sinkhorn potential estimation  we proved the
convergence of our algorithm also in the inﬁnite case. We empirically assessed our method on a
number of synthetic and real experiments  showing that it exhibits good qualitative and quantitative
performance. While in this work we have considered FW iterates that are a convex combination of
Dirac’s delta  models with higher regularity (e.g. mixture of Gaussians) might be more suited to
approximate the barycenter of distributions with smooth density. Hence  in the future we plan to
investigate whether the perspective adopted in this work could be extended also to other barycenter
estimators.

References
[1] R. A. Adams and J. J. F. Fournier. Sobolev spaces. Elsevier  2003.

[2] M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM J. Math. Analysis 

43(2):904–924  2011.

[3] K. Aliprantis  C. D. and Border. Inﬁnite Dimensional Analysis: a Hitchhiker’s guide. Springer

Science & Business Media  2006.

[4] David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding. In
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms  SODA
’07  pages 1027–1035  Philadelphia  PA  USA  2007. Society for Industrial and Applied Mathe-
matics.

[5] Francis Bach  Simon Lacoste-Julien  and Guillaume Obozinski. On the equivalence between

herding and conditional gradient algorithms. arXiv preprint arXiv:1203.4523  2012.

[6] Jean-David Benamou  Guillaume Carlier  Marco Cuturi  Luca Nenna  and Gabriel Peyré.
Iterative bregman projections for regularized transportation problems. SIAM J. Scientiﬁc
Computing  37(2)  2015.

[7] J Frédéric Bonnans and Alexander Shapiro. Perturbation analysis of optimization problems.

Springer Science & Business Media  2013.

[8] Nicholas Boyd  Geoffrey Schiebinger  and Benjamin Recht. The alternating descent conditional
gradient method for sparse inverse problems. SIAM Journal on Optimization  27(2):616–639 
2017.

[9] Kristian Bredies and Hanna Katriina Pikkarainen. Inverse problems in spaces of measures.

ESAIM: Control  Optimisation and Calculus of Variations  19(1):190–218  2013.

9

[10] Lenaic Chizat  Gabriel Peyré  Bernhard Schmitzer  and François-Xavier Vialard. Scaling algo-
rithms for unbalanced optimal transport problems. Mathematics of Computation  87(314):2563–
2609  2018.

[11] G. Chouquet. Lectures on Analysis  Vol. II. W. A. Bejamin  Inc.  Reading  MA  USA.  1969.

[12] S. Claici  E. Chien  and J. Solomon. Stochastic Wasserstein Barycenters. ArXiv e-prints 

February 2018.

[13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances

in Neural Information Processing Systems  pages 2292–2300  2013.

[14] Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In Eric P.
Xing and Tony Jebara  editors  Proceedings of the 31st International Conference on Machine
Learning  volume 32 of Proceedings of Machine Learning Research  pages 685–693  Bejing 
China  22–24 Jun 2014. PMLR.

[15] V. F. Demyanov and A. M. Rubinov. The minimization of smooth convex functional on a convex

set. J. SIAM Control.  5(2):280–294  1967.

[16] V. F. Demyanov and A. M. Rubinov. Minimization of functionals in normed spaces. J. SIAM

Control.  6(1):73–88  1968.

[17] Luc Devroye  Laszlo Gyorﬁ  et al. No empirical probability measure can converge in the total

variation sense for all distributions. The Annals of Statistics  18(3):1496–1499  1990.

[18] Pierre Dognin  Igor Melnyk  Youssef Mroueh  Jerret Ross  Cicero Dos Santos  and Tom Sercu.

Wasserstein barycenter model ensembling. arXiv preprint arXiv:1902.04999  2019.

[19] Joseph C Dunn and S Harshbarger. Conditional gradient algorithms with open loop step size

rules. Journal of Mathematical Analysis and Applications  62(2):432–444  1978.

[20] Pavel Dvurechenskii  Darina Dvinskikh  Alexander Gasnikov  Cesar Uribe  and Angelia Nedich.
Decentralize and randomize: Faster algorithm for wasserstein barycenters.
In S. Bengio 
H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in
Neural Information Processing Systems 31  pages 10760–10770. Curran Associates  Inc.  2018.

[21] Jean Feydy  Thibault Séjourné  François-Xavier Vialard  Shun-Ichi Amari  Alain Trouvé  and
Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences.
International Conference on Artiﬁcial Intelligence and Statistics (AIStats)  2019.

[22] J. Franklin and J. Lorenz. On the scaling of multidimensional matrices. Linear Algebra and its

Applications  114:717–735  1989.

[23] Aude Genevay  Lénaic Chizat  Francis Bach  Marco Cuturi  and Gabriel Peyré. Sample
complexity of sinkhorn divergences. International Conference on Artiﬁcial Intelligence and
Statistics (AIStats)  2018.

[24] Aude Genevay  Marco Cuturi  Gabriel Peyré  and Francis Bach. Stochastic optimization for
large-scale optimal transport.
In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and
R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages 3440–3448.
Curran Associates  Inc.  2016.

[25] Aude Genevay  Gabriel Peyré  and Marco Cuturi. Learning generative models with sinkhorn
divergences. In International Conference on Artiﬁcial Intelligence and Statistics  pages 1608–
1617  2018.

[26] Alexandre Gramfort  Gabriel Peyré  and Marco Cuturi. Fast optimal transport averaging of
neuroimaging data. In International Conference on Information Processing in Medical Imaging 
pages 261–272. Springer  2015.

[27] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1) 

pages 427–435  2013.

10

[28] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Interna-

tional Conference on Machine Learning  pages 427–435  2013.

[29] L Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk USSR  1942.

[30] Paul Knopp and Richard Sinkhorn. A note concerning simultaneous integral equations. Cana-

dian Journal of Mathematics  20:855–861  1968.

[31] Simon Lacoste-Julien  Fredrik Lindsten  and Francis Bach. Sequential kernel herding: Frank-

wolfe optimization for particle ﬁltering. arXiv preprint arXiv:1501.02056  2015.

[32] Bas Lemmens and Roger Nussbaum. Nonlinear Perron-Frobenius Theory  volume 189. Cam-

bridge University Press  2012.

[33] Bas Lemmens and Roger Nussbaum. Birkhoff’s version of Hilbert’s metric and its applications

in analysis. arXiv preprint arXiv:1304.7921  2013.

[34] M. V Menon. Reduction of a matrix with positive elements to a doubly stochastic matrix. Proc.

Amer. Math. Soc.  18:244–247  1967.

[35] Krikamol Muandet  Kenji Fukumizu  Bharath Sriperumbudur  Bernhard Schölkopf  et al. Kernel
mean embedding of distributions: A review and beyond. Foundations and Trends R in Machine
Learning  10(1-2):1–141  2017.

[36] Roger Nussbaum. Hilbert’s projective metric and iterated nonlinear maps. Mem. Amer. Math.

Soc.  391:1–137  1988.

[37] Roger Nussbaum. Entropy minimization  Hilbert’s projective metric and scaling integral kernels.

Journal of Functional Analysis  115:45–99  1993.

[38] Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends R

in Machine Learning  11(5-6):355–607  2019.

[39] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals

of Probability  pages 1679–1706  1994.

[40] Julien Rabin  Gabriel Peyré  Julie Delon  and Marc Bernot. Wasserstein barycenter and its
application to texture mixing. In International Conference on Scale Space and Variational
Methods in Computer Vision  pages 435–446. Springer  2011.

[41] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic

matrices. Paciﬁc J. Math.  21(2):343–348  1967.

[42] Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their

approximations. Constructive approximation  26(2):153–172  2007.

[43] Alex Smola  Arthur Gretton  Le Song  and Bernhard Schölkopf. A Hilbert space embedding
for distributions. In International Conference on Algorithmic Learning Theory  pages 13–31.
Springer  2007.

[44] Justin Solomon  Fernando De Goes  Gabriel Peyré  Marco Cuturi  Adrian Butscher  Andy
Nguyen  Tao Du  and Leonidas Guibas. Convolutional wasserstein distances: Efﬁcient optimal
transportation on geometric domains. ACM Transactions on Graphics (TOG)  34(4):66  2015.

[45] Justin Solomon  Raif M. Rustamov  Leonidas Guibas  and Adrian Butscher. Wasserstein
propagation for semi-supervised learning. In Proceedings of the 31st International Conference
on International Conference on Machine Learning - Volume 32  ICML’14  pages I–306–I–314.
JMLR.org  2014.

[46] Le Song. Learning via Hilbert space embedding of distributions. 2008.

[47] Bharath K Sriperumbudur  Kenji Fukumizu  and Gert RG Lanckriet. Universality  charac-
teristic kernels and RKHS embedding of measures. Journal of Machine Learning Research 
12(Jul):2389–2410  2011.

11

[48] Bharath K Sriperumbudur  Arthur Gretton  Kenji Fukumizu  Bernhard Schölkopf  and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine
Learning Research  11(Apr):1517–1561  2010.

[49] Sanvesh Srivastava  Cheng Li  and David B Dunson. Scalable bayes via barycenter in wasserstein

space. The Journal of Machine Learning Research  19(1):312–346  2018.

[50] Matthew Staib  Sebastian Claici  Justin M Solomon  and Stefanie Jegelka. Parallel streaming
wasserstein barycenters. In Advances in Neural Information Processing Systems  pages 2647–
2658  2017.

[51] Elias M Stein. Singular integrals and differentiability properties of functions (PMS-30)  vol-

ume 30. Princeton university press  2016.

[52] C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften.

Springer Berlin Heidelberg  2008.

[53] Holger Wendland. Scattered data approximation  volume 17. Cambridge university press  2004.
[54] J. Ye  P. Wu  J. Z. Wang  and J. Li. Fast discrete distribution clustering using wasserstein
barycenter with sparse support. IEEE Transactions on Signal Processing  65(9):2317–2332 
May 2017.

[55] VV Yurinskii. Exponential inequalities for sums of random vectors. Journal of multivariate

analysis  6(4):473–499  1976.

12

,Giulia Luise
Saverio Salzo
Massimiliano Pontil
Carlo Ciliberto