2018,Representer Point Selection for Explaining Deep Neural Networks,We propose to explain the predictions of a deep neural network  by pointing to the set of what we call representer points in the training set  for a given test point prediction. Specifically  we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points  with the weights corresponding to what we call representer values  which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points  and negative values corresponding to inhibitory points  which as we show provides considerably more insight. Our method is also much more scalable  allowing for real-time feedback in a manner not feasible with influence functions.,Representer Point Selection for

Explaining Deep Neural Networks

Chih-Kuan Yeh∗

Joon Sik Kim ∗

Ian E.H. Yen
Machine Learning Department
Carnegie Mellon University

Pradeep Ravikumar

{cjyeh  joonsikk  eyan  pradeepr}@cs.cmu.edu

Pittsburgh  PA 15213

Abstract

We propose to explain the predictions of a deep neural network  by pointing to the
set of what we call representer points in the training set  for a given test point pre-
diction. Speciﬁcally  we show that we can decompose the pre-activation prediction
of a neural network into a linear combination of activations of training points  with
the weights corresponding to what we call representer values  which thus capture
the importance of that training point on the learned parameters of the network. But
it provides a deeper understanding of the network than simply training point inﬂu-
ence: with positive representer values corresponding to excitatory training points 
and negative values corresponding to inhibitory points  which as we show provides
considerably more insight. Our method is also much more scalable  allowing for
real-time feedback in a manner not feasible with inﬂuence functions.

1

Introduction

As machine learning systems start to be more widely used  we are starting to care not just about the
accuracy and speed of the predictions  but also why it made its speciﬁc predictions. While we need not
always care about the why of a complex system in order to trust it  especially if we observe that the
system has high accuracy  such trust typically hinges on the belief that some other expert has a richer
understanding of the system. For instance  while we might not know exactly how planes ﬂy in the air 
we trust some experts do. In the case of machine learning models however  even machine learning
experts do not have a clear understanding of why say a deep neural network makes a particular
prediction. Our work proposes to address this gap by focusing on improving the understanding of
experts  in addition to lay users. In particular  expert users could then use these explanations to further
ﬁne-tune the system (e.g. dataset/model debugging)  as well as suggest different approaches for
model training  so that it achieves a better performance.
Our key approach to do so is via a representer theorem for deep neural networks  which might be of
independent interest even outside the context of explainable ML. We show that we can decompose
the pre-activation prediction values into a linear combination of training point activations  with
the weights corresponding to what we call representer values  which can be used to measure the
importance of each training point has on the learned parameter of the model. Using these representer
values  we select representer points – training points that have large/small representer values – that
could aid the understanding of the model’s prediction.
Such representer points provide a richer understanding of the deep neural network than other ap-
proaches that provide inﬂuential training points  in part because of the meta-explanation underlying
our explanation: a positive representer value indicates that a similarity to that training point is excita-

∗Equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

tory  while a negative representer value indicates that a similarity to that training point is inhibitory 
to the prediction at the given test point. It is in these inhibitory training points where our approach
provides considerably more insight compared to other approaches: speciﬁcally  what would cause
the model to not make a particular prediction? In one of our examples  we see that the model makes
an error in labeling an antelope as a deer. Looking at its most inhibitory training points  we see that
the dataset is rife with training images where there are antelopes in the image  but also some other
animals  and the image is labeled with the other animal. These thus contribute to inhibitory effects
of small antelopes with other big objects: an insight that as machine learning experts  we found
deeply useful  and which is difﬁcult to obtain via other explanatory approaches. We demonstrate the
utility of our class of representer point explanations through a range of theoretical and empirical
investigations.

2 Related Work

There are two main classes of approaches to explain the prediction of a model. The ﬁrst class of
approaches point to important input features. Ribeiro et al. [1] provide such feature-based explanations
that are model-agnostic; explaining the decision locally around a test instance by ﬁtting a local linear
model in the region. Ribeiro et al. [2] introduce Anchors  which are locally sufﬁcient conditions
of features that “holds down” the prediction so that it does not change in a local neighborhood.
Such feature based explanations are particularly natural in computer vision tasks  since it enables
visualizing the regions of the input pixel space that causes the classiﬁer to make certain predictions.
There are numerous works along this line  particularly focusing on gradient-based methods that
provide saliency maps in the pixel space [3  4  5  6].
The second class of approaches are sample-based  and they identify training samples that have the
most inﬂuence on the model’s prediction on a test point. Among model-agnostic sample-based
explanations are prototype selection methods [7  8] that provide a set of “representative” samples
chosen from the data set. Kim et al. [9] provide criticism alongside prototypes to explain what are
not captured by prototypes. Usually such prototype and criticism selection is model-agnostic and
used to accelerate the training for classiﬁcations. Model-aware sample-based explanation identify
inﬂuential training samples which are the most helpful for reducing the objective loss or making the
prediction. Recently  Koh and Liang [10] provide tractable approximations of inﬂuence functions that
characterize the inﬂuence of each sample in terms of change in the loss. Anirudh et al. [11] propose a
generic approach to inﬂuential sample selection via a graph constructed using the samples.
Our approach is based on a representer theorem for deep neural network predictions. Representer
theorems [12] in machine learning contexts have focused on non-parametric regression  speciﬁcally
in reproducing kernel Hilbert spaces (RKHS)  and which loosely state that under certain conditions
the minimizer of a loss functional over a RKHS can be expressed as a linear combination of
kernel evaluations at training points. There have been recent efforts at leveraging such insights
to compositional contexts [13  14]  though these largely focus on connections to non-parametric
estimation. Bohn et al. [13] extend the representer theorem to compositions of kernels  while Unser
[14] draws connections between deep neural networks to such deep kernel estimation  speciﬁcally
deep spline estimation. In our work  we consider the much simpler problem of explaining pre-
activation neural network predictions in terms of activations of training points  which while less
illuminating from a non-parametric estimation standpoint  is arguably much more explanatory  and
useful from an explainable ML standpoint.

3 Representer Point Framework
Consider a classiﬁcation problem  of learning a mapping from an input space X ⊆ Rd (e.g.  images)
to an output space Y ⊆ R (e.g.  labels)  given training points x1  x2  ...xn  and corresponding
labels y1  y2  ...yn. We consider a neural network as our prediction model  which takes the form
ˆyi = σ(Φ(xi  Θ)) ⊆ Rc  where Φ(xi  Θ) = Θ1fi ⊆ Rc and fi = Φ2(xi  Θ2) ⊆ Rf is the last
intermediate layer feature in the neural network for input xi. Note that c is the number of classes 
f is the dimension of the feature  Θ1 is a matrix ⊆ Rc×f   and Θ2 is all the parameters to generate
the last intermediate layer from the input xi. Thus Θ = {Θ1  Θ2} are all the parameters of our
neural network model. The parameterization above connotes splitting of the model as a feature model
Φ2(xi  Θ2) and a prediction network with parameters Θ1. Note that the feature model Φ2(xi  Θ2)

2

(cid:8) 1

n

(cid:80)n
Φ(xt  Θ∗) = (cid:80)n

i L(xi  yi  Θ) + g(||Θ||)(cid:9) for some non-
(cid:80)n

can be arbitrarily deep  or simply the identity function  so our setup above is applicable to general
feed-forward networks.
Our goal is to understand to what extent does one particular training point xi affect the prediction
ˆyt of a test point xt as well as the learned weight parameter Θ. Let L(x  y  Θ) be the loss  and
i L(xi  yi  Θ) be the empirical risk. To indicate the form of a representer theorem  suppose we
1
n
solve for the optimal parameters Θ∗ = arg minΘ
decreasing g. We would then like our pre-activation predictions Φ(xt  Θ) to have the decomposition:
i αik(xt  xi). Given such a representer theorem  αik(xt  xi) can be seen as the
contribution of the training data xi on the testing prediction Φ(xt  Θ). However  such representer
theorems have only been developed for non-parametric predictors  speciﬁcally where Φ lies in a
reproducing kernel Hilbert space. Moreover  unlike the typical RKHS setting  ﬁnding a global
minimum for the empirical risk of a deep network is difﬁcult  if not impossible  to obtain. In the
following  we provide a representer theorem that addresses these two points: it holds for deep neural
networks  and for any stationary point solution.
Theorem 3.1. Let us denote the neural network prediction function by ˆyi = σ(Φ(xi  Θ))  where
Φ(xi  Θ) = Θ1fi and fi = Φ2(xi  Θ2). Suppose Θ∗ is a stationary point of the optimization
problem: arg minΘ
0. Then we have the decomposition:

i L(xi  yi  Θ)) + g(||Θ1||)(cid:9)  where g(||Θ1||) = λ||Θ1||2 for some λ >
(cid:80)n

(cid:8) 1

n

n(cid:88)

Φ(xt  Θ∗) =

k(xt  xi  αi) 

where αi = 1−2λn
given xt.

∂L(xi yi Θ)

∂Φ(xi Θ) and k(xt  xi  αi) = αif T

i ft  which we call a representer value for xi

i

n(cid:88)

i=1

n(cid:88)

n(cid:88)

Proof. Note that for any stationary point  the gradient of the loss with respect to Θ1 is equal to 0.
We therefore have

∂L(xi  yi  Θ)

i=1

∂Θ1

+ 2λΘ∗

1 = 0 ⇒ Θ∗

1 = − 1
2λn

∂L(xi  yi  Θ)

i=1

∂Θ1

=

αif T
i

(1)

n(cid:88)

1
n

where αi = − 1

2λn

∂L(xi yi Θ)

∂Φ(xi Θ) by the chain rule. We thus have that

Φ(xt  Θ∗) = Θ∗

1ft =

k(xt  xi  αi) 

(2)

where k(xt  xi  αi) = αif T

i ft by simply plugging in the expression (1) into (2).

i=1

1jft =(cid:80)n

We note that αi can be seen as the resistance for training example feature fi towards minimizing the
norm of the weight matrix Θ1. Therefore  αi can be used to evaluate the importance of the training
data xi have on Θ1. Note that for any class j  Φ(xt  Θ∗)j = Θ∗
i=1 k(xt  xi  αi)j holds by
(2). Moreover  we can observe that for k(xt  xi  αi)j to have a signiﬁcant value  two conditions must
i ft should have a large value. Therefore  we
be satisﬁed: (a) αij should have a large value  and (b) f T
interpret the pre-activation value Φ(xt  Θ)j as a weighted sum for the feature similarity f T
i ft with
the weight αij. When ft is close to fi with a large positive weight αij  the prediction score for class j
is increased. On the other hand  when ft is close to fi with a large negative weight αij  the prediction
score for class j is then decreased.
We can thus interpret the training points with negative representer values as inhibitory points that
suppress the activation value  and those with positive representer values as excitatory examples that
does the opposite. We demonstrate this notion with examples further in Section 4.2. We note that
such excitatory and inhibitory points provide a richer understanding of the behavior of the neural
network: it provides insight both as to why the neural network prefers a particular prediction  as well
as why it does not  which is typically difﬁcult to obtain via other sample-based explanations.

3

3.1 Training an Interpretable Model by Imposing L2 Regularization.

Theorem 3.1 works for any model that performs a linear matrix multiplication before the activation
σ  which is quite general and can be applied on most neural-network-like structures. By simply
introducing a L2 regularizer on the weight with a ﬁxed λ > 0  we can easily decompose the pre-
softmax prediction value as some ﬁnite linear combinations of a function between the test and train
data. We now state our main algorithm. First we solve the following optimization problem:

Θ∗ = arg min

Θ

1
n

L(yi  Φ(xi  Θ)) + λ||Θ1||2.

(3)

n(cid:88)

i

can then decompose Φ(xt  Θ) =(cid:80)n

Note that for the representer point selection to work  we would need to achieve a stationary point
with high precision. In practice  we ﬁnd that using a gradient descent solver with line search or
LBFGS solver to ﬁne-tune after converging in SGD can achieve highly accurate stationary point.
Note that we can perform the ﬁne-tuning step only on Θ1  which is usually efﬁcient to compute. We
i k(xt  xi  αi) by Theorem 3.1 for any arbitrary test point xt 
where k(xt  xi  αi) is the contribution of training point xi on the pre-softmax prediction Φ(xt  Θ).
We emphasize that imposing L2 weight decay is a common practice to avoid overﬁtting for deep
neural networks  which does not sacriﬁce accuracy while achieving a more interpretable model.

3.2 Generating Representer Points for a Given Pre-trained Model.

We are also interested in ﬁnding representer points for a given model Φ(Θgiven) that has already
been trained  potentially without imposing the L2 regularizer. While it is possible to add the L2
regularizer and retrain the model  the retrained model may converge to a different stationary point 
and behave differently compared to the given model  in which case we cannot use the resulting
representer points as explanations. Accordingly  we learn the parameters Θ while imposing the L2
regularizer  but under the additional constraint that Φ(xi  Θ) be close to Φ(xi  Θgiven). In this case 
our learning objective becomes Φ(xi  Θgiven) instead of yi  and our loss L(xi  yi  Θ) can be written
as L(Φ(xi  Θgiven)  Φ(xi  Θ)).
Deﬁnition 3.1. We say that a convex loss function L(Φ(xi  Θgiven)  Φ(xi  Θ)) is “suitable” to an
activation function σ  if it holds that for any Θ∗ ∈ arg minΘ L(Φ(xi  Θgiven)  Φ(xi  Θ))  we have
σ(Φ(xi  Θ∗)) = σ(Φ(xi  Θgiven)).
Assume that we are given such a loss function L that is “suitable to” the activation function σ. We
can then solve the following optimization problem:

(cid:41)

L(Φ(xi  Θgiven)  Φ(xi  Θ)) + λ||Θ1||2

.

(4)

(cid:40)

n(cid:88)

i

Θ∗ ∈ arg min

Θ

1
n

The optimization problem can be seen to be convex under the assumptions on the loss function. The
parameter λ > 0 controls the trade-off between the closeness of σ(Φ(X  Θ)) and σ(Φ(X  Θgiven)) 
and the computational cost. For a small λ  σ(Φ(X  Θ)) could be arbitrarily close to σ(Φ(X  Θgiven)) 
while the convergence time may be long. We note that the learning task in Eq. (4) can be seen as
learning from a teacher network Θgiven and imposing a regularizer to make the student model Θ
capable of generating representer points. In practice  we may take Θgiven as an initialization for
Θ and perform a simple line-search gradient descent with respect to Θ1 in (4). In our experiments 
we discover that the training for (4) can converge to a stationary point in a short period of time  as
demonstrated in Section 4.5.
We now discuss our design for the loss function that is mentioned in (4). When σ is the soft-
max activation  we choose the softmax cross-entropy loss  which computes the cross entropy
between σ(Φ(xi  Θgiven)) and σ(Φ(xi  Θ)) for Lsoftmax(Φ(xi  Θgiven)  Φ(xi  Θ)). When σ is
2 max(Φ(xi  Θ)  0)(cid:12) Φ(xi  Θ)−
ReLU activation  we choose LReLU(Φ(xi  Θgiven)  Φ(xi  Θ)) = 1
max(Φ(xi  Θgiven)  0) (cid:12) Φ(xi  Θ)  where (cid:12) is the element-wise product. In the following Propo-
sition  we show that Lsoftmax and LReLU are convex  and satisfy the desired suitability property in
Deﬁnition 3.1. The proof is provided in the supplementary material.
Proposition 3.1. The loss functions Lsoftmax and LReLU are both convex in Θ1. Moreover  Lsoftmax
is “suitable to” the softmax activation  and LReLU is “suitable to” the ReLU activation  following
Deﬁnition 3.1.

4

Figure 1: Pearson correlation between the actual and approximated softmax output (expressed as
a linear combination) for train (left) and test (right) data in CIFAR-10 dataset. The correlation is
almost 1 for both cases.

calculate Φ(xt  Θ∗) =(cid:80)n
predicted output σ((cid:80)n

As a sanity check  we perform experiments on the CIFAR-10 dataset [15] with a pre-trained VGG-16
network [16]. We ﬁrst solve (4) with loss Lsoftmax(Φ(xi  Θ)  Φ(xi  Θgiven)) for λ = 0.001  and then
i=1 k(xt  xi  αi) as in (2) for all train and test points. We note that the
computation time for the whole procedure only takes less than a minute  given the pre-trained model.
We compute the Pearson correlation coefﬁcient between the actual output σ(Φ(xt  Θ)) and the
i=1 k(xt  xi  αi)) for multiple points and plot them in Figure 1. The correlation

is almost 1 for both train and test data  and most points lie at the both ends of y = x line.
We note that Theorem 3.1 can be applied to any hidden layer with ReLU activation by deﬁning a
sub-network from input x and the output being the hidden layer of interest. The training could be
done in a similar fashion by replacing Lsoftmax with LReLU. In general  any activation can be used
with a derived "suitable loss".

4 Experiments

We perform a number of experiments with multiple datasets and evaluate our method’s performance
and compare with that of the inﬂuence functions.2 The goal of these experiments is to demonstrate
that selecting the representer points is efﬁcient and insightful in several ways. Additional experi-
ments discussing the differences between our method and the inﬂuence function are included in the
supplementary material.

4.1 Dataset Debugging

Figure 2: Dataset debugging performance for several methods. By inspecting the training points
using the representer value  we are able to recover the same amount of mislabeled training points as
the inﬂuence function (right) with the highest test accuracy compared to other methods (left).

2Source code available at github.com/chihkuanyeh/Representer_Point_Selection.

5

To evaluate the inﬂuence of the samples  we consider a scenario where humans need to inspect the
dataset quality to ensure an improvement of the model’s performance in the test data. Real-world
data is bound to be noisy  and the bigger the dataset becomes  the more difﬁcult it will be for humans
to look for and ﬁx mislabeled data points. It is crucial to know which data points are more important
than the others to the model so that prioritizing the inspection can facilitate the debugging process.
To show how well our method does in dataset debugging  we run a simulated experiment on CIFAR-
10 dataset [17] with a task of binary classiﬁcation with logistic regression for the classes automobiles
and horses. The dataset is initially corrupted  where 40 percent of the data has the labels ﬂipped 
which naturally results in a low test accuracy of 0.55. The simulated user will check some fraction of
the train data based on the order set by several metrics including ours  and ﬁx the labels. With the
corrected version of the dataset  we retrain the model and record the test accuracies for each metrics.
For our method  we train an explainable model by mimimizing (3) as explained in section 3.1. The
L2 weight decay is set to 1e−2 for all methods for fair comparison. All experiments are repeated for
5 random splits and we report the average result. In Figure 2 we report the results for four different
metrics: “ours” picks the points with bigger |αij| for training instance i and its corresponding label j;
“inﬂuence” prioritizes the training points with bigger inﬂuence function value; and “random” picks
random points. We observe that our method recovers the same amount of training data as the inﬂuence
function while achieving higher testing accuracy. Nevertheless  both methods perform better than the
random selection method.

4.2 Excitatory (Positive) and Inhibitory (Negative) Examples

We visualize the training points with high representer values (both positive and negative) for some
test points in Animals with Attributes (AwA) dataset [18] and compare the results with those of the
inﬂuence functions. We use a pre-trained Resnet-50 [19] model and ﬁne-tune on the AwA dataset to
reach over 90 percent testing accuracy. We then generate representer points as described in section
3.2. For computing the inﬂuence functions  just as described in [10]  we froze all top layers of the
model and trained the last layer. We report top three points for two test points in the following
Figures 3 and 4. In Figure 3  which is an image of three grizzly bears  our method correctly returns
three images that are in the same class with similar looks  similar to the results from the inﬂuence
function. The positive examples excite the activation values for a particular class and supports the
decision the model is making. For the negative examples  just like the inﬂuence functions  our method
returns images that look like the test image but are labeled as a different class. In Figure 4  for the
image of a rhino the inﬂuence function could not recover useful training points  while ours does 
including the similar-looking elephants or zebras which might be confused as rhinos  as negatives.
The negative examples work as inhibitory examples for the model – they suppress the activation
values for a particular class of a given test point because they are in a different class despite their
striking similarity to the test image. Such inhibitory points thus provide a richer understanding  even
to machine learning experts  of the behavior of deep neural networks  since they explicitly indicate
training points that lead the network away from a particular label for the given test point. More
examples can be found in the supplementary material.

Figure 3: Comparison of top three positive and negative inﬂuential training images for a test point
(left-most column) using our method (left columns) and inﬂuence functions (right columns).

6

OursInfluence FunctionFigure 4: Here we can observe that our method provides clearer positive and negative examples while
the inﬂuence function fails to do so.

4.3 Understanding Misclassiﬁed Examples

The representer values can be used to understand the model’s mistake on a test image. Consider a test
image of an antelope predicted as a deer in the left-most panel of Figure 5. Among 181 test images
of antelopes  the total number of misclassiﬁed instances is 15  among which 12 are misclassiﬁed as
deer. All of those 12 test images of antelopes had the four training images shown in Figure 5 among
the top inhibitory examples. Notice that we can spot antelopes even in the images labeled as zebra
or elephant. Such noise in the labels of the training data confuses the model – while the model sees
elephant and antelope  the label forces the model to focus on just the elephant. The model thus learns
to inhibit the antelope class given an image with small antelopes and other large objects. This insight
suggests for instance that we use multi-label prediction to train the network  or perhaps clean the
dataset to remove such training examples that would be confusing to humans as well. Interestingly 
the model makes the same mistake (predicting deer instead of antelope) on the second training image
shown (third from the left of Figure 5)  and this suggests that for the training points  we should
expect most of the misclassiﬁcations to be deer as well. And indeed  among 863 training images of
antelopes  8 are misclassiﬁed  and among them 6 are misclassiﬁed as deer.

Figure 5: A misclassiﬁed test image (left) and the set of four training images that had the most
negative representer values for almost all test images in which the model made the same mistakes.
The negative inﬂuential images all have antelopes in the image despite the label being a different
animal.

4.4 Sensitivity Map Decomposition

Φ(xt  Θ∗) =(cid:80)n

i αif T

∂xt

=(cid:80)n

From Theorem 3.1  we have seen that the pre-softmax output of the neural network can be decomposed
as the weighted sum of the product of the training point feature and the test point feature  or
i ft. If we take the gradient with respect to the test input xt for both sides 
we get ∂Φ(xt Θ∗)
. Notice that the LHS is the widely-used notion of sensitivity map
i αi
(gradient-based attribution)  and the RHS suggests that we can decompose this sensitivity map into a
weighted sum of sensitivity maps that are native to each i-th training point. This gives us insight into
how sensitivities of training points contribute to the sensitivity of the given test image.
In Figure 6  we demonstrate two such examples  one from the class zebra and one from the class
moose from the AwA dataset. The ﬁrst column shows the test images whose sensitivity maps we wish
to decompose. For each example  in the following columns we show top four inﬂuential representer

∂f T
i ft
∂xt

7

OursInfluence Functionpoints in the the top row  and visualize the decomposed sensitivity maps in the bottom. We used
SmoothGrad [20] to obtain the sensitivity maps.
For the ﬁrst example of a zebra  the sensitivity map on the test image mainly focuses on the face of the
zebra. This means that inﬁnitesimally changing the pixels around the face of the zebra would cause
the greatest change in the neuron output. Notice that the focus on the head of the zebra is distinctively
the strongest in the fourth representer point (last column) when the training image manifests clearer
facial features compared to other training points. For the rest of the training images that are less
demonstrative of the facial features  the decomposed sensitivity maps accordingly show relatively
higher focus on the background than on the face. For the second example of a moose  a similar
trend can be observed – when the training image exhibits more distinctive bodily features of the
moose than the background (ﬁrst  second  third representer points)  the decomposed sensitivity map
highlights the portion of the moose on the test image more compared to training images with more
features of the background (last representer point). This provides critical insight into the contribution
of the representer points towards the neuron output that might not be obvious just from looking at the
images itself.

Figure 6: Sensitivity map decomposition using representer points  for the class zebra (above two
rows) and moose (bottom two rows). The sensitivity map on the test image in the ﬁrst column can be
readily seen as the weighted sum of the sensitivity maps for each training point. The less the training
point displays spurious features from the background and more of the features related to the object of
interest  the more focused the decomposed sensitivity map corresponding to the training point is at
the region the test sensitivity map mainly focuses on.

4.5 Computational Cost and Numerical Instabilities

Computation time is particularly an issue for computing the inﬂuence function values [10] for a large
dataset  which is very costly to compute for each test point. We randomly selected a subset of test
points  and report the comparison of the computation time in Table 1 measured on CIFAR-10 and
AwA datasets. We randomly select 50 test points to compute the values for all train data  and recorded
the average and standard deviation of computation time. Note that the inﬂuence function does not
need the ﬁne-tuning step when given a pre-trained model  hence the values being 0  while our method

8

ﬁrst optimizes for Θ∗ using line-search then computes the representer values. However  note that the
ﬁne-tuning step is a one time cost  while the computation time is spent for every testing image we
analyze. Our method signiﬁcantly outperforms the inﬂuence function  and such advantage will favor
our method when a larger number of data points is involved. In particular  our approach could be
used for real-time explanations of test points  which might be difﬁcult with the inﬂuence function
approach.

Inﬂuence Function

Ours

Dataset
CIFAR-10

AwA

Fine-tuning

0
0

Computation
267.08 ± 248.20
172.71 ± 32.63

Fine-tuning
7.09 ± 0.76
12.41 ± 2.37

Computation
0.10 ± 0.08
0.19 ± 0.12

Table 1: Time required for computing an inﬂuence function / representer value for all training points
and a test point in seconds. The computation of Hessian Vector Products for inﬂuence function alone
took longer than our combined computation time.

While ranking the training points according to their inﬂuence function values  we have observed
numerical instabilities  more discussed in the supplementary material. For CIFAR-10  over 30 percent
of the test images had all zero training point inﬂuences  so inﬂuence function was unable to provide
positive or negative inﬂuential examples. The distribution of the values is demonstrated in Figure 7 
where we plot the histogram of the maximum of the absolute values for each test point in CIFAR-10.
Notice that over 300 testing points out of 1 000 lie in the ﬁrst bin for the inﬂuence functions (right).
We checked that all data in the ﬁrst bin had the exact value of 0. Roughly more than 200 points lie in
range [10−40  10−28]  the values which may create numerical instabilities in computations. On the
other hand  our method (left) returns non-trivial and more numerically stable values across all test
points.

Figure 7: The distribution of inﬂuence/representer values for a set of randomly selected 1 000 test
points in CIFAR-10. While ours have more evenly spread out larger values across different test points
(left)  the inﬂuence function values can be either really small or become zero for some points  as seen
in the left-most bin (right).

5 Conclusion and Discussion

In this work we proposed a novel method of selecting representer points  the training examples that
are inﬂuential to the model’s prediction. To do so we introduced the modiﬁed representer theorem
that could be generalized to most deep neural networks  which allows us to linearly decompose the
prediction (activation) value into a sum of representer values. The optimization procedure for learning
these representer values is tractable and efﬁcient  especially when compared against the inﬂuence
functions proposed in [10]. We have demonstrated our method’s advantages and performances on
several large-scale models and image datasets  along with some insights on how these values allow
the users to understand the behaviors of the model.
An interesting direction to take from here would be to use the representer values for data poisoning
just like in [10]. Also to truly see if our method is applicable to several domains other than image
dataset with different types of neural networks  we plan to extend our method to NLP datasets with
recurrent neural networks. The result of a preliminary experiment is included in the supplementary
material.

9

Acknowledgements

We acknowledge the support of DARPA via FA87501720152  and Zest Finance.

References
[1] Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. Why should i trust you?: Explaining
the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  pages 1135–1144. ACM  2016.

[2] Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. Anchors: High-precision model-

agnostic explanations. 2018.

[3] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034 
2013.

[4] Avanti Shrikumar  Peyton Greenside  and Anshul Kundaje. Learning important features through

propagating activation differences. arXiv preprint arXiv:1704.02685  2017.

[5] Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic attribution for deep networks.

arXiv preprint arXiv:1703.01365  2017.

[6] Sebastian Bach  Alexander Binder  Grégoire Montavon  Frederick Klauschen  Klaus-Robert
Müller  and Wojciech Samek. On pixel-wise explanations for non-linear classiﬁer decisions by
layer-wise relevance propagation. PloS one  10(7):e0130140  2015.

[7] Jacob Bien and Robert Tibshirani. Prototype selection for interpretable classiﬁcation. The

Annals of Applied Statistics  pages 2403–2424  2011.

[8] Been Kim  Cynthia Rudin  and Julie A Shah. The bayesian case model: A generative approach
for case-based reasoning and prototype classiﬁcation. In Advances in Neural Information
Processing Systems  pages 1952–1960  2014.

[9] Been Kim  Rajiv Khanna  and Oluwasanmi O Koyejo. Examples are not enough  learn to
criticize! criticism for interpretability. In Advances in Neural Information Processing Systems 
pages 2280–2288  2016.

[10] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions.

In International Conference on Machine Learning  pages 1885–1894  2017.

[11] Rushil Anirudh  Jayaraman J Thiagarajan  Rahul Sridhar  and Timo Bremer. Inﬂuential sample

selection: A graph signal processing approach. arXiv preprint arXiv:1711.05407  2017.

[12] Bernhard Schölkopf  Ralf Herbrich  and Alex J Smola. A generalized representer theorem. In

International conference on computational learning theory  pages 416–426. Springer  2001.

[13] Bastian Bohn  Michael Griebel  and Christian Rieger. A representer theorem for deep kernel

learning. arXiv preprint arXiv:1709.10441  2017.

[14] Michael Unser.

A representer theorem for deep neural networks.

arXiv:1802.09210  2018.

arXiv preprint

[15] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[16] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[17] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[18] Yongqin Xian  Christoph H Lampert  Bernt Schiele  and Zeynep Akata. Zero-shot learning-a
comprehensive evaluation of the good  the bad and the ugly. arXiv preprint arXiv:1707.00600 
2017.

10

[19] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[20] Daniel Smilkov  Nikhil Thorat  Been Kim  Fernanda Viégas  and Martin Wattenberg. Smooth-

grad: removing noise by adding noise. arXiv preprint arXiv:1706.03825  2017.

[21] Andrew L Maas  Raymond E Daly  Peter T Pham  Dan Huang  Andrew Y Ng  and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting
of the association for computational linguistics: Human language technologies-volume 1  pages
142–150. Association for Computational Linguistics  2011.

11

,Chih-Kuan Yeh
Joon Kim
Ian En-Hsu Yen
Pradeep Ravikumar