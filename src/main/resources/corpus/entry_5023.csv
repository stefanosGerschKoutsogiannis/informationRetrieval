2018,SimplE Embedding for Link Prediction in Knowledge Graphs,Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927  Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity  whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable  and certain types of background knowledge can be incorporated into these embeddings through weight tying. 
We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. 
We show empirically that  despite its simplicity  SimplE outperforms several state-of-the-art tensor factorization techniques.
SimplE's code is available on GitHub at https://github.com/Mehran-k/SimplE.,SimplE Embedding for Link Prediction in Knowledge

Graphs

Seyed Mehran Kazemi

University of British Columbia

Vancouver  BC  Canada
smkazemi@cs.ubc.ca

David Poole

University of British Columbia

Vancouver  BC  Canada

poole@cs.ubc.ca

Abstract

Knowledge graphs contain knowledge about the world and provide a structured
representation of this knowledge. Current knowledge graphs contain only a small
subset of what is true in the world. Link prediction approaches aim at predicting
new links for a knowledge graph given the existing links among the entities. Tensor
factorization approaches have proved promising for such link prediction problems.
Proposed in 1927  Canonical Polyadic (CP) decomposition is among the ﬁrst tensor
factorization approaches. CP generally performs poorly for link prediction as it
learns two independent embedding vectors for each entity  whereas they are really
tied. We present a simple enhancement of CP (which we call SimplE) to allow
the two embeddings of each entity to be learned dependently. The complexity
of SimplE grows linearly with the size of embeddings. The embeddings learned
through SimplE are interpretable  and certain types of background knowledge can
be incorporated into these embeddings through weight tying. We prove SimplE
is fully expressive and derive a bound on the size of its embeddings for full
expressivity. We show empirically that  despite its simplicity  SimplE outperforms
several state-of-the-art tensor factorization techniques. SimplE’s code is available
on GitHub at https://github.com/Mehran-k/SimplE.

1

Introduction

During the past two decades  several knowledge graphs (KGs) containing (perhaps probabilistic)
facts about the world have been constructed. These KGs have applications in several ﬁelds including
search  question answering  natural language processing  recommendation systems  etc. Due to the
enormous number of facts that could be asserted about our world and the difﬁculty in accessing and
storing all these facts  KGs are incomplete. However  it is possible to predict new links in a KG
based on the existing ones. Link prediction and several other related problems aiming at reasoning
with entities and relationships are studied under the umbrella of statistical relational learning (SRL)
[12  31  7]. The problem of link prediction for KGs is also known as knowledge graph completion. A
KG can be represented as a set of (head   relation  tail ) triples1. The problem of KG completion can
be viewed as predicting new triples based on the existing ones.
Tensor factorization approaches have proved to be an effective SRL approach for KG completion
[29  4  39  26]. These approaches consider embeddings for each entity and each relation. To predict
whether a triple holds  they use a function which takes the embeddings for the head and tail entities
and the relation as input and outputs a number indicating the predicted probability. Details and
discussions of these approaches can be found in several recent surveys [27  43].

1Triples are complete for relations.

(individual   property  value).

They are sometimes written as (subject  verb  object) or

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

One of the ﬁrst tensor factorization approaches is the canonical Polyadic (CP) decomposition [15].
This approach learns one embedding vector for each relation and two embedding vectors for each
entity  one to be used when the entity is the head and one to be used when the entity is the tail. The
head embedding of an entity is learned independently of (and is unrelated to) its tail embedding. This
independence has caused CP to perform poorly for KG completion [40]. In this paper  we develop a
tensor factorization approach based on CP that addresses the independence among the two embedding
vectors of the entities. Due to the simplicity of our model  we call it SimplE (Simple Embedding).
We show that SimplE: 1- can be considered a bilinear model  2- is fully expressive  3- is capable
of encoding background knowledge into its embeddings through parameter sharing (aka weight
tying)  and 4- performs very well empirically despite (or maybe because of) its simplicity. We also
discuss several disadvantages of other existing approaches. We prove that many existing translational
approaches (see e.g.  [4  17  41  26]) are not fully expressive and we identify severe restrictions on
what they can represent. We also show that the function used in ComplEx [39  40]  a state-of-the-art
approach for link prediction  involves redundant computations.

=(cid:80)d

2 Background and Notation
We represent vectors with lowercase letters and matrices with uppercase letters. Let v  w  x ∈ Rd
be vectors of length d. We deﬁne (cid:104)v  w  x(cid:105) .
j=1 v[j] ∗ w[j] ∗ x[j]  where v[j]  w[j]  and x[j]
represent the jth element of v  w and x respectively. That is  (cid:104)v  w  x(cid:105) .
= (v (cid:12) w) · x where (cid:12)
represents element-wise (Hadamard) multiplication and · represents dot product. I d represents an
identity matrix of size d. [v1; v2; . . . ; vn] represents the concatenation of n vectors v1  v2  . . . and vn.
Let E and R represent the set of entities and relations respectively. A triple is represented as (h  r   t) 
where h ∈ E is the head  r ∈ R is the relation  and t ∈ E is the tail of the triple. Let ζ represent the
set of all triples that are true in a world (e.g.  (paris  capitalOf   france))  and ζ(cid:48) represent the ones
that are false (e.g.  (paris  capitalOf   italy)). A knowledge graph KG is a subset of ζ. A relation
r is reﬂexive on a set E of entities if (e  r   e) ∈ ζ for all entities e ∈ E. A relation r is symmetric
on a set E of entities if (e1   r   e2 ) ∈ ζ ⇐⇒ (e2   r   e1 ) ∈ ζ for all pairs of entities e1  e2 ∈ E 
and is anti-symmetric if (e1   r   e2 ) ∈ ζ ⇐⇒ (e2   r   e1 ) ∈ ζ(cid:48). A relation r is transitive on
a set E of entities if (e1   r   e2 ) ∈ ζ ∧ (e2   r   e3 ) ∈ ζ ⇒ (e1   r   e3 ) ∈ ζ for all e1  e2  e3 ∈ E.
The inverse of a relation r  denoted as r−1  is a relation such that for any two entities ei and ej 
(ei   r   ej ) ∈ ζ ⇐⇒ (ej   r−1   ei ) ∈ ζ.
An embedding is a function from an entity or a relation to one or more vectors or matrices of
numbers. A tensor factorization model deﬁnes two things: 1- the embedding functions for entities
and relations  2- a function f taking the embeddings for h  r and t as input and generating a prediction
of whether (h  r   t) is in ζ or not. The values of the embeddings are learned using the triples in a
KG. A tensor factorization model is fully expressive if given any ground truth (full assignment of
truth values to all triples)  there exists an assignment of values to the embeddings of the entities and
relations that accurately separates the correct triples from incorrect ones.

3 Related Work

Translational Approaches deﬁne additive functions over embeddings. In many translational ap-
proaches  the embedding for each entity e is a single vector ve ∈ Rd and the embedding for each
relation r is a vector vr ∈ Rd(cid:48)
and two matrices Pr ∈ Rd(cid:48)×d and Qr ∈ Rd(cid:48)×d. The dissimilarity
function for a triple (h  r   t) is deﬁned as ||Prvh + vr − Qrvt||i (i.e. encouraging Prvh + vr ≈ Qrvt)
where ||v||i represents norm i of vector v. Translational approaches having this dissimilarity function
usually differ on the restrictions they impose on Pr and Qr. In TransE [4]  d = d(cid:48)  Pr = Qr = I d.
In TransR [22]  Pr = Qr. In STransE [26]  no restrictions are imposed on the matrices. FTransE
[11]  slightly changes the dissimilarity function deﬁning it as ||Prvh + vr − αQrvt||i for a value of
α that minimizes the norm for each triple. In the rest of the paper  we let FSTransE represent the
FTransE model where no restrictions are imposed over Pr and Qr.
Multiplicative Approaches deﬁne product-based functions over embeddings. DistMult [46]  one of
the simplest multiplicative approaches  considers the embeddings for each entity and each relation
to be ve ∈ Rd and vr ∈ Rd respectively and deﬁnes its similarity function for a triple (h  r   t)

2

function of ComplEx for a triple (h  r   t) is deﬁned as Real((cid:80)d

as (cid:104)vh  vr  vt(cid:105). Since DistMult does not distinguish between head and tail entities  it can only
model symmetric relations. ComplEx [39] extends DistMult by considering complex-valued instead
of real-valued vectors for entities and relations. For each entity e  let ree ∈ Rd and ime ∈ Rd
represent the real and imaginary parts of the embedding for e. For each relation r  let rer ∈ Rd
and imr ∈ Rd represent the real and imaginary parts of the embedding for r. Then the similarity
j=1(reh[j] + imh[j]i) ∗ (rer[j] +
imr[j]i) ∗ (ret[j] − imt[j]i))  where Real(α + βi) = α and i2 = −1. One can easily verify that
the function used by ComplEx can be expanded and written as (cid:104)reh  rer  ret(cid:105) + (cid:104)reh  imr  imt(cid:105) +
(cid:104)imh  rer  imt(cid:105) − (cid:104)imh  imr  ret(cid:105). In RESCAL [28]  the embedding vector for each entity e is
ve ∈ Rd and for each relation r is vr ∈ Rd×d and the similarity function for a triple (h  r   t) is
vr · vec(vh ⊗ vt)  where ⊗ represents the outer product of two vectors and vec(.) vectorizes the input
matrix. HolE [32] is a multiplicative model that is isomorphic to ComplEx [14].
Deep Learning Approaches generally use a neural network that learns how the head  relation  and
tail embeddings interact. E-MLP [37] considers the embeddings for each entity e to be a vector
ve ∈ Rd  and for each relation r to be a matrix Mr ∈ R2k×m and a vector vr ∈ Rm. To make a
prediction about a triple (h  r   t)  E-MLP feeds [vh; vt] ∈ R2d into a two-layer neural network whose
weights for the ﬁrst layer are the matrix Mr and for the second layer are vr. ER-MLP [10]  considers
the embeddings for both entities and relations to be single vectors and feeds [vh; vr; vt] ∈ R3d into a
two layer neural network. In [35]  once the entity vectors are provided by the convolutional neural
network and the relation vector is provided by the long-short time memory network  for each triple
the vectors are concatenated similar to ER-MLP and are fed into a four-layer neural network. Neural
tensor network (NTN) [37] combines E-MLP with several bilinear parts (see Subsection 5.4 for a
deﬁnition of bilinear models).

4 SimplE: A Simple Yet Fully Expressive Model

In canonical Polyadic (CP) decomposition [15]  the embedding for each entity e has two vectors
he  te ∈ Rd  and for each relation r has a single vector vr ∈ Rd. he captures e’s behaviour as the
head of a relation and te captures e’s behaviour as the tail of a relation. The similarity function
for a triple (e1   r   e2 ) is (cid:104)he1  vr  te2(cid:105). In CP  the two embedding vectors for entities are learned
independently of each other: observing (e1   r   e2 ) ∈ ζ only updates he1 and te2  not te1 and he2.
Example 1. Let likes(p  m) represent if a person p likes a movie m and acted(m  a) represent who
acted in which movie. Which actors play in a movie is expected to affect who likes the movie. In CP 
observations about likes only update the t vector of movies and observations about acted only update
the h vector. Therefore  what is being learned about movies through observations about acted does
not affect the predictions about likes and vice versa.

SimplE takes advantage of the inverse of relations to address the independence of the two vectors for
each entity in CP. While inverse of relations has been used for other purposes (see e.g.  [20  21  6]) 
using them to address the independence of the entity vectors in CP is a novel contribution.
Model Deﬁnition: SimplE considers two vectors he  te ∈ Rd as the embedding of each entity e
(similar to CP)  and two vectors vr  vr−1 ∈ Rd for each relation r. The similarity function of SimplE
2 ((cid:104)hei  vr  tej(cid:105) + (cid:104)hej   vr−1   tei(cid:105))  i.e. the average of the CP
for a triple (ei   r   ej ) is deﬁned as 1
scores for (ei   r   ej ) and (ej   r−1   ei ). In our experiments  we also consider a different variant 
which we call SimplE-ignr. During training  for each correct (incorrect) triple (ei   r   ej )  SimplE-ignr
updates the embeddings such that each of the two scores (cid:104)hei  vr  tej(cid:105) and (cid:104)hej   vr−1   tei(cid:105) become
larger (smaller). During testing  SimplE-ignr ignores r−1s and deﬁnes the similarity function to be
(cid:104)hei  vr  tej(cid:105).
Learning SimplE Models: To learn a SimplE model  we use stochastic gradient descent with mini-
batches. In each learning iteration  we iteratively take in a batch of positive triples from the KG  then
for each positive triple in the batch we generate n negative triples by corrupting the positive triple. We
use Bordes et al. [4]’s procedure to corrupt positive triples. The procedure is as follows. For a positive
triple (h  r   t)  we randomly decide to corrupt the head or tail. If the head is selected  we replace h in
the triple with an entity h(cid:48) randomly selected from E −{h} and generate the corrupted triple (h(cid:48)  r   t).
If the tail is selected  we replace t in the triple with an entity t(cid:48) randomly selected from E − {t} and
generate the corrupted triple (h  r   t(cid:48)). We generate a labelled batch LB by labelling positive triples as

3

Figure 1: hes and vrs in the proof of Proposition 1.

h(e0)
h(e1)
h(e2)
. . .

h(e|E|−1)

v(r0)
v(r1)
. . .

v(r|R|−1)

0 . . . 0
0 . . . 0
1 . . . 0

0 . . . 0 . . . 1
0 . . . 0 . . . 0
1 . . . 0 . . . 0

1
0
0
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0
1
0
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0

0 . . . 1 . . . 0
0 . . . 0 . . . 0
1 . . . 1 . . . 0

0 . . . 1
1 . . . 1
0 . . . 0

0 . . . 0 . . . 1

0
1
0

0
0
0

1

0 . . . 0

0
1
0

0
0
1

0

1
0
0

0
0
1

0

0
1
0

0
1
0

0

0 . . . 0
0 . . . 0
1 . . . 0

0 . . . 1
0 . . . 0
0 . . . 0

1 . . . 1

(cid:80)
((h r  t) l)∈LB sof tplus(−l· φ(h  r   t)) + λ||θ||2

+1 and negatives as −1. Once we have a labelled batch  following [39] we optimize the L2 regularized
negative log-likelihood of the batch: minθ
2  where
θ represents the parameters of the model (the parameters in the embeddings)  l represents the label
of a triple  φ(h  r   t) represents the similarity score for triple (h  r   t)  λ is the regularization hyper-
parameter  and sof tplus(x) = log(1 + exp(x)). While several previous works (e.g.  TransE  TransR 
STransE  etc.) consider a margin-based loss function  Trouillon and Nickel [38] show that the
margin-based loss function is more prone to overﬁtting compared to log-likelihood.

5 Theoretical Analyses

In this section  we provide some theoretical analyses of SimplE and other existing approaches.

5.1 Fully Expressiveness

The following proposition establishes the full expressivity of SimplE.
Proposition 1. For any ground truth over entities E and relations R containing γ true facts  there
exists a SimplE model with embedding vectors of size min(|E| · |R|  γ + 1) that represents that
ground truth.
Proof. First  we prove the |E| · |R| bound. With embedding vectors of size |E| ∗ |R|  for each entity
ei we let the n-th element of hei = 1 if (n mod |E|) = i and 0 otherwise  and for each relation rj we
let the n-th element of vrj = 1 if (n div |E|) = j and 0 otherwise (see Fig 1). Then for each ei and
rj  the product of hei and vrj is 0 everywhere except for the (j ∗ |E| + i)-th element. So for each
entity ek  we set the (j ∗ |E| + i)-th element of tek to be 1 if (ei   rj   ek ) holds and −1 otherwise.
Now we prove the γ + 1 bound. Let γ be zero (base of the induction). We can have embedding
vectors of size 1 for each entity and relation  setting the value for entities to 1 and for relations to −1.
Then (cid:104)hei   vrj   tek(cid:105) is negative for every entities ei and ek and relation rj. So there exists embedding
vectors of size γ + 1 that represents this ground truth. Let us assume for any ground truth where
γ = n − 1 (1 ≤ n ≤ |R||E|2)  there exists an assignment of values to embedding vectors of size
n that represents that ground truth (assumption of the induction). We must prove for any ground
truth where γ = n  there exists an assignment of values to embedding vectors of size n + 1 that
represents this ground truth. Let (ei   rj   ek ) be one of the n true facts. Consider a modiﬁed ground
truth which is identical to the ground truth with n true facts  except that (ei   rj   ek ) is assigned false.
The modiﬁed ground truth has n − 1 true facts and based on the assumption of the induction  we can
represent it using some embedding vectors of size n. Let q = (cid:104)hei  vrj   tek(cid:105) where hei  vrj and tek
are the embedding vectors that represent the modiﬁed ground truth. We add an element to the end of
all embedding vectors and set it to 0. This increases the vector sizes to n + 1 but does not change
any scores. Then we set the last element of hei to 1  vrj to 1  and tek to q + 1. This ensures that
(cid:104)hei  vrj   tek(cid:105) > 0 for the new vectors  and no other score is affected.

DistMult is not fully expressive as it forces relations to be symmetric. It has been shown in [40] that
ComplEx is fully expressive with embeddings of length at most |E| · |R|. According to the universal
approximation theorem [5  16]  under certain conditions  neural networks are universal approximators
of continuous functions over compact sets. Therefore  we would expect there to be a representation

4

based on neural networks that can approximate any ground truth  but the number of hidden units
might have to grow with the number of triples. Wang et al. [44] prove that TransE is not fully
expressive. Proposition 2 proves that not only TransE but also many other translational approaches
are not fully expressive. The proposition also identiﬁes severe restrictions on what relations these
approaches can represent.
Proposition 2. FSTransE is not fully expressive and has the following restrictions. R1 : If a relation
r is reﬂexive on ∆ ⊂ E  r must also be symmetric on ∆  R2 : If r is reﬂexive on ∆ ⊂ E  r must also
be transitive on ∆  and R3 : If entity e1 has relation r with every entity in ∆ ⊂ E and entity e2 has
relation r with one of the entities in ∆  then e2 must have the relation r with every entity in ∆.

(prs1 + vr) = α2
α3

α1qrs1 = α4qrs1  where α4 = α2α1
α3

Proof. For any entity e and relation r  let pre = Prve and qre = Qrve. For a triple (h  r   t) to hold 
we should ideally have prh + vr = αqrt for some α. We assume s1  s2  s3 and s4 are entities in ∆.
R1 : A relation r being reﬂexive on ∆ implies prs1 + vr = α1qrs1 and prs2 + vr = α2qrs2. Suppose
(s1   r   s2 ) holds as well. Then we know prs1 + vr = α3qrs2. Therefore  prs2 + vr = α2qrs2 =
α2
α3
R2 : A relation r being reﬂexive implies prs1 + vr = α1qrs1  prs2 + vr = α2qrs2  and prs3 +
vr = α3qrs3. Suppose (s1   r   s2 ) and (s2   r   s3 ) hold. Then we know prs1 + vr = α4qrs2 and
prs2 + vr = α5qrs3. We can conclude prs1 + vr = α4qrs2 = α4
α5qrs3 = α6qrs3 
α2
where α6 = α4α5
α2
R3 : Let e2 have relation r with s1. We know pre1 + vr = α1qrs1  pre1 + vr = α2qrs2  and
pre2 + vr = α3qrs1. We can conclude pre2 + vr = α3qrs1 = α3
α2qrs2 = α4qrs2 
α1
where α4 = α3α2
α1
Corollary 1. Other variants of translational approaches such as TransE  FTransE  STransE  TransH
[41]  and TransR [22] also have the restrictions mentioned in Proposition 2.

. The above equality proves (s1   r   s3 ) must hold.

. Therefore  (s2   r   s1 ) must holds.

(prs2 + vr) = α4
α2

(pre1 + vr) = α3
α1

. Therefore  (e2   r   s2 ) must hold.

5.2

Incorporating Background Knowledge into the Embeddings

In SimplE  each element of the embedding vector of the entities can be considered as a feature of the
entity and the corresponding element of a relation can be considered as a measure of how important
that feature is to the relation. Such interpretability allows the embeddings learned through SimplE for
an entity (or relation) to be potentially transferred to other domains. It also allows for incorporating
observed features of entities into the embeddings by ﬁxing one of the elements of the embedding
vector of the observed value. Nickel et al. [30] show that incorporating such features helps reduce the
size of the embeddings.
Recently  incorporating background knowledge into tensor factorization approaches has been the
focus of several studies. Towards this goal  many existing approaches rely on post-processing steps
or add additional terms to the loss function to penalize predictions that violate the background
knowledge [34  42  45  13  9]. Minervini et al. [25] show how background knowledge in terms
of equivalence and inversion can be incorporated into several tensor factorization models through
parameter tying2. Incorporating background knowledge by parameter tying has the advantage of
guaranteeing the predictions follow the background knowledge for all embeddings. In this section  we
show how three types of background knowledge  namely symmetry  anti-symmetry  and inversion  can
be incorporated into the embeddings of SimplE by tying the parameters3 (we ignore the equivalence
between two relations as it is trivial).
Proposition 3. Let r be a relation such that for any two entities ei and ej we have (ei   r   ej ) ∈
ζ ⇐⇒ (ej   r   ei ) ∈ ζ (i.e. r is symmetric). This property of r can be encoded into SimplE by tying
the parameters vr−1 to vr.
Proof. If (ei   r   ej ) ∈ ζ  then a SimplE model makes (cid:104)hei  vr  tej(cid:105) and (cid:104)hej   vr−1  tei(cid:105) positive. By
tying the parameters vr−1 to vr  we can conclude that (cid:104)hej   vr  tei(cid:105) and (cid:104)hei  vr−1  tej(cid:105) also become
positive. Therefore  the SimplE model predicts (ej   r   ei ) ∈ ζ.

2Although their incorporation of inversion into DistMult is not correct as it has side effects.
3Note that such background knowledge can be exerted on some relations selectively and not on the others.

This is different than  e.g.  DistMult which enforces symmetry on all relations.

5

Proposition 4. Let r be a relation such that for any two entities ei and ej we have (ei   r   ej ) ∈
ζ ⇐⇒ (ej   r   ei ) ∈ ζ(cid:48) (i.e. r is anti-symmetric). This property of r can be encoded into SimplE by
tying the parameters vr−1 to the negative of vr.
Proof. If (ei   r   ej ) ∈ ζ  then a SimplE model makes (cid:104)hei  vr  tej(cid:105) and (cid:104)hej   vr−1  tei(cid:105) positive. By
tying the parameters vr−1 to the negative of vr  we can conclude that (cid:104)hej   vr  tei(cid:105) and (cid:104)hei  vr−1  tej(cid:105)
become negative. Therefore  the SimplE model predicts (ej   r   ei ) ∈ ζ(cid:48).
Proposition 5. Let r1 and r2 be two relations such that for any two entities ei and ej we have
(ei   r1   ej ) ∈ ζ ⇐⇒ (ej   r2   ei ) ∈ ζ (i.e. r2 is the inverse of r1). This property of r1 and r2 can be
encoded into SimplE by tying the parameters vr
Proof. If (ei   r1   ej ) ∈ ζ  then a SimplE model makes (cid:104)hei   vr1  tej(cid:105) and (cid:104)hej   vr
By tying the parameters vr
(cid:104)hej   vr2   tei(cid:105) also become positive. Therefore  the SimplE model predicts (ej   r2   ei ) ∈ ζ.

  we can conclude that (cid:104)hei  vr

to vr1 and vr2 to vr

  tei(cid:105) positive.
  tej(cid:105) and

−1
2

to vr1.

−1
1

to vr2 and vr

−1
2

−1
2

−1
1

−1
1

5.3 Time Complexity and Parameter Growth

As described in [3]  to scale to the size of the current KGs and keep up with their growth  a relational
model must have a linear time and memory complexity. Furthermore  one of the important challenges
in designing tensor factorization models is the trade-off between expressivity and model complexity.
Models with many parameters usually overﬁt and give poor performance. While the time complexity
for TransE is O(d) where d is the size of the embedding vectors  adding the projections as in STransE
(through the two relation matrices) increases the time complexity to O(d2). Besides time complexity 
the number of parameters to be learned from data grows quadratically with d. A quadratic time
complexity and parameter growth may arise two issues: 1- scalability problems  2- overﬁtting. Same
issues exist for models such as RESCAL and NTNs that have quadratic or higher time complexities
and parameter growths. DistMult and ComplEx have linear time complexities and the number of
their parameters grow linearly with d.
The time complexity of both SimplE-ignr and SimplE is O(d)  i.e.
linear in the size of vector
embeddings. SimplE-ignr requires one multiplication between three vectors for each triple. This
number is 2 for SimplE and 4 for ComplEx. Thus  with the same number of parameters  SimplE-ignr
and SimplE reduce the computations by a factor of 4 and 2 respectively compared to ComplEx.

5.4 Family of Bilinear Models
Bilinear models correspond to the family of models where the embedding for each entity e is ve ∈ Rd 
for each relation r is Mr ∈ Rd×d (with certain restrictions)  and the similarity function for a triple
(h  r   t) is deﬁned as vT
h Mrvt. These models have shown remarkable performance for link prediction
in knowledge graphs [31]. DistMult  ComplEx  and RESCAL are known to belong to the family of
bilinear models. We show that SimplE (and CP) also belong to this family.
DistMult can be considered a bilinear model which restricts the Mr matrices to be diagonal as
in Fig. 2(a). For ComplEx  if we consider the embedding for each entity e to be a single vector
[ree; ime] ∈ R2d  then it can be considered a bilinear model with its Mr matrices constrained
according to Fig. 2(b). RESCAL can be considered a bilinear model which imposes no constraints on
the Mr matrices. Considering the embedding for each entity e to be a single vector [he; te] ∈ R2d 
CP can be viewed as a bilinear model with its Mr matrices constrained as in Fig 2(c). For a triple
(e1   r   e2 )  multiplying [he1; te1 ] to Mr results in a vector ve1r whose ﬁrst half is zero and whose
second half corresponds to an element-wise product of he1 to the parameters in Mr. Multiplying
ve1r to [he2 ; te2 ] corresponds to ignoring he2 (since the ﬁrst half of ve1r is zeros) and taking the
dot-product of the second half of ve1r with te2. SimplE can be viewed as a bilinear model similar to
CP except that the Mr matrices are constrained as in Fig 2(d). The extra parameters added to the
matrix compared to CP correspond to the parameters in the inverse of the relations.
The constraint over Mr matrices in SimplE is very similar to the constraint in DistMult. vT
h Mr in both
SimplE and DistMult can be considered as an element-wise product of the parameters  except that
the Mrs in SimplE swap the ﬁrst and second halves of the resulting vector. Compared to ComplEx 
SimplE removes the parameters on the main diagonal of Mrs. Note that several other restrictions on

6

Figure 2: The constraints over Mr matrices for bilinear models (a) DistMult  (b) ComplEx  (c) CP 
and (d) SimplE. The lines represent where the parameters are; other elements of the matrices are
constrained to be zero. In ComplEx  the parameters represented by the dashed line is tied to the
parameters represented by the solid line and the parameters represented by the dotted line is tied to
the negative of the dotted-and-dashed line.

the Mr matrices are equivalent to SimplE. Viewing SimplE as a single-vector-per-entity model makes
it easily integrable (or compatible) with other embedding models (in knowledge graph completion 
computer vision and natural language processing) such as [35  47  36].

5.5 Redundancy in ComplEx

As argued earlier  with the same number of parameters  the number of computations in ComplEx
are 4x and 2x more than SimplE-ignr and SimplE. Here we show that a portion of the computations
performed by ComplEx to make predictions is redundant. Consider a ComplEx model with embedding
vectors of size 1 (for ease of exposition). Suppose the embedding vectors for h  r and t are [α1 + β1i] 
[α2 + β2i]  and [α3 + β3i] respectively. Then the probability of (h  r   t) being correct according to
ComplEx is proportional to the sum of the following four terms: 1) α1α2α3  2) α1β2β3  3) β1α2β3 
and 4) −β1β2α3. It can be veriﬁed that for any assignment of (non-zero) values to αis and βis  at
least one of the above terms is negative. This means for a correct triple  ComplEx uses three terms to
overestimate its score and then uses a term to cancel the overestimation.
The following example shows how this redundancy in ComplEx may affect its interpretability:
Example 2. Consider a ComplEx model with embeddings of size 1. Consider entities e1  e2 and e3
with embedding vectors [1 + 4i]  [1 + 6i]  and [3 + 2i] respectively  and a relation r with embedding
vector [1 + i]. According to ComplEx  the score for triple (e1   r   e3 ) is positive suggesting e1
probably has relation r with e3. However the score for triple (e2   r   e3 ) is negative suggesting e2
probably does not have relation r with e3. Since the only difference between e1 and e2 is that the
imaginary part changes from 4 to 6  it is difﬁcult to associate a meaning to these numbers.

6 Experiments and Results

Datasets: We conducted experiments on two standard benchmarks: WN18 a subset of Wordnet [24] 
and FB15k a subset of Freebase [2]. We used the same train/valid/test sets as in [4]. WN18 contains
40  943 entities  18 relations  141  442 train  5  000 validation and 5  000 test triples. FB15k contains
14  951 entities  1  345 relations  483  142 train  50  000 validation  and 59  071 test triples.
Baselines: We compare SimplE with several existing tensor factorization approaches. Our baselines
include canonical Polyadic (CP) decomposition  TransE  TransR  DistMult  NTN  STransE  ER-MLP 
and ComplEx. Given that we use the same data splits and objective function as ComplEx  we report
the results of CP  TransE  DistMult  and ComplEx from [39]. We report the results of TransR and
NTN from [27]  and ER-MLP from [32] for further comparison.
Evaluation Metrics: To measure and compare the performances of different models  for each test
triple (h  r   t) we compute the score of (h(cid:48)  r   t) triples for all h(cid:48) ∈ E and calculate the ranking
rankh of the triple having h  and we compute the score of (h  r   t(cid:48)) triples for all t(cid:48) ∈ E and calculate
the ranking rankt of the triple having t. Then we compute the mean reciprocal rank (MRR) of
these rankings as the mean of the inverse of the rankings: M RR = 1
 
rankt
where tt represents the test triples. MRR is a more robust measure than mean rank  since a single bad
ranking can largely inﬂuence mean rank.

(h r  t)∈tt

(cid:80)

2∗|tt|

1

rankh

+ 1

7

 (a) (b) (c) (d) Table 1: Results on WN18 and FB15k. Best results are in bold.

MRR

Filter
0.075
0.454
0.605
0.822
0.530
0.657
0.712
0.941
0.939
0.942

Raw
0.058
0.335
0.427
0.532
−
0.469
0.528
0.587
0.576
0.588

WN18

1

0.049
0.089
0.335
0.728
−
−
0.626
0.936
0.938
0.939

Hit@

3

0.080
0.823
0.876
0.914
−
−
0.775
0.945
0.940
0.944

MRR

10

0.125
0.934
0.940
0.936
0.661
0.934
0.863
0.947
0.941
0.947

Filter
0.326
0.380
0.346
0.654
0.250
0.543
0.288
0.692
0.700
0.727

Raw
0.152
0.221
0.198
0.242
−

0.252
0.155
0.242
0.237
0.239

FB15k

1

0.219
0.231
0.218
0.546
−
−
0.173
0.599
0.625
0.660

Hit@

3

0.376
0.472
0.404
0.733
−
−
0.317
0.759
0.754
0.773

10

0.532
0.641
0.582
0.824
0.414
0.797
0.501
0.840
0.821
0.838

Model

CP

TransE
TransR
DistMult

NTN

STransE
ER-MLP
ComplEx
SimplE-ignr

SimplE

Bordes et al. [4] identiﬁed an issue with the above procedure for calculating the MRR (hereafter
referred to as raw MRR). For a test triple (h  r   t)  since there can be several entities h(cid:48) ∈ E for which
(h(cid:48)  r   t) holds  measuring the quality of a model based on its ranking for (h  r   t) may be ﬂawed.
That is because two models may rank the test triple (h  r   t) to be second  when the ﬁrst model ranks
a correct triple (e.g.  from train or validation set) (h(cid:48)  r   t) to be ﬁrst and the second model ranks
an incorrect triple (h(cid:48)(cid:48)  r   t) to be ﬁrst. Both these models will get the same score for this test triple
when the ﬁrst model should get a higher score. To address this issue  [4] proposed a modiﬁcation
to raw MRR. For each test triple (h  r   t)  instead of ﬁnding the rank of this triple among triples
(h(cid:48)  r   t) for all h(cid:48) ∈ E (or (h  r   t(cid:48)) for all t(cid:48) ∈ E)  they proposed to calculate the rank among triples
(h(cid:48)  r   t) only for h(cid:48) ∈ E such that (h(cid:48)  r   t) (cid:54)∈ train ∪ valid ∪ test. Following [4]  we call this
measure ﬁltered MRR. We also report hit@k measures. The hit@k for a model is computed as the
percentage of test triples whose ranking (computed as described earlier) is less than or equal k.
Implementation: We implemented SimplE in TensorFlow [1]. We tuned our hyper-parameters over
the validation set. We used the same search grid on embedding size and λ as [39] to make our results
directly comparable to their results. We ﬁxed the maximum number of iterations to 1000 and the
number of batches to 100. We set the learning rate for WN18 to 0.1 and for FB15k to 0.05 and used
adagrad to update the learning rate after each batch. Following [39]  we generated one negative
example per positive example for WN18 and 10 negative examples per positive example in FB15k.
We computed the ﬁltered MRR of our model over the validation set every 50 iterations for WN18
and every 100 iterations for F B15k and selected the iteration that resulted in the best validation
ﬁltered MRR. The best embedding size and λ values on WN18 for SimplE-ignr were 200 and 0.001
respectively  and for SimplE were 200 and 0.03. The best embedding size and λ values on FB15k for
SimplE-ignr were 200 and 0.03 respectively  and for SimplE were 200 and 0.1.

6.1 Entity Prediction Results

Table 1 shows the results of our experiments. It can be viewed that both SimplE-ignr and SimplE do
a good job compared to the existing baselines on both datasets. On WN18  SimplE-ignr and SimplE
perform as good as ComplEx  a state-of-the-art tensor factorization model. On FB15k  SimplE
outperforms the existing baselines and gives state-of-the-art results among tensor factorization
approaches. SimplE (and SimplE-ignr) work especially well on this dataset in terms of ﬁltered MRR
and hit@1  so SimplE tends to do well at having its ﬁrst prediction being correct.
The table shows that models with many parameters (e.g.  NTN and STransE) do not perform well
on these datasets  as they probably overﬁt. Translational approaches generally have an inferior
performance compared to other approaches partly due to their representation restrictions mentioned
in Proposition 2. As an example for the friendship relation in FB15k  if an entity e1 is friends
with 20 other entities and another entity e2 is friends with only one of those 20  then according to
Proposition 2 translational approaches force e2 to be friends with the other 19 entities as well (same
goes for  e.g.  netﬂix genre in FB15k and has part in WN18). The table also shows that bilinear
approaches tend to have better performances compared to translational and deep learning approaches.
Even DistMult  the simplest bilinear approach  outperforms many translational and deep learning
approaches despite not being fully expressive. We believe the simplicity of embeddings and the
scoring function is a key property for the success of SimplE.

8

Table 2: Background Knowledge Used in Section 6.2.

Rule

(ei   hyponym  ej ) ∈ ζ ⇔ (ej   hypernym  ei ) ∈ ζ

(ei   memberMeronym  ej ) ∈ ζ ⇔ (ej   memberHolonym  ei ) ∈ ζ
(ei   instanceHyponym  ej ) ∈ ζ ⇔ (ej   instanceHypernym  ei ) ∈ ζ

(ei   hasPart  ej ) ∈ ζ ⇔ (ej   partOf   ei ) ∈ ζ

(ei   memberOfDomainTopic  ej ) ∈ ζ ⇔ (ej   synsetDomainTopicOf   ei ) ∈ ζ
(ei   memberOfDomainUsage  ej ) ∈ ζ ⇔ (ej   synsetDomainUsageOf   ei ) ∈ ζ
(ei   memberOfDomainRegion  ej ) ∈ ζ ⇔ (ej   synsetDomainRegionOf   ei ) ∈ ζ

(ei   similarTo  ej ) ∈ ζ ⇔ (ej   similarTo  ei ) ∈ ζ

Rule Number

1
2
3
4
5
6
7
8

6.2

Incorporating background knowledge

When background knowledge is available  we might expect that a knowledge graph might not
include redundant information because it is implied by background knowledge and so the methods
that do not include the background knowledge can never learn it. In section 5.2  we showed how
background knowledge that can be formulated in terms of three types of rules can be incorporated
into SimplE embeddings. To test this empirically  we conducted an experiment on WN18 in which
we incorporated several such rules into the embeddings as outlined in Propositions 3  4  and 5. The
rules can be found in Table 2. As can be viewed in Table 2  most of the rules are of the form
∀ei  ej ∈ E : (ei   r1   ej ) ∈ ζ ⇔ (ej   r2   ei ) ∈ ζ. For (possibly identical) relations such as r1 and r2
participating in such a rule  if both (ei   r1   ej ) and (ej   r2   ei ) are in the training set  one of them
is redundant because one can be inferred from the other. We removed redundant triples from the
training set by randomly removing one of the two triples in the training set that could be inferred from
the other one based on the background rules. Removing redundant triples reduced the number of
triples in the training set from (approximately) 141K to (approximately) 90K  almost 36% reduction
in size. Note that this experiment provides an upper bound on how much background knowledge can
improve the performance of a SimplE model.
We trained SimplE-ignr and SimplE (with tied parameters according to the rules) on this new training
dataset with the best hyper-parameters found in the previous experiment. We refer to these two models
as SimplE-ignr-bk and SimplE-bk. We also trained another SimplE-ignr and SimplE models on this
dataset  but without incorporating the rules into the embeddings. For sanity check  we also trained a
ComplEx model over this new dataset. We found that the ﬁltered MRR for SimplE-ignr  SimplE  and
ComplEx were respectively 0.221  0.384  and 0.275. For SimplE-ignr-bk and SimplE-bk  the ﬁltered
MRRs were 0.772 and 0.776 respectively  substantially higher than the case without background
knowledge. In terms of hit@k measures  SimplE-ignr gave 0.219  0.220  and 0.224 for hit@1 
hit@3 and hit@10 respectively. These numbers were 0.334  0.404  and 0.482 for SimplE  and 0.254 
0.280 and 0.313 for ComplEx. For SimplE-ignr-bk  these numbers were 0.715  0.809 and 0.877 and
for SimplE-bk they were 0.715  0.818 and 0.883  also substantially higher than the models without
background knowledge. The obtained results validate that background knowledge can be effectively
incorporated into SimplE embeddings to improve its performance.

7 Conclusion

We proposed a simple interpretable fully expressive bilinear model for knowledge graph completion.
We showed that our model  called SimplE  performs very well empirically and has several interesting
properties. For instance  three types of background knowledge can be incorporated into SimplE by
tying the embeddings. In future  SimplE could be improved or may help improve relational learning in
several ways including: 1- building ensembles of SimplE models as [18] do it for DistMult  2- adding
SimplE to the relation-level ensembles of [44]  3- explicitly modelling the analogical structures
of relations as in [23]  4- using [8]’s 1-N scoring approach to generate many negative triples for a
positive triple (Trouillon et al. [39] show that generating more negative triples improves accuracy) 
5- combining SimplE with symbolic approaches (e.g.  with [19]) to improve property prediction  6-
combining SimplE with (or use SimplE as a sub-component in) techniques from other categories of
relational learning as [33] do with ComplEx  7- incorporating other types of background knowledge
(e.g.  entailment) into SimplE embeddings.

9

References
[1] Martın Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467  2016.

[2] Kurt Bollacker  Colin Evans  Praveen Paritosh  Tim Sturge  and Jamie Taylor. Freebase: a
collaboratively created graph database for structuring human knowledge. In ACM SIGMOD  pages
1247–1250. AcM  2008.

[3] Antoine Bordes  Nicolas Usunier  Alberto Garcia-Duran  Jason Weston  and Oksana Yakhnenko.

Irreﬂexive and hierarchical relations as translations. arXiv preprint arXiv:1304.7158  2013.

[4] Antoine Bordes  Nicolas Usunier  Alberto Garcia-Duran  Jason Weston  and Oksana Yakhnenko.

Translating embeddings for modeling multi-relational data. In NIPS  pages 2787–2795  2013.

[5] George Cybenko. Approximations by superpositions of a sigmoidal function. Mathematics of

Control  Signals and Systems  2:183–192  1989.

[6] Rajarshi Das  Shehzaad Dhuliawala  Manzil Zaheer  Luke Vilnis  Ishan Durugkar  Akshay
Krishnamurthy  Alex Smola  and Andrew McCallum. Go for a walk and arrive at the answer:
Reasoning over paths in knowledge bases using reinforcement learning. NIPS Workshop on AKBC 
2017.

[7] Luc De Raedt  Kristian Kersting  Sriraam Natarajan  and David Poole. Statistical relational
artiﬁcial intelligence: Logic  probability  and computation. Synthesis Lectures on Artiﬁcial
Intelligence and Machine Learning  10(2):1–189  2016.

[8] Tim Dettmers  Pasquale Minervini  Pontus Stenetorp  and Sebastian Riedel. Convolutional 2d

knowledge graph embeddings. In AAAI  2018.

[9] Boyang Ding  Quan Wang  Bin Wang  and Li Guo. Improving knowledge graph embedding
In Proceedings of the 56th Annual Meeting of the Association for

using simple constraints.
Computational Linguistics  2018.

[10] Xin Dong  Evgeniy Gabrilovich  Geremy Heitz  Wilko Horn  Ni Lao  Kevin Murphy  Thomas
Strohmann  Shaohua Sun  and Wei Zhang. Knowledge vault: A web-scale approach to probabilistic
knowledge fusion. In ACM SIGKDD  pages 601–610. ACM  2014.

[11] Jun Feng  Minlie Huang  Mingdong Wang  Mantong Zhou  Yu Hao  and Xiaoyan Zhu. Knowl-

edge graph embedding by ﬂexible translation. In KR  pages 557–560  2016.

[12] Lise Getoor and Ben Taskar. Introduction to statistical relational learning. MIT press  2007.

[13] Shu Guo  Quan Wang  Lihong Wang  Bin Wang  and Li Guo. Jointly embedding knowledge
graphs and logical rules. In Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing  pages 192–202  2016.

[14] Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex

embeddings for link prediction. arXiv preprint arXiv:1702.05563  2017.

[15] Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Studies in

Applied Mathematics  6(1-4):164–189  1927.

[16] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks 

4(2):251–257  1991.

[17] Guoliang Ji  Shizhu He  Liheng Xu  Kang Liu  and Jun Zhao. Knowledge graph embedding via

dynamic mapping matrix. In ACL (1)  pages 687–696  2015.

[18] Rudolf Kadlec  Ondrej Bajgar  and Jan Kleindienst. Knowledge base completion: Baselines

strike back. arXiv preprint arXiv:1705.10744  2017.

[19] Seyed Mehran Kazemi and David Poole. Relnn: A deep neural model for relational learning. In

AAAI  2018.

10

[20] Ni Lao and William W Cohen. Relational retrieval using a combination of path-constrained

random walks. Machine learning  81(1):53–67  2010.

[21] Yankai Lin  Zhiyuan Liu  Huanbo Luan  Maosong Sun  Siwei Rao  and Song Liu. Modeling

relation paths for representation learning of knowledge bases. EMNLP  2015.

[22] Yankai Lin  Zhiyuan Liu  Maosong Sun  Yang Liu  and Xuan Zhu. Learning entity and relation

embeddings for knowledge graph completion. In AAAI  pages 2181–2187  2015.

[23] Hanxiao Liu  Yuexin Wu  and Yiming Yang. Analogical inference for multi-relational embed-

dings. AAAI  2018.

[24] George A Miller. Wordnet: a lexical database for english. Communications of the ACM 

38(11):39–41  1995.

[25] Pasquale Minervini  Luca Costabello  Emir Muñoz  Vít Nováˇcek  and Pierre-Yves Vanden-
bussche. Regularizing knowledge graph embeddings via equivalence and inversion axioms. In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases  pages
668–683. Springer  2017.

[26] Dat Quoc Nguyen  Kairit Sirts  Lizhen Qu  and Mark Johnson. Stranse: a novel embedding

model of entities and relationships in knowledge bases. In NAACL-HLT  2016.

[27] Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowl-

edge base completion. arXiv preprint arXiv:1703.08098  2017.

[28] Maximilian Nickel  Volker Tresp  and Hans-Peter Kriegel. A three-way model for collective

learning on multi-relational data. In ICML  volume 11  pages 809–816  2011.

[29] Maximilian Nickel  Volker Tresp  and Hans-Peter Kriegel. Factorizing yago: scalable machine

learning for linked data. In World Wide Web  pages 271–280. ACM  2012.

[30] Maximilian Nickel  Xueyan Jiang  and Volker Tresp. Reducing the rank in relational factoriza-

tion models by including observable patterns. In NIPS  pages 1179–1187  2014.

[31] Maximilian Nickel  Kevin Murphy  Volker Tresp  and Evgeniy Gabrilovich. A review of
relational machine learning for knowledge graphs. Proceedings of the IEEE  104(1):11–33  2016.

[32] Maximilian Nickel  Lorenzo Rosasco  Tomaso A Poggio  et al. Holographic embeddings of

knowledge graphs. In AAAI  pages 1955–1961  2016.

[33] Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. In NIPS  pages

3791–3803  2017.

[34] Tim Rocktäschel  Matko Bošnjak  Sameer Singh  and Sebastian Riedel. Low-dimensional
embeddings of logic. In Proceedings of the ACL 2014 Workshop on Semantic Parsing  pages
45–49  2014.

[35] Adam Santoro  David Raposo  David G Barrett  Mateusz Malinowski  Razvan Pascanu  Peter
Battaglia  and Tim Lillicrap. A simple neural network module for relational reasoning. In NIPS 
2017.

[36] Michael Schlichtkrull  Thomas N Kipf  Peter Bloem  Rianne van den Berg  Ivan Titov  and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference  pages 593–607. Springer  2018.

[37] Richard Socher  Danqi Chen  Christopher D Manning  and Andrew Ng. Reasoning with neural

tensor networks for knowledge base completion. In NIPS  2013.

[38] Théo Trouillon and Maximilian Nickel. Complex and holographic embeddings of knowledge

graphs: a comparison. arXiv preprint arXiv:1707.01475  2017.

[39] Théo Trouillon  Johannes Welbl  Sebastian Riedel  Éric Gaussier  and Guillaume Bouchard.

Complex embeddings for simple link prediction. In ICML  pages 2071–2080  2016.

11

[40] Théo Trouillon  Christopher R Dance  Johannes Welbl  Sebastian Riedel  Éric Gaussier  and
Guillaume Bouchard. Knowledge graph completion via complex tensor factorization. arXiv
preprint arXiv:1702.06879  2017.

[41] Zhen Wang  Jianwen Zhang  Jianlin Feng  and Zheng Chen. Knowledge graph embedding by

translating on hyperplanes. In AAAI  pages 1112–1119  2014.

[42] Quan Wang  Bin Wang  Li Guo  et al. Knowledge base completion using embeddings and rules.

In International Joint Conference on Artiﬁcial Intelligence  pages 1859–1866  2015.

[43] Quan Wang  Zhendong Mao  Bin Wang  and Li Guo. Knowledge graph embedding: A
survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering 
29(12):2724–2743  2017.

[44] Yanjie Wang  Rainer Gemulla  and Hui Li. On multi-relational link prediction with bilinear

models. AAAI  2018.

[45] Zhuoyu Wei  Jun Zhao  Kang Liu  Zhenyu Qi  Zhengya Sun  and Guanhua Tian. Large-scale
knowledge base completion: Inferring via grounding network sampling over selected instances.
In Proceedings of the 24th ACM International on Conference on Information and Knowledge
Management  pages 1331–1340. ACM  2015.

[46] Bishan Yang  Wen-tau Yih  Xiaodong He  Jianfeng Gao  and Li Deng. Embedding entities and

relations for learning and inference in knowledge bases. ICLR  2015.

[47] Hanwang Zhang  Zawlin Kyaw  Shih-Fu Chang  and Tat-Seng Chua. Visual translation embed-

ding network for visual relation detection. In CVPR  volume 1  page 5  2017.

12

,Tim Rocktäschel
Sebastian Riedel
Seyed Mehran Kazemi
David Poole