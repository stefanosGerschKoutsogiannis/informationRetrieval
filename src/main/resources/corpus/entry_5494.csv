2016,The Parallel Knowledge Gradient Method for Batch Bayesian Optimization,In many applications of black-box optimization  one can evaluate multiple points simultaneously  e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment.  In this paper  we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction  this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points  and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms  especially when function evaluations are noisy.,The Parallel Knowledge Gradient Method

for Batch Bayesian Optimization

Jian Wu  Peter I. Frazier

Cornell University
Ithaca  NY  14853

{jw926  pf98}@cornell.edu

Abstract

In many applications of black-box optimization  one can evaluate multiple points
simultaneously  e.g. when evaluating the performances of several different neural
networks in a parallel computing environment. In this paper  we develop a novel
batch Bayesian optimization algorithm — the parallel knowledge gradient method.
By construction  this method provides the one-step Bayes optimal batch of points
to sample. We provide an efﬁcient strategy for computing this Bayes-optimal
batch of points  and we demonstrate that the parallel knowledge gradient method
ﬁnds global optima signiﬁcantly faster than previous batch Bayesian optimization
algorithms on both synthetic test functions and when tuning hyperparameters of
practical machine learning algorithms  especially when function evaluations are
noisy.

1

Introduction

In Bayesian optimization [19] (BO)  we wish to optimize a derivative-free expensive-to-evaluate
function f with feasible domain A ⊆ Rd 

min
x∈A f (x) 

with as few function evaluations as possible. In this paper  we assume that membership in the domain
A is easy to evaluate and we can evaluate f only at points in A. We assume that evaluations of f are
either noise-free  or have additive independent normally distributed noise. We consider the parallel
setting  in which we perform more than one simultaneous evaluation of f.
BO typically puts a Gaussian process prior distribution on the function f  updating this prior
distribution with each new observation of f  and choosing the next point or points to evaluate
by maximizing an acquisition function that quantiﬁes the beneﬁt of evaluating the objective as a
function of where it is evaluated. In comparison with other global optimization algorithms  BO often
ﬁnds “near optimal” function values with fewer evaluations [19]. As a consequence  BO is useful
when function evaluation is time-consuming  such as when training and testing complex machine
learning algorithms (e.g. deep neural networks) or tuning algorithms on large-scale dataset (e.g.
ImageNet) [4]. Recently  BO has become popular in machine learning as it is highly effective in
tuning hyperparameters of machine learning algorithms [8  9  19  22].
Most previous work in BO assumes that we evaluate the objective function sequentially [13]  though a
few recent papers have considered parallel evaluations [3  5  18  25]. While in practice  we can often
evaluate several different choices in parallel  such as multiple machines can simultaneously train the
machine learning algorithm with different sets of hyperparameters. In this paper  we assume that
we can access q ≥ 1 evaluations simultaneously at each iteration. Then we develop a new parallel
acquisition function to guide where to evaluate next based on the decision-theoretical analysis.
Our Contributions. We propose a novel batch BO method which measures the information gain
of evaluating q points via a new acquisition function  the parallel knowledge gradient (q-KG). This

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

method is derived using a decision-theoretic analysis that chooses the set of points to evaluate next
that is optimal in the average-case with respect to the posterior when there is only one batch of points
remaining. Naively maximizing q-KG would be extremely computationally intensive  especially when
q is large  and so  in this paper  we develop a method based on inﬁnitesimal perturbation analysis (IPA)
[25] to evaluate q-KG’s gradient efﬁciently  allowing its efﬁcient optimization. In our experiments
on both synthetic functions and tuning practical machine learning algorithms  q-KG consistently
ﬁnds better function values than other parallel BO algorithms  such as parallel EI [2  19  25]  batch
UCB [5] and parallel UCB with exploration [3]. q-KG provides especially large value when function
evaluations are noisy. The code in this paper is available at https://github.com/wujian16/qKG.
The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 gives
background on Gaussian processes and deﬁnes notation used later. Section 4 proposes our new
acquisition function q-KG for batch BO. Section 5 provides our computationally efﬁcient approach
to maximizing q-KG. Section 6 presents the empirical performance of q-KG and several benchmarks
on synthetic functions and real problems. Finally  Section 7 concludes the paper.
2 Related work
Within the past several years  the machine learning community has revisited BO [8  9  18  19  20 
22] due to its huge success in tuning hyperparameters of complex machine learning algorithms.
BO algorithms consist of two components: a statistical model describing the function and an
acquisition function guiding evaluations. In practice  Gaussian Process (GP) [16] is the mostly
widely used statistical model due to its ﬂexibility and tractability. Much of the literature in BO
focuses on designing good acquisition functions that reach optima with as few evaluations as possible.
Maximizing this acquisition function usually provides a single point to evaluate next  with common
acquisition functions for sequential Bayesian optimization including probability of improvement
(PI)[23]  expected improvement (EI) [13]  upper conﬁdence bound (UCB) [21]  entropy search (ES)
[11]  and knowledge gradient (KG) [17].
Recently  a few papers have extended BO to the parallel setting  aiming to choose a batch of points to
evaluate next in each iteration  rather than just a single point. [10  19] suggests parallelizing EI by
iteratively constructing a batch  in each iteration adding the point with maximal single-evaluation EI
averaged over the posterior distribution of previously selected points. [10] also proposes an algorithm
called “constant liar"  which iteratively constructs a batch of points to sample by maximizing single-
evaluation while pretending that points previously added to the batch have already returned values.
There are also work extending UCB to the parallel setting. [5] proposes the GP-BUCB policy  which
selects points sequentially by a UCB criterion until ﬁlling the batch. Each time one point is selected 
the algorithm updates the kernel function while keeping the mean function ﬁxed. [3] proposes an
algorithm combining UCB with pure exploration  called GP-UCB-PE. In this algorithm  the ﬁrst
point is selected according to a UCB criterion; then the remaining points are selected to encourage the
diversity of the batch. These two algorithms extending UCB do not require Monte Carlo sampling 
making them fast and scalable. However  UCB criteria are usually designed to minimize cumulative
regret rather than immediate regret  causing these methods to underperform in BO  where we wish to
minimize simple regret.
The parallel methods above construct the batch of points in an iterative greedy fashion  optimizing
some single-evaluation acquisition function while holding the other points in the batch ﬁxed. The
acquisition function we propose considers the batch of points collectively  and we choose the batch to
jointly optimize this acquisition function. Other recent papers that value points collectively include
[2] which optimizes the parallel EI by a closed-form formula  [15  25]  in which gradient-based
methods are proposed to jointly optimize a parallel EI criterion  and [18]  which proposes a parallel
version of the ES algorithm and uses Monte Carlo Sampling to optimize the parallel ES acquisition
function.
We compare against methods from a number of these previous papers in our numerical experiments 
and demonstrate that we provide an improvement  especially in problems with noisy evaluations.
Our method is also closely related to the knowledge gradient (KG) method [7  17] for the non-batch
(sequential) setting  which chooses the Bayes-optimal point to evaluate if only one iteration is left
[17]  and the ﬁnal solution that we choose is not restricted to be one of the points we evaluate.
(Expected improvement is Bayes-optimal if the solution is restricted to be one of the points we
evaluate.) We go beyond this previous work in two aspects. First  we generalize to the parallel setting.

2

Second  while the sequential setting allows evaluating the KG acquisition function exactly  evaluation
requires Monte Carlo in the parallel setting  and so we develop more sophisticated computational
techniques to optimize our acquisition function. Recently  [26] studies a nested batch knowledge
gradient policy. However  they optimize over a ﬁnite discrete feasible set  where the gradient of KG
does not exist. As a result  their computation of KG is much less efﬁcient than ours. Moreover  they
focus on a nesting structure from materials science not present in our setting.
3 Background on Gaussian processes

In this section  we state our prior on f  brieﬂy discuss well known results about Gaussian processes
(GP)  and introduce notation used later. We put a Gaussian process prior over the function f : A → R 
which is speciﬁed by its mean function µ(x) : A → R and kernel function K(x1  x2) : A × A → R.
We assume either exact or independent normally distributed measurement errors  i.e. the evaluation
y(xi) at point xi satisﬁes

y(xi) | f (xi) ∼ N (f (xi)  σ2(xi)) 

where σ2 : A → R+ is a known function describing the variance of the measurement errors. If σ2 is
not known  we can also estimate it as we do in Section 6.
Supposing we have measured f at n points x(1:n) := {x(1)  x(2) ···   x(n)} and obtained corre-
sponding measurements y(1:n)  we can then combine these observed function values with our prior to
obtain a posterior distribution on f. This posterior distribution is still a Gaussian process with the
mean function µ(n) and the kernel function K (n) as follows
µ(n)(x) = µ(x)

K(x(1:n)  x(1:n)) + diag{σ2(x(1)) ···   σ2(x(n))}(cid:17)−1
(cid:16)
(cid:16)
K(x(1:n)  x(1:n)) + diag{σ2(x(1)) ···   σ2(x(n))}(cid:17)−1

+ K(x  x(1:n))

K (n)(x1  x2) = K(x1  x2)

− K(x1  x(1:n))

(y(1:n) − µ(x(1:n))) 

K(x(1:n)  x2).

(3.1)

4 Parallel knowledge gradient (q-KG)

In this section  we propose a novel parallel Bayesian optimization algorithm by generalizing the
concept of the knowledge gradient from [7] to the parallel setting. The knowledge gradient policy in
[7] for discrete A chooses the next sampling decision by maximizing the expected incremental value
of a measurement  without assuming (as expected improvement does) that the point returned as the
optimum must be a previously sampled point.
We now show how to compute this expected incremental value of an additional iteration in the
parallel setting. Suppose that we have observed n function values. If we were to stop measuring now 
minx∈A µ(n)(x) would be the minimum of the predictor of the GP. If instead we took one more batch
of samples  minx∈A µ(n+q)(x) would be the minimum of the predictor of the GP. The difference
between these quantities  minx∈A µ(n)(x)−minx∈A µ(n+q)(x)  is the increment in expected solution
quality (given the posterior after n + q samples) that results from the additional batch of samples.
This increment in solution quality is random given the posterior after n samples  because
minx∈A µ(n+q)(x) is itself a random vector due to its dependence on the outcome of the sam-
ples. We can compute the probability distribution of this difference (with more details given below) 
and the q-KG algorithm values the sampling decision z(1:q) := {z1  z2 ···   zq} according to its
expected value  which we call the parallel knowledge gradient factor  and indicate it using the notation
q-KG. Formally  we deﬁne the q-KG factor for a set of candidate points to sample z(1:q) as

where En [·] := E(cid:2)·|x(1:n)  y(1:n)(cid:3) is the expectation taken with respect to the posterior distribution

x∈A µ(n)(x) − En

q-KG(z(1:q)  A) = min

after n evaluations. Then we choose to evaluate the next batch of q points that maximizes the parallel
knowledge gradient 

(4.1)

(cid:20)

(cid:21)
x∈A µ(n+q)(x)|y(z(1:q))

min

 

(4.2)

q-KG(z(1:q)  A).

max

z(1:q)⊂A

3

By construction  the parallel knowledge gradient policy is Bayes-optimal for minimizing the minimum
of the predictor of the GP if only one decision is remaining. The q-KG algorithm will reduce to
the parallel EI algorithm if function evaluations are noise-free and the ﬁnal recommendation is
restricted to the previous sampling decisions. Because under the two conditions above  the increment
in expected solution quality will become

µ(n)(x) −

min

x∈x(1:n)

min

x∈x(1:n)∪z(1:q)

µ(n+q)(x) = min y(1:n) − min

y(1:n)  min
x∈z(1:q)

µ(n+q)(x)

(cid:26)

(cid:27)

(cid:19)+

(cid:18)

=

min y(1:n) − min
x∈z(1:q)

µ(n+q)(x)

 

which is exactly the parallel EI acquisition function. However  computing q-KG and its gradient is
very expensive. We will address the computational issues in Section 5. The full description of the
q-KG algorithm is summarized as follows.

Algorithm 1 The q-KG algorithm
Require: the number of initial stage samples I  and the number of main stage sampling iterations N.
1: Initial Stage: draw I initial samples from a latin hypercube design in A  x(i) for i = 1  . . .   I .
2: Main Stange:
3: for s = 1 to N do
4:
5:

q ) = argmaxz(1:q)⊂Aq-KG(z(1:q)  A)
q )  re-train the hyperparameters of the GP by MLE  and

2  ···   z∗
1   z∗
Solve (4.2)  i.e. get (z∗
2  ···   z∗
1   z∗
Sample these points (z∗
update the posterior distribution of f.
6: end for
7: return x∗ = argminx∈Aµ(I+N q)(x).
5 Computation of q-KG

In this section  we provide the strategy to maximize q-KG by a gradient-based optimizer. In Section 5.1
and Section 5.2  we describe how to compute q-KG and its gradient when A is ﬁnite in (4.1).
Section 5.3 describes an effective way to discretize A in (4.1). The readers should note that there are
two As here  one is in (4.1) which is used to compute the q-KG factor given a sampling decision z(1:q).
The other is the feasible domain in (4.2) (z(1:q) ⊂ A) that we optimize over. We are discretizing the
ﬁrst A.

5.1 Estimating q-KG when A is ﬁnite in (4.1)

Following [7]  we express µ(n+q)(x) as

µ(n+q)(x) = µ(n)(x) + K (n)(x  z(1:q))

(cid:16)

+diag{σ2(z(1)) ···   σ2(z(q))}(cid:17)−1(cid:16)

K (n)(z(1:q)  z(1:q))

y(z(1:q)) − µ(n)(z(1:q))

(cid:17)

.

Because y(z(1:q)) − µ(n)(z(1:q)) is normally distributed with zero mean and covariance matrix
K (n)(z(1:q)  z(1:q))+diag{σ2(z(1)) ···   σ2(z(q))} with respect to the posterior after n observations 
we can rewrite µ(n+q)(x) as

µ(n+q)(x) = µ(n)(x) + ˜σn(x  z(1:q))Zq 

(5.1)

where Zq is a standard q-dimensional normal random vector  and

˜σn(x  z(1:q)) = K (n)(x  z(1:q))(D(n)(z(1:q))T )−1 

where D(n)(z(1:q)) is the Cholesky factor of the covariance matrix K (n)(z(1:q)  z(1:q)) +
diag{σ2(z(1)) ···   σ2(z(q))}. Now we can compute the q-KG factor using Monte Carlo sam-
pling when A is ﬁnite: we can sample Zq  compute (5.1)  then plug in (4.1)  repeat many times and
take average.

4

(cid:16)

(cid:18) ∂

∂zij

5.2 Estimating the gradient of q-KG when A is ﬁnite in (4.1)
In this section  we propose an unbiased estimator of the gradient of q-KG using IPA when A is ﬁnite.
Accessing a stochastic gradient makes optimization much easier. By (5.1)  we express q-KG as

where g = minx∈A µ(n)(x)− minx∈A(cid:0)µ(n)(x) + ˜σn(x  z(1:q))Zq

q-KG(z(1:q)  A) = EZq

g(z(1:q)  A  Zq)

K are continuously differentiable  one can show that (please see the details in the supplementary
materials)

(cid:17)
(cid:1). Under the condition that µ and

(5.2)

 

(cid:19)

∂

∂zij

q-KG(z(1:q)  A) = EZq

g(z(1:q)  A  Zq)

 

(5.3)

where zij is the jth dimension of the ith point in z(1:q). By the formula of g 

∂

∂zij

g(z(1:q)  A  Zq) =

∂

∂zij

µ(n)(x∗(before)) − ∂
∂zij

µ(n)(x∗(after))

where x∗(before) = argminx∈Aµ(n)(x)  x∗(after) = argminx∈A(cid:0)µ(n)(x) + ˜σn(x  z(1:q))Zq

˜σn(x∗(after)  z(1:q))Zq

− ∂
∂zij

(cid:1) 

and

˜σn(x∗(after)  z(1:q)) =

∂

∂zij

(cid:18) ∂

(cid:19)

K (n)(x∗(after)  z(1:q))

(D(n)(z(1:q))T )−1

(cid:18) ∂

∂zij
−K (n)(x∗(after)  z(1:q))(D(n)(z(1:q))T )−1
(D(n)(z(1:q))T )−1.

D(n)(z(1:q))T

(cid:19)

∂zij

Now we can sample many times and take average to estimate the gradient of q-KG via (5.3). This
technique is called inﬁnitesimal perturbation analysis (IPA) in gradient estimation [14]. Since we can
estimate the gradient of q-KG efﬁciently when A is ﬁnite  we will apply some standard gradient-based
optimization algorithms  such as multi-start stochastic gradient ascent to maximize q-KG.

5.3 Approximating q-KG when A is inﬁnite in (4.1) through discretization
We have speciﬁed how to maximize q-KG when A is ﬁnite in (4.1)  but usually A is inﬁnite. In this
case  we will discretize A to approximate q-KG  and then maximize over the approximate q-KG. The
discretization itself is an interesting research topic [17].
In this paper  the discrete set An is not chosen statically  but evolves over time: speciﬁcally  we
suggest drawing M samples from the global optima of the posterior distribution of the Gaussian
process (please refer to [11  18] for a description of this technique). This sample set  denoted
by AM
n   is then extended by the locations of previously sampled points x(1:n) and the set of candidate
points z(1:q). Then (4.1) can be restated as

q-KG(z(1:q)  An) = min
x∈An

µ(n)(x) − En

min
x∈An

µ(n+q)(x)|y(z(1:q))

 

(5.4)

n ∪ x(1:n) ∪ z(1:q). For the experimental evaluation we recompute AM

where An = AM
iteration after updating the posterior of the Gaussian process.
6 Numerical experiments
We conduct experiments in two different settings: the noise-free setting and the noisy setting. In
both settings  we test the algorithms on well-known synthetic functions chosen from [1] and practical
problems. Following previous literature [19]  we use a constant mean prior and the ARD Mat´ern 5/2
kernel. In the noisy setting  we assume that σ2(x) is constant across the domain A  and we estimate
it together with other hyperparameters in the GP using maximum likelihood estimation (MLE). We
set M = 1000 to discretize the domain following the strategy in Section 5.3. In general  the q-KG

n in every

5

(cid:20)

(cid:21)

Figure 1: Performances on noise-free synthetic functions with q = 4. We report the mean and the standard
deviation of the log10 scale of the immediate regret vs. the number of function evaluations.

algorithm performs as well or better than state-of-art benchmark algorithms on both synthetic and
real problems. It performs especially well in the noisy setting.
Before describing the details of the empirical results  we highlight the implementation details of our
method and the open-source implementations of the benchmark methods. Our implementation inherits
the open-source implementation of parallel EI from the Metrics Optimization Engine [24] 
which is fully implemented in C++ with a python interface. We reuse their GP regression and GP
hyperparameter ﬁtting methods and implement the q-KG method in C++. Besides comparing to
parallel EI in [24]  we also compare our method to a well-known heuristic parallel EI implemented in
Spearmint [12]  the parallel UCB algorithm (GP-BUCB) and parallel UCB with pure exploration
(GP-UCB-PE) both implemented in Gpoptimization [6].
6.1 Noise-free problems
In this section  we focus our attention on the noise-free setting  in which we can evaluate the objective
exactly. We show that parallel knowledge gradient outperforms or is competitive with state-of-art
benchmarks on several well-known test functions and tuning practical machine learning algorithms.
6.1.1 Synthetic functions

First  we test our algorithm along with the benchmarks on 4 well-known synthetic test functions:
Branin2 on the domain [−15  15]2  Rosenbrock3 on the domain [−2  2]3  Ackley5 on the domain
[−2  2]5  and Hartmann6 on the domain [0  1]6. We initiate our algorithms by randomly sampling
2d + 2 points from a Latin hypercube design  where d is the dimension of the problem. Figure 3
reports the mean and the standard deviation of the base 10 logarithm of the immediate regret by
running 100 random initializations with batch size q = 4.
The results show that q-KG is signiﬁcantly better on Rosenbrock3  Ackley5 and Hartmann6  and is
slightly worse than the best of the other benchmarks on Branin2. Especially on Rosenbrock3 and
Ackley5  q-KG makes dramatic progress in early iterations.
6.1.2 Tuning logistic regression and convolutional neural networks (CNN)

In this section  we test the algorithms on two practical problems: tuning logistic regression on the
MNIST dataset and tuning CNN on the CIFAR10 dataset. We set the batch size to q = 4.

6

020406080100120140160function evaluations−3.0−2.5−2.0−1.5−1.0−0.50.00.51.01.5the log10 scale of the immediate regret2d BraninNoNoise function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG020406080100120140160function evaluations−3−2−10123the log10 scale of the immediate regret3d RosenbrockNoNoise function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG050100150200250function evaluations−0.4−0.20.00.20.40.60.8the log10 scale of the immediate regret5d AckleyNoNoise function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG020406080100120140160180function evaluations−1.5−1.0−0.50.00.5the log10 scale of the immediate regret6d HartmannNoNoise function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKGFirst  we tune logistic regression on the MNIST dataset. This task is to classify handwritten digits
from images  and is a 10-class classiﬁcation problem. We train logistic regression on a training set
with 60000 instances with a given set of hyperparameters and test it on a test set with 10000 instances.
We tune 4 hyperparameters: mini batch size from 10 to 2000  training iterations from 100 to 10000 
the (cid:96)2 regularization parameter from 0 to 1  and learning rate from 0 to 1. We report the mean and
standard deviation of the test error for 20 independent runs. From the results  one can see that both
algorithms are making progress at the initial stage while q-KG can maintain this progress for longer
and results in a better algorithm conﬁguration in general.

Figure 2: Performances on tuning machine learning algorithms with q = 4

In the second experiment  we tune a CNN on CIFAR10 dataset. This is also a 10-class classiﬁcation
problem. We train the CNN on the 50000 training data with certain hyperparameters and test it on
the test set with 10000 instances. For the network architecture  we choose the one in tensorflow
tutorial. It consists of 2 convolutional layers  2 fully connected layers  and on top of them is a softmax
layer for ﬁnal classiﬁcation. We tune totally 8 hyperparameters: the mini batch size from 10 to 1000 
training epoch from 1 to 10  the (cid:96)2 regularization parameter from 0 to 1  learning rate from 0 to 1 
the kernel size from 2 to 10  the number of channels in convolutional layers from 10 to 1000  the
number of hidden units in fully connected layers from 100 to 1000  and the dropout rate from 0 to 1.
We report the mean and standard deviation of the test error for 5 independent runs. In this example 
the q-KG is making better (more aggressive) progress than parallel EI even in the initial stage and
maintain this advantage to the end. This architecture has been carefully tuned by the human expert 
and achieve a test error around 14%  and our automatic algorithm improves it to around 11%.
6.2 Noisy problems
In this section  we study problems with noisy function evaluations. Our results show that the
performance gains over benchmark algorithms from q-KG evident in the noise-free setting are even
larger in the noisy setting.
6.2.1 Noisy synthetic functions

We test on the same 4 synthetic functions from the noise-free setting  and add independent gaussian
noise with standard deviation σ = 0.5 to the function evaluation. The algorithms are not given this
standard deviation  and must learn it from data.
The results in Figure 4 show that q-KG is consistently better than or at least competitive with
all competing methods. Also observe that the performance advantage of q-KG is larger than for
noise-free problems.
6.2.2 Noisy logistic regression with small test sets

Testing on a large test set such as ImageNet is slow  especially when we must test many times for
different hyperparameters. To speed up hyperparameter tuning  we may instead test the algorithm on
a subset of the testing data to approximate the test error on the full set. We study the performance of
our algorithm and benchmarks in this scenario  focusing on tuning logistic regression on MNIST.
We train logistic regression on the full training set of 60  000  but we test the algorithm by testing on
1  000 randomly selected samples from the test set  which provides a noisy approximation of the test
error on the full test set.

7

010203040506070function evaluations0.050.100.150.200.250.30test errorLogistic Regression on MNISTMOE-qEISpearmint-qEIqKG102030405060708090function evaluations0.100.150.200.250.300.35CNN on CIFAR10Spearmint-qEIqKGFigure 3: Performances on noisy synthetic functions with q = 4. We report the mean and the standard deviation
of the log10 scale of the immediate regret vs. the number of function evaluations.

Figure 4: Tuning logistic regression on smaller test sets with q = 4

We report the mean and standard deviation of the test error on the full set using the hyperparameters
recommended by each parallel BO algorithm for 20 independent runs. The result shows that q-KG
is better than both versions of parallel EI  and its ﬁnal test error is close to the noise-free test error
(which is substantially more expensive to obtain). As we saw with synthetic test functions  q-KG’s
performance advantage in the noisy setting is wider than in the noise-free setting.
Acknowledgments
The authors were partially supported by NSF CAREER CMMI-1254298  NSF CMMI-1536895  NSF
IIS-1247696  AFOSR FA9550-12-1-0200  AFOSR FA9550-15-1-0038  and AFOSR FA9550-16-1-
0046.
7 Conclusions
In this paper  we introduce a novel batch Bayesian optimization method q-KG  derived from a
decision-theoretical perspective  and develop a computational method to implement it efﬁciently. We
show that q-KG outperforms or is competitive with the state-of-art benchmark algorithms on several
synthetic functions and in tuning practical machine learning algorithms.

8

020406080100120140160function evaluations−2.5−2.0−1.5−1.0−0.50.00.51.01.5the log10 scale of the immediate regret2d Branin function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG050100150200250300350function evaluations−2.0−1.5−1.0−0.50.00.51.01.52.02.5the log10 scale of the immediate regret3d Rosenbrock function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG050100150200250function evaluations−0.20.00.20.40.60.8the log10 scale of the immediate regret5d Ackley function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG020406080100120140160180function evaluations−0.6−0.4−0.20.00.20.40.6the log10 scale of the immediate regret6d Hartmann function with batch size 4GP-BUCBGP-UCB-PEMOE-qEISpearmint-qEIqKG010203040506070function evaluations0.050.100.150.200.250.300.35test errorLogistic Regression on MNIST with Smaller Test SetsMOE-qEISpearmint-qEIqKGReferences
[1] Bingham  D. (2015). Optimization test problems. http://www.sfu.ca/~ssurjano/optimization.

html.

[2] Chevalier  C. and Ginsbourger  D. (2013). Fast computation of the multi-points expected improvement with

applications in batch selection. In Learning and Intelligent Optimization  pages 59–69. Springer.

[3] Contal  E.  Buffoni  D.  Robicquet  A.  and Vayatis  N. (2013). Parallel gaussian process optimization with
upper conﬁdence bound and pure exploration. In Machine Learning and Knowledge Discovery in Databases 
pages 225–240. Springer.

[4] Deng  L. and Yu  D. (2014). Deep learning: Methods and applications. Foundations and Trends in Signal

Processing  7(3–4):197–387.

[5] Desautels  T.  Krause  A.  and Burdick  J. W. (2014). Parallelizing exploration-exploitation tradeoffs in

gaussian process bandit optimization. The Journal of Machine Learning Research  15(1):3873–3923.

[6] etal  E. C.

(2015).

gpoptimization.

Gpoptimization.

https://reine.cmla.ens-cachan.fr/e.contal/

[7] Frazier  P.  Powell  W.  and Dayanik  S. (2009). The knowledge-gradient policy for correlated normal beliefs.

INFORMS journal on Computing  21(4):599–613.

[8] Gardner  J. R.  Kusner  M. J.  Xu  Z. E.  Weinberger  K. Q.  and Cunningham  J. (2014). Bayesian

optimization with inequality constraints. In ICML  pages 937–945.

[9] Gelbart  M.  Snoek  J.  and Adams  R. (2014). Bayesian optimization with unknown constraints.

In
Proceedings of the Thirtieth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-14) 
pages 250–259  Corvallis  Oregon. AUAI Press.

[10] Ginsbourger  D.  Le Riche  R.  and Carraro  L. (2010). Kriging is well-suited to parallelize optimization.

In Computational Intelligence in Expensive Optimization Problems  pages 131–162. Springer.

[11] Hernández-Lobato  J. M.  Hoffman  M. W.  and Ghahramani  Z. (2014). Predictive entropy search for
efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems 
pages 918–926.

[12] Jasper Snoek  Hugo Larochelle  R. P. A. e. (2015). Spearmint. https://github.com/HIPS/Spearmint.
[13] Jones  D. R.  Schonlau  M.  and Welch  W. J. (1998). Efﬁcient global optimization of expensive black-box

functions. Journal of Global optimization  13(4):455–492.

[14] L’Ecuyer  P. (1990). A uniﬁed view of the IPA  SF  and LR gradient estimation techniques. Management

Science  36(11):1364–1383.

[15] Marmin  S.  Chevalier  C.  and Ginsbourger  D. (2015). Differentiating the multipoint expected improve-
ment for optimal batch design. In International Workshop on Machine Learning  Optimization and Big Data 
pages 37–48. Springer.

[16] Rasmussen  C. E. (2006). Gaussian processes for machine learning.
[17] Scott  W.  Frazier  P.  and Powell  W. (2011). The correlated knowledge gradient for simulation optimization
of continuous parameters using gaussian process regression. SIAM Journal on Optimization  21(3):996–1026.
[18] Shah  A. and Ghahramani  Z. (2015). Parallel predictive entropy search for batch global optimization of

expensive objective functions. In Advances in Neural Information Processing Systems  pages 3312–3320.

[19] Snoek  J.  Larochelle  H.  and Adams  R. P. (2012). Practical bayesian optimization of machine learning

algorithms. In Advances in neural information processing systems  pages 2951–2959.

[20] Snoek  J.  Swersky  K.  Zemel  R.  and Adams  R. (2014). Input warping for bayesian optimization
of non-stationary functions. In Proceedings of the 31st International Conference on Machine Learning
(ICML-14)  pages 1674–1682.

[21] Srinivas  N.  Krause  A.  Seeger  M.  and Kakade  S. M. (2010). Gaussian process optimization in the
bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on
Machine Learning (ICML-10)  pages 1015–1022.

[22] Swersky  K.  Snoek  J.  and Adams  R. P. (2013). Multi-task bayesian optimization. In Advances in neural

information processing systems  pages 2004–2012.

[23] Törn  A. and Zilinskas  A. (1989). Global optimization  volume 350 of lecture notes in computer science.
[24] Wang  J.  Clark  S. C.  Liu  E.  and Frazier  P. I. (2014). Metrics optimization engine. http://yelp.

github.io/MOE/. Last accessed on 2016-01-21.

[25] Wang  J.  Clark  S. C.  Liu  E.  and Frazier  P. I. (2015a). Parallel bayesian global optimization of expensive

functions.

[26] Wang  Y.  Reyes  K. G.  Brown  K. A.  Mirkin  C. A.  and Powell  W. B. (2015b). Nested-batch-mode
learning and stochastic optimization with an application to sequential multistage testing in materials science.
SIAM Journal on Scientiﬁc Computing  37(3):B361–B381.

9

,Jian Wu
Peter Frazier
Mingzhe Wang
Yihe Tang
Jian Wang
Jia Deng