2014,On Prior Distributions and Approximate Inference for Structured Variables,We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable  we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general  we show that approximate inference using convex subsets is tractable  and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result  inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task  we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.,On Prior Distributions and Approximate Inference

for Structured Variables

Oluwasanmi Koyejo

Psychology Dept.  Stanford
sanmi@stanford.edu

Rajiv Khanna

ECE Dept.  UT Austin

rajivak@utexas.edu

Joydeep Ghosh

ECE Dept.  UT Austin

ghosh@ece.utexas.edu

Russell A. Poldrack

Psychology Dept.  Stanford

poldrack@stanford.edu

Abstract

We present a general framework for constructing prior distributions with struc-
tured variables. The prior is deﬁned as the information projection of a base dis-
tribution onto distributions supported on the constraint set of interest. In cases
where this projection is intractable  we propose a family of parameterized approx-
imations indexed by subsets of the domain. We further analyze the special case
of sparse structure. While the optimal prior is intractable in general  we show
that approximate inference using convex subsets is tractable  and is equivalent to
maximizing a submodular function subject to cardinality constraints. As a re-
sult  inference using greedy forward selection provably achieves within a factor
of (1-1/e) of the optimal objective value. Our work is motivated by the predictive
modeling of high-dimensional functional neuroimaging data. For this task  we
employ the Gaussian base distribution induced by local partial correlations and
consider the design of priors to capture the domain knowledge of sparse support.
Experimental results on simulated data and high dimensional neuroimaging data
show the effectiveness of our approach in terms of support recovery and predictive
accuracy.

1

Introduction

Data in scientiﬁc and commercial disciplines are increasingly characterized by high dimensions and
relatively few samples. For such cases  a-priori knowledge gleaned from expertise and experimental
evidence are invaluable for recovering meaningful models. In particular  knowledge of restricted
degrees of freedom such as sparsity or low rank has become an important design paradigm  en-
abling the recovery of parsimonious and interpretable results  and improving storage and prediction
efﬁciency for high dimensional problems. In Bayesian models  such restricted degrees of freedom
can be captured by incorporating structural constraints on the design of the prior distribution. Prior
distributions for structured variables can be designed by combining conditional distributions - each
capturing portions of the problem structure  into a hierarchical model. In other cases  researchers
design special purpose prior distributions to match the application at hand. In the case of sparsity 
an example of the former approach is the spike and slab prior [1  2]  and an example of the latter
approach is the horseshoe prior [3].
We describe a framework for designing prior distributions when the a-priori information include
structural constraints. Our framework follows the maximum entropy principle [4  5]. The distribu-
tion is chosen as one that incorporates known information  but is as difﬁcult as possible to discrim-
inate from the base distribution with respect to relative entropy. The maximum entropy approach

1

has been especially successful with domain knowledge expressed as expectation constraints. In such
cases  the solution is given by a member of the exponential family [6  7] e.g. quadratic constraints
result in the Gaussian distribution. Our work extends this framework to the design of prior distribu-
tions when a-priori information include domain constraints.
Our main technical contributions are as follows:

domain constraints is given by its restriction (Section 2).

• We show that under standard assumptions  the information projection of a base density to
• We show the equivalence between relative entropy inference with data observation con-
• When such restriction is intractable  we propose a family of parameterized approximations

straints and Bayes rule for continuous variables

indexed by subsets of the domain (Section 2.1).

We consider approximate inference in the special case of sparse structure:

tion (Section 3).

• We characterize the restriction precisely  showing that it is given by a conditional distribu-
• We show that the approximate sparse support estimation problem is submodular. As a
e ) factor optimality (Sec-

result  greedy forward selection is efﬁcient and guarantees (1- 1
tion 3.1).

Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging
data  measured by cognitive neuroscientists for analyzing the human brain. The data are repre-
sented using hundreds of thousands of variables. Yet due to real world constraints  most experimen-
tal datasets contain only a few data samples [8]. The proposed approach is applied to predictive
modeling of simulated data and high-dimensional neuroimaging data  and is compared to Bayesian
hierarchical models and non-probabilistic sparse predictive models  showing superior support re-
covery and predictive accuracy (Section 4). Due to space constraints  all proofs are provided in the
supplement.

1.1 Preliminaries

This section includes notation and a few basic deﬁnitions. Vectors are denoted by lower case x and
matrices by capital X. xi j denotes the (i  j)th entry of the matrix X. xi : denotes the ith row of
X and x: j denotes the jth column. Let |X| denote the determinant of X. Sets are denoted by sans
serif e.g. S. The reals are denoted by R. [n] denotes the set of integers {1  . . .   n}  and ℘(n) denotes
the power set of [n]. Let X be either a countable set  or a complete separable metric space equipped
with the standard Borel σ-algebra of measurable set. Let P denote the set of probability densities
on X. For the remainder of this paper  we make the following assumption:
Assumption 1. All distributions P are absolutely continuous with respect to the dominating mea-
sure ν so there exists a density p ∈ P that satisﬁes dP = pdν.
To simplify notation  we use use the standard dν = dx. We also assume that all densities are
bounded. As a consequence of Assumption 1  the relative entropy is given in terms of the densities
as:

(cid:90)

X

2

KL(q(cid:107)p) =

q(x) log

q(x)
p(x)

dx.

The relative entropy is strictly convex with respect to its ﬁrst argument. The information projection
of a probability density p to a constraint set A is given by the solution of:

KL(q(cid:107)p) s.t. q ∈ A.

inf
q∈P

delta functional  denoted by δ(·)  is a generalized set functional that satisﬁes(cid:82)
A f (x)dx  and(cid:82)
(cid:82)

We will only consider projections where A is a closed convex set so the inﬁmum is achieved. The
X δA(x)f (x)dx =
X δA(x)dx = 1  for some some A ⊆ X. The set of domain restricted densities 
denoted by FA for A ⊂ X  is the set of probability density functions supported on A i.e. FA = {q ∈
P | q(x) = 0 ∀ x /∈ A} ∪ {δ{x} ∀ x ∈ A} ⊂ FA ⊂ P = FX. Further  note that FA is closed and
convex for any A ⊆ X (including nonconvex A).

Restriction is a standard approach for deﬁning distributions on subsets A ⊆ X. An important special
case we will consider is when A is a measure zero subset of X. The common conditional density is
one such example  the existence of which follows from the disintegration theorem [9]. Restrictions
of measure require extensive technical tools in the general case [10]. We will employ the following
simplifying condition for the remainder of this manuscript:
Condition 2. The sample space X is a subset of Euclidean space with ν given by the Lebesgue
measure. Alternatively  X is a countable set with ν given by the counting measure.

Let P be a probability distribution on X. Under Assumption 1 and Condition 2  the restriction of
the density p to the set A ⊂ X  if it exists  is given by:

(cid:40) p(x)
(cid:82)
A p(x)dx x ∈ A 

otherwise.

q(x) =

0

2 Priors for structured variables
We assume a-priori information identifying the structure of X via the sub-domain A ⊂ X. We also
assume a pre-deﬁned base distribution P with associated density p. Without loss of generality  let
p have support everywhere1 on X i.e. p(x) > 0 ∀ x ∈ X. Following the principle of minimum
discrimination information  we select the prior as the information projection of the base density p
to FA. Our ﬁrst result identiﬁes the equivalence between information projection subject to domain
constraints and density restriction.
Theorem 3. Under Condition 2  the information projection of the density p to the constraint set FA 
if it exists  is the restriction of p to the domain A.

Theorem 3 gives principled justiﬁcation for the domain restriction approach to structured prior de-
sign. Examples of density restriction in the literature include the truncated Gaussian  Beta and
Gamma densities [11]  and the restriction of the matrix-variate Gaussian to the manifold of low
rank matrices [12]. Various properties of the restriction  such as its shape  and tail behavior (up to
re-scaling) follow directly from the base density. Thus the properties of the resulting prior are more
amenable to analysis when the base measure is well understood. Next  we consider a corollary of
Theorem 3 that was introduced by Williams [13].
Corollary 4. Consider the product space X = W × Y. Let domain constraint be given by W × {ˆy}
for some ˆy ∈ Y. Under Condition 2  the information projection of p to FW×{ ˆy} is given by p(w|ˆy)δ ˆy.
In the Bayesian literature  p(w) is known as the prior  p(y|w) is the likelihood and p(w|ˆy) is the
posterior density given the observation y = ˆy. Corollary 4 considers the information projection
of the joint density p(w  y) given observed data  and shows that the solution recovers the Bayesian
posterior. Williams [13] considered a generalization of Corollary 4  but did not consider projection
to data constraints2. While Corollary 4 has been widely applied in the literature e.g. [14]  to the best
of our knowledge  the presented result is the ﬁrst formal proof.

2.1 Approximate inference for structured variables via tractable subsets

For many structural constraints of interest  restriction requires the computation of an intractable
normalization constant. In theory  rejection sampling and Markov Chain Monte Carlo (MCMC)
inference methods [15] do not require normalized probabilities. However  as many structured sub-
domains are measure zero sets with respect to the dominating measure  randomly generated samples
generated from the base distribution are unlikely to lie in the constrained domains e.g.
random
samples from a multivariate Gaussian are not sparse. Hence rejection sampling fails  and MCMC
suffers from low acceptance probabilities. As a result  inference on such structured sub-domains
typically requires specialized methods e.g. [11  12]. In the following  we propose a class of varia-
tional approximations based on an inner representation of the structured subdomain. Let {Si ∈ A}
represent a (possibly overlapping) partitioning of A into subsets. We deﬁne the domain restricted

1When this condition is violated  we simply redeﬁne X as the subdomain supporting p.
2Speciﬁcally  Williams [13] noted “Relative information has been deﬁned only for unconditional distribu-

tions  which say nothing about the relative probabilities of events of probability zero.“

3

pA

FA

pA∩C

FC

P

p

(a) Gaussian restriction

(b) Sequential projections

Figure 1: (a) Gaussian density and restriction to diagonal line shown. (b) Illustration of Theorem 5;
sequence of information projections P →FA →FC and P →FA∩C are equivalent.

density sets generated by these partitions as FSi  and their union D =(cid:83)FSi. Note that by deﬁnition

each FSi ⊆ D ⊆ FA ⊆ FX. Our approach is to approximate the optimization over densities in FA by
optimizing over D - a smaller subset of tractable densities.
Approximate inference is generally most successful when the approximation accounts for observed
data. Inspired by the results of Corollary 4  we consider such a projection. Let pA(w  y) be the
information projection of the joint distribution p(x  y) to the set FA×{ ˆy}. We propose approximate
inference via the following rule:

pS∗   ˆy = arg min
q∈D×F{ ˆy}

KL(q(w  y)(cid:107)pA(w  y)) = arg min

S

min

q∈FS×{ ˆy}

KL(q(w  y)(cid:107)pA(w  y))

.

(1)

(cid:20)

(cid:21)

Our proposed approach may be decomposed into two steps. The inner step is solved by estimating
a parameterized set of prior densities {qS} corresponding to choices of S  and the outer step is
solved by the selection of the optimal subset S∗. The solution is given by pS∗   ˆy(w  y) = pS∗ (w|ˆy)δ ˆy
(Corollary 4) with the associated approximate posterior given by pS∗ (w|ˆy).
The following theorem considers the effect of a sequence of domain constrained information pro-
jections (see Fig. 1b)  which will useful for subsequent results.
Theorem 5. Let π : [n] (cid:55)→ [n] be a permutation function and {Cπ(i) | Cπ(i) ⊂ X} represent a

sequence of sets with non empty intersection B =(cid:84) Ci (cid:54)= ∅. Given a base density p  let q0 = p  and

deﬁne the sequence of information projections:

qi = arg min
q∈FCπ(i)

KL(q(cid:107)qi−1).

Under Condition 2  q∗ = qN is independent of π. Further q∗ = min
q∈FB

KL(q(cid:107)p).

We apply Theorem 5 to formulate equivalent solutions of (1) that may be simpler to solve.
Corollary 6. Let pS∗   ˆy(w  y) be the solution of (1)  then the posterior distribution pS∗ (w|ˆy) is given
by:
(2)

KL(q(w)(cid:107)p(w|ˆy)).

KL(q(w)(cid:107)pA(w|ˆy)) = arg min
q∈D

pS∗ (w|ˆy) = arg min
q∈D

Corollary 6 implies that we can estimate the approximate structured posterior directly as the in-
formation projection of the unstructured posterior distribution p(w|ˆy). Upon further examination 
Corollary 6 also suggests that the proposed approximation is most useful when there exist subsets
of A such that the restriction of the base density to each subset leads to tractable inference. Further 
the result is most accurate when one of the subsets S∗ ∈ A captures most of the posterior proba-
bility mass. When the optimal subset S∗ is known  the structured prior density associated with the
structured posterior can be computed as shown in the following corollary.
Corollary 7. Let pS∗   ˆy(w  y) be the solution of (1). Deﬁne the density pS∗ (w) as:
KL(q(w)(cid:107)p(w)).

(3)

KL(q(w)(cid:107)pA(w)) = arg min
q∈FS∗

pS∗ (w) = arg min
q∈FS∗

then pS∗ (w) is the prior distribution corresponding to the Bayesian posterior pS∗ (w|ˆy).

4

(cid:20)

(cid:90)

(cid:90)

3 Priors for sparse structure

We now consider a special case of the proposed framework for sparse structured variables. A d
dimensional variable x ∈ X is k-sparse if d − k of its entries take a default value of ci i.e |{i | xi =
ci}| = d−k. In Euclidean space X = Rd and in most cases  ci = 0 ∀ i. Similarly  the distribution P
on the domain X is k-sparse if all random variables X ∼ P are at most k-sparse. The support of x ∈
X is the set supp(x) = {i | xi (cid:54)= ci} ∈ ℘(d). Let S ⊂ X denote the set of variables with support s
i.e. S = {x ∈ X s.t. supp(x) = s}. We will use the notation xS = {xi | i ∈ s}  and its complement
xS(cid:48) = {xi | i ∈ s(cid:48)}  where s(cid:48) = [d]\s. The domain of k sparse vectors is given by the union of all
possible
subset S is a convex set  in fact given by linear subspaces with basis {ei | i ∈ s}. Further  while the
information projection of a base density p to A is generally intractable  the information projection to
its convex subsets S turn out to be computationally tractable. We investigate the application of the
proposed approximation scheme using these subsets.
Consider the information projection of an arbitrary probability measure P with density3 p to the set

(d−k)!k! sparse support sets as A =(cid:83) Si. While the sparse domain A is non-convex  each

d!

D =(cid:83)FSi given by:

(cid:21)

q∈D KL(q(cid:107)p) = min
S∈{Si}

min

min
q∈FS

KL(q(cid:107)p)

= min

S∈{Si} KL(pS(cid:107)p).

(cid:90)

(cid:90)

(cid:90)

Applying Theorem 3  we can compute that pS = p(x)δS(x)/Z  where Z is a normalization factor:

Z =

p(x) =

p(xS  xS(cid:48))δS(x) =

S

X

X

p(xS|xS(cid:48))p(xS(cid:48))δS(x) = p(xS(cid:48) = cS(cid:48)).

Thus  the normalization factor is a marginal density at xS(cid:48) = cS(cid:48). We may now compute the restric-
tion explicitly:

pS(x) =

p(xS|xS(cid:48))p(xS(cid:48))δS(x)

p(xS(cid:48) = cS(cid:48))

= p(xS|xS(cid:48) = cS(cid:48))δS(x).

(4)

In other words  the information projection to a sparse support domain is the density of xS conditioned
on xS(cid:48) = cS(cid:48). The resulting gap is:

KL(pS(cid:107)p) =

pS(x) log

=

pS(x) log

pS(x)
p(x)

p(x)

p(x)p(xS(cid:48) = cS(cid:48))

= − log p(xS(cid:48) = cS(cid:48)).

S
Thus  for a given target sparsity k  we solve:

S

s∗ = arg max

|s|=k

J(s)  where J(s) = log p(xS(cid:48) = cS(cid:48)).

(5)

3.1 Submodularity and Efﬁcient Inference

In this section  we show that the cost function J(s) is monotone submodular  and describe the greedy
forward selection algorithm for efﬁcient inference. Let F : ℘(d) (cid:55)→ R represent a set function. F
is normalized if F (∅) = 0. A bounded F can be normalized as ˜F (s) = F (s) − F (∅) with no
effect on optimization. F is monotonic  if for all subsets u ⊂ v ⊆ ℘(d) it holds that F (u) ≤ F (v).
F is submodular  if for all subsets u  v ⊆ m it holds that F (u ∪ v) + F (u ∩ v) ≤ F (u) + F (v).
Submodular functions have a diminishing returns property [16] i.e.
the marginal gain of adding
elements decreases with the size of the set.
Theorem 8. Let J : ℘(d) (cid:55)→ R  J(s) = log p(xS(cid:48) = cS(cid:48))  and deﬁne ˜J(s) = J(s) − J(∅)  then
˜J(s) is normalized and monotone submodular.

While constrained maximization of submodular functions is generally NP-hard  a simple greedy
forward selection heuristic has been shown to perform almost as well as the optimal in practice  and
is known to have strong theoretical guarantees.

3Where p may represent the conditional densities as in Section 2.1. To simplify the discussion  we suppress

the dependence on ˆy.

5

Theorem 9 (Nemhauser et al. [16]). In the case of any normalized  monotonic submodular function

F  the set s∗ obtained by the greedy algorithm achieves at least a constant fraction(cid:0)1 − 1
objective value obtained by the optimal solution i.e. F (s∗) =(cid:0)1 − 1

(cid:1) of the

F (s).

(cid:1) max

|s|≤k

e

e

In addition  no polynomial time algorithm can provide a better approximation guarantee unless P =
NP [17]. An additional beneﬁt of the greedy approach is that it does not require the decision of the
support size k to be made at training time. As an anytime algorithm  training can be stopped at any k
based on computational constraints  while still returning meaningful results. An interesting special
case occurs when the base density takes a product form.
Corollary 10. Let J(s) be deﬁned as in Theorem 8 and suppose the base density is product form i.e.

p(x) =(cid:81)d

i=1 p(xi)  then J(s) is linear.

In particular  deﬁne h = {p(xi = 0) ∀ i ∈ [d]}  then the solution of (5) is given by set of dimensions
associated with the smallest k values of h.

4 Experiments

We present experimental results comparing the proposed sparse approximate inference projection to
other sparsity inducing models. We performed experiments to test the models ability to estimate the
support of the reconstructed targets and the predictive regression accuracy. The regression accuracy

was measured using the coefﬁcient of determination R2 = 1 −(cid:80)(ˆy − y)2/(cid:80)(y − ¯y)2 where y

is the target response with sample mean ¯y and ˆy is the predicted response. R2 measures the gain in
predictive accuracy compared to a mean model and has a maximum value of 1. The support recovery
was measured using the AUC of the recovered support with respect to the true s∗.
The baseline models are: (i) regularized least squares (Ridge)  (ii) least absolute shrinkage and se-
lection (Lasso) [18]  (iii) automatic relevance determination (ARD) [19]  (iv) Spike and Slab [1  2].
Ridge and Lasso were optimized using implementations from the scikit-learn python package [20].
While Ridge does not return sparse weights  it was included as a baseline for regression performance.
We implemented ARD using iterative re-weighted Lasso as suggested by Wipf and Nagarajan [19].
The noise variance hyperparameter for Ridge and ARD were selected from the set 10{−4 −3 ... 4}.
Lasso was evaluated using the default scikit-learn implementation where the hyperparameter is se-
lected from 100 logarithmically spaced values based on the maximum correlation between the fea-
tures and the response. For each of these models  the hyperparameter was selected in an inner 5-fold
cross validation loop. For speed and scalability  we used a publicly available implementation of
Spike and Slab [21]  which uses a mean ﬁeld variational approximation. In addition to the weights 
Spike and Slab estimates the probability that each dimension is non zero. As Spike and Slab does
not return sparse estimates  sparsity was estimated by thresholding this posterior at 0.5 for each di-
mension (SpikeSlab0.5 )  we also tested the full spike and slab posterior prediction for regression
performance alone (SpikeSlabFull).
The proposed projection approach is designed to be applicable to any probabilistic model. Thus  we
applied the projection approach as additional post-processing for the two Bayesian model baselines.
The ﬁrst method is a projection of the standard Gaussian regression posterior (Sparse-G ) (more
details in supplement). The second is a projection of the spike and spike and slab approximate
posterior (SpikeSlabKL). We note that since the spike and slab approximate posterior uses the mean
ﬁeld approximation  the posterior distribution is in product form and the projection is straightforward
using Corollary 10. Support size selection: The selection of the hyperparameter k - specifying the
sparsity  can be solved by standard model selection routines such as cross-validation. We found that
support size selection using sequential Bayes factors [22] was particularly effective  thus the support
size was selected as the ﬁrst k where log p(y|Sk+1) − log p(y|Sk) < .

4.1 Simulated Data
We generated random high dimensional feature vectors ai ∈ Rd with ai j ∼ N (0  1). The re-
sponse was generated as yi = w(cid:62)ai + νi where νi represents independent additive noise with

νi ∼ N(cid:0)0  σ2(cid:1) for all i ∈ [n]. We set σ2 implicitly via the signal to noise ration (SNR) as

6

(a) AUC as a function of n:k ratio

(b) R2 as a function of n:k ratio

(c) AUC as a function of SNR

(d) R2 as a function of SNR

Figure 2: Simulated data performance: support recovery (AUC ) and regression (R2 ).

SNR = var(y)/σ2  where var(y) is the variance of y. In each experiment  we sampled a sparse
weight vector w by sampling k dimensions at random with from [d]  then we sampled values
wi ∼ N (0  1) and set other dimensions to zero. We performed a series of tests to investigate the
performance of the model in different scenarios. Each experiment was run 10 times with separate
training and test sets. We present the average results on the test set.
Our ﬁrst experiment tested the performance of all models with limited samples. Here we set
k = 20  d = 10  000 and an SNR of 20dB. The number of training values was varied from
n = 100  . . .   400 with 200 test samples. Fig. 2a shows the model performance in terms of sup-
port recovery. With limited training samples  Sparse-G outperformed all the baselines including
Lasso. We also found that SpikeSlabKL consistently outperformed SpikeSlab0.5. We speculate that
the signiﬁcant gap between Sparse-G and SpikeSlabKL may be partly due to the mean ﬁeld assump-
tion in the underlying Spike and Slab. Fig. 2b shows the corresponding regression performance.
Again  we found that Sparse-G outperformed all other baselines  with Ridge achieving the worst
performance.
Our second experiment tested the performance of all models with high levels of noise. Here we
set k = 20  d = 10  000 and n = 200 with 200 test samples. We varied the SNR from 40dB to
−10dB (note that σ2 increases as SNR is decreased). Fig. 2c shows the support recovery perfor-
mance of the different models. We found a performance gap between Sparse-G and Lasso  more
pronounced than in the small sample test. The SpikeSlab0.5 was the worst performing model  but
the performance was improved by SpikeSlabKL. Only Sparse-G achieved perfect support recovery
at low noise (high SNR ) levels. The regression performance is shown in Fig. 2d. While ARD and
Lasso matched Sparse-G at low noise levels (high SNR)  their performance degraded much faster at
higher noise levels (low SNR).

4.2 Functional Neuroimaging Data

Functional magnetic resonance imaging (fMRI) is an important tool for non-invasive study of brain
activity.
fMRI studies involve measurements of blood oxygenation (which are sensitive to the
amount of local neuronal activity) while the participant is presented with a stimulus or cognitive
task. Neuroimaging signals are then analyzed to identify which brain regions which exhibit a sys-
tematic response to the stimulation  and thus to infer the functional properties of those brain regions
[23]. Functional neuroimaging datasets typically consist of a relatively small number of correlated

7

5101520n:k0.50.60.70.80.91.0Support AUCSparse-GLassoARDSpikeSlabKLSpikeSlab0.55101520n:k0.00.20.40.60.81.0R2Sparse-GLassoRidgeARDSpikeSlabFullSpikeSlab0.5SpikeSlabKL403020100-10Signal-to-Noise Ratio(dB)0.50.60.70.80.91.0Support AUCSparse-GLassoSpikeSlab0.5SpikeSlabKLARD403020100-10Signal-to-Noise Ratio(dB)0.00.20.40.60.81.0R2Sparse-GLassoRidgeSpikeSlab0.5SpikeSlabKLARDSpikeSlabFullFigure 3: Support selected by Sparse-G applied to fMRI data with 100 000 voxels. Slices are across
the vertical dimension. Selected voxels are in red.

high dimensional brain images. Hence  capturing the inherent structural properties of the imaging
data is critical for robust inference.
FMRI data were collected from 126 participants while the subjects performed a stop-signal task [24].
For each subject  contrast images were computed for “go” trials and successful “stop” trials using a
general linear model with FMRIB Software Library (FSL)  and these contrast images were used for
regression against estimated stop-signal reaction times. We used the normalized Laplacian of the 3-
dimensional spatial graph of the brain image voxels to deﬁne the precision matrix. This corresponds
to the observation that nearby voxels tend to have similar functional activation. We present the 10-
fold cross validation performance of all models tested on this data. We tested all models using the
high dimensional 100 000 voxel brain image and measured average predictive R2 . The results are:
Sparse-G (0.051)  Lasso (-0.271)  Ridge (-0.473)  ARD (-0.478). The negative test R2 for baseline
models show worse predictive performance than the test mean predictor  and indicate the difﬁculty
of this task. Even with the mean ﬁeld variational inference  the Spike and Slab models did not
scale to this dataset. Only Sparse-G achieved a positive R2 . The support selected by Sparse-G with
all 100 000 voxels is shown in Fig. 3  sliced across the vertical dimension. The recovered voxels
show biologically plausible brain locations including the orbitofrontal cortex  dorsolateral prefrontal
cortex  putamen  anterior cingulate  and parietal cortex  which are correlated with the observed re-
sponse. Further neuroscientiﬁc interpretation and validation will be included in an extended version
of the paper.

5 Conclusion

We present a principled approach for enforcing structure in Bayesian models via structured prior se-
lection based on the maximum entropy principle. The prior is deﬁned by the information projection
of the base measure to the set of distributions supported on the constraint domain. We focus on the
case of sparse structure. While the optimal prior is intractable in general  we show that approximate
inference using selected convex subsets is equivalent to maximizing a submodular function subject
to cardinality constraints  and propose an efﬁcient greedy forward selection procedure which is guar-
anteed to achieve within a (1− 1
e ) factor of the global optimum. For future work  we plan to explore
applications of our approach with other structural constraints such as low rank and structured spar-
sity for matrix-variate sample spaces. We also plan to explore more complicated base distributions
on other samples spaces.
Acknowledgments:
Roadmap for Medical Research grants UL1-DE019580  RL1MH083269  RL1DA024853  PL1MH083271).

fMRI data was provided by the Consortium for Neuropsychiatric Phenomics (NIH

8

References
[1] T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. JASA  83(404):1023–

1032  1988.

[2] H. Ishwaran and J.S. Rao. Spike and slab variable selection: frequentist and bayesian strategies. Annals

of Statistics  pages 730–773  2005.

[3] C. M Carvalho  N.G. Polson  and J.G. Scott. The horseshoe estimator for sparse signals. Biometrika  97

(2):465–480  2010.

[4] E.T. Jaynes.

Information Theory and Statistical Mechanics. Physical Review Online Archive  106(4):

620–630  1957.

[5] S. Kullback. Information Theory and Statistics. Dover  1959.
[6] D. MacKay. Information Theory  Inference and Learning Algorithms. Cambridge University Press  2003.
[7] O. Koyejo and J. Ghosh. A representation approach for relative entropy minimization with expectation

constraints. In ICML WDDL workshop  2013.

[8] R.A. Poldrack. Inferring mental states from neuroimaging data: From reverse inference to large-scale

decoding. Neuron  72(5):692–697  2011.

[9] J.T. Chang and D. Pollard. Conditioning as disintegration. Statistica Neerlandica  51(3):287–317  1997.
[10] A.N. Kolmogorov. Foundations of the theory of probability. Chelsea  New York  1933.
[11] P. Damien and S.G. Walker. Sampling truncated normal  beta  and gamma densities. J. of Computational

and Graphical Statistics  10(2)  2001.

[12] M. Park and J. Pillow. Bayesian inference for low rank spatiotemporal neural receptive ﬁelds. In NIPS 

pages 2688–2696. 2013.

[13] P. Williams. Bayesian conditionalisation and the principle of minimum information. The British Journal

for the Philosophy of Science  31(2):131–144  1980.

[14] O. Koyejo and J. Ghosh. Constrained Bayesian inference for low rank multitask learning. In UAI  2013.
[15] C.P. Robert  G. Casella  and C.P. Robert. Monte Carlo statistical methods  volume 58. Springer New

York  1999.

[16] G.L. Nemhauser  L.A. Wolsey  and M.L. Fisher. An analysis of approximations for maximizing submod-

ular set functions. Mathematical Programming  14(1):265–294  1978.

[17] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM  45(4):634–652  1998.
[18] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

pages 267–288  1996.

[19] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In NIPS  pages 1625–1632 

2007.

[20] F. et. al. Pedregosa. Scikit-learn: Machine learning in Python. JMLR  12:2825–2830  2011.
[21] Michalis K Titsias and Miguel L´azaro-Gredilla. Spike and slab variational inference for multi-task and

multiple kernel learning. In NIPS  volume 24  pages 2339–2347  2011.

[22] Robert E Kass and Adrian E Raftery. Bayes factors. JASA  90(430):773–795  1995.
[23] T.M. Mitchell  R. Hutchinson  R.S. Niculescu  F. Pereira  X. Wang  M. Just  and S. Newman. Learning to

decode cognitive states from brain images. Mach. Learn.  57(1-2):145–175  2004.

[24] Corey N White  Eliza Congdon  Jeanette A Mumford  Katherine H Karlsgodt  Fred W Sabb  Nelson B
Freimer  Edythe D London  Tyrone D Cannon  Robert M Bilder  and Russell A Poldrack. Decompos-
ing decision components in the stop-signal task: A model-based approach to individual differences in
inhibitory control. Journal of Cognitive Neuroscience  2014.

9

,Wouter Koolen
Oluwasanmi Koyejo
Rajiv Khanna
Joydeep Ghosh
Russell Poldrack
Zhaobin Kuang
Sinong Geng
David Page
Jianlong Chang
xinbang zhang
Yiwen Guo
GAOFENG MENG
SHIMING XIANG
Chunhong Pan