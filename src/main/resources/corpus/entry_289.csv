2011,A reinterpretation of the policy oscillation phenomenon in approximate policy iteration,A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach  although fast  is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition  it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally  we report scores in the Tetris problem that improve on existing dynamic programming based results.,A reinterpretation of the policy oscillation
phenomenon in approximate policy iteration

Department of Information and Computer Science

Paul Wagner

Aalto University School of Science

PO Box 15400  FI-00076 Aalto  Finland

pwagner@cis.hut.fi

Abstract

A majority of approximate dynamic programming approaches to the reinforce-
ment learning problem can be categorized into greedy value function methods and
value-based policy gradient methods. The former approach  although fast  is well
known to be susceptible to the policy oscillation phenomenon. We take a fresh
view to this phenomenon by casting a considerable subset of the former approach
as a limiting special case of the latter. We explain the phenomenon in terms of this
view and illustrate the underlying mechanism with artiﬁcial examples. We also
use it to derive the constrained natural actor-critic algorithm that can interpolate
between the aforementioned approaches. In addition  it has been suggested in the
literature that the oscillation phenomenon might be subtly connected to the grossly
suboptimal performance in the Tetris benchmark problem of all attempted approx-
imate dynamic programming methods. We report empirical evidence against such
a connection and in favor of an alternative explanation. Finally  we report scores in
the Tetris problem that improve on existing dynamic programming based results.

1

Introduction

We consider the reinforcement learning problem in which one attempts to ﬁnd a good policy for
controlling a stochastic nonlinear dynamical system. Many approaches to the problem are value-
based and build on the methodology of simulation-based approximate dynamic programming [1  2 
3  4  5]. In this setting  there is no ﬁxed set of data to learn from  but instead the target system  or
typically a simulation of it  is actively sampled during the learning process. This learning setting is
often described as interactive learning (e.g.  [1  §3]).
The majority of these methods can be categorized into greedy value function methods (critic-only)
and value-based policy gradient methods (actor-critic) (e.g.  [1  6]). The former approach  although
fast  is susceptible to potentially severe policy oscillations in presence of approximations. This phe-
nomenon is known as the policy oscillation (or policy chattering) phenomenon [7  8]. The latter
approach has better convergence guarantees  with the strongest case being for Monte Carlo eval-
uation with ‘compatible’ value function approximation. In this case  convergence w.p.1 to a local
optimum can be established under mild assumptions [9  6  4].
Bertsekas has recently called attention to the currently not well understood policy oscillation phe-
nomenon [7]. He suggests that a better understanding of it is needed and that such understanding
“has the potential to alter in fundamental ways our thinking about approximate DP.” He also notes
that little progress has been made on this topic in the past decade. In this paper  we will try to shed
more light on this topic. The motivation is twofold. First  the policy oscillation phenomenon is inti-
mately connected to some aspects of the learning dynamics at the very heart of approximate dynamic

An extended version of this paper is available at http://users.ics.tkk.fi/pwagner/.

1

programming; the lack of understanding in the former implies a lack of understanding in the latter.
In the long run  this state might well be holding back important theoretical developments in the ﬁeld.
Second  methods not susceptible to oscillations have a much better suboptimality bound [7]  which
gives also immediate value to a better understanding of oscillation-predisposing conditions.
The policy oscillation phenomenon is strongly associated in the literature with the popular Tetris
benchmark problem. This problem has been used in numerous studies to evaluate different learning
algorithms (see [10  11]). Several studies  including [12  13  14  11  15  16  17]  have been con-
ducted using a standard set of features that were originally proposed in [12]. This setting has posed
considerable difﬁculties to some approximate dynamic programming methods. Impressively fast
initial improvement followed by severe degradation was reported in [12] using a greedy approxi-
mate policy iteration method. This degradation has been taken in the literature as a manifestation of
the policy oscillation phenomenon [12  8].
Policy gradient and greedy approximate value iteration methods have shown much more stable be-
havior in the Tetris problem [13  14]  although it has seemed that this stability tends to come at the
price of speed (see esp. [13]). Still  the performance levels reached by even these methods fall way
short of what is known to be possible. The typical performance levels obtained with approximate
dynamic programming methods have been around 5 000 points [12  8  13  16]  while an improve-
ment to around 20 000 points has been obtained in [14] by considerably lowering the discount factor.
On the other hand  performance levels between 300 000 and 900 000 points were obtained recently
with the very same features using the cross-entropy method [11  15]. It has been hypothesized in
[7] that this grossly suboptimal performance of even the best-performing approximate dynamic pro-
gramming methods might also have some subtle connection to the oscillation phenomenon. In this
paper  we will also brieﬂy look into these potential connections.
The structure of the paper is as follows. After providing background in Section 2  we discuss the
policy oscillation phenomenon in Section 3 along with three examples  one of which is novel and
generalizes the others. We develop a novel view to the policy oscillation phenomenon in Sections 4
and 5. We validate the view also empirically in Section 6 and proceed to looking for the suggested
connection between the oscillation phenomenon and the convergence issues in the Tetris problem.
We report empirical evidence that indeed suggests a shared explanation to the policy degradation
observed in [12  8] and the early stagnation of all the rest of the attempted approximate dynamic
programming methods. However  it seems that this explanation is not primarily related to the oscil-
lation phenomenon but to numerical instability.

2 Background
A Markov decision process (MDP) is deﬁned by a tuple M = (S A P  r)  where S and A denote
the state and action spaces. St ∈ S and At ∈ A denote random variables on time t  and s  s(cid:48) ∈ S
and a  b ∈ A denote state and action instances. P(s  a  s(cid:48)) = P(St+1 = s(cid:48)|St = s  At = a)
deﬁnes the transition dynamics and r(s  a) ∈ R deﬁnes the expected immediate reward function. A
(soft-)greedy policy π∗(a|s  Q) is a (stochastic) mapping from states to actions and is based on the
value function Q. A parameterized policy π(a|s  θ) is a stochastic mapping from states to actions
and is based on the parameter vector θ. Note that we use π∗ to denote a (soft-)greedy policy  not an
optimal policy. The action value functions Q(s  a) and A(s  a) are estimators of the γ-discounted
t γtE[r(St  At)|S0 = s  A0 = a  π] that follows some (s  a) under some π.

cumulative reward(cid:80)

The state value function V is an estimator of such cumulative reward that follows some s.
In policy iteration  the current policy is fully evaluated  after which a policy improvement step is
taken based on this evaluation. In optimistic policy iteration  policy improvement is based on an
incomplete evaluation. In value iteration  just a one-step lookahead improvement is made at a time.
In greedy value function reinforcement learning (e.g.  [2  3])  the current policy on iteration k is
usually implicit and is greedy (and thus deterministic) with respect to the value function Qk−1 of
the previous policy:

(cid:26) 1

0

π∗(a|s  Qk−1) =

if a = arg maxb Qk−1(s  b)
otherwise.

(1)

Improvement is obtained by estimating a new value function Qk for this policy  after which the
process repeats. Soft-greedy iteration is obtained by slightly softening π∗ in some way so that

2

(cid:32)
φ∗(s  a) −(cid:88)

(cid:33)

π∗(a|s  Qk−1) > 0 ∀a  s  the Gibbs soft-greedy policy class with a temperature τ (Boltzmann
exploration) being a common choice:

π∗(a|s  Qk−1) ∝ eQk−1(s a)/τ .

(2)
We note that (1) becomes approximated by (2) arbitrarily closely as τ → 0 and that this corresponds
to scaling the action values toward inﬁnity.
A common choice for approximating Q is to obtain a least-squares ﬁt using a linear-in-parameters
approximator ˜Q with the feature basis φ∗:

(3)
For the soft-greedy case  one option is to use an approximator that will obtain an approximation of
an advantage function (see [9]):1

˜Qk(s  a  wk) = w(cid:62)

k φ∗(s  a) ≈ Qk(s  a) .

˜Ak(s  a  wk) = w(cid:62)

k

π∗(b|s  ˜Ak−1)φ∗(s  b)

≈ Ak(s  a) .

(4)

b

Convergence properties depend on how the estimation is performed and on the function approxi-
mator class with which Q is being approximated. For greedy approximate policy iteration in the
general case  policy convergence is guaranteed only up to bounded sustained oscillation [2]. Opti-
mistic variants can permit asymptotic convergence in parameters  although the corresponding policy
can manifest sustained oscillation even then [8  2  7]. For the case of greedy approximate value
iteration  a line of research has provided solid (although restrictive) conditions for the approximator
class for having asymptotic parameter convergence (reviewed in  e.g.  [3])  whereas the question of
policy convergence in these cases has been left quite open. In the rest of the paper  our focus will be
on non-optimistic approximate policy iteration.
In policy gradient reinforcement learning (e.g.  [9  6  4  5])  the current policy on iteration k is
explicitly represented using some differentiable stochastic policy class π(θ)  the Gibbs policy with
some basis φ being a common choice:

π(a|s  θ) ∝ eθ(cid:62)φ(s a) .

(5)
Improvement is obtained via stochastic gradient ascent: θk+1 = θk + αk∂J(θk)/∂θ.
In actor-
critic (value-based policy gradient) methods that implement a policy gradient based approximate
policy iteration scheme  the so-called ‘compatibility condition’ is fulﬁlled if the value function is
approximated using (4) with φ∗ = φ and π(θk) in place of π∗( ˜Ak−1) (e.g.  [9]). In this case  the
value function parameter vector w becomes the natural gradient estimate η for the policy π(a|s  θ) 
leading to the natural actor-critic algorithm [13  4]:

(6)
Here  convergence w.p.1 to a local optimum is established for Monte Carlo evaluation under stan-
dard assumptions (properly diminishing step-sizes and ergodicity of the sampling process  roughly
speaking) [9  6]. Convergence into bounded suboptimality is obtained under temporal difference
evaluation [6  5].

η = w .

3 The policy oscillation phenomenon

It is well known that greedy policy iteration can be non-convergent under approximations. The
widely used projected equation approach can manifest convergence behavior that is complex and not
well understood  including bounded but potentially severe sustained policy oscillations [7  8  18].
Similar consequences arise in the context of partial observability for approximate or incomplete state
estimation (e.g.  [19  20  21]).
It is important to remember that sustained policy oscillation can take place even under (asymptotic)
value function convergence (e.g.  [7  8]). Policy convergence can be established under various re-
strictions. Continuously soft-greedy action selection (which is essentially a step toward the policy

1The approach in [4] is needed to permit temporal difference evaluation in this case.

3

gradient approach) has been found to have a beneﬁcial effect in cases of both value function ap-
proximation and partial observability [22]. A notable approach is introduced in [7] wherein it is
also shown that the suboptimality bound for a converging policy sequence is much better. Interest-
ingly  for the special case of Monte Carlo estimation of action values  it is also possible to establish
convergence by solely modifying the exploration scheme  which is known as consistent exploration
[23] or MCESP [24].

Figure 1: Oscillatory examples. Boxes marked with yk denote observations (aggregated states).

Circles marked with wk illustrate receptive ﬁelds of the basis functions. Only non-zero rewards are

shown. Start states: s1 (1.1)  s0 (1.2)  and s1 (1.3). Arrows leading out indicate termination.

The setting likely to be the simplest possible in which oscillation occurs even with Monte Carlo
policy evaluation is depicted in Figure 1.1 (adapted from [21]). The actions al and ar are available
in the decision states s1 and s2. Both states are observed as y1. The only reward is obtained
with the decision sequence (s1  al; s2  ar). Greedy value function methods that operate without
state estimation will oscillate between the policies π(y1) = al and π(y1) = ar  excluding the
exceptions mentioned above. This example can also be used to illustrate how local optima can
arise in the presence of approximations by changing the sign of the reward that follows (s2  ar)
(see [20]). Figure 1.2 (adapted from [25]) shows a more complex case in which a deterministic
optimal solution is attainable. The actions a[1 3] are available in the only decision state s0  which
is observed as y0. Oscillation will occur when using temporal difference evaluation but not with
Monte Carlo evaluation. These two POMDP examples are trivially equivalent to value function
approximation using hard aggregation. Figure 1.3 (a novel example inspired by the classical XOR
problem) shows how similar counterexamples can be constructed also for the case of softer value
function approximation. The action values are approximated with ˜Q(s1  al) = w1 l  ˜Q(s2  al) =
0.5w1 l + 0.5w2 l  ˜Q(s3  al) = w2 l  ˜Q(s1  ar) = w1 r  ˜Q(s2  ar) = 0.5w1 r + 0.5w2 r  and
˜Q(s3  ar) = w2 r. The only reward is obtained with the decision sequence (s1  al; s2  ar; s3  al).
Oscillation will occur even with Monte Carlo evaluation. For other examples  see e.g. [8  19].
A detailed description of the oscillation phenomenon can be found in [8  §6.4] (see also [12  7]) 
where it is described in terms of cyclic sequences in the so-called greedy partition of the value
function parameter space. Although this line of research has provided a concise description of
the phenomenon  it has not fully answered the question of why approximations can introduce such
cyclic sequences in greedy policy iteration and why strong convergence guarantees exist for the
policy gradient based counterpart of this methodology. We will proceed by taking a different view
by casting a considerable subset of the former approach as a special case of the latter.

4 Approximations and attractive stochastic policies

In this section  we brieﬂy and informally examine how policy oscillation arises in the examples in
Section 3. In all cases  oscillation is caused by the presence of an attractive stochastic policy  these
attractors being induced by approximations. In the case of partial observability without proper state
estimation (Figure 1.1)  the policy class is incapable of representing differing action distributions
for the same observation with differing histories. This makes the optimal sequence (y1  al; y1  ar)
inexpressible for deterministic policies  whereas a stochastic policy can still emit it every now and
then by chance. In Figure 1.3  the same situation is arrived at due to the insufﬁcient capacity of the
approximator: the speciﬁed value function approximator cannot express such value estimates that

4

would lead to an implicit greedy policy that attains the optimal sequence (s1  al; s2  ar; s3  al). Gen-
erally speaking  in these cases  oscillation follows from a mismatch between the main policy class
and the exploration policy class: stochastic exploration can occasionally reach the reward  but the
deterministic main policy is incapable of exploiting this opportunity. The opportunity nevertheless
keeps appearing in the value function  leading to repeated failing attempts to exploit it. Consistent
exploration avoids the problem by limiting exploration to only expressible sequences.
Temporal difference evaluation effectively solves for an implicit Markov model [26]  i.e.  it gains
variance reduction in policy evaluation by making the Markov assumption. When this assumption
fails  the value function shows non-existent improvement opportunities. In Figure 1.2  an incorrect
Markov assumption leads to a TD solution that corresponds to a model in which  e.g.  the actually
impossible sequence (y0  a2  r = 0; y2 −  r = +1; end) becomes possible and attractive. Generally
speaking  oscillation results in this case from perceived but non-existent improvement opportunities
that vanish once an attempt is made to exploit them. This vanishing is caused by changes in the
sampling distribution that leads to a different implicit Markov model and  consequently  to a different
ﬁxed point (see [27  18]).
In summary  stochastic policies can become attractive due to deterministically unreachable or com-
pletely non-existing improvement opportunities that appear in the value function. In all cases  the
class of stochastic policies allows gradually increasing the attempt of exploitation of such an oppor-
tunity until it is either optimally exploited or it has vanished enough so as to have no advantage over
alternatives  at which point a stochastic equilibrium is reached.

5 Policy oscillation as sustained overshooting

In this section  we focus more carefully on how attractive stochastic policies lead to sustained policy
oscillation when viewed within the policy gradient framework. We begin by looking at a natural
actor-critic algorithm that uses the Gibbs policy class (5). We iterate by fully estimating ˜Ak in (4)
for the current policy π(θk)  as shown in [4]  and then a gradient update is performed using (6):

θk+1 = θk + αηk .

(7)

Now let us consider some policy π(θk) from such a policy sequence generated by (7) and denote the
corresponding value function estimate by ˜Ak and the natural gradient estimate by ηk. It is shown in
[13] that taking a very long step in the direction of the natural gradient ηk will approach in the limit
a greedy update (1) for the value function ˜Ak:

π(a|s  θk + αηk) lim= π∗(a|s  ˜Ak)   α → ∞  θk (cid:54)→ ∞  η (cid:54)= 0  ∀s  a .

(8)

The resulting policy will have the form

π(a|s  θk + αηk) ∝ eθ(cid:62)

k φ(s a)+αη(cid:62)

k φ(s a) .

(9)
k φ(s  a) dominating the sum when α → ∞. Thus  this type
The proof in [13] is based on the term αη(cid:62)
of a greedy update is a special case of a natural gradient update in which the step-size approaches
inﬁnity.
However  the requirement that θk (cid:54)→ ∞ will hold only during the ﬁrst step using a constant α → ∞ 
assuming a bounded initial θ. Thus  natural gradient based policy iteration using such a very large
but constant step-size does not approach greedy value function based policy iteration after the ﬁrst
such iteration. Little is needed  however  to make the equality apply in the case of full policy
iteration. The cleanest way  in theory  is to use a steeply increasing step-size schedule.
Theorem 1. Let π(θk) denote the kth policy obtained from (7) using the step-sizes α[0 k−1] and
natural gradients η[0 k−1]. Let π∗(wk) denote the kth policy obtained from (1) with inﬁnitely small
added softness and using a value function (4)  with φ∗ = φ and ˜A(w0) being evaluated for π(θ0).
Assume θ0 to be bounded from inﬁnity. Assume all ηk to be bounded from zero and inﬁnity.
If
α0 → ∞ and αk/αk−1 → ∞  ∀k > 0  then π(θk+1) =lim π∗(wk).

Proof. The equivalence after the ﬁrst iteration is proven in [13] with the requirement that the sum
0 φ(s  a). For α0 → ∞  this holds if θ0 is bounded and
in (9) is dominated by the last term α0η(cid:62)

5

η0 (cid:54)= 0. By writing the parameter vector after the second iteration as θ2 = θ0 + α0η0 + α1η1  the
sum becomes

(10)
The requirement for the result in [13] to still apply is that the last term keeps dominating the sum.
Assuming θ0 (cid:54)→ ∞  η0 (cid:54)→ ∞  and η1 (cid:54)= 0  then this condition is maintained if α1 → ∞ and
α1/α0 → ∞. That is  the step-size in the second iteration needs to approach inﬁnity much faster
than the step-size in the ﬁrst iteration. The rest follows by induction.

1 φ(s  a) .

θ(cid:62)
0 φ(s  a) + α0η(cid:62)

0 φ(s  a) + α1η(cid:62)

However  once the ﬁrst update is performed using such a very large step-size  it is no longer possible
to revert back to more conventional step-size schedules: once θ has become very large  any update
performed using a much smaller α will have virtually no effect on the policy. In the following  a
more practical alternative is discussed that both avoids the related numerical issues and that allows
gradual interpolation back toward conventional policy gradient iteration. It also makes it easier to
illustrate the resulting process  which we will do shortly. However  a slight modiﬁcation to the
natural actor-critic algorithm is required.
More precisely  we constrain the magnitude of θ by enforcing (cid:107)θ(cid:107) ≤ c after each update  where (cid:107)θ(cid:107)
is some measure of the magnitude of θ and c is some positive constant. Here the update equation (7)
is replaced by:

if (cid:107)θk + αηk(cid:107) ≤ c
otherwise 

(11)

(cid:26) θk + αηk

θk+1 =

τc(θk + αηk)

where τc = c/(cid:107)θk + αηk(cid:107).
Theorem 2. Let π(θk) and π∗(wk) be as previously  except that (7) is replaced with (11). Let
˜A(w0) to be evaluated for π(θ0). Assume θ0 (cid:54)→ ∞ and all ηk (cid:54)= 0. If c → ∞ and α/c → ∞  then
π(θk+1) =lim π∗(wk).

Proof. The proof in [13] for a single iteration of the unmodiﬁed algorithm requires that the last term
of the sum in (9) dominates. This holds if α/(cid:107)θk(cid:107) → ∞ and ηk (cid:54)= 0. This is ensured during the ﬁrst
iteration by having θ0 (cid:54)→ ∞. After the kth iteration  (cid:107)θk(cid:107) ≤ c due to the constraint in (11)  and the
last term will dominate as long as α/c → ∞ and ηk (cid:54)= 0.
The constraint affects the policy π(θk+1) only when (cid:107)θk + αηk(cid:107) > c  in which case the magnitude
of the parameter vector is scaled down with a factor τc so that it becomes equal to c. This has
a diminishing effect on the resulting policy as c → ∞ because the Gibbs distribution becomes
increasingly insensitive to scaling of the parameter vector when its magnitude approaches inﬁnity:

π(τcθ) lim= π(θ)  

∀τc  θ such that(cid:107)τcθ(cid:107) → ∞  (cid:107)θ(cid:107) → ∞ .

(12)

With a constant α → ∞ and ﬁnite c  the resulting constrained natural actor-critic algorithm (CNAC)
is analogous to soft-greedy iteration in which on-policy Boltzmann exploration with temperature
τ = 1/c is used: constraining the magnitude of θ will effectively ensure some minimum level
of stochasticity in the corresponding policy (there is a mismatch between the algorithms even for
τ = 1/c whenever (cid:107)η(cid:107) (cid:54)= 1). If the soft-greedy method uses (4) for policy evaluation  then exact
equivalence in the limit is obtained when c → ∞ while maintaining α/c → ∞. Lowering α
interpolates toward a conventional natural gradient method. These considerations apply also for (3)
in place of (4) in the soft-greedy method if the indices of the maximizing actions become estimated
equally in both cases: arg maxa
Greedy policy iteration searches in the space of deterministic policies. As noted  the sequence of
greedy policies that is generated by such a process can be approximated arbitrarily closely with the
Gibbs policy class (2) with τ → 0. For this class  the parameters of all deterministic policies lie at
inﬁnity in different directions in the parameter space  whereas stochastic policies are obtained with
ﬁnite parameter values (except for vanishingly narrow directions along diagonals). From this point
of view  the search is conducted on the surface of an ∞-radius sphere: each iteration performs a
jump from inﬁnity in one direction to inﬁnity in some other direction. Based on Theorems 1 and 2 
we observe that the policy sequence that results from these jumps can be approximated arbitrarily
closely with a natural actor-critic method using very large step-sizes.

˜A(s  a  wA) = arg maxb

˜Q(s  b  wQ)  ∀s.

6

The soundness of such a process obviously requires some special structure for the gradient land-
scape.
In informal terms  what sufﬁces is that the performance landscape has a monotonically
increasing proﬁle up to inﬁnity in the direction of a gradient that is estimated at any point. This
condition is established if all attractors in the parameter space reside at inﬁnity and if the gradi-
ent ﬁeld is not  loosely speaking  too ‘curled’. Although we ignore the latter requirement  we note
that the former requirement is satisﬁed when only deterministic attractors exist in the Gibbs pol-
icy space. This condition holds when the problem is fully Markovian and the value function is
represented exactly  which leads to the standard result for MDPs stating that there always exists a
deterministic policy that is globally optimal  that there are no locally optimal policies and that any
potential stochastic optimal policies are not attractive (e.g.  [1  §A.2]). However  when these condi-
tions do not hold and there is an attractor in the policy space that corresponds to a stochastic policy 
there is a ﬁnite attractor in the parameter space that resides inside the ∞-radius sphere.

Figure 2: Performance landscapes and estimated gradient ﬁelds for the examples in Figure 1.

The required special structure is clearly visible in Figure 2.1a  in which the performance landscape
and the gradient ﬁeld are shown for the fully observable (and thus Markovian) counterpart of the
problem from Figure 1.1. This structure can be seen also in Figure 2.2a  in which the problem from
Figure 1.2 is evaluated using Monte Carlo evaluation. The redundant parameters θs1 al and θs2 al in
the former and θy0 a3 in the latter were ﬁxed to zero. In these cases  movement in the direction of
the natural gradient keeps improving performance up to inﬁnity  i.e.  there are no ﬁnite optima in the
way. This structure is clearly lost in Figure 2.1b  which shows the evaluation for the non-Markovian
problem from Figure 1.1. The same holds for the temporal differences based gradient ﬁeld for the
problem from Figure 1.2 that is shown in Figure 2.2b. In essence  the sustained policy oscillation
that results from using very large step-sizes or greedy updates in the latter two cases (2.1b and 2.2b)
is caused by sustained overshooting over the ﬁnite attractor in the policy parameter space.
Another implication of the equivalence between very long natural gradient updates and greedy up-
dates is that  contrary to what is sometimes suggested in the literature  the natural actor-critic ap-
proach has an inherent capability for a speed that is comparable to parametric greedy approaches
with linear-in-parameters value function approximation. This is because whatever initial improve-
ment speed can be achieved with the latter due to greedy updates  the same speed can be also
achieved with the former using the same basis together with very long steps and constraining. This
effectively corresponds to an attempt to exploit whatever remains of the special structure of a Marko-
vian problem  making the use of a very large α in constrained policy improvement analogous to
using a small λ in policy evaluation. Constraining (cid:107)θ(cid:107) also enables interpolating back toward con-
ventional natural policy gradient learning (in addition to offering a crude way of maintaining ex-
plorativity): in cases of partial Markovianity  very long (near-inﬁnite) natural gradient steps can be
used to quickly ﬁnd the rough direction of the strongest attractors  after which gradually decreasing
the step-size allows an ascent toward some ﬁnite attractor.

6 Empirical results

In this section  we apply several variants of the natural actor-critic algorithm and some greedy policy
iteration algorithms to the Tetris problem using the standard features from [12]. For policy improve-
ment  we use the original natural actor-critic (NAC) from [4]  a constrained one (CNAC) that uses
(11) and a very large α  and a soft-greedy policy iteration algorithm (SGPI) that uses (2). For policy
evaluation  we use LSPE [28] and an SVD-based batch solver (pinv). The B matrix in LSPE was
initialized to 0.5I and the policy was updated after every 100th episode. We used the advantage
estimator from (4) unless otherwise stated. We used a simple initial policy (θmaxh = θholes = −3)
that scores around 20 points.

7

θs1 arθs2 ar−202−2021aθy1 alθy1 ar−202−2021bθy0 a1θy0 a2−202−2022aθy0 a1θy0 a2  −202−202return−  +2bFigure 3: Empirical results for the Tetris problem. See the text for details.

Figure 3.1 shows that with soft-greedy policy iteration (SGPI)  it is in fact possible to avoid policy
degradation by using a suitable amount of softening. Results for constrained natural actor-critic
(CNAC) with α = 1010 are shown in Figure 3.2. The algorithm can indeed emulate greedy updates
(SGPI) and the associated policy degradation. Unconstrained natural actor-critic (NAC)  shown in
Figure 3.3  failed to match the behavior and speed of SGPI and CNAC with any step-size (only
selected step-sizes are shown). Results for all algorithms when using the Q estimator in (3) are
shown in Figure 3.4 (technically  CNAC and NAC are not using a natural gradient now). SGPI and
CNAC match perfectly while reaching transiently a level around 50 000 points in just 2 iterations.
We did observe a presence of oscillation-predisposing structure during several runs. There were
optima at ﬁnite parameter values along several consecutively estimated gradient directions  but these
optima did not usually form closed basins of attraction in the full parameter space. At such points 
the performance landscape was reminiscent of what was illustrated in Figure 2.1b  except that there
was a tendency for a slope toward an open end of the valley (ridge) at ﬁnite distance. As a result 
oscillations were mainly transient with suitably chosen learning parameter values.
However  a commonality among all the algorithms was that the relevant matrices became quickly
highly ill-conditioned. This was the case especially when using (4)  with which condition numbers
were typically above 109 upon convergence/stagnation. Figures 3.5 and 3.6 show performance levels
and typical condition numbers for NAC with different discount factors. It can be seen that the inferior
results obtained with a too high γ (cf. [14  12]) are associated with serious ill-conditioning.
In contrast to typical approximate dynamic programming methods  the cross-entropy method in-
volves numerically more stable computations and  moreover  the computations are based on infor-
mation from a distribution of policies. Currently  we expect that the policy oscillation or chattering
phenomenon is not the main cause for neither policy degradation nor stagnation in this problem. In-
stead  it seems that  for both greedy and gradient approaches  the explanation is related to numerical
instabilities that stem possibly both from the involved estimators and from insufﬁcient exploration.

Acknowledgments
We thank Lucian Bus¸oniu and Dimitri Bertsekas for valuable discussions. This work has been
ﬁnancially supported by the Academy of Finland through the Centre of Excellence Programme.

8

05101500.511.52x 104IterationAverage scoreSGPI    τ = 0.1 τ = 0.01τ = 0.001105101500.511.52x 104IterationAverage scoreCNAC   c = 10 c = 50c = 500205101500.511.52x 104IterationAverage scoreNAC     α = 50   α = 75 α = 1000α = 100003051015012345x 104IterationAverage scoreQ estimator from (3)  SGPI"CNAC""NAC" (α=500)"NAC" (α=1010)4051000.511.522.53x 104IterationAverage scoreNAC    γ = 0.8  γ = 0.9γ = 0.975    γ = 15051010810101012IterationCondition numberNAC    γ = 0.8  γ = 0.9γ = 0.975    γ = 16References

[1] C. Szepesv´ari. Algorithms for reinforcement learning. Morgan & Claypool Publishers  2010.
[2] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc  2005.
[3] L. Bus¸oniu  R. Babuˇska  B. De Schutter  and D. Ernst. Reinforcement learning and dynamic programming

using function approximators. CRC Press  2010.

[4] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing  71(7-9):1180–1190  2008.
[5] S. Bhatnagar  R. S. Sutton  M. Ghavamzadeh  and M. Lee. Natural actor-critic algorithms. Automatica 

45(11):2471–2482  2009.

[6] V. R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Optimization 

42(4):1143–1166  2004.

[7] D. P. Bertsekas. Approximate policy iteration: A survey and some new methods. Technical report 

Massachusetts Institute of Technology  Cambridge  US  2010.

[8] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc  1996.
[9] R. S. Sutton  D. Mcallester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement learning

with function approximation. In Advances in Neural Information Processing Systems  2000.
[10] C. Thiery and B. Scherrer. Building Controllers for Tetris. ICGA Journal  32(1):3–11  2009.
[11] I. Szita and A. L¨orincz. Learning Tetris using the noisy cross-entropy method. Neural Computation 

18(12):2936–2941  2006.

[12] D. P. Bertsekas and S. Ioffe. Temporal differences-based policy iteration and applications in neuro-
dynamic programming. Technical report  Massachusetts Institute of Technology  Cambridge  US  1996.
[13] S. M. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems  2002.
[14] M. Petrik and B. Scherrer. Biasing approximate dynamic programming with a lower discount factor. In

Advances in Neural Information Processing Systems  2008.

[15] C. Thiery and B. Scherrer. Improvements on learning Tetris with cross entropy. ICGA Journal  32(1):23–

33  2009.

[16] V. Farias and B. Roy. Tetris: A study of randomized constraint sampling. In Probabilistic and Randomized

Methods for Design Under Uncertainty  pages 189–201. Springer  2006.

[17] V. Desai  V. Farias  and C. Moallemi. A smoothed approximate linear program. In Advances in Neural

Information Processing Systems  2009.

[18] G. J. Gordon. Reinforcement learning with function approximation converges to a region. In Advances

in Neural Information Processing Systems  2001.

[19] S. P. Singh  T. Jaakkola  and M. I. Jordan. Learning without state-estimation in partially observable
In Proceedings of the Eleventh International Conference on Machine

markovian decision processes.
Learning  volume 31  page 37  1994.

[20] M. D. Pendrith and M. J. McGarity. An analysis of direct reinforcement learning in non-markovian

domains. In Proceedings of the Fifteenth International Conference on Machine Learning  1998.

[21] T. J. Perkins. Action value based reinforcement learning for POMDPs. Technical report  University of

Massachusetts  Amherst  MA  USA  2001.

[22] T. J. Perkins and D. Precup. A convergent form of approximate policy iteration. In Advances in Neural

Information Processing Systems  2003.

[23] P. A. Crook and G. Hayes. Consistent exploration improves convergence of reinforcement learning on

POMDPs. In AAMAS 2007 Workshop on Adaptive and Learning Agents  2007.

[24] T. J. Perkins. Reinforcement learning for POMDPs based on action values and stochastic optimization. In
Proceedings of the Eighteenth National Conference on Artiﬁcial Intelligence  pages 199–204. American
Association for Artiﬁcial Intelligence  2002.

[25] G. J. Gordon. Chattering in SARSA(λ). Technical report  Carnegie Mellon University  Pittsburgh  PA 

USA  1996.

[26] R. Parr  L. Li  G. Taylor  C. Painter-Wakeﬁeld  and M. L. Littman. An analysis of linear models  linear
In Proceedings of the

value-function approximation  and feature selection for reinforcement learning.
25th International Conference on Machine learning  pages 752–759. ACM  2008.

[27] D. P. Bertsekas and H. Yu. Q-learning and enhanced policy iteration in discounted dynamic programming.

In Decision and Control (CDC)  2010 49th IEEE Conference on  pages 1409–1416. IEEE  2010.

[28] A. Nedi´c and D. P. Bertsekas. Least squares policy evaluation algorithms with linear function approxima-

tion. Discrete Event Dynamic Systems: Theory and Applications  13(1–2):79–110  2003.

9

,Jacob Gardner
Gustavo Malkomes
Roman Garnett
Kilian Weinberger
Dennis Barbour
John Cunningham
Rebekka Burkholz
Alina Dubatovka