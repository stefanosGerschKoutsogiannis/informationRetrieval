2018,New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity,As an incremental-gradient algorithm  the hybrid stochastic gradient descent (HSGD)  enjoys  merits of both stochastic and full gradient methods for finite-sum minimization problem. However  the existing rate-of-convergence analysis for HSGD is made under with-replacement sampling (WRS) and is restricted to convex problems. It is not clear whether HSGD still carries these advantages under the common practice of without-replacement sampling (WoRS) for non-convex problems. In this paper  we affirmatively answer this open question by showing that under WoRS and for both convex and non-convex problems  it is still possible for HSGD (with constant step-size) to match full gradient descent in rate of convergence  while maintaining comparable sample-size-independent incremental first-order oracle  complexity to stochastic gradient descent. For a special class of finite-sum problems with linear prediction models  our convergence results can be further improved in some cases. Extensive numerical results confirm our theoretical affirmation and demonstrate the favorable efficiency of WoRS-based HSGD.,New Insight into Hybrid Stochastic Gradient Descent:
Beyond With-Replacement Sampling and Convexity

Pan Zhou∗

Xiao-Tong Yuan†

Jiashi Feng∗

∗ Learning & Vision Lab  National University of Singapore  Singapore

† B-DAT Lab  Nanjing University of Information Science & Technology  Nanjing  China

pzhou@u.nus.edu

xtyuan@nuist.edu.cn

elefjia@nus.edu.sg

Abstract

As an incremental-gradient algorithm  the hybrid stochastic gradient descent (HS-
GD) enjoys merits of both stochastic and full gradient methods for ﬁnite-sum
problem optimization. However  the existing rate-of-convergence analysis for
HSGD is made under with-replacement sampling (WRS) and is restricted to con-
vex problems. It is not clear whether HSGD still carries these advantages under
the common practice of without-replacement sampling (WoRS) for non-convex
problems. In this paper  we afﬁrmatively answer this open question by showing
that under WoRS and for both convex and non-convex problems  it is still possi-
ble for HSGD (with constant step-size) to match full gradient descent in rate of
convergence  while maintaining comparable sample-size-independent incremental
ﬁrst-order oracle complexity to stochastic gradient descent. For a special class of
ﬁnite-sum problems with linear prediction models  our convergence results can be
further improved in some cases. Extensive numerical results conﬁrm our theoretical
afﬁrmation and demonstrate the favorable efﬁciency of WoRS-based HSGD.

Introduction

1
We consider the following ﬁnite-sum minimization problem:

(cid:88)n

1
n

i=1

fi(x) 

min
x∈X f (x) :=

(1)
where each individual fi(x) is (cid:96)-smooth and the feasible set X ⊆ Rd is convex. In the ﬁeld of
machine learning  formulation (1) encapsulates a large body of optimization problems including
least square regression  logistic regression and deep neural networks training  to name a few. Such a
problem can be solved by various algorithms  e.g. full gradient descent (FGD) [1]  stochastic GD
(SGD) [2]  hybrid SGD [3]  SDCA [4] and SVRG [5].
In this paper  we are particularly interested in Hybrid SGD (HSGD) [3  6  7] which is an inexact
gradient method that iteratively samples an evolving mini-batch of the terms in (1) for gradient
estimation. The iteration of HSGD is given by

xk+1 = ΦX(cid:0)xk − ηkgk(cid:1)   with gk =

(cid:88)

where ΦX (·) denotes the Euclidean projection onto X   ηk is the learning rate  and Sk denotes the set
of the sk selected samples at the k-th iteration. In early iterations  HSGD selects a few samples to
compute the full gradient approximately; and along with more iterations  sk is increased gradually 
leading to more accurate full gradient estimation. Such a mechanism allows HSGD to simultaneously
enjoy the merits of both SGD and FGD  i.e. rapid initial process of SGD and constant learning rate
ηk without sacriﬁcing the convergence rate of FGD [6].
Motivation. Though HSGD has been shown  both in theory and practice  to bridge smoothly the
gap between full and stochastic gradient descent methods  its rate-of-convergence analysis remains
restrictive in several aspects.
32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

∇fik (xk) 

ik∈Sk

1
sk

(a)

(b)

Figure 1: Comparison of WoRS-based HSGD. (a)
WoRS vs. WRS in HSGD: optimizing a softmax
regression model with a single full pass over the
data letter. (b) Comparison among randomized
algorithms for optimizing a feedforward neural
network with 50 full passes over the data sen-
sorless. HSGD-exp and HSGD-lin respective-
ly denote WoRS based HSGD with exponentially
and linearly increasing mini-batch sizes (ref. Sec-
tion 3.2 and 3.4). See more results in supplement.

First  the convergence behavior of HSGD un-
der without-replacement sampling (WoRS) is
not clear.
In the existing analysis [6]  the s-
tochastic gradient is assumed to be computed
under with-replacement sampling (WRS). But
for stochastic optimization  it is a more com-
mon practice to use WoRS  i.e.  to pass the loss
functions fi(x) sequentially  after random shuf-
ﬂing  without revisiting any of them [8  9]. This
makes signiﬁcant discrepancy between the the-
oretical guarantee and practical implementation.
As shown in Figure 1 (a)  WoRS tends to provide
better performance than WRS in actual imple-
mentation.
Second  the convergence behavior of HSGD for
non-convex problems is not clear. Prior con-
vergence guarantees on HSGD are limited to
convex problems. Bertsekas [3] established lin-
ear convergence of HSGD for least square problems. Friedlander et al. [6] proved that HSGD
converges linearly for strongly convex problems with exponentially increasing sk  and sub-linearly
for arbitrary convex problems with polynomially increasing sk. Unfortunately  non-convex conver-
gence guarantee on HSGD is still absent  though highly desirable in machine learning applications
and extensively studied in other stochastic algorithms  e.g. SVRG [10  11]. In Figure 1 (b)  HSGD has
sharper convergence behavior than several state-of-the-art SGD methods in training neural networks.
Third  the Incremental First-order Oracle (IFO) complexity (i.e. stochastic gradient computation; see
Deﬁnition 2) of HSGD is largely left unknown. Although Friedlander et al. [6] showed that HSGD
maintains steady convergence rates of FGD  its IFO complexity is not explicitly analyzed  making it
less clear where HSGD should be positioned w.r.t. existing stochastic gradient approaches in overall
computational complexity.
Summary of contributions. In this work  we address the aforementioned three limitations in the
existing analysis of HSGD. We analyze the rate-of-convergence of HSGD under WoRS in a wide
problem spectrum including strongly convex  non-strongly convex and non-convex problems. Table 1
summarizes our main results on IFO complexity of HSGD (WoRS) and compares them against
state-of-the-art WoRS-oriented results for (stochastic) gradient methods. These results are divided
into two groups: for general problems and for a special class of problems with linear prediction loss
fi(x) = h(a(cid:62)
i x). As shown in the bottom row of Table 1  we contribute several new theoretical
insights into HSGD  which are elaborated in the following paragraphs.
The bounds highlighted in green: For both general and certain specially structured strongly convex
problems  HSGD is n× faster than FGD. Compared to the results for SAGA and AVRG [12]  the
IFO complexity of HSGD is not relying on the sample size n but dependent on 1/. This suggests
that HSGD will converge faster when n dominates 1/. Finally  compared to the results for SGD in
linear prediction problems [13]  ours has removed the dependency on the logarithm term log (κ/).
The bounds highlighted in red: To our best knowledge  for the ﬁrst time these new results establish
guarantees on WoRS-based stochastic approaches for non-strongly convex and non-convex problems.
The bounds highlighted in blue: If the loss function h(a(cid:62)
i x) in the linearly structured problem is
i x (but f (x) may still be non-strongly convex)  HSGD has O (1/)
strongly convex in terms of a(cid:62)
IFO complexity. The least square regression and logistic regression (with a bounded feasible set)
models have such a linear prediction structure.
The bounds highlighted in brown: When the specially structured problem is non-strongly convex 
HSGD converges to the minimum of problem (1)  while SGD can only be shown to converge to a
sub-optimum up to some statistical error (see footnote 2 below Table 1).
Related work. Understanding randomized algorithms under WoRS and random reshufﬂing is gaining
considerable attention in recent years. By focusing on least squares problems  Recht et al. [14] utilized
arithmetic-mean inequality on matrices to show that for randomized algorithms  WoRS is always
faster than WRS if the data are randomly generated from a certain distribution. For more general

2

0200040006000800010000−10−8−6−4−202Sample NumberObjective Distance log(f − f*) HSGD−exp (WRS)HSGD−exp (WoRS)050100150200246810121416IFO/nObjective value f(x) SGDSVRGSAGAAVRGSCGCHSGD−expHSGD−linTable 1: Comparison of IFO complexity for randomized algorithms under WoRS. κ = (cid:96)/ρ denotes
the condition number of (cid:96)-smooth and ρ-strong convex cases for problem (1). Best viewed in color.
Specially Structured Problem with fi(x) = h(a(cid:62)

General Problem

i x)

Stro. conv.

Non-Stro. conv. Non-conv.

f (·) is
stro. conv.

f (·) is

non-stro. conv.

2≤  for stro. conv.  E[f (xa)−f (x∗)]≤  for non-stro. conv.  E(cid:107)∇f (xa)(cid:107)2

2≤  for non-conv.

h(·) is
stro. conv.

—
—
—

O(cid:0) 1
O(cid:0) 1

—



(cid:1)
(cid:1)

(cid:1)(cid:1)
(cid:1)(cid:1)



Metric: E(cid:107)xa−x∗(cid:107)2
FGD [9]

(cid:1)
O(cid:0) nκ2
SAGA [12] O(cid:0)nκ2log(cid:0) 1
AVRG [12] O(cid:0)nκ2 log(cid:0) 1
O(cid:0) κ2
(cid:1)
O(cid:0) κ
(cid:1)

HSGD

—







—
—
—

O(cid:0) 1
O(cid:0) 1

3

(cid:1)1
(cid:1)1



—

(cid:1)
O(cid:0) nκ2
— O(cid:0)nκ2 log(cid:0) 1
(cid:1)(cid:1)
— O(cid:0)nκ2 log(cid:0) 1
(cid:1)(cid:1)
O(cid:0) κ2
O(cid:0) 1
(cid:1)
(cid:1)
 log(cid:0) κ
(cid:1)(cid:1)
O(cid:0) κ
O(cid:0) κ
O(cid:0) 1
(cid:1)
(cid:1)

—

2









—
—
—

O(cid:0) 1
O(cid:0) 1
O(cid:0) 1

2

3

(cid:1)
(cid:1)2
(cid:1)

—

Metric: E[f (xa)−f (x∗)]≤  for both stro. and non-stro. conv.  E(cid:107)∇f (xa)(cid:107)2
SGD [13]
HSGD
1 Our IFO complexity for arbitrary convex cases appears higher than the non-convex ones  as we use sub-
2 ≤  for non-convex cases.
optimality metric E [f (xa)−f (x∗)] ≤  for convex cases while E(cid:107)∇f (xa)(cid:107)2
√
√
2 Corollary 1 in [13] provides E [f (xa)−f (x∗)] ≤ RT /k+2(12+
√
n where D denotes the diameter
2D)/
of the domain X   k is the iteration number and RT ∼ O(D(cid:96)/
k) is the regret bound of SGD for (1). The
term 2(12 +

√
n is a statistical error which is an artifact from the regret analysis approach.

2≤  for non-conv.

2D)/

√

3

2

3







smooth and strongly convex problems  Gürbüzbalaban et al. [9] proved that gradient descent based
on random reshufﬂing enjoys O(1/k2) rate of convergence after k epoches  as opposed to O(1/k)
under WRS. But this analysis does not explicitly explain why WoRS works well after a few (or
even just one) passes over the data. To answer such a central question  by leveraging regret analysis 
Shamir et al. [13] proved that for a special class of loss functions fi(x) = h(a(cid:62)
i x)  SGD and SVRG
using WoRS can achieve competitive IFO complexity to their WRS counterparts. More recently 
Ying et al. [12] proved that for strongly convex problems  both SAGA [15] and their proposed AVRG
algorithm achieve linear convergence rate with WoRS. Recently  Zhou et al. [7] applied the HSGD
algorithm for solving sparsity or rank-constrained problems and proved its linear convergence rate
under the restricted strong convex and smooth conditions. Our work differs from these prior works:
1) For the ﬁrst time  we provide WoRS based theoretical analysis for HSGD. 2) Our analysis covers
non-strongly convex and non-convex cases which are not covered by the current WoRS analysis of
stochastic gradient methods.

2 Preliminaries

2(cid:107)x1 − x2(cid:107)2

We ﬁrst introduce the concepts of strong convexity and Lipschtiz smoothness which are commonly
used in analyzing stochastic gradient methods [4  5  16  17  18  19].
Deﬁnition 1 (Strong convexity and Lipschitz smoothness). We say a function g(x) is ρ-strongly-
convex if there exists a positive constant ρ such that ∀x1  x2 ∈ X   g(x1)≥ g(x2) + (cid:104)∇g(x2)  x1 −
x2(cid:105) + ρ
2. Moreover  we say g(x) is (cid:96)-smooth if there exists a positive constant (cid:96) such that
(cid:107)∇g(x1) − ∇g(x2)(cid:107)2 ≤ (cid:96)(cid:107)x1 − x2(cid:107)2.
In all our analysis  we will impose the basic Assumption 1 to bound stochastic gradient variance.
Assumption 1 (Bounded gradient). For each loss fi(x)  the distance between its gradient ∇fi(x)
and the full gradient ∇f (x) is upper bounded as maxi (cid:107)∇fi(x)−∇f (x)(cid:107)2≤ G.
If fi(x) is (cid:96)-smooth and the domain of interest X is bounded  then the bounded gradient assumption
can be naturally implied. We explicitly write out this assumption for the sake of notation simplicity.
Following [5  20  21]  we also employ the incremental ﬁrst order oracle (IFO) complexity as the
computational complexity metric for solving the ﬁnite-sum minimization problem (1).
Deﬁnition 2. An IFO takes an index i ∈ [n] and a point x ∈ X   and returns the pair (fi(x) ∇fi(x)).
The IFO complexity can more accurately reﬂect the overall computational performance of a ﬁrst-order
algorithm  as objective value and gradient evaluation usually dominate the per-iteration complexity.

3

Input: Initial point x0  sample index set S ={1 ···  n}  learning rate {ηk}  mini-batch size {sk}.
for k = 0 to T − 1 do

Algorithm 1 Hybrid SGD under WoRS

Select sk samples Sk by WoRS from S −(cid:83)k−1
(cid:80)
Update xk+1 = ΦX(cid:0)xk − ηkgk(cid:1).

Compute the gradient gk = 1
sk

i=0 Si.
∇fik (xk).

ik∈Sk

end for
Output: xa sampled uniformly from {xk}T−1
or {xk}T−1

k=(cid:98)0.5T(cid:99) for non-strongly/non-convex problems.

k=0 for strong convex and linearly structured problems

3 General Analysis for HSGD under WoRS

The WoRS-based HSGD algorithm is outlined in Algorithm 1. Here we systematically analyze
its convergence performance for strongly/non-strongly convex and non-convex problems. Similar
to [13]  we focus our analysis on the scenario where a single pass (or less) over data is of interest 
which occurs  e.g. in streaming data analysis. According to our empirical study (see  e.g.  Figure 3) 
running Algorithm 1 for a single pass over data can provide satisfactory accuracy in many cases.

3.1 A key lemma

It is well understood that unbiased gradient estimation with gradually vanishing variance is important
for accelerating randomized algorithms [5  15]. This is because the increasingly more accurate
estimate of full gradient allows the algorithm to move ahead with more aggressive step-size to
decrease the objective value. However  for WoRS implementation  the mini-batch terms selected at
each iteration are no longer statistically independent  leading to biased gradient estimate gk  i.e.

E[gk] = E(cid:104) 1

(cid:88)

sk

ik∈Sk

∇fik (xk)

(cid:105) (cid:54)= ∇f (xk).

Such a biased estimate gk brings a challenge to bounding its variance E(cid:107)gk−∇f (xk)(cid:107)2
techniques such as Bernstein inequality [22] and those existing bounds on E(cid:107)gk − ∇f (xk)(cid:107)2
WRS [23]. To tackle this challenge  we introduce the following sequence of random variables:

2 with common
2 under

zk = ¯µk − ∇f (xk)

k = n −(cid:80)k−1
i=0 si in which S =
where ¯µk := 1
s(cid:48)
{1  2 ···   n} denotes the index set of all samples. We can prove zk’s form a martingale  i.e.
(cid:105)
E[zk | zk−1  . . .   z0] = zk−1. Moreover  we can show that its squared Euclidian norm is bounded by

and z0 = 0 
i=0 Si and s(cid:48)
(cid:104)

k = S −(cid:83)k−1
(cid:3) ≤ 4G2

(cid:80)
E(cid:2)(cid:107)zk(cid:107)2

∇fik (xk)  S(cid:48)

2 | zk−1  . . .   z0

ik∈S(cid:48)

1 − (n − bk)2 − bk
n(n − bk)

n − bk

 

k

k

i=0 si. Similarly  we deﬁne a sequence of ¯zi for the process of without-replacement

where bk =(cid:80)k−1
sampling a subset (cid:98)Si of size(cid:98)si from S(cid:48)
(cid:88)
ik∈(cid:99)Si

¯zi =

1(cid:98)si
E(cid:2)(cid:107) ¯zi(cid:107)2

2 | ¯zi−1  . . .   ¯z0

Also  we can prove that ¯zi is a martingale with bounded norm:

k of size s(cid:48)
k:
∇fik (xk) − ¯µk

and

¯z0 = 0.

Based on the above arguments  we formulate the k-th WoRS as a stochastic process consisting of
i=0 Si after k − 1
two phases. In the ﬁrst phase  we are given s(cid:48)
times of WoRS over all the data. The sampling result is recorded by zk. Then  in the second phase 
we sample sk data from the remaining s(cid:48)
k in a without-replacement fashion 

k samples indexed by S(cid:48)

k samples indexed by S(cid:48)

(cid:3) ≤ 4G2(cid:98)si

(cid:104)
1 − (cid:98)si − 1

s(cid:48)

k

.

(cid:105)
k = S −(cid:83)k−1

4

which corresponds to ¯zi. Based on such a WoRS process  we have

E[(cid:107)gk − ∇f (xk)(cid:107)2

2]≤2E[(cid:107) ¯µk − ∇f (xk)(cid:107)2
(cid:20)
(cid:20)
2 | ¯zsk−1  . . .   ¯z0; zk−1  . . .   z0]
2 | zk−1  . . .   z0] + 2E[(cid:107) ¯zsk(cid:107)2
1 − (n − bk)2− bk
1 − sk − 1
8G2
n − bk
n(n − bk)
sk

2+(cid:107)gk − ¯µk(cid:107)2
2]

(cid:21)

(cid:21)

+

.

= 2E[(cid:107)zk(cid:107)2
≤ 8G2
n − bk

The above claim leads to the following Lemma 1 which is key to our WoRS-based convergence
analysis in the sections to follow. Notice  the above results on gradient variance are some intermediate
results for proving Lemma 1  whose full proofs are deferred to Appendix A.

Lemma 1. The gradient gk estimated by WoRS in Algorithm 1 satisﬁes E(cid:2)(cid:107)gk−∇f (xk)(cid:107)2

(cid:3) ≤ 24G2

.

2

sk

From Lemma 1  we ﬁnd that the gradient variance E[(cid:107)gk−∇f (xk)(cid:107)2
2] in Algorithm 1 is controlled
by 1/sk. Accordingly  the estimated gradient becomes increasingly more accurate and stable. This
means that by gradually increasing the mini-batch size  HSGD under WoRS can reduce variance 
similar to SVRG and SAGA  but without requiring to integrate historical gradients or full gradient of
the snapshot point into current gradient estimate. In the following sections  we will extensively use
Lemma 1 to analyze HSGD under WoRS.
By applying Bernstein inequality  Friedlander et al. [6] showed that E[(cid:107)gk−∇f (xk)(cid:107)2
if the sk samples selected at iteration k are different  but are sampled from the entire data set.
In contrast  our considered WoRS strategy assumes the sk different samples are drawn from the
remaining set S − ∪k−1
i=0 Si  and thus needs to take into account the statistical dependence among
iterations to bound the stochastic gradient variance.

2] = O(cid:0) n−sk

(cid:1)

nsk

3.2 Strongly convex functions

We analyze the convergence behavior of both the computed solution x and the objective f (x) under
the strongly convex setting. Our convergence result on the computed solution is stated in Theorem 1.
Theorem 1. Suppose f (x) is ρ-strongly-convex and each fi(x) is (cid:96)-smooth. With learning rate ηk =
ρ
(cid:96)2 and mini-batch size sk = τ

(cid:107)x0−x∗(cid:107)2 max(cid:0) 324

(cid:1)  we have

18(cid:96)2 and τ ≥

ρ2   432
(cid:96)2

G2

ζk where ζ = 1 − ρ
E(cid:107)xa − x∗(cid:107)2

2 ≤(cid:0)1 − ρ2

(cid:1)T(cid:107)x0 − x∗(cid:107)2

2 

18(cid:96)2

where xa is the output solution of Algorithm 1 and T is the number of iterations.

1

1

A proof of this result is given in Appendix B.1. From Theorem 1  if mini-batch size is increased
at an exponential rate
18(cid:96)2   then the objective in HSGD converges linearly at the

rate of O(cid:0)(1 − γ)k(cid:1) for strongly convex problems. This implies that HSGD enjoys the merits of

1−γ with γ = ρ

both SGD and FGD. Speciﬁcally  similar to SGD  the per-iteration computation of HSGD is cheap
as it is free of computing the full gradient ∇f (x). Meanwhile  it uses a constant learning rate and
enjoys the steady convergence rate of FGD. As the condition number κ = (cid:96)/ρ is usually large in
realistic problems  the exponential rate
1−γ is actually only slightly above one. This means even
a moderate-scale dataset allows plenty of HSGD iterations in one epoch to decrease the objective
value sufﬁciently  as illustrated in Figure 2 and 3. Friedlander et al. [6] proved that HSGD has linear
convergence rate under WRS. Theorem 1 generalizes the result to WoRS. Then we can derive the
IFO complexity of HSGD for strongly-convex problems in the following corollary  for which proof is
given in Appendix B.2.
2 ≤   the IFO
Corollary 1. Suppose the assumptions in Theorem 1 hold. To achieve E(cid:107)xa − x∗(cid:107)2
ρ denotes the condition number of the objective f (x).
From Corollary 1  the IFO complexity of HSGD for strongly convex problems is at the order of
   HSGD can be superior to

complexity of HSGD is O(cid:0) κ2G2
O(cid:0) κ2
(cid:1)  which is not relying on the sample size n. So when n dominates 1
at each iteration and adopting a diminishing learning rate ηk = O(cid:0) 1

the algorithms with complexity linearly relying on n  such as SVRG and SAGA.
Gürbüzbalaban et al. [9] showed that by processing each individual fi(x) with random shufﬂing
2   1)  the IFO

(cid:1) where κ = (cid:96)

(cid:1) with β ∈ ( 1





kβ

5







n).

(cid:1) for achieving E(cid:107)xa − x∗(cid:107)2

2 ≤ . So HSGD is n times faster than
FGD. This is because at each iteration  unlike FGD requiring to access all data  HSGD only samples
a mini-batch for gradient estimation without sacriﬁcing convergence rate. Ying et al. [12] proved that

complexity of FGD is O(cid:0)κ2 n
(cid:1)(cid:1).
under WoRS  both SAGA and AVRG converge linearly and have IFO complexity of O(cid:0)nκ2 log(cid:0) 1
 log(cid:0) κ
(cid:1)(cid:1) by measuring the objective (see Section 4). Here we can also establish the shaper
O(cid:0) κ

√
Hence  HSGD will outperform SAGA and AVRG if n dominates 1
the data scale is huge while the desired accuracy  is moderately small (e.g. 1/
Shamir [13] proved that for linearly structured problems  SGD under WoRS has IFO complexity

   which is usually the case when

convergence behavior of the objective value. The result is presented in Theorem 2 with proof provided
in Appendix B.3.
Theorem 2. Assume f (x) is ρ-strongly-convex and each fi(x) is (cid:96)-smooth. Let learning rate ηk = 1
(cid:96)
and mini-batch size sk = τ
ρ[f (x0)−f (x∗)] . Then the output xa of Algorithm 1
satisﬁes

E [f (xa) − f (x∗)] ≤(cid:0)1 − ρ
Moreover  to achieve E[f (xa) − f (x∗)] ≤   the IFO complexity of HSGD is O(cid:0) κG2
ly mini-batch size. But it has lower complexity O(cid:0) κ
which is in contrast to the complexity O(cid:0) κ2

ρ .
Theorem 2 shows that HSGD also enjoys linear convergence rate on the objective by using exponential-
2 ≤ . This is because
(cid:96)   while the analysis on the solution

(cid:1)T
(cid:1)  where κ = (cid:96)
(cid:1) under the measurement E[f (xa)− f (x∗)] ≤ 

(cid:1) for achieving E(cid:107)xa − x∗(cid:107)2

the objective analysis allows to use more aggressive step-size 1
requires smaller learning rate ρ

(cid:96)2 . In this way  HSGD with larger step-size converges faster.

(f (x0) − f (x∗)).

ζk with ζ = 1− ρ

2(cid:96) and τ ≥

6G2

2(cid:96)







3.3 Non-strongly convex functions

We proceed to analyze the convergence performance of HSGD for non-strongly convex problems. Our
result for this case is summarized in Theorem 3. To our best knowledge  this is the ﬁrst convergence
guarantee of WoRS-based methods for non-strongly convex problems.
Theorem 3. Suppose f (x) is convex and each fi(x) is (cid:96)-smooth. Assume that (cid:107)x1−x2(cid:107)2≤ D holds
for ∀x1  x2 ∈X . Then with the learning rate ηk = 1
2(cid:96) and mini-batch size sk = (k + 1)2  we have

E[f (xa) − f (x∗)] ≤ 4(cid:96)D2 + 24GD

T

+

48G2
(cid:96)T 2  

where xa denotes the output solution of Algorithm 1 and T is the number of iterations.

A proof of this result is given in Appendix B.4. Theorem 3 shows that if one expands the mini-batch
was established for WoRS-based SGD in a special class of convex problems with fi(x) = hi((cid:104)ai  x(cid:105)).
A detailed comparison between their result and ours for such a structured formulation will be discussed
k=0 (cid:107)gk−∇f (xk)(cid:107)2 < +∞  Friedlander et al. [6] showed

size at O(cid:0)k2(cid:1)  then the convergence rate of HSGD under WoRS is O(cid:0) 1
in Section 4. Under the assumption(cid:80)+∞
that WRS-based HSGD outputs f (xa) − f (x∗) = O(cid:0) 1
if HSGD selects at least O(cid:0)k2(cid:1) samples at the k-th iteration due to E[(cid:107)gk−∇f (xk)(cid:107)2
IFO complexity of HSGD is O(cid:0) (6GD+(cid:96)D2)3

(cid:1). In [13]  a sub-linear rate
(cid:1). However  such an assumption holds only
2] = O(cid:0) n−sk
(cid:1).

In this way  their result under WRS is of the same order as ours under WoRS. The following corollary
gives the corresponding IFO complexity. A proof of this result is given in Appendix B.5.
Corollary 2. Suppose the assumptions in Theorem 3 hold. To achieve E[f (xa) − f (x∗)] ≤   the

(cid:1).

nsk

T

T

3

3.4 Non-convex functions

Now we analyze HSGD for non-convex problems  which to our knowledge has not yet been addressed
elsewhere in literature. The result is stated in Theorem 4 with proof provided in Appendix B.6.
Theorem 4. Suppose each fi(x) is (cid:96)-smooth and for ∀x1  x2 ∈X   (cid:107)x1 − x2(cid:107)2 ≤ D. With learning
2(cid:96) and mini-batch size sk = k + 1  the output xa of Algorithm 1 with T iterations satisﬁes
rate ηk = 1

E(cid:2)(cid:107)∇f (xa)(cid:107)2

(cid:3) ≤ 4(cid:96)2D2 + 35G2

.

2

T

6

linearly expanding the mini-batch size at each iteration. Here we follow the convention in [10  11  23]
to adopt the value (cid:107)∇f (xa)(cid:107)2
2 as a measurement of quality for approximate stationary solutions.
Then we drive the IFO complexity of HSGD in the following corollary with proof in Appendix B.7.

Theorem 4 guarantees that for non-convex problems  HSGD exhibits O(cid:0) 1
Corollary 3. Suppose the assumptions in Theorem 4 hold. To achieve E(cid:2)(cid:107)∇f (xa)(cid:107)2
complexity of the HSGD in Algorithm 1 is O(cid:0) (4(cid:96)2D2+35G2)2
arbitrary convex problems and E(cid:2)(cid:107)∇f (xa)(cid:107)2

(cid:3) ≤  for non-convex problems.

The IFO complexity for non-convex problems looks lower than that for non-strongly convex ones
in Corollary 2. This is because we use E [f (xa − f (x∗)] ≤  as sub-optimality measurement for

(cid:1) rate of convergence by
(cid:3) ≤   the IFO

(cid:1).

2

T

2

2

4 Analysis for Linearly Structured Problems

We further consider a special case of problem (1) where each fi(x) has a linear prediction structure:



i=1

1
n

fi(x)  where fi(x) = h((cid:104)ai  x(cid:105)).

f (x) :=

2 (bi − a(cid:62)

(2)
Here ai denotes the i-th sample vector and h(·) denotes a convex loss function. Such a formulation
covers several common problems in machine learning  such as fi(x) = 1
i x)2 for least
square regression and fi(x) = log(1 + exp(−bia(cid:62)
i x)) for logistic regression  where bi is the real or
binary target output. Such a special problem setting has been considered in [13] for analyzing SGD
under WoRS. To make a comprehensive comparison  we specify our strongly convex analysis to (2) 
and improve our non-strongly-convex results when the surrogate loss h(·) is strongly convex.
Strongly convex case. In this case  according to Theorem 2  HSGD converges linearly and its IFO

complexity is O(cid:0) κ
(cid:1). By comparison  SGD under WoRS in [13] converges at O(cid:0) κ
IFO complexity O(cid:0) κ
 log(cid:0) κ
complexity O(cid:0)(cid:0)n + κ log(cid:0) 1

(cid:1)(cid:1) and has
(cid:1)(cid:1)  slightly higher than ours due to the presence of the factor log(cid:0) κ
(cid:1).
(cid:1)(cid:1) in ridge regression with the measurement E[f (x)− f (x∗)].
(cid:1)(cid:1) log(cid:0) 1

Moreover  it is allowed in HSGD to use constant step-size which is required to be shrinking in [13].
On this special problem  other results on general strongly convex problems can also be applied.
As discussed in Section 3.2  HSGD is n times faster than FGD [9]  and is superior to SAGA [12]
and AVRG [12] when n dominates 1
 . Shamir [13] showed that SVRG [5] under WoRS has IFO

T log(cid:0) 1

Comparatively  such an IFO complexity is still higher than HSGD when sample size n is large and
the desired accuracy is moderately small.
Non-strongly convex case with strongly-convex h(·). When the loss f (x) in (2) is non-strongly
convex but the surrogate loss h(·) is strongly convex  we show an improved convergence rate in
Theorem 5 than that in Theorem 3 for general cases. See proof of Theorem 5 in Appendix C.1.
i x) is (cid:96)-smooth and h(·) is α-strongly convex. Let σ(A) denote
Theorem 5. Suppose fi(x) = h(a(cid:62)
the smallest non-zero singular value of the matrix A = [a(cid:62)
n ] and µ = ασ(A). If the
ζk with τ ≥
(cid:1)T
2(cid:96)   we have
learning rate ηk = 1

1 ; a(cid:62)
µ[f (x0)−f (x∗)] and ζ = 1 − µ
(f (x0) − f (x∗)) 

E [f (xa) − f (x∗)] ≤(cid:0)1 − µ

(cid:96) and mini-batch size sk = τ

2 ; . . .   a(cid:62)

24G2

T









(cid:88)n

2(cid:96)

where xa denotes the output solution of Algorithm 1 and T is the number of iterations.
Theorem 5 shows if the function h(a(cid:62)
i x  by
exponentially sampling the data at each iteration  HSGD converges linearly even though f (x) might
be non-strongly convex. Based on Theorem 5  we further derive the IFO complexity of Algorithm 1
for such a special problem  as summarized in Corollary 4 with proof in Appendix C.2.
Corollary 4. Suppose the assumptions in Theorem 5 hold. To achieve E[f (xa) − f (x∗)] ≤  for the

special problem  the IFO complexity of the proposed algorithm is O(cid:0) (cid:96)G2

i x) is strongly convex in terms of the linear prediction a(cid:62)

(cid:1).

µ2

√
It is interesting to compare Theorem 5 and Corollary 4 with those existing ones for SGD. Particularly 
it was shown by Shamir [13] that E [f (xa) − f (x∗)] ≤ RT /T + 2(12 +
√
n for SGD 
2D)/
where RT is the regret bound of SGD on problem (2)  at the order of O(D(cid:96)
T ). This gives a

√

7

√

√

T ) and IFO complexity of O(1/2). However  there exists an accuracy
convergence rate of O(1/
barrier O (1/
n which is the artifact brought by
analyzing the regret. In sharp contrast  our result in Theorem 5 guarantees that HSGD converges to
the global optimum of problem (2). More importantly  provided that h(·) is strongly convex  HSGD

n) due to the statistical error term 2(12 +

has superior IFO complexity of O(cid:0) 1

(cid:1) to the SGD complexity O(cid:0) 1

(cid:1) given in [13].

√
2D)/

√



2

5 Experiments

We compare HSGD with several state-of-the-art algorithms  including SGD [2]  SVRG [5] 
SAGA [15]  AVRG [12] and SCGC [23]  under WoRS for all. We consider two set-
s of learning tasks.
(cid:96)2-regularized logistic re-
gression minx
  where bi is the target output of ai.
minx
The other one is a non-convex problem of training multi-layer neural networks. We run simulations
on 10 datasets (see Appendix D). Hyper-parameters of all the algorithms are tuned to best.

(cid:2)log(1 + exp(−bia(cid:62)
i x)) + λ
(cid:80)k
exp(a(cid:62)
i xj )
l=1exp(a(cid:62)

(cid:3) and k-classes softmax regression
(cid:105)

(cid:80)n
(cid:104) λ
2k(cid:107)xj(cid:107)2

The ﬁrst contains two convex problems:

2−1{bi = j} log

(cid:80)n

(cid:80)k

2(cid:107)x(cid:107)2

i xl)

j=1

i=1

1
n

1
n

i=1

2

(a) Logistic regression. From left to right: ijcnn  A09  w08 and rcv11.

(b) Softmax regression. From left to right: protein  satimage  sensorless and mnist.

Figure 2: Single-epoch processing: comparison of randomized algorithms for a single pass over data.

5.1 Convex problems

As the ﬁrst set of problems are strongly convex  we follow Theorem 2 to exponentially expand the
mini-batch size sk in HSGD with τ = 1. We run FGD until the gradient (cid:107)∇f (x)(cid:107)2 ≤ 10−10. Then
use the output as the optimal value f∗ for sub-optimality estimation in Figure 1 (a)  2 and 3.
Single-epoch processing in well-conditioned problems. We ﬁrst consider the case where the
optimization problem is well-conditioned with strong regularization  such that good results can be
obtained after only one epoch of data pass. Single-epoch learning is common in online learning. For
two problems  we respectively set their regularization parameters to λ = 0.01 and λ = 0.1.
Figure 2 summarizes the numerical results. On the simulated well-conditioned tasks most algorithms
achieve high accuracy after one epoch  while HSGD (WoRS) converges much faster. This conﬁrms

Corollary 1 that HSGD is cheaper in IFO complexity (O(cid:0) κ2
algorithms (O(cid:0)nκ2 log(cid:0) 1

(cid:1)) than other considered variance-reduced
(cid:1)(cid:1)) when the desired accuracy is moderately low and data size is large.





Multi-epoch processing in ill-conditioned problems. To solve more challenging problems  a
method usually needs multiple cycles of data processing to reach high accuracy solution. Thus we
develop a practical implementation of HSGD for multiple epochs processing. After visiting all data
in one full pass  it continues to increase the mini-batch size  allowing possible with-replacement
sampling  until sk > n. After that  HSGD degenerates to standard FGD. But this does not happen in

8

010000200003000040000−12−10−8−6−4−20Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD050001000015000200002500030000−12−10−8−6−4−202Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD010000200003000040000−12−10−8−6−4−202Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD05000100001500020000−10−505Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0500010000−8−6−4−202Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD01000200030004000−8−6−4−20Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0500100015002000−6−5−4−3−2−101Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0200004000060000−8−6−4−2024Sample NumberObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGDour testing cases  since we set the exponential rate sufﬁciently small. To generate more challenging
optimization tasks  we reset the regularization strength parameter in softmax regression as λ = 0.001.
Figure 3 shows that HSGD under WoRS outperforms all compared algorithms. These observations
align well with those in Figure 2  implying HSGD has sharper convergence behavior when the sample
size n is large and the desired accuracy is moderate. The convergence curves of HSGD also conﬁrm
the effectiveness of our practical implementation in continuously decreasing the objective value.

Figure 3: Multi-epoch processing: comparison of randomized algorithms for multiple passes over
data (Softmax regression. From left to right: protein  satimage  sensorless and letter).

5.2 Non-convex problems

Here we evaluate HSGD for optimizing a three-layer feedforward neural network with a logistic loss
on ijcnn1 and covtype and softmax loss on sensorless (see Figure 1 (b)). For both cases we set
λ = 0.01. The network has an architecture of d − 30 − c  where d and c respectively denote the
input and output dimension and 30 is the neuron number in the hidden layer. We test two versions of
HSGD  namely HSGD-lin and HSGD-exp  respectively with linearly and exponentially increasing
mini-batch size from s0 = 1. We use the same initialization for all algorithms.
From Figure 4  HSGD-exp exhibits similar convergence behavior as above: it decreases the loss very
quickly. Comparatively  HSGD-lin outputs more accurate solutions with linearly increasing batch
size  which is consistent with Theorem 4. We note HSGD-lin behaves differently in Figure 4 (a) and
(b). In Figure 4 (a)  it converges relatively slowly at the beginning  while in Figure 4 (b) much faster 
of which we attribute the reason to the different characteristics of data.

Figure 4: Non-convex results: comparison of randomized algorithms on forward neural networks.

(a) ijcnn1

(b) covtype

6 Conclusion

We analyzed the rate-of-convergence of HSGD under WoRS for strongly/arbitrarily convex and
non-convex problems. We proved that under WoRS  HSGD with constant step-size can match FG
descent in convergence rate  while maintaining comparable sample-size-independent IFO complexity
to SGD. Compared to the variance-reduced SGD methods such as SVRG and SAGA  HSGD tends to
gain better efﬁciency and scalability in the setting where the sample size is large while the required
optimization accuracy is moderately small. Numerical results conﬁrmed our theoretical results.

Acknowledgements

Jiashi Feng was partially supported by NUS startup R-263-000-C08-133  MOE Tier-I R-263-000-
C21-112  NUS IDS R-263-000-C67-646  ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-
D17-112. Xiao-Tong Yuan was supported in part by Natural Science Foundation of China (NSFC)
under Grant 61522308 and Grant 61876090  and in part by Tencent AI Lab Rhino-Bird Joint Research
Program No.JR201801.

9

0481216−8−6−4−20IFO/nObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0481216−8−6−4−20IFO/nObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0481216−8−6−4−20IFO/nObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0481216−8−6−4−202IFO/nObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0481216−5−4−3−2−10IFO/nObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0481216−7−6−5−4−3−2−101IFO/nObjective Distance log(f − f*) SGDSVRGSAGAAVRGSCGCHSGD0102030405000.511.522.533.54IFO/nObjective value f(x) SGDSVRGSAGAAVRGSCGCHSGD−expHSGD−lin01020304050−6−4−20246IFO/nlog(k∇f(x)k22) SGDSVRGSAGAAVRGSCGCHSGD−expHSGD−lin01020304050051015IFO/nObjective value f(x) SGDSVRGSAGAAVRGSCGCHSGD−expHSGD−lin01020304050−4−3−2−10IFO/nlog(k∇f(x)k22) SGDSVRGSAGAAVRGSCGCHSGD−expHSGD−linReferences
[1] M. A. Cauchy. Méthode générale pour la résolution des systèmes d’équations simultanées. Comptesrendus

des séances de l’Académie des sciences de Paris  25:536–538  1847. 1

[2] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics 

22(3):400–407  1951. 1  8

[3] D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM Journal on

Optimization  7(4):913–926  1997. 1  2

[4] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. J. of Machine Learning Research  14(Feb):567–599  2013. 1  3

[5] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Proc. Conf. Neutral Information Processing Systems  pages 315–323  2013. 1  3  4  7  8

[6] M. P. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting. SIAM Journal

on Scientiﬁc Computing  34(3):A1380–A1405  2012. 1  2  5  6

[7] P. Zhou  X. Yuan  and J. Feng. Efﬁcient stochastic gradient hard thresholding. In Proc. Conf. Neutral

Information Processing Systems  2018. 1  3

[8] L. Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proc. Symposium

on Learning and Data Science  Paris  2009. 2

[9] M. Gürbüzbalaban  A. Ozdaglar  and P. Parrilo. Why random reshufﬂing beats stochastic gradient descent.

arXiv preprint arXiv:1510.08560  2015. 2  3  5  7

[10] S. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for nonconvex

optimization. In Proc. Int’l Conf. Machine Learning  pages 314–323  2016. 2  7

[11] Z. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In Proc. Int’l Conf.

Machine Learning  pages 699–707  2016. 2  7

[12] B. Ying  K. Yuan  and A. H. Sayed. Convergence of variance-reduced stochastic learning under random

reshufﬂing. arXiv preprint arXiv:1708.01383  2017. 2  3  6  7  8

[13] O. Shamir. Without-replacement sampling for stochastic gradient methods.

Information Processing Systems  pages 46–54  2016. 2  3  4  6  7  8

In Proc. Conf. Neutral

[14] B. Recht and C. Ré. Toward a noncommutative arithmetic-geometric mean inequality: conjectures 

case-studies  and consequences. In Conf. on Learning Theory  pages 1–11  2012. 2

[15] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Proc. Conf. Neutral Information Processing Systems  pages
1646–1654  2014. 3  4  8

[16] N. L. Roux  M. Schmidt  and F. R. Bach. A stochastic gradient method with an exponential convergence
rate for ﬁnite training sets. In Proc. Conf. Neutral Information Processing Systems  pages 2663–2671 
2012. 3

[17] A. Defazio  J. Domke  and T. S. Caetano. Finito: A faster  permutable incremental gradient method for big

data problems. In Proc. Int’l Conf. Machine Learning  pages 1125–1133  2014. 3

[18] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Proc. Conf. Neutral

Information Processing Systems  pages 3384–3392  2015. 3

[19] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In ACM SIGACT

Symposium on Theory of Computing  pages 1200–1205  2017. 3

[20] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization.

In Proc. Int’l Conf. Machine Learning  pages 353–361  2015. 3

[21] Q. Lin  Z. Lu  and L. Xiao. An accelerated proximal coordinate gradient method. In Proc. Conf. Neutral

Information Processing Systems  pages 3059–3067  2014. 3

[22] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. In Proc. Int’l

Conf. Machine Learning  2017. 4

[23] L. Lei and M. Jordan. Less than a single pass: Stochastically controlled stochastic gradient. In Artiﬁcial

Intelligence and Statistics  pages 148–156  2017. 4  7  8

10

,Guillaume Papa
Stéphan Clémençon
Aurélien Bellet
Pan Zhou
Xiaotong Yuan
Jiashi Feng