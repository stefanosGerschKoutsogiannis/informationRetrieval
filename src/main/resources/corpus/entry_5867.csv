2017,Variational Laws of Visual Attention for Dynamic Scenes,Computational models of visual attention are at the crossroad of disciplines like cognitive science  computational neuroscience  and computer vision. This paper proposes a model of attentional scanpath that is based on the principle that there are foundational laws that drive the emergence of visual attention. We devise variational laws of the eye-movement that rely on a generalized view of the Least Action Principle in physics. The potential energy  captures details  as well as peripheral visual features  while the kinetic energy corresponds with the classic interpretation in analytic mechanics. In addition  the Lagrangian contains a brightness invariance term  which characterizes significantly the scanpath trajectories. We obtain differential equations of visual attention as the stationary point of the generalized action  and we propose an algorithm to estimate the model parameters.  Finally  we report experimental results to validate the model in tasks of saliency detection.,Variational Laws of

Visual Attention for Dynamic Scenes

Dario Zanca

DINFO  University of Florence

DIISM  University of Siena
dario.zanca@unifi.it

Marco Gori

DIISM  University of Siena
marco@diism.unisi.it

Abstract

Computational models of visual attention are at the crossroad of disciplines like
cognitive science  computational neuroscience  and computer vision. This paper
proposes a model of attentional scanpath that is based on the principle that there
are foundational laws that drive the emergence of visual attention. We devise varia-
tional laws of the eye-movement that rely on a generalized view of the Least Action
Principle in physics. The potential energy captures details as well as peripheral
visual features  while the kinetic energy corresponds with the classic interpretation
in analytic mechanics. In addition  the Lagrangian contains a brightness invariance
term  which characterizes signiﬁcantly the scanpath trajectories. We obtain differ-
ential equations of visual attention as the stationary point of the generalized action 
and we propose an algorithm to estimate the model parameters. Finally  we report
experimental results to validate the model in tasks of saliency detection.

1

Introduction

Eye movements in humans constitute an essential mechanism to disentangle the tremendous amount
of information that reaches the retina every second. This mechanism in adults is very sophisticated.
In fact  it involves both bottom-up processes  which depend on raw input features  and top-down
processes  which include task dependent strategies [2; 3; 4]. It turns out that visual attention is
interwound with high level cognitive processes  so as its deep understanding seems to be trapped
into a sort of eggs-chicken dilemma. Does visual scene interpretation drive visual attention or the
other way around? Which one “was born” ﬁrst? Interestingly  this dilemma seems to disappears
in newborns: despite their lack of knowledge of the world  they exhibit mechanisms of attention to
extract relevant information from what they see [5]. Moreover  there are evidences that the very ﬁrst
ﬁxations are highly correlated among adult subjects who are presented with a new input [25]. This
shows that they still share a common mechanism that drive early ﬁxations  while scanpaths diverge
later under top-down inﬂuences.
Many attempts have been made in the direction of modeling visual attention. Based on the feature
integration theory of attention [14]  Koch and Ullman in [9] assume that human attention operates
in the early representation  which is basically a set of feature maps. They assume that these maps
are then combined in a central representation  namely the saliency map  which drives the attention
mechanisms. The ﬁrst complete implementation of this scheme was proposed by Itti et al. in [10].
In that paper  feature maps for color  intensity and orientation are extracted at different scales.
Then center-surround differences and normalization are computed for each pixel. Finally  all this
information is combined linearly in a centralized saliency map. Several other models have been
proposed by the computer vision community  in particular to address the problem of reﬁning saliency
maps estimation. They usually differ in the deﬁnition of saliency  while they postulate a centralized
control of the attention mechanism through the saliency map. For instance  it has been claimed that

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the attention is driven according to a principle of information maximization [16] or by an opportune
selection of surprising regions [17]. A detailed description of the state of the art is given in [8].
Machine learning approaches have been used to learn models of saliency. Judd et al. [1] collected
1003 images observed by 15 subjects and trained an SVM classiﬁer with low-  middle-  and high-level
features. More recently  automatic feature extraction methods with convolutional neural networks
achieved top level performance on saliency estimation [26; 18].
Most of the referred papers share the idea that saliency is the product of a global computation. Some
authors also provide scanpaths of image exploration  but to simulate them over the image  they all use
the procedure deﬁned by [9]. The winner-take-all algorithm is used to select the most salient location
for the ﬁrst ﬁxation. Then three rules are introduced to select the next location: inhibition-of-return 
similarity preference  and proximity preference. An attempt of introducing biological biases has been
made by [6] to achieve more realistic saccades and improve performance.
In this paper  we present a novel paradigm in which visual attention emerges from a few unifying
functional principles. In particular  we assume that attention is driven by the curiosity for regions with
many details  and by the need to achieve brightness invariance  which leads to ﬁxation and motion
tracking. These principles are given a mathematical expression by a variational approach based on
a generalization of least action  whose stationary point leads to the correspondent Euler-Lagrange
differential equations of the focus of attention. The theory herein proposed offers an intriguing model
for capturing a mechanisms behind saccadic eye movements  as well as object tracking within the
same framework. In order to compare our results with the state of the art in the literature  we have
also computed the saliency map by counting the visits in each pixel over a given time window  both
on static and dynamic scenes. It is worth mentioning that while many papers rely on models that are
purposely designed to optimize the approximation of the saliency map  for the proposed approach
such a computation is obtained as a byproduct of a model of scanpath.
The paper is organized as follows. Section 2 provides a mathematical description of the model and
the Euler-Lagrange equations of motion that describe attention dynamics. The technical details 
including formal derivation of the motion equations  are postponed to the Appendix. In the Section 3
we describe the experimental setup and show performance of the model in a task of saliency detection
on two popular dataset of images [12; 11] and one dataset of videos [27]. Some conclusions and
critical analysis are ﬁnally drawn in Section 4.

2 The model

In this section  we propose a model of visual attention that takes place in the earliest stage of vision 
which we assume to be completely data driven. We begin discussing the driving principles.

2.1 Principles of visual attention

The brightness signal b(t  x) can be thought of as a real-valued function

(1)
where t is the time and x = (x1  x2) denotes the position. The scanpath over the visual input is
deﬁned as

b : R+ × R2 → R

x : R+ → R2

The scanpath x(t) will be also referred to as trajectory or observation.
Three fundamental principles drive the model of attention. They lead to the introduction of the
correspondent terms of the Lagrangian of the action.

(2)

(3)

i) Boundedness of the trajectory

Trajectory x(t) is bounded into a deﬁned area (retina). This is modeled by a harmonic
oscillator at the borders of the image which constraints the motion within the retina1:

(cid:0) (li − xi)2 · [xi > li] + (xi)2 · [xi < 0](cid:1)

V (x) = k

(cid:88)

1Here  we use Iverson’s notation  according to which if p is a proposition then [p] = 1 if p=true and

[p] = 0 otherwise

i=1 2

2

where k is the elastic constant  li is the i-th dimension of the rectangle which represents the
retina2.

ii) Curiosity driven principle

Visual attention is attracted by regions with many details  that is where the magnitude of
the gradient of the brightness is high. In addition to this local ﬁeld  the role of peripheral
information is included by processing a blurred version p(t  x) of the brightness b(t  x). The
modulation of these two terms is given by

C(t  x) = b2

x cos2(ωt) + p2

x sin2(ωt) 

(4)

where bx and px denote the gradient w.r.t. x. Notice that the alternation of the local and
peripheral ﬁelds has a fundamental role in avoiding trapping into regions with too many
details.

iii) brightness invariance

Trajectories that exhibit brightness invariance are motivated by the need to perform ﬁxation.
Formally  we impose the constraint ˙b = bt + bx ˙x = 0. This is in fact the classic constraint
that is widely used in computer vision for the estimation of the optical ﬂow [20]. Its
soft-satisfaction can be expressed by the associated term

B(t  x  ˙x) =(cid:0)bt + bx ˙x(cid:1)2

.

(5)

Notice that  in the case of static images  bt = 0  and the term is fully satisﬁed for trajectory
x(t) whose velocity ˙x is perpendicular to the gradient  i.e.when the focus is on the borders
of the objects. This kind of behavior favors coherent ﬁxation of objects. Interestingly  in
case of static images  the model can conveniently be simpliﬁed by using the upper bound of
the brightness as follows:

B(t  x  ˙x) = ˙b2(t  x) = (∂bt + bx ˙x)2 ≤
x ˙x2 := ¯B(t  x  ˙x)

≤ 2b2

t + 2b2

(6)

This inequality comes from the parallelogram law of Hilbert spaces. As it will be seen the
rest of the paper  this approximation signiﬁcantly simpliﬁes the motion equations.

2.2 Least Action Principle

Visual attention scanpaths are modeled as the motion of a particle of mass m within a potential ﬁeld.
This makes it possible to construct the generalized action

(cid:90) T

S =

L(t  x  ˙x) dt

where L = K − U  where K is the kinetic energy

0

K( ˙x) =

1
2

m ˙x2

and U is a generalized potential energy deﬁned as

U (t  x  ˙x) = V (x) − ηC(t  x) + λB(t  x  ˙x).

(7)

(8)

(9)

Here  we assume that η  λ > 0. Notice  in passing that while V and B get the usual sign of potentials 
C comes with the ﬂipped sign. This is due to the fact that  whenever it is large  it generates an
attractive ﬁeld. In addition  we notice that the brightness invariance term is not a truly potential 
since it depends on both the position and the velocity. However  its generalized interpretation as a
“potential” comes from considering that it generates a force ﬁeld. In order to discover the trajectory
we look for a stationary point of the action in Eq. (7)  which corresponds to the Euler-Lagrange
equations

d
dt

∂L
∂ ˙xi

=

∂L
∂xi

 

(10)

2A straightforward extension can be given for circular retina.

3

where i = 1  2 for the two motion coordinates. The right-hand term in (10) can be written as

Likewise we have

= ηCx − Vx − λBx.

∂L
∂x

d
dt

∂L
∂ ˙x

= m¨x − λ

d
dt

B ˙x

so as the general motion equation turns out to be

m¨x − λ

d
dt

B ˙x + Vx − ηCx + λBx = 0.

(11)

(12)

(13)

These are the general equations of visual attention. In the Appendix we give the technical details of
the derivations. Throughout the paper  the proposed model is referred to as the EYe MOvement Laws
(EYMOL).

2.3 Parameters estimation with simulated annealing

Different choices of parameters lead to different behaviors of the system. In particular  weights
can emphasize the contribution of curiosity or brightness invariance terms. To better control the
system we use two different parameters for the curiosity term  namely ηb and ηp  to weight b and p
contributions respectively. The best values for the three parameters (ηb  ηp  λ) are estimated using
the algorithm of simulated annealing (SA). This method allows to perform iterative improvements 
starting from a known state i. At each step  the SA considers some neighbouring state j of the current
state  and probabilistically moves to the new state j or stays on the current state i. For our speciﬁc
problem  we limit our search to a parallelepiped-domain D of possible values  due to theoretical
bounds and numerical3 issues. Distance between states i and j is proportional with a temperature T  
which is initialized to 1 and decreases over time as Tk = α ∗ Tk−1  where k identiﬁes the iteration
step  and 0 << α < 1. The iteration step is repeated until the system reaches a state that is good
enough for the application  which in our case is to maximize the NSS similarity between human
saliency maps and simulated saliency maps.
Only a batch of a 100 images from CAT2000-TRAIN is used to perform the SA algorithm4. This
batch is created by randomly selecting 5 images from each of the 20 categories of the dataset. To
start the SA  parameters are initialized in the middle point of the 3-dimensional parameters domain
D. The process is repeated 5 times  on different sub-samples  to select 5 parameters conﬁgurations.
Finally  those conﬁgurations together with the average conﬁguration are tested on the whole dataset 
to select the best one.

Select an initial state i ∈ D
T ← 1
do

Algorithm 1 In the psedo-code  P() is the acceptance probability and score() is computed as the average of
NSS scores on the sample batch of 100 images.
1: procedure SIMULATEDANNEALING
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end procedure

Generate random state j  neighbor of i
if P(score(i)  score(j)) ≥ Random(0  1) then

i ← j
end if
T ← α ∗ T
while T ≥ 0.01

3Too high values for ηb or ηp produce numerically unstable and unrealistic trajectories for the focus of

attention.

4Each step of the SA algorithm needs evaluation over all the selected images. Considering the whole dataset

would be very expensive in terms of time.

4

Model version

MIT1003

NSS

AUC

CAT2000-TRAIN

AUC

NSS

V1 (approx. br. inv.)
V2 (exact br. inv.)

0.7996 (0.0002)
0.7990 (0.0003)

1.2784 (0.0003)
1.2865 (0.0039)

0.8393 (0.0001)
0.8376 (0.0013)

1.8208 (0.0015)
1.8103 (0.0137)

Table 1: Results on MIT1003 [1] and CAT2000-TRAIN [11] of the two different version of EYMOL. Between
brackets is indicated the standard error.

3 Experiments

To quantitative evaluate how well our model predicts human ﬁxations  we deﬁned an experimen-
tal setup for salient detection both in images and in video. We used images from MIT1003 [1] 
MIT300 [12] and CAT2000 [11]  and video from SFU [27] eye-tracking database. Many of the design
choices were common to both experiments; when they differ  it is explicitly speciﬁed.

3.1

Input pre-processing

All input images are converted to gray-scale. Peripheral input p is implemented as a blurred versions
of the brightness b. This blurred version is obtained by convolving the original gray-scale image
with a Gaussian kernel. For the images only  an algorithm identiﬁes the rectangular zone of the
input image in which the totality of information is contained in order to compute li in (14). Finally
both b and p are multiplied by a Gaussian blob centered in the middle of the frame in order to make
brightness gradients smaller as we move toward periphery and produce a center bias.

3.2 Saliency maps computation

Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18]  the
saliency map is not itself the central component of our model but it can be naturally calculated from
the visual attention laws in (13). The output of the model is a trajectory determined by a system of
two second ordered differential equations  provided with a set of initial conditions. Since numerical
integration of (13) does not raise big numerical difﬁculties  we used standard functions of the python
scientiﬁc library SciPy [21].
Saliency map is then calculated by summing up the most visited locations during a sufﬁciently large
number of virtual observations. For images  we collected data by running the model 199 times  each
run was randomly initialized almost at the center of the image and with a small random velocity 
and integrated for a running time corresponding to 1 second of visual exploration. For videos  we
collected data by running the model 100 times  each run was initialized almost at the center of the
ﬁrst frame of the clip and with a small random velocity.
Model that have some blur and center bias on the saliency map can improve their score with respect
to some metrics. A grid search over blur radius and center parameter σ have been used  in order to
maximize AUC-Judd and NSS score on the training data of CAT2000 in the case of images  and on
SFU in case of videos.

3.3 Saliency detection on images

Two versions of the the model have been evaluated. The ﬁrst version V1 implementing brightness
invariance in the approximated form (6)  the second version V2 implementing the brightness invari-
ance in its exact form  as described in the Appendix. Model V1 and V2 have been compared on the
MIT1003 and CAT2000-TRAIN datasets  since they provide public data about ﬁxations. Parameters
estimation have been conducted independently for the two models and the best conﬁguration for each
one is used in this comparison. Results are statistically equivalent (see Table2) and this proves that 
in the case of static images  the approximation is very good and does not cause loss in the score.
For further experiments we decided to use the approximated form V1 due to its simpler form of the
equation that also reduces time of computation.
Model V1 has been evaluated in two different dataset of eye-tracking data: MIT300 and CAT2000-
TEST. In this case  scores were ofﬁcially provided by MIT Saliency Benchmark Team [15]. De-
scription of the metrics used is provided in [13]. Table 2 and Table 3 shows the scores of our

5

Itti-Koch [10]  implem. by [19]
AIM [16]
Judd Model [1]
AWS [24]
eDN [18]
EYMOL

MIT300

AUC SIM EMD CC NSS KL
1.03
0.75
1.18
0.77
1.12
0.81
0.74
1.07
0.82
1.14
0.77
1.53

4.26
4.73
4.45
4.62
4.56
3.64

0.97
0.79
1.18
1.01
1.14
1.06

0.37
0.31
0.47
0.37
0.45
0.43

0.44
0.40
0.42
0.43
0.44
0.46

Table 2: Results on MIT300 [12] provided by MIT Saliency Benchmark Team [15]. The models are sorted
chronologically. In bold  the best results for each metric and benchmarks.

Itti-Koch [10]  implem. by [19]
AIM [16]
Judd Model [1]
AWS [24]
eDN [18]
EYMOL

CAT2000-TEST

AUC SIM EMD CC NSS KL
0.92
0.77
1.13
0.76
0.94
0.84
0.94
0.76
0.85
0.97
1.67
0.83

0.48
0.44
0.46
0.49
0.52
0.61

1.06
0.89
1.30
1.09
1.30
1.78

0.42
0.36
0.54
0.42
0.54
0.72

3.44
3.69
3.60
3.36
2.64
1.91

Table 3: Results on CAT2000 [11] provided by MIT Saliency Benchmark Team [15]. The models are sorted
chronologically. In bold  the best results for each metric and benchmarks.

model compared with ﬁve other popular method [10; 16; 1; 24; 18]  which have been selected to be
representative of different approaches. Despite its simplicity  our model reaches best score in half of
the cases and for different metrics.

3.4 Saliency detection on dynamic scenes

We evaluated our model in a task of saliency detection with the dataset SFU [27]. The dataset contains
12 clips and ﬁxations of 15 observers  each of them have watched twice every video. Table 4 provides
a comparison with other four model. Also in this case  despite of its simplicity and even if it was not
designed for the speciﬁc task  our model competes well with state-of-the-art models. Our model can
be easily run in real-time to produce an attentive scanpath. In some favorable case  it shows evidences
of tracking moving objects on the scene.

EYMOL Itti-Koch [10]

SFU Eye-Tracking Database

Surprise [17]

Judd Model [1] HEVC [28]

Mean AUC
Mean NSS

0.817
1.015

0.70
0.28

0.66
0.48

0.77
1.06

0.83
1.41

Table 4: Results on the video dataset SFU [27]. Scores are calculated as the mean of AUC and NSS metrics of
all frames of each clip  and then averaged for the 12 clips.

4 Conclusions

In this paper we investigated how human attention mechanisms emerge in the early stage of vision 
which we assume completely data-driven. The proposed model consists of differential equations 
which provide a real-time model of scanpath. These equations are derived in a generalized framework
of least action  which nicely resembles related derivations of laws in physics. A remarkable novelty
concerns the uniﬁed interpretation of curiosity-driven movements and the brightness invariance term
for ﬁxation and tracking  that are regarded as mechanisms that jointly contribute to optimize the
acquisition of visual information. Experimental results on both image and video datasets of saliency
are very promising  especially if we consider that the proposed theory offers a truly model of eye
movements  whereas the computation of the saliency maps only arises as a byproduct.

6

In future work  we intend to investigate behavioural data  not only in terms of saliency maps  but also
by comparing actual generated scanpaths with human data in order to discover temporal correlations.
We aim at providing the integration of the presented model with a theory of feature extraction that is
still expressed in terms of variational-based laws of learning [29].

Appendix: Euler-Lagrange equations

In this section we explicitly compute the differential laws of visual attention that describe the visual
attention scanpath  as the Euler-Lagrange equations of the action functional (7).
First  we compute the partial derivatives of the different contributions w.r.t. x  in order to compute
the exact contributions of (11). For the retina boundaries 

(cid:88)

(cid:0) − 2 (li − xi) · [xi > li] + 2xi · [xi < 0](cid:1)

Vx = k

The curiosity term (4)

i=1 2

Cx =2cos2(ωt)bx · bxx + 2sin2(ωt)px · pxx

For the term of brightness invariance 

Bx =

∂
∂x

(bt + bx ˙x)2

= 2 (bt + bx ˙x) (btx + bxx ˙x)

Since we assume b ∈ C2(t  x)  by the Schwarz’s theorem5  we have that btx = bxt  so that

(18)
(19)
We proceed by computing the contribution in (12). Derivative w.r.t. ˙x of the brightness invariance
term is

Bx = 2 (bt + bx ˙x) (bxt + bxx ˙x)

= 2(˙b)(˙bx)

(14)

(15)

(16)

(17)

(20)

(21)
(22)

B ˙x =

∂
∂ ˙x

(bt + bx ˙x)2

= 2 (bt + bx ˙x) bx
= 2(˙b)(bx)

(cid:16)¨bbx + ˙b˙bx

(cid:17)

d
dt

So that  total derivative w.r.t. t can be write as

(23)
We observe that ¨b ≡ ¨b(t  x  ˙x  ¨x) is the only term which depends on second derivatives of x. Since
we are interested in expressing EL in an explicit form for the variable ¨x  we explore more closely its
contribution

B ˙x =2

¨b(t  x  ˙x  ¨x) =

˙b

d
dt
d
=
dt
=˙bt + ˙bx · ˙x + bx · ¨x

(bt + bx ˙x)

(cid:16)
(cid:16)

d
dt

B ˙x =2

(˙bt + ˙bx · ˙x + bx · ¨x)bx + ˙b˙bx
(˙bt + ˙bx · ˙x)bx + ˙b˙bx

+ 2(bx · ¨x)bx

(cid:17)

(24)

(25)

(26)
(27)

(28)

(cid:17)

(29)
5Schwarz’s theorem states that  if f : Rn → R has continuous second partial derivatives at any given point

=2

in Rn  then ∀i  j ∈ {1  ...  n} it holds fxixj = fxj xi

Substituting it in (23) we have

7

So that  from (12) we get

(cid:16)

= m¨x − 2λ

∂L
∂ ˙x

d
dt

(cid:16)

(˙bt + ˙bx · ˙x)bx + ˙b˙bx + (bx · ¨x)bx

(cid:17)

(30)

Euler-Lagrange equations. Combining (11) and (30)  we get Euler-Lagrange equation of attention

m¨x − 2λ

(˙bt + ˙bx · ˙x)(bx) + (˙b)(˙bx) + (bx · ¨x)bx

= ηCx − Vx − λBx

(31)

In order to obtain explicit form for the variable ¨x  we re-write the equation as to move to the left all
contributes which do not depend on that variable.

m¨x − 2λ(bx · ¨x)bx =ηCx − Vx − λBx + 2λ((˙bt + ˙bx · ˙x)(bx) + (˙b)(˙bx))

(cid:17)

In matrix form  the equation is(cid:18)m¨x1

which gives us the system of two differential equations

(cid:124)

A=(A1 A2)

(cid:123)(cid:122)

= ηCx − Vx + 2λ(˙bt + ˙bx · ˙x)(bx)

(cid:18)2λ(bx1 ¨x1 + bx2 ¨x2)bx1

(cid:125)
(cid:19)
(cid:19)
(cid:18)A1
(cid:26)m¨x1 − 2λ(bx1 ¨x1 + bx2 ¨x2)bx1 = A1

2λ(bx1 ¨x1 + bx2 ¨x2)bx2

m¨x2 − 2λ(bx1 ¨x1 + bx2 ¨x2)bx2 = A2

−

A2

=

m¨x2

(cid:19)

(32)
(33)

(34)

(35)

(36)

(37)

(38)

(39)

)¨x1 − 2λ(bx1bx2)¨x2

−2λ(bx1bx2 )¨x1 + (m − 2λb2

= A1
)¨x2 = A2

x2

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(m − 2λb2

) A1
−2λ(bx1bx2 ) A2

x1

(cid:12)(cid:12)(cid:12)(cid:12)

) −2λ(bx1 bx2)
(m − 2λb2
)

x2

(cid:12)(cid:12)(cid:12)(cid:12)   D2 =

x1

We deﬁne

Grouping by same variable (cid:26)(m − 2λb2
(cid:12)(cid:12)(cid:12)(cid:12)(m − 2λb2
(cid:12)(cid:12)(cid:12)(cid:12)A1 −2λ(bx1bx2 )


−2λ(bx1bx2 )

(m − 2λb2

D1 =

D =

A2

x1

x2

)

¨x1 =

¨x2 =

D1
D

D2
D

By the Cramer’s method we get differential equation of visual attention for the two spatial component 
i.e.

Notice that  this raise to a further condition over the parameter λ. In particular  in the case values of
b(t  x) are normalized in the range [0  1]  it imposes to chose
m
4

D (cid:54)= 0 =⇒ λ <

(40)

In fact 

(cid:16)

D = (m − 2λb2

)(m − 2λb2

x1

m − 2λ(b2

= m

+ b2
x1

)

x1

(cid:17)

x2

For values of bx = 0  we have that

so that ∀t  we must impose

D = m2 > 0

D > 0.

8

) − 4λ2(bx1bx2)2

(41)

(42)

(43)

(44)

If λ > 0  then

m − 2λ(b2

x1

+ b2
x1

) > 0

λ <

2(b2
x1

m
+ b2
x1

)

The quantity on the right reaches its minimum at m
4

  so that the condition

guarantees the well-posedness of the problem.

0 < λ <

m
4

(45)

(46)

(47)

References
[1] Judd  T.  Ehinger  K.  Durand  F.  Torralba  A.: Learning to Predict Where Humans Look. IEEE

International Conference on Computer Vision (2009)

[2] Itti  L.  Koch  C.: Computational modelling of visual attention. Nature Reviews Neuroscience 

vol 3  n 3  pp 194–203. (2001)

[3] Connor  C.E.  Egeth  H.E.  Yantis  S.: Visual Attention: Bottom-Up Versus Top-Down. Current

Biology  vol 14  n 19  pp R850–R852. (2004)

[4] McMains  S.  Kastner  S.: Interactions of Top-Down and Bottom-Up Mechanisms in Human

Visual Cortex. Society for Neuroscience  vol 31  n 2  pp 587–597. (2011)

[5] Hainline  L.  Turkel  J.  Abramov  I.  Lemerise  E.  Harris  C.M.: Characteristics of saccades in

human infants. Vision Research  vol 24  n 12  pp 1771–1780. (1984)

[6] Le Meur  O.  Liu  Z.: Saccadic model of eye movements for free-viewing condition. Vision

Research  vol 116  pp 152–164. (2015)

[7] Gelfand  I.M.  Fomin  S.V.: Calculus of Variation. Englewood : Prentice Hall (1993)

[8] Borji  A.  Itti  L.: State-of-the-Art in Visual Attention Modeling. IEEE Transactions on Pattern

Analysis and Machine Intelligence  vol 35  n 1. (2013)

[9] Koch  C.  Ullman  S.: Shifts in selective visual attention: towards the underlying neural circuitry.

Springer Human Neurobiology  vol 4  n 4  pp 219-227. (1985)

[10] Itti  L.  Koch  C.: A Model of Saliency-Based Visual Attention for Rapid Scene Analysis. IEEE

Transactions on Pattern Analysis and Machine Intelligence  vol 20  n 11. (1998)

[11] Borji  A.  Itti  L.: CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research.

arXiv:1505.03581. (2015)

[12] Judd  T.  Durand  F.  Torralba  A.:: A Benchmark of Computational Models of Saliency to

Predict Human Fixations. MIT Technical Report. (2012)

[13] Bylinskii  Z.  Judd  T.  Oliva  A.  Torralba  A.: What do different evaluation metrics tell us about

saliency models? arXiv:1604.03605. (2016)

[14] Treisman  A.M.  Gelade  G.: A Feature Integration Theory of Attention. Cognitive Psychology 

vol 12  pp 97-136. (1980)

[15] Bylinskii  Z.  Judd  T.  Borji  A.  Itti  L.  Durand  F.  Torralba  A.: MIT Saliency Benchmark.

http://saliency.mit.edu/

[16] Bruce  N.  Tsotsos  J.: Attention based on information maximization. J. Vis.  vol 7  n 9. (2007)

[17] Itti  L.  Baldi  P.: Bayesian Surprise Attracts Human Attention. Vision Research  vol 49  n 10 

pp 1295–1306. (2009)

9

[18] Vig  E.  Dorr  M.  Cox  D.: Large-Scale Optimization of Hierarchical Features for Saliency
Prediction in Natural Images. IEEE Conference on Computer Vision and Pattern Recognition.
(2014)

[19] Harel  J.: A Saliency Implementation in MATLAB . http://www.klab.caltech.edu/ harel/share/g-

bvs.php

[20] Horn  B.K.P.  Schunck  B.G.: Determining optical ﬂow. Artiﬁcial Intelligence  vol 17  n 1  pp

185-203. (1981)

[21] Jones  E.  Travis  O.  Peterson  P.: SciPy: Open source scientiﬁc tools for Python.

http://www.scipy.org/. (2001)

[22] Zhang  J.  Sclaroff  S.: Saliency detection: a Boolean map approach . Proc. of the IEEE

International Conference on Computer Vision. (2013)

[23] Cornia  M.  Baraldi  L.  Serra  G.  Cucchiara  R.: Predicting Human Eye Fixations via an

LSTM-based Saliency Attentive Model. http://arxiv.org/abs/1611.09571. (2016)

[24] Garcia-Diaz  A.  Leborán  V.  Fdez-Vida  X.R.  Pardo  X.M.: On the relationship between
optical variability  visual saliency  and eye ﬁxations: Journal of Vision  vol 12  n 6  pp 17. (2012)

[25] Tatler  B.W.  Baddeley  R.J.  Gilchrist  I.D.: Visual correlates of ﬁxation selection: Effects of

scale and time. Vision Research  vol 45  n 5  pp 643-659. (2005)

[26] Kruthiventi  S.S.S.  Ayush  K.  Venkatesh  R.:DeepFix: arXiv:1510.02927. (2015)

[27] Hadizadeh  H.  Enriquez  M.J.  Bajic  I.V.: Eye-Tracking Database for a Set of Standard Video

Sequences. IEEE Transactions on Image Processing. (2012)

[28] Xu  M.  Jiang  L.  Ye  Z.  Wang  Z.: Learning to Detect Video Saliency With HEVC Features.

IEEE Transactions on Image Processing. (2017)

[29] Maggini  M.  Rossi  A.: On-line Learning on Temporal Manifolds. AI*IA 2016 Advances in

Artiﬁcial Intelligence Springer International Publishing  pp 321–333. (2016)

10

,Dario Zanca
Marco Gori