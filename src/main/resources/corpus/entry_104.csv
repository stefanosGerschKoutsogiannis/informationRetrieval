2019,A Latent Variational Framework for Stochastic Optimization,This paper provides a unifying theoretical framework for stochastic optimization algorithms by means of a latent stochastic variational problem. Using techniques from stochastic control  the solution to the variational problem is shown to be equivalent to that of a Forward Backward Stochastic Differential Equation (FBSDE). By solving these equations  we recover a variety of existing adaptive stochastic gradient descent methods. This framework establishes a direct connection between stochastic optimization algorithms and a secondary latent inference problem on gradients  where a prior measure on gradient observations determines the resulting algorithm.,A Latent Variational Framework for Stochastic

Optimization

Philippe Casgrain

Department of Statistical Sciences

University of Toronto
Toronto  ON  Canada

p.casgrain@mail.utoronto.ca

Abstract

This paper provides a unifying theoretical framework for stochastic optimization
algorithms by means of a latent stochastic variational problem. Using techniques
from stochastic control  the solution to the variational problem is shown to be equiv-
alent to that of a Forward Backward Stochastic Differential Equation (FBSDE).
By solving these equations  we recover a variety of existing adaptive stochastic
gradient descent methods. This framework establishes a direct connection between
stochastic optimization algorithms and a secondary latent inference problem on
gradients  where a prior measure on gradient observations determines the resulting
algorithm.

1

Introduction

Stochastic optimization algorithms are tools which are crucial to solving optimization problems
arising in machine learning. The initial motivation for these algorithms comes from the fact that
computing the gradients of a target loss function becomes increasingly difﬁcult as the scale and
dimension of an optimization problem grows larger. In these large-scale optimization problems 
deterministic gradient-based optimization algorithms perform poorly due to the computational load
of repeatedly computing gradients. Stochastic optimization algorithms remedy this issue by replacing
exact gradients of the target loss with a computationally cheap gradient estimator  trading off noise in
gradient estimates for computational efﬁciency at each step.
To illustrate this idea  consider the problem of minimizing a generic risk function f : Rd ! R  taking
the form
(1)

`(x;z)  

f (x) =

1
|N|

Â
z2N

where ` : Rd ⇥ Z ! R  and where we deﬁne the set N := {zi 2 Z   i = 1  . . .  N} to be a set of
training points. In this deﬁnition  we interpret `(x;z) as the model loss at a single training point z 2 N
for the parameters x 2 Rd.
When N and d are typically large  computing the gradients of f can be time-consuming. Knowing
this  let us consider the path of an optimization algorithm as given by {xt}t2N. Rather than computing
— f (xt) directly at each point of the optimization process  we may instead collect noisy samples of
gradients as

gt =

1
|Nm
t |

Â
z2Nm

t

—x`(xt;z)  

(2)

where for each t  Nm
t ✓ N is an independent sample of size m from the set of training points. We
assume that m ⌧ N is chosen small enough so that gt can be computed at a signiﬁcantly lower cost

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

than — f (xt). Using the collection of noisy gradients {gt}t2N  stochastic optimization algorithms
construct an estimator c— f (xt) of the gradient — f (xt) in order to determine the next step xt+1 of the
optimizer.
This paper presents a theoretical framework which provides new perspectives on stochastic optimiza-
tion algorithms  and explores the implicit model assumptions that are made by existing ones. We
achieve this by extending the approach taken by Wibisono et al. (2016) to stochastic algorithms. The
key step in our approach is to interpret the task of optimization with a stochastic algorithm as a latent
variational problem. As a result  we can recover algorithms from this framework which have built-in
online learning properties. In particular  these algorithms use an online Bayesian ﬁlter on the stream
of noisy gradient samples  gt  to compute estimates of — f (xt). Under various model assumptions on
— f and g  we recover a number of common stochastic optimization algorithms.

1.1 Related Work

There is a rich literature on stochastic optimization algorithms as a consequence of their effectiveness
in machine learning applications. Each algorithm introduces its own variation on the gradient

estimator c— f (xt) as well as other features which can improve the speed of convergence to an

optimum. Amongst the simplest of these is stochastic gradient descent and its variants Robbins
and Monro (1951)  which use an estimator based on single gradient samples. Others  such as Lucas
et al. (2018); Nesterov  use momentum and acceleration as features to enhance convergence  and can
be interpreted as using exponentially weighted moving averages as gradient estimators. Adaptive
gradient descent methods such as AdaGrad from Duchi et al. (2011) and Adam from Kingma and Ba
(2014) use similar moving average estimators  as well as dynamically updated normalization factors.
For a survey paper which covers many modern stochastic optimization methods  see Ruder (2016).
There exist a number of theoretical interpretations of various aspects of stochastic optimization. Cesa-
Bianchi et al. (2004) have shown a parallel between stochastic optimization and online learning. Some
previous related works  such as Gupta et al. (2017) provide a general model for adaptive methods 
generalizing the subgradient projection approach of Duchi et al. (2011). Aitchison (2018) use a
Bayesian model to explain the various features of gradient estimators used in stochastic optimization
algorithms . This paper differs from these works by naturally generating stochastic algorithms from a
variational principle  rather than attempting to explain their individual features. This work is most
similar to that of Wibisono et al. (2016) who provide a variational model for continuous deterministic
optimization algorithms.
There is a large body of research on continuous-time approximations to deterministic optimization
algorithms via dynamical systems (ODEs) (da Silva and Gazeau (2018); Krichene et al. (2015); Su
et al. (2014); Wilson et al. (2016))  as well as approximations to stochastic optimization algorithms by
stochastic differential equations (SDEs) (Krichene and Bartlett (2017); Mertikopoulos and Staudigl
(2018); Raginsky and Bouvrie (2012); Xu et al. (2018a b)). In particular  the most similar of these
works  Raginsky and Bouvrie (2012); Xu et al. (2018a b)  study continuous approximations to
stochastic mirror descent by adding exogenous Brownian noise to the continuous dynamics derived
in Wibisono et al. (2016). This work differs by deriving continuous stochastic dynamics for optimizers
from a broader theoretical framework  rather than positing the continuous dynamics as-is. Although
the equations studied in these papers may resemble some of the results derived in this one  they
differ in a number of ways. Firstly  this paper ﬁnds that the source of randomness present in the
optimizer dynamics obtained in this paper are not generated by an exogenous source of noise  but are
in fact an explicit function of the randomness generated by observed stochastic gradients during the
optimization process. Another important difference is that the optimizer dynamics presented in this
paper make no use of the gradients of the objective function  — f (which is inaccessible to a stochastic
optimizer)  and are only a function of the stream of stochastic gradients gt.

1.2 Contribution

To the author’s knowledge  this is the ﬁrst paper to produce a theoretical model for stochastic
optimization based on a variational interpretation. This paper extends the continuous variational
framework Wibisono et al. (2016) to model stochastic optimization. From this model  we derive
optimality conditions in the form of a system of forward-backward stochastic differential equations
(FBSDEs)  and provide bounds on the expected rate of convergence of the resulting optimization

2

algorithm to the optimum. By discretizing solutions of the continuous system of equations  we
can recover a number of well-known stochastic optimization algorithms  demonstrating that these
algorithms can be obtained as solutions of the variational model under various assumptions on the
loss function  f (x)  that is being minimized.

1.3 Paper Structure

In Section 2 we deﬁne a continuous-time surrogate model of stochastic optimization. Section 3
uses this model to motivate a stochastic variational problem over optimizers  in which we search
for stochastic optimization algorithms which achieve optimal average performance over a collection
of minimization problems. In Section 4 we show that the necessary and sufﬁcient conditions for
optimality of the variational problem can be expressed as a system of Forward-Backward Stochastic
Differential Equations. Theorem 4.2 provides rates of convergence for the optimal algorithm to the
optimum of the minimization problem. Lastly  Section 5 recovers SGD  mirror descent  momentum 
and other optimization algorithms as discretizations of the continuous optimality equations derived in
Section 4 under various model assumptions. The proofs of the mathematical results of this paper are
found within the appendices.

2 A Statistical Model for Stochastic Optimization

t )t0 as a controlled process satisfying Xn

Over the course of the section  we present a variational model for stochastic optimization. The
ultimate objective will be to construct a framework for measuring the average performance of an
algorithm over a random collection of optimization problems. We deﬁne random variables in an
ambient probability space (W P  G = {Gt}t2[0 T ])  where Gt is a ﬁltration which we will deﬁne at
a later point in this section. We assume that loss functions are drawn from a random variable
f : W ! C1(Rd). Each draw from the random variable satisﬁes f (x) 2 R for ﬁxed x 2 Rd  and f is
assumed to be an almost-surely continuously differentiable in x. In addition  we make the technical
assumption that Ek— f (x)k2 < • for all x 2 Rd.
We deﬁne an optimizer X = (Xn
t 2 Rd for all t  0  with
initial condition X0 2 Rd. The paths of X are assumed to be continuously differentiable in time so that
the dynamics of the optimizer may be written as dXn
t = nt dt  where nt 2 Rd represents the control 
where we use the superscript to express the explicit dependence of Xn on the control n. We may
t = X0 +R t
also write the optimizer in its integral form as Xn
0 nu du  demonstrating that the optimizer
is entirely characterized by a pair (n X0) consisting of a control process n and an initial condition
X0. Using an explicit Euler discretization with step size e > 0  the optimizer can be approximately
represented through the update rule Xn
t + e nt. This leads to the interpretation of nt as the
(inﬁnitesimal) step the algorithm takes at each point t during the optimization process.
In order to capture the essence of stochastic optimization  we construct our model so that optimizers
have restricted access to the gradients of the loss function f . Rather than being able to directly observe
— f over the path of Xn
t   we assume that the algorithm may only use a noisy source of gradient samples 
modeled by a càdlàg semi-martingale1 g = (gt)t0. As a simple motivating example  we can consider
the model gt = — f (Xn
t ) + xt  where xt is a white noise process. This particular model for the noisy
gradient process can be interpreted as consisting of observing — f (Xn
t ) plus an independent source of
noise. This concrete example will be useful to keep in mind to make sense of the results which we
present over the course of the paper.
To make the concept of information restriction mathematically rigorous  we restrict ourselves only to
optimizers Xn which are measurable with respect to the information generated by the noisy gradient

t+e ⇡ Xn

process g. To do this  we ﬁrst deﬁne the global ﬁltration G   as Gt = s(gu)u2[0 t]  f as the sigma

algebra generated by the paths of g as well as the realizations of the loss surface f . The ﬁltration Gt
is deﬁned so that it contains the complete set of information generating the optimization problem
until time t.

1A càdlàg (continue à droite  limite à gauche) process is a continuous time process that is almost-surely
right-continuous with ﬁnite left limit at each point t. A semi-martingale is the sum of a process of ﬁnite variation
and a local martingale. For more information on continuous time stochastic processes and these deﬁnitions  see
the canonical text Jacod and Shiryaev (2013).

3

Next  we deﬁne the coarser ﬁltration Ft = s (gu)u2[0 t] ⇢ Gt generated strictly by the paths of the noisy
gradient process. This ﬁltration represents the total set of information available to the optimizer up
until time t. This allows us to formally restrict the ﬂow of information to the algorithm by restricting
ourselves to optimizers which are adapted to Ft. More precisely  we say that the optimizer’s control
n is admissible if

n 2 A :=⇢w = (wt)t0 : w is F -adapted   EZ T

0 kwtk2+k— f (Xw

t )k2 dt < • .

The set of optimizers generated by A can be interpreted as the set of optimizers which may only use
the source of noisy gradients  which have bounded expected travel distance and have square-integrable
gradients over their path.

(3)

3 The Optimizer’s Variational Problem

Having deﬁned the set of admissible optimization algorithms  we set out to select those which are
optimal in an appropriate sense. We proceed similarly to Wibisono et al. (2016)  by proposing an
objective functional which measures the performance of the optimizer over a ﬁnite time period.
The motivation for the optimizer’s performance metric comes from a physical interpretation of the
optimization process. We can think of our optimization process as a particle traveling through a
potential ﬁeld deﬁne by the target loss function f . As the particle travels through the potential ﬁeld  it
may either gain or lose momentum depending on its location and velocity  which will in turn affect
the particle’s trajectory. Naturally  we may seek to ﬁnd the path of a particle which reaches the
optimum of the loss function while minimizing the total amount of kinetic and potential energy that
is spent. We therefore turn to the Lagrangian interpretation of classical mechanics  which provides a
framework for obtaining solutions to this problem. Over the remainder of this section  we lay out the
Lagrangian formalism for the optimization problem we deﬁned in Section 2.
To deﬁne a notion of energy in the optimization process  we provide a measure of distance in the
parameter space. We use the Bregman Divergence as the measure of distance within our parameter
space  which can embed additional information about the geometry of the optimization problem. The
Bregman divergence  Dh  is deﬁned as

Dh(y x) = h(y) h(x)h—h(x) y xi

(4)
where h : Rd ! R is a strictly convex function satisfying h 2 C2. We assume here that the gradients of
h are L-Lipschitz smooth for a ﬁxed constant L > 0. The choice of h determines the way we measure
distance  and is typically chosen so that it mimics features of the loss function f . In particular  this
quantity plays a central role in mirror descent and non-linear sub-gradient algorithms. For more
information on this connection and on Bregman Divergence  see Nemirovsky and Yudin (1983)
and Beck and Teboulle (2003).
We deﬁne the total energy in our problem as the kinetic energy  accumulated through the movement
of the optimizer  and the potential energy generated by the loss function f . Under the assumption that
f almost surely admits a global minimum x? = argminx2Rd f (x)  we may represent the total energy
via the Bregman Lagrangian as

L (t X n) = egt (eat DhX + eat n X
}

Kinetic Energy

{z

|

ebt ( f (X) f (x?))
}
|

Potential Energy

{z

)  

(5)

for ﬁxed inputs (t X n)  and where we assume that g a b : R+ ! R are deterministic  and satisfy
g a b 2 C1. The functions g a b can be interpreted as hyperparameters which tune the energy
present at any state of the optimization process. An important property to note is that the Lagrangian
is itself a random variable due to the randomness introduced by the latent loss function f .
The objective is then to ﬁnd an optimizer within the admissible set A which can get close to the
minimum x? = minx2Rd f (x)  while simultaneously minimizing the energy cost over a ﬁnite time
period [0 T ]. The approach taken in classical mechanics and in Wibisono et al. (2016) ﬁxes the
endpoint of the optimizer at x?. Since we assume that the function f is not directly visible to our
optimizer  it is not possible to add a constraint of this type that will hold almost surely. Instead  we
introduce a soft constraint which penalizes the algorithm’s endpoint in proportion to its distance to

4

the global minimum  f (XT ) f (x?). As such  we deﬁne the expected action functional J : A ! R
as
(6)

L (t Xn

t  nt)dt

J (n) = EhZ T
|

0

Total Path Energy

{z

Soft End Point Constraint i  
+edT f (Xn
T ) f (x?)
|
}
{z

}

where dT 2 C1 is assumed to be an additional model hyperparameter  which controls the strength of
the soft constraint.
With this deﬁnition in place  the objective will be to select amongst admissible optimizers for those
which minimize the expected action. Hence  we seek optimizers which solve the stochastic variational
problem

n⇤ = arg min
n2A

J (n) .

(7)

Remark 1. Note that the variational problem (7) is identical to the one with Lagrangian

˜L (t X n) = egt (eat DhX + eat n X ebt f (X))

(8)
T )  since they differ by constants independent of n. Because of this  the

and terminal penalty edT f (Xn
results presented in Section 4 also hold the case where x? and f (x?) do not exist or are inﬁnite.

4 Critical Points of the Expected Action Functional

In order to solve the variational problem (7)  we make use techniques from the calculus of variations
and inﬁnite dimensional convex analysis to provide optimality conditions for the variational prob-
lem (7). To address issues of information restriction  we rely on the stochastic control techniques
developed by Casgrain and Jaimungal (2018a b c).
The approach we take relies on the fact that a necessary condition for the optimality of a Gâteaux
differentiable functional J is that its Gâteaux derivative vanishes in all directions. Computing the
Gâteaux derivative of J   we ﬁnd an equivalence between the Gâteaux derivative vanishing and a
system of Forward-Backward Stochastic Differential Equations (FBSDEs)  yielding a generalization
of the Euler-Lagrange equations to the context of our optimization problem. The precise result is
stated in Theorem 4.1 below.
Theorem 4.1 (Stochastic Euler-Lagrange Equation). A control n⇤ 2 A is a critical point of J if
and only if (( ∂ L
d✓∂ L
∂n ◆t
✓∂ L
∂X ◆t
✓∂ L
∂n ◆t

t + eat n⇤t ) —h(Xn⇤
) eat —2h(Xn⇤
)⌘  

∂X ◆tFt dt + dMt 8t < T   ✓∂ L
∂n ◆T

= edT Eh— f (XT )FTi  

∂n )  M ) is a solution to the system of FBSDEs 

= egt⇣—h(Xn⇤

t + eat n⇤t ) —h(Xn⇤

= E✓∂ L

)n⇤t  ebt — f (Xn⇤

t

))

where we deﬁne the processes

= egt +at(—h(Xn⇤

and where the process M = (Mt)t2[0 T ] is an F -adapted martingale. As a consequence  if the
solution to this FBSDE is unique  then it is the unique critical point of the functional J up to null
sets.
Proof. See Appendix C

(10)

(11)

(9)

t

t

t

Theorem 4.1 presents an analogue of the Euler-Lagrange equation with free terminal boundary. Rather
than obtaining an ODE as in the classical result  we obtain an FBSDE2  with backwards process
2For a background on FBSDEs  we point readers to Carmona (2016); Ma et al. (1999); Pardoux and Tang
(1999). At a high level  the solution to an FBSDE of the form (9) consists of a pair of processes (∂ L/∂n  M ) 
which simultaneously satisfy the dynamics and the boundary condition of (9). Intuitively  the martingale part of
the solution can be interpreted as a random process which guides (∂ L/∂X)t towards the boundary condition at
time T .

5

(∂ L/∂n)t  and forward state processes E[(∂ L/∂X)t|Ft] R t
. We can also interpret the
dynamics of equation (9) as being the ﬁltered optimal dynamics of (Wibisono et al.  2016  Equation
2.3)  E[(∂ L/∂X)t|Ft]  plus the increments of data-dependent martingale Mt  with mechanics similar
to that of the ‘innovations process’ of ﬁltering theory. This martingale term should not be interpreted
as a source of noise  but as an explicit function of the data  as is evident from its explicit form

0 knuk du and Xn⇤

t

Mt = EZ T

0 ✓∂ L
∂X ◆u

du edT — f (XT )Ft .

A feature of equation (9)  is that optimality relies on the projection of (∂ L/∂X)t onto Ft. Thus 
the optimization algorithm makes use of past noisy gradient observations in order to make local
gradient predictions. Local gradient predictions are updated using a Bayesian mechanism  where
the prior model for — f is conditioned with the noisy gradient information contained in Ft. This
demonstrates that the solution depends only on the gradients of f along the path of Xt and no higher
order properties.

(12)

(13)

Et = Dh(x? Xn⇤

4.1 Expected Rates of Convergence of the Continuous Algorithm
Using the dynamics (9) we obtain a bound on the rate of convergence of the continuous optimization
algorithm that is analogous to Wibisono et al. (2016  Theorem 2.1). We introduce the Lyapunov
energy functional

t + eat nt) + ebt⇣ f (Xn⇤

t

) f (x?)⌘ [—h(Xn⇤ + eat n) Xn⇤ + eat n]t  

where we deﬁne x? to be a global minimum of f . Under additional model assumptions  and by
showing that this quantity is a super-martingale with respect to the ﬁltration F   we obtain an upper
bound for the expected rate of convergence from Xt towards the minimum.
Theorem 4.2 (Convergence Rate). Assume that the function f is almost surely convex and that
the scaling conditions ˙gt = eat and ˙bt  eat hold. Moreover  assume that in addition to h having
L-Lipschitz smooth gradients  h is also µ-strongly-convex with µ > 0. Deﬁne x? = argminx2Rd f (x)
to be a global minimum of f . If x? exists almost surely  the optimizer deﬁned by FBSDE (9) satisﬁes

E [ f (Xt) f (x?)] = O⇣ebt max1  E⇥ [egt M ]t⇤ ⌘  

(14)
where [egt M ]t represents the quadratic variation of the process egt Mt  where M is the martingale
part of the solution deﬁned in Theorem 4.1.
Proof. See Appendix D.
We may interpret the term E [ [egt M ]t] as a penalty on the rate of convergence  which scales with
the amount of noise present in our gradient observations. To see this  note that if there is no noise
in our gradient observations  we obtain that Ft = Gt  and hence Mt ⌘ 0  which recovers the exact
deterministic dynamics of Wibisono et al. (2016) and the optimal convergence rate O(ebt ). If
the noise in our gradient estimates is large  we can expect E [ [eg M ]t] to grow at quickly and to
counteract the shrinking effects of ebt . Thus  in the case of a convex objective function f   any
presence of gradient noise will proportionally hurt rate of convergence to an optimum. We also point
out  that there will be a nontrivial dependence of E [ [eg M ]t] on all model hyperparameters  the
speciﬁc deﬁnition of the random variable f   and the model for the noisy gradient stream  (gt)t0.
Remark 2. We do not assume that the conditions of Theorem 4.2 carry throughout the remainder of
the paper. In particular  Sections 5 study models which may not guarantee almost-sure convexity of
the latent loss function.

5 Recovering Discrete Optimization Algorithms

In this section  we use the optimality equations of Theorem 4.1 to produce discrete stochastic
optimization algorithms. The procedure we take is as follows. We ﬁrst deﬁne a model for the processes
(— f (Xt) gt)t2[0 T ]. Second  we solve the optimality FBSDE (9) in closed form or approximate the
solution via the ﬁrst-order singular perturbation (FOSP) technique  as described in Appendix A.
Lastly  we discretize the solutions with a simple Forward-Euler scheme in order to recover discrete
algorithms.

6

Over the course of Sections 5.1 and 5.2  we show that various simple models for (— f (Xt) gt)t2[0 T ]
and different speciﬁcations of h produce many well-known stochastic optimization algorithms. These
establish the conditions  in the context of the variational problem of Section 2  under which each of
these algorithms are optimal. As a consequence  this allows us to understand the prior assumptions
which these algorithms make on the gradients of the objective function they are trying to minimize 
and the way noise is introduced in the sampling of stochastic gradients  (gt)t0.
5.1 Stochastic Gradient Descent and Stochastic Mirror Descent
Here we propose a Gaussian model on gradients which loosely represents the behavior of mini-batch
stochastic gradient descent with a training set of size n and mini-batches of size m. By specifying
a martingale model for — f (Xt)  we recover the stochastic gradient descent and stochastic mirror
descent algorithms as solutions to the variational problem described in Section 2.
Let us assume that — f (Xt) = sW f
t )t0 is a Brownian motion. Next  assume
that the noisy gradients samples obtained from mini-batches over the course of the optimization 
t )  where r =p(nm)/m and W e is an independent
evolve according to the model gt = s (W f
copy of W f
to scale in m and n as it does with mini-batches.
Using symmetry  we obtain the trivial solution to the gradient ﬁlter  E[— f (Xt)|Ft] = (1 + r2)1gt 
implying that the best estimate of the gradient at the point Xt will be the most recent mini-batch
sample observed. re-scaled by a constant depending on n and m. Using this expression for the ﬁlter 
we obtain the following result.
Proposition 5.1. The FOSP approximation to the solution of the optimality equations (9) can be
expressed as

t . Here  we choose r so that V[gt] = (n/m)V[— f (Xt)] = O(m1)  which allows the variance

t   where s > 0 and (W f

t + rW e

0 eau+bu+gu dgu.

t ⌘ dt  

dXt = eat⇣—h⇤—h(Xt) ˜Ft(1 + r2)1gt Xn⇤

(15)
0 eau+bu+gu du) is a deterministic
0 eau+bu+gu du. When h has the form h(x) = x|Mx for a symmetric
positive-deﬁnite matrix M  the FOSP approximation is exact  and (15) is the exact solution to
the optimality FBSDE (9). The martingale portion of the solution to (9) can be expressed as

where h⇤ is the convex dual of h and where ˜Ft = egt (F0 +R t
learning rate with F0 = edT R T
Mt = M0  (1 + r2)1R t
Proof. See Appendix E.1.
To obtain a discrete optimization algorithm from the result of 5.1  we employ a forward-Euler
discretization of the ODE (15) on the ﬁnite mesh T = {t0 = 0   tk+1 = tk + eatk : k 2 N}. This
discretization results in the update rule
(16)
corresponding exactly to mirror descent (e.g. see Beck and Teboulle (2003)) using the noisy mini-
batch gradients gt and a time-varying learning rate ˜Ftk. Moreover  setting h(x) = 1
2kxk2  we recover
the update rule Xtk+1  Xtk =  ˜Ftk gtk  exactly corresponding to the mini-batch SGD with a time-
dependent learning rate.
This derivation demonstrates that the solution to the variational problem described in Section 2  under
the assumption of a Gaussian model for the evolution of gradients  recovers mirror descent and SGD.
In particular  the martingale gradient model proposed in this section can be roughly interpreted as
assuming that gradients behave as random walks over the path of the optimizer. Moreover  the optimal
gradient ﬁlter E[— f (Xt)|Ft] = (1 + r2)1gt shows that  for the algorithm to be optimal  mini-batch
gradients should be re-scaled in proportion to (1 + r2)1 = m/n.

Xtk+1 = —h⇤—h(Xtk ) ˜Ftk gtk  

5.2 Kalman Gradient Descent and Momentum Methods
Using a linear state-space model for gradients  we can recover both the Kalman Gradient Descent
algorithm of Vuckovic (2018) and momentum-based optimization methods of Polyak (1964). We
assume that each component of — f (Xt) = (—i f (Xt))d
i=1 is modeled independently as a linear dif-
fusive process. Speciﬁcally  we assume that there exist processes yi = (yi t)t0 so that for each i 
—i f (Xt) = b|yi t  where yi t 2 R ˜d is the solution to the linear SDE dyi t = Ayi tdt + LdWi t. In partic-
ular  we the notation ˆyi  j t to refer to element (i  j) of ˆy 2 Rd⇥ ˜d  and use the notation ˆy·  j t = ( ˆyi  j t)d
i=1.

7

We assume here that A L 2 R ˜d⇥ ˜d are positive deﬁnite matrices and each of the Wi = (Wi t)t0 are
independent ˜d-dimensional Brownian Motions.
Next  we assume that we may write each element of a noisy gradient process as gi t = b|yi · t + sxi t 
where s > 0 and where xi = (xi t)t0 are independent white noise processes. Noting that
E[—i f (Xt+h)|Ft] = b|eAhyi t  we ﬁnd that this model implicitly assumes that gradients are ex-
pected decrease in exponentially in magnitude as a function of time  at a rate determined by the
eigenvalues of the matrix A. The parameters s and L can be interpreted as controlling the scale of the
noise within the observation and signal processes.
Using this model  we obtain that the ﬁlter can be expressed as E[—i f (Xt)|Ft] = b| ˆyi t  where ˆyi t =
E[yi t|Ft]. The process ˆyi t is expressed as the solution to the Kalman-Bucy3 ﬁltering equations

d ˆyi t = A ˆyi t dt + s1 ¯Pt bd ˆBi t  

˙¯P = A ¯Pt  ¯P|

t A s2 ¯Ptbb| ¯P|

t + LL|  

(17)

with the initial conditions ˆyi 0 = 0 and ¯P0 = E[yi 0y|
i 0]  and where we deﬁne innovations process
d ˆBi t = s1 (gi t  b| ˆyi t) dt with the property that each ˆBi is an independent F -adapted Brownian
motion.
Inserting the linear state space model and its ﬁlter into the optimality equations (9) we obtain the
following result.
Proposition 5.2 (State-Space Model Solution to the FOSP). Assume that the gradient state-space
model described above holds. The FOSP approximation to the solution of the optimality equations (9)
can be expressed as

j=1 ˜F j t ˆy·  j t) Xn⇤

t )dt  

dXt = eat(—h⇤(—h(Xt) Â ˜d

(18)
0 eau+bu+gub|eA(tu) du) 2 R ˜d is a deterministic learning rate  where
0 eau+bu+gueAu du can be chosen to
have arbitrarily large eigenvalues by scaling dT . The martingale portion of the solution of (9) can be

where ˜Ft = egt (b|eAtF0 +R t
eA represents the matrix exponential  and where F0 = edT eAT R T
expressed as Mt = M0  s1R t

0 eau+bu+gub|eA(tu) ¯Pubd ˆBu.

Proof. See Appendix E.2

5.2.1 Kalman Gradient Descent
In order to recover Kalman Gradient Descent  we discretize the processes Xn⇤
and ˆy over the ﬁnite
t
mesh T   deﬁned in equation (18). Applying a Forward-Euler-Maruyama discretization of (18) and
the ﬁltering equations (17)  we obtain the discrete dynamics

yi tk+1 = (I  eatk A)yi tk + Leat wi k  

gi tk = b|yi tk + seat xi k  

(19)

where each of the xi k and wi k are standard Gaussian random variables of appropriate size. The
ﬁlter ˆyi k = E[ytk|{gtk0}k
k0=1] for the discrete equations can be written as the solution to the discrete
Kalman ﬁltering equations  provided in Appendix B. Discretizing the process Xn⇤ over T with the
Forward-Euler scheme  we obtain discrete dynamics for the optimizer in terms of the Kalman Filter ˆy 
as

Xtk+1 = —h⇤⇣—h(Xtk ) Â ˜d

j=1 ˜F j tk ˆy·  j k⌘  

(20)

yielding a generalized version of Kalman gradient descent of Vuckovic (2018) with ˜d states for each
2kxk2  ˜d = 1 and b = 1 recovers the original Kalman gradient
gradient element. Setting h(x) = 1
descent algorithm with a time-varying learning rate.
Just as in Section 5.1  we interpret each gtk as being a mini-batch gradient  as with equation (2). The
algorithm (20) computes a Kalman ﬁlter from these noisy mini-batch observations and uses it to
update the optimizer’s position.

3For information on continuous time ﬁltering and the Kalman-Bucy ﬁlter we refer the reader to the text of

Bensoussan (2004) or the lecture notes of Van Handel (2007).

8

5.2.2 Momentum and Generalized Momentum Methods
By considering the asymptotic behavior of the Kalman gradient descent method described in Sec-
tion 5.2.1  we recover a generalized version of momentum gradient descent methods  which includes
mirror descent behavior  as well as multiple momentum states. Let us assume that at = a0 re-
mains constant in time. Then  using the asymptotic update rule for the Kalman ﬁlter  as shown in
Proposition B.2  and equation (20)  we obtain the update rule

Xtk+1 = —h⇤⇣—h(Xtk ) Â ˜d

j=1 ˜F j tk ˆy·  j k⌘  

ˆyi · k = ˜A K•b| ˜A ˆyi · k + K•gi k  

(21)
where ˜A = Iea0A and where K• 2 R ˜d is deﬁned in the statement of the Proposition B.2. This yields
a generalized momentum update rule where we keep track of ˜d momentum states with ( ˆyi  j k) ˜d
j=1 
and update its position using a linear update rule. This algorithm can be seen as being most similar
to the Aggregated Momentum technique of Lucas et al. (2018)  which also keeps track of multiple
momentum states which decay at different rates.
Under the special case where ˜d = 1  b = 1  and h = 1
update rule of Polyak (1964) as

2kxk2 we recover the exact momentum algorithm
(22)
ˆyi k = p1 ˆyk + p2 gtk  
where we have a scalar learning rate ˜Ftk  where p1 = ˜A K•b| ˜A  p2 = K• are positive scalars  and
where gtk are mini-batch draws from the gradient as in equation 2.
The recovery of the momentum algorithm of Polyak (1964) has some interesting consequences. Since
p1 and p2 are functions of the model parameters s  A and a0  we obtain a direct relationship between
the optimal choice for the momentum model parameters  the assumed scale of gradient noise s  L > 0
and the assumed expected rate of decay of gradients  as given by eAt. This result gives insight as
to how momentum parameters should be chosen in terms of their prior beliefs on the optimization
problem.

Xtk+1  Xtk =  ˜Ftk ˆyk  

6 Discussion and Future Research Directions

Over the course of the paper we present a variational framework on optimizers  which interprets the
task of stochastic optimization as an inference problem on a latent surface that we wish to optimize.
By solving a variational problem over continuous optimizers with asymmetric information  we ﬁnd
that optimal algorithms should satisfy a system of FBSDEs projected onto the ﬁltration F generated
by the noisy observations of the latent process.
By solving these FBSDEs and obtaining continuous-time optimizers  we ﬁnd a direct relationship
between the measure assigned to the latent surface and its relationship to how data is observed.
In particular  assigning simple prior models to the pair of processes (— f (Xt) gt)t2[0 T ]  recovers
a number of well-known and widely used optimization algorithms. The fact that this framework
can naturally recover these algorithms begs further study. In particular  it is still an open question
whether it is possible to recover other stochastic algorithms via this framework  particularly those
with second-order scaling adjustments such as ADAM or AdaGrad.
From a more technical perspective  the intent is to further explore properties of the optimization model
presented here and the form of the algorithms it suggests. In particular  the optimality FBSDE 9 is
nonlinear  high-dimensional and intractable in general  making it difﬁcult to use existing FBSDE
approximation techniques  so new tools may need to be developed to understand the full extent of its
behavior.
Lastly  numerical work on the algorithms generated by this framework can provide some insights
as to which prior gradient models work well when discretized. The extension of simplectic and
quasi-simplectic stochastic integrators applied to the BSDEs and SDEs that appear in this paper also
has the potential for interesting future work.

References
Laurence Aitchison. A uniﬁed theory of adaptive stochastic gradient descent as bayesian ﬁltering.

arXiv preprint arXiv:1807.07540  2018.

9

Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31(3):167–175  2003.

Alain Bensoussan. Stochastic control of partially observable systems. Cambridge University Press 

2004.

René Carmona. Lectures on BSDEs  stochastic control  and stochastic differential games with

ﬁnancial applications  volume 1. SIAM  2016.

Philippe Casgrain and Sebastian Jaimungal. Mean ﬁeld games with partial information for algorithmic

trading. arXiv preprint arXiv:1803.04094  2018a.

Philippe Casgrain and Sebastian Jaimungal. Mean-ﬁeld games with differing beliefs for algorithmic

trading. arXiv preprint arXiv:1810.06101  2018b.

Philippe Casgrain and Sebastian Jaimungal. Trading algorithms with learning in latent alpha models.

arXiv preprint arXiv:1806.04472  2018c.

Nicolo Cesa-Bianchi  Alex Conconi  and Claudio Gentile. On the generalization ability of on-line

learning algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  2004.

André Belotto da Silva and Maxime Gazeau. A general system of differential equations to model

ﬁrst order adaptive algorithms. arXiv preprint arXiv:1810.13108  2018.

John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12(Jul):2121–2159  2011.

Vineet Gupta  Tomer Koren  and Yoram Singer. A uniﬁed approach to adaptive regularization in

online and stochastic optimization. arXiv preprint arXiv:1706.06569  2017.

Jean Jacod and Albert Shiryaev. Limit theorems for stochastic processes  volume 288. Springer

Science & Business Media  2013.

Svetlana Jankovi´c  Miljana Jovanovi´c  and Jasmina Djordjevi´c. Perturbed backward stochastic

differential equations. Mathematical and Computer Modelling  55(5-6):1734–1745  2012.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

Walid Krichene and Peter L Bartlett. Acceleration and averaging in stochastic descent dynamics. In

Advances in Neural Information Processing Systems  pages 6796–6806  2017.

Walid Krichene  Alexandre Bayen  and Peter L Bartlett. Accelerated mirror descent in continuous
and discrete time. In Advances in neural information processing systems  pages 2845–2853  2015.
James Lucas  Shengyang Sun  Richard Zemel  and Roger Grosse. Aggregated momentum: Stability

through passive damping. arXiv preprint arXiv:1804.00325  2018.

Jin Ma  J-M Morel  and Jiongmin Yong. Forward-backward stochastic differential equations and

their applications. Number 1702. Springer Science & Business Media  1999.

Panayotis Mertikopoulos and Mathias Staudigl. On the convergence of gradient-like ﬂows with noisy

gradient input. SIAM Journal on Optimization  28(1):163–197  2018.

Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method

efﬁciency in optimization. 1983.

Yu Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).

In Sov. Math. Dokl  volume 27.

Etienne Pardoux and Shanjian Tang. Forward-backward stochastic differential equations and quasi-

linear parabolic pdes. Probability Theory and Related Fields  114(2):123–150  1999.

Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics  4(5):1–17  1964.

Maxim Raginsky and Jake Bouvrie. Continuous-time stochastic mirror descent on a network:
Variance reduction  consensus  convergence. In 2012 IEEE 51st IEEE Conference on Decision and
Control (CDC)  pages 6793–6800. IEEE  2012.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical

statistics  pages 400–407  1951.

Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint

arXiv:1609.04747  2016.

10

Weijie Su  Stephen Boyd  and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems  pages 2510–2518  2014.

Ramon Van Handel. Stochastic calculus  ﬁltering  and stochastic control. Course notes.  URL

http://www. princeton. edu/˜ rvan/acm217/ACM217. pdf  2007.

James Vuckovic. Kalman gradient descent: Adaptive variance reduction in stochastic optimization.

arXiv preprint arXiv:1810.12273  2018.

Jean Walrand and Antonis Dimakis. Random processes in systems - lecture notes. Department of
Electrical Engineering and Computer Sciences  University of California  Berkeley CA 94720 
August 2006.

Andre Wibisono  Ashia C Wilson  and Michael I Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences  113(47):E7351–E7358 
2016.

Ashia C Wilson  Benjamin Recht  and Michael I Jordan. A lyapunov analysis of momentum methods

in optimization. arXiv preprint arXiv:1611.02635  2016.

Pan Xu  Tianhao Wang  and Quanquan Gu. Accelerated stochastic mirror descent: From continuous-
time dynamics to discrete-time algorithms. In International Conference on Artiﬁcial Intelligence
and Statistics  pages 1087–1096  2018a.

Pan Xu  Tianhao Wang  and Quanquan Gu. Continuous and discrete-time accelerated stochastic
mirror descent for strongly convex functions. In International Conference on Machine Learning 
pages 5488–5497  2018b.

11

,Philippe Casgrain