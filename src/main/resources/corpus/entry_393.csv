2007,Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis,We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm  which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations  we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods  and also appear to converge in a larger class of models.,Adaptive Embedded Subgraph Algorithms using

Walk-Sum Analysis

Venkat Chandrasekaran  Jason K. Johnson  and Alan S. Willsky

Department of Electrical Engineering and Computer Science

Massachusetts Institute of Technology

venkatc@mit.edu  jasonj@mit.edu  willsky@mit.edu

Abstract

We consider the estimation problem in Gaussian graphical models with arbitrary
structure. We analyze the Embedded Trees algorithm  which solves a sequence of
problems on tractable subgraphs thereby leading to the solution of the estimation
problem on an intractable graph. Our analysis is based on the recently developed
walk-sum interpretation of Gaussian estimation. We show that non-stationary it-
erations of the Embedded Trees algorithm using any sequence of subgraphs con-
verge in walk-summable models. Based on walk-sum calculations  we develop
adaptive methods that optimize the choice of subgraphs used at each iteration with
a view to achieving maximum reduction in error. These adaptive procedures pro-
vide a signiﬁcant speedup in convergence over stationary iterative methods  and
also appear to converge in a larger class of models.

1 Introduction

Stochastic processes deﬁned on graphs offer a compact representation for the Markov structure in a
large collection of random variables. We consider the class of Gaussian processes deﬁned on graphs 
or Gaussian graphical models  which are used to model natural phenomena in many large-scale ap-
plications [1  2]. In such models  the estimation problem can be solved by directly inverting the
information matrix. However  the resulting complexity is cubic in the number of variables  thus
being prohibitively complex in applications involving hundreds of thousands of variables. Algo-
rithms such as Belief Propagation and the junction-tree method are effective for computing exact
estimates in graphical models that are tree-structured or have low treewidth [3]  but for graphs with
high treewidth the junction-tree approach is intractable.

We describe a rich class of iterative algorithms for estimation in Gaussian graphical models with
arbitrary structure. Speciﬁcally  we discuss the Embedded Trees (ET) iteration [4] that solves a
sequence of estimation problems on trees  or more generally tractable subgraphs  leading to the so-
lution of the original problem on the intractable graph. We analyze non-stationary iterations of the
ET algorithm that perform inference calculations on an arbitrary sequence of subgraphs. Our anal-
ysis is based on the recently developed walk-sum interpretation of inference in Gaussian graphical
models [5]. We show that in the broad class of so-called walk-summable models  the ET algorithm
converges for any arbitrary sequence of subgraphs used. The walk-summability of a model is easily
tested [5  6]  thus providing a simple sufﬁcient condition for the convergence of such non-stationary
algorithms. Previous convergence results [6  7] analyzed stationary or “cyclo-stationary” iterations
that use the same subgraph at each iteration or cycle through a ﬁxed sequence of subgraphs. The
focus of this paper is on analyzing  and developing algorithms based on  arbitrary non-stationary
iterations that use any (non-cyclic) sequence of subgraphs  and the recently developed concept of
walk-sums appears to be critical to this analysis.

1

Given this great ﬂexibility in choosing successive iterative steps  we develop algorithms that adap-
tively optimize the choice of subgraphs to achieve maximum reduction in error. These algorithms
take advantage of walk-sum calculations  which are useful in showing that our methods minimize
an upper-bound on the error at each iteration. We develop two procedures to adaptively choose sub-
graphs. The ﬁrst method ﬁnds the best tree at each iteration by solving an appropriately formulated
maximum-weight spanning tree problem  with the weight of each edge being a function of the par-
tial correlation coefﬁcient of the edge and the residual errors at the nodes that compose the edge.
The second method  building on this ﬁrst method  adds extra edges in a greedy manner to the tree
resulting from the ﬁrst method to form a thin hypertree. Simulation results demonstrate that these
non-stationary algorithms provide a signiﬁcant speedup in convergence over stationary and cyclic
iterative methods. Since the class of walk-summable models is broad (including attractive models 
diagonally dominant models  and so-called pairwise-normalizable models)  our methods provide a
convergent  computationally attractive method for inference. We also provide empirical evidence to
show that our adaptive methods (with a minor modiﬁcation) converge in many non-walk-summable
models when stationary iterations diverge. The estimation problem in Gaussian graphical models
involves solving a linear system with a sparse  symmetric  positive-deﬁnite matrix. Such systems
are commonly encountered in other areas of machine learning and signal processing as well [8  9].
Therefore  our methods are broadly applicable beyond estimation in Gaussian models.

Some of the results presented here appear in more detail in a longer paper [10]  which provides
complete proofs as well as a detailed description of walk-sum diagrams that give a graphical inter-
pretation of our algorithms (we show an example in this paper). The report also considers problems
involving communication “failure” between nodes for distributed sensor network applications.

2 Background

Let G = (V E) be a graph with vertices V   and edges E ⊂(cid:0)V
Here (cid:0)V

(cid:1) that link pairs of vertices together.
(cid:1) represents the set of all unordered pairs of vertices. Consider a Gaussian distribution in

2

2

information form [5] p(x) ∝ exp{− 1
2 xT Jx + hT x}  where J−1 is the covariance matrix and J−1h
is the mean. The matrix J  also called the information matrix  is sparse according to graph G  i.e.
Js t = Jt s = 0 if and only if {s  t} /∈ E. Thus  G represents the graph with respect to which p(x)
is Markov  i.e. p(x) satisﬁes the conditional independencies implied by the separators of G. The
Gaussian mean estimation problem reduces to solving the following linear system of equations:

(1)
where x is the mean vector. Convergent iterations that compute the mean can also be used in turn to
compute variances using a variety of methods [4  11]. Thus  we focus on the problem of estimating
the mean at each node. Throughout the rest of this paper  we assume that J is normalized to have
1’s along the diagonal.1 Such a re-scaling does not affect the convergence results in this paper  and
our analysis and algorithms can be easily generalized to the un-normalized case [10].

Jx = h 

2.1 Walk-sums
We give a brief overview of the walk-sum framework developed in [5]. Let J = I − R. The off-
diagonal entries of the matrix R have the same sparsity structure as that of J  and consequently that
of the graph G. For Gaussian processes deﬁned on graphs  element Rs t corresponds to the condi-
tional correlation coefﬁcient between the variables at vertices s and t conditioned on knowledge of
all the other variables (also known as the partial correlation coefﬁcient [5]). A walk is a sequence of
vertices {wi}‘
i=0 such that each step {wi  wi+1} ∈ E  0 ≤ i ≤ ‘ − 1  with no restriction on crossing
the same vertex or traversing the same edge multiple times. The weight of a walk is the product of the
i=0 Rwi wi+1.
We then have that (R‘)s t is the sum of the weights of all length-‘ walks from s to t in G. With this
point of view  we can interpret J−1 as follows:

edge-wise partial correlation coefﬁcients of the edges composing the walk: φ(w)  Q‘−1

∞X

∞X

(J−1)s t = ((I − R)−1)s t =

(R‘)s t =

φ(s ‘→ t) 

(2)

1This can be achieved by performing the transformation ˜J ← D

containing the diagonal entries of J.

− 1

2 JD

− 1

2   where D is a diagonal matrix

‘=0

‘=0

2

where φ(s ‘→ t) represents the sum of the weights of all the length-‘ walks from s to t (the set
of all such walks is ﬁnite). Thus  (J−1)s t is the length-ordered sum over all walks in G from s
to t. This  however  is a very speciﬁc way to compute the inverse that converges if the spectral
radius (R) < 1. Other algorithms may compute walks according to different orders (rather than
length-based orders). To analyze arbitrary algorithms that submit to a walk-sum interpretation  the
following concept of walk-summability was developed in [5]. A model is said to be walk-summable
if for each pair of vertices s  t ∈ V   the absolute sum over all walks from s to t in G converges:

|φ(w)| < ∞.

(3)

¯φ(s → t)   X

w∈W(s→t)

Here  W(s → t) represents the set of all walks from s to t  and ¯φ(s → t) denotes the absolute
walk-sum2 over this set. Based on the absolute convergence condition  walk-summability implies
that walk-sums over a countable set of walks in G can be computed in any order. As a result  we
have the following interpretation in walk-summable models:

(J−1)s t = φ(s → t) 

xt = (J−1h)t =X

(4)

hsφ(s → t)   φ(h;∗ → t) 

s∈V

(5)
where the wildcard character ∗ denotes a union over all vertices in V   and φ(h;W) denotes a re-
weighting of each walk in W by the corresponding h value at the starting node. Note that in (4) we
relax the constraint that the sum is ordered by length  and do not explicitly specify an ordering on
the walks (such as in (2)). In words  (J−1)s t is the walk-sum over the set of all walks from s to t 
and xt is the walk-sum over all walks ending at t  re-weighted by h.
As shown in [5]  the walk-summability of a model is equivalent to ( ¯R) < 1  where ¯R denotes the
matrix of the absolute values of the elements of R. Also  a broad class of models are walk-summable 
including diagonally-dominant models  so-called pairwise normalizable models  and models for
which the underlying graph G is non-frustrated  i.e. each cycle has an even number of negative
partial correlation coefﬁcients. Walk-summability implies that a model is valid  i.e. has positive-
deﬁnite information/covariance.

Concatenation of walks We brieﬂy describe the concatenation operation for walks and walk-sets 
which plays a key role in walk-sum analysis. Let u = u0 ··· uend and v = vstartv1 ··· v‘(v) be walks
with uend = vstart. The concatenation of these walks is deﬁned to be u· v   u0 ··· uendv1 ··· v‘(v).
Now consider a walk-set U with all walks ending at uend and another walk-set V with all walks
beginning at vstart. If uend = vstart  then the concatenation of U and V is deﬁned:

U ⊗ V   {u · v : u ∈ U  v ∈ V}.

2.2 Embedded Trees algorithm

We describe the Embedded Trees iteration that performs a sequence of updates on trees  or more
generally tractable subgraphs  leading to the solution of (1) on an intractable graph. Each iteration
involves an inference calculation on a subgraph of all the variables V . Let (V S) be some subgraph
of G  i.e. S ⊂ E (see examples in Figure 1). Let J be split according to S as J = JS − KS  so that
the entries of J corresponding to edges in S are assigned to JS  and those corresponding to E\S are
part of KS. The diagonal entries of J are all part of JS; thus  KS has zeroes along the diagonal.3
JSbx(n) = KSbx(n−1) + h. If JS is invertible  and it is tractable to apply J−1S to a vector  then ET
Based on this splitting  we can transform (1) to JS x = KS x+h  which suggests a natural recursion:
offers an effective method to solve (1) (assuming (J−1S KS) < 1). If the subgraph used changes
with each iteration  then we obtain the following non-stationary ET iteration:

(6)
where {Sn}∞
n=1 is any arbitrary sequence of subgraphs. An important degree of freedom is the
choice of the subgraph Sn at iteration n  which forms the focus of Section 4 of this paper. In [10] we
also consider a more general class of algorithms that update subsets of variables at each iteration.

bx(n) = J−1Sn

(KSnbx(n−1) + h) 

2We generally denote the walk-sum of the set W(∼) by φ(∼).
3KS can have non-zero diagonal in general  but we only consider the zero diagonal case here.

3

Figure 1: (Left) G and three embedded trees S1 S2 S3; (Right) Corresponding walk-sum diagram.
3 Walk-Sum Analysis and Convergence of the Embedded Trees algorithm

Sn−→ t)

Sn−→ t)

Sn−→ t) 

In this section  we provide a walk-sum interpretation for the ET algorithm. Using this analysis  we
show that the non-stationary ET iteration (6) converges in walk-summable models for an arbitrary
choice of subgraphs {Sn}∞
n=1. Before proceeding with the analysis  we point out that one potential
complication with the ET algorithm is that the matrix JS corresponding to some subgraph S may be
indeﬁnite or singular  even if the original model J is positive-deﬁnite. Importantly  such a problem
never arises in walk-summable models with JS being positive-deﬁnite for any subgraph S if J is
walk-summable. This is easily seen because walks in the subgraph S are a subset of the walks
in G  and thus if absolute walk-sums in G are well-deﬁned  then so are absolute walk-sums in S.
Therefore  JS is walk-summable  and hence  positive-deﬁnite.
Consider the following recursively deﬁned set of walks for s  t ∈ V :
Wn(s → t) =

(cid:20)
= Wn−1(s → ∗) ⊗ W(∗ E\Sn(1)−→ •) ⊗ W(• Sn−→ t) [ W(s

∪u v∈V Wn−1(s → u) ⊗ W(u

(cid:21) [ W(s

E\Sn(1)−→ v) ⊗ W(v

(7)
with W0(s → t) = ∅. Here  ∗ and • are used as wildcard characters (a union over all elements in V ) 
and ⊗ denotes concatenation of walk-sets as described previously. The set Wn−1(s → ∗) denotes
walks that start at node s computed at the previous iteration. The middle term W(∗ E\Sn(1)−→ •)
denotes a length-1 walk (called a hop) across an edge in E\Sn. Finally  W(• Sn−→ t) denotes walks
in Sn that end at node t. Thus  the ﬁrst term in (7) refers to previously computed walks starting at s 
which hop across an edge in E\Sn  and then ﬁnally propagate only in Sn (ending at t). The second
Sn−→ t) denotes walks from s to t that only live within Sn. The following proposition
term W(s
(proved in [10]) shows that the walks contained in these walk-sets are precisely those computed by
the ET algorithm at iteration n. For simplicity  we denote φ(Wn(s → t)) by φn(s → t).
Proposition 1 Let bx(n) be the estimate at iteration n in the ET algorithm (6) with initial guess
bx(0) = 0. Then bx(n)
can be interpreted as a walk-sum algorithm: bx(n)

We note that the classic Gauss-Jacobi algorithm [6]  a stationary iteration with JS = I and KS = R 
in this method computes all walks up to length n
ending at t. Figure 1 gives an example of a walk-sum diagram  which provides a graphical repre-
sentation of the walks accumulated by the walk-sets (7). The diagram is the three-level graph on the
right  and corresponds to an ET iteration based on the subgraphs S1 S2 S3 of the 3 × 3 grid G (on
the left). Each level n in the diagram consists of the subgraph Sn used at iteration n (solid edges) 
and information from the previous level (iteration) n − 1 is transmitted through the dashed edges
in E\Sn. The directed nature of these dashed edges is critical as they capture the one-directional
ﬂow of computations from iteration to iteration  while the undirected edges within each level capture
the inference computation at each iteration. Consider a node v at level n of the diagram. Walks in
the diagram that start at any node and end at v at level n  re-weighted by h  are exactly the walks

t = φn(h;∗ → t) =P

s∈V hsφn(s → t) in walk-summable models.

t

computed by the ET algorithm inbx(n)

v . For more examples of such diagrams  see [10].

Given this walk-sum interpretation of the ET algorithm  we can analyze the walk-sets (7) to prove
the convergence of ET in walk-summable models by showing that the walk-sets eventually contain
all the walks required for the computation of J−1h in (5). We have the following convergence
theorem for which we only provide a brief sketch of the complete proof [10].

4

Theroem 1 Letbx(n) be the estimate at iteration n in the ET algorithm (6) with initial guessbx(0) =
0. Then bx(n) → J−1h element-wise as n → ∞ in walk-summable models.

Proof outline: Proving this statement is done in the following stages.
Validity: The walks in Wn are valid walks in G  i.e. Wn(s → t) ⊆ W(s → t).
Nesting: The walk-sets Wn(s → t) are nested  i.e. Wn−1(s → t) ⊆ Wn(s → t) ∀n.
Completeness: Let w ∈ W(s → t). There exists an N > 0 such that w ∈ WN (s → t). Using the
nesting property  we conclude that for all n ≥ N  w ∈ Wn(s → t).
These steps combined together allow us to conclude that φn(s → t) → φ(s → t) as n → ∞. This
conclusion relies on the fact that φ(Wn) → φ(∪nWn) as n → ∞ for a sequence of nested walk-sets
Wn−1 ⊆ Wn in walk-summable models  which is a consequence of the sum-partition theorem for
absolutely summable series [5  10  12]. Given the walk-sum interpretation from Proposition 1  one

can check thatbx(n) → J−1h element-wise as n → ∞. (cid:3)
sequence of subgraphs withbx(0) = 0. It is then straightforward to show that convergence can be

Thus  the ET algorithm converges to the correct solution of (1) in walk-summable models for any

achieved for any initial guess [10]. Note that we have taken advantage of the absolute convergence
property in walk-summable models (3) by not focusing on the order in which walks are computed 
but only that they are eventually computed.
In [10]  we prove that walk-summability is also a
necessary condition for the complete ﬂexibility in the choice of subgraphs — there exists at least
one sequence of subgraphs that results in a divergent ET iteration in non-walk-summable models.

4 Adaptive algorithms

Let e(n) = x−bx(n) be the error at iteration n and let h(n) = Je(n) = h−Jbx(n) be the corresponding

residual error (which is tractable to compute). We begin by describing an algorithm to choose the
“next-best” tree Sn in the ET iteration (6). The error at iteration n can be re-written as follows:

e(n) = (J−1 − J−1Sn

)h(n−1).

t = φ(h(n−1);∗ G\Sn−→ t)  where G\Sn denotes walks
Thus  we have the walk-sum interpretation e(n)
that do not live entirely within Sn. Using this expression for the error  we have the following bound
that is tight for attractive models (Rs t ≥ 0 for all s  t ∈ V ) and non-negative h(n−1):

ke(n)k‘1 = X

t∈V

|φ(h(n−1);∗ G\Sn−→ t)|

≤ ¯φ(|h(n−1)|;G\Sn)
= ¯φ(|h(n−1)|;G) − ¯φ(|h(n−1)|;Sn).

(8)
Hence  minimizing the error at iteration n corresponds to ﬁnding the tree Sn that maximizes the
second term ¯φ(|h(n−1)|;Sn). This leads us to the following maximum walk-sum tree problem:

arg max

Sn a tree

¯φ(|h(n−1)|;Sn)

Finding the optimal such tree is combinatorially complex. Therefore  we develop a relaxation that
minimizes a looser upper bound than (8). Speciﬁcally  consider an edge {u  v} and all the walks that
live on this single edge W({u  v}) = {uv  vu  uvu  vuv  uvuv  vuvu  . . .}. One can check that the
contribution based on these single-edge walks can be computed as:

¯φ(|h(n−1)|; w) =

| + |h(n−1)

v

|(cid:17) |Ru v|

1 − |Ru v| .

σu v = X

w∈W({u v})

This weight provides a measure of the error-reduction capacity of edge {u  v} by itself at iteration
n. These single-edge walks for edges in Sn are a subset of all the walks in Sn  and consequently
provide a lower-bound on ¯φ(|h(n−1)|;Sn). Therefore  the maximization

arg max

Sn a tree

σu v

(11)

(9)

(10)

(cid:16)|h(n−1)

u

X

{u v}∈Sn

5

Figure 2: Grayscale images of residual errors in an 8 × 8 grid at successive iterations  and corre-
sponding trees chosen by adaptive method.

Figure 3: Grayscale images of residual errors in an 8 × 8 grid at successive iterations  and corre-
sponding hypertrees chosen by adaptive method.

is equivalent to minimizing a looser upper-bound than (8). This relaxed problem can be solved
efﬁciently using a maximum-weight spanning tree algorithm that has complexity O(|E| log log |V |)
for sparse graphs [13].

Given the maximum-weight spanning tree of the graph  a natural extension is to build a thin hyper-
tree by adding extra “strong” edges to the tree  subject to the constraint that the resulting graph has
low treewidth. Unfortunately  to do so optimally is an NP-hard optimization problem [14]. Hence 
we settle on a simple greedy algorithm. For each edge not included in the tree  in order of decreas-
ing edge weight  we add the edge to the graph if two conditions are met: ﬁrst  we are able to easily
verify that the treewidth stays less than M  and second  the length of the unique path in Sn between
the endpoints is less than L. In order to bound the tree width  we maintain a counter at each node
of the total number of added edges that result in a path through that node. Comparing to another
method for constructing junction trees from spanning trees [15]  one can check that the maximum
node count is an upper-bound on the treewidth. We note that by using an appropriate directed repre-
sentation of Sn relative to an arbitrary root  it is simple to identify the path between two nodes with
complexity linear in path length (< L).4 Hence  the additional complexity of this greedy algorithm
over that of the tree-selection procedure described previously is O(L|E|).
In Figure 2 and Figure 3 we present a simple demonstration of the tree and hypertree selection
procedures respectively  and the corresponding change in error achieved. The grayscale images
represent the residual errors at the nodes of an 8 × 8 grid similar to G in Figure 1 (with white
representing 1 and black representing 0)  and the graphs beside them show the trees/hypertrees
chosen based on these residual errors using the methods described above (the grid edge partial
correlation coefﬁcients are the same for all edges). Notice that the ﬁrst tree in Figure 2 tries to
include as many edges as possible that are incident on the nodes with high residual error. Such
edges are useful for capturing walks ending at the high-error nodes  which contribute to the set of
walks in (5). The ﬁrst hypertree in Figure 3 actually includes all the edges incident on the high-
error nodes. The residual errors after inference on these subgraphs are shown next in Figure 2 and
Figure 3. As expected  the hypertree seems to achieve greater reduction in error compared to the
spanning tree. Again  at this iteration  the subgraphs chosen by our methods adapt based on the
errors at the various nodes.

5 Experimental illustration

5.1 Walk-summable models

We test the adaptive algorithms on densely connected nearest-neighbor grid-structured models (sim-
ilar to G in Figure 1). We generate random grid models — the grid edge partial correlation coef-

4One sets two pointers into the tree starting from any two nodes and then iteratively walks up the tree  always

advancing from the point that is deeper in the tree  until the nearest ancestor of the two nodes is reached.

6

Figure 4: (Left) Average number of iterations required for the normalized residual to reduce by a
factor of 10−6 over 100 randomly generated 75 × 75 grid models; (Center) Convergence plot for a
randomly generated 511×511 grid model; (Right) Convergence range in terms of partial correlation
for 16-node cyclic model with edges to neighbors two steps away.

Figure 5: (Left) 16-node graphical model; (Right) two embedded spanning trees T1  T2.

ﬁcients are chosen uniformly from [−1  1] and R is scaled so that ( ¯R) = 0.99. The vector h is
chosen to be the all-ones vector. The table on the left in Figure 4 shows the average number of
iterations required by various algorithms to reduce the normalized residual error
by a factor
of 10−6. The average was computed based on 100 randomly generated 75 × 75 grid models. The
plot in Figure 4 shows the decrease in the normalized residual error as a function of the number of
iterations on a randomly generated 511 × 511 grid model. All these models are poorly conditioned
because they are barely walk-summable (( ¯R) = 0.99). The stationary one-tree iteration uses a tree
similar to S1 in Figure 1  and the two-tree iteration alternates between trees similar to S1 and S3 in
Figure 1 [4]. The adaptive hypertree method uses M = 6 and L = 8. We also note that in practice
the per-iteration costs of the adaptive tree and hypertree algorithms are roughly comparable.

kh(n)k2
kh(0)k2

These results show that our adaptive algorithms demonstrate signiﬁcantly superior convergence
properties compared to stationary methods  thus providing a convergent  computationally attrac-
tive method for estimation in walk-summable models. Our methods are applicable beyond Gaussian
estimation to other problems that require solution of linear systems based on sparse  symmetric 
positive-deﬁnite matrices. Several recent papers that develop machine learning algorithms are based
on solving such systems of equations [8  9]; in fact  both of these papers involve linear systems
based on diagonally-dominant matrices  which are walk-summable.

5.2 Non-walk-summable models

Next  we give empirical evidence that our adaptive methods provide convergence over a broader
range of models than stationary iterations. One potential complication in non-walk-summable mod-
els is that the subgraph models chosen by the stationary and adaptive algorithms may be indeﬁnite
or singular even though J is positive-deﬁnite. In order to avoid this problem in the adaptive ET
algorithm  the trees Sn chosen at each iteration must be valid (i.e.  have positive-deﬁnite JSn). A
simple modiﬁcation to the maximum-weight spanning tree algorithm achieves this goal — we add
an extra condition to the algorithm to test for diagonal dominance of the chosen tree model (as
all symmetric  diagonally-dominant models are positive deﬁnite [6]). That is  at each step of the
maximum-weight spanning tree algorithm  we only add an edge if it does not create a cycle and
maintains a diagonally-dominant tractable subgraph model. Consider the 16-node model on the left
in Figure 5. Let all the edge partial correlation coefﬁcients be r. The range of r for which J is
positive-deﬁnite is roughly (−0.46  0.25)  and the range for which the model is walk-summable is
(−0.25  0.25) (in this range all the algorithms  both stationary and adaptive  converge). For the one-
tree iteration we use tree T1  and for the two-tree iteration we alternate between trees T1 and T2 (see

7

Figure 5). As the table on the right in Figure 4 demonstrates  the adaptive tree algorithm without the
diagonal-dominance (DD) check provides convergence over a much broader range of models than
the one-tree and two-tree iterations  but not for all valid models. However  the modiﬁed adaptive
tree algorithm with the DD check appears to converge almost up to the validity threshold. We have
also observed such behavior empirically in many other (though not all) non-walk-summable models
where the adaptive ET algorithm with the DD condition converges while stationary methods diverge.
Thus  our adaptive methods  compared to stationary iterations  not only provide faster convergence
rates in walk-summable models but also converge for a broader class of models.

6 Discussion

We analyze non-stationary iterations of the ET algorithm that use any sequence of subgraphs for
estimation in Gaussian graphical models. Our analysis is based on the recently developed walk-sum
interpretation of inference in Gaussian models  and we show that the ET algorithm converges for
any sequence of subgraphs used in walk-summable models. These convergence results motivate
the development of methods to choose subgraphs adaptively at each iteration to achieve maximum
reduction in error. The adaptive procedures are based on walk-sum calculations  and minimize an
upper bound on the error at each iteration. Our simulation results show that the adaptive algorithms
provide a signiﬁcant speedup in convergence over stationary methods. Moreover  these adaptive
methods also seem to provide convergence over a broader class of models than stationary algorithms.

Our adaptive algorithms are greedy in that they only choose the “next-best” subgraph. An interest-
ing question is to develop tractable methods to compute the next K best subgraphs jointly to achieve
maximum reduction in error after K iterations. The experiment with non-walk-summable models
suggests that walk-sum analysis could be useful to provide convergent algorithms for non-walk-
summable models  perhaps with restrictions on the order in which walk-sums are computed. Fi-
nally  subgraph preconditioners have been shown to improve the convergence rate of the conjugate-
gradient method; using walk-sum analysis to select such preconditioners is of clear interest.

References
[1] M. Luettgen  W. Carl  and A. Willsky. Efﬁcient multiscale regularization with application to optical ﬂow.

IEEE Transactions on Image Processing  3(1):41–64  Jan. 1994.

[2] P. Rusmevichientong and B. Van Roy. An Analysis of Turbo Decoding with Gaussian densities.

Advances in Neural Information Processing Systems 12  2000.

In

[3] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kauffman  San Mateo  CA  1988.
[4] E. Sudderth  M. Wainwright  and A. Willsky. Embedded Trees: Estimation of Gaussian processes on

graphs with cycles. IEEE Transactions on Signal Processing  52(11):3136–3150  Nov. 2004.

[5] D. Malioutov  J. Johnson  and A. Willsky. Walk-Sums and Belief Propagation in Gaussian Graphical

Models. Journal of Machine Learning Research  7:2031–2064  Oct. 2006.

[6] R. Varga. Matrix Iterative Analysis. Springer-Verlag  New York  2000.
[7] R. Bru  F. Pedroche  and D. Szyld. Overlapping Additive and Multiplicative Schwarz iterations for H-

matrices. Linear Algebra and its Applications  393:91–105  Dec. 2004.

[8] D. Zhou  J. Huang  and B. Scholkopf. Learning from Labeled and Unlabeled data on a directed graph. In

Proceedings of the 22nd International Conference on Machine Learning  2005.

[9] D. Zhou and C. Burges. Spectral Clustering and Transductive Learning with multiple views. In Proceed-

ings of the 24th International Conference on Machine Learning  2007.

[10] V. Chandrasekaran  J. Johnson  and A. Willsky. Estimation in Gaussian Graphical Models using Tractable

Subgraphs: A Walk-Sum Analysis. To appear in IEEE Transactions on Signal Processing.

[11] D. Malioutov  J. Johnson  and A. Willsky. GMRF variance approximation using spliced wavelet bases. In

IEEE International Conference on Acoustics  Speech and Signal Processing  2007.

[12] R. Godement. Analysis I: Convergence  Elementary Functions. Springer-Verlag  New York  2004.
[13] T. Cormen  C. Leiserson  R. Rivest  and C. Stein. Introduction to Algorithms. MIT Press  2001.
[14] N. Srebro. Maximum Likelihood Markov Networks: An Algorithmic Approach. Master’s thesis  Mas-

sachusetts Institute of Technology  2000.

[15] F. Kschischang  B. Frey  and H. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transac-

tions on Information Theory  47(2):498–519  Feb. 2001.

8

,Zhijian Liu
Haotian Tang
Yujun Lin
Song Han