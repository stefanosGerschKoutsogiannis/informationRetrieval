2013,The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited,Hypergraphs allow to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or  on tensor methods which are only applicable under special conditions. In this paper we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element  is a family of regularization functionals based on  the total variation on hypergraphs.,The Total Variation on Hypergraphs - Learning on

Hypergraphs Revisited

Matthias Hein  Simon Setzer  Leonardo Jost and Syama Sundar Rangapuram

Department of Computer Science

Saarland University

Abstract

Hypergraphs allow one to encode higher-order relationships in data and are thus a
very ﬂexible modeling tool. Current learning methods are either based on approx-
imations of the hypergraphs via graphs or on tensor methods which are only appli-
cable under special conditions. In this paper  we present a new learning framework
on hypergraphs which fully uses the hypergraph structure. The key element is a
family of regularization functionals based on the total variation on hypergraphs.

1

Introduction

Graph-based learning is by now well established in machine learning and is the standard way to deal
with data that encode pairwise relationships. Hypergraphs are a natural extension of graphs which
allow to model also higher-order relations in data. It has been recognized in several application
areas such as computer vision [1  2]  bioinformatics [3  4] and information retrieval [5  6] that such
higher-order relations are available and help to improve the learning performance.
Current approaches in hypergraph-based learning can be divided into two categories. The ﬁrst one
uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for
graphs [7  8  9]. While tensor methods are mathematically quite appealing  they are limited to so-
called k-uniform hypergraphs  that is  each hyperedge contains exactly k vertices. Thus  they are not
able to model mixed higher-order relationships. The second main approach can deal with arbitrary
hypergraphs [10  11]. The basic idea of this line of work is to approximate the hypergraph via a stan-
dard weighted graph. In a second step  one then uses methods developed for graph-based clustering
and semi-supervised learning. The two main ways of approximating the hypergraph by a standard
graph are the clique and the star expansion which were compared in [12]. One can summarize [12]
by stating that no approximation fully encodes the hypergraph structure. Earlier  [13] have proven
that an exact representation of the hypergraph via a graph retaining its cut properties is impossible.
In this paper  we overcome the limitations of both existing approaches. For both clustering and semi-
supervised learning the key element  either explicitly or implicitly  is the cut functional. Our aim is
to directly work with the cut deﬁned on the hypergraph. We discuss in detail the differences of the
hypergraph cut and the cut induced by the clique and star expansion in Section 2.1. Then  in Section
2.2  we introduce the total variation on a hypergraph as the Lovasz extension of the hypergraph
cut. Based on this  we propose a family of regularization functionals which interpolate between
the total variation and a regularization functional enforcing smoother functions on the hypergraph
corresponding to Laplacian-type regularization on graphs. They are the key for the semi-supervised
learning method introduced in Section 3. In Section 4  we show in line of recent research [14  15  16 
17] that there exists a tight relaxation of the normalized hypergraph cut. In both learning problems 
convex optimization problems have to be solved for which we derive scalable methods in Section
5. The main ingredients of these algorithms are proximal mappings for which we provide a novel
algorithm and analyze its complexity. In the experimental section 6  we show that fully incorporating
hypergraph structure is beneﬁcial. All proofs are moved to the supplementary material.

1

2 The Total Variation on Hypergraphs

A large class of graph-based algorithms in semi-supervised learning and clustering is based either
explicitly or implicitly on the cut. Thus  we discuss ﬁrst in Section 2.1 the hypergraph cut and the
corresponding approximations.In Section 2.2  we introduce in analogy to graphs  the total variation
on hypergraphs as the Lovasz extension of the hypergraph cut.

2.1 Hypergraphs  Graphs and Cuts

of a vertex i ∈ V is deﬁned as di =(cid:80)
as |e| = (cid:80)

Hypergraphs allow modeling relations which are not only pairwise as in graphs but involve multiple
vertices. In this paper  we consider weighted undirected hypergraphs H = (V  E  w) where V is
the vertex set with |V | = n and E the set of hyperedges with |E| = m. Each hyperedge e ∈ E
corresponds to a subset of vertices  i.e.  to an element of 2V . The vector w ∈ Rm contains for
each hyperedge e its non-negative weight we. In the following  we use the letter H also for the
incidence matrix H ∈ R|V |×|E| which is for i ∈ V and e ∈ E  Hi e =
. The degree

(cid:26)1

if i ∈ e 
else.

0

e∈E weHi e and the cardinality of an edge e can be written
j∈V Hj e. We would like to emphasize that we do not impose the restriction that the

hypergraph is k-uniform  i.e.  that each hyperedge contains exactly k vertices.
The considered class of hypergraphs contains the set of undirected  weighted graphs which is equiv-
alent to the set of 2-uniform hypergraphs. The motivation for the total variation on hypergraphs
comes from the correspondence between the cut on a graph and the total variation functional. Thus 
we recall the deﬁnition of the cut on weighted graphs G = (V  W ) with weight matrix W . Let
C = V \C denote the complement of C in V . Then  for a partition (C  C)  the cut is deﬁned as

This standard deﬁnition of the cut carries over naturally to a hypergraph H

cutG(C  C) =

i j : i∈C j∈C

wij.

(cid:88)
(cid:88)

cutH (C  C) =

e∈E:

e∩C(cid:54)=∅  e∩C(cid:54)=∅

we.

(1)

Thus  the cut functional on a hypergraph is just the sum of the weights of the hyperedges which have
vertices both in C and C. It is not biased towards a particular way the hyperedge is cut  that is  how
many vertices of the hyperedge are in C resp. C. This emphasizes that the vertices in a hyperedge
belong together and we penalize every cut of a hyperedge with the same value.
In order to handle hypergraphs with existing methods developed for graphs  the focus in previous
works [11  12] has been on transforming the hypergraph into a graph. In [11]  they suggest using
the clique expansion (CE)  i.e.  every hyperedge e ∈ H is replaced with a fully connected subgraph
where every edge in this subgraph has weight we|e| . This leads to the cut functional cutCE 

cutCE(C  C) :=

e∈E:

e∩C(cid:54)=∅  e∩C(cid:54)=∅

we|e| |e ∩ C||e ∩ C|.

(2)

(cid:88)

Note that in contrast to the hypergraph cut (1)  the value of cutCE depends on the way each hyper-
edge is cut since the term |e∩ C||e∩ C| makes the weights dependent on the partition. In particular 
the smallest weight is attained if only a single vertex is split off  whereas the largest weight is attained
if the partition of the hyperedge is most balanced. In comparison to the hypergraph cut  this leads
to a bias towards cuts that favor splitting off single vertices from a hyperedge which in our point of
view is an undesired property for most applications. We illustrate this with an example in Figure
1  where the minimum hypergraph cut (cutH) leads to a balanced partition  whereas the minimum
clique expansion cut (cutCE) not only cuts an additional hyperedge but is also unbalanced. This is
due to its bias towards splitting off single nodes of a hyperedge. Another argument against the clique
expansion is computational complexity. For large hyperedges the clique expansion leads to (almost)
fully connected graphs which makes computations slow and is prohibitive for large hypergraphs.
We omit the discussion of the star graph approximation of hypergraphs discussed in [12] as it is
shown there that the star graph expansion is very similar to the clique expansion. Instead  we want
to recall the result of Ihler et al. [13] which states that in general there exists no graph with the same
vertex set V which has for every partition (C  C) the same cut value as the hypergraph cut.

2

Figure 1: Minimum hypergraph cut cutH vs. minimum cut of the clique expansion cutCE: For edge
weights w1 = w4 = 10  w2 = w5 = 0.1 and w3 = 0.6 the minimum hypergraph cut is (C1  C 1)
which is perfectly balanced. Although cutting one hyperedge more and being unbalanced  (C2  C 2)
is the optimal cut for the clique expansion approximation.

Finally  note that for weighted 3-uniform hypergraphs it is always possible to ﬁnd a corresponding
graph such that any cut of the graph is equal to the corresponding cut of the hypergraph.
Proposition 2.1. Suppose H = (V  E  w) is a weighted 3-uniform hypergraph. Then  W ∈
R|V |×|V | deﬁned as W = 1
2 Hdiag(w)H T deﬁnes the weight matrix of a graph G = (V  W ) where
each cut of G has the same value as the corresponding hypergraph cut of H.

2.2 The Total Variation on Hypergraphs

In this section  we deﬁne the total variation on hypergraphs. The key technical element is the Lovasz
extension which extends a set function  seen as a mapping on 2V   to a function on R|V |.
Deﬁnition 2.1. Let ˆS : 2V → R be a set function with ˆS(∅) = 0. Let f ∈ R|V |  let V be ordered
such that f1 ≤ f2 ≤ . . . ≤ fn and deﬁne Ci = {j ∈ V | j > i}. Then  the Lovasz extension
S : R|V | → R of ˆS is given by

n(cid:88)

i=1

(cid:16) ˆS(Ci−1) − ˆS(Ci)
(cid:17)

=

n−1(cid:88)

i=1

S(f ) =

fi

ˆS(Ci)(fi+1 − fi) + f1 ˆS(V ).

Note that for the characteristic function of a set C ⊂ V   we have S(1C) = ˆS(C).
It is well-known that the Lovasz extension S is a convex function if and only if ˆS is submodular
[18]. For graphs G = (V  W )  the total variation on graphs is deﬁned as the Lovasz extension of the
graph cut [18] given as T VG : R|V | → R  TVG(f ) = 1
Proposition 2.2. The total variation TVH : R|V | → R on a hypergraph H = (V  E  w) deﬁned as
the Lovasz extension of the hypergraph cut  ˆS(C) = cutH (C  C)  is a convex function given by

(cid:80)n
i j=1 wij|fi − fj|.
(cid:17)
(cid:88)

(cid:16)

2

TVH (f ) =

we

fi − min
j∈e

fj

max
i∈e

=

e∈E

we max
i j∈e

|fi − fj|.

(cid:88)

e∈E

as  TVH (f ) = (cid:80)

Note that the total variation of a hypergraph cut reduces to the total variation on graphs if H is
2-uniform (standard graph). There is an interesting relation of the total variation on hypergraphs
to sparsity inducing group norms. Namely  deﬁning for each edge e ∈ E the difference operator
De : R|V | → R|V |×|V | by (Def )ij = fi − fj if i  j ∈ e and 0 otherwise  TVH can be written
e∈E we (cid:107)Def(cid:107)∞  which can be seen as inducing group sparse structure on the
gradient level. The groups are the hyperedges and thus are typically overlapping. This could lead
potentially to extensions of the elastic net on graphs to hypergraphs.
It is known that using the total variation on graphs as a regularization functional in semi-supervised
learning (SSL) leads to very spiky solutions for small numbers of labeled points. Thus  one would
like to have regularization functionals enforcing more smoothness of the solutions. For graphs this
is achieved by using the family of regularization functionals ΩG p : R|V | → R 

ΩG p(f ) =

1
2

wij|fi − fj|p.

n(cid:88)

i j=1

3

For p = 2 we get the regularization functional of the graph Laplacian which is the basis of a large
class of methods on graphs. In analogy to graphs  we deﬁne a corresponding family on hypergraphs.
Deﬁnition 2.2. The regularization functionals ΩH p : R|V | → R for a hypergraph H = (V  E  w)
are deﬁned for p ≥ 1 as

(cid:17)p

(cid:16)

(cid:88)

e∈E

ΩH p(f ) =

we

fi − min
j∈e

max
i∈e

fj

.

Lemma 2.1. The functionals ΩH p : R|V | → R are convex.
Note that ΩH 1(f ) = TVH (f ). If H is a graph and p ≥ 1  ΩH p reduces to the Laplacian regulariza-
tion ΩG p. Note that for characteristic functions of sets  f = 1C  it holds ΩH p(1C) = cutH (C  C).
Thus  the difference between the hypergraph cut and its approximations such as clique and star
expansion carries over to ΩH p and ΩGCE  p  respectively.

3 Semi-supervised Learning

With the regularization functionals derived in the last section  we can immediately write down a
formulation for two-class semi-supervised learning on hypergraphs similar to the well-known ap-
proaches of [19  20]. Given the label set L we construct the vector Y ∈ Rn with Yi = 0 if i /∈ L
and Yi equal to the label in {−1  1} if i ∈ L. We propose solving

f∗ = arg min
f∈R|V |

1
2

(cid:107)f − Y (cid:107)2

2 + λ ΩH p(f ) 

(3)

where λ > 0 is the regularization parameter. In Section 5  we discuss how this convex optimization
problem can be solved efﬁciently for the case p = 1 and p = 2. Note  that other loss functions than
the squared loss could be used. However  the regularizer aims at contracting the function and we
use the label set {−1  1} so that f∗ ∈ [−1  1]|V |. Hence  on the interval [−1  1] the squared loss
behaves very similar to other margin-based loss functions. In general  we recommend using p = 2
as it corresponds to Laplacian-type regularization for graphs which is known to work well. For
graphs p = 1 is known to produce spiky solutions for small numbers of labeled points. This is due
to the effect that cutting “out” the labeled points leads to a much smaller cut than  e.g.  producing a
balanced partition. However  in the case where one has only a small number of hyperedges this effect
is much smaller and we will see in the experiments that p = 1 also leads to reasonable solutions.

4 Balanced Hypergraph Cuts

In Section 2.1  we discussed the difference between the hypergraph cut (1) and the graph cut of
the clique expansion (2) of the hypergraph and gave a simple example in Figure 1 where these
cuts yield quite different results. Clearly  this difference carries over to the famous normalized cut
criterion introduced in [21  22] for clustering of graphs with applications in image segmentation.
For a hypergraph the ratio resp. normalized cut can be formulated as

RCut(C  C) =

cutH (C  C)

|C||C|

  NCut(C  C) =

cutH (C  C)
vol(C) vol(C)

 

which incorporate different balancing criteria. Note  that in contrast to the normalized cut for graphs
the normalized hypergraph cut allows no relaxation into a linear eigenproblem (spectral relaxation).
Thus  we follow a recent line of research [14  15  16  17] where it has been shown that the standard
spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight  in
fact exact  relaxation can be formulated in terms of a nonlinear eigenproblem. Although nonlinear
eigenproblems are non-convex  one can compute nonlinear eigenvectors quite efﬁciently at the price
of loosing global optimality. However  it has been shown that the potentially non-optimal solutions
of the exact relaxation  outperform in practice the globally optimal solution of the loose relaxation 
often by large margin. In this section  we extend their approach to hypergraphs and consider general
  where ˆS : 2V → R+
balanced hypergraph cuts Bcut(C  C) of the form  Bcut(C  C) = cutH (C C)
is a non-negative  symmetric set function (that is ˆS(C) = ˆS(C)). For the normalized cut one has

ˆS(C)

4

ˆS(C) = vol(C) vol(C) whereas for the Cheeger cut one has ˆS(C) = min{vol C  vol C}. Other
examples of balancing functions can be found in [16]. Our following result shows that the balanced
hypergraph cut also has an exact relaxation into a continuous nonlinear eigenproblem [14].
Theorem 4.1. Let H = (V  E  w) be a ﬁnite  weighted hypergraph and S : R|V | → R be the Lovasz
extension of the symmetric  non-negative set function ˆS : 2V → R. Then  it holds that

(cid:80)

(cid:0) max

i∈e
S(f )

e∈E we

fi − min
j∈e

fj

(cid:1)

(cid:0) max

i∈e
S(f )

cutH (C  C)

.

ˆS(C)

(cid:1)

.

= min
C⊂V
Further  let f ∈ R|V | and deﬁne Ct := {i ∈ V | fi > t}. Then 

min
f∈R|V |

(cid:80)

e∈E we

fi − min
j∈e

fj

min
t∈R

cutH (Ct  Ct)

ˆS(Ct)

≤

The last part of the theorem shows that “optimal thresholding” (turning f ∈ RV into a partition)
among all level sets of any f ∈ R|V | can only lead to a better or equal balanced hypergraph cut.
The question remains how to minimize the ratio Q(f ) = TVH (f )
. As discussed in [16]  every
Lovasz extension S can be written as a difference of convex positively 1-homogeneous functions1
S = S1 − S2. Moreover  as shown in Prop. 2.2 the total variation TVH is convex. Thus  we have
to minimize a non-negative ratio of a convex and a difference of convex (d.c.) function. We employ
the RatioDCA algorithm [16] shown in Algorithm 1. The main part is the convex inner problem. In

S(f )

Algorithm 1 RatioDCA – Minimization of a non-negative ratio of 1-homogeneous d.c. functions
1: Objective: Q(f ) = R1(f )−R2(f )
2: repeat
3:
4:

S1(f )−S2(f ) . Initialization: f 0 = random with(cid:13)(cid:13)f 0(cid:13)(cid:13) = 1  λ0 = Q(f 0)
(cid:8)R1(u) −(cid:10)u  r2(f k)(cid:11) + λk(cid:0)S2(u) −(cid:10)u  s1(f k)(cid:11)(cid:1)(cid:9)

s1(f k) ∈ ∂S1(f k)  r2(f k) ∈ ∂R2(f k)
f k+1 = arg min
(cid:107)u(cid:107)2≤1
λk+1 = (R1(f k+1) − R2(f k+1))/(S1(f k+1) − S2(f k+1))

5:

6: until |λk+1−λk|
7: Output: eigenvalue λk+1 and eigenvector f k+1.

< 

λk

our case R1 = T VH   R2 = 0  and thus the inner problem reads

min(cid:107)u(cid:107)2≤1{TVH (u) + λk(cid:0)S2(u) −(cid:10)u  s1(f k)(cid:11)(cid:1)}.

(4)
For simplicity we restrict ourselves to submodular balancing functions  in which case S is convex
and thus S2 = 0. For the general case  see [16]. Note that the balancing functions of ratio/normalized
cut and Cheeger cut are submodular. It turns out that the inner problem is very similar to the semi-
supervised learning formulation (3). The efﬁcient solution of both problems is discussed next.

5 Algorithms for the Total Variation on Hypergraphs

The problem (3) we want to solve for semi-supervised learning and the inner problem (4) of Ra-
tioDCA have a common structure. They are the sum of two convex functions: one of them is the
novel regularizer ΩH p and the other is a data term denoted by G here  cf.  Table 1. We propose
solving these problems using a primal-dual algorithm  denoted by PDHG  which was proposed in
[23  24]. Its main idea is to iteratively solve for each convex term in the objective function a proximal
problem. The proximal map proxg w.r.t. a mapping g : Rn → R is deﬁned by

proxg(˜x) = arg min

x∈Rn

(cid:107)x − ˜x(cid:107)2

2 + g(x)}.

{ 1
2

1A function f : Rd → R is positively 1-homogeneous if ∀α > 0  f (αx) = αf (x).

5

The key idea is that often proximal problems can be solved efﬁciently leading to fast convergence
of the overall algorithm. We see in Table 1 that for both G the proximal problems have an explicit
solution. However  note that smooth convex terms can also be directly exploited [25]. For ΩH p  we
distinguish two cases  p = 1 and p = 2. Detailed descriptions of the algorithms can be found in the
supplementary material.

G(f ) = 1

2(cid:107)f − Y (cid:107)2

2

proxτ G(f )(˜x) = 1

1+τ (˜x + τ Y )

G(f ) = −(cid:104)s1(f k)  f(cid:105) + ι(cid:107)·(cid:107)2≤1(f )
proxτ G(f )(˜x) =
max{1 (cid:107)˜x+τ s1(f k)(cid:107)2}

˜x+τ s1(f k)

Table 1: Data term and proximal map for SSL (3) (left) and the inner problem of RatioDCA (4)
(right).The indicator function is deﬁned as ι(cid:107)·(cid:107)2≤1(x) = 0  if (cid:107)x(cid:107)2 ≤ 1 and +∞ otherwise.

(cid:88)

PDHG algorithm for ΩH 1. Let me be the number of vertices in hyperedge e ∈ E. We write

λΩH 1(f ) = F (Kf ) :=

(5)
where the rows of the matrices Ke ∈ Rme n are the i-th standard unit vectors for i ∈ e and the
functionals F(e j) : Rme → R are deﬁned as

(F(e 1)(Kef ) + F(e 2)(Kef )) 

e∈E

F(e 1)(α(e 1)) = λwe max(α(e 1))  F(e 2)(α(e 2)) = −λwe min(α(e 2)).

In contrast to the function G  we need in the PDHG algorithm the proximal maps for the conjugate
functions of F(e j). They are given by

where Sλwe = {x ∈ Rme : (cid:80)me

F ∗
(e 1) = ιSλwe
i=1 xi = λwe  xi ≥ 0} is the scaled simplex in Rme. The solutions
(e 1) and F ∗
(e 1) are the orthogonal projections onto the simplexes
  respectively. These projections can be done in linear time [26].

of the proximal problem for F ∗
written here as PSe
and P−Se
With the proximal maps we have presented so far  the PDHG algorithm has the following form.

(e 2) = ι−Sλwe

  F ∗

λwe

λwe

 

Algorithm 2 PDHG for ΩH 1
1: Initialization: f (0) = ¯f (0) = 0  θ ∈ [0  1]  σ  τ > 0 with στ < 1/(2 maxi=1 ... n{ci})
2: repeat
3:

e∈E Hi e is the number of hyperedges the vertex i lies in. It is important to
point out here that the algorithm decouples the problem in the sense that in every iteration we
solve subproblems which treat the functionals G  F(e 1)  F(e 2) separately and thus can be solved
efﬁciently.

PDHG algorithm for ΩH 2. We deﬁne the matrices Ke as above. Moreover  we introduce for
every hyperedge e ∈ E the functional

Hence  we can write ΩH 2(f ) =(cid:80)

(6)
e∈E Fe(Kef ). As we show in the supplementary material  the
e are not indicator functions and we thus solve the corresponding proximal

conjugate functions F ∗
problems via proximal problems for Fe. More speciﬁcally  we exploit the fact that

Fe(αe) = λwe(max(αe) − min(αe))2.

proxσF ∗

e

(˜αe) = ˜αe − prox 1

σ Fe (˜αe) 

(7)

and use the following novel result concerning the proximal problem on the right-hand side of (7).

6

¯f (k)) 
¯f (k)) 

e ∈ E
e ∈ E

e (α(e 1)(k+1)

+ α(e 2)(k+1)

))

λwe

λwe

4:

+ σKe

α(e 1)(k+1)
α(e 2)(k+1)

(α(e 1)(k)
(α(e 2)(k)

= PSe
= P−Se

f (k+1) = proxτ G(f (k) − τ(cid:80)

+ σKe
e∈E KT
¯f (k+1) = f (k+1) + θ(f (k+1) − f (k))

5:
6:
7: until relative duality gap < 
8: Output: f (k+1).

The value ci = (cid:80)

Prop. \ Dataset
Number of classes
|V |
|E|
|E| of Clique Exp.

(cid:80)
e∈E |e|

Zoo
7
101
42
1717
10201

Mushrooms Covertype (4 5) Covertype (6 7)
2
8124
112
170604
65999376

2
37877
123
454522
1348219153

2
12240
104
146880
143008092

20Newsgroups
4
16242
100
65451
53284642

Table 2: Datasets used for SSL and clustering. Note that the clique expansion leads for all datasets
to a graph which is close to being fully connected as all datasets contain large hyperedges. For
covertype (6 7) the weight matrix needs over 10GB of memory  the original hypergraph only 4MB.

Proposition 5.1. For any σ > 0 and any ˜αe ∈ Rme the proximal map

prox 1

σ Fe (˜αe) = arg min
αe∈Rme

(cid:107)αe − ˜αe(cid:107)2

2 +

{ 1
2

1
σ

λwe(max(αe) − min(αe))2}

can be computed with O(me log me) arithmetic operations.
A corresponding algorithm which is new to the best of our knowledge is provided in the supplemen-
tary material. We note here that the complexity is due to the fact that we sort the input vector ˜αe.
The PDHG algorithm for p = 2 is provided in the supplementary material. It has the same structure
as Algorithm 2 with the only difference that we now solve (7) for every hyperedge.

6 Experiments

The method of Zhou et al [11] seems to be the standard algorithm for clustering and SSL on hyper-
graphs. We compare to them on a selection of UCI datasets summarized in Table 2. Zoo  Mushrooms
and 20Newsgroups2 have been used also in [11] and contain only categorical features. As in [11] 
a hyperedge of weight one is created by all data points which have the same value of a categorical
feature. For covertype we quantize the numerical features into 10 bins of equal size. Two datasets
are created each with two classes (4 5 and 6 7) of the original dataset.

In [11]  they suggest using a regularizer induced by the nor-

Semi-supervised Learning (SSL).
malized Laplacian LCE arising from the clique expansion

where DCE is a diagonal matrix with entries dEC(i) = (cid:80)

− 1
CEHW (cid:48)H T D

LCE = I − D

2

− 1
CE 

2

diagonal matrix with entries w(cid:48)(e) = we/|e|. The SSL problem can then be formulated as

e∈E Hi e

we|e| and W (cid:48) ∈ R|E|×|E| is a

λ > 0 

arg minf∈R|V |{(cid:107)f − Y (cid:107)2

2 + λ(cid:104)f  LCEf(cid:105)}.

which needs 2(cid:80)

The advantage of this formulation is that the solution can be found via a linear system. However  as
Table 2 indicates the obvious downside is that LCE is a potentially very dense matrix and thus one
needs in the worst case |V |2 memory and O(|V |3) computations. This is in contrast to our method
e∈E |e| + |V | memory. For the largest example (covertype 6 7)  where the clique
expansion fails due to memory problems  our method takes 30-100s (depending on λ). We stop our
method for all experiments when we achieve a relative duality gap of 10−6. In the experiments we
do 10 trials for different numbers of labeled points. The reg. parameter λ is chosen for both methods
from the set 10−k  where k = {0  1  2  3  4  5  6} via 5-fold cross validation. The resulting errors
and standard deviations can be found in the following table(ﬁrst row lists the no. of labeled points).
Our SSL methods based on ΩH p  p = 1  2 outperform consistently the clique expansion technique
of Zhou et al [11] on all datasets except 20newsgroups3. However  20newsgroups is a very difﬁcult
dataset as only 10 267 out of the 16 242 data points are different which leads to a minimum possible
error of 9.6%. A method based on pairwise interaction such as the clique expansion can better deal

2This is a modiﬁed version by Sam Roweis of the original 20 newsgroups dataset available at http:

//www.cs.nyu.edu/˜roweis/data/20news_w100.mat.

3Communications with the authors of [11] could not clarify the difference to their results on 20newsgroups

7

with such label noise as the large hyperedges for this dataset accumulate the label noise. On all
other datasets we observe that incorporating hypergraph structure leads to much better results. As
expected our squared TV functional (p = 2) outperforms slightly the total variation (p = 1) even
though the difference is small. Thus  as ΩH 2 reduces to the standard regularization based on the
graph Laplacian  which is known to work well  we recommend ΩH 2 for SSL on hypergraphs.

Zoo
Zhou et al.
ΩH 1
ΩH 2
Mushr.
Zhou et al.
ΩH 1
ΩH 2
covert45
Zhou et al.
ΩH 1
ΩH 2
covert67
ΩH 1
ΩH 2
20news
Zhou et al.
ΩH 1
ΩH 2

30
40.7± 14.2
2.2 ± 2.1
2.9 ± 2.3

35
29.7 ± 8.8
0.7 ± 1.0
0.9 ± 1.4

50
25.3± 14.4
1.9 ± 3.0
1.6 ± 2.9

20
25
35.1± 17.2
30.3 ± 7.9
2.9 ± 3.0
1.4 ± 2.2
2.3 ± 1.9
1.5 ± 2.4
80
20
40
10.3±2.0
15.5 ± 12.8 10.9±4.4
5.6 ± 1.9
19.5± 10.5
10.8±3.7
6.4 ± 2.7
9.8 ± 4.5
18.4 ± 7.4
80
40
20
16.6±6.4
18.9 ± 4.6
18.3±5.2
7.6 ± 3.5
21.4 ± 0.9
17.6±2.6
16.1 ± 4.1 10.9 ± 4.9 5.9 ± 3.7
20.7 ± 2.0
20
40
40.6 ± 8.9
6.4±10.4
25.2 ± 18.3 4.3 ± 9.6
20
45.5 ± 7.5
65.7 ± 6.1
55.0 ± 4.8

200
9.3 ± 1.0
5.6 ± 3.8
3.0 ± 0.6
200
20.4±2.9
1.5 ± 1.3
1.0 ± 1.1
200
1.2 ± 0.9
1.1 ± 0.8
40
200
34.4 ± 3.1 31.5 ± 1.4 29.8 ± 4.0 27.0 ± 1.3 27.3 ± 1.5 25.7 ± 1.4 25.0 ± 1.3
61.4±6.1
34.7±3.6
48.0±6.0
34.1±2.0

100
9.0 ± 4.5
5.7 ± 2.2
6.3 ± 2.5
100
17.6±5.2
6.2 ± 3.8
4.6 ± 3.4
100
1.8 ± 0.8
1.4 ± 1.1
100
42.4±3.3
38.3±2.7

40
32.9± 16.8
0.7 ± 1.5
0.8 ± 1.7
120
8.8 ± 1.4
5.4 ± 2.4
4.5 ± 1.8
120
18.4±5.1
4.5 ± 3.6
3.3 ± 3.1
120
1.3 ± 0.9
1.0 ± 0.8
120
40.9±3.2
38.1±2.6

45
27.6± 10.8
0.9 ± 1.4
1.2 ± 1.8
160
8.8 ± 2.3
4.9 ± 3.8
4.4 ± 2.1
160
19.2±4.0
2.6 ± 1.6
2.2 ± 1.8
160
0.9 ± 0.4
0.7 ± 0.4
160
36.1±1.5
35.0±2.8

80
3.3 ± 2.5
2.2 ± 1.4
80
46.2±3.7
40.3±3.0

60
9.5 ± 2.7
7.4 ± 3.8
9.9 ± 5.5
60
17.2±6.7
12.6±4.3

60
3.6 ± 3.2
2.1 ± 2.0
60
53.2±5.7
45.0±5.9

Test error and standard deviation of the SSL methods over 10 runs for varying number of labeled

points.

Clustering. We use the normalized hypergraph cut as clustering objective. For more than two
clusters we recursively partition the hypergraph until the desired number of clusters is reached.
For comparison we use the normalized spectral clustering approach based on the Laplacian LCE
[11](clique expansion). The ﬁrst part (ﬁrst 6 columns) of the following table shows the clustering
errors (majority vote on each cluster) of both methods as well as the normalized cuts achieved by
these methods on the hypergraph and on the graph resulting from the clique expansion. Moreover 
we show results (last 4 columns) which are obtained based on a kNN graph (unit weights) which
is built based on the Hamming distance (note that we have categorical features) in order to check if
the hypergraph modeling of the problem is actually useful compared to a standard similarity based
graph construction. The number k is chosen as the smallest number for which the graph becomes
connected and we compare results of normalized 1-spectral clustering [14] and the standard spectral
clustering [22]. Note that the employed hypergraph construction has no free parameter.

Dataset
Mushrooms
Zoo
20-newsgroup
covertype (4 5)
covertype (6 7)

Clustering Error %
Ours
10.98
16.83
47.77
22.44
8.16

[11]
32.25
15.84
33.20
22.44

-

Hypergraph Ncut
[11]
Ours
0.0013
0.0011
0.6784
0.6739
0.0176
0.0303
0.0018
0.0022
8.18e-4

-

Graph(CE) Ncut
[11]
Ours
0.7053
0.6991
5.1703
5.1315
2.3846
1.8492
0.7400
0.6691
0.6882

-

Clustering Error %
[14]
48.2
5.94
66.38
22.44
45.85

[22]
48.2
5.94
66.38
22.44
45.85

kNN-Graph Ncut
[22]
[14]
1e-4
1e-4
1.636
1.636
0.1031
0.1034
0.02182
0.0152
0.0041
0.0041

First  we observe that our approach optimizing the normalized hypergraph cut yields better or similar
results in terms of clustering errors compared to the clique expansion (except for 20-newsgroup for
the same reason given in the previous paragraph). The improvement is signiﬁcant in case of Mush-
rooms while for Zoo our clustering error is slightly higher. However  we always achieve smaller
normalized hypergraph cuts. Moreover  our method sometimes has even smaller cuts on the graphs
resulting from the clique expansion  although it does not directly optimize this objective in contrast
to [11]. Again  we could not run the method of [11] on covertype (6 7) since the weight matrix is
very dense. Second  the comparison to a standard graph-based approach where the similarity struc-
ture is obtained using the Hamming distance on the categorical features shows that using hypergraph
structure is indeed useful. Nevertheless  we think that there is room for improvement regarding the
construction of the hypergraph which is a topic for future research.

Acknowledgments
M.H. would like to acknowledge support by the ERC Starting Grant NOLEPRO and L.J. acknowl-
edges support by the DFG SPP-1324.

8

References
[1] Y. Huang  Q. Liu  and D. Metaxas. Video object segmentation by hypergraph cut. In CVPR  pages 1738

– 1745  2009.

[2] P. Ochs and T. Brox. Higher order motion models and spectral clustering. In CVPR  pages 614–621 

2012.

[3] S. Klamt  U.-U. Haus  and F. Theis. Hypergraphs and cellular networks. PLoS Computational Biology 

5:e1000385  2009.

[4] Z. Tian  T. Hwang  and R. Kuang. A hypergraph-based learning algorithm for classifying gene expression

and arraycgh data with prior knowledge. Bioinformatics  25:2831–2838  2009.

[5] D. Gibson  J. Kleinberg  and P. Raghavan. Clustering categorical data: an approach based on dynamical

systems. VLDB Journal  8:222–236  2000.

[6] J. Bu  S. Tan  C. Chen  C. Wang  H. Wu  L. Zhang  and X. He. Music recommendation by uniﬁed hyper-
graph: Combining social media information and music content. In Proc. of the Int. Conf. on Multimedia
(MM)  pages 391–400  2010.

[7] A. Shashua  R. Zass  and T. Hazan. Multi-way clustering using super-symmetric non-negative tensor

factorization. In ECCV  pages 595–608  2006.

[8] S. Rota Bulo and M. Pellilo. A game-theoretic approach to hypergraph clustering. In NIPS  pages 1571–

1579  2009.

[9] M. Leordeanu and C. Sminchisescu. Efﬁcient hypergraph clustering. In AISTATS  pages 676–684  2012.
[10] S. Agarwal  J. Lim  L. Zelnik-Manor  P. Petrona  D. J. Kriegman  and S. Belongie. Beyond pairwise

clustering. In CVPR  pages 838–845  2005.

[11] D. Zhou  J. Huang  and B. Sch¨olkopf. Learning with hypergraphs: Clustering  classiﬁcation  and embed-

ding. In NIPS  pages 1601–1608  2006.

[12] S. Agarwal  K. Branson  and S. Belongie. Higher order learning with graphs. In ICML  pages 17–24 

2006.

[13] E. Ihler  D. Wagner  and F. Wagner. Modeling hypergraphs by graphs with the same mincut properties.

Information Processing Letters  45:171–175  1993.

[14] M. Hein and T. B¨uhler. An inverse power method for nonlinear eigenproblems with applications in 1-

spectral clustering and sparse PCA. In NIPS  pages 847–855  2010.

[15] A. Szlam and X. Bresson. Total variation and Cheeger cuts. In ICML  pages 1039–1046  2010.
[16] M. Hein and S. Setzer. Beyond spectral clustering - tight relaxations of balanced graph cuts. In NIPS 

pages 2366–2374  2011.

[17] T. B¨uhler  S. Rangapuram  S. Setzer  and M. Hein. Constrained fractional set programs and their applica-

tion in local clustering and community detection. In ICML  pages 624–632  2013.

[18] F. Bach. Learning with submodular functions: A convex optimization perspective. CoRR  abs/1111.6453 

2011.

[19] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. Machine Learning  56:209–239  2004.
[20] D. Zhou  O. Bousquet  T. N. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global consistency.

In NIPS  volume 16  pages 321–328  2004.

[21] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Patt. Anal. Mach. Intell. 

22(8):888–905  2000.

[22] U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing  17:395–416  2007.
[23] E. Esser  X. Zhang  and T. F. Chan. A general framework for a class of ﬁrst order primal-dual algorithms
for convex optimization in imaging science. SIAM Journal on Imaging Sciences  3(4):1015–1046  2010.
[24] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to

imaging. J. of Math. Imaging and Vision  40:120–145  2011.

[25] L. Condat. A primaldual splitting method for convex optimization involving lipschitzian  proximable and

linear composite terms. J. Optimization Theory and Applications  158(2):460–479  2013.

[26] K. Kiwiel. On Linear-Time algorithms for the continuous quadratic knapsack problem. J. Opt. Theory

Appl.  134(3):549–554  2007.

9

,Matthias Hein
Simon Setzer
Leonardo Jost
Syama Sundar Rangapuram
Tim Salimans
Ian Goodfellow
Wojciech Zaremba
Vicki Cheung
Alec Radford
Xi Chen
Chang Xiao
Peilin Zhong
Changxi Zheng