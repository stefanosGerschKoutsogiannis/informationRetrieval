2019,Surfing: Iterative Optimization Over Incrementally Trained Deep Networks,We investigate a sequential optimization procedure to minimize the empirical risk functional $f_{\hat\theta}(x) = \frac{1}{2}\|G_{\hat\theta}(x) - y\|^2$ for certain families of deep networks $G_{\theta}(x)$. The approach is to optimize a sequence of objective functions that use network parameters obtained during different stages of the training process.  When initialized with random parameters $\theta_0$  we show that the objective  $f_{\theta_0}(x)$ is ``nice'' and easy to optimize with gradient descent. As learning is carried out  we obtain a sequence of generative networks $x \mapsto G_{\theta_t}(x)$ and associated risk functions $f_{\theta_t}(x)$  where $t$ indicates a stage of stochastic gradient descent during training.  Since the parameters of the network do not change by very much in each step  the surface evolves slowly and can be incrementally optimized. The algorithm is formalized and analyzed for a family of expansive networks. We call the procedure {\it surfing} since it rides along the peak of the evolving (negative) empirical risk function  starting from a smooth surface at the beginning of learning and ending with a wavy nonconvex surface after learning is complete.  Experiments show how surfing can be used to find the global optimum and for compressed sensing even when direct gradient descent on the final learned network fails.,Surﬁng: Iterative Optimization Over
Incrementally Trained Deep Networks

Ganlin Song

Zhou Fan

Department of Statistics and Data Science

Department of Statistics and Data Science

Yale University

ganlin.song@yale.edu

Yale University

zhou.fan@yale.edu

John Lafferty

Department of Statistics and Data Science

Yale University

john.lafferty@yale.edu

Abstract

We investigate a sequential optimization procedure to minimize the empirical
risk functional fbθ(x) = 1
2 kGbθ(x) − yk2 for certain families of deep networks
Gθ(x). The approach is to optimize a sequence of objective functions that use
network parameters obtained during different stages of the training process. When
initialized with random parameters θ0  we show that the objective fθ0 (x) is “nice”
and easy to optimize with gradient descent. As learning is carried out  we obtain a
sequence of generative networks x 7→ Gθt (x) and associated risk functions fθt (x) 
where t indicates a stage of stochastic gradient descent during training. Since the
parameters of the network do not change by very much in each step  the surface
evolves slowly and can be incrementally optimized. The algorithm is formalized
and analyzed for a family of expansive networks. We call the procedure surﬁng
since it rides along the peak of the evolving (negative) empirical risk function 
starting from a smooth surface at the beginning of learning and ending with a wavy
nonconvex surface after learning is complete. Experiments show how surﬁng can
be used to ﬁnd the global optimum and for compressed sensing even when direct
gradient descent on the ﬁnal learned network fails.

1

Introduction

Intensive recent research has provided insight into the performance and mathematical properties of
deep neural networks  improving understanding of their strong empirical performance on different
types of data. Some of this work has investigated gradient descent algorithms that optimize the
weights of deep networks during learning (Du et al.  2018b a; Davis et al.  2018; Li and Yuan  2017;
Li and Liang  2018). In this paper we focus on optimization over the inputs to an already trained deep
network in order to best approximate a target data point. Speciﬁcally  we consider the least squares
objective function

fbθ(x) =

1
2

kGbθ(x) − yk2

where Gθ(x) denotes a multi-layer feed-forward network and bθ denotes the parameters of the network
after training. The network is considered to be a mapping from a latent input x ∈ Rk to an output
Gθ(x) ∈ Rn with k ≪ n. A closely related objective is to minimize fθ A(x) = 1
2 kAGθ(x) − Ayk2
where A is a random matrix.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

initial network

partially trained network

fully trained network

target y

−4

0

x1

4

−4

4

0

x2

−4

0

x1

4

−4

4

0

x2

−4

0

x1

4

−4

4

0

x2

−4

0

x1

4

−4

4

0

x2

−4

0

x1

4

−4

4

0

x2

−4

0

x1

4

−4

4

0

x2

Figure 1: Behavior of the surfaces x 7→ − 1
2 kGθt (x) − yk2 for two targets y shown for three levels
of training from random networks (left) to fully trained networks (right) on Fashion MNIST data.
The network structure has two fully connected layers and two transposed convolution layers with
batch normalization  trained as a VAE.

Hand and Voroninski (2019) study the behavior of the function fθ0 A in a compressed sensing frame-
work where y = Gθ0 (x0) is generated from a random network with parameters θ0 = (W1  . . .   Wd)
drawn from Gaussian matrix ensembles; thus  the network is not trained. In this setting  it is shown
that the surface is very well behaved. In particular  outside of small neighborhoods around x0 and a
scalar multiple of −x0  the function fθ0 A(x) always has a descent direction.
When the parameters of the network are trained  the landscape of the function fbθ(x) can be compli-
cated; it will in general be nonconvex with multiple local optima. Figure 1 illustrates the behavior of
the surfaces as they evolve from random networks (left) to fully trained networks (right) for 4-layer
networks trained on Fashion MNIST using a variational autoencoder. For each of two target values y 
three surfaces x 7→ − 1

2 kGθt (x) − yk2 are shown for different levels of training.

This paper explores the following simple idea. We incrementally optimize a sequence of objective

functions fθ0   fθ1   . . .   fθT where the parameters θ0  θ1  . . .   θT = bθ are obtained using stochastic
gradient descent in θ during training. When initialized with random parameters θ0  we show that
the empirical risk function fθ0 (x) = 1
2 kGθ0 (x) − yk2 is “nice” and easy to optimize with gradient
descent. As learning is carried out  we obtain a sequence of generative networks x 7→ Gθt (x) and
associated risk functions fθt (x)  where t indicates an intermediate stage of stochastic gradient descent
during training. Since the parameters of the network do not change by very much in each step (Du
et al.  2018a b)  the surface evolves slowly. We initialize x for the current network Gθt (x) at the
optimum x∗
t−1 found for the previous network Gθt−1 (x) and then carry out gradient descent to obtain
the updated point x∗

t = argminx fθt (x).

We call this process surﬁng since it rides along the peaks of the evolving (negative) empirical
risk function  starting from a smooth surface at the beginning of learning and ending with a wavy
nonconvex surface after learning is complete. We formalize this algorithm in a manner that makes it
amenable to analysis. First  when θ0 is initialized so that the weights are random Gaussian matrices 
we prove a theorem showing that the surface has a descent direction at each point outside of a small
neighborhood. The analysis of Hand and Voroninski (2019) does not directly apply in our case since
the target y is an arbitrary test point  and not necessarily generated according to the random network.
We then give an analysis that describes how projected gradient descent can be used to proceed from
the optimum of one network to the next. Our approach is based on the fact that the ReLU network
and squared error objective result in a piecewise quadratic surface. Experiments are run to show
how surﬁng can be used to ﬁnd the global optimum and for compressed sensing even when direct
gradient descent fails  using several experimental setups with networks trained with both VAE and
GAN techniques.

2

2 Background and Previous Results

In this work we treat the problem of approximating an observed vector y in terms of the output Gbθ(x)
of a trained generative model. Traditional generative processes such as graphical models are statistical
models that deﬁne a distribution over a sample space. When deep networks are viewed as generative
models  the distribution is typically singular  being a deterministic mapping of a low-dimensional
latent random vector to a high-dimensional output space. Certain forms of “reversible deep networks”
allow for the computation of densities and inversion (Dinh et al.  2017; Kingma and Dhariwal  2018;
Chen et al.  2018).

The variational autoencoder (VAE) approach training a generative (decoder) network is to model
the conditional probability of x given y as Gaussian with mean µ(y) and covariance Σ(y) assuming
that a priori x ∼ N (0  Ik) is Gaussian. The mean and covariance are treated as the output of a
secondary (encoder) neural network. The two networks are trained by maximizing the evidence
lower bound (ELBO) with coupled gradient descent algorithms—one for the encoder network  the
other for the decoder network Gθ(x) (Kingma and Welling  2014). Whether ﬁtting the networks
using a variational or GAN approach (Goodfellow et al.  2014; Arjovsky et al.  2017)  the problem of
“inverting” the network to obtain x∗ = argmin fθ(x) is not addressed by the training procedure.

In the now classical compressed sensing framework (Candes et al.  2006; Donoho et al.  2006)  the
problem is to reconstruct a sparse signal after observing multiple linear measurements  possibly with
added noise. More recent work has begun to investigate generative deep networks as a replacement
for sparsity in compressed sensing. Bora et al. (2017) consider identifying y = G(x0) from linear
measurements Ay by optimizing f (x) = 1
2 kAy − AG(x)k2. Since this objective is nonconvex 
it is not guaranteed that gradient descent will converge to the true global minimum. However 
for certain classes of ReLU networks it is shown that so long as a point bx is found for which
f (bx) is sufﬁciently close to zero  then ky − G(bx)k is also small. For the case where y does
not lie in the image of G  an oracle type bound is shown implying that the solution bx satisﬁes
kG(bx) − yk2 ≤ C inf x kG(x) − yk2 + δ for some small error term δ. The authors observe that in
experiments the error seems to converge to zero when bx is computed using simple gradient descent;

but an analysis of this phenomenon is not provided.

Hand and Voroninski (2019) establish the important result that for a d-layer random network and
random measurement matrix A  the least squares objective has favorable geometry  meaning that
outside two small neighborhoods there are no ﬁrst order stationary points  neither local minima nor
saddle points. We describe their setup and result in some detail  since it provides a springboard for
the surﬁng algorithm.
Let G : Rk → Rn be a d-layer fully connected feedforward generative neural network  which has the
form

G(x) = σ(Wd...σ(W2σ(W1x))...)

where σ is the ReLU activation function. The matrix Wi ∈ Rni×ni−1 is the set of weights for the ith
layer and ni is number of the neurons in this layer with k = n0 < n1 < ... < nd = n. If x0 ∈ Rk
is the input then AG(x0) is a set of random linear measurements of the signal y = G(x0). The
objective is to minimize fA θ0 (x) = 1
where θ0 = (W1  . . .   Wd) is the
set of weights.

2(cid:13)(cid:13)AGθ0 (x) − AGθ0 (x0)(cid:13)(cid:13)2

Due to the fact that the nonlinearities σ are rectiﬁed linear units  Gθ0 (x) is a piecewise linear function.
It is convenient to introduce notation that absorbs the activation σ into weight matrix Wi  denoting

W+ x = diag(W x > 0)W.

For a ﬁxed W   the matrix W+ x zeros out the rows of W that do not have a positive dot product with
x; thus  σ(W x) = W+ xx. We further deﬁne W1 + x = diag(W1x > 0) W1 and

Wi + x = diag(WiWi−1 + x...W1 + xx > 0) Wi.

With this notation  we can rewrite the generative network Gθ0 in what looks like a linear form 

noting that each matrix Wi + x depends on the input x.

Gθ0 (x) = Wd + xWd−1 + x...W1 + xx 

3

If fA θ0 (x) is differentiable at x  we can write the gradient as

∇fA θ0 (x) = (cid:16) 1Y

i=d

Wi + x(cid:17)T

AT A(cid:16) 1Y

i=d

Wi + x(cid:17)x −(cid:16) 1Y

i=d

Wi + x(cid:17)T

AT A(cid:16) 1Y

i=d

Wi + x0(cid:17)x0.

In this expression  one can see intuitively that under the assumption that A and Wi are Gaussian
matrices  the gradient ∇fθ0 (x) should concentrate around a deterministic vector vx x0 . Hand and
Voroninski (2019) establish sufﬁcient conditions for concentration of the random matrices around
deterministic quantities  so that vx x0 has norm bounded away from zero if x is sufﬁciently far from
x0 or a scalar multiple of −x0. Their results show that for random networks having a sufﬁciently
expansive number of neurons in each layer  the objective fA θ0 has a landscape favorable to gradient
descent.

We build on these ideas  showing ﬁrst that optimizing with respect to x for a random network and
arbitrary signal y can be done with gradient descent. This requires modiﬁed proof techniques  since
it is no longer assumed that y = Gθ0 (x0). In fact  y can be arbitrary and we wish to approximate
it as Gbθ(x(y)) for some x(y). Second  after this initial optimization is carried out  we show how
projected gradient descent can be used to track the optimum as the network undergoes a series of
small changes. Our results are stated formally in the following section.

3 Theoretical Results

Suppose we have a sequence of networks G0  G1  . . .   GT generated from the training process. For
instance  we may take a network with randomly initialized weights as G0  and record the network
after each step of gradient descent in training; GT = G is the ﬁnal trained network.
For a given vector y ∈ Rn  we wish to minimize the
objective f (x) = 1
2 kAG(x) − Ayk2 with respect
to x for the ﬁnal network G  where either A = I ∈
Rn×n  or A ∈ Rm×n is a measurement matrix with
i.i.d. N (0  1/m) entries in a compressed sensing
context. Write
1
2

Algorithm 1 Surﬁng
Input: Sequence of networks θ0  θ1  . . .   θT
1: x−1 ← 0
2: for t = 0 to T do
3:
4:
5:
6:
7:
Output: xT

The idea is that we ﬁrst minimize f0  which has a
nicer landscape  to obtain the minimizer x0. We
then apply gradient descent on ft for t = 1  2  ...  T
successively  starting from the minimizer xt−1 for the previous network.

until convergence
xt ← x

x ← x − η∇fθt (x)

ft(x) =

kAGt(x) − Ayk2 

∀ t ∈ [T ].

(1)

x ← xt−1
repeat

We provide some theoretical analysis in partial support of this algorithmic idea. First  we show that at
random initialization G0  all critical points of f0(x) are localized to a small ball around zero. Second 
we show that if G0  . . .   GT are obtained from a discretization of a continuous ﬂow  along which the
global minimizer of ft(x) is unique and Lipschitz-continuous  then a projected-gradient version of
surﬁng can successively ﬁnd the minimizers for G1  . . .   GT starting from the minimizer for G0.
We consider expansive feedforward neural networks G : Rk × Θ 7→ Rn given by

G(x  θ) = V σ(Wd . . . σ(W2σ(W1x + b1) + b2) . . . + bd).

Here  d is the number of intermediate layers (which we will treat as constant)  σ is the ReLU
activation function σ(x) = max(x  0) applied entrywise  and θ = (V  W1  ...  Wd  b1  ...  bd) are the
network parameters. The input dimension is k ≡ n0  each intermediate layer i ∈ [d] has weights
Wi ∈ Rni×ni−1 and biases bi ∈ Rni   and a linear transform V ∈ Rn×nd is applied in the ﬁnal layer.
For our ﬁrst result  consider ﬁxed y ∈ Rn and a random initialization G0(x) ≡ G(x  θ0) where θ0
has Gaussian entries (independent of y). If the network is sufﬁciently expansive at each intermediate
layer  then the following shows that with high probability  all critical points of f0(x) belong to a
small ball around 0. More concretely  the directional derivative D−x/kxkf0(x) satisﬁes

D−x/kxkf0(x) ≡ lim
t→0+

f0(x − tx/kxk) − f0(x)

t

< 0.

(2)

Thus −x/kxk is a ﬁrst-order descent direction of the objective f0 at x.

4

Theorem 3.1. Fix y ∈ Rn. Let V have N (0  1/n) entries  let bi and Wi have N (0  1/ni) entries for
each i ∈ [d]  and suppose these are independent. There exist d-dependent constants C  C ′  c  ε0 > 0
such that for any ε ∈ (0  ε0)  if

1. n ≥ nd and ni > C(ε−2 log ε−1)ni−1 log ni for all i ∈ [d]  and

2. Either A = I and m = n  or A ∈ Rm×n has i.i.d. N (0  1/m) entries (independent of

V  {bi}  {Wi}) where m ≥ Ck(ε−1 log ε−1) log(n1 . . . nd) 

then with probability at least 1 − C(e−cεm + nde−cε4nd−1 + Pd−1

outside the ball kxk ≤ C ′ε(1 + kyk) satisﬁes (2).

i=1 nie−cε2ni−1 )  every x ∈ Rk

We defer the proof to the supplementary material. Note that if instead G0 were correlated with y  say
y = G0(x∗) for some input x∗ with kx∗k ≍ 1  then x∗ would be a global minimizer of f0(x)  and
we would have kyk ≍ kxdk ≍ . . . ≍ kx1k ≍ kx∗k ≍ 1 in the above network where xi ∈ Rni is the
output of the ith layer. The theorem shows that for a random initialization of G0 which is independent
of y  the minimizer is instead localized to a ball around 0 which is smaller in radius by the factor ε.

For our second result  consider a network ﬂow

Gs(x) ≡ G(x  θ(s))

for s ∈ [0  S]  where θ(s) = (V (s)  W1(s)  b1(s)  . . .   Wd(s)  bd(s)) evolve continuously in a time
parameter s. As a model for network training  we assume that G0  . . .   GT are obtained by discrete
sampling from this ﬂow via Gt = Gδt  corresponding to s ≡ δt for a small time discretization step δ.

We assume boundedness of the weights and uniqueness and Lipschitz-continuity of the global
minimizer along this ﬂow.

Assumption 3.2. There are constants M  L < ∞ such that

1. For every i ∈ [d] and s ∈ [0  S] 

kWi(s)k ≤ M.

2. The global minimizer x∗(s) = argminx f (x  θ(s)) is unique and satisﬁes

kx∗(s) − x∗(s′)k ≤ L|s − s′|

where f (x  θ(s)) = 1

2 kAG(x  θ(s)) − Ayk2.

Fixing θ  the function G(x  θ) is continuous and piecewise-linear in x. For each x ∈ Rk  there is at
least one linear piece P0 (a polytope in Rk) of this function that contains x. For a slack parameter
τ > 0  consider the rows given by

where

S(x  θ  τ ) = {(i  j) : |w⊤

i jxi−1 + bi j| ≤ τ } 

xi−1 = σ(Wi−1 . . . σ(W1x + b1) . . . + bi−1)

is the output of the (i − 1)th layer for this input x  and v⊤
i j   and bi j are respectively the jth
row of V   the jth row of Wi and the jth entry of bi in θ. This set S(x  θ  τ ) represents those neurons
that are close to 0 before ReLU thresholding  and hence whose activations may change after a small
change of the network input x. Deﬁne

j   w⊤

P(x  θ  τ ) = {P0  P1  . . .   PG}

as the set of all linear pieces Pg whose activation patterns differ from P0 only in rows belonging to
S(x  θ  τ ). That is  for every x′ ∈ Pg ∈ P(x  θ  τ ) and (i  j) /∈ S(x  θ  τ )  we have

sign(w⊤

i jxi−1 + bi j)

i jx′

i−1 + bi j) = sign(w⊤

where x′

i−1 is the output of the (i − 1)th layer for input x′.

With this deﬁnition  we consider a stylized projected-gradient surﬁng procedure in Algorithm 2 
where ProjP is the orthogonal projection onto the polytope P .

5

Algorithm 2 Projected-gradient Surﬁng
Input: Network ﬂow {G(·  θ(s)) : s ∈ [0  S]}  parameters δ  τ  η > 0.
1: Initialize x0 = argminx f (x  θ(0)).
2: for t = 1  . . .   T do
3:
4:
5:
6:

for each linear piece Pg ∈ P(xt−1  θ(δt)  τ ) do

x ← ProjPg (x − η∇f (x  θ(δt)))

x ← xt−1
repeat

7:

8:

until convergence
x(g)
t ← x

xt ← x(g)

t

9:
Output: xT

for g ∈ {0  . . .   G} that achieves the minimum value of f (x(g)

  θ(δt)).

t

The complexity of this algorithm depends on the number of pieces G to be optimized over in each
step. We expect this to be small in practice when the slack parameter τ is chosen sufﬁciently small 
and provide a heuristic argument in the supplement indicating why this may be the case.

The following shows that for any τ > 0  there is a sufﬁciently ﬁne time discretization δ depending
on τ  M  L such that Algorithm 2 tracks the global minimizer. In particular  for the ﬁnal objective
fT (x) = f (x  θ(δT )) corresponding to the network GT   the output xT is the global minimizer of
fT (x). We remark that the time discretization δ may need to be smaller for deeper networks  as G(x)
corresponding to a deeper network may have a larger Lipschitz constant in x. The speciﬁc dependence
i=1 kWik  which is a conservative bound

below arises from bounding this Lipschitz constant byQd

also used and discussed in greater detail in Szegedy et al. (2014); Virmaux and Scaman (2018).
Theorem 3.3. Suppose Assumption 3.2 holds. For any τ > 0  if δ < τ /(L max(M  1)d+1) and
x0 = argminx f (x  θ(0))  then the iterates xt in Algorithm 2 are given by xt = argminx f (x  θ(δt))
for each t = 1  . . .   T .

Proof. For any ﬁxed θ  let x  x′ ∈ Rk be two inputs to G(x  θ). If xi  x′
i are the corresponding
outputs of the ith layer  using the assumption kWik ≤ M and the fact that the ReLU activation σ is
1-Lipschitz  we have

kxi − x′

ik = kσ(Wixi−1 + bi) − σ(Wix′

i−1 + bi)k

≤ k(Wixi−1 + bi) − (Wix′
≤ M kxi−1 − x′

i−1k ≤ . . . ≤ M ikx − x′k.

i−1 + bi)k

Let x∗(s) = argminx f (x  θ(s)). By assumption  kx∗(s − δ) − x∗(s)k ≤ Lδ. For the network with
parameter θ(s) at time s  let x∗ i(s) and x∗ i(s − δ) be the outputs at the ith layer corresponding to
inputs x∗(s) and x∗(s − δ). Then for any i ∈ [d] and j ∈ [ni]  the above yields

|(wi j(s)⊤x∗ i(s − δ) + bi j) − (wi j(s)⊤x∗ i(s) + bi j)| ≤ kwi j(s)kkx∗ i(s − δ) − x∗ i(s)k

≤ M · M ikx∗(s − δ) − x∗(s)k ≤ M i+1Lδ.

For δ < τ /(L max(M  1)d+1)  this implies that for every (i  j) where |wi j(s)⊤x∗ i(s−δ)+bi j| ≥ τ  
we have

sign(wi j(s)⊤x∗ i(s − δ) + bi j) = sign(wi j(s)⊤x∗ i(s) + bi j).

That is  x∗(s) ∈ Pg for some Pg ∈ P(x∗(s − δ)  θ(s)  τ ).
Assuming that xt−1 = x∗(δ(t − 1))  this implies that the next global minimizer x∗(δt) belongs to
some Pg ∈ P(xt−1  θ(δt)  τ ). Since f (x  θ(δt)) is quadratic on Pg  projected gradient descent over
Pg in Algorithm 2 converges to x∗(δt)  and hence Algorithm 2 yields xt = x∗(δt). The result then
follows from induction on t.

4 Experiments

We present experiments to illustrate the performance of surﬁng over a sequence of networks during
training compared with gradient descent over the ﬁnal trained network. We mainly use the Fashion-

6

Input dimension

5

% successful

# iterations

% successful

# iterations

Model
Regular Adam 98.7
Surﬁng
100
Regular Adam 737
Surﬁng
775
Model
Regular Adam 56.0
Surﬁng
81.7
Regular Adam 464
Surﬁng
547

10
VAE
100
100
1330
1404
WGAN
84.3
97.3
1227
1450

20

5

10

20

100
100
8215
10744

90.3
99.3
3702
4986

DCGAN

68.7
98.7
4560
6514

80.0
96.3
18937
33294

WGAN-GP

64.7
95.7
1915
2394

64.7
97.3
15445
25991

48.3
78.3
618
741

47.0
83.7
463
564

Table 1: Surﬁng compared against direct gradient descent over the ﬁnal trained network  for various
generative models with input dimensions k = 5  10  20. Shown are percentages of “successful”

solutions bxT satisfying kbxT − x∗k < 0.01  and 75th-percentiles of the total number of gradient
descent steps used (across all networks G0  . . .   GT for surﬁng) until kbxT − x∗k < 0.01 was reached.

1.0

0.8

y
c
n
e
u
q
e
r
f

0.6

0.4

0.2

0.0

0.0

0.5

DCGAN  dim 5

Regular Adam
Surfing

1.0

2.0
distance to the truth

1.5

2.5

3.0

1.0

0.8

y
c
n
e
u
q
e
r
f

0.6

0.4

0.2

0.0

0

DCGAN  dim 10

Regular Adam
Surfing

1
distance to the truth

2

3

1.0

0.8

y
c
n
e
u
q
e
r
f

0.6

0.4

0.2

0.0

0

1

DCGAN  dim 20

Regular Adam
Surfing

2

3

distance to the truth

4

5

Figure 2: Distribution of distance between solution bxT and the truth x∗ for DCGAN trained models 

comparing surﬁng (red) to regular gradient descent (blue) over the ﬁnal network. Both procedures use
Adam in their gradient descent computations. The results indicate that direct descent often succeeds 
but can also converge to a point that is far from the optimum. By moving along the optimum of the
evolving surface  surﬁng is able to move closer to the optimum in these cases.

MNIST dataset to carry out the simulations  which is similar to MNIST in many characteristics  but
is more difﬁcult to train. We build multiple generative models  trained using VAE (Kingma and
Welling  2014)  DCGAN (Radford et al.  2015)  WGAN (Arjovsky et al.  2017) and WGAN-GP
(Gulrajani et al.  2017). The structure of the generator/decoder networks that we use are the same
as those reported by Chen et al. (2016); they include two fully connected layers and two transposed
convolution layers with batch normalization after each layer (Ioffe and Szegedy  2015). We use the
simple surﬁng algorithm in these experiments  rather than the projected-gradient algorithm proposed
for theoretical analysis. Note also that the network architectures do not precisely match the expansive
relu networks used in our analysis. Instead  we experiment with architectures and training procedures
that are meant to better reﬂect the current state of the art.
We ﬁrst consider the problem of minimizing the objective f (x) = 1
2 kG(x) − G(x∗)k2 and recovering
the image generated from a trained network G(x) = GθT (x) with input x∗. We run surﬁng by taking
a sequence of parameters θ0  θ1  ...  θT for T = 100  where θ0 are the initial random parameters and
the intermediate θt’s are taken every 40 training steps  and we use Adam (Kingma and Ba  2014) to
carry out gradient descent in x over each network Gθt . We compare this to “regular Adam”  which
uses Adam to optimize over x in only the ﬁnal trained network GθT for T = 100.

To ensure that the runtime of surﬁng is comparable to that of a single initialization of regular Adam 
we do not run Adam until convergence for each intermediate network in surﬁng. Instead  we use
a ﬁxed schedule of iterations for the networks Gθ0   . . .   GθT −1   and run Adam to convergence in
only the ﬁnal network GθT . The total number of iterations for networks Gθ0   . . .   GθT −1 is set as the
75th-percentile of the iteration count required for convergence of regular Adam. These are split across
the networks proportional to a deterministic schedule that allots more steps to the earlier networks
where the landscape of G(x) changes more rapidly  and fewer steps to later networks where this
landscape stabilizes.

7

e
c
n
e
g
r
e
v
n
o
c
 
f
o
 
n
o
i
t
r
o
p
o
r
p

0.8

0.6

0.4

0.2

0.0

10

DCGAN

Regular Adam
Surfing

20 30 4050 70 100

number of measurements

200

e
c
n
e
g
r
e
v
n
o
c
 
f
o
 
n
o
i
t
r
o
p
o
r
p

1.0

0.8

0.6

0.4

0.2

0.0

400

10

WGAN

Regular Adam
Surfing

20 30 4050 70 100

number of measurements

200

400

Figure 3: Compressed sensing setting for exact recovery. As a function of the number of random
measurements m  the lines show the proportion of times surﬁng (red) and regular gradient descent
with Adam (blue) are able to recover the true signal y = G(x)  using DCGAN and WGAN.

r
o
r
r
e
 
n
o
i
t
c
u
r
t
s
n
o
c
e
r

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

10

20

30

DCGAN

Regular Adam
Surfing

40

50

70

100

200

400

number of measurements

r
o
r
r
e
 
n
o
i
t
c
u
r
t
s
n
o
c
e
r

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

10

20

30

WGAN

Regular Adam
Surfing

40

50

70

100

200

400

number of measurements

Figure 4: Compressed sensing setting for approximation  or rate-distortion. As a function of the
number of random measurements m  the box plots summarize the distribution of the per-pixel
reconstruction errors for DCGAN and WGAN trained models  using surﬁng (red) and regular
gradient descent with Adam (blue).

For each network training condition  we apply surﬁng and regular Adam for 300 trials  where in
each trial a randomly generated x∗ and initial point xinit are chosen uniformly from the hypercube

k. The table also shows the 75th-percentile for the total number of gradient descent iterations taken
(across all networks for surﬁng)  verifying that the runtime of surﬁng was typically 1–2x that of

[−1  1]k. Table 1 shows the percentage of trials where the solutions bxT satisfy our criterion for
successful recovery kbxT − x∗k < 0.01  for different models and over three different input dimensions
regular Adam. We also provide the distributions of kbxT − x∗k under each setting: Figure 2 shows
the results for DCGAN  and results for the other models are collected in the supplementary material.
We next consider the compressed sensing problem with objective f (x) = 1
2 kAG(x) − AG(x∗)k2
where A ∈ Rm×n is the Gaussian measurement matrix. We carry out 200 trials for each choice
of number of measurements m. The parameters θt for surﬁng are taken every 100 training steps.
As before  we record the proportion of the solutions that are close to the truth x∗ according to

kbxT − x∗k < 0.01. Figure 3 shows the results for DCGAN and WGAN trained networks with input
dimension k = 20.
Lastly  we consider the objective f (x) = 1
2 kAG(x)−Ayk2  where y is a real image from the hold-out
test data. This can be thought of as a rate-distortion setting  where the error varies as a function of
the number of measurements used. We carry out the same experiments as before and compute the

average per-pixel reconstruction error q 1

the distributions of the reconstruction error as the number of measurements m varies.

n kG(bxT ) − yk2 as in Bora et al. (2017). Figure 4 shows

5 Discussion

This paper has explored the idea of incrementally optimizing a sequence of objective risk functions
obtained for models that are slowly changing during stochastic gradient descent during training.
When initialized with random parameters θ0  we have shown that the empirical risk function fθ0 (x) =

8

1
2 kGθ0 (x) − yk2 is well behaved and easy to optimize. The surﬁng algorithm initializes x for the
current network Gθt (x) at the optimum x∗
t−1 found for the previous network Gθt−1 (x) and then
carries out gradient descent to obtain the updated point x∗
t = argminx fθt (x). Our experiments show
that this scheme has merit  and often signiﬁcantly outperforms direct gradient descent on the ﬁnal
model alone.

On the theoretical side  our main technical result applies and extends ideas of Hand and Voroninski
(2019) to show that for random ReLU networks that are sufﬁciently expansive  the surface of fθ0 (x)
is well-behaved for arbitrary target vectors y. This result may be of independent interest  but it
is essential for the surﬁng algorithm because initially the model is poor  with high approximation
error. The analysis for the incremental scheme uses projected gradient descent  although we ﬁnd
that simple gradient descent works well in practice. The analysis assumes that the argmin over the
surface evolves continuously in training. This assumption is necessary—if the global minimum is
discontinuous as a function of t  so that the minimizer “jumps” to a far away point  then the surﬁng
procedure will fail in practice.

In our experiments  we see that simple surﬁng can indeed be effective for mapping outputs y to inputs
x for the trained network  where it often outperforms direct gradient descent for a range of deep
network architectures and training procedures. However  these simulations also point to the fact that
in some settings  direct gradient descent itself can be surprisingly effective. A deeper understanding
of this phenomenon could lead to more advanced surﬁng algorithms that are able to ride to the ﬁnal
optimum even more efﬁciently and often.

Acknowledgments

Research supported in part by NSF grants DMS-1513594  CCF-1839308  DMS-1916198  and a
J.P. Morgan Faculty Research Award.

References

Arjovsky  M.  Chintala  S.  and Bottou  L. (2017). Wasserstein GAN. arXiv:1701.07875.

Bora  A.  Jalal  A.  Price  E.  and Dimakis  A. G. (2017). Compressed sensing using generative
models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 537–546. JMLR. org.

Candes  E. J.  Romberg  J. K.  and Tao  T. (2006). Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics  59(8):1207–1223.

Chen  T. Q.  Rubanova  Y.  Bettencourt  J.  and Duvenaud  D. K. (2018). Neural ordinary differential
equations. In Bengio  S.  Wallach  H.  Larochelle  H.  Grauman  K.  Cesa-Bianchi  N.  and Garnett 
R.  editors  Advances in Neural Information Processing Systems 31  pages 6571–6583. Curran
Associates  Inc.

Chen  X.  Duan  Y.  Houthooft  R.  Schulman  J.  Sutskever  I.  and Abbeel  P. (2016). Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems  pages 2172–2180.

Davis  D.  Drusvyatskiy  D.  Kakade  S.  and Lee  J. D. (2018). Stochastic subgradient method

converges on tame functions. Foundations of Computational Mathematics  pages 1–36.

Dinh  L.  Sohl-Dickstein  J.  and Bengio  S. (2017). Density estimation using real NVP.

arXiv:1605.08803.

Donoho  D. L. et al. (2006). Compressed sensing.

IEEE Transactions on information theory 

52(4):1289–1306.

Du  S. S.  Lee  J. D.  Li  H.  Wang  L.  and Zhai  X. (2018a). Gradient descent ﬁnds global minima of

deep neural networks. arXiv preprint arXiv:1811.03804.

Du  S. S.  Zhai  X.  Poczos  B.  and Singh  A. (2018b). Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054.

9

Goodfellow  I.  Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S.  Courville  A.  and
Bengio  Y. (2014). Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680.

Gulrajani  I.  Ahmed  F.  Arjovsky  M.  Dumoulin  V.  and Courville  A. C. (2017). Improved training
of wasserstein GANs. In Advances in Neural Information Processing Systems  pages 5767–5777.

Hand  P. and Voroninski  V. (2019). Global guarantees for enforcing deep generative priors by

empirical risk. IEEE Transactions on Information Theory.

Ioffe  S. and Szegedy  C. (2015). Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv:1502.03167.

Kingma  D. P. and Ba  J. (2014). Adam: A method for stochastic optimization. arXiv:1412.6980.

Kingma  D. P. and Dhariwal  P. (2018). Glow: Generative ﬂow with invertible 1x1 convolutions. In
Bengio  S.  Wallach  H.  Larochelle  H.  Grauman  K.  Cesa-Bianchi  N.  and Garnett  R.  editors 
Advances in Neural Information Processing Systems 31  pages 10215–10224. Curran Associates 
Inc.

Kingma  D. P. and Welling  M. (2014). Auto-encoding variational Bayes. In 2nd International

Conference on Learning Representations  ICLR 2014.

Li  Y. and Liang  Y. (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems  pages 8157–
8166.

Li  Y. and Yuan  Y. (2017). Convergence analysis of two-layer neural networks with relu activation. In
Guyon  I.  Luxburg  U. V.  Bengio  S.  Wallach  H.  Fergus  R.  Vishwanathan  S.  and Garnett  R. 
editors  Advances in Neural Information Processing Systems 30  pages 597–607. Curran Associates 
Inc.

Radford  A.  Metz  L.  and Chintala  S. (2015). Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv:1511.06434.

Szegedy  C.  Zaremba  W.  Sutskever  I.  Bruna  J.  Erhan  D.  Goodfellow  I.  and Fergus  R. (2014).
Intriguing properties of neural networks. In International Conference on Learning Representations.

Virmaux  A. and Scaman  K. (2018). Lipschitz regularity of deep neural networks: Analysis and

efﬁcient estimation. In Advances in Neural Information Processing Systems  pages 3835–3844.

10

,Ganlin Song
Zhou Fan
John Lafferty