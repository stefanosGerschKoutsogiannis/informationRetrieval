2013,Multiscale Dictionary Learning for Estimating Conditional Distributions,Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem.  It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features  which are massive-dimensional.  We propose a multiscale dictionary learning model  which expresses the conditional response density as a convex combination of dictionary densities  with the densities used and their weights dependent on the path through a tree decomposition of the feature space.  A fast graph partitioning algorithm is applied to obtain the tree decomposition  with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner.  The algorithm scales efficiently to approximately one million features.  State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.,Multiscale Dictionary Learning for
Estimating Conditional Distributions

Francesca Petralia

Department of Genetics and Genomic Sciences

Icahn School of Medicine at Mt Sinai

New York  NY 10128  U.S.A.

francesca.petralia@mssm.edu

Joshua Vogelstein
Child Mind Institute

Department of Statistical Science

Duke University

Durham  North Carolina 27708  U.S.A.

jo.vo@duke.edu

David B. Dunson

Department of Statistical Science

Duke University

Durham  North Carolina 27708  U.S.A.

dunson@stat.duke.edu

Abstract

Nonparametric estimation of the conditional distribution of a response given high-
dimensional features is a challenging problem. It is important to allow not only the
mean but also the variance and shape of the response density to change ﬂexibly
with features  which are massive-dimensional. We propose a multiscale dictio-
nary learning model  which expresses the conditional response density as a convex
combination of dictionary densities  with the densities used and their weights de-
pendent on the path through a tree decomposition of the feature space. A fast graph
partitioning algorithm is applied to obtain the tree decomposition  with Bayesian
methods then used to adaptively prune and average over different sub-trees in a
soft probabilistic manner. The algorithm scales efﬁciently to approximately one
million features. State of the art predictive performance is demonstrated for toy
examples and two neuroscience applications including up to a million features.

1

Introduction

Massive datasets are becoming an ubiquitous by-product of modern scientiﬁc and industrial ap-
plications. These data present statistical and computational challenges because many previously
developed analysis approaches do not scale-up sufﬁciently. Challenges arise because of the ultra
high-dimensionality and relatively low sample size. Parsimonious models for such big data assume
that the density in the ambient space concentrates around a lower-dimensional (possibly nonlinear)
subspace. A plethora of methods are emerging to estimate such lower-dimensional subspaces [1  2].
We are interested in using such lower-dimensional embeddings to obtain estimates of the conditional
distribution of some target variable(s). This conditional density estimation setting arises in a number
of important application areas  including neuroscience  genetics  and video processing. For exam-
ple  one might desire automated estimation of a predictive density for a neurologic phenotype of
interest  such as intelligence  on the basis of available data for a patient including neuroimaging. The
challenge is to estimate the probability density function of the phenotype nonparametrically based
on a 106 dimensional image of the subject’s brain. It is crucial to avoid parametric assumptions
on the density  such as Gaussianity  while allowing the density to change ﬂexibly with predictors.
Otherwise  one can obtain misleading predictions and poorly characterize predictive uncertainty.

1

There is a rich machine learning and statistical literature on conditional density estimation of a re-
sponse y ∈ Y given a set of features (predictors) x = (x1  x2  . . .   xp)T ∈ X⊆ Rp. Common
approaches include hierarchical mixtures of experts [3  4]  kernel methods [5  6  7]  Bayesian ﬁnite
mixture models [8  9  10] and Bayesian nonparametrics [11  12  13  14]. However  there has been
limited consideration of scaling to large p settings  with the variational Bayes approach of [9] being
a notable exception. For dimensionality reduction  [9] follow a greedy variable selection algorithm.
Their approach does not scale to the sized applications we are interested in. For example  in a prob-
lem with p = 1  000 and n = 500  they reported a CPU time of 51.7 minutes for a single analysis.
We are interested in problems with p having many more orders of magnitude  requiring a faster
computing time while also accommodating ﬂexible nonlinear dimensionality reduction (variable se-
lection is a limited sort of dimension reduction). To our knowledge  there are no nonparametric
density regression competitors to our approach  which maintain a characterization of uncertainty in
estimating the conditional densities; rather  all sufﬁciently scalable algorithms provide point predic-
tions and/or rely on restrictive assumptions such as linearity.
In big data problems  scaling is often accomplished using divide-and-conquer techniques. However 
as the number of features increases  the problem of ﬁnding the best splitting attribute becomes
intractable  so that CART  MARS and multiple tree models cannot be efﬁciently applied. Similarly 
mixture of experts becomes computationally demanding  since both mixture weights and dictionary
densities are predictor dependent. To improve efﬁciency  sparse extensions relying on different
variable selection algorithms have been proposed [15]. However  performing variable selection in
high dimensions is effectively intractable: algorithms need to efﬁciently search for the best subsets
of predictors to include in weight and mean functions within a mixture model  an NP-hard problem
[16].
In order to efﬁciently deal with massive datasets  we propose a novel multiscale approach which
starts by learning a multiscale dictionary of densities. This tree is efﬁciently learned in a ﬁrst stage
using a fast and scalable graph partitioning algorithm applied to the high-dimensional observations
[17]. Expressing the conditional densities f (y|x) for each x ∈ X as a convex combination of
coarse-to-ﬁne scale dictionary densities  the learning problem in the second stage estimates the
corresponding multiscale probability tree. This is accomplished in a Bayesian manner using a novel
multiscale stick-breaking process  which allows the data to inform about the optimal bias-variance
tradeoff; weighting coarse scale dictionary densities more highly decreases variance while adding
to bias. This results in a model that borrows information across different resolution levels and
reaches a good compromise in terms of the bias-variance tradeoff. We show that the algorithm
scales efﬁciently to millions of features.

2 Setting
Let X : Ω → X ⊆ Rp be a p-dimensional Euclidean vector-valued predictor random variable  taking
values x ∈ X   with a marginal probability distribution fX. Similarly  let Y : Ω → Y be a target-
valued random variable (e.g.  Y ⊆ R). For inferential expedience  we posit the existence of a latent
variable η : Ω → M ⊆ X   where M is only d “dimensional” and d (cid:28) p. Note that M need not be a
linear subspace of X   rather  M could be  for example  a union or afﬁne subspaces  or a smooth com-
pact Riemannian manifold. Regardless of the nature of M  we assume that we can approximately
decompose the joint distribution as follows  fX Y η = fX Y |ηfη = fY |X ηfX|ηfη ≈ fY |ηfX|ηfη.
Hence  we assume that the signal approximately concentrates around a low-dimensional latent
space  fY |X η = fY |η. This is a much less restrictive assumption than the commonplace assumption
in manifold learning that the marginal distribution fX concentrates around a low-dimensional latent
space.
To provide some intuition for our model  we provide the following concrete example where the
distribution of y ∈ R is a Gaussian function of the coordinate η ∈ M along the swissroll  which is
embedded in a high-dimensional ambient space. Speciﬁcally  we sample the manifold coordinate 
η ∼ U (0  1). We sample x = (x1  . . .   xp)T as follows

x1 = η sin(η)

; x2 = η cos(η)

; xr ∼ N (0  1) r ∈ {3  . . .   p}

Finally  we sample y from N (µ(η)  σ(η)). Clearly  x and y are conditionally independent given η 
which is the low-dimensional signal manifold. In particular  x lives on a swissroll embedded in a

2

p-dimensional ambient space  but y is only a function of the coordinate η along the swissroll M.
The left panels of Figure 1 depict this example when µ(η) = η and σ(η) = η + 1.

Figure 1: Illustration of our generative model and algorithm on a swissroll. The top left panel
shows the manifold M (a swissroll) embedded in a p-dimensional ambient space  where the color
indicates the coordinate along the manifold  η (only the ﬁrst 3 dimensions are shown for visualization
purposes). The bottom left panel shows the distribution of y as a function of η  in particular  fY |η =
N (η  η + 1). The middle and right panels show our estimates of fY |η at scales 3 and 4  respectively 
which follow from partitioning our data. Sample size was n = 10  000.
3 Goal

Our goal is to develop an approach to learn about fY |X from n pairs of observations that we as-
sume are exchangeable samples from the joint distribution  (xi  yi) ∼ fX Y ∈ F. Let Dn =
{(xi  yi)}i∈[n]  where [n] = {1  . . .   n}. More speciﬁcally  we seek to obtain a posterior over fY |X.
We insist that our approach satisﬁes several desiderata  including most importantly: (i) scales up
to p ≈ 106 in reasonable time  (ii) yields good empirical results  and (iii) automatically adapts to
the complexity of the data corpus. To our knowledge  no extant approach for estimating conditional
densities or posteriors thereof satisﬁes even our ﬁrst criterion.

4 Methodology

4.1 Ms. Deeds Framework

We propose here a general modular approach which we refer to as multiscale dictionary learning for
estimating conditional distributions (“Ms. Deeds”). Ms. Deeds consists of two components: (i) a
tree decomposition of the space  and (ii) an assumed form of the conditional probability model.

Tree Decomposition A tree decomposition τ yields a multiscale partition of the data or the am-
bient space in which the data live. Let (W  ρW   FW ) be a measurable metric space  where FW is a
Borel probability measure  W  and ρW : W×W → R is a metric on W. Let BW
r (w) be the ρW -ball
inside W of radius r > 0 centered at w ∈ W. For example  W could be the data corpus Dn  or it
could be X × Y. We deﬁne a tree decomposition as in [2  18]. A partition tree τ of W consists of a
collection of cells  τ = {Cj k}j∈Z k∈Kj . At each scale j  the set of cells Cj = {Cj k}k∈Kj provides
a disjoint partition of W almost everywhere. We deﬁne j = 0 as the root node. For each j > 0 
each set has a unique parent node. Denote

Aj k = {(j(cid:48)  k(cid:48)) : Cj k ⊆ Cj(cid:48) k(cid:48)  j(cid:48) < j}   Dj k = {(j(cid:48)  k(cid:48)) : Cj(cid:48) k(cid:48) ⊆ Cj k  j(cid:48) > j}

respectively the ancestors and the descendants of node (j  k).

3

Unlike classical harmonic theory which presupposes τ (e.g.  in wavelets [19])  we choose to learn
τ from the data. Previously  Chen et al. [18] developed a multiscale measure estimation strategy 
and proved that there exists a scale j such that the approximate measure is within some bound of
the true measure  under certain relatively general assumptions. We decided to simply partition the
x’s  ignoring the y’s in the partitioning strategy. Our justiﬁcation for this choice is as follows. First 
sometimes there are many different y’s for many different applications. In such cases  we do not
want to bias the partitioning to any speciﬁc y’s  all the more so when new unknown y’s may later
emerge. Second  because the x’s are so much higher dimensional than the y’s in our applications of
interest  the partitions would be dominated by the x’s  unless we chose a partitioning strategy that
emphasized the y’s. Thus  our strategy mitigates this difﬁculty (while certainly introducing others).
Given that we are going to partition using only the x’s  we still face the choice of precisely how
to partition. A fully Bayesian approach would construct a large number of partitions  and integrate
over them to obtain posteriors. However  such a fully Bayesian strategy remains computationally in-
tractable at scale  so we adopt a hybrid strategy. Speciﬁcally  we employ METIS [17]  a well-known
relatively efﬁcient multiscale partitioning algorithm with demonstrably good empirical performance
on a wide range of graphs. Given n observations  i.e. xi = (xi1  . . .   xip)T ∈ X for i ∈ [n]  the
graph construction follows via computing all pairwise distances using ρ(xu  xv) = (cid:107)˜xu − ˜xv(cid:107)2 
where ˜x is the whitened x (i.e.  mean subtracted and variance normalized). We let there be an edge
between xu and xv whenever e−ρ(xu xv)2
> t  where t is some threshold chosen to elicit the desired
sparsity level. Applying METIS recursively on the graph constructed in this way yields a single tree
(see supplementary material for further details).

Conditional Probability Model Given the tree decomposition of the data  we place a non-
parametric prior over the tree. Speciﬁcally  we deﬁne fY |X as

fY |X =

πj kj (x)fj kj (x)(y|x)

(1)

(cid:88)

j∈Z

(cid:89)

such that(cid:80)

where(cid:80)

where kj(x) is the set at scale j where x has been allocated and πj kj (x) are weights across scales
j∈Z πj kj (x) = 1. We let weights in Eq. (1) be generated by a stick-breaking process
[20]. For each node Cj k in the partition tree  we deﬁne a stick length Vj k ∼ Beta(1  α). The
parameter α encodes the complexity of the model  with α = 0 corresponding to the case in which
f (y|x) = f (y). The stick-breaking process is deﬁned as

πj k = Vj k

(j(cid:48) k(cid:48))∈Aj k

[1 − Vj(cid:48) k(cid:48)]  

(2)

(j(cid:48) k(cid:48))∈Aj k

πj(cid:48) k(cid:48) = 1. The implication of this is that each scale within a path is weighted
to optimize the bias/variance trade-off across scales. We refer to this prior as a multiscale stick-
breaking process. Note that this Bayesian nonparametric prior assigns a positive probability to all
possible paths  including those not observed in the training data. Thus  by adopting this Bayesian
formulation  we are able to obtain posterior estimates for any newly observed data  regardless of
the amount and variability of training data. This is a pragmatically useful feature of the Bayesian
formulation  in addition to the alleviation of the need to choose a scale [18].
Each fj k in Eq. (1) is an element of a family of distributions. This family might be quite general 
e.g.  all possible conditional densities  or quite simple  e.g.  Gaussian distributions. Moreover  the
family can adapt with j or k  being more complex at the coarser scales (for which nj k’s are larger) 
and simpler for the ﬁner scales (or partitions with fewer samples). We let the family of conditional
densities for y be Gaussian for simplicity  that is  we assume that fj k = N (µj k  σj k) with µj k ∈ R
and σj k ∈ R+. Because we are interested in posteriors over the conditional distribution fY |X  we
place relatively uninformative but conjugate priors on µj k and σj k  speciﬁcally  assuming the y’s
have been whitened and are unidimensional  µj k ∼ N (0  1) and σj k = IG(a  b). Obviously  other
choices  such as ﬁnite or inﬁnite mixtures of Gaussians are also possible for continuous valued data.

Inference

4.2
We introduce the latent variable (cid:96)i ∈ Z  for i = [n]  denoting the multiscale level used by the ith
observation. Let nj k be the number of observations in Cj k. Let kh(xi) be a variable indicating the

4

set at level h where xi has been allocated. Each Gibbs sampler iteration can be summarized in the
following steps:

(i) Update (cid:96)i by sampling from the multinomial full conditional:

Pr((cid:96)i = j |·) = πj kj (xi)fj kj (xi)(yi|xi)/

πs ks(xi)fs ks(xi)(yi|xi)

with β(cid:48) = 1 + nj k and α(cid:48) = α +(cid:80)

(ii) Update stick-breaking random variable Vj k  for any j ∈ Z and k ∈ Kj  from Beta(β(cid:48)  α(cid:48))
(iii) Update µj k and σj k  for any j ∈ Z and k ∈ Kj  by sampling from

(r s)∈Dj k

(cid:88)

s∈Z

µj k ∼ N (υj kνj k ¯yj k  υj k)  

nr s.

σj k ∼ IG(cid:0)aσ  b + 0.5(cid:80)

(yi − µj k)2(cid:1)

i∈Ij k

where υj k = (1 + νj k)−1  νj k = nj k/σj k aσ = a + nj k/2  ¯yj k being the average of
the observations {yi} allocated to cell Cj k and Ij k = {i : (cid:96)i = j  xi ∈ Cj k}.

To make predictions  the Gibbs sampler was run with up to 20  000 iterations  including a burn-
in of 1  000 (see Supplementary material for details). Gibbs sampler chains were stopped testing
normality of normalized averages of functions of the Markov chain [21]. Parameters (a  b) and α
involved in the prior density of parameters σj k’s and Vj k’s were set to (3  1) and 1  respectively.
All predictions used a leave-one-out strategy.

4.3 Simulation Studies

In order to assess the predictive performance of the proposed model  we considered the four different
simulation scenarios described below:
(1) Nonlinear Mixture We ﬁrst consider a relatively simple yet nonlinear joint model  with a condi-
tional Gaussian distribution y|η ∼ |η|N (µ1  σ1) + (1 − |η|)N (µ2  σ2)  a marginal distribution for
each dimension of x  xr|η ∼ N (η  σx)  r ∈ {1  2  . . .   p}  and a uniform distribution over the la-
tent manifold η ∼ sin(U (0  c)). In the simulations we let (µ1  σ1) = (−2  1)  (µ2  σ2) = (2  1) 
σx = 0.1  and c = 20  and p = 1000. Thus  fY |X is a highly nonlinear function of x  and even η 
and x is high-dimensional.
(2) Swissroll We then return to the swissroll example of Figure 1; in Figure 3 we show results for
(µ  σ) = (η  1).
(3) Linear Subspace Letting Γ ∈ Rp+1×q and Θ be a q × d “diagonal” matrix (meaning all entires
other than the ﬁrst d < q elements of the diagonal are zero)  we assume the following model:
Y  X|η ∼ Np+1(ΓΘη  I)  where Γ ∼ Sp+1 d indicates Γ is uniformly sampled from the set of all
orthonormal d frames in Rp+1 (a Stiefel manifold)  θii ∼ IG(aθ  bθ) for i ∈ {1  . . .   d} and all other
elements of Θ are zero  and η ∼ Nd(0  I). In the simulation  we let q = d = 5  (αθ  βθ) = (1  0.25).
(4) Union of Linear Subspaces This model is a direct extension of the linear subspace model 
as it is a union of subspaces. We let the dimensionality of each subspace vary to demonstrate
g=1 ωgNp+1(ΓgΘgη  I) 
ω ∼ Dirichlet(α)  η ∼ Nd(0  I)  where Γ ∼ Sp+1 g and Θg is “diagonal” with θii ∼ IG(ag  bg)
for i ∈ {1  . . .   g}  and the remaining elements of Θ are zero. In the simulation  we let G = 5 
α = (1  . . .   1)T  (αg  βg) = (αθ  βθ) as above.

the generality of our procedure. Speciﬁcally  we assume Y  X|η ∼ (cid:80)G

4.4 Neuroscience Applications

We assessed the predictive performance of the proposed method on two very different neuroimaging
datasets. For all analyses  each variable was normalized by subtracting its mean and dividing by
its standard deviation. The prior speciﬁcation and Gibbs sampler described in §4.1 and 4.2 were
utilized.
In the ﬁrst experiment we investigated the extent to which we could predict creativity (as measured
via the Composite Creativity Index [22]) via a structural connectome dataset collected at the Mind
Research Network (data were collected as described in Jung et al.
[23]). For each subject  we
estimate a 70 vertex undirected weighted brain-graph using the Magnetic Resonance Connectome
Automated Pipeline (MRCAP) [24] from diffusion tensor imaging data [25]. Because our graphs are

5

undirected and lack self-loops  we have a total of p =(cid:0)70

(cid:1) = 2  415 potential weighted edges. The

2

p-dimensional feature vector is deﬁned by the natural logarithm of the vectorized matrix described
above.
The second dataset comes from a resting-state functional magnetic resonance experiment as part of
the Autism Brain Imaging Data Exchange [26]. We selected the Yale Child Study Center for analy-
sis. Each brain-image was processed using the Conﬁgurable Pipeline for Analysis of Connectomes
(CPAC) [27]. For each subject  we computed a measure of normalized power at each voxel called
fALFF [28]. To ensure the existence of nonlinear signal relating these predictors  we let yi corre-
spond to an estimate of overall head motion in the scanner  called mean framewise displacement
(FD) computed as described in Power et al. [29]. In total  there were p = 902  629 voxels.

m deﬁned as rA

4.5 Evaluation Criteria
m = φ(M SB)/φ(A)  where
To compare algorithmic performance we considered rA
φ is the quantity of interest (for example  CPU time in seconds or mean squared error)  MSB is
our approach and A is the competitor algorithm. To obtain mean-squared error estimates from
MSB  we select our posterior mean as a point-estimate (the comparison algorithms do not generate
posterior predictions  only point estimates). For each simulation scenario  we sampled multiple
datasets and compute the matched distribution of rA
m. In other words  rather than running simulations
and reporting the distribution of performance for each algorithm  we compare the algorithms per
simulation. This provides a much more informative indication of algorithmic performance  in that
we indicate the fraction of simulations one algorithm outperforms another on some metric. For each
example  we sampled 20 datasets to obtain estimates of the distribution over rA
m. All experiments
were performed on a typical workstation  Intel Core i7-2600K Quad-Core Processor with 8192 MB
of RAM.

5 Results

5.1

Illustrative Example

The middle and right panels of Figure 1 depict the quality of partitioning and density estimation
for the swissroll example described in §2  with the ambient dimension p = 1000 and the predictive
manifold dimension d = 1. We sampled n = 104 samples for this illustration. At scale 3 we have 4
partitions  and at scale 4 we have 8 (note that the partition tree  in general  need not be binary). The
top panels are color coded to indicate which xi’s fall into which partition. Although imperfect  it
should be clear that the data are partitioned very well. The bottom panels show the resulting estimate
of the posteriors at the two scales. These posteriors are piecewise constant  as they are invariant to
the manifold coordinate within a given partition.
To obviate the need to choose a scale to use to make a prediction  we choose to adopt a Bayesian
approach and integrate across scales. Figure 2 shows the estimated density of two observations
of model (1) with parameters (µ1  σ1) = (−2  1)  (µ2  σ2) = (2  1)  σx = 0.1  and c = 20 for
different sample sizes. Posteriors of the conditional density fY |X were computed for various sample
sizes. Figure 2 suggests that our estimate of fY |X approaches the true density as the number of
observations in the training set increases. We are unable to compare our strategy for posterior
estimation to previous literature because we are unaware of previous Bayesian approaches for this
problem that scale up to problems of this size. Therefore  we numerically compare the performance
of our point-estimates (which we deﬁne as the posterior mean of ˆfY |X) with the predictions of the
competitor algorithms.

5.2 Quantitative Comparisons for Simulated Data

Figure 3 compares the numerical performance of our algorithm (MSB) with Lasso (black)  CART
(red)  and PC regression (green) in terms of both mean-squared error (top) and CPU time (bottom)
for models (2)  (3)  and (4) in the left  middle  and right panels respectively. These ﬁgures show
relative performance on a per simulation basis  thus enabling a much more powerful comparison
than averaging performance for each algorithm over a set of simulations. Note that these three
simulations span a wide range of models  including nonlinear smooth manifolds such as the swissroll

6

Figure 2: Illustrative example of model (1) sug-
gesting that our posterior estimates of the con-
ditional density are converging as n increases
even when fY |η is highly nonlinear and fX|η
is very high-dimensional. True (red) and es-
timated (black) density (50th percentile: solid
line  2.5th and 97.5th percentiles: dashed lines)
for two data positions along the manifold (top
panels: η ≈ −0.9  bottom panels: η ≈ 0.5)
considering different training set sizes.

(model 2)  relatively simple linear subspace manifolds (model 3)  and a union of linear subspaces
model (model 4 ; which is neither linear nor a manifold).
In terms of predictive accuracy  the top panels show that for all three simulations  in every dimen-
sionality that we considered—including p = 0.5 × 106—MSB is more accurate than either Lasso 
CART  or PC regression. Note that this is the case even though MSB provides much more infor-
mation about the posterior fY |X  yielding an entire posterior over fY |X  rather than merely a point
estimate.
In terms of computational time  MSB is much faster than the competitors for large p and n  as shown
in the bottom three panels. The supplementary materials show that computational time for MSB is
relatively constant as a function of p  whereas Lasso’s computational time grows considerably with
p. Thus  for large enough p  MSB is signiﬁcantly faster that Lasso. MSB is faster than CART and
PC regression for all p and n under consideration. Thus  it is clear from these simulations that MSB
has better scaling properties—in terms of both predictive accuracy and computational time—than
the competitor methods.

Figure 3: Numerical results for various simulation scenarios. Top plots depict the relative mean-
squared error of MSB (our approach)  versus CART (red)  Lasso (black)  and PC regression (green)
for as a function of ambient dimension of x. Bottom plots depict the ratio of CPU time as a function
of sample size. The three simulation scenarios are: swissroll (left)  linear subspaces (middle)  union
of linear subspaces (right). MSB outperforms both CART  Lasso  and PC regression in all three
scenarios regardless of ambient dimension (rA
mse < 1 for all p). MSB compute time is relatively
constant as n or p increase  whereas Lasso’s compute time increases  thus  as n or p increase  MSB
CPU time becomes less than Lasso’s. MSB was always signiﬁcantly faster than CART and PC
regression  regardless of n or p. For all panels  n = 100 when p varies  and p = 300k when n
varies  where k indicates 1000  e.g.  300k= 3 × 105.

7

−4−202400.30.60.9n=100f(y|η=−0.9)−4−202400.30.6yf(y|η=0.5)−4−202400.30.60.9n=150−4−202400.30.6−4−202400.30.60.9n=200−4−202400.30.650k100k01pMSE Ratio(2) Swissroll10020030001sample sizeTime Ratio50k100k01p(3) Linear Subspace100k200k300k024p50k100k01p(4) Union of Linear Subspaces10020030001sample sizeTable 1: Neuroscience application quantitative performance comparisons. Squared error predictive
accuracy per subject (using leave-one-out) was computed. We report the mean and standard devi-
ation (s.d.) across subjects of squared error  and CPU time (in seconds). We compare multiscale
stick-breaking (MSB)  CART  Lasso  random forest (RF)  and PC regression. MSB outperforms all
the competitors in terms of predictive accuracy and scalability. Only MSB and Lasso even ran for
the ≈ 106 dimensional application. Bold indicates best MSE  ∗ indicates best CPU time.

DATA
CREATIVITY

n
108

p

2 415

MOVEMENT

56

≈ 106

MODEL
MSB
CART
LASSO∗

RF
MSB∗
LASSO

PC REGRESSION

MSE (S.D.)
0.56 (0.85)
1.10 (1.00)
∗
0.63 (0.95)
0.57(0.90)
0.65 (0.88)
∗
0.76 (0.90)
1.02 (0.98)

TIME (S.D.)
1.1 (0.02)
0.9 (0.01)
0.40 (0.10)∗
78.2 (0.59)
0.46 (0.37)
20.98 (2.31)
96.18 (9.66)

∗

5.3 Quantitative Comparisons for Neuroscience Applications

Table 1 shows the mean and standard deviation of point-estimate predictions per subject (using
leave-one-out) for the two neuroscience applications that we investigated: (i) predicting creativity
from diffusion MRI (creativity) and  (ii) predicting head motion based on functional MRI (move-
ment). For the creativity application  p was relatively small  “merely” 2  415  so we could run Lasso 
CART  and random forests (RF) [30]. For the movement application  p was nearly one million.
For both applications  MSB yielded improved predictive accuracy over all competitors. Although
CART and Lasso were faster than MSB on the relatively low-dimensional predictor example (cre-
ativity)  their computational scaling was poor  such that CART yielded a memory fault on the higher-
dimensional case  and Lasso required substantially more time than MSB.

6 Discussion

In this work we have introduced a general formalism to estimate conditional distributions via mul-
tiscale dictionary learning. An important property of any such strategy is the ability to scale up to
ultrahigh-dimensional predictors. We considered simulations and real-data examples where the di-
mensionality of the predictor space approached one million. To our knowledge  no other approach
to learn conditional distributions can run at this scale. Our approach explicitly assumes that the pos-
terior fY |X can be well approximated by projecting x onto a lower-dimensional space  fY |X ≈ fY |η 
where η ∈ M ⊂ Rd  and x ∈ Rd. Note that this assumption is much less restrictive than assuming
that x is close to a low-dimensional space; rather  we only assume that the part of fX that “mat-
ters” to predict y lives near a low-dimensional subspace. Because a fully Bayesian strategy remains
computationally intractable at this scale  we developed an empirical Bayes approach  estimating the
partition tree based on the data  but integrating over scales and posteriors.
We demonstrate that even though we obtain posteriors over the conditional distribution fY |X  our
approach  dubbed multiscale stick-breaking (MSB)  outperforms several standard machine learning
algorithms in terms of both predictive accuracy and computational time  as the sample size (n) and
ambient dimension (p) increase. This improvement was demonstrated when the M was a swissroll 
a latent subspace  a union of latent subspaces  and real data (for which the latent space may not even
exist).
In future work  we will extend these numerical results to obtain theory on posterior convergence.
Indeed  while multiscale methods beneﬁt from a rich theoretical foundation [2]  the relative advan-
tages and disadvantages of a fully Bayesian approach  in which one can estimate posteriors over all
functionals of fY |X at all scales  remains relatively unexplored.

References

[1] I. U. Rahman  I. Drori  V. C. Stodden  and D. L. Donoho. Multiscale representations for manifold- valued

data. SIAM J. Multiscale Model  4:1201–1232  2005.

8

[2] W.K. Allard  G. Chen  and M. Maggioni. Multiscale geometric methods for data sets II: geometric

wavelets. Applied and Computational Harmonic Analysis  32:435–462  2012.

[3] R. A. Jacobs  M. I. Jordan  S. J. Nowlan  and G. E. Hinton. Adaptive mixture of local experts. Neural

Computation  3:79–87  1991.

[4] W. X. Jiang and M. A. Tanner. Hierarchical mixtures-of-experts for exponential family regression models:

approximation and maximum likelihood estimation. Annals of Statistics  27:987–1011  1999.

[5] J. Q. Fan  Q. W. Yao  and H. Tong. Estimation of conditional densities and sensitivity measures in

nonlinear dynamical systems. Biometrika  83:189–206  1996.

[6] M. P. Holmes  G. A. Gray  and C. L. Isbell. Fast kernel conditional density estimation: a dual-tree Monte

Carlo approach. Computational statistics & data analysis  54:1707–1718  2010.

[7] G. Fu  F. Y. Shih  and H. Wang. A kernel-based parametric method for conditional density estimation.

Pattern recognition  44:284–294  2011.

[8] D. J. Nott  S. L. Tan  M. Villani  and R. Kohn. Regression density estimation with variational methods

and stochastic approximation. Journal of Computational and Graphical Statistics  21:797–820  2012.

[9] M. N. Tran  D. J. Nott  and R. Kohn. Simultaneous variable selection and component selection for
regression density estimation with mixtures of heteroscedastic experts. Electronic Journal of Statistics 
6:1170–1199  2012.

[10] A. Norets and J. Pelenis. Bayesian modeling of joint and conditional distributions. Journal of Economet-

rics  168:332–346  2012.

[11] J. E. Grifﬁn and M. F. J. Steel. Order-based dependent Dirichlet processes. Journal of the American

Statistical Association  101:179–194  2006.

[12] D. B. Dunson  N. Pillai  and J. H. Park. Bayesian density regression. Journal of the Royal Statistical

Society Series B-Statistical Methodology  69:163–183  2007.

[13] Y. Chung and D. B. Dunson. Nonparametric Bayes conditional distribution modeling with variable selec-

tion. Journal of the American Statistical Association  104:1646–1660  2009.

[14] S. T. Tokdar  Y. M. Zhu  and J. K. Ghosh. Bayesian density regression with logistic Gaussian process and

subspace projection. Bayesian Analysis  5:319–344  2010.

[15] I. Mossavat and O. Amft. Sparse bayesian hierarchical mixture of experts. IEEE Statistical Signal Pro-

cessing Workshop (SSP)  2011.

[16] Isabelle Guyon and Andr´e Elisseeff. An introduction to variable and feature selection. The Journal of

Machine Learning Research  3:1157–1182  2003.

[17] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs.

SIAM Journal on Scientiﬁc Computing 20  1:359392  1999.

[18] G. Chen  M. Iwen  S. Chin  and M. Maggioni. A fast multiscale framework for data in high-dimensions:

Measure estimation  anomaly detection  and compressive measurements. In VCIP  2012 IEEE  2012.

[19] Ingrid Daubechies. Ten Lectures on Wavelets (CBMS-NSF Regional Conference Series in Applied Math-

ematics). SIAM: Society for Industrial and Applied Mathematics  1992.

[20] J. Sethuraman. A constructive denition of Dirichlet priors. Statistica Sinica  4:639–650  1994.
[21] Didier Chauveau and Jean Diebolt. An automated stopping rule for mcmc convergence assessment. Com-

putational Statistics  14:419–442  1998.

[22] R. Arden  R. S. Chavez  R. Grazioplene  and R. E. Jung. Neuroimaging creativity: a psychometric view.

Behavioural brain research  214:143–156  2010.

[23] R. ˜E. Jung  R. Grazioplene  A. Caprihan  R.˜S. Chavez  and R.˜J.

Haier. White matter integrity  creativity  and psychopathology: Disentangling constructs with diffusion
tensor imaging. PloS one  5(3):e9818  2010.

[24] W. ˜R. Gray  J. ˜A. Bogovic  J. ˜T. Vogelstein  B. ˜A. Landman  J˙ L. Prince  and R.˜J. Vogelstein. Magnetic

resonance connectome automated pipeline: an overview. IEEE pulse  3(2):42–8  March 2010.

[25] Susumu Mori and Jiangyang Zhang. Principles of diffusion tensor imaging and its applications to basic

neuroscience research. Neuron  51(5):527–39  September 2006.

[26] ABIDE. http://fcon 1000.projects.nitrc.org/indi/abide/.
[27] S. Sikka  J. ˜T. Vogelstein  and M.˜P. Milham. Towards Automated Analysis of Connectomes: The Conﬁg-

urable Pipeline for the Analysis of Connectomes (C-PAC). Neuroinformatics  2012.

[28] Q-H. Zou  C-Z. Zhu  Y. Yang  X-N. Zuo  X-Y. Long  Q-J. Cao  Y-F ˙Wang  and Y-F. Zang. An improved
approach to detection of amplitude of low-frequency ﬂuctuation (ALFF) for resting-state fMRI: fractional
ALFF. Journal of neuroscience methods  172(1):137–141  July 2008.

[29] J. D. Power  K. A. Barnes  C. J. Stone  and R. A. Olshen. Spurious but systematic correlations in functional

connectivity MRI networks arise from subject motion. Neuroimage  59:2142–2154  2012.

[30] Leo Breiman. Statistical Modeling : The Two Cultures. Statistical Science  16(3):199–231  2001.

9

,Francesca Petralia
Joshua Vogelstein
David Dunson
Emmanuel Abbe
Sanjeev Kulkarni
Eun Jee Lee
Mikhail Yurochkin
Sebastian Claici
Edward Chien
Farzaneh Mirzazadeh
Justin Solomon