2013,Generalized Method-of-Moments for Rank Aggregation,In this paper we propose a class of efficient Generalized Method-of-Moments(GMM) algorithms for computing parameters of the Plackett-Luce model  where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons  and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique  and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run significantly faster than the classical Minorize-Maximization (MM) algorithm  while achieving competitive statistical efficiency.,Generalized Method-of-Moments for Rank

Aggregation

Hossein Azari Souﬁani

SEAS

Harvard University

azari@fas.harvard.edu

William Z. Chen

Statistics Department
Harvard University

wchen@college.harvard.edu

David C. Parkes

SEAS

Harvard University

parkes@eecs.harvard.edu

Lirong Xia

Computer Science Department
Rensselaer Polytechnic Institute

Troy  NY 12180  USA

xial@cs.rpi.edu

Abstract

In this paper we propose a class of efﬁcient Generalized Method-of-Moments
(GMM) algorithms for computing parameters of the Plackett-Luce model  where
the data consists of full rankings over alternatives. Our technique is based on
breaking the full rankings into pairwise comparisons  and then computing param-
eters that satisfy a set of generalized moment conditions. We identify conditions
for the output of GMM to be unique  and identify a general class of consistent
and inconsistent breakings. We then show by theory and experiments that our al-
gorithms run signiﬁcantly faster than the classical Minorize-Maximization (MM)
algorithm  while achieving competitive statistical efﬁciency.

1

Introduction

In many applications  we need to aggregate the preferences of agents over a set of alternatives to
produce a joint ranking. For example  in systems for ranking the quality of products  restaurants  or
other services  we can generate an aggregate rank through feedback from individual users. This idea
of rank aggregation also plays an important role in multiagent systems  meta-search engines [4] 
belief merging [5]  crowdsourcing [15]  and many other e-commerce applications.
A standard approach towards rank aggregation is to treat input rankings as data generated from
a probabilistic model  and then learn the MLE of the input data. This idea has been explored in
both the machine learning community and the (computational) social choice community. The most
popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2  13]  the Plackett-
Luce model (PL for short) [17  13]  the random utility model [18]  and the Mallows (Condorcet)
model [14  3]. In machine learning  researchers have focused on designing efﬁcient algorithms to
estimate parameters for popular models; e.g. [8  12  1]. This line of research is sometimes referred
to as learning to rank [11].
Recently  Negahban et al. [16] proposed a rank aggregation algorithm  called Rank Centrality (RC) 
based on computing the stationary distribution of a Markov chain whose transition matrix is deﬁned
according to the data (pairwise comparisons among alternatives). The authors describe the approach
as being model independent  and prove that for data generated according to BTL  the output of RC
converges to the ground truth  and the performance of RC is almost identical to the performance of

1

MLE for BTL. Moreover  they characterized the convergence rate and showed experimental com-
parisons.
Our Contributions. In this paper  we take a generalized method-of-moments (GMM) point of view
towards rank aggregation. We ﬁrst reveal a new and natural connection between the RC algo-
rithm [16] and the BTL model by showing that RC algorithm can be interpreted as a GMM estimator
applied to the BTL model.
The main technical contribution of this paper is a class of GMMs for parameter estimation under
the PL model  which generalizes BTL and the input consists of full rankings instead of pairwise
comparisons as in the case of BTL and RC algorithm.
Our algorithms ﬁrst break full rankings into pairwise comparisons  and then solve the generalized
moment conditions to ﬁnd the parameters. Each of our GMMs is characterized by a way of breaking
full rankings. We characterize conditions for the output of the algorithm to be unique  and we also
obtain some general characterizations that help us to determine which method of breaking leads to
a consistent GMM. Speciﬁcally  full breaking (which uses all pairwise comparisons in the ranking)
is consistent  but adjacent breaking (which only uses pairwise comparisons in adjacent positions) is
inconsistent.
We characterize the computational complexity of our GMMs  and show that the asymptotic com-
plexity is better than for the classical Minorize-Maximization (MM) algorithm for PL [8]. We also
compare statistical efﬁciency and running time of these methods experimentally using both synthetic
and real-world data  showing that all GMMs run much faster than the MM algorithm.
For the synthetic data  we observe that many consistent GMMs converge as fast as the MM algo-
rithm  while there exists a clear tradeoff between computational complexity and statistical efﬁciency
among consistent GMMs.
Technically our technique is related to the random walk approach [16]. However  we note that
our algorithms aggregate full rankings under PL  while the RC algorithm aggregates pairwise com-
parisons. Therefore  it is quite hard to directly compare our GMMs and RC fairly since they are
designed for different types of data. Moreover  by taking a GMM point of view  we prove the consis-
tency of our algorithms on top of theories for GMMs  while Negahban et al. proved the consistency
of RC directly.
2 Preliminaries
Let C = {c1  ..  cm} denote the set of m alternatives. Let D = {d1  . . .   dn} denote the data  where
each dj is a full ranking over C. The PL model is a parametric model where each alternative ci is
i=1 γi = 1. Let (cid:126)γ = (γ1  . . .   γm) and Ω denote the
i=1 γi = 1}.

parameterized by γi ∈ (0  1)  such that(cid:80)m
parameter space. Let ¯Ω denote the closure of Ω. That is  ¯Ω = {(cid:126)γ : ∀i  γi ≥ 0 and (cid:80)m

Given (cid:126)γ∗ ∈ Ω  the probability for a ranking d = [ci1 (cid:31) ci2 (cid:31) ··· (cid:31) cim ] is deﬁned as follows.

l=1 γil

l=2 γil

× ··· ×

γim−1

γim−1 + γim

PrPL(d|(cid:126)γ) =

γi1(cid:80)m

γi2(cid:80)m

×

In the BTL model  the data is composed of pairwise comparisons instead of rankings  and the
model is parameterized in the same way as PL  such that PrBTL(ci1 (cid:31) ci2|(cid:126)γ) =
(cid:80)
.
γi1 + γi2
BTL can be thought of as a special case of PL via marginalization  since PrBTL(ci1 (cid:31) ci2|(cid:126)γ) =

PrPL(d|(cid:126)γ). In the rest of the paper  we denote Pr = PrPL.

γi1

d:ci1(cid:31)cc2

Generalized Method-of-Moments (GMM) provides a wide class of algorithms for parameter estima-
tion. In GMM  we are given a parametric model whose parametric space is Ω ⊆ Rm  an inﬁnite
(cid:80)
series of q × q matrices W = {Wt : t ≥ 1}  and a column-vector-valued function g(d  (cid:126)γ) ∈ Rq.
For any vector (cid:126)a ∈ Rq and any q × q matrix W   we let (cid:107)(cid:126)a(cid:107)W = ((cid:126)a)T W(cid:126)a. For any data D  let
d∈D g(d  (cid:126)γ)  and the GMM method computes parameters (cid:126)γ(cid:48) ∈ Ω that minimize
g(D  (cid:126)γ) = 1
(cid:107)g(D  (cid:126)γ(cid:48))(cid:107)Wn  formally deﬁned as follows:
n
GMMg(D W) = {(cid:126)γ(cid:48) ∈ Ω : (cid:107)g(D  (cid:126)γ(cid:48))(cid:107)Wn = inf
(cid:126)γ∈Ω

(1)
Since Ω may not be compact (as is the case for PL)  the set of parameters GMMg(D W) can be
empty. A GMM is consistent if and only if for any (cid:126)γ∗ ∈ Ω  GMMg(D W) converges in probability
to (cid:126)γ∗ as n → ∞ and the data is drawn i.i.d. given (cid:126)γ∗. Consistency is a desirable property for GMMs.

(cid:107)g(D  (cid:126)γ)(cid:107)Wn}

2

It is well-known that GMMg(D W) is consistent if it satisﬁes some regularity conditions plus the
following condition [7]:
Condition 1. Ed|(cid:126)γ∗ [g(d  (cid:126)γ)] = 0 if and only if (cid:126)γ = (cid:126)γ∗.
Example 1. MLE as a consistent GMM: Suppose the likelihood function is twice-differentiable 
then the MLE is a consistent GMM where g(d  (cid:126)γ) = (cid:53)(cid:126)γ log Pr(d|(cid:126)γ) and Wn = I.
Example 2. Negahban et al. [16] proposed the Rank Centrality (RC) algorithm that aggregates
pairwise comparisons DP = {Y1  . . .   Yn}.1 Let aij denote the number of ci (cid:31) cj in DP and it is
assumed that for any i (cid:54)= j  aij + aji = k. Let dmax denote the maximum pairwise defeats for an
alternative. RC ﬁrst computes the following m × m column stochastic matrix:

(cid:26)

1 −(cid:80)

PRC(DP )ij =

aij/(kdmax)
l(cid:54)=i ali/(kdmax)

if i (cid:54)= j
if i = j

Then  RC computes (PRC(DP ))T ’s stationary distribution (cid:126)γ as the output.
Let X ci(cid:31)cj (Y ) =
RC(d) · (cid:126)γ. It is not hard to check that the output of RC is the output of GMMgRC .
Let gRC(d  (cid:126)γ) = P ∗
Moreover  GMMgRC satisﬁes Condition 1 under the BTL model  and as we will show later in Corol-
lary 4  GMMgRC is consistent for BTL.

if Y = [ci (cid:31) cj]
otherwise

X ci(cid:31)cj
l(cid:54)=i X cl(cid:31)ci

if i (cid:54)= j
if i = j .

−(cid:80)

and P ∗

RC(Y ) =

0

(cid:26) 1

(cid:26)

3 Generalized Method-of-Moments for the Plakett-Luce model
In this section we introduce our GMMs for rank aggregation under PL. In our methods  q = m 
Wn = I and g is linear in (cid:126)γ. We start with a simple special case to illustrate the idea.
Example 3. For any full ranking d over C  we let
• X ci(cid:31)cj (d) =

(cid:26) 1

ci (cid:31)d cj
otherwise

0

(cid:26)

−(cid:80)

d∈D P (d)

X ci(cid:31)cj (d)
l(cid:54)=i X cl(cid:31)ci(d)

if i (cid:54)= j
if i = j

• P (d) be an m × m matrix where P (d)ij =
• gF (d  (cid:126)γ) = P (d) · (cid:126)γ and P (D) = 1
For example 
1/2
1/2 −1/2
1/2

(cid:34) −1

1/2
1
−3/2

(cid:80)

(cid:35)

0

n

let m = 3  D = {[c1 (cid:31) c2 (cid:31) c3]  [c2 (cid:31) c3 (cid:31) c1]}. Then P (D) =

. The corresponding GMM seeks to minimize (cid:107)P (D) · (cid:126)γ(cid:107)2

2 for (cid:126)γ ∈ Ω.



−(cid:80)

i

γ∗
γ∗
i +γ∗
γ∗
γ∗
i +γ∗

j

l

if i (cid:54)= j
if i = j

  which means that
It is not hard to verify that (Ed|(cid:126)γ∗ [P (d)])ij =
Ed|(cid:126)γ∗ [gF (d  (cid:126)γ∗)] = Ed|(cid:126)γ∗ [P (d)] · (cid:126)γ∗ = 0.
It is not hard to verify that (cid:126)γ∗ is the only solution
to Ed|(cid:126)γ∗ [gF (d  (cid:126)γ)] = 0. Therefore  GMMgF satisﬁes Condition 1. Moreover  we will show in
Corollary 3 that GMMgF is consistent for PL.

l(cid:54)=i

l

In the above example  we count all pairwise comparisons in a full ranking d to build P (d)  and deﬁne
g = P (D) · (cid:126)γ to be linear in (cid:126)γ. In general  we may consider some subset of pairwise comparisons.
This leads to the deﬁnition of our class of GMMs based on the notion of breakings. Intuitively  a
breaking is an undirected graph over the m positions in a ranking  such that for any full ranking
d  the pairwise comparisons between alternatives in the ith position and jth position are counted to
construct PG(d) if and only if {i  j} ∈ G.
Deﬁnition 1. A breaking is a non-empty undirected graph G whose vertices are {1  . . .   m}. Given
any breaking G  any full ranking d over C  and any ci  cj ∈ C  we let

1The BTL model in [16] is slightly different from that in this paper. Therefore  in this example we adopt an

equivalent description of the RC algorithm.

3

(cid:26) 1

G

0

(d) =

{Pos(ci  d)  Pos(cj  d)} ∈ G and ci (cid:31)d cj
otherwise

• X ci(cid:31)cj
tion of ci in d.
• PG(d) be an m × m matrix where PG(d)ij =
• gG(d  (cid:126)γ) = PG(d) · (cid:126)γ
• GMMG(D) be the GMM method that solves Equation (1) for gG and Wn = I.2

X ci(cid:31)cj
l(cid:54)=i X cl(cid:31)ci

if i (cid:54)= j
if i = j

−(cid:80)

(d)
(d)

(cid:26)

G

G

  where Pos(ci  d) is the posi-

In this paper  we focus on the following breakings  illustrated in Figure 1.
• Full breaking: GF is the complete graph. Example 3 is the GMM with full breaking.
• Top-k breaking: for any k ≤ m  Gk
• Bottom-k breaking: for any k ≥ 2  Gk
• Adjacent breaking: GA = {{1  2} {2  3}  . . .  {m − 1  m}}.
P = {{k  i} : i (cid:54)= k}.
• Position-k breaking: for any k ≥ 2  Gk

B = {{i  j} : i  j ≥ m + 1 − k  j (cid:54)= i}.3

T = {{i  j} : i ≤ k  j (cid:54)= i}.

(a) Full breaking.

(b) Top-3 breaking.

(c) Bottom-3 breaking.

(d) Adjacent breaking.

(e) Position-2 breaking.

Figure 1: Example breakings for m = 6.

Intuitively  the full breaking contains all the pairwise comparisons that can be extracted from each
agent’s full rank information in the ranking; the top-k breaking contains all pairwise comparisons
that can be extracted from the rank provided by an agent when she only reveals her top k alternatives
and the ranking among them; the bottom-k breaking can be computed when an agent only reveals
her bottom k alternatives and the ranking among them; and the position-k breaking can be computed
when the agent only reveals the alternative that is ranked at the kth position and the set of alternatives
ranked in lower positions.

We note that Gm
T = Gm
P .
Gk
l=1 Gl
We are now ready to present our GMM algorithm (Algorithm 1) parameterized by a breaking G.

B = GF   G1

T = G1

P   and for any k ≤ m − 1  Gk

T ∪ Gm−k

B

= GF   and

T =(cid:83)k

2To simplify notation  we use GMMG instead of GMMgG.
3We need k ≥ 2 since Gk

B is empty.

4

6	
3	
4	
5	
2	
1	
6	
3	
4	
5	
2	
1	
6	
3	
4	
5	
2	
1	
6	
3	
4	
5	
2	
1	
6	
3	
4	
5	
2	
1	
Algorithm 1: GMMG(D)
Input: A breaking G and data D = {d1  . . .   dn} composed of full rankings.
Output: Estimation GMMG(D) of parameters under PL.
d∈D PG(d) in Deﬁnition 1.

(cid:80)

1 Compute PG(D) = 1
n
2 Compute GMMG(D) according to (1).
3 return GMMG(D).

Step 2 can be further simpliﬁed according to the following theorem. Due to the space constraints 
most proofs are relegated to the supplementary materials.
Theorem 1. For any breaking G and any data D  there exists (cid:126)γ ∈ ¯Ω such that PG(D) · (cid:126)γ = 0.
Theorem 1 implies that in Equation (1)  inf(cid:126)γ∈Ω g(D  (cid:126)γ)T Wng(D  (cid:126)γ)} = 0. Therefore  Step 2 can
be replaced by: 2∗ Let GMMG = {(cid:126)γ ∈ Ω : PG(D) · (cid:126)γ = 0}.
3.1 Uniqueness of Solution
It is possible that for some data D  GMMG(D) is empty or non-unique. Our next theorem charac-
terizes conditions for |GMMG(D)| = 1 and |GMMG(D)| (cid:54)= ∅. A Markov chain (row stochastic
matrix) M is irreducible  if any state can be reached from any other state. That is  M only has one
communicating class.
Theorem 2. Among the following three conditions  1 and 2 are equivalent for any breaking G and
any data D. Moreover  conditions 1 and 2 are equivalent to condition 3 if and only if G is connected.

1. (I + PG(D)/m)T is irreducible.
2. |GMMG(D)| = 1.
3. GMMG(D) (cid:54)= ∅.
Corollary 1. For the full breaking  adjacent breaking  and any top-k breaking  the three statements
in Theorem 2 are equivalent for any data D. For any position-k (with k ≥ 2) and any bottom-k
(with k ≤ m − 1)  1 and 2 are not equivalent to 3 for some data D.

Ford  Jr. [6] identiﬁed a necessary and sufﬁcient condition on data D for the MLE under PL to be
unique  which is equivalent to condition 1 in Theorem 2. Therefore  we have the following corollary.
Corollary 2. For the full breaking GF   |GMMGF (D)| = 1 if and only if |MLEP L(D)| = 1.
3.2 Consistency
We say a breaking G is consistent (for PL)  if GMMG is consistent (for PL). Below  we show that
some breakings deﬁned in the last subsection are consistent. We start with general results.
Theorem 3. A breaking G is consistent if and only if Ed|(cid:126)γ∗ [g(d  (cid:126)γ∗)] = 0  which is equivalent to
the following equalities:

Pr(ci (cid:31) cj|{Pos(ci  d)  Pos(cj  d)} ∈ G)

Pr(cj (cid:31) ci|{Pos(ci)  Pos(cj)} ∈ G)

γ∗
γ∗

i

j

for all i (cid:54)= j 

=

.

(2)

Theorem 4. Let G1  G2 be a pair of consistent breakings.
1. If G1 ∩ G2 = ∅  then G1 ∪ G2 is also consistent.
2. If G1 (cid:40) G2 and (G2 \ G1) (cid:54)= ∅  then (G2 \ G1) is also consistent.
Continuing  we show that position-k breakings are consistent  then use this and Theorem 4 as build-
ing blocks to prove additional consistency results.
Proposition 1. For any k ≥ 1  the position-k breaking Gk

l=1 Gl

P   GF = Gm

We recall that Gk
. Therefore  we have the
following corollary.
T is consistent  and for any k ≥ 2 
Corollary 3. The full breaking GF is consistent; for any k  Gk
B is consistent.
Gk
Theorem 5. Adjacent breaking GA is consistent if and only if all components in (cid:126)γ∗ are the same.

T   and Gk

T

T = (cid:83)k

P is consistent.
B = GF \ Gm−k

5

Lastly  the technique developed in this section can also provide an independent proof that the RC
algorithm is consistent for BTL  which is implied by the main theorem in [16]:
Corollary 4. [16] The RC algorithm is consistent for BTL.

RC is equivalent to GM MgRC that satisﬁes Condition 1. By checking similar conditions as we did
in the proof of Theorem 3  we can prove that GM MgRC is consistent for BTL.
The results in this section suggest that if we want to learn the parameters of PL  we should use
consistent breakings  including full breaking  top-k breakings  bottom-k breakings  and position-k
breakings. The adjacent breaking seems quite natural  but it is not consistent  thus will not provide a
good estimate to the parameters of PL. This will also be veriﬁed by experimental results in Section 4.

3.3 Complexity
We ﬁrst characterize the computational complexity of our GMMs.
Proposition 2. The computational complexity of the MM algorithm for PL [8] and our GMMs are
listed below.
• MM: O(m3n) per iteration.
• GMM (Algorithm 1) with full breaking: O(m2n + m2.376)  with O(m2n) for breaking and
O(m2.376) for computing step 2∗ in Algorithm 1 (matrix inversion).

• GMM with adjacent breaking: O(mn + m2.376)  with O(mn) for breaking and O(m2.376)

for computing step 2∗ in Algorithm 1.
• GMM with top-k breaking: O((m + k)kn + m2.376)  with O((m + k)kn) for breaking and
O(m2.376) for computing step 2∗ in Algorithm 1.

It follows that the asymptotic complexity of the GMM algorithms is better than for the classical MM
algorithm. In particular  the GMM with adjacent breaking and top-k breaking for constant k’s are
the fastest. However  we recall that the GMM with adjacent breaking is not consistent  while the
other algorithms are consistent. We would expect that as data size grows  the GMM with adjacent
breaking will provide a relatively poor estimation to (cid:126)γ∗ compared to the other methods.
Moreover in the statistical setting in order to gain consistency we need regimes that m = o(n) and
large ns are going to lead to major computational bottlenecks. All the above algorithms (MM and
different GMMs) have linear complexity in n  hence  the coefﬁcient for n is essential in determining
the tradeoffs between these methods. As it can be seen above the coefﬁcient for n is linear in m for
top-k breaking and quadratic for full breaking while it is cubic in m for the MM algorithm. This
difference is illustrated through experiments in Figure 5.
Among GMMs with top-k breakings  the larger the k is  the more information we use in a single
ranking  which comes at a higher computational cost. Therefore  it is natural to conjecture that for
with small k. In other words 
the same data  GMMGk
we expect to see the following time-efﬁciency tradeoff among GMMGk
for different k’s  which is
veriﬁed by the experimental results in the next section.
Conjecture 1. (time-efﬁciency tradeoff) for any k1 < k2  GMMGk1
provides a better estimate to the ground truth.
4 Experiments
The running time and statistical efﬁciency of MM and our GMMs are examined for both synthetic
data and a real-world sushi dataset [9]. The synthetic datasets are generated as follows.
• Generating the ground truth: for m ≤ 300  the ground truth (cid:126)γ∗ is generated from the Dirichlet
distribution Dir((cid:126)1).
• Generating data: given a ground truth (cid:126)γ∗  we generate up to 1000 full rankings from PL.
We implemented MM [8] for 1  3  10 iterations  as well as GMMs with full breaking  adjacent
breaking  and top-k breaking for all k ≤ m − 1.

with large k converges faster than GMMGk

runs faster  while GMMGk2

T

T

T

T

T

6

We focus on the following representative criteria. Let (cid:126)γ denote the output of the algorithm.
• Mean Squared Error: MSE = E((cid:107)(cid:126)γ − (cid:126)γ∗(cid:107)2
2).
• Kendall Rank Correlation Coefﬁcient: Let K((cid:126)γ  (cid:126)γ∗) denote the Kendall tau distance between
the ranking over components in (cid:126)γ and the ranking over components in (cid:126)γ∗. The Kendall correlation
is 1 − 2 K((cid:126)γ (cid:126)γ∗)
m(m−1)/2.
All experiments are run on a 1.86 GHz Intel Core 2 Duo MacBook Air. The multiple repetitions
for the statistical efﬁciency experiments in Figure 3 and experiments for sushi data in Figure 5 have
been done using the odyssey cluster. All the codes are written in R project and they are available as
a part of the package ”StatRank”.
4.1 Synthetic Data
In this subsection we focus on comparisons among MM  GMM-F (full breaking)  and GMM-A
(adjacent breaking). The running time is presented in Figure 2. We observe that GMM-A (adjacent
breaking) is the fastest and MM is the slowest  even for one iteration.
The statistical efﬁciency is shown in Figure 3. We observe that in regard to the MSE criterion 
GMM-F (full breaking) performs as well as MM for 10 iterations (which converges)  and that these
are both better than GMM-A (adjacent breaking). For the Kendall correlation criterion  GMM-F (full
breaking) has the best performance and GMM-A (adjacent breaking) has the worst performance.
Statistics are calculated over 1840 trials. In all cases except one  GMM-F (full breaking) outperforms
MM which outperforms GMM-A (adjacent breaking) with statistical signiﬁcance at 95% conﬁdence.
The only exception is between GMM-F (full breaking) and MM for Kendall correlation at n = 1000.

Figure 2: The running time of MM (one iteration)  GMM-F (full breaking)  and GMM-A (adjacent breaking) 
plotted in log-scale. On the left  m is ﬁxed at 10. On the right  n is ﬁxed at 10. 95% conﬁdence intervals are
too small to be seen. Times are calculated over 20 datasets.

Figure 3: The MSE and Kendall correlation of MM (10 iterations)  GMM-F (full breaking)  and GMM-A
(adjacent breaking). Error bars are 95% conﬁdence intervals.
4.2 Time-Efﬁciency Tradeoff among Top-k Breakings
Results on the running time and statistical efﬁciency for top-k breakings are shown in Figure 4. We
recall that top-1 is equivalent to position-1  and top-(m − 1) is equivalent to the full breaking.
For n = 100  MSE comparisons between successive top-k breakings are statistically signiﬁcant at
95% level from (top-1  top-2) to (top-6  top-7). The comparisons in running time are all signiﬁcant at
95% conﬁdence level. On average  we observe that top-k breakings with smaller k run faster  while
top-k breakings with larger k have higher statistical efﬁciency in both MSE and Kendall correlation.
This justiﬁes Conjecture 1.

7

0.010.101.002505007501000n (agents)GMM−AGMM−FMMTime (log scale) (s)0.011.00255075100m (alternatives)Time (log scale) (s)0.0000.0010.0020.0030.0042505007501000n (agents)GMM−AGMM−FMMMSE0.50.60.70.82505007501000n (agents)Kendall Correlation4.3 Experiments for Real Data
In the sushi dataset [9]  there are 10 kinds of sushi (m = 10) and the amount of data n is varied 
randomly sampling with replacement. We set the ground truth to be the output of MM applied to
all 5000 data points. For the running time  we observe the same as for the synthetic data: GMM
(adjacent breaking) runs faster than GMM (full breaking)  which runs faster than MM (The results
on running time can be found in supplementary material B).
Comparisons for MSE and Kendall correlation are shown in Figure 5. In both ﬁgures  95% conﬁ-
dence intervals are plotted but too small to be seen. Statistics are calculated over 1970 trials.

Figure 4: Comparison of GMM with top-k breakings as k is varied. The x-axis represents k in the top-k
breaking. Error bars are 95% conﬁdence intervals and m = 10  n = 100.

Figure 5: The MSE and Kendall correlation criteria and computation time for MM (10 iterations)  GMM-F
(full breaking)  and GMM-A (adjacent breaking) on sushi data.

For MSE and Kendall correlation  we observe that MM converges fastest  followed by GMM (full
breaking)  which outperforms GMM (adjacent breaking) which does not converge. Differences be-
tween performances are all statistically signiﬁcant with 95% conﬁdence (with exception of Kendall
correlation and both GMM methods for n = 200  where p = 0.07). This is different from com-
parisons for synthetic data (Figure 3). We believe that the main reason is because PL does not ﬁt
sushi data well  which is a fact recently observed by Azari et al. [1]. Therefore  we cannot ex-
pect that GMM converges to the output of MM on the sushi dataset  since the consistency results
(Corollary 3) assumes that the data is generated under PL.
5 Future Work
We plan to work on the connection between consistent breakings and preference elicitation. For
example  even though the theory in this paper is developed for full ranks  the notion of top-k and
bottom-k breaking are implicitly allowing some partial order settings. More speciﬁcally  top-k
breaking can be achieved from partial orders that include full rankings for the top-k alternatives.
Acknowledgments
This work is supported in part by NSF Grants No. CCF- 0915016 and No. AF-1301976. Lirong
Xia acknowledges NSF under Grant No. 1136996 to the Computing Research Association for the
CIFellows project and an RPI startup fund. We thank Joseph K. Blitzstein  Edoardo M. Airoldi 
Ryan P. Adams  Devavrat Shah  Yiling Chen  G´abor C´ardi and members of Harvard EconCS group
for their comments on different aspects of this work. We thank anonymous NIPS-13 reviewers  for
helpful comments and suggestions.

8

0.00000.00050.00100.00150.00200.0025123456789k (Top k Breaking)MSE (n = 100)0.60.70.8123456789k (Top k Breaking)Kendall Correlation (n = 100)0.10.20.30.4123456789k (Top k Breaking)Time (n = 100)0.00000.00050.00100.001510002000300040005000n (agents)GMM−AGMM−FMMMSE0.40.60.81.010002000300040005000n (agents)Kendall Correlation010020010002000300040005000n (agents)TimeReferences

[1] Hossein Azari Souﬁani  David C. Parkes  and Lirong Xia. Random utility theory for social choice. In
Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)  pages 126–
134  Lake Tahoe  NV  USA  2012.

[2] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. The method of

paired comparisons. Biometrika  39(3/4):324–345  1952.

[3] Marquis de Condorcet. Essai sur l’application de l’analyse `a la probabilit´e des d´ecisions rendues `a la

pluralit´e des voix. Paris: L’Imprimerie Royale  1785.

[4] Cynthia Dwork  Ravi Kumar  Moni Naor  and D. Sivakumar. Rank aggregation methods for the web. In

Proceedings of the 10th World Wide Web Conference  pages 613–622  2001.

[5] Patricia Everaere  S´ebastien Konieczny  and Pierre Marquis. The strategy-proofness landscape of merg-

ing. Journal of Artiﬁcial Intelligence Research  28:49–105  2007.

[6] Lester R. Ford  Jr. Solution of a ranking problem from binary comparisons. The American Mathematical

Monthly  64(8):28–33  1957.

[7] Lars Peter Hansen. Large Sample Properties of Generalized Method of Moments Estimators. Economet-

rica  50(4):1029–1054  1982.

[8] David R. Hunter. MM algorithms for generalized Bradley-Terry models.

volume 32  pages 384–406  2004.

In The Annals of Statistics 

[9] Toshihiro Kamishima. Nantonac collaborative ﬁltering: Recommendation based on order responses. In
Proceedings of the Ninth International Conference on Knowledge Discovery and Data Mining (KDD) 
pages 583–588  Washington  DC  USA  2003.

[10] David A. Levin  Yuval Peres  and Elizabeth L. Wilmer. Markov Chains and Mixing Times. American

Mathematical Society  2008.

[11] Tie-Yan Liu. Learning to Rank for Information Retrieval. Springer  2011.
[12] Tyler Lu and Craig Boutilier. Learning Mallows models with pairwise preferences. In Proceedings of the
Twenty-Eighth International Conference on Machine Learning (ICML 2011)  pages 145–152  Bellevue 
WA  USA  2011.

[13] Robert Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley  1959.
[14] Colin L. Mallows. Non-null ranking model. Biometrika  44(1/2):114–130  1957.
[15] Andrew Mao  Ariel D. Procaccia  and Yiling Chen. Better human computation through principled voting.
In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI)  Bellevue  WA  USA  2013.
[16] Sahand Negahban  Sewoong Oh  and Devavrat Shah. Iterative ranking from pair-wise comparisons. In
Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)  pages 2483–
2491  Lake Tahoe  NV  USA  2012.

[17] Robin L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society. Series C

(Applied Statistics)  24(2):193–202  1975.

[18] Louis Leon Thurstone. A law of comparative judgement. Psychological Review  34(4):273–286  1927.

9

,Hossein Azari Soufiani
William Chen
David Parkes
Lirong Xia
Gabriel Goh
Andrew Cotter
Maya Gupta
Michael Friedlander
Jack Goetz
Ambuj Tewari
Paul Zimmerman