2010,LSTD with Random Projections,We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular  we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm.,LSTD with Random Projections

Mohammad Ghavamzadeh  Alessandro Lazaric  Odalric-Ambrym Maillard  R´emi Munos

INRIA Lille - Nord Europe  Team SequeL  France

Abstract

We consider the problem of reinforcement learning in high-dimensional spaces
when the number of features is bigger than the number of samples. In particular 
we study the least-squares temporal difference (LSTD) learning algorithm when
a space of low dimension is generated with a random projection from a high-
dimensional space. We provide a thorough theoretical analysis of the LSTD with
random projections and derive performance bounds for the resulting algorithm.
We also show how the error of LSTD with random projections is propagated
through the iterations of a policy iteration algorithm and provide a performance
bound for the resulting least-squares policy iteration (LSPI) algorithm.

Introduction

1
Least-squares temporal difference (LSTD) learning [3  2] is a widely used reinforcement learning
(RL) algorithm for learning the value function V π of a given policy π. LSTD has been successfully
applied to a number of problems especially after the development of the least-squares policy iteration
(LSPI) algorithm [9]  which extends LSTD to control problems by using it in the policy evaluation
step of policy iteration. More precisely  LSTD computes the ﬁxed point of the operator ΠT π  where
T π is the Bellman operator of policy π and Π is the projection operator onto a linear function
space. The choice of the linear function space has a major impact on the accuracy of the value
function estimated by LSTD  and thus  on the quality of the policy learned by LSPI. The problem
of ﬁnding the right space  or in other words the problems of feature selection and discovery  is an
important challenge in many areas of machine learning including RL  or more speciﬁcally  linear
value function approximation in RL.
To address this issue in RL  many researchers have focused on feature extraction and learning.
Mahadevan [13] proposed a constructive method for generating features based on the eigenfunctions
of the Laplace-Beltrami operator of the graph built from observed system trajectories. Menache et
al. [16] presented a method that starts with a set of features and then tunes both features and the
weights using either gradient descent or the cross-entropy method. Keller et al. [7] proposed an
algorithm in which the state space is repeatedly projected onto a lower dimensional space based on
the Bellman error and then states are aggregated in this space to deﬁne new features. Finally  Parr et
al. [17] presented a method that iteratively adds features to a linear approximation architecture such
that each new feature is derived from the Bellman error of the existing set of features.
A more recent approach to feature selection and discovery in value function approximation in RL is
to solve RL in high-dimensional feature spaces. The basic idea here is to use a large number of fea-
tures and then exploit the regularities in the problem to solve it efﬁciently in this high-dimensional
space. Theoretically speaking  increasing the size of the function space can reduce the approxima-
tion error (the distance between the target function and the space) at the cost of a growth in the
estimation error. In practice  in the typical high-dimensional learning scenario when the number of
features is larger than the number of samples  this often leads to the overﬁtting problem and poor
prediction performance. To overcome this problem  several approaches have been proposed includ-
ing regularization. Both (cid:96)1 and (cid:96)2 regularizations have been studied in value function approximation
in RL. Farahmand et al. presented several (cid:96)2-regularized RL algorithms by adding (cid:96)2-regularization
to LSTD and modiﬁed Bellman residual minimization [4] as well as ﬁtted value iteration [5]  and
proved ﬁnite-sample performance bounds for their algorithms. There have also been algorithmic
work on adding (cid:96)1-penalties to the TD [12]  LSTD [8]  and linear programming [18] algorithms.

1

In this paper  we follow a different approach based on random projections [21]. In particular  we
study the performance of LSTD with random projections (LSTD-RP). Given a high-dimensional
linear space F  LSTD-RP learns the value function of a given policy from a small (relative to the
dimension of F) number of samples in a space G of lower dimension obtained by linear random
projection of the features of F. We prove that solving the problem in the low dimensional random
space instead of the original high-dimensional space reduces the estimation error at the price of a
“controlled” increase in the approximation error of the original space F. We present the LSTD-
RP algorithm and discuss its computational complexity in Section 3. In Section 4  we provide the
ﬁnite-sample analysis of the algorithm. Finally in Section 5  we show how the error of LSTD-RP is
propagated through the iterations of LSPI.
2 Preliminaries
For a measurable space with domain X   we let S(X ) and B(X ; L) denote the set of probability
measures over X and the space of measurable functions with domain X and bounded in absolute
value by 0 < L < ∞  respectively. For a measure µ ∈ S(X ) and a measurable function f :
X → R  we deﬁne the (cid:96)2(µ)-norm of f as ||f||2
||f||∞ = supx∈X |f (x)|  and for a set of n states X1  . . .   Xn ∈ X the empirical norm of f as
||f||2
i .
i=1 u2
We consider the standard RL framework [20] in which a learning agent interacts with a stochastic
environment and this interaction is modeled as a discrete-time discounted Markov decision process
(MDP). A discount MDP is a tuple M = (cid:104)X  A  r  P  γ(cid:105) where the state space X is a bounded closed
subset of a Euclidean space  A is a ﬁnite (|A| < ∞) action space  the reward function r : X×A → R
is uniformly bounded by Rmax  the transition kernel P is such that for all x ∈ X and a ∈ A 
P (·|x  a) is a distribution over X   and γ ∈ (0  1) is a discount factor. A deterministic policy π :
X → A is a mapping from states to actions. Under a policy π  the MDP M is reduced to a Markov

(cid:80)n
t=1 f (Xt)2. Moreover  for a vector u ∈ Rn we write its (cid:96)2-norm as ||u||2

µ = (cid:82) f (x)2µ(dx)  the supremum norm of f as

2 =(cid:80)n

n = 1
n

1−γ ) → B(X ; Vmax) deﬁned
the unique ﬁxed-point of the Bellman operator T π : B(X ; Vmax = Rmax
X P π(dy|x)V (y). We also deﬁne the optimal value function V ∗ as
the unique ﬁxed-point of the optimal Bellman operator T ∗ : B(X ; Vmax) → B(X ; Vmax) deﬁned

chain Mπ = (cid:104)X   Rπ  P π  γ(cid:105) with reward Rπ(x) = r(cid:0)x  π(x)(cid:1)  transition kernel P π(·|x) = P(cid:0) ·
|x  π(x)(cid:1)  and stationary distribution ρπ (if it admits one). The value function of a policy π  V π  is
by (T πV )(x) = Rπ(x) + γ(cid:82)
by (T ∗V )(x) = maxa∈A(cid:2)r(x  a) + γ(cid:82)
X P (dy|x  a)V (y)(cid:3). Finally  we denote by T the truncation
operator at threshold Vmax  i.e.  if |f (x)| > Vmax then T (f )(x) = sgn(cid:0)f (x)(cid:1)Vmax.
where φ(·) = (cid:0)ϕ1(·)  . . .   ϕD(·)(cid:1)(cid:62)
the feature vector Ψ (·) = (cid:0)ψ1(·)  . . .   ψd(·)(cid:1)(cid:62)

To approximate a value function V ∈ B(X ; Vmax)  we ﬁrst deﬁne a linear function space F spanned
by the basis functions ϕj ∈ B(X ; L)  j = 1  . . .   D  i.e.  F = {fα | fα(·) = φ(·)(cid:62)α  α ∈ RD} 
is the feature vector. We deﬁne the orthogonal projection of
V onto the space F w.r.t. norm µ as ΠF V = arg minf∈F ||V − f||µ. From F we can gener-
ate a d-dimensional (d < D) random space G = {gβ | gβ(·) = Ψ (·)(cid:62)β  β ∈ Rd}  where
is deﬁned as Ψ (·) = Aφ(·) with A ∈ Rd×D
be a random matrix whose elements are drawn i.i.d. from a suitable distribution  e.g.  Gaussian
N (0  1/d). Similar to the space F  we deﬁne the orthogonal projection of V onto the space G
w.r.t. norm µ as ΠGV = arg ming∈G ||V − g||µ. Finally  for any function fα ∈ F  we deﬁne
m(fα) = ||α||2 supx∈X ||φ(x)||2.
3 LSTD with Random Projections
The objective of LSTD with random projections (LSTD-RP) is to learn the value function of a
given policy from a small (relative to the dimension of the original space) number of samples in a
low-dimensional linear space deﬁned by a random projection of the high-dimensional space. We
show that solving the problem in the low dimensional space instead of the original high-dimensional
space reduces the estimation error at the price of a “controlled” increase in the approximation error.
In this section  we introduce the notations and the resulting algorithm  and discuss its computational
complexity. In Section 4  we provide the ﬁnite-sample analysis of the algorithm.
We use the linear spaces F and G with dimensions D and d (d < D) as deﬁned in Section 2. Since in
the following the policy is ﬁxed  we drop the dependency of Rπ  P π  V π  and T π on π and simply
use R  P   V   and T . Let {Xt}n
t=1 be a sample path (or trajectory) of size n generated by the Markov

2

n = 1
n

t=1 y2
feature matrix deﬁned at {Xt}n

chain Mπ  and let v ∈ Rn and r ∈ Rn  deﬁned as vt = V (Xt) and rt = R(Xt)  be the value and
reward vectors of this trajectory. Also  let Ψ = [Ψ (X1)(cid:62); . . . ; Ψ (Xn)(cid:62)] be the feature matrix
denote by(cid:98)ΠG : Rn → Gn the orthogonal projection onto Gn  deﬁned by(cid:98)ΠGy = arg minz∈Gn ||y −
deﬁned at these n states and Gn = {Ψβ | β ∈ Rd} ⊂ Rn be the corresponding vector space. We
(cid:80)n
{Φα | α ∈ RD} as (cid:98)ΠF y = arg minz∈Fn ||y − z||n  where Φ = [φ(X1)(cid:62); . . . ; φ(Xn)(cid:62)] is the
z||n  where ||y||2
t . Similarly  we can deﬁne the orthogonal projection onto Fn =
t=1. Note that for any y ∈ Rn  the orthogonal projections (cid:98)ΠGy and
(cid:98)ΠF y exist and are unique.
empirical operator(cid:98)ΠG(cid:98)T   where (cid:98)T is the pathwise Bellman operator deﬁned as (cid:98)T y = r + γ(cid:98)P y. The
operator (cid:98)P : Rn → Rn is deﬁned as ((cid:98)P y)t = yt+1 for 1 ≤ t < n and ((cid:98)P y)n = 0. As shown
in [11]  (cid:98)T is a γ-contraction in (cid:96)2-norm  thus together with the non-expansive property of (cid:98)ΠG  it
guarantees the existence and uniqueness of the pathwise-LSTD ﬁxed point ˆv ∈ Rn  ˆv = (cid:98)ΠG(cid:98)T ˆv.
LSTD-RP(cid:0)D  d {Xt}n

We consider the pathwise-LSTD algorithm introduced in [11]. Pathwise-LSTD takes a single tra-
jectory {Xt}n
t=1 of size n generated by the Markov chain as input and returns the ﬁxed point of the

Note that the uniqueness of ˆv does not imply the uniqueness of the parameter ˆβ such that ˆv = Ψ ˆβ.

t=1  φ  γ(cid:1)

t=1 {R(Xt)}n

Cost

Compute

• the reward vector rn×1 ; rt = R(Xt)
• the high-dimensional feature matrix Φn×D = [φ(X1)(cid:62); . . . ; φ(Xn)(cid:62)]
• the projection matrix Ad×D whose elements are i.i.d. samples from N (0  1/d)
• the low-dim feature matrix Ψn×d = [Ψ (X1)(cid:62); . . . ; Ψ (Xn)(cid:62)] ; Ψ (·) = Aφ(·)
• ˜Ad×d = Ψ(cid:62)(Ψ − γΨ(cid:48))

O(n)
O(nD)
O(dD)
O(ndD)
O(nd)
O(nd + nd2) + O(nd)
return either ˆβ = ˜A−1˜b or ˆβ = ˜A+˜b ( ˜A+ is the Moore-Penrose pseudo-inverse of ˜A) O(d2 + d3)

• the matrix (cid:98)P Ψ = Ψ(cid:48)

n×d = [Ψ (X2)(cid:62); . . . ; Ψ (Xn)(cid:62); 0(cid:62)]

˜bd×1 = Ψ(cid:62)r

 

Figure 1: The pseudo-code of the LSTD with random projections (LSTD-RP) algorithm.

√

Figure 1 contains the pseudo-code and the computational cost of the LSTD-RP algorithm. The total
computational cost of LSTD-RP is O(d3 + ndD)  while the computational cost of LSTD in the
high-dimensional space F is O(D3 + nD2). As we will see  the analysis of Section 4 suggests
that the value of d should be set to O(
n). In this case the numerical complexity of LSTD-RP is
O(n3/2D)  which is better than O(D3)  the cost of LSTD in F when n < D (the case considered
in this paper). Note that the cost of making a prediction is D in LSTD in F and dD in LSTD-RP.
4 Finite-Sample Analysis of LSTD with Random Projections
In this section  we report the main theoretical results of the paper. In particular  we derive a per-
formance bound for LSTD-RP in the Markov design setting  i.e.  when the LSTD-RP solution is
compared to the true value function only at the states belonging to the trajectory used by the al-
gorithm (see Section 4 in [11] for a more detailed discussion). We then derive a condition on the
number of samples to guarantee the uniqueness of the LSTD-RP solution. Finally  from the Markov
design bound we obtain generalization bounds when the Markov chain has a stationary distribution.

4.1 Markov Design Bound
Theorem 1. Let F and G be linear spaces with dimensions D and d (d < D) as deﬁned in Section 2.
t=1 be a sample path generated by the Markov chain Mπ  and v  ˆv ∈ Rn be the vectors
Let {Xt}n
whose components are the value function and the LSTD-RP solution at {Xt}n
t=1. Then for any
δ > 0  whenever d ≥ 15 log(8n/δ)  with probability 1− δ (the randomness is w.r.t. both the random
(cid:32)(cid:114)
(cid:33)
sample path and the random projection)  ˆv satisﬁes

(cid:114)

(cid:35)

(cid:114) d

(cid:34)
||v −(cid:98)ΠF v||n +

8 log(8n/δ)

m((cid:98)ΠF v)

||v−ˆv||n ≤

1(cid:112)1 − γ2

+

γVmaxL
1 − γ

νn

n

8 log(4d/δ)

+

 

1
n
(1)

d

3

where the random variable νn is the smallest strictly positive eigenvalue of the sample-based Gram
matrix 1

n Ψ(cid:62)Ψ. Note that m((cid:98)ΠF v) = m(fα) with fα be any function in F such that fα(Xt) =

((cid:98)ΠF v)t for 1 ≤ t ≤ n.

Before stating the proof of Theorem 1  we need to prove the following lemma.
Lemma 1. Let F and G be linear spaces with dimensions D and d (d < D) as deﬁned in Section 2.
Let {Xi}n
i=1 be n states and fα ∈ F. Then for any δ > 0  whenever d ≥ 15 log(4n/δ)  with
probability 1 − δ (the randomness is w.r.t. the random projection)  we have

g∈G ||fα − g||2

inf

n ≤ 8 log(4n/δ)

d

m(fα)2.

(2)

Proof. The proof relies on the application of a variant of Johnson-Lindenstrauss (JL) lemma which
states that the inner-products are approximately preserved by the application of the random matrix
d log(4n/δ). Thus for d ≥
A (see e.g.  Proposition 1 in [14]). For any δ > 0  we set 2 = 8
15 log(4n/δ)  we have  ≤ 3/4 and as a result 2/4 − 3/6 ≥ 2/8 and d ≥ log(4n/δ)
2/4−3/6. Thus  from
Proposition 1 in [14]  for all 1 ≤ i ≤ n  we have |φ(Xi) · α − Aφ(Xi) · Aα| ≤ ||α||2||φ(Xi)||2 ≤
 m(fα) with high probability. From this result  we deduce that with probability 1 − δ
|φ(Xi) · α − Aφ(Xi) · Aα|2 ≤ 8 log(4n/δ)

n ≤ ||fα − gAα||2

n(cid:88)

m(fα)2.

n =

g∈G ||fα − g||2

inf

d

1
n

i=1

Proof of Theorem 1. For any ﬁxed space G  the performance of the LSTD-RP solution can be
bounded according to Theorem 1 in [10] as

||v − ˆv||n ≤

1(cid:112)1 − γ2

||v −(cid:98)ΠGv||n +

γVmaxL
1 − γ

(cid:114) d

(cid:16)(cid:114)

νn

(cid:17)

 

+

1
n

8 log(2d/δ(cid:48))

n

(3)

with probability 1 − δ(cid:48) (w.r.t. the random sample path). From the triangle inequality  we have
||v −(cid:98)ΠGv||n ≤ ||v −(cid:98)ΠF v||n + ||(cid:98)ΠF v −(cid:98)ΠGv||n = ||v −(cid:98)ΠF v||n + ||(cid:98)ΠF v −(cid:98)ΠG((cid:98)ΠF v)||n.
n. Since ||v−(cid:98)ΠF v||n is independent of g  we have arg inf g∈G ||v−g||2
||v−(cid:98)ΠF v||2
n +||(cid:98)ΠF v−g||2
The equality in Eq. 4 comes from the fact that for any vector g ∈ G  we can write ||v − g||2
n  and thus (cid:98)ΠGv = (cid:98)ΠG((cid:98)ΠF v). From Lemma 1  if d ≥ 15 log(4n/δ(cid:48)(cid:48))  with
arg inf g∈G ||(cid:98)ΠF v − g||2
(cid:114)
probability 1 − δ(cid:48)(cid:48) (w.r.t. the choice of A)  we have
||(cid:98)ΠF v −(cid:98)ΠG((cid:98)ΠF v)||n ≤

m((cid:98)ΠF v).

8 log(4n/δ(cid:48)(cid:48))

n =
n =

(5)

(4)

d

We conclude from a union bound argument that Eqs. 3 and 5 hold simultaneously with probability
at least 1 − δ(cid:48) − δ(cid:48)(cid:48). The claim follows by combining Eqs. 3–5  and setting δ(cid:48) = δ(cid:48)(cid:48) = δ/2.

1
1 − γ

||v − ¯v||n ≤

1(cid:112)1 − γ2

O((cid:112)D/n).

||v −(cid:98)ΠF v||n +

Remark 1. Using Theorem 1  we can compare the performance of LSTD-RP with the performance
of LSTD directly applied in the high-dimensional space F. Let ¯v be the LSTD solution in F  then
up to constants  logarithmic  and dominated factors  with high probability  ¯v satisﬁes

By comparing Eqs. 1 and 6  we notice that 1) the estimation error of ˆv is of order O((cid:112)d/n)  and
thus  is smaller than the estimation error of ¯v  which is of order O((cid:112)D/n)  and 2) the approximation
error of ˆv is the approximation error of ¯v  ||v − (cid:98)ΠF v||n  plus an additional term that depends on
m((cid:98)ΠF v) and decreases with d  the dimensionality of G  with the rate O((cid:112)1/d). Hence  LSTD-RP
the gain achieved in the estimation error. Note that m((cid:98)ΠF v) highly depends on the value function

may have a better performance than solving LSTD in F whenever this additional term is smaller than
V that is being approximated and the features of the space F. It is important to carefully tune the
value of d as both the estimation error and the additional approximation error in Eq. 1 depend on
d. For instance  while a small value of d signiﬁcantly reduces the estimation error (and the need for
samples)  it may amplify the additional approximation error term  and thus  reduce the advantage of
LSTD-RP over LSTD. We may get an idea on how to select the value of d by optimizing the bound

(6)

4

(cid:115)

m((cid:98)ΠF v)

nνn(1 − γ)

.

d =

(7)

(8)

nνn(1 + γ)

1
1 − γ

1(cid:112)1 − γ2

√
Therefore  when n samples are available the optimal value for d is of the order O(
value of d in Eq. 7  we can rewrite the bound of Eq. 1 as (up to the dominated term 1/n)
1 − γ

(cid:113)
γVmaxL m((cid:98)ΠF v)(cid:0)

(cid:112)8 log(8n/δ)

||v −(cid:98)ΠF v||n +

||v − ˆv||n ≤

γVmaxL

1 + γ

(cid:1)1/4.

n). Using the

F  and observe the role of the term m((cid:98)ΠF v). For further discussion on m((cid:98)ΠF v) refer to [14] and

Using Eqs. 6 and 8  it would be easier to compare the performance of LSTD-RP and LSTD in space
for the case of D = ∞ to Section 4.3 of this paper.
Remark 2. As discussed in the introduction  when the dimensionality D of F is much bigger than
the number of samples n  the learning algorithms are likely to overﬁt the data. In this case  it is
reasonable to assume that the target vector v itself belongs to the vector space Fn. We state this
condition using the following assumption:
Assumption 1. (Overﬁtting). For any set of n points {Xi}n
i=1  there exists a function f ∈ F such
that f (Xi) = V (Xi)  1 ≤ i ≤ n .
n Φ(cid:62)Φ to be
Assumption 1 is equivalent to require that the rank of the empirical Gram matrices 1
bigger than n. Note that Assumption 1 is likely to hold whenever D (cid:29) n  because in this case we
can expect that the features to be independent enough on {Xi}n
n Φ(cid:62)Φ to be
bigger than n (e.g.  if the features are linearly independent on the samples  it is sufﬁcient to have
D ≥ n). Under Assumption 1 we can remove the empirical approximation error term in Theorem 1
and deduce the following result.
Corollary 1. Under Assumption 1 and the conditions of Theorem 1  with probability 1− δ (w.r.t. the
random sample path and the random space)  ˆv satisﬁes

i=1 so that the rank of 1

(cid:114)

1(cid:112)1 − γ2

m((cid:98)ΠF v) +

8 log(8n/δ)

d

||v − ˆv||n ≤

(cid:114) d

(cid:16)(cid:114)

γVmaxL
1 − γ

8 log(4d/δ)

νn

n

(cid:17)

.

+

1
n

4.2 Uniqueness of the LSTD-RP Solution
While the results in the previous section hold for any Markov chain  in this section we assume that
the Markov chain Mπ admits a stationary distribution ρ and is exponentially fast β-mixing with
parameters ¯β  b  κ  i.e.  its β-mixing coefﬁcients satisfy βi ≤ ¯β exp(−biκ) (see e.g.  Sections 8.2
and 8.3 in [10] for a more detailed deﬁnition of β-mixing processes). As shown in [11  10]  if
ρ exists  it would be possible to derive a condition for the existence and uniqueness of the LSTD
solution depending on the number of samples and the smallest eigenvalue of the Gram matrix deﬁned

according to the stationary distribution ρ  i.e.  G ∈ RD×D   Gij = (cid:82) ϕi(x)ϕj(x)ρ(dx). We

smallest eigenvalue of the Gram matrix H ∈ Rd×d   Hij = (cid:82) ψi(x)ψj(x)ρ(dx) in the random
Section 2 with D > d + 2(cid:112)2d log(2/δ) + 2 log(2/δ). Let the elements of the projection matrix A be

now discuss the existence and uniqueness of the LSTD-RP solution. Note that as D increases  the
smallest eigenvalue of G is likely to become smaller and smaller. In fact  the more the features in F 
the higher the chance for some of them to be correlated under ρ  thus leading to an ill-conditioned
matrix G. On the other hand  since d < D  the probability that d independent random combinations
of ϕi lead to highly correlated features ψj is relatively small. In the following we prove that the
space G is indeed bigger than the smallest eigenvalue of G with high probability.
Lemma 2. Let δ > 0 and F and G be linear spaces with dimensions D and d (d < D) as deﬁned in
Gaussian random variables drawn from N (0  1/d). Let the Markov chain Mπ admit a stationary
distribution ρ. Let G and H be the Gram matrices according to ρ for the spaces F and G  and ω
and χ be their smallest eigenvalues. We have with probability 1 − δ (w.r.t. the random space)

(cid:32)

(cid:114)

(cid:114)

(cid:33)2

χ ≥ D
d

ω

1 −

−

d
D

2 log(2/δ)

D

.

(9)

Proof. Let β ∈ Rd be the eigenvector associated to the smallest eigenvalue χ of H  from the
deﬁnition of the features Ψ of G (H = AGA(cid:62)) and linear algebra  we obtain

5

√

where ξ is the smallest eigenvalue of the random matrix AA(cid:62)  or in other words 
ξ is the smallest
singular value of the D × d random matrix A(cid:62)  i.e.  smin(A(cid:62)) =
dA.
Note that if the elements of A are drawn from the Gaussian distribution N (0  1/d)  the elements
of B are standard Gaussian random variables  and thus  the smallest eigenvalue of AA(cid:62)  ξ  can be
min(B(cid:62))/d. There has been extensive work on extreme singular values of random
written as ξ = s2
matrices (see e.g.  [19]). For a D × d random matrix with independent standard normal random
variables  such as B(cid:62)  we have with probability 1 − δ (see [19] for more details)

ξ. We now deﬁne B =

√

√

(cid:17)
d −(cid:112)2 log(2/δ)
(cid:33)2
(cid:114)
From Eq. 11 and the relation between ξ and smin(B(cid:62))  we obtain

) ≥(cid:16)√
(cid:32)
(cid:114)

D −

smin(B

√

(cid:62)

.

(11)

(12)

 

χ||β||2

2 = β

(cid:62)

(cid:62)

χβ = β

Hβ = β

(cid:62)

(cid:62)

AGA

β ≥ ω||A
(cid:62)

β||2

2 = ω β

(cid:62)

(cid:62)

AA

β ≥ ω ξ ||β||2
2  

(10)

with probability 1 − δ. The claim follows by replacing the bound for ξ from Eq. 12 in Eq. 10.

ξ ≥ D
d

1 −

−

d
D

2 log(2/δ)

D

The result of Lemma 2 is for Gaussian random matrices. However  it would be possible to extend
this result using non-asymptotic bounds for the extreme singular values of more general random
matrices [19]. Note that in Eq. 9  D/d is always greater than 1 and the term in the parenthesis
approaches 1 for large values of D. Thus  we can conclude that with high probability the smallest
eigenvalue χ of the Gram matrix H of the randomly generated low-dimensional space G is bigger
than the smallest eigenvalue ω of the Gram matrix G of the high-dimensional space F.
Lemma 3. Let δ > 0 and F and G be linear spaces with dimensions D and d (d < D) as deﬁned in
Gaussian random variables drawn from N (0  1/d). Let the Markov chain Mπ admit a stationary
distribution ρ. Let G be the Gram matrix according to ρ for space F and ω be its smallest eigenvalue.
Let {Xt}n
t=1 be a trajectory of length n generated by a stationary β-mixing process with stationary
distribution ρ. If the number of samples n satisﬁes

Section 2 with D > d + 2(cid:112)2d log(2/δ) + 2 log(2/δ). Let the elements of the projection matrix A be

(cid:26) Λ(n  d  δ/2)
δ + log+(cid:0)max{18(6e)2(d+1)  ¯β}(cid:1)  then with probability
 − 6L

t=1  i.e.  ||gβ||n = 0
n Ψ(cid:62)Ψ satisiﬁes
(cid:41)1/κ

where Λ(n  d  δ) = 2(d + 1) log n + log e
1 − δ  the features ψ1  . . .   ψd are linearly independent on the states {Xt}n
(cid:40)
implies β = 0  and the smallest eigenvalue νn of the sample-based Gram matrix 1
νn ≥ √
√

n >

288L2 d Λ(n  d  δ/2)

ωD

max

(cid:118)(cid:117)(cid:117)(cid:116) 2Λ(n  d  δ

1 −

(cid:27)1/κ(cid:32)

(cid:33)−2

Λ(n  d  δ
2 )

2 log( 2
δ )

2 log(2/δ)

(cid:115)

−

d
D

(cid:114)

(cid:114)

 

(13)

2 )

max

  1

> 0 .

(cid:114)

(cid:114)

1 −

ν =

D

  1

b

√
ω
2

D
d

n

b

−

d
D

D

(14)
Proof. The proof follows similar steps as in Lemma 4 in [10]. A sketch of the proof is available
in [6].

n Φ(cid:62)Φ in the high-dimensional space F.

By comparing Eq. 13 with Eq. 13 in [10]  we can see that the number of samples needed for the
n Ψ(cid:62)Ψ in G to be invertible with high probability is less than that for its
empirical Gram matrix 1
counterpart 1
4.3 Generalization Bound
In this section  we show how Theorem 1 can be generalized to the entire state space X when the
Markov chain Mπ has a stationary distribution ρ. We consider the case in which the samples
{Xt}n
t=1 are obtained by following a single trajectory in the stationary regime of Mπ  i.e.  when X1
is drawn from ρ. As discussed in Remark 2 of Section 4.1  it is reasonable to assume that the high-
dimensional space F contains functions that are able to perfectly ﬁt the value function V in any ﬁnite
number n (n < D) of states {Xt}n

t=1  thus we state the following theorem under Assumption 1.

6

Theorem 2. Let δ > 0 and F and G be linear spaces with dimensions D and d (d < D) as deﬁned
in Section 2 with d ≥ 15 log(8n/δ). Let {Xt}n
t=1 be a path generated by a stationary β-mixing
process with stationary distribution ρ. Let ˆV be the LSTD-RP solution in the random space G. Then
under Assumption 1  with probability 1 − δ (w.r.t. the random sample path and the random space) 
||V − T ( ˆV )||ρ ≤

d
ν
where ν is a lower bound on the eigenvalues of the Gram matrix 1

2(cid:112)1 − γ2

m(ΠF V ) +

8 log(24n/δ)

+    (15)

(cid:114)

n

(cid:17)

(cid:114)

2γVmaxL
1 − γ

(cid:16)(cid:114)
n Ψ(cid:62)Ψ deﬁned by Eq. 14 and
(cid:27)1/κ
(cid:26) Λ(n  d  δ/3)

8 log(12d/δ)

1
n

+

d

(cid:115)

 = 24Vmax

2Λ(n  d  δ/3)

n

max

b

  1

.

with Λ(n  d  δ) deﬁned as in Lemma 3. Note that T in Eq. 15 is the truncation operator deﬁned in
Section 2.

Proof. The proof is a consequence of applying concentration of measures inequalities for β-mixing
processes and linear spaces (see Corollary 18 in [10]) on the term ||V − T ( ˆV )||n  using the fact that
||V − T ( ˆV )||n ≤ ||V − ˆV ||n  and using the bound of Corollary 1. The bound of Corollary 1 and
the lower bound on ν  each one holding with probability 1 − δ(cid:48)  thus  the statement of the theorem
(Eq. 15) holds with probability 1 − δ by setting δ = 3δ(cid:48).
Remark 1. An interesting property of the bound in Theorem 2 is that the approximation error of
V in space F  ||V − ΠF V ||ρ  does not appear and the error of the LSTD solution in the randomly
projected space only depends on the dimensionality d of G and the number of samples n. However
this property is valid only when Assumption 1 holds  i.e.  at most for n ≤ D. An interesting case
here is when the dimension of F is inﬁnite (D = ∞)  so that the bound is valid for any number
of samples n. In [15]  two approximation spaces F of inﬁnite dimension were constructed based
on a multi-resolution set of features that are rescaled and translated versions of a given mother
function. In the case that the mother function is a wavelet  the resulting features  called scrambled
wavelets  are linear combinations of wavelets at all scales weighted by Gaussian coefﬁcients. As a
results  the corresponding approximation space is a Sobolev space H s(X ) with smoothness of order
s > p/2  where p is the dimension of the state space X . In this case  for a function fα ∈ H s(X ) 
it is proved that the (cid:96)2-norm of the parameter α is equal to the norm of the function in H s(X )  i.e. 
||α||2 = ||fα||H s(X ). We do not describe those results further and refer the interested readers to [15].
What is important about the results of [15] is that it shows that it is possible to consider inﬁnite
dimensional function spaces for which supx ||φ(x)||2 is ﬁnite and ||α||2 is expressed in terms of the
norm of fα in F. In such cases  m(ΠF V ) is ﬁnite and the bound of Theorem 2  which does not
contain any approximation error of V in F  holds for any n. Nonetheless  further investigation is
needed to better understand the role of ||fα||H s(X ) in the ﬁnal bound.
Remark 2. As discussed in the introduction  regularization methods have been studied in solving
high-dimensional RL problems. Therefore  it is interesting to compare our results for LSTD-RP with
those reported in [4] for (cid:96)2-regularized LSTD. Under Assumption 1  when D = ∞  by selecting the
features as described in the previous remark and optimizing the value of d as in Eq. 7  we obtain

||V − T ( ˆV )||ρ ≤ O

.

(16)
Although the setting considered in [4] is different than ours (e.g.  the samples are i.i.d.)  a quali-
tative comparison of Eq. 16 with the bound in Theorem 2 of [4] shows a striking similarity in the
performance of the two algorithms. In fact  they both contain the Sobolev norm of the target func-
tion and have a similar dependency on the number of samples with a convergence rate of O(n−1/4)
(when the smoothness of the Sobolev space in [4] is chosen to be half of the dimensionality of X ).
This similarity asks for further investigation on the difference between (cid:96)2-regularized methods and
random projections in terms of prediction performance and computational complexity.
5 LSPI with Random Projections
In this section  we move from policy evaluation to policy iteration and provide a performance bound
for LSPI with random projections (LSPI-RP)  i.e.  a policy iteration algorithm that uses LSTD-RP
at each iteration. LSPI-RP starts with an arbitrary initial value function V−1 ∈ B(X ; Vmax) and
its corresponding greedy policy π0. At the ﬁrst iteration  it approximates V π0 using LSTD-RP and

(cid:16)(cid:113)||fα||Hs(X ) n

−1/4(cid:17)

7

returns a function ˆV0  whose truncated version ˜V0 = T ( ˆV0) is used to build the policy for the second
iteration. More precisely  π1 is a greedy policy w.r.t. ˜V0. So  at each iteration k  a function ˆVk−1 is
computed as an approximation to V πk−1  and then truncated  ˜Vk−1  and used to build the policy πk.1
Note that in general  the measure σ ∈ S(X ) used to evaluate the ﬁnal performance of the LSPI-
RP algorithm might be different from the distribution used to generate samples at each iteration.
Moreover  the LSTD-RP performance bounds require the samples to be collected by following the
policy under evaluation. Thus  we need Assumptions 1-3 in [10] in order to 1) deﬁne a lower-
bounding distribution µ with constant C < ∞  2) guarantee that with high probability a unique
LSTD-RP solution exists at each iteration  and 3) deﬁne the slowest β-mixing process among all the
mixing processes Mπk with 0 ≤ k < K.
Theorem 3. Let δ > 0 and F and G be linear spaces with dimensions D and d (d < D) as deﬁned
in Section 2 with d ≥ 15 log(8Kn/δ). At each iteration k  we generate a path of size n from the
stationary β-mixing process with stationary distribution ρk−1 = ρπk−1. Let n satisfy the condition in
Eq. 13 for the slower β-mixing process. Let V−1 be an arbitrary initial value function  ˆV0  . . .   ˆVK−1
( ˜V0  . . .   ˜VK−1) be the sequence of value functions (truncated value functions) generated by LSPI-
RP  and πK be the greedy policy w.r.t. ˜VK−1. Then  under Assumption 1 and Assumptions 1-3
in [10]  with probability 1 − δ (w.r.t. the random samples and the random spaces)  we have
||φ(x)||2

∗ − V πK||σ ≤

8 log(24Kn/δ)

(cid:34)

||V

(17)

4γ

(1 − γ)2

(cid:40)
(1 + γ)(cid:112)CCσ µ
(cid:115)
(cid:16)(cid:114)

2Vmax(cid:112)1 − γ2

(cid:114)
(cid:35)

C
ωµ

(cid:115)
(cid:17)

d

sup
x∈X

(cid:41)

+

2γVmaxL
1 − γ

d
νµ

8 log(12Kd/δ)

n

+

1
n

+ E

K−1

2 Rmax

+ γ

 

where Cσ µ is the concentrability term from Deﬁnition 2 in [1]  ωµ is the smallest eigenvalue of the
Gram matrix of space F w.r.t. µ  νµ is ν from Eq. 14 in which ω is replaced by ωµ  and E is  from
Theorem 2 written for the slowest β-mixing process.

Proof. The proof follows similar lines as in the proof of Thm. 8 in [10] and is available in [6].

Remark. The most critical issue about Theorem 3 is the validity of Assumptions 1-3 in [10]. It
is important to note that Assumption 1 is needed to bound the performance of LSPI independent
from the use of random projections (see [10]). On the other hand  Assumption 2 is explicitly related
to random projections and allows us to bound the term m(ΠF V ). In order for this assumption to
hold  the features {ϕj}D
j=1 of the high-dimensional space F should be carefully chosen so as to be
linearly independent w.r.t. µ.
6 Conclusions
Learning in high-dimensional linear spaces is particularly appealing in RL because it allows to have
a very accurate approximation of value functions. Nonetheless  the larger the space  the higher
the need of samples and the risk of overﬁtting. In this paper  we introduced an algorithm  called
LSTD-RP  in which LSTD is run in a low-dimensional space obtained by a random projection of
the original high-dimensional space. We theoretically analyzed the performance of LSTD-RP and
showed that it solves the problem of overﬁtting (i.e.  the estimation error depends on the value of
the low dimension) at the cost of a slight worsening in the approximation accuracy compared to the
high-dimensional space. We also analyzed the performance of LSPI-RP  a policy iteration algorithm
that uses LSTD-RP for policy evaluation. The analysis reported in the paper opens a number of inter-
esting research directions such as: 1) comparison of LSTD-RP to (cid:96)2 and (cid:96)1 regularized approaches 
and 2) a thorough analysis of the case when D = ∞ and the role of ||fα||H s(X ) in the bound.
Acknowledgments This work was supported by French National Research Agency through the
projects EXPLO-RA n◦ ANR-08-COSI-004 and LAMPADA n◦ ANR-09-EMER-007  by Ministry
of Higher Education and Research  Nord-Pas de Calais Regional Council and FEDER through the
“contrat de projets ´etat region 2007–2013”  and by PASCAL2 European Network of Excellence.

1Note that the MDP model is needed to generate a greedy policy πk. In order to avoid the need for the
model  we can simply move to LSTD-Q with random projections. Although the analysis of LSTD-RP can be
extended to action-value functions and LSTD-RP-Q  for simplicity we use value functions in the following.

8

References
[1] A. Antos  Cs. Szepesvari  and R. Munos. Learning near-optimal policies with Bellman-residual
minimization based ﬁtted policy iteration and a single sample path. Machine Learning Journal 
71:89–129  2008.

[2] J. Boyan. Least-squares temporal difference learning. Proceedings of the 16th International

Conference on Machine Learning  pages 49–56  1999.

[3] S. Bradtke and A. Barto. Linear least-squares algorithms for temporal difference learning.

Machine Learning  22:33–57  1996.

[4] A. M. Farahmand  M. Ghavamzadeh  Cs. Szepesv´ari  and S. Mannor. Regularized policy
In Proceedings of Advances in Neural Information Processing Systems 21  pages

iteration.
441–448. MIT Press  2008.

[5] A. M. Farahmand  M. Ghavamzadeh  Cs. Szepesv´ari  and S. Mannor. Regularized ﬁtted Q-
iteration for planning in continuous-space Markovian decision problems. In Proceedings of
the American Control Conference  pages 725–730  2009.

[6] M. Ghavamzadeh  A. Lazaric  O. Maillard  and R. Munos. LSPI with random projections.

Technical Report inria-00530762  INRIA  2010.

[7] P. Keller  S. Mannor  and D. Precup. Automatic basis function construction for approximate
dynamic programming and reinforcement learning. In Proceedings of the Twenty-Third Inter-
national Conference on Machine Learning  pages 449–456  2006.

[8] Z. Kolter and A. Ng. Regularization and feature selection in least-squares temporal difference
learning. In Proceedings of the Twenty-Sixth International Conference on Machine Learning 
pages 521–528  2009.

[9] M. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning

Research  4:1107–1149  2003.

[10] A. Lazaric  M. Ghavamzadeh  and R. Munos. Finite-sample analysis of least-squares policy

iteration. Technical Report inria-00528596  INRIA  2010.

[11] A. Lazaric  M. Ghavamzadeh  and R. Munos. Finite-sample analysis of LSTD. In Proceedings
of the Twenty-Seventh International Conference on Machine Learning  pages 615–622  2010.
[12] M. Loth  M. Davy  and P. Preux. Sparse temporal difference learning using lasso. In IEEE
Symposium on Approximate Dynamic Programming and Reinforcement Learning  pages 352–
359  2007.

[13] S. Mahadevan. Representation policy iteration. In Proceedings of the Twenty-First Conference

on Uncertainty in Artiﬁcial Intelligence  pages 372–379  2005.

[14] O. Maillard and R. Munos. Compressed least-squares regression. In Proceedings of Advances

in Neural Information Processing Systems 22  pages 1213–1221  2009.

[15] O. Maillard and R. Munos. Brownian motions and scrambled wavelets for least-squares re-

gression. Technical Report inria-00483014  INRIA  2010.

[16] I. Menache  S. Mannor  and N. Shimkin. Basis function adaptation in temporal difference

reinforcement learning. Annals of Operations Research  134:215–238  2005.

[17] R. Parr  C. Painter-Wakeﬁeld  L. Li  and M. Littman. Analyzing feature generation for value-
In Proceedings of the Twenty-Fourth International Conference on

function approximation.
Machine Learning  pages 737–744  2007.

[18] M. Petrik  G. Taylor  R. Parr  and S. Zilberstein. Feature selection using regularization in
approximate linear programs for Markov decision processes. In Proceedings of the Twenty-
Seventh International Conference on Machine Learning  pages 871–878  2010.

[19] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular

values. In Proceedings of the International Congress of Mathematicians  2010.

[20] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIP Press  1998.
[21] S. Vempala. The Random Projection Method. American Mathematical Society  2004.

9

,Sebastian Tschiatschek
Rishabh Iyer
Jeff Bilmes
Alaa Saade
Florent Krzakala
Lenka Zdeborová
Qinshi Wang
Wei Chen