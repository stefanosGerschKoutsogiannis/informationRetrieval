2019,Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning,In an effort to better understand the different ways in which the discount factor affects the optimization process in reinforcement learning  we designed a set of experiments to study each effect in isolation. Our analysis reveals that the common perception that poor performance of low discount factors is caused by (too) small action-gaps requires revision. We propose an alternative hypothesis that identifies the size-difference of the action-gap across the state-space as the primary cause. We then introduce a new method that enables more homogeneous action-gaps by mapping value estimates to a logarithmic space. We prove convergence for this method under standard assumptions and demonstrate empirically that it indeed enables lower discount factors for approximate reinforcement-learning methods. This in turn allows tackling a class of reinforcement-learning problems that are challenging to solve with traditional methods.,Using a Logarithmic Mapping to Enable Lower
Discount Factors in Reinforcement Learning

Harm van Seijen

Microsoft Research Montréal

harm.vanseijen@microsoft.com

Mehdi Fatemi

Microsoft Research Montréal

mehdi.fatemi@microsoft.com

Arash Tavakoli

Imperial College London

a.tavakoli@imperial.ac.uk

Abstract

In an effort to better understand the different ways in which the discount factor
affects the optimization process in reinforcement learning  we designed a set of
experiments to study each effect in isolation. Our analysis reveals that the common
perception that poor performance of low discount factors is caused by (too) small
action-gaps requires revision. We propose an alternative hypothesis that identiﬁes
the size-difference of the action-gap across the state-space as the primary cause.
We then introduce a new method that enables more homogeneous action-gaps by
mapping value estimates to a logarithmic space. We prove convergence for this
method under standard assumptions and demonstrate empirically that it indeed
enables lower discount factors for approximate reinforcement-learning methods.
This in turn allows tackling a class of reinforcement-learning problems that are
challenging to solve with traditional methods.

1

Introduction

In reinforcement learning (RL)  the objective that one wants to optimize for is often best described
as an undiscounted sum of rewards (e.g.  maximizing the total score in a game) and a discount
factor is merely introduced so as to avoid some of the optimization challenges that can occur when
directly optimizing on an undiscounted objective [Bertsekas and Tsitsiklis  1996]. In this scenario  the
discount factor plays the role of a hyper-parameter that can be tuned to obtain a better performance
on the true objective. Furthermore  for practical reasons  a policy can only be evaluated for a ﬁnite
amount of time  making the effective performance metric a ﬁnite-horizon  undiscounted objective.1
To gain a better understanding of the interaction between the discount factor and a ﬁnite-horizon 
undiscounted objective  we designed a number of experiments to study this relation. One surprising
ﬁnding is that for some problems a low discount factor can result in better asymptotic performance 
when a ﬁnite-horizon  undiscounted objective is indirectly optimized through the proxy of an inﬁnite-
horizon  discounted sum. This motivates us to look deeper into the effect of the discount factor on the
optimization process.
We analyze why in practice the performance of low discount factors tends to fall ﬂat when combined
with function approximation  especially in tasks with long horizons. Speciﬁcally  we refute a number
of common hypotheses and present a new one instead  identifying the primary culprit to be the size-

1As an example  in the seminal work of Mnih et al. [2015]  the (undiscounted) score of Atari games is

reported with a time-limit of 5 minutes per game.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

difference of the action gap (i.e.  the difference between the values of the best and the second-best
actions of a state) across the state-space.
Our main contribution is a new method that yields more homogeneous action-gap sizes for sparse-
reward problems. This is achieved by mapping the update target to a logarithmic space and performing
updates in that space instead. We prove convergence of this method under standard conditions.
Finally  we demonstrate empirically that our method achieves much better performance for low
discount factors than previously possible  providing supporting evidence for our new hypothesis.
Combining this with our analytical result that there exist tasks where low discount factors outperform
higher ones asymptotically suggests that our method can unlock a performance on certain problems
that is not achievable by contemporary RL methods.

2 Problem Setting
Consider a Markov decision process (MDP  [Puterman  1994]) M = (cid:104)S A  P  R  S0(cid:105)  where S
denotes the set of states  A the set of actions  R the reward function R : S × A × S → R  P the
transition probability function P : S × A × S → [0  1]  and S0 the starting state distribution. At
each time step t  the agent observes state st ∈ S and takes action at ∈ A. The agent observes
the next state st+1  drawn from the transition probability distribution P (st  at ·)  and a reward
rt = R(st  at  st+1). A terminal state is one that  once entered  terminates the interaction with the
environment; mathematically  it can be interpreted as an absorbing state that transitions only to itself
with a corresponding reward of 0. The behavior of an agent is deﬁned by a policy π  which  at time
step t  takes as input the history of states  actions  and rewards  s0  a0  r0  s1  a1  ....rt−1  st  and
outputs a distribution over actions  in accordance to which action at is selected. If action at only
depends on the current state st  we will call the policy a stationary one; if the policy depends on
more than the current state st  we will call the policy non-stationary.
We deﬁne a task to be the combination of an MDP M and a performance metric F . The metric F is
a function that takes as input a policy π and outputs a score that represents the performance of π on
M. By contrast  we deﬁne the learning metric Fl to be the metric that the agent optimizes. Within
the context of this paper  unless otherwise stated  the performance metric F considers the expected 
ﬁnite-horizon  undiscounted sum of rewards over the start-state distribution; the learning metric Fl
considers the expected  inﬁnite-horizon  discounted sum of rewards:

F (π  M ) = E

;

Fl(π  M ) = E

γiri

 

(1)

(cid:34)h−1(cid:88)

i=0

(cid:35)

(cid:12)(cid:12)(cid:12)π  M

ri

(cid:34) ∞(cid:88)

i=0

(cid:35)

(cid:12)(cid:12)(cid:12)π  M

where the horizon h and the discount factor γ are hyper-parameters of F and Fl  respectively.
The optimal policy of a task  π∗  is the policy that maximizes the metric F on the MDP M. Note
that in general π∗ will be a non-stationary policy. In particular  the optimal policy depends besides
the current state on the time step. We denote the policy that is optimal w.r.t. the learning metric
Fl by π∗
l . Because Fl is not a ﬁnite-horizon objective  there exists a stationary  optimal policy for
it  considerably simplifying the learning problem.2 Due to the difference between the learning and
performance metrics  the policy that is optimal w.r.t. the learning metric does not need to be optimal
w.r.t. the performance metric. We call the difference in performance between π∗
l and π∗  as measured
by F   the metric gap:

∆F = F (π∗  M ) − F (π∗

l   M )

The relation between γ and the metric gap will be analyzed in Section 3.1.
We consider model-free  value-based methods. These are methods that aim to ﬁnd a good policy by
iteratively improving an estimate of the optimal action-value function Q∗  which  generally  predicts
the expected discounted sum of rewards under the optimal policy π∗
l conditioned on state-action pairs.
The canonical example is Q-learning [Watkins and Dayan  1992]  which updates its estimates as
follows:

Qt+1(st  at) := (1 − α)Qt(st  at) + α

rt + γ max

 

(2)

(cid:16)

(cid:17)
a(cid:48) Qt(st+1  a(cid:48))

2This is the main reason why optimizing on an inﬁnite-horizon objective  rather than a ﬁnite-horizon one  is

an attractive choice.

2

where α ∈ [0  1] is the step-size. The action-value function is commonly estimated using a function
approximator with weight vector θ: Q(s  a; θ). Deep Q-Networks (DQN) [Mnih et al.  2015] use
a deep neural network as function approximator and iteratively improve an estimate of Q∗ by
minimizing a sequence of loss functions:

Li(θi) = Es a r s(cid:48)[(yDQN
yDQN
i

= r + γ max

with

− Q(s  a; θi))2]  

i

a(cid:48) Q(s(cid:48)  a(cid:48); θi−1) 

(3)
(4)

The weight vector from the previous iteration  θi−1  is encoded using a separate target network.

3 Analysis of Discount Factor Effects

3.1 Effect on Metric Gap

Figure 1: Illustrations of three different tasks (blue diamond: starting position; green circle: positive
object; red circle: negative object; gray arrows: wind direction; numbers indicate rewards). The
graphs show the performance—as measured by F —on these tasks for π∗ (black  dotted line) and π∗
l
(red  solid line) as function of the discount factor of the learning metric. The difference between the
two represents the metric gap.

The question that is central to this section is the following: given a ﬁnite-horizon  undiscounted
performance metric  what can be said about the relation between the discount factor of the learning
metric and the metric gap?
To study this problem  we designed a variety of different tasks and measured the dependence between
the metric gap and the discount factor. In Figure 1  we illustrate three of those tasks  as well as the
metric gap on those tasks as function of the discount factor. In each task  an agent  starting from
a particular position  has to collect rewards by collecting the positive objects while avoiding the
negative objects. The transition dynamics of tasks A and B is deterministic; whereas  in task C
wind blows in the direction of the arrows  making the agent move towards left with a 40% chance 
regardless of its performed action. For all three tasks  the horizon of the performance metric is 12.
On task A  where a small negative reward has to be traded off for a large positive reward that is
received later  high discount factors result in a smaller metric gap. By contrast  on task B  low
discount factors result in a smaller metric gap. The reason is that for high discount factors the optimal
learning policy takes the longer route by ﬁrst trying to collect the large object  before going to the
small object. However  with a performance metric horizon of 12  there is not enough time to take the
long route and get both rewards. The low discount factor takes a shorter route by ﬁrst going to the
smaller object and is able to collect all objects in time. On task C  a trade-off has to be made between
the risk of falling into the negative object (due to domain stochasticity) versus taking a longer detour
that minimizes this risk. On this task  the optimal policy π∗ is non-stationary (the optimal action
depends on the time step). However  because the learning objective Fl is not ﬁnite-horizon  it has
a stationary optimal policy π∗
l . Hence  the metric gap cannot be reduced to 0 for any value of the
discount factor. The best discount factor is something that is not too high nor too low.
While the policy π∗
l is derived from an inﬁnite-horizon metric  this does not preclude it from being
learned with ﬁnite-length training episodes. As an example  consider using Q-learning to learn π∗
l for
any of the tasks from Figure 1. With a uniformly random behavior policy and training episodes of
length 12 (the same as the horizon of the performance metric)  there is a non-zero probability for
each state-action pair that it will be visited within an episode. Hence  with the right step-size decay
schedule  convergence in the limit can be guaranteed [Jaakkola et al.  1994]. A key detail to enable

3

ACB-1+5+1+5-5+10.00.20.40.60.81.001234performanceA0.00.20.40.60.81.05.05.25.45.65.86.0performanceB0.00.20.40.60.81.00.00.10.20.3performanceCFigure 2: Chain task consisting of 50 states and two terminal ones. Each (non-terminal) state has
two actions: aL which results in transitioning to the left with probability 1 − p and to the right with
probability p  and vice versa for the other action  aR. All rewards are 0  except for transitioning to
the far-left or far-right terminal states that result in rL and rR  respectively.

this is that the state that is reached at the ﬁnal time step is not treated as a terminal state (which has
value 0 by default)  but normal bootstrapping occurs [Pardo et al.  2018].
A ﬁnite-horizon performance metric is not essential to observe strong dependence of the metric gap
on γ. For example  if on task B the performance metric would measure the number of steps it takes
to collect all objects  a similar graph is obtained. In general  the examples in this section demonstrate
that the best discount factor is task-dependent and can be anywhere in the range between 0 and 1.

l ) being 1.

3.2 Optimization Effects
The performance of π∗
l gives the theoretical limit of what the agent can achieve given its learning
metric. However  the discount factor also affects the optimization process; for some discount factors 
ﬁnding π∗
l could be more challenging than for others. In this section  using the task shown in Figure 2 
we evaluate the correlation between the discount factor and how hard it could be to ﬁnd π∗
l . It is easy
to see that the policy that always takes the left action aL maximizes both discounted and undiscounted
sum of rewards for any discount factor or horizon value  respectively. We deﬁne the learning metric
Fl as before (1)  but use a different performance metric F . Speciﬁcally  we deﬁne F to be 1 if the
policy takes aL in every state  and 0 otherwise. The metric gap for this setting of F and Fl is 0  with
the optimal performance (for π∗ and π∗
To study the optimization effects under function approximation 
we use linear function approximation with features constructed
by tile-coding [Sutton  1996]  using tile-widths of 1  2  3  and 5.
A tile-width of w corresponds to a binary feature that is non-zero
for w neighbouring states and zero for the remaining ones. The
number and offset of the tilings are such that any value function
can be represented. Hence  error-free reconstruction of the optimal
action-value function is possible in principle  for any discount
factor. Note that for a width of 1  the representation reduces to a
tabular one.
To keep the experiment as simple as possible  we remove explo-
ration effects by performing update sweeps over the entire state-
action space (using a step-size of 0.001) and measure performance
at the end of each update sweep. Figure 3 shows the performance
during early learning (average performance over the ﬁrst 10  000
sweeps) as well as the ﬁnal performance (average between sweeps
100  000 and 110  000).
These experiments demonstrate a common empirical observation:
when using function approximation  low discount factors do not
work well in sparse-reward domains. More speciﬁcally  the main
observations are: 1) there is a sharp drop in ﬁnal performance
for discount factors below some threshold; 2) this threshold value
depends on the tile-width  with larger ones resulting in worse (i.e. 
higher) threshold values; and 3) the tabular representation performs well for all discount factors.
It is commonly believed that the action gap has a strong inﬂuence on the optimization process
[Bellemare et al.  2016  Farahmand  2011]. The action gap of a state s is deﬁned as the difference
in Q∗ between the best and the second best actions at that state. To examine this common belief 
we start by evaluating two straightforward hypotheses involving the action gap: 1) lower discount
factors cause poor performance because they result in smaller action gaps; 2) lower discount factors
cause poor performance because they result in smaller relative action gaps (i.e  the action gap of a

Figure 3: Early performance
(top) and ﬁnal performance (bot-
tom) on the chain task.

4

0.20.40.60.81.00.00.20.40.60.81.0average performancew: 1w: 2w: 3w: 50.20.40.60.81.00.00.20.40.60.81.0average performancew: 1w: 2w: 3w: 5state divided by the maximum action-value of that state). Since both hypotheses are supported by the
results from Figure 3  we performed more experiments to test them. To test the ﬁrst hypothesis  we
performed the same experiment as above  but with rewards that are a factor 100 larger. This in turn
increases the action gaps by a factor 100 as well. Hence  to validate the ﬁrst hypothesis  this change
should improve (i.e.  lower) the threshold value where the performance falls ﬂat. To test the second
hypothesis  we pushed all action-values up by 100 through additional rewards  reducing the relative
action-gap. Hence  to validate the second hypothesis  performance should degrade for this variation.
However  neither of the modiﬁcations caused signiﬁcant changes to the early or ﬁnal performance 
invalidating these hypotheses. The corresponding graphs can be found in the supplementary material.
Because our two naïve action-gap hypotheses have failed  we
propose an alternative hypothesis: lower discount factors cause
poor performance because they result in a larger difference in the
action-gap sizes across the state-space. To illustrate the statement
about the difference in action-gap sizes  we deﬁne a metric  which
we call the action-gap deviation κ  that aims to capture the notion
of action-gap variations. Speciﬁcally  let X be a random variable
and let S + ⊆ S be the subset of states that have a non-zero action
gap. X draws uniformly at random a state s ∈ S + and outputs
log10 (AG(s))  where AG(s) is the action gap of state s. We now
deﬁne κ to be the standard deviation of the variable X. Figure 4
plots κ as function of the discount factor for the task in Figure 2.
To test this new hypothesis  we have to develop a method that reduces the action-gap deviation κ for
low discount factors  without changing the optimal policy. We do so in the next section.

Figure 4: Action-gap deviation
as function of discount factor.

4 Logarithmic Q-learning

In this section  we introduce our new method  logarithmic Q-learning  which reduces the action-gap
deviation κ for sparse-reward domains. We present the method in three steps  in each step adding a
layer of complexity in order to extend the generality of the method. In the supplementary material 
we prove convergence of the method in its most general form. As the ﬁrst step  we now consider
domains with deterministic dynamics and rewards that are either positive or zero.

4.1 Deterministic Domains with Positive Rewards

(cid:101)Qt+1(st  at) := (1 − α)(cid:101)Qt(st  at) + αf

Our method is based on the same general approach as used by Pohlen et al. [2018]: mapping the
update target to a different space and performing updates in that space instead. We indicate the
mapping function by f  and its inverse by f−1. Values in the mapping space are updated as follows:

a(cid:48) f−1(cid:16)(cid:101)Qt(st+1  a(cid:48))
(cid:17)(cid:17)
Note that (cid:101)Q in this equation is not an estimate of an expected return; it is an estimate of an expected
to (cid:101)Q. Because the updates occur in the mapping space  κ is now measured w.r.t. (cid:101)Q. That is  the action
gap of state s is now deﬁned in the mapping space as (cid:101)Q(s  abest) − (cid:101)Q(s  a2nd best).

return mapped to a different space. To obtain a regular Q-value the inverse mapping has to be applied

(cid:16)

rt + γ max

.

(5)

To reduce κ  we propose to use a logarithmic mapping function.
Speciﬁcally  we propose the following mapping function:

f (x) := c ln(x + γk) + d  

(6)
with inverse function: f−1(x) = e(x−d)/c − γk   where c  d  and
k are mapping hyper-parameters.
To understand the effect of (6) on κ  we plot κ  based on action gaps
in the logarithmic space  on a variation of the chain task (Figure 2)
that uses rR = 0 and p = 0. We also plot κ based on actions in the
regular space. Figure 5 shows that with an appropriate value of k 
the action-gap deviation can almost be reduced to 0 for low values
of γ. Setting k too high increases the deviation a little  while setting it too low increases it a lot for

Figure 5: Action-gap deviation
as function of discount factor.

5

0.20.40.60.81.0010200.250.500.751.000123reglog  k=40log  k=50log  k=200low discount factors. In short  k controls the smallest Q-value that can still be accurately represented
(i.e.  for which the action gap in the log-space is still signiﬁcant). Roughly  the smallest value that
can still be accurately represented is about γk. In other words  the cut-off point lies approximately
at a state from which it takes k time steps to experience a +1 reward. Setting k too high causes
actions that have 0 value in the regular space to have a large negative value in the log-space. This
can increase the action gap substantially for the corresponding states  thus  resulting in an overall
increase of the action-gap deviation.
The parameters c and d scale and shift values in the logarithmic space and do not have any effect on
the action-gap deviation. The parameter d controls the initialization of the Q-values. Setting d as
follows:

(7)
ensures that f−1(0) = qinit for any value of c  k  and γ. This can be useful in practice  e.g.  when

using neural networks to represent (cid:101)Q  as it enables standard initialization methods (which produce
output values around 0) while still ensuring that the initialized (cid:101)Q values correspond with qinit in

the regular space. The parameter c scales values in the log-space. For most tabular and linear
methods  scaling values does not affect the optimization process. Nevertheless  in deep RL methods
more advanced optimization techniques are commonly used and  thus  such scaling can impact the
optimization process signiﬁcantly. In all our experiments  except the deep RL experiments  we ﬁxed
d according to the equation above with qinit = 0 and used c = 1.
In stochastic environments  the approach described in this section causes issues  because averaging
over stochastic samples in the log-space produces an underestimate compared to averaging in the
regular space and then mapping the result to the log-space. Speciﬁcally  if X is a random variable 
E [ln(X)] ≤ ln (E[X]) (i.e.  Jensen’s inequality). Fortunately  within our speciﬁc context  there is a
way around this limitation that we discuss in the next section.

d = −c ln(qinit + γk)  

4.2 Stochastic Domains with Positive Rewards

The step-size α generally conﬂates two forms of averaging: averaging of stochastic update targets
due to environment stochasticity  and  in the case of function approximation  averaging over different
states. To amend our method for stochastic environments  ideally  we would separate these forms
of averaging and perform the averaging over stochastic update targets in the regular space and the
averaging over different states in the log-space. While such a separation is hard to achieve  the
approach presented below  which is inspired by the above observation  achieves many of the same

beneﬁts. In particular  it enables convergence of (cid:101)Q to f (Q∗)  even when the environment is stochastic.

Let βlog be the step-size for averaging in the log-space  and βreg be the step-size for averaging in
the regular space. We amend the approach from the previous section by computing an alternative
update target that is based on performing an averaging operation in the regular space. Speciﬁcally 
the update target Ut is transformed into an alternative update target ˆUt as follows:

with Ut := rt + γ maxa(cid:48) f−1((cid:101)Qt(st+1  a(cid:48))). The modiﬁed update target ˆUt is used for the update in

(8)

 

ˆUt := f−1((cid:101)Qt(st  at)) + βreg
(cid:101)Qt+1(st  at) := (cid:101)Qt(st  at) + βlog

(cid:16)

(cid:17)
Ut − f−1((cid:101)Qt(st  at)
(cid:17)
(cid:16)
f ( ˆUt) − (cid:101)Qt(st  at)

the log-space:

.

(9)

Note that if βreg = 1  then ˆUt = Ut  and update (9) reduces to update (5) from the previous section 
with α = βlog.
The conditions for convergence are discussed in the next section 
but one of the conditions is that βreg should go to 0 in the limit.
From a more practical point of view  when using ﬁxed values
for βreg and βlog  βreg should be set sufﬁciently small to keep
underestimation of values due to the averaging in the log-space
under control. To illustrate this  we plot the RMS error on a
positive-reward variant of the chain task (rR = 0  rL = +1  p =
0.25). The RMS error plotted is based on the difference between

f−1((cid:101)Q(s  a)) and Q∗(s  a) over all state-action pairs. We used a tile-width of 1  corresponding with

Figure 6: RMS error.

6

02505007501000# update sweeps0.000.050.100.150.20RMS errorlog = 0.001  reg=1log = 0.01  reg=0.1log = 1  reg=0.001a tabular representation  and used k = 200. Note that for βreg = 1  which reduces the method to the
one from the previous section  the error never comes close to zero.

4.3 Stochastic Domains with Positive and/or Negative Rewards

We now consider the general case where the rewards can be both positive or negative (or zero). It
might seem that we can generalize to negative rewards simply by replacing x in the mapping function
(6) by x + D  where D is a sufﬁciently large constant that prevents x + D from becoming negative.
The problem with this approach  as we will demonstrate empirically  is that it does not decrease κ for
low discount factors. Hence  in this section we present an alternative approach  based on decomposing
the Q-value function into two functions.
Consider a decomposition of the reward rt into two components  r+

t   as follows:

(cid:26)rt

0

r+
t

:=

if rt ≥ 0
otherwise

r−

t

:=

;

(cid:26)|rt|

t and r−
if rt < 0
otherwise

0
t − r−

.

(10)

t and r−

to r+
following update targets:

t are always non-negative and that rt = r+

t . To train these value functions  we construct the

Note that r+
t at all times. By decomposing
the observed reward in this manner  these two reward components can be used to train two separate

Q-value functions: (cid:101)Q+  which represents the value function in the mapping space corresponding
t   and (cid:101)Q−  which plays the same role for r−

t + γf−1(cid:16)(cid:101)Q+
(cid:16)
f−1((cid:101)Q+
t   respectively  based on (8)  which are then used to update (cid:101)Q+ and (cid:101)Q− 
Qt(s  a) := f−1(cid:16)(cid:101)Q+

with ˜at+1 := arg maxa(cid:48)
t and ˆU−
modiﬁed into ˆU +
respectively  based on (9). Action-selection at time t is based on Qt  which we deﬁne as follows:

t + γf−1(cid:16)(cid:101)Q−
(cid:17)

t (st+1  a(cid:48))) − f−1((cid:101)Q−

(cid:17) − f−1(cid:16)(cid:101)Q−

. These update targets are

; U−

t

:= r−

t (st+1  ˜at+1)

(11)

t (s  a)

t (s  a)

U +
t

:= r+

t (st+1  ˜at+1)

t (st+1  a(cid:48)))

(cid:17)

(cid:17)

(cid:17)

(12)

In the supplementary material  we prove convergence of logarithmic Q-learning under similar
conditions as regular Q-learning. In particular  the product βlog t · βreg t has to satisfy the same
conditions as αt does for regular Q-learning. There is one additional condition on βreg t  which states
that it should go to zero in the limit.
We now compute κ for the full version of the chain task. Because

deﬁnition of κ to this situation. We consider three generalizations:

there are two functions  (cid:101)Q+ and (cid:101)Q−  we have to generalize the
1) κ is based on the action-gaps of (cid:101)Q+ (‘log plus-only’); 2) κ is
based on the action-gaps of (cid:101)Q− (‘log min-only’); and 3) κ is based
on the action-gaps of both (cid:101)Q− and (cid:101)Q+ (‘log both’). Furthermore 

we plot a version that resolves the issue of negative rewards naïvely 
by adding a value D = 1 to the input of the log-function (‘log
bias’). We plot κ for these variants in Figure 7  using k = 200 
together with κ for regular Q-learning (‘reg’). Interestingly  only
for the ‘log plus-only’ variant κ is small for all discount factors.
Further analysis showed that the reason for this is that under the
optimal policy  the chance that the agent moves from a state close to the positive terminal state to the
negative terminal state is very small  which means that k = 200 is too small to make the action-gaps

for (cid:101)Q− homogeneous. However  as we will see in the next section  the performance with k = 200 is
good for all discount factors  demonstrating that not having homogeneous action-gaps for (cid:101)Q− is not

Figure 7: Action-gap deviation
as function of discount factor.

a huge issue. We argue that this could be because of the behavior related to the nature of positive
and negative rewards: it might be worthwhile to travel a long distance to get a positive reward  but
avoiding a negative reward is typically a short-horizon challenge.

7

0.20.40.60.81.001020reglog biaslog plus-onlylog min-onlylog both5 Experiments

We test our method by returning to the full version of the chain task
and the same performance metric F as used in Section 3.2  which
measures whether or not the greedy policy is optimal. We used
k = 200  βreg = 0.1  and βlog = 0.01 (the value of βreg · βlog is
equal to the value of α used in Section 3.2). Figure 8 plots the result
for early learning as well as the ﬁnal performance. Comparing
these graphs with the graphs from Figure 3 shows that logarithmic
Q-learning has successfully resolved the optimization issues of
regular Q-learning related to the use of low discount factors in
conjunction with function approximation.
Combined with the observation from Section 3.1 that the best
discount factor is task-dependent  and the convergence proof in
the supplementary material  which guarantees that logarithmic Q-
learning converges to the same policy as regular Q-learning  these
results demonstrate that logarithmic Q-learning is able to solve
tasks that are challenging to solve with Q-learning. Speciﬁcally  if
a ﬁnite-horizon performance metric is used and the task is such that
the metric gap is substantially smaller for lower discount factors 
but performance falls ﬂat for these discount factors due to function
approximation.
Finally  we test our approach in a more complex setting by compar-
ing the performance of DQN [Mnih et al.  2015] with a variant of it
that implements our method  which we will refer to as LogDQN.3
To enable easy baseline comparisons  we used the Dopamine framework for our experiments [Castro
et al.  2018]. This framework not only contains open-source code of several important deep RL
methods  but also contains the results obtained with these methods for a set of 60 games from the
Arcade Learning Environment [Bellemare et al.  2013  Machado et al.  2018]. This means that direct
comparison to some important baselines is possible.
Our LogDQN implementation consists of a modiﬁcation of the Dopamine’s DQN code. Speciﬁcally 

in order to adapt DQN’s model to provide estimates of both (cid:101)Q+ and (cid:101)Q−  the ﬁnal output layer is
doubled in size  and half of it is used to estimate (cid:101)Q+ while the other half estimates (cid:101)Q−. All the other
layers are shared between (cid:101)Q+ and (cid:101)Q− and remain unchanged. Because both (cid:101)Q+ and (cid:101)Q− are updated
does not change. Furthermore  because (cid:101)Q+ and (cid:101)Q− are updated simultaneously using a single pass

using the same samples  the replay memory does not require modiﬁcation  so the memory footprint

Figure 8: Early performance
(top) and ﬁnal performance (bot-
tom) on the chain task.

through the model  the computational cost of LogDQN and DQN is similar. Further implementation
details are provided in the supplementary material.
The published Dopamine baselines are obtained on a stochastic version of Atari using sticky actions
[Machado et al.  2018] where with 25% probability the environment executes the action from
the previous time step instead of the agent’s new action. Hence  we conducted all our LogDQN
experiments on this stochastic version of Atari as well.
While Dopamine provides baselines for 60 games in total  we only consider the subset of 55 games
for which human scores have been published  because only for these games a ‘human-normalized
score’ can be computed  which is deﬁned as:

ScoreAgent − ScoreRandom
ScoreHuman − ScoreRandom

.

(13)

We use Table 2 from Wang et al. [2016] to retrieve the human and random scores.
We optimized hyper-parameters using a subset of 6 games. In particular  we performed a scan over
the discount factor γ between γ = 0.84 and γ = 0.99. For DQN  γ = 0.99 was optimal; for
LogDQN  the best value in this range was γ = 0.96. We tried lower γ values as well  such as γ = 0.1
and γ = 0.5  but this did not improve the overall performance over these 6 games. For the other
hyper-parameters of LogDQN we used k = 100  c = 0.5  βlog = 0.0025  and βreg = 0.1. The

3The code for the experiments can be found at: https://github.com/microsoft/logrl

8

0.20.40.60.81.00.00.20.40.60.81.0average performancew: 1w: 2w: 3w: 50.20.40.60.81.00.00.20.40.60.81.0average performancew: 1w: 2w: 3w: 5Figure 10: Relative performance of LogDQN w.r.t. DQN (positive percentage means LogDQN
outperforms DQN). Orange bars indicate a performance difference larger than 50%; dark-blue bars
indicate a performance difference between 10% and 50%; light-blue bars indicate a performance
difference smaller than 10%.

product of βlog and βreg is 0.00025  which is the same value as the (default) step-size α of DQN. We
used different values of d for the positive and negative heads: we set d based on (7) with qinit = 1 for
the positive head  and qinit = 0 for the negative head. Results from the hyper-parameter optimization 
as well as further implementation details are provided in the supplementary material.
Figure 10 shows the performance of LogDQN compared to DQN
per game  using the same comparison equation as used by Wang
et al. [2016]:

ScoreLogDQN − ScoreDQN

max(ScoreDQN  ScoreHuman) − ScoreRandom

.

where ScoreLogDQN/DQN is computed by averaging over the last
10% of each learning curve (i.e.  last 20 epochs).
Figure 9 shows the mean and median of the human-normalized
score of LogDQN  as well as DQN. We also plot the performance
of the other baselines that Dopamine provides: C51 [Bellemare
et al.  2017]  Implicit Quantile Networks [Dabney et al.  2018] 
and Rainbow [Hessel et al.  2018]. These baselines are just for
reference; we have not attempted to combine our technique with
the techniques that these other baselines make use of.

6 Discussion and Future Work
Our results provide strong evidence for our hypothesis that large
differences in action-gap sizes are detrimental to the performance
of approximate RL. A possible explanation could be that optimiz-
ing on the L2-norm (3) might drive towards an average squared-
error that is similar across the state-space. However  the error-
landscape required to bring the approximation error below the
action-gap across the state-space has a very different shape if the
action-gap is orders of magnitude different in size across the state-
space. This mismatch between the required error-landscape and
that produced by the L2-norm might lead to an ineffective use
of the function approximator. Further experiments are needed to
conﬁrm this.
The strong performance we observed for γ = 0.96 in the deep RL setting is unlikely solely due to a
difference in metric gap. We suspect that there are also other effects at play that make LogDQN as
effective as it is. On the other hand  at (much) lower discount factors  the performance was not as
good as it was for the high discount factors. We believe a possible reason could be that since such low
values are very different than the original DQN settings  some of the other DQN hyper-parameters
might no longer be ideal in the low discount factor region. An interesting future direction would be
to re-evaluate some of the other hyper-parameters in the low discount factor region.

Figure 9: Human-normalized
mean (left) and median (right)
scores on 55 Atari games for
LogDQN and various other al-
gorithms.

9

DoubleDunkSkiingStarGunnerKangarooKrullAssaultIceHockeyJamesbondHeroBeamRiderAmidarCentipedeGopherMsPacmanRiverraidTimePilotAlienSolarisVentureKungFuMasterAsteroidsBowlingMontezumaRevengePitfallFishingDerbyPrivateEyeGravitarRobotankBerzerkSeaquestBoxingYarsRevengeDemonAttackPongPhoenixCrazyClimberBankHeistAtlantisQbertUpNDownBattleZoneRoadRunnerEnduroChopperCommandFreewayNameThisGameTennisSpaceInvadersWizardOfWorZaxxonTutankhamAsterixFrostbiteBreakoutVideoPinball-100%0%100%200%300%MeanMedianAcknowledgments

We like to thank Kimia Nadjahi for her contributions to a convergence proof of an early version
of logarithmic Q-learning. This early version ultimately was replaced by a signiﬁcantly improved
version that required a different convergence proof.

References
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc  1996.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G. Belle-
mare  Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Petersen 
Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan Wierstra 
Shane Legg  and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature  518(7540):529–533  2015.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons  1994.

Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning  8(3):279–292  1992.

Tommi Jaakkola  Michael I. Jordan  and Satinder P. Singh. On the convergence of stochastic iterative
dynamic programming algorithms. In Advances in Neural Information Processing Systems  pages
703–710  1994.

Fabio Pardo  Arash Tavakoli  Vitaly Levdik  and Petar Kormushev. Time limits in reinforcement
learning. In Proceedings of the 35th International Conference on Machine Learning  volume 80 
pages 4045–4054  2018.

Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse

coding. In Advances in Neural Information Processing Systems  pages 1038–1044  1996.

Marc G. Bellemare  Georg Ostrovski  Arthur Guez  Philip Thomas  and Rémi Munos. Increasing the
action gap: New operators for reinforcement learning. In Proceedings of the 30th AAAI Conference
on Artiﬁcial Intelligence  pages 1476–1483  2016.

Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. In Advances in

Neural Information Processing Systems  pages 172–180  2011.

Tobias Pohlen  Bilal Piot  Todd Hester  Mohammad Gheshlaghi Azar  Dan Horgan  David Budden 
Gabriel Barth-Maron  Hado van Hasselt  John Quan  Mel Veˇcerík  Matteo Hessel  Rémi Munos 
and Olivier Pietquin. Observe and look further: Achieving consistent performance on Atari. arXiv
preprint arXiv:1805.11593  2018.

Pablo Samuel Castro  Subhodeep Moitra  Carles Gelada  Saurabh Kumar  and Marc G. Belle-
mare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint
arXiv:1812.06110  2018.

Marc G. Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research  47:
253–279  2013.

Marlos C. Machado  Marc G. Bellemare  Erik Talvitie  Joel Veness  Matthew Hausknecht  and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artiﬁcial Intelligence Research  61:523–562  2018.

Ziyu Wang  Tom Schaul  Matteo Hessel  Hado van Hasselt  Marc Lanctot  and Nando de Freitas.
In Proceedings of the 33rd

Dueling network architectures for deep reinforcement learning.
International Conference on Machine Learning  volume 48  pages 1995–2003  2016.

Marc G. Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning  volume 70 
pages 449–458  2017.

10

Will Dabney  Georg Ostrovski  David Silver  and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning  volume 80  pages 1096–1105  2018.

Matteo Hessel  Joseph Modayil  Hado van Hasselt  Tom Schaul  Georg Ostrovski  Will Dabney  Dan
Horgan  Bilal Piot  Mohammad Azar  and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence 
pages 3215–3222  2018.

11

,Harm Van Seijen
Mehdi Fatemi
Arash Tavakoli