2013,Learning Prices for Repeated Auctions with Strategic Buyers,Inspired by real-time ad exchanges for online display advertising  we consider the problem of inferring a buyer's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism.  We model the buyer as a strategic agent  whose goal is to maximize her long-term surplus  and we are interested in mechanisms that maximize the seller's long-term revenue. We present seller algorithms that are no-regret when the buyer discounts her future surplus --- i.e. the buyer prefers showing advertisements to users sooner rather than later. We also give a lower bound on regret that increases as the buyer's discounting weakens and shows  in particular  that any seller algorithm will suffer linear regret if there is no discounting.,Learning Prices for Repeated Auctions

with Strategic Buyers

Kareem Amin

University of Pennsylvania

akareem@cis.upenn.edu

Afshin Rostamizadeh
rostami@google.com

Google Research

Abstract

Umar Syed

Google Research

usyed@google.com

Inspired by real-time ad exchanges for online display advertising  we consider the
problem of inferring a buyer’s value distribution for a good when the buyer is
repeatedly interacting with a seller through a posted-price mechanism. We model
the buyer as a strategic agent  whose goal is to maximize her long-term surplus 
and we are interested in mechanisms that maximize the seller’s long-term revenue.
We deﬁne the natural notion of strategic regret — the lost revenue as measured
against a truthful (non-strategic) buyer. We present seller algorithms that are no-
(strategic)-regret when the buyer discounts her future surplus — i.e.
the buyer
prefers showing advertisements to users sooner rather than later. We also give a
lower bound on strategic regret that increases as the buyer’s discounting weakens
and shows  in particular  that any seller algorithm will suffer linear strategic regret
if there is no discounting.

Introduction

1
Online display advertising inventory — e.g.  space for banner ads on web pages — is often sold via
automated transactions on real-time ad exchanges. When a user visits a web page whose advertising
inventory is managed by an ad exchange  a description of the web page  the user  and other relevant
properties of the impression  along with a reserve price for the impression  is transmitted to bidding
servers operating on behalf of advertisers. These servers process the data about the impression and
respond to the exchange with a bid. The highest bidder wins the right to display an advertisement
on the web page to the user  provided that the bid is above the reserve price. The amount charged
the winner  if there is one  is settled according to a second-price auction. The winner is charged the
maximum of the second-highest bid and the reserve price.
Ad exchanges have been a boon for advertisers  since rich and real-time data about impressions
allow them to target their bids to only those impressions that they value. However  this precise
targeting has an unfortunate side effect for web page publishers. A nontrivial fraction of ad exchange
auctions involve only a single bidder. Without competitive pressure from other bidders  the task of
maximizing the publisher’s revenue falls entirely to the reserve price setting mechanism. Second-
price auctions with a single bidder are equivalent to posted-price auctions. The seller offers a price
for a good  and a buyer decides whether to accept or reject the price (i.e.  whether to bid above or
below the reserve price).
In this paper  we consider online learning algorithms for setting prices in posted-price auctions where
the seller repeatedly interacts with the same buyer over a number of rounds  a common occurrence
in ad exchanges where the same buyer might be interested in buying thousands of user impressions
daily. In each round t  the seller offers a good to a buyer for price pt. The buyer’s value vt for the
good is drawn independently from a ﬁxed value distribution. Both vt and the value distribution are
known to the buyer  but neither is observed by the seller. If the buyer accepts price pt  the seller
receives revenue pt  and the buyer receives surplus vt − pt. Since the same buyer participates in

1

the auction in each round  the seller has the opportunity to learn about the buyer’s value distribution
and set prices accordingly. Notice that in worst-case repeated auctions there is no such opportunity
to learn  while standard Bayesian auctions assume knowledge of a value distribution  but avoid
addressing how or why the auctioneer was ever able to estimate this distribution.
Taken as an online learning problem  we can view this as a ‘bandit’ problem [18  16]  since the
revenue for any price not offered is not observed (e.g.  even if a buyer rejects a price  she may
well have accepted a lower price). The seller’s goal is to maximize his expected revenue over all
T rounds. One straightforward way for the seller to set prices would therefore be to use a no-
regret bandit algorithm  which minimizes the difference between seller’s revenue and the revenue
that would have been earned by offering the best ﬁxed price p∗ in hindsight for all T rounds; for
a no-regret algorithm (such as UCB [3] or EXP3 [4])  this difference is o(T ). However  we argue
that traditional no-regret algorithms are inadequate for this problem. Consider the motivations of a
buyer interacting with an ad exchange where the prices are set by a no-regret algorithm  and suppose
for simplicity that the buyer has a ﬁxed value vt = v for all t. The goal of the buyer is to acquire
the most valuable advertising inventory for the least total cost  i.e.  to maximize her total surplus
!t v − pt  where the sum is over rounds where the buyer accepts the seller’s price. A naive buyer
might simply accept the seller’s price pt if and only if vt ≥ pt; a buyer who behaves this way
is called truthful. Against a truthful buyer any no-regret algorithm will eventually learn to offer
prices pt ≈ v on nearly all rounds. But a more savvy buyer will notice that if she rejects prices in
earlier rounds  then she will tend to see lower prices in later rounds. Indeed  suppose the buyer only
accepts prices below some small amount . Then any no-regret algorithm will learn that offering
prices above  results in zero revenue  and will eventually offer prices below that threshold on nearly
all rounds. In fact  the smaller the learner’s regret  the faster this convergence occurs. If v $  then
the deceptive buyer strategy results in a large gain in total surplus for the buyer  and a large loss
in total revenue for the seller  relative to the truthful buyer. While the no-regret guarantee certainly
holds — in hindsight  the best price is indeed  — it seems fairly useless.
In this paper  we propose a deﬁnition of strategic regret that accounts for the buyer’s incentives  and
give algorithms that are no-regret with respect to this deﬁnition. In our setting  the seller chooses a
learning algorithm for selecting prices and announces this algorithm to the buyer. We assume that
the buyer will examine this algorithm and adopt whatever strategy maximizes her expected surplus
over all T rounds. We deﬁne the seller’s strategic regret to be the difference between his expected
revenue and the expected revenue he would have earned if  rather than using his chosen algorithm
to set prices  he had instead offered the best ﬁxed price p∗ on all rounds and the buyer had been
truthful. As we have seen  this revenue can be much higher than the revenue of the best ﬁxed price
in hindsight (in the example above  p∗ = v). Unless noted otherwise  throughout the remainder of
the paper the term “regret” will refer to strategic regret.
We make one further assumption about buyer behavior  which is based on the observation that in
many important real-world markets — and particularly in online advertising — sellers are far more
willing to wait for revenue than buyers are willing to wait for goods. For example  advertisers are
often interested in showing ads to users who have recently viewed their products online (this practice
is called ‘retargeting’)  and the value of these user impressions decays rapidly over time. Or consider
an advertising campaign that is tied to a product launch. A user impression that is purchased long
after the launch (such as the release of a movie) is almost worthless. To model this phenomenon we
multiply the buyer’s surplus in each round by a discount factor: If the buyer accepts the seller’s price
pt in round t  she receives surplus γt(vt − pt)  where {γt} is a nonincreasing sequence contained in
t=1 γt the buyer’s ‘horizon’  since it is analogous to the seller’s
the interval (0  1]. We call Tγ = !T
horizon T. The buyer’s horizon plays a central role in our analysis.
Summary of results: In Sections 4 and 5 we assume that discount rates decrease geometrically:
γt = γt−1 for some γ ∈ (0  1]. In Section 4 we consider the special case that the buyer has a ﬁxed
value vt = v for all rounds t  and give an algorithm with regret at most O(Tγ√T ). In Section 5 we
allow the vt to be drawn from any distribution that satisﬁes a certain smoothness assumption  and
give an algorithm with regret at most ˜O(T α + T 1/α
) where α ∈ (0  1) is a user-selected parameter.
Note that for either algorithm to be no-regret (i.e.  for regret to be o(T ))  we need that Tγ = o(T ). In
Section 6 we prove that this requirement is necessary for no-regret: any seller algorithm has regret at
least Ω(Tγ). The lower bound is proved via a reduction to a non-repeated  or ‘single-shot’  auction.
That our regret bounds should depend so crucially on Tγ is foreshadowed by the example above  in

γ

2

which a deceptive buyer foregoes surplus in early rounds to obtain even more surplus is later rounds.
A buyer with a short horizon Tγ will be unable to execute this strategy  as she will not be capable of
bearing the short-term costs required to manipulate the seller.
2 Related work
Kleinberg and Leighton study a posted price repeated auction with goods sold sequentially to T bid-
ders who either all have the same ﬁxed private value  private values drawn from a ﬁxed distribution 
or private values that are chosen by an oblivious adversary (an adversary that acts independently of
observed seller behavior) [15] (see also [7  8  14]). Cesa-Bianchi et al. study a related problem of
setting the reserve price in a second price auction with multiple (but not repeated) bidders at each
round [9]. Note that none of these previous works allow for the possibility of a strategic buyer  i.e.
one that acts non-truthfully in order to maximize its surplus. This is because a new buyer is consid-
ered at each time step and if the seller behavior depends only on previous buyers  then the setting
immediately becomes strategyproof.
Contrary to what is studied in these previous theoretical settings  electronic exchanges in practice see
the same buyer appearing in multiple auctions and  thus  the buyer has incentive to act strategically.
In fact  [12] ﬁnds empirical evidence of buyers’ strategic behavior in sponsored search auctions 
which in turn negatively affects the seller’s revenue. In the economics literature  ‘intertemporal price
discrimination’ refers to the practice of using a buyer’s past purchasing behavior to set future prices.
Previous work [1  13] has shown  as we do in Section 6  that a seller cannot beneﬁt from conditioning
prices on past behavior if the buyer is not myopic and can respond strategically. However  in contrast
to our work  these results assume that the seller knows the buyer’s value distribution.
Our setting can be modeled as a nonzero sum repeated game of incomplete information  and there is
extensive literature on this topic. However  most previous work has focused only on characterizing
the equilibria of these games. Further  our game has a particular structure that allows us to design
seller algorithms that are much more efﬁcient than generic algorithms for solving repeated games.
Two settings that are distinct from what we consider in this paper  but where mechanism design and
learning are combined  are the multi-armed bandit mechanism design problem [6  5  11] and the
incentive compatible regression/classiﬁcation problem [10  17]. The former problem is motivated
by sponsored search auctions  where the challenge is to elicit truthful values from multiple bidding
advertisers while also efﬁciently estimating the click-through rate of the set of ads that are to be
allocated. The latter problem involves learning a discriminative classiﬁer or regression function
in the batch setting with training examples that are labeled by selﬁsh agents. The goal is then to
minimize error with respect to the truthful labels.
Finally  Arora et al. proposed a notion of regret for online learning algorithms  called policy regret 
that accounts for the possibility that the adversary may adapt to the learning algorithm’s behavior
[2]. This resembles the ability  in our setting  of a strategic buyer to adapt to the seller algorithm’s
behavior. However  even this stronger deﬁnition of regret is inadequate for our setting. This is
because policy regret is equivalent to standard regret when the adversary is oblivious  and as we
explained in the previous section  there is an oblivious buyer strategy such that the seller’s standard
regret is small  but his regret with respect to the best ﬁxed price against a truthful buyer is large.
3 Preliminaries and Model
We consider a posted-price model for a single buyer repeatedly purchasing items from a single seller.
Associated with the buyer is a ﬁxed distribution D over the interval [0  1]  which is known only to
the buyer. On each round t  the buyer receives a value vt ∈V⊆ [0  1] from the distribution D. The
seller  without observing this value  then posts a price pt ∈P⊆ [0  1]. Finally  the buyer selects
an allocation decision at ∈{ 0  1}. On each round t  the buyer receives an instantaneous surplus of
at(vt − pt)  and the seller receives an instantaneous revenue of atpt.
We will be primarily interested in designing the seller’s learning algorithm  which we will denote A.
Let v1:t denote the sequence of values observed on the ﬁrst t rounds  (v1  ...  vt)  deﬁning p1:t and
a1:t analogously. A is an algorithm that selects each price pt as a (possibly randomized) function
of (p1:t−1  a1:t−1). As is common in mechanism design  we assume that the seller announces his

3

choice of algorithm A in advance. The buyer then selects her allocation strategy in response. The
buyer’s allocation strategy B generates allocation decisions at as a (possibly randomized) function
of (D  v1:t  p1:t  a1:t−1).
Notice that a choice of A  B and D ﬁxes a distribution over the sequences a1:T and p1:T. This in
turn deﬁnes the seller’s total expected revenue:

SellerRevenue(A B D  T ) = E"!T

t=1 atpt## A B D$ .

In the most general setting  we will consider a buyer whose surplus may be discounted through time.
In fact  our lower bounds will demonstrate that a sufﬁciently decaying discount rate is necessary for
a no-regret learning algorithm. We will imagine therefore that there exists a nonincreasing sequence
{γt ∈ (0  1]} for the buyer. For a choice of T  we will deﬁne the effective “time-horizon” for the
buyer as Tγ =!T
We assume that the seller is faced with a strategic buyer who adapts to the choice of A. Thus  let
B∗(A D) be a surplus-maximizing buyer for seller algorithm A and value distribution is D. In other
words  for all strategies B we have

t=1 γt. The buyer’s expected total discounted surplus is given by:
BuyerSurplus(A B D  T ) = E"!T

t=1 γtat(vt − pt)## A B D$ .

BuyerSurplus(A B∗(A D) D  T ) ≥ BuyerSurplus(A B D  T ).

We are now prepared to deﬁne the seller’s regret. Let p∗ = arg maxp∈P p PrD[v ≥ p]  the revenue-
maximizing choice of price for a seller that knows the distribution D  and simply posts a price of
p∗ on every round. Against such a pricing strategy  it is in the buyer’s best interest to be truthful 
accepting if and only if vt ≥ p∗  and the seller would receive a revenue of T p∗ Prv∼D[v ≥ p∗].
Informally  a no-regret algorithm is able to learn D from previous interactions with the buyer  and
converge to selecting a price close to p∗. We therefore deﬁne regret as:

Regret(A D  T ) = T p∗ Prv∼D[v ≥ p∗] − SellerRevenue(A B∗(A D) D  T ).

Finally  we will be interested in algorithms that attain o(T ) regret (meaning the averaged re-
gret goes to zero as T → ∞) for the worst-case D.
In other words  we say A is no-regret if
supD Regret(A D  T ) = o(T ). Note that this deﬁnition of worst-case regret only assumes that Na-
ture’s behavior (i.e.  the value distribution) is worst-case; the buyer’s behavior is always presumed
to be surplus maximizing.
4 Fixed Value Setting
In this section we consider the case of a single unknown ﬁxed buyer value  that is V = {v} for
some v ∈ (0  1]. We show that in this setting a very simple pricing algorithm with monotonically
decreasing price offerings is able to achieve O(Tγ√T ) when the buyer discount is γt = γt−1. Due
to space constraints many of the proofs for this section appear in Appendix A.

Monotone algorithm: Choose parameter β ∈ (0  1)  and initialize a0 = 1 and
p0 = 1. In each round t ≥ 1 let pt = β1−at−1pt−1.

In the Monotone algorithm  the seller starts at the maximum price of 1  and decreases the price
by a factor of β whenever the buyer rejects the price  and otherwise leaves it unchanged. Since
Monotone is deterministic and the buyer’s value v is ﬁxed  the surplus-maximizing buyer algorithm
B∗(Monotone  v) is characterized by a deterministic allocation sequence a∗1:T ∈{ 0  1}T.1
The following lemma partially characterizes the optimal buyer allocation sequence.
Lemma 1. The sequence a∗1  . . .   a∗T is monotonically nondecreasing.

1If there are multiple optimal sequences  the buyer can then choose to randomize over the set of sequences.
In such a case  the worst case distribution (for the seller) is the one that always selects the revenue minimizing
optimal sequence. In that case  let a

1:T denote the revenue-minimizing buyer-optimal sequence.

∗

4

In other words  once a buyer decides to start accepting the offered price at a certain time step  she
will keep accepting from that point on. The main idea behind the proof is to show that if there does
exist some time step t% where a∗t" = 1 and a∗t"+1 = 0  then swapping the values so that a∗t" = 0 and
a∗t"+1 = 1 (as well potentially swapping another pair of values) will result in a sequence with strictly
better surplus  thereby contradicting the optimality of a∗1:T. The full proof is shown in Section A.1.
Now  to ﬁnish characterizing the optimal allocation sequence  we provide the following lemma 
which describes time steps where the buyer has with certainty begun to accept the offered price.
Lemma 2. Let cβ γ = 1 + (1 − β)Tγ
a∗t+1 = 1.
A detailed proof is presented in Section A.2. These lemmas imply the following regret bound.
Theorem 1. Regret(Monotone  v  T ) ≤ vT%1 − β
cβ γ& + vβ% dβ γ
Proof. By Lemmas 1 and 2 we receive no revenue until at most round +dβ γ  + 1  and from that
round onwards we receive at least revenue β&dβ γ ’ per round. Thus

log( cβ γ
log(1/β)   then for any t > dβ γ we have

and dβ γ =

cβ γ&.

+ 1

v )

cβ γ

T

Regret(Monotone  v  T ) = vT −

β&dβ γ ’ ≤ vT − (T − dβ γ − 1)βdβ γ +1

’t=&dβ γ ’+1

cβ γ

√T
1+√T

and rearranging proves the theorem.

then Regret(Monotone  v  T ) ≤ √T(4vTγ + 2v log( 1

Noting that βdβ γ = v
Tuning the learning parameter simpliﬁes the bound further and provides a O(Tγ√T ) regret bound.
Note that this tuning parameter does not assume knowledge of the buyer’s discount parameter γ.
Corollary 1. If β =
v)) + v .
The computation used to derive this corollary are found in Section A.3. This corollary shows that it
is indeed possible to achieve no-regret against a strategic buyer with a unknown ﬁxed value as long
as Tγ = o(√T ). That is  the effective buyer horizon must be more than a constant factor smaller
than the square-root of the game’s ﬁnite horizon.
5 Stochastic Value Setting
We next give a seller algorithm that attains no-regret when the set of prices P is ﬁnite  the buyer’s
discount is γt = γt−1  and the buyer’s value vt for each round is drawn from a ﬁxed distribution D
that satistﬁes a certain continuity assumption  detailed below.

|P|

  T α

i &. For each phase i = 1  2  3  . . . of length Ti rounds:

Phased algorithm: Choose parameter α ∈ (0  1). Deﬁne Ti ≡ 2i and Si ≡
min% Ti
Offer each price p ∈P for Si rounds  in some ﬁxed order; these are the explore
rounds. Let Ap i = Number of explore rounds in phase i where price p was offered
and the buyer accepted. For the remaining Ti−|P|Si rounds of phase i  offer price
˜pi = arg maxp∈P p Ap i

in each round; these are the exploit rounds.

Si

The Phased algorithm proceeds across a number of phases. Each phase consists of explore rounds
followed by exploit rounds. During explore rounds  the algorithm selects each price in some ﬁxed
order. During exploit rounds  the algorithm repeatedly selects the price that realized the greatest
revenue during the immediately preceding explore rounds.
First notice that a strategic buyer has no incentive to lie during exploit rounds (i.e. it will accept any
price pt < vt and reject any price pt > vt)  since its decisions there do not affect any of its future
prices. Thus  the exploit rounds are the time at which the seller can exploit what it has learned from
the buyer during exploration. Alternatively  if the buyer has successfully manipulated the seller into
offering a low price  we can view the buyer as “exploiting” the seller.

5

During explore rounds  on the other hand  the strategic buyer can beneﬁt by telling lies which will
cause it to witness better prices during the corresponding exploit rounds. However  the value of
these lies to the buyer will depend on the fraction of the phase consisting of explore rounds. Taken
to the extreme  if the entire phase consists of explore rounds  the buyer is not interested in lying.
In general  the more explore rounds  the more revenue has to be sacriﬁced by a buyer that is lying
during the explore rounds. For the myopic buyer  the loss of enough immediate revenue at some
point ceases to justify her potential gains in the future exploit rounds.
Thus  while traditional algorithms like UCB balance exploration and exploitation to ensure conﬁ-
dence in the observed payoffs of sampled arms  our Phased algorithm explores for two purposes:
to ensure accurate estimates  and to dampen the buyer’s incentive to mislead the seller. The seller’s
balancing act is to explore for long enough to learn the buyer’s value distribution  but leave enough
exploit rounds to beneﬁt from the knowledge.
Continuity of the value distribution The preceding argument required that the distribution D
does not exhibit a certain pathology. There cannot be two prices p  p% that are very close but
p Prv∼D[v ≥ p] and p% Prv∼D[v ≥ p%] are very different. Otherwise  the buyer is largely indif-
ferent to being offered prices p or p%  but distinguishing between the two prices is essential for the
seller during exploit rounds. Thus  we assume that the value distribution D is K-Lipschitz  which
eliminates this problem: Deﬁning F (p) ≡ Prv∼D[v ≥ p]  we assume there exists K > 0 such that
|F (p) − F (p%)|≤ K|p − p%| for all p  p% ∈ [0  1]. This assumption is quite mild  as our Phased
algorithm does not need to know K  and the dependence of the regret rate on K will be logarithmic.
Theorem 2. Assume F (p) ≡ Prv∼D[v ≥ p] is K-Lipschitz. Let ∆= min p∈P\{p∗} p∗F (p∗) −
pF (p)  where p∗ = arg maxp∈P pF (p). For any parameter α ∈ (0  1) of the Phased algorithm
there exist constants c1  c2  c3  c4 such that

Regret(Phased D  T ) ≤ c1|P|T α + c2 |P|
∆1/α T 1/α
γ
= ˜O(T α + T 1/α
).

+ c3 |P|

∆2/α (log T )1/α
(log T + log(K/∆))1/α + c4|P|

γ

The complete proof of Theorem 2 is rather technical  and is provided in Appendix B.
To gain further intuition about the upper bounds proved in this section and the previous section  it
helps to parametrize the buyer’s horizon Tγ as a function of T  e.g. Tγ = T c for 0 ≤ c ≤ 1. Writing
it in this fashion  we see that the Monotone algorithm has regret at most O(T c+ 1
2 )  and the Phased
algorithm has regret at most ˜O(T √c) if we choose α = √c. The lower bound proved in the next
section states that  in the worst case  any seller algorithm will incur a regret of at least Ω(T c).
6 Lower Bound
In this section we state the main lower bound  which establishes a connection between the regret of
any seller algorithm and the buyer’s discounting. Speciﬁcally  we prove that the regret of any seller
algorithm is Ω(Tγ). Note that when T = Tγ — i.e.  the buyer does not discount her future surplus
— our lower bound proves that no-regret seller algorithms do not exist  and thus it is impossible for
the seller to take advantage of learned information. For example  consider the seller algorithm that
uniformly selects prices pt from [0  1]. The optimal buyer algorithm is truthful  accepting if pt < vt 
as the seller algorithm is non-adaptive  and the buyer does not gain any advantage by being more
strategic. In such a scenario the seller would quickly learn a good estimate of the value distribution
D. What is surprising is that a seller cannot use this information if the buyer does not discount her
future surplus. If the seller attempts to leverage information learned through interactions with the
buyer  the buyer can react accordingly to negate this advantage.
The lower bound further relates regret in the repeated setting to regret in a particular single-shot
game between the buyer and the seller. This demonstrates that  against a non-discounted buyer  the
seller is no better off in the repeated setting than he would be by repeatedly implementing such a
single-shot mechanism (ignoring previous interactions with the buyer). In the following section we
describe the simple single-shot game.

6

6.1 Single-Shot Auction
We call the following game the single-shot auction. A seller selects a family of distributions S
indexed by b ∈ [0  1]  where each Sb is a distribution on [0  1] ×{ 0  1}. The family S is revealed to
a buyer with unknown value v ∈ [0  1]  who then must select a bid b ∈ [0  1]  and then (p  a) ∼S b
is drawn from the corresponding distribution.
As usual  the buyer gets a surplus of a(v − p)  while the seller enjoys a revenue of ap. We restrict
the set of seller strategies to distributions that are incentive compatible and rational. S is incentive
compatible if for all b  v ∈ [0  1]  E(p a)∼Sb[a(v−p)] ≤ E(p a)∼Sv [a(v−p)]. It is rational if for all v 
E(p a)∼Sv [a(v−p)] ≥ 0 (i.e. any buyer maximizing expected surplus is actually incentivised to play
the game). Incentive compatible and rational strategies exist: drawing p from a ﬁxed distribution
(i.e. all Sb are the same)  and letting a = 1{b ≥ p} sufﬁces.2
We deﬁne the regret in the single-shot setting of any incentive-compatible and rational strategy S
with respect to value v as

SSRegret(S  v) = v − E(p a)∼Sv [ap].

t=1 at(vt − pt)$

t=1 γtat(vt − pt)$ ≤ E"!T

The following loose lower bound on SSRegret(S  v) is straightforward  and establishes that a
seller’s revenue cannot be a constant fraction of the buyer’s value for all v. The full proof is provided
in the appendix (Section C.1).
Lemma 3. For any incentive compatible and rational strategy S there exists v ∈ [0  1] such that
12.
SSRegret(S  v) ≥ 1
6.2 Repeated Auction
Returning to the repeated setting  our main lower bound will make use of the following technical
lemma  the full proof of which is provided in the appendix (Section C.1). Informally  the Lemma
states that the surplus enjoyed by an optimal buyer algorithm would only increase if this surplus
were viewed without discounting.
Lemma 4. Let the buyer’s discount sequence {γt} be positive and nonincreasing. For any
seller algorithm A  value distribution D  and surplus-maximizing buyer algorithm B∗(A D) 
E"!T
Notice if at(vt − pt) ≥ 0 for all t  then the Lemma 4 is trivial. This would occur if the buyer only
ever accepts prices less than its value (at = 1 only if pt ≤ vt). However  Lemma 4 is interesting
in that it holds for any seller algorithm A. It’s easy to imagine a seller algorithm that incentivizes
the buyer to sometimes accept a price pt > vt with the promise that this will generate better prices
in the future (e.g. setting pt" = 1 and offering pt = 0 for all t > t% only if at" = 1 and otherwise
setting pt = 1 for all t > t%).
Lemmas 3 and 4 let us prove our main lower bound.
Theorem 3. Fix a positive  nonincreasing  discount sequence {γt}. Let A be any seller algorithm
for the repeated setting. There exists a buyer value distribution D such that Regret(A D  T ) ≥
12 Tγ. In particular  if Tγ =Ω( T )  no-regret is impossible.
Proof. Let {ab t  pb t} be the sequence of prices and allocations generated by playing B∗(A  b)
against A. For each b ∈ [0  1] and p ∈ [0  1) ×{ 0  1}  let µb(p  a) = 1
t=1 γt1{ab t =
a}1{pb t = p}. Notice that µb(p  a) > 0 for countably many (p  a) and let Ωb = {(p  a) ∈
[0  1] ×{ 0  1} : µb(p  a) > 0}. We think of µb as being a distribution. It’s in fact a random measure
since the {ab t  pb t} are themselves random. One could imagine generating µb by playing B∗(A  b)
against A and observing the sequence {ab t  pb t}. Every time we observe a price pb t = p and
allocation ab t = a  we assign 1
γt additional mass to (p  a) in µb. This is impossible in practice 
but the random measure µb has a well-deﬁned distribution.
Now consider the following strategy S for the single-shot setting. Sb is induced by drawing a µb 
then drawing (p  a) ∼ µb. Note that for any b ∈ [0  1] and any measurable function f

Tγ !T

1

Tγ

2This subclass of auctions is even ex post rational.

7

E(p a)∼Sb[f (a  p)] = Eµb∼Sb*E(p a)∼µb[f (a  b) | µb]+ = 1
Thus the strategy S is incentive compatible  since for any b  v ∈ [0  1]

Tγ

E"!T

t=1 γtf (ab t  pb t)$.

1
Tγ

E  T
’t=1

E(p a)∼Sb[a(v − p)] =

BuyerSurplus(A B∗(A  b)  v  T )
γtav t(v − pv t)- = E(p a)∼Sv [a(v − p)]
where the inequality follows from the fact that B∗(A  v) is a surplus-maximizing algorithm for a
buyer whose value is v. The strategy S is also rational  since for any v ∈ [0  1]

γtab t(v − pb t)- =
E  T
’t=1

BuyerSurplus(A B∗(A  v)  v  T ) =

1
Tγ

1
Tγ

1
Tγ

≤

BuyerSurplus(A B∗(A  v)  v  T ) ≥ 0
E(p a)∼Sv [a(v − p)] =
where the inequality follows from the fact that a surplus-maximizing buyer algorithm cannot earn
negative surplus  as a buyer can always reject every price and earn zero surplus.

γtav t(v − pv t)- =

1
Tγ

1
Tγ

E  T
’t=1

1
Tγ

γtav tpv t-/

E  T
’t=1
(1 − rt)av tpv t-

t=1 rt. Note that rt ≥ 0. We have the following for any v ∈ [0  1]:

Let rt = 1 − γt and Tr =!T
TγSSRegret(S  v) = Tγ(v − E(p a)∼Sv [ap]) = Tγ.v −
γtav tpv t- = (T − Tr)v − E  T
= Tγv − E  T
’t=1
’t=1
av tpv t- + E  T
= T v − E  T
rtav tpv t- − Trv
’t=1
’t=1
= Regret(A  v  T )+E  T
rtav tpv t-−Trv = Regret(A  v  T )+E  T
rt(av tpv t − v)-
’t=1
’t=1
t=1 rt(av tpv t − v)$  tells us that: E"!T
A closer look at the quantity E"!T
t=1 rt(av tpv t − v)$ ≤
t=1(1 − γt)av t(v − pv t)$ ≤ 0  where the last inequality
E"!T
follows from Lemma 4. Therefore TγSSRegret(S  v) ≤ Regret(A  v  T ) and taking D to be the
point-mass on the value v ∈ [0  1] which realizes Lemma 3 proves the statement of the theorem.
7 Conclusion
In this work  we have analyzed the performance of revenue maximizing algorithms in the setting of
a repeated posted-price auction with a strategic buyer. We show that if the buyer values inventory in
the present more than in the far future  no-regret (with respect to revenue gained against a truthful
buyer) learning is possible. Furthermore  we provide lower bounds that show such an assumption
is in fact necessary. These are the ﬁrst bounds of this type for the presented setting. Future direc-
tions of study include studying buyer behavior under weaker polynomial discounting rates as well
understanding when existing “off-the-shelf” bandit-algorithm (UCB  or EXP3)  perhaps with slight
modiﬁcations  are able to perform well against strategic buyers.

t=1 rtav t(pv t − v)$ = −E"!T

Acknowledgements
We thank Corinna Cortes  Gagan Goel  Yishay Mansour  Hamid Nazerzadeh and Noam Nisan for
early comments on this work and pointers to relevent literature.

8

References
[1] Alessandro Acquisti and Hal R. Varian. Conditioning prices on purchase history. Marketing

Science  24(3):367–381  2005.

[2] Raman Arora  Ofer Dekel  and Ambuj Tewari. Online bandit learning against an adaptive

adversary: from regret to policy regret. In ICML  2012.

[3] Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine learning  47(2-3):235–256  2002.

[4] Peter Auer  Nicolo Cesa-Bianchi  Yoav Freund  and Robert E Schapire. The nonstochastic

multiarmed bandit problem. Journal on Computing  32(1):48–77  2002.

[5] Moshe Babaioff  Robert D Kleinberg  and Aleksandrs Slivkins. Truthful mechanisms with
implicit payment computation. In Proceedings of the Conference on Electronic Commerce 
pages 43–52. ACM  2010.

[6] Moshe Babaioff  Yogeshwer Sharma  and Aleksandrs Slivkins. Characterizing truthful multi-
In Proceedings of Conference on Electronic Commerce  pages

armed bandit mechanisms.
79–88. ACM  2009.

[7] Ziv Bar-Yossef  Kirsten Hildrum  and Felix Wu.

Incentive-compatible online auctions for
digital goods. In Proceedings of Symposium on Discrete Algorithms  pages 964–970. SIAM 
2002.

[8] Avrim Blum  Vijay Kumar  Atri Rudra  and Felix Wu. Online learning in online auctions. In

Proceedings Symposium on Discrete algorithms  pages 202–204. SIAM  2003.

[9] Nicolo Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. Regret minimization for reserve
In Proceedings of the Symposium on Discrete Algorithms.

prices in second-price auctions.
SIAM  2013.

[10] Ofer Dekel  Felix Fischer  and Ariel D Procaccia. Incentive compatible regression learning.

Journal of Computer and System Sciences  76(8):759–777  2010.

[11] Nikhil R Devanur and Sham M Kakade. The price of truthfulness for pay-per-click auctions.

In Proceedings of the Conference on Electronic commerce  pages 99–106. ACM  2009.

[12] Benjamin Edelman and Michael Ostrovsky. Strategic bidder behavior in sponsored search

auctions. Decision support systems  43(1):192–198  2007.

[13] Drew Fudenberg and J. Miguel Villas-Boas. Behavior-Based Price Discrimination and Cus-

tomer Recognition. Elsevier Science  Oxford  2007.

[14] Jason Hartline. Dynamic posted price mechanisms  2001.
[15] Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret
for online posted-price auctions. In Symposium on Foundations of Computer Science  pages
594–605. IEEE  2003.

[16] Volodymyr Kuleshov and Doina Precup. Algorithms for the multi-armed bandit problem.

Journal of Machine Learning  2010.

[17] Reshef Meir  Ariel D Procaccia  and Jeffrey S Rosenschein. Strategyproof classiﬁcation with

shared inputs. Proc. of 21st IJCAI  pages 220–225  2009.

[18] Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins

Selected Papers  pages 169–177. Springer  1985.

9

,Kareem Amin
Afshin Rostamizadeh
Umar Syed
hengshuai yao
Csaba Szepesvari
Richard Sutton
Joseph Modayil
Shalabh Bhatnagar
Zeyuan Allen-Zhu