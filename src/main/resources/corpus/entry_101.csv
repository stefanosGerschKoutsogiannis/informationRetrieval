2013,A Kernel Test for Three-Variable Interactions,We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence  using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute  and are used in powerful three-variable interaction tests  which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable  but their combined effect has a strong influence. This makes the Lancaster test especially suited to finding structure in directed graphical models  where it outperforms competing nonparametric tests in detecting such V-structures.,A Kernel Test for Three-Variable Interactions

Dino Sejdinovic  Arthur Gretton

Gatsby Unit  CSML  UCL  UK

{dino.sejdinovic  arthur.gretton}@gmail.com

Wicher Bergsma

Department of Statistics  LSE  UK

w.p.bergsma@lse.ac.uk

Abstract

We introduce kernel nonparametric tests for Lancaster three-variable interaction
and for total independence  using embeddings of signed measures into a repro-
ducing kernel Hilbert space. The resulting test statistics are straightforward to
compute  and are used in powerful interaction tests  which are consistent against
all alternatives for a large family of reproducing kernels. We show the Lancaster
test to be sensitive to cases where two independent causes individually have weak
inﬂuence on a third dependent variable  but their combined effect has a strong
inﬂuence. This makes the Lancaster test especially suited to ﬁnding structure in
directed graphical models  where it outperforms competing nonparametric tests in
detecting such V-structures.

1 Introduction

The problem of nonparametric testing of interaction between variables has been widely treated in
the machine learning and statistics literature. Much of the work in this area focuses on measuring
or testing pairwise interaction: for instance  the Hilbert-Schmidt Independence Criterion (HSIC) or
Distance Covariance [1  2  3]  kernel canonical correlation [4  5  6]  and mutual information [7].
In cases where more than two variables interact  however  the questions we can ask about their
interaction become signiﬁcantly more involved. The simplest case we might consider is whether the
i=1 PXi   as considered in Rd by [8]. This is already
a more general question than pairwise independence  since pairwise independence does not imply
total (mutual) independence  while the implication holds in the other direction. For example  if
X and Y are i.i.d. uniform on {−1  1}  then (X  Y  XY ) is a pairwise independent but mutually
dependent triplet [9]. Tests of total and pairwise independence are insufﬁcient  however  since they
do not rule out all third order factorizations of the joint distribution.

variables are mutually independent  PX = Qd

An important class of high order interactions occurs when the simultaneous effect of two vari-
ables on a third may not be additive. In particular  it may be possible that X ⊥⊥ Z and Y ⊥⊥ Z 
whereas ¬ ((X  Y ) ⊥⊥ Z) (for example  neither adding sugar to coffee nor stirring the coffee in-
dividually have an effect on its sweetness but the joint presence of the two does).
In addition 
study of three-variable interactions can elucidate certain switching mechanisms between positive
and negative correlation of two genes expressions  as controlled by a third gene [10]. The presence
of such interactions is typically tested using some form of analysis of variance (ANOVA) model
which includes additional interaction terms  such as products of individual variables. Since each
such additional term requires a new hypothesis test  this increases the risk that some hypothesis test
will produce a false positive by chance. Therefore  a test that is able to directly detect the presence
of any kind of higher-order interaction would be of a broad interest in statistical modeling. In the
present work  we provide to our knowledge the ﬁrst nonparametric test for three-variable interaction.
This work generalizes the HSIC test of pairwise independence  and has as its test statistic the norm

1

of an embedding of an appropriate signed measure to a reproducing kernel Hilbert space (RKHS).
When the statistic is non-zero  all third order factorizations can be ruled out. Moreover  this test is
applicable to the cases where X  Y and Z are themselves multivariate objects  and may take values
in non-Euclidean or structured domains.1

One important application of interaction measures is in learning structure for graphical models. If
the graphical model is assumed to be Gaussian  then second order interaction statistics may be used
to construct an undirected graph [11  12]. When the interactions are non-Gaussian  however  other
approaches are brought to bear. An alternative approach to structure learning is to employ condi-
tional independence tests. In the PC algorithm [13  14  15]  a V-structure (a directed graphical model
with two independent parents pointing to a single child) is detected when an independence test be-
tween the parent variables accepts the null hypothesis  while a test of dependence of the parents
conditioned on the child rejects the null hypothesis. The PC algorithm gives a correct equivalence
class of structures subject to the causal Markov and faithfulness assumptions  in the absence of
hidden common causes. The original implementations of the PC algorithm rely on partial correla-
tions for testing  and assume Gaussianity. A number of algorithms have since extended the basic
PC algorithm to arbitrary probability distributions over multivariate random variables [16  17  18] 
by using nonparametric kernel independence tests [19] and conditional dependence tests [20  18].
We observe that our Lancaster interaction based test provides a strong alternative to the conditional
dependence testing approach  and is seen to outperform earlier approaches in detecting cases where
independent parent variables weakly inﬂuence the child variable when considered individually  but
have a strong combined inﬂuence.

We begin our presentation in Section 2 with a deﬁnition of interaction measures  these being the
signed measures we will embed in an RKHS. We cover this embedding procedure in Section 3. We
then proceed in Section 4 to deﬁne pairwise and three way interactions. We describe a statistic to
test mutual independence for more than three variables  and provide a brief overview of the more
complex high-order interactions that may be observed when four or more variables are considered.
Finally  we provide experimental benchmarks in Section 5.

2 Interaction Measure

An interaction measure [21  22] associated to a multidimensional probability distribution P of a ran-
dom vector (X1  . . .   XD) taking values in the product space X1 ×· · ·×XD is a signed measure ∆P
that vanishes whenever P can be factorised in a non-trivial way as a product of its (possibly mul-
tivariate) marginal distributions. For the cases D = 2  3 the correct interaction measure coincides
with the the notion introduced by Lancaster [21] as a formal product

∆LP =

D

Yi=1(cid:0)P ∗Xi − PXi(cid:1)  

(1)

j=1 P ∗Xij

where each productQD′
(cid:0)Xi1   . . .   XiD′(cid:1). We will term the signed measure in (1) the Lancaster interaction measure. In the

case of a bivariate distribution  the Lancaster interaction measure is simply the difference between
the joint probability distribution and the product of the marginal distributions (the only possible
non-trivial factorization for D = 2)  ∆LP = PXY − PX PY   while in the case D = 3  we obtain

signiﬁes the joint probability distribution PXi1···Xi

D′ of a subvector

∆LP = PXY Z − PXY PZ − PY Z PX − PXZ PY + 2PX PY PZ .

It is readily checked that

(X  Y ) ⊥⊥ Z ∨ (X  Z) ⊥⊥ Y ∨ (Y  Z) ⊥⊥ X ⇒ ∆LP = 0.

(2)

(3)

For D > 3  however  (1) does not capture all possible factorizations of the joint distribution  e.g. 
for D = 4  it need not vanish if (X1  X2) ⊥⊥ (X3  X4)  but X1 and X2 are dependent and X3 and
X4 are dependent. Streitberg [22] corrected this deﬁnition using a more complicated construction
with the M¨obius function on the lattice of partitions  which we describe in Section 4.3.
In this

1As the reader might imagine  the situation becomes more complex again when four or more variables

interact simultaneously; we provide a brief technical overview in Section 4.3.

2

work  however  we will focus on the case of three variables and formulate interaction tests based on
embedding of (2) into an RKHS.

The implication (3) states that the presence of Lancaster interaction rules out the possibility of any
factorization of the joint distribution  but the converse is not generally true; see Appendix C for de-
tails. In addition  it is important to note the distinction between the absence of Lancaster interaction
and the total (mutual) independence of (X  Y  Z)  i.e.  PXY Z = PX PY PZ . While total indepen-
dence implies the absence of Lancaster interaction  the signed measure ∆totP = PXY Z −PXPY PZ
associated to the total (mutual) independence of (X  Y  Z) does not vanish if  e.g.  (X  Y ) ⊥⊥ Z  but
X and Y are dependent.

In this contribution  we construct the non-parametric test for the hypothesis ∆LP = 0 (no Lancaster
interaction)  as well as the non-parametric test for the hypothesis ∆totP = 0 (total independence) 
based on the embeddings of the corresponding signed measures ∆LP and ∆totP into an RKHS.
Both tests are particularly suited to the cases where X  Y and Z take values in a high-dimensional
space  and  moreover  they remain valid for a variety of non-Euclidean and structured domains  i.e. 
for all topological spaces where it is possible to construct a valid positive deﬁnite function; see [23]
for details. In the case of total independence testing  our approach can be viewed as a generalization
of the tests proposed in [24] based on the empirical characteristic functions.

3 Kernel Embeddings

We review the embedding of signed measures to a reproducing kernel Hilbert space. The RKHS
norms of such embeddings will then serve as our test statistics. Let Z be a topological space.
According to the Moore-Aronszajn theorem [25  p. 19]  for every symmetric  positive deﬁnite
function (henceforth kernel) k : Z × Z → R  there is an associated reproducing kernel Hilbert
space (RKHS) Hk of real-valued functions on Z with reproducing kernel k. The map ϕ : Z → Hk 
ϕ : z 7→ k(·  z) is called the canonical feature map or the Aronszajn map of k. Denote by M(Z)
the Banach space of all ﬁnite signed Borel measures on Z. The notion of a feature map can then be
extended to kernel embeddings of elements of M(Z) [25  Chapter 4].
Deﬁnition 1. (Kernel embedding) Let k be a kernel on Z  and ν ∈ M(Z). The kernel embedding

for all f ∈ Hk.

of ν into the RKHS Hk is µk(ν) ∈ Hk such that ´ f (z)dν(z) = hf  µk(ν)i
Alternatively  the kernel embedding can be deﬁned by the Bochner integral µk(ν) = ´ k(·  z) dν(z).
If a measurable kernel k is a bounded function  it is straightforward to show using the Riesz repre-
sentation theorem that µk(ν) exists for all ν ∈ M(Z).2 For many interesting bounded kernels k 
including the Gaussian  Laplacian and inverse multiquadratics  the embedding µk : M(Z) → Hk is
injective. Such kernels are said to be integrally strictly positive deﬁnite (ISPD) [26  p. 4]. A related
but weaker notion is that of a characteristic kernel [20  27]  which requires the kernel embedding
to be injective only on the set M1
+(Z) of probability measures. In the case that k is ISPD  since
Hk is a Hilbert space  we can introduce a notion of an inner product between two signed measures
ν  ν′ ∈ M(Z) 

Hk

hhν  ν′iik := hµk(ν)  µk(ν′)i

= ˆ k(z  z′)dν(z)dν′(z′).

Hk

Since µk is injective  this is a valid inner product and induces a norm on M(Z)  for which
kνkk = hhν  νii1/2
k = 0 if and only if ν = 0. This fact has been used extensively in the literature to
formulate: (a) a nonparametric two-sample test based on estimation of maximum mean discrepancy
i.i.d.∼ Q [28] and (b) a nonparametric indepen-
kP − Qkk  for samples {Xi}n
i.i.d.∼ PXY
dence test based on estimation of kPXY − PX PY kk⊗l  for a joint sample {(Xi  Yi)}n
[19] (the latter is also called a Hilbert-Schmidt independence criterion)  with kernel k ⊗ l on the
product space deﬁned as k(x  x′)l(y  y′). When a bounded characteristic kernel is used  the above
tests are consistent against all alternatives  and their alternative interpretation is as a generalization
[29  3] of energy distance [30  31] and distance covariance [2  32].

i.i.d.∼ P   {Yi}m

i=1

i=1

i=1

2Unbounded kernels can also be considered  however [3].

k (Z) ⊂ M(Z)  which satisfy a ﬁnite moment condition  i.e.  M1/2

In this case  one can still study embeddings
k (Z) =

of the signed measures M1/2

nν ∈ M(Z) : ´ k1/2(z  z) d|ν|(z) < ∞o .

3

Table 1: V -statistic estimates of hhν  ν ′iik⊗l in the two-variable case

ν\ν ′
PXY
PX PY

PXY

1
n2 (K ◦ L)++

PX PY

1
n3 (KL)++
1
n4 K++L++

In this article  we extend this approach to the three-variable case  and formulate tests for both
the Lancaster interaction and for the total independence  using simple consistent estimators of
k∆LP kk⊗l⊗m and k∆totP kk⊗l⊗m respectively  which we describe in the next Section. Using the
same arguments as in the tests of [28  19]  these tests are also consistent against all alternatives as
long as ISPD kernels are used.

4 Interaction Tests

Notational remarks: Throughout the paper  ◦ denotes an Hadamard (entrywise) product. Let A be
an n × n matrix  and K a symmetric n × n matrix. We will ﬁx the following notational conventions:
i=1 Aij denotes the sum of all elements of the j-th
j=1 Aij denotes the sum of all elements of the i-th row of A; A++ =
j=1 Aij denotes the sum of all elements of A; K+ = 11⊤K  i.e.  [K+]ij = K+j = Kj+ 

1 denotes an n × 1 column of ones; A+j = Pn
column of A; Ai+ = Pn
Pn
i=1Pn
and(cid:2)K⊤+(cid:3)ij = Ki+ = K+i.

4.1 Two-Variable (Independence) Test

We provide a short overview of the kernel independence test of [19]  which we write as the RKHS
norm of the embedding of a signed measure. While this material is not new (it appears in [28  Section
7.4])  it will help deﬁne how to proceed when a third variable is introduced  and the signed measures
become more involved. We begin by expanding the squared RKHS norm kPXY − PX PY k2
k⊗l as
inner products  and applying the reproducing property 

kPXY − PX PY k2

k⊗l = EXY EX ′Y ′ k(X  X′)l(Y  Y ′) + EX EX ′ k(X  X′)EY EY ′ l(Y  Y ′)

− 2EX ′Y ′ [EX k(X  X′)EY l(Y  Y ′)]  

(4)

where (X  Y ) and (X′  Y ′) are independent copies of random variables on X × Y with distribution
PXY .
Given a joint sample {(Xi  Yi)}n
k⊗l is
obtained by substituting corresponding empirical means into (4)  which can be represented using
Gram matrices K and L (Kij = k(Xi  Xj)  Lij = l(Yi  Yj )) 

i.i.d.∼ PXY   an empirical estimator of kPXY − PX PY k2

i=1

ˆEXY ˆEX ′Y ′ k(X  X′)l(Y  Y ′) =

ˆEX ˆEX ′ k(X  X′)ˆEY ˆEY ′l(Y  Y ′) =

ˆEX ′Y ′hˆEX k(X  X′)ˆEY l(Y  Y ′)i =

1
n2

1
n4

1
n3

n

n

Xa=1
Xa=1
Xa=1

n

n

n

Xb=1
Xb=1
Xb=1

n

KabLab =

1
n2 (K ◦ L)++  

n

Xc=1
Xc=1

n

n

Xd=1

KabLcd =

1
n4

K++L++ 

KacLbc =

1
n3 (KL)++ .

Since these are V-statistics [33  Ch. 5]  there is a bias of OP (n−1); U-statistics may be used if an
unbiased estimate is needed. Each of the terms above corresponds to an estimate of an inner product

hhν  ν′iik⊗l for probability measures ν and ν′ taking values in {PXY   PX PY }  as summarized in

Table 1. Even though the second and third terms involve triple and quadruple sums  each of the
empirical means can be computed using sums of all terms of certain matrices  where the dominant
computational cost is in computing the matrix product KL. In fact  the overall estimator can be

4

Table 2: V -statistic estimates of hhν  ν ′iik⊗l⊗m in the three-variable case

ν\ν ′

nPXY Z

n2PXY PZ

n2PXZ PY

n2PY Z PX

n3PX PY PZ

nPXY Z

(K ◦ L ◦ M )++

n2PXY PZ
n2PXZ PY
n2PY Z PX
n3PX PY PZ

((K ◦ L) M )++
(K ◦ L)++ M++

((K ◦ M ) L)++

((M ◦ L) K)++

tr(K+ ◦ L+ ◦ M+)

(M KL)++

(K ◦ M )++ L++

(KLM )++
(KM L)++

(KL)++M++

(KM )++L++

(L ◦ M )++ K++

(LM )++K++

K++L++M++

1

computed in an even simpler form (see Proposition 9 in Appendix F)  as (cid:13)(cid:13)(cid:13)

=
n2 (K ◦ HLH)++   where H = I − 1
11⊤ is the centering matrix. Note that by the idempotence of
H  we also have that (K ◦ HLH)++ = (HKH ◦ HLH)++. In the rest of the paper  for any Gram
matrix K  we will denote its corresponding centered matrix HKH by ˜K. When three variables are
present  a two-variable test already allows us to determine whether for instance (X  Y ) ⊥⊥ Z  i.e. 
whether PXY Z = PXY PZ . It is sufﬁcient to treat (X  Y ) as a single variable on the product space
X × Y  with the product kernel k ⊗ l. Then  the Gram matrix associated to (X  Y ) is simply K ◦ L 
and the corresponding V -statistic is 1
.3 What is not obvious  however  is if a

ˆPXY − ˆPX ˆPY(cid:13)(cid:13)(cid:13)

k⊗l

n

2

n2 (cid:16)K ◦ L ◦ ˜M(cid:17)++

V-statistic for the Lancaster interaction (which can be thought of as a surrogate for the composite
hypothesis of various factorizations) can be obtained in a similar form. We will address this question
in the next section.

4.2 Three-Variable Tests

As in the two-variable case  it sufﬁces to derive V-statistics for inner products hhν  ν′iik⊗l⊗m  where
ν and ν′ take values in all possible combinations of the joint and the products of the marginals  i.e. 
PXY Z   PXY PZ   etc. Again  it is easy to see that these can be expressed as certain expectations of
kernel functions  and thereby can be calculated by an appropriate manipulation of the three Gram
matrices. We summarize the resulting expressions in Table 2 - their derivation is a tedious but
straightforward linear algebra exercise. For compactness  the appropriate normalizing terms are
moved inside the measures considered.

Based on the individual RKHS inner product estimators  we can now easily derive estimators for
various signed measures arising as linear combinations of PXY Z   PXY PZ   and so on. The ﬁrst such
measure is an “incomplete” Lancaster interaction measure ∆(Z)P = PXY Z +PX PY PZ −PY Z PX −
PXZ PY   which vanishes if (Y  Z) ⊥⊥ X or (X  Z) ⊥⊥ Y   but not necessarily if (X  Y ) ⊥⊥ Z. We
obtain the following result for the empirical measure ˆP .

Proposition 2 (Incomplete Lancaster interaction). (cid:13)(cid:13)(cid:13)∆(Z)

ˆP(cid:13)(cid:13)(cid:13)

2

k⊗l⊗m

= 1

n2 (cid:16) ˜K ◦ ˜L ◦ M(cid:17)++

.

ˆP . Unlike in the two-variable case where either
Analogous expressions hold for ∆(X)
matrix or both can be centered  centering of each matrix in the three-variable case has a different
meaning. In particular  one requires centering of all three kernel matrices to perform a “complete”
Lancaster interaction test  as given by the following Proposition.

ˆP and ∆(Y )

Proposition 3 (Lancaster interaction). (cid:13)(cid:13)(cid:13)∆L ˆP(cid:13)(cid:13)(cid:13)

2

k⊗l⊗m

= 1

n2 (cid:16) ˜K ◦ ˜L ◦ ˜M(cid:17)++

.

The proofs of these Propositions are given in Appendix A. We summarize various hypotheses and
the associated V-statistics in the Appendix B. As we will demonstrate in the experiments in Section
5  while particularly useful for testing the factorization hypothesis  i.e.  for (X  Y ) ⊥⊥ Z ∨ (X  Z) ⊥⊥

Y ∨ (Y  Z) ⊥⊥ X  the statistic (cid:13)(cid:13)(cid:13)∆L ˆP(cid:13)(cid:13)(cid:13)

2

k⊗l⊗m

individual hypotheses (Y  Z) ⊥⊥ X  (X  Z) ⊥⊥ Y   or (X  Y ) ⊥⊥ Z  or for total independence testing 

can also be used for powerful tests of either the

3In general  however  this approach would require some care since  e.g.  X and Y could be measured on

very different scales  and the choice of kernels k and l needs to take this into account.

5

i.e.  PXY Z = PX PY PZ   as it vanishes in all of these cases. The null distribution under each of these
hypotheses can be estimated using a standard permutation-based approach described in Appendix
D.

Another way to obtain the Lancaster interaction statistic is as the RKHS norm of the joint “cen-
tral moment” ΣXY Z = EXY Z[(kX − µX ) ⊗ (lY − µY ) ⊗ (mZ − µZ )] of RKHS-valued random
variables kX   lY and mZ (understood as an element of the tensor RKHS Hk ⊗ Hl ⊗ Hm). This is
related to a classical characterization of the Lancaster interaction [21  Ch. XII]: there is no Lancaster
interaction between X  Y and Z if and only if cov [f (X)  g(Y )  h(Z)] = 0 for all L2 functions f   g
and h. There is an analogous result in our case (proof is given in Appendix A)  which states
Proposition 4. k∆LP kk⊗l⊗m = 0 if and only if cov [f (X)  g(Y )  h(Z)] = 0 for all f ∈ Hk 
g ∈ Hl  h ∈ Hm.

And ﬁnally  we give an estimator of the RKHS norm of the total independence measure ∆totP .
Proposition 5 (Total independence). Let ∆tot ˆP = ˆPXY Z − ˆPX ˆPY ˆPZ . Then:
1
n6

1
n2 (K ◦ L ◦ M )++ −

tr(K+ ◦ L+ ◦ M+) +

K++L++M++.

2
n4

=

2

(cid:13)(cid:13)(cid:13)∆tot ˆP(cid:13)(cid:13)(cid:13)

k⊗l⊗m

The proof follows simply from reading off the corresponding inner-product V-statistics from the
Table 2. While the test statistic for total independence has a somewhat more complicated form than
that of Lancaster interaction  it can also be computed in quadratic time.

4.3 Interaction for D > 3

Streitberg’s correction of the interaction measure for D > 3 has the form

∆S P = Xπ

(−1)|π|−1 (|π| − 1)!JπP 

(5)

where the sum is taken over all partitions of the set {1  2  . . .   n}  |π| denotes the size of the partition
(number of blocks)  and Jπ : P 7→ Pπ is the partition operator on probability measures  which for
a ﬁxed partition π = π1|π2| . . . |πr maps the probability measure P to the product measure Pπ =
j=1 Pπj   where Pπj is the marginal distribution of the subvector (Xi : i ∈ πj) . The coefﬁcients
correspond to the M¨obius inversion on the partition lattice [34]. While the Lancaster interaction
has an interpretation in terms of joint central moments  Streitberg’s correction corresponds to joint

Qr
cumulants [22  Section 4]. Therefore  a central moment expression like EX1...Xn [(cid:16)k(1)
· · · ⊗ (cid:16)k(n)

− µX1(cid:17) ⊗
− µXn(cid:17)] does not capture the correct notion of the interaction measure. Thus  while

one can in principle construct RKHS embeddings of higher-order interaction measures  and compute
RKHS norms using a calculus of V -statistics and Gram-matrices analogous to that of Table 2  it does
not seem possible to avoid summing over all partitions when computing the corresponding statistics 
yielding a computationally prohibitive approach in general. This can be viewed by analogy with the
scalar case  where it is well known that the second and third cumulants coincide with the second
and third central moments  whereas the higher order cumulants are neither moments nor central
moments  but some other polynomials of the moments.

Xn

X1

4.4 Total independence for D > 3

In general  the test statistic for total independence in the D-variable case is

2

ˆPX1:D −

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

D

Yi=1

ˆPXi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

i=1 k(i)

ND

=

+

K (i)

ab −

2

nD+1

n

Xa=1

D

Yi=1

n

Xb=1

K (i)
ab

1
n2

1

n2D

n

n

D

Yi=1
Xb=1
Xa=1
Xa=1
Yi=1
Xb=1

D

n

n

K (i)
ab .

A similar statistic for total independence is discussed by [24] where testing of total independence
based on empirical characteristic functions is considered. Our test has a direct interpretation in terms
of characteristic functions as well  which is straightforward to see in the case of translation invariant
kernels on Euclidean spaces  using their Bochner representation  similarly as in [27  Corollary 4].

6

Marginal independence tests: Dataset A

Marginal independence tests: Dataset B

 

)
r
o
r
r
e

I
I

e
p
y
T
(

e
t
a
r

e
c
n
a
t
p
e
c
c
a

l
l
u
N

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

 

0

1

3

5

9

7
11
Dimension

13

15

17

19

1

3

5

2var: X ⊥⊥ Y
2var: X ⊥⊥ Z
2var: (X  Y ) ⊥⊥ Z
∆L: (X  Y ) ⊥⊥ Z

13

15

17

19

7

9

11
Dimension

Figure 1: Two-variable kernel independence tests and the test for (X  Y ) ⊥⊥ Z using the Lancaster
statistic

)
r
o
r
r
e

I
I

e
p
y
T
(

e
t
a
r

e
c
n
a
t
p
e
c
c
a

l
l
u
N

1

0.8

0.6

0.4

0.2

0

Total independence test: Dataset A

Total independence test: Dataset B

1

0.8

0.6

0.4

0.2

 

0

∆L: total indep.
∆tot : total indep.

 

1

3

5

9

7
11
Dimension

13

15

17

19

1

3

5

7

9

11
Dimension

13

15

17

19

Figure 2: Total independence: ∆tot ˆP vs. ∆L ˆP .

5 Experiments

We investigate the performance of various permutation based tests that use the Lancaster statistic

2

(cid:13)(cid:13)(cid:13)∆L ˆP(cid:13)(cid:13)(cid:13)

k⊗l⊗m

X  Y and Z are random vectors of increasing dimensionality:

and the total independence statistic(cid:13)(cid:13)(cid:13)∆tot ˆP(cid:13)(cid:13)(cid:13)

2

k⊗l⊗m

on two synthetic datasets where

Dataset A: Pairwise independent  mutually dependent data. Our ﬁrst dataset is a triplet of
i.i.d.∼ N (0  Ip)  W ∼ Exp( 1√2
random vectors (X  Y  Z) on Rp × Rp × Rp  with X  Y
) 
Z1 = sign(X1Y1)W   and Z2:p ∼ N (0  Ip−1)  i.e.  the product of X1Y1 determines the sign of
Z1  while the remaining p − 1 dimensions are independent (and serve as noise in this example).4
In this case  (X  Y  Z) is clearly a pairwise independent but mutually dependent triplet. The mutual
dependence becomes increasingly difﬁcult to detect as the dimensionality p increases.

Dataset B: Joint dependence can be easier to detect. In this example  we consider a triplet of
random vectors (X  Y  Z) on Rp × Rp × Rp  with X  Y

i.i.d.∼ N (0  Ip)  Z2:p ∼ N (0  Ip−1)  and

Z1 =




1 + ǫ 
1 + ǫ 

X 2
w.p. 1/3 
Y 2
w.p. 1/3 
X1Y1 + ǫ  w.p. 1/3 

where ǫ ∼ N (0  0.12). Thus  dependence of Z on pair (X  Y ) is stronger than on X and Y individ-
ually.

4Note that there is no reason for X  Y and Z to have the same dimensionality p - this is done for simplicity

of exposition.

7

)
r
o
r
r
e

I
I

e
p
y
T
(

e
t
a
r

e
c
n
a
t
p
e
c
c
a

l
l
u
N

1

0.8

0.6

0.4

0.2

0

V-structure discovery: Dataset A

V-structure discovery: Dataset B

1

0.8

0.6

0.4

0.2

 

0

2var: Factor

∆L: Factor

CI: X ⊥⊥ Y |Z

 

1

3

5

9

7
11
Dimension

13

15

17

19

1

3

5

7

9

11
Dimension

13

15

17

19

Figure 3: Factorization hypothesis: Lancaster statistic vs. a two-variable based test; Test for X ⊥⊥
Y |Z from [18]

In all cases  we use permutation tests as described in Appendix D. The test level is set to α = 0.05 
sample size to n = 500  and we use gaussian kernels with bandwidth set to the interpoint median
distance.
In Figure 1  we plot the null hypothesis acceptance rates of the standard kernel two-
variable tests for X ⊥⊥ Y (which is true for both datasets A and B  and accepted at the correct
rate across all dimensions) and for X ⊥⊥ Z (which is true only for dataset A)  as well as of the
standard kernel two-variable test for (X  Y ) ⊥⊥ Z  and the test for (X  Y ) ⊥⊥ Z using the Lancaster
statistic. As expected  in dataset B  we see that dependence of Z on pair (X  Y ) is somewhat easier
to detect than on X individually with two-variable tests. In both datasets  however  the Lancaster
interaction appears signiﬁcantly more sensitive in detecting this dependence as dimensionality p

2

2

k⊗l⊗m

increases. Figure 2 plots the Type II error of total independence tests with statistics (cid:13)(cid:13)(cid:13)∆L ˆP(cid:13)(cid:13)(cid:13)
and(cid:13)(cid:13)(cid:13)∆tot ˆP(cid:13)(cid:13)(cid:13)

k⊗l⊗m
. The Lancaster statistic outperforms the total independence statistic everywhere

apart from the Dataset B when the number of dimensions is small (between 1 and 5). Figure 3 plots
the Type II error of the factorization test  i.e.  test for (X  Y ) ⊥⊥ Z ∨ (X  Z) ⊥⊥ Y ∨ (Y  Z) ⊥⊥ X
with Lancaster statistic with Holm-Bonferroni correction as described in Appendix D  as well as
the two-variable based test (which performs three standard two-variable tests and applies the Holm-
Bonferroni correction). We also plot the Type II error for the conditional independence test for
X ⊥⊥ Y |Z from [18]. Under assumption that X ⊥⊥ Y (correct on both datasets)  negation of each
of these three hypotheses is equivalent to the presence of V-structure X → Z ← Y   so the rejection
of the null can be viewed as a V-structure detection procedure. As dimensionality increases  the
Lancaster statistic appears signiﬁcantly more sensitive to the interactions present than the competing
approaches  which is particularly pronounced in Dataset A.

6 Conclusions

We have constructed permutation-based nonparametric tests for three-variable interactions  includ-
ing the Lancaster interaction and total independence. The tests can be used in datasets where only
higher-order interactions persist  i.e.  variables are pairwise independent; as well as in cases where
joint dependence may be easier to detect than pairwise dependence  for instance when the effect of
two variables on a third is not additive. The ﬂexibility of the framework of RKHS embeddings of
signed measures allows us to consider variables that are themselves multidimensional. While the to-
tal independence case readily generalizes to more than three dimensions  the combinatorial nature of
joint cumulants implies that detecting interactions of higher order requires signiﬁcantly more costly
computation.

References

[1] A. Gretton  O. Bousquet  A. Smola  and B. Sch¨olkopf. Measuring statistical dependence with Hilbert-

Schmidt norms. In ALT  pages 63–78  2005.

[2] G. Sz´ekely  M. Rizzo  and N.K. Bakirov. Measuring and testing dependence by correlation of distances.

Ann. Stat.  35(6):2769–2794  2007.

8

[3] D. Sejdinovic  B. Sriperumbudur  A. Gretton  and K. Fukumizu. Equivalence of distance-based and

RKHS-based statistics in hypothesis testing. Ann. Stat.  41(5):2263–2291  2013.

[4] F. R. Bach and M. I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res.  3:1–48  2002.
[5] K. Fukumizu  F. Bach  and A. Gretton. Statistical consistency of kernel canonical correlation analysis. J.

Mach. Learn. Res.  8:361–383  2007.

[6] J. Dauxois and G. M. Nkiet. Nonlinear canonical analysis and independence tests. Ann. Stat.  26(4):1254–

1278  1998.

[7] D. Pal  B. Poczos  and Cs. Szepesvari. Estimation of renyi entropy and mutual information based on

generalized nearest-neighbor graphs. In NIPS 23  2010.

[8] A. Kankainen. Consistent Testing of Total Independence Based on the Empirical Characteristic Function.

PhD thesis  University of Jyv¨askyl¨a  1995.

[9] S. Bernstein. The Theory of Probabilities. Gastehizdat Publishing House  Moscow  1946.

[10] M. Kayano  I. Takigawa  M. Shiga  K. Tsuda  and H. Mamitsuka. Efﬁciently ﬁnding genome-wide three-

way gene interactions from transcript- and genotype-data. Bioinformatics  25(21):2735–2743  2009.

[11] N. Meinshausen and P. Buhlmann. High dimensional graphs and variable selection with the lasso. Ann.

Stat.  34(3):1436–1462  2006.

[12] P. Ravikumar  M.J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance estimation by

minimizing ℓ1-penalized log-determinant divergence. Electron. J. Stat.  4:935–980  2011.
[13] J. Pearl. Causality: Models  Reasoning and Inference. Cambridge University Press  2001.
[14] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. 2nd edition  2000.
[15] M. Kalisch and P. Buhlmann. Estimating high-dimensional directed acyclic graphs with the PC algorithm.

J. Mach. Learn. Res.  8:613–636  2007.

[16] X. Sun  D. Janzing  B. Sch¨olkopf  and K. Fukumizu. A kernel-based causal learning algorithm. In ICML 

pages 855–862  2007.

[17] R. Tillman  A. Gretton  and P. Spirtes. Nonlinear directed acyclic structure learning with weakly additive

noise models. In NIPS 22  2009.

[18] K. Zhang  J. Peters  D. Janzing  and B. Schoelkopf. Kernel-based conditional independence test and

application in causal discovery. In UAI  pages 804–813  2011.

[19] A. Gretton  K. Fukumizu  C.-H. Teo  L. Song  B. Sch¨olkopf  and A. Smola. A kernel statistical test of

independence. In NIPS 20  pages 585–592  Cambridge  MA  2008. MIT Press.

[20] K. Fukumizu  A. Gretton  X. Sun  and B. Sch¨olkopf. Kernel measures of conditional dependence.

In

NIPS 20  pages 489–496  2008.

[21] H.O. Lancaster. The Chi-Squared Distribution. Wiley  London  1969.
[22] B. Streitberg. Lancaster interactions revisited. Ann. Stat.  18(4):1878–1885  1990.
[23] K. Fukumizu  B. Sriperumbudur  A. Gretton  and B. Schoelkopf. Characteristic kernels on groups and

semigroups. In NIPS 21  pages 473–480  2009.

[24] A. Kankainen. Consistent Testing of Total Independence Based on the Empirical Characteristic Function.

PhD thesis  University of Jyv¨askyl¨a  1995.

[25] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.

Kluwer  2004.

[26] B. Sriperumbudur  K. Fukumizu  and G. Lanckriet. Universality  characteristic kernels and rkhs embed-

ding of measures. J. Mach. Learn. Res.  12:2389–2410  2011.

[27] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Hilbert space embeddings

and metrics on probability measures. J. Mach. Learn. Res.  11:1517–1561  2010.

[28] A. Gretton  K. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola. A kernel two-sample test. J. Mach.

Learn. Res.  13:723–773  2012.

[29] D. Sejdinovic  A. Gretton  B. Sriperumbudur  and K. Fukumizu. Hypothesis testing using pairwise dis-

tances and associated kernels. In ICML  2012.

[30] G. Sz´ekely and M. Rizzo. Testing for equal distributions in high dimension. InterStat  (5)  November

2004.

[31] L. Baringhaus and C. Franz. On a new multivariate two-sample test. J. Multivariate Anal.  88(1):190–206 

2004.

[32] G. Sz´ekely and M. Rizzo. Brownian distance covariance. Ann. Appl. Stat.  4(3):1233–1303  2009.
[33] R. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley  New York  1980.
[34] T.P. Speed. Cumulants and partition lattices. Austral. J. Statist.  25:378–388  1983.
[35] S. Holm. A simple sequentially rejective multiple test procedure. Scand. J. Statist.  6(2):65–70  1979.
[36] A. Gretton  K. Fukumizu  Z. Harchaoui  and B. Sriperumbudur. A fast  consistent kernel two-sample test.

In NIPS 22  Red Hook  NY  2009. Curran Associates Inc.

9

,Dino Sejdinovic
Arthur Gretton
Wicher Bergsma
Daniel Soudry
Ron Meir
Yarin Gal
Zoubin Ghahramani