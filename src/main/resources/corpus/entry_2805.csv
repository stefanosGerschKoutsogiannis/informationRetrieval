2008,Large Margin Taxonomy Embedding for Document Categorization,Applications of multi-class classification  such as document categorization  often appear in cost-sensitive settings. Recent work has significantly improved the state of the art by moving beyond ``flat'' classification through incorporation of class hierarchies [Cai and Hoffman 04]. We present a novel algorithm that goes beyond hierarchical classification and estimates the latent semantic space that underlies the class hierarchy. In this space  each class is represented by a prototype and classification is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efficiently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization.,Large Margin Taxonomy Embedding with an

Application to Document Categorization

Kilian Weinberger
Yahoo! Research

Olivier Chapelle
Yahoo! Research

kilian@yahoo-inc.com

chap@yahoo-inc.com

Abstract

Applications of multi-class classiﬁcation  such as document categorization  often
appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state
of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class
hierarchies [4]. We present a novel algorithm that goes beyond hierarchical clas-
siﬁcation and estimates the latent semantic space that underlies the class hierarchy.
In this space  each class is represented by a prototype and classiﬁcation is done
with the simple nearest neighbor rule. The optimization of the semantic space
incorporates large margin constraints that ensure that for each instance the correct
class prototype is closer than any other. We show that our optimization is convex
and can be solved efﬁciently for large data sets. Experiments on the OHSUMED
medical journal data base yield state-of-the-art results on topic categorization.

1 Introduction

Multi-class classiﬁcation is a problem that arises in many applications of machine learning. In many
cases the cost of misclassiﬁcation varies strongly between classes. For example  in the context of
object recognition it may be signiﬁcantly worse to misclassify a male pedestrian as a trafﬁc light
than as a female pedestrian. Similarly  in the context of document categorization it seems more
severe to misclassify a medical journal on heart attack as a publication on athlete’s foot than on
Coronary artery disease. Although the scope of the proposed method is by no means limited to
text data and topic hierarchies  for improved clarity we will restrict ourselves to terminology from
document categorization throughout this paper.
The most common approach to document categorization is to reduce the problem to a “ﬂat” classi-
ﬁcation problem [13]. However  it is often the case that the topics are not just discrete classes  but
are nodes in a complex taxonomy with rich inter-topic relationships. For example  web pages can be
categorized into the Yahoo! web taxonomy or medical journals can be categorized into the Medical
Subject Headings (MeSH) taxonomy. Moving beyond ﬂat classiﬁcation to settings that utilize these
hierarchical representations of topics has signiﬁcantly pushed the state-of-the art [4  15]. Additional
information about inter-topic relationships can for example be leveraged through cost-sensitive de-
cision boundaries or knowledge sharing between documents from closely related classes.
In reality  however  the topic taxonomy is a crude approximation of topic relations  created by an
editor with knowledge of the true underlying semantic space of topics. In this paper we propose a
method that moves beyond hierarchical presentations and aims to re-discover the continuous latent
semantic space underlying the topic taxonomy. Instead of regarding document categorization as
classiﬁcation  we will think of it as a regression problem where new documents are mapped into this
latent semantic topic space. Very different from approaches like LSI or LDA [1  7]  our algorithm is
entirely supervised and explicitly embeds the topic taxonomy and the documents into a single latent
semantic space with “semantically meaningful” Euclidean distances.

1

Topic taxonomy

T

Low dimensional semantic space

F

High dimensional input space

X

P

W

stroke

arthritis

heart
attack 

pneumonia

class

prototypes

!pα

embedded

inputs

W!xi

original
inputs

!xi

Figure 1: A schematic layout of our taxem method (for Taxonomy Embedding). The classes are
embedded as prototypes inside the semantic space. The input documents are mapped into the same
space  placed closest to their topic prototypes.

In this paper we derive a method to embed the taxonomy of topics into a latent semantic space in
form of topic prototypes. A new document can be classiﬁed by ﬁrst mapping it into this space and
then assigning the label of the closest prototype. A key contribution of our paper is the derivation
of a convex problem that learns the regressor for the documents and the placement of the prototypes
in a single optimization. In particular  it places the topic prototypes such that for each document
the prototype of the correct topic is much closer than any other prototype by a large margin. We
show that this optimization is a special instance of semi-deﬁnite programs [2]  that can be solved
efﬁciently [16] for large data sets.
Our paper is structured as follows: In section 2 we introduce necessary notation and a ﬁrst version
of the algorithm based on a two-step approach of ﬁrst embedding the hierarchical taxonomy into a
semantic space and then regressing the input documents close to their respective topic prototypes.
In section 3 we extend our model to a single optimization that learns both steps in one convex op-
timization with large margin constraints. We evaluate our method in section 4 and demonstrate
state-of-the-art results on eight different document categorization tasks from the OHSUMED med-
ical journal data set. Finally  we relate our method to previous work in section 5 and conclude in
section 6.

2 Method

We assume that our input consists of documents  represented as a set of high dimensional sparse
vectors (cid:31)x1(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) (cid:31)xn (cid:31) X of dimensionality d. Typically  these could be binary bag of words
indicators or tﬁdf scores.
In addition  the documents are accompanied by single topic labels
y1(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) yn (cid:31)(cid:123) 1(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) c(cid:125)
that lie in some taxonomy T with c total topics. This taxonomy T gives
rise to some cost matrix C (cid:31) Rc(cid:215) c  where C(cid:31)(cid:30) (cid:30) 0 deﬁnes the cost of misclassifying an element
of topic (cid:30) as (cid:29) and C(cid:31)(cid:31) = 0. Technically  we only require knowledge of the cost matrix C  which
could also be obtained from side-information independent of a topic taxonomy. In this paper we will
not focus on how C is obtained. However  we would like to point out that a common way to infer a
cost matrix from a taxonomy is to set C(cid:31)(cid:30) to the length of the shortest path between node (cid:30) and (cid:29) 
but other approaches have also been studied [3].
Throughout this paper we denote document indices as i(cid:44) j (cid:31) (cid:123) 1(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) n(cid:125)
(cid:30)(cid:44) (cid:29) (cid:31)(cid:123) 1(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) c(cid:125)
Figure 1 illustrates our setup schematically. We would like to create a low dimensional semantic
feature space F in which we represent each topic (cid:30) as a topic prototype (cid:31)p(cid:31) (cid:31)F and each document
(cid:31)xi (cid:31) X as a low dimensional vector (cid:31)zi (cid:31) F. Our goal is to discover a representation of the
data where distances reﬂect true underlying dissimilarities and proximity to prototypes indicates
topic membership. In other words  documents on the same or related topics should be close to the
respective topic prototypes  documents on highly different topics should be well separated.

. Matrices are written in bold (e.g. C) and vectors have top arrows (e.g. (cid:31)xi).

and topic indices as

2

Throughout this paper we will assume that F = Rc  however our method can easily be adapted to
even lower dimensional settings F = Rr where r < c. As an essential part of our method is to
embed the classes that are typically found in a taxonomy  we refer to our algorithm as taxem (short
for “taxonomy embedding”).

Embedding topic prototypes
The ﬁrst step of our algorithm is to embed the document taxonomy into a Euclidean vector
space. More formally  we derive topic prototypes (cid:126)p1  . . .   (cid:126)pc ∈ F based on the cost matrix C 
where (cid:126)pα is the prototype that represents topic α. To simplify notation  we deﬁne the matrix
P = [(cid:126)p1  . . .   (cid:126)pc] ∈ Rc×c whose columns consist of the topic prototypes.
There are many ways to derive the prototypes from the cost matrix C. By far the simplest method
is to ignore the cost matrix C entirely and let PI = I  where I ∈ Rc×c denotes the identity matrix.
√
2
This results in a c dimensional feature space  where the class-prototypes are all in distance
from each other at the corner of a (c-1)-dimensional simplex. We will refer to PI as the simplex
prototypes.
Better results can be expected when the prototypes of similar topics are closer than those of dissim-
ilar topics. We use the cost matrix C as an estimate of dissimilarity and aim to place the prototypes
such that the distance (cid:107)(cid:126)pα − (cid:126)pβ(cid:107)2

2 reﬂects the cost speciﬁed in Cαβ. More formally  we set

c(cid:88)

Pmds = argminP

((cid:107)(cid:126)pα − (cid:126)pβ(cid:107)2

2 − Cαβ)2.

(1)

α β=1

√

If the cost matrix C deﬁnes squared Euclidean distances (e.g. when the cost is obtained through the
shortest path between nodes and then squared)  we can solve eq. (1) with metric multi-dimensional
scaling [5]. Let us denote ¯C = − 1
2 HCH  where the centering matrix H is deﬁned as H = I −
c 11(cid:62)  and let its eigenvector decomposition be ¯C = VΛV(cid:62). We obtain the solution by setting
1
Pmds =
Both prototypes embeddings PI and Pmds are still independent of the input data {(cid:126)xi}. Before we
can derive a more sophisticated method to place the prototypes with large margin constraints on the
document vectors  we will brieﬂy describe the mapping W : X → F of the input documents into
the low dimensional feature space F.

ΛV. We will refer to Pmds as the mds prototypes.1

Document regression
Assume for now that we have found a suitable embedding P of the class-prototypes. We need to
ﬁnd an appropriate mapping W : X → F  that maps each input (cid:126)xi with label yi as close as possible
to its topic prototype (cid:126)pyi. We can ﬁnd such a linear transformation (cid:126)zi = W(cid:126)xi by setting

W = argminW

(cid:107)(cid:126)pyi − W(cid:126)xi(cid:107)2 + λ(cid:107)W(cid:107)2
F .

(2)

(cid:88)

i

Here  λ is the weight of the regularization of W  which is necessary to prevent potential overﬁtting
due to the high number of parameters in W. The minimization in eq. (2) is an instance of linear
ridge regression and has the closed form solution

W = PJX(cid:62)(XX(cid:62) + λI)−1 

(3)
where X = [(cid:126)x1  . . . (cid:126)xn] and J ∈ {0  1}c×n  with Jαi = 1 if and only if yi = α. Please note that
eq. (3) can be solved very accurately without the need to ever compute the d × d matrix inverse
(XX(cid:62) + λI)−1 explicitly  by solving with linear conjugate gradient for each row of W indepen-
dently.

Inference
Given an input vector (cid:126)xt we ﬁrst map it into F and estimate its label as the topic with the closest
prototype (cid:126)pα

ˆyt = argminα(cid:107)(cid:126)pα − W(cid:126)xt(cid:107)2.

(4)

1If ¯C does not contain Euclidean distances one can use the common approximation of forcing negative
eigenvalues in Λ to zero and thereby fall back onto the projection of C onto the cone of positive semi-deﬁnite
matrices.

3

Topic taxonomy

T

α

!eα

I

yi
High dimensional input space

!xi

X
A

!eyi

!x!i

Rd

Rc

!pyi
!zi

P

embedded

inputs

Low dimensional semantic space

!pα

F

class

prototypes

large
margin

Rc

Figure 2: The schematic layout of the large-margin embedding of the taxonomy and the documents.
As a ﬁrst step  we represent topic (cid:30) as the vector (cid:31)e(cid:31) and document (cid:31)xi as (cid:31)x(cid:31)
i = A(cid:31)xi. We then learn the
matrix P whose columns are the prototypes (cid:31)p(cid:31) = P(cid:31)e(cid:31) and which deﬁnes the ﬁnal transformation
of the documents (cid:31)zi = P(cid:31)x(cid:31)
i. This ﬁnal transformation is learned such that the correct prototype (cid:31)pyi
is closer to (cid:31)zi than any other prototype (cid:31)p(cid:31) by a large margin.

For a given set of labeled documents ((cid:31)x1(cid:44) y1)(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) ((cid:31)xn(cid:44) yn) we measure the quality of our semantic
space with the averaged cost-sensitive misclassiﬁcation loss 

Cyi (cid:136)yi(cid:46)

(5)

E =

1
n

3 Large Margin Prototypes

n(cid:31)

i=1

So far we have introduced a two step approach: First  we ﬁnd the prototypes P based on the cost
matrix C  then we learn the mapping (cid:31)x (cid:29) W(cid:31)x that maps each input closest to the prototype of its
class. However  learning the prototypes independent of the data (cid:123) (cid:31)xi(cid:125)
is far from optimal in order
to reduce the loss in (5). In this section we will create a joint optimization problem that places the
prototypes P and learns the mapping W while minimizing an upper bound on (5).

Combined learning
In our attempt to learn both mappings jointly  we are faced with a “chicken and egg” problem. We
want to map the input documents closest to their prototypes and at the same time place the prototypes
where the documents of the respective topic are mapped to. Therefore our ﬁrst task is to de-tangle
this mutual dependency of W and P. Let us deﬁne A as the following matrix product:

A = JX(cid:30)(XX(cid:30) + (cid:28) I)(cid:29)1(cid:46)

(6)
It follows immediately form eqs. (3) and (6) that W = PA. Note that eq. (6) is entirely independent
of P and can be pre-computed before the prototypes have been positioned. With this relation we
have reduced the problem of determining W and P to the single problem of determining P. Figure 2
illustrates the new schematic layout of the algorithm.
i = A(cid:31)xi and let (cid:31)e(cid:31) = [0(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) 1(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) 0](cid:30) be the vector with all zeros and a single 1 in the (cid:30)th
Let (cid:31)x(cid:31)
position. We can then rewrite both  the topic prototypes (cid:31)p(cid:31) and the low dimensional documents (cid:31)zi 
as vectors within the range of P : Rc (cid:29) Rc:

(cid:31)p(cid:31) = P(cid:31)e(cid:31)(cid:44) and (cid:31)zi = P(cid:31)x(cid:31)
i(cid:46)

(7)

Optimization
Ideally we would like to learn P to minimize (5) directly. However  this function is non-continuous
and non-differentiable. For this reason we will derive a surrogate loss function that strictly
bounds (5) from above.

4

The loss for a speciﬁc document (cid:126)xi is zero if its corresponding vector (cid:126)zi is closer to the correct
prototype (cid:126)pyi than to any other prototype (cid:126)pα. For better generalization it would be preferable if
prototype (cid:126)pyi was in fact much closer by a large margin. We can go even further and demand that
prototypes that would incur a larger misclassiﬁcation loss should be further separated than those
with a small cost. More explicitly  we will try to enforce a margin of Cyiα. We can express this
condition as a set of “soft” inequality constraints  in terms of squared-distances 
2 + ξiα 

(8)
where the slack-variable ξiα ≥ 0 absorbs the amount of violation of prototype (cid:126)pα into the margin of
(cid:126)x(cid:48)
i. Given this formulation  we create an upper bound on the loss function (5):

2 + Cyiα ≤ (cid:107)P((cid:126)eα − (cid:126)x(cid:48)

(cid:107)P((cid:126)eyi − (cid:126)x(cid:48)

∀i  α(cid:54)= yi

i)(cid:107)2

i)(cid:107)2

Theorem 1 Given a prototype matrix P  the training error (5) is bounded above by 1
n
Proof: First  note that we can rewrite the assignment of the closest prototype (4) as ˆyi =
argminα(cid:107)P((cid:126)eα − (cid:126)x(cid:48)
2 ≥ 0 for all i (with
equality when ˆyi = yi). We therefore obtain:

It follows that (cid:107)P((cid:126)eyi − (cid:126)x(cid:48)

2 − (cid:107)P((cid:126)eˆyi − (cid:126)x(cid:48)

iα ξiα.

i)(cid:107)2

(cid:80)

The result follows immediately from (9) and that ξiα ≥ 0:

i)(cid:107)2.
ξiˆyi = (cid:107)P((cid:126)eyi − (cid:126)x(cid:48)
(cid:88)

i)(cid:107)2

2 + Cyi ˆyi − (cid:107)P((cid:126)eˆyi − (cid:126)x(cid:48)

i)(cid:107)2

ξiα ≥(cid:88)

ξiˆyi ≥(cid:88)

Cyi ˆyi.

i α

i

i

i)(cid:107)2
2 ≥ Cyi ˆyi.

(9)

(10)

(11)

Theorem 1  together with the constraints in eq. (8)  allows us to create an optimization problem that
minimizes an upper bound on the average loss in eq. (5) with maximum-margin constraints:

Minimize (cid:88)

P

i α

(1) (cid:107)P((cid:126)eyi − (cid:126)x(cid:48)
(2) ξiα ≥ 0

ξiα subject to:
i)(cid:107)2

2 + Cyiα ≤ (cid:107)P((cid:126)eα − (cid:126)x(cid:48)

i)(cid:107)2

2 + ξiα

Note that if we have a very large number of classes  it might be beneﬁcial to choose P ∈ Rr×c with
r < c. However  the convex formulation described in the next paragraph requires P to be square.

Convex formulation
The optimization in eq. (11) is not convex. The constraints of type (8) are quadratic with respect to
P. Intuitively  any solution P gives rise to inﬁnitely many solutions as any rotation of P results in
the same objective value and also satisﬁes all constraints. We can make (11) invariant to rotation by
deﬁning Q = P(cid:62)P  and rewriting all distances in terms of Q 
i)(cid:62)Q((cid:126)eα − (cid:126)x(cid:48)

(12)
Note that the distance formulation in eq. (12) is linear with respect to Q. As long as the matrix Q
is positive semi-deﬁnite  we can re-decompose it into Q = P(cid:62)P. Hence  we enforce positive semi-
deﬁniteness of Q by adding the constraint Q (cid:23) 0. We can now solve (11) in terms of Q instead of
P with the large-margin constraints

i) = (cid:107)(cid:126)eα − (cid:126)x(cid:48)

2 = ((cid:126)eα − (cid:126)x(cid:48)

(cid:107)P((cid:126)eα − (cid:126)x(cid:48)

i(cid:107)2
Q.

i)(cid:107)2

∀i  α(cid:54)= yi

(cid:107)(cid:126)eyi − (cid:126)x(cid:48)

i(cid:107)2
Q + Cyiα ≤ (cid:107)(cid:126)eα − (cid:126)x(cid:48)

i(cid:107)2
Q + ξiα.

(13)

Regularization
If the size of the training data n is small compared to the number of parameters c2  we might run
into problems of overﬁtting to the training data set. To counter those effects  we add a regularization
term to the objective function.
Even if the training data might differ from the test data  we know that the taxonomy does not change.
It is straight-forward to verify that if the mapping A was perfect  i.e. for all i we have (cid:126)x(cid:48)
i = (cid:126)eyi  Pmds
satisﬁes all constraints (8) as equalities with zero slack. This gives us conﬁdence that the optimal
solution P for the test data should not deviate too much from Pmds. We will therefore penalize

5

Top category
# samples n
# topics c
# nodes

A

7544
424
519

B

4772
160
312

C

4858
453
610

D

2701
339
608

E

7300
457
559

F

1961
151
218

G

8694
425
533

H

8155
150
170

Table 1: Statistics of the different OHSUMED problems. Note that not all nodes are populated and
that we pruned all strictly un-populated subtrees.

(cid:107)Q − ¯C(cid:107)2
taxem with regularized objective becomes:

F   where ¯C = P(cid:62)

Minimize (1 − µ)(cid:88)

Q

(1) (cid:107)(cid:126)eyi − (cid:126)x(cid:48)
(2) ξiα ≥ 0
(3) Q (cid:23) 0

mdsPmds (as deﬁned in section 2). The ﬁnal convex optimization of

ξiα + µ(cid:107)Q − ¯C(cid:107)2
i(cid:107)2
Q + ξiα

i(cid:107)2
Q + Cyiα ≤ (cid:107)(cid:126)eα − (cid:126)x(cid:48)

i α

F subject to:

(14)

The constant µ ∈ [0  1] regulates the impact of the regularization term. The optimization in (14) is
an instance of a semideﬁnite program (SDP) [2]. Although SDPs can often be expensive to solve 
the optimization (14) falls into a special category2 and can be solved very efﬁciently with special
purpose sub-gradient solvers even with millions of constraints [16]. Once the optimal solution Q∗
is found  one can obtain the position of the prototypes with a simple svd or cholesky decomposition
Q∗ =P(cid:62)P and consequently also obtains the mapping W from W = PA.

4 Results

We evaluated our algorithm taxem on several classiﬁcation problems derived from categorizing pub-
lications in the public OHSUMED medical journal data base into the Medical Subject Headings
(MeSH) taxonomy.

Setup and data set description
We used the OHSUMED 87 corpus [9]  which consists of abstracts and titles of medical publica-
tions. Each of these entries has been assigned one or more categories in the MeSH taxonomy3. We
used the 2001 version of these MeSH headings resulting in about 20k categories organized in a tax-
onomy. To preprocess the data we proceeded as follows: First  we discarded all database entries with
empty abstracts  which left us with 36890 documents. We tokenized (after stop word removal and
stemming) each abstract  and represented the corresponding bag of words as its d = 60727 dimen-
sional tﬁdf scores (normalized to unit length). We removed all topic categories that did not appear
in the MeSH taxonomy (due to out-dated topic names). We further removed all subtrees of nodes
that were populated with one or less documents. The top categories in the OHSUMED data base are
“orthogonal” — for instance the B top level category is about organism while C is about diseases.
We thus created 8 independent classiﬁcation problems out of the top categories A B C D E F G H.
For each problem  we kept only the abstracts that were assigned exactly one category in that tree 
making each problem single-label. The statistics of the different problems are summarized in Ta-
ble 1. For each problem  we created a 70%/30% random split in training and test samples  ensuring
however that each topic had at least one document in the training corpus.

Document Categorization
The classiﬁcation results on the OHSUMED data set are summarized in Table 2. We set the regular-
ization constants to be λ = 1 for the regression and µ = 0.1 for the SDP. Preliminary experiments
on data set B showed that regularization was important but the exact settings of the µ and λ had
no crucial impact. We derived the cost-matrix C from the tree hop-distance in all experiments. We

2The solver described in [16] utilizes that many constraints are inactive and that the sub-gradient can be

efﬁciently derived from previous gradient steps.

3see http://en.wikipedia.org/wiki/Medical_Subject_Headings

6

SVM tax PI-taxem Pmds-taxem LM-taxem

data
A
B
C
D
E
F
G
H
all

SVM 1/all MCSVM SVM cost

2.17
1.50
2.41
3.10
3.44
2.59
3.98
2.42
2.78

2.13
1.38
2.32
2.76
3.42
2.65
4.12
2.48
2.77

2.11
1.64
2.25
2.92
3.26
2.66
3.89
2.40
2.77

1.96
1.52
2.25
2.82
3.25
2.69
3.82
2.32
2.65

2.11
1.57
2.30
2.82
3.45
2.63
4.10
2.45
2.79

2.33
1.99
2.61
3.05
3.05
2.77
3.63
2.24
2.73

1.95
1.39
2.16
2.66
3.05
2.51
3.59
2.17
2.50

Table 2: The cost-sensitive test error results on various ohsumed classiﬁcation data sets. The algo-
rithms are from left to right: one vs. all SVM  MCSVM [6]  cost-sensitive MCSVM  Hierarchical
SVM [4]  simplex regression  mds regression  large-margin taxem. The best results (up to statistical
signiﬁcance) are highlighted in bold. The taxem algorithm obtains the lowest overall loss and the
lowest individual loss on each data set except B.

compared taxem against four commonly used algorithms for document categorization: 1. A lin-
ear support vector machine (SVM) trained in one vs. all mode (SVM 1/all) [12]  2. the Crammer
and Singer multi-class SVM formulation (MCSVM) [6]  3. the Cai and Hoffmann SVM classiﬁer
with cost-sensitive loss function (SVM cost) [4]  4. the Cai and Hoffmann SVM formulation with
a cost sensitive hierarchical loss function (SVM tax) [4]. All SVM classiﬁers were trained with
regularization constant C = 1 (which worked best on problem B; this value is also commonly used
in text classiﬁcation when the documents have unit length). Further  we also evaluated the differ-
ence between our large margin formulation (taxem) and the results with the simplex (PI-taxem)
and mds (Pmds-taxem) prototypes. To check the signiﬁcance of our results we applied a standard
t-test with a 5% conﬁdence interval. The best results up to statistical signiﬁcance are highlighted in
bold font. The ﬁnal entry in Table 2 shows the average error over all test points in all data sets. Up
to statistical signiﬁcance  taxem obtains the lowest loss on all data sets and the lowest overall loss.
Ignoring statistical signiﬁcance  taxem has the lowest loss on all data sets except B. All algorithms
had comparable speed during test-time. The computation time required for solving eq. (6) and the
optimization (14) was on the order of several minutes with our MATLABTM implementation on a
standard IntelTM 1.8GHz core 2 duo processor (without parallelization efforts).

5 Related Work

In recent years  several algorithms for document categorization have been proposed. Several authors
proposed adaptations of support vector machines that incorporate the topic taxonomy through cost-
sensitive loss re-weighting and classiﬁcation at multiple nodes in the hierarchy [4  8  11]. Our
algorithm is based on a very different intuition. It differs from all these methods in that it learns
a low dimensional semantic representation of the documents and classiﬁes by ﬁnding the nearest
prototype.
Most related to our work is probably the work by Karypis and Han [10]. Although their algorithm
also reduces the dimensionality with a linear projection  their low dimensional space is obtained
through supervised clustering on the document data. In contrast  the semantic space obtained with
taxem is obtained through a convex optimization with maximum margin constraints. Further  the
low dimensional representation of our method is explicitly constructed to give rise to meaningful
Euclidean distances.
The optimization with large-margin constraints was partially inspired by recent work on large margin
distance metric learning for nearest neighbor classiﬁcation [16]. However our formulation is a much
more light-weight optimization problem with O(cn) constraints instead of O(n2) as in [16]. The
optimization problem in section 3 is also related to recent work on automated speech recognition
through discriminative training of Gaussian mixture models [14].

7

6 Conclusion

In this paper  we have presented a novel framework for classiﬁcation with inter-class relationships
based on taxonomy embedding and supervised dimensionality reduction. We derived a single convex
optimization problem that learns an embedding of the topic taxonomy as well as a linear mapping
from the document space to the resulting low dimensional semantic space.
As future work we are planning to extend our algorithm to the more general setting of document
categorization with multiple topic memberships and multi-modal topic distributions. Further  we
are keen to explore the implications of our proposed conversion of discrete topic taxonomies into
continuous semantic spaces. This framework opens new interesting directions of research that go
beyond mere classiﬁcation. A natural step is to consider the document matching problem (e.g.
of web pages and advertisements) in the semantic space: a fast nearest neighbor search can be
performed in a joint low dimensional space without having to resort to classiﬁcation all together.
Although this paper is presented in the context of document categorization  it is important to empha-
size that our method is by no means limited to text data or class hierarchies. In fact  the proposed
algorithm can be applied in almost all multi-class settings with cost-sensitive loss functions (e.g.
object recognition in computer vision).

References
[1] D. Blei  A. Ng  M. Jordan  and J. Lafferty. Latent Dirichlet Allocation. Journal of Machine Learning

Research  3(4-5):993–1022  2003.

[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[3] A. Budanitsky and G. Hirst. Semantic distance in wordnet: An experimental  application-oriented eval-
uation of ﬁve measures. In Workshop on WordNet and Other Lexical Resources  in the North American
Chapter of the Association for Co mputational Linguistics (NAACL)  2001.

[4] L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In ACM

13th Conference on Information and Knowledge Management  2004.

[5] T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall  London  1994.
[6] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector ma-

chines. Journal of Machine Learning Research  2:265–292  2001.

[7] S. Deerwester  S. Dumais  G. Furnas  T. Landauer  and R. Harshman. Indexing by latent semantic analysis.

Journal of the American Society for Information Science  41(6):391–407  1990.

[8] S. Dumais and H. Chen. Hierarchical classiﬁcation of Web content. In Proceedings of SIGIR-00  23rd
ACM International Conference on Research and Development in Information Retrieval  pages 256–263.
ACM Press  New York  US  2000.

[9] W. Hersh  C. Buckley  T. J. Leone  and D. Hickam. OHSUMED: an interactive retrieval evaluation and
new large test collection for research. In SIGIR ’94: Proceedings of the 17th annual international ACM
conference on Research and development in information retrieval  pages 192–201. Springer-Verlag New
York  Inc.  1994.

[10] G. Karypis  E. Hong  and S. Han. Concept indexing a fast dimensionality reduction algorithm with appli-
cations to document retrieval & categorization  2000. Technical Report: 00-016 karypis  han@cs.umn.edu
Last updated on.

[11] T.-Y. Liu  Y. Yang  H. Wan  H.-J. Zeng  Z. Chen  and W.-Y. Ma. Support vector machines classiﬁcation

with a very large-scale taxonomy. SIGKDD Explorations Newsletter  7(1):36–43  2005.

[12] R. Rifkin and A. Klautau. In Defense of One-Vs-All Classiﬁcation. The Journal of Machine Learning

Research  5:101–141  2004.

[13] F. Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys  34(1):1–47 

2002.

[14] F. Sha and L. K. Saul. Large margin hidden markov models for automatic speech recognition. In Advances

in Neural Information Processing Systems 19  Cambridge  MA  2007. MIT Press.

[15] A. Weigend  E. Wiener  and J. Pedersen. Exploiting Hierarchy in Text Categorization.

Retrieval  1(3):193–216  1999.

Information

[16] K. Q. Weinberger and L. K. Saul. Fast solvers and efﬁcient implementations for distance metric learning.

pages 1160–1167  2008.

8

,Xia Qu
Prashant Doshi
Huasen Wu
Xin Liu