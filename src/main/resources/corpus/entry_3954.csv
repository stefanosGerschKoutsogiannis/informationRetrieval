2017,On the Complexity of Learning Neural Networks,The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take  in the face of existing complexity-theoretic lower bounds? A first step might be to show that data generated by neural networks with a single hidden layer  smooth activation functions and benign input distributions can be learned efficiently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used)  and inputs drawn from any logconcave distribution  there is a family of one-hidden-layer functions whose output is a sum gate that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover  this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.,On the Complexity of Learning Neural Networks

Le Song

Georgia Institute of Technology

Atlanta  GA 30332

lsong@cc.gatech.edu

Santosh Vempala

Georgia Institute of Technology

Atlanta  GA 30332

vempala@gatech.edu

Georgia Institute of Technology

Georgia Institute of Technology

John Wilmes

Atlanta  GA 30332

wilmesj@gatech.edu

Bo Xie

Atlanta  GA 30332

bo.xie@gatech.edu

Abstract

The stunning empirical successes of neural networks currently lack rigorous the-
oretical explanation. What form would such an explanation take  in the face of
existing complexity-theoretic lower bounds? A ﬁrst step might be to show that
data generated by neural networks with a single hidden layer  smooth activation
functions and benign input distributions can be learned efﬁciently. We demonstrate
here a comprehensive lower bound ruling out this possibility: for a wide class
of activation functions (including all currently used)  and inputs drawn from any
logconcave distribution  there is a family of one-hidden-layer functions whose
output is a sum gate  that are hard to learn in a precise sense: any statistical query
algorithm (which includes all known variants of stochastic gradient descent with
any loss function) needs an exponential number of queries even using tolerance
inversely proportional to the input dimensionality. Moreover  this hard family of
functions is realizable with a small (sublinear in dimension) number of activation
units in the single hidden layer. The lower bound is also robust to small perturba-
tions of the true weights. Systematic experiments illustrate a phase transition in the
training error as predicted by the analysis.

1

Introduction

It is well-known that Neural Networks (NN’s) provide universal approximate representations [11  6  2]
and under mild assumptions  i.e.  any real-valued function can be approximated by a NN. This holds
for a wide class of activation functions (hidden layer units) and even with only a single hidden layer
(although there is a trade-off between depth and width [8  20]). Typically learning a NN is done by
stochastic gradient descent applied to a loss function comparing the network’s current output to the
values of the given training data; for regression  typically the function is just the least-squares error.
Variants of gradient descent include drop-out  regularization  perturbation  batch gradient descent etc.
In all cases  the training algorithm has the following form:

Repeat:

1. Compute a ﬁxed function FW (.) deﬁned by the current network weights W on a subset

of training examples.

2. Use FW (.) to update the current weights W .

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

The empirical success of this approach raises the question: what can NN’s learn efﬁciently in theory?
In spite of much effort  at the moment there are no satisfactory answers to this question  even with
reasonable assumptions on the function being learned and the input distribution.
When learning involves some computationally intractable optimization problem  e.g.  learning an
intersection of halfspaces over the uniform distribution on the Boolean hypercube  then any training
algorithm is unlikely to be efﬁcient. This is the case even for improper learning (when the complexity
of the hypothesis class being used to learn can be greater than the target class). Such lower bounds are
unsatisfactory to the extent they rely on discrete (or at least nonsmooth) functions and distributions.
What if we assume that the function to be learned is generated by a NN with a single hidden layer of
smooth activation units  and the input distribution is benign? Can such functions be learned efﬁciently
by gradient descent?
Our main result is a lower bound  showing a simple and natural family of functions generated by
1-hidden layer NN’s using any known activation function (e.g.  sigmoid  ReLU)  with each input
drawn from a logconcave input distribution (e.g.  Gaussian  uniform in an interval)  are hard to learn
by a wide class of algorithms  including those in the general form above. Our ﬁnding implies that
efﬁcient NN training algorithms need to use stronger assumptions on the target function and input
distribution  more so than Lipschitzness and smoothness even when the true data is generated by a
NN with a single hidden layer.
The idea of the lower bound has two parts. First  NN updates can be viewed as statistical queries to
the input distribution. Second  there are many very different 1-layer networks  and in order to learn
the correct one  any algorithm that makes only statistical queries of not too small accuracy has to
make an exponential number of queries. The lower bound uses the SQ framework of Kearns [13] as
generalized by Feldman et al. [9].

1.1 Statistical query algorithms

A statistical query (SQ) algorithm is one that solves a computational problem over an input dis-
tribution; its interaction with the input is limited to querying the expected value of of a bounded
function up to a desired accuracy. More precisely  for any integer t > 0 and distribution D over X  a
VSTAT(t) oracle takes as input a query function f : X → [0  1] with expectation p = ED(f (x))
and returns a value v such that

(cid:12)(cid:12)(cid:12) E

x∼D

(cid:114)

(cid:40)

(cid:12)(cid:12)(cid:12) ≤ max

(cid:41)

(f (x)) − v

1
t

 

p(1 − p)

t

.

The bound on the RHS is the standard deviation of t independent Bernoulli coins with desired
expectation  i.e.  the error that even a random sample of size t would yield. In this paper  we study
SQ algorithms that access the input distribution only via the VSTAT(t) oracle. The remaining
computation is unrestricted and can use randomization (e.g.  to determine which query to ask next).
In the case of an algorithm training a neural network via gradient descent  the relevant query functions
are derivatives of the loss function.
The statistical query framework was ﬁrst introduced by Kearns for supervised learning problems [14]
using the STAT(τ ) oracle  which  for τ ∈ R+  responds to a query function f : X → [0  1] with a
value v such that | ED(f )−v| ≤ τ. The STAT(√τ ) oracle can be simulated by the VSTAT(O(1/τ ))
oracle. The VSTAT oracle was introduced by [9] who extended these oracles to general problems
over distributions.

1.2 Main result
We will describe a family C of functions f : Rn → R that can be computed exactly by a small NN 
but cannot be efﬁciently learned by an SQ algorithm. While our result applies to all commonly
used activation units  we will use sigmoids as a running example. Let σ(z) be the sigmoid gate that
goes to 0 for z < 0 and goes to 1 for z > 0. The sigmoid gates have sharpness parameter s  i.e. 
σ(x) = σs(x) = (1 + e−sx)−1. Note that the parameter s also bounds the Lipschitz constant of
σ(x).

2

∈ C is a ﬁxed concept (function).

A function f : Rn → R can be computed exactly by a single layer NN with sigmoid gates precisely
when it is of the form f (x) = h(σ(g(x))  where g : Rn → Rm and h : Rm → R are afﬁne  and σ
acts component-wise. Here  m is the number of hidden units  or sigmoid gates  of the of the NN.
In the case of a learning problem for a class C of functions f : X → R  the input distribution to the
algorithm is over labeled examples (x  f∗(x))  where x ∼ D for some underlying distribution D on
X  and f∗
As mentioned in the introduction  we can view a typical NN learning algorithm as a statistical query
(SQ) algorithm: in each iteration  the algorithm constructs a function based on its current weights
(typically a gradient or subgradient)  evaluates it on a batch of random examples from the input
distribution  then uses the evaluations to update the weights of the NN. Then we have the following
result.
Theorem 1.1. Let n ∈ N  and let λ  s ≥ 1. There exists an explicit family C of functions f : Rn →
[−1  1]  representable as a single hidden layer neural network with O(s√n log(λsn)) sigmoid units
of sharpness s  a single output sum gate and a weight matrix with condition number O(poly(n  s  λ)) 
and an integer t = Ω(s2n) s.t. the following holds. Any (randomized) SQ algorithm A that uses λ-
Lipschitz queries to VSTAT(t) and weakly learns C with probability at least 1/2  to within regression
error 1/√t less than any constant function over i.i.d. inputs from any logconcave distribution of unit
variance on R requires 2Ω(n)/(λs2) queries.

The Lipschitz assumption on the statistical queries is satisﬁed by all commonly used algorithms
for training neural networks can be simulated with Lipschitz queries (e.g.  gradients of natural loss
functions with regularizers). This assumption can be omitted if the output of the hard-to-learn family
C is represented with bounded precision.
Informally  Theorem 1.1 shows that there exist simple realizable functions that are not efﬁciently
learnable by NN training algorithms with polynomial batch sizes  assuming the algorithm allows for
error as much as the standard deviation of random samples for each query. We remark that in practice 
large batch sizes are seldom used for training NNs  not just for efﬁciency  but also since moderately
noisy gradient estimates are believed to be useful for avoiding bad local minima. Even NN training
algorithms with larger batch sizes will require Ω(t) samples to achieve lower error  whereas the NNs

that represent functions in our class C have only (cid:101)O(√t) parameters.
φm((cid:80)

Our lower bound extends to a broad family of activation units  including all the well-known ones
(ReLU  sigmoid  softplus etc.  see Section 3.1). In the case of sigmoid gates  the functions of C
take the following form (cf. Figure 1.1). For a set S ⊆ {1  . . .   n}  we deﬁne fm S(x1  . . .   xn) =

i∈S xi)  where

φm(x) = −(2m + 1) +

σ

(4k − 1)

s

x −

+ σ

(1.1)

(cid:18)

m(cid:88)

k=−m

(cid:19)

(cid:18) (4k + 1)

(cid:19)

s

− x

.

Then C = {fm S : S ⊆ {1  . . .   n}}. We call the functions fm S  along with φm  the s-wave
functions. It is easy to see that they are smooth and bounded. Furthermore  the size of the NN
representing this hard-to-learn family of functions is only ˜O(s√n)  assuming the query functions
(e.g.  gradients of loss function) are poly(s  n)-Lipschitz. We note that the lower bounds hold
regardless of the architecture of the model  i.e.  NN used to learn.
Our lower bounds are asymptotic  but we show empirically in Section 4 that they apply even at
practical values of n and s. We experimentally observe a threshold for the quantity s√n  above
which stochastic gradient descent fails to train the NN to low error—that is  regression error below
that of the best constant approximation— regardless of choices of gates  architecture used to learning 
learning rate  batch size  etc.
The condition number upper bound for C is signiﬁcant in part because there do exist SQ algorithms
for learning certain families of simple NNs with time complexity polynomial in the condition number
of the weight matrix (the tensor factorization based algorithm of Janzamin et al. [12] can easily
be seen to be SQ). Our results imply that this dependence cannot be substantially improved (see
Section 1.3).
Remark 1. The class of input distributions can be relaxed further. Rather than being a product
distribution  it sufﬁces if the distribution is in isotropic position and invariant under reﬂections across

3

Figure 1.1: (a) The sigmoid function  the L1-function ψ constructed from sigmoid functions  and the
nearly-periodic “wave” function φ constructed from ψ. (b) The architecture of the NNs computing
the wave functions.

and permutations of coordinate axes. And instead of being logconcave  it sufﬁces for marginals to
be unimodal with variance σ  density O(1/σ) at the mode  and density Ω(1/σ) within a standard
deviation of the mode.

Overall  our lower bounds suggest that even the combination of small network size  smooth  standard
activation functions  and benign input distributions is insufﬁcient to make learning a NN easy  even
improperly via a very general family of algorithms. Instead  stronger structural assumptions on the
NN  such as a small condition number  and very strong structural properties on the input distribution 
are necessary to make learning tractable. It is our hope that these insights will guide the discovery of
provable efﬁciency guarantees.

1.3 Related Work

There is much work on complexity-theoretic hardness of learning neural networks [4  7  15]. These
results have shown the hardness of learning functions representable as small (depth 2) neural networks
over discrete input distributions. Since these input distributions bear little resemblance to the real-
world data sets on which NNs have seen great recent empirical success  it is natural to wonder whether
more realistic distributional assumptions might make learning NNs tractable. Our results suggest
that benign input distributions are insufﬁcient  even for functions realized as small networks with
standard  smooth activation units.
Recent independent work of Shamir [17] shows a smooth family of functions for which the gradient
of the squared loss function is not informative for training a NN over a Gaussian input distribution
(more generally  for distributions with rapidly decaying Fourier coefﬁcients). In fact  for this setting
the paper shows an exponentially small bound on the gradient  relying on the ﬁne structure of the
Gaussian distribution and of the smooth functions (see [16] for a follow-up with experiments and
further ideas). These smooth functions cannot be realized in small NNs using the most commonly
studied activation units (though a related non-smooth family of functions for which the bounds apply
can be realized by larger NNs using ReLU units). In contrast our bounds are (a) in the more general
SQ framework  and in particular apply regardless of the loss function  regularization scheme  or
speciﬁc variant of gradient descent (b) apply to functions actually realized as small NNs using any of
a wide family of activation units (c) apply to any logconcave input distribution and (d) are robust to
small perturbations of the input layer weights.
Also related is the tensor-based algorithm of Janzamin et al. [12] to learn a 1-layer network under
nondegeneracy assumptions on the weight matrix. The complexity is polynomial in the dimension 
size of network being learned and condition number of the weight matrix. Since their tensor
decomposition can also be implemented as a statistical query algorithm  our results give a lower
bound indicating that such a polynomial dependence on the dimension and condition number is
unavoidable.
Other algorithmic results for learning NNs apply in very restricted settings. For example  polynomial-
time bounds are known for learning NNs with a single hidden ReLU layer over Gaussian inputs under

4

σ(1/s+x)σ(1/s−x)ψ(x)=σ(1/s+x)+σ(1/s−x)−1φm(x)=ψ(x)+ψ(x−4/s)+ψ(x+4/s)+···the assumption that the hidden units use disjoint sets of inputs [5]  as well as for learning a single
ReLU [10] and for learning sparse polynomials via NNs [1].

1.4 Proof ideas

To prove Theorem 1.1  we wish to estimate the number of queries used by a statistical query algorithm
learning the family of s-wave functions  regardless of the strategy employed by the algorithm. To that
end  we estimate the statistical dimension of the family of s-wave functions. Statistical dimension
is a key concept in the study of SQ algorithms  and is known to characterize the query complexity
of supervised learning via SQ algorithms [3  19  9]. Brieﬂy  a family C of distributions (e.g.  over
labeled examples) has “statistical dimension d with average correlation ¯γ” if every (1/d)-fraction
of C has average correlation ¯γ; this condition implies that C cannot be learned with fewer than O(d)
queries to VSTAT(O(1/¯γ)). See Section 2 for precise statements.
The SQ literature for supervised learning of boolean functions is rich. However  lower bounds for
regression problems in the SQ framework have so far not appeared in the literature  and the existing
notions of statistical dimension are too weak for this setting. We state a new  strengthened notion
of statistical dimension for regression problems (Deﬁnition 2)  and show that lower bounds for this
dimension transfer to query complexity bounds (Theorem 2.1). The essential difference from the
statistical dimension for learning is that we must additionally bound the average covariances of
indicator functions (or  rather  continuous analogues of indicators) on the outputs of functions in
C. The essential claim in our lower bounds is therefore in showing that a typical pair of (indicator
functions on outputs of) s-wave functions has small covariance.
In other words  to prove Theorem 1.1  it sufﬁces to upper-bound the quantity
E[(χ ◦ fm S)(χ ◦ fm T )] − E[χ ◦ fm S]E[χ ◦ fm T )

(1.2)
for most pairs fm S  fm T of s-wave functions  where χ is some smoothed version of an indicator

function. Write h(t) = χ(φm(t))  so χ(fm S(x1  . . .   xn)) = h((cid:80)
(cid:88)
(cid:88)
(cid:88)
So to estimate Eq. (1.2)  it sufﬁces to show that the expectation of h((cid:80)
when we condition on the value of z =(cid:80)

xi) |
i∈S∩T
E

(x1 ... xn)∼D

(cid:88)

(cid:88)

xi i∈T\S

xi i∈S\T

xi + z))

xi = z)

i∈T\S

i∈S\T

i∈S

(h(

xi)h(

E

E

i∈T

=

(h(

i∈S∩T xi.

i∈S xi). We have

(h(

xi + z)) .

i∈S xi) doesn’t change much

We now observe that if χ is Lipschitz  and φm is “close to” a periodic function with period θ > 0 
then h is also “close to” a periodic function with period θ > 0 (see Section 3 for a precise statement).
Under this near-periodicity assumption  we are now able to show for any logconcave distribution D(cid:48)
(cid:18) θ
on R of variance σ > θ  and any translation z ∈ R  that
In particular  conditioning on the value of z =(cid:80)

(cid:12)(cid:12)(cid:12) = O
i∈S∩T xi has little effect on the value of h((cid:80)

i∈S xi).
The combination of these observations gives the query complexity lower bound. Precise statements
of some of the technical lemmas are given in Section 3; the complete proof appears in the full version
of this paper [18].

(h(x + z) − h(x))

(cid:12)(cid:12)(cid:12) E

(|h(x)|) .

E
x∼D

(cid:19)

x∼D

σ

2 Statistical dimension

We now give a precise deﬁnition of the statistical dimension with average correlation for regression
problems  extending the concept introduced in [9].
Let C be a ﬁnite family of functions f : X → R over some domain X  and let D be a distribution
over X. The average covariance and the average correlation of C with respect to D are
ρD(f  g)

(cid:88)

(cid:88)

CovD(f  g)

and

ρD(C) =

1
|C|2

f g∈C

CovD(C) =

1
|C|2

f g∈C

5

(cid:112)

where ρD(f  g) = CovD(f  g)/
ρD(f  g) = 0 otherwise.
For y ∈ R and  > 0  we deﬁne the -soft indicator function χ()

y

: R → R as

Var(f ) Var(g) when both Var(f ) and Var(g) are nonzero  and

χ()
y (x) = χy(x) = max{0  1/ − (1/)2|x − y|}.

(cid:48)

(cid:48)

(cid:48)
y) ≤ (max{  µ(y)})2¯γ where C

So χy is (1/)2-Lipschitz  is supported on (y −   y + )  and has norm (cid:107)χy(cid:107)1 = 1.
Deﬁnition 2. Let ¯γ > 0  let D be a probability distribution over some domain X  and let C
be a family of functions f : X → [−1  1] that are identically distributed as random variables
over D. The statistical dimension of C relative to D with average covariance ¯γ and precision  
denoted by -SDA(C  D  ¯γ)  is deﬁned to be the largest integer d such that the following holds:
(cid:48)) ≤ ¯γ. Moreover 
for every y ∈ R and every subset C
y ◦ f ) for some
CovD(C
f ∈ C.
Note that the parameter µ(y) is independent of the choice of f ∈ C. The application of this notion of
dimension is given by the following theorem.
Theorem 2.1. Let D be a distribution on a domain X and let C be a family of functions f :
X → [−1  1] identically distributed as random variables over D. Suppose there is d ∈ R and
λ ≥ 1 ≥ ¯γ > 0 such that -SDA(C  D  ¯γ) ≥ d  where  ≤ ¯γ/(2λ). Let A be a randomized algorithm
learning C over D with probability greater than 1/2 to regression error less than Ω(1) − 2√¯γ. If
A only uses queries to VSTAT(t) for some t = O(1/¯γ)  which are λ-Lipschitz at any ﬁxed x ∈ X 
then A uses Ω(d) queries.

⊆ C of size |C
(cid:48)
y = {χ()

| > |C|/d  we have ρD(C

y ◦ f : f ∈ C} and µ(y) = ED(χ()

A version of the theorem for Boolean functions is proved in [9]. For completeness  in the full version
of this paper [18] we include a proof of Theorem 2.1  following ideas in [19  Theorem 2].
As a consequence of Theorem 2.1  there is no need to consider an SQ algorithm’s query strategy in
order to obtain lower bounds on its query complexity. Instead  the lower bounds follow directly from
properties of the concept class itself  in particular from bounds on average covariances of indicator
functions. Theorem 1.1 will therefore follow from Theorem 2.1 by analyzing the statistical dimension
of the s-wave functions.

3 Estimates of statistical dimension for one-layer functions

We now present the most general context in which we obtain SQ lower bounds.
A function φ : R → R is (M  δ  θ)-quasiperiodic if there exists a function ˜φ : R → R which is
periodic with period θ such that |φ(x) − ˜φ(x)| < δ for all x ∈ [−M  M ]. In particular  any periodic
function with period θ is (M  δ  θ)-quasiperiodic for all M  δ > 0.
Lemma 3.1. Let n ∈ N and let θ > 0. There exists ¯γ = O(θ2/n) such that for all  > 0  there
exist M = O(√n log(n/(θ)) and δ = Ω(3θ/√n) and a family C0 of afﬁne functions g : Rn → R
of bounded operator norm with the following property. Suppose φ : R → [−1  1] is (M  δ  θ)-
quasiperiodic and Varx∼U (0 θ)(φ(x)) = Ω(1). Let D be logconcave distribution with unit variance
on R. Then for C = {φ ◦ g : g ∈ C0}  we have -SDA(C  Dn  ¯γ) ≥ 2Ω(n)θ2. Furthermore  the
functions of C are identically distributed as random variables over Dn.
In other words  we have statistical dimension bounds (and hence query complexity bounds) for
functions that are sufﬁciently close to periodic. However  the activation units of interest are generally
monotonic increasing functions such as sigmoids and ReLUs that are quite far from periodic. Hence 
in order to apply Lemma 3.1 in our context  we must show that the activation units of interest can be
combined to make nearly periodic functions.
As an intermediate step  we analyze activation functions in L1(R)  i.e.  functions whose absolute
value has bounded integral over the whole real line. These L1-functions analyzed in our framework
are themselves constructed as afﬁne combinations of the usual activation functions. For example  for
the sigmoid unit with sharpness s  we study the following L1-function (cf. (1.1)):

ψ(x) = σ

s − x

− 1.

(3.1)

(cid:18) 1

(cid:19)

(cid:19)

(cid:18) 1

s

+ x

+ σ

6

(cid:82) r
We now describe the properties of the integrable functions ψ that will be used in the proof.
Deﬁnition 3. For ψ ∈ L1(R)  we say the essential radius of ψ is the number r ∈ R such that
−r |ψ| = (5/6)(cid:107)ψ(cid:107)1.
Deﬁnition 4. We say ψ ∈ L1(R) has the mean bound property if for all x ∈ R and  > 0  we have

(cid:90) x+

(cid:18) 1



(cid:19)

ψ(x) = O

x− |ψ(x)|

.

In particular  if ψ is bounded  and monotonic nonincreasing (resp. nondecreasing) for sufﬁciently
large positive (resp. negative) inputs  then ψ satisﬁes Deﬁnition 4. Alternatively  it sufﬁces for ψ to
have bounded ﬁrst derivative.
To complete the proof of Theorem 1.1  we show that we can combine activation units ψ satisfying
the above properties in a function which is close to periodic  i.e.  which satisﬁes the hypotheses of
Lemma 3.1 above.
Lemma 3.2. Let ψ ∈ L1(R) have the mean bound property and let r > 0 be such that ψ has
essential radius at most r and (cid:107)ψ(cid:107)1 = Θ(r). Let M  δ > 0. Then there is a pair of afﬁne
functions h : Rm → R and g : R → Rm such that if φ(x) = h(ψ(g(x)))  where ψ is applied
component-wise  then φ is (M  δ  4r)-quasiperiodic. Furthermore  φ(x) ∈ [−1  1] for all x ∈ R  and
Varx∼U (0 4r)(φ(x)) = Ω(1)  and we may take m = (1/r) · O(max{m1  M})  where m1 satisﬁes

(cid:90) ∞

m1

(|ψ(x)| + |ψ(−x)|)dx < 4δr .

We now sketch how Lemmas 3.1 and 3.2 imply Theorem 1.1 for sigmoid units.
Sketch of proof of Theorem 1.1. The sigmoid function σ with sharpness s is not even in L1(R)  so it
is unsuitable as the function ψ of Lemma 3.2. Instead  we deﬁne ψ to be an afﬁne combination of σ
gates as in Eq. (3.1). Then ψ satisﬁes the hypotheses of Lemma 3.2.
Let θ = 4r and let ¯γ = O(θ2/n) be as given by the statement of Lemma 3.1. Let  = ¯γ/(2λ)  and
let M = O(√n log(n/(θ)) and δ = Ω(3θ/√n) be as given by the statement of Lemma 3.1. By
Lemma 3.2  there is m ∈ N and functions h : Rm → R and g : R → Rm such that φ = h ◦ ψ ◦ g is
(M  δ  θ)-quasiperiodic and satisﬁes the hypotheses of Lemma 3.1. Therefore  we have a family C0 of
afﬁne functions f : Rn → R such that for C = {φ◦f : f ∈ C0} satisﬁes -SDA(C  D  ¯γ) ≥ 2Ω(n)θ2.
Therefore  the functions in C satisfy the hypothesis of Theorem 2.1  giving the query complexity
lower bound.
All details are given in the full version of the paper [18].

3.1 Different activation functions

ψ(x) = σ(x + 1/s) − σ(x) + σ(−x + 1/s) − σ(−x) − 1

Similar proofs give corresponding lower bounds for activation functions other than sigmoids. In
every case  we reduce to gates satisfying the hypotheses of Lemma 3.2 by constructing an appropriate
L1-function ψ as an afﬁne combination of of the activation functions.
For example  let σ(x) = σs(x) = max{0  sx} denote the ReLU unit with slope s. Then the afﬁne
combination
(3.2)
is in L1(R)  and is zero for |x| ≥ 1/s (and hence has the mean bound property and essential radius
O(1/s)). The proof of Theorem 1.1 therefore goes through almost identically  the slope-s ReLU
units replacing the s-sharp sigmoid units. In particular  there is a family of single hidden layer
NNs using O(s√n log(λsn) slope-s ReLU units  which is not learned by any SQ algorithm using
fewer than 2Ω(n)/(λs2) queries to VSTAT(O(s2n))  when inputs are drawn i.i.d. from a logconcave
distribution.
Similarly  we can consider the s-sharp softplus function σ(x) = log(exp(sx) + 1). Then Eq. (3.2)
again gives an appropriate L1(R) function to which we can apply Lemma 3.2 and therefore follow
the proof of Theorem 1.1. For softsign functions σ(x) = x/(|x| + 1)  we use the afﬁne combination

ψ(x) = σ(x + 1) + σ(−x + 1) .

7

(a) normal distribution

(b) exp(−|xi|) distribution

(c) uniform l1 ball

(d) normal distribution

(e) exp(−|xi|) distribution

(f) uniform l1 ball

Figure 4.1: Test error vs sharpness times square-root of dimension. Each curve corresponds to a
different input dimension n. The ﬂat line corresponds to the best error by a constant function.

In the case of softsign functions  this function ψ converges much more slowly to zero as |x| → ∞
compared to sigmoid units. Hence  in order to obtain an adequate quasiperiodic function as an afﬁne
combination of ψ-units  a much larger number of ψ-units is needed: the bound on the number m
of units in this case is polynomial in the Lipschitz parameter λ of the query functions  and a larger
polynomial in the input dimension n. The case of other commonly used activation functions  such as
ELU (exponential linear) or LReLU (Leaky ReLU)  is similar to those discussed above.

4 Experiments
In the experiments  we show how the errors  E(f (x) − y)2  change with respect to the sharpness
parameter s and the input dimension n for two input distributions: 1) multivariate normal distribution 

2) coordinate-wise independent exp(−|xi|)  and 3) uniform in the l1 ball {x :(cid:80)

i |xi| ≤ n}.

For a given sharpness parameter s ∈ {0.01  0.02  0.05  0.1  0.2  0.5  1  2}  input dimension d ∈
{50  100  200} and input distribution  we generate the true function according to Eqn. 1.1. There are
a total of 50 000 training data points and 1000 test data points. We then learn the true function with
fully-connected neural networks of both ReLU and sigmoid activation functions. The best test error
is reported among the following different hyper-parameters.
The number of hidden layers we used is 1  2  and 4. The number of hidden units per layer varies from
4n to 8n. The training is carried out using SGD with 0.9 momentum  and we enumerate learning
rates from 0.1  0.01 and 0.001 and batch sizes from 64  128 and 256.
From Theorem 1.1  learning such functions should become difﬁcult as s√n increases over a threshold.
In Figure 4.1  we illustrate this phenomenon. Each curve corresponds to a particular input dimension
n and each point in the curve corresponds to a particular smoothness parameter s. The x-axis is s√n
and the y-axis denotes the test errors. We can see that at roughly s√n = 5  the problem becomes
hard even empirically.

Acknowledgments

The authors are grateful to Vitaly Feldman for discussions about statistical query lower bounds  and
for suggestions that simpliﬁed the presentation of our results  and also to Adam Kalai for an inspiring
discussion. This research was supported in part by NSF grants CCF-1563838 and CCF-1717349.

8

References
[1] Alexandr Andoni  Rina Panigrahy  Gregory Valiant  and Li Zhang. Learning polynomials with
neural networks. In International Conference on Machine Learning  pages 1908–1916  2014.

[2] Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.

IEEE Transactions on Information theory  39(3):930–945  1993.

[3] Avrim Blum  Merrick Furst  Jeffrey Jackson  Michael Kearns  Yishay Mansour  and Steven
Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier
analysis. In STOC  pages 253–262  1994.

[4] Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is NP-complete. Neural

Networks  5(1):117–127  1992.

[5] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with

gaussian inputs. CoRR  abs/1702.07966  2017.

[6] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of

Control  Signals  and Systems (MCSS)  2(4):303–314  1989.

[7] Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf’s. In
Proceedings of the 29th Conference on Learning Theory  COLT 2016  New York  USA  June
23-26  2016  pages 815–830  2016.

[8] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In

Conference on Learning Theory  pages 907–940  2016.

[9] Vitaly Feldman  Elena Grigorescu  Lev Reyzin  Santosh Vempala  and Ying Xiao. Statistical
algorithms and a lower bound for planted clique. In Proceedings of the 45th annual ACM
Symposium on Theory of Computing  pages 655–664. ACM  2013.

[10] Surbhi Goel  Varun Kanade  Adam R. Klivans  and Justin Thaler. Reliably learning the ReLU

in polynomial time. CoRR  abs/1611.10258  2016.

[11] Kurt Hornik  Maxwell Stinchcombe  and Halbert White. Multilayer feedforward networks are

universal approximators. Neural networks  2(5):359–366  1989.

[12] Majid Janzamin  Hanie Sedghi  and Anima Anandkumar. Generalization bounds for neural

networks through tensor factorization. CoRR  abs/1506.08473  2015.

[13] Michael Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the ACM 

45(6):983–1006  1998.

[14] Michael J. Kearns. Efﬁcient noise-tolerant learning from statistical queries. In Proceedings
of the Twenty-Fifth Annual ACM Symposium on Theory of Computing  May 16-18  1993  San
Diego  CA  USA  pages 392–401  1993.

[15] Adam R. Klivans. Cryptographic hardness of learning. In Encyclopedia of Algorithms  pages

475–477. 2016.

[16] Shai Shalev-Shwartz  Ohad Shamir  and Shaked Shammah. Failures of deep learning. CoRR 

abs/1703.07950  2017.

[17] Ohad Shamir.

Distribution-speciﬁc hardness of learning neural networks.

abs/1609.01037  2016.

CoRR 

[18] Le Song  Santosh Vempala  John Wilmes  and Bo Xie. On the complexity of learning neural

networks. arXiv preprint arXiv:1707.04615  2017.

[19] B. Szörényi. Characterizing statistical query learning:simpliﬁed notions and proofs. In ALT 

pages 186–200  2009.

[20] Matus Telgarsky. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485  2016.

9

,Le Song
Santosh Vempala
John Wilmes
Bo Xie
Pang Wei Koh
Kai-Siang Ang
Hubert Teo
Percy Liang