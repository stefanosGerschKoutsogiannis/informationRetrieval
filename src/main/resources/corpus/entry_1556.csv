2019,Disentangling Influence: Using disentangled representations to audit model predictions,Motivated by the need to audit complex and black box models  there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper  we develop disentangled influence audits  a procedure to audit the indirect influence of features. Specifically  we show that disentangled representations provide a mechanism to identify proxy features in the dataset  while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show  for each individual or in aggregate  which of these proxy features affects the classifier being audited the most. In this respect  our method is more powerful than existing methods for ascertaining feature influence.,Disentangling Inﬂuence: Using disentangled
representations to audit model predictions∗

Charles T. Marx
Haverford College

cmarx@haverford.edu

Richard Lanas Phillips

Cornell University

rlp246@cornell.edu

Sorelle A. Friedler
Haverford College

sorelle@cs.haverford.edu

Carlos Scheidegger
University of Arizona

cscheid@cs.arizona.edu

Suresh Venkatasubramanian

University of Utah

suresh@cs.utah.edu

Abstract

Motivated by the need to audit complex and black box models  there has been
extensive research on quantifying how data features inﬂuence model predictions.
Feature inﬂuence can be direct (a direct inﬂuence on model outcomes) and indirect
(model outcomes are inﬂuenced via proxy features). Feature inﬂuence can also
be expressed in aggregate over the training or test data or locally with respect to
a single point. Current research has typically focused on one of each of these
dimensions. In this paper  we develop disentangled inﬂuence audits  a procedure
to audit the indirect inﬂuence of features. Speciﬁcally  we show that disentangled
representations provide a mechanism to identify proxy features in the dataset  while
allowing an explicit computation of feature inﬂuence on either individual outcomes
or aggregate-level outcomes. We show through both theory and experiments that
disentangled inﬂuence audits can both detect proxy features and show  for each
individual or in aggregate  which of these proxy features affects the classiﬁer
being audited the most. In this respect  our method is more powerful than existing
methods for ascertaining feature inﬂuence.

1

Introduction

As machine learning models have become increasingly complex  there has been a growing subﬁeld of
work on interpreting and explaining the predictions of these models [24  11]. In order to assess the
importance of particular features to aggregated or individual model predictions  a variety of feature
inﬂuence techniques have been developed.
The goal of explaining model predictions is of particular importance in the context of fairness in
machine learning. In human-centered prediction tasks such as recidivism prediction and consumer
ﬁnance  understanding how protected attributes such as gender or race affect a prediction can improve
transparency and aligns with the principles of contestable design [14]. While direct inﬂuence methods
[7  12  20  25] focus on determining how a feature is used directly by the model to determine an
outcome  it is also possible for the model to access protected information through proxy variables
– variables which are closely related the protected attribute. Indirect feature inﬂuence techniques
[1  2  15] report that a feature is important if that feature or a proxy has an inﬂuence on the model
outcomes.

∗This research was funded in part by the NSF under grants DMR-1709351  IIS-1633387  IIS-1633724  and
IIS-1815238  by the DARPA SD2 program  and the Arnold and Mabel Beckman Foundation. The Titan Xp used
for this research was donated by the NVIDIA Corporation.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Feature inﬂuence methods can focus on the inﬂuence of a feature taken over all instances in the
training or test set [7  2]  or on the local feature inﬂuence on a single individual item of the training
or test set [25  20] (both of which are different than the inﬂuence of a speciﬁc training instance
on a model’s parameters [16]). Both the global perspective given by considering the impact of a
feature on all training and/or test instances as well as the local  individual perspective can be useful to
investigate the fairness of a machine learning model. Consider  for example  the question of fairness
in an automated hiring decision: determining the indirect inﬂuence of gender on all test outcomes
could help us understand whether the system had disparate impacts overall  while an individual-level
feature audit could help determine if a speciﬁc person’s predictions were due in part to their gender.2

Our Work.
In this paper we present a general technique to perform both global and individual-level
indirect inﬂuence audits. Our technique is modular – it solves the indirect inﬂuence problem by
reduction to a direct inﬂuence problem  allowing us to beneﬁt from existing techniques.
Our key insight is that disentangled representations can be used to do indirect inﬂuence computation.
The idea of a disentangled representation is to learn independent factors of variation that reﬂect the
natural symmetries of a data set. This approach has been very successful in generating representations
in deep learning that can be manipulated while creating realistic inputs [3  4  9  17  26]. Disentan-
glement has recently been shown to be a useful property for learning and evaluating fair machine
learning models [6  18]. Related methods use competitive learning to ensure a representation is free
of protected information while preserving other information [8  21].
In our context  the idea is to disentangle the inﬂuence of the feature whose (indirect) inﬂuence we
want to compute. By doing this  we obtain a representation in which we can manipulate the feature
directly to estimate its inﬂuence. Our approach has a number of advantages. We can connect indirect
inﬂuence in the native representation to direct inﬂuence in the disentangled representation. Our
method creates a disentangled model: a wrapper to the original model with the disentangled features
as inputs. This implies that it works for (almost) any model for which direct inﬂuence methods work 
and also allows us to use any future developed direct inﬂuence model.
Speciﬁcally  our disentangled inﬂuence audits approach provides the following contributions:

1. Theoretical and experimental justiﬁcation that the disentangled model and associated disen-
tangled inﬂuence audits we create provides an accurate indirect inﬂuence audit of complex 
and potentially black box  models.

2. Quality measures  based on the error of the disentanglement and the error of the reconstruc-

tion of the original input  that can be associated with the audit results.

3. An indirect inﬂuence method that can work in association with both global and individual-
level feature inﬂuence mechanisms. Our disentangled inﬂuence audits can additionally audit
continuous features and image data; types of audits that were not possible with previous
indirect audit methods (without additional preprocessing).

2 Our Methodology

2.1 Theoretical background
Let P and X denote sets of attributes with associated domains P and X . P represents features of
interest: these could be protected attributes of the data or any other features whose inﬂuence we wish
to determine. For convenience we will assume that P consists of the values taken by a single feature –
our exposition and techniques work more generally. X represents other attributes of the data that may
or may not be inﬂuenced by features in P . An instance is thus a point (p  x) ∈ P × X . Let Y denote
the space of labels for a learning task (Y = {+1 −1} for binary classiﬁcation or R for regression).

Disentangled representation. Our goal is to ﬁnd an alternate representation of an instance (p  x).
Speciﬁcally  we would like to construct x(cid:48) ∈ X (cid:48) that represents all factors of variation that are
independent of P   as well as an invertible mapping f such that f (p  x) = (p  x(cid:48)). We will refer to the
original domain as D = P × X and the associated new domain as D(cid:48) = P × X (cid:48).

2While unrelated to feature inﬂuence  the idea of recourse [28] also emphasizes the importance of individual-

level explanations of an outcome or how to change it.

2

n!

z⊆x

|z|!(n−|z|−1)!

φp(M  x) =(cid:80)

Direct and indirect inﬂuence. Given a model M : D → Y   a direct inﬂuence measure quan-
tiﬁes the degree to which any particular feature inﬂuences the outcome of M on a speciﬁc input.
In our experiments  we use the SHAP values proposed by [20] that are inspired by the Shap-
ley values in game theory  but our framework applies to an arbitrary direct inﬂuence function
φ. For a model M and input x  the inﬂuence of feature p is deﬁned by SHAP as [20  Eq. 8]
[Mx (z) − Mx (z \ p)] where (cid:107)z(cid:107) denotes the number of nonzero
entries in z  z ⊆ x is a vector whose nonzero entries are a subset of the nonzero entries in x  z \ p de-
notes z with the feature p set to zero  and n is the number of features. Finally  Mx(z) = E[M (z)|zS] 
the conditional expected value of the model subject to ﬁxing all the nonzero entries of z (S is the set
of nonzero entries in z).
Indirect inﬂuence attempts to capture how a feature might inﬂuence the outcome of a model even
if it is not explicitly represented in the data  i.e its inﬂuence is via proxy features. The above direct
inﬂuence measure cannot capture these effects because the information encoded in protected feature
p might be retained in other features even if p is removed. We propose the following deﬁnition of
indirect inﬂuence via a reduction to direct inﬂuence:
Deﬁnition 1 (Indirect Inﬂuence). The indirect inﬂuence of a feature p on the prediction of a model
M (p  x) is the direct inﬂuence φp of p on the prediction of the disentangled model (M ◦ f−1)(p  x(cid:48)):
(1)

II p(M  (p  x)) = φp(M ◦ f−1  (p  x(cid:48)))

Equation (1) states that the indirect inﬂuence of p is the direct inﬂuence of p when considering M as
acting on the disentangled representation instead of the original features. Whereas direct inﬂuence
measures the sensitivity of M to changes in p independent of all other features  indirect inﬂuence
also considers how p inﬂuences the prediction of M through proxy features for p. Hence  indirect
inﬂuence is inherently speciﬁc to a data distribution and should be interpreted with respect to the
joint distribution for (p  x) observed during training.3 Here  a proxy for p consists of a set of features
S and a function g that predicts p: i.e such that g(xS) (cid:39) p. Note that if there are no features that can
predict p  then the indirect and direct inﬂuence of p are the same (because the only proxy for p is
itself).

Dealing with errors.
In practice  it might not be possible to perfectly learn the invertible mapping
f from (p  x) to (p  x(cid:48)). In particular  assume that our decoder function is some g (cid:54)= f−1. While
we do not provide an explicit formula for the dependence of the inﬂuence function parameters  we
note that it is a linear function of the predictions  and so we can begin to understand the errors in the
inﬂuence estimates by looking at the behavior of the predictor with respect to p.
Model output can be written as ˆy = (M ◦ g)(p  x(cid:48)). Recalling that g(p  x(cid:48)) = (p  ˆx)  the partial
derivative of ˆy with respect to p can be written as ∂(cid:98)y
∂p .
∂p + ∂M
Consider the term ∂x(cid:48)
∂p . If the disentangled representation is perfect  then this term is zero (because x(cid:48)
is unaffected by p)  and therefore we get ∂(cid:98)y
∂p which is as we would expect. If the reconstruction
is perfect (but not necessarily the disentangling)  then the term ∂ ˆx
∂x(cid:48) is 1. What remains is the partial
derivative of M with respect to the latent encoding (x(cid:48)  p).

∂p = ∂(M◦g)

∂p = ∂M

∂p = ∂M

∂p = ∂M

∂p + ∂M

∂ ˆx

∂x(cid:48) ∂x(cid:48)

∂ ˆx

∂ ˆx

∂ ˆx

2.2

Implementation

Our overall process requires two separate pieces: 1) a method to create disentangled representations 
and 2) a method to audit direct features. In most experiments in this paper  we use methods related to
adversarial autoencoders [22] to generate disentangled representations  and Shapley values from the
shap technique for auditing direct features [20] (as described above in Section 2.1).
We train a disentangled representation to estimate (p  x(cid:48)) for each feature of interest p. This allows
us to compute representations with only two factors in a supervised manner  avoiding many of the
practical issues affecting methods for learning disentangled representations [19]. A key limitation
of this approach is that while easier to train  it potentially requires one to train many disentangled
representations. This means the technique may be most useful in domains such as fairness where we

3See the related perspective of disentangled representations as recovering symmetries in underlying world

states  which directly inspired our approach [13].

3

care speciﬁcally about the impact of one or a small collection of distinguished features that may or
may not be directly used as inputs to the model.

Disentangled representations via adversarial autoencoders We create disentangled repre-
sentations by training three separate neural networks  which we denote f  g  and h (see
Figure 1). Networks f and g are autoencoders:
the image of f has lower dimension-
ality than the domain of f  and the training process seeks for g ◦ f to be an approx-
through gradient descent on the reconstruction error ||(g ◦ f )(x) − x||.
imate identity 
Unlike regular autoencoders  g is also given di-
rect access to the protected attribute. Adversar-
ial autoencoders [22]  in addition  use an ancil-
lary network h that attempts to recover the pro-
tected attribute from the image of f  without ac-
cess to p itself. (Note the slight abuse of notation
here: h is assumed not to have access to p  while
g does have access to it.) During the training of
f and g  we seek to reduce ||(g ◦ f )(x) − x|| 
but also to increase the error of the discrimina-
tor h ◦ f. The optimization process of h tries
to recover the protected attribute from the code
generated by f. (h and f are the adversaries.)
When the process converges to an equilibrium 
the code generated by f will contain no infor-
mation about p that is useful to h  but g ◦ f
still reconstructs the original data correctly: f
disentangles p from the other features.
The loss functions used to codify this process are
LEnc = MSE(x  ˆx) − β MSE(p  ˆp)  LDec =
MSE(x  ˆx)  and LDisc = MSE(p  ˆp)  where
MSE is the mean squared error and β is a hyper-
parameter determining the importance of disen-
tanglement relative to reconstruction. When p is
a binary feature  LEnc and LDisc are adjusted to
use binary cross entropy loss between p and ˆp.

Figure 1: System diagram when auditing the in-
direct inﬂuence of feature p on the outcomes of
model M for instance (p  x). The autoencoder
g ◦ f learns the instance (p  x) as a function of
independent factors (p  x(cid:48)). The independence of
p and x(cid:48) is enforced by the adversary h. The dis-
entangled representation (p  x(cid:48)) is the input for the
disentangled model M(cid:48) = M ◦ g which is audited
using the direct inﬂuence algorithm I.

Disentangled feature audits Concretely  our method works as follows  where the variable names
match the diagram in Figure 1:

DISENTANGLED-INFLUENCE-AUDIT(X  M )
1
2
3
4
5
6

return {SHAPp for p in FEATURES(X)}

for p in FEATURES(X)

(f   g  h) = DISENTANGLED-REPRESENTATION(X  p) // (h is not used)
M(cid:48) = g ◦ M
X(cid:48) = {f (x) for x in X}
SHAPp = DIRECT-INFLUENCE(X(cid:48)  p  M(cid:48))

While we use shap in our implementations  our framework applies to other direct inﬂuence functions
as well. We note here one important difference in the interpretation of disentangled inﬂuence values
when contrasted with regular Shapley values. Because the inﬂuence of each feature is determined on
a different disentangled model  the scores we get are not directly interpretable as a partition of the
model’s prediction. For example  consider a dataset in which feature p1 is responsible for 50% of the
direct inﬂuence  while feature p2 is a perfect proxy for p1  but shows 0% inﬂuence under a direct
audit. Relative judgments of feature importance remain sensible.

4

pxfencoderx(cid:48)pgdecoderpˆxMmodeltobeauditeddisentangledmodel:M(cid:48)ˆyIfeatureinﬂuencealgorithmx(cid:48)ˆpdiscriminatorhFigure 2: Synthetic x + y data direct shap (left) and indirect (right) feature inﬂuences using a
handcrafted (top row) or learned disentangled representation (bottom row). In each plot  a point
represents the inﬂuence of a feature on a single prediction. Points far from the center indicate high
importance  so a row with many points far from the center indicates a feature which is deemed
important. As expected  the direct inﬂuence method shap reports that the only important features are
x and y  but our methods capture that x and y are perfect proxies for 2x  2y  x2  y2 so these features
have equal indirect inﬂuence.

3 Experiments

In this section  we will assess the extent to which disentangled inﬂuence audits are able to identify
sources of indirect inﬂuence to a model and quantify its error. All data and code4 for the described
method and below experiments is available in the Supplementary Materials.

3.1 Synthetic x + y Regression Data

In order to evaluate whether the indirect inﬂuence calculated by the disentangled inﬂuence audits
correctly captures all inﬂuence of individual-level features on an outcome  we will consider inﬂuence
on a simple synthetic x + y dataset. It includes 5 000 instances of two variables x and y drawn
independently from a uniform distribution over [0  1] that are added to determine the label x + y. It
also includes proxy variables 2x  x2  2y  and y2. A random noise variable c is also included that is
drawn independently of x and y uniformly from [0  1]. The model we are auditing is a handcrafted
model that contains no hidden layers and has ﬁxed weights of 1 corresponding to x and y and weights
of 0 for all other features (i.e.  it directly computes x + y). We use shap as the direct inﬂuence
delegate method [20].5
In order to examine the impact of the quality of the disentangled representation on the results  we
considered both a handcrafted disentangled representation and a learned one. For the former  nine
unique models were handcrafted to disentangle each of the nine features perfectly (see Supplementary
Materials for details). The learned disentangled representation is created according to the adversarial
autoencoder methodology described in more detail in the previous section.
The results for the handcrafted disentangled representation (top of Figure 2) are as expected: features
x and y are the only ones with direct inﬂuence  all x or y based features have the same amount of
indirect inﬂuence  while all features including c have zero inﬂuence. Using the learned disentangled
representation introduces the potential for error: the resulting inﬂuences (bottom of Figure 2) show
more variation between features  but the same general trends as in the handcrafted test case.

4Code is available at: https://github.com/charliemarx/disentangling-influence
5This method is available via pip install shap. See also: https://github.com/slundberg/shap

5

SHAP value (impact on model output)0.00.5-0.50.00.5-0.50.00.5-0.50.00.5-0.5HighLowFeature ValueDirect InfluenceHandcrafted DRLearned DRIndirect InfluenceAdditionally  note that since shap gives inﬂuence results per individual instance  we can also see
that (for both models) instances with larger (or  respectively  smaller) 2x or 2y values give larger
(respectively  smaller) results for the label x + y  i.e.  have larger absolute inﬂuences on the outcomes.

3.1.1 Error Analyses

There are two main sources of error for disentangled inﬂuence audits: error in the reconstruction
of the original input x and error in the disentanglement of p from x(cid:48) such that the discriminator is
able to accurately predict some ˆp close to p. We will measure the former error in two ways. First 
we will consider the reconstruction error  which we deﬁne as x − ˆx. Second  we consider the
prediction error  which is M (x) − M (ˆx) - a measure of the impact of the reconstruction error on the
model to be audited. Reconstruction and prediction errors close to 0 indicate that the disentangled
model M(cid:48) is similar to the model M being audited. Common measures for disentanglement such
(cid:80)n
as the mutual information gap (MIG) do not apply well to our method since we disentangle the
features one at a time  as opposed to simultaneously [5]. We measure the disentanglement error 
i=1(p − ˆp)2/var(p) where var(p) is the variance of p. A disentanglement error of below 1
as 1
n
indicates that information about that feature may have been revealed  i.e.  that there may be indirect
inﬂuence that is not accounted for in the resulting inﬂuence score. In addition to the usefulness of
these error measures during training time  they also provide information that helps us to assess the
quality of the indirect inﬂuence audit  including at the level of the error for an individual instance.

Figure 3: Errors on the synthetic x + y data for the reconstruction error (left) when taken across
inﬂuence audits for each feature  prediction error (middle)  and disentanglement error (right). Optimal
is reconstruction error and prediction error of 0 for all features (indicating no errors in autoencoding) 
and disentanglement error of 1 for all features (indicating p and x(cid:48) are independent).

These inﬂuence experiments on the x + y dataset demonstrate the importance of a good disentangled
representation to the quality of the resulting indirect inﬂuence measures  since the handcrafted zero-
error disentangled representation clearly results in more accurate inﬂuence results. Each of the error
types described above are given for the learned disentangled representation in Figure 3. While most
features have reconstruction and prediction errors close to 0 and disentanglement errors close to 1  a
few features also have some far outlying instances. For example  we can see that the c  2c  and c2
variables have high prediction error on some instances  and this is reﬂected in the incorrect indirect
inﬂuence that they’re found to have on the learned representation for some instances.

3.2 dSprites Image Classiﬁcation

The second synthetic dataset is the dSprites
dataset commonly used in the disentangled rep-
resentations literature to disentangle indepen-
dent factors that are sources of variation [23].
The dataset consists of 737  280 images (64×64
pixels) of a white shape (a square  ellipse  or
heart) on a black background. The independent
latent factors are x position  y position  orienta-
tion  scale  and shape. The images were down-
sampled to 16 × 16 resolution and the half of
the data in which the shapes are largest were

Figure 4: dSprites data indirect latent factor inﬂu-
ences on a model predicting shape.

6

HighLowFeature Valueshapescaleorient.x pos.y pos.SHAP value (impact on model output)0-0.0150.015used. The binary classiﬁcation task is to predict whether the shape is a heart. A good disentangled
representation should be able to separate the shape from the other latent factors.

Figure 5: The mean squared reconstruction error (left)  absolute prediction error (middle)  and
absolute disentanglement error (right) of the latent factors in the dSprites data under an indirect
inﬂuence audit. Optimal is reconstruction error and prediction error of 0  and disentanglement error
of 1. We see that the quality of the disentangled representation varies for the dSprites data.

In this experiment we seek to quantify the indirect inﬂuence of each latent factor on a model trained
to predict the shape from an image. Since shape is the label and the latent factors are independent 
we expect the feature shape to have more indirect inﬂuence on the model than any other latent
factor. Note that a direct inﬂuence audit is impossible since the latent factors are not themselves
features of the data. Model and disentangled representation training information can be found in the
Supplementary Material.
The indirect inﬂuence audit  shown in Figure 4  correctly identiﬁes shape as the most important latent
factor  and also correctly shows the other four factors as having essentially zero indirect inﬂuence.
However  the audit struggles to capture the extent of the indirect inﬂuence of shape since the resulting
shap values are small.
The associated error measures for the dSprites inﬂuence audit are shown in Figure 5. We report the
reconstruction error as the mean squared error between x and ˆx for each latent factor. The prediction
error is the difference between M (x) and M (ˆx) of the model’s estimate of the probability the shape
is a heart. While the reconstruction errors are relatively low (less than 0.05 for all but y position) the
prediction error and disentanglement errors are high. A high prediction error indicates that the model
is sensitive to the errors in reconstruction and the indirect inﬂuence results may be unstable  which
may explain the low shap values for shape in the indirect inﬂuence audit.

3.3 Adult Income Data

Figure 6: Ten selected features for Adult dataset. Direct (left) and indirect (right) inﬂuence are shown.
For all features  see Supplemental Material. Low values indicate a one-hot encoded feature is false.
Features with many points far from the center (shown here using width of a cluster) are identiﬁed as
being of high importance. These results indicate that features sex=Male  relationship=Husband
and workclass=Private may be used by the model via proxy variables since they have higher
indirect inﬂuence than direct inﬂuence.

7

Finally  we will consider a real-world dataset containing Adult Income data that is commonly used as
a test case in the fairness-aware machine learning community. The Adult dataset includes 14 features
describing type of work  demographic information  and capital gains information for individuals
from the 1994 U.S. census [27]. The classiﬁcation task is predicting whether an individual makes
more or less than $50 000 per year. Preprocessing  model  and disentangled representation training
information are included in the Supplementary Material.
Direct and indirect inﬂuence audits on the Adult dataset are given in Figure 6 and in the Supplementary
Material. While many of the resulting inﬂuence scores are the same in both the direct and indirect
cases  the disentangled inﬂuence audits ﬁnds substantially more inﬂuence based on sex than the
direct inﬂuence audit - this is not surprising given the large inﬂuence that sex is known to have on
U.S. income. Other important features in a fairness context  such as nationality  are also shown to
have indirect inﬂuences that are not apparent on a direct inﬂuence audit. The error results (Figure 7
and the Supplementary Material) indicate that while the error is low across all three types of errors for
many features  the disentanglement errors are higher (further from 1) for some rare-valued features.
This means that despite the indirect inﬂuence that the audit did ﬁnd  there may be additional indirect
inﬂuence it did not pick up for those features.

Figure 7: The reconstruction error (left)  prediction error (middle)  and disentanglement error (right)
of selected Adult Income features under an indirect inﬂuence audit. Optimal is a reconstruction error
and prediction error of 0  and a disentanglement error of 1 for all features. See the Supplementary
Material for the complete ﬁgure.

3.4 Comparison to Other Methods

Figure 8: Comparison on the synthetic x + y data of the disentangled inﬂuence audits using the
handcrafted (left) or learned (middle) disentangled representation with the BBA approach of [2] (right).
According to our deﬁnition of indirect inﬂuence and using shap the features x  y  2x  2y  x2  y2
should have the same inﬂuence and c  2c  c2 should have no inﬂuence.

Here  we compare the disentangled inﬂuence audits results to results on the same datasets and models
by the indirect inﬂuence technique introduced in [2]  which we will refer to as BBA (black-box
auditing).6 However  this is not a direct comparison  since BBA is not able to determine feature
inﬂuence for individual instances  only inﬂuence for a feature taken over all instances. In order to
compare to our results  we will thus take the mean over all instances of the absolute value of the per
feature disentangled inﬂuence. BBA was designed to audit classiﬁers  so in order to compare to the
results of disentangled inﬂuence audits we will consider the obscured data they generate as input into
our regression models and then report the average change in mean squared error for the case of the
synthetic x + y data. (BBA cannot handle dSprites image data as input.)

6This method is available via pip install BlackBoxAuditing. See also: https://github.com/

algofairness/BlackBoxAuditing

8

A comparison of the disentangled inﬂuence and
BBA results on the synthetic x + y data shown in
ﬁgure 8 shows that all three variants of indirect
inﬂuence are able to determine that the c  2c  c2
variables have comparatively low inﬂuence on
the model. The disentangled inﬂuence with a
handcrafted disentangled representation shows
the correct indirect inﬂuence of each feature 
while the learned disentangled representation in-
ﬂuence is somewhat more noisy  and the BBA re-
sults suffer from relying on the mean squared er-
ror (i.e.  the amount of inﬂuence changes based
on the feature’s value).
Figure 9 shows the mean absolute disentangled
inﬂuence per feature on the x-axis and the BBA
inﬂuence results on the y-axis. The features with
large disentangled inﬂuence and low BBA score
are marital.status=Married-civ-spouse and relationship=Husband. BBA can only detect
inﬂuence present in pairwise dimensions  and not more complex high-dimensional correlations;
perhaps this is why marital status is found to have such a large inﬂuence by the disentangled inﬂuence
audit and not by BBA. The feature with large BBA score is age  and the reconstruction error on the
disentangled inﬂuence audit for that feature indicates that the audit may not have picked up the full
inﬂuence of that feature. It’s clear that overall the disentangled inﬂuence audits technique is much
better able to ﬁnd features with possible indirect inﬂuence on this dataset and model: most of the BBA
inﬂuences are clustered near zero  while the disentangled inﬂuence values provide more variation and
potential for insight.

Figure 9: Comparison on the Adult data of the
disentangled inﬂuence audits versus the BBA indi-
rect inﬂuence approach of [2]. Our disentangled
feature audits identiﬁes more plausible  potentially-
inﬂuential features than BBA. See text for details.

SHAP vs. LIME In Section 3.3  we use SHAP audits as our direct and indirect audit sources. As
Lee and Lundberg argue  the SHAP values are  in essence  a variation of the LIME method [20]  one
that provides weights for samples and features that are consistent with the Shapley value formulation
from game theory. As a result  the audits for LIME are not fundamentally different than those for
SHAP; we provide them in the Supplementary Material.

4 Discussion and Conclusion

In this paper  we introduce the idea of disentangling inﬂuence: using the ideas from disentangled
representations to allow for indirect inﬂuence audits. We show via theory and experiments that this
method works across a variety of problems and data types including classiﬁcation and regression
as well as numerical  categorical  and image data. The methodology allows us to turn any future
developed direct inﬂuence measures into indirect inﬂuence measures. In addition to the strengths of
the technique demonstrated here  disentangled inﬂuence audits have the added potential to allow for
multidimensional indirect inﬂuence audits that would  e.g.  allow a fairness audit on both race and
gender to be performed (without using a single combined race and gender feature [10]). We hope this
opens the door for more nuanced fairness audits.

9

References
[1] J. Adebayo and L. Kagal. Iterative orthogonal feature projection for diagnosing bias in black-box models.

arXiv preprint arXiv:1611.04967  2016.

[2] P. Adler  C. Falk  S. A. Friedler  T. Nix  G. Rybeck  C. Scheidegger  B. Smith  and S. Venkatasubramanian.
Auditing black-box models for indirect inﬂuence. Knowledge and Information Systems  54(1):95–122 
2018.

[3] A. A. Alemi  I. Fischer  J. V. Dillon  and K. Murphy. Deep variational information bottleneck. International

Conference on Learning Representations  2016.

[4] Y. Bengio  A. Courville  and P. Vincent. Representation learning: A review and new perspectives. IEEE

transactions on pattern analysis and machine intelligence  35(8):1798–1828  2013.

[5] T. Q. Chen  X. Li  R. B. Grosse  and D. K. Duvenaud. Isolating sources of disentanglement in variational

autoencoders. In Advances in Neural Information Processing Systems  pages 2610–2620  2018.

[6] E. Creager  D. Madras  J.-H. Jacobsen  M. A. Weis  K. Swersky  T. Pitassi  and R. Zemel. Flexibly fair

representation learning by disentanglement. arXiv preprint arXiv:1906.02589  2019.

[7] A. Datta  S. Sen  and Y. Zick. Algorithmic transparency via quantitative input inﬂuence: Theory and
experiments with learning systems. In Proceedings of 37th IEEE Symposium on Security and Privacy 
2016.

[8] H. Edwards and A. Storkey. Censoring representations with an adversary. In Proceedings of the 33th

International Conference on Machine Learning  2016.

[9] B. Esmaeili  H. Wu  S. Jain  A. Bozkurt  N. Siddharth  B. Paige  D. H. Brooks  J. Dy  and J.-W. van de
Meent. Structured disentangled representations. In K. Chaudhuri and M. Sugiyama  editors  Proceedings
of Machine Learning Research  volume 89  pages 2525–2534. PMLR  16–18 Apr 2019.

[10] S. A. Friedler  C. Scheidegger  S. Venkatasubramanian  S. Choudhary  E. P. Hamilton  and D. Roth.
A comparative study of fairness-enhancing interventions in machine learning. In Proceedings of the
Conference on Fairness  Accountability  and Transparency  pages 329–338. ACM  2019.

[11] R. Guidotti  A. Monreale  S. Ruggieri  F. Turini  F. Giannotti  and D. Pedreschi. A survey of methods for

explaining black box models. ACM computing surveys (CSUR)  51(5):93  2018.

[12] A. Henelius  K. Puolamäki  H. Boström  L. Asker  and P. Papapetrou. A peek into the black box: exploring

classiﬁers by randomization. Data Min Knowl Disc  28:1503–1529  2014.

[13] I. Higgins  D. Amos  D. Pfau  S. Racaniere  L. Matthey  D. Rezende  and A. Lerchner. Towards a deﬁnition

of disentangled representations. arXiv preprint arXiv:1812.02230  2018.

[14] T. Hirsch  K. Merced  S. Narayanan  Z. E. Imel  and D. C. Atkins. Designing contestability: Interaction
In Proceedings of the 2017 Conference on Designing

design  machine learning  and mental health.
Interactive Systems  pages 95–99. ACM  2017.

[15] B. Kim  M. Wattenberg  J. Gilmer  C. Cai  J. Wexler  F. Viegas  and R. Sayres.

Interpretability be-
yond feature attribution: Quantitative testing with concept activation vectors (tcav). arXiv preprint
arXiv:1711.11279  2017.

[16] P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence functions. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70  pages 1885–1894. JMLR. org  2017.

[17] A. Kumar  P. Sattigeri  and A. Balakrishnan. Variational inference of disentangled latent concepts from

unlabeled observations. International Conference on Learning Representations  2017.

[18] F. Locatello  G. Abbati  T. Rainforth  S. Bauer  B. Schölkopf  and O. Bachem. On the fairness of

disentangled representations. arXiv preprint arXiv:1905.13662  2019.

[19] F. Locatello  S. Bauer  M. Lucic  S. Gelly  B. Schölkopf  and O. Bachem. Challenging common assumptions

in the unsupervised learning of disentangled representations. arXiv preprint arXiv:1811.12359  2018.

[20] S. M. Lundberg and S.-I. Lee. A uniﬁed approach to interpreting model predictions. In Advances in Neural

Information Processing Systems  pages 4765–4774  2017.

[21] D. Madras  E. Creager  T. Pitassi  and R. Zemel. Learning adversarially fair and transferable representations.

In Proceedings of the 35th International Conference on Machine Learning  2018.

10

[22] A. Makhzani  J. Shlens  N. Jaitly  I. Goodfellow  and B. Frey. Adversarial autoencoders. arXiv preprint

arXiv:1511.05644  2015.

[23] L. Matthey  I. Higgins  D. Hassabis  and A. Lerchner. dsprites: Disentanglement testing sprites dataset.

https://github.com/deepmind/dsprites-dataset/  2017.

[24] C. Molnar. Interpretable machine learning: A guide for making black box models explainable. Christoph

Molnar  Leanpub  2018.

[25] M. T. Ribeiro  S. Singh  and C. Guestrin. "Why Should I Trust You?": Explaining the Predictions of Any

Classiﬁer. In Proc. ACM KDD  2016.

[26] M. Tschannen  O. Bachem  and M. Lucic. Recent advances in autoencoder-based representation learning.

arXiv preprint arXiv:1812.05069  2018.

[27] I. M. L. R. University of California. Adult income dataset. https://archive.ics.uci.edu/ml/

datasets/adult.

[28] B. Ustun  A. Spangher  and Y. Liu. Actionable recourse in linear classiﬁcation. In Proceedings of the

Conference on Fairness  Accountability  and Transparency  pages 10–19. ACM  2019.

11

,Ruitong Huang
Tor Lattimore
András György
Csaba Szepesvari
Charles Marx
Richard Phillips
Sorelle Friedler
Carlos Scheidegger
Suresh Venkatasubramanian