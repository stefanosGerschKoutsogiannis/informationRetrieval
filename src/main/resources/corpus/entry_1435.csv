2012,Density-Difference Estimation,We address the problem of estimating the difference between two probability densities. A naive approach  is a two-step procedure of first estimating two densities separately and then computing their difference. However  such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage. In this paper  we propose a single-shot procedure  for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be utilized in L2-distance approximation. Finally  we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.,Density-Difference Estimation

Masashi Sugiyama1 Takafumi Kanamori2 Taiji Suzuki3

Song Liu1

Marthinus Christoffel du Plessis1
1Tokyo Institute of Technology  Japan
3University of Tokyo  Japan

Ichiro Takeuchi4
2Nagoya University  Japan
4Nagoya Institute of Technology  Japan

Abstract

We address the problem of estimating the difference between two probability den-
sities. A naive approach is a two-step procedure of ﬁrst estimating two densities
separately and then computing their difference. However  such a two-step proce-
dure does not necessarily work well because the ﬁrst step is performed without re-
gard to the second step and thus a small estimation error incurred in the ﬁrst stage
can cause a big error in the second stage. In this paper  we propose a single-shot
procedure for directly estimating the density difference without separately esti-
mating two densities. We derive a non-parametric ﬁnite-sample error bound for
the proposed single-shot density-difference estimator and show that it achieves the
optimal convergence rate. We then show how the proposed density-difference es-
timator can be utilized in L2-distance approximation. Finally  we experimentally
demonstrate the usefulness of the proposed method in robust distribution compar-
ison such as class-prior estimation and change-point detection.

1

Introduction

When estimating a quantity consisting of two elements  a two-stage approach of ﬁrst estimating
the two elements separately and then approximating the target quantity based on the estimates of
the two elements often performs poorly  because the ﬁrst stage is carried out without regard to the
second stage and thus a small estimation error incurred in the ﬁrst stage can cause a big error in the
second stage. To cope with this problem  it would be more appropriate to directly estimate the target
quantity in a single-shot process without separately estimating the two elements.
A seminal example that follows this general idea is pattern recognition by the support vector ma-
chine [1]: Instead of separately estimating two probability distributions of patterns for positive and
negative classes  the support vector machine directly learns the boundary between the two classes
that is sufﬁcient for pattern recognition. More recently  a problem of estimating the ratio of two
probability densities was tackled in a similar fashion [2  3]: The ratio of two probability densities is
directly estimated without going through separate estimation of the two probability densities.
In this paper  we further explore this line of research  and propose a method for directly estimating
the difference between two probability densities in a single-shot process. Density differences would
be more desirable than density ratios because density ratios can diverge to inﬁnity even under a
mild condition (e.g.  two Gaussians [4])  whereas density differences are always ﬁnite as long as
each density is bounded. Density differences can be used for solving various machine learning tasks
such as class-balance estimation under class-prior change [5] and change-point detection in time
series [6].
For this density-difference estimation problem  we propose a single-shot method  called the least-
squares density-difference (LSDD) estimator  that directly estimates the density difference without
separately estimating two densities. LSDD is derived with in the framework of kernel regularized
least-squares estimation  and thus it inherits various useful properties: For example  the LSDD

1

solution can be computed analytically in a computationally efﬁcient and stable manner  and all
tuning parameters such as the kernel width and the regularization parameter can be systematically
and objectively optimized via cross-validation. We derive a ﬁnite-sample error bound for the LSDD
estimator and show that it achieves the optimal convergence rate in a non-parametric setup.
We then apply LSDD to L2-distance estimation and show that it is more accurate than the differ-
ence of KDEs  which tends to severely under-estimate the L2-distance [7]. Because the L2-distance
is more robust against outliers than the Kullback-Leibler divergence [8]  the proposed L2-distance
estimator can lead to the paradigm of robust distribution comparison. We experimentally demon-
strate the usefulness of LSDD in semi-supervised class-prior estimation and unsupervised change
detection.

2 Density-Difference Estimation

In this section  we propose a single-shot method for estimating the difference between two proba-
bility densities from samples  and analyze its theoretical properties.

Problem Formulation and Naive Approach: First  we formulate the problem of density-
difference estimation. Suppose that we are given two sets of independent and identically distributed
samples X := {xi}n
(cid:2)
i(cid:2)=1 from probability distributions on Rd with densities
p(x) and p

(x)  respectively. Our goal is to estimate the density difference 

i=1 and X (cid:2)

:= {x(cid:2)

(cid:2)

i(cid:2)}n
f (x) := p(x) − p

(cid:2)

(x) 

from the samples X and X (cid:2).
A naive approach to density-difference estimation is to use kernel density estimators (KDEs). How-
ever  we argue that the KDE-based density-difference estimator is not the best approach because
of its two-step nature. Intuitively  good density estimators tend to be smooth and thus the differ-
ence between such smooth density estimators tends to be over-smoothed as a density-difference
estimator [9]. To overcome this weakness  we give a single-shot procedure of directly estimating the
density difference f (x) without separately estimating the densities p(x) and p

(x).

(cid:2)

Least-Squares Density-Difference Estimation:
difference model g(x) to the true density-difference function f (x) under the squared loss:

In our proposed approach  we ﬁt a density-

argmin

g

(cid:2) (cid:3)
(cid:2)(cid:5)

n+n

(cid:2)=1

(cid:4)2

g(x) − f (x)
(cid:6)

dx.

(cid:7)

−(cid:2)x − c(cid:2)(cid:2)2
2σ2
1  . . .   x(cid:2)

We use the following Gaussian kernel model as g(x):

where (c1  . . .   cn  cn+1  . . .   cn+n(cid:2) ) := (x1  . . .   xn  x(cid:2)
n + n
For the model (1)  the optimal parameter θ∗ is given by

(cid:2) is large  we may use only a subset of {x1  . . .   xn  x(cid:2)
(cid:8)

(cid:2) (cid:3)

(cid:4)2

g(x) =

θ(cid:2) exp

 

(1)

1  . . .   x(cid:2)

θ(cid:4)Hθ − 2h(cid:4)θ

n(cid:2) ) are Gaussian kernel centers. If
n(cid:2)} as Gaussian kernel centers.

(cid:9)
(cid:6)
)-dimensional vector deﬁned as
−(cid:2)c(cid:2) − c(cid:2)(cid:2)(cid:2)2

= H−1h 

(cid:7)

 

4σ2

where H is the (n + n

θ∗

(cid:2)
(cid:2)

(cid:2)

θ

g(x) − f (x)
:= argmin
(cid:7)
(cid:6)
) × (n + n
(cid:2)
−(cid:2)x − c(cid:2)(cid:2)2
(cid:7)
−(cid:2)x − c(cid:2)(cid:2)2

(cid:6)
(cid:6)

2σ2

exp

exp

2σ2

H(cid:2) (cid:2)(cid:2) :=

h(cid:2) :=

(cid:2)

θ

dx = argmin

2σ2

(cid:2)

) matrix and h is the (n + n

(cid:7)
−(cid:2)x − c(cid:2)(cid:2)(cid:2)2
(cid:6)
dx = (πσ2)d/2 exp
−(cid:2)x(cid:2) − c(cid:2)(cid:2)2
(cid:9)

exp
p(x)dx −
(cid:8)

(cid:7)

(x(cid:2)

2σ2

exp

(cid:2)
p

θ(cid:4)Hθ − 2(cid:10)h

θ + λθ(cid:4)θ

 

)dx(cid:2)

.

(cid:10)θ := argmin

θ

Replacing the expectations in h by empirical estimators and adding an (cid:5)2-regularizer to the objective
function  we arrive at the following optimization problem:
(cid:4)

(2)

2

where λ (≥ 0) is the regularization parameter and(cid:10)h is the (n + n
(cid:6)

(cid:6)

(cid:7)

(cid:2)

n(cid:5)

(cid:2)(cid:5)

n

(cid:10)h(cid:2) :=

1
n

exp

i=1

exp

i(cid:2)=1

2σ2

− 1
n(cid:2)

−(cid:2)xi − c(cid:2)(cid:2)2
(cid:10)θ = (H + λI)

−1(cid:10)h 

Taking the derivative of the objective function in Eq.(2) and equating it to zero  we can obtain the
solution analytically as

where I denotes the identity matrix.

Finally  a density-difference estimator (cid:10)f (x)  which we call the least-squares density-difference
(LSDD) estimator  is given as (cid:10)f (x) =

(cid:10)θ(cid:2) exp

(cid:2)(cid:5)

(cid:6)

(cid:7)

n+n

(cid:7)

)-dimensional vector deﬁned as
−(cid:2)x(cid:2)

i(cid:2) − c(cid:2)(cid:2)2
2σ2

.

(cid:2)

Non-Parametric Error Bound: Here  we theoretically analyze an estimation error of LSDD.
We assume n
the Gaussian kernel with width γ: kγ(x  x(cid:2)
(cid:14)
modiﬁed LSDD estimator that is more suitable for non-parametric error analysis1:

= n  and let Hγ be the reproducing kernel Hilbert space (RKHS) corresponding to
. Let us consider a slightly

) = exp

(cid:15)

(cid:13)

(cid:16)

(cid:2)g(cid:2)2

L2(Rd) − 2

g(xi) − 1

g(x(cid:2)
i(cid:2) )

+ λ(cid:2)g(cid:2)2Hγ

.

(cid:10)f := argmin

g∈Hγ

.

2σ2

−(cid:2)x − c(cid:2)(cid:2)2
(cid:11)−(cid:2)x − x(cid:2)(cid:2)2/γ2
n(cid:5)

(cid:12)

n

i(cid:2)=1

(cid:2)=1

n(cid:5)

i=1

1
n

Then we have the following theorem:
Theorem 1. Suppose that there exists a constant M such that (cid:2)p(cid:2)∞ ≤ M and (cid:2)p
Suppose also that the density difference f = p − p
That is  f ∈ Bα

(cid:2)(cid:2)∞ ≤ M.
(cid:2) is a member of Besov space with regularity α.

2 ∞ is the Besov space with regularity α  and

2 ∞ where Bα
(cid:2)f(cid:2)Bα

2 ∞ := (cid:2)f(cid:2)L2(Rd) + sup

t>0

−αωr L2(Rd)(f  t)) < c for r = (cid:6)α(cid:7) + 1 

(t

where (cid:6)α(cid:7) denotes the largest integer less than or equal to α and ωr L2(Rd) is the r-th modulus of
smoothness (see [10] for the deﬁnitions). Then  for all  > 0 and p ∈ (0  1)  there exists a constant
K > 0 depending on M  c    and p such that for all n ≥ 1  τ ≥ 1  and λ > 0  the LSDD estimator
(cid:15)

(cid:14)

(cid:10)f in Hγ satisﬁes
(cid:2)(cid:10)f − f(cid:2)2

L2(Rd) +λ(cid:2)(cid:10)f(cid:2)2Hγ

−(1−p)(1+)d

− 2(1−p)d

(1++ 1−p
4 )

≤ K

−d +γ2α +

γ

λpn

γ

+

1+p
3p−p2
1+p n

λ

2

1+p

+

τ
n2λ

+

τ
n

λγ
with probability not less than 1 − 4e

−τ.

2α+d

(2α+d)(1+p)+(−p+p) and γ = n

−
If we set λ = n
small  then we immediately have the following corollary.
Corollary 1. Suppose that the same assumptions as Theorem 1 hold. Then  for all ρ  ρ
exists a constant K > 0 depending on M  c  ρ  and ρ

density-difference estimator (cid:10)f with appropriate choice of γ and λ satisﬁes

> 0  there
(cid:2) such that  for all n ≥ 1 and τ ≥ 1  the

(2α+d)(1+p)+(−p+p)   and take  and p sufﬁciently

−

(cid:2)

1

(cid:2)(cid:10)f − f(cid:2)2

L2(Rd) + λ(cid:2)(cid:10)f(cid:2)2Hγ

(cid:3)

≤ K

− 2α

2α+d +ρ + τ n

n

−1+ρ

(cid:2)(cid:4)

with probability not less than 1 − 4e

−τ.

1More speciﬁcally  the regularizer is replaced from the squared (cid:2)2-norm of parameters to the squared RKHS-
norm of a learned function  which is necessary to establish consistency. Nevertheless  we use the squared
(cid:2)2-norm of parameters in experiments because it is simpler and seems to perform well in practice.

3

− 2α

2α+d is the optimal learning rate to estimate a function in Bα

Note that n
2 ∞. Therefore  the density-
difference estimator with a Gaussian kernel achieves the optimal learning rate by appropriately
choosing the regularization parameter and the Gaussian width. Because the learning rate depends
on α  the LSDD estimator has adaptivity to the smoothness of the true function.
It is known that  if the naive KDE with a Gaussian kernel is used for estimating a probability density
with regularity α > 2  the optimal learning rate cannot be achieved [11  12]. To achieve the optimal
rate by KDE  we should choose a kernel function speciﬁcally tailored to each regularity α [13].
However  such a kernel function is not non-negative and it is difﬁcult to implement it in practice.
On the other hand  our LSDD estimator can always achieve the optimal learning rate for a Gaussian
kernel without regard to regularity α.

(cid:2)

= {x(cid:2)

Model Selection by Cross-Validation: The above theoretical analysis showed the superiority of
LSDD. However  in practice  the performance of LSDD depends on the choice of models (i.e. 
the kernel width σ and the regularization parameter λ). Here  we show that the model can be
optimized by cross-validation (CV). More speciﬁcally  we ﬁrst divide the samples X = {xi}n
i=1
and X (cid:2)
}T
t=1  respectively. Then we obtain a
t)  and
compute its hold-out error for Xt and X (cid:2)

density-difference estimate (cid:10)ft(x) from X\Xt and X (cid:2)\X (cid:2)
i(cid:2)=1 into T disjoint subsets {Xt}T
t=1 and {X (cid:2)
(cid:5)
(cid:10)ft(x) +

t (i.e.  all samples without Xt and X (cid:2)

(cid:2) (cid:10)ft(x)2dx − 2|Xt|

(cid:10)ft(x(cid:2)

(cid:5)

CV(t) :=

i(cid:2)}n

t as

) 

2|X (cid:2)

t

|

t

x∈Xt

x(cid:2)∈X (cid:2)

t

where |X| denotes the number of elements in the set X . We repeat this hold-out validation proce-
dure for t = 1  . . .   T   and compute the average hold-out error. Finally  we choose the model that
minimizes the average hold-out error.

3 L2-Distance Estimation by LSDD

(cid:4)

(cid:2)

(cid:2)

(cid:2)

) =

(cid:2)

) =

) :=

dx 

(x))

(x) 

f (x(cid:2)

i(cid:2)}n

(cid:2)
i(cid:2)=1.

L2(p  p

L2(p  p

(p(x) − p
(cid:2)

(cid:2)
In this section  we consider the problem of approximating the L2-distance between p(x) and p

(cid:2)
f (x)p(x)dx −(cid:17)
(cid:17)
with an LSDD estimator (cid:10)f (x) and approximate the expectations by empirical averages  we obtain
) ≈ (cid:10)h
(cid:4)(cid:10)θ. Similarly  for another expression L2(p  p
an LSDD estimator (cid:10)f (x) gives L2(p  p
H(cid:10)θ.
) ≈(cid:10)θ
Although(cid:10)h
(cid:4)(cid:10)θ and(cid:10)θ
(cid:4)(cid:10)θ −(cid:10)θ
) := 2(cid:10)h
(cid:10)L2(X  X (cid:2)

from their independent and identically distributed samples X := {xi}n
i=1 and X (cid:2)
(cid:2)
(x(cid:2)
(cid:17)
For an equivalent expression L2(p  p
)p
H(cid:10)θ themselves give approximations to L2(p  p

)dx(cid:2)  if we replace f (x)

f (x)2dx  replacing f (x) with

their combination  deﬁned by

)  we argue that the use of

H(cid:10)θ 

:= {x(cid:2)

form β(cid:10)h

(cid:4)(cid:10)θ + (1 − β)(cid:10)θ

is more sensible. To explain the reason  let us consider a generalized L2-distance estimator of the

H(cid:10)θ  where β is a real scalar. If the regularization parameter λ (≥ 0) is

H(cid:10)θ =(cid:10)h

small  this can be expressed as

(cid:4)(cid:10)θ + (1 − β)(cid:10)θ

(4)
where op denotes the probabilistic order. Thus  up to Op(λ)  the bias introduced by regularization
(i.e.  the second term in the right-hand side of Eq.(4) that depends on λ) can be eliminated if β = 2 

H−2(cid:10)h + op(λ) 
which yields Eq.(3). Note that  if no regularization is imposed (i.e.  λ = 0)  both(cid:10)h
yield(cid:10)h

H−1(cid:10)h − λ(2 − β)(cid:10)h
H−1(cid:10)h  the ﬁrst term in the right-hand side of Eq.(4).

(cid:4)(cid:10)θ and(cid:10)θ

H(cid:10)θ

β(cid:10)h

(cid:4)

(cid:4)

(3)

(cid:2)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

2

(cid:2)

(cid:4)

(cid:4)

4

Eq.(3) is actually equivalent to the negative of the optimal objective value of the LSDD optimization
problem without regularization (i.e.  Eq.(2) with λ = 0). This can be naturally interpreted through a
lower bound of L2(p  p

) obtained by Legendre-Fenchel convex duality [14]:

(cid:2)

(cid:18)

(cid:6)(cid:2)

(cid:7)

(cid:2)

(cid:19)

(cid:2)

(cid:2)

L2(p  p

) = sup

2

g

g(x)p(x)dx −

g(x(cid:2)

(cid:2)
)p

(x(cid:2)

)dx(cid:2)

−

g(x)2dx

 

where the supremum is attained at g = f. If the expectations are replaced by empirical estima-
tors and the Gaussian kernel model (1) is used as g  the above optimization problem is reduced
to the LSDD objective function without regularization (see Eq.(2)). Thus  LSDD corresponds to
approximately maximizing the above lower bound and Eq.(3) is its maximum value.

H(cid:10)θ ≥ (cid:10)h
(cid:4)(cid:10)θ −(cid:10)θ
(cid:4)(cid:10)θ ≥ (cid:10)θ
Through eigenvalue decomposition of H  we can show that 2(cid:10)h
Thus  our approximator (3) is not less than the plain approximators(cid:10)h
H(cid:10)θ.
(cid:4)(cid:10)θ and(cid:10)θ

H(cid:10)θ.

(cid:4)

(cid:4)

(cid:4)

4 Experiments
In this section  we experimentally demonstrate the usefulness of LSDD. A MATLAB R(cid:7) implemen-
tation of LSDD used for experiments is available from

“http://sugiyama-www.cs.titech.ac.jp/˜sugi/software/LSDD/”.

(cid:2)

(cid:4)

(cid:4)

  (4π)

  (4π)

−1I d)

−1I d).

Illustration: Let N (x; μ  Σ) be the multi-dimensional normal density with mean vector μ and
variance-covariance matrix Σ with respect to x  and let
(cid:2)
and p

p(x) = N (x; (μ  0  . . .   0)

(x) = N (x; (0  0  . . .   0)

(cid:2)
We ﬁrst illustrate how LSDD behaves under d = 1 and n = n
= 200. We compare LSDD with
KDEi (KDE with two Gaussian widths chosen independently by least-squares cross-validation [15])
and KDEj (KDE with two Gaussian widths chosen jointly to minimize the LSDD criterion [9]). The
number of folds in cross-validation is set to 5 for all methods.
Figure 1 depicts density-difference estimation results obtained by LSDD  KDEi  and KDEj for μ = 0
(i.e.  f (x) = p(x) − p
(x) = 0). The ﬁgure shows that LSDD and KDEj give accurate estimates
of the density difference f (x) = 0. On the other hand  the estimate obtained by KDEi is rather
ﬂuctuated  although both densities are reasonably well approximated by KDEs. This illustrates an
advantage of directly estimating the density difference without going through separate estimation of
each density. Figure 2 depicts the results for μ = 0.5 (i.e.  f (x) (cid:9)= 0)  showing again that LSDD
performs well. KDEi and KDEj give the same estimation result for this dataset  which slightly
underestimates the peaks.
Next  we compare the performance of L2-distance approximation based on LSDD  KDEi  and KDEj.
For μ = 0  0.2  0.4  0.6  0.8 and d = 1  5  we draw n = n
= 200 samples from the above p(x)
(cid:2)
(x). Figure 3 depicts the mean and standard error of estimated L2-distances over 1000 runs
and p
as functions of mean μ. When d = 1 (Figure 3(a))  the LSDD-based L2-distance estimator gives
the most accurate estimates of the true L2-distance  whereas the KDEi-based L2-distance estimator
slightly underestimates the true L2-distance when μ is large. This is caused by the fact that KDE
tends to provide smooth density estimates (see Figure 2(b) again): Such smooth density estimates
are accurate as density estimates  but the difference of smooth density estimates yields a small L2-
distance estimate [7]. The KDEj-based L2-distance estimator tends to improve this drawback of
KDEi  but it still slightly underestimates the true L2-distance when μ is large.
When d = 5 (Figure 3(b))  the KDE-based L2-distance estimators even severely underestimate
the true L2-distance when μ is large. On the other hand  the LSDD-based L2-distance estimator
still gives reasonably accurate estimates of the true L2-distance even when d = 5. However  we
note that LSDD also slightly underestimates the true L2-distance when μ is large  because slight
underestimation tends to yield smaller variance and thus such stabilized solutions are more accurate
in terms of the bias-variance trade-off.

(cid:2)

Semi-Supervised Class-Balance Estimation:
In real-world pattern recognition tasks  changes in
class balance between the training and test phases are often observed. In such cases  naive classiﬁer

5

f(x)
f(x)^

0.5

1

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

(cid:98)

−1

−0.5

p(x)−p’(x)
^^
p(x)−p’(x)
p(x)
^
p(x)
p’(x)
^
p’(x)

0.5

1

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

(cid:98)

−1

−0.5

−1

−0.5

0
x

(a) LSDD

0
x

(b) KDEi

Figure 1: Estimation of density difference when μ = 0 (i.e.  f (x) = p(x) − p

(cid:2)

f(x)
f(x)^

0.5

1

0
x

(c) KDEj

(x) = 0).

f(x)
f(x)^

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

(cid:98)

1.5

1

0.5

0

−0.5

−1

−1.5

(cid:98)

f(x)
f(x)^

−1

−0.5

0

0.5

1

1.5

1.5

1

0.5

0

−0.5

−1

−1.5

(cid:98)

(cid:98)

1.5

1

0.5

0

−0.5

−1

−1.5

(cid:98)

p(x)−p’(x)
^^
p(x)−p’(x)
p(x)
^
p(x)
p’(x)
^
p’(x)

−1

−0.5

0

0.5

1

1.5

x

(a) LSDD

x

(b) KDEi

Figure 2: Estimation of density difference when μ = 0.5 (i.e.  f (x) = p(x) − p
(cid:2)

(x) (cid:9)= 0).

−1

−0.5

0

0.5

1

1.5

x

(c) KDEj

True
LSDD
KDE i
KDE j

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

e
c
n
a
t
s
i
d
2
L

 

0

(cid:98)

0

0.1

0.2

0.3

True
LSDD
KDE i
KDE j

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

e
c
n
a
t
s
i
d
2
L

 

0.5

0.6

0.7

0.8

0

(cid:98)

0

0.1

0.2

0.3

0.4
μ

0.4
μ

0.5

0.6

0.7

0.8

(b) d = 5
(cid:2)
Figure 3: L2-distance estimation by LSDD  KDEi  and KDEj for n = n
Gaussian mean μ. Means and standard errors over 1000 runs are plotted.

(a) d = 1

= 200 as functions of the

training produces signiﬁcant estimation bias because the class balance in the training dataset does
not properly reﬂect that of the test dataset.
Here  we consider a binary pattern recognition task of classifying pattern x ∈ Rd to class y ∈
{+1 −1}. Our goal is to learn the class balance of a test dataset in a semi-supervised learning setup
where unlabeled test samples are provided in addition to labeled training samples [16]. The class
balance in the test set can be estimated by matching a mixture of class-wise training input densities 

qtest(x; π) := πptrain(x|y = +1) + (1 − π)ptrain(x|y = −1) 

to the test input density ptest(x) [5]  where π ∈ [0  1] is a mixing coefﬁcient to learn. See Figure 4
for schematic illustration. Here  we use the L2-distance estimated by LSDD and the difference of
KDEs for this distribution matching. Note that  when LSDD is used to estimate the L2-distance 
separate estimation of ptrain(x|y = ±1) is not involved  but the difference between ptest(x) and
qtest(x; π) is directly estimated.
We use four UCI benchmark datasets (http://archive.ics.uci.edu/ml/)  where we ran-
domly choose 10 labeled training samples from each class and 50 unlabeled test samples following
= 0.1  0.2  . . .   0.9. Figure 6 plots the mean and standard error of the squared
true class-prior π
difference between true and estimated class-balances π and the misclassiﬁcation error by a weighted
(cid:5)2-regularized least-squares classiﬁer [17] with weighted cross-validation [18] over 1000 runs. The
results show that LSDD tends to provide better class-balance estimates than the KDEi-based  the
KDEj-based  and the EM-based methods [5]  which are translated into lower classiﬁcation errors.

∗

6

(cid:4)

(cid:4)

(cid:4)

]

  y(t + 1)

  . . .   y(t + k − 1)

Unsupervised Change Detection: The objective of change detection is to discover abrupt prop-
erty changes behind time-series data. Let y(t) ∈ Rm be an m-dimensional time-series sample at
(cid:4) ∈ Rkm be a subsequence of time
time t  and let Y (t) := [y(t)
series at time t with length k. We treat the subsequence Y (t) as a sample  instead of a single point
y(t)  by which time-dependent information can be incorporated naturally [6]. Let Y(t) be a set of r
retrospective subsequence samples starting at time t: Y(t) := {Y (t)  Y (t + 1)  . . .   Y (t + r − 1)}.
Our strategy is to compute a certain dissimilarity measure between two consecutive segments Y(t)
and Y(t+r)  and use it as the plausibility of change points (see Figure 5). As a dissimilarity measure 
we use the L2-distance estimated by LSDD and the Kullback-Leibler (KL) divergence estimated by
the KL importance estimation procedure (KLIEP) [2  3]. We set k = 10 and r = 50.
First  we use the IPSJ SIG-SLP Corpora and Environments for Noisy Speech Recognition (CEN-
SREC) dataset (http://research.nii.ac.jp/src/en/CENSREC-1-C.html). This
dataset is provided by the National Institute of Informatics  Japan that records human voice in a
noisy environment such as a restaurant. The top graphs in Figure 7(a) display the original time-
series (true change points were manually annotated) and change scores obtained by KLIEP and
LSDD. The graphs show that the LSDD-based change score indicates the existence of change points
more clearly than the KLIEP-based change score.
Next  we use a dataset taken from the Human Activity Sensing Consortium (HASC) challenge
2011 (http://hasc.jp/hc2011/)  which provides human activity information collected by
portable three-axis accelerometers. Because the orientation of the accelerometers is not necessarily
ﬁxed  we take the (cid:5)2-norm of the 3-dimensional data. The HASC dataset is relatively simple  so
we artiﬁcially added zero-mean Gaussian noise with standard deviation 5 at each time point with
probability 0.005. The top graphs in Figure 7(b) display the original time-series for a sequence of
actions “jog”  “stay”  “stair down”  “stay”  and “stair up” (there exists 4 change points at time 540 
1110  1728  and 2286) and the change scores obtained by KLIEP and LSDD. The graphs show that
the LSDD score is much more stable and interpretable than the KLIEP score.
Finally  we compare the change-detection performance more systematically using the receiver op-
erating characteristic (ROC) curves (i.e.  the false positive rate vs. the true positive rate) and the
area under the ROC curve (AUC) values. In addition to LSDD and KLIEP  we test the L2-distance
estimated by KDEi and KDEj and native change detection methods based on autoregressive models
(AR) [19]  subspace identiﬁcation (SI) [20]  singular spectrum transformation (SST) [21]  one-class
support vector machine (SVM) [22]  kernel Fisher discriminant analysis (KFD) [23]  and kernel
change-point detection (KCP) [24]. Tuning parameters included in these methods were manually op-
timized. For 10 datasets taken from each of the CENSREC and HASC data collections  mean ROC
curves and AUC values are displayed at the bottom of Figure 7(b). The results show that LSDD tends
to outperform other methods and is comparable to state-of-the-art native change-detection methods.

5 Conclusions

In this paper  we proposed a method for directly estimating the difference between two probability
density functions without density estimation. The proposed method  called the least-squares density-
difference (LSDD)  was derived within the framework of kernel least-squares estimation  and its
solution can be computed analytically in a computationally efﬁcient and stable manner. Furthermore 
LSDD is equipped with cross-validation  and thus all tuning parameters such as the kernel width and
the regularization parameter can be systematically and objectively optimized. We derived a ﬁnite-
sample error bound for LSDD in a non-parametric setup  and showed that it achieves the optimal
convergence rate. We also proposed an L2-distance estimator based on LSDD  which nicely cancels
a bias caused by regularization. Through experiments on class-prior estimation and change-point
detection  the usefulness of the proposed LSDD was demonstrated.

Acknowledgments: We would like to thank Wittawat Jitkrittum for his comments and Za¨ıd Har-
chaoui for providing us a program code of kernel change-point detection. MS was supported by
MEXT KAKENHI 23300069 and AOARD  TK was supported by MEXT KAKENHI 24500340 
TS was supported by MEXT KAKENHI 22700289  the Aihara Project  the FIRST program from
JSPS initiated by CSTP  and the Global COE Program “The research and training center for new
development in mathematics”  MEXT  Japan  MCdP was supported by MEXT Scholarship  SL was
supported by the JST PRESTO program  and IT was supported by MEXT KAKENHI 23700165.

7

p

tr ain

(x|y = −1)

p

train

(x|y = +1)

pte st (x)

(

y (t + r )

f

f

g

g

g

h

h

h

i

i

Time

j

k

l

Y (t + r )

Y (t + r + 1)

y (t)

a

a

Y (t)

Y (t + 1)

b

b

b

r

b

e

d

(

c

c

c

k

Y (t + r − 1)

e

f

g

j

k
(

l

Y (t + 2r − 1)

)

Figure 4: Class-balance estimation.

x

Y (t)

Y (t + r)

Figure 5: Change-point detection.

 

r
o
r
r
e
d
e
r
a
u
q
s
 
e
c
n
a
a
b
 
s
s
a
C

l

l

0.25

0.2

0.15

0.1

0.05

0

LSDD
KDEi

KDEj
EM

0.2

0.4

0.6

π*

0.8

e
t
a
r
 
n
o
i
t
a
c
fi
i
s
s
a
l
c
s
i

M

0.5

0.4

0.3

0.2

0.1

0.2

0.4

0.6

π*

0.8

(a) Australian dataset

 

r
o
r
r
e
d
e
r
a
u
q
s
 
e
c
n
a
a
b
 
s
s
a
C

l

l

0.15

0.1

0.05

0.45

0.2

0.4

0.6

π*

0.8

 

r
o
r
r
e
d
e
r
a
u
q
s
 
e
c
n
a
a
b
 
s
s
a
C

l

l

0.3
0.25
0.2
0.15
0.1
0.05
0

0.2

0.4

0.6

π*

0.8

e
t
a
r
 
n
o
i
t
a
c
fi
i
s
s
a
l
c
s
i

M

0.4

0.35

0.3

0.25

0.2

0.2

0.4

0.6

π*

0.8

(b) Diabetes dataset

e
t
a
r
 
n
o
i
t
a
c
fi
i
s
s
a
l
c
s
i

M

0.5

0.4

0.3

0.2

0.2

0.4

0.6

π*

0.8

(c) German dataset

 

r
o
r
r
e
d
e
r
a
u
q
s
 
e
c
n
a
a
b
 
s
s
a
C

l

l

e
t
a
r
 
n
o
i
t
a
c
fi
i
s
s
a
l
c
s
i

M

0.25

0.2

0.15

0.1

0.05

0

0.5

0.4

0.3

0.2

0.1

0.2

0.4

0.6

π*

0.8

0.2

0.4

0.6

π*

0.8

(d) Statlogheart dataset

Figure 6: Results of semi-supervised class-balance estimation. Top: Squared error of class balance
estimation. Bottom: Misclassiﬁcation error by a weighted (cid:5)2-regularized least-squares classiﬁer.

Original

data

500

1000

1500

2000

2500

3000

KLIEP score

500

1000

1500

2000

2500

3000

LSDD score

500

1000

1500
Time

2000

2500

3000

(cid:3)

LSDD
KDEi
KDEj
KLIEP
AR
SI
SST
SVM
KFD
KCP

0.8

1

0.1
0
−0.1
−0.2

0

40

20

0

0

1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 

e
v
i
t
i
s
o
p
 
e
u
r
T

0

(cid:3)
0

0.2

Original data

500

1000

1500

KLIEP score

500

1000

1500

LSDD score

500

1000

Time

1500

(cid:3)

LSDD
KDEi
KDEj
KLIEP
AR
SI
SST
SVM
KFD
KCP

0.8

1

0.4

0.6

False positive rate

5

0

−5

0

40

20

0

2

1

0

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 

e
v
i
t
i
s
o
p
 
e
u
r
T

0

(cid:3)
0

0.2

0.4

0.6

False positive rate

AUC LSDD KDEi KDEj KLIEP AR SI SST SVM KFD KCP
.635 .749 .756 .580 .773 .905 .913
Mean .879 .755 .705
.014 .016 .023
.030 .013 .012 .023 .032 .013 .024
SE

AUC LSDD KDEi KDEj KLIEP AR SI SST SVM KFD KCP
.638 .799 .762 .764 .815 .856 .730
Mean .843 .764 .751
.013 .029 .036
.020 .026 .020 .016 .018 .023 .032
SE

(a) Speech data

(b) Accelerometer data

Figure 7: Results of unsupervised change detection. From top to bottom: Original time-series 
change scores obtained by KLIEP and LSDD  mean ROC curves over 10 datasets  and AUC values
for 10 datasets. The best method and comparable ones in terms of mean AUC values by the t-test at
the signiﬁcance level 5% are indicated with boldface. “SE” stands for “Standard error”.

8

References
[1] V. N. Vapnik. Statistical Learning Theory. Wiley  New York  NY  USA  1998.
[2] M. Sugiyama  T. Suzuki  S. Nakajima  H. Kashima  P. von B¨unau  and M. Kawanabe. Di-
rect importance estimation for covariate shift adaptation. Annals of the Institute of Statistical
Mathematics  60(4):699–746  2008.

[3] X. Nguyen  M. J. Wainwright  and M. I. Jordan. Estimating divergence functionals and the
IEEE Transactions on Information Theory 

likelihood ratio by convex risk minimization.
56(11):5847–5861  2010.

[4] C. Cortes  Y. Mansour  and M. Mohri. Learning bounds for importance weighting. In Advances

in Neural Information Processing Systems 23  pages 442–450  2010.

[5] M. Saerens  P. Latinne  and C. Decaestecker. Adjusting the outputs of a classiﬁer to new a

priori probabilities: A simple procedure. Neural Computation  14(1):21–41  2002.

[6] Y. Kawahara and M. Sugiyama. Sequential change-point detection based on direct density-

ratio estimation. Statistical Analysis and Data Mining  5(2):114–127  2012.

[7] N. Anderson  P. Hall  and D. Titterington. Two-sample test statistics for measuring discrep-
ancies between two multivariate probability density functions using kernel-based density esti-
mates. Journal of Multivariate Analysis  50(1):41–54  1994.

[8] A. Basu  I. R. Harris  N. L. Hjort  and M. C. Jones. Robust and efﬁcient estimation by min-

imising a density power divergence. Biometrika  85(3):549–559  1998.

[9] P. Hall and M. P. Wand. On nonparametric discrimination using density differences.

Biometrika  75(3):541–547  1988.

[10] M. Eberts and I. Steinwart. Optimal learning rates for least squares SVMs using Gaussian
kernels. In Advances in Neural Information Processing Systems 24  pages 1539–1547  2011.
[11] R. H. Farrell. On the best obtainable asymptotic rates of convergence in estimation of a density

function at a point. The Annals of Mathematical Statistics  43(1):170–180  1972.

[12] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall 

London  UK  1986.

[13] E. Parzen. On the estimation of a probability density function and mode. The Annals of

Mathematical Statistics  33(3):1065–1076  1962.

[14] R. T. Rockafellar. Convex Analysis. Princeton University Press  Princeton  NJ  USA  1970.
[15] W. H¨ardle  M. M¨uller  S. Sperlich  and A. Werwatz. Nonparametric and Semiparametric

Models. Springer  Berlin  Germany  2004.

[16] O. Chapelle  B. Sch¨olkopf  and A. Zien  editors. Semi-Supervised Learning. MIT Press 

Cambridge  MA  USA  2006.

[17] R. Rifkin  G. Yeo  and T. Poggio. Regularized least-squares classiﬁcation.

In Advances in
Learning Theory: Methods  Models and Applications  pages 131–154. IOS Press  Amsterdam 
the Netherlands  2003.

[18] M. Sugiyama  M. Krauledat  and K.-R. M¨uller. Covariate shift adaptation by importance

weighted cross validation. Journal of Machine Learning Research  8:985–1005  May 2007.

[19] Y. Takeuchi and K. Yamanishi. A unifying framework for detecting outliers and change points
from non-stationary time series data. IEEE Transactions on Knowledge and Data Engineering 
18(4):482–489  2006.

[20] Y. Kawahara  T. Yairi  and K. Machida. Change-point detection in time-series data based on
In Proceedings of the 7th IEEE International Conference on Data

subspace identiﬁcation.
Mining  pages 559–564  2007.

[21] V. Moskvina and A. A. Zhigljavsky. An algorithm based on singular spectrum analysis for
change-point detection. Communication in Statistics: Simulation & Computation  32(2):319–
352  2003.

[22] F. Desobry  M. Davy  and C. Doncarli. An online kernel change detection algorithm. IEEE

Transactions on Signal Processing  53(8):2961–2974  2005.

[23] Z. Harchaoui  F. Bach  and E. Moulines. Kernel change-point analysis. In Advances in Neural

Information Processing Systems 21  pages 609–616  2009.

[24] S. Arlot  A. Celisse  and Z. Harchaoui. Kernel change-point detection. Technical Report

1202.3878  arXiv  2012.

9

,Pinghua Gong
Jieping Ye
Kenji Kawaguchi
Sekitoshi Kanai
Yasuhiro Fujiwara
Sotetsu Iwamura