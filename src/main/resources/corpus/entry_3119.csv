2009,Learning to Explore and Exploit in POMDPs,A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation  in partially observable environments. The method subsumes traditional exploration  in which the agent takes actions to gather information about the environment  and active learning  in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the specific problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.,Learning to Explore and Exploit in POMDPs

Chenghui Cai  Xuejun Liao  and Lawrence Carin
Department of Electrical and Computer Engineering

Duke University

Durham  NC 27708-0291  USA

Abstract

A fundamental objective in reinforcement learning is the maintenance of a proper
balance between exploration and exploitation. This problem becomes more chal-
lenging when the agent can only partially observe the states of its environment.
In this paper we propose a dual-policy method for jointly learning the agent be-
havior and the balance between exploration exploitation  in partially observable
environments. The method subsumes traditional exploration  in which the agent
takes actions to gather information about the environment  and active learning  in
which the agent queries an oracle for optimal actions (with an associated cost for
employing the oracle). The form of the employed exploration is dictated by the
speciﬁc problem. Theoretical guarantees are provided concerning the optimality
of the balancing of exploration and exploitation. The effectiveness of the method
is demonstrated by experimental results on benchmark problems.

1 Introduction

A fundamental challenge facing reinforcement learning (RL) algorithms is to maintain a proper
balance between exploration and exploitation. The policy designed based on previous experiences
is by construction constrained  and may not be optimal as a result of inexperience. Therefore  it
is desirable to take actions with the goal of enhancing experience. Although these actions may not
necessarily yield optimal near-term reward toward the ultimate goal  they could  over a long horizon 
yield improved long-term reward. The fundamental challenge is to achieve an optimal balance
between exploration and exploitation; the former is performed with the goal of enhancing experience
and preventing premature convergence to suboptimal behavior  and the latter is performed with the
goal of employing available experience to deﬁne perceived optimal actions.

For a Markov decision process (MDP)  the problem of balancing exploration and exploitation has
been addressed successfully by the E3 [4  5] and R-max [2] algorithms. Many important applica-
tions  however  have environments whose states are not completely observed  leading to partially
observable MDPs (POMDPs). Reinforcement learning in POMDPs is challenging  particularly in
the context of balancing exploration and exploitation. Recent work targeted on solving the explo-
ration vs. exploitation problem is based on an augmented POMDP  with a product state space over
the environment states and the unknown POMDP parameters [9]. This  however  entails solving a
complicated planning problem  which has a state space that grows exponentially with the number
of unknown parameters  making the problem quickly intractable in practice. To mitigate this com-
plexity  active learning methods have been proposed for POMDPs  which borrow similar ideas from
supervised learning  and apply them to selectively query an oracle (domain expert) for the optimal
action [3]. Active learning has found success in many collaborative human-machine tasks where
expert advice is available.

In this paper we propose a dual-policy approach to balance exploration and exploitation in POMDPs 
by simultaneously learning two policies with partially shared internal structure. The ﬁrst policy 
termed the primary policy  deﬁnes actions based on previous experience; the second policy  termed

1

the auxiliary policy  is a meta-level policy maintaining a proper balance between exploration and
exploitation. We employ the regionalized policy representation (RPR) [6] to parameterize both
policies  and perform Bayesian learning to update the policy posteriors. The approach applies in
either of two cases: (i) the agent explores by randomly taking the actions that have been insufﬁciently
tried before (traditional exploration)  or (ii) the agent explores by querying an oracle for the optimal
action (active learning). In the latter case  the agent is assessed a query cost from the oracle  in
addition to the reward received from the environment. Either (i) or (ii) is employed as an exploration
vehicle  depending upon the application.

The dual-policy approach possesses interesting convergence properties  similar to those of E3 [5]
and Rmax [2]. However  our approach assumes the environment is a POMDP while E3 and Rmax
both assume an MDP environment. Another distinction is that our approach learns the agent policy
directly from episodes  without estimating the POMDP model. This is in contrast to E3 and Rmax
(both learn MDP models) and the active-learning method in [3] (which learns POMDP models).

2 Regionalized Policy Representation

We ﬁrst provide a brief review of the regionalized policy representation  which is used to parame-
terize the primary policy and the auxiliary policy as discussed above. The material in this section is
taken from [6]  with the proofs omitted here.

Deﬁnition 2.1 A regionalized policy representation is a tuple (A  O  Z  W  µ  π). The A and O are
respectively a ﬁnite set of actions and observations. The Z is a ﬁnite set of belief regions. The W is
the belief-region transition function with W (z  a  o′  z′) denoting the probability of transiting from
z to z′ when taking action a in z results in observing o′. The µ is the initial distribution of belief
regions with µ(z) denoting the probability of initially being in z. The π are the region-dependent
stochastic policies with π(z  a) denoting the probability of taking action a in z.

We denote A = {1  2  . . .   |A|}  where |A| is the cardinality of A. Similarly  O = {1  2  . . .   |O|}
and Z = {1  2  . . .   |Z|}. We abbreviate (a0  a1  . . .   aT ) as a0:T and similarly  (o1  o2  . . .   aT )
as o1:T and (z0  z1  . . .   zT ) as z0:T   where the subscripts indexes discrete time steps. The history
ht = {a0:t−1  o1:t} is deﬁned as a sequence of actions performed and observations received up to
t. Let Θ = {π  µ  W } denote the RPR parameters. Given ht  the RPR yields a joint probability
distribution of z0:t and a0:t as follows

p(a0:t  z0:t|o1:t  Θ) = µ(z0)π(z0  a0)Qt

(1)
By marginalizing z0:t out in (1)  we obtain p(a0:t|o1:t  Θ). Furthermore  the history-dependent
distribution of action choices is obtained as follows:

τ =1W (zτ −1  aτ −1  oτ   zτ )π(zτ   aτ )

p(aτ |hτ   Θ) = p(a0:τ |o1:τ   Θ)[p(a0:τ −1|o1:τ −1  Θ)]−1

which gives a stochastic policy for choosing the action aτ . The action choice depends solely on the
historical actions and observations  with the unobservable belief regions marginalized out.

2.1 Learning Criterion

Bayesian learning of the RPR is based on the experiences collected from the agent-environment
interaction. Assuming the interaction is episodic  i.e.  it breaks into subsequences called episodes
[10]  we represent the experiences by a set of episodes.

Deﬁnition 2.2 An episode is a sequence of agent-environment
an absorbing state that
(ak
rk
Tk
a  and r are respectively observations  actions  and immediate rewards.

interactions terminated in
An episode is denoted by
transits to itself with zero reward.
)  where the subscripts are discrete times  k indexes the episodes  and o 

1 · · · ok
Tk

1 ak

ak
Tk

0 ok

0 rk

1rk

Deﬁnition 2.3 (The RPR Optimality Criterion) Let D(K) = {(ak
k=1
be a set of episodes obtained by an agent interacting with the environment by following policy
Π to select actions  where Π is an arbitrary stochastic policy with action-selecting distributions
pΠ(at|ht) > 0  ∀ action at  ∀ history ht. The RPR optimality criterion is deﬁned as

1 · · · ok
Tk

1 ak

ak
Tk

0 ok

1 rk

0rk

)}K

rk
Tk

	
	

bV (D(K); Θ)

def.
= 1

K PK

k=1PTk

2

t=0 γtrk

t

t

t

τ =0 p(ak
τ |hk
τ =0 pΠ(ak

τ  Θ)
τ |hk
τ )

(2)

where hk
0 < γ < 1 is the discount  and Θ denotes the RPR parameters.

t is the history of actions and observations up to time t in the k-th episode 

1 · · · ok

t = ak

1 ak

0 ok

parameterized by Θ for an inﬁnite number of steps. Therefore  the RPR resulting from maximization

Throughout the paper  we call bV (D(K); Θ) the empirical value function of Θ. It is proven in [6]
that limK→∞ bV (D(K); Θ) is the expected sum of discounted rewards by following the RPR policy
of bV (D(K); Θ) approaches the optimal as K is large (assuming |Z| is appropriate). In the Bayesian

setting discussed below  we use a noninformative prior for Θ  leading to a posterior of Θ peaked at
the optimal RPR  therefore the agent is guaranteed to sample the optimal or a near-optimal policy
with overwhelming probability.

2.2 Bayesian Learning

Let G0(Θ) represent the prior distribution of the RPR parameters. We deﬁne the posterior of Θ as

(3)

(4)

p(Θ|D(K)  G0)

def.

= bV (D(K); Θ)G0(Θ)[bV (D(K))]−1

LB({qk

prior as a product of Dirichlet distributions 

is an empirical value function  thus (3) is a non-standard use of Bayes rule. However  (3) indeed
gives a distribution whose shape incorporates both the prior and the empirical information.

where bV (D(K)) =R bV (D(K); Θ)G0(Θ)dΘ is the marginal empirical value. Note that bV (D(K); Θ)
Since each term in bV (D(K); Θ) is a product of multinomial distributions  it is natural to choose the
i=1Dir(cid:0)π(i  1)  · · ·   π(i  |A|)(cid:12)(cid:12)(cid:12)ρi(cid:1) 
where p(µ|υ) = Dir(cid:0)µ(1)  · · ·   µ(|Z|)(cid:12)(cid:12)υ(cid:1)  p(π|ρ) = Q|Z|
i=1Dir(cid:0)W (i  a  o  1)  · · ·   W (i  a  o  |Z|)(cid:12)(cid:12)ωi a o(cid:1); ρi = {ρi m}|A|
p(W |ω) = Q|A|

m=1 
υ = {υi}|Z|
j=1 are hyper-parameters. With the prior thus chosen  the
posterior in (3) is a large mixture of Dirichlet products  and therefore posterior analysis by Gibbs
sampling is inefﬁcient. To overcome this  we employ the variational Bayesian technique [1] to obtain

i=1  and ωi a o = {ωi a o j}|Z|

G0(Θ) = p(µ|υ)p(π|ρ)p(W |ω)

a=1Q|O|

o=1Q|Z|

a variational posterior by maximizing a lower bound to lnR bV (D(K); Θ)G0(Θ)dΘ 

t }  g(Θ)) = lnZ bV (D(K); Θ)G0(Θ)dΘ − KL({qk
k=1 PTk
K PK

0:t) ≥ 1  g(Θ) ≥ 1 R g(Θ)dΘ = 1 

t }  g(Θ) are variational distributions satisfying qk

where {qk
and 1
the Kullback-Leibler (KL) distance between probability measure q and p.
The factorized form {qt(z0:t)g(Θ)} represents an approximation of the weighted joint posterior of
Θ and z’s when the lower bound reaches the maximum  and the corresponding g(Θ) is called the
variational approximate posterior of Θ. The lower bound maximization is accomplished by solving
{qt(z0:t)} and g(Θ) alternately  keeping one ﬁxed while solving for the other. The solutions are
summarized in Theorem 2.4; the proof is in [6].

t=1P|Z|

and KL(qkp) denotes

0:t) = 1; νk

0:t)g(Θ)}||{νk

	
b
bυi)  andfW (i  a  o  j) = eψ(bωi a o j )−ψ(

1:t eΘ)

where eΘ = {eπ eµ fW } is a set of under-normalized probability mass functions  with eπ(i  m) =

and ψ is the digamma function. The g(Θ) has the same form as the prior G0 in (4)  except that the
hyper-parameter are updated as

bρi m)  eµ(i) = eψ(bυi)−ψ(

Theorem 2.4 Given the initialization bρ = ρ  bυ = υ  bω = ω  iterative application of the following

updates produces a sequence of monotonically increasing lower bounds LB({qk
converges to a maxima. The update of {qk

t }  g(Θ))  which

eψ(bρi m)−ψ(

bωi a o j ) 



z (zk
qk

0:t) = σk

t p(zk

0:t|ak





t p(zk

0:t  Θ|ak

0:t  ok

1:t)
V (D(K))

t (zk
γ trk

τ =0 pΠ(ak

τ |hk
τ )

zk
0  ···  zk

t =1

t =

t

t p(ak

0:t|ok

0:t  ok

qk
t (zk

t } is

|Z|
i=1

t(zk

1:t)})

|A|
m=1

|A|
j=1

bυi = υi +PK

k=1PTk

t=0σk

t φk

t 0(i)

3

where ξk

t τ (i  j) = p(zk

bρi a = ρi a +PK
bωi a o j = ωi a o j +PK
t = (cid:2)γtrk

t=0Pt
k=1PTk
k=1PTk
t=0Pt
1:t eΘ)  φk
1:t eΘ)(cid:3)(cid:2)Qt

τ +1 = j|ak
0:t|ok
t p(ak

τ = i  zk

0:t  ok

σk

τ   a)

t τ (i)δ(ak
t φk
τ =0σk
t ξk
τ =1σk
t τ −1(i  j)δ(ak
t τ (i) = p(zk

τ =0 pΠ(ak

τ |hk

τ −1  a)δ(ok

τ   o)

1:t eΘ)  and

τ = i|ak

0:t  ok

τ )bV (D(K)|eΘ)(cid:3)−1

(5)

3 Dual-RPR: Joint Policy for the Agent Behavior and the Trade-Off

Between Exploration and Exploitation

Assume that the agent uses the RPR described in Section 2 to govern its behavior in the unknown
POMDP environment (the primary policy). Bayesian learning employs the empirical value function

bV (D(K); Θ) in (2) in place of a likelihood function  to obtain the posterior of the RPR parameters

Θ. The episodes D(K) may be obtained from the environment by following an arbitrary stochastic
policy Π with pΠ(a|h) > 0  ∀ a  ∀ h. Although any such Π guarantees optimality of the resulting
RPR  the choice of Π affects the convergence speed. A good choice of Π avoids episodes that do
not bring new information to improve the RPR  and thus the agent does not have to see all possible
episodes before the RPR becomes optimal.

In batch learning  all episodes are collected before the learning begins  and thus Π is pre-chosen
and does not change during the learning [6]. In online learning  however  the episodes are collected
during the learning  and the RPR is updated upon completion of each episode. Therefore there is
a chance to exploit the RPR to avoid repeated learning in the same part of the environment. The
agent should recognize belief regions it is familiar with  and exploit the existing RPR policy there;
in belief regions inferred as new  the agent should explore. This balance between exploration and
exploitation is performed with the goal of accumulating a large long-run reward.

We consider online learning of the RPR (as the primary policy) and choose Π as a mixture of two
policies: one is the current RPR Θ (exploitation) and the other is an exploration policy Πe. This gives
the action-choosing probability pΠ(a|h) = p(y = 0|h)p(a|h  Θ  y = 0)+p(y = 1|h)p(a|h  Πe  y =
1)  where y = 0 (y = 1) indicates exploitation (exploration). The problem of choosing good Π then
reduces to a proper balance between exploitation and exploration: the agent should exploit Θ when
doing so is highly rewarding  while following Πe to enhance experience and improve Θ.
An auxiliary RPR is employed to represent the policy for balancing exploration and exploitation 
i.e.  the history-dependent distribution p(y|h). The auxiliary RPR shares the parameters {µ  W }
with the primary RPR  but with π = {π(z  a) : a ∈ A  z ∈ Z} replaced by λ = {λ(z  y) : y =
0 or 1  z ∈ Z}  where λ(z  y) is the probability of choosing exploitation (y = 0) or exploration
(y = 1) in belief region z. Let λ have the prior

p(λ|u) =Q|Z|

i=1Beta(cid:16)λ(i  0)  λ(i  1)(cid:12)(cid:12)(cid:12)u0  u1(cid:17).

(6)
In order to encourage exploration when the agent has little experience  we choose u0 = 1 and u1 > 1
so that  at the beginning of learning  the auxiliary RPR always suggests exploration. As the agent
accumulates episodes of experience  it comes to know a certain part of the environment in which the
episodes have been collected. This knowledge is reﬂected in the auxiliary RPR  which  along with
the primary RPR  is updated upon completion of each new episode.

Since the environment is a POMDP  the agent’s knowledge should be represented in the space of
belief states. However  the agent cannot directly access the belief states  because computation of
belief states requires knowing the true POMDP model  which is not available. Fortunately  the RPR
formulation provides a compact representation of H = {h}  the space of histories  where each
history h corresponds to a belief state in the POMDP. Within the RPR formulation  H is represented
internally as the set of distributions over belief regions z ∈ Z  which allows the agent to access
H based on a subset of samples from H. Let Hknown be the part of H that has become known to
the agent  i.e.  the primary RPR is optimal in Hknown and thus the agent should begin to exploit
upon entering Hknown. As will be clear below  Hknown can be identiﬁed by Hknown = {h : p(y =
0|h  Θ  λ) ≈ 1}  if the posterior of λ is updated by

t=0Pt
k=1PTk
bui 0 = u0 +PK
bui 1 = max(cid:0)η  u1 −PK
k=1PTk

τ =0σk

t φk

t=0Pt

t τ (i) 
τ =0yk

t γtc φk

t τ (i)(cid:1) 

4

(7)

(8)

t   the
where η is a small positive number  and σk
t = rmeta if the goal is reached at time t in
meta-reward received at t in episode k. We have mk
episode k  and mk
t = 0 otherwise  where rmeta > 0 is a constant. When Πe is provided by an oracle
(active learning)  a query cost c > 0 is taken into account in (8)  by subtracting c from u1. Thus  the
probability of exploration is reduced each time the agent makes a query to the oracle (i.e.  yk
t = 1).

t is the same in (5) except that rk

t is replaced by mk

After a certain number of queries bui 1 becomes the small positive number η (it never becomes zero

due to the max operator)  at which point the agent stops querying in belief region z = i.
In (7) and (8)  exploitation always receives a “credit”  while exploration never receives credit (ex-
ploration is actually discredited when Πe is an oracle). This update makes sure that the chance
of exploitation monotonically increases as the episodes accumulate. Exploration receives no credit
because it has been pre-assigned a credit (u1) in the prior  and the chance of exploration should
monotonically decrease with the accumulation of episodes. The parameter u1 represents the agent’s
prior for the amount of needed exploration. When c > 0  u1 is discredited by the cost and the agent
needs a larger u1 (than when c = 0) to obtain the same amount of exploration. The fact that the
amount of exploration monotonically increases with u1 implies that  one can always ﬁnd a large
enough u1 to ensure that the primary RPR is optimal in Hknown = {h : p(y = 0|h  Θ  λ) ≈ 1}.
However  an unnecessarily large u1 makes the agent over-explore and leads to slow convergence.
Let umin
exists in the
analysis below. The possible range of umin

denote the minimum u1 that ensures optimality in Hknown. We assume umin

is examined in the experiments.

1

1

1

4 Optimality and Convergence Analysis

Let M be the true POMDP model. We ﬁrst introduce an equivalent expression for the empirical
value function in (2) 

T

bV (E (K)

; Θ) = PE (K)

T PT

t=0γtrtp(a0:t  o1:t  rt|y0:t = 0  Θ  M ) 

(9)

where the ﬁrst summation is over all elements in E (K)
T ⊆ ET   and ET = {(a0:T   o1:T   r0:T ) : at ∈
A  ot ∈ O  t = 0  1  · · ·   T } is the complete set of episodes of length T in the POMDP  with no
repeated elements. The condition y0:t = 0  which is an an abbreviation for yτ = 0 ∀ τ = 0  1  · · ·   t 
; Θ) is the empirical value

indicates that the agent always follows the RPR (Θ) here. Note bV (E (K)

  as is bV (D(K); Θ) on D(K). When T = ∞ 1  the two are identical

function of Θ deﬁned on E (K)
up to a difference in acquiring the episodes: E (K)
is a simple enumeration of distinct episodes
while D(K) may contain identical episodes. The multiplicity of an episode in D(K) results from the
sampling process (by following a policy to interact with the environment). Note that the empirical
value function deﬁned using E (K)
is interesting only for theoretical analysis  because the evaluation
requires knowing the true POMDP model  not available in practice. We deﬁne the optimistic value
function

T

T

T

T

T ; Θ λ  Πe) =X

TXt=0

γt

1Xy0 ···  yt=0(cid:0)rt +(Rmax−rt)∨t

τ =0 yτ(cid:1)p(a0:t  o1:t  rt  y0:t|Θ λ M Πe) (10)

E (K)

T

bVf (E (K)

The following lemma is proven in the Appendix.

where ∨t
τ =0yτ indicates that the agent receives rt if and only if yτ = 0 at all time steps τ =
1  2  · · ·   t; otherwise  it receives Rmax at t  which is an upper bound of the rewards in the environ-
; Θ  λ  Πe).

ment. Similarly we can deﬁne bV (D(K); Θ  λ  Πe)  the equivalent expression for bVf (E (K)
Lemma 4.1 Let bV (E (K)

Let
  Θ  λ  Πe) be the probability of executing the exploration policy Πe at least once in

; Θ  λ  Πe)  and Rmax be deﬁned as above.

; Θ)  bVf (E (K)

T

T

T

  under the auxiliary RPR (Θ  λ) and the exploration policy Πe. Then

Pexlpore(E (K)
some episode in E (K)

T

T

Pexlpore(E (K)

T

  Θ  λ  Πe) ≥

1 − γ
Rmax

T

|bV (E (K)

; Θ) − bVf (E (K)

T

; Θ  λ  Πe)|.

1An episode almost always terminates in ﬁnite time steps in practice and the agent stays in the absorbing
state with zero reward for the remaining inﬁnite steps after an episode is terminated [10]. The inﬁnite horizon
is only to ensure theoretically all episodes have the same horizon length.

5

Proposition 4.2 Let Θ be the optimal RPR on E (K)
∞ and Θ∗ be the optimal RPR in the complete
POMDP environment. Let the auxiliary RPR hyper-parameters (λ) be updated according to (7) and
(8)  with u1 ≥ umin

. Let Πe be the exploration policy and ǫ ≥ 0. Then either (a) bV (E∞; Θ) ≥
bV (E∞; Θ∗) − ǫ  or (b) the probability that the auxiliary RPR suggests executing Πe in some episode

∞ is at least ǫ(1−γ)
Rmax

unseen in E (K)

1

.

Proof:

Pexlpore(E (\K)

∞   bV (E (K)

= E∞ \ E (K)

∞   Θ  λ  Πe) ≥

∞ ; Θ∗) which  together with Lemma 4.1  implies

It is sufﬁcient to show that if (a) does not hold  then (b) must hold. Let us assume
∞ ; Θ∗)  which
∞ . We show below that

∞ ; Θ∗) − ǫ. where E (\K)

∞ ; Θ) < bV (E (\K)
∞ ; Θ  λ  Πe) ≥ bV (E (\K)

∞ ; Θ) ≥ bV (E (K)
bV (E∞; Θ) < bV (E∞; Θ∗) − ǫ. Because Θ is optimal in E (K)
implies bV (E (\K)
bVf (E (\K)
∞ ; Θ)i
Rmax hbVf (E (\K)
∞ ; Θ  λ  Πe) − bV (E (\K)
∞ ; Θ)i ≥
Rmax hbV (E (\K)
∞ ; Θ∗) − bV (E (\K)
∞ ; Θ∗). By construction  bVf (E (\K)

We now show bVf (E (\K)

∞ ; Θ  λ  Πe) ≥ bV (E (\K)

∞ ; Θ  λ  Πe) is an
optimistic value function  in which the agent receives Rmax at any time t unless if yτ = 0 at τ =
0  1  · · ·   t. However  yτ = 0 at τ = 0  1  · · ·   t implies that {hτ : τ = 0  1  · · ·   t} ⊂ Hknown  By
the premise  λ is updated according to (7) and (8) and u1 ≥ umin
  therefore Θ is optimal in Hknown
(see the discussions following (7) and (8))  which implies Θ is optimal in {hτ : τ = 0  1  · · ·   t}.
Thus  the inequality holds.
Q.E.D.

ǫ(1 − γ)

Rmax

1 − γ

1 − γ

≥

∞

1

Proposition 4.2 shows that whenever the primary RPR achieves less accumulative reward than
the optimal RPR by ǫ 
the auxiliary RPR suggests exploration with a probability exceeding
max. Conversely  whenever the auxiliary RPR suggests exploration with a probability
ǫ(1 − γ)R−1
smaller than ǫ(1 − γ)R−1
max  the primary RPR achieves ǫ-near optimality. This ensures that the agent
is either receiving sufﬁcient rewards or it is performing sufﬁcient exploration.

5 Experimental Results

Our experiments are based on Shuttle  a benchmark POMDP problem [7]  with the following setup.
The primary policy is a RPR with |Z| = 10 and a prior in (4)  with all hyper-parameters initially
set to one (which makes the initial prior non-informative). The auxiliary policy is a RPR sharing
{µ  W } with the primary RPR and having a prior for λ as in (6). The prior of λ is initially biased
towards exploration by using u0 = 1 and u1 > 1. We consider various values of u1 to examine
the different effects. The agent performs online learning: upon termination of each new episode 
the primary and auxiliary RPR posteriors are updated by using the previous posteriors as the current
priors. The primary RPR update follows Theorem 2.4 with K = 1 while the auxiliary RPR update
follows (7) and (8) for λ (it shares the same update with the primary RPR for µ and W ). We perform
100 independent Monte Carlo runs. In each run  the agent starts learning from a random position
in the environment and stops learning when Ktotal episodes are completed. We compare various
methods that the agent uses to balance exploration and exploitation: (i) following the auxiliary RPR 
with various values of u1  to adaptively switch between exploration and exploitation; (ii) randomly
switching between exploration and exploitation with a ﬁxed exploration rate Pexplore (various values
of Pexplore are examined). When performing exploitation  the agent follows the current primary RPR
(using the Θ that maximizes the posterior); when performing exploration  it follows an exploration
policy Πe. We consider two types of Πe: (i) taking random actions and (ii) following the policy
obtained by solving the true POMDP using PBVI [8] with 2000 belief samples. In either case 
rmeta = 1 and η = 0.001. In case (ii)  the PBVI policy is the oracle and incurs a query cost c.
We report: (i) the sum of discounted rewards accrued within each episode during learning; these
rewards result from both exploitation and exploration. (ii) the quality of the primary RPR upon
termination of each learning episode  represented by the sum of discounted rewards averaged over
251 episodes of following the primary RPR (using the standard testing procedure for Shuttle: each
episode is terminated when either the goal is reached or a maximum of 251 steps is taken); these
rewards result from exploitation alone. (iii) the exploration rate Pexplore in each learning episode 
which is the number of time steps at which exploration is performed divided by the total time steps in

6

a given episode. In order to examine the optimality  the rewards in (i)-(ii) has the corresponding op-
timal rewards subtracted  where the optimal rewards are obtained by following the PBVI policy; the
difference are reported  with zero difference indicating optimality and minus difference indicating
sub-optimality. All results are averaged over the 100 Monte Carlo runs. The results are summarized
in Figure 1 when Πe takes random actions and in Figure 2 when Πe is an oracle (the PBVI policy).

 
 
 
 
 
 

d
r
a
w
e
r
 

i

g
n
n
r
a
e

l
 

d
e
u
r
c
c
A

 
 
 
 
 
 
 
 
 

d
r
a
w
e
r
 
l

a
m

i
t

p
o

 
s
u
n
m

i

0

−2

−4

−6

−8

−10

−12

−14

−16
0

 

 

Dual−RPR  u
=2
1
Dual−RPR  u
=20
1
Dual−RPR  u
=200
1
 = 0
RPR  P
 = 0.1
RPR  P
 = 0.3
RPR  P
RPR  P
 = 1.0

explore

explore

explore

explore

 
 
 
 
 
 

d
r
a
w
e
r
 

g
n

i
t
s
e

t
 

d
e
u
r
c
c
A

 
 
 
 
 
 
 
 

d
r
a
w
e
r
 
l

a
m

i
t

p
o

 
s
u
n
m

i

500

1000

1500

2000

2500

3000

Number of episodes used in the learning phase

0

−2

−4

−6

−8

−10

 

−12
0

 

Dual−RPR  u
=2
1
Dual−RPR  u
=20
1
Dual−RPR  u
=200
1
 = 0
RPR  P
 = 0.1
RPR  P
 = 0.3
RPR  P
RPR  P
 = 1.0

explore

explore

explore

explore
2500

500

1000

1500

2000

Number of episodes used in the learning phase

3000

e

t

a
r
 

n
o

i
t

l

a
r
o
p
x
E

1

0.8

0.6

0.4

0.2

 

0
0

 

Dual−RPR  u
=2
1
Dual−RPR  u
=20
1
Dual−RPR  u
=200
1

500

1000

1500

2000

2500

3000

Number of episodes used in the learning phase

Figure 1: Results on Shuttle with a random exploration policy  with Ktotal = 3000. Left: accumulative
discounted reward accrued within each learning episode  with the corresponding optimal reward subtracted.
Middle: accumulative discounted rewards averaged over 251 episodes of following the primary RPR obtained
after each learning episode  again with the corresponding optimal reward subtracted. Right: the rate of explo-
ration in each learning episode. All results are averaged over 100 independent Monte Carlo runs.

20

40

60

Number of episodes used in the learning phase

0

−5

−10

−15

 

−20
0

0

−5

−10

−15

 

 
 
 
 
 
 

d
r
a
w
e
r
 

g
n

i
t
s
e

t
 

d
e
u
r
c
c
A

 
 
 
 
 
 
 
 

d
r
a
w
e
r
 
l

a
m

i
t

p
o

 
s
u
n
m

i

100

 

 
 
 
 
 
 

d
r
a
w
e
r
 

g
n

i
t
s
e

t
 

d
e
u
r
c
c
A

 
 
 
 
 
 
 
 

d
r
a
w
e
r
 
l

a
m

i
t

p
o

 
s
u
n
m

i

Dual−RPR  u
=2
1
Dual−RPR  u
=10
1
Dual−RPR  u
=20
1
 = 0.158
RPR  P
 = 0.448
RPR  P
 = 0.657
RPR  P
RPR  P
 = 1.0

explore

explore

explore

explore
80

Dual−RPR  u
=2
1
Dual−RPR  u
=10
1
Dual−RPR  u
=20
1
 = 0.081
RPR  P
 = 0.295
RPR  P
 = 0.431
RPR  P
RPR  P
 = 1.0

explore

explore

explore

 
 
 
 
 
 

d
r
a
w
e
r
 

i

g
n
n
r
a
e

l
 

d
e
u
r
c
c
A

 
 
 
 
 
 
 
 
 

d
r
a
w
e
r
 
l

a
m

i
t

p
o

 
s
u
n
m

i

 
 
 
 
 
 

d
r
a
w
e
r
 

i

g
n
n
r
a
e

l
 

d
e
u
r
c
c
A

 
 
 
 
 
 
 
 
 

d
r
a
w
e
r
 
l

a
m

i
t

p
o

 
s
u
n
m

i

0

−2

−4

−6

−8

−10

 

−12
0

0

−2

−4

−6

−8

−10

 

−12
0

Dual−RPR  u
=2
1
Dual−RPR  u
=10
1
Dual−RPR  u
=20
1
 = 0.158
RPR  P
 = 0.448
RPR  P
 = 0.657
RPR  P
RPR  P
 = 1.0

explore

explore

explore

explore

20

40

60

80

Number of episodes used in the learning phase

 

1

0.8

0.6

0.4

0.2

e

t

a
r
 

n
o

i
t

l

a
r
o
p
x
E

100

 

 

0
0

1

 

Dual−RPR  u
=2
1
Dual−RPR  u
=10
1
Dual−RPR  u
=20
1

20

40

60

80

Number of episodes used in the learning phase

100

 

Dual−RPR  u
=2
1
Dual−RPR  u
=10
1
Dual−RPR  u
=20
1

e

t

a
r
 

n
o

i
t

l

a
r
o
p
x
E

0.8

0.6

0.4

0.2

Dual−RPR  u
=2
1
Dual−RPR  u
=10
1
Dual−RPR  u
=20
1
 = 0.081
RPR  P
 = 0.295
RPR  P
 = 0.431
RPR  P
RPR  P
 = 1.0

explore

explore

explore

explore
80

20

40

60

Number of episodes used in the learning phase

explore
80

100

 

−20
0

20

40

60

Number of episodes used in the learning phase

100

 

0
0

20

40

60

80

100

Number of episodes used in the learning phase

Figure 2: Results on Shuttle with an oracle exploration policy incurring cost c = 1 (top row) and c = 3
(bottom row)  and Ktotal = 100. Each ﬁgure in a row is a counterpart of the corresponding ﬁgure in Figure 1 
with the random Πe replaced by the oracle Πe. See the captions there for details.

It is seen from Figure 1 that  with random exploration and u1 = 2  the primary policy converges
to optimality and  accordingly  Pexplore drops to zero  after about 1500 learning episodes. When
u1 increases to 20  the convergence is slower:
it does not occur (and Pexplore > 0) until after
abound 2500 learning episodes. With u1 increased to 200  the convergence does not happen and
Pexplore > 0.2 within the ﬁrst 3000 learning episodes. These results verify our analysis in Section
3 and 4: (i) the primary policy improves as Pexplore decreases; (ii) the agent explores when it is
not acting optimally and it is acting optimally when it stops exploring; (iii) there exists ﬁnite u1
such that the primary policy is optimal if Pexplore = 0. Although u1 = 2 may still be larger than
  it is small enough to ensure convergence within 1500 episodes. We also observe from Figure
umin
1 that: (i) the agent explores more efﬁciently when it is adaptively switched between exploration
and exploitation by the auxiliary policy  than when the switch is random; (ii) the primary policy
cannot converge to optimality when the agent never explores; (iii) the primary policy may converge

1

7

to optimality when the agent always takes random actions  but it may need inﬁnite learning episodes
to converge.
The results in Figure 2  with Πe being an oracle  provide similar conclusions as those in Figure
1 when Πe is random. However  there are two special observations from Figure 2: (i) Pexplore is
affected by the query cost c: with a larger c  the agent performs less exploration. (ii) the convergence
rate of the primary policy is not signiﬁcantly affected by the query cost. The reason for (ii) is that the
oracle always provides optimal actions  thus over-exploration does not harm the optimality; as long
as the agent takes optimal actions  the primary policy continually improves if it is not yet optimal 
or it remains optimal if it is already optimal.

6 Conclusions

We have presented a dual-policy approach for jointly learning the agent behavior and the optimal
balance between exploitation and exploration  assuming the unknown environment is a POMDP. By
identifying a known part of the environment in terms of histories (parameterized by the RPR)  the ap-
proach adaptively switches between exploration and exploitation depending on whether the agent is
in the known part. We have provided theoretical guarantees for the agent to either explore efﬁciently
or exploit efﬁciently. Experimental results show good agreement with our theoretical analysis and
that our approach ﬁnds the optimal policy efﬁciently. Although we empirically demonstrated the
existence of a small u1 to ensure efﬁcient convergence to optimality  further theoretical analysis is
  the tight lower bound of u1  which ensures convergence to optimality with
needed to ﬁnd umin
just the right amount of exploration (without over-exploration). Finding the exact umin
is difﬁcult
because of the partial observability. However  it is hopeful to ﬁnd a good approximation to umin
. In
the worst case  the agent can always choose to be optimistic  like in E3 and Rmax. An optimistic
agent uses a large u1  which usually leads to over-exploration but ensures convergence to optimality.

1

1

1

The authors would like to thank the anonymous reviewers for their valuable comments and sugges-
tions. This work is supported by AFOSR.

Proof of Lemma 4.1: We expand (10) as 

Vf (E (K)

T

; Θ  λ  Πe) =

T

t=0γtrtp(a0:t  o1:t  rt|y0:t = 0  Θ  M )p(y0:t = 0|Θ  λ)

(K)
T

T

t=0 γtRmax

y0:t6=0p(a0:t  o1:t  rt|y0:t  Θ  M  Πe)p(y0:t|Θ  λ)

where y0:t is an an abbreviation for yτ = 0 ∀ τ = 0  · · ·   t and y0:t 6= 0 is an an abbreviation for ∃ 0 ≤ τ ≤ t
satisfying yτ 6= 0. The sum

. The difference between (9) and (11) is

is over all episodes in E (K)

V (E (K)
|

  Θ) −

V (E (K)

T

; Θ  λ)| =

t=0γtrtp(a0:t  o1:t  rt|y0:t = 0  Θ  M )(1 − p(y0:t = 0|Θ  λ))

E

(K)
T

T

t=0 γtRmax

y0:t6=0p(a0:t  o1:t  rt|y0:t  Θ  M  Πe)p(y0:t|Θ  λ)

T

t=0 γtrtp(a0:t  o1:t  rt|y0:t = 0  Θ  M )

y0:t6=0p(y0:t|Θ  λ)

T

t=0 γtRmax

y0:t6=0p(a0:t  o1:t  rt|y0:t  Θ  M  Πe)p(y0:t|Θ  λ)

γtrt

y0:t6=0

p(a0:t  o1:t  rt|y0:t = 0  Θ  M ) −

p(a0:t  o1:t  rt|y0:t  Θ  M  Πe)

p(y0:t|Θ  λ)

T

t=0 γtRmax

y0:t6=0p(y0:t|Θ  λ) =

T

t=0γtRmax(1 − p(y0:t = 0|Θ  λ))

(1 − p(y0:T = 0|Θ  λ))

T

t=0γtRmax ≤

(1 − p(y0:T = 0|Θ  λ))

E

(K)
T

where

y0:t6=0 is a sum over all sequences {y0:t : ∃ 0 ≤ τ ≤ t satisfying yτ 6= 0}.

Q.E.D.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)

i






T

7 Acknowledgements

E

+









(K)
T

(K)
T

E

E

E

(K)
T

(K)
T

E



(cid:12)(cid:12)(cid:12)


b
X

h

−

−

Appendix

b
b
(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)X
TX





(K)
T

(K)
T

(K)
T

(K)
T

t=0

T

E

E

E

E

=

=

≤

≤





8

T




rt

Rmax

(K)
T

E
Rmax
1 − γ

References
[1] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis  Gatsby Computa-

tional Neuroscience Unit  Univertisity College London  2003.

[2] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal rein-

forcement learning. Journal of Machine Learning Research  3(OCT):213–231  2002.

[3] F. Doshi  J. Pineau  and N. Roy. Reinforcement learning with limited reinforcement: Using Bayes risk for
active learning in POMDPs. In Proceedings of the 25th international conference on Machine learning 
pages 256–263. ACM  2008.

[4] M. Kearns and D. Koller. Efﬁcient reinforcement learning in factored mdps. In Proc. of the Sixteenth

International Joint Conference of Artiﬁcial Intelligence  pages 740–747  1999.

[5] M. Kearns and S. P. Singh. Near-optimal performance for reinforcement learning in polynomial time. In

Proc. ICML  pages 260–268  1998.

[6] H. Li  X. Liao  and L. Carin. Multi-task reinforcement learning in partially observable stochastic envi-

ronments. Journal of Machine Learning Research  10:1131–1186  2009.

[7] M.L. Littman  A.R. Cassandra  and L.P. Kaelbling. Learning policies for partially observable environ-

ments: scaling up. In ICML  1995.

[8] J. Pineau  G. Gordon  and S. Thrun. Point-based value iteration: An anytime algorithm for POMDPs. In
Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI)  pages 1025
– 1032  August 2003.

[9] P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains.

In International Symposiu on Artiﬁcial Intelligence and Mathmatics (ISAIM)  2008.

[10] R. Sutton and A. Barto. Reinforcement learning: An introduction. MIT Press  Cambridge  MA  1998.

9

,Miao Xu
Rong Jin
Zhi-Hua Zhou
Jun Shu
Qi Xie
Lixuan Yi
Qian Zhao
Sanping Zhou
Zongben Xu
Deyu Meng