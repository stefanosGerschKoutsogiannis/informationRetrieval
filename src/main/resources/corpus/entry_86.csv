2019,A General Framework for Symmetric Property Estimation,In this paper we provide a general framework for estimating symmetric properties of distributions from i.i.d. samples. For a broad class of symmetric properties we identify the  {\em easy} region where empirical estimation works and the {\em difficult} region where more complex estimators are required. We show that by approximately computing the profile maximum likelihood (PML) distribution \cite{ADOS16} in this difficult region we obtain a symmetric property estimation framework that is sample complexity optimal for many properties in a broader parameter regime than previous universal estimation approaches based on PML. The resulting algorithms based on these \emph{pseudo PML distributions} are also more practical.,A General Framework for Symmetric Property

Estimation

Moses Charikar
Stanford University

moses@cs.stanford.edu

Kirankumar Shiragur

Stanford University

shiragur@stanford.edu

Aaron Sidford

Stanford University

sidford@stanford.edu

Abstract

In this paper we provide a general framework for estimating symmetric properties
of distributions from i.i.d. samples. For a broad class of symmetric properties we
identify the easy region where empirical estimation works and the difﬁcult region
where more complex estimators are required. We show that by approximately
computing the proﬁle maximum likelihood (PML) distribution [ADOS16] in this
difﬁcult region we obtain a symmetric property estimation framework that is
sample complexity optimal for many properties in a broader parameter regime than
previous universal estimation approaches based on PML. The resulting algorithms
based on these pseudo PML distributions are also more practical.

1

Introduction

Symmetric property estimation is a fundamental and well studied problem in machine learning
and statistics. In this problem  we are given n i.i.d samples from an unknown distribution1 p and
asked to estimate f(p)  where f is a symmetric property (i.e. it does not depend on the labels of the
symbols). Over the past few years  the computational and sample complexities for estimating many
symmetric properties have been extensively studied. Estimators with optimal sample complexities
have been obtained for several properties including entropy [VV11b  WY16a  JVHW15]  distance to
uniformity [VV11a  JHW16]  and support [VV11b  WY15].
All aforementioned estimators were property speciﬁc and therefore  a natural question is to design
a universal estimator. In [ADOS16]  the authors showed that the distribution that maximizes the
proﬁle likelihood  i.e. the likelihood of the multiset of frequencies of elements in the sample  referred
to as proﬁle maximum likelihood (PML) distribution  can be used as a universal plug-in estimator.
[ADOS16] showed that computing the symmetric property on the PML distribution is sample
complexity optimal in estimating support  support coverage  entropy and distance to uniformity
within accuracy ✏>
n0.2499 . Further  this also holds for distributions that approximately optimize the
PML objective with the approximation factor affecting the values of ✏ for which it holds.
Acharya et al. [ADOS16] posed two important and natural open questions. The ﬁrst was to give
an efﬁcient algorithm for ﬁnding an approximate PML distribution  which was recently resolved
in [CSS19]. The second open question is whether PML is sample competitive in all regimes of the
accuracy parameter ✏? In this work  we make progress towards resolving this open question.
Firstly  we show that the PML distribution based plug-in estimator achieves optimal sample complex-
ity for all ✏ for the problem of estimating support size. Next  we introduce a variation of the PML
distribution that we call the pseudo PML distribution. Using this  we give a general framework for
estimating a symmetric property. For entropy and distance to uniformity  this pseudo PML based
framework achieves optimal sample complexity for a broader regime of the accuracy parameter than
was known for the vanilla PML distribution.

1

1Throughout the paper  distribution refers to discrete distribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We provide a general framework that could  in principle be applied to estimate any separable

symmetric property f  meaning f(p) can be written in the form ofPx2D f(px). This motivation
behind this framework is: for any symmetric property f that is separable  the estimate for f(p)
can be split into two parts: f(p) = Px2B f(px) +Px2G f(px)  where B and G are a (property
dependent) disjoint partition of the domain D. We refer to G as the good set and B as the bad set.
Intuitively  G is the subset of domain elements whose contribution to f(p) is easy to estimate  i.e
a simple estimator such as empirical estimate (with correction bias) works. For many symmetric
properties  ﬁnding an appropriate partition of the domain is often easy. Many estimators in the
literature [JVHW15  JHW16  WY16a] make such a distinction between domain elements. The more

the work in these estimators is dedicated towards estimating this contribution using sophisticated
techniques such as polynomial approximation. Our work gives a uniﬁed approach to estimating the

interesting and difﬁcult case is estimating the contribution of the bad set: Px2B f(px). Much of
contribution of the bad set. We propose a PML based estimator for estimatingPx2B f(px). We
show that computing the PML distribution only on the set B is sample competitive for entropy and
distance to uniformity for almost all interesting parameter regimes thus (partially) handling the open
problem proposed in [ADOS16]. Additionally  requiring that the PML distribution be computed on a
subset B ✓D reduces the input size for the PML subroutine and results in practical algorithms (See
Section 6).
To summarize  the main contributions of our work are:

parameter ✏ that one can obtain for universal symmetric property estimation via PML.

• We make progress on an open problem of [ADOS16] on broadening the range of error
• We give a general framework for applying PML to new symmetric properties.
• As a byproduct of our framework  we obtain more practical algorithms that invoke PML on

smaller inputs (See Section 6).

1.1 Related Work

For many natural properties  there has been extensive work on designing efﬁcient estimators both
with respect to computational time and sample complexity [HJWW17  HJM17  AOST14  RVZ17 
ZVV+16  WY16b  RRSS07  WY15  OSW16  VV11b  WY16a  JVHW15  JHW16  VV11a]. We
deﬁne and state the optimal sample complexity for estimating support  entropy and distance to
uniformity. For entropy  we also discuss the regime in which the empirical distribution is sample
optimal.

1

N

✏ ) [WY16a]. Further if ✏< log N

Entropy: For any distribution p 2 D  the entropy H(p) def= Px2D px log px. For ✏  log N
(the interesting regime)  where N def= |D|  the optimal sample complexity for estimating H(p)
within additive accuracy ✏ is O( N
N   then [WY16a] showed that
log N
empirical distribution is optimal.
Distance to uniformity: For any distribution p 2 D  the distance to uniformity kp  uk1
Px2D |px  1
estimating kp  uk1 within additive accuracy ✏ is O( N
Support: For any distribution p 2 D  the support of distribution S(p) def= |{x 2D | px > 0}|.
Estimating support is difﬁcult in general because we need sufﬁciently large number of samples to
observe elements with small probability values. Suppose for all x 2D   if px 2{ 0}[ [ 1
k   1]  then
[WY15] showed that the optimal sample complexity for estimating support within additive accuracy
✏k is O( k

def=
N |  where u is the uniform distribution over D. The optimal sample complexity for

log N

1

✏2 ) [VV11a  JHW16].

log k log2 1
✏ ).

PML was introduced by Orlitsky et al. [OSS+04] in 2004. The connection between PML and
universal estimators was ﬁrst studied in [ADOS16]. As discussed in the introduction  PML based
plug-in estimator applies to a restricted regime of error parameter ✏. There have been several other
approaches for designing universal estimators for symmetric properties. Valiant and Valiant [VV11b]
adopted and rigorously analyzed a linear programming based approach for universal estimators
proposed by [ET76] and showed that it is sample complexity optimal in the constant error regime
for estimating certain symmetric properties (namely  entropy and support size). Recent work of Han
et al. [HJW18] applied a local moment matching based approach in designing efﬁcient universal

2

symmetric property estimators for a single distribution. [HJW18] achieves the optimal sample
complexity in restricted error regimes for estimating the power sum function  support and entropy.
Recently  [YOSW18] gave a different uniﬁed approach to property estimation. They devised an
estimator that uses n samples and achieves the performance attained by the empirical estimator with
nplog n samples for a wide class of properties and for all underlying distributions. This result is
further strengthened to n log n samples for Shannon entropy and a broad class of other properties
including `1-distance in [HO19b].
Independently of our work  authors in [HO19a] propose truncated PML that is slightly different but
similar in the spirit to our idea of pseudo PML; refer [HO19a] for further details.

1.2 Organization of the Paper
In Section 2 we provide basic notation and deﬁnitions. We present our general framework in Section 3
and state all our main results. In Section 4  we provide proofs of the main results of our general
framework. In Section 5  we use these results to establish the sample complexity of our estimator
in the case of entropy (See Section 5.1) and distance to uniformity (See Section 5.2). Due to
space constraints  many proofs are deferred to the appendix. In Section 6  we provide experimental
results for estimating entropy using pseudo PML and other state-of-the-art estimators. Here we also
demonstrate the practicality of our approach.

2 Preliminaries
Let [a] denote all integers in the interval [1  a]. Let D ⇢ [0  1]DR be the set of all distributions
supported on domain D and let N be the size of the domain. Throughout this paper we restrict our
attention to discrete distributions and assume that we receive a sequence of n independent samples
from an underlying distribution p 2 D. Let Dn be the set of all length n sequences and yn 2D n
be one such sequence with yn
i denoting its ith element. The probability of observing sequence yn is:

P(p  yn) def= Yx2D

pf(yn x)
x

where f(yn  x) = |{i 2 [n] | yn
i = x}| is the frequency/multiplicity of symbol x in sequence yn and
px is the probability of domain element x 2D . We next formally deﬁne proﬁle  PML distribution
and approximate PML distribution.
+ is  def=
Deﬁnition 2.1 (Proﬁle). For a sequence yn 2D n  its proﬁle denoted  =( yn) 2 Zn
((j))j2[n] where (j) def= |{x 2D| f(yn  x) = j}| is the number of domain elements with frequency
j in yn. We call n the length of proﬁle  and use n denote the set of all proﬁles of length n. 2
For any distribution p 2 D  the probability of a proﬁle  2 n is deﬁned as:

P(p  yn)

(1)

P(p  ) def=

X{yn2Dn | (yn)=}

The distribution that maximizes the probability of a proﬁle  is the proﬁle maximum likelihood
distribution and we formally deﬁne it next.
Deﬁnition 2.2 (Proﬁle maximum likelihood distribution). For any proﬁle  2 n  a Proﬁle Maximum
Likelihood (PML) distribution ppml  2 D is: ppml  2 arg maxp2D P(p  ) and P(ppml   ) is
the maximum PML objective value. Further  a distribution p
pml  2 D is a -approximate PML
distribution if P(p
We next provide formal deﬁnitions for separable symmetric property and an estimator.
Deﬁnition 2.3 (Separable Symmetric Property). A symmetric property f : D ! R is separable
if for any p 2 D  f (p) def= Px2D g(px)  for some function g : R ! R. Further for any subset
S ⇢D   we deﬁne fS(p) def= Px2S g(px).

2The proﬁle does not contain (0)  the number of unseen domain elements.

pml   )   · P(ppml   ).

3

Deﬁnition 2.4. A property estimator is a function ˆf : Dn ! R  that takes as input n samples and
returns the estimated property value. The sample complexity of ˆf for estimating a symmetric property
f(p) is the number of samples needed to estimate f up to accuracy ✏ and with constant probability.
The optimal sample complexity of a property f is the minimum number of samples of any estimator.

3 Main Results

As discussed in the introduction  one of our motivations was to provide a better analysis for the PML
distribution based plug-in estimator. In this direction  we ﬁrst show that the PML distribution is
sample complexity optimal in estimating support in all parameter regimes. Estimating support is
difﬁcult in general and all previous works make the assumption that the minimum non-zero probability
value of the distribution is at least 1
k . In our next result  we show that the PML distribution under this
constraint is sample complexity optimal for estimating support.
Theorem 3.1. The PML distribution 3 based plug-in estimator is sample complexity optimal in
estimating support for all regimes of error parameter ✏.

For support  we show that an approximate PML distribution is sample complexity optimal as well.
Theorem 3.2. For any constant ↵> 0  an exp(✏2n1↵)-approximate PML distribution 3 based
plug-in estimator is sample complexity optimal in estimating support for all regimes of error ✏.

We defer the proof of both these theorems to Appendix A.
For entropy and distance to uniformity  we study a variation of the PML distribution we call the
pseudo PML distribution and present a general framework for symmetric property estimation based
on this. We show that this pseudo PML based general approach gives an estimator that is sample
complexity optimal for estimating entropy and distance to uniformity in broader parameter regimes.
To motivate and understand this general framework we ﬁrst deﬁne new generalizations of the proﬁle 
PML and approximate PML distributions.
Deﬁnition 3.3 (S-pseudo Proﬁle). For any sequence yn 2D n and S ✓D   its S-pseudo proﬁle
denoted S = S(yn) is  def= (S(j))j2[n] where S(j) def= |{x 2 S | f(yn  x) = j}| is the number
of domain elements in S with frequency j in yn. We call n the length of S as it represents the length
of the sequence yn from which this pseudo proﬁle was constructed. Let n
S denote the set of all
S-pseudo proﬁles of length n.
For any distribution p 2 D  the probability of a S-pseudo proﬁle S 2 n

S is deﬁned as:

P(p  S) def=

P(p  yn)

(2)

X

{yn2Dn | S (yn)=S}

P⇣|fS(p)  ˆf(S)| ✏⌘   

4

We next deﬁne the S-pseudo PML and (  S )-approximate pseudo PML distributions that are
analogous to the PML and approximate PML distributions.
Deﬁnition 3.4 (S-pseudo PML distribution). For any S-pseudo proﬁle S 2 n
pS 2 D is a S-pseudo PML distribution if pS 2 arg maxp2D P(p  S).
Deﬁnition 3.5 ((  S )-approximate pseudo PML distribution). For any proﬁle S 2 n
tion p

S 2 D is a (  S )-approximate pseudo PML distribution if P(p

  S)   · P(p

S  a distribution

S  a distribu-
  S).

S

S

For notational convenience  we also deﬁne the following function.
Deﬁnition 3.6. For any subset S ✓D   the function Freq : n
and returns the set with all distinct frequencies in S.
Using the deﬁnitions above  we next give an interesting generalization of Theorem 3 in [ADOS16].
Theorem 3.7. For a symmetric property f and S ✓D   suppose there is an estimator ˆf : n
S ! R 
such that for any p and S ⇠ p the following holds 

S ! 2Z+ takes input a S-psuedo proﬁle

3 Under the constraint that its minimum non-zero probability value is at least 1

k . This assumption is also

necessary for the results in [ADOS16] to hold.

then for any F 2 2Z+  a (  S )-approximate pseudo PML distribution p

S satisﬁes:

P⇣|fS(p)  fS(p

S

)| 2✏⌘ 

n|F|
 P (Freq (S) ✓ F) + P (Freq (S) 6✓ F) .

Note that in the theorem above  the error probability with respect to a pseudo PML distribution based
estimator has dependency on n|F|
and P (Freq (S) 6✓ F). However Theorem 3 in [ADOS16] has

error probability epn
 . This is the bottleneck in showing that PML works for all parameter regimes
and the place where pseudo PML wins over the vanilla PML based estimator  getting non-trivial
results for entropy and distance to uniformity. We next state our general framework for estimating
symmetric properties. We use the idea of sample splitting which is now standard in the literature
[WY16a  JVHW15  JHW16  CL11  Nem03].

1   xn

Algorithm 1 General Framework for Symmetric Property Estimation
1: procedure PROPERTY ESTIMATION(x2n  f  F)
Let x2n = (xn
2 )  where xn
2:
Deﬁne S def= {y 2D | f (xn
3:
Construct proﬁle S  where S(j) def= |{y 2 S | f(xn
4:
Find a (  S )-approximate pseudo PML distribution p
5:
return fS(p
6:
S
7: end procedure

1 and xn
1   y) 2 F}.

2   y) = j}|.
S and empirical distribution ˆp on xn
2 .
) + f ¯S(ˆp) + correction bias with respect to f ¯S(ˆp).

2 represent ﬁrst and last n samples of x2n respectively.

Then for any sequence x2n = (xn

) (recall p
) is the property value of distribution p

In the above general framework  the choice of F depends on the symmetric property of interest.
Later  in the case of entropy and distance to uniformity  we will choose F to be the region where
the empirical estimate fails; it is also the region that is difﬁcult to estimate. One of the important
properties of the above general framework is that fS(p
S is a (  S )-approximate pseudo
S
PML distribution and fS(p
S on subset of domain elements
S
S ✓D ) is close to fS(p) with high probability. Below we state this result formally.
Theorem 3.8. For any symmetric property f  let G ✓D and F  F0 2 2Z+. If for all S0 2 2G  there
exists an estimator ˆf : n
P⇣|fS0(p)  ˆf(S0)| 2✏⌘   and P (Freq (S0) 6✓ F0)  .
P⇣|fS(p)  fS(p
+  + PS /2 2G  

S0 ! R  such that for any p and S0 ⇠ p satisﬁes 

where S is a random set S def= {y 2D | f (xn
Using the theorem above  we already have a good estimate for fS(p) for appropriately chosen
frequency subsets F  F0 and G ✓D . Further  we choose these subsets F  F0 and G carefully so that
the empirical estimate f ¯S(ˆp) plus the correction bias with respect to f ¯S is close to f ¯S(p). Combining
these together  we get the following results for entropy and distance to uniformity.
N 1↵⌘ for any constant ↵> 0  then for estimating entropy 
Theorem 3.9. If error parameter ✏> ⌦⇣ log N

the estimator 1 for  = n log n is sample complexity optimal.
For entropy  we already know from [WY16a] that the empirical distribution is sample complexity
optimal if ✏< c log N
N for some constant c > 0. Therefore the interesting regime for entropy estimation

n|F 0|

)| > 4✏⌘ 
1   y) 2 F} and S

def= S(xn
2 ).

2 ) 
1   xn

(3)

S

N ⌘ and our estimator works for almost all such ✏.

is when ✏> ⌦⇣ log N
Theorem 3.10. Let ↵> 0 and error parameter ✏> ⌦
uniformity  the estimator 1 for  = np n log n
Note that the estimator in [JHW17] also requires that the error parameter ✏  1
some constant.

is sample complexity optimal.

N 18↵  then for estimating distance from

N C   where C > 0 is

1

N



5

4 Analysis of General Framework for Symmetric Property Estimation

S

Here we provide proofs of the main results for our general framework (Theorem‘3.7 and 3.8). These
results weakly depend on the property and generalize results in [ADOS16]. The PML based estimator
in [ADOS16] is sample competitive only for a restricted error parameter regime and this stems from
the large number of possible proﬁles of length n. Our next lemma will be useful to address this issue
and later we show how to use this result to prove Theorems 3.7 and 3.8.
Lemma 4.1. For any subset S ✓D and F 2 2Z+  if set B is deﬁned as B def= {S 2
S | Freq (S) ✓ F}  then the cardinality of set B is upper bounded by (n + 1)|F|.
n
Proof of Theorem 3.7. Using the law of total probability we have 
P⇣|fS(p)  fS(p

)| 2✏  Freq (S) ✓ F⌘
)| 2✏  Freq (S) 6✓ F⌘  
)| 2✏  Freq (S) ✓ F⌘ + P (Freq (S) 6✓ F) .
(S) > . For   1  we have
)  ˆf(S)| ✏.
)ˆf(S)| 2✏. Note
S | Freq (S) ✓ F and |fS(p) 
)| 2✏}. From the previous discussion  we get p (S)  / for all S 2 BF S ˆf. Therefore 
(n + 1)|F| .

Consider any S ⇠ p. If p (S) > /  then we know that p
p (S) > that implies |fS(p)  ˆf(S)| ✏. Further p
Using triangle inequality we get  |fS(p)fS(p
we wish to upper bound the probability of set: BF S ˆf
fS(p
S

)| 2✏⌘ = P⇣|fS(p)  fS(p
+ P⇣|fS(p)  fS(p
 P⇣|fS(p)  fS(p

)|| fS(p)ˆf(S)|+|fS(p

(S) > implies |fS(p

def= {S 2 n

S

S

S

S

S

S

S

S

p (S) 


|BF S ˆf|




S | Freq (S) ✓ F} and invoke Lemma 4.1.

S

)| 2✏  Freq (S) ✓ F⌘ = XS2BF S ˆf

P⇣|fS(p)  fS(p
In the ﬁnal inequality  we use BF S ˆf ✓{ S 2 n
Proof for Theorem 3.8. Using Bayes rule we have:
P⇣|fS(p)  fS(p

S

S

S

In

the

second

)| > 2✏⌘ = XS0✓D
 XS022G

)| > 2✏ | S = S0⌘ P (S = S0)
)| > 2✏ | S = S0⌘ P (S = S0) + PS /2 2G .

P⇣|fS(p)  fS(p
P⇣|fS(p)  fS(p
use PS0 /22G P⇣|fS(p)  fS(p
is upper bounded by  PS022G P⇣|fS0(p)  fS0(p
 + P (Freq (S0) 6✓ F0)i P (S = S0) 
)| > 2✏⌘.
)| > 2✏ | S = S0⌘ = P⇣|fS0(p)  fS0(p

PS /2 2G.
PS022Gh n|F0|
P⇣|fS(p)  fS(p
we usePS022G P (S = S0)  1 and P (Freq (S) 6✓ F0  S = S0)  . The theorem follows by

In the ﬁrst up-
per bound  we removed randomness associated with the random set S and used
In the ﬁrst inequal-
ity above  we invoke Theorem 3.7 using conditions from Equation (3). In the second inequality 

)| > 2✏  S = S0⌘
)| > 2✏⌘ P (S = S0) 

(4)

S
the above expression and

inequality  we
the ﬁrst

combining all the analysis together.

term on the right side of

S0
+ .

note that

Consider

n|F0|

S0

S

it



5 Applications of the General Framework

Here we provide applications of our general framework (deﬁned in Section 3) using results from the
previous section. We apply our general framework to estimate entropy and distance to uniformity. In
Section 5.1 and Section 5.2 we analyze the performance of our estimator for entropy and distance to
uniformity estimation respectively.

6

5.1 Entropy estimation

In order to prove our main result for entropy (Theorem 3.9)  we ﬁrst need the existence of an estimator
for entropy with some desired properties. The existence of such an estimator will be crucial to bound
the failure probability of our estimator. A result analogous to this is already known in [ADOS16]
(Lemma 2) and the proof of our result follows from a careful observation of [ADOS16  WY16a]. We
state this result here but defer the proof to appendix.

Lemma 5.1. Let ↵> 0  ✏> ⌦⇣ log N
)
py
there exists an S-pseudo proﬁle based estimator that use the optimal number of samples  has bias
less than ✏ and if we change any sample  changes by at most c · n↵
Combining the above lemma with Theorem 3.8  we next prove that our estimator deﬁned in Algo-
rithm 1 is sample complexity optimal for estimating entropy in a broader regime of error ✏.

N 1↵⌘ and S ✓D   then for entropy on subset S (Py2S py log 1

n   where c is a constant.

Proof for Theorem 3.9. Let f(p) represent the entropy of distribution p and ˆf be the estimator in
Lemma 5.1. Deﬁne F def= [0  c1 log n] for constant c1  40. Given the sequence x2n  the random set
1   y)  c1 log n}. Let F0 def= [0  8c1 log n]  then by derivation in
S is deﬁned as S def= {y 2D | f (xn
Lemma 6 [ADOS16] (or by simple application of Chernoff 4) we have 

P (Freq (S) 6✓ F0) = P (9y 2D such that f(xn
Further let G def= {x 2D | px  2c1 log N
n4 . Further for all S0 2 2G we have 

n

1

1   y)  c1 log n and f(xn

2   y) > 8c1 log n) 

1
n5 .

}  then by Equation 48 in [WY16a] we have  PS /2 2G 

P (Freq (S0) 6✓ F0) = P (9y 2 S0 such that f(xn

2   y) > 8c1 log n)   for  =

1
n5 .

Note for all x 2 S0  px  2c1 log N
and the above inequality also follows from Chernoff. All that
remains now is to upper bound . Using the estimator constructed in Lemma 5.1 and further combined
with McDiarmid’s inequality  we have 

n

Substituting all these parameters together in Theorem 3.8 we have 

P⇣|fS0(p)  ˆf(S0)| 2✏⌘  2 exp✓ 2✏2
P⇣|fS(p)  fS(p

)| > 2✏⌘ 

n )2◆   for  = exp2✏2n12↵ .
+ P (Freq (S) 6✓ F0) + PS /2 2G

n|F 0|

n(c n↵

S



 exp2✏2n12↵ n9c1 log n +

1
n4 

2
n4 .

(5)

In the ﬁrst inequality  we use Theorem 3.8. In the second inequality  we substituted the values for

1

✏ ) and ✏> ⌦⇣ log3 N
N 14↵⌘.

     and PS /2 2G. In the ﬁnal inequality we used n =⇥( N

log N

Our ﬁnal goal is to estimate f(p)  and to complete the proof we need to argue that f ¯S(ˆp) + the
correction bias with respect to f ¯S is close to f ¯S(p)  where recall ˆp is the empirical distribution
on sequence xn
2 . The proof for this follows immediately from [WY16a] (Case 2 in the proof of
Proposition 4). [WY16a] bound the bias and variance of the empirical estimator with a correction
bias and applying Markov inequality on their result we get P⇣|f ¯S(p)  (f ¯S(ˆp) + | ¯S|n )| > 2✏⌘  1
3 
where | ¯S|n is the correction bias in [WY16a]. Using triangle inequality  our estimator fails if either
|f ¯S(p)  (f ¯S(ˆp) + | ¯S|n )| > 2✏ or |fS(p)  fS(p
)| > 2✏. Further by union bound the failure
S
n4   which is a constant.
probability is at most 1

3 + 2

4Note probability of many events in this proof can be easily bounded by application of Chernoff. These
bounds on probabilities are also shown in [ADOS16  WY16a] and we use these inequalities by omitting details.

7

5.2 Distance to Uniformity estimation
Here we prove our main result for distance to uniformity estimation (Theorem 3.10). First  we show
existence of an estimator for distance to uniformity with certain desired properties. Similar to entropy 
a result analogous to this is shown in [ADOS16] (Lemma 2) and the proof of our result follows
from the careful observation of [ADOS16  JHW17]. We state this result here but defer the proof to
Appendix C.

Lemma 5.2. Let ↵> 0 and S ✓D   then for distance to uniformity on S (Py2S |py  1
N |) there
exists an S-pseudo proﬁle based estimator that use the optimal number of samples  has bias at most ✏
and if we change any sample  changes by at most c · n↵
Combining the above lemma with Theorem 3.8 we provide the proof for Theorem 3.10.

n   where c is a constant.

Proof for Theorem 3.10. Let f(p) represent the distance to uniformity for distribution p and ˆf be
] for some constant
the estimator in Lemma 5.2. Deﬁne F = [ n
c1  40. Given the sequence x2n  the random set S is deﬁned as S def= {y 2D | f (xn
1   y) 2 F}.
]  then by derivation in Lemma 7 of [ADOS16] (also
Let F0 = [ n
  n
shown in [JHW17] 5) we have 

N q c1n log n

N +q c1n log n

N +q 8c1n log n

N q 8c1n log n
P (Freq (S) 6✓ F0) = P (9y 2D such that f(xn

1   y) 2 F and f(xn

2   y) /2 F0) 

1
n4 .

  n

N

N

N

N

N q 2c1 log n

nN   1

N +q 2c1 log n
1   y) 2 F and px /2 G) 

log n
n1✏ .

nN ]}  then using Lemma 2 in [JHW17]

Further let G def= {x 2D | px 2 [ 1
we get 

Further for all S0 2 2G we have 

PS /2 2G = P (9y 2D such that f(xn
P (Freq (S0) 6✓ F0) = P (9y 2 S0 such that f(xn

2   y) > 8c1 log n)   for  =

1
n

.

Note for all x 2 S0  px 2 G and the above result follows from [JHW17] (Lemma 1). All that remains
now is to upper bound . Using the estimator constructed in Lemma 5.2 and further combined with
McDiarmid’s inequality  we have 

n )2◆   for  = exp2✏2n12↵ .

Substituting all these parameters in Theorem 3.8 we get 

P⇣|fS0(p)  ˆf(S0)| 2✏⌘  2 exp✓ 2✏2
P⇣|fS(p)  fS(p

)| > 2✏⌘ 

n|F 0|

n(c n↵

S



+ P (Freq (S) 6✓ F0) + PS /2 2G

 exp2✏2n12↵ n2q 8c1n log n

N

+

log n
n1✏ +

1
n  o(1) .

(6)

In the ﬁrst inequality  we use Theorem 3.8. In the second inequality  we substituted values for     

and PS /2 2G. In the ﬁnal inequality we used n =⇥( N

log N

1

✏2 ) and ✏> ⌦

1

N 18↵.

Our ﬁnal goal is to estimate f(p)  and to complete the proof we argue that f ¯S(ˆp) + correction bias
with respect to f ¯S is close to f ¯S(p)  where recall ˆp is the empirical distribution on sequence xn
2 . The
proof for this case follows immediately from [JHW17] (proof of Theorem 2). [JHW17] deﬁne three
kinds of events E1 E2 and E3  the proof for our empirical case follows from the analysis of bias and
variance of events E1 and E2. Further combining results in [JHW17] with Markov inequality we get
P (|f ¯S(p)  f ¯S(ˆp)| > 2✏)  1
3  and the correction bias here is zero. Using triangle inequality  our
estimator fails if either |f ¯S(p)  (f ¯S(ˆp) + | ¯S|n )| > 2✏ or |fS(p)  fS(p
)| > 2✏. Further by union
bound the failure probability is upper bounded by 1

3 + o(1)  which is a constant.

S

5Similar to entropy  for many events their probabilities can be bounded by simple application of Chernoff

and have already been shown in [ADOS16  JHW17]. We omit details for these inequalities.

8

6 Experiments

We performed two different sets of experiments for entropy estimation – one to compare performance
guarantees and the other to compare running times. In our pseudo PML approach  we divide the
samples into two parts. We run the empirical estimate on one (this is easy) and the PML estimate
on the other. For the PML estimate  any algorithm to compute an approximate PML distribution
can be used in a black box fashion. An advantage of the pseudo PML approach is that it can
use any algorithm to estimate the PML distribution as a black box  providing both competitive
performance and running time efﬁciency. In our experiments  we use the heuristic algorithm in
[PJW17] to compute an approximate PML distribution. In the ﬁrst set of experiments detailed below 
we compare the performance of the pseudo PML approach with raw [PJW17] and other state-of-the-
art estimators for estimating entropy. Our code is available at https://github.com/shiragur/
CodeForPseudoPML.git

)

E
S
M
R

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
m

 
t

o
o
R

101

100

10-1

10-2

10-3

10-4

103

Entropy - Mix 2 Uniforms

Our Work
PJW17
MLE
VV11b
JVHW15

104

105

106

107

Sample size

)

E
S
M
R

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
m

 
t

o
o
R

101

100

10-1

10-2

10-3

10-4

103

Entropy - Zipf(0.5)

Our Work
PJW17
MLE
VV11b
JVHW15

104

105

106

107

Sample size

)

E
S
M
R

(
 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
m

 
t

o
o
R

101

100

10-1

10-2

10-3

103

Entropy - Zipf(1)

Our Work
PJW17
MLE
VV11b
JVHW15

104

105

106

107

Sample size

Each plot depicts the performance of various algorithms for estimating entropy of different distribu-
tions with domain size N = 105. Each data point represents 50 random trials. “Mix 2 Uniforms” is a
mixture of two uniform distributions  with half the probability mass on the ﬁrst N/10 symbols  and
Zipf(↵) ⇠ 1/i↵ with i 2 [N ]. MLE is the naive approach of using the empirical distribution with
correction bias; all the remaining algorithms are denoted using bibliographic citations. In our algo-
rithm we pick threshold = 18 (same as [WY16a]) and our set F = [0  18] (input of Algorithm 1)  i.e.
we use the PML estimate on frequencies  18 and empirical estimate on the rest. Unlike Algorithm 1 
we do not perform sample splitting in the experiments – we believe this requirement is an artifact of
our analysis. For estimating entropy  the error achieved by our estimator is competitive with [PJW17]
and other state-of-the-art entropy estimators. Note that our results match [PJW17] for small sample
sizes because not many domain elements cross the threshold and for a large fraction of the samples 
we simply run the [PJW17] algorithm.
In the second set of experiments we demonstrate the running time efﬁciency of our approach. In
these experiments  we compare the running time of our algorithm using [PJW17] as a subroutine to
the raw [PJW17] algorithm on the Zipf(1) distribution. The second row is the fraction of samples on
which our algorithm uses the empirical estimate (plus correction bias). The third row is the ratio of
the running time of [PJW17] to our algorithm. For large sample sizes  the entries in the EmpFrac row
have high value  i.e. our algorithm applies the simple empirical estimate on large fraction of samples;
therefore  enabling 10x speedup in the running times.
5 ⇤ 104
0.505
3.561

5 ⇤ 103
0.317
1.205

5 ⇤ 106
0.886
12.196

Samples size

EmpFrac
Speedup

103
0.184
0.824

104
0.372
1.669

105
0.562
4.852

5 ⇤ 105
0.695
9.552

106
0.752
13.337

Acknowledgments
We thank the reviewers for the helpful comments  great suggestions  and positive feedback. Moses
Charikar was supported by a Simons Investigator Award  a Google Faculty Research Award and
an Amazon Research Award. Aaron Sidford was partially supported by NSF CAREER Award
CCF-1844855.

References
[ADOS16] Jayadev Acharya  Hirakendu Das  Alon Orlitsky  and Ananda Theertha Suresh. A
uniﬁed maximum likelihood approach for optimal distribution property estimation.
CoRR  abs/1611.02960  2016.

9

[AOST14] Jayadev Acharya  Alon Orlitsky  Ananda Theertha Suresh  and Himanshu Tyagi. The
complexity of estimating rényi entropy. In Proceedings of the Twenty-Sixth Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 1855–1869  2014.

[CL11] T. Tony Cai and Mark G. Low. Testing composite hypotheses  hermite polynomials and
optimal estimation of a nonsmooth functional. Ann. Statist.  39(2):1012–1041  04 2011.

[CSS19] Moses Charikar  Kirankumar Shiragur  and Aaron Sidford. Efﬁcient Proﬁle Maxi-
mum Likelihood for Universal Symmetric Property Estimation. arXiv e-prints  page
arXiv:1905.08448  May 2019.

[ET76] Bradley Efron and Ronald Thisted. Estimating the number of unsen species: How many

words did shakespeare know? Biometrika  63(3):435–447  1976.

[HJM17] Yanjun Han  Jiantao Jiao  and Rajarshi Mukherjee. On Estimation of $L_{r}$-Norms in

Gaussian White Noise Models. arXiv e-prints  page arXiv:1710.03863  Oct 2017.

[HJW18] Yanjun Han  Jiantao Jiao  and Tsachy Weissman. Local moment matching: A uniﬁed
methodology for symmetric functional estimation and distribution estimation under
wasserstein distance. arXiv preprint arXiv:1802.08405  2018.

[HJWW17] Yanjun Han  Jiantao Jiao  Tsachy Weissman  and Yihong Wu. Optimal rates of entropy

estimation over Lipschitz balls. arXiv e-prints  page arXiv:1711.02141  Nov 2017.

[HO19a] Yi Hao and Alon Orlitsky. The broad optimality of proﬁle maximum likelihood  2019.

[HO19b] Yi Hao and Alon Orlitsky. Data ampliﬁcation: Instance-optimal property estimation 

2019.

[JHW16] J. Jiao  Y. Han  and T. Weissman. Minimax estimation of the l1 distance. In 2016 IEEE

International Symposium on Information Theory (ISIT)  pages 750–754  July 2016.

[JHW17] Jiantao Jiao  Yanjun Han  and Tsachy Weissman. Minimax Estimation of the L1

Distance. arXiv e-prints  page arXiv:1705.00807  May 2017.

[JVHW15] J. Jiao  K. Venkat  Y. Han  and T. Weissman. Minimax estimation of functionals of
discrete distributions. IEEE Transactions on Information Theory  61(5):2835–2885 
May 2015.

[Nem03] Arkadi Nemirovski. On tractable approximations of randomly perturbed convex con-
straints. In 42nd IEEE International Conference on Decision and Control (IEEE Cat.
No. 03CH37475)  volume 3  pages 2419–2422. IEEE  2003.

[OSS+04] A. Orlitsky  S. Sajama  N. P. Santhanam  K. Viswanathan  and Junan Zhang. Algo-
rithms for modeling distributions over large alphabets. In International Symposium on
Information Theory  2004. ISIT 2004. Proceedings.  pages 304–304  2004.

[OSW16] Alon Orlitsky  Ananda Theertha Suresh  and Yihong Wu. Optimal prediction of the num-
ber of unseen species. Proceedings of the National Academy of Sciences  113(47):13283–
13288  2016.

[PJW17] D. S. Pavlichin  J. Jiao  and T. Weissman. Approximate Proﬁle Maximum Likelihood.

ArXiv e-prints  December 2017.

[RRSS07] S. Raskhodnikova  D. Ron  A. Shpilka  and A. Smith. Strong lower bounds for approxi-
mating distribution support size and the distinct elements problem. In 48th Annual IEEE
Symposium on Foundations of Computer Science (FOCS’07)  pages 559–569  Oct 2007.

[RVZ17] Aditi Raghunathan  Gregory Valiant  and James Zou. Estimating the unseen from

multiple populations. CoRR  abs/1707.03854  2017.

[Tim14] Aleksandr Filippovich Timan. Theory of approximation of functions of a real variable 

volume 34. Elsevier  2014.

10

[VV11a] G. Valiant and P. Valiant. The power of linear estimators. In 2011 IEEE 52nd Annual

Symposium on Foundations of Computer Science  pages 403–412  Oct 2011.

[VV11b] Gregory Valiant and Paul Valiant. Estimating the unseen: An n/log(n)-sample estimator
for entropy and support size  shown optimal via new clts. In Proceedings of the Forty-
third Annual ACM Symposium on Theory of Computing  STOC ’11  pages 685–694 
New York  NY  USA  2011. ACM.

[WY15] Y. Wu and P. Yang. Chebyshev polynomials  moment matching  and optimal estimation

of the unseen. ArXiv e-prints  April 2015.

[WY16a] Y. Wu and P. Yang. Minimax rates of entropy estimation on large alphabets via best
polynomial approximation. IEEE Transactions on Information Theory  62(6):3702–
3720  June 2016.

[WY16b] Yihong Wu and Pengkun Yang. Sample complexity of the distinct elements problem.

arXiv e-prints  page arXiv:1612.03375  Dec 2016.

[YOSW18] Hao Yi  Alon Orlitsky  Ananda Theertha Suresh  and Yihong Wu. Data ampliﬁcation:
A uniﬁed and competitive approach to property estimation. In Advances in Neural
Information Processing Systems  pages 8834–8843  2018.

[ZVV+16] James Zou  Gregory Valiant  Paul Valiant  Konrad Karczewski  Siu On Chan  Kaitlin
Samocha  Monkol Lek  Shamil Sunyaev  Mark Daly  and Daniel G. MacArthur. Quanti-
fying unobserved protein-coding variants in human populations provides a roadmap for
large-scale sequencing projects. Nature Communications  7:13293 EP –  10 2016.

11

,Moses Charikar
Kirankumar Shiragur
Aaron Sidford