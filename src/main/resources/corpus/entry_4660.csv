2012,Regularized Off-Policy TD-Learning,We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD)  which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods  such as TDC  and a convex-concave saddle-point formulation of non-smooth convex optimization  which enables first-order solvers and feature selection using online convex regularization. A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence  sparse feature selection capability and low computational cost of the RO-TD algorithm.,Regularized Off-Policy TD-Learning

Bo Liu  Sridhar Mahadevan
Computer Science Department
University of Massachusetts

Amherst  MA 01003

{boliu  mahadeva}@cs.umass.edu

Ji Liu

Computer Science Department

University of Wisconsin

Madison  WI 53706

ji-liu@cs.wisc.edu

Abstract

We present a novel l1 regularized off-policy convergent TD-learning method
(termed RO-TD)  which is able to learn sparse representations of value functions
with low computational complexity. The algorithmic framework underlying RO-
TD integrates two key ideas: off-policy convergent gradient TD methods  such
as TDC  and a convex-concave saddle-point formulation of non-smooth convex
optimization  which enables ﬁrst-order solvers and feature selection using online
convex regularization. A detailed theoretical and experimental analysis of RO-TD
is presented. A variety of experiments are presented to illustrate the off-policy
convergence  sparse feature selection capability and low computational cost of the
RO-TD algorithm.

1 Introduction

Temporal-difference (TD) learning is a widely used method in reinforcement learning (RL). Al-
though TD converges when samples are drawn “on-policy” by sampling from the Markov chain
underlying a policy in a Markov decision process (MDP)  it can be shown to be divergent when
samples are drawn “off-policy”. Off-policy methods are of wider applications since they are able to
learn while executing an exploratory policy  learn from demonstrations  and learn multiple tasks in
parallel [2]. Sutton et al. [20] introduced convergent off-policy temporal difference learning algo-
rithms  such as TDC  whose computation time scales linearly with the number of samples and the
number of features. Recently  a linear off-policy actor-critic algorithm based on the same framework
was proposed in [2].
Regularizing reinforcement learning algorithms leads to more robust methods that can scale up to
large problems with many potentially irrelevant features. LARS-TD [7] introduced a popular ap-
proach of combining l1 regularization using Least Angle Regression (LARS) with the least-squares
TD (LSTD) framework. Another approach was introduced in [5] (LCP-TD) based on the Linear
Complementary Problem (LCP) formulation  an optimization approach between linear program-
ming and quadratic programming. LCP-TD uses “warm-starts”  which helps signiﬁcantly reduce
the burden of l1 regularization. A theoretical analysis of l1 regularization was given in [4]  including
error bound analysis with ﬁnite samples in the on-policy setting. Another approach integrating the
Dantzig Selector with LSTD was proposed in [3]  overcoming some of the drawbacks of LARS-TD.
An approximate linear programming approach for ﬁnding l1 regularized solutions of the Bellman
equation was presented in [17]. All of these approaches are second-order methods  requiring com-
plexity approximately cubic in the number of (active) features. Another approach to feature selec-
tion is to greedily add new features  proposed recently in [15]. Regularized ﬁrst-order reinforcement
learning approaches have recently been investigated in the on-policy setting as well  wherein con-
vergence of l1 regularized temporal difference learning is discussed in [16] and mirror descent [6] is
used in [11].

1

In this paper  the off-policy TD learning problem is formulated from the stochastic optimization
perspective. A novel objective function is proposed based on the linear equation formulation of
the TDC algorithm. The optimization problem underlying off-policy TD methods  such as TDC 
is reformulated as a convex-concave saddle-point stochastic approximation problem  which is both
convex and incrementally solvable. A detailed theoretical and experimental study of the RO-TD
algorithm is presented.
Here is a brief roadmap to the rest of the paper. Section 2 reviews the basics of MDPs  RL and recent
work on off-policy convergent TD methods  such as the TDC algorithm. Section 3 introduces the
proximal gradient method and the convex-concave saddle-point formulation of non-smooth convex
optimization. Section 4 presents the new RO-TD algorithm. Convergence analysis of RO-TD is
presented in Section 5. Finally  in Section 6  experimental results are presented to demonstrate the
effectiveness of RO-TD.

2 Reinforcement Learning and the TDC Algorithm

A Markov Decision Process (MDP) is deﬁned by the tuple (S  A  P a
ss0   R  γ)  comprised of a set of
states S  a set of (possibly state-dependent) actions A (As)  a dynamical system model comprised of
ss0 specifying the probability of transition to state s0 from state s under action
the transition kernel P a
a  a reward model R  and 0 ≤ γ < 1 is a discount factor. A policy π : S → A is a deterministic
mapping from states to actions. Associated with each policy π is a value function V π  which is the
ﬁxed point of the Bellman equation:

V π(s) = T πV π(s) = Rπ(s) + γP πV π(s)

where Rπ is the expected immediate reward function (treated here as a column vector) and P π is
the state transition function under ﬁxed policy π  and T π is known as the Bellman operator. In what
follows  we often drop the dependence of V π  T π  Rπ on π  for notational simplicity. In linear value
function approximation  a value function is assumed to lie in the linear span of a basis function
matrix Φ of dimension |S| × d  where d is the number of linear independent features. Hence 
V ≈ ˆV = Φθ. The vector space of all value functions is a normed inner product space  where the
“length” of any value function f is measured as ||f||2
s ξ(s)f 2(s) = f0Ξf weighted by Ξ 
where Ξ is deﬁned in Figure 1. For the t-th sample  φt φ0
t  θt and δt are deﬁned in Figure 1. TD
learning uses the following update rule θt+1 = θt + αtδtφt  where αt is the stepsize. However 
TD is only guaranteed to converge in the on-policy setting  although in many off-policy situations 
it still has satisfactory performance [21]. TD with gradient correction (TDC) [20] aims to minimize
the mean-square projected Bellman error (MSPBE) in order to guarantee off-policy convergence.
MSPBE is deﬁned as

Ξ = P

MSPBE(θ) = kΦθ − ΠT (Φθ)k2

(1)
To avoid computing the inverse matrix (ΦT ΞΦ)−1 and to avoid the double sampling problem [19]
in (1)  an auxiliary variable w is deﬁned

Ξ = (ΦT Ξ(T Φθ − Φθ))T (ΦT ΞΦ)−1ΦT Ξ(T Φθ − Φθ)

w = (ΦT ΞΦ)−1ΦT Ξ(T Φθ − Φθ)

(2)

The two time-scale gradient descent learning method TDC [20] is deﬁned below
t wt)  wt+1 = wt + βt(δt − φT

0(φT

where −αtγφt

θt+1 = θt + αtδtφt − αtγφt
0(φT

(3)
t wt) is the term for correction of gradient descent direction  and βt = ηαt  η > 1.

t wt)φt

3 Proximal Gradient and Saddle-Point First-Order Algorithms

We now introduce some background material from convex optimization. The proximal mapping
associated with a convex function h is deﬁned as:1

proxh(x) = arg min
u

(h(u) +

ku − xk2)

1
2

(4)

1The proximal mapping can be shown to be the resolvent of the subdifferential of the function h.

2

A

1
2 may not be unique.

• Ξ is a diagonal matrix whose entries ξ(s) are given by a positive probability distribution
over states. Π = Φ(ΦT ΞΦ)−1ΦT Ξ is the weighted least-squares projection operator.
• A square root of A is a matrix B satisfying B2 = A and B is denoted as A
1
2 . Note that
• [· ·] is a row vector  and [·;·] is a column vector.
• For the t-th sample  φt (the t-th row of Φ)  φ0

corresponding to st  s0
t θt) − φT
0T
order TD learning methods  and δt = (rt + γφ
error. Also  xt = [wt; θt]  αt is a stepsize  βt = ηαt  η > 0.

t (the t-th row of Φ0) are the feature vectors
t  respectively. θt is the coefﬁcient vector for t-th sample in ﬁrst-
t θt is the temporal difference

• m  n are conjugate numbers if 1
m is
• ρ is l1 regularization parameter  λ is the eligibility trace factor  N is the sample size  d

the m-norm of vector x.

j |xj|m) 1

n = 1  m ≥ 1  n ≥ 1. ||x||m = (P

m + 1

is the number of basis functions  p is the number of active basis functions.

Figure 1: Notation used in this paper.

In the case of h(x) = ρkxk1(ρ > 0)  which is particularly important for sparse feature selection 
the proximal operator turns out to be the soft-thresholding operator Sρ(·)  which is an entry-wise
shrinkage operator:

proxh(x)i = Sρ(xi) = max(xi − ρ  0) − max(−xi − ρ  0)

(5)
where i is the index  and ρ is a threshold. With this background  we now introduce the proximal
gradient method. If the optimization problem is
x∗ = arg min
x∈X

(f(x) + h(x))

(6)

wherein f(x) is a convex and differentiable loss function and the regularization term h(x) is convex 
possibly non-differentiable and computing proxh is not expensive  then computation of (6) can be
carried out using the proximal gradient method:

xt+1 = proxαth (xt − αt∇f(xt))

(7)

where αt > 0 is a (decaying) stepsize  a constant or it can be determined by line search.

3.1 Convex-concave Saddle-Point First Order Algorithms

The key novel contribution of our paper is a convex-concave saddle-point formulation for regular-
ized off-policy TD learning. A convex-concave saddle-point problem is formulated as follows. Let
x ∈ X  y ∈ Y   where X  Y are both nonempty bounded closed convex sets  and f(x) : X → R
If there exists a function ϕ(· ·) such that f(x) can be represented as
be a convex function.
f(x) := supy∈Y ϕ(x  y)  then the pair (ϕ  Y ) is referred as the saddle-point representation of f.
The optimization problem of minimizing f over X is converted into an equivalent convex-concave
saddle-point problem SadV al = inf x∈Xsupy∈Y ϕ(x  y) of ϕ on X×Y . If f is non-smooth yet con-
vex and well structured  which is not suitable for many existing optimization approaches requiring
smoothness  its saddle-point representation ϕ is often smooth and convex. Thus  convex-concave
saddle-point problems are  therefore  usually better suited for ﬁrst-order methods [6]. A compre-
hensive overview on extending convex minimization to convex-concave saddle-point problems with
uniﬁed variational inequalities is presented in [1]. As an example  consider f(x) = ||Ax − b||m
which admits a bilinear minimax representation

f(x) := kAx − bkm = max
kykn≤1

yT (Ax − b)

where m  n are conjugate numbers. Using the approach in [13]  Equation (8) can be solved as

xt+1 = xt − αtAT yt  yt+1 = Πn(yt + αt(Axt − b))

where Πn is the projection operator of y onto the unit ln-ball kykn ≤ 1 which is deﬁned as

Πn(y) = min(1  1/kykn)y  n = 2  3 ···   Π∞(yi) = min(1  1/|yi|)yi

and Π∞ is an entrywise operator.

3

(8)

(9)

(10)

4 Regularized Off-policy Convergent TD-Learning

We now describe a novel algorithm  regularized off-policy convergent TD-learning (RO-TD)  which
combines off-policy convergence and scalability to large feature spaces. The objective function
is proposed based on the linear equation formulation of the TDC algorithm. Then the objective
function is represented via its dual minimax problem. The RO-TD algorithm is proposed based on
the primal-dual subgradient saddle-point algorithm  and inspired by related methods in [12] [13].

4.1 Objective Function of Off-policy TD Learning

In this subsection  we describe the objective function of the regularized off-policy RL problem. We
now ﬁrst formulate the two updates of θt  wt into a single iteration by rearranging the two equations
in (3) as xt+1 = xt − αt(Atxt − bt)  where xt = [wt; θt] 
t)T
t)T

ηφt(φt − γφ0
φt(φt − γφ0

(cid:20) ηφtφt

(cid:20) ηrtφt

  bt =

At =

γφ0

(11)

(cid:21)

(cid:21)

rtφt

T

T

tφt

Following [20]  the TDC algorithm solution follows from the linear equation Ax = b  where

A = E[At]  b = E[bt]  x = [w; θ]

(12)
There are some issues regarding the objective function  which arise from the online convex opti-
mization and reinforcement learning perspectives  respectively. The ﬁrst concern is that the objective
function should be convex and stochastically solvable. Note that A  At are neither PSD nor symmet-
ric  and it is not straightforward to formulate a convex objective function based on them. The second
concern is that since we do not have knowledge of A  the objective function should be separable so
that it is stochastically solvable based on At  bt. The other concern regards the sampling condition
in temporal difference learning: double-sampling. As pointed out in [19]  double-sampling is a
necessary condition to obtain an unbiased estimator if the objective function is the Bellman resid-
ual or its derivatives (such as projected Bellman residual)  wherein the product of Bellman error or
projected Bellman error metrics are involved. To overcome this sampling condition constraint  the
product of TD errors should be avoided in the computation of gradients. Consequently  based on the
linear equation formulation in (12) and the requirement on the objective function discussed above 
we propose the regularized loss function as

L(x) = kAx − bkm + h(x)

(13)

Here we also enumerate some intuitive objective functions and give a brief analysis on the reasons
why they are not suitable for regularized off-policy ﬁrst-order TD learning. One intuitive idea is
to add a sparsity penalty on MSPBE  i.e.  L(θ) = MSPBE(θ)+ρkθk1. Because of the l1 penalty
term  the solution to ∇L = 0 does not have an analytical form and is thus difﬁcult to compute.
The second intuition is to use the online least squares formulation of the linear equation Ax = b.
1
2 does not exist and thus
However  since A is not symmetric and positive semi-deﬁnite (PSD)  A
Ax = b cannot be reformulated as minx∈X||A
2. Another possible idea is to attempt
to ﬁnd an objective function whose gradient is exactly Atxt − bt and thus the regularized gradient
is proxαth(xt)(Atxt − bt). However  since At is not symmetric  this gradient does not explicitly
correspond to any kind of optimization problem  not to mention a convex one2.

2 x − A− 1

2 b||2

1

4.2 RO-TD Algorithm Design

In this section  the problem of (13) is formulated as a convex-concave saddle-point problem  and the
RO-TD algorithm is proposed. Analogous to (8)  the regularized loss function can be formulated as
(14)

yT (Ax − b) + h(x)

kAx − bkm + h(x) = max
kykn≤1

Similar to (9)  Equation (14) can be solved via an iteration procedure as follows  where xt = [wt; θt].

xt+ 1
2

= xt − αtAT
t yt
xt+1 = proxαth(xt+ 1
)

2

= yt + αt(Atxt − bt)

 

 

yt+ 1
2
yt+1 = Πn(yt+ 1

2

)

(15)

2Note that the A matrix in GTD2’s linear equation representation is symmetric  yet is not PSD  so it cannot

be formulated as a convex problem.

4

The averaging step  which plays a crucial role in stochastic optimization convergence  generates the
approximate saddle-points [6  12]

(cid:16)Xt

(cid:17)−1Xt

(cid:16)Xt

(cid:17)−1Xt

i=0

i=0

i=0

i=0

αi

αi

αiyi

¯xt =

αixi  ¯yt =

(16)
Due to the computation of At in (15) at each iteration  the computation cost appears to be O(N d2) 
where N  d are deﬁned in Figure 1. However  the computation cost is actually O(N d) with a linear
t At  Atxt − bt. Denoting yt = [y1 t; y2 t]  where y1 t; y2 t
algebraic trick by computing not At but yT
are column vectors of equal length  we have
1 tφt) + γφT

(φt − γφ0
Atxt − bt can be computed according to Equation (3) as follows:

h
Atxt − bt =(cid:2) −η(δt − φT

(18)
Both (17) and (18) are of linear computation complexity. Now we are ready to present the RO-TD
algorithm:

t wt)φt; γ(φT

0 − δtφt

t)T (ηyT

t At =
yT

1 t + yT

2 tφ0
t)

t wt)φt

2 t)φt

t (yT

t (yT

(17)

ηφT

i

(cid:3)

Algorithm 1 RO-TD
Let π be some ﬁxed policy of an MDP M  and let the sample set S = {si  ri  si
some ﬁxed basis.
1: repeat
2:
3:
4:
5:
6: until t = N;
7: Compute ¯xN   ¯yN as in Equation (16) with t = N

Compute φt  φt
Compute yT
Compute xt+1  yt+1 as in Equation (15)
Set t ← t + 1;

At  Atxt − bt in Equation (17) and (18).

0 and TD error δt = (rt + γφ

t θt) − φT
0T
t θt

t

0}N
i=1. Let Φ be

There are some design details of the algorithm to be elaborated. First  the regularization term h(x)
can be any kind of convex regularization  such as ridge regression or sparsity penalty ρ||x||1. In case
of h(x) = ρ||x||1  proxαth(·) = Sαtρ(·). In real applications the sparsiﬁcation requirement on θ
and auxiliary variable w may be different  i.e.  h(x) = ρ1kθk1 + ρ2kwk1  ρ1 6= ρ2  one can simply
replace the uniform soft thresholding Sαtρ by two separate soft thresholding operations Sαtρ1  Sαtρ2
and thus the third equation in (15) is replaced by the following 

h

i

=

xt+ 1
2

(19)
Another concern is the choice of conjugate numbers (m  n). For ease of computing Πn  we use
(2  2)(l2 ﬁt)  (+∞  1)(uniform ﬁt) or (1  +∞). m = n = 2 is used in the experiments below.

)  wt+1 = Sαtρ2(wt+ 1

  θt+1 = Sαtρ1(θt+ 1

; θt+ 1

wt+ 1
2

)

2

2

2

4.3 RO-GQ(λ) Design

θt+1 = θt + αt[δtet − γ(1 − λ)wt

GQ(λ)[10] is a generalization of the TDC algorithm with eligibility traces and off-policy learning
of temporally abstract predictions  where the gradient update changes from Equation (3) to
t φtφt)

(20)
The central element is to extend the MSPBE function to the case where it incorporates eligibility
traces. The objective function and corresponding linear equation component At  bt can be written
as follows:

¯φt+1]  wt+1 = wt + βt(δtet − wT

T et

"
At = (cid:2) ηφT

At =

L(θ) = ||Φθ − ΠT πλΦθ||2
ηet(φt − γ ¯φt+1)T
et(φt − γ ¯φt+1)T

T

Ξ

ηφtφt

γ(1 − λ) ¯φt+1eT

t

#

  bt =
At  Atxt − bt is

Atxt − bt = (cid:2) −η(δtet − φT

t (yT

yT

t

1 tφt) + γ(1 − λ)eT

t (yT

2 t

t

Similar to Equation (17) and (18)  the computation of yT

(23)
where eligibility traces et  and ¯φt  T πλ are deﬁned in [10]. Algorithm 2  RO-GQ(λ)  extends the
RO-TD algorithm to include eligibility traces.

t wtφt); γ(1 − λ)(eT

(φt − γ ¯φt+1)T (ηyT

¯φt+1)
t wt) ¯φt+1 − δtet

1 t + yT

2 t)et

(cid:21)

(cid:20) ηrtet
(cid:3)

rtet

(21)

(22)

(cid:3)

5

Algorithm 2 RO-GQ(λ)
Let π and Φ be as deﬁned in Algorithm 1. Starting from s0.
1: repeat
2:
3:
4:
5:
6:
7: until st is an absorbing state;
8: Compute ¯xt  ¯yt as in Equation (16)

Compute φt  ¯φt+1 and TD error δt = (rt + γ ¯φT
Compute yT
Compute xt+1  yt+1 as in Equation (15)
Choose action at  and get st+1
Set t ← t + 1;

At  Atxt − bt in Equation (23).

t

t+1θt) − φT
t θt

4.4 Extension

It is also worth noting that there exists another formulation of the loss function different from Equa-
tion (13) with the following convex-concave formulation as in [14  6] 

min
x

1
2

kAx − bk2

2 + ρkxk1 =

(bT y − ρ

2 yT y)

(cid:16)

max

kAT yk∞≤1
max

kuk∞≤1 y

= min
x

xT u + yT (Ax − b) − ρ

2 yT y

(cid:17)

(24)

which can be solved iteratively without the proximal gradient step as follows  which serves as a
counterpart of Equation (15) 

xt+1 = xt − αtρ(ut + At
= ut + αt
ρ

ut+ 1
2

T yt)

 

yt+1 = yt + αt
ρ

(Atxt − bt − ρyt)

xt

  ut+1 = Π∞(ut+ 1

2

)

(25)

5 Convergence Analysis of RO-TD

Assumption 1 (MDP)[20]: The underlying Markov Reward Process (MRP) M = (S  P  R  γ) is ﬁ-
nite and mixing  with stationary distribution π. Assume that ∃ a scalar Rmax such that V ar[rt|st] ≤
Rmax holds w.p.1.
Assumption 2 (Basis Function)[20]: Φ is a full column rank matrix  namely  Φ comprises a linear
independent set of basis functions w.r.t all sample states in sample set S. Also  assume the fea-
0
t) is an i.i.d sequence 
tures (φt  φ
∀t kφtk∞ < +∞ kφ0
Assumption 3 (Subgradient Boundedness)[12]: Assume for the bilinear convex-concave loss
At and
function deﬁned in (14)  the sets X  Y are closed compact sets. Then the subgradient yT
Atxt − bt in RO-TD algorithm are uniformly bounded  i.e.  there exists a constant L such that

0
t) have uniformly bounded second moments. Finally  if (st  at  s

tk∞ < +∞.

t

kAtxt − btk ≤ L (cid:13)(cid:13)yT

t

(cid:13)(cid:13) ≤ L.

At

Proposition 1: The approximate saddle-point ¯xt of RO-TD converges w.p.1 to the global minimizer
of the following 

x∗ = arg min
x∈X

kAx − bkm + ρkxk1

(26)

Proof Sketch: See the supplementary material for details.

6 Empirical Results

We now demonstrate the effectiveness of the RO-TD algorithm against other algorithms across a
number of benchmark domains. LARS-TD [7]  which is a popular second-order sparse reinforce-
ment learning algorithm  is used as the baseline algorithm for feature selection and TDC is used as
the off-policy convergent RL baseline algorithm  respectively.

6

Figure 2: Illustrative examples of the convergence of RO-TD using the Star and Random-walk
MDPs.

6.1 MSPBE Minimization and Off-Policy Convergence

This experiment aims to show the minimization of MSPBE and off-policy convergence of the RO-
TD algorithm. The 7 state star MDP is a well known counterexample where TD diverges monoton-
ically and TDC converges. It consists of 7 states and the reward w.r.t any transition is zero. Because
of this  the star MDP is unsuitable for LSTD-based algorithms  including LARS-TD since ΦT R = 0
always holds. The random-walk problem is a standard Markov chain with 5 states and two absorb-
ing state at two ends. Three sets of different bases Φ are used in [20]  which are tabular features 
inverted features and dependent features respectively. An identical experiment setting to [20] is used
for these two domains. The regularization term h(x) is set to 0 to make a fair comparison with TD
and TDC. α = 0.01  η = 10 for TD  TDC and RO-TD. The comparison with TD  TDC and RO-TD
is shown in the left subﬁgure of Figure 2  where TDC and RO-TD have almost identical MSPBE
(Axt − b) and kAxt − bk2  wherein
over iterations. The middle subﬁgure shows the value of yT
(Axt − b). Note that for this problem  the Slater
kAxt − bk2 is always greater than the value of yT
condition is satisﬁed so there is no duality gap between the two curves. As the result shows  TDC
and RO-TD perform equally well  which illustrates the off-policy convergence of the RO-TD algo-
rithm. The result of random-walk chain is averaged over 50 runs. The rightmost subﬁgure of Figure
2 shows that RO-TD is able to reduce MSPBE over successive iterations w.r.t three different basis
functions.

t

t

6.2 Feature Selection

In this section  we use the mountain car example with a variety of bases to show the feature selection
capability of RO-TD. The Mountain car MDPis an optimal control problem with a continuous two-
dimensional state space. The steep discontinuity in the value function makes learning difﬁcult for
bases with global support. To make a fair comparison  we use the same basis function setting as in
[7]  where two dimensional grids of 2  4  8  16  32 RBFs are used so that there are totally 1365 basis
functions. For LARS-TD  500 samples are used. For RO-TD and TDC  3000 samples are used by
executing 15 episodes with 200 steps for each episode  stepsize αt = 0.001  and ρ1 = 0.01  ρ2 =
0.2. We use the result of LARS-TD and l2 LSTD reported in [7]. As the result shows in Table 1 
RO-TD is able to perform feature selection successfully  whereas TDC and TD failed. It is worth
noting that comparing the performance of RO-TD and LARS-TD is not the focus of this paper since
LARS-TD is not convergent off-policy and RO-TD’s performance can be further optimized using
the mirror-descent approach with the Mirror-Prox algorithm [6] which incorporates mirror descent
with an extragradient [9]  as discussed below.

Algorithm

Success(20/20)

Steps

LARS-TD
142.25 ± 9.74

100%

RO-TD
100%

147.40 ± 13.31

l2 LSTD TDC TD
0% 0%
-
-

0%
-

Table 1: Comparison of TD  LARS-TD  RO-TD  l2 LSTD  TDC and TD

7

01020304050607080901000102030405060708090100SweepsMSPBEComparison of MSPBE  TDTDCRO−TD02040608010012014016018020002468101214Sweeps||Ax−b||2 and yT(Ax−b)  yT(Ax−b)||Ax−b||20204060801001201401601802000.020.030.040.050.060.070.080.090.10.110.12SweepsMSPBEMSPBE Minimization  InvertedTabularDependentExperiment\Method

Experiment 1
Experiment 2

RO-GQ(λ)
6.9 ± 4.82
14.7 ± 10.70

GQ(λ)

11.3 ± 9.58
27.2 ± 6.52

LARS-TD

-
-

Table 2: Comparison of RO-GQ(λ)  GQ(λ)  and LARS-TD on Triple-Link Inverted Pendulum Task
showing minimum number of learning episodes.

6.3 High-dimensional Under-actuated Systems

The triple-link inverted pendulum [18] is a highly nonlinear under-actuated system with 8-
dimensional state space and discrete action space. The state space consists of the angles and angular
velocity of each arm as well as the position and velocity of the car. The discrete action space is
{0  5Newton −5Newton}. The goal is to learn a policy that can balance the arms for Nx steps
within some minimum number of learning episodes. The allowed maximum number of episodes
is 300. The pendulum initiates from zero equilibrium state and the ﬁrst action is randomly chosen
to push the pendulum away from initial state. We test the performance of RO-GQ(λ)  GQ(λ) and
LARS-TD. Two experiments are conducted with Nx = 10  000 and 100  000  respectively. Fourier
basis [8] with order 2 is used  resulting in 6561 basis functions. Table 2 shows the results of this
experiment  where RO-GQ(λ) performs better than other approaches  especially in Experiment 2 
which is a harder task. LARS-TD failed in this domain  which is mainly not due to LARS-TD itself
but the quality of samples collected via random walk.
To sum up  RO-GQ(λ) tends to outperform GQ(λ) in all aspects  and is able to outperform LARS-
TD based policy iteration in high dimensional domains  as well as in selected smaller MDPs where
LARS-TD diverges (e.g.  the star MDP). It is worth noting that the computation cost of LARS-TD
is O(N dp3)  where that for RO-TD is O(N d). If p is linear or sublinear w.r.t d  RO-TD has a
signiﬁcant advantage over LARS-TD. However  compared with LARS-TD  RO-TD requires ﬁne
tuning the parameters of αt  ρ1  ρ2 and is usually not as sample efﬁcient as LARS-TD. We also ﬁnd
that tuning the sparsity parameter ρ2 generates an interpolation between GQ(λ) and TD learning 
where a large ρ2 helps eliminate the correction term of TDC update and make the update direction
more similar to the TD update.

7 Conclusions

This paper presents a novel uniﬁed framework for designing regularized off-policy convergent RL
algorithms combining a convex-concave saddle-point problem formulation for RL with stochastic
ﬁrst-order methods. A detailed experimental analysis reveals that the proposed RO-TD algorithm
is both off-policy convergent and is robust to noisy features. There are many interesting future
directions for this research. One direction for future work is to extend the subgradient saddle-
point solver to a more generalized mirror descent framework. Mirror descent is a generalization of
subgradient descent with non-Euclidean distance [1]  and has many advantages over gradient descent
in high-dimensional spaces. In [6]  two algorithms to solve the bilinear saddle-point formulation are
proposed based on mirror descent and the extragradient [9]  such as the Mirror-Prox algorithm. [6]
also points out that the Mirror-Prox algorithm may be further optimized via randomization. To scale
to larger MDPs  it is possible to design SMDP-based mirror-descent methods as well.

Acknowledgments

This material is based upon work supported by the Air Force Ofﬁce of Scientiﬁc Research (AFOSR)
under grant FA9550-10-1-0383  and the National Science Foundation under Grant Nos. NSF CCF-
1025120  IIS-0534999  IIS-0803288  and IIS-1216467 Any opinions  ﬁndings  and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reﬂect
the views of the AFOSR or the NSF. We thank M. F. Duarte for helpful discussions.

8

References
[1] A. Ben-Tal and A. Nemirovski. Non-Euclidean restricted memory level method for large-scale

convex optimization. Mathematical Programming  102(3):407–456  2005.

[2] T. Degris  M. White  and R. S. Sutton. Linear off-policy actor-critic. In International Confer-

ence on Machine Learning  2012.

[3] M. Geist  B. Scherrer  A. Lazaric  and M. Ghavamzadeh. A Dantzig Selector Approach to

Temporal Difference Learning. In International Conference on Machine Learning  2012.

[4] M. Ghavamzadeh  A. Lazaric  R. Munos  and M. Hoffman. Finite-Sample Analysis of Lasso-

TD . In Proceedings of the 28th International Conference on Machine Learning  2011.

[5] J. Johns  C. Painter-Wakeﬁeld  and R. Parr. Linear complementarity for regularized policy
evaluation and improvement. In Proceedings of the International Conference on Neural Infor-
mation Processing Systems  2010.

[6] A. Juditsky and A. Nemirovski. Optimization for Machine Learning  chapter First-Order Meth-

ods for Nonsmooth Convex Large-Scale Optimization. MIT Press  2011.

[7] J. Zico Kolter and A. Y. Ng. Regularization and feature selection in least-squares temporal
difference learning. In Proceedings of 27 th International Conference on Machine Learning 
2009.

[8] G. Konidaris  S. Osentoski  and PS Thomas. Value function approximation in reinforcement
learning using the fourier basis. In Proceedings of the Twenty-Fifth Conference on Artiﬁcial
Intelligence  2011.

[9] G. M. Korpelevich. The extragradient method for ﬁnding saddle points and other problems.

1976.

[10] H.R. Maei and R.S. Sutton. GQ (λ): A general gradient algorithm for temporal-difference
prediction learning with eligibility traces. In Proceedings of the Third Conference on Artiﬁcial
General Intelligence  pages 91–96  2010.

[11] S. Mahadevan and B. Liu. Sparse Q-learning with Mirror Descent.

Conference on Uncertainty in AI  2012.

In Proceedings of the

[12] A. Nedi´c and A. Ozdaglar. Subgradient methods for saddle-point problems. Journal of opti-

mization theory and applications  142(1):205–228  2009.

[13] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19:1574–1609  2009.

[14] Y. Nesterov.

Gradient methods for minimizing composite objective function.

www.optimization-online.org  2007.

[15] C. Painter-Wakeﬁeld and R. Parr. Greedy algorithms for sparse reinforcement learning.

International Conference on Machine Learning  2012.

In

In

[16] C. Painter-Wakeﬁeld and R. Parr. L1 regularized linear temporal difference learning. Technical

report  Duke CS Technical Report TR-2012-01  2012.

[17] M. Petrik  G. Taylor  R. Parr  and S. Zilberstein. Feature selection using regularization in ap-
proximate linear programs for Markov decision processes. In Proceedings of the International
Conference on Machine learning (ICML)  2010.

[18] J. Si and Y. Wang. Online learning control by association and reinforcement. IEEE Transac-

tions on Neural Networks  12:264–276  2001.

[19] R. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press  1998.
[20] R.S. Sutton  H.R. Maei  D. Precup  S. Bhatnagar  D. Silver  C. Szepesv´ari  and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approxi-
mation. In International Conference on Machine Learning  pages 993–1000  2009.

[21] J. Zico Kolter. The Fixed Points of Off-Policy TD. In Advances in Neural Information Pro-

cessing Systems 24  pages 2169–2177  2011.

9

,Eunho Yang
Pradeep Ravikumar
Genevera Allen
Zhandong Liu
Junbang Liang
Ming Lin
Vladlen Koltun