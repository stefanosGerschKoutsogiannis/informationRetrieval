2010,t-logistic regression,We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss  function. An efficient block coordinate descent optimization  scheme can be derived for estimating the parameters. Because of the  nature of the loss function  our algorithm is tolerant to label noise. Furthermore  unlike other algorithms which employ non-convex   loss functions  our algorithm is fairly robust to the choice of  initial values. We verify both these observations empirically on a  number of synthetic and real datasets.,t-Logistic Regression

Nan Ding2  S.V. N. Vishwanathan1 2

Departments of 1Statistics and 2Computer Science

Purdue University

ding10@purdue.edu  vishy@stat.purdue.edu

Abstract

We extend logistic regression by using t-exponential families which were intro-
duced recently in statistical physics. This gives rise to a regularized risk mini-
mization problem with a non-convex loss function. An efﬁcient block coordinate
descent optimization scheme can be derived for estimating the parameters. Be-
cause of the nature of the loss function  our algorithm is tolerant to label noise.
Furthermore  unlike other algorithms which employ non-convex loss functions 
our algorithm is fairly robust to the choice of initial values. We verify both these
observations empirically on a number of synthetic and real datasets.

1

Introduction

Many machine learning algorithms minimize a regularized risk [1]:

J(θ) = Ω(θ) + Remp(θ)  where Remp(θ) =

1
m

l(xi  yi  θ).

(1)

m�i=1

Here  Ω is a regularizer which penalizes complex θ; and Remp  the empirical risk  is obtained by
averaging the loss l over the training dataset {(x1  y1)  . . .   (xm  ym)}. In this paper our focus is on
binary classiﬁcation  wherein features of a data point x are extracted via a feature map φ and the
label is usually predicted via sign(�φ(x)  θ�). If we deﬁne the margin of a training example (x  y) as
u(x  y  θ) := y �φ(x)  θ�  then many popular loss functions for binary classiﬁcation can be written
as functions of the margin. Examples include1

(0 − 1 loss)
(Hinge Loss)
(Exponential Loss)
(Logistic Loss).

l(u) = 0 if u > 0 and 1 otherwise .
l(u) = max(0  1 − u)
l(u) = exp(−u)
l(u) = log(1 + exp(−u))

(2)
(3)
(4)
(5)
The 0 − 1 loss is non-convex and difﬁcult to handle; it has been shown that it is NP-hard to even
approximately minimize the regularized risk with the 0 − 1 loss [2]. Therefore  other loss functions
can be viewed as convex proxies of the 0 − 1 loss. Hinge loss leads to support vector machines
(SVMs)  exponential loss is used in Adaboost  and logistic regression uses the logistic loss.
Convexity is a very attractive property because it ensures that the regularized risk minimization
problem has a unique global optimum [3]. However  as was recently shown by Long and Servedio
[4]  learning algorithms based on convex loss functions are not robust to noise2. Intuitively  the
convex loss functions grows at least linearly with slope |l�(0)| as u ∈ (−∞  0)  which introduces
the overwhelming impact from the data with u � 0. There has been some recent and some not-
so-recent work on using non-convex loss functions to alleviate the above problem. For instance  a
recent manuscript by [5] uses the cdf of the Guassian distribution to deﬁne a non-convex loss.

1We slightly abuse notation and use l(u) to denote l(u(x  y  θ)).
2Although  the analysis of [4] is carried out in the context of boosting  we believe  the results hold for a

larger class of algorithms which minimize a regularized risk with a convex loss function.

1

loss

exp

Hinge

6

4

2

Logistic

In this paper  we continue this line of inquiry and propose a non-convex loss function which is
ﬁrmly grounded in probability theory. By
extending logistic regression from the ex-
ponential family to the t-exponential fam-
ily  a natural extension of exponential family
of distributions studied in statistical physics
[6–10]  we obtain the t-logistic regression
algorithm. Furthermore  we show that a
simple block coordinate descent scheme can
be used to solve the resultant regularized
risk minimization problem. Analysis of this
procedure also intuitively explains why t-
logistic regression is able to handle label
noise.
Our paper is structured as follows: In sec-
tion 2 we brieﬂy review logistic regression
especially in the context of exponential fam-
ilies. In section 3  we review t-exponential families  which form the basis for our proposed t-logistic
regression algorithm introduced in section 4. In section 5 we utilize ideas from convex multiplica-
tive programming to design an optimization strategy. Experiments that compare our new approach
to existing algorithms on a number of publicly available datasets are reported in section 6  and the
paper concludes with a discussion and outlook in section 7. Some technical details as well as extra
experimental results can be found in the supplementary material.

Figure 1: Some commonly used loss functions for binary
classiﬁcation. The 0-1 loss is non-convex. The hinge  expo-
nential  and logistic losses are convex upper bounds of the
0-1 loss.

0-1 loss

margin

-4

-2

0

2

4

2 Logistic Regression

Since we build upon the probabilistic underpinnings of logistic regression  we brieﬂy review some
salient concepts. Details can be found in any standard textbook such as [11] or [12]. Assume we are
given a labeled dataset (X  Y) = {(x1  y1)  . . .   (xm  ym)} with the xi’s drawn from some domain
X and the labels yi ∈ {±1}. Given a family of conditional distributions parameterized by θ  using
Bayes rule  and making a standard iid assumption about the data allows us to write

p(θ | X  Y) = p(θ)

p(yi| xi; θ)/p(Y | X) ∝ p(θ)

p(yi| xi; θ)

(6)

m�i=1

m�i=1

where p(Y | X) is clearly independent of θ. To model p(yi| xi; θ)  consider the conditional expo-
nential family of distributions

with the log-partition function g(θ | x) given by

p(y| x; θ) = exp (�φ(x  y)  θ� − g(θ | x))  

g(θ | x) = log (exp (�φ(x  +1)  θ�) + exp (�φ(x −1)  θ�)) .

(8)
2 φ(x)  and denote u = y �φ(x)  θ� then it is easy to see

If we choose the feature map φ(x  y) = y
that p(y| x; θ) is the logistic function

p(y| x; θ) =

exp(u/2) + exp(−u/2)
By assuming a zero mean isotropic Gaussian prior N (0  1√λ
logarithms  we can rewrite (6) as

exp(u/2)

=

1

.

1 + exp(−u)
I) for θ  plugging in (9)  and taking

(7)

(9)

− log p(θ | X  Y) =

λ
2 �θ�2 +

log (1 + exp (−yi �φ(xi)  θ�)) + const. .

(10)

Logistic regression computes a maximum a-posteriori (MAP) estimate for θ by minimizing (10) as
a function of θ. Comparing (1) and (10) it is easy to see that the regularizer employed in logistic
2 �θ�2  while the loss function is the negative log-likelihood − log p(y| x; θ)  which
regression is λ
thanks to (9) can be identiﬁed with the logistic loss (5).

m�i=1

2

3

t-Exponential family of Distributions

In this section we will look at generalizations of the log and exp functions which were ﬁrst intro-
duced in statistical physics [6–9]. Some extensions and machine learning applications were pre-
sented in [13]. In fact  a more general class of functions was studied in these publications  but for
our purposes we will restrict our attention to the so-called t-exponential and t-logarithm functions.
The t-exponential function expt for (0 < t < 2) is deﬁned as follows:
if t = 1
otherwise.

expt(x) :=�exp(x)

(11)

[1 + (1 − t)x]1/(1−t)

+

where (·)+ = max(·  0). Some examples are shown in Figure 2. Clearly  expt generalizes the usual
exp function  which is recovered in the limit as t → 1. Furthermore  many familiar properties of exp
are preserved: expt functions are convex  non-decreasing  non-negative and satisfy expt(0) = 1 [9].
But expt does not preserve one very important property of exp  namely expt(a + b) �= expt(a) ·
expt(b). One can also deﬁne the inverse of expt namely logt as

logt(x) :=�log(x)

�x1−t − 1� /(1 − t)

if t = 1
otherwise.

(12)

Similarly  logt(ab) �= logt(a) + logt(b). From Figure 2  it is clear that expt decays towards 0 more
slowly than the exp function for 1 < t < 2. This important property leads to a family of heavy
tailed distributions which we will later exploit.

t = 1.5

exp(x)

t = 0.5
t → 0

expt
7
6
5
4
3
2
1

-3 -2 -1 0 1 2

x

logt

2
1
0
-1
-2
-3

t → 0

t = 0.5

log(x)

t = 1.5

1 2 3 4 5 6 7

x

t = 1 (logistic)

loss

t = 1.3
t = 1.6
t = 1.9

0-1 loss

6

4

2

-4

-2

0

2

4

margin

Figure 2: Left: expt and Middle: logt for various values of t indicated. The right ﬁgure depicts the
t-logistic loss functions for different values of t. When t = 1  we recover the logistic loss

Analogous to the exponential family of distributions  the t-exponential family of distributions is
deﬁned as [9  13]:

p(x; θ) := expt (�φ(x)  θ� − gt(θ)) .

(13)
A prominent member of the t-exponential family is the Student’s-t distribution [14]. Just like in the
exponential family case  gt the log-partition function ensures that p(x; θ) is normalized. However 
no closed form solution exists for computing gt exactly in general. A closely related distribution 
which often appears when working with t-exponential families is the so-called escort distribution
[9  13]:

qt(x; θ) := p(x; θ)t/Z(θ)

(14)

where Z(θ) =� p(x; θ)tdx is the normalizing constant which ensures that the escort distribution

integrates to 1.
Although gt(θ) is not the cumulant function of the t-exponential family  it still preserves convexity.
In addition  it is very close to being a moment generating function
∇θgt(θ) = Eqt(x;θ) [φ(x)] .

(15)
The proof is provided in the supplementary material. A general version of this result appears as
Lemma 3.8 in Sears [13] and a version specialized to the generalized exponential families appears
as Proposition 5.2 in [9]. The main difference from ∇θg(θ) of the normal exponential family is that
now ∇θgt(θ) is equal to the expectation of its escort distribution qt(x; θ) instead of p(x; θ).

3

4 Binary Classiﬁcation with the t-exponential Family

In t-logistic regression we model p(y| x; θ) via a conditional t-exponential family distribution

p(y| x; θ) = expt (�φ(x  y)  θ� − gt(θ | x))  
where 1 < t < 2  and compute the log-partition function gt by noting that

(16)

expt (�φ(x  +1)  θ� − gt(θ | x)) + expt (�φ(x −1)  θ� − gt(θ | x)) = 1.

(17)
Even though no closed form solution exists  one can compute gt given θ and x using numerical
techniques efﬁciently.
The Student’s-t distribution can be regarded as a counterpart of the isotropic Gaussian prior in the
t-exponential family [14]. Recall that a one dimensional Student’s-t distribution is given by

St(x|µ  σ  v) =

(18)
where Γ(·) denotes the usual Gamma function and v > 1 so that the mean is ﬁnite. If we select t
satisfying −(v + 1)/2 = 1/(1 − t) and denote 

 

Γ((v + 1)/2)

√vπΓ(v/2)σ1/2�1 +

(x − µ)2

vσ �−(v+1)/2

then by some simple but tedious calculation (included in the supplementary material)

Ψ =� Γ((v + 1)/2)

√vπΓ(v/2)σ1/2�−2/(v+1)
St(x|µ  σ  v) = expt(−˜λ(x − µ)2/2 − ˜gt)

 

where

˜λ =

and

˜gt =

2Ψ

(t − 1)vσ

Ψ − 1
t − 1

.

(19)

(20)

Therefore  we work with the Student’s-t prior in our setting:

p(θ) =

p(θj) =

St(θj|0  2/λ  (3 − t)/(t − 1)).

d�j=1

d�j=1

Here  the degree of freedom for Student’s-t distribution is chosen such that it also belongs to the
expt family  which in turn yields v = (3 − t)/(t − 1). The Student’s-t prior is usually preferred to
the Gaussian prior when the underlying distribution is heavy-tailed. In practice  it is known to be a
robust3 alternative to the Gaussian distribution [16  17].
As before  if we let φ(x  y) = y
2 φ(x) and plot the negative log-likelihood − log p(y| x; θ)  then we
no longer obtain a convex loss function (see Figure 2). Similarly  − log p(θ) is no longer convex
when we use the Student’s-t prior. This makes optimizing the regularized risk challenging  therefore
we employ a different strategy.
Since logt is also a monotonically increasing function  instead of working with log  we can equiva-
lently work with the logt function (12) and minimize the following objective function:

ˆJ(θ) = − logt p(θ)

=

1

p(yi| xi; θ)/p(Y | X)

m�i=1
t − 1�p(θ)
p(yi| xi; θ)/p(Y | X)�1−t
m�i=1
j /2 − ˜gt)�
m�i=1�1 + (1 − t)(� yi
�
�

d�j=1�1 + (1 − t)(−˜λθ2
�
��
d�j=1

m�i=1

rj(θ)

rj (θ)

=

li(θ) + const.

where p(Y | X) is independent of θ. Using (13)  (18)  and (11)  we can further write
ˆJ(θ) ∝

2

+

1
1 − t

 

(21)

φ(xi)  θ� − gt(θ | xi))�
��
�

li(θ)

+const. .

(22)

3There is no unique deﬁnition of robustness. For example  one of the deﬁnitions is through the outlier-

proneness [15]: p(θ | X  Y  xn+1  yn+1) → p(θ | X  Y) as xn+1 → ∞.

4

Since t > 1  it is easy to see that rj(θ) > 0 is a convex function of θ. On the other hand  since gt
is convex and t > 1 it follows that li(θ) > 0 is also a convex function of θ. In summary  ˆJ(θ) is
a product of positive convex functions. In the next section we will present an efﬁcient optimization
strategy for dealing with such problems.

5 Convex Multiplicative Programming

In convex multiplicative programming [18] we are interested in the following optimization problem:

min

θ

P(θ) �

zn(θ)

s.t. θ ∈ Rd 

(23)

N�n=1

where zn(θ) are positive convex functions. Clearly  (22) can be identiﬁed with (23) by setting
N = d+m and identifying zn(θ) = rn(θ) for n = 1  . . .   d and zn+d(θ) = ln(θ) for n = 1  . . .   m.
The optimal solutions to the problem (23) can be obtained by solving the following parametric
problem (see Theorem 2.1 of Kuno et al. [18]):

ξn ≥ 1.

min

min

ξ

θ

N�n=1

MP(θ  ξ) �

ln(θ) = −� yn

The optimization problem in (24) is very reminiscent of logistic regression. In logistic regression 

N�n=1
ξnzn(θ) s.t. θ ∈ Rd  ξ > 0 
2 φ(xn)  θ� − gt(θ | xn)�.
2 φ(xn)  θ� + g(θ | xn)  while here ln(θ) = 1 + (1− t)�� yn

The key difference is that in t-logistic regression each data point xn has a weight (or inﬂuence) ξn
associated with it.
Exact algorithms have been proposed for solving (24) (for instance  [18]). However  the computa-
tional cost of these algorithms grows exponentially with respect to N which makes them impractical
for our purposes. Instead  we apply a block coordinate descent based method. The main idea is to
minimize (24) with respect to θ and ξ separately.
ξ-Step: Assume that θ is ﬁxed  and denote ˜zn = zn(θ) to rewrite (24) as:

(24)

min

ξ

N�n=1

N�n=1

ξn ˜zn s.t.

ξ > 0 

ξn ≥ 1.

(25)

Since the objective function is linear in ξ and the feasible region is a convex set  (25) is a con-
vex optimization problem. By introducing a non-negative Lagrange multiplier γ ≥ 0  the partial
Lagrangian and its gradient with respect to ξn� can be written as

L(ξ  γ) =

ξn ˜zn + γ ·�1 −

ξn�

N�n=1

N�n=1

∂
∂ξn�

L(ξ  γ) = ˜zn� − γ �n�=n�

ξn.

(26)

(27)

Setting the gradient to 0 obtains γ =

˜zn��n�=n� ξn
K.K.T. conditions [3]  we can conclude that�N

. Since ˜zn� > 0  it follows that γ cannot be 0. By the
n=1 ξn = 1. This in turn implies that γ = ˜zn� ξn� or

(ξ1  . . .   ξN ) = (γ/˜z1  . . .   γ/˜zN )  with γ =

1
N

˜z
n .

(28)

Recall that ξn in (24) is the weight (or inﬂuence) of each term zn(θ). The above analysis shows
that γ = ˜zn(θ)ξn remains constant for all n. If ˜zn(θ) becomes very large then its inﬂuence ξn
is reduced. Therefore  points with very large loss have their inﬂuence capped and this makes the
algorithm robust to outliers.
θ-Step: In this step we ﬁx ξ > 0 and solve for the optimal θ. This step is essentially the same as
logistic regression  except that each component has a weight ξ here.

N�n=1

min

θ

N�n=1

ξnzn(θ) s.t. θ ∈ Rd .

5

(29)

This is a standard unconstrained convex optimization problem which can be solved by any off the
shelf solver. In our case we use the L-BFGS Quasi-Newton method. This requires us to compute
the gradient ∇θzn(θ):
for n = 1  . . .   d ∇θzn(θ) = ∇θrn(θ) = (t − 1)˜λθn · en
for n = 1  . . .   m ∇θzn+d(θ) = ∇θln(θ) = (1 − t)� yn
= (1 − t)� yn

φ(xn) − ∇θgt(θ | xn)�
φ(xn) − Eqt(yn| xn;θ)� yn

where en denotes the d dimensional vector with one at the n-th coordinate and zeros elsewhere (n-th
unit vector). qt(y| x; θ) is the escort distribution of p(y| x; θ) (16):

φ(xn)��  

2

2

2

(30)

qt(y| x; θ) =

p(y| x; θ)t

p(+1| x; θ)t + p(−1| x; θ)t .

The objective function is monotonically decreasing and is guaranteed to converge to a stable point
of P(θ). We include the proof in the supplementary material.

6 Experimental Evaluation

Our experimental evaluation is designed to answer four natural questions: 1) How does the gener-
alization capability (measured in terms of test error) of t-logistic regression compare with existing
algorithms such as logistic regression and support vector machines (SVMs) both in the presence and
absence of label noise? 2) Do the ξ variables we introduced in the previous section have a natu-
ral interpretation? 3) How much overhead does t-logistic regression incur as compared to logistic
regression? 4) How sensitive is the algorithm to initialization? The last question is particularly
important given that the algorithm is minimizing a non-convex loss.
To answer the above questions empirically we use six datasets  two of which are synthetic. The
Long-Servedio dataset is an artiﬁcially constructed dataset to show that algorithms which minimize
a differentiable convex loss are not tolerant to label noise Long and Servedio [4]. The examples have
21 dimensions and play one of three possible roles: large margin examples (25%  x1 2 ... 21 = y);
pullers (25%  x1 ... 11 = y  x12 ... 21 = −y); and penalizers (50%  Randomly select and set 5 of
the ﬁrst 11 coordinates and 6 out of the last 10 coordinates to y  and set the remaining coordinates
to −y). The Mease-Wyner is another synthetic dataset to test the effect of label noise. The input x
is a 20-dimensional vector where each coordinate is uniformly distributed on [0  1]. The label y is
+1 if�5
j=1 xj ≥ 2.5 and −1 otherwise [19]. In addition  we also test on Mushroom  USPS-N (9
vs. others)  Adult  and Web datasets  which are often used to evaluate machine learning algorithms
(see Table 1 in supplementary material for details).
For simplicity  we use the identity feature map φ(x) = x in all our experiments  and set t ∈
{1.3  1.6  1.9} for t-logistic regression. Our comparators are logistic regression  linear SVMs4  and
an algorithm (the probit) which employs the probit loss  L(u) = 1 − erf (2u)  used in Brown-
Boost/RobustBoost [5]. We use the L-BFGS algorithm [21] for the θ-step in t-logistic regression.
L-BFGS is also used to train logistic regression and the probit loss based algorithms. Label noise is
added by randomly choosing 10% of the labels in the training set and ﬂipping them; each dataset is
tested with and without label noise. We randomly select and hold out 30% of each dataset as a vali-
dation set and use the rest of the 70% for 10-fold cross validation. The optimal parameters namely λ
for t-logistic and logistic regression and C for SVMs is chosen by performing a grid search over the

parameter space�2−7 −6 ... 7� and observing the prediction accuracy over the validation set. The

convergence criterion is to stop when the change in the objective function value is less than 10−4.
All code is written in Matlab  and for the linear SVM we use the Matlab interface of LibSVM [22].
Experiments were performed on a Qual-core machine with Dual 2.5 Ghz processor and 32 Gb RAM.

In Figure 3  we plot the test error with and without label noise. In the latter case  the test error of
t-logistic regression is very similar to logistic regression and Linear SVM (with 0% test error in

4We also experimented with RampSVM [20]  however  the results are worser than the other algorithms. We

therefore report these results in the supplementary material.

6

)

%

(
r
o
r
r
E
t
s
e
T

)

%

(
r
o
r
r
E
t
s
e
T

32

24

16

8

0

6.0

4.5

3.0

1.5

0.0

6.0

4.5

3.0

1.5

0.0

16.8

16.0

15.2

14.4

logis.

t=1.3 t=1.6 t=1.9 probit SVM

logis.

t=1.3 t=1.6 t=1.9 probit SVM

1.2

0.9

0.6

0.3

0.0

3.2

2.4

1.6

0.8

0.0

logis.

t=1.3 t=1.6 t=1.9 probit SVM

Figure 3: The test error rate of various algorithms on six datasets (left to right  top: Long-Servedio 
Mease-Wyner  Mushroom; bottom: USPS-N  Adult  Web) with and without 10% label noise. All
algorithms are initialized with θ = 0. The blue (light) bar denotes a clean dataset while the magenta
(dark) bar are the results with label noise added. Also see Table 3 in the supplementary material.

Long-Servedio and Mushroom datasets)  with a slight edge on some datasets such as Mease-Wyner.
When label noise is added  t-logistic regression (especially with t = 1.9) shows signiﬁcantly5 better
performance than all the other algorithms on all datasets except the USPS-N  where it is marginally
outperformed by the probit.
To obtain Figure 4 we used the noisy version of the datasets  chose one of the 10 folds used in the
previous experiment  and plotted the distribution of the 1/z ∝ ξ obtained after training with t = 1.9.
To distinguish the points with noisy labels we plot them in cyan while the other points are plotted in
red. Analogous plots for other values of t can be found in the supplementary material. Recall that ξ
denotes the inﬂuence of a point. One can clearly observe that the ξ of the noisy data is much smaller
than that of the clean data  which indicates that the algorithm is able to effectively identify these
points and cap their inﬂuence. In particular  on the Long-Servedio dataset observe the 4 distinct
spikes. From left to right  the ﬁrst spike corresponds to the noisy large margin examples  the second
spike represents the noisy pullers  the third spike denotes the clean pullers  while the rightmost spike
corresponds to the clean large margin examples. Clearly  the noisy large margin examples and the
noisy pullers are assigned a low value of ξ thus capping their inﬂuence and leading to the perfect
classiﬁcation of the test set. On the other hand  logistic regression is unable to discriminate between
clean and noisy training samples which leads to bad performance on noisy datasets.
Detailed timing experiments can be found in Table 4 in the supplementary material. In a nutshell 
t-logistic regression takes longer to train than either logistic regression or the probit. The reasons
are not difﬁcult to see. First  there is no closed form expression for gt(θ | x). We therefore resort
to pre-computing it at some ﬁxed locations and using a spline method to interpolate values at other
locations. Second  since the objective function is not convex several iterations of the ξ and θ steps
might be needed. Surprisingly  the L-BFGS algorithm  which is not designed to optimize non-
convex functions  is able to minimize (22) directly in many cases. When it does converge  it is often
faster than the convex multiplicative programming algorithm. However  on some cases (as expected)
it fails to ﬁnd a direction of descent and exits. A common remedy for this is the bundle L-BFGS
with a trust-region approach. [21]
Given that the t-logistic objective function is non-convex  one naturally worries about how different
initial values affect the quality of the ﬁnal solution. To answer this question  we initialized the
algorithm with 50 different randomly chosen θ ∈ [−0.5  0.5]d  and report test performances of
the various solutions obtained in Figure 5. Just like logistic regression which uses a convex loss
and hence converges to the same solution independent of the initialization  the solution obtained

5We provide the signiﬁcance test results in Table 2 of supplementary material.

7

300

240

180

120

60

y
c
n
e
u
q
e
r
F

0
0.0

0.2

0.4

0.6

0.8

1.0

600

450

300

150

y
c
n
e
u
q
e
r
F

60

45

30

15

0
0.0

1200

900

600

300

1000

800

600

400

200

0.2

0.4

0.6

0.8

1.0

0
0.0

0.2

0.4

0.6

0.8

1.0

8000

6000

4000

2000

0
0.0

0.2

0.4

0.6

0.8

1.0

0
0.0

0.2

0.4

0.6

0.8

1.0

0
0.0

0.2

0.4

0.6

0.8

1.0

ξ

ξ

ξ

Figure 4: The distribution of ξ obtained after training t-logistic regression with t = 1.9 on datasets
with 10% label noise. Left to right  top: Long-Servedio  Mease-Wyner  Mushroom; bottom: USPS-
N  Adult  Web. The red (dark) bars (resp. cyan (light) bars) indicate the frequency of ξ assigned to
points without (resp. with) label noise.

by t-logistic regression seems fairly independent of the initial value of θ. On the other hand  the
performance of the probit ﬂuctuates widely with different initial values of θ.

probit

t = 1.9

t = 1.6

t = 1.3

logistic

probit

t = 1.9

t = 1.6

t = 1.3

logistic

0

10

20

30

0

10

20

30

40

0.00

0.15

0.30

0.45

3.0

4.5

6.0

7.5
TestError(%)

9.0

15

18

21

TestError(%)

24

1.5

2.0

2.5

3.0

3.5

TestError(%)

Figure 5: The Error rate by different initialization. Left to right  top: Long-Servedio  Mease-Wyner 
Mushroom; bottom: USPS-N  Adult  Web.

7 Discussion and Outlook

In this paper  we generalize logistic regression to t-logistic regression by using the t-exponential
family. The new algorithm has a probabilistic interpretation and is more robust to label noise. Even
though the resulting objective function is non-convex  empirically it appears to be insensitive to ini-
tialization. There are a number of avenues for future work. On Long-Servedio experiment  if the
label noise is increased signiﬁcantly beyond 10%  the performance of t-logistic regression may de-
grade (see Fig. 6 in supplementary materials). Understanding and explaining this issue theoretically
and empirically remains an open problem. It will be interesting to investigate if t-logistic regression
can be married with graphical models to yield t-conditional random ﬁelds. We will also focus on
better numerical techniques to accelerate the θ-step  especially a faster way to compute gt.

8

References
[1] Choon Hui Teo  S. V. N. Vishwanthan  Alex J. Smola  and Quoc V. Le. Bundle methods for

regularized risk minimization. J. Mach. Learn. Res.  11:311–365  January 2010.

[2] S. Ben-David  N. Eiron  and P.M. Long. On the difﬁculty of approximately maximizing agree-

ments. J. Comput. System Sci.  66(3):496–514  2003.

[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  Cambridge 

England  2004.

[4] Phil Long and Rocco Servedio. Random classiﬁcation noise defeats all convex potential boost-

ers. Machine Learning Journal  78(3):287–304  2010.

[5] Yoav Freund. A more robust boosting algorithm. Technical Report Arxiv/0905.2138  Arxiv 

May 2009.

[6] J. Naudts. Deformed exponentials and logarithms in generalized thermostatistics. Physica A 

316:323–334  2002. URL http://arxiv.org/pdf/cond-mat/0203489.

[7] J. Naudts. Generalized thermostatistics based on deformed exponential and logarithmic func-

tions. Physica A  340:32–40  2004.

[8] J. Naudts. Generalized thermostatistics and mean-ﬁeld theory. Physica A  332:279–300  2004.
[9] J. Naudts. Estimators  escort proabilities  and φ-exponential families in statistical physics.

Journal of Inequalities in Pure and Applied Mathematics  5(4)  2004.

[10] C. Tsallis. Possible generalization of boltzmann-gibbs statistics. J. Stat. Phys.  52  1988.
[11] Christopher Bishop. Pattern Recognition and Machine Learning. Springer  2006.
[12] Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The Elements of Statistical Learning.

Springer  New York  2 edition  2009.

[13] Timothy D. Sears. Generalized Maximum Entropy  Convexity  and Machine Learning. PhD

thesis  Australian National University  2008.

[14] Andre Sousa and Constantino Tsallis. Student’s t- and r-distributions: Uniﬁed derivation from

an entropic variational principle. Physica A  236:52–57  1994.

[15] A O’hagan. On outlier rejection phenomena in bayes inference. Royal Statistical Society  41

(3):358–367  1979.

[16] Kenneth L. Lange  Roderick J. A. Little  and Jeremy M. G. Taylor. Robust statistical modeling
using the t distribution. Journal of the American Statistical Association  84(408):881–896 
1989.

[17] J. Vanhatalo  P. Jylanki  and A. Vehtari. Gaussian process regression with student-t likelihood.

In Neural Information Processing System  2009.

[18] Takahito Kuno  Yasutoshi Yajima  and Hiroshi Konno. An outer approximation method for
minimizing the product of several convex functions on a convex set. Journal of Global Opti-
mization  3(3):325–335  September 1993.

[19] David Mease and Abraham Wyner. Evidence contrary to the statistical view of boosting. J.

Mach. Learn. Res.  9:131–156  February 2008.

[20] R. Collobert  F.H. Sinz  J. Weston  and L. Bottou. Trading convexity for scalability. In W.W.
Cohen and A. Moore  editors  Machine Learning  Proceedings of the Twenty-Third Interna-
tional Conference (ICML 2006)  pages 201–208. ACM  2006.

[21] J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research.

Springer  1999.

[22] C.C. Chang and C.J. Lin. LIBSVM: a library for support vector machines  2001. Software

available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

[23] Fabian Sinz. UniverSVM: Support Vector Machine with Large Scale CCCP Functionality 
2006. Software available at http://www.kyb.mpg.de/bs/people/fabee/universvm.
html.

9

,Cong Han Lim