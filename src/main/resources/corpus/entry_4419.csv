2010,Probabilistic Deterministic Infinite Automata,We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop and sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model  given a finite training sequence  can be interpreted as averaging over multiple PDFAs of varying structure  where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.,Probabilistic Deterministic Inﬁnite Automata

David Pfau

{pfau@neurotheory {bartlett fwood}@stat}.columbia.edu

Columbia University  New York  NY 10027  USA

Nicholas Bartlett

Frank Wood

Abstract

We propose a novel Bayesian nonparametric approach to learning with probabilis-
tic deterministic ﬁnite automata (PDFA). We deﬁne and develop a sampler for a
PDFA with an inﬁnite number of states which we call the probabilistic determin-
istic inﬁnite automata (PDIA). Posterior predictive inference in this model  given
a ﬁnite training sequence  can be interpreted as averaging over multiple PDFAs of
varying structure  where each PDFA is biased towards having few states. We sug-
gest that our method for averaging over PDFAs is a novel approach to predictive
distribution smoothing. We test PDIA inference both on PDFA structure learning
and on both natural language and DNA data prediction tasks. The results suggest
that the PDIA presents an attractive compromise between the computational cost
of hidden Markov models and the storage requirements of hierarchically smoothed
Markov models.

1

Introduction

The focus of this paper is a novel Bayesian framework for learning with probabilistic deterministic
ﬁnite automata (PDFA) [9]. A PDFA is a generative model for sequential data (PDFAs are reviewed
in Section 2).
Intuitively a PDFA is similar to a hidden Markov model (HMM) [10] in that it
consists of a set of states  each of which when visited emits a symbol according to an emission
probability distribution. It differs from an HMM in how state-to-state transitions occur; transitions
are deterministic in a PDFA and nondeterministic in an HMM.
In our framework for learning with PDFAs we specify a prior over the parameters of a single large
PDFA that encourages state reuse. The inductive bias introduced by the PDFA prior provides a soft
constraint on the number of states used to generate the data. We take the limit as the number of states
becomes inﬁnite  yielding a model we call the probabilistic deterministic inﬁnite automata (PDIA).
Given a ﬁnite training sequence  the PDIA posterior distribution is an inﬁnite mixture of PDFAs.
Samples from this distribution form a ﬁnite sample approximation to this inﬁnite mixture  and can
be drawn via Markov chain Monte Carlo (MCMC) [6]. Using such a mixture we can average over
our uncertainty about the model parameters (including state cardinality) in a Bayesian way during
prediction and other inference tasks. We ﬁnd that averaging over a ﬁnite number of PDFAs trained
on naturalistic data leads to better predictive performance than using a single “best” PDFA.
We chose to investigate learning with PDFAs because they are intermediate in expressive power be-
tween HMMs and ﬁnite-order Markov models  and thus strike a good balance between generaliza-
tion performance and computational efﬁciency. A single PDFA is known to have relatively limited
expressivity. We argue that a ﬁnite mixture of PDFAs has greater expressivity than that of a single
PDFA but is not as expressive as a probabilistic nondeterministic ﬁnite automata (PNFA)1. A PDIA
is clearly highly expressive; an inﬁnite mixture over the same is even more so. Even though ours is
a Bayesian approach to PDIA learning  in practice we only ever deal with a ﬁnite approximation to
the full posterior and thus limit our discussion to ﬁnite mixtures of PDFAs.

1PNFAs with no ﬁnal probability are equivalent to hidden Markov models [3]

1

While model expressivity is a concern  computational considerations often dominate model choice.
We show that prediction in a trained mixture of PDFAs can have lower asymptotic cost than forward
prediction in the PNFA/HMM class of models. We also present evidence that averaging over PDFAs
gives predictive performance superior to HMMs trained with standard methods on naturalistic data.
We ﬁnd that PDIA predictive performance is competitive with that of ﬁxed-order  smoothed Markov
models with the same number of states. While sequence learning approaches such as the HMM
and smoothed Markov models are well known and now highly optimized  our PDIA approach to
learning is novel and is amenable to future improvement.
Section 2 reviews PDFAs  Section 3 introduces Bayesian PDFA inference  Section 4 presents ex-
perimental results on DNA and natural language  and Section 5 discusses related work on PDFA
induction and the theoretical expressive power of mixtures of PDFAs. In Section 6 we discuss ways
in which PDIA predictive performance might be improved in future research.

2 Probabilistic Deterministic Finite Automata

A PDFA is formally deﬁned as a 5-tuple M = (Q  Σ  δ  π  q0)  where Q is a ﬁnite set of states  Σ is a
ﬁnite alphabet of observable symbols  δ : Q×Σ → Q is the transition function from a state/symbol
pair to the next state  π : Q× Σ → [0  1] is the probability of the next symbol given a state and q0 is
the initial state.2 Throughout this paper we will use i to index elements of Q  j to index elements of
Σ  and t to index elements of an observed string. For example  δij is shorthand for δ(qi  σj)  where
qi ∈ Q and σj ∈ Σ.
Given a state qi  the probability that the next symbol takes the value σj is given by π(qi  σj). We
use the shorthand πqi for the state-speciﬁc discrete distribution over symbols for state qi. We can
also write σ|qi ∼ πqi where σ is a random variable that takes values in Σ. Given a state qi and a
symbol σj  however  the next state qi(cid:48) is deterministic: qi(cid:48) = δ(qi  σj). Generating from a PDFA
involves ﬁrst generating a symbol stochastically given the state the process is in: xt|ξt ∼ πξt where
ξt ∈ Q is the state at time t. Next  given ξt and xt transitioning deterministically to the next state:
ξt+1 = δ(ξt  xt). This is the reason for the confusing “probabilistic deterministic” name for these
models. Turning this around  given data  q0  and δ  there is no uncertainty about the path through
the states. This is a primary source of computational savings relative to HMMs.
PDFAs are more general than nth-order Markov models (i.e. m-gram models  m = n + 1)  but less
expressive than hidden Markov models (HMMs)[3]. For the case of nth-order Markov models  we
can construct a PDFA with one state per sufﬁx x1x2 . . . xn. Given a state and a symbol xn+1  the
unique next state is the one corresponding to the sufﬁx x2 . . . xn+1. Thus nth-order Markov models
are a subclass of PDFAs with O(|Σ|n) states. For an HMM  given data and an initial distribution
over states  there is a posterior probability for every path through the state space. PDFAs are those
HMMs for which  given a unique start state  the posterior probability over paths is degenerate at a
single path. As we explain in Section 5  mixtures of PDFAs are strictly more expressive than single
PDFAs  but still less expressive than PNFAs.

3 Bayesian PDFA Inference

We start our description of Bayesian PDFA inference by deﬁning a prior distribution over the pa-
rameters of a ﬁnite PDFA. We then show how to analytically marginalize nuisance parameters out
of the model and derive a Metropolis-Hastings sampler for posterior inference using the resulting
collapsed representation. We discuss the limit of our model as the number of states in the PDFA goes
to inﬁnity. We call this limit the probabilistic deterministic inﬁnite automaton (PDIA). We develop
a PDIA sampler that carries over from the ﬁnite case in a natural way.

3.1 A PDFA Prior

We assume that the set of states Q  set of symbols Σ  and initial state q0 of a PDFA are known but
that the transition and emission functions are unknown. The PDFA prior then consists of a prior
over both the transition function δ and the emission probability function π. In the ﬁnite case δ and

2In general q0 may be replaced by a distribution over initial states.

2

π are representable as ﬁnite matrices  with one column per element of Σ and one row per element
of Q. For each column j (j co-indexes columns and set elements) of the transition matrix δ  our
prior stipulates that the elements of that column are i.i.d. draws from a discrete distribution φj over
Q  that is  δij ∼ [φ1  . . .   φ|Σ|]  0 ≤ i ≤ |Q| − 1. The φj represent transition tendencies given
a symbol  if the ith element of φj is large then state qi is likely to be transitioned to anytime the
last symbol was σj. The φj’s are themselves given a shared Dirichlet prior with parameters αµ 
where α is a concentration and µ is a template transition probability vector. If the ith element of µ
is large then the ith state is likely to be transitioned to regardless of the emitted symbol. We place
a uniform Dirichlet prior on µ itself  with γ total mass and average over µ during inference. This
hierarchical Dirichlet construction encourages both general and context speciﬁc state reuse. We also
place a uniform Dirichlet prior over the per-state emission probabilities πqi with β total mass which
smooths emission distribution estimates. Formally:

µ|γ |Q| ∼ Dir (γ/|Q|  . . .   γ/|Q|)
φj|α  µ ∼ Dir(αµ)
πqi|β |Σ| ∼ Dir(β/|Σ|  . . .   β/|Σ|)

δij ∼ φj

(1)
(2)

where 0 ≤ i ≤ |Q| − 1 and 1 ≤ j ≤ |Σ|. Given a sample from this model we can run the PDFA
to generate a sequence of T symbols. Using ξt to denote the state of the PDFA at position t in the
sequence:

ξ0 = q0 

x0 ∼ πq0 

ξt = δ(ξt−1  xt−1) 

xt ∼ πξt

We choose this particular inductive bias  with transitions tied together within a column of δ  because
we wanted the most recent symbol emission to be informative about what the next state is. If we
instead had a single Dirichlet prior over all elements of δ  transitions to a few states would be highly
likely no matter the context and those states would dominate the behavior of the automata. If we
tied together rows of δ instead of columns  being in a particular state would tell us more about the
sequence of states we came from than the symbols that got us there.
Note that this prior stipulates a fully connected PDFA in which all states may transition to all others
and all symbols may be emitted from each state. This is slightly different that the canonical ﬁnite
state machine literature where sparse connectivity is usually the norm.

3.2 PDFA Inference

Given observational data  we are interested in learning a posterior distribution over PDFAs. We do
this by GIbbs sampling the transition matrix δ with π and φj integrated out. To start inference we
need the likelihood function for a ﬁxed PDFA; it is given by

p(x0:T|π  δ) = π(ξ0  x0)

π(ξt  xt).

T(cid:89)

t=1

matrix δ as cij = (cid:80)T

Remember that ξt|ξt−1  xt−1 is deterministic given the transition function δ. We can marginalize π
out of this expression and express the likelihood of the data in a form that depends only on the counts
of symbols emitted from each state. Deﬁne the count matrix c for the sequence x0:T and transition
t=0 Iij(ξt  xt)  where Iij(ξt  xt) is an indicator function for the automaton
being in state qi when it generates xt  i.e. ξt = qi and xt = σj. This matrix c = [cij] gives the
number of times each symbol is emitted from each state. Due to multinomial-Dirichlet conjugacy
we can express the probability of a sequence given the transition function δ  the count matrix c and
β:

p(x0:T|δ  c  β) =

p(x0:T|π  δ)p(π|β)dπ =

(3)

(cid:90)

|Q|−1(cid:89)

i=0

Γ(β)
Γ( β|Σ|)|Σ|

(cid:81)|Σ|
Γ(β +(cid:80)|Σ|

j=1 Γ( β|Σ| + cij)
j=1 cij)

If the transition matrix δ is observed we have a closed-form expression for its likelihood given µ
with all φj’s marginalized out. Let vij be the number of times state qi is transitioned to given that
σj was the last symbol emitted  i.e. vij is the number of times δi(cid:48)j = qi for all states i(cid:48) in the column

3

j. The marginal likelihood of δ in terms of µ is then:

p(δ|µ  α) =

p(δ|φ)p(φ|µ  α)dφ =

(cid:90)

|Σ|(cid:89)

j=1

(cid:81)|Q|−1

Γ(α)

i=0 Γ(αµi)

(cid:81)|Q|−1

i=0 Γ(αµi + vij)

Γ(α + |Q|)

(4)

We perform posterior inference in the ﬁnite model by sampling elements of δ and the vector µ. One
can sample δij given the rest of the matrix δ−ij using

p(δij|δ−ij  x0:T   µ  α) ∝ p(x0:T|δij  δ−ij)p(δij|δ−ij  µ  α)

(5)

Both terms on the right hand side of this equation have closed-form expressions  the ﬁrst given in
(3). The second can be found from (4) and is

P (δij = qi(cid:48)|δ−ij  α  µ) = αµi(cid:48) + vi(cid:48)j
α + |Q| − 1

(6)
where vi(cid:48)j is the number of elements in column j equal to qi(cid:48) excluding δij. As |Q| is ﬁnite 
we compute (5) for all values of δij and normalize to produce the required conditional probability
distribution.
Note that in (3)  the count matrix c may be profoundly impacted by changing even a single element
of δ. The values in c depend on the speciﬁc sequence of states the automata used to generate x.
Changing the value of a single element of δ affects the state trajectory the PDFA must follow to
generate x0:T . Among other things this means that some elements of c that were nonzero may
become zero  and vice versa.
We can reduce the computational cost of inference by deleting transitions δij for which the corre-
sponding counts cij become 0. In practical sampler implementations this means that one need not
even represent transitions corresponding to zero counts. The likelihood of the data (3) does not de-
pend on the value of δij if symbol σj is never emitted while the machine is in state qi. In this case
sampling from (5) is the same as sampling without conditioning on the data at all. Thus  if while
sampling we change some transition that renders cij = 0 for some values for each of i and j  we can
delete δij until another transition is changed such that cij becomes nonzero again  when we sample
δij anew. Under the marginal joint distribution of a column of δ the row entries in that column are
exchangeable  and so deleting an entry of δ has the same effect as marginalizing it out. When all
δij for some state qi are marginalized out  we can say the state itself is marginalized out. When
we delete an element from a column of δ  we replace the |Q| − 1 in the denominator of (6) with
I(vij (cid:54)= 0)  the number of entries in the jth column of δ that are not marginalized
D+
i=0
out yielding

j =(cid:80)|Q|−1

P (δij = qi(cid:48)|δ−ij  α  µ) = αµi(cid:48) + vi(cid:48)j
α + D+
j

.

(7)

If when sampling δij it is assigned it a state qi(cid:48) such that some ci(cid:48)j(cid:48) which was zero is now nonzero 
we simply reinstantiate δi(cid:48)j(cid:48) by drawing from (7) and update D+
j(cid:48). When sampling a single δij
there can be many such transitions as the path through the machine dictated by x0:T may use many
transitions in δ that were deleted. In this case we update incrementally  increasing D+
j and vij as we
go.
While it is possible to construct a Gibbs sampler using (5) in this collapsed representation  such a
sampler requires a Monte Carlo integration over a potentially large subset of the marginalized-out
transitions in δ  which may be costly. A simpler strategy is to pretend that all entries of δ exist but
are sampled in a “just-in-time” manner. This gives rise to a Metropolis Hastings (MH) sampler for δ
where the proposed value for δij is either one of the instantiated states or any one of the equivalent
marginalized out states. Any time any marginalized out element of δ is required we can pretend as
if we had just sampled its value  and we know that because its value had no effect on the likelihood
of the data  we know that it would have been sampled directly from (7). It is in this sense that all
marginalized out states are equivalent – we known nothing more about their connectivity structure
than that given by the prior in (7).
For the MH sampler  denote the set of non-marginalized out δ entries δ+ = {δij : cij > 0}. We
propose a new value qi∗ for one δij ∈ δ+ according to (7). The conditional posterior probability

4

PDIA PDIA-MAP HMM-EM bigram trigram 4-gram 5-gram 6-gram SSM
4.78
19 358
3.56

9.71
28
3.77

4.80
5 592
3.73
341

4.69
10 838
3.72
1 365

6.45
382
3.75
21

5.13
2 023
3.74
85

AIW 5.13
365.6
DNA 3.72
64.7

5.46
379
3.72
54

7.89
52
3.76
19

5

314 166

Table 1: PDIA inference performance relative to HMM and ﬁxed order Markov models. Top rows:
perplexity. Bottom rows: number of states in each model. For the PDIA this is an average number.

(cid:32)

α(δij = qi∗|δij = qi(cid:48)) = min

of this proposal is proportional to p(x0:T|δij = qi∗   δ+−ij)P (δij = qi∗|δ+−ij). The Hastings cor-
rection exactly cancels out the proposal probability in the accept/reject ratio leaving an MH accept
probability for the δij being set to qi∗ given that its previous value was qi(cid:48) of
p(x0:T|δij = qi∗   δ+−ij)
p(x0:T|δij = qi(cid:48)  δ+−ij)

(8)
Whether qi∗ is marginalized out or not  evaluating p(x0:T|δij = qi∗   δ+−ij) may require reinstantiat-
ing marginalized out elements of δ. As before  these values are sampled from (7) on a just-in-time
schedule. If the new value is accepted  all δij ∈ δ+ for which cij = 0 are removed  and then move
to the next transition in δ to sample.
In the ﬁnite case  one can sample µ by Metropolis-Hastings or use a MAP estimate as in [7]. Hy-
perparameters α  β and γ can be sampled via Metropolis-Hastings updates. In our experiments we
use Gamma(1 1) hyperpriors.

(cid:33)

1 

.

3.3 The Probabilistic Deterministic Inﬁnite Automaton

We would like to avoid placing a strict upper bound on the number of states so that model complexity
can grow with the amount of training data. To see how to do this  consider what happens when
|Q| → ∞. In this case  the right hand side of equations (1) and (2) must be replaced by inﬁnite
dimensional alternatives

µ ∼ PY(γ  d0  H)
φj ∼ PY(α  d  µ)
δij ∼ φj

where PY stands for Pitman Yor process and H in our case is a geometric distribution over the
integers with parameter λ. The resulting hierarchical model becomes the hierarchical Pitman-Yor
process (HPYP) over a discrete alphabet [14]. The discount parameters d0 and d are particular to the
inﬁnite case  and when both are zero the HPYP becomes the well known hierarchical Dirichlet pro-
cess (HDP)  which is the inﬁnite dimensional limit of (1) and (2) [15]. Given a ﬁnite amount of data 
there can only be nonzero counts for a ﬁnite number of state/symbol pairs  so our marginalization
procedure from the ﬁnite case will yield a δ with at most T elements. Denote these non-marginalized
out entries by δ+. We can sample the elements of δ+ as before using (8) provided that we can pro-
pose from the HPYP. In many HPYP sampler representations this is easy to do. We use the Chinese
restaurant franchise representation [15] in which the posterior predictive distribution of δij given
δ+−ij can be expressed with φj and µ integrated out as
P (δij = qi(cid:48)|δ+−ij  α  γ) = E

γ + w·
i κi are stochastic bookkeeping counts
required by the Chinese Restaurant franchise sampler. These counts must themselves be sampled
[15]. The discount hyperparameters can also be sampled by Metropolis-Hastings.

where wi(cid:48)  ki(cid:48)j  κi(cid:48)  w· =(cid:80)

i kij  and κ· =(cid:80)

(cid:34)
i wi  k·j =(cid:80)

vi(cid:48)j − ki(cid:48)jd
α + D+
j

(cid:18) wi(cid:48) − κi(cid:48)d0

+ α + k·jd
α + D+
j

+ γ + κ·d0
γ + w·

(cid:19)(cid:35)

H(qi(cid:48))

(9)

4 Experiments and Results

To test our PDIA inference approach we evaluated it on discrete natural sequence prediction and
compared its performance to HMMs and smoothed n-gram models. We trained the models on two

5

Figure 1: Subsampled PDIA sampler trace for Alice in Wonderland. The top trace is the joint log
likelihood of the model and training data  the bottom trace is the number of states.

datasets: a character sequence from Alice in Wonderland [2] and a short sequence of mouse DNA.
The Alice in Wonderland (AIW) dataset was preprocessed to remove all characters but letters and
spaces  shift all letters from upper to lower case  and split along sentence dividers to yield a 27-
character alphabet (a-z and space). We trained on 100 random sentences (9 986 characters) and
tested on 50 random sentences (3 891 characters). The mouse DNA dataset consisted of a fragment
of chromosome 2 with 194 173 base pairs  which we treated as a single unbroken string. We used
the ﬁrst 150 000 base pairs for training and the rest for testing. For AIW  the state of the PDIA
model was always set to q0 at the start of each sentence. For DNA  the state of the PDIA model at
the start of the test data was set to the last state of the model after accepting the training data. We
placed Gamma(1 1) priors over α  β and γ  set λ = .001  and used uniform priors for d0 and d.
We evaluated the performance of the learned models by calculating the average per character pre-
dictive perplexity of the test data. For training data x1:T and test data y1:T (cid:48) this is given by
2− 1
T (cid:48) log2 P (y1:T (cid:48)|x1:T ). It is a measure of the average uncertainty the model has about what character
comes next given the sequence up to that point  and is at most |Σ|. We evaluated the probability of
the test data incrementally  integrating the test data into the model in the standard Bayesian way.
Test perplexity results are shown in Table 1 on the ﬁrst line of each subtable. Each sample passed
through every instantiated transition. Every ﬁfth sample for AIW and every tenth sample for DNA
after burn-in was used for prediction. For AIW  we ran 15 000 burn-in samples and used 3 500
samples for predictive inference. Subsampled sampler diagnostic plots are shown in Figure 1 that
demonstrate the convergence properties of our sampler. When modeling the DNA dataset we burn-in
for 1 000 samples and use 900 samples for inference. For the smoothed n-gram models  we report
thousand-sample average perplexity results for hierarchical Pitman-Yor process (HPYP) [14] models
of varying Markov order (1 through 5 notated as bigram through 6-gram) after burning each model
in for one hundred samples. We also show the performance of the single particle incremental variant
of the sequence memoizer (SM) [5]  the SM being the limit of an n-gram model as n → ∞. We also
show results for a hidden Markov model (HMM) [8] trained using expectation-maximization (EM).
We determined the best number of hidden states by cross-validation on the test data (a procedure
used here to produce optimistic HMM performance for comparison purposes only).
The performance of the PDIA exceeds that of the HMM and is approximately equal to that of
a smoothed 4-gram model  though it does not outperform very deep  smoothed Markov models.
This is in contrast to [16]  which found that PDFAs trained on natural language data were able to
predict as well as unsmoothed trigrams  but were signiﬁcantly worse than smoothed trigrams  even
when averaging over multiple learned PDFAs. As can be seen in the second line of each subtable
in Table 1  the MAP number of states learned by the PDIA is signiﬁcantly lower than that of the
n-gram model with equal predictive performance.
Unlike the HMM  the computational complexity of PDFA prediction does not depend on the number
of states in the model because only a single path through the states is followed. This means that the
asymptotic cost of prediction for the PDIA is O(LT (cid:48))  where L is the number of posterior samples
and T (cid:48) is the length of the test sequence. For any single HMM it is O(KT (cid:48))  where K is the number
of states in the HMM. This is because all possible paths must be followed to achieve the given HMM
predictive performance (although a subset of possible paths could be followed if doing approximate

6

0.511.522.53x 104−1.98−1.96−1.94−1.92−1.9−1.88−1.86x 104IterationsLog Likelihood0.511.522.53x 104325350375400425450StatesFigure 2: Two PNFAs outside the class of PDFAs.
(a) can be represented by a mixture of two
PDFAs  one following the right branch from state 0  the other following the left branch. (b)  in
contrast  cannot be represented by any ﬁnite mixture of PDFAs.

inference). In PDIA inference we too can choose the number of samples used for prediction  but
here even a single sample has empirical prediction performance superior to averaging over all paths
in an HMM. The computational complexity of smoothing n-gram inference is equivalent to PDIA
inference  however  the storage cost for the large n-gram models is signiﬁcantly higher than that of
the estimated PDIA for the same predictive performance.

5 Theory and Related Work

The PDIA posterior distribution takes the form of an inﬁnite mixture of PDFAs. In practice  we
run a sampler for some number of iterations and approximate the posterior with a ﬁnite mixture
of PDFAs. For this reason  we now consider the expressive power of ﬁnite mixtures of PDFAs.
We show that they are strictly more expressive than PDFAs  but strictly less expressive than hidden
Markov models. Probabilistic non-deterministic ﬁnite automata (PNFA) are a strictly larger model
class than PDFAs. For example  the PNFA in 2(a) cannot be expressed as a PDFA [3]. However 
it can be expressed as a mixture of two PDFAs  one with Q = {q0  q1  q3} and the other with
Q = {q0  q2  q3}. Thus mixtures of PDFAs are a strictly larger model class than PDFAs. In general 
any PNFA where the nondeterministic transitions can only be visited once can be expressed as a
mixture of PDFAs. However  if we replace transitions to q3 with transitions to q0  as in 2(b)  there
is no longer any equivalent ﬁnite mixture of PDFAs  since the nondeterministic branch from q0 can
be visited an arbitrary number of times.
Previous work on PDFA induction has focused on accurately discovering model structure when the
true generative mechanism is a PDFA. State merging algorithms do this by starting with the trivial
PDFA that only accepts the training data and merging states that pass a similarity test [1  17]  and
have been proven to identify the correct model in the limit of inﬁnite data. State splitting algorithms
start at the opposite extreme  with the trivial single-state PDFA  and split states that pass a difference
test [12  13]. These algorithms return only a deterministic estimate  while ours naturally expresses
uncertainty about the learned model.
To test if we can learn the generative mechanism given our inductive bias  we trained the PDIA on
data from three synthetic grammars: the even process [13]  the Reber grammar [11] and the Feldman
grammar [4]  which have up to 7 states and 7 symbols in the alphabet. In each case the mean number
of states discovered by the model approached the correct number as more data was used in training.
Results are presented in Figure 3. Furthermore  the predictive performance of the PDIA was nearly
equivalent to the actual data generating mechanism.

6 Discussion

Our Bayesian approach to PDIA inference can be interpreted as a stochastic search procedure for
PDFA structure learning where the number of states is unknown. In Section 5 we presented evidence
that PDFA samples from our PDIA inference algorithm have the same characteristics as the true
generative process. This in and of itself may be of interest to the PDFA induction community.

7

0123A/0.5A/0.5A/0.8A/0.6B/0.4B/0.2(a)(b)012A/0.5A/0.5A/0.8A/0.6B/0.4B/0.2(a) Even

(b) Reber

(c) Feldman

(d) Posterior marginal PDIA state cardinality distribution

Figure 3: Three synthetic PDFAs: (a) even process [13]  (b) Reber grammar [11]  (c) Feldman
grammar [4]. (d) posterior mean and standard deviation of number of states discovered during PDIA
inference for varying amounts of data generated by each of the synthetic PDFAs. PDIA inference
discovers PDFAs with the correct number of states

We ourselves are more interested in establishing new ways to produce smoothed predictive con-
ditional distributions. Inference in the PDIA presents a completely new approach to smoothing 
smoothing by averaging over PDFA model structure rather than hierarchically smoothing related
emission distribution estimates. Our PDIA approach gives us an attractive ability to trade-off be-
tween model simplicity in terms of number of states  computational complexity in terms of asymp-
totic cost of prediction  and predictive perplexity. While our PDIA approach may not yet outperform
the best smoothing Markov model approaches in terms of predictive perplexity alone  it does out-
perform them in terms of model complexity required to achieve the same predictive perplexity  and
outperforms HMMs in terms of asymptotic time complexity of prediction. This suggests that a
future combination of smoothing over model structure and smoothing over emission distributions
could produce excellent results. PDIA inference gives researchers another tool to choose from when
building models. If very fast prediction is desirable and the predictive perplexity difference between
the PDIA and  for instance  the most competitive n-gram is insigniﬁcant from an application per-
spective  then doing ﬁnite sample inference in the PDIA offers a signiﬁcant computational advantage
in terms of memory.
We indeed believe the most promising approach to improving PDIA predictive performance is to
construct a smoothing hierarchy over the state speciﬁc emission distributions  as is done in the
smoothing n-gram models. For an n-gram  where every state corresponds to a sufﬁx of the sequence 
the predictive distributions for a sufﬁx is smoothed by the predictive distribution for a shorter sufﬁx 
for which there are more observations. This makes it possible to increase the size of the model indef-
initely without generalization performance suffering [18]. In the PDIA  by contrast  the predictive
probabilities for states are not tied together. Since states of the PDIA are not uniquely identiﬁed
by sufﬁxes  it is no longer clear what the natural smoothing hierarchy is. It is somewhat surprising
that PDIA learning works nearly as well as n-gram modeling even without a smoothing hierarchy
for its emission distributions. Imposing a hierarchical smoothing of the PDIA emission distributions
remains an open problem.

8

01A/0.5B/0.5B/1.001B/1.023456T/0.5P/0.5S/0.6X/0.4V/0.3T/0.7V/0.5X/0.5P/0.5S/0.5E/1.0to 0from 60123456B/0.8125B/0.8125B/0.75A/0.1875A/0.5625A/0.5625B/0.0625A/0.1875B/0.4375A/0.25A/0.9375B/0.4375A/0.75B/0.2510^110^210^310^410^510^612345678ObservationsStates Even ProcessFeldman GrammarReber GrammarReferences
[1] R. Carrasco and J. Oncina. Learning stochastic regular grammars by means of a state merging method.

Grammatical Inference and Applications  pages 139–152  1994.
in Wonderland.

Alice’s Adventures

[2] L. Carroll.

http://www.gutenberg.org/etext/11.

Macmillan 

1865.

URL

[3] P. Dupont  F. Denis  and Y. Esposito. Links between probabilistic automata and hidden Markov models:
probability distributions  learning models and induction algorithms. Pattern recognition  38(9):1349–
1371  2005.

[4] J. Feldman and J.F. Hanna. The structure of responses to a sequence of binary events. Journal of Mathe-

matical Psychology  3(2):371–387  1966.

[5] J. Gasthaus  F. Wood  and Y. W. Teh. Lossless compression based on the Sequence Memoizer. In Data

Compression Conference 2010  pages 337–345  2010.

[6] A. Gelman  J. B. Carlin  H. S. Stern  and D. B. Rubin. Bayesian data analysis. Chapman & Hall  New

York  1995.

[7] D. J. C. MacKay and L.C. Bauman Peto. A hierarchical Dirichlet language model. Natural language

engineering  1(2):289–307  1995.

[8] K. Murphy.

Hidden Markov model

(HMM)

toolbox

for Matlab 

2005.

URL

http://www.cs.ubc.ca/ murphyk/Software/HMM/hmm.html.

[9] M.O. Rabin. Probabilistic automata. Information and control  6(3):230–245  1963.
[10] L. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Pro-

ceedings of the IEEE  77:257–286  1989.

[11] A.S. Reber. Implicit learning of artiﬁcial grammars. Journal of verbal learning and verbal behavior  6

(6):855–863  1967.

[12] D. Ron  Y. Singer  and N. Tishby. The power of amnesia: Learning probabilistic automata with variable

memory length. Machine learning  25(2):117–149  1996.

[13] C.R. Shalizi and K.L. Shalizi. Blind construction of optimal nonlinear recursive predictors for discrete
sequences. In Proceedings of the 20th conference on Uncertainty in Artiﬁcial Intelligence  pages 504–511.
UAI Press  2004.

[14] Y. W. Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of

the Association for Computational Linguistics  pages 985–992  2006.

[15] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581  2006.

[16] F. Thollard. Improving probabilistic grammatical inference core algorithms with post-processing tech-

niques. In Eighteenth International Conference on Machine Learning  pages 561–568  2001.

[17] F. Thollard  P. Dupont  and C. del la Higuera. Probabilistic DFA inference using Kullback-Leibler diver-
gence and minimality. In Seventeenth International Conference on Machine Learning  pages 975–982.
Citeseer  2000.

[18] F. Wood  C. Archambeau  J. Gasthaus  L. James  and Y. W. Teh. A stochastic memoizer for sequence data.
In Proceedings of the 26th International Conference on Machine Learning  pages 1129–1136  Montreal 
Canada  2009.

9

,David Pfau
Nicholas Bartlett
Frank Wood
Sewoong Oh
Kiran Thekumparampil
Shusen Wang
Fred Roosta
Peng Xu
Michael Mahoney
Wei-Da Chen
Shan-Hung (Brandon) Wu