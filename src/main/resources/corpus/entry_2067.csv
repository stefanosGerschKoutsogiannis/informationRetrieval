2016,Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation,Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ε-ball around the observed data  which is only correct in the limit ε→0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ε→0  and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases  learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.,Fast -free Inference of Simulation Models with

Bayesian Conditional Density Estimation

George Papamakarios
School of Informatics
University of Edinburgh

g.papamakarios@ed.ac.uk

Iain Murray

School of Informatics
University of Edinburgh
i.murray@ed.ac.uk

Abstract

Many statistical models can be simulated forwards but have intractable likelihoods.
Approximate Bayesian Computation (ABC) methods are used to infer properties
of these models from data. Traditionally these methods approximate the posterior
over parameters by conditioning on data being inside an -ball around the observed
data  which is only correct in the limit → 0. Monte Carlo methods can then draw
samples from the approximate posterior to approximate predictions or error bars on
parameters. These algorithms critically slow down as → 0  and in practice draw
samples from a broader distribution than the posterior. We propose a new approach
to likelihood-free inference based on Bayesian conditional density estimation.
Preliminary inferences based on limited simulation data are used to guide later
simulations. In some cases  learning an accurate parametric representation of the
entire true posterior distribution requires fewer model simulations than Monte Carlo
ABC methods need to produce a single sample from an approximate posterior.

1

Introduction

A simulator-based model is a data-generating process described by a computer program  usually
with some free parameters we need to learn from data. Simulator-based modelling lends itself
naturally to scientiﬁc domains such as evolutionary biology [1]  ecology [24]  disease epidemics [10] 
economics [8] and cosmology [23]  where observations are best understood as products of underlying
physical processes. Inference in these models amounts to discovering plausible parameter settings
that could have generated our observed data. The application domains mentioned can require properly
calibrated distributions that express uncertainty over plausible parameters  rather than just point
estimates  in order to reach scientiﬁc conclusions or make decisions.
As an analytical expression for the likelihood of parameters given observations is typically not avail-
able for simulator-based models  conventional likelihood-based Bayesian inference is not applicable.
An alternative family of algorithms for likelihood-free inference has been developed  referred to as
Approximate Bayesian Computation (ABC). These algorithms simulate the model repeatedly and
only accept parameter settings which generate synthetic data similar to the observed data  typically
gathered in a real-world experiment.
Rejection ABC [21]  the most basic ABC algorithm  simulates the model for each setting of proposed
parameters  and rejects parameters if the generated data is not within a certain distance from the
observations. The accepted parameters form a set of independent samples from an approximate
posterior. Markov Chain Monte Carlo ABC (MCMC-ABC) [13] is an improvement over rejection
ABC which  instead of independently proposing parameters  explores the parameter space by perturb-
ing the most recently accepted parameters. Sequential Monte Carlo ABC (SMC-ABC) [2  5] uses
importance sampling to simulate a sequence of slowly-changing distributions  the last of which is an
approximation to the parameter posterior.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Conventional ABC algorithms such as the above suffer from three drawbacks. First  they only
represent the parameter posterior as a set of (possibly weighted or correlated) samples. A sample-
based representation easily gives estimates and error bars of individual parameters  and model
predictions. However these computations are noisy  and it is not obvious how to perform some other
computations using samples  such as combining posteriors from two separate analyses. Second  the
parameter samples do not come from the correct Bayesian posterior  but from an approximation
based on assuming a pseudo-observation that the data is within an -ball centred on the data actually
observed. Third  as the -tolerance is reduced  it can become impractical to simulate the model
enough times to match the observed data even once. When simulations are expensive to perform 
good quality inference becomes impractical.
We propose a parametric approach to likelihood-free inference  which unlike conventional ABC does
not suffer from the above three issues. Instead of returning samples from an -approximation to
the posterior  our approach learns a parametric approximation to the exact posterior  which can be
made as accurate as required. Preliminary ﬁts to the posterior are used to guide future simulations 
which can reduce the number of simulations required to learn an accurate approximation by orders
of magnitude. Our approach uses conditional density estimation with Bayesian neural networks 
and draws upon advances in parametric density estimation  stochastic variational inference  and
recognition networks  as discussed in the related work section.

2 Bayesian conditional density estimation for likelihood-free inference

2.1 Simulator-based models and ABC

Let θ be a vector of parameters controlling a simulator-based model  and let x be a data vector
generated by the model. The model may be provided as a probabilistic program that can be easily
simulated  and implicitly deﬁnes a likelihood p(x| θ)  which we assume we cannot evaluate. Let
p(θ) encode our prior beliefs about the parameters. Given an observation xo  we are interested in the
parameter posterior p(θ | x = xo) ∝ p(x = xo | θ) p(θ).
As the likelihood p(x = xo | θ) is unavailable  conventional Bayesian inference cannot be carried out.
The principle behind ABC is to approximate p(x = xo | θ) by p((cid:107)x − xo(cid:107) < | θ) for a sufﬁciently
small value of   and then estimate the latter—e.g. by Monte Carlo—using simulations from the
model. Hence  ABC approximates the posterior by p(θ | (cid:107)x − xo(cid:107) < )  which is typically broader
and more uncertain. ABC can trade off computation for accuracy by decreasing   which improves the
approximation to the posterior but requires more simulations from the model. However  the approxi-
mation becomes exact only when  → 0  in which case simulations never match the observations 
p((cid:107)x − xo(cid:107) < | θ) → 0  and existing methods break down. In this paper  we refer to p(θ | x = xo)
as the exact posterior  as it corresponds to setting  = 0 in p(θ | (cid:107)x − xo(cid:107) < ).
In most practical applications of ABC  x is taken to be a ﬁxed-length vector of summary statistics
that is calculated from data generated by the simulator  rather than the raw data itself. Extracting
statistics is often necessary in practice  to reduce the dimensionality of the data and maintain
p((cid:107)x − xo(cid:107) < | θ) to practically acceptable levels. For the purposes of this paper  we will make no
distinction between raw data and summary statistics  and we will regard the calculation of summary
statistics as part of the data generating process.

2.2 Learning the posterior

Rather than using simulations from the model in order to estimate an approximate likelihood 
p((cid:107)x − xo(cid:107) < | θ)  we will use the simulations to directly estimate p(θ | x = xo). We will run
simulations for parameters drawn from a distribution  ˜p(θ)  which we shall refer to as the proposal
prior. The proposition below indicates how we can then form a consistent estimate of the exact
posterior  using a ﬂexible family of conditional densities  qφ(θ | x)  parameterized by a vector φ.
Proposition 1. We assume that each of a set of N pairs (θn  xn) was independently generated by
(1)
n qφ(θn | xn) is maximized w.r.t. φ
(2)

In the limit N → ∞  the probability of the parameter vectors(cid:81)

xn ∼ p(x| θn).

θn ∼ ˜p(θ)

if and only if

qφ(θ | x) ∝

˜p(θ)
p(θ)

p(θ | x) 

and

2

p(θ) p(θ | x) exists.

provided a setting of φ that makes qφ(θ | x) proportional to ˜p(θ)
Intuition: if we simulated enough parameters from the prior  the density estimator qφ would learn a
conditional of the joint prior model over parameters and data  which is the posterior p(θ | x). If we
simulate parameters drawn from another distribution  we need to “importance reweight” the result. A
more detailed proof can be found in Section A of the supplementary material.
The proposition above suggests the following procedure for learning the posterior: (a) propose a
set of parameter vectors {θn} from the proposal prior; (b) for each θn run the simulator to obtain a
corresponding data vector xn; (c) train qφ with maximum likelihood on {θn  xn}; and (d) estimate
the posterior by

ˆp(θ | x = xo) ∝

p(θ)
˜p(θ)

qφ(θ | xo).

(3)

This procedure is summarized in Algorithm 2.

mixture of K Gaussian components qφ(θ | x) =(cid:80)

2.3 Choice of conditional density estimator and proposal prior
In choosing the types of density estimator qφ(θ | x) and proposal prior ˜p(θ)  we need to meet the
following criteria: (a) qφ should be ﬂexible enough to represent the posterior but easy to train with
maximum likelihood; (b) ˜p(θ) should be easy to evaluate and sample from; and (c) the right-hand
side expression in Equation (3) should be easily evaluated and normalized.
We draw upon work on conditional neural density estimation and take qφ to be a Mixture Density
Network (MDN) [3] with fully parameterized covariance matrices. That is  qφ takes the form of a
k αk N (θ | mk  Sk)  whose mixing coefﬁcients
{αk}  means {mk} and covariance matrices {Sk} are computed by a feedforward neural network
parameterized by φ  taking x as input. Such an architecture is capable of representing any conditional
distribution arbitrarily accurately—provided the number of components K and number of hidden
units in the neural network are sufﬁciently large—while remaining trainable by backpropagation.
The parameterization of the MDN is detailed in Section B of the supplementary material.
We take the proposal prior to be a single Gaussian ˜p(θ) = N (θ | m0  S0)  with mean m0 and full
covariance matrix S0. Assuming the prior p(θ) is a simple distribution (uniform or Gaussian  as is
typically the case in practice)  then this choice allows us to calculate ˆp(θ | x = xo) in Equation (3)
analytically. That is  ˆp(θ | x = xo) will be a mixture of K Gaussians  whose parameters will be a
function of {αk  mk  Sk} evaluated at xo (as detailed in Section C of the supplementary material).
2.4 Learning the proposal prior
Simple rejection ABC is inefﬁcient because the posterior p(θ | x = xo) is typically much narrower
than the prior p(θ). A parameter vector θ sampled from p(θ) will rarely be plausible under
p(θ | x = xo) and will most likely be rejected. Practical ABC algorithms attempt to reduce the
number of rejections by modifying the way they propose parameters; for instance  MCMC-ABC and
SMC-ABC propose new parameters by perturbing parameters they already consider plausible  in the
hope that nearby parameters remain plausible.
In our framework  the key to efﬁcient use of simulations lies in the choice of proposal prior. If we
take ˜p(θ) to be the actual prior  then qφ(θ | x) will learn the posterior for all x  as can be seen from
Equation (2). Such a strategy however is grossly inefﬁcient if we are only interested in the posterior
for x = xo. Conversely  if ˜p(θ) closely matches p(θ | x = xo)  then most simulations will produce
samples that are highly informative in learning qφ(θ | x) for x = xo. In other words  if we already
knew the true posterior  we could use it to construct an efﬁcient proposal prior for learning it.
We exploit this idea to set up a ﬁxed-point system. Our strategy becomes to learn an efﬁcient proposal
prior that closely approximates the posterior as follows: (a) initially take ˜p(θ) to be the prior p(θ);
(b) propose N samples {θn} from ˜p(θ) and corresponding samples {xn} from the simulator  and
train qφ(θ | x) on them; (c) approximate the posterior using Equation (3) and set ˜p(θ) to it; (d) repeat
until ˜p(θ) has converged. This procedure is summarized in Algorithm 1.
In the procedure above  as long as qφ(θ | x) has only one Gaussian component (K = 1) then ˜p(θ)
remains a single Gaussian throughout. Moreover  in each iteration we initialize qφ with the density

3

Algorithm 1: Training of proposal prior
initialize qφ(θ | x) with one component
˜p(θ) ← p(θ)
repeat

for n = 1..N do

sample θn ∼ ˜p(θ)
sample xn ∼ p(x| θn)

end
retrain qφ(θ | x) on {θn  xn}
˜p(θ) ← p(θ)

˜p(θ) qφ(θ | xo)
until ˜p(θ) has converged;

Algorithm 2: Training of posterior
initialize qφ(θ | x) with K components
// if qφ available by Algorithm 1
// initialize by replicating its
// one component K times
for n = 1..N do

sample θn ∼ ˜p(θ)
sample xn ∼ p(x| θn)
end
train qφ(θ | x) on {θn  xn}
ˆp(θ | x = xo) ← p(θ)

˜p(θ) qφ(θ | xo)

estimator learnt in the iteration before  thus we keep training qφ throughout. This initialization allows
us to use a small sample size N in each iteration  thus making efﬁcient use of simulations.
As we shall demonstrate in Section 3  the procedure above learns Gaussian approximations to the true
posterior fast: in our experiments typically 4–6 iterations of 200–500 samples each were sufﬁcient.
This Gaussian approximation can be used as a rough but cheap approximation to the true posterior 
or it can serve as a good proposal prior in Algorithm 2 for efﬁciently ﬁne-tuning a non-Gaussian
multi-component posterior. If the second strategy is adopted  then we can reuse the single-component
neural density estimator learnt in Algorithm 1 to initialize qφ in Algorithm 2. The weights in the ﬁnal
layer of the MDN are replicated K times  with small random perturbations to break symmetry.

2.5 Use of Bayesian neural density estimators

To make Algorithm 1 as efﬁcient as possible  the number of simulations per iteration N should
be kept small  while at the same time it should provide a sufﬁcient training signal for qφ. With
a conventional MDN  if N is made too small  there is a danger of overﬁtting  especially in early
iterations  leading to over-conﬁdent proposal priors and an unstable procedure. Early stopping could
be used to avoid overﬁtting; however a signiﬁcant fraction of the N samples would have to be used
as a validation set  leading to inefﬁcient use of simulations.
As a better alternative  we developed a Bayesian version of the MDN using Stochastic Variational
Inference (SVI) for neural networks [12]. We shall refer to this Bayesian version of the MDN as
MDN-SVI. An MDN-SVI has two sets of adjustable parameters of the same size  the means φm and
the log variances φs. The means correspond to the parameters φ of a conventional MDN. During
training  Gaussian noise of variance exp φs is added to the means independently for each training
example (θn  xn). The Bayesian interpretation of this procedure is that it optimizes a variational
Gaussian posterior with a diagonal covariance matrix over parameters φ. At prediction time  the noise
is switched off and the MDN-SVI behaves like a conventional MDN with φ = φm. Section D of the
supplementary material details the implementation and training of MDN-SVI. We found that using
an MDN-SVI instead of an MDN improves the robustness and efﬁciency of Algorithm 1 because
(a) MDN-SVI is resistant to overﬁtting  allowing us to use a smaller number of simulations N; (b) no
validation set is needed  so all samples can be used for training; and (c) since overﬁtting is not an
issue  no careful tuning of training time is necessary.

3 Experiments

We showcase three versions of our approach: (a) learning the posterior with Algorithm 2 where qφ
is a conventional MDN and the proposal prior ˜p(θ) is taken to be the actual prior p(θ)  which we
refer to as MDN with prior; (b) training a proposal prior with Algorithm 1 where qφ is an MDN-SVI 
which we refer to as proposal prior; and (c) learning the posterior with Algorithm 2 where qφ is an
MDN-SVI and the proposal prior ˜p(θ) is taken to be the one learnt in (b)  which we refer to as MDN
with proposal. All MDNs were trained using Adam [11] with its default parameters.
We compare to three ABC baselines: (a) rejection ABC [21]  where parameters are proposed from the
prior and are accepted if (cid:107)x − xo(cid:107) < ; (b) MCMC-ABC [13] with a spherical Gaussian proposal 
whose variance we manually tuned separately in each case for best performance; and (c) SMC-

4

Figure 1: Results on mixture of two Gaussians. Left: approximate posteriors learnt by each strategy
for xo = 0. Middle: full conditional density qφ(θ|x) leant by the MDN trained with prior. Right:
full conditional density qφ(θ|x) learnt by the MDN-SVI trained with proposal prior. Vertical dashed
lines show the location of the observation xo = 0.

ABC [2]  where the sequence of ’s was exponentially decayed  with a decay rate manually tuned
separately in each case for best performance. MCMC-ABC was given the unrealistic advantage of
being initialized with a sample from rejection ABC  removing the need for an otherwise necessary
burn-in period. Code for reproducing the experiments is provided in the supplementary material and
at https://github.com/gpapamak/epsilon_free_inference.

3.1 Mixture of two Gaussians

(cid:0)x| θ  σ2

1

(cid:1) + (1 − α)N

(cid:0)x| θ  σ2

2

(cid:1) 

The ﬁrst experiment is a toy problem where the goal is to infer the common mean θ of a mixture of
two 1D Gaussians  given a single datapoint xo. The setup is

and

p(x| θ) = αN

p(θ) = U(θ | θα  θβ)

(4)
where θα = −10  θβ = 10  α = 0.5  σ1 = 1  σ2 = 0.1 and xo = 0. The posterior can be calculated
analytically  and is proportional to an equal mixture of two Gaussians centred at xo with variances σ2
1
and σ2
2  restricted to [θα  θβ]. This problem is often used in the SMC-ABC literature to illustrate the
difﬁculty of MCMC-ABC in representing long tails. Here we use it to demonstrate the correctness of
our approach and its ability to accurately represent non-Gaussian long-tailed posteriors.
Figure 1 shows the results of neural density estimation using each strategy. All MDNs have one
hidden layer with 20 tanh units and 2 Gaussian components  except for the proposal prior MDN
which has a single component. Both MDN with prior and MDN with proposal learn good parametric
approximations to the true posterior  and the proposal prior is a good Gaussian approximation to it.
We used 10K simulations to train the MDN with prior  whereas the prior proposal took 4 iterations
of 200 simulations each to train  and the MDN with proposal took 1000 simulations on top of the
previous 800. The MDN with prior learns the posterior distributions for a large range of possible
observations x (middle plot of Figure 1)  whereas the MDN with proposal gives accurate posterior
probabilities only near the value actually observed (right plot of Figure 1).

3.2 Bayesian linear regression

p(x| θ) =(cid:81)

(cid:0)xi | θT ui  σ2(cid:1) 

In Bayesian linear regression  the goal is to infer the parameters θ of a linear map from noisy
observations of outputs at known inputs. The setup is

and

i N

p(θ) = N (θ | m  S)

(5)
where we took m = 0  S = I  σ = 0.1  randomly generated inputs {ui} from a standard Gaussian 
and randomly generated observations xo from the model. In our setup  θ and x have 6 and 10
dimensions respectively. The posterior is analytically tractable  and is a single Gaussian.
All MDNs have one hidden layer of 50 tanh units and one Gaussian component. ABC methods were
run for a sequence of decreasing ’s  up to their failing points. To measure the approximation quality
to the posterior  we analytically calculated the KL divergence from the true posterior to the learnt
posterior (which for ABC was taken to be a Gaussian ﬁt to the set of returned posterior samples).
The left of Figure 2 shows the approximation quality vs ; MDN methods are shown as horizontal

5

−3−2−10123θTrueposteriorMDNwithpriorProposalpriorMDNwithproposal−15−10−5051015x−10−50510θMean75%ofmass99%ofmassxo=0−15−10−5051015x−10−50510θMean75%ofmass99%ofmassxo=0Figure 2: Results on Bayesian linear regression. Left: KL divergence from true posterior to
approximation vs ; lower is better. Middle: number of simulations vs KL divergence; lower left is
better. Note that number of simulations is total for MDNs  and per effective sample for ABC. Right:
Posterior marginals for θ1 as computed by each method. ABC posteriors (represented as histograms)
correspond to the setting of  that minimizes the KL in the left plot.

lines. As  is decreased  ABC methods sample from an increasingly better approximation to the
true posterior  however they eventually reach their failing point  or take prohibitively long. The best
approximations are achieved by MDN with proposal and a very long run of SMC-ABC.
The middle of Figure 2 shows the increase in number of simulations needed to improve approximation
quality (as  decreases). We quote the total number of simulations for MDN training  and the number
of simulations per effective sample for ABC. Section E of the supplementary material describes how
the number of effective samples is calculated. The number of simulations per effective sample should
be multiplied by the number of effective samples needed in practice. Moreover  SMC-ABC will not
work well with only one particle  so many times the quoted cost will always be needed. Here  MDNs
make more efﬁcient use of simulations than Monte Carlo ABC methods. Sequentially ﬁtting a prior
proposal was more than ten times cheaper than training with prior samples  and more accurate.

3.3 Lotka–Volterra predator-prey population model

The Lotka–Volterra model is a stochastic Markov jump process that describes the continuous time
evolution of a population of predators interacting with a population of prey. There are four possible
reactions: (a) a predator being born  (b) a predator dying  (c) a prey being born  and (d) a prey being
eaten by a predator. Positive parameters θ = (θ1  θ2  θ3  θ4) control the rate of each reaction. Given
a set of statistics xo calculated from an observed population time series  the objective is to infer θ.
We used a ﬂat prior over log θ  and calculated a set of 9 statistics x. The full setup is detailed in
Section F of the supplementary material. The Lotka–Volterra model is commonly used in the ABC
literature as a realistic model which can be simulated  but whose likelihood is intractable. One of
the properties of Lotka–Volterra is that typical nature-like observations only occur for very speciﬁc
parameter settings  resulting in narrow  Gaussian-like posteriors that are hard to recover.
The MDN trained with prior has two hidden layers of 50 tanh units each  whereas the MDN-SVI
used to train the proposal prior and the MDN-SVI trained with proposal have one hidden layer of 50
tanh units. All three have one Gaussian component. We found that using more than one components
made no difference to the results; in all cases the MDNs chose to use only one component and switch
the rest off  which is consistent with our observation about the near-Gaussianity of the posterior.
We measure how well each method retrieves the true parameter values that were used to generate
xo by calculating their log probability under each learnt posterior; for ABC a Gaussian ﬁt to the
posterior samples was used. The left panel of Figure 3 shows how this log probability varies with  
demonstrating the superiority of MDN methods over ABC. In the middle panel we can see that MDN
training with proposal makes efﬁcient use of simulations compared to training with prior and ABC;
note that for ABC the number of simulations is only for one effective sample. In the right panel  we
can see that the estimates returned by MDN methods are more conﬁdent around the true parameters
compared to ABC  because the MDNs learn the exact posterior rather than an inﬂated version of it
like ABC does (plots for the other three parameters look similar).
We found that when training an MDN with a well-tuned proposal that focuses on the plausible region 
an MDN with fewer parameters is needed compared to training with the prior. This is because the

6

10−110010110−210−1100101102103KLdivergenceRej.ABCMCMC-ABCSMC-ABCMDNwithpriorProposalpriorMDNwithprop.10−210−1100101102103104KLdivergence100101102103104105106107#simulations(pereffectivesampleforABC)Rej.ABCMCMC-ABCSMC-ABCMDNwithpriorProposalpriorMDNwithprop.−0.20−0.15−0.10−0.050.000.05θ1TrueposteriorMDNwithpriorProposalpriorMDNwithprop.MCMC-ABCSMC-ABCFigure 3: Results on Lotka–Volterra. Left: negative log probability of true parameters vs ; lower
is better. Middle: number of simulations vs negative log probability; lower left is better. Note that
number of simulations is total for MDNs  but per effective sample for ABC. Right: Estimates of log θ1
with 2 standard deviations. ABC estimates used many more simulations with the smallest feasible .

MDN trained with proposal needs to learn only the local relationship between x and θ near xo  as
opposed to in the entire domain of the prior. Hence  not only are savings achieved in number of
simulations  but also training the MDN itself becomes more efﬁcient.

3.4 M/G/1 queue model

The M/G/1 queue model describes the processing of a queue of continuously arriving jobs by a
single server. In this model  the time the server takes to process each job is independently and
uniformly distributed in the interval [θ1  θ2]. The time interval between arrival of two consecutive
jobs is independently and exponentially distributed with rate θ3. The server observes only the time
intervals between departure of two consecutive jobs. Given a set of equally-spaced percentiles xo of
inter-departure times  the task is to infer parameters θ = (θ1  θ2  θ3). This model is easy to simulate
but its likelihood is intractable  and it has often been used as an ABC benchmark [4  16]. Unlike
Lotka–Volterra  data x is weakly informative about θ  and hence the posterior over θ tends to be
broad and non-Gaussian. In our setup  we placed ﬂat independent priors over θ1  θ2 − θ1 and θ3  and
we took x to be 5 equally spaced percentiles  as detailed in Section G of the supplementary material.
The MDN trained with prior has two hidden layers of 50 tanh units each  whereas the MDN-SVI
used to train the proposal prior and the one trained with proposal have one hidden layer of 50 tanh
units. As observed in the Lotka–Volterra demo  less capacity is required when training with proposal 
as the relationship to be learned is local and hence simpler  which saves compute time and gives a
more accurate ﬁnal posterior. All MDNs have 8 Gaussian components (except the MDN-SVI used
to train the proposal prior  which always has one)  which  after experimentation  we determined are
enough for the MDNs to represent the non-Gaussian nature of the posterior.
Figure 4 reports the log probability of the true parameters under each posterior learnt—for ABC 
the log probability was calculated by ﬁtting a mixture of 8 Gaussians to posterior samples using
Expectation-Maximization—and the number of simulations needed to achieve it. As before  MDN
methods are more conﬁdent compared to ABC around the true parameters  which is due to ABC com-
puting a broader posterior than the true one. MDN methods make more efﬁcient use of simulations 
since they use all of them for training and  unlike ABC  do not throw a proportion of them away.

4 Related work

Regression adjustment. An early parametric approach to ABC is regression adjustment  where a
parametric regressor is trained on simulation data in order to learn a mapping from x to θ. The
learnt mapping is then used to correct for using a large   by adjusting the location of posterior
samples gathered by e.g. rejection ABC. Beaumont et al. [1] used linear regressors  and later Blum
and François [4] used neural networks with one hidden layer that separately predicted the mean
and variance of θ. Both can be viewed as rudimentary density estimators and as such they are a
predecessor to our work. However  they were not ﬂexible enough to accurately estimate the posterior 
and they were only used within some other ABC method to allow for a larger . In our work  we
make conditional density estimation ﬂexible enough to approximate the posterior accurately.

7

10−210−1100101−5051015Neg.logprobabilityoftrueparametersRej.ABCMCMC-ABCSMC-ABCMDNwithpriorProposalpriorMDNwithprop.−5051015Neg.logprobabilityoftrueparameters100101102103104105106#simulations(pereffectivesampleforABC)Rej.ABCMCMC-ABCSMC-ABCMDNwithpriorProposalpriorMDNwithprop.Rej.ABCMCMCABCSMCABCMDNpriorProp.priorMDNprop.−5.0−4.8−4.6−4.4−4.2−4.0−3.8logθ1TruevalueFigure 4: Results on M/G/1. Left: negative log probability of true parameters vs ; lower is better.
Middle: number of simulations vs negative log probability; lower left is better. Note that number
of simulations is total for MDNs  and per effective sample for ABC. Right: Estimates of θ2 with 2
standard deviations; ABC estimates correspond to the lowest setting of  used.

Synthetic likelihood. Another parametric approach is synthetic likelihood  where parametric models
are used to estimate the likelihood p(x| θ). Wood [24] used a single Gaussian  and later Fan et al.
[7] used a mixture Gaussian model. Both of them learnt a separate density model of x for each θ by
repeatedly simulating the model for ﬁxed θ. More recently  Meeds and Welling [14] used a Gaussian
process model to interpolate Gaussian likelihood approximations between different θ’s. Compared
to learning the posterior  synthetic likelihood has the advantage of not depending on the choice of
proposal prior. Its main disadvantage is the need of further approximate inference on top of it in order
to obtain the posterior. In our work we directly learn the posterior  eliminating the need for further
inference  and we address the problem of correcting for the proposal prior.
Efﬁcient Monte Carlo ABC. Recent work on ABC has focused on reducing the simulation cost of
sample-based ABC methods. Hamiltonian ABC [15] improves upon MCMC-ABC by using stochasti-
cally estimated gradients in order to explore the parameter space more efﬁciently. Optimization Monte
Carlo ABC [16] explicitly optimizes the location of ABC samples  which greatly reduces rejection
rate. Bayesian optimization ABC [10] models p((cid:107)x − xo(cid:107) | θ) as a Gaussian process and then uses
Bayesian optimization to guide simulations towards the region of small distances (cid:107)x − xo(cid:107). In our
work we show how a signiﬁcant reduction in simulation cost can also be achieved with parametric
methods  which target the posterior directly.
Recognition networks. Our use of neural density estimators for learning posteriors is reminiscent of
recognition networks in machine learning. A recognition network is a neural network that is trained
to invert a generative model. The Helmholtz machine [6]  the variational auto-encoder [12] and
stochastic backpropagation [22] are examples where a recognition network is trained jointly with the
generative network it is designed to invert. Feedforward neural networks have been used to invert
black-box generative models [18] and binary-valued Bayesian networks [17]  and convolutional
neural networks have been used to invert a physics engine [25]. Our work illustrates the potential of
recognition networks in the ﬁeld of likelihood-free inference  where the generative model is ﬁxed 
and inference of its parameters is the goal.
Learning proposals. Neural density estimators have been employed in learning proposal distributions
for importance sampling [20] and Sequential Monte Carlo [9  19]. Although not our focus here  our
ﬁt to the posterior could also be used within Monte Carlo inference methods. In this work we see
how far we can get purely by ﬁtting a series of conditional density estimators.

5 Conclusions

Bayesian conditional density estimation improves likelihood-free inference in three main ways: (a) it
represents the posterior parametrically  as opposed to as a set of samples  allowing for probabilistic
evaluations later on in the pipeline; (b) it targets the exact posterior  rather than an -approximation
of it; and (c) it makes efﬁcient use of simulations by not rejecting samples  by interpolating between
samples  and by gradually focusing on the plausible parameter region. Our belief is that neural density
estimation is a tool with great potential in likelihood-free inference  and our hope is that this work
helps in establishing its usefulness in the ﬁeld.

8

10−410−310−210−1−3−2−101234Neg.logprobabilityoftrueparametersRej.ABCMCMC-ABCSMC-ABCMDNwithpriorProposalpriorMDNwithprop.−3−2−10123Neg.logprobabilityoftrueparameters100101102103104105106107108#simulations(pereffectivesampleforABC)Rej.ABCMCMC-ABCSMC-ABCMDNwithpriorProposalpriorMDNwithprop.Rej.ABCMCMCABCSMCABCMDNpriorProp.priorMDNprop.024681012θ2TruevalueAcknowledgments
We thank Amos Storkey for useful comments. George Papamakarios is supported by the Centre for
Doctoral Training in Data Science  funded by EPSRC (grant EP/L016427/1) and the University of
Edinburgh  and by Microsoft Research through its PhD Scholarship Programme.

References
[1] M. A. Beaumont  W. Zhang  and D. J. Balding. Approximate Bayesian Computation in population genetics.

Genetics  162:2025–2035  Dec. 2002.

[2] M. A. Beaumont  J.-M. Cornuet  J.-M. Marin  and C. P. Robert. Adaptive Approximate Bayesian

Computation. Biometrika  96(4):983–990  2009.

[3] C. M. Bishop. Mixture density networks. Technical Report NCRG/94/004  Aston University  1994.
[4] M. G. B. Blum and O. François. Non-linear regression models for Approximate Bayesian Computation.

Statistics and Computing  20(1):63–73  2010.

[5] F. V. Bonassi and M. West. Sequential Monte Carlo with adaptive weights for Approximate Bayesian

Computation. Bayesian Analysis  10(1):171–187  Mar. 2015.

[6] P. Dayan  G. E. Hinton  R. M. Neal  and R. S. Zemel. The Helmholtz machine. Neural Computation  7:

889–904  1995.

[7] Y. Fan  D. J. Nott  and S. A. Sisson. Approximate Bayesian Computation via regression density estimation.

Stat  2(1):34–48  2013.

[8] C. Gouriéroux  A. Monfort  and E. Renault. Indirect inference. Journal of Applied Econometrics  8(S1):

S85–S118  1993.

[9] S. Gu  Z. Ghahramani  and R. E. Turner. Neural adaptive Sequential Monte Carlo. Advances in Neural

Information Processing Systems 28  pages 2629–2637  2015.

[10] M. U. Gutmann and J. Corander. Bayesian optimization for likelihood-free inference of simulator-based

statistical models. arXiv e-prints  abs/1501.03291v3  2015.

[11] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd International

Conference on Learning Representations  2014.

[12] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. Proceedings of the 2nd International

Conference on Learning Representations  2013.

[13] P. Marjoram  J. Molitor  V. Plagnol  and S. Tavaré. Markov chain Monte Carlo without likelihoods.

Proceedings of the National Academy of Sciences  100(26):15324–15328  Dec. 2003.

[14] E. Meeds and M. Welling. GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation.

Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence  30  2014.

[15] E. Meeds  R. Leenders  and M. Welling. Hamiltonian ABC. Proceedings of the 31st Conference on

Uncertainty in Artiﬁcial Intelligence  pages 582–591  2015.

[16] T. Meeds and M. Welling. Optimization Monte Carlo: Efﬁcient and embarrassingly parallel likelihood-free

inference. Advances in Neural Information Processing Systems 28  pages 2071–2079  2015.

[17] Q. Morris. Recognition networks for approximate inference in BN20 networks. Proceedings of the 17th

Conference on Uncertainty in Artiﬁcial Intelligence  pages 370–377  2001.

[18] V. Nair  J. Susskind  and G. E. Hinton. Analysis-by-synthesis by learning to invert generative black boxes.

Proceedings of the 18th International Conference on Artiﬁcial Neural Networks  5163:971–981  2008.

[19] B. Paige and F. Wood. Inference networks for Sequential Monte Carlo in graphical models. Proceedings

of the 33rd International Conference on Machine Learning  2016.

[20] G. Papamakarios and I. Murray. Distilling intractable generative models. Probabilistic Integration

Workshop at Neural Information Processing Systems  2015.

[21] J. K. Pritchard  M. T. Seielstad  A. Perez-Lezaun  and M. W. Feldman. Population growth of human
Y chromosomes: a study of Y chromosome microsatellites. Molecular Biology and Evolution  16(12):
1791–1798  1999.

[22] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in
deep generative models. Proceedings of the 31st International Conference on Machine Learning  pages
1278–1286  2014.

[23] C. M. Schafer and P. E. Freeman. Likelihood-free inference in cosmology: Potential for the estimation of

luminosity functions. Statistical Challenges in Modern Astronomy V  pages 3–19  2012.

[24] S. N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature  466(7310):

1102–1104  2010.

[25] J. Wu  I. Yildirim  J. J. Lim  B. Freeman  and J. Tenenbaum. Galileo: Perceiving physical object properties
by integrating a physics engine with deep learning. Advances in Neural Information Processing Systems
28  pages 127–135  2015.

9

,Franziska Meier
Philipp Hennig
Stefan Schaal
George Papamakarios
Iain Murray
Vatsal Sharan
Sham Kakade
Percy Liang
Gregory Valiant