2018,Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels,Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet  their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover  due to DNNs' rich capacity  errors in training labels can hamper performance. To combat this problem  mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However  as we show in this paper  MAE can perform poorly with DNNs and large-scale datasets. Here  we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm  while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10  CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.,Generalized Cross Entropy Loss for Training Deep

Neural Networks with Noisy Labels

Zhilu Zhang
Mert R. Sabuncu
Electrical and Computer Engineering

Meinig School of Biomedical Engineering

Cornell University

zz452@cornell.edu  msabuncu@cornell.edu

Abstract

Deep neural networks (DNNs) have achieved tremendous success in a variety of
applications across many disciplines. Yet  their superior performance comes with
the expensive cost of requiring correctly annotated large-scale datasets. Moreover 
due to DNNs’ rich capacity  errors in training labels can hamper performance. To
combat this problem  mean absolute error (MAE) has recently been proposed as
a noise-robust alternative to the commonly-used categorical cross entropy (CCE)
loss. However  as we show in this paper  MAE can perform poorly with DNNs and
challenging datasets. Here  we present a theoretically grounded set of noise-robust
loss functions that can be seen as a generalization of MAE and CCE. Proposed loss
functions can be readily applied with any existing DNN architecture and algorithm 
while yielding good performance in a wide range of noisy label scenarios. We report
results from experiments conducted with CIFAR-10  CIFAR-100 and FASHION-
MNIST datasets and synthetically generated noisy labels.

1

Introduction

The resurrection of neural networks in recent years  together with the recent emergence of large
scale datasets  has enabled super-human performance on many classiﬁcation tasks [21  28  30].
However  supervised DNNs often require a large number of training samples to achieve a high level
of performance. For instance  the ImageNet dataset [6] has 3.2 million hand-annotated images.
Although crowdsourcing platforms like Amazon Mechanical Turk have made large-scale annotation
possible  some error during the labeling process is often inevitable  and mislabeled samples can
impair the performance of models trained on these data. Indeed  the sheer capacity of DNNs to
memorize massive data with completely randomly assigned labels [42] proves their susceptibility to
overﬁtting when trained with noisy labels. Hence  an algorithm that is robust against noisy labels
for DNNs is needed to resolve the potential problem. Furthermore  when examples are cheap and
accurate annotations are expensive  it can be more beneﬁcial to have datasets with more but noisier
labels than less but more accurate labels [18].
Classiﬁcation with noisy labels is a widely studied topic [8]. Yet  relatively little attention is given
to directly formulating a noise-robust loss function in the context of DNNs. Our work is motivated
by Ghosh et al. [9] who theoretically showed that mean absolute error (MAE) can be robust against
noisy labels under certain assumptions. However  as we demonstrate below  the robustness of MAE
can concurrently cause increased difﬁculty in training  and lead to performance drop. This limitation
is particularly evident when using DNNs on complicated datasets. To combat this drawback  we
advocate the use of a more general class of noise-robust loss functions  which encompass both MAE
and CCE. Compared to previous methods for DNNs  which often involve extra steps and algorithmic
modiﬁcations  changing only the loss function requires minimal intervention to existing architectures

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

and algorithms  and thus can be promptly applied. Furthermore  unlike most existing methods  the
proposed loss functions work for both closed-set and open-set noisy labels [40]. Open-set refers to
the situation where samples associated with erroneous labels do not always belong to a ground truth
class contained within the set of known classes in the training data. Conversely  closed-set means that
all labels (erroneous and correct) come from a known set of labels present in the dataset.
The main contributions of this paper are two-fold. First  we propose a novel generalization of CCE
and present a theoretical analysis of proposed loss functions in the context of noisy labels. And
second  we report a thorough empirical evaluation of the proposed loss functions using CIFAR-10 
CIFAR-100 and FASHION-MNIST datasets  and demonstrate signiﬁcant improvement in terms of
classiﬁcation accuracy over the baselines of MAE and CCE  under both closed-set and open-set noisy
labels.
The rest of the paper is organized as follows. Section 2 discusses existing approaches to the problem.
Section 3 introduces our noise-robust loss functions. Section 4 presents and analyzes the experiments
and result. Finally  section 5 concludes our paper.

2 Related Work

Numerous methods have been proposed for learning with noisy labels with DNNs in recent years.
Here  we brieﬂy review the relevant literature. Firstly  Sukhbaatar and Fergus [35] proposed account-
ing for noisy labels with a confusion matrix so that the cross entropy loss becomes

1
N

NXn=1

 log(

cXi

1
N

NXn=1

(1)

L(✓) =

 log p(ey =eyn|xn ✓ ) =

p(ey =eyn|y = i)p(y = i|xn ✓ )) 
where c represents number of classes ey represents noisy labels  y represents the latent true labels
and p(ey =eyn|y = i) is the (eyn  i)’th component of the confusion matrix. Usually  the real confusion

is because the right hand side of Eq. 1 is minimized when p(y = i|xn ✓ ) = 1 for i = eyn and 0

matrix is unknown. Several methods have been proposed to estimate it [11  14  32  17  12]. Yet 
accurate estimations can be hard to obtain. Even with the real confusion matrix  training with the
above loss function might be suboptimal for DNNs. Assuming (1) a DNN with enough capacity
to memorize the training set  and (2) a confusion matrix that is diagonally dominant  minimizing
the cross entropy with confusion matrix is equivalent to minimizing the original CCE loss. This
otherwise  8 n.
In the context of support vector machines  several theoretically motivated noise-robust loss functions
like the ramp loss  the unhinged loss and the savage loss have been introduced [5  38  27]. More
generally  Natarajan et al. [29] presented a way to modify any given surrogate loss function for binary
classiﬁcation to achieve noise-robustness. However  little attention is given to alternative noise robust
loss functions for DNNs. Ghosh et al. [10  9] proved and empirically demonstrated that MAE is
robust against noisy labels. This paper can be seen as an extension and generalization of their work.
Another popular approach attempts at cleaning up noisy labels. Veit et al. [39] suggested using
a label cleaning network in parallel with a classiﬁcation network to achieve more noise-robust
prediction. However  their method requires a small set of clean labels. Alternatively  one could
gradually replace noisy labels by neural network predictions [33  36]. Rather than using predictions
for training  Northcutt et al. [31] offered to prune the correct samples based on softmax outputs. As
we demonstrate below  this is similar to one of our approaches. Instead of pruning the dataset once 
our algorithm iteratively prunes the dataset while training until convergence.
Other approaches include treating the true labels as a latent variable and the noisy labels as an
observed variable so that EM-like algorithms can be used to learn true label distribution of the dataset
[41  18  37]. Techniques to re-weight conﬁdent samples have also been proposed. Jiang et al. [16]
used a LSTM network on top of a classiﬁcation model to learn the optimal weights on each sample 
while Ren  et al. [34] used a small clean dataset and put more weights on noisy samples which have
gradients closer to that of the clean dataset. In the context of binary classiﬁcation  Liu et al. [24]
derived an optimal importance weighting scheme for noise-robust classiﬁcation. Our method can
also be viewed as re-weighting individual samples; instead of explicitly obtaining weights  we use
the softmax outputs at each iteration as the weightings. Lastly  Azadi et al. [2] proposed a regularizer
that encourages the model to select reliable samples for noise-robustness. Another method that uses

2

knowledge distillation for noisy labels has also been proposed [23]. Both of these methods also
require a smaller clean dataset to work.

3 Generalized Cross Entropy Loss for Noise-Robust Classiﬁcations

3.1 Preliminaries
We consider the problem of k-class classiﬁcation. Let X⇢ Rd be the feature space and Y =
{1 ···   c} be the label space. In an ideal scenario  we are given a clean dataset D = {(xi  yi)}n
i=1 
where each (xi  yi) 2 (X⇥Y ). A classiﬁer is a function that maps input feature space to the label
space f : X! Rc. In this paper  we consider the common case where the function is a DNN with
the softmax output layer. For any loss function L  the (empirical) risk of the classiﬁer f is deﬁned
as RL(f ) = ED[L(f (x)  yx)]   where the expectation is over the empirical distribution. The most
commonly used loss for classiﬁcation is cross entropy. In this case  the risk becomes:

RL(f ) = ED[L(f (x; ✓)  yx)] = 

1
n

nXi=1

cXj=1

yij log fj(xi; ✓) 

(2)

where ✓ is the set of parameters of the classiﬁer  yij corresponds to the j’th element of one-hot
encoded label of the sample xi  yi = eyi 2{ 0  1}c such that 1>yi = 1 8 i  and fj denotes the j’th
element of f. Note that Pn
j=1 fj(xi; ✓) = 1  and fj(xi; ✓)  0 8j  i  ✓  since the output layer is a
softmax. The parameters of DNN can be optimized with empirical risk minimization.
We denote a dataset with label noise by D⌘ = {(xi eyi)}n
respect to each sample such that p(eyi = k|yi = j  xi) = ⌘(xi)

i=1 whereeyi’s are the noisy labels with

assumption that noise is conditionally independent of inputs given the true labels so that

jk . In this paper  we make the common

p(eyi = k|yi = j  xi) = p(eyi = k|yi = j) = ⌘jk.

In general  this noise is deﬁned to be class dependent. Noise is uniform with noise rate ⌘  if
⌘jk = 1  ⌘ for j = k  and ⌘jk = ⌘
c1 for j 6= k. The risk of classiﬁer with respect to noisy dataset
is then deﬁned as R⌘
Let f⇤ be the global minimizer of the risk RL(f ). Then  the empirical risk minimization under loss
function L is deﬁned to be noise tolerant [26] if f⇤ is a global minimum of the noisy risk R⌘
L(f ).
A loss function is called symmetric if  for some constant C 

L(f ) = ED⌘ [L(f (x) eyx)].

cXj=1

L(f (x)  j) = C 

8x 2X   8f.

(3)

c   then under uniform label noise  for any f  R⌘

The main contribution of Ghosh et al. [10] is they proved that if loss function is symmetric and
L(f )  0. Hence  f⇤ is also the
⌘< c1
global minimizer for R⌘
and L is noise tolerant. Moreover  if RL(f⇤) = 0  then L is also noise
L
tolerant under class dependent noise.
Being a nonsymmetric and unbounded loss function  CCE is sensitive to label noise. On the contrary 
MAE  as a symmetric loss function  is noise robust. For DNNs with a softmax output layer  MAE can
be computed as:

L(f⇤)  R⌘

LM AE(f (x)  ej) = ||ej  f (x)||1 = 2  2fj(x).

(4)
With this particular conﬁguration of DNN  the proposed MAE loss is  up to a constant of proportion-
ality  the same as the unhinged loss Lunh(f (x)  ej) = 1  fj(x) [38].
3.2 Lq Loss for Classiﬁcation
In this section  we will argue that MAE has some drawbacks as a classiﬁcation loss function for
DNNs  which are normally trained on large scale datasets using stochastic gradient based techniques.
Let’s look at the gradient of the loss functions:
i=1 
i=1 r✓fyi(xi; ✓)

fyi (xi;✓)r✓fyi(xi; ✓)

for MAE/unhinged loss.

@L(f (xi; ✓)  yi)

for CCE

(5)

nXi=1

@✓

=(Pn
Pn

1

3

(a)

(b)

(c)

Figure 1: (a)  (b) Test accuracy against number of epochs for training with CCE (orange) and MAE
(blue) loss on clean data with (a) CIFAR-10 and (b) CIFAR-100 datasets. (c) Average softmax
prediction for correctly (solid) and wrongly (dashed) labeled training samples  for CCE (orange) and
Lq (q = 0.7  blue) loss on CIFAR-10 with uniform noise (⌘ = 0.4).

Thus  in CCE  samples with softmax outputs that are less congruent with provided labels  and hence
smaller fyi(xi; ✓) or larger 1/fyi(xi; ✓)  are implicitly weighed more than samples with predictions
that agree more with provided labels in the gradient update. This means that  when training with CCE 
more emphasis is put on difﬁcult samples. This implicit weighting scheme is desirable for training
with clean data  but can cause overﬁtting to noisy labels. Conversely  since the 1/fyi(xi; ✓) term is
absent in its gradient  MAE treats every sample equally  which makes it more robust to noisy labels.
However  as we demonstrate empirically  this can lead to signiﬁcantly longer training time before
convergence. Moreover  without the implicit weighting scheme to focus on challenging samples  the
stochasticity involved in the training process can make learning difﬁcult. As a result  classiﬁcation
accuracy might suffer.
To demonstrate this  we conducted a simple experiment using ResNet [13] optimized with the default
setting of Adam [19] on the CIFAR datasets [20]. Fig. 1(a) shows the test accuracy curve when trained
with CCE and MAE respectively on CIFAR-10. As illustrated clearly  it took signiﬁcantly longer
to converge when trained with MAE. In agreement with our analysis  there was also a compromise
in classiﬁcation accuracy due to the increased difﬁculty of learning useful features. These adverse
effects become much more severe when using a more difﬁcult dataset  such as CIFAR-100 (see
Fig. 1(b)). Not only do we observe signiﬁcantly slower convergence  but also a substantial drop in test
accuracy when using MAE. In fact  the maximum test accuracy achieved after 2000 epochs  a long
time after training using CCE has converged  was 38.29%  while CCE achieved an higher accuracy
of 39.92% after merely 7 epochs! Despite its theoretical noise-robustness  due to the shortcoming
during training induced by its noise-robustness  we conclude that MAE is not suitable for DNNs with
challenging datasets like ImageNet.
To exploit the beneﬁts of both the noise-robustness provided by MAE and the implicit weighting
scheme of CCE  we propose using the the negative Box-Cox transformation [4] as a loss function:

Lq(f (x)  ej) =

(1  fj(x)q)

q

 

(6)

where q 2 (0  1]. Using L’Hôpital’s rule  it can be shown that the proposed loss function is equivalent
to CCE for limq!0 Lq(f (x)  ej)  and becomes MAE/unhinged loss when q = 1. Hence  this loss is
a generalization of CCE and MAE. Relatedly  Ferrari and Yang [7] viewed the maximization of Eq. 6
as a generalization of maximum likelihood and termed the loss function Lq  which we also adopt.
Theoretically  for any input x  the sum of Lq loss with respect to all classes is bounded by:

c  c(1q)

q



(1  fj(x)q)

q

c  1
q

.



cXj=1

Using this bound and under uniform noise with ⌘  1  1

c   we can show (see Appendix)

A  (RLq (f⇤)  RLq ( ˆf ))  0 

4

(7)

(8)

where A = ⌘[1c(1q)]
q(c1⌘c) < 0  f⇤ is the global minimizer of RLq (f )  and ˆf is the global minimizer
of R⌘
(f ). The larger the q  the larger the constant A  and the tighter the bound of Eq. 8. In the
Lq
extreme case of q = 1 (i.e.  for MAE)  A = 0 and RLq ( ˆf ) = RLq (f⇤). In other words  for q values
approaching 1  the optimum of the noisy risk will yield a risk value (on the clean data) that is close to
( ˆf )) is
f⇤  which implies noise tolerance. It can also be shown that the difference (R⌘
Lq
bounded under class dependent noise  provided RLq (f⇤) = 0 and qij < qii 8i 6= j (see Thm 2 in
Appendix).
The compromise on noise-robustness when using Lq over MAE prompts an easier learning process.
Let’s look at the gradients of Lq loss to see this:

(f⇤)  R⌘
Lq

1

@Lq(f (xi; ✓)  yi)

@✓

= fyi(xi; ✓)q(

fyi(xi; ✓)r✓fyi(xi; ✓)) = fyi(xi; ✓)q1r✓fyi(xi; ✓) 
where fyi(xi; ✓) 2 [0  1] 8 i and q 2 (0  1). Thus  relative to CCE  Lq loss weighs each sample by an
additional fyi(xi; ✓)q so that less emphasis is put on samples with weak agreement between softmax
outputs and the labels  which should improve robustness against noise. Relative to MAE  a weighting
of fyi(xi; ✓)q1 on each sample can facilitate learning by giving more attention to challenging
datapoints with labels that do not agree with the softmax outputs. On one hand  larger q leads to a
more noise-robust loss function. On the other hand  too large of a q can make optimization strenuous.
Hence  as we will demonstrate empirically below  it is practically useful to set q between 0 and 1 
where a tradeoff equilibrium is achieved between noise-robustness and better learning dynamics.

3.3 Truncated Lq Loss
Since a tighter bound inPc
truncated Lq loss:

j=1 L(f (x  j)) would imply stronger noise tolerance  we propose the
Ltrunc(f (x)  ej) =⇢Lq(k)

if fj(x)  k
if fj(x) > k

Lq(f (x)  ej)

(9)

where 0 < k < 1  and Lq(k) = (1  kq)/q. Note that  when k ! 0  the truncated Lq loss becomes
the normal Lq loss. Assuming k  1/c  the sum of truncated Lq loss with respect to all classes is
bounded by (see Appendix):

dLq(

1
d

) + (c  d)Lq(k) 

cXj=1

Ltrunc(f (x)  ej)  cLq(k) 

(10)

where d = max(1  (1q)1/q
for the truncated Lq loss  Lq(k)  is smaller than that for the Lq loss of Eq. 7  if

). It can be veriﬁed that the difference between upper and lower bounds

k

d[Lq(k) L q(

1
d

)] <

c(1q)  1

q

.

(11)

As an example  when k  0.3  the above inequality is satisﬁed for all q and c. When k  0.2  the
inequality is satisﬁed for all q and c  10. Since the derived bounds in Eq. 7 and Eq. 10 are tight 
introducing the threshold k can thus lead to a more noise tolerant loss function.
If the softmax output for the provided label is below a threshold  truncated Lq loss becomes a constant.
Thus  the loss gradient is zero for that sample  and it does not contribute to learning dynamics. While
Eq. 10 suggests that a larger threshold k leads to tighter bounds and hence more noise-robustness 
too large of a threshold would precipitate too many discarded samples for training. Ideally  we
would want the algorithm to train with all available clean data and ignore noisy labels. Thus the
optimal choice of k would depend on the noise in the labels. Hence  k can be treated as a (bounded)
hyper-parameter and optimized. In our experiments  we set k = 0.5 that yields a tighter bound for
truncated Lq loss  and which we observed to work well empirically.
A potential problem arises when training directly with this loss function. When the threshold is
relatively large (e.g.  k = 0.5 in a 10-class classiﬁcation problem)  at the beginning of the training
phase  most of the softmax outputs can be signiﬁcantly smaller than k  resulting in a dramatic drop

5

in the number of effective samples. Moreover  it is suboptimal to prune samples based on softmax
values at the beginning of training. To circumvent the problem  observe that  by deﬁnition of the
truncated Lq loss:
nXi=1

viLq(f (xi; ✓)  yi) + (1  vi)Lq(k) 

Ltrunc(f (xi; ✓)  yi) = argmin

where vi = 0 if fyi(xi)  k and vi = 1 otherwise  and ✓ represents the parameters of the classiﬁer.
Optimizing the above loss is the same as optimizing the following:

nXi=1

argmin

(12)

✓

✓

nXi=1

nXi=1

nXi=1

argmin

✓

viLq(f (xi; ✓)  yi)  viLq(k) = argmin
✓ w2[0 1]n

wiLq(f (xi; ✓)  yi) L q(k)

wi 

(13)
because for any ✓  the optimal wi is 1 if Lq(f (xi; ✓)  yi) L q(k) and 0 if Lq(f (xi; ✓)  yi) > Lq(k).
Hence  we can optimize the truncated Lq loss by optimizing the right hand side of Eq. 13. If Lq is
convex with respect to the parameters ✓  optimizing Eq. 13 is a biconvex optimization problem  and
the alternative convex search (ACS) algorithm [3] can be used to ﬁnd the global minimum. ACS
iteratively optimizes ✓ and w while keeping the other set of parameters ﬁxed. Despite the high
non-convexity of DNNs  we can apply ACS to ﬁnd a local minimum. We refer to the update of w as
"pruning". At every step of iteration  pruning can be carried out easily by computing f (xi; ✓(t)) for
all training samples. Only samples with fyi(xi; ✓(t))  k and Lq(f (xi; ✓)  yi) L q(k) are kept for
updating ✓ during that iteration (and hence wi = 1 ). The additional computational complexity from
the pruning steps is negligible. Interestingly  the resulting algorithm is similar to that of self-paced
learning [22].

Algorithm 1 ACS for Training with Lq Loss
Input Noisy dataset D⌘  total iterations T   threshold k

Initialize w(0)

i = 1 8 i

i=1 w(0)

while t < T do

Update ✓(0) = argmin✓Pn
Update w(t) = argminwPn
Update ✓(t) = argmin✓Pn

i=1 w(0)

i Lq(f (xi; ✓)  yi) L q(k)Pn
i=1 wiLq(f (xi; ✓(t1))  yi) L q(k)Pn
i Lq(f (xi; ✓)  yi) L q(k)Pn

i=1 w(t)

i

Output ✓(T )

i=1 wi [Pruning Step]
i

i=1 w(t)

4 Experiments

The following setup applies to all of the experiments conducted. Noisy datasets were produced by
artiﬁcially corrupting true labels. 10% of the training data was retained for validation. To realistically
mimic a noisy dataset while justiﬁably analyzing the performance of the proposed loss function  only
the training and validation data were contaminated  and test accuracies were computed with respect
to true labels. A mini-batch size of 128 was used. All networks used ReLUs in the hidden layers
and softmax layers at the output. All reported experiments were repeated ﬁve times with random
initialization of neural network parameters and randomly generated noisy labels each time. We
compared the proposed functions with CCE  MAE and also the confusion matrix-corrected CCE  as
shown in Eq. 1. Following [32]  we term this "forward correction". All experiments were conducted
with identical optimization procedures and architectures  changing only the loss functions.

4.1 Toward a Better Understanding of Lq Loss
To better grasp the behavior of Lq loss  we implemented different values of q and uniform noise
at different noise levels  and trained ResNet-34 with the default setting of Adam on CIFAR-10.
As shown in Fig. 2  when trained on clean dataset  increasing q not only slowed down the rate of
convergence  but also lowered the classiﬁcation accuracy. More interesting phenomena appeared
when trained on noisy data. When CCE (q = 0) was used  the classiﬁer ﬁrst learned predictive

6

(a)

(d)

(b)

(e)

(c)

(f)

Figure 2: The test accuracy and validation loss against number of epochs for training with Lq loss at
different values of q. (a) and (d): ⌘ = 0.0; (b) and (e): ⌘ = 0.2; (c) and (f): ⌘ = 0.6.

patterns  presumably from the noise-free labels  before overﬁtting strongly to the noisy labels  in
agreement with Arpit et al.’s observations [1]. Training with increased q values delayed overﬁtting
and attained higher classiﬁcation accuracies. One interpretation of this behavior is that the classiﬁer
could learn more about predictive features before overﬁtting. This interpretation is supported by our
plot of the average softmax values with respect to the correctly and wrongly labeled samples on the
training set for CCE and Lq (q = 0.7) loss  and with 40% uniform noise (Fig. 1(c)). For CCE  the
average softmax for wrongly labeled samples remained small at the beginning  but grew quickly when
the model started overﬁtting. Lq loss  on the other hand  resulted in signiﬁcantly smaller softmax
values for wrongly labeled data. This observation further serves as an empirical justiﬁcation for the
use of truncated Lq loss as described in section 3.3.
We also observed that there was a threshold of q beyond which overﬁtting never kicked in before
convergence. When ⌘ = 0.2 for instance  training with Lq loss with q = 0.8 produced an overﬁtting-
free training process. Empirically  we noted that  the noisier the data  the larger this threshold is.
However  too large of a q hampers the classiﬁcation accuracy  and thus a larger q is not always
preferred. In general  q can be treated as a hyper-parameter that can be optimized  say via monitoring
validation accuracy. In remaining experiments  we used q = 0.7  which yielded a good compromise
between fast convergence and noise robustness (no overﬁtting was observed for ⌘  0.5).
4.2 Datasets
CIFAR-10/CIFAR-100: ResNet-34 was used as the classiﬁer optimized with the loss functions
mentioned above. Per-pixel mean subtraction  horizontal random ﬂip and 32 ⇥ 32 random crops after
padding with 4 pixels on each side was performed as data preprocessing and augmentation. Following
[15]  we used stochastic gradient descent (SGD) with 0.9 momentum  a weight decay of 104 and
learning rate of 0.01  and divided it by 10 after 40 and 80 epochs (120 in total) for CIFAR-10  and
after 80 and 120 (150 in total) for CIFAR-100. To ensure a fair comparison  the identical optimization
scheme was used for truncated Lq loss. We trained with the entire dataset for the ﬁrst 40 epochs for
CIFAR-10 and 80 for CIFAR-100  and started pruning and training with the pruned dataset afterwards.
Pruning was done every 10 epochs. To prevent overﬁtting  we used the model at the optimal epoch

7

Table 1: Average test accuracy and standard deviation (5 runs) on experiments with closed-set noise.
We report accuracies of the epoch where validation accuracy is maximum. Forward T and ˆT represent
forward correction with the true and estimated confusion matrices  respectively [32]. q = 0.7 was
used for all experiments with Lq loss and truncated Lq loss. Best 2 accuracies are bold faced.

Datasets

Loss Functions

FASHION
MNIST

CIFAR-10

CIFAR-100

CCE
MAE

Forward T
Forward ˆT
Lq
Trunc Lq
CCE
MAE

Forward T
Forward ˆT
Lq
Trunc Lq
CCE
MAE

Forward T
Forward ˆT
Trunc Lq

Lq

0.2

93.24 ± 0.12
80.39 ± 4.68
93.64 ± 0.12
93.26 ± 0.10
93.35 ± 0.09
93.21 ± 0.05
86.98 ± 0.44
83.72 ± 3.84
88.63 ± 0.14
87.99 ± 0.36
89.83 ± 0.20
89.7 ± 0.11
58.72 ± 0.26
15.80 ± 1.38
63.16 ± 0.37
39.19 ± 2.61
66.81 ± 0.42
67.61 ± 0.18

Uniform Noise
Noise Rate ⌘

0.4

0.6

92.09 ± 0.18
79.30 ± 6.20
92.69 ± 0.20
92.24 ± 0.15
92.58 ± 0.11
92.60 ± 0.17
81.88 ± 0.29
67.00 ± 4.45
85.07 ± 0.29
83.25 ± 0.38
87.13 ± 0.22
87.62 ± 0.26
48.20 ± 0.65
9.03 ± 1.54
54.65 ± 0.88
31.05 ± 1.44
61.77 ± 0.24
62.64 ± 0.33

90.29 ± 0.35
82.41 ± 5.29
91.16 ± 0.16
90.54 ± 0.10
91.30 ± 0.20
91.56 ± 0.16
74.14 ± 0.56
64.21 ± 5.28
79.12 ± 0.64
74.96 ± 0.65
82.54 ± 0.23
82.70 ± 0.23
37.41 ± 0.94
7.74 ± 1.48
44.62 ± 0.82
19.12 ± 1.95
53.16 ± 0.78
54.04 ± 0.56

0.8

86.20 ± 0.68
74.73 ± 5.26
87.59 ± 0.35
85.57 ± 0.86
88.01 ± 0.22
88.33 ± 0.38
53.82 ± 1.04
38.63 ± 2.62
64.30 ± 0.70
54.64 ± 0.44
64.07 ± 1.38
67.92 ± 0.60
18.10 ± 0.82
3.76 ± 0.27
24.83 ± 0.71
8.99 ± 0.58
29.16 ± 0.74
29.60 ± 0.51

0.1

94.06 ± 0.05
74.03 ± 6.32
94.33 ± 0.10
94.09 ± 0.10
93.51 ± 0.17
93.53 ± 0.11
90.69 ± 0.17
82.61 ± 4.81
91.32 ± 0.21
90.52 ± 0.26
90.91 ± 0.22
90.43 ± 0.25
66.54 ± 0.42
13.38 ± 1.84
71.05 ± 0.30
45.96 ± 1.21
68.36 ± 0.42
68.86 ± 0.14

Class Dependent Noise

Noise Rate ⌘

0.2

0.3

93.72 ± 0.14
63.03 ± 3.91
94.03 ± 0.11
93.66 ± 0.09
93.24 ± 0.14
93.36 ± 0.07
88.59 ± 0.34
52.93 ± 3.60
90.35 ± 0.26
89.09 ± 0.47
89.33 ± 0.17
89.45 ± 0.29
59.20 ± 0.18
11.50 ± 1.16
71.08 ± 0.22
42.46 ± 2.16
66.59 ± 0.22
66.59 ± 0.23

92.72 ± 0.21
58.14 ± 0.14
93.91 ± 0.14
93.52 ± 0.16
92.21 ± 0.27
92.76 ± 0.14
86.14 ± 0.40
50.36 ± 5.55
89.25 ± 0.43
86.79 ± 0.36
85.45 ± 0.74
87.10 ± 0.22
51.40 ± 0.16
8.91 ± 0.89
70.76 ± 0.26
38.13 ± 2.97
61.45 ± 0.26
61.87 ± 0.39

0.4

89.82 ± 0.31
56.04 ± 3.76
93.65 ± 0.11
88.53 ± 4.81
89.53 ± 0.53
91.62 ± 0.34
80.11 ±1.44
45.52 ± 0.13
88.12 ± 0.32
83.55 ± 0.58
76.74± 0.61
82.28 ± 0.67
42.74 ± 0.61
8.20 ± 1.04
70.82 ± 0.45
34.44 ± 1.93
47.22 ± 1.15
47.66 ± 0.69

Table 2: Average test accuracy on experiments with CIFAR-10. We replicated the exact experimental
setup as in [40]. The reported accuracies are the average last epoch accuracies after training for 100
epochs. ⌘ = 40%. CCE  Forward and method by Wang et al. are adapted for direct comparison.

Noise type
CIFAR-10 + CIFAR-100 (open-set noise)
CIFAR-10 (closed-set noise)

CCE [40]

62.92
62.38

64.18
77.81

Forward [40] Wang  et al. [40] MAE Lq

79.28
78.15

75.06
74.31

71.10
64.79

Trunc Lq
79.55
79.12

based on maximum validation accuracy for pruning. Uniform noise was generated by mapping a true
label to a random label through uniform sampling. Following Patrini  et al. [32] class dependent noise
was generated by mapping TRUCK ! AUTOMOBILE  BIRD ! AIRPLANE  DEER ! HORSE 
and CAT $ DOG with probability ⌘ for CIFAR-10. For CIFAR-100  we simulated class-dependent
noise by ﬂipping each class into the next circularly with probability ⌘.
We also tested noise-robustness of our loss function on open-set noise using CIFAR-10. For a direct
comparison  we followed the identical setup as described in [40]. For this experiment  the classiﬁer
was trained for only 100 epochs. We observed validation loss plateaued after about 10 epochs  and
hence started pruning the data afterwards at 10-epoch intervals. The open-set noise was generated by
using images from the CIFAR-100 dataset. A random CIFAR-10 label was assigned to these images.
FASHION-MNIST: ResNet-18 was used. The identical data preprocessing  augmentation  and
optimization procedure as in CIFAR-10 was deployed for training. To generate a realistic class
dependent noise  we used the t-SNE [25] plot of the dataset to associated classes with similar
embeddings  and mapped BOOT ! SNEAKER   SNEAKER ! SANDALS  PULLOVER ! SHIRT 
COAT $ DRESS with probability ⌘.

4.3 Results and Discussion

Experimental results with closed-set noise is summarized in Table 1. For uniform noise  proposed loss
functions outperformed the baselines signiﬁcantly  including forward correction with the ground truth
confusion matrices. In agreement with our theoretical expectations  truncating the Lq loss enhanced
results. For class dependent noise  in general Forward T offered the best performance  as it relied on
the knowledge of the ground truth confusion matrix. Truncated Lq loss produced similar accuracies
as Forward ˆT for FASHION-MNIST and better results for CIFAR datasets  and outperformed the
other baselines at most noise levels for all datasets. While using Lq loss improved over baselines for
CIFAR-100  no improvements were observed for FASHION-MNIST and CIFAR-10 datasets. We
believe this is in part because very similar classes were grouped together for the confusion matrices
and consquently the DNNs might falsely put high conﬁdence on wrongly labeled samples.

8

In general  classiﬁcation accuracy for both uniform and class dependent noise would be further
improved relative to baselines with optimized q and k values and more number of epochs. Based on
the experimental results  we believe the proposed approach would work well when correctly labeled
data can be differentiated from wrongly labeled data based on softmax outputs  which is often the
case with large-scale data and expressive models. We also observed that MAE performed poorly for
all datasets at all noise levels  presumably because DNNs like ResNet struggled to optimize with
MAE loss  especially on challenging datasets such as CIFAR-100.
Table 2 summarizes the results for open-set noise with CIFAR-10. Following Wang et al. [40]  we
reported the last-epoch test accuracy after training for 100 epochs. We also repeated the closed-set
noise experiment with their setup. Using Lq loss noticeably prevented overﬁtting  and using truncated
Lq loss achieved better results than the state-of-the-art method for open-set noise reported in [40].
Moreover  our method is signiﬁcantly easier to implement. Lastly  note that the poor performance of
Lq loss compared to MAE is due to the fact that test accuracy reported here is long after the model
started overﬁtting  since a shallow CNN without data augmentation was deployed for this experiment.

5 Conclusion

In conclusion  we proposed theoretically grounded and easy-to-use classes of noise-robust loss
functions  the Lq loss and the truncated Lq loss  for classiﬁcation with noisy labels that can be
employed with any existing DNN algorithm. We empirically veriﬁed noise robustness on various
datasets with both closed- and open-set noise scenarios.

Acknowledgments
This work was supported by NIH R01 grants (R01LM012719 and R01AG053949)  the NSF NeuroNex
grant 1707312  and NSF CAREER grant (1748377).

References
[1] Devansh Arpit  Stanisław Jastrz˛ebski  Nicolas Ballas  David Krueger  Emmanuel Bengio 
Maxinder S Kanwal  Tegan Maharaj  Asja Fischer  Aaron Courville  Yoshua Bengio  et al. A
closer look at memorization in deep networks. arXiv preprint arXiv:1706.05394  2017.

[2] Samaneh Azadi  Jiashi Feng  Stefanie Jegelka  and Trevor Darrell. Auxiliary image regulariza-

tion for deep cnns with noisy labels. arXiv preprint arXiv:1511.07069  2015.

[3] Mokhtar S Bazaraa  Hanif D Sherali  and Chitharanjan M Shetty. Nonlinear programming:

theory and algorithms. John Wiley & Sons  2013.

[4] George EP Box and David R Cox. An analysis of transformations. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 211–252  1964.

[5] J Paul Brooks. Support vector machines with the ramp loss and the hard margin loss. Operations

research  59(2):467–479  2011.

[6] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition  2009. CVPR 2009.
IEEE Conference on  pages 248–255. IEEE  2009.

[7] Davide Ferrari  Yuhong Yang  et al. Maximum lq-likelihood estimation. The Annals of Statistics 

38(2):753–783  2010.

[8] Benoît Frénay and Michel Verleysen. Classiﬁcation in the presence of label noise: a survey.

IEEE transactions on neural networks and learning systems  25(5):845–869  2014.

[9] Aritra Ghosh  Himanshu Kumar  and PS Sastry. Robust loss functions under label noise for

deep neural networks. In AAAI  pages 1919–1925  2017.

[10] Aritra Ghosh  Naresh Manwani  and PS Sastry. Making risk minimization tolerant to label

noise. Neurocomputing  160:93–107  2015.

9

[11] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta-

tion layer. 2016.

[12] Bo Han  Jiangchao Yao  Gang Niu  Mingyuan Zhou  Ivor Tsang  Ya Zhang  and Masashi
Sugiyama. Masking: A new perspective of noisy supervision. arXiv preprint arXiv:1805.08193 
2018.

[13] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[14] Dan Hendrycks  Mantas Mazeika  Duncan Wilson  and Kevin Gimpel. Using trusted data to
train deep networks on labels corrupted by severe noise. arXiv preprint arXiv:1802.05300 
2018.

[15] Gao Huang  Yu Sun  Zhuang Liu  Daniel Sedra  and Kilian Q Weinberger. Deep networks with
stochastic depth. In European Conference on Computer Vision  pages 646–661. Springer  2016.

[16] Lu Jiang  Zhengyuan Zhou  Thomas Leung  Li-Jia Li  and Li Fei-Fei. Mentornet: Regularizing

very deep neural networks on corrupted labels. arXiv preprint arXiv:1712.05055  2017.

[17] Ishan Jindal  Matthew Nokleby  and Xuewen Chen. Learning deep networks from noisy labels
with dropout regularization. In Data Mining (ICDM)  2016 IEEE 16th International Conference
on  pages 967–972. IEEE  2016.

[18] Ashish Khetan  Zachary C Lipton  and Anima Anandkumar. Learning from noisy singly-labeled

data. arXiv preprint arXiv:1712.04577  2017.

[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[21] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[22] M Pawan Kumar  Benjamin Packer  and Daphne Koller. Self-paced learning for latent variable

models. In Advances in Neural Information Processing Systems  pages 1189–1197  2010.

[23] Yuncheng Li  Jianchao Yang  Yale Song  Liangliang Cao  Jiebo Luo  and Jia Li. Learning from

noisy labels with distillation. arXiv preprint arXiv:1703.02391  2017.

[24] Tongliang Liu and Dacheng Tao. Classiﬁcation with noisy labels by importance reweighting.

IEEE Transactions on pattern analysis and machine intelligence  38(3):447–461  2016.

[25] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research  9(Nov):2579–2605  2008.

[26] Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions

on cybernetics  43(3):1146–1151  2013.

[27] Hamed Masnadi-Shirazi and Nuno Vasconcelos. On the design of loss functions for classi-
ﬁcation: theory  robustness to outliers  and savageboost. In Advances in neural information
processing systems  pages 1049–1056  2009.

[28] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[29] Nagarajan Natarajan  Inderjit S Dhillon  Pradeep K Ravikumar  and Ambuj Tewari. Learning
with noisy labels. In Advances in neural information processing systems  pages 1196–1204 
2013.

10

[30] Hyeonwoo Noh  Seunghoon Hong  and Bohyung Han. Learning deconvolution network for
semantic segmentation. In Proceedings of the IEEE International Conference on Computer
Vision  pages 1520–1528  2015.

[31] Curtis G Northcutt  Tailin Wu  and Isaac L Chuang. Learning with conﬁdent examples: Rank

pruning for robust classiﬁcation with noisy labels. arXiv preprint arXiv:1705.01936  2017.

[32] Giorgio Patrini  Alessandro Rozza  Aditya Krishna Menon  Richard Nock  and Lizhen Qu.
Making deep neural networks robust to label noise: a loss correction approach. stat  1050:22 
2017.

[33] Scott Reed  Honglak Lee  Dragomir Anguelov  Christian Szegedy  Dumitru Erhan  and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596  2014.

[34] Mengye Ren  Wenyuan Zeng  Bin Yang  and Raquel Urtasun. Learning to reweight examples

for robust deep learning. arXiv preprint arXiv:1803.09050  2018.

[35] Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks.

arXiv preprint arXiv:1406.2080  2(3):4  2014.

[36] Daiki Tanaka  Daiki Ikami  Toshihiko Yamasaki  and Kiyoharu Aizawa. Joint optimization

framework for learning with noisy labels. arXiv preprint arXiv:1803.11364  2018.

[37] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural

networks. In Advances in Neural Information Processing Systems  pages 5601–5610  2017.

[38] Brendan Van Rooyen  Aditya Menon  and Robert C Williamson. Learning with symmetric
label noise: The importance of being unhinged. In Advances in Neural Information Processing
Systems  pages 10–18  2015.

[39] Andreas Veit  Neil Alldrin  Gal Chechik  Ivan Krasin  Abhinav Gupta  and Serge Belongie.
Learning from noisy large-scale datasets with minimal supervision. In The Conference on
Computer Vision and Pattern Recognition  2017.

[40] Yisen Wang  Weiyang Liu  Xingjun Ma  James Bailey  Hongyuan Zha  Le Song  and Shu-Tao

Xia. Iterative learning with open-set noisy labels. arXiv preprint arXiv:1804.00092  2018.

[41] Tong Xiao  Tian Xia  Yi Yang  Chang Huang  and Xiaogang Wang. Learning from massive
noisy labeled data for image classiﬁcation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 2691–2699  2015.

[42] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

11

,Zhilu Zhang
Mert Sabuncu