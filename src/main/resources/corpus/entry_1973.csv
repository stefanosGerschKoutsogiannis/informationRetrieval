2019,Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery,This paper develops a new class of nonconvex regularizers for low-rank matrix recovery. Many regularizers are motivated as convex relaxations of the \emph{matrix rank} function. Our new factor group-sparse regularizers are motivated as a relaxation of the \emph{number of nonzero columns} in a factorization of the matrix. These nonconvex regularizers are sharper than the nuclear norm; indeed  we show they are related to Schatten-$p$ norms with arbitrarily small $0 < p \leq 1$. Moreover  these factor group-sparse regularizers can be written in a factored form that enables efficient and effective nonconvex optimization; notably  the method does not use singular value decomposition. We provide generalization error bounds for low-rank matrix completion which show improved upper bounds for Schatten-$p$ norm reglarization as $p$ decreases. Compared to the max norm and the factored formulation of the nuclear norm  factor group-sparse regularizers are more efficient  accurate  and robust to the initial guess of rank. Experiments show promising performance of factor group-sparse regularization for low-rank matrix completion and robust principal component analysis.,Factor Group-Sparse Regularization for Efﬁcient

Low-Rank Matrix Recovery

Jicong Fan

Cornell University
Ithaca  NY 14850

jf577@cornell.edu

Lijun Ding

Cornell University
Ithaca  NY 14850

ld446@cornell.edu

Yudong Chen

Cornell University
Ithaca  NY 14850

yudong.chen@cornell.edu

Madeleine Udell
Cornell University
Ithaca  NY 14850

udell@cornell.edu

Abstract

This paper develops a new class of nonconvex regularizers for low-rank matrix
recovery. Many regularizers are motivated as convex relaxations of the matrix rank
function. Our new factor group-sparse regularizers are motivated as a relaxation of
the number of nonzero columns in a factorization of the matrix. These nonconvex
regularizers are sharper than the nuclear norm; indeed  we show they are related
to Schatten-p norms with arbitrarily small 0 < p ≤ 1. Moreover  these factor
group-sparse regularizers can be written in a factored form that enables efﬁcient
and effective nonconvex optimization; notably  the method does not use singular
value decomposition. We provide generalization error bounds for low-rank matrix
completion which show improved upper bounds for Schatten-p norm reglarization
as p decreases. Compared to the max norm and the factored formulation of the
nuclear norm  factor group-sparse regularizers are more efﬁcient  accurate  and
robust to the initial guess of rank. Experiments show promising performance
of factor group-sparse regularization for low-rank matrix completion and robust
principal component analysis.

1

Introduction

Low-rank matrices appear throughout the sciences and engineering  in ﬁelds as diverse as computer
science  biology  and economics [1]. One canonical low-rank matrix recovery problem is low-rank
matrix completion (LRMC) [2  3  4  5  6  7  8  9  10]  which aims to recover a low-rank matrix
from a few entries. LRMC has been used to impute missing data  make recommendations  discover
latent structure  perform image inpainting  and classiﬁcation [11  12  1]. Another important low-rank
recovery problem is robust principal components analysis (RPCA) [13  14  15  16  17]  which aims
to recover a low-rank matrix from sparse but arbitrary corruptions. RPCA is often used for denoising
and image/video processing [18].
LRMC Take LRMC as an example. Suppose M ∈ Rm×n is a low-rank matrix with rank(M ) =
r (cid:28) min(m  n). We wish to recover M from a few observed entries. Let Ω ⊂ [m] × [n] index the
observed entries. Suppose card(Ω)  the number of observations  is sufﬁciently large. A natural idea
is to recover the missing entries by solving

minimize

X

rank(X)  subject to PΩ(X) = PΩ(M ) 

(1)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

where the operator PΩ : Rm×n → Rm×n acts on any X ∈ Rm×n in the following way:
(PΩ(X))ij = Xij if (i  j) ∈ Ω and 0 if (i  j) /∈ Ω. However  since the direct rank minimiza-
tion problem (1) is NP-hard  a standard approach is to replace the rank with a tractable surrogae
R(X) and solve

minimize

X

R(X)  subject to PΩ(X) = PΩ(M ).

(2)

Below we review typical choices of R(X) to provide context for our factor group-sparse regularizers.

Nuclear and Schatten-p norms One popular convex surrogate function for the rank function is
the nuclear norm (also called trace norm)  which is deﬁned as the sum of singular values:

min(m n)(cid:88)

i=1

(cid:107)X(cid:107)∗ :=

σi(X) 

(3)

(cid:16) min(m n)(cid:88)

(cid:17)1/p

where σi(X) denotes the i-th largest singular value of X ∈ Rm×n. Variants of the nuclear norm 
including the truncated nuclear norm [19] and weighted nuclear norm [20]  sometimes perform
better empirically on imputation tasks.
The Schatten-p norms1 with 0 ≤ p ≤ 1 [21  22  23] form another important class of rank surrogates:

(cid:107)X(cid:107)Sp :=

σp
i (X)

.

(4)

For p = 1  we have (cid:107)X(cid:107)1
S1
surrogate for rank(X). In the extreme case p = 0  (cid:107)X(cid:107)0
S0
wish to minimize. Thus we see (cid:107)X(cid:107)p
Sp
the nuclear norm. Instead of (1)  we hope to solve

= (cid:107)X(cid:107)∗  the nuclear norm. For 0 < p < 1  (cid:107)X(cid:107)p

is a nonconvex
= rank(X)  which is exactly what we
with 0 < p < 1 interpolates between the rank function and

Sp

i=1

X

(cid:107)X(cid:107)p
Sp

  subject to PΩ(X) = PΩ(M ) 

minimize

(5)
with 0 < p ≤ 1. Smaller values of p (0 < p ≤ 1) are better approximations of the rank function
and may lead to better recovery performance for LRMC and RPCA. However  for 0 < p < 1 the
problem (5) is nonconvex  and it is not generally possible to guarantee we ﬁnd a global optimal
solution. Even worse  common algorithms for minimizing the nuclear norm and Schatten-p norm
cannot scale to large matrices because they compute the singular value decomposition (SVD) in every
iteration of the optimization [2  3  24].

Factor regularizations A few SVD-free methods have been develoepd to recover large low-rank
matrices. For example  the work in [25  26] uses the well-known fact that

1
2

(cid:0)(cid:107)A(cid:107)2
(cid:0)(cid:107)A(cid:107)2

(cid:1) 

(cid:1).

(6)

(7)

(cid:107)X(cid:107)∗ = min

AB=X

(cid:107)A(cid:107)F(cid:107)B(cid:107)F = min

AB=X

F + (cid:107)B(cid:107)2

F

where A ∈ Rm×d  B ∈ Rd×n  and d ≥ rank(X). For LRMC they suggest solving

minimize

A B

1
2

(cid:107)PΩ(M − AB)(cid:107)2

λ
2

F +

(cid:0)(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:1) in (6). This expression matches

1
2

F + (cid:107)B(cid:107)2

In this paper  we use the name factored nuclear norm (F-nuclear norm for short) for the variational
characterization of nuclear norm as minAB=X
the nuclear norm when d is chosen large enough. Srebro and Salakhutdinov [27] proposed a weighted
F-nuclear norm; the corresponding formulation of matrix completion is similar to (7). Note that to
solve (7) we must ﬁrst choose the value of d. We require d ≥ r := rank(M ) to be able to recover (or
even represent) M. Any d ≥ r gives the same solution AB to (7). However  as d increases from r 
the difﬁculty of optimizing the objective increases. Indeed  we observe in our experiments that the
recovery error is larger for large d using standard algorithms  particularly when the proportion of
observed entries is low. In practice  it is difﬁcult to guess r  and generally a very large d is required.
The methods of [28] and [29] estimate r dynamically.

F

1Note that formally (cid:107) · (cid:107)Sp with 0 ≤ p < 1 is a quasi-norm  not a norm; abusively  we still use the term

“norm” in this paper.

2

(cid:0)(cid:107)A(cid:107)∗+(cid:107)B(cid:107)∗

(cid:1)2

2

Another SVD-free surrogate of rank is the max norm  proposed by Srebro and Shraibman [30]:

(cid:107)X(cid:107)max = min

AB=X

(cid:0) max

i

(cid:107)ai(cid:107)(cid:1)(cid:0) max

(cid:107)bj(cid:107)(cid:1) 

j

(8)

where ai and bj denotes the i-th row of A and the j-th row of BT respectively. Lee et al. [31]
proposed several efﬁcient algorithms to solve optimization problems with the max norm. Foygel and
Srebro [5] provided recovery guarantees for LRMC using the max norm as a regularizer.
Another very different approach uses implicit regularization. Gunasekar et al. [32] show that for full
dimensional factorization without any regularization  gradient descent with small enough step size
and initialized close enough to the origin converges to the minimum nuclear norm solution. However 
convergence slows as the initial point and step size converge to zero  making this method impractical.
Shang et al. [33] provided the following characterization of the Schatten-1/2 norm:

(cid:107)X(cid:107)S1/2 = min

AB=X

(cid:107)A(cid:107)∗(cid:107)B(cid:107)∗ = min

AB=X

.

(9)

  one can minimize (cid:107)A(cid:107)∗ + (cid:107)B(cid:107)∗  which is much
Hence instead of directly minimizing (cid:107)X(cid:107)1/2
S1/2
easier when r ≤ d (cid:28) min(m  n). But again  this method and its extension (cid:107)A(cid:107)∗ + 1
F proposed
in [34] require d ≥ r  and the computational cost increases with larger d. Figure 1(d) shows these
approaches are nearly as expensive as directly minimizing (cid:107)X(cid:107)p
when d is large. We call the
Sp
regularizers minAB=X ((cid:107)A(cid:107)∗ + (cid:107)B(cid:107)∗) and minAB=X ((cid:107)A(cid:107)∗ + 1
F ) the Bi-nuclear norm and
F2+nuclear norm respectively.

2(cid:107)B(cid:107)2

2(cid:107)B(cid:107)2

Our methods and contributions
In this paper  we propose a new class of factor group-sparse
regularizers (FGSR) as a surrogate for the rank of X. To derive our regularizers  we introduce the
factorization AB = X and seek to minimize the number of nonzero columns of A or BT . Each
factor group-sparse regularizer is formed by taking the convex relaxation of the number of nonzero
columns. These regularizers are convex functions of the factors A and B but capture the nonconvex
Schatten-p (quasi-)norms of X using the nonconvex factorization constraint X = AB.

• We show that these regularizers match arbitrarily sharp Schatten-p norms: for each 0 < p(cid:48) ≤ 1 
there is some p < p(cid:48) for which we exhibit a factor group-sparse regularizer equal to the sum of
the pth powers of the singular values of X.
• For a class of p  we propose a generalized factorization model that enables us to minimize
(cid:107)X(cid:107)p
Sp

• We show in experiments that the resulting algorithms improve on state-of-the-art methods for
• We prove generalization error bounds for LRMC with Schatten-p norm regularization  which

without performing the SVD.

LRMC and RPCA.

explain the superiority of our methods over nuclear norm minimization.

Notation Throughout this paper  (cid:107) · (cid:107) denotes the Euclidean norm of a vector argument. We
factor X ∈ Rm×n as A = [a1  a2 ···   ad] ∈ Rm×d and B = [b1  b2 ···   bd]T ∈ Rd×n  where
d ≥ r := rank(X)  and aj and bj are column vectors. Without loss of generality  we assume m ≤ n.
All proofs appear in the supplement.

2 FGSRs match Schatten-p norms with p = 2

3 or 1
2.

Let nnzc(A) denote the number of nonzero columns of matrix A. Write the rank of X ∈ Rm×n as

rank(X) = min
AB=X

Now relax: notice nnzc(A) ≥(cid:80)d

nnzc(A) = min
AB=X

nnzc(BT ) = min
AB=X

1
2

this relaxation in (10) gives a factored characterization of the Schatten-p norm with p = 1

2 or 2
3.

j=1 (cid:107)aj(cid:107) when (cid:107)aj(cid:107) ≤ 1 for each column j. We show that using

(cid:0)nnzc(A) + nnzc(BT )(cid:1).

(10)

3

Theorem 1. Fix α > 0. For any matrix X ∈ Rm×n with rank(X) = r ≤ d ≤ min(m  n) 

d(cid:88)
d(cid:88)

j=1

j=1

min
j=1 aj bT
j

X=(cid:80)d
X=(cid:80)d

min
j=1 aj bT
j

(cid:107)aj(cid:107) + (cid:107)bj(cid:107) =2

(X)

(cid:107)aj(cid:107) +

j=1

3α1/3

2

(cid:107)bj(cid:107)2 =

α
2

σ2/3
j

(X).

r(cid:88)

σ1/2
j

r(cid:88)

j=1

(11)

(12)

Denote the SVD of X as X = UX SX V T
B = S1/2
Motivated by this theorem  we deﬁne the following factor group-sparse regularizers (FGSR):

X ; in equation (12)  when A = α1/3UX S2/3

X . Equality holds in equation (11) when A = UX S1/2

X and B = α−1/3S1/3

X .
X V T

X V T

X and

FGSR1/2(X) :=

1
2

min

AB=X
2

(cid:107)A(cid:107)2 1 + (cid:107)BT(cid:107)2 1.

(13)

FGSR2/3(X) :=

where (cid:107)A(cid:107)2 1 :=(cid:80)d
(14)
j=1 (cid:107)aj(cid:107). Theorem 1 shows that FGSR2/3 has the same value regardless of the
r(cid:88)

choice of α  which justiﬁes the deﬁnition. As a corollary of Theorem 1  we see

(cid:107)A(cid:107)2 1 +

(cid:107)B(cid:107)2
F  

r(cid:88)

3α1/3

AB=X

min

α
2

FGSR2/3(X) =

σ2/3
j

(X) = (cid:107)X(cid:107)2/3
S2/3

.

FGSR1/2(X) =

σ1/2
j

(X) = (cid:107)X(cid:107)1/2
S1/2

 

j=1

j=1

To solve optimization problems involving these surrogates for the rank  we can use the deﬁnition
of the FGSR and optimize over the factors A and B. It is easier to minimize FGSR2/3(X) than to
minimize FGSR1/2(X) because the latter has two nonsmooth terms.
As surrogates for the rank function  FGSR2/3 and FGSR1/2 have the following advantages:

rem 1 are tighter approximations to the rank of X.

• Tighter rank approximation. Compared to the nuclear norm  the spectral quantities in Theo-
• Robust to rank initialization. The iterative algorithms we propose in Sections 4 and 6 to
minimize FGSR2/3 and FGSR1/2 quickly force some of the columns of A and BT to zero 
where they remain. Hence the number of nonzero columns is reduced dynamically  and converges
to r quickly in experiments: these methods are rank-revealing. In constrast  iterative methods to
minimize the F-nuclear norm or max norm never produce an exactly-rank-r iterate after a ﬁnite
number of iterations.
• Low computational cost. Most optimization methods for solving problems with the Schatten-p
norm perform SVD on X at every iteration  with time complexity of O(m2n) (supposing
m ≤ n) [21  22]. In contrast  the natural algorithm to minimize FGSR2/3 and FGSR1/2 does
not use the SVD  as the regularizers are simple (not spectral) functions of the factors. The main
computational cost is to form AB  which has a time complexity of O(d(cid:48)mn) when the iterates
A and B have d(cid:48) nonzero columns. The complexity of LRMC can be as low as O(d(cid:48)card(Ω)).

3 Toward exact rank minimization
In the previous section  we developed a factored representation for (cid:107)X(cid:107)p
Sp
section develops a similar representation for (cid:107)X(cid:107)p
with arbitrarily small p.
Sp
Theorem 2. Fix α > 0  and choose q ∈ {1  1
r ≤ d ≤ min(m  n)  we have

2   1

4  ···}. For any matrix X ∈ Rm×n with rank(X) =

when p = 2

3 or 1

2. This

(cid:107)aj(cid:107)q + α(cid:107)bj(cid:107) =(1 + 1/q)αq/(q+1)

1
q

σq/(q+1)
j

(X) 

(15)

1
q

(cid:107)aj(cid:107)q +

α
2

(cid:107)bj(cid:107)2 =(1/2 + 1/q)αq/(q+2)

σ2q/(2+q)
j

(X).

(16)

r(cid:88)
r(cid:88)

j=1

d(cid:88)
d(cid:88)

j=1

j=1

min
j=1 aj bT
j

X=(cid:80)d
X=(cid:80)d

min
j=1 aj bT
j

j=1

4

By choosing an appropriate q  these representations give arbitrarily tight approximations to the rank 
since (cid:107)X(cid:107)p
Sp

→ rank(X) as p → 0. For example  use (16) in Theorem 2 when q = 1

4 to see

(cid:107)aj(cid:107)1/4 +

1
1/4

α
2

(cid:107)bj(cid:107)2 = 4.5α1/9

σ2/9
i

(X) = 4.5α1/9(cid:107)X(cid:107)2/9
S2/9

.

(17)

d(cid:88)

j=1

(cid:80)d

min
j=1 aj bT

j =X

d(cid:88)

i=1

Equality holds in equation (15) when A = α1/(q+1)UX S1/(q+1)
in equation (16)  when A = α1/(q+2)UX S2/(q+2)

and B = α−1/(q+2)Sq/(q+2)

X

X

X

X .
V T

and B = α−1/(q+1)Sq/(q+1)

X

X ;
V T

4 Application to low-rank matrix completion

As an application  we model noiseless matrix completion using FGSR as a surrogate for the rank:

minimize

X

FGSR(X) 

subject to PΩ(X) = PΩ(M ).

(18)

Take FGSR2/3 as an example. We rewrite (18) as

minimize
X A B

(cid:107)A(cid:107)2 1 +

(cid:107)B(cid:107)2
F  

α
2

subject to X = AB  PΩ(X) = PΩ(M ).

(19)

This problem is separable in the three blocks of unknowns X  A  and B. We propose to use the
Alternating Direction Method of Multipliers (ADMM) [35  36  37] with linearization to solve this
problem  as the ADMM subproblem for A has no closed-form solution. Details are in the supplement.
Now consider an application to noisy matrix completion. Suppose we observe PΩ(Me) with
Me = M + E  where E represents measurement noise. Model the problem using FGSR2/3 as

minimize

A B

(cid:107)A(cid:107)2 1 +

(cid:107)B(cid:107)2

F +

α
2

β
2

(cid:107)PΩ(Me − AB)(cid:107)2
F .

(20)

We can still solve the problem via linearized ADMM. However  proximal alternating linearized
minimization (PALM) [38  39] gives a more efﬁcient method. Details are in the supplement.
Motivated by Theorem 2  we can also model noisy matrix completion with a sharper rank surrogate:

minimize

A B

1
2

(cid:107)PΩ(Me − AB)(cid:107)2

(cid:107)A(cid:107)q

2 q +

(cid:107)BT(cid:107)2

F

α
2

 

(21)

(cid:17)

2   1

4  ···} and (cid:107)A(cid:107)2 q :=

where q ∈ {1  1
. When q < 1  we suggest solving the
problem (21) using PALM coupled with iteratively reweighted minimization [24]. According to
the number of degrees of freedom of low-rank matrix  we suggest d = |Ω|/(m + n) in practical
applications.

5 Generalization error bound for LRMC

Above  we proposed a method to solve LRMC problems using a FGSR as a rank surrogate. Here  we
develop an upper bound on the error of the resulting estimator using a new generalization bound for
LRMC with a Schatten-p norm constraint. Similar bounds are available for LRMC using the nuclear
norm [30] and max norm [5].
Consider the following observation model. A matrix M is corrupted with iid N (0  2) noise E to
form Me = M + E. Suppose each entry of Me is observed independently with probability ρ and
the number of observed entries is |Ω|  where E|Ω| = ρmn.
Choose q ∈ {1  1
(cid:107)AB(cid:107)p
Sp

= Rp. Then use Theorem 2 to see that the following problem has the same solution 

2+q . For any γ > 0  consider a solution (A  B) to (21). Let

4  ···} and p = 2q

2   1

minimize
≤Rp rank(X)≤d
(cid:107)X(cid:107)p
Sp

(cid:107)PΩ(Me − X)(cid:107)2
F .

(22)

Therefore  we may solve (21) using the methods described above to ﬁnd a solution to (22) efﬁciently.
In this section  we provide generalization error bounds for the solution ˆM of (22).

5

(cid:16) 1
(cid:16)(cid:80)d
j=1 (cid:107)aj(cid:107)q(cid:17)1/q

F + γ

q

≤ Rp  ˆM is the optimal solution of (22)  and |Ω| ≥ 32

mn for some constant ς. Hence it is
mn for some constant 0. The following theorem provides a

5.1 Bound with optimal solution
√
Without loss of generality  we may assume (cid:107)M(cid:107)∞ ≤ ς/
√
reasonable to assume that  = 0/
generalization error bound for the solution of (22).
Theorem 3. Suppose (cid:107)M(cid:107)p
3 n log2 n.
Sp
Denote ζ := max{(cid:107)M(cid:107)∞ (cid:107) ˆM(cid:107)∞}. Then there exist numerical constants c1 and c2 such that the
following inequality holds with probability at least 1 − 5n−2
√
(4

(23)
When |Ω| is sufﬁciently large  we see that the second term in the brace of (23) is the dominant term 
which decreases as p decreases. A more complicated but more informative bound can be found in the
supplement (inequality (24)). In sum  Theorem 3 shows it is possible to reduce the matrix completion
error by using a smaller p in (22) or a smaller q in (21).

(cid:33)1−p/2 .

c1ζ 2 n log n

|Ω|

30 + c2ζ)2 n log n
|Ω|

(cid:107)M − ˆM(cid:107)2

F ≤ max

  (5.5 +

(cid:32)

10)Rp

√

5.2 Bound with arbitrary A and B

Since (21) and (22) are nonconvex problems  it is difﬁcult to guarantee that an optimization method
has found a globally optimal solution. The following theorem provides a generalization bound for
any feasible point ( ˆA  ˆB) of (21):
Theorem 4. Suppose Me = M + E. For any ˆA and ˆB  let ˆM = ˆA ˆB and d be the number of
nonzero columns of ˆA. Deﬁne ζ := max{(cid:107)M(cid:107)∞ (cid:107) ˆM(cid:107)∞}. Then there exists a numerical constant
C0  such that with probability at least 1 − 2 exp(−n)  the following inequality holds:

(cid:107)M − ˆM(cid:107)F

√

≤ (cid:107)PΩ(Me − ˆM )(cid:107)F

+

mn

(cid:107)E(cid:107)F√

mn

+ C0ζ

(cid:112)|Ω|

|Ω|

(cid:17)1/4

(cid:16) nd log n
(cid:1)1/4. We hope that a smaller

.

Theorem 4 indicates that if the training error (cid:107)PΩ(Me − ˆA ˆB)(cid:107)F and the number d of nonzero
columns of ˆA are small  the matrix completion error is small. In particular  if E = 0 and PΩ(Me −

ˆA ˆB) = 0  the matrix completion error is upper-bounded by C0ζ(cid:0) nd log n

q in (21) can lead to smaller training error and d such that the upper bound of matrix completion error
is smaller. Indeed  in our experiments  we ﬁnd that smaller q often leads to smaller matrix completion
error  but the improvement saturates quickly as q decreases. We ﬁnd q = 1 or 1
2 (corresponding to
5) are enough to provide high matrix completion accuracy and
a Schatten-p norm with p = 2
outperform max norm and nuclear norm.

3 or 2

|Ω|

6 Application to robust PCA

Suppose a fraction of entries in a matrix are corrupted in random locations. Formally  we observe

(24)
where M is a low-rank matrix and E is the sparse corruption matrix whose nonzero entries may be
arbitrary. The robust principal component analysis (RPCA) asks to recover M from Me; a by-now
classic approach uses nuclear norm minimization [13]. We propose to use FGSR instead  and solve

Me = M + E 

minimize

A B E

(cid:107)A(cid:107)q

2 q +

1
q

α
2

(cid:107)B(cid:107)2

F + λ(cid:107)E(cid:107)1 

subject to Me = AB + E 

(25)

where q ∈ {1  1

4  ···}. An optimization algorithm is detailed in the supplement.

2   1

7 Numerical results

7.1 Matrix completion

Baseline methods We compare the FGSR regularizers with the nuclear norm  truncated nuclear
norm [19]  weighted nuclear norm [20]  F-nuclear norm  max norm [31]  Riemannian pursuit [29] 

6

2  1

3  1

Schatten-p norm  Bi-nuclear norm [33]  and F2+nuclear norm [34]. We choose the parameters of
all methods to ensure they perform as well as possible. Details about the optimizations  parameters 
evaluation metrics are in the supplement. All experiments present the average of ten trials.
Noiseless synthetic data We generate random matrices of size 500 × 500 and rank 50. More
details about the experiment are in the supplement. In Figure 1(a)  the factored methods all use
factors of size d = 1.5r. We see the Schatten-p norm (p = 2
4 )  Bi-nuclear norm  F2+nuclear
norm  FGSR2/3  and FGSR1/2 have similar performances and outperform other methods when the
missing rate (proportion of unobserved entries) is high. In particular  the F-nuclear norm outperforms
the nuclear norm because the bound d on the rank is binding. In Figure 1(b) and (c)  in which the
missing rates are high  the max norm and F-nuclear norm are sensitive to the initial rank d  while the
F2+nuclear norm  Bi-nuclear norm  FGSR2/3  and FGSR1/2 always have nearly zero recovery error.
Interestingly  the max norm and F-nuclear norm are robust to the initial rank when the missing rate is
much lower than 0.6 in this experiment. In Figure 1(d)  we compare the computational time in the
case of missing rate= 0.7  in which  for fair comparison  the optimization algorithms of all methods
were stopped when the relative change of the recovered matrix was less than 10−5 or the number of
iterations reached 1000. The computational cost of nuclear norm  truncated nuclear norm  weighted
nuclear norm  and Schatten- 1
2 norm are especially large  as they require computing the SVD in every
iteration. The computational costs of max norm  F-nuclear norm  F2+nuclear norm  and Bi-nuclear
norm increase quickly as the initial rank d increases. In contrast  our FGSR2/3 and FGSR1/2 are very
efﬁcient even when the initial rank is large  because they are SVD-free and able to reduce the size
of the factors in the progress of optimization. While Riemannian pursuit is a bit faster than FGSR 
FGSR has lower error. Note that the Riemannian pursuit code mixes C and MATLAB  while all other
methods are written in pure MATLAB  explaining (part of) its more nimble performance.

Figure 1: Matrix completion on noiseless synthetic data (r = 50): (a) the effect of missing rate on
recovery error; (b)(c) the effect of rank initialization on recovery error (missing rate = 0.6 or 0.7);
(d) the effect of rank initialization on computational cost (missing rate = 0.7).

Noisy synthetic data We simulate a noisy matrix completion problem by adding Gaussian noise
to low-rank random matrices. We omit F2+nuclear norm and Bi-nuclear norm from these results
because they are less efﬁcient that FGSR2/3 and FGSR1/2 but perform similarly on recovery error.
The recovery errors for different missing rate are reported in Figure 2 (a) and (b) for SNR = 10 and
SNR = 5 (SNR:= (cid:107)M(cid:107)F /(cid:107)E(cid:107)F ) respectively. The max norm outperforms the nuclear norm when
the missing rate is low. The recovery errors of Schatten- 1
2 norm  FGSR2/3  and FGSR1/2 are much
lower than those of others. Figure 2(c) demonstrates that our FGSR2/3 and FGSR1/2 are robust to
the initial rank  while max norm and F-nuclear norm degrade as the initial rank increases. In Figure

7

0.500.550.600.650.700.750.800.85Missing rate00.20.40.60.8Relative recovery error(a)Nuclear normTruncated nuclear normWeighted nuclear normMax normF-Nuclear normRiemannian pursuitSchatten-2/3 normSchatten-1/2 normSchatten-1/4 normF2+Nuclear normBi-Nuclear normFGSR-2/3FGSR-1/21.0r1.5r2.0r2.5r3.0r3.5r4.0r4.5r5.0rd (initialization of rank)00.10.20.30.4Relative recovery error(b)1.0r1.5r2.0r2.5r3.0r3.5r4.0r4.5r5.0rd (initialization of rank)00.10.20.30.4Relative recovery error(c)1.0r1.5r2.0r2.5r3.0r3.5r4.0r4.5r5.0rd (initialization of rank)0102030405060Computational time (s)(d)2(d)  we see decreasing p from 1 to 2/9 reduces the recovery error signiﬁcantly  but the recovery
error stabilizes for smaller p. This result is consistent with Theorem 3.

Figure 2: Matrix completion on noisy synthetic data: (a)(b) recovery error when SNR = 10 or 5; (c)
the effect of rank initialization on recovery error (SNR = 10  missing rate = 0.5); (d) the effect of p
in Schatten-p norm (using FGSR when p < 1).

Figure 3: NMAE and RMSE on Movielens-1M data (Υ: known entries; Ω: sampled entries from Υ)

Real data We consider the MovieLens-1M dataset [40]  which consists of 1 million ratings (1 to
5) for 3900 movies by 6040 users. The movies rated by less than 5 users are deleted in this study
because the corresponding ratings may never be recovered when the matrix rank is higher than 5. We
randomly sample 70% or 50% of the known ratings of each user and perform matrix completion. The
normalized mean absolute error (NMAE) [3  8] and normalized root-mean-squared-error (RMSE) [8]
are reported in Figure 3  in which each value is the average of ten repeated trials and the standard
deviation is less than 0.0003. Although Riemannian pursuit can adaptively determine the rank  its
performance is not satisfactory. As the initial rank increases  the NMAE and RMSE of max norm

8

0.10.20.30.40.50.60.70.8Missing rate00.10.20.30.40.50.6Relative recovery error(a)0.10.20.30.40.50.60.70.8Missing rate00.10.20.30.40.50.6Relative recovery error(b)Nuclear normTruncated nuclear normWeighted nuclear normMax normF-Nuclear normRiemannian pursuitSchatten-1/2 normFGSR-2/3FGSR-1/21.0r1.5r2.0r2.5r3.0r3.5r4.0r4.5r5.0rd (initialization of rank)0.050.10.150.20.250.3Relative recovery error(c)Max normF-Nuclear normFGSR-2/3FGSR-1/212/32/52/92/172/332/652/1292/257p of Schatten-p norm (FGSR)0.050.10.150.20.250.3Relative recovery error(d)missing rate=0.5missing rate=0.75102030405060708090100d (initialization of rank)0.1650.170.1750.180.185NMAE||/||=0.75102030405060708090100d (initialization of rank)0.2250.230.2350.240.2450.25RMSE||/||=0.75102030405060708090100d (initialization of rank)0.1650.170.1750.180.185NMAE||/||=0.55102030405060708090100d (initialization of rank)0.2250.230.2350.240.2450.25RMSE||/||=0.5F-Nuclear normMax normRiemannian pursuitFGSR-2/3FGSR-1/2and F-nuclear norm increase. In contrast  FGSR2/3 and FGSR1/2 have consistent low NMAE and
RMSE. Moreover  FGSR1/2 outperforms FGSR2/3.

7.2 Robust PCA
We simulate a corrupted matrix as Me = M + E  where M is a random matrix of size 500 × 500
with rank 50 and E is a sparse matrix whose nonzero entries are N (0  2). Deﬁne the signal-noise-
ratio SNRc := σ/  where σ denotes the standard deviation of the entries of M. Figure 4(a) and (b)
show the recovery errors for different noise densities (proportion of nonzero entries of E). When the
noise density is high  FGSR2/3 and FGSR1/2 outperform nuclear norm and F-nuclear norm. Figure
4(c) and (d) shows again that unlide the F-nuclear norm  FGSR2/3 and FGSR1/2 are not sensitive
to the initial rank  and that FGSR1/2 outperforms FGSR2/3 slightly when the noise density is high.
More results  including for image denoising  appear in the supplement.

Figure 4: RPCA on synthetic data: (a)(b) recovery error when SNRc = 1 or 0.2; (c)(d) the effect of
rank initialization on recovery error (SNRc = 1  noise density = 0.3 or 0.5).

8 Conclusion

This paper proposed a class of nonconvex surrogates for matrix rank that we call Factor Group-Sparse
Regularizers (FGSRs). These FGSRs give a factored formulation for certain Schatten-p norms with
arbitrarily small p. These FGSRs are tighter surrogates for the rank than the nuclear norm  can be
optimized without the SVD  and perform well in denoising and completion tasks regardless of the
initial choice of rank. In addition  we provide generalization error bounds for LRMC using the FGSR
(or  more generally  any Schatten-p norm) as a regularizer. Our experimental results demonstrate the
proposed methods2 achieve state-of-the-art performances in LRMC and RPCA.
These experiments provide compelling evidence that PALM and ADMM may often (perhaps always)
converge to the global optimum of these problems. A full convergence theory is an interesting
problem for future work. A proof of global convergence would reveal the required sample complexity
for LRMC and RPCA with FGSR as a computationally tractable rank proxy.

Acknowledgements

The authors gratefully acknowledge support from DARPA Award FA8750-17-2-0101 and NSF
CCF-1740822.

2The MATLAB codes of the proposed methods are available at https://github.com/udellgroup/Codes-of-

FGSR-for-effecient-low-rank-matrix-recovery

9

0.10.150.20.250.30.350.40.450.50.550.6Noise density00.050.10.150.20.250.3Relative recovery error(a)0.10.150.20.250.30.350.40.450.50.550.6Noise density00.10.20.30.40.50.6Relative recovery error(b)Nuclear normF-Nuclear normFGSR-2/3FGSR-1/21.0r1.5r2.0r2.5r3.0r3.5r4.0r4.5r5.0rd (initialization of rank)00.020.040.060.080.1Relative recovery error(c)1.0r1.5r2.0r2.5r3.0r3.5r4.0r4.5r5.0rd (initialization of rank)00.050.10.150.20.250.3Relative recovery error(d)F-Nuclear normFGSR-2/3FGSR-1/2References
[1] Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal

on Mathematics of Data Science  1(1):144–160  2019.

[2] Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics  9(6):717–772  2009.

[3] Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm

regularized linear least squares problems. Paciﬁc Journal of optimization  6(615-640):15  2010.

[4] Benjamin Recht. A simpler approach to matrix completion. J. Mach. Learn. Res.  12:3413–3430  December

2011.

[5] Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruction. In

Proceedings of the 24th Annual Conference on Learning Theory  pages 315–340  2011.

[6] Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th Annual

Symposium on Foundations of Computer Science (FOCS)   pages 651–660. IEEE  2014.

[7] Yudong Chen  Srinadh Bhojanapalli  Sujay Sanghavi  and Rachel Ward. Coherent matrix completion. In
Proceedings of the 31st International Conference on Machine Learning (ICML-14)  pages 674–682. JMLR
Workshop and Conference Proceedings  2014.

[8] Ohad Shamir and Shai Shalev-Shwartz. Matrix completion with the trace norm: learning  bounding  and

transducing. The Journal of Machine Learning Research  15(1):3401–3423  2014.

[9] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.

Transactions on Information Theory  62(11):6535–6579  2016.

IEEE

[10] Jicong Fan and Madeleine Udell. Online high rank matrix completion. In The IEEE Conference on

Computer Vision and Pattern Recognition (CVPR)  June 2019.

[11] Andrew Goldberg  Ben Recht  Junming Xu  Robert Nowak  and Xiaojin Zhu. Transduction with matrix
completion: Three birds with one stone. In Advances in Neural Information Processing Systems 23  pages
757–765. Curran Associates  Inc.  2010.

[12] Madeleine Udell  Corinne Horn  Reza Zadeh  Stephen Boyd  et al. Generalized low rank models. Founda-

tions and Trends R(cid:13) in Machine Learning  9(1):1–118  2016.

[13] Emmanuel J. Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis? J.

ACM  58(3):1–37  2011.

[14] Jiashi Feng  Huan Xu  and Shuicheng Yan. Online robust PCA via stochastic optimization. In Advances in

Neural Information Processing Systems  pages 404–412  2013.

[15] Qian Zhao  Deyu Meng  Zongben Xu  Wangmeng Zuo  and Lei Zhang. Robust principal component

analysis with complex noise. In International Conference on Machine Learning  pages 55–63  2014.

[16] Daniel Pimentel-Alarcón and Robert Nowak. Random consensus robust PCA. In Artiﬁcial Intelligence

and Statistics  pages 344–352  2017.

[17] J. Fan and T. W. S. Chow. Exactly robust kernel principal component analysis. IEEE Transactions on

Neural Networks and Learning Systems  pages 1–13  2019.

[18] T. Bouwmans  S. Javed  H. Zhang  Z. Lin  and R. Otazo. On the applications of robust PCA in image and

video processing. Proceedings of the IEEE  106(8):1427–1457  Aug 2018.

[19] Yao Hu  Debing Zhang  Jieping Ye  Xuelong Li  and Xiaofei He. Fast and accurate matrix completion via
truncated nuclear norm regularization. IEEE Transactions on Pattern Analysis and Machine Intelligence 
35(9):2117–2130  2013.

[20] Shuhang Gu  Lei Zhang  Wangmeng Zuo  and Xiangchu Feng. Weighted nuclear norm minimization with
application to image denoising. In Proceedings of the IEEE conference on computer vision and pattern
recognition  pages 2862–2869  2014.

[21] Feiping Nie  Heng Huang  and Chris Ding. Low-rank matrix recovery via efﬁcient Schatten p-norm
minimization. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence  AAAI’12 
pages 655–661. AAAI Press  2012.

10

[22] Lu Liu  Wei Huang  and Di-Rong Chen. Exact minimum rank approximation via Schatten p-norm

minimization. Journal of Computational and Applied Mathematics  267:218 – 227  2014.

[23] Greg Ongie  Rebecca Willett  Robert D. Nowak  and Laura Balzano. Algebraic variety models for high-rank
matrix completion. In Proceedings of the 34th International Conference on Machine Learning  pages
2691–2700  Sydney  Australia  06–11 Aug 2017. PMLR.

[24] Karthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization. Journal

of Machine Learning Research  13(Nov):3441–3473  2012.

[25] Nathan Srebro  Jason Rennie  and Tommi S. Jaakkola. Maximum-margin matrix factorization. In Advances

in neural information processing systems  pages 1329–1336  2005.

[26] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd international conference on Machine learning  pages 713–719.
ACM  2005.

[27] Nathan Srebro and Ruslan R Salakhutdinov. Collaborative ﬁltering in a non-uniform world: Learning with
the weighted trace norm. In J. D. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R. S. Zemel  and A. Culotta 
editors  Advances in Neural Information Processing Systems 23  pages 2056–2064. Curran Associates 
Inc.  2010.

[28] Zaiwen Wen  Wotao Yin  and Yin Zhang. Solving a low-rank factorization model for matrix completion by
a nonlinear successive over-relaxation algorithm. Mathematical Programming Computation  4(4):333–361 
2012.

[29] Mingkui Tan  Ivor W Tsang  Li Wang  Bart Vandereycken  and Sinno Jialin Pan. Riemannian pursuit for

big matrix recovery. In International Conference on Machine Learning  pages 1539–1547  2014.

[30] Nathan Srebro and Adi Shraibman. Rank  trace-norm and max-norm. In International Conference on

Computational Learning Theory  pages 545–560. Springer  2005.

[31] Jason D. Lee  Ben Recht  Nathan Srebro  Joel Tropp  and Ruslan R. Salakhutdinov. Practical large-scale
optimization for max-norm regularization. In Advances in Neural Information Processing Systems  pages
1297–1305  2010.

[32] Suriya Gunasekar  Blake E Woodworth  Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems 
pages 6151–6159  2017.

[33] Fanhua Shang  Yuanyuan Liu  and James Cheng. Tractable and scalable Schatten quasi-norm approxima-

tions for rank minimization. In Artiﬁcial Intelligence and Statistics  pages 620–629  2016.

[34] Fanhua Shang  James Cheng  Yuanyuan Liu  Zhi-Quan Luo  and Zhouchen Lin. Bilinear factor matrix
norm minimization for robust pca: Algorithms and applications. IEEE transactions on pattern analysis
and machine intelligence  40(9):2066–2080  2017.

[35] Yu Wang  Wotao Yin  and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth optimiza-

tion. Journal of Scientiﬁc Computing  pages 1–35  2015.

[36] Qinghua Liu  Xinyue Shen  and Yuantao Gu. Linearized admm for non-convex non-smooth optimization

with convergence analysis. arXiv preprint arXiv:1705.02502  2017.

[37] Wenbo Gao  Donald Goldfarb  and Frank E Curtis. Admm for multiafﬁne constrained optimization. arXiv

preprint arXiv:1802.09592  2018.

[38] Jérôme Bolte  Shoham Sabach  and Marc Teboulle. Proximal alternating linearized minimization for

nonconvex and nonsmooth problems. Mathematical Programming  146(1-2):459–494  2014.

[39] J. Fan  M. Zhao  and T. W. S. Chow. Matrix completion via sparse factorization solved by accelerated

proximal alternating linearized minimization. IEEE Transactions on Big Data  pages 1–1  2018.

[40] MovieLens dataset. https://grouplens.org/datasets/movielens/.

11

,Jicong Fan
Lijun Ding
Yudong Chen
Madeleine Udell