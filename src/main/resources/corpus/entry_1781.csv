2019,Efficient online learning with kernels for adversarial large scale problems,We are interested in a framework of online learning with kernels for low-dimensional  but large-scale and potentially adversarial datasets. 
We study the computational and theoretical performance of online variations of kernel Ridge regression. Despite its simplicity  the algorithm we study is the first  to achieve the optimal regret for a wide range of kernels with a per-round complexity of order $n^\alpha$ with $\alpha < 2$. 

The algorithm we consider is based on approximating the kernel with the linear span of basis functions. Our contributions are twofold: 1) For the Gaussian kernel  we propose to build the basis beforehand (independently of the data) through Taylor expansion. For $d$-dimensional inputs  we provide a (close to) optimal regret of order $O((\log n)^{d+1})$ with per-round time complexity and space complexity $O((\log n)^{2d})$. This makes the algorithm a suitable choice as soon as $n \gg e^d$ which is likely to happen in a scenario with small dimensional and large-scale dataset; 2) For general kernels with low effective dimension  the basis functions are updated sequentially  adapting to the data  by sampling Nyström points. In this case  our algorithm improves the computational trade-off known for online kernel regression.,Efﬁcient online learning with Kernels for adversarial

large scale problems

Rémi Jézéquel

Pierre Gaillard

Alessandro Rudi

INRIA - Département d’Informatique de l’École Normale Supérieure

PSL Research University  Paris  France

{remi.jezequel pierre.gaillard alessandro.rudi}@inria.fr

Abstract

We are interested in a framework of online learning with kernels for low-
dimensional  but large-scale and potentially adversarial datasets. We study the
computational and theoretical performance of online variations of kernel Ridge
regression. Despite its simplicity  the algorithm we study is the ﬁrst to achieve the
optimal regret for a wide range of kernels with a per-round complexity of order nα
with α < 2.
The algorithm we consider is based on approximating the kernel with the linear
span of basis functions. Our contributions are twofold: 1) For the Gaussian kernel 
we propose to build the basis beforehand (independently of the data) through
Taylor expansion. For d-dimensional inputs  we provide a (close to) optimal regret
of order O((log n)d+1) with per-round time complexity and space complexity
O((log n)2d). This makes the algorithm a suitable choice as soon as n (cid:29) ed which
is likely to happen in a scenario with small dimensional and large-scale dataset; 2)
For general kernels with low effective dimension  the basis functions are updated
sequentially  adapting to the data  by sampling Nyström points. In this case  our
algorithm improves the computational trade-off known for online kernel regression.

1

Introduction

Nowadays the volume and the velocity of data ﬂows are deeply increasing. Consequently  many
applications need to switch from batch to online procedures that can treat and adapt to data on
the ﬂy. Furthermore to take advantage of very large datasets  non-parametric methods are gaining
increasing momentum in practice. Yet the latter often suffer from slow rates of convergence and bad
computational complexities. At the same time  data is getting more complicated and simple stochastic
assumptions  such as i.i.d. data  are often not satisﬁed. In this paper  we try to combine these different
aspects due to large scale and arbitrary data. We build a non-parametric online procedure based on
kernels  which is efﬁcient for large data sets and achieves close to optimal theoretical guarantees.
Online learning is a subﬁeld of machine learning where a learner sequentially interacts with an
environment and tries to learn and adapt on the ﬂy to the observed data as one goes along. We
consider the following sequential setting. At each iteration t ≥ 1  the learner receives some input

xt ∈ X ; makes a prediction(cid:98)yt ∈ R and the environment reveals the output yt ∈ R. The inputs xt

and the outputs yt are sequentially chosen by the environment and can be arbitrary. Learner’s goal is
to minimize his cumulative regret

n(cid:88)
(yt −(cid:98)yt)2 − n(cid:88)

(cid:0)yt − f (xt)(cid:1)2

(1)
uniformly over all functions f in a space of functions H. We will consider Reproducing Kernel
Hilbert Space (RKHS) H  [see next section or Aro50  for more details]. It is worth noting here that

Rn(f ) :=

t=1

t=1

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

all the properties of a RKHS are controlled by the associated kernel function k : X ×X → R  usually
known in closed form  and that many function spaces of interest are (or are contained in) RKHS 
e.g. when X ⊆ Rd: polynomials of arbitrary degree  band-limited functions  analytic functions with
given decay at inﬁnity  Sobolev spaces and many others [BTA11].

Previous work Kernel regression in a statistical setting has been widely studied by the statistics
community. Our setting of online kernel regression with adversarial data is more recent. Most of the
existing work focuses on the linear setting (i.e.  linear kernel). First work on online linear regression
dates back to [Fos91]. [BKM+15] provided the minimax rates (together with an algorithm) and we
refer the reader to references therein for a recent overview of the literature in the linear case. We only
recall relevant work for this paper. [AW01  Vov01] designed the nonlinear Ridge forecaster (denoted
AWV). In linear regression (linear kernel)  it achieves the optimal regret of order O(d log n) uniformly
over all (cid:96)2-bounded vectors. The latter can be extended to kernels (see Deﬁnition (3)) which we refer
to as Kernel-AWV. With regularization parameter λ > 0  it obtains a regret upper-bounded for all
f ∈ H as

Rn(f ) (cid:46) λ(cid:13)(cid:13)f(cid:13)(cid:13)2

is the effective dimension  where Knn :=(cid:0)k(xi  xj)(cid:1)

+ B2deff(λ)   where deff(λ) := Tr(Knn

(cid:0)Knn + λIn)−1(cid:1)

√

(2)
1≤i j≤n ∈ Rn×n denotes the kernel matrix at
time n. The above upper-bound on the regret is essentially optimal (see remark 2.1). Yet the per
round complexity and the space complexity of Kernel-AWV are O(n2). In this paper  we aim at
reducing this complexity while keeping optimal regret guarantees.
Though the literature on online contextual learning is vast  little considers non-parametric function
classes. Related work includes [Vov06] that considers the Exponentially Weighted Average forecaster
or [HM07] which considers bounded Lipschitz function set and Lipschitz loss functions  while here
we focus on the square loss. Minimax rates for general function sets H are provided by [RST13].
RKHS spaces were ﬁrst considered in [Vov05] though they only obtain O(
n) rates which are
suboptimal for our problem. More recently  a regret bound of the form (2) was proved by [ZK10] for
a clipped version of kernel Ridge regression and by [CLV17b] for a clipped version of Kernel Online
Newton Step (KONS) for general exp-concave loss functions.
The computational complexity (O(n2) per round) of these algorithms is however prohibitive for large
datasets. [CLV17b] and [CLV17a] provide approximations of KONS to get manageable complexities.
However  these come with deteriorated regret guarantees. [CLV17b] improves the time and space
complexities by a factor γ ∈ (0  1) enlarging the regret upper-bound by 1/γ. [CLV17a] designs an
efﬁcient approximation of KONS based on Nyström approximation [SSL00  WS01] and restarts with

per-round complexities O(cid:0)m2) where m is the number of Nyström points. Yet their regret bound

suffers an additional multiplicative factor m with respect to (2) because of the restarts. Furthermore 
contrary to our results  the regret bounds of [CLV17b] and [CLV17a] are not with respect to all
functions in H but only with functions f ∈ H such that f (xt) ≤ C for all t ≥ 1 where C is a
parameter of their algorithm. Since C comes has a multiplicative factor of their bounds  their results
are sensitive to outliers that may lead to large C. Other relevant approximation schemes of Online
Kernel Learning have been done by [LHW+16] and [ZL19]. The authors consider online gradient
descent algorithms which they approximate using different approximation schemes (as Nyström and
√
random features). However since they use general Lipschitz loss functions and consider (cid:96)1-bounded
dual norm of functions f  their regret bounds of order O(
n) are hardly comparable to ours and
seem suboptimal in n in our restrictive setting with square loss and kernels with small effective
dimension (such as Gaussian kernel).

Contributions and outline of the paper The main contribution of the paper is to analyse a variant
of Kernel-AWV that we call PKAWV (see Deﬁnition (4)). Despite its simplicity  it is to our knowledge
the ﬁrst algorithm for kernel online regression that recovers the optimal regret (see bound (2)) with
an improved space and time complexity of order (cid:28) n2 per round. Table 1 summarizes the regret
rates and complexities obtained by our algorithm and the ones of [CLV17b  CLV17a].
Our procedure consists simply in applying Kernel-AWV while  at time t ≥ 1  approximating the
RKHS H with a linear subspace ˜Ht of smaller dimension. In Theorem 3  PKAWV suffers an
additional approximation term with respect to the optimal bound of Kernel-AWV which can be
made small enough by properly choosing ˜Ht. To achieve the optimal regret with low computational

2

Algorithm
PKAWV
Sketched-KONS [CLV17b] (c > 0)
Pros-N-KONS [CLV17a]

Regret

(log n)d+1
c(log n)d+1
(log n)2d+1

Kernel

Gaussian

(cid:1)d
deff(λ) ≤(cid:0) log n
(cid:1)γ
deff(λ) ≤(cid:0) n

General

λ

√

λ

2 − 1

γ <

Per-round complexity

(log n)2d

(log n)2d

(cid:0)n/c(cid:1)2
(cid:0)n/c(cid:1)2

1−γ2

n

4γ

4γ(1−γ)
(1+γ)2

PKAWV
Sketched-KONS [CLV17b] (c > 0)
Pros-N-KONS [CLV17a]

γ

n
cn

γ+1 log n
γ+1 log n

γ

4γ

(1+γ)2 log n

n

n

Table 1: Order in n of the best possible regret rates achievable by the algorithms and corresponding
per-round time-complexity. Up to log n  the rates obtained by PKAWV are optimal.

complexity  ˜Ht needs to approximate H well and to be low dimensional with an easy-to-compute
projection. We provide two relevant constructions for ˜Ht.
In section 3.1  we focus on the Gaussian kernel that we approximate by a ﬁnite set of basis functions.
The functions are deterministic and chosen beforehand by the learner independently of the data. The
number of functions included in the basis is a parameter to be optimized and ﬁxes an approximation-
computational trade-off. Theorem 4 shows that PKAWV satisﬁes (up to log) the optimal regret

bounds (2) while enjoying a per-round space and time complexity of O(cid:0) log2d(cid:0) n
(cid:1)(cid:1). For the
Gaussian kernel  this corresponds to O(cid:0)deff(λ)2(cid:1) which is known to be optimal even in the statistical

regret with a computational complexity of O(cid:0)deff(λ)4/(1−γ)(cid:1). The latter is o(n2) (for well-tuned λ)

setting with i.i.d. data.
In section 3.2  we consider data adaptive approximation spaces ˜Ht based on Nyström approximation.
At time t ≥ 1  we approximate any kernel H by sampling a subset of the input vectors {x1  . . .   xt}.
If the kernel satisﬁes the capacity condition deff(λ) ≤ (n/λ)γ for γ ∈ (0  1)  the optimal regret is then
of order deff(λ) = O(nγ/(1+γ)) for well-tuned parameter λ. Our method then recovers the optimal
2 − 1. Furthermore  if the sequence of input vectors xt is given beforehand to the
as soon as γ <
algorithm  the per-round complexity needed to reach the optimal regret is improved to O(deff(λ)4)
and our algorithm can achieve it for all γ ∈ (0  1).
Finally  we perform in Section 4 several experiments based on real and simulated data to compare the
performance (in regret and in time) of our methods with competitors.

√

λ

Notations We recall here basic notations that we will use throughout the paper. Given a vector
v ∈ Rd  we write v = (v(1)  . . .   v(d)). We denote by N0 = N ∪ {0} the set of non-negative integers
and for p ∈ Nd
0  |p| = p(1) + ··· + p(d). By a sligh abuse of notation  we denote by (cid:107) · (cid:107) both
the Euclidean norm and the norm for the Hilbert space H. Write v(cid:62)w  the dot product between
v  w ∈ RD. The conjugate transpose for linear operator Z on H will be denoted Z∗. The notation (cid:46)
will refer to approximate inequalities up to logarithmic multiplicative factors. Finally  we will denote
a ∨ b = max(a  b) and a ∧ b = min(a  b)  for a  b ∈ R.

2 Background
Kernels. Let k : X × X → R be a positive deﬁnite kernel [Aro50] that we assume to be bounded
(i.e.  supx∈X k(x  x) ≤ κ2 for some κ > 0). The function k is characterized by the existence of a
feature map φ : X → RD  with D ∈ N ∪ {∞}1 such that k(x  x(cid:48)) = φ(x)(cid:62)φ(x(cid:48)). Moreover the
reproducing kernel Hilbert space (RKHS) associated to k is characterized by H = {f | f (x) =
w(cid:62)φ(x)  w ∈ RD  x ∈ X}  with inner product (cid:104)f  g(cid:105)H := v(cid:62)w  for f  g ∈ H deﬁned by f (x) =
v(cid:62)φ(x)  g(x) = w(cid:62)φ(x) and v  w ∈ RD. For more details and different characterizations of k H 
see [Aro50  BTA11]. It’s worth noting that the knowledge of φ is not necessary when working
i=1 αiφ(xi)  with αi ∈ R  xi ∈ X and ﬁnite p ∈ N  indeed
i=1 αik(xi  x)  and moreover (cid:107)f(cid:107)2H = α(cid:62)Kppα  with Kpp the

with functions of the form f = (cid:80)p
i=1 αiφ(xi)(cid:62)φ(x) =(cid:80)p
f (x) =(cid:80)p

kernel matrix associated to the set of points x1  . . .   xp.

1when D = ∞ we consider RD as the space of squared summable sequences.

3

Kernel-AWV. The Azoury-Warmuth-Vovk forecaster (denoted AWV) on the space of linear func-
tions on X = Rd has been introduced and analyzed in [AW01  Vov01]. We consider here a
straightforward generalization to kernels (denoted Kernel-AWV) of the nonlinear Ridge forecaster
(AWV) introduced by [AW01  Vov01] on the space of linear functions on X = Rd. At iteration t ≥ 1 

Kernel-AWV predicts(cid:98)yt = (cid:98)ft(xt)  where
(cid:40)t−1(cid:88)

(cid:98)ft ∈ argmin

f∈H

s=1

(cid:0)ys − f (xs)(cid:1)2

+ λ(cid:13)(cid:13)f(cid:13)(cid:13)2

(cid:41)

+ f (xt)2

.

(3)

A variant of this algorithm  more used in the context of data independently sampled from distribution 
is known as kernel Ridge regression. It corresponds to solving the problem above  without the last
penalization term f (xt)2.
Optimal regret for Kernel-AWV. In the next proposition  we state a preliminary result which proves
that Kernel-AWV achieves a regret depending on the eigenvalues of the kernel matrix.
Proposition 1. Let λ  B > 0. For any RKHS H  for all n ≥ 1  for all inputs x1  . . .   xn ∈ X and all
y1  . . .   yn ∈ [−B  B]  the regret of Kernel-AWV is upper-bounded for all f ∈ H as

Rn(f ) ≤ λ(cid:13)(cid:13)f(cid:13)(cid:13)2

n(cid:88)

(cid:18)

+ B2

log

1 +

λk(Knn)

λ

 

(cid:19)

where λk(Knn) denotes the k-th largest eigenvalue of Knn.

k=1

The proof is a direct consequence of the known regret bound of AWV in the ﬁnite dimensional linear
regression setting—see Theorem 11.8 of [CBL06] or Theorem 2 of [GGHS18]. For completeness 
we reproduce the analysis for inﬁnite dimensional space (RKHS) in Appendix C.1. In online linear
regression in dimension d  the above result implies the optimal rate of convergence dB2 log(n)+O(1)
(see [GGHS18] and [Vov01]). As shown by the following proposition  Proposition 1 yields optimal
regret (up to log) of the form (2) for online kernel regression.
Proposition 2. For all n ≥ 1  λ > 0 and all input sequences x1  . . .   xn ∈ X  

(cid:18)

n(cid:88)

k=1

(cid:19)

(cid:16)

(cid:17)

(cid:0)λ(cid:1) .

log

1 +

λk(Kn)

λ

≤ log

e +

enκ2

λ

deff

Combined with Proposition 1  this entails that Kernel-AWV satisﬁes (up to the logarithmic factor)
the optimal regret bound (2). As discussed in the introduction  such an upper-bound on the regret is
not new and was already proved by [ZK10] or by [CLV17b] for other algorithms. An advantage of
Kernel-AWV is that it does not require any clipping and thus the beforehand knowledge of B > 0 to
obtained Proposition 1. Furthermore  we slightly improve the constants in the above proposition.
Remark 2.1 (Optimal regret under the capacity condition). Assuming the capacity condition (deff(λ) ≤
(n/λ)γ for 0 ≤ γ ≤ 1)  the rate of the regret bound (2) can be made explicit. As we show now 
this matches existing minimax lower rates in the stochastic setting. Under the capacity condition 
optimizing λ (cid:39) nγ/(1+γ) to minimize the r.h.s. of (2)  the regret bound is then of order Rn(f ) ≤
O(nγ/(1+γ)) (up to logs). If the data (x1  y1)  . . .   (xn  yn) is i.i.d. according to some distribution
ρ over X × R  we can apply a standard online to batch conversion (see [CBCG04]). The estimator
¯fn = 1
n

(cid:80)n
t=1 ft satisﬁes for any f ∈ H the upper-bound on its excess risk

n

≤ O(n

− 1

E( ¯fn) − E(f ) ≤ E

(cid:2)(f (X) − Y )2(cid:3). This corresponds to the known minimax lower rate in this

where E(f ) := E(X Y )∼ρ
stochastic setting as shown by Theorem 2 (applied with c = 1 and b = 1/γ) of [CDV07].
It is worth pointing out that in the worst case deff(λ) ≤ κ2n/λ for any bounded kernel. In particular 
√
optimizing the bound yields λ = O(
n log n). In the
special case of the Gaussian kernel (which we consider in Section 3.1)  the latter can be improved

to deff(λ) (cid:46)(cid:0) log(n/λ)(cid:1)d (see [ABRW18]) which entails Rn(f ) ≤ O(cid:0)(log n)d+1(cid:1) for well tuned

n log n) and a regret bound of order O(

1+γ )  

√

(cid:20) Rn(f )

(cid:21)

value of λ.

4

3 Online Kernel Regression with projections

In the previous section we have seen that Kernel-AWV achieves optimal regret. Yet  it has computa-
tional requirements that are O(n3) in time and O(n2) in space  for n steps of the algorithm  making it
unfeasible in the context of large scale datasets  i.e. n (cid:29) 105. In this paper  we consider and analyze
a simple variation of Kernel-AWV denoted PKAWV. At time t ≥ 1  for a regularization parameter

λ > 0 and a linear subspace ˜Ht of H the algorithm predicts(cid:98)yt = (cid:98)ft(xt)  where
(cid:41)

(cid:0)ys − f (xs)(cid:1)2

+ λ(cid:13)(cid:13)f(cid:13)(cid:13)2

+ f (xt)2

.

(4)

(cid:98)ft = argmin

f∈ ˜Ht

(cid:40)t−1(cid:88)

s=1

In the next subsections  we explicit relevant approximations ˜Ht (typically the span of a small number
of basis functions) of H that trade-off good approximation with a low computational cost. Appendix H
details how (4) can be efﬁciently implemented in these cases.
The result below bounds the regret of the PKAWV for any function f ∈ H and holds for any bounded
kernel and any explicit subspace ˜H associated with projection P . The cost of the approximation of

H by ˜H is measured by the important quantity µ :=(cid:13)(cid:13)(I − P )C 1/2
(cid:19)

operator.
Theorem 3. Let ˜H be a linear subspace of H and P the Euclidean projection onto ˜H. When PKAWV
is run with λ > 0 and ﬁxed subspaces ˜Ht = ˜H  then for all f ∈ H

(cid:13)(cid:13)2  where Cn is the covariance

(cid:18)

n

λj(Knn)

nµB2

+ B2

for any sequence (x1  y1)  . . .   (xn  yn) ∈ X × [−B  B] where µ :=(cid:13)(cid:13)(I − P )C 1/2
(cid:80)n
t=1 φ(xt) ⊗ φ(xt).

λ2

j=1

+ (µ + λ)

1 +

log

λ

n

 

(5)

(cid:13)(cid:13)2 and Cn :=

Rn(f ) ≤ λ(cid:13)(cid:13)f(cid:13)(cid:13)2

n(cid:88)

The proof of Thm. 3 is deferred to Appendix D.1 and is the consequence of a more general Thm. 9.

3.1 Learning with Taylor expansions and Gaussian kernel for very large data set

In this section we focus on non-parametric regression with the widely used Gaussian kernel deﬁned
by k(x  x(cid:48)) = exp(−(cid:107)x − x(cid:48)(cid:107)2/(2σ2)) for x  x(cid:48) ∈ X and σ > 0 and the associated RKHS H.
Using the results of the previous section with a ﬁxed linear subspace ˜H which is the span of a
basis of O(polylog(n/λ)) functions  we prove that PKAWV achieves optimal regret. This leads to a
computational complexity that is only O(n polylog(n/λ)) for optimal regret. We need a basis that
(1) approximates very well the Gaussian kernel and at the same time (2) whose projection is easy to
compute. We consider the following basis of functions  for k ∈ Nd
0 
xt
√

ψki(x(i))  where ψt(x) =

d(cid:89)

gk(x) =

− x2

2σ2 .

(6)

e

i=1

σt

t!

For one dimensional data  this corresponds to Taylor expansion of the Gaussian kernel. Our theorem
below states that PKAWV (see (4)) using for all iterations t ≥ 1

˜Ht = Span(GM )

0}
with GM = {gk | |k| ≤ M  k ∈ Nd

where |k| := k1 + ··· + kd  for k ∈ Nd
0  gets optimal regret while enjoying low complexity. The
size of the basis M controls the trade-off between approximating well the Gaussian kernel (to incur
low regret) and large computational cost. Theorem 4 optimizes M so that the approximation term of
Theorem 3 (due to kernel approximation) is of the same order than the optimal regret.
Theorem 4. Let λ > 0  n ∈ N and let R  B > 0. Assume that (cid:107)xt(cid:107) ≤ R and |yt| ≤ B. When

(cid:7)  then running PKAWV using GM as set of functions achieves a regret

M =(cid:6) 8R2
n(cid:88)
Moreover  its per iteration computational cost is O(cid:0)(cid:0)3 + 1

∀f ∈ H  Rn(f ) ≤ λ(cid:13)(cid:13)f(cid:13)(cid:13)2

(cid:1)2d(cid:1) in space and time.

σ2 ∨ 2 log n
λ∧1

bounded by

λj(Knn)

(cid:18)

(cid:19)

3B2

1 +

log

j=1

+

λ

2

.

d log n
λ∧1

5

(cid:16)

(cid:17)d

n
λ

(cid:16)(cid:16)

(cid:17)d(cid:17)

n
λ

Therefore PKAWV achieves a regret-bound only deteriorated by a multiplicative factor of 3/2 with
respect to the bound obtained by Kernel-AWV (see Prop. 1). From Prop. 2 this also yields (up to log)
the optimal bound (2).
In particular  it is known [ABRW18] for the Gaussian kernel that

deff(λ) ≤ 3

41
d

R2
2σ2 +

3
d

log

log

6 +

= O

have |GM| (cid:46) deff(λ). The per-round space and time complexities are thus O(cid:0)deff(λ)2(cid:1). Though our

The upper-bound is matching even in the i.i.d. setting for nontrivial distributions. In this case  we

method is quite simple (since it uses ﬁxed explicit embedding) it is able to recover results -in terms
of computational time and bounds in the adversarial setting- that are similar to results obtained in the
more restrictive i.i.d. setting obtained via much more sophisticated methods  like learning with (1)
Nyström with importance sampling via leverage scores [RCR15]  (2) reweighted random features
[Bac17  RR17]  (3) volume sampling [DWH18]. By choosing λ = (B/(cid:107)f(cid:107))2  to minimize the r.h.s.
of the regret bound of the theorem  we get

.

Rn(f ) (cid:46)(cid:16)

n(cid:107)f(cid:107)2H
B2

log

(cid:17)d+1

B2 + B2.

(7)

Note that the optimal λ does not depend on n and can be optimized in practice through standard
online calibration methods. For instance  one can run in parallel subroutines of the algorithm  each
d+1   . . .   0}. The subroutines can
using a different value of λ in the ﬁnite grid Λ := {n2k  k = −n
then be sequentially combined with an expert advice algorithm such as the Exponentially Weighted
Average forecaster [CBL06] at an additional negligible cost of order O(B2 log |Λ|) in the regret
(using the fact that the squared loss is exp-concave on [0  B]). Similarly  though we use a ﬁxed
number of features M in the experiments  the latter could be increased slowly over time thanks to
online calibration techniques.

1

3.2 Nyström projection

(cid:111)

(cid:110)

The previous two subsections considered a deterministic function basis (independent of the data) to
approximate speciﬁc RKHS. Here  we analyse Nyström projections [RCR15] that are data dependent
and work for any RKHS. It consists in sequentially updating a dictionary It ⊂ {x1  . . .   xt} and
using

˜Ht = Span

φ(x)  x ∈ It

.

(8)
If the points included into It are well-chosen  the latter may approximate well the solution of (3)
which belongs to the linear span of {φ(x1)  . . .   φ(xt)}. The inputs xt might be included in the
dictionary independently and uniformly at random. Here  we build the dictionary by following the
KORS algorithm of [CLV17a] which is based on approximate leverage scores. At time t ≥ 1  it
evaluates the importance of including xt to obtain an accurate projection Pt by computing its leverage
score. Then  it decides to add it or not  by drawing a Bernoulli random variable. The points are
never dropped from the dictionary so that I1 ⊂ I2 ⊂ ···In. With their notations  choosing ε = 1/2
and remarking that (cid:107)ΦT
(cid:107)2  their Proposition 1 can be rewritten as
follows.
Proposition 5. [CLV17a  Prop. 1] Let δ > 0  n ≥ 1  µ > 0. Then  the sequence of dictionaries
I1 ⊂ I2 ⊂ ··· ⊂ In learned by KORS with parameters µ and β = 12 log(n/δ) satisﬁes w.p. 1 − δ 

t (I − Pt)Φt(cid:107) = (cid:107)(I − Pt)C 1/2

Furthermore  the algorithm runs in O(cid:0)deff(µ)2 log4(n)(cid:1) space and O(cid:0)deff(µ)2(cid:1) time per iteration.

|It| ≤ 9deff(µ) log(cid:0)2n/δ(cid:1)2

(cid:13)(cid:13)(I − Pt)C 1/2

(cid:13)(cid:13)2 ≤ µ

∀t ≥ 1 

and

.

t

t

Using this approximation result together with Thm. 9 (which is a more general version of Thm. 3) 
we can bound the regret of PKAWV with KORS. The proof is postponed to Appendix E.1.
Theorem 6. Let n ≥ 1  δ > 0 and λ ≥ µ > 0. Assume that the dictionaries (It)t≥1 are built
according to Proposition 5. Then  probability at least 1 − δ  PKAWV with the subspaces ˜Ht deﬁned
in (8) satisﬁes the regret upper-bound

Rn ≤ λ(cid:107)f(cid:107)2 + B2deff(λ) log(cid:0)e + enκ2/λ(cid:1) + 2B2(|In| + 1)

nµ
λ

 

and the algorithm runs in O(deff(µ)2) space O(deff(µ)2) time per iteration.

6

Sketched-KONS [CLV17b]
Pros-N-KONS [CLV17a]
PKAWV
PKAWV (beforehand features)

n
R
g
o
l

n
g
o
l

1

γ

1+γ

0

0

4γ

(1+γ)2

2γ
1+γ

1

log m
log n

1

n
R
g
o
l

n
g
o
l

γ

1+γ

0

0

n
R
g
o
l

n
g
o
l

1

γ

1+γ

0

0

4γ

(1+γ)2

1

2γ
1+γ

log m
log n

1

2γ
1+γ

log m
log n

Figure 1: Comparison of the theoretical regret rate log Rn/ log n according to the size of the
dictionary log m/ log n considered by PKAWV  Sketched-KONS and Pros-N-KONS for optimized
parameters when deff(λ) ≤ (n/λ)γ with γ = 0.2 
2−1  0.6 (from left to right). The value γ/(1+γ)
corresponds to the optimal rate.

√

The last term of the regret upper-bound above corresponds to the approximation cost of using the
approximation (8) in PKAWV. This cost is controlled by the parameter µ > 0 which trades-off
between having a small approximation error (small µ) and a small dictionary of size |In| ≈ deff(µ)
(large µ) and thus a small computational complexity. For the Gaussian Kernel  using that deff(λ) ≤

O(cid:0) log(n/λ)d(cid:1)  the above theorem yields for the choice λ = 1 and µ = n−2 a regret bound of
order Rn ≤ O(cid:0)(log n)d+1(cid:1) with a per-round time and space complexity of order O(|In|2) =
O(cid:0)(log n)2d+4(cid:1). We recover a similar result to the one obtained in Section 3.1.
Explicit rates under the capacity condition Assuming the capacity condition deff(λ(cid:48)) ≤(cid:0)n/λ(cid:48)(cid:1)γ

for 0 ≤ γ ≤ 1 and λ(cid:48) > 0  which is a classical assumption made on kernels [RCR15]  the following
corollary provides explicit rates for the regret according to the size of the dictionary m ≈ |In|.
Corollary 7. Let n ≥ 1 and m ≥ 1. Assume that deff(λ(cid:48)) ≤ (n/λ(cid:48))γ for all λ(cid:48) > 0. Then  under
the assumptions of Theorem 6  PKAWV with µ = nm−1/γ has a dictionary of size |In| (cid:46) m and a
regret upper-bounded with high-probability as

(cid:40)

Rn (cid:46)

1+γ

γ

n
nm

1

2− 1

2γ

2γ

1−γ2

if m ≥ n
otherwise

γ

1+γ

for λ = n
for λ = nm

1

2− 1

2γ

.

The per-round space and time complexity of the algorithm is O(m2) per iteration.

γ

The rate of order n
1+γ is optimal in this case (it corresponds to optimizing (2) in λ). If the dictionary
is large enough m ≥ n2γ/(1−γ2)  the approximation term is negligible and the algorithm recovers
the optimal rate. This is possible for a small dictionary m = o(n) whenever 2γ/(1 − γ2) < 1 
2 − 1. The rates obtained in Corollary 7 can be compared to the one
which corresponds to γ <
obtained by Sketched-KONS of [CLV17b] and Pros-N-KONS of [CLV17a] which also provide a
similar trade-off between the dictionary size m and a regret bound. The forms of the regret bounds in
m  µ  λ of the algorithms can be summarized as follows

√

 λ + deff(λ) + nmµ

λ + n
m(λ + deff(λ)) + nµ
λ

m deff(λ)

λ

Rn (cid:46)

for PKAWV with KORS
for Sketched-KONS
for Pros-N-KONS

.

(9)

When deff(λ) ≤ (n/λ)γ  optimizing these bounds in λ  PKAWV performs better than Sketched-
KONS as soon as γ ≤ 1/2 and the latter cannot obtain the optimal rate λ + deff(λ) = n
1+γ if
m = o(n). Furthermore  because of the multiplicative factor m  Pros-N-KONS can’t either reached
the optimal rate even for m = n. Figure 1 plots the rate in n of the regret of these algorithms when
enlarging the size m of the dictionary. We can see that for γ = 1/4  PKAWV is the only algorithm
that achieves the optimal rate nγ/(1+γ) with m = o(n) features. The rate of Pros-N-KONS cannot
beat 4γ/(1 + γ)2 and stops improving even when the size of the dictionary increases. This is because
Pros-N-KONS is restarted whenever a point is added in the dictionary which is too costly for large
dictionaries. It is worth pointing out that these rates are for a well-tuned value of λ. However  such
an optimization can be performed at a small cost using expert advice algorithm on a ﬁnite grid of λ.

γ

7

Furthermore  w.h.p.
complexity O(m2).

n
nm

Rn (cid:46)
the dictionary is of size |In| (cid:46) m leading to a per-round space and time

for λ = n
for λ = nm

− 1

2γ

− 1

2γ

1+γ

.

2γ

1+γ

if m ≥ n
otherwise

γ

√

Figure 2: Average classiﬁcation error and time on: (top) code-rna (n = 2.7 × 105  d = 8); (bottom)
SUSY (n = 6 × 106  d = 22).

Beforehand known features We may assume that the sequence of feature vectors xt is given
in advance to the learner while only the outputs yt are sequentially revealed (see [GGHS18] or
[BKM+15] for details). In this case  the complete dictionary In ⊂ {x1  . . .   xn} may be computed
beforehand and PKAWV can be used with the ﬁx subspace ˜H = Span(φ(x)  x ∈ In). In this case 
the regret upper-bound can be improved to Rn (cid:46) λ + deff(λ) + nµ
λ by removing a factor m in the
last term (see (9)).
Corollary 8. Under the notation and assumptions of Corollary 7  PKAWV used with dictionary In
and parameter µ = nm−1/γ achieves with high probability

(cid:40)

γ

1+γ

m compared to the “sequen-
The suboptimal rate due to a small dictionary is improved by a factor
tially revealed features” setting. Furthermore  since 2γ/(1 + γ) < 1 for all γ ∈ (0  1)  the algorithm
is able to recover the optimal rate nγ/(1+γ) for all γ ∈ (0  1) with a dictionary of sub-linear size
m (cid:28) n. We leave for future work the question whether there is really a gap between these two
settings or if this gap from a suboptimality of our analysis.

4 Experiments

We empirically test PKAWV against several state-of-the-art algorithms for online kernel regression.
In particular  we test our algorithms in (1) an adversarial setting [see Appendix G]  (2) on large scale
datasets. The following algorithms have been tested:

• Kernel-AWV for adversarial setting or Kernel Ridge Regression for i.i.d. real data settings;
• Pros-N-Kons [CLV17b];
• Fourier Online Gradient Descent (FOGD  [LHW+16]);
• PKAWV(or PKRR for real data settings) with Taylor expansions (M ∈ {2  3  4})
• PKAWV(or PKRR for real data settings) with Nyström

The algorithms above have been implemented in python with numpy (the code for our algorithm
is in Appendix H.2). The code necessary to reproduce the following experiments is available on
GitHub at https://github.com/Remjez/kernel-online-learning. For most algorithms  we
used hyperparameters from the respective papers. For all algorithms and all experiments  we set
σ = 1 [except for SUSY where σ = 4  to match accuracy results from RCR17] and λ = 1. When

8

√
using KORS  we set µ = 1  β = 1 and ε = 0.5 as in [CLV17b]. The number of random-features in
FOGD is ﬁxed to 1000 and the learning rate η is 1/
n. All experiments have been done on a single
desktop computer (Intel Core i7-6700) with a timeout of 5-min per algorithm. The results of the
algorithms are only recorded up to this time.
Large scale datasets. The algorithms are evaluated on four datasets from UCI machine learning
repository. In particular  casp (regression) and ijcnn1  cod-rna  SUSY (classiﬁcation) [see Ap-
pendix G for casp and ijcnn1] ranging from 4 × 104 to 6 × 106 datapoints. For all datasets  we
scaled x in [−1  1]d and y in [−1  1]. In Figs. 2 and 4 we show the average loss (square loss for
regression and classiﬁcation error for classiﬁcation) and the computational costs of the considered
algorithm.
In all the experiments PKAWV with M = 2 approximates reasonably well the performance of
kernel forecaster and is usually very fast. We remark that using PKAWV M = 2 on the ﬁrst million
examples of SUSY  we achieve in 10 minutes on a single desktop  the same average classiﬁcation
error obtained with speciﬁc large scale methods for i.i.d. data [RCR17]  although Kernel-AWV is
using a number of features reduced by a factor 100 with respect to the one used in for FALKON in
the same paper. Indeed they used r = 104 Nyström centers  while with M = 2 we used r = 190
features  validating empirically the effectiveness of the chosen features for the Gaussian kernel. This
shows the effectiveness of the proposed approach for large scale machine learning problems with a
moderate dimension d.

References
[ABRW18] Jason Altschuler  Francis Bach  Alessandro Rudi  and Jonathan Weed. Massively
scalable sinkhorn distances via the nystr\" om method. arXiv preprint arXiv:1812.05189 
2018.

[Aro50] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American

mathematical society  68(3):337–404  1950.

[AW01] Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density
estimation with the exponential family of distributions. Machine Learning  43(3):211–
246  2001.

[Bac17] Francis Bach. On the equivalence between kernel quadrature rules and random feature

expansions. Journal of Machine Learning Research  18(21):1–38  2017.

[BKM+15] Peter L. Bartlett  Wouter M. Koolen  Alan Malek  Eiji Takimoto  and Manfred K.
Warmuth. Minimax Fixed-Design Linear Regression. JMLR: Workshop and Conference
Proceedings  40:1–14  2015. Proceedings of COLT’2015.

[BTA11] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in

probability and statistics. Springer Science & Business Media  2011.

[CBCG04] Nicolo Cesa-Bianchi  Alex Conconi  and Claudio Gentile. On the generalization ability
of on-line learning algorithms. IEEE Transactions on Information Theory  50(9):2050–
2057  2004.

[CBL06] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  2006.

[CDV07] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

[CKS11] Andrew Cotter  Joseph Keshet  and Nathan Srebro. Explicit approximations of the

gaussian kernel. arXiv preprint arXiv:1109.4603  2011.

[CLV17a] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Efﬁcient second-order
online kernel learning with adaptive embedding. In Neural Information Processing
Systems  2017.

9

[CLV17b] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Second-order kernel online
convex optimization with adaptive sketching. In International Conference on Machine
Learning  2017.

[DWH18] Michał Derezi´nski  Manfred K Warmuth  and Daniel Hsu. Correcting the bias in least
squares regression with volume-rescaled sampling. arXiv preprint arXiv:1810.02453 
2018.

[Fos91] Dean P. Foster. Prediction in the worst case. The Annals of Statistics  19(2):1084–1090 

1991.

[GGHS18] Pierre Gaillard  Sébastien Gerchinovitz  Malo Huard  and Gilles Stoltz. Uniform regret
bounds over Rd for the sequential linear regression problem with the square loss. arXiv
preprint arXiv:1805.11386  2018.

[HM07] Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In Inter-
national Conference on Computational Learning Theory  pages 499–513. Springer 
2007.

[LHW+16] Jing Lu  Steven CH Hoi  Jialei Wang  Peilin Zhao  and Zhi-Yong Liu. Large scale online
kernel learning. The Journal of Machine Learning Research  17(1):1613–1655  2016.
[OLBC10] Frank WJ Olver  Daniel W Lozier  Ronald F Boisvert  and Charles W Clark. NIST

handbook of mathematical functions. Cambridge University Press  2010.

[RCR15] Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. Less is more: Nyström
computational regularization. In Advances in Neural Information Processing Systems 
pages 1657–1665  2015.

[RCR17] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. Falkon: An optimal large
scale kernel method. In Advances in Neural Information Processing Systems  pages
3888–3898  2017.

[RR17] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with
random features. In Advances in Neural Information Processing Systems  pages 3215–
3225  2017.

[RST13] A. Rakhlin  K. Sridharan  and A.B. Tsybakov. Empirical entropy  minimax regret and

minimax risk. Bernoulli  2013. To appear.

[SHS06] Ingo Steinwart  Don Hush  and Clint Scovel. An explicit description of the reproducing
kernel hilbert spaces of gaussian rbf kernels. IEEE Transactions on Information Theory 
52(10):4635–4643  2006.

[SSL00] AJ Smola  B Schölkopf  and P Langley. Sparse greedy matrix approximation for machine
learning. In 17th International Conference on Machine Learning  Stanford  2000  pages
911–911  2000.

[Vov01] Vladimir Vovk. Competitive on-line statistics.

69(2):213–248  2001.

International Statistical Review 

[Vov05] V. Vovk. On-line regression competitive with reproducing kernel hilbert spaces. arXiv 

2005.

[Vov06] V. Vovk. Metric entropy in competitive on-line prediction. arXiv  2006.
[WS01] Christopher KI Williams and Matthias Seeger. Using the nyström method to speed up
kernel machines. In Advances in neural information processing systems  pages 682–688 
2001.

[ZK10] Fedor Zhdanov and Yuri Kalnishkan. An identity for kernel ridge regression.

In
International Conference on Algorithmic Learning Theory  pages 405–419. Springer 
2010.

[ZL19] Xiao Zhang and Shizhong Liao. Incremental randomized sketching for online kernel
learning. In International Conference on Machine Learning  pages 7394–7403  2019.

10

,Rémi Jézéquel
Alessandro Rudi