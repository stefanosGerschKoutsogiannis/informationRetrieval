2018,Discretely Relaxing Continuous Variables for tractable Variational Inference,We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed "DIRECT" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased  zero-variance gradient estimates)  eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points  permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition  our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods  however  we identify a popular model structure which is practical  and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over 10^2352 log-likelihood evaluations  we train on datasets with over two-million points in just seconds.,Discretely Relaxing Continuous Variables

for tractable Variational Inference

Trefor W. Evans

University of Toronto

Prasanth B. Nair

University of Toronto

trefor.evans@mail.utoronto.ca

pbn@utias.utoronto.ca

Abstract

We explore a new research direction in Bayesian variational inference with discrete
latent variable priors where we exploit Kronecker matrix algebra for efﬁcient and
exact computations of the evidence lower bound (ELBO). The proposed "DIRECT"
approach has several advantages over its predecessors; (i) it can exactly compute
ELBO gradients (i.e. unbiased  zero-variance gradient estimates)  eliminating
the need for high-variance stochastic gradient estimators and enabling the use of
quasi-Newton optimization methods; (ii) its training complexity is independent of
the number of training points  permitting inference on large datasets; and (iii) its
posterior samples consist of sparse and low-precision quantized integers which
permit fast inference on hardware limited devices.
In addition  our DIRECT
models can exactly compute statistical moments of the parameterized predictive
posterior without relying on Monte Carlo sampling. The DIRECT approach is not
practical for all likelihoods  however  we identify a popular model structure which
is practical  and demonstrate accurate inference using latent variables discretized as
extremely low-precision 4-bit quantized integers. While the ELBO computations
considered in the numerical studies require over 102352 log-likelihood evaluations 
we train on datasets with over two-million points in just seconds.

1

Introduction

Hardware restrictions posed by mobile devices make Bayesian inference particularly ill-suited for
on-board machine learning. This is unfortunate since the safety afforded by Bayesian statistics is
extremely valuable in many prominent mobile applications. For example  the cost of erroneous
decisions are very high in autonomous driving or mobile robotic control. The robustness and
uncertainty quantiﬁcation provided by Bayesian inference is therefore extremely valuable for these
applications provided inference can be performed on-board in real-time [1  2].
Outside of mobile applications  resource efﬁciency is still an important concern. For example 
deployed models making billions of predictions per day can incur substantial energy costs  making
energy efﬁciency an important consideration in modern machine learning architectures [3].
We approach the problem of efﬁcient Bayesian inference by considering discrete latent variable
models such that posterior samples of the variables will be quantized and sparse  leading to efﬁcient
inference computations with respect to energy  memory and computational requirements. Training a
model with a discrete prior is typically very slow and expensive  requiring the use of high variance
Monte Carlo gradient estimators to learn the variational distribution. The main contribution of this
work is the development of a method to rapidly learn the variational distribution for such a model
without the use of any stochastic estimators; the objective function will be computed exactly at
each iteration. To our knowledge  such an approach has not been taken for variational inference of
large-scale probabilistic models.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this paper  we compare our work not only to competing stochastic variational inference (SVI)
methods for discrete latent variables  but also to the more general SVI methods for continuous latent
variables. We make this comparison with continuous variables by discretely relaxing continuous priors
using a discrete prior with a ﬁnite support set that contains much of the structure and information as its
continuous analogue. Using this discretized prior we show that we can make use of Kronecker matrix
algebra for efﬁcient and exact ELBO computations. We will call our technique DIRECT (DIscrete
RElaxation of ConTinous variables). We summarize our main contributions below:
• We efﬁciently and exactly compute the ELBO using a discrete prior even when this computation
requires more likelihood evaluations than the number of atoms in the known universe. This
achieves unbiased  zero-variance gradients which we show outperforms competing Monte Carlo
sampling alternatives that give high-variance gradient estimates while learning.
• Complexity of our ELBO computations are independent of the quantity of training data using the
• At inference time  we can exactly compute the statistical moments of the parameterized predictive
• Using a discrete prior  our models admit sparse posterior samples that can be represented as
quantized integer values to enable efﬁcient inference  particularly on hardware limited devices.
• We present the DIRECT approach for generalized linear models and deep Bayesian neural networks
• Our empirical studies demonstrate superior performance relative to competing SVI methods on

posterior distribution  unlike competing techniques which rely on Monte Carlo sampling.

for regression  and discuss approximations that allow extensions to many other models.

DIRECT method  making the proposed approach amenable to big data applications.

problems with as many as 2 million training points.

The paper will proceed as follows; section 2 contains a background on variational inference and poses
the learning problem to be addressed while section 3 outlines the central ideas of the DIRECT method 
demonstrating the approach on several popular probabilistic models. Section 4 discusses limitations
of the proposed approach and outlines some work-arounds  for instance  we discuss how to go beyond
mean-ﬁeld variational inference. We empirically demonstrate our approaches in section 5  and
conclude in section 6. Our full code is available at https://github.com/treforevans/direct.

2 Variational Inference Background

We begin with a review of variational inference  a method for approximating probability densities in
Bayesian statistics [4–9]. We introduce a regression problem for motivation; given X P Rnd  y P
Rn  a d-dimensional dataset of size n  we wish to evaluate y at an untried point x by constructing
a statistical model that depends on the b latent variables in the vector w P Rb. After specifying a
prior over the latent variables  Prpwq  and selecting a probabilistic model structure that admits the
likelihood Prpy|wq  we may proceed with Bayesian inference to determine the posterior Prpw|yq
which generally requires analytically intractable computations.
Variational inference turns the task of computing a posterior into an optimization problem. By
introducing a family of probability distributions qθpwq parameterized by θ  we minimize the Kullback-
Leibler divergence to the exact posterior [9]. This equates to maximization of the evidence lower
bound (ELBO) which we can write as follows for a continuous or discrete prior  respectively

Prior

ELBO

ELBOpθq » qθpwq log Prpy|wq  log Prpwq  log qθpwq	dw 
ELBOpθq  qT log (cid:96)  log p  log q	 

(1)

(2)

i1  log p  tlog Prpwiqum

where log (cid:96)  tlog Prpy|wiqum
W P Rbm is the entire support set of the discrete prior.
It is immediately evident that computing the ELBO is challenging when b is large  since in the
continuous case eq. (1) is a b-dimensional integral  and in the discrete case the size of the sum in eq. (2)
generally increases exponentially with respect to b. Typically  the ELBO is not explicitly computed
and instead  a Monte Carlo estimate of the gradient of the ELBO with respect to the variational

i1  q  tqθpwiqum

i1  and twium

i1 

2

parameters θ is found  allowing stochastic gradient descent to be performed. We will outline some
existing techniques to estimate ELBO gradients with respect to the variational parameters  θ.
For continuous priors  the reparameterization trick [10] can be used to perform variational inference.
The technique uses Monte Carlo estimates of the gradient of the evidence lower bound (ELBO) which
is maximized during the training procedure. While this approach has been employed successfully
for many large-scale models  we ﬁnd that discretely relaxing continuous latent variable priors can
improve training and inference performance when using our proposed DIRECT technique which
computes the ELBO (and its gradients) exactly.
When the latent variable priors are discrete  reparameterization cannot be applied  however  the
REINFORCE [11] estimator may be used to provide an unbiased estimate of the ELBO during
training (alternatively called the score function estimator [12]  or likelihood ratio estimator [13]).
Empirically  the REINFORCE gradient estimator is found to give a high-variance when compared
with reparameterization  leading to a slow learning process. Unsurprisingly  we ﬁnd that our proposed
DIRECT technique trains signiﬁcantly faster than a model trained using a REINFORCE estimator.
Recent work in variational inference with discrete latent variables has largely focused on continuous
relaxations of discrete variables such that reparameterization can be applied to reduce gradient
variance compared to REINFORCE. One example is CONCRETE [14  15] and its extensions [16  17].
We consider an opposing direction by identifying how the ELBO (eq. (2)) can be computed exactly
for a class of discretely relaxed probabilistic models such that the discrete latent variable model can be
trained more easily then its continuous counterpart. We outline this approach in the following section.

3 DIRECT: Efﬁcient ELBO Computations with Kronecker Matrix Algebra

We outline the central ideas of the DIRECT method and illustrate its application on several proba-
bilistic models. The DIRECT method allows us to efﬁciently and exactly compute the ELBO which
has several advantages over existing SVI techniques for discrete latent variable models such as  zero-
variance gradient estimates  the ability to use a super-linearly convergent quasi-Newton optimizer
(since our objective is deterministic)  and the per-iteration complexity is independent of training
set size. We will also discuss advantages at inference time such as the ability to exactly compute
predictive posterior statistical moments  and to exploit sparse and low-precision posterior samples.
To begin  we consider a discrete prior over our latent variables whose support set W forms a Cartesian
tensor product grid as most discrete priors do (e.g. any prior that factorizes between variables) so that
we can write

W 

m b    b 1T
1 b 1T
swT
m
1T
2 b    b 1T
m b swT
m
...
...
...
1T
m b 1T
m b    b swT

...

b

 



(3)

where 1m P Rm denotes a vector of ones  swi P Rm contains the sm discrete values that the ith
latent variable wi can take1  m  smb  and b denotes the Kronecker product [18]. Since the number
q  log p  log (cid:96)  and log q can be written as a sum of Kronecker product vectors (i.e.°iÂb

of columns of W P Rbmb increases exponentially with respect to b  it is evident that computing
the ELBO in eq. (2) is typically intractable when b is large. For instance  forming and storing the
matrices involved naively require exponential time and memory. We can alleviate this concern if
j1 f piq
j  
j P Rm). If we ﬁnd this structure  then we never need to explicitly compute or store a vector
where f piq
of length m. This is because eq. (2) would simply require multiple inner products between Kronecker
product vectors which the following result demonstrates can be computed extremely efﬁciently.
Proposition 1. The inner product between two Kronecker product vectors k  bb
a  bb

i1apiq can be computed as follows [18] 

i1kpiq  and

aT k 

apiq T kpiq 

(4)

1The discrete values that the ith latent variable can take  swi  may be chosen a priori or learned during ELBO
maximization (may be helpful for coarse discretizations). For the sake of simplicity  we focus on the former.

b¹i1

3

where apiq P Rm  a P Rmb  kpiq P Rm  and k P Rmb.
This result enables substantial savings in the computation of the ELBO since each inner product

computation is reduced from the naive exponential Opsmbq cost to a linear Opbsmq cost.
if the prior is chosen to factorize between latent variables  as it often is  (i.e. Prpwq ±b
i1pi admits a Kronecker product structure where pi  tPrpwiswijqu

We now discuss how the Kronecker product structure of the variables in eq. (2) can be achieved. Firstly 
i1 Prpwiq)
then p  bb
m.
The following result demonstrates how this structure for p enables log p to be written as a sum of b
Kronecker product vectors.
Proposition 2. The element-wise logarithm of the Kronecker product vector k  bb
written as a sum of b Kronecker product vectors as follows 

i1kpiq can be

m
j1 P p0  1q

log k 

log kpiq 

(5)

bài1

where kpiq P Rm  k P Rmb contain positive values  and ` is a generalization of the Kronecker
sum [19] for vectors which we deﬁne as follows

log kpiq 

bài1

b¸i1 i1âj1

1m
 b log kpiq b bâji1

1m
.

(6)

The proof is trivial. We will ﬁrst consider a mean-ﬁeld variational distribution that factorizes over
i1 log qi can be written as a sum of
latent variables such that both q  bb
m are used as the variational
parameters  θ  with the use of the softmax function. For the mean-ﬁeld case we can rewrite eq. (2) as

Kronecker product vectors  where qj  tPrpwjswjiqu

i1qi and log q  `b

m
i1 P p0  1q

ELBOpθq  qT log (cid:96) 

qT

i log pi 

qT

i log qi 

(7)

b¸i1

b¸i1

where we use the fact that qi deﬁnes a valid probability distribution for the ith latent variable such
that qT
i 1m  1. We extend results to unfactorized prior and variational distributions later in section 4.
The structure of log (cid:96) depends on the probabilistic model used; in the worst case  log (cid:96) can always
be represented as a sum of m Kronecker product vectors. However  many models admit a far more
compact structure where dramatic savings can be realized as we demonstrate in the following sections.

3.1 Generalized Linear Regression

We ﬁrst focus on the popular class of Bayesian generalized linear models (GLMs) for regression.
While the Bayesian integrals that arise in GLMs can be easily computed in the case of conjugate
priors  for general priors inference is challenging.
This highly general model architecture has been applied in a vast array of application areas. Recently 
Wilson et al. [20] used a scalable Bayesian generalized linear model with Gaussian priors on the
output layer of deep neural network with notable empirical success. They also considered the ability
to train the neural network simultaneously with the approximate Gaussian process which we also
have the ability to do if a practitioner were to require such an architecture.
Consider the generalized linear regression model y  Φw    where   N p0  σ2Iq  and Φ 
tφjpxiqui j P Rnb contains the evaluations of the basis functions on the training data. The following
result demonstrates how the ELBO can be exactly and efﬁciently computed  assuming the factorized
prior and variational distributions over w discussed earlier. Note that we also consider a prior over σ2.
Theorem 1. The ELBO can be exactly computed for a discretely relaxed regression GLM as follows

ELBOpθq  

n
2

qT
σ log σ2 

1

σ σ2yT y  2sTΦT y  sT ΦT Φs  diagpΦT ΦqT s2
2qT
j hj	 
b¸i1qT

i log qi  qT

σ log pσ  qT

i log pi  qT

σ log qσ 

(8)

qT

b¸j1

4

i1 φ2

ijub

j1 P Rmb  and s  tqT

where qσ  pσ P Rm are factorized variational and prior distributions over the Gaussian noise
variance σ2 for which we consider the discrete positive values σ2 P Rm  respectively. Also  we use

A proof is provided in appendix A of the supplementary material. We can pre-compute the terms yT y 
ΦT y  H  and ΦT Φ before training begins (since these do not depend on the variational parameters)
such that the ﬁnal complexity of the proposed DIRECT method outlined in Theorem 1 is only

j°n
the shorthand notation H  tsw2
Opbsm  b2q. This complexity is independent of the number of training points  making the proposed

technique ideal for massive datasets. Also  each of the pre-computed terms can easily be updated as
more data is observed making the techniques amenable to online learning applications.

j swjub

j1 P Rb.

Predictive Posterior Computations Typically  the predictive posterior distribution is found by
sampling the variational distribution at a large number of points and running the model forward for
each sample. To exactly compute the statistical moments  a model would have to be run forward at
every point in the hypothesis space with is typically intractable  however  we can exploit Kronecker
matrix algebra to efﬁciently compute these moments exactly. For example  the exact predictive
posterior mean for our generalized linear regression model is computed as follows

Epyq 

m¸i1

qpwiq» y Prpy|wiqdy   ΦWq  Φs 

(9)

j swjub

j1 P Rb  and Φ P R1b contains the basis functions evaluated at x. This
where s  tqT
computation is highly efﬁcient  requiring just Opbq time per test point. It can be shown that a similar
scheme can be derived to exactly compute higher order statistical moments  such as the predictive
posterior variance  for generalized linear regression models and other DIRECT models.
We have shown how to exactly compute statistical moments  and now we show how to exploit our
discrete prior to compute predictive posterior samples extremely efﬁciently. This sampling approach
may be preferable to the exact computation of statistical moments on hardware limited devices where
we need to perform inference with extreme memory  energy and computational efﬁciency. The

quantized integer array because of the discrete support of the prior which enables extremely compact
storage in memory. Much work has been done elsewhere in the machine learning community to
quantize variables for storage compression purposes since memory is a very restrictive constraint on
mobile devices [21–24]. However  we can go beyond this to additionally reduce computational and

latent variable posterior samplesW P Rbnum. samples will of course be represented as a low-precision
energy demands for the evaluation of ΦW. One approach is to constrain the elements of sw to be 0

or a power of 2 so that multiplication operations simply become efﬁcient bit-shift operations [25–27].
An even more efﬁcient approach is to employ basis functions with discrete outputs so that Φ can
also be represented as a low-precision quantized integer array. For example  a rounding operation
could be applied to continuous basis functions. Provided that the quantization schemes are an afﬁne
mapping of integers to real numbers (i.e. the quantized values are evenly spaced)  then inference can
be conducted using extremely efﬁcient integer arithmetic [28]. Either of these approaches enable
extremely efﬁcient on-device inference.

3.2 Deep Neural Networks for Regression

We consider the hierarchical model structure of a Bayesian deep neural network for regression.
Considering a DIRECT approach for this architecture is not conceptually challenging so long as
an appropriate neuron activation function is selected. We would like a non-linear activation that
maintains a compact representation of the log-likelihood evaluated at every point in the hypothesis
space  i.e. we would like log (cid:96) to be represented as a sum of as few Kronecker product vectors as
possible. Using a power function for the activation can maintain a compact representation; the natural
choice being a quadratic activation function (i.e. output x2 for input x).

It can be shown that the ELBO can be exactly computed in Op(cid:96)smpb{(cid:96)q4(cid:96)q for a deep Bayesian neural

network with (cid:96) layers  where we assume a quadratic activation function and an equal distribution
of discrete latent variables between network layers. This complexity evidently enables scalable
Bayesian inference for models of moderate depth  and like we found for the regression GLM model of
section 3.1  computational complexity is independent of the quantity of training data  making this ap-
proach ideal for large datasets. We outline this model and the computation of its ELBO in appendix D.

5

4 Limitations & Extensions

In generality  when the support of the prior is on a Cartesian grid  any prior  likelihood  or variational
distribution (or log-distribution) can be expressed using the proposed Kronecker matrix representation 
however  this representation will not always be compact enough to be practical. We can see this
by viewing these probability distributions over the hypothesis space as high-dimensional tensors.
In section 3  we exploited some popular models whose variational probability tensors  and whose
prior  likelihood and variational log-probability tensors all admit a low-rank structure  however  other
models may not admit this structure  in which case their representation will not be so compact. In the
interest of generalizing the technique  we outline a likelihood  a prior  and a variational distribution
that does not admit a compact representation of the ELBO and discuss several ways the DIRECT
method can still be used to efﬁciently compute  or lower bound the ELBO. We hope that these
extensions inspire future research directions in approximate Bayesian inference.

Generalized Linear Logistic Regression Logistic regression models do not easily admit a
compact representation for exact ELBO computations  however  we will demonstrate that we can
efﬁciently compute a lower-bound of the ELBO by leveraging developed algebraic techniques. To
demonstrate  we will consider a generalized linear logistic regression model which is commonly
employed for classiﬁcation problems. Such a model could easily be extended to a deep architecture
following Bradshaw et al. [2]  if desired. All terms in the ELBO in eq. (7) can be computed
exactly for this model except the term involving the log-likelihood  for which the following result
demonstrates an efﬁcient computation of the lower bound.
Theorem 2. For a generalized linear logistic regression model with classiﬁcation training labels
y P t0  1un  the class-conditional probability Prpyi0|wq  p1  exppΦri  :swqq1  and with the
assumption that training examples are sampled independently  the following inequality holds

n¸i1# ±b
±b

j1 qT
j1 qT

qT log (cid:96) ¥ sTΦT y 
We prove this result in appendix B of the supplement. This computation can be performed in Opsmbnq

time  where dependence on n is evident unlike in the case of the exact computations described in
section 3. As a result  stochastic optimization techniques should be considered. Using this lower
bound  the log-likelihood is accurately approximated for hypotheses that correctly classify the training
data  however  hypotheses that conﬁdently misclassify training labels may be over-penalized. In
appendix B we further discuss the accuracy of this approximation and discuss a stable implementation.

j exppφijswjq
j exppφijswjq °b

i φijswj

if yi  0
if yi  1

j1 qT

(10)

q °r

j1 qpiq

i1 αiÂb
j  tPrpwjswjk|iqu

Unfactorized Variational Distributions We now consider going beyond a mean-ﬁeld variational
distribution to account for correlations between latent variables. Considering a ﬁnite mixture of
factorized categorical distributions as is used in latent structure analysis [29  30]  we can write
j   where α P p0  1qr is a vector of mixture probabilities for r components 

m.

m
k1 P p0  1q

and qpiq
While q can evidently be expressed as a compact sum of Kronecker product vectors  log q is more
challenging to compute than in the mean-ﬁeld case  however  the following result demonstrates how
we can lower-bound the term involving log q in the ELBO (eq. (7)).
Theorem 3. The following inequality holds when we consider a ﬁnite mixture of factorized categorical
distributions for qθpwq 

qT log q ¥

max

1 

taiPp0 1q mub

i1

r¸j1

αj b¸i1

qpjq T
i

log ai  αj

qpjq T
i

qpjq
i
ai

b¹i1

 2

αk

r¸kj1

qpjq T
i

b¹i1

qpkq
i

ai 
 

where a  bb

i1ai  ai P p0  1q

m is the center of the Taylor series approximation of log q.

We prove this result in appendix C and discuss a stable implementation. Note that if the mixture
variational distribution q degenerates to a mean-ﬁeld distribution equal to a  then the ELBO will be
computed exactly  and as q moves away from a  the ELBO will be underestimated.

6

j1 ppiq

i1 αiÂb

distribution given by p °r

Unfactorized Prior Distributions To consider an unfactorized prior  we assume a prior mixture
j . When we use this mixture distribution for the prior  p
can evidently be expressed as a compact sum of Kronecker product vectors but log p cannot. The
following result demonstrates how we can still lower-bound the term involving log p in the ELBO
(eq. (2)). For simplicity  we assume that the variational distribution factorizes  however  the result
could easily be extended to the case of a mixture variational distribution.
Theorem 4. The following inequality holds when we consider a ﬁnite mixture of factorized categorical
distributions for pθpwq 

qT log p ¥

αi

r¸i1

b¸j1

j log ppiq
qT

j

The proof is trivial by Jensen’s inequality. Note that the equality only holds when the prior mixture
degenerates to a factorized distribution with all mixture components equivalent.

Unbiased Stochastic Entropy and Prior Expectation Gradients We previously showed how to
lower bound the ELBO terms qT log p and qT log q when the variational and/or prior distributions
do not factor  however  optimizing this bound introduces bias and does not guarantee convergence to
a local optimum of the true ELBO. Here we reintroduce REINFORCE to deliver unbiased gradient
estimates for these terms. The REINFORCE estimator typically has high variance  however  since
gradient estimates for these terms are so cheap  a massive number of samples can be used per
stochastic gradient descent (SGD) iteration to decrease variance. Since we can still compute the
expensive qT log (cid:96) term exactly when q is an unfactorized mixture distribution  its gradient can be
computed exactly. The unbiased gradient estimator of qT log q is expressed as follows2

B
Bθ

qT log q 

1
2

 

(11)

Bθ log q  12
 
qT B

B
Bθ

1
2t

t¸i1 log qpsiq  12

where si P Rb is the ith of t samples from the variational distribution used in the Monte Carlo gradient
estimator. It is evident that this surrogate loss can be easily optimized using automatic differentiation 
and the per-sample computations are extremely cheap.

5 Numerical Studies

5.1 Comparison with REINFORCE

As discussed in section 2  we cannot reparameterize because of the discrete latent variable priors
considered  however  we can directly compare the optimization performance of the proposed tech-
niques with the REINFORCE gradient estimator [11]. In ﬁg. 1  we compare ELBO maximization
performance between the proposed DIRECT  and the REINFORCE methods. For this study we gen-
erated a dataset from a random weighting of b  20 random Fourier features of a squared exponential
kernel [31] and corrupted by independent Gaussian noise. We use a generalized linear regression

model as described in section 3.1 which uses the same features with sm  3. We consider a prior over
σ2  and a mean-ﬁeld variational distribution giving smpb  1q  63 variational parameters which we

initialize to be the same as the prior; a uniform categorical distribution. For DIRECT  a L-BFGS
optimizer is used [32] and stochastic gradient descent is used for REINFORCE with a varying number
of samples used for the Monte Carlo gradient estimator. Both methods use full batch training and are
implemented using TensorFlow [33]. It can be seen that DIRECT greatly outperforms REINFORCE
both in the number of iterations and computational time. As we move to a large n or a larger b  the
difference between the proposed DIRECT technique and REINFORCE becomes more profound. The
superior scaling with respect to n was expected since we had shown in section 3.1 that the DIRECT
computational runtime is independent of n. However  the improved scaling with respect to b is an
interesting result and may be attributed to the fact that as the dimension of the variational parameter
space increases  there is more value in having low (or zero) variance estimates of the gradient.

2We used the identity  log q  1 d

B log q

Bθ

 1
2

B

Bθ log q  12  where d denotes an elementwise product.

7

Figure 1: Convergence rates of a GLM trained with REINFORCE verses the proposed DIRECT
method. The DIRECT method greatly outperforms REINFORCE in iterations and wall-clock time.

5.2 Relaxing Gaussian Priors on UCI Regression Datasets

In this section  we consider discretely relaxing a continuous Gaussian prior on the weights of a gener-
alized linear regression model. This allows us to compare performance between a reparameterization
gradient estimator for a continuous prior and our DIRECT method for a relaxed  discrete prior.
Considering regression datasets from the UCI repository  we report the mean and standard deviation
of the root mean squared error (RMSE) from 10-fold cross validation3. Also presented is the mean
training time per fold on a machine with two E5-2680 v3 processors and 128Gb of RAM  and the
expected sparsity (percentage of zeros) within a posterior sample. All models use b  2000 basis
functions. Further details of the experimental setup can be found in appendix E. In table 1  we see
the results of our studies across several model-types. In the left column  the “REPARAM Mean-
Field” model uses a (continuous) Gaussian prior  an uncorrelated Gaussian variational distribution
and reparameterization gradients. The right two models use a discrete relaxation of a Gaussian
prior (DIRECT) with support at 15 discrete values  allowing storage of each latent variable sample
as a vector of 4-bit quantized integers. Therefore  each ELBO evaluation requires 152000 ¡ 102352
log-likelihood evaluations  however  these computation can be done quickly by exploiting Kronecker
matrix algebra. We compute the ELBO as described in section 3.1 for the “DIRECT Mean-Field”
model  and use the low-variance  unbiased gradient estimator described in eq. (11) for the “DIRECT
5-Mixture SGD” model which uses a mixture distribution with r  5 components  and t  3000
Monte Carlo samples for the entropy gradient estimator.
The boldface entries indicate top performance on each dataset  where it is evident that the DIRECT
method not only outperformed REPARAM on most datasets but also trained much faster  particularly
on the large datasets due to the independence of dataset size on computational complexity. The

seconds on all datasets  including electric with over 2 million points. The DIRECT mixture model

DIRECT mean-ﬁeld model contains smb  30  000 variational parameters  however  training took just
contains smbr  150  000 variational parameters  and since the gradient estimates are stochastic 

average training times are on the order of hundreds of seconds across all datasets. While the time for
precomputations does depend on dataset size  its contribution to the overall timings are negligible 
being well under one second for the largest dataset  electric. Additionally  it is evident that posterior
samples from the DIRECT model tend to be very sparse. For example  the DIRECT models on the
gas dataset admit posterior samples that are over 84% sparse on average  meaning that over 1680
weights are expected to be zero in a posterior sample with b  2000 elements. This would yield
massive computational savings on hardware limited devices. Samples from the DIRECT models on
the electric dataset are over 99.6% sparse.
Comparing the DIRECT mean-ﬁeld model to the mixture model  we observe gains in the RMSE
performance on many datasets  as we would expect with the increased ﬂexibility of the variational
distribution. While we only showed the posterior mean in our results  we would expect an even
larger disparity in the quality of the predictive uncertainty which was not analyzed. In table 2 of
the supplement  we present results for a DIRECT mixture model that uses the ELBO lower bound
presented in Theorem 3. This model does not perform as well as the DIRECT mixture model trained
using an unbiased SGD approach  as would be expected  however  it does train faster since its

390% train  10% test per fold. We use folds from https://people.orie.cornell.edu/andrew/code

8

020406080100Iterations1301201101009080ELBO101100101102Time Elapsed (s)1301201101009080DIRECTREINFORCE 1 sampleREINFORCE 10 sampleDataset

n

d

Time RMSE

Sparsity Time RMSE

Sparsity RMSE

Sparsity

Continuous Prior

REPARAM Mean-Field

Discrete 4-bit Prior

DIRECT Mean-Field

DIRECT 5-Mixture SGD

8
4
9
8
25 5
4
5
33 5
5
7
5
6
7
5
13 5
12 5
11 5
5
9
5
8
8
5
10 5
5
5
11 5
128 5
19 46
26 47
20 48
26 50
18 51
9
58
20 58
385 61
27 61

challenger 23
fertility
100
automobile 159
servo
167
194
cancer
209
hardware
308
yacht
392
autompg
506
housing
forest
517
stock
536
pendulum 630
768
energy
1030
concrete
1066
solar
1503
airfoil
wine
1599
2565
gas
3338
skillcraft
sml
4137
parkinsons 5875
15000
poletele
elevators
16599
45730
protein
48827
kegg
53500
ctslice
63608
keggu
434874 3
3droad
song
515345 90 158 0.537  0.002
583250 77 169 0.94  0.006
buzz
electric
2049280 11 500 9.26  4.47

0.515  0.284 0%
0%
0.161  0.043
0%
0.425  0.2
0%
0.524  0.184
0%
27.488  5.45
0%
1.796  1.537
0%
0.815  0.17
0%
4.05  0.739
0%
3.014  0.567
0%
1.378  0.148
0%
0.751  0.338
0%
1.465  0.26
78.852  21.73 0%
10.347  2.847 0%
0%
0.902  0.171
2.071  0.271 0%
0%
0.939  0.33
0%
0.27  0.052
0%
0.273  0.029
0.327  0.013 0%
0.158  0.009 0%
12.487  0.363 0%
0%
0.247  0.156
0%
0.642  0.006
0.178  0.012 0%
4.415  0.113 0%
0.122  0.004 0%
141 11.057  0.091 0%
0%
0%
0%

1
2
10
10
4
11
1
10
10
2
8
1
1
10
10
11
11
1
7
1
1
10
1
11
1
2
1
2
2
1
1

0.523  0.248 17%
0.159  0.041 17%
0.129  0.063 51%
0.271  0.08 35%
22.954  3.09 19%
0.401  0.048 51%
96%
0.234  0.07
2.564  0.363 31%
2.752  0.405 40%
17%
1.363  0.15
0.011  0.003 98%
1.329  0.282 68%
3.272  0.332 99%
5.316  0.716 82%
0.787  0.192 23%
2.175  0.349 48%
0.472  0.044 54%
0.211  0.058 84%
0.253  0.016 97%
0.677  0.044 57%
0.651  0.034 13%
13.65  0.348 16%
0.124  0.003 99%
0.619  0.007 76%
0.222  0.009 96%
6.063  0.122 19%
0.139  0.004 87%
10.493  0.105 40%
0.501  0.002 32%
1.007  0.007 82%
0.575  0.032 99.6% 0.557  0.055 99.6%

17%
0.525  0.246
17%
0.16  0.041
0.122  0.056 51%
35%
0.274  0.077
22.937  3.135 19%
0.401  0.046 51%
0.225  0.082 96%
2.543  0.362 31%
2.699  0.361 39%
1.357  0.155 17%
0.008  0.001 98%
1.312  0.253 63%
2.911  0.309 99%
82%
5.477  0.632
23%
0.788  0.189
45%
2.156  0.316
0.469  0.042 54%
0.184  0.063 76%
0.253  0.016 97%
57%
0.671  0.047
13%
0.613  0.083
13.369  0.431 17%
0.124  0.003 99%
0.618  0.007 60%
95%
0.205  0.004
42%
5.478  0.137
87%
0.136  0.006
10.354  0.077 33%
0.498  0.002 28%
80%
0.959  0.004

Table 1: Mean and standard deviation of test error  average training time  and average expected
sparsity of a posterior sample from 10-fold cross validation on UCI regression datasets.

objective is evaluated deterministically  and its RMSE performance is still marginally better than the
DIRECT mean-ﬁeld model on many datasets.

6 Conclusions

We have shown that by discretely relaxing continuous priors  variational inference can be performed
accurately and efﬁciently using our DIRECT method. We have demonstrated that through the
use of Kronecker matrix algebra  the ELBO of a discretely relaxed model can be efﬁciently and
exactly computed even when this computation requires signiﬁcantly more log-likelihood evaluations
than the number of atoms in the known universe. Through this ability to exactly perform ELBO
computations we achieve unbiased  zero-variance gradient estimates using automatic differentiation
which we show signiﬁcantly outperforms competing Monte Carlo alternatives that admit high-variance
gradient estimates. We also demonstrate that the computational complexity of ELBO computations
is independent of the quantity of training data using the DIRECT method  making the proposed
approaches amenable to big data applications. At inference time  we show that we can again use
Kronecker matrix algebra to exactly compute the statistical moments of the parameterized predictive
posterior distribution  unlike competing techniques which rely on Monte Carlo sampling. Finally  we
discuss and demonstrate how posterior samples can be sparse and can be represented as quantized
integer values to enable efﬁcient inference which is particularly powerful on hardware limited devices 
or if energy efﬁciency is a major concern.
We illustrate the DIRECT approach on several popular models such as mean-ﬁeld variational inference
for generalized linear models and deep Bayesian neural networks for regression. We also discuss
some models which do not admit a compact representation for exact ELBO computations. For these
cases  we discuss and demonstrate novel extensions to the DIRECT method that allow efﬁcient
computation of a lower bound of the ELBO  and we demonstrate how an unfactorized variational
distribution can be used by introducing a manageable level of stochasticity into the gradients. We
hope that these new approaches for ELBO computations will inspire new model structures and
research directions in approximate Bayesian inference.

9

Acknowledgements

Research funded by an NSERC Discovery Grant and the Canada Research Chairs program.

References
[1] S. Thrun  W. Burgard  and D. Fox. Probabilistic robotics. MIT press  2005.
[2]

J. Bradshaw  A. G. d. G. Matthews  and Z. Ghahramani. Adversarial Examples  Uncertainty 
and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks. Tech. rep. 2017.
[3] C. Louizos  K. Ullrich  and M. Welling. “Bayesian compression for deep learning”. In: Ad-

vances in Neural Information Processing Systems. 2017  pp. 3288–3298.

[4] M. I. Jordan  Z. Ghahramani  T. S. Jaakkola  and L. K. Saul. “An introduction to variational

methods for graphical models”. In: Machine learning 37.2 (1999)  pp. 183–233.

[5] M. J. Wainwright and M. I. Jordan. “Graphical models  exponential families  and variational

inference”. In: Foundations and Trends in Machine Learning 1.1–2 (2008)  pp. 1–305.

[6] M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley. “Stochastic variational inference”. In:

The Journal of Machine Learning Research 14.1 (2013)  pp. 1303–1347.

[7] R. Ranganath  S. Gerrish  and D. Blei. “Black box variational inference”. In: Artiﬁcial Intelli-

gence and Statistics. 2014  pp. 814–822.

[8] A. Kucukelbir  D. Tran  R. Ranganath  A. Gelman  and D. M. Blei. “Automatic differentiation
variational inference”. In: The Journal of Machine Learning Research 18.1 (2017)  pp. 430–
474.

[9] D. M. Blei  A. Kucukelbir  and J. D. McAuliffe. “Variational inference: A review for statisti-

cians”. In: Journal of the American Statistical Association 112.518 (2017)  pp. 859–877.

[10] D. P. Kingma and M. Welling. “Auto-encoding variational Bayes”. In: arXiv preprint

arXiv:1312.6114 (2013).

[11] R. J. Williams. “Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning”. In: Reinforcement Learning. 1992  pp. 5–32.

[12] M. C. Fu. “Gradient estimation”. In: Handbooks in operations research and management

science 13 (2006)  pp. 575–616.

[13] P. W. Glynn. “Likelihood ratio gradient estimation for stochastic systems”. In: Communications

of the ACM 33.10 (1990)  pp. 75–84.

[14] E. Jang  S. Gu  and B. Poole. “Categorical reparameterization with Gumbel-softmax”. In:

arXiv preprint arXiv:1611.01144 (2016).

[15] C. J. Maddison  A. Mnih  and Y. W. Teh. “The concrete distribution: A continuous relaxation

of discrete random variables”. In: arXiv preprint arXiv:1611.00712 (2016).

[16] G. Tucker  A. Mnih  C. J. Maddison  J. Lawson  and J. Sohl-Dickstein. “REBAR: Low-
variance  unbiased gradient estimates for discrete latent variable models”. In: Advances in
Neural Information Processing Systems. 2017  pp. 2627–2636.

[17] W. Grathwohl  D. Choi  Y. Wu  G. Roeder  and D. Duvenaud. “Backpropagation through
the void: Optimizing control variates for black-box gradient estimation”. In: International
Conference on Learning Representations. 2017.

[18] C. F. Van Loan. “The ubiquitous Kronecker product”. In: Journal of Computational and

Applied Mathematics 123.1 (2000)  pp. 85–100.

[19] R. A. Horn and C. R. Johnson. Topics in Matrix analysis. Cambridge university press  1994 

p. 208.

[20] A. G. Wilson  Z. Hu  R. Salakhutdinov  and E. P. Xing. “Deep kernel learning”. In: Artiﬁcial

Intelligence and Statistics. 2016  pp. 370–378.

[21] W. Chen  J. Wilson  S. Tyree  K. Weinberger  and Y. Chen. “Compressing neural networks with
the hashing trick”. In: International Conference on Machine Learning. 2015  pp. 2285–2294.
[22] Y. Gong  L. Liu  M. Yang  and L. Bourdev. “Compressing deep convolutional networks using

vector quantization”. In: arXiv preprint arXiv:1412.6115 (2014).

[23] S. Han  H. Mao  and W. J. Dally. “Deep compression: Compressing deep neural networks
with pruning  trained quantization and huffman coding”. In: arXiv preprint arXiv:1510.00149
(2015).

10

[24] A. Zhou  A. Yao  Y. Guo  L. Xu  and Y. Chen. “Incremental network quantization: Towards

lossless cnns with low-precision weights”. In: arXiv preprint arXiv:1702.03044 (2017).
I. Hubara  M. Courbariaux  D. Soudry  R. El-Yaniv  and Y. Bengio. “Binarized neural net-
works”. In: Advances in neural information processing systems. 2016  pp. 4107–4115.

[25]

[26] F. Li  B. Zhang  and B. Liu. “Ternary weight networks”. In: arXiv preprint arXiv:1605.04711

(2016).

[27] M. Rastegari  V. Ordonez  J. Redmon  and A. Farhadi. “Xnor-net: Imagenet classiﬁcation
using binary convolutional neural networks”. In: European Conference on Computer Vision.
Springer. 2016  pp. 525–542.

[28] B. Jacob  S. Kligys  B. Chen  M. Zhu  M. Tang  A. Howard  H. Adam  and D. Kalenichenko.
“Quantization and Training of Neural Networks for Efﬁcient Integer-Arithmetic-Only Infer-
ence”. In: arXiv preprint arXiv:1712.05877 (2017).

[29] P. Lazarsfeld and N. Henry. Latent structure analysis. Houghton Mifﬂin Company  Boston 

Massachusetts  1968.

[30] L. A. Goodman. “Exploratory latent structure analysis using both identiﬁable and unidentiﬁable

models”. In: Biometrika 61.2 (1974)  pp. 215–231.

[31] A. Rahimi and B. Recht. “Random features for large-scale kernel machines”. In: Advances in

Neural Information Processing Systems. 2007  pp. 1177–1184.

[32] R. H. Byrd  P. Lu  J. Nocedal  and C. Zhu. “A limited memory algorithm for bound constrained

optimization”. In: SIAM Journal on Scientiﬁc Computing 16.5 (1995)  pp. 1190–1208.

[33] M. Abadi et al. “TensorFlow: A System for Large-Scale Machine Learning.” In: OSDI. Vol. 16.

2016  pp. 265–283.

[34] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  2006.
[35] F. Nielsen and K. Sun. “Guaranteed bounds on the Kullback-Leibler divergence of univariate

mixtures using piecewise log-sum-exp inequalities”. In: arXiv:1606.05850 (2016).

[36] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[37] D. Tran  A. Kucukelbir  A. B. Dieng  M. Rudolph  D. Liang  and D. M. Blei. “Edward: A library
for probabilistic modeling  inference  and criticism”. In: arXiv preprint arXiv:1610.09787
(2016).

11

,Trefor Evans
Prasanth Nair