2019,Initialization of ReLUs for Dynamical Isometry,Deep learning relies on good initialization schemes and hyperparameter choices prior to training a neural network. Random weight initializations induce random network ensembles  which give rise to the trainability  training speed  and sometimes also generalization ability of an instance. In addition  such ensembles provide theoretical insights into the space of candidate models of which one is selected during training. The results obtained so far rely on mean field approximations that assume infinite layer width and that study average squared signals. We derive the joint signal output distribution exactly  without mean field assumptions  for fully-connected networks with Gaussian weights and biases  and analyze deviations from the mean field results. For rectified linear units  we further discuss limitations of the standard initialization scheme  such as its lack of dynamical isometry  and propose a simple alternative that overcomes these by initial parameter sharing.,Initialization of ReLUs for Dynamical Isometry

Rebekka Burkholz

Department of Biostatistics

Harvard T.H. Chan School of Public Health
655 Huntington Avenue  Boston  MA 02115

rburkholz@hsph.harvard.edu

Alina Dubatovka

Department of Computer Science

ETH Zurich

Universitätstrasse 6  8092 Zurich
alina.dubatovka@inf.ethz.ch

Abstract

Deep learning relies on good initialization schemes and hyperparameter choices
prior to training a neural network. Random weight initializations induce random
network ensembles  which give rise to the trainability  training speed  and some-
times also generalization ability of an instance. In addition  such ensembles provide
theoretical insights into the space of candidate models of which one is selected
during training. The results obtained so far rely on mean ﬁeld approximations
that assume inﬁnite layer width and that study average squared signals. We derive
the joint signal output distribution exactly  without mean ﬁeld assumptions  for
fully-connected networks with Gaussian weights and biases  and analyze deviations
from the mean ﬁeld results. For rectiﬁed linear units  we further discuss limitations
of the standard initialization scheme  such as its lack of dynamical isometry  and
propose a simple alternative that overcomes these by initial parameter sharing.

1

Introduction

Deep learning relies critically on good parameter initialization prior to training. Two approaches are
commonly employed: random network initialization [4  7  14] and transfer learning [26] (including
unsupervised pre-training)  where a network that was trained for a different task or a part of it
is retrained and extended by additional network layers. While the latter can speed up training
considerably and also improve the generalization ability of the new model  its bias towards the
original task can also hinder successful training if the learned features barely relate to the new task.
Random initialization of parameters  meanwhile  requires careful tuning of the distributions from
which neural network weights and biases are drawn. While heterogeneity of network parameters is
needed to produce meaningful output  a too big variance can also dilute the original signal. To avoid
exploding or vanishing gradients  the distributions can be adjusted to preserve signal variance from
layer to layer. This enables the training of very deep networks by simple stochastic gradient descent
(SGD) without the need of computationally intensive corrections as batch normalization [8] or variants
thereof [12]. This approach is justiﬁed by the similar update rules of gradient back-propagation and
signal forward propagation [20]. In addition to trainability  good parameter initializations also seem
to support the generalization ability of the trained  overparametrized network. According to [3]  the
parameter values remain close to the initialized ones  which has a regularization effect.
An early example of approximate signal variance preservation is proposed in [4] for fully connected
feed forward neural networks  an important building block of most common neural architectures.
Inspired by those derivations  He et al. [7] found that for rectiﬁed linear units (ReLUs) and Gaussian
weight initialization w ∼ N (µ  σ2) the optimal choice is zero mean µ = 0  variance σ2 = 2/N and
zero bias b = 0  where N refers to the number of neurons in a layer. These ﬁndings are conﬁrmed by
mean ﬁeld theory  which assumes inﬁnitely wide network layers to employ the central limit theorem
and focus on normal distributions. Similar results have been obtained for tanh [16  18  20]  residual
networks with different activation functions [24]  and convolutional neural networks [23]. The same

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

derivations also lead to the insight that inﬁnitely wide fully-connected neural networks approximately
learn the kernel of a Gaussian process [11]. According to these works  not only the signal variance
but also correlations between signals corresponding to different inputs need to be preserved to ensure
good trainability of initialized neural networks. This way  the average eigenvalue of the signal input-
output Jacobian in mean ﬁeld neural networks is steered towards 1. Furthermore  a high concentration
of the full spectral density of the Jacobian close to 1 seems to support higher training speeds [14  15].
This property is called dynamical isometry and is better realized by orthogonal weight initializations
[19]. So far  these insights rely on the mean ﬁeld assumption of inﬁnite layer width. [6  5] have
derived ﬁnite size corrections for the average squared signal norm and answered the question when
the mean ﬁeld assumption holds.
In this article  we determine the exact signal output distribution without requiring mean ﬁeld ap-
proximations. For fully-connected network ensembles with Gaussian weights and biases for general
nonlinear activation functions  we ﬁnd that the output distribution only depends on the scalar products
between different inputs. We therefore focus on their propagation through a network ensemble. In
particular  we study a linear transition operator that advances the signal distribution layer-wise. We
conjecture that the spectral properties of this operator can be more informative of trainability than
the average spectral density of the input-output Jacobian. Additionally  the distribution of the cosine
similarity indicates how well an initialized network can distinguish different inputs.
We further discuss when network layers of ﬁnite width are well represented by mean ﬁeld analysis and
when they are not. Furthermore  we highlight important differences in the analysis. By specializing
our derivations to ReLUs  we ﬁnd variants of the He initialization [7] that fulﬁll the same criteria but
also suffer from the same lack of dynamical isometry [14]. In consequence  such initialized neural
networks cannot be trained effectively without batch normalization for high depth. To overcome
this problem  we propose a simple initialization scheme for ReLU layers that guarantees perfect
dynamical isometry. A subset of the weights can still be drawn from Gaussian distributions or
chosen as orthogonal while the remaining ones are designed to ensure full signal propagation. Both
consistently outperform the He initialization in our experiments on MNIST and CIFAR-10.

2 Signal propagation through Gaussian neural network ensembles

2.1 Background and notation

ij ∼ N(cid:16)

We study fully-connected neural network ensembles with zero mean Gaussian weights and biases.
We thus make the following assumption:
An ensemble {G}L Nl φ σw σb of fully-connected feed forward neural networks consists of networks
with depths L  widths Nl  l = 0  ...  L  independently normally distributed weights and biases with
  and non-decreasing activation function φ : R → R. Starting
w(l)
from the input vector x(0)  signal x(l) propagates through the network  as usual  as:
h(l) = W(l)x(l−1) + b(l) 

x(l) = φ

0  σ2
b l

  b(l)

(cid:17)

(cid:17)

0  σ2

w l

 

i ∼ N(cid:16)
(cid:16)
h(l)(cid:17)
(cid:17)
(cid:16)

Nl−1(cid:88)

x(l)
i = φ

h(l)
i

 

h(l)
i =

ij x(l−1)
w(l)

j

+ b(l)
i

 

j=1

  xi instead of x(l−1)

for l = 1  . . .   L  where h(l) is the pre-activation at layer l  W(l) is the weight matrix  and b(l) is the
bias vector. If not indicated otherwise  1-dimensional functions applied to vectors are applied to each
component separately. To ease notation  we follow the convention to suppress the superscript (l) and
write  for instance  xi instead of x(l)
  when the layer
i
reference is clear from the context.
Ideally  the initialized network is close to the trained one with high probability and can be reached
fast in a small number of training steps. Hence  our ﬁrst goal is to understand the ensemble above
and the trainability of an initialized network without requiring mean ﬁeld approximations of inﬁnite
Nl. In particular  we derive the probability distribution of the output x(L). Within this framework 
i = 0. Even though it preserves the variance for ReLUs  i.e.  φ(x) = max{0  x}  as activation

our second goal is to learn how to improve on the He initialization  i.e.  the choice σw l =(cid:112)2/Nl

  and xi instead of x(l+1)

and b(l)

i

i

2

functions [7]  neither this parameter choice nor orthogonal weights lead to dynamical isometry [14].
Thus  the average spectrum of the input-output Jacobian is not concentrated around 1 for higher
depths and inﬁnite width. In consequence  ReLUs are argued to be an inferior choice compared to
sigmoids [14]. Thus  our third goal is to provide an initialization scheme for ReLUs that overcomes
the resulting problems and provides dynamical isometry.
We start with our results about the signal propagation for general activation functions. The proofs
for all theorems are given in the supplementary material. As we show  the signal output distribution
depends on the input distribution only via scalar products of the inputs. Higher order terms do
not propagate through a network ensemble at initialization. In consequence  we can focus on the
distribution of such scalar products later on to derive meaningful criteria for the trainability of
initialized deep neural networks.

(cid:17)

0  σ2
w

j x2

j + σ2
b

2.2 General activation functions

j=1 wijxj + bi ∼ N(cid:16)

hi of the current layer is normally distributed as hi =(cid:80)Nl

Let’s ﬁrst assume that the signal x of the previous layer is given. Then  each pre-activation component
 
since the weights and bias are independently normally distributed with zero mean. The non-linear
monotonically increasing transformation xi = φ(hi) is distributed as xi ∼ Φ
  where φ−1
denotes the generalized inverse of φ  i.e. φ−1(x) := inf{y ∈ R | φ(y) ≥ x}  Φ the cumulative
distribution function (cdf) of a standard normal random variable  and σ2 = σ2
b . Thus  we
only need to know the distribution of |x|2 as input to compute the distribution of xi. The signal
propagation is thus reduced to a 1-dimensional problem. Note that the assumption of equal σ2
w for all
j + σ2
w jx2
b i
j  which depends on the parameters

incoming edges into a neuron are crucial for this result. Otherwise  hi ∼ N(cid:16)
would require the knowledge of the distribution of(cid:80)
Proposition 1. Let the probability density p0(z) of the squared input norm |x(0)|2 =(cid:80)N0

(cid:80)
(cid:16) φ−1(·)
(cid:17)
w|x|2 + σ2

w however  we can compute the probability distribution of outputs.

w j. Based on σ2
σ2

(cid:17)
(cid:17)2

0 (cid:80)

be known. Then  the distribution pl(z) of the squared signal norm |x(l)|2 depends only on the
distribution of the previous layer pl−1(z) as transformed by a linear operator Tl : L1(R+) →
L1(R+) so that pl = Tl(pl−1). Tl is deﬁned as

w j = σ2

j σ2

w jx2

x(0)
i

i=1

(cid:16)

j σ2

σ

(cid:90) ∞

0

Tl(p)[z] =

kl(y  z)p(y) dy 

(1)

where k(y  z) is the distribution of the squared signal z at layer l given the squared signal at
∗Nl−1
φ(hy)2(z)  where ∗ stands for convolution and pφ(hy)2 (z)
(cid:1). This distribution serves to compute the cumulative distribution function
the previous layer y so that kl(y  z) = p
denotes the distribution of the squared transformed pre-activation hy  which is normally distributed

as hy ∼ N(cid:0)0  σ2

wy2 + σ2
b

(cdf) of each signal component x(l)

i as

(cid:90) ∞

0

(cid:33)

(cid:32)

(cid:112)σ2

φ−1(x)
wz + σ2
b

Fx(l)

i

(x) =

dz pl−1(z)Φ

 

(2)

where φ−1 denotes the generalized inverse of φ and Φ the cdf of a standard normal random variable.
Accordingly  the components are jointly distributed as

(cid:90) ∞
where we use the abbreviation σz =(cid:112)σ2
with f  i.e.  by induction  f∗N (z) = f ∗ f∗(N−1)(z) =(cid:82) z

b .
wz + σ2

1  ... x(l)
Nl

Fx(l)

(x) =

dz pl−1(z)ΠNl

0

(cid:18) φ−1(xi)

(cid:19)

i=1Φ

σz

As common  the N-fold convolution of a function f ∈ L1(R+) is deﬁned as repeated convolution
0 f (x)f∗(N−1)(z − x) dx. In Prop. 1  we
note the radial symmetry of the output distribution. It only depends on the squared norm of the input.
For a single input x(0)  p0(z) is given by the indicator function p0(z) = 1|x(0)|2(z). Interestingly 

 

(3)

3

(a) Squared signal norm distribution at different
depths for Nl = 200. The initial distribution (L =
0) is deﬁned by MNIST.

tion parameters σw =(cid:112)2/Nl  σb = 0.

(b) Eigenvalues λ corresponding to eigenfunctions
ym of Tl. Nl = 10 (black circles)  Nl = 20 (blue
triangles)  Nl = 100 (red +).

Figure 1: Layer-wise transition of the squared signal norm distribution for ReLUs with He initializa-

distribution. px(L) =(cid:81)L
linear operator(cid:81)L

mean ﬁeld analysis also focuses on the average or the squared signal  which is likewise updated
layer-wise. Prop. 1 explains and justiﬁes the focus of mean ﬁeld theory on the squared signal norm.
More information is not transmitted from layer to layer to determine the state (distribution) of a single
neuron. The difference to mean ﬁeld theory here is that we regard the full distribution pl−1 of the
previous layer instead of its average only on inﬁnitely large layers. The linear operator Tl governs this
l=1 Tlpx(0)  where the product is deﬁned by function composition. Hence  the
l=1 Tl can also be interpreted as the Jacobian corresponding to the (linear) function
that maps the squared input norm distribution to the squared output norm distribution. Tl is different
from the signal input output Jacobian studied in mean ﬁeld random matrix theory  yet  its spectral
properties can also inform us about the trainability of the network ensemble. Conveniently  we only
have to study one spectrum and not a distribution of eigenvalues that are potentially coupled as in
random matrix theory. For any nonlinear activation function  Tl can be approximated numerically on
an equidistant grid. The convolution in the kernel deﬁnition can be computed efﬁciently with the
help of Fast Fourier Transformations. The eigenvalues of the matrix approximating Tl deﬁne the
approximate signal propagation along the eigendirections.
However  we only receive the full picture when we extend our study to look at the joint output
distribution  i.e.  the outputs corresponding to different inputs.
Proposition 2. The same component of pre-activations of signals h1  ...  hD corresponding to
different inputs x(0)
D   are jointly normally distributed with zero mean and covariance matrix
V deﬁned by

1   ...  x(0)

w(cid:104)xi  xj(cid:105) + σ2

b

vij = Cov(hi  hj) = σ2

(4)

for i  j = 1  ...  D conditional on the signals xi of the previous layer corresponding to x(0)
denotes the number of data points.

i

  where D

After non-linear activation  the signals are not jointly normally distributed anymore. But their
distribution is a function of the squared norms and scalar products between signals of the previous
layer only. Thus  it is sufﬁcient to propagate the joint distribution of three variables that can
attain different values  i.e.  |x1|2  |x2|2   (cid:104)x1  x2(cid:105)  through the layers to determine the joint output
distribution of two signals x1 and x1 corresponding to different inputs. No other information about the
joint distribution of inputs  e.g.  higher moments  can inﬂuence the ensemble output distribution and
thus our choice of weight and bias parameters. In consequence  the focus on quantities corresponding
to the above in mean ﬁeld theory is justiﬁed for Gaussian parameter initialization and does not require
any approximation. Yet  the mean ﬁeld assumption that pre-activation signals are exactly normally
distributed and not only conditional on the previous signal is approximate. Accordingly  the output
distribution for ﬁnite neural networks does not follow a Gaussian process with average covariance
matrix V as in mean ﬁeld theory [11]. V follows a probability distribution that is determined by

4

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.0000.0050.0100.0150.02005101520|x|2PLllllllllll0123456789lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.5−50−40−30−20−100mlthe previous layers. For the initialization scheme for ReLUs that we propose later  we can state
the distribution of V explicitly. First however  we analyze ReLUs in the standard framework and
specialize the above theorems.

2.3 Rectiﬁed Linear Units (ReLUs)

The minimum initialization criterion to avoid vanishing or exploding gradients is to preserve the
expected squared signal norm. For ﬁnite networks  this is given as follows.
Corollary 3. For ReLUs  the expectation value of the squared signal conditional on the squared
signal of the previous layer is given by:

E(cid:16)|x(l)|2(cid:12)(cid:12) |x(l−1)|2 = y

(cid:17)

= (σ2

w ly + σ2

b l)

Nl
2

.

Consequently  the expectation of the ﬁnal squared signal norm depends on the initial input as:

(5)

(6)

E(cid:16)|x(L)|2(cid:12)(cid:12)|x(0)|2(cid:17)

=|x(0)|2ΠL

l=1

w l

Nlσ2
2

+ σ2

b L

NL
2

+

σ2
b l

Nl
2

ΠL

n=l+1

w n

Nnσ2
2

L−1(cid:88)

l=1

Similar relations for the expected signal components and their variance follow from Eq. (6) and are
covered in the supplementary material. [6] has derived a simpler version of Eq. (6) for equal σw l and
Nl across layers.
A straightforward way to preserve the average squared signal or the squared output signal norm

distribution is exactly the He initialization σb l = 0 and σw l =(cid:112)2/Nl [7]  which is also conﬁrmed

l=1Nlσ2

b l = 0. We only need to fulﬁll
w l ≈ 1. In case that we normalize the input so that |x(0)|2 = 1 

joint distribution of the variables(cid:0)|x1|2 |x2|2 (cid:104)x1  x2(cid:105)(cid:1) given(cid:0)|x1|2 |x2|2 (cid:104)x1  x2(cid:105)(cid:1). As this is

by mean ﬁeld analysis. Yet  we have many more choices even when σ2
one condition  i.e.  0.5LΠL
b l (cid:54)= 0 is also a valid option and we have 2L − 1 degrees of freedom.
σ2
There remains the question whether there exist further criteria to be fulﬁlled that improve the
trainability of the initial network ensemble. The whole output distribution could provide those and its
derivation is given in the supplementary material. According to Prop. 2  it is guided by the layer-wise
computationally intensive to obtain  we focus on marginals  i.e.  the distributions of |x|2 and (cid:104)x1  x2(cid:105).
These are sufﬁcient to highlight several drawbacks of the initialization approach and provide us with
insights to propose an alternative that overcomes these shortcomings.
First  we focus on |x(l)|2 and derive a closed form solution for the integral kernel kl(y  z) of Tl
in Prop. 1 and analyse some of its spectral properties for ReLUs. This allows us to reason about
the shape of the stationary distribution of Tl  i.e.  the limit output distribution for networks with
increasing depth.
Proposition 4. For ReLUs  the linear operator Tl in Prop. 1 is deﬁned by

(cid:32)

Nl(cid:88)

(cid:18)Nl

(cid:19) 1

k

σ2
y

k=1

pχ2

k

(cid:19)(cid:33)

(cid:18) z

σ2
y

kl(y  z) = 0.5Nl

δ0(z) +

wy + σ2

b   where δ0(z) denotes the δ-distribution peaked at 0 and pχ2

the density of
the χ2 distribution with k degrees of freedom. For σb = 0  the functions fm(y) = ym1]0 ∞](y) are
eigenfunctions of Tl for any m ∈ R (even though they are not elements of L1(R+) and thus not
normalizable as probability measures) with corresponding eigenvalue λl m ∈ R: Tlfm = λl mfm
with

k

λl m = 0.5Nl−m−1

1

Nl(cid:88)

(cid:18)Nl

(cid:19) Γ(k/2 − m − 1)

σ2m+2
w

k=1

k

Γ(k/2)

Note that  for σb = 0  the eigenfunctions ym cannot be normalized on R+  as the antiderivative
diverges at zero for m ≤ −1. However  if we discretize Tl in numerical experiments they can
be normalized and the real eigenvectors representing probability distributions attain shapes ≈ ym.

5

with σy =(cid:112)σ2

(7)

(8)

Speciﬁcally  for the He values σb l = 0 and σw l = (cid:112)2/Nl  numerical experiments reveal a

Fig. 1a provides an example of the output distribution for 9 layers each consisting of Nl = 200
neurons with He initialization parameters. The average squared signal is indeed preserved but
becomes more right tailed for deeper layers. Fig. 1b shows the corresponding eigenvalues of Tl
as in Prop. 4. In summary  we observe a window mcrit < m ≤ −1 with eigenvalues λl m < 1.
relation mcrit ≈ −3.2559793 − 1.6207083Nl. Signal parts within this window are damped down in
deeper layers  while the remaining parts explode. Only ymcrit is preserved through the layers and
depends on the choice of σw l. Interestingly  for m = −1  λl m is independent of σw l and given by
λl −1 = 1 − 0.5Nl. Thus  it approaches λl −1 = 1 for increasing Nl. For the He initialization  ymcrit
converges to the δ0(y) for increasing Nl. In contrast to mean ﬁeld analysis  not the whole space of
eigenfunctions corresponds to eigenvalue 1 for the He initialization. In particular  eigenvalues bigger
than one exist that can be problematic for exploding gradients. To reduce their number  broader layers
promise better protection as well as smaller values of σw l. Ultimately  we care about the product of
layer-wise eigenvalues  i.e.  the eigenvalues of ΠlTl. Again  setting these to 1 imposes a constraint
w l like in Cor. 3. Hence  we gain no additional constraint on our initial
only on the product Πlσ2
parameters and have no means to prevent eigenvalues larger than 1.
The biggest challenge for trainability  however  is the ability to differentiate similar signals. We
therefore study the evolution of the cosine similarity (cid:104)x(l)
2 or the
unnormalized scalar product through layers l.
Theorem 5. For ReLUs  let x1 = φ(h1)  x2 = φ(h2) be the same signal component  i.e.  neuron 
where each corresponds to a different input x(0)
v11v22 of the
pre-activations h1  h2 be given  where V denotes the 2-dimensional covariance matrix as deﬁned in
Prop. 2. Then  the correlation after non-linear activation is

√
2 . Let the correlation ρ = v12/

2 (cid:105) of two inputs x(0)

1 or x(0)

and x(0)

1   x(l)

1

(cid:112)1 − ρ2 − 1 + 2πρg(ρ)
(cid:19)

π − 1

Cor (x1  x2) =

(cid:18)

(cid:82) ∞

g(ρ) is deﬁned as g(ρ) = 1√
2π
g(1) = 0.5. The average of the sum of all components E ((cid:104)x1  x2(cid:105)) conditional on the previous layer
is:

0 Φ

u

ρ√
1−ρ2

(cid:32)

√

E ((cid:104)x1  x2(cid:105) | ρ) = Nl

v11v22

g(ρ)ρ +

.

(9)

exp(cid:0)− 1
2 u2(cid:1) du for |ρ| (cid:54)= 1 and g(−1) = 0 
(cid:33)
(cid:112)1 − ρ2
(cid:16) v12y

(cid:16)√

≈ Nl

(ρ + 1).

v11v22

(cid:17)

(cid:17)

(10)

√

1
4

2π

b

2π

(σ2

K0

√

exp

2 det(V )

(cid:113)

b)(σ2

1
det(V )

v11v22
det(V ) y

w|x1|2+σ2

(cid:82) ∞

w|x2|2+σ2
b)

w(cid:104)x1 x2(cid:105)+σ2
σ2

. Interestingly  for σb = 0  ρ =

∗Nl
prod(t) 
and K0(w) =

Furthermore  conditional on the signals of the previous layer  (cid:104)x1  x2(cid:105) is distributed as f
where fprod(y) = (1 − g(ρ)) δ0(y) +
0 cos(w sinh(t)) dt denotes the modiﬁed Bessel function of second kind.
Note that [2] studies a similar integral but in the mean ﬁeld limit. The correlation of the signal
components only depends on ρ (and is always smaller than ρ). Analogous to the c-map in mean ﬁeld
approaches [18]  the actual quantity of interest would be the distribution of the correlation ρ  i.e. 
(cid:104)x1 x2(cid:105)
|x1||x2| coincides with the cosine
ρ =
similarity of the two signals. The preservation of this quantity on average has been shown to be the
most indicative criterion for trainability of ReLU residual networks [24]. We therefore take a closer
look at its distribution. To save computational time and space  we sample Nl components iid from a 2
dimensional normal distribution as introduced in Prop. 2 and transform the components by ReLUs to
obtain two vectors x1 and x2 and calculate their cosine similarity. Repeating this procedure 106 times
results in Fig. 2. First  we note that correlations can only be positive after the ﬁrst layer  since all
signal components are positive (or zero) after transformation by ReLUs. Negative cosine similarities
cannot be propagated through Gaussian ReLU ensembles. Data transformation to obtain positive
inputs can mitigate this issue. Yet  Eq. (10) highlights an unavoidable problem for deep models  i.e. 
the average cosine similarity increases from layer to layer until it reaches 1 at high depths. Then  all
signals become parallel and thus indistinguishable.
While this effect cannot be mitigated completely within our initialization scheme  a slightly smaller
choice of σw than the He initialization reduces the average cosine distance and a smaller number

6

of neurons in one layer increases the variance of the cosine distance  as shown in Fig. 2. A higher
variance increases the probability that smaller values of the cosine distance can be propagated.
We hypothesize that this effect contributes to the better trainability of ReLU neural networks with
DropOut or DropConnect [22]  since both reduce the effective number of neurons Nl. Yet  a smaller
σw and DropOut (or DropConnect) introduce a risk of vanishing gradients in deep neural networks
[17]. An adjustment of σw by the DropOut rate to avoid this effect [17] would also destroy possible
beneﬁcial effects on the cosine similarity.

For smaller σ2 = 1/N  [1] observes a
phenomenon related to the cosine sim-
ilarity  i.e. shattered gradients. How-
ever  in this setting  the effect of van-
ishing gradients and increasing corre-
lations are indistinguishable. In fact 
the authors observe decreasing corre-
lations  while we highlight the prob-
lem of increasing ones for the He ini-
tialization.
Interestingly  in the He
case (σ2 = 2/N)  [1] ﬁnds that also
batch normalization cannot provide
better trainability. For “typical” inputs
that are shown to be common in net-
works with batch normalization (but
not in networks without  which we
study here)  the covariance between
outputs corresponding to different in-
puts decays exponentially.

Figure 2: Probability distribution of the cosine similarity
conditional on the previous layer with |x1| = |x2| = 1 and
(cid:104)x1  x2(cid:105) equals 0 (circles)  0.25 (squares)  0.5 (diamonds) 
and 0.75 (triangles) for Nl = 100 (dashed lines and ﬁlled
symbols) and Nl = 200 (lines and unﬁlled symbols) neurons.
We therefore discuss an alternative solution that [1] proposes also for convolutional and residual
neural networks and has ﬁrst been introduced by [21].

3 Alternative initialization of fully-connected ReLU deep neural networks

The issues of training deep neural networks with ReLUs are caused by the fact that negative signal
can never propagate through the network and a neuron’s state is zero in half of the cases. Hence  we
discuss an initialization  where the full signal is transmitted. We set the bias vector b(l)
i = 0 and the
weight matrices W (l) ∈ RNl−1×Nl are initially determined by a submatrix W (l)

2 × Nl

2 as

0 ∈ R Nl−1

(cid:34)

W (l) =

W (l)
0
−W (l)

0

−W (l)
0
W (l)
0

(cid:35)

.

i =(cid:80)

j

i

j

> 0  we have x(l)

ij x(l−1)

j w(l)

i =(cid:80)Nl/2

i = h(l)
j=1 w(l)

and x(l)
0 ijh(l−1)

Regardless of the choice of W (l)
0   we receive a signal vector x(l)  where half of the entries correspond
to the positive part of the pre-activations and the second half to the negative part  i.e.  if i ≤ Nl/2
and h(l)
i+Nl/2 = 0 or the other way around for
h(l)
i < 0. This way  effectively h(l)
is propagated so that we have initially
l=0 W (l)
0 h0. Note that we still have to train the full Nl−1Nl parameters
of W (l) and can learn non-linear functions. [21] observed that convolutional neural networks even
trained the ﬁrst layers to resemble linear neural networks  which inspired this choice of initialization.
0 ij ∼
In this setting  we have several good choices of W (l)
In this
case  our assumptions from the previous sections are met and we can repeat the analysis for networks
of width Nl/2 and φ(h) = h  i.e. set the activation function to the identity. The same parameter
choice as in Cor. 3  e.g.  σ2
w l = 2/Nl  preserves the variance and now also the cosine distance
between signals corresponding to different inputs. The analysis is rather straight-forward and the
follows a product

linear networks h(L) =(cid:81)L
N(cid:16)
output distribution is deﬁned by the distribution of(cid:81)L

as before. We call this variant Gaussian submatrix (GSM) initialization.

0 . First  we assume iid entries w(l)

0 x(0). (cid:81)L

l=0 W (l)

l=0 W (l)

0

(cid:17)

0  σ2

w l

Wishart distribution with known spectrum [13  14].

7

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.000.030.060.090.120.000.250.500.751.00cosine similarityPAccording to [14] however  dynamical isometry leads to better training results and speed. This
demands an input-output Jacobian close to the identity or a spectrum of the signal propagation
operator Tl that is highly concentrated around 1. Previously  this was believed to be better achievable
by tanh or sigmoid rather than by ReLU as choice of activation functions [14]. Yet  in the parameter
sharing setting above  perfect dynamical isometry for h can be achieved by orthogonal W (l)
0   i.e.  it is
drawn uniformly at random from all matrices W fulﬁlling W T W = σ2
w l = 1. This is
our second initialization proposal in addition to GSM.
Alternatively  [25] recommends to shift the signal h(l)
to enable negative
signal to pass through a ReLU activation instead of the proposed parameter sharing solution. We also
considered a similar approach but decided for the proposed one as it is point-symmetric  guaranties
therefore perfect dynamical isometry  is computational less intensive  as it does not need to compute
a data dependent bias b(l)
as in batch normalization  and is more convenient for theoretical analysis 
i
which can rely on a rich literature on linear deep neural networks.

i by a non-zero bias b(l)

w lI with σ2

i

4 Experiments for different initialization schemes

initialization with parameters σb = 0  σw =(cid:112)2/N for He and GSM initialization  and σw = 1 for

Figure 3: Classiﬁcation test accuracy on MNIST for different widths N  depths L  and weight

orthogonal W0 after 104 SGD steps. We report the average of 100 realizations and the corresponding
0.95 conﬁdence interval. The right plot is a section of the left. Note that the legends apply to both
plots.

We train fully-connected ReLU feed forward networks of different depth consisting of L = 1  . . .   10
hidden layers with the same number of neurons Nl = N = 100  300  500 and an additional softmax
classiﬁcation layer on MNIST [10] and CIFAR-10 [9] to compare three different initialization
schemes: the standard He initialization and our two proposals in Sec. 3  i.e.  GSM and orthogonal
weights. We focus on minimizing the cross-entropy by Stochastic Gradient Descent (SGD) without
batch normalization or any data augmentation techniques. Hence  our goal is not to outperform the
classiﬁcation state of the art but to compare the initialization schemes under similar realistic conditions.
Since deep networks normally require a smaller learning rate than the ones with a small number of
hidden layers  as in Ref. [14]  we adapt the learning rate to (0.0001 + 0.003 · exp(−step/104))/L
for MNIST and (0.00001 + 0.0005 · exp(−step/104))/L for CIFAR-10 for 104 SGD steps with a
batch size of 100 in all cases. To reduce the number of parameters and speed up computations  we
clipped original CIFAR-10 images to 28 × 28 size. For each conﬁguration  we train 100 instances
on MNIST and 30 instances on CIFAR-10 and report the average accuracy with a 0.95 conﬁdence
interval in Fig. 3 and Fig. 4 respectively. Each experiment on MNIST was run on 1 Nvidia GTX 1080
Ti GPU  while each experiment on CIFAR-10 was performed on 4 Nvidia GTX 1080 Ti GPUs.
Note that the accuracy on CIFAR-10 is lower than for convolutional architectures  as we restrict
ourselves to deep fully-connected networks to focus on their trainability. [1] shows that a similar

8

initialization with parameters σb = 0  σw =(cid:112)2/N for He and GSM initialization  and σw = 1 for

Figure 4: Classiﬁcation test accuracy on CIFAR-10 for different widths N  depths L  and weight

orthogonal W0 after 104 SGD steps. We report the average of 30 realizations and the corresponding
0.95 conﬁdence interval. The right plot is a section of the left.

orthogonal initialization improves training results also for convolutional and residual neural networks.
As suggested by our theoretical analysis  both proposed initialization schemes consistently outperform
the He initialization and show stable training results  in particular  for deeper network architectures 
where the He initialized networks decrease in accuracy. GSM and orthogonal W0 both perform better
for higher width N  while orthogonal W0 seems to be the most reliable choice.

5 Discussion

We have introduced a framework for the analysis of deep fully-connected feed forward neural
networks at initialization with zero mean normally distributed weights and biases. It is exact  does
not rely on mean ﬁeld approximations  provides distribution information of output and joint output
signals  and applies to networks with arbitrary layer widths. It has led to the insight that only the
scalar products between inputs determine the shape of the output distribution  but it is not inﬂuenced
by higher interaction terms.
Hence  for ReLUs  we have analysed the propagation of these quantities through the deep neural
network ensemble. While mean ﬁeld analysis provides only the He initialization for good training
results  we have extended the number of possible parameter choices that avoid vanishing or exploding
gradients. However  no parameter choice can avoid the tendency that signals become more aligned
with increasing depth. Deep ReLU Gaussian neural network ensembles cannot distinguish different
input correlations and are therefore not well trainable without batch normalization. Even batch
normalization does not guaranty the transmission of correlations between different inputs.
As solution to this problem  we have discussed an alternative but simple initialization scheme that
relies on initial parameter sharing. One variant guarantees perfect dynamical isometry. Experiments
on MNIST and CIFAR-10 demonstrate that deeper fully-connected ReLU networks can become
better trainable in the proposed way than by the standard approach.

Acknowledgments

We would like to thank Joachim M. Buhmann and Alkis Gotovos for their valuable feedback on the
manuscript and the reviewers for their insightful comments. This work was partially funded by the
Swiss Heart Failure Network  PHRT122/2018DRI14 (J. M. Buhmann  PI). RB was supported by a
grant from the US National Cancer Institute (1R35CA220523).

9

References
[1] David Balduzzi  Marcus Frean  Lennox Leary  J. P. Lewis  Kurt Wan-Duo Ma  and Brian McWilliams. The
shattered gradients problem: If resnets are the answer  then what is the question? In Doina Precup and
Yee Whye Teh  editors  Proceedings of the 34th International Conference on Machine Learning  volume 70
of Proceedings of Machine Learning Research  pages 342–350  International Convention Centre  Sydney 
Australia  06–11 Aug 2017. PMLR.

[2] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Y. Bengio  D. Schuurmans 
J. D. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in Neural Information Processing
Systems 22  pages 342–350. Curran Associates  Inc.  2009.

[3] Simon S. Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes

over-parameterized neural networks. In International Conference on Learning Representations  2019.

[4] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and
networks.
Statistics  AISTATS 2010  Chia Laguna Resort  Sardinia  Italy  May 13-15  2010  pages 249–256  2010.

[5] Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In S. Bengio 
H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural
Information Processing Systems 31  pages 582–591. Curran Associates  Inc.  2018.

[6] Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. In
S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in
Neural Information Processing Systems 31  pages 571–581. Curran Associates  Inc.  2018.

[7] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the 2015 IEEE International
Conference on Computer Vision (ICCV)  ICCV ’15  pages 1026–1034  Washington  DC  USA  2015. IEEE
Computer Society.

[8] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Francis Bach and David Blei  editors  Proceedings of the 32nd International
Conference on Machine Learning  volume 37 of Proceedings of Machine Learning Research  pages
448–456  Lille  France  07–09 Jul 2015. PMLR.

[9] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).

Technical report  2009.

[10] Yann Lecun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. In Proceedings of the IEEE  pages 2278–2324  1998.

[11] Jaehoon Lee  Jascha Sohl-dickstein  Jeffrey Pennington  Roman Novak  Sam Schoenholz  and Yasaman
Bahri. Deep neural networks as gaussian processes. In International Conference on Learning Representa-
tions  2018.

[12] Dmytro Mishkin and Jiri Matas. All you need is a good init. In 4th International Conference on Learning
Representations  ICLR 2016  San Juan  Puerto Rico  May 2-4  2016  Conference Track Proceedings  2016.

[13] Thorsten Neuschel. Plancherel-rotach formulae for average characteristic polynomials of products of
products of ginibre random matrices and the fuss-catalan distribution. Random Matrices: Theory and
Applications  3(1)  2014.

[14] Jeffrey Pennington  Samuel S. Schoenholz  and Surya Ganguli. Resurrecting the sigmoid in deep learning
through dynamical isometry: theory and practice. In Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems 2017  4-9 December 2017  Long Beach 
CA  USA  pages 4788–4798  2017.

[15] Jeffrey Pennington  Samuel S. Schoenholz  and Surya Ganguli. The emergence of spectral universality in
deep networks. In International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2018  9-11
April 2018  Playa Blanca  Lanzarote  Canary Islands  Spain  pages 1924–1932  2018.

[16] Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha Sohl-Dickstein  and Surya Ganguli. Exponential
expressivity in deep neural networks through transient chaos. In D. D. Lee  M. Sugiyama  U. V. Luxburg 
I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages 3360–3368.
Curran Associates  Inc.  2016.

10

[17] Arnu Pretorius  Elan van Biljon  Steve Kroon  and Herman Kamper. Critical initialisation for deep signal
propagation in noisy rectiﬁer neural networks. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman 
N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages
5717–5726. Curran Associates  Inc.  2018.

[18] Maithra Raghu  Ben Poole  Jon M. Kleinberg  Surya Ganguli  and Jascha Sohl-Dickstein. On the expressive
power of deep neural networks. In Proceedings of the 34th International Conference on Machine Learning 
ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages 2847–2854  2017.

[19] Andrew M. Saxe  James L. McClelland  and Surya Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. In 2nd International Conference on Learning Representations 
ICLR 2014  Banff  AB  Canada  April 14-16  2014  Conference Track Proceedings  2014.

[20] Samuel S. Schoenholz  Justin Gilmer  Surya Ganguli  and Jascha Sohl-Dickstein. Deep information
propagation. In 5th International Conference on Learning Representations  ICLR 2017  Toulon  France 
April 24-26  2017  Conference Track Proceedings  2017.

[21] Wenling Shang  Kihyuk Sohn  Diogo Almeida  and Honglak Lee. Understanding and improving convolu-
tional neural networks via concatenated rectiﬁed linear units. In Proceedings of the 33rd International
Conference on International Conference on Machine Learning - Volume 48  ICML’16  pages 2217–2225.
JMLR.org  2016.

[22] Li Wan  Matthew Zeiler  Sixin Zhang  Yann Le Cun  and Rob Fergus. Regularization of neural networks
using dropconnect. In Sanjoy Dasgupta and David McAllester  editors  Proceedings of the 30th Interna-
tional Conference on Machine Learning  volume 28 of Proceedings of Machine Learning Research  pages
1058–1066  Atlanta  Georgia  USA  17–19 Jun 2013. PMLR.

[23] Lechao Xiao  Yasaman Bahri  Jascha Sohl-Dickstein  Samuel Schoenholz  and Jeffrey Pennington. Dy-
namical isometry and a mean ﬁeld theory of CNNs: How to train 10 000-layer vanilla convolutional neural
networks. In Jennifer Dy and Andreas Krause  editors  Proceedings of the 35th International Conference
on Machine Learning  volume 80 of Proceedings of Machine Learning Research  pages 5393–5402 
Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.

[24] Ge Yang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In I. Guyon  U. V.
Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural
Information Processing Systems 30  pages 7103–7114. Curran Associates  Inc.  2017.

[25] Greg Yang  Jeffrey Pennington  Vinay Rao  Jascha Sohl-Dickstein  and Samuel S. Schoenholz. A mean

ﬁeld theory of batch normalization. In International Conference on Learning Representations  2019.

[26] Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod Lipson. How transferable are features in deep neural
networks? In Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors 
Advances in Neural Information Processing Systems 27  pages 3320–3328. Curran Associates  Inc.  2014.

11

,Jacob Gardner
Gustavo Malkomes
Roman Garnett
Kilian Weinberger
Dennis Barbour
John Cunningham
Rebekka Burkholz
Alina Dubatovka