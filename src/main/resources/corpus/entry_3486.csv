2017,Mixture-Rank Matrix Approximation for Collaborative Filtering,Low-rank matrix approximation (LRMA) methods have achieved excellent accuracy among today's collaborative filtering (CF) methods. In existing LRMA methods  the rank of user/item feature matrices is typically fixed  i.e.  the same rank is adopted to describe all users/items. However  our studies show that submatrices with different ranks could coexist in the same user-item rating matrix  so that approximations with fixed ranks cannot perfectly describe the internal structures of the rating matrix  therefore leading to inferior recommendation accuracy. In this paper  a mixture-rank matrix approximation (MRMA) method is proposed  in which user-item ratings can be characterized by a mixture of LRMA models with different ranks. Meanwhile  a learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to MRMA. Experimental studies on MovieLens and Netflix datasets demonstrate that MRMA can outperform six state-of-the-art LRMA-based CF methods in terms of recommendation accuracy.,Mixture-Rank Matrix Approximation

for Collaborative Filtering

Dongsheng Li1 Chao Chen1 Wei Liu2∗ Tun Lu3 4 Ning Gu3 4

1IBM Research - China
2Tencent AI Lab  China

Stephen M. Chu1

3School of Computer Science  Fudan University  China

4Shanghai Key Laboratory of Data Science  Fudan University  China

{ldsli  cshchen  schu}@cn.ibm.com  wliu@ee.columbia.edu  {lutun  ninggu}@fudan.edu.cn

Abstract

Low-rank matrix approximation (LRMA) methods have achieved excellent ac-
curacy among today’s collaborative ﬁltering (CF) methods. In existing LRMA
methods  the rank of user/item feature matrices is typically ﬁxed  i.e.  the same rank
is adopted to describe all users/items. However  our studies show that submatrices
with different ranks could coexist in the same user-item rating matrix  so that
approximations with ﬁxed ranks cannot perfectly describe the internal structures
of the rating matrix  therefore leading to inferior recommendation accuracy. In
this paper  a mixture-rank matrix approximation (MRMA) method is proposed  in
which user-item ratings can be characterized by a mixture of LRMA models with
different ranks. Meanwhile  a learning algorithm capitalizing on iterated condition
modes is proposed to tackle the non-convex optimization problem pertaining to
MRMA. Experimental studies on MovieLens and Netﬂix datasets demonstrate that
MRMA can outperform six state-of-the-art LRMA-based CF methods in terms of
recommendation accuracy.

1

Introduction

T .

Low-rank matrix approximation (LRMA) is one of the most popular methods in today’s collaborative
ﬁltering (CF) methods due to high accuracy [11  12  13  17]. Given a targeted user-item rating matrix
R ∈ Rm×n  the general goal of LRMA is to ﬁnd two rank-k matrices U ∈ Rm×k and V ∈ Rn×k
such that R ≈ ˆR = U V T . After obtaining the user and item feature matrices  the recommendation
score of the i-th user on the j-th item can be obtained by the dot product between their corresponding
feature vectors  i.e.  UiVj
In existing LRMA methods [12  13  17]  the rank k is considered ﬁxed  i.e.  the same rank is adopted
to describe all users and items. However  in many real-world user-item rating matrices  e.g.  Movielens
and Netﬂix  users/items have a signiﬁcantly varying number of ratings  so that submatrices with
different ranks could coexist. For instance  a submatrix containing users and items with few ratings
should be of a low rank  e.g.  10 or 20  and a submatrix containing users and items with many ratings
may be of a relatively higher rank  e.g.  50 or 100. Adopting a ﬁxed rank for all users and items
cannot perfectly model the internal structures of the rating matrix  which will lead to imperfect
approximations as well as degraded recommendation accuracy.
In this paper  we propose a mixture-rank matrix approximation (MRMA) method  in which user-item
ratings are represented by a mixture of LRMA models with different ranks. For each user/item  a
probability distribution with a Laplacian prior is exploited to describe its relationship with different

∗This work was conducted while the author was with IBM.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

LRMA models  while a joint distribution of user-item pairs is employed to describe the relationship
between the user-item ratings and different LRMA models. To cope with the non-convex optimization
problem associated with MRMA  a learning algorithm capitalizing on iterated condition modes
(ICM) [1] is proposed  which can obtain a local maximum of the joint probability by iteratively
maximizing the probability of each variable conditioned on the rest. Finally  we evaluate the proposed
MRMA method on Movielens and Netﬂix datasets. The experimental results show that MRMA can
achieve better accuracy compared against state-of-the-art LRMA-based CF methods  further boosting
the performance for recommender systems leveraging matrix approximation.

2 Related Work

Low-rank matrix approximation methods have been leveraged by much recent work to achieve
accurate collaborative ﬁltering  e.g.  PMF [17]  BPMF [16]  APG [19]  GSMF [20]  SMA [13] 
etc. These methods train one user feature matrix and one item feature matrix ﬁrst and use these
feature matrices for all users and items without any adaptation. However  all these methods adopt
ﬁxed rank values for the targeted user-item rating matrices. Therefore  as analyzed in this paper 
submatrices with different ranks could coexist in the rating matrices and only adopting a ﬁxed rank
cannot achieve optimal matrix approximation. Besides stand-alone matrix approximation methods 
ensemble methods  e.g.  DFC [15]  LLORMA [12]  WEMAREC [5]  etc.  and mixture models  e.g. 
MPMA [4]  etc.  have been proposed to improve the recommendation accuracy and/or scalability
by weighing different base models across different users/items. However  the above methods do not
consider using different ranks to derive different base models. In addition  it is desirable to borrow
the idea of mixture-rank matrix approximation (MRMA) to generate more accurate base models in
the above methods and further enhance their accuracy.
In many matrix approximation-based collaborative ﬁltering methods  auxiliary information  e.g. 
implicit feedback [9]  social information [14]  contextual information [10]  etc.  is introduced to
improve the recommendation quality of pure matrix approximation methods. The idea of MRMA is
orthogonal to these methods  and can thus be employed by these methods to further improve their
recommendation accuracy. In general low-rank matrix approximation methods  it is non-trivial to
directly determine the maximum rank of a targeted matrix [2  3]. Candès et al. [3] proved that a
non-convex rank minimization problem can be equivalently transformed into a convex nuclear norm
minimization problem. Based on this ﬁnding  we can easily determine the range of ranks for MRMA
and choose different K values (the maximum rank in MRMA) for different datasets.

3 Problem Formulation

In this paper  upper case letters such as R  U  V denote matrices  and k denotes the rank for matrix
approximation. For a targeted user-item rating matrix R ∈ Rm×n  m denotes the number of users  n
denotes the number of items  and Ri j denotes the rating of the i-th user on the j-th item. ˆR denotes
the low-rank approximation of R. The general goal of k-rank matrix approximation is to determine
user and item feature matrices  i.e.  U ∈ Rm×k  V ∈ Rn×k  such that R ≈ ˆR = U V T . The rank k is
considered low  because k (cid:28) min{m  n} can achieve good performance in many CF applications.
In real-world rating matrices  e.g.  Movielens and Netﬂix  users/items have a varying number of
ratings  so that a lower rank which best describes users/items with less ratings will easily underﬁt the
users/items with more ratings  and similarly a higher rank will easily overﬁt the users/items with less
ratings. A case study is conducted on the Movielens (1M) dataset (with 1M ratings from 6 000 users
on 4 000 movies)  which conﬁrms that internal submatrices with different ranks indeed coexist in the
rating matrix. Here  we run the probabilistic matrix factorization (PMF) method [17] using k = 5
and k = 50  and then compare the root mean square errors (RMSEs) for the users/items with less
than 10 ratings and more than 50 ratings.
As shown in Table 1  when the rank is 5  the users/items with less than 10 ratings achieve lower
RMSEs than the cases when the rank is 50. This indicates that the PMF model overﬁts the users/items
with less than 10 ratings when k = 50. Similarly  we can conclude that the PMF model underﬁts
the users/items with more than 50 ratings when k = 5. Moreover  PMF with k = 50 achieves lower
RMSE (higher accuracy) than PMF with k = 5  but the improvement comes with sacriﬁced accuracy
for the users and items with a small number of ratings  e.g.  less than 10. This study shows that PMF

2

Table 1: The root mean square errors (RMSEs) of PMF [17] for users/items with different numbers
of ratings when rank k = 5 and k = 50.

#user ratings < 10
#user ratings > 50
#item ratings < 10
#item ratings > 50

All

rank = 5
0.9058
0.8416
0.9338
0.8520
0.8614

rank = 50

0.9165
0.8352
0.9598
0.8418
0.8583

with ﬁxed rank values cannot perfectly model the internal mixture-rank structure of the rating matrix.
To this end  it is desirable to model users and items with different ranks.

4 Mixture-Rank Matrix Approximation (MRMA)

µα

U k
i

bα

αk
i

σ2
U

Ri j

i = {1  ...  m}

bβ

µβ

V k
j

βk
j
k = {1  ...  K}

j = {1  ...  n}

σ2
V

σ2

Figure 1: The graphical model for the proposed mixture-rank matrix approximation (MRMA) method.

m(cid:89)

n(cid:89)

K(cid:88)

Following the idea of PMF  we exploit a probabilistic model with Gaussian noise to model the
ratings [17]. As shown in Figure 1  the conditional distribution over the observed ratings for the
mixture-rank model can be deﬁned as follows:

j N (Ri j|U k

i V k
j

T

Pr(R|U  V  α  β  σ2) =

i=1

αk
i βk

  σ2)]1i j  

[
j=1
k=1

(1)
where N (x|µ  σ2) denotes the probability density function of a Gaussian distribution with mean µ
and variance σ2. K is the maximum rank among all internal structures of the user-item rating matrix.
αk and βk are the weight vectors of the rank-k matrix approximation model for all users and items 
respectively. Thus  αk
j denote the weights of the rank-k model for the i-th user and j-th item 
respectively. U k and V k are the feature matrices of the rank-k matrix approximation model for all
users and items  respectively. Likewise  U k
j denote the feature vectors of the rank-k model
for the i-th user and j-th item  respectively. 1i j is an indication function  which will be 1 if Ri j is
observed and 0 otherwise.
By placing a zero mean isotropic Gaussian prior [6  17] on the user and item feature vectors  we have

i and V k

i and βk

Pr(U k|σ2

U ) =

N (U k

i |0  σ2

U I)  Pr(V k|σ2

V ) =

N (V k

j |0  σ2

V I).

(2)

i=1

j=1

For αk and βk  we choose a Laplacian prior here  because the models with most suitable ranks
for each user/item should be with large weights  i.e.  αk and βk should be sparse. By placing the
Laplacian prior on the user and item weight vectors  we have

Pr(αk|µα  bα) =

L(αk

i |µα  bα)  Pr(βk|µβ  bβ) =

L(βk

j |µβ  bβ) 

(3)

m(cid:89)

m(cid:89)

n(cid:89)

n(cid:89)

i=1

j=1

3

where µα and bα are the location parameter and scale parameter of the Laplacian distribution for α 
respectively  and accordingly µβ and bβ are the location parameter and scale parameter for β.
The log of the posterior distribution over the user and item features and weights can be given as
follows:

=

U   σ2

K(cid:88)

l = ln Pr(U  V  α  β|R  σ2  σ2

∝ ln(cid:2) Pr(R|U  V  α  β  σ2) Pr(U|σ2
V   µα  bα  µβ  bβ)
U ) Pr(V |σ2
m(cid:88)
j N (Ri j|U k
n(cid:88)
K(cid:88)
n(cid:88)
K(cid:88)

n(cid:88)
(cid:2) ln
m(cid:88)
K(cid:88)
m(cid:88)
K(cid:88)

i )2 − 1
2σ2
V

− 1
2σ2
U

i (V k

αk
i βk

(V k

(U k

1i j

k=1

k=1

i=1

j=1

k=1

j=1

i=1

i )2 − 1
2

− 1
bα

|αk
i − µα| − 1
bβ

k=1

i=1

k=1

j=1

V ) Pr(α|µα  bα) Pr(β|µβ  bβ)(cid:3)
j )T   σ2I)(cid:3)

Km ln σ2

Kn ln σ2
V

U − 1
2

K(cid:88)

k=1

|βk
j − µβ| − 1
2

m ln b2

α − 1
2

(4)

n ln b2

β + C 

K(cid:88)

k=1

where C is a constant that does not depend on any parameters. Since the above optimization problem
is difﬁcult to solve directly  we obtain its lower bound using Jensen’s inequality and then optimize
the following lower bound:

l(cid:48) = − 1
2σ2

i=1

j=1

n(cid:88)
m(cid:88)
K(cid:88)
m(cid:88)
K(cid:88)
m(cid:88)

k=1

i=1

k=1

i=1

− 1
2σ2
U

− 1
bα

(cid:2) K(cid:88)

1i j

αk
i βk

k=1

(U k

i )2 − 1
2σ2
V

|αk
i − µα| − 1
bβ

j (Ri j − U k
n(cid:88)
K(cid:88)
K(cid:88)
n(cid:88)

(V k

k=1

j=1

k=1

j=1

j )T )2(cid:3) − 1

2

m(cid:88)

n(cid:88)

i=1

j=1

i (V k

1i j ln σ2

i )2 − 1
2

Km ln σ2

U − 1
2

Kn ln σ2
V

(5)

|βk
j − µβ| − 1
2

Km ln b2

α − 1
2

Kn ln b2

β + C.

If we keep the hyperparameters of the prior distributions ﬁxed  then maximizing l(cid:48) is similar to the
popular least square error minimization with (cid:96)2 regularization on U and V and (cid:96)1 regularization on α
and β. However  keeping the hyperparameters ﬁxed may easily lead to overﬁtting because MRMA
models have many parameters.

5 Learning MRMA Models

The optimization problem deﬁned in Equation 5 is very likely to overﬁt if we cannot precisely
estimate the hyperparameters  which automatically control the generalization capacity of the MRMA
model. For instance  σU and σV will control the regularization of U and V . Therefore  it is more
desirable to estimate the parameters and hyperparameters simultaneously during model training. One
possible way is to estimate each variable by its maximum a priori (MAP) value while conditioned
on the rest variables and then iterate until convergence  which is also known as iterated conditional
modes (ICM) [1].
The ICM procedure for maximizing Equation 5 is presented as follows.
Initialization: Choose initial values for all variables and parameters.
ICM Step: The values of U  V   α and β can be updated by solving the following minimization
problems when conditioned on other variables or hyperparameters.

∀k ∈ {1  ...  K} ∀i ∈ {1  ...  m} :

n(cid:88)
n(cid:88)

j=1

(cid:8) 1
(cid:8) 1

2σ2

2σ2

(cid:2) K(cid:88)
(cid:2) K(cid:88)

k=1

1i j

1i j

i ← arg min
U k
U(cid:48)

i ← arg min
αk
α(cid:48)

αk
i βk

j (Ri j − U k

i (V k

αk
i βk

j (Ri j − U k

i (V k

j )T )2(cid:3) +
j )T )2(cid:3) +

K(cid:88)
K(cid:88)

k=1

(U k

i )2(cid:9) 
i − µα|(cid:9).

|αk

1
2σ2
U

1
bα

k=1

j=1

k=1

4

The hyperparameters can be learned as their maximum likelihood estimates by setting their partial
derivatives on l(cid:48) to 0.

∀k ∈ {1  ...  K} ∀j ∈ {1  ...  n} :

(cid:8) 1
j ← arg min
V k
(cid:8) 1
V (cid:48)

j ← arg min
βk
β(cid:48)

2σ2

2σ2

m(cid:88)
m(cid:88)

i=1

1i j

1i j

(cid:2) K(cid:88)
(cid:2) K(cid:88)

k=1

i=1

k=1

αk
i βk

j (Ri j − U k

i (V k

αk
i βk

j (Ri j − U k

i (V k

i=1

j=1

σ2 ← m(cid:88)
n(cid:88)
U ← K(cid:88)
m(cid:88)
n(cid:88)
V ← K(cid:88)

σ2

σ2

k=1

i=1

1i j

k=1

αk
i βk

i (V k

(cid:2) K(cid:88)
m(cid:88)
j )T )2(cid:3)/
j (Ri j − U k
i )2/Km  µα ← K(cid:88)
m(cid:88)
n(cid:88)
j )2/Kn  µβ ← K(cid:88)

αk

k=1

i=1

i=1

(U k

(V k

βk
j /Kn  bβ =

i /Km  bα =

j=1

j )T )2(cid:3) +
j )T )2(cid:3) +
n(cid:88)

1i j 

K(cid:88)
m(cid:88)
n(cid:88)
K(cid:88)

k=1

i=1

K(cid:88)
K(cid:88)

k=1

(V k

j )2(cid:9) 
j − µβ|(cid:9).

|βk

1
2σ2
V

1
bβ

k=1

|αk
i − µα|/Km 

|βk
j − µβ|/Kn.

k=1

j=1

k=1

j=1

k=1

j=1

Repeat: until convergence or the maximum number of iterations reached.
Note that ICM is sensitive to initial values. Our empirical studies show that setting the initial values
√
of U k and V k by solving the classic PMF method can achieve good performance. Regarding α
and β  one of the proper initial values should be 1/
K (K denotes the number of sub-models in
the mixture model). To improve generalization performance and enable online learning [7]  we can
update U  V  α  β using stochastic gradient descent. Meanwhile  the (cid:96)1 norms in learning α and β
can be approximated by the smoothed (cid:96)1 method [18]. To deal with massive datasets  we can use the
alternating least squares (ALS) method to learn the parameters of the proposed MRMA model  which
is amenable to parallelization.

6 Experiments

i

i

j

j

(cid:80)

(cid:80)

(cid:113)(cid:80)

This section presents the experimental results of the proposed MRMA method on three well-known
datasets: 1) MovieLens 1M dataset (∼1 million ratings from 6 040 users on 3 706 movies); 2)
MovieLens 10M dataset (∼10 million ratings from 69 878 users on 10 677 movies); 3) Netﬂix Prize
dataset (∼100 million ratings from 480 189 users on 17 770 movies). For all accuracy comparisons 
we randomly split each dataset into a training set and a test set by the ratio of 9:1. All results are
reported by averaging over 5 different splits. The root mean square error (RMSE) is adopted to
measure the rating prediction accuracy of different algorithms  which can be computed as follows:
D( ˆR) =
1i j (1i j indicates that entry (i  j) appears in the
test set). The normalized discounted cumulative gain (NDCG) is adopted to measure the item
ranking accuracy of different algorithms  which can be computed as follows: N DCG@N =
i=1(2reli − 1)/ log2(i + 1)  and IDCG is the DCG value

1i j(Ri j − ˆRi j)2/(cid:80)
DCG@N/IDCG@N (DCG@N =(cid:80)N

with perfect ranking).
In ICM-based learning  we adopt  = 0.00001 as the convergence threshold and T = 300 as the
maximum number of iterations. Considering efﬁciency  we only choose a subset of ranks  e.g. 
{10  20  30  ...  300} rather than {1  2  3  ...  300}  in MRMA. The parameters of all the compared
algorithms are adopted from their original papers because all of them are evaluated on the same
datasets.
We compare the recommendation accuracy of MRMA with six matrix approximation-based collabo-
rative ﬁltering algorithms as follows: 1) BPMF [16]  which extends the PMF method from a Baysian
view and estimates model parameters using a Markov chain Monte Carlo scheme; 2) GSMF [20] 
which learns user/item features with group sparsity regularization in matrix approximation; 3) LLOR-
MA [12]  which ensembles the approximations from different submatrices using kernel smoothing;
4) WEMAREC [5]  which ensembles different biased matrix approximation models to achieve higher

5

Figure 2: Root mean square error comparison
between MRMA and PMF with different ranks.

Figure 3: The accuracy and efﬁciency tradeoff of
MRMA.

accuracy; 5) MPMA [4]  which combines local and global matrix approximations using a mixture
model; 6) SMA [13]  which yields a stable matrix approximation that can achieve good generalization
performance.

6.1 Mixture-Rank Matrix Approximation vs. Fixed-Rank Matrix Approximation

Given a ﬁxed rank k  the corresponding rank-k model in MRMA is identical to probabilistic matrix
factorization (PMF) [17]. In this experiment  we compare the recommendation accuracy of MRMA
with ranks in {10  20  50  100  150  200  250  300} against those of PMF with ﬁxed ranks on the
MovieLens 1M dataset. For PMF  we choose 0.01 as the learning rate  0.01 as the user feature
regularization coefﬁcient  and 0.001 as the item feature regularization coefﬁcient  respectively. The
convergence condition is the same as MRMA.
As shown in Figure 2  when the rank increases from 10 to 300  PMF can achieve RMSEs between
0.86 and 0.88. However  the RMSE of MRMA is about 0.84 when mixing all these ranks from 10 to
300. Meanwhile  the accuracy of PMF is not stable when k ≤ 100. For instance  PMF with k = 10
achieves better accuracy than k = 20 but worse accuracy than k = 50. This is because ﬁxed rank
matrix approximation cannot be perfect for all users and items  so that many users and items either
underﬁt or overﬁt at a ﬁxed rank less than 100. Yet when k > 100  only overﬁtting occurs and PMF
achieves consistently better accuracy when k increases  which is because regularization terms can
help improve generalization capacity. Nevertheless  PMF with all ranks achieves lower accuracy
than MRMA  because individual users/items can give the sub-models with the optimal ranks higher
weights in MRMA and thus alleviate underﬁtting or overﬁtting.

6.2 Sensitivity of Rank in MRMA

In MRMA  the set of ranks decide the performance of the ﬁnal model. However  it is neither efﬁcient
nor necessary to choose all the ranks in [1  2  ...  K]. For instance  a rank-k approximation will
be very similar to rank-(k − 1) and rank-(k + 1) approximations  i.e.  they may have overlapping
structures. Therefore  a subset of ranks will be sufﬁcient. Figure 3 shows 5 different settings of
rank combinations  in which set 1 = {10  20  30  ...  300}  set 2 = {20  40  ...  300}  set 3 =
{30  60  ...  300}  set 4 = {50  100  ...  300}  and set 5 = {100  200  300}. As shown in this ﬁgure 
RMSE decreases when more ranks are adopted in MRMA  which is intuitive because more ranks will
help users/items better choose the most appropriate components. However  the computation time
also increases when more ranks are adopted in MRMA. If a tradeoff between accuracy and efﬁciency
is required  then set 2 or set 3 will be desirable because they achieve slightly worse accuracies but
signiﬁcantly less computation overheads.
MRMA only contains three sub-models with different ranks in set 5 = {100  200  300}  but it still
signiﬁcantly outperforms PMF with ranks ranging from 10 to 300 in recommendation accuracy (as
shown in Figure 2). This further conﬁrms that MRMA can indeed discover the internal mixture-rank
structure of the user-item rating matrix and thus achieve better recommendation accuracy due to
better approximation.

6

0.800.820.840.860.88k=10k=20k=50k=100k=150k=200k=250k=300MRMARMSEModelPMFMRMA0.800.820.840.86set 1set 2set 3set 4set 5 0 1000RMSEcomputation time (s)rank settingRMSEcomputation timeTable 2: RMSE comparison between MRMA and six state-of-the-art matrix approximation-based
collaborative ﬁltering algorithms on MovieLens (10M) and Netﬂix datasets. Note that MRMA
statistically signiﬁcantly outperforms the other algorithms with 95% conﬁdence level.

BPMF [16]
GSMF [20]

LLORMA [12]
WEMAREC [5]

MPMA [4]
SMA [13]
MRMA

MovieLens (10M)
0.8197 ± 0.0004
0.8012 ± 0.0011
0.7855 ± 0.0002
0.7775 ± 0.0007
0.7712 ± 0.0002
0.7682 ± 0.0003
0.7634 ± 0.0009

Netﬂix

0.8421 ± 0.0002
0.8420 ± 0.0006
0.8275 ± 0.0004
0.8143 ± 0.0001
0.8139 ± 0.0003
0.8036 ± 0.0004
0.7973 ± 0.0002

Table 3: NDCG comparison between MRMA and six state-of-the-art matrix approximation-based
collaborative ﬁltering algorithms on Movielens (1M) and Movielens (10M) datasets. Note that
MRMA statistically signiﬁcantly outperforms the other algorithms with 95% conﬁdence level.

Metric

Data | Method
BPMF
GSMF

M
1

s
n
e
l
e
i
v
o
M

M
0
1

s
n
e
l
e
i
v
o
M

N=1

MPMA
SMA
MRMA
BPMF
GSMF

0.6870 ± 0.0024
0.6909 ± 0.0048
0.7025 ± 0.0027
LLORMA
WEMAREC 0.7048 ± 0.0015
0.7020 ± 0.0005
0.7042 ± 0.0033
0.7153 ± 0.0027
0.6563 ± 0.0005
0.6708 ± 0.0012
0.6829 ± 0.0014
LLORMA
WEMAREC 0.7013 ± 0.0003
0.6908 ± 0.0006
0.7002 ± 0.0006
0.7048 ± 0.0006

MPMA
SMA
MRMA

NDCG@N

N=5

0.6981 ± 0.0029
0.7031 ± 0.0023
0.7101 ± 0.0005
0.7089 ± 0.0016
0.7114 ± 0.0018
0.7109 ± 0.0011
0.7182 ± 0.0005
0.6845 ± 0.0003
0.6995 ± 0.0008
0.7066 ± 0.0005
0.7176 ± 0.0006
0.7133 ± 0.0002
0.7134 ± 0.0004
0.7219 ± 0.0001

N=10

0.7525 ± 0.0009
0.7555 ± 0.0017
0.7626 ± 0.0023
0.7617 ± 0.0041
0.7606 ± 0.0006
0.7607 ± 0.0008
0.7672 ± 0.0013
0.7467 ± 0.0007
0.7566 ± 0.0017
0.7632 ± 0.0004
0.7703 ± 0.0002
0.7680 ± 0.0001
0.7679 ± 0.0003
0.7743 ± 0.0001

N=20

0.8754 ± 0.0008
0.8769 ± 0.0011
0.8811 ± 0.0010
0.8796 ± 0.0005
0.8805 ± 0.0007
0.8801 ± 0.0004
0.8837 ± 0.0004
0.8691 ± 0.0002
0.8748 ± 0.0004
0.8782 ± 0.0012
0.8824 ± 0.0006
0.8808 ± 0.0004
0.8809 ± 0.0002
0.8846 ± 0.0001

6.3 Accuracy Comparison

6.3.1 Rating Prediction Comparison

Table 2 compares the rating prediction accuracy between MRMA and six matrix approximation-
based collaborative ﬁltering algorithms on MovieLens (10M) and Netﬂix datasets. Note that among
the compared algorithms  BPMF  GSMF  MPMA and SMA are stand-alone algorithms  while
LLORMA and WEMAREC are ensemble algorithms. In this experiment  we adopt the set of ranks
as {10  20  50  100  150  200  250  300} due to efﬁciency reason  which means that the accuracy of
MRMA should not be optimal. However  as shown in Table 2  MRMA statistically signiﬁcantly
outperforms all the other algorithms with 95% conﬁdence level. The reason is that MRMA can
choose different rank values for different users/items  which can achieve not only globally better
approximation but also better approximation in terms of individual users or items. This further
conﬁrms that mixture-rank structure indeed exists in user-item rating matrices in recommender
systems. Thus  it is desirable to adopt mixture-rank matrix approximations rather than ﬁxed-rank
matrix approximations for recommendation tasks.

6.3.2 Item Ranking Comparison

Table 3 compares the NDCGs of MRMA with the other six state-of-the-art matrix approximation-
based collaborative ﬁltering algorithms on Movielens (1M) and Movielens (10M) datasets. Note that
for each dataset  we keep 20 ratings in the test set for each user and remove users with less than 5

7

ratings in the training set. As shown in the results  MRMA can also achieve higher item ranking
accuracy than the other compared algorithms thanks to the capability of better capturing the internal
mixture-rank structures of the user-item rating matrices. This experiment demonstrates that MRMA
can not only provide accurate rating prediction but also achieve accurate item ranking for each user.

6.4

Interpretation of MRMA

Table 4: Top 10 movies with largest β values for sub-models with rank k = 20 and k = 200 in MRMA.
Here  #ratings stands for the average number of ratings in the training set for the corresponding
movies.

rank=20

rank=200

movie name

β

#ratings

movie name

β

#ratings

Smashing Time

Gate of Heavenly Peace

Man of the Century

Mamma Roma
Dry Cleaning
Dear Jesse

Skipped Parts

The Hour of the Pig

Inheritors

Dangerous Game

0.6114
0.6101
0.6079
0.6071
0.6071
0.6063
0.6057
0.6055
0.6042
0.6034

2.4

American Beauty
Groundhog Day

Fargo

Face/Off

2001: A Space Odyssey

Shakespeare in Love
Saving Private Ryan

The Fugitive
Braveheart
Fight Club

0.9219
0.9146
0.8779
0.8693
0.8608
0.8553
0.8480
0.8404
0.8247
0.8153

1781.4

To better understand how users/items weigh different sub-models in the mixture model of MRMA 
we present the top 10 movies which have largest β values for sub-models with rank=20 and rank=200 
show their β values  and compare their average numbers of ratings in the training set in Table 4.
Intuitively  the movies with more ratings (e.g.  over 1000 ratings) should weigh higher towards more
complex models  and the movies with less ratings (e.g.  under 10 ratings) should weigh higher towards
simpler models in MRMA.
As shown in Table 4  the top 10 movies with largest β values for the sub-model with rank 20 have
only 2.4 ratings on average in the training set. On the contrary  the top 10 movies with largest β values
for the sub-model with rank 200 have 1781.4 ratings on average in the training set  and meanwhile
these movies are very popular and most of them are Oscar winners. This conﬁrms our previous claim
that MRMA can indeed weigh more complex models (e.g.  rank=200) higher for movies with more
ratings to prevent underﬁtting  and weigh less complex models (e.g.  rank=20) higher for the movies
with less ratings to prevent overﬁtting. A similar phenomenon has also been observed from users
with different α values  and we omit the results due to space limit.

7 Conclusion and Future Work

This paper proposes a mixture-rank matrix approximation (MRMA) method  which describes user-
item ratings using a mixture of low-rank matrix approximation models with different ranks to achieve
better approximation and thus better recommendation accuracy. An ICM-based learning algorithm is
proposed to handle the non-convex optimization problem pertaining to MRMA. The experimental
results on MovieLens and Netﬂix datasets demonstrate that MRMA can achieve better accuracy than
six state-of-the-art matrix approximation-based collaborative ﬁltering methods  further pushing the
frontier of recommender systems. One of the possible extensions of this work is to incorporate other
inference methods into learning the MRMA model  e.g.  variational inference [8]  because ICM may
be trapped in local maxima and therefore cannot achieve global maxima without properly chosen
initial values.

Acknowledgement

This work was supported in part by the National Natural Science Foundation of China under Grant
No. 61332008 and NSAF under Grant No. U1630115.

8

References
[1] J. Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society. Series B

(Methodological)  pages 259–302  1986.

[2] E. J. Candès and B. Recht. Exact matrix completion via convex optimization. Communications of the

ACM  55(6):111–119  2012.

[3] E. J. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

Transactions on Information Theory  56(5):2053–2080  2010.

IEEE

[4] C. Chen  D. Li  Q. Lv  J. Yan  S. M. Chu  and L. Shang. MPMA: mixture probabilistic matrix approxi-
mation for collaborative ﬁltering. In Proceedings of the 25th International Joint Conference on Artiﬁcial
Intelligence (IJCAI ’16)  pages 1382–1388  2016.

[5] C. Chen  D. Li  Y. Zhao  Q. Lv  and L. Shang. WEMAREC: Accurate and scalable recommendation
through weighted and ensemble matrix approximation. In Proceedings of the 38th International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’15)  pages 303–312 
2015.

[6] D. Dueck and B. Frey. Probabilistic sparse matrix factorization. University of Toronto technical report

PSI-2004-23  2004.

[7] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic gradient descent 

2015. arXiv:1509.01240.

[8] M. I. Jordan  Z. Ghahramani  T. S. Jaakkola  and L. K. Saul. An introduction to variational methods for

graphical models. Machine learning  37(2):183–233  1999.

[9] Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In Proceed-
ings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD
’14)  pages 426–434. ACM  2008.

[10] Y. Koren. Collaborative ﬁltering with temporal dynamics. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD ’09)  pages 447–456. ACM 
2009.

[11] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems. Computer 

42(8)  2009.

[12] J. Lee  S. Kim  G. Lebanon  and Y. Singer. Local low-rank matrix approximation. In Proceedings of The

30th International Conference on Machine Learning (ICML ’13)  pages 82–90  2013.

[13] D. Li  C. Chen  Q. Lv  J. Yan  L. Shang  and S. Chu. Low-rank matrix approximation with stability. In The

33rd International Conference on Machine Learning (ICML ’16)  pages 295–303  2016.

[14] H. Ma  H. Yang  M. R. Lyu  and I. King. Sorec: social recommendation using probabilistic matrix
factorization. In Proceedings of the 17th ACM conference on Information and knowledge management
(CIKM ’08)  pages 931–940. ACM  2008.

[15] L. W. Mackey  M. I. Jordan  and A. Talwalkar. Divide-and-conquer matrix factorization. In Advances in

Neural Information Processing Systems (NIPS ’11)  pages 1134–1142  2011.

[16] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo.
In Proceedings of the 25th international conference on Machine learning (ICML ’08)  pages 880–887.
ACM  2008.

[17] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in Neural Information

Processing Systems (NIPS ’08)  pages 1257–1264  2008.

[18] M. Schmidt  G. Fung  and R. Rosales. Fast optimization methods for L1 regularization: A comparative
study and two new approaches. In European Conference on Machine Learning (ECML ’07)  pages 286–297.
Springer  2007.

[19] K.-C. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized linear

least squares problems. Paciﬁc Journal of Optimization  6(15):615–640  2010.

[20] T. Yuan  J. Cheng  X. Zhang  S. Qiu  and H. Lu. Recommendation by mining multiple user behaviors with
group sparsity. In Proceedings of the 28th AAAI Conference on Artiﬁcial Intelligence (AAAI ’14)  pages
222–228  2014.

9

,José Bento
Nate Derbinsky
Javier Alonso-Mora
Jonathan Yedidia
Dongsheng Li
Chao Chen
Wei Liu
Tun Lu
Ning Gu
Stephen Chu