2016,Natural-Parameter Networks: A Class of Probabilistic Neural Networks,Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient  they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data  as is often done in probabilistic graphical models. To address these problems  we propose a class of probabilistic neural networks  dubbed natural-parameter networks (NPN)  as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN  NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment  efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer  as byproducts  may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.,Natural-Parameter Networks:

A Class of Probabilistic Neural Networks

Hao Wang  Xingjian Shi  Dit-Yan Yeung

Hong Kong University of Science and Technology

{hwangaz xshiab dyyeung}@cse.ust.hk

Abstract

Neural networks (NN) have achieved state-of-the-art performance in various appli-
cations. Unfortunately in applications where training data is insufﬁcient  they are
often prone to overﬁtting. One effective way to alleviate this problem is to exploit
the Bayesian approach by using Bayesian neural networks (BNN). Another short-
coming of NN is the lack of ﬂexibility to customize different distributions for the
weights and neurons according to the data  as is often done in probabilistic graphi-
cal models. To address these problems  we propose a class of probabilistic neural
networks  dubbed natural-parameter networks (NPN)  as a novel and lightweight
Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family
distributions to model the weights and neurons. Different from traditional NN
and BNN  NPN takes distributions as input and goes through layers of transfor-
mation before producing distributions to match the target output distributions. As
a Bayesian treatment  efﬁcient backpropagation (BP) is performed to learn the
natural parameters for the distributions over both the weights and neurons. The
output distributions of each layer  as byproducts  may be used as second-order
representations for the associated tasks such as link prediction. Experiments on
real-world datasets show that NPN can achieve state-of-the-art performance.

1

Introduction

Recently neural networks (NN) have achieved state-of-the-art performance in various applications
ranging from computer vision [12] to natural language processing [20]. However  NN trained by
stochastic gradient descent (SGD) or its variants is known to suffer from overﬁtting especially
when training data is insufﬁcient. Besides overﬁtting  another problem of NN comes from the
underestimated uncertainty  which could lead to poor performance in applications like active learning.
Bayesian neural networks (BNN) offer the promise of tackling these problems in a principled way.
Early BNN works include methods based on Laplace approximation [16]  variational inference (VI)
[11]  and Monte Carlo sampling [18]  but they have not been widely adopted due to their lack of
scalability. Some recent advances in this direction seem to shed light on the practical adoption of
BNN. [8] proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the
marginal likelihood is used to infer the weights. Recently  [10] used an online version of expectation
propagation (EP)  called ‘probabilistic back propagation’ (PBP)  for the Bayesian learning of NN 
and [4] proposed ‘Bayes by Backprop’ (BBB)  which can be viewed as an extension of [8] based on
the ‘reparameterization trick’ [13]. More recently  an interesting Bayesian treatment called ‘Bayesian
dark knowledge’ (BDK) was designed to approximate a teacher network with a simpler student
network based on stochastic gradient Langevin dynamics (SGLD) [1].
Although these recent methods are more practical than earlier ones  several outstanding problems
remain to be addressed: (1) most of these methods require sampling either at training time [8  4  1] or
at test time [4]  incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1]  methods

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

based on online EP or VI do not involve sampling  but they need to compute the predictive density
by integrating out the parameters  which is computationally inefﬁcient; (3) these methods assume
Gaussian distributions for the weights and neurons  allowing no ﬂexibility to customize different
distributions according to the data as is done in probabilistic graphical models (PGM).
To address the problems  we propose natural-parameter networks (NPN) as a class of probabilistic
neural networks where the input  target output  weights  and neurons can all be modeled by arbitrary
exponential-family distributions (e.g.  Poisson distributions for word counts) instead of being limited
to Gaussian distributions. Input distributions go through layers of linear and nonlinear transformation
deterministically before producing distributions to match the target output distributions (previous
work [21] shows that providing distributions as input by corrupting the data with noise plays the
role of regularization). As byproducts  output distributions of intermediate layers may be used as
second-order representations for the associated tasks. Thanks to the properties of the exponential
family [3  19]  distributions in NPN are deﬁned by the corresponding natural parameters which can
be learned efﬁciently by backpropagation. Unlike [4  1]  NPN explicitly propagates the estimates of
uncertainty back and forth in deep networks. This way the uncertainty estimates for each layer of
neurons are readily available for the associated tasks. Our experiments show that such information is
helpful when neurons of intermediate layers are used as representations like in autoencoders (AE). In
summary  our main contributions are:

• We propose NPN as a class of probabilistic neural networks. Our model combines the merits
of NN and PGM in terms of computational efﬁciency and ﬂexibility to customize the types
of distributions for different types of data.
• Leveraging the properties of the exponential family  some sampling-free backpropagation-
compatible algorithms are designed to efﬁciently learn the distributions over weights by
learning the natural parameters.
• Unlike most probabilistic NN models  NPN obtains the uncertainty of intermediate-layer
neurons as byproducts  which provide valuable information to the learned representations.
Experiments on real-world datasets show that NPN can achieve state-of-the-art performance
on classiﬁcation  regression  and unsupervised representation learning tasks.

2 Natural-Parameter Networks

The exponential family refers to an important class of distributions with useful algebraic properties.
Distributions in the exponential family have the form p(x|η) = h(x)g(η) exp{ηT u(x)}  where x is
the random variable  η denotes the natural parameters  u(x) is a vector of sufﬁcient statistics  and
g(η) is the normalizer. For a given type of distributions  different choices of η lead to different shapes.
For example  a univariate Gaussian distribution with η = (c  d)T corresponds to N (− c
Motivated by this observation  in NPN  only the natural parameters need to be learned to model the
distributions over the weights and neurons. Consider an NPN which takes a vector random distribution
(e.g.  a multivariate Gaussian distribution) as input  multiplies it by a matrix random distribution 
goes through nonlinear transformation  and outputs another distribution. Since all three distributions
in the process can be speciﬁed by their natural parameters (given the types of distributions)  learning
and prediction of the network can actually operate in the space of natural parameters. For example  if
we use element-wise (factorized) gamma distributions for both the weights and neurons  the NPN
counterpart of a vanilla network only needs twice the number of free parameters (weights) and
neurons since there are two natural parameters for each univariate gamma distribution.

2d  − 1
2d ).

2.1 Notation and Conventions

We use boldface uppercase letters like W to denote matrices and boldface lowercase letters like
b for vectors. Similarly  a boldface number (e.g.  1 or 0) represents a row vector or a matrix with
identical entries. In NPN  o(l) is used to denote the values of neurons in layer l before nonlinear
transformation and a(l) is for the values after nonlinear transformation. As mentioned above  NPN
tries to learn distributions over variables rather than variables themselves. Hence we use letters
without subscripts c  d  m  and s (e.g.  o(l) and a(l)) to denote ‘random variables’ with corresponding
distributions. Subscripts c and d are used to denote natural parameter pairs  such as Wc and Wd.
Similarly  subscripts m and s are for mean-variance pairs. Note that for clarity  many operations used
below are implicitly element-wise  for example  the square z2  division z
∂b  the

b  partial derivative ∂z

2

m = xi  a(0)

z. For the data D = {(xi  yi)}N
i=1 
gamma function Γ(z)  logarithm log z  factorial z!  1 + z  and 1
(cid:54)= 0 resemble AE’s denoising effect.) as
we set a(0)
input of the network and yi denotes the output targets (e.g.  labels and word counts). In the following
text we drop the subscript i (and sometimes the superscript (l)) for clarity. The bracket (· ·) denotes
concatenation or pairs of vectors.

s = 0 (Input distributions with a(0)
s

2.2 Linear Transformation in NPN

c   W(l)

c ij  W(l)

i j p(W(l)

c ij  W(l)

ij |W(l)

d ) =(cid:81)

d ij)  where the pair (W(l)

Here we ﬁrst introduce the linear form of a general NPN. For simplicity  we assume distributions
with two natural parameters (e.g.  gamma distributions  beta distributions  and Gaussian distribu-
tions)  η = (c  d)T   in this section. Speciﬁcally  we have factorized distributions on the weight
matrices  p(W(l)|W(l)
d ij) is the
corresponding natural parameters. For b(l)  o(l)  and a(l) we assume similar factorized distributions.
In a traditional NN  the linear transformation follows o(l) = a(l−1)W(l) + b(l) where a(l−1) is the
output from the previous layer. In NN a(l−1)  W(l)  and b(l) are deterministic variables while in
NPN they are exponential-family distributions  meaning that the result o(l) is also a distribution. For
convenience of subsequent computation it is desirable to approximate o(l) using another exponential-
family distribution. We can do this by matching the mean and variance. Speciﬁcally  after computing
m   W(l)
(W(l)
d through
the mean o(l)
(a(l−1)

(1)
(2)
(3)
where ◦ denotes the element-wise product and the bijective function f (· ·) maps the natural parame-
d2 ) in gamma distributions).
ters of a distribution into its mean and variance (e.g.  f (c  d) = ( c+1−d   c+1
Similarly we use f−1(· ·) to denote the inverse transformation. W(l)
s are the
mean and variance of W(l) and b(l) obtained from the natural parameters. The computed o(l)
m and
o(l)
s can then be used to recover o(l)
d   which will subsequently facilitate the feedforward
computation of the nonlinear transformation described in Section 2.3.

c   W(l)
s ) = f (W(l)
m and variance o(l)
m   a(l−1)
) = f (a(l−1)
s = a(l−1)
o(l)
d ) = f−1(o(l)
c   o(l)

  a(l−1)
)  o(l)
s + a(l−1)
s ) 

d ) and (b(l)
s of o(l) as follows:

m = a(l−1)
m ◦ W(l)
(W(l)

m + b(l)
m  
m ) + (a(l−1)

d )  we can get o(l)

d
s W(l)

s ) = f (b(l)

m   and b(l)

m )W(l)

s + b(l)
s  

m   W(l)

s   b(l)

c and o(l)

◦ a(l−1)

m

m   b(l)

c   b(l)

s

m   o(l)

s

c

m W(l)

c and o(l)

(o(l)

2.3 Nonlinear Transformation in NPN

After we obtain the linearly transformed distribution over o(l) deﬁned by natural parameters o(l)
c and
d   an element-wise nonlinear transformation v(·) (with a well deﬁned inverse function v−1(·)) will
o(l)
be imposed. The resulting activation distribution is pa(a(l)) = po(v−1(a(l)))|v−1(cid:48)
(a(l))|  where po
is the factorized distribution over o(l) deﬁned by (o(l)
Though pa(a(l)) may not be an exponential-family distribution  we can approximate it with one 
p(a(l)|a(l)
s of pa(a(l))
are obtained  we can compute corresponding natural parameters with f−1(· ·) (approximation
accuracy is sufﬁcient according to preliminary experiments). The feedforward computation is:
−1(am  as).

d )  by matching the ﬁrst two moments. Once the mean a(l)

po(o|oc  od)v(o)do  as =

po(o|oc  od)v(o)2do − a2

m and variance a(l)

c   o(l)
d ).

m  (ac  ad) = f

c   a(l)

am =

(cid:90)

(cid:90)

(4)

Here the key computational challenge is computing the integrals in Equation (4). Closed-form
solutions are needed for their efﬁcient computation. If po(o|oc  od) is a Gaussian distribution  closed-
form solutions exist for common activation functions like tanh(x) and max(0  x) (details are in
Section 3.2). Unfortunately this is not the case for other distributions. Leveraging the convenient
form of the exponential family  we ﬁnd that it is possible to design activation functions so that the
integrals for non-Gaussian distributions can also be expressed in closed form.
Theorem 1. Assume an exponential-family distribution po(x|η) = h(x)g(η) exp{ηT u(x)}  where
the vector u(x) = (u1(x)  u2(x)  . . .   uM (x))T (M is the number of natural parameters). If activa-

tion function v(x) = r − q exp(−τ ui(x)) is used  the ﬁrst two moments of v(x) (cid:82) po(x|η)v(x)dx

3

Table 1: Activation Functions for Exponential-Family Distributions

Distribution
Beta Distribution
Rayleigh Distribution
Gamma Distribution
Poisson Distribution
Gaussian Distribution

Γ(c)Γ(d) xc−1(1 − x)d−1
σ2 exp{− x2
Γ(c) dcxc−1 exp{−dx}

Probability Density Function
p(x) = Γ(c+d)
p(x) = x
p(x) = 1
p(x) = cx exp{−c}
x!
− 1
p(x) = (2πσ2)

2 exp{− 1

2σ2 }

Activation Function
qxτ   τ ∈ (0  1)
r − q exp{−τ x2}
r − q exp{−τ x}
r − q exp{−τ x}
ReLU  tanh  and sigmoid

Support

[0  1]
(0  +∞)
(0  +∞)
Nonnegative interger
(−∞  +∞)

and(cid:82) po(x|η)v(x)2dx  can be expressed in closed form. Here i ∈ {1  2  . . .   M} (different ui(x)
let η = (η1  η2  . . .   ηM )  (cid:101)η = (η1  η2  . . .   ηi − τ  . . .   ηM )  and (cid:98)η =

corresponds to a different set of activation functions) and r  q  and τ are constants.

Proof. We ﬁrst
(η1  η2  . . .   ηi − 2τ  . . .   ηM ). The ﬁrst moment of v(x) is

2σ2 (x − µ)2}

E(v(x)) = r − q

h(x)g(η) exp{ηT u(x) − τ ui(x)} dx

g((cid:101)η) exp{(cid:101)ηT u(x)} dx = r − q
g((cid:98)η)
Similarly the second moment can be computed as E(v(x)2) = r2 + q2 g(η)

g((cid:101)η)

= r − q

g(η)

h(x)

g(η)

g((cid:101)η)
g((cid:101)η)
− 2rq g(η)

.

.

A more detailed proof is provided in the supplementary material. With Theorem 1  what remains is to
ﬁnd the constants that make v(x) strictly increasing and bounded (Table 1 shows some exponential-
family distributions and their possible activation functions). For example in Equation (4)  if v(x) =
r − q exp(−τ x)  am = r − q( od
In the backpropagation  for distributions with two natural parameters the gradient consists of two
terms. For example  ∂E
∂oc

od+τ )oc for the gamma distribution.

  where E is the error term of the network.

◦ ∂am

= ∂E
∂am

◦ ∂as

+ ∂E
∂as

∂oc

∂oc

(cid:90)
(cid:90)

for l = 1 : L do

Algorithm 1 Deep Nonlinear NPN
1: Input: Data D = {(xi  yi)}N
2: for t = 1 : T do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for

end for
Compute the error E from (o(L)
for l = L : 1 do
∂E

end for
Update W(l)

c   W(l)

d   b(l)

Compute

  ∂E
∂b(l)
m

∂W(l)
m

∂W(l)

∂E

 

c

s

i=1  number of iterations T   learning rate ρt  number of layers L.

Apply Equation (1)-(4) to compute the linear and nonlinear transformation in layer l.

  o(L)

d ) or (a(L)

c

  a(L)
d ).

  and ∂E
∂b(l)
s

. Compute

∂E

∂W(l)

c

 

∂E

∂W(l)
d

  ∂E
∂b(l)

c

  and ∂E
∂b(l)
d

.

c   and b(l)

d in all layers.

2.4 Deep Nonlinear NPN

Naturally layers of nonlinear NPN can be stacked to form a deep NPN1  as shown in Algorithm 12. A
deep NPN is in some sense similar to a PGM with a chain structure. Unlike PGM in general  however 
NPN does not need costly inference algorithms like variational inference or Markov chain Monte
Carlo. For some chain-structured PGM (e.g  hidden Markov models)  efﬁcient inference algorithms
also exist due to their special structure. Similarly  the Markov property enables NPN to be efﬁciently
trained in an end-to-end backpropagation learning fashion in the space of natural parameters.
PGM is known to be more ﬂexible than NN in the sense that it can choose different distributions to
depict different relationships among variables. A major drawback of PGM is its scalability especially

1Although the approximation accuracy may decrease as NPN gets deeper during feedforward computation  it

can be automatically adjusted according to data during backpropagation.

2Note that since the ﬁrst part of Equation (1) and the last part of Equation (4) are canceled out  we can

directly use (a(l)

m   a(l)

s ) without computing (a(l)

c   a(l)

d ) here.

4

when the PGM is deep. Different from PGM  NN stacks relatively simple computational layers and
learns the parameters using backpropagation  which is computationally more efﬁcient than most
algorithms for PGM. NPN has the potential to get the best of both worlds. In terms of ﬂexibility 
different types of exponential-family distributions can be chosen for the weights and neurons. Using
gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version
of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid
activation resembles a Bayesian treatment of sigmoid belief networks [17]. If Poisson distributions
are chosen for the neurons  NPN becomes a neural analogue of deep Poisson factor analysis [26  9].
Note that similar to the weight decay in NN  we may add the KL divergence between the prior
distributions and the learned distributions on the weights to the error E for regularization (we use
isotropic Gaussian priors in the experiments). In NPN  the chosen prior distributions correspond to
priors in Bayesian models and the learned distributions correspond to the approximation of posterior
distributions on weights. Note that the generative story assumed here is that weights are sampled
from the prior  and then output is generated (given all data) from these weights.

3 Variants of NPN

In this section  we introduce three NPN variants with different properties to demonstrate the ﬂexibility
and effectiveness of NPN. Note that in practice we use a transformed version of the natural parameters 
referred to as proxy natural parameters here  instead of the original ones for computational efﬁciency.
For example  in gamma distributions p(x|c  d) = Γ(c)−1dcxc−1 exp(−dx)  we use proxy natural
parameters (c  d) during computation rather than the natural parameters (c − 1 −d).

3.1 Gamma NPN

The gamma distribution with support over positive values is an important member of the exponential
family. The corresponding probability density function is p(x|c  d) = Γ(c)−1dcxc−1 exp(−dx) with
(c − 1 −d) as its natural parameters (we use (c  d) as proxy natural parameters). If we assume
gamma distributions for W(l)  b(l)  o(l)  and a(l)  an AE formed by NPN becomes a deep and
nonlinear version of nonnegative matrix factorization [14]. To see this  note that this AE with
activation v(x) = x and zero biases b(l) is equivalent to ﬁnding a factorization of matrix X such that
W(l) where H denotes the middle-layer neurons and W(l) has nonnegative entries
d can be learned

X = H(cid:81)L

c   and b(l)

c   W(l)

d   b(l)

d ) = f−1(o(l)

d2 ) to compute (W(l)

from gamma distributions. In this gamma NPN  parameters W(l)
following Algorithm 1. We detail the algorithm as follows:
Linear Transformation: Since gamma distributions are assumed here  we can use the function
d )  and
f (c  d) = ( c
(o(l)
Nonlinear Transformation: With the proxy natural parameters for the gamma distributions over
o(l)  the mean a(l)
for the nonlinearly transformed distribution over a(l) would
be obtained with Equation (4). Following Theorem 1  closed-form solutions are possible with
v(x) = r(1 − exp(−τ x)) (r = q and ui(x) = x) where r and τ are constants. Using this new
activation function  we have (see Section 2.1 and 6.1 of the supplementary material for details on the
function and derivation):

c   b(l)
s ) during the probabilistic linear transformation in Equation (1)-(3).

m and variance a(l)
s

s ) = f (W(l)

s ) = f (b(l)

d )  (b(l)

m   W(l)

c   W(l)

m   b(l)

m   o(l)

c   o(l)

d   c

am =

po(o|oc  od)v(o)do = r(1 − ooc
Γ(oc)

d

◦ Γ(oc) ◦ (od + τ )−oc ) = r(1 − (

od

od + τ

)oc) 

l= L
2

(cid:90)

as = r2((

od

od + 2τ

)oc − (

od

od + τ

)2oc).

Error: With o(L)

c

and o(L)

d   we can compute the regression error E as the negative log-likelihood:

E = (log Γ(o(L)

c

) − o(L)

c

◦ log o(L)

d − (o(L)

c − 1) ◦ log y + o(L)

d

◦ y)1T  

where y is the observed output corresponding to x. For classiﬁcation  cross-entropy loss can be used
as E. Following the computation ﬂow above  BP can be used to learn W(l)

c   W(l)

d   b(l)

c   and b(l)
d .

5

Figure 1: Predictive distributions for PBP  BDK  dropout NN  and NPN. The shaded regions corre-
spond to ±3 standard deviations. The black curve is the data-generating function and blue curves
show the mean of the predictive distributions. Red stars are the training data.
3.2 Gaussian NPN

Different from the gamma distribution which has support over positive values only  the Gaussian
distribution  also an exponential-family distribution  can describe real-valued random variables. This
makes it a natural choice for NPN. We refer to this NPN variant with Gaussian distributions over both
the weights and neurons as Gaussian NPN. Details of Algorithm 1 for Gaussian NPN are as follows:
Linear Transformation: Besides support over real values  another property of Gaussian distributions
is that the mean and variance can be used as proxy natural parameters  leading to an identity mapping
function f (c  d) = (c  d) which cuts the computation cost. We can use this function to compute
(W(l)
m   o(l)
s )
during the probabilistic linear transformation in Equation (1)-(3).
Nonlinear Transformation: If the sigmoid activation v(x) = σ(x) =
1+exp(−x) is used  am in
Equation (4) would be (convolution of Gaussian with sigmoid is approximated by another sigmoid):

d ) = f−1(o(l)

d )  and (o(l)

s ) = f (W(l)

s ) = f (b(l)

d )  (b(l)

m   W(l)

c   W(l)

m   b(l)

c   b(l)

c   o(l)

1

am =

N (o|oc  diag(od)) ◦ σ(o)do ≈ σ(

oc

) 

(5)

(cid:90)
(cid:90)

2

(1 + ζ 2od) 1
m ≈ σ(

(6)

√

α(oc + β)

) − a2
m 

(1 + ζ 2α2od)1/2

2 + 1)  and ζ 2 = π/8. Similar approximation can be applied for

N (o|oc  diag(od)) ◦ σ(o)2do − a2
√
2  β = − log(

as =
where α = 4 − 2
activation v(x) = tanh(x) since tanh(x) = 2σ(2x) − 1.
If the ReLU activation v(x) = max(0  x) is used  we can use the techniques in [6] to obtain the ﬁrst
two moments of max(z1  z2) where z1 and z2 are Gaussian random variables. Full derivation for
v(x) = σ(x)  v(x) = tanh(x)  and v(x) = max(0  x) is left to the supplementary material.
Error: With o(L)
and o(L)
in the last layer  we can then compute the error E as the KL divergence
d ))(cid:107)N (ym  diag()))  where  is a vector with all entries equal to a small
KL(N (o(L)
d )1T − K log ). For
value . Hence the error E = 1
classiﬁcation tasks  cross-entropy loss is used. Following the computation ﬂow above  BP can be
used to learn W(l)

c − y)T − K + (log o(L)

1T + ( 1
o(L)

  diag(o(L)

2 ( 
o(L)

c   W(l)

)(o(L)

d   b(l)

c   and b(l)
d .

d

d

c

d

c

3.3 Poisson NPN

The Poisson distribution  as another member of the exponential family  is often used to model counts
(e.g.  counts of words  topics  or super topics in documents). Hence for text modeling  it is natural to
assume Poisson distributions for neurons in NPN. Interestingly  this design of Poisson NPN can be
seen as a neural analogue of some Poisson factor analysis models [26].
Besides closed-form nonlinear transformation  another challenge of Poisson NPN is to map the pair
(o(l)
c of Poisson distributions. According to the central limit theorem 
m − 1)2 + 8o(l)
we have o(l)
(2o(l)
s ) (see Section 3 and 6.3 of the supplementary
material for proofs  justiﬁcations  and detailed derivation of Poisson NPN).

s ) to the single parameter o(l)

m − 1 +

4 (2o(l)

m   o(l)

(cid:113)

c = 1

4 Experiments

In this section we evaluate variants of NPN and other state-of-the-art methods on four real-world
datasets. We use Matlab (with GPU) to implement NPN  AE variants  and the ‘vanilla’ NN trained
with dropout SGD (dropout NN). For other baselines  we use the Theano library [2] and MXNet [5].

6

−6−4−20246−100−80−60−40−20020406080100YXTable 2: Test Error Rates on MNIST

Method
Error

BDK
BBB
1.38% 1.34%
Table 3: Test Error Rates for Different Size of Training Data

Dropout1 Dropout2

1.33%

1.40%

gamma NPN Gaussian NPN

1.27%

1.25%

Size
NPN
Dropout
BDK

4.1 Toy Regression Task

500

100

2 000

10 000
29.97% 13.79% 7.89% 3.28%
32.58% 15.39% 8.78% 3.53%
30.08% 14.34% 8.31% 3.55%

To gain some insights into NPN  we start with a toy 1d regression task so that the predicted mean and
variance can be visualized. Following [1]  we generate 20 points in one dimension from a uniform
distribution in the interval [−4  4]. The target outputs are sampled from the function y = x3 + n 
where n ∼ N (0  9). We ﬁt the data with the Gaussian NPN  BDK  and PBP (see the supplementary
material for detailed hyperparameters). Figure 1 shows the predicted mean and variance of NPN 
BDK  and PBP along with the mean provided by the dropout NN (for larger versions of ﬁgures please
refer to the end of the supplementary materials). As we can see  the variance of PBP  BDK  and NPN
diverges as x is farther away from the training data. Both NPN’s and BDK’s predictive distributions
are accurate enough to keep most of the y = x3 curve inside the shaded regions with relatively low
variance. An interesting observation is that the training data points become more scattered when
x > 0. Ideally  the variance should start diverging from x = 0  which is what happens in NPN.
However  PBP and BDK are not sensitive enough to capture this dispersion change. In another dataset 
Boston Housing  the root mean square error for PBP  BDK  and NPN is 3.01  2.82  and 2.57.

4.2 MNIST Classiﬁcation

The MNIST digit dataset consists of 60 000 training images and 10 000 test images. All images
are labeled as one of the 10 digits. We train the models with 50 000 images and use 10 000 images
for validation. Networks with a structure of 784-800-800-10 are used for all methods  since 800
works best for the dropout NN (denoted as Dropout1 in Table 2) and BDK (BDK with a structure of
784-400-400-10 achieves an error rate of 1.41%). We also try the dropout NN with twice the number
of hidden neurons (Dropout2 in Table 2) for fair comparison. For BBB  we directly quote their results
from [4]. We implement BDK and NPN using the same hyperparameters as in [1] whenever possible.
Gaussian priors are used for NPN (see the supplementary material for detailed hyperparameters).
As shown in Table 2  BDK and BBB achieve comparable performance
with dropout NN (similar to [1]  PBP is not included in the comparison
since it supports regression only)  and gamma NPN slightly outperforms
dropout NN. Gaussian NPN is able to achieve a lower error rate of
1.25%. Note that BBB with Gaussian priors can only achieve an error
rate of 1.82%; 1.34% is the result of using Gaussian mixture priors. For
reference  the error rate for dropout NN with 1600 neurons in each hidden
layer is 1.40%. The time cost per epoch is 18.3s  16.2s  and 6.4s for NPN 
BDK  NN respectively. Note that BDK is in C++ and NPN is in Matlab.
To evaluate NPN’s ability as a Bayesian treatment to avoid overﬁtting 
we vary the size of the training set (from 100 to 10 000 data points) and compare the test error rates.
As shown in Table 3  the margin between the Gaussian NPN and dropout NN increases as the training
set shrinks. Besides  to verify the effectiveness of the estimated uncertainty  we split the test set into
9 subsets according NPN’s estimated variance (uncertainty) a(L)
s 1T for each sample and show the
accuracy for each subset in Figure 2. We can ﬁnd that the more uncertain NPN is  the lower the
accuracy  indicating that the estimated uncertainty is well calibrated.

Figure 2: Classiﬁcation accuracy
for different variance (uncertainty).
Note that ‘1’ in the x-axis means
s 1T ∈ [0  0.04)  ‘2’ means
a(L)
s 1T ∈ [0.04  0.08)  etc.
a(L)

4.3 Second-Order Representation Learning

Besides classiﬁcation and regression  we also consider the problem of unsupervised representation
learning with a subsequent link prediction task. Three real-world datasets  Citeulike-a  Citeulike-t 
and arXiv  are used. The ﬁrst two datasets are from [22  23]  collected separately from CiteULike in
different ways to mimic different real-world settings. The third one is from arXiv as one of the SNAP
datasets [15]. Citeulike-a consists of 16 980 documents  8 000 terms  and 44 709 links (citations).

7

12345678900.20.40.60.81VarianceAccuracyMethod
Citeulike-a
Citeulike-t
arXiv

SAE
1104.7
2109.8
4232.7

Table 4: Link Rank on Three Datasets
SDAE
992.4
1356.8
2916.1

gamma NPN
851.7 (935.8)
1342.3 (1400.7)
2796.4 (3038.8)

VAE
980.8
1599.6
3367.2

Gaussian NPN
750.6 (823.9)
1280.4 (1330.7)
2687.9 (2923.8)

Poisson NPN
690.9 (5389.7)
1354.1 (9117.2)
2684.1 (10791.3)

[!h]

Citeulike-t consists of 25 975 documents  20 000 terms  and 32 565 links. The last dataset  arXiv 
consists of 27 770 documents  8 000 terms  and 352 807 links.
The task is to perform unsupervised representation learning before feeding the extracted representa-
tions (middle-layer neurons) into a Bayesian LR algorithm [3]. We use the stacked autoencoder (SAE)
[7]  stacked denoising autoencoder (SDAE) [21]  variational autoencoder (VAE) [13] as baselines
(hyperparameters like weight decay and dropout rate are chosen by cross validation). As in SAE 
we use different variants of NPN to form autoencoders where both the input and output targets are
bag-of-words (BOW) vectors for the documents. The network structure for all models is B-100-50
(B is the number of terms). Please refer to the supplementary material for detailed hyperparameters.
One major advantage of NPN over SAE and SDAE is that the learned repre-
sentations are distributions instead of point estimates. Since representations
from NPN contain both the mean and variance  we call them second-
order representations. Note that although VAE also produces second-order
representations  the variance part is simply parameterized by multilayer
perceptrons while NPN’s variance is naturally computed through propaga-
tion of distributions. These 50-dimensional representations with both mean
and variance are fed into a Bayesian LR algorithm for link prediction (for
deterministic AE the variance is set to 0).
We use links among 80% of the nodes (documents) to train the Bayesian LR and use other links as
the test set. link rank and AUC (area under the ROC curve) are used as evaluation metrics. The link
rank is the average rank of the observed links from test nodes to training nodes. We compute the
AUC for every test node and report the average values. By deﬁnition  lower link rank and higher
AUC indicate better predictive performance and imply more powerful representations.
Table 4 shows the link rank for different models. For fair comparison we also try all baselines with
double budget (a structure of B-200-50) and report whichever has higher accuracy. As we can see  by
treating representations as distributions rather than points in a vector space  NPN is able to achieve
much lower link rank than all baselines  including VAE with variance information. The numbers in
the brackets show the link rank of NPN if we discard the variance information. The performance
gain from variance information veriﬁes the effectiveness of the variance (uncertainty) estimated by
NPN. Among different variants of NPN  the Gaussian NPN seems to perform better in datasets with
fewer words like Citeulike-t (only 18.8 words per document). The Poisson NPN  as a more natural
choice to model text  achieves the best performance in datasets with more words (Citeulike-a and
arXiv). The performance in AUC is consistent with that in terms of the link rank (see Section 4 of the
supplementary material). To further verify the effectiveness of the estimated uncertainty  we plot the
reconstruction error and the variance o(L)
s 1T for each data point of Citeulike-a in Figure 3. As we
can see  higher uncertainty often indicates not only higher reconstruction error E but also higher
variance in E.

Figure 3: Reconstruction error
and estimated uncertainty for
each data point in Citeulike-a.

5 Conclusion

We have introduced a family of models  called natural-parameter networks  as a novel class of proba-
bilistic NN to combine the merits of NN and PGM. NPN regards the weights and neurons as arbitrary
exponential-family distributions rather than just point estimates or factorized Gaussian distributions.
Such ﬂexibility enables richer descriptions of hierarchical relationships among latent variables and
adds another degree of freedom to customize NN for different types of data. Efﬁcient sampling-free
backpropagation-compatible algorithms are designed for the learning of NPN. Experiments show that
NPN achieves state-of-the-art performance on classiﬁcation  regression  and representation learning
tasks. As possible extensions of NPN  it would be interesting to connect NPN to arbitrary PGM to
form fully Bayesian deep learning models [24  25]  allowing even richer descriptions of relationships
among latent variables. It is also worth noting that NPN cannot be deﬁned as generative models
and  unlike PGM  the same NPN model cannot be used to support multiple types of inference (with
different observed and hidden variables). We will try to address these limitations in our future work.

8

05101520050100150200250300350VarianceReconstruction errorReferences
[1] A. K. Balan  V. Rathod  K. P. Murphy  and M. Welling. Bayesian dark knowledge. In NIPS  2015.

[2] F. Bastien  P. Lamblin  R. Pascanu  J. Bergstra  I. J. Goodfellow  A. Bergeron  N. Bouchard  and Y. Bengio.
Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS
2012 Workshop  2012.

[3] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York  Inc.  Secaucus  NJ 

USA  2006.

[4] C. Blundell  J. Cornebise  K. Kavukcuoglu  and D. Wierstra. Weight uncertainty in neural network. In

ICML  2015.

[5] T. Chen  M. Li  Y. Li  M. Lin  N. Wang  M. Wang  T. Xiao  B. Xu  C. Zhang  and Z. Zhang. Mxnet: A ﬂex-
ible and efﬁcient machine learning library for heterogeneous distributed systems. CoRR  abs/1512.01274 
2015.

[6] C. E. Clark. The greatest of a ﬁnite set of random variables. Operations Research  9(2):145–162  1961.

[7] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. Book in preparation for MIT Press  2016.

[8] A. Graves. Practical variational inference for neural networks. In NIPS  2011.

[9] R. Henao  Z. Gan  J. Lu  and L. Carin. Deep poisson factor modeling. In NIPS  2015.

[10] J. M. Hernández-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of Bayesian

neural networks. In ICML  2015.

[11] G. E. Hinton and D. Van Camp. Keeping the neural networks simple by minimizing the description length

of the weights. In COLT  1993.

[12] A. Karpathy and F. Li. Deep visual-semantic alignments for generating image descriptions. In CVPR 

2015.

[13] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. CoRR  abs/1312.6114  2013.

[14] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS  2001.

[15] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.

stanford.edu/data  June 2014.

[16] J. MacKay David. A practical Bayesian framework for backprop networks. Neural computation  1992.

[17] R. M. Neal. Learning stochastic feedforward networks. Department of Computer Science  University of

Toronto  1990.

[18] R. M. Neal. Bayesian learning for neural networks. PhD thesis  University of Toronto  1995.

[19] R. Ranganath  L. Tang  L. Charlin  and D. M. Blei. Deep exponential families. In AISTATS  2015.

[20] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int. J. Approx. Reasoning  50(7):969–978  2009.

[21] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P.-A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. JMLR  11:3371–3408 
2010.

[22] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientiﬁc articles. In KDD 

2011.

[23] H. Wang  B. Chen  and W.-J. Li. Collaborative topic regression with social regularization for tag recom-

mendation. In IJCAI  2013.

[24] H. Wang  N. Wang  and D. Yeung. Collaborative deep learning for recommender systems. In KDD  2015.

[25] H. Wang and D. Yeung. Towards Bayesian deep learning: A framework and some existing methods. TKDE 

2016  to appear.

[26] M. Zhou  L. Hannah  D. B. Dunson  and L. Carin. Beta-negative binomial process and poisson factor

analysis. In AISTATS  2012.

9

,Hao Wang
Xingjian SHI
Dit-Yan Yeung