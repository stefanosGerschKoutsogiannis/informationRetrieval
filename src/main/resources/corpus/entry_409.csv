2016,Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy,We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each training set with only one iteration of Newton's method. We show theoretically that we can iteratively increase the sample size while applying single Newton iterations without line search and staying within the statistical accuracy of the regularized empirical risk. In particular  we can double the size of the training set in each iteration when the number of samples is sufficiently large. Numerical experiments on various datasets confirm the possibility of increasing the sample size by factor 2 at each iteration which implies that Ada Newton achieves the statistical accuracy of the full training set with about two passes over the dataset.,Adaptive Newton Method for Empirical Risk

Minimization to Statistical Accuracy

Aryan Mokhtari?

University of Pennsylvania
aryanm@seas.upenn.edu

Hadi Daneshmand?

ETH Zurich  Switzerland

hadi.daneshmand@inf.ethz.ch

Aurelien Lucchi

ETH Zurich  Switzerland
aurelien.lucchi@inf.ethz.ch

Thomas Hofmann

ETH Zurich  Switzerland

thomas.hofmann@inf.ethz.ch

Alejandro Ribeiro

University of Pennsylvania
aribeiro@seas.upenn.edu

Abstract

We consider empirical risk minimization for large-scale datasets. We introduce
Ada Newton as an adaptive algorithm that uses Newton’s method with adaptive
sample sizes. The main idea of Ada Newton is to increase the size of the train-
ing set by a factor larger than one in a way that the minimization variable for
the current training set is in the local neighborhood of the optimal argument of
the next training set. This allows to exploit the quadratic convergence property
of Newton’s method and reach the statistical accuracy of each training set with
only one iteration of Newton’s method. We show theoretically that we can iter-
atively increase the sample size while applying single Newton iterations without
line search and staying within the statistical accuracy of the regularized empirical
risk. In particular  we can double the size of the training set in each iteration when
the number of samples is sufﬁciently large. Numerical experiments on various
datasets conﬁrm the possibility of increasing the sample size by factor 2 at each
iteration which implies that Ada Newton achieves the statistical accuracy of the
full training set with about two passes over the dataset.1

1

Introduction

A hallmark of empirical risk minimization (ERM) on large datasets is that evaluating descent direc-
tions requires a complete pass over the dataset. Since this is undesirable due to the large number of
training samples  stochastic optimization algorithms with descent directions estimated from a subset
of samples are the method of choice. First order stochastic optimization has a long history [19  17]
but the last decade has seen fundamental progress in developing alternatives with faster convergence.
A partial list of this consequential literature includes Nesterov acceleration [16  2]  stochastic aver-
aging gradient [20  6]  variance reduction [10  26]  and dual coordinate methods [23  24].
When it comes to stochastic second order methods the ﬁrst challenge is that while evaluation of
Hessians is as costly as evaluation of gradients  the stochastic estimation of Hessians has proven
more challenging. This difﬁculty is addressed by incremental computations in [9] and subsampling
in [7] or circumvented altogether in stochastic quasi-Newton methods [21  12  13  11  14]. Despite
this incipient progress it is nonetheless fair to say that the striking success in developing stochastic
ﬁrst order methods is not matched by equal success in the development of stochastic second order
methods. This is because even if the problem of estimating a Hessian is solved there are still four
challenges left in the implementation of Newton-like methods in ERM:

1?The ﬁrst two authors have contributed equally in this work.

(i) Global convergence of Newton’s method requires implementation of a line search subrou-

tine and line searches in ERM require a complete pass over the dataset.

(ii) The quadratic convergence advantage of Newton’s method manifests close to the optimal

solution but there is no point in solving ERM problems beyond their statistical accuracy.

(iii) Newton’s method works for strongly convex functions but loss functions are not strongly

convex for many ERM problems of practical importance.

(iv) Newton’s method requires inversion of Hessians which is costly in large dimensional ERM.

Because stochastic Newton-like methods can’t use line searches [cf. (i)]  must work on problems
that may be not strongly convex [cf. (iii)]  and never operate very close to the optimal solution [cf
(ii)]  they never experience quadratic convergence. They do improve convergence constants and  if
efforts are taken to mitigate the cost of inverting Hessians [cf. (iv)] as in [21  12  7  18] they result
in faster convergence. But since they still converge at linear rates they do not enjoy the foremost
beneﬁts of Newton’s method.
In this paper we attempt to circumvent (i)-(iv) with the Ada Newton algorithm that combines the use
of Newton iterations with adaptive sample sizes [5]. Say the total number of available samples is
N  consider subsets of n  N samples  and suppose the statistical accuracy of the ERM associated
with n samples is Vn (Section 2). In Ada Newton we add a quadratic regularization term of order Vn
to the empirical risk – so that the regularized risk also has statistical accuracy Vn – and assume that
for a certain initial sample size m0  the problem has been solved to its statistical accuracy Vm0. The
sample size is then increased by a factor ↵> 1 to n = ↵m0. We proceed to perform a single Newton
iteration with unit stepsize and prove that the result of this update solves this extended ERM problem
to its statistical accuracy (Section 3). This permits a second increase of the sample size by a factor
↵ and a second Newton iteration that is likewise guaranteed to solve the problem to its statistical
accuracy. Overall  this permits minimizing the empirical risk in ↵/(↵  1) passes over the dataset
and inverting log↵ N Hessians. Our theoretical results provide a characterization of the values of
↵ that are admissible with respect to different problem parameters (Theorem 1). In particular  we
show that asymptotically on the number of samples n and with proper parameter selection we can set
↵ = 2 (Proposition 2). In such case we can optimize to within statistical accuracy in about 2 passes
over the dataset and after inversion of about 3.32 log10 N Hessians. Our numerical experiments
verify that ↵ = 2 is a valid factor for increasing the size of the training set at each iteration while
performing a single Newton iteration for each value of the sample size.

2 Empirical risk minimization

We aim to solve ERM problems to their statistical accuracy. To state this problem formally consider
an argument w 2 Rp  a random variable Z with realizations z and a convex loss function f (w; z).
We want to ﬁnd an argument w⇤ that minimizes the statistical average loss L(w) := EZ[f (w  Z)] 
(1)

L(w) = argmin

w⇤ := argmin

EZ[f (w  Z)].

w

w

The loss in (1) can’t be evaluated because the distribution of Z is unknown. We have  however 
access to a training set T = {z1  . . .   zN} containing N independent samples z1  . . .   zN that we
can use to estimate L(w). We therefore consider a subset Sn ✓T and settle for minimization of the
empirical risk Ln(w) := (1/n)Pn

k=1 f (w  zk) 

w†n := argmin

Ln(w) = argmin

f (w  zk) 

w

w

where  without loss of generality  we have assumed Sn = {z1  . . .   zn} contains the ﬁrst n elements
of T . The difference between the empirical risk in (2) and the statistical loss in (1) is a fundamental
quantities in statistical learning. We assume here that there exists a constant Vn  which depends on
the number of samples n  that upper bounds their difference for all w with high probability (w.h.p) 
(3)

w.h.p.

w |L(w)  Ln(w)| Vn 
sup

That the statement in (3) holds with w.h.p means that there exists a constant  such that the inequality
holds with probability at least 1  . The constant Vn depends on  but we keep that dependency

2

1
n

nXk=1

(2)

implicit to simplify notation. For subsequent discussions  observe that bounds Vn of order Vn =
O(1/pn) date back to the seminal work of Vapnik – see e.g.  [25  Section 3.4]. Bounds of order
Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not
uncommon in practice  [1  8  3].
An important consequence of (1) is that there is no point in solving (2) to an accuracy higher than Vn.
Indeed  if we ﬁnd a variable w for which Ln(wn)Ln(w†)  Vn ﬁnding a better approximation of
w† is moot because (3) implies that this is not necessarily a better approximation of the minimizer
w⇤ of the statistical loss. We say the variable wn solves the ERM problem in (2) to within its
statistical accuracy. In particular  this implies that adding a regularization of order Vn to (2) yields
a problem that is essentially equivalent. We can then consider a quadratic regularizer of the form
cVn/2kwk2 to deﬁne the regularized empirical risk Rn(w) := Ln(w) + (cVn/2)kwk2 and the
corresponding optimal argument

w⇤n := argmin

w

Rn(w) = argmin

w

Ln(w) +

cVn
2 kwk2.

(4)

Since the regularization in (4) is of order Vn and (3) holds  the difference between Rn(w⇤n) and
L(w⇤) is also of order Vn – this may be not as immediate as it seems; see [22]. Thus  we can
say that a variable wn satisfying Rn(wn)  Rn(w⇤n)  Vn solves the ERM problem to within its
statistical accuracy. We accomplish this goal in this paper with the Ada Newton algorithm which we
introduce in the following section.

3 Ada Newton
To solve (4) suppose the problem has been solved to within its statistical accuracy for a set Sm ⇢
Sn with m = n/↵ samples where ↵> 1. Therefore  we have found a variable wm for which
Rm(wm)  Rm(w⇤m)  Vm. Our goal is to update wm using the Newton step in a way that the
updated variable wn estimates w⇤n with accuracy Vn. To do so compute the gradient of the risk Rn
evaluated at wm

rRn(wm) =

1
n

nXk=1

rf (wm  zk) + cVnwm 

as well as the Hessian Hn of Rn evaluated at wm

r2f (wm  zk) + cVnI 
and update wm with the Newton step of the regularized risk Rn to compute

Hn := r2Rn(wm) =

1
n

nXk=1
wn = wm  H1

(5)

(6)

(8)

n rRn(wm).

(7)
Note that the stepsize of the Newton update in (7) is 1  which avoids line search algorithms requiring
extra computation. The main contribution of this paper is to derive a condition that guarantees that
wn solves Rn to within its statistical accuracy Vn. To do so  we ﬁrst assume the following conditions
are satisﬁed.
Assumption 1. The loss functions f (w  z) are convex with respect to w for all values of z. More-
over  their gradients rf (w  z) are Lipschitz continuous with constant M

krf (w  z)  rf (w0  z)k  Mkw  w0k 

for all z.

Assumption 2. The loss functions f (w  z) are self-concordant with respect to w for all z.
Assumption 3. The difference between the gradients of the empirical loss Ln and the statistical
average loss L is bounded by V 1/2

for all w with high probability 

n

w krL(w)  rLn(w)k  V 1/2
n  
sup

w.h.p.

(9)

The conditions in Assumption 1 imply that the average loss L(w) and the empirical loss Ln(w)
are convex and their gradients are Lipschitz continuous with constant M. Thus  the empirical risk

3

Algorithm 1 Ada Newton
1: Parameters: Sample size increase constants ↵0 > 1 and 0 << 1.
2: Input: Initial sample size n = m0 and argument wn = wm0 with krRn(wn)k < (p2c)Vn
3: while n  N do {main loop}
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end while

Update argument and index: wm = wn and m = n. Reset factor ↵ = ↵0 .
repeat {sample size backtracking loop}
Increase sample size: n = min{↵m  N}.
Compute gradient [cf. (5)]: rRn(wm) = (1/n)Pn
Compute Hessian [cf. (6)]: Hn = (1/n)Pn
Newton Update [cf. (7)]: wn = wm  H1
Compute gradient [cf. (5)]: rRn(wn) = (1/n)Pn
Backtrack sample size increase ↵ = ↵.
until krRn(wn)k < (p2c)Vn

k=1 r2f (wm  zk) + cVnI
n rRn(wm)

k=1 rf (wm  zk) + cVnwm

k=1 rf (wn  zk) + cVnwn

n = O(1/pn). This is a conservative rate for the law of large numbers.

Rn(w) is strongly convex with constant cVn and its gradients rRn(w) are Lipschitz continuous
with parameter M + cVn. Likewise  the condition in Assumption 2 implies that the average loss
L(w)  the empirical loss Ln(w)  and the empirical risk Rn(w) are also self-concordant. The condi-
tion in Assumption 3 says that the gradients of the empirical risk converge to their statistical average
at a rate of order V 1/2
n . If the constant Vn in condition (3) is of order not faster than O(1/n) the
condition in Assumption 3 holds if the gradients converge to their statistical average at a rate of
order V 1/2
In the following theorem  given Assumptions 1-3  we state a condition that guarantees the variable
wn evaluated as in (7) solves Rn to within its statistical accuracy Vn.
Theorem 1. Consider the variable wm as a Vm-optimal solution of the risk Rm  i.e.  a solution
such that Rm(wm)  Rm(w⇤m)  Vm. Let n = ↵m > m  consider the risk Rn associated with
sample set Sn S m  and suppose assumptions 1 - 3 hold. If the sample size n is chosen such that
(10)

2(n  m)

+

1
4



and

cVn

✓ 2(M + cVm)Vm
144✓Vm +

n

2(n  m)

◆1/2

nc1/2 +(2 + p2)c1/2 + ckw⇤k (Vm  Vn)
kw⇤k2◆2

c(Vm  Vn)

(cVn)1/2

2

(Vnm + Vm) + 2 (Vm  Vn) +

 Vn

(11)

are satisﬁed  then the variable wn  which is the outcome of applying one Newton step on the variable
wm as in (7)  has sub-optimality error Vn with high probability  i.e. 
w.h.p.

(12)

Rn(wn)  Rn(w⇤n)  Vn 

Proof. See Section 4.

Theorem 1 states conditions under which we can iteratively increase the sample size while applying
single Newton iterations without line search and staying within the statistical accuracy of the regu-
larized empirical risk. The constants in (10) and (11) are not easy to parse but we can understand
them qualitatively if we focus on large m. This results in a simpler condition that we state next.
Proposition 2. Consider a learning problem in which the statistical accuracy satisﬁes Vm  ↵Vn
for n = ↵m and limn!1 Vn = 0. If the regularization constant c is chosen so that

✓ 2↵M
c ◆1/2

+

2(↵  1)
↵c1/2 <

1
4

 

(13)

then  there exists a sample size ˜m such that (10) and (11) are satisﬁed for all m > ˜m and n = ↵m.
In particular  if ↵ = 2 we can satisfy (10) and (11) with c > 16(2pM + 1)2.

4

Proof. That the condition in (11) is satisﬁed for all m > ˜m follows simply because the left hand side
m and the right hand side is of order Vn. To show that the condition in (10) is satisﬁed
is of order V 2
for sufﬁciently large m observe that the third summand in (10) is of order O((Vm  Vn)/V 1/2
n )
and vanishes for large m. In the second summand of (10) we make n = ↵m to obtain the second
summand in (13) and in the ﬁrst summand replace the ratio Vm/Vn by its bound ↵ to obtain the ﬁrst
summand of (13). To conclude the proof just observe that the inequality in (13) is strict.
The condition Vm  ↵Vn is satisﬁed if Vn = 1/n and is also satisﬁed if Vn = 1/pn because
p↵<↵ . This means that for most ERM problems we can progress geometrically over the sample
size and arrive at a solution wN that solves the ERM problem RN to its statistical accuracy VN as
long as (13) is satisﬁed .
The result in Theorem 1 motivates deﬁnition of the Ada Newton algorithm that we summarize in
Algorithm 1. The core of the algorithm is in steps 6-9. Step 6 implements an increase in the
sample size by a factor ↵ and steps 7-9 implement the Newton iteration in (5)-(7). The required
input to the algorithm is an initial sample size m0 and a variable wm0 that is known to solve the
ERM problem with accuracy Vm0. Observe that this initial iterate doesn’t have to be computed
with Newton iterations. The initial problem to be solved contains a moderate number of samples
m0  a mild condition number because it is regularized with constant cVm0  and is to be solved to a
moderate accuracy Vm0 – recall that Vm0 is of order Vm0 = O(1/m0) or order Vm0 = O(1/pm0)
depending on regularity assumptions. Stochastic ﬁrst order methods excel at solving problems with
moderate number of samples m0 and moderate condition to moderate accuracy.
We remark that the conditions in Theorem 1 and Proposition 2 are conceptual but that the constants
involved are unknown in practice. In particular  this means that the allowed values of the factor ↵ that
controls the growth of the sample size are unknown a priori. We solve this problem in Algorithm 1
by backtracking the increase in the sample size until we guarantee that wn minimizes the empirical
risk Rn(wn) to within its statistical accuracy. This backtracking of the sample size is implemented
in Step 11 and the optimality condition of wn is checked in Step 12. The condition in Step 12 is
on the gradient norm that  because Rn is strongly convex  can be used to bound the suboptimality
Rn(wn)  Rn(w⇤n) as

(14)

Rn(wn)  Rn(w⇤n) 

1
2cVnkrRn(wn)k2.

Observe that checking this condition requires an extra gradient computation undertaken in Step
10. That computation can be reused in the computation of the gradient in Step 5 once we exit
the backtracking loop. We emphasize that when the condition in (13) is satisﬁed  there exists a
sufﬁciently large m for which the conditions in Theorem 1 are satisﬁed for n = ↵m. This means
that the backtracking condition in Step 12 is satisﬁed after one iteration and that  eventually  Ada
Newton progresses by increasing the sample size by a factor ↵. This means that Algorithm 1 can
be thought of as having a damped phase where the sample size increases by a factor smaller than ⇢
and a geometric phase where the sample size grows by a factor ⇢ in all subsequent iterations. The
computational cost of this geometric phase is of not more than ↵/(↵  1) passes over the dataset
and requires inverting not more than log↵ N Hessians. If c > 16(2pM + 1)2  we make ↵ = 2
for optimizing to within statistical accuracy in about 2 passes over the dataset and after inversion of
about 3.32 log10 N Hessians.

4 Convergence Analysis

In this section we study the proof of Theorem 1. The main idea of the Ada Newton algorithm is
introducing a policy for increasing the size of training set from m to n in a way that the current
variable wm is in the Newton quadratic convergence phase for the next regularized empirical risk
Rn. In the following proposition  we characterize the required condition to guarantee staying in the
local neighborhood of Newton’s method.
Proposition 3. Consider the sets Sm and Sn as subsets of the training set T such that Sm ⇢S n ⇢
T . We assume that the number of samples in the sets Sm and Sn are m and n  respectively. Further 
deﬁne wm as an Vm optimal solution of the risk Rm  i.e.  Rm(wm)  Rm(w⇤m)  Vm. In addition 
deﬁne n(w) := rRn(w)Tr2Rn(w)1rRn(w)1/2 as the Newton decrement of variable w

5

associated with the risk Rn. If Assumption 1-3 hold  then Newton’s method at point wm is in the
quadratic convergence phase for the objective function Rn  i.e.  n(wm) < 1/4  if we have
1
4

n + (p2c + 2pc + ckw⇤k)(Vm  Vn)

(2(n  m)/n)V 1/2

✓ 2(M + cVm)Vm

◆1/2

(cVn)1/2

cVn

+



w.h.p.
(15)

Proof. See Section 7.1 in the supplementary material.

From the analysis of Newton’s method we know that if the Newton decrement n(w) is smaller
than 1/4  the variable w is in the local neighborhood of Newton’s method; see e.g.  Chapter 9 of [4].
From the result in Proposition 3  we obtain a sufﬁcient condition to guarantee that n(wm) < 1/4
which implies that wm  which is a Vm optimal solution for the regularized empirical loss Rm  i.e. 
Rm(wm)  Rm(w⇤m)  Vm  is in the local neighborhood of the optimal argument of Rn that
Newton’s method converges quadratically.
Unfortunately  the quadratic convergence of Newton’s method for self-concordant functions is in
terms of the Newton decrement n(w) and it does not necessary guarantee quadratic conver-
gence in terms of objective function error. To be more precise  we can show that n(wn) 
n(wm)2; however  we can not conclude that the quadratic convergence of Newton’s method
implies Rn(wn)  Rn(w⇤n)  0(Rn(wm)  Rn(w⇤n))2.
In the following proposition we try
to characterize an upper bound for the error Rn(wn)  Rn(w⇤n) in terms of the squared error
(Rn(wm)  Rn(w⇤n))2 using the quadratic convergence property of Newton decrement.
Proposition 4. Consider wm as a variable that is in the local neighborhood of the optimal argument
of the risk Rn where Newton’s method has a quadratic convergence rate  i.e.  n(wm)  1/4. Re-
call the deﬁnition of the variable wn in (7) as the updated variable using Newton step. If Assumption
1 and 2 hold  then the difference Rn(wn)  Rn(w⇤n) is upper bounded by
Rn(wn)  Rn(w⇤n)  144(Rn(wm)  Rn(w⇤n))2.

(16)

Proof. See Section 7.2 in the supplementary material.

The result in Proposition 4 provides an upper bound for the sub-optimality Rn(wn)  Rn(w⇤n) in
terms of the sub-optimality of variable wm for the risk Rn  i.e.  Rn(wm) Rn(w⇤n). Recall that we
know that wm is in the statistical accuracy of Rm  i.e.  Rm(wm)  Rm(w⇤m)  Vm  and we aim to
show that the updated variable wn stays in the statistical accuracy of Rn  i.e.  Rn(wn) Rn(w⇤n) 
Vn. This can be done by showing that the upper bound for Rn(wn)  Rn(w⇤n) in (16) is smaller
than Vn. We proceed to derive an upper bound for the sub-optimality Rn(wm)  Rn(w⇤n) in the
following proposition.
Proposition 5. Consider the sets Sm and Sn as subsets of the training set T such that Sm ⇢S n ⇢
T . We assume that the number of samples in the sets Sm and Sn are m and n  respectively. Further 
deﬁne wm as an Vm optimal solution of the risk Rm  i.e.  Rm(wm)  R⇤m  Vm. If Assumption
1-3 hold  then the empirical risk error Rn(wm) Rn(w⇤n) of the variable wm corresponding to the
set Sn is bounded above by
Rn(wm)Rn(w⇤n)  Vm+

(Vnm + Vm)+2 (Vm  Vn)+

c(Vm  Vn)

2(n  m)

n

2

kw⇤k2 w.h.p.
(17)

Proof. See Section 7.3 in the supplementary material.

The result in Proposition 5 characterizes the sub-optimality of the variable wm  which is an Vm
sub-optimal solution for the risk Rm  with respect to the empirical risk Rn associated with the set
Sn.
The results in Proposition 3  4  and 5 lead to the result in Theorem 1. To be more precise  from the
result in Proposition 3 we obtain that the condition in (10) implies that wm is in the local neigh-
borhood of the optimal argument of Rn and n(wm)  1/4. Hence  the hypothesis of Proposition
4 is satisﬁed and we have Rn(wn)  Rn(w⇤n)  144(Rn(wm)  Rn(w⇤n))2. This result paired
with the result in Proposition 5 shows that if the condition in (11) is satisﬁed we can conclude that
Rn(wn)  Rn(w⇤n)  Vn which completes the proof of Theorem 1.

6

∗ N
R
−

)

w

(
N
R

0

10

-2

10

-4

10

-6

10

-8

10

-10

10

SGD
SAGA
Newton
Ada Newton

∗ N
R
−

)

w

(
N
R

0

10

-2

10

-4

10

-6

10

-8

10

-10

10

SGD
SAGA
Newton
Ada Newton

0

5

10
15
Number of passes

20

25

0

10

20

30

40
50
Runtime (s)

60

70

80

90

Figure 1: Comparison of SGD  SAGA  Newton  and Ada Newton in terms of number of effective
passes over dataset (left) and runtime (right) for the protein homology dataset.

5 Experiments

In this section  we study the performance of Ada Newton and compare it with state-of-the-art in
solving a large-scale classiﬁcation problem. In the main paper we only use the protein homology
dataset provided on KDD cup 2004 website. Further numerical experiments on various datasets
can be found in Section 7.4 in the supplementary material. The protein homology dataset contains
N = 145751 samples and the dimension of each sample is p = 74. We consider three algorithms
to compare with the proposed Ada Newton method. One of them is the classic Newton’s method
with backtracking line search. The second algorithm is Stochastic Gradient Descent (SGD) and the
last one is the SAGA method introduced in [6]. In our experiments  we use logistic loss and set the
regularization parameters as c = 200 and Vn = 1/n.
The stepsize of SGD in our experiments is 2⇥ 102. Note that picking larger stepsize leads to faster
but less accurate convergence and choosing smaller stepsize improves the accuracy convergence
with the price of slower convergence rate. The stepsize for SAGA is hand-optimized and the best
performance has been observed for ↵ = 0.2 which is the one that we use in the experiments.
For Newton’s method  the backtracking line search parameters are ↵ = 0.4 and  = 0.5. In the
implementation of Ada Newton we increase the size of the training set by factor 2 at each iteration 

i.e.  ↵ = 2 and we observe that the condition krRn(wn)k < (p2c)Vn is always satisﬁed and there

is no need for reducing the factor ↵. Moreover  the size of initial training set is m0 = 124. For
the warmup step that we need to get into to the quadratic neighborhood of Newton’s method we
use the gradient descent method. In particular  we run gradient descent with stepsize 103 for 100
iterations. Note that since the number of samples is very small at the beginning  m0 = 124  and the
regularizer is very large  the condition number of problem is very small. Thus  gradient descent is
able to converge to a good neighborhood of the optimal solution in a reasonable time. Notice that
the computation of this warm up process is very low and is equal to 12400 gradient evaluations.
This number of samples is less than 10% of the full training set. In other words  the cost is less than
10% of one pass over the dataset. Although  this cost is negligible  we consider it in comparison
with SGD  SAGA  and Newton’s method. We would like to mention that other algorithms such as
Newton’s method and stochastic algorithms can also be used for the warm up process; however  the
gradient descent method sounds the best option since the gradient evaluation is not costly and the
problem is well-conditioned for a small training set .
The left plot in Figure 1 illustrates the convergence path of SGD  SAGA  Newton  and Ada Newton
for the protein homology dataset. Note that the x axis is the total number of samples used divided
by the size of the training set N = 145751 which we call number of passes over the dataset. As
we observe  The best performance among the four algorithms belongs to Ada Newton. In particular 
Ada Newton is able to achieve the accuracy of RN (w)  R⇤N < 1/N by 2.4 passes over the dataset
which is very close to theoretical result in Theorem 1 that guarantees accuracy of order O(1/N )
after ↵/(↵  1) = 2 passes over the dataset. To achieve the same accuracy of 1/N Newton’s
method requires 7.5 passes over the dataset  while SAGA needs 10 passes. The SGD algorithm can
not achieve the statistical accuracy of order O(1/N ) even after 25 passes over the dataset.
Although  Ada Newton and Newton outperform SAGA and SGD  their computational complexity
are different. We address this concern by comparing the algorithms in terms of runtime. The right

7

plot in Figure 1 demonstrates the convergence paths of the considered methods in terms of runtime.
As we observe  Newton’s method requires more time to achieve the statistical accuracy of 1/N
relative to SAGA. This observation justiﬁes the belief that Newton’s method is not practical for
large-scale optimization problems  since by enlarging p or making the initial solution worse the
performance of Newton’s method will be even worse than the ones in Figure 1. Ada Newton resolves
this issue by starting from small sample size which is computationally less costly. Ada Newton
also requires Hessian inverse evaluations  but the number of inversions is proportional to log↵ N.
Moreover  the performance of Ada Newton doesn’t depend on the initial point and the warm up
process is not costly as we described before. We observe that Ada Newton outperforms SAGA
signiﬁcantly. In particular it achieves the statistical accuracy of 1/N in less than 25 seconds  while
SAGA achieves the same accuracy in 62 seconds. Note that since the variable wN is in the quadratic
neighborhood of Newton’s method for RN the convergence path of Ada Newton becomes quadratic
eventually when the size of the training set becomes equal to the size of the full dataset. It follows
that the advantage of Ada Newton with respect to SAGA is more signiﬁcant if we look for a sub-
optimality less than Vn. We have observed similar performances for other datasets such as A9A 
W8A  COVTYPE  and SUSY – see Section 7.4 in the supplementary material.

6 Discussions

As explained in Section 4  Theorem 1 holds because condition (10) makes wm part of the quadratic
convergence region of Rn. From this fact  it follows that the Newton iteration makes the subopti-
mality gap Rn(wn) Rn(w⇤n) the square of the suboptimality gap Rn(wm) Rn(w⇤n). This yields
condition (11) and is the fact that makes Newton steps valuable in increasing the sample size. If we
replace Newton iterations by any method with linear convergence rate  the orders of both sides on
condition (11) are the same. This would make aggressive increase of the sample size unlikely.
In Section 1 we pointed out four reasons that challenge the development of stochastic Newton meth-
ods. It would not be entirely accurate to call Ada Newton a stochastic method because it doesn’t
rely on stochastic descent directions. It is  nonetheless  a method for ERM that makes pithy use of
the dataset. The challenges listed in Section 1 are overcome by Ada Newton because:

(i) Ada Newton does not use line searches. Optimality improvement is guaranteed by increas-

ing the sample size.

(ii) The advantages of Newton’s method are exploited by increasing the sample size at a rate
that keeps the solution for sample size m in the quadratic convergence region of the risk
associated with sample size n = ↵m. This allows aggressive growth of the sample size.

(iii) The ERM problem is not necessarily strongly convex. A regularization of order Vn is added

to construct the empirical risk Rn

(iv) Ada Newton inverts approximately log↵ N Hessians. To be more precise  the total number
of inversion could be larger than log↵ N because of the backtracking step. However  the
backtracking step is bypassed when the number of samples is sufﬁciently large.

It is fair to point out that items (ii) and (iv) are true only to the extent that the damped phase in
Algorithm 1 is not signiﬁcant. Our numerical experiments indicate that this is true but the conclusion
is not warranted by out theoretical bounds except when the dataset is very large. This suggests the
bounds are loose and that further research is warranted to develop tighter bounds.

References
[1] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-

lems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[3] L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural Informa-

tion Processing Systems 20  Vancouver  British Columbia  Canada  pages 161–168  2007.

[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press  New York 

NY  USA  2004.

8

[5] Hadi Daneshmand  Aur´elien Lucchi  and Thomas Hofmann. Starting small - learning with adaptive
sample sizes. In Proceedings of the 33nd International Conference on Machine Learning  ICML 2016 
New York City  NY  USA  pages 1463–1471  2016.

[6] Aaron Defazio  Francis R. Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information Process-
ing Systems 27  Montreal  Quebec  Canada  pages 1646–1654  2014.

[7] Murat A. Erdogdu and Andrea Montanari. Convergence rates of sub-sampled Newton methods. In Ad-
vances in Neural Information Processing Systems 28: Annual Conference on Neural Information Pro-
cessing Systems 2015  Montreal  Quebec  Canada  pages 3052–3060  2015.

[8] Roy Frostig  Rong Ge  Sham M. Kakade  and Aaron Sidford. Competing with the empirical risk mini-
mizer in a single pass. In Proceedings of The 28th Conference on Learning Theory  COLT 2015  Paris 
France  July 3-6  2015  pages 728–763  2015.

[9] Mert G¨urb¨uzbalaban  Asuman Ozdaglar  and Pablo Parrilo. A globally convergent incremental Newton

method. Mathematical Programming  151(1):283–313  2015.

[10] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduc-
tion. In Advances in Neural Information Processing Systems 26. Lake Tahoe  Nevada  United States. 
pages 315–323  2013.

[11] Aurelien Lucchi  Brian McWilliams  and Thomas Hofmann. A variance reduced stochastic newton

method. arXiv  2015.

[12] Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE Transactions

on Signal Processing  62(23):6089–6104  2014.

[13] Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory BFGS. Journal of

Machine Learning Research  16:3151–3181  2015.

[14] Philipp Moritz  Robert Nishihara  and Michael I. Jordan. A linearly-convergent stochastic L-BFGS algo-
rithm. Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics  AISTATS
2016  Cadiz  Spain  pages 249–258  2016.

[15] Yu Nesterov. Introductory Lectures on Convex Programming Volume I: Basic course. Citeseer  1998.
[16] Yurii Nesterov et al. Gradient methods for minimizing composite objective function. 2007.
[17] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM

Journal on Control and Optimization  30(4):838–855  1992.

[18] Zheng Qu  Peter Richt´arik  Martin Tak´ac  and Olivier Fercoq. SDNA: stochastic dual Newton ascent for
empirical risk minimization. In Proceedings of the 33nd International Conference on Machine Learning 
ICML 2016  New York City  NY  USA  June 19-24  2016  pages 1823–1832  2016.

[19] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  pages 400–407  1951.

[20] Nicolas Le Roux  Mark W. Schmidt  and Francis R. Bach. A stochastic gradient method with an expo-
nential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems
25. Lake Tahoe  Nevada  United States.  pages 2672–2680  2012.

[21] Nicol N. Schraudolph  Jin Yu  and Simon G¨unter. A stochastic quasi-Newton method for online convex
In Proceedings of the Eleventh International Conference on Artiﬁcial Intelligence and

optimization.
Statistics  AISTATS 2007  San Juan  Puerto Rico  pages 436–443  2007.

[22] Shai Shalev-Shwartz  Ohad Shamir  Nathan Srebro  and Karthik Sridharan. Learnability  stability and

uniform convergence. The Journal of Machine Learning Research  11:2635–2670  2010.

[23] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss.

The Journal of Machine Learning Research  14:567–599  2013.

[24] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regu-

larized loss minimization. Mathematical Programming  155(1-2):105–145  2016.

[25] Vladimir Vapnik. The nature of statistical learning theory. Springer Science & Business Media  2013.
[26] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.

SIAM Journal on Optimization  24(4):2057–2075  2014.

9

,Aryan Mokhtari
Hadi Daneshmand
Aurelien Lucchi
Thomas Hofmann
Alejandro Ribeiro