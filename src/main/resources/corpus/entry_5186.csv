2016,Split LBI: An Iterative Regularization Path with Structural Sparsity,An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration  hence called \emph{Split LBI}. Despite its simplicity  Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping  Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore  some $\ell_2$ error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking.,Split LBI: An Iterative Regularization Path with

Structural Sparsity

Chendi Huang1  Xinwei Sun1  Jiechao Xiong1  Yuan Yao2 1

1Peking University  2Hong Kong University of Science and Technology

{cdhuang  sxwxiaoxiaohehe  xiongjiechao}@pku.edu.cn  yuany@ust.hk

Abstract

An iterative regularization path with structural sparsity is proposed in this paper
based on variable splitting and the Linearized Bregman Iteration  hence called Split
LBI. Despite its simplicity  Split LBI outperforms the popular generalized Lasso
in both theory and experiments. A theory of path consistency is presented that
equipped with a proper early stopping  Split LBI may achieve model selection
consistency under a family of Irrepresentable Conditions which can be weaker than
the necessary and sufﬁcient condition for generalized Lasso. Furthermore  some (cid:96)2
error bounds are also given at the minimax optimal rates. The utility and beneﬁt of
the algorithm are illustrated by applications on both traditional image denoising
and a novel example on partial order ranking.

Introduction

1
In this paper  consider the recovery from linear noisy measurements of β(cid:63) ∈ Rp  which satisﬁes the
following structural sparsity that the linear transformation γ(cid:63) := Dβ(cid:63) for some D ∈ Rm×p has most
of its elements being zeros. For a design matrix X ∈ Rn×p  let

y = Xβ(cid:63) +   γ(cid:63) = Dβ(cid:63) (S = supp (γ(cid:63))   s = |S|)  

(1.1)
where  ∈ Rn has independent identically distributed components  each of which has a sub-Gaussian
distribution with parameter σ2 (E[exp(ti)] ≤ exp(σ2t2/2)). Here γ(cid:63) is sparse  i.e. s (cid:28) m. Given
(y  X  D)  the purpose is to estimate β(cid:63) as well as γ(cid:63)  and in particular  recovers the support of γ(cid:63).
There is a large literature on this problem. Perhaps the most popular approach is the following
(cid:96)1-penalized convex optimization problem 

(cid:18) 1

(cid:19)

arg min

β

2n

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)Dβ(cid:107)1

.

(1.2)

Such a problem can be at least traced back to [ROF92] as a total variation regularization for image
denoising in applied mathematics; in statistics it is formally proposed by [Tib+05] as fused Lasso. As
D = I it reduces to the well-known Lasso [Tib96] and different choices of D include many special
cases  it is often called generalized Lasso [TT11] in statistics.
Various algorithms are studied for solving (1.2) at ﬁxed values of the tuning parameter λ  most of
which is based on the Split Bregman or ADMM using operator splitting ideas (see for examples
[GO09; YX11; Wah+12; RT14; Zhu15] and references therein). To avoid the difﬁculty in dealing
with the structural sparsity in (cid:107)Dβ(cid:107)1  these algorithms exploit an augmented variable γ to enforce
sparsity while keeping it close to Dβ.
On the other hand  regularization paths are crucial for model selection by computing estimators as
functions of regularization parameters. For example  [Efr+04] studies the regularization path of
standard Lasso with D = I  the algorithm in [Hoe10] computes the regularization path of fused

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Lasso  and the dual path algorithm in [TT11] can deal with generalized Lasso. Recently  [AT16]
discussed various efﬁcient implementations of the the algorithm in [TT11]  and the related R package
genlasso can be found in CRAN repository. All of these are based on homotopy method of solving
convex optimization (1.2).
Our departure here  instead of solving (1.2)  is to look at an extremely simple yet novel iterative
scheme which ﬁnds a new regularization path with structural sparsity. We are going to show that
it works in a better way than genlasso  in both theory and experiments. To see this  deﬁne a loss
function which splits Dβ and γ 

(cid:96) (β  γ) :=

(cid:107)y − Xβ(cid:107)2

2 +

1
2n

1
2ν

(cid:107)γ − Dβ(cid:107)2

2 (ν > 0).

(1.3)

Now consider the following iterative algorithm 

βk+1 = βk − κα∇β(cid:96)(βk  γk) 
zk+1 = zk − α∇γ(cid:96)(βk  γk) 
γk+1 = κ · prox(cid:107)·(cid:107)1(zk+1) 

(1.4a)
(1.4b)
(1.4c)
where the initial choice z0 = γ0 = 0 ∈ Rm  β0 = 0 ∈ Rp  parameters κ > 0  α > 0  ν > 0 
and the proximal map associated with a convex function h is deﬁned by proxh(z) = arg minx (cid:107)z −
x(cid:107)2/2 + h(x)  which is reduced to the shrinkage operator when h is taken to be the (cid:96)1-norm 
prox(cid:107)·(cid:107)1 (z) = S (z  1) where

S (z  λ) = sign(z) · max (|z| − λ  0) (λ ≥ 0).

In fact  without the sparsity enforcement (1.4c)  the algorithm is called the Landweber Iteration
in inverse problems [YRC07]  also known as L2-Boost [BY02] in statistics. When D = I and
ν → 0 which enforces γ = Dβ = β  the iteration (1.4) is reduced (by dropping (1.4a)) to the
popular Linearized Bregman Iteration (LBI) for linear regression or compressed sensing which is
ﬁrstly proposed in [Yin+08]. The simple iterative scheme returns the whole regularization path 
at the same cost of computing one Lasso estimator at a ﬁxed regularization parameter using the
iterative soft-thresholding algorithm. However  LBI regularization path could be better than Lasso
regularization path which is always biased. In fact  recently [Osh+16] shows that under nearly the
same conditions as standard Lasso  LBI may achieve sign-consistency but with a less biased estimator
than Lasso  which in the limit dynamics will reach the bias-free Oracle estimator.
The difference between (1.4) and the standard LBI lies in the partial sparsity control on γ  which
splits the structural sparsity on Dβ into a sparse γ and Dβ by controlling their gap (cid:107)γ − Dβ(cid:107)2/(2ν).
Thereafter algorithm (1.4) is called Split LBI in this paper.
Split LBI generates a sequence (βk  γk)k∈N which indeed deﬁnes a discrete regularization path.
Furthermore  the path can be more accurate than that of generalized Lasso  in terms of Area Under
Curve (AUC) measurement of the order of regularization paths becoming nonzero in consistent with
the ground truth sparsity pattern. The following simple experiment illustrates these properties.
Example 1. Consider two problems: standard Lasso and 1-D fused Lasso.
In both cases  set
n = p = 50  and generate X ∈ Rn×p denoting n i.i.d. samples from N (0  Ip)   ∼ N (0  In) 
j = 2 (if 1 ≤ j ≤ 10)  −2 (if 11 ≤ j ≤ 15)  and 0 (otherwise). For Lasso
y = Xβ(cid:63) + . β(cid:63)
we choose D = I  and for 1-D fused Lasso we choose D = [D1; D2] ∈ R(p−1+p)×p such that
(D1β)j = βj − βj+1 (for 1 ≤ j ≤ p − 1) and D2 = Ip. The left panel of Figure 1 shows the
regularization paths by genlasso ({Dβλ}) and by iteration (1.4) (linear interpolation of {γk}) with
κ = 200 and ν ∈ {1  5  10}  respectively. The generalized Lasso path is in fact piecewise linear with
respect to λ while we show it along t = 1/λ for a comparison. Note that the iterative paths exhibit a
variety of different shapes depending on the choice of ν. However  in terms of order of those curves
entering into nonzero range  these iterative paths exhibit a better accuracy than genlasso. Table 1
shows this by the mean AUC of 100 independent experiments in each case  where the increase of ν
improves the model selection accuracy of Split LBI paths and beats that of generalized Lasso.

Why does the simple iterative algorithm (1.4) work  even better than the generalized Lasso? In this
paper  we aim to answer it by presenting a theory for model selection consistency of (1.4).
Model selection and estimation consistency of generalized Lasso (1.2) has been studied in previous
work. [SSR12] considered the model selection consistency of the edge Lasso  with a special D in

2

Figure 1: Left shows {Dβλ} (t = 1/λ) by genlasso and {γk} (t = kα) by Split LBI (1.4) with
ν = 1  5  10  for 1-D fused Lasso. Right is a comparison between our family of Irrepresentable
Condition (IRR(ν)) and IC in [Vai+13]  with log-scale horizontal axis. As ν grows  IRR(ν) can be
signiﬁcantly smaller than IC0 and IC1  so that our model selection condition is easier to be met!

Table 1: Mean AUC (with standard deviation) comparisons where Split LBI (1.4) beats genlasso.
Left is for the standard Lasso. Right is for the 1-D fused Lasso in Example 1.
genlasso

genlasso

Split LBI

Split LBI

1

5

10

1

5

10

.9426
(.0390)

.9845
(.0185)

.9969
(.0065)

.9982
(.0043)

.9705
(.0212)

.9955
(.0056)

.9996
(.0014)

.9998
(.0009)

(1.2)  which has applications over graphs. [LYY13] provides an upper bound of estimation error by
assuming the design matrix X is a Gaussian random matrix. In particular  [Vai+13] proposes a general
condition called Identiﬁability Criterion (IC) for sign consistency. [LST13] establishes a general
framework for model selection consistency for penalized M-estimators  proposing an Irrepresentable
Condition which is equivalent to IC from [Vai+13] under the speciﬁc setting of (1.2). In fact both of
these conditions are sufﬁcient and necessary for structural sparse recovery by generalized Lasso (1.2)
in a certain sense.
However  as we shall see soon  the beneﬁts of exploiting algorithm (1.4) not only lie in its algorithmic
simplicity  but also provide a possibility of theoretical improvement on model selection consistency.
Below a new family of Irrepresentable Condition depending on ν will be presented for iteration (1.4) 
under which model selection consistency can be established. Moreover  this family can be weaker
than IC as the parameter ν grows  which sheds light on the superb performance of Split LBI we
observed above. The main contributions of this paper can be summarized as follows: (A) a new
iterative regularization path with structural sparsity by (1.4); (B) a theory of path consistency which
shows the model selection consistency of (1.4)  under some weaker conditions than generalized
Lasso  together with (cid:96)2 error bounds at minimax optimal rates. Further experiments are given with
applications on 2-D image reconstruction and partial order estimation.

1.1 Notation
For matrix Q with m rows (D for example) and J ⊆ {1  2  . . .   m}  let QJ = QJ · be the submatrix
of Q with rows indexed by J. However  for Q ∈ Rn×p (X for example) and J ⊆ {1  2  . . .   p}  let
QJ = Q· J be the submatrix of Q with columns indexed by J  abusing the notation.
Sometimes we use (cid:104)a  b(cid:105) := aT b  denoting the inner product between vectors a  b. PL denotes the
projection matrix onto a linear subspace L  Let L1 + L2 := {ξ1 + ξ2 : ξ ∈ L1  ξ ∈ L2} for
subspaces L1  L2. For a matrix Q  let Q† denotes the Moore-Penrose pseudoinverse of Q  and we
recall that Q† = (QT Q)†QT . Let λmin(Q)  λmax(Q) denotes the smallest and largest singular value
(i.e. eigenvalue if Q is symmetric) of Q. For symmetric matrices P and Q  Q (cid:31) P (or Q (cid:23) P )
means that Q − P is positive (semi)-deﬁnite  respectively. Let Q∗ := QT /n.

3

2 Path Consistency of Split LBI

2.1 Basic Assumptions

For the identiﬁability of β(cid:63)  we assume that β(cid:63) and its estimators of interest are restricted in

since replacing β(cid:63) with “the projection of β(cid:63) onto L” does not change the model.
Note that (cid:96)(β  γ) is quadratic  and we can deﬁne its Hessian matrix which depends on ν > 0

L := (ker(X) ∩ ker(D))⊥ = Im(cid:0)X T(cid:1) + Im(cid:0)DT(cid:1)  
(cid:19)
(cid:18)X∗X + DT D/ν −DT /ν

H(ν) := ∇2(cid:96) (β  γ) ≡

−D/ν

Im/ν

.

(2.1)

We make the following assumptions on H.
Assumption 1 (Restricted Strong Convexity (RSC)). There is a constant λH > 0 such that

≥ λH

(β ∈ L  γS ∈ Rs) .

(2.2)

(cid:0)βT   γT

S

(cid:19)

(cid:18) β

γS

(cid:1) · H(β S) (β S) ·
(cid:13)(cid:13)(cid:13)(cid:13)HSc (β S)H

ρ∈[−1 1]s

sup

2

γS

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:18) β
(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)∞
(cid:18)0p

ρ

Remark 1. Since the true parameter supp(γ(cid:63)) = supp(Dβ(cid:63)) = S  it is equivalent to say that the loss
(cid:96)(β  γ) is strongly convex when restricting on the sparse subspace corresponding to support of γ(cid:63).
Assumption 2 (Irrepresentable Condition (IRR)). There is a constant η ∈ (0  1] such that

†
(β S) (β S) ·

≤ 1 − η.

(2.3)

(cid:13)(cid:13)(cid:13)HSc (β S)H

Remark 2. IRR here directly generalizes the Irrepresentable Condition from standard Lasso [ZY06]
and other algorithms [Tro04]  to the partial Lasso: minβ γ((cid:96) (β  γ) + λ(cid:107)γ(cid:107)1). Following the standard
Lasso  one version of the Irrepresentable Condition should be

(cid:19)
(β S) is the value of gradient (subgradient) of (cid:96)1 penalty function (cid:107) · (cid:107)1 on (β(cid:63); γ(cid:63)
β = 0p 
ρ(cid:63)
because β is not assumed to be sparse and hence is not penalized. Assumption 2 slightly strengthens
this by a supremum over ρ  for uniform sparse recovery independent to a particular sign pattern of γ(cid:63).

≤ 1 − η  where ρ(cid:63)

†
(β S) (β S)ρ(cid:63)

(cid:18)0p

(cid:13)(cid:13)(cid:13)∞

S). Here ρ(cid:63)

(β S) =

ρ(cid:63)
S

(β S)

.

2.2 Equivalent Conditions and a Comparison Theorem

The assumptions above  though being natural  are not convenient to compare with that in [Vai+13].
Here we present some equivalent conditions  followed by a comparison theorem showing that IRR
can be weaker than IC in [Vai+13]  a necessary and sufﬁcient for model selection consistency of
generalized Lasso.
First of all  we introduce some notations. Given γ  minimizing (cid:96) solves β = A†(νX∗y + DT γ) 
where A := νX∗X + DT D. Substituting A†(νX∗y + DT γk) for βk in (1.4b)  and dropping (1.4a) 
we have

zk+1 = zk + α(DA†X∗y − Σγk) 
γk+1 = κ · prox(cid:107)·(cid:107)1(zk+1) 

Σ :=(cid:0)I − DA†DT(cid:1) /ν  A = νX∗X + DT D.

where

(2.5)
In other words  Σ is the Schur complement of Hβ β in Hessian matrix H(ν). Comparing (2.4) with
the standard LBI (D = I) studied in [Osh+16]  we know that Σ in our paper plays the similar role of
X∗X in their paper. In order to obtain path consistency results of standard LBI in [Osh+16]  they
propose “Restricted Strong Convexity” and “Irrpresentable Condition” on X∗X. So in this paper 
we can obtain similar assumptions on Σ (instead of H)  which actually prove to be equivalent with
Assumption 1 and 2  and closely related to literature.
Precisely  by Lemma 6 in Supplementary Information we know that Assumption 1 is equivalent to

(2.4a)
(2.4b)

4

Assumption 1(cid:48) (Restricted Strong convexity (RSC)). There is a constant λΣ > 0 such that

ΣS S (cid:23) λΣI.

(2.6)
Remark 3. Lemma 2 in Supplementary Information says ΣS S (cid:31) 0 ⇔ ker(DSc) ∩ ker(X) ⊆
ker(DS)  which is also a natural assumption for the uniqueness of β(cid:63). Actually  if it fails  then there
will be some β such that DSc β = 0  Xβ = 0 while DSβ (cid:54)= 0. Thus for any β(cid:48)(cid:63) := β(cid:63) + β  we have
y = Xβ(cid:48)(cid:63) +   supp(Dβ(cid:48)(cid:63)) ⊆ supp(Dβ(cid:63)) = S  while DSβ(cid:48)(cid:63) (cid:54)= DSβ(cid:63). Therefore one can neither
estimate β(cid:63) nor DSβ(cid:63) even if the support set is known or has been exactly recovered.
When ΣS S (cid:31) 0  Lemma 7 in Supplementary Information implies that Assumption 2 is equivalent to
Assumption 2(cid:48) (Irrepresentable condition (IRR)). There is a constant η ∈ (0  1] such that

S S

(2.7)
Remark 4. For standard Lasso problems (D = I)  it is easy to derive Σ = X∗(1 + νXX∗)−1X ≈
X∗X when ν is small. So Assumption 1(cid:48) approximates the usual Restricted Strong Convexity
SXS (cid:23) λΣI and Assumption 2(cid:48) approximates the usual Irrepresentable Condition
assumption X∗
(cid:107)X∗
SXS)−1(cid:107)∞ ≤ 1 − η for standard Lasso problems.
Sc XS(X∗
The left hand side of (2.7) depends on parameter ν. From now on  deﬁne

(cid:13)(cid:13)(cid:13)ΣSc SΣ−1

(cid:13)(cid:13)(cid:13)∞

≤ 1 − η.

IRR(ν) :=

IRR(ν)  IRR(∞) := lim

ν→+∞ IRR(ν).

(2.8)

Now we are going to compare Assumption 2(cid:48) with the assumption in [Vai+13]. Let W be a matrix
whose columns form an orthogonal basis of ker(DSc)  and deﬁne

(cid:13)(cid:13)(cid:13)ΣSc SΣ−1
(cid:16)

S S

(cid:13)(cid:13)(cid:13)∞   IRR(0) := lim
(cid:17)T(cid:16)

ν→0

ΩS :=

IC0 :=(cid:13)(cid:13)ΩS(cid:13)(cid:13)∞   IC1 := min

†
Sc

D

u∈ker(DT

Sc )

X∗XW(cid:0)W T X∗XW(cid:1)†

(cid:17)

W T − I

(cid:13)(cid:13)ΩSsign (DSβ(cid:63)) − u(cid:13)(cid:13)∞ .

DT
S  

[Vai+13] proved the sign consistency of the generalized Lasso estimator of (1.2) for speciﬁcally
chosen λ  under the assumption IC1 < 1 along with ker(DSc) ∩ ker(X) = {0}. As we shall see
later  the same conclusion holds under the assumption IRR(ν) ≤ 1 − η along with Assumption 1(cid:48)
which is equivalent to ker(DSc) ∩ ker(X) ⊆ ker(DS). Which assumption is weaker to be satisﬁed?
The following theorem answers this  whose proof is in Supplementary Information.
Theorem 1 (Comparisons between IRR in Assumption 2(cid:48) and IC in [Vai+13]).

1. IC0 ≥ IC1.
2. IRR(0) exists  and IRR(0) = IC0.
3. IRR(∞) exists  and IRR(∞) = 0 if and only if ker(X) ⊆ ker(DS).

From this comparison theorem with a design matrix X of full column rank  as ν grows  IRR(ν) <
IC1 ≤ IC0  hence Assumption 2(cid:48) is weaker than IC. Now recall the setting of Example 1 where
ker(X) = 0 generically. In the right panel of Figure 1  the (solid and dashed) horizontal red lines
denote IC0  IC1  and we see the blue curve denoting IRR(ν) approaches IC0 when ν → 0 and
approaches 0 when ν → +∞  which illustrates Theorem 1 (here each of IC0  IC1  IRR(ν) is the
mean of 100 values calculated under 100 generated X’s). Although IRR(0) = IC0 is slightly larger
than IC1  IRR(ν) can be signiﬁcantly smaller than IC1 if ν is not tiny. On the right side of the
vertical line  IRR(ν) drops below 1  indicating that Assumption 2(cid:48) is satisﬁed while the assumption
in [Vai+13] fails.
Remark 5. Despite that Theorem 1 suggests to adopt a large ν  ν can not be arbitrarily large. From
Assumption 1(cid:48) and the deﬁnition of Σ  1/ν ≥ (cid:107)Σ(cid:107)2 ≥ (cid:107)ΣS S(cid:107)2 ≥ λΣ. So if ν is too large  λΣ has to
be small enough  which will deteriorates the estimator in terms of (cid:96)2 error shown in the next.

2.3 Consistency of Split LBI

We are ready to establish the theorems on path consistency of Split LBI (1.4)  under Assumption 1
and 2. The proofs are based on a careful treatment of the limit dynamics of (1.4) and collected in
Supplementary Information. Before stating the theorems  we need some deﬁnitions and constants.

5

(2.9)

 

(2.10)

Let the compact singular value decomposition (compact SVD) of D be

and (V  ˜V ) be an orthogonal square matrix. Let the compact SVD of X ˜V /

D = U ΛV T (cid:0)Λ ∈ Rr×r  Λ (cid:31) 0  U ∈ Rm×r  V ∈ Rp×r(cid:1)  
  V1 ∈ R(p−r)×r(cid:48)(cid:17)
ΛX =(cid:112)Λmax (X∗X)  λD = λmin (Λ)   ΛD = Λmax (Λ)   λ1 = λmin (Λ1) .

  Λ1 (cid:31) 0  U1 ∈ Rn×r(cid:48)

Λ1 ∈ Rr(cid:48)×r(cid:48)

n = U1Λ1V T
1

(cid:16)

n be

and let (V1  ˜V1) be an orthogonal square matrix. Let

X ˜V /

√

√

(2.11)
We see ΛD is the largest singular value of D  λD is the smallest nonzero singular value of D  and λ2
1
is the smallest nonzero eigenvalue of ˜V T X∗X ˜V . If D has full column rank  then r = p  r(cid:48) = 0  and
˜V   U1  Λ1  V1  λ1 all drop  while ˜V1 ∈ R(p−r)×(p−r) is an orthogonal square matrix.
The following theorem says that under Assumption 1 and 2  Split LBI will automatically evolve in
an “oracle” subspace (unknown to us) restricted within the support set of (β(cid:63)  γ(cid:63)) before leaving it 
and if the signal parameters is strong enough  sign consistency will be reached. Moreover  (cid:96)2 error
bounds on γk and βk are given.
Theorem 2 (Consistency of Split LBI). Under Assumption 1 and 2  suppose κ is large enough to
satisfy

(cid:18)

κ ≥ 4
η

1 +

1
λD

+

ΛX
λ1λD

and κα(cid:107)H(cid:107)2 < 2. Let
η
8σ

¯τ :=

· λD
ΛX

X + Λ2


(cid:18) ΛX

D)

2σ
λH

+

λD

(cid:115)

(cid:19)1 +
(cid:18)
(cid:114) n

·

2 (1 + νΛ2
λH ν
(1 + ΛD)(cid:107)β(cid:63)(cid:107)2 +
(cid:107)
(cid:106) ¯τ

  K :=

log m

α

  λ(cid:48)

H := λH (1 − κα(cid:107)H(cid:107)2/2) > 0.

Then with probability not less than 1 − 6/m − 3 exp(−4n/5)  we have all the following properties.
1. No-false-positive: The solution has no false-positive  i.e. supp(γk) ⊆ S  for 0 ≤ kα ≤ τ.
2. Sign consistency of γk: Once the signal is strong enough such that

min := (DSβ(cid:63))min ≥
γ(cid:63)

16σ

ηλ(cid:48)

H (1 − 5α/¯τ )

· ΛX ΛD

λ2
D

(2 log s + 5 + log(8ΛD))

then γk has sign consistency at K  i.e. sign (γK) = sign (Dβ(cid:63)).

(cid:19)(cid:19)

 

(2.12)

ΛX
λ2
D

+

λH λ2

D + Λ2
X
λ1λ2
D

(cid:114)

log m

 

n
(2.13)

(cid:114)

s log m

n

.

(cid:114)

s log m

(cid:114)

r(cid:48) log m

+

2σ
λ1

n

n
+ ν · 2σ · λ1ΛX + Λ2

X

.

λ1λ2
D

3. (cid:96)2 consistency of γk:

(cid:107)γK − Dβ(cid:63)(cid:107)2 ≤

4. (cid:96)2 consistency of βk:

42σ

ηλ(cid:48)

H (1 − α/¯τ )

· ΛX
λD

(cid:107)βK − β(cid:63)(cid:107)2 ≤

42σ

ηλ(cid:48)

H (1 − α/¯τ )

· λ1ΛX (1 + λD) + Λ2

X

λ1λ2
D

Despite that the sign consistency of γk can be established here  usually one can not expect Dβk
recovers the sparsity pattern of γ(cid:63) due to the variable splitting. As shown in the last term of (cid:96)2 error
bound of βk  increasing ν will sacriﬁce its accuracy. However  one can remedy this by projecting
βk on to a subspace using the support set of γk  and obtain a good estimator ˜βk with both sign
consistency and (cid:96)2 consistency at the minimax optimal rates.

6

Consequently  if additionally SK = S  then the last term on the right hand side drops for
k = K  and it reaches

(cid:13)(cid:13)(cid:13) ˜βK − β(cid:63)(cid:13)(cid:13)(cid:13)2

≤

80σ

ηλ(cid:48)

H (1 − α/¯τ )

· ΛX

(cid:13)(cid:13)(cid:13)D

†
Sc
k

k∩Sβ(cid:63)(cid:13)(cid:13)(cid:13)2

DSc

.

+ 2

r(cid:48) log m

n

(cid:114)
(cid:0)ΛD + λ2
(cid:1)
(cid:18) ΛX

λ3
D

D

+

2σ
λ(cid:48)

s log m

n
λ(cid:48)
H λ2

+

D + Λ2
X
λ1λ2
D

(cid:19)(cid:114)

r(cid:48) log m

n

.

Theorem 3 (Consistency of revised version of Split LBI). Under Assumption 1 and 2  suppose κ is
large enough to satisfy (2.12)  and κα(cid:107)H(cid:107)2 < 2. ¯τ   K  λ(cid:48)
H are deﬁned the same as in Theorem 2.
(cid:17) = I − D
Deﬁne

  ˜βk := PSk βk.

(cid:16)

Sk := supp(γk)  PSk := P

DSc
k = ∅  deﬁne PSk = I. Then we have the following properties.

DSc
k

ker

k

If Sc

†
Sc
k

1. Sign consistency of ˜βk: If the γ(cid:63)

min condition (2.13) holds  then with probability not less

than 1 − 8/m − 3 exp(−4n/5)  there holds sign(D ˜βK) = sign(Dβ(cid:63)).

2. (cid:96)2 consistency of ˜βk: With probability not less than 1 − 8/m − 2r(cid:48)/m2 − 3 exp(−4n/5) 

we have that for 0 ≤ kα ≤ ¯τ 

(cid:32)

(cid:13)(cid:13)(cid:13) ˜βk − β(cid:63)(cid:13)(cid:13)(cid:13)2

≤

(cid:114)

(cid:33)
(cid:19)(cid:114)

n

√
10
s
λ(cid:48)
H kα

+

2σ
λ(cid:48)

H

2σ
λ(cid:48)

+

(cid:18) ΛX

H

+

λ2
D

· ΛX ΛD

s log m

λ3
D
λ(cid:48)
D + Λ2
H λ2
X
λ1λ2
D

Remark 6. Note that r(cid:48) ≤ min(n  p− r). In many real applications  r(cid:48) is very small. So the dominant

(cid:96)2 error rate is O((cid:112)s log m/n)  which is minimax optimal [LST13; LYY13].

H

λ2
D

3 Experiments

3.1 Parameter Setting

Parameter κ should be large enough according to (2.12). Moreover  step size α should be small
enough to ensure the stability of Split LBI. When ν  κ are determined  α can actually be determined
by α = ν/(κ(1 + νΛ2

D)) (see (C.6) in Supplementary Information).

X + Λ2

3.2 Application: Image Denoising
Consider the image denoising problem in [TT11]. The original image is resized to 50 × 50  and reset
with only four colors  as in the top left image in Figure 2. Some noise is added by randomly changing
some pixels to be white  as in the bottom left. Let G = (V  E) is the 4-nearest-neighbor grid graph on
pixels  then β = (βR  βG  βB) ∈ R3|V | since there are 3 color channels (RGB channels). X = I3|V |
and D = diag(DG  DG  DG)  where DGδ ∈ R|E|×|V | is the gradient operator on graph G deﬁned
by (DGx)(eij) = xi − xj  eij ∈ E. Set ν = 180  κ = 100. The regularization path of Split LBI is
shown in Figure 2  where as t evolves  images on the path gradually select visually salient features
before picking up the random noise. Now compare the AUC (Area Under Curve) of genlasso and
Split LBI algorithm with different ν. For simplicity we show the AUC corresponding to the red
color channel. Here ν ∈ {1  20  40  60  . . .   300}. As shown in the right panel of Figure 2  with the
increase of ν  Split LBI beats genlasso with higher AUC values.

3.3 Application: Partial Order Ranking for Basketball Teams

Here we consider a new application on the ranking of p = 12 FIBA basketball teams into partial
orders. The teams are listed in Figure 3. We collected n = 134 pairwise comparison game results
mainly from various important championship such as Olympic Games  FIBA World Championship

7

Figure 2: Left is image denoising results by Split LBI. Right shows the AUC of Split LBI (blue solid
line) increases and exceeds that of genlasso (dashed red line) as ν increases.

Figure 3: Partial order ranking for basketball teams. Top left: {βλ} (t = 1/λ) by genlasso and
˜βk (t = kα) by Split LBI. Top right: grouping result just passing t5. Bottom: FIBA ranking.

jk

− β(cid:63)

and FIBA Basketball Championship in 5 continents from 2006–2014 (8 years is not too long for teams
to keep relatively stable levels while not too short to have enough samples). For each sample indexed
by k and corresponding team pair (i  j)  yk = si − sj is the score difference between team i and j.
+ k where β(cid:63) ∈ Rp measures the strength of these teams. So the
We assume a model yk = β(cid:63)
ik
design matrix X ∈ Rn×p is deﬁned by its k-th row: xk ik = 1  xk jk = −1  xk l = 0 (l (cid:54)= ik  jk).
In sports  teams of similar strength often meet than those in different levels. Thus we hope to ﬁnd a
coarse grained partial order ranking by adding a structural sparsity on Dβ(cid:63) where D = cX (c scales
the smallest nonzero singular value of D to be 1).
The top left panel of Figure 3 shows {βλ} by genlasso and ˜βk by Split LBI with ν = 1 and κ = 100.
Both paths give the same partial order at early stages  though the Split LBI path looks qualitatively
better. For example  the top right panel shows the same partial order after the change point t5. It is
interesting to compare it against the FIBA ranking in September  2014  shown in the bottom. Note
that the average basketball level in Europe is higher than that of in Asia and Africa  hence China can
get more FIBA points than Germany based on the dominant position in Asia  so is Angola in Africa.
But their true levels might be lower than Germany  as indicated in our results. Moreover  America
(FIBA points 1040.0) itself forms a group  agreeing with the common sense that it is much better
than any other country. Spain  having much higher FIBA ranking points (705.0) than the 3rd team
Argentina (455.0)  also forms a group alone. It is the only team that can challenge America in recent
years  and it enters both ﬁnals against America in 2008 and 2012.

Acknowledgments

The authors were supported in part by National Basic Research Program of China under grants
2012CB825501 and 2015CB856000  as well as NSFC grants 61071157 and 11421110001.

8

Original Figuret =9.3798t =23.7812Noisy Figuret =60.5532t =617.1275References

[AT16]

[BY02]

[Efr+04]

[GO09]

[Hoe10]

[LST13]

[LYY13]

[Moe12]

[Osh+16]

[ROF92]

[RT14]

[SSR12]

[Tib+05]

[Tib96]

[Tro04]

[TT11]

[Vai+13]

[Wah+12]

[Yin+08]

[YRC07]

[YX11]

[Zha06]

[Zhu15]

[ZY06]

Taylor B. Arnold and Ryan J. Tibshirani. “Efﬁcient Implementations of the Generalized Lasso Dual
Path Algorithm”. In: Journal of Computational and Graphical Statistics 25.1 (2016)  pp. 1–27.
Peter Bühlmann and Bin Yu. “Boosting with the L2-Loss: Regression and Classiﬁcation”. In:
Journal of American Statistical Association 98 (2002)  pp. 324–340.
B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. “Least angle regression”. In: The Annals of
statistics 32.2 (2004)  pp. 407–499.
Tom Goldstein and Stanley Osher. “Split Bregman method for large scale fused Lasso”. In: SIAM
Journal on Imaging Sciences 2.2 (2009)  pp. 323–343.
Holger Hoeﬂing. “A Path Algorithm for the Fused Lasso Signal Approximator”. In: Journal of
Computational and Graphical Statistics 19.4 (2010)  pp. 984–1006.
Jason D Lee  Yuekai Sun  and Jonathan E Taylor. “On model selection consistency of penalized
M-estimators: a geometric theory”. In: Advances in Neural Information Processing Systems (NIPS)
26. 2013  pp. 342–350.
Ji Liu  Lei Yuan  and Jieping Ye. “Guaranteed Sparse Recovery under Linear Transformation”. In:
Proceedings of The 30th International Conference on Machine Learning. 2013  pp. 91–99.
Michael Moeller. “Multiscale Methods for Polyhedral Regularizations and Applications in High
Dimensional Imaging”. PhD thesis. Germany: University of Muenster  2012.
Stanley Osher  Feng Ruan  Jiechao Xiong  Yuan Yao  and Wotao Yin. “Sparse recovery via
differential inclusions”. In: Applied and Computational Harmonic Analysis (2016). DOI: 10.
1016/j.acha.2016.01.002.
Leonid I. Rudin  Stanley Osher  and Emad Fatemi. “Nonlinear Total Variation Based Noise
Removal Algorithms”. In: Physica D: Nonlinear Phenomena 60.1-4 (Nov. 1992)  pp. 259–268.
Aaditya Ramdas and Ryan J. Tibshirani. “Fast and Flexible ADMM Algorithms for Trend Filter-
ing”. In: Journal of Computational and Graphical Statistics (2014). DOI: 10.1080/10618600.
2015.1054033.
James Sharpnack  Aarti Singh  and Alessandro Rinaldo. “Sparsistency of the edge lasso over
graphs”. In: International Conference on Artiﬁcial Intelligence and Statistics. 2012  pp. 1028–
1036.
Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. “Sparsity and
smoothness via the fused lasso”. In: Journal of the Royal Statistical Society Series B (2005) 
pp. 91–108.
Robert Tibshirani. “Regression shrinkage and selection via the lasso”. In: Journal of the Royal
Statistical Society. Series B (Methodological) (1996)  pp. 267–288.
Joel A. Tropp. “Greed is good: Algorithmic results for sparse approximation”. In: IEEE Trans.
Inform. Theory 50.10 (2004)  pp. 2231–2242.
Ryan J. Tibshirani and Jonathan Taylor. “The solution path of the generalized lasso”. In: The
Annals of Statistics 39.3 (June 2011)  pp. 1335–1371.
S. Vaiter  G. Peyre  C. Dossal  and J. Fadili. “Robust Sparse Analysis Regularization”. In: IEEE
Transactions on Information Theory 59.4 (Apr. 2013)  pp. 2001–2016.
Bo Wahlberg  Stephen Boyd  Mariette Annergren  and Yang Wang. “An ADMM Algorithm for a
Class of Total Variation Regularized Estimation Problems”. In: IFAC Proceedings Volumes. 16th
IFAC Symposium on System Identiﬁcation 45.16 (2012)  pp. 83–88.
Wotao Yin  Stanley Osher  Jerome Darbon  and Donald Goldfarb. “Bregman Iterative Algorithms
for Compressed Sensing and Related Problems”. In: SIAM Journal on Imaging Sciences 1.1 (2008) 
pp. 143–168.
Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. “On Early Stopping in Gradient Descent
Learning”. In: Constructive Approximation 26.2 (2007)  pp. 289–315.
Gui-Bo Ye and Xiaohui Xie. “Split Bregman method for large scale fused Lasso”. In: Computa-
tional Statistics & Data Analysis 55.4 (2011)  pp. 1552–1569.
Fuzhen Zhang. The Schur Complement and Its Applications. Springer Science & Business Media 
2006. 308 pp. ISBN: 978-0-387-24273-6.
Yunzhang Zhu. “An augmented ADMM algorithm with application to the generalized lasso prob-
lem”. In: Journal of Computational and Graphical Statistics (2015). DOI: 10.1080/10618600.
2015.1114491.
Peng Zhao and Bin Yu. “On Model Selection Consistency of Lasso”. In: Journal of Machine
Learning Research 7 (2006)  pp. 2541–2567.

9

,Chendi Huang
Xinwei Sun
Jiechao Xiong
Yuan Yao