2018,Distributed Multi-Player Bandits - a Game of Thrones Approach,We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms  and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets  compared to the optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players  and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O\left(\log^{2}T\right). This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.,Distributed Multi-Player Bandits - a Game of

Thrones Approach

Ilai Bistritz

Stanford University

Amir Leshem

Bar Ilan University

bistritz@stanford.edu

Amir.Leshem@biu.ac.il

Abstract

We consider a multi-armed bandit game where N players compete for K arms
for T turns. Each player has different expected rewards for the arms  and the in-
stantaneous rewards are independent and identically distributed. Performance is
measured using the expected sum of regrets  compared to the optimal assignment
of arms to players. We assume that each player only knows her actions and the
reward she received each turn. Players cannot observe the actions of other players 
and no communication between players is possible. We present a distributed al-

gorithm and prove that it achieves an expected sum of regrets of near-O(cid:0)log2 T(cid:1).

This is the ﬁrst algorithm to achieve a poly-logarithmic regret in this fully dis-
tributed scenario. All other works have assumed that either all players have the
same vector of expected rewards or that communication between players is possi-
ble.

1

Introduction

In online learning problems  an agent needs to learn on the run how to behave optimally. The crux
of these problems is the trade-off between exploration and exploitation. This trade-off is well cap-
tured by the multi-armed bandit problem  which has attracted enormous attention from the research
community. Recently  there has been a growing interest in the case of the multi-player multi-armed
bandit. In the multi-player scenario  the nature of the interaction between the players can take many
forms. Players may want to solve the problem of ﬁnding the best mutual arm as a team [1–6]  or
may compete over the arms as resources they all individually require [7–19].
The idea of regret in the competitive multi-player multi-armed bandit problem is the expected sum
of regrets and is deﬁned as the performance loss compared to the optimal assignment of arms to
players. The rationale for this notion of regret is formulated from the designer’s perspective  who
wants the distributed system of individuals to converge to a globally good solution.
Many works have considered a scenario where all the players have the same expectations for the
rewards of all arms. Some of these works assume that communication between players is possible
[10–12  14  19]  whereas others consider a fully distributed scenario [7  13  15].
One of the main reasons for studying resource allocation bandits is their applications in cognitive
radio or wireless networks in general. In these scenarios  the channels are interpreted as arms and
the channel gains as the rewards. However  since users are scattered in space  the physical reality
dictates that different arms have different expected channel gains for different players.
This essential generalization for a matrix of expectations introduces a famous combinatorial opti-
mization problem known as the assignment problem [20]. Achieving a sublinear expected sum of
regrets in a distributed manner requires a distributed solution to the assignment problem  which by
itself has been explored extensively  e.g. [21  22].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

This generalization was ﬁrst considered in [9]  and later enhanced in [8]  where an algorithm that
achieves an expected sum of regrets of near-O (log T ) was presented. However  this algorithm
requires communication between players. It is based on the distributed auction algorithm in [21] 
which is not fully distributed. It requires that players can observe the bids of other players. This was
possible in [8  9] since it was assumed that the players could observe the actions of other players 
which allows them to communicate by using the arm choices as a signaling method. In [19]  the
authors suggest an algorithm that only assumes that users can sense all the channels without knowing
which channels was chosen by whom. This algorithm requires less communication than [8]  but has
no regret guarantees. In wireless networks  assuming that each user can hear all other transmissions
(fully connected network) is very demanding in practice. In a fully distributed scenario  players
only have access to their previous actions and rewards. However  to date there is no completely
distributed algorithm that converges to the exact optimal solution of the assignment problem. The
fully distributed multi-armed bandit problem remains unresolved.
Our work generalizes [7] for different expectations for different players and [8  9  19] for a fully
distributed scenario with no communication between players.
Recently  very powerful payoff-based dynamics were introduced [23–25]. These dynamics only
require each player to know her own action and the reward she received for that action. Speciﬁcally 
the dynamics in [24] guarantee that the optimal sum of utilities strategy proﬁle will be played a
sufﬁciently large portion of the time  even if it is not a Nash equilibrium. The crucial issue of
applying these results to our case is that they all assume interdependent games. In an interdependent
game  each group of players can always inﬂuence at least one player from outside this group. In the
multiplayer multi-armed bandit collision model  this does not hold. A player in a collision receives
zero reward. Nothing that other players (who chose other arms) can do will change that.
In this paper  we suggest novel modiﬁed dynamics that behave similarly to [24]  but in our non-
interdependent game. Speciﬁcally  they guarantee that the optimal solution to the assignment prob-
lem is played a considerable amount of time. We present a fully distributed multi-player multi-armed
bandit algorithm for the resource allocation and collision scenario  based on these modiﬁed dynam-
ics. By fully distributed we mean that players only have access to their own actions and rewards.

This is the ﬁrst algorithm that achieves a poly-logarithmic expected sum of regrets  near- O(cid:0)log2 T(cid:1) 

with a matrix of expected rewards and no communication at all between players.

2 Problem Formulation
We consider a stochastic game with the set of players N = {1  ...  N} and a ﬁnite time horizon T .
The horizon T is not known in advance by any of the players. The discrete turn index is denoted by t.
The strategy space of each player is a set of K arms with indices that are denoted by i  j = 1  ...  K.
We assume that K ≥ N. At each turn t  all players simultaneously pick one arm each. The arm that
player n chooses at time t is an (t) and the strategy proﬁle at time t is a (t). Players do not know
which arms the other players chose  and need not even know how many other players are there.
Deﬁne the set of players that chose arm i in strategy proﬁle a
Ni (a) = {n| an = i} .
Deﬁne the no-collision indicator of arm i in strategy proﬁle a

(1)

(cid:110) 0

1

(cid:12)(cid:12)(cid:12)Ni (a)
(cid:12)(cid:12)(cid:12) > 1

o.w.

R =

T(cid:88)

N(cid:88)

υn (a∗) − T(cid:88)

N(cid:88)

t=1

n=1

t=1

n=1

2

ηi (a) =

.

(2)

The instantaneous utility of player n in strategy proﬁle a (t) in time t is

υn (a (t)) = rn an(t) (t) ηan(t) (a (t))

(3)
where rn an(t) (t) is a random reward which is assumed to have a continuous distribution on [0  1].
The sequence of rewards {rn i (t)}t of arm i for player n is i.i.d. (“in time”) with expectation µn i.
Next we deﬁne the expected total regret  which we want our distributed algorithm to minimize.
Deﬁnition 1. Denote the expected utility of player n in strategy proﬁle a by gn (a) = E {υn (a)}.
The total regret is deﬁned as the random variable

rn an(t) (t) ηan(t) (a (t))

(4)

where

N(cid:88)

n=1

gn (a) .

a∗ = arg max

a

(5)

The expected total regret ¯R (cid:44) E {R} is the average of (4) over the randomness of the rewards
{rn i (t)}t  that dictate the random actions {an (t)}.
The problem in (5) is no other than the famous assignment problem [20] on the N × K matrix of
expectations {µn i}. In this sense  our problem is a generalization of the distributed assignment
problem to an online learning framework.
Assuming continuously distributed rewards is well justiﬁed in wireless networks. Given no collision 
the quality of an arm (channel) always has a continuous measure like the SNR or the channel gain.
However  this assumption is only used in two arguments and can be easily replaced without changing
the analysis in this paper. The ﬁrst argument is that since the probability for zero reward in a non-
collision is zero  players can safely rule out collisions in their estimation of the expected reward.
In the case where the probability for a zero reward is not zero  we can assume instead that each
player can observe her collision indicator in addition to her reward. Knowing whether other players
chose the same arm is a very modest requirement compared to assuming that players can observe
the actions of other players. The second argument is that the continuity of the rewards’ distributions
makes the solution of (5)  with the estimated expectations  unique with probability 1. We can assume
instead that {µn i} are generated at random using a continuous distribution  so the optimal solution
is unique with probability 1 (i.e.  “for almost all games”)  with arbitrary distributions for the rewards
that have expectations {µn i}.
According to the seminal work in [26]  the optimal regret of the single-player case is logarithmic;
i.e.  O (log T ). Players do not help each other; hence  we expect the expected total regret lower
bound to be logarithmic at best. The next proposition shows that this is indeed the case.
Proposition 1. The expected total regret is at least Ω (log T ).

Proof. See Section 8 (supplementary material).

3 The Game of Thrones Algorithm

When all players have the same arm expectations  the exploration phase is used to identify the N
best arms. Once the best arms are identiﬁed  players need to coordinate to be sure that each of them
will sit on a different “chair” (see the Musical Chairs algorithm in [7]). When players have different
arm expectations  a non-cooperative game is induced where the estimated expected rewards serve
as utilities. In this game  each player has a speciﬁc chair (throne) she must sit on to avoid causing a
linear regret. This throne is the unique arm a player must play in the allocation that maximizes the
sum of the expected rewards of all players. Any other solution will result in linear (in T ) expected
total regret. Note that our assignment problem has a unique optimal allocation with probability 1 (as
shown in Lemma 4).
The total time needed for exploration increases with T since the cost of being wrong becomes
higher. When T is known to the players  a long enough exploration can be accomplished at the
beginning of the game. In order to maintain the right balance between exploration and exploitation
when T is not known in advance to the players  we divide the T turns into epochs  one starting
immediately after the other. Each epoch is further divided into three phases - exploration  Game
of Thrones (GoT) and exploitation. During the exploration phase  players estimate the expected
reward of each arm. The goal of the GoT phase is to let the players distributedly identify the optimal
solution for the assignment problem on the estimated expected rewards from the exploration phase.
It is done by playing a game with the estimated expectations as utilities  using random dynamics
that probabilistically prefer strategy proﬁles with a higher sum of utilities. In the exploitation phase 
each player plays the constant action she deduced from the GoT phase. The division into epochs
is depicted in Fig. 1. The GoT Algorithm and GoT Dynamics are described in Algorithm 1 and
Algorithm 2  respectively.

3

Figure 1: Epochs structure. Depicted are the ﬁrst and the k-th epochs.

Algorithm 1 Game of Thrones Algorithm
Initialization - Set on i = 0 and sn i (0) = 0 for all i. Set δ > 0  0 < ρ < 1 and ε > 0. Deﬁne kT
as the index of the last epoch where the horizon is T .

For each epoch k = 1  ...  kT

1. Exploration Phase - for the next c1 turns

(a) Sample an arm i uniformly at random from all K arms.
(b) Receive rn i (t) and set ηi (a (t)) = 0 if rn i (t) = 0 and ηi (a (t)) = 1 otherwise.
(c) If ηi (a (t)) = 1 then update on i = on i + 1 and sn i (t) = sn i (t − 1) + rn i (t).
(d) Estimate the expectation of the arm i by µk

  for each i = 1  ...  K.

2. GoT Phase - for the next c2k1+δ turns  play according to Algorithm 2 with ε and ρ.

(a) Starting from the dg = (cid:6)ρc2k1+δ(cid:7)-th turn inside the GoT Phase  keep track on the

n i = sn i(t)
on i

number of times each action was played that resulted in being content

c2k1+δ(cid:88)

F n

t (i) =

I (an (l) = i  Mn (l) = C)

where I is the indicator function.

3. Exploitation Phase - for the next c32k turns  play

l=dg

ak
n = arg max

i

F n

t (i)

End

(6)

(7)

(8)

(9)

(10)

(11)

Algorithm 2 Game of Thrones Dynamics
Initialization - Let c ≥ N. Each player n has a personal state Mn  either content C or discontent
D  which determines her mixed strategy. Each player also keeps a baseline action an and her utility
un (a).
is un. Denote un max = max
In each turn during the GoT Phase

a

• A content player chooses an action according to

(cid:40)

εc

(cid:12)(cid:12)(cid:12)An

(cid:12)(cid:12)(cid:12)−1

1 − εc

pan
n =

an (cid:54)= an

an = an

.

• A discontent player chooses an action uniformly at random; i.e. 

pan
n =

1

|An|   ∀an ∈ An.

The transitions between C and D are determined as follows:

• If an = an and un > 0  then a content player remains content with probability 1

• If an (cid:54)= an or un = 0 or Mn = D  then (C/D denoting either C or D)

[an  C/D] →

End

[an  C] → [an  C]

un

εun max−un

un max

1 − un

un max

εun max−un .

(cid:40)

[an  C]
[an  D]

4

In this paper  we prove the following main result.
Theorem 1 (Main Theorem). Assume that the rewards {rn i (t)}t are independent in n and i.i.d. in
time t  with continuous distributions on [0  1] with positive expectations {µn i}. Let the game have a
ﬁnite horizon T   unknown to the players. Denote the optimal objective by J1 = max
n=1 gn (a)
and the second best one by J2. Let each player play according to Algorithm 1  with a small enough
ε  exploration phase length of c1 > 16N 2K
(J1−J2)2 and δ > 0. Then  for large enough T   the expected
total regret is upper bounded by

(cid:80)N

a

¯R ≤ 3c2N log2+δ

2

+ 2

= O

log2+δ T

.

(cid:19)

(cid:16)

(cid:17)

(cid:18) T

c3

T ≥ E−1(cid:88)
(cid:0)c1 + c2k1+δ + c32k(cid:1) ≥ c3
(cid:16) T

(cid:17)

k=1

(cid:0)2E − 2(cid:1)

(12)

(13)

Proof. Let δ > 0. Denote the number of epochs that start within T turns by E. Since

+ 2

c3

E is upper bounded by E ≤ log2
. Denote by Pe k and Pc k the error probabilities of the
exploration and GoT phases of epoch k respectively. Observe that if none of these errors occurred 
the optimal solution to (5) is played in the k-th exploitation phase  which adds no additional regret
to the total regret. We will prove in Lemma 2 and Lemma 5 that Pe k ≤ 4K 2e−k and Pc k ≤

(cid:1) is the mixing time of the Markov chain of
(cid:1) depends on N  K and ε   so there exists a k0 such that for all

(cid:0) 1

  where A0 is a constant and Tm

− c2(1−ρ)
1728Tm( 1
8 )

A0e
the GoT Dynamics. Note that Tm
k > k0 we have

(cid:0) 1

8

k1+δ

8

− c2(1−ρ)
1728Tm( 1
8 )
e

kδ

<

.

(14)

We now bound the expected total regret of epoch k > k0  denoted by ¯Rk  as follows

¯Rk ≤(cid:0)c1 + c2k1+δ(cid:1) N +

(cid:32)

4K 2e−k + A0e

− c2 (1−ρ)
1728Tm( 1
8 )

c32kN ≤

1
2

k1+δ(cid:33)

c1N + 2A0c3N βk + c2k1+δN (15)

(cid:19)

(cid:18) T

c3

for some constant β < 1. We conclude that  for some additive constant C 

E(cid:88)

k=1

E(cid:88)

k=k0+1

¯R =

¯Rk ≤

(a)

C + 2c2N

k1+δ ≤ C + 2c2N E2+δ ≤

(b)

C + 2c2N log2+δ

2

+ 2

(16)

where (a) follows since completing the last epoch to a full epoch increases ¯Rk  and (b) is (13).

If either the exploration or the GoT phases fail  the regret becomes linear with T . Like many other
online learning algorithms  we avoid a linear expected regret by ensuring that the error probabilities
vanish with T . By using instead a single epoch with a constant duration for the ﬁrst two phases  we
obtain that with high probability (in T ) our algorithm achieves a constant regret (as in [7]). However 
our main result is formulated using the more conservative formulation of the expected regret.

4 Exploration Phase - Estimation of the Expected Rewards

In this section  we describe the exploration phase  and analyze its addition to the expected total
regret. At the beginning of the game  players still do not have any evaluation of the K different
arms. They estimate these values on the run  based on the rewards they get. We propose a pure
exploration phase where each player picks an arm uniformly at random  similar to the one suggested
in [7]. Note that in contrast to [7]  we do not assume that T is known to the players. Hence  the
exploration phase is repeated in each epoch. In each epoch  only a constant number c1 of turns is

5

dedicated to exploration. However  the estimation uses all the previous exploration phases  so that
the number of samples for estimation grows linearly with time.
The estimation of the expected rewards is never perfect. Hence  the optimal solution to the assign-
ment problem given the estimated expectations might be different from the optimal solution with
the correct expectations. However  if the uncertainty of the true value of each expectation is small
enough  we expect both of these optimal assignments to coincide. This is exactly the precision we
require from the estimation  as formulated in the following lemma.
Lemma 1. Assume that {µn i} are known up to an uncertainty of ∆  i.e.  |ˆµn i − µn i| ≤ ∆ for
each n and i for some {ˆµn i}. Denote the optimal assignment by a1 = arg max
n=1 gn (a)
n=1 gn (a1). Denote the second best objective and the corresponding

and its objective by J1 =(cid:80)N

(cid:80)N

a

assignment by J2 and a2  respectively. If ∆ < J1−J2

2N then

N(cid:88)

n=1

N(cid:88)

n=1

arg max

a

gn (a) = arg max

a

ˆµn an ηan (a)

(17)

so that the optimal assignment does not change due to the uncertainty.

Proof. See Section 8 (supplementary material).

If the exploration phase is long enough  players know their arm expectations accurately enough with
a very small failure probability. The following lemma concludes this section by providing an upper
bound for the probability that the estimation for epoch k failed.

Lemma 2 (Exploration Error Probability). Let(cid:8)µk
ηan (a). Also denote J1 = (cid:80)N

ing all the exploration phases up to epoch k. Denote a∗ = arg max
arg max
J2. If the length of the exploration phase satisﬁes c1 > 16N 2K

(cid:9) be the estimated reward expectations us-

(cid:80)N
n=1 gn (a) and ak∗ =
n=1 gn (a∗) and the second best1 objective by
(J1−J2)2   then after the k-th epoch we have

(cid:80)N

n=1 µk

n an

n i

a

a

Pe k (cid:44) Pr(cid:0)a∗ (cid:54)= ak∗(cid:1) ≤ 4K 2e−k.

(18)

Proof. See Section 8 (supplementary material).

5 Game of Thrones Dynamics Phase

In this section we analyze the game of thrones (GoT) dynamics between players. These dynamics
guarantee that the optimal state will be played a signiﬁcant amount of time  and only require the
players to know their own action and the received payoff on each turn. Note that these dynamics
assume deterministic utilities. We use the estimated expected reward of each arm as the utility for
this step  and zero if a collision occurred. This means that players ignore the numerical reward they
receive by choosing the arm  as long as it is positive.
Deﬁnition 2. The game of thrones G of epoch k has the N players of the original multi-armed
bandit game. Each player can choose from among the K arms  so An = {1  ...  K} for each n. The
utility of player n in the strategy proﬁle a = (a1  ...  aN ) is

un (a) = µk

n an

ηan (a)

(19)

where µk
have ended  up to epoch k. Also denote un max = max

n an is the estimation of the expected reward of arm an  from all the exploration phases that

un (a).

a

Our dynamics belong to the family introduced in [23–25]. These dynamics guarantee that the opti-
mal sum of utilities strategy proﬁles will be played a sufﬁciently large portion of the turns. However 
they all rely on the following structural property of the game  called interdependence.

1Note that this is the second best objective and not the second best allocation  so J2 < J1. If all allocations

have the same objective then this Lemma trivially holds with c1 ≥ 1.

6

m∈J Am such that un (a(cid:48)

J   a−J ) (cid:54)= un (aJ   a−J ).

J ∈(cid:81)

Deﬁnition 3. A game G with ﬁnite action spaces A1  ... AN is interdependent if for every strategy
proﬁle a ∈ A1 × ... × AN and every set of players J ⊂ N  there exists a player n /∈ J and a choice
of actions a(cid:48)
Our GoT is not interdependent. To see this  pick any strategy proﬁle a such that some players are
in a collision while others are not. Choose J as the set of all players that are not in a collision. All
players outside this set are in a collision  and there does not exist any colliding player that the actions
of the non-colliding players can make her utility non-zero.
The GoT Dynamics in Algorithm 2 modify [24] such that interdependency is no longer needed.
Note that in comparison with [24]  our dynamics assign zero probability that a player with un = 0
(in a collision) will be content. Additionally  we do not need to keep the benchmark utility as part
of the state. A player knows with probability 1 whether there was a collision  and if there was not 
she gets the same utility for the same arm. Our dynamics require that each player uses c ≥ N. The
number of players N might be unknown. In this case  players can use c ≥ K  since the number of
arms is known and K ≥ N by deﬁnition of the problem.

The GoT dynamics induce a Markov chain over the state space Z = (cid:81)N

n=1 (An × M)  where
M = {C  D}. The transition matrix of this Markov chain is denoted by P ε. The following lemma
characterizes the recurrence classes of the unperturbed chain P 0 (with ε = 0). In [24]  interdepen-
dency was used to prove the same result. This is the sole reason interdependency was required in the
ﬁrst place. We provide an alternative proof that does not require interdependency but instead uses
the fact that in our modiﬁed dynamics  no player can be content with un = 0. Note that this proof
exploits the structure of the GoT  and cannot be applied to a more general game.
Lemma 3. Denote by D0 the set of all the discontent states (all players are discontent) and by
C0 the set of all singleton content states (all players are content). The recurrence classes of the
unperturbed process P 0 are D0 and all z ∈ C0.

Proof. In P 0  there is no path between the discontent states and the content ones. Moreover  all
the discontent states are connected and all the content states are absorbing (i.e.  singletons). Now
assume there is a different recurrence class. In any state in this class  denoted zC/D  not all the
players are content  otherwise this is a z ∈ C0 singleton. Denote one of the discontent players by
n. Since she chooses her action at random  there is a positive probability that she will pick the same
arm as any of the content players. By doing so  she changes the state of this player to discontent
with probability 1. With ε = 0  every discontent player remains so with probability one. On the next
turn  a discontent player may again choose the arm of a content player with a positive probability.
By repeating this process  we conclude that there is a positive probability that all players become
discontent. Hence  zC/D is connected to D in P 0. We conclude that this different recurrence class
is in fact connected to D  which is a contradiction.

The process Z of the GoT dynamics is a regular perturbed Markov chain  deﬁned as follows.
Deﬁnition 4. P ε is called a regular perturbed Markov Process if P ε is ergodic for all sufﬁciently
small ε > 0 and for every z  z(cid:48) ∈ Z we have
lim
ε→0+

zz(cid:48) = P 0
P ε
zz(cid:48)

(20)

and if P ε

zz(cid:48) > 0 for some ε > 0 then

< ∞

(21)

0 < lim
ε→0+

P ε
zz(cid:48)

εr(z→z(cid:48))

for some real non-negative r (z → z(cid:48)) that is called the resistance of the transition z → z(cid:48).
Next we deﬁne stochastic stability  which is a powerful convergence analysis tool.
Deﬁnition 5. Let P ε be regular perturbed Markov process and µε its unique stationary distribution
that exists for ε > 0. A state z ∈ Z is stochastically stable if and only if

µε (z) > 0.

lim
ε→0+

(22)

In [24]  it is shown for their dynamics that only the states with the maximal sum of utilities are
stochastically stable. For a small enough ε the dynamics will visit the stochastically stable states

7

very often. However  there might be several stochastically stable states and the dynamics might
ﬂuctuate between them. Fortunately  in our case  as shown in the following lemma  there is a unique
optimal state with probability one. For a small enough ε the unique optimal state is played more
than half of the times  which allows for the players to distributedly agree on the optimal solution.
This uniqueness is due to the continuous distribution of the rewards that makes the distribution of
the empirical estimation for the expectations continuous as well.
Lemma 4. The optimal solution to max

n=1 un (a) is unique with probability 1.

(cid:80)N

Proof. First note that an optimal solution must not have any collisions  otherwise it can be improved

(cid:9) be the estimated reward expectations in epoch k. For two different

a

. However  ˜a and
a∗ must differ in at least one assignment. Since the distributions of the rewards rn an are con-
n an (as a sum of the average of the rewards). Hence

n=1 µk

n=1 µk

n=1 µk

n a∗

n ˜an

n

= (cid:80)N

since K ≥ N. Let(cid:8)µk
solutions ˜a (cid:54)= a∗ to be optimal  we must have (cid:80)N
tinuous  so are the distributions of(cid:80)N
(cid:16)(cid:80)N
=(cid:80)N

(cid:17)

Pr

n i

n=1 µk

n a∗

n=1 µk

n ˜an

= 0  and the result follows.

n

Next we show that only the unique optimal state is stochastically stable. This means that after
enough time  the action that a player played most of the time is highly likely to be part of the unique
optimal solution. This is crucial for the success of the exploitation phase.
Theorem 2. Deﬁne ak∗ = arg max

cally stable state is z∗ =(cid:2)ak∗  C N(cid:3) with probability 1.

n=1 un (a). Under the GoT dynamics  the unique stochasti-

(cid:80)N

a

Proof. See Section 8 (supplementary material).

((cid:101)a1  ...  ˜aN ) where(cid:101)an = arg max

Now we can prove the main lemma of this section that gives an upper bound for the probability that
the GoT phase does not lead to the optimal solution.
Lemma 5 (GoT Error Probability). Let δ > 0. Deﬁne ak∗ = arg max

(cid:80)N
n=1 un (a) and (cid:101)a =

t (i) for all n. For a small enough ε  the error probability of the
F n
k-th GoT phase  which is the probability that another strategy proﬁle than ak∗ will be played in the
exploitation phase  is bounded as follows

a

i

Pc k (cid:44) Pr(cid:0)(cid:101)a (cid:54)= ak∗(cid:1) ≤ A0e

− c2(1−ρ)
1728Tm( 1
8 )

k1+δ

(23)

where A0 is a constant with respect to t (or k)  and may depend on N  K  ε and the initial state.

Proof. See Section 8 (supplementary material).

6 Numerical Simulations and Practical Considerations

The total regret compares the sum of utilities to the ideal one that could have been achieved in a cen-
tralized scenario. With no communication between players and with a matrix of expected rewards 
the gap to this ideal naturally increases. In this scenario  converging to the exact optimal solution
might take a long time  even for the (unknown) optimal algorithm. Our main result provides theo-
retical guarantees for the asymptotic performance of our algorithm  which suggest that performance
improves with time on its way to converge to the optimal solution. The simulations in this section
complete the picture by showing how the sum of utilities behaves in the non-asymptotic regime.
We simulated a multi-armed bandit game with {µn i} that are chosen independently and uniformly
at random in [0.05  0.95]. The rewards are generated as rn i (t) = µn i + zn i (t) where {zn i (t)}
are independent and uniformly distributed on [−0.05  0.05] for each n  i.
In the simulations presented here we use δ = 0 since it yields good results in practice. We conjecture
that the bound (23) is not tight for our particular Markov chain and indicator function  since it applies
for all Markov chains with the same mixing time and all functions on the states. This explains why
modest choices of c2 are large enough and the kδ factor in the exponent is not needed in practice.

8

The lengths of the phases should be chosen so that the exploitation phase occupies most of the turns
already in early epochs  while allowing for a considerable GoT phase. Note that the exploration
(c1) is much easier than the GoT phase (c2) and achieves a good accuracy relatively fast. Hence
we choose c1 = 1000  c2 = c3 = 6000. We use ρ = 1
2 in the simulations we present  since the
performance is very similar for ρ values not too close to zero or one. We use c = N  that gives the
highest possible escape probability of εc from a content state.

In Fig. 2  we present the sample mean of the accumulated sum of utilities(cid:80)N

(cid:80)t

1
t

n=1

τ =1 un (a (τ ))
as a function of time t  averaged over 100 experiments. The performance was normalized by the
optimal solution to the assignment problem (for each experiment). On the left graph we compare our
sum of utilities for N = K = 5 to that of the selﬁsh algorithm  reported to achieve good performance
for this problem in [17]  and to a random choice of arms. The selﬁsh algorithm consists of each
player playing a standard upper conﬁdence bound (UCB) algorithm  treating collisions as any other
value for the reward. Both algorithms perform much better than the random selection. Our sum of
utilities is slightly better and is increasing with time. More importantly  our algorithm has provably
performance guarantees while [17] have none. On its way to converge to the optimal solution  our
algorithm performs very well straight from the beginning. While visiting near-optimal solutions
inﬂicts linear regret at the beginning  it is very satisfying in practice considering that players cannot
communicate and have a matrix of expected rewards. Similar results were obtained for different
choices of c1  c2  c3. On the right graph  we present the median and the best 90% of the sample
mean of the sum of utilities for K = N = 6 and ε = 0.01  0.001  0.0001. It is evident that our
algorithm behaves very similarly in all the 100 experiments  indicating that it is robust and rarely
fails. Additionally  our algorithm behaves very similarly for a wide range of ε values (two orders
of magnitude). This supports the intuition that there is no threshold phenomenon on ε (becoming
“small enough”)  since the dynamics prefer states with a higher sum of utilities for all ε < 1.

Figure 2: Sample mean of the sum of utilities as a function of time  averaged over 100 experiments.

7 Conclusions and Future Work

In this paper  we considered a multi-player multi-armed bandit game where players compete over
the arms as resources. In contrast to all existing multi-player bandit problems  we allow for different
arm expected rewards between players and assume each player only knows her own actions and
rewards. We proposed a novel fully distributed algorithm that achieves a poly-logarithmic expected

total regret of near-O(cid:0)log2 T(cid:1) when the horizon T is unknown to the players.

Our simulations suggest that tuning the parameters for our algorithm is a relatively easy task in prac-
tice. The algorithm designer can do so by simulating a random model for the unknown environment
and varying the parameters  knowing that only a very slack accuracy is needed for the tuning.
It is still an open question whether the lower bound Ω (log T ) on the expected total regret is tight for
a fully distributed algorithm.
Our game is not a general one but has a structure that allowed us to modify the dynamics such
that the interdependence assumption can be dropped. We conjecture that the same structure can
be exploited to accelerate the convergence rate of the GoT dynamics  speciﬁcally by relaxing the
c ≥ N condition.

9

t×10600.511.522.53sample mean - sum of utilities0.20.30.40.50.60.70.80.91GoTSelfish UCBRandom Selectiont×10600.511.522.533.544.55sample mean - sum of utilities0.70.750.80.850.90.951ǫ=0.0001  medianǫ=0.0001  best 90% ǫ=0.001  medianǫ=0.001  best 90%ǫ=0.01  best 90%ǫ=0.01  medianReferences
[1] S. Shahrampour  A. Rakhlin  and A. Jadbabaie  “Multi-armed bandits in multi-agent net-
works ” in Acoustics  Speech and Signal Processing (ICASSP)  2017 IEEE International Con-
ference on  2017  pp. 2786–2790.

[2] E. Hillel  Z. S. Karnin  T. Koren  R. Lempel  and O. Somekh  “Distributed exploration in multi-

armed bandits ” in Advances in Neural Information Processing Systems  2013  pp. 854–862.

[3] P. Landgren  V. Srivastava  and N. E. Leonard  “Distributed cooperative decision-making in
multiarmed bandits: Frequentist and Bayesian algorithms ” in Decision and Control (CDC) 
2016 IEEE 55th Conference on  2016  pp. 167–172.

[4] N. Cesa-Bianchi  C. Gentile  Y. Mansour  and A. Minora  “Delay and cooperation in non-

stochastic bandits ” in Conference on Learning Theory  2016  pp. 605–622.

[5] B. Szorenyi  R. Busa-Fekete  I. Hegedus  R. Orm´andi  M. Jelasity  and B. K´egl  “Gossip-based
distributed stochastic bandit algorithms ” in International Conference on Machine Learning 
2013  pp. 19–27.

[6] N. Korda  B. Sz¨or´enyi  and L. Shuai  “Distributed clustering of linear bandits in peer to peer
networks ” in International Conference on Machine Learning  vol. 48  2016  pp. 1301–1309.
[7] J. Rosenski  O. Shamir  and L. Szlak  “Multi-player bandits–a musical chairs approach ” in

International Conference on Machine Learning  2016  pp. 155–163.

[8] N. Nayyar  D. Kalathil  and R. Jain  “On regret-optimal learning in decentralized multi-player
multi-armed bandits ” IEEE Transactions on Control of Network Systems  vol. PP  no. 99  pp.
1–1  2016.

[9] D. Kalathil  N. Nayyar  and R. Jain  “Decentralized learning for multiplayer multiarmed ban-

dits ” IEEE Transactions on Information Theory  vol. 60  no. 4  pp. 2331–2345  2014.

[10] K. Liu and Q. Zhao  “Distributed learning in multi-armed bandit with multiple players ” IEEE

Transactions on Signal Processing  vol. 58  no. 11  pp. 5667–5681  2010.

[11] S. Vakili  K. Liu  and Q. Zhao  “Deterministic sequencing of exploration and exploitation for
multi-armed bandit problems ” IEEE Journal of Selected Topics in Signal Processing  vol. 7 
no. 5  pp. 759–767  2013.

[12] L. Lai  H. Jiang  and H. V. Poor  “Medium access in cognitive radio networks: A competi-
tive multi-armed bandit framework ” in Signals  Systems and Computers  2008 42nd Asilomar
Conference on  2008  pp. 98–102.

[13] A. Anandkumar  N. Michael  A. K. Tang  and A. Swami  “Distributed algorithms for learning
and cognitive medium access with logarithmic regret ” IEEE Journal on Selected Areas in
Communications  vol. 29  no. 4  pp. 731–745  2011.

[14] H. Liu  K. Liu  and Q. Zhao  “Learning in a changing world: Restless multiarmed bandit with
unknown dynamics ” IEEE Transactions on Information Theory  vol. 59  no. 3  pp. 1902–1916 
2013.

[15] O. Avner and S. Mannor  “Concurrent bandits and cognitive radio networks ” in Joint European
Conference on Machine Learning and Knowledge Discovery in Databases  2014  pp. 66–81.
[16] N. Evirgen and A. Kose  “The effect of communication on noncooperative multiplayer multi-

armed bandit problems ” in arXiv preprint arXiv:1711.01628  2017  2017.

[17] L. Besson and E. Kaufmann  “Multi-player bandits revisited ” in Algorithmic Learning Theory 

2018  pp. 56–92.

[18] J. Cohen  A. H´eliou  and P. Mertikopoulos  “Learning with bandit feedback in potential
games ” in Proceedings of the 31th International Conference on Neural Information Process-
ing Systems  2017.

[19] O. Avner and S. Mannor  “Multi-user lax communications: a multi-armed bandit approach ” in
INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communica-
tions  IEEE  2016  pp. 1–9.

[20] C. H. Papadimitriou and K. Steiglitz  Combinatorial optimization: algorithms and complexity.

Courier Corporation  1998.

10

[21] D. P. Bertsekas  “The auction algorithm: A distributed relaxation method for the assignment

problem ” Annals of operations research  vol. 14  no. 1  pp. 105–123  1988.

[22] M. M. Zavlanos  L. Spesivtsev  and G. J. Pappas  “A distributed auction algorithm for the
assignment problem ” in Decision and Control  2008. CDC 2008. 47th IEEE Conference on 
2008  pp. 1212–1217.

[23] B. S. Pradelski and H. P. Young  “Learning efﬁcient Nash equilibria in distributed systems ”

Games and Economic behavior  vol. 75  no. 2  pp. 882–897  2012.

[24] J. R. Marden  H. P. Young  and L. Y. Pao  “Achieving pareto optimality through distributed

learning ” SIAM Journal on Control and Optimization  vol. 52  no. 5  pp. 2753–2770  2014.

[25] A. Menon and J. S. Baras  “Convergence guarantees for a decentralized algorithm achieving

pareto optimality ” in American Control Conference (ACC)  2013  2013.

[26] T. L. Lai and H. Robbins  “Asymptotically efﬁcient adaptive allocation rules ” Advances in

applied mathematics  vol. 6  no. 1  pp. 4–22  1985.

[27] K.-M. Chung  H. Lam  Z. Liu  and M. Mitzenmacher  “Chernoff-Hoeffding bounds for Markov
chains: Generalized and simpliﬁed ” in 29th International Symposium on Theoretical Aspects
of Computer Science  2012  p. 124.

11

,Mahdi Karami
Dale Schuurmans
Csaba Szepesvari
Ilai Bistritz
Amir Leshem