2019,Toward a Characterization of Loss Functions for Distribution Learning,In this work we study loss functions for learning and evaluating probability distributions over large discrete domains. Unlike  classification or regression where a wide variety of loss functions are used  in the distribution learning and density estimation literature  very few losses outside the dominant \emph{log loss} are applied. We aim to understand this fact  taking an axiomatic approach to the design of loss functions for distributions. We start by proposing a set of desirable criteria that any good loss function should satisfy. Intuitively  these criteria require that the loss function faithfully evaluates a candidate distribution  both in expectation and when estimated on a few samples. Interestingly  we observe that \emph{no loss function} possesses all of these criteria. However  one can circumvent this issue by introducing a natural restriction on the set of candidate distributions.  Specifically  we require that candidates are \emph{calibrated} with respect to the target distribution  i.e.  they may contain less information than the target but otherwise do not significantly distort the truth. We show that  after restricting to this set of distributions  the log loss and a large variety of other losses satisfy the desired criteria. These results pave the way for future investigations of distribution learning that look beyond the log loss  choosing a loss function based on  application or domain need.,Toward a Characterization of Loss Functions for

Distribution Learning

Nika Haghtalab∗
Cornell University

nika@cs.cornell.edu

Cameron Musco∗
UMass Amherst

cmusco@cs.umass.edu

Bo Waggoner†
U. Colorado

bwag@colorado.edu

Abstract

In this work we study loss functions for learning and evaluating probability dis-
tributions over large discrete domains. Unlike classiﬁcation or regression where
a wide variety of loss functions are used  in the distribution learning and density
estimation literature  very few losses outside the dominant log loss are applied.
We aim to understand this fact  taking an axiomatic approach to the design of loss
functions for distributions. We start by proposing a set of desirable criteria that
any good loss function should satisfy. Intuitively  these criteria require that the loss
function faithfully evaluates a candidate distribution  both in expectation and when
estimated on a few samples. Interestingly  we observe that no loss function pos-
sesses all of these criteria. However  one can circumvent this issue by introducing
a natural restriction on the set of candidate distributions. Speciﬁcally  we require
that candidates are calibrated with respect to the target distribution  i.e.  they may
contain less information than the target but otherwise do not signiﬁcantly distort
the truth. We show that  after restricting to this set of distributions  the log loss and
a large variety of other losses satisfy the desired criteria. These results pave the
way for future investigations of distribution learning that look beyond the log loss 
choosing a loss function based on application or domain need.

1

Introduction

Estimating a probability distribution given independent samples from that distribution is a fundamental
problem in machine learning and statistics [e.g. 23  2  24  5]. In machine learning applications  the
distribution of interest is often over a very large but ﬁnite sample space  e.g.  the set of all English
sentences up to a certain length or images of a ﬁxed size in their RGB format.
A central problem is evaluating the learned distribution  most commonly using a loss function. Such
evaluation is an important task in its own right as well as central to some learning techniques. Given
a ground truth distribution p over a set of outcomes X and a sample x ∼ p  a loss function (cid:96)(q  x)
evaluates the performance of a candidate distribution q in predicting x. Generally  (cid:96)(q  x) will be
higher if q places smaller probability on x. Thus  in expectation over x ∼ p  the loss will be lower
for candidate distributions that closely match p.
The dominant loss applied in practice is the log loss (cid:96)(q  x) = ln(1/qx)  which corresponds to the
learning technique of log likelihood maximization. Surprisingly  few other losses are ever considered.
This is in sharp contrast to other areas of machine learning  including in supervised learning where
different applications have necessitated the use of different losses  such as the squared loss  hinge
loss  (cid:96)1 loss  etc. However  alternative loss functions can be beneﬁcial for distribution learning on
large domains  as we show with a brief motivating example.

∗Research conducted while at Microsoft Research  New England.
†Research conducted while at Microsoft Research  New York City.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Motivating example.
In many learning applications  one seeks to ﬁt a complex distribution with a
simple model that cannot fully capture its complexity. This includes e.g.  noise tolerant or agnostic
learning. As an example  consider modeling the distribution over English words with a character
trigram model. While this model  trained by minimizing log loss  ﬁts the distribution of English
words relatively well  its performance signiﬁcantly degrades if a small amount of mostly-irrelevant
data is added  e.g. if the dataset includes a small fraction of foreign language words. The model is
unable to ﬁt the ‘tail’ of the distribution (corresponding to foreign words)  however  in trying to do so
it performs signiﬁcantly worse on the ‘head’ of the distribution (corresponding to common English
words). This is due to the fact that minimizing log loss requires qx to not be much smaller than px
for all x. A more robust loss function  such as the log log loss  (cid:96)(q  x) = ln(ln(1/qx))  emphasizes
the importance of ﬁtting the ‘head’ and is less sensitive to the introduction of the foreign words. See
Figure 1 and the full version of the paper for details.

Samples from q1

Samples from q2

brappost

hild
me
on
ther

oneems

to

the
not
of

log loss(p) = 7.45

log log loss(p) = 1.91
log loss(q1) = 11.25

log log loss(q1) = 2.22

log loss(q2) = 12.26

log log loss(q2) = 2.18

Figure 1: Modeling the distribution of English words  corrupted with 12% French and German words with
character trigrams. Distribution q1 is trained by minimizing log loss. q2 achieves worse log loss but better log
log loss and better performance at ﬁtting the ‘head’ of the the target p  indicating that log log loss may be more
appropriate in this application. See the full version for more details.

Loss function properties.
In this paper  we start by understanding the desirable properties of
log loss and seek to identity other loss functions with such properties that can have applications
in various domains. A key characteristic of the log loss is that it is (strictly) proper. That is  the
true underlying distribution p (uniquely) minimizes the expected loss on samples drawn from p.
Properness is essential for loss functions  as without it minimizing the expected loss leads to choosing
an incorrect candidate distribution even when the target distribution is fully known. Log loss is also
local (sometimes termed pointwise). That is  the loss of q on sample x is a function of the probability
qx and not of qx(cid:48) for x(cid:48) (cid:54)= x. Local losses are preferred in machine learning  where qx is often
implicitly represented as the output of a likelihood function applied to x  but where fully computing
q requires at least linear time in the size of the sample space N and is infeasible for large domains 
such as learning the distribution of all English sentences up to a certain length.
It is well-known that log loss is the unique local and strictly proper loss function [19  22  13]. Thus 
requiring strict properness and locality already restricts us to using the log loss. At the same time 
these restrictive properties are not sufﬁcient for effective distribution learning  because:

• A candidate distribution may be far from the target yet have arbitrarily close to optimal loss.
Motivated by this problem  we deﬁne strongly proper losses that  if given a candidate far from
the target  will give an expected loss signiﬁcantly worse than optimal.
• A candidate distribution might be far from the target  yet on a small number of samples  it may
be likely to have smaller empirical loss than that of the target. This motivates our deﬁnition of
sample-proper losses.
• On a small number of samples  the empirical loss of a distribution may be far from its expected

loss  making evaluation impossible. This motivates our deﬁnition of concentrating losses.

Naively  it seems we cannot satisfy all our desired criteria: our only local strictly proper loss is the
log loss  which in fact fails to satisfy the concentration requirement (see Example 4). We propose
to overcome this challenge by restricting the set of candidate distributions  speciﬁcally to ones that
satisfy the reasonable condition of calibration. We then consider the properties of loss functions on 
not the set of all possible distributions  but the set of calibrated distributions.

Calibration and results. We call a candidate distribution q calibrated with respect to a target p
if all elements to which q assigns probability α actually occur on average with probability α in

2

the target distribution.3 This can also be interpreted as requiring q to be a coarsening of p  i.e.  a
calibrated distribution may contain less information than p but does otherwise not distort information.
While for simplicity we focus on exactly calibrated distributions  in the full version we extend our
results to a natural notion of approximate calibration. Our main results show that the calibration
constraint overcomes the impossibility of satisfying properness along with the our three desired
criteria.
Main results (Informal summary). Any (local) loss (cid:96)(q  x):=f
and monotonically increasing has the following properties subject to calibration:

such that f is strictly concave

(cid:16) 1

(cid:17)

qx

1. (cid:96) is strictly proper  i.e.  the target distribution minimizes expected loss.

2. If in addition f satisﬁes left-strong-concavity  then (cid:96) is strongly proper  i.e.  distributions

far from the target have signiﬁcantly worse loss.

3. If in addition to the above f grows relatively slowly  then (cid:96) is sample proper i.e.  on few
samples  distributions far from the target have higher empirical loss with high probability.

4. Under these same conditions  (cid:96) concentrates i.e.  on few samples  a distribution’s empirical

loss is a reliable estimate of its expected loss with high probability.

The above criteria are formally introduced in Section 3. Each criteria is parameterized and different
losses satisfy them with different parameters. We illustrate a few examples in Table 1 below. We
emphasize that all losses shown below achieve relatively strong bounds  only depending polylogarith-
mically on the domain size N. Thus  we view all of these loss functions as viable alternatives to the
log loss  which may be useful in different applications.

(cid:16)

ln 1
qx

(cid:96)(q  x)

ln 1
qx
for p ∈ (0  1]

(cid:17)p
(cid:16)

(cid:17)2

ln ln 1
qx

ln e2
qx

Strong Properness
E (cid:96)(q; x) − E (cid:96)(p; x)

Ω(2)

Ω(cid:0)2 (ln N )p−1(cid:1)
(cid:17)

(cid:16) 2

Ω

ln N

Ω(2)

Concentration

sample size m(γ  N )
˜O

(cid:18)
(cid:18)
(cid:18)
(cid:18)

˜O

˜O

˜O

γ−2 ln
γ−2 ln
γ−2 ln ln
γ−2 ln

γ

(cid:17)2(cid:19)
(cid:16) N
(cid:17)2p(cid:19)
(cid:16) N
(cid:17)2(cid:19)
(cid:16) N
(cid:17)4(cid:19)
(cid:16) N

γ

γ

γ

Sample Properness
sample size m(  N )

O(cid:0)−4 (ln N )2(cid:1)
O(cid:0)−4 (ln N )2(cid:1)
O(cid:0)−4(ln ln N )2(ln N )2(cid:1)
O(cid:0)−4(ln N )4(cid:1)

Table 1: Examples of loss function that demonstrate strong properness  sample properness  and concentration 
when restricted to calibrated distributions. In the above  N is the distributions support size  :=(cid:107)p − q(cid:107)1 is the
(cid:96)1 distance between p and q  and γ is an approximation parameter for concentration (see Section 4.2 for details).
We assume for simplicity that  ≥ 1/N and hide dependencies on a success probability parameter for sample
properness and concentration. ˜O(·) suppresses logarithmic dependence on 1/ and 1/γ.

1.1 Related work

Our work is directly inspired by applications of distribution estimation in very high-dimensional
spaces  such as language modeling [18]. However  we do not know of work in this area that takes a
systematic approach to designing loss functions.
A conceptually related research problem is that of learning distributions using computationally
and statistically efﬁcient algorithms. Beyond loss function minimization  a number of general-
purpose methods have been proposed for this problem  including using histograms  nearest neighbor
estimators  etc. See [15] for a survey of these methods. Much of the work in this space focuses
on learning structured or parametric distributions [7  16  17  6]  e.g.  monotone distributions or
mixtures of Gaussians. On the other hand  learning an unstructured discrete distribution with support
size N within (cid:96)1 distance  requires poly(N  1/) samples. Thus  works in this space typically
focus on designing computationally efﬁcient algorithms for optimal estimation using large sample
sets [24]. In comparison  we focus on unstructured distributions with prohibitively large supports and

3This deﬁnition is an adaptation of the standard calibration criterion applied to sequences of predictions

made by a forecaster [8  11]. See discussion in the full version of the paper.

3

characterize loss functions that only require polylog(N ) sample complexity to estimate. We do not
introduce a general algorithm for distribution learning — as any such algorithm would require Ω(N )
samples. Rather  motivated by tailored algorithms used in complex domains such as natural language
processing  our work characterizes loss functions that could be used by a variety of algorithms.
Outside distribution learning  loss functions (termed scoring rules) have been studied for decades in
the information elicitation literature  which seeks to incentivize experts  such as weather forecasters 
to give accurate predictions [e.g. 4  14  22  12  13]. The notion of loss function properness  for
example  comes from this literature. Recent research has made some connections between information
elicitation and loss functions in machine learning; however  it has focused mostly on the classiﬁcation
and regression and not distribution learning [1  12  20  21  9]. Our work can be viewed as a
contribution to the literature on evaluating forecasters by showing that  if the forecaster is constrained
to be calibrated  then a variety of simple local loss functions become (strongly  sample) proper.

denoted by p(B):=(cid:80)

2 Preliminaries
We work with distributions over a ﬁnite domain X with |X| = N. The set of all distributions over X
is denoted by ∆X . We denote a distribution p ∈ {0  1}N over X by a vector of probabilities  where
px is the probability p places on x ∈ X . For any set B ⊆ X   the total probability p places on B is
x∈B px. We use X to denote a random variable on X whose distribution is
speciﬁed in context. We also consider point mass distributions δx ∈ ∆X where δx
x(cid:48) = 1 [x = x(cid:48)].
Throughout this paper  we typically use p to denote the true (or target) distribution and q to denote a
candidate or predicted distribution. For any two distributions p and q  the total variation distance
2(cid:107)p − q(cid:107)1  where (cid:107) · (cid:107)1 denotes
between them is deﬁned by TV(p  q):= supB⊆X p(B) − q(B) = 1
the (cid:96)1 norm of a vector. Together  (cid:96)1 and the total variation distance are two of the most widely used
measures of distance between distributions.
To measure the quality of a candidate distribution q given samples from p  machine learning typically
turns to loss functions. A loss function is a function (cid:96) : ∆X × X → R where (cid:96)(q  x) is the loss
assigned to candidate q on outcome x. Given a target distribution p  the expected loss for candidate
q is deﬁned as (cid:96)(q; p):= EX∼p [(cid:96)(q  X)] . A loss function is called proper if (cid:96)(p; p) ≤ (cid:96)(q; p) for
all p (cid:54)= q  and strictly proper if the inequality is always strict4. Two common examples of proper
loss functions are the log loss function (cid:96)(q  x) = ln( 1
) (with the logarithm always taken base e in
qx
2(cid:107)δx − q(cid:107)2
this paper) and the quadratic loss (cid:96)(q  x) = 1
2. A loss function (cid:96) is called local if (cid:96)(q  x)
is a function of qx alone. For example  the log loss is local while the quadratic loss is not.
Our main results are characterized by the geometry of the loss functions we consider. For simplicity 
we will generally assume functions are differentiable  although our results can be extended.
Deﬁnition 1 (Strongly Concave). A function f : [0 ∞] → R is β-strongly concave if for all z  z(cid:48) in
the domain of f  f (z) ≤ f (z(cid:48)) + ∇f (z(cid:48)) · (z − z(cid:48)) − β
We also consider a relaxation of strong concavity that helps us in analyzing functions that have a
large curvature close to the origin but ﬂatten out as we move farther from it.
Deﬁnition 2 (Left-Strongly Concave). A function f : [0 ∞] → R is β(z)-left-strongly concave if
the function restricted to [0  z] is β(z)-strongly concave  for all z.

2 (z − z(cid:48))2.

As discussed  a natural assumption on the set of candidate distributions is calibration. Formally:
Deﬁnition 3 (Calibration). Given a distribution q ∈ ∆X   let Bt(q) = {x : qx = t}. When it is clear
from the context  we suppress q in the deﬁnition of Bt. We say that q is calibrated with respect to p 
if q(Bt(q)) = p(Bt(q)) for all t ∈ [0  1]. We let C(p) denote the set of all calibrated distributions
with respect to p.

In other words  q is calibrated with respect to p if points assigned probability qx = t have average
probability t under p.
In other words  p can be “coarsened” to q by taking subsets of points
and replacing their probabilities with the subset average. Note that the uniform distribution q =
N ) is calibrated with respect to all p  and that p is calibrated with respect to itself. Also
N   . . .   1
( 1
4Our use of “properness” is inspired the literature on proper scoring rules. It is not to be confused with
“properness” in learning theory where the learned hypothesis must belong to a pre-determined class of hypotheses.

4

note that there are only ﬁnitely many values t ∈ [0  1] for which Bt is non-empty. We denote the set
of these values by T (q) = {t : Bt (cid:54)= ∅}.
We refer an interested reader to the full version of the paper for a more detailed discussion of the
notion of calibration and its connections to similar notions used in forecasting theory  e.g. [8  11]. See
the full version for a discussion of how our results can be extended to a natural notion of approximate
calibration.

3 Three Desirable Properties of Loss Functions

In this section  we deﬁne three criteria and discuss why any desirable loss function should demonstrate
them. We use examples of loss functions  such as the log loss (cid:96)log-loss(q  x) = ln( 1
) and the linear
qx
loss (cid:96)lin-loss(q  x) = −qx to help demonstrate the existence or lack of these criteria.

3.1 Strong Properness

Recall that a loss function is strictly proper if all incorrect candidate distributions yield a higher
expected loss value than the target distribution. Here  we expand this to strong properness where this
gap in expected loss grows with distance from the target distribution. We also extend both deﬁnitions
to hold over a speciﬁc domain of candidate distributions  rather than all distributions.
Deﬁnition 4 (Calibrated Properness). Let P : ∆X → 2∆X be a domain function  that is  P(p) ⊆
∆X is a restricted set of distributions. A loss function (cid:96) is proper over P if for all p ∈ ∆X  
p ∈ argminq∈P(p) (cid:96)(q; p). A loss function is said to be strictly proper over P if the argmin is
always unique. When P(p) = C(p)  i.e. is the set of calibrated distributions w.r.t. p  we call such a
loss function (strictly) calibrated proper.
Example 1. It is well-known that (cid:96)log-loss(q  x) = ln
is the unique local proper loss function
(up to scaling) over the unrestricted domain P(p) = ∆X [3]. Indeed  it is known that the difference
in expected log loss of a prediction q and the target distribution p is the KL-divergence  i.e.

(cid:16) 1

(cid:17)

qx

.

px ln

(cid:96)log-loss(q; p) − (cid:96)log-loss(p; p) = KL(p  q):=

(1)
Furthermore  the KL-divergence is strictly positive for p (cid:54)= q. This proves that the log loss is strictly
proper over ∆X   and as a result  is strictly calibrated proper as well.
On the other hand  (cid:96)lin-loss(q  x) = −qx is not proper over ∆X . This is due to that fact that the
minimizer of this loss is the point mass distribution δx for x = argmaxx px. For example  for target
3 )  distribution q = (0  1) yields a lower (cid:96)lin-loss than that of p. Note  however 
distribution p = ( 1
that such a choice of q is not calibrated with respect to p. When loss minimization is constrained
to the set of calibrated distributions  C(p) = {( 1
2 )}  p minimizes the expected linear loss.
Indeed  in Section 4 we show more generally that the linear loss and in fact many reasonable local
loss functions are calibrated proper.

3 )  ( 1

3   2

3   2

2   1

(cid:88)

x

(cid:18) px

(cid:19)

qx

While strict properness is an important baseline guarantee  we would like a “stronger” property: If
q is signiﬁcantly incorrect in the sense of being far from p  then the expected loss of q should be
signiﬁcantly worse. This motivates the following deﬁnition. An analogous deﬁnition has appeared in
the context of mechanism design in [10].
Deﬁnition 5 (Strong Calibrated Properness). A loss function (cid:96) is β-strongly proper over a domain
function P if for all p ∈ ∆X   for all q ∈ P(p)  (cid:96)(q; p) − (cid:96)(p; p) ≥ β
1 . When P(p) =
C(p)  we call such functions β-strongly calibrated proper and when P(p) = ∆X   we simply refer to
them as β-strongly proper.
Example 2. The log loss is 1-strongly proper. This is equivalent to Pinsker’s inequality  which
states that for all p and q  KL(p  q) ≥ 2TV(p  q)2. Together with (1) and the fact that TV(p  q) =
2 (cid:107)p − q(cid:107)1  this shows that log loss is 1-strongly proper (and thus also 1-strongly calibrated proper.)
As we will see in Section 4  strong calibrated properness relates to the notion of strong concavity (of
the inverse loss function) in (cid:96)1 norm. We refer the interested reader to the full version of the paper for
a discussion of the use of alternative norms in the deﬁnition of strong properness. In the full version

2 (cid:107)p − q(cid:107)2

1

5

we extend the study of normed concavity of loss functions to strong properness of a loss function
over ∆X .

3.2 Sample-properness
So far  we have focused on the loss a candidate q receives in expectation over x ∼ p. Of course 
if one is attempting to learn p  this expectation can generally not be computed. We would like the
notion of properness to carry over to the setting when the loss on q is estimated using a small set of
samples from p. We say that a loss function is sample-proper if within a small number  all candidate
distributions that are sufﬁciently far from p yield a loss that is larger than that of p on the samples.
In the remainder of this paper  let ˆp denote the empirical distribution corresponding to samples drawn
from p. Note that the average loss of any q on the samples can be written (cid:96)(q; ˆp). Formally:
Deﬁnition 6 (Calibrated Sample-Properness). A loss function (cid:96) is m(  δ  N )-sample proper over a
function domain P if  for all p ∈ ∆X and all q ∈ P(p) with (cid:107)p − q(cid:107)1 ≥   with probability at least
1 − δ over m(  δ  N ) i.i.d. samples from p  we have (cid:96)(p; ˆp) < (cid:96)(q; ˆp). When P(p) = C(p)  we
call such functions calibrated m(  δ  N )-sample proper.

Example 3. A folklore theorem states that (cid:96)log-loss is O(cid:0) 1
a result it is calibrated O(cid:0) 1

(cid:1)(cid:1)-sample proper.

(cid:1)(cid:1)-sample proper over ∆X   and as

2 ln(cid:0) 1

2 ln(cid:0) 1

Now consider (cid:96)lin-loss(q  x) = −qx. Since it is not a proper loss function over ∆X   by deﬁnition
it is not sample proper over ∆X for any m(  δ  N ). When restricting to calibrated distributions
however  as we claimed in Example 1 linear loss is calibrated proper in expectation. It is interesting
to note that linear loss is not sample proper for any m(  δ  N ) ∈ o (N ). To observe this  consider
2(N/2−2) for x = 3  . . .   N/2 and px = 0 for
p where p1 = 1
2(N−2) for x = 3  . . .   N. Let
x = N/2 + 1  . . .   N. Consider q where q1 = q2 = 1
ˆp be the empirical distribution. With a constant probability  ˆp1 ≤ 1
4. Let
ν =

m and ˆp2 ≥ 1

m  and px =

4 and qx =

m  p2 = 1

4 − 1√

4− 1√

4 + 1√

2(N/2−2) − 1

2(N−2) = Θ( 1

N ). Therefore 

1

1

1

δ

δ

when m ∈ o (N ). Furthermore  note that q is calibrated w.r.t. p with two non-empty buckets
(q) = {3  . . .   N}. Moreover  (cid:107)p − q(cid:107)1 = Θ(1). Thus  for (cid:96)lin-loss
B 1
to be calibrated m(  δ  N )-sample proper  we must have m(Θ(1)  Θ(1)  N ) ∈ Ω (N ).
4

(q) = {1  2} and B 1

2(N−2)

3.3 Concentration

Beyond sample properness  when the expected loss (cid:96)(q; p) is estimated from a small i.i.d. sample
from p  we would like the empirical loss to remain faithful to the true value. For example  one might
hope that minimizing loss on that sample will result in a distribution that has small loss on p. This
will hold as long as the empirical loss well approximates the true expected loss with high probability.
Deﬁnition 7 (Calibrated Concentration). A loss function (cid:96) concentrates over domain function P
with m(γ  δ  N ) samples if for all p ∈ ∆X   for all q ∈ P(p)  for m(γ  δ  N ) i.i.d. samples from p 
Pr [|(cid:96)(q; ˆp) − (cid:96)(q; p)| ≥ γ] ≤ δ. When P(p) = C(p)  we say that (cid:96) calibrated concentrates with
m(γ  δ  N ) samples.5

5We use γ to denote difference in loss to avoid confusion with   which generally means a distance between

distributions.

6

(cid:96)(q; ˆp) − (cid:96)(p; ˆp) =

ˆpx(px − qx)

N(cid:88)

x=1

ˆp1√
m

=

=

1√
m
= − 1
m

+ ν

+

m

−ˆp2√
(cid:18) 1
(cid:18) 1

4

+ Θ

− 1√
m

(cid:19)

N

N/2(cid:88)
(cid:19)

x=3

< 0 

N(cid:88)
(cid:18) 1

N

ˆpx

(cid:19)

x=N/2+1

ˆpx − ν

− 1√
m

1
4

+ Θ

Example 4. We can easily see that log loss does not concentrate with o(N ) samples over ∆X . Let p
be the uniform distribution and q be uniform on X \ {x}. With high probability  x is not sampled 
and (cid:96)(q; ˆp) is ﬁnite. Yet (cid:96)(q; p) = ∞. Note that although this example is extreme  its conclusion is
robust: one can make an arbitrarily large ﬁnite gap. As we will see  the log loss  along with many
other reasonable loss will concentrate with a small number of samples over calibrated distributions.

4 Main Results

Looking back at the criteria deﬁned in Section 3  we are immediately faced with an impossibility result:
no local loss function exists that satisﬁes properness  o(N )-sample properness  and concentration
with o(N ) samples. This is because log loss is the unique local loss function that satisﬁes the ﬁrst
property and as shown in Example 4 it does not concentrate. In this section  we show that a broad
class of local loss functions with certain niceness properties satisﬁes the above three criteria over
calibrated domains. Speciﬁcally  we consider loss functions (cid:96)(q  x) that are non-increasing in qx
and are inversely concave: (cid:96)(q  x) = f ( 1
) for some concave function f. Similarly  we say that (cid:96) is
qx
inversely strongly concave if the corresponding f is strongly concave.

4.1 Calibrated and Strong Calibrated Properness

In this section  we show that any (strongly) nice loss function is (strongly) proper over the domain of
calibrated distributions. More formally.
Theorem 1 (Strict Properness). Suppose the local loss function (cid:96) is such that (cid:96)(q  x) = f ( 1
concave f function. Then  (cid:96) is strictly proper over the domain function C.
qx
Theorem 2 (Strong Properness). Suppose the loss function (cid:96) is such that (cid:96)(q  x) = f ( 1
qx
is non-decreasing and is C(x)
for x ≥ 1. Then for all p ∈ ∆X and q ∈ C(p) 
(cid:96)(q; p) − (cid:96)(p; p) ≥ C

) where f
x2 -left-strongly concave where C(x) is non-increasing and non-negative

(cid:18) 4N

· (cid:107)p − q(cid:107)2

) for a

(cid:19)

1

.

(cid:107)p − q(cid:107)1

128

We defer the proof of Theorem 2 to the full version.
We begin with the proof of Theorem 1  which relies on a key property of calibration stated in Lemma
1. At a high level  this lemma shows that the average value of 1/px and 1/qx is the same over
instances x such that qx = t  which is also equal to 1/t.
Lemma 1. For any distribution p ∈ ∆X and q ∈ C(p)  and for any t ∈ [0  1]  we have
EX∼p

t   where Bt = {x : qx = t}.

(cid:12)(cid:12)(cid:12) X ∈ Bt

(cid:104) 1

= 1

(cid:105)

pX

Proof. We have

(cid:20) 1

pX

E

(cid:21)

(cid:12)(cid:12)(cid:12) X ∈ Bt

(cid:88)

x∈Bt

=

px

p(B)

1
px

=

|Bt|
p(Bt)

=

1
t

.

Proof of Theorem 1. Suppose (cid:96)(q  x) = f ( 1
) for a strictly concave f. Consider any q that is
qx
calibrated with respect to p. Recall that Bt = {x : qx = t} and T (q) = {t : |Bt| (cid:54)= ∅} is a ﬁnite set.

(cid:18) 1
(cid:19)

pX

=

(cid:20)
(cid:18) 1

f

t

p(Bt) E

p(Bt)f

(cid:88)
(cid:88)

t∈T (q)

t∈T (q)

(cid:96)(p; p) =

=

(cid:21)

(cid:19) (cid:12)(cid:12)(cid:12) X ∈ Bt
(cid:88)
(cid:88)

≤ (cid:88)
(cid:18) 1
(cid:19)

t∈T (q)

(cid:18)

E

(cid:20) 1

pX

(cid:21)(cid:19)

(cid:12)(cid:12)(cid:12) X ∈ Bt

p(Bt)f

t∈T (q)

x∈Bt

pxf

qx

= (cid:96)(q; p) 

where the second transition is by Jensen’s inequality and the third transition is by Lemma 1. If f is
strictly concave and there exists a Bt where q and p disagree  then the inequality is strict.

7

4.2 Concentration

√

(cid:17)

for nonnega-
z for all z ≥ 1 and some constant c.

qx

(cid:16) 1

The (strong) properness of a loss function  as discussed in Section 4.1  is only concerned with
loss functions in expectation. In this section  we consider ﬁnite sample guarantees. Recall that (cid:96)
concentrates over P(p) (Deﬁnition 7) if  with m(γ  δ  N ) samples  the empirical loss (cid:96)(q; ˆp) of a
distribution q ∈ P(p) is γ-close to its true loss (cid:96)(q; p) with probability 1 − δ. Concentration can be
difﬁcult to achieve: by Example 4  even the log loss does not concentrate for any sample size o(N )
for general q ∈ ∆X . However  as we show below  when q is calibrated  many natural loss functions 
including log loss  indeed concentrate. All that is needed is that the loss function is inverse concave 
increasing  and does not grow too quickly as qx → 0.
Theorem 3 (Concentration). Suppose (cid:96) is a local loss function with (cid:96)(q  x) = f
tive  increasing  concave f (z). Suppose further that f (z) ≤ c
Then (cid:96) concentrates over the domain function C for any m(γ  δ  N ) ≤ N  such that
m(γ  δ  N ) ≥ c1 · f (β)2 ln 1
δ·min(1 γ2/c2) . That is  for any p ∈ ∆X   q ∈ C(p)  drawing at

where c1 is a ﬁxed constant and β:=
least m(γ  δ  N ) samples guarantees |(cid:96)(q; ˆp) − (cid:96)(q; p)| ≤ γ with probability ≥ 1 − δ.
Note that γ bounds the absolute difference between (cid:96)(q; ˆp) and (cid:96)(q; p). The desired difference
may depend on the relative scale of the loss function. If e.g.  we take (cid:96)(q  x) and scale to obtain
(cid:96)(cid:48)(q  x) = α · (cid:96)(q  x) for some α  the desired error γ scales by α  f (β) and c both scale by α  and
thus we can see that the sample complexity remains ﬁxed.
We defer the proof of Theorem 3 to the full version of the paper. At a high level  Theorem 3 holds
because calibration helps us avoid worst-case instances (as in Example 4) using a very simple fact:
when q is calibrated  we have qx
N for all x. This rules out very low probability events that
px
contribute signiﬁcantly to (cid:96)(q; p) but require many samples to identify. To prove Theorem 3 we
partition X into Ω containing elements of very small probability  and X \ Ω. With high probability 
no element of Ω is ever sampled from p. Conditioned on this  the loss is bounded (and its expectation
does not change much)  so a concentration result can be applied.

≥ 1

16N 8

δ

 

γ2

4.3 Sample Properness

Lastly  we turn our attention to calibrated sample properness. Recall that a loss function is sample
proper if all candidate distributions that are sufﬁciently far from p have a loss that is larger p on
the empirical distribution ˆp corresponding to a small number of samples from p. It is not hard
to see that sample properness of a loss function is a direct consequence of its concentration and
strong properness. For any candidate distribution q for which (cid:107)q − p(cid:107)1 is large  strong properness
(Theorem 2) implies that (cid:96)(q; p) is signiﬁcantly larger than (cid:96)(p; p). Furthermore  concentration
(Theorem 3) implies that with high probability (cid:96)(q; p) ≈ (cid:96)(q; ˆp) and (cid:96)(p; p) ≈ (cid:96)(p; ˆp). Therefore 
with high probability  (cid:96)(q; ˆp) > (cid:96)(p; ˆp). Formally in the full version of the paper we prove:
Theorem 4 (Sample properness). Suppose (cid:96) is a local loss function with (cid:96)(q  x) = f ( 1
) for
√
qx
nonnegative  increasing  concave f (z). Suppose further that f (z) ≤ c
z for all z ≥ 1 and some
constant c and that f is C(x)
x2 -left-strongly concave for where C(x) is nonincreasing and nonnegative
for x ≥ 1. Then for all p ∈ ∆X and q ∈ C(p)  if ˆp is the empirical distribution constructed from m
independent samples of p with m ≤ N and

(cid:16)
(cid:16)

m ≥
(cid:32)

(cid:20)

(cid:16) 4N(cid:107)p−q(cid:107)1

C

288N 8
4N(cid:107)p−q(cid:107)1

(cid:17) (cid:107)p−q(cid:107)2

1

128c

c1 · f (β)2 ln 1

δ

(cid:17)(cid:107)p − q(cid:107)2(cid:17)2  
(cid:21)2(cid:33)   then (cid:96)(q; ˆp) > (cid:96)(p; ˆp) with prob. ≥ 1−δ.

where c1 is constant and β:=

δ·min

1 

C

4.4 Application of the Main Results to Loss Functions

We now instantiate Theorems 2  3  and 4 for one example of a natural loss function (cid:96)(q  x) =
). Refer to Table 1 for other loss functions and see the full version for details on its derivation.
ln ln( 1
qx

8

First  note that ln ln(z) is C(z)/z2-left-strongly concave for C(z) = (1+ln(z))
non-increasing and non-negative for z ≥ 1 and ln ln(z) ≤ √
such that (cid:107)p − q(cid:107)1 ≥  we have

.6 Moreover  C(z) is
z. Using these  for any p and q ∈ C(p)

ln(z)2

• By Theorem 2  (cid:96)(q; p) − (cid:96)(p; p) ≥ Ω(

• By Theorem 3  an empirical distribution ˆp of ˜O(cid:0)γ−2 ln ln(N )2 ln(1/δ)(cid:1) i.i.d samples from
• By Theorem 4  an empirical distribution ˆp of ˜O(cid:0)−4 ln ln(N ln(N ))2 ln(1/δ) ln(N )(cid:1) i.i.d

p is sufﬁcient such that |(cid:96)(q; ˆp) − (cid:96)(q; p)| ≤ γ with probability 1 − δ.

samples from p is sufﬁcient such that (cid:96)(q; ˆp) > (cid:96)(p; ˆp) with probability 1 − δ.

2

ln(N/) ).

5 Discussion

In this work  we characterized loss functions that meet three desirable properties: properness in
expectation  concentration  and sample properness. We demonstrated that no local loss function
meets all of these properties over the domain of all candidate distributions. But  if one enforces the
criterion of calibration (or approximate calibration as discussed in the full version)  then many simple
loss functions have good properties for evaluating learned distributions over large discrete domains.
We hope that our work provides a starting point for several future research directions.
One natural question is to understand how to select a loss function based on the application domain.
Our example for language modeling  from the introduction  motivates the idea that log loss is not
the best choice always. Understanding this more formally  for example in the framework of robust
distribution learning  could provide a systematic approach for selecting loss functions based on the
needs of the domain. Our work also leaves open the question of designing compuationally and
statistically efﬁcient learning algorithms for different loss functions under the constraint that the
candidate q is (approximately) calibrated. One challenge in designing computationally efﬁcient
algorithms is that the space of calibrated distributions is not convex. We present some advances
towards dealing with this challenge in the full version by providing an efﬁcient procedure for
‘projecting’ a non-calibrated distribution on the space of approximately calibrated distribution. It
remains to be seen if iteratively applying this procedure could be useful in designing an efﬁcient
algorithm for minimizing the loss on calibrated distributions.

Acknowledgements

We thank Adam Kalai for signiﬁcant involvement in early stages of this project and for suggesting
the idea of exploring alternatives to the log loss under calibration restrictions. We also thank Gautam
Kamath for helpful discussions.

References
[1] Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property
elicitation. In Proceedings of the 28th Conference on Computational Learning Theory (COLT) 
pages 4–22  2015.

[2] Tu˘gkan Batu  Lance Fortnow  Ronitt Rubinfeld  Warren D. Smith    and Patrick White. Testing
that distributions are close. In Proceedings of the 41st Symposium on Foundations of Computer
Science (FOCS)  pages 259–269  2000.

[3] José M Bernardo. Expected information as expected utility. Annals of Statistics  pages 686–690 

1979.

[4] Glenn W. Brier. Veriﬁcation of forecasts expressed in terms of probability. Monthly Weather

Review  78(1):1–3  1950.

[5] Clément L Canonne. A survey on distribution testing: Your data is big. but is it blue? In

Electronic Colloquium on Computational Complexity (ECCC)  volume 22  pages 1–1  2015.

6In the full version  we show that function f is b(z)-left-strongly concave if for all z  f(cid:48)(cid:48)(z) ≤ −b(z).

9

[6] Siu-On Chan  Ilias Diakonikolas  Rocco A Servedio  and Xiaorui Sun. Learning mixtures of
structured distributions over discrete domains. In Proceedings of the 24th Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA)  pages 1380–1394  2013.

[7] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for
proper learning mixtures of gaussians. In Proceedings of the 27th Conference on Computational
Learning Theory (COLT)  pages 1183–1213  2014.

[8] A. Philip Dawid. The well-calibrated Bayesian. Journal of the American Statistical Association 

77(379):605–610  1982.

[9] Werner Ehm  Tilmann Gneiting  Alexander Jordan  and Fabian Krüger. Of quantiles and
expectiles: consistent scoring functions  Choquet representations and forecast rankings. Journal
of the Royal Statistical Society: Series B (Statistical Methodology)  78(3):505–562  2016.

[10] Amos Fiat  Anna Karlin  Elias Koutsoupias  and Angelina Vidali. Approaching utopia: strong
truthfulness and externality-resistant mechanisms. In Proceedings of the 4th Innovations in
Theoretical Computer Science Conference  ITCS ’12  pages 221–230  2013.

[11] Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. Biometrika  85(2):379–390  1998.

[12] Rafael Frongillo and Ian Kash. Vector valued property elicitation. In Proceedings of the 28th

Algorithmic Learning Theory (ALT)  pages 710–727  2015.

[13] Tilman Gneiting and Adrian E. Raftery. Strictly proper scoring rules  prediction  and estimation.

Journal of the American Statistical Association  102(477):359–378  2007.

[14] Irving J. Good. Rational decisions. Journal of the Royal Statistical Society  14(1):107–114 

1952.

[15] Alan Julian Izenman. Recent developments in nonparametric density estimation. Journal of the

American Statistical Association  86(413):205–224  1991.

[16] Adam Kalai  Ankur Moitra  and Gregory Valiant. Disentangling Gaussians. Communications

of the ACM  55(2):113–120  February 2012.

[17] Michael Kearns  Yishay Mansour  Dana Ron  Ronitt Rubinfeld  Robert E Schapire  and Linda
Sellie. On the learnability of discrete distributions. In Proceedings of the 26th Annual ACM
Symposium on Theory of Computing (STOC)  pages 273–282  1994.

[18] Christopher D Manning  Christopher D Manning  and Hinrich Schütze. Foundations of statistical

natural language processing. MIT press  1999.

[19] John McCarthy. Measures of the value of information. Proceedings of the National Academy of

Sciences  42(9):654–655  1956.

[20] Harikrishna Narasimhan  Harish G. Ramaswamy  Aadirupa Saha  and Shivani Agarwal. Con-
sistent multiclass algorithms for complex performance measures. In Proceedings of the 32nd
International Conference on Machine Learning (ICML)  pages 2398–2407  2015.

[21] Harish G. Ramaswamy and Shivani Agarwal. Convex calibration dimension for multiclass loss

matrices. Journal of Machine Learning Research  17:1–45  2016.

[22] Leonard J. Savage. Elicitation of personal probabilities and expectations. Journal of the

American Statistical Association  66(336):783–801  1971.

[23] Bernard W Silverman. Density estimation for statistics and data analysis. Monographs on

Statistics and Applied Probability  1986.

[24] Gregory Valiant and Paul Valiant.

In
Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC)  pages
142–155  2016.

Instance optimal learning of discrete distributions.

10

,Nika Haghtalab
Cameron Musco
Bo Waggoner