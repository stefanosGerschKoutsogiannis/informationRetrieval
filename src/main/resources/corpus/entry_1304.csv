2018,Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters,We study the decentralized distributed computation of discrete approximations for the regularized Wasserstein barycenter of a finite set of continuous probability measures distributedly stored over a network. We assume there is a network of agents/machines/computers  and each agent holds a private continuous probability measure and seeks to compute the barycenter of all the measures in the network by getting samples from its local measure and exchanging information with its neighbors. Motivated by this problem  we develop  and analyze  a novel accelerated primal-dual stochastic gradient method for general stochastic convex optimization problems with linear equality constraints. Then  we apply this method to the decen- tralized distributed optimization setting to obtain a new algorithm for the distributed semi-discrete regularized Wasserstein barycenter problem. Moreover  we show explicit non-asymptotic complexity for the proposed algorithm. Finally  we show the effectiveness of our method on the distributed computation of the regularized Wasserstein barycenter of univariate Gaussian and von Mises distributions  as well as some applications to image aggregation.,Decentralize and Randomize: Faster Algorithm for

Wasserstein Barycenters

Pavel Dvurechensky  Darina Dvinskikh

Weierstrass Institute for Applied Analysis and Stochastics 

Institute for Information Transmission Problems RAS

{pavel.dvurechensky darina.dvinskikh}@wias-berlin.de

Alexander Gasnikov

Moscow Institute of Physics and Technology 

Institute for Information Transmission Problems RAS

gasnikov@yandex.ru

César A. Uribe

Massachusetts Institute of Technology

cauribe@mit.edu

Angelia Nedi´c

Arizona State University 

Moscow Institute of Physics and Technology

angelia.nedich@asu.edu

Abstract

We study the decentralized distributed computation of discrete approximations
for the regularized Wasserstein barycenter of a ﬁnite set of continuous probability
measures distributedly stored over a network. We assume there is a network of
agents/machines/computers  and each agent holds a private continuous probability
measure and seeks to compute the barycenter of all the measures in the network
by getting samples from its local measure and exchanging information with its
neighbors. Motivated by this problem  we develop  and analyze  a novel accelerated
primal-dual stochastic gradient method for general stochastic convex optimization
problems with linear equality constraints. Then  we apply this method to the decen-
tralized distributed optimization setting to obtain a new algorithm for the distributed
semi-discrete regularized Wasserstein barycenter problem. Moreover  we show
explicit non-asymptotic complexity for the proposed algorithm. Finally  we show
the effectiveness of our method on the distributed computation of the regularized
Wasserstein barycenter of univariate Gaussian and von Mises distributions  as well
as some applications to image aggregation.1

1

Introduction

Optimal transport (OT) [30  25] has become increasingly popular in the machine learning and
optimization community. Given a basis space (e.g.  pixel grid) and a transportation cost function (e.g. 
squared Euclidean distance)  the OT approach deﬁnes a distance between two objects (e.g.  images) 
modeled as two probability measures on the basis space  as the minimal cost of transportation of the
ﬁrst measure to the second. Besides images  these probability measures or histograms can model other
real-world objects like videos  texts  etc. The optimal transport distance leads to outstanding results
in unsupervised learning [4  7]  semi-supervised learning [42]  clustering [24]  text classiﬁcation [27] 
as well as in image retrieval  clustering and classiﬁcation [38  11  39]  statistics [20  36]  economics

1The full version of this paper can be found in the supplementary material and is also available as [15].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

and ﬁnance [5]  condensed matter physics [8]  and other applications [26]. From the computational
point of view  the optimal transport distance (or Wasserstein distance) between two histograms of
size n requires solving a linear program  which typically requires O(n3 log n) arithmetic operations.
An alternative approach is based on entropic regularization of this linear program and application of
either Sinkhorn’s algorithm [11] or stochastic gradient descent [22]  both requiring O(n2) arithmetic
operations  which can be too costly in the large-scale context.
Given a set of objects  the optimal transport distance naturally deﬁnes their mean representative. For
example  the 2-Wasserstein barycenter [2] is an object minimizing the sum of squared 2-Wasserstein
distances to all objects in a set. Wasserstein barycenters capture the geometric structure of objects 
such as images  better than the barycenter with respect to the Euclidean or other distances [12].
If the objects in the set are randomly sampled from some distribution  theoretical results such
as central limit theorem [14] or conﬁdence set construction [20] have been proposed  providing
the basis for the practical use of Wasserstein barycenter. However  calculating the Wasserstein
barycenter of m measures includes repeated computation of m Wasserstein distances. The entropic
regularization approach was extended for this case in [6]  with the proposed algorithm having a
O(mn2) complexity  which can be very large if m and n are large. Moreover  in the large-scale
setup  storage and processing of transportation plans  required to calculate Wasserstein distances 
can be intractable for local computation. On the other hand  recent studies [34  40  37  46  31] on
accelerated distributed convex optimization algorithms demonstrated their efﬁciency for convex
optimization problems over arbitrary networks with inherently distributed data  i.e.  the data is
produced by a distributed network of sensors [35  33  32] or the transmission of information is limited
by communication or privacy constraints  i.e.  only limited amount of information can be shared
across the network.
Motivated by the limited communication issue and the computational complexity of the Wasserstein
barycenter problem for large amounts of data stored in a network of computers  we use the entropy
regularization of the Wasserstein distance and propose a decentralized algorithm to calculate an
approximation to the Wasserstein barycenter of a set of probability measures. We solve the problem
in a distributed manner on a connected and undirected network of agents oblivious to the network
topology. Each agent locally holds a possibly continuous probability distribution  can sample from
it  and seeks to cooperatively compute the barycenter of all probability measures exchanging the
information with its neighbors. We consider the semi-discrete case  which means that we ﬁx the
discrete support for the barycenter and calculate a discrete approximation for the barycenter.
Related work. Unlike [44]  we propose a decentralized distributed algorithm for the computation of
the regularized Wasserstein barycenter of a set of continuous measures. Working with continuous
distributions requires the application of stochastic procedures like stochastic gradient method as in
[22]  where it is applied for regularized Wasserstein distance  but not for Wasserstein barycenter. This
idea was extended to the case of non-regularized barycenter in [43  10]  where parallel algorithms
were developed. The critical difference between the parallel and the decentralized setting is that  in
the former  the topology of the computational network is ﬁxed to be a star topology and it is known
in advance by all the machines  forming a master/slave architecture. We seek to further scale up
the barycenter computation to a huge number of input measures using arbitrary network topologies.
Moreover  unlike [43]  we use entropic regularization to take advantage of the problem smoothness
and obtain faster rates of convergence for the optimization procedure. Unlike [10]  we ﬁx the support
of the barycenter  which leads to a convex optimization problem and allows us to prove complexity
bounds for our algorithm.
The well-developed approach based on
Sinkhorn’s algorithm [11  6  13] naturally
leads to parallel algorithms. Nevertheless  its
application to continuous distributions requires
discretization of these distributions  leading to
computational intractability when one desires
good accuracy and  hence  has to use ﬁne
discretization with large n  which leads to the
necessity of solving an optimization problem of large dimension. Thus  this approach is not directly
applicable in our setting of continuous distributions  and it is not clear whether it is applicable in the
decentralized distributed setting with arbitrary networks.

PAPER
[11  6  13]
[22]
[43  10]
OUR ALG. 2

Table 1: Summary of literature.

√
×
√
√

DECENTR.

CONT.

BARYC.

×
×
×
√

×
√
√
√

2

Recently  an alternative accelerated-gradient-based approach was shown to give better results than
the Sinkhorn’s algorithm for Wasserstein distance [18  19]. Moreover  accelerated gradient methods
have natural extensions for the decentralized distributed setting [40  45  28]. Nevertheless  existing
distributed optimization algorithms can not be applied to the barycenter problem in our setting of
continuous distributions as these algorithms are either designed for deterministic problems or for
stochastic primal problem  whereas in our case the dual problem is a stochastic problem. Table
1 summarizes the existing literature on Wasserstein barycenter calculation and shows our contribution.

Contributions. We propose a novel algorithm for general stochastic optimization problems with
linear constraints  namely the Accelerated Primal-Dual Stochastic Gradient Method (APDSGD).
Based on this algorithm  we introduce a distributed algorithm for the computation of a discrete
approximation for regularized Wasserstein barycenter of a set of continuous distributions stored
distributedly over a network (connected and undirected) with unknown arbitrary topology. For
our algorithm  we provide iteration and arithmetic operations complexity in terms of the problem
parameters. Finally  we demonstrate the effectiveness of our algorithm on the distributed computation
of the regularized Wasserstein barycenter of a set of von Mises distributions for various network
topologies and network sizes. Moreover  we show some initial results on the problem of image
aggregation for two datasets  namely  a subset of the MNIST digit dataset [29] and subset of the IXI
Magnetic Resonance dataset [1].
Paper organization. In Section 2 we present the regularized Wasserstein barycenter problem for
the semi-discrete case and its distributed computation over networks. In Section 3 we introduce a
new algorithm for general stochastic optimization problems with linear constraints and analyze its
convergence rate. Section 4 extends this algorithm and introduces our method for the distributed
computation of regularized Wasserstein barycenter. Section 5 shows the experimental results for the
proposed algorithm. The supplementary material contains the full version of the paper  including an
appendix with the proofs  as well as additional results of numerical experiments.
+(X ) the set of positive Radon probability measures on a metric space
Notation. We deﬁne M1
l=1 al = 1} the probability simplex. We denote by δ(x) the Dirac
X   and S1(n) = {a ∈ Rn
measure at point x  and ⊗ the Kronecker product. We refer to λmax(W ) as the maximum eigenvalue
m]T ∈ Rmn 
1  ···   pT
of a symmetric matrix W . We use bold symbols for stacked vectors p = [pT
where p1  ...  pm ∈ Rn. In this case [p]i = pi – the i-th block of p. For a vector λ ∈ Rn  we denote by
l=1([p]l)2 as 2-norm.

[λ]l its l-th component. We refer to the Euclidean norm of a vector (cid:107)p(cid:107)2 :=(cid:80)n

+ |(cid:80)n

2 The Distributed Wasserstein Barycenter Problem

In this section  we present the problem of decentralized distributed computation of regularized
Wasserstein barycenters for a family of possibly continuous probability measures distributed over
a network. First  we provide the necessary background for regularized Wasserstein distance and
barycenter. Then  we give the details of the distributed formulation of the optimization problem
deﬁning Wasserstein barycenter  which is a minimization problem with linear equality constraint. To
deal with this constraint  we make a transition to the dual problem  which  as we show  due to the
presence of continuous distributions  is a smooth stochastic optimization problem.
Regularized semi-discrete formulation of optimal transport problem. We consider entropic
regularization for the optimal transport problem and the corresponding regularized Wasserstein
distance and barycenter [11]. Let µ ∈ M1
+(Y) with density q(y)  and a discrete probability measure
i=1[p]iδ(zi) with weights given by vector p ∈ S1(n) and ﬁnite support given by points
z1  . . .   zn ∈ Z from a metric space Z. The regularized Wasserstein distance in semi-discrete setting
between continuous measure µ and discrete measure ν is deﬁned as2

ν = (cid:80)n

Wγ(µ  ν) = min
π∈Π(µ ν)

(cid:40) n(cid:88)

(cid:90)

Y

i=1

(cid:90)

n(cid:88)

Y

i=1

(cid:41)

(cid:18) πi(y)

(cid:19)

ξ

ci(y)πi(y)dy + γ

πi(y) log

dy

 

(1)

2Formally  the ρ-Wasserstein distance for ρ ≥ 1 is (W0(µ  ν))

ρ if Y = Z and ci(y) = dρ(zi  y)  d being a
distance on Y. For simplicity  we refer to (1) as regularized Wasserstein distance in a general situation since our
algorithm does not rely on any speciﬁc choice of cost ci(y).

1

3

(cid:40)

(cid:41)

n(cid:88)

(cid:90)

Y

where ci(y) = c(zi  y) is a cost function for transportation of a unit of mass from point zi ∈ Z to
point y ∈ Y  ξ is the uniform distribution on Y × Z  and the set of admissible coupling measures π
is deﬁned as

Π(µ  ν) =

π ∈ M1

+(Y) × S1(n) :

πi(y) = q(y)  y ∈ Y 

πi(y)dy = pi ∀ i = 1  . . .   n

.

i=1

i=1

i=1

(2)

min

min

p∈S1(n)

p1=···=pm

m(cid:88)

p1 ... pm∈S1(n)

Wγ µi(p) =

regularized Wasserstein barycenter ν and wish to ﬁnd it in the form ν = (cid:80)n

For a set of measures µi ∈ M1
+(Z)  i = 1  . . .   m  we ﬁx the support z1  . . .   zn ∈ Z of their
i=1[p]iδ(zi)  where
p ∈ Sn(1). Then the regularized Wasserstein barycenter in the semi-discrete setting is deﬁned as the
m(cid:88)
solution to the following convex optimization problem3

Wγ µi(pi) 
where we used notation Wγ µ(p) := Wγ(µ  ν) for ﬁxed probability measure µ.
Network constraints in the distributed barycenter problem. We now describe the distributed
optimization setting for solving the second problem in (2). We assume that each measure µi is held
by an agent i on a network and this agent can sample from this measure. We model such a network
as a ﬁxed connected undirected graph G = (V  E)  where V is the set of m nodes  and E is the set
of edges. We assume that the graph G does not have self-loops. The network structure imposes
information constraints; speciﬁcally  each node i has access to µi only and can exchange information
only with its immediate neighbors  i.e.  nodes j s.t. (i  j) ∈ E.
We represent the communication constraints imposed by the network by introducing a single equality
constraint instead of p1 = ··· = pm in (2). To do so  we deﬁne the Laplacian matrix ¯W∈ Rm×m
of the graph G such that a) [ ¯W ]ij = −1 if (i  j) ∈ E  b) [ ¯W ]ij = deg(i) if i = j  c) [ ¯W ]ij = 0
otherwise. Here deg(i) is the degree of the node i  i.e.  the number of neighbors of the node. Finally 
deﬁne the communication matrix (also referred to as an interaction matrix) by W := ¯W ⊗ In.
Assuming that G is undirected and connected  the Laplacian matrix ¯W is symmetric and positive
semideﬁnite. Furthermore  the vector 1 is the unique (up to a scaling factor) eigenvector associated
with the zero eigenvalue. W inherits the properties of ¯W   i.e.  it is symmetric and positive semideﬁnite.
W p = 0 if and only if p1 = ··· = pm  where we deﬁned stacked column vector
Moreover 
m]T ∈ Rmn. Using this fact  we equivalently rewrite problem (2) as the maximization
p = [pT
problem with linear equality constraint

1  ···   pT

√

− m(cid:88)

i=1

max
p1 ... pm∈S1(n) 

√

W p=0

Wγ µi(pi).

(3)

√

max

(cid:41)

1  ···   λT

√
(cid:104)λi  [

(cid:40) m(cid:88)

p1 ... pm∈S1(n)
√

Dual formulation of the barycenter problem. Given that problem (3) is an optimization problem
m]T ∈ Rmn
with linear constraints  we introduce a stacked vector of dual variables λ = [λT
for the constraints

W p = 0 in (3). Then  the Lagrangian dual problem for (3) is
√
W∗
min
γ µi([
λ∈Rmn
√
√
where [
W λ]i denote the i-th n-dimensional block of vectors
W λ re-
γ µi(·) is the Fenchel-
W λ]i  pi(cid:105) was used  and W∗
spectively  the equality
Legendre transform of Wγ µi (pi). The following Lemma states that each W∗
γ µi(·) is a smooth
function with Lipschitz-continuous gradient and can be expressed as an expectation of a function of
additional random argument.
+(Y) with density q(·)  the Fenchel-Legendre conjugate for Wγ µ(p) is
Lemma 1. Given µ ∈ M1

W p]i(cid:105) − Wγ µi(pi)
m(cid:80)

W λ]i) 
√

W p]i(cid:105) =

W p]i and [

= min
λ∈Rmn

m(cid:88)

W p and

m(cid:80)

(cid:104)λi  [

√

√

(cid:104)[

(4)

i=1

i=1

i=1

i=1

W∗

γ µ(¯λ) = EY ∼µγ log

1

q(Y )

exp

(cid:32)

(cid:18) [¯λ]l − cl(Y )

(cid:19)(cid:33)

 

γ

n(cid:88)

l=1

and its gradient is 1/γ-Lipschitz-continuous w.r.t. 2-norm.

3For simplicity  we assume equal weights for each Wγ µi (p) and do not normalize the sum dividing by m.

Our results can be directly generalized to the case of non-negative weights summing up to 1.

4

Denote ¯λ =
tive in the r.h.s. of (4). Then  by the chain rule  the l-th n-dimensional block of ∇W∗

m]T = [¯λT

1   . . .   ¯λT

1   . . .   [

W λ]T

W λ]T

γ (λ) – the dual objec-

γ (λ) is

m]T and W∗

√

√

√
W λ = [[

(cid:2)∇W∗
γ (λ)(cid:3)

=

l

(cid:34)
∇ m(cid:88)

(cid:35)

m(cid:88)

√

√
γ µi([

W∗

W λ]i)

=

W lj∇W∗

γ µj (¯λj)  l = 1  ...  m.

(5)

i=1

l

j=1

It follows from (5) and Lemma 1 that the dual problem (4) is a smooth stochastic convex optimization
problem. This is in contrast to [28]  where the primal problem is a stochastic optimization problem.
Moreover  as opposed to the existing literature on stochastic convex optimization  we not only
need to solve the dual problem but also need to reconstruct an approximate solution for the primal
problem (3)  which is the barycenter. In the next section  we develop a novel accelerated primal-dual
stochastic gradient method for a general smooth stochastic optimization problem  which is dual to
some optimization problem with linear equality constraints. Furthermore  in Section 4  we apply our
general algorithm to the particular case of primal-dual pair of problems (3) and (4).

3 General Primal-Dual Framework for Stochastic Optimization

(P )

min
x∈Q⊆E

{f (x) : Ax = b}  

In this section  we consider a general smooth stochastic convex optimization problem which is dual
to some optimization problem with linear equality constraints. Extending our works [16  21  9  17 
19  3  18]  we develop a novel algorithm for its solution and reconstruction of the primal variable
together with convergence rate analysis. Unlike prior works  we consider the stochastic primal-dual
pair of problems and one of our contributions consists in providing a primal-dual extension of the
accelerated stochastic gradient method. We believe that our algorithm can be used for problems other
than regularized Wasserstein barycenter problem and  thus  we  ﬁrst  provide a general algorithm and 
then  apply it to the barycenter problem. We introduce new notation since this section is independent
of the others and is focused on a general optimization problem.
General setup. For any ﬁnite-dimensional real vector space E  we denote by E∗ its dual  by (cid:107) · (cid:107)
a norm on E and by (cid:107) · (cid:107)∗ the norm on E∗ which is dual to (cid:107) · (cid:107)  i.e. (cid:107)λ(cid:107)∗ = max(cid:107)x(cid:107)≤1(cid:104)λ  x(cid:105).
For a linear operator A : E1 → E2  the adjoint operator AT : E∗
1 in deﬁned by (cid:104)u  Ax(cid:105) =
(cid:104)AT u  x(cid:105) 
x ∈ E1. We say that a function f : E → R has a L-Lipschitz-continuous
gradient w.r.t. norm (cid:107) · (cid:107)∗ if it is continuously differentiable and its gradient satisﬁes Lipschitz
condition (cid:107)∇f (x) − ∇f (y)(cid:107)∗ ≤ L(cid:107)x − y(cid:107) 
Our main goal in this section  is to provide an algorithm for a primal-dual (up to a sign) pair of
problems

∀x  y ∈ E.

∀u ∈ E∗
2  

2 → E∗

(cid:26)

(cid:0)−f (x) − (cid:104)AT λ  x(cid:105)(cid:1)(cid:27)
(cid:0)−f (x) − (cid:104)AT λ  x(cid:105)(cid:1) = (cid:104)λ  b(cid:105) + f∗(−AT λ) and as-

(cid:104)λ  b(cid:105) + max
x∈Q

.

(D) min
λ∈Λ

where Q is a simple closed convex set  A : E → H is given linear operator  b ∈ H is given 
Λ = H∗. We deﬁne ϕ(λ) := (cid:104)λ  b(cid:105) + maxx∈Q
sume it to be smooth with L-Lipschitz-continuous gradient. Here f∗ is the Fenchel-Legendre
dual for f. We also assume that f∗(−AT λ) = EξF ∗(−AT λ  ξ)  where ξ is random vector
and F ∗ is the Fenchel-Legendre conjugate function to some function F (x  ξ)  i.e.
it satisﬁes
F ∗(−AT λ  ξ) = max
{(cid:104)−AT λ  x(cid:105) − F (x  ξ)}. F ∗(¯λ  ξ) is assumed to be smooth and  hence
x∈Q
∇¯λF ∗(¯λ  ξ) = x(¯λ  ξ)  where x(¯λ  ξ) is the solution of the maximization problem x(¯λ  ξ) =
{(cid:104)¯λ  x(cid:105) − F (x  ξ)}. Under these assumptions  the dual problem (D) can be accessed by a
arg max
x∈Q
stochastic oracle (Φ(λ  ξ) ∇Φ(λ  ξ)) satisfying EξΦ(λ  ξ) = ϕ(λ)  Eξ∇Φ(λ  ξ) = ∇ϕ(λ)  which
we use in our algorithm.
Accelerated primal-dual stochastic gradient method. Next  we provide an accelerated algorithm
for the primal-dual pair of problems (P ) − (D). The idea is to apply accelerated stochastic gradient
method to the dual problem (D)  endow it with a step in the primal space and show that the new
algorithm allows also approximating the solution to the primal problem. We additionally assume
that the variance of the stochastic approximation ∇Φ(λ  ξ) for the gradient of ϕ can be controlled
and made as small as we desire. This can be done  for example by mini-batching the stochastic
approximation. Finally  since ∇Φ(λ  ξ) = b − A∇F ∗(−AT λ  ξ) = b − Ax(−AT λ  ξ)  on each
iteration  to ﬁnd ∇Φ(λ ξ) we ﬁnd the vector x(−AT λ  ξ) and use it for the primal iterates.

5

Theorem 1. Let ϕ have L-Lipschitz-continuous gradient w.r.t. 2-norm and (cid:107)λ∗(cid:107)2 ≤ R  where λ∗ is
a solution of dual problem (D). Given desired accuracy ε  assume that  at each iteration of Algorithm
1  the stochastic gradient ∇Φ(λk  ξk) is chosen in such a way that Eξ(cid:107)∇Φ(λk  ξk) − ∇ϕ(λk)(cid:107)2
2 ≤
. Then  for any ε > 0 and N ≥ 0  and expectation E w.r.t. all the randomness ξ1  . . .   ξN   the

εLαk
Ck
output ˆxN generated by the Algorithm 1 satisﬁes

f (EˆxN ) − f

∗ ≤ 16LR2

N 2 +

ε
2

and (cid:107)AEˆxN − b(cid:107)2 ≤ 16LR

N 2 +

ε
2R

.

(6)

In step 7 of Algorithm 1 we can use a batch of size M and 1
k+1) to update
r=1 x(λk+1  ξr
ˆxk+1. Then  under reasonable assumptions  ˆxN concentrates around EˆxN [23] and  if f is Lipschitz-
M
continuous  we obtain that (6) holds with large probability with ˆxN instead of EˆxN .

(cid:80)M

4 Solving the Barycenter Problem

Algorithm 1 Accelerated Primal-Dual Stochastic Gradient
Method (APDSGD)
Input: Number of iterations N.
1: C0 = α0 = 0  η0 = ζ0 = λ0 = ˆx0 = 0.
2: for k = 0  . . .   N − 1 do
3:

In this section  we apply the general al-
gorithm APDSGD to solve the primal-
dual pair of problems (3)-(4) and ap-
proximate the regularized Wasserstein
barycenter which is a solution to (3).
First  in Lemma 2  we make several
technical steps to take care of the as-
sumption of Theorem (1). Then  we
introduce a change of dual variable so
that the step 5 of Algorithm 1 becomes
feasible for decentralized distributed
setting. After that  we provide our al-
gorithm for regularized Wasserstein
barycenter problem with its complex-
ity analysis.
Lemma 2. The gradient of the objective function W∗
Lipschitz-continuous w.r.t. 2-norm. If its stochastic approximation is deﬁned as

4:
5:
6:
7:
8: end for
Output: The points ˆxN   ηN .

Find αk+1 > 0 from Ck+1 := Ck +αk+1 = 2Lα2
τk+1 = αk+1/Ck+1.
λk+1 = τk+1ζk + (1 − τk+1)ηk
ζk+1 = ζk − αk+1∇Φ(λk+1  ξk+1).
ηk+1 = τk+1ζk+1 + (1 − τk+1)ηk.
ˆxk+1 = τk+1x(λk+1  ξk+1) + (1 − τk+1)ˆxk.

γ (λ) in the dual problem (4) is λmax(W )/γ-

k+1.

γ µj (¯λj)  i = 1  ...  m  with

m(cid:88)

j=1

1
M

γ (λ)]i =

[(cid:101)∇W∗
(cid:101)∇W∗

√

W ij(cid:101)∇W∗
M(cid:88)
r ∼µj  j=1 ... m r=1 ... M(cid:101)∇W∗
γ (λ) − ∇W∗

r=1

Y j

γ µj (¯λj) =

pj(¯λj  Y j
r )  and [pj(¯λj  Y j
√
where M is the batch size  ¯λj := [
µj  j = 1  ...  m. Then E

r ∼µj  j=1 ... m r=1 ... M(cid:107)(cid:101)∇W∗

E
Y j

W λ]j  j = 1  ...  m  Y j

r )]l =

(cid:80)n
exp(([¯λj]l − cl(Y j
r ))/γ)
(cid:96)=1 exp(([¯λj](cid:96) − c(cid:96)(Y j
1   ...  Y j
γ (λ) = ∇W∗
γ (λ)(cid:107)2

2 ≤ λmax(W )m/M  λ ∈ Rmn.

r is a sample from the measure
γ (λ) and

.

(7)

r ))/γ)

(8)

j=1

√

(cid:80)m

√
γ µj ([

W ij(cid:101)∇W∗

αkε   the assumptions of Theorem 1 hold.

Based on this lemma  we see that if  on each iteration of Algorithm 1  the mini-batch size Mk satisﬁes
Mk ≥ mγCk
For the particular problem (4) the step 5 of Algorithm 1 can be written block-wise [ζk+1]i =
[ζk]i − αk+1
W λk+1]j)  i = 1  ...  m. Unfortunately  this update can not
be made in the decentralized setting since the sparsity pattern of
W ij can be different from Wij
√
and this will require some agents to get information not only from their neighbors. To overcome this
obstacle  we change the variables and denote ¯λ =
W ζ. Then the step 5
of Algorithm 1 becomes [¯ζk+1]i = [¯ζk]i − αk+1
Theorem 2. Under the assumptions of Section 2  Algorithm 2 after N =
iterations returns an approximation ˆpN for the barycenter  which satisﬁes

W η  ¯ζ =
γ µj ([¯λk+1]j)  i = 1  ...  m.

(cid:80)m
j=1 Wij(cid:101)∇W∗

16λmax(W )R2/(εγ)

W λ  ¯η =

(cid:112)

√

√

√

m(cid:88)

Wγ µi (E[ˆpN ]i) − m(cid:88)

i=1

i=1

√

Wγ µi([p
∗

]i) ≤ ε 

(cid:107)

6

W EˆpN(cid:107)2 ≤ ε/R.

(9)

The total complexity is O

(cid:18)

mn max

(cid:26)(cid:113) λmax(W )R2

εγ

  λmax(W )mR2

ε2

(cid:27)(cid:19)

arithmetic operations.

Algorithm 2 Distributed computation of Wasserstein
barycenter
Input: Each agent i ∈ V is assigned its measure µi.
1: All agents set [¯η0]i = [¯ζ0]i = [¯λ0]i = [ˆp0]i = 0 ∈ Rn 
C0 = α0 = 0 and N
2: For each agent i ∈ V :
3: for k = 0  . . .   N − 1 do
4:

We underline that even if the measures
µi  i = 1  ...  m are discrete with large
support size  it can be more efﬁcient
to apply our stochastic algorithm than
a deterministic algorithm. We now ex-
plain it in more details. If a measure µ
is discrete  then W∗
γ µ(¯λ) in Lemma
1 is represented as a ﬁnite expecta-
tion  i.e.  a sum of functions instead of
an integral  and can be found explic-
itly. In the same way  its gradient and 
hence  ∇W∗
γ (λ) in (5) can be found
explicitly in a deterministic way. Then
a deterministic accelerated decentral-
ized algorithm can be applied to ap-
proximate the regularized barycenter.
Let us assume for simplicity that the
support of measure µ is of the size n.
Then the calculation of the exact gradi-
ent of W∗
γ µ(¯λ) requires O(n2) arith-
metic operations and the overall com-
plexity of the deterministic algorithm
is O
. For
comparison  the complexity of our randomized approach in Theorem 2 is proportional to n  but not to
n2. So  our randomized approach is superior in the regime of large n.

Find αk+1 > 0 from Ck+1 := Ck +αk+1 = 2Lα2
τk+1 = αk+1/Ck+1.
Set Mk+1 = max{1  (cid:100)mγCk+1/(αk+1ε)(cid:101)}
[¯λk+1]i = τk+1[¯ζk]i + (1 − τk+1)[¯ηk]i
µi and set (cid:101)∇W∗
r }Mk+1
Generate Mk+1 samples {Y i
Share (cid:101)∇W∗
(cid:80)m
j=1 Wij(cid:101)∇W∗
[¯ζk+1]i = [¯ζk]i − αk+1
[¯ηk+1]i = τk+1[¯ζk+1]i + (1 − τk+1)[¯ηk+1]i
[ˆpk+1]i = τk+1pi([¯λk+1]i  Y i
where pi(· ·) is deﬁned in (7).4

γ µj ([¯λk+1]j)
1 ) + (1 − τk+1)[ˆpk+1]i 

γ µi([¯λk+1]i) with {j | (i  j) ∈ E}

12: end for
Output: ˆpN .

γ µi ([¯λk+1]i) as in (7).

mn2(cid:112)

λmax(W )R2/γε

from the measure

r=1

5:
6:
7:

8:
9:
10:
11:

(cid:16)

(cid:17)

k+1.

5 Experimental Results

γ (λ) and C(ˆp) := (cid:107)√

In this section  we present experimental results for Algorithm 2. Initially  we consider a set of agents
over a network  where each agent i can samples from a privately held random variable Yi ∼ N (θi  v2
i ) 
where N (θ  v2) is a univariate Gaussian distribution with mean θ and variance v2. Moreover  we
set θi ∈ [−4  4] and vi ∈ [0.1  0.6]. The objective is to compute a discrete distribution p ∈ S1(n)
that solves (2). We assume n = 100 and the support of p is a set of 100 equally spaced points on
the segment [−5  5]. Figure 1 shows the performance of Algorithm 2 for four classes of networks:
complete  cycle  star  and Erd˝os-Rényi. Moreover  we show the behavior for different network sizes 
namely: m = 10  100  200  500. Particularly we use two metrics: the function value of the dual
problem and the distance to consensus  i.e.  W∗
W ˆp(cid:107)2. As expected  when the
network is a complete graph  the convergence to the ﬁnal value and the distance to consensus decreases
rapidly. Nevertheless  the performance in graphs with degree regularity  such as the cycle graph
and the Erd˝os-Rényi random graph  is similar to a complete graph with much less communication
overhead. For the star graph  which has the worst case between the maximum and minimum number
of neighbors among all nodes  the algorithm performs poorly. Figure 2 shows the convergence of the
local barycenter of a set of von Mises distributions. Each agent over an Erd˝os-Rényi random graph
can access private realizations from a von Mises random variable. Particularly  for the cases of von
Mises distributions  we have used the angle between two points distance function. Figure 3 shows the
computed local barycenter of 9 agents in a network of 500 nodes at different iteration numbers. Each
agent holds a local copy of a sample of the digit 2 (56 × 56 image) from the MNIST dataset [29].
All agents converge to the same image that structurally represents the aggregation of the original
500 images held over the network. Finally  Figure 4 shows a simple example of an application of
Wasserstein barycenter on medical image aggregation where we have 4 agents connected over a cycle
graph and each agent holds a magnetic resonance image (256 × 256) from the IXI dataset [1].

4In the experiments  we use

1 )  which does not
change the statement of Theorem 2  but reduces the variance of ˆpN in practice. Moreover  under mild assumptions 
we can obtain high-probability analogue to inequalities (9).

r ) instead of pi([¯λk+1]i  Y i

r=1 pi([¯λk+1]i  Y i

Mk+1

1

(cid:80)Mk+1

7

Figure 1: Dual function value and distance to consensus for 200  100  10  500 agents  Mk = 100 and γ = 0.1.

Figure 2: Wasserstein barycenter of von Mises distributions for 10 agents at different iteration numbers.

Figure 3: Wasserstein barycenter of digit 2 from the MNIST dataset [29]. Each block shows a subset of 9
randomly selected local barycenters at different time instances.

Figure 4: Wasserstein barycenter for a subset of images from the IXI dataset [1]. Each block shows the local
barycenters of 4 agents at different time instances.

6 Conclusions

We propose a novel distributed algorithm for regularized Wasserstein barycenter problem for a set
of continuous measures stored distributedly over a network of agents. Our algorithm is based on
a new general algorithm for the solution of stochastic convex optimization problems with linear
constraints. In contrast to the recent literature  our algorithm can be executed over arbitrary connected
and static networks where nodes are oblivious to the network topology  which makes it suitable for
large-scale network optimization setting. Additionally  our analysis indicates that the randomization
strategy provides faster convergence rates than the deterministic procedure when the support size of
the barycenter is large. The implementation of our algorithm on real networks  requires further work 
as well as its extension to the decentralized distributed setting of Sinkhorn-type algorithms [6] for
regularized Wasserstein barycenter and other related algorithms  e.g.  Wasserstein propagation [41].

8

CycleErd˝os-RényiStarComplete2004006008001 000−2−101Iterationsm=200F(˜λk)2004006008001 000−2−101Iterationsm=1002004006008001 000−2−101Iterationsm=102004006008001 000−2−101Iterationsm=5002004006008001 00000.20.40.60.81Iterationsm=200C(ˆpk)2004006008001 00000.20.40.60.81Iterationsm=1002004006008001 00000.20.40.60.81Iterationsm=102004006008001 00000.20.40.60.81Iterationsm=500N=10π/2π3π/2N=1000π/2π3π/2N=2000π/2π3π/2N=5000π/2π3π/2N=1N=1000N=2000N=3000N=4000N=1N=100N=1000N=6000N=10000Acknowledgments

The work of A. Nedi´c and C.A. Uribe in Sect. 5 is supported by the National Science Foundation
under grant no. CPS 15-44953. The research by P. Dvurechensky  D. Dvinskikh  and A. Gasnikov in
Sect. 3 and Sect. 4 was funded by the Russian Science Foundation (project 18-71-10108).

References
[1] IXI Dataset. http://brain-development.org/ixi-dataset/. Accessed: 2018-05-17.

[2] M. Agueh and G. Carlier. Barycenters in the wasserstein space. SIAM Journal on Mathematical Analysis 

43(2):904–924  2011.

[3] A. S. Anikin  A. V. Gasnikov  P. E. Dvurechensky  A. I. Tyurin  and A. V. Chernov. Dual approaches to the
minimization of strongly convex functionals with a simple structure under afﬁne constraints. Computational
Mathematics and Mathematical Physics  57(8):1262–1276  Aug 2017.

[4] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein GAN. arXiv:1701.07875  2017.

[5] M. Beiglböck  P. Henry-Labordere  and F. Penkner. Model-independent bounds for option prices: a mass

transport approach. Finance and Stochastics  17(3):477–501  2013.

[6] J.-D. Benamou  G. Carlier  M. Cuturi  L. Nenna  and G. Peyré. Iterative bregman projections for regularized

transportation problems. SIAM Journal on Scientiﬁc Computing  37(2):A1111–A1138  2015.

[7] J. Bigot  R. Gouet  T. Klein  and A. López. Geodesic PCA in the wasserstein space by convex pca. Ann.

Inst. H. Poincaré Probab. Statist.  53(1):1–26  02 2017.

[8] G. Buttazzo  L. De Pascale  and P. Gori-Giorgi. Optimal-transport formulation of electronic density-

functional theory. Physical Review A  85(6):062502  2012.

[9] A. Chernov  P. Dvurechensky  and A. Gasnikov. Fast primal-dual gradient method for strongly convex
minimization problems with linear constraints. In Y. Kochetov  M. Khachay  V. Beresnev  E. Nurminski  and
P. Pardalos  editors  Discrete Optimization and Operations Research: 9th International Conference  DOOR
2016  Vladivostok  Russia  September 19-23  2016  Proceedings  pages 391–403. Springer International
Publishing  2016.

[10] S. Claici  E. Chien  and J. Solomon. Stochastic Wasserstein barycenters. In J. Dy and A. Krause  editors 
Proceedings of the 35th International Conference on Machine Learning  volume 80 of Proceedings of
Machine Learning Research  pages 999–1008. PMLR  2018.

[11] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges  L. Bottou 
M. Welling  Z. Ghahramani  and K. Q. Weinberger  editors  Advances in Neural Information Processing
Systems 26  pages 2292–2300. Curran Associates  Inc.  2013.

[12] M. Cuturi and A. Doucet. Fast computation of wasserstein barycenters. In International Conference on

Machine Learning  pages 685–693  2014.

[13] M. Cuturi and G. Peyré. A smoothed dual approach for variational wasserstein problems. SIAM Journal

on Imaging Sciences  9(1):320–343  2016.

[14] E. del Barrio  E. Gine  and C. Matran. Central limit theorems for the wasserstein distance between the

empirical and the true distributions. The Annals of Probability  27(2):1009–1071  1999.

[15] P. Dvurechensky  D. Dvinskikh  A. Gasnikov  C. A. Uribe  and A. Nedi´c. Decentralize and randomize:

Faster algorithm for Wasserstein barycenters. arXiv:1806.03915  2018.

[16] P. Dvurechensky and A. Gasnikov. Stochastic intermediate gradient method for convex problems with

stochastic inexact oracle. Journal of Optimization Theory and Applications  171(1):121–145  2016.

[17] P. Dvurechensky  A. Gasnikov  E. Gasnikova  S. Matsievsky  A. Rodomanov  and I. Usik. Primal-
dual method for searching equilibrium in hierarchical congestion population games. In Supplementary
Proceedings of the 9th International Conference on Discrete Optimization and Operations Research and
Scientiﬁc School (DOOR 2016) Vladivostok  Russia  September 19 - 23  2016  pages 584–595  2016.
arXiv:1606.08988.

9

[18] P. Dvurechensky  A. Gasnikov  and A. Kroshnin. Computational optimal transport: Complexity by
accelerated gradient descent is better than by Sinkhorn’s algorithm. In J. Dy and A. Krause  editors 
Proceedings of the 35th International Conference on Machine Learning  volume 80 of Proceedings of
Machine Learning Research  pages 1367–1376  2018. arXiv:1802.04367.

[19] P. Dvurechensky  A. Gasnikov  S. Omelchenko  and A. Tiurin. Adaptive similar triangles method: a stable

alternative to Sinkhorn’s algorithm for regularized optimal transport. arXiv:1706.07622  2017.

[20] J. Ebert  V. Spokoiny  and A. Suvorikova. Construction of non-asymptotic conﬁdence sets in 2-Wasserstein

space. arXiv:1703.03658  2017.

[21] A. V. Gasnikov  E. V. Gasnikova  Y. E. Nesterov  and A. V. Chernov. Efﬁcient numerical methods for
entropy-linear programming problems. Computational Mathematics and Mathematical Physics  56(4):514–
524  2016.

[22] A. Genevay  M. Cuturi  G. Peyré  and F. Bach. Stochastic optimization for large-scale optimal transport. In
D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information
Processing Systems 29  pages 3440–3448. Curran Associates  Inc.  2016.

[23] V. Guigues  A. Juditsky  and A. Nemirovski. Non-asymptotic conﬁdence bounds for the optimal value of a

stochastic program. Optimization Methods and Software  32(5):1033–1058  2017.

[24] N. Ho  X. Nguyen  M. Yurochkin  H. H. Bui  V. Huynh  and D. Phung. Multilevel clustering via Wasserstein
means. In D. Precup and Y. W. Teh  editors  Proceedings of the 34th International Conference on Machine
Learning  volume 70 of Proceedings of Machine Learning Research  pages 1501–1509  International
Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[25] L. Kantorovich. On the translocation of masses. (Doklady) Acad. Sci. URSS (N.S.)  37:199–201  1942.

[26] S. Kolouri  S. R. Park  M. Thorpe  D. Slepcev  and G. K. Rohde. Optimal mass transport: Signal processing

and machine-learning applications. IEEE Signal Processing Magazine  34(4):43–59  July 2017.

[27] M. J. Kusner  Y. Sun  N. I. Kolkin  and K. Q. Weinberger. From word embeddings to document distances.
In Proceedings of the 32nd International Conference on International Conference on Machine Learning -
Volume 37  ICML’15  pages 957–966. JMLR.org  2015.

[28] G. Lan  S. Lee  and Y. Zhou. Communication-efﬁcient algorithms for decentralized and stochastic

optimization. Mathematical Programming  pages 1–48  2018.

[29] Y. LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/  1998.

[30] G. Monge. Mémoire sur la théorie des déblais et des remblais. Histoire de l’Académie Royale des Sciences

de Paris  1781.

[31] A. Nedi´c  A. Olshevsky  W. Shi  and C. A. Uribe. Geometrically convergent distributed optimization with
uncoordinated step-sizes. In American Control Conference (ACC)  2017  pages 3950–3955. IEEE  2017.

[32] A. Nedi´c  A. Olshevsky  and C. A. Uribe. Distributed learning for cooperative inference. arXiv preprint

arXiv:1704.02718  2017.

[33] A. Nedi´c  A. Olshevsky  and C. A. Uribe. Fast convergence rates for distributed non-bayesian learning.

IEEE Transactions on Automatic Control  62(11):5538–5553  2017.

[34] A. Nedi´c  A. Olshevsky  and W. Shi. Achieving geometric convergence for distributed optimization over

time-varying graphs. SIAM Journal on Optimization  27(4):2597–2633  2017.

[35] R. Olfati-Saber  E. Franco  E. Frazzoli  and J. S. Shamma. Belief Consensus and Distributed Hypothesis

Testing in Sensor Networks  pages 169–182. Springer Berlin Heidelberg  Berlin  Heidelberg  2006.

[36] V. M. Panaretos and Y. Zemel. Amplitude and phase variation of point processes. Ann. Statist.  44(2):771–

812  04 2016.

[37] A. Rogozin  C. A. Uribe  A. Gasnikov  N. Malkovsky  and A. Nedi´c. Optimal distributed optimization on

slowly time-varying graphs. arXiv preprint arXiv:1805.06045  2018.

[38] Y. Rubner  C. Tomasi  and L. J. Guibas. The earth mover’s distance as a metric for image retrieval.

International journal of computer vision  40(2):99–121  2000.

10

[39] R. Sandler and M. Lindenbaum. Nonnegative matrix factorization with earth mover’s distance metric for
image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence  33(8):1590–1602  Aug
2011.

[40] K. Scaman  F. R. Bach  S. Bubeck  Y. T. Lee  and L. Massoulié. Optimal algorithms for smooth and
strongly convex distributed optimization in networks. In Proceedings of the 34th International Conference
on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages 3027–3036  2017.

[41] J. Solomon  F. De Goes  G. Peyré  M. Cuturi  A. Butscher  A. Nguyen  T. Du  and L. Guibas. Convolutional
wasserstein distances: Efﬁcient optimal transportation on geometric domains. ACM Transactions on
Graphics (TOG)  34(4):66  2015.

[42] J. Solomon  R. M. Rustamov  L. Guibas  and A. Butscher. Wasserstein propagation for semi-supervised
learning. In Proceedings of the 31st International Conference on International Conference on Machine
Learning - Volume 32  ICML’14  pages I–306–I–314. JMLR.org  2014.

[43] M. Staib  S. Claici  J. M. Solomon  and S. Jegelka. Parallel streaming wasserstein barycenters. In Advances

in Neural Information Processing Systems  pages 2644–2655  2017.

[44] C. A. Uribe  D. Dvinskikh  P. Dvurechensky  A. Gasnikov  and A. Nedi´c. Distributed computation of
Wasserstein barycenters over networks. In 2018 IEEE 57th Annual Conference on Decision and Control
(CDC)  pages 6544–6549  Dec 2018.

[45] C. A. Uribe  S. Lee  A. Gasnikov  and A. Nedi´c. Optimal algorithms for distributed optimization. 2017.

arXiv:1712.00232.

[46] C. A. Uribe  S. Lee  A. Gasnikov  and A. Nedi´c. A dual approach for optimal algorithms in distributed

optimization over networks. arXiv preprint arXiv:1809.00710  2018.

11

,Pavel Dvurechenskii
Darina Dvinskikh
Alexander Gasnikov
Cesar Uribe
Angelia Nedich