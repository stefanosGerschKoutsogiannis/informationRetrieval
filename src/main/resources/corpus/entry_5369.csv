2018,Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds,This paper studies the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function. Under the assumption that the objective function satisfies restricted strong convexity (RSC)  we analyze orthogonal matching pursuit (OMP)  a greedy algorithm that is used heavily in applications  and obtain support recovery result as well as a tight generalization error bound for OMP. Furthermore  we obtain lower bounds for OMP  showing that both our results on support recovery and generalization error are tight up to logarithmic factors. To the best of our knowledge  these support recovery and generalization bounds are the first such matching upper and lower bounds (up to logarithmic factors) for {\em any} sparse regression algorithm under the RSC assumption.,Support Recovery for Orthogonal Matching Pursuit:

Upper and Lower bounds

Raghav Somani∗

Microsoft Research  India
t-rasom@microsoft.com

Chirag Gupta∗†

Machine Learning Department 

Carnegie Mellon University
chiragg@andrew.cmu.edu

Prateek Jain

Microsoft Research  India
prajain@microsoft.com

Praneeth Netrapalli

Microsoft Research  India
praneeth@microsoft.com

Abstract

We study the problem of sparse regression where the goal is to learn a sparse
vector that best optimizes a given objective function. Under the assumption that
the objective function satisﬁes restricted strong convexity (RSC)  we analyze
orthogonal matching pursuit (OMP)  a greedy algorithm that is used heavily in
applications  and obtain a support recovery result as well as a tight generalization
error bound for the OMP estimator. Further  we show a lower bound for OMP 
demonstrating that both our results on support recovery and generalization error
are tight up to logarithmic factors. To the best of our knowledge  these are the ﬁrst
such tight upper and lower bounds for any sparse regression algorithm under the
RSC assumption.

1

Introduction

The goal in sparse regression is to ﬁnd the optimal sparse vector that minimizes a given objective
function. Sparse regression is an important problem in statistical machine learning since sparse
models lead to better generalization guarantees when the feature dimension is high or data is less 
eg  high-dimensional statistics [19]  bioinformatics [18]  etc. Sparse models also have a smaller
memory footprint and are thus useful for resource-constrained machine learning [9]. For simplicity
of exposition  we focus on the problem of sparse linear regression (SLR)  which is a representative
problem in this domain. Results for this problem typically extend easily to the general case. Given
A ∈ Rn×d and y  the goal of SLR is to recover a sparse vector ¯x that minimizes (cid:107)Ax − y(cid:107)2
2.
The unconditional version of sparse regression can be shown to be NP-hard via a reduction to 3-set
cover [14]. However  the problem has been studied heavily under a variety of assumptions such
as incoherence [7]  null-space property [8]  restricted isometry property (RIP) or restricted strong
convexity (RSC) [4  15]. RSC  in particular  is one of the weakest and most popular assumptions for
sparse regression problems and has been studied in the context of various algorithms [27  11  1  13].
In this paper  we study the SLR problem under RSC condition.
Typically SLR is studied with one of two goals: a) support recovery  i.e.  recovering support (or

features) of ¯x and b) bounding generalization error(cid:0)(cid:107)A(x−¯x)(cid:107)2

2/n(cid:1) which bounds excess error on

unseen test points if each row of A is sampled from a ﬁxed distribution. In general  support recovery

∗Equal contribution
†Work done in part while Chirag Gupta was a Research Fellow at Microsoft Research  India

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

is a more fundamental and challenging problem as a strong support recovery result usually tends to
provide strong generalization error bound.
Existing sparse regression algorithms can be broadly categorized into three categories: a) (cid:96)1 min-
imization or LASSO based algorithms [6  5  1]  b) non-convex penalty based methods [2  11  13] 
c) greedy methods [22  17  27  12]. In this work  we focus on OMP which is a greedy method that
incrementally adds elements to support based on the amount of reduction in training error. Owing to
its simplicity  ﬂexibility  and strong practical performance  OMP is one of the most celebrated and
practically used algorithms for sparse regression.
OMP has been shown to provide support recovery in noiseless settings  i.e.  when y = A¯x  under
various conditions like incoherence [8]  null-space property  RIP/RSC [28] etc. In the noisy setting 
while the generalization error of OMP has been studied [28] under RSC  these bounds do not match
known lower bounds [29] in terms of the restricted strong convexity constant. In fact  the tightest
known generalization error upper bound for polynomial time algorithms is a factor of restricted strong
convexity constant worse than the known lower bound [29  28  11  30]. Furthermore  strong support
recovery results under RSC are also known only for a non-convex SCAD/MCP penalty based method
[13]. For greedy methods  there have been several recent works [20  21  25] that consider the problem
of support recovery. However  none of these works give strong results for this problem.
In this work  we signiﬁcantly improve upon these support recovery results for OMP. We show that
if the smallest element of ¯x is larger than an appropriate noise level  then OMP recovers the full
support of ¯x (see Theorem 3.1). As noted in remarks 3 and 4 below the theorem  our result has a
better dependence on the restricted condition number than the ones in [20  21  25]. The proof of
Theorem 3.1 exploits the fact that if a certain element of ¯x is not included in the current support set 
then a single step of OMP should lead to a large additive decrease in the error. In addition  we present
a generalization error analysis for OMP.
Finally  we provide matching lower bounds for our support recovery and generalization error results.
To this end  we construct a design matrix that ensures that OMP picks incorrect indices until a large
number of elements are added to the support set (see Theorems 4.2  4.3). As the support set size has
to increase arbitrarily for recovery  this also implies poor generalization error (see Theorem 4.3).
We note that our lower bound results are unconditional and are directly applicable to OMP. In contrast 
existing lower bounds such as [29] obtain a lower bound for generalization error of any polynomial
time algorithm assuming NP (cid:54)⊂ P/poly. Moreover  these lower bound results are restricted to
algorithms which recover exactly s∗-sparse vectors  where s∗ = |supp(¯x)| and hence do not apply to
OMP if it adds more than s∗ elements to the support set  which is the more meaningful scenario to
consider. Moreover  if each element of ¯x is large  then the claim of [29] is almost vacuous as one can
recover the support exactly which is the main problem in SLR. In that case  while the generalization
error lower bound of [29] holds  it does not preclude the OMP algorithm from recovering the correct
support (see Section 4).

Notation: Matrices are typically written in bold capital letters (such as A and Σ)  vectors are
typically written in bold small case letters (such as x and η) and universal constants independent
of problem parameters are written as C1  C2  etc. For a matrix A  Ai represents its ith column
and AS represent the sub-matrix of A with columns in the index set S. ρ+
s (AT A) are
restricted smoothness and restricted strong convexity constants of the matrix A (deﬁned below).

s and (cid:101)κs when used without parameter 
s (AT A) and(cid:101)κs(AT A) respectively. The non-zero element of ¯x with the

(cid:101)κs(AT A) := ρ+

represent ρ+
least absolute value is denoted as ¯xmin.

s (AT A)  ρ−

s (AT A)  ρ−

1 (AT A)/ρ−

s (AT A) for all s > 0. ρ+

s   ρ−

2 Preliminaries and Setting

In this section  we will present some preliminaries and the problem setting considered in this paper.
Broadly  we are interested in sparse estimation problems where we are given a function Q(·) and we
wish to solve minx:(cid:107)x(cid:107)0≤s∗ Q(x). This problem is in general NP-hard even when Q(·) is a quadratic
function. So  we consider this problem under restricted strong convexity (RSC) and restricted
smoothness (RSS) assumptions. While part of our results apply to this general setting  for simplicity
of presentation  we focus on the case where Q(·) is a quadratic. More concretely  in the sparse linear
regression problem where we are given a measurement matrix A ∈ Rn×d and response y ∈ Rn

2

Algorithm 1 Orthogonal Matching Pursuit (OMP)
1: procedure OMP(s)
2:
3:
4:

i rk−1|

S0 = φ  x0 = 0  r0 = y
for k = 1  2  . . .   s do
j ← arg max
|AT
i(cid:54)∈Sk−1
Sk ← Sk−1 ∪ {j}
xk ← arg min
supp(x)⊆Sk
rk ← y − Axk

(cid:107)Ax − y(cid:107)2

2

5:
6:

7:
end for
8:
return xs
9:
10: end procedure

and we wish to solve min(cid:107)x(cid:107)0≤s∗ (cid:107)Ax − y(cid:107)2
restricted strong convexity and restricted smoothness properties [4]:
Deﬁnition 2.1 (Restricted strong convexity (RSC)). A is said to be restricted strongly convex at
level s with parameter ρ−

s if for every x and z such that (cid:107)x − z(cid:107)0 ≤ s  we have

2. We assume that the measurement matrix A satisﬁes

Deﬁnition 2.2 (Restricted smoothness (RSS)). A is said to be restricted smooth at level s with
parameter ρ+

2 ≥ ρ−

(cid:107)Ax − Az(cid:107)2

s (cid:107)x − z(cid:107)2
2 .
s if for every x and z such that (cid:107)z − x(cid:107)0 ≤ s  we have
s (cid:107)x − z(cid:107)2
2 .

(cid:107)Ax − Az(cid:107)2

2 ≤ ρ+

The above deﬁnitions capture the standard strong convexity and smoothness properties but only in
sparse directions. Similarly  we can deﬁne a notion of restricted condition number.
Deﬁnition 2.3 (Restricted condition number). The restricted condition number at level s of a matrix
A is deﬁned as

(cid:101)κs(AT A) =

ρ+
1
ρ−

s

.

(2.1)

s   and(cid:101)κs respectively. For our lower bound matrices in Section 4 we show that

Throughout this paper  we assume that A satisﬁes the above properties and denote the corresponding
parameters as ρ−
these properties are satisﬁed.
Deﬁnition 2.4 ((cid:96)∞ − norm). We deﬁne the (cid:96)∞ − norm of a matrix A:

s   ρ+

(cid:107)A(cid:107)∞ := max
(cid:107)x(cid:107)∞=1

(cid:107)Ax(cid:107)∞

(2.2)

We work under the generative model where ¯x is an s∗-sparse vector supported on S∗  that generates
the data. More concretely  we assume that the measurements y are generated as noisy linear
measurements of ¯x:

(2.3)
where each element of η is a mean zero sub-Gaussian random variable with parameter σ. This means
that for some constant C  we have 

y = A¯x + η 

P{|ηi| > t} ≤ C exp(cid:0)−t2/2σ2(cid:1) .

The non-zero element of ¯x with the least absolute value is denoted as ¯xmin.
In this problem setting  there are two critical questions:

1. Support recovery: The goal here is to recover the support of ¯x after observing y and
A. This question can also be posed as estimating ¯x in the (cid:96)∞ norm i.e.  ﬁnd ˆx such that
(cid:107)ˆx − ¯x(cid:107)∞ is small.
2. Generalization error: Here  the goal is to compute an ˆx such that (cid:107)A(ˆx − ¯x)(cid:107)2 is small.
This quantity is essentially the generalization error when the learned ˆx is used to make
prediction over test data generated from same distribution as training data A and y.

3

Apart from(cid:101)κ(·)  we also use κs(·) = ρ+

Table 1: Comparison between our results and several prior results on support recovery for Sparse
Linear Regression. HTP refers to Hard Thresholding Pursuit  PHT refers to Partial Hard Thresholding 
and IHT referes to Iterative Hard Thresholding. These are all thresholding based greedy algorithms.
s (·). All values are correct upto constants; we have
skipped order notation in the interest of succinctness. Support expansion refers to the value of s in
the paper. The |¯xmin| column refers to the condition for support recovery guarantee. All support
recovery happens with some probability δ  and we incur polynomial factors of log(d/δ) in the |¯xmin|
condition. We skip these in the interest of succinctness.

s (·)/ρ−

Related Work

Support expansion (s)

Yuan et al. [25] [HTP]

Shen et al. [20] [HTP]

Shen et al. [21] [PHT(r)]

Jain et al. [11] [IHT]
Zhang [28] [OMP]
Theorem 3.1 [OMP]

s∗ + κ2

2ss∗
κ2
2ss∗
κ2
2s min{s∗  r}
2s+s∗ s∗
(cid:101)κs+s∗ s∗ log κs+s∗
κ2
(cid:101)κs+s∗ s∗ log κs+s∗

|¯xmin| lower bound

ρ+
1 s

ρ+
1 s

σ

σ

ρ

σ

√

√

√
s√
√
−
2s
κ2s
−
√
s+s∗
ρ
κ2s
−
ρ
2s
–
√
–
γ · σ

ρ+
1
−
s+s∗

ρ

We note that in both the above problems we are allowed to output ˆx that may have s ≥ s∗ elements
in the support. This is a standard and crucial relaxation needed to provide strong guarantees under
weak assumptions for SLR. This work considers orthogonal matching pursuit (OMP) [16  23] for
solving both of the above problems. OMP is one of the most popular methods for sparse optimization
and it is essentially a greedy method that incrementally estimates the support of ¯x by adding one
element at a time. See Algorithm 1 for a pseudo-code of OMP for SLR.
In Section 3 we show our upper bounds for the performance of OMP with respect to both the problems
above  under the RSS/RSC conditions. In Section 4  we provide a matching lower bound (upto
logarithmic factors) which shows that there exist certain sparse linear regression problems on which
OMP cannot perform signiﬁcantly better than the error bounds given by our analysis. In Section 5 we
show some simple simulations to ground our results.

3 Upper bounds for OMP

We ﬁrst present our key contribution which is a support recovery bound for OMP under RSC/RSS.
Theorem 3.1 (Support Recovery for OMP). Let A ∈ Rn×d and ¯x ∈ Rd be a s∗-sparse vector. Let
y = A¯x + η and let ˆxs be the output of OMP after s iterations  where

and (cid:101)κs+s∗
(cid:13)(cid:13)(cid:13)AT

is
S∗\SAS(AT

S AS)−1(cid:13)(cid:13)(cid:13)∞

the

(cid:32)

(cid:33)

5ρ+
s+s∗
ρ−
s+s∗

s ≥ C1(cid:101)κs+s∗ s∗ · log
≤ γ where S = supp(ˆxs). Then  for every δ ∈ (cid:0)0  e−68(cid:1)  if
|¯xmin| ≥(cid:16)

restricted condition number

(cid:17) σ

(Deﬁnition 2.3).

Moreover 

(cid:114)

(3.1)

√

let

 

1 +

2 (1 + γ)

ρ−
s+s∗

ρ+
1 log
and s + s∗ ≥ log (1/δ)  then S∗ ⊆ supp(ˆxs) and (cid:107)ˆxs − ¯x(cid:107)∞ ≤ σ
least 1 − 7δ. Here C1 = 664 is a universal constant.
Remark 1: ρ−
restricted strong convexity of the normalized objective 1
n. Similarly 

s+s∗ is the RSC constant of the (cid:107)Ax − y(cid:107)2
(cid:113)

s+s∗ is n times the
n(cid:107)Ax − y(cid:107)2
2 whose scale is independent of
√
n. Thus |¯xmin| essentially scales as 1/
n.

2 objective. Hence ρ−

log (s/δ) with probability at

√

ρ+
1 hides a

−
ρ
s

 

d
δ

(cid:113) 2

4

Remark 2: The γ parameter in the above theorem is somewhat similar to the standard incoherence
parameter [24]  although the incoherence parameter can be signiﬁcantly larger than γ. Further 
existing results for OMP [26] require the incoherence parameter to be strictly less than 1 while our
analysis holds for arbitrary values of γ. Thus  our results apply to more general design matrices A.
Remark 3: Our assumption on |¯xmin| is better at least by a factor of
assumptions made in recent work that analyzes OMP for support recovery [20  21  25] (see Table 1).
Remark 4: To the best of our knowledge  [13] is the only known support recovery result for LASSO
under RSC  that provides strong guarantees as our result above. However  the non-convex penalty
based algorithm of [13] might produce iterates which are dense  so intermediate steps can be more
expensive than sparsity preserving OMP. Furthermore  while qualitatively  our bound is similar to
the bound of [13]  their proof requires n ≥ (cid:107)¯x(cid:107)2
1 log d which  naïvely  for many problems with
imbalanced non-zero elements of ¯x can be as large as (s∗)2.

√(cid:101)κ than corresponding

Proof Sketch of Theorem 3.1 (see Appendix B.2 for details): Theorem 3.2 (stated below) guar-
antees that OMP has a very small objective value after a certain number of support expansion steps.
This guarantees small generalization error (Theorem 3.3)  but not support recovery. To guarantee
support recovery  our proof critically exploits a novel observation (Lemma B.4 in Appendix B.2) that
if at any iteration of OMP  full support recovery has not happened  then OMP decreases function
value by a ﬁxed  but small  additive constant. Theorem 3.2 allows us to say that even this small
constant decrement cannot happen for too long since the objective value is already small. Overall 
this means that support recovery must happen soon after we have small objective value.
Let s be the iteration index that is sufﬁcient to satisfy the conditions for Theorem 3.2. From
Theorem 3.2 we have with probability at least 1 − 2δ 
2 ≤ (cid:107)A¯x − y(cid:107)2
≤ (cid:107)η(cid:107)2

σ2sρ+
1 log(d/δ)
ρ+
s+s∗

(cid:107)Axs − y(cid:107)2

Suppose any one of the support index has not been recovered (that is  |S∗\S| > 0) then if j ∈ (S∗\S)c
is selected by OMP in its (s + 1)th iteration  we have by step 4 of Algorithm 1 

2 + 40σ2s log(d/δ)

2 + 40

(3.2)

.

S∗\Srs

(cid:13)(cid:13)(cid:13)AT

j rs|.
≤ |AT
In Lemma B.4  we lower bound the LHS of (3.3) as follows:

(cid:13)(cid:13)(cid:13)∞
(cid:113)
2 (1 + γ)(cid:1)
with probability at least 1 − 2δ. Since |¯xmin| ≥(cid:0)1 +
(cid:0)AT
(cid:1)2

2(1 + γ)σ
√

(3.3) with (3.4) gives 

s+s∗|¯xmin| −

(cid:13)(cid:13)(cid:13)AT

(cid:13)(cid:13)(cid:13)∞

≥ ρ−

S∗\Srs

σ2 log

√

.

j rs

d
δ

≤ 1
ρ+
1

ρ+
1 log(d/δ) 

(cid:113)

σ
−
s+s∗

ρ

ρ+
1 log (d/δ)  combining

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

(3.9)

This gives us an additive decrease in the function value:

(cid:107)Axs+1 − y(cid:107)2

xj

2 ≤ min
= (cid:107)Axs − y(cid:107)2

(cid:107)Ajxj − rs(cid:107)2
2 − 1
ρ+
1

2

(cid:0)AT

j rs

(cid:1)2 ≤ (cid:107)Axs − y(cid:107)2

2 − σ2 log(d/δ)

Suppose that for another l iterations  the full support is not recovered. Then 

(cid:107)Axs+l − y(cid:107)2

Further it can be shown that the function value at iteration s + l cannot be too small 

(cid:107)Axs+l − y(cid:107)2

2 ≥ (cid:107)η(cid:107)2

(3.8)
with probability at least 1 − δ. Therefore combining (3.8) and (3.2) and plugging them in (3.7)  we
ﬁnally get 

2 ≤ (cid:107)Axs − y(cid:107)2

2 − σ2l log(d/δ).

2 − σ2(s + l + s∗) − 4σ2(s + l + s∗)(cid:112)log(d/δ) 

l ≤ 80s + s + s∗ = O (s) .

5

Therefore with good probability  OMP recovers the full support in O(s) iterations. See Appendix B.2
for details.
We now bound the training error for OMP after running a certain number of iterations (which are
fewer than the number of iterations required for support recovery as shown in Theorem 3.1). The
proof of this theorem follows via a modiﬁcation of the proof of Lemma A.5 in [28]. See Appendix B.1
for the proof.
Theorem 3.2 (Training Error for OMP). Consider the setting of Theorem 3.1. Also  let

s ≥ 8(cid:101)κs+s∗ s∗ · log

(cid:32)

(cid:33)

.

5ρ+
s+s∗
ρ−
s+s∗

Then with probability 1 − 2δ  the output ˆxs of OMP after s steps satisﬁes:
σ2s log(d/δ)

1
n

(cid:107)Aˆxs − y(cid:107)2

2 ≤ 1
n

(cid:107)A¯x − y(cid:107)2

2 + 40

· ρ+
1
n

ρ+
s+s∗

.

(3.10)

Given good objective value decrease  we can show a tight generalization error on the output of
OMP. While in general support recovery is the main goal of a sparse regression algorithm  in several
problem scenarios one might not care about support recovery and focus only on the accuracy of the
learned predictor. See Appendix B.3 for the proof.
Theorem 3.3 (Generalization Error for OMP). Consider the setting of Theorem 3.1. Let ˆxs be the
output of OMP after s iterations. For any constant C1 ≥ 8  there exists a constant C2(≤ 9C1) such
that if s satisﬁes 

then with probability at least 1 − 4δ 

(cid:32)

(cid:33)

C1(cid:101)κs+s∗ s∗ · log
s − ¯x)(cid:13)(cid:13)2
(cid:13)(cid:13)A(ˆxOMP

1
n

5ρ+
s+s∗
ρ−
s+s∗

2 ≤ C2

≥ s ≥ 8(cid:101)κs+s∗ s∗ · log
σ2(cid:101)κs+s∗ s∗

(cid:32)

· log

n

5ρ+
s+s∗
ρ−
s+s∗

5ρ+
s+s∗
ρ−
s+s∗

(cid:33)

(cid:32)

(cid:33)

 

· log

d
δ

.

(3.11)

3.1 Gaussian ensemble

Finally  we instantiate the above theorems for a Gaussian ensemble  i.e.  when A is sampled from a
Gaussian distribution N (0  Σ). We denote the maximum and the minimum singular values of Σ as
σmax and σmin and the condition number of Σ as κ(Σ). To the best of our knowledge  the following
is the best known generalization error guarantee in this setting in terms of the dependence on κ(Σ).
Corollary 3.3.1 (Gaussian ensemble: generalization error). Let the rows of the matrix A ∈ Rn×d be
sampled from N (0  Σ) where Σii ≤ 1 ∀ i ∈ [d] and ¯x be a s∗-sparse vector. Let ˆxs be the output of
OMP after s iterations and S = supp(ˆxs) be the support recovered  where 

s = C2κ(Σ) · log (45κ(Σ)) s∗  n > 4C1

  and s + s∗ ≥ log
for any δ > 0. Then with probability at least 1 − 4δ − e−C0n  the following holds:

s log d
σmin(Σ)

1
δ

 

(cid:13)(cid:13)A(ˆxOMP

s − ¯x)(cid:13)(cid:13)2

1
n

2 ≤ C3

σ2κ(Σ)s∗

n

· log (45κ (Σ)) · log

d
δ

Here C0  C1  C3 and C4 are universal constants independent of any problem parameters.

Note the linear dependence of generalization error on κ(Σ). This matches the lower bound of [29] 
although technically the bound does not apply to OMP as s > s∗. The proof follows directly from
Theorem 3.3 along with standard concentration results. See Appendix B.3 for details.
We now present support recovery result for Gaussian ensembles. For simplicity  we consider the case
when A is sampled from N (0  I). This can also be extended to N (0  Σ) but involves cumbersome
linear algebraic computations  which we avoid for simplicity.

6

Corollary 3.3.2 (Gaussian ensemble: support recovery). Let the rows of the matrix A ∈ Rn×d be
sampled from N (0  Id×d) and ¯x be a s∗-sparse vector. Suppose further that |¯xmin| ≥ 23σ
.
Let ˆxs be the output of OMP after s iterations and S = supp(ˆxs) be the support recovered  where 

n

s ≥ C1s∗  n > C2(s∗)2 log

1
δ
for any δ > 0. Then S∗ ⊆ supp(ˆxs) and (cid:107)ˆxs − ¯x(cid:107)∞ ≤ 2σ
with probability at least
1 − e−C0n − 9δ. Here C0  C1 and C2 are universal constants independent of any problem parameter.
This matches the bounds of [13] up to constants. The proof directly follows from Theorem 3.1 along
with standard Gaussian concentration results. See Appendix B.3 for details.

  and s + s∗ ≥ log

d
δ

n

 

(cid:113) 2 log(s/δ)

(cid:113) log(d/δ)

4 Lower bounds for OMP

In this section  we provide lower bounds on the performance of OMP  both in terms of support
recovery and generalization error. These bounds show that:

• The imperative quantities we make assumptions on in the upper bound section  viz:(cid:101)κs+s∗

and γ are relevant and meaningful.

• Given bounds on these quantities  our results are tight  up to logarithmic factors.

To provide these lower bounds  we construct matrices M() that are parametrized by . We ﬁx ¯x to

be an s∗-sparse vector such that: (cid:26)¯xi =(cid:112)1/s∗

if 1 ≤ i ≤ s∗ 
if s∗ < i.

¯xi = 0

(4.1)
Thus  S∗ := supp(¯x) = {1  2  . . .   s∗}. All our lower bound theorems use this ﬁxed vector which is
independent of the noise level σ. Our results are thus stronger than a typical minimax rate in which ¯x
can be scaled based on σ. For instance  the lower bounds of [29]  [30] use such a strategy. Also  the
support is distributed evenly across the ¯xi’s (4.1). Thus  we show that even large elements are not
recovered.
We now deﬁne M() ∈ Rn×d for a given  ∈ [0  1]  any s∗ ≤ d ≤ n in the following manner: M()
1:s∗
= n  ∀ i ∈ [s∗]. For i ∈ [d]\ [s∗]  each column
are random orthogonal vectors such that
vector is deﬁned as 

(cid:13)(cid:13)(cid:13)M()
(cid:114) 1 − 

i

(cid:13)(cid:13)(cid:13)2
s∗(cid:88)

2

s∗

j=1

M()

i =

√

 gi 

M()
j +
i gj = 0 for all i (cid:54)= j.

(4.2)

i M()

2 = n  gT

1:s∗ = 0 and gT

where gi is such that (cid:107)gi(cid:107)2
The intuition behind this construction is that OMP would prefer the average direction M()
any of the correct directions M()
the other orthogonal vectors of the matrix.
The parameter  is set carefully to ensure that the condition number of the matrix does not increase
too much  so that M() satisﬁes the constraints of Theorem 3.3 and Theorem 3.1 (upto constants).
This is captured in the next lemma:
Lemma 4.1. The matrix M() satisﬁes

  where i ∈ S∗. Thus  we add a scaled version of M()

S∗ ¯x over
S∗ ¯x to each of

i

(cid:0)M()(cid:1) ≤ 4(1 + 2(1 − )s) = O(s)
• (cid:101)κs
(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)(cid:13)M()T

S∗\SM()

S M()

M()T

(cid:16)

≤

•

S

S

1√
s∗(1−)

for S ∩ S∗ = φ.

We now use the above construction to show that in the noiseless case  i.e.  when y = M()¯x  OMP
fails to recover any of the support elements in S∗ for some . Similarly  we show that in the noisy
case  support recovery fails and hence the generalization error of OMP is also large and matches the
upper bound provided in Theorem 3.3. Proofs for this section can be found in Appendix C.

7

4.1 Noiseless case
For the deterministic noiseless case  i.e.  σ = 0  we consider the matrix M() for  = (1 − 3/2s∗) and
show that OMP requires to add all the elements in support to recover the correct support.
Theorem 4.2. For every value of d  n and s∗ where s∗ ≤ d ≤ n  there exists a design matrix
A ∈ Rn×d and a s∗-sparse vector ¯x (deﬁned in (4.2)  (4.1)) such that the following holds true for
OMP when applied to the sparse linear regression problem with y = A¯x and when OMP is executed
for s ≤ d − s∗ iterations:

• (cid:101)κs(A) ≤ 16(s/s∗) and γ ≤(cid:112)2/3.

• The support set S recovered by OMP after s iterations is disjoint from S∗  i.e.  S∗ ∩ S = φ.

Our support recovery result in Theorem 3.1 requires s ≥ C(cid:101)κs+s∗ s∗ and one natural question is
with support sets of size s ≥(cid:101)κs+s∗ s∗. This in turn implies that the number of rows in A (i.e.  sample
complexity) should also scale with(cid:101)κs+s∗.

whether running OMP for s iterations is necessary for recovering the actual support. This theorem
guarantees that it is indeed the case  i.e.  if design matrix A is ill-conditioned then OMP has to work

Note that the lower bound results of [29]  [30] do not provide any insights for how the sample
complexity of an algorithm should scale with κs+s∗ for support recovery. In fact for this problem
their results are vacuous if |¯xmin| is reasonably large. For instance  with the ¯x deﬁned in (4.1) and
the design matrix proposed by [29]  OMP can recover the true support of ¯x exactly after just O (s∗)
iterations with n = s∗ log d samples. Thus  a large condition number of A in their construction does
not imply difﬁculty in recovery for OMP.

behavior with respect to the restricted condition number(cid:101)κs+s∗. For this section  we consider the

4.2 Noisy case
For the noisy case  i.e.  σ (cid:54)= 0  we can study both support recovery as well as generalization error
matrix M() for  = (1 − 1/4s∗). That is  we show that with high probability  OMP starts recovering
the correct support only after d1−α iterations for some constant α > 0. This further implies that
the generalization error cannot be better than the lower bound on generalization error we showed in
Theorem 3.3 (upto constants).
Theorem 4.3. For every value of d and s∗  and any constants α ∈ (0  1)  δ ∈ (0  1)  such that

8 ≤ s∗ ≤ s ≤ d1−α and d ≥ max(cid:8)32 log (1/δ)   41/α(cid:9)  there exists a sparse linear regression
problem with y = A¯x + η  η ∼ N(cid:0)0  σ2In×n
(cid:1)  with design matrix A  and a s∗-sparse vector ¯x
• (cid:101)κs(A) ≤ 36 (s/s∗) for all s and γ ≤ 1/2 

deﬁned in (4.2) (4.1) such that the following holds:

• With probability at least 1 − δ  the output ˆxs of OMP after s steps satisﬁes:

2 ≥ σ2(cid:101)κs+s∗ s∗

18n

· log

d
δ

 

(cid:107)Aˆxs − A¯x(cid:107)2

1
n

• Support set S recovered by OMP after s iterations is disjoint from S∗.

Note that the dependence of the generalization error bound on(cid:101)κs+s∗ matches our generalization error

bound in Theorem 3.3. Interestingly  for our construction  noise ends up helping recovery because
while Theorem 4.2 ensures that the recovery of true support elements does not occur till the very last
step  noise can only help in recovering one of the true elements. However  the probability of picking
up the correct element by chance is tiny as we restrict s ≤ d1−α. We in fact believe that the result
holds generally for any s and d. However  proving it turns out to be quite intricate since it requires
ﬁner results about the the behavior of the order statistics of independent Gaussian variables.

5 Simulations

In this section  we present simulations that verify our results. In particular  we generate a matrix
M() ∈ R1000×100  and a ﬁxed s∗ = 10-sparse vector ¯x by using the construction given in (4.2) and

8

(4.1) where  ∈ (0  1). We then generate y = M()¯x + σN (0  In×n) and apply OMP for recovering
the support of ¯x. ˆs() denotes the index or support set size that is needed by OMP for fully recovering
the support of ¯x.

Note that we can also compute the actual value of(cid:101)κ for M(); in general the restricted condition

number of M() increases with decreasing   thus increasing the difﬁculty of the support recovery
problem.

term (σ2).

Figure 1: Number of iterations required for recovering the full support of ¯x with respect to the

restricted condition number ((cid:101)κs+s∗) of the design matrix and the sub-Gaussian parameter of the noise
(cid:101)κ(M()) of M() generated by varying  ∈ (0  1). Theorem 4.2 claims that for σ = 0  full recovery
requires(cid:101)κs to be smaller than O(d/s∗)  which is observed in Figure 1(a). For larger variance σ2  full
recovery requires larger number of iterations for smaller(cid:101)κ.
As mentioned in the remark below Theorem 4.3  adding noise can only help in case of large(cid:101)κ as our

Figure 1(a) plots ˆs() (i.e. support size required for full recovery) against restricted condition number

(a) Varying condition number

(b) Varying noise variance

construction precludes full recovery unless s = d. We observe this behavior in both Figure 1(a) and
1(b)  where slightly larger value of σ ends up helping support recovery  but for larger values of noise
variance  OMP’s performance is similar to an algorithm that simply selects each feature uniformly at
random.

6 Conclusion

In this paper  we analyze OMP for the sparse regression problem under RSC/RSS assumptions. We
obtain support recovery and generalization guarantees for OMP under this setting. We also provide
lower bounds for OMP showing that our results are tight up to logarithmic factors. We note that our
results signiﬁcantly improve upon a long list of existing results for greedy methods and match the
best known results for sparse regression that use nonconvex penalty based methods. In contrast to
nonconvex penalty methods however  OMP guarantees the sparsity of intermediate iterates and hence
can be much more efﬁcient. We also verify our results with synthetic experiments.

9

References
[1] Alekh Agarwal  Sahand Negahban  and Martin J. Wainwright. Fast global convergence of
gradient methods for high-dimensional statistical recovery. Ann. Statist.  40(5):2452–2482  10
2012.

[2] Thomas Blumensath and Mike E Davies. Gradient pursuit for non-linear sparse signal modelling.

In Signal Processing Conference  2008 16th European  pages 1–5. IEEE  2008.

[3] Stéphane Boucheron and Maud Thomas. Concentration inequalities for order statistics. arXiv

preprint arXiv:1207.7209  2012.

[4] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theor.  51(12):4203–

4215  December 2005.

[5] Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling.

Inverse problems  23(3):969  2007.

[6] Emmanuel Candes  Terence Tao  et al. The dantzig selector: Statistical estimation when p is

much larger than n. The Annals of Statistics  35(6):2313–2351  2007.

[7] David L Donoho  Michael Elad  and Vladimir N Temlyakov. Stable recovery of sparse over-
complete representations in the presence of noise. IEEE Transactions on information theory 
52(1):6–18  2006.

[8] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing 

volume 1.

[9] Chirag Gupta  Arun Sai Suggala  Ankit Goyal  Harsha Vardhan Simhadri  Bhargavi Paranjape 
Ashish Kumar  Saurabh Goyal  Raghavendra Udupa  Manik Varma  and Prateek Jain. Protonn:
Compressed and accurate knn for resource-scarce devices. In International Conference on
Machine Learning  pages 1331–1340  2017.

[10] D. Hsu  S. M. Kakade  and T. Zhang. A tail inequality for quadratic forms of subgaussian

random vectors. ArXiv e-prints  October 2011.

[11] Prateek Jain  Ambuj Tewari  and Purushottam Kar. On iterative hard thresholding methods for
high-dimensional m-estimation. In Advances in Neural Information Processing Systems  pages
685–693  2014.

[12] Ali Jalali  Christopher C Johnson  and Pradeep K Ravikumar. On learning discrete graphical
models using greedy methods. In Advances in Neural Information Processing Systems  pages
1935–1943  2011.

[13] Po-Ling Loh  Martin J Wainwright  et al. Support recovery without incoherence: A case for

nonconvex regularization. The Annals of Statistics  45(6):2455–2482  2017.

[14] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on

computing  24(2):227–234  1995.

[15] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix
completion: Optimal bounds with noise. Journal of Machine Learning Research  13(May):1665–
1697  2012.

[16] Y. C. Pati  R. Rezaiifar  Y. C. Pati R. Rezaiifar  and P. S. Krishnaprasad. Orthogonal matching
pursuit: Recursive function approximation with applications to wavelet decomposition. In
Proceedings of the 27 th Annual Asilomar Conference on Signals  Systems  and Computers 
pages 40–44  1993.

[17] Yagyensh Chandra Pati  Ramin Rezaiifar  and Perinkulam Sambamurthy Krishnaprasad. Or-
thogonal matching pursuit: Recursive function approximation with applications to wavelet
decomposition. In Signals  Systems and Computers  1993. 1993 Conference Record of The
Twenty-Seventh Asilomar Conference on  pages 40–44. IEEE  1993.

10

[18] Amela Preli´c  Stefan Bleuler  Philip Zimmermann  Anja Wille  Peter Bühlmann  Wilhelm
Gruissem  Lars Hennig  Lothar Thiele  and Eckart Zitzler. A systematic comparison and
evaluation of biclustering methods for gene expression data. Bioinformatics  22(9):1122–1129 
2006.

[19] Pradeep Ravikumar  Martin J Wainwright  John D Lafferty  et al. High-dimensional ising model
selection using l1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319 
2010.

[20] Jie Shen and Ping Li. On the iteration complexity of support recovery via hard thresholding

pursuit. In International Conference on Machine Learning  pages 3115–3124  2017.

[21] Jie Shen and Ping Li. Partial hard thresholding: Towards a principled analysis of support

recovery. In Advances in Neural Information Processing Systems  pages 3127–3137  2017.

[22] Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal

matching pursuit. IEEE Transactions on information theory  53(12):4655–4666  2007.

[23] Joel A. Tropp and Anna C. Gilbert. Signal recovery from random measurements via orthogonal

matching pursuit. IEEE TRANS. INFORM. THEORY  53:4655–4666  2007.

[24] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery
using l1-constrained quadratic programming (lasso). IEEE transactions on information theory 
55(5):2183–2202  2009.

[25] Xiaotong Yuan  Ping Li  and Tong Zhang. Exact recovery of hard thresholding pursuit. In
D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural
Information Processing Systems 29  pages 3558–3566. Curran Associates  Inc.  2016.

[26] Tong Zhang. On the consistency of feature selection using greedy least squares regression.

Journal of Machine Learning Research  10(Mar):555–568  2009.

[27] Tong Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations.

IEEE transactions on information theory  57(7):4689–4708  2011.

[28] Tong Zhang. Sparse recovery with orthogonal matching pursuit under rip. IEEE Transactions

on Information Theory  57(9):6215–6221  2011.

[29] Yuchen Zhang  Martin J Wainwright  and Michael I Jordan. Lower bounds on the performance
of polynomial-time algorithms for sparse linear regression. In Conference on Learning Theory 
pages 921–948  2014.

[30] Yuchen Zhang  Martin J Wainwright  Michael I Jordan  et al. Optimal prediction for sparse
linear models? lower bounds for coordinate-separable m-estimators. Electronic Journal of
Statistics  11(1):752–799  2017.

11

,Raghav Somani
Chirag Gupta
Prateek Jain
Praneeth Netrapalli