2007,A Risk Minimization Principle for a Class of Parzen Estimators,This paper explores the use of a Maximal Average Margin (MAM) optimality principle for the design of learning algorithms. It is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical Parzen window classifier. A direct relation with the Rademacher complexities is established  as such facilitating analysis and providing a notion of certainty of prediction. This analysis is related to Support Vector Machines by means of a margin transformation. The power of the MAM principle is illustrated further by application to ordinal regression tasks  resulting in an $O(n)$ algorithm able to process large datasets in reasonable time.,A Risk Minimization Principle
for a Class of Parzen Estimators

Kristiaan Pelckmans  Johan A.K. Suykens  Bart De Moor
Department of Electrical Engineering (ESAT) - SCD/SISTA

K.U.Leuven University

Kasteelpark Arenberg 10  Leuven  Belgium

Kristiaan.Pelckmans@esat.kuleuven.be

Abstract

This paper1 explores the use of a Maximal Average Margin (MAM) optimality
principle for the design of learning algorithms. It is shown that the application
of this risk minimization principle results in a class of (computationally) simple
learning machines similar to the classical Parzen window classiﬁer. A direct rela-
tion with the Rademacher complexities is established  as such facilitating analysis
and providing a notion of certainty of prediction. This analysis is related to Sup-
port Vector Machines by means of a margin transformation. The power of the
MAM principle is illustrated further by application to ordinal regression tasks 
resulting in an O(n) algorithm able to process large datasets in reasonable time.

1 Introduction

The quest for efﬁcient machine learning techniques which (a) have favorable generalization capac-
ities  (b) are ﬂexible for adaptation to a speciﬁc task  and (c) are cheap to implement is a pervasive
theme in literature  see e.g. [14] and references therein. This paper introduces a novel concept for
designing a learning algorithm  namely the Maximal Average Margin (MAM) principle. It closely
resembles the classical notion of maximal margin as lying on the basis of perceptrons  Support Vec-
tor Machines (SVMs) and boosting algorithms  see a.o. [14  11]. It however optimizes the average
margin of points to the (hypothesis) hyperplane  instead of the worst case margin as traditional. The
full margin distribution was studied earlier in e.g. [13]  and theoretical results were extended and
incorporated in a learning algorithm in [5].

The contribution of this paper is twofold. On a methodological level  we relate (i) results in structural
risk minimization  (ii) data-dependent (but dimension-independent) Rademacher complexities [8  1 
14] and a new concept of ’certainty of prediction’  (iii) the notion of margin (as central is most
state-of-the-art learning machines)  and (iv) statistical estimators as Parzen windows and Nadaraya-
Watson kernel estimators.
In [10]  the principle was already shown to underlie the approach of
mincuts for transductive inference over a weighted undirected graph. Further  consider the model-
class consisting of all models with bounded average margin (or classes with a ﬁxed Rademacher
complexity as we will indicate lateron). The set of such classes is clearly nested  enabling structural
risk minimization [8].

On a practical level  we show how the optimality principle can be used for designing a computation-
ally fast approach to (large-scale) classiﬁcation and ordinal regression tasks  much along the same

1Acknowledgements - K. Pelckmans is supported by an FWO PDM. J.A.K. Suykens and B. De Moor are a
(full) professor at the Katholieke Universiteit Leuven  Belgium. Research supported by Research Council KUL:
GOA AMBioRICS  CoE EF/05/006 OPTEC  IOF-SCORES4CHEM  several PhD/postdoc & fellow grants;
Flemish Government: FWO: PhD/postdoc grants  projects G.0452.04  G.0499.04  G.0211.05  G.0226.06 
G.0321.06  G.0302.07  (ICCoS  ANMMM  MLDM); IWT: PhD Grants  McKnow-E  Eureka-Flite+ Belgian
Federal Science Policy Ofﬁce: IUAP P6/04  EU: ERNSI;

1

lines as Parzen classiﬁers and Nadaraya-Watson estimators. It becomes clear that this result enables
researchers on Parzen windows to beneﬁt directly from recent advances in kernel machines  two
ﬁelds which have evolved mostly separately. It must be emphasized that the resulting learning rules
were already studied in different forms and motivated by asymptotic and geometric arguments  as
e.g. the Parzen window classiﬁer [4]  the ’simple classiﬁer’ as in [12] chap. 1  probabilistic neural
networks [15]  while in this paper we show how an (empirical) risk based optimality criterion un-
derlies this approach. A number of experiments conﬁrm the use of the resulting cheap learning rules
for providing a reasonable (baseline) performance in a small time-window.

The following notational conventions are used throughout the paper. Let the random vector
(X  Y ) ∈ Rd × {−1  1} obey a (ﬁxed but unknown) joint distribution PXY from a probability
space (Rd×{−1  1} P). Let Dn = {(Xi  Yi)}n
i=1 be sampled i.i.d. according to PXY . Let y ∈ Rn
be deﬁned as y = (Y1  . . .   Yn)T ∈ {−1  1}n and X = (X1  . . .   Xn)T ∈ Rn×d. This paper
is organized as follows. The next section illustrates the principle of maximal average margin for
classiﬁcation problems. Section 3 investigates the close relationship with Rademacher complexi-
ties  Section 4 develops the maximal average margin principle for ordinal regression  and Section
5 reports experimental results of application of the MAM to classiﬁcation and ordinal regression
tasks.

2 Maximal Average Margin for Classiﬁers

2.1 The Linear Case

Let the class of hypotheses be deﬁned as

H =nf (·) : Rd → R  w ∈ Rd(cid:12)(cid:12)(cid:12)∀x ∈ Rd : f (x) = wT x  kwk2 = 1o .

Consequently  the signed distance of a sample (X  Y ) to the hyper-plane wT x = 0  or the margin
M (w) ∈ R  can be deﬁned as

(1)

M (w) =

.

(2)

SVMs maximize the worst-case margin. We instead focus on the ﬁrst moment of the margin distri-
bution. Maximizing the expected (average) margin follows from solving

Y (wT X)
kwk2

M∗ = max

w

E(cid:20) Y (wT X)

kwk2 (cid:21) = max

f∈H

E [Y f (X)] .

(3)

Remark that the non-separable case does not require the need for slack-variables. The empirical
counterpart becomes

ˆM = max

w

1
n

n

Xi=1

Yi(wT Xi)

kwk2

 

(4)

nPn
which can be written as a constrained convex problem as minw − 1
i=1 Yi(wT Xi) s.t. kwk2 ≤
nPn
1. The Lagrangian with multiplier λ ≥ 0 becomes L(w  λ) = − 1
2 (wT w − 1).
i=1 Yi(wT Xi) + λ
By switching the minimax problem to a maximin problem (application of Slater’s condition)  the
ﬁrst order condition for optimality ∂L(w λ)

∂w = 0 gives

wn =

1
λn

YiXi =

1
λn

XT y 

(5)

n

Xi=1

where wn ∈ Rd denotes the optimum to (4). The corresponding parameter λ can be found by
n kPn
npyT XXT y since the
substituting (5) in the constraint wT w = 1  or λ = 1
optimum is obviously taking place when wT w = 1. It becomes clear that the above derivations
remain valid as n → ∞  resulting in the following theorem.
Theorem 1 (Explicit Actual Optimum for the MAMC) The function f (x) = wT x in H maxi-
mizing the expected margin satisﬁes

i=1 YiXik2 = 1

kwk2 (cid:21) =
1
λ
where λ is a normalization constant such that kw∗k2 = 1.

E(cid:20) Y (wT X)

arg max

w

E[XY ]   w∗ 

(6)

2

2.2 Kernel-based Classiﬁer and Parzen Window

It becomes straightforward to recast the resulting classiﬁer as a kernel classiﬁer by mapping the
input data-samples X in a feature space ϕ : Rd → Rdϕ where dϕ is possibly inﬁnite. In particular 
we do not have to resort to Lagrange duality in a context of convex optimization (see e.g. [14  9] for
an overview) or functional analysis in a Reproducing Kernel Hilbert Space. Speciﬁcally 

wT

n ϕ(X) =

1
λn

n

Xi=1

YiK(Xi  X) 

(7)

where K : Rd × Rd → R is deﬁned as the inner product such that ϕ(X)T ϕ(X′) = K(X  X′)
for any X  X′. Conversely  any function K corresponds with the inner product of a valid map ϕ
npyT Ωy with
if the function K is positive deﬁnite. As previously  the term λ becomes λ = 1
kernel matrix Ω ∈ Rn×n where Ωij = K(Xi  Xj) for all i  j = 1  . . .   n. Now the class of
positive deﬁnite Mercer kernels can be used as they induce a proper mapping ϕ. A classical choice
is the use of a linear kernel (or K(X  X′) = X T X′)  a polynomial kernel of degree p ∈ N0 (or
2/σ))  or a dedicated
K(X  X′) = (X T X′ + b)p)  an RBF kernel (or K(X  X′) = exp(−kX − X′k2
kernel for a speciﬁc application (e.g. a string kernel  a Fisher kernel  see e.g. [14] and references
therein). Figure 1.a depicts an example of a nonlinear classiﬁer based on the well-known Ripley
dataset  and the contourlines score the ’certainty of prediction’ as explained in the next section.

The expression (7) is similar (proportional) to the classical Parzen window for classiﬁcation  but
differs in the use of a positive deﬁnite (Mercer) kernel K instead of the pdf κ( X−·h ) with bandwidth
h > 0  and in the form of the denominator. The classical motivation of statistical kernel estimators is
based on asymptotic theory in low dimensions (i.e d = O(1))  see e.g. [4]  chap. 10 and references.
The functional form of the optimal rule (7) is similar to the ’simple classiﬁer’ described in [12] 
chap. 1. Thirdly  this estimator was also termed and empirically validated as a probabilistic neural
network by [15]. The novel element from above result is the derivation of a clear (both theoretical
and empirical) optimality principle of the rule  as opposed to the asymptotic results of [4] and the
geometric motivations in [12  15]. As a direct byproduct  it becomes straightforward to extend
the Parzen window classiﬁer easily with an additional intercept term or other parametric parts  or
towards additive (structured) models as in [9].

3 Analysis and Rademacher Complexities

The quantity of interest in the analysis of the generalization performance is the probability of pre-
dicting a mistake (the risk R(w; PXY ))  or

where I(z) equals one if z is true  and zero otherwise.

R(w; PXY ) = PXY (cid:0)Y (wT ϕ(X)) ≤ 0(cid:1) = E(cid:2)I(Y (wT ϕ(X)) ≤ 0)(cid:3)  

(8)

3.1 Rademacher Complexity

Let {σi}n
−1) = 1

i=1 taken from the set {−1  1}n be Bernoulli random variables with P (σ = 1) = P (σ =
2 . The empirical Rademacher complexity is then deﬁned [8  1] as

ˆRn(H)   Eσ"sup

f∈H

n

Xi=1

2

n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

σif (Xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

X1  . . .   Xn#  

(9)

where the expectation is taken over the choice of the binary vector σ = (σ1  . . .   σn)T ∈ {−1  1}n.
It is observed that the empirical Rademacher complexity deﬁnes a natural complexity measure to
study the maximal average margin classiﬁer  as both the deﬁnitions of the empirical Rademacher
complexity and the maximal average margin resemble closely (see also [8]). The following result
was given in [1]  Lemma 22  but we give an alternative proof by exploiting the structure of the
optimal estimate explicitly.
Lemma 1 (Trace bound for the Empirical Rademacher Complexity for H) Let Ω ∈ Rn×n be
deﬁned as Ωij = K(Xi  Xj) for all i  j = 1  . . .   n  then

ˆRn(H) ≤

2

nptr(Ω).

3

(10)

i=1 σi(wT ϕ(Xi)) = σT Ωσ√σT Ωσ

Proof: The proof goes along the same lines as the classical bound on the empirical Rademacher
complexity for kernel machines outlined in [1]  Lemma 22. Speciﬁcally  once a vector σ ∈ {−1  1}n
i=1 σif (Xi) equals the solution as in (7) or
is ﬁxed  it is immediately seen that the maxf∈H
= √σT Ωσ. Now  application of the expectation operator E
maxwPn
ˆRn(H) = E(cid:20) 2

over the choice of the Rademacher variables gives

√σT Ωσ(cid:21) ≤

nPn

2 =

n

2

2

1

1

1

n(cid:0)E(cid:2)σT Ωσ(cid:3)(cid:1)

2

n
Xi j
n n
Xi=1

E [σiσj] K(Xi  Xj)

K(Xi  Xi)!
nptr(Ω) 

=

2

2

2

1

=

(11)

where the inequality is based on application of Jensen’s inequality. This proves the Lemma. (cid:3)

in the case of the RBF kernel

Remark that in the case of a kernel with constant trace (as e.g.

gives as in [8  1].

whereptr(Ω) = √n)  it follows from this result that also the (expected) Rademacher complexity
E[ ˆRn(H)] ≤ptr(Ω). In general  one has that E[K(X  X)] equals the trace of the integral operator
TK deﬁned on L2(PX ) deﬁned as TK(f ) = R K(X  Y )f (X)dPX (X) as in [1]. Application of
McDiarmid’s inequality on the variable Z = supf∈H(cid:0)E[Y (wT ϕ(X))] − 1
i=1 Yi(wT ϕ(Xi))(cid:1)
Lemma 2 (Deviation Inequality) Let 0 < Bϕ < ∞ be a ﬁxed constant such that supz kϕ(z)k2
= supzpK(z  z) ≤ Bϕ such that |wT ϕ(z)| ≤ Bφ  and let δ ∈ R+
0 be ﬁxed. Then with probability
exceeding 1 − δ  one has for any w ∈ Rd that

nPn

E[Y (wT ϕ(X))] ≥

Yi(wT ϕ(Xi)) − ˆRn(H) − 3Bϕs 2 ln(cid:0) 2
δ(cid:1)n

.

1
n

n

Xi=1

(12)

Therefore it follows that one maximizes the expected margin by maximizing the empirical average
margin  while controlling the empirical Rademacher complexity by choice of the model class (ker-
nel). In the case of RBF kernels  Bϕ = 1  resulting in a reasonable tight bound. It is now illustrated
how one can obtain a practical upper-bound to the ’certainty of prediction’ using f (x) = wT

n x.

Theorem 2 (Occurrence of Mistakes) Given an i.i.d. sample Dn = {(Xi  Yi)}n
B ∈ R such that supzpK(z  z) ≤ Bϕ  and a ﬁxed δ ∈ R+
1 − δ  one has for all w ∈ Rd that
pyT Ωy
P (cid:0)Y (wT ϕ(X)) ≤ 0(cid:1) ≤

Bϕ − E[Y (wT ϕ(X))]

≤ 1 −

ˆRn(H)
Bϕ

nBϕ

Bϕ

+

i=1  a constant
0 . Then  with probability exceeding

δ(cid:1)n 
+ 3s 2 ln(cid:0) 2
 .

(13)

Proof: The proof follows directly from application of Markov’s inequality on the positive random
variable Bϕ − Y (wT ϕ(X))  with expectation Bϕ − E[Y (wT ϕ(X))]  estimated accurately by the
sample average as in the previous theorem. (cid:3)
More generally  one obtains that with probability exceeding 1 − δ that for any w ∈ Rd and for any
ρ such that −Bϕ < ρ < Bϕ that
δ(cid:1)n 
Bϕ + ρs 2 ln(cid:0) 2
P (cid:0)Y (wT ϕ(X)) ≤ −ρ(cid:1) ≤
  
with probability exceeding 1 − δ < 1. This results in a practical assessment of the ’certainty’ of a
prediction as follows. At ﬁrst  note that the random variable Y (wT
n ϕ(x)) for a ﬁxed X = x can take
two values: either −|wT
n ϕ(x)|. Therefore P (Y (wT
n ϕ(x)) ≤ 0) = P (Y (wT
n ϕ(x)) =

 pyT Ωy

Bϕ + ρ −

n ϕ(x)| or |wT

ˆRn(H)
Bϕ + ρ

n(Bϕ + ρ)

3Bϕ

(14)

Bϕ

+

+

4

1

0.8

0.6

0.4

0.2

0

2

X

 

Class prediction
class 1
class 2

1

0.8

0.6

0.4

0.2

0

2

X

 

−0.2

 

−1.2

−1

−0.8 −0.6 −0.4 −0.2
X1
(a)

0

0.2

0.4

0.6

0.8

−0.2

 

−1.2

−1

0

0.2

0.4

0.6

0.8

−0.8 −0.6 −0.4 −0.2
X1
(b)

Figure 1: Example of (a) the MAM classiﬁer and (b) the SVM on the Ripley dataset. The contourlines
representtheestimateofcertaintyofprediction(’scores’)asderivedinTheorem2fortheMAMclassiﬁerfor
(a) andasinCorollary1forthecaseofSVMswith g(z) = min(1  max(−1  z)) where |z| < 1 corresponds
with the inner part of the margin of the SVM (b). While the contours in (a) give an overall score of the
predictions thescoresgivenin(b)focustowardsthemarginoftheSVM.

n ϕ(x)) ≤ −|wT

n ϕ(x)|) ≤ P (Y (wT

n ϕ(x)|) as Y can only take the two values −1 or 1. Thus
−|wT
the event ’Y 6= sign(wT x∗)’ for samples X = x∗ occurs with probability lower than the rhs. of
(13) with ρ = |wT x∗|. When asserting this for a number nv ∈ N of samples X ∼ PX with
nv → ∞  a misprediction would occur less than δnv times. In this sense  one can use the latent
variable wT ϕ(x∗) as an indication of how ’certain’ the prediction is. Figure 1.a gives an example
of the MAM classiﬁer  together with the level plots indicating the certainty of prediction. Remark
however that the described ’certainty of prediction’ statement differs from a conditional statement
of the risk given as P (Y (wT ϕ(X)) < 0 | X = x∗). The essential difference with the probabilistic
estimates based on the density estimates resulting from the Parzen window estimator is that results
become independent of the data dimension  as one avoids estimating the joint distribution.

3.2 Transforming the Margin Distribution

Consider the case where the assumption of a reasonable constant B such that P (kXk2 < B) = 1 is
unrealistic. Then  a transformation of the random variable Y (wT X) can be fruitful using a monotone
increasing function g : R → R with a constant B′ϕ ≪ B such that |g(z)| ≤ B′ϕ  and g(0) = 0. In
the choice of a proper transformation  two counteracting effects should be traded properly. At ﬁrst 
a small choice of B improves the bound as e.g. described in Lemma 2. On the other hand  such a
transformation would make the expected value E[g(Y (wT ϕ(X)))] smaller than E[Y (wT ϕ(X))].
Modifying Theorem 2 gives

Corollary 1 (Occurrence of Mistakes  bis) Given i.i.d. samples Dn = {(Xi  Yi)}n
i=1  and a ﬁxed
δ ∈ R+
0 . Let g : R → R be a monotonically increasing function with Lipschitz constant 0 < Lg <
∞  let B′ϕ ∈ R such that |g(z)| ≤ B′ϕ for all z  and g(0) = 0. Then with probability exceeding
1 − δ  one has for any ρ such that −B′ϕ ≤ ρ ≤ B′ϕ and w ∈ Rd that

B′ϕ
B′ϕ + ρ−

1

nPn

i=1 g(Yi(wT

n ϕ(Xi))) − Lg ˆRn(H) − 3B′ϕq 2 log( 2

n

δ )

.

n ϕ(X))) ≤ −ρ(cid:1) ≤

P (cid:0)g(Y (wT
This result follows straightforwardly from Theorem 2 using the property that ˆRn(g ◦ H) ≤
n ϕ(X))) ≤ 0(cid:1) ≤ 1−E[Y g(wT ϕ(X))]
Lg ˆRn(H)  see e.g.
.
Similar as in the previous section  corollary 1 can be used to score the certainty of prediction by
considering for each X = x∗ the value of g(wT x∗) and g(−wT x∗). Figure 1.b gives an example by
considering the clipping transformation g(z) = min(1  max(−1  z)) ∈ [−1  1] such that B′ϕ = 1.

[1]. When ρ = 0  one has P (cid:0)g(Y (wT

B′ϕ + ρ

(15)

1

5

Note that this a-priori choice of the function g is not dependent on the (empirical) optimality criterion
at hand.

3.3 Soft-margin SVMs and MAM classiﬁers

Except the margin-based mechanisms  the MAM classiﬁer shares other properties with the soft-
margin maximal margin classiﬁer (SVM) as well. Consider the following saturation function g(z) =
(1 − z)+  where (·)+ is deﬁned as (z)+ = z if z ≥ 0  and zero otherwise (g(0) = 0). Application
of this function to the MAM formulation of (4)  one obtains for a C > 0

n

w −
max

Xi=1(cid:0)1 − Yi(wT ϕ(Xi))(cid:1)+

which is similar to the support vector machine (see e.g.
explicit  consider the following formulation of (16)

s.t. wT w = C 

(16)

[14]). To make this equivalence more

min
w ξ

ξi

s.t. wT w ≤ C and Yi(wT ϕ(Xi)) ≥ 1 − ξi  ξi ≥ 0 ∀i = 1  . . .   n 

(17)

which is similar to the SVM. Consider the following modiﬁcation

n

Xi=1

n

Xi=1

with rY denoting the ranks of all Yi in y. This expression simpliﬁes expression for wn as wn =
1
λn Xdy. It is seen that using kernels as before  the resulting estimator of the order of the responses
corresponding to x and x′ becomes

ˆfK(x  x′) = sign (m(x) − m(x′))   where m(x) =

1
λn

n

Xi=1

K(Xi  x) rY (i).

(22)

6

min
w ξ

ξi

s.t. wT w ≤ C and Yi(wT ϕ(Xi)) ≥ 1 − ξi

∀i = 1  . . .   n 

(18)

which is equivalent to (4) as in the optimum  Yi(wT ϕ(Xi)) = (1 − ξi) for all i. Thus  omission of
the slack constraints ξi ≥ 0 in the SVM formulation results in the Parzen window classiﬁer.
4 Maximal Average Margin for Ordinal Regression

Along the same lines as [6]  the maximal average margin principle can be applied to ordinal re-
gression tasks. Let (X  Y ) ∈ Rd × {1  . . .   m} with distribution PXY . The w ∈ Rd maximizing
P (I(wT (ϕ(X) − ϕ(X)′)(Y − Y ′) > 0)) can be found by solving for the maximal average margin
between pairs as follows

M∗ = max

w

E(cid:20) sign(Y − Y ′)wT (ϕ(X) − ϕ(X)′)

kwk2

(cid:21) .

Given n i.i.d. samples {(Xi  Yi)}n

i=1  empirical risk minimization is obtained by solving

w −
min

1
n

n

Xi j=1

sign(Yj − Yi)wT (ϕ(Xj) − ϕ(Xi)) s.t. kwk2 ≤ 1.

The Lagrangian with multiplier λ ≥ 0 becomes L(w  λ) = − 1
nPi j wT sign(Yj − Yi)(ϕ(Xj) −
2 (wT w−1). Let there be n′ couples (i  j). Let Dy ∈ {−1  0  1}n′×n such that Dy ki = 1
ϕ(Xi))+ λ
and Dy kj = −1 if the kth couple equals (i  j). Then  by switching the minimax problem to a
maximin problem  the ﬁrst order condition for optimality ∂L(w λ)
∂w = 0 gives the expression. wn =
1
λn XDy1n′. Now the parameter λ can be found by substituting
λ′nPYi<Yj
y XT X Dy1n′. Now the key element is the

(ϕ(Xj) − ϕ(Xi)) = 1

(5) in the constraint wT w = 1  or λ = 1
computation of dy = Dy1n′. Note that

(19)

(20)

(21)

n′ DT

nq1T
Xj=1
sign(Yj − Yi)   ry(i) 

n

dy(i) =

 

oMAM
LS−SVM
oSVM
oGP

120

100

80

60

40

20

y
c
n
e
u
q
e
r
F

 
0
0.5

0.55

0.6

0.65

0.7

0.75

τ

0.8

0.85

0.9

0.95

1

(a)

Data (train/test)
Bank(1) (100/8.092)
Bank(1) (500/7.629)
Bank(1) (5.000/3.192)
Bank(1) (7.500/692)
Bank(2) (100/8.092)
Bank(2) (500/7.629)
Bank(2) (5.000/3.192)
Bank(2) (7.500/692)
Cpu(1) (100/20.540)
Cpu(1) (500/20.140)
Cpu(1) (5.000/15.640)
Cpu(1) (7.500/13.140)
Cpu(1) (15.000/5.640)
(b)

0.37
0.49
0.56
0.57
0.81
0.83
0.86
0.88
0.44
0.50
0.57
0.60
0.69

oMAM LS-SVM oSVM oGP
0.41
0.50

0.46
0.55

0.43
0.51
0.56

-

-
-

-
-

0.84
0.86
0.88

-

0.62
0.66
0.68

-
-

0.87
0.87

-
-

0.64
0.66

-
-
-

0.80
0.81

-
-

0.63
0.65

-
-
-

Figure 2: ResultsonordinalregressiontasksusingoMAM(22)ofO(n) aregressionontherank-transformed
responsesusingLS-SVMs[16]of O(n2) − O(n3) ordinalSVMsandordinalGaussianProcessesforprefer-
entiallearningof O(n4) − O(n6). TheresultsareexpressedasKendall’s τ (with −1 ≤ τ ≤ 1)computedon
thevalidationdatasets. Figure(a)reportsthenumericalresultsoftheartiﬁciallygenerateddata Table(b)gives
theresultonanumberoflargescaleddatasetsdescribedin[2] ifthecomputationtooklessthan5minutes.

Remark that the estimator m : Rd → R equals (except for the normalization term) the Nadaraya-
Watson kernel based on the rank-transform rY of the responses. This observation suggest the appli-
cation of standard regression tools based on the rank-transformed responses as in [7]. Experiments
conﬁrm the use of the proposed ranking estimator  and also motivate the use of a more involved
function approximation tools as e.g. LS-SVMs [16] based on the rank-transformed responses.

5 Illustrative Example

i=1 ⊂ R5 × R with n = 100 and a validation set {(X v

Table 2.b provides numerical results on the 13 classiﬁcation (including 100 randomizations) bench-
mark datasets as described in [11]. The choice of an appropriate kernel parameter was obtained by
cross-validation over a range of bandwidths from σ = 1e − 2 to σ = 1e15. The results illustrate
that the Parzen window classiﬁer performs in general slightly (but not signiﬁcantly so) worse than
the other methods  but obviously reduces the required amount of memory and computation time
(i.e. O(n) versus O(n2) − O(n3)). Hence  it is advised to use the Parzen classiﬁer as a cheap
base-line method  or to use it in a context where time- or memory requirements are stringent. The
ﬁrst artiﬁcial dataset for testing the ordinal regression scheme is constructed as follows. The train-
i )}nv
ing set {(Xi  Yi)}n
i=1 ⊂ R5 × R
i   Y v
with nv = 250 is constructed such that Zi = (wT
i with
i )3 + ev
X v
i = (wT
∗
∗
w∗ ∈ N (0  1)  X  X v ∼ N (0  I5)  and e  ev ∼ N (0  0.25). Now Y (and Y v) are generated pre-
serving the order implied by {Zi}100
i=1) with the intervals χ2-distributed with 5 degrees
of freedom. Figure 2.a shows the results of a Monte Carlo experiment relating both the O(n) pro-
posed estimator (22)  a LS-SVM regressor of O(n2) − O(n3) on the rank-transformed responses
{(Xi  rY (i))}  the O(n4) − O(n6) SVM approach as proposed in [3] and the Gaussian Process
approach of O(n4) − O(n6) given in [2]. The performance of the different algorithms is expressed
in terms of Kendall’s τ computed on the validation data. Table 2.b reports the results on some large
scale datasets as described in [2]  imposing a maximal computation time of 5 minutes. Both tests
suggest the competitive nature of the proposed O(n) procedure  while clearly showing the beneﬁt
of using function estimation (as e.g. LS-SVMs) based on the rank-transformed responses.

i=1 (and {Z v

Xi)3 + ei and Z v

i }250

7

6 Conclusion

This paper discussed the use of the MAM risk optimality principle for designing a learning ma-
chine for classiﬁcation and ordinal regression. The relation with classical methods including Parzen
windows and Nadaraya-Watson estimators is established  while the relation with the empirical
Rademacher complexity is used to provide a measure of ’certainty of prediction’. Empirical exper-
iments show the applicability of the O(n) algorithms on real world problems  trading performance
somewhat for computational efﬁciency with respect to state-of-the art learning algorithms.

References

[1] P.L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3:463–482  2002.

[2] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learning

Research  6:1019–1041  2006.

[3] W. Chu and S. S. Keerthi. New approaches to support vector ordinal regression. In in Proc. of Interna-

tional Conference on Machine Learning  pages 145–152. 2005.

[4] L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag 

1996.

[5] A. Garg and D. Roth. Margin distribution and learning algorithms.

In Proceedings of the Fifteenth
International Conference on Machine Learning (ICML)  pages 210–217. Morgan Kaufmann Publishers 
2003.

[6] R. Herbrich  T. Graepel  and K. Obermayer. Large margin rank boundaries for ordinal regression. Ad-

vances in Large Margin Classiﬁers  pages 115–132  2000. MIT Press  Cambridge  MA.

[7] R.L. Iman and W.J. Conover. The use of the rank transform in regression. Technometrics  21(4):499–509 

1979.

[8] V. Koltchinski. Rademacher penalties and structural risk minimization. IEEE Transactions on Information

Theory  47(5):1902–1914  1999.

[9] K. Pelckmans. Primal-Dual kernel Machines. PhD thesis  Faculty of Engineering  K.U.Leuven  May.

2005. 280 p.  TR 05-95.

[10] K. Pelckmans  J. Shawe-Taylor  J.A.K. Suykens  and B. De Moor. Margin based transductive graph
In Proceedings of the Eleventh International Conference on Artiﬁcial

cuts using linear programming.
Intelligence and Statistics  (AISTATS 2007)  pp. 360-367  San Juan  Puerto Rico  2007.

[11] G. R¨atsch  T. Onoda  and K.-R. M¨uller. Soft margins for adaboost. Machine Learning  42(3):287 – 320 

2001.

[12] B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[13] J. Shawe-Taylor and N. Cristianini. Further results on the margin distribution.

In Proceedings of the
twelfth annual conference on Computational learning theory (COLT)  pages 278–285. ACM Press  1999.
[14] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press 

2004.

[15] D.F. Specht. Probabilistic neural networks. Neural Networks  3:110–118  1990.
[16] J.A.K. Suykens  T. van Gestel  J. De Brabanter  B. De Moor  and J. Vandewalle. Least Squares Support

Vector Machines. World Scientiﬁc  Singapore  2002.

8

,Zeyuan Allen-Zhu
Yuanzhi Li