2017,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization,We propose a new randomized coordinate descent method for a convex optimization template  with broad applications. Our analysis relies on a novel combination of four ideas applied to the primal-dual gap function: smoothing  acceleration  homotopy  and coordinate descent with non-uniform sampling. As a result  our method features the first convergence rate guarantees among the coordinate descent methods  that are the best-known under a variety of common structure assumptions on the template. We provide numerical evidence to support the theoretical results with a comparison to state-of-the-art algorithms.,Smooth Primal-Dual Coordinate Descent Algorithms

for Nonsmooth Convex Optimization

Ahmet Alacaoglu1

Quoc Tran-Dinh2

Olivier Fercoq3

Volkan Cevher1

1Laboratory for Information and Inference Systems (LIONS)  EPFL  Lausanne  Switzerland

{ahmet.alacaoglu  volkan.cevher}@epfl.ch

2 Department of Statistics and Operations Research  UNC-Chapel Hill  NC  USA

quoctd@email.unc.edu

3 LTCI  Télécom ParisTech  Université Paris-Saclay  Paris  France

olivier.fercoq@telecom-paristech.fr

Abstract

We propose a new randomized coordinate descent method for a convex optimization
template with broad applications. Our analysis relies on a novel combination
of four ideas applied to the primal-dual gap function: smoothing  acceleration 
homotopy  and coordinate descent with non-uniform sampling. As a result  our
method features the ﬁrst convergence rate guarantees among the coordinate descent
methods  that are the best-known under a variety of common structure assumptions
on the template. We provide numerical evidence to support the theoretical results
with a comparison to state-of-the-art algorithms.

F (cid:63) = min
x∈Rp

Introduction

{F (x) = f (x) + g(x) + h(Ax)}  

1
We develop randomized coordinate descent methods to solve the following composite convex problem:
(1)
where f : Rp → R  g : Rp → R ∪ {+∞}  and h : Rm → R ∪ {+∞} are proper  closed and
convex functions  A ∈ Rm×p is a given matrix. The optimization template (1) covers many important
applications including support vector machines  sparse model selection  logistic regression  etc. It is
also convenient to formulate generic constrained convex problems by choosing an appropriate h.
Within convex optimization  coordinate descent methods have recently become increasingly popular
in the literature [1–6]. These methods are particularly well-suited to solve huge-scale problems
arising from machine learning applications where matrix-vector operations are prohibitive [1].
To our knowledge  there is no coordinate descent method for the general three-composite form (1)
within our structure assumptions studied here that has rigorous convergence guarantees. Our paper
speciﬁcally ﬁlls this gap. For such a theoretical development  coordinate descent algorithms require
speciﬁc assumptions on the convex optimization problems [1  4  6]. As a result  to rigorously handle
the three-composite case  we assume that (i) f is smooth  (ii) g is non-smooth but decomposable
(each component has an “efﬁciently computable” proximal operator)  and (iii) h is non-smooth.
Our approach:
In a nutshell  we generalize [4  7] to the three composite case (1). For this purpose 
we combine several classical and contemporary ideas: We exploit the smoothing technique in [8] 
the efﬁcient implementation technique in [4  14]  the homotopy strategy in [9]  and the nonuniform
coordinate selection rule in [7] in our algorithm  to achieve the best known complexity estimate for
the template.
Surprisingly  the combination of these ideas is achieved in a very natural and elementary primal-dual
gap-based framework. However  the extension is indeed not trivial since it requires to deal with the
composition of a non-smooth function h and a linear operator A.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

While our work has connections to the methods developed in [7  10  11]  it is rather distinct. First 
we consider a more general problem (1) than the one in [4  7  10]. Second  our method relies on
Nesterov’s accelerated scheme rather than a primal-dual method as in [11]. Moreover  we obtain
the ﬁrst rigorous convergence rate guarantees as opposed to [11]. In addition  we allow using any
sampling distribution for choosing the coordinates.
Our contributions: We propose a new smooth primal-dual randomized coordinate descent method
for solving (1) where f is smooth  g is nonsmooth  separable and has a block-wise proximal operator 
and h is a general nonsmooth function. Under such a structure  we show that our algorithm achieves
the best known O(n/k) convergence rate  where k is the iteration count and to our knowledge  this is
the ﬁrst time that this convergence rate is proven for a coordinate descent algorithm.
We instantiate our algorithm to solve special cases of (1) including the case g = 0 and constrained
problems. We analyze the convergence rate guarantees of these variants individually and discuss the
choices of sampling distributions.
Exploiting the strategy in [4  14]  our algorithm can be implemented in parallel by breaking up the
full vector updates. We also provide a restart strategy to enhance practical performance.
Paper organization: We review some preliminary results in Section 2. The main contribution of
this paper is in Section 3 with the main algorithm and its convergence guarantee. We also present
special cases of the proposed algorithm. Section 4 provides numerical evidence to illustrate the
performance of our algorithms in comparison to existing methods. The proofs are deferred to the
supplementary document.
2 Preliminaries
Notation: Let [n] := {1  2 ···   n} be the set of n positive integer indices. Let us decompose
the variable vector x into n-blocks denoted by xi as x = [x1; x2;··· ; xn] such that each block xi
i=1 pi = p. We also decompose the identity matrix Ip of Rp into n
block as Ip = [U1  U2 ···   Un]  where Ui ∈ Rp×pi has pi unit vectors. In this case  any vector
i x for i ∈ [n]. We
deﬁne the partial gradients as ∇if (x) = U(cid:62)
i ∇f (x) for i ∈ [n]. For a convex function f  we use
dom (f ) to denote its domain  f∗(x) := supu
proxf (x) := arg minu
X   δX (·) denotes its indicator function. We also need the following weighted norms:
(cid:107)xi(cid:107)2
(cid:107)x(cid:107)2

(cid:8)u(cid:62)x − f (u)(cid:9) to denote its Fenchel conjugate  and
(cid:8)f (u) + (1/2)(cid:107)u − x(cid:107)2(cid:9) to denote its proximal operator. For a convex set
[α] =(cid:80)n
[α])2 =(cid:80)n

has the size pi ≥ 1 with(cid:80)n
x ∈ Rp can be written as x = (cid:80)n

i=1 Uixi  and each block becomes xi = U(cid:62)

(i) = (cid:104)Hixi  xi(cid:105) 
i=1 Lα

(i))2 = (cid:104)H−1

i yi  yi(cid:105) 
i=1 L−α

i

((cid:107)yi(cid:107)∗
((cid:107)y(cid:107)∗

((cid:107)yi(cid:107)∗

(i))2.

i (cid:107)xi(cid:107)2
(i) 

(2)

Here  Hi ∈ Rpi×pi is a symmetric positive deﬁnite matrix  and Li ∈ (0 ∞) for i ∈ [n] and α > 0.
In addition  we use (cid:107) · (cid:107) to denote (cid:107) · (cid:107)2.
Formal assumptions on the template: We require the following assumptions to tackle (1):
Assumption 1. The functions f  g and h are all proper  closed and convex. Moreover  they satisfy
(a) The partial derivative ∇if (·) of f is Lipschitz continuous with the Lipschitz constant
(i) ≤ ˆLi(cid:107)di(cid:107)(i) for all x ∈ Rp  di ∈ Rpi.

(b) The function g is separable  which has the following form g(x) =(cid:80)n

ˆLi ∈ [0  +∞)  i.e.  (cid:107)∇if (x + Uidi) − ∇if (x)(cid:107)∗

i=1 gi(xi).

(c) One of the following assumptions for h holds for Subsections 3.3 and 3.4  respectively:

i. h is Lipschitz continuous which is equivalent to the boundedness of dom (h∗).
ii. h is the indicator function for an equality constraint  i.e.  h(Ax) := δ{c}(Ax).

Now  we brieﬂy describe the main techniques used in this paper.
Acceleration: Acceleration techniques in convex optimization date back to the seminal work of
Nesterov in [13]  and is one of standard techniques in convex optimization. We exploit such a scheme
to achieve the best known O(1/k) rate for the nonsmooth template (1).
Nonuniform distribution: We assume that ξ is a random index on [n] associated with a probability
distribution q = (q1 ···   qn)(cid:62) such that

P{ξ = i} = qi > 0  i ∈ [n] 

and

qi = 1.

(3)

n(cid:88)

i=1

2

n for all i ∈ [n]  we obtain the uniform distribution. Let i0  i1 ···   ik be i.i.d. realizations
When qi = 1
of the random index ξ after k iteration. We deﬁne Fk+1 = σ(i0  i1 ···   ik) as the σ-ﬁeld generated
by these realizations.
Smoothing techniques: We can write the convex function h(u) = supy {(cid:104)u  y(cid:105) − h∗(y)} using
its Fenchel conjugate h∗. Since h in (1) is convex but possibly nonsmooth  we smooth h as

hβ(u) := max
y∈Rm

(4)
where ˙y ∈ Rm is given and β > 0 is the smoothness parameter. Moreover  the quadratic function
β(u)  the unique
b(y  ˙y) = 1
solution of this concave maximization problem in (4)  i.e.:

2(cid:107)y − ˙y(cid:107)2 is deﬁned based on a given norm in Rm. Let us denote by y∗

 

(cid:110)(cid:104)u  y(cid:105) − h∗(y) − β
2(cid:107)y − ˙y(cid:107)2(cid:111)

2(cid:107)y − ˙y(cid:107)2(cid:111)
= proxβ−1h∗(cid:0) ˙y + β−1u(cid:1)  

(cid:110)(cid:104)u  y(cid:105) − h∗(y) − β

y∗
β(u) := arg max
y∈Rm

(5)

where proxh∗ is the proximal operator of h∗.
equivalently that dom (h∗) is bounded  then it holds that

If we assume that h is Lipschitz continuous  or

hβ(u) ≤ h(u) ≤ hβ(u) + βD2
h∗
2

  where Dh∗ := max

y∈dom(h∗)

(cid:107)y − ˙y(cid:107) < +∞.

(6)

Let us deﬁne a new smoothed function ψβ(x) := f (x) + hβ(Ax). Then  ψβ is differentiable  and its
block partial gradient

∇iψβ(x) = ∇if (x) + A(cid:62)

i y∗

β(Ax)

β

is also Lipschitz continuous with the Lipschitz constant Li(β) := ˆLi +
Assumption 1  and Ai ∈ Rm×pi is the i-th block of A.
Homotopy:
In smoothing-based methods  the choice of the smoothness parameter is critical. This
choice may require the knowledge of the desired accuracy  number of maximum iterations or the
diameters of the primal and/or dual domains as in [8]. In order to make this choice ﬂexible and our
method applicable to the constrained problems  we employ a homotopy strategy developed in [9] for
deterministic algorithms  to gradually update the smoothness parameter while making sure that it
converges to 0.
3 Smooth primal-dual randomized coordinate descent
In this section  we develop a smoothing primal-dual method to solve (1). Or approach is to combine
the four key techniques mentioned above: smoothing  acceleration  homotopy  and randomized
coordinate descent. Similar to [7] we allow to use arbitrary nonuniform distribution  which may allow
to design a good distribution that captures the underlying structure of speciﬁc problems.
3.1 The algorithm
Algorithm 1 below smooths  accelerates  and randomizes the coordinate descent method.
Algorithm 1. SMooth  Accelerate  Randomize The Coordinate Descent (SMART-CD)
Input: Choose β1 > 0 and α ∈ [0  1] as two input parameters. Choose x0 ∈ Rp.
i )α and qi := (B0
1 Set B0
2 Set τ0 := min{qi | 1 ≤ i ≤ n} ∈ (0  1] for i ∈ [n]. Set ¯x0 = ˜x0 := x0.

for i ∈ [n]. Compute Sα :=(cid:80)n

for all i ∈ [n].

i := ˆLi +

i=1(B0

i )α
Sα

(cid:107)Ai(cid:107)2

β1

(cid:107)Ai(cid:107)2

(7)
  where ˆLi is given in

for k ← 0  1 ···   kmax do
Update ˆxk := (1 − τk)¯xk + τk ˜xk and compute ˆuk := Aˆxk.
Compute the dual step y∗
Select a block coordinate ik ∈ [n] according to the probability distribution q.
Set ˜xk+1 := ˜xk  and compute the primal ik-block coordinate:

k+1h∗(cid:0) ˙y + β−1

k+1 ˆuk(cid:1) .

(ˆuk) = proxβ

k := y∗

βk+1

−1

(cid:110)(cid:104)∇ik f (ˆxk) + A(cid:62)

ik

˜xk+1
ik

:= argmin
xik∈Rpik

k  xik − ˆxk
y∗

ik

(cid:105) + gik (xik ) +

τkBk
ik
2τ0

(cid:107)xik − ˜xk

ik

(cid:107)2

(ik)

(cid:111)

.

3
4

5

6

7

8

9

10

11

(˜xk+1 − ˜xk).

Update ¯xk+1 := ˆxk + τk
τ0
Compute τk+1 ∈ (0  1) as the unique positive root of τ 3 + τ 2 + τ 2
Update βk+2 := βk+1
1+τk+1

for i ∈ [n].

and Bk+1

:= ˆLi +

(cid:107)Ai(cid:107)2
βk+2

i

k τ − τ 2

k = 0.

end for

3

τ0

From the update ¯xk := ˆxk−1 + τk−1

that ˆxk := (1 − τk)(cid:0)ˆxk−1 + τk−1

(˜xk − ˜xk−1) and ˆxk := (1 − τk)¯xk + τk ˜xk  it directly follows

(˜xk − ˜xk−1)(cid:1) + τk ˜xk. Therefore  it is possible to implement the

τ0

algorithm without forming ¯xk.
3.2 Efﬁcient implementation
While the basic variant in Algorithm 1 requires full vector updates at each iteration  we exploit the
idea in [4  14] and show that we can partially update these vectors in a more efﬁcient manner.
Algorithm 2. Efﬁcient SMART-CD
Input: Choose a parameter β1 > 0 and α ∈ [0  1] as two input parameters. Choose x0 ∈ Rp.
1 Set B0
2 Set τ0 := min{qi | 1 ≤ i ≤ n} ∈ (0  1] for i ∈ [n] and c0 = (1 − τ0). Set u0 = ˜z0 := x0.

for i ∈ [n]. Compute Sα :=(cid:80)n

i )α and qi := (B0

for all i ∈ [n].

i := ˆLi +

i=1(B0

i )α
Sα

(cid:107)Ai(cid:107)2

β1

for k ← 0  1 ···   kmax do
Compute the dual step y∗
Select a block coordinate ik ∈ [n] according to the probability distribution q.
Let ∇k

i := ∇ik f (ckuk + ˜zk) + A(cid:62)

(ckAuk + A˜zk) := proxβ

βk+1

k+1h∗(cid:0) ˙y + β−1

−1

k+1(ckAuk + A˜zk)(cid:1) .
(cid:111)

.

(ik)

ik

(cid:110)(cid:104)∇k

βk+1

y∗
i   t(cid:105) + gik (t + ˜zk

(ckAuk + A˜zk). Compute
(cid:107)t(cid:107)2

τkBk
ik
2τ0

) +

ik

tk+1
ik

:= arg min
t∈Rpik

.

ik

ik

:= ˜zk
ik
:= uk
ik

+ tk+1
− 1−τk/τ0

Update ˜zk+1
Update uk+1
Compute τk+1 ∈ (0  1) as the unique positive root of τ 3 + τ 2 + τ 2
Update βk+2 := βk+1
1+τk+1

for i ∈ [n].

and Bk+1

:= ˆLi +

(cid:107)Ai(cid:107)2
βk+2

tk+1
ik

ck

ik

.

i

k τ − τ 2

k = 0.

end for

3
4

5

6

7

8

9

10

11

Proposition 3.1. Let ck =(cid:81)k

We present the following result which shows the equivalence between Algorithm 1 and Algorithm 2 
the proof of which can be found in the supplementary document.

l=0(1 − τl)  ˆzk = ckuk + ˜zk and ¯zk = ck−1uk + ˜zk. Then  ˜xk = ˜zk 

ˆxk = ˆzk and ¯xk = ¯zk  for all k ≥ 0  where ˜xk  ˆxk  and ¯xk are deﬁned in Algorithm 1.
According to Algorithm 2  we never need to form or update full-dimensional vectors. Only times
that we need ˆxk are when computing the gradient and the dual variable y∗
. We present two special
cases which are common in machine learning  in which we can compute these steps efﬁciently.
Remark 3.2. Under the following assumptions  we can characterize the per-iteration complexity
explicitly. Let A  M ∈ Rm×p  and

βk+1

(a) f has the form f (x) =(cid:80)m

j=1 φj(e(cid:62)

j M x)  where ej is the jth standard unit vector.

(b) h is separable as in h(Ax) = δ{c}(Ax) or h(Ax) = (cid:107)Ax(cid:107)1.

Assuming that we store and maintain the residuals rk
u h = Auk 
˜z h = A˜zk  then we have the per-iteration cost as O(max{|{j | Aji (cid:54)= 0}| |{j | Mji (cid:54)= 0}|})
rk
arithmetic operations. If h is partially separable as in [3]  then the complexity of each iteration will
remain moderate.

u f = M uk  rk

˜z f = M ˜zk  rk

3.3 Case 1: Convergence analysis of SMART-CD for Lipschitz continuous h
We provide the following main theorem  which characterizes the convergence rate of Algorithm 1.
Theorem 3.3. Let x(cid:63) be an optimal solution of (1) and let β1 > 0 be given. In addition  let
τ0 := min{qi | i ∈ [n]} ∈ (0  1] and β0 := (1 + τ0)β1 be given parameters. For all k ≥ 1  the

sequence(cid:8)¯xk(cid:9) generated by Algorithm 1 satisﬁes:
where C∗(x0) := (1 − τ0)(Fβ0(x0) − F (cid:63)) +(cid:80)n

E(cid:2)F (¯xk) − F (cid:63)(cid:3) ≤

C∗(x0)

τ0(k − 1) + 1

+

τ0B0
i
2qi

i=1

(cid:107)x(cid:63)

 

β1(1 + τ0)D2
h∗
2(τ0k + 1)
i − x0
i(cid:107)2
(i) and Dh∗ is as deﬁned by (6).

(8)

4

In the special case when we use uniform distribution  τ0 = qi = 1/n  the convergence rate reduces to

where C∗(x0) := (1 − 1
convergence rate of Algorithm 1 is

E(cid:2)F (¯xk) − F (cid:63)(cid:3) ≤ nC∗(x0)
n )(Fβ0 (x0) − F (cid:63)) +(cid:80)n
(cid:17)
O(cid:16) n

k + n − 1

B0
i

i=1

 

k

(n + 1)β0D2
h∗

 

+
2k + 2n
i − x0
i(cid:107)2
(i). This estimate shows that the

2 (cid:107)x(cid:63)

which is the best known so far to the best of our knowledge.
3.4 Case 2: Convergence analysis of SMART-CD for non-smooth constrained optimization
In this section  we instantiate Algorithm 1 to solve constrained convex optimization problem with
possibly non-smooth terms in the objective. Clearly  if we choose h(·) = δ{c}(·) in (1) as the indicator
function of the set {c} for a given vector c ∈ Rm  then we obtain a constrained problem:

F (cid:63) := min
x∈Rp

{F (x) = f (x) + g(x) | Ax = c}  

(9)

where f and g are deﬁned as in (1)  A ∈ Rm×p  and c ∈ Rm.
We can specify Algorithm 1 to solve this constrained problem by modifying the following two steps:

(a) The update of y∗

βk+1

(Aˆxk) at Step 5 is changed to

y∗

βk+1

(Aˆxk) := ˙y + 1

βk+1

(Aˆxk − c) 

(10)

which requires one matrix-vector multiplication in Aˆxk.

(b) The update of τk at Step 9 and βk+1 at Step 10 are changed to

(11)

τk+1 := τk
1+τk

and βk+2 := (1 − τk+1)βk+1.

Now  we analyze the convergence of this algorithm by providing the following theorem.

(10) and (11) and let y(cid:63) be an arbitrary optimal solution of the dual problem of (9). In addition 
let τ0 := min{qi | i ∈ [n]} ∈ (0  1] and β0 := (1 + τ0)β1 be given parameters. Then  we have the
following estimates:

Theorem 3.4. Let(cid:8)¯xk(cid:9) be the sequence generated by Algorithm 1 for solving (9) using the updates
 E(cid:2)F (¯xk) − F (cid:63)(cid:3) ≤ C∗(x0)
E(cid:2)(cid:107)A¯xk − b(cid:107)(cid:3)
where C∗(x0) := (1 − τ0)(Fβ0(x0) − F (cid:63)) +(cid:80)n
lower bound always holds −(cid:107)y(cid:63)(cid:107)E(cid:2)(cid:107)A¯xk − b(cid:107)(cid:3) ≤ E(cid:2)F (¯xk) − F (cid:63)(cid:3).

2(τ0(k−1)+1) + (cid:107)y(cid:63)(cid:107)E(cid:2)(cid:107)A¯xk − b(cid:107)(cid:3)  
(cid:104)(cid:107)y(cid:63) − ˙y(cid:107) +(cid:0)(cid:107)y(cid:63) − ˙y(cid:107)2 + 2β−1

1 C∗(x0)(cid:1)1/2(cid:105)

i − x0
i(cid:107)2
(i). We note that the following

τ0(k−1)+1 + β1(cid:107)y(cid:63)− ˙y(cid:107)2

τ0(k−1)+1

τ0B0
i
2qi

(cid:107)x(cid:63)

(12)

≤

i=1

β1

 

3.5 Other special cases
We consider the following special cases of Algorithm 1:
The case h = 0:
In this case  we obtain an algorithm similar to the one studied in [7] except that
we have non-uniform sampling instead of importance sampling. If the distribution is uniform  then
we obtain the method in [4].
The case g = 0:
In this case  we have F (x) = f (x) + h(Ax)  which can handle the linearly
constrained problems with smooth objective function. In this case  we can choose τ0 = 1  and the
coordinate proximal gradient step  Step 7 in Algorithm 1  is simpliﬁed as

˜xk+1
ik

:= ˜xk
ik

− qik

τkBk
ik

H−1

ik

(cid:16)∇ik f (ˆxk) + A(cid:62)

ik

y∗

βk+1

(ˆuk)

(cid:17)

.

(13)

(14)

In addition  we replace Step 8 with
¯xk+1
i = ˆxk

i +

We then obtain the following results:

i − ˜xk
(˜xk+1

i )  ∀i ∈ [n].

τk
qi

5

Corollary 3.5. Assume that Assumption 1 holds. Let τ0 = 1  β1 > 0 and Step 7 and 8 of Algorithm 1
be updated by (13) and (14)  respectively. If  in addition  h is Lipschitz continuous  then we have

E(cid:2)F (¯xk) − F (cid:63)(cid:3) ≤ 1

n(cid:88)

i=1

B0
i
2q2
i

k

(cid:107)x(cid:63)

i − x0
i(cid:107)2
(i) +

β1D2
h∗
k + 1

 

(15)

where Dh∗ is deﬁned by (6).
If  instead of Lipschitz continuous h  we have h(·) = δ{c}(·) to solve the constrained problem (9)
with g = 0  then we have

k + β1(cid:107)y(cid:63)− ˙y(cid:107)2

(cid:104)(cid:107)y(cid:63) − ˙y(cid:107) +(cid:0)(cid:107)y(cid:63) − ˙y(cid:107)2 + 2β−1

+ (cid:107)y(cid:63)(cid:107)E(cid:2)(cid:107)A¯xk − b(cid:107)(cid:3)  
1 C∗(x0)(cid:1)1/2(cid:105)

2k

(16)

 

 E(cid:2)F (¯xk) − F (cid:63)(cid:3) ≤ C∗(x0)
E(cid:2)(cid:107)A¯xk − b(cid:107)(cid:3)
n(cid:80)

≤ β1

(cid:107)x(cid:63)

i − x0
i(cid:107)2
(i).

k

B0
i
2q2
i

i=1

where C∗(x0) :=

3.6 Restarting SMART-CD
It is known that restarting an accelerated method signiﬁcantly enhances its practical performance
when the underlying problem admits a (restricted) strong convexity condition. As a result  we describe
below how to restart (i.e.  the momentum term) in Efﬁcient SMART-CD. If the restart is injected in
the k-th iteration  then we restart the algorithm with the following steps:



uk+1 ← 0 
u f ← 0 
rk+1
u h ← 0 
rk+1
← y∗
˙y
βk+1 ← β1 
τk+1 ← τ0 
← 1.
ck

(ckrk

u h + rk

˜z h) 

βk+1

The ﬁrst three steps of the restart procedure is for restarting the primal variable which is classical
[15]. Restarting ˙y is also suggested in [9]. The cost of this procedure is essentially equal to the cost
of one iteration as described in Remark 3.2  therefore even restarting once every epoch will not cause
a signiﬁcant difference in terms of per-iteration cost.
4 Numerical evidence
We illustrate the performance of Efﬁcient SMART-CD in brain imaging and support vector machines
applications. We also include one representative example of a degenerate linear program to illustrate
why the convergence rate guarantees of our algorithm matter. We compare SMART-CD with Vu-
Condat-CD [11]  which is a coordinate descent variant of Vu-Condat’s algorithm [16]  FISTA [17] 
ASGARD [9]  Chambolle-Pock’s primal-dual algorithm [18]  L-BFGS [19] and SDCA [5].
4.1 A degenerate linear program: Why do convergence rate guarantees matter?
We consider the following degenerate linear program studied in [9]:

(2 ≤ j ≤ d) 

(17)

xp −(cid:80)p−1

k=1 xk = 1 

xp ≥ 0.

k=1 xk = 0 

s.t. (cid:80)p−1

2xp

min
x∈Rp


Here  the constraint xp −(cid:80)p−1
(cid:34)p−1(cid:88)
xk  xp − p−1(cid:88)

f (x) = 2xp 

Ax =

where

we can ﬁt this problem and its dual form into our template (1).

k=1

k=1

k=1

6

constraint qualiﬁcation condition  which guarantees the primal-dual optimality. If we deﬁne

k=1 xk = 0 is repeated d times. This problem satisﬁes the linear

g(x) = δ{xp≥0}(xp) 

xk  . . .   xp − p−1(cid:88)

(cid:35)(cid:62)

h(Ax) = δ{c}(Ax) 

xk

 

c = [1  0  . . .   0](cid:62) 

Figure 1: The convergence behavior of 3 algorithms on a degenerate linear program.

For this experiment  we select the dimensions p = 10 and d = 200. We implement our algorithm and
compare it with Vu-Condat-CD. We also combine our method with the restarting strategy proposed
above. We use the same mapping to ﬁt the problem into the template of Vu-Condat-CD.
Figure 1 illustrates the convergence behavior of Vu-Condat-CD and SMART-CD. We compare
primal suboptimality and feasibility in the plots. The explicit solution of the problem is used to
generate the plot with primal suboptimality. We observe that degeneracy of the problem prevents
Vu-Condat-CD from making any progress towards the solution  where SMART-CD preserves O(1/k)
rate as predicted by theory. We emphasize that the authors in [11] proved almost sure convergence
for Vu-Condat-CD but they did not provide a convergence rate guarantee for this method. Since the
problem is certainly non-strongly convex  restarting does not signiﬁcantly improve performance of
SMART-CD.
4.2 Total Variation and (cid:96)1-regularized least squares regression with functional MRI data
In this experiment  we consider a computational neuroscience application where prediction is done
based on a sequence of functional MRI images. Since the images are high dimensional and the number
of samples that can be taken is limited  TV-(cid:96)1 regularization is used to get stable and predictive
estimation results [20]. The convex optimization problem we solve is of the form:

1

2(cid:107)M x − b(cid:107)2 + λr(cid:107)x(cid:107)1 + λ(1 − r)(cid:107)x(cid:107)TV.

min
x∈Rp

(18)

This problem ﬁts to our template with
2(cid:107)M x − b(cid:107)2 

f (x) = 1

g(x) = λr(cid:107)x(cid:107)1 

h(u) = λ(1 − r)(cid:107)u(cid:107)1 

where D is the 3D ﬁnite difference operator to deﬁne a total variation norm (cid:107) · (cid:107)TV and u = Dx.
We use an fMRI dataset where the primal variable x is 3D image of the brain that contains 33177
voxels. Feature matrix M has 768 rows  each representing the brain activity for the corresponding
example [20]. We compare our algorithm with Vu-Condat’s algorithm  FISTA  ASGARD  Chambolle-
Pock’s primal-dual algorithm  L-BFGS and Vu-Condat-CD.

Figure 2: The convergence of 7 algorithms for problem (18). The regularization parameters for the
ﬁrst plot are λ = 0.001  r = 0.5  for the second plot are λ = 0.001  r = 0.9  for the third plot are
λ = 0.01  r = 0.5 .
Figure 2 illustrates the convergence behaviour of the algorithms for different values of the regu-
larization parameters. Per-iteration cost of SMART-CD and Vu-Condat-CD is similar  therefore
the behavior of these two algorithms are quite similar in this experiment. Since Vu-Condat’s 

7

02004006008001000epoch10-610-410-2100102F(x)-F*SMART-CDSMART-CD-RestartVu-Condat-CD02004006008001000epoch10-610-410-2100102||Ax-c||020406080100time (s)8000850090009500F(x)Chambolle-PockVu-CondatFISTAASGARDL-BFGSVu-Condat-CDSMART-CD020406080100time (s)8000850090009500F(x)020406080100time (s)8000850090009500F(x)Chambolle-Pock’s  FISTA and ASGARD methods work with full dimensional variables  they have
slow convergence in time. L-BFGS has a close performance to coordinate descent methods.
The simulation in Figure 2 is performed using benchmarking tool of [20]. The algorithms are tuned
for the best parameters in practice.
4.3 Linear support vector machines problem with bias
In this section  we consider an application of our algorithm to support vector machines (SVM)
problem for binary classiﬁcation. Given a training set with m examples {a1  a2  . . .   am} such that
ai ∈ Rp and class labels {b1  b2  . . . bm} such that bi ∈ {−1  +1}  we deﬁne the soft margin primal
support vector machines problem with bias as

As it is a common practice  we solve its dual formulation  which is a constrained problem:

0  1 − bi((cid:104)ai  w(cid:105) + w0)

(cid:16)
(cid:8) 1
2λ(cid:107)M D(b)x(cid:107)2 −(cid:80)m

i=1 xi
0 ≤ xi ≤ Ci  i = 1 ···   m 

(cid:17)
(cid:9)

i=1

min
w∈Rp

Ci max

m(cid:88)
 min
(cid:107)M D(b)x(cid:107)2 − m(cid:88)

x∈Rm

s.t.

+ λ

2(cid:107)w(cid:107)2.

b(cid:62)x = 0 

(19)

(20)

where D(b) represents a diagonal matrix that has the class labels bi in its diagonal and M ∈ Rp×m is
formed by the example vectors. If we deﬁne

f (x) =

1
2λ

xi 

gi(xi) = δ{0≤xi≤Ci} 

c = 0  A = b(cid:62) 

i=1

then  we can ﬁt this problem into our template in (9).
We apply the speciﬁc version of SMART-CD for constrained setting from Section 3.4 and compare
with Vu-Condat-CD and SDCA. Even though SDCA is a state-of-the-art method for SVMs  we are
not able to handle the bias term using SDCA. Hence  it only applies to (20) when b(cid:62)x = 0 constraint
is removed. This causes SDCA not to converge to the optimal solution when there is bias term in the
problem (19). The following table summarizes the properties of the classiﬁcation datasets we used.

Data Set
rcv1.binary [21  22]
a8a [21  23]
gisette [21  24]

Training Size Number of Features Convergence Plot
20 242
22 696
6 000

Figure 3  plot 1
Figure 3  plot 2
Figure 3  plot 3

47 236
123
5 000

Figure 3 illustrates the performance of the algorithms for solving the dual formulation of SVM in (20).
We compute the duality gap for each algorithm and present the results with epochs in the horizontal
axis since per-iteration complexity of the algorithms is similar. As expected  SDCA gets stuck at
a low accuracy since it ignores one of the constraints in the problem. We demonstrate this fact in
the ﬁrst experiment and then limit the comparison to SMART-CD and Vu-Condat-CD. Equipped
with restart strategy  SMART-CD shows the fastest convergence behavior due to the restricted strong
convexity of (20).

Figure 3: The convergence of 4 algorithms on the dual SVM (20) with bias. We only used SDCA in
the ﬁrst dataset since it stagnates at a very low accuracy.
5 Conclusions
Coordinate descent methods have been increasingly deployed to tackle huge scale machine learning
problems in recent years. The most notable works include [1–6]. Our method relates to several works

8

100101102epoch10-410-310-210-1100Duality gapSMART-CDSMART-CD-RestartVu-Condat-CDSDCA100101102epoch10-410-310-210-1100Duality gapSMART-CDSMART-CD-RestartVu-Condat-CD100101102epoch10-510-410-310-210-1100Duality gapSMART-CDSMART-CD-RestartVu-Condat-CDin the literature including [1  4  7  9  10  12]. The algorithms developed in [2–4] only considered
a special case of (1) with h = 0  and cannot be trivially extended to apply to general setting (1).
Here  our algorithm can be viewed as an adaptive variant of the method developed in [4] extended to
the sum of three functions. The idea of homotopy strategies relate to [9] for ﬁrst-order primal-dual
methods. This paper further extends such an idea to randomized coordinate descent methods for
solving (1). We note that a naive application of the method developed in [4] to the smoothed problem
with a carefully chosen ﬁxed smoothness parameter would result in the complexity O(n2/k)  whereas
using our homotopy strategy on the smoothness parameter  we reduced this complexity to O(n/k).
With additional strong convexity assumption on problem template (1)  it is possible to obtain O(1/k2)
rate by using deterministic ﬁrst-order primal-dual algorithms [9  18]. It remains as future work to
incorporate strong convexity to coordinate descent methods for solving nonsmooth optimization
problems with a faster convergence rate.
Acknowledgments
The work of O. Fercoq was supported by a public grant as part of the Investissement d’avenir project 
reference ANR-11-LABX-0056-LMH  LabEx LMH. The work of Q. Tran-Dinh was partly supported
by NSF grant  DMS-1619884  USA. The work of A. Alacaoglu and V. Cevher was supported by
European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation
programme (grant agreement no 725594 - time-data).

References
[1] Y. Nesterov  “Efﬁciency of coordinate descent methods on huge-scale optimization problems ”

SIAM Journal on Optimization  vol. 22  no. 2  pp. 341–362  2012.

[2] P. Richtárik and M. Takáˇc  “Iteration complexity of randomized block-coordinate descent
methods for minimizing a composite function ” Mathematical Programming  vol. 144  no. 1-2 
pp. 1–38  2014.

[3] P. Richtárik and M. Takáˇc  “Parallel coordinate descent methods for big data optimization ”

Mathematical Programming  vol. 156  no. 1-2  pp. 433–484  2016.

[4] O. Fercoq and P. Richtárik  “Accelerated  parallel  and proximal coordinate descent ” SIAM

Journal on Optimization  vol. 25  no. 4  pp. 1997–2023  2015.

[5] S. Shalev-Shwartz and T. Zhang  “Stochastic dual coordinate ascent methods for regularized

loss minimization ” Journal of Machine Learning Research  vol. 14  pp. 567–599  2013.

[6] I. Necoara and D. Clipici  “Parallel random coordinate descent method for composite mini-
mization: Convergence analysis and error bounds ” SIAM J. on Optimization  vol. 26  no. 1 
pp. 197–226  2016.

[7] Z. Qu and P. Richtárik  “Coordinate descent with arbitrary sampling i: Algorithms and com-

plexity ” Optimization Methods and Software  vol. 31  no. 5  pp. 829–857  2016.

[8] Y. Nesterov  “Smooth minimization of non-smooth functions ” Math. Prog.  vol. 103  no. 1 

pp. 127–152  2005.

[9] Q. Tran-Dinh  O. Fercoq  and V. Cevher  “A smooth primal-dual optimization framework for

nonsmooth composite convex minimization ” arXiv preprint arXiv:1507.06243  2015.

[10] O. Fercoq and P. Richtárik  “Smooth minimization of nonsmooth functions with parallel

coordinate descent methods ” arXiv preprint arXiv:1309.5885  2013.

[11] O. Fercoq and P. Bianchi  “A coordinate descent primal-dual algorithm with large step size and

possibly non separable functions ” arXiv preprint arXiv:1508.04625  2015.

[12] Y. Nesterov and S.U. Stich  “Efﬁciency of the accelerated coordinate descent method on
structured optimization problems ” SIAM J. on Optimization  vol. 27  no. 1  pp. 110–123  2017.
[13] Y. Nesterov  “A method for unconstrained convex minimization problem with the rate of
convergence O(1/k2) ” Doklady AN SSSR  vol. 269  translated as Soviet Math. Dokl.  pp. 543–
547  1983.

[14] Y. T. Lee and A. Sidford  “Efﬁcient accelerated coordinate descent methods and faster algorithms
for solving linear systems ” in Foundations of Computer Science (FOCS)  2013 IEEE Annual
Symp. on  pp. 147–156  IEEE  2013.

9

[15] B. O’Donoghue and E. Candes  “Adaptive restart for accelerated gradient schemes ” Foundations

of computational mathematics  vol. 15  no. 3  pp. 715–732  2015.

[16] B. C. V˜u  “A splitting algorithm for dual monotone inclusions involving cocoercive operators ”

Advances in Computational Mathematics  vol. 38  no. 3  pp. 667–681  2013.

[17] A. Beck and M. Teboulle  “A fast iterative shrinkage-thresholding algorithm for linear inverse

problems ” SIAM journal on imaging sciences  vol. 2  no. 1  pp. 183–202  2009.

[18] A. Chambolle and T. Pock  “A ﬁrst-order primal-dual algorithm for convex problems with
applications to imaging ” Journal of mathematical imaging and vision  vol. 40  no. 1  pp. 120–
145  2011.

[19] R. H. Byrd  P. Lu  J. Nocedal  and C. Zhu  “A limited memory algorithm for bound constrained

optimization ” SIAM Journal on Scientiﬁc Computing  vol. 16  no. 5  pp. 1190–1208  1995.

[20] E. D. Dohmatob  A. Gramfort  B. Thirion  and G. Varoquaux  “Benchmarking solvers for tv-(cid:96)1
least-squares and logistic regression in brain imaging ” in Pattern Recognition in Neuroimaging 
2014 International Workshop on  pp. 1–4  IEEE  2014.

[21] C.-C. Chang and C.-J. Lin  “Libsvm: a library for support vector machines ” ACM transactions

on intelligent systems and technology (TIST)  vol. 2  no. 3  p. 27  2011.

[22] D. D. Lewis  Y. Yang  T. G. Rose  and F. Li  “Rcv1: A new benchmark collection for text
categorization research ” Journal of Machine Learning Research  vol. 5  no. Apr  pp. 361–397 
2004.

[23] M. Lichman  “UCI machine learning repository ” 2013.
[24] I. Guyon  S. Gunn  A. Ben-Hur  and G. Dror  “Result analysis of the nips 2003 feature selection

challenge ” in Advances in neural information processing systems  pp. 545–552  2005.

[25] P. Tseng  “On accelerated proximal gradient methods for convex-concave optimization ” Sub-

mitted to SIAM J. Optim  2008.

10

,Ahmet Alacaoglu
Quoc Tran Dinh
Olivier Fercoq
Volkan Cevher
Vrettos Moulos