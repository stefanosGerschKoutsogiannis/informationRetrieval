2018,Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance,Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However  in most situations the  Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn approximation  proving that it enjoys the same smoothness as its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand  high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand  the gradient formula allows to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.,DifferentialPropertiesofSinkhornApproximationforLearningwithWassersteinDistanceGiuliaLuise1AlessandroRudi2MassimilianoPontil1 3CarloCiliberto1 4{g.luise.16 m.pontil}@ucl.ac.ukalessandro.rudi@inria.frc.ciliberto@imperial.ac.uk1DepartmentofComputerScience UniversityCollegeLondon London UK.2INRIA-Départementd’informatique ÉcoleNormaleSupérieure-PSLResearchUniversity Paris France.3IstitutoItalianodiTecnologia Genova Italy.4DepartmentofElectricalandElectronicEngineering ImperialCollege London UK.AbstractApplicationsofoptimaltransporthaverecentlygainedremarkableattentionasaresultofthecomputationaladvantagesofentropicregularization.However inmostsituationstheSinkhornapproximationtotheWassersteindistanceisreplacedbyaregularizedversionthatislessaccuratebuteasytodifferentiate.InthisworkwecharacterizethedifferentialpropertiesoftheoriginalSinkhornapproximation provingthatitenjoysthesamesmoothnessofitsregularizedversionandweexplicitlyprovideanefﬁcientalgorithmtocomputeitsgradient.Weshowthatthisresultbeneﬁtsboththeoryandapplications:ononehand highordersmoothnessconfersstatisticalguaranteestolearningwithWassersteinapproximations.Ontheotherhand thegradientformulaisusedtoefﬁcientlysolvelearningandoptimizationproblemsinpractice.Promisingpreliminaryexperimentscomplementouranalysis.1IntroductionApplicationsofoptimaltransporthavebeengainingincreasingattentioninmachinelearning.ThissuccessismainlyduetotherecentintroductionoftheSinkhorndistance[1 2] whichoffersanefﬁcientalternativetotheheavycostofevaluatingtheWassersteindistancedirectly.Thecomputa-tionaladvantageshavemotivatedrecentapplicationsinoptimizationandlearningoverthespaceofprobabilitydistributions wheretheWassersteindistanceisanaturalmetric.However inthesesettingsadoptingtheSinkhornapproximationrequiressolvingafurtheroptimizationproblemwithrespecttothecorrespondingapproximationfunctionratherthanonlyevaluatingitinapoint.Thisconsistsinabi-levelproblem[3]forwhichitischallengingtoderiveanoptimizationapproach[4].Asaconsequence aregularizedversionoftheSinkhornapproximationisusuallyconsidered[5 6 7 8 9] forwhichitispossibletoefﬁcientlycomputeagradientandthusemployitinﬁrst-orderoptimizationmethods[8].Morerecently alsoefﬁcientautomaticdifferentiationstrategieshavebeenproposed[10] withapplicationsrangingfromdictionarylearning[11]toGANs[4]anddiscriminatanalysis[12].Anaturalquestioniswhethertheeasiertractabilityofthisregularizationispaidintermsofaccuracy.Indeed whileasadirectconsequenceof[13]itcanbeshownthattheoriginalSinkhornapproachprovidesasharpapproximationtotheWassersteindistance[13] thesameisnotguaranteedforitsregularizedversion.InthisworkwerecallboththeoreticallyandempiricallythatinoptimizationproblemstheoriginalSinkhornapproximationissigniﬁcantlymorefavorablethanitsregularizedcounterpart whichhasbeenindeednoticedtohaveatendencytoﬁndover-smoothsolutions[14].WetakethisasamotivationtostudythedifferentialpropertiesofthesharpSinkhornwiththegoalofderivingastrategytoaddressoptimizationandlearningproblemsoverprobabilitydistributions.Theprincipalcontributionsofthis32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.workaretwofold.Firstly weshowthatbothSinkhornapproximationsarehighlysmoothfunctions namelyC∞functionsintheinteriorofthesimplex.Despitethecomparabledifferentialproperties sharpandregularizedSinkhornapproximationsshowaratherdifferentbehaviourwhenadoptedinoptimizationproblemssuchasthecomputationofbarycenters[8].Asaby-productoftheproofofthesmoothness weobtainanexplicitformulatoefﬁcientlycomputethegradientofthesharpSinkhornapproximation whichprovestobeviablealternativetoautomaticdifferentiation[10].Asasecondmaincontribution weprovideanovelsoundapproachtothechallengingproblemoflearningwithSinkhornloss recentlyconsideredin[6].Inparticular weleveragethesmoothnessoftheSinkhornapproximationtostudythegeneralizationpropertiesofastructuredpredictionestimatoradaptedfrom[15]tothissetting provingconsistencyandﬁnitesamplebounds.Weprovidepreliminaryempiricalevidenceoftheeffectivenessoftheproposedapproach.2Background:OptimalTransportandWassersteinDistanceOptimaltransporttheoryinvestigateshowtocompareprobabilitymeasuresoveradomainX.Givenadistancefunctiond:X×X→RbetweenpointsonX(e.g.theEuclideandistanceonX=Rd) thegoalofoptimaltransportisto“translate”(orlift)ittodistancesbetweenprobabilitydistributionsoverX.ThisallowstoequipthespaceP(X)ofprobabilitymeasuresonXwithametricreferredtoasWassersteindistance which foranyµ ν∈P(X)andp≥1isdeﬁned(see[16])asWpp(µ ν)=infπ∈Π(µ ν)ZX×Xdp(x y)dπ(x y) (1)whereWppdenotesthep-thpowerofWpandwhereΠ(µ ν)isthesetofprobabilitymeasuresontheproductspaceX×Xwhosemarginalscoincidewithµandν;namelyΠ(µ ν)={π∈P(X×X)suchthatP1#π=µ P2#π=ν} (2)withPi(x1 x2)=xitheprojectionoperatorsfori=1 2andPi#πthepush-forwardofπ[16].Wassersteindistanceondiscretemeasures.Inthefollowingwefocusonmeasureswithdiscretesupport.Inparticular weconsiderdistributionsµ ν∈P(X)thatcanbewrittenaslinearcombi-nationsµ=Pni=1aiδxiandν=Pmj=1bjδyjofDirac’sdeltascentredataﬁnitenumbernandmofpoints(xi)ni=1and(yj)mj=1inX.Inorderforµandνtobeprobabilities thevectorweightsa=(a1 ... an)>∈∆nandb=(b1 ... bm)>∈∆mmustbelongrespectivelytothenandm-dimensionalsimplex deﬁnedas∆n=(cid:8)p∈Rn+(cid:12)(cid:12)p>1n=1(cid:9)(3)whereRn+isthesetofvectorsp∈Rnwithnon-negativeentriesand1n∈Rndenotesthevectorofallones sothatp>1n=Pni=1piforanyp∈Rn.Inthissetting theevaluationoftheWassersteindistancecorrespondstosolvinganetworkﬂowproblem[17]intermsoftheweightvectorsaandbWpp(µ ν)=minT∈Π(a b)hT Mi(4)whereM∈Rn×misthecostmatrixwithentriesMij=d(xi yj)p hT MiistheFrobeniusproductTr(T>M)andΠ(a b)denotesthetransportationpolytopeΠ(a b)={T∈Rn×m+:T1m=a T>1n=b} (5)whichspecializesΠ(µ ν)inEq.(2)tothissettingandcontainsallpossiblejointprobabilitieswithmarginals“corresponding”toa b.Inthefollowing withsomeabuseofnotation wewilldenotebyWp(a b)theWassersteindistancebetweenthetwodiscretemeasuresµandνwithcorrespondingweightvectorsaandb.AnEfﬁcientApproximationoftheWassersteinDistance.SolvingtheoptimizationinEq.(4)iscomputationallyveryexpensive[1].Toovercometheissue thefollowingregularizedversionoftheproblemisconsidered eSλ(a b)=minT∈Π(a b)hT Mi−1λh(T)withh(T)=−n mXi j=1Tij(logTij−1)(6)2whereλ>0isaregularizationparameter.Indeed asobservedin[1] theadditionoftheentropyhmakestheproblemsigniﬁcantlymoreamenabletocomputations.Inparticular theoptimizationinEq.(6)canbesolvedefﬁcientlyviaSinkhorn’smatrixscalingalgorithm[18].WerefertothefunctioneSλastheregularizedSinkhorn.IncontrasttotheWassersteindistance theregularizedSinkhornisdifferentiable(actuallysmooth asweshowinthisworkinThm.2)andhenceparticularlyappealingforpracticalapplicationswherethegoalistosolveaminimizationoverprobabilityspaces.Indeed thisapproximationhasbeenrecentlyusedwithsuccessinsettingsrelatedtobarycenterestimation[8 9 19] supervisedlearning[6]anddictionarylearning[7].3Motivation:aBetterApproximationoftheWassersteinDistanceThecomputationalbeneﬁtprovidedbytheregularizedSinkhornispaidintermsoftheapproximationwithrespecttotheWassersteindistance.Indeed theentropicterminEq.(6)perturbsthevalueoftheoriginalfunctionalinEq.(4)byatermproportionalto1/λ leadingtopotentiallyverydifferentbehavioursofthetwofunctions(seeExample1foranexampleofthiseffectinpractice).Inthissense anaturalcandidateforabetterapproximationisSλ(a b)=hTλ MiwithTλ=argminT∈Π(a b)hT Mi−1λh(T)(7)thatcorrespondstoeliminatingthecontributionoftheentropicregularizerh(Tλ)fromeSλafterthetransportplanTλhasbeenobtained.ThefunctionSλwasoriginallyintroducedin[1]astheSinkhornapproximation althoughrecentliteratureonthetopichasoftenadoptedthisnamefortheregularizedversionEq.(6).Toavoidconfusion inthefollowingwewillrefertoSλasthesharpSinkhorn.NotethatwewillinterchangeablyusethenotationsSλ(a b)andSλ(µ ν).Theabsenceofthetermh(Tλ)isreﬂectedinafasterrateatapproximatingtheWassersteindistance.Proposition1.Letλ>0.Foranypairofdiscretemeasuresµ ν∈P(X)withrespectiveweightsa∈∆nandb∈∆m wehave(cid:12)(cid:12)Sλ(µ ν)−W(µ ν)(cid:12)(cid:12)≤c1e−λ(cid:12)(cid:12)eSλ(µ ν)−W(µ ν)(cid:12)(cid:12)≤c2λ−1 (8)wherec1 c2areconstantsindependentofλ dependingonthesupportofµandν.TheproofoftheexponentialdecayinerrorofSλinEq.(8)(Left)followsfrom[13](Prop.5.1) whilethecorrespondingboundforeSλEq.(8)(Right)isadirectconsequenceof[20](Prop.2.1).Detailsarepresentedinthesupplementarymaterial.Prop.1suggeststhatthesharpSinkhornprovidesamorenaturalapproximationoftheWassersteindistance.Thisintuitionisfurthersupportedbythefollowingdiscussionwherewecomparethebehaviourofthetwoapproximationsontheproblemofﬁndinganoptimaltransportbarycenterofprobabilitydistributions.WassersteinBarycenters.FindingthebarycenterofasetofdiscreteprobabilitymeasuresD=(µi)Ni=1isachallengingprobleminappliedoptimaltransportsettings[8].TheWassersteinbarycenterisdeﬁnedasµ∗W=argminµBW(µ D) BW(µ D)=NXi=1αiW(µ µi) (9)namelythepointµ∗WminimizingtheweightedaveragedistancebetweenalldistributionsinthesetD withαiscalarweights.FindingtheWassersteinbarycenteriscomputationallyveryexpensiveandthetypicalapproachistoapproximateitwiththebarycenter˜µ∗λ obtainedbysubstitutingtheWassersteindistanceWwiththeregularizedSinkhorneSλinthetheobjectivefunctionalofEq.(9).However inlightoftheresultinProp.1 itisnaturaltoaskwhetherthecorrespondingbaricenterµ∗λofthesharpSinkhornSλcouldprovideabetterestimateoftheWassersteinone.WhilewedeferathoroughempiricalcomparisonofthetwobarycenterstoSec.6 hereweconsiderasimplescenarioinwhichthesharpSinkhorncanbeprovedtobeasigniﬁcantlybetterapproximationoftheWassersteindistance.305101520Bins00.20.40.60.81Massz ySinkhorn = 5 = 50 = 100 = 500Figure1:Comparisonofthesharp(Blue)andregularized(Orange)barycentersoftwoDirac’sdeltas(Black)centeredin0and20fordifferentvaluesofλ.Example1(BarycenteroftwoDeltas).WeconsidertheproblemofestimatingthebarycenteroftwoDirac’sdeltasµ1=δz µ2=δycenteredatz=0andy=nwithz y∈Randnaneveninteger.LetX={x0 ... xn}⊂Rbethesetofallintegersbetween0andnandMthecostmatrixwithsquaredEuclideandistances.Assuminguniformweightsα1=α2 itiswell-knownthattheWassersteinbarycenteristhedeltacenteredontheeuclideanmeanofzandy µ∗W=δz+y2.Adirectcalculation(seeAppendixA)showsinsteadthattheregularizedSinkhornbarycenter˜µ∗λ=Pni=0aiδxitendstospreadthemassacrossallxi∈X accordinglytotheamountofregularization ai∝e−λ((z−xi)2+(y−xi)2)i=0 ... n (10)behavingsimilarlytoa(discretized)Gaussianwithstandarddeviationofthesameorderoftheregularizationλ−1.Onthecontrary thesharpSinkhornbarycenterequalstheWassersteinone namelyµ∗λ=µ∗Wforeveryλ>0.AnexampleofthisbehaviourisreportedinFig.1.MainChallengesoftheSharpSinkhorn.Theexampleabove togetherwithProp.1 providesastrongargumentinsupportofadoptingthesharpSinkhornoveritsregularizedversion.However whilethegradientoftheregularizedSinkhorncanbeeasilycomputed(see[8]orSec.4) anexplicitformforthegradientofthesharpSinkhornhasnotbeenconsidered.Whileapproachesbasedonautomaticdifferentiationhavebeensuccessfullyrecentlyadopted[4 11 4 12] inthisworkweareinterestedininvestigatingtheanalyticpropertiesofthegradientofthesharpSinkhorn forwhichweprovideanexplicitalgorithminthefollowing.4DifferentialPropertiesofSinkhornApproximationsInthissectionwepresentaproofofthesmoothnessofthetwoSinkhornapproximationsintroducedabove andtheexplicitderivationofaformulaforthegradientofSλ.TheseresultswillbekeytoemploythesharpSinkhorninpracticalapplications.TheyareobtainedleveragingtheImplicitFunctionTheorem[21]viaaprooftechniqueanalogoustothatin[12 22 23] whichweoutlineinthissectionanddiscussindetailintheappendix.Theorem2.Foranyλ>0 theSinkhornapproximationseSλandSλ:∆n×∆n→RareC∞intheinterioroftheirdomain.Thm.2guaranteesbothSinkhornapproximationstobeinﬁnitelydifferentiable.InSec.5thisresultwillallowustoderiveanestimatorforsupervisedlearningwithSinkhornlossandcharacterizeitscorrespondingstatisticalproperties(i.e.universalconsistencyandlearningrates).TheproofofThm.2isinstrumentaltoderiveaformulaforthegradientofSλ.Wediscusshereitsmainelementsandstepswhilereferringtothesupplementarymaterialforthecompleteproof.Sketchoftheproof.TheproofofThm.2hingesonthecharacterizationofthe(Lagrangian)dualproblemoftheregularizedSinkhorninEq.(6).Thiscanbeformulated(seee.g.[1])asmaxα βLa b(α β) La b(α β)=α>a+β>b−1λn mXi j=1e−λ(Mij−αi−βj)(11)4Algorithm1Computationof∇aSλ(a b)Input:a∈∆n b∈∆m costmatrixM∈Rn m+ λ>0.T=SINKHORN(a b M λ) ¯T=T1:n 1:(m−1)L=T(cid:12)M ¯L=L1:n 1:(m−1)D1=diag(T1m) D2=diag(¯T>1n)−1H=D1−¯TD2¯T> f=−L1m+¯TD2¯L>1ng=H−1fReturn:g−1n(g>1n)withdualvariablesα∈Rnandβ∈Rm.BySinkhorn’sscalingtheorem[18] theoptimalprimalsolutionTλinEq.(7)canbeobtainedfromthedualsolution(α∗ β∗)ofEq.(11)asTλ=diag(eλα∗)e−λMdiag(eλβ∗) (12)whereforanyv∈Rn thevectorev∈Rndenotestheelement-wiseexponentiationofv(analogouslyformatrices)anddiag(v)∈Rn×nisthediagonalmatrixwithdiagonalcorrespondingtov.SincebothSinkhornapproximationsaresmoothfunctionsofTλ itissufﬁcienttoshowthatTλ(a b)itselfissmoothasafunctionofaandb.GiventhecharacterizationofEq.(12)intermsofthedualsolution thisamountstoprovethatα∗(a b)andβ∗(a b)aresmoothwithrespectto(a b) whichisshownleveragingtheImplicitFunctionTheorem[21].ThegradientofSinkhornapproximations.WenowdiscusshowtoderivethegradientofSinkhornapproximationswithrespecttooneofthetwovariables.Inbothcases thedualproblemintroducedinEq.(11)playsafundamentalrole.Inparticular aspointedoutin[8] thegradientoftheregularizedSinkhornapproximationcanbeobtaineddirectlyfromthedualsolutionas∇aeSλ(a b)=α∗(a b) foranya∈Rnandb∈Rm.Thischaracterizationispossiblebecauseofwell-knownpropertiesofprimalanddualoptimizationproblems[17].ThesharpSinkhornapproximationdoesnothaveaformulationintermsofadualproblemandthereforeasimilarargumentdoesnotapply.Nevertheless weshowherethatitisstillpossibletoobtainitsgradientinclosedformintermsofthedualsolution.Theorem3.LetM∈Rn×mbeacostmatrix a∈∆n b∈∆mandλ>0.LetLa b(α β)bedeﬁnedasin(11) withargmaxin(α∗ β∗).LetTλbedeﬁnedasinEq.(12).Then ∇aSλ(a b)=PT∆n(cid:0)AL1m+B¯L>1n(cid:1)(13)whereL=Tλ(cid:12)M∈Rn×mistheentry-wisemultiplicationbetweenTλandMand¯L∈Rn×m−1correspondstoLwiththelastcolumnremoved.ThetermsA∈Rn×nandB∈Rn×m−1are[AB]=−λD(cid:2)∇2(α β)La b(α∗ β∗)(cid:3)−1 (14)withD=[I0]thematrixconcatenatingthen×nidentitymatrixIandthematrix0∈Rn×m−1withallentriesequaltozero.TheoperatorPT∆ndenotestheprojectionontothetangentplaneT∆n={x∈Rn:Pni=1xi=0}tothesimplex∆n.TheproofofThm.3canbefoundinthesupplementarymaterial(Sec.C).TheresultisobtainedbyﬁrstnotingthatthegradientofSλischaracterized(viathechainrule)intermsofthethegradients∇aα∗(a b)and∇aβ∗(a b)ofthedualsolutions.ThemaintechnicalstepoftheproofistoshowthatthesegradientscorrespondrespectivelytothetermsAandBdeﬁnedinEq.(14).ToobtainthegradientofSλinpractice itisnecessarytocomputetheHessian∇2(α β)La b(α∗ β∗)ofthedualfunctional.Adirectcalculationshowsthatthiscorrespondstothematrix∇2(α β)L(α∗ β∗)=(cid:20)diag(a)¯Tλ¯Tλ>diag(¯b)(cid:21) (15)where¯Tλ(equivalently¯b)correspondstoTλ(respectivelyb)withthelastcolumn(element)removed.Seethesupplementarymaterial(Sec.C)forthedetailsofthisderivation.5Figure2:NestedEllipses:(Left)Sampleinputdata.(Middle)Regularized(Right)sharpSinkhornbarycenters.Fromthediscussionabove itfollowsthatthegradientofSλcanbeobtainedinclosedformintermsofthetransportplanTλ.Alg.1reportsanefﬁcientapproachtoperformthisoperation.ThealgorithmcanbederivedbysimplealgebraicmanipulationofEq.(13) giventhecharacterizationoftheHessianinEq.(15).Werefertothesupplementarymaterialforthedetailedderivationofthealgorithm.BarycenterswiththesharpSinkhorn.UsingAlg.1wecannowapplytheacceleratedgradientdescentapproachproposedin[8]toﬁndbarycenterswithrespecttothesharpSinkhorn.Fig.2reportsaqualitativeexperimentinspiredbytheonein[8] withthegoalofcomparingthetwoSinkhornbarycenters.Weconsidered30imagesofrandomnestedellipsesona50×50grid.Weinterpreteachimageasadistributionwithsupportonpixels.ThecostmatrixisgivenbythesquaredEuclideandistancesbetweenpixels.Fig.2showssomeexamplesimagesinthedatasetandthecorrespondingbarycentersofthetwoSinkhornapproximations.Whilethebarycenter˜µ∗λofeSλsuffersablurryeffect theSλbarycenterµ∗λisverysharp suggestingabetterestimateoftheidealone.Computationalconsiderations.DifferentiationofsharpSinkhorncanbeefﬁcientlycarriedoutalsoviaAutomaticDifferentiation(AD)[4].HerewecommentonthecomputationalcomplexityofAlg.1andempiricallycomparethecomputationaltimesofourapproachandADasdimensionsandnumberofiterationsgrow.ExperimentswererunonaIntel(R)Xeon(R)CPUE3-1240v3@3.40GHzwith16GBRAM.Theimplementationofthiscomparisonisavailableonline1.ByleveragingtheSherman-Woodburymatrixidentity itispossibletoshowthatthetotalcostofcomputingthegradient∇aSλ(a b)witha∈∆nandb∈∆mviaAlg.1isO(nmmin(n m)).Inparticular assumem≤n.Then themostexpensiveoperationsare:O(nm2)formatrixmultiplicationandO(m3)forinvertinganm×mpositivedeﬁnitematrix.Bothoperationshavebeenwell-studiedinthenumericsliteratureandefﬁcientoff-the-shelfimplementations(BLAS LAPACK)areavailable whichexploitthelow-levelparallelstructureofmodernarchitectures(e.g.Choleskyandtriangularinversion).Therefore evenifapriorithegradienthascomparablealgorithmiccomplexityascomputingtheoriginalWasserstein itisreasonabletoexpectittobemoreefﬁcientinpractice.WecomparedthegradientobtainedwithAlg.1andAutomaticDifferentiation(AD)onrandomhistogramswithdifferentn(yaxis) m(xaxis) andreg.λ=0.02.Fromlefttoright wereporttheratiotime(AD)/time(Alg.1)forL=10 L=50 L=100iterations.TheresultsshowninFig.3areaveragedon10differentruns.ExperimentsshowthatthereexistregimesinwhichthegradientcomputedinclosedformisaviablealternativetoAutomaticDifferentiation dependingonthetask.Inparticular itseemsthatastheratiobetweenthesupportsnandmofthetwodistributionsbecomesmoreunbalanced Alg.1isconsistentlyfasterthanAD.Accuracyandapproximationerrors.Weconcludethisdiscussiononcomputationalconsiderationwithanoteontheaccuracyofthemethod.Apriori theexpressionTλ=diag(eλα∗)e−λMdiag(eλβ∗)whichisusedtoderiveAlg.1holds‘atconvergence’ whileinpractisethereisalimitedbudget(intermsoftimeandmemory)forthecomputationofTλ i.e.limitednumberofiterations.In[24]asimilarissueisaddressed.InFig.4weempiricallyshowthatplugginganapproximationTLλobtainedwithaﬁxednumberLofiterationsintheformulaforthegradientallowstoreachanwithrespecttothe‘truegradient’comparableorslightlybetterthanautomaticdifferentiation.Errorsaremeasuredas‘2normofthedifferencebetweenapproximatedgradientand‘truegradient’ wherethe‘truegradient’isobtainedviaautomaticdifferentiationsetting105asmaximumnumberofiterations.Weshowhowerrorsdecreasewithrespecttothenumberofiterationsinatoyexamplewithn=m=2000andregularizationλ=0.01 0.02 0.05.Exampleswithn>>mcanbefoundinAppendixC.1.1https://github.com/GiulsLu/OT-gradients6Figure3:Ratiooftime(AD)/time(Alg.1)for10 50 and100iterationsoftheSinkhornalgorithmFigure4:AccuracyoftheGradientobtainedwithAlg.1orADwithrespecttothenumberofiterations5LearningwithSinkhornLossFunctionsGiventhecharacterizationofsmoothnessforbothSinkhornapproximations inthissectionwefocusonaspeciﬁcapplication:supervisedlearningwithaSinkhornlossfunction.Indeed theresultofThm.2willallowtocharacterizethestatisticalguaranteesofanestimatordevisedforthisproblemintermsofitsuniversalconsistencyandlearningrates.Differentlyfrom[6] whichadoptedanempiricalriskminimizationapproach weaddresstheprobleminEq.(16)fromastructuredpredictionperspective[25]followingarecenttrendofworksaddressingtheproblemwithinthesettingofstatisticallearningtheory[15 26 27 28 29].Thiswillallowustostudyalearningalgorithmwithstrongtheoreticalguaranteesthatcanbeefﬁcientlyappliedinpractice.ProblemSetting.TheproblemoflearningwiththeregularizedSinkhornhasbeenrecentlycon-sideredin[6]andcanbeformulatedasfollows.LetXbeaninputspaceandY=∆nasetofhistograms.Thegoalistoapproximateaminimizeroftheexpectedriskminf:X→YE(f) E(f)=ZX×YS(f(x) y)dρ(x y)(16)givenaﬁnitenumberoftrainingpoints(xi yi)Ni=1independentlysampledfromtheunknowndistri-butionρonX×Y.ThelossfunctionS:Y×Y→RmeasurespredictionerrorsandinoursettingcorrespondstoeitherSλoreSλ.StructuredPredictionEstimator.Givenatrainingset(xi yi)Ni=1 weconsiderbf:X→Ythestructuredpredictionestimatorproposedin[15] deﬁnedsuchthatˆf(x)=argminy∈YNXi=1αi(x)S(y yi)(17)foranyx∈X.Theweightsαi(x)arelearnedfromthedataandcanbeinterpretedasscoressuggestingthecandidateoutputdistributionytobeclosetoaspeciﬁcoutputdistributionyiobservedintrainingaccordingtothemetricS.Whiledifferentlearningstrategiescanbeadoptedtolearntheαscores weconsiderthekernel-basedapproachin[15].Inparticular givenapositivedeﬁnitekernelk:X×X→R[30] wehaveα(x)=(α1(x) ... α(x))>=(K+γNI)−1Kx(18)whereγ>0isaregularizationparameterwhileK∈RN×NandKx∈RNarerespectivelytheempiricalkernelmatrixwithentriesKij=k(xi xj)andtheevaluationvectorwithentries(Kx)i=k(x xi) foranyi j=1 ... N.ApproachesbasedonNyström[31]orrandomfeatures[32]canbeemployedtolowerthecomputationalcomplexityoflearningαfromO(n3)toO(n√n)whilemaintainingsametheoreticalguaranteesinthefollowing[33 34].7Remark1(StructuredPredictionandDifferentiabilityofSinkhorn).ThecurrentworkprovidesbothatheoreticalandpracticalcontributiontotheproblemoflearningwithSinkhornapproximations.Ononehand thesmoothnessguaranteedbyThm.2willallowustocharacterizethegeneralizationpropertiesoftheestimator(seebelow).Ontheotherhand Thm.3providesanefﬁcientapproachtosolvetheprobleminEq.(17).IndeednotethatthisoptimizationcorrespondstosolvingabarycenterproblemintheformofEq.(9).GeneralizationPropertiesofbf.WenowcharacterizethetheoreticalpropertiesoftheestimatorintroducedinEq.(17).Westartbyshowingbfisuniversallyconsistent namelythatitachievesminimumexpectedriskasthenumberoftrainingpointsNincreases.Toavoidtechnicalissuesontheboundary inthefollowingwewillrequireY=∆nforsome>0tobethesetofpointsp∈∆nwithpi≥foranyi=1 ... n.ThemaintechnicalstepinthiscontextistoshowthatforanysmoothlossfunctiononY theestimatorinEq.(17)isconsistent.Inthissense thecharacterizationofsmoothnessinThm.2iskeytoprovethefollowingresult incombinationwithThm.4in[15].Theproofcanbefoundinthesupplementarymaterial.Theorem4(UniversalConsistency).LetY=∆n λ>0andSbeeithereSλorSλ.Letkbeaboundedcontinuousuniversal2kernelonX.ForanyN∈NandanydistributionρonX×YletbfN:X→YbetheestimatorinEq.(17)trainedwith(xi yi)Ni=1pointsindependentlysampledfromρandγN=N−1/4.ThenlimN→∞E(bfN)=minf:X→YE(f)withprobability1.Toourknowledge Thm.4istheﬁrstresultcharacterizingtheuniversalconsistencyofanestimatorminimizinganapproximationtotheWassersteindistance.LearningRates.Understandardregularityconditionsontheproblem ouranalysisalsoallowstoproveexcessriskbounds.Sincetheseconditionsaresigniﬁcantlytechnicalwegiveaninformalformulationofthetheorem(seeSec.Dfortherigorousstatementandproof).Theorem5(Excessriskbounds-Informal).LetbfN:X→YbetheestimatorinEq.(17)withγ=N−1/2.Understandardregularityconditionsonρ(seesupplementarymaterial) E(bfN)−minf:X→YE(f)=O(N−1/4)withhighprobabilitywithrespecttosamplingoftrainingdata.Remark2.Recentlyin[36]aSinkhorndivergencewithautocorrelationtermshasbeenprovedtobeasymmetricpositivedeﬁnitefunctionandhencemoresuitableaslossfunctioninalearningscenario.ThestatisticalguaranteesofThm.4andThm.5stillholdtrueforsuchloss.Weconcludethissectionwithanoteonpreviouswork.Werecallthat[6]hasprovidedtheﬁrstgeneralizationboundsforanestimatorminimizingtheregularizedSinkhornloss.InThm.5howeverwecharacterizetheexcessriskboundsoftheestimatorinEq.(17).Thetwoapproachesandanalysisarebasedondifferentassumptionsontheproblem.Therefore acomparisonofthecorrespondinglearningratesisoutsidethescopeofthisanalysisandisleftforfuturework.6ExperimentsWepresenthereexperimentscomparingthetwoSinkhornapproximationsempirically.Optimizationwasperformedwiththeacceleratedgradientfrom[8]forSλandBregmanprojections[9]foreSλ.BarycenterswithSinkhornApproximations.WecomparedthequalityofSinkhornbarycentersintermsoftheirapproximationofthe(ideal)Wassersteinbarycenter.Weconsidereddiscretedistributionson100bins correspondingtotheintegersfrom1to100andasquaredEuclideancostmatrixM.Wegenerateddatasetsof10measureseach whereonlyk=1 2 10 50(randomlychosen)consecutivebinsaredifferentfromzero withthenon-zeroentriessampleduniformlybetween0and1(andthennormalizedtosumupto1).WeempiricallychosetheSinkhornregularizationparameterλ2Thisisastandardassumptionsforuniversalconsistency(see[35]).Example:k(x x0)=e−kx−x0k2/σ.8SupportImprovement1%2%10%50%BW(˜µ∗λ)−BW(µ∗λ)14.914±0.07612.482±0.1352.736±0.5690.258±0.012Table1:AverageabsoluteimprovementintermsoftheidealWassersteinbarycenterfunctionalBWinEq.(9)forsharpvsregularizedSinkhornforbarycentersofrandommeasureswithsparsesupport.ReconstructionError(%)Misclassiﬁcationrate#ClassesSλeSλHell[26]KDE[37]oftheclassiﬁer(%)23.7±0.64.9±0.98.0±2.412.0±4.10.024±0.003422.2±0.931.8±1.129.2±0.840.8±4.20.076±0.0081038.9±0.944.9±2.548.3±2.464.9±1.40.178±0.012Table2:AveragereconstructionerrorsoftheSinkhorn Hellinger andKDEestimatorsontheGoogleQuickDrawreconstructionproblem.Errorsmeasuredbyadigitclassiﬁerwithbasemisclassiﬁcationreportedinlastcolumn.tobethesmallestvaluesuchthattheoutputTλoftheSinkhornalgorithmwouldbewithin10−6fromthetransportpolytopein1000iterations.Tab.1reportstheabsoluteimprovementofthebarycenterofthesharpSinkhornwithrespecttotheoneobtainedwiththeregularizedSinkhorn averagedover10independentdatasetgenerationforeachsupportsizek.Ascanbenoticed thesharpSinkhornconsistentlyoutperformsitsregularizedcounterpart.Theimprovementismoreevidentformeasureswithsparsesupportandtendstoreduceasthesupportincreases.ThisisinlinewiththeremarkinExample1andthefactthattheregularizationtermineSλencouragesoversmoothedsolutions.LearningwithWassersteinloss.WeevaluatedtheSinkhornapproximationsinanimagerecon-structionproblemsimilartotheoneconsideredin[37]forstructuredprediction.Givenanimagedepictingadrawing thegoalistolearnhowtoreconstructthelowerhalfoftheimage(output)giventheupperhalf(input).Similarlyto[8]weinterpreteach(half)imageasanhistogramwithmasscorrespondingtothegraylevels(normalizedtosumupto1).Forallexperiments accordingto[15] weevaluatedtheperformanceofthereconstructionintermsoftheclassiﬁcationaccuracyofanimagerecognitionSVMclassiﬁertrainedonaseparatedataset.TotrainthestructuredpredictionestimatorinEq.(17)weusedaGaussiankernelwithbandwithσandregularizationparameterγselectedbycross-validation.GoogleQuickDraw.Wecomparedtheperformanceofthetwoestimatorsonachallengingdataset.Weselectedc=2 4 10classesfromtheGoogleQuickDrawdataset[38]whichconsistsinimagesofsize28×28pixels.Wetrainedthestructuredpredictionestimatorson1000imagesperclassandtestedonother1000images.Werepeatedtheseexperiments5times eachtimerandomlysamplingadifferenttrainingandtestdataset.Tab.2reportsthereconstructionerror(i.e.theclassiﬁcationerroroftheSVMclassiﬁer)overimagesreconstructedbytheSinkhornestimators thestructuredpredictionestimatorwithHellingerloss[15]andtheKernelDependencyEstimator(KDE)[37].LastcolumnreportsthebasemisclassiﬁcationerroroftheSVMclassiﬁeronthegroundtruth(i.e.thecompletedigits) providingalowerboundonthesmallestpossiblereconstructionerror.BothSinkhornestimatorsperformsigniﬁcantlybetterthantheircompetitors(excepttheHellingerdistanceoutperformingeSλon4classes).Thisisinlinewiththeintuitionthatoptimaltransportmetricsrespectthewaythemassisdistributedonimages[1 8].Moreover itisinterestingtonotethattheestimatorofthesharpSinkhornprovidesalwaysbetterreconstructionsthanitsregularizedcounterpart.7ConclusionsInthispaperweinvestigatedthedifferentialpropertiesofSinkhornapproximations.Weprovedthehighordersmoothnessofthetwofunctionsandderivedasaby-productoftheproofanexplicitalgorithmtoefﬁcientlycomputethegradientofthesharpSinkhorn.Thecharacterizationofsmooth-nessprovedtobeakeytooltostudythestatisticalpropertiesoftheSinkhornapproximationaslossfunction.Inparticularweconsideredastructuredpredictionestimatorforwhichweproveduniversalconsistencyandexcessriskbounds.Futureworkwillfocusonfurtherapplicationsandamoreextensivecomparisonwiththeexistingliterature.9AcknowledgmentsThisworkwassupportedinpartbyEPSRCGrantN.EP/P009069/1 bytheEuropeanResearchCouncil(grantSEQUOIA724063) UKDefenceScienceandTechnologyLaboratory(Dstl)andEngineeringandPhysicalResearchCouncil(EPSRC)undergrantEP/P009069/1.ThisispartofthecollaborationbetweenUSDOD UKMODandUKEPSRCundertheMultidisciplinaryUniversityResearchInitiative.References[1]MarcoCuturi.Sinkhorndistances:Lightspeedcomputationofoptimaltransport.InC.J.C.Burges L.Bottou M.Welling Z.Ghahramani andK.Q.Weinberger editors AdvancesinNeuralInformationProcessingSystems26 pages2292–2300.CurranAssociates Inc. 2013.[2]GabrielPeyré MarcoCuturi etal.Computationaloptimaltransport.Technicalreport 2017.[3]StephanDempe.Foundationsofbilevelprogramming.SpringerScience&BusinessMedia 2002.[4]AudeGenevay GabrielPeyré andMarcoCuturi.Learninggenerativemodelswithsinkhorndivergences.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics pages1608–1617 2018.[5]NicolasCourty RémiFlamary andDevisTuia.Domainadaptationwithregularizedoptimaltransport.InECML/PKDD2014 LNCS pages1–16 Nancy France September2014.[6]CharlieFrogner ChiyuanZhang HosseinMobahi MauricioAraya-Polo andTomasoPoggio.Learningwithawassersteinloss.InProceedingsofthe28thInternationalConferenceonNeuralInformationProcessingSystems-Volume2 NIPS’15 pages2053–2061 Cambridge MA USA 2015.MITPress.[7]AntoineRolet MarcoCuturi andGabrielPeyré.Fastdictionarylearningwithasmoothedwassersteinloss.InArthurGrettonandChristianC.Robert editors Proceedingsofthe19thInternationalConferenceonArtiﬁcialIntelligenceandStatistics volume51ofProceedingsofMachineLearningResearch pages630–638 Cadiz Spain 09–11May2016.PMLR.[8]MarcoCuturiandArnaudDoucet.Fastcomputationofwassersteinbarycenters.InEricP.XingandTonyJebara editors Proceedingsofthe31stInternationalConferenceonMachineLearning volume32ofProceedingsofMachineLearningResearch pages685–693 Bejing China 22–24Jun2014.PMLR.[9]Jean-DavidBenamou GuillaumeCarlier MarcoCuturi LucaNenna andGabrielPeyré.Iterativebregmanprojectionsforregularizedtransportationproblems.SIAMJ.ScientiﬁcComputing 37(2) 2015.[10]NicolasBonneel GabrielPeyré andMarcoCuturi.Wassersteinbarycentriccoordinates:histogramregressionusingoptimaltransport.ACMTrans.Graph. 35(4):71–1 2016.[11]MorganASchmitz MatthieuHeitz NicolasBonneel FredNgole DavidCoeurjolly MarcoCuturi GabrielPeyré andJean-LucStarck.Wassersteindictionarylearning:Optimaltransport-basedunsupervisednonlineardictionarylearning.SIAMJournalonImagingSciences 11(1):643–678 2018.[12]RémiFlamary MarcoCuturi NicolasCourty andAlainRakotomamonjy.Wassersteindiscriminantanalysis.MachineLearning May2018.[13]R.CominettiandJ.SanMartín.Asymptoticanalysisoftheexponentialpenaltytrajectoryinlinearprogramming.MathematicalProgramming 67(1):169–187 Oct1994.[14]J.Ye P.Wu J.Z.Wang andJ.Li.Fastdiscretedistributionclusteringusingwassersteinbarycenterwithsparsesupport.IEEETransactionsonSignalProcessing 65(9):2317–2332 May2017.[15]CarloCiliberto LorenzoRosasco andAlessandroRudi.Aconsistentregularizationapproachforstructuredprediction.InAdvancesinNeuralInformationProcessingSystems pages4412–4420.2016.[16]C.Villani.OptimalTransport:OldandNew.GrundlehrendermathematischenWissenschaften.SpringerBerlinHeidelberg 2008.[17]D.BertsimasandJ.Tsitsiklis.IntroductiontoLinearOptimization.AthenaScientiﬁc 1997.[18]RichardSinkhornandPaulKnopp.Concerningnonnegativematricesanddoublystochasticmatrices.PaciﬁcJ.Math. 21(2):343–348 1967.[19]JasonAltschuler JonathanWeed andPhilippeRigollet.Near-lineartimeapproximationalgorithmsforoptimaltransportviasinkhorniteration.InNIPS pages1961–1971 2017.10[20]MarcoCuturiandGabrielPeyré.Asmootheddualapproachforvariationalwassersteinproblems.SIAMJ.ImagingSciences 9(1):320–343 2016.[21]C.H.Edwards.AdvancedCalculusofSeveralVariables.DoverBooksonMathematics.DoverPublications 2012.[22]YoshuaBengio.Gradient-basedoptimizationofhyperparameters.Neuralcomputation 12(8) 2000.[23]OlivierChapelle VladimirVapnik OlivierBousquet andSayanMukherjee.Choosingmultipleparametersforsupportvectormachines.Machinelearning 46(1-3):131–159 2002.[24]FabianPedregosa.Hyperparameteroptimizationwithapproximategradient.arXivpreprintarXiv:1602.02355 2016.[25]GHBakir THofmann BSchölkopf AJSmola BTaskar andSVNVishwanathan.Predictingstructureddata.neuralinformationprocessing 2007.[26]CarloCiliberto AlessandroRudi LorenzoRosasco andMassimilianoPontil.Consistentmultitasklearningwithnonlinearoutputrelations.InAdvancesinNeuralInformationProcessingSystems 2017.[27]AntonOsokin FrancisBach andSimonLacoste-Julien.Onstructuredpredictiontheorywithcalibratedconvexsurrogatelosses.InAdvancesinNeuralInformationProcessingSystems pages302–313 2017.[28]AnnaKorba AlexandreGarcia andFlorenced’AlchéBuc.Astructuredpredictionapproachforlabelranking.arXivpreprintarXiv:1807.02374 2018.[29]AlessandroRudi CarloCiliberto GianMariaMarconi andLorenzoRosasco.Manifoldstructuredprediction.InAdvancesinNeuralInformationProcessingSystems31:AnnualConferenceonNeuralInformationProcessingSystems2018 NeurIPS2018 3-8December2018 Montréal Canada. pages5615–5626 2018.[30]NachmanAronszajn.Theoryofreproducingkernels.TransactionsoftheAmericanmathematicalsociety 68(3):337–404 1950.[31]AlexJSmolaandBernhardSchölkopf.Sparsegreedymatrixapproximationformachinelearning.2000.[32]AliRahimiandBenjaminRecht.Randomfeaturesforlarge-scalekernelmachines.InAdvancesinneuralinformationprocessingsystems pages1177–1184 2008.[33]AlessandroRudiandLorenzoRosasco.Generalizationpropertiesoflearningwithrandomfeatures.InAdvancesinNeuralInformationProcessingSystems pages3215–3225 2017.[34]AlessandroRudi LuigiCarratino andLorenzoRosasco.Falkon:Anoptimallargescalekernelmethod.InAdvancesinNeuralInformationProcessingSystems pages3888–3898 2017.[35]IngoSteinwartandAndreasChristmann.Supportvectormachines.SpringerScience&BusinessMedia 2008.[36]J.Feydy T.Séjourné F.-X.Vialard S.-i.Amari A.Trouvé andG.Peyré.InterpolatingbetweenOptimalTransportandMMDusingSinkhornDivergences.ArXive-prints October2018.[37]J.Weston O.Chapelle A.Elisseeff B.Schölkopf andV.Vapnik.Kerneldependencyestimation.InAdvancesinNeuralInformationProcessingSystems15 pages873–880 Cambridge MA USA October2003.Max-Planck-Gesellschaft MITPress.[38]IncGoogle.QuickDrawDataset.https://github.com/googlecreativelab/quickdraw-dataset.[39]T.KolloandD.vonRosen.AdvancedMultivariateStatisticswithMatrices.MathematicsandItsApplications.SpringerNetherlands 2006.[40]MiroslavFiedler.Boundsforeigenvaluesofdoublystochasticmatrices.LinearAlgebraanditsApplications 5(3):299–310 1972.[41]H.Brezis.FunctionalAnalysis SobolevSpacesandPartialDifferentialEquations.Universitext.SpringerNewYork 2010.[42]AlainBerlinetandChristineThomas-Agnan.ReproducingkernelHilbertspacesinprobabilityandstatistics.SpringerScience&BusinessMedia 2011.11[43]V.Moretti.SpectralTheoryandQuantumMechanics:WithanIntroductiontotheAlgebraicFormulation.UNITEXT.SpringerMilan 2013.[44]AndreaCaponnettoandErnestoDeVito.Optimalratesfortheregularizedleast-squaresalgorithm.FoundationsofComputationalMathematics 7(3):331–368 2007.[45]JunhongLin AlessandroRudi LorenzoRosasco andVolkanCevher.Optimalratesforspectralalgorithmswithleast-squaresregressionoverhilbertspaces.AppliedandComputationalHarmonicAnalysis 2018.12,Priya Donti
Brandon Amos
J. Zico Kolter
Giulia Luise
Alessandro Rudi
Massimiliano Pontil
Carlo Ciliberto