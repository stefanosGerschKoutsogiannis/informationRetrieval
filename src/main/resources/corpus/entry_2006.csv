2019,Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks,Natural gradient descent has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function  but little is known theoretically about its convergence properties  especially for \emph{non-linear} networks. In this work  we analyze for the first time the speed of convergence to global optimum for natural gradient descent on non-linear neural networks with the squared error loss. We identify two conditions which guarantee the global convergence: (1) the Jacobian matrix (of network's output for all training cases w.r.t the parameters) is full row rank and (2) the Jacobian matrix is stable for small perturbations around the initialization. For two-layer ReLU neural networks (i.e. with one hidden layer)  we prove that these two conditions do hold throughout the training under the assumptions that the inputs do not degenerate and the network is over-parameterized. We further extend our analysis to more general loss function with similar convergence property. Lastly  we show that K-FAC  an approximate natural gradient descent method  also converges to global minima under the same assumptions.,Fast Convergence of Natural Gradient Descent

for Overparameterized Neural Networks

Guodong Zhang1 2  James Martens3  Roger Grosse1 2
University of Toronto1  Vector Institute2  DeepMind3

{gdzhang  rgrosse}@cs.toronto.edu  jamesmartens@google.com

Abstract

Natural gradient descent has proven effective at mitigating the effects of patho-
logical curvature in neural network optimization  but little is known theoretically
about its convergence properties  especially for nonlinear networks. In this work 
we analyze for the ﬁrst time the speed of convergence of natural gradient descent
on nonlinear neural networks with squared-error loss. We identify two conditions
which guarantee efﬁcient convergence from random initializations: (1) the Jacobian
matrix (of network’s output for all training cases with respect to the parameters)
has full row rank  and (2) the Jacobian matrix is stable for small perturbations
around the initialization. For two-layer ReLU neural networks  we prove that these
two conditions do in fact hold throughout the training  under the assumptions of
nondegenerate inputs and overparameterization. We further extend our analysis
to more general loss functions. Lastly  we show that K-FAC  an approximate
natural gradient descent method  also converges to global minima under the same
assumptions  and we give a bound on the rate of this convergence.

1

Introduction

Because training large neural networks is costly  there has been much interest in using second-
order optimization to speed up training [Becker and LeCun  1989  Martens  2010  Martens and
Grosse  2015]  and in particlar natural gradient descent [Amari  1998  1997]. Recently  scalable
approximations to natural gradient descent have shown practical success in a variety of tasks and
architectures [Martens and Grosse  2015  Grosse and Martens  2016  Wu et al.  2017  Zhang et al. 
2018a  Martens et al.  2018]. Natural gradient descent has an appealing interpretation as optimizing
over a Riemannian manifold using an intrinsic distance metric; this implies the updates are invariant
to transformations such as whitening [Ollivier  2015  Luk and Grosse  2018]. It is also closely
connected to Gauss-Newton optimization  suggesting it should achieve fast convergence in certain
settings [Pascanu and Bengio  2013  Martens  2014  Botev et al.  2017].
Does this intuition translate into faster convergence? Amari [1998] provided arguments in the
afﬁrmative  as long as the cost function is well approximated by a convex quadratic. However  it
remains unknown whether natural gradient descent can optimize neural networks faster than gradient
descent — a major gap in our understanding. The problem is that the optimization of neural networks
is both nonconvex and non-smooth  making it difﬁcult to prove nontrivial convergence bounds. In
general  ﬁnding a global minimum of a general non-convex function is an NP-complete problem  and
neural network training in particular is NP-complete [Blum and Rivest  1992].
However  in the past two years  researchers have ﬁnally gained substantial traction in understanding
the dynamics of gradient-based optimization of neural networks. Theoretically  it has been shown
that gradient descent starting from a random initialization is able to ﬁnd a global minimum if the
network is wide enough [Li and Liang  2018  Du et al.  2018b a  Zou et al.  2018  Allen-Zhu et al. 
2018  Oymak and Soltanolkotabi  2019]. The key technique of those works is to show that neural

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

networks become well-behaved if they are largely overparameterized in the sense that the number of
hidden units is polynomially large in the size of the training data. However  most of these works have
focused on standard gradient descent  leaving open the question of whether similar statements can be
made about other optimizers.
Most convergence analysis of natural gradient descent has focused on simple convex quadratic
objectives (e.g. [Martens  2014]). Very recently  the convergence properties of NGD were studied in
the context of linear networks [Bernacchia et al.  2018]. While the linearity assumption simpliﬁes the
analysis of training dynamics [Saxe et al.  2013]  linear networks are severely limited in terms of their
expressivity  and it’s not clear which conclusions will generalize from linear to nonlinear networks.
In this work  we analyze natural gradient descent for nonlinear networks. We give two simple
and generic conditions on the Jacobian matrix which guarantee efﬁcient convergence to a global
minimum. We then apply this analysis to a particular distribution over two-layer ReLU networks
which has recently been used to analyze the convergence of gradient descent [Li and Liang  2018  Du
et al.  2018a  Oymak and Soltanolkotabi  2019]. We show that for sufﬁciently high network width 
NGD will converge to the global minimum. We give bounds on the convergence rate of two-layer
ReLU networks that are much better than the analogous bounds that have been proven for gradient
descent [Du et al.  2018b  Wu et al.  2019  Oymak and Soltanolkotabi  2019]  while allowing for
much higher learning rates. Moreover  in the limit of inﬁnite width  and assuming a squared error
loss  we show that NGD converges in just one iteration. The main contributions of our work are
summarized as follows:

• We provide the ﬁrst convergence result for natural gradient descent in training randomly-
initialized overparameterized neural networks where the number of hidden units is polyno-
mially larger than the number of training samples. We show that natural gradient descent
gives an O(min(G1)) improvement in convergence rate given the same learning rate as
gradient descent  where G1 is a Gram matrix that depends on the data.
• We show that natural gradient enables us to use a much larger step size  resulting in an even
faster convergence rate. Speciﬁcally  the maximal step size of natural gradient descent is
O (1) for (polynomially) wide networks.
• We show that K-FAC [Martens and Grosse  2015]  an approximate natural gradient descent
method  also converges to global minima with linear rate  although this result requires a
higher level of overparameterization compared to GD and exact NGD.

• We analyze the generalization properties of NGD  showing that the improved convergence

rates provably don’t come at the expense of worse generalization.

2 Related Works

Recently  there have been many works studying the optimization problem in deep learning  i.e.  why
in practice many neural network architectures reliably converge to global minima (zero training error).
One popular way to attack this problem is to analyze the underlying loss surface [Hardt and Ma 
2016  Kawaguchi  2016  Kawaguchi and Bengio  2018  Nguyen and Hein  2017  Soudry and Carmon 
2016]. The main argument of those works is that there are no bad local minima. It has been proven
that gradient descent can ﬁnd global minima [Ge et al.  2015  Lee et al.  2016] if the loss surface
satisﬁes: (1) all local minima are global and (2) all saddle points are strict in the sense that there
exists at least one negative curvature direction. Unfortunately  most of those works rely on unrealistic
assumptions (e.g.  linear activations [Hardt and Ma  2016  Kawaguchi  2016]) and cannot generalize
to practical neural networks. Moreover  Yun et al. [2018] shows that small nonlinearity in shallow
networks can create bad local minima.
Another way to understand the optimization of neural networks is to directly analyze the optimization
dynamics. Our work also falls within this category. However  most work in this direction focuses
on gradient descent. Bartlett et al.  Arora et al. [2019a] studied the optimization trajectory of deep
linear networks and showed that gradient descent can ﬁnd global minima under some assumptions.
Previously  the dynamics of linear networks have also been studied by Saxe et al. [2013]  Advani
and Saxe [2017]. For nonlinear neural networks  a series of papers [Tian  2017  Brutzkus and
Globerson  2017  Du et al.  2017  Li and Yuan  2017  Zhang et al.  2018b] studied a speciﬁc class of
shallow two-layer neural networks together with strong assumptions on input distribution as well
as realizability of labels  proving global convergence of gradient descent. Very recently  there are

2

some works proving global convergence of gradient descent [Li and Liang  2018  Du et al.  2018b a 
Allen-Zhu et al.  2018  Zou et al.  2018  Gao et al.  2019] or adaptive gradient methods [Wu et al. 
2019] on overparameterized neural networks. More speciﬁcally  Li and Liang [2018]  Allen-Zhu et al.
[2018]  Zou et al. [2018] analyzed the dynamics of weights and showed that the gradient cannot be
small if the objective value is large. On the other hand  Du et al. [2018b a]  Wu et al. [2019] studied
the dynamics of the outputs of neural networks  where the convergence properties are captured by a
Gram matrix. Our work is very similar to Du et al. [2018b]  Wu et al. [2019]. We note that these
papers all require the step size to be sufﬁciently small to guarantee the global convergence  leading to
slow convergence.
To our knowledge  there is only one paper [Bernacchia et al.  2018] studying the global convergence
of natural gradient for neural networks. However  Bernacchia et al. [2018] only studied deep linear
networks with inﬁnitesimal step size and squared error loss functions. In this sense  our work is the
ﬁrst one proving global convergence of natural gradient descent on nonlinear networks.
There have been many attempts to understand the generalization properties of neural networks
since Zhang et al. [2016]’s seminal paper. Researchers have proposed norm-based generalization
bounds [Neyshabur et al.  2015  2017  Bartlett and Mendelson  2002  Bartlett et al.  2017  Golowich
et al.  2017]  compression bounds [Arora et al.  2018] and PAC-Bayes bounds [Dziugaite and Roy 
2017  2018  Zou et al.  2018]. Recently  overparameterization of neural networks together with
good initialization has been believed to be one key factor of good generalization. Neyshabur et al.
[2019] empirically showed that wide neural networks stay close to the initialization  thus leading to
good generalization. Theoretically  researchers did prove that overparameterization as well as linear
convergence jointly restrict the weights to be close to the initialization [Du et al.  2018b a  Allen-Zhu
et al.  2018  Zou et al.  2018  Arora et al.  2019b]. The most closely related paper is Arora et al.
[2019b]  which shows that the optimization and generalization phenomenon can be explained by a
Gram matrix. The main difference is that our analysis is based on natural gradient descent  which
converges faster and provably generalizes as well as gradient descent.
Concurrently and independently  Cai et al. [2019] showed that natural gradient descent (they call it
Gram-Gauss-Newton) enjoys quadratic convergence rate guarantee for overparameterized networks
on regression problems. Additionally  they showed that it is much cheaper to precondition the gradient
in the output space when the number of data points is much smaller than the number of parameters.

3 Convergence Analysis of Natural Gradient Descent

We begin our convergence analysis of natural gradient descent – under appropriate conditions – for
the neural network optimization problem. Formally  we consider a generic neural network f (✓  x)
2 (u  y)2 for simplicity1  where ✓ 2 Rm
with a single output and squared error loss `(u  y) = 1
denots all parameters of the network (i.e. weights and biases). Given a training dataset {(xi  yi)}n
i=1 
we want to minimize the following loss function:

One main focus of this paper is to analyze the following procedure:

L(✓) =

1
n

1
2n

nXi=1

` (f (✓  xi)  yi) =

nXi=1
✓(k + 1) = ✓(k)  ⌘F(✓(k))1 @L(✓(k))

@✓(k)

(f (✓  xi)  yi)2 .

 

(1)

(2)

where ⌘> 0 is the step size  and F is the Fisher information matrix associated with the network’s
predictive distribution over y (which is implied by its loss function and is N (f (✓  xi)  1) for the
squared error loss) and the dataset’s distribution over x.
As shown by Martens [2014]  the Fisher F is equivalent to the generalized Gauss-Newton matrix 
deﬁned as Exi⇥J>i H`Ji⇤ if the predictive distribution is in the exponential family  such as categorical

distribution (for classiﬁcation) or Gaussian distribution (for regression). Ji is the Jacobian matrix
of ui with respect to the parameters ✓ and H` is the Hessian of the loss `(u  y) with respect to the
network prediction u (which is I in our setting). Therefore  with the squared error loss  the Fisher
1It is easy to extend to multi-output networks and other loss functions  here we focus on single-output and

quadratic just for notational simplicity.

3

matrix can be compactly written as F = E⇥J>i Ji⇤ = 1

n J>J (which coincides with classical Gauss-
Newton matrix)  where J = [J>1   ...  J>n ]> is the Jacobian matrix for the whole dataset. In practice 
when the number of parameters m is larger than number of samples n we have  the Fisher information
matrix F = 1
n J>J is surely singular. In that case  we take the generalized inverse [Bernacchia et al. 
2018] F† = nJ>G1G1J with G = JJ>  which gives the following update rule:

✓(k + 1) = ✓(k)  ⌘J>JJ>1

(u  y) 

where u = [u1  ...  un]> = [f (✓  x1)  ...  f (✓  xn)]> and y = [y1  ...  yn]>.
We now introduce two conditions on the network f✓ that sufﬁce for proving the global convergence
of NGD to a minimizer which achieves zero training loss (and is therefore a global minimizer).
To motivate these two conditions we make the following observations. First  the global minimizer
is characterized by the condition that the gradient in the output space is zero for each case (i.e.
ruL(✓) = 0). Meanwhile  local minima are characterized by the condition that the gradient with
respect to the parameters r✓L(✓) is zero. Thus  one way to avoid ﬁnding local minima that aren’t
global is to ensure that the parameter gradient is zero if and only if the output space gradient (for each
case) is zero. It’s not hard to see that this property holds as long as G remains non-singular throughout
optimization (or equivalently that J always has full row rank). The following two conditions ensure
that this happens  by ﬁrst requiring that this property hold at initialization time  and second that J
changes slowly enough that it remains true in a big enough neighborhood around ✓(0).
Condition 1 (Full row rank of Jacobian matrix). The Jacobian matrix J(0) at the initialization has
full row rank  or equivalently  the Gram matrix G(0) = J(0)J(0)> is positive deﬁnite.
Remark 1. Condition 1 implies that m  n  which means the Fisher information matrix is singular
and we have to use the generalized inverse except in the case where m = n.
Condition 2 (Stable Jacobian). There exists 0  C < 1
2 such that for all parameters ✓ that satisfy
k✓  ✓(0)k2  3kyu(0)k2
pmin(G(0))

  we have

kJ(✓)  J(0)k2 

C

3pmin(G(0)).

(3)

(4)

(5)

This condition shares the same spirit with the Lipschtiz smoothness assumption in classical optimiza-
tion theory. It implies (with small C) that the network is close to a linearized network [Lee et al. 
2019] around the initialization and therefore natural gradient descent update is close to the gradient
descent update in the output space. Along with Condition 1  we have the following theorem.
Theorem 1 (Natural gradient descent). Let Condition 1 and 2 hold. Suppose we optimize with NGD
using a step size ⌘  12C

(1+C)2 . Then for k = 0  1  2  ... we have

ku(k)  yk2

2  (1  ⌘)k ku(0)  yk2
2 .

2 is the squared error loss up to a constant. Due to space constraints we only

To be noted  ku(k)  yk2
give a short sketch of the proof here. The full proof is given in Appendix B.
Proof Sketch. Our proof relies on the following insights. First  if the Jacobian matrix has full row
rank  this guarantees linear convergence for inﬁnitesimal step size. The linear convergence property
restricts the parameters to be close to the initialization  which implies the Jacobian matrix is always
full row rank throughout the training  and therefore natural gradient descent with inﬁnitesimal step
size converges to global minima. Furthermore  given the network is close to a linearized network
(since the Jacobian matrix is stable with respect to small perturbations around the initialization)  we
are able to extend the proof to discrete time with a large step size.
In summary  we prove that NGD exhibits linear convergence to the global minimizer of the neural
network training problem  under Conditions 1 and 2. We believe our arguments in this section are
general (i.e.  architecture-agnostic)  and can serve as a recipe for proving global convergence of
natural gradient descent in other settings.

3.1 Other Loss Functions
We note that our analysis can be easily extended to more general loss function class. Here  we take
the class of functions that are µ-strongly convex with L-Lipschitz gradients as an example. Note that

4

strongly convexity is a very mild assumption since we can always add L2 regularization to make the
convex loss strongly convex. Therefore  this function class includes regularized cross-entropy loss
(which is typically used in classiﬁcation) and squared error (for regression). For this type of loss  we
need a strong version of Condition 2.
Condition 3 (Stable Jacobian). There exists 0  C < 1
k✓  ✓(0)k2  3(1+)kyu(0)k2
2pmin(G(0))

1+ such that for all parameters ✓ that satisfy

where  = L
µ

(6)
Theorem 2. Under Condition 1 and 3  but with µ-strongly convex loss function `(· ·) with L-Lipschitz
gradient ( = L

  then we have for k = 0  1  2  ...

kJ(✓)  J(0)k2 

C

3pmin(G(0)).

1(1+)C
(1+C)2

µ+L

µ )  and we set the step size ⌘  2
2 ✓1 

ku(k)  yk2

2⌘µL

µ + L◆k

ku(0)  yk2
2.

(7)

The key step of proving Theorem 2 is to show if m is large enough  then natural gradient descent is
approximately gradient descent in the output space. Thus the results can be easily derived according to
standard bounds for convex optimization. Due to the page limit  we defer the proof to the Appendix C.
Remark 2. In Theorem 2  the convergence rate depends on the condition number  = L
µ   which
can be removed if we take into the curvature information of the loss function. In other words  we
expect that the bound has no dependency on  if we use the Fisher matrix rather than the classical
Gauss-Newton (assuming Euclidean metric in the output space [Luk and Grosse  2018]) in Theorem 2.

4 Optimizing Overparameterized Neural Networks

In Section 3  we analyzed the convergence properties of natural gradient descent  under the abstract
Conditions 1 and 2. In this section  we make our analysis concrete by applying it to a speciﬁc type of
overparameterized network (with a certain random initialization). We show that Conditions 1 and 2
hold with high probability. We therefore establish that NGD exhibits linear convergence to a global
minimizer for such networks.

4.1 Notation
We let [m] = {1  2  ...  m}. We use ⌦   to denote the Kronecker and Hadamard products. And we
use ⇤ and ? to denote row-wise and column-wise Khatri-Rao products  respectively. For a matrix A 
we use Aij to denote its (i  j)-th entry. We use k·k 2 to denote the Euclidean norm of a vector or
spectral norm of a matrix and k·k F to denote the Frobenius norm of a matrix. We use max(A) and
min(A) to denote the largest and smallest eigenvalue of a square matrix  and max(A) and min(A)
to denote the largest and smallest singular value of a (possibly non-square) matrix. For a positive
deﬁnite matrix A  we use A to denote its condition number  i.e.  max(A)/min(A). We also use
h· ·i to denote the standard inner product between two vectors. Given an event E  we use I{E} to
denote the indicator function for E.

4.2 Problem Setup
Formally  we consider a neural network of the following form:

f (w  a  x) =

1
pm

mXr=1

ar(w>r x) 

(8)

where x 2 Rd is the input  w =⇥w>1   ...  w>r⇤> 2 Rmd is the weight matrix (formed into a vector)
of the ﬁrst layer  ar 2 R is the output weight of hidden unit r and (·) is the ReLU activation
function (acting entry-wise for vector arguments). For r 2 [m]  we initialize the weights of ﬁrst layer
wr ⇠N (0 ⌫ 2I) and output weight ar ⇠ unif [{1  +1}].
Following Du et al. [2018b]  Wu et al. [2019]  we make the following assumption on the data.

5

Figure 1: Visualization of natural gradient update and gradient descent update in the output space (for a
randomly initialized network). We take two classes (4 and 9) from MNIST [LeCun et al.  1998] and generate
the targets (denoted as star in the ﬁgure) by f (x) = x  0.5 + 0.3 ⇥N (0  I) where x 2 R2 is one-hot target.
We get natural gradient update by running 100 iterations of conjugate gradient [Martens  2010]. The ﬁrst row:
a MLP with two hidden layers and 100 hidden units in each layer. The second row: a MLP with two hidden
layers and 6000 hidden units in each layer. In both cases  ReLU activation function was used. We interpolate
the step size from 0 to 1. For the over-parameterized network (in the second row)  natural gradient descent
(implemented by conjugate gradient) matches output space gradient well.

Assumption 1. For all i  kxik2 = 1 and |yi| = O (1). For any i 6= j  xi   xj.
This very mild condition simply requires the inputs and outputs have standardized norms  and that
different input vectors are distinguishable from each other. Datasets that do not satisfy this condition
can be made to do so via simple pre-processing.
Following Du et al. [2018b]  Oymak and Soltanolkotabi [2019]  Wu et al. [2019]  we only optimize
the weights of the ﬁrst layer2  i.e.  ✓ = w. Therefore  natural gradient descent can be simpliﬁed to

w(k + 1) = w(k)  ⌘J>(JJ>)1(u  y).

(9)

Though this is only a shallow fully connected neural network  the objective is still non-smooth and
non-convex [Du et al.  2018b] due to the use of ReLU activation function. We further note that this
two-layer network model has been useful in understanding the optimization and generalization of
deep neural networks [Xie et al.  2016  Li and Liang  2018  Du et al.  2018b  Arora et al.  2019b  Wu
et al.  2019]  and some results have been extended to multi-layer networks [Du et al.  2018a].
Following Du et al. [2018b]  Wu et al. [2019]  we deﬁne the limiting Gram matrix as follows:
Deﬁnition 1 (Limiting Gram Matrix). The limiting Gram matrix G1 2 Rn⇥n is deﬁned as follows.
For (i  j)- entry  we have

G1ij = Ew⇠N (0 ⌫2I)⇥x>i xjIw>xi  0  w>xj  0 ⇤ = x>i xj

⇡  arccos(x>i xj)

2⇡

.

(10)

This matrix coincides with neural tangent kernel [Jacot et al.  2018] for ReLU activation function.
As shown by Du et al. [2018b]  this matrix is positive deﬁnite and we deﬁne its smallest eigenvalue
0   min(G1) > 0. In the same way  we can deﬁne its ﬁnite version G(t) = J(t)J(t)> with
(i  j)-entry Gij(t) = 1

m x>i xjPr2[m] Iwr(t)>xi  0  wr(t)>xj  0 .

4.3 Exact Natural Gradient Descent

In this subsection  we present our result for this setting. The main difﬁculty is to show that Conditions 1
and 2 hold. Here we state our main result.
Theorem 3 (Natural Gradient Descent for overparameterized Networks). Under Assumption 1  if
we i.i.d initialize wr ⇠N (0 ⌫ 2I)  ar ⇠ unif[{1  +1}] for r 2 [m]  we set the number of hidden
2We ﬁx the second layer just for simplicity. Based on the same analysis  one can also prove global convergence

for jointly training both layers.

6

−2−1012−2−1012−2−1012−2−1012−2−1012−2−1012−2−1012nodes m =⌦ ⇣ n4

⌫24

03⌘  and the step size ⌘ = O(1)  then with probability at least 1   over the

random initialization we have for k = 0  1  2  ...

ku(k)  yk2

2  (1  ⌘)k ku(0)  yk2
2.

(11)

Even though the objective is non-convex and non-smooth  natural gradient descent with a constant
step size enjoys a linear convergence rate. For large enough m  we show that the learning rate can be
chosen up to 1  so NGD can provably converge within O (1) steps. Compared to analogous bounds
for gradient descent [Du et al.  2018a  Oymak and Soltanolkotabi  2019  Wu et al.  2019]  we improve
the maximum allowable learning rate from O(1/n) to O(1) and also get rid of the dependency on 0.
Overall  NGD (Theorem 3) gives an O(0/n) improvement over gradient descent.
Our strategy to prove this result will be to show that for the given choice of random initialization 
Condition 1 and 2 hold with high probability. For proving Condition 1 hold  we used matrix
concentration inequalities. For Condition 2  we show that kJ J(0)k2 = Om1/6  which implies

the Jacobian is stable for wide networks. For detailed proof  we refer the reader to the Appendix D.1.

4.4 Approximate Natural Gradient Descent with K-FAC
Exact natural gradient descent is quite expensive in terms of computation or memory. In training deep
neural networks  K-FAC [Martens and Grosse  2015] has been a powerful optimizer for leveraging
curvature information while retaining tractable computation. The K-FAC update rule for the two-layer
ReLU network is given by

w(k + 1) = w(k)  ⌘⇥(X>X)1 ⌦ (S(k)>S(k))1⇤
}

{z

KFAC

|

F1

J(k)>(u(k)  y).

(12)

where X 2 Rn⇥d denotes the matrix formed from the n input vectors (i.e. X = [x1  ...  xn]>)  and
S = [0(Xw1)  ...  0(Xwm)] 2 Rn⇥m is the matrix of pre-activation derivatives. Under the same
argument as the Gram matrix G1  we get that S1S1> is strictly positive deﬁnite with smallest
eigenvalue S (see Appendix D.3 for detailed proof).
We show that for sufﬁciently wide networks  K-FAC does converge linearly to a global minimizer. We
further show  with a particular transformation on the input data  K-FAC does match the optimization
performance of exact natural gradient for two-layer ReLU networks. Here we state the main result.
Theorem 4 (K-FAC). Under the same assumptions as in Theorem 3  plus the additional assumption

that rank(X) = d  if we set the number of hidden units m = O✓
3◆ and step size
⌘ = OminX>X  then with probability at least 1   over the random initialization  we have

for k = 0  1  2  ...

n4
S4
X>X

⌫24

ku(k)  yk2

2 ✓1 

⌘

max(X>X)◆k

ku(0)  yk2
2.

(13)

The key step in proving Theorem 4 is to show

u(k + 1)  u(k) ⇡⇥X(X>X)1X>  I⇤ (y  u(k)) .

(14)
Remark 3. The convergence rate of K-FAC is captured by the condition number of the matrix X>X 
as opposed to gradient descent [Du et al.  2018b  Oymak and Soltanolkotabi  2019]  for which the
convergence rate is determined by the condition number of the Gram matrix G.
Remark 4. The dependence of the convergence rate on X>X in Theorem 4 may seem paradoxical 
as K-FAC is invariant to invertible linear transformations of the data (including those that would
change X>X). But we note that said transformations would also make the norms of the input vectors
non-uniform  thus violating Assumption 1 in a way that isn’t repairable. Interestingly  there exists an
invertible linear transformation which  if applied to the input vectors and followed by normalization 
produces vectors that simultaneously satisfy Assumption 1 and the condition X>X = 1 (thus
improving the bound in Theorem 4 substantially). See Appendix A for details. Notably  K-FAC is not
invariant to such pre-processing  as the normalization step is a nonlinear operation.

7

To quantify the degree of overparameterization (which is a function of the network width m) required
to achieve global convergence under our analysis  we must estimate S. To this end  we observe that
G = XX>  SS>  and then apply the following lemma:
Lemma 1. [Schur [1911]] For two positive deﬁnite matrices A and B  we have

max (A  B)  max
min (A  B)  min

i

i

Aiimax(B)

Aiimin(B)

(15)

The diagonal entries of XX> are all 1 since the inputs are normalized. Therefore  we have 0  S
according to Lemma 1  and hence K-FAC requires a slightly higher degree of overparameterization
than exact NGD under our analysis.

4.5 Bounding 0
As pointed out by Allen-Zhu et al. [2018]  it is unclear if 1/0 is small or even polynomial. Here 
we bound  using matrix concentration inequalities and harmonic analysis. To leverage harmonic
analysis  we have to assume the data xi are drawn i.i.d. from the unit sphere3.
Theorem 5. Under this assumption on the training data  with probability 1  n exp(n/4) 

0   min(G1)  n/2  where  2 (0  0.5)

(16)

Basically  Theorem 5 says that the Gram matrix G1 should have high chance of having large smallest
eigenvalue if the training data are uniformly distributed. Intuitively  we would expect the smallest
eigenvalue to be very small if all xi are similar to each other. Therefore  some notion of diversity of
the training inputs is needed. We conjecture that the smallest eigenvalue would still be large if the
data are -separable (i.e.  kxi  xjk2   for any pair i  j 2 [n])  an assumption adopted by Li and
Liang [2018]  Allen-Zhu et al. [2018]  Zou et al. [2018].

5 Generalization analysis

It is often speculated that NGD or other preconditioned gradient descent methods (e.g.  Adam)
perform worse than gradient descent in terms of generalization [Wilson et al.  2017]. In this section 
we show that NGD achieves the same generalization bounds which have been proved for GD  at least
for two-layer ReLU networks.
Consider a loss function ` : R ⇥ R ! R. The expected risk over the data distribution D and the
empirical risk over a training set S = {(xi  yi)}n

LD(f ) = E(x y)⇠D [`(f (x)  y)] and LS(f ) =

`(f (xi)  yi)

(17)

i=1 are deﬁned as
1
n

nXi=1

It has been shown [Neyshabur et al.  2019] that the Redemacher complexity [Bartlett and Mendel-
son  2002] for two-layer ReLU networks depends on kw  w(0)k2. By the standard Rademacher
complexity generalization bound  we have the following bound (see Appendix E.1 for proof):
Theorem 6. Given a target error parameter ✏> 0 and failure probability  2 (0  1). Suppose
0   1 ✏ 1. For any 1-Lipschitz loss function  with
⌫ = O✏p0 and m  ⌫2polyn  1
probability at least 1   over random initialization and training samples  the two-layer neural
network f (w  a) trained by NGD for k  ⌦⇣ 1
✏⌘ iterations has expected loss LD(f (w  a)) =

E(x y)⇠D [`(f (w  a  x)  y)] bounded as:

⌘ log 1

LD(f (w  a)) r 2y>(G1)1y

n

+ 3r log(6/)

2n

+ ✏

(18)

which matches the bound for gradient descent in Arora et al. [2019b]. For detailed proof  we refer the
reader to the Appendix E.1.

3This assumption is not too stringent since the inputs are already normalized. Moreover  we can relax the
assumption of unit sphere input to separable input  which is used in Li and Liang [2018]  Allen-Zhu et al. [2018] 
Zou et al. [2018]. See Oymak and Soltanolkotabi [2019] (Theorem I.1) for more details.

8

6 Conclusion

We’ve analyzed for the ﬁrst time the rate of convergence to a global optimum for (both exact and
approximate) natural gradient descent on nonlinear neural networks. Particularly  we identiﬁed two
conditions which guarantee the global convergence  i.e.  the Jacobian matrix with respect to the
parameters has full row rank and stable for perturbations around the initialization. Based on these
insights  we improved the convergence rate of gradient descent by a factor of O(0/n) on two-layer
ReLU networks by using natural gradient descent. Beyond that  we also showed that the improved
convergence rates don’t come at the expense of worse generalization.

Acknowledgements

We thank Jeffrey Z. HaoChen  Shengyang Sun and Mufan Li for helpful discussion. RG acknowledges
support from the CIFAR Canadian AI Chairs program and the Ontario MRIS Early Researcher Award.

References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural

networks. arXiv preprint arXiv:1710.03667  2017.

Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via over-

parameterization. arXiv preprint arXiv:1811.03962  2018.

Shun-ichi Amari. Neural learning in structured parameter spaces-natural riemannian gradient. In

Advances in neural information processing systems  pages 127–133  1997.

Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation  10(2):251–276 

1998.

Sanjeev Arora  Rong Ge  Behnam Neyshabur  and Yi Zhang. Stronger generalization bounds for

deep nets via a compression approach. arXiv preprint arXiv:1802.05296  2018.

Sanjeev Arora  Nadav Cohen  Noah Golowich  and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In International Conference on Learning Representations 
2019a. URL https://openreview.net/forum?id=SkMQg3C5K7.

Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584  2019b.

Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

Peter L Bartlett  David P Helmbold  and Philip M Long. Gradient descent with identity initialization
efﬁciently learns positive-deﬁnite linear transformations by deep residual networks. Neural
computation  pages 1–26.

Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems  pages 6240–6249  2017.
S. Becker and Y. LeCun. Improving the convergence of backpropagation learning with second order

methods. In Proceedings of the 1988 Connectionist Models Summer School  1989.

Alberto Bernacchia  Mate Lengyel  and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. In Advances in Neural Information Processing
Systems  pages 5945–5954  2018.

Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural

Networks  5:117–127  1992.

Aleksandar Botev  Hippolyt Ritter  and David Barber. Practical Gauss-Newton optimisation for deep

learning. In International Conference on Machine Learning  2017.

9

Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press  2004.

Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 605–614. JMLR. org  2017.

Tianle Cai  Ruiqi Gao  Jikai Hou  Siyu Chen  Dong Wang  Di He  Zhihua Zhang  and Liwei Wang.
A gram-gauss-newton method learning overparameterized deep neural networks for regression
problems. arXiv preprint arXiv:1905.11675  2019.

Simon S Du  Jason D Lee  and Yuandong Tian. When is a convolutional ﬁlter easy to learn? arXiv

preprint arXiv:1709.06129  2017.

Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds global

minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018a.

Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054  2018b.

Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008  2017.

Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential

privacy. In Advances in Neural Information Processing Systems  pages 8440–8450  2018.

Jürgen Forster. A linear lower bound on the unbounded error probabilistic communication complexity.

Journal of Computer and System Sciences  65(4):612–625  2002.

Weihao Gao  Ashok Makkuva  Sewoong Oh  and Pramod Viswanath. Learning one-hidden-layer
In The 22nd International Conference on

neural networks under general input distributions.
Artiﬁcial Intelligence and Statistics  pages 1950–1959  2019.

Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online stochastic

gradient for tensor decomposition. In Conference on Learning Theory  pages 797–842  2015.

Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity of

neural networks. arXiv preprint arXiv:1712.06541  2017.

Roger Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution

layers. In International Conference on Machine Learning  2016.

Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231 

2016.

Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems  pages
8580–8589  2018.

Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information

processing systems  pages 586–594  2016.

Kenji Kawaguchi and Yoshua Bengio. Depth with nonlinearity creates no bad local minima in resnets.

arXiv preprint arXiv:1810.09038  2018.

Yann LeCun  Léon Bottou  Yoshua Bengio  Patrick Haffner  et al. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

Jaehoon Lee  Lechao Xiao  Samuel S Schoenholz  Yasaman Bahri  Jascha Sohl-Dickstein  and Jeffrey
Pennington. Wide neural network of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720  2019.

Jason D Lee  Max Simchowitz  Michael I Jordan  and Benjamin Recht. Gradient descent converges

to minimizers. arXiv preprint arXiv:1602.04915  2016.

10

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems  pages 8168–
8177  2018.

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.

In Advances in Neural Information Processing Systems  pages 597–607  2017.

Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. arXiv

preprint arXiv:1808.1340  2018.

James Martens. Deep learning via hessian-free optimization. In ICML  volume 27  pages 735–742 

2010.

James Martens. New insights and perspectives on the natural gradient method. arXiv preprint

arXiv:1412.1193  2014.

James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate

curvature. In International conference on machine learning  pages 2408–2417  2015.

James Martens  Jimmy Ba  and Matt Johnson. Kronecker-factored curvature approximations for

recurrent neural networks. In International Conference on Learning Representations  2018.

Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learning.

2018.

Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. Norm-based capacity control in neural

networks. In Conference on Learning Theory  pages 1376–1401  2015.

Behnam Neyshabur  Srinadh Bhojanapalli  and Nathan Srebro. A pac-bayesian approach to spectrally-

normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564  2017.

Behnam Neyshabur  Zhiyuan Li  Srinadh Bhojanapalli  Yann LeCun  and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations  2019. URL https://openreview.net/forum?id=BygfghAcYX.

Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70  pages 2603–2612. JMLR.
org  2017.

Yann Ollivier. Riemannian metrics for neural networks I: Feedforward networks. Information and

Inference  4:108–153  2015.

Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674 
2019.

Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint

arXiv:1301.3584  2013.

Andrew M Saxe  James L McClelland  and Surya Ganguli. Exact solutions to the nonlinear dynamics

of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120  2013.

Jssai Schur. Bemerkungen zur theorie der beschränkten bilinearformen mit unendlich vielen verän-

derlichen. Journal für die reine und Angewandte Mathematik  140:1–28  1911.

Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70  pages 3404–3413. JMLR. org  2017.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends R

in Machine Learning  8(1-2):1–230  2015.

11

Ashia C Wilson  Rebecca Roelofs  Mitchell Stern  Nati Srebro  and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems  pages 4148–4158  2017.

Xiaoxia Wu  Simon S Du  and Rachel Ward. Global convergence of adaptive gradient methods for an

over-parameterized neural network. arXiv preprint arXiv:1902.07111  2019.

Yuhuai Wu  Elman Mansimov  Shun Liao  Roger Grosse  and Jimmy Ba. Scalable trust-region method
for deep reinforcement learning using Kronecker-factored approximation. In Neural Information
Processing Systems  2017.

Bo Xie  Yingyu Liang  and Le Song. Diverse neural network learns true target functions. arXiv

preprint arXiv:1611.03131  2016.

Chulhee Yun  Suvrit Sra  and Ali Jadbabaie. Small nonlinearities in activation functions create bad

local minima in neural networks. arXiv preprint arXiv:1802.03487  2018.

Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

Guodong Zhang  Shengyang Sun  David Duvenaud  and Roger Grosse. Noisy natural gradient as
variational inference. In International Conference on Machine Learning  pages 5847–5856  2018a.
Xiao Zhang  Yaodong Yu  Lingxiao Wang  and Quanquan Gu. Learning one-hidden-layer relu

networks via gradient descent. arXiv preprint arXiv:1806.07808  2018b.

Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent optimizes

over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888  2018.

12

,Guodong Zhang
James Martens
Roger Grosse