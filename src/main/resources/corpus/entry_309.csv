2017,ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization,Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous  problems in machine learning  statistics and signal processing. However  it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization  they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper  we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function  which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side  given the local sharpness characterized by an exponent $\theta\in(0  1]$   we show that the proposed ADMM enjoys an improved iteration complexity of $\widetilde O(1/\epsilon^{1-\theta})$\footnote{$\widetilde O()$ suppresses a logarithmic factor.} in the deterministic setting and an iteration complexity of $\widetilde O(1/\epsilon^{2(1-\theta)})$ in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side  we demonstrate that the proposed algorithms converge comparably to  if not much faster than  ADMM with a fine-tuned fixed penalty parameter.,ADMM without a Fixed Penalty Parameter:

Faster Convergence with New Adaptive Penalization

Yi Xu†  Mingrui Liu†  Qihang Lin‡  Tianbao Yang†

†Department of Computer Science  The University of Iowa  Iowa City  IA 52242  USA
‡Department of Management Sciences  The University of Iowa  Iowa City  IA 52242  USA

{yi-xu  mingrui-liu  qihang-lin  tianbao-yang}@uiowa.edu

Abstract

Alternating direction method of multipliers (ADMM) has received tremendous
interest for solving numerous problems in machine learning  statistics and signal
processing. However  it is known that the performance of ADMM and many of its
variants is very sensitive to the penalty parameter of a quadratic penalty applied
to the equality constraints. Although several approaches have been proposed for
dynamically changing this parameter during the course of optimization  they do not
yield theoretical improvement in the convergence rate and are not directly applica-
ble to stochastic ADMM. In this paper  we develop a new ADMM and its linearized
variant with a new adaptive scheme to update the penalty parameter. Our methods
can be applied under both deterministic and stochastic optimization settings for
structured non-smooth objective function. The novelty of the proposed scheme lies
at that it is adaptive to a local sharpness property of the objective function  which
marks the key difference from previous adaptive scheme that adjusts the penalty
parameter per-iteration based on certain conditions on iterates. On theoretical side 
given the local sharpness characterized by an exponent θ ∈ (0  1]  we show that the

proposed ADMM enjoys an improved iteration complexity of (cid:101)O(1/1−θ)1 in the
deterministic setting and an iteration complexity of (cid:101)O(1/2(1−θ)) in the stochastic

setting without smoothness and strong convexity assumptions. The complexity in
either setting improves that of the standard ADMM which only uses a ﬁxed penalty
parameter. On the practical side  we demonstrate that the proposed algorithms
converge comparably to  if not much faster than  ADMM with a ﬁne-tuned ﬁxed
penalty parameter.

1

Introduction

Our problem of interest is the following convex optimization problem that commonly arises in
machine learning  statistics and signal processing:

F (x) (cid:44) f (x) + ψ(Ax)

min
x∈Ω

(1)
where Ω ⊆ Rd is a closed convex set  f : Rd → R and ψ : Rm → R are proper lower-semicontinuous
convex functions  and A ∈ Rm×d is a matrix. In this paper  we consider solving (1) by alternating
direction method of multipliers (ADMM) in two paradigms  namely deterministic optimization and
stochastic optimization. In both paradigms  ADMM has been employed widely for solving the
regularized statistical learning problems like (1) due to its capability of tackling the sophisticated
structured regularization term ψ(Ax) in (1) (e.g.  the generalized lasso (cid:107)Ax(cid:107)1)  which is often an
1(cid:101)O() suppresses a logarithmic factor.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

obstacle for applying other methods such as proximal gradient method. As follows  we describe
the standard ADMM and its variants for solving (1) in different optimization paradigms. It is worth
mentioning that all algorithms presented in this paper can be easily extended to handle a more general
term ψ(A(x) + c)  where A is a linear mapping.
To apply ADMM  the original problem (1) is ﬁrst cast into an equivalent constrained optimization
problem via decoupling:

min

x∈Ω y∈Rm

f (x) + ψ(y) 

s.t. y = Ax.

(2)

An augmented Lagrangian function for (2) is deﬁned as

L(x  y  λ) = f (x) + ψ(y) − λ(cid:62)(Ax − y) +

(3)
where β is a constant called penalty parameter and λ ∈ Rm is a dual variable. Then  the standard
ADMM solves problem (1) by executing the following three steps in each iteration:

(cid:107)Ax − y(cid:107)2
2 

β
2

xτ +1 = arg min
x∈Ω

L(x  yτ   λτ ) = arg min
x∈Ω

f (x) +

β
2

(cid:13)(cid:13)(cid:13)(cid:13)(Ax − yτ ) − 1
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(Axτ +1 − y) − 1

β
2

λτ

β

β

2

 

(cid:13)(cid:13)(cid:13)(cid:13)2

2

 

λτ

(4)

(5)

yτ +1 = arg min
x∈Ω
λτ +1 = λτ − β(Axτ +1 − yτ +1).

L(xτ +1  y  λτ ) = arg min
y∈Rm

ψ(y) +

(6)
When A is not an identity matrix  solving the subproblem (4) above for xτ +1 might be difﬁcult. To
alleviate the issue  linearized ADMM [33  34  8] has been proposed  which solves the following
problem instead of (4):

(cid:13)(cid:13)(cid:13)(cid:13)(Ax − yτ ) − 1

β

(cid:13)(cid:13)(cid:13)(cid:13)2

2

λτ

+

1
2

(cid:107)x − xτ(cid:107)2
G 

(7)

xτ +1 = arg min
x∈Ω
√

f (x) +

β
2

where (cid:107)x(cid:107)G =
x(cid:62)Gx and G ∈ Rd×d is a positive semi-deﬁnite matrix. By setting G =
γI − βA(cid:62)A (cid:23) 0  the term x(cid:62)A(cid:62)Ax in (7) vanishes. It has been established that both standard
ADMM and linearized ADMM have an O(1/t) convergence rate for solving (2) [8]   where t is the
number of iterations. Under a minor condition  this result implies an O(1/) iteration complexity for
solving the original problem (1) (see Corollary 1).
In addition  we consider ADMM for solving (1) in stochastic optimization with

(cid:80)n

f (x) = Eξ[f (x; ξ)]

(8)
where ξ is a random variable. This formulation captures many risk minimization problems in
machine learning where ξ denotes a data point sampled from a distribution and f (x; ξ) denotes a
loss function of the model x on the data ξ. It also covers as a special case the empirical loss where
i=1 f (x; ξi) with n is the number of samples. For these problems  computing f (x)
f (x) = 1
n
itself might be prohibitive (e.g.  when n is very large) or even impossible. To address this issue 
one usually considers the stochastic optimization paradigm  where it is assumed that f (x; ξ) and
its subgradient ∂f (x; ξ) can be efﬁciently computed. To solve the stochastic optimization problem 
stochastic ADMM algorithms have been proposed [21  23]  which update yτ +1 and λτ +1 the same
to (5) and (6)  respectively  but update xτ +1 as

λτ

β
2

f (xτ ; ξτ )+∂f (xτ ; ξτ )(cid:62)(x−xτ )+

(9)
xτ +1 = arg min
x∈Ω
where ξτ is a random sample  ητ is a stepsize and Gτ = γI − βητ A(cid:62)A (cid:23) I [23] or Gτ = I [21].
Other stochastic variants of ADMM for general convex optimization were also proposed in [23  35].
These work have established an O(1/
t) convergence rate of stochastic ADMM for solving (2) with
f (x) being (8). Under a minor condition  we can also show that these stochastic ADMM algorithms
suffer from a higher iteration complexity of O(1/2) for ﬁnding an -optimal solution to the original
problem (1) (see Corollary 3).
Although the variants of ADMM with fast convergence rates have been developed under smoothness 
strong convexity and other regularity conditions (e.g.  the matrix A has full rank)  the best iteration

√

ητ

+

Gτ

2

(cid:13)(cid:13)(cid:13)(cid:13)(Ax − yτ ) − 1

β

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:107)x − xτ(cid:107)2

2

complexities of deterministic ADMM and stochastic ADMM for general convex optimization remain
O(1/) and O(1/2)  respectively. On the other hand  many studies have reported that the perfor-
mance of ADMM is very sensitive to the penalty parameter β. How to address or alleviate this issue
has attracted many studies and remains an active topic. In particular  it remains an open question
how to quantify the improvement in ADMM’s theoretical convergence by using adaptive penalty
parameters. Of course  the answer to this question depends on the adaptive scheme being used.
Almost all previous works focus on using self-adaptive schemes that update the penalty parameter
during the course of optimization according to the historical iterates (e.g.  by balancing the primal
residual and dual residual). However  there is hitherto no quantiﬁable improvement in terms of
convergence rate or iteration complexity for these self-adaptive schemes.
In this paper  we focus on the design of adaptive penalization for both deterministic and stochastic
ADMM and show that  with the proposed adaptive updating scheme on the penalty parameter  the
theoretical convergence properties of ADMM can be improved without imposing any smoothness and
strong convexity assumptions on the objective function. The key difference between the proposed
adaptive scheme and previous self-adaptive schemes is that the proposed penalty parameter is adaptive
to an local sharpness property of the objective function  namely the local error bound (see Deﬁnition 1).
Given the exponent constant θ ∈ (0  1] that characterizes this local sharpness property  we show that

the proposed deterministic ADMM enjoys an improved iteration complexity of (cid:101)O(1/1−θ)2 and the
proposed stochastic ADMM enjoys an iteration complexity of (cid:101)O(1/2(1−θ))  both of which improve

the complexity of their standard counterparts which only use a ﬁxed penalty parameter. To the best of
our knowledge  this is the ﬁrst evidence that an adaptive penalty parameter used in ADMM can lead
to provably lower iteration complexities. We call the proposed ADMM algorithms locally adaptive
ADMM because of its adaptivity to the problem’s local property.

2 Related Work

(cid:80)n

t) for general convex problems and (cid:101)O(1/t) for strongly convex problems.

Since there is a tremendous amount of studies on ADMM  the review below mainly focuses on the
ADMMs with a variable penalty parameter. A convergence rate of O(1/t) was ﬁrst shown for both the
standard and linearized variants of ADMM [8  19  9] on general non-smooth and non-strongly convex
problems. Later  smoothness and strong convexity assumptions are introduced to develop faster
√
convergence rates of ADMMs [22  3  11  6]. Stochastic ADMM was considered in [21  23] with a
convergence rate of O(1/
Recently  many variance reduction techniques have been borrowed into stochastic ADMM to achieve
improved convergence rates for ﬁnite-sum optimization problems where f (x) = 1
i=1 fi(x)
n
under the smoothness and strong convexity assumptions [37  36  24]. Nevertheless  most of these
aforementioned works focus on using a constant penalty parameter.
He et al. [10] analyzed ADMM with self-adaptive penalty parameters. The motivation for their
self-adaptive penalty is to balance the order of the primal residual and the dual residual. However 
the convergence of ADMM with self-adaptive penalty is not guaranteed unless the adaptive scheme
is turned off after a number of iterations. Additionally  their self-adaptive rule requires computing
the deterministic subgradient of f (x) so that is not appropriate for stochastic optimization. Tian
& Yuan [25] proposed a variant of ADMM with variable penalty parameters. Their analysis and
algorithm require the smoothness assumption of ψ(Ax) and full column rank of the A matrix. Zhou
et al. [15] focused on solving low-rank representation by linearized ADMM and also proposed a
non-decreasing self-adaptive penalty scheme. However  their scheme is only applicable to an equality
constraint Ax + By = c with c (cid:54)= 0. Recently  Xu et al. [31] proposed a self-adaptive penalty
scheme for ADMM based on the Barzilai and Borwein gradient methods. The convergence of their
ADMM relies on the analysis in He et al. [10] and thus requires the penalty parameter to be ﬁxed after
a number of iterations. In contrast  our adaptive scheme fpr the penalty parameter is different from
the previous methods in the following aspects: (i) it is adaptive to the local sharpness property of the
problem; (ii) it allows the penalty parameter to increase to inﬁnity as the algorithm proceeds; (iii) it
can be employed for both deterministic and stochastic ADMMs as well as their linearized versions.
It is also notable that the presented algorithms and their convergence theory share many similarities
with the recent developments leveraging the local error bound condition [32  30  29]  where similar
iteration complexities have been established. However  we would like to emphasize that the newly

2(cid:101)O() suppresses a logarithmic factor.

3

proposed ADMM algorithms are more effective to tackle problems with structured regularizers (e.g. 
generalized lasso) than the methods in [32  30  29]  and have an additional unique feature of using
adaptive penalty parameter.

3 Preliminaries

Recall that the problem of our interest:

F (x) (cid:44) f (x) + ψ(Ax) 

min
x∈Ω

(10)
where Ω ⊆ Rd is a closed convex set  f : Rd → (−∞  +∞] and ψ : Rm → (−∞  +∞] are proper
lower-semicontinuous convex functions  and A ∈ Rm×d is a matrix. Let Ω∗ and F∗ denote the
optimal set of (10) and the optimal value  respectively. We present some assumptions that will be
used in the paper.
Assumption 1. For the convex optimization problem (10)  we assume (a) there exist known x0 ∈ Ω
and 0 ≥ 0 such that F (x0) − F∗ ≤ 0; (b) Ω∗ is a non-empty convex compact set; (c) there exists a
constant ρ such that (cid:107)∂ψ(y)(cid:107)2 ≤ ρ for all y; (d) ψ is deﬁned everywhere.
For a positive semi-deﬁnite matrix G  the G-norm is deﬁned as (cid:107)x(cid:107)G =
x(cid:62)Gx. Let B(x  r) =
{u ∈ Rd : (cid:107)u − x(cid:107)2 ≤ r} denote the Euclidean ball centered x with a radius r. We denote by
dist(x  Ω∗) the distance between x and the set Ω∗  i.e.  dist(x  Ω∗) = minv∈Ω∗ (cid:107)x − v(cid:107)2. We
denote by S the -sublevel set of F (x)  respectively  i.e.  S = {x ∈ Ω : F (x) ≤ F∗ + }.
Local Sharpness. Below  we introduce a condition  namely local error bound condition  to character-
ize the local sharpness property of the objective function.
Deﬁnition 1 (Local error bound (LEB)). A function F (x) is said to satisfy a local error bound
condition on the -sublevel set if there exist θ ∈ (0  1] and c > 0 such that for any x ∈ S

√

dist(x  Ω∗) ≤ c(F (x) − F∗)θ.

(11)

Remark: We will refer to θ as the local sharpness parameter. A recent study [1] has shown that the
local error bound condition is equivalent to the famous Kurdyka - Łojasiewicz (KL) property [13] 
which characterizes that under a transformation of ψ(s) = csθ  the function F (x) can be made sharp
around the optimal solutions  i.e  the norm of subgradient of the transformed function ψ(F (x) − F∗)
is lowered bounded by a constant 1. Note that by allowing θ = 0 in the above condition we can
capture a full spectrum of functions. However  a broad family of functions can have a sharper upper
bound  i.e.  with a non-zero constant θ in the above condition. For example  for functions that are
semi-algebraic and continuous  the above inequality is known to hold on any compact set (c.f. [1] and
references therein). The value of θ has been revealed for many functions (c.f. [18  14  20  1  32]).

4 Locally Adaptive ADMM for Deterministic Optimization

Since the proposed locally adaptive ADMM algorithm builds upon the standard ADMM  we ﬁrst
present the detailed steps of ADMM in Algorithm 1. Note that if we set G = 0 ∈ Rd×d  it gives
the standard ADMM; and if we use G = γI − βA(cid:62)A (cid:23) 0  it gives the linearized variant  which
can make the computation of xτ +1 easier. To ensure G (cid:23) 0  the minimum valid value for γ in the
linearized variant is β(cid:107)A(cid:107)2
2. To present the convergence result of ADMM (Algorithm 1)  we ﬁrst
introduce some notations.

u =

(cid:32) x
(cid:33)
t(cid:88)
(cid:98)ut =

y
λ

1
t

τ =1

uτ  

  F(u) =

 −A(cid:62)λ
t(cid:88)

Ax − y

λ

xτ  

  
(cid:98)yt =

(cid:98)xt =

1
t

τ =1

t(cid:88)

τ =1

1
t

yτ  

(cid:98)λt =

t(cid:88)

τ =1

1
t

λτ .

We recall the convergence result of [8] for the equality constrained problem (2)  which does not
assume any smoothness  strong convexity and other regularity conditions.

4

Algorithm 1 ADMM(x0  β  t)
1: Input: x0 ∈ Ω  the penalty parameter β  the number
2: Initialize: x1 = x0  y1 = Ax1  λ1 = 0  γ = β(cid:107)A(cid:107)2

of iterations t
and G = γI − βA(cid:62)A or G = 0.

2

Algorithm 2 LA-ADMM (x0  β1  K  t)
1: Input: x0 ∈ Ω  the number of stages
K  and the number of iterations t per
stage  initial value of penalization pa-
rameter β1

τ =1 xτ /t

3: for τ = 1  . . .   t do
4:
5:
6: end for

2: for k = 1  . . .   K do
3:
4:
5: end for
6: Output: xK

Let xk = ADMM(xk−1  βk  t)
Update βk+1 = 2βk

Update xτ +1 by (7)  yτ +1 by (5) 
Update λτ +1 by (6)

Proposition 1 (Theorem 4.1 in [8]). For any x ∈ Ω  y ∈ Rm and λ ∈ Rm  we have
β(cid:107)y − y1(cid:107)2

7: Output:(cid:98)xt =(cid:80)t
f ((cid:98)xt) + ψ((cid:98)yt) − [f (x) + ψ(y)] + ((cid:98)ut − u)(cid:62)F(u) ≤ (cid:107)x − x1(cid:107)2
to (2). When t → ∞  ((cid:98)xt (cid:98)yt) converges to the optimal solutions of (2) in a rate of O(1/t).
Corollary 1. Suppose Assumption 1.c and 1.d hold. Let(cid:98)xt be the output of ADMM. For any x ∈ Ω 

Since our goal is to solve the problem (1)  next we present a corollary exhibiting the convergence of
ADMM for solving the original problem (1). All omitted proofs can be found in the supplement.

Remark: The above result establishes a convergence rate for the variational inequality pertained

(cid:107)λ − λ1(cid:107)2

2βt

2t

2t

+

+

G

2

2

.

we have

F ((cid:98)xt) − F (x) ≤ (cid:107)x − x0(cid:107)2

G

2t

β(cid:107)A(cid:107)2

2(cid:107)x − x0(cid:107)2
2t

2

+

+

ρ2
2βt

.

Remark: For the standard ADMM with G = 0 the ﬁrst term in the R.H.S vanishes. For the linearized
ADMM with G = γI − βA(cid:62)A (cid:23) 0  we can bound (cid:107)x − x0(cid:107)2
2. One can also derive
a theoretically optimal value of β by setting x = x∗ ∈ Ω∗ and minimizing the upper bound  which
results in β =
for the linearized
ADMM. Finally  the above result implies that the iteration complexity of standard and linearized
ADMM for ﬁnding an -optimal solution of (1) is O

(cid:16) ρ(cid:107)A(cid:107)2(cid:107)x−x0(cid:107)2

for the standard ADMM or β =

G ≤ γ(cid:107)x − x0(cid:107)2

2(cid:107)A(cid:107)2(cid:107)x∗−x0(cid:107)2

(cid:107)A(cid:107)2(cid:107)x∗−x0(cid:107)2

(cid:17)

√

.

ρ

ρ



Next  we present our locally adaptive ADMM and our main result in this section regarding its iteration
complexity. The proposed algorithm is described in Algorithm 2  which is referred to as LA-ADMM.
The algorithm runs with multiple stages by calling ADMM at each stage with a warm start and a
constant number of iterations t. The penalty parameter βk is increased by a constant factor larger
than 1 (e.g.  2) after each stage and has an initial value dependent on ρ  (cid:107)A(cid:107)2  0  θ and the targeted
accuracy . The convergence result of LA-ADMM employing G = γI − βA(cid:62)A is established below.
A slightly better result in terms of a constant factor can be established for employing G = 0.
Theorem 2. Suppose Assumption 1 holds and F (x) obeys a local error bound condition on the -
  we have F (xK) − F∗ ≤
sublevel. Let β1 = 2ρ1−θ
(cid:107)A(cid:107)20

2. The iteration complexity of LA-ADMM for achieving an 2-optimal solution is (cid:101)O(1/1−θ).

(cid:108) 8ρ(cid:107)A(cid:107)2 max(1 c2)

  K = (cid:100)log2(0/)(cid:101) and t =

(cid:109)

1−θ

Remark: There are two levels of adaptivity to the local sharpness of the penalty parameter. First 
the initial value β1 in Algorithm 3 depends on the local sharpness parameter θ. Second  the time
interval to increase the penalty parameter is determined by the value of t which is also dependent on
θ. Compared to the iteration complexity O(1/) of vanilla ADMM  LA-ADMM can enjoy a lower
iteration complexity.

5 Locally Adaptive ADMM for Stochastic Optimization

In this section  we consider stochastic optimization problem as the following:

(12)
where ξ is a random variable and f (x; ξ) : Rd → (−∞  +∞] is a proper lower-semicontinuous
convex function for each realization of ξ. For this problem  in addition to Assumption 1  we make

F (x) (cid:44) Eξ[f (x; ξ)] + ψ(Ax) 

min
x∈Ω

5

Algorithm 3 SADMM(x0  η  β  t  Ω )
1: Input: x0 ∈ Rd  a step size η  penalty
parameter β  the number of iterations t
and a domain Ω.

2: Initialize: x1 = x0  y1 = Ax1  λ1 = 0
3: for τ = 1  . . .   t do
4:
5:
6: end for

Update xτ +1 by (9) and yτ +1 by (5)
Update λτ +1 by (6)

7: Output:(cid:98)xt =(cid:80)t

τ =1 xτ /t

Algorithm 4 LA-SADMM (x0  η1  β1  D1  K  t)
1: Input: x0 ∈ Rd  the number of stages K  the num-
ber of iterations t per stage  the initial step size η1 
the initial parameter β1 and the initial radius D1.
Let xk = SADMM(xk−1  ηk  βk  t Bk ∩ Ω)
Update ηk+1 = ηk/2 and βk+1 = 2βk  Dk+1 =
Dk/2.
5: end for
6: Output: xK

2: for k = 1  . . .   K do
3:
4:

the following assumption for our development  which is a standard assumption for many previous
stochastic gradient methods.
Assumption 2. For the stochastic optimization problem (12)  we assume that there exists a constant
R such that (cid:107)∂f (x; ξ)(cid:107)2 ≤ R almost surely for any x ∈ Ω.
We present a framework of stochastic ADMM (SADMM) in Algorithm 3. The convergence results
for solving the equivalent constrained optimization problem of stochastic ADMM with different
choices of Gτ have been established in [21  23  35].
Below  we will focus on Gτ = γI − ηβA(cid:62)A (cid:23) I because it leads to computationally more efﬁcient
update for xτ +1 than other two choices for high-dimensional problems. Using Gτ = I will yield a
similar convergence bound except for a constant term and using the idea of AdaGrad for computing
Gτ will lead to the same order of convergence in the worst-case  which we will postpone to future
work for exploration. The corollary below will be used in our analysis.
Corollary 3. Suppose Assumption 1.c  1.d and Assumption 2 hold. Let Gτ = γI − ηβA(cid:62)A (cid:23) I in
Algorithm 3. For any x ∈ Ω 

(cid:19)

+

ρ2
2βt

ρ(cid:107)A(cid:107)2(cid:107)x1 − xt+1(cid:107)2

t

+

F ((cid:98)xt) − F (x) ≤ ηR2

2

(cid:18) β(cid:107)A(cid:107)2

2

+

γ(cid:107)x1 − x(cid:107)2

2(cid:107)x1 − x(cid:107)2
2t
(E[gτ ] − gτ )(cid:62)(xτ − x).

2ηt

2

+

t(cid:88)

+

1
t

τ =1

√

√
τ  the above result implies an O(1/

Remark: Taking expectation on both sides will yield the expectational convergence bound. We can
also use an analysis of large deviation to bound the last term to obtain the convergence with high
probability. In particular  by setting η ∝ 1/
t) convergence
rate  i.e.  O(1/2) iteration complexity of stochastic ADMM.
Next  we discuss our locally adaptive stochastic ADMM (LA-SADMM) algorithm in Algorithm 4.
The key idea is similar to LA-ADMM  i.e.  calling SADMM in multiple stages with warm start. The
step size ηk in each call of SADMM is ﬁxed and decreases by a certain fraction after one stage. The
penalty parameter is updated similarly to that in LA-ADMM but with a different initial value. A key
difference from LA-ADMM is that we employ a domain shrinking approach to modify the domain
of the solutions xτ +1 at each stage. For the k-th stage  the domain for x is the intersection of Ω
and Bk = B(xk−1  Dk)  where the latter is a ball with a radius of Dk centered at xk−1 (the initial
solution of the k-th stage). The radius Dk will decrease geometrically between stages. The purpose
of using the domain shrinking approach is to tackle the last term of the upper bound in Corollary 3 so
that it can decrease geometrically as the stage number increases. A similar idea has been adopted
in [29  7  5]. Note that during each SADMM  we can use the three choices of Gτ as mentioned before.
Below we only present the convergence result of the variant with Gτ = γI − ηkβkA(cid:62)A.
Theorem 4. Suppose Assumptions 1 and 2 hold and F (x) obeys the local error bound condition
on S. Given δ ∈ (0  1)  let ˜δ = δ/K  K = (cid:100)log2( 0
  D1 ≥ c0
6R2   β1 = 6R2
1−θ  
(cid:107)A(cid:107)2
20
  ρ2(cid:107)A(cid:107)2
  12ρ(cid:107)A(cid:107)2D1
R2 } and Gτ =
t be the smallest integer such that t ≥ max{ 6912R2 log(1/˜δ)D2
2I − η1β1A(cid:62)A (cid:23) I. Then LA-SADMM guarantees that  with a probability 1 − δ  we have F (xK) −
F∗ ≤ 2. The iteration complexity of LA-SADMM for achieving an 2-optimal solution with a high

probability 1 − δ is (cid:101)O(log(1/δ)/2(1−θ))  provided D1 = O( c0

 )(cid:101)  η1 = 0

2
0

0

1

2

(1−θ) ).

6

= D(s)

1

1   K  ts)

x(s) =LA-ADMM(x(s−1)  β(s)
ts+1 = ts21−θ  β(s+1)
= β(s)

1   K  ts)
1 /21−θ

1

6R2   β1 = 6R2
(cid:107)A(cid:107)2
20

x(s) =LA-SADMM(x(s−1)  η1  β1  D(s)
1 21−θ
ts+1 = ts22(1−θ)  D(s+1)

Algorithm 6 LA-SADMM with Restarting
and  ≤ 0/2
1: Input: t1  D(1)
1
2: Initialization: x(0)  η1 = 0
3: for s = 1  2  . . .   do
4:
5:
6: end for
7: Output: x(S)

Algorithm 5 LA-ADMM with Restarting
1: Input: t1  β(1)
1
2: Initialization: x(0)
3: for s = 1  2  . . .   do
4:
5:
6: end for
7: Output: x(S)
Remark: Interestingly  unlike that in LA-ADMM  the initial value β1 does not depend on θ. The
adaptivity of the penalty parameters lies on the time interval t which determines when the value of β
is increased. The difference comes from the ﬁrst two terms in Corollary 3.
Before ending this section  we discuss two points. First  both Theorem 2 and Theorem 4 exhibit the
dependence of the two algorithms on the c parameter (e.g.  t in Algorithm 2 and D1 in Algorithm 4)
that is usually unknown. Nevertheless  this issue can be easily addressed by using another level of
restarting and increasing sequence of t and D1 similar to the practical variants in [29  32]. Due to
the limit of space  we only present the algorithms in Algorithm 5 and Algorithm 6 with their formal
guarantee presented in supplement. The conclusion is that under mild conditions as long as β(1)
in Algorithm 5 is sufﬁciently small  t1 and D(1)
in Algorithm 6 are sufﬁciently large  the iteration
1

complexities remain (cid:101)O(1/1−θ) and (cid:101)O(1/2(1−θ)) when θ in LEB condition is known. Second  these

variants can be even employed when the local sharpness parameter θ is unknown by simply setting it
to 0  and still enjoy reduced iteration complexities in terms of a multiplicative factor compared to
vanilla ADMMs. Detailed results are included in the supplement.

6 Applications and Experiments

In this section  we present some experimental results of the proposed algorithms for solving three
tasks  namely generalized LASSO  robust regression with a low-rank regularizer (RR-LR) and
learning low-rank representation. For generalized lasso  our experiment focuses on comparing the
proposed LA-SADMM with SADMM. For the latter tasks  we focus on comparing the proposed
LA-ADMM with previous linearized ADMM with and without self-adaptive penalty parameters.
We ﬁrst consider generalized LASSO  which can ﬁnd applications in many problems in statistics and
machine learning [28]. The objective of generalized LASSO can be expressed as:

n(cid:88)

i=1

min
x∈Rd

F (x) =

1
n

(cid:96)(x(cid:62)ai  bi) + δ(cid:107)Ax(cid:107)1

(13)

where (ai  bi) is a set of pairs of training data  i = 1  . . .   n  δ ≥ 0 is a regularization parameter 
A ∈ Rm×d is a speciﬁed matrix  and (cid:96)(z  b) is a convex loss function in terms of z. The above
formulation include many formulations as special cases  e.g.  the standard LASSO where A = I ∈
Rd×d [26]  fused LASSO that penalizes the (cid:96)1 norm of both the coefﬁcients and their successive
differences [27]  graph-guided fused LASSO (GGLASSO) where A = F ∈ Rm×d encodes some
graph information about features [12]  and sparse graph-guided fused LASSO (S-GGLASSO) where
(cid:107)Ax(cid:107)1 = δ2(cid:107)x(cid:107)1 + δ1(cid:107)F x(cid:107)1 [21].
Let us ﬁrst discuss the local sharpness parameter of generalized lasso with different loss
functions. For the loss function  let us ﬁrst consider piecewise linear loss function such as
hinge loss (cid:96)(z  b) = max(0  1 − bz)  absolute loss (cid:96)(z  b) = |z − b| and -insensitive loss
(cid:96)(z  b) = max(|z − b| −   0). Then the objective is a polyhedral function. According to the
results in [32]  the local sharpness parameter is θ = 1. It then implies that both LA-ADMM and
LA-SADMM enjoy linear convergence results for solving the problem (13) with a piecewise linear
loss function. To the best of our knowledge  these are the ﬁrst linear convergence results of ADMM
without smoothness and strong convexity conditions. One can also consider piecewise quadratic loss
such as square loss (cid:96)(z  b) = (z − b)2 for b ∈ R and squared hinge loss (cid:96)(z  b) = max(0  1 − bz)2
for b ∈ {1 −1}. According to [14]  the problem with convex piecewise quadratic loss has a local

) and (cid:101)O(1/) for LA-ADMM and LA-SADMM.

sharpness parameter θ = 1/2  implying (cid:101)O(1/

√

7

(a) SVM + GGLASSO

(b) SVM + GGLASSO

(c) RR + LR

(d) SVM + S-GGLASSO

(e) SVM + S-GGLASSO

(f) LRR

Figure 1: Comparison of different algorithms for solving different tasks. RR + LR represents robust
regression with a low rank regularizer. LRR represents low-rank representation.

For more examples with different values of θ  we refer readers to [32  30  29  17].

SVM Classiﬁcation with GGLASSO and S-GGLASSO Regularizers To generate the A
matrix  we ﬁrst need to construct a dependency graph of features. We follow [21] to generate a
dependency graph by sparse inverse covariance selection [4]. Speciﬁcally  we get the estimator
of the inverse covariance matrix denoted by ˆΣ−1 via sparse inverse covariance estimation with
ij   where i  j ∈ {1  . . .   d}  i (cid:54)= j  an edge
the graphical lasso [4]. For each nonzero entry ˆΣ−1
between i and j is created. If we denote by G ≡ {V E} the resulting graph  where V is a set of
d vertices  which correspond to d features in the data  and E = {e1  . . .   em} denotes the set of
m edges between elements of V  where ei consists of a tuple of two elements  then the k-th row
of A has two non-zero elements corresponding to the k-th edge ek = (i  j) ∈ E with Ak i = 1
and Ak j = −1. We choose two medium-scale data sets from libsvm website  namely w8a data
(n = 49749  d = 300) and gisette data (n = 6000  d = 5000)  to conduct the experiment. In
the process of estimating inverse covariance matrix  we choose a penalty parameter to be 0.01
that renders the percentage of non-zero elements of the A matrix to be around 3% for w8a
√
data and 1% for gisette data. We compare the performance of the LA-SADMM algorithm with
SADMM [23]  where in SADMM we use Gτ = γI − βητ A(cid:62)A (cid:23) I with ητ ∝ η1/
τ. For
fairness  we set the same initial solution with all zero entries. We ﬁx the value of regularization
parameters (δ in GGLASSO and δ1  δ2 in S-GGLASSO) to be 1
n  where n is the number of
samples. For SADMM  we tune both η1 and β from {10−5:1:5} . For LA-SADMM  we set the
initial step size and penalty parameter to their theoretical value in Theorem 4  and select D1 from
{100  1000}. The values of t in LA-SADMM is set to 105 and 5 × 104 for w8a and gisette  respec-
tively. The results of comparing the objective values versus the number of iterations are presented
in Figure 1 (a b d e). We can see that LA-SADMM exhibits a much faster convergence than SADMM.

Robust Regression with a Low-rank Regularizer The objective function is F (X) =
λ(cid:107)X(cid:107)∗ + (cid:107)AX − C(cid:107)1. We can form an equality constraint Y = AX − C and solve the problem
by linearized ADMM. The value of the local sharpness parameter of this problem is still an open
problem. We compare the proposed LA-ADMM  the vanilla linearized ADMM with a ﬁxed
penalty parameter (ADMM)  the linearized ADMM with self-adaptive penalty proposed in [15]
(ADMM-AP)  and the linearized ADMM with residual balancing in [10  2] (ADMM-RB). We
construct a synthetic data where A ∈ R1000×100 is generated following a Gaussian distribution with
mean 0 and standard deviation 1. To construct C ∈ R1000×50  we ﬁrst generate X ∈ R100×50 and

8

# of iterations×10600.511.522.53objective0.130.1350.140.1450.150.1550.16w8aSADMMLA-SADMM# of iterations×1050246810objective0.250.30.350.40.450.50.55gisetteSADMMLA-SADMM# of iterations020406080100log(objective)1010.51111.51212.513synthetic dataADMM-bestADMM-worstADMM-APADMM-RBLA-ADMM# of iterations×10601234objective0.1350.140.1450.150.1550.160.1650.170.175w8aSADMMLA-SADMM# of iterations×1050246810objective0.250.30.350.40.450.50.55gisetteSADMMLA-SADMM# of iterations02004006008001000log(objective)55.566.57shapeADMM-bestADMM-worstADMM-APADMM-RBLA-ADMMretain only its top 20 components denoted by ˆX and then let C = A ˆX + ε  where ε is a Gaussian
noise matrix with mean zero and standard deviation 0.01. We set λ = 100. For the vanilla linearized
ADMM  we try different penalty parameters from {10−3:1:3} and report the best performance
(using β = 0.01) and worst performance (using β = 0.001). To demonstrate the capability of
adaptive ADMM  we choose β = 0.001 as the initial step size for LA-ADMM and ADMM-AP.
Other parameters of ADMM-AP is the same as suggested in the original paper. For LA-ADMM  we
implement its restarting variant (Algorithm 5)  and start with the number of inner iterations t = 2 and
increase its value by a factor 2 after 10 stages  and also increase the value of β by 10 times after each
stage. The results are reported in Figure 1 (c)  from which we can see that LA-ADMM performs
comparably with ADMM with the best penalty parameter and also better than ADMM-AP. We also
include the results in terms of running time in the supplement.
Low-rank Representation [16] The objective function is F (X) = λ(cid:107)X(cid:107)∗ + (cid:107)AX − A(cid:107)2 1  where
A ∈ Rn×d is a data matrix. We used the shape image 3 and set λ = 10. For the vanilla linearized
ADMM  we try different penalty parameters from {10−3:1:3} and report the best performance (using
β = 0.1) and worst performance (using β = 0.01). To demonstrate the capability of adaptive
ADMM  we choose β = 0.01 as the initial step size for LA-ADMM and ADMM-AP. Other
parameters of ADMM-AP is the same as suggested in the original paper. For LA-ADMM  we
start with the number of inner iterations t = 20 and increase its value by a factor 2 after 2 stages 
and also increase the value of β by 2 times after each stage. The results are reported in Figure 1
(f)  from which we can see that LA-ADMM performs comparably with ADMM with the best
penalty parameter and also better than ADMM-AP. We can see from the ﬁgure that the results of
ADMM-worst and ADMM-AP are quite similar. We also include the results in terms of running time
in the supplement.

7 Conclusion

In this paper  we have presented a new theory of (linearized) ADMM for both deterministic and
stochastic optimization with adaptive penalty parameters. The new adaptive scheme is different
from previous self-adaptive schemes and is adaptive to the local sharpness of the problem. We
have established faster convergence of the proposed algorithms of ADMM with penalty parameters
adaptive to the local sharpness parameter. Experimental results have demonstrated the superior
performance of the proposed stochastic and deterministic adaptive ADMM.

Acknowlegements

We thank the anonymous reviewers for their helpful comments. Y. Xu  M. Liu and T. Yang are
partially supported by National Science Foundation (IIS-1463988  IIS-1545995). Y. Xu would like to
thank Yan Yan for useful discussions on the low-rank representation experiments.

References
[1] J. Bolte  T. P. Nguyen  J. Peypouquet  and B. Suter. From error bounds to the complexity of

ﬁrst-order descent methods for convex functions. CoRR  abs/1510.08234  2015.

[2] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in
Machine Learning  3(1):1–122  2011.

[3] W. Deng and W. Yin. On the global and linear convergence of the generalized alternating

direction method of multipliers. Journal of Scientiﬁc Computing  66(3):889–916  2016.

[4] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the

graphical lasso. Biostatistics  9  2008.

[5] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization  ii: Shrinking procedures and optimal algorithms. SIAM
Journal on Optimization  23(4):2061–2089  2013.

3http://pages.cs.wisc.edu/~swright/TVdenoising/

9

[6] T. Goldstein  B. O’Donoghue  S. Setzer  and R. Baraniuk. Fast alternating direction optimization

methods. SIAM Journal on Imaging Sciences  7(3):1588–1623  2014.

[7] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on
Learning Theory (COLT)  pages 421–436  2011.

[8] B. He and X. Yuan. On the o(1/n) convergence rate of the douglas-rachford alternating direction

method. SIAM Journal on Numerical Analysis  50(2):700–709  2012.

[9] B. He and X. Yuan. On non-ergodic convergence rate of douglas–rachford alternating direction

method of multipliers. Numerische Mathematik  130(3):567–577  2015.

[10] B. S. He  H. Yang  and S. L. Wang. Alternating direction method with self-adaptive penalty
parameters for monotone variational inequalities. Journal of Optimization Theory and Applica-
tions  106(2):337–356  2000.

[11] M. Hong and Z.-Q. Luo. On the linear convergence of the alternating direction method of

multipliers. Mathematical Programming  pages 1–35  2016.

[12] S. Kim  K.-A. Sohn  and E. P. Xing. A multivariate regression approach to association analysis

of a quantitative trait network. Bioinformatics  25(12):i204–i212  2009.

[13] K. Kurdyka. On gradients of functions deﬁnable in o-minimal structures. Annales de l’institut

Fourier  48(3):769 – 783  1998.

[14] G. Li. Global error bounds for piecewise convex polynomials. Math. Program.  137(1-2):37–64 

2013.

[15] Z. Lin  R. Liu  and Z. Su. Linearized alternating direction method with adaptive penalty for
low-rank representation. In Advances In Neural Information Processing Systems (NIPS)  pages
612–620  2011.

[16] G. Liu  Z. Lin  and Y. Yu. Robust subspace segmentation by low-rank representation. In
Proceedings of the 27th international conference on machine learning (ICML-10)  pages 663–
670  2010.

[17] M. Liu and T. Yang. Adaptive accelerated gradient converging methods under holderian error

bound condition. CoRR  abs/1611.07609  2017.

[18] Z.-Q. Luo and J. F. Sturm. Error bound for quadratic systems. Applied Optimization  33:383–

404  2000.

[19] R. D. Monteiro and B. F. Svaiter. Iteration-complexity of block-decomposition algorithms and
the alternating direction method of multipliers. SIAM Journal on Optimization  23(1):475–507 
2013.

[20] I. Necoara  Y. Nesterov  and F. Glineur. Linear convergence of ﬁrst order methods for non-

strongly convex optimization. CoRR  abs/1504.06298  2015.

[21] H. Ouyang  N. He  L. Tran  and A. G. Gray. Stochastic alternating direction method of
multipliers. Proceedings of the 30th International Conference on Machine Learning (ICML) 
28:80–88  2013.

[22] Y. Ouyang  Y. Chen  G. Lan  and E. Pasiliao Jr. An accelerated linearized alternating direction

method of multipliers. SIAM Journal on Imaging Sciences  8(1):644–681  2015.

[23] T. Suzuki. Dual averaging and proximal gradient descent for online alternating direction
multiplier method. In Proceedings of The 30th International Conference on Machine Learning 
pages 392–400  2013.

[24] T. Suzuki. Stochastic dual coordinate ascent with alternating direction method of multipliers. In
Proceedings of The 31st International Conference on Machine Learning  pages 736–744  2014.

10

[25] W. Tian and X. Yuan. Faster alternating direction method of multipliers with a worst-case

o(1/n2) convergence rate. 2016.

[26] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society (Series B)  58:267–288  1996.

[27] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via
the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
67(1):91–108  2005.

[28] R. J. Tibshirani  J. Taylor  et al. The solution path of the generalized lasso. The Annals of

Statistics  39(3):1335–1371  2011.

[29] Y. Xu  Q. Lin  and T. Yang. Stochastic convex optimization: Faster local growth implies faster
global convergence. In Proceedings of the 34th International Conference on Machine Learning
(ICML)  pages 3821–3830  2017.

[30] Y. Xu  Y. Yan  Q. Lin  and T. Yang. Homotopy smoothing for non-smooth problems with lower
complexity than O(1/). In Advances In Neural Information Processing Systems 29 (NIPS) 
pages 1208–1216  2016.

[31] Z. Xu  M. A. T. Figueiredo  and T. Goldstein. Adaptive admm with spectral penalty parameter

selection. CoRR  abs/1605.07246  2016.

[32] T. Yang and Q. Lin. Rsg: Beating subgradient method without smoothness and strong convexity.

CoRR  abs/1512.03107  2016.

[33] X. Zhang  M. Burger  X. Bresson  and S. Osher. Bregmanized nonlocal regularization for
deconvolution and sparse reconstruction. SIAM Journal on Imaging Sciences  3(3):253–276 
2010.

[34] X. Zhang  M. Burger  and S. Osher. A uniﬁed primal-dual algorithm framework based on

bregman iteration. Journal of Scientiﬁc Computing  46(1):20–46  2011.

[35] P. Zhao  J. Yang  T. Zhang  and P. Li. Adaptive stochastic alternating direction method of
multipliers. In Proceedings of the 32nd International Conference on Machine Learning (ICML) 
pages 69–77  2015.

[36] S. Zheng and J. T. Kwok. Fast-and-light stochastic admm. In The 25th International Joint

Conference on Artiﬁcial Intelligence (IJCAI-16)  2016.

[37] W. Zhong and J. T.-Y. Kwok. Fast stochastic alternating direction method of multipliers. In
Proceedings of The 31st International Conference on Machine Learning  pages 46–54  2014.

11

,Yannick Schwartz
Bertrand Thirion
Gael Varoquaux
Yi Xu
Mingrui Liu
Qihang Lin
Tianbao Yang