2018,Leveraged volume sampling for linear regression,Suppose an n x d design matrix in a linear regression problem is given  
but the response for each point is hidden unless explicitly requested. 
The goal is to sample only a small number k << n of the responses  
and then produce a weight vector whose sum of squares loss over *all* points is at most 1+epsilon times the minimum. 
When k is very small (e.g.  k=d)  jointly sampling diverse subsets of
points is crucial. One such method called "volume sampling" has
a unique and desirable property that the weight vector it produces is an unbiased
estimate of the optimum. It is therefore natural to ask if this method
offers the optimal unbiased estimate in terms of the number of
responses k needed to achieve a 1+epsilon loss approximation.

Surprisingly we show that volume sampling can have poor behavior when
we require a very accurate approximation -- indeed worse than some
i.i.d. sampling techniques whose estimates are biased  such as
leverage score sampling. 
We then develop a new rescaled variant of volume sampling that
produces an unbiased estimate which avoids
this bad behavior and has at least as good a tail bound as leverage
score sampling: sample size k=O(d log d + d/epsilon) suffices to
guarantee total loss at most 1+epsilon times the minimum
with high probability. Thus  we improve on the best previously known
sample size for an unbiased estimator  k=O(d^2/epsilon).

Our rescaling procedure leads to a new efficient algorithm
for volume sampling which is based
on a "determinantal rejection sampling" technique with
potentially broader applications to determinantal point processes.
Other contributions include introducing the
combinatorics needed for rescaled volume sampling and developing tail
bounds for sums of dependent random matrices which arise in the
process.,Leveraged volume sampling for linear regression

Michał Derezi´nski and Manfred K. Warmuth

Department of Computer Science
University of California  Santa Cruz

mderezin@berkeley.edu  manfred@ucsc.edu

Daniel Hsu

Computer Science Department
Columbia University  New York

djhsu@cs.columbia.edu

Abstract

Suppose an n ⇥ d design matrix in a linear regression problem is given  but the
response for each point is hidden unless explicitly requested. The goal is to sample
only a small number k ⌧ n of the responses  and then produce a weight vector
whose sum of squares loss over all points is at most 1 + ✏ times the minimum.
When k is very small (e.g.  k = d)  jointly sampling diverse subsets of points
is crucial. One such method called volume sampling has a unique and desirable
property that the weight vector it produces is an unbiased estimate of the optimum.
It is therefore natural to ask if this method offers the optimal unbiased estimate in
terms of the number of responses k needed to achieve a 1 + ✏ loss approximation.
Surprisingly we show that volume sampling can have poor behavior when we
require a very accurate approximation – indeed worse than some i.i.d. sampling
techniques whose estimates are biased  such as leverage score sampling. We then
develop a new rescaled variant of volume sampling that produces an unbiased
estimate which avoids this bad behavior and has at least as good a tail bound as
leverage score sampling: sample size k = O(d log d + d/✏) sufﬁces to guarantee
total loss at most 1 + ✏ times the minimum with high probability. Thus we improve
on the best previously known sample size for an unbiased estimator  k = O(d2/✏).
Our rescaling procedure leads to a new efﬁcient algorithm for volume sampling
which is based on a determinantal rejection sampling technique with potentially
broader applications to determinantal point processes. Other contributions include
introducing the combinatorics needed for rescaled volume sampling and developing
tail bounds for sums of dependent random matrices which arise in the process.

1

Introduction

Consider a linear regression problem where the input points in Rd are provided  but the associated
response for each point is withheld unless explicitly requested. The goal is to sample the responses
for just a small subset of inputs  and then produce a weight vector whose total square loss on all n
points is at most 1 + ✏ times that of the optimum.1 This scenario is relevant in many applications
where data points are cheap to obtain but responses are expensive. Surprisingly  with the aid of
having all input points available  such multiplicative loss bounds are achievable without any range
dependence on the points or responses common in on-line learning [see  e.g.  8].
A natural and intuitive approach to this problem is volume sampling  since it prefers “diverse” sets of
points that will likely result in a weight vector with low total loss  regardless of what the corresponding
responses turn out to be [11]. Volume sampling is closely related to optimal design criteria [18  26] 
which are appropriate under statistical models of the responses; here we study a worst-case setting
where the algorithm must use randomization to guard itself against worst-case responses.

1The total loss being 1 + ✏ times the optimum is the same as the regret being ✏ times the optimum.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Volume sampling and related determinantal point processes are employed in many machine learning
and statistical contexts  including linear regression [11  13  26]  clustering and matrix approxima-
tion [4  14  15]  summarization and information retrieval [19  23  24]  and fairness [6  7]. The
availability of fast algorithms for volume sampling [11  26] has made it an important technique in the
algorithmic toolbox alongside i.i.d. leverage score sampling [17] and spectral sparsiﬁcation [5  25].
It is therefore surprising that using volume sampling in the context of linear regression  as suggested
in previous works [11  26]  may lead to suboptimal performance. We construct an example in which 
even after sampling up to half of the responses  the loss of the weight vector from volume sampling
is a ﬁxed factor >1 larger than the minimum loss. Indeed  this poor behavior arises because for
any sample size >d  the marginal probabilities from volume sampling are a mixture of uniform
probabilities and leverage score probabilities  and uniform sampling is well-known to be suboptimal
when the leverage scores are highly non-uniform.
A possible recourse is to abandon volume sampling in
favor of leverage score sampling [17  33]. However 
all i.i.d. sampling methods  including leverage score
sampling  suffer from a coupon collector problem that
prevents their effective use at small sample sizes [13].
Moreover  the resulting weight vectors are biased
(when regarded as estimators for the least squares
solution based on all responses). This is a nuisance
when averaging multiple solutions (e.g.  as produced
in distributed settings). In contrast  volume sampling
offers multiplicative loss bounds even with sample
sizes as small as d and it is the only known non-trivial
method that gives unbiased weight vectors [11].
We develop a new solution  called leveraged volume sampling  that retains the aforementioned beneﬁts
of volume sampling while avoiding its ﬂaws. Speciﬁcally  we propose a variant of volume sampling
based on rescaling the input points to “correct” the resulting marginals. On the algorithmic side  this
leads to a new determinantal rejection sampling procedure which offers signiﬁcant computational
advantages over existing volume sampling algorithms  while at the same time being strikingly simple
to implement. We prove that this new sampling scheme retains the beneﬁts of volume sampling (like
unbiasedness) but avoids the bad behavior demonstrated in our lower bound example. Along the
way  we prove a new generalization of the Cauchy-Binet formula  which is needed for the rejection
sampling denominator. Finally  we develop a new method for proving matrix tail bounds for leveraged
volume sampling. Our analysis shows that the unbiased least-squares estimator constructed this way
achieves a 1 + ✏ approximation factor from a sample of size O(d log d + d/✏)  addressing an open
question posed by [11].

Figure 1: Plots of the total loss for the sam-
pling methods (averaged over 100 runs) ver-
sus sample size (shading is standard error) for
the libsvm dataset cpusmall [9].

Experiments. Figure 1 presents experimental evidence on a benchmark dataset (cpusmall from the
libsvm collection [9]) that the potential bad behavior of volume sampling proven in our lower bound
does occur in practice. Appendix E shows more datasets and a detailed discussion of the experiments.
In summary  leveraged volume sampling avoids the bad behavior of standard volume sampling  and
performs considerably better than leverage score sampling  especially for small sample sizes k.

Related work. Despite the ubiquity of volume sampling in many contexts already mentioned above 
it has only recently been analyzed for linear regression. Focusing on small sample sizes  [11] proved
multiplicative bounds for the expected loss of size k = d volume sampling. Because the estimators
produced by volume sampling are unbiased  averaging a number of such estimators produced an
estimator based on a sample of size k = O(d2/✏) with expected loss at most 1 + ✏ times the optimum.
It was shown in [13] that if the responses are assumed to be linear functions of the input points
plus white noise  then size k = O(d/✏) volume sampling sufﬁces for obtaining the same expected
bounds. These noise assumptions on the response vector are also central to the task of A-optimal
design  where volume sampling is a key technique [2  18  28  29]. All of these previous results were
concerned with bounds that hold in expectation; it is natural to ask if similar (or better) bounds can
also be shown to hold with high probability  without noise assumptions. Concentration bounds for
volume sampling and other strong Rayleigh measures were studied in [30]  but these results are not
sufﬁcient to obtain the tail bounds for volume sampling.

2

Other techniques applicable to our linear regression problem include leverage score sampling [17]
and spectral sparsiﬁcation [5  25]. Leverage score sampling is an i.i.d. sampling procedure which
achieves tail bounds matching the ones we obtain here for leveraged volume sampling  however it
produces biased weight vectors and experimental results (see [13] and Appendix E) show that it
has weaker performance for small sample sizes. A different and more elaborate sampling technique
based on spectral sparsiﬁcation [5  25] was recently shown to be effective for linear regression [10] 
however this method also does not produce unbiased estimates  which is a primary concern of this
paper and desirable in many settings. Unbiasedness seems to require delicate control of the sampling
probabilities  which we achieve using determinantal rejection sampling.

Outline and contributions. We set up our task of subsampling for linear regression in the next
section and present our lower bound for standard volume sampling. A new variant of rescaled volume
sampling is introduced in Section 3. We develop techniques for proving matrix expectation formulas
for this variant which show that for any rescaling the weight vector produced for the subproblem is
unbiased.
Next  we show that when rescaling with leverage scores  then a new algorithm based on rejection
sampling is surprisingly efﬁcient (Section 4): Other than the preprocessing step of computing leverage
scores  the runtime does not depend on n (a major improvement over existing volume sampling
algorithms). Then  in Section 4.1 we prove multiplicative loss bounds for leveraged volume sampling
by establishing two important properties which are hard to prove for joint sampling procedures. We
conclude in Section 5 with an open problem and with a discussion of how rescaling with approximate
leverage scores gives further time improvements for constructing an unbiased estimator.

2 Volume sampling for linear regression

In this section  we describe our linear regression setting  and review the guarantees that standard
volume sampling offers in this context. Then  we present a surprising lower bound which shows that
under worst-case data  this method can exhibit undesirable behavior.

2.1 Setting
Suppose the learner is given n input vectors x1  . . .   xn 2 Rd  which are arranged as the rows of an
n ⇥ d input matrix X. Each input vector xi has an associated response variable yi 2 R from the
response vector y 2 Rn. The goal of the learner is to ﬁnd a weight vector w 2 Rd that minimizes
the square loss:

w⇤ def

= argmin
w2Rd

L(w)  where L(w)

def
=

(x>i w  yi)2 = kXw  yk2.

nXi=1

Given both matrix X and vector y  the least squares solution can be directly computed as w⇤ = X+y 
where X+ is the pseudo-inverse. Throughout the paper we assume w.l.o.g. that X has (full) rank d.2
In our setting  the learner is initially given the entire input matrix X  while response vector y remains
hidden. The learner is then allowed to select a subset S of row indices in [n] = {1  . . .   n} for
using matrix X and the partial vector of observed responses. Finally  the learner is evaluated by the
loss over all rows of X (including the ones with unobserved responses)  and the goal is to obtain a
multiplicative loss bound  i.e.  that for some ✏> 0 

which the corresponding responses yi are revealed. The learner next constructs an estimate bw of w⇤

L(bw)  (1 + ✏) L(w⇤).

2.2 Standard volume sampling
Given X 2 Rn⇥d and a size k  d  standard volume sampling jointly chooses a set S of k indices in
[n] with probability

2Otherwise just reduce X to a subset of independent columns. Also assume X has no rows of all zeros (every

weight vector has the same loss on such rows  so they can be removed).

Pr(S) =

det(X>SXS)

nd
kd det(X>X)

 

3

where XS is the submatrix of the rows from X indexed by the set S. The learner then obtains
the responses yi  for i 2 S  and uses the optimum solution w⇤S = (XS)+yS for the subproblem
(XS  yS) as its weight vector. The sampling procedure can be performed using reverse iterative
sampling (shown on the right)  which  if carefully implemented  takes O(nd2) time (see [11  13]).
The key property (unique to volume sampling) is that the subsam-
pled estimator w⇤S is unbiased  i.e.

E[w⇤S] = w⇤  where w⇤ = argmin

w

L(w).

As discussed in [11]  this property has important practical implica-
tions in distributed settings: Mixtures of unbiased estimators remain
unbiased (and can conveniently be used to reduce variance). Also
if the rows of X are in general position  then for volume sampling
(1)

(X>X)1.

E⇥(X>SXS)1⇤ =

n  d + 1
k  d + 1

Reverse iterative sampling
VolumeSample(X  k) :

S [n]
while |S| > k
8i2S : qi 
Sample i / qi out of S
S S\{i}

det(X>S\iXS\i)
det(X>S XS )

end

return S

This is important because in A-optimal design bounding tr((X>SXS)1) is the main concern. Given
these direct connections of volume sampling to linear regression  it is natural to ask whether this
distribution achieves a loss bound of (1 + ✏) times the optimum for small sample sizes k.

2.3 Lower bound for standard volume sampling
We show that standard volume sampling cannot guarantee 1 + ✏ multiplicative loss bounds on some
instances  unless over half of the rows are chosen to be in the subsample.
Theorem 1 Let (X  y) be an n ⇥ d least squares problem  such that

X =0BB@

Id⇥d
 Id⇥d
...

 Id⇥d

1CCA   y =0BB@

1d
0d
...
0d

1CCA  

where > 0.

Let w⇤S = (XS)+yS be obtained from size k volume sampling for (X  y). Then 

and there is a > 0 such that for any k  n
2  

E[L(w⇤S)]
L(w⇤)  1 +

lim
!0

n  k
n  d

 

Pr✓L(w⇤S) ⇣1 +

1

2⌘L(w⇤)◆ >

(2)

(3)

1
4

.

Proof In Appendix A we show (2)  and that for the chosen (X  y) we have L(w⇤) =Pd

i=1(1  li)
(see (8))  where li = x>i (X>X)1xi is the i-th leverage score of X. Here  we show (3). The
marginal probability of the i-th row under volume sampling (as given by [12]) is
n  k
n  d

Pr(i 2 S) = ✓l i + (1  ✓) 1 = 1  ✓ (1  li)  where ✓ =

Next  we bound the probability that all of the ﬁrst d input vectors were selected by volume sampling:

(4)

.



i=1(1li)

dYi=1

n  k
n  d

Pr(i 2 S) =

dYi=1⇣1 

Pd
(1  li)⌘  exp⇣ 
L(w⇤) ⌘ 
z }| {
Pr[d] ✓ S (⇤)
where (⇤) follows from negative associativity of volume sampling (see [26]). If for some i 2 [d] we
3 and any k  n
have i 62 S  then L(w⇤S)  1. So for  such that L(w⇤) = 2
2 :
3⌘  1  exp⇣
2

L(w⇤)◆  1  exp⇣
z }| {

Pr✓L(w⇤S) ⇣1 +

Note that this lower bound only makes use of the negative associativity of volume sampling and the
form of the marginals. However the tail bounds we prove in Section 4.1 rely on more subtle properties
of volume sampling. We begin by creating a variant of volume sampling with rescaled marginals.

n  k
n  d ·

2

3⌘ >

n  k
n  d

1

2⌘

1
2 ·

1
4

.

2/3

4

3 Rescaled volume sampling
Given any size k  d  our goal is to jointly sample k row indices ⇡1  . . .  ⇡ k with replacement
(instead of a subset S of [n] of size k  we get a sequence ⇡ 2 [n]k). The second difference to standard
volume sampling is that we rescale the i-th row (and response) by 1pqi
  where q = (q1  ...  qn) is any
discrete distribution over the set of row indices [n]  such thatPn
i=1 qi = 1 and qi > 0 for all i 2 [n].
We now deﬁne q-rescaled size k volume sampling as a joint sampling distribution over ⇡ 2 [n]k  s.t.
Pr(⇡) ⇠ det⇣ kXi=1
(5)
q-rescaled size k volume sampling:
=P|⇡|i=1

Using the following rescaling matrix Q⇡
e⇡ie>⇡i 2 Rn⇥n  we rewrite the determinant
as det(X>Q⇡X). As in standard volume sampling  the normalization factor in rescaled volume
sampling can be given in a closed form through a novel extension of the Cauchy-Binet formula (proof
in Appendix B.1).

x⇡ix>⇡i⌘ kYi=1

1
q⇡i

q⇡i.

1
q⇡i

def

Proposition 2 For any X 2 Rn⇥d  k  d and q1  . . .   qn > 0  such thatPn

i=1 qi = 1  we have

det(X>Q⇡X)

q⇡i = k(k1)··· (kd+1) det(X>X).

X⇡2[n]k

kYi=1

Given a matrix X 2 Rn⇥d  vector y 2 Rn and a sequence ⇡ 2 [n]k  we are interested in a least-
squares problem (Q1/2
⇡ y)  which selects instances indexed by ⇡  and rescales each of them
by the corresponding 1/pqi. This leads to a natural subsampled least squares estimator

⇡ X  Q1/2

w⇤⇡ = argmin

w

kXi=1

1

q⇡ix>⇡iw  y⇡i2 = (Q1/2

⇡ X)+Q1/2

⇡ y.

The key property of standard volume sampling is that the subsampled least-squares estimator is
unbiased. Surprisingly this property is retained for any q-rescaled volume sampling (proof in Section
3.1). As we shall see  this will give us great leeway for choosing q to optimize our algorithms.
Theorem 3 Given a full rank X 2 Rn⇥d and a response vector y 2 Rn  for any q as above  if ⇡ is
sampled according to (5)  then

E[w⇤⇡] = w⇤  where w⇤ = argmin

w

kXw  yk2.

The matrix expectation equation (1) for standard volume sampling (discussed in Section 2) has a
natural extension to any rescaled volume sampling  but now the equality turns into an inequality
(proof in Appendix B.2):
Theorem 4 Given a full rank X 2 Rn⇥d and any q as above  if ⇡ is sampled according to (5)  then

E⇥(X>Q⇡X)1⇤ 

1

kd+1

(X>X)1.

3.1 Proof of Theorem 3
We show that the least-squares estimator w⇤⇡ = (Q1/2
⇡ y produced from any q-rescaled vol-
ume sampling is unbiased  illustrating a proof technique which is also useful for showing Theorem 4 
as well as Propositions 2 and 5. The key idea is to apply the pseudo-inverse expectation formula for
standard volume sampling (see e.g.  [11]) ﬁrst on the subsampled estimator w⇤⇡  and then again on
the full estimator w⇤. In the ﬁrst step  this formula states:

⇡ X)+Q1/2

(Q1/2

⇡ X)+Q1/2

w⇤⇡

}|

{
⇡ y = XS2([k]

d)

z

(Q1/2

⇡S X)+Q1/2

⇡S y 

w⇤⇡S

}|

{

z

det(X>Q⇡S X)
det(X>Q⇡X)

5

elements of set S. Note that since S is of size d  we can decompose the determinant:

|S| = d} and ⇡S denotes a subsequence of ⇡ indexed by the

= {S ✓{ 1  . . .   k} :

where[k]
d def

Whenever this determinant is non-zero  w⇤⇡S is the exact solution of a system of d linear equations:

det(X>Q⇡S X) = det(X⇡S )2 Yi2S

1
q⇡i

.

1
pq⇡i

x>⇡iw =

y⇡i 

for

i 2 S.

1
pq⇡i
1pq⇡i

(1)

d det(X>X)  and obtain:
d)✓ Yi2[k]\S

Thus  the rescaling of each equation by
cancels out  and we can simply write w⇤⇡S =
(X⇡S )+y⇡S. (Note that this is not the case for sets larger than d whenever the optimum solu-
tion incurs positive loss.) We now proceed with summing over all ⇡ 2 [n]k. Following Proposition 2 
we deﬁne the normalization constant as Z = d!k
q⇡i◆ det(X⇡S )2(X⇡S )+y⇡S
Z E[w⇤⇡] =X⇡2[n]k✓ kYi=1
= ✓k
d◆ X¯⇡2[n]d
= ✓k
d◆d! XS2([n]

q⇡i◆ det(X>Q⇡X) w⇤⇡ =X⇡2[n]k XS2([k]
kdYi=1
det(X¯⇡)2(X¯⇡)+y¯⇡ X˜⇡2[n]kd
qi◆kd
det(XS)2(XS)+yS✓ nXi=1
Note that in (1) we separate ⇡ into two parts  ¯⇡ and ˜⇡ (respectively  for subsets S and [k]\S)  and
d counts the number of ways that S can be
sum over them separately. The binomial coefﬁcientk
“placed into” the sequence ⇡. In (2) we observe that whenever ¯⇡ has repetitions  determinant det(X¯⇡)
is zero  so we can switch to summing over sets. Finally  (3) again uses the standard size d volume
sampling unbiasedness formula  now for the least-squares task (X  y)  and the fact that qi’s sum to 1.

z
✓k
d◆d! det(X>X) w⇤.

}|

{

(3)
=

q˜⇡i

d)

(2)

Z

4 Leveraged volume sampling: a natural rescaling

then matrix 1
of the covariance matrix because E[q1
X>X.

Determinantal rejection sampling
1: Input: X2Rn⇥d  q = ( l1
d   . . .   ln
2: s max{k  4d2}
3: repeat
4:
5:
6: until Accept = true

Rescaled volume sampling can be viewed as select-
ing a sequence ⇡ of k rank-1 matrices from the co-
variance matrix X>X =Pn
i=1 xix>i . If ⇡1  . . .  ⇡ k
are sampled i.i.d. from q  i.e.  Pr(⇡) = Qk
i=1 q⇡i 
k X>Q⇡X is an unbiased estimator
⇡i x⇡ix>⇡i] =
In rescaled volume sampling (5)  Pr(⇡) ⇠
Qk
i=1 q⇡i det(X>Q⇡X)
  and the latter volume ratio
introduces a bias to that estimator. However  we show
that this bias vanishes when q is exactly proportional to the leverage scores (proof in Appendix B.3).
Proposition 5 For any q and X as before  if ⇡ 2 [n]k is sampled according to (5)  then
def
= x>i (X>X)1xi.

Sample ⇡1  . . .  ⇡ s i.i.d. ⇠ (q1  . . .   qn)
Sample Accept ⇠ Bernoulli⇣ det( 1
[1..n]X)⇡  k

7: S VolumeSample(Q1/2

det(X>X) ⌘

8: return ⇡S

d )  k  d

s X>Q⇡ X)

det(X>X)

  . . .  

ln

li

E[Q⇡] = (kd) I + diag⇣ l1

q1

qn⌘  where

k X>Q⇡X] = X>E[ 1

k Q⇡]X = X>X if and only if qi = li

In particular  E[ 1
This special rescaling  which we call leveraged volume sampling  has other remarkable properties.
Most importantly  it leads to a simple and efﬁcient algorithm we call determinantal rejection sampling:
Repeatedly sample O(d2) indices ⇡1  . . .  ⇡ s i.i.d. from q = ( l1
d )  and accept the sample
with probability proportional to its volume ratio. Having obtained a sample  we can further reduce its
size via reverse iterative sampling. We show next that this procedure not only returns a q-rescaled
volume sample  but also exploiting the fact that q is proportional to the leverage scores  it requires
(surprisingly) only a constant number of iterations of rejection sampling with high probability.

d > 0 for all i 2 [n].

d   . . .   ln

6

Theorem 6 Given the leverage score distribution q = ( l1
d ) and the determinant det(X>X)
for matrix X 2 Rn⇥d  determinantal rejection sampling returns sequence ⇡S distributed according
to leveraged volume sampling  and w.p. at least 1 ﬁnishes in time O((d2+ k)d2 ln( 1
Proof We use a composition property of rescaled volume sampling (proof in Appendix B.4):
Lemma 7 Consider the following sampling procedure  for s > k:

d   . . .   ln

 )).

Then ⇡S is distributed according to q-rescaled size k volume sampling from X.
First  we show that the rejection sampling probability in line 5 of the algorithm is bounded by 1:

⇡ s⇠ X

S k⇠ 0B@

1pq⇡1
. . .
1pq⇡s

det( 1

s X>Q⇡X)

det(X>X)

(q-rescaled size s volume sampling) 

x>⇡s

x>⇡1

(standard size k volume sampling).

1CA =Q1/2
[1..n]X⇡
 ✓ 1
X>Q⇡X(X>X)1⌘◆d
X>Q⇡X(X>X)1⌘ (⇤)
= det⇣ 1
tr⇣ 1
x>i (X>X)1xi⌘d
=⇣ 1
trQ⇡X(X>X)1X>⌘d
=⇣ 1
sXi=1

d
li

ds

ds

s

d

s

= 1 

where (⇤) follows from the geometric-arithmetic mean inequality for the eigenvalues of the underlying
matrix. This shows that sequence ⇡ is drawn according to q-rescaled volume sampling of size s. Now 
Lemma 7 implies correctness of the algorithm. Next  we use Proposition 2 to compute the expected
value of acceptance probability from line 5 under the i.i.d. sampling of line 4:

X⇡2[n]s✓ sYi=1

q⇡i◆ det( 1

s X>Q⇡X)

det(X>X)

=

s(s1) . . . (sd+1)

sd

⇣1 

d

s⌘d

 1 

d2
s 

3
4

 

 )/ ln( 4

where we also used Bernoulli’s inequality and the fact that s  4d2 (see line 2). Since the expected
value of the acceptance probability is at least 3
4  an easy application of Markov’s inequality shows
that at each trial there is at least a 50% chance of it being above 1
2. So  the probability of at least r
trials occurring is less than (1  1
4 )r. Note that the computational cost of one trial is no more than the
cost of SVD decomposition of matrix X>Q⇡X (for computing the determinant)  which is O(sd2).
The cost of reverse iterative sampling (line 7) is also O(sd2) with high probability (as shown by
[13]). Thus  the overall runtime is O((d2 + k)d2r)  where r  ln( 1
4.1 Tail bounds for leveraged volume sampling
An analysis of leverage score sampling  essentially following [33  Section 2] which in turn draws
from [31]  highlights two basic sufﬁcient conditions on the (random) subsampling matrix Q⇡ that
lead to multiplicative tail bounds for L(w⇤⇡).
It is convenient to shift to an orthogonalization of the linear regression task (X  y) by replacing
matrix X with a matrix U = X(X>X)1/2 2 Rn⇥d. It is easy to check that the columns of U have
unit length and are orthogonal  i.e.  U>U = I. Now  v⇤ = U>y is the least-squares solution for
the orthogonal problem (U  y) and prediction vector Uv⇤ = UU>y for (U  y) is the same as the
prediction vector Xw⇤ = X(X>X)1X>y for the original problem (X  y). The same property
holds for the subsampled estimators  i.e.  Uv⇤⇡ = Xw⇤⇡  where v⇤⇡ = (Q1/2
⇡ y. Volume
sampling probabilities are also preserved under this transformation  so w.l.o.g. we can work with the
orthogonal problem. Now L(v⇤⇡) can be rewritten as

3 ) w.p. at least 1  .

⇡ U)+Q1/2

L(v⇤⇡) = kUv⇤⇡  yk2 (1)

= kUv⇤  yk2 + kU(v⇤⇡  v⇤)k2 (2)

(6)
where (1) follows via Pythagorean theorem from the fact that U(v⇤⇡  v⇤) lies in the column span
of U and the residual vector r = Uv⇤  y is orthogonal to all columns of U  and (2) follows from
U>U = I. By the deﬁnition of v⇤⇡  we can write kv⇤⇡  v⇤k as follows:
(7)
d⇥d

kv⇤⇡  v⇤k = k(U>Q⇡U)1 U>Q⇡(y  Uv⇤)k  k(U>Q⇡U)1

= L(v⇤) + kv⇤⇡  v⇤k2 

kkU>Q⇡ r

d⇥1

k 

7

where kAk denotes the matrix 2-norm (i.e.  the largest singular value) of A; when A is a vector  then
kAk is its Euclidean norm. This breaks our task down to showing two key properties:

1. Matrix multiplication: Upper bounding the Euclidean norm kU>Q⇡ rk 
2. Subspace embedding: Upper bounding the matrix 2-norm k(U>Q⇡U)1k.

We start with a theorem that implies strong guarantees for approximate matrix multiplication with
leveraged volume sampling. Unlike with i.i.d. sampling  this result requires controlling the pairwise
dependence between indices selected under rescaled volume sampling. Its proof is an interesting
application of a classical Hadamard matrix product inequality from [3] (Proof in Appendix C).
Theorem 8 Let U 2 Rn⇥d be a matrix s.t. U>U = I. If sequence ⇡ 2 [n]k is selected using
leveraged volume sampling of size k  2d

✏   then for any r 2 Rn 
2  ✏krk2.

U>Q⇡r  U>r

1
k

E

Next  we turn to the subspace embedding property. The following result is remarkable because
standard matrix tail bounds used to prove this property for leverage score sampling are not applicable
to volume sampling. In fact  obtaining matrix Chernoff bounds for negatively associated joint
distributions like volume sampling is an active area of research  as discussed in [21]. We address
this challenge by deﬁning a coupling procedure for volume sampling and uniform sampling without
replacement  which leads to a curious reduction argument described in Appendix D.
Theorem 9 Let U 2 Rn⇥d be a matrix s.t. U>U = I. There is an absolute constant C  s.t. if
sequence ⇡ 2 [n]k is selected using leveraged volume sampling of size k  C d ln( d

 )  then

Pr✓min⇣ 1

k

U>Q⇡U⌘ 

1

8◆  .

w⇤⇡ = argmin

Theorems 8 and 9 imply that the unbiased estimator w⇤⇡ produced from leveraged volume sampling
achieves multiplicative tail bounds with sample size k = O(d log d + d/✏).
Corollary 10 Let X 2 Rn⇥d be a full rank matrix. There is an absolute constant C  s.t. if sequence
✏  then for estimator
⇡ 2 [n]k is selected using leveraged volume sampling of size k  Cd ln( d
⇡ (Xw  y)k2 
we have L(w⇤⇡)  (1 + ✏) L(w⇤) with probability at least 1  .
Proof Let U = X(X>X)1/2. Combining Theorem 8 with Markov’s inequality  we have that for
large enough C  kU>Q⇡ rk2  ✏ k2
82 krk2 w.h.p.  where r = y  Uv⇤. Finally following (6) and (7)
above  we have that w.h.p.
L(w⇤⇡)  L(w⇤) + k(U>Q⇡U)1k2 kU>Q⇡ rk2  L(w⇤) +
5 Conclusion

k2
82 krk2 = (1 + ✏) L(w⇤).

kQ1/2

82
k2 ✏

 )+ d

w

We developed a new variant of volume sampling which produces the ﬁrst known unbiased subsampled
least-squares estimator with strong multiplicative loss bounds. In the process  we proved a novel
extension of the Cauchy-Binet formula  as well as other fundamental combinatorial equalities.
Moreover  we proposed an efﬁcient algorithm called determinantal rejection sampling  which is to our
knowledge the ﬁrst joint determinantal sampling procedure that (after an initial O(nd2) preprocessing

step for computing leverage scores) produces its k samples in time eO(d2+k)d2)  independent of the
data size n. When n is very large  the preprocessing time can be reduced to eO(nd + d5) by rescaling

with sufﬁciently accurate approximations of the leverage scores. Surprisingly the estimator stays
unbiased and the loss bound still holds with only slightly revised constants. For the sake of clarity we
presented the algorithm based on rescaling with exact leverage scores in the main body of the paper.
However we outline the changes needed when using approximate leverage scores in Appendix F.
In this paper we focused on tail bounds. However we conjecture that there are also volume sampling
✏ ).
based unbiased estimators achieving expected loss bounds E[L(w⇤⇡)]  (1+✏)L(w⇤) with size O( d

8

Acknowledgements
Michał Derezi´nski and Manfred K. Warmuth were supported by NSF grant IIS-1619271. Daniel Hsu
was supported by NSF grant CCF-1740833.

References
[1] Nir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and approximate

nearest neighbors. SIAM Journal on computing  39(1):302–322  2009.

[2] Zeyuan Allen-Zhu  Yuanzhi Li  Aarti Singh  and Yining Wang. Near-optimal design of experi-
ments via regret minimization. In Doina Precup and Yee Whye Teh  editors  Proceedings of the
34th International Conference on Machine Learning  volume 70 of Proceedings of Machine
Learning Research  pages 126–135  International Convention Centre  Sydney  Australia  2017.

[3] T Ando  Roger A. Horn  and Charles R. Johnson. The singular values of a Hadamard product:

A basic inequality. Journal of Linear and Multilinear Algebra  21(4):345–365  1987.

[4] Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM

Journal on Matrix Analysis and Applications  34(4):1464–1499  2013.

[5] Joshua Batson  Daniel A Spielman  and Nikhil Srivastava. Twice-Ramanujan sparsiﬁers. SIAM

Journal on Computing  41(6):1704–1721  2012.

[6] L Elisa Celis  Amit Deshpande  Tarun Kathuria  and Nisheeth K Vishnoi. How to be fair and

diverse? arXiv:1610.07183  October 2016.

[7] L Elisa Celis  Vijay Keswani  Damian Straszak  Amit Deshpande  Tarun Kathuria  and
Nisheeth K Vishnoi. Fair and diverse dpp-based data summarization. arXiv:1802.04023 
February 2018.

[8] N. Cesa-Bianchi  P. M. Long  and M. K. Warmuth. Worst-case quadratic loss bounds for on-line
prediction of linear functions by gradient descent. IEEE Transactions on Neural Networks 
7(3):604–619  1996. Earlier version in 6th COLT  1993.

[9] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology  2:27:1–27:27  2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.

[10] Xue Chen and Eric Price. Condition number-free query and active learning of linear families.

CoRR  abs/1711.10051  2017.

[11] Michał Derezi´nski and Manfred K Warmuth. Unbiased estimates for linear regression via
volume sampling. In Advances in Neural Information Processing Systems 30  pages 3087–3096 
Long Beach  CA  USA  December 2017.

[12] Michał Derezi´nski and Manfred K. Warmuth. Reverse iterative volume sampling for linear

regression. Journal of Machine Learning Research  19(23):1–39  2018.

[13] Michał Derezi´nski and Manfred K. Warmuth. Subsampling for ridge regression via regularized
volume sampling. In Proceedings of the 21st International Conference on Artiﬁcial Intelligence
and Statistics  2018.

[14] Amit Deshpande and Luis Rademacher. Efﬁcient volume sampling for row/column subset
selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer
Science  FOCS ’10  pages 329–338  Washington  DC  USA  2010.

[15] Amit Deshpande  Luis Rademacher  Santosh Vempala  and Grant Wang. Matrix approximation
and projective clustering via volume sampling. In Proceedings of the Seventeenth Annual
ACM-SIAM Symposium on Discrete Algorithm  SODA ’06  pages 1117–1126  Philadelphia  PA 
USA  2006.

9

[16] Petros Drineas  Malik Magdon-Ismail  Michael W. Mahoney  and David P. Woodruff. Fast
approximation of matrix coherence and statistical leverage. J. Mach. Learn. Res.  13(1):3475–
3506  December 2012.

[17] Petros Drineas  Michael W Mahoney  and S Muthukrishnan. Sampling algorithms for `2
regression and applications. In Proceedings of the seventeenth annual ACM-SIAM symposium
on Discrete algorithm  pages 1127–1136  2006.

[18] Valerii V. Fedorov  William J. Studden  and E. M. Klimko  editors. Theory of optimal experi-

ments. Probability and mathematical statistics. Academic Press  New York  1972.

[19] Mike Gartrell  Ulrich Paquet  and Noam Koenigstein. Bayesian low-rank determinantal point
processes. In Proceedings of the 10th ACM Conference on Recommender Systems  RecSys ’16 
pages 349–356  New York  NY  USA  2016.

[20] David Gross and Vincent Nesme. Note on sampling without replacing from a ﬁnite collection

of matrices. arXiv:1001.2738  January 2010.

[21] Nicholas JA Harvey and Neil Olver. Pipage rounding  pessimistic estimators and matrix
concentration. In Proceedings of the twenty-ﬁfth annual ACM-SIAM symposium on Discrete
algorithms  pages 926–945. SIAM  2014.

[22] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of

the American statistical association  58(301):13–30  1963.

[23] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. In Proceed-
ings of the 28th International Conference on Machine Learning  pages 1193–1200. Omnipress 
2011.

[24] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now

Publishers Inc.  Hanover  MA  USA  2012.

[25] Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsiﬁcation in almost-linear time.
In Foundations of Computer Science (FOCS)  2015 IEEE 56th Annual Symposium on  pages
250–269. IEEE  2015.

[26] Chengtao Li  Stefanie Jegelka  and Suvrit Sra. Polynomial time algorithms for dual volume
sampling. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and
R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 5045–5054.
2017.

[27] Michael W. Mahoney. Randomized algorithms for matrices and data. Found. Trends Mach.

Learn.  3(2):123–224  February 2011.

[28] Zelda E. Mariet and Suvrit Sra. Elementary symmetric polynomials for optimal experimental
design. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and
R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 2136–2145.
2017.

[29] Aleksandar Nikolov  Mohit Singh  and Uthaipon Tao Tantipongpipat. Proportional volume
sampling and approximation algorithms for A-optimal design. arXiv:1802.08318  July 2018.
[30] Robin Pemantle and Yuval Peres. Concentration of Lipschitz functionals of determinantal and
other strong rayleigh measures. Combinatorics  Probability and Computing  23(1):140–160 
2014.

[31] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science  FOCS
’06  pages 143–152  Washington  DC  USA  2006.

[32] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Compu-

tational Mathematics  12(4):389–434  August 2012.

[33] David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends®

in Theoretical Computer Science  10(1–2):1–157  2014.

10

,Michal Derezinski
Manfred K. Warmuth
Daniel Hsu