2019,The Label Complexity of Active Learning from Observational Data,Counterfactual learning from observational data involves learning a classifier on an entire population based on data that is observed conditioned on a selection policy. This work considers this problem in an active setting  where the learner additionally has access to unlabeled examples and can choose to get a subset of these labeled by an oracle. 

Prior work on this problem uses disagreement-based active learning  along with an importance weighted loss estimator to account for counterfactuals  which leads to a high label complexity. We show how to instead incorporate a more efficient counterfactual risk minimizer into the active learning algorithm. This requires us to modify both the counterfactual risk to make it amenable to active learning  as well as the active learning process to make it amenable to the risk. We provably demonstrate that the result of this is an algorithm which is statistically consistent as well as more label-efficient than prior work.,The Label Complexity of Active Learning from

Observational Data

Songbai Yan

University of California San Diego

yansongbai@eng.ucsd.edu

Kamalika Chaudhuri

University of California San Diego

kamalika@cs.ucsd.edu

Tara Javidi

University of California San Diego

tjavidi@eng.ucsd.edu

Abstract

Counterfactual learning from observational data involves learning a classiﬁer on an
entire population based on data that is observed conditioned on a selection policy.
This work considers this problem in an active setting  where the learner additionally
has access to unlabeled examples and can choose to get a subset of these labeled
by an oracle.
Prior work on this problem uses disagreement-based active learning  along with
an importance weighted loss estimator to account for counterfactuals  which leads
to a high label complexity. We show how to instead incorporate a more efﬁcient
counterfactual risk minimizer into the active learning algorithm. This requires us
to modify both the counterfactual risk to make it amenable to active learning  as
well as the active learning process to make it amenable to the risk. We provably
demonstrate that the result of this is an algorithm which is statistically consistent
as well as more label-efﬁcient than prior work.

1

Introduction

Counterfactual learning from observational data is an emerging problem that arises naturally in many
applications. In this problem  the learner is given observational data – a set of examples selected
according to some policy along with their labels – as well as access to the policy that selects the
examples  and the goal is to construct a classiﬁer with high performance on an entire population 
not just the observational data distribution. An example is learning to predict if a treatment will be
effective based on features of a patient. Here  we have some observational data on how the treatment
works for patients that were assigned to it  but if the treatment is given only to a certain category of
patients  then the data is not reﬂective of the population. Thus the main challenge in counterfactual
learning is how to counteract the effect of the observation policy and build a classiﬁer that applies
more widely.
This work considers counterfactual learning in the active setting  which has received very recent
attention in a few different contexts [25  21  3]. In addition to observational data  the learner has
an online stream of unlabeled examples drawn from the underlying population distribution  and the
ability to selectively label a subset of these in an interactive manner. The learner’s goal is to again
build a classiﬁer while using as few label queries as possible. The advantage of the active over the
passive is its potential for more label-efﬁcient solutions; the question however is how to do this
algorithmically.
Prior work in this problem has looked at both probabilistic inference [21  3] as well as a standard
classiﬁcation [25]  which is the setting of our work. [25] uses a modiﬁed version of disagreement-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

based active learning [7  9  4  11]  along with an importance weighted empirical risk to account for
the population. However  a problem with this approach is that the importance weighted risk estimator
can have extremely high variance when the importance weights – that reﬂect the inverse of how
frequently an instance in the population is selected by the policy – are high; this may happen if  for
example  certain patients are rarely given the treatment. This high variance in turn results in high
label requirement for the learner.
The problem of high variance in the loss estimator is addressed in the passive case by minimizing a
form of counterfactual risk [22] – an importance weighted loss that combines a variance regularizer
and importance weight clipping or truncation to achieve low generalization error. A plausible solution
is to use this risk for active learning as well. However  this cannot be readily achieved for two reasons.
The ﬁrst is that the variance regularizer itself is a function of the entire dataset  and is therefore
challenging to use in interactive learning where data arrives sequentially. The second reason is
that the minimizer of the (expected) counterfactual risk depends on n  the data size  which again is
inconvenient for learning in an interactive manner.
In this work  we address both challenges. To address the ﬁrst  we use  instead of a variance
regularizer  a novel regularizer based on the second moment; the advantage is that it decomposes
across multiple segments of the data set as which makes it amenable for active learning. We provide
generalization bounds for this modiﬁed counterfactual risk minimizer  and show that it has almost
the same performance as counterfactual risk minimization with a variance regularizer [22]. The
second challenge arises because disagreement-based active learning ensures statistical consistency
by maintaining a set of plausible minimizers of the expected risk. This is problematic when the
minimizer of the expected risk itself changes between iterations as in the case with our modiﬁed
regularizer. We address this challenge by introducing a novel variant of disagreement-based active
learning which is always guaranteed to maintain the population error minimizer in its plausible set.
Additionally  to improve sample efﬁciency  we then propose a third novel component – a new sampling
algorithm for correcting sample selection bias that selectively queries labels of those examples which
are underrepresented in the observational data. Combining these three components gives us a new
algorithm. We prove this newly proposed algorithm is statistically consistent – in the sense that
it converges to the true minimizer of the population risk given enough data. We also analyze its
label complexity  show it is better than prior work [25]  and demonstrate the contribution of each
component of the algorithm to the label complexity bound.

2 Related Work

We consider learning with logged observational data where the logging policy that selects the samples
to be observed is known to the learner. The standard approach is importance sampling to derive an
unbiased loss estimator [19]  but this is known to suffer from high variance. One common approach
for reducing variance is to clip or truncate the importance weights [6  22]  and we provide a new
principled method for choosing the clipping threshold with theoretical guarantees. Another approach
is to add a regularizer based on empirical variance to the loss function to favor models with low loss
variance [17  22  18]. Our second moment regularizer achieves a similar effect  but has the advantage
of being applicable to active learning with theoretical guarantees.
In this work  in addition to logged observational data  we allow the learner to actively acquire
additional labeled examples. The closest to our work is [25]  the only known work in the same setting.
[25] and our work both use disagreement-based active learning (DBAL) framework [7  9  4  11]
and multiple importance sampling [24] for combining actively acquired examples with logged
observational data. [25] uses an importance weighted loss estimator which leads to high variance
and hence high sample complexity. In our work  we incorporate a more efﬁcient variance-controlled
importance sampling into active learning and show that it leads to a better label complexity.
[3] and [21] consider active learning for predicting individual treatment effect which is similar to
our task. They take a Bayesian approach which does not need to know the logging policy  but
assumes the true model is from a known distribution family. Additionally  they do not provide
label complexity bounds. A related line of research considers active learning for domain adaptation 
and their methods are mostly based on heuristics [20  27]  utilizing a clustering structure [14]  or
non-parametric methods [15]. In other related settings  [26] considers warm-starting contextual

2

bandits targeting at minimizing the cumulative regret instead of the ﬁnal prediction error; [16] studies
active learning with bandit feedback without any logged observational data.

3 Problem Setup
We are given an instance space X   a label space Y = {−1  +1}  and a hypothesis class H ⊂ YX .
Let D be an underlying data distribution over X × Y. For simplicity  we assume H is a ﬁnite set  but
our results can be generalized to VC-classes by standard arguments [23  18].
In the passive setting for learning with observational data  the learner has access to a logged obser-
vational dataset generated from the following process. First  m examples {(Xt  Yt)}m
t=1 are drawn
i.i.d. from D. Then a logging policy Q0 : X → [0  1] that describes the probability of observing
the label is applied. In particular  for each example (Xt  Yt) (1 ≤ t ≤ m)  an independent Bernoulli
random variable Zt with expectation Q0(Xt) is drawn  and then the label Yt is revealed to the learner
if Zt = 11. We call T0 = {(Xt  Yt  Zt)}m
t=1 the logged dataset. We assume the learner knows the
logging policy Q0  and only observes instances {Xt}m
t=1  and revealed labels
{Yt | Zt = 1}m
t=1.
In the active learning setting  in addition to the logged dataset  the learner has access to a stream of
online data. In particular  there is a stream of additional n examples {(Xt  Yt)}m+n
t=m+1 drawn i.i.d.
from distribution D. At time t (m < t ≤ m + n)  the learner applies a query policy to compute an
indicator Zt ∈ {0  1}  and then the label Yt is revealed if Zt = 1. The computation of Zt may in
general be randomized  and is based on the observed logged data T0  previously observed instances
{Xi}t
We focus on the active learning setting  and the goal of the learner is to learn a classiﬁer h ∈ H from
observed logged data and online data. Fixing D  Q0  m  n  the performance is measured by: (1) the
error rate l(h) := PrD(h(X) (cid:54)= Y ) of the output classiﬁer  and (2) the number of label queries on
the online data. Note that the error rate is over the entire population D instead of conditioned on the
logging policy  and that we assume the labels of the logged data T0 come at no cost. In this work  we
are interested in the situation where n  the size of the online stream  is smaller than m.

i=m+1  and observed labels {Yi | Zi = 1}t−1

i=m+1  decisions{Zi}t−1

t=1  indicators {Zt}m

i=m+1.

Notation Unless otherwise speciﬁed  all probabilities and expectations are over the draw of all
random variables {(Xt  Yt  Zt)}m+n
t=1 . Deﬁne q0 = inf x Q0(x). Deﬁne the optimal classiﬁer h(cid:63) =
arg minh∈H l(h)  ν = l(h(cid:63)). For any r > 0  h ∈ H  deﬁne the r−ball around h as B(h  r) =
{h(cid:48) ∈ H : Pr(h(X) (cid:54)= h(cid:48)(X)) ≤ r}. For any C ⊆ H  deﬁne the disagreement region DIS(C) =
{x ∈ X : ∃h1 (cid:54)= h2 ∈ C  h1(X) (cid:54)= h2(X)}.
Due to space limit  all proofs are postponed to Appendix.

4 Variance-Controlled Importance Sampling for Passive Learning with

Observational Data

(cid:80)

t

m

Q0(Xt)

1{h(Xt)(cid:54)=Yt}Zt

In the passive setting  the standard method to overcome sample selection bias is to optimize the
importance weighted (IW) loss l(h  T0) = 1
. This loss is an unbiased estimator of
m
− l(h))2 can be high  leading
the population error Pr(h(X) (cid:54)= Y )  but its variance 1
to poor solutions. Previous work addresses this issue by adding a variance regularizer [17  22  18] and
clipping/truncating the importance weight [6  22]. However  the variance regularizer is challenging to
use in interactive learning when data arrives sequentially  and it is unclear how the clipping/truncating
threshold should be chosen to yield good theoretical guarantees.
In this paper  as an alternative to the variance regularizer  we propose a novel second moment
regularizer which achieves a similar error bound to the variance regularizer [18]; and this motivates a
principled choice of the clipping threshold.

E( 1{h(X)(cid:54)=Y }Z

Q0(X)

1This generating process implies the standard unconfoundedness assumption in the counterfactual inference
In other words  the label Yt is conditionally

literature: Pr(Yt  Zt | Xt) = Pr(Yt | Xt) Pr(Zt | Xt).
independent with the action Zt (indicating whether the label is observed) given the instance Xt.

3

4.1 Second-Moment-Regularized Empirical Risk Minimization

Intuitively  between two classiﬁers with similarly small training loss l(h  T0)  the one with lower
(cid:80)
variance should be preferred  since its population error l(h) would be small with a higher probability
than the one with higher variance. Existing work encourages low variance by regularizing the loss
(cid:80)
i( 1{h(Xi)(cid:54)=Yi}Zi
)2 − l(h  T0)2. Here  we propose to
with the estimated variance ˆVar(h  T0) = 1
m
Q0(Xi)
i( 1{h(Xi)(cid:54)=Yi}Zi
regularize with the estimated second moment ˆV(h  T0) = 1
)2  an upper bound
(cid:113)
m
of ˆVar(h  T0). We have the following generalization error bound for regularized ERM.
(cid:114)

ˆV(h  T0). For any δ > 0  then with proba-
|H|
δ

Theorem 1. Let ˆh = arg minh∈H l(h  T0) +
|H|
bility at least 1 − δ  l(ˆh) − l(h(cid:63)) ≤ 28 log
δ
3mq0

E 1{h(cid:63)(X)(cid:54)=Y }

Q0(X) +

4 log
m

4 log
m

(cid:113)

Q0(Xi)

|H|
δ

|H|
δ

+

.

4 log
3
2 q2
0

m

(cid:113) log |H|
(cid:113)

|S2|

Theorem 1 shows an error rate similar to the one for the variance regularizer [18]. However  the
advantage of using the second moment is the decomposability: ˆV(h  S1 ∪ S2) =
|S1|+|S2| ˆV(h  S1) +
|S1|+|S2| ˆV(h  S2). This makes it easier to analyze for active learning that we will discuss later.
Recall for the unregularized importance sampling loss minimizer ˆhIW = arg minh∈H l(h  T0)  the
error bound is ˜O( log |H|

Q0(X) )) [8  25]. In Theorem 1  the extra

m min( l(h(cid:63))

  E 1

|S1|

mq0

+

q0

(cid:113)E 1{h(X)(cid:54)=Y }

1
3
2 q2
0
  and is negligible when m is large.

m

Q0(X)

ˆV(h  T0) around

term is due to the deviation of
In this case  learning with a second moment regularizer gives a better generalization bound.
This improvement in generalization error is due to the regularizer instead of tighter analysis. Similar
to [17  18]  we show in Theorem 2 that for some distributions  the error bound in Theorem 1 cannot
be achieved by any algorithm that simply optimizes the unregularized empirical loss.
ν2   there is a sample space X × Y  a hypothesis class H 
Theorem 2. For any 0 < ν < 1
  and that with probability
a distribution D  and a logging policy Q0 such that ν
q0
t=1  if ˆh = arg minh∈H l(h  S)  then l(ˆh) ≥
100 over the draw of S = {(Xt  Yt  Zt)}m
at least
l(h(cid:63)) + 1
mq0

> E 1{h(cid:63)(X)(cid:54)=Y }

3   m ≥ 49

(cid:113) ν

Q0(X)

mq0

+

.

1

4.2 Clipped Importance Sampling

1

1

i=1

1[

Q0(Xi)

(cid:80)m

1{h(Xi)(cid:54)=Yi}Zi

The variance and hence the error bound for second-moment regularized ERM can still be high if
Q0(x) is large. This
Q0(X) factor arises inevitably to guarantee the importance weighted estimator is
unbiased. Existing work alleviates the variance issue at the cost of some bias by clipping or truncating
the importance weight. In this paper  we focus on clipping  where the loss estimator becomes
Q0(Xi) ≤ M ]. This estimator is no longer unbiased  but
l(h; T0  M ) := 1
m
as the weight is clipped at M  so is the variance. Although studied previously [6  22]  to the best of
our knowledge  it remains unclear how the clipping threshold M can be chosen in a principled way.
We propose to choose M0 = inf{M(cid:48) ≥ 1 | 2M(cid:48) log
Q0(X) > M(cid:48))}. This choice
minimizes an error bound for the clipped second-moment regularized ERM and we formally show
this in Appendix E. Example 30 in Appendix E shows this clipping threshold avoids outputting
suboptimal classiﬁers. The choice of M0 implies that the clipping threshold should be larger as the
sample size m increases  which conﬁrms the intuition that with a larger sample size the variance
becomes less of an issue than the bias. We have the following generalization error bound.

≥ PrX (

|H|
δ

m

1

1

Theorem 3. Let ˆh = arg minh∈H l(h; T0  M0) +
probability at least 1 − δ 

|H|
δ

4 log
m

ˆV(h; T0  M0). For any δ > 0  with

l(ˆh)− l(h(cid:63)) ≤ 34 log
3m

|H|
δ

M0 +

4 log
m 3

2

4 log
m

|H|
δ

E 1{h(cid:63)(X) (cid:54)= Y }

Q0(X)

1[

1

Q0(X)

≤ M0].

(cid:113)

(cid:113)
(cid:115)

|H|
δ

M 2

0 +

4

We always have M0 ≤ 1
that without clipping asymptotically.

as PrX (

q0

1

Q0(X) > 1
q0

) = 0. Thus  this error bound is always no worse than

5 Active Learning with Observational Data

Next  we consider active learning where in addition to a logged observational dataset the learner
has access to a stream of unlabeled samples from which it can actively query for labels. The main
challenges are how to control the variance due to the observational data with active learning  and how
to leverage the logged observational data to reduce the number of label queries beyond simply using
them for warm-start.
To address these challenges  we ﬁrst propose a nontrivial change to the Disagreement-Based Active
Learning (DBAL) so that the variance-controlled importance sampling objective can be incorporated.
This modiﬁed algorithm also works in a general cost-sensitive active learning setting which we
believe is of independent interest. Second  we show how to combine logged observational data with
active learning through multiple importance sampling (MIS). Finally  we propose a novel sample
selection bias correction technique to query regions under-explored in the observational data more
frequently. We provide theoretical analysis demonstrating that the proposed method gives better label
complexity guarantees than previous work [25] and alternative methods.

Key Technique 1: Disagreement-Based Active Learning with Variance-Controlled
Importance Sampling

The DBAL framework is a widely-used general framework for active learning [7  9  4  11]. This
framework iteratively maintains a candidate set Ct to be a conﬁdence set for the optimal classiﬁer. A
disagreement region Dt is then deﬁned accordingly to be the set of instances on which there are two
classiﬁers in Ct that predict labels differently. At each iteration  it draws a set of unlabeled instances.
The labels for instances falling inside the disagreement region are queried; otherwise  the labels are
inferred according to the unanimous prediction of the candidate set. These instances with inferred or
queried labels are then used to shrink the candidate set.
The classical DBAL framework only considers the unregularized 0-1 loss. As discussed in the
previous section  with observational data  unregularized loss leads to suboptimal label complexity.
However  directly adding a regularizer breaks the statistical consistency of DBAL  since the proof of
its consistency is contingent on two properties: (1) the minimizer of the population loss l(h) stays in
all candidate sets with high probability; (2) the loss difference l(h1  S)− l(h2  S) for any h1  h2 ∈ Ct
does not change no matter how examples outside the disagreement region Dt are labeled.
Unfortunately  if we add a variance based regularizer (either estimated variance or second moment) 
ˆV(h  S) has to change as the sample size n increases  and
the objective function l(h  S) +
so does the optimal classiﬁer w.r.t. regularized population loss ˜hn = arg min l(h) +
n V (h).
Consequently  ˜hn may not stay in all candidate sets. Besides  the difference of the regularized
ˆV(h2  S)) changes if labels of examples outside the
loss l(h1  S) +
disagreement region Dt are modiﬁed  breaking the second property.
To resolve the consistency issues  we ﬁrst carefully choose the deﬁnition of the candidate set
and guarantee the optimal classiﬁer w.r.t. the prediction error h(cid:63) = arg min l(h)  instead of the
regularized loss ˜hn  stays in candidate sets with high probability. Moreover  instead of the plain
variance regularizer  we apply the second moment regularizer and exploit its decomposability property
to bound the difference of the regularized loss for ensuring consistency.

ˆV(h1  S) − (l(h2  S) +

(cid:113) λ

(cid:113) λ

(cid:113) λ

(cid:113) λ

n

n

n

Key Technique 2: Multiple Importance Sampling

MIS addresses how to combine logged observational data with actively collected data for training
classiﬁers [2  25]. To illustrate this  for simplicity  we assume a ﬁxed query policy Q1 is used
for active learning. To make use of both T0 = {(Xi  Yi  Zi)}m
i=1 collected by Q0 and T1 =
{(Xi  Yi  Zi)}m+n
i=m+1 collected by Q1  one could optimize the unbiased importance weighted error
1{h(Xi)(cid:54)=Yi}Zi
(m+n)Q1(Xi) which can have high

estimator lIS(h  T0 ∪ T1) =(cid:80)m

(m+n)Q0(Xi) +(cid:80)m+n

1{h(Xi)(cid:54)=Yi}Zi

i=m+1

i=1

5

(cid:80)m+n

m+n

variance and lead to poor generalization error. Here  we apply the MIS estimator lMIS(h  T0 ∪ T1) :=
1{h(Xi)(cid:54)=Yi}Zi
mQ0(Xi)+nQ1(Xi) which effectively treats the data T0 ∪ T1 as drawn from a mixture policy
. lMIS is also unbiased  but has lower variance than lIS and thus gives better error bounds.

mQ0+nQ1

i=1

Key Technique 3: Active Sample Selection Bias Correction

Another advantage to consider active learning is that the learner can apply a strategy to correct
the sample selection bias  which improves label efﬁciency further. This strategy is inspired from
the following intuition: due to sample selection bias caused by the logging policy  labels for some
regions of the sample space may be less likely to be observed in the logged data  thus increasing the
uncertainty in these regions. To counter this effect  during active learning  the learner should query
more labels from such regions.
We formalize this intuition as follows. Suppose we would like to design a single query strategy
Q1 : X → [0  1] that determines the probability of querying the label for an instance during the active
learning phase. For any Q1  we have the following generalization error bound for learning with n
logged examples and m unlabeled examples from which the learner can select and query for labels
(for simplicity of illustration  we use the unclipped estimator here)

δ

+

l(h1) − l(h2) ≤ l(h1  S) − l(h2  S) +

4 log 2|H|
3(mq0 + n)
We propose to set Q1(x) = 1{mQ0(x) < m
2 Q0(x) + n} which only queries instances if Q0(x) is
small. This leads to fewer queries while guarantees an error bound close to the one achieved by
setting Q1(x) ≡ 1 that queries every instance. In Appendix E we give an example  Example 31 
showing the reduction of queries due to this strategy.
The sample selection bias correction strategy is complementary to the DBAL technique. We note that
a similar query strategy is proposed in [25]  but the strategy here stems from a tighter analysis and
can be applied with variance control techniques discussed in Section 4  and thus gives better label
complexity guarantees as to be discussed in the analysis section.

mQ0(X) + nQ1(X)

2|H|
δ

log

.

(cid:115)
4E 1{h1(X) (cid:54)= h2(X)}

5.1 Algorithm

Putting things together  our proposed algorithm is shown as Algorithm 1. It takes the logged data
and an epoch schedule as input. It assumes the logging policy Q0 and its distribution f (x) =
Pr(Q0(X) ≤ x) are known (otherwise  these quantities can be estimated with unlabeled data).
Algorithm 1 uses the DBAL framework that recursively shrinks a candidate set C and its cor-
responding disagreement region D to save label queries by not querying examples outside D.
In particular  at iteration k  it computes a clipping threshold Mk (step 5) and MIS weights
which are used to deﬁne the clipped MIS error estimator and two
wk(x) :=
second moment estimators

mQ0(Xi)+(cid:80)k

j=1 τiQi(Xi)

m+nk

i=1

m+nk(cid:88)
m+nk(cid:88)
m+nk(cid:88)

i=1

l(h; ˜Sk  Mk) :=

ˆV(h1  h2; ˜Sk  Mk) :=

ˆV(h; ˜Sk  Mk) :=

1

m + nk

1

m + nk

1

m + nk

i=1

wk(Xi)Zi1{h(Xi) (cid:54)= ˜Yi}1{wk(Xi) ≤ Mk} 

k(Xi)Zi1{h1(Xi) (cid:54)= h2(Xi)}1{wk(Xi) ≤ Mk} 
w2

k(Xi)Zi1{h(Xi) (cid:54)= ˜Yi}1{wk(Xi) ≤ Mk}.
w2

The algorithm shrinks the candidate set Ck+1 by eliminating classiﬁers whose estimated error is
larger than a threshold that takes the minimum empirical error and the second moment into account
(step 7)  and deﬁnes a corresponding disagreement region Dk+1 = DIS(Ck+1) as the set of all
instances on which there are two classiﬁers in the candidate set Ck+1 that predict labels differently.
It derives a query policy Qk+1 with the sample selection bias correction strategy (step 9). At the
end of iteration k  it draws τk+1 unlabeled examples. For each example X with Qk+1(X) > 0  if

6

X ∈ Dk+1  the algorithm queries for the actual label Y and sets ˜Y = Y   otherwise it infers the label
and sets ˜Y = ˆhk(X). These examples {X} and their inferred or queried labels { ˜Y } are then used in
subsequent iterations. In the last step of the algorithm  a classiﬁer that minimizes the clipped MIS
error with the second moment regularizer over all received data is returned.

1: Input: conﬁdence δ  logged data T0  epoch schedule τ1  . . .   τK  n =(cid:80)K

Algorithm 1 Disagreement-Based Active Learning with Logged Observational Data
i=1 τi.

2: ˜S0 ← T0; C0 ← H; D0 ← X ; n0 = 0
3: for k = 0  . . .   K − 1 do
σ1(k  δ  M ) ← ( M
4:
Choose Mk = inf{M ≥ 1 |
ˆhk ← arg minh∈Ck l(h; ˜Sk  Mk)
Deﬁne the candidate set Ck+1 ← {h ∈ Ck
σ2(k  δk) ˆV(h  ˆhk; ˜Sk  Mk)}

+ M 2
(m+nk)
2M

γ1σ1(k  δk  Mk) + γ1

(cid:113)

5:
6:
7:

|H|
δk

) log

m+nk

m+nk

log

3
2

|H|
δ ; σ2(k  δ) = 1
≥ Pr( m+nk

mQ0(X)+nk

m+nk

|H|
δ ; δk ←

log
> M/2)}

δ

2(k+1)(k+2)

|

l(h; ˜Sk  Mk) ≤ l(ˆhk; ˜Sk  Mk) +

Qk+1(x) ← 1{mQ0(x) +(cid:80)k

Deﬁne the Disagreement Region Dk+1 ← {x ∈ X | ∃h1  h2 ∈ Ck+1 s.t. h1(x) (cid:54)= h2(x)}
nk+1 ← nk + τk+1
Draw τk+1 samples {(Xt  Yt)}m+nk+1
for t = m + nk + 1 to m + nk+1 do

i=1 τiQi(x) < m
t=m+nk+1  and present {Xt}m+nk+1

2 Q0(x) + nk+1};

t=m+nk+1 to the learner.

Zt ← Qk+1(Xt)
if Zt = 1 then

8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for
20: Output ˆh = arg minh∈CK l(h; ˜SK  Mk) + γ1

end for
˜Tk+1 ← {Xt  ˜Yt  Zt}m+nk+1

end if

If Xt ∈ Dk+1  query for label: ˜Yt ← Yt; otherwise infer ˜Yt ← ˆhk(Xt).

t=m+nk+1  ˜Sk+1 ← ˜Sk ∪ ˜Tk+1;

(cid:113) 1

m+n log

|H|
δK

ˆV(h; ˜SK  Mk).

5.2 Analysis

(cid:115)

We have the following generalization error bound for Algorithm 1. Despite not querying for all labels 
our algorithm achieves the same asymptotic bound as the one that queries labels for all online data.
Theorem 4. Let M = inf{M(cid:48) ≥ 1 | 2M(cid:48)
mQ0(X)+n ≥ M(cid:48)/2)} be the ﬁnal clipping
threshold used in step 20. There is an absolute constant c0 > 1 such that for any δ > 0  with
probability at least 1 − δ 

≥ Pr( m+n

m+n log

|H|
δK

1{ m + n

mQ0(X) + n

≤ M} log

|H|
δ

+

M log

|H|
δ
m + n

+

M 2

log

|H|
δ
(m + n) 3

2

).

mQ0(X) + n

l(ˆh) ≤ l(h(cid:63))+c0(

E 1{h(cid:63)(X) (cid:54)= Y }
r Pr(cid:0)DIS(B(h(cid:63)  r)) ∩(cid:8)x : Q0(x) ≤ 1

1

t

Next  we analyze the number of labels queried by Algorithm 1 with the help of following deﬁnitions.
Deﬁnition 5. For any t ≥ 1  r > 0  deﬁne the modiﬁed disagreement coefﬁcient ˜θ(r  t) :=

(cid:9)(cid:1). Deﬁne ˜θ := supr>2ν

˜θ(r  2m

n ).

(cid:113)

The modiﬁed disagreement coefﬁcient ˜θ(r  t) measures the probability of the intersection of two sets:
the disagreement region for the r-ball around h(cid:63) and where the propensity score Q0(x) is smaller than
t . It characterizes the size of the querying region of Algorithm 1. Note that the standard disagreement
1
coefﬁcient [10]  which is widely used for analyzing DBAL in the classical active learning setting 
can be written as θ(r) := ˜θ(r  1). Here  the modiﬁed disagreement coefﬁcient modiﬁes the standard
deﬁnition to account for the reduction of the number of label queries due to the sample selection bias
correction strategy: Algorithm 1 only queries examples on which Q0(x) is lower than some threshold 
hence ˜θ(r  t) ≤ θ(r). Moreover  our modiﬁed disagreement coefﬁcient ˜θ is always smaller than the
modiﬁed disagreement coefﬁcient of [25] (denoted by θ(cid:48)) which is used to analyze their algorithm.

7

n to be the size ratio of logged and online data  let τk = 2k  deﬁne
} to be the minimum ratio between the clipping threshold Mk and
by the choice of Mk)  and deﬁne

Additionally  deﬁne α = m
ξ = min1≤k≤K{Mk/ m+nk
mq0+nk
maximum MIS weight m+nk
mq0+nk
¯M = max1≤k≤K Mk to be the maximum clipping threshold. Recall q0 = inf X Q0(X).
The following theorem upper-bounds the number of label queries by Algorithm 1.
Theorem 6. There is an absolute constant c1 > 1 such that for any δ > 0  with probability at least
1 − δ  the number of labels queried by Algorithm 1 is at most:

(ξ ≤ 1 since Mk ≤ m+nk

mq0+nk

˜θ · (nν +

c1

nνξ

αq0 + 1

log

|H| log n

δ

+

¯M ξ log n√
nα

|H| log n

δ

log

+

ξ log n
αq0 + 1

log

|H| log n

δ

).

(cid:114)

(cid:115)

5.3 Discussion

ν ˜θ log |H| · ( 1
≥ E 1{h(cid:63)(X)(cid:54)=Y }
1+αQ0(X) .
νθ log |H| · ( 1

ν ˜θ log |H| ·(cid:16) M
(cid:16)
(cid:16)
ν ˜θ log |H| ·(cid:16)
(cid:16)
(cid:16)
νθ log |H| ·(cid:16)
(cid:16)
log |H| ·(cid:16)
(cid:16)
(cid:16)

≥ 1

(1+α)2q0

1+αq0

q0+α

1+αQ0(X)

2 E 1{h(cid:63)(X)(cid:54)=Y }
(cid:17)
(cid:17)

(cid:17)(cid:17)

(cid:17)(cid:17)

(cid:17)

In this subsection  we compare the theoretical performance of the proposed algorithm and some
alternatives to understand the effect of proposed techniques. We present some empirical results in
Section F in Appendix.
The theoretical performance of learning algorithms is captured by label complexity  which is deﬁned
as the number of label queries required during the active learning phase to guarantee the test error
of the output classiﬁer to be at most ν +  (here ν = l(h(cid:63)) is the optimal error   and  is the target
excess error). This can be derived by combining the upper bounds on the error (Theorem 4) and the
number of queries (Theorem 6).
• The label complexity is ˜O

1{

for

2 E 1{h(cid:63)(X)(cid:54)=Y }

1+αQ0(X)

(1+α) + 1

Algorithm 1. This is derived from Theorem 4  6.

• The label complexity is ˜O

1

(1+αq0) + 1

is derived by setting the ﬁnal clipping threshold MK = 1+α
1+αq0

. It is worse since 1+α
1+αq0

1+αQ0(X) ≤ M}(cid:17)(cid:17)
(cid:17)(cid:17)

1+α

without clipping. This

≥ M.

 + ν
2 )

1

1+αq0

if regularizers are removed further. This

• The label complexity is ˜O

is worse since

ν

1+αq0

• The label complexity is ˜O

• The label complexity is ˜O
technique. It can be shown
• The label complexity is ˜O

bias correction strategy. Here the standard disagreement coefﬁcient θ is used (θ ≥ ˜θ).

 + ν
2 )

1

1+αq0

if we further remove the sample selection

1

(1+αq0) + ν(q0+α)
2(1+α)2q0

if we further remove the MIS

  so MIS gives a better label complexity bound.

if DBAL is further removed. Here 
all n online examples are queried. This demonstrates that DBAL decreases the label complexity
bound by a factor of νθ which is at most 1 by deﬁnition.

(1+αq0) + ν(q0+α)
2(1+α)2q0

1

• Finally  the label complexity is ˜O

for [25]  the only known algorithm in
our setting. Here  θ(cid:48) ≥ ˜θ 
≥ M
1+α. Thus  the label complexity
of the proposed algorithm is better than [25]. This improvement is made possible by the second
moment regularizer  the principled clipping technique  and thereby the improved sample selection
bias correction strategy.

1+αQ0(X)   and

νθ(cid:48) log |H| · ν+
≥ E 1{h(cid:63)(X)(cid:54)=Y }

1+αq0

1+αq0

1+αq0

2

ν

1

1

6 Conclusion

We consider active learning with logged observational data where the learner is given an observational
data set selected according to some logging policy  and can actively query for additional labels
from an online data stream. Previous work applies disagreement-based active learning with an

8

importance weighted loss estimator to account for counterfactuals  which has high variance and
leads to a high label complexity. In this work  we utilize variance control techniques for importance
weighted estimators  and propose a novel variant of DBAL to make it amenable to variance-controlled
importance sampling. Based on these improvements  a new sample selection bias correction strategy
is proposed to further boost label efﬁciency. Our theoretical analysis shows that the proposed
algorithm is statistically consistent and more label-efﬁcient than prior work and alternative methods.

Acknowledgement We thank NSF under CCF 1513883 and 1719133 for support.

References
[1] Vowpal Wabbit. https://github.com/JohnLangford/vowpal_wabbit/.

[2] Aman Agarwal  Soumya Basu  Tobias Schnabel  and Thorsten Joachims. Effective evaluation
using logged bandit feedback from multiple loggers. arXiv preprint arXiv:1703.06180  2017.

[3] Onur Atan  William R. Zame  and Mihaela van der Schaar. Sequential patient recruitment and
allocation for adaptive clinical trials. In Kamalika Chaudhuri and Masashi Sugiyama  editors 
Proceedings of Machine Learning Research  volume 89 of Proceedings of Machine Learning
Research  pages 1891–1900. PMLR  16–18 Apr 2019.

[4] M.-F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. J. Comput. Syst. Sci. 

75(1):78–89  2009.

[5] P Borjesson and C-E Sundberg. Simple approximations of the error function q (x) for commu-

nications applications. IEEE Transactions on Communications  27(3):639–643  1979.

[6] Léon Bottou  Jonas Peters  Joaquin Quiñonero-Candela  Denis X Charles  D Max Chickering 
Elon Portugaly  Dipankar Ray  Patrice Simard  and Ed Snelson. Counterfactual reasoning and
learning systems: The example of computational advertising. The Journal of Machine Learning
Research  14(1):3207–3260  2013.

[7] D. A. Cohn  L. E. Atlas  and R. E. Ladner. Improving generalization with active learning.

Machine Learning  15(2)  1994.

[8] Corinna Cortes  Yishay Mansour  and Mehryar Mohri. Learning bounds for importance

weighting. In Advances in neural information processing systems  pages 442–450  2010.

[9] S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In NIPS 

2007.

[10] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML  2007.
[11] Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends R(cid:13)

in Machine Learning  7(2-3):131–309  2014.

[12] D. Hsu. Algorithms for Active Learning. PhD thesis  UC San Diego  2010.

[13] Tzu-Kuo Huang  Alekh Agarwal  Daniel J Hsu  John Langford  and Robert E Schapire. Efﬁcient
and parsimonious agnostic active learning. In Advances in Neural Information Processing
Systems  pages 2755–2763  2015.

[14] David Kale  Marjan Ghazvininejad  Anil Ramakrishna  Jingrui He  and Yan Liu. Hierarchical
active transfer learning. In Proceedings of the 2015 SIAM International Conference on Data
Mining  pages 514–522. SIAM  2015.

[15] Samory Kpotufe and Guillaume Martinet. Marginal singularity  and the beneﬁts of labels in

covariate-shift. In Conference On Learning Theory  pages 1882–1886  2018.

[16] Akshay Krishnamurthy  Alekh Agarwal  Tzu-Kuo Huang  Hal Daumé  III  and John Langford.
Active learning for cost-sensitive classiﬁcation. In Doina Precup and Yee Whye Teh  editors 
Proceedings of the 34th International Conference on Machine Learning  volume 70 of Pro-
ceedings of Machine Learning Research  pages 1915–1924  International Convention Centre 
Sydney  Australia  06–11 Aug 2017. PMLR.

9

[17] A Maurer and M Pontil. Empirical bernstein bounds and sample variance penalization. In

COLT 2009-The 22nd Conference on Learning Theory  2009.

[18] Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives.

In Advances in Neural Information Processing Systems  pages 2971–2980  2017.

[19] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational

studies for causal effects. Biometrika  70(1):41–55  1983.

[20] Avishek Saha  Piyush Rai  Hal Daumé  Suresh Venkatasubramanian  and Scott L DuVall.
Active supervised domain adaptation. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases  pages 97–112. Springer  2011.

[21] Iiris Sundin  Peter Schulam  Eero Siivola  Aki Vehtari  Suchi Saria  and Samuel Kaski.
Active learning for decision-making from imbalanced observational data. arXiv preprint
arXiv:1904.05268  2019.

[22] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from
logged bandit feedback. In International Conference on Machine Learning  pages 814–823 
2015.

[23] VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of

events to their probabilities. Theory of Probability and its Applications  16(2):264  1971.

[24] Eric Veach and Leonidas J Guibas. Optimally combining sampling techniques for monte carlo
rendering. In Proceedings of the 22nd annual conference on Computer graphics and interactive
techniques  pages 419–428. ACM  1995.

[25] Songbai Yan  Kamalika Chaudhuri  and Tara Javidi. Active learning with logged data. In

International Conference on Machine Learning  pages 5517–5526  2018.

[26] Chicheng Zhang  Alekh Agarwal  Hal Daumé III  John Langford  and Sahand N Negahban.
Warm-starting contextual bandits: Robustly combining supervised and bandit feedback. arXiv
preprint arXiv:1901.00301  2019.

[27] Zihan Zhang  Xiaoming Jin  Lianghao Li  Guiguang Ding  and Qiang Yang. Multi-domain
active learning for recommendation. In Thirtieth AAAI Conference on Artiﬁcial Intelligence 
2016.

[28] Andre M Zubkov and Aleksandr A Serov. A complete proof of universal inequalities for the
distribution function of the binomial law. Theory of Probability & Its Applications  57(3):539–
544  2013.

10

,Songbai Yan
Kamalika Chaudhuri
Tara Javidi