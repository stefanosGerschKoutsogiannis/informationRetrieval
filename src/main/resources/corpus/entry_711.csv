2019,Bayesian Batch Active Learning as Sparse Subset Approximation,Leveraging the wealth of unlabeled data produced in recent years provides great potential for improving supervised models. When the cost of acquiring labels is high  probabilistic active learning methods can be used to greedily select the most informative data points to be labeled. However  for many large-scale problems standard greedy procedures become computationally infeasible and suffer from negligible model change. In this paper  we introduce a novel Bayesian batch active learning approach that mitigates these issues. Our approach is motivated by approximating the complete data posterior of the model parameters. While naive batch construction methods result in correlated queries  our algorithm produces diverse batches that enable efficient active learning at scale. We derive interpretable closed-form solutions akin to existing active learning procedures for linear models  and generalize to arbitrary models using random projections. We demonstrate the benefits of our approach on several large-scale regression and classification tasks.,Bayesian Batch Active Learning as

Sparse Subset Approximation

Robert Pinsler

Department of Engineering
University of Cambridge

rp586@cam.ac.uk

Jonathan Gordon

Department of Engineering
University of Cambridge

jg801@cam.ac.uk

Eric Nalisnick

Department of Engineering
University of Cambridge

etn22@cam.ac.uk

José Miguel Hernández-Lobato

Department of Engineering
University of Cambridge

jmh233@cam.ac.uk

Abstract

Leveraging the wealth of unlabeled data produced in recent years provides great
potential for improving supervised models. When the cost of acquiring labels is
high  probabilistic active learning methods can be used to greedily select the most
informative data points to be labeled. However  for many large-scale problems
standard greedy procedures become computationally infeasible and suffer from
negligible model change. In this paper  we introduce a novel Bayesian batch
active learning approach that mitigates these issues. Our approach is motivated by
approximating the complete data posterior of the model parameters. While naive
batch construction methods result in correlated queries  our algorithm produces
diverse batches that enable efﬁcient active learning at scale. We derive interpretable
closed-form solutions akin to existing active learning procedures for linear models 
and generalize to arbitrary models using random projections. We demonstrate the
beneﬁts of our approach on several large-scale regression and classiﬁcation tasks.

1

Introduction

Much of machine learning’s success stems from leveraging the wealth of data produced in recent
years. However  in many cases expert knowledge is needed to provide labels  and access to these
experts is limited by time and cost constraints. For example  cameras could easily provide images
of the many ﬁsh that inhabit a coral reef  but an ichthyologist would be needed to properly label
each ﬁsh with the relevant biological information. In such settings  active learning (AL) [1] enables
data-efﬁcient model training by intelligently selecting points for which labels should be requested.
Taking a Bayesian perspective  a natural approach to AL is to choose the set of points that maximally
reduces the uncertainty in the posterior over model parameters [2]. Unfortunately  solving this combi-
natorial optimization problem is NP-hard. Most AL methods iteratively solve a greedy approximation 
e.g. using maximum entropy [3] or maximum information gain [2  4]. These approaches alternate
between querying a single data point and updating the model  until the query budget is exhausted.
However  as we discuss below  sequential greedy methods have severe limitations in modern machine
learning applications  where datasets are massive and models often have millions of parameters.
A possible remedy is to select an entire batch of points at every AL iteration. Batch AL approaches
dramatically reduce the computational burden caused by repeated model updates  while resulting in
much more signiﬁcant learning updates. It is also more practical in applications where the cost of

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) MAXENT

(b) BALD

(c) Ours

Figure 1: Batch construction of different AL methods on cifar10  shown as a t-SNE projection [12].
Given 5000 labeled points (colored by class)  a batch of 200 points (black crosses) is queried.

acquiring labels is high but can be parallelized. Examples include crowd-sourcing a complex labeling
task  leveraging parallel simulations on a compute cluster  or performing experiments that require
resources with time-limited availability (e.g. a wet-lab in natural sciences). Unfortunately  naively
constructing a batch using traditional acquisition functions still leads to highly correlated queries [5] 
i.e. a large part of the budget is spent on repeatedly choosing nearby points. Despite recent interest in
batch methods [5–8]  there currently exists no principled  scalable Bayesian batch AL algorithm.
In this paper  we propose a novel Bayesian batch AL approach that mitigates these issues. The key
idea is to re-cast batch construction as optimizing a sparse subset approximation to the log posterior
induced by the full dataset. This formulation of AL is inspired by recent work on Bayesian coresets
[9  10]. We leverage these similarities and use the Frank-Wolfe algorithm [11] to enable efﬁcient
Bayesian AL at scale. We derive interpretable closed-form solutions for linear and probit regression
models  revealing close connections to existing AL methods in these cases. By using random
projections  we further generalize our algorithm to work with any model with a tractable likelihood.
We demonstrate the beneﬁts of our approach on several large-scale regression and classiﬁcation tasks.

2 Background
We consider discriminative models p(y|x  θ) parameterized by θ ∈ Θ  mapping from inputs x ∈ X
to a distribution over outputs y ∈ Y. Given a labeled dataset D0 = {xn  yn}N
n=1  the learning task
consists of performing inference over the parameters θ to obtain the posterior distribution p(θ|D0).
In the AL setting [1]  the learner is allowed to choose the data points from which it learns. In addition
to the initial dataset D0  we assume access to (i) an unlabeled pool set Xp = {xm}M
m=1  and (ii) an
oracle labeling mechanism which can provide labels Yp = {ym}M
m=1 for the corresponding inputs.
Probabilistic AL approaches choose points by considering the posterior distribution of the model
parameters. Without any budget constraints  we could query the oracle M times  yielding the
complete data posterior through Bayes’ rule 

p(θ|D0) p(Yp|Xp  θ)

p(θ|D0 ∪ (Xp Yp)) =

(1)
where here p(θ|D0) plays the role of the prior. While the complete data posterior is optimal from a
Bayesian perspective  in practice we can only select a subset  or batch  of points D(cid:48) = (X (cid:48) Y(cid:48)) ⊆ Dp
due to budget constraints. From an information-theoretic perspective [2]  we want to query points
X (cid:48) ⊆ Xp that are maximally informative  i.e. minimize the expected posterior entropy 

p(Yp|Xp D0)

 

X ∗ = arg min

X (cid:48)⊆Xp  |X (cid:48)|≤b

EY(cid:48)∼p(Y(cid:48)|X (cid:48) D0) [H [θ|D0 ∪ (X (cid:48) Y(cid:48))]]  

(2)

where b is a query budget. Solving Eq. (2) directly is intractable  as it requires considering all possible
subsets of the pool set. As such  most AL strategies follow a myopic approach that iteratively chooses
a single point until the budget is exhausted. Simple heuristics  e.g. maximizing the predictive entropy
(MAXENT)  are often employed [13  5]. Houlsby et al. [4] propose BALD  a greedy approximation
to Eq. (2) which seeks the point x that maximizes the decrease in expected entropy:
H [θ|D0] − Ey∼p(y|x D0) [H [θ|x  y D0]] .

(3)

x∗ = arg min
x∈Xp

While sequential greedy strategies can be near-optimal in certain cases [14  15]  they become severely
limited for large-scale settings. In particular  it is computationally infeasible to re-train the model

2

after every acquired data point  e.g. re-training a ResNet [16] thousands of times is clearly impractical.
Even if such an approach were feasible  the addition of a single point to the training set is likely
to have a negligible effect on the parameter posterior distribution [5]. Since the model changes
only marginally after each update  subsequent queries thus result in acquiring similar points in data
space. As a consequence  there has been renewed interest in ﬁnding tractable batch AL formulations.
Perhaps the simplest approach is to naively select the b highest-scoring points according to a standard
acquisition function. However  such naive batch construction methods still result in highly correlated
queries [5]. This issue is highlighted in Fig. 1  where both MAXENT (Fig. 1a) and BALD (Fig. 1b)
expend a large part of the budget on repeatedly choosing nearby points.

3 Bayesian batch active learning as sparse subset approximation

We propose a novel probabilistic batch AL algorithm that mitigates the issues mentioned above. Our
method generates batches that cover the entire data manifold (Fig. 1c)  and  as we will show later  are
highly effective for performing posterior inference over the model parameters. Note that while our
approach alternates between acquiring data points and updating the model for several iterations in
practice  we restrict the derivations hereafter to a single iteration for simplicity.
The key idea behind our batch AL approach is to choose a batch D(cid:48)  such that the updated log
posterior log p(θ|D0 ∪ D(cid:48)) best approximates the complete data log posterior log p(θ|D0 ∪ Dp). In
AL  we do not have access to the labels before querying the pool set. We therefore take expectation

w.r.t. the current predictive posterior distribution p(Yp|Xp D0) =(cid:82) p(Yp|Xp  θ) p(θ|D0)dθ. The

expected complete data log posterior is thus

E
Yp

[log p(θ|D0 ∪ (Xp Yp))] = E
Yp

[log p(θ|D0) + log p(Yp|Xp  θ) − log p(Yp|Xp D0)]

= log p(θ|D0) + E
Yp

[log p(Yp|Xp  θ)] + H[Yp|Xp D0]

= log p(θ|D0) +

[log p(ym|xm  θ)] + H [ym|xm D0]

(cid:32)

M(cid:88)

m=1

E
ym

(cid:124)

(cid:123)(cid:122)

Lm(θ)

(cid:33)

 

(4)

(cid:125)

where the ﬁrst equality uses Bayes’ rule (cf. Eq. (1))  and the third equality assumes conditional
independence of the outputs given the inputs. This assumption holds for the type of factorized
predictive posteriors we consider  e.g. as induced by Gaussian or Multinomial likelihood models.

Batch construction as sparse approximation Taking inspiration from Bayesian coresets [9  10] 
we re-cast Bayesian batch construction as a sparse approximation to the expected complete data log
posterior. Since the ﬁrst term in Eq. (4) only depends on D0  it sufﬁces to choose the batch that
m Lm(θ). Similar to Campbell and Broderick [10]  we view Lm : Θ (cid:55)→ R and
m Lm as vectors in function space. Letting w ∈ {0  1}M be a weight vector indicating which
m wmLm (with slight abuse of notation) 

best approximates(cid:80)
L =(cid:80)
points to include in the AL batch  and denoting L(w) =(cid:80)

we convert the problem of constructing a batch to a sparse subset approximation problem  i.e.

w∗ = minimize

w

(cid:107)L − L(w)(cid:107)2

subject to wm ∈ {0  1} ∀m 

1m ≤ b.

(5)

(cid:88)

m

Intuitively  Eq. (5) captures the key objective of our framework: a “good" approximation to L implies
that the resulting posterior will be close to the (expected) posterior had we observed the complete pool
set. Since solving Eq. (5) is generally intractable  in what follows we propose a generic algorithm to
efﬁciently ﬁnd an approximate solution.

Inner products and Hilbert spaces We propose to construct our batches by solving Eq. (5) in a
Hilbert space induced by an inner product (cid:104)Ln Lm(cid:105) between function vectors  with associated norm
(cid:107) · (cid:107). Below  we discuss the choice of speciﬁc inner products. Importantly  this choice introduces a
notion of directionality into the optimization procedure  enabling our approach to adaptively construct
query batches while implicitly accounting for similarity between selected points.

3

(cid:28)

(cid:29)

N(cid:88)

m=1

and replace the cardinality constraint with a polytope constraint. Let σm = (cid:107)Lm(cid:107)  σ =(cid:80)

Frank-Wolfe optimization To approximately solve the optimization problem in Eq. (5) we follow
the work of Campbell and Broderick [10]  i.e. we relax the binary weight constraint to be non-negative
m σm  and
K ∈ RM×M be a kernel matrix with Kmn = (cid:104)Lm Ln(cid:105). The relaxed optimization problem is

minimize

w

(1 − w)T K (1 − w)

subject to wm ≥ 0 ∀m 

wmσm = σ 

(6)

(cid:88)

m

where we used (cid:107)L − L(w)(cid:107)2 = (1 − w)T K (1 − w). The polytope has vertices {σ/σm 1m}M
m=1
and contains the point w = [1  1  . . .   1]T . Eq. (6) can be solved efﬁciently using the Frank-Wolfe
algorithm [11]  yielding the optimal weights w∗ after b iterations. The complete AL procedure 
Active Bayesian CoreSets with Frank-Wolfe optimization (ACS-FW)  is outlined in Appendix A (see
Algorithm A.1). The key computation in Algorithm A.1 (Line 6) is

L − L(w) 

Ln

1
σn

=

1
σn

(1 − wm)(cid:104)Lm Ln(cid:105)  

(7)

which only depends on the inner products (cid:104)Lm Ln(cid:105) and norms σn = (cid:107)Ln(cid:107). At each iteration  the
algorithm greedily selects the vector Lf most aligned with the residual error L − L(w). The weights
w are then updated according to a line search along the f th vertex of the polytope (recall that the
optimum of a convex objective over a polytope—as in Eq. (6)—is attained at the vertices)  which by
construction is the f th-coordinate unit vector. This corresponds to adding at most one data point to
the batch in every iteration. Since the algorithm allows to re-select indices from previous iterations 
the resulting weight vector has ≤ b non-zero entries. Empirically  we ﬁnd that this property leads to
smaller batches as more data points are acquired.
Since it is non-trivial to leverage the continuous weights returned by the Frank-Wolfe algorithm in a
principled way  the ﬁnal step of our algorithm is to project the weights back to the feasible space 
i.e. set ˜w∗
m > 0  and 0 otherwise. While this projection step increases the approximation
error  we show in Section 7 that our method is still effective in practice. We leave the exploration of
alternative optimization procedures that do not require this projection step to future work.
Choice of inner products We employ weighted inner products of the form (cid:104)Ln Lm(cid:105)ˆπ =
Eˆπ [(cid:104)Ln Lm(cid:105)]  where we choose ˆπ to be the current posterior p(θ|D0). We consider two spe-
ciﬁc inner products with desirable analytical and computational properties; however  other choices
are possible. First  we deﬁne the weighted Fisher inner product [17  10]

m = 1 if w∗

(cid:2)∇θLn(θ)T∇θLm(θ)(cid:3)  

(cid:104)Ln Lm(cid:105)ˆπ F = E

ˆπ

(8)

which is reminiscent of information-theoretic quantities but requires taking gradients of the expected
log-likelihood terms1 w.r.t. the parameters. In Section 4  we show that for speciﬁc models this choice
leads to simple  interpretable expressions that are closely related to existing AL procedures.
An alternative choice that lifts the restriction of having to compute gradients is the weighted Euclidean
inner product  which considers the marginal likelihood of data points [10] 

(cid:104)Ln Lm(cid:105)ˆπ 2 = E

ˆπ

[Ln(θ)Lm(θ)] .

(9)

The key advantage of this inner product is that it only requires tractable likelihood computations. In
Section 5 this will prove highly useful in providing a black-box method for these computations in any
model (that has a tractable likelihood) using random feature projections.
Method overview In summary  we (i) consider the Lm in Eq. (4) as vectors in function space
and re-cast batch construction as a sparse approximation to the full data log posterior from Eq. (5);
(ii) replace the cardinality constraint with a polytope constraint in a Hilbert space  and relax the
binary weight constraint to non-negativity; (iii) solve the resulting optimization problem in Eq. (6)
using Algorithm A.1; (iv) construct the AL batch by including all points xm ∈ Xp with w∗

m > 0.

1Note that the entropy term in Lm (see Eq. (4)) vanishes under this norm as the gradient for θ is zero.

4

4 Analytic expressions for linear models

In this section  we use the weighted Fisher inner product from Eq. (8) to derive closed-form expres-
sions of the key quantities of our algorithm for two types of models: Bayesian linear regression and
probit regression. Although the considered models are relatively simple  they can be used ﬂexibly to
construct more powerful models that still admit closed-form solutions. For example  in Section 7
we demonstrate how using neural linear models [18  19] allows to perform efﬁcient AL on several
regression tasks. We consider arbitrary models and inference procedures in Section 5.

Linear regression Consider the following model for scalar Bayesian linear regression 

yn = θT xn + n 

n ∼ N (0  σ2
0) 

θ ∼ p(θ) 

(10)

N(cid:0)θ; (X T X + σ2

where p(θ) is a factorized Gaussian prior with unit variance; extensions to richer Gaussian priors are
straightforward. Given a labeled dataset D0  the posterior is given in closed form as p(θ|D0  σ2
0) =
0I)−1. For this model  a closed-form

0I)−1X T y  Σθ

0(X T X + σ2

(cid:1) with Σθ = σ2

expression for the inner product in Eq. (8) is

xT

(cid:104)Ln Lm(cid:105)ˆπ F =
where ˆπ is chosen to be the posterior p(θ|D0  σ2
0). See Appendix B.1 for details on this derivation.
We can make a direct comparison with BALD [2  4] by treating the squared norm of a data point
with itself as a greedy acquisition function 2 αACS(xn;D0) = (cid:104)Ln Ln(cid:105)ˆπ F   yielding 
n Σθxn

n xm
σ4
0

n Σθxm 

(cid:18)

(cid:19)

(11)

xT

xT

xT

αACS(xn;D0) =

n xn
σ4
0

xT

n Σθxn 

αBALD(xn;D0) =

1
2

log

1 +

σ2
0

.

(12)

n xn. Ignoring the xT

n Σθxn  but BALD wraps the term in a logarithm whereas
The two functions share the term xT
αACS scales it by xT
n xn term in αACS makes the two quantities proportional—
exp(2αBALD(xn;D0)) ∝ αACS(xn;D0)—and thus equivalent under a greedy maximizer. Another
n Σθxn is very similar to a leverage score [20–22]  which is computed as
observation is that xT
n (X T X)−1xn and quantiﬁes the degree to which xn inﬂuences the least-squares solution. We
xT
can then interpret the xT
n xn term in αACS as allowing for more contribution from the current instance
xn than BALD or leverage scores would.

p(yn|xn  θ) = Ber(cid:0)Φ(θT xn)(cid:1)  

Probit regression Consider the following model for Bayesian probit regression 

(13)
where Φ(·) represents the standard Normal cumulative density function (cdf)  and p(θ) is assumed to
be a factorized Gaussian with unit variance. We obtain a closed-form solution for Eq. (8)  i.e.

θ ∼ p(θ) 

(cid:104)Ln Lm(cid:105)ˆπ F = xT
µT

θ xi

(cid:112)1 + xT

i Σθxi

n xm

BvN (ζn  ζm  ρn m) − Φ(ζn)Φ(ζm)

(cid:112)1 + xT

xT

n Σθxm

(cid:112)1 + xT

n Σθxn

mΣθxm

ζi =

ρn m =

(14)

(cid:17)

 

(cid:16)

where BvN(·) is the bi-variate Normal cdf. We again view αACS(xn;D0) = (cid:104)Ln Ln(cid:105)ˆπ F as an
acquisition function and re-write Eq. (14) as

(cid:32)

(cid:32)

(cid:33)(cid:33)

(cid:112)1 + 2xT

1

n Σθxn

 

(15)

αACS(xn;D0) = xT

n xn

Φ (ζn) (1 − Φ (ζn)) − 2T

ζn 

where T(· ·) is Owen’s T function [23]. See Appendix B.2 for the full derivation of Eqs. (14) and (15).
Eq. (15) has a simple and intuitive form that accounts for the magnitude of the input vector and a
regularized term for the predictive variance.

2We only introduce αACS to compare to other acquisition functions; in practice we use Algorithm A.1.

5

5 Random projections for non-linear models

In Section 4  we have derived closed-form expressions of the weighted Fisher inner product for two
speciﬁc types of models. However  this approach suffers from two shortcomings. First  it is limited to
models for which the inner product can be evaluated in closed form  e.g. linear regression or probit

regression. Second  the resulting algorithm requires O(cid:0)|P|2(cid:1) computations to construct a batch 

restricting our approach to moderately-sized pool sets.
We address both of these issues using random feature projections  allowing us to approximate the
key quantities required for the batch construction. In Algorithm A.2  we introduce a procedure that
works for any model with a tractable likelihood  scaling only linearly in the pool set size |P|. To keep
the exposition simple  we consider models in which the expectation of Ln(θ) w.r.t. p(yn|xn D0) is
tractable  but we stress that our algorithm could work with sampling for that expectation as well.
While it is easy to construct a projection for the weighted Fisher inner product [10]  its dependence
on the number of model parameters through the gradient makes it difﬁcult to scale it to more complex
models. We therefore only consider projections for the weighted Euclidean inner product from
Eq. (9)  which we found to perform comparably in practice. The appropriate projection is [10]

(16)
i.e. ˆLn represents the J-dimensional projection of Ln in Euclidean space. Given this projection  we
are able to approximate inner products as dot products between vectors 

[Ln(θ1) ···  Ln(θJ )]T  

ˆLn =

1√
J

θj ∼ ˆπ 

(cid:104)Ln Lm(cid:105)ˆπ 2 ≈ ˆLT

ˆLm 

n

(17)
ˆLm can be viewed as an unbiased sample estimator of (cid:104)Ln Lm(cid:105)ˆπ 2 using J Monte Carlo
where ˆLT
samples from the posterior ˆπ. Importantly  Eq. (16) can be calculated for any model with a tractable
likelihood. Since in practice we only require inner products of the form (cid:104)L − L(w) Ln/σn(cid:105)ˆπ 2 
batches can be efﬁciently constructed in O(|P|J) time. As we show in Section 7  this enables us to
scale our algorithm up to pool sets comprising hundreds of thousands of examples.

n

6 Related work

Bayesian AL approaches attempt to query points that maximally reduce model uncertainty. Common
heuristics to this intractable problem greedily choose points where the predictive posterior is most
uncertain  e.g. maximum variance and maximum entropy [3]  or that maximally improve the expected
information gain [2  4]. Scaling these methods to the batch setting in a principled way is difﬁcult for
complex  non-linear models. Recent work on improving inference for AL with deep probabilistic
models [24  13] used datasets with at most 10 000 data points and few model updates.
Consequently  there has been great interest in batch AL recently. The literature is dominated by
non-probabilistic methods  which commonly trade off diversity and uncertainty. Many approaches are
model-speciﬁc  e.g. for linear regression [25]  logistic regression [26  27]  and k-nearest neighbors
[28]; our method works for any model with a tractable likelihood. Others [6–8] follow optimization-
based approaches that require optimization over a large number of variables. As these methods scale
quadratically with the number of data points  they are limited to smaller pool sets.
Probabilistic batch methods mostly focus on Bayesian optimization problems. Several approaches
select the batch that jointly optimizes the acquisition function [29  30]. As they scale poorly with
the batch size  greedy batch construction algorithms are often used instead [31–34]. A common
strategy is to impute the labels of the selected data points and update the model accordingly [33].
Our approach also uses the model to predict the labels  but importantly it does not require to update
the model after every data point. Moreover  most of the methods in Bayesian optimization employ
Gaussian process models. While AL with non-parametric models [35] could beneﬁt from that work 
scaling such models to large datasets remains challenging. Our work therefore provides the ﬁrst
principled  scalable and model-agnostic Bayesian batch AL approach.
Similar to us  Sener and Savarese [5] formulate AL as a core-set selection problem. They construct
batches by solving a k-center problem  attempting to minimize the maximum distance to one of the
k queried data points. Since this approach heavily relies on the geometry in data space  it requires

6

BALD

ACS-FW

(a) t = 1

(b) t = 2

(c) t = 3

(d) t = 10

(e) t = 1

(f) t = 2

(g) t = 3

(h) t = 10

Figure 2: Batches constructed by BALD (top) and ACS-FW (bottom) on a probit regression task.
10 training data points (red  blue) were sampled from a standard bi-variate Normal  and labeled
according to p(y|x) = Ber(5x1 + 0x2). At each step t  one unlabeled point (black cross) is queried
from the pool set (colored according to acquisition function4; bright is higher). The current mean
decision boundary of the model is shown as a black line. Best viewed in color.

an expressive feature representation. For example  Sener and Savarese [5] only consider ConvNet
representations learned on highly structured image data. In contrast  our work is inspired by Bayesian
coresets [9  10]  which enable scalable Bayesian inference by approximating the log-likelihood of a
labeled dataset with a sparse weighted subset thereof. Consequently  our method is less reliant on a
structured feature space and only requires to evaluate log-likelihood terms.

7 Experiments and results

We perform experiments3 to answer the following questions: (1) does our approach avoid correlated
queries  (2) is our method competitive with greedy methods in the small-data regime  and (3) does
our method scale to large datasets and models? We address questions (1) and (2) on several linear
and probit regression tasks using the closed-form solutions derived in Section 4  and question (3)
on large-scale regression and classiﬁcation datasets by leveraging the projections from Section 5.
Finally  we provide a runtime evaluation for all regression experiments. Full experimental details are
deferred to Appendix C.

Does our approach avoid correlated queries? In Fig. 1  we have seen that traditional AL methods
are prone to correlated queries. To investigate this further  in Fig. 2 we compare batches selected
by ACS-FW and BALD on a simple probit regression task. Since BALD has no explicit batch
construction mechanism  we naively choose the b = 10 most informative points according to BALD.
While the BALD acquisition function does not change during batch construction  αACS(xn;D0)
rotates after each selected data point. This provides further intuition about why ACS-FW is able to
spread the batch in data space  avoiding the strongly correlated queries that BALD produces.

Is our method competitive with greedy methods in the small-data regime? We evaluate the
performance of ACS-FW on several UCI regression datasets. We compare against (i) RANDOM:
select points randomly; (ii) MAXENT: naively construct batch using top b points according to
maximum entropy criterion (equivalent to BALD in this case); (iii) MAXENT-SG: use MAXENT
with sequential greedy strategy (i.e. b = 1); (iv) MAXENT-I: sequentially acquire single data
point  impute missing label and update model accordingly. Starting with 20 labeled points sampled
randomly from the pool set  we use each AL method to iteratively grow the training dataset by
requesting batches of size b = 10 until the budget of 100 queries is exhausted. To guarantee fair
comparisons  all methods use the same neural linear model  i.e. a Bayesian linear regression model
with a deterministic neural network feature extractor [19]. In this setting  posterior inference can be

3Source code is available at https://github.com/rpinsler/active-bayesian-coresets.
4We use αACS (see Eq. (15)) as an acquisition function for ACS-FW only for the sake of visualization.

7

Table 1: Final test RMSE on UCI regression datasets averaged over 40 (year: 5) seeds. MAXENT-I
and MAXENT-SG require order(s) of magnitudes more model updates and are thus not directly
comparable.
N
308
506
768
9568
515 345

ACS-FW
1.031±0.0438
3.799±0.0858
0.855±0.0259
4.984±0.0366
12.194±0.0596

RANDOM
1.272±0.0593
4.068±0.0852
0.959±0.0337
5.108±0.0468
13.165±0.0307

MAXENT

0.923±0.0319
3.640±0.0652
1.443±0.0857
5.022±0.0428
13.030±0.0975

MAXENT-I
0.865±0.0276
3.467±0.0676
0.927±0.0461
4.834±0.0313
N/A

MAXENT-SG
0.971±0.0350
3.458±0.0682
1.055±0.0740
4.855±0.0339
N/A

yacht
boston
energy
power
year

d
6
13
8
4
90

Table 2: Runtime in seconds on UCI regression datasets averaged over 40 (year: 5) seeds. We
report mean batch construction time (BT/it.) and total time (TT/it.) per AL iteration  as well as total
cumulative time (total). MAXENT-I requires order(s) of magnitudes more model updates and is thus
not directly comparable.

RANDOM

BT/it. TT/it.
8.9
12.4
12.1
9.4
381.2

0.0
0.0
0.0
0.4
30.2

total
88.6
123.6
121.4
94.0
3811.6

BT/it.
1.3
2.4
3.9
53.0
3391.5

yacht
boston
energy
power
year

MAXENT
TT/it.
10.2
14.5
16.0
61.7
3746.5

total
101.7
144.8
159.6
617.0
37 464.6

ACS-FW

MAXENT-I

BT/it. TT/it.
9.1
12.4
12.6
10.2
463.8

0.0
0.1
0.1
0.8
53.0

total
107.2
132.7
137.8
179.8
28 475.2

BT/it. TT/it.
105.7
12.3
157.9
23.5
37.5
170.5
609.1
517.3
N/A
N/A

total
1057.4
1578.6
1704.9
6090.7
N/A

done in closed form [19]. The model is re-trained for 1000 epochs after every AL iteration using
Adam [36]. After each iteration  we evaluate RMSE on a held-out set. Experiments are repeated for
40 seeds  using randomized 80/20% train-test splits. We also include a medium-scale experiment
on power that follows the same protocol; however  for ACS-FW we use projections instead of the
closed-form solutions as they yield improved performance and are faster. Further details  including
architectures and learning rates  are in Appendix C.
The results are summarized in Table 1. ACS-FW consistently outperforms RANDOM by a large
margin (unlike MAXENT)  and is mostly on par with MAXENT on smaller datasets. While the results
are encouraging  greedy methods such as MAXENT-SG and MAXENT-I still often yield better results
in these small-data regimes. We conjecture that this is because single data points do have signiﬁcant
impact on the posterior. The beneﬁts of using ACS-FW become clearer with increasing dataset size:
as shown in Fig. 3  ACS-FW achieves much more data-efﬁcient learning on larger datasets.

(a) yacht

(b) energy

(c) year

Figure 3: Test RMSE on UCI regression datasets averaged over 40 (a-b) and 5 (c) seeds during AL.
Error bars denote two standard errors.

Does our method scale to large datasets and models? Leveraging the projections from Section 5 
we apply ACS-FW to large-scale datasets and complex models. We demonstrate the beneﬁts of our
approach on year  a UCI regression dataset with ca. 515 000 data points  and on the classiﬁcation
datasets cifar10  SVHN and Fashion MNIST. Methods requiring model updates after every data point
(e.g. MAXENT-SG  MAXENT-I) are impractical in these settings due to their excessive runtime.
For year  we again use a neural linear model  start with 200 labeled points and allow for batches of
size b = 1000 until the budget of 10 000 queries is exhausted. We average the results over 5 seeds 

8

020406080100Number of samples from pool set0.01.53.04.56.07.5RMSE020406080100Number of samples from pool set0.51.52.53.54.5RMSE0200040006000800010000Number of samples from pool set121416182022RMSEACS-FW (ours)MaxEntRandom(a) cifar10

(b) SVHN

(c) Fashion MNIST

Figure 4: Test accuracy on classiﬁcation tasks over 5 seeds. Error bars denote two standard errors.

using randomized 80/20% train-test splits. As can be seen in Fig. 3c  our approach signiﬁcantly
outperforms both RANDOM and MAXENT during the entire AL process.
For the classiﬁcation experiments  we start with 1000 (cifar10: 5000) labeled points and request
batches of size b = 3000 (5000)  up to a budget of 12 000 (20 000) points. We compare to RANDOM 
MAXENT and BALD  as well as two batch AL algorithms  namely K-MEDOIDS and K-CENTER
[5]. Performance is measured in terms of accuracy on a holdout test set comprising 10 000 (Fashion
MNIST: 26 032  as is standard) points  with the remainder used for training. We use a neural linear
model with a ResNet18 [16] feature extractor  trained from scratch at every AL iteration for 250
epochs using Adam [36]. Since posterior inference is intractable in the multi-class setting  we resort
to variational inference with mean-ﬁeld Gaussian approximations [37  38].
Fig. 4 demonstrates that in all cases ACS-FW signiﬁcantly outperforms RANDOM  which is a strong
baseline in AL [5  13  24]. Somewhat surprisingly  we ﬁnd that the probabilistic methods (BALD
and MAXENT)  provide strong baselines as well  and consistently outperform RANDOM. We discuss
this point and provide further experimental results in Appendix D. Finally  Fig. 4 demonstrates
that in all cases ACS-FW performs at least as well as its competitors  including state-of-the-art
non-probabilistic batch AL approaches such as K-CENTER. These results demonstrate that ACS-FW
can usefully apply probabilistic reasoning to AL at scale  without any sacriﬁce in performance.

Runtime Evaluation Runtime comparisons between different AL methods on the UCI regression
datasets are shown in Table 2. For methods with ﬁxed AL batch size b (RANDOM  MAXENT and
MAXENT-I)  the number of AL iterations is given by the total budget divided by b (e.g. 100/10 = 10
for yacht). Thus  the total cumulative time (total) is given by the total time per AL iteration (TT/it.)
times the number of iterations. MAXENT-I iteratively constructs the batch by selecting a single data
point  imputing its label  and updating the model; therefore the batch construction time (BT/it.) and
the total time per AL iteration take roughly b times as long as for MAXENT (e.g. 10x for yacht). This
approach becomes infeasible for very large batch sizes (e.g. 1000 for year). The same holds true for
MAXENT-SG  which we have omitted here as the runtimes are similar to MAXENT-I. ACS-FW
constructs batches of variable size  and hence the number of iterations varies.
As shown in Table 2  the batch construction times of ACS-FW are negligble compared to the total
training times per AL iteration. Although ACS-FW requires more AL iterations than the other
methods  the total cumulative runtimes are on par with MAXENT. Note that both MAXENT and
MAXENT-I require to compute the entropy of a Student’s T distribution  for which no batch version
was available in PyTorch as we performed the experiments. Parallelizing this computation would
likely further speed up the batch construction process.

8 Conclusion and future work

We have introduced a novel Bayesian batch AL approach based on sparse subset approximations.
Our methodology yields intuitive closed-form solutions  revealing its connection to BALD as well as
leverage scores. Yet more importantly  our approach admits relaxations (i.e. random projections) that
allow it to tackle challenging large-scale AL problems with general non-linear probabilistic models.
Leveraging the Frank-Wolfe weights in a principled way and investigating how this method interacts
with alternative approximate inference procedures are interesting avenues for future work.

9

05000100001500020000Number of samples from pool set0.700.730.760.790.820.850.88Accuracy020004000600080001000012000Number of samples from pool set0.700.750.800.850.900.95Accuracy020004000600080001000012000Number of samples from pool set0.820.840.860.880.900.920.94AccuracyACS-FW (ours)BALDK-CenterK-MedoidsMaxEntRandomAcknowledgments

Robert Pinsler receives funding from iCASE grant #1950384 with support from Nokia. Jonathan
Gordon  Eric Nalisnick and José Miguel Hernández-Lobato were funded by Samsung Research 
Samsung Electronics Co.  Seoul  Republic of Korea. We thank Adrià Garriga-Alonso  James
Requeima  Marton Havasi  Carl Edward Rasmussen and Trevor Campbell for helpful feedback and
discussions.

References
[1] Burr Settles. Active learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine

Learning  6(1):1–114  2012.

[2] David JC MacKay. Information-based objective functions for active data selection. Neural

computation  4(4):590–604  1992.

[3] Claude Elwood Shannon. A mathematical theory of communication. Bell System Technical

Journal  27(3):379–423  1948.

[4] Neil Houlsby  Ferenc Huszár  Zoubin Ghahramani  and Máté Lengyel. Bayesian active learning

for classiﬁcation and preference learning. arXiv Preprint arXiv:1112.5745  2011.

[5] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set

approach. In International Conference on Learning Representations  2018.

[6] Ehsan Elhamifar  Guillermo Sapiro  Allen Yang  and S Shankar Sasrty. A convex optimization
framework for active learning. In IEEE International Conference on Computer Vision  pages
209–216  2013.

[7] Yuhong Guo. Active instance sampling via matrix partition. In Advances in Neural Information

Processing Systems  pages 802–810  2010.

[8] Yi Yang  Zhigang Ma  Feiping Nie  Xiaojun Chang  and Alexander G Hauptmann. Multi-class
active learning by uncertainty sampling with diversity maximization. International Journal of
Computer Vision  113(2):113–127  2015.

[9] Jonathan Huggins  Trevor Campbell  and Tamara Broderick. Coresets for scalable Bayesian
logistic regression. In Advances in Neural Information Processing Systems  pages 4080–4088 
2016.

[10] Trevor Campbell and Tamara Broderick. Automated scalable Bayesian inference via Hilbert

coresets. The Journal of Machine Learning Research  20(1):551–588  2019.

[11] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research

Logistics Quarterly  3(1-2):95–110  1956.

[12] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine

Learning Research  9(Nov):2579–2605  2008.

[13] Yarin Gal  Riashat Islam  and Zoubin Ghahramani. Deep Bayesian active learning with image

data. arXiv Preprint arXiv:1703.02910  2017.

[14] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in
active learning and stochastic optimization. Journal of Artiﬁcial Intelligence Research  42:
427–486  2011.

[15] Sanjoy Dasgupta. Analysis of a greedy active learning strategy.

Information Processing Systems  pages 337–344  2005.

In Advances in Neural

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

10

[17] Oliver Johnson and Andrew Barron. Fisher information inequalities and the central limit

theorem. Probability Theory and Related Fields  129(3):391–409  2004.

[18] Andrew Gordon Wilson  Zhiting Hu  Ruslan Salakhutdinov  and Eric P Xing. Deep kernel

learning. In Artiﬁcial Intelligence and Statistics  pages 370–378  2016.

[19] Carlos Riquelme  George Tucker  and Jasper Snoek. Deep Bayesian bandits showdown. In

International Conference on Learning Representations  2018.

[20] Petros Drineas  Malik Magdon-Ismail  Michael W Mahoney  and David P Woodruff. Fast
approximation of matrix coherence and statistical leverage. Journal of Machine Learning
Research  13(Dec):3475–3506  2012.

[21] Ping Ma  Michael W Mahoney  and Bin Yu. A statistical perspective on algorithmic leveraging.

The Journal of Machine Learning Research  16(1):861–911  2015.

[22] Michal Derezinski  Manfred K Warmuth  and Daniel J Hsu. Leveraged volume sampling for
linear regression. In Advances in Neural Information Processing Systems  pages 2510–2519 
2018.

[23] Donald B Owen. Tables for computing bivariate normal probabilities. The Annals of Mathemat-

ical Statistics  27(4):1075–1090  1956.

[24] José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable
learning of Bayesian neural networks. In International Conference on Machine Learning  pages
1861–1869  2015.

[25] Kai Yu  Jinbo Bi  and Volker Tresp. Active learning via transductive experimental design. In

International Conference on Machine Learning  pages 1081–1088  2006.

[26] Steven CH Hoi  Rong Jin  Jianke Zhu  and Michael R Lyu. Batch mode active learning and its
application to medical image classiﬁcation. In International Conference on Machine Learning 
pages 417–424  2006.

[27] Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In Advances in

Neural Information Processing Systems  pages 593–600  2008.

[28] Kai Wei  Rishabh Iyer  and Jeff Bilmes. Submodularity in data subset selection and active

learning. In International Conference on Machine Learning  pages 1954–1963  2015.

[29] Clément Chevalier and David Ginsbourger. Fast computation of the multi-points expected
improvement with applications in batch selection. In International Conference on Learning and
Intelligent Optimization  pages 59–69  2013.

[30] Amar Shah and Zoubin Ghahramani. Parallel predictive entropy search for batch global
optimization of expensive objective functions. In Advances in Neural Information Processing
Systems  pages 3330–3338  2015.

[31] Javad Azimi  Alan Fern  and Xiaoli Z Fern. Batch Bayesian optimization via simulation

matching. In Advances in Neural Information Processing Systems  pages 109–117  2010.

[32] Emile Contal  David Buffoni  Alexandre Robicquet  and Nicolas Vayatis. Parallel gaussian
process optimization with upper conﬁdence bound and pure exploration. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases  pages 225–240 
2013.

[33] Thomas Desautels  Andreas Krause  and Joel W Burdick. Parallelizing exploration-exploitation
tradeoffs in gaussian process bandit optimization. The Journal of Machine Learning Research 
15(1):3873–3923  2014.

[34] Javier González  Zhenwen Dai  Philipp Hennig  and Neil Lawrence. Batch Bayesian optimiza-

tion via local penalization. In Artiﬁcial Intelligence and Statistics  pages 648–657  2016.

11

[35] Ashish Kapoor  Kristen Grauman  Raquel Urtasun  and Trevor Darrell. Active learning with
Gaussian processes for object categorization. In IEEE International Conference on Computer
Vision  pages 1–8  2007.

[36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv Preprint

arXiv:1412.6980  2014.

[37] Martin J Wainwright  Michael I Jordan  et al. Graphical models  exponential families  and
variational inference. Foundations and Trends R(cid:13) in Machine Learning  1(1–2):1–305  2008.
[38] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty

in neural networks. arXiv Preprint arXiv:1505.05424  2015.

12

,Robert Pinsler
Jonathan Gordon
Eric Nalisnick
José Miguel Hernández-Lobato