2019,A Communication Efficient Stochastic Multi-Block Alternating Direction Method of Multipliers,The alternating direction method of multipliers (ADMM) has recently received tremendous interests for distributed large scale optimization in machine learning  statistics  multi-agent networks and related applications. In this paper  we propose a new parallel multi-block stochastic ADMM for distributed stochastic optimization  where each node is only required to perform simple stochastic gradient descent updates. The proposed ADMM is fully parallel  can solve problems with arbitrary block structures  and has a convergence rate comparable to or better than existing state-of-the-art ADMM methods for stochastic optimization. Existing stochastic (or deterministic) ADMMs require each node to exchange its updated primal variables across nodes at each iteration and hence cause significant amount of communication overhead. Existing ADMMs require roughly the same number of inter-node communication rounds as the number of in-node computation rounds. In contrast  the number of communication rounds required by our new ADMM is only the square root of the number of computation rounds.,A Communication Efﬁcient Stochastic Multi-Block

Alternating Direction Method of Multipliers

Hao Yu
Amazon

eeyuhao@gmail.com

Abstract

The alternating direction method of multipliers (ADMM) has recently received
tremendous interests for distributed large scale optimization in machine learning 
statistics  multi-agent networks and related applications. In this paper  we propose a
new parallel multi-block stochastic ADMM for distributed stochastic optimization 
where each node is only required to perform simple stochastic gradient descent
updates. The proposed ADMM is fully parallel  can solve problems with arbitrary
block structures  and has a convergence rate comparable to or better than existing
state-of-the-art ADMM methods for stochastic optimization. Existing stochastic
(or deterministic) ADMMs require each node to exchange its updated primal
variables across nodes at each iteration and hence cause signiﬁcant amounts of
communication overhead. Existing ADMMs require roughly the same number of
inter-node communication rounds as the number of in-node computation rounds.
In contrast  the number of communication rounds required by our new ADMM is
only the square root of the number of computation rounds.

Introduction

1
Fix integer N ≥ 2. Consider multi-block linearly constrained stochastic convex programs given by:

N(cid:88)

N(cid:88)

min

xi∈Xi ∀i

f (x) =

i=1

i=1

fi(xi) s.t.

Aixi = b 

(1)

where xi ∈ Rdi   Ai ∈ Rm×di  b ∈ Rm  Xi ⊆ Rdi are closed convex sets  and fi(xi) =
[x1; x2; . . . ; xN ] ∈ R(cid:80)N
Eξ[fi(xi; ξ)] are convex functions. To have a compact representation of (1)  we deﬁne x =
Rm×(cid:80)N
i=1 fi(xi) and A = [A1  A2  . . .   AN ] ∈

i=1 di  X =(cid:81)N
i=1 di. Note that constraint(cid:80)N

i=1 Xi  f (x) =(cid:80)N

i=1 Aixi = b now can be written as Ax = b.

The problem (1) captures many important applications in machine learning  network scheduling 
statistics and ﬁnance. For example  (stochastic) linear programs that are too huge to be solved over a
single node can be written as (1). To solve such large scale linear programs in a distributed manner 
we can save each Ai and fi(·) at a separate node and let each node iteratively solves smaller sub-
problems (with necessary inter-node communication). Another important application of formulation
(1) is the distributed consensus training of a machine learning model over N nodes [15  17  23]
described as follows:
• In an online training setup  i.i.d. realizations of fi(·; ξ) are sampled at each node. In an ofﬂine
training setup  fi(xi) = Eξ[fi(xi; ξ)] are approximated by 1
j=1 fij(xi) where Ni is the
number of training samples at node i and each fij(·) represents one training sample.
• To enforce all N nodes are training the same model  our constraint Ax = b is given by xi = xj
for all i (cid:54)= j ∈ {1  2  . . .   N}. (In fact  we only need such constraints for pairs (i  j) that construct
a connected graph for all nodes.)

(cid:80)Ni

Ni

The Alternating Direction Method of Multipliers (ADMM) is an effective and popular method to
solve linearly constrained convex programs  especially distributed consensus optimiation [28  5] 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

since it often yields distributed implementations with low complexity [4]. Conventional ADMMs are
developed for the special case of problem (1) with N = 2 and/or deterministic fi(xi). To solve a
two-block problem (1) where f1 is a stochastic function and f2 is a deterministic function  previous
works [21  25  31  1] have developed stochastic (two-block) ADMMs to solve problem (1) with
N = 2. It is unclear whether these methods can be extended to solve the case N ≥ 3. In fact  even for
problem (1) where all fi(xi) are deterministic  [6] proves that the classical (two-block) ADMM  on
which the stochastic versions in [21  25] are built  converges for N = 2 but diverges for N ≥ 3. To
solve stochastic convex program (1) with N ≥ 3  randomized block coordinate updated ADMMs with
O(1/2) convergence are developed in [27  11]. Due to the challenging stochastic objective functions 
the convergence rate of stochastic ADMMs is fundamentally slower than deterministic ADMMs  i.e. 
O(1/2) v.s. O(1/) [13  7  11]. The O(1/2) convergence is optimal since it is optimal even for
unconstrained stochastic convex optimization without strong convexity [20]. However  in distributive
implementations of ADMMs  each node has to pass its most recent xi value to its neighbors or a
fusion center and then updates the dual variable λ. Existing stochastic ADMM methods [21  25  11]
require a communication step immediately after each xi computation step. In practice  the inter-node
communication over TCP/IP is much slower than in-node memory computations and often requires
additional set-up time such that communication overhead is the performance bottleneck of most
distributed optimization methods.
As a consequence  communication efﬁcient optimization recently attracted a lot of research interests
[29  14  24  15  17  18  23]. Work [17] proposes a primal-dual method that can solve problem (1)
with stochastic objective functions using O(1/2) computation iterations and O(1/) communication
iterations. However  the method in [17] requires each objective function fi(·) to satisfy the stringent
condition that there exists M such that fi(u) ≤ fi(v) + (cid:104)d  u − v(cid:105) + M(cid:107)u − v(cid:107) for any u  v and
d ∈ ∂fi(v) . Such a condition is more stringent than the smoothness when u and v are far apart from
each other. For example  the simple scalar smooth function f (x) = x2 does not satisfy this condition
over X = R. Work [18] proposes a communication efﬁcient method to solve deterministic convex
programs based on the quadratic penalty method and can obtain an -optimal solution with O(1/2+δ)
computation rounds (δ is a positive constant) and O(1/) communication rounds. For distributed
consensus optimization over a network  which can be formulated as a special case of problem (1)
where Ai and b are chosen to ensure all xi are identical  mixing or local averaging based methods
with fast convergence (and low communication overhead) are recently developed in [26  22  23  19].
Our Contributions: This paper proposes a new communication efﬁcient stochastic multi-block
ADMM which has communication rounds less frequently than computation rounds. For stochastic
convex programs with general convex objective functions  our algorithm can achieve an -solution
with O(1/2) computation1 rounds and O(1/) communication rounds. That is  our communication
efﬁcient ADMM has the same computation convergence rate as the ADMM in [11] but only requires
the square root of communication rounds required by the method in [11]. For stochastic convex
√
programs with strongly convex objective functions  our algorithm can achieve an -accuracy solution
with ˜O(1/) computation rounds and ˜O(1/
) communication rounds2. The fast computation
convergence (and even faster communication convergence) for strongly convex stochastic programs is
not possessed by the ADMM in [11]. When applying our new multi-block ADMM to the special case
of two-block problems  our algorithm has the same computation convergence as existing two-block
stochastic ADMM methods in [21  25  31  1]. However  the number of communication rounds used
by our ADMM is only the squared root of these previous methods.
Notations: This paper uses (cid:107)A(cid:107) to denote the spectral norm of matrix A; (cid:107)z(cid:107) to denote the Euclidean
norm of vector z; and (cid:104)y  z(cid:105) = yTz to denote the inner product of vectors y and z. If symmetric
matrix Q (cid:23) 0 is positive semi-deﬁnite  then we deﬁne (cid:107)z(cid:107)2
2 Formulation and New Algorithm
Following the convention in [8]  a function h(x) is said to be convex with modulus µ  or equivalently 
µ-convex  if h(x) − µ
2(cid:107)x(cid:107)2 is convex. The µ-convex deﬁnition uniﬁes the conventional deﬁnitions of
convexity and strong convexity. That is  a general convex function  which is not necessarily strongly
convex  is convex with modulus µ = 0; and a strongly convex function is convex with modulus µ > 0.
Throughout this paper  convex program (1) is assumed to satisfy the following standard assumption:

Q = zTQz for any vector z.

1A computation round of our algorithm is a just a single iteration of the SGD update.
2A logarithm factor log( 1

 ) is hidden in the notation ˜O(·).

2

∗

∗

). That is  x∗ is an optimal solution
) ∆=

∗ ∈ Rm is a Lagrange multiplier attaining strong duality q(λ
∗
  Ax − b(cid:105)} is the Lagrangian dual function.

Assumption 1. Convex program (1) has a saddle point (x∗  λ
and λ
inf{xi∈Xi ∀i}{f (x) + (cid:104)λ
Note that strong duality in Assumption 1 is often stated as its equivalent “KKT conditions”  e.g.  in
[7]. A mild sufﬁcient condition for Assumption 1 to hold is (1) has at least one feasible point and the
domain of each fi(xi) includes Xi as an interior [3].
Assume unbiased subgradients Gi(xi; ξ) satisfying Eξ[Gi(xi; ξ)] = ∂fi(xi) ∀xi ∈ Xi
[G1(x1; ξ)T  . . .   GN (xN ; ξ)T]T ∈ R(cid:80)N
for each function fi(xi) can be sampled. Denote the stacked column vector G(x; ξ) ∆=

i=1 di. We have Eξ[G(x; ξ)] = ∂f (x).

) = f (x∗)  where q(λ

∗

Consider the communication efﬁcient stochastic multi-block ADMM described in Algorithm 1.
Since fi(xi) are stochastic  φi(xi) deﬁned in (2) is fundamentally unknown. However  each φi(xi)
is ν(t)-convex and its unbiased stochastic subgradient is available as long as we have unbiased
stochastic subgradients of fi(xi). The sub-procedure STO-LOCAL involved in Algorithm 1 is a
simple stochastic subgradient decent (SGD) procedure (with particular choices of parameters  starting
points and averaging schemes) to minimize φ(t)

i (·) over set Xi and is described in Algorithm 2.

Algorithm 1 Two-Layer Communication Efﬁcient ADMM
1: Input: Algorithm parameters T   {ρ(t)}t≥1  {ν(t)}t≥1 and {K (t)}t≥1.
2: Initialize arbitrary y(0)
3: while t ≤ T do
4:

i ∈ Xi ∀i  r(0) =(cid:80)N
i (xi) ∆=fi(xi) + ρ(t)(cid:10)r(t−1) +

Each node i deﬁnes

i=1 Aiy(0)

ρ(t) λ(t−1)  Aixi − b

(cid:11) +

φ(t)

N

1

i − b  λ(0) = 0  and t = 1.

ν(t)
2

(cid:107)xi − y(t−1)

i

(cid:107)2

and in parallel updates x(t)
i
  y(t)

(x(t)

i

  y(t)

i using local sub-procedure Algorithm 2 via

i ) = STO-LOCAL(φ(t)

  K (t))

5:

Each node i passes x(t)
i

r(t) via

and y(t)

i between nodes or to a parameter server. Update λ(t) and

i

i (·) Xi  y(t−1)
(cid:17)
i − b

Aix(t)

λ(t) =λ(t−1) + ρ(t)(cid:16) N(cid:88)

i=1

Aiy(t)

i − b.

r(t) =

N(cid:88)

i=1

(2)

(3)

(4)

(5)

(6)

6:
7: end while
8: Output: x(T ) =

Update t ← t + 1.
1(cid:80)T

t=1 ρ(t)

(cid:80)T

t=1 ρ(t)x(t)

Algorithm 2 STO-LOCAL(φ(z) Z  zinit  K)
1: Input: µ: strong convexity modulus of φ(z); Algorithm parameters: k0 > 0; γ(k) =

2

µ(k+k0)  ∀k ∈ {1  2  . . .   K}.
2: Initialize z(0) = zinit and k = 1.
3: while k ≤ K do
4:

Observe an unbiased gradient ζ(k) such that E[ζ(k)] = ∂φ(z(k−1)) and update z(k) via

(cid:104)
z(k−1) − γ(k)ζ(k)(cid:105)

z(k) = PZ

where PZ [·] is the projection onto Z.

5: end while

6: Output: ((cid:98)z  z(K)) where(cid:98)z is the time average of {z(0)  . . .   z(K)} deﬁned in Lemmas 1 or 2.

3

Lagrangian based methods. It is helpful to enforce the linear constraint.

We now justify why Algorithm 1 is a two-layer ADMM method. (See Supplement 6.1 for a more
detailed discussion.)
• The Lagrange multiplier update (4) is identical to that used in existing ADMM methods or other
• At the ﬁrst sight  the primal update in Algorithm (4) is quite different from existing deterministic
ADMMs in [10  4  7]  which require to solve an “argmin" problem  or stochastic ADMMs in
[21  25  11]  which perform a single gradient descent step . However  with a simple manipulation 
it is not difﬁcult to show that that function φ(t)
i (xi) in (2) is similar to the “argmin" target in the
proximal Jacobi ADMM method [7] with the distinction that the proximal term (cid:107)xi − y(t−1)
(cid:107)2 is
rather than x(t−1)
regarding a newly introduced variable y(t−1)

.

i

i

i

i

i (·) Xi  y(t)

Since each call of Algorithm 2 incurs K (t) SGD update  T iterations of Algorithm 1 use(cid:80)T
Algorithm 1 uses T = O(1/) communication rounds and(cid:80)T

√
Recall that the fastest stochastic ADMMs in [21  25  11] can solve general convex problem (1) (with
N = 2) with O(1/
T ) convergence. That is  to obtain a solution with  errors for both the objective
value and the constraint violation  the ADMMs in [21  25  11] require O(1/2) computation steps 
each of which uses a single gradient evaluation and variable update. The ADMMs in [21  25  11] has a
single layer structure and hence are communication inefﬁcient in the sense that each computation step
involves a communication steps. Thus  the communication complexity of these stochastic ADMMs
is also O(1/2). Compared with existing ADMMs in [21  25  11]  Algorithm 1 has a two layer
structure where each outer layer step involves a single inter-node communication step given by (4)-(5)
and calls the sub-procedure  i.e. Algorithm 2  STO-LOCAL(φ(t)
  K (t))  which is run by
each node locally and in parallel and hence does not incur any inter-node communication overhead.
t=1 K (t)
computation steps. We shall show that to achieve an  solution for general convex problem (1) 
t=1 K (t) = O(1/2) computation steps.
That is  Algorithm 1 is as fast as existing fastest stochastic ADMMs but uses only a square root of the
number of communications rounds in [21  25  11].
Note that inter-node communication in Algoirthm 1 can be either centralized or decentralized. To use
centralized communication  we can let all nodes pass their x(t)
to a parameter server  where (4)-(5) are
i
executed  and then pull the updated λ(t) and r(t) from the server. It is possible to implement (4)-(5)
using decentralized communication by exploring the structure of matrix A = [A1  A2  . . .   AN ]. For
example  consider distributed machine learning in a line network where Ax = b is given by N − 1
equality constraints xi − xi+1 = 0  i ∈ {1  2  . . .   N − 1}. In this case  λ(t)
i only depend on
x(t)
i+1 . Thus  to implement Algorithm 1  each
i
node only needs to send its local x(t)
from its neighbors in the line network.
i
2.1 Basic Facts of Algorithm 2
Since each iteration of Algorithm 1 calles Algorithm 2  which essentially applies SGD with carefully
i (·). This subsection provides
designed step size rules to newly introduced objective functions φ(t)
some useful insight of SGD for strongly convex stochastic minimization.
It is known that SGD can have O(1/) convergence for strongly convex minimization. The next two
lemmas summarize the convergence of SGD Algorithm 2. When characterizing O(1/) rate  our
lemmas also include a push-back term involving the last iteration solution. This term ensures when
the SGD solution from Algorithm 2 is used in the outer-level ADMM dynamics  the accumulated
error of our ﬁnal solution does not explode. It also explains why we use y(t−1)
  which is the last
iteration solution from the SGD sub-procedure  rather than conventional x(t−1)
to deﬁne φ(t)
i (xi).
Lemma 1 ([16]). Assume φ(z) is a µ-convex function (µ > 0) over set Z and there exists a constant
B such that the unbiased subgradient ζ(k) used in Algorithm 2 satisﬁes E[(cid:107)ζ(k)(cid:107)2] ≤ B2 ∀k ∈
{1  2  . . .   K}. If we take k0 = 1 in Algorithm 2  then for all z ∈ Z  we have

i+1 and are only used to updates x(t+1)

and x(t+1)
and r(t)
j

to and pull λ(t)
j

and x(t)

and r(t)

i

i

i

i

E[φ((cid:98)z)] ≤ φ(z) − µ
(cid:124)
(cid:80)K−1

k=0 (k + k0)z(k).

2

where(cid:98)z =

1(cid:80)K−1

k=0 (k+k0)

E[(cid:107)z(K) − z(cid:107)2]

+

2B2

µ(K + 1)

 

(7)

(cid:123)(cid:122)

(7)-term (I)

(cid:125)

4

Remark 1. It is ﬁrstly shown in [16] that Algorithm 2 with k0 = 1 (vanilla SGD with a particular
averaging scheme) has O(1/) convergence for non-smooth strongly convex problems. Note that (7)
holds for all z ∈ Z (not necessarily the minimizer of φ(·)). The push-back term (7)-term (I) is often
ignored in convergence rate analysis for SGD but is important for our analysis of Algorithm 1.
Recall that a function h(x) is said to be L-smooth if its gradient ∇h(x) is Lipschitz with modulus
L. The next lemma is new and extends Lemma 1 to smooth minimization such that the error term
depends only on the variance of stochastic gradients (using a different averaging scheme).
Lemma 2. Assume φ(z) is a L-smooth and µ-convex function (µ > 0) with conditional number
µ and there exists σ > 0 such that unbiased gradient ζ(k) (at point z(k−1)) in Algorithm 2
κ = L
satisﬁes E[(cid:107)ζ(k) − ∇φ(z(k−1))(cid:107)2] ≤ σ2 ∀k ∈ {1  2  . . .   K}. If we take integer k0 > 2κ  then for
any z ∈ Z  we have
E[φ((cid:98)z)] ≤φ(z) +
where(cid:98)z =
(cid:80)K
k=1(k+k0−1)

(cid:0)E[(cid:107)z − z(0)(cid:107)2] − E[(cid:107)z − z(K)(cid:107)2](cid:1) − µ
(cid:80)K
k=1(k + k0 − 1)z(k).

2K(K + 2k0 − 1)
1

E[(cid:107)z − z(K)(cid:107)2] +

(K + 2k0 − 1)µ

0 − k0)

2k0σ2

µ(k2

(8)

2

√

Proof. See Supplement 6.6.
3 Performance Analysis of Algorithm 1
This section shows that Algorithm 1 can achieve an -accuracy solution using O(1/2) computation
rounds and O(1/) communication rounds for general convex stochastic programs; or using ˜O(1/)
computation rounds and ˜O(1/
) communication rounds for strongly convex stochastic programs.
3.1 General objective functions (possibly non-smooth non-strongly convex)
Theorem 1. Consider convex program (1) under Assumption 1. Let (x∗  λ
deﬁned in Assumption 1. Assume that
• The constraint set X is bounded  i.e.  there exists constant R > 0 such that (cid:107)x(cid:107) ≤ R ∀x ∈ X .
• The function f (x) has unbiased stochastic subgradients with a bounded second order moment  i.e. 
For all T ≥ 1  if we choose any ﬁxed ρ(t) = ρ > 0  ν(t) = ν ≥ 8ρ(cid:107)A(cid:107)2  K (t) = K ≥ T in
output  then

Algorithm 1 and the sub-procedure STO-LOCAL (Algorithm 2) uses(cid:98)z deﬁned in Lemma 1 as the

there exists constant D > 0 such that Eξ[(cid:107)G(x; ξ)(cid:107)2] ≤ D2 ∀x ∈ X .

) be any saddle point

∗

E[f (x(T ))] ≤ f (x

∗

) +

(cid:107)x

∗ − y(0)(cid:107)2 +

C
2νT

(cid:80)T

E[(cid:107)Ax(T ) − b(cid:107)] ≤ 1
T

(cid:114)

ν

ν

ν

ρν(cid:107)x∗ − y(0)(cid:107)2 + 24ρD2

+ 24(ρ)3(cid:107)A(cid:107)2 ((cid:107)A(cid:107)R+(cid:107)b(cid:107))2

t=1 x(t); Q = (2(cid:107)λ∗(cid:107) +

of computation rounds is(cid:80)T

+ 96νρR2/(cid:0)1 −
(cid:1)(cid:1)2 is an absolute constant (irrelevant to T ); and C ∆= 4(cid:107)A(cid:107)2Q+12D2+12ρ2(cid:107)A(cid:107)2((cid:107)A(cid:107)R+

(cid:114)
where x(T ) = 1
t
8ρ(cid:107)A(cid:107)2
(cid:107)b(cid:107))2 + 48ν2R2 is also an absolute constant.
Proof. See Supplement 6.7.
Remark 2. After T outer-level rounds  Algorithm 1 yields a solution with error O(1/T ). Note that
the number of communication rounds is equal to the number of outer-level rounds and the number
t=1 K (t) = O(T 2) when K (t) = T ∀t. Thus  to obtain an -solution 
Algorithm 1 uses O(1/) communication rounds and O(1/2) computation rounds.
Remark 3. If we choose ν(t) = ν = 8ρ(cid:107)A(cid:107)2 in Theorem 1 and further analyze the dependence on
T ρ(cid:107)A(cid:107)2) and E[(cid:107)Ax(T ) − b(cid:107)] ≤ O( 1
(cid:107)A(cid:107) in (9)-(10)  we have E[f (x(T ))] ≤ f (x∗) + O( 1
T ( 1
ρ +
(cid:107)A(cid:107))). If (cid:107)A(cid:107) is large  to balance the dependence on (cid:107)A(cid:107) in (9)-(10)  we shall choose ρ = 1(cid:107)A(cid:107) such
T (cid:107)A(cid:107)). In general  ρ can be controlled to trade
that the error terms in both (9) and (10) are order O( 1
off between objective error and constraint error. For distributed consensus optimization considered in
[26  22  23  19] (assuming di = 1 without loss of generality)  we can choose any A  b that sufﬁces to
ensure the consistence of local solutions  e.g.  Null{A} =Span{1} and b = 0. Our method does not
necessarily require A = I − W with a stochastic matrix W encoding the network topology as some
methods in [26  22  23  19]. Nevertheless  even when ung A = I − W  our communication overhead
can possibly have a better dependence on W. Note that a stochastic matrix W ensures (cid:107)A(cid:107) ≤ 2.
The convergence in [26  22  23  19] (using a doubly stochastic or symmetric PSD W for mixing)
further depends on 1/(1 − max{|λ2(W)| |λN (W)|}) or the eigen-gap λ1(W)/λN−1(W)  which
can be much larger than constant 2 when some eigenvalues are extreme.

(9)

(10)

ν
√
2T
Q
ρ

5

(cid:80)T

where x(T ) = 1
t

t=1 x(t) .

positive integer k0 ≥ 2(1 + L
∗
E[f (x(T ))] ≤ f (x

1

) +

T (T + 1)

(cid:16)
(cid:16) 4(cid:107)λ∗(cid:107)

c1(cid:107)x

t=1 ρ(t)x(t); and c1

E[(cid:107)Ax(T ) − b(cid:107)] ≤

2

T (T + 1)

ρ

(cid:80)T

1(cid:80)T

where x(T ) =
(2k0−1)(cid:107)A(cid:107)2 are two constants.

t=1 ρ(t)

4k0σ2

+

∗ − y(0)(cid:107)2 +

c2
ρ

(cid:107)x

log(T + 1)

∗ − y(0)(cid:107) +

(cid:112)c2 log(T + 1)
√
c1√
ρ
∆= ρ(cid:107)A(cid:107)2 + (ρ(cid:107)A(cid:107)2+µ)(k2
2(2k0−1)2

ρ

(cid:17)

(13)

(14)

0−k0)

and c2

∆=

3.2 Smooth objective functions
For unconstrained stochastic smooth minimization  the constant factor in the SGD convergence
rate is determined by the variance that can be signiﬁcantly less than the second order moment for
non-smooth stochastic minimization[20]. Such a property enable us to speed up SGD by averaging
multiple i.i.d. stochastic gradients  e.g.  mini-batch SGD. In this subsection  we show that Algorithm
1 has a similar property when f (·) in problem (1) is smooth.
Theorem 2. Consider convex program (1) with µ-convex (possibly µ = 0) objective function under
Assumption 1. Let (x∗  λ
• The function f (x) is L-smooth.
• The function f (x) has unbiased stochastic gradients with a bounded variance  i.e.  there exists

constant σ > 0 such that Eξ[(cid:107)G(x; ξ) − ∇f (x)(cid:107)2] ≤ σ2 ∀x ∈ X .

If the sub-procedure STO-LOCAL (Algorithm 2) uses(cid:98)z deﬁned in Lemma 2 as the output  then

) be any saddle point deﬁned in Assumption 1. Assume that

Algorithm 1 ensures:
• General Convex (µ = 0): For all T ≥ 1  if we choose any ﬁxed ρ(t) = ρ > 0  ν(t) = ν ≥ ρ(cid:107)A(cid:107)2 

∗

K (t) = K = T and positive integer k0 ≥ 2 L+ν
ν   then we have
(cid:115)
∗ − y(0)(cid:107)2 +

E[f (x(T ))] ≤ f (x

ν(k0 + 1)

(cid:107)x

1
T

) +

∗

1
T

2k0σ2

ν

E[(cid:107)Ax(T ) − b(cid:107)] ≤ 1
T

ν(k0 + 1)

(cid:107)x

∗ − y(0)(cid:107) + 2

2ρ

(cid:16) 2

ρ

4
∗(cid:107) +

(cid:107)λ

(cid:115)

(cid:17)

k0σ2

ρν

(11)

(12)

• Strongly Convex (µ > 0): For all T ≥ 1  if we choose ρ ≤ µ

3(cid:107)A(cid:107)2   ρ(t) = tρ  ν(t) = tρ(cid:107)A(cid:107)2 

µ ) and K (t) = (2k0 − 1)t  then we have

(cid:17)

T 2

2 T (T + 1) = O(T 2)  Algorithm 1 requires ˜O( 1

Proof. See Supplement 6.8.
Remark 4. If f (x) in convex program (1) is strongly convex  Algorithm 1 can obtain a solution
(cid:80)T
with error O( log(T )
) after T outer-level rounds. Recall the number of communication rounds
is equal to the number of outer-level rounds and the number of computation rounds is equal to
t=1 K (t) = 2k0−1
 ) communication rounds and
2 ) computation rounds to obtain an -solution.

˜O( 1
3.3 Non-smooth strongly convex objective functions
There is a fourth case  where the stochastic objective function f (x) is strongly convex but possibly
non-smooth  uncovered in the previous subsections. In this case  we assume the following condition
(originally introduced in [17]): There exists constant M > 0 such that
f (x) ≤ f (y) + (cid:104)d  x − y(cid:105) + M(cid:107)x − y(cid:107) 

(15)
for all x  y ∈ X and d ∈ ∂f (y). This condition is assumed throughout [17] to develop a different
communication efﬁcient primal-dual method. Supplement 6.9 shows this condition is almost as
√
useful as smoothness and under this condition  our communication efﬁcient ADMM can achieve
an -accuracy solution with ˜O(1/) computation rounds and ˜O(1/
) communication rounds for
non-smooth strongly convex stochastic optimization.
4 Experiments
4.1 Distributed Stochastic Optimization with Noisy Stochastic Gradient Information

Consider simple stochastic optimization given by

6

3(cid:88)

i=1

min

Eci[(cid:107)xi − ci(cid:107)2
2]

(16)

1 = x∗

2 = x∗

s.t. x1 = x2  x2 = x3

xi ∈ [−1  1]3 ∀i ∈ {1  2  . . .   3}

(17)
(18)
where ci ∼ N (¯ci  σ2
i I) satisfy normal distributions with ¯c1 = [−2.0871 −0.3702  0.2302]T  σ1 =
0.1  ¯c2 = [−0.5556 −0.4413  0.2869]T  σ2 = 0.2  ¯c3 = [−1.4991 −1.8286 −2.0477]T and σ3 =
0.1. Solving this problem with Algorithm 1 only requires each node to access samples of local ci and
does not use the true value ¯ci and σi which are fundamentally unavailable. However  by assuming the
knowledge of ¯ci and σi  we can convert this stochastic optimization to a deterministic problem and
3 = [−1 −0.88003599 −0.51020207]T
use CVXPY [9] to obtain the unique solution x∗
such that we can evaluate the performance of Algorithm 1. Since the objective function is smooth and
strongly convex  by Theorem 2  using time-varying parameters in Algorithm 1 has faster convergence.
We run Algorithm 1 with constant ρ  ν according to3 Theorem 1 and with time-varying ρ(t)  ν(t)
according to Theorem 2  respectively. Note that if an algorithm has O(1/β) convergence  then its
error should decay like O(1/t1/β) where t is the iteration index.
Figures 1 plots the distance to x∗ versus the computation round index or the communication round
index in a log-log scale. It also plots baseline curves 1/t
β corresponding to O(1/β) convergence
proven in the theorems. Note that in a log-log scale  curves 1/t
β become straight lines with
slopes − 1
β . That is  if our algorithm has the proven convergence rate  the error curves should be
eventually parallel to corresponding baseline for large t. In Figures 1  we observe the numerical
result is consistent with our theoretical rate proven in our theorems. This simple experiment veriﬁes
the correctness of our theorems. Our multi-core implementation of Algorithm 1 uses Python 3.7
and MPI4PY. In an experiment over a machine with a multi-core Intel Xeon Processor E5-2682
2.5GHz. Each computation round takes 0.3ms and each communication round takes 43.7ms. Note
communication becomes more relatively expensive as more parallel nodes/cores are involved.

1

1

(a)

(c)

(b)

(d)

Figure 1: Performance of Algorithm 1 to solve stochastic optimization (16)-(18): (a)& (b) conver-
gence w.r.t. # of computation rounds; (c)&(d) convergence w.r.t. # of communication rounds.

3Since f (x) is also smooth  using constant ρ  ν according to Theorem 2 can give a similar (slightly better)
performance. Theoretically  by using K (t) = t rather than K (t) = T for a ﬁxed T   the rate is slightly worse  i.e.
O(log(T )/T ) v.s. O(1/T ). However  we ﬁnd the performance degradation for large T regions is negligible
when using K (t) = t. In contrast  using K (t) = t enable the algorithm converge faster for small t. We use
K (t) = t when performing the numerical experiments in this paper.

7

10(cid:88)

1
10

1
Ni

Ni(cid:88)

4.2 Distributed l1 Regularized Logistic Regression
Consider a distributed l1 regularized logistic regression problem (over 10 nodes) given by:

ijxi)) + µ(cid:107)xi(cid:107)1

i=1

j=1

min

log(1 + exp(bij(aT

(19)
with each optimization variable xi ∈ Rd. Each node contains Ni training pairs (aij  bij)  where
aij ∈ Rd is a feature vector and bij ∈ {−1  1} is the corresponding label. To ensure all nodes yield a
consistent model  consensus constraints are needed to enforce all xi are equal. Note that conventional
two-block ADMMs must introduce a dummy block (server node) z and add constraints xi = z. (See
e.g.  [4  21  25].) However  such an ADMM method requires all nodes to pass the updated xi value
to the (server) node corresponding to the z block and hence can turn z node into a communication
bottleneck in large networks. In contrast  using a multi-block ADMM method allows arbitrary
linear constraints  e.g.  constraints xi = xi+1 ∀i that ensure all xi are equal  and the corresponding
multi-block ADMM only uses communication between adjacent blocks. Alternatively  consider a line
network where only one-hop transmission is allowed  then our ADMM naturally yields a protocol
that is faithful to the network communication restriction. In general  given an arbitrary network
communication topology  our multi-block ADMM can always yield an implementable distributed
protocol by adding constraints xi = xj for links (i  j) existing in the network.
We generate a problem instance in a way similarly to [4]. Our problem instance uses d = 100 
Ni = 105 for all i and µ = 0.002. Each feature vector aij is generated from a standard normal
distribution. We choose a true weight vector xtrue ∈ Rd with 10 non-zero entries from a standard
ijxtrue + ni) where noise ni ∼ N (0  σ2
normal distribution and then generate the label bij = sign(aT
i )
with ﬁxed constants σi randomly generated from a uniform distribution Unif[0  1]. Figures 2 compares
Algorithm 1 with RPDBUS ADMM proposed in [11]  where the number of communication rounds is
the same that of computation rounds  and DCS in [17]  where the number of communication rounds
is the square root of that of computation rounds. We observe that Algorithm 1 has fastest convergence
with respect to both computation and communication.

(a)

(c)

(b)

(d)

Figure 2: Distributed l1 regularized logistic regression: (a)& (b) performance w.r.t. # of computation
rounds; (c)&(d) performance w.r.t. # of communication rounds
5 Conclusions
This paper proposes a new communication efﬁcient multi-block ADMM for linearly constrained
stochastic optimization. This method is as fast as (or faster than) existing stochastic ADMMs but the
associated communication overhead is only the square root of that required by existing ADMMs.

8

References
[1] Samaneh Azadi and Suvrit Sra. Towards an optimal stochastic alternating direction method of
multipliers. In International Conference on Machine Learning (ICML)  pages 620–628  2014.

[2] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  second edition  1999.

[3] Dimitri P. Bertsekas  Angelia Nedi´c  and Asuman E. Ozdaglar. Convex Analysis and Optimiza-

tion. Athena Scientiﬁc  2003.

[4] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends in Machine Learning  3(1):1–122  2011.

[5] Tsung-Hui Chang  Mingyi Hong  Wei-Cheng Liao  and Xiangfeng Wang. Asynchronous
distributed admm for large-scale optimization—part i: Algorithm and convergence analysis.
IEEE Transactions on Signal Processing  64(12):3118–3130  2016.

[6] Caihua Chen  Bingsheng He  Yinyu Ye  and Xiaoming Yuan. The direct extension of ADMM
for multi-block convex minimization problems is not necessarily convergent. Mathematical
Programming  155:57–79  2016.

[7] Wei Deng  Ming-Jun Lai  Zhimin Peng  and Wotao Yin. Parallel multi-block ADMM with

o(1/k) convergence. Journal of Scientiﬁc Computing  71(2):712–736  2017.

[8] Wei Deng and Wotao Yin. On the global and linear convergence of the generalized alternating

direction method of multipliers. Journal of Scientiﬁc Computing  66(3):889–916  2016.

[9] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research  17(83):1–5  2016.

[10] Jonathan Eckstein and Dimitri P Bertsekas. On the Douglas-Rachford splitting method and
the proximal point algorithm for maximal monotone operators. Mathematical Programming 
55(1-3):293–318  1992.

[11] Xiang Gao  Yangyang Xu  and Shuzhong Zhang. Randomized primal-dual proximal block

coordinate updates. arXiv:1605.05969  2016.

[12] Bingsheng He  Hong-Kun Xu  and Xiaoming Yuan. On the proximal Jacobian decomposition
of ALM for multiple-block separable convex minimization problems and its relationship to
ADMM. Journal of Scientiﬁc Computing  66(3):1204–1217  2016.

[13] Bingsheng He and Xiaoming Yuan. On the O(1/n) convergence rate of the Douglas-Rachford

alternating direction method. SIAM Journal on Numerical Analysis  50(2):700–709  2012.

[14] Martin Jaggi  Virginia Smith  Martin Takác  Jonathan Terhorst  Sanjay Krishnan  Thomas
Hofmann  and Michael I Jordan. Communication-efﬁcient distributed dual coordinate ascent.
Advances in Neural Information Processing Systems (NIPS)  2014.

[15] Jakub Koneˇcn`y  H Brendan McMahan  Daniel Ramage  and Peter Richtárik. Federated opti-

mization: Distributed machine learning for on-device intelligence. arXiv:1610.02527  2016.

[16] Simon Lacoste-Julien  Mark Schmidt  and Francis Bach. A simpler approach to obtaining an
O(1/t) convergence rate for the projected stochastic subgradient method. arXiv:1212.2002 
2012.

[17] Guanghui Lan  Soomin Lee  and Yi Zhou. Communication-efﬁcient algorithms for decentralized

and stochastic optimization. arXiv:1701.03961  2017.

[18] Huan Li  Cong Fang  and Zhouchen Lin. Convergence rates analysis of the quadratic penalty
method and its applications to decentralized distributed optimization. arXiv:1711.10802  2017.

[19] Angelia Nedi´c  Alex Olshevsky  and Michael G Rabbat. Network topology and communication-
computation tradeoffs in decentralized optimization. Proceedings of the IEEE  106(5):953–976 
2018.

9

[20] Arkadi Nemirovski  Anatoli Juditsky  Guanghui Lan  and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization  19(4):1574–
1609  2009.

[21] Hua Ouyang  Niao He  Long Tran  and Alexander Gray. Stochastic alternating direction method

of multipliers. In International Conference on Machine Learning (ICML)  2013.

[22] Shi Pu and Angelia Nedi´c. Distributed stochastic gradient tracking methods. arXiv:1805.11454 

2018.

[23] Kevin Scaman  Francis Bach  Sébastien Bubeck  Yin Tat Lee  and Laurent Massoulié. Opti-
mal algorithms for non-smooth distributed optimization in networks. In Advances in Neural
Information Processing Systems (NeurIPS)  2018.

[24] Virginia Smith  Simone Forte  Chenxin Ma  Martin Takác  Michael I Jordan  and Martin
Jaggi. CoCoA: A general framework for communication-efﬁcient distributed optimization.
arXiv:1611.02189  2016.

[25] Taiji Suzuki. Dual averaging and proximal gradient descent for online alternating direction

multiplier method. In International Conference on Machine Learning (ICML)  2013.

[26] César A Uribe  Soomin Lee  Alexander Gasnikov  and Angelia Nedi´c. Optimal algorithms for

distributed optimization. ArXiv:1712.00232  2017.

[27] Huahua Wang  Arindam Banerjee  and Zhi-Quan Luo. Parallel direction method of multipliers.

Advances in Neural Information Processing Systems (NIPS)  2014.

[28] Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers. In

IEEE Conference on Decision and Control (CDC)  pages 5445–5450  2012.

[29] Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate

ascent. Advances in Neural Information Processing Systems (NIPS)  2013.

[30] Hao Yu and Michael J. Neely. A simple parallel algorithm with an O(1/t) convergence rate for

general convex programs. SIAM Journal on Optimization  27(2):759–783  2017.

[31] Wenliang Zhong and James Kwok. Fast stochastic alternating direction method of multipliers.

In International Conference on Machine Learning (ICML)  pages 46–54  2014.

10

,Hao Yu