2019,Learning Local Search Heuristics for Boolean Satisfiability,We present an approach to learn SAT solver heuristics from scratch through deep reinforcement learning with a curriculum. In particular  we incorporate a graph neural network in a stochastic local search algorithm to act as the variable selection heuristic. We consider Boolean satisfiability problems from different classes and learn specialized heuristics for each class. Although we do not aim to compete with the state-of-the-art SAT solvers in run time  we demonstrate that the learned heuristics allow us to find satisfying assignments in fewer steps compared to a generic heuristic  and we provide analysis of our results through experiments.,Learning Local Search Heuristics for

Boolean Satisﬁability

Emre Yolcu

Carnegie Mellon University

eyolcu@cs.cmu.edu

Barnabás Póczos

Carnegie Mellon University
bapoczos@cs.cmu.edu

Abstract

We present an approach to learn SAT solver heuristics from scratch through deep
reinforcement learning with a curriculum. In particular  we incorporate a graph
neural network in a stochastic local search algorithm to act as the variable selection
heuristic. We consider Boolean satisﬁability problems from different classes and
learn specialized heuristics for each class. Although we do not aim to compete
with the state-of-the-art SAT solvers in run time  we demonstrate that the learned
heuristics allow us to ﬁnd satisfying assignments in fewer steps compared to a
generic heuristic  and we provide analysis of our results through experiments.

1

Introduction

Recently there has been a surge of interest in applying machine learning to combinatorial optimiza-
tion [7  24  32  27  9]. The problems of interest are often NP-complete and traditional methods
efﬁcient in practice usually rely on heuristics or produce approximate solutions. These heuristics
are commonly manually-designed  requiring signiﬁcant insight into the problem. Similar to the way
that the recent developments in deep learning have transformed research in computer vision [28] and
artiﬁcial intelligence [43] by moving from engineered methods to ones that are learned from data and
experience  the expectation is that it will lead to advancements in search and optimization algorithms
as well. Interest in this line of work has been fueled by the developments in neural networks that
operate on graphs [5] since many combinatorial problems can be naturally represented using graphs.
A problem that is becoming a popular target for machine learning is satisﬁability [11]. Boolean
satisﬁability (abbreviated SAT) is the decision problem of determining whether there exists a satisfying
assignment for a given Boolean formula. The task of ﬁnding such assignments if they exist or proving
unsatisﬁability otherwise is referred to as SAT solving. SAT is the canonical NP-complete problem
[13]. It is heavily researched  and there exist efﬁcient heuristic algorithms that can solve problems
involving millions of variables and clauses. In addition to its fundamental place in complexity theory 
SAT is practically relevant  and there is a plethora of problems arising from artiﬁcial intelligence 
circuit design  planning  and automated theorem proving that can be reduced to SAT [37].
Assuming we are given instances from a known class of SAT problems  it is an interesting question
whether we can discover a heuristic from scratch specialized to that class which improves upon a
generic one. While handcrafting such heuristics for every single class is not feasible  an alternative is
to sample problems from a class and learn a heuristic by training to solve the problems. In practice 
we are often interested in solving similar problems coming from the same distribution  which makes
this setting worth studying. In this paper we focus on stochastic local search (SLS) and propose a
learnable algorithm with a variable selection heuristic computed by a graph neural network. Through
reinforcement learning  we train from scratch a suite of solvers with heuristics specialized to different
classes  and demonstrate that this approach can lead to algorithms that require fewer  although costlier 
steps to arrive at a solution compared to a generic heuristic.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2 Related work

Lately  neural networks have seen an increased interest in applications to SAT solving. Selsam et al.
[42] propose an approach where a graph neural network (called NeuroSAT) is trained to classify SAT
problems as satisﬁable or unsatisﬁable. They provide evidence that as a result of training to predict
satisﬁability on their speciﬁcally crafted dataset  a local search algorithm is synthesized  through
which they can often decode the satisfying assignments. In their work  the graph neural network
itself acts as a learnable solver while we use it as a relatively cheap heuristic in an explicitly speciﬁed
search algorithm. It is unclear whether their approach can be applied to learning distribution-speciﬁc
algorithms as it is hypothesized to be crucial that NeuroSAT be trained on “patternless” problems 
hence the reason for our approach. In a more recent work  Selsam and Bjørner [41] also use a
simpliﬁed NeuroSAT to guide the search process of an existing solver. Another similar approach
is that of Lederman et al. [31]  where they attempt to learn improved heuristics to solve quantiﬁed
Boolean formulas through reinforcement learning while using a heuristic computed by a graph neural
network. A critical difference of our work from theirs is that we have a minimal set of features
(one-hot encodings of the assignments) just enough to obtain a lossless encoding of the solver state
while they make use of a large set of handcrafted features. On another front  Amizadeh et al. [1] use a
graph neural network and a training procedure mimicking reinforcement learning to directly produce
solutions to the Circuit-SAT problem.
SAT solving community has also experimented with more practical applications of machine learning
to SAT  possibly the most successful example being SATzilla [48]. Closest to our approach are the
works of Fukunaga [18  19]  KhudaBukhsh et al. [25]  Illetskova et al. [22]. Brieﬂy summarized  they
evolve heuristics through genetic algorithms by combining existing primitives  with the latter two
aiming to specialize the created heuristics to particular problem classes. We adopt a similar approach 
with the crucial difference that our approach involves learning heuristics from experience as opposed
to combining designed ones. There have also been other approaches utilizing reinforcement learning
to discover variable selection heuristics [33  34  35  29  17]  although they focus mostly on crafting
reward functions. We turn our attention to learning with little prior knowledge  and use a terminal
reward which is nonzero only when a satisfying assignment is found.

3 Background

3.1 Boolean formulas

We limit our attention to Boolean formulas in conjunctive normal form (CNF)  which consist of the
following components: a list of n Boolean variables (x1  . . .   xn)  and a list of m clauses (c1  . . .   cm)
where each clause cj is a disjunction (∨) of literals (a literal refers to different polarities of a Boolean
variable: xi and ¬xi). The CNF formula is the conjunction (∧) of all clauses. Throughout the paper 
φ : {0  1}n → {0  1} refers to a Boolean formula  X ∈ {0  1}n to a particular truth assignment to the
list of variables (x1  . . .   xn)  and φ(X) is simply the truth value of the Boolean formula evaluated at
assignment X. Also  n and m always refer to the number of variables and clauses  respectively.

3.2 Local search

SAT solvers based on stochastic local search
start with a random initial candidate solution and
iteratively reﬁne it by moving to neighboring so-
lutions until arriving at a satisfying assignment.
Neighboring solutions differ by the assignment
of a single variable  that is  the assigned value
of a variable is ﬂipped between solutions. Al-
gorithm 1 shows the pseudocode for a generic
stochastic local search algorithm for SAT.
As an example heuristic  the SelectVariable
function of WalkSAT [39] ﬁrst randomly selects
a clause unsatisﬁed by the current assignment
and ﬂips the variable within that clause which 
when ﬂipped  would result in the fewest number

trials K  maximum number of ﬂips L

X ← Initialize(φ)
for j ← 1 to L do

Algorithm 1 Local search for SAT
Input: Boolean formula φ  maximum number of
1: for i ← 1 to K do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end for
12: return unsolved

return X
index ← SelectVariable(φ  X)
X ← Flip(X  index)

if φ(X) = 1 then

else

end if

end for

2

of previously satisﬁed clauses becoming unsatisﬁed  or with some probability (referred to as walk
probability) it selects a variable randomly within the clause.
Our algorithm ﬁts into the same template  with the difference being that the function SelectVariable
employs a graph neural network to select a variable. Unlike WalkSAT we do not limit the selection to
the variables in unsatisﬁed clauses  so it is possible to pick a variable that occurs only in currently
satisﬁed clauses. However  similar to WalkSAT  with some probability we also randomly select a
variable from a randomly selected unsatisﬁed clause.

3.3 Graph neural networks

Graph neural networks (GNNs) are a family of neural network architectures that operate on graphs
with their computation structured accordingly [21  38  20  5].
In order to explain our speciﬁc
architecture in Section 4 more easily  here we describe a formalism for GNNs similar to the message-
passing framework of Gilmer et al. [20].
Assume we have an undirected graph G = (V  E) where V is the set of vertices (nodes) and
E ⊆ V × V is the set of edges. Further suppose that for each node v ∈ V we have a row vector
v ∈ Rd0
V . Similarly 
h0
for an edge vw ∈ E between two nodes v  w ∈ V we have a row vector hvw ∈ RdE of edge features
with some dimension dE. A GNN maps each node to a vector space embedding by iteratively
updating the representation of the node based on its neighbors. In this formalism we do not extract
edge features. At each iteration t ∈ {1  . . .   T}  for each v ∈ V we update its previous representation
ht−1

V of node features (possibly derived from node labels) with some dimension d0

to ht

V as

v ∈ Rdt

v

ht−1

v

 

(cid:88)

M t(cid:0)ht−1

v

ht
v = U t

w∈N (v)

(cid:1)  

  ht−1

w   hvw

where N (v) denotes the set of neighbors of node v. Message functions M t and update functions U t
are differentiable functions whose parameters are learned. After T iterations  we obtain the extracted
features hT

v for each node v in the graph.

For a compact notation  deﬁne the stacked node features H t
where V (i) is the i-th node in the graph under some indexing. Similarly deﬁne HE ∈ R|E|×dE to be
the stacked edge features. Then a GNN with T iterations computes H T
fθ is a function parameterized by θ that encodes each node of the graph G as a real-valued vector.

(cid:0)H 0
V   HE  G(cid:1) where

V such that H t

V = fθ

V (i)

V ∈ R|V |×dt

V (i ·) = ht

4 Model

In this section we ﬁrst describe the graphical representation for CNF formulas  then explain the exact
input of the model  and ﬁnally deﬁne the architecture of the graph neural network that we use.

4.1 Factor graph of CNF formulas

We opt for a factor graph representation of CNF formulas.
With this representation we obtain an undirected bipartite graph
with two node types (variable and clause) and two edge types
(positive and negative polarities). For each variable in the
formula we have a node and for each clause we have a node of
a different type. Between a clause and each variable that occurs
in it there is an edge whose type depends on the polarity of the
variable in the clause.
Note that unlike the graphical representation employed by Sel-
sam et al. [42] and Lederman et al. [31]  each variable (as
opposed to a literal) has a corresponding node  which results in
a slightly more compact mapping of a CNF formula to a graph.
Figure 1 displays the factor graph of a simple CNF formula.

x1

x2

x3

c1

c2

c3

c4

c5

Figure 1: Factor graph of the for-
mula (x1 ∨ x3) ∧ (x2 ∨ ¬x3) ∧
(¬x1 ∨ x3) ∧ (¬x1 ∨ ¬x2) ∧ (x1 ∨
¬x3). Solid and dashed edges cor-
respond respectively to positive and
negative polarities of variables in
clauses.

3

4.2

Input representation

For variable selection  we use a GNN that takes as input a formula φ with an assignment X to its
variables and outputs a vector of probabilities corresponding to variables (described further in the
next section). The actual input to the model consists of the adjacency information of the factor graph
and node features. Edge features are apparent from the adjacency matrices and do not act as explicit
inputs to the model.
Since the factor graph of a CNF formula is bipartite and there are edges of two different types  we
store a pair of n × m biadjacency matrices A = (A+  A−) such that A+(i  j) = 1{xi ∈ cj} and
A−(i  j) = 1{¬xi ∈ cj}.
As node features we use one-hot vectors. When variable assignments are taken into account  there are
three node types: variable assigned True  variable assigned False  and clause. More concretely  node
v ∈ Rn×3 and clause
features are stored as a pair H0 = (H 0
c ∈ Rm×3. As a result  the pair (A  H0) is all that is needed to perform local search on a
features H 0
formula using the GNN variable selection heuristic.
For a single run of the local search algorithm  H 0
v is set at ﬁrst to reﬂect a random initial assignment
to variables and at each search iteration the row corresponding to the variable selected by the heuristic
is modiﬁed to ﬂip its assignment.

c ) of stacked variable features H 0

v   H 0

4.3 Policy network architecture

In Section 3.3 we explained the abstract GNN architecture. The speciﬁc GNN that we implement 
which we call the policy network along with the output layer  can be described as an instance of this
architecture. In particular  it has four different message functions  one for each combination of nodes
(variable  clause) and edges (positive  negative)  and two different update functions for variables
and clauses. The policy network runs for T iterations  and as its components we have the following
learnable functions  with dependence on parameters omitted for brevity  where t ∈ {1  . . .   T}:

• M t
• M t
• U t

variable from clauses they positively and negatively occur in.

v− : Rdt−1 → Rdt compute the incoming messages to each
v+ : Rdt−1 → Rdt and M t
c− : Rdt−1 → Rdt compute the incoming messages to each
c+ : Rdt−1 → Rdt and M t
clause from variables that occur positively and negatively in them.
v : Rdt−1 × Rdt → Rdt and U t
c : Rdt−1 × Rdt → Rdt update the representations of
variables and clauses based on their previous representation and the sum of the incoming
messages.

• Z : RdT → R produces a score given the extracted node features of a variable.

In the actual implementation we have dt = d for t > 0  that is  the same at each iteration  and d0 = 3
for input features. With a slight abuse of notation we will assume that we can apply the functions
above to matrices  in which context they mean row-wise application. Having described the individual
components  let f t be the function  with dependence on parameters omitted again  that computes the
node representations of a graph with adjacency A at iteration t. We can write f t compactly as

c

M t

(cid:18)

(cid:21)(cid:19)

v  H t
c)
v+(H t−1
v−(H t−1

(cid:20)M t
(cid:20)A+
HT =(cid:0)f T ◦ f T−1 ◦ ··· ◦ f 1(cid:1) (H0)
v )(cid:1)
ˆp = softmax(cid:0)Z(H T

H t−1

  U t
c

)
)

 

U t
v

=

.
For the same graph  the policy network computes a probability vector ˆp ∈ Rn over variables as

A−

M t

v

c

c

  [A+ A−]

where softmax(y) = exp(y)/(cid:80) exp(yi). In order to refer to the above computation concisely we

deﬁne the function πθ computed by the policy network as ˆp = πθ(φ  X) where we assume H0 will
be obtained from the initial assignment X  and A will be obtained from the formula φ. As before  θ
is the list of all neural network parameters. In our implementation  all of M t
c− 
c  Z are multilayer perceptrons (MLP). Also  we use T = 2 which is the minimum number of
v  U t
U t
iterations to allow messages to travel between variables that occur together in a clause. Remaining
hyperparameters are described in Appendix A.

v−  M t

v+  M t

c+  M t

f t((H t−1

  H t−1

c

)) = (H t

(cid:32)

(cid:18)

v

H t−1

(cid:21)(cid:62)(cid:20)M t

c+(H t−1
c−(H t−1

v

v

)
)

(cid:21)(cid:19)(cid:33)

4

(a) Random 3-SAT (b) Clique detection

(c) Vertex cover

(d) Graph coloring

(e) Dominating set

Figure 2: Examples of factor graphs exhibiting distinct structures  each corresponding to problems
sampled from different distributions. Structures of the graphs corresponding to formulas have been
studied in the SAT solving community as a way to explain the effectiveness of CDCL [6] on industrial
problems [2]. With a similar intuition  our aim is to learn heuristics that can exploit the structure.

5 Data

As our goal is to learn specialized heuristics for different classes of satisﬁability problems  we
generate problems from various distributions to train on. There are ﬁve problem classes we perform
experiments with: random 3-SAT  clique detection  vertex cover  graph coloring  dominating set.
Table 1 presents the notation for the problem classes that we consider.

Table 1: As before  n and m refer to
the number of variables and clauses. For
graph problems N refers to the number
of vertices in the graph and p to the prob-
ability that an edge exists. k refers to the
problem speciﬁc size parameter. From
each distribution D on the table we can
sample a CNF formula φ ∼ D.

Distribution
Class
Random 3-SAT rand3(n  m)
k-clique
cliquek(N  p)
k-cover
coverk(N  p)
k-coloring
colork(N  p)
k-domset
domsetk(N  p)

Random 3-SAT1 is of theoretical interest in computa-
tional complexity and also serves as a common bench-
mark for SLS-based SAT algorithms. The latter four are
NP-complete graph problems. For each of these problems
we sample Erd˝os–Rényi graphs [16] from the distribution
denoted as G(N  p) to mean that it has N > 0 vertices
and between any two vertices an edge exists independently
with probability p ∈ [0  1]. Then we encode them as SAT
problems. As a result we obtain ﬁve parameterized ran-
dom distributions that we can sample formulas from. For
training and evaluation we generate problems of various
sizes (made explicit later) from these ﬁve families of dis-
tributions. It is worth noting that the generated problem
instances are not particularly difﬁcult for state-of-the-art
SAT solvers  and for our purpose they serve as simple
benchmarks to help demonstrate the feasibility of a purely
learning-based approach.

In creating problem instances we use CNFgen [30] to generate formulas and Minisat [15] to ﬁlter out
the unsatisﬁable ones. Since our local search algorithm is an incomplete solver it can only ﬁnd an
assignment if one exists and otherwise returns unsolved.

6 Training

6.1 Markov decision process formulation

To learn heuristics through reinforcement learning [44]  we formalize local search for SAT as a
Markov decision process (MDP). For each problem distribution D shown on Table 1 we have a
separate MDP represented as a tuple (SD A T  R  γ) with the following components:

• SD is the set of possible states. Each state is characterized by a pair s = (φ  X)  the CNF
formula and a truth assignment to its variables. At the start of an episode we sample a
formula φ ∼ D with n variables and m clauses  and a uniformly random initial assignment
1For random 3-SAT  experimental research [40  14] indicates that formulas with a ratio of the number of
clauses to the number of variables approximately 4.26 and a large enough number of variables are near the
satisﬁability threshold  that is  the probability of a sampled formula being satisﬁable is near 1/2. We focus on
random 3-SAT problems near the threshold.

5

X ∈ {0  1}n where each element is either 0 or 1 with probability 1/2. The episode
terminates either when we arrive at a satisfying assignment or after L (a predetermined
constant) steps are taken.

have A(s) = {1  . . .   n} where n is the number of variables in φ.

• A is formally a function that maps states to available actions. For a state s = (φ  X) we
• T : SD × {1  . . .   n} → SD is the transition function  mapping from a state-action pair to
the next state. It is deﬁned as T (s  a) = T ((φ  X)  a) = (φ  Flip(X  a)) where Flip simply
negates the assignment of the variable indexed by a ∈ {1  . . .   n}.
• R : SD → {0  1} is the reward function  deﬁned as R(s) = R((φ  X)) = φ(X)  that is  1
• γ ∈ (0  1] is the discount factor  which we set to a constant strictly less than 1 in order to

for a satisfying assignment and 0 otherwise.

encourage ﬁnding solutions in fewer steps.

With the MDP deﬁned as above  the problem of learning a good heuristic is equivalent to ﬁnding
an optimal policy π which maximizes the expected accumulated reward obtained when starting at a
random initial state and sampling a trajectory by taking actions according to π. In trying to ﬁnd an
optimal policy  we use the REINFORCE algorithm [47]. As our policy we have a function ρθ(φ  X)
which returns an action (variable index) a ∼ πθ(φ  X) where πθ is the policy network we described in
Section 4.3. With probability 1/2  ρθ returns a randomly selected variable from a randomly selected
unsatisﬁed clause.
When training to learn a heuristic for a problem distribution D  at each training iteration we sample a
formula φ ∼ D and generate multiple trajectories for the same formula with several different initial
assignments X. Then we accumulate the policy gradient estimates from all trajectories and perform a
single update to the parameters. Algorithm 2 in Appendix B shows the pseudocode for training.

6.2 Curriculum learning

In our MDP formulation  a positive reward is achieved only when an episode ends at a state corre-
sponding to a satisfying assignment. This means that to have non-zero policy gradient estimates  a
solution to the SAT problem has to be found. For difﬁcult problems  we are unlikely to arrive at a
solution by taking uniformly random actions half the time  which is what happens at the beginning of
training with a policy network that has randomly initialized parameters. This may not prevent learning 
but makes it prohibitively slow. In order to solve this problem  we opt for curriculum learning [8].
With curriculum learning  training is performed on a sequence of problems of increasing difﬁculty.
The intuition is that the policy learned for easier problems should at least partially generalize to more
difﬁcult problems  and that this should lead to faster improvement compared to training directly on
the difﬁcult problem of interest. Speciﬁcally  we follow an approach where we run the training loop
in sequence for distributions of increasing size within the same problem class.
As an example  assuming our goal is to learn heuristics for rand3(25  106)  we begin by training to
solve rand3(5  21) for which a positive reward is obtained often enough to yield a quick improvement
in the policy. This improvement translates to larger problems  and we continue training the same
policy network to solve rand3(10  43) for which it becomes easier to quickly ﬁnd a solution compared
to starting from scratch. More speciﬁcally  at the ith curriculum step we train on a small distribution
Di for a ﬁxed number of steps while evaluating on a slightly larger distribution Di+1. For the next
step  we train on Di+1 beginning with the model parameters from before that took the lowest median
number of steps during evaluation on Di+1. In this manner we keep stepping up to larger problems
and ﬁnally we train on the distribution of the desired size.

7 Experiments

Evaluation As a baseline we have the SLS algorithm WalkSAT as described by Selman et al.
[39]. Note that although there have been improvements over WalkSAT in the last three decades  we
selected it due to its simplicity and its foundational role in the development of the state-of-the-art
SAT solvers that use SLS [3  10]. That being said  competing with the state-of-the-art SAT solvers in
run time is not our primary goal as the scalability of the heuristic computed by the GNN is currently
bound to be lacking  which we expect will be alleviated by the ongoing rapid advancements in

6

Table 2: Performance of the learned heuristics and WalkSAT. In the ﬁrst column  n and m on the
second line of each cell refers to the maximum number of variables and clauses in the sampled
formulas. For graph problems  the size of the graph G(N  p) and the size of the factor graph
corresponding to the SAT encoding are different. At each cell  there are three metrics (top to bottom):
average number of steps  median number of steps  percentage solved.

rand3(n  m)

cliquek(N  p)

coverk(N  p)

colork(N  p)

domsetk(N  p) WalkSAT

rand3(50  213)

clique3(20  0.05)
n = 60  m = 1725

cover5(9  0.5)
n = 55  m = 790

color5(20  0.5)
n = 100  m = 480

domset4(12  0.2)
n = 60  m = 995

367
273
84%
529
750
48%
749
750
0%
675
750
16%
729
750
0%

743
750
0%
116
57

100%
750
750
0%
748
750
0%
660
750
16%

749
750
0%
623
750
16%
181
115
100%
750
750
0%
304
169
76%

736
750
0%
743
750
0%
750
750
0%
342
223
88%
748
750
0%

642
750
20%
725
750
0%
224
162
100%
645
750
16%
205
121
100%

385
297
80%
237
182
100%
319
280
96%
416
379
80%
217
140
100%

domain-speciﬁc processors. Hence  WalkSAT serves mostly as a “sanity-check” for the learned
heuristics. This has been the case for other purely neural network-based approaches to SAT solving or
combinatorial optimization  and these kinds of studies are currently more of scientiﬁc than practical
interest. Nevertheless  we provide a comparison to WalkSAT.
In order to evaluate the algorithms  we sample 50 satisﬁable formulas from ﬁve problem distributions
to obtain evaluation sets  and perform 25 search trials (with each trial starting at a random initial
assignment) using the learned heuristics and WalkSAT. Each algorithm runs for a maximum of
750 steps unless speciﬁed otherwise and has a walk probability of 1/2. We model our evaluation
methodology after that of KhudaBukhsh et al. [25] and report three metrics for each evaluation set on
Table 2: the average number of steps  the median of the median number of steps (the inner median
is over trials on each problem and the outer median is over the problems in the evaluation sets)  the
percentage of instances considered solved (median number of steps less than the allowed number of
steps). For all the results reported in the next section we follow the same method. Our implementation
is available at https://github.com/emreyolcu/sat.

7.1 Results

Comparison to WalkSAT Table 2 summarizes the performance of the learned heuristics and
WalkSAT. Each column on the table corresponds to a heuristic trained to solve problems from a
certain class. For each heuristic we follow a curriculum as explained in Section 6.2  that is  we train
on incrementally larger problems of the same class and ﬁnally train on the distribution that we want
to evaluate the performance on. Each row of the table corresponds to an evaluation set.
After training  the learned heuristics require fewer steps than WalkSAT to solve their respective
problems. The difference is larger for graph problems than it is for random 3-SAT  which makes
sense as there is no particularly regular structure to exploit in its factor graph. While the number of
steps is reduced  we should emphasize that the running time of our algorithm is much longer than
WalkSAT. With this work  our goal has not been to produce a practical SAT solver  but to demonstrate
that this approach might allow us to create specialized algorithms from scratch for speciﬁc problems.

Specialization to problem classes As expected  the performance of each heuristic degrades on
unseen classes of problems compared to the one it is specialized to. This provides evidence that the
learned heuristics exploit class-speciﬁc features of the problems. Also  it is interesting to note that the
performance of the learned heuristics on vertex cover and dominating set problems correlate. They
are indeed similar problems  and it is not surprising to see that a good heuristic for one also performs
well for the other. Their similarity is also visible from their example factor graphs in Figure 2.

7

Figure 3: Curriculum during training. The plots display the average number of steps taken over
multiple trials during training when learning to solve SAT problems. Each plot includes two curves 
one for the small distribution from which we sample training problems  and another for a ﬁxed set of
evaluation problems from a larger distribution of the same problem class.

Effect of the curriculum Example training runs of the curriculum that we employ are shown in
Figure 3. They demonstrate that the learned heuristic for a smaller distribution transfers to a slightly
larger distribution from the same class  which allows us to step up to larger problems.

Generic heuristics Each learned heuristic is ex-
pected to exploit the speciﬁc structure of its problem
class as much as possible. Intuition suggests that
when there is no discernible structure to exploit  the
resulting heuristic may generalize relatively well to
other classes. As an experiment  we learn a heuristic
to solve satisﬁable formulas from the SR distribution 
created by Selsam et al. [see 42  section 4] to generate
formulas that are difﬁcult to classify as satisﬁable or
unsatisﬁable based on crude statistics. We then eval-
uate this heuristic on the graph problems. Random
3-SAT near the threshold is another similarly difﬁcult
distribution  although its difﬁculty is an asymptotic
statement and not necessarily useful for our case with
small formulas. Still  we repeat the experiment with
random 3-SAT for completeness. Table 3 shows that
a heuristic learned on SR(10) generalizes better to
the slightly larger and different problem distributions
compared to a heuristic learned on rand3(10  43).

Table 3: Generalization from SR(10) to dif-
ferent problem distributions. Each cell dis-
plays the average number of steps and the
percentage of problems solved.

SR(10)

rand3(10  43)

clique3(10  0.1)
n = 30  m = 420

cover4(8  0.5)
n = 40  m = 450

color4(15  0.5)
n = 60  m = 200

domset4(12  0.2)
n = 60  m = 995

363
98%

410
86%

204
98%

499
70%

649
4%

643
16%

390
84%

653
16%

Search behavior Table 2 shows that the largest differences between the learned heuristics and
WalkSAT are on clique detection and vertex cover problems. In order to gain an understanding of
how the learned heuristics behave differently  we look at a few statistics of how they traverse the
search space. Table 4 shows the ratio of the ﬂips during the search that ﬂip a previously ﬂipped
variable (undo)  move to a solution that increases (upwards)  does not change (sideways)  or decreases
(downwards) the number of unsatisﬁed clauses. The reported statistics are computed by taking into
account only the ﬂips that are made deterministically by the heuristics  although if a variable was
previously randomly ﬂipped and the heuristic chooses to ﬂip the same variable again then this is
counted as an “undo”. The striking difference is that the learned heuristics make sideways moves far
less often  and instead appear to zoom in on a solution with downwards moves. It is also intriguing
that they make “bad” moves (upwards) with relatively high frequency.

Table 4: Macroscopic comparison of the search behaviors of the learned heuristics and WalkSAT.

clique3(20  0.05)

cover5(9  0.5)

Heuristic
Learned
WalkSAT

Undo Upwards
0.26
0.33

0.28
0.16

Sideways Downwards

0.14
0.54

0.57
0.29

Undo Upwards
0.37
0.39

0.38
0.24

Sideways Downwards

0.13
0.40

0.48
0.35

8

010002000050100150200rand3(5 21)rand3(10 43)010002000204060clique3(5 0.2)clique3(10 0.1)0100020000100200300cover2(5 0.5)cover3(7 0.5)0100020000100200300color3(5 0.5)color3(10 0.5)01000200050100150200domset2(5 0.2)domset3(7 0.2)0.00.20.40.60.81.0Iteration0.00.20.40.60.81.0Average#stepsLearned WalkSAT

Table 5: Performance of the learned heuristics
and WalkSAT on problems with graphs from
distributions unseen during training. Each
cell displays the average number of steps and
the percentage of problems solved.

Different random graphs Above experiments on
SAT encoded graph problems all use the Erd˝os–Rényi
model for random graphs. Although the results on
Table 2 provide evidence that each heuristic is spe-
cialized to its respective problem class  they do not
allow us to conclude that the heuristics should still
work for different random graphs. Ideally  the heuris-
tic learned on graph coloring problems should more
heavily be exploiting the fact that the formula is the
SAT encoding of a coloring problem than the fact
that the encoding is for a graph sampled according to
the Erd˝os–Rényi model.
To test whether the heuristics can generalize to dif-
ferent random graphs  for each graph problem we
generate satisﬁable instances on graphs from four dis-
tributions (random regular [26]  random geometric
[36]  Barabási–Albert [4]  Watts–Strogatz [46]) with
the number of vertices varying from 10 to 15 and use
these instances to re-evaluate the previously learned
heuristics. Table 5 shows the results compared to WalkSAT. Each row on the table corresponds to a
set of 60 problems with varying problem speciﬁc size parameters k and graphs sampled from the
four distributions. While the shift in the evaluation distribution causes some decline in the relative
performance of the learned heuristics against WalkSAT  they can still ﬁnd solutions in fewer steps
than WalkSAT most of the time. Recall that the goal here is not necessarily to perform better than
WalkSAT  but being able to do so provides evidence of the robustness of the learned heuristics.

285
98%
370
92%
193
100%
255
92%

cliquek
3 ≤ k ≤ 5
coverk
4 ≤ k ≤ 6

colork
3 ≤ k ≤ 5
domsetk
2 ≤ k ≤ 4

198
100%
306
90%
162
100%
271
92%

Larger problems
In order to show the extent to
which a learned heuristic can generalize to larger
problems  we run the heuristic learned only on the
small problems from SR(10) to solve satisﬁable
problems sampled from SR(n) for much larger n
with a varying number of maximum steps. Figure 4
shows that as we increase the number of maximum
iterations  the percentage of problems solved scales
accordingly with no sign of plateauing. For compar-
ison  WalkSAT results are also included.

8 Conclusion

Figure 4: Generalization of the SR(10)
heuristic to larger problems. Solid and dashed
lines correspond to the learned heuristic and
WalkSAT  respectively.

We have shown that it is possible to use a graph
neural network to learn distribution-speciﬁc variable
selection heuristics for local search from scratch. As
formulated  the approach does not require access to
the solutions of the training problems  and this makes it viable to automatically learn specialized
heuristics for a variety of problem distributions. While the learned heuristics are not competitive with
the state-of-the-art SAT solvers in run time  after specialization they can ﬁnd solutions consistently in
fewer steps than a generic heuristic.
There are a number of avenues for improvement. Arguably  the most crucial next step would be
to reduce the cost of the heuristic to scale to larger problem instances. If a GNN is to be used for
computing the heuristic  more lightweight architectures than we have can be of interest. Also  in
this work we focused on stochastic local search  however  the SAT solvers used in practice perform
backtracking search. Consequently  model-based algorithms such as Monte Carlo tree search [12]
can provide critical improvements. As a step towards practicality  it is also important to incorporate
the suite of heuristics in an algorithm portfolio. In this work  we have achieved promising results for
learning heuristics  and we hope that this helps pave the way for automated algorithm design.

9

102103104Numberofmaximumiterations020406080100PercentageofproblemssolvedLearned SR(10)Learned SR(20)Learned SR(40)Learned SR(60)Learned SR(80)WalkSAT SR(10)WalkSAT SR(20)WalkSAT SR(40)WalkSAT SR(60)WalkSAT SR(80)References
[1] Saeed Amizadeh  Sergiy Matusevych  and Markus Weimer. Learning to solve Circuit-SAT: An
unsupervised differentiable approach. In International Conference on Learning Representations.
2019.

[2] Carlos Ansótegui  Jesús Giráldez-Cru  and Jordi Levy. The community structure of SAT
formulas. In Theory and Applications of Satisﬁability Testing – SAT 2012  pages 410–423 
2012.

[3] Adrian Balint and Uwe Schöning. Choosing probability distributions for stochastic local search
and the role of make versus break. In Theory and Applications of Satisﬁability Testing – SAT
2012  pages 16–29  2012.

[4] Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. Science 

286(5439):509–512  October 1999.

[5] Peter W. Battaglia  Jessica B. Hamrick  Victor Bapst  Alvaro Sanchez-Gonzalez  Vinicius Zam-
baldi  Mateusz Malinowski  Andrea Tacchetti  David Raposo  Adam Santoro  Ryan Faulkner 
Caglar Gulcehre  Francis Song  Andrew Ballard  Justin Gilmer  George Dahl  Ashish Vaswani 
Kelsey Allen  Charles Nash  Victoria Langston  Chris Dyer  Nicolas Heess  Daan Wierstra 
Pushmeet Kohli  Matt Botvinick  Oriol Vinyals  Yujia Li  and Razvan Pascanu. Relational
inductive biases  deep learning  and graph networks. arXiv:1806.01261  October 2018.

[6] Roberto J. Bayardo  Jr. and Robert C. Schrag. Using CSP look-back techniques to solve
real-world SAT instances. In Proceedings of the Fourteenth National Conference on Artiﬁcial
Intelligence and Ninth Conference on Innovative Applications of Artiﬁcial Intelligence  pages
203–208  1997.

[7] Irwan Bello  Hieu Pham  Quoc V. Le  Mohammad Norouzi  and Samy Bengio. Neural combi-

natorial optimization with reinforcement learning. arXiv:1611.09940  January 2017.

[8] Yoshua Bengio  Jérôme Louradour  Ronan Collobert  and Jason Weston. Curriculum learning.
In Proceedings of the 26th Annual International Conference on Machine Learning  pages 41–48 
2009.

[9] Yoshua Bengio  Andrea Lodi  and Antoine Prouvost. Machine learning for combinatorial

optimization: A methodological tour d’horizon. arXiv:1811.06128  November 2018.

[10] Armin Biere. Yet another local search solver and Lingeling and friends entering the SAT

competition 2014. In Proceedings of SAT Competition 2014  pages 39–40. 2014.

[11] Armin Biere  Marijn J. H. Heule  Hans van Maaren  and Toby Walsh. Handbook of Satisﬁability.

IOS Press  2009.

[12] Cameron B. Browne  Edward Powley  Daniel Whitehouse  Simon M. Lucas  Peter I. Cowling 
Philipp Rohlfshagen  Stephen Tavener  Diego Perez  Spyridon Samothrakis  and Simon Colton.
A survey of Monte Carlo tree search methods. IEEE Transactions on Computational Intelligence
and AI in Games  4(1):1–43  March 2012.

[13] Stephen A. Cook. The complexity of theorem-proving procedures. In Proceedings of the Third

Annual ACM Symposium on Theory of Computing  pages 151–158  1971.

[14] James M. Crawford and Larry D. Auton. Experimental results on the crossover point in random

3-SAT. Artiﬁcial Intelligence  81(1):31–57  March 1996.

[15] Niklas Eén and Niklas Sörensson. An extensible SAT-solver. In Theory and Applications of

Satisﬁability Testing – SAT 2004  pages 502–518  2004.

[16] Paul Erd˝os and Alfréd Rényi. On random graphs. I. Publicationes Mathematicae  6:290–297 

1959.

[17] Andreas Fröhlich  Armin Biere  Christoph Wintersteiger  and Youssef Hamadi. Stochastic local
search for satisﬁability modulo theories. In Proceedings of the Twenty-Ninth AAAI Conference
on Artiﬁcial Intelligence  pages 1136–1143  2015.

[18] Alex S. Fukunaga. Evolving local search heuristics for SAT using genetic programming. In

Genetic and Evolutionary Computation – GECCO 2004  pages 483–494  2004.

[19] Alex S. Fukunaga. Automated discovery of local search heuristics for satisﬁability testing.

Evolutionary Computation  16(1):31–61  March 2008.

10

[20] Justin Gilmer  Samuel S. Schoenholz  Patrick F. Riley  Oriol Vinyals  and George E. Dahl.
In Proceedings of the 34th International

Neural message passing for quantum chemistry.
Conference on Machine Learning  pages 1263–1272  2017.

[21] Michele Gori  Gabriele Monfardini  and Franco Scarselli. A new model for learning in graph
domains. In Proceedings of the 2005 IEEE International Joint Conference on Neural Networks 
volume 2  pages 729–734  July 2005.

[22] Marketa Illetskova  Alex R. Bertels  Joshua M. Tuggle  Adam Harter  Samuel Richter  Daniel R.
Tauritz  Samuel Mulder  Denis Bueno  Michelle Leger  and William M. Siever. Improving
performance of CDCL SAT solvers by automated design of variable selection heuristics. In
2017 IEEE Symposium Series on Computational Intelligence (SSCI)  pages 1–8  November
2017.

[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Proceedings of the 32nd International Conference on
International Conference on Machine Learning  pages 448–456  2015.

[24] Elias Khalil  Hanjun Dai  Yuyu Zhang  Bistra Dilkina  and Le Song. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems
30  pages 6348–6358. 2017.

[25] Ashiqur R. KhudaBukhsh  Lin Xu  Holger H. Hoos  and Kevin Leyton-Brown. SATenstein:
Automatically building local search SAT solvers from components. Artiﬁcial Intelligence  232:
20–42  March 2016.

[26] Jeong Han Kim and Van H. Vu. Generating random regular graphs. In Proceedings of the

Thirty-Fifth Annual ACM Symposium on Theory of Computing  pages 213–222  2003.

[27] Wouter Kool  Herke van Hoof  and Max Welling. Attention  learn to solve routing problems! In

International Conference on Learning Representations. 2019.

[28] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E. Hinton. ImageNet classiﬁcation with deep
convolutional neural networks. In Advances in Neural Information Processing Systems 25 
pages 1097–1105. 2012.

[29] Michail G. Lagoudakis and Michael L. Littman. Learning to select branching rules in the DPLL
procedure for satisﬁability. Electronic Notes in Discrete Mathematics  9:344–359  June 2001.
[30] Massimo Lauria  Jan Elffers  Jakob Nordström  and Marc Vinyals. CNFgen: A generator of
crafted benchmarks. In Theory and Applications of Satisﬁability Testing – SAT 2017  pages
464–473  2017.

[31] Gil Lederman  Markus N. Rabe  Edward A. Lee  and Sanjit A. Seshia. Learning heuristics for

automated reasoning through deep reinforcement learning. arXiv:1807.08058  April 2019.

[32] Zhuwen Li  Qifeng Chen  and Vladlen Koltun. Combinatorial optimization with graph convolu-
tional networks and guided tree search. In Advances in Neural Information Processing Systems
31  pages 539–548. 2018.

[33] Jia Hui Liang  Vijay Ganesh  Pascal Poupart  and Krzysztof Czarnecki. Exponential recency
weighted average branching heuristic for SAT solvers. In Proceedings of the Thirtieth AAAI
Conference on Artiﬁcial Intelligence  pages 3434–3440  2016.

[34] Jia Hui Liang  Vijay Ganesh  Pascal Poupart  and Krzysztof Czarnecki. Learning rate based
branching heuristic for SAT solvers. In Theory and Applications of Satisﬁability Testing – SAT
2016  pages 123–140  2016.

[35] Jia Hui Liang  Hari Govind V.K.  Pascal Poupart  Krzysztof Czarnecki  and Vijay Ganesh. An
empirical study of branching heuristics through the lens of global learning rate. In Theory and
Applications of Satisﬁability Testing – SAT 2017  pages 119–135  2017.

[36] Mathew Penrose. Random Geometric Graphs. Oxford University Press  May 2003.
[37] Stuart Russell and Peter Norvig. Artiﬁcial Intelligence: A Modern Approach. Prentice Hall

Press  3rd edition  2009.

[38] Franco Scarselli  Marco Gori  Ah Chung Tsoi  Markus Hagenbuchner  and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks  20(1):61–80  January
2009.

11

[39] Bart Selman  Henry A. Kautz  and Bram Cohen. Local search strategies for satisﬁability testing.

In Cliques  Coloring  and Satisﬁability  pages 521–532  1993.

[40] Bart Selman  David G. Mitchell  and Hector J. Levesque. Generating hard satisﬁability problems.

Artiﬁcial Intelligence  81(1):17–29  March 1996.

[41] Daniel Selsam and Nikolaj Bjørner. Guiding high-performance SAT solvers with unsat-core
predictions. In Theory and Applications of Satisﬁability Testing – SAT 2019  pages 336–353 
2019.

[42] Daniel Selsam  Matthew Lamm  Benedikt Bünz  Percy Liang  Leonardo de Moura  and David L.
In International Conference on

Dill. Learning a SAT solver from single-bit supervision.
Learning Representations. 2019.

[43] David Silver  Thomas Hubert  Julian Schrittwieser  Ioannis Antonoglou  Matthew Lai  Arthur
Guez  Marc Lanctot  Laurent Sifre  Dharshan Kumaran  Thore Graepel  Timothy Lillicrap 
Karen Simonyan  and Demis Hassabis. A general reinforcement learning algorithm that masters
chess  shogi  and Go through self-play. Science  362(6419):1140–1144  December 2018.

[44] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press 

1st edition  1998.

[45] Tijmen Tieleman and Geoffrey E. Hinton. RMSProp: Divide the gradient by a running average

of its recent magnitude. Slides of lecture “Neural Networks for Machine Learning”  2012.

[46] Duncan J. Watts and Steven H. Strogatz. Collective dynamics of ‘small-world’ networks. Nature 

393(6684):440–442  June 1998.

[47] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine Learning  8(3):229–256  May 1992.

[48] Lin Xu  Frank Hutter  Holger H. Hoos  and Kevin Leyton-Brown. SATzilla: Portfolio-based
algorithm selection for SAT. Journal of Artiﬁcial Intelligence Research  32(1):565–606  June
2008.

12

,Emre Yolcu
Barnabas Poczos