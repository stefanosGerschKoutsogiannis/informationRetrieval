2019,Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem,We prove several fundamental statistical bounds for entropic OT with the squared Euclidean cost between subgaussian probability measures in arbitrary dimension.
First  through a new sample complexity result we establish the rate of convergence of entropic OT for empirical measures.
Our analysis improves exponentially on the bound of Genevay et al.~(2019) and extends their work to unbounded measures.
Second  we establish a central limit theorem for entropic OT  based on techniques developed by Del Barrio and Loubes~(2019).
Previously  such a result was only known for finite metric spaces.
As an application of our results  we develop and analyze a new technique for estimating the entropy of a random variable corrupted by gaussian noise.,Statistical bounds for entropic optimal transport:
sample complexity and the central limit theorem

Gonzalo Mena

Harvard

Jonathan Niles-Weed

NYU

Abstract

We prove several fundamental statistical bounds for entropic OT with the squared
Euclidean cost between subgaussian probability measures in arbitrary dimension.
First  through a new sample complexity result we establish the rate of convergence
of entropic OT for empirical measures. Our analysis improves exponentially on
the bound of Genevay et al. (2019) and extends their work to unbounded measures.
Second  we establish a central limit theorem for entropic OT  based on techniques
developed by Del Barrio and Loubes (2019). Previously  such a result was only
known for ﬁnite metric spaces. As an application of our results  we develop and
analyze a new technique for estimating the entropy of a random variable corrupted
by gaussian noise.

1

Introduction

Optimal transport is an increasingly popular tool for the analysis of large data sets in high dimension 
with applications in domain adaptation (Courty et al.  2014  2017)  image recognition (Li et al. 
2013; Rubner et al.  2000; Sandler and Lindenbaum  2011)  and word embedding (Alvarez-Melis and
Jaakkola  2018; Grave et al.  2018). Its ﬂexibility and simplicity have made it an attractive choice for
practitioners and theorists alike  and its ubiquity as a machine learning tool continues to grow (see 
e.g.  Peyré et al.  2019; Kolouri et al.  2017  for surveys).
Much of the recent interest in optimal transport has been driven by algorithmic advances  chief
among them the popularization of entropic regularization as a tool of solving large-scale OT problems
quickly (Cuturi  2013). Not only has this proposal been shown to yield near-linear-time algorithms for
the original optimal transport problem (Altschuler et al.  2017)  but it also appears to possess useful
statistical properties which make it an attractive choice for machine learning applications (Rigollet and
Weed  2018; Genevay et al.  2017; Schiebinger et al.  2019; Montavon et al.  2016). For instance  in a
recent breakthrough work  Genevay et al. (2019) established that even though the empirical version of
√
standard OT suffers from the “curse of dimensionality” (see  e.g. Dudley  1969)  the empirical version
of entropic OT always converges at the parametric 1/
n rate for compactly supported probability
measures. This result suggests that entropic OT may be signiﬁcantly more useful than unregularized
OT for inference tasks when the dimension is large. However  obtaining rigorous guarantees for
the performance of entropic OT in practice requires a more thorough understanding of its statistical
behavior.

1.1 Summary of contributions

We prove new results on the relation between the population and empirical version of the entropic
cost  that is  between S(P  Q) and S(Pn  Qn) (deﬁned in Section 1.2  below). These results give
the ﬁrst characterization of the large-sample behavior of entropic OT for unbounded probability
measures in arbitrary dimension. Speciﬁcally  we obtain: (i) New sample complexity bounds on
E|S(P  Q)− S(Pn  Qn)|: ﬁrst  we improve on the results of Genevay et al. (2019) by an exponential

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

factor  and then  extend these to unbounded measures (Section 2). (ii) A central limit theorem
characterizing the ﬂuctuations S(Pn  Qn)− ES(Pn  Qn) when P and Q are subgaussian (Section 3).
Such a central limit theorem was previously only known for probability measures supported on
a ﬁnite number of points (Bigot et al.  2017; Klatt et al.  2018). We use completely different
techniques  inspired by recent work of Del Barrio and Loubes (2019)  to prove our theorem for
general subgaussian distributions.
As an application of our results  we show how entropic OT can be used to shed new light on the
entropy estimation problem for random variables corrupted by subgaussian noise (Section 4). This
problem has gained recent interest in machine learning (Goldfeld et al.  2018  2019) as a tool for
obtaining a theoretically sound understanding of the Information Bottleneck Principle in deep learning
(Tishby and Zaslavsky  2015). We design and analyze a new estimator for this problem based on
entropic OT.
Finally  we provide simulations which give empirical validation for our theoretical claims (Section 5).

1.2 Background and preliminaries
Let P  Q ∈ P(Rd) be two probability measures and let Pn and Qn be the empirical measures from
the independent samples {Xi}i≤n ∼ P n and {Yi}i≤n ∼ Qn. We deﬁne the squared Wasserstein
distance between P and Q (Villani  2008) as follows:

(cid:20)(cid:90)

(cid:21)

W 2

2 (P  Q) := inf

π∈Π(P Q)

1
2

X×Y

(cid:107)x − y(cid:107)2 dπ(x  y)

 

(1)

where Π(P  Q) is the set of all joint distributions with marginals equal to P and Q  respectively. We
focus on a entropy regularized version of the above cost (Cuturi  2013; Peyré et al.  2019)  deﬁned as

(cid:20)(cid:90)

(cid:21)

(cid:90)

(cid:82) log dα

S(P  Q) := inf

π∈Π(P Q)

X×Y

(cid:107)x − y(cid:107)2 dπ(x  y) + H(π|P ⊗ Q)

1
2

 

(2)

where H(α|β) denotes the relative entropy between probability measures α and β deﬁend by
dβ (x)dα(x) if α (cid:28) β and +∞ otherwise. By rescaling the measures P and Q and the
regularization parameter   it sufﬁces to analyze the case  = 1  which we denote by S(P  Q). Note
2(cid:107) · (cid:107)2 in the deﬁnition of S(P  Q)  since most of our
that we have considered the squared cost 1
bounds heavily depend on this cost. However  more general costs c(x  y) may be considered  and
indeed some of our results (e.g. Proposition 4) are stated for more general c(x  y). We leave a full
analysis of the general case to future work.
The general theory of entropic OT (Csiszár  1975) implies that S(P  Q) possesses a dual formulation:

(cid:90)

(cid:90)

(cid:90)

S(P  Q) =

sup

f∈L1(P ) g∈L1(Q)

f (x) dP (x)+

g(y) dQ(y)−

ef (x)+g(y)− 1

2||x−y||2

dP (x)dQ(y)+1  

(3)
and that as long as P and Q have ﬁnite second moments  the supremum is attained at a pair of optimal
potentials (f  g) satisfying

ef (x)+g(y)− 1

2||x−y||2

dQ(y) = 1 P -a.s. 

ef (x)+g(y)− 1

2||x−y||2

dP (x) = 1 Q-a.s.

Conversely  any f ∈ L1(P )  g ∈ L1(Q) satisfying (4) are optimal potentials.
We focus throughout on subgaussian probability measures. We say that a distribution P ∈ P(Rd)
is σ2-subgaussian for σ ≥ 0 if EP e
2dσ2 ≤ C for any
constant C ≥ 2  then P is Cσ2-subgaussian. Note that if P is subgaussian  then EP ev(cid:62)X < ∞
for all v ∈ Rd. Conversely  standard results (see  e.g.  Vershynin  2018) imply that our deﬁnition is
satisﬁed if EP eu(cid:62)X ≤ e(cid:107)u(cid:107)2σ2/2 for all u ∈ Rd.

2dσ2 ≤ 2. By Jensen’s inequality  if EP e

(cid:107)X(cid:107)2

(cid:107)X(cid:107)2

(4)

2

(cid:90)

2 Sample complexity for the entropic transportation cost for general

subgaussian measures

One rigorous statistical beneﬁt of entropic OT is its sample complexity  i.e.  the minimum number
of samples required for the empirical entropic OT cost S(Pn  Qn) to be an accurate estimate of
S(P  Q). As noted above  unregularized OT suffers from the curse of dimensionality: in general  the
2 (P  Q) no faster than n−1/d for measures in Rd.
Wasserstein distance W 2
Strikingly  Genevay et al. (2019) established that the statistical performance of the entropic OT cost
is signiﬁcantly better. They show:1
Theorem 1 (Genevay et al.  2019  Theorem 3). Let P and Q be two probability measures on a
bounded domain in Rd of diameter D. Then

2 (Pn  Qn) converges to W 2

(cid:18)

(cid:19) eD2/√

n

 

(5)

EP Q|S(P  Q) − S(Pn  Qn)| ≤ KD d

sup
P Q

where KD d is a constant depending on D and d.

1 +

1

(cid:98)d/2(cid:99)

This impressive result offers powerful evidence that entropic OT converges signiﬁcantly faster than its
unregularized counterpart. The drawbacks of this result are that it applies only to bounded measures 
and  perhaps more critically in applications  the rate scales exponentially in D and 1/  even in
dimension 1. Therefore  while the qualitative message of Theorem 1 is clear  it does not offer useful
quantitative bounds as soon as the measure is unbounded or lies in a set of large diameter.
Our ﬁrst theorem is a signiﬁcant sharpening of Theorem 1. We ﬁrst state it for the case where  = 1.
Theorem 2. If P and Q are σ2-subgaussian  then

EP Q|S(P  Q) − S(Pn  Qn)| ≤ Kd(1 + σ(cid:100)5d/2(cid:101)+6)

1√
n

.

(6)

If we denote by P  and Q the pushforwards of P and Q under the map x (cid:55)→ −1/2x  then it is easy
to see that

S(P  Q) = S(P   Q) .

We immediately obtain the following corollary.
Corollary 1. If P and Q are σ2-subgaussian  then

EP Q|S(P  Q) − S(Pn  Qn)| ≤ Kd · 

(cid:18)

1 +

(cid:19) 1√

n

.

σ(cid:100)5d/2(cid:101)+6
(cid:100)5d/4(cid:101)+3

If we compare Corollary 1 with Theorem 1  we note that the polynomial prefactor in Corollary 1
has higher degree than the one in Theorem 1  pointing to a potential weakness of our bound. On the
other hand  the exponential dependence on D2/ has completely disappeared. Moreover  the brittle
quantity D  ﬁnite only for compactly supported measures  has been replaced by the more ﬂexible
subgaussian variance proxy σ2.
The improvements in Theorem 2 are obtained via two different methods. First  a simple argument
allows us to remove the exponential term and bound the desired quantity by an empirical process 
as in Genevay et al. (2019). Much more challenging is the extension to measures with unbounded
support. The proof technique of Genevay et al. (2019) relies on establishing uniform bounds on the
derivatives of the optimal potentials  but this strategy cannot succeed if the support of P and Q is not
compact. We therefore employ a more careful argument based on controlling the Hölder norms of the
optimal potentials on compact sets. A chaining bound completes our proof.
In Proposition 1 below (whose proof we defer to the supplement) we show that if (f  g) is a pair of
optimal potentials for σ2-subgaussian distributions P and Q  then we may control the size of f and
its derivatives.
Proposition 1. Let P and Q be σ2-subgaussian distributions. There exist optimal dual potentials
(f  g) for P and Q such that for any multi-index α with |α| = k 

|Dα(f − 1
2

(cid:107) · (cid:107)2)(x)| ≤ Ck d

σk(σ + σ2)k

k = 0
otherwise 

(7)

(cid:26) 1 + σ4

1We have specialized their result to the squared Euclidean cost.

3

if (cid:107)x(cid:107) ≤ √

dσ  and

|Dα(f − 1
2

(cid:107) · (cid:107)2)(x)| ≤ Ck d

√

(cid:26) 1 + (1 + σ2)(cid:107)x(cid:107)2
σk((cid:112)σ(cid:107)x(cid:107) + σ(cid:107)x(cid:107))k

k = 0
otherwise 

(8)

dσ  where Ck d is a constant depending only on k and d.

if (cid:107)x(cid:107) >
We denote by Fσ the set of functions satisfying the bounds (7) and (8). The following proposition
shows that it sufﬁces to control an empirical process indexed by this set.
Proposition 2. Let P   Q  and Pn be ˜σ2-subgaussian distributions  for a possibly random ˜σ ∈ [0 ∞).
Then

|S(Pn  Q) − S(P  Q)| ≤ 2 sup
u∈F˜σ

|EP u − EPn u| .

(9)

Proof. We deﬁne the operator Aα β(u  v) for the pair of probability measures (α  β) and functions
(u  v) ∈ L1(α) ⊗ L1(β) as:

(cid:90)

(cid:90)

(cid:90)

Aα β(u  v) =

u(x) dα(x) +

v(y) dβ(y) −

eu(x)+v(y)− 1

2||x−y||2

dα(x)dβ(y) + 1 .

Denote by (fn  gn) a pair of optimal potentials for (Pn  Q) and (f  g) for (P  Q)  respectively. By
Proposition A.1 in the supplement  we can choose smooth optimal potentials (f  g) and (fn  gn) so
that the condition (4) holds for all x  y ∈ Rd. Proposition 1 shows that f  fn ∈ F˜σ.
Strong duality implies that S(P  Q) = AP Q(f  g) and S(Pn  Q) = APn Q(fn  gn). Moreover  by
the optimality of (f  g) and (fn  gn) for their respective dual problems  we obtain
AP Q(fn  gn) − APn Q(fn  gn) ≤ AP Q(f  g) − APn Q(fn  gn) ≤ AP Q(f  g) − APn Q(f  g) .
We conclude that

|S(P  Q) − S(Pn  Q)| = |AP Q(f  g) − APn Q(fn  gn)|

≤ |AP Q(f  g) − APn Q(f  g)| + |AP Q(fn  gn) − APn Q(fn  gn)| .
to

|AP Q(f  g) − APn Q(f  g)|

differences

bound

the

and

sufﬁces

therefore

It
|AP Q(fn  gn) − APn Q(fn  gn)|.

Upon deﬁning h(x) :=(cid:82) eg(y)− 1
(cid:16)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:90)

AP Q(f  g)−APn Q(f  g) =
Since (f  g) satisfy ef (x)h(x) = 1 for all x ∈ Rd  the second term above vanishes. Therefore
|AP Q(f  g)−APn Q(f  g)| =

f (x)(dP (x)− dPn(x))

ef (x)h(x)(dP (x)− dPn(x))

u(x)(dP (x)− dPn(x))

2||x−y||2

dQ(y) we have
f (x)(dP (x)− dPn(x))

+

(cid:16)(cid:90)
(cid:17)
(cid:12)(cid:12)(cid:12) ≤ sup

u∈F˜σ

(cid:12)(cid:12)(cid:12)(cid:90)

.

(cid:17)
(cid:12)(cid:12)(cid:12) .

Analogously 

|AP Q(fn  gn) − APn Q(fn  gn)| ≤ sup
u∈F˜σ

This proves the claim.

(cid:12)(cid:12)(cid:12)(cid:90)

u(x)(dP (x) − dPn(x))

(cid:12)(cid:12)(cid:12) .

Proposition 2 can be extended to apply to simultaneously varying Pn and Qn.
Corollary 2. Let P   Q  Pn  and Qn be ˜σ2-subgaussian distributions  where ˜σ ∈ [0 ∞) is possibly
random. Then
|S(Pn  Qn) − S(P  Q)| (cid:46) sup
u∈F˜σ

u(x)(dQ(x) − dQn(x))

u(x)(dP (x) − dPn(x))

(cid:12)(cid:12)(cid:12) + sup

(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:90)

u∈F˜σ

(cid:12)(cid:12)(cid:12)

almost surely.

4

Proof. By the triangle inequality 

|S(Pn  Qn) − S(P  Q)| ≤ |S(P  Q) − S(Pn  Q)| + |S(Pn  Q) − S(Pn  Qn)| .

(10)

Since P   Q  Pn  and Qn are all ˜σ2-subgaussian  Proposition 2 can be applied to both terms.
The majority of our work goes into bounding the resulting empirical process. Let s ≥ 2. Fix a
constant Cs d and denote by F s the set of functions satisfying

|f (x)| ≤ Cs d(1 + (cid:107)x(cid:107)2)
|Dαf (x)| ≤ Cs d(1 + (cid:107)x(cid:107)s)

∀α : |α| ≤ s .
1+σ3s f ∈ F s for all f ∈ Fσ.

1

(11)
(12)

Proposition 1 establishes that if Cs d is large enough  then
The key result is the following covering number bound  whose proof we defer to the supplement.
Denote by N (ε F s  L2(Pn)) the covering number with respect to the (random) metric L2(Pn)

deﬁned by (cid:107)f(cid:107)L2(Pn) =(cid:0) 1

n

i=1 f (Xi)2(cid:1)1/2.
(cid:80)n

Proposition 3. Let s = (cid:100)d/2(cid:101) + 1. If P is σ2-subgaussian and Pn is an empirical distribution  then
there exists a random variable L depending on the sample X1  . . .   Xn satisfying EL ≤ 2 such that

and

log N (ε F s  L2(Pn)) ≤ CdLd/2sε−d/s(1 + σ2d)  

(cid:107)f(cid:107)2

L2(Pn) ≤ Cd(1 + Lσ4) .

max
f∈F s

We can now prove Theorem 2.

Proof of Theorem 2. Let ˜σ be the inﬁmum over all τ > 0 such that P   Q  Pn  and Qn are all
τ 2-subgaussian. By Lemma A.2 in the supplement  ˜σ is ﬁnite almost surely.
By Corollary 2 

EP Q|S(P  Q) − S(Pn  Qn)| (cid:46) E sup
u∈F˜σ

u(x)(dP (x) − dPn(x))

+ E sup
u∈F˜σ

u(x)(dQ(x) − dQn(x))

We will show how to bound the ﬁrst term  and the second will follow in exactly the same way.

For any set of functions F  we write (cid:107)P − Pn(cid:107)F = supu∈F ((cid:82) u(x)(dP (x)− dPn(x))). Recall that 

for s = (cid:100)d/2(cid:101) + 1  if u ∈ F˜σ then

1+˜σ3s u ∈ F s. Therefore

1

(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12) .

E(cid:107)P − Pn(cid:107)Fσ ≤ E(1 + ˜σ3s)(cid:107)P − Pn(cid:107)F s

≤ (E(1 + ˜σ3s)2)1/2(E(cid:107)P − Pn(cid:107)2F s )1/2 .

Then by Giné and Nickl (2016  Theorem 3.5.1 and Exercise 2.3.1)  we have

E(cid:107)P − Pn(cid:107)2F s (cid:46) 1
n

E

maxf∈F s (cid:107)f(cid:107)2

L2 (Pn )

(cid:32)(cid:90) (cid:113)
(cid:32)(cid:90) Cd

0

√

(cid:33)2
(cid:112)log 2N (τ F s  L2(Pn)) dτ
(cid:33)2

1 + Ld/2sτ−d/s(1 + σ2d) dτ

(cid:113)

(cid:33)2

Ld/4sτ−d/2s dτ

≤ Cd

1
n

E

0

≤ Cd

1
n

(1 + σ2d)E

≤ Cd

1
n

(1 + σ2d)E

(1+Lσ4)

√

(cid:32)(cid:90) Cd
(cid:104)
(1 + Lσ4)1−d/2s(cid:105)

(1+Lσ4)

0

 

5

where in the last step we have used that d/2s < 1 so that τ−d/2s is integrable in a neighborhood of
the origin. Applying the bound on EL yields that this expression is bounded by Cd(1 + σ2d+4) 1
n.

Lemma A.4 in the supplement shows that E ˜σ2k ≤ Ckσ2k for all positive integers k. Combining
these bounds yields

E(cid:107)P − Pn(cid:107)Fσ ≤ Cd(1 + σ3s)(1 + σd+2)

1√
n

 

as desired.

3 A central limit theorem for entropic OT

The results of Section 2 show that  for general subgaussian measures  the empirical quantity
S(Pn  Qn) converges to S(P  Q) in expectation at the parametric rate. However  in order to use en-
tropic OT for rigorous statistical inference tasks  much ﬁner control over the deviations of S(Pn  Qn)
is needed  for instance in the form of asymptotic distributional limits. In this section  we accom-
plish this goal by showing a central limit theorem (CLT) for S(Pn  Qn)  valid for any subgaussian
measures.
Bigot et al. (2017) and Klatt et al. (2018) have shown CLTs for entropic OT when the measures lie
in a ﬁnite metric space (or  equivalently  when P and Q are ﬁnitely supported). Apart from being
restrictive in practice  these results do not shed much light on the general situation because OT on
ﬁnite metric spaces behaves quite differently from OT on Rd.2 Very recently  distributional limits for
general measures possessing 4 + δ moments have been obtained for unregularized OT by Del Barrio
and Loubes (2019). Our proof follows their approach.
We prove the following.
Theorem 3. Let X1  . . . Xn ∼ P be an i.i.d. sequence  and denote by (f g) the optimal potentials
in (4). If P is subgaussian  then

√

n (S(Pn  Q) − E(S(Pn  Q))

D→ N (0  VarP (f (X)))  

(13)

(14)
Likewise  let X1  . . .   Xn ∼ P and Y1 ∼ Ym ∼ Q are two i.i.d. sequences independent of each other.
Assume P and Q are both subgaussian. Denote λ := limm n→∞ n

lim
n→∞ n Var(S(Pn  Q)) = VarP (f (X)) .

m+n ∈ (0  1).

(S(Pn  Qm) − E(S(Pn  Qm))

D→ N (0  (1 − λ) VarP (f (X1)) + λ VarQ(g(Y1)))  

and

Then(cid:114) mn

m + n

(15)

(16)

and

lim

m n→∞

mn

m + n

Var(S(Pn  Qm)) = (1 − λ) VarP (f (X)) + λ VarQ(g(Y )).

The proof is deeply inspired by the method developed in Del Barrio and Loubes (2019) for the
squared Wasserstein distance  and we roughly follow the same strategy.

Proof of Theorem 3. The proof  in the one-sample case  proceeds as follows:

potentials for (P  Q) uniformly on compact sets.

(a) In Proposition A.2 we show that the optimal potentials for (Pn  Q) convergence to optimal

(b) Letting Rn := S(Pn  Q) −(cid:82) f (x)dPn(x)  we show in Proposition A.3 in the supplement 
(cid:82) f (x)dPn. Then  (13) and (14) are simply the limit statements (in distribution and L2 

(c) The above convergence indicates S(Pn  Q) can be approximated by the linear quantity

that this uniform convergence implies that limn→∞ n Var(Rn) = 0.

respectively) applied to this linearization.

2A thorough discussion of the behavior of unregularized OT for ﬁnitely supported measures can be found in

Sommerfeld and Munk (2018) and Weed and Bach (2018).

6

We omit the proof of the two-sample case as the changes to the argument (see Theorem 3.3. in
Del Barrio and Loubes (2019)  for the squared Wasserstein distance) adapt in a straightforward way
to the entropic case.

4 Application to entropy estimation

In this section  we give an application of entropic OT to the problem of entropy estimation. First  in
Proposition 4 we establish a new relation between entropic OT and the differential entropy of the
convolution of two measures. Then  as a corollary of this and the previous sections results we prove
Theorem 4  stating that entropic OT provides us with a novel estimator for the differential entropy of
the (independent) sum of a subgaussian random variable and a gaussian random variable  and for
which performance guarantees are available.
Throughout this section ν denotes a translation invariant measure. Whenever P has a density p with

respect to ν  we deﬁne its ν-differential entropy as h(P ) := −(cid:82) p(x) log p(x)dν(x) = −H(P|ν).

The following proposition links the differential entropy of a convolution with the entropic cost.
Proposition 4. Let Φg be the measure with ν-density φg(y) = Z−1
g e−g(y) for a smooth g (Zg is the
normalizing constant)  and deﬁne Q = P ∗ Φg  with P ∈ P(Rd) arbitrary. The ν-density of Q  q(y) 
satisﬁes

(cid:90)

(cid:90)

q(y) =

φg(y − x)dP (x) =

Z−1
g e−g(y−x)dP (x).

Consider the cost function c(x  y) := g(x− y) (not necessarily quadratic). Then  the optimal entropic
transport cost and differential entropy are linked through

h(P ∗ Φg) = S(P  P ∗ Φg) + log(Zg).

(17)

Proof. Deﬁne a more general entropic transportation cost involving the generic c and probability
measures α  β 3:

Sα⊗β(P  Q) := inf

π∈Π(P Q)

c(x  y)dπ(x  y) + H(π|α ⊗ β)

.

(18)

(cid:20)(cid:90)

Observe we may re-write (18) as

(cid:20)(cid:90)

(cid:21)

(cid:21)

Sα⊗β(P  Q) =

c(x  y)dπ(x  y) + H(π|P ⊗ Q)

+ H(P ⊗ Q|α ⊗ β)

inf

π∈Π(P Q)

X×Y

= S(P  Q) + H(P ⊗ Q|α ⊗ β).

Additionally  it can be veriﬁed an alternative representation for (18) is the following

Sα⊗β(P  Q) = inf

π∈Π(P Q)

H

π

− log(Z) 

where Z is the number making Λ := Z−1e−cα ⊗ β a bona ﬁde probability measure.
Now  take α = P   β = ν and Q = P ∗ Φg in the above expressions. For these choices we have
Z = Zg. Indeed  by the translation invariance of ν  we have

(19)

(20)

(cid:18)

(cid:19)

(cid:12)(cid:12)(cid:12)(cid:12)Z−1e−cα ⊗ β
(cid:90) (cid:18)(cid:90)
(cid:90) (cid:18)(cid:90)
(cid:90)

ZgdP (x) = Zg.

(cid:19)

(cid:19)

e−g(y−x)dν(y)

dP (x)

e−g(y)dν(y)

dP (x)

(cid:90)(cid:90)

Z =

e−c(x y)dP (x)dν(y) =

=

=

3Notice α ⊗ β need not be probability measures for the relative entropy H(·|α ⊗ β) to make sense. In

Léonard (2014) it is argued it sufﬁces that this product is σ-ﬁnite.

7

(a) If m = n 

(b) The limit

(cid:114) mn

m + n

(cid:19)

(cid:18) 1√

.

n

EP|ˆh(Q) − h(Q)| ≤ O

sup
P

(cid:16)ˆh(Q) − E(ˆh(Q)

(cid:17) D→ N (0  λ VarQ(log q(Y )))

Then  dΛ(x  y) = dP (x)φg(y − x)dν(y)  and by marginalization we deduce Λ ∈ Π(P  P ∗ Φg).
Therefore  the right side of (20) equals H(Λ|Λ) − log Zg = − log Zg. Finally  we combine (19) and
(20) to obtain

− log Zg = S(P  P ∗ Φg) + H (P ⊗ (P ∗ Φg)|P ⊗ ν)  

and achieve the ﬁnal conclusion after noting that

H(P ⊗ (P ∗ Φg)|P ⊗ ν) = H(P|P ) + H (P ∗ Φg|ν) = H (P ∗ Φg|ν) = −h(P ∗ Φg).

Now we can state the following theorem.
Theorem 4. Let P be subgaussian  and Φg = N (0  Id). Denote Q = P ∗ Φg the distribution
of the sum of an independent samples from P and Φg  and deﬁne the plug in estimator ˆh(Q) =
S(Pn  Qm) + log Zg where Pn and Qm are independent samples from P and Q. Then 

(21)

holds  where λ = limm n→∞ n
λ VarQ(log q(Y )).

m+n . Moreover 

limm n→∞ mn

m+n Var(ˆh(Q)) =

Proof. (a) is a simple re-statement of Theorem 2 in the light of Proposition 4. (b) is a re-statement of
Theorem 3  after noting in this case the optimal potentials are (f  g) = (− log Zg − log q).

√

The rate 1/
(2019) (see also Weed  2018)  but this estimator lacks distributional limits.

n in Theorem 4 is also achieved by a different estimator proposed by Goldfeld et al.

Figure 1: Top row: ES(Pn  Qn) as a function of n ∈ {1e3  2e3  5e3  1e4  1.5e4}  computed from
16  000 repetitions for each value of n. The shading corresponds to one standard deviation of
(cid:113) nn
S(Pn  Qn) − ES(Pn  Qn)  assuming the asymptotics of Theorem 3 are valid. Error bars are one
sample standard deviation long on each side. Both x and y axes are in logarithmic scale. Bottom
n+n (S(Pn  Qn) − ES(Pn  Qn))) when n = 1.5e4. Ground truth (numerical
row: histograms of
integration) is shown with black solid lines.

8

5 Empirical results

We provide empirical evidence supporting and illustrating our theoretical ﬁndings. We focus on the
entropy estimation problem because there are closed form expressions for the potentials (see Theorem
4)  and because it allows a comparison with the estimator studied in (Goldfeld et al.  2019) 4.
Speciﬁcally  consider X ∼ P = 1
2 (N (1d  Id) + N (−1d  Id))  the mixture of the gaussians centered
at 1d := (1  . . .   1) and −1d. We aim to estimate the entropy of the new mixture Q = P ∗ Φg.
Figure 1  top  shows the convergence of ES(Pn  Qn) to S(P  Q). Consistent with the bound in
Theorem 2 and Corollary 1  S(Pn  Qn) is a worse estimator for S(P  Q) when d is large or the
regularization parameter is small. We also plot the predicted (shading) and actual (bars) ﬂuctuations
of S(Pn  Qn) around its mean. Though the CLT holds only in the asymptotic limit  these experiments
reveal that the empirical ﬂuctuations in the ﬁnite-n regime are broadly consistent with the predictions
of the CLT. Figure 1  bottom  shows that the empirical distribution of the rescaled ﬂuctuations is an
excellent match for the predicted normal distribution.
In Figure 2 we compare the performance between entropic OT-based estimators from Theorem 4 and
ˆhm.g.(Q)  the one from (Goldfeld et al.  2019)  where h(P ∗ Φg) is estimated as the entropy of the
mixture of gaussians Pn ∗ Φg  in turn approximated by Monte Carlo integration. We consider two
OT-based estimators  ˆhind(Q) where Pn  Qn are completely independent (i.e.  the one used for Figure
1)  and ˆhpaired(Q) where samples Qn are drawn by adding gaussian noise to Pn. Observe that our
sample complexity and CLT results are only available for ˆhind(Q).
Results show a clear pattern of dominance  with Eˆhpaired(Q) achieving the fastest convergence. The
main caveat is the extra memory cost: while ˆhm.g.(Q) can be computed sequentially with each opera-
tion requiring O(n) memory  in the most naive implementation (used here) both ˆhpaired(Q)  ˆhind(Q)
demand O(n2) space for storing the matrix Di j = e−||xi−yj||2/2  to which the Sinkhorn algorithm
is applied. This memory requirement might be alleviated with the use of stochastic methods (Genevay
et al.  2016; Bercu and Bigot  2018).
We leave for future work both the implementation of more scalable methods for entropic OT  and a
detailed theoretical analysis of different entropic OT-based estimators (e.g. ˆhpaired(Q) v.s. ˆhind(Q))
that may bring about a better understanding of their observed substantial differences. Additionally  in
future work we will explore extensions of our results beyond the subgaussian case  and provide lower
bounds as in Goldfeld et al. (2019).

Figure 2: Comparison between Eˆhind(Q)  Eˆhpaired(Q)  Eˆhm.g.(Q). Details are the same as in Figure
1.

References

Altschuler  J.  Weed  J.  and Rigollet  P. (2017). Near-linear time approximation algorithms for
optimal transport via sinkhorn iteration. In Advances in Neural Information Processing Systems 
pages 1961–1971.

4We don’t present comparisons with the recent estimator presented in Berrett et al. (2019). This general
n−consistent and a CLT is available without a centering constant (as our ES(Pn  Qn)).

purpose estimator is
However  empirical results show the one in Goldfeld et al. (2019) performs much better.

√

9

Alvarez-Melis  D. and Jaakkola  T. S. (2018). Gromov-Wasserstein alignment of word embedding
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language

spaces.
Processing  Brussels  Belgium  October 31 - November 4  2018  pages 1881–1890.

Bercu  B. and Bigot  J. (2018). Asymptotic distribution and convergence rates of stochastic al-
gorithms for entropic optimal transportation between probability measures. arXiv preprint
arXiv:1812.09150.

Berrett  T. B.  Samworth  R. J.  Yuan  M.  et al. (2019). Efﬁcient multivariate entropy estimation via

k-nearest neighbour distances. The Annals of Statistics  47(1):288–318.

Bigot  J.  Cazelles  E.  and Papadakis  N. (2017). Central limit theorems for sinkhorn divergence
between probability distributions on ﬁnite spaces and statistical applications. arXiv preprint
arXiv:1711.08947.

Courty  N.  Flamary  R.  and Tuia  D. (2014). Domain adaptation with regularized optimal transport.

In ECML PKDD  pages 274–289.

Courty  N.  Flamary  R.  Tuia  D.  and Rakotomamonjy  A. (2017). Optimal transport for domain

adaptation. IEEE Trans. Pattern Anal. Mach. Intell.  39(9):1853–1865.

Csiszár  I. (1975). I-divergence geometry of probability distributions and minimization problems.

The Annals of Probability  pages 146–158.

Cuturi  M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in

neural information processing systems  pages 2292–2300.

Del Barrio  E. and Loubes  J.-M. (2019). Central limit theorems for empirical transportation cost in

general dimension. The Annals of Probability  47(2):926–951.

Dudley  R. M. (1969). The speed of mean Glivenko-Cantelli convergence. Ann. Math. Statist 

40:40–50.

Genevay  A.  Chizat  L.  Bach  F.    Cuturi  M.  and Peyré  G. (2019). Sample complexity of sinkhorn
divergences. In Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS).

Genevay  A.  Cuturi  M.  Peyré  G.  and Bach  F. (2016). Stochastic optimization for large-scale

optimal transport. In Advances in Neural Information Processing Systems  pages 3440–3448.

Genevay  A.  Peyré  G.  and Cuturi  M. (2017). Learning generative models with sinkhorn divergences.

arXiv preprint arXiv:1706.00292.

Giné  E. and Nickl  R. (2016). Mathematical foundations of inﬁnite-dimensional statistical models.
Cambridge Series in Statistical and Probabilistic Mathematics  [40]. Cambridge University Press 
New York.

Goldfeld  Z.  Berg  E. v. d.  Greenewald  K.  Melnyk  I.  Nguyen  N.  Kingsbury  B.  and Polyanskiy 

Y. (2018). Estimating information ﬂow in neural networks. arXiv preprint arXiv:1810.05728.

Goldfeld  Z.  Greenewald  K.  Polyanskiy  Y.  and Weed  J. (2019). Convergence of smoothed

empirical measures with applications to entropy estimation. arXiv preprint arXiv:1905.13576.

Grave  E.  Joulin  A.  and Berthet  Q. (2018). Unsupervised alignment of embeddings with Wasser-

stein procrustes. arXiv preprint arXiv:1805.11222.

Klatt  M.  Tameling  C.  and Munk  A. (2018). Empirical regularized optimal transport: Statistical

theory and applications. arXiv preprint arXiv:1810.09880.

Kolouri  S.  Park  S. R.  Thorpe  M.  Slepcev  D.  and Rohde  G. K. (2017). Optimal mass transport:
Signal processing and machine-learning applications. IEEE Signal Process. Mag.  34(4):43–59.

Léonard  C. (2014). Some properties of path measures. In Séminaire de Probabilités XLVI  pages

207–230. Springer.

10

Li  P.  Wang  Q.  and Zhang  L. (2013). A novel earth mover’s distance methodology for image

matching with Gaussian mixture models. In ICCV  pages 1689–1696.

Montavon  G.  Müller  K.-R.  and Cuturi  M. (2016). Wasserstein training of restricted boltzmann

machines. In Advances in Neural Information Processing Systems  pages 3718–3726.

Peyré  G.  Cuturi  M.  et al. (2019). Computational optimal transport. Foundations and Trends R(cid:13) in

Machine Learning  11(5-6):355–607.

Rigollet  P. and Weed  J. (2018). Entropic optimal transport is maximum-likelihood deconvolution.

Comptes rendus Mathématique  356(11–12).

Rubner  Y.  Tomasi  C.  and Guibas  L. J. (2000). The earth mover’s distance as a metric for image

retrieval. International journal of computer vision  40(2):99–121.

Sandler  R. and Lindenbaum  M. (2011). Nonnegative matrix factorization with earth mover’s distance
metric for image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 
33(8):1590–1602.

Schiebinger  G.  Shu  J.  Tabaka  M.  Cleary  B.  Subramanian  V.  Solomon  A.  Liu  S.  Lin  S. 
Berube  P.  Lee  L.  et al. (2019). Reconstruction of developmental landscapes by optimal-transport
analysis of single-cell gene expression sheds light on cellular reprogramming. Cell. To appear.

Sommerfeld  M. and Munk  A. (2018). Inference for empirical Wasserstein distances on ﬁnite spaces.

J. R. Stat. Soc. Ser. B. Stat. Methodol.  80(1):219–238.

Tishby  N. and Zaslavsky  N. (2015). Deep learning and the information bottleneck principle. In

2015 IEEE Information Theory Workshop (ITW)  pages 1–5. IEEE.

Vershynin  R. (2018). High-dimensional probability: An introduction with applications in data

science  volume 47. Cambridge University Press.

Villani  C. (2008). Optimal transport: old and new  volume 338. Springer Science & Business

Media.

Weed  J. (2018). Sharper rates for estimating differential entropy under gaussian convolutions.

Technical report  Massachusetts Institute of Technology.

Weed  J. and Bach  F. (2018). Sharp asymptotic and ﬁnite-sample rates of convergence of empirical

measures in Wasserstein distance. Bernoulli. To appear.

11

,Gonzalo Mena
Jonathan Niles-Weed