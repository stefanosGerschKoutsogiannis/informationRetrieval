2016,Mapping Estimation for Discrete Optimal Transport,We are interested in the computation of the transport map of an Optimal Transport problem. Most of the computational approaches of Optimal Transport use the Kantorovich relaxation of the problem to learn a probabilistic coupling $\mgamma$ but do not address the problem of learning the underlying transport map $\funcT$ linked to the original Monge problem. Consequently  it lowers the potential usage of such methods in contexts where out-of-samples computations are mandatory. In this paper we propose a new way to jointly learn the coupling and an approximation of the transport map. We use a jointly convex formulation which can be efficiently optimized. Additionally  jointly learning the coupling and the transport map allows to smooth the result of the Optimal Transport and generalize it to out-of-samples examples. Empirically  we show the interest and the relevance of our method in two tasks: domain adaptation and image editing.,Mapping Estimation for Discrete Optimal Transport

Micha¨el Perrot

Univ Lyon  UJM-Saint-Etienne  CNRS 
Lab. Hubert Curien UMR 5516  F-42023
michael.perrot@univ-st-etienne.fr

Nicolas Courty

Universit´e de Bretagne Sud 
IRISA  UMR 6074  CNRS 

courty@univ-ubs.fr

R´emi Flamary

Universit´e Cˆote d’Azur 

Lagrange  UMR 7293   CNRS  OCA

remi.flamary@unice.fr

Amaury Habrard

Univ Lyon  UJM-Saint-Etienne  CNRS 
Lab. Hubert Curien UMR 5516  F-42023
amaury.habrard@univ-st-etienne.fr

Abstract

We are interested in the computation of the transport map of an Optimal Transport
problem. Most of the computational approaches of Optimal Transport use the
Kantorovich relaxation of the problem to learn a probabilistic coupling γ but do
not address the problem of learning the underlying transport map T linked to
the original Monge problem. Consequently  it lowers the potential usage of such
methods in contexts where out-of-samples computations are mandatory. In this
paper we propose a new way to jointly learn the coupling and an approximation of
the transport map. We use a jointly convex formulation which can be efﬁciently
optimized. Additionally  jointly learning the coupling and the transport map allows
to smooth the result of the Optimal Transport and generalize it to out-of-samples
examples. Empirically  we show the interest and the relevance of our method in
two tasks: domain adaptation and image editing.

1

Introduction

In recent years Optimal Transport (OT) [1] has received a lot of attention in the machine learning
community [2  3  4  5]. This gain of interest comes from several nice properties of OT when used
as a divergence to compare discrete distributions: (i) it provides a sound and theoretically grounded
way of comparing multivariate probability distributions without the need for estimating parametric
versions and (ii) by considering the geometry of the underlying space through a cost metric  it can
encode useful information about the nature of the problem.
OT is usually expressed as an optimal cost functional but it also enjoys a dual variational formula-
tion [1  Chapter 5]. It has been proven useful in several settings. As a ﬁrst example it corresponds to
the Wasserstein distance in the space of probability distributions. Using this distance it is possible to
compute means and barycentres [6  7] or to perform a PCA in the space of probability measures [8].
This distance has also been used in subspace identiﬁcation problems for analysing the differences
between distributions [9]  in graph based semi-supervised learning to propagate histogram labels
across nodes [4] or as a way to deﬁne a loss function for multi-label learning [5]. As a second example
OT enjoys a variety of bounds for the convergence rate of empirical to population measures which can
be used to derive new probabilistic bounds for the performance of unsupervised learning algorithms
such as k-means [2]. As a last example OT is a mean of interpolation between distributions [10] that
has been used in Bayesian inference [11]  color transfer [12] or domain adaptation [13].
On the computational side  despite some results with ﬁnite difference schemes [14]  one of the major
gain is the recent development of regularized versions that leads to efﬁcient algorithms [3  7  15]. Most

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

OT formulations are based on the computation of a (probabilistic) coupling matrix that can be seen
as a bi-partite graph between the bins of the distributions. This coupling  also called transportation
matrix  corresponds to an empirical transport map which suffers from some drawbacks: it can only be
applied to the examples used to learn it. In other words when a new dataset (or sample) is available 
one has to recompute an OT problem to deal with the new instances which can be prohibitive for some
applications in particular when the task is similar or related. From a machine learning standpoint  this
also means that we do not know how to ﬁnd a good approximation of a transport map computed from
a small sample that can be generalized to unseen data. This is particularly critical when one considers
medium or large scale applications such as image editing problems. In this paper  we propose to
bridge this gap by learning an explicit transformation that can be interpreted as a good approximation
of the transport map. As far as we know  this is the ﬁrst approach that addresses directly this problem
of out-of-sample mapping.
Our formulation is based on classic regularized regression and admits two appealing interpretations.
On the one hand  it can be seen as learning a transformation regularized by a transport map. On the
other hand  we can see it as the computation of the transport map regularized w.r.t. the deﬁnition
of a transformation (e.g.
linear  non-linear  . . . ). This results in an optimization problem that
jointly learns both the transport map and the transformation. This formulation can be efﬁciently
solved thanks to alternating block-coordinate descent and actually beneﬁts the two models: (i) we
obtain smoother transport maps that must be compliant with a transformation that can be used on
out-of-sample examples and (ii) the transformation is able to take into account some geometrical
information captured by OT. See Figure 1 for an illustration. We provide some empirical evidence for
the usefulness of our approach in domain adaptation and image editing. Beyond that  we think that
this paper can open the door to new research on the generalization ability of OT.
The rest of the paper is organized as follows. Section 2 introduces some notations and preliminaries
in optimal transport. We present our approach in Section 3. Our experimental evaluation is given in
Section 4 and we conclude in Section 5.

2 Background
Monge problem Let ΩS ∈ Rds and ΩT ∈ Rdt be two separable metric spaces such that any
probability measure on ΩS  respectively ΩT   is a Radon measure. By considering a cost function
c : ΩS × ΩT → [0 ∞[  Monge’s formulation of the OT problem is to ﬁnd a transport map
T : ΩS → ΩT (also known as a push-forward operator) between two probability measures µS on
ΩS and µT on ΩT realizing the inﬁmum of the following function:

(cid:27)

(cid:26)(cid:90)

ΩS

inf

c(x  T (x))dµS (x)  T #µS = µT

.

(1)

When reaching this inﬁmum  the corresponding map T is an optimal transport map. It associates one
point from ΩS to a single point in ΩT . Therefore  the existence of this map is not always guaranteed 
as when for example µS is a Dirac and µT is not. As such  the existence of solutions for this problem
can in general not be established when µS and µT are supported on a different number of Diracs. Yet 
in a machine learning context  data samples usually form discrete distributions  but can be seen as
observations of a regular  continuous (with respect to the Lebesgue measure) underlying distribution 
thus fulﬁlling existence conditions (see [1  Chapter 9]). As such  assuming the existence of T calls
for a relaxation of the previous problem.

Kantorovich relaxation The Kantorovitch formulation of OT [16] is a convex relaxation of the
Monge problem. Let us deﬁne Π as the set of all probabilistic couplings in P(ΩS × ΩT )  the space
of all joint distributions with marginals µS and µT . The Kantorovitch problem seeks for a general
coupling γ ∈ Π between ΩS and ΩT :

(cid:90)

γ0 = arg min

γ∈Π

ΩS×ΩT

c(xs  xt)dγ(xs  xt).

(2)

The optimal coupling always exists [1  Theorem 4.1]. This leads to a simple formulation of the
OT problem in the discrete case  i.e. whenever µS and µT are only accessible through discrete
i}nt
samples Xs = {xs
i=1. The corresponding empirical distributions can be
where δx is the Dirac function at location

written as ˆµS = (cid:80)ns

i}ns
i=1  and Xt = {xt
i=1 ps

and ˆµT = (cid:80)nt

i=1 pt

iδxt

i δxs

i

i

2

where (cid:104)· ·(cid:105)F is the Frobenius dot product1 and C ≥ 0 is the cost matrix related to the function c.
Barycentric mapping Once the probabilistic coupling γ0 has been computed  one needs to map
the examples from ΩS to ΩT . This mapping can be conveniently expressed with respect to the set of
examples Xt as the following barycentric mapping [11  13  12]:

γ0(i  j)c(x  xt

j) 

(4)

(cid:99)xs

nt(cid:88)

j=1

i is a given source sample and(cid:99)xs

x∈ΩT

i = arg min

Figure 1: Illustration of the mappings estimated on the clown dataset with a linear (top) and nonlinear
(bottom) mapping (best viewed in color).

x ∈ Ω. ps

simplex  i.e.(cid:80)ns
empirical distributions deﬁned as ˆΠ =(cid:8)γ ∈ (R+)ns×nt| γ1nt = ˆµS   γT 1ns = ˆµT(cid:9) where 1n is a

i are probability masses associated to the i-th sample and belong to the probability
i = 1. Let ˆΠ be the set of probabilistic couplings between the two
i=1 ps

i =(cid:80)nt

i and pt

i=1 pt

n-dimensional vector of ones. Problem 2 becomes:
γ0 = arg min

γ∈ ˆΠ

(cid:104)γ  C(cid:105)F  

(3)

i is its corresponding image. When the cost function is the
where xs
squared (cid:96)2 distance  i.e. c(x  x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2
2  this barycentre corresponds to a weighted average
and the sample is mapped into the convex hull of the target examples. For all source samples  this
barycentric mapping can therefore be expressed as:

(cid:99)Xs = Bγ0(Xs) = diag(γ01nt)−1γ0Xt.

In the rest of the paper we will focus on a uniform sampling  i.e.

from µS and µT   whence (cid:99)Xs = nsγ0Xt. The main drawback of the mapping (5) is that it does not

(5)
the examples are drawn i.i.d.

allow the projection of out-of-sample examples which do not have been seen during the learning
process of γ0. It means that to transport a new example xs ∼ ΩS one has to compute the coupling
matrix γ0 again using this new example. Also  while some authors consider speciﬁc regularization of
γ [3  13] to control the nature of the coupling  inducing speciﬁc properties of the transformation T
(i.e. regularity  divergence free  etc.) is hard to achieve.
In the next section we present a relaxation of the OT problem  which consists in jointly learning γ and
T . We derive the corresponding optimization problem  and show its usefulness in speciﬁc scenarios.

3 Contributions

3.1

Joint learning of T and γ

In this paper we propose to solve the problem of optimal transport by jointly learning the matrix γ
and the transformation function T . First of all  we denote H the space of transformations from ΩT

1(cid:104)A  B(cid:105)F = Tr(AT B)

3

to ΩT and using a slight abuse of notations Xs and Xt are matrices where each line is an example
respectively drawn from ΩS and ΩT . We propose the following optimisation problem:

(cid:104)γ  C(cid:105)F +

λT
dsdt

λγ

1

(cid:107)T (Xs) − nsγXt(cid:107)2F +

nsdt

R(T )

max(C)

f (γ  T ) =

arg min
T∈H γ∈ ˆΠ

(6)
where T (Xs) is a short-hand for the application of T on each example in Xs  R(·) is a regularization
term on T and λγ  λT are hyper-parameters controlling the trade-off between the three terms in the
optimization problem. The ﬁrst term in (6) depends on both T and γ and controls the closeness
between the transformation induced by T and the barycentric interpolation obtained from γ. The
second term only depends on γ and corresponds to the standard optimal transport loss. The third term
regularizes T to ensure a better generalization.
A standard approach to solve problem (6) is to use block-coordinate descent (BCD) [17]  where the
idea is to alternatively optimize for T and γ. In the next theorem we show that under some mild
assumptions on the regularization term R(·) and the function space H this problem is jointly convex.
Note that in this case we are guaranteed to converge to the optimal solution only if we are strictly
convex w.r.t. T and γ. While this is not the case for γ  the algorithm works well in practice and
a small regularization term can be added if theoretical convergence is required. The proof of the
following theorem can be found in the supplementary.
Theorem 1. Let H be a convex space and R(·) be a convex function. Problem (6) is jointly convex
in T and γ.

As discussed above we propose to solve optimization problem (6) using a block coordinate descent
approach. As such we need to ﬁnd an efﬁcient way to solve: (i) for γ when T is ﬁxed and (ii) for
T when γ is ﬁxed. To solve the problem w.r.t. γ with a ﬁxed T   a common approach is to use the
Frank-Wolfe algorithm [12  18]. It is a procedure for solving any convex constrained optimization
problem with a convex and continuously differentiable objective function over a compact convex
subset of any vector space. This algorithm can ﬁnd an  approximation of the optimal solution in
O(1/) iterations [19]. A detailed algorithm is given in the supplementary material. In the next
section we discuss the solution of the minimization w.r.t. T with ﬁxed γ for different functional
spaces.
3.2 Choosing H
In the previous subsection we presented our method when considering a general set of transformations
H. In this section we propose several possibilities for the choice of a convex set H. On the one hand 
we propose to deﬁne H as a set of linear transformations from ΩS to ΩT . On the other hand  using
the kernel trick  we propose to consider non-linear transformations. A summary of the approach can
be found in Algorithm 1.
Linear transformations A ﬁrst way to deﬁne H is to consider linear transformations induced by a
ds × dt real matrix L:

H =

T : ∃ L ∈ Rds×dt

 ∀xs ∈ ΩS   T (xs) = xsT L

(7)
Furthermore  we deﬁne R(T ) = (cid:107)L − I(cid:107)2F where I is the identity matrix. We choose to bias L
toward I in order to ensure that the examples are not moved too far away from their initial position.
In this case we can rewrite optimization problem (6) as:

.

(cid:110)

(cid:111)

arg min

L∈Rds×dt  γ∈ ˆΠ

1

nsdt

(cid:107)XsL − nsγXt(cid:107)2F +

λγ

max(C)

(cid:104)γ  C(cid:105)F +

λT
dsdt

(8)

(cid:107)L − I(cid:107)2F .
(cid:19)

(cid:18) 1

According to Algorithm 1 a part of our procedure requires to solve optimization problem (8) when γ
is ﬁxed. One solution is to use the following closed form for L:

XT

L =

nsdt

s Xs +

(9)
where (·)−1 is the matrix inverse (Moore-Penrose pseudo-inverse when the matrix is singular). In the
previous deﬁnition of H  we considered non biased linear transformations. However it is sometimes
desirable to add a bias to the transformation. The equations being very similar in spirit to the non
biased case we refer the interested reader to the supplementary material.

s nsγXt +

nsdt

XT

I

I

λT
dsdt

(cid:19)−1(cid:18) 1

λT
dsdt

4

Algorithm 1: Joint Learning of L and γ.

input
output : L  γ.

: Xs  Xt source and target examples and λγ  λT hyper parameters.

1 begin
2
3
4
5
6
7

Initialize k = 0  γ0 ∈ ˆΠ and L0 = I
repeat

Learn γk+1 solving problem (6) with ﬁxed Lk using a Frank-Wolfe approach.
Learn Lk+1 using Equation (9)  (12) or their biased counterparts with ﬁxed γk+1.
Set k = k + 1.
until convergence

Non-linear transformations
In some cases a linear transformation is not sufﬁcient to approximate
the transport map. Hence  we propose to consider non-linear transformations. Let φ be a non-linear
H 
we can deﬁne H for a given set of examples Xs as:

function associated to a kernel function k : ΩS × ΩS → R such that k(xs  xs(cid:48)) =(cid:10)φ(xs)  φ(xs(cid:48))(cid:11)
)(cid:1) where
where kXs(xsT ) is a short-hand for the vector(cid:0)k(xs  xs
)(cid:1).
where kXs (·) is a short-hand for the vector(cid:0)k(·  xs

(cid:110)
(cid:111)
T : ∃ L ∈ Rns×dt∀xs ∈ ΩS   T (xs) = kXs(xsT )L
···

∈ Xs. In this case optimization problem (6) becomes:

(cid:104)γ  C(cid:105)F +
k(·  xs

λT
nsdt

)(cid:1) =(cid:0)φ(xs

(cid:107)kXs(Xs)L − nsγXt(cid:107)2F +

(cid:107)kXs (·)L(cid:107)2F .

1 ···   xs
xs

max(C)
···

1)

1) k(xs  xs
2)

arg min

L∈Rns×dt  γ∈ ˆΠ

··· φ(xs

ns

k(xs  xs
ns

1

nsdt

H =

(10)

(11)

1)

ns

ns

As in the linear case there is a closed form solution for L when γ is ﬁxed:

λγ

(cid:19)−1

L =

kXs (Xs) +

λT
d2 I

nsdt

1

nsdt

nsγXt.

(12)

(cid:18) 1

As in the linear case it might be interesting to use a bias (Presented in the supplementary material).

3.3 Discussion on the quality of the transport map approximation

In this section we propose to discuss some theoretical considerations about our framework and more
precisely on the quality of the learned transformation T . To assess this quality we consider the
Frobenius norm between T and the true transport map  denoted T ∗  that we would obtain if we could
solve Monge’s problem. Let Bˆγ be the empirical barycentric mapping of Xs using the probabilistic
coupling ˆγ learned between Xs and Xt. Similarly let Bγ0 be the theoretical barycentric mapping
associated with the probabilistic coupling γ0 learned on µS   µT the whole distributions and which
corresponds to the solution of Kantorovich’s problem. Using a slight abuse of notations we denote by
Bˆγ(xs) and Bγ0(xs) the projection of xs ∈ Xs by these barycentric mappings. Using the triangle
inequality  some standard properties on the square function  the deﬁnition of H and [20  Theorem 2] 
we have with high probability that (See the supplementary material for a justiﬁcation):

E

xs∼ΩS

(cid:107)T (xs) − T

∗

(xs)(cid:107)2F ≤ 4

(cid:88)
(cid:88)

xs∈Xs

+ 4

(cid:19)

(cid:18) 1√

(cid:107)T (xs) − Bˆγ(xs)(cid:107)2F + O
ns
(cid:107)Bˆγ(xs) − Bγ0 (xs)(cid:107)2F + 2 E
xs∼ΩS

(cid:107)Bγ0 (xs) − T

∗

(xs)(cid:107)2F .

(13)

xs∈Xs

ties. The ﬁrst quantity (cid:80)
xs∈Xs
From Inequality 13 we assess the quality of the learned transformation T w.r.t. three key quanti-
(cid:107)T (xs) − Bˆγ(xs)(cid:107)2F   is a measure of the difference between the
learned transformation and the empirical barycentric mapping. We minimize it in Problem (6). The
(cid:80)
second and third quantities are theoretical and hard to bound because  as far as we know  there
is a lack of theoretical results related to these terms in the literature. Nevertheless  we expect
(cid:107)Bˆγ(xs) − Bγ0 (xs)(cid:107)2F to decrease uniformly with respect to the number of examples as it
corresponds to a measure of how well the empirical barycentric mapping estimates the theoretical
one. Similarly  we expect Exs∼ΩS (cid:107)Bγ0 (xs) − T ∗(xs)(cid:107)2F to be small as it characterizes that the
theoretical barycentric mapping is a good approximation of the true transport map. This depends of
course on the expressiveness of the set H considered. We think that this discussion opens up new
theoretical perspectives for OT in Machine Learning but these are beyond the scope of this paper.

xs∈Xs

5

Table 1: Accuracy on the Moons dataset. Color-code: the darker the result  the better.

Angle 1NN GFK SA OT L1L2 OTE

OTLin

OTLinB

OTKer

OTKerB

T

γ

T

γ

T

γ

T

γ

10
20
30
40
50
60
70
80
90

100.0 99.9 100.0 97.9 99.6 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
93.1 95.0 98.7 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
93.1
99.9 100.0 100.0 100.0 100.0
84.0 90.6 98.4 100.0 99.8
84.0
74.4 83.7 95.8 100.0 98.3
99.7
98.5
77.1
99.1
97.5
97.8
73.1 77.8 87.7
61.7
96.8
97.0
96.4
72.3 71.0 88.3
41.2
23.1
72.3 64.5 89.0
88.0
94.3
83.1
74.2
80.7
76.9
72.3 57.3 73.6
20.7
68.1
19.4
34.2 51.0 58.1
67.9
55.4

95.8
92.5
90.8
90.2
79.4
61.0
36.2
43.1

99.9
98.7
97.6
97.2
94.7
81.0
68.0

99.8
98.1
97.5
95.8
88.2
76.6
67.1

99.6
99.1
96.6
82.5
73.9
57.6

87.3
86.3
77.5
58.8
51.3

99.7
99.1
96.6
80.8
74.0
56.3

99.7
99.2
96.8
81.5
74.1
55.8

4 Experiments

4.1 Domain Adaptation

Datasets We consider two domain adaptation (DA) datasets  namely Moons [21] and Ofﬁce-
Caltech [22]. The Moons dataset is a binary classiﬁcation task where the source domain corresponds
to two intertwined moons  each one representing a class. The target domain is built by rotating the
source domain with an angle ranging from 10 to 90 degrees. It leads to 9 different adaptation tasks
of increasing difﬁculty. The examples are two dimensional and we consider 300 source and target
examples for training and 1000 target examples for testing. The Ofﬁce-Caltech dataset is a 10 class
image classiﬁcation task with 4 domains corresponding to images coming from different sources:
amazom (A)  dslr (D)  webcam (W) and Caltech10 (C). There are 12 adaptation tasks where each
domain is in turn considered as the source or the target (denoted source → target). To represent
the images we use the deep learning features of size 4096 named decaf6 [23]. During the training
process we consider all the examples from the source domain and half of the examples from the target
domain  the other half being used as the test set.

Methods We consider 6 baselines. The ﬁrst one is a simple 1-Nearest-Neighbour (1NN) using
the original source examples only. The second and third ones are two widely used DA approaches 
namely Geodesic Flow Kernel (GFK) [22] and Subspace Alignment (SA) [24]. The fourth to sixth
baselines are OT based approaches: the classic OT method (OT)  OT with entropy based regularization
(OTE) [3] and OT with (cid:96)1(cid:96)2 regularization (L1L2) [13]. We present the results of our approach with
the linear (OTLin) and kernel (OTKer) versions of T and their biased counterpart (*B). For OT based
methods the idea is to (i) compute the transport map between the source and the target  (ii) project
the source examples and (iii) classify the target examples using a 1NN on the projected source.

t

t

and a labelled target testing set Xtest

Experimental Setup We consider the following experimental setup for all the methods and datasets.
All the results presented in this section are averaged over 10 trials. For each trial we consider three
sets of examples  a labelled source training set denoted Xs  ys  an unlabelled target training set
denoted Xtrain
. The model is learned on Xs  ys and Xtrain
and evaluated on Xtest
t with a 1NN learned on Xs  ys. All the hyper-parameters are tuned according
to a grid search on the source and target training instances using a circular validation procedure
derived from [21  25] and described in the supplementary material. For GFK and SA we choose
the dimension of the subspace d ∈ {3  6  . . .   30}  for L1L2 and OTE we set the parameter for
entropy regularization in {10−6  10−5  . . .   105}  for L1L2 we choose the class related parameter
η ∈ {10−5  10−4  . . .   102}  for all our methods we choose λT   λγ ∈ {10−3  10−2  . . .   100}.
The results on the Moons and Ofﬁce-Caltech datasets are respectively given in Table 1 and 2. A ﬁrst
important remark is that the coupling γ and the transformation T almost always obtain the same
results. It shows that our method is able to learn a good approximation T of the transport map induced
by γ. In terms of accuracy our approach tends to give the best results. It shows that we are effectively
able to move closer the distributions in a relevant way. For the Moons dataset  the last 6 approaches
(including ours) based on OT obtain similar results until 40 degrees while the other methods fail to
obtain good results at 20 degrees. Beyond 50 degrees  our approaches give signiﬁcantly better results
than the others. Furthermore they are more stable when the difﬁculty of the problem increases which

t

6

Table 2: Accuracy on the Ofﬁce-Caltech dataset. Color-code: the darker the result  the better.

Task

1NN GFK SA OT L1L2 OTE OTLin
γ

OTLinB

OTKer

OTKerB

T

γ

T

γ

T

T

γ
D → W 89.5 93.3 95.6 77.0 95.7 95.7 97.3 97.3 97.3 97.3 98.4 98.5 98.5 98.5
D → A 62.5 77.2 88.5 70.8 74.9 74.8 85.7 85.7 85.8 85.8 89.9 89.9 89.5 89.5
D → C 51.8 69.7 79.0 68.1 67.8 68.0 77.2 77.2 77.4 77.4 69.1 69.2 69.3 69.3
W → D 99.2 99.8 99.6 74.1 94.4 94.4 99.4 99.4 99.8 99.8 97.2 97.2 96.9 96.9
W → A 62.5 72.4 79.2 67.6 71.3 71.3 81.5 81.5 81.4 81.4 78.5 78.3 78.5 78.8
W → C 59.5 63.7 55.0 63.1 67.8 67.8 75.9 75.9 75.4 75.4 72.7 72.7 65.1 63.3
A → D 65.2 75.9 83.8 64.6 70.1 70.5 80.6 80.6 80.4 80.5 65.6 65.5 71.9 71.5
A → W 56.8 68.0 74.6 66.8 67.2 67.3 74.6 74.6 74.4 74.4 66.4 64.8 70.0 68.9
A → C 70.1 75.7 79.2 70.4 74.1 74.3 81.8 81.8 81.6 81.6 84.4 84.4 84.5 84.5
C → D 75.9 79.5 85.0 66.0 69.8 70.2 87.1 87.1 87.2 87.2 70.1 70.0 78.6 78.6
C → W 65.2 70.7 74.4 59.2 63.8 63.8 78.3 78.3 78.5 78.5 80.0 80.4 73.5 73.4
C → A 85.8 87.1 89.3 75.2 76.6 76.7 89.9 89.9 89.7 89.7 82.4 82.2 83.6 83.5
70.3 77.8 81.9 68.6 74.5 74.6 84.1 84.1 84.1 84.1 79.6 79.4 80.0 79.7
Mean

can be interpreted as a beneﬁt from our regularization. In the supplementary material we propose
an illustration of the transformation learned by our approach. For Ofﬁce-Caltech  our methods are
signiﬁcantly better than other approaches which illustrates the potential of our method for difﬁcult
tasks. To conclude  forcing OT to simultaneously learn coupling and transformation seems beneﬁcial.

4.2 Seamless copy in images with gradient adaptation

We propose here a direct application of our mapping estimation in the context of image editing.
While several papers using OT are focusing on color adaptation [12  26]  we explore here a new
variant in the domain of image editing: the seamless editing or cloning in images. In this context  one
may desire to import a region from a given source image to a target image. As a direct copy of the
region leads to inaccurate results in the ﬁnal image nearby the boundaries of the copied selection  a
very popular method  proposed by P´erez and co-workers [27]  allows to seamlessly blend the target
image and the selection. This technique  coined as Poisson Image Editing  operates in the gradient
domain of the image. Hence  the gradients of the selection operate as a guidance ﬁeld for an image
reconstruction based on membrane interpolation with appropriate boundary conditions extracted from
the target image (See the supplementary material for more details).
Though appealing  this technique is prone to errors due local contrast change or false colors resulting
from the integration. While some solutions combining both gradient and color domains exist [28] 
this editing technique usually requires the source and target images to have similar colors and contrast.
Here  we propose to enhance the genericity of this technique by forcing the gradient distribution from
the source image to follow the gradient distribution in the target image. As a result  the seamless
cloning not only blends smoothly the copied region in the target domain  but also constraints the color
dynamics to that of the target image. Hence  a part of the style of the target image is preserved. We
start by learning a transfer function Ts→t : R6 → R6 with our method  where 6 denotes the vertical
and horizontal components of gradient per color  and we then directly solve the same system as [27].
When dealing with images  the number of source and target gradients are largely exceeding tens of
thousands and it is mandatory to consider methods that scale appropriately. As such  our technique can
readily learn the transfer function Ts→t over a limited set of gradients and generalizes appropriately
to unseen gradients. Three illustrations of this method are proposed in a context of face swapping
in Figure 2. As one can observe  the original method of Poisson image editing [27] (3rd column)
tends to preserve the color dynamics of the original image and fails in copying the style of the target
image. Our method was tested with a linear and kernel version of Ts→t  that was learned with only
500 gradients sampled randomly from both sources (λT = 10−2  λT = 103 for respectively the
linear and kernel versions  and λγ = 10−7 for both cases). As a general qualitative comment  one
can observe that the kernel version of Ts→t is better at preserving the dynamics of the gradient  while
the linear version tends to ﬂatten the colors. In this low-dimensional space  this illustrates the need of
a non-linear transformation. Regarding the computational time  the gradient adaptation is of the same

7

Figure 2: Illustrations of seamless copies with gradient adaptation. Each row is composed of the
source image  the corresponding selection zone Ω described as a binary mask  and the target image.
We compare here the two linear (4th column) and kernel (5th column) versions of the map Ts→t with
the original method of [27] (2nd column) (best viewed in color).

order of magnitude as the Poisson equation solving  and each example is computed in less than 30s
on a standard personal laptop. In the supplementary material we give other examples of the method.

5 Conclusion

In this paper we proposed a jointly convex approach to learn both the coupling γ and a transformation
T approximating the transport map given by γ. It allowed us to apply a learned transport to a set
of out-of-samples examples not seen during the learning process. Furthermore  jointly learning
the coupling and the transformation allowed us to regularize the transport by enforcing a certain
smoothness on the transport map. We also proposed several possibilities to choose H the set of
possible transformations. We presented some theoretical considerations on the generalization ability
of the learned transformation T . Hence we discussed that under the assumption that the barycentric
mapping generalizes well and is a good estimate of the true transformation  then T learned with our
method should be a good approximation of the true transformation. We have shown that our approach
is efﬁcient in practice on two different tasks: domain adaptation and image editing.
The framework presented in this paper opens the door to several perspectives. First  from a theoretical
standpoint the bound proposed raises some questions on the generalization ability of the barycentric
mapping and on the estimation of the quality of the true barycentric mapping with respect to the target
transformation. On a more practical side  note that in recent years regularized OT has encountered a
growing interest and several methods have been proposed to control the behaviour of the transport.
As long as these regularization terms are convex  one could imagine using them in our framework.
Another perspective could be to use our framework in a mini-batch setting where instead of learning
from the whole dataset we can estimate a single function T from several couplings γ optimized on
different splits of the examples. As a last example we believe that our framework could allow the use
of the notion of OT in deep architectures as  contrary to the coupling γ  the function T can be used
on out-of-samples examples.

Acknowledgments

This work was supported in part by the french ANR project LIVES ANR-15-CE23-0026-03.

8

References
[1] C. Villani. Optimal transport: old and new. Grund. der mathematischen Wissenschaften. Springer  2009.

[2] G. Canas and L. Rosasco. Learning probability measures with respect to optimal transport metrics. In

NIPS. 2012.

[3] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS  2013.

[4] J. Solomon  R. Rustamov  G. Leonidas  and A. Butscher. Wasserstein propagation for semi-supervised

learning. In ICML  2014.

[5] C. Frogner  C. Zhang  H. Mobahi  M. Araya  and T. Poggio. Learning with a Wasserstein loss. In NIPS.

2015.

[6] M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In ICML  2014.

[7] J.-D. Benamou  G. Carlier  M. Cuturi  L. Nenna  and G. Peyr´e. Iterative Bregman projections for regularized

transportation problems. SISC  2015.

[8] V. Seguy and M. Cuturi. Principal geodesic analysis for probability measures under the optimal transport

metric. In NIPS. 2015.

[9] J. Mueller and T. Jaakkola. Principal differences analysis: Interpretable characterization of differences

between distributions. In NIPS. 2015.

[10] R. McCann. A convexity principle for interacting gases. Advances in Mathematics  128(1)  1997.

[11] S. Reich. A nonparametric ensemble transform method for bayesian inference. SISC  2013.

[12] S. Ferradans  N. Papadakis  G. Peyr´e  and J.-F. Aujol. Regularized discrete optimal transport. SIIMS  2014.

[13] N. Courty  R. Flamary  and D. Tuia. Domain adaptation with regularized optimal transport. In ECML

PKDD  2014.

[14] J.-D. Benamou  B. D Froese  and A. M Oberman. Numerical solution of the optimal transportation problem

using the Monge–Amp`ere equation. Journal of Computational Physics  260  2014.

[15] M. Cuturi and G. Peyr´e. A smoothed dual approach for variational Wasserstein problems. SIIMS  2016.

[16] L. Kantorovich. On the translocation of masses. C.R. (Doklady) Acad. Sci. URSS (N.S.)  37  1942.

[17] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal

of Optimization Theory and Applications  109(3)  2001.

[18] M. Frank and P. Wolfe. An algorithm for quadratic programming. NRL  3(1-2)  1956.

[19] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML  2013.

[20] M. Perrot and A. Habrard. Regressive virtual metric learning. In NIPS  2015.

[21] L. Bruzzone and M. Marconcini. Domain adaptation problems: A DASVM classiﬁcation technique and a

circular validation strategy. IEEE PAMI  32(5)  2010.

[22] B. Gong  Y. Shi  F. Sha  and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In

CVPR  2012.

[23] J. Donahue  Y. Jia  O. Vinyals  J. Hoffman  N. Zhang  E. Tzeng  and T. Darrell. Decaf: A deep convolutional

activation feature for generic visual recognition. In ICML  2014.

[24] B. Fernando  A. Habrard  M. Sebban  and T. Tuytelaars. Unsupervised visual domain adaptation using

subspace alignment. In ICCV  2013.

[25] E. Zhong  W. Fan  Q. Yang  O. Verscheure  and J. Ren. Cross validation framework to choose amongst

models and datasets for transfer learning. In ECML PKDD  2010.

[26] J. Solomon  F. De Goes  G. Peyr´e  M. Cuturi  A. Butscher  A. Nguyen  T. Du  and L. Guibas. Convolutional

Wasserstein distances. ACM Trans. on Graphics  34(4)  2015.

[27] P. P´erez  M. Gangnet  and A. Blake. Poisson image editing. ACM Trans. on Graphics  22(3)  2003.

[28] F. Deng  S. J. Kim  Y.-W. Tai  and M. Brown. ACCV  chapter Color-Aware Regularization for Gradient

Domain Image Manipulation. Springer Berlin Heidelberg  Berlin  Heidelberg  2012.

9

,Michaël Perrot
Nicolas Courty
Rémi Flamary
Amaury Habrard