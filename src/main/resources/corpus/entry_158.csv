2017,A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent,We study the generalization error of randomized learning algorithms -- focusing on stochastic gradient descent (SGD) -- using a novel combination of PAC-Bayes and algorithmic stability. Importantly  our generalization bounds hold for all posterior distributions on an algorithm's random hyperparameters  including distributions that depend on the training data. This inspires an adaptive sampling algorithm for SGD that optimizes the posterior at runtime. We analyze this algorithm in the context of our generalization bounds and evaluate it on a benchmark dataset. Our experiments demonstrate that adaptive sampling can reduce empirical risk faster than uniform sampling while also improving out-of-sample accuracy.,A PAC-Bayesian Analysis of Randomized Learning

with Application to Stochastic Gradient Descent

Ben London

blondon@amazon.com

Amazon AI

Abstract

We study the generalization error of randomized learning algorithms—focusing
on stochastic gradient descent (SGD)—using a novel combination of PAC-Bayes
and algorithmic stability. Importantly  our generalization bounds hold for all pos-
terior distributions on an algorithm’s random hyperparameters  including distribu-
tions that depend on the training data. This inspires an adaptive sampling algo-
rithm for SGD that optimizes the posterior at runtime. We analyze this algorithm
in the context of our generalization bounds and evaluate it on a benchmark dataset.
Our experiments demonstrate that adaptive sampling can reduce empirical risk
faster than uniform sampling while also improving out-of-sample accuracy.

1

Introduction

Randomized algorithms are the workhorses of modern machine learning. One such algorithm is
stochastic gradient descent (SGD)  a ﬁrst-order optimization method that approximates the gradient
of the learning objective by a random point estimate  thereby making it efﬁcient for large datasets.
Recent interest in studying the generalization properties of SGD has led to several breakthroughs.
Notably  Hardt et al. [10] showed that SGD is stable with respect to small perturbations of the
training data  which let them bound the risk of a learned model. Related studies followed thereafter
[13  16]. Simultaneously  Lin and Rosasco [15] derived risk bounds that show that early stopping
acts as a regularizer in multi-pass SGD (echoing studies of incremental gradient descent [19]).
In this paper  we study generalization in randomized learning  with SGD as a motivating example.
Using a novel analysis that combines PAC-Bayes with algorithmic stability (reminiscent of [17]) 
we prove new generalization bounds for randomized learning algorithms  which apply to SGD un-
der various assumptions on the loss function and optimization objective. Our bounds improve on
related work in two important ways. While some previous bounds for SGD [1  10  13  16] hold in
expectation over draws of the training data  our bounds hold with high probability. Further  existing
generalization bounds for randomized learning [6  7] only apply to algorithms with ﬁxed distribu-
tions (such as SGD with uniform sampling); thanks to our PAC-Bayesian treatment  our bounds hold
for all posterior distributions  meaning they support data-dependent randomization. The penalty for
overﬁtting the posterior to the data is captured by the posterior’s divergence from a ﬁxed prior.
Our generalization bounds suggest a sampling strategy for SGD that adapts to the training data and
model  focusing on useful examples while staying close to a uniform prior. We therefore propose
an adaptive sampling algorithm that dynamically updates its distribution using multiplicative weight
updates (similar to boosting [8  21]  focused online learning [22] and exponentiated gradient dual
coordinate ascent [4]). The algorithm requires minimal tuning and works with any stochastic gra-
dient update rule. We analyze the divergence of the adaptive posterior and conduct experiments
on a benchmark dataset  using several combinations of update rule and sampling utility function.
Our experiments demonstrate that adaptive sampling can reduce empirical risk faster than uniform
sampling while also improving out-of-sample accuracy.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

2 Preliminaries
Let X denote a compact domain; let Y denote a set of labels; and let Z (cid:44) X × Y denote their
Cartesian product. We assume there exists an unknown  ﬁxed distribution  D  supported on Z.
Given a dataset of examples  S (cid:44) (z1  . . .   zn) = ((x1  y1)  . . .   (xn  yn))  drawn independently
and identically from D  we wish to learn the parameters of a predictive model  X (cid:55)→ Y  from a class
of hypotheses  H  which we assume is a subset of Euclidean space. We have access to a deterministic
learning algorithm  A : Z n×Θ → H  which  given S  and some hyperparameters  θ ∈ Θ  produces
a hypothesis  h ∈ H.
We measure the quality of a hypothesis using a loss function  L : H×Z → [0  M ]  which we assume
is M-bounded1 and λ-Lipschitz (see Appendix A for the deﬁnition). Let L(A(S  θ)  z) denote the
loss of a hypothesis that was output by A(S  θ) when applied to example z. Ultimately  we want the
learning algorithm to have low expected loss on a random example; i.e.  low risk  denoted R(S  θ) (cid:44)
Ez∼D[L(A(S  θ)  z)].
(The learning algorithm should always be clear from context.) Since this
expectation cannot be computed  we approximate it by the average loss on the training data; i.e.  the
empirical risk  ˆR(S  θ) (cid:44) 1
i=1 L(A(S  θ)  zi)  which is what most learning algorithms attempt
to minimize. By bounding the difference of the two  G(S  θ) (cid:44) R(S  θ) − ˆR(S  θ)  which we refer
to as the generalization error  we obtain an upper bound on R(S  θ).
Throughout this document  we will view a randomized learning algorithm as a deterministic learning
algorithm whose hyperparameters are randomized. For instance  stochastic gradient descent (SGD)
performs a sequence of hypothesis updates  for t = 1  . . .   T   of the form
ht ← Ut(ht−1  zit) (cid:44) ht−1 − ηt∇F (ht−1  zit) 

(cid:80)n

n

using a sequence of random example indices  θ = (i1  . . .   iT )  sampled according to a distribution 
P  on Θ = {1  . . .   n}T . The objective function  F : H × Z → R+  may be different from L; it is
usually chosen as an optimizable upper bound on L  and need not be bounded. The parameter ηt is
a step size for the update at iteration t. SGD can be viewed as taking a dataset  S  drawing θ ∼ P 
then running a deterministic algorithm  A(S  θ)  which executes the sequence of hypothesis updates.
Since learning is randomized  we will deal with the expected loss over draws of random hyperparam-
eters. We therefore overload the above notation for a distribution  P  on the hyperparameter space 
Θ; let R(S  P) (cid:44) Eθ∼P[R(S  θ)]  ˆR(S  P) (cid:44) Eθ∼P[ ˆR(S  θ)]  and G(S  P) (cid:44) R(S  P) − ˆR(S  P).

2.1 Relationship to PAC-Bayes
Conditioned on the training data  a posterior distribution  Q  on the hyperparameter space  Θ  in-
duces a distribution on the hypothesis space  H. If we ignore the learning algorithm altogether and
think of Q as a distribution on H directly  then Eh∼Q[L(h  z)] is the Gibbs loss; that is  the expected
loss of a random hypothesis. The Gibbs loss has been studied extensively using PAC-Bayesian anal-
ysis (also known simply as PAC-Bayes) [3  9  14  18  20]. In the PAC-Bayesian learning framework 
we ﬁx a prior distribution  P  then receive some training data  S ∼ Dn  and learn a posterior dis-
tribution  Q. PAC-Bayesian bounds frame the generalization error  G(S  Q)  as a function of the
posterior’s divergence from the prior  which penalizes overﬁtting the posterior to the training data.
In Section 4  we derive new upper bounds on G(S  Q) using a novel PAC-Bayesian treatment. While
traditional PAC-Bayes analyzes distributions directly on H  we instead analyze distributions on
Θ. Thus  instead of applying the loss directly to a random hypothesis  we apply it to the output
of a learning algorithm  whose inputs are a dataset and a random hyperparameter instantiation.
This distinction is subtle  but important.
In our framework  a random hypothesis is explicitly a
function of the learning algorithm  whereas in traditional PAC-Bayes this dependence may only be
implicit—for instance  if the posterior is given by random permutations of a learned hypothesis. The
advantage of making the learning aspect explicit is that it isolates the source of randomness  which
may help in analyzing the distribution of learned hypotheses. Indeed  it may be difﬁcult to map the
output of a randomized learning algorithm to a distribution on the hypothesis space. That said  the
disadvantage of making learning explicit is that  due to the learning algorithm’s dependence on the
training data and hyperparameters  the generalization error could be sensitive to certain examples or
hyperparameters. This condition is quantiﬁed with algorithmic stability  which we discuss next.

1Accommodating unbounded loss functions is possible [11]  but requires additional assumptions.

2

3 Algorithmic Stability

Informally  algorithmic stability measures the change in loss when the inputs to a learning algorithm
are perturbed; a learning algorithm is stable if small perturbations lead to proportional changes in the
loss. In other words  a learning algorithm should not be overly sensitive to any single input. Stability
is crucial for learnability [23]  and has also been linked to differentially private learning [24]. In this
section  we discuss several notions of stability tailored for randomized learning algorithms. From

this point on  let DH(v  v(cid:48)) (cid:44)(cid:80)|v|

1{vi (cid:54)= v(cid:48)

i} denote the Hamming distance.

i=1

3.1 Deﬁnitions of Stability

The literature traditionally measures stability with respect to perturbations of the training data. We
refer to this general property as data stability. Data stability has been deﬁned in many ways. The
following deﬁnitions  originally proposed by Elisseeff et al. [6]  are designed to accommodate ran-
domized algorithms via an expectation over the hyperparameters  θ ∼ P.
Deﬁnition 1 (Uniform Stability). A randomized learning algorithm  A  is βZ-uniformly stable with
respect to a loss function  L  and a distribution  P on Θ  if

sup

S S(cid:48)∈Z n:DH(S S(cid:48))=1

sup
z∈Z

sup

i∈{1 ... n}

E
z∼D

E
θ∼P

E

S∼Dn

Deﬁnition 2 (Pointwise Hypothesis Stability). For a given dataset  S  let Si z denote the result of
replacing the ith example with example z. A randomized learning algorithm  A  is βZ-pointwise
hypothesis stable with respect to a loss function  L  and a distribution  P on Θ  if

(cid:12)(cid:12)(cid:12) ≤ βZ .
(cid:12)(cid:12)(cid:12) E
θ∼P [L(A(S  θ)  z) − L(A(S(cid:48)  θ)  z)]
(cid:2)(cid:12)(cid:12)L(A(S  θ)  zi) − L(A(Si z  θ)  zi)(cid:12)(cid:12)(cid:3) ≤ βZ .

Uniform stability measures the maximum change in loss from replacing any single training example 
whereas pointwise hypothesis stability measures the expected change in loss on a random example
when said example is removed from the training data. Under certain conditions  βZ-uniform stability
implies βZ-pointwise hypothesis stability  but not vice versa. Thus  while uniform stability enables
sharper bounds  pointwise hypothesis stability supports a wider range of learning algorithms.
In addition to data stability  we might also require stability with respect to changes in the hyperpa-
rameters. From this point forward  we will assume that the hyperparameter space  Θ  decomposes
t=1 Θt. For instance  Θ could be the set of all sequences of
example indices  {1  . . .   n}T   such as one would sample from in SGD.
Deﬁnition 3 (Hyperparameter Stability). A randomized learning algorithm  A  is βΘ-uniformly
stable with respect to a loss function  L  if

into the product of T subspaces  (cid:81)T

sup
S∈Z n

sup
z∈Z

sup

θ θ(cid:48)∈Θ:DH(θ θ(cid:48))=1

|L(A(S  θ)  z) − L(A(S  θ(cid:48))  z)| ≤ βΘ.

When A is both βZ-uniformly and βΘ-uniformly stable  we say that A is (βZ   βΘ)-uniformly stable.
Remark 1. For SGD  Deﬁnition 3 can be mapped to Bousquet and Elisseeff’s [2] original deﬁnition
of uniform stability using the resampled example sequence. Yet their generalization bounds would
still not apply because the resampled data is not i.i.d. and SGD is not a symmetric learning algorithm.

3.2 Stability of Stochastic Gradient Descent

For non-vacuous generalization bounds  we will need the data stability coefﬁcient  βZ  to be of order
˜O(n−1). Additionally  certain results will require the hyperparameter stability coefﬁcient  βΘ  to be
√
nT ). (If T = Θ(n)  as it often is  then βΘ = ˜O(T −1) sufﬁces.) In this section  we
of order ˜O(1/
review some conditions under which these requirements are satisﬁed by SGD. We rely on standard
characterizations of the objective function—namely  convexity  Lipschitzness and smoothness—the
deﬁnitions of which are deferred to Appendix A  along with all proofs from this section.
A recent study by Hardt et al. [10] proved that some special cases of SGD—when examples are sam-
pled uniformly  with replacement—satisfy βZ-uniform stability (Deﬁnition 1) with βZ = O(n−1).
We extend their work (speciﬁcally  [10  Theorem 3.7]) in the following result for SGD with a convex
objective function  when the step size is at most inversely proportional to the current iteration.

3

Proposition 1. Assume that the loss function  L  is λ-Lipschitz  and that the objective function  F  
is convex  λ-Lipschitz and σ-smooth. Suppose SGD is run for T iterations with a uniform sampling
distribution  P  and step sizes ηt ∈ [0  η/t]  for η ∈ [0  2/σ]. Then  SGD is both βZ-uniformly stable
and βZ-pointwise hypothesis stable with respect to L and P  with

βZ ≤ 2λ2η (ln T + 1)

n

.

(1)

When T = Θ(n)  Equation 1 is ˜O(n−1)  which is acceptable for proving generalization.
If we do not assume that the objective function is convex  we can borrow a result (with small modi-
ﬁcation2) from Hardt et al. [10  Theorem 3.8].
Proposition 2. Assume that the loss function  L  is M-bounded and λ-Lipschitz  and that the objec-
tive function  F   is λ-Lipschitz and σ-smooth. Suppose SGD is run for T iterations with a uniform
sampling distribution  P  and step sizes ηt ∈ [0  η/t]  for η ≥ 0. Then  SGD is both βZ-uniformly
stable and βZ-pointwise hypothesis stable with respect to L and P  with

(cid:19)(cid:0)2λ2η(cid:1) 1
ση+1(cid:1). As ση approaches 1  the rate becomes O(n−1/2)  which  as will become evident
11(cid:1) ≈ O(n−1)  which sufﬁces for generalization.

to O(cid:0)n
small—say  η = (10σ)−1—then we get O(cid:0)n− 10

Assuming T = Θ(n)  and ignoring constants that depend on M  λ  σ and η  Equation 2 reduces

in Section 4  yields generalization bounds that are suboptimal  or even vacuous. However  if ση is

(cid:18) M + (ση)−1

ση

ση+1 T

ση+1 .

− 1

βZ ≤

n − 1

(2)

We can obtain even tighter bounds for βZ-pointwise hypothesis stability (Deﬁnition 2) by adopting
a data-dependent view. The following result for SGD with a convex objective function is adapted
from work by Kuzborskij and Lampert [13  Theorem 3].
Proposition 3. Assume that the loss function  L  is λ-Lipschitz  and that the objective function  F   is
convex  λ-Lipschitz and σ-smooth. Suppose SGD starts from an initial hypothesis  h0  and is run for
T iterations with a uniform sampling distribution  P  and step sizes ηt ∈ [0  η/t]  for η ∈ [0  2/σ].
Then  SGD is βZ-pointwise hypothesis stable with respect to L and P  with

βZ ≤ 2λη (ln T + 1)(cid:112)2σ Ez∼D[L(h0  z)]

.

(3)

n

Importantly  Equation 3 depends on the risk of the initial hypothesis  h0. If h0 happens to be close
to a global optimum—that is  a good ﬁrst guess—then Equation 3 could be tighter than Equation 1.
Kuzborskij and Lampert also proved a data-dependent bound for non-convex objective functions
[13  Theorem 5]  which  under certain conditions  might be tighter than Equation 2. Though not
presented herein  Kuzborskij and Lampert’s bound is worth noting.
√
As we will later show  we can obtain stronger generalization guarantees by combining βZ-uniform
stability with βΘ-uniform stability (Deﬁnition 3)  provided βΘ = ˜O(1/
nT ). Prior stability analy-
ses of SGD [10  13] have not addressed this form of stability. Elisseeff et al. [6] proved (βZ   βΘ)-
uniform stability for certain bagging algorithms  but did not consider SGD. In light of Remark 1  it is
tempting to map βΘ-uniform stability to Bousquet and Elisseeff’s [2] uniform stability and thereby
leverage their study of various regularized objective functions. However  their analysis crucially re-
lies on exact minimization of the learning objective  whereas SGD with a ﬁnite number of steps only
ﬁnds an approximate minimizer. Thus  to our knowledge  no prior work applies to this problem. As
a ﬁrst step  we prove uniform stability  with respect to both data and hyperparameters  for SGD with
a strongly convex objective function and decaying step sizes.
Proposition 4. Assume that the loss function  L  is λ-Lipschitz  and that the objective function  F  
is γ-strongly convex  λ-Lipschitz and σ-smooth. Suppose SGD is run for T iterations with a uniform
sampling distribution  P  and step sizes ηt (cid:44) (γt + σ)−1. Then  SGD is (βZ   βΘ)-uniformly stable
with respect to L and P  with

√
When T = Θ(n)  the βΘ bound in Equation 4 is O(1/

nT )  which supports good generalization.

βZ ≤ 2λ2
γn

and βΘ ≤ 2λ2
γT

.

(4)

2Hardt et al.’s deﬁnition of stability and theorem statement differ slightly from ours. See Appendix A.1.

4

4 Generalization Bounds

In this section  we present new generalization bounds for randomized learning algorithms. While
prior work [6  7] has addressed this topic  ours is the ﬁrst PAC-Bayesian treatment (the beneﬁts of
which will be discussed momentarily). Recall that in the PAC-Bayesian framework  we ﬁx a prior
distribution  P  on the hypothesis space  H; then  given a sample of training data  S ∼ Dn  we learn
a posterior distribution  Q  also on H. In our extension for randomized learning algorithms  P and Q
are instead supported on the hyperparameter space  Θ. Moreover  while traditional PAC-Bayes stud-
ies Eh∼Q[L(h  z)]  we study the expected loss over draws of hyperparameters  Eθ∼Q[L(A(S  θ)  z)].
Our goal will be to upper-bound the generalization error of the posterior  G(S  Q)  which thereby
upper-bounds the risk  R(S  Q)  by a function of the empirical risk  ˆR(S  Q).
Importantly  our bounds are polynomial in δ−1  for a free parameter δ ∈ (0  1)  and hold with prob-
ability at least 1 − δ over draws of a ﬁnite training dataset. This stands in contrast to related bounds
[1  10  13  16] that hold in expectation. While expectation bounds are useful for gaining insight into
generalization behavior  high-probability bounds are sometimes preferred. Provided the loss is M-
bounded  it is always possible to convert a high-probability bound of the form PrS∼Dn{G(S  Q) ≤
B(δ)} ≥ 1 − δ to an expectation bound of the form ES∼Dn [G(S  Q)] ≤ B(δ) + δM.
Another useful property of PAC-Bayesian bounds is that they hold simultaneously for all posteriors 
including those that depend on the training data. In Section 3  we assumed that hyperparameters
were sampled according to a ﬁxed distribution; for instance  sampling training example indices for
SGD uniformly at random. However  in certain situations  it may be advantageous to sample accord-
ing to a data-dependent distribution. Following the SGD example  suppose most training examples
are easy to classify (e.g.  far from the decision boundary)  but some are difﬁcult (e.g.  near the deci-
sion boundary  or noisy). If we sample points uniformly at random  we might encounter mostly easy
examples  which could slow progress on difﬁcult examples. If we instead focus training on the difﬁ-
cult set  we might converge more quickly to an optimal hypothesis. Since our PAC-Bayesian bounds
hold for all hyperparameter posteriors  we can characterize the generalization error of algorithms
that optimize the posterior using the training data. Existing generalization bounds for randomized
learning [6  7]  or SGD in particular [1  10  13  15  16]  cannot address such algorithms. Of course 
there is a penalty for overﬁtting the posterior to the data  which is captured by the posterior’s diver-
gence from the prior.
Our ﬁrst PAC-Bayesian theorem requires the weakest stability condition  βZ-pointwise hypothesis
stability  but the bound is sublinear in δ−1. Our second bound is polylogarithmic in δ−1  but requires
the stronger stability conditions  (βZ   βΘ)-uniform stability. All proofs are deferred to Appendix B.
Theorem 1. Suppose a randomized learning algorithm  A  is βZ-pointwise hypothesis stable with
respect to an M-bounded loss function  L  and a ﬁxed prior  P on Θ. Then  for any n ≥ 1 and
δ ∈ (0  1)  with probability at least 1− δ over draws of a dataset  S ∼ Dn  every posterior  Q on Θ 
satisﬁes

(cid:19)

+ 12M βZ

 

(5)

(cid:115)(cid:18) χ2(Q(cid:107)P) + 1
(cid:17)2 − 1
(cid:105)

δ

(cid:19)(cid:18) 2M 2

n

G(S  Q) ≤

(cid:104)(cid:16) Q(θ)P(θ)

is the χ2 divergence from P to Q.

where χ2(Q(cid:107)P) (cid:44) Eθ∼P
Theorem 2. Suppose a randomized learning algorithm  A  is (βZ   βΘ)-uniformly stable with re-
t=1 Θt. Then 
for any n ≥ 1  T ≥ 1 and δ ∈ (0  1)  with probability at least 1−δ over draws of a dataset  S ∼ Dn 
every posterior  Q on Θ  satisﬁes

spect to an M-bounded loss function  L  and a ﬁxed product measure  P on Θ =(cid:81)T
(cid:19)

(cid:19)(cid:18) (M + 2nβZ )2

G(S  Q) ≤ βZ +

DKL(Q(cid:107)P) + ln

2
δ

+ 4T β2
Θ

 

(6)

n

where DKL(Q(cid:107)P) (cid:44) Eθ∼Q

ln

is the KL divergence from P to Q.

(cid:115)
(cid:18)
(cid:17)(cid:105)
(cid:16) Q(θ)P(θ)

2

(cid:104)

Since Theorems 1 and 2 hold simultaneously for all hyperparameter posteriors  they provide gen-
eralization guarantees for SGD with any sampling distribution. Note that the stability requirements
only need to be satisﬁed by a ﬁxed product measure  such as a uniform distribution. This simple

5

sampling distribution can have(cid:0)O(n−1)  O(T −1)(cid:1)-uniform stability under certain conditions  as

demonstrated in Section 3.2. In the following  we apply Theorem 2 to SGD with a strongly convex
objective function  leveraging Proposition 4 to upper-bound the stability coefﬁcients.
Corollary 1. Assume that the loss function  L  is M-bounded and λ-Lipschitz  and that the objective
function  F   is γ-strongly convex  λ-Lipschitz and σ-smooth. Let P denote a uniform prior on
{1  . . .   n}T . Then  for any n ≥ 1  T ≥ 1 and δ ∈ (0  1)  with probability at least 1 − δ over draws
of a dataset  S ∼ Dn  SGD with step sizes ηt (cid:44) (γt + σ)−1 and any posterior sampling distribution 
Q on {1  . . .   n}T   satisﬁes

G(S  Q) ≤ 2λ2
γn

+

2

DKL(Q(cid:107)P) + ln

2
δ

+

16λ4
γ2T

n

(cid:19)(cid:18) (M + 4λ2/γ)2

(cid:19)

.

(cid:115)

(cid:18)

When the divergence is polylogarithmic in n  and T = Θ(n)  the generalization bound is ˜O(n−1/2).
In the special case of uniform sampling  the KL divergence is zero  yielding a O(n−1/2) bound.
Importantly  Theorem 1 does not require hyperparameter stability  and is therefore of interest for
analyzing non-convex objective functions  since it is not known whether uniform hyperparameter
stability can be satisﬁed without (strong) convexity. One can use Equation 2 (or [13  Theorem 5]) to
upper-bound βZ in Equation 5 and thereby obtain a generalization bound for SGD with a non-convex
objective function  such as neural network training. We leave this substitution to the reader.
Equation 6 holds with high probability over draws of a dataset  but the generalization error is an
expected value over draws of hyperparameters. To obtain a bound that holds with high probability
over draws of both data and hyperparameters  we consider posteriors that are product measures.
Theorem 3. Suppose a randomized learning algorithm  A  is (βZ   βΘ)-uniformly stable with re-
t=1 Θt. Then 
for any n ≥ 1  T ≥ 1 and δ ∈ (0  1)  with probability at least 1−δ over draws of a dataset  S ∼ Dn 
and hyperparameters  θ ∼ Q  from any posterior product measure  Q on Θ 

spect to an M-bounded loss function  L  and a ﬁxed product measure  P on Θ =(cid:81)T

(cid:114)

(cid:115)

(cid:18)

2
δ

(cid:113)

(cid:19)(cid:18) (M + 2nβZ )2

n

(cid:19)

+ 4T β2
Θ

.

(7)

G(S  θ) ≤ βZ + βΘ

2 T ln

+

2

DKL(Q(cid:107)P) + ln

4
δ

nT )  then βΘ

√
δ vanishes at a rate of ˜O(n−1/2). We can apply Theorem 3 to
If βΘ = ˜O(1/
SGD in the same way we applied Theorem 2 in Corollary 1. Further  note that a uniform distribution
is a product distribution. Thus  if we eschew optimizing the posterior  then the KL divergence dis-
appears  leaving a O(n−1/2) derandomized generalization bound for SGD with uniform sampling.3

2 T ln 2

5 Adaptive Sampling for Stochastic Gradient Descent

The PAC-Bayesian theorems in Section 4 motivate data-dependent posterior distributions on the
hyperparameter space. Intuitively  certain posteriors may improve  or speed up  learning from a
given dataset. For instance  suppose certain training examples are considered valuable for reducing
empirical risk; then  a sampling posterior for SGD should weight those examples more heavily
than others  so that the learning algorithm can  probabilistically  focus its attention on the valuable
examples. However  a posterior should also try to stay close to the prior  to control the divergence
penalty in the generalization bounds.
Based on this idea  we propose a sampling procedure for SGD (or any variant thereof) that constructs
a posterior based on the training data  balancing the utility of the sampling distribution with its di-
vergence from a uniform prior. The algorithm operates alongside the learning algorithm  iteratively
generating the posterior as a sequence of conditional distributions on the training data. Each itera-
tion of training generates a new distribution conditioned on the previous iterations  so the posterior
dynamically adapts to training. We therefore call our algorithm adaptive sampling SGD.

3We can achieve the same result by pairing Proposition 4 with Elisseeff et al.’s generalization bound for
algorithms with (βZ   βΘ)-uniform stability [6  Theorem 15]. However  Elisseeff et al.’s bound only applies
to ﬁxed product measures on Θ  whereas Theorem 3 applies more generally to any posterior product measure 
and when P = Q  Equation 7 is within a constant factor of Elisseeff et al.’s bound.

6

utility function  f : Z × H → R; amplitude  α ≥ 0; decay  τ ∈ (0  1).

Algorithm 1 Adaptive Sampling SGD
Require: Examples  (z1  . . .   zn) ∈ Z n; initial hypothesis  h0 ∈ H; update rule  Ut : H×Z → H;
1: (q1  . . .   qn) ← 1
2: for t = 1  . . .   T do
3:
4:
5:
6: return hT

(cid:46) Draw index it proportional to sampling weights
(cid:46) Update hypothesis
(cid:46) Update sampling weight for it

it ∼ Qt ∝ (q1  . . .   qn)
ht ← Ut(ht−1  zit)
qit ← qτ

(cid:46) Initialize sampling weights uniformly

exp (α f (zit  ht))

it

Algorithm 1 maintains a set of nonnegative sampling weights  (q1  . . .   qn)  which deﬁne a distribu-
tion on the dataset. The posterior probability of the ith example in the tth iteration  given the previous
iterations  is proportional to the ith weight: Qt(i) (cid:44) Q(it = i| i1  . . .   it−1) ∝ qi. The sampling
weights are initialized to 1  thereby inducing a uniform distribution. At each iteration  we draw an
index  it ∼ Qt  and use example zit to update the hypothesis. We then update the weight for it
multiplicatively as qit ← qτ
exp (α f (zit  ht))  where: f (zit   ht) is a utility function of the chosen
example and current hypothesis; α ≥ 0 is an amplitude parameter  which controls the aggressiveness
of the update; and τ ∈ (0  1) is a decay parameter  which lets qi gradually forget past updates.
The multiplicative weight update (line 5) can be derived by choosing a sampling distribution for
the next iteration  t + 1  that maximizes the expected utility while staying close to a reference
distribution. Consider the following constrained optimization problem:

it

n(cid:88)

i=1

The term (cid:80)n

max

Qt+1∈∆n

Qt+1(i)f (zi  ht) − 1
α

DKL(Qt+1(cid:107)Qτ
t ).

(8)

i=1

Qt+1(i)f (zi  ht) is the expected utility under the new distribution  Qt+1. This
is offset by the KL divergence  which acts as a regularizer  penalizing Qt+1 for diverging from a
reference distribution  Qτ
i . The decay parameter  τ  controls the temperature of
the reference distribution  allowing it to interpolate between the current distribution (τ = 1) and a
uniform distribution (τ = 0). The amplitude parameter  α  scales the inﬂuence of the regularizer
relative to the expected utility. We can solve Equation 8 analytically using the method of Lagrange
multipliers  which yields

t   where Qτ

t (i) ∝ qτ

t+1(i) ∝ Qτ
Q(cid:63)

t (i) exp (α f (zit  ht) − 1) ∝ qτ

i exp (α f (zit  ht)) .

Updating qi for all i = 1  . . .   n is impractical for large n  so we approximate the above solution by
only updating the weight for the last sampled index  it  effectively performing coordinate ascent.
The idea of tuning the empirical data distribution through multiplicative weight updates is reminis-
cent of AdaBoost [8] and focused online learning [22]  but note that Algorithm 1 learns a single
hypothesis  not an ensemble. In this respect  it is similar to SelﬁeBoost [21]. One could also draw
parallels to exponentiated gradient dual coordinate ascent [4]. Finally  note that when the gradi-
ent estimate is unbiased (i.e.  weighted by the inverse sampling probability)  we obtain a variant of
importance sampling SGD [25]  though we do not necessarily need unbiased gradient estimates.
It is important to note that we do not actually need to compute the full posterior distribution—which
would take O(n) time per iteration—in order to sample from it. Indeed  using an algorithm and data
structure described in Appendix C  we can sample from and update the distribution in O(log n) time 
using O(n) space. Thus  the additional iteration complexity of adaptive sampling is logarithmic in
the size of the dataset  which suitably efﬁcient for learning from large datasets.
In practice  SGD is typically applied with mini-batching  whereby multiple examples are drawn at
each iteration  instead of just one. Given the massive parallelism of today’s computing hardware 
mini-batching is simply a more efﬁcient way to process a dataset  and can result in more accurate
gradient estimates than single-example updates. Though Algorithm 1 is stated for single-example
updates  it can be modiﬁed for mini-batching by replacing line 3 with multiple independent draws
from Qt  and line 5 with sampling weight updates for each unique4 example in the mini-batch.

4If an example is drawn multiple times in a mini-batch  its sampling weight is only updated once.

7

5.1 Divergence Analysis

Recall that our generalization bounds use the posterior’s divergence from a ﬁxed prior to penalize the
posterior for overﬁtting the training data. Thus  to connect Algorithm 1 to our bounds  we analyze
the adaptive posterior’s divergence from a uniform prior on {1  . . .   n}T . This quantity reﬂects the
potential cost  in generalization performance  of adaptive sampling. The goal of this section is to
upper-bound the KL divergence resulting from Algorithm 1 in terms of interpretable  data-dependent
quantities. All proofs are deferred to Appendix D.
Our analysis requires introducing some notation. Given a sequence of sampled indices  (i1  . . .   it) 
let Ni t (cid:44) |{t(cid:48) : t(cid:48) < t  it(cid:48) = i}| denote the number of times that index i was chosen before iteration
t. Let Oi j denote the jth iteration in which i was chosen; for instance  if i was chosen at iterations
13 and 47  then Oi 1 = 13 and Oi 2 = 47. With these deﬁnitions  we can state the following bound 
which exposes the inﬂuences of the utility function  amplitude and decay on the KL divergence.
Theorem 4. Fix a uniform prior  P  a utility function  f : Z × H → R  an amplitude  α ≥ 0  and a
decay  τ ∈ (0  1). If Algorithm 1 is run for T iterations  then its posterior  Q  satisﬁes

DKL(Q(cid:107)P) ≤ T(cid:88)

E

(i1 ... it)∼Q

α
n

(cid:34) Nit t(cid:88)

n(cid:88)

Ni t(cid:88)

(cid:35)

t=2

i=1

j=1

k=1

f (zit  hOit j ) τ Nit t−j −

f (zi  hOi k ) τ Ni t−k

. (9)

Equation 9 can be interpreted as measuring  on average  how the cumulative past utilities of each
sampled index  it  differ from the cumulative utilities of any other index  i.5 When the posterior
becomes too focused on certain examples  this difference is large. The accumulated utilities decay
exponentially  with the rate of decay controlled by τ. The amplitude  α  scales the entire bound 
which means that aggressive posterior updates may adversely affect generalization.
An interesting special case of Theorem 4 is when the utility function is nonnegative  which results
in a simpler  more interpretable bound.
Theorem 5. Fix a uniform prior  P  a nonnegative utility function  f : Z ×H → R+  an amplitude 
α ≥ 0  and a decay  τ ∈ (0  1). If Algorithm 1 is run for T iterations  then its posterior  Q  satisﬁes

DKL(Q(cid:107)P) ≤ α
1 − τ

E

(i1 ... it)∼Q

f (zit  ht)

.

(10)

T−1(cid:88)

t=1

(cid:104)

(cid:105)

Equation 10 is simply the sum of expected utilities computed over T −1 iterations of training  scaled
by α/(1 − τ ). The implications of this bound are interesting when the utility function is deﬁned as
the loss  f (z  h) (cid:44) L(h  z); then  if SGD quickly converges to a hypothesis with low maximal loss
on the training data  it can reduce the generalization error.6 The caveat is that tuning the amplitude
or decay to speed up convergence may actually counteract this effect.
It is worth noting that similar guarantees hold for a mini-batch variant of Algorithm 1. The bounds
are essentially unchanged  modulo notational intricacies.

6 Experiments

To demonstrate the effectiveness of Algorithm 1  we conducted several experiments with the CIFAR-
10 dataset [12]. This benchmark dataset contains 60 000 (32×32)-pixel RGB images from 10 object
classes  with a standard  static partitioning into 50 000 training examples and 10 000 test examples.
We speciﬁed the hypothesis class as the following convolutional neural network architecture: 32
(3 × 3) ﬁlters with rectiﬁed linear unit (ReLU) activations in the ﬁrst and second layers  followed
by (2 × 2) max-pooling and 0.25 dropout7; 64 (3 × 3) ﬁlters with ReLU activations in the third and
fourth layers  again followed by (2 × 2) max-pooling and 0.25 dropout; ﬁnally  a fully-connected 
512-unit layer with ReLU activations and 0.5 dropout  followed by a fully-connected  10-output
softmax layer. We trained the network using the cross-entropy loss. We emphasize that our goal was

5When Ni t = 0 (i.e.  i has not yet been sampled)  a summation over j = 1  . . .   Ni t evaluates to zero.
6This interpretation concurs with ideas in [10  22].
7It can be shown that dropout improves data stability [10  Lemma 4.4].

8

not to achieve state-of-the-art results on the dataset; rather  to evaluate Algorithm 1 in a simple  yet
realistic  application.
Following the intuition that sampling should focus on difﬁcult examples  we experimented with two
utility functions for Algorithm 1 based on common loss functions. For an example z = (x  y)  with
h(x  y) denoting the predicted probability of label y given input x under hypothesis h  let

f0(z  h) (cid:44) 1{arg maxy(cid:48)∈Y h(x  y(cid:48)) (cid:54)= y} and f1(z  h) (cid:44) 1 − h(x  y).

The ﬁrst utility function  f0  is the 0-1 loss; the second  f1  is the L1 loss  which accounts for
uncertainty in the most likely label. We combined these utility functions with two parameter update
rules: standard SGD with decreasing step sizes  ηt (cid:44) η/(1+νt) ≤ η/(νt)  for η > 0 and ν > 0; and
AdaGrad [5]  a variant of SGD that automatically tunes a separate step size for each parameter. We
used mini-batches of 100 examples per update. The combination of utility functions and update rules
yields four adaptive sampling algorithms: AdaSamp-01-SGD  AdaSamp-01-AdaGrad  AdaSamp-
L1-SGD and AdaSamp-L1-AdaGrad. We compared these to their uniform sampling counterparts 
Unif-SGD and Unif-AdaGrad.
We tuned all hyperparameters using random subsets of the training data for cross-validation. We then
ran 10 trials of training and testing  using different seeds for the pseudorandom number generator
at each trial to generate different random initializations8 and training sequences. Figures 1a and 1b
plot learning curves of the average cross-entropy and accuracy  respectively  on the training data;
Figure 1c plots the average accuracy on the test data. We found that all adaptive sampling variants
reduced empirical risk (increased training accuracy) faster than their uniform sampling counterparts.
Further  AdaGrad with adaptive sampling exhibited modest  yet consistent  improvements in test
accuracy in early iterations of training. Figure 1d illustrates the effect of varying the amplitude
parameter  α. Higher values of α led to faster empirical risk reduction  but lower test accuracy—a
sign of overﬁtting the posterior to the data  which concurs with Theorems 4 and 5 regarding the
inﬂuence of α on the KL divergence. Figure 1e plots the KL divergence from the conditional prior 
Pt  to the conditional posterior  Qt  given sampled indices (i1  . . .   it−1); i.e.  DKL(Qt(cid:107)Pt). The
sampling distribution quickly diverged in early iterations  to focus on examples where the model
erred  then gradually converged to a uniform distribution as the empirical risk converged.

(a) Train loss

(b) Train accuracy

(c) Test accuracy

(d) Impact of α

(e) DKL(Qt(cid:107)Pt)

Figure 1: Experimental results on CIFAR-10  averaged over 10 random initializations and training
runs. (Best viewed in color.) Figure 1a plots learning curves of training cross-entropy (lower is
better). Figures 1b and 1c  respectively  plot train and test accuracies (higher is better). Figure 1d
highlights the impact of the amplitude parameter  α  on accuracy. Figure 1e plots the KL divergence
from the conditional prior  Pt  to the conditional posterior  Qt  given sampled indices (i1  . . .   it−1).

7 Conclusions and Future Work

We presented new generalization bounds for randomized learning algorithms  using a novel combi-
nation of PAC-Bayes and algorithmic stability. The bounds inspired an adaptive sampling algorithm
for SGD that dynamically updates the sampling distribution based on the training data and model.
Experimental results with this algorithm indicate that it can reduce empirical risk faster than uniform
sampling while also improving out-of-sample accuracy. Future research could investigate different
utility functions and distribution updates  or explore the connections to related algorithms. We are
also interested in providing stronger generalization guarantees  with polylogarithmic dependence on
δ−1  for non-convex objective functions  but proving ˜O(1/
nT )-uniform hyperparameter stability
without (strong) convexity is difﬁcult. We hope to address this problem in future work.

√

8Each training algorithm started from the same initial hypothesis.

9

References
[1] L. Bottou and O. Bousquet. The tradeoffs of large scale learning.

Processing Systems  2008.

In Neural Information

[2] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning

Research  2:499–526  2002.

[3] O. Catoni. Pac-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learn-
ing  volume 56 of Institute of Mathematical Statistics Lecture Notes – Monograph Series. In-
stitute of Mathematical Statistics  2007.

[4] M. Collins  A. Globerson  T. Koo  X. Carreras  and P. Bartlett. Exponentiated gradient algo-
rithms for conditional random ﬁelds and max-margin Markov networks. Journal of Machine
Learning Research  9:1775–1822  2008.

[5] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[6] A. Elisseeff  T. Evgeniou  and M. Pontil. Stability of randomized learning algorithms. Journal

of Machine Learning Research  6:55–79  2005.

[7] J. Feng  T. Zahavy  B. Kang  H. Xu  and S. Mannor. Ensemble robustness of deep learning

algorithms. CoRR  abs/1602.02389  2016.

[8] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. In Computational Learning Theory  1995.

[9] P. Germain  A. Lacasse  F. Laviolette  and M. Marchand. PAC-Bayesian learning of linear

classiﬁers. In International Conference on Machine Learning  2009.

[10] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic

gradient descent. In International Conference on Machine Learning  2016.

[11] A. Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. In Inter-

national Conference on Machine Learning  2014.

[12] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical

report  University of Toronto  2009.

[13] I. Kuzborskij and C. Lampert. Data-dependent stability of stochastic gradient descent. CoRR 

abs/1703.01678  2017.

[14] J. Langford and J. Shawe-Taylor. PAC-Bayes and margins. In Neural Information Processing

Systems  2002.

[15] J. Lin and L. Rosasco. Optimal learning for multi-pass stochastic gradient methods. In Neural

Information Processing Systems  2016.

[16] J. Lin  R. Camoriano  and L. Rosasco. Generalization properties and implicit regularization

for multiple passes SGM. In International Conference on Machine Learning  2016.

[17] B. London  B. Huang  and L. Getoor. Stability and generalization in structured prediction.

Journal of Machine Learning Research  17(222):1–52  2016.

[18] D. McAllester. PAC-Bayesian model averaging. In Computational Learning Theory  1999.
[19] L. Rosasco and S. Villa. Learning with incremental iterative regularization. In Neural Infor-

mation Processing Systems  2015.

[20] M. Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classiﬁcation.

Journal of Machine Learning Research  3:233–269  2002.

[21] S. Shalev-Shwartz. Selﬁeboost: A boosting algorithm for deep learning. CoRR  abs/1411.3436 

2014.

[22] S. Shalev-Shwartz and Y. Wexler. Minimizing the maximal loss: How and why. In Interna-

tional Conference on Machine Learning  2016.

[23] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Learnability  stability and uniform

convergence. Journal of Machine Learning Research  11:2635–2670  2010.

[24] Y. Wang  J. Lei  and S. Fienberg. Learning with differential privacy: Stability  learnability and
the sufﬁciency and necessity of ERM principle. Journal of Machine Learning Research  17
(183):1–40  2016.

[25] P. Zhao and T. Zhang. Stochastic optimization with importance sampling for regularized loss

minimization. In International Conference on Machine Learning  2015.

10

,Vasilis Syrgkanis
Alekh Agarwal
Haipeng Luo
Robert Schapire
Brian Dolhansky
Jeff Bilmes
Ben London
Edward Hughes
Joel Leibo
Matthew Phillips
Karl Tuyls
Edgar Dueñez-Guzman
Antonio García Castañeda
Iain Dunning
Tina Zhu
Kevin McKee
Raphael Koster
Heather Roff
Thore Graepel
Juyeon Heo
Sunghwan Joo
Taesup Moon