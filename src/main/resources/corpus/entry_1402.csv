2014,Deep Symmetry Networks,The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability  such as pose and part deformation. These sources of variation can be represented by symmetry groups  sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group  but cannot handle other groups. As a result  these groups' effects have to be approximated by small translations  which often requires augmenting datasets and leads to high sample complexity. In this paper  we introduce deep symmetry networks (symnets)  a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets  they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.,Deep Symmetry Networks

Robert Gens

Pedro Domingos

Department of Computer Science and Engineering

University of Washington

Seattle  WA 98195-2350  U.S.A.

{rcg pedrod}@cs.washington.edu

Abstract

The chief difﬁculty in object recognition is that objects’ classes are obscured by
a large number of extraneous sources of variability  such as pose and part de-
formation. These sources of variation can be represented by symmetry groups 
sets of composable transformations that preserve object identity. Convolutional
neural networks (convnets) achieve a degree of translational invariance by com-
puting feature maps over the translation group  but cannot handle other groups.
As a result  these groups’ effects have to be approximated by small translations 
which often requires augmenting datasets and leads to high sample complexity.
In this paper  we introduce deep symmetry networks (symnets)  a generalization
of convnets that forms feature maps over arbitrary symmetry groups. Symnets
use kernel-based interpolation to tractably tie parameters and pool over symmetry
spaces of any dimension. Like convnets  they are trained with backpropagation.
The composition of feature transformations through the layers of a symnet pro-
vides a new approach to deep learning. Experiments on NORB and MNIST-rot
show that symnets over the afﬁne group greatly reduce sample complexity relative
to convnets by better capturing the symmetries in the data.

Introduction

1
Object recognition is a central problem in vision. What makes it challenging are all the nuisance
factors such as pose  lighting  part deformation  and occlusion. It has been shown that if we could
remove these factors  recognition would be much easier [2  18]. Convolutional neural networks
(convnets) [16]  the current state-of-the-art method for object recognition  capture only one type of
invariance (translation); the rest have to be approximated via it and standard features. In practice 
the best networks require enormous datasets which are further expanded by afﬁne transformations
[7  13] yet are sensitive to imperceptible image perturbations [24]. We propose deep symmetry
networks  a generalization of convnets based on symmetry group theory [21] that makes it possible
to capture a broad variety of invariances  and correspondingly improves generalization.
A symmetry group is a set of transformations that preserve the identity of an object and obey the
group axioms. Most of the visual nuisance factors are symmetry groups themselves  and by incor-
porating them into our model we are able to reduce the sample complexity of learning from data
transformed by these groups. Deep symmetry networks (symnets) form feature maps over any sym-
metry group  rather than just the translation group. A feature map in a deep symmetry network
is deﬁned analogously to convnets as a ﬁlter that is applied at all points in the symmetry space.
Each layer in our general architecture is constructed by applying every symmetry in the group to
the input  computing features on the transformed input  and pooling over neighborhoods. The entire
architecture is then trained by backpropagation. In this paper  we instantiate the architecture with the
afﬁne group  resulting in deep afﬁne networks. In addition to translation  the afﬁne group includes
rotation  scaling and shear. The afﬁne group of the two-dimensional plane is six-dimensional (i.e. 
an afﬁne transformation can be represented by a point in 6D afﬁne space). The key challenge with

1

extending convnets to afﬁne spaces is that it is intractable to explicitly represent and compute with
a high-dimensional feature map. We address this by approximating the map using kernel functions 
which not only interpolate but also control pooling in the feature maps. Compared to convnets 
this architecture substantially reduces sample complexity on image datasets involving 2D and 3D
transformations.
We share with other researchers the hypothesis that explanatory factors cannot be disentangled un-
less they are represented in an appropriate symmetry space [4  11]. Our adaptation of a repre-
sentation to work in symmetry space is similar in some respects to the use of tangent distance in
nearest-neighbor classiﬁers [23]. Symnets  however  are deep networks that compute features in
symmetry space at every level. Whereas the tangent distance approximation is only locally accurate 
symnet feature maps can represent large displacements in symmetry space. There are other deep
networks that reinterpret the invariance of convolutional networks. Scattering networks [6] are cas-
cades of wavelet decompositions designed to be invariant to particular Lie groups  where translation
and rotation invariance have been demonstrated so far. The M-theory of Anselmi et al. [2] con-
structs features invariant to a symmetry group by using statistics of dot products with group orbits.
We differ from these networks in that we model multiple symmetries jointly in each layer  we do not
completely pool out a symmetry  and we discriminatively train our entire architecture. The ﬁrst two
differences are important because objects and their subparts may have relative ﬂexibility but not total
invariance along certain dimensions of symmetry space. For example  a leg of a person can be seen
in some but not all combinations of rotation and scale relative to the torso. Without discriminative
training  scattering networks and M-theory are limited to representing features whose invariances
may be inappropriate for a target concept because they are ﬁxed ahead of time  either by the wavelet
hierarchy of the former or unsupervised training of the latter. The discriminative training of symnets
yields features with task-oriented invariance to their sub-features. In the context of digit recognition
this might mean learning the concept of a ‘0’ with more rotation invariance than a ‘6’  which would
incur loss if it had positive weights in the region of symmetry space where a ‘9’ would also ﬁre.
Much of the vision literature is devoted to features that reduce or remove the effects of certain sym-
metry groups  e.g.  [19  18]. Each feature by itself is not discriminative for object recognition  so
structure is modeled separately  usually with a representation that does not generalize to novel view-
points (e.g.  bags-of-features) or with a rigid alignment algorithm that cannot represent uncertainty
over geometry (e.g. [9  20]). Compared to symnets  these features are not learned  have invariance
limited to a small set of symmetries  and destroy information that could be used to model object
sub-structure. Like deformable part models [10]  symnets can model and penalize relative transfor-
mations that compose up the hierarchy  but can also capture additional symmetries.
Symmetry group theory has made a limited number of appearances in machine learning [8]. A few
applications are discussed by Kondor [12]  and they are also used in determinantal point processes
[14]. Methods for learning transformations from examples [25  11] could potentially beneﬁt from
being embedded in a deep symmetry network. Symmetries in graphical models [22] lead to effective
lifted probabilistic inference algorithms. Deep symmetry networks may be applicable to these and
other areas.
In this paper  we ﬁrst review symmetry group theory and its relation to sample complexity. We then
describe symnets and their afﬁne instance  and develop new methods to scale to high-dimensional
symmetry spaces. Experiments on NORB and MNIST-rot show that afﬁne symnets can reduce by a
large factor the amount of data required to achieve a given accuracy level.

2 Symmetry Group Theory

A symmetry of an object is a transformation that leaves certain properties of that object intact [21].
A group is a set S with an operator ∗ on it with the four properties of closure  associativity  an
identity element  and an inverse element. A symmetry group is a type of group where the group
elements are functions and the operator is function composition. A simple geometric example is
the symmetry group of a square  which consists of four reﬂections and {0  1  2  3} multiples of 90-
degree rotations. These transformations can be composed together to yield one of the original eight
symmetries. The identity element is the 0-degree rotation. Each symmetry has a corresponding
inverse element. Composition of these symmetries is associative.

2

Lie groups are continuous symmetry groups whose elements form a smooth differentiable manifold.
For example  the symmetries of a circle include reﬂections and rotations about the center. The afﬁne
group is a set of transformations that preserves collinearity and parallel lines. The Euclidean group
is a subgroup of the afﬁne group that preserves distances  and includes the set of rigid body motions
(translations and rotations) in three-dimensional space.
The elements of a symmetry group can be represented as matrices. In this form  function composi-
tion can be performed via matrix multiplication. The transformation P followed by Q (also denoted
Q ◦ P) is computed as R = QP. In this paper we treat the transformation matrix P as a point
in D-dimensional space  where D depends on the particular representation of the symmetry group
(e.g.  D = 6 for afﬁne transformations in the plane).
A generating set of a group is a subset of the group such that any group element can be expressed
through combinations of generating set elements and their inverses. For example  a generating set
of the translation symmetry group is {x → x +   y → y + } for inﬁnitesimal . We deﬁne the
k-neighborhood of element f in group S under generating set G as the subset of S that can be
expressed as f composed with elements of G or their inverses at most k times. With the previous
example  the k-neighborhood of a translation vector f would take the shape of a diamond centered
at f in the xy-plane.
The orbit of an object x is the set of objects obtained by applying each element of a symmetry group
to x. Formally  a symmetry group S acting on a set of objects X deﬁnes an orbit for each x ∈ X:
Ox = {s∗ x : s ∈ S}. For example  the orbit of an image I(u) whose points are transformed by the
rotation symmetry group s ∗ I(u) = I(s−1 ∗ u) is the set of images resulting from all rotations of
that image. If two orbits share an element  they are the same orbit. In this way  a symmetry group
a Oa. If a data distribution D(x  y) has
the property that all the elements of an orbit share the same label y  S imposes a constraint on the
hypothesis class of a learner  effectively lowering its VC-dimension and sample complexity [1].

S partitions the set of objects into unique orbits X = (cid:83)

3 Deep Symmetry Networks

Deep symmetry networks represent rich compositional structure that incorporates invariance to high-
dimensional symmetries. The ideas behind these networks are applicable to any symmetry group  be
it rigid-body transformations in 3D or permutation groups over strings. The architecture of a symnet
consists of several layers of feature maps. Like convnets  these feature maps beneﬁt from weight
tying and pooling  and the whole network is trained with backpropagation. The maps and the ﬁlters
they apply are in the dimension D of the chosen symmetry group S.
A deep symmetry network has L layers l ∈ {1  ...  L} each with Il features and corresponding
feature maps. A feature is the dot-product of a set of weights with a corresponding set of values from
a local region of a lower layer followed by a nonlinearity. A feature map represents the application
of a ﬁlter at all points in symmetry space. A feature at point P is computed from the feature maps
of the lower layer at points in the k-neighborhood of P. As P moves in the symmetry space of a
feature map  so does its neighborhood of inputs in the lower layer. Feature map i of layer l is denoted
M [l  i] : RD → R  a scalar function of the D-dimensional symmetry space. Given a generating set
G ⊂ S  the points in the k-neighborhood of the identity element are stored in an array T[ ]. Each
ﬁlter i of layer l deﬁnes a weight vector w[l  i  j] for each point T[j] in the k-neighborhood. The
vector w[l  i  j] is the size of Il−1  the number of features in the underlying layer. For example  a
feature in an afﬁne symnet that detects a person would have positive weight for an arm sub-feature in
the region of the k-neighborhood that would transform the arm relative to the person (e.g.  smaller 
rotated  and translated relative to the torso). The value of feature map i in layer l at point P is
the dot-product of weights and underlying feature values in the neighborhood of P followed by a
nonlinearity:

M [l  i](P) =

v(P  l  i) = (cid:80)|T|

x(P(cid:48)) =

σ (v(P  l  i))

(cid:42) S(M [l − 1  0])(P(cid:48))
(cid:43)
j w[l  i  j] · x(P ◦ T[j])

. . .

S(M [l − 1  Il−1])(P(cid:48))

3

(1)
(2)

(3)

sum-pooling  S(M [l  i])(P) = (cid:82) M [l  i](P − Q)K(Q) dQ; for max-pooling  S(M [l  i])(P) =

Figure 1: The evaluation of point P in map M [l  i]. The elements of the k-neighborhood of P are
computed P ◦ T[j]. Each point in the neighborhood is evaluated in the pooled feature maps of the
lower layer l − 1. The pooled maps are computed with kernels on the underlying feature maps. The
dashed line intersects the points in the pooled map whose values form x(P ◦ T[j]) in Equation 3; it
also intersects the contours of kernels used to compute those pooled values. The value of the feature
is the sum of the dot-products w[l  i  j] · x(P ◦ T[j]) over all j  followed by a nonlinearity.
where σ is the nonlinearity (e.g.  tanh(x) or max(x  0))  v(P  l  i) is the dot product  P ◦ T[j] rep-
resents element j in the k-neighborhood of P  and x(P(cid:48)) is the vector of values from the underlying
pooled maps at point P(cid:48). This deﬁnition is a generalization of feature maps in convnets1. Similarly 
the same ﬁlter weights w[l  i  j] are tied across all points P in feature map M [l  i]. The evaluation
of a point in a feature map is visualized in Figure 1.
Feature maps M [l  i] are pooled via kernel convolution to become S(M [l  i]).
In the case of
maxQ M [l  i](P− Q)K(Q). The kernel K(Q) is also a scalar function of the D-dimensional sym-
metry space. In the previous example of a person feature  the arm feature map could be pooled over
a wide range of rotations but narrow range of translations and scales so that the person feature allows
for moveable but not unrealistic arms. Each ﬁlter can specify the kernels it uses to pool lower layers 
but for the sake of brevity and analogy to convnets we assume that the feature maps of a layer are
pooled by the same kernel. Note that convnets discretize these operations  subsample the pooled
map  and use a uniform kernel K(Q) = 1{(cid:107)Q(cid:107)∞ < r}.
As with convnets  the values of points in a symnet feature map are used by higher symnet layers 
layers of fully connected hidden units  and ultimately softmax classiﬁcation. Hidden units take the
familiar form o = σ(Wx + b)  with input x  output o  weight matrix W  and bias b. The log-loss
c exp (wc · x + bc))  where Y = i is
the true label  wc and bc are the weight vector and bias for class c  and the summation is over the
classes. The input image is treated as a feature map (or maps  if color or stereo) with values in the
translation symmetry space.
Deep symmetry networks are trained with backpropagation and are amenable to the same best prac-
tices as convnets. Though feature maps are deﬁned as continuous  in practice the maps and their
gradients are evaluated on a ﬁnite set of points P ∈ M [l  i]. We provide the partial derivative of the
loss L with respect to a weight vector.

of the softmax L on an instance is −wi · x − bi + log ((cid:80)

= (cid:80)

∂L

∂w[l  i  j]
∂M [l  i](P)
∂w[l  i  j]

=

∂L

∂M [l i](P)

∂M [l i](P)

P∈M [l i]
σ(cid:48) (v(P  l  i)) x(P ◦ T[j])

∂w[l i j]

(4)

(5)

1The neighborhood that deﬁnes a square ﬁlter in convnets is the reference point translated by up to k times

in x and k times in y.

4

Layer lFeature map iLayer l-1Feature maps 0 1 2Layer l-1Pooled feature maps 0 1 2KernelsFigure 2: The feature hierarchy of a three-layer deep afﬁne net is visualized with and without
pooling. From top to bottom  the layers (A B C) contain one  ﬁve  and four feature maps  each cor-
responding to a labeled part of the cartoon ﬁgure. Each horizontal line represents a six-dimensional
afﬁne feature map  and bold circles denote six-dimensional points in the map. The dashed lines
represent the afﬁne transformation from a feature to the location of one of its ﬁlter points. For clar-
ity  only a subset of ﬁlter points are shown. Left: Without pooling  the hierarchy represents a rigid
afﬁne transformation among all maps. Another point on feature map A is visualized in grey. Right:
Feature maps B1 and C1 are pooled with a kernel that gives those features ﬂexibility in rotation.
The partial derivative of the loss L with respect to the value of a point in a lower layer is

∂L

∂M [l − 1  i](P)
∂M [l  i(cid:48)](P(cid:48))
∂M [l − 1  i](P)

(cid:80)Il
i(cid:48)(cid:80)

=

= σ(cid:48) (v(P(cid:48)  l  i(cid:48)))(cid:80)|T|

P(cid:48)∈M [l i(cid:48)]

∂L

∂M [l i(cid:48)](P(cid:48))

∂M [l i(cid:48)](P(cid:48))
∂M [l−1 i](P)

j w[l  i(cid:48)  j][i] ∂S(M [l−1 i])(P(cid:48)◦T[j])

∂M [l−1 i](P)

(6)

(7)

where the gradient of the pooled feature map ∂S(M [l i])(P)
∂M [l i](Q)

equals K(P − Q) for sum-pooling.

None of this treatment depends explicitly on the dimensionality of the space except for the kernel
and transformation composition which have polynomial dependence on D. In the next section we
apply this architecture to the afﬁne group in 2D  but it could also be applied to the afﬁne group in
3D or any other symmetry group.
4 Deep Afﬁne Networks
We instantiate a deep symmetry network with the
afﬁne symmetry group in the plane. The afﬁne
symmetry group contains transformations capa-
ble of rotating  scaling  shearing  and translating
two-dimensional points. The transformation is de-
scribed by six coordinates:

(cid:21)

(cid:20) x(cid:48)

y(cid:48)

=

(cid:20) a b

(cid:21)(cid:20) x

c

d

y

(cid:21)

(cid:21)

(cid:20) e

f

+

This means that each of the feature maps M [l  i]
and elements T[j] of the k-neighborhood is repre-
sented in six dimensions. The identity transforma-
tion is a = d = 1  b = c = e = f = 0. The generating
set of the afﬁne symmetry group contains six el-
ements  each of which is obtained by adding  to
one of the six coordinates in the identity transform.
This generating set is visualized in Figure 3.
A deep afﬁne network can represent a rich part hierarchy where each weight of a feature modulates
the response to a subpart at a point in the afﬁne neighborhood. The geometry of a deep afﬁne network
is best understood by tracing a point on a feature map through its ﬁlter point transforms into lower
layers. Figure 2 visualizes this structure without and with pooling on the left and right sides of
the diagram  respectively. Without pooling  the feature hierarchy deﬁnes a rigid afﬁne relationship
between the point of evaluation on a map and the location of its sub-features. In contrast  a pooled
value on a sub-feature map is computed from a neighborhood deﬁned by the kernel of points in
afﬁne space; this can represent model ﬂexibility along certain dimensions of afﬁne space.

Figure 3: The six transformations in the gener-
ating set of the afﬁne group applied to a square
(exaggerated  = 0.2  identity is black square).

5

AB1B2C2B3C3B4C4B5C1AB1B2C2B3C3B4C4B5C1AB1B2B3B4B5C1C2C3C4AB1B2C2B3C3B4C4B5C1B1C1B1C1AB1B2B3B4B5C1C2C3C4−101−101Assuming(cid:80)|T|

5 Scaling to High-Dimensional Symmetry Spaces
It would be intractable to explicitly represent the high-dimensional feature maps of symnets. Even a
subsampled grid becomes unwieldy at modest dimensions (e.g.  a grid in afﬁne space with ten steps
per axis has 106 points). Instead  each feature map is evaluated at N control points. The control
points are local maxima of the feature in symmetry space  found by Gauss-Newton optimization 
each initialized from a prior. This can be seen as a form of non-maximum suppression. Since the
goal is recognition  there is no need to approximate the many points in symmetry space where the
feature is not present. The map is then interpolated with kernel functions; the shape of the function
also controls pooling.
5.1 Transformation Optimization
Convnets max-pool a neighborhood of translation space by exhaustive evaluation of feature loca-
tions. There are a number of algorithms that solve for a maximal feature location in symmetry space
but they are not efﬁcient when the feature weights are frequently adjusted [9  20]. We adopt an
iterative approach that dovetails with the deﬁnition of our features.
If a symnet is based on a Lie group  gradient based optimization can be used to ﬁnd a point P∗
that locally maximizes the feature value (Equation 1) initialized at point P. In our experiments
(cid:80)|T|
with deep afﬁne nets  we follow the forward compositional (FC) warp [3] to align ﬁlters with the
image. An extension of Lucas-Kanade  FC solves for an image alignment. We adapt this procedure
j (cid:107)w[l  i  j] − x(P ◦ ∆P ◦ T[j])(cid:107)2. We run an FC
to our ﬁlters and weight vectors: min∆P
alignment for each of the N control points in feature map M [l  i]  each initialized from a prior.
j (cid:107)x(P ◦ ∆P ◦ T[j])(cid:107)2 is constant  this procedure locally maximizes the dot product
between the ﬁlter and the map in Equation 2. Each iteration of FC takes a Gauss-Newton step to
solve for a transformation of the neighborhood of the feature in the underlying map ∆P  which is
then composed with the control point: P ← P ◦ ∆P.
5.2 Kernels
Given
=
{(P1  v1)  . . .   (PN   vN )}
feature
map M [l  i]  we use kernel-based interpolation to compute
a pooled map S(M [l  i]).
The kernel performs three
functions: penalizing relative locations of sub-features
in symmetry space (cf.
[10])  interpolating the map  and
pooling a region of the map. These roles could be split
into separate ﬁlter-speciﬁc kernels that are then convolved
appropriately.
The choice of these kernels will vary
with the application.
In our experiments  we lump these
functions into a single kernel for a layer. We use a Gaussian
kernel K(Q) = e−qT Σ−1q where q is the D-dimensional
vector representation of Q and the D×D covariance matrix
Σ controls the shape and extent of the kernel. Several
instances of this kernel are shown in Figure 4. Max-pooling
produced the best results on our tests.
6 Experiments
In our experiments we test the hypothesis that a deep network with access to a larger symmetry group
will generalize better from fewer examples  provided those symmetries are present in the data. In
particular  theory suggests that a symnet will have better sample complexity than another classiﬁer
on a dataset if it is based on a symmetry group that generates variations present in that dataset [1].
We compare deep afﬁne symnets to convnets on the MNIST-rot and NORB image classiﬁcation
datasets  which ﬁnely sample their respective symmetry spaces such that learning curves measure
the amount of augmentation that would be required to achieve similar performance. On both datasets
afﬁne symnets achieve a substantial reduction in sample complexity. This is particularly remarkable
on NORB because its images are generated by a symmetry space in 3D. Symnet running time was
within an order of magnitude of convnets  and could be greatly optimized.

Figure 4: Contours of three 6D Gaus-
sian kernels visualized on a surface
in afﬁne space. Points are visualized
by an oriented square transformed by
the afﬁne transformation at that point.
Each kernel has a different covariance
matrix Σ.

in D-dimensional

a

set

of N local

optima O∗

6.1 MNIST-rot
MNIST-rot [15] consists of 28x28 pixel greyscale images: 104 for training  2 × 103 for validation 
and 5× 104 for testing. The images are sampled from the MNIST digit recognition dataset and each

6

Figure 5: Impact of training set size on MNIST-rot test performance for architectures that use either
one convolutional layer or one afﬁne symnet layer.
is rotated by a random angle in the uniform distribution [0  2π]. With transformations that apply
to the whole image  MNIST-rot is a good testbed for comparing the performance of a single afﬁne
layer to a single convnet layer.
We modiﬁed the Theano [5] implementation of convolutional networks so that the network consisted
of a single layer of convolution and maxpooling followed by a hidden layer of 500 units and then
softmax classiﬁcation. The afﬁne net layer was directly substituted for the convolutional layer. The
control points of the afﬁne net were initialized at uniformly random positions with rotations oriented
around the image center  and each control point was locally optimized with four iterations of Gauss-
Newton updates. The ﬁlter points of the afﬁne net were arranged in a square grid. Both the afﬁne
net and the convnet compute a dot-product and use the sigmoid nonlinearity. Both networks were
trained with 50 epochs of mini-batch gradient descent with momentum  and test results are reported
on the network with lowest error on the validation set2. The convnet did best with small 5 × 5
ﬁlters and the symnet with large 20 × 20 ﬁlters. This is not surprising because the convnet must
approximate the large rotations of the dataset with translations of small patches. The afﬁne net can
pool directly in this space of rotations with large ﬁlters.
Learning curves for the two networks are presented in Figure 5. We observe that the afﬁne symnet
roughly halves the error of the convnet. With small sample sizes  the symnet achieves an accuracy
for which the convnet requires about eight times as many samples.

6.2 NORB
MNIST-rot is a synthetic dataset with symmetries that are not necessarily representative of real
images. The NORB dataset [17] contains 2× 108× 108 pixel stereoscopic images of 50 toys in ﬁve
categories: quadrupeds  human ﬁgures  airplanes  trucks  and cars. Five of the ten instances of each
category are reserved for the test set. Each toy is photographed on a turntable from an exhaustive
set of angles and lighting conditions. Each image is then perturbed by a random translation shift 
planar rotation  luminance change  contrast change  scaling  distractor object  and natural image
background. A sixth blank category containing just the distractor and background is also used. As
in other papers  we downsample the images to 2×48×48. To compensate for the effect of distractors
in smaller training sets  we also train and test on a version of the dataset that is centrally-cropped to
2 × 24 × 24. We report results for whichever version had lower validation error. In our experiments
we train on a variable subset of the ﬁrst training fold  using the ﬁrst 2 × 103 images of the second
fold for validation. Our results use both of the testing folds.
We compare architectures that use two convolutional layers or two afﬁne ones  which performed
better than single-layer ones. As with the MNIST-rot experiments  the symnet and convnet layers
are followed by a layer of 500 hidden units and softmax classiﬁcation. The symnet control points in
the ﬁrst layer were arranged in three concentric rings in translation space  with 8 points spaced across
rotation (200 total points). Control points in the second layer were ﬁxed at the center of translation
2Grid search over learning rate {.1  .2}  mini-batch size {10  50  100}  ﬁlter size {5  10  15  20  25}  num-
ber of ﬁlters {20  50  80}  pooling size (convnet) {2  3  4}  and number of control points (symnet) {5  10  20}.

7

Figure 6: Impact of training set size on NORB test performance for architectures with two convolu-
tional or afﬁne symnet layers followed by a fully connected layer and then softmax classiﬁcation.

space arranged over 8 rotations and up to 2 vertical scalings (16 total points) to approximate the
effects of elevation change. Control points were not iteratively optimized due to the small size of
object parts in downsampled images. The ﬁlter points of the ﬁrst layer of the afﬁne net were arranged
in a square grid. The second layer ﬁlter points were arranged in a circle in translation space at a 3
or 4 pixel radius  with 8 ﬁlter points evenly spaced across rotation at each translation. We report the
test results of the networks with lowest validation error on a range of hyperparameters3.
The learning curves for convnets and afﬁne symnets are shown in Figure 6. Even though the primary
variability in NORB is due to rigid 3D transformations  we ﬁnd that our afﬁne networks still have
an advantage over convnets. A 3D rotation can be locally approximated with 2D scales  shears  and
rotations. The afﬁne net can represent these transformations and so it beneﬁted from larger ﬁlter
patches. The translation approximation of the convnet is unable to properly align larger features to
the true symmetries  and so it performed better with smaller ﬁlters. The convnet requires about four
times as much data to reach the accuracy of the symnet with the smallest training set. Larger ﬁlters
capture more structure than smaller ones  allowing symnets to generalize better than convnets  and
effectively giving each symnet layer the power of more than one convnet layer.
The left side of the graph may be more indicative of the types of gains symnets may have over
convnets in more realistic datasets that do not have thousands of images of identical 3D shapes.
With the ability to apply more realistic transformations to sub-parts  symnets may also be better able
to reuse substructure on datasets with many interrelated or ﬁne-grained categories. Since symnets
are a clean generalization of convnets  they should beneﬁt from the learning  regularization  and
efﬁciency techniques used by state-of-the-art networks [13].
7 Conclusion
Symmetry groups underlie the hardest challenges in computer vision. In this paper we introduced
deep symmetry networks  the ﬁrst deep architecture that can compute features over any symmetry
group. It is a natural generalization of convolutional neural networks that uses kernel interpolation
and transformation optimization to address the difﬁculties in representing high-dimensional feature
maps. In experiments on two image datasets with 2D and 3D variability  afﬁne symnets achieved
higher accuracy than convnets while using signiﬁcantly less data.
Directions for future work include extending to other symmetry groups (e.g.  lighting  3D space) 
modeling richer distortions  incorporating probabilistic inference  and scaling to larger datasets.
Acknowledgments
This research was partly funded by ARO grant W911NF-08-1-0242  ONR grants N00014-13-1-
0720 and N00014-12-1-0312  and AFRL contract FA8750-13-2-0019. The views and conclusions
contained in this document are those of the authors and should not be interpreted as necessarily
representing the ofﬁcial policies  either expressed or implied  of ARO  ONR  AFRL  or the United
States Government.
control point translation spacing (symnet) {2  3}  momentum {0  0.5  0.9}  others as in MNIST-rot.

3Grid search over ﬁlter size in each layer {6  9}  pooling size in each layer (convnet) {2  3  4}  ﬁrst layer

8

References
[1] Y. S. Abu-Mostafa. Hints and the VC dimension. Neural Computation  5(2):278–288  1993.
[2] F. Anselmi  J. Z. Leibo  L. Rosasco  J. Mutch  A. Tacchetti  and T. Poggio. Unsupervised learning of

invariant representations in hierarchical architectures. ArXiv preprint 1311.4158  2013.

[3] S. Baker and I. Matthews. Lucas-Kanade 20 years on: A unifying framework. International Journal of

Computer Vision  56(3):221–255  2004.

[4] Y. Bengio  A. Courville  and P. Vincent. Representation learning: A review and new perspectives. IEEE

Transactions on Pattern Analysis and Machine Intelligence  35(8):1798–1828  2013.

[5] J. Bergstra  O. Breuleux  F. Bastien  P. Lamblin  R. Pascanu  G. Desjardins  J. Turian  D. Warde-Farley 
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for
Scientiﬁc Computing Conference  2010.

[6] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Transactions on Pattern Analysis

and Machine Intelligence  35(8):1872–1886  2013.

[7] D. Ciresan  U. Meier  and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2012.

[8] P. Diaconis. Group representations in probability and statistics. Institute of Mathematical Statistics  1988.
[9] B. Drost  M. Ulrich  N. Navab  and S. Ilic. Model globally  match locally: Efﬁcient and robust 3D object
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2010.
[10] P. Felzenszwalb  D. McAllester  and D. Ramanan. A discriminatively trained  multiscale  deformable part

model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2008.

[11] G. E. Hinton  A. Krizhevsky  and S. D. Wang. Transforming auto-encoders. In Proceedings of the Twenty-

First International Conference on Artiﬁcial Neural Networks  2011.

[12] I. R. Kondor. Group theoretical methods in machine learning. Columbia University  2008.
[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems 25  2012.

[14] A. Kulesza and B. Taskar. Determinantal point processes for machine learning. ArXiv preprint 1207.6083 

2012.

[15] H. Larochelle  D. Erhan  A. Courville  J. Bergstra  and Y. Bengio. An empirical evaluation of deep archi-
tectures on problems with many factors of variation. In Proceedings of the Twenty-Fourth International
Conference on Machine Learning  2007.

[16] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[17] Y. LeCun  F. J. Huang  and L. Bottou. Learning methods for generic object recognition with invariance to
pose and lighting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
2004.

[18] T. Lee and S. Soatto. Video-based descriptors for object recognition.

29(10):639–652  2011.

Image and Vision Computing 

[19] D. G. Lowe. Object recognition from local scale-invariant features. In Proceedings of the IEEE Confer-

ence on Computer Vision and Pattern Recognition  1999.

[20] F. Lu and E. Milios. Robot pose estimation in unknown environments by matching 2D range scans.

Journal of Intelligent and Robotic Systems  18(3):249–275  1997.

[21] W. Miller. Symmetry groups and their applications. Academic Press  1972.
[22] M. Niepert. Markov chains on orbits of permutation groups. In Proceedings of the Twenty-Eight Confer-

ence on Uncertainty in Artiﬁcial Intelligence  2012.

[23] P. Simard  Y. LeCun  and J. S. Denker. Efﬁcient pattern recognition using a new transformation distance.

In Advances in Neural Information Processing Systems 5  1992.

[24] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.
properties of neural networks. International Conference on Learning Representations  2014.

Intriguing

[25] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural

Computation  14(4):715–770  2002.

9

,Robert Gens
Pedro Domingos
David Duvenaud
Dougal Maclaurin
Jorge Iparraguirre
Rafael Bombarell
Timothy Hirzel
Alan Aspuru-Guzik
Ryan Adams
Aidan Gomez
Mengye Ren
Raquel Urtasun
Roger Grosse