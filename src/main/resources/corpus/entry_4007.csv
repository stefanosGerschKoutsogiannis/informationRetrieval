2019,On the convergence of single-call stochastic extra-gradient methods,Variational inequalities have recently attracted considerable interest in machine learning as a flexible paradigm for models that go beyond ordinary loss function minimization (such as generative adversarial networks and related deep learning systems). In this setting  the optimal O(1/t) convergence rate for solving smooth monotone variational inequalities is achieved by the Extra-Gradient (EG) algorithm and its variants. Aiming to alleviate the cost of an extra gradient step per iteration (which can become quite substantial in deep learning)  several algorithms have been proposed as surrogates to Extra-Gradient with a single oracle call per iteration. In this paper  we develop a synthetic view of such algorithms  and we complement the existing literature by showing that they retain a $O(1/t)$ ergodic convergence rate in smooth  deterministic problems. Subsequently  beyond the monotone deterministic case  we also show that the last iterate of single-call  stochastic extra-gradient methods still enjoys a $O(1/t)$ local convergence rate to solutions of non-monotone variational inequalities that satisfy a second-order sufficient condition.,On the Convergence of Single-Call
Stochastic Extra-Gradient Methods

Yu-Guan Hsieh

Univ. Grenoble Alpes  LJK and ENS Paris

38000 Grenoble  France.
yu-guan.hsieh@ens.fr

Franck Iutzeler

Univ. Grenoble Alpes  LJK
38000 Grenoble  France.

franck.iutzeler@univ-grenoble-alpes.fr

Jérôme Malick

CNRS  LJK

38000 Grenoble  France.

jerome.malick@univ-grenoble-alpes.fr

Univ. Grenoble Alpes  CNRS  Inria  Grenoble INP  LIG

Panayotis Mertikopoulos

38000 Grenoble  France.

panayotis.mertikopoulos@imag.fr

Abstract

Variational inequalities have recently attracted considerable interest in machine
learning as a ﬂexible paradigm for models that go beyond ordinary loss function
minimization (such as generative adversarial networks and related deep learning
systems). In this setting  the optimal O(1/t) convergence rate for solving smooth
monotone variational inequalities is achieved by the Extra-Gradient (EG) algorithm
and its variants. Aiming to alleviate the cost of an extra gradient step per iteration
(which can become quite substantial in deep learning applications)  several algo-
rithms have been proposed as surrogates to Extra-Gradient with a single oracle
call per iteration. In this paper  we develop a synthetic view of such algorithms 
and we complement the existing literature by showing that they retain a O(1/t)
ergodic convergence rate in smooth  deterministic problems. Subsequently  beyond
the monotone deterministic case  we also show that the last iterate of single-call 
stochastic extra-gradient methods still enjoys a O(1/t) local convergence rate
to solutions of non-monotone variational inequalities that satisfy a second-order
sufﬁcient condition.

1

Introduction

Deep learning is arguably the fastest-growing ﬁeld in artiﬁcial intelligence: its applications range from
image recognition and natural language processing to medical anomaly detection  drug discovery  and
most ﬁelds where computers are required to make sense of massive amounts of data. In turn  this has
spearheaded a proliﬁc research thrust in optimization theory with the twofold aim of demystifying
the successes of deep learning models and of providing novel methods to overcome their failures.
Introduced by Goodfellow et al. [20]  generative adversarial networks (GANs) have become the
youngest torchbearers of the deep learning revolution and have occupied the forefront of this drive
in more ways than one. First  the adversarial training of deep neural nets has given rise to new
challenges regarding the efﬁcient allocation of parallelizable resources  the compatibility of the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Lipschitz

Lipschitz + Strong





Ergodic

Deterministic
Stochastic

1/

1/t
√
t [13  18]

Last Iterate

Unknown
Unknown



1/t

1/t



Ergodic

Last Iterate

e−ρt [18  25  31]



1/t



Table 1: The best known global convergence rates for single-call extra-gradient methods in monotone VI
problems; logarithmic factors ignored throughout. A box indicates a contribution from this paper.

chosen architectures  etc. Second  the loss landscape in GANs is no longer that of a minimization
problem but that of a zero-sum  min-max game – or  more generally  a variational inequality (VI).
Variational inequalities are a ﬂexible and widely studied framework in optimization which  among
others  incorporates minimization  saddle-point  Nash equilibrium  and ﬁxed point problems. As such 
there is an extensive literature devoted to solving variational inequalities in different contexts; for an
introduction  see [4  17] and references therein. In particular  in the setting of monotone variational
inequalities with Lipschitz continuous operators  it is well known that the optimal rate of convergence
is O(1/t)  and that this rate is achieved by the Extra-Gradient (EG) algorithm of Korpelevich [23]
and its Bregman variant  the Mirror-Prox (MP) algorithm of Nemirovski [32].1
These algorithms require two projections and two oracle calls per iteration  so they are more costly
than standard Forward-Backward / descent methods. As a result  there are two complementary
strands of literature aiming to reduce one (or both) of these cost multipliers – that is  the number of
projections and/or the number of oracle calls per iteration. The ﬁrst class contains algorithms like
the Forward-Backward-Forward (FBF) method of Tseng [43]  while the second focuses on gradient
extrapolation mechanisms like Popov’s modiﬁed Arrow–Hurwicz algorithm [37].
In deep learning  the latter direction has attracted considerably more interest than the former. The
main reason for this is that neural net training often does not involve constraints (and  when it does 
they are relatively cheap to handle). On the other hand  gradient calculations can become very costly 
so a decrease in the number of oracle calls could offer signiﬁcant practical beneﬁts. In view of this 
our aim in this paper is (i) to develop a synthetic approach to methods that retain the anticipatory
properties of the Extra-Gradient algorithm while making a single oracle call per iteration; and (ii) to
derive quantitative convergence results for such single-call extra-gradient (1-EG) algorithms.

Our contributions. Our ﬁrst contribution complements the existing literature (reviewed below and
in Section 3) by showing that the class of 1-EG algorithms under study attains the optimal O(1/t)
convergence rate of the two-call method in deterministic variational inequalities with a monotone 
Lipschitz continuous operator. Subsequently  we show that this rate is also achieved in stochastic
variational inequalities with strongly monotone operators provided that the optimizer has access to an
oracle with bounded variance (but not necessarily bounded second moments).
Importantly  this stochastic result concerns both the method’s “ergodic average” (a weighted average
of the sequence of points generated by the algorithm) as well as its “last iterate” (the last generated
point). The reason for this dual focus is that averaging can be very useful in convex/monotone
landscapes  but it is not as beneﬁcial in non-monotone problems (where Jensen’s inequality does
not apply). On that account  last-iterate convergence results comprise an essential stepping stone for
venturing beyond monotone problems.
Armed with these encouraging results  we then focus on non-monotone problems and show that  with
high probability  the method’s last iterate exhibits a O(1/t) local convergence rate to solutions of
non-monotone variational inequalities that satisfy a second-order sufﬁcient condition. To the best of
our knowledge  this is the ﬁrst convergence rate guarantee of this type for stochastic  non-monotone
variational inequalities.

Related work. The prominence of Extra-Gradient/Mirror-Prox methods in solving variational
inequalities and saddle-point problems has given rise to a vast corpus of literature which we cannot
hope to do justice here. Especially in the context of adversarial networks  there has been a ﬂurry

1Korpelevich [23] proved the method’s asymptotic convergence for pseudomonotone variational inequalities.

The O(1/t) convergence rate was later established by Nemirovski [32] with ergodic averaging.

2

of recent activity relating variants of the Extra-Gradient algorithm to GAN training  see e.g.  [9  14 
18  19  24  28  44] and references therein. For concreteness  we focus here on algorithms with a
single-call structure and refer the reader to Sections 3–5 for additional details.
The ﬁrst variant of Extra-Gradient with a single oracle call per iteration dates back to Popov [37].
This algorithm was subsequently studied by  among others  Chiang et al. [10]  Rakhlin and Sridharan
[38  39] and Gidel et al. [18]; see also [13  25] for a “reﬂected” variant  [14  30  31  36] for an
“optimistic” one  and Section 3 for a discussion of the differences between these variants. In the context
of deterministic  strongly monotone variational inequalities with Lipschitz continuous operators  the
last iterate of the method was shown to exhibit a geometric convergence rate [18  25  31  42]; similar
geometric convergence results also extend to bilinear saddle-point problems [18  36  42]  even though
the operator involved is not strongly monotone. In turn  this implies the convergence of the method’s
ergodic average  but at a O(1/t) rate (because of the hysteresis of the average). In view of this 
the fact that 1-EG methods retain the optimal O(1/t) convergence rate in deterministic variational
inequalities without strong monotonicity assumptions closes an important gap in the literature.2
At the local level  the geometric convergence results discussed above echo a surge of interest in local
convergence guarantees of optimization algorithms applied to games and saddle-point problems 
see e.g.  [1  3  15  24] and references therein. In more detail  Liang and Stokes [24] proved local
geometric convergence for several algorithms in possibly non-monotone saddle-point problems under
a local smoothness condition. In a similar vein  Daskalakis and Panageas [15] analyzed the limit
points of (optimistic) gradient descent  and showed that local saddle points are stable stationary
points; subsequently  Adolphs et al. [1] and Mazumdar et al. [27] proposed a class of algorithms that
eliminate stationary points which are not local Nash equilibria.
Geometric convergence results of this type are inherently deterministic because they rely on an
associated resolvent operator being ﬁrmly nonexpansive – or  equivalently  rely on the use of the
center manifold theorem. In a stochastic setting  these techniques are no longer applicable because
the contraction property cannot be maintained in the presence of noise; in fact  unless the problem at
hand is amenable to variance reduction – e.g.  as in [6  9  21] – geometric convergence is not possible
√
if the noise process is even weakly isotropic. Instead  for monotone problems  Cui and Shanbhag [13]
and Gidel et al. [18] showed that the ergodic average of the method attains a O(1/
t) convergence
rate. Our global convergence results for stochastic variational inequalities improve this rate to O(1/t)
in strongly monotone variational inequalities for both the method’s ergodic average and its last iterate.
In the same light  our local O(1/t) convergence results for non-monotone variational inequalities
provide a key extension of local  deterministic convergence results to a fully stochastic setting  all the
while retaining the fastest convergence rate for monotone variational inequalities.
For convenience  our contributions relative to the state of the art are summarized in Table 1.

2 Problem setup and blanket assumptions

Variational inequalities. We begin by presenting the basic variational inequality framework that
we will consider throughout the sequel. To that end  let X be a nonempty closed convex subset of Rd 
and let V : Rd → Rd be a single-valued operator on Rd. In its most general form  the variational
inequality (VI) problem associated to V and X can be stated as:

(VI)

Find x(cid:63) ∈ X such that (cid:104)V (x(cid:63))  x − x(cid:63)(cid:105) ≥ 0 for all x ∈ X .
To provide some intuition about (VI)  we discuss two important examples below:
Example 1 (Loss minimization). Suppose that V = ∇f for some smooth loss function f on X = Rd.
Then  x(cid:63) ∈ X is a solution to (VI) if and only if ∇f (x(cid:63)) = 0  i.e.  if and only if x(cid:63) is a critical point
of f. Of course  if f is convex  any such solution is a global minimizer.
Example 2 (Min-max optimization). Suppose that X decomposes as X = Θ × Φ with Θ = Rd1 
Φ = Rd2  and assume V = (∇θL −∇φL) for some smooth function L(θ  φ)  θ ∈ Θ  φ ∈ Φ. As in
2A few weeks after the submission of our paper  we were made aware of a very recent preprint by Mokhtari
et al. [30] which also establishes a O(1/t) convergence rate for the algorithm’s “optimistic” variant in saddle-
point problems (in terms of the Nikaido–Isoda gap function). To the best of our knowledge  this is the closest
result to our own in the literature.

3

Example 1 above  the solutions to (VI) correspond to the critical points of L; if  in addition  L is
convex-concave  any solution x(cid:63) = (θ(cid:63)  φ(cid:63)) of (VI) is a global saddle-point  i.e. 
for all θ ∈ Θ and all φ ∈ Φ.

L(θ(cid:63)  φ) ≤ L(θ(cid:63)  φ(cid:63)) ≤ L(θ  φ(cid:63))

Given the original formulation of GANs as (stochastic) saddle-point problems [20]  this observation
has been at the core of a vigorous literature at the interface between optimization  game theory  and
deep learning  see e.g.  [9  14  18  24  28  36  44] and references therein.

The operator analogue of convexity for a function is monotonicity  i.e. 

(cid:104)V (x(cid:48)) − V (x)  x(cid:48) − x(cid:105) ≥ 0

for all x  x(cid:48) ∈ Rd.

Speciﬁcally  when V = ∇f for some sufﬁciently smooth function f  this condition is equivalent to f
being convex [4]. In this case  following Nesterov [34  35] and Juditsky et al. [22]  the quality of a
candidate solution ˆx ∈ X can be assessed via the so-called error (or merit) function

and/or its restricted variant

Err(ˆx) = sup
x∈X

(cid:104)V (x)  ˆx − x(cid:105)

ErrR(ˆx) = max
x∈XR

(cid:104)V (x)  ˆx − x(cid:105) 

where XR ≡ X ∩ BR(0) = {x ∈ X : (cid:107)x(cid:107) ≤ R} denotes the “restricted domain” of the problem.
More precisely  we have the following basic result.
Lemma 1 (Nesterov  2007). Assume V is monotone. If x(cid:63) is a solution of (VI)  we have Err(x(cid:63)) = 0
and ErrR(x(cid:63)) = 0 for all sufﬁciently large R. Conversely  if ErrR(ˆx) = 0 for large enough R > 0
and some ˆx ∈ XR  then ˆx is a solution of (VI).
In light of this result  Err and ErrR will be among our principal measures of convergence in the
sequel.

Blanket assumptions. With all this in hand  we present below the main assumptions that will
underlie the bulk of the analysis to follow.
Assumption 1. The solution set X (cid:63) of (VI) is nonempty.
Assumption 2. The operator V is β-Lipschitz continuous  i.e. 

(cid:107)V (x(cid:48)) − V (x)(cid:107) ≤ β(cid:107)x(cid:48) − x(cid:107)

for all x  x(cid:48) ∈ Rd.

Assumption 3. The operator V is monotone.

In some cases  we will also strengthen Assumption 3 to:
Assumption 3(s). The operator V is α-strongly monotone  i.e. 

(cid:104)V (x(cid:48)) − V (x)  x(cid:48) − x(cid:105) ≥ α(cid:107)x(cid:48) − x(cid:107)2

for some α > 0 and all x  x(cid:48) ∈ Rd.

Throughout our paper  we will be interested in sequences of points Xt ∈ X generated by algorithms
that can access the operator V via a stochastic oracle [33].3 Formally  this is a black-box mechanism
which  when called at Xt ∈ X   returns the estimate

Vt = V (Xt) + Zt 

(1)

where Zt ∈ Rd is an additive noise variable satisfying the following hypotheses:

a) Zero-mean:
b) Finite variance: E[(cid:107)Zt(cid:107)2 | Ft] ≤ σ2.

E[Zt | Ft] = 0.

In the above  Ft denotes the history (natural ﬁltration) of Xt  so Xt is adapted to Ft by deﬁnition; on
the other hand  since the t-th instance of Zt is generated randomly from Xt  Zt is not adapted to Ft.
Obviously  if σ2 = 0  we have the deterministic  perfect feedback case Vt = V (Xt).

3Depending on the algorithm  the sequence index t may take positive integer or half-integer values (or both).

4

3 Algorithms

The Extra-Gradient algorithm.
Extra-Gradient (EG) algorithm of Korpelevich [23] can be stated in recursive form as

In the general framework outlined in the previous section  the

Xt+1/2 = ΠX (Xt − γtVt)
Xt+1 = ΠX (Xt − γtVt+1/2)

(EG)
where ΠX (y) := arg minx∈X(cid:107)y − x(cid:107) denotes the Euclidean projection of y ∈ Rd onto the closed
convex set X and γt > 0 is a variable step-size sequence. Using this formulation as a starting point 
the main idea behind the method can be described as follows: at each t = 1  2  . . .   the oracle is
called at the algorithm’s current – or base – state Xt to generate an intermediate – or leading – state
Xt+1/2; subsequently  the base state Xt is updated to Xt+1 using gradient information from the
leading state Xt+1/2  and the process repeats. Heuristically  the extra oracle call allows the algorithm
to “anticipate” the landscape of V and  in so doing  to achieve improved convergence results relative
to standard projected gradient / forward-backward methods; for a detailed discussion  we refer the
reader to [7  17] and references therein.

Single-call variants of the Extra-Gradient algorithm. Given the signiﬁcant computational over-
head of gradient calculations  a key desideratum is to drop the second oracle call in (EG) while
retaining the algorithm’s “anticipatory” properties. In light of this  we will focus on methods that
perform a single oracle call at the leading state Xt+1/2  but replace the update rule for Xt+1/2 (and 
possibly  Xt as well) with a proxy that compensates for the missing gradient. Concretely  we will
examine the following family of single-call extra-gradient (1-EG) algorithms:

1. Past Extra-Gradient (PEG) [10  18  37]:

Xt+1/2 = ΠX (Xt − γtVt−1/2)
Xt+1 = ΠX (Xt − γtVt+1/2)

[Proxy: use Vt−1/2 instead of Vt in the calculation of Xt+1/2]

2. Reﬂected Gradient (RG) [8  13  25]:

Xt+1/2 = Xt − (Xt−1 − Xt)
Xt+1 = ΠX (Xt − γtVt+1/2)

[Proxy: use (Xt−1 − Xt)/γt instead of Vt in the calculation of Xt+1/2; no projection]

3. Optimistic Gradient (OG) [14  30  31  36]:

Xt+1/2 = ΠX (Xt − γtVt−1/2)
Xt+1 = Xt+1/2 + γtVt−1/2 − γtVt+1/2

(PEG)

(RG)

(OG)

[Proxy: use Vt−1/2 instead of Vt in the calculation of Xt+1/2; use Xt+1/2 + γtVt−1/2 instead
of Xt in the calculation of Xt+1; no projection]

These are the main algorithmic schemes that we will consider  so a few remarks are in order. First 
given the extensive literature on the subject  this list is not exhaustive; see e.g.  [30  31  36] for a
generalization of (OG)  [26] for a variant that employs averaging to update the algorithm’s base state
Xt  and [19] for a proxy deﬁned via “negative momentum”. Nevertheless  the algorithms presented
above appear to be the most widely used single-call variants of (EG)  and they illustrate very clearly
the two principal mechanisms for approximating missing gradients: (i) using past gradients (as in the
PEG and OG variants); and/or (ii) using a difference of successive states (as in the RG variant).
We also take this opportunity to provide some background and clear up some issues on terminology
regarding the methods presented above. First  the idea of using past gradients dates back at least to
Popov [37]  who introduced (PEG) as a “modiﬁed Arrow–Hurwicz” method a few years after the
original paper of Korpelevich [23]; the same algorithm is called “meta” in [10] and “extrapolation
from the past” in [18] (but see also the note regarding optimism below). The terminology “Reﬂected

5

Gradient” and the precise formulation that we use here for (RG) is due to Malitsky [25]. The well-
known primal-dual algorithm of Chambolle and Pock [8] can be seen as a one-sided  alternating
variant of the method for saddle-point problems; see also [44] for a more recent take.
Finally  the terminology “optimistic” is due to Rakhlin and Sridharan [38  39]  who provided a uniﬁed
view of (PEG) and (EG) based on the sequence of oracle vectors used to update the algorithm’s
leading state Xt+1/2.4 Because the framework of [38  39] encompasses two different algorithms 
there is some danger of confusion regarding the use of the term “optimism”; in particular  both (EG)
and (PEG) can be seen as instances of optimism. The speciﬁc formulation of (OG) that we present
here is the projected version of the algorithm considered by Daskalakis et al. [14];5 by contrast  the
“optimistic” method of Mertikopoulos et al. [28] is equivalent to (EG) – not (PEG) or (OG).
The above shows that there can be a broad array of single-call extra-gradients methods depending on
the speciﬁc proxy used to estimate the missing gradient  whether it is applied to the algorithm’s base
or leading state  when (or where) a projection operator is applied  etc. The contact point of all these
algorithms is the unconstrained setting (X = Rd) where they are exactly equivalent:
Proposition 1. Suppose that the 1-EG methods presented above share the same initialization  X0 =
X1 ∈ X   V1/2 = 0  and are run with the same  constant step-size γt ≡ γ for all t ≥ 1. If X = Rd 
the generated iterates Xt coincide for all t ≥ 1.
The proof of this proposition follows by a simple rearrangement of the update rules for (PEG)  (RG)
and (OG)  so we omit it. In the projected case  the 1-EG updates presented above are no longer
equivalent – though  of course  they remain closely related.

4 Deterministic analysis

We begin with the deterministic analysis  i.e.  when the optimizer receives oracle feedback of the
form (1) with σ = 0. In terms of presentation  we keep the global and local cases separated and
we interleave our results for the generated sequence Xt and its ergodic average. To streamline our
presentation  we defer the details of the proofs to the paper’s supplement and only discuss here the
main ideas.

4.1 Global convergence
Our ﬁrst result below shows that the algorithms under study achieve the optimal O(1/t) ergodic
convergence rate in monotone problems with Lipschitz continuous operators.
Theorem 1. Suppose that V satisﬁes Assumptions 1–3. Assume further that a 1-EG algorithm is run
with perfect oracle feedback and a constant step-size γ < 1/(cβ)  where c = 1 +
2 for the RG
variant and c = 2 for the PEG and OG variants. Then  for all R > 0  we have

√

(cid:0) ¯Xt

(cid:1) ≤ R2 + (cid:107)X1 − X1/2(cid:107)2

2γt

where ¯Xt = t−1(cid:80)t

ErrR

s=1 Xs+1/2 is the ergodic average of the algorithm’s sequence of leading states.
This result shows that the EG and 1-EG algorithms share the same convergence rate guarantees  so
we can safely drop one gradient calculation per iteration in the monotone case. The proof of the
theorem is based on the following technical lemma which enables us to treat the different variants of
the 1-EG method in a uniﬁed way.
Lemma 2. Assume that V satisﬁes Assumption 3 (monotonicity). Suppose further that the sequence
(Xt)t∈N/2 of points in Rd satisﬁes the following “quasi-descent” inequality with µs  λs ≥ 0:

(cid:107)Xs+1 − p(cid:107)2 ≤ (cid:107)Xs − p(cid:107)2 − 2λs(cid:104)V (Xs+1/2)  Xs+1/2 − p(cid:105) + µs − µs+1

(3)

4More precisely  Rakhlin and Sridharan [38  39] use the term Optimistic Mirror Descent (OMD) in reference
to the Mirror-Prox method of Nemirovski [32]  itself a variant of (EG) with projections deﬁned by means of a
Bregman function; for a related treatment  see Nesterov [34] and Juditsky et al. [22].
5To see this  note that the difference between two consecutive intermediate steps Xt−1/2 and Xt+1/2 can be
written as Xt+1/2 = ΠX (Xt−1/2 − (γt−1 + γt)Vt−1/2 + γt−1Vt−3/2). Writing (OG) in the form presented
above shows that (OG) can also be viewed as a single-call variant of the FBF method of Tseng [43].

6

for all p ∈ XR and all s ∈ {1  . . .   t}. Then 

(cid:32)(cid:80)t

ErrR

(cid:80)t

s=1 λsXs+1/2

s=1 λs

(cid:33)

2(cid:80)t

≤ R2 + µ1
s=1 λs

.

Remark 1. For Examples 1 and 2 it is possible to state both Theorem 1 and Lemma 2 with more
adapted measures. We refer the readers to the supplement for more details.
The use of Lemma 2 is tailored to time-averaged sequences like ¯Xt  and relies on establishing a
suitable “quasi-descent inequality” of the form (3) for the iterates of 1-EG. Doing this requires in turn
a careful comparison of successive iterates of the algorithm via the Lipschitz continuity assumption
for V ; we defer the precise treatment of this argument to the paper’s supplement.
On the other hand  because the role of averaging is essential in this argument  the convergence of
the algorithm’s last iterate requires signiﬁcantly different techniques. To the best of our knowledge 
there are no comparable convergence rate guarantees for Xt under Assumptions 1–3; however  if
Assumption 3 is strengthened to Assumption 3(s)  the convergence of Xt to the (necessarily unique)
solution of (VI) occurs at a geometric rate. For completeness  we state here a consolidated version of
the geometric convergence results of Malitsky [25]  Gidel et al. [18]  and Mokhtari et al. [31].
Theorem 2. Assume that V satisﬁes Assumptions 1  2 and 3(s)  and let x(cid:63) denote the (necessarily
unique) solution of (VI). If a 1-EG algorithm is run with a sufﬁciently small step-size γ  the generated
sequence Xt converges to x(cid:63) at a rate of (cid:107)Xt − x(cid:63)(cid:107) = O(exp(−ρ t)) for some ρ > 0.

4.2 Local convergence

We continue by presenting a local convergence result for deterministic  non-monotone problems. To
state it  we will employ the following notion of regularity in lieu of Assumptions 1–3 and 3(s).
Deﬁnition 3. We say that x(cid:63) is a regular solution of (VI) if V is C 1-smooth in a neighborhood of x(cid:63)
and the Jacobian JacV (x(cid:63)) is positive-deﬁnite along rays emanating from x(cid:63)  i.e. 

z(cid:62) JacV (x(cid:63))z ≡ d(cid:88)

zi

∂Vi
∂xj

(x(cid:63))zj > 0

i j=1

for all z ∈ Rd \{0} that are tangent to X at x(cid:63).
This notion of regularity is an extension of similar conditions that have been employed in the local
analysis of loss minimization and saddle-point problems. More precisely  if V = ∇f for some
loss function f  this deﬁnition is equivalent to positive-deﬁniteness of the Hessian along qualiﬁed
constraints [5  Chap. 3.2]. As for saddle-point problems and smooth games  variants of this condition
can be found in several different sources  see e.g.  [16  24  29  40  41] and references therein.
Under this condition  we obtain the following local geometric convergence result for 1-EG methods.
Theorem 4. Let x(cid:63) be a regular solution of (VI). If a 1-EG method is run with perfect oracle
feedback and is initialized sufﬁciently close to x(cid:63) with a sufﬁciently small constant step-size we have
(cid:107)Xt − x(cid:63)(cid:107) = O(exp(−ρ t)) for some ρ > 0.
The proof of this theorem relies on showing that (i) V essentially behaves like a smooth  strongly
monotone operator close to x(cid:63); and (ii) if the method is initialized in a small enough neighborhood
of x(cid:63)  it will remain in said neighborhood for all t. As a result  Theorem 4 essentially follows by
“localizing” Theorem 2 to this neighborhood.
As a preamble to our stochastic analysis in the next section  we should state here that  albeit
straightforward  the proof strategy outlined above breaks down if we have access to V only via a
stochastic oracle. In this case  a single “bad” realization of the feedback noise Zt could drive the
process away from the attraction region of any local solution of (VI). For this reason  the stochastic
analysis requires signiﬁcantly different tools and techniques and is considerably more intricate.

5 Stochastic analysis

We now present our analysis for stochastic variational inequalities with oracle feedback of the form (1).
For concreteness  given that the PEG variant of the 1-EG method employs the most straightforward

7

(a) Strongly monotone [1 = 1  2 = 0] 

deterministic  last iterate

(b) Monotone [1 = 0  2 = 1] 
deterministic  ergodic averaging

(c) Non-monotone [1 = 1  2 = −1]  iid
Z ∼ N (0  .01)  last iterate (b = 15)

Figure 1: Illustration of the performance of EG and 1-EG in the (a priori non-monotone) saddle-point problem

L(θ  φ) = 21θ

(cid:62)
A1θ + 2

(cid:0)θ

A2θ(cid:1)2 − 21φ

(cid:62)

B1φ − 2
(cid:62)

(cid:0)φ

B2φ(cid:1)2 + 4θ

(cid:62)

(cid:62)
Cφ

on the full unconstrained space X = Rd = Rd1×d2 with d1 = d2 = 1000 and A1  B1  A2  B2 (cid:31) 0. We choose
three situations representative of the settings considered in the paper: (a) linear convergence of the last iterate of
the deterministic methods in strongly monotone problems; (b) the O(1/t) convergence of the ergodic average
in monotone  deterministic problems; and (c) the O(1/t) local convergence rate of the method’s last iterate in
stochastic  non-monotone problems. For (a) and (b)  the origin is the unique solution of (VI)  and for (c) it is a
regular solution thereof. We observe that 1-EG consistently outperforms EG in terms of oracle calls for a ﬁxed
step-size  and the observed rates are consistent with the rates reported in Table 1.

proxy mechanism  we will focus on this variant throughout; for the other variants  the proofs and
corresponding explicit expressions follow from the same rationale (as in the case of Theorem 1).

5.1 Global convergence

√

As we mentioned in the introduction  under Assumptions 1–3  Cui and Shanbhag [13] and Gidel
et al. [18] showed that 1-EG methods attain a O(1/
t) ergodic convergence rate. By strengthening
Assumption 3 to Assumption 3(s)  we show that this result can be augmented in two synergistic ways:
under Assumptions 1  2 and 3(s)  both the last iterate and the ergodic average of 1-EG achieve a
O(1/t) convergence rate.
Theorem 5. Suppose that V satisﬁes Assumptions 1  2 and 3(s)  and assume that (PEG) is run
with stochastic oracle feedback of the form (1) and a step-size of the form γt = γ/(t + b) for some
γ > 1/α and b ≥ 4βγ. Then  the generated sequence of the algorithm’s base states satisﬁes

while its ergodic average ¯Xt = t−1(cid:80)t

E[(cid:107)Xt − x(cid:63)(cid:107)2] ≤ 6γ2σ2
αγ − 1

1
t

+ o

s=1 Xs enjoys the bound

E[(cid:107) ¯Xt − x(cid:63)(cid:107)2] ≤ 6γ2σ2
αγ − 1

log t

t

+ o

 

(cid:19)
(cid:18) 1
(cid:18) log t

t

t

(cid:19)

.

√
Regarding our proof strategy for the last iterate of the process  we can no longer rely either on a
contraction argument or the averaging mechanism that yields the O(1/
t) ergodic convergence rate.
Instead  we show in the appendix that Xt is (stochastically) quasi-Fejér in the sense of [11  12]; then 
leveraging the method’s speciﬁc step-size  we employ successive numerical sequence estimates to
control the summability error and obtain the O(1/t) rate.

5.2 Local convergence

We proceed to examine the convergence of the method in the stochastic  non-monotone case. Our
main result in this regard is the following.
Theorem 6. Let x(cid:63) be a regular solution of (VI) and ﬁx a tolerance level δ > 0. Suppose further
that (PEG) is run with stochastic oracle feedback of the form (1) and a variable step-size of the form
γt = γ/(t + b) for some γ > 1/α and large enough b. Then:

8

020406080100# Oracle Calls10−1610−1310−1010−710−410−1∥x−x*∥2EG1-EGγ=0.1γ=0.2γ=0.3γ=0.4100101102103# Oracle Calls10−310−210−1100∥x−x*∥2EG1-EGγ=0.2γ=0.4γ=0.6γ=0.8100101102103104# Oracle Calls10−310−210−1∥x−x*∥2EG1-EGγ=0.3γ=0.6γ=0.9γ=1.2(a) There are neighborhoods U and U1 of x(cid:63) in X such that  if X1/2 ∈ U  X1 ∈ U1  the event

E∞ = {Xt+1/2 ∈ U for all t = 1  2  . . .}

occurs with probability at least 1 − δ.
(b) Conditioning on the above  we have:

(cid:18) 1

(cid:19)

 

t

E[(cid:107)Xt − x(cid:63)(cid:107)2 | E∞] ≤ 4γ2(M 2 + σ2)
(αγ − 1)(1 − δ)

1
t

+ o

where M = supx∈U(cid:107)V (x)(cid:107) < ∞ and α = inf x∈U(cid:104)V (x)  x − x(cid:63)(cid:105)/(cid:107)x − x(cid:63)(cid:107)2 > 0.

The ﬁniteness of M and the positivity of α are both consequences of the regularity of x(cid:63) and their
values only depend on the size of the neighborhood U. Taking a larger U would increase the
algorithm’s certiﬁed initialization basin but it would also negatively impact its convergence rate (since
M would increase while α would decrease). Likewise  the neighborhood U1 only depends on the
size of U and  as we explain in the appendix  it sufﬁces to take U1 to be “one fourth” of U.
From the above  it becomes clear that the situation is signiﬁcantly more involved than the correspond-
ing deterministic analysis. This is also reﬂected in the proof of Theorem 6 which requires completely
new techniques  well beyond the straightforward localization scheme underlying Theorem 4. More
precisely  a key step in the proof (which we detail in the appendix) is to show that the iterates of the
method remain close to x(cid:63) for all t with arbitrarily high probability. In turn  this requires showing
that the probability of getting a string of “bad” noise realizations of arbitrary length is controllably
small. Even then however  the global analysis still cannot be localized because conditioning changes
the probability law under which the oracle noise is unbiased. Accounting for this conditional bias
requires a surprisingly delicate probabilistic argument which we also detail in the supplement.

6 Concluding remarks

Our aim in this paper was to provide a synthetic view of single-call surrogates to the Extra-Gradient
algorithm  and to establish optimal convergence rates in a range of different settings – deterministic 
stochastic  and/or non-monotone. Several interesting avenues open up as a result  from extending the
theory to more general Bregman proximal settings  to developing an adaptive version as in the recent
work [2] for two-call methods. We defer these research directions to future work.

Acknowledgments

This work beneﬁted from ﬁnancial support by MIAI Grenoble Alpes (Multidisciplinary Institute in
Artiﬁcial Intelligence). P. Mertikopoulos was partially supported by the French National Research
Agency (ANR) grant ORACLESS (ANR–16–CE33–0004–01) and the EU COST Action CA16228
“European Network for Game Theory” (GAMENET).

References
[1] Adolphs  Leonard  Hadi Daneshmand  Aurelien Lucchi  Thomas Hofmann. 2019. Local saddle point
optimization: a curvature exploitation approach. AISTATS ’19: Proceedings of the 22nd International
Conference on Artiﬁcial Intelligence and Statistics.

[2] Bach  Francis  Kﬁr Y. Levy. 2019. A universal algorithm for variational inequalities adaptive to smoothness

and noise. COLT ’19: Proceedings of the 32nd Annual Conference on Learning Theory.

[3] Balduzzi  David  Sebastien Racaniere  James Martens  Jakob Foerster  Karl Tuyls  Thore Graepel. 2018.
ICML ’18: Proceedings of the 35th International

The mechanics of n-player differentiable games.
Conference on Machine Learning.

[4] Bauschke  Heinz H.  Patrick L. Combettes. 2017. Convex Analysis and Monotone Operator Theory in

Hilbert Spaces. 2nd ed. Springer  New York  NY  USA.

[5] Bertsekas  Dimitri P. 1997. Nonlinear programming. Journal of the Operational Research Society 48(3)

334–334.

[6] Bo¸t  Radu Ioan  Panayotis Mertikopoulos  Mathias Staudigl  Phan Tu Vuong. 2019. Forward-backward-
forward methods with variance reduction for stochastic variational inequalities. https://arxiv.org/
abs/1902.03355.

9

[7] Bubeck  Sébastien. 2015. Convex optimization: Algorithms and complexity. Foundations and Trends in

Machine Learning 8(3-4) 231–358.

[8] Chambolle  Antonin  Thomas Pock. 2011. A ﬁrst-order primal-dual algorithm for convex problems with

applications to imaging. Journal of Mathematical Imaging and Vision 40(1) 120–145.

[9] Chavdarova  Tatjana  Gauthier Gidel  François Fleuret  Simon Lacoste-Julien. 2019. Reducing noise in

GAN training with variance reduced extragradient. https://arxiv.org/abs/1904.08598.

[10] Chiang  Chao-Kai  Tianbao Yang  Chia-Jung Lee  Mehrdad Mahdavi  Chi-Jen Lu  Rong Jin  Shenghuo
Zhu. 2012. Online optimization with gradual variations. COLT ’12: Proceedings of the 25th Annual
Conference on Learning Theory.

[11] Combettes  Patrick L. 2001. Quasi-Fejérian analysis of some optimization algorithms. Dan Butnariu  Yair
Censor  Simeon Reich  eds.  Inherently Parallel Algorithms in Feasibility and Optimization and Their
Applications. Elsevier  New York  NY  USA  115–152.

[12] Combettes  Patrick L.  Jean-Christophe Pesquet. 2015. Stochastic quasi-Fejér block-coordinate ﬁxed point

iterations with random sweeping. SIAM Journal on Optimization 25(2) 1221–1248.

[13] Cui  Shisheng  Uday V. Shanbhag. 2016. On the analysis of reﬂected gradient and splitting methods for
monotone stochastic variational inequality problems. CDC ’16: Proceedings of the 57th IEEE Annual
Conference on Decision and Control.

[14] Daskalakis  Constantinos  Andrew Ilyas  Vasilis Syrgkanis  Haoyang Zeng. 2018. Training GANs with

optimism. ICLR ’18: Proceedings of the 2018 International Conference on Learning Representations.

[15] Daskalakis  Constantinos  Ioannis Panageas. 2018. The limit points of (optimistic) gradient descent in
min-max optimization. NIPS’18: Proceedings of the 31st International Conference on Neural Information
Processing Systems.

[16] Facchinei  Francisco  Christian Kanzow. 2007. Generalized Nash equilibrium problems. 4OR 5(3)

173–210.

[17] Facchinei  Francisco  Jong-Shi Pang. 2003. Finite-Dimensional Variational Inequalities and Complemen-

tarity Problems. Springer Series in Operations Research  Springer.

[18] Gidel  Gauthier  Hugo Berard  Gaëtan Vignoud  Pascal Vincent  Simon Lacoste-Julien. 2019. A variational
inequality perspective on generative adversarial networks. ICLR ’19: Proceedings of the 2019 International
Conference on Learning Representations.

[19] Gidel  Gauthier  Reyhane Askari Hemmat  Mohammad Pezehski  Rémi Le Priol  Gabriel Huang  Simon
Lacoste-Julien  Ioannis Mitliagkas. 2019. Negative momentum for improved game dynamics. AISTATS
’19: Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics.

[20] Goodfellow  Ian J.  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair 
Aaron Courville  Yoshua Bengio. 2014. Generative adversarial nets. NIPS ’14: Proceedings of the 27th
International Conference on Neural Information Processing Systems.

[21] Iusem  Alfredo N.  Alejandro Jofré  Roberto I. Oliveira  Philip Thompson. 2017. Extragradient method with
variance reduction for stochastic variational inequalities. SIAM Journal on Optimization 27(2) 686–724.
[22] Juditsky  Anatoli  Arkadi Semen Nemirovski  Claire Tauvel. 2011. Solving variational inequalities with

stochastic mirror-prox algorithm. Stochastic Systems 1(1) 17–58.

[23] Korpelevich  G. M. 1976. The extragradient method for ﬁnding saddle points and other problems. Èkonom.

i Mat. Metody 12 747–756.

[24] Liang  Tengyuan  James Stokes. 2019. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. AISTATS ’19: Proceedings of the 22nd International Conference on
Artiﬁcial Intelligence and Statistics.

[25] Malitsky  Yura. 2015. Projected reﬂected gradient methods for monotone variational inequalities. SIAM

Journal on Optimization 25(1) 502–520.

[26] Malitsky  Yura. 2019. Golden ratio algorithms for variational inequalities. Mathematical Programming

1–28.

[27] Mazumdar  Eric V  Michael I Jordan  S Shankar Sastry. 2019. On ﬁnding local nash equilibria (and only

local nash equilibria) in zero-sum games. https://arxiv.org/abs/1901.00838.

[28] Mertikopoulos  Panayotis  Bruno Lecouat  Houssam Zenati  Chuan-Sheng Foo  Vijay Chandrasekhar 
Georgios Piliouras. 2019. Optimistic mirror descent in saddle-point problems: Going the extra (gradient)
mile. ICLR ’19: Proceedings of the 2019 International Conference on Learning Representations.

unknown payoff functions. Mathematical Programming 173(1-2) 465–507.

[29] Mertikopoulos  Panayotis  Zhengyuan Zhou. 2019. Learning in games with continuous action sets and
[30] Mokhtari  Aryan  Asuman Ozdaglar  Sarath Pattathil. 2019. Convergence rate of O(1/k) for optimistic
gradient and extra-gradient methods in smooth convex-concave saddle point problems. https://arxiv.
org/pdf/1906.01115.pdf.

10

[31] Mokhtari  Aryan  Asuman Ozdaglar  Sarath Pattathil. 2019. A uniﬁed analysis of extra-gradient and
optimistic gradient methods for saddle point problems: proximal point approach. https://arxiv.org/
abs/1901.08511v2.

[32] Nemirovski  Arkadi Semen. 2004. Prox-method with rate of convergence O(1/t) for variational inequalities
with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM
Journal on Optimization 15(1) 229–251.

[33] Nesterov  Yurii. 2004. Introductory Lectures on Convex Optimization: A Basic Course. No. 87 in Applied

Optimization  Kluwer Academic Publishers.

[34] Nesterov  Yurii. 2007. Dual extrapolation and its applications to solving variational inequalities and related

problems. Mathematical Programming 109(2) 319–344.

[35] Nesterov  Yurii. 2009. Primal-dual subgradient methods for convex problems. Mathematical Programming

120(1) 221–259.

[36] Peng  Wei  Yu-Hong Dai  Hui Zhang  Lizhi Cheng. 2019. Training GANs with centripetal acceleration.

https://arxiv.org/abs/1902.08949.

[37] Popov  Leonid Denisovich. 1980. A modiﬁcation of the Arrow–Hurwicz method for search of saddle

points. Mathematical Notes of the Academy of Sciences of the USSR 28(5) 845–848.

[38] Rakhlin  Alexander  Karthik Sridharan. 2013. Online learning with predictable sequences. COLT ’13:

Proceedings of the 26th Annual Conference on Learning Theory.

[39] Rakhlin  Alexander  Karthik Sridharan. 2013. Optimization  learning  and games with predictable se-
quences. NIPS ’13: Proceedings of the 26th International Conference on Neural Information Processing
Systems.

[40] Ratliff  Lillian J  Samuel A Burden  S Shankar Sastry. 2013. Characterization and computation of local
nash equilibria in continuous games. 2013 51st Annual Allerton Conference on Communication  Control 
and Computing (Allerton). IEEE  917–924.

[41] Rosen  J. B. 1965. Existence and uniqueness of equilibrium points for concave N-person games. Econo-

metrica 33(3) 520–534.

[42] Tseng  Paul. 1995. On linear convergence of iterative methods for the variational inequality problem.

Journal of Computational and Applied Mathematics 60(1-2) 237–252.

[43] Tseng  Paul. 2000. A modiﬁed forward-backward splitting method for maximal monotone mappings.

SIAM Journal on Control and Optimization 38(2) 431–446.

[44] Yadav  Abhay  Sohil Shah  Zheng Xu  David Jacobs  Tom Goldstein. 2018. Stabilizing adversarial nets
with prediction methods. ICLR ’18: Proceedings of the 2018 International Conference on Learning
Representations.

11

,Yu-Guan Hsieh
Franck Iutzeler
Jérôme Malick
Panayotis Mertikopoulos