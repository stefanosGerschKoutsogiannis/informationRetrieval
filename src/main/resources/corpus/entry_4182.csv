2018,Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate,Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for ``overfitted'' / interpolated classifiers appears to be  ubiquitous in high-dimensional data  having been observed in deep networks  kernel machines  boosting and random forests. Their performance is consistently robust  even when the data contain large amounts of label noise. 

Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes  including  geometric simplicial interpolation algorithm and singularly weighted $k$-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in  classification and regression problems. Moreover  the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions.

Finally  this paper suggests a way to explain the phenomenon of adversarial examples  which are seemingly ubiquitous in modern machine learning  and also discusses some connections to kernel machines and random forests in the interpolated regime.,Overﬁtting or perfect ﬁtting? Risk bounds for

classiﬁcation and regression rules that interpolate

Mikhail Belkin

The Ohio State University

Daniel Hsu

Columbia University

Partha P. Mitra

Cold Spring Harbor Laboratory

Abstract

Many modern machine learning models are trained to achieve zero or near-zero
training error in order to obtain near-optimal (but non-zero) test error. This phe-
nomenon of strong generalization performance for “overﬁtted” / interpolated clas-
siﬁers appears to be ubiquitous in high-dimensional data  having been observed in
deep networks  kernel machines  boosting and random forests. Their performance
is consistently robust even when the data contain large amounts of label noise.
Very little theory is available to explain these observations. The vast majority of
theoretical analyses of generalization allows for interpolation only when there is
little or no label noise. This paper takes a step toward a theoretical foundation
for interpolated classiﬁers by analyzing local interpolating schemes  including
geometric simplicial interpolation algorithm and singularly weighted k-nearest
neighbor schemes. Consistency or near-consistency is proved for these schemes in
classiﬁcation and regression problems. Moreover  the nearest neighbor schemes
exhibit optimal rates under some standard statistical assumptions.
Finally  this paper suggests a way to explain the phenomenon of adversarial ex-
amples  which are seemingly ubiquitous in modern machine learning  and also
discusses some connections to kernel machines and random forests in the interpo-
lated regime.

1

Introduction

The central problem of supervised inference is to predict labels of unseen data points from a set
of labeled training data. The literature on this subject is vast  ranging from classical parametric
and non-parametric statistics [48  49] to more recent machine learning methods  such as kernel
machines [39]  boosting [36]  random forests [15]  and deep neural networks [25]. There is a
wealth of theoretical analyses for these methods based on a spectrum of techniques including non-
parametric estimation [46]  capacity control such as VC-dimension or Rademacher complexity [40] 
and regularization theory [42]. In nearly all of these results  theoretical analysis of generalization
requires “what you see is what you get” setup  where prediction performance on unseen test data
is close to the performance on the training data  achieved by carefully managing the bias-variance
trade-off. Furthermore  it is widely accepted in the literature that interpolation has poor statistical
properties and should be dismissed out-of-hand. For example  in their book on non-parametric
statistics  Györﬁ et al. [26  page 21] say that a certain procedure “may lead to a function which
interpolates the data and hence is not a reasonable estimate”.
Yet  this is not how many modern machine learning methods are used in practice. For instance  the
best practice for training deep neural networks is to ﬁrst perfectly ﬁt the training data [35]. The
resulting (zero training loss) neural networks after this ﬁrst step can already have good performance
on test data [53]. Similar observations about models that perfectly ﬁt training data have been

E-mail: mbelkin@cse.ohio-state.edu  djhsu@cs.columbia.edu  mitra@cshl.edu

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

made for other machine learning methods  including boosting [37]  random forests [19]  and kernel
machines [12]. These methods return good classiﬁers even when the training data have high levels of
label noise [12  51  53].
An important effort to show that ﬁtting the training data exactly can under certain conditions be
theoretically justiﬁed is the margins theory for boosting [37] and other margin-based methods [6 
24  28  29  34]. However  this theory lacks explanatory power for the performance of classiﬁers that
perfectly ﬁt noisy labels  when it is known that no margin is present in the data [12  51]. Moreover 
margins theory does not apply to regression and to functions (for regression or classiﬁcation) that
interpolate the data in the classical sense [12].
In this paper  we identify the challenge of providing a rigorous understanding of generalization in
machine learning models that interpolate training data. We take ﬁrst steps towards such a theory by
proposing and analyzing interpolating methods for classiﬁcation and regression with non-trivial risk
and consistency guarantees.

Related work. Many existing forms of generalization analyses face signiﬁcant analytical and
conceptual barriers to being able to explain the success of interpolating methods.

Capacity control. Existing capacity-based bounds (e.g.  VC dimension  fat-shattering dimension 
Rademacher complexity) for empirical risk minimization [3  4  7  28  37] do not give useful
risk bounds for functions with zero empirical risk whenever there is non-negligible label noise.
This is because function classes rich enough to perfectly ﬁt noisy training labels generally have
capacity measures that grow quickly with the number of training data  at least with the existing
notions of capacity [12]. Note that since the training risk is zero for the functions of interest  the
generalization bound must bound their true risk  as it equals the generalization gap (difference
between the true and empirical risk). Whether such capacity-based generalization bounds exist is
open for debate.

Stability. Generalization analyses based on algorithmic stability [8  14] control the difference
between the true risk and the training risk  assuming bounded sensitivity of an algorithm’s output
to small changes in training data. Like standard uses of capacity-based bounds  these approaches
are not well-suited to settings when training risk is identically zero but true risk is non-zero.

Regularization. Many analyses are available for regularization approaches to statistical inverse
problems  ranging from Tikhonov regularization to early stopping [9  16  42  52]. To obtain a
risk bound  these analyses require the regularization parameter  (or some analogous quantity)
to approach zero as the number of data n tends to inﬁnity. However  to get (the minimum norm)
interpolation  we need  ! 0 while n is ﬁxed  causing the bounds to diverge.
Smoothing. There is an extensive literature on local prediction rules in non-parametric statistics [46 
49]. Nearly all of these analyses require local smoothing (to explicitly balance bias and variance)
and thus do not apply to interpolation. (Two exceptions are discussed below.)

Recently  Wyner et al. [51] proposed a thought-provoking explanation for the performance of Ad-
aBoost and random forests in the interpolation regime  based on ideas related to “self-averaging” and
localization. However  a theoretical basis for these ideas is not developed in their work.
There are two important exceptions to the aforementioned discussion of non-parametric methods.
First  the nearest neighbor rule (also called 1-nearest neighbor  in the context of the general family
of k-nearest neighbor rules) is a well-known interpolating classiﬁcation method  though it is not
generally consistent for classiﬁcation (and is not useful for regression when there is signiﬁcant
amount of label noise). Nevertheless  its asymptotic risk can be shown to be bounded above by
twice the Bayes risk [18].1 A second important (though perhaps less well-known) exception is the
non-parametric smoothing method of Devroye et al. [21] based on a singular kernel called the Hilbert
kernel (which is related to Shepard’s method [41]). The resulting estimate of the regression function
interpolates the training data  yet is proved to be consistent for classiﬁcation and regression.
The analyses of the nearest neighbor rule and Hilbert kernel regression estimate are not based on
bounding generalization gap  the difference between the true risk and the empirical risk. Rather  the
true risk is analyzed directly by exploiting locality properties of the prediction rules. In particular  the
1More precisely  the expected risk of the nearest neighbor rule converges to E[2⌘(X)(1  ⌘(X))]  where ⌘
is the regression function; this quantity can be bounded above by 2R⇤(1  R⇤)  where R⇤ is the Bayes risk.

2

prediction at a point depends primarily or entirely on the values of the function at nearby points. This
inductive bias favors functions where local information in a neighborhood can be aggregated to give
an accurate representation of the underlying regression function.

What we do. Our approach to understanding the generalization properties of interpolation methods
is to understand and isolate the key properties of local classiﬁcation  particularly the nearest neighbor
rule. First  we construct and analyze an interpolating function based on multivariate triangulation
and linear interpolation on each simplex (Section 3)  which results in a geometrically intuitive and
theoretically tractable prediction rule. Like nearest neighbor  this method is not statistically consistent 
but  unlike nearest neighbor  its asymptotic risk approaches the Bayes risk as the dimension becomes
large  even when the Bayes risk is far from zero—a kind of “blessing of dimensionality”2. Moreover 
under an additional margin condition the difference between the Bayes risk and our classiﬁer is
exponentially small in the dimension.
A similar ﬁnding holds for regression  as the method is nearly consistent when the dimension is high.
Next  we propose a weighted & interpolated nearest neighbor (wiNN) scheme based on singular
weight functions (Section 4). The resulting function is somewhat less natural than that obtained
by simplicial interpolation  but like the Hilbert kernel regression estimate  the prediction rule is
statistically consistent in any dimension. Interestingly  conditions on the weights to ensure consistency
become less restrictive in higher dimension—another “blessing of dimensionality”. Our analysis
provides the ﬁrst known non-asymptotic rates of convergence to the Bayes risk for an interpolated
predictor  as well as tighter bounds under margin conditions for classiﬁcation. In fact  the rate
achieved by wiNN regression is statistically optimal under a standard minimax setting3.
Our results also suggest an explanation for the phenomenon of adversarial examples [44]  which
are seemingly ubiquitous in modern machine learning. In Section 5  we argue that interpolation
inevitably results in adversarial examples in the presence of any amount of label noise. When these
schemes are consistent or nearly consistent  the set of adversarial examples (where the interpolating
classiﬁer disagrees with the Bayes optimal) has small measure but is asymptotically dense. Our
analysis is consistent with the empirical observations that such examples are difﬁcult to ﬁnd by
random sampling [22]  but are easily discovered using targeted optimization procedures  such as
Projected Gradient Descent [30].
Finally  we discuss the difference between direct and inverse interpolation schemes; and make some
connections to kernel machines  and random forests in (Section 6).
Proofs of the main results  along with an informal discussion of some connections to graph-based
semi-supervised learning  are given in the full version of the paper [11] on arXiv.4

2 Preliminaries

The goal of regression and classiﬁcation is to construct a predictor ˆf given labeled training data
(x1  y1)  . . .   (xn  yn) 2 Rd ⇥ R  that performs well on unseen test data  which are typically assumed
to be sampled from the same distribution as the training data. In this work  we focus on interpolating
methods that construct predictors ˆf satisfying ˆf (xi) = yi for all i = 1  . . .   n.
Algorithms that perfectly ﬁt training data are not common in statistical and machine learning literature.
The prominent exception is the nearest neighbor rule  which is among of the oldest and best-
understood classiﬁcation methods. Given a training set of labeled example  the nearest neighbor rule
predicts the label of a new point x to be the same as that of the nearest point to x within the training
set. Mathematically  the predicted label of x 2 Rd is yi  where i 2 arg mini0=1 ... n kx xi0k. (Here 
k·k always denotes the Euclidean norm.) As discussed above  the classiﬁcation risk of the nearest
neighbor rule is asymptotically bounded by twice the Bayes (optimal) risk [18]. The nearest neighbor
2This does not remove the usual curse of dimensionality  which is similar to the standard analyses of k-NN

and other non-parametric methods.

3An earlier version of this article paper contained a bound with a worse rate of convergence based on a loose
analysis. The subsequent work [13] found that a different Nadaraya-Watson kernel regression estimate (with a
singular kernel) could achieve the optimal convergence rate; this inspired us to seek a tighter analysis of our
wiNN scheme.

4https://arxiv.org/abs/1806.05161

3

rule provides an important intuition that such classiﬁers can (and perhaps should) be constructed
using local information in the feature space.
In this paper  we analyze two interpolating schemes  one based on triangulating and constructing the
simplicial interpolant for the data  and another  based on weighted nearest neighbors with singular
weight function.

2.1 Statistical model and notations
We assume (X1  Y1)  . . .   (Xn  Yn)  (X  Y ) are iid labeled examples from Rd ⇥ [0  1]. Here 
i=1 are the iid training data  and (X  Y ) is an independent test example from the same
((Xi  Yi))n
distribution. Let µ denote the marginal distribution of X  with support denoted by supp(µ);
and let ⌘ : Rd ! R denote the conditional mean of Y given X  i.e.  the function given by
⌘(x) := E(Y | X = x). For (binary) classiﬁcation  we assume the range of Y is {0  1}
(so ⌘(x) = P(Y = 1 | X = x))  and we let f⇤ : Rd !{ 0  1} denote the Bayes opti-
mal classiﬁer  which is deﬁned by f⇤(x) := 1
{⌘(x)>1/2}. This classiﬁer minimizes the risk
{f (X)6=Y }] = P(f (X) 6= Y ) under zero-one loss  while the conditional mean
R0/1(f ) := E[1
function ⌘ minimizes the risk Rsq(g) := E[(g(X)  Y )2] under squared loss.
The goal of our analyses will be to establish excess risk bounds for empirical predictors ( ˆf and ˆ⌘  based
on training data) in terms of their agreement with f⇤ for classiﬁcation and with ⌘ for regression. For
classiﬁcation  the expected risk can be bounded as E[R0/1( ˆf )] R 0/1(f⇤) + P( ˆf (X) 6= f⇤(X)) 
while for regression  the expected mean squared error is precisely E[Rsq(ˆ⌘(X))] = Rsq(⌘) +
E[(ˆ⌘(X)  ⌘(X)2]. Our analyses thus mostly focus on P( ˆf (X) 6= f⇤(X)) and E[(ˆ⌘(X)  ⌘(X))2]
(where the probability and expectations are with respect to both the training data and the test example).

2.2 Smoothness  margin  and regularity conditions

Below we list some standard conditions needed for further development.
(A  ↵)-smoothness (Hölder). For all x  x0 in the support of µ  |⌘(x)  ⌘(x0)| A ·k x  x0k↵.
(B  )-margin condition [31  45]. For all t  0  µ({x 2 Rd : |⌘(x)  1/2| t})  B · t.
h-hard margin condition [32]. For all x in the support of µ  |⌘(x)  1/2| h > 0.
(c0  r0)-regularity [5]. For all 0 < r  r0 and x 2 supp(µ)  (supp(µ)\B(x  r))  c0(B(x  r)) 
where  is the Lebesgue measure on Rd  and B(c  r) := {x 2 Rd : kx  ck  r} denotes the ball
of radius r around c.

The regularity condition from Audibert and Tsybakov [5] is not very restrictive. For example  if
supp(µ) = B(0  1)  then c0 ⇡ 1/2 and r0  1.
Uniform distribution condition.
In what follows  we mostly assume uniform marginal distribution
µ over a certain domain. This is done for the sake of simplicity and is not an essential condition.
For example  in every statement the uniform measure can be substituted (with a potential change of
constants) by an arbitrary measure with density bounded from below.

3

Interpolating scheme based on multivariate triangulation

In this section  we describe and analyze an interpolating scheme based on multivariate triangulation.
Our main interest in this scheme is in its natural geometric properties and the risk bounds for
regression and classiﬁcation which compare favorably to those of the original nearest neighbor rule
(despite the fact that neither is statistically consistent in general).

3.1 Deﬁnition and basic properties
We deﬁne an interpolating function ˆ⌘ : Rd ! R based on training data ((xi  yi))n
i=1 from Rd ⇥R and
a (multivariate) triangulation scheme T . This function is simplicial interpolation [20  27]. We assume
without loss of generality that the (unlabeled) examples x1  . . .   xn span Rd. The triangulation

4

x1
0

0

x2

1

x3

0

1

x1
0

that a linear (afﬁne)

ˆ = yi for i = 1  . . .   d + 1.

degenerate simplices5 with vertices at the unlabeled examples; these simplices intersect only at

the set of unlabeled examples (x(1)  . . .   x(d+1)) that are vertices for a simplex containing x. Let
LT (x) be the corresponding set of labeled examples ((x(1)  y(1))  . . .   (x(d+1)  y(d+1))).6 For any

scheme T partitions the convex hull bC := conv(x1  . . .   xn) of the unlabeled examples into non-
<d-dimensional faces. Each x 2 bC is contained in at least one of these simplices; let UT (x) denote
point x 2 bC  we deﬁne ˆ⌘(x) to be the unique linear interpolation of LT (x) at x (deﬁned below). For
points x /2 bC  we arbitrarily assert UT (x) = LT (x) = ?  and deﬁne ˆ⌘(x) := 1/2.
Recall
interpolation of
(v1  y1)  . . .   (vd+1  yd+1) 2 Rd ⇥ R at a new point
x 2 Rd is given by the system of equations ˆ0 + xT ˆ 
where ( ˆ0  ˆ) are (unique) solutions to the system of
equations ˆ0 + vT
i
The predictions of the plug-in classiﬁer based on
simplicial interpolation are qualitatively very differ-
ent from those of the nearest neighbor rule. This is
true even when restricting attention to a single sim-
plex. Suppose  for example  that ⌘(x) < 1/2 for
all x 2 conv(x1  . . .   xd+1)  so the Bayes classiﬁer
predicts 0 for all x in the simplex. On the other hand 
due to label noise  we may have some yi = 1. Sup-
pose in fact that only yd+1 = 1  while yi = 0 for all
i = 1  . . .   d. In this scenario (depicted in Figure 1
for d = 2)  the nearest neighbor rule (erroneously) predicts 1 on a larger fraction of the simplex
than the plug-in classiﬁer based on ˆ⌘. The difference can be striking in high dimensions: 1/d for
nearest neighbor versus 1/2d for simplicial interpolation in d-dimensional version of Figure 1. This
provides an intuition why  in contrast to the nearest neighbor rule  simplicial interpolation can yield
to classiﬁers that are nearly optimal in high dimensions.
Proposition 3.1. Suppose v1  . . .   vd+1 are vertices of a non-degenerate simplex in Rd  and x
is in their convex hull with barycentric coordinates (w1  . . .   wd+1). The linear interpolation of

x2
x3
Simplicial interpolation
Figure 1: Comparison of nearest neighbor
and simplicial interpolation. Consider three
labeled examples from R2 ⇥{ 0  1}: (x1  0) 
(x2  0)  (x3  1). Depicted in gray are the re-
gions (within conv(x1  x2  x3)) on which the
nearest neighbor classiﬁer and simplicial in-
terpolation classiﬁer predict 1.

Nearest neighbor

(v1  y1)  . . .   (vd+1  yd+1) 2 Rd ⇥ R at x is given byPd+1

i=1 wiyi.

One consequence of Proposition 3.1 for ˆ⌘ is that if x is contained in two adjacent simplices (that
share a <d-dimensional face)  then it does not matter which simplex is used to deﬁne UT (x); the
value of ˆ⌘(x) is the same in any case. Geometrically  we see that the restriction of the interpolating
linear function to a face of the simplex coincides with the interpolating linear function constructed on
a sub-simplex formed by that face. Therefore  we deduce that ˆ⌘ is a piecewise linear and continuous
interpolation of the data (x1  y1)  . . .   (xn  yn) on conv(x1  . . .   xn).
We note that our prediction rule requires only locating the vertices of the simplex containing a
given point  rather than the considerably harder problem of constructing a full triangulation. In fact 
locating the containing simplex in a Delaunay triangulation reduces to solving polynomial-size linear
programs [23]; in contrast  computing the full Delaunay triangulation has complexity exponential in
the (intrinsic) dimension [2].

3.2 Mean squared error
We ﬁrst illustrate the behavior of simplicial interpolation in a simple regression setting. Here 
(X1  Y1)  . . .   (Xn  Yn)  (X  Y ) are iid labeled examples from Rd ⇥ [0  1]. For simplicity  we assume
that µ is the uniform distribution on a full-dimensional compact and convex subset of Rd.
In general  each Yi may deviate from its conditional mean ⌘(Xi) by a non-negligible amount 
and hence any function that interpolates the training data is “ﬁtting noise”. Nevertheless  in high
dimension  the mean squared error of such a function will be quite close to that of the (optimal)
conditional mean function.

5We say a simplex in Rd is non-degenerate if it has non-zero d-dimensional Lebesgue measure.
6Of course  some points x have more than one containing simplex; we will see that the ambiguity in deﬁning

UT (x) and LT (x) for such points is not important.

5

2

1

2

d + 2

A0E[ˆ↵0

T ] +

T ] +

4E[µ(Rd \ bC)] + A2E[ˆ2↵

Theorem 3.2. Assume µ is the uniform distribution on a full-dimensional compact and convex
subset of Rd; ⌘ satisﬁes the (A  ↵)-smoothness condition; and the conditional variance function x 7!
var(Y | X = x) satisﬁes the (A0 ↵ 0)-smoothness condition. Let ˆT := supx2bC diam(conv(UT (x)))
denote the maximum diameter of any simplex in the triangulation T derived from X1  . . .   Xn. Then
2
E[(ˆ⌘(X)  ⌘(X))2] 
d + 2E[(Y  ⌘(X))2].
Corollary 3.3. In addition to the assumptions in Theorem 3.2  assume supp(µ) is a simple polytope
in Rd and T is constructed using Delaunay triangulation. Then lim supn!1 E[(ˆ⌘(X)  ⌘(X))2] 
d+2E[(Y  ⌘(X))2].
3.3 Classiﬁcation risk
We now analyze the statistical risk of the plug-in classiﬁer based on ˆ⌘  given by ˆf (x) := 1
{ˆ⌘(x)>1/2}.
As in Section 3.2  we assume that µ is the uniform distribution on a full-dimensional compact and
convex subset of Rd.
We ﬁrst observe that under the same conditions as Corollary 3.3  the asymptotic excess risk of ˆf is
O(1/pd). Moreover  when the conditional mean function satisﬁes a margin condition  this 1/pd
can be replaced with a quantity that is exponentially small in d  as we show next.
Theorem 3.4. Suppose ⌘ satisﬁes the h-hard margin condition. As above  assume µ is the uniform
distribution on a simple polytope in Rd  and T is constructed using Delaunay triangulation. Further-
more  assume ⌘ is Lipschitz away from the class boundary (i.e.  on {x 2 supp(µ) : |⌘(x)1/2| > 0})
and that the class boundary @ has ﬁnite d1-dimensional volume7. Then  for some absolute constants
c1  c2 > 0 (which may depend on h)  lim supn!1 E[R0/1( ˆf )] R 0/1(f⇤) · (1 + c1ec2d).
Remark 3.5. The asymptotic risk bounds show that the risk of ˆf can be very close to the Bayes risk in
high dimensions  thus exhibiting a certain “blessing of dimensionality". This stands in contrast to the
nearest neighbor rule  whose asymptotic risk does not diminish with the dimension and is bounded
by twice the Bayes risk  2R0/1(f⇤).
4

Interpolating nearest neighbor schemes

In this section  we describe a weighted nearest neighbor scheme that  like the 1-nearest neighbor rule 
interpolates the training data  but is similar to the classical (unweighted) k-nearest neighbor rule in
terms of other properties  including convergence and consistency. (The classical k-nearest neighbor
rule is not generally an interpolating method except when k = 1.)

4.1 Weighted & interpolated nearest neighbors
For a given x 2 Rd  let x(i) be the i-th nearest neighbor of x among the training data ((xi  yi))n
from Rd ⇥ R  and let y(i) be the corresponding label. Let w(x  z) be a function Rd ⇥ Rd ! R. A
weighted nearest neighbor scheme is simply a function of the form

i=1

ˆ⌘(x) := Pk
Pk

i=1 w(x  x(i))y(i)

.

i=1 w(x  x(i))

In what follows  we investigate the properties of interpolating schemes of this type.
We will need two key observations for the analyses of these algorithms.

That implies thatPk

Conditional independence. The ﬁrst key observation is that  under the usual iid sampling assump-
tions on the data  the ﬁrst k nearest neighbors of x are conditionally independent given X(k+1).
i=1 w(x  X(i))Y(i) is a sum of conditionally iid random variables8. Hence 
under a mild condition on w(x  X(i))  we expect them to concentrate around their expected value.
7I.e.  lim✏!0 µ(@ + B(0 ✏ )) = 0  where “+” denotes the Minkowski sum  i.e.  the ✏-neighborhood of @.
8Note that these variables are not independent in the ordering given by the distance to x  but a random

permutation makes them independent.

6

Assuming some smoothness of ⌘  that value is closely related to ⌘(x) = E(Y | X = x)  thus
allowing us to establish bounds and rates.
Interpolation and singular weight functions. The second key point is that ˆ⌘(x) is an interpolating
scheme  provided that w(x  z) has a singularity when z = x. Indeed  it is easily seen that if
limz!x w(x  z) = 1  then limx!xi ˆ⌘(x) = yi. Extending ˆ⌘ continuously to the data points
yields a weighted & interpolated nearest neighbor (wiNN) scheme.

We restrict attention to singular weight functions of the following radial type. Fix a positive integer k
and a decreasing function  : R+ ! R+ with a singularity at zero  (0) = +1. We take

w(x  z) :=  kx  zk

kx  x(k+1)k! .

Concretely  we will consider  that diverge near t = 0 as t 7!  log(t) or t 7! t  > 0.
Remark 4.1. The denominator kx  x(k+1)k in the argument of  is not strictly necessary  but it
allows for convenient normalization in view of the conditional independence of k-nearest neighbors
given x(k+1). Note that the weights depend on the sample and are thus data-adaptive.
Remark 4.2. Although w(x  x(i)) are unbounded for singular weight functions  concentration only
requires certain bounded moments. Geometrically  the volume of the region around the singularity
needs to be small enough. For radial weight functions that we consider  this condition is more easily
satisﬁed in high dimension. Indeed  the volume around the singularity becomes exponentially small
in high dimension.
Our wiNN schemes are related to Nadaraya-Watson kernel regression [33  50]. The use of singular
kernels in the context of interpolation was originally proposed by Shepard [41]; they do not appear
to be commonly used in machine learning and statistics  perhaps due to a view that interpolating
schemes are unlikely to generalize or even be consistent; the non-adaptive Hilbert kernel regression
estimate [21] (essentially  k = n and  = d) is the only exception we know of.

state a risk bound for wiNN schemes

4.2 Mean squared error
We ﬁrst
(X1  Y1)  . . .   (Xn  Yn)  (X  Y ) are iid labeled examples from Rd ⇥ R.
Theorem 4.3. Let ˆ⌘ be a wiNN scheme with singular weight function . Assume the following:
1. µ is the uniform distribution on a compact subset of Rd and satisﬁes the (c0  r0) regularity

in a regression setting.

Here 

condition for some c0 > 0 and r0 > 0.

2. ⌘ satisﬁes the (A  ↵)-smoothness for some A>0 and ↵>0.
3. (t) = t for some 0 << d/ 2.

Let Z0 := (supp(µ))/(B(0  1))  and assume n > 2Z0k/(c0rd
rk+1 n(x0) be the distance from x0 to its (k + 1)st nearest neighbor among X1  . . .   Xn. Then

0). For any x0 2 supp(µ)  let

E⇥(ˆ⌘(X)  ⌘(X))2⇤  A2E[rk+1 n(X)2↵] + ¯2⇣kek/4 +

d

c0(d  2)k⌘ 

where ¯2 := supx2supp(µ) E[(Y  ⌘(x))2 | X = x].
The bound in Theorem 4.3 is stated in terms of the expected distance to the (k + 1)st nearest neighbor
raised to the 2↵ power; this is typically bounded by O((k/n)2↵/d). Choosing k = n2↵/(2↵+d) leads
to a convergence rate of n2↵/(2↵+d)  which is minimax optimal.

4.3 Classiﬁcation risk
We now analyze the statistical risk of the plug-in classiﬁer ˆf (x) = 1
As in Section 3.3  it is straigtforward obtain a risk bound for ˆf under the same conditions as
Theorem 4.3. Choosing k = n2↵/(2↵+d) leads to a convergence rate of n↵/(2↵+d).

{ˆ⌘(x)>1/2} based on ˆ⌘.

7

We now give a more direct analysis  largely based on that of Chaudhuri and Dasgupta [17] for the
standard k-nearest neighbor rule  that leads to improved rates under favorable conditions. Our most
general theorem along these lines is a bit lengthy to state  and hence we defer it to the full version of
the paper. But a simple corollary is as follows.
Corollary 4.4. Let ˆ⌘ be a wiNN scheme with singular weight function   and let ˆf be the correspond-
ing plug-in classiﬁer. Assume the following:

1. µ is the uniform distribution on a compact subset of Rd and satisﬁes the (c0  r0) regularity

condition for some c0 > 0 and r0 > 0.

P( ˆf (X) 6= f⇤(X))  B  + A✓ Z0p

c0 ◆↵/d!

0/Z0. Then for any 0 << 1/2 

2. ⌘ satisﬁes the (A  ↵)-smoothness and (B  )-margin conditions for some A>0  ↵>0  B>0   0.
3. (t) = t for some 0 << d/ 2.
Let Z0 := (supp(µ))/(B(0  1))  and assume k/n < p  c0rd
+ exp 
np

4k 2c0(d  2)
Remark 4.5. For consistency  we set k := n(2+)↵/((2+)↵+d)  and in the bound  we plug-in
p := 2k/n and  := A(Z0p/c0)↵/d. This leads to a convergence rate of n↵/(↵(2+)+d).
Remark 4.6. The factor 1/k in the ﬁnal term in Corollary 4.4 results from an application of Chebyshev
inequality. Under additional moment conditions  which are satisﬁed for certain functions  (e.g. 
(t) =  log(t)) with better-behaved singularity at zero than t  it can be replaced by e⌦(2k).
Additionally  while the condition (t) = t is convenient for analysis  it is sufﬁcient to assume that
 approaches inﬁnity no faster than t.

d

2 ✓1 

k

np◆2! +

.

5 Ubiquity of adversarial examples in interpolated learning

The recently observed phenomenon of adversarial examples [44] in modern machine learning has
drawn a signiﬁcant degree of interest. It turns out that by introducing a small perturbation to the
features of a correctly classiﬁed example (e.g.  by changing an image in a visually imperceptible way
or even by modifying a single pixel [43]) it is nearly always possible to induce neural networks to
mis-classify a given input in a seemingly arbitrary and often bewildering way.
We will now discuss how our analyses  showing that Bayes optimality is compatible with interpolating
the data  provide a possible mechanism for these adversarial examples to arise. Indeed  such examples
are seemingly unavoidable in interpolated learning and  thus  in much of the modern practice. As we
show below  any interpolating inferential procedure must have abundant adversarial examples in the
presence of any amount of label noise. In particular  in consistent on nearly consistent schemes  like
those considered in this paper  while the predictor agrees with the Bayes classiﬁer on the bulk of the
probability distribution  every “incorrectly labeled” training example (i.e.  an example whose label is
different from the output of the Bayes optimal classiﬁer) has a small “basin of attraction” with every
point in the basin misclassiﬁed by the predictor. The total probability mass of these “adversarial”
basins is negligible given enough training data  so that a probability of misclassifying a randomly
chosen point is low. However  assuming non-zero label noise  the union of these adversarial basins
asymptotically is a dense subset of the support for the underlying probability measure and hence
there are misclassiﬁed examples in every open set. This is indeed consistent with the extensive
empirical evidence for neural networks. While their output is observed to be robust to random feature
noise [22]  adversarial examples turn out to be quite difﬁcult to avoid and can be easily found by
targeted optimization methods such as PCG [30]. We conjecture that it may be a general property or
perhaps a weakness of interpolating methods  as some non-interpolating local classiﬁcation rules can
be robust against certain forms of adversarial examples [47].
To substantiate this discussion  we now provide a formal mathematical statement. For simplicity  let
us consider a binary classiﬁcation setting. Let µ be a probability distribution with non-zero density
deﬁned on a compact domain ⌦ ⇢ Rd and assume non-zero label noise everywhere  i.e.  for all x 2 ⌦ 
0 <⌘ (x) < 1  or equivalently  P(f⇤(x) 6= Y | X = x) > 0. Let ˆfn be a consistent interpolating
classiﬁer constructed from n iid sampled data points (e.g.  the classiﬁer constructed in Section 4.3).

8

Let An = {x 2 ⌦ : ˆfn(x) 6= f⇤(x)} be the set of points at which ˆfn disagrees with the Bayes
optimal classiﬁer f⇤; in other words  An is the set of “adversarial examples” for ˆfn. Consistency of ˆf
implies that  with probability one  limn!1 µ(An) = 0 or  equivalently  limn!1 k ˆfn  f⇤kL2
µ = 0.
On the other hand  the following result shows that the sets An are asymptotically dense in ⌦  so that
there is an adversarial example arbitrarily close to any x.
Theorem 5.1. For any ✏> 0 and  2 (0  1)  there exists N 2 N  such that for all n  N  with
probability    every point in ⌦ is within distance 2✏ of the set An.
Proof sketch. Let (X1  Y1)  . . .   (Xn  Yn) be the training data used to construct ˆfn. Fix a ﬁnite
✏-cover of ⌦ with respect to the Euclidean distance. Since ˆfn is interpolating and ⌘ is never
zero nor one  for every i  there is a non-zero probability (over the outcome of the label Yi) that
ˆfn(Xi) = Yi 6= f⇤(Xi); in this case  the training point Xi is an adversarial example for ˆfn. By
choosing n = n(µ  ✏  ) large enough  we can ensure that with probability at least  over the random
draw of the training data  every element of the cover is within distance ✏ of at least one adversarial
example  upon which every point in ⌦ is within distance 2✏ (by triangle inequality) of the same.

µ  it is
A similar argument for regression shows that while an interpolating ˆ⌘ may converge to ⌘ in L2
generally impossible for it to converge in L1 unless there is no label noise. An even more striking
result is that for the Hilbert scheme of Devroye et al.  the regression estimator almost surely does
not converge at any ﬁxed point  even for the simple case of a constant function corrupted by label
noise [21]. This means that with increasing sample size n  at any given point x misclassiﬁcation will
occur an inﬁnite number of times with probability one. We expect similar behavior to hold for the
interpolation schemes presented in this paper.

6 Discussion and connections

In this paper  we considered two types of algorithms  one based on simplicial interpolation and
another based on interpolation by weighted nearest neighbor schemes. It may be useful to think of
nearest neighbor schemes as direct methods  not requiring optimization  while our simplicial scheme
is a simple example of an inverse method  using (local) matrix inversion to ﬁt the data. Most popular
machine learning methods  such as kernel machines  neural networks  and boosting  are inverse
schemes. While nearest neighbor and Nadaraya-Watson methods often show adequate performance 
they are rarely best-performing algorithms in practice. We conjecture that the simplicial interpolation
scheme may provide insights into the properties of interpolating kernel machines and neural networks.
To provide some evidence for this line of thought  we show that in one dimension simplicial in-
terpolation is indeed a special case of interpolating kernel machine. We will brieﬂy sketch the
argument without going into the details. Consider the space H of real-valued functions f with the
H =R (df / dx)2 + 2f 2 dx. This space is a reproducing kernel Hilbert Space correspond-
norm kfk2
ing to the Laplace kernel e|xz|. It can be seen that as  ! 0 the minimum norm interpolant
f⇤ = arg minf2H 8if (xi)=yi kfkH is simply linear interpolation between adjacent points on the line.
Note that this is the same as our simplicial interpolating method.
Interestingly  a version of random forests similar to PERT [19] also produces linear interpolation
in one dimension (in the limit  when inﬁnitely many trees are sampled). For simplicity assume
that we have only two data points x1 < x2 with labels 0 and 1 respectively. A tree that correctly
{x>t}  where t 2 [x1  x2). Choosing a
classiﬁes those points is simply a function of the form 1
random t uniformly from [x1  x2)  we observe that Et2[x1 x2] 1
{x>t} is simply the linear function
interpolating between the two data points. The extension of this argument to more than two data
points in dimension one is straightforward. It would be interesting to investigate the properties of
such methods in higher dimension. We note that it is unclear whether a random forest method of
this type should be considered a direct or inverse method. While there is no explicit optimization
involved  sampling is often used instead of optimization in methods like simulated annealing.
Finally  we note that while kernel machines (which can be viewed as two-layer neural networks) are
much more theoretically tractable than general neural networks  none of the current theory applies in
the interpolated regime in the presence of label noise [12]. We hope that simplicial interpolation can
shed light on their properties and lead to better understanding of modern inferential methods.

9

Acknowledgements

We would like to thank Raef Bassily  Luis Rademacher  Sasha Rakhlin  and Yusu Wang for conversa-
tions and valuable comments. We acknowledge funding from NSF. DH acknowledges support from
NSF grants CCF-1740833 and DMR-1534910. PPM acknowledges support from the Crick-Clay
Professorship (CSHL) and H N Mahabala Chair (IITM). This work grew out of discussions origi-
nating at the Simons Institute for the Theory of Computing in 2017  and we thank the Institute for
the hospitality. PPM and MB thank ICTS (Bangalore) for their hospitality at the 2017 workshop on
Statistical Physics Methods in Machine Learning.

References
[1] Fernando Affentranger and John A Wieacker. On the convex hull of uniform random points in a

simpled-polytope. Discrete & Computational Geometry  6(3):291–305  1991.

[2] Nina Amenta  Dominique Attali  and Olivier Devillers. Complexity of delaunay triangulation for
points on lower-dimensional polyhedra. In Proceedings of the Eighteenth Annual ACM-SIAM
Symposium on Discrete Algorithms  SODA ’07  pages 1106–1113  2007.

[3] Martin Anthony and Peter L Bartlett. Function learning from interpolation. In Computational
Learning Theory: Second European Conference  EUROCOLT 95  Barcelona Spain  March
1995  Proceedings  pages 211–221  1995.

[4] Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations.

Cambridge University Press  1999.

[5] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classiﬁers. The

Annals of statistics  35(2):608–633  2007.

[6] Peter Bartlett  Dylan J Foster  and Matus Telgarsky. Spectrally-normalized margin bounds for

neural networks. In NIPS  2017.

[7] Peter L Bartlett  Philip M Long  and Robert C Williamson. Fat-shattering and the learnability

of real-valued functions. Journal of Computer and System Sciences  52(3):434–452  1996.

[8] Raef Bassily  Kobbi Nissim  Adam Smith  Thomas Steinke  Uri Stemmer  and Jonathan Ullman.
Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM
symposium on Theory of Computing  pages 1046–1059. ACM  2016.

[9] Frank Bauer  Sergei Pereverzev  and Lorenzo Rosasco. On regularization algorithms in learning

theory. Journal of complexity  23(1):52–72  2007.

[10] Mikhail Belkin  Irina Matveeva  and Partha Niyogi. Regularization and semi-supervised
learning on large graphs. In International Conference on Computational Learning Theory 
pages 624–638. Springer  2004.

[11] Mikhail Belkin  Daniel Hsu  and Partha Mitra. Overﬁtting or perfect ﬁtting? risk bounds for
classiﬁcation and regression rules that interpolate. arXiv preprint arXiv:1806.05161  2018.
URL https://arxiv.org/abs/1806.05161.

[12] Mikhail Belkin  Siyuan Ma  and Soumik Mandal. To understand deep learning we need to
understand kernel learning. In Proceedings of the 35th International Conference on Machine
Learning  pages 541–549  2018.

[13] Mikhail Belkin  Alexander Rakhlin  and Alexandre B. Tsybakov. Does data interpolation

contradict statistical optimality? arXiv preprint arXiv:1806.09471  2018.

[14] Olivier Bousquet and André Elisseeff. Stability and generalization. J. Mach. Learn. Res.  2:

499–526  March 2002. ISSN 1532-4435.

[15] Leo Breiman. Random forests. Machine learning  45(1):5–32  2001.
[16] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

10

[17] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classiﬁ-

cation. In Advances in Neural Information Processing Systems  pages 3437–3445  2014.

[18] Thomas Cover and Peter Hart. Nearest neighbor pattern classiﬁcation. IEEE transactions on

information theory  13(1):21–27  1967.

[19] Adele Cutler and Guohua Zhao. Pert-perfect random tree ensembles. Computing Science and

Statistics  33:490–497  2001.

[20] Scott Davies. Multidimensional triangulation and interpolation for reinforcement learning. In

Advances in Neural Information Processing Systems  pages 1005–1011  1997.

[21] Luc Devroye  Laszlo Györﬁ  and Adam Krzy˙zak. The hilbert kernel regression estimate. Journal

of Multivariate Analysis  65(2):209–227  1998.

[22] Alhussein Fawzi  Seyed-Mohsen Moosavi-Dezfooli  and Pascal Frossard. Robustness of
classiﬁers: from adversarial to random noise. In Advances in Neural Information Processing
Systems  pages 1632–1640  2016.

[23] Komei Fukuda. Polyhedral computation FAQ. Technical report  Swiss Federal Institute of
Technology  Lausanne and Zurich  Switzerland  2004. URL https://www.cs.mcgill.ca/
~fukuda/download/paper/polyfaq.pdf.

[24] Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity of

neural networks. In Thirty-First Annual Conference on Learning Theory  2018.

[25] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016.

[26] László Györﬁ  Michael Kohler  Adam Krzyzak  and Harro Walk. A Distribution-Free Theory of

Nonparametric Regression. Springer series in statistics. Springer  2002.

[27] John H Halton. Simplicial multivariable linear interpolation. Technical Report TR91-002 

University of North Carolina at Chapel Hill  Department of Computer Science  1991.

[28] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the

generalization error of combined classiﬁers. Annals of Statistics  pages 1–50  2002.

[29] Tengyuan Liang  Tomaso Poggio  Alexander Rakhlin  and James Stokes. Fisher-rao metric 

geometry  and complexity of neural networks. arXiv preprint arXiv:1711.01530  2017.

[30] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations  2018.

[31] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of

Statistics  27(6):1808–1829  1999.

[32] Pascal Massart and Élodie Nédélec. Risk bounds for statistical learning. The Annals of Statistics 

34(5):2326–2366  2006.

[33] Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications  9(1):

141–142  1964.

[34] Behnam Neyshabur  Srinadh Bhojanapalli  and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on
Learning Representations  2018.

[35] Ruslan Salakhutdinov. Deep learning tutorial at the Simons Institute  Berkeley  2017. URL

https://simons.berkeley.edu/talks/ruslan-salakhutdinov-01-26-2017-1.

[36] Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press  2012.

[37] Robert E Schapire  Yoav Freund  Peter Bartlett  and Wee Sun Lee. Boosting the margin: a new

explanation for the effectiveness of voting methods. Ann. Statist.  26(5)  1998.

11

[38] Rolf Schneider. Discrete aspects of stochastic geometry. Handbook of discrete and computa-

tional geometry  page 255  2004.

[39] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2001.

[40] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[41] Donald Shepard. A two-dimensional interpolation function for irregularly-spaced data. In

Proceedings of the 1968 23rd ACM national conference  1968.

[42] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business

Media  2008.

[43] Jiawei Su  Danilo Vasconcellos Vargas  and Sakurai Kouichi. One pixel attack for fooling deep

neural networks. arXiv preprint arXiv:1710.08864  2017.

[44] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Good-
fellow  and Rob Fergus. Intriguing properties of neural networks. In International Conference
on Learning Representations  2014.

[45] Alexander B Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of

Statistics  32(1):135–166  2004.

[46] Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics.

Springer  New York  2009.

[47] Yizhen Wang  Somesh Jha  and Kamalika Chaudhuri. Analyzing the robustness of nearest
neighbors to adversarial examples. In Proceedings of the 35th International Conference on
Machine Learning  pages 5133–5142  2018.

[48] Larry Wasserman. All of statistics. Springer  2004.
[49] Larry Wasserman. All of nonparametric statistics. Springer  2006.
[50] Geoffrey S Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics 

Series A  pages 359–372  1964.

[51] Abraham J Wyner  Matthew Olson  Justin Bleich  and David Mease. Explaining the success of
adaboost and random forests as interpolating classiﬁers. Journal of Machine Learning Research 
18(48):1–33  2017.

[52] Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. On early stopping in gradient descent

learning. Constructive Approximation  26(2):289–315  2007.

[53] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning
Representations  2017.

[54] Xiaojin Zhu  Zoubin Ghahramani  and John D Lafferty. Semi-supervised learning using gaussian
ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03)  pages 912–919  2003.

12

,Kari Rantanen
Antti Hyttinen
Matti Järvisalo
Mikhail Belkin
Daniel Hsu
Partha Mitra