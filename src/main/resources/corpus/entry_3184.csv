2015,Learning with Symmetric Label Noise: The Importance of Being Unhinged,Convex potential minimisation is the de facto approach to binary classification. However  Long and Servedio [2008] proved that under symmetric label noise (SLN)  minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper  we propose a convex  classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss  where one does not clamp at zero; hence  we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM  and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.,Learning with Symmetric Label Noise: The

Importance of Being Unhinged

Brendan van Rooyen∗ †

Aditya Krishna Menon† ∗

Robert C. Williamson∗ †

∗The Australian National University

†National ICT Australia

{ brendan.vanrooyen  aditya.menon  bob.williamson }@nicta.com.au

Abstract

Convex potential minimisation is the de facto approach to binary classiﬁcation.
However  Long and Servedio [2010] proved that under symmetric label noise
(SLN)  minimisation of any convex potential over a linear function class can re-
sult in classiﬁcation performance equivalent to random guessing. This ostensibly
shows that convex losses are not SLN-robust. In this paper  we propose a convex 
classiﬁcation-calibrated loss and prove that it is SLN-robust. The loss avoids the
Long and Servedio [2010] result by virtue of being negatively unbounded. The
loss is a modiﬁcation of the hinge loss  where one does not clamp at zero; hence 
we call it the unhinged loss. We show that the optimal unhinged solution is equiv-
alent to that of a strongly regularised SVM  and is the limiting solution for any
convex potential; this implies that strong (cid:96)2 regularisation makes most standard
learners SLN-robust. Experiments conﬁrm the unhinged loss’ SLN-robustness is
borne out in practice. So  with apologies to Wilde [1895]  while the truth is rarely
pure  it can be simple.

1 Learning with symmetric label noise

Binary classiﬁcation is the canonical supervised learning problem. Given an instance space X  and
samples from some distribution D over X × {±1}  the goal is to learn a scorer s : X → R with low
misclassiﬁcation error on future samples drawn from D. Our interest is in the more realistic scenario
where the learner observes samples from some corruption D of D  where labels have some constant
probability of being ﬂipped  and the goal is still to perform well with respect to D. This problem is
known as learning from symmetric label noise (SLN learning) [Angluin and Laird  1988].
Long and Servedio [2010] showed that there exist linearly separable D where  when the learner
observes some corruption D with symmetric label noise of any nonzero rate  minimisation of any
convex potential over a linear function class results in classiﬁcation performance on D that is equiv-
alent to random guessing. Ostensibly  this establishes that convex losses are not “SLN-robust” and
motivates the use of non-convex losses [Stempfel and Ralaivola  2009  Masnadi-Shirazi et al.  2010 
Ding and Vishwanathan  2010  Denchev et al.  2012  Manwani and Sastry  2013].
In this paper  we propose a convex loss and prove that it is SLN-robust. The loss avoids the result
of Long and Servedio [2010] by virtue of being negatively unbounded. The loss is a modiﬁca-
tion of the hinge loss where one does not clamp at zero; thus  we call it the unhinged loss. This
loss has several appealing properties  such as being the unique convex loss satisfying a notion of
“strong” SLN-robustness (Proposition 5)  being classiﬁcation-calibrated (Proposition 6)  consistent
when minimised on D (Proposition 7)  and having an simple optimal solution that is the difference
of two kernel means (Equation 8). Finally  we show that this optimal solution is equivalent to that of
a strongly regularised SVM (Proposition 8)  and any twice-differentiable convex potential (Proposi-
tion 9)  implying that strong (cid:96)2 regularisation endows most standard learners with SLN-robustness.

1

The classiﬁer resulting from minimising the unhinged loss is not new [Devroye et al.  1996  Chap-
ter 10]  [Sch¨olkopf and Smola  2002  Section 1.2]  [Shawe-Taylor and Cristianini  2004  Section
5.1]. However  establishing this classiﬁer’s (strong) SLN-robustness  uniqueness thereof  and its
equivalence to a highly regularised SVM solution  to our knowledge is novel.

2 Background and problem setup
Fix an instance space X. We denote by D a distribution over X × {±1}  with random variables
(X  Y) ∼ D. Any D may be expressed via the class-conditionals (P  Q) = (P(X | Y = 1)  P(X |
Y = −1)) and base rate π = P(Y = 1)  or via the marginal M = P(X) and class-probability
function η : x (cid:55)→ P(Y = 1 | X = x). We interchangeably write D as DP Q π or DM η.

2.1 Classiﬁers  scorers  and risks
A scorer is any function s : X → R. A loss is any function (cid:96) : {±1} × R → R. We use (cid:96)−1  (cid:96)1 to
refer to (cid:96)(−1 ·) and (cid:96)(1 ·). The (cid:96)-conditional risk L(cid:96) : [0  1] × R → R is deﬁned as L(cid:96) : (η  v) (cid:55)→
η · (cid:96)1(v) + (1 − η) · (cid:96)−1(v). Given a distribution D  the (cid:96)-risk of a scorer s is deﬁned as

LD
(cid:96) (s) .= E

(X Y)∼D

[(cid:96)(Y  s(X))]  

(1)

(cid:96) (s) = E
X∼M

[L(cid:96)(η(X)  s(X))]. For a set S  LD

so that LD
(cid:96) (S) is the set of (cid:96)-risks for all scorers in S.
A function class is any F ⊆ RX. Given some F  the set of restricted Bayes-optimal scorers for a
loss (cid:96) are those scorers in F that minimise the (cid:96)-risk:

.= Argmin
The set of (unrestricted) Bayes-optimal scorers is SD ∗
(cid:96)-regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer:

LD
(cid:96) (s).
(cid:96) = SD F ∗

s∈F

(cid:96)

(cid:96)

SD F ∗

for F = RX. The restricted

regretD F

(cid:96)

(s) .= LD

(cid:96) (s) − inf
t∈F

LD
(cid:96) (t).

Binary classiﬁcation is concerned with the zero-one loss  (cid:96)01 : (y  v) (cid:55)→ (cid:74)yv < 0(cid:75) + 1

2(cid:74)v = 0(cid:75).
A loss (cid:96) is classiﬁcation-calibrated if all its Bayes-optimal scorers are also optimal for zero-one
loss: (∀D) SD ∗
01 . A convex potential is any loss (cid:96) : (y  v) (cid:55)→ φ(yv)  where φ : R → R+ is
convex  non-increasing  differentiable with φ(cid:48)(0) < 0  and φ(+∞) = 0 [Long and Servedio  2010 
Deﬁnition 1]. All convex potentials are classiﬁcation-calibrated [Bartlett et al.  2006  Theorem 2.1].

(cid:96) ⊆ SD ∗

2.2 Learning with symmetric label noise (SLN learning)

The problem of learning with symmetric label noise (SLN learning) is the following [Angluin and
Laird  1988  Kearns  1998  Blum and Mitchell  1998  Natarajan et al.  2013]. For some notional
“clean” distribution D  which we would like to observe  we instead observe samples from some
corrupted distribution SLN(D  ρ)  for some ρ ∈ [0  1/2). The distribution SLN(D  ρ) is such that
the marginal distribution of instances is unchanged  but each label is independently ﬂipped with
probability ρ. The goal is to learn a scorer from these corrupted samples such that LD
01(s) is small.
For any quantity in D  we denote its corrupted counterparts in SLN(D  ρ) with a bar  e.g. M for
the corrupted marginal distribution  and η for the corrupted class-probability function; additionally 
when ρ is clear from context  we will occasionally refer to SLN(D  ρ) by D. It is easy to check that
the corrupted marginal distribution M = M  and [Natarajan et al.  2013  Lemma 7]

(∀x ∈ X) η(x) = (1 − 2ρ) · η(x) + ρ.

(2)

3 SLN-robustness: formalisation

We consider learners ((cid:96)  F) for a loss (cid:96) and a function class F  with learning being the search for
some s ∈ F that minimises the (cid:96)-risk. Informally  ((cid:96)  F) is “robust” to symmetric label noise (SLN-
robust) if minimising (cid:96) over F gives the same classiﬁer on both the clean distribution D  which

2

the learner would like to observe  and SLN(D  ρ) for any ρ ∈ [0  1/2)  which the learner actually
observes. We now formalise this notion  and review what is known about SLN-robust learners.

3.1 SLN-robust learners: a formal deﬁnition
For some ﬁxed instance space X  let ∆ denote the set of distributions on X×{±1}. Given a notional
“clean” distribution D  Nsln : ∆ → 2∆ returns the set of possible corrupted versions of D the learner
may observe  where labels are ﬂipped with unknown probability ρ:
1
2

SLN(D  ρ) | ρ ∈
Equipped with this  we deﬁne our notion of SLN-robustness.
Deﬁnition 1 (SLN-robustness). We say that a learner ((cid:96)  F) is SLN-robust if
01(SD F ∗

(∀D ∈ ∆) (∀D ∈ Nsln(D)) LD

Nsln : D (cid:55)→

01(SD F ∗

(cid:96)

) = LD

(cid:19)(cid:27)

0 

.

).

(cid:96)

(3)

(cid:26)

(cid:20)

That is  SLN-robustness requires that for any level of label noise in the observed distribution D  the
classiﬁcation performance (wrt D) of the learner is the same as if the learner directly observes D.
Unfortunately  a widely adopted class of learners is not SLN-robust  as we will now see.

3.2 Convex potentials with linear function classes are not SLN-robust
Fix X = Rd  and consider learners with a convex potential (cid:96)  and a function class of linear scorers

Flin = {x (cid:55)→ (cid:104)w  x(cid:105) | w ∈ Rd}.

This captures e.g. the linear SVM and logistic regression  which are widely studied in theory and
applied in practice. Disappointingly  these learners are not SLN-robust: Long and Servedio [2010 
Theorem 2] give an example where  when learning under symmetric label noise  for any convex
potential (cid:96)  the corrupted (cid:96)-risk minimiser over Flin has classiﬁcation performance equivalent to
random guessing on D. This implies that ((cid:96)  Flin) is not SLN-robust1 as per Deﬁnition 1.
Proposition 1 (Long and Servedio [2010  Theorem 2]). Let X = Rd for any d ≥ 2. Pick any convex
potential (cid:96). Then  ((cid:96)  Flin) is not SLN-robust.

3.3 The fallout: what learners are SLN-robust?

In light of Proposition 1  there are two ways to proceed in order to obtain SLN-robust learners: either
we change the class of losses (cid:96)  or we change the function class F.
The ﬁrst approach has been pursued in a large body of work that embraces non-convex losses
[Stempfel and Ralaivola  2009  Masnadi-Shirazi et al.  2010  Ding and Vishwanathan  2010 
Denchev et al.  2012  Manwani and Sastry  2013]. While such losses avoid the conditions of Proposi-
tion 1  this does not automatically imply that they are SLN-robust when used with Flin. In Appendix
B  we present evidence that some of these losses are in fact not SLN-robust when used with Flin.
The second approach is to consider suitably rich F that contains the Bayes-optimal scorer for D 
e.g. by employing a universal kernel. With this choice  one can still use a convex potential loss  and
in fact  owing to Equation 2  any classiﬁcation-calibrated loss.
Proposition 2. Pick any classiﬁcation-calibrated (cid:96). Then  ((cid:96)  RX) is SLN-robust.
Both approaches have drawbacks. The ﬁrst approach has a computational penalty  as it requires
optimising a non-convex loss. The second approach has a statistical penalty  as estimation rates
with a rich F will require a larger sample size. Thus  it appears that SLN-robustness involves a
computational-statistical tradeoff. However  there is a variant of the ﬁrst option: pick a loss that is
convex  but not a convex potential. Such a loss would afford the computational and statistical ad-
vantages of minimising convex risks with linear scorers. Manwani and Sastry [2013] demonstrated
that square loss  (cid:96)(y  v) = (1 − yv)2  is one such loss. We will show that there is a simpler loss that
is convex and SLN-robust  but is not in the class of convex potentials by virtue of being negatively
unbounded. To derive this loss  we ﬁrst re-interpret robustness via a noise-correction procedure.

1Even if we were content with a difference of  ∈ [0  1/2] between the clean and corrupted minimisers’

performance  Long and Servedio [2010  Theorem 2] implies that in the worst case  = 1/2.

3

4 A noise-corrected loss perspective on SLN-robustness

We now re-express SLN-robustness to reason about optimal scorers on the same distribution  but
with two different losses. This will help characterise a set of “strongly SLN-robust” losses.

4.1 Reformulating SLN-robustness via noise-corrected losses
Given any ρ ∈ [0  1/2)  Natarajan et al. [2013  Lemma 1] showed how to associate with a loss (cid:96) a
noise-corrected counterpart (cid:96) such that LD
Deﬁnition 2 (Noise-corrected loss). Given any loss (cid:96) and ρ ∈ [0  1/2)  the noise-corrected loss (cid:96) is

(s). The loss (cid:96) is deﬁned as follows.

(cid:96) (s) = LD

(cid:96)

(∀y ∈ {±1}) (∀v ∈ R) (cid:96)(y  v) =

(1 − ρ) · (cid:96)(y  v) − ρ · (cid:96)(−y  v)

1 − 2ρ

.

(4)

Since (cid:96) depends on the unknown parameter ρ  it is not directly usable to design an SLN-robust
learner. Nonetheless  it is a useful theoretical device  since  by construction  for any F  SD F ∗
=
SD F ∗
= SD F ∗
.
(cid:96)
(cid:96)
Ghosh et al. [2015  Theorem 1] proved a sufﬁcient condition on (cid:96) such that this holds  namely 

. This means that a sufﬁcient condition for ((cid:96)  F) to be SLN-robust is for SD F ∗

(cid:96)

(cid:96)

(∃C ∈ R)(∀v ∈ R) (cid:96)1(v) + (cid:96)−1(v) = C.

(5)

Interestingly  Equation 5 is necessary for a stronger notion of robustness  which we now explore.

4.2 Characterising a stronger notion of SLN-robustness

As the ﬁrst step towards a stronger notion of robustness  we rewrite (with a slight abuse of notation)

LD
(cid:96) (s) = E

(X Y)∼D

[(cid:96)(Y  s(X))] =

E

(Y S)∼R(D s)

[(cid:96)(Y  S)] .= L(cid:96)(R(D  s)) 

where R(D  s) is a distribution over labels and scores. Standard SLN-robustness requires that label
noise does not change the (cid:96)-risk minimisers  i.e. that if s is such that L(cid:96)(R(D  s)) ≤ L(cid:96)(R(D  s(cid:48)))
for all s(cid:48)  the same relation holds with D in place of D. Strong SLN-robustness strengthens this
notion by requiring that label noise does not affect the ordering of all pairs of joint distributions over
labels and scores. (This of course trivially implies SLN-robustness.) As with the deﬁnition of D 
given a distribution R over labels and scores  let R be the corresponding distribution where labels
are ﬂipped with probability ρ. Strong SLN-robustness can then be made precise as follows.
Deﬁnition 3 (Strong SLN-robustness). Call a loss (cid:96) strongly SLN-robust if for every ρ ∈ [0  1/2) 

(∀R  R(cid:48)) L(cid:96)(R) ≤ L(cid:96)(R(cid:48)) ⇐⇒ L(cid:96)(R) ≤ L(cid:96)(R(cid:48)).

We now re-express strong SLN-robustness using a notion of order equivalence of loss pairs  which
simply requires that two losses order all distributions over labels and scores identically.
Deﬁnition 4 (Order equivalent loss pairs). Call a pair of losses ((cid:96)  ˜(cid:96)) order equivalent if

(∀R  R(cid:48)) L(cid:96)(R) ≤ L(cid:96)(R(cid:48)) ⇐⇒ L˜(cid:96)(R) ≤ L˜(cid:96)(R(cid:48)).

(cid:96)

= SD F ∗

Clearly  order equivalence of ((cid:96)  (cid:96)) implies SD F ∗
  which in turn implies SLN-robustness.
It is thus not surprising that we can relate order equivalence to strong SLN-robustness of (cid:96).
Proposition 3. A loss (cid:96) is strongly SLN-robust iff for every ρ ∈ [0  1/2)  ((cid:96)  (cid:96)) are order equivalent.
This connection now lets us exploit a classical result in decision theory about order equivalent losses
being afﬁne transformations of each other. Combined with the deﬁnition of (cid:96)  this lets us conclude
that the sufﬁcient condition of Equation 5 is also necessary for strong SLN-robustness of (cid:96).
Proposition 4. A loss (cid:96) is strongly SLN-robust if and only if it satisﬁes Equation 5.

(cid:96)

We now return to our original goal  which was to ﬁnd a convex (cid:96) that is SLN-robust for Flin (and
ideally more general function classes). The above suggests that to do so  it is reasonable to consider
those losses that satisfy Equation 5. Unfortunately  it is evident that if (cid:96) is convex  non-constant  and
bounded below by zero  then it cannot possibly be admissible in this sense. But we now show that
removing the boundedness restriction allows for the existence of a convex admissible loss.

4

5 The unhinged loss: a convex  strongly SLN-robust loss

Consider the following simple  but non-standard convex loss:

(cid:96)unh
1

(v) = 1 − v and (cid:96)unh−1 (v) = 1 + v.

Compared to the hinge loss  the loss does not clamp at zero  i.e. it does not have a hinge. (Thus  pecu-
liarly  it is negatively unbounded  an issue we discuss in §5.3.) Thus  we call this the unhinged loss2.
The loss has a number of attractive properties  the most immediate being is its SLN-robustness.

5.1 The unhinged loss is strongly SLN-robust

1

Since (cid:96)unh
(v) + (cid:96)unh−1 (v) = 2  Proposition 4 implies that (cid:96)unh is strongly SLN-robust  and thus that
((cid:96)unh  F) is SLN-robust for any F. Further  the following uniqueness property is not hard to show.
Proposition 5. Pick any convex loss (cid:96). Then 
(∃C ∈ R) (cid:96)1(v) + (cid:96)−1(v) = C ⇐⇒ (∃A  B  D ∈ R) (cid:96)1(v) = −A · v + B  (cid:96)−1(v) = A · v + D.
That is  up to scaling and translation  (cid:96)unh is the only convex loss that is strongly SLN-robust.

Returning to the case of linear scorers  the above implies that ((cid:96)unh  Flin) is SLN-robust. This does
not contradict Proposition 1  since (cid:96)unh is not a convex potential as it is negatively unbounded. Intu-
itively  this property allows the loss to offset the penalty incurred by instances that are misclassiﬁed
with high margin by awarding a “gain” for instances that correctly classiﬁed with high margin.

5.2 The unhinged loss is classiﬁcation calibrated

SLN-robustness is by itself insufﬁcient for a learner to be useful. For example  a loss that is uni-
formly zero is strongly SLN-robust  but is useless as it is not classiﬁcation-calibrated. Fortunately 
the unhinged loss is classiﬁcation-calibrated  as we now establish. For technical reasons (see §5.3) 
we operate with FB = [−B  +B]X  the set of scorers with range bounded by B ∈ [0 ∞).
Proposition 6. Fix (cid:96) = (cid:96)unh. For any DM η  B ∈ [0 ∞)  SD FB  ∗
= {x (cid:55)→ B · sign(2η(x) − 1)}.
Thus  for every B ∈ [0 ∞)  the restricted Bayes-optimal scorer over FB has the same sign as the
Bayes-optimal classiﬁer for 0-1 loss. In the limiting case where F = RX  the optimal scorer is
attainable if we operate over the extended reals R ∪ {±∞}  so that (cid:96)unh is classiﬁcation-calibrated.

(cid:96)

5.3 Enforcing boundedness of the loss

While the classiﬁcation-calibration of (cid:96)unh is encouraging  Proposition 6 implies that its (unre-
stricted) Bayes-risk is −∞. Thus  the regret of every non-optimal scorer s is identically +∞  which
hampers analysis of consistency.
In orthodox decision theory  analogous theoretical issues arise
when attempting to establish basic theorems with unbounded losses [Ferguson  1967  pg. 78].
We can side-step this issue by restricting attention to bounded scorers  so that (cid:96)unh is effectively
bounded. By Proposition 6  this does not affect the classiﬁcation-calibration of the loss. In the con-
√
text of linear scorers  boundedness of scorers can be achieved by regularisation: instead of work-
λ}  where λ > 0  so
ing with Flin  one can instead use Flin λ = {x (cid:55)→ (cid:104)w  x(cid:105) | ||w||2 ≤ 1/
λ for R = supx∈X ||x||2. Observe that as ((cid:96)unh  F) is SLN-robust for any F 
that Flin λ ⊆ F
((cid:96)unh  Flin λ) is SLN-robust for any λ > 0. As we shall see in §6.3  working with Flin λ also lets us
establish SLN-robustness of the hinge loss when λ is large.

√

R/

5.4 Unhinged loss minimisation on corrupted distribution is consistent

Using bounded scorers makes it possible to establish a surrogate regret bound for the unhinged loss.
This shows classiﬁcation consistency of unhinged loss minimisation on the corrupted distribution.

2This loss has been considered in Sriperumbudur et al. [2009]  Reid and Williamson [2011] in the context
of maximum mean discrepancy; see the Appendix. The analysis of its SLN-robustness is to our knowledge
novel.

5

Proposition 7. Fix (cid:96) = (cid:96)unh. Then  for any D  ρ ∈ [0  1/2)  B ∈ [1 ∞)  and scorer s ∈ FB 

regretD

01(s) ≤ regretD FB

(cid:96)

(s) =

1

1 − 2ρ

· regretD FB

(cid:96)

(s).

Standard rates of convergence via generalisation bounds are also trivial to derive; see the Appendix.

6 Learning with the unhinged loss and kernels

We now show that the optimal solution for the unhinged loss when employing regularisation and
kernelised scorers has a simple form. This sheds further light on SLN-robustness and regularisation.

6.1 The centroid classiﬁer optimises the unhinged loss
Consider minimising the unhinged risk over the class of kernelised scorers FH λ = {s : x (cid:55)→
λ} for some λ > 0  where Φ : X → H is a feature mapping into a
(cid:104)w  Φ(x)(cid:105)H | ||w||H ≤ 1/
reproducing kernel Hilbert space H with kernel k. Equivalently  given a distribution3 D  we want

√

w∗
unh λ = argmin
w∈H

E

(X Y)∼D

[1 − Y · (cid:104)w  Φ(X)(cid:105)] +

(cid:104)w  w(cid:105)H.

λ
2

(6)

The ﬁrst-order optimality condition implies that
·

w∗
unh λ =

1
λ

E

(X Y)∼D

[Y · Φ(X)]  

(cid:18)

·

·

E

(X Y)∼D

[Y · k(X  x)] = x (cid:55)→ 1
λ

which is the kernel mean map of D [Smola et al.  2007]  and thus the optimal unhinged scorer is
unh λ : x (cid:55)→ 1
s∗
λ

.
(8)
From Equation 8  the unhinged solution is equivalent to a nearest centroid classiﬁer [Manning et al. 
2008  pg. 181] [Tibshirani et al.  2002] [Shawe-Taylor and Cristianini  2004  Section 5.1]. Equation
8 gives a simple way to understand the SLN-robustness of ((cid:96)unh  FH λ)  as the optimal scorers on
the clean and corrupted distributions only differ by a scaling (see the Appendix):

[k(X  x)] − (1 − π) · E
X∼Q

π · E
X∼P

[k(X  x)]

(7)

(cid:19)

(9)

(∀x ∈ X)

E

(X Y)∼D

[Y · k(X  x)] =

1

1 − 2ρ

·

E

(X Y)∼D

(cid:2)Y · k(X  x)(cid:3) .

Interestingly  Servedio [1999  Theorem 4] established that a nearest centroid classiﬁer (which they
termed “AVERAGE”) is robust to a general class of label noise  but required the assumption that
M is uniform over the unit sphere. Our result establishes that SLN robustness of the classiﬁer
holds without any assumptions on M. In fact  Ghosh et al. [2015  Theorem 1] lets one quantify the
unhinged loss’ performance under a more general noise model; see the Appendix for discussion.

6.2 Practical considerations

We note several points relating to practical usage of the unhinged loss with kernelised scorers. First 
cross-validation is not required to select λ  since changing λ only changes the magnitude of scores 
not their sign. Thus  for the purposes of classiﬁcation  one can simply use λ = 1.
Second  we can easily extend the scorers to use a bias regularised with strength 0 < λb (cid:54)= λ. Tuning
λb is equivalent to computing s∗
Third  when H = Rd for d small  we can store w∗
unh λ explicitly  and use this to make predictions.
For high (or inﬁnite) dimensional H  we can either make predictions directly via Equation 8  or
use random Fourier features [Rahimi and Recht  2007] to (approximately) embed H into some low-
dimensional Rd  and then store w∗
unh λ as usual. (The latter requires a translation-invariant kernel.)
We now show that under some assumptions  w∗
unh λ coincides with the solution of two established
methods; the Appendix discusses some further relationships  e.g. to the maximum mean discrepancy.

unh λ as per Equation 8  and tuning a threshold on a holdout set.

3Given a training sample S ∼ Dn  we can use plugin estimates as appropriate.

6

6.3 Equivalence to a highly regularised SVM and other convex potentials

There is an interesting equivalence between the unhinged solution and that of a highly regularised
SVM. This has been noted in e.g. Hastie et al. [2004  Section 6]  which showed how SVMs approach
a nearest centroid classiﬁer  which is of course the optimal unhinged solution.
Proposition 8. Pick any D and Φ : X → H with R = supx∈X ||Φ(x)||H < ∞. For any λ > 0  let

w∗
hinge λ = argmin
w∈H

E

(X Y)∼D

[max(0  1 − Y · (cid:104)w  Φ(x)(cid:105)H)] +

(cid:104)w  w(cid:105)H

λ
2

unh λ.

hinge λ = w∗

be the soft-margin SVM solution. Then  if λ ≥ R2  w∗
Since ((cid:96)unh  FH λ) is SLN-robust  it follows that for (cid:96)hinge : (y  v) (cid:55)→ max(0  1−yv)  ((cid:96)hinge  FH λ)
is similarly SLN-robust provided λ is sufﬁciently large. That is  strong (cid:96)2 regularisation (and a
bounded feature map) endows the hinge loss with SLN-robustness4. Proposition 8 can be generalised
to show that w∗
unh λ is the limiting solution of any twice differentiable convex potential. This shows
that strong (cid:96)2 regularisation endows most learners with SLN-robustness. Intuitively  with strong
regularisation  one only considers the behaviour of a loss near zero; since a convex potential φ has
φ(cid:48)(0) < 0  it will behave similarly to its linear approximation around zero  viz. the unhinged loss.
Proposition 9. Pick any D  bounded feature mapping Φ : X → H  and twice differentiable convex
potential φ with φ(cid:48)(cid:48)([−1  1]) bounded. Let w∗

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) w∗

||w∗

lim
λ→∞

φ λ be the minimiser of the regularised φ risk. Then 
− w∗
||w∗

= 0.

unh λ

unh λ||H

φ λ

φ λ||H

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

H

6.4 Equivalence to Fisher Linear Discriminant with whitened data

For binary classiﬁcation on DM η  the Fisher Linear Discriminant (FLD) ﬁnds a weight vector pro-
portional to the minimiser of square loss (cid:96)sq : (y  v) (cid:55)→ (1 − yv)2 [Bishop  2006  Section 4.1.5] 

sq λ = (EX∼M [XXT ] + λI)−1 · E(X Y)∼D[Y · X].
w∗

(10)
By Equation 9  and the fact that the corrupted marginal M = M  w∗
sq λ is only changed by a scaling
factor under label noise. This provides an alternate proof of the fact that ((cid:96)sq  Flin) is SLN-robust5
[Manwani and Sastry  2013  Theorem 2]. Clearly  the unhinged loss solution w∗
unh λ is equivalent to
the FLD and square loss solution w∗
sq λ when the input data is whitened i.e. E
X∼M
a well-speciﬁed F  e.g. with a universal kernel  both the unhinged and square loss asymptotically
recover the optimal classiﬁer  but the unhinged loss does not require a matrix inversion. With a
misspeciﬁed F  one cannot in general argue for the superiority of the unhinged loss over square loss 
or vice-versa  as there is no universally good surrogate to the 0-1 loss [Reid and Williamson  2010 
Appendix A]; the Appendix illustrate examples where both losses may underperform.

(cid:2)XXT(cid:3) = I. With

7 SLN-robustness of unhinged loss: empirical illustration

We now illustrate that the unhinged loss’ SLN-robustness is empirically manifest. We reiterate
that with high regularisation  the unhinged solution is equivalent to an SVM (and in the limit any
classiﬁcation-calibrated loss) solution. Thus  we do not aim to assert that the unhinged loss is
“better” than other losses  but rather  to demonstrate that its SLN-robustness is not purely theoretical.
the unhinged risk minimiser performs well on the example of Long
We ﬁrst show that
and Servedio [2010] (henceforth LS10).
Figure 1 shows the distribution D  where X =
2} and all three instances
{(1  0)  (γ  5γ)  (γ −γ)} ⊂ R2  with marginal distribution M = { 1
are deterministically positive. We pick γ = 1/2. The unhinged minimiser perfectly classiﬁes all
three points  regardless of the level of label noise (Figure 1). The hinge minimiser is perfect when
there is no noise  but with even a small amount of noise  achieves a 50% error rate.

4   1

4   1

4Long and Servedio [2010  Section 6] show that (cid:96)1 regularisation does not endow SLN-robustness.
5Square loss escapes the result of Long and Servedio [2010] since it is not monotone decreasing.

7

Unhinged
Hinge 0% noise
Hinge 1% noise

0.5

1

1

0.5

−0.5

−1

Figure 1: LS10 dataset.

Hinge
0.00 ± 0.00
0.15 ± 0.27
0.21 ± 0.30
0.38 ± 0.37
0.42 ± 0.36
0.47 ± 0.38

t-logistic
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.22 ± 0.08
0.22 ± 0.08
0.39 ± 0.23

Unhinged
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.34 ± 0.48

ρ = 0
ρ = 0.1
ρ = 0.2
ρ = 0.3
ρ = 0.4
ρ = 0.49

Table 1: Mean and standard deviation of the 0-
1 error over 125 trials on LS10. Grayed cells
denote the best performer at that noise rate.

We next consider empirical risk minimisers from a random training sample: we construct a training
set of 800 instances  injected with varying levels of label noise  and evaluate classiﬁcation perfor-
mance on a test set of 1000 instances. We compare the hinge  t-logistic (for t = 2) [Ding and
Vishwanathan  2010] and unhinged minimisers using a linear scorer without a bias term  and regu-
larisation strength λ = 10−16. From Table 1  even at 40% label noise  the unhinged classiﬁer is able
to ﬁnd a perfect solution. By contrast  both other losses suffer at even moderate noise rates.
We next report results on some UCI datasets  where we additionally tune a threshold so as to ensure
the best training set 0-1 accuracy. Table 2 summarises results on a sample of four datasets. (The
Appendix contains results with more datasets  performance metrics  and losses.) Even at noise close
to 50%  the unhinged loss is often able to learn a classiﬁer with some discriminative power.

Hinge
0.00 ± 0.00
0.01 ± 0.03
0.06 ± 0.12
0.17 ± 0.20
0.35 ± 0.24
0.60 ± 0.20

t-Logistic
0.00 ± 0.00
0.01 ± 0.03
0.04 ± 0.05
0.09 ± 0.11
0.24 ± 0.16
0.49 ± 0.20

Unhinged
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.01
0.02 ± 0.07
0.13 ± 0.22
0.45 ± 0.33

(a) iris.

Hinge
0.00 ± 0.00
0.10 ± 0.08
0.19 ± 0.11
0.31 ± 0.13
0.39 ± 0.13
0.50 ± 0.16

t-Logistic
0.00 ± 0.00
0.11 ± 0.02
0.15 ± 0.02
0.22 ± 0.03
0.33 ± 0.04
0.48 ± 0.04

Unhinged
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.01 ± 0.00
0.02 ± 0.02
0.34 ± 0.21

ρ = 0
ρ = 0.1
ρ = 0.2
ρ = 0.3
ρ = 0.4
ρ = 0.49

ρ = 0
ρ = 0.1
ρ = 0.2
ρ = 0.3
ρ = 0.4
ρ = 0.49

Hinge
0.05 ± 0.00
0.06 ± 0.01
0.06 ± 0.01
0.08 ± 0.04
0.14 ± 0.10
0.45 ± 0.26

t-Logistic
0.05 ± 0.00
0.07 ± 0.02
0.08 ± 0.03
0.11 ± 0.05
0.24 ± 0.13
0.49 ± 0.16

Unhinged
0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.01
0.09 ± 0.10
0.46 ± 0.30

(b) housing.

Hinge
0.05 ± 0.00
0.15 ± 0.03
0.21 ± 0.03
0.25 ± 0.03
0.31 ± 0.05
0.48 ± 0.09

t-Logistic
0.04 ± 0.00
0.24 ± 0.00
0.24 ± 0.00
0.24 ± 0.00
0.24 ± 0.00
0.40 ± 0.24

Unhinged
0.19 ± 0.00
0.19 ± 0.01
0.19 ± 0.01
0.19 ± 0.03
0.22 ± 0.05
0.45 ± 0.08

ρ = 0
ρ = 0.1
ρ = 0.2
ρ = 0.3
ρ = 0.4
ρ = 0.49

ρ = 0
ρ = 0.1
ρ = 0.2
ρ = 0.3
ρ = 0.4
ρ = 0.49

(c) usps0v7.

(d) splice.

Table 2: Mean and standard deviation of the 0-1 error over 125 trials on UCI datasets.

8 Conclusion and future work

We proposed a convex  classiﬁcation-calibrated loss  proved that is robust to symmetric label noise
(SLN-robust)  showed it is the unique loss that satisﬁes a notion of strong SLN-robustness  estab-
lished that it is optimised by the nearest centroid classiﬁer  and showed that most convex potentials 
such as the SVM  are also SLN-robust when highly regularised. So  with apologies to Wilde [1895]:

While the truth is rarely pure  it can be simple.

Acknowledgments

NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program. The authors thank
Cheng Soon Ong for valuable comments on a draft of this paper.

8

References
Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning  2(4):343–370  1988.
Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk bounds. Journal

of the American Statistical Association  101(473):138 – 156  2006.

Christopher M Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York  Inc.  2006.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Conference on

Computational Learning Theory (COLT)  pages 92–100  1998.

Vasil Denchev  Nan Ding  Hartmut Neven  and S.V.N. Vishwanathan. Robust classiﬁcation with adiabatic

quantum optimization. In International Conference on Machine Learning (ICML)  pages 863–870  2012.

Luc Devroye  L´aszl´o Gy¨orﬁ  and G´abor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer  1996.
In Advances in Neural Information Processing
Nan Ding and S.V.N. Vishwanathan. t-logistic regression.

Systems (NIPS)  pages 514–522. Curran Associates  Inc.  2010.

Thomas S. Ferguson. Mathematical Statistics: A Decision Theoretic Approach. Academic Press  1967.
Aritra Ghosh  Naresh Manwani  and P. S. Sastry. Making risk minimization tolerant to label noise. Neurocom-

puting  160:93 – 107  2015.

Trevor Hastie  Saharon Rosset  Robert Tibshirani  and Ji Zhu. The entire regularization path for the support
vector machine. Journal of Machine Learning Research  5:1391–1415  December 2004. ISSN 1532-4435.
Michael Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the ACM  5(6):392–401 

November 1998.

Philip M. Long and Rocco A. Servedio. Random classiﬁcation noise defeats all convex potential boosters.

Machine Learning  78(3):287–304  2010. ISSN 0885-6125.

Christopher D. Manning  Prabhakar Raghavan  and Hinrich Sch¨utze. Introduction to Information Retrieval.

Cambridge University Press  New York  NY  USA  2008. ISBN 0521865719  9780521865715.

Naresh Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE Transactions on Cybernetics 

43(3):1146–1151  June 2013.

Hamed Masnadi-Shirazi  Vijay Mahadevan  and Nuno Vasconcelos. On the design of robust classiﬁers for

computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2010.

Nagarajan Natarajan  Inderjit S. Dhillon  Pradeep D. Ravikumar  and Ambuj Tewari. Learning with noisy

labels. In Advances in Neural Information Processing Systems (NIPS)  pages 1196–1204  2013.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural

Information Processing Systems (NIPS)  pages 1177–1184  2007.

Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning Research 

11:2387–2422  December 2010.

Mark D Reid and Robert C Williamson. Information  divergence and risk for binary experiments. Journal of

Machine Learning Research  12:731–817  Mar 2011.

Bernhard Sch¨olkopf and Alexander J Smola. Learning with kernels  volume 129. MIT Press  2002.
Rocco A. Servedio. On PAC learning using Winnow  Perceptron  and a Perceptron-like algorithm. In Confer-

ence on Computational Learning Theory (COLT)  1999.

John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni. Press  2004.
Alex Smola  Arthur Gretton  Le Song  and Bernhard Sch¨olkopf. A Hilbert space embedding for distributions.

In Algorithmic Learning Theory (ALT)  2007.

Bharath K. Sriperumbudur  Kenji Fukumizu  Arthur Gretton  Gert R. G. Lanckriet  and Bernhard Sch¨olkopf.
Kernel choice and classiﬁability for RKHS embeddings of probability distributions. In Advances in Neural
Information Processing Systems (NIPS)  2009.

Guillaume Stempfel and Liva Ralaivola. Learning SVMs from sloppily labeled data.
Networks (ICANN)  volume 5768  pages 884–893. Springer Berlin Heidelberg  2009.

In Artiﬁcial Neural

Robert Tibshirani  Trevor Hastie  Balasubramanian Narasimhan  and Gilbert Chu. Diagnosis of multiple cancer
types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences  99(10):
6567–6572  2002.

Oscar Wilde. The Importance of Being Earnest  1895.

9

,Brendan van Rooyen
Aditya Menon
Robert Williamson