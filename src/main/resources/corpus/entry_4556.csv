2017,Ranking Data with Continuous Labels through Oriented Recursive Partitions,We formulate a supervised learning problem  referred to as continuous ranking  where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space X and the goal is to order all possible observations x in X by means of a scoring function s : X → R so that s(X) and Y tend to increase or decrease together with highest probability. This problem generalizes bi/multi-partite ranking to a certain extent and the task of finding optimal scoring functions s(x) can be naturally cast as optimization of a dedicated functional cri- terion  called the IROC curve here  or as maximization of the Kendall τ related to the pair (s(X)  Y ). From the theoretical side  we describe the optimal elements of this problem and provide statistical guarantees for empirical Kendall τ maximiza- tion under appropriate conditions for the class of scoring function candidates. We also propose a recursive statistical learning algorithm tailored to empirical IROC curve optimization and producing a piecewise constant scoring function that is fully described by an oriented binary tree. Preliminary numerical experiments highlight the difference in nature between regression and continuous ranking and provide strong empirical evidence of the performance of empirical optimizers of the criteria proposed.,Ranking Data with Continuous Labels
through Oriented Recursive Partitions

Stephan Cl´emenc¸on

Mastane Achab
LTCI  T´el´ecom ParisTech  Universit´e Paris-Saclay

75013 Paris  France

first.last@telecom-paristech.fr

Abstract

We formulate a supervised learning problem  referred to as continuous ranking 
where a continuous real-valued label Y is assigned to an observable r.v. X taking
its values in a feature space X and the goal is to order all possible observations
x in X by means of a scoring function s : X → R so that s(X) and Y tend to
increase or decrease together with highest probability. This problem generalizes
bi/multi-partite ranking to a certain extent and the task of ﬁnding optimal scoring
functions s(x) can be naturally cast as optimization of a dedicated functional cri-
terion  called the IROC curve here  or as maximization of the Kendall τ related to
the pair (s(X)  Y ). From the theoretical side  we describe the optimal elements of
this problem and provide statistical guarantees for empirical Kendall τ maximiza-
tion under appropriate conditions for the class of scoring function candidates. We
also propose a recursive statistical learning algorithm tailored to empirical IROC
curve optimization and producing a piecewise constant scoring function that is
fully described by an oriented binary tree. Preliminary numerical experiments
highlight the difference in nature between regression and continuous ranking and
provide strong empirical evidence of the performance of empirical optimizers of
the criteria proposed.

1

Introduction

The predictive learning problem considered in this paper can be easily stated in an informal fashion 
as follows. Given a collection of objects of arbitrary cardinality  N ≥ 1 say  respectively described
. . .   xN in a feature space X   the goal is to learn how to order them by
by characteristics x1 
increasing order of magnitude of a certain unknown continuous variable y. To ﬁx ideas  the attribute
y can represent the ’size’ of the object and be difﬁcult to measure  as for the physical measurement of
microscopic bodies in chemistry and biology or the cash ﬂow of companies in quantitative ﬁnance
and the features x may then correspond to indirect measurements. The most convenient way to
deﬁne a preorder on a feature space X is to transport the natural order on the real line onto it by
means of a (measurable) scoring function s : X → R: an object with charcateristics x is then said to
be ’larger’ (’strictly larger’  respectively) than an object described by x(cid:48) according to the scoring rule
s when s(x(cid:48)) ≤ s(x) (when s(x) < s(x(cid:48))). Statistical learning boils down here to build a scoring
function s(x)  based on a training data set Dn = {(X1  Y1)  . . .   (Xn  Yn)} of objects for which
the values of all variables (direct and indirect measurements) have been jointly observed  such that
s(X) and Y tend to increase or decrease together with highest probability or  in other words  such
that the ordering of new objects induced by s(x) matches that deﬁned by their true measures as well
as possible. This problem  that shall be referred to as continuous ranking throughout the article can
be viewed as an extension of bipartite ranking  where the output variable Y is assumed to be binary
and the objective can be naturally formulated as a functional M-estimation problem by means of the
concept of ROC curve  see [7]. Refer also to [4]  [11]  [1] for approaches based on the optimization

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

of summary performance measures such as the AUC criterion in the binary context. Generalization
to the situation where the random label is ordinal and may take a ﬁnite number K ≥ 3 of values
is referred to as multipartite ranking and has been recently investigated in [16] (see also e.g. [14]) 
where distributional conditions guaranteeing that ROC surface and the VUS criterion can be used
to determine optimal scoring functions are exhibited in particular.
It is the major purpose of this paper to formulate the continuous ranking problem in a quantitative
manner and explore the connection between the latter and bi/multi-partite ranking. Intuitively  op-
timal scoring rules would be also optimal for any bipartite subproblem deﬁned by thresholding the
continuous variable Y with cut-off t > 0  separating the observations X such that Y < t from
those such that Y > t. Viewing this way continuous ranking as a continuum of nested bipartite
ranking problems  we provide here sufﬁcient conditions for the existence of such (optimal) scoring
rules and we introduce a concept of integrated ROC curve (IROC curve in abbreviated form) that
may serve as a natural performance measure for continuous ranking  as well as the related notion of
integrated AUC criterion  a summary scalar criterion  akin to Kendall tau. Generalization properties
of empirical Kendall tau maximizers are discussed in the Supplementary Material. The paper also
introduces a novel recursive algorithm that solves a discretized version of the empirical integrated
ROC curve optimization problem  producing a scoring function that can be computed by means of
a hierarchical combination of binary classiﬁcation rules. Numerical experiments providing strong
empirical evidence of the relevance of the approach promoted in this paper are also presented.
The paper is structured as follows. The probabilistic framework we consider is described and key
concepts of bi/multi-partite ranking are brieﬂy recalled in section 2. Conditions under which optimal
solutions of the problem of ranking data with continuous labels exist are next investigated in section
3  while section 4 introduces a dedicated quantitative (functional) performance measure  the IROC
curve. The algorithmic approach we propose in order to learn scoring functions with nearly optimal
IROC curves is presented at length in section 5. Numerical results are displayed in section 6. Some
technical proofs are deferred to the Supplementary Material.

2 Notation and Preliminaries
Throughout the paper  the indicator function of any event E is denoted by I{E}. The pseudo-inverse
of any cdf F (t) on R is denoted by F −1(u) = inf{s ∈ R : F (s) ≥ u}  while U([0  1]) denotes the
uniform distribution on the unit interval [0  1].

2.1 The probabilistic framework

Given a continuous real valued r.v. Y representing an attribute of an object  its ’size’ say  and a
random vector X taking its values in a (typically high dimensional euclidian) feature space X
modelling other observable characteristics of the object (e.g. ’indirect measurements’ of the size
of the object)  hopefully useful for predicting Y   the statistical learning problem considered here
is to learn from n ≥ 1 training independent observations Dn = {(X1  Y1) 
. . .   (Xn  Yn)} 
drawn as the pair (X  Y )  a measurable mapping s : X → R  that shall be referred to as a
scoring function throughout the paper  so that the variables s(X) and Y tend to increase or de-
crease together: ideally  the larger the score s(X)  the higher the size Y . For simplicity  we as-
sume throughout the article that X = Rd with d ≥ 1 and that the support of Y ’s distribution
is compact  equal to [0  1] say. For any q ≥ 1  we denote by λq the Lebesgue measure on Rq
equipped with its Borelian σ-algebra and suppose that the joint distribution FX Y (dxdy) of the
pair (X  Y ) has a density fX Y (x  y) w.r.t. the tensor product measure λd ⊗ λ1. We also intro-
duces the marginal distributions FY (dy) = fY (y)λ1(dy) and FX (dx) = fX (x)λd(dx)  where
y∈[0 1] fX Y (x  y)λ1(dy) as well as the condi-
tional densities fX|Y =y(x) = fX Y (x  y)/fY (y) and fY |X=x(y) = fX Y (x  y)/fX (x). Observe
incidentally that the probabilistic framework of the continuous ranking problem is quite similar to
that of distribution-free regression. However  as shall be seen in the subsequent analysis  even if
the regression function m(x) = E[Y | X = x] can be optimal under appropriate conditions  just
like for regression  measuring ranking performance involves criteria that are of different nature than
the expected least square error and plug-in rules may not be relevant for the goal pursued here  as
depicted by Fig. 2 in the Supplementary Material.

x∈X fX Y (x  y)λd(dx) and fX (x) = (cid:82)

fY (y) = (cid:82)

2

Scoring functions. The set of all scoring functions is denoted by S here. Any scoring function
s ∈ S deﬁnes a total preorder on the space X : ∀(x  x(cid:48)) ∈ X 2  x (cid:22)s x(cid:48) ⇔ s(x) ≤ s(x(cid:48)). We also
set x ≺s x(cid:48) when s(x) < s(x(cid:48)) and x =s x(cid:48) when s(x) = s(x(cid:48)) for (x  x(cid:48)) ∈ X 2.

2.2 Bi/multi-partite ranking
Suppose that Z is a binary label  taking its values in {−1  +1} say  assigned to the r.v. X. In bipartite
ranking  the goal is to pick s in S so that the larger s(X)  the greater the probability that Y is equal
to 1 ideally. In other words  the objective is to learn s(x) such that the r.v. s(X) given Y = +1
is as stochastically larger1 as possible than the r.v. s(X) given Y = −1: the difference between
¯Gs(t) = P{s(X) ≥ t | Y = +1} and ¯Hs(t) = P{s(X) ≥ t | Y = −1} should be thus maximal
for all t ∈ R. This can be naturally quantiﬁed by means of the notion of ROC curve of a candidate
s ∈ S  i.e. the parametrized curve t ∈ R (cid:55)→ ( ¯Hs(t)  ¯Gs(t))  which can be viewed as the graph
of a mapping ROCs : α ∈ (0  1) (cid:55)→ ROCs(α)  connecting possible discontinuity points by linear
segments (so that ROCs(α) = ¯Gs ◦ (1 − H−1
s (1 − α) 
where Hs = 1 − ¯Hs). A basic Neyman Pearson’s theory argument shows that the optimal elements
s∗(x) related to this natural (functional) bipartite ranking criterion (i.e. scoring functions whose
ROC curve dominates any other ROC curve everywhere on (0  1)) are transforms (T ◦ η)(x) of
the posterior probability η(x) = P{Z = +1 | X = x}  where T : SUPP(η(X)) → R is any
strictly increasing borelian mapping. Optimization of the curve in sup norm has been considered in
[7] or in [8] for instance. However  given its functional nature  in practice the ROC curve of any
s ∈ S is often summarized by the area under it  which performance measure can be interpreted in a
probabilistic manner  as the theoretical rate of concording pairs
AUC(s) = P{s(X) < s(X(cid:48)) | Z = −1  Z(cid:48) = +1} +

P{s(X) = s(X(cid:48)) | Z = −1  Z(cid:48) = +1}  
(1)
where (X(cid:48)  Z(cid:48)) denoted an independent copy of (X  Z). A variety of algorithms aiming at max-
imizing the AUC criterion or surrogate pairwise criteria have been proposed and studied in the
literature  among which [11]  [15] or [3]  whereas generalization properties of empirical AUC max-
imizers have been studied in [5]  [1] and [12]. An analysis of the relationship between the AUC and
the error rate is given in [9].
Extension to the situation where the label Y takes at least three ordinal values (i.e. multipartite
ranking) has been also investigated  see e.g. [14] or [6]. In [16]  it is shown that  in contrast to the
bipartite setup  the existence of optimal solutions cannot be guaranteed in general and conditions on
(X  Y )’s distribution ensuring that optimal solutions do exist and that extensions of bipartite ranking
criteria such as the ROC manifold and the volume under it can be used for learning optimal scoring
rules have been exhibited. An analogous analysis in the context of continuous ranking is carried out
in the next section.

s )(1 − α) when Hs has no ﬂat part in H−1

1
2

3 Optimal elements in ranking data with continuous labels

In this section  a natural deﬁnition of the set of optimal elements for continuous ranking is ﬁrst
proposed. Existence and characterization of such optimal scoring functions are next discussed.

3.1 Optimal scoring rules for continuous ranking
Considering a threshold value y ∈ [0  1]  a considerably weakened (and discretized) version of the
problem stated informally above would consist in ﬁnding s so that the r.v. s(X) given Y > y is
as stochastically larger than s(X) given Y < y as possible. This subproblem coincides with the
bipartite ranking problem related to the pair (X  Zy)  where Zy = 2I{Y > y} − 1. As brieﬂy
recalled in subsection 2.2  the optimal set S∗
y is composed of the scoring functions that induce the
same ordering as

ηy(X) = P{Y > y | X} = 1 − (1 − py)/(1 − py + pyΦy(X)) 
where py = 1 − FY (y) = P{Y > y} and Φy(X) = (dFX|Y >y/dFX|Y <y)(X).

1Given two real-valued r.v.’s U and U(cid:48)  recall that U is said to be stochastically larger than U(cid:48) when

P{U ≥ t} ≥ P{U(cid:48) ≥ t} for all t ∈ R.

3

A continuum of bipartite ranking problems. The rationale behind the deﬁnition of the set S∗ of
optimal scoring rules for continuous ranking is that any element s∗ should score observations x in
the same order as ηy (or equivalently as Φy).
Deﬁnition 1. (OPTIMAL SCORING RULE) An optimal scoring rule for the continuous ranking prob-
lem related to the random pair (X  Y ) is any element s∗ that fulﬁlls: ∀y ∈ (0  1) 

In other words  the set of optimal rules is deﬁned as S∗ =(cid:84)

∀(x  x(cid:48)) ∈ X 2  ηy(x) < ηy(x(cid:48)) ⇒ s∗(x) < s∗(x(cid:48)).

y∈(0 1) S∗
y .

(2)

It is noteworthy that  although the deﬁnition above is natural  the set S∗ can be empty in absence of
any distributional assumption  as shown by the following example.
Example 1. As a counter-example  consider the distributions FX Y such that FY = U([0  1]) and
FX|Y =y = N (|2y − 1|  (2y − 1)2). Observe that (X  1 − Y ) d=(X  Y )  so that Φ1−t = Φ−1
for all
t ∈ (0  1) and there exists t (cid:54)= 0 s.t. Φt is not constant. Hence  there exists no s∗ in S such that (2)
holds true for all t ∈ (0  1).
Remark 1. (INVARIANCE) We point out that the class S∗ of optimal elements for continuous rank-
ing thus deﬁned is invariant by strictly increasing transform of the ’size’ variable Y (in particular 
a change of unit has no impact on the deﬁnition of S∗): for any borelian and strictly increasing
mapping H : (0  1) → (0  1)  any scoring function s∗(x) that is optimal for the continuous ranking
problem related to the pair (X  Y ) is still optimal for that related to (X  H(Y )) (since  under these
hypotheses  for any y ∈ (0  1): Y > y ⇔ H(Y ) > H(y)).

t

3.2 Existence and characterization of optimal scoring rules

We now investigate conditions guaranteeing the existence of optimal scoring functions for the con-
tinuous ranking problem.
Proposition 1. The following assertions are equivalent.

1. For all 0 < y < y(cid:48) < 1  for all (x  x(cid:48)) ∈ X 2: Φy(x) < Φy(x(cid:48)) ⇒ Φy(cid:48)(x) ≤ Φy(cid:48)(x(cid:48)).
2. There exists an optimal scoring rule s∗ (i.e. S∗ (cid:54)= ∅).
3. The regression function m(x) = E[Y | X = x] is an optimal scoring rule.
4. The collection of probability distributions FX|Y =y(dx) = fX|Y =y(x)λd(dx)  y ∈ (0  1)
satisﬁes the monotone likelihood ratio property: there exist s∗ ∈ S and  for all 0 < y <
y(cid:48) < 1  an increasing function ϕy y(cid:48) : R → R+ such that: ∀x ∈ Rd 

fX|Y =y(cid:48)
fX|Y =y

(x) = ϕy y(cid:48)(s∗(x)).

Refer to the Appendix section for the technical proof. Truth should be said  assessing that Assertion
1. is a very challenging statistical task. However  through important examples  we now describe (not
uncommon) situations where the conditions stated in Proposition 1 are fulﬁlled.
Example 2. We give a few important examples of probabilistic models fulﬁlling the properties listed
in Proposition 1.
• Regression model. Suppose that Y = m(X) +   where m : X → R is a borelian function and 
is a centered r.v. independent from X. One may easily check that m ∈ S∗.
• Exponential families. Suppose that fX|Y =y(x) = exp(κ(y)T (x) − ψ(y))f (x) for all x ∈ Rd 
where f : Rd → R+ is borelian  κ : [0  1] → R is a borelian strictly increasing function and

T : Rd → R is a borelian mapping such that ψ(y) = log(cid:82)

x∈Rd exp(κ(y)T (x))f (x)dx < +∞.

We point out that  although the regression function m(x) is an optimal scoring function when
S∗ (cid:54)= ∅  the continuous ranking problem does not coincide with distribution-free regression (notice
incidentally that  in this case  any strictly increasing transform of m(x) belongs to S∗ as well). As
depicted by Fig. 2 the least-squares criterion is not relevant to evaluate continuous ranking perfor-
mance and naive plug-in strategies should be avoided  see Remark 3 below. Dedicated performance
criteria are proposed in the next section.

4

4 Performance measures for continuous ranking

We now investigate quantitative criteria for assessing the performance in the continuous ranking
problem  which practical machine-learning algorithms may rely on. We place ourselves in the situ-
ation where the set S∗ is not empty  see Proposition 1 above.
A functional performance measure. It follows from the view developped in the previous section
that  for any (s  s∗) ∈ S × S∗ and for all y ∈ (0  1)  we have:

∀α ∈ (0  1)  ROCs y(α) ≤ ROCs∗ y(α) = ROC∗

(3)
denoting by ROCs y the ROC curve of any s ∈ S related to the bipartite ranking subproblem
(X  Zy) and by ROC∗
y the corresponding optimal ROC curve  i.e. the ROC curve of strictly increas-
ing transforms of ηy(x). Based on this observation  it is natural to design a dedicated performance
measure by aggregating these ’sub-criteria’. Integrating over y w.r.t. a σ-ﬁnite measure µ with sup-

port equal to [0  1]  this leads to the following deﬁnition IROCµ s(α) =(cid:82) ROCs y(α)µ(dy). The

functional criterion thus deﬁned inherits properties from the ROCs y’s (e.g. monotonicity  concav-
ity). In addition  the curve IROCµ s∗ with s∗ ∈ S∗ dominates everywhere on (0  1) any other curve
IROCµ s for s ∈ S. However  except in pathologic situations (e.g. when s(x) is constant)  the curve
IROCµ s is not invariant when replacing Y ’s distribution by that of a strictly increasing transform
H(Y ). In order to guarantee that this desirable property is fulﬁlled (see Remark 1)  one should
integrate w.r.t. Y ’s distribution (which boils down to replacing Y by the uniformly distributed r.v.
FY (Y )).
Deﬁnition 2. (INTEGRATED ROC/AUC CRITERIA) The integrated ROC curve of any scoring rule
s ∈ S is deﬁned as: ∀α ∈ (0  1) 

y(α) 

IROCs(α) =

ROCs y(α)FY(dy) = E [ROCs Y(α)] .

The integrated AUC criterion is deﬁned as the area under the integrated ROC curve: ∀s ∈ S 

IAUC(s) =

IROCs(α)dα.

α=0

The following result reveals the relevance of the functional/summary criteria deﬁned above for the
continuous ranking problem. Additional properties of IROC curves are listed in the Supplementary
Material.
Theorem 1. Let s∗ ∈ S. The following assertions are equivalent.

1. The assertions of Proposition 1 are fulﬁlled and s∗ is an optimal scoring function in the

(4)

(5)

(cid:90) 1

y=0

(cid:90) 1

sense given by Deﬁnition 1.

2. For all α ∈ (0  1)  IROCs∗ (α) = E [ROC∗
Y]  where AUC∗
3. We have IAUCs∗ = E [AUC∗

Y(α)].

y =(cid:82) 1

If S∗ (cid:54)= ∅  then we have: ∀s ∈ S 

α=0 ROC∗

y(α)dα for all y ∈ (0  1).

Y(α)]  

for any α ∈ (0  1  )

IROCs(α) ≤ IROC∗(α) def= E [ROC∗
IAUC(s) ≤ IAUC∗ def= E [AUC∗
Y] .
In addition  for any borelian and strictly increasing mapping H : (0  1) → (0  1)  replacing Y by
H(Y ) leaves the curves IROCs  s ∈ S  unchanged.
Equipped with the notion deﬁned above  a scoring rule s1 is said to be more accurate than an-
other one s2 if IROCs2(α) ≤ IROCs1 (α) for all α ∈ (0  1).The IROC curve criterion thus
provides a partial preorder on S. Observe also that  by virtue of Fubini’s theorem  we have

IAUC(s) = (cid:82) AUCy(s)FY(dy) for all s ∈ S  denoting by AUCy(s) the AUC of s related to

the bipartite ranking subproblem (X  Zy). Just like the AUC for bipartite ranking  the scalar IAUC
criterion deﬁnes a full preorder on S for continuous ranking. Based on a training dataset Dn of inde-
pendent copies of (X  Y )  statistical versions of the IROC/IAUC criteria can be straightforwardly
computed by replacing the distributions FY   FX|Y >t and FX|Y <t by their empirical counterparts in
(3)-(5)  see the Supplementary Material for further details. The lemma below provides a probabilis-
tic interpretation of the IAUC criterion.

5

Lemma 1. Let (X(cid:48)  Y (cid:48)) be a copy of the random pair (X  Y ) and Y (cid:48)(cid:48) a copy of the r.v. Y . Suppose
that (X  Y )  (X(cid:48)  Y (cid:48)) and Y (cid:48)(cid:48) are deﬁned on the same probability space and are independent. For
all s ∈ S  we have:

IAUC(s) = P{s(X) < s(X(cid:48)) | Y < Y(cid:48)(cid:48) < Y(cid:48)} +

(6)
This result shows in particular that a natural statistical estimate of IAUC(s) based on Dn involves
U-statistics of degree 3. Its proof is given in the Supplementary Material for completeness.
The Kendall τ statistic. The quantity (6) is akin to another popular way to measure the tendency to
deﬁne the same ordering on the statistical population in a summary fashion:

P{s(X) = s(X(cid:48)) | Y < Y(cid:48)(cid:48) < Y(cid:48)} .

1
2

dτ (s)

def

= P{(s(X) − s(X(cid:48))) · (Y − Y (cid:48)) > 0} +
= P{s(X) < s(X(cid:48)) | Y < Y (cid:48)} +

1
2

P{X =s X(cid:48)}  

1
2

P{s(X) = s(X(cid:48))}

(7)

where (X(cid:48)  Y (cid:48)) denotes an independent copy of (X  Y )  observing that P{Y < Y (cid:48)} = 1/2. The
empirical counterpart of (7) based on the sample Dn  given by
I{(s(Xi) − s(Xj)) · (Yi − Yj) > 0} +

I{s(Xi) = s(Xj)}

(cid:98)dn(s) =

(cid:88)

(cid:88)

1

2

n(n − 1)

i<j

n(n − 1)

i<j

(8)
is known as the Kendall τ statistic and is widely used in the context of statistical hypothesis testing.
The quantity (7) shall be thus referred to as the (theoretical or true) Kendall τ. Notice that dτ (s) is
invariant by strictly increasing transformation of s(x) and thus describes properties of the order it
deﬁnes. The following result reveals that the class S∗  when non empty  is the set of maximizers of
the theoretical Kendall τ. Refer to the Supplementary Material for the technical proof.
Proposition 2. Suppose that S∗ (cid:54)= ∅. For any (s  s∗) ∈ S × S∗  we have: dτ (s) ≤ dτ (s∗).
Equipped with these criteria  the objective expressed above in an informal manner can be now for-
mulated in a quantitative manner as a (possibly functional) M-estimation problem.
In practice 
the goal pursued is to ﬁnd a reasonable approximation of a solution to the optimization problem
maxs∈S dτ (s) (respectively maxs∈S IAUC(s))  where the supremum is taken over the set of all
scoring functions s : X → R. Of course  these criteria are unknown in general  just like (X  Y )’s
probability distribution  and the empirical risk minimization (ERM in abbreviated form) paradigm
(see [10]) invites for maximizing the statistical version (8) over a class S0 ⊂ S of controlled com-
plexity when considering the criterion dτ (s) for instance. The generalization capacity of empirical
maximizers of the Kendall τ can be straightforwardly established using results in [5]. More details
are given in the Supplementary Material.
Before describing a practical algorithm for recursive maximization of the IROC curve  a few re-
marks are in order.
Remark 2. (ON KENDALL τ AND AUC) We point out that  in the bipartite ranking problem (i.e.
when the output variable Z takes its values in {−1  +1}  see subsection 2.2) as well  the AUC
criterion can be expressed as a function of the Kendall τ related to the pair (s(X)  Z) when the r.v.
s(X) is continuous. Indeed  we have in this case 2p(1−p)AUC(s) = dτ (s)  where p = P{Z = +1}
and dτ (s) = P{(s(X) − s(X(cid:48))) · (Z − Z(cid:48)) > 0}  denoting by (X(cid:48)  Z(cid:48)) an independent copy of
(X  Z).
Remark 3. (CONNECTION TO DISTRIBUTION-FREE REGRESSION) Consider the nonparametric
regression model Y = m(X) +   where  is a centered r.v. independent from X. In this case  it is
well-known that the regression function m(X) = E[Y | X] is the (unique) solution of the expected
least squares minimization. However  although m ∈ S∗  the least squares criterion is far from
appropriate to evaluate ranking performance  as depicted by Fig. 2. Observe additionally that  in
contrast to the criteria introduced above  increasing transformation of the output variable Y may
have a strong impact on the least squares minimizer: except for linear stransforms  E[H(Y ) | X] is
not an increasing transform of m(X).
Remark 4. (ON DISCRETIZATION) Bi/multi-partite algorithms are not directly applicable to the
continuous ranking problem. Indeed a discretization of the interval [0  1] would be ﬁrst required but
this would raise a difﬁcult question outside our scope: how to choose this discretization based on
the training data? We believe that this approach is less efﬁcient than ours which reveals problem-
speciﬁc criteria  namely IROC and IAUC.

6

Figure 1: A scoring function described by an oriented binary subtree T . For any element x ∈ X   one
may compute the quantity sT (x) very fast in a top-down fashion by means of the heap structure:
starting from the initial value 2J at the root node  at each internal node Cj k  the score remains
unchanged if x moves down to the left sibling  whereas one subtracts 2J−(j+1) from it if x moves
down to the right.

5 Continuous Ranking through Oriented Recursive Partitioning

It is the purpose of this section to introduce the algorithm CRANK  a speciﬁc tree-structured learning
algorithm for continuous ranking.

5.1 Ranking trees and Oriented Recursive Partitions

Decision trees undeniably ﬁgure among the most popular techniques  in supervised and unsuper-
vised settings  refer to [2] or [13] for instance. This is essentially due to the visual model summary
they provide  in the form of a binary tree graphic that permits to describe predictions by means of
a hierachichal combination of elementary rules of the type ”X (j) ≤ κ” or ”X (j) > κ”  comparing
the value taken by a (quantitative) component of the input vector X (the split variable) to a certain
threshold (the split value). In contrast to local learning problems such as classiﬁcation or regression 
predictive rules for a global problem such as ranking cannot be described by a (tree-structured) par-
tition of the feature space: cells (corresponding to the terminal leaves of the binary decision tree)
must be ordered so as to deﬁne a scoring function. This leads to the deﬁnition of ranking trees
as binary trees equipped with a ”left-to-right” orientation  deﬁning a tree-structured collection of
anomaly scoring functions  as depicted by Fig. 1. Binary ranking trees have been in the context of
bipartite ranking in [7] or in [3] and in [16] in the context of multipartite ranking. The root node
of a ranking tree TJ of depth J ≥ 0 represents the whole feature space X : C0 0 = X   while each
internal node (j  k) with j < J and k ∈ {0  . . .   2j − 1} corresponds to a subset Cj k ⊂ X   whose
left and right siblings respectively correspond to disjoint subsets Cj+1 2k and Cj+1 2k+1 such that
Cj k = Cj+1 2k ∪Cj+1 2k+1. Equipped with the left-to-right orientation  any subtree T ⊂ TJ deﬁnes
a preorder on X : elements lying in the same terminal cell of T being equally ranked. The scoring
function related to the oriented tree T can be written as:

(cid:88)

(cid:18)

(cid:19)

Cj k: terminal leaf of T

2J

1 − k
2j

· I{x ∈ Cj k}.

(9)

sT (x) =

5.2 The CRANK algorithm

Based on Proposition 2  as mentioned in the Supplementary Material  one can try to build from
the training dataset Dn a ranking tree by recursive empirical Kendall τ maximization. We propose
below an alternative tree-structured recursive algorithm  relying on a (dyadic) discretization of the
’size’ variable Y . At each iteration  the local sample (i.e. the data lying in the cell described by the
current node) is split into two halves (the highest/smallest halves  depending on Y ) and the algorithm
calls a binary classiﬁcation algorithm A to learn how to divide the node into right/left children. The
theoretical analysis of this algorithm and its connection with approximation of IROC∗ are difﬁcult
questions that will be adressed in future work.
Indeed we found out that the IROC cannot be

7

represented as a parametric curve contrary to the ROC  which renders proofs much more difﬁcult
than in the bipartite case.

THE CRANK ALGORITHM

1. Input. Training data Dn  depth J ≥ 1  binary classiﬁcation algorithm A.
2. Initialization. Set C0 0 = X .
3. Iterations. For j = 0  . . .   J − 1 and k = 0  . . .   2J − 1 

Zi = 2I{Yi > yj k} − 1 to any data point i lying in Cj k  i.e. such that Xi ∈ Cj k.
{(Xi  Yi) : 1 ≤ i ≤ n  Xi ∈ Cj k}  producing a classiﬁer gj k : Cj k → {−1  +1}.

(a) Compute a median yj k of the dataset {Y1  . . .     Yn} ∩ Cj k and assign the binary label
(b) Solve the binary classiﬁcation problem related to the input space Cj k and the training set
(c) Set Cj+1 2k = {x ∈ Cj k  gj k = +1} = Cj k \ Cj+1 2k+1.
4. Output. Ranking tree TJ = {Cj k : 0 ≤ j ≤ J  0 ≤ k < D}.

Of course  the depth J should be chosen such that 2J ≤ n. One may also consider continuing to
split the nodes until the number of data points within a cell has reached a minimum speciﬁed in
advance. In addition  it is well known that recursive partitioning methods fragment the data and the
unstability of splits increases with the depth. For this reason  a ranking subtree must be selected. The
growing procedure above should be classically followed by a pruning stage  where children of a same
parent are progressively merged until the root T0 is reached and a subtree among the sequence T0 ⊂
. . . ⊂ TJ with nearly maximal IAUC should be chosen using cross-validation. Issues related to the
implementation of the CRANK algorithm and variants (e.g. exploiting randomization/aggregation)
will be investigated in a forthcoming paper.

6 Numerical Experiments

In order to illustrate the idea conveyed by Fig. 2 that the least squares criterion is not appropriate for
the continuous ranking problem we compared on a toy example CRANK with CART. Recall that
the latter is a regression decision tree algorithm which minimizes the MSE (Mean Squared Error).
We also runned an alternative version of CRANK which maximizes the empirical Kendall τ instead
of the empirical IAUC: this method is refered to as KENDALL from now on. The experimental
setting is composed of a unidimensional feature space X = [0  1] (for visualization reasons) and a
simple regression model without any noise: Y = m(X). Intuitively  a least squares strategy can
miss slight oscillations of the regression function  which are critical in ranking when they occur in
high probability regions as they affect the order among the feature space. The results are presented
in Table 1. See Supplementary Material for further details.

CRANK
KENDALL
CART

IAUC Kendall τ
0.95
0.94
0.61

0.92
0.93
0.58

MSE
0.10
0.10

7.4 × 10−4

Table 1: IAUC  Kendall τ and MSE empirical measures

7 Conclusion

This paper considers the problem of learning how to order objects by increasing ’size’  modeled as a
continuous r.v. Y   based on indirect measurements X. We provided a rigorous mathematical formu-
lation of this problem that ﬁnds many applications (e.g. quality control  chemistry) and is referred
to as continuous ranking. In particular  necessary and sufﬁcient conditions on (X  Y )’s distribution
for the existence of optimal solutions are exhibited and appropriate criteria have been proposed for
evaluating the performance of scoring rules in these situations. In contrast to distribution-free re-
gression where the goal is to recover the local values taken by the regression function  continuous

8

ranking aims at reproducing the preorder it deﬁnes on the feature space as accurately as possible.
The numerical results obtained via the algorithmic approaches we proposed for optimizing the cri-
teria aforementioned highlight the difference in nature between these two statistical learning tasks.

Acknowledgments

This work was supported by the industrial chair Machine Learning for Big Data from T´el´ecom
ParisTech and by a public grant (Investissement d’avenir project  reference ANR-11-LABX-0056-
LMH  LabEx LMH).

References
[1] S. Agarwal  T. Graepel  R. Herbrich  S. Har-Peled  and D. Roth. Generalization bounds for the

area under the ROC curve. J. Mach. Learn. Res.  6:393–425  2005.

[2] L. Breiman  J. Friedman  R. Olshen  and C. Stone. Classiﬁcation and Regression Trees.

Wadsworth and Brooks  1984.

[3] G. Cl´emenc¸on  M. Depecker  and N. Vayatis. Ranking Forests. J. Mach. Learn. Res.  14:39–73 

2013.

[4] S. Cl´emenc¸on  G. Lugosi  and N.Vayatis. Ranking and scoring using empirical risk minimiza-

tion. In Proceedings of COLT 2005  volume 3559  pages 1–15. Springer.  2005.

[5] S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and empirical risk minimization of u-

statistics. The Annals of Statistics  36:844–874  2008.

[6] S. Cl´emenc¸on and S. Robbiano. The TreeRank Tournament algorithm for multipartite ranking.

Journal of Nonparametric Statistics  25(1):107–126  2014.

[7] S. Cl´emenc¸on and N. Vayatis. Tree-based ranking methods. IEEE Transactions on Information

Theory  55(9):4316–4336  2009.

[8] S. Cl´emenc¸on and N. Vayatis. The RankOver algorithm: overlaid classiﬁcation rules for opti-

mal ranking. Constructive Approximation  32:619–648  2010.

[9] Corinna Cortes and Mehryar Mohri. Auc optimization vs. error rate minimization. In Advances

in neural information processing systems  pages 313–320  2004.

[10] L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer 

1996.

[11] Y. Freund  R. D. Iyer  R. E. Schapire  and Y. Singer. An efﬁcient boosting algorithm for

combining preferences. Journal of Machine Learning Research  4:933–969  2003.

[12] Aditya Krishna Menon and Robert C Williamson. Bipartite ranking: a risk-theoretic perspec-

tive. Journal of Machine Learning Research  17(195):1–102  2016.

[13] J.R. Quinlan. Induction of Decision Trees. Machine Learning  1(1):1–81  1986.

[14] S. Rajaram and S. Agarwal. Generalization bounds for k-partite ranking. In NIPS 2005 Work-

shop on Learn to rank  2005.

[15] A. Rakotomamonjy. Optimizing Area Under Roc Curve with SVMs. In Proceedings of the

First Workshop on ROC Analysis in AI  2004.

[16] S. Robbiano S. Cl´emenc¸on and N. Vayatis. Ranking data with ordinal labels: optimality and

pairwise aggregation. Machine Learning  91(1):67–104  2013.

9

,Stéphan Clémençon
Mastane Achab