2014,A provable SVD-based algorithm for learning topics in dominant admixture corpus,Topic models  such as Latent Dirichlet Allocation (LDA)  posit that documents are drawn from admixtures of distributions over words  known as topics. The inference problem of recovering topics from such a collection of documents drawn from admixtures  is NP-hard. Making a strong assumption called separability  [4] gave the first provable algorithm for inference. For the widely used LDA model  [6] gave a provable algorithm using clever tensor-methods. But [4  6] do not learn topic vectors with bounded $l_1$ error (a natural measure for probability vectors). Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural  simple components such as SVD  which provably solves the inference problem for the model with bounded $l_1$ error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this  we introduce topic specific Catchwords  a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption  which is empirically verified on real corpora  a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding  can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others. Apart from the simplicity of the algorithm  the sample complexity has near optimal dependence on $w_0$  the lowest probability that a topic is dominant  and is better than [4]. Empirical evidence shows that on several real world corpora  both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].,A provable SVD-based algorithm for learning topics

in dominant admixture corpus

Trapit Bansal†  C. Bhattacharyya‡∗

Department of Computer Science and Automation

Indian Institute of Science
Bangalore -560012  India

†trapitbansal@gmail.com
‡chiru@csa.iisc.ernet.in

Ravindran Kannan
Microsoft Research

India

kannan@microsoft.com

Abstract

Topic models  such as Latent Dirichlet Allocation (LDA)  posit that documents
are drawn from admixtures of distributions over words  known as topics. The
inference problem of recovering topics from such a collection of documents drawn
from admixtures  is NP-hard. Making a strong assumption called separability  [4]
gave the ﬁrst provable algorithm for inference. For the widely used LDA model 
[6] gave a provable algorithm using clever tensor-methods. But [4  6] do not learn
topic vectors with bounded l1 error (a natural measure for probability vectors).
Our aim is to develop a model which makes intuitive and empirically supported
assumptions and to design an algorithm with natural  simple components such as
SVD  which provably solves the inference problem for the model with bounded l1
error. A topic in LDA and other models is essentially characterized by a group of
co-occurring words. Motivated by this  we introduce topic speciﬁc Catchwords 
a group of words which occur with strictly greater frequency in a topic than any
other topic individually and are required to have high frequency together rather
than individually. A major contribution of the paper is to show that under this
more realistic assumption  which is empirically veriﬁed on real corpora  a singu-
lar value decomposition (SVD) based algorithm with a crucial pre-processing step
of thresholding  can provably recover the topics from a collection of documents
drawn from Dominant admixtures. Dominant admixtures are convex combination
of distributions in which one distribution has a signiﬁcantly higher contribution
than the others. Apart from the simplicity of the algorithm  the sample complexity
has near optimal dependence on w0  the lowest probability that a topic is domi-
nant  and is better than [4]. Empirical evidence shows that on several real world
corpora  both Catchwords and Dominant admixture assumptions hold and the pro-
posed algorithm substantially outperforms the state of the art [5].

1

Introduction

Topic models [1] assume that each document in a text corpus is generated from an ad-mixture of
topics  where  each topic is a distribution over words in a Vocabulary. An admixture is a convex
combination of distributions. Words in the document are then picked in i.i.d. trials  each trial has a
multinomial distribution over words given by the weighted combination of topic distributions. The
problem of inference  recovering the topic distributions from such a collection of documents  is
provably NP-hard. Existing literature pursues techniques such as variational methods [2] or MCMC
procedures [3] for approximating the maximum likelihood estimates.

∗http://mllab.csa.iisc.ernet.in/tsvd

1

Given the intractability of the problem one needs further assumptions on topics to derive polynomial
time algorithms which can provably recover topics. A possible (strong) assumption is that each
document has only one topic but the collection can have many topics. A document with only one
topic is sometimes referred as a pure topic document. [7] proved that a natural algorithm  based
on SVD  recovers topics when each document is pure and in addition  for each topic  there is a set
of words  called primary words  whose total frequency in that topic is close to 1. More recently 
[6] show using tensor methods that if the topic weights have Dirichlet distribution  we can learn
the topic matrix. Note that while this allows non-pure documents  the Dirichlet distribution gives
essentially uncorrelated topic weights.
In an interesting recent development [4  5] gave the ﬁrst provable algorithm which can recover topics
from a corpus of documents drawn from admixtures  assuming separability. Topics are said to be
separable if in every topic there exists at least one Anchor word. A word in a topic is said to be an
Anchor word for that topic if it has a high probability in that topic and zero probability in remaining
topics. The requirement of high probability in a topic for a single word is unrealistic.

Our Contributions: Topic distributions  such as those learnt in LDA  try to model the co-
occurrence of a group of words which describes a theme. Keeping this in mind we introduce the
notion of Catchwords. A group of words are called Catchwords of a topic  if each word occurs
strictly more frequently in the topic than other topics and together they have high frequency. This
is a much weaker assumption than separability. Furthermore we observe  empirically  that posterior
topic weights assigned by LDA to a document often have the property that one of the weights is
signiﬁcantly higher than the rest. Motivated by this observation  which has not been exploited by
topic modeling literature  we suggest a new assumption. It is natural to assume that in a text corpus 
a document  even if it has multiple themes  will have an overarching dominant theme. In this paper
we focus on document collections drawn from dominant admixtures. A document collection is said
to be drawn from a dominant admixture if for every document  there is one topic whose weight is
signiﬁcantly higher than the other topics and in addition  for every topic  there is a small fraction of
documents which are nearly purely on that topic. The main contribution of the paper is to show that
under these assumptions  our algorithm  which we call TSVD  indeed provably ﬁnds a good approx-
imation in total l1 error to the topic matrix. We prove a bound on the error of our approximation
which does not grow with dictionary size d  unlike [5] where the error grows linearly with d.
Empirical evidence shows that on semi-synthetic corpora constructed from several real world
datasets  as suggested by [5]  TSVD substantially outperforms the state of the art [5]. In partic-
ular it is seen that compared to [5] TSVD gives 27% lower error in terms of l1 recovery on 90% of
the topics.

smaller. Let Sk = {x = (x1  x2  . . .   xk) : xl ≥ 0;(cid:80)

Problem Deﬁnition: d  k  s will denote respectively  the number of words in the dictionary  num-
ber of topics and number of documents. d  s are large  whereas  k is to be thought of as much
l xl = 1}. For each topic  there is a ﬁxed
vector in Sk giving the probability of each word in that topic. Let M be the d × k matrix with these
vectors as its columns.
Documents are picked in i.i.d.
To pick document j  one ﬁrst picks a k-vector
W1j  W2j  . . .   Wkj of topic weights according to a ﬁxed distribution on Sk. Let P· j = MW· j
be the weighted combination of the topic vectors. Then the m words of the document are picked in
i.i.d. trials; each trial picks a word according to the multinomial distribution with P· j as the proba-
bilities. All that is given as data is the frequency of words in each document  namely  we are given
the d × s matrix A  where Aij = Number of occurrences of word i in Document j
. Note that E(A|W) = P 
where  the expectation is taken entry-wise.
In this paper we consider the problem of ﬁnding M given A.

trials.

m

2 Previous Results

In this section we review literature related to designing provable algorithms for topic models. For an
overview of topic models we refer the reader to the excellent survey of [1]. Provable algorithms for
recovering topic models was started by [7]. Latent Semantic Indexing (LSI) [8] remains a successful
method for retrieving similar documents by using SVD. [7] showed that one can recover M from a

2

collection of documents  with pure topics  by using SVD based procedure under the additional Pri-
mary Words assumption. [6] showed that in the admixture case  if one assumes Dirichlet distribution
for the topic weights  then  indeed  using tensor methods  one can learn M to l2 error provided some
added assumptions on numerical parameters like condition number are satisﬁed.
The ﬁrst provably polynomial time algorithm for admixture corpus was given in [4  5]. For a topic
l  a word i is an anchor word if: Mi l ≥ p0 and Mi l(cid:48) = 0 ∀l(cid:48) (cid:54)= l.
Theorem 2.1 [4] If every topic has an anchor word  there is a polynomial time algorithm that
returns an ˆM such that with high probability 

k(cid:88)

d(cid:88)

l=1

i=1

| ˆMil − Mil| ≤ dε provided s ≥ Max

O

(cid:26)

(cid:18) k6 log d

(cid:19)

a4ε2p6

0γ2m

(cid:18) k4

(cid:19)(cid:27)

  O

γ2a2

 

where  γ is the condition number of E(W W T )  a is the minimum expected weight of a topic and m
is the number of words in each document.

Note that the error grows linearly in the dictionary size d  which is often large. Note also the
dependence of s on parameters p0  which is  1/p6
0 and on a  which is 1/a4. If  say  the word “run” is
an anchor word for the topic “baseball” and p0 = 0.1  then the requirement is that every 10 th word
in a document on this topic is “run”. This seems too strong to be realistic. It would be more realistic
to ask that a set of words like - “run”  “hit”  “score”  etc. together have frequency at least 0.1 which
is what our catchwords assumption does.

3 Learning Topics from Dominant Admixtures

Informally  a document is said to be drawn from a Dominant Admixture if the document has one
dominant topic. Besides its simplicity  we show empirical evidence from real corpora to demonstrate
that topic dominance is a reasonable assumption. The Dominant Topic assumption is weaker than
the Pure Topic assumption. More importantly  SVD based procedures proposed by [7] will not
apply. Inspired by the Primary Words assumption we introduce the assumption that each topic has a
set of Catchwords which individually occur more frequently in that topic than others. This is again
a much weaker assumption than both Primary Words and Anchor Words assumptions and can be
veriﬁed experimentally. In this section we establish that by applying SVD on a matrix  obtained by
thresholding the word-document matrix  and subsequent k-means clustering can learn topics having
Catchwords from a Dominant Admixture corpus.

3.1 Assumptions: Catchwords and Dominant admixtures
δ ≤ 0.08
Let α  β  ρ  δ  ε0 be non-negative reals satisfying: β + ρ ≤ (1 − δ)α 
Dominant topic Assumption (a) For j = 1  2  . . .   s  document j has a dominant topic l(j) such
that Wl(j) j ≥ α and Wl(cid:48)j ≤ β  ∀l(cid:48) (cid:54)= l(j).
(b) For each topic l  there are at least ε0w0s documents in each of which topic l has weight at least
1 − δ.
Catchwords Assumption: There are k disjoint sets of words - S1  S2  . . .   Sk such that with ε

deﬁned in (5)  ∀i ∈ Sl  ∀l(cid:48) (cid:54)= l  Mil(cid:48) ≤ ρMil  (cid:80)

α + 2δ ≤ 0.5 

i∈Sl
∀i ∈ Sl  mδ2αMil ≥ 8 ln

Mil ≥ p0 

(cid:18) 20

(cid:19)

εw0

.

(1)

Part (b) of the Dominant Topic Assumption is in a sense necessary for “identiﬁability” - namely for
the model to have a set of k document vectors so that every document vector is in the convex hull of
these vectors. The Catchwords assumption is natural to describe a theme as it tries to model a unique
group of words which is likely to co-occur when a theme is expressed. This assumption is close to
topics discovered by LDA like models  which try to model co-occurence of words. If α  δ ∈ Ω(1) 
then  the assumption (1) says Mil ∈ Ω∗(1/m). In fact if Mil ∈ o(1/m)  we do not expect to see
word i (in topic l)  so it cannot be called a catchword at all.

3

A slightly different (but equivalent) description of the model will be useful to keep in mind. What
is ﬁxed (not stochastic) are the matrices M and the distribution of the weight matrix W. To pick
document j  we can ﬁrst pick the dominant topic l in document j and condition the distribution of
W· j on this being the dominant topic. One could instead also think of W· j being picked from a
l=1 MilWlj and pick the m words of the document

mixture of k distributions. Then  we let Pij =(cid:80)k

in i.i.d multinomial trials as before. We will assume that

Tl = {j : l is the dominant topic in document j} satisﬁes |Tl| = wls 

where  wl is the probability of topic l being dominant. This is only approximately valid  but the
error is small enough that we can disregard it.
For ζ ∈ {0  1  2  . . .   m}  let pi(ζ  l) be the probability that j ∈ Tl and Aij = ζ/m and qi(ζ  l) the
corresponding “empirical probability”:

(cid:90)

(cid:18)m

(cid:19)

pi(ζ  l) =

W· j

ζ

ij(1 − Pij)m−ζProb(W· j | j ∈ Tl) Prob(j ∈ Tl)  where  P· j = MW· j.
P ζ
(2)

qi(ζ  l) =

1
s

|{j ∈ Tl : Aij = ζ/m}| .

(3)

Note that pi(ζ  l) is a real number  whereas  qi(ζ  l) is a random variable with E(qi(ζ  l)) = pi(ζ  l).
We need a technical assumption on the pi(ζ  l) (which is weaker than unimodality).
No-Local-Min Assumption: We assume that pi(ζ  l) does not have a local minimum  in the sense:
(4)

pi(ζ  l) > Min(pi(ζ − 1  l)  pi(ζ + 1  l)) ∀ ζ ∈ {1  2  . . .   m − 1}.

The justiﬁcation for this assumption is two-fold. First  generally  Zipf’s law kind of behaviour where
the number of words plotted against relative frequencies declines as a power function has often been
observed. Such a plot is monotonically decreasing and indeed satisﬁes our assumption. But for
Catchwords  we do not expect this behaviour - indeed  we expect the curve to go up initially as the
relative frequency increases  then reach a maximum and then decline. This is a unimodal function
and also satisﬁes our assumption.
Relative sizes of parameters: Before we close this section  a discussion on the values of the pa-
rameters is in order. Here  s is large. For asymptotic analysis  we can think of it as going to inﬁnity.
1/w0 is also large and can be thought of as going to inﬁnity. [In fact  if 1/w0 ∈ O(1)  then  in-
tuitively  we see that there is no use of a corpus of more than constant size - since our model has
i.i.d. documents  intuitively  the number of samples we need should depend mainly on 1/w0]. m is
(much) smaller  but need not be constant.
c refers to a generic constant independent of m  s  1/w0  ε  δ; its value may be different in different
contexts.

3.2 The TSVD Algorithm

Existing SVD based procedures for clustering on raw word-document matrices fail because the
spread of frequencies of a word within a topic is often more (at least not signiﬁcantly less) than the
gap between the word’s frequencies in two different topics. Hypothetically  the frequency for the
word run  in the topic Sports  may range upwards of 0.01  say. But in other topics  it may range
from  say  0 to 0.005. The success of the algorithm will lie on correctly identifying the dominant
topics such as sports by identifying that the word run has occurred with high frequency. In this
example  the gap (0.01-0.005) between Sports and other topics is less than the spread within Sports
(1.0-0.01)  so a 2-clustering approach (based on SVD) will split the topic Sports into two. While
this is a toy example  note that if we threshold the frequencies at say 0.01  ideally  sports will be all
above and the rest all below the threshold  making the succeeding job of clustering easy.
There are several issues in extending beyond the toy case. Data is not one-dimensional. We will use
different thresholds for each word; word i will have a threshold ζi/m. Also  we have to compute
ζi/m. Ideally we would not like to split any Tl  namely  we would like that for each l and and each
i  either most j ∈ Tl have Aij > ζi/m or most j ∈ Tl have Aij ≤ ζi/m. We will show that

4

our threshold procedure indeed achieves this. One other nuance: to avoid conditioning  we split
the data A into two parts A(1) and A(2)  compute the thresholds using A(1) and actually do the
thresholding on A(2). We will assume that the intial A had 2s columns  so each part now has s
columns. Also  T1  T2  . . .   Tk partitions the columns of A(1) as well as those of A(2). The columns
of thresholded matrix B are then clustered  by a technique we call Project and Cluster  namely 
we project the columns of B to its k−dimensional SVD subspace and cluster in the projection.
The projection before clustering has recently been proven [9] (see also [10]) to yield good starting
cluster centers. The clustering so found is not yet satisfactory. We use the classic Lloyd’s k-means
algorithm proposed by [12]. As we will show  the partition produced after clustering  {R1  . . .   Rk}
of A(2) is close to the partition induced by the Dominant Topics  {T1  . . .   Tk}. Catchwords of topic
l are now (approximately) identiﬁed as the most frequently occurring words in documents in Rl.
Finally  we identify nearly pure documents in Tl (approximately) as the documents in which the
catchwords occur the most. Then we get an approximation to M· l by averaging these nearly pure
documents. We now describe the precise algorithm.

3.3 Topic recovery using Thresholded SVD

Threshold SVD based K-means (TSVD)

(cid:18) 1

900c2
0

√

√
ε0
αp0δ
k
640m

αp0
k3m

 

(cid:19)

 

.

(5)

ε = Min

1. Randomly partition the columns of A into two matrices A(1) and A(2) of s columns each.
2. Thresholding

(a) Compute Thresholds on A(1) For each i  let ζi be the highest value of ζ ∈

{0  1  2  . . .   m} such that |{j : A(1)

ij > ζ

(b) Do the thresholding on A(2): Bij =

(cid:40)√
m}| ≥ w0
ζi

2 s; |{j : A(1)
if A(2)
otherwise

0

ij = ζ

m}| ≤ 3εw0s.

ij > ζi/m and ζi ≥ 8 ln(20/εw0)

.

3. SVD Find the best rank k approximation B(k) to B.
4. Identify Dominant Topics

(a) Project and Cluster Find (approximately) optimal k-means clustering of the columns

of B(k).

(b) Lloyd’s Algorithm Using the clustering found in Step 4(a) as the starting clustering 
(c) Let R1  R2  . . .   Rk be the k−partition of [s] corresponding to the clustering after

apply Lloyd’s k-means algorithm to the columns of B (B  not B(k)).
Lloyd’s. //*Will prove that Rl ≈ Tl*//

(a) For each i  l  compute g(i  l) = the ((cid:98)ε0w0s/2(cid:99))th highest element of {A(2)

mδ2 ln(20/εw0)  Maxl(cid:48)(cid:54)=lγ g(i  l(cid:48))(cid:1)(cid:9)   where  γ =

: j ∈ Rl}.

ij

5. Identify Catchwords

(b) Let Jl = (cid:8)i : g(i  l) > Max(cid:0) 4
6. Find Topic Vectors Find the (cid:98)ε0w0s/2(cid:99) highest(cid:80)

(1+δ)(β+ρ).

1−2δ

ij among all j ∈ [s] and return
A(2)
the average of these A· j as our approximation ˆM· l to M· l.

i∈Jl

Theorem 3.1 Main Theorem Under the Dominant Topic  Catchwords and No-Local-Min assump-
tions  the algorithm succeeds with high probability in ﬁnding an ˆM so that

|Mil − ˆMil| ∈ O(kδ)  provided 1s ∈ Ω∗(cid:18) 1

(cid:18) k6m2

(cid:19)(cid:19)

.

w0

α2p2
0

+

m2k
ε2
0δ2αp0

+

d

ε0δ2

(cid:88)

i l

1The superscript ∗ hides a logarithmic factor in dsk/δfail  where  δfail > 0 is the desired upper bound on the

probability of failure.

5

√

√

√

Mil/

A note on the sample complexity is in order. Notably  the dependence of s on w0 is best possible
(namely s ∈ Ω∗(1/w0)) within logarithmic factors  since  if we had fewer than 1/w0 documents  a
topic which is dominant with probability only w0 may have none of the documents in the collection.
The dependence of s on d needs to be at least d/ε0w0δ2: to see this  note that we only assume
that there are r = O(ε0w0s) nearly pure documents on each topic. Assuming we can ﬁnd this
set (the algorithm approximately does)  their average has standard deviation of about
r in
coordinate i. If topic vector M· l has O(d) entries  each of size O(1/d)  to get an approximation
of M· l to l1 error δ  we need the per coordinate error 1/
dr to be at most δ/d which implies
s ≥ d/ε0w0δ2. Note that to get comparable error in [4]  we need a quadratic dependence on d.
There is a long sequence of Lemmas to prove the theorem. To improve the readability of the paper
we relegate the proofs to supplementary material [14]. The essence of the proof lies in proving
that the clustering step correctly identiﬁes the partition induced by the dominant topics. For this 
we take advantage of a recent development on the k−means algorithm from [9] [see also [10]] 
where  it is shown that under a condition called the Proximity Condition  Lloyd’s k means algorithm
starting with the centers provided by the SVD-based algorithm  correctly identiﬁes almost all the
documents’ dominant topics. We prove that indeed the Proximity Condition holds. This calls for
machinery from Random Matrix theory (in particular bounds on singular values). We prove that the
singular values of the thresholded word-document matrix are nicely bounded. Once the dominant
topic of each document is identiﬁed  we are able to ﬁnd the Catchwords for each topic. Now  we
rely upon part (b.) of the Dominant Topic assumption : that is there is a small fraction of nearly Pure
Topic-documents for each topic. The Catchwords help isolate the nearly pure-topic documents and
hence ﬁnd the topic vectors. The proofs are complicated by the fact that each step of the algorithm
induces conditioning on the data – for example  after clustering  the document vectors in one cluster
are not independent anymore.

4 Experimental Results

We compare the thresholded SVD based k-means (TSVD2) algorithm 3.3 with the algorithms of
[5]  Recover-KL and Recover-L2  using the code made available by the authors3. We observed
the results of Recover-KL to be better than Recover-L2  and report here the results of Recover-KL
(abbreviated R-KL)  full set of results can be found in supplementary section 5. We ﬁrst provide
empirical support for the algorithm assumptions in Section 3.1  namely the dominant topic and the
catchwords assumption. Then we show on 4 different semi-synthetic data that TSVD provides as
good or better recovery of topics than the Recover algorithms. Finally on real-life datasets  we show
that the algorithm performs as well as [5] in terms of perplexity and topic coherence.

k   ε = 1

Implementation Details: TSVD parameters (w0  ε  ε0  γ) are not known in advance for real cor-
pus. We tested empirically for multiple settings and the following values gave the best performance.
Thresholding parameters used were: w0 = 1
6. For ﬁnding the catchwords  γ = 1.1  ε0 = 1
in step 5. For ﬁnding the topic vectors (step 6)  taking the top 50% (ε0w0 = 1
k ) gave empirically
better results. The same values were used on all the datasets tested. The new algorithm is sensitive
to the initialization of the ﬁrst k-means step in the projected SVD space. To remedy this  we run 10
independent random initializations of the algorithm with K-Means++ [13] and report the best result.
Datasets: We use four real word datasets in the experiments. As pre-processing steps we removed
standard stop-words  selected the vocabulary size by term-frequency and removed documents with
less than 20 words. Datasets used are: (1) NIPS4: Consists of 1 500 NIPS full papers  vocabulary
of 2 000 words and mean document length 1023. (2) NYT4: Consists of a random subset of 30 000
documents from the New York Times dataset  vocabulary of 5 000 words and mean document length
238. (3) Pubmed4: Consists of a random subset of 30 000 documents from the Pubmed abstracts
dataset  vocabulary of 5 030 words and mean document length 58. (4) 20NewsGroup5 (20NG):
Consist of 13 389 documents  vocabulary of 7 118 words and mean document length 160.

3

2Resources available at: http://mllab.csa.iisc.ernet.in/tsvd
3http://www.cs.nyu.edu/˜halpern/files/anchor-word-recovery.zip
4http://archive.ics.uci.edu/ml/datasets/Bag+of+Words
5http://qwone.com/˜jason/20Newsgroups

6

Corpus
NIPS
NYT

Pubmed
20NG

s

1500
30000
30000
13389

k
50
50
50
20

% s with Dominant % s with Pure % Topics CW Mean
Topics (δ = 0.05) with CW Frequency

Topics (α = 0.4)

56.6%
63.7%
62.2%
74.1%

2.3%
8.5%
5.1%
39.5%

96%
98%
78%
85%

0.05
0.07
0.05
0.06

Table 1: Algorithm Assumptions. For dominant topic assumption  fraction of documents with satisfy
the assumption for (α  β) = (0.4  0.3) are shown. % documents with almost pure topics (δ = 0.05 
i.e. 95% pure) are also shown. Last two columns show results for catchwords (CW) assumption.

4.1 Algorithm Assumptions

To check the dominant topic and catchwords assumptions  we ﬁrst run 1000 iterations of Gibbs
sampling on the real corpus and learn the posterior document-topic distribution ({W. j}) for each
document in the corpus (by averaging over 10 saved-states separated by 50 iterations after the 500
burn-in iterations). We will use this posterior document-topic distribution as the document generat-
ing distribution to check the two assumptions.
Dominant topic assumption: Table 1 shows the fraction of the documents in each corpus which
satisfy this assumption with α = 0.4 (minimum probability of dominant topic) and β = 0.3 (maxi-
mum probability of non-dominant topics). The fraction of documents which have almost pure topics
with highest topic weight at least 0.95 (δ = 0.05) is also shown. The results indicate that the domi-
nant topic assumption is well justiﬁed (on average 64% documents satisfy the assumption) and there
is also a substantial fraction of documents satisfying almost pure topic assumption.
Catchwords assumption: We ﬁrst ﬁnd a k-clustering of the documents {T1  . . .   Tk} by assigning
all documents which have highest posterior probability for the same topic into one cluster. Then
we use step 5 of TSVD (Algorithm 3.3) to ﬁnd the set of catchwords for each topic-cluster  i.e.
{S1  . . .   Sk}  with the parameters: 0w0 = 1
3k   γ = 2.3 (taking into account constraints in Section
3.1  α = 0.4  β = 0.3  δ = 0.05  ρ = 0.07). Table 1 reports the fraction of topics with non-empty
set of catchwords and the average per topic frequency of the catchwords6. Results indicate that
most topics on real data contain catchwords (Table 1  second-last column). Moreover  the average
per-topic frequency of the group of catchwords for that topic is also quite high (Table 1  last column).

4.2 Empirical Results

Semi-synthetic Data: Following [5]  we generate semi-synthetic corpora from LDA model trained
by MCMC  to ensure that the synthetic corpora retain the characteristics of real data. Gibbs sam-
pling7 is run for 1000 iterations on all the four datasets and the ﬁnal word-topic distribution is used
to generate varying number of synthetic documents with document-topic distribution drawn from a
symmetric Dirichlet with hyper-parameter 0.01. For NIPS  NYT and Pubmed we use k = 50 topics 
for 20NewsGroup k = 20  and mean document lengths of 1000  300  100 and 200 respectively. Note
that the synthetic data is not guaranteed to satisfy dominant topic assumption for every document
(on average about 80% documents satisfy the assumption for value of (α  β) tested in Section 4.1).
Topic Recovery on Semi-synthetic Data: We learn the word-topic distribution ( ˆM) for the semi-
synthetic corpora using TSVD and the Recover algorithms of [5]. Given these learned topic dis-
tributions and the original data-generating distribution (M)  we align the topics of M and ˆM by
bipartite matching and evaluate the l1 distance between each pair of topics. We report the average
of l1 error across topics (called l1 reconstruction-error [5]) in Table 2 for TSVD and Recover-KL
(R-KL). TSVD has smaller error on most datasets than the R-KL algorithm. We observed perfor-
mance of TSVD to be always better than Recover-L2 (see supplement Table 1 for full results). Best
performance is observed on NIPS which has largest mean document length  indicating that larger
m leads to better recovery. Results on 20NG are slightly worse than R-KL for smaller sample size 
but performance improves for larger number of documents. While the error-values in Table 2 are

6(cid:16) 1

k

(cid:80)k

(cid:80)

(cid:80)

(cid:17)

l=1

1|Tl|

i∈Sl

j∈Tl

Aij

7Dirichlet hyperparameters used: document-topic = 0.03 and topic-word = 1

7

Corpus

Documents

NIPS

Pubmed

20NG

NYT

40 000
50 000
60 000
40 000
50 000
60 000
40 000
50 000
60 000
40 000
50 000
60 000

R-KL
0.308
0.308
0.311
0.332
0.326
0.328
0.120
0.114
0.110
0.208
0.206
0.200

TSVD

0.115 (62.7%)
0.145 (52.9%)
0.131 (57.9%)
0.288 (13.3%)
0.280 (14.1%)
0.284 (13.4%)
0.124 (-3.3%)
0.113 (0.9%)
0.106 (3.6%)
0.195 (6.3%)
0.185 (10.2%)
0.194 (3.0%)

Figure 1: Histogram of l1 error across
topics (40 000 documents). TSVD(blue 
solid border) gets smaller error on most
topics than R-KL(green  dashed border).

Table 2: l1 reconstruction error on various semi-synthetic
datasets. Brackets in the last column give percent improve-
ment over R-KL (best performing Recover algorithm).
Full results in supplementary.

averaged values across topics  Figure 1 shows that TSVD algorithm achieves much better topic re-
covery for majority of the topics (>90%) on most datasets (overall average improvement of 27% 
full results in supplement Figure 1).
Topic Recovery on Real Data: To evaluate perplexity [2] on real data  the held-out sets con-
sist of 350 documents for NIPS  10000 documents for NYT and Pubmed  and 6780 documents for
20NewsGroup. TSVD achieved perplexity measure of 835 (NIPS)  1307 (Pubmed)  1555 (NYT) 
2390 (20NG) while Recover-KL achieved 754 (NIPS)  1188 (Pubmed)  1579 (NYT)  2431 (20NG)
(refer to supplement Table 2 for complete results). TSVD gives comparable perplexity with Recover-
KL  results being slightly better on NYT and 20NewsGroup which are larger datasets with moder-
ately high mean document lengths. We also ﬁnd comparable results on Topic Coherence [11] (see
Table 2 in supplementary for topic coherence results and Table 3 for list of top words of topics).
Summary: We evaluated the proposed algorithm  TSVD  rigorously on multiple datasets with re-
spect to the state of the art [5] (Recover-KL and Recover-L2)  following the evaluation methodology
of [5]. In Table 2 we show that the l1 reconstruction error for the new algorithm is small and on
average 19.6% better than the best results of the Recover algorithms [5]. In Figure 1  we show that
TSVD achieves signiﬁcantly better recover on majority of the topics. We also demonstrate that on
real datasets the algorithm achieves comparable perplexity and topic coherence to Recover algo-
rithms. Moreover  we show on multiple real world datasets that the algorithm assumptions are well
justiﬁed in practice.

Conclusion

Real world corpora often exhibits the property that in every document there is one topic dominantly
present. A standard SVD based procedure will not be able to detect these topics  however TSVD 
a thresholded SVD based procedure  as suggested in this paper  discovers these topics. While SVD
is time-consuming  there have been a host of recent sampling-based approaches which make SVD
easier to apply to massive corpora which may be distributed among many servers. We believe that
apart from topic recovery  thresholded SVD can be applied even more broadly to similar problems 
such as matrix factorization  and will be the basis for future research.

Acknowledgements TB was supported by a Department of Science and Technology (DST) grant.

References
[1] Blei  D. Introduction to probabilistic topic models. Communications of the ACM  pp. 77–84 

2012.

8

0102030400.000.150.300.450.600.750.901.051.20NIPS0102030400.000.150.300.450.600.750.901.051.20NYTL1 Reconstruction ErrorAlgorithmR−KLTSVDNumber of Topics[2] Blei  D.  Ng  A.  and Jordan  M. Latent Dirichlet allocation. Journal of Machine Learning Re-
search  pp. 3:993–1022  2003. Preliminary version in Neural Information Processing Systems
2001.

[3] Grifﬁths  T. L. and Steyvers  M. Finding scientiﬁc topics. Proceedings of the National Academy

of Sciences  101:5228–5235  2004.

[4] Arora  S.  Ge  R.  and Moitra  A. Learning topic models – going beyond SVD. In Foundations

of Computer Science  2012b.

[5] Arora  S.  Ge  R.  Halpern  Y.  Mimno  D.  Moitra  A.  Sontag  D.  Wu  Y.  and Zhu M. A
practical algorithm for topic modeling with provable guarantees. In Internation Conference on
Machine Learning  2013

[6] Anandkumar  A.  Foster  D.  Hsu  D.  Kakade  S.  and Liu  Y. A Spectral Algorithm for Latent

Dirichlet Allocation In Neural Information Processing Systems  2012.

[7] Papadimitriou  C.  Raghavan  P.  Tamaki H.  and Vempala S. Latent semantic indexing: a prob-
abilistic analysis. Journal of Computer and System Sciences  pp. 217–235  2000. Preliminary
version in PODS 1998.

[8] Deerwester  S.  Dumais  S.  Landauer  T.  Furnas  G.  and Harshman  R. Indexing by latent
semantic analysis. Journal of the American Society for Information Science  pp. 391–407 
1990.

[9] Kumar  A.  and Kannan  R. Clustering with spectral norm and the k-means algorithm.

Foundations of Computer Science  2010

In

[10] Awashti  P.  and Sheffet  O. Improved spectral-norm bounds for clustering. In Proceedings of

Approx/Random  2012.

[11] Mimno  D.  Wallach  H.  Talley  E.  Leenders  M. and McCallum  A. Optimizing semantic
coherence in topic models. In Empirical Methods in Natural Language Processing  pp. 262–
272  2011.

[12] Lloyd  Stuart P. Least squares quantization in PCM  IEEE Transactions on Information Theory

28 (2): 129137 1982.

[13] Arthur  D.  and Vassilvitskii  S. K-means++: The advantages of careful seeding. In Proceed-

ings of ACM-SIAM symposium on Discrete algorithms  pp. 1027–1035  2007

[14] Supplementary material

9

,Trapit Bansal
Chiranjib Bhattacharyya
Ravindran Kannan
Chuan-Yung Tsai
Andrew Saxe
Andrew Saxe
David Cox