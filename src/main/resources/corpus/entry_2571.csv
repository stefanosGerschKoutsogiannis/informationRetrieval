2018,Improved Algorithms for Collaborative PAC Learning,We study a recent model of collaborative PAC learning where $k$ players with $k$ different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small error on all tasks  there is a collaborative algorithm that finds a single classifier for all tasks and has $O((\ln (k))^2)$ times the worst-case sample complexity for learning a single task. In this work  we design new algorithms for both the realizable and the non-realizable setting  having sample complexity only $O(\ln (k))$ times the worst-case sample complexity for learning a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters are even better than previous algorithms that are allowed to output different classifiers for different tasks.,Improved Algorithms for Collaborative PAC

Learning

Huy Lê Nguy˜ên

Northeastern University

Boston  MA 02115

Lydia Zakynthinou

Northeastern University

Boston  MA 02115

College of Computer and Information Science

College of Computer and Information Science

hu.nguyen@northeastern.edu

zakynthinou.l@northeastern.edu

Abstract

We study a recent model of collaborative PAC learning where k players with
k different tasks collaborate to learn a single classiﬁer that works for all tasks.
Previous work showed that when there is a classiﬁer that has very small error
on all tasks  there is a collaborative algorithm that ﬁnds a single classiﬁer for all
tasks and has O((ln(k))2) times the worst-case sample complexity for learning
a single task. In this work  we design new algorithms for both the realizable
and the non-realizable setting  having sample complexity only O(ln(k)) times the
worst-case sample complexity for learning a single task. The sample complexity
upper bounds of our algorithms match previous lower bounds and in some range
of parameters are even better than previous algorithms that are allowed to output
different classiﬁers for different tasks.

1

Introduction



d ln

(cid:17)

(cid:16) 1



(cid:16)

(cid:16) 1

δ

(cid:16) 1

(cid:17)(cid:17)(cid:17)

There has been a lot of work in machine learning concerning learning multiple tasks simultaneously 
ranging from multi-task learning [3  4]  to domain adaptation [10  11]  to distributed learning [2  7  14].
Another area in similar spirit to this work is meta-learning  where one leverages samples from many
different tasks to train a single algorithm that adapts well to all tasks (see e.g. [8]).
In this work  we focus on a model of collaborative PAC learning  proposed by [5]. In the classic
PAC learning setting introduced by [13]  where PAC stands for probably approximately correct 
the goal is to learn a task by drawing from a distribution of samples. The optimal classiﬁer that
achieves the lowest error on the task with respect to the given distribution is assumed to come
from a concept class F of VC dimension d. The VC theorem [1] states that for any instance
labeled samples sufﬁce to learn a classiﬁer that achieves low
m δ = O
error with probability at least 1 − δ  where the error depends on .
In the collaborative model  there are k players attempting to learn their own tasks  each task involving
a different distribution of samples. The goal is to learn a single classiﬁer that also performs well
on all the tasks. One example from [5]  which motivates this problem  is having k hospitals with
different patient demographics which want to predict the overall occurrence of a disease. In this case 
it would be more ﬁtting as well as cost efﬁcient to develop and distribute a single classiﬁer to all the
hospitals. In addition  the requirement for a single classiﬁer is imperative in settings where there are
fairness concerns. For example  consider the case that the goal is to ﬁnd a classiﬁer that predicts loan
defaults for a bank by gathering information from bank stores located in neighborhoods with diverse
socioeconomic characteristics. In this setting  the samples provided by each bank store come from
different distributions while it is desired to guarantee low error rates for all the neighborhoods. Again 
in this setting  the bank should employ a single classiﬁer among all the neighborhoods.

+ ln

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(cid:17)(cid:17)
(cid:16) k
(cid:16) ln(k)

δ

(cid:16)

(cid:17)

(cid:16) 1



(cid:17)(cid:17)(cid:17)

(cid:16) 1

δ

If each player were to learn a classiﬁer for their task without collaboration  they would each have to
draw a sufﬁcient number of samples from their distribution to train their classiﬁer. Therefore  solving
k tasks independently would require k · m δ samples in the worst case. Thus  we are interested
in algorithms that utilize samples from all players and solve all k tasks with sample complexity
o

(cid:17)(cid:17)(cid:17)

(cid:16) k

(cid:16) 1

(cid:16) 1

(cid:17)

(cid:16)

+ ln

d ln

.





δ

(cid:16) ln2(k)

(cid:16)

(cid:16) 1

(cid:17)

(cid:16) 1

(cid:17)(cid:17)(cid:17)

Blum et al. [5] give an algorithm with sample complexity O
for the realizable setting  that is  assuming the existence of a single classiﬁer with zero error on all the
tasks. They also extend this result by proving that a slightly modiﬁed algorithm returns a classiﬁer
with error   under the relaxed assumption that there exists a classiﬁer with error /100 on all the
tasks. In addition  they prove a lower bound showing that there is a concept class with d = Θ(k)
where Ω

samples are necessary.

(cid:16) k

(d + k) ln

+ k ln

δ





 ln

(cid:16) k

(cid:17)(cid:17)(cid:17)

(cid:16) 1

(cid:17)(cid:16)

(cid:16) k

δ

(cid:17)

(cid:16) 1





δ

d ln

d ln

 ln

+ k ln

and O

+ k + ln

In this work  we give two new algorithms based on multiplicative weight updates which have sample
complexities O
for the
realizable setting. Our ﬁrst algorithm matches the sample complexity of [5] for the variant of the
problem in which the algorithm is allowed to return different classiﬁers to the players and our second
algorithm has the sample complexity almost matching the lower bound of [5] when d = Θ(k) and
for typical values of δ. Both are presented in Section 3. Independently of our work  [6] use the
multiplicative weight update approach and achieve the same bounds as we do in that section.
Moreover  in Section 4  we extend our results to the non-realizable setting  presenting two algorithms
that generalize the algorithms for the realizable setting. These algorithms learn a classiﬁer with
error at most (2 + α)OPT +  on all the tasks  where α is set to a constant value  and have sample
complexities O
.
With constant α  these sample complexities are the same as in the realizable case. Finally  we give two
algorithms with randomized classiﬁers whose error probability over the random choice of the example
and the classiﬁer’s randomness is at most (1+α)OPT+ for all tasks. The sample complexities of these
algorithms are O
.

(cid:17)(cid:17)(cid:17)
(cid:16) k
(cid:17)(cid:17)(cid:17)
(cid:16) k

(cid:16) ln(k)
(cid:16)
(cid:16) ln(k)

(cid:16) 1
(cid:16) 1

(cid:17)
(cid:16) 1
(cid:17)
(cid:16) 1

(cid:17)
(cid:16) 1
(cid:16) 1

(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)

(cid:17)(cid:16)
(cid:16) k

(cid:16) 1
(cid:16) 1

(cid:16) k

(cid:17)(cid:16)

(cid:16) 1

(d + k) ln

α4 ln

and O

and O

+ k ln

+k ln

+k ln

(cid:16)

(cid:17)

(cid:17)

+ ln

+ln

d ln

d ln

d ln

α4

α

δ

δ

δ





α32



δ

α32 ln

δ



δ

2 Model
In the traditional PAC learning model  there is a space of instances X and a set Y = {0  1} of possible
labels for the elements of X . A classiﬁer f : X → Y  which matches each element of X to a label  is
called a hypothesis. The error of a hypothesis with respect to a distribution D on X × Y is deﬁned as
errD(f ) = Pr(x y)∼D[f (x) (cid:54)= y]. Let OPT = inf
f∈F errD(f )  where F is a class of hypotheses. In the
realizable setting we assume that there exists a target classiﬁer with zero error  that is  there exists
f∗ ∈ F with errD(f∗) = OPT = 0 for all i ∈ [k]. Given parameters (  δ)  the goal is to learn a
classiﬁer that has error at most   with probability at least 1 − δ. In the non-realizable setting  the
optimal classiﬁer f∗ is deﬁned to have errD(f∗) ≤ OPT + ε for any ε > 0. Given parameters (  δ)
and a new parameter α  which can be considered to be a constant  the goal is to learn a classiﬁer that
has error at most (1 + α)OPT +   with probability at least 1 − δ.
By the VC theorem and its known extension  the desired guarantee can be achieved in both settings by
drawing a set of samples of size m δ = O
and returning the classiﬁer with
minimum error on that sample. More precisely  in the non-realizable setting  m δ = C
α

+
  where C is also a constant. We consider an algorithm OF (S)  where S is a set of samples
ln
drawn from an arbitrary distribution D over the domain X × {0  1}  that returns a hypothesis f0
whose error on the sample set satisﬁes errS(f0) ≤ inf
f∈F errS(f ) + ε for any ε > 0  if such a hypothesis
exists. The VC theorem guarantees that if |S| = m δ  then errD(f0) ≤ (1 + α)errS(f0) + .
In the collaborative model  there are k players with distributions D1  . . .   Dk. Similarly  OPT =
errDi(f ) and the goal is to learn a single good classiﬁer for all distributions. In [5]  the
f∈F max
inf
i∈[k]

(cid:17)(cid:17)(cid:17)

(cid:17)(cid:17)

(cid:16) 1

(cid:16) 1

(cid:16) 1

(cid:16) 1

(cid:16) 1

(cid:16)

(cid:17)

(cid:17)

(cid:16)

+ ln

d ln

d ln

δ

δ







2

authors consider two variants of the model for the realizable setting  the personalized and the
centralized. In the former the algorithm can return a different classiﬁer to each player  while in
the latter it must return a single good classiﬁer. For the personalized variant  Blum et al. give an
algorithm with almost the same sample complexity as the lower bound they provide. We focus on the
more restrictive centralized variant of the model  for which the algorithm that Blum et al. give does
not match the lower bound. We note that the algorithms we present are improper  meaning that the
classiﬁer they return is not necessarily in the concept class F.

3 Sample complexity upper bounds for the realizable setting

In this section  we present two algorithms and prove their sample complexity.
Both algorithms employ multiplicative weight updates  meaning that in each round they ﬁnd a
classiﬁer with low error on the weighted mixture of the distributions and double the weights of the
players for whom the classiﬁer did not perform well. In this way  the next sample set drawn will
include more samples from these players’ distributions so that the next classiﬁer will perform better
on them. To identify the players for whom the classiﬁer of the round did not perform well  the
algorithms test the classiﬁer on a small number of samples drawn from each player’s distribution. If
the error of the classiﬁer on the sample is low  then the error on the player’s distribution can not be
too high and vise versa. In the end  both algorithms return the majority function over all the classiﬁers
of the rounds  that is  for each point x ∈ X   the label assigned to x is the label that the majority of
the classiﬁers assign to x.
We note that for typical values of δ  Algorithm R2 is better than Algorithm R1. However  Algorithm
R1 is always better than the algorithm of [5] for the centralized variant of the problem and matches
their number of samples in the personalized variant  so we present both algorithms in this section.
In the algorithms of [5]  the players are divided into classes based on the number of rounds for
which that player’s task is not solved with low error. The number of classes could be as large as the
number of rounds  which is Θ(log(k))  and their algorithm uses roughly m δ samples from each
class. On the other hand  Algorithm R1 uses only m δ samples across all classes and saves a factor
of Θ(log(k)) in the sample complexity. This requires analyzing the change in all classes together as
opposed to class by class.

Algorithm R1

(cid:16)

:= 1; t := 5(cid:100)log(k)(cid:101); (cid:48) := /6; δ(cid:48) := δ/(3t);
i=1 w(r−1)

  where Φ(r−1) =(cid:80)k

w(r−1)

Di

i

i

;

(cid:17)

i

i=1

Φ(r−1)

(cid:80)k
(cid:40)

Initialize: ∀i ∈ [k] w(0)
for r = 1 to t do
˜D(r−1) ← 1
Draw a sample set S(r) of size m(cid:48)/16 δ(cid:48) from ˜D(r−1);
f (r) ← OF (S(r));
Gr ← TEST(f (r)  k  (cid:48)  δ(cid:48));
2w(r−1)
 
Update: w(r)
w(r−1)
 
end for
return fR1 = maj({f (r)}t
r=1)
Procedure TEST(f (r)  k  (cid:48)  δ(cid:48))
for i = 1 to k do

if i /∈ Gr
otherwise

i =

;

i

i

(cid:16) 1

(cid:48) ln

(cid:17)(cid:17)

(cid:16) k

δ(cid:48)

from Di;

Draw a sample set Ti of size O
4 (cid:48)};

end for
return {i | errTi(f (r)) ≤ 3

Algorithm R1 runs for t = Θ(log(k)) rounds and learns a classiﬁer f (r) in each round r that has low
error on the weighted mixture of the distributions ˜D(r−1). For each player at least 0.6t of the learned
classiﬁers are “good”  meaning that they have error at most (cid:48) = /6 on the player’s distribution.
Since the algorithm returns the majority of the classiﬁers  in order for an instance to be mislabeled  at

3

least 0.5t of the total number of classiﬁers should mislabel it. This implies that at least 0.1t of the
“good” classiﬁers of that player should mislabel it  which amounts to 1/6 of the “good” classiﬁers.
Therefore  the error of the majority of the functions for that player is at most 6(cid:48) = .
To identify the players for whom the classiﬁer of the round does not perform well  Algorithm R1
uses a procedure called TEST. This procedure draws O
samples from each player’s
distribution and tests the classiﬁer on these samples. If the error for a player’s sample set is at most
3(cid:48)/4 then TEST concludes that the classiﬁer is good for that player and adds them to the returned set
Gr. The samples that the TEST requires from each player sufﬁce to make it capable of distinguishing
between the players with error more than (cid:48) and players with error at most (cid:48)/2 with respect to their
distributions  with high probability.
Theorem 1. For any   δ ∈ (0  1)  and hypothesis class F of VC dimension d  Algorithm R1 returns
a classiﬁer fR1 with errDi(fR1) ≤  ∀i ∈ [k] with probability at least 1 − δ using m samples  where

(cid:16) k

(cid:16) 1

(cid:17)(cid:17)

(cid:48) ln

δ(cid:48)

(cid:16) ln(k)

(cid:16)



(cid:16) 1

(cid:17)



(cid:16) k

(cid:17)(cid:17)(cid:17)

.

δ

m = O

d ln

+ k ln

The proof of Theorem 1 is very similar to the one for Algorithm R2 so we omit it and refer the
reader to Appendix A. Algorithm R1 is the natural boosting alternative to the algorithm of [5] for
the centralized variant of the model. Although it is discussed in [5] and mentioned to have the same
sample complexity as their algorithm  it turns out that it is more efﬁcient. Its sample complexity is
slightly better (or the same  depending on the parameter regime) compared to the one of the algorithm
for the personalized setting presented in [5]  which is O

(cid:16) log(k)

(cid:17)(cid:17)(cid:17)

(cid:16) k

(cid:16) 1

(d + k) ln

+ k ln

(cid:16)

(cid:17)

.





δ

However  in the setting of the lower bound in [5] where k = Θ(d)  there is a gap of log(k) multi-
plicatively between the sample complexity of Algorithm R1 and the lower bound. This difference
stems from the fact that in every round  the algorithm uses roughly Θ(k) samples to ﬁnd a classiﬁer
but roughly Θ(k log(k)) samples to test the classiﬁer for k tasks. Motivated by this discrepancy 
we develop Algorithm R2  which is similar to Algorithm R1 but uses fewer samples to test the
performance of each classiﬁer on the players’ distributions. To achieve high success probability 
Algorithm R2 uses a higher number of rounds.

Algorithm R2

(cid:17)(cid:109)

(cid:108)
(cid:16) k
(cid:17)
  where Φ(r−1) =(cid:80)k

δ

; (cid:48) := /6; δ(cid:48) := δ/(4t);

i=1 w(r−1)

i

;

i

log

(cid:16)

Φ(r−1)

:= 1; t := 150

Initialize: ∀i ∈ [k] w(0)
for r = 1 to t do
˜D(r−1) ← 1
Draw a sample set S(r) of size m(cid:48)/16 δ(cid:48) from ˜D(r−1);
f (r) ← OF (S(r));
Gr ← FASTTEST(f (r)  k  (cid:48)  δ(cid:48));
Update: w(r)

(cid:80)k
(cid:40)

w(r−1)

Di

i=1

;

i

if i /∈ Gr
otherwise

i

 

i =

2w(r−1)
w(r−1)
 
end for
return fR2 = maj({f (r)}t
Procedure FASTTEST(f (r)  k  (cid:48)  δ(cid:48))
for i = 1 to k do

r=1);

i

Draw a sample set Ti of size O
4 (cid:48)};

end for
return {i | errTi(f (r)) ≤ 3

(cid:16) 1

(cid:48)

(cid:17)

from Di;

More speciﬁcally  Algorithm R2 runs for t = 150(cid:100)log( k
identify the players for whom the classiﬁer of the round does not perform well requires O
samples from each player. This helps us save one logarithmic factor in the second term of the sample
complexity of Algorithm R1. We call this new test FASTTEST. The fact that FASTTEST uses less

(cid:17)
(cid:16) 1
δ )(cid:101) rounds. In addition  the test it uses to
(cid:48)

4

samples causes it to be less successful at distinguishing the players for whom the classiﬁer was “good”
from the players for whom it was not  meaning that it has constant probability of making a mistake
for a given player at a given round. There are two types of mistakes that FASTTEST can make: to
return i /∈ Gr and double the weight of i when the classiﬁer is good for i’s distribution and to return
i ∈ Gr and not double the weight of i when the classiﬁer is not good.
Theorem 2. For any   δ ∈ (0  1)  and hypothesis class F of VC dimension d  Algorithm R2 returns
a classiﬁer fR2 with errDi(fR2) ≤  ∀i ∈ [k] with probability at least 1 − δ using m samples  where

(cid:16) 1



(cid:16) k

(cid:17)(cid:16)

δ

(cid:16) 1

(cid:17)



m = O

ln

d ln

+ k + ln

(cid:16) 1

(cid:17)(cid:17)(cid:17)

.

δ

To prove the correctness and sample complexity of Algorithm R2  we need Lemma 2.1  which
describes the set Gr that the FASTTEST returns. Its proof uses the multiplicative forms of the
Chernoff bounds and is in Appendix A.
Lemma 2.1. FASTTEST(f (r)  k  (cid:48)  δ(cid:48)) is such that the following two properties hold  each with
probability at least 0.99  for given round r ∈ [t] and player i ∈ [k].

(a) If errDi(f (r)) > (cid:48)  then i /∈ Gr.
2   then i ∈ Gr.
(b) If errDi(f (r)) ≤ (cid:48)

i = |{r | r ∈ [t] and errDi(f (r)) > (cid:48)}|.

Proof of Theorem 2. First  we prove that Algorithm R2 indeed learns a good classiﬁer  meaning
that  with probability at least 1 − δ  for every player i ∈ [k] the returned classiﬁer fR2 has error
errDi (fR2) ≤ . Let e(t)
i be the number of rounds for which the classiﬁer’s error on Di was more
than (cid:48)  i.e. e(t)
Claim 2.1. With probability at least 1 − δ  e(t)
If the claim holds  then with probability at least 1− δ  less than 0.4t functions have error more than (cid:48)
on Di  ∀i ∈ [k]. Therefore  with probability at least 1 − δ  errDi(fR2) ≤ 0.6
0.1 (cid:48) ≤  for every i ∈ [k].

i < 0.4t ∀i ∈ [k].

Proof of Claim 2.1. Let us denote by I (r) the set of players having errDi(f (r)) > (cid:48)
2 in round r  i.e. 
2 }. We condition on the randomness in the ﬁrst r − 1 rounds and
I (r) = {i ∈ [k] | errDi(f (r)) > (cid:48)
compute E[Φ(r) | Φ(r−1)]. By linearity of expectation  the following hold for round r:
(cid:16)
(cid:17)
k(cid:88)

(cid:88)

(cid:16)

1

w(r−1)

i

errDi (f (r))

w(r−1)

i

errDi(f (r))

(cid:17) ≥ 1

Φ(r−1)

err ˜D(r−1) (f (r)) =

Φ(r−1)

i=1

By the deﬁnition of I (r)  errDi(f (r)) > (cid:48)
least 1 − δ(cid:48)  err ˜D(r−1) (f (r)) ≤ (cid:48)
probability at least 1 − δ(cid:48) 

(1)
2 for i ∈ I (r). From the VC theorem  with probability at
16. Using these two bounds and inequality (1)  it follows that with

i∈I (r)\Gr

(cid:88)

i∈I (r)\Gr

w(r−1)

i

≤ 1
8

Φ(r−1).

(2)

For the rest of the analysis  we will condition our probability space to the event that inequality (2)
holds for all t rounds. By the union bound  this event happens with probability 1 − tδ(cid:48) = 1 − δ/4.
Consider the set of players i /∈ I (r) ∪ Gr. These are the players for whom the classiﬁer of the round
performed well but FASTTEST made a mistake and did not include them in the set Gr. By linearity
of expectation:

(cid:34) (cid:80)

+ (cid:80)

w(r−1)

i

(2)  Lemma 2.1(b)

i∈I (r)\Gr
≤

i /∈I (r)∪Gr
(0.125 + 0.01)Φ(r−1)

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Φ(r−1)

w(r−1)

i

(3)

E[ (cid:80)

i /∈Gr

w(r−1)

i

| Φ(r−1)] = E

5

E[Φ(r) | Φ(r−1)] = E

Thus  the expected value of the potential function in round r conditioned on its value in the previous
round is bounded by

w(r−1)

(cid:88)

 k(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Φ(r−1)
(cid:105) ≤ δ/2. It follows that with probability at least 1 − δ

w(r−1)

i /∈Gr

i=1

+

i

i

By the deﬁnition of the expected value  this implies that E[Φ(r)] ≤ 1.135 E[Φ(r−1)]. Conditioned
on the fact that inequality (2) holds for all rounds  which is true with probability at least 1 − δ
4  we
can conclude that E[Φ(t)] ≤ k(1.135)t  by induction. Using Markov’s inequality we can state that
Pr

Φ(t) ≥ E[Φ(t)]

(cid:104)

4 − δ

2 = 1 − 3δ

δ/2

4

 (3)≤ 1.135Φ(r−1).

Φ(t) ≤ 2k(1.135)t

.

(4)

i

. Let m(r)

δ
denote the number of rounds r(cid:48)  up until and
We now need a lower bound for w(t)
including round r  for which the procedure FASTTEST made a mistake and returned i ∈ Gr(cid:48)
i − m(r−1)
] ≤ 0.01 so
although errDi(f (r(cid:48))) > (cid:48). From Lemma 2.1(a)  it follows that E[m(r)
i = m(r)
for M (r)
. Therefore  the sequence
{M (r)
i }t
r=0 is a super-martingale. In addition to this  since we can make at most one mistake
in each round  it holds that M (r)
< 1. Using the Azuma-Hoeffding inequality with
M (0)

i − 0.01 · 0 = 0 and the fact that t ≥ 150 we calculate that

i − 0.01r it holds that E[M (r)

i − M (r−1)

] ≤ M (r−1)

| M (r−1)

i = m(0)

i

i

i

i

i

i

(cid:104)

(cid:105) ≤ exp

(cid:16) − (0.17t)2

(cid:17) ≤ δ

i ≥ 0.18t
m(t)

Pr

.
i < 0.18t holds ∀i ∈ [k] with probability at least 1 − δ
4.

4k

2t

By union bound  m(t)

i

i −m(t)

i > 2e(t)

i −0.18t < 2k(1.135)t
150 t + 1

⇒ e(t)
150 t + 0.183t < 0.4t

i −0.18t holds for all i ∈ [k] with probability at least 1 − δ

The number of times a weight is doubled throughout the algorithm is log(w(t)
i ) and it is at least
the number of times the error of the classiﬁer was more than (cid:48) minus the number of times the
i − m(t)
error was more than (cid:48) but the FASTTEST made a mistake  which is exactly e(t)
. So
i ≥ 2e(t)
w(t)
4. Combining this
with the bound from inequality (4) we have that with probability at least 1 − δ:
i ≤ Φ(t) ⇒ 2e(t)
w(t)
⇒ e(t)
i < 0.18t + 1
It remains to bound the number of samples. FASTTEST is called t = 150(cid:100)log( k
quires O

(cid:16)
(cid:16) k
δ )(cid:101) times  so it re-
(cid:16)
(cid:17)
(cid:16) 1
(cid:17) 1
(cid:16) k
samples in total. The number of samples required to learn
(cid:17)(cid:17)(cid:17)
(cid:1)(cid:7)(cid:1) we get
samples. Substituting (cid:48) = /6 and δ(cid:48) = δ/(4t) = δ/(cid:0)600(cid:6)log(cid:0) k
(cid:17)(cid:16)
(cid:16) k

each round’s classiﬁer is m(cid:48)/16 δ(cid:48)  so for all rounds there are required O

(cid:17) k
(cid:17)
(cid:16) 1

i − 0.18t < 1 + log

(cid:16) 1
(cid:16) 1

samples in total. From the addition of the two bounds

+ t log(1.135)

(cid:16) k

(cid:17)

(cid:17)

(cid:16)

= O

d ln

log

log

(cid:4)

ln

(cid:48)

(cid:48)

(cid:48)

δ

δ

δ

δ

δ

+

δ(cid:48)
 log

O
above  the overall sample complexity bound is:

+ ln

d ln

δ



δ

 log

(cid:17)(cid:17)

(cid:16) k
(cid:16) k
(cid:17)(cid:17)(cid:17)
(cid:16) log(k)
(cid:16) 1
(cid:16) k
(cid:17)(cid:16)

δ

O

ln



δ

(cid:16) 1

(cid:17)



(cid:16) 1

(cid:17)(cid:17)(cid:17)

δ

d ln

+ k + ln

4 Sample complexity upper bounds for the non-realizable setting

We design Algorithms NR1 and NR2 for the non-realizable setting  which generalize the results of
Algorithms R1 and R2  respectively.

6

Theorem 3. For any   δ ∈ (0  1)  7/6 < α < 1  and hypothesis class F of VC dimension d 
Algorithm NR1 returns a classiﬁer fNR1 such that errDi(fNR1) ≤ (2 + α)OPT +  holds for all i ∈ [k]
with probability 1 − δ using m samples  where

(cid:16) ln(k)

(cid:16)

α4

(cid:16) 1

(cid:17)



(cid:16) k

(cid:17)(cid:17)(cid:17)

.

δ

m = O

d ln

+ k ln

Theorem 4. For any   δ ∈ (0  1)  5/4 < α < 1  and hypothesis class F of VC dimension d 
Algorithm NR2 returns a classiﬁer fNR2 such that errDi(fNR2) ≤ (2 + α)OPT +  holds for all i ∈ [k]
with probability 1 − δ using m samples  where

(cid:16) 1

(cid:16) k

(cid:17)(cid:16)

(cid:16) 1

(cid:17)



(cid:16) 1

(cid:17)

α

(cid:16) 1

(cid:17)(cid:17)(cid:17)

.

δ

m = O

ln

α4

δ

d ln

+ k ln

+ ln

Their main modiﬁcation compared to the algorithms in the previous section is that these algorithms
use a smoother update rule. Algorithm NR2 is presented here and Algorithm NR1 is in Appx. B.1.

:= 1; α(cid:48) := α/40; t := 2(cid:100)ln(4k/δ)/α(cid:48)3(cid:101); (cid:48) := /64; δ(cid:48) :=

i

(cid:16)

Di

(cid:17)
  where Φ(r−1) :=(cid:80)k
(cid:16)
(cid:16) 1
(cid:17)(cid:17)(cid:17)
(cid:16) 1
(cid:17)
(cid:16) 1
i=1 w(r−1)
(cid:16) 1
(cid:17)(cid:17)
(cid:16) 1
S(r) (f (r))+3(cid:48)   α(cid:48)(cid:17)

from Di;

α(cid:48)(cid:48) ln

+ ln

d ln

α(cid:48)(cid:48)

α(cid:48)

δ(cid:48)

(cid:48)

i

;

from ˜D(r−1);

Algorithm NR2
1: Initialization: ∀i ∈ [k] w(0)

δ/(4t);

2: for r = 1  . . .   t do
w(r−1)
˜D(r−1) ← 1
3:
Φ(r−1)
Draw a sample set S(r) of size O
f (r) ← OF (S(r));
for i = 1  . . .   k do

i=1

4:

i

(cid:80)k

Draw a sample set Ti of size O
errTi (f (r))α(cid:48)2
i ← min
s(r)
Update: w(r)

(cid:16)
(1+3α(cid:48))err
i ← w(r−1)

i

(1 + s(r)

)

i

end for

9:
10:
11: end for
12:
13: return fNR2 = maj({f (r)}t

r=1);

5:
6:
7:

8:

Algorithm NR2 faces a similar challenge as Algorithm R2. Given a player i  since the number of
samples Ti used to estimate errDi(f (r)) in each round is low  the estimation is not very accurate.
Ideally  we would want the inequality

|errTi(f (r)) − errDi(f (r))| ≤ α(cid:48) · errDi(f (r)) + (cid:48)

to hold for all players and all rounds with high probability. The “good” classiﬁers are now deﬁned
as the ones corresponding to rounds for which the inequality holds and errTi(f (r)) is not very high
i < α(cid:48)). The expected number of rounds that either one of these
(an indication of which is that s(r)
properties does not hold is a constant fraction of the rounds (≈ tα(cid:48)) and due to the high number of
rounds it is concentrated around that value  as in Algorithm R2. The proof of Theorem 4 is in Appx.
B.2.
We note that the classiﬁers returned by these algorithms have a multiplicative approximation factor
of almost 2 on the error. A different approach would be to allow for randomized classiﬁers with
low error probability over both the randomness of the example and the classiﬁer. We design two
algorithms  NR1-AVG and NR2-AVG that return a classiﬁer which satisﬁes this form of guarantee on
the error without the 2-approximation factor but use roughly α
 times more samples. The returned
classiﬁer is a randomized algorithm that  given an element x  chooses one of the classiﬁers of all
rounds uniformly at random and returns the label that this classiﬁer gives to x. For any distribution
over examples  the error probability of this randomized classiﬁer is exactly the average of the error
probability of classiﬁers f (1)  f (2)  . . .   f (t)  hence the AVG in the names. The algorithms as well as
the proofs of their corresponding theorems can be found in Appx. B.3 and B.4.

7

Theorem 5. For any   δ ∈ (0  1)  24/25 < α < 1  and hypothesis class F of VC dimension d 
Algorithm NR1-AVG returns a classiﬁer fNR1-AVG such that for the expected error errDi (fNR1-AVG) ≤
(1 + α)OPT +  holds for all i ∈ [k] with probability 1 − δ using m samples  where

(cid:16) ln(k)

(cid:16)

α32

(cid:16) 1

(cid:17)



(cid:16) k

(cid:17)(cid:17)(cid:17)

.

δ

m = O

d ln

+ k ln

Theorem 6. For any   δ ∈ (0  1)  30/29 < α < 1  and hypothesis class F of VC dimension d 
Algorithm NR2-AVG returns a classiﬁer fNR2-AVG such that for the expected error errDi (fNR2-AVG) ≤
(1 + α)OPT +  holds for all i ∈ [k] with probability 1 − δ using m samples  where

(cid:16) 1

(cid:16) k

(cid:17)(cid:16)

α32 ln

δ

(cid:16) 1

(cid:17)



(cid:16) 1

(cid:17)(cid:17)(cid:17)

.

δ

(d + k) ln

+ ln

m = O

5 Discussion

The problem has four parameters  d  k   and δ  so there are many ways to compare the sample
complexity of the algorithms. In the non-realizable setting there is one more parameter α  but this
is set to be a constant in the beginning of the algorithms. Our sample complexity upper bounds are
summarized in the following table.

Table 1: Sample complexity upper bounds

Algorithm 1

(cid:16)
(cid:16)
(cid:16)

(cid:16) ln(k)
(cid:16) ln(k)
(cid:16) ln(k)

α4



α32

d ln

d ln

d ln

Realizable
Non-realizable
(2 + α approx.)
Non-realizable
(randomized)

O

O

O

(cid:16) 1
(cid:16) 1
(cid:16) 1







(cid:17)
(cid:17)
(cid:17)

+k ln

+k ln

+k ln

(cid:16) k
(cid:16) k
(cid:16) k

δ

δ

δ

(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)

(cid:17)
(cid:17)

(cid:16) 1
(cid:16) 1





d ln

d ln

Algorithm 2

(cid:16) ln(k/δ)
(cid:16) ln(k/δ)
(cid:16) ln(k/δ)

α4



(cid:16)
(cid:16)
(cid:16)

O

O

O

α32

(cid:16) 1

α

(cid:16) 1
(cid:17)(cid:17)(cid:17)
(cid:16) 1
(cid:17)
(cid:16) 1
(cid:17)(cid:17)(cid:17)

+ln

δ

δ

+ k + ln

+k ln

(cid:17)

(cid:16) 1



(d + k) ln

+ ln

δ

(cid:17)(cid:17)(cid:17)

1

(cid:17)(cid:17)

(cid:16) k

(cid:16) k

Usually δ can be considered constant  since it represents the required error probability  or  in the
high success probability regime  δ =
poly(k). For both of these natural settings  we can see that
Algorithm 2 is better than Algorithm 1  except for the case of the expected error guarantee. If we
assume k = Θ(d)  then Algorithm 2 is always better than Algorithm 1.
In the realizable setting  Algorithm R1 is always better than the algorithm of [5] for the centralized
variant of the problem and matches their number of samples in the personalized variant. In addition 
Theorem 4.1 of [5] states that the sample complexity of any algorithm in the collaborative model
  given that d = Θ(k) and   δ ∈ (0  0.1)  and this holds even for the personalized
is Ω
variant. For d = Θ(k)  the sample complexity of Algorithm R2 is exactly ln
times the sample
complexity for learning one task. Furthermore  when |F| = 2d (e.g. the hard instance for the lower
bound of [5])  only m δ = O
samples are required in the non-collaborative setting
instead of the general bound of the VC theorem  so the sample complexity bound for Algorithm R2 is
and matches exactly the lower bound of [5] up to lower order terms.
O

(cid:16)
(cid:16) 1
(cid:17)(cid:17)(cid:17)

(cid:17)(cid:17)(cid:17)

(cid:16) k

(cid:16) k

(cid:17) 1

(cid:16) 1

(cid:16) 1

d + k + ln

d + ln

(cid:16)

(cid:17)

(cid:16)

 ln

ln

δ

δ

δ



δ



δ

In the non-realizable setting  our generalization of algorithms R1 and R2  NR1 and NR2 respectively 
have the same sample complexity as in the realizable setting and match the error guarantee for
OPT = 0. If OPT (cid:54)= 0  they guarantee an error of a factor 2 multiplicatively on OPT. The randomized
classiﬁers returned by Algorithms NR1-AVG and NR2-AVG avoid this factor of 2 in their expected
error guarantee. However  to learn such classiﬁers  there are required O

times more samples.

(cid:16) 1

(cid:17)



8

Acknowledgements

We thank the anonymous reviewers for their helpful remarks and for pointing us to the idea of slightly
modifying the algorithms in the non-realizable setting so that the optimal error is unknown. This work
was partially supported by NSF CAREER 1750716 and a Graduate fellowship from Northeastern
University’s College of Computer and Information Science.

References
[1] Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.

Cambridge University Press  New York  NY  USA  1st edition  2009.

[2] Maria-Florina Balcan  Avrim Blum  Shai Fine  and Yishay Mansour. Distributed learning  com-
munication complexity and privacy. In Proceedings of the 25th Conference on Computational
Learning Theory (COLT)  pages 26.1–26.22  2012.

[3] Jonathan Baxter. A Bayesian/information theoretic model of learning to learn via multiple task

sampling. Machine Learning  28(1):7–39  July 1997.

[4] Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research 

12(1):149–198  March 2000.

[5] Avrim Blum  Nika Haghtalab  Ariel D. Procaccia  and Mingda Qiao. Collaborative PAC
learning. In Proceedings of the 30th Annual Conference on Neural Information Processing
Systems (NIPS)  pages 2389–2398  2017.

[6] Jiecao Chen  Qin Zhang  and Yuan Zhou. Tight bounds for collaborative PAC learning via

multiplicative weights. CoRR  abs/1805.09217  2018.

[7] Ofer Dekel  Ran Gilad-Bachrach  Ohad Shamir  and Lin Xiao. Optimal distributed online
prediction. In Proceedings of the 28th International Conference on Machine Learning (ICML) 
pages 713–720  2011.

[8] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In Proceedings of the 34th International Conference on Machine
Learning (ICML)  pages 1126–1135  2017.

[9] Christos Koufogiannakis and Neal E. Young. A nearly linear-time PTAS for explicit fractional

packing and covering linear programs. Algorithmica  70(4):648–674  December 2014.

[10] Yishay Mansour  Mehryar Mohri  and Afshin Rostamizadeh. Domain adaptation: Learning
bounds and algorithms. In Proceedings of the 22nd Conference on Computational Learning
Theory (COLT)  pages 19–30  2009.

[11] Yishay Mansour  Mehryar Mohri  and Afshin Rostamizadeh. Domain adaptation with multiple
sources. In Proceedings of the 23rd Annual Conference on Neural Information Processing
Systems (NIPS)  pages 1041–1048  2009.

[12] Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomization and Proba-
bilistic Techniques in Algorithms and Data Analysis. Cambridge University Press  2nd edition 
2017.

[13] L. G. Valiant. A theory of the learnable. Commun. ACM  27(11):1134–1142  November 1984.

[14] Jialei Wang  Mladen Kolar  and Nathan Srebro. Distributed multi-task learning. In Proceedings
of the 19th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages
751–760  2016.

9

,Huy Nguyen
Lydia Zakynthinou