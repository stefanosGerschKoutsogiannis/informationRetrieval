2019,Retrosynthesis Prediction with Conditional Graph Logic Network,Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently  computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules  but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work  we propose a new approach to this task using the Conditional Graph Logic Network  a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied  implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of 8.2% over current state-of-the-art methods on the benchmark dataset  our model also offers interpretations for the prediction.,Retrosynthesis Prediction with

Conditional Graph Logic Network

Hanjun Dai‡†⇤  Chengtao Li2  Connor W. Coley⇧  Bo Dai‡  Le Song†

‡Google Research  Brain Team  {hadai  bodai}@google.com

2Galixir Inc.  chengtao.li@galixir.com

⇧Massachusetts Institute of Technology  ccoley@mit.edu

†Georgia Institute of Technology  Ant Financial  lsong@cc.gatech.edu

Abstract

Retrosynthesis is one of the fundamental problems in organic chemistry. The task
is to identify reactants that can be used to synthesize a speciﬁed product molecule.
Recently  computer-aided retrosynthesis is ﬁnding renewed interest from both
chemistry and computer science communities. Most existing approaches rely on
template-based models that deﬁne subgraph matching rules  but whether or not a
chemical reaction can proceed is not deﬁned by hard decision rules. In this work 
we propose a new approach to this task using the Conditional Graph Logic Network 
a conditional graphical model built upon graph neural networks that learns when
rules from reaction templates should be applied  implicitly considering whether
the resulting reaction would be both chemically feasible and strategic. We also
propose an efﬁcient hierarchical sampling to alleviate the computation cost. While
achieving a signiﬁcant improvement of 8.1% over current state-of-the-art methods
on the benchmark dataset  our model also offers interpretations for the prediction.

1

Introduction

Retrosynthesis planning is the procedure of identifying a series of reactions that lead to the synthesis
of target product. It is ﬁrst formalized by E. J. Corey [1] and now becomes one of the fundamental
problems in organic chemistry. Such problem of “working backwards from the target” is challenging 
due to the size of the search space–the vast numbers of theoretically-possible transformations–and thus
requires the skill and creativity from experienced chemists. Recently  various computer algorithms [2]
work in assistance to experienced chemists and save them tremendous time and effort.
The simplest formulation of retrosynthesis is to take the target product as input and predict possible
reactants 1. It is essentially the “reverse problem” of reaction prediction. In reaction prediction  the
reactants (sometimes reagents as well) are given as the input and the desired outputs are possible
products. In this case  atoms of desired products are the subset of reactants atoms  since the side
products are often ignored (see Fig 1). Thus models are essentially designed to identify this subset in
reactant atoms and reassemble them to be the product. This can be treated as a deductive reasoning
process. In sharp contrast  retrosynthesis is to identify the superset of atoms in target products  and
thus is an abductive reasoning process and requires “creativity” to be solved  making it a harder
problem. Although recent advances in graph neural networks have led to superior performance in
reaction prediction [3  4  5]  such advances do not transfer to retrosynthesis.
Computer-aided retrosynthesis designs have been deployed over the past years since [6]. Some of
them are completely rule-based systems [7] and do not scale well due to high computation cost and

⇤Work done while Hanjun was at Georgia Institute of Technology
1We will focus on this “single step” version of retrosynthesis in our paper.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Reactions

O

O

O

O

O

O

S

O

NH

O

F

F

O

F

F

F

F

F

O

O

O

F

F

N
H

O

O

O

NH

O

H2N

O

C O
O O

S

O

O

N

NH

O

O

O

O

O

O

F

F

F

O

NH

O

NH

C
C

F O
F F

O

F

F

F

C:1

C:3

C:2

N:5

C:4

N:4

C:5

C:1

O:2

Retrosynthesis Templates
heteroatom alkylation and arylation

O

C:2

+

C:3

C:1

N:4

C:5

O

S

O

acylation and related processes
C:3

C:3

O

C:1

O:2

F

F

F

+

C:4

N:5

Figure 1: Chemical reactions and the retrosynthesis templates. The reaction centers are highlighted in
each participant of the reaction. These centers are then extracted to form the corresponding template.
Note that the atoms belong to the reaction side products (the dashed box in ﬁgure) are missing.
incomplete coverage of the rules  especially when rules are expert-deﬁned and not algorithmically
extracted [2]. Despite these limitations  they are very useful for encoding chemical transformations
and easy to interpret. Based on this  the retrosim [8] uses molecule and reaction ﬁngerprint similarities
to select the rules to apply for retrosynthesis. Other approaches have used neural classiﬁcation models
for this selection task [9]. On the other hand  recently there have also been attempts to use the
sequence-to-sequence model to directly predict SMILES 2 representation of reactants [10  11] (and
for the forward prediction problem  products [12  13]). Albeit simple and expressive  these approaches
ignore the rich chemistry knowledge and thus require huge amount of training. Also such models
lack interpretable reasoning behind their predictions.
The current landscape of computer-aided synthesis planning motivated us to pursue an algorithm
that shares the interpretability of template-based methods while taking advantage of the scalability
and expressiveness of neural networks to learn when such rules apply. In this paper  we propose
Conditional Graph Logic Network towards this direction  where chemistry knowledge about reaction
templates are treated as logic rules and a conditional graphical model is introduced to tolerate the
noise in these rules. In this model  the variables are molecules while the synthetic relationships to
be inferred are deﬁned among groups of molecules. Furthermore  to handle the potentially inﬁnite
number of possible molecule entities  we exploit the neural graph embedding in this model.
Our contribution can be summarized as follows:
1) We propose a new graphical model for the challenging retrosynthesis task. Our model brings both
the beneﬁt of the capacity from neural embeddings  and the interpretability from tight integration
of probabilistic models and chemical rules.

2) We propose an efﬁcient hierarchical sampling method for approximate learning by exploiting
the structure of rules. Such algorithm not only makes the training feasible  but also provides
interpretations for predictions.

3) Experiments on the benchmark datasets show a signiﬁcant 8.1% improvement over existing

state-of-the-art methods in top-one accuracy.

Other related work: Recently there have been works using machine learning to enhance the
rule systems. Most of them treat the rule selection as multi-class classiﬁcation [9] or hierarchical
classiﬁcation [14] where similar rules are grouped into subcategories. One potential issue is that the
model size grows with the number of rules. Our work directly models the conditional joint probability
of both rules and the reactants using embeddings  where the model size is invariant to the rules.
On the other hand  researchers have tried to tackle the even harder problem of multi-step retrosyn-
thesis [15  16] using single-step retrosynthesis as a subroutine. So our improvement in single-step
retrosynthesis could directly transfer into improvement of multi-step retrosynthesis [8].

2 Background
A chemical reaction can be seen as a transformation from set of N reactant molecules {Ri}N
i=1 to
an outcome molecule O. Without loss of generality  we work with single-outcome reactions in this
paper  as this is a standard formulation of the retrosynthetic problem and multi-outcome reactions
can be split into multiple single-outcome ones. We refer to the set of atoms changed (e.g.  bond
being added or deleted) during the reaction as reaction centers. Given a reaction  the corresponding

2https://www.daylight.com/dayhtml/doc/theory/theory.smiles.html.

2

retrosynthesis template T is represented by a subgraph pattern rewriting rule 3

N (T ) 

1 + rT

2 + . . . + rT

T := oT ! rT

(1)
where N (·) represents the number of reactant subgraphs in the template  as illustrated in Figure. 1.
Generally we can treat the subgraph pattern oT as the extracted reaction center from O  and rT
i   i 2
1  2  . . .   N (T ) as the corresponding pattern inside i-th reactant  though practically this will include
neighboring structures of reaction centers as well.
We ﬁrst introduce the notations to represent these chemical entities:
• Subgraph patterns: we use lower case letters to represent the subgraph patterns.
• Molecule: we use capital letters to represent the molecule graphs. By default  we use O for an
outcome molecule  and R for a reactant molecule  or M for any molecule in general.
• Set: sets are represented by calligraphic letters. We use M to denote the full set of possible
molecules  T to denote all extracted retrosynthetic templates  and F to denote all the subgraph
patterns that are involved in the known templates. We further use Fo to denote the subgraphs
appearing in reaction outcomes  and Fr to denote those appearing in reactants  with F = FoSFr.
Task: Given a production or target molecule O  the goal of a one-step retrosynthetic analysis is to
identify a set of reactant molecules R2 P(M) that can be used to synthesize the target O. Here
P(M) is the power set of all molecules M.
3 Conditional Graph Logic Network
Let I[m ✓ M ] : F ⇥ M 7! {0  1} be the predicate that indicates whether subgraph pattern m is
a subgraph inside molecule M. This can be checked via subgraph matching. Then the use of a
retrosynthetic template T : oT ! rT
N (T ) for reasoning about a reaction can be
decomposed into two-step logic. First 

2 + . . . + rT

1 + rT

i=1

I[rT

i ✓ R⇡(i)] 

(2)
where the subgraph pattern oT from the reaction template T is matched against the product O  i.e. 
oT is a subgraph of the product O. Second 

I. Match template: O(T ) := I[oT ✓ O] · I[T 2T ] 
II. Match reactants: O T (R) := O(T ) · I[|R| = N (T )] ·QN (T )

(3)
where the set of subgraph patterns {r1  . . .   rN (T )} from the reaction template are matched against
the set of reactants R. The logic is that the size of the set of reactant R has to match the number of
patterns in the reaction template T   and there exists a permutation ⇡(·) of the elements in the reactant
set R such that each reactant matches a corresponding subgraph pattern in the template.
Since there will still be uncertainty in whether the reaction is possible from a chemical perspective
even when the template matches  we want to capture such uncertainty by allowing each template/or
logic reasoning rule to have a different conﬁdence score. More speciﬁcally  we will use a template
score function w1(T  O) given the product O  and the reactant score function w2(R  T  O) given the
template T and the product O. Thus the overall probabilistic models for the reaction template T and
the set of molecules R are designed as

I. Match template:
II. Match reactants:

p(T|O) / exp (w1(T  O)) · O(T ) 
p(R|T  O) / exp (w2(R  T  O)) · O T (R).

(4)
(5)
Given the above two step probabilistic reasoning models  the joint probability of a single-step
retrosythetic proposal using reaction template T and reactant set R can be written as
(6)
p (R  T|O) / exp (w1 (T  O) + w2 (R  T  O)) · O (T ) O T (R)  
In this energy-based model  whether the graphical model (GM) is directed or undirected is a design
choice. We will present our directed GM design and the corresponding partition function in Sec 4
shortly. We name our model as Conditional Graph Logic Network (GLN) (Fig. 2)  as it is a conditional
graphical model deﬁned with logic rules  where the logic variables are graph structures (i.e.  molecules 
subgraph patterns  etc.). In this model  we assume that satisfying the templates is a necessary condition
for the retrosynthesis  i.e.  p (R  T|O) 6= 0 only if O (T ) and O T (R) are nonzero. Such restriction
provides sparse structures into the model  and makes this abductive type of reasoning feasible.

3Commonly encoded using SMARTS/SMIRKS patterns

3

/

ℱ

ℳ

!('|% #)
!(#|%)

*≔#→{'}
!(ℛ|% *)

Figure 2: Retrosynthesis pipeline with GLN. The three dashed boxes from top to bottom represent
set of templates T   subgraphs F and molecules M. Different colors represent retrosynthesis routes
with different templates. The dashed lines represent potentially possible routes that are not observed.
Reaction centers in products O are highlighted.

Reaction type conditional model: In some situations when performing the retrosynthetic analysis 
the human expert may already have a certain type c of reaction in mind. In this case  our model can
be easily adapted to incorporate this as well:

p(R  T|O  c) / exp (w1 (T  O) + w2 (R  T  O)) · O (T ) O T (R) I[T 2T c]

where Tc is the set of retrosynthesis templates that belong to reaction type c.
GLN is related but signiﬁcantly different from Markov Logic Network (MLN  which also uses
graphical model to model uncertainty in logic rules). MLN treats the predicates of logic rules as
latent variables  and the inference task is to get the posterior for them. While in GLN  the task is the
structured prediction  and the predicates are implemented with subgraph matching. We show more
details on this connection in Appendix A.

(7)

4 Model Design
Although the model we deﬁned so far has some nice properties  the design of the components plays
a critical role in capturing the uncertainty in the retrosynthesis. We ﬁrst describe a decomposable
design of p(T|O) in Sec. 4.1  for learning and sampling efﬁciency consideration; then in Sec. 4.2 we
describe the parameterization of the scoring functions w1  w2 in detail.
4.1 Decomposable design of p(T|O)
Depending on how speciﬁc the reaction rules are  the template set T could be as large as the
total number of reactions in extreme case. Thus directly model p(T|O) can lead to difﬁculties in
learning and inference. By revisiting the logic rule deﬁned in Eq. (2)  we can see the subgraph
pattern oT plays a critical role in choosing the template. Since we represent the templates as
T = (oT ! rT
i N (T )
i=1 )  it is natural to decompose the energy function w1(T  O) in Eq. (4) as
  O⌘. Meanwhile  recall the template matching rule is also
w1(T  O) = v1oT   O + v2⇣rT
i N (T )
p(T|O) = p(oT  rT
Z(O)expv1(oT   O) · I⇥oT 2 O⇤⇣exp⇣v2⇣rT
i=1 ) 2T ]⌘  
i N (T )
Z (O) =Po2F exp (v1(o  O)) · I [o 2 O] ·⇣P{r}2P(F) exp (v2 ({r}   O)) · I[(o !{ r}) 2T ]⌘
Here we abuse the notation a bit to denote the set of subgraph patterns as {r}.
With such decomposition  we can further speed up both the training and inference for p(T|O)  since
the number of valid reaction centers per molecule and number of templates per reaction center
are much smaller than total number of templates. Speciﬁcally  we can sample T ⇠ p(T|O) by

= 1
where the partition function Z (O) is deﬁned as:

decomposable  so we obtain the resulting template probability model as:

  O⌘⌘ · I[(oT !rT

i N (T )

i=1

i N (T )

i=1

i=1

|O)

(8)

(9)

4

ﬁrst sampling reaction center p(o|O) / exp (v1(o  O)) · I [o 2 O] and then choosing the subgraph
patterns for reactants p({r}|O  o) / exp (v2 ({r}   O) · I[(o !{ r}) 2T ]). In the end we obtain
the templated represented as (o !{ r}).
In the literature there have been several attempts for modeling and learning p(T|O)  e.g.  multi-class
classiﬁcation [9] or multiscale model with human deﬁned template hierarchy [14]. The proposed
decomposable design follows the template speciﬁcation naturally  and thus has nice graph structure
parameterization and interpretation as will be covered in the next subsection.
Finally the directed graphical model design of Eq. (6) is written as
p(R  T|O) =

i=1 ⌘ + w2 (R  T  O)⌘⌘ · O (T ) O T (R)
i N (T )
where Z(T  O) =PR2P(M) exp (w2(R  T  O)) · O T (R) sums over all subsets of molecules.

Z(O)Z(T O) exp⇣⇣v1oT   O + v2⇣rT

(10)

1

4.2 Graph Neuralization for v1  v2 and w2

hl+1

hl

(12)

hl+1

Since the arguments of the energy functions w1  w2 are molecules  which can be represented by
graphs  one natural choice is to design the parameterization based on the recent advances in graph
neural networks (GNN) [17  18  19  20  21  22]. Here we ﬁrst present a brief review of the general
form of GNNs  and then explain how we can utilize them to design the energy functions.

The graph embedding is a function g : MSF 7! Rd that maps a graph into d-dimensional vector.
We denote G = (V G E G) as the graph representation of some molecule or subgraph pattern  where
V G = {vi}|V G|
i=1 is the set of bonds (edges).
We represent each undirected bond as two directional edges. Generally  the embedding of the graph
is computed through the node embeddings hvi that are computed in an iterative fashion. Speciﬁcally 
let h0
vi = xvi initially  where xvi is a vector of node features  like the atomic number  aromaticity 
etc. of the corresponding atom. Then the following update operator is applied recursively:

i=1 is the set of atoms (nodes) and E G =ei = (e1

i ) |E G|

i   e2

(✓4xu!v))

v = F (xv (hl

(11)
This procedure repeats for L steps. While there are many design choices for the so-called message
passing operator F   we use the structure2vec [21] due to its simplicity and efﬁcient c++ binding with
RDKit. Finally we have the parameterization

u  xu!v u2N (v)) where xu!v is the feature of edge u ! v.
v = (✓1xv + ✓2 Xu2N (v)

u + ✓3 Xu2N (v)

where (·) is some nonlinear activation function  e.g.  relu or tanh  and ✓ = {✓1  . . .  ✓ 4} are the
learnable parameters. Let the node embedding hv = hL
v be the last output of F   then the ﬁnal graph
|V G|Pv2V G hv. Note that
embedding is obtained via averaging over node embeddings: g(G) = 1
attention [23] or other order invariant aggregation can also be used for such aggregation.
With the knowledge of GNN  we introduce the concrete parametrization for each component:
• Parameterizing v1: Given a molecule O  v1 can be viewed as a scoring function of possible
reaction centers inside O. Since the subgraph pattern o is also a graph  we parameterize it with inner
product  i.e.  v1(o  O) = g1(o)>g2(O). Such form can be treated as computing the compatibility
between o and O. Note that due to our design choice  v1(o  O) can be written as v1(o  O) =

i=1 varies for different template
T . Inspired by the DeepSet [24]  we use average pooling over the embeddings of each subgraph
pattern to represent this set. Speciﬁcally 

Pv2V O h>v g1(o). Such form allows us to see the contribution of compatibility from each atom in O.
• Parameterizing v2: The size of set of subgraph patternsrT
i N (T )
  O) = g3(O)>0@ 1
i ))1A
N (T )Xi=1
i N (T )
v2(rT
g6(R)! .
w2(R  T  O) = g5(O)> 1
|R| XR2R

• Parameterizing w2: This energy function also needs to take the set as input. Following the same
design as v2  we have

g4(rT

N (T )

(13)

(14)

i=1

5

Note that our GLN framework isn’t limited to the speciﬁc parameterization above and is compatible
with other parametrizations. For example  one can use condensed graph of reaction [25] to represent
R as a single graph. Other chemistry specialized GNNs [3  26] can also be easily applied here. For
the ablation study on these design choices  please refer to Appendix C.1.
5 MLE with Efﬁcient Inference
Given dataset D = {(Oi  Ti Ri)}|D|
i=1 with |D| reactions  we denote the parameters in
w1 (T  O)   w2 (T R  O) as ⇥= ( ✓1 ✓ 2)  respectively. The maximum log-likelihood estima-
tion (MLE) is a natural choice for parameter estimation. Since 8 (O  T R) ⇠D   O (T ) = 1
and O T (R) = 1  we have the MLE optimization as
:= bED [log p (R|T  O) p (T|O)]
(15)
= bED [w1 (T  O) + w2 (R  T  O)  log Z (O)  log Z (O  T )]  
+bED [r⇥w2 (R  T  O)] bEO T ER|T O [r⇥w2 (R  T  O)]  

r⇥` (⇥) = bED [r⇥w1 (T  O)] bEOET|O [r⇥w1 (T  O)]

The gradient of ` (⇥) w.r.t. ⇥ can be derived4 as

` (⇥)

(16)

max

⇥

where ET|O [·] and ER|O T [·] stand for the expectation w.r.t. current model p (T|O) and p (R  T|O) 
respectively. With the gradient estimator (16)  we can apply the stochastic gradient descent (SGD)
algorithm for optimizing (15).
Efﬁcient inference for gradient approximation: Since R2 P(M) is a combinatorial space  gen-
erally the expensive MCMC algorithm is required for sampling from p (R|T  O) to approximate (16).
However  this can be largely accelerated by scrutinizing the logic property in the proposed model. Re-
call that the matching between template and reactants is the necessary condition for p (R  T|O)  0
by design. On the other hand  given O  only a few templates T with reactants R have nonzero O (T )
and O T (R). Then  we can sample T and R by importance sampling on restricted supported
templates instead of MCMC over P (M). Rigorously  given O  we denote the matched templates as
TO and the matched reactants based on T as RT O  where
(17)
Then  the importance sampling leads to an

TO = {T : O (T ) 6= 0 8T 2T } and RT O = {R : O T (R) 6= 0 8R 2 P (M)}
Algorithm 1 Importance Sampling for br⇥` (⇥)
unbiased gradient approximation br⇥` (⇥)
as illustrated in Algorithm 1. To make the
1: Input (R  T  O) ⇠D   p (R|T  O) and p (T|O).
algorithm more efﬁcient in practice  we
2: Construct TO according to O (T ).
have adopted the following accelerations:
3: Sample ˜T / exp (w1 (T  O))   8T 2T O in hierar-
• 1) Decomposable modeling of p(T|O)
4: Construct RT O according to O T (R).
• 2) Cache the computed TO and R (T  O)
5: Sample ˜R/ exp (w2 (R  T  O)).
6: Compute stochastic approximation br⇥` (⇥) with
sample⇣R  T  ˜R  ˜T   O⌘ by (16).
In a dataset with 5 ⇥ 104 reactions  |TO|
is about 80 and |RT O| is roughly 10 on
average. Therefore  we reduce the actual
computational cost to a manageable constant. We further reduce the computation cost of sampling
by generating the T and R uniformly from the support. Although these samples only cover the
support of the model  we avoid the calculation of the forward pass of neural networks  achieving
better computational complexity. In our experiment  such an approximation already achieves state-of-
the-art results. We would expect recent advances in energy based models would further boost the
performance  which we leave as future work to investigate.
Remark on RT O: Note that to get all possible sets of reactants that match the reaction template T
and product O  we can efﬁciently use graph edit tools without limiting the reactants to be known in
the dataset. This procedure works as follows: given a template T = oT ! rT
1) Enumerate all matches between subgraph pattern oT and target product O.
2) Instantiate a copy of the reactant atoms according to rT
3) Copy over all of the connected atoms and atom properties from O.

chical way  as in Sec. 4.1.

as described in Sec. 4.1;

N for each match.

N 
1 + . . . + rT

in advance.

1   . . .   rT

4We adopt the conventions 0 log 0 = 0 [27]  which is justiﬁed by continuity since x log x ! 0 as x ! 0.

6

i })}k

This process is a routine in most Cheminformatics packages. In our paper we use runReactants
from RDKit with the improvement of stereochemistry handling 5 to realize this.
Further acceleration via beam search: Given a product O  the prediction involves ﬁnding the
pair (R  T ) that maximizes p(R  T|O). One possibility is to ﬁrst enumerate T 2 T (O) and then
R2R T O. This is acceptable by exploiting the sparse support property induced by logic rules.
A more efﬁcient way is to use beam search with size k. Firstly we ﬁnd k reaction centers {oi}k
i=1 with
i=1 we score the corresponding v2({r}   O)· I [(o !{ r}) 2T ].
top v1(o  O). Next for each o 2{ oi}k
In this stage the top k pairs {(oTj  {rTj
j=1 (i.e.  the templates) that maximize v1(o|O) +
v2({r}   O) are kept. Finally using these templates  we choose the best R2 Sk
j=1 RTj  O that
maximizes total score w1 (T  O) + w2 (R  T  O). Fig. 2 provides a visual explanation.
6 Experiment
Dataset: We mainly evaluate our method on a benchmark dataset named USPTO-50k  which
contains 50k reactions of 10 different types in the US patent literature. We use exactly the same
training/validation/test splits as Coley et al. [8]  which contain 80%/10%/10% of the total 50k
reactions. Table 1 contains the detailed information about the benchmark. Additionally  we also build
a dataset from the entire USPTO 1976-2016 to verify the scalability of our method.
Baselines: Baseline algorithms consist of rule-based ones and neural network-based ones  or both.
The expertSys is an expert system based on retrosynthetic reaction rules  where the rule is selected
according to the popularity of the corresponding reaction type. The seq2seq [10] and transformer [11]
are neural sequence-to-sequence-based learning model [28] implemented with LSTM [29] or Trans-
former [30]. These models encode the canonicalized SMILES representation of the target compound
as input  and directly output canonical SMILES of reactants. We also include some data-driven
template-based models. The retrosim [8] uses direct calculation of molecular similarities to rank the
rules and resulting reactants. The neuralsym [9] models p(T|O) as multi-class classiﬁcation using
MLP. All the results except neuralsym are obtained from their original reports  since we have the
same experiment setting. Since neuralsym is not open-source  we reimplemented it using their best
reported ELU512 model with the same method for parameter tuning.
Evaluation metric: The evaluation metric we used is the top-k exact match accuracy  which is
commonly used in the literature. This metric compares whether the predicted set of reactants are
exactly the same as ground truth reactants. The comparison is performed between canonical SMILES
strings generated by RDKit.
Setup of GLN: We use rdchiral [31] to extract the retrosynthesis templates from the training set.
After removing duplicates  we obtained 11 647 unique template rules in total for USPTO-50k. These
rules represent 93.3% coverage of the test set. That is to say  for each test instance we try to apply
these rules and see if any of the rules gives exact match. Thus this is the theoretical upper bound of
the rule-based approach using this particular degree of speciﬁcity  which is high enough for now. For
more information about the statistics of these rules  please refer to Table 2.
We train our model for up to 150k updates with batch size of 64. It takes about 12 hours to train with a
single GTX 1080Ti GPU. We tune embedding sizes in {128  256}  GNN layers {3  4  5} and GNN ag-
gregation in {max  mean  sum} using validation set. Our code is released at https://github.com/Hanjun-
Dai/GLN. More details are included in Appendix B.

6.1 Main results
We present the top-k exact match accuracy in Table 3  where k ranges from {1  3  5  10  20  50}. We
evaluate both the reaction class unknown and class conditional settings. Using the reaction class as
prior knowledge represents some situations where the chemists already have an idea of how they
would like to synthesize the product.
In all settings  our proposed GLN outperforms the baseline algorithms. And particularly for top-1
accuracy  our model performs signiﬁcantly better than the second best method  with 8.1% higher
accuracy with unknown reaction class  and 8.9% higher with reaction class given. This demonstrates
the advantage of our method in this difﬁcult setting and potential applicability in reality.

5https://github.com/connorcoley/rdchiral.

7

USPTO 50k

# train
# val
# test
# rules

# reaction types

40 008
5 001
5 007
11 647

10

Table 1: Dataset information.

Rule coverage
# unique centers

Avg. # centers per mol
Avg. # rules per mol

Avg. # reactants

93.3%
9 078
29.31
83.85
1.71

Table 2: Reaction and tem-
plate set information.

methods

1

3

5

10

20

50

Top-k accuracy %

transformer[11]

retrosim[8]
neuralsym[9]

GLN

expertSys[10]
seq2seq[10]
retrosim[8]
neuralsym[9]

GLN

57.3
54.7
65.3
69.0

62.7
63.3
72.4
75.6

/

Reaction class unknown
37.9
37.3
44.4
52.5

74.1
78.9
83.7
Reaction class given as prior
65.1
61.7
88.1
85.1
90.0

52.3
52.4
73.8
76.0
79.1

59.1
57.0
81.2
81.4
85.2

35.4
37.4
52.9
55.3
64.2

/

82.0
82.2
89.0

68.6
65.9
91.8
86.5
92.3

/

85.3
83.1
92.4

69.5
70.7
92.9
86.9
93.2

Table 3: Top-k exact match accuracy.

O

O

O

O

O

O

O

O

N

NH

NH

Cl

O

O

N

NH

NH

Cl

O

O

N

NH

NH

Cl

O

O

N

NH

NH

Cl

O

O

O

O

NH

S

O

Cl

Cl

O

O

NH

S

O

Cl

Cl

O

O

NH

S

O

O

O

NH

S

O

O

N

O

NH 2

O

S

NH

O

O

O

O

Cl

NH

OH

Ground truth

Cl

O

N

O

NH

O

S

O

O

OH

Cl

O

Similarity=0.9

Cl

O

N

O

O

N

O

Correct

O

S

NH

O

O

Cl

NH

O

S

O

O

O

OH

Cl

NH

O

OH

Cl

O

Similarity=0.9

Cl

NH 2

NH

O

NH 2

NH 2

NH

O

N

N

N

N

O

S

N

O

N

O

S

O

F

NH

NH

O

F

I

O

S

N

O

N

F

N H

N H

O

S

O

N

O

Ground truth

F

O

S

N

O

O

S

N

O

O

S

N

O

N

N

N

O

S

O

O

S

O

O

S

O

F

NH

F

F

NH

F

F

NH

F

NH

O

NH

O

NH

O

F

OH

O

F

O

S

NH

O

O

N

S

O

N

N

Similarity=0.82

F

NH

O

S

O

N

O

N

S

O

NH

O

Similarity=0.87

F

O

S

NH

O

F

OH

O

F

N

Similarity=0.82

NH2

O

NH

O

N

S

O

N

NH2

Figure 3: Example successful predictions.

Figure 4: Example failed predictions.

Moreover  our performance in the reaction class unknown setting even outperforms expertSys
and seq2seq in the reaction conditional setting. Since the transformer paper didn’t report top-k
performance for k > 10  we leave it as blank. Meanwhile  Karpov et al. [11] also reports the result
when training using training+validation set and tuning on the test set. With this extra priviledge 
the top-1 accuracy of transformer is 42.7% which is still worse than our performance. This shows
the beneﬁt of our logic powered deep neural network model comparing to purely neural models 
especially when the amount of data is limited.
Since the theoretical upper bound of this rule-based implementation is 93.3%  the top-50 accuracy
for our method in each setting is quite close to this limit. This shows the probabilistic model we built
matches the actual retrosynthesis target well.

6.2

Interpret the predictions

Visualizing the predicted synthesis: In Fig 3 and 4  we visualize the ground truth reaction and
the top 3 predicted reactions (see Appendix C.6 for high resolution ﬁgures). For each reaction  we
also highlight the corresponding reaction cores (i.e.  the set of atoms get changed). This is done
by matching the subgraphs from predicted retrosynthesis template with the target compound and
generated reactants  respectively. Fig 3 shows that our correct prediction also gets almost the same
reaction cores predicted as the ground truth. In this particular case  the explanation of our prediction
aligns with the existing reaction knowledge.
Fig 4 shows a failure mode where none of the top-3 prediction matches. In this case we calculated
the similarity between predicted reactants and ground truth ones using Dice similarity from RDKit.
We ﬁnd these are still similar in the molecule ﬁngerprint level  which suggests that these predictions
could be the potentially valid but unknown ones in the literature.

8

Top-1 prediction

Bottom-1 prediction

True reaction core

Top-1 prediction

Bottom-1 prediction

True reaction core

l

s
e
u
c
e
o
M

l

s
r
e
t
n
e
C

Figure 5: Reaction center prediction visualization. Red atoms indicate positive match scores  while
blue ones having negative scores. The darkness of the color shows the magnitude of the score. Green
parts highlight the substructure match between molecules and center structures.
Visualizing the reaction center prediction: Here we visualize the prediction of probabilistic mod-
eling of reaction center. This is done by calculating the inner product of each atom embedding in
target molecule with the subgraph pattern embedding. Fig 5 shows the visualization of scores on
the atoms that are part of the reaction center. The top-1 prediction assigns positive scores to these
atoms (red ones)  while the bottom-1 prediction (i.e.  prediction with least probability) assigns large
negative scores (blue ones). Note that although the reaction center in molecule and the corresponding
subgraph pattern have the same structure  the matching scores differ a lot. This suggests that the
model has learned to predict the activity of substructures inside molecule graphs.

6.3 Study of the performance
In addition to the overall numbers in Table 3  we provide detailed study of the performances. This
includes per-category performance  the accuracy of each module in hierarchical sampling and also
the effect of the beam size. Due to the space limit  please refer to Appendix C.

32.8
56.1

35.8
60.8

retrosim

neuralsym GLN
39.3
top-1
63.7
top-10
Table 4: Top-k accuracy on USPTO-full.

6.4 Large scale experiments on USPTO-full
To see how this method scales up with the dataset
size  we create a large dataset from the entire set
of reactions from USPTO 1976-2016. There are
1 808 937 raw reactions in total. For the reactions
with multiple products  we duplicate them into multiple ones with one product each. After removing
the duplications and reactions with wrong atom mappings  we obtain roughly 1M unique reactions 
which are further divided into train/valid/test sets with size 800k/100k/100k.
We train on single GPU for 3 days and report with the model having best validation accuracy. The
results are presented in Table 4. We compare with the best two baselines from previous sections.
Despite the noisiness of the full USPTO set relative to the clean USPTO-50k  our method still
outperforms the two best baselines in top-k accuracies.
7 Discussion
Evaluation: Retrosynthesis usually does not have a single right answer. Evaluation in this work is to
reproduce what is reported for single-step retrosynthesis. This is a good  but imperfect benchmark 
since there are potentially many reasonable ways to synthesize a single product.
Limitations: We share the limitations of all template-based methods. In our method  the template
designs  more speciﬁcally  their speciﬁcities  remain as a design art and are hard to decide beforehand.
Also  the scalability is still an issue since we rely on subgraph isomorphism during preprocessing.
Future work: The subgraph isomorphism part can potentially be replaced with predictive model 
while during inference the fast inner product search [32] can be used to reduce computation cost. Also
actively building templates or even inducing new ones could enhance the capacity and robustness.

Acknowledgments
We would like to thank anonymous reviewers for providing constructive feedbacks. This project
was supported in part by NSF grants CDS&E-1900017 D3SC  CCF-1836936 FMitF  IIS-1841351 
CAREER IIS-1350983 to L.S.

9

References
[1] Elias JAMES Corey. The logic of chemical synthesis: multistep synthesis of complex carbogenic molecules

(nobel lecture). Angewandte Chemie International Edition in English  30(5):455–465  1991.

[2] Connor W. Coley  William H. Green  and Klavs F. Jensen. Machine learning in computer-aided synthesis

planning. 51(5):1281–1289  . doi: 10.1021/acs.accounts.8b00087.

[3] Wengong Jin  Connor Coley  Regina Barzilay  and Tommi Jaakkola. Predicting organic reaction outcomes
with weisfeiler-lehman network. In Advances in Neural Information Processing Systems  pages 2607–2616 
2017.

[4] Connor W. Coley  Wengong Jin  Luke Rogers  Timothy F. Jamison  Tommi S. Jaakkola  William H. Green 
Regina Barzilay  and Klavs F. Jensen. A graph-convolutional neural network model for the prediction of
chemical reactivity. 10(2):370–377  . doi: 10.1039/C8SC04228D.

[5] John Bradshaw  Matt J Kusner  Brooks Paige  Marwin HS Segler  and José Miguel Hernández-Lobato. A

generative model for electron paths. 2018.

[6] EJ Corey and W Todd Wipke. Computer-assisted design of complex organic syntheses. Science  166

(3902):178–192  1969.

[7] Sara Szymkuc  Ewa P. Gajewska  Tomasz Klucznik  Karol Molga  Piotr Dittwald  Michał Startek  Michał
Bajczyk  and Bartosz A. Grzybowski. Computer-assisted synthetic planning: The end of the beginning. 55
(20):5904–5937. doi: 10.1002/anie.201506101.

[8] Connor W Coley  Luke Rogers  William H Green  and Klavs F Jensen. Computer-assisted retrosynthesis

based on molecular similarity. ACS Central Science  3(12):1237–1245  2017.

[9] Marwin H. S. Segler and Mark P. Waller. Neural-symbolic machine learning for retrosynthesis and reaction

prediction. 23(25):5966–5971. doi: 10.1002/chem.201605499.

[10] Bowen Liu  Bharath Ramsundar  Prasad Kawthekar  Jade Shi  Joseph Gomes  Quang Luu Nguyen  Stephen
Ho  Jack Sloane  Paul Wender  and Vijay Pande. Retrosynthetic reaction prediction using neural sequence-
to-sequence models. ACS Central Science  3(10):1103–1113  2017.

[11] Pavel Karpov  Guillaume Godin  and I Tetko. A transformer model for retrosynthesis. 2019.

[12] Philippe Schwaller  Theophile Gaudin  David Lanyi  Costas Bekas  and Teodoro Laino. “found in
translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-
sequence models. Chemical science  9(28):6091–6098  2018.

[13] Philippe Schwaller  Teodoro Laino  Theophile Gaudin  Peter Bolgar  Costas Bekas  and Alpha A. Lee.
Molecular transformer for chemical reaction prediction and uncertainty estimation. doi: 10.26434/chemrxiv.
7297379.v1.

[14] Javier L. Baylon  Nicholas A. Cilfone  Jeffrey R. Gulcher  and Thomas W. Chittenden. Enhancing
retrosynthetic reaction prediction with deep learning using multiscale reaction classiﬁcation. 59(2):
673–688.

[15] Marwin HS Segler  Mike Preuss  and Mark P Waller. Planning chemical syntheses with deep neural

networks and symbolic ai. Nature  555(7698):604  2018.

[16] John S Schreck  Connor W Coley  and Kyle JM Bishop. Learning retrosynthetic planning through self-play.

arXiv preprint arXiv:1901.06569  2019.

[17] Franco Scarselli  Marco Gori  Ah Chung Tsoi  Markus Hagenbuchner  and Gabriele Monfardini. The

graph neural network model. IEEE Transactions on Neural Networks  20(1):61–80  2008.

[18] David K Duvenaud  Dougal Maclaurin  Jorge Iparraguirre  Rafael Bombarell  Timothy Hirzel  Alán
Aspuru-Guzik  and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints.
In Advances in neural information processing systems  pages 2224–2232  2015.

[19] Tao Lei  Wengong Jin  Regina Barzilay  and Tommi Jaakkola. Deriving neural architectures from sequence
and graph kernels. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 2024–2033. JMLR. org  2017.

[20] Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard Zemel. Gated graph sequence neural networks.

arXiv preprint arXiv:1511.05493  2015.

10

[21] Hanjun Dai  Bo Dai  and Le Song. Discriminative embeddings of latent variable models for structured

data. In International conference on machine learning  pages 2702–2711  2016.

[22] Will Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on large graphs. In

Advances in Neural Information Processing Systems  pages 1024–1034  2017.

[23] Petar Veliˇckovi´c  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Lio  and Yoshua Bengio.

Graph attention networks. arXiv preprint arXiv:1710.10903  2017.

[24] Manzil Zaheer  Satwik Kottur  Siamak Ravanbakhsh  Barnabas Poczos  Ruslan R Salakhutdinov  and
Alexander J Smola. Deep sets. In Advances in neural information processing systems  pages 3391–3401 
2017.

[25] Frank Hoonakker  Nicolas Lachiche  Alexandre Varnek  and Alain Wagner. Condensed graph of reaction:

considering a chemical reaction as one single pseudo molecule.

[26] Justin Gilmer  Samuel S Schoenholz  Patrick F Riley  Oriol Vinyals  and George E Dahl. Neural message
passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70  pages 1263–1272. JMLR. org  2017.

[27] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons  2012.

[28] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural networks. In

Advances in neural information processing systems  pages 3104–3112  2014.

[29] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

[30] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems  pages 5998–6008  2017.

[31] Connor W Coley  William H Green  and Klavs F Jensen. Rdchiral: An rdkit wrapper for handling
stereochemistry in retrosynthetic template extraction and application. Journal of chemical information and
modeling  .

[32] Ruiqi Guo  Sanjiv Kumar  Krzysztof Choromanski  and David Simcha. Quantization based fast inner

product search. In Artiﬁcial Intelligence and Statistics  pages 482–490  2016.

[33] Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning  62(1-2):107–136 

2006.

[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[35] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural networks?

arXiv preprint arXiv:1810.00826  2018.

11

,Hanjun Dai
Chengtao Li
Connor Coley
Bo Dai
Le Song