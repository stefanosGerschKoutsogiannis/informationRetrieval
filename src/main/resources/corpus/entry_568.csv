2013,Approximate Gaussian process inference for the drift function in stochastic differential equations,We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from incomplete observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector  we develop an approximate EM algorithm to deal with the unobserved  latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression.,Approximate Gaussian process inference for the drift

of stochastic differential equations

Andreas Ruttor

Philipp Batz

Computer Science  TU Berlin

Computer Science  TU Berlin

andreas.ruttor@tu-berlin.de

philipp.batz@tu-berlin.de

Manfred Opper

Computer Science  TU Berlin

manfred.opper@tu-berlin.de

Abstract

We introduce a nonparametric approach for estimating drift functions in systems
of stochastic differential equations from sparse observations of the state vector.
Using a Gaussian process prior over the drift as a function of the state vector  we
develop an approximate EM algorithm to deal with the unobserved  latent dynam-
ics between observations. The posterior over states is approximated by a piecewise
linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the
drift is facilitated by a sparse Gaussian process regression.

1

Introduction

Gaussian process (GP) inference methods have been successfully applied to models for dynamical
systems  see e.g. [1–3]. Usually  these studies have dealt with discrete time dynamics  where one
uses a GP prior for modeling transition function and the measurement function of the system. On
the other hand  many dynamical systems in the physical world evolve in continuous time and the
noisy dynamics is described naturally in terms of stochastic differential equations (SDE). SDEs
have also attracted considerable interest in the NIPS community in recent years [4–7]. So far most
inference approaches have dealt with the posterior prediction of state variables between observations
(smoothing) and the estimation of parameters contained in the drift function  which governs the
deterministic part of the microscopic time evolution. Since the drift is usually a nonlinear function
of the state vector  a nonparametric estimation using Gaussian process priors would be a natural
choice  when a large number of data is available. A recent result by [8  9] presented an important
step in this direction. The authors have shown that GPs are a conjugate family to SDE likelihoods. In
fact  if an entire path of dense observations of the state dynamics is observed  the posterior process
over the drift is exactly a GP. Unfortunately  this simplicity is lost  when observations are not dense 
but separated by larger time intervals.
In [8] this sparse  incomplete observation case has been
treated by a Gibbs sampler  which alternates between sampling complete state paths of the SDE and
creating GP samples for the drift. A nontrivial problem is the sampling from SDE paths conditioned
on observations. Second  the densely sampled hidden paths are equivalent to a large number of
imputed observations  for which the matrix inversions required by the GP posterior predictions can
become computationally costly. It was shown in [8] that in the univariate case for GP priors with
precision operators (the inverses of covariance kernels) which are differential operators efﬁcient
predictions can be realized in terms of the solutions of differential equations.

In this paper  we develop an alternative approximate expectation maximization (EM) method for
inference from sparse observations  which is faster than the sampling approach and can also be
applied to arbitrary kernels and multivariate SDEs. In the E-Step we approximate expectations over

1

state paths by those of a locally ﬁtted Ornstein-Uhlenbeck model. The M-step for computing the
maximum posterior GP prediction of the drift depends on a continuum of function values and is thus
approximated by a sparse GP.

The paper is organized as follows. Section 2 introduces stochastic differential equations and section
3 discusses GP based inference for completely observed paths. In section 4 our approximate EM
algorithm is derived and its performance is demonstrated on a variety of SDEs in section 6. Section
7 presents a discussion.

2 Stochastic differential equations

We consider continuous-time univariate Markov processes of the diffusion type  where the dynamics

of a d-dimensional state vector Xt ∈ Rd is given by the stochastic differential equation (SDE)

dXt = f (Xt)dt + D1/2dW.

(1)
The vector function f (x) = (f 1(x)  . . .   f d(x)) deﬁnes the deterministic drift and W is a Wiener
process  which models additive white noise. D is the diffusion matrix  which we assume to be
independent of x. We will not attempt a rigorous treatment of probability measures over continuous
time paths here  but will mostly assume for our derivations that the process can be approximated
with a discrete time process Xt in the Euler-Maruyama discretization [10]  where the times t ∈ G
are on a regular grid G = {0  ∆t  2∆t  . . .} and where ∆t is some small microscopic time. The
discretized process is given by

Xt+∆t − Xt = f (Xt)∆t + D1/2√∆t ǫt 

(2)
where ǫt ∼ N (0  I) is a sequence of i.i.d. Gaussian noise vectors. We will usually take the limit
∆t → 0 only in expressions where (Riemann) sums are over nonrandom quantities  i.e. where
expectations over paths have been carried out and can be replaced by ordinary integrals.

3 Bayesian Inference for dense observations

Suppose we observe a path of n d-dimensional observations X0:T = (Xt)t∈G over the time interval
[0  T ]. Since for ∆t → 0  the transition probabilities of the process are Gaussian 
||Xt+∆t − Xt − f (Xt)∆t||2#  

pf (X0:T|f ) ∝ exp"−

(3)

1

2∆tXt

.
= (f (Xt))t∈G at these observations

the probability density for the path with a given drift function f
can be written as the product

where

pf (X0:T|f ) = p0(X0:T )L(X0:T|f ) 

p0(X0:T ) ∝ exp"−

1

2∆tXt

||Xt+∆t − Xt||2#

(4)

(5)

is the measure over paths without drift  i.e. a discretized version of the Wiener measure  and a term
which we will call likelihood in the following 

L(X0:T|f ) = exp"−

1

2Xt

||f (Xt)||2 ∆t +Xt

hf (Xt)  Xt+∆t − Xti# .

(6)

.
= u⊤D−1v and the corresponding squared norm

Here we have introduced the inner product hu  vi
||u||2 .

= u⊤D−1u to avoid cluttered notation.

To attempt a nonparametric Bayesian estimate of the drift function f (x)  we note that the expo-
nent in (6) contains the drift f at most quadratically. Hence it becomes clear that a conjugate prior
to the drift for this model is given by a Gaussian process  i.e. we assume that for each component
f ∼ P0(f ) = GP(0  K)  where K is a kernel [11]  a fact which was recently observed in [8]. We de-
note probabilities over the drift f by upper case symbols in order to avoid confusion with path prob-
abilities. Although a more general model is possible  we will restrict ourselves to the case where the

2

Figure 1: The left ﬁgure shows a snippet of the double well sample path in black and observations
as red dots. The right picture displays the estimated drift function for the double well model after
initialization  where the red line denotes the true drift function and the black line the mean function
with corresponding 95%-conﬁdence bounds (twice the standard deviation) in blue. One can clearly
see that the larger distance between the consecutive points leads to a wrong prediction.

GP priors over the components f j(x)  j = 1  . . .   d of the drift are independent (with usually differ-
ent kernels) and we assume that we have a diagonal diffusion matrix D = diag(σ2
d). In this
case  the GP posteriors of f j(x) are independent  too  and we can estimate drift components inde-
pendently by ordinary GP regression. We deﬁne data vectors by dj = ((X j
t∈G\{T } 
the kernel matrix Kj = (K j(Xs  Xt))s t∈G  and the test vector kj(x) = (K j(x  Xt))⊤
t∈G. Then a
standard calculation [11] shows that the posterior process over drift functions f has a posterior mean
and a GP posterior variance at an arbitrary point x is given by

t+∆t−X j

1  . . .   σ2

t )/∆t)⊤

¯f j(x) = kj(x)⊤ Kj +

σ2
j
∆t

I!−1

dj 

σ2

f j (x) = K j(x  x)−kj(x)⊤ Kj +

σ2
j
∆t

I!−1

kj(x). (7)

Note that σ2
j /∆t plays the role of the variance of the observation noise in the standard regression
case. In practice  the number of observations can be quite large for a ﬁne time discretization  and a
fast computation of (7) could become infeasible. A possible way out of this problem—as suggested
by [8]—could be a restriction to kernels for which the inverse kernel  the precision operator  is a
differential operator. A well known machine learning approach  which is based on a sparse Gaussian
process approximation  applies to arbitrary kernels and generalizes easily to multivariate SDE. We
have resorted speciﬁcally to the optimal Kullback-Leibler sparsity [1  12]  where the likelihood term
of a GP model is replaced by another effective likelihood  which depends only on a smaller set of
variables fs.

4 MAP Inference for sparse observations

The simple GP regression approach outlined in the previous section cannot be applied when obser-
.
= Xτk   k = 1  . . .   n
vations are sparse in time. In this setting  we assume that n observations yk
are obtained at (for simplicity) regular intervals τk = kτ   where τ ≫ ∆t is much larger than the
microscopic time scale. In this case  a discretization in (6)  where the sum over the microscopic grid
t ∈ G would be replaced by a sum over macroscopic times τk and ∆t by τ   would correspond to
a discrete time dynamical model of the form (1) again replacing ∆t by τ . But this discretization
would give a bad approximation to the true SDE dynamics. The estimator of the drift would give
some (approximate) estimation of the mean of the transition kernel over macroscopic times τ . How-
ever  this does usually not give a good approximation for the original drift. This can be seen in ﬁgure
1  where the red line corresponds to the true drift (of the so called double-well model [4]) and the
black line to its prediction based on observations with τ = 0.2 and the naive estimation method.

To deal with this problem  we treat the process Xt for times t between consecutive observations
kτ < t < (k + 1)τ as a latent unobserved random variable with a posterior path measure given by

p(X0:T|y  f ) ∝ p(X0:T|f )

n

Yk=1

δ(yk − Xkτ ) 

3

(8)

where y is the collection of observations yk and δ(·) denotes the Dirac-distribution encoding the
fact that the process is known perfectly at times τk. Our goal is to use an EM algorithm to compute
the maximum posterior (MAP) prediction for the drift function f (x). Unfortunately  exact posterior
expectations are intractable and one needs to work with suitable approximations.

4.1 Approximate EM algorithm

The EM algorithm cycles between two steps

1. In the E-step  we compute the expected negative logarithm of the complete data likelihood

L(f   q) = −Eq [ln L(X0:T|f )]  

(9)

where q denotes a measure over paths which approximates the intractable posterior
p(X0:T|y  fold) for the previous estimate fold of the drift.

2. In the M-Step  we recompute the drift function as

fnew = arg min

f

(L(f   q) − ln P0(f )) .

(10)

To compute the expectation in the E-step  we use (6) and take the limit ∆t → 0 at the end  when
expectations have been computed. As f (x) is a time-independent function  this yields

1

∆t→0

Eq(cid:2)||f (Xt)||2∆t − 2hf (Xt)  Xt+∆t − Xti(cid:3)

2Xt
−Eq[ln L(X0:T|f )] = lim
2Z T
Eq(cid:2)||f (Xt)||2 − 2hf (Xt)  gt(Xt)i(cid:3) dt
2Z ||f (x)||2A(x)dx −Z hf (x)  y(x)i dx.

=

=

1

1

0

Here qt(x) is the marginal density of Xt computed from the approximate posterior path measure q.
We have also deﬁned the corresponding approximate posterior drift

(11)

(12)

(13)

gt(x) = lim
∆t→0

1
∆t

Eq[Xt+∆t − Xt|Xt = x] 

as well as the functions

A(x) =Z T

0

qt(x)dt

and

gt(x)qt(x)dt.

y(x) =Z T

0

There are two main problems for a practical realization of this EM algorithm:

1. We need to ﬁnd tractable path measures q  which lead to good approximations for marginal

densities and posterior drifts given arbitrary prior drift functions f (x).

2. The M-Step requires a functional optimization  because (11) shows that L(f   q) − ln P0(f )
is actually a functional of f (x)  i.e. it contains a continuum of values f (x)  where x ∈ Rd.

4.2 Linear drift approximation: The Ornstein-Uhlenbeck bridge

For given drift f (·) and times t ∈ Ik in the interval Ik = [k τ ; (k + 1)τ ] between two consecutive
observations  the exact posterior marginal pt(x) equals the density of Xt = x conditioned on the
fact that Xkτ = yk and X(k+1)τ = yk+1. This can be expressed by the transition densities of the
homogeneous Markov diffusion process with drift f (x). We denote this quantity by ps(Xt+s|Xt)
being the density of the random variable Xt+s at time t + s conditioned on Xt at time t. Using the
Markov property  this yields the representation

pt(x) ∝ p(k+1)τ −t(yk+1|x)pt−kτ (x|yk) for t ∈ Ik.

(14)

As functions of t and x  the second factor fulﬁlls a forward Fokker-Planck equation and the ﬁrst one
a Kolmogorov backward equation [13]. Both are partial differential equations. Since exact compu-
tations are not feasible for general drift functions  we approximate the transition density ps(x|xk) in
each interval Ik by that of a process  where the drift f (x) is replaced by its local linearization

f (x) ≈ fou(x  t) = f (xk) − Γk(x − xk) with Γk = −∇f (xk).

(15)

4

This is equivalent to assuming that for t ∈ Ik the dynamics is approximated by the homogeneous
Ornstein-Uhlenbeck process [13]

dXt = [f (yk) − Γk(Xt − yk)]dt + D1/2dW 

(16)

which is also used to build computationally efﬁcient hierarchical models [14  15]  as in this case
the marginal posterior can be calculated analytically. Here the transition density is a multivariate
Gaussian

where αk = yk + Γ−1
using the matrix exponential

k f (yk) is the stationary mean and the variance Ss = AsB−1

s

q(k)
s

(x|y) = N (cid:0)x|αk + e−Γks(y − αk); Ss(cid:1)
k (cid:21) s(cid:19)(cid:20) 0
(cid:20) As
Bs(cid:21) = exp(cid:18)(cid:20) Γk
I(cid:21) .

D
0 −Γ⊤
(x) = N (x|m(t); C(t)) of the marginal posterior

(18)

(17)

is calculated

(20)

(21)

(22)

(23)

Then we obtain the Gaussian approximation q(k)
for t ∈ Ik by multiplying the two transition densities  where

t

C(t) = (cid:16)e−Γ⊤

m(t) = C(t) e−Γ⊤

k (tk+1−t)S−1

tk+1−te−Γk(tk+1−t) + S−1

and

k (tk+1−t)S−1

tk+1−t(cid:16)yk+1 − αk + e−Γk(tk+1−t)αk(cid:17)

+ C(t) S−1

t−tk(cid:16)αk + e−Γk(t−tk)(yk − αk)(cid:17) .

t−tk(cid:17)−1

By inspecting mean and variance we see that the distribution is a equivalent to a bridge between the
points X = yk and X = yk+1 and collapses to point masses at these points.

Within this approximation  we can estimate parameters such as the diffusion D using the approxi-
mate evidence

n−1

Finally  in this approximation we obtain for the posterior drift

p(y|f ) ≈ pou(y) = p(x1)

q(k)
τ

(yk+1|yk)

(19)

Yj=1

gt(x) = lim
∆t→0

1
∆t

E [Xt+∆t − Xt|Xt = x  Xτ = yk+1]

= f (yk) − Γk(x − yk) + De−Γ⊤

k (tk+1−t)S−1

tk+1−t(yk+1 − αk − e−Γk(tk+1−t)(x − αk))

as shown in appendix A in the supplementary material.

4.3 Sparse M-Step approximation

To cope with the functional optimization  we resort to a sparse approximation for replacing the
inﬁnite set f by a sparse set fs. Here the GP posteriors (for each component of the drift) is replaced
by one that is closest in the KL sense. Following appendix B in the supplementary material  we ﬁnd
that in the sparse approximation the likelihood (11) is replaced by

Ls(f   q) =

1

2Z ||E0[f (x)|fs]||2 A(x) dx −Z hE0[f (x)|fs]  y(x)i dx 

where the conditional expectation is over the GP prior. In order to avoid cluttered notation  it should
s   y(x)  σ2 
be noted that in the following results for a component f j  the quantities Λs  fs  ks  K−1
similar to (7) depend on the component j  but not A(x).

This is easily computed as

Hence

with

Λs =

1
σ2

E0[f (x)|fs] = k⊤

s (x)K−1

s fs.

1
2

s Λsf s − f ⊤
f ⊤

s ds

Ls(f   q) =
s (x)dx(cid:27) K−1

s  

5

K−1

s (cid:26)Z ks(x) A(x) k⊤

ds =

1
σ2

K−1

s Z ks(x) y(x) dx.

With these results  the approximate MAP estimate is

¯fs(x) = k⊤

s (x)(I + ΛsKs)−1ds.

(24)

The integrals over x in (23) can be computed analytically for many kernels of interest such as
polynomial and RBF ones. However  we have done this for 1-dimensional models only. For higher
dimensions  we found it more efﬁcient to treat both the time integration in (13) and the x integrals
by sampling  where time points t are drawn uniformly at random and x points from the multivariate
Gaussian qt(x).
A related expression for the variance σ2
s (x)(I + ΛKs)−1Λsks(x) can only be
viewed as a crude estimate  because it does not include the impact of the GP ﬂuctuations on the path
probabilities.

s (x) = K(x  x) − k⊤

5 A crude estimate of an approximation error

Unfortunately  there is no guarantee that this approximation to the EM algorithm will always in-
crease the exact likelihood p(y|f ). Here  we will develop a crude estimate how p(y|f ) differs
.
from the the Ornstein-Uhlenbeck approximation (19) to lowest order in the difference δf (Xt  t)
=
f (Xt) − fou(Xt  t) between drift function and its approximation.
Our estimate is based on the exact expression

p(y|f ) =Z dp0(X0:T ) eln L(X0:T |f )

n

Yk=1

δ(yk − Xkτ )

(25)

where the Wiener measure p0 is deﬁned in (5) and the likelihood L(X0:T|f ) in (6). The Ornstein-
Uhlenbeck approximation (19) can expressed in a similar way: we just have to replace L(X0:T|f )
by a functional Lou(X0:T|f ) which in turn is obtained by replacing f (Xt) with the linearized drift
fou(Xt  t) in (6). The difference in free energies (negative log evidences) can be expressed ex-
actly by an expectation over the posterior OU processes and then expanded (similar to a cumulant
expansion) in a Taylor series in ∆L = − ln(L/Lou). The ﬁrst two terms are given by

.

∆F

= −{ln p(y|f ) − ln pou(y)} = − ln Eq(cid:2)e−∆L(cid:3) ≈ Eq [∆L] −

The computation of the ﬁrst term is similar to (11) and requires only the marginal qt and the posterior
gt. The second term contains the posterior variance and requires two-time covariances of the OU
process. We concentrate on the ﬁrst term which we further expand in the difference δf (Xt  t). This
yields

1
2

Varq [∆L] ± . . .

(26)

∆F ≈ Eq [∆L] ≈Z T

0

Eq [hδf (Xt  t)  fou(Xt  t) − gt(Xt)i] dt.

(27)

This expression could be evaluated in order to estimate the inﬂuence of nonlinear parts of the drift
on the approximation error.

6 Experiments

In all experiments  we used different versions of the following general kernel  which is a linear
combination of a RBF and a polynomial kernel 

K(x1  x2) = c σRBF exp(cid:18)−

(x1 − x2)T (x1 − x2)

2l2

RBF

(cid:19) + (1 − c)(cid:0)1 + x⊤

1 x2(cid:1)p

 

(28)

where the hyperparameters σRBF and lRBF denote the variance and length scale of the RBF kernel
and p denotes the order of the polynomial kernel.

Also  we determined the sparse points for the GP algorithm in each case by ﬁrst constructing a
histogram over the observations and then selecting the set of histogram midpoints of each histogram
bin which contained at least a certain number bmin of observations. In our experiments  we chose
bmin = 5.

6

Figure 2: The ﬁgures show the estimated drift functions for the double well model (left) and the
periodic diffusion model (right) after completion of the EM algorithm. Again  the black and blue
lines denote mean and 95%-conﬁdence bounds  while the red lines indicate the true drift functions.

6.1 One-dimensional toy models

First we test our algorithm on two toy data sets  the double well model with dynamics given by the
SDE

(29)

(30)

and a diffusion model driven by a periodic drift

dx = 4(x − x3)dt + dW

dx = sin(x)dt + dW.

For both models  we simulated a path of size M = 105 on a regular grid with width ∆t = 0.01 from
the corresponding SDE and kept every 20th sample point as observation  resulting in N = 5000
data points. We initialized the EM Algorithm by running the sparse GP for the observation points
without any imputation and subsequently computed the expectation operators by analytically evalu-
ating the expressions on the same time grid as the simulated path and summing over the time steps.
An alternative initialization strategy which consists of generating a full trajectory of the same size as
the original path using Brownian bridge sampling between observations did not bring any noticeable
performance improvements. Since we cannot guarantee that the likelihood increases in every itera-
tion due to the approximation in the E-step  we resort to a simple heuristic by assuming convergence
once L stabilizes up to some minor ﬂuctuation. In our experiments convergence was typically at-
tained after a few (< 10) iterations. For the double well model we used an equal weighting c = 0.5
between kernels with hyperparameters σRBF = 1  lRBF = 0.5 and p = 5  whereas for the periodic
model we used an RBF kernel (c = 1) with the same values for σRBF and lRBF.

6.2 Application to a real data set

As an example of a real world data set  we used the NGRIP ice core data (provided by Niels-
Bohr institute in Copenhagen  http://www.iceandclimate.nbi.ku.dk/data/)  which
provides an undisturbed ice core record containing climatic information stretching back into the
last glacial. Speciﬁcally  this data set as shown in ﬁgure 3 contains 4918 observations of oxygen
isotope concentration δ18O over a time period from the present to roughly 1.23 · 105 years into

the past. Since there are generally less isotopes in ice formed under cold conditions  the isotope
concentration can be regarded as an indicator of past temperatures.

Recent research [16] suggest to model the rapid paleoclimatic changes exhibited in the data set
by a simple dynamical system with polynomial drift function of order p = 3 as canonical model
which allows for bistability. This corresponds to a meta stable state at higher temperatures close to
marginal stability and a stable state at low values  which is consistent with other research on this
data set  linking a stable state of oxygen isotopes to a baseline temperature and a region at higher
values corresponding to the occurrence of rapid temperature spikes. For this particular problem we
ﬁrst tried to determine the diffusion constant σ of the data. Therefore we estimated the likelihood of
the data set for 40 ﬁxed values of σ in an interval from 0.3 to 11.5 by running the EM algorithm with
a polynomial kernel (c = 0) of order p = 3 for each value in turn. The resulting drift function with
the highest likelihood is shown in ﬁgure 3. The result seems to conﬁrm the existence of a metastable
state of oxygen isotope concentration and a stable state at lower values.

7

Figure 3: The ﬁgure on the left displays the NGRIP data set  while the picture on the right shows
the estimated drift in black with corresponding 95%-conﬁdence bounds denoting twice the standard
deviation in blue for the optimal diffusion value ˆσ = 2.9.

Figure 4: The left ﬁgure shows the empirical density for the two-dimensional model  together with
the vector ﬁelds of the actual drift function given in blue and the estimated drift given in red. The
right picture shows a snippet from the full sample in black together with the ﬁrst 20 observations
denoted by red dots.

6.3 Two-dimensional toy model

As an example of a two dimensional system  we simulated from a process with the following SDE:

dx = (x(1 − x2 − y2) − y)dt + dW1 
dy = (y(1 − x2 − y2) + y)dt + dW2.

(31)

(32)

For this model we simulated a path of size M = 106 on a regular grid with width ∆t = 0.002 from
the corresponding SDE and kept every 100th sample point as observation  resulting in N = 104 data
points. In the inference shown in ﬁgure 4 we used a polynomial kernel (c = 0) of order p = 4.

7 Discussion

It would be interesting to replace the ad hoc local linear approximation of the posterior drift by a
more ﬂexible time dependent Gaussian model. This could be optimized in a variational EM approx-
imation by minimizing a free energy in the E-step  which contains the Kullback-Leibler divergence
between the linear and true processes. Such a method could be extended to noisy observations and
the case  where some components of the state vector are not observed. Finally  this method could be
turned into a variational Bayesian approximation  where one optimizes posteriors over both drifts
and over state paths. The path probabilities are then inﬂuenced by the uncertainties in the drift
estimation  which would lead to more realistic predictions of error bars.

Acknowledgments This work was supported by the European Community’s Seventh Framework
Programme (FP7  2007-2013) under the grant agreement 270327 (CompLACS).

8

References

[1] Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. JMLR

WC&P  5:567–574  2009.

[2] Marc Deisenroth and Shakir Mohamed. Expectation propagation in Gaussian process dynamical systems.
In P. Bartlett  F.C.N. Pereira  C.J.C. Burges  L. Bottou  and K.Q. Weinberger  editors  Advances in Neural
Information Processing Systems 25  pages 2618–2626. 2012.

[3] Jonathan Ko and Dieter Fox. GP-BayesFilters: Bayesian ﬁltering using Gaussian process prediction and

observation models. Autonomous Robots  27(1):75–90  July 2009.

[4] C´edric Archambeau  Manfred Opper  Yuan Shen  Dan Cornford  and John Shawe-Taylor. Variational
inference for diffusion processes. In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in
Neural Information Processing Systems 20  pages 17–24. MIT Press  Cambridge  MA  2008.

[5] Jos´e Bento Ayres Pereira  Morteza Ibrahimi  and Andrea Montanari. Learning networks of stochastic
In J. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R.S. Zemel  and A. Culotta 

differential equations.
editors  Advances in Neural Information Processing Systems 23  pages 172–180. 2010.

[6] Danilo J. Rezende  Daan Wierstra  and Wulfram Gerstner. Variational learning for recurrent spiking
In J. Shawe-Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger  editors 

networks.
Advances in Neural Information Processing Systems 24  pages 136–144. 2011.

[7] Simon Lyons  Amos Storkey  and Simo Sarkka. The coloured noise expansion and parameter estimation
In P. Bartlett  F.C.N. Pereira  C.J.C. Burges  L. Bottou  and K.Q. Weinberger 

of diffusion processes.
editors  Advances in Neural Information Processing Systems 25  pages 1961–1969. 2012.

[8] Omiros Papaspiliopoulos  Yvo Pokern  Gareth O. Roberts  and Andrew M. Stuart. Nonparametric esti-

mation of diffusions: a differential equations approach. Biometrika  99(3):511–531  2012.

[9] Yvo Pokern  Andrew M. Stuart  and J.H. van Zanten. Posterior consistency via precision operators
for Bayesian nonparametric drift estimation in SDEs. Stochastic Processes and their Applications 
123(2):603–628  2013.

[10] P. E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Springer  New

York  corrected edition  June 2011.

[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press  2006.

[12] Lehel Csat´o  Manfred Opper  and Ole Winther. TAP Gibbs free energy  belief propagation and sparsity.
In T. G. Dietterich  S. Becker  and Z. Ghahramani  editors  Advances in Neural Information Processing
Systems 14  pages 657–663. MIT Press  2002.

[13] C. W. Gardiner. Handbook of Stochastic Methods. Springer  Berlin  second edition  1996.

[14] Manfred Opper  Andreas Ruttor  and Guido Sanguinetti. Approximate inference in continuous time
In J. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R.S. Zemel  and A. Cu-

Gaussian-jump processes.
lotta  editors  Advances in Neural Information Processing Systems 23  pages 1831–1839. 2010.

[15] Florian Stimberg  Manfred Opper  and Andreas Ruttor. Bayesian inference for change points in dynamical
systems with reusable states—a Chinese restaurant process approach. JMLR WC&P  22:1117–1124 
2012.

[16] Frank Kwasniok. Analysis and modelling of glacial climate transitions using simple dynamical systems.
Philosophical Transactions of the Royal Society A: Mathematical  Physical and Engineering Sciences 
371(1991)  2013.

9

,Andreas Ruttor
Philipp Batz
Manfred Opper
Agnieszka Grabska-Barwinska
Jonathan Pillow