2015,Fighting Bandits with a New Kind of Smoothness,We focus on the adversarial multi-armed bandit problem. The EXP3 algorithm of Auer et al. (2003) was shown to have a regret bound of $O(\sqrt{T N \log N})$  where $T$ is the time horizon and $N$ is the number of available actions (arms). More recently  Audibert and Bubeck (2009) improved the bound by a logarithmic factor via an entirely different method. In the present work  we provide a new set of analysis tools  using the notion of convex smoothing  to provide several novel algorithms with optimal guarantees. First we show that regularization via the Tsallis entropy matches the minimax rate of Audibert and Bubeck (2009) with an even tighter constant; it also fully generalizes EXP3. Second we show that a wide class of perturbation methods lead to near-optimal bandit algorithms as long as a simple condition on the perturbation distribution $\mathcal{D}$ is met: one needs that the hazard function of $\mathcal{D}$ remain bounded. The Gumbel  Weibull  Frechet  Pareto  and Gamma distributions all satisfy this key property; interestingly  the Gaussian and Uniform distributions do not.,Fighting Bandits with a New Kind of Smoothness

Jacob Abernethy

University of Michigan

jabernet@umich.edu

Chansoo Lee

University of Michigan

chansool@umich.edu

Ambuj Tewari

University of Michigan
tewaria@umich.edu

Abstract

We provide a new analysis framework for the adversarial multi-armed bandit
problem. Using the notion of convex smoothing  we deﬁne a novel family of
algorithms with minimax optimal regret guarantees. First  we show that regular-
ization via the Tsallis entropy  which includes EXP3 as a special case  matches
the O(pN T ) minimax regret with a smaller constant factor. Second  we show
that a wide class of perturbation methods achieve a near-optimal regret as low
as O(pN T log N )  as long as the perturbation distribution has a bounded haz-
ard function. For example  the Gumbel  Weibull  Frechet  Pareto  and Gamma
distributions all satisfy this key property and lead to near-optimal algorithms.

1

Introduction

The classic multi-armed bandit (MAB) problem  generally attributed to the early work of Robbins
(1952)  poses a generic online decision scenario in which an agent must make a sequence of choices
from a ﬁxed set of options. After each decision is made  the agent receives some feedback in the
form of a loss (or gain) associated with her choice  but no information is provided on the outcomes
of alternative options. The agent’s goal is to minimize the total loss over time  and the agent is thus
faced with the balancing act of both experimenting with the menu of choices while also utilizing
the data gathered in the process to improve her decisions. The MAB framework is not only mathe-
matically elegant  but useful for a wide range of applications including medical experiments design
(Gittins  1996)  automated poker playing strategies (Van den Broeck et al.  2009)  and hyperparam-
eter tuning (Pacula et al.  2012).
Early MAB results relied on stochastic assumptions (e.g.  IID) on the loss sequence (Auer et al. 
2002; Gittins et al.  2011; Lai and Robbins  1985). As researchers began to establish non-stochastic 
worst-case guarantees for sequential decision problems such as prediction with expert advice (Little-
stone and Warmuth  1994)  a natural question arose as to whether similar guarantees were possible
for the bandit setting. The pioneering work of Auer  Cesa-Bianchi  Freund  and Schapire (2003) an-
swered this in the afﬁrmative by showing that their algorithm EXP3 possesses nearly-optimal regret
bounds with matching lower bounds. Attention later turned to the bandit version of online linear
optimization  and several associated guarantees were published the following decade (Abernethy
et al.  2012; Dani and Hayes  2006; Dani et al.  2008; Flaxman et al.  2005; McMahan and Blum 
2004).
Nearly all proposed methods have relied on a particular algorithmic blueprint; they reduce the ban-
dit problem to the full-information setting  while using randomization to make decisions and to
estimate the losses. A well-studied family of algorithms for the full-information setting is Follow
the Regularized Leader (FTRL)  which optimizes the objective function of the following form:

arg min

L>x + R(x)

(1)

where K is the decision set  L is (an estimate of) the cumulative loss vector  and R is a regularizer 
a convex function with suitable curvature to stabilize the objective. The choice of regularizer R is

x2K

1

critical to the algorithm’s performance. For example  the EXP3 algorithm (Auer  2003) regularizes
with the entropy function and achieves a nearly optimal regret bound when K is the probability sim-
plex. For a general convex set  however  other regularizers such as self-concordant barrier functions
(Abernethy et al.  2012) have tighter regret bounds.
Another class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL)
(Kalai and Vempala  2005) whose foundations date back to the earliest work in adversarial online
learning (Hannan  1957). Here we choose a distribution D on RN  sample a random vector Z ⇠D  
and solve the following linear optimization problem

arg min

(L + Z)>x.

(2)

x2K

FTPL is computationally simpler than FTRL due to the linearity of the objective  but it is analytically
much more complex due to the randomness. For every different choice of D  an entirely new set of
techniques had to be developed (Devroye et al.  2013; Van Erven et al.  2014). Rakhlin et al. (2012)
and Abernethy et al. (2014) made some progress towards unifying the analysis framework. Their
techniques  however  are limited to the full-information setting.
In this paper  we propose a new analysis framework for the multi-armed bandit problem that uniﬁes
the regularization and perturbation algorithms. The key element is a new kind of smoothness prop-
erty  which we call differential consistency. It allows us to generate a wide class of both optimal and
near-optimal algorithms for the adversarial multi-armed bandit problem. We summarize our main
results:

1. We show that regularization via the Tsallis entropy leads to the state-of-the-art adversarial MAB
algorithm  matching the minimax regret rate of Audibert and Bubeck (2009) with a tighter con-
stant. Interestingly  our algorithm fully generalizes EXP3.

2. We show that a wide array of well-studied noise distributions lead to near-optimal regret bounds
(matching those of EXP3). Furthermore  our analysis reveals a strikingly simple and appealing
sufﬁcient condition for achieving O(pT ) regret: the hazard rate function of the noise distribution
must be bounded by a constant. We conjecture that this requirement is in fact both necessary and
sufﬁcient.

2 Gradient-Based Prediction Algorithms for the Multi-Armed Bandit

Let us now introduce the adversarial multi-armed bandit problem. On each round t = 1  . . .   T  
a learner must choose a distribution pt 2 N over the set of N available actions. The adversary
(Nature) chooses a vector gt 2 [1  0]N of losses  the learner samples it ⇠ pt  and plays action it.
After selecting this action  the learner observes only the value gt it  and receives no information as
to the values gt j for j 6= it. This limited information feedback is what makes the bandit problem
much more challenging than the full-information setting in which the entire gt is observed.
The learner’s goal is to minimize the regret. Regret is deﬁned to be the difference in the realized
loss and the loss of the best ﬁxed action in hindsight:

RegretT := max
i2[N ]

(gt i  gt it).

(3)

TXt=1

To be precise  we consider the expected regret  where the expectation is taken with respect to the
learner’s randomization.

Loss vs. Gain Note: We use the term “loss” to refer to g  although the maximization in (3) would
imply that g should be thought of as a “gain” instead. We use the former term  however  as we
impose the assumption that gt 2 [1  0]N throughout the paper.
2.1 The Gradient-Based Algorithmic Template

Our results focus on a particular algorithmic template described in Framework 1  which is a slight
variation of the Gradient Based Prediction Algorithm (GBPA) of Abernethy et al. (2014). Note that

2

the algorithm (i) maintains an unbiased estimate of the cumulative losses ˆGt  (ii) updates ˆGt by
adding a single round estimate ˆgt that has only one non-zero coordinate  and (iii) uses the gradient
of a convex function ˜ as sampling distribution pt. The choice of ˜ is ﬂexible but ˜ must be a
differentiable convex function and its derivatives must always be a probability distribution.
Framework 1 may appear restrictive but it has served as the basis for much of the published work on
adversarial MAB algorithms (Auer et al.  2003; Kujala and Elomaa  2005; Neu and Bart´ok  2013).
First  the GBPA framework essentially encompasses all FTRL and FTPL algorithms (Abernethy
et al.  2014)  which are the core techniques not only for the full information settings  but also for
the bandit settings. Second  the estimation scheme ensures that ˆGt remains an unbiased estimate of
Gt. Although there is some ﬂexibility  any unbiased estimation scheme would require some kind
of inverse-probability scaling—information theory tells us that the unbiased estimates of a quantity
that is observed with only probabilty p must necessarily involve ﬂuctuations that scale as O(1/p).

Framework 1: Gradient-Based Prediction Alg. (GBPA) Template for Multi-Armed Bandit
GBPA( ˜): ˜ is a differentiable convex function such that r ˜ 2 N and ri ˜ > 0 for all i.
Initialize ˆG0 = 0
for t = 1 to T do

Nature: A loss vector gt 2 [1  0]N is chosen by the Adversary
Sampling: Learner chooses it according to the distribution p( ˆGt1) = rt( ˆGt1)
Cost: Learner “gains” loss gt it
Estimation: Learner “guesses” ˆgt := gt it
Update: ˆGt = ˆGt1 + ˆgt

pit ( ˆGt1)

eit

Lemma 2.1. Deﬁne (G) ⌘ maxi Gi so that we can write the expected regret of GBPA( ˜) as

Then  the expected regret of the GBPA( ˜) can be written as:

ERegretT =( GT ) PT

t=1hr ˜( ˆGt1)  gti.

ERegretT  ˜(0)  (0)
}

{z

overestimation penalty

|

+Ei1 ... it1 ( ˆGT )  ˜( ˆGT )
}

underestimation penalty

{z

|

+

TXt=1

Eit[D ˜( ˆGt  ˆGt1)| ˆGt1]
}
|

divergence penalty

{z

 

(4)

where the expectations are over the sampling of it.

Proof. Let ˜ be a valid convex function for the GBPA. Consider GBPA( ˜) being run on the loss
sequence g1  . . .   gT . The algorithm produces a sequence of estimated losses ˆg1  . . .   ˆgT . Now
consider GBPA-NE( ˜)  which is GBPA( ˜) run with the full information on the deterministic loss
sequence ˆg1  . . .   ˆgT (there is no estimation step  and the learner updates ˆGt directly). The regret of
this run can be written as

( ˆGT ) PT

t=1hr ˜( ˆGt1)  ˆgti 

and (GT )  ( ˆGT ) by the convexity of . Hence  it sufﬁces to show that the GBPA-NE( ˜) has
regret at most the righthand side of Equation 4  which is a fairly well-known result in online learning
literature; see  for example  (Cesa-Bianchi and Lugosi  2006  Theorem 11.6) or (Abernethy et al. 
2014  Section 2). For completeness  we included the full proof in Appendix A.

2.2 A New Kind of Smoothness

What has emerged as a guiding principle throughout machine learning is that enforcing stability of
an algorithm can often lead immediately to performance guarantees—that is  small modiﬁcations of
the input data should not dramatically alter the output. In the context of GBPA  algorithmic stability
is guaranteed as long as the dervative r ˜(·) is Lipschitz. Abernethy et al. (2014) explored a set of
conditions on r2 ˜(·) that lead to optimal regret guarantees for the full-information setting. Indeed 

3

this work discussed different settings where the regret depends on an upper bound on either the
nuclear norm or the operator norm of this hessian.
In short  regret in the full information setting relies on the smoothness of the choice of ˜. In the
bandit setting  however  merely a uniform bound on the magnitude of r2 ˜ is insufﬁcient to guar-
antee low regret; the regret (Lemma 2.1) involves terms of the form D ˜( ˆGt1 + ˆgt  ˆGt1)  where
the incremental quantity ˆgt can scale as large as the inverse of the smallest probability of p( ˆGt1).
What is needed is a stronger notion of the smoothness that bounds r2 ˜ in correspondence with r ˜ 
and we propose the following deﬁnition:
Deﬁnition 2.2 (Differential Consistency). For constants   C > 0  we say that a convex function
˜(·) is (  C )-differentially-consistent if for all G 2 (1  0]N  
˜(G)  C(ri ˜(G)).

r2

ii

We now prove a useful bound that emerges from differential consistency  and in the following two
sections we shall show how this leads to regret guarantees.
Theorem 2.3. Suppose ˜ is (  C )-differentially-consistent for constants C   > 0. Then diver-
gence penalty at time t in Lemma 2.1 can be upper bounded as:

Eit[D ˜( ˆGt  ˆGt1)| ˆGt1]  C

NXi=1⇣ri ˜( ˆGt1)⌘1

.

Proof. For the sake of clarity  we drop the subscripts; we use ˆG to denote the cumulative estimate
ˆGt1  ˆg to denote the marginal estimate ˆgt = ˆGt  ˆGt1  and g to denote the true loss gt.
Note that by deﬁnition of Algorithm 1  ˆg is a sparse vector with one non-zero and non-positive
coordinate ˆgit = gt i/ri ˜( ˆG). Plus  it is conditionally independent given ˆG. For a ﬁxed it  Let
so that h00(r) = (ˆg/kˆgk)>r2 ˜⇣ ˆG + tˆg/kˆgk⌘ (ˆg/kˆgk) = e>itr2 ˜⇣ ˆG  teit⌘ eit. Now we can

h(r) := D ˜( ˆG + rˆg/kˆgk  ˆG) = D ˜( ˆG + reit  ˆG) 

write

The ﬁrst inequality is by the supposition and the second inequality is due to the convexity of ˜
which guarantees that ri is an increasing function in the i-th coordinate. Interestingly  this part
of the proof critically depends on the fact that the we are in the “loss” setting where g is always
non-positive.

3 A Minimax Bandit Algorithm via Tsallis Smoothing

The design of a multi-armed bandit algorithm in the adversarial setting proved to be a challenging
task. Ignoring the dependence on N for the moment  we note that the initial published work on
EXP3 provided only an O(T 2/3) guarantee (Auer et al.  1995)  and it was not until the ﬁnal version
of this work (Auer et al.  2003) that the authors obtained the optimal O(pT ) rate. For the more

4

0 h00(r) dr ds

R s
i=1 P[it = i]R kˆgk
Eit[D ˜( ˆG + ˆg  ˆG)| ˆG] =PN
0 e>i r2 ˜⇣ ˆG  rei⌘ ei dr ds
R s
i=1 ri ˜( ˆG)R kˆgk
=PN
0 C⇣ri ˜( ˆG  rei)⌘
R s
i=1 ri ˜( ˆG)R kˆgk
PN
0 C⇣ri ˜( ˆG)⌘
R s
i=1 ri ˜( ˆG)R kˆgk
PN
i=1⇣ri ˜( ˆG)⌘1+R kˆgk
R s
= CPN
i=1⇣ri ˜( ˆG)⌘1
i  CPN
2 PN

i=1⇣ri ˜( ˆG)⌘1

0 dr ds

dr ds

dr ds

= C

0

0

0

0

0

g2

.

general setting of online linear optimization  several sub-optimal rates were achieved (Dani and
Hayes  2006; Flaxman et al.  2005; McMahan and Blum  2004) before the desired pT was obtained
(Abernethy et al.  2012; Dani et al.  2008).
We can view EXP3 as an instance of GBPA where the potential function ˜(·) is the Fenchel con-
jugate of the Shannon entropy. For any p 2 N  the (negative) Shannon entropy is deﬁned as
H(p) := Pi pi log pi  and its Fenchel conjugate is H ?(G) = supp2N{hp  Gi  ⌘H (p)}. In
⌘ log (Pi exp(⌘Gi)) . By
fact  we have a closed-form expression for the supremum: H ?(G) = 1
inspecting the gradient of the above expression  it is easy to see that EXP3 chooses the distribution
pt = rH ?(G) every round.
The tighter EXP3 bound given by Auer et al. (2003) scaled according to O(pT N log N ) and the
authors provided a matching lower bound of the form ⌦(pT N ). It remained an open question for
some time whether there exists a minimax optimal algorithm that does not contain the log term un-
til Audibert and Bubeck (2009) proposed the Implicitly Normalized Forecaster (INF). The INF is
implicitly deﬁned via a specially-designed potential function with certain properties. It was not im-
mediately clear from this result how to deﬁne a minimax-optimal algorithm using the now-standard
tools of regularization and Bregman divergence.
More recently  Audibert et al. (2011) improved upon Audibert and Bubeck (2009)  extending the
results to the combinatorial setting  and they also discovered that INF can be interpreted in terms
of Bregman divergences. We give here a reformulation of INF that leads to a very simple analysis
in terms of our notion of differential consistency. Our reformulation can be viewed as a variation
of EXP3  where the key modiﬁcation is to replace the Shannon entropy function with the Tsallis
entropy1 for parameter 0 <↵< 1:

In particular  the choice of ↵ = 1

ERegret  2q N T

↵(1↵) .

2 gives a regret of no more than 4pN T .

Proof of Theorem 3.1. We will bound each penalty term in Lemma 2.1. Since S↵ is non-positive 
the underestimation penalty is upper bounded by 0 and the overestimation penalty is at most
( min S↵). The minimum of S↵ occurs at (1/N  . . .   1/N ). Hence 

(overestimation penalty)  

⌘

1  ↵ 1 

NXi=1

1

N ↵!  ⌘(N 1↵  1).

(6)

1More precisely  the function we give here is the negative Tsallis entropy according to its original deﬁnition.

5

S↵(p) =

1

1  ↵⇣1 X p↵
i⌘ .

S↵(·) ! H(·)

as ↵ ! 1.

This particular function  proposed by Tsallis (1988)  possesses a number of natural properties. The
Tsallis entropy is in fact a generalization of the Shannon entropy  as one obtains the latter as a special
case of the former asymptotically. That is  it is easy to prove the following uniform convergence:

We emphasize again that one can easily show that Tsallis-smoothing bandit algorithm is indeed
identical to INF using the appropriate parameter mapping  although our analysis is simpler due to
the notion of differential consistency (Deﬁnition 2.2).
Theorem 3.1. Let ˜(G) = maxp2N{hp  Gi  ⌘S↵(p)}. Then the GBPA( ˜) has regret at most
(5)

+

.

ERegret  ⌘

N 1↵  1
1  ↵

N ↵T
⌘↵

Before proving the theorem  we note that it immediately recovers the EXP3 upper bound as a special
case ↵ ! 1. An easy application of L’Hˆopital’s rule shows that as ↵ ! 1  N 1↵1
1↵ ! log N and
N ↵/↵ ! N. Choosing ⌘ = p(N log N )/T   we see that the right-hand side of (5) tends to
2pT N log N. However the choice ↵ ! 1 is clearly not the optimal choice  as we show in the
following statement  which directly follows from the theorem once we see that N 1↵  1 < N 1↵.
Corollary 3.2. For any ↵ 2 (0  1)  if we choose ⌘ =q ↵N 12↵

(1↵)T then we have

Now it remains to upper bound the divergence penalty with (⌘↵)1N ↵T . We observe that straight-
forward calculus gives r2S↵(p) = ⌘↵diag(p↵2
N ). Let IN (·) be the indicator function
of N; that is  IN (x) = 0 for x 2 N and IN (x) = 1 for x /2 N. It is clear that ˜(·) is
the dual of the function S↵(·) + IN (·)  and moreover we observe that r2S↵(p) is a sub-hessian of
S↵(·) + IN (·) at p(G)  following the setup of Penot (1994). Taking advantage of Proposition 3.2
in the latter reference  we conclude that r2S↵(p(G)) is a super-hessian of ˜= S⇤↵ at G. Hence 

  . . .   p↵2

1

r2 ˜(G)  (⌘↵)1diag(p2↵

1

(G)  . . .   p2↵

N (G))

for any G. What we have stated  indeed  is that ˜ is (2  ↵  (⌘↵)1)-differentially-consistent  and
thus applying Theorem 2.3 gives

D ˜( ˆGt  ˆGt1)  (⌘↵)1

NXi=1⇣pi( ˆGt1)⌘1↵

.

Noting that the 1
to any probability distribution p1  . . .   pN to obtain

↵-norm and the

1↵-norm are dual to each other  we can apply H¨older’s inequality

1

NXi=1

p1↵
i

=

p1↵
i

NXi=1

· 1  NXi=1

p

1↵
1↵

i !1↵ NXi=1

1

↵!↵

1

= (1)1↵N ↵ = N ↵.

So  the divergence penalty is at most (⌘↵)1N ↵  which completes the proof.

4 Near-Optimal Bandit Algorithms via Stochastic Smoothing

Let D be a continuous distribution over an unbounded support with probability density function f
and cumulative density function F . Consider the GBPA( ˜(G;D)) where

˜(G;D) = EZ1 ... ZN

i {Gi + Zi}
max

iid⇠D

which is a stochastic smoothing of (maxi Gi) function. Since the max function is convex  ˜ is also
convex. By Bertsekas (1973)  we can swap the order of differentiation and expectation:

˜(G;D) = EZ1 ... ZN

iid⇠D

ei⇤  where i⇤ = arg max

i=1 ... N {Gi + Zi}.

(7)

Even if the function is not differentiable everywhere  the swapping is still possible with any sub-
gradient as long as they are bounded. Hence  the ties between coordinates (which happen with
It is clear that r ˜ is in the
probability zero anyways) can be resolved in an arbitrary manner.
probability simplex  and note that

@ ˜
@Gi

= EZ1 ... ZN 1{Gi + Zi > Gj + Zj 8j 6= i}
= E ˜Gj⇤

[PZi[Zi > ˜Gj⇤  Gi]] = E ˜Gj⇤

(8)
where ˜Gj⇤ = maxj6=i Gj + Zj. The unbounded support condition guarantees that this partial
derivative is non-zero for all i given any G. So  ˜(G;D) satisﬁes the requirements of Algorithm 1.
4.1 Connection to Follow the Perturbed Leader

[1  F ( ˜Gj⇤  Gi)]

There is a straightforward way to efﬁciently implement the sampling step of the bandit GBPA (Al-
gorithm 1) with a stochastically smoothed function. Instead of evaluating the expectation of Equa-
tion 7  we simply take a random sample. In fact  this is equivalent to Follow the Perturbed Leader
Algorithm (FTPL) (Kalai and Vempala  2005) for bandit settings. On the other hand  implementing
the estimation step is hard because generally there is no closed-form expression for r ˜.
To address this issue  Neu and Bart´ok (2013) proposed Geometric Resampling (GR). GR uses an
iterative resampling process to estimate ri ˜. This process gives an unbiased estimate when allowed

6

to run for an unbounded number of iterations. Even when we truncate the resampling process after
M iterations  the extra regret due to the estimation bias is at most N T
eM (additive term). Since the
lower bound for the multi-armed bandit problem is O(pN T )  any choice of M = O(pN T ) does
not affect the asymptotic regret of the algorithm. In summary  all our GBPA regret bounds in this
section hold for the corresponding FTPL algorithm with an extra additive N T
Despite the fact that perturbation-based algorithms provide a natural randomized decision strategy 
they have seen little applications mostly because they are hard to analyze. But one should expect
general results to be within reach: the EXP3 algorithm  for example  can be viewed through the
lens of perturbations  where the noise is distributed according to the Gumbel distribution. Indeed 
an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about
through the use of exponentially-distributed noise  and the same perturbation strategy has more
recently been utilized in the work of Neu and Bart´ok (2013) and Koc´ak et al. (2014). However 
a more general understanding of perturbation methods has remained elusive. For example  would
Gaussian noise be sufﬁcient for a guarantee? What about  say  the Weibull distribution?

eM term in the bound.

4.2 Hazard Rate analysis
In this section  we show that the performance of the GBPA( ˜(G;D)) can be characterized by the
hazard function of the smoothing distribution D. The hazard rate is a standard tool in survival
analysis to describe failures due to aging; for example  an increasing hazard rate models units that
deteriorate with age while a decreasing hazard rate models units that improve with age (a counter
intuitive but not illogical possibility). To the best of our knowledge  the connection between hazard
rates and design of adversarial bandit algorithms has not been made before.
Deﬁnition 4.1 (Hazard rate function). Hazard rate function of a distribution D is

hD(x) :=

f (x)

1  F (x)

For the rest of the section  we assume that D is unbounded in the direction of +1  so that the hazard
function is well-deﬁned everywhere. This assumption is for the clarity of presentation and can be
easily removed (Appendix B).
Theorem 4.2. The regret of the GBPA on ˜(L) = EZ1 ... Zn⇠D maxi{Gi + ⌘Zi} is at most:

N (sup hD)

⌘

T + ⌘EZ1 ... Zn⇠Dhmax

i

Zii

Proof. We analyze each penalty term in Lemma 2.1. Due to the convexity of   the underestimation
penalty is non-positive. The overestimation penalty is clearly at most EZ1 ... Zn⇠D[maxi Zi]  and
Lemma 4.3 proves the N (sup hD) upper bound on the divergence penalty.
It remains to provide the tuning parameter ⌘. Suppose we scale the perturbation Z by ⌘> 0  i.e.  we
add ⌘Zi to each coordinate. It is easy to see that E[maxi=1 ... n ⌘Xi] = ⌘E[maxi=1 ... n Xi]. For the
divergence penalty  let F⌘ be the CDF of the scaled random variable. Observe that F⌘(t) = F (t/⌘)
and thus f⌘(t) = 1
Lemma 4.3. The divergence penalty of the GBPA with ˜(G) = EZ⇠D maxi{Gi + Zi} is at most
N (sup hD) each round.
Proof. Recall the gradient expression in Equation 8. The i-th diagonal entry of the Hessian is:

⌘ f (t/⌘). Hence  the hazard rate scales by 1/⌘  which completes the proof.

˜(G) =

r2

ii

[1  F ( ˜Gj⇤  Gi)] = E ˜Gj⇤ @
E ˜Gj⇤
[h( ˜Gj⇤  Gi)(1  F ( ˜Gj⇤  Gi))]
[1  F ( ˜Gj⇤  Gi)]

@
@Gi
= E ˜Gj⇤
 (sup h)E ˜Gj⇤
= (sup h)ri(G)

@Gi

(1  F ( ˜Gj⇤  Gi)) = E ˜Gj⇤

f ( ˜Gj⇤  Gi)
(9)

where ˜Gj⇤ = maxj6=i{Gj + Zj} which is a random variable independent of Zi. We now apply
Theorem 2.3 with  = 1 and C = (sup h) to complete the proof.

7

i=1 Zi]

E[maxN
log N + 0
N 1/↵(1  1/↵)
k )
↵N 1/↵/(↵  1)
log (↵) + 10

O( 1
k!(log N ) 1

supx hD(x)
1 as x ! 0
at most 2↵
k at x = 0
↵ at x = 0
 as x ! 1 log N +(↵1) log log N 

Distribution
Gumbel(µ = 1  = 1)
Frechet (↵> 1)
Weibull*( = 1  k  1)
Pareto*(xm = 1 ↵ )
Gamma(↵  1  )
Table 1: Distributions that give O(pT N log N ) regret FTPL algorithm. The parameterization fol-
lows Wikipedia pages for easy lookup. We denote the Euler constant (⇡ 0.58) by 0. Distributions
marked with (*) need to be slightly modiﬁed using the conditioning trick explained in Appendix B.2.
The maximum of Frechet hazard function has to be computed numerically (Elsayed  2012  p. 47)
but elementary calculations show that it is bounded by 2↵ (Appendix D).

O(pT N log N ) Param.
N/A
↵ = log N
k = 1 (Exponential)
↵ = log N
 = ↵ = 1 (Exponential)

Corollary 4.4. Follow the Perturbed Leader Algorithm with distributions in Table 1 (restricted to a
certain range of parameters)  combined with Geometric Resampling (Section 4.1) with M = pN T  
has an expected regret of order O(pT N log N ).
Table 1 provides the two terms we need to bound. We derive the third column of the table in
Appendix C using Extreme Value Theory (Embrechts et al.  1997). Note that our analysis in the
proof of Lemma 4.3 is quite tight; the only place we have an inequality is when we upper bound the
hazard rate. It is thus reasonable to pose the following conjecture:
Conjecture 4.5. If a distribution D has a monotonically increasing hazard rate hD(x) that does
not converge as x ! +1 (e.g.  Gaussian)  then there is a sequence of losses that will incur at least
a linear regret.

The intuition is that if adversary keeps incurring a high loss for the i-th arm  then with high prob-
ability ˜Gj⇤  Gi will be large. So  the expectation in Equation 9 will be dominated by the hazard
function evaluated at large values of ˜Gj⇤  Gi.
Acknowledgments.
1453304. A. Tewari acknowledges the support of NSF under CAREER grant IIS-1452099.

J. Abernethy acknowledges the support of NSF under CAREER grant IIS-

References
J. Abernethy  E. Hazan  and A. Rakhlin.

Interior-point methods for full-information and bandit

online learning. IEEE Transactions on Information Theory  58(7):4164–4175  2012.

J. Abernethy  C. Lee  A. Sinha  and A. Tewari. Online linear optimization via smoothing. In COLT 

pages 807–823  2014.

J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT 

pages 217–226  2009.

J.-Y. Audibert  S. Bubeck  and G. Lugosi. Minimax policies for combinatorial prediction games. In

COLT  2011.

P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Machine

Learning Research  3:397–422  2003.

P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. Gambling in a rigged casino: The adver-

sarial multi-arm bandit problem. In FOCS  1995.

P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2-3):235–256  2002.

P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal of Computuataion  32(1):48–77  2003. ISSN 0097-5397.

D. P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. Journal

of Optimization Theory and Applications  12(2):218–231  1973. ISSN 0022-3239.

8

N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University Press 

2006.

V. Dani and T. P. Hayes. Robbing the bandit: less regret in online geometric optimization against an

adaptive adversary. In SODA  pages 937–943  2006.

V. Dani  T. Hayes  and S. Kakade. The price of bandit information for online optimization. In NIPS 

2008.

L. Devroye  G. Lugosi  and G. Neu. Prediction by random-walk perturbation. In Conference on

Learning Theory  pages 460–473  2013.

E. Elsayed. Reliability Engineering. Wiley Series in Systems Engineering and Management.
ISBN 9781118309544. URL https://books.google.com/books?id=

Wiley  2012.
NdjF5G6tfLQC.

P. Embrechts  C. Kl¨uppelberg  and T. Mikosch. Modelling Extremal Events: For Insurance and
Finance. Applications of mathematics. Springer  1997. ISBN 9783540609315. URL https:
//books.google.com/books?id=BXOI2pICfJUC.

A. D. Flaxman  A. T. Kalai  and H. B. McMahan. Online convex optimization in the bandit setting:

gradient descent without a gradient. In SODA  pages 385–394  2005. ISBN 0-89871-585-7.

J. Gittins. Quantitative methods in the planning of pharmaceutical research. Drug Information

Journal  30(2):479–487  1996.

J. Gittins  K. Glazebrook  and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons 

2011.

J. Hannan. Approximation to bayes risk in repeated play. In M. Dresher  A. W. Tucker  and P. Wolfe 

editors  Contributions to the Theory of Games  volume III  pages 97–139  1957.

A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. Journal of Computer

and System Sciences  71(3):291–307  2005.

T. Koc´ak  G. Neu  M. Valko  and R. Munos. Efﬁcient learning by implicit exploration in bandit

problems with side observations. In NIPS  pages 613–621. Curran Associates  Inc.  2014.

J. Kujala and T. Elomaa. On following the perturbed leader in the bandit setting. In Algorithmic

Learning Theory  pages 371–385. Springer  2005.

T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied

Mathematics  6(1):4–22  1985.

N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation 

108(2):212–261  1994. ISSN 0890-5401.

H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an adap-

tive adversary. In COLT  pages 109–123  2004.

G. Neu and G. Bart´ok. An efﬁcient algorithm for learning with semi-bandit feedback. In Algorithmic

Learning Theory  pages 234–248. Springer  2013.

M. Pacula  J. Ansel  S. Amarasinghe  and U.-M. OReilly. Hyperparameter tuning in bandit-based
adaptive operator selection. In Applications of Evolutionary Computation  pages 73–82. Springer 
2012.

J.-P. Penot. Sub-hessians  super-hessians and conjugation. Nonlinear Analysis: Theory  Meth-
ods & Applications  23(6):689–702  1994. URL http://www.sciencedirect.com/
science/article/pii/0362546X94902127.

S. Rakhlin  O. Shamir  and K. Sridharan. Relax and randomize: From value to algorithms.

Advances in Neural Information Processing Systems  pages 2141–2149  2012.

In

H. Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc.  58(5):

527–535  1952.

C. Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Statistical Physics  52

(1-2):479–487  1988.

G. Van den Broeck  K. Driessens  and J. Ramon. Monte-carlo tree search in poker using expected

reward distributions. In Advances in Machine Learning  pages 367–381. Springer  2009.

T. Van Erven  W. Kotlowski  and M. K. Warmuth. Follow the leader with dropout perturbations. In

COLT  2014.

9

,Prem Gopalan
Chong Wang
David Blei
Jacob Abernethy
Chansoo Lee
Ambuj Tewari