2019,XLNet: Generalized Autoregressive Pretraining for Language Understanding,With the capability of modeling bidirectional contexts  denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling.
However  relying on corrupting the input with masks  BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy.
In light of these pros and cons  we propose XLNet  a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation.
Furthermore  XLNet integrates ideas from Transformer-XL  the state-of-the-art autoregressive model  into pretraining.
Empirically  under comparable experiment setting  XLNet outperforms BERT on 20 tasks  often by a large margin  including question answering  natural language inference  sentiment analysis  and document ranking.,XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1  Zihang Dai∗12  Yiming Yang1  Jaime Carbonell1 

{zhiliny dzihang yiming jgc rsalakhu}@cs.cmu.edu  qvl@google.com

Ruslan Salakhutdinov1  Quoc V. Le2

1Carnegie Mellon University  2Google AI Brain Team

Abstract

With the capability of modeling bidirectional contexts  denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However  relying on corrupt-
ing the input with masks  BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons  we
propose XLNet  a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore  XLNet integrates ideas
from Transformer-XL  the state-of-the-art autoregressive model  into pretraining.
Empirically  under comparable experiment setting  XLNet outperforms BERT on
20 tasks  often by a large margin  including question answering  natural language
inference  sentiment analysis  and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7  22  27  28  10]. Typically  these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora  and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea  different unsupervised pretraining objectives have been explored in
literature. Among them  autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7  27  28]. Speciﬁcally  given a text sequence x = (x1 ···   xT )  AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward)  it is not effective at modeling deep bidirectional contexts. On the
contrary  downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison  AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10]  which has been
the state-of-the-art pretraining approach. Given the input token sequence  a certain portion of tokens
are replaced by a special symbol [MASK]  and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective  BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

bidirectional contexts for reconstruction. As an immediate beneﬁt  this closes the aforementioned
bidirectional information gap in AR language modeling  leading to improved performance. However 
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time  resulting in a pretrain-ﬁnetune discrepancy. Moreover  since the predicted tokens are
masked in the input  BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words  BERT assumes the predicted tokens are independent of each
other given the unmasked tokens  which is oversimpliﬁed as high-order  long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives  in this work  we propose
XLNet  a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly  instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els  XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation  the context for each position can
consist of tokens from both left and right. In expectation  each position learns to utilize contextual
information from all positions  i.e.  capturing bidirectional context.
• Secondly  as a generalized AR language model  XLNet does not rely on data corruption. Hence 
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile 
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens  eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective  XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling  XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining  which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution  we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically  under comparable experiment setting  XLNet consistently outperforms BERT [10] on a
wide spectrum of problems including GLUE language understanding tasks  reading comprehension
tasks like SQuAD and RACE  text classiﬁcation tasks such as Yelp and IMDB  and the ClueWeb09-B
document ranking task.
Related Work The idea of permutation-based AR modeling has been explored in [32  12]  but there
are several key differences. Firstly  previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts. Technically  to construct a valid target-aware prediction
distribution  XLNet incorporates the target position into the hidden state via two-stream attention
while previous permutation-based AR models relied on implicit position awareness inherent to their
MLP architectures. Finally  for both orderless NADE and XLNet  we would like to emphasize that
“orderless” does not mean that the input sequence can be randomly permuted but that the model
allows for different factorization orders of the distribution.
Another related idea is to perform autoregressive denoising in the context of text generation [11] 
which only considers a ﬁxed order though.

2 Proposed Method

2.1 Background

In this section  we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1 ···   xT ]  AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

max

θ

log pθ(x) =

T(cid:88)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

 

(1)

T(cid:88)

log pθ(xt | x<t) =

log

t=1

t=1

2

where hθ(x1:t−1) is a context representation produced by neural models  such as RNNs or Transform-
ers  and e(x) denotes the embedding of x. In comparison  BERT is based on denoising auto-encoding.
Speciﬁcally  for a text sequence x  BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The
training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

max

θ

mt log pθ(xt | ˆx) =

mt log

t=1

t=1

T(cid:88)

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1)  
(cid:80)

(2)

where mt = 1 indicates xt is masked  and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1  Hθ(x)2 ···   Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2)  BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison  the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks  which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison  AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left)  while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result  the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

According to the comparison above  AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.
Borrowing ideas from orderless NADE [32]  we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally  for a sequence x of length T   there are T ! different orders to perform a valid
autoregressive factorization. Intuitively  if model parameters are shared across all factorization orders 
in expectation  the model will learn to gather information from all positions on both sides.
To formalize the idea  let ZT be the set of all possible permutations of the length-T index sequence
[1  2  . . .   T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then  our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially  for a text sequence x  we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training  in expectation  xt has seen every possible element xi (cid:54)= xt in
the sequence  hence being able to capture the bidirectional context. Moreover  as this objective ﬁts
into the AR framework  it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order  not the
sequence order. In other words  we keep the original sequence order  use the positional encodings
corresponding to the original sequence  and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary  since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture  we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in the Appendix A.7 with Figure 4.

3

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 1: (a): Content stream attention  which is the same as the standard self-attention. (b): Query
stream attention  which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.
While the permutation language modeling objective has desired properties  naive implementation with
standard Transformer parameterization may not work. To see the problem  assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation  i.e.  pθ(Xzt =
x | xz<t) =
  where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict  i.e.  the value of zt. Consequently  the
same distribution is predicted regardless of the target position  which is not able to learn useful
representations (see Appendix A.1 for a concrete example). To avoid this problem  we propose to
re-parameterize the next-token distribution to be target position aware:

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

exp(cid:0)e(x)(cid:62)gθ(xz<t   zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t   zt))

(cid:80)

pθ(Xzt = x | xz<t) =

 

(4)

where gθ(xz<t  zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction  how to formulate gθ(xz<t  zt) remains a non-trivial problem. Among other
possibilities  we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work  there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt  gθ(xz<t  zt) should only use the position zt and not the content xzt  otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t  gθ(xz<t   zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction  we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t)  or abbreviated as hzt  which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t  zt)  or abbreviated as gzt  which only has access to the contex-

tual information xz<t and the position zt  but not the content xzt  as discussed above.

Computationally  the ﬁrst layer query stream is initialized with a trainable vector  i.e. g(0)
i = w 
while the content stream is set to the corresponding word embedding  i.e. h(0)
i = e(xi). For each
self-attention layer m = 1  . . .   M  the two streams of representations are schematically2 updated

2To avoid clutter  we omit the implementation details including multi-head attention  residual connection 
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$( )g$( )h’( )g’( )h(( )g(( )h)( )g)( )h$($)g$($)AttentionQK  Vh$($)g$($)AttentionQK  V(b)(a)h$( )g$( )h’( )g’( )h(( )g(( )h)( )g)( )with a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

zt

  KV = h(m−1)
  KV = h(m−1)

z<t

z≤t

; θ) 

; θ) 

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q  K  V denote the query  key  and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention  so during ﬁnetuning  we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally 
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts  it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty  we choose to only predict the last
tokens in a factorization order. Formally  we split z into a non-target subsequence z≤c and a target
subsequence z>c  where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence  i.e. 

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

max

θ

Ez∼ZT

 |z|(cid:88)

t=c+1

= Ez∼ZT

log pθ(xzt | xz<t )

(5)

.

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e.  |z| /(|z| − c) ≈ K. For unselected tokens  their query representations need not
be computed  which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework  we incorporate the state-of-the-art AR
language model  Transformer-XL [9]  into our pretraining framework  and name our method after it.
We integrate two important techniques in Transformer-XL  namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier  which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality  suppose we have two segments taken from
a long sequence s; i.e.  ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then  based on the permutation ˜z  we process the ﬁrst segment  and then
cache the obtained content representations ˜h(m) for each layer m. Then  for the next segment x  the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

  KV =

(cid:104)˜h(m−1)  h(m−1)

(cid:105)

z≤t

; θ)

where [.  .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus  the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation  the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally  Figure 1 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.7 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments  e.g.  a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase  following BERT  we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally  the input to our model is the same as BERT: [CLS  A  SEP  B  SEP] 
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although

5

we follow the two-segment data format  XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).
Relative Segment Encodings Architecturally  different from BERT that adds an absolute segment
embedding to the word embedding at each position  we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence  if
i and j are from the same segment  we use a segment encoding sij = s+ or otherwise sij = s− 
where s+ and s− are learnable model parameters for each attention head. In other words  we only
consider whether the two positions are within the same segment  as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e.  only
modeling the relationships between positions. When i attends to j  the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij  where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally  the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First  the
inductive bias of relative encodings improves generalization [9]. Second  it opens the possibility of
ﬁnetuning on tasks that have more than two input segments  which is not possible using absolute
segment encodings.

2.6 Discussion

Comparing Eq. (2) and (5)  we observe that both BERT and XLNet perform partial prediction  i.e. 
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked  it is impossible to make any meaningful predictions. In addition  for both BERT
and XLNet  partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However  the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference  let’s consider a concrete example [New  York  is  a  city]. Suppose
both BERT and XLNet select the two tokens [New  York] as the prediction targets and maximize
log p(New York | is a city). Also suppose that XLNet samples the factorization order [is  a  city 
New  York]. In this case  BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city) 

JXLNet = log p(New | is a city) + log p(York | New  is a city).

Notice that XLNet is able to capture the dependency between the pair (New  York)  which is omitted
by BERT. Although in this example  BERT learns some dependency pairs such as (New  city) and
(York  city)  it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
For more formal analysis and further discussion  please refer to Appendix A.5.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10]  we use the BooksCorpus [40] and English Wikipedia as part of our pretraining
data  which have 13GB plain text combined. In addition  we include Giga5 (16GB text) [26] 
ClueWeb 2012-B (extended from [5])  and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl 
which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17]  we
obtain 2.78B  1.09B  4.75B  4.30B  and 19.97B subword pieces for Wikipedia  BooksCorpus  Giga5 
ClueWeb  and Common Crawl respectively  which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large  which
results in a similar model size. During pretraining  we always use a full sequence length of 512.
Firstly  to provide a fair comparison with BERT (section 3.2)  we also trained XLNet-Large-wikibooks
on BooksCorpus and Wikipedia only  where we reuse all pretraining hyper-parameters as in the
original BERT. Then  we scale up the training of XLNet-Large by using all the datasets described
above. Speciﬁcally  we train on 512 TPU v3 chips for 500K steps with an Adam weight decay
optimizer  linear learning rate decay  and a batch size of 8192  which takes about 5.5 days. It was

6

observed that the model still underﬁts the data at the end of training. Finally  we perform ablation
study (section 3.4) based on the XLNet-Base-wikibooks.
Since the recurrence mechanism is introduced  we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large  we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed3. We employ an idea of span-based prediction  where we ﬁrst sample a
length L ∈ [1 ···   5]  and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.
We use a variety of natural language understanding datasets to evaluate the performance of our
method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.

3.2 Fair Comparison with BERT

82.8/85.5

75.1

SQuAD1.1 SQuAD2.0 RACE MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B
86.7/92.8
90.2

Model
BERT-Large
(Best of 3)
XLNet-Large-
wikibooks
Table 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in
BERT. We use the best of 3 BERT variants for comparison; i.e.  the original BERT  BERT with whole word
masking  and BERT without next sentence prediction.

91.8 81.2

85.1/87.8

88.2/94.0

87.3

93.0

91.4 74.0

94.0

88.4

93.9

65.2

91.1

94.4

90.0

77.4

88.7

63.7

Here  we ﬁrst compare the performance of BERT and XLNet in a fair setting to decouple the effects
of using more data and the improvement from BERT to XLNet. In Table 1  we compare (1) best
performance of three different variants of BERT and (2) XLNet trained with the same data and
hyperparameters. As we can see  trained on the same data with an almost identical training recipe 
XLNet outperforms BERT by a sizable margin on all the considered datasets.

3.3 Results After Scaling Up

RACE
GPT [28]
BERT [25]
BERT+DCMN∗ [38]
RoBERTa [21]
XLNet

Accuracy Middle High Model

NDCG@20 ERR@20

59.0
72.0
74.1
83.2
85.4

62.9
76.6
79.5
86.5
88.6

57.4
70.1
71.8
81.8
84.0

DRMM [13]
KNRM [8]
Conv [8]
BERT†
XLNet

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 2: Comparison with state-of-the-art results on the test set of RACE  a reading comprehension task  and on
ClueWeb09-B  a document ranking task. ∗ indicates using ensembles. † indicates our implementations. “Middle”
and “High” in RACE are two subsets representing middle and high school difﬁculty levels. All BERT  RoBERTa 
and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

After the initial publication of our manuscript  a few other pretrained models were released such as
RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from
1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs  we
exclude ALBERT from the following results as it is hard to lead to scientiﬁc conclusions. To obtain
relatively fair comparison with RoBERTa  the experiment in this section is based on full data and
reuses the hyper-parameters of RoBERTa  as described in section 3.1.
The results are presented in Tables 2 (reading comprehension & document ranking)  3 (question
answering)  4 (text classiﬁcation) and 5 (natural language understanding)  where XLNet generally
outperforms BERT and RoBERTa. In addition  we make two more interesting observations:

3Hyperparameters for pretraining and ﬁnetuning are in Appendix A.4.

7

EM F1

F1

EM

SQuAD1.1

SQuAD2.0
Dev set results (single model)
BERT [10]
RoBERTa [21]
XLNet
Test set results on leaderboard (single model  as of Dec 14  2019)
BERT∗ [10]
RoBERTa [21]
XLNet

BERT† [10]
RoBERTa [21]
XLNet

83.061
89.795
90.689

80.005
86.820
87.926

78.98
86.5
87.9

81.77
89.4
90.6

84.1
88.9
89.7

90.9
94.6
95.1

Table 3: Results on SQuAD  a reading comprehension dataset. † marks our runs with the ofﬁcial code. We are
not able to obtain the test results on SQuAD1.1 from the organizers after submitting our result for more than one
month.

Model
CNN [15]
DPCNN [15]
Mixed VAT [31  23]
ULMFiT [14]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.20

2.90
2.64

-

2.16
1.89
1.37

32.39
30.58

-

29.98
29.32
27.05

0.84
0.88
0.70
0.80
0.64
0.60

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

34.17
31.67

-

4.45

2.63
2.11

Table 4: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

MNLI

86.6/-

92.3
94.7
94.9

90.2/90.2
90.8/90.8

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
RoBERTa [21]
XLNet
Multi-task ensembles on test (from leaderboard as of Oct 28  2019)
MT-DNN∗ [20]
RoBERTa∗ [21]
XLNet∗
Table 5: Results on GLUE. ∗ indicates using ensembles  and † denotes single-task results in a multi-task row.
All dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower
section shows comparison with state-of-the-art results on the public leaderboard.

87.9/87.4
90.8/90.2
90.9/90.9†

96.5
96.7
97.1†

89.9
90.2
90.4†

96.0
98.9
99.0†

86.3
88.2
88.5

89.0
89.0
92.5

92.7
92.3
92.9

91.1
92.2
93.0

68.4
67.8
70.2

88.0
90.9
90.8

70.4
86.6
85.9

93.2
96.4
97.0

60.6
68.0
69.0

90.0
92.4
92.5

91.3
92.2
92.3

-
-
-

• For explicit reasoning tasks like SQuAD and RACE that involve longer context  the performance
gain of XLNet is usually larger. This superiority at dealing with longer context could come from
the Transformer-XL backbone in XLNet.
• For classiﬁcation tasks that already have abundant supervised examples such as MNLI (>390K) 

Yelp (>560K) and Amazon (>3M)  XLNet still lead to substantial gains.

3.4 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally  there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective alone  especially compared to
• The importance of using Transformer-XL as the backbone neural architecture.
• The necessity of some implementation details including span-based prediction  the bidirectional

the denoising auto-encoding objective used by BERT.

input pipeline  and next-sentence prediction.

With these purposes in mind  in Table 6  we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8)  the original BERT-Base model (row 1)  and an additional Transformer-XL

8

baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison  all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: The results of BERT on RACE are taken from [38]. We run BERT on the other datasets using the
ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter to control
the optimization difﬁculty (see Section 2.3).

Examining rows 1 - 4 of Table 6  we can see both Transformer-XL and the permutation LM clearly
contribute the superior performance of XLNet over BERT. Moreover  if we remove the memory
caching mechanism (row 5)  the performance clearly drops  especially for RACE which involves the
longest context among the 4 tasks. In addition  rows 6 - 7 show that both span-based prediction and
the bidirectional input pipeline play important roles in XLNet. Finally  we unexpectedly ﬁnd the the
next-sentence prediction objective proposed in the original BERT does not necessarily lead to an
improvement in our setting. Hence  we exclude the next-sentence prediction objective from XLNet.
Finally  we also perform a qualitative study of the attention patterns  which is included in Appendix
A.6 due to page limit.

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective  including integrating Transformer-XL and the careful design
of the two-stream attention mechanism. XLNet achieves substantial improvement over previous
pretraining objectives on various tasks.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project  Jamie Callan for providing the ClueWeb dataset  Youlong Cheng  Yanping Huang and Shibo
Wang for providing ideas to improve our TPU implementation  Chenyan Xiong and Zhuyun Dai
for clarifying the setting of the document ranking task. ZY and RS were supported by the Ofﬁce of
Naval Research grant N000141812861  the National Science Foundation (NSF) grant IIS1763562 
the Nvidia fellowship  and the Siebel scholarship. ZD and YY were supported in part by NSF under
the grant IIS-1546329 and by the DOE-Ofﬁce of Science under the grant ASCR #KJ040201.

References
[1] Rami Al-Rfou  Dokook Choe  Noah Constant  Mandy Guo  and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444  2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review  2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853  2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems  pages 400–406  2000.

9

[5] Jamie Callan  Mark Hoy  Changkuk Yoo  and Le Zhao. Clueweb09 data set  2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org  2019.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems  pages 3079–3087  2015.

[8] Zhuyun Dai  Chenyan Xiong  Jamie Callan  and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining  pages 126–134. ACM  2018.

[9] Zihang Dai  Zhilin Yang  Yiming Yang  William W Cohen  Jaime Carbonell  Quoc V Le 
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860  2019.

[10] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 
2018.

[11] William Fedus  Ian Goodfellow  and Andrew M Dai. Maskgan: better text generation via ﬁlling

in the_. arXiv preprint arXiv:1801.07736  2018.

[12] Mathieu Germain  Karol Gregor  Iain Murray  and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning  pages 881–889 
2015.

[13] Jiafeng Guo  Yixing Fan  Qingyao Ai  and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management  pages 55–64. ACM  2016.

[14] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146  2018.

[15] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)  pages 562–570  2017.

[16] Vid Kocijan  Ana-Maria Cretu  Oana-Maria Camburu  Yordan Yordanov  and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290  2019.

[17] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226  2018.
[18] Guokun Lai  Qizhe Xie  Hanxiao Liu  Yiming Yang  and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683  2017.

[19] Zhenzhong Lan  Mingda Chen  Sebastian Goodman  Kevin Gimpel  Piyush Sharma  and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv
preprint arXiv:1909.11942  2019.

[20] Xiaodong Liu  Pengcheng He  Weizhu Chen  and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504  2019.

[21] Yinhan Liu  Myle Ott  Naman Goyal  Jingfei Du  Mandar Joshi  Danqi Chen  Omer Levy  Mike
Lewis  Luke Zettlemoyer  and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692  2019.

[22] Bryan McCann  James Bradbury  Caiming Xiong  and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems  pages
6294–6305  2017.

[23] Takeru Miyato  Andrew M Dai  and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725  2016.

[24] Aaron van den Oord  Nal Kalchbrenner  and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759  2016.

[25] Xiaoman Pan  Kai Sun  Dian Yu  Heng Ji  and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993  2019.

10

[26] Robert Parker  David Graff  Junbo Kong  Ke Chen  and Kazuaki Maeda. English gigaword
ﬁfth edition  linguistic data consortium. Technical report  Technical Report. Linguistic Data
Consortium  Philadelphia  Tech. Rep.  2011.

[27] Matthew E Peters  Mark Neumann  Mohit Iyyer  Matt Gardner  Christopher Clark  Ken-
ton Lee  and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365  2018.

[28] Alec Radford  Karthik Narasimhan  Tim Salimans  and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf  2018.

[29] Pranav Rajpurkar  Robin Jia  and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822  2018.

[30] Pranav Rajpurkar  Jian Zhang  Konstantin Lopyrev  and Percy Liang. Squad: 100 000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250  2016.

[31] Devendra Singh Sachan  Manzil Zaheer  and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[32] Benigno Uria  Marc-Alexandre Côté  Karol Gregor  Iain Murray  and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research  17(1):7184–
7220  2016.

[33] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez 
Łukasz Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems  pages 5998–6008  2017.

[34] Alex Wang  Amanpreet Singh  Julian Michael  Felix Hill  Omer Levy  and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie  Zihang Dai  Eduard Hovy  Minh-Thang Luong  and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848  2019.

[36] Chenyan Xiong  Zhuyun Dai  Jamie Callan  Zhiyuan Liu  and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval  pages 55–64. ACM  2017.

[37] Zhilin Yang  Zihang Dai  Ruslan Salakhutdinov  and William W Cohen. Breaking the softmax

bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953  2017.

[38] Shuailiang Zhang  Hai Zhao  Yuwei Wu  Zhuosheng Zhang  Xi Zhou  and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381 
2019.

[39] Xiang Zhang  Junbo Zhao  and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems  pages 649–657  2015.

[40] Yukun Zhu  Ryan Kiros  Rich Zemel  Ruslan Salakhutdinov  Raquel Urtasun  Antonio Torralba 
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision  pages 19–27  2015.

11

,Maithra Raghu
Justin Gilmer
Jason Yosinski
Jascha Sohl-Dickstein
Zhilin Yang
Zihang Dai
Yiming Yang
Jaime Carbonell
Russ Salakhutdinov
Quoc Le