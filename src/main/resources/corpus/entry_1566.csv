2011,Distributed Delayed Stochastic Optimization,We analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information. The  main application of our results is to the development of distributed  minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel  which may give rise to delays due to  asynchrony. Our main contribution is to show that for smooth stochastic problems  the delays are asymptotically negligible. In  application to distributed optimization  we show $n$-node architectures whose optimization error in stochastic problems---in spite of asynchronous delays---scales asymptotically as $\order(1 / \sqrt{nT})$  which is known to be optimal even in the absence of delays.,Distributed Delayed Stochastic Optimization

Alekh Agarwal

John C. Duchi

Department of Electrical Engineering and Computer Sciences

University of California  Berkeley

Berkeley  CA 94720

{alekh jduchi}@eecs.berkeley.edu

Abstract

We analyze the convergence of gradient-based optimization algorithms
whose updates depend on delayed stochastic gradient information. The
main application of our results is to the development of distributed mini-
mization algorithms where a master node performs parameter updates while
worker nodes compute stochastic gradients based on local information in
parallel  which may give rise to delays due to asynchrony. Our main contri-
bution is to show that for smooth stochastic problems  the delays are asymp-
totically negligible.
In application to distributed optimization  we show
n-node architectures whose optimization error in stochastic problems—in

spite of asynchronous delays—scales asymptotically as O(1/√nT )  which

is known to be optimal even in the absence of delays.

1

Introduction

We focus on stochastic convex optimization problems of the form

minimize

f (x)

for

x∈X

f (x) := EP [F (x; ξ)] =ZΞ

F (x; ξ)dP (ξ) 

(1)

where X ⊆ Rd is a closed convex set  P is a probability distribution over Ξ  and F (· ; ξ) is
convex for all ξ ∈ Ξ  so that f is convex. Classical stochastic gradient algorithms [18  16]
iteratively update a parameter x(t) ∈ X by sampling ξ ∼ P   computing g(t) = ∇F (x(t); ξ) 
and performing the update x(t + 1) = ΠX (x(t) − α(t)g(t))  where ΠX denotes projection
onto the set X and α(t) ∈ R is a stepsize. In this paper  we analyze asynchronous gradient
methods  where instead of receiving current information g(t)  the procedure receives out
of date gradients g(t − τ (t)) = ∇F (x(t − τ (t))  ξ)  where τ (t) is the (potentially random)
delay at time t. The central contribution of this paper is to develop algorithms that—under
natural assumptions about the functions F in the objective (1)—achieve asymptotically
optimal convergence rates for stochastic convex optimization in spite of delays.
Our model of delayed gradient information is particularly relevant in distributed optimiza-
tion scenarios  where a master maintains the parameters x while workers compute stochastic
gradients of the objective (1) using a local subset of the data. Master-worker architectures
are natural for distributed computation  and other researchers have considered models sim-
ilar to those in this paper [12  10]. By allowing delayed and asynchronous updates  we can
avoid synchronization issues that commonly handicap distributed systems.
Distributed optimization has been studied for several decades  tracing back at least to
seminal work of Bertsekas and Tsitsiklis ([3  19  4]) on asynchronous computation and
minimization of smooth functions where the parameter vector is distributed. More recent
work has studied problems in which each processor or node i in a network has a local
function fi  and the goal is to minimize the sum f (x) = 1
i=1 fi(x) [12  13  17  7]. Our
work is closest to Nedi´c et al.’s asynchronous incremental subgradient method [12]  who

nPn

1

Master

g1(t − n)
x(t)

x(t + 1)

g2(t − n + 1)

gn(t − 1)

x(t + n − 1)

1

2

3

n

Figure 1: Cyclic delayed update architecture. Workers compute gradients in parallel  passing out-
of-date (stochastic) gradients gi(t − τ ) = ∇fi(x(t − τ )) to master. Master responds with current
parameters. Diagram shows parameters and gradients communicated between rounds t and t+n−1.

analyze gradient projection steps taken using out-of-date gradients. See Figure 1 for an
illustration. Nedi´c et al. prove that the procedure converges  and a slight extension of their
results shows that the optimization error of the procedure after T iterations is at most

O(pτ /T )  τ being the delay in gradients. Without delay  a centralized stochastic gradient
algorithm attains convergence rate O(1/√T ). All the approaches mentioned above give

slower convergence than this centralized rate in distributed settings  paying a penalty for
data being split across a network; as Dekel et al. [5] note  one would expect that parallel
computation actually speeds convergence. Langford et al. [10] also study asynchronous
methods in the setup of stochastic optimization and attempt to remove the penalty for the
delayed procedure under an additional smoothness assumption; however  their paper has a
technical error (see the long version [2] for details). The main contributions of our paper
are (1) to remove the delay penalty for smooth functions and (2) to demonstrate beneﬁts
in convergence rate by leveraging parallel computation even in spite of delays.
We build on results of Dekel et al. [5]  who give reductions of stochastic optimization al-
gorithms (e.g. [8  9]) to show that for smooth objectives f   when n processors compute
stochastic gradients in parallel using a common parameter x it is possible to achieve conver-

gence rate O(1/√T n). The rate holds so long as most processors remain synchronized for

most of the time [6]. We show similar results  but we analyze the eﬀects of asynchronous gra-
dient updates where all the nodes in the network can suﬀer delays  quantifying the impact of
the delays. In application to distributed optimization  we show that under diﬀerent network

assumptions  we achieve convergence rates ranging from O(min{n3/T  (n/T )2/3} + 1/√T n)
to O(min{n/T  1/T 2/3} + 1/√T n)  which is O(1/√nT ) asymptotically in T . The time nec-
essary to achieve ǫ-optimal solution to the problem (1) is asymptotically O(1/nǫ2)  a factor
of n–the size of the network –better than a centralized procedure in spite of delay. Proofs of
our results can be found in the long version of this paper [2].

is de-
:= supx:kxk≤1 hz  xi. The subdiﬀerential set of a function f at a point x is

Notation We denote a general norm by k·k  and its associated dual norm k·k∗
ﬁned as kzk∗
∂f (x) :=(cid:8)g ∈ Rd | f (y) ≥ f (x) + hg  y − xi for all y ∈ dom f(cid:9) . A function f is G-Lipschitz
w.r.t. the norm k·k on X if ∀x  y ∈ X   |f (x)− f (y)| ≤ Gkx − yk  and f is L-smooth on X if
L
2 kx − yk2 .
k∇f (x) − ∇f (y)k∗ ≤ Lkx − yk   equivalently  f (y) ≤ f (x)+h∇f (x)  y − xi+
A convex function h is c-strongly convex with respect to a norm k·k over X if

h(y) ≥ h(x) + hg  y − xi +

2 Setup and Algorithms

c
2 kx − yk2

for all x  y ∈ X and g ∈ ∂h(x).

(2)

To build intuition for the algorithms we analyze  we ﬁrst describe the delay-free algorithm
underlying our approach: the dual averaging algorithm of Nesterov [15].1 The dual averaging
algorithm is based on a strongly convex proximal function ψ(x); we assume without loss
that ψ(x) ≥ 0 for all x ∈ X and (by scaling) that ψ is 1-strongly convex.

1Essentially identical results to those we present here also hold for extensions of mirror de-

scent [14]  but we omit these for lack of space.

2

At time t  the algorithm updates a dual vector z(t) and primal vector x(t) ∈ X using a
subgradient g(t) ∈ ∂F (x(t); ξ(t))  where ξ(t) is drawn i.i.d. according to P :
1

z(t + 1) = z(t) + g(t)

and x(t + 1) = argmin

(3)

x∈X nhz(t + 1)  xi +

α(t + 1)

ψ(x)o.

For the remainder of the paper  we will use the following three essentially standard assump-
tions [8  9  20] about the stochastic optimization problem (1).
Assumption I (Lipschitz Functions). For P -a.e. ξ  the function F (· ; ξ) is convex. More-
over  for any x ∈ X   and v ∈ ∂F (x; ξ)  E[kvk2
∗
Assumption II (Smooth Functions). The expected function f has L-Lipschitz continuous
gradient  and for all x ∈ X the variance bound E[k∇f (x) − ∇F (x; ξ)k2
∗
Assumption III (Compactness). For all x ∈ X   ψ(x) ≤ R2/2.
Several commonly used functions satisfy the above assumptions  for example:

] ≤ σ2 holds.

] ≤ G2.

(i) The logistic loss: F (x; ξ) = log[1+exp(hx  ξi)]. The objective F satisﬁes Assumptions I
(ii) Least squares: F (x; ξ) = (a − hx  bi)2 where ξ = (a  b) for a ∈ Rd and b ∈ R  satisﬁes

and II so long as kξk∗

has ﬁnite second moment.

has ﬁnite fourth moment.

Assumptions I and II if X is compact and kξk∗

+

T

σR

√T(cid:19) .

Under Assumption III  assumptions I and II imply ﬁnite-sample convergence rates for the
t=1 x(t + 1). Under Assumption I 
R/(G√t) [15  20]. The result is sharp to constant factors [14  1]  but can be further improved
using Assumption II. Building on work of Juditsky et al. [8] and Lan [9]  Dekel et al. [5 
Appendix A] show that the stepsize choice α(t)−1 = L + σR√t  yields the convergence rate

T PT
update (3). Deﬁne the time averaged vectorbx(T ) := 1
dual averaging satisﬁes E[f (bx(T ))] − f (x∗) = O(RG/√T ) for the stepsize choice α(t) =
E[f (bx(T ))] − f (x∗) = O(cid:18) LR2

Delayed Optimization Algorithms We now turn to extending the dual averaging (3)
update to the setting in which instead of receiving a current gradient g(t) at time t  the
procedure receives a gradient g(t − τ (t))  that is  a stochastic gradient of the objective (1)
computed at the point x(t − τ (t)). Our analysis admits any sequence τ (t) of delays as long
as the mapping t 7→ t − τ (t) is one-to-one  and satisﬁes E[τ (t)2] ≤ B2 < ∞.
We consider the dual averaging algorithm with g(t) replaced by g(t − τ (t)):
1
z(t + 1) = z(t) + g(t − τ (t)) and x(t + 1) = argmin
By combining the techniques Nedi´c et al. [12] developed with the convergence proofs of dual
averaging [15]  it is possible to show that so long as E[τ (t)] ≤ B < ∞ for all t  Assumptions I
and III and the stepsize choice α(t) = R
the next section we show how to avoid the √B penalty.

ψ(x)o. (5)
x∈X nhz(t + 1)  xi +
give E[f (bx(T ))] − f (x∗) = O(RG√B/√T ). In

α(t + 1)

G√Bt

(4)

3 Convergence rates for delayed optimization of smooth functions

We now state and discuss the implications of two results for asynchronous stochastic gradient
methods. Our ﬁrst convergence result is for the update rule (5)  while the second averages
several stochastic subgradients for every update  each with a potentially diﬀerent delay.

3.1 Simple delayed optimization

Our focus in this section is to remove the √B penalty for the delayed update rule (5) using
Assumption II  which arises for non-smooth optimization because subgradients can vary
drastically even when measured at near points. We show that under the smoothness condi-
tion  the errors from delay become second order: the penalty is asymptotically negligible.

3

Ef (bx(T )) − T f (x∗) ≤

Theorem 1. Let x(t) be deﬁned by the update (5). Deﬁne α(t)−1 = L + η(t)  where

η(t) = η√t or η(t) ≡ η√T for all t. The average bx(T ) =PT

LR2 + 6τ GR

+

+

2ηR2
√T

σ2
η√T

+ 4

T

t=1 x(t + 1)/T satisﬁes
LG2(τ + 1)2 log T

η2T

.

We make a few remarks about the theorem. The log T factor on the last term is not present
when using the ﬁxed stepsize of η√T . Furthermore  though we omit it here for lack of space 
the analysis also extends to random delays as long as E[τ (t)2] ≤ B2; see the long version [2]
for details. Finally  based on Assumption II  we can set η = σ/R  which makes the rate
asymptotically O(σR/√T )  which is the same as the delay-free case so long as τ = o(T 1/4).

The take-home message from Theorem 1 is thus that the penalty in convergence rate due
to the delay τ (t) is asymptotically negligible. In the next section  we show the implications
of this result for robust distributed stochastic optimization algorithms.

3.2 Combinations of delays

In some scenarios—including distributed settings similar to those we discuss in the next
section—the procedure has access not to only a single delayed gradient but to several stochas-
tic gradients with diﬀerent delays. To abstract away the essential parts of this situation  we
assume that the procedure receives n stochastic gradients g1  . . .   gn ∈ Rd  where each has
a potentially diﬀerent delay τ (i). Let λ = (λi)n
i=1 be (an unspeciﬁed) vector in probability
simplex. Then the procedure performs the following updates at time t:

nXi=1

z(t + 1) = z(t) +

The next theorem builds on the proof of Theorem 1.

λigi(t− τ (i))  x(t + 1) = argmin

ψ(x)(cid:9). (6)
Theorem 2. Under Assumptions I–III  let α(t) = (L + η(t))−1 and η(t) = η√t or η(t) ≡
η√T for all t. The average bx(T ) =PT
2LR2 + 4Pn

x∈X (cid:8)hz(t + 1)  xi +

t=1 x(t + 1)/T for the update sequence (6) satisﬁes

i=1 λiLG2(τ (i) + 1)2 log T

i=1 λiτ (i)GR
T

+ 6Pn

α(t + 1)

η2T

1

f (bx(T )) − f (x∗) ≤

+

4ηR2
√T

+

1
η√T

nXi=1

E(cid:13)(cid:13)

λi[∇f (x(t − τ (i))) − gi(t − τ (i))](cid:13)(cid:13)2

∗

.

We illustrate the consequences of Theorem 2 for distributed optimization in the next section.

4 Distributed Optimization

We now turn to what we see as the main purpose and application of the above results:
developing robust and eﬃcient algorithms for distributed stochastic optimization. Our main
motivations here are machine learning applications where the data is so large that it cannot
ﬁt on a single computer. Examples of the form (1) include logistic or linear regression  as
described respectively in Sec. 2(i) and (ii). We consider both stochastic and online/streaming
scenarios for such problems. In the simplest setting  the distribution P in the objective (1)
is the empirical distribution over an observed dataset  that is  f (x) = 1
i=1 F (x; ξi). We
divide the N samples among n workers so that each worker has an N/n-sized subset of data.
In online learning applications  the distribution P is the unknown distribution generating
the data  and each worker receives a stream of independent data points ξ ∼ P . Worker i
uses its subset of the data  or its stream  to compute gi ∈ Rd  an estimate of the gradient
∇f of the global f . We assume that gi is an unbiased estimate of ∇f (x)  which is satisﬁed 
for example  in the online setting or when each worker computes the gradient gi based on
samples picked at random without replacement from its subset of the data.
The architectural assumptions we make are based oﬀ of master/worker topologies  but the
convergence results in Section 3 allow us to give procedures robust to delay and asynchrony.
The architectures build on the na¨ıve scheme of having each worker simultaneously com-
pute a stochastic gradient and send it to the master  which takes a gradient step on the

NPN

4

x(t)

x(t − 1)

x(t − 2)

M

M

g(t − 1)

g(t − 2)

x(t − 3)

x(t − 4)

g(t − 3)

g(t − 4)

(a)

(b)

Figure 2: Master-worker averaging network. (a): parameters stored at diﬀerent nodes at time t.
A node at distance d from master has the parameter x(t − d). (b): gradients computed at diﬀerent
nodes. A node at distance d from master computes gradient g(t − d).

1

3 g1(t − d)

+ 1

3 g2(t − d − 2) + 1

3 g3(t − d − 2)

Depth d

1

{x(t − d)  g2(t − d − 2)  g3(t − d − 2)}

g2(t − d − 1)

g3(t − d − 1)

Depth d + 1

2

{x(t − d − 1)}

3

{x(t − d − 1)}

Figure 3: Communication of gra-
dient information toward master
node at time t from node 1 at dis-
tance d from master. Information
stored at time t by node i in brack-
ets to right of node.

averaged gradient. While the n gradients are computed in parallel in the na¨ıve scheme 
accumulating and averaging n gradients at the master takes Ω(n) time  oﬀsetting the gains
of parallelization  and the procedure is non-robust to laggard workers.

Cyclic Delayed Architecture This protocol is the delayed update algorithm mentioned
in the introduction  and it computes n stochastic gradients of f (x) in parallel. Formally 
worker i has parameter x(t−τ ) and computes gi(t−τ ) = ∇F (x(t−τ ); ξi(t)) ∈ Rd  where ξi(t)
is a random variable sampled at worker i from the distribution P . The master maintains
a parameter vector x ∈ X . At time t  the master receives gi(t − τ ) from some worker i 
computes x(t + 1) and passes it back to worker i only. Other workers do not see x(t + 1)
and continue their gradient computations on stale parameter vectors. In the simplest case 
each node suﬀers a delay of τ = n  though our analysis applies to random delays as well.
Recall Fig. 1 for a description of the process.

Locally Averaged Delayed Architecture At a high level  the protocol we now describe
combines the delayed updates of the cyclic delayed architecture with averaging techniques
of previous work [13  7]. We assume a network G = (V E)  where V is a set of n nodes
(workers) and E are the edges between the nodes. We select one of the nodes as the master 
which maintains the parameter vector x(t) ∈ X over time.
The algorithm works via a series of multicasting and aggregation steps on a spanning tree
rooted at the master node. In the broadcast phase  the master sends its current parameter
vector x(t) to its immediate neighbors. Simultaneously  every other node broadcasts its
current parameter vector (which  for a depth d node  is x(t − d)) to its children in the
spanning tree. See Fig. 2(a). Every worker computes its local gradient at its new parameter
(see Fig. 2(b)). The communication then proceeds from leaves toward the root. The leaf
nodes communicate their gradients to their parents  and the parent takes the gradients of
the leaf nodes from the previous round (received at iteration t − 1) and averages them with
its own gradient  passing this averaged gradient back up the tree. Again simultaneously 
each node takes the averaged gradient vectors of its children from the previous rounds 
averages them with its current gradient vector  and passes the result up the spanning tree.
See Fig. 3. The master node receives an average of delayed gradients from the entire tree 
giving rise to updates of the form (6). We note that this is similar to the MPI all-reduce
operation  except our implementation is non-blocking since we average delayed gradients
with diﬀerent delays at diﬀerent nodes.

5

4.1 Convergence rates for delayed distributed minimization

1

τ 2m

T 2/3  

2 kxk2

mPm

T (cid:27) +

We turn now to corollaries of the results from the previous sections that show even asyn-
chronous distributed procedures achieve asymptotically faster rates (over centralized proce-
dures). The key is that workers can pipeline updates by computing asynchronously and in
parallel  so each worker can compute a low variance estimate of the gradient ∇f (x). We
ignore the constants L  G  R  and σ  which are not dependent on the characteristics of
the network. We also assume that each worker i uses m independent samples ξi(j) ∼ P  
j = 1  . . .   m  to compute the stochastic gradient as gi(t) = 1
j=1 ∇F (x(t); ξi(j)). Using
the cyclic protocol as in Fig. 1  Theorem 1 gives the following result.
Corollary 1. Let ψ(x) = 1
2  assume the conditions in Theorem 1  and assume that
each worker uses m samples ξ ∼ P to compute the gradient it communicates to the master.

E[f (bx(T ))] − f (x∗) = O(cid:18) min(cid:26) τ 2/3

Then with the choice η(t) = max{τ 2/3T −1/3 pT /m} the update (5) satisﬁes
√T m(cid:19).
2] = E[k∇f (x) − ∇F (x; ξ)k2
Proof Noting that σ2 = E[k∇f (x) − gi(t)k2
when workers use m independent stochastic gradient samples  the corollary is immediate.
As in Theorem 1  the corollary generalizes to random delays as long as Eτ 2(t) ≤ B2 < ∞ 
with τ replaced by B in the result. So long as B = o(T 1/4)  the ﬁrst term in the bound is
asymptotically negligible  and we achieve a convergence rate of O(1/√T n) when m = O(n).
The cyclic delayed architecture has the drawback that information from a worker can take
τ = O(n) time to reach the master. While the algorithm is robust to delay  the downside
of the architecture is that the essentially τ 2m or τ 2/3 term in the bounds above can be
quite large. To address the large n drawback  we turn our attention to the locally averaged
architecture described by Figs. 2 and 3  where delays can be smaller since they depend
only on the height of a spanning tree in the network. As a result of the communication
procedure  the master receives a convex combination of the stochastic gradients evaluated at
i=1 λigi(t −
τ (i)) for some λ in the simplex  where τ (i) is the delay of worker i  which puts us in
the setting of Theorem 2. We now make the reasonable assumption that the gradient
errors ∇f (x(t)) − gi(t) are uncorrelated across the nodes in the network.2
In statistical
applications  for example  each worker may own independent data or receive streaming data
from independent sources. We also set ψ(x) = 1

each worker i. Speciﬁcally  the master receives gradients of the form gλ(t) =Pn

2]/m = O(1/m)

2  and observe

This gives the following corollary to Theorem 2.
Corollary 2. Set λi = 1
denote the average of the delays τ (i) and τ (i)2. Under the conditions of Theorem 2 

n for all i  ψ(x) = 1

Ek∇f (x(t − τ (i))) − gi(t − τ (i))k2
2 .
2  and η(t) = σ√T /R√n. Let ¯τ and τ 2

=

2 kxk2
nXi=1
λ2
i
2 kxk2

nXi=1

E(cid:13)(cid:13)(cid:13)(cid:13)

2

2

λi∇f (x(t − τ (i))) − gi(t − τ (i))(cid:13)(cid:13)(cid:13)(cid:13)
E [f (bx(T )) − f (x∗)] = O  LR2

T

+

¯τ GR

T

+

LG2R2nτ 2

σ2T

+

Rσ

√T n! .

In this architecture  the delay τ is
bounded by the graph diameter D. Furthermore  we can use a slightly diﬀerent stepsize set-

Asymptotically  E[f (bx(T ))] − f (x∗) = O(1/√T n).
ting as in Corollary 1 to get an improved rate of O(min{(D/T )2/3  nD2/T} + 1/√T n). It is

also possible—but outside the scope of this extended abstract—to give fast(er) convergence
rates dependent on communication costs (details can be found in the long version [2]).

4.2 Running-time comparisons

We now explicitly study the running times of the centralized stochastic gradient algo-
rithm (3)  the cyclic delayed protocol with the update (5)  and the locally averaged ar-
chitecture with the update (6). To make comparisons more cleanly  we avoid constants 

2Similar results continue to hold under weak correlation.

6

Centralized (3)

Cyclic (5)

Local (6)

Ef (bx) − f (x∗) = O(cid:18)r 1
T(cid:19)
Ef (bx) − f (x∗) = O(cid:18)min(cid:18) n2/3
T (cid:19) +
Ef (bx) − f (x∗) = O(cid:18)min(cid:18) D2/3
T (cid:19) +

T 2/3  
T 2/3  

nD2

n3

1

√T n(cid:19)
√nT(cid:19)

1

Table 1: Upper bounds on optimization error after T units of time. See text for details.

2 ≤ 1
m .

mPm

j=1 ∇F (x; ξj)k2

assuming that the bound σ2 on Ek∇f (x) − ∇F (x; ξ)k2 is 1  and that sampling ξ ∼ P and
evaluating ∇F (x; ξ) requires unit time. It is also clear that if we receive m uncorrelated
samples of ξ  the variance Ek∇f (x) − 1
Now we state our assumptions on the relative times used by each algorithm. Let T be
the number of units of time allocated to each algorithm  and let the centralized  cyclic
delayed and locally averaged delayed algorithms complete Tcent  Tcycle and Tdist iterations 
respectively  in time T . It is clear that Tcent = T . We assume that the distributed methods
use mcycle and mdist samples of ξ ∼ P to compute stochastic gradients. For concreteness  we
assume that communication is of the same order as computing the gradient of one sample
∇F (x; ξ).
In the cyclic setup of Sec. 3.1  it is reasonable to assume that mcycle = Ω(n)
to avoid idling of workers. For mcycle = Ω(n)  the master requires mcycle
units of time to
receive one gradient update  so mcycle
n Tcycle = T . In the local communication framework  if
each node uses mdist samples to compute a gradient  the master receives a gradient every
mdist units of time  and hence mdistTdist = T . We summarize our assumptions by saying
that in T units of time  each algorithm performs the following number of iterations:

n

Tcent = T 

Tcycle =

T n

mcycle

 

and

Tdist =

T

mdist

.

(7)

Combining with the bound (4) and Corollaries 1 and 2  we get the results in Table 1.
Asymptotically in the number of units of time T   both the cyclic and locally communicating
stochastic optimization schemes have the same convergence rate. Comparing the lower
order terms  since D ≤ n for any network  the locally averaged algorithm always guarantees
better performance than the cyclic algorithm. For speciﬁc graph topologies  however  we
can quantify the time improvements (assuming we are in the n2/3/T 2/3 regime):

• n-node cycle or path: D = n so that both methods have the same convergence rate.
• √n-by-√n grid: D = √n  so the distributed method has a factor of n2/3/n1/3 =
• Balanced trees and expander graphs: D = O(log n)  so the distributed method has

n1/3 improvement over the cyclic architecture.

a factor—ignoring logarithmic terms—of n2/3 improvement over cyclic.

5 Numerical Results

Though this paper focuses mostly on the theoretical analysis of delayed stochastic methods 
it is important to understand their practical aspects. To that end  we use the cyclic delayed
method (6) to solve a somewhat large logistic regression problem:

minimize

x

f (x) =

1
N

NXi=1

log(1 + exp(−bi hai  xi))

subject to kxk2 ≤ R.

(8)

We use the Reuters RCV1 dataset [11]  which consists of N ≈ 800000 news articles  each la-
beled with a combination of the four labels economics  government  commerce  and medicine.
In the above example  the vectors ai ∈ {0  1}d  d ≈ 105  are feature vectors representing the
words in each article  and the labels bi are 1 if the article is about government  −1 otherwise.

We simulate the cyclic delayed optimization algorithm (5) for the problem (8) for several
choices of the number of workers n and the number of samples m computed at each worker.
We summarize the results in Figure 4. We ﬁx an ǫ (in this case  ǫ = .05)  then measure the

7

800

600

400

200

y
c
a
r
u
c
c
a

ǫ

o
t

e
m
T

i

y
c
a
r
u
c
c
a

ǫ

o
t

e
m
T

i

1000

800

600

400

0

1

2

3

4

5

6

8

10 12

15 18

22 26

1

Number of workers

(a)

2
Number of workers

6

3

4

5

8

10

12

(b)

Figure 4: Estimated time to compute ǫ-accurate solution to the objective (8) as a function of
the number of workers n. See text for details. Plot (a): convergence time assuming the cost of
communication to the master and gradient computation are same. Plot (b): convergence time
assuming the cost of communication to the master is 16 times that of gradient computation.

time it takes the stochastic algorithm (5) to output anbx such that f (bx) ≤ inf x∈X f (x) + ǫ.

We perform each experiment ten times. The two plots diﬀer in the amount of time C
required to communicate the parameters x between the master and the workers (relative to
the amount of time to compute the gradient on one sample in the objective (8)). For the
left plot in Fig. 4(a)  we assume that C = 1  while in Fig. 4(b)  we assume that C = 16.
For Fig. 4(a)  each worker uses m = n samples to compute a stochastic gradient for the
objective (8). The plotted results show the delayed update (5) enjoys speedup (the ratio of
time to ǫ-accuracy for an n-node system versus the centralized procedure) nearly linear in
the number n of worker machines until n ≥ 15 or so. Since we use the stepsize choice η(t) ∝
term in the convergence rate presumably becomes non-negligible for larger n. This expands
on earlier experimental work with a similar method [10]  which experimentally demonstrated
linear speedup for small values of n  but did not investigate larger network sizes.
In Fig. 4(b)  we study the eﬀects of more costly communication by assuming that com-
munication is C = 16 times more expensive than gradient computation. As argued in the
long version [2]  we set the number of samples each worker computes to m = Cn = 16n

pt/n  which yields the predicted convergence rate given by Corollary 1  the n2m/T ≈ n3/T

and correspondingly reduce the damping stepsize η(t) ∝pt/(Cn). In the regime of more

expensive communication—as our theoretical results predict—small numbers of workers still
enjoy signiﬁcant speedups over a centralized method  but eventually the cost of communi-
cation and delays mitigate some of the beneﬁts of parallelization. The alternate choice of
stepsize η(t) = n2/3T −1/3 gives qualitatively similar performance.

6 Conclusion and Discussion

In this paper  we have studied delayed dual averaging algorithms for stochastic optimiza-
tion  showing applications of our results to distributed optimization. We showed that for
smooth problems  we can preserve the performance beneﬁts of parallelization over central-
ized stochastic optimization even when we relax synchronization requirements. Speciﬁcally 
we presented methods that take advantage of distributed computational resources and are
robust to node failures  communication latency  and node slowdowns. In addition  though
we omit these results for brevity  it is possible to extend all of our expected convergence
results to guarantees with high-probability.

Acknowledgments

AA was supported by a Microsoft Research Fellowship and NSF grant CCF-1115788  and
JCD was supported by the NDSEG Program and Google. We are very grateful to Ofer
Dekel  Ran Gilad-Bachrach  Ohad Shamir  and Lin Xiao for communicating of their proof
of the bound (4). We would also like to thank Yoram Singer and Dimitri Bertsekas for
reading a draft of this manuscript and giving useful feedback and references.

8

References

[1] A. Agarwal  P. Bartlett  P. Ravikumar  and M. Wainwright.

Information-theoretic
lower bounds on the oracle complexity of convex optimization. In Advances in Neural
Information Processing Systems 23  2009.

[2] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. URL

http://arxiv.org/abs/1104.5525  2011.

[3] D. P. Bertsekas. Distributed asynchronous computation of ﬁxed points. Mathematical

Programming  27:107–120  1983.

[4] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical

Methods. Prentice-Hall  Inc.  1989.

[5] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online

prediction using mini-batches. URL http://arxiv.org/abs/1012.1367  2010.

[6] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Robust distributed online

prediction. URL http://arxiv.org/abs/1012.1370  2010.

[7] J. Duchi  A. Agarwal  and M. Wainwright. Dual averaging for distributed optimization:
convergence analysis and network scaling. IEEE Transactions on Automatic Control 
to appear  2011.

[8] A. Juditsky  A. Nemirovski  and C. Tauvel. Solving variational inequalities with the

stochastic mirror-prox algorithm. URL http://arxiv.org/abs/0809.0815  2008.

[9] G. Lan.

ematical Programming Series A 
http://www.ise.uﬂ.edu/glan/papers/OPT SA4.pdf.

2010.

An optimal method for stochastic composite optimization. Math-
appear. URL

Online ﬁrst 

to

[10] J. Langford  A. Smola  and M. Zinkevich. Slow learners are fast. In Advances in Neural

Information Processing Systems 22  pages 2331–2339  2009.

[11] D. Lewis  Y. Yang  T. Rose  and F. Li. RCV1: A new benchmark collection for text

categorization research. Journal of Machine Learning Research  5:361–397  2004.

[12] A. Nedi´c  D.P. Bertsekas  and V.S. Borkar. Distributed asynchronous incremental
subgradient methods.
In D. Butnariu  Y. Censor  and S. Reich  editors  Inherently
Parallel Algorithms in Feasibility and Optimization and their Applications  volume 8 of
Studies in Computational Mathematics  pages 381–407. Elsevier  2001.

[13] A. Nedi´c and A. Ozdaglar. Distributed subgradient methods for multi-agent optimiza-

tion. IEEE Transactions on Automatic Control  54:48–61  2009.

[14] A. Nemirovski and D. Yudin. Problem Complexity and Method Eﬃciency in Optimiza-

tion. Wiley  New York  1983.

[15] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical

Programming A  120(1):261–283  2009.

[16] B. T. Polyak. Introduction to optimization. Optimization Software  Inc.  1987.

[17] S. S. Ram  A. Nedi´c  and V. V. Veeravalli. Distributed stochastic subgradient projection
algorithms for convex optimization. Journal of Optimization Theory and Applications 
147(3):516–545  2010.

[18] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical

Statistics  22:400–407  1951.

[19] J. Tsitsiklis. Problems in decentralized decision making and computation. PhD thesis 

Massachusetts Institute of Technology  1984.

[20] L. Xiao. Dual averaging methods for regularized stochastic learning and online opti-

mization. Journal of Machine Learning Research  11:2543–2596  2010.

9

,Fajwel Fogel
Alexandre d'Aspremont
Milan Vojnovic
Mikhail Figurnov
Aizhan Ibraimova
Dmitry Vetrov
Pushmeet Kohli
Tobias Plötz
Stefan Roth