2015,Online F-Measure Optimization,The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score  it avoids disadvantages of simple metrics like the error rate  especially in cases of imbalanced class distributions. The problem of optimizing the F-measure  that is  of developing learning algorithms that perform optimally in the sense of this measure  has recently been tackled by several authors. In this paper  we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover  first experimental results are presented  showing that our method performs well in practice.,Online F-Measure Optimization

R´obert Busa-Fekete

Department of Computer Science
University of Paderborn  Germany

busarobi@upb.de

Bal´azs Sz¨or´enyi

Technion  Haifa  Israel /

MTA-SZTE Research Group on
Artiﬁcial Intelligence  Hungary

Krzysztof Dembczy´nski

Institute of Computing Science

Pozna´n University of Technology  Poland
kdembczynski@cs.put.poznan.pl

szorenyibalazs@gmail.com

Eyke H¨ullermeier

Department of Computer Science
University of Paderborn  Germany

eyke@upb.de

Abstract

The F-measure is an important and commonly used performance metric for bi-
nary prediction tasks. By combining precision and recall into a single score  it
avoids disadvantages of simple metrics like the error rate  especially in cases of
imbalanced class distributions. The problem of optimizing the F-measure  that
is  of developing learning algorithms that perform optimally in the sense of this
measure  has recently been tackled by several authors. In this paper  we study
the problem of F-measure maximization in the setting of online learning. We
propose an efﬁcient online algorithm and provide a formal analysis of its conver-
gence properties. Moreover  ﬁrst experimental results are presented  showing that
our method performs well in practice.

1

Introduction

=

Being rooted in information retrieval [16]  the so-called F-measure is nowadays routinely used as a

binary labels y = (y1  . . .   yt)  the F-measure is deﬁned as

performance metric in various prediction tasks. Given predictions�y = (�y1  . . .  �yt) ∈ {0  1}t of t

(1)

F (y �y) =

2�t
2 · precision(y �y) · recall(y �y)
i=1 yi�yi
precision(y �y) + recall(y �y) ∈ [0  1]  
�t
i=1 yi +�t
i=1�yi
where precision(y �y) = �t
i=1 yi�yi/�t
i=1�yi  recall(y �y) = �t
i=1 yi�yi/�t

i=1 yi  and where
0/0 = 1 by deﬁnition. Compared to measures like the error rate in binary classiﬁcation  maximizing
the F-measure enforces a better balance between performance on the minority and majority class;
therefore  it is more suitable in the case of imbalanced data. Optimizing for such an imbalanced
measure is very important in many real-world applications where positive labels are signiﬁcantly
less frequent than negative ones. It can also be generalized to a weighted harmonic average of pre-
cision and recall. Yet  for the sake of simplicity  we stick to the unweighted mean  which is often
referred to as the F1-score or the F1-measure.
Given the importance and usefulness of the F-measure [19  20  18  10  11]  it is natural to look for
learning algorithms that perform optimally in the sense of this measure. However  optimizing the
F-measure is a quite challenging problem  especially because the measure is not decomposable over
the binary predictions. This problem has received increasing attention in recent years and has been
tackled by several authors [18  9  10  11]. However  most of this work has been done in the standard
setting of batch learning.

1

In this paper  we study the problem of F-measure optimization in the setting of online learning
[4  2]  which is becoming increasingly popular in machine learning. In fact  there are many applica-
tions in which training data is arriving progressively over time  and models need to be updated and
maintained incrementally. In our setting  this means that in each round t the learner ﬁrst outputs a

1. ﬁrst an instance xt ∈ X is observed by the learner 

prediction�yt and then observes the true label yt. Formally  the protocol in round t is as follows:
2. then the predicted label�yt for xt is computed on the basis of the ﬁrst t instances (x1  . . .   xt) 
the t− 1 labels (y1  . . .   yt−1) observed so far  and the corresponding predictions (�y1  . . .  �yt−1) 

3. ﬁnally  the label yt is revealed to the learner.
The goal of the learner is then to maximize

(2)

F(t) = F ((y1  . . .   yt)  (�y1  . . .  �yt))

over time. Optimizing the F-measure in an online fashion is challenging mainly because of the

non-decomposability of the measure  and the fact that the�yt cannot be changed after round t.

As a potential application of online F-measure optimization consider the recommendation of news
from RSS feeds or tweets [1]. Besides  it is worth mentioning that online methods are also relevant
in the context of big data and large-scale learning  where the volume of data  despite being ﬁnite 
prevents from processing each data point more than once [21  7]. Treating the data as a stream  online
algorithms can then be used as single-pass algorithms. Note  however  that single-pass algorithms
are evaluated only at the end of the training process  unlike online algorithms that are supposed to
learn and predict simultaneously.
We propose an online algorithm for F-measure optimization  which is not only very efﬁcient but also
easy to implement. Unlike other methods  our algorithm does not require extra validation data for
tuning a threshold (that separates between positive and negative predictions)  and therefore allows
the entire data to be used for training. We provide a formal analysis of the convergence properties
of our algorithm and prove its statistical consistency under different assumptions on the learning
process. Moreover  ﬁrst experimental results are presented  showing that our method performs well
in practice.

2 Formal Setting

ρ(x 1)

η(x) dµ(x).

In this paper  we consider a stochastic setting in which (x1  y1)  . . .   (xt  yt) are assumed to be i.i.d.
samples from some unknown distribution ρ(·) on X × Y  where Y = {0  1} is the label space and
X is some instance space. We denote the marginal distribution of the feature vector X by µ(·).1
Then  the posterior probability of the positive class  i.e.  the conditional probability that Y = 1
ρ(x 0)+ρ(x 1) . The prior distribution of class 1 can
given X = x  is η(x) = P(Y = 1| X = x) =
be written as π1 = P(Y = 1) =�x∈X
Let B = {f : X −→ {0  1}} be the set of all binary classiﬁers over the set X . The F-measure of a
binary classiﬁer f ∈ B is calculated as
2�X
η(x) dµ(x) +�X

According to [19]  the expected value of (1) converges to F (f ) with t → ∞ when f is used to

calculate�y  i.e. �yt = f (xt). Thus  limt→∞ E�F�(y1  . . .   yt)  (f (x1)  . . .   f (xt))�� = F (f ).

Now  let G = {g : X −→ [0  1]} denote the set of all probabilistic binary classiﬁers over the set
X   and let T ⊆ B denote the set of binary classiﬁers that are obtained by thresholding a classiﬁer
g ∈ G—that is  classiﬁers of the form

E [η(X)] + E [f (X)]

η(x)f (x) dµ(x)

2E [η(X)f (X)]

f (x) dµ(x)

=

F (f ) =

�X

.

is true and 0 otherwise.

for some threshold τ ∈ [0  1]  where�·� is the indicator function that evaluates to 1 if its argument

1X is assumed to exhibit the required measurability properties.

gτ (x) =�g(x) ≥ τ�

(3)

2

According to [19]  the optimal F-score computed as maxf∈B F (f ) can be achieved by a thresholded
classiﬁer. More precisely  let us deﬁne the thresholded F-measure as

F (τ ) = F (ητ ) =

Then the optimal threshold τ∗ can be obtained as

2�X
η(x)�η(x) ≥ τ� dµ(x)
η(x) dµ(x) +�X�η(x) ≥ τ� dµ(x)

�X

τ∗ = argmax
0≤τ≤1

F (τ ) .

=

2E [η(X)�η(X) ≥ τ�]
E [η(X)] + E [�η(X) ≥ τ�]

(4)

(5)

Clearly  for the classiﬁer in the form of (3) with g(x) = η(x) and τ = τ∗  we have F (gτ ) = F (τ∗).
Then  as shown by [19] (see their Theorem 4)  the performance of any binary classiﬁer f ∈ B
cannot exceed F (τ∗)  i.e.  F (f ) ≤ F (τ∗) for all f ∈ B. Therefore  estimating posteriors ﬁrst and
adjusting a threshold afterward appears to be a reasonable strategy. In practice  this seems to be the
most popular way of maximizing the F-measure in a batch mode; we call it the 2-stage F-measure
maximization approach  or 2S for short. More speciﬁcally  the 2S approach consists of two steps:
ﬁrst  a classiﬁer is trained for estimating the posteriors  and second  a threshold is tuned on the
posterior estimates. For the time being  we are not interested in the training of this classiﬁer but
focus on the second step  that is  the labeling of instances via thresholding posterior probabilities.
i=1 of labeled instances are given as training
For doing this  suppose a ﬁnite set DN = {(xi  yi)}N
are provided by a classiﬁer g ∈ G. Next  one might deﬁne the F-score obtained by applying the
threshold classiﬁer gτ on the data DN as follows:
�N
i=1 yi�τ ≤ g(xi)�
�N
i=1 yi +�N
i=1�τ ≤ g(xi)�

information. Moreover  suppose estimates �pi = g(xi) of the posterior probabilities pi = η(xi)

In order to ﬁnd an optimal threshold τN ∈ argmax0≤τ≤1 F (τ ; g DN )  it sufﬁces to search the
ﬁnite set {�p1  . . .  �pN}  which requires time O(N log N ). In [19]  it is shown that F (τ ; g DN ) P−→
F (gτ ) as N → ∞ for any τ ∈ (0  1)  and [11] provides an even stronger result: If a classiﬁer gDN
is induced from DN by an L1-consistent learner 2 and a threshold τN is obtained by maximizing (6)
) P−→ F (τ∗) as N −→ ∞ (under mild assumptions on the
on an independent set D�N   then F (gτN
DN
data distribution).

F (τ ; g DN ) =

(6)

3 Maximizing the F-Measure on a Population Level

In this section we assume that the data distribution is known. According to the analysis in the
previous section  optimizing the F-measure boils down to ﬁnding the optimal threshold τ∗. At this
point  an observation is in order.
Remark 1. In general  the function F (τ ) is neither convex nor concave. For example  when X is
ﬁnite  then the denominator and enumerator of (4) are step functions  whence so is F (τ ). Therefore 
gradient methods cannot be applied for ﬁnding τ∗.

Nevertheless  τ∗ can be found based on a recent result of [20]  who show that ﬁnding the root of

h(τ ) =�x∈X

max (0  η(x) − τ ) dµ(x) − τ π1

(7)

is a necessary and sufﬁcient condition for optimality. Note that h(τ ) is continuos and strictly de-
creasing  with h(0) = π1 and h(1) = −π1. Therefore  h(τ ) = 0 has a unique solution which
is τ∗. Moreover  [20] also prove an interesting relationship between the optimal threshold and the
F-measure induced by that threshold: F (τ∗) = 2τ∗.
The marginal distribution of the feature vectors  µ(·)  induces a distribution ζ(·) on the posteriors:
derivative of dµ
dζ   and ζ(p) the density of observing an instance x for which the probability of the
2A learning algorithm  viewed as a map from samples DN to classiﬁers gDN   is called L1-consistent w.r.t.

ζ(p) =�x∈X�η(x) = p� dµ(x)for all p ∈ [0  1]. By deﬁnition �η(x) = p� is the Radon-Nikodym
the data distribution ρ if limN→∞ PDN ∼ρ��x∈X |gDN (x) − η(x)| dµ(x) > �� = 0 for all � > 0.

3

0

positive label is p. We shall write concisely dν(p) = ζ(p) dp. Since ν(·) is an induced probability
measure  the measurable transformation allows us to rewrite the notions introduced above in terms
of ν(·) instead of µ(·)—see  for example  Section 1.4 in [17]. For example  the prior probability
�X
η(x) dµ can be written equivalently as� 1
h(τ ) =� 1
max (0  p − τ ) dν(p) − τ� 1
p dν(p) − τ�� 1
=� 1
1 dν(p) +� 1

p dν(p) =� 1
p dν(p)�

0 p dν(p). Likewise  (7) can be rewritten as follows:

p − τ dν(p) − τ� 1

Equation (8) will play a central role in our analysis. Note that precise knowledge of ν(·) sufﬁces to
ﬁnd the maxima of F (τ ). This is illustrated by two examples presented in Appendix E  in which we
assume speciﬁc distributions for ν(·)  namely uniform and Beta distributions.
4 Algorithmic Solution

p dν(p)

(8)

0

0

0

τ

τ

τ

�pt = gt(xt) of the probability η(xt). We would like to stress again that the focus of our analysis is

In this section  we provide an algorithmic solution to the online F-measure maximization problem.
For this  we shall need in each round t some classiﬁer gt ∈ G that provides us with some estimate
on optimal thresholding instead of classiﬁer learning. Thus  we assume the sequence of classiﬁers
g1  g2  . . . to be produced by an external online learner  for example  logistic regression trained by
stochastic gradient descent.
As an aside  we note that F-measure maximization is not directly comparable with the task that
is most often considered and analyzed in online learning  namely regret minimization [4]. This is
mainly because the F-measure is a non-decomposable performance metric. In fact  the cumulative

outcome yt [11]. In the case of the F-measure  the score F(t)  and therefore the optimal prediction

regret is a summation of a per-round regret rt  which only depends on the prediction�yt and the true
�yt  depends on the entire history  that is  all observations and decisions made by the learner till time

t. This is discussed in more detail in Section 6.
The most naive way of forecasting labels is to implement online learning as repeated batch learning 
i=1 in each time step t. Obviously 
that is  to apply a batch learner (such as 2S) to Dt = {(xi  yi)}t
however  this strategy is prohibitively expensive  as it requires storage of all data points seen so
far (at least in mini-batches)  as well as optimization of the threshold τt and re-computation of the
classiﬁer gt on an ever growing number of examples.
In the following  we propose a more principled technique to maximize the online F-measure. Our
approach is based on the observation that h(τ∗) = 0 and h(τ )(τ − τ∗) < 0 for any τ ∈ [0  1] such
that τ �= τ∗ [20]. Moreover  it is a monotone decreasing continuous function. Therefore  ﬁnding
the optimal threshold τ∗ can be viewed as a root ﬁnding problem. In practice  however  h(τ ) is not

known and can only be estimated. Let us deﬁne h�τ  y �y� = y�y−τ (y +�y) . For now  assume η(x)
to be known and write concisely�h(τ ) = h(τ  y �η(x) ≥ τ�). We can compute the expectation of
�h(τ ) with respect to the data distribution for a ﬁxed threshold τ as follows:
p +�p ≥ τ� dν(p)�
1 dν(p)� = h(τ )

E��h(τ )� = E [h(τ  y �η(x) ≥ τ�)] = E [y�η(x) ≥ τ� − τ (y +�η(x) ≥ τ�)]

p�p ≥ τ� dν(p) − τ�� 1
p dν(p) − τ�� 1
p dν(p) +� 1

=� 1
=� 1

Thus  an unbiased estimate of h(τ ) can be obtained by evaluating�h(τ ) for an instance x. This
suggests designing a stochastic approximation algorithm that is able to ﬁnd the root of h(·) similarly
to the Robbins-Monro algorithm [12]. Exploiting the relationship between the optimal F-measure
and the optimal threshold  F (τ∗) = 2τ∗  we deﬁne the threshold in time step t as

(9)

0

0

0

τ

τ

τt =

1
2

F(t) =

at
bt

where at =

bt =

yi +

t�i=1

t�i=1�yi .

yi�yi 

t�i=1

4

(10)

With this threshold  the ﬁrst differences between thresholds  i.e. τt+1− τt  can be written as follows.

Proposition 2. If thresholds τt are deﬁned according to (10) and�yt+1 as�η(xt+1) > τt�  then

(11)
The proof of Prop. 2 is deferred to Appendix A. According to (11)  the method we obtain “almost”
coincides with the update rule of the Robbins-Monro algorithm. There are  however  some notable
differences. In particular  the sequence of coefﬁcients  namely the values 1/bt+1  does not consist
of predeﬁned real values converging to zero (as fast as 1/t). Instead  it consists of random quantities

(τt+1 − τt)bt+1 = h(τt  yt+1 �yt+1) .

that depend on the history  namely the observed labels y1  . . .   yt and the predicted labels�y1  . . .  �yt.
Moreover  these “coefﬁcients” are not independent of h(τt  yt+1 �yt+1) either.

In spite of these
additional difﬁculties  we shall present a convergence analysis of our algorithm in the next section.
The pseudo-code of our online F-measure op-
timization algorithm  called Online F-measure
Optimizer (OFO)  is shown in Algorithm 1.
The forecast rule can be written in the form of

Observe the instance xt

Algorithm 1 OFO
1: Select g0 from B  and set τ0 = 0
2: for t = 1 → ∞ do
3:
4:
�pt ← gt−1(xt)
5:
�yt ←��pt ≥ τt−1�
Observe label yt
6:
Calculate F(t) = 2at
7:
bt
gt ← A(gt−1  xt  yt) � update the classiﬁer
8:
9: return τT

� estimate posterior
� current prediction

deﬁned in (10) and pt = η(xt). In practice  we

�yt =�pt ≥ τt−1� for xt where the threshold is
use �pt = gt−1(xt) as an estimate of the true

posterior pt.
In line 8 of the code  an online
learner A : G × X × Y −→ G is assumed 
which produces classiﬁers gt by incrementally
updating the current classiﬁer with the newly
observed example  i.e.  gt = A(gt−1  xt  yt).
In our experimental study  we shall test and compare various state-of-the-art online learners as pos-
sible choices for A.
5 Consistency

and τt = at
bt

In this section  we provide an analysis of the online F-measure optimizer proposed in the previous
section. More speciﬁcally  we show the statistical consistency of the OFO algorithm: The sequence
of online thresholds and F-scores produced by this algorithm converge  respectively  to the optimal
threshold τ∗ and the optimal thresholded F-score F (τ∗) in probability. As a ﬁrst step  we prove this
result under the assumption of knowledge about the true posterior probabilities; then  in a second
step  we consider the case of estimated posteriors.
Theorem 3. Assume the posterior probabilities pt = η(xt) of the positive class to be known in each
step of the online learning process. Then  the sequences of thresholds τt and online F-scores F(t)
produced by OFO both converge in probability to their optimal values τ∗ and F (τ∗)  respectively:

For any � > 0  we have limt→∞ P�|τt − τ∗| > �� = 0 and limt→∞ P�|F(t) − F (τ∗)| > �� = 0.
Here is a sketch of the proof of this theorem  the details of which can be found in the supplementary
material (Appendix B):
• We focus on {τt}∞t=1  which is a stochastic process the ﬁltration of which is deﬁned as
Ft = {y1  . . .   yt �y1  . . .  �yt}. For this ﬁltration  one can show that�h(τt) is Ft-measurable
and E��h(τt)|Ft� = h(τt) based on (9).
bt+1�h(τt)���Ft� =
• As a ﬁrst step  we can decompose the update rule given in (11) as follows: E� 1
t� conditioned on the ﬁltration Ft (see Lemma 7).
bt+2 h(τt) + O� 1
• Next  we show that the sequence 1/bt behaves similarly to 1/t  in the sense that�∞t=1 E�1/b2
t� <
∞ (see Lemma 8). Moreover  one can show that�∞t=1 E [1/bt] ≥�∞t=1
• Although h(τ ) is not differentiable on [0  1] in general (it can be piecewise linear  for example) 
one can show that its ﬁnite difference is between −1 − π1 and −π1 (see Proposition 9 in the
appendix). As a consequence of this result  our process deﬁned in (11) does not get stuck even
close to τ∗.

2t = ∞.

b2

1

1

• The main part of the proof is devoted to analyzing the properties of the sequence of βt =
E�(τt − τ∗)2�for which we show that limt→∞ βt = 0  which is sufﬁcient for the statement

5

of the theorem. Our proof follows the convergence analysis of [12]. Nevertheless  our analysis
essentially differs from theirs  since in our case  the coefﬁcients cannot be chosen freely. In-
stead  as explained before  they depend on the labels observed and predicted so far. In addition 
the noisy estimation of h(·) depends on the labels  too  but the decomposition step allows us to
handle this undesired effect.
Remark 4. In principle  the Robbins-Monro algorithm can be applied for ﬁnding the root of h(·)
as well. This yields an update rule similar to (11)  with 1/bt+1 replaced by C/t for a constant
C > 0. In this case  however  the convergence of the online F-measure is difﬁcult to analyze (if at
all)  because the empirical process cannot be written in a nice form. Moreover  as it has been found
in the analysis  the coefﬁcient C should be set ≈ 1/π1 (see Proposition 9 and the choice of {kt} at
the end of the proof of Theorem 3). Yet  since π1 is not known beforehand  it needs to be estimated
from the samples  which implies that the coefﬁcients are not independent of the noisy evaluations
of h(·)—just like in the case of the OFO algorithm. Interestingly  OFO seems to properly adjust
the values 1/bt+1 in an adaptive manner (bt is a sum of two terms  the ﬁrst of which is tπ1 in
expectation)  which is a very nice property of the algorithm. Empirically  based on synthetic data 
we found the performance of the original Robbins-Monro algorithm to be on par with OFO.

As already announced  we are now going to relax the assumption of known posterior probabilities

pt = η(xt). Instead  estimates�pt = gt(xt) ≈ pt of these probabilities are obtained by classiﬁers gt

that are provided by the external online learner in Algorithm 1. More concretely  assume an online
learner A : G × X × Y −→ G  where G is the set of probabilistic classiﬁers. Given a current model
gt and a new example (xt  yt)  this learner produces an updated classiﬁer gt+1 = A(gt  xt  yt).
Showing a consistency result for this scenario requires some assumptions on the online learner.
With this formal deﬁnition of online learner  a statistical consistency result similar to Theorem 3
can be shown. The proof of the following theorem is again deferred to supplementary material
(Appendix C).
Theorem 5. Assume that the classiﬁers (gt)∞t=1 in the OFO framework are provided by an online

learner for which the following holds: There is a λ > 0 such that E��x∈X |η(x) − gt(x)| dµ(x)� =
O(t−λ) . Then  F(t)
This theorem’s requirement on the online learner is stronger than what is assumed by [11] and
recalled in Footnote 2. First  the learner is trained online and not in a batch mode. Second  we also
require that the L1 error of the learner goes to 0 with a convergence rate of order t−λ.
It might be interesting to note that a universal rate of convergence cannot be established without
assuming regularity properties of the data distribution  such as smoothness via absolute continuity.
Results of that kind are beyond the scope of this study. Instead  we refer the reader to [5  6] for
details on L1 consistency and its connection to the rate of convergence.

P→ F (τ∗) and τt

P→ τ∗.

6 Discussion

Regret optimization and stochastic approximation: Stochastic approximation algorithms can be ap-
plied for ﬁnding the optimum of (4) or  equivalently  to ﬁnd the unique root of (8) based on noisy
evaluations—the latter formulation is better suited for the classic version of the Robbins-Monro root
ﬁnding algorithm [12]. These algorithms are iterative methods whose analysis focuses on the dif-
ference of F (τt) from F (τ∗)  where τt denotes the estimate of τ∗ in iteration t  whereas our online

prediction for yi in round i. This difference is crucial because F (τt) only depends on τt and in
addition  if τt is close to τ∗ then F (τt) is also close to F (τ∗) (see [19] for concentration proper-

different from F (τ∗) even if the current estimate τt is close to τ∗ in case the number of previous
incorrect predictions is large.
In online learning and online optimization it is common to work with the notion of (cumulative)

setting is concerned with the distance of F ((y1  . . .   yt)  (�y1  . . .  �yt)) from F (τ∗)  where�yi is the
ties)  whereas in the online F-measure optimization setup  F ((y1  . . .   yt)  (�y1  . . .  �yt)) can be very
regret. In our case  this notion could be interpreted either as�t
i=1 |F ((y1  . . .   yi)  (�y1  . . .  �yi)) −
F (τ∗)| or as�t
i=1 |yi −�yi|. After division by t  the former becomes the average accuracy of the
because |F ((y1  . . .   yi)  (�y1  . . .  �yi)) − F (τ∗)| itself is an aggregate measure of our performance

F-measure over time and the latter the accuracy of our predictions. The former is hard to interpret

6

Table 1: Main statistics of the benchmark datasets and one pass F-scores obtained by OFO and 2S
methods on various datasets. The bold numbers indicate when the difference is signiﬁcant between
the performance of OFO and 2S methods. The signiﬁcance level is set to one sigma that is estimated
based on the repetitions.

Dataset
gisette
news20.bin
Replab
WebspamUni
epsilon
covtype
url
SUSY
kdda
kddb

#pos
3500
9997
10797

Learner:
#neg #features
3500
5000
9999 1355191
353754
34874
254
212189 137811
2000
249778 250222
297711 283301
54
792145 1603985 3231961
18

#instances
7000
19996
45671
350000
500000
581012
2396130
5000000 2287827 2712173
8918054 7614730 1303324 20216830 0.927 0.926

20012498 17244034 2768464 29890095

LogReg

Pegasos

Perceptron

2S OFO

OFO
2S
OFO
2S
0.950 0.935 0.935 0.920
0.954 0.955
0.879 0.883 0.908 0.930
0.879 0.876
0.924 0.923
0.926 0.928 0.914 0.914
0.912 0.918 0.914 0.910 0.927 0.912
0.884 0.886 0.862 0.872
0.878 0.872
0.754 0.760 0.732 0.719
0.761 0.762
0.962 0.963
0.951 0.950 0.971 0.972
0.762 0.762 0.754 0.745 0.710 0.720
0.921 0.926 0.913 0.927
0.934 0.934 0.930 0.929 0.923 0.928

over the ﬁrst t rounds  which thus makes no sense to aggregate again. The latter  on the other hand 

differs qualitatively from our ultimate goal; in fact  |F ((y1  . . .   yt)  (�y1  . . .  �yt)) − F (τ∗)| is the

alternate measure that we are aiming to optimize for instead of the accuracy.
Online optimization of non-decomposable measures: Online optimization of the F-measure can be
seen as a special case of optimizing non-decomposable loss functions as recently considered by [9].
Their framework essentially differs from ours in several points. First  regarding the data generation
process  the adversarial setup with oblivious adversary is assumed  unlike our current study where
a stochastic setup is assumed. From this point of view  their assumption is more general since
the oblivious adversary captures the stochastic setup. Second  the set of classiﬁers is restricted to
differentiable parametric functions  which may not include the F-measure maximizer. Therefore 
their proof of vanishing regret does in general not imply convergence to the optimal F-score. Seen
from this point of view  their result is weaker than our proof of consistency (i.e.  convergence to
the optimal F-measure in probability if the posterior estimates originate from a consistent learner).
Finally  there are some other non-decomposable performance measures which are intensively used
in many practical applications. Their optimization had already been investigated in the online or
one-pass setup. The most notable such measure might be the area under the ROC curve (AUC)
which had been investigated in an online learning framework by [21  7].

7 Experiments

In this section  the performance of the OFO algorithm is evaluated in a one-pass learning scenario
on benchmark datasets  and compared with the performance of the 2-stage F-measure maximization
appraoch (2S) described in Section 2. We also assess the rate of convergence of the OFO algorithm
in a pure online learning setup.3
The online learner A in OFO was implemented in different ways  using Logistic Regression
(LOGREG)  the classical Perceptron algorithm (PERCEPTRON) [13] and an online linear SVM called
PEGASOS [14]. In the case of LOGREG  we applied the algorithm introduced in [15] which handles
L1 and L2 regularization. The hyperparameters of the methods and the validation procedures are
described below and in more detail in Appendix D. If necessary  the raw outputs of the learners were
turned into valid probability estimates  i.e.  they were rescaled to [0  1] using logistic transform.
We used in the experiments nine datasets taken from the LibSVM repository of binary classiﬁcation
tasks.4 Many of these datasets are commonly used as benchmarks in information retrieval where the
F-score is routinely applied for model selection. In addition  we also used the textual data released
in the Replab challenge of identifying relevant tweets [1]. We generated the features used by the
winner team [8]. The main statistics of the datasets are summarized in Table 1.

3Additional results of experiments conducted on synthetic data are presented in Appendix F.
4
http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/binary.html

7

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
o
c
s
−
F
 
)
e
n

i
l

n
O

(

0
 
0
10

SUSY

 

One−pass+LogReg
Online+LogReg
One−pass+Pegasos
Online+Pegasos
One−pass+Perceptron
Online+Peceptron

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
o
c
s
−
F
 
)
e
n

i
l

n
O

(

WebspamUni

 

One−pass+LogReg
Online+LogReg
One−pass+Pegasos
Online+Pegasos
One−pass+Perceptron
Online+Peceptron

2
10

4
10

6
10

Num. of samples

0
 
0
10

1
10

2
10
Num. of samples

3
10

4
10

5
10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
o
c
s
−
F
 
)
e
n

i
l

n
O

(

0
 
0
10

kdda

 

One−pass+LogReg
Online+LogReg
One−pass+Pegasos
Online+Pegasos
One−pass+Perceptron
Online+Peceptron

2
10

4
10

6
10

Num. of samples

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
o
c
s
−
F
 
)
e
n

i
l

n
O

(

0
 
0
10

url

 

One−pass+LogReg
Online+LogReg
One−pass+Pegasos
Online+Pegasos
One−pass+Perceptron
Online+Peceptron

2
10

4
10

Num. of samples

6
10

Figure 1: Online F-scores obtained by OFO algorithm on various dataset. The dashed lines represent
the one-pass performance of the OFO algorithm from Table 1 which we considered as baseline.

One-pass learning.
In one-pass learning  the learner is allowed to read the training data only
once  whence online learners are commonly used in this setting. We run OFO along with the three
classiﬁers trained on 80% of the data. The learner obtained by OFO is of the form gτt
t   where t
is the number of training samples. The rest 20% of the data was used to evaluate gτt
in terms of
t
the F-measure. We run every method on 10 randomly shufﬂed versions of the data and averaged
results. The means of the F-scores computed on the test data are shown in Table 1. As a baseline 
we applied the 2S approach. More concretely  we trained the same set of learners on 60% of the
data and validated the threshold on 20% by optimizing (6). Since both approaches are consistent 
the performance of OFO should be on par with the performance of 2S. This is conﬁrmed by the
results  in which signiﬁcant differences are observed in only 7 of 30 cases. These differences in
performance might be explained by the ﬁniteness of the data. The advantage of our approach over
2S is that there is no need of validation and the data needs to be read only once  therefore it can
be applied in a pure one-pass learning scenario. The hyperparameters of the learning methods are
chosen based on the performance of 2S. We tuned the hyperparameters in a wide range of values
which we report in Appendix D.

Online learning. The OFO algorithm has also been evaluated in the online learning scenario in
terms of the online F-measure (2). The goal of this experiment is to assess the convergence rate of
OFO. Since the optimal F-measure is not known for the datasets  we considered the test F-scores
reported in Table 1. The results are plotted in Figure 1 for four benchmark datasets (the plots for
the remaining datasets can be found in Appendix G). As can be seen  the online F-score converges
to the test F-score obtained in one-pass evalaution in almost every case. There are some exceptions
in the case of PEGASOS and PERCEPTRON. This might be explained by the fact that SVM-based
methods as well as the PERCEPTRON tend to produce poor probability estimates in general (which
is a main motivation for calibration methods turning output scores into valid probabilities [3]).

8 Conclusion and Future Work

This paper studied the problem of online F-measure optimization. Compared to many conven-
tional online learning tasks  this is a speciﬁcally challenging problem  mainly because of the non-
decomposable nature of the F-measure. We presented a simple algorithm that converges to the
optimal F-score when the posterior estimates are provided by a sequence of classiﬁers whose L1
error converges to zero as fast as t−λ for some λ > 0. As a key feature of our algorithm  we note
that it is a purely online approach; moreover  unlike approaches such as 2S  there is no need for a
hold-out validation set in batch mode. Our promising results from extensive experiments validate
the empirical efﬁcacy of our algorithm.
For future work  we plan to extend our online optimization algorithm to a broader family of complex
performance measures which can be expressed as ratios of linear combinations of true positive  false
positive  false negative and true negative rates [10]; the F-measure also belongs to this family. More-
over  going beyond consistency  we plan to analyze the rate of convergence of our OFO algorithm.
This might be doable thanks to several nice properties of the function h(τ ). Finally  an intriguing
question is what can be said about the case when some bias is introduced because the classiﬁer gt
does not converge to η.
Acknowledgments. Krzysztof Dembczy´nski is supported by the Polish National Science Centre
under grant no. 2013/09/D/ST6/03917. The research leading to these results has received funding
from the European Research Council under the European Union’s Seventh Framework Programme
(FP/2007-2013) / ERC Grant Agreement n. 306638.

8

References
[1] E. Amig´o  J. C. de Albornoz  I. Chugur  A. Corujo  J. Gonzalo  T. Mart´ın-Wanton  E. Meij 
M. de Rijke  and D. Spina. Overview of RepLab 2013: Evaluating online reputation monitoring
systems. In CLEF  volume 8138  pages 333–352  2013.

[2] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[3] R. Busa-Fekete  B. K´egl  T. ´Eltet˝o  and Gy. Szarvas. Tune and mix: Learning to rank using

ensembles of calibrated multi-class classiﬁers. Machine Learning  93(2–3):261–292  2013.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University

Press  2006.

[5] L. Devroye and L. Gy¨orﬁ. Nonparametric Density Estimation: The L1 View. Wiley  NY  1985.
[6] L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer 

NY  1996.

[7] W. Gao  R. Jin  S. Zhu  and Z.-H. Zhou. One-pass AUC optimization. In ICML  volume 30:3 

pages 906–914  2013.

[8] V. Hangya and R. Farkas. Filtering and polarity detection for reputation management on tweets.

In Working Notes of CLEF 2013 Evaluation Labs and Workshop  2013.

[9] P. Kar  H. Narasimhan  and P. Jain. Online and stochastic gradient methods for non-

decomposable loss functions. In NIPS  2014.

[10] N. Nagarajan  S. Koyejo  R. Ravikumar  and I. Dhillon. Consistent binary classiﬁcation with

generalized performance metrics. In NIPS  pages 2744–2752  2014.

[11] H. Narasimhan  R. Vaish  and Agarwal S. On the statistical consistency of plug-in classiﬁers

for non-decomposable performance measures. In NIPS  2014.

[12] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statist.  22(3):400–

407  1951.

[13] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization

in the brain. Psychological Review  65(6):386–408  1958.

[14] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver

for SVM. In ICML  pages 807–814  2007.

[15] Y. Tsuruoka  J. Tsujii  and S. Ananiadou. Stochastic gradient descent training for L1-

regularized log-linear models with cumulative penalty. In ACL  pages 477–485  2009.

[16] C.J. van Rijsbergen. Foundation and evalaution. Journal of Documentation  30(4):365–373 

1974.

[17] S. R. S. Varadhan. Probability Theory. New York University  2000.
[18] W. Waegeman  K. Dembczy´nski  A. Jachnik  W. Cheng  and E. H¨ullermeier. On the Bayes-
optimality of F-measure maximizers. Journal of Machine Learning Research  15(1):3333–
3388  2014.

[19] N. Ye  K. M. A. Chai  W. S. Lee  and H. L. Chieu. Optimizing F-measure: A tale of two

approaches. In ICML  2012.

[20] M. Zhao  N. Edakunni  A. Pocock  and G. Brown. Beyond Fano’s inequality: Bounds on the
optimal F-score  BER  and cost-sensitive risk and their implications. JMLR  pages 1033–1090 
2013.

[21] P. Zhao  S. C. H. Hoi  R. Jin  and T. Yang. Online AUC maximization.

233–240  2011.

In ICML  pages

9

,Hesham Mostafa
Lorenz. Mueller
Giacomo Indiveri
Róbert Busa-Fekete
Balázs Szörényi
Krzysztof Dembczynski
Eyke Hüllermeier
Moran Feldman
Amin Karbasi
Ehsan Kazemi