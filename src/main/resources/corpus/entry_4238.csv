2017,Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery,In machine learning and compressed sensing  it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper  we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end  we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory  2017]. We show that under proper conditions  PHT recovers an arbitrary "s"-sparse signal within O(s kappa log kappa) iterations where "kappa" is an appropriate condition number. Specifying the PHT operator  we obtain the best known result for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT compared to other popular recovery methods.,Partial Hard Thresholding: Towards A Principled

Analysis of Support Recovery

Jie Shen

Ping Li

Department of Computer Science

Department of Statistics and Biostatistics

School of Arts and Sciences

Department of Computer Science

Rutgers University
New Jersey  USA

js2007@rutgers.edu

Rutgers University
New Jersey  USA

pingli@stat.rutgers.edu

Abstract

In machine learning and compressed sensing  it is of central importance to under-
stand when a tractable algorithm recovers the support of a sparse signal from its
compressed measurements. In this paper  we present a principled analysis on the
support recovery performance for a family of hard thresholding algorithms. To
this end  we appeal to the partial hard thresholding (PHT) operator proposed re-
cently by Jain et al. [IEEE Trans. Information Theory  2017]. We show that under
proper conditions  PHT recovers an arbitrary s-sparse signal within O(sκ log κ)
iterations where κ is an appropriate condition number. Specifying the PHT opera-
tor  we obtain the best known results for hard thresholding pursuit and orthogonal
matching pursuit with replacement. Experiments on the simulated data comple-
ment our theoretical ﬁndings and also illustrate the effectiveness of PHT.

1 Introduction

This paper is concerned with the problem of recovering an arbitrary sparse signal from a set of its

(compressed) measurements. We say that a signal ¯x ∈ Rd is s-sparse if there are no more than s
non-zeros in ¯x. This problem  together with its many variants  have found a variety of successful
applications in compressed sensing  machine learning and statistics. Of particular interest is the
setting where ¯x is the true signal and only a small number of linear measurements are given  referred
to as compressed sensing. Such instance has been exhaustively studied in the last decade  along with
a large body of elegant work devoted to efﬁcient algorithms including ℓ1-based convex optimization
and hard thresholding based greedy pursuits [7  6  15  8  3  5  11]. Another quintessential example is
the sparsity-constrained minimization program recently considered in machine learning [30  2  14 
22]  for which the goal is to efﬁciently learn the global sparse minimizer ¯x from a set of training
data. Though in most cases  the underlying signal can be categorized into either of the two classes 
we note that it could also be other object such as the parameter of logistic regression [19]. Hence  for
a uniﬁed analysis  this paper copes with an arbitrary sparse signal and the results to be established
quickly apply to the special instances above.

It is also worth mentioning that while one can characterize the performance of an algorithm and
can evaluate the obtained estimate from various aspects  we are speciﬁcally interested in the qual-
ity of support recovery. Recall that for sparse recovery problems  there are two prominent metrics:
the ℓ2 distance and the support recovery. Theoretical results phrased in terms of the ℓ2 metric is
also referred to as parameter estimation  on which most of the previous papers emphasized. Under
this metric  many popular algorithms  e.g.  the Lasso [24  27] and hard thresholding based algo-
rithms [9  3  15  8  10  22]  are guaranteed with accurate approximation up to the energy of noise.
Support recovery is another important factor to evaluate an algorithm  which is also known as feature

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

selection or variable selection. As one of the earliest work  [25] offered sufﬁcient and necessary con-
ditions under which orthogonal matching pursuit and basis pursuit identify the support. The theory
was then developed by [35  32  27] for the Lasso estimator and by [29] for the garrotte estimator.

Typically  recovering the support of a target signal is more challenging than parameter estimation.
For instance  [18] showed that the restricted eigenvalue condition sufﬁces for the Lasso to produce
an accurate estimate whereas in order to recover the sign pattern  a more stringent mutual incoher-
ence condition has to be imposed [27]. However  as has been recognized  if the support is detected
precisely by a method  then the solution admits the optimal statistical rate [27]. In this regard  re-
search on support recovery continues to be a central theme in recent years [33  34  31  4  17]. Our
work follows this line and studies the support recovery performance of hard thresholding based algo-
rithms  which enjoy superior computational efﬁciency to the convex programs when manipulating a
huge volume of data [26].

We note that though [31  4] have carried out theoretical understanding for hard thresholding pur-
suit (HTP) [10]  showing that HTP identiﬁes the support of a signal within a few iterations  neither
of them obtained the general results in this paper. To be more detailed  under the restricted isome-
try property (RIP) condition [6]  our iteration bound holds for an arbitrary sparse signal of interest 
while the results from [31  4] hold either for the global sparse minimizer or for the true sparse signal.
Using a relaxed sparsity condition  we obtain a clear iteration complexity O(sκ log κ) where κ is
a proper condition number. In contrast  it is hard to quantify the bound of [31] (see Theorem 3
therein). From the algorithmic perspective  we consider a more general algorithm than HTP. In fact 
we appeal to the recently proposed partial hard thresholding (PHT) operator [13] and demonstrate
novel results  which in turn indicates the best known iteration complexity for HTP and orthogonal
matching pursuit with replacement (OMPR) [12]. Thereby  the results in this paper considerably
extend our earlier work on HTP [23]. It is also worth mentioning that  though our analysis hinges on
the PHT operator  the support recovery results to be established are stronger than the results in [13]
since they only showed parameter estimation of PHT. Finally  we remark that while a couple of
previous work considered signals that are not exactly sparse (e.g.  [4])  we in this paper focus on the
sparse case. Extensions to the generic signals are left as interesting future directions.

Contribution. The contribution of this paper is summarized as follows. We study the iteration
complexity of the PHT algorithm  and show that under the RIP condition or the relaxed sparsity con-
dition (to be clariﬁed)  PHT recovers the support of an arbitrary s-sparse signal within O(sκ log κ)
iterations. This strengthens the theoretical results of [13] where only parameter estimation of PHT
was established. Thanks to the generality of the PHT operator  our results shed light on the support
recovery performance of a family of prevalent iterative algorithms. As two extreme cases of PHT 
the new results immediately apply to HTP and OMPR  and imply the best known bound.

Roadmap. The remainder of the paper is organized as follows. We describe the problem setting 
as well as the partial hard thresholding operator in Section 2  followed by the main results regarding
the iteration complexity. In Section 3  we sketch the proof of the main results and list some useful
lemmas which might be of independent interest. Numerical results are illustrated in Section 4 and
Section 5 concludes the paper and poses several interesting future work. The detailed proof of our
theoretical results is deferred to the appendix (see the supplementary ﬁle).

Notation. We collect the notation that is involved in this paper. The upper-right letter C and its
subscript variants (e.g.  C1) are used to denote absolute constants whose values may change from
appearance to appearance. For a vector x ∈ Rd  its ℓ2 norm is denoted by kxk. The support set
of x is denoted by supp (x) which indexes the non-zeros in x. With a slight abuse  supp (x  k)
is the set of indices for the k largest (in magnitude) elements. Ties are broken lexicographically.
We interchangeably write kxk0 or |supp (x)| to signify the cardinality of supp (x). We will also
consider a vector restricted on a support set. That is  for a d-dimensional vector x and a support
set T ⊂ {1  2  . . .   d}  depending on the context  xT can either be a |T|-dimensional vector by
extracting the elements belonging to T or a d-dimensional vector by setting the elements outside T
to zero. The complement of a set T is denoted by T .
We reserve ¯x ∈ Rd for the target s-sparse signal whose support is denoted by S. The quantity
¯xmin > 0 is the minimum absolute element in ¯xS  where we recall that ¯xS ∈ Rs consists of the
non-zeros of ¯x. The PHT algorithm will depend on a carefully chosen function F (x). We write
its gradient as ∇F (x) and we use ∇kF (x) as a shorthand of (∇F (x))supp(∇F (x) k)  i.e.  the top k
absolute components of ∇F (x).

2

2 Partial Hard Thresholding

To pursue a sparse solution  hard thresholding has been broadly invoked by many popular greedy
algorithms. In the present work  we are interested in the partial hard thresholding operator which
sheds light upon a uniﬁed design and analysis for iterative algorithms employing this operator and
the hard thresholding operator [13]. Formally  given a support set T and a freedom parameter r > 0 
the PHT operator which is used to produce a k-sparse approximation to z is deﬁned as follows:

PHTk (z; T  r) := arg min

x∈Rd kx − zk   s.t. kxk0 ≤ k  |T \ supp (x)| ≤ r.

(1)

The ﬁrst constraint simply enforces a k-sparse solution. To gain intuition on the second one  consider
that T is the support set of the last iterate of an iterative algorithm  for which |T| ≤ k. Then
the second constraint ensures that the new support set differs from the previous one by at most r
positions. As a special case  one may have noticed that the PHT operator reduces to the standard
hard thresholding when picking the freedom parameter r ≥ k. On the other spectrum  if we look at
the case where r = 1  the PHT operator yields the interesting algorithm termed orthogonal matching
pursuit with replacement [12]  which in general replaces one element in each iteration.

It has been shown in [13] that the PHT operator can be computed in an efﬁcient manner for a general
support set T and a freedom parameter r. In this paper  our major focus will be on the case |T| = k1.
Then Lemma 1 of [13] indicates that PHTk (z; T  r) is given as follows:

(2)
where HTk(·) is the standard hard thresholding operator that sets all but the k largest absolute com-
ponents of a vector to zero.

T   r(cid:1)   PHTk (z; T  r) = HTk(cid:0)zT∪top(cid:1)  

top = supp(cid:0)z

Equipped with the PHT operator  we are now in the position to describe a general iterative greedy al-
gorithm  termed PHT(r) where r is the freedom parameter in (1). At the t-th iteration  the algorithm
reveals the last iterate xt−1 as well as its support set St−1  and returns a new solution as follows:

zt = xt−1 − η∇F (xt−1) 
yt = PHTk(cid:0)zt; St−1  r(cid:1)   St = supp(cid:0)yt(cid:1)  
F (x)  s.t. supp (x) ⊂ St.

xt = arg min

x∈Rd

Above  we note that η > 0 is a step size and F (x) is a proxy function which should be carefully
chosen (to be clariﬁed later). Typically  the sparsity parameter k equals s  the sparsity of the target
signal ¯x. In this paper  we consider a more general choice of k which leads to novel results. For
further clarity  several comments on F (x) are in order.

First  one may have observed that in the context of sparsity-constrained minimization  the proxy
function F (x) used above is chosen as the objective function [30  14]. In that scenario  the target
signal is a global optimum and PHT(r) proceeds as projected gradient descent. Nevertheless  recall
that our goal is to estimate an arbitrary signal ¯x.
It is not realistic to look for a function F (x)
such that our target happens to be its global minimizer. The remedy we will offer is characterizing
a deterministic condition between ¯x and ∇F (¯x) which is analogous to the signal-to-noise ratio
condition  so that any function F (x) fulﬁlling that condition sufﬁces. In this light  we ﬁnd that F (x)
behaves more like a proxy that guides the algorithm to a given target. Remarkably  our analysis also
encompasses the situation considered in [30  14].

Second  though it is not being made explicitly  one should think of F (x) as containing the mea-
surements or the training data. Consider  for example  recovering ¯x from y = A¯x where A is a
design matrix and y is the response (both are known). A natural way would be running the PHT(r)
algorithm with F (x) = ky − Axk2. One may also think of the logistic regression model where y
is a binary vector (label)  A is a collection of training data (feature)  and F (x) is the logistic loss
evaluated on the training samples.

With the above clariﬁcation  we are ready to make assumptions on F (x).
It turns out that two
properties of F (x) are vital for our analysis: restricted strong convexity and restricted smoothness.
These two conditions were proposed by [16] and have been standard in the literature [34  1  14  22].

1Our results actually hold for |T | ≤ k. But we observe that the size of T we will consider is usually equal

to k. Hence  for ease of exposition  we take |T | = k. This is also the case considered in [12].

3

Deﬁnition 1. We say that a differentiable function F (x) satisﬁes the property of restricted strong
convexity (RSC) at sparsity level s with parameter ρ−s > 0 if for all x  x′ ∈ Rd with kx − x′k0 ≤ s 

F (x) − F (x′) − h∇F (x′)  x − x′i ≥

ρ−s
2 kx − x′k2 .

Likewise  we say that F (x) satisﬁes the property of restricted smoothness (RSS) at sparsity level s
with parameter ρ+

s > 0 if for all x  x′ ∈ Rd with kx − x′k0 ≤ s  it holds that
ρ+
2 kx − x′k2 .
s

F (x) − F (x′) − h∇F (x′)  x − x′i ≤

We call κs = ρ+
condition number of the Hessian matrix of F (x) restricted on s-sparse directions.

s /ρ−s as the condition number of the problem  since it is essentially identical to the

2.1 Deterministic Analysis

The following proposition shows that under very mild conditions  PHT(r) either terminates or re-
covers the support of an arbitrary s-sparse signal ¯x using at most O(sκ2s log κ2s) iterations.
Proposition 2. Consider the PHT(r) algorithm with k = s. Suppose that F (x) is ρ−2s-RSC and ρ+
2s-
RSS  and the step size η ∈ (0  1/ρ+
2s/ρ−2s. Then PHT(r) either terminates or recovers
the support of ¯x within O(sκ log κ) iterations provided that ¯xmin ≥ 4√2+2√κ

2s). Let κ := ρ+

k∇2sF (¯x)k.

ρ−
2s

A few remarks are in order. First  we remind the reader that under the conditions stated above  it is
not guaranteed that PHT(r) succeeds. We say that PHT(r) fails if it terminates at some time stamp t
but St 6= S. This indeed happens if  for example  we feed it with a bad initial point and pick a
very small step size. In particular  if x0
  then the algorithm makes no progress.
The crux to remedy this issue is imposing a lower bound on η or looking at more coordinates in
each iteration  which is the theme below. However  the proposition is still useful because it asserts
that as far as we make sure that PHT(r) runs long enough (i.e.  O(sκ log κ) iterations)  it recovers
the support of an arbitrary sparse signal. We also note that neither the RIP condition nor a relaxed
sparsity is assumed in this proposition.

min > η(cid:13)(cid:13)∇F (x0)(cid:13)(cid:13)∞

The ¯xmin-condition above is natural  which can be viewed as a generalization of the well-known
signal-to-noise ratio (SNR) condition. This follows by considering the noisy compressed sensing

problem  where y = A¯x + e and F (x) = ky − Axk2. Here  the vector e is some noise. Now the
RSC and RSS imply for any 2s-sparse x

Hence

ρ−2s kxk2 ≤ kAxk2 ≤ ρ+

2s kxk2 .

In fact  the ¯xmin-condition has been used many times to establish support recovery. See  for exam-
ple  [31  4  23].

k∇2sF (¯x)k =(cid:13)(cid:13)(cid:13)

(A⊤e)2s(cid:13)(cid:13)(cid:13)

= Θ(kek)

In the following  we strengthen Prop. 2 by considering the RIP condition which requires a well-
bounded condition number  i.e.  κ ≤ O(1).
Theorem 3. Consider the PHT(r) algorithm with k = s. Suppose that F (x) is ρ−2s+r-RSC and
2s+r/ρ−2s+r be the condition number which is smaller than 1 + 1/(√2 + ν)
2s+r-RSS. Let κ := ρ+
ρ+
where ν = √s − r + 2. Pick the step size η = η′/ρ+
< η′ ≤ 1. Then

2s+r such that κ − 1√2+ν

PHT(r) recovers the support of ¯x within

tmax = log κ

log(1/β)

+

log(√2/(1 − λ))

log(1/β)

+ 2!k¯xk0

iterations  provided that for some constant λ ∈ (0  1)

¯xmin ≥
Above  β = (√2 + ν)(κ − η′) ∈ (0  1).

2ν + 6
λρ−2s+r k∇s+rF (¯x)k .

4

We remark several aspects of the theorem. The most important part is that Theorem 3 offers the
theoretical justiﬁcation that PHT(r) always recovers the support. This is achieved by imposing an
RIP condition (i.e.  bounding the condition number from the above) and using a proper step size.

We also make the iteration bound explicit  in order to examine the parameter dependency. First  we
note that tmax scales approximately linearly with λ. This conforms the intuition because a small λ
actually indicates a large signal-to-noise ratio  and hence easy to distinguish the support of interest
from the noise. The freedom parameter r is mainly encoded in the coefﬁcient β through the quantity
ν. Observe that when increasing the scalar r  we have a small β  and hence fewer iterations. This
is not surprising since a large value of r grants the algorithm more freedom to look at the current
iterate. Indeed  in the best case  PHT(s) is able to recover the support in O(1) iterations while
PHT(1) has to take O(s) steps. However  if we investigate the conditions  we ﬁnd that we need a
stronger RSC/RSS condition to afford a large freedom parameter.

It is also interesting to contrast Theorem 3 to [31  4]  which independently built state-of-the-art sup-
port recovery results for HTP. As has been mentioned  [31] made use of the optimality of the target
signal  which is a restricted setting compared to our result. Their iteration bound (see Theorem 1
therein)  though provides an appealing insight  does not have a clear parameter dependence on the
natural parameters of the problem (e.g.  sparsity and condition number). [4] developed O(k¯xk0)
iteration complexity for compressed sensing. Again  they conﬁned to a special signal whereas we
carry out a generalization that allows us to analyze a family of algorithms.

Though the RIP condition has been ubiquitous in the literature  many researchers point out that it
is not realistic in practical applications [18  20  21]. This is true for large-scale machine learning
problems  where the condition number may grow with the sample size (hence one cannot upper
bound it with a constant). A clever solution was ﬁrst (to our knowledge) suggested by [14]  where
they showed that using the sparsity parameter k = O(κ2s) guarantees convergence of projected
gradient descent. The idea was recently employed by [22  31] to show an RIP-free condition for
sparse recovery  though in a technically different way. The following theorem borrows this elegant
idea to prove RIP-free results for PHT(r).
Theorem 4. Consider the PHT(r) algorithm. Suppose that F (x) is ρ−2k-RSC and ρ+
κ := ρ+
η ∈ (0  1/ρ+

2k/ρ−2k be the condition number. Further pick k ≥ s +(cid:16)1 +

2k). Then the support of ¯x is included in the iterate of PHT(r) within

4

2k)2(cid:17) min{s  r} where

2k-RSS. Let

η2(ρ−

tmax =(cid:18) 3 log κ

log(1/µ)

+

2 log(2/(1 − λ))

log(1/µ)

+ 2(cid:19)k¯xk0

iterations  provided that for some constant λ ∈ (0  1) 

¯xmin ≥

Above  we have µ = 1 − ηρ−

2k (1−ηρ+
2k)

.

2

√κ + 3
λρ−2k k∇k+sF (¯x)k .

We discuss the salient features of Theorem 4 compared to Prop. 2 and Theorem 3. First  note that
we can pick η = 1/(2ρ+
2k) in the above theorem  which results in µ = O(1 − 1/κ). So the itera-
tion complexity is essentially given by O(sκ log κ) that is similar to the one in Prop. 2. However 
in Theorem 4  the sparsity parameter k is set to be O(s + κ2 min{s  r}) which guarantees support
inclusion. We pose an open question of whether the ¯xmin-condition might be reﬁned  in that it
currently scales with √κ which is stringent for ill-conditioned problems. Another important conse-
quence implied by the theorem is that the sparsity parameter k actually depends on the minimum of
s and r. Consider r = 1 which corresponds to the OMPR algorithm. Then k = O(s + κ2) sufﬁces.
In contrast  previous work of [14  31  22  23] only obtained theoretical result for k = O(κ2s)  owing
to a restricted problem setting. We also note that even in the original OMPR paper [12] and its latest
version [13]  such an RIP-free condition was not established.

2.2 Statistical Results

Until now  all of our theoretical results are phrased in terms of deterministic conditions (i.e.  RSC 
RSS and ¯xmin). It is known that these conditions can be satisﬁed by prevalent statistical models

5

such as linear regression and logistic regression. Here  we give detailed statistical results for sparse
linear regression  and we refer the reader to [1  14  22  23] for other applications.

Consider the sparse linear regression model

yi = hai  ¯xi + ei 

1 ≤ i ≤ N 

where ai are drawn i.i.d. from a sub-gaussian distribution with zero mean and covariance Σ ∈ Rd×d
and ei are drawn i.i.d. from N (0  ω2). We presume that the diagonal elements of Σ are properly
scaled  i.e.  Σjj ≤ 1 for 1 ≤ j ≤ d. Let A = (a⊤1 ; . . . ; a⊤N ) and y = (y1; . . . ; yN ). Our goal is to
2 ky − Axk2. Let
recover ¯x from the knowledge of A and y. To this end  we may choose F (x) = 1
σmin(Σ) and σmax(Σ) be the smallest and the largest singulars of Σ  respectively. Then it is known
that with high probability  F (x) satisﬁes the RSC and RSS properties at the sparsity level K with
parameters

ρ−K = σmin(Σ) − C1 ·

K log d

N

 

ρ+
K = σmax(Σ) + C2 ·

K log d

N

 

respectively. It is also known that with high probability  the following holds:

(3)

(4)

k∇KF (¯x)k ≤ 2ωr K log d

N

.

See [1] for a detailed discussion. With these probabilistic arguments on hand  we investigate the
sufﬁcient conditions under which the preceding deterministic results hold.

For Prop. 2  recall that the sparsity level of RSC and RSS is 2s. Hence  if we pick the sample size
N = q · 2C1s log d/σmin(Σ) for some q > 1  then

4√2 + 2√κ2s

ρ−2s

k∇2sF (¯x)k ≤ 4ω

2√2 +q σmax(Σ)
σmin(Σ) ·q 1+C2/qC1
(1 − 1/q)pqC1σmin(Σ)

1−1/q

.

The right-hand side is monotonically decreasing with q  which indicates that as soon as we pick
q large enough  it becomes smaller than ¯xmin. To be more concrete  consider that the covariance
matrix Σ is the identity matrix for which σmin(Σ) = σmax(Σ) = 1. Now suppose that q ≥ 2 
which gives an upper bound

Thus  in order to fulﬁll the ¯xmin-condition in Prop. 2  it sufﬁces to pick

4√2 + 2√κ2s

ρ−2s

k∇2sF (¯x)k ≤

.

√qC1

8ω(2√2 +p2 + C2/C1)
!2).

q = max(2  8ω(2√2 +p2 + C2/C1)

√C1 ¯xmin

For Theorem 3  it essentially asks for a well-conditioned design matrix at the sparsity level 2s + r.
Note that (3) implies κ2s+r ≥ σmax(Σ)/σmin(Σ)  which in return requires a well-conditioned
covariance matrix. Thus  to guarantee that κ2s+r ≤ 1 + ǫ for some ǫ > 0  it sufﬁces to choose Σ
such that σmax(Σ)/σmin(Σ) < 1 + ǫ and pick N = q · C1(2s + r) log d/σmin(Σ) with

q =

1 + ǫ + C−1

1 C2σmax(Σ)/σmin(Σ)

1 + ǫ − σmax(Σ)/σmin(Σ)

.

2k)  which results in k ≥ s+ (16κ2

Finally  Theorem 4 asserts support inclusion by expanding the support size of the iterates. Suppose
that η = 1/(2ρ+
2k + 1) min{r  s}. Given that the condition number
κ2k is always greater than 1  we can pick k ≥ s + 20κ2
2k min{r  s}. At a ﬁrst sight  this seems to
be weird in that k depends on the condition number κ2k which itself relies on the choice of k. In the
following  we present concrete sample complexity showing that this condition can be met. We will
focus on two extreme cases: r = 1 and r = s.
For r = 1  we require k ≥ s + 20κ2
obtain ρ−2k = 1
of the design matrix κ2k ≤ (2 + C2

2k. Let us pick N = 4C1k log d/σmin(Σ). In this way  we
)σmax(Σ). It then follows that the condition number

)σmax(Σ)/σmin(Σ). Consequently  we can set the parameter

2k ≤ (1 + C2

2 σmin(Σ) and ρ+

2C1

C1

k = s + 20(cid:18)(cid:18)2 +

C2

σmin(Σ)(cid:19)2
C1(cid:19) σmax(Σ)

.

6

(cid:13)(cid:13)xt − ¯x(cid:13)(cid:13) ≤ α · βt(cid:13)(cid:13)x0 − ¯x(cid:13)(cid:13) + ψ1 
(cid:13)(cid:13)xt − ¯x(cid:13)(cid:13) ≤ γ(cid:13)(cid:13)¯xS t(cid:13)(cid:13) + ψ2 
√2|¯xp+q| > αγ · β∆−1(cid:13)(cid:13)¯x{p+1 ... s}(cid:13)(cid:13) + Ψ 

Ψ = αψ2 + ψ1 +

1

ρ−2k k∇2F (¯x)k  

where

Note that the above quantities depend only on the covariance matrix. Again  if Σ is the identity
matrix  the sample complexity is O(s log d).
For r = s  likewise k ≥ 20κ2

2ks sufﬁces. Following the deduction above  we get

k = 20(cid:18)(cid:18)2 +

C2

σmin(Σ)(cid:19)2
C1(cid:19) σmax(Σ)

s.

3 Proof Sketch

We sketch the proof and list some useful lemmas which might be of independent interest. The
high-level proof technique follows from the recent work of [4] which performs an RIP analysis for
compressed sensing. But for our purpose  we have to deal with the freedom parameter r as well as
the RIP-free condition. We also need to generalize the arguments in [4] to show support recovery
results for arbitrary sparse signals. Indeed  we prove the following lemma which is crucial for our
analysis. Below we assume without loss of generality that the elements in ¯x are in descending order
according to the magnitude.
Lemma 5. Consider the PHT(r) algorithm. Assume that F (x) is ρ−2k-RSC and ρ+
assume that the sequence of {xt}t≥0 satisﬁes

2k-RSS. Further

(5)

(6)

for positive α  ψ1  γ  ψ2 and 0 < β < 1. Suppose that at the n-th iteration (n ≥ 0)  Sn contains the
indices of top p (in magnitude) elements of ¯x. Then  for any integer 1 ≤ q ≤ s − p  there exists an
integer ∆ ≥ 1 determined by

such that Sn+∆ contains the indices of top p + q elements of ¯x provided that Ψ ≤ √2λ¯xmin for
some constant λ ∈ (0  1).
We isolate this lemma here since we ﬁnd it inspiring and general. The lemma states that under
proper conditions  as far as one can show that the sequence satisﬁes (5) and (6)  then after a few
iterations  PHT(r) captures more correct indices in the iterate. Note that the condition (5) states that
the sequence should contract with a geometric rate  and the condition (6) follows immediately from
the fully corrective step (i.e.  minimizing F (x) over the new support set).

The next theorem concludes that under the conditions of Lemma 5  the total iteration complexity for
support recovery is proportional to the sparsity of the underlying signal.

Theorem 6. Assume same conditions as in Lemma 5. Then PHT(r) successfully identiﬁes the sup-

port of ¯x using(cid:16) log 2

2 log(1/β) + log(αγ/(1−λ))

log(1/β) + 2(cid:17)k¯xk0 number of iterations.

The detailed proofs of these two results are given in the appendix. Armed with them  it remains to
show that PHT(r) satisﬁes the condition (5) under different settings.

Proof Sketch for Prop. 2. We start with comparing F (zt
S t ) and F (xt−1). For the sake  we record
several important properties. First  due to the fully corrective step  the support set of ∇F (xt−1) is
orthogonal to St−1. That means for any subset Ω ⊂ St−1  zt
Ω and for any set Ω ⊂ St−1 
zt
Ω = −η∇ΩF (xt−1). We also note that due to the PHT operator  any element of zt
S t\S t−1 is not
smaller than that of zt

S t−1\S t . These critical facts together with the RSS condition result in

Ω = xt−1

F (xt) − F (xt−1) ≤ F (zt

S t ) − F (xt−1) ≤ −η(1 − ηρ+

2s)(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S t\S t−1(cid:13)(cid:13)(cid:13)

2

.

7

Since St \ St−1 consists of the top elements of ∇F (xt−1)  we can show that

(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S t\S t−1(cid:13)(cid:13)(cid:13)

2

≥

2ρ−2s(cid:12)(cid:12)St \ St−1(cid:12)(cid:12)
|St \ St−1| + |S \ St−1|(cid:0)F (xt−1) − F (¯x)(cid:1) .

Using the argument of Prop. 21  we establish the linear convergence of the iterates  i.e.  the condi-
tion (5). The result then follows.

Proof Sketch for Theorem 3. To prove this theorem  we present a more careful analysis on the prob-

lem structure. In particular  let T = supp(cid:0)∇F (xt−1  r)(cid:1)  J t = St−1∪T   and consider the elements
of ∇F (xt−1). Since T contains the largest elements  any element outside T is smaller than those
of T . Then we may compare the elements of ∇F (xt−1) on S \ T and S \ T . Though they have

different number of components  we can show the relationship between the averaged energy:

1

|T \ S|(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)T\S(cid:13)(cid:13)(cid:13)

2

≥

1

|S \ T|(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S\T(cid:13)(cid:13)(cid:13)

2

.

Using this equation followed by some standard relaxation  we can bound (cid:13)(cid:13)¯xJ t(cid:13)(cid:13) in terms of
(cid:13)(cid:13)xt−1 − ¯x(cid:13)(cid:13) as follows.
Lemma 7. Assume that F (x) satisﬁes the properties of RSC and RSS at sparsity level k + s + r.
Let ρ− := ρ−k+s+r and ρ+ := ρ+
k+s+r. Consider the support set J t = St−1 ∪ supp(cid:0)∇F (xt−1)  r(cid:1).
We have for any 0 < θ ≤ 1/ρ+ 

(cid:13)(cid:13)¯xJ t(cid:13)(cid:13) ≤ ν(1 − θρ−)(cid:13)(cid:13)xt−1 − ¯x(cid:13)(cid:13) +
where ν = √s − r + 2. In particular  picking θ = 1/ρ+ gives
(cid:13)(cid:13)¯xJ t(cid:13)(cid:13) ≤ ν(cid:18)1 −
κ(cid:19)(cid:13)(cid:13)xt−1 − ¯x(cid:13)(cid:13) +

1

ν
ρ− k∇s+rF (¯x)k  

ν
ρ− k∇s+rF (¯x)k .

Note that the lemma also applies to the two-stage thresholding algorithms (e.g.  CoSaMP [15])
whose ﬁrst step is expanding the support set.

On the other hand  we also know that

2

(cid:13)(cid:13)(cid:13)

zt

J t\S t(cid:13)(cid:13)(cid:13) ≤(cid:13)(cid:13)(cid:13)

zt

J t\S(cid:13)(cid:13)(cid:13)

≤ −

Then we prove the claim

1 − ηρ+

2k

2η

1 − ηρ+

2k

2η

F (xt) − F (xt−1) ≤ −

Proof Sketch for Theorem 4. The proof idea of Theorem 4 is inspired by [31]  though we give a

.
This is because J t \ St contains the r smallest elements of zt
J t . It then follows that(cid:13)(cid:13)¯xJ t\S t(cid:13)(cid:13) can be
upper bounded by(cid:13)(cid:13)xt−1 − ¯x(cid:13)(cid:13). Finally  we note that St = (J t \ St) ∪ J t. Hence  (5) follows.
tighter and a more general analysis. We ﬁrst observe that St \ St−1 contains larger elements than
St−1 \ St  due to PHT. This enables us to show that
(cid:13)(cid:13)zt
S t − xt−1(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S t\S t−1(cid:13)(cid:13)(cid:13)

To this end  we consider whether r is larger than s. If r ≥ s  then it is possible that(cid:12)(cid:12)St \ St−1(cid:12)(cid:12) ≥ s.
If(cid:12)(cid:12)St \ St−1(cid:12)(cid:12) < s ≤ r  then the above does not hold. But we may partition the set S \ St−1 as a
union of T1 = S \ (St ∪ St−1) and T2 = (St \ St−1) ∩ S  and show that the ℓ2-norm of F (xt−1)
on T1 is smaller than that on T2 if k = s + κ2s. In addition  the RSC condition gives
ρ−2k
ρ−2k (cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S t\S t−1(cid:13)(cid:13)(cid:13)
4 (cid:13)(cid:13)¯x − xt−1(cid:13)(cid:13)
Since T2 ⊂ St \ St−1  it implies the desired bound by rearranging the terms.
The case r < s follows in a reminiscent way. The proof is complete.

≥(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S\S t−1(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S t\S t−1(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)S t\S t−1(cid:13)(cid:13)(cid:13)

ρ−2k (cid:13)(cid:13)(cid:13)(cid:0)∇F (xt−1)(cid:1)T1(cid:13)(cid:13)(cid:13)

≥ ρ−2k(cid:0)F (xt−1) − F (¯x)(cid:1) .

≥ ρ−2k(cid:0)F (xt−1) − F (¯x)(cid:1) .

In this case  using the RSC condition and the PHT property  we can show that

≤ F (¯x) − F (xt−1) +

2

+

1

2

.

1

2

.

2

2

2

2

8

4 Simulation

We complement our theoretical results by performing numerical experiments in this section.
In
particular  we are interested in two aspects: ﬁrst  the number of iterations required to identify the
support of an s-sparse signal; second  the tradeoff between the iteration number and percentage of
success resulted from different choices of the freedom parameter r.

We consider the compressed sensing model y = A¯x + 0.01e  where the dimension d = 200 and the
entries of A and e are i.i.d. normal variables. Given a sparsity level s  we ﬁrst uniformly choose the
support of ¯x  and assign values to the non-zeros with i.i.d. normals. There are two conﬁgurations:
the sparsity s and the sample size N . Given s and N   we independently generate 100 signals and
test PHT(r) on them. We say PHT(r) succeeds in a trial if it returns an iterate with correct support
within 10 thousands iterations. Otherwise we mark the trial as failure.
Iteration numbers to be
reported are counted only on those success trials. The step size η is ﬁxed to be the unit  though one
can tune it using cross-validation for better performance.

s
n
o

i
t

a
r
e

t
i

#

 

N = 200

r = 1
r = 2
r = 5
r = 10
r = 100

80
70
60
50
40
30
20
10
0
1 10 20 30 40 50 60 70 80 90100

 

#non−zeros

s
s
e
c
c
u
s
 
f

o

 

t

e
g
a
n
e
c
r
e
p

N = 200

 

r = 1
r = 2
r = 5
r = 10
r = 100

100

80

60

40

20

 

0
1 10 20 30 40 50 60 70 80 90100

#non−zeros

s
n
o

i
t

a
r
e

t
i

#

14
12
10
8
6
4
2
1

s = 10

r = 1

r = 2

r = 5
r = 100

50

100

150

#measurements

s
s
e
c
c
u
s
 
f

o

 

t

e
g
a
n
e
c
r
e
p

100

80

60

40

20

 

0
1

200

s = 10

 

r = 1
r = 2
r = 5
r = 10
r = 100

50
#measurements

100

150

200

Figure 1: Iteration number and success percentage against sparsity and sample size. The ﬁrst
panel shows that the iteration number grows linearly with the sparsity. The choice r = 5 sufﬁces to
guarantee a minimum iteration complexity. The second panel shows comparable statistical perfor-
mance for different choices of r. The third one illustrates how the iteration number changes with the
sample size and the last panel depicts phase transition.

To study how the iteration number scales with the sparsity in practice  we ﬁx N = 200 and tune s
from 1 to 100. We test different freedom parameter r on these signals. The results are shown in the
leftmost ﬁgure in Figure 1. As our theory predicted  we observe that within O(s) iterations  PHT(r)
precisely identiﬁes the true support. In the second subﬁgure  we plot the percentage of success
against the sparsity. It appears that PHT(r) lays on top of each other. This is possibly because we
used a sufﬁciently large sample size.

Next  we ﬁx s = 10 and vary N from 1 to 200. Surprisingly  from the rightmost ﬁgure  we do
not observe performance degrade using a large freedom parameter. So we conjecture that the ¯xmin-
condition we established can be reﬁned.

Figure 1 also illustrates an interesting phenomenon: after a particular threshold  say r = 5  PHT(r)
does not signiﬁcantly reduces the iteration number by increasing r. This cannot be explained by our
theorems in the paper. We leave it as a promising research direction.

5 Conclusion and Future Work

In this paper  we have presented a principled analysis on a family of hard thresholding algorithms.
To facilitate our analysis  we appealed to the recently proposed partial hard thresholding operator.
We have shown that under the RIP condition or the relaxed sparsity condition  the PHT(r) algorithm
recovers the support of an arbitrary sparse signal ¯x within O(k¯xk0 κ log κ) iterations  provided that
a generalized signal-to-noise ratio condition is satisﬁed. On account of our uniﬁed analysis  we have
established the best known bound for HTP and OMPR. We have also illustrated that the simulation
results agree with our ﬁnding that the iteration number is proportional to the sparsity.

There are several interesting future directions. First  it would be interesting to examine if we can
close the logarithmic factor log κ in the iteration bound. Second  it is also useful to study RIP-free
conditions for two-stage PHT algorithms such as CoSaMP. Finally  we pose the open question of
whether one can improve the √κ factor in the ¯xmin-condition.

Acknowledgements.
1360971. We thank the anonymous reviewers for valuable comments.

The work is supported in part by NSF-Bigdata-1419210 and NSF-III-

9

References

[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Fast global convergence of gradient methods

for high-dimensional statistical recovery. The Annals of Statistics  40(5):2452–2482  2012.

[2] S. Bahmani  B. Raj  and P. T. Boufounos. Greedy sparsity-constrained optimization. Journal

of Machine Learning Research  14(1):807–841  2013.

[3] T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. Applied

and Computational Harmonic Analysis  27(3):265–274  2009.

[4] J.-L. Bouchot  S. Foucart  and P. Hitczenko. Hard thresholding pursuit algorithms: number of

iterations. Applied and Computational Harmonic Analysis  41(2):412–435  2016.

[5] T. T. Cai and L. Wang. Orthogonal matching pursuit for sparse signal recovery with noise.

IEEE Trans. Information Theory  57(7):4680–4688  2011.

[6] E. J. Candès and T. Tao. Decoding by linear programming. IEEE Trans. Information Theory 

51(12):4203–4215  2005.

[7] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

Journal on Scientiﬁc Computing  20(1):33–61  1998.

[8] W. Dai and O. Milenkovic. Subspace pursuit for compressive sensing signal reconstruction.

IEEE Trans. Information Theory  55(5):2230–2249  2009.

[9] I. Daubechies  M. Defrise  and C. D. Mol. An iterative thresholding algorithm for linear in-
verse problems with a sparsity constraint. Communications on Pure and Applied Mathematics 
57(11):1413–1457  2004.

[10] S. Foucart. Hard thresholding pursuit: An algorithm for compressive sensing. SIAM Journal

on Numerical Analysis  49(6):2543–2563  2011.

[11] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Applied and

Numerical Harmonic Analysis. Birkhäuser  2013.

[12] P. Jain  A. Tewari  and I. S. Dhillon. Orthogonal matching pursuit with replacement.

In
Proceedings of the 25th Annual Conference on Neural Information Processing Systems  pages
1215–1223  2011.

[13] P. Jain  A. Tewari  and I. S. Dhillon. Partial hard thresholding. IEEE Trans. Information Theory 

63(5):3029–3038  2017.

[14] P. Jain  A. Tewari  and P. Kar. On iterative hard thresholding methods for high-dimensional M-
estimation. In Proceedings of the 28th Annual Conference on Neural Information Processing
Systems  pages 685–693  2014.

[15] D. Needell and J. A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate

samples. Applied and Computational Harmonic Analysis  26(3):301–321  2009.

[16] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-
dimensional analysis of M -estimators with decomposable regularizers. In Proceedings of the
23rd Annual Conference on Neural Information Processing Systems  pages 1348–1356  2009.

[17] S. Osher  F. Ruan  J. Xiong  Y. Yao  and W. Yin. Sparse recovery via differential inclusions.

Applied and Computational Harmonic Analysis  41(2):436–469  2016.

[18] Y. R. Peter J. Bickel and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig selector.

The Annals of Statistics  pages 1705–1732  2009.

[19] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A

convex programming approach. IEEE Trans. Information Theory  59(1):482–494  2013.

[20] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted eigenvalue properties for correlated

gaussian designs. Journal of Machine Learning Research  11:2241–2259  2010.

10

[21] M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements.

IEEE

Trans. Information Theory  59(6):3434–3447  2013.

[22] J. Shen and P. Li. A tight bound of hard thresholding. arXiv preprint arXiv:1605.01656  2016.

[23] J. Shen and P. Li. On the iteration complexity of support recovery via hard thresholding pursuit.
In Proceedings of the 34th International Conference on Machine Learning  pages 3115–3124 
2017.

[24] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society: Series B (Methodological)  58(1):267–288  1996.

[25] J. A. Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Trans. Infor-

mation Theory  50(10):2231–2242  2004.

[26] J. A. Tropp and S. J. Wright. Computational methods for sparse solution of linear inverse

problems. Proceedings of the IEEE  98(6):948–958  2010.

[27] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
ℓ1-constrained quadratic programming (Lasso). IEEE Trans. Information Theory  55(5):2183–
2202  2009.

[28] J. Wang  S. Kwon  P. Li  and B. Shim. Recovery of sparse signals via generalized orthogonal

matching pursuit: A new analysis. IEEE Trans. Signal Processing  64(4):1076–1089  2016.

[29] M. Yuan and Y. Lin. On the non-negative garrotte estimator. Journal of the Royal Statistical

Society: Series B (Statistical Methodology)  69(2):143–161  2007.

[30] X.-T. Yuan  P. Li  and T. Zhang. Gradient hard thresholding pursuit for sparsity-constrained
optimization. In Proceedings of the 31st International Conference on Machine Learning  pages
127–135  2014.

[31] X.-T. Yuan  P. Li  and T. Zhang. Exact recovery of hard thresholding pursuit. In Proceedings
of the 30th Annual Conference on Neural Information Processing Systems  pages 3558–3566 
2016.

[32] T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal

of Machine Learning Research  10:555–568  2009.

[33] T. Zhang. Some sharp performance bounds for least squares regression with L1 regularization.

The Annals of Statistics  37(5A):2109–2144  2009.

[34] T. Zhang. Sparse recovery with orthogonal matching pursuit under RIP. IEEE Trans. Informa-

tion Theory  57(9):6215–6221  2011.

[35] P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning

Research  7:2541–2563  2006.

11

,Harikrishna Narasimhan
David Parkes
Yaron Singer
Jie Shen
Ping Li