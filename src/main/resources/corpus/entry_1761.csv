2019,Efficient Symmetric Norm Regression via Linear Sketching,We provide efficient algorithms for overconstrained linear regression problems with size $n \times d$ when the loss function is a symmetric norm (a norm invariant under sign-flips and coordinate-permutations). An important class of symmetric norms are Orlicz norms  where for a function  $G$ and a vector $y \in \mathbb{R}^n$  the corresponding Orlicz norm $\|y\|_G$ is defined as the unique value $\alpha$ such that $\sum_{i=1}^n G(|y_i|/\alpha) = 1$. When the loss function is an Orlicz norm  our algorithm produces a $(1 + \varepsilon)$-approximate solution for an arbitrarily small constant $\varepsilon > 0$ in input-sparsity time  improving over the previously best-known algorithm which produces a $d \cdot \polylog n$-approximate solution. When the loss function is a general symmetric norm  our algorithm produces a $\sqrt{d} \cdot \polylog n \cdot \mathrm{mmc}(\ell)$-approximate solution in input-sparsity time  where $\mathrm{mmc}(\ell)$ is a quantity related to the symmetric norm under consideration. To the best of our knowledge  this is the first input-sparsity time algorithm with provable guarantees for the general class of symmetric norm regression problem. Our results shed light on resolving the universal sketching problem for linear regression  and the techniques might be of independent interest to numerical linear algebra problems more broadly.,Efﬁcient Symmetric Norm Regression via Linear

Sketching⇤

Zhao Song

University of Washington

magic.linuxkde@gmail.com

Ruosong Wang

Carnegie Mellon University
ruosongw@andrew.cmu.edu

University of California  Los Angeles

Toyota Technological Institute at Chicago

Lin F. Yang

linyang@ee.ucla.edu

Hongyang Zhang

hongyanz@ttic.edu

Peilin Zhong

Columbia University

pz2225@columbia.edu

Abstract

We provide efﬁcient algorithms for overconstrained linear regression problems
with size n⇥ d when the loss function is a symmetric norm (a norm invariant under
sign-ﬂips and coordinate-permutations). An important class of symmetric norms
are Orlicz norms  where for a function G and a vector y 2 Rn  the corresponding
Orlicz norm kykG is deﬁned as the unique value ↵ such thatPn
i=1 G(|yi|/↵) = 1.
When the loss function is an Orlicz norm  our algorithm produces a (1 + ")-
approximate solution for an arbitrarily small constant "> 0 in input-sparsity
time  improving over the previously best-known algorithm which produces a
d · polylog n-approximate solution. When the loss function is a general symmetric
norm  our algorithm produces a pd · polylog n · mmc(`)-approximate solution
in input-sparsity time  where mmc(`) is a quantity related to the symmetric norm
under consideration. To the best of our knowledge  this is the ﬁrst input-sparsity
time algorithm with provable guarantees for the general class of symmetric norm
regression problem. Our results shed light on resolving the universal sketching
problem for linear regression  and the techniques might be of independent interest
to numerical linear algebra problems more broadly.

Introduction

1
Linear regression is a fundamental problem in machine learning. For a data matrix A 2 Rn⇥d and a
response vector b 2 Rn with n  d  the overconstrained linear regression problem can be formulated
as solving the following optimization problem:
(1)

min

x2Rd L(Ax  b) 

where L : Rn ! R is a loss function. Via the technique of linear sketching  we have witnessed many
remarkable speedups for linear regression for a wide range of loss functions. Such technique involves
designing a sketching matrix S 2 Rr⇥n  and showing that by solving a linear regression instance on
the data matrix SA and the response vector Sb  which is usually much smaller in size  one can obtain

⇤All authors contribute equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: M-estimators

HUBER ⇢ x2/2

`1  `2
“FAIR"

c(|x| c/2)

|x| c
|x| > c
2(p1 + x2/2  1)
c2 (|x|/c  log(1 + |x|/c))

an approximate solution to the original linear regression instance in (1). Sarlós showed in [29] that by
taking S as a Fast Johnson-Lindenstrauss Transform matrix [1]  one can obtain (1 + ")-approximate
solutions to the least square regression problem (L(y) = kyk2
2) in O(nd log n + poly(d/")) time.
The running time was later improved to O(nnz(A) + poly(d/")) [12  26  28  23  15]. Here nnz(A)
is the number of non-zero entries in the data matrix A  which could be much smaller than nd for
sparse data matrices. This technique was later generalized to other loss functions. By now  we
p) [18  26  35  16  32]  the

have eO(nnz(A) + poly(d/")) time algorithms for `p norms (L(y) = kykp

quantile loss function [36]  M-estimators [14  13] and the Tukey loss function [11].
Despite we have successfully applied the technique of linear sketching to many different loss functions 
ideally  it would be more desirable to design algorithms that work for a wide range of loss functions 
instead of designing a new sketching algorithm for every speciﬁc loss function. Naturally  this leads
to the following problem  which is the linear regression version of the universal sketching problem2
studied in streaming algorithms [10  9]. We note that similar problems are also asked and studied for
various algorithmic tasks  including principal component analysis [31]  sampling [21]  approximate
nearest neighbor search [4  3]  discrepancy [17  8]  sparse recovery [27] and mean estimation with
statistical queries [19  22].
Problem 1. Is it possible to design sketching algorithms for linear regression  that work for a wide
range of loss functions?

Prior to our work  [14  13] studied this problem in terms of M-estimators  where the loss function
employs the form L(y) =Pn
i=1 G(yi) for some function G. See Table 1 for a list of M-estimators.
However  much less is known for the case where the loss function L(·) is a norm  except for `p norms.
Recently  Andoni et al. [2] tackle Problem 1 for Orlicz norms  which can be seen as a scale-invariant
version of M-estimators. For a function G and a vector y 2 Rn with y 6= 0  the corresponding Orlicz
norm kykG is deﬁned as the unique value ↵ such that

nXi=1

G(|yi|/↵) = 1.

(2)

When y = 0  we deﬁne kykG to be 0. Note that Orlicz norms include `p norms as special cases  by
taking G(z) = |z|p for some p  1. Under certain assumptions on the function G  [2] obtains the
ﬁrst input-sparsity time algorithm for solving Orlicz norm regression. More precisely  in eO(nnz(A) +
poly(d log n)) time  their algorithm obtains a solutionbx 2 Rd such that kAbx bkG  d· polylog n·
minx2Rd kAx  bkG.
There are two natural problems left open by the work of [2]. First  the algorithm in [2] has approxi-
mation ratio as large as d · polylog n. Although this result is interesting from a theoretical point of
view  such a large approximation ratio is prohibitive for machine learning applications in practice.
Is it possible to obtain an algorithm that runs in eO(nnz(A) + poly(d/")) time  with approximation
ratio 1 + "  for arbitrarily small "  similar to the case of `p norms? Moreover  although Orlicz norm
includes a wide range of norms  many other important norms  e.g.  top-k norms (the sum of absolute
values of the leading k coordinates of a vector)  max-mix of `p norms (e.g. max{kxk2  ckxk1} for
some c > 0)  and sum-mix of `p norms (e.g. kxk2 + ckxk1 for some c > 0)  are not Orlicz norms.
More complicated examples include the k-support norm [5] and the box-norm [25]  which have found
applications in sparse recovery. In light of Problem 1  it is natural to ask whether it is possible to apply
the technique of linear sketching to a broader class of norms. In this paper  we obtain afﬁrmative
answers to both problems  and make progress towards ﬁnally resolving Problem 1.

denote its i-th row  viewed as a column vector. For n real numbers x1  x2  . . .   xn  we deﬁne

Notations. We use eO(f ) to denote f polylog f. For a matrix A 2 Rn⇥d  we use Ai 2 Rd to

2https://sublinear.info/index.php?title=Open_Problems:30.

2

diag(x1  x2  . . .   xn) 2 Rn⇥n to be the diagonal matrix where the i-th diagonal entry is xi. For a
vector x 2 Rn and p  1  we use kxkp to denote its `p norm  and kxk0 to denote its `0 norm  i.e. 
the number of non-zero entries in x. For two vectors x  y 2 Rn  we use hx  yi to denote their inner
product. For any n > 0  we use [n] to denote the set {1  2  . . .   n}. For 0  p  1  we deﬁne Ber(p)
to be the Bernoulli distribution with parameter p. We use Sn1 to denote the unit `2 sphere in Rn 
i.e.  Sn1 = {x 2 Rn |k xk2 = 1}. We use R0 to denote the set of all non-negative real numbers 
i.e.  R0 = {x 2 R | x  0}.
1.1 Our Contributions
Algorithm for Orlicz Norms. Our ﬁrst contribution is a uniﬁed algorithm which produces (1 + ")-
approximate solutions to the linear regression problem in (1)  when the loss function L(·) is an Orlicz
norm. Before introducing our results  we ﬁrst give our assumptions on the function G which appeared
in (2).
Assumption 1. We assume the function G : R ! R0 satisﬁes the following properties:

1. G is a strictly increasing convex function on [0 1);
2. G(0) = 0  and for all x 2 R  G(x) = G(x);
3. There exists some CG > 0  such that for all 0 < x < y  G(y)/G(x)  CG(y/x)2.

The ﬁrst two conditions in Assumption 1 are necessary to make sure the corresponding Orlicz norm
k·k G is indeed a norm  and the third condition requires the function G to have at most quadratic
growth  which can be satisﬁed by all M-estimators in Table 1 and is also required by prior work [2].
Notice that our assumptions are weaker than those in [2]. In [2]  it is further required that G(x) is a
linear function when x > 1  and G is twice differentiable on an interval (0  G) for some G > 0.
Given our assumptions on G  our main theorem is summarized as follows.
Theorem 1. For a function G that satisﬁes Assumption 1  there exists an algorithm that  on any input

A 2 Rn⇥d and b 2 Rn  ﬁnds a vector x⇤ in time eO(nnz(A) + poly(d/"))  such that with probability
at least 0.9  kAx⇤  bkG  (1 + ") minx2Rd kAx  bkG.
To the best of our knowledge  this is the ﬁrst input-sparsity time algorithm with (1 + ")-approximation
guarantee  that goes beyond `p norms  the quantile loss function  and M-estimators. See Table 2 for
a more comprehensive comparison with previous results.
Algorithm for Symmetric Norms. We further study the case when the loss function L(·) is a
symmetric norm. Symmetric norm is a more general class of norms  which includes all norms that
are invariant under sign-ﬂips and coordinate-permutations. Formally  we deﬁne symmetric norms as
follow.
Deﬁnition 1. A norm k·k
k(s1y1  s2y2  . . .   snyn)k` for any permutation  and any assignment of si 2 {1  1}.
Symmetric norm includes `p norms and Orlicz norms as special cases. It also includes all examples
provided in the introduction  i.e.  top-k norms  max-mix of `p norms  sum-mix of `p norms  the
k-support norm [5] and the box-norm [25]  as special cases. Understanding this general set of loss
functions can be seen as a preliminary step to resolve Problem 1. Our main result for symmetric
norm regression is summarized in the following theorem.
Theorem 2. Given a symmetric norm k·k `  there exists an algorithm that  on any input A 2 Rn⇥d
and b 2 Rn  ﬁnds a vector x⇤ in time eO(nnz(A) + poly(d))  such that with probability at least 0.9 
kAx⇤  bk`  pd · polylog n · mmc(`) · minx2Rd kAx  bk`.
In the above theorem  mmc(`) is a characteristic of the symmetric norm k·k `  which has been proven
to be essential in streaming algorithms for symmetric norms [7]. See Deﬁnition 7 for the formal
deﬁnition of mmc(`)  and Section 3 for more details about mmc(`). In particular  for `p norms with
p  2  top-k norms with k  n/ polylog n  max-mix of `2 norm and `1 norm (max{kxk2  ckxk1} for
some c > 0)  sum-mix of `2 norm and `1 norm (kxk2 + ckxk1 for some c > 0)  the k-support norm 
and the box-norm  mmc(`) can all be upper bounded by polylog n  which implies our algorithm has
approximation ratio pd · polylog n for all these norms. This clearly demonstrates the generality of

if k(y1  y2  . . .   yn)k` =

is called a symmetric norm 

`

our algorithm.

3

Table 2: Comparison among input-sparsity time linear regression algorithms

Reference

Loss Function

Approximation Ratio

[18  26  35  16  32]

[36]

[14  13]

[2]

Theorem 1
Theorem 2

`p norms

Quantile loss function

M-estimators
Orlicz norms
Orlicz norms

Symmetric norms

1 + "
1 + "
1 + "

d · polylog n

1 + "

pd · polylog n · mmc(`)

Empirical Evaluation.
datasets. Our empirical results quite clearly demonstrate the practicality of our methods.

In Section E of the supplementary material  we test our algorithms on real

1.2 Technical Overview

Similar to previous works on using linear sketching to speed up solving linear regression  our core
technique is to provide efﬁcient dimensionality reduction methods for Orlicz norms and general
symmetric norms. In this section  we discuss the techniques behind our results.

Row Sampling Algorithm for Orlicz Norms. Compared to prior work on Orlicz norm regres-
sion [2] which is based on random projection3  our new algorithm is based on row sampling. For
a given matrix A 2 Rn⇥d  our goal is to output a sparse weight vector w 2 Rn with at most
poly(d log n/") non-zero entries  such that with high probability  for all x 2 Rd 
(3)
(1  ")kAx  bkG  kAx  bkG w  (1 + ")kAx  bkG.
Here  for a weight vector w 2 Rn and a vector y 2 Rn  the weighted Orlicz norm kykG w is deﬁned
as the unique value ↵ such thatPn
i=1 wiG(|yi|/↵) = 1. See Deﬁnition 4 for the formal deﬁnition of
weighted Orlicz norm. To obtain a (1 + ")-approximate solution to Orlicz norm regression  by (3)  it
sufﬁces to solve

min

x2Rd kAx  bkG w.

(4)

(5)

Since the vector w 2 Rn has at most poly(d log n/") non-zero entries  and we can ignore all rows
of A with zero weights  there are at most poly(d log n/") remaining rows in A in the optimization
problem in (4). Furthermore  as we show in Lemma 3  k·k G w is a seminorm  which implies
we can solve the optimization problem in (4) in poly(d log n/") time  by simply solving a convex
program with size poly(d log n/"). Thus  we focus on how to obtain the weight vector w 2 Rn in
the remaining part. Furthermore  by taking A to be a matrix whose ﬁrst d columns are A and last
column is b  to satisfy (3)  it sufﬁces to ﬁnd a weight vector w such that for all x 2 Rd+1 

(1  ")kAxkG  kAxkG w  (1 + ")kAxkG.

Hence  we ignore the response vector b in the remaining part of the discussion.
We obtain the weight vector w via importance sampling. We compute a set of sampling probabilities
i=1 for each row of the data matrix A  and sample the rows of A according to these probabilities.
{pi}n
The i-th entry of the weight vector w is then set to be wi = 1/pi with probability pi and wi = 0 with
probability 1  pi. However  unlike `p norms  Orlicz norms are not “entry-wise” norms  and it is not
even clear that such a sampling process gives an unbiased estimation. Our key insight here is that for
a vector Ax with unit Orlicz norm  if for all x 2 Rd 

(1  ")

G((Ax)i) 

wiG((Ax)i)  (1 + ")

G((Ax)i) 

(6)

nXi=1

nXi=1

nXi=1

then (5) holds  which follows from the convexity of the function G. See Lemma 7 and its proof for
i=1  such that the
more details. Therefore  it remains to develop a way to deﬁne and calculate {pi}n
total number of sampled rows is small.

3Even for `p norms with p < 2  embeddings based on random projections will necessarily induce a distortion

factor polynomial in d  as shown in [32].

4

i=1 ui = d.

2kxk2

2 = kUik2

2kU xk2

i=1 ui.

2. It is also clear thatPn

Our method for deﬁning and computing sampling probabilities pi is inspired by row sampling
algorithms for `p norms [18]. Here  the key is to obtain an upper bound on the contribution of
each entry to the summationPn
i=1 G((Ax)i). Indeed  suppose for some vector u 2 Rn such that
G(Ax)i  ui for all x 2 Rd with kAxkG = 1  we can then sample each row of A with sampling
probability proportional to ui. Now  by standard concentration inequalities and a net argument  (6)
holds with high probability. It remains to upper bound the total number of sampled rows  which is
proportional toPn
We use the case of `2 norm  i.e.  G(x) = x2  as an example to illustrate our main ideas for choosing
the vector u 2 Rn. Suppose U 2 Rn⇥d is an orthonormal basis matrix of the column space of A 
then the leverage score4 is deﬁned to be the squared `2 norm of each row of U. Indeed  leverage
score gives an upper bound on the contribution of each row to kU xk2
2  since by Cauchy-Schwarz
inequality  for each row Ui of U  we have hUi  xi2  kUik2
2  and thus we can
set ui = kUik2
For general Orlicz norms  leverage scores are no longer upper bounds on G((U x)i). Inspired by the
role of orthonormal bases in the case of `2 norm  we ﬁrst deﬁne well-conditioned basis for general
Orlicz norms as follow.
Deﬁnition 2. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1.
We say U 2 Rn⇥d is a well-conditioned basis with condition number G = G(U ) if for all x 2 Rd 
kxk2  kU xkG  Gkxk2.
Given this deﬁnition  when kU xkG = 1  by Cauchy-Schwarz inequality and monotonicity of G  we
can show that G((U x)i)  G(kUik2kxk2)  G(kUik2kU xkG)  G(kUik2). This also leads to our
deﬁnition of Orlicz norm leverage scores.
Deﬁnition 3. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1.
For a given matrix A 2 Rn⇥d and a well-conditioned basis U of the column space of A  the Orlicz
norm leverage score of the i-th row of A is deﬁned to be G(kUik2).
It remains to give an upper bound on the summation of Orlicz norm leverage scores of all rows. Unlike
the `2 norm  it is not immediately clear how to use the deﬁnition of well-conditioned basis to obtain
such an upper bound for general Orlicz norms. To achieve this goal  we use a novel probabilistic
argument. Suppose one takes x to be a vector with i.i.d. Gaussian random variables. Then each entry
of U x has the same distribution as kUik2 · gi  where {gi}n
i=1 is a set of standard Gaussian random
variables. Thus  with constant probability Pn
i=1 G((U x)i) is an upper bound on the summation
of Orlicz norm leverage scores. Furthermore  by the growth condition of the function G  we have
G. Now by Deﬁnition 2  kU xkG  Gkxk2  and kxk2  O(pd) with
Pn
constant probability by tail inequalities of Gaussian random variables. This implies an upper bound
on the summation of Orlicz norm leverage scores. See Lemma 4 and its proof for more details.
Our approach for constructing well-conditioned bases is inspired by [30]. In Lemma 5  we show that
given a subspace embedding ⇧ which embeds the column space of A with Orlicz norm k·k G into
the `2 space with distortion   then one can construct a well-conditioned basis with condition number
G  . The running time is dominated by calculating ⇧A and doing a QR-decomposition on ⇧A. To
this end  we can use the oblivious subspace embedding for Orlicz norms in Corollary 125 to construct
well-conditioned bases. The embedding in Corollary 12 has O(d) rows and  = poly(d log n)  and
calculating ⇧A can be done in eO(nnz(A) + poly(d)) time. Using such an embedding to construct
kwk0  poly(d log n/") in time eO(nnz(A) + poly(d)).
We would like to remark that our sampling algorithm still works if the third condition in Assumption 1
does not hold. In general  suppose the function G : R ! R satisﬁes that for all 0 < x < y 
G(y)/G(x)  CG(y/x)p  for the Orlicz norm induced by G  given a well-conditioned basis with
condition number G  our sampling algorithm returns a matrix with roughly O((pdG)p · d/"2)
rows such that Theorem 1 holds. One may use the Löwner–John ellipsoid as the well-conditioned

the well-conditioned basis  our row sampling algorithm produces a vector w that satisﬁes (6) with

i=1 G((U x)i)  CGkU xk2

4See  e.g.  [24]  for a survey on leverage scores.
5Alternatively  we can use the oblivious subspace embedding in [2] for this step. However  as we have
discussed  the oblivious subspace embedding in [2] requires stronger assumptions on the function G : R ! R0
than those in Assumption 1  which restricts the class of Orlicz norms to which our algorithm can be applied.

5

basis (as in [18]) which has condition number G = pd for any norm. However  calculating the
Löwner–John ellipsoid requires at least O(nd5) time. Moreover  our method described above fails
when p > 2 since it requires an oblivious subspace embedding with poly(d) distortion  and it is
known that such embedding does not exist when p > 2 [10]. Since we focus on input-sparsity time
algorithms in this paper  we only consider the case that p  2.
Finally  we would like to compare our sampling algorithm with that in [13]. First  the algorithm
in [13] works for M-estimators  while we focus on Orlicz norms. Second  our deﬁnitions for Orlicz
norm leverage score and well-conditioned basis  as given in Deﬁnition 2 and 3  are different from all
previous works and are closely related to the Orlicz norm under consideration. The algorithm in [13] 
on the other hand  simply uses `p leverage scores. Under our deﬁnition  we can prove that the sum of
leverage scores is bounded by O(CGd2
G) (Lemma 4)  whose proof requires a novel probabilistic
argument. In contrast  the upper bound on sum of leverage scores in [13] is O(pnd) (Lemma 38 in
[11]). Thus  the algorithm in [13] runs in an iterative manner since in each round the algorithm can
merely reduce the dimension from n to O(pnd)  while our algorithm is one-shot.

Oblivious Subspace Embeddings for Symmetric Norms. To obtain a faster algorithm for linear
regression when the loss function is a general symmetric norm  we show that there exists a distribution
over embedding matrices  such that if S is a random matrix drawn from that distribution  then for
any n ⇥ d matrix A  with constant probability  for all x 2 Rd  kAxk`  kSAxk2  poly(d log n) ·
mmc(`) ·k Axk`. Moreover  the embedding matrix S is sparse  and calculating SA requires only
eO(nnz(A) + poly(d)) time. Another favorable property of S is that it is an oblivious subspace
embeeding  meaning the distribution of S does not depend on A. To achieve this goal  it is sufﬁcient
to construct a random diagonal matrix D such that for any ﬁxed vector x 2 Rn 

and

Pr[kDxk2  ⌦(1/ poly(d log n)) ·k xk`]  1  exp(⌦(d log n)) 

Pr[kDxk2  poly(d log n) · mmc(`) ·k xk`]  1  O(1/d).

(7)

(8)

Our construction is inspired by the sub-sampling technique in [20]  which was used for sketching
symmetric norms in data streams [7]. Throughout the discussion  we use ⇠(q) 2 Rn to denote a vector
with q non-zero entries and each entry is 1/pq. Let us start with a special case where the vector
x 2 Rn has s non-zero entries and each non-zero entry is 1. It is easy to see kxk` = psk⇠(s)k`.
Now consider a random diagonal matrix D which corresponds to a sampling process  i.e.  each
diagonal entry is set to be 1 with probability p and 0 with probability 1  p. Our goal is to
show thatp1/pk⇠(1/p)k` ·k Dxk2 is a good estimator of kxk`. If p =⇥( d log n/s)  then with
probability at least 1  exp (⌦(d log n))  Dx will contain at least one non-zero entry from x  in
which case (7) is satisﬁed. However  we do not know s in advance. Thus  we use t = O(log n)
different matrices D1  D2  . . .   Dt  where Di has sampling probability 1/2i. Clearly at least one
such Dj can establish (7). For the upper bound part  if p is much smaller than 1/s  then Dx will
never contain a non-zero entry from x. Otherwise  in expectation Dx will contain ps non-zero
entries  in which case our estimation will be roughly psk⇠(1/p)k`  which can be upper bounded by
O(log n · mmc(`) · psk⇠(s)k`). At this point  (8) follows from Markov’s inequality. See Section C.5
for the formal argument  and Section 3 for a detailed discussion on mmc(`).
To generalize the above argument to general vectors  for a vector x 2 Rn  we conceptually partition
its entries into ⇥(log n) groups  where the i-th group contains entries with magnitude in [2i  2i+1).
By averaging  at least one group of entries contributes at least ⌦(1/ log n) fraction to the value of
kxk`. To establish (7)  we apply the lower bound part of the argument in the previous paragraph to
this “contributing” group. To establish (8)  we apply the upper bound part of the argument to all
groups  which will only induce an additional O(log n) factor in the approximation ratio  by triangle
inequality.
Since our oblivious subspace embedding embeds a given symmetric norm into the `2 space  in order
to obtain an approximate solution to symmetric norm regression  we only need to solve a least squares
regression instance with much smaller size. This is another advantage of our subspace embedding 
since the least square regression problem is a well-studied problem in optimization and numerical
linear algebra  for which many efﬁcient algorithms are known  both in theory and in practice.

6

2 Linear Regression for Orlicz Norms

G) 
i=1 G(kUik2)  O(CGd2

 ⇧A be a QR-decomposition of 1

In this section  we introduce our results for Orlicz norm regression. We ﬁrst give the deﬁnition of
weighted Orlicz norm.
Deﬁnition 4. For a function G that satisﬁes Assumption 1 and a weight vector w 2 Rn such that
wi  0 for all i 2 [n]  for a vector x 2 Rn  ifPn
i=1 wi · |xi| = 0  then the weighted Orlicz norm
kxkG w is deﬁned to be 0. Otherwise  the weighted Orlicz norm kxkG w is deﬁned as the unique
value ↵> 0 such thatPn
i=1 wiG(|xi|/↵) = 1.
When wi = 1 for all i 2 [n]  we have kxkG w = kxkG where kxkG is the (unweighted) Orlicz norm.
It is well known that k·k G is a norm. We show in the following lemma that k·k G w is a seminorm.
Lemma 3. For a function G that satisﬁes Assumption 1 and a weight vector w 2 Rn such that wi  0
for all i 2 [n]  for all x  y 2 Rn  we have (i) kxkG w  0  (ii) kx + ykG w  kxkG w + kykG w  and
(iii) kaxkG w = |a| ·k xkG w for all a 2 R.
Leverage Scores and Well-Conditioned Bases for Orlicz Norms. The following lemma estab-
lishes an upper bound on the summation of Orlicz norm leverage scores deﬁned in Deﬁnition 3.
Lemma 4. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1. Let
U 2 Rn⇥d be a well-conditioned basis with condition number G as in Deﬁnition 2. Then we have
Pn
Now we show that given a subspace embedding which embeds the column space of A with Orlicz
norm k·k G into the `2 space with distortion   then one can construct a well-conditioned basis with
condition number G  .
Lemma 5. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1.
For a given matrix A 2 Rn⇥d and an embedding matrix ⇧ 2 Rs⇥n  suppose for all x 2 Rd 
kAxkG  k⇧Axk2  kAxkG. Let Q · R = 1
 ⇧A. Then AR1 is
a well-conditioned basis (see Deﬁnition 2) with G(AR1)  .
The following lemma shows how to estimate Orlicz norm leverage scores given a change of basis
matrix R 2 Rd⇥d  in eO(nnz(A) + poly(d)) time.
Lemma 6. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1. For
a given matrix A 2 Rn⇥d and R 2 Rd⇥d  there exists an algorithm that outputs {ui}n
i=1 such that
with probability at least 0.99  ui =⇥( G(k(AR1)ik2)) for all 1  i  n. The algorithm runs in
eO(nnz(A) + poly(d)) time.

The Row Sampling Algorithm. Based on the notion of Orlicz norm leverage scores and well-
conditioned bases  we design a row sampling algorithm for Orlicz norms.
Lemma 7. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1.
Let U 2 Rn⇥d be a well-conditioned basis with condition number G = G(U ) as in Deﬁnition
2. For sufﬁciently small " and   and sufﬁciently large constant C  let {pi}n
i=1 be a set of sampling
probabilities satisfying pi  min1  C (log(1/) + d log(1/")) "2G (kUik2) . Let w be a vector
whose i-th entry is set to be wi = 1/pi with probability pi and wi = 0 with probability 1  pi  then
with probability at least 1   for all x 2 Rd  we have (1 ")kU xkG  kU xkG w  (1 + ")kU xkG.
Solving Linear Regression for Orlicz Norms. Now we combine all ingredients to give an algo-
rithm for Orlicz norm regression. We use A 2 Rn⇥(d+1) to denote a matrix whose ﬁrst d columns
are A and the last column is b. The algorithm is described in Figure 1  and we prove its running time
and correctness in Theorem 8. We assume we are given an embedding matrix ⇧  such that for all
x 2 Rd+1  kAxkG  k⇧Axk2  kAxkG. The construction of ⇧ and the value  will be given in
Corollary 12. In Section D.1 of the supplementary material  we use Theorem 8 and Corollary 12 to
formally prove Theorem 1.

7

1. For the given embedding matrix ⇧  calculate ⇧A and invoke QR-decomposition on

i=1 be a set of sampling probabilities with

⇧A/ to obtain Q · R =⇧ A/.
2. Invoke Lemma 6 to obtain {ui}n
3. For a sufﬁciently large constant C  let {pi}n
pi  min1  C · d · "2 log(1/") · Gk(AR1)ik2   and w be a vector whose i-th
entry wi = 1/pi with probability pi and wi = 0 with probability 1  pi.
4. Calculate x⇤ = argminx2Rd kAx  bkG w. Return x⇤.

i=1 such that ui =⇥( G(k(AR1)ik2)).

Figure 1: Algorithm for Orlicz norm regression

Theorem 8. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1.
Given an embedding matrix ⇧  such that for all x 2 Rd  kAxkG  k⇧Axk2  kAxkG  with
probability at least 0.9  the algorithm in Figure 1 outputs x⇤ 2 Rd in time poly(d/") + TQR(⇧A) 
such that kAx⇤  bkG  (1 + ") minx2Rd kAx  bkG. Here  TQR(⇧A) is the running time for
calculating ⇧A and invoking QR-decomposition on ⇧A.

3 Linear Regression for Symmetric Norms

In this section  we introduce SymSketch  a subspace embedding for symmetric norms.

Deﬁnition of SymSketch. We ﬁrst formally deﬁne SymSketch. Due to space limitation  we
give the deﬁnition of Gaussian embeddings  CountSketch embeddings and their compositions in
Section C.1.1 of the supplementary material.

Deﬁnition 5 (Symmetric Norm Sketch (SymSketch)). Let t = ⇥(log n). Let eD 2 Rn(t+1)⇥n be a
matrix deﬁned as eD =⇥(w0D0)> (w1D1)> . . .
(wtDt)>⇤>  where for each i 2{ 0  1  . . .   t} 
Di = diag(zi 1  zi 2  . . .   zi n) 2 Rn⇥n and zi j ⇠ Ber(1/2i) for each j 2 [n]. Moreover  wi =
k(1  1  . . .   1  0  . . .   0)k` (there are 2i 1s). Let ⇧ 2 RO(d)⇥n(t+1) be a composition of Gaussian
embedding and CountSketch embedding (Deﬁnition 12) with " = 0.1  and S =⇧ eD. We say
S 2 RO(d)⇥n is a SymSketch.
Modulus of Concentration. Now we give the deﬁnition of mmc(`) for a symmetric norm.
Deﬁnition 6 ([7]). Let X denote the uniform distribution over Sn1. The median of a symmetric norm
k·k ` is the unique value M` such that Prx⇠X [kxk`  M`]  1/2 and Prx⇠X [kxk`  M`]  1/2.
Deﬁnition 7 ([7]). For a given symmetric norm k·k `  we deﬁne the modulus of concentration to be
mc(`) = maxx2Sn1 kxk`/M`  and deﬁne the maximum modulus of concentration to be mmc(`) =
maxk2[n] mc(`(k))  where k·k `(k) is a norm on Rk which is deﬁned to be k(x1  x2  . . .   xk)k`(k) =
k(x1  x2  . . .   xk  0  . . .   0)k`.
It has been shown in [7] that mmc(`) =⇥( n1/21/p) for `p norms when p > 2  mmc(`) = ⇥(1)

for `p norms when p  2  mmc(`) = e⇥(pn/k) for top-k norms  and mmc(`) = O(log n) for the

k-support norm [5] and the box-norm [25]. We show that mmc(`) is upper bounded by O(1) for
max-mix of `2 norm and `1 norm and sum-mix of `2 norm and `1 norm.
Lemma 9. For a real number c > 0  let kxk`a = kxk2 + ckxk1 and kxk`b = max{kxk2  ckxk1}.
We have mmc(`a) = O(1) and mmc(`b) = O(1).
Moreover  we show that for an Orlicz norm k·k G induced by a function G which satisﬁes Assump-
tion 1  mmc(`) is upper bounded by O(pCG log n).
Lemma 10. For an Orlicz norm k·k G on Rn induced by a function G which satisﬁes Assumption 1 
mmc(`) is upper bounded by O(pCG log n).

8

Subspace Embedding. The following theorem shows that SymSketch is a subspace embedding.
Theorem 11. Let S 2 RO(d)⇥n be a SymSketch as deﬁned in Deﬁnition 5. For a given matrix
A 2 Rn⇥d  with probability at least 0.9  for all x 2 Rd 

⌦⇣1/(pd · log3 n)⌘ ·k Axk`  kSAxk2  O⇣mmc(`) · d2 · log5/2 n⌘ ·k Axk`.

Furthermore  the running time of computing SA is eO(nnz(A) + poly(d)).
Combine Theorem 11 with Lemma 10  we have the following corollary.
Corollary 12. Let k·k G be an Orlicz norm induced by a function G which satisﬁes Assumption 1.
Let S 2 RO(d)⇥n be a SymSketch as deﬁned in Deﬁnition 5. For a given matrix A 2 Rn⇥d  with
probability at least 0.9  for all x 2 Rd 

⌦⇣1/(pd · log3 n)⌘ ·k Axk`  kSAxk2  O⇣pCG · d2 · log7/2 n⌘ ·k Axk`.

Furthermore  the running time of computing SA is eO(nnz(A) + poly(d)).

4 Conclusion

In this paper  we give efﬁcient algorithms for solving the overconstrained linear regression problem 
when the loss function is a symmetric norm. For the special case when the loss function is an Orlicz

norm  our algorithm produces a (1 + ")-approximate solution in eO(nnz(A) + poly(d/")) time. When
the loss function is a general symmetric norm  our algorithm produces a pd · polylog n · mmc(`)-
approximate solution in eO(nnz(A) + poly(d)) time.

In light of Problem 1  there are a few interesting problems that remain open. Is that possible to design
an algorithm that produces (1 + ")-approximate solutions to the linear regression problem  when
the loss function is a general symmetric norm? Furthermore  is that possible to use the technique of
linear sketching to speed up the overconstrained linear regression problem  when the loss function is
a general norm? Answering these problems could lead to a better understanding of Problem 1.

Acknowledgements

P. Zhong is supported in part by NSF grants (CCF-1703925  CCF-1421161  CCF-1714818  CCF-
1617955 and CCF-1740833)  Simons Foundation (#491119)  Google Research Award and a Google
Ph.D. fellowship. R. Wang is supported in part by NSF grant IIS-1763562  Ofﬁce of Naval Research
(ONR) grants (N00014-18-1-2562  N00014-18-1-2861)  and Nvidia NVAIL award. Part of this work
was done while Z. Song  L. F. Yang  H. Zhang and P. Zhong were interns at IBM Research - Almaden
and while Z. Song  R. Wang and H. Zhang were visiting the Simons Institute for the Theory of
Computing. Z. Song and P. Zhong would like to thank Alexandr Andoni  Kenneth L. Clarkson  Yin
Tat Lee  Eric Price  Clifford Stein and David P. Woodruff for insight discussions.

9

References
[1] N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss

transform. In STOC  pages 557–563  2006.

[2] A. Andoni  C. Lin  Y. Sheng  P. Zhong  and R. Zhong. Subspace embedding and linear regression

with Orlicz norm. In ICML  pages 224–233  2018.

[3] A. Andoni  A. Naor  A. Nikolov  I. Razenshteyn  and E. Waingarten. Hölder homeomorphisms

and approximate nearest neighbors. In FOCS  pages 159–169  2018.

[4] A. Andoni  H. L. Nguyen  A. Nikolov  I. Razenshteyn  and E. Waingarten. Approximate near

neighbors for general symmetric norms. In STOC  pages 902–913  2017.

[5] A. Argyriou  R. Foygel  and N. Srebro. Sparse prediction with the k-support norm. In NIPS 

pages 1457–1465  2012.

[6] H. Auerbach. On the area of convex curves with conjugate diameters. PhD thesis  University of

Lwów  1930.

[7] J. Błasiok  V. Braverman  S. R. Chestnut  R. Krauthgamer  and L. F. Yang. Streaming symmetric

norms via measure concentration. In STOC  pages 716–729  2017.

[8] P. Brändén. Hyperbolic polynomials and the kadison-singer problem.

arXiv:1809.03255  2018.

arXiv preprint

[9] V. Braverman  S. R. Chestnut  D. P. Woodruff  and L. F. Yang. Streaming space complexity of

nearly all functions of one variable on frequency vectors. In PODS  pages 261–276  2016.

[10] V. Braverman and R. Ostrovsky. Zero-one frequency laws. In STOC  pages 281–290  2010.

[11] K. L. Clarkson  R. Wang  and D. P. Woodruff. Dimensionality reduction for tukey regression.

In ICML  pages 1262–1271  2019.

[12] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity

time. In STOC  pages 81–90  2013.

[13] K. L. Clarkson and D. P. Woodruff. Input sparsity and hardness for robust subspace approxima-

tion. In FOCS  pages 310–329  2015.

[14] K. L. Clarkson and D. P. Woodruff. Sketching for M-estimators: A uniﬁed approach to robust

regression. In SODA  pages 921–939  2015.

[15] M. B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In SODA 

pages 278–287  2016.

[16] M. B. Cohen and R. Peng. `p row sampling by lewis weights. In STOC  pages 183–192  2015.
[17] D. Dadush  A. Nikolov  K. Talwar  and N. Tomczak-Jaegermann. Balancing vectors in any

norm. In FOCS  2018.

[18] A. Dasgupta  P. Drineas  B. Harb  R. Kumar  and M. W. Mahoney. Sampling algorithms and

coresets for `p regression. SIAM Journal on Computing  38(5):2060–2078  2009.

[19] V. Feldman  C. Guzmán  and S. S. Vempala. Statistical query algorithms for mean vector

estimation and stochastic convex optimization. In SODA  pages 1265–1277  2017.

[20] P. Indyk and D. P. Woodruff. Optimal approximations of the frequency moments of data streams.

In STOC  pages 202–208  2005.

[21] Y. T. Lee  Z. Song  and S. S. Vempala. Algorithmic theory of odes and sampling from well-

conditioned logconcave densities. arXiv preprint arXiv:1812.06243  2018.

[22] J. Li  A. Nikolov  I. Razenshteyn  and E. Waingarten. On mean estimation for general norms

with statistical queries. In COLT  2019.

10

[23] M. Li  G. L. Miller  and R. Peng. Iterative row sampling. In FOCS  pages 127–136  2013.
[24] M. W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends R in

Machine Learning  3(2):123–224  2011.

[25] A. M. McDonald  M. Pontil  and D. Stamos. Spectral k-support norm regularization. In NIPS 

pages 3644–3652  2014.

[26] X. Meng and M. W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and

applications to robust linear regression. In STOC  pages 91–100  2013.

[27] V. Nakos  Z. Song  and Z. Wang. Robust sparse recovery via m-estimators. Manuscript  2019.

[28] J. Nelson and H. L. Nguyeben. Sparsity lower bounds for dimensionality reducing maps. In STOC 

[29] T. Sarlós. Improved approximation algorithms for large matrices via random projections. In

pages 101–110  2013.

FOCS  pages 143–152  2006.

[30] C. Sohler and D. P. Woodruff. Subspace embeddings for the l1-norm with applications. In

STOC  pages 755–764  2011.

[31] Z. Song  D. P. Woodruff  and P. Zhong. Towards a zero-one law for column subset selection. In

NeurIPS  2019.

[32] R. Wang and D. P. Woodruff. Tight bounds for `p oblivious subspace embeddings. In SODA 

pages 1825–1843  2019.

[33] P. Wojtaszczyk. Banach spaces for analysts  volume 25. Cambridge University Press  1996.
[34] D. P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in

Theoretical Computer Science  10(1–2):1–157  2014.

[35] D. P. Woodruff and Q. Zhang. Subspace embeddings and `p-regression using exponential

random variables. In COLT  pages 546–567  2013.

[36] J. Yang  X. Meng  and M. Mahoney. Quantile regression for large-scale applications. In ICML 

pages 881–887  2013.

11

,Balázs Szörényi
Róbert Busa-Fekete
Adil Paul
Eyke Hüllermeier
Zhao Song
Ruosong Wang
Lin Yang
Hongyang Zhang
Peilin Zhong