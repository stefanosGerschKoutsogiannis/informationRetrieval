2019,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset,Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect  many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper  we propose a novel data-set which consists of over 1 million images of physical 3D objects with seven factors of variation  such as object color  shape  size and position. In order to be able to control all the factors of variation precisely  we built an experimental platform where the objects are being moved by a robotic arm. In addition  we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation  and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly  but that model and hyperparameter selection is an effective means of transferring information to the real world.,On the Transfer of Inductive Bias
from Simulation to the Real World:

a New Disentanglement Dataset

Muhammad Waleed Gondal1⇤†
Francesco Locatello12
Olivier Bachem4

Martin Breidt3

Valentin Volchkov1

Manuel Wüthrich1⇤

Bernhard Schölkopf1

Ðor ¯de Miladinovi´c2
Joel Akpo1

Stefan Bauer1†

1Max Planck Institute for Intelligent Systems
2Department of Computer Science ETH Zurich
3Max Planck Institute for Biological Cybernetics

4Google Research  Brain Team

Abstract

Learning meaningful and compact representations with disentangled semantic
aspects is considered to be of key importance in representation learning. Since
real-world data is notoriously costly to collect  many recent state-of-the-art disen-
tanglement models have heavily relied on synthetic toy data-sets. In this paper 
we propose a novel data-set which consists of over one million images of physical
3D objects with seven factors of variation  such as object color  shape  size and
position. In order to be able to control all the factors of variation precisely  we
built an experimental platform where the objects are being moved by a robotic
arm. In addition  we provide two more datasets which consist of simulations of
the experimental setup. These datasets provide for the ﬁrst time the possibility to
systematically investigate how well different disentanglement methods perform on
real data in comparison to simulation  and how simulated data can be leveraged
to build better representations of the real world. We provide a ﬁrst experimental
study of these questions and our results indicate that learned models transfer poorly 
but that model and hyperparameter selection is an effective means of transferring
information to the real world.

1

Introduction

In representation learning it is commonly assumed that a high-dimensional observation X is generated
from low-dimensional factors of variation G. The goal is usually to revert this process by searching
for a latent embedding Z which replicates the underlying generative factors G  e.g. shape  size or
color. Learning well-disentangled representations of complex sensory data has been identiﬁed as
one of the key challenges in the quest for artiﬁcial intelligence (AI) [2  45  31  3  48  29  54]  since
they should contain all the information present in the observations in a compact and interpretable
structure [2  26  8] while being independent from the task at hand [15  33].
Disentangled representations may be useful for (semi-)supervised learning of downstream tasks 
transfer and few-shot learning [2  49  39]. Further  such representations allow to ﬁlter out nuisance
factors [27]  to perform interventions and to answer counterfactual questions [44  50  45]. First

⇤These authors contributed equally.
†Correspondence to: waleed.gondal@tue.mpg.de  stefan.bauer@inf.ethz.ch

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

applications of algorithms for learning disentangled representations have been applied to visual
concept learning  sequence modeling  curiosity-based exploration or even domain adaptation in
reinforcement learning [51  30  42  20  22  34  54]. The research community is in general agreement
on the importance of this paradigm and much progress has been made in the past years  particularly
on the algorithmic level [e.g. 18  24]  fundamental understanding [e.g. 17  52] and experimental
evaluation [38]. However  research has thus far focused on synthetic toy datasets.
The main motivation for using synthetic datasets is that they are cheap  easy to generate and the
independent generative factors can be easily controlled. However  real-world recordings exhibit
imperfections such as chromatic aberrations in cameras and complex surface properties of objects (e.g.
reﬂectance  radiance and irradiance)  making transfer learning from synthetic to real data a nontrivial
task. Despite the growing importance of the ﬁeld and the potential societal impact in the medical
domain or fair decision making [e.g. 6  10  37]  the performance of state-of-the-art disentanglement
learning on real-world data is unknown.

Figure 1: Datasets: In the top row samples from previously published datasets are shown from left to
right: dSprites  Noisy-dSprites  Scream-dSprites  3dshapes  Cars3D and SmallNORB. In the second
row (again left to right) we provide simple simulated data  highly realistically simulated data and
real-world data examples of the newly collected dataset.

To bridge the gap between simulation and the physical world  we built a recording platform which
allows to investigate the following research questions: (i) How well do unsupervised state-of-the-art
algorithms transfer from rendered images to physical recordings? (ii) How much does this transfer
depend on the quality of the simulation? (iii) Can we learn representations on low dimensional
recordings and transfer them from the current state-of-the-art of 64 ⇥ 64 images to high quality
images? (iv) How much supervision is necessary to encode the necessary inductive biases? (v)
Are the confounding and distortions of real-world recordings beneﬁcial for learning disentangled
representations? (vi) Can we disentangle causal mechanisms [44  28  29  45] in the data generating
process? (vii) Are disentangled representations useful for solving the real-world downstream tasks?
While answering all of the above questions is beyond the scope of this paper  our key contributions
can be summarized as follows:

• We introduce the ﬁrst real-world 3D data set recorded in a controlled environment  deﬁned
by 7 factors of variation: object color  object shape  object size  camera height  background
color and two degrees of freedom of motion of a robotic arm. The dataset is made publicly
available3.
• We provide synthetic images produced by computer graphics with two levels of realism.
Since the robot arm and the objects are printed from a 3D template  we can ensure a close
similarity between the realistic renderings and the real-world recordings.

3https://github.com/rr-learning/disentanglement_dataset

2

of the two simulated datasets contains the same number of images as well.

• The collected datatset of physical 3D objects consists of over one million images  and each
• We investigate the role of inductive bias and the transfer of different hyper-parameter settings
between the different simulations and the real-world and the requirements on the quality of
the simulation for a succesful transfer.

2 Background and Related Work

···

ZK1

ZK

G1

K1

GK1

GK

···

X

Z1

Z2

G1

G2

We assume a set of observations of a (potentially high di-
mensional) random variable X which is generated by K
unobserved causes of variation (generative factors) G =
[G1  . . .   GK] (i.e.  G ! X) that do not cause each other.
These latent factors represent elementary ingredients to the
causal mechanism generating X [44  45]. The elementary in-
gredients Gi  i = 1  . . .   K of the causal process work on their
own and are changeable without affecting others  reﬂecting the
independent mechanisms assumption [49]. However  for some
of the factors a hierarchical structure may exist for which this
may only hold true when seeing the hierachical structure as a
whole as one component. The graphical model corresponding
to this framework and adapted from [52] is depicted in ﬁgure 2.
The hirachical structure of the factors G1
K1 and GK1 might
represent one compositional process e.g. connected joints of
a robot arm. The most commonly accepted understanding of
disentanglement [2] is that each learned feature in Z should
capture one factor of variation in G.
Current state-of-the-art disentanglement approaches use the
framework of variational auto-encoders (VAEs) [25]. The
(high-dimensional) observations x are modelled as being gen-
erated from some latent features z with chosen prior p(z)
according to the probabilistic model p✓(x|z)p(z). The generative model p✓(x|z) as well as the
proxy posterior q(z|x) can be represented by neural networks  which are optimized by maximizing
the variational lower bound (ELBO) of log p(x1  . . .   xN )
LV AE =PN

Figure 2: Graphical Model  where
G = (G1  G2  . . .   GK) are the
generative factors (color  shape  size 
...)
and X the recorded images.
The aim of disentangled representa-
tion learning is to learn variables Zi
that capture the independent mecha-
nisms Gi.

Since the above objective does not enforce any structure on the latent space  except for some similarity
to the typically chosen isotropic Gaussian prior p(z)  various proposals for more structure imposing
regularization have been made. Using some sort of supervision [e.g. 43  4  35  40  9] or proposing
completely unsupervised [e.g. 19  24  7  27  13] learning approaches. [19] proposed the -VAE
penalizing the Kullback-Leibler divergence (KL) term in the VAE objective more strongly  which en-
courages similarity to the factorized prior distribution. Others used techniques to encourage statistical
independence between the different components in Z  e.g.  FactorVAE [24] or -TCVAE [7]  while

i=1 Eq(z|x(i))[log p✓(x(i)|z)]  DKL(q(z|x(i))kp(z))

DIP-VAE proposed to encourage factorization of the inferred prior q(z) =R q(z|x)p(x) dx. For

other related work we refer to the detailed descriptions in the recent empirical study [38].

2.1 Established Datasets for the Unsupervised Learning of Disentangled Representations
Real-world data is costly to generate and groundtruth is often not available since signiﬁcant con-
founding may exist. To bypass this limitation  many recent state-of-the-art disentanglement models
[55  24  7  18  8] have heavily relied on synthetic toy datasets  trying to solve a simpliﬁed version of
the problem in the hope that the conclusions drawn might likewise be valid for real-world settings. A
quantitative summary of the most widely used datasets for learning disentangled representations is
provided in table 1.

Dataset Descriptions: For quantitative analysis  dSprites4 is the most commonly used dataset.
This synthetic dataset [18] contains binary 2D images of hearts  ellipses and squares in low resolution.

4https://github.com/deepmind/dsprites-dataset

3

Dataset
dSprites
Noisy dspirtes
Scream dSprites
SmallNORB
Cars3D
3dshapes
MPI3D-toy
MPI3D-realistic
MPI3D-real

Factors of
Variation

5
5
5
5
3
6
7
7
7

Resolution
64 ⇥ 64
64 ⇥ 64
64 ⇥ 64
128 ⇥ 128
64 ⇥ 64
64 ⇥ 64
64 ⇥ 64
256 ⇥ 256
512 ⇥ 512

# of Images

737 280
737 280
737 280
48 600
17 568
480 000
1 036 800
1 036 800
1 036 800

3D Real-World
7
7
7
3
3
3
3
3
3

7
7
7
7
7
7
7
7
3

Table 1: Summary of the properties of different datasets. The newly contributed datasets are
emphasized.

In Color-dSprites the shapes are colored with a random color  Noisy-dSprites considers white-colored
shapes on a noisy background and in Scream-dSprites the background is replaced with a random patch
in a random color shade extracted from the famous The Scream painting [41]. The dSprites shape is
embedded into the image by inverting the color of its pixel. The SmallNORB5 dataset contains images
of 50 toys belonging to 5 generic categories: four-legged animals  human ﬁgures  airplanes  trucks 
and cars. The objects were imaged by two cameras under 6 lighting conditions  9 elevations (30 to
70 degrees every 5 degrees)  and 18 azimuths (0 to 340 every 20 degrees) [32]. For Cars3D6 199
CAD models from [14] were used to generate 64x64 color renderings from 24 rotation angles each
offset by 15 degrees [46]. Recently  3dshapes was made publicly available7  a dataset of 3D shapes
procedurally generated from 6 ground truth independent latent factors. These factors are ﬂoor colour 
wall colour  object colour  scale  shape and orientation [24].

3 Bridging the Gap Between Simulation and the Real World: A Novel

Dataset

While other real-world recordings  e.g. CelebA [36]  exist  they offer only qualitative evaluations.
However  a more controlled dataset is needed to quantitatively investigate the effects of inductive
biases  sample complexity and the interplay of simulations and the real-world.

3.1 Controlled Recording Setup

Figure 3: Renderings of the developed robotic platform: On the left a view from a 30 angle from
the top (note that one panel in front and the top panels have been removed such that the interior of
the platform is visible. During recordings  the platform is entirely closed. Middle: the robotic arm
carrying a red cube (the entire cage is hidden). Right: frontal view without the black shielding (note
the three cameras at different heights).

5https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/
6http://www.scottreed.info/files/nips2015-analogy-data.tar.gz
7https://github.com/deepmind/3dshapes-dataset/

4

In order to record a controlled dataset of physical 3D objects  we built the mechanical platform
illustrated in ﬁgure 3. It consists of three cameras mounted at different heights  a robotic manipulator
carrying a 3D printed object (which can be swapped) in the center of the platform and a rotating table
at the bottom. The platform is shielded with black sheets from all sides to avoid any intrusion of
external factors (e.g. light) and the whole environment is relatively uniformly illuminated by three
light sources installed within the platform.

3.1.1 Factors of Variation
The generative factors of variation G mentioned in section 2 are listed in the following for our
recording setup.

Object Color: All objects have one of six different colors: red (255  0  0)  green (0  255  0)  blue
(0  0  255)  white (255  255  255)  olive (210 210 80) and brown (153 76 0) (see ﬁgure 4).

Figure 4: We show all the object colors while maintaining the other factors constant.

Figure 5: We show all object shapes while maintaining all other factors constant.

Object Shape: There are objects of four different shapes in
the dataset: a cylinder  a hexagonal prism  a cube  a sphere 
a pyramid with square base and a cone. All objects exhibit
rotational symmetries about some axes  however the kinemat-
ics of the robot are such that these axes never align with the
degrees of freedom of the robot. This is important because it
ensures that the robot degrees of freedom are observable given
the images.

Object Size: There are objects of two different sizes in the
dataset  categorized as large (roughly 65mm in diameter) and
small (roughly 45 mm in diameter).

Figure 6: We show the two object
sizes while maintaining all other fac-
tors constant.

Camera Height: The dataset is recorded with three cameras
mounted at three different heights (see ﬁgure 7 on the right)  which represents another factor of
variation in the images.

Figure 7: Three images on the left: we vary the backround color. Three images on the right: we vary
the camera height.

5

Background Color: The rotation table (see ﬁgure 7) allows us to change background color. Note
that for all images in the dataset we orient the table in such a way that only one background color is
visible at a time. The colors are: sea green  salmon and purple.

Degrees of Freedom of the Robotic Arm: Each object is mounted on the tip of the manipulator
shown in ﬁgure 3. This manipulator has two degrees of freedom  a rotation about a vertical axis at
the base and a second rotation about a horizontal axis. We move each joint in a range of 180 in 40
equal steps (see ﬁgure 8 and ﬁgure 9). Note that these two factors of variation are independent  just
like all other factors (i.e. we record all possible combinations between the two).

Figure 8: Motion along ﬁrst DOF while maintaining the other factors constant. Note that in total we
record 40 steps  here we only show 6 due to space constraints.

Figure 9: Motion along second DOF while maintaining the other factors constant. Note that in total
we record 40 steps  here we only show 6 due to space constraints.

3.2 Simulated Data

In addition to the real-world dataset we recorded two simulated datasets of the same setup  hence all
factors of variation are identical across the three datasets. One of the simulated datasets is designed
to be as realistic as possible and the synthetic images are visually practically indistinguishable from
real images (see ﬁgure 1 middle). For the second simulated dataset we used a deliberately simpliﬁed
model (see ﬁgure 1 left)  which allows to investigate transfer from simpliﬁed models to real data.
The synthetic data was generated using Autodesk 3ds Max(2018). Most parts of the scene were
imported from SolidWorks CAD ﬁles that were originally used to construct the experimental stage
including the manipulator and 3D printing of the test objects. The surface shaders are based on
Autodesk Physical material with hand-tuned shading parameters  based on full resolution reference
images. The camera poses were initialized from the CAD data and then manually ﬁne-tuned using
reference images. The realistic synthetic images were obtained using the Autodesk Raytracer (ART)
with three rectangular light sources  mimicking the LED panels. The simpliﬁed images were rendered
with the Quicksilver hardware renderer.

4 First Experimental Evaluations of (unsupervised) Disentanglement

Methods on Real-World Data

Some ﬁelds have been able to narrow the gap between simulation and reality [56  5  23]  which has
led to remarkable achievements (e.g. for in-hand manipulation [1]). In contrast  for disentanglement
methods this gap has not been bridged yet  state-of-the-art algorithms seem to have difﬁculties to
transfer learned representations even between toy datasets [38]. The proposed dataset will enable
the community to systematically investigate how such transfer of information between simulations
with different degrees of realism and real data can be achieved. In the following we present a ﬁrst
experimental study in this directions.

6

4.1 Experimental Protocol

We apply all the disentanglement methods (-VAE  FactorVAE  -TCVAE  DIP-VAE-I  DIP-VAE-II 
AnnealedVAE) which were used in a recent large-scale study [38] to our three datasets. Due to space
constraints  the models are abbreviated with numbers one to ﬁve in the plots in the same order. We
use (disentanglement_lib) and we evaluate on the same scores as [38]. In all the experiments 
we used images with resolution 64x64. This resolution is used in the recent large-scale evaluations
and by state-of-the-art disentanglement learning algorithms [38]. Each of the six methods is trained
on each of the three datasets with ﬁve different hyperparameter settings (see table 2 in the appendix
for details) and with three different random seeds  leading to a total of 270 models. Each model is
trained for 300 000 iterations on Tesla V100 gpus. Details about the evaluation metrics can be found
in appendix C.

4.2 Experimental Results

Figure 10: Reconstruction scores of different methods (0=-VAE  1=FactorVAE  2=-TCVAE 
3=DIP-VAE-I  4=DIP-VAE-II  5=AnnealedVAE) on the realistic synthetic dataset  the toy synthetic
dataset and the real dataset.

Reconstruction Across Datasets: Figure 10 shows that there is a difference in reconstruction score
across datasets: The score is the lowest on real data  followed by the realistic simulated dataset (R)
and the simple toy (T) images. This indicates that there is a signiﬁcant difference in the distribution
of the real data compared to the simulated data  and that it is harder to learn a representation of the
real data than of the simulated data. However  the relative behaviour of different methods seems to be
similar across all three datasets  which indicates that despite the differences  the simulated data may
be useful for model selection.

Figure 11: The Mutual Information Gap (MIG) scores attained by different methods for the following
evaluations (from left to right): (a) trained and evaluated on synthetic realistic  (b) trained and
evaluated on synthetic toy  (c) trained and evaluated on real  (d) trained on synthetic realistic and
evaluated on real  (e) trained on synthetic toy and evaluated on real. The variance is due to different
hyper-parameters and seeds.

Direct Transfer of Representations:
In ﬁgure 11 we show the Mutual Information Gap (MIG)
scores attained by different methods for different evaluations. The same plots for different metrics
look qualitatively similar (see ﬁgure 22 in the appendix). Given the high variance  it is difﬁcult to
make conclusive statements. However  it seems quite clear that all methods perform signiﬁcantly
better when they are trained and evaluated on the same dataset (three plots on the left). Direct transfer
of learned representations from simulated to real data (two plots on the right) seems to work rather
poorly.

7

Transfer of Hyperparameters: We have seen that transferring
representations directly from simulated to real data seems to work
poorly. However  it may be possible to instead transfer information
at a higher-level  such as the choice of the method and its hyperpa-
rameters as an inductive bias.
In order to quantitatively evaluate whether such a transfer is possible 
we pick the model (including hyperparameters) which performs best
in simulation (according to a metric chosen at random)  and we
compute the probability of outperforming (according to a metric and
seed chosen randomly) a model which was chosen at random. If no
transfer is possible  we would expect this probability to be 50%.
However  we ﬁnd that model selection from realistic simulated ren-
derings (R) outperforms random model selection 72% of the time
while transferring the model from the simpler synthetic images (T)
to real-world data even beats random selection 78% of the time.
This ﬁnding is conﬁrmed by ﬁgure 12  where we show the rank-
correlation of the performance of models (including hyperparame-
ters) trained on one dataset with the performance of these models
trained on another dataset. The performance of a model trained on
some dataset seems to be highly correlated with the performance of that model trained on any other
dataset. In ﬁgure 12 we use the DCI disentanglement metric as a score  however  qualitatively similar
results can be observed using most of the disentanglement metrics (see ﬁgure 25 in the appendix).
Summary These results indicate that the simulated and the real data distribution have some sim-
ilarities  and that these similarities can be exploited through model and hyperparameter selection.
Surprisingly  it seems that the transfer of models from the synthetic toy dataset may work even better
than the transfer from the realistic synthetic dataset.

Figure 12: Rank-correlation
of the DCI disentanglement
scores of different models
(including hyperparameters)
across different data sets.

5 Conclusions

Despite the intended applications of disentangled representation learning algorithms to real data in
ﬁelds such as robotics  healthcare and fair decision making [e.g. 6  10  20]  state-of-the-art approaches
have only been systematically evaluated on synthetic toy datasets. Our work effectively complements
related efforts [e.g. 38] to address current challenges of representation learning  offering the possibility
of investigating the role of inductive biases  sample complexity  transfer learning and the use of labels
using real-world images.
A key aspect of our datasets is that we provide rendered images of increasing complexity for the
same setup used to capture the real-world recordings. The different recordings offer the possibility of
investigating the question if disentangled representations can be transferred from simulation to the
real world and how the transferability depends on the degree of realism of the simulation. Beyond the
evaluation of representation learning algorithms  the proposed dataset can likewise be used for other
tasks such as 3D reconstruction and scene rendering [12] or learning compositional visual concepts
[21]. Furthermore  we are planning to use the novel experimental setup for recording objects with
more complicated shapes and textures under more difﬁcult conditions  such as dependence among
different factors.

Acknowledgments
This research was partially supported by the Max Planck ETH Center for Learning Systems and
Google Cloud. We thank Alexander Neitz and Arash Mehrjou for useful discussions. We would also
like to thank Felix Grimminger  Ludovic Righetti  Stefan Schaal  Julian Viereck and Felix Widmaier
whose work served as a starting point for the development of the robotic platform in the present paper.

8

References
[1] Marcin Andrychowicz  Bowen Baker  Maciek Chociej  Rafal Jozefowicz  Bob McGrew  Jakub
Pachocki  Arthur Petron  Matthias Plappert  Glenn Powell  Alex Ray  et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177  2018.

[2] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE transactions on pattern analysis and machine intelligence  35(8):1798–
1828  2013.

[3] Yoshua Bengio  Yann LeCun  et al. Scaling learning algorithms towards ai. Large-scale kernel

machines  34(5):1–41  2007.

[4] Diane Bouchacourt  Ryota Tomioka  and Sebastian Nowozin. Multi-level variational autoen-

coder: Learning disentangled representations from grouped observations. In AAAI  2018.

[5] Konstantinos Bousmalis  George Trigeorgis  Nathan Silberman  Dilip Krishnan  and Dumitru
Erhan. Domain separation networks. In Advances in Neural Information Processing Systems 
pages 343–351  2016.

[6] Agisilaos Chartsias  Thomas Joyce  Giorgos Papanastasiou  Scott Semple  Michelle Williams 
David Newby  Rohan Dharmakumar  and Sotirios A Tsaftaris. Factorised spatial representation
learning: application in semi-supervised myocardial segmentation. In International Conference
on Medical Image Computing and Computer-Assisted Intervention  pages 490–498. Springer 
2018.

[7] Tian Qi Chen  Xuechen Li  Roger B Grosse  and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In Advances in Neural Information Processing
Systems  pages 2610–2620  2018.

[8] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems  pages 2172–2180  2016.

[9] Brian Cheung  Jesse A Livezey  Arjun K Bansal  and Bruno A Olshausen. Discovering hidden
factors of variation in deep networks. In Workshop at International Conference on Learning
Representations  2015.

[10] E. Creager  D. Madras  J-H. Jacobson  M. Weis  K. Swersky  T. Pitassi  and R. Zemel. Flexibly

fair representation learning by disentanglement. In ICML  page to appear  2019.

[11] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations  2018.
[12] SM Ali Eslami  Danilo Jimenez Rezende  Frederic Besse  Fabio Viola  Ari S Morcos  Marta
Garnelo  Avraham Ruderman  Andrei A Rusu  Ivo Danihelka  Karol Gregor  et al. Neural scene
representation and rendering. Science  360(6394):1204–1210  2018.

[13] Babak Esmaeili  Hao Wu  Sarthak Jain  Siddharth Narayanaswamy  Brooks Paige  and
arXiv preprint

Jan-Willem Van de Meent. Hierarchical disentangled representations.
arXiv:1804.02086  2018.

[14] Sanja Fidler  Sven Dickinson  and Raquel Urtasun. 3d object detection and viewpoint estimation
with a deformable 3d cuboid model. In Advances in neural information processing systems 
pages 611–619  2012.

[15] Ian Goodfellow  Honglak Lee  Quoc V Le  Andrew Saxe  and Andrew Y Ng. Measuring
invariances in deep networks. In Advances in neural information processing systems  pages
646–654  2009.

[16] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122  2018.
[17] Irina Higgins  David Amos  David Pfau  Sebastien Racaniere  Loic Matthey  Danilo Rezende 
and Alexander Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint
arXiv:1812.02230  2018.

9

[18] Irina Higgins  Loic Matthey  Arka Pal  Christopher Burgess  Xavier Glorot  Matthew Botvinick 
Shakir Mohamed  and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations 
2017.

[19] Irina Higgins  Loic Matthey  Arka Pal  Christopher Burgess  Xavier Glorot  Matthew Botvinick 
Shakir Mohamed  and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations 
2017.

[20] Irina Higgins  Arka Pal  Andrei Rusu  Loic Matthey  Christopher Burgess  Alexander Pritzel 
Matthew Botvinick  Charles Blundell  and Alexander Lerchner. Darla: Improving zero-shot
transfer in reinforcement learning. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70  pages 1480–1490. JMLR. org  2017.

[21] Irina Higgins  Nicolas Sonnerat  Loic Matthey  Arka Pal  Christopher P Burgess  Matko
Bosnjak  Murray Shanahan  Matthew Botvinick  Demis Hassabis  and Alexander Lerchner.
Scan: Learning hierarchical compositional visual concepts. arXiv preprint arXiv:1707.03389 
2017.

[22] Irina Higgins  Nicolas Sonnerat  Loic Matthey  Arka Pal  Christopher P Burgess  Matko
Bošnjak  Murray Shanahan  Matthew Botvinick  Demis Hassabis  and Alexander Lerchner.
Scan: Learning hierarchical compositional visual concepts. In International Conference on
Learning Representations  2018.

[23] Stephen James  Paul Wohlhart  Mrinal Kalakrishnan  Dmitry Kalashnikov  Alex Irpan  Julian
Ibarz  Sergey Levine  Raia Hadsell  and Konstantinos Bousmalis. Sim-to-real via sim-to-sim:
Data-efﬁcient robotic grasping via randomized-to-canonical adaptation networks. arXiv preprint
arXiv:1812.07252  2018.

[24] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983 

2018.

[25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International

Conference on Learning Representations  2014.

[26] Tejas D Kulkarni  William F Whitney  Pushmeet Kohli  and Josh Tenenbaum. Deep convolu-
tional inverse graphics network. In Advances in neural information processing systems  pages
2539–2547  2015.

[27] Abhishek Kumar  Prasanna Sattigeri  and Avinash Balakrishnan. Variational inference of
disentangled latent concepts from unlabeled observations. In International Conference on
Learning Representations  2017.

[28] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept

learning through probabilistic program induction. Science  350(6266):1332–1338  2015.

[29] Brenden M Lake  Tomer D Ullman  Joshua B Tenenbaum  and Samuel J Gershman. Building

machines that learn and think like people. Behavioral and Brain Sciences  40  2017.

[30] Adrien Laversanne-Finot  Alexandre Pere  and Pierre-Yves Oudeyer. Curiosity driven explo-
ration of learned disentangled goal spaces. In Conference on Robot Learning  pages 487–504 
2018.

[31] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. nature  521(7553):436 

2015.

[32] Yann LeCun  Fu Jie Huang  Leon Bottou  et al. Learning methods for generic object recognition

with invariance to pose and lighting. In CVPR (2)  pages 97–104. Citeseer  2004.

[33] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their
equivariance and equivalence. In IEEE conference on computer vision and pattern recognition 
pages 991–999  2015.

10

[34] Timothée Lesort  Natalia Díaz-Rodríguez  Jean-Franois Goudou  and David Filliat. State

representation learning for control: An overview. Neural Networks  2018.

[35] Yen-Cheng Liu  Yu-Ying Yeh  Tzu-Chien Fu  Wei-Chen Chiu  Sheng-De Wang  and Yu-
Chiang Frank Wang. Detach and adapt: Learning cross-domain disentangled deep representation.
arXiv preprint arXiv:1705.01314  2017.

[36] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE international conference on computer vision  pages
3730–3738  2015.

[37] Francesco Locatello  Gabriele Abbati  Tom Rainforth  Stefan Bauer  Bernhard Scölkopf 
arXiv preprint

and Olivier Bachem. On the fairness of disentangled representations.
arXiv:1905.13662  2019.

[38] Francesco Locatello  Stefan Bauer  Mario Lucic  Gunnar Raetsch  Sylvain Gelly  Bernhard
Schölkopf  and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In Proceedings of the 36th International Conference on Machine
Learning. PMLR  2019.

[39] Francesco Locatello  Michael Tschannen  Stefan Bauer  Gunnar Rätsch  Bernhard Schölkopf 
and Olivier Bachem. Disentangling factors of variation using few labels. arXiv preprint
arXiv:1905.01258  2019.

[40] Michael F Mathieu  Junbo Jake Zhao  Junbo Zhao  Aditya Ramesh  Pablo Sprechmann  and
Yann LeCun. Disentangling factors of variation in deep representation using adversarial training.
In Advances in Neural Information Processing Systems  pages 5040–5048  2016.

[41] Edvard Munch. The scream  1893.

[42] Ashvin V Nair  Vitchyr Pong  Murtaza Dalal  Shikhar Bahl  Steven Lin  and Sergey Levine. Vi-
sual reinforcement learning with imagined goals. In Advances in Neural Information Processing
Systems  pages 9209–9220  2018.

[43] Siddharth Narayanaswamy  T Brooks Paige  Jan-Willem Van de Meent  Alban Desmaison  Noah
Goodman  Pushmeet Kohli  Frank Wood  and Philip Torr. Learning disentangled representations
with semi-supervised deep generative models. In Advances in Neural Information Processing
Systems  pages 5925–5935  2017.

[44] Judea Pearl. Causality. Cambridge university press  2009.

[45] Jonas Peters  Dominik Janzing  and Bernhard Schölkopf. Elements of causal inference: founda-

tions and learning algorithms. MIT press  2017.

[46] Scott E Reed  Yi Zhang  Yuting Zhang  and Honglak Lee. Deep visual analogy-making. In

Advances in neural information processing systems  pages 1252–1260  2015.

[47] Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the
f-statistic loss. In Advances in Neural Information Processing Systems  pages 185–194  2018.

[48] Jürgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computa-

tion  4(6):863–879  1992.

[49] Bernhard Schölkopf  Dominik Janzing  Jonas Peters  Eleni Sgouritsa  Kun Zhang  and Joris
Mooij. On causal and anticausal learning. In International Conference on Machine Learning 
pages 1255–1262  2012.

[50] P. Spirtes  C. Glymour  and R. Scheines. Causation  prediction  and search. Springer-Verlag.

(2nd edition MIT Press 2000)  1993.

[51] Xander Steenbrugge  Sam Leroux  Tim Verbelen  and Bart Dhoedt. Improving generalization for
abstract reasoning tasks using disentangled feature representations. In Workshop on Relational
Representation Learning at Neural Information Processing Systems  2018.

11

[52] Raphael Suter  Djordje Miladinovic  Bernhard Schölkopf  and Stefan Bauer. Robustly disen-
tangled causal mechanisms: Validating deep representations for interventional robustness. In
Proceedings of the 36th International Conference on Machine Learning. PMLR  2019.

[53] Jakub M Tomczak and Max Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120 

2017.

[54] Sjoerd van Steenkiste  Francesco Locatello  Jürgen Schmidhuber  and Olivier Bachem.
arXiv preprint

Are disentangled representations helpful for abstract visual reasoning?
arXiv:1905.12506  2019.

[55] Nicholas Watters  Loic Matthey  Christopher P Burgess  and Alexander Lerchner. Spatial
broadcast decoder: A simple architecture for learning disentangled representations in vaes.
arXiv preprint arXiv:1901.07017  2019.

[56] Jingwei Zhang  Lei Tai  Peng Yun  Yufeng Xiong  Ming Liu  Joschka Boedecker  and Wolfram
Burgard. Vr-goggles for robots: Real-to-sim domain adaptation for visual control. IEEE
Robotics and Automation Letters  4(2):1148–1155  2019.

12

,Muhammad Waleed Gondal
Manuel Wuthrich
Djordje Miladinovic
Francesco Locatello
Martin Breidt
Valentin Volchkov
Joel Akpo
Olivier Bachem
Bernhard Schölkopf
Stefan Bauer