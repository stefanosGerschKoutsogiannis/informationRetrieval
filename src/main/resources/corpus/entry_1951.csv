2010,Extended Bayesian Information Criteria for Gaussian Graphical Models,Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure  information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso  which is a likelihood penalization technique. In this paper we establish the asymptotic consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case  our treatment allows for growth in the number of non-zero parameters in the true model  which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjuction with the graphical lasso  and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.,Extended Bayesian Information Criteria for Gaussian

Graphical Models

Rina Foygel

University of Chicago

rina@uchicago.edu

Mathias Drton

University of Chicago

drton@uchicago.edu

Abstract

Gaussian graphical models with sparsity in the inverse covariance matrix are of
signiﬁcant interest in many modern applications. For the problem of recovering
the graphical structure  information criteria provide useful optimization objectives
for algorithms searching through sets of graphs or for selection of tuning parame-
ters of other methods such as the graphical lasso  which is a likelihood penaliza-
tion technique. In this paper we establish the consistency of an extended Bayesian
information criterion for Gaussian graphical models in a scenario where both the
number of variables p and the sample size n grow. Compared to earlier work on
the regression case  our treatment allows for growth in the number of non-zero pa-
rameters in the true model  which is necessary in order to cover connected graphs.
We demonstrate the performance of this criterion on simulated data when used in
conjunction with the graphical lasso  and verify that the criterion indeed performs
better than either cross-validation or the ordinary Bayesian information criterion
when p and the number of non-zero parameters q both scale with n.

1

Introduction

This paper is concerned with the problem of model selection (or structure learning) in Gaussian
graphical modelling. A Gaussian graphical model for a random vector X = (X1  . . .   Xp) is de-
termined by a graph G on p nodes. The model comprises all multivariate normal distributions
N(µ  Θ−1) whose inverse covariance matrix satisﬁes that Θjk = 0 when {j  k} is not an edge in G.
For background on these models  including a discussion of the conditional independence interpreta-
tion of the graph  we refer the reader to [1].
In many applications  in particular in the analysis of gene expression data  inference of the graph G is
of signiﬁcant interest. Information criteria provide an important tool for this problem. They provide
the objective to be minimized in (heuristic) searches over the space of graphs and are sometimes
used to select tuning parameters in other methods such as the graphical lasso of [2]. In this work
we study an extended Bayesian information criterion (BIC) for Gaussian graphical models. Given a
sample of n independent and identically distributed observations  this criterion takes the form

BICγ(E) = −2ln( ˆΘ(E)) + |E| log n + 4|E|γ log p 

(1)
where E is the edge set of a candidate graph and ln( ˆΘ(E)) denotes the maximized log-likelihood
function of the associated model. (In this context an edge set comprises unordered pairs {j  k} of
distinct elements in {1  . . .   p}.) The criterion is indexed by a parameter γ ∈ [0  1]; see the Bayesian
If γ = 0  then the classical BIC of [4] is recovered  which is
interpretation of γ given in [3].
well known to lead to (asymptotically) consistent model selection in the setting of ﬁxed number of
variables p and growing sample size n. Consistency is understood to mean selection of the smallest
true graph whose edge set we denote E0. Positive γ leads to stronger penalization of large graphs
and our main result states that the (asymptotic) consistency of an exhaustive search over a restricted

1

model space may then also hold in a scenario where p grows moderately with n (see the Main
Theorem in Section 2). Our numerical work demonstrates that positive values of γ indeed lead to
improved graph inference when p and n are of comparable size (Section 3).
The choice of the criterion in (1) is in analogy to a similar criterion for regression models that was
ﬁrst proposed in [5] and theoretically studied in [3  6]. Our theoretical study employs ideas from
these latter two papers as well as distribution theory available for decomposable graphical models.
As mentioned above  we treat an exhaustive search over a restricted model space that contains all
decomposable models given by an edge set of cardinality |E| ≤ q. One difference to the regression
treatment of [3  6] is that we do not ﬁx the dimension bound q nor the dimension |E0| of the smallest
true model. This is necessary for connected graphs to be covered by our work.
In practice  an exhaustive search is infeasible even for moderate values of p and q. Therefore  we
must choose some method for preselecting a smaller set of models  each of which is then scored
by applying the extended BIC (EBIC). Our simulations show that the combination of EBIC and
graphical lasso gives good results well beyond the realm of the assumptions made in our theoretical
analysis. This combination is consistent in settings where both the lasso and the exhaustive search
are consistent but in light of the good theoretical properties of lasso procedures (see [7])  studying
this particular combination in itself would be an interesting topic for future work.

2 Consistency of the extended BIC for Gaussian graphical models

2.1 Notation and deﬁnitions

In the sequel we make no distinction between the edge set E of a graph on p nodes and the asso-
ciated Gaussian graphical model. Without loss of generality we assume a zero mean vector for all
distributions in the model. We also refer to E as a set of entries in a p × p matrix  meaning the 2|E|
entries indexed by (j  k) and (k  j) for each {j  k} ∈ E. We use ∆ to denote the index pairs (j  j)
for the diagonal entries of the matrix.
Let Θ0 be a positive deﬁnite matrix supported on ∆ ∪ E0. In other words  the non-zero entries
of Θ0 are precisely the diagonal entries as well as the off-diagonal positions indexed by E0; note
that a single edge in E0 corresponds to two positions in the matrix due to symmetry. Suppose the
random vectors X1  . . .   Xn are independent and distributed identically according to N(0  Θ−1
0 ).
Let S = 1
i be the sample covariance matrix. The Gaussian log-likelihood function
n
simpliﬁes to

i XiX T

(cid:80)

ln(Θ) = n
2

[log det(Θ) − trace(SΘ)] .

max = max
σ2

j

(Θ−1

0 )jj.

We introduce some further notation. First  we deﬁne the maximum variance of the individual nodes:

Next  we deﬁne θ0 = mine∈E0 |(Θ0)e|  the minimum signal over the edges present in the graph.
(For edge e = {j  k}  let (Θ0)e = (Θ0)jk = (Θ0)kj.) Finally  we write λmax for the maximum
eigenvalue of Θ0. Observe that the product σ2
maxλmax is no larger than the condition number of Θ0
because 1/λmin(Θ0) = λmax(Θ−1
max.

0 ) ≥ σ2

(2)

(3)

2.2 Main result

Suppose that n tends to inﬁnity with the following asymptotic assumptions on data and model:



E0 is decomposable  with |E0| ≤ q 
maxλmax ≤ C 
σ2
p = O(nκ)  p → ∞ 
γ0 = γ − (1 − 1
(p + 2q) log p × λ2
max
θ2
0

4κ) > 0 

= o(n)

Here C  κ > 0 and γ are ﬁxed reals  while the integers p  q  the edge set E0  the matrix Θ0  and
thus the quantities σ2
max  λmax and θ0 are implicitly allowed to vary with n. We suppress this latter
dependence on n in the notation. The ‘big oh’ O(·) and the ‘small oh’ o(·) are the Landau symbols.

2

Main Theorem. Suppose that conditions (3) hold. Let E be the set of all decomposable models E
with |E| ≤ q. Then with probability tending to 1 as n → ∞ 
E∈E BICγ(E).

E0 = arg min

That is  the extended BIC with parameter γ selects the smallest true model E0 when applied to any
subset of E containing E0.
In order to prove this theorem we use two techniques for comparing likelihoods of different mod-
els. Firstly  in Chen and Chen’s work on the GLM case [6]  the Taylor approximation to the log-
likelihood function is used and we will proceed similarly when comparing the smallest true model
E0 to models E which do not contain E0. The technique produces a lower bound on the decrease in
likelihood when the true model is replaced by a false model.
Theorem 1. Suppose that conditions (3) hold. Let E1 be the set of models E with E (cid:54)⊃ E0 and
|E| ≤ q. Then with probability tending to 1 as n → ∞ 

ln(Θ0) − ln( ˆΘ(E)) > 2q(log p)(1 + γ0) ∀ E ∈ E1.

Secondly  Porteous [8] shows that in the case of two nested models which are both decomposable 
the likelihood ratio (at the maximum likelihood estimates) follows a distribution that can be ex-
pressed exactly as a log product of Beta distributions. We will use this to address the comparison
between the model E0 and decomposable models E containing E0 and obtain an upper bound on
the improvement in likelihood when the true model is expanded to a larger decomposable model.
Theorem 2. Suppose that conditions (3) hold. Let E0 be the set of decomposable models E with
E ⊃ E0 and |E| ≤ q. Then with probability tending to 1 as n → ∞ 

ln( ˆΘ(E)) − ln( ˆΘ(E0)) < 2(1 + γ0)(|E| − |E0|) log p ∀E ∈ E0\{E0}.

Proof of the Main Theorem. With probability tending to 1 as n → ∞  both of the conclusions of
Theorems 1 and 2 hold. We will show that both conclusions holding simultaneously implies the
desired result.
Observe that E ⊂ E0 ∪ E1. Choose any E ∈ E\{E0}. If E ∈ E0  then (by Theorem 2):

BICγ(E) − BICγ(E0) = −2(ln( ˆΘ(E)) − ln( ˆΘ(E0))) + 4(1 + γ0)(|E| − |E0|) log p > 0.

If instead E ∈ E1  then (by Theorem 1  since |E0| ≤ q):

BICγ(E) − BICγ(E0) = −2(ln( ˆΘ(E)) − ln( ˆΘ(E0))) + 4(1 + γ0)(|E| − |E0|) log p > 0.

Therefore  for any E ∈ E\{E0}  BICγ(E) > BICγ(E0)  which yields the desired result.

Some details on the proofs of Theorems 1 and 2 are given in the Appendix in Section 5.

3 Simulations

In this section  we demonstrate that the EBIC with positive γ indeed leads to better model selection
properties in practically relevant settings. We let n grow  set p ∝ nκ for various values of κ  and
apply the EBIC with γ ∈ {0  0.5  1} similarly to the choice made in the regression context by [3]. As
mentioned in the introduction  we ﬁrst use the graphical lasso of [2] (as implemented in the ‘glasso’
package for R) to deﬁne a small set of models to consider (details given below). From the selected
set we choose the model with the lowest EBIC. This is repeated for 100 trials for each combination
of values of n  p  γ in each scaling scenario. For each case  the average positive selection rate (PSR)
and false discovery rate (FDR) are computed.
We recall that the graphical lasso places an (cid:96)1 penalty on the inverse covariance matrix. Given a
penalty ρ ≥ 0  we obtain the estimate

ˆΘρ = arg min

Θ

−ln(Θ) + ρ(cid:107)Θ(cid:107)1.

(4)

3

Figure 1: The chain (top) and the ‘double chain’ (bottom) on 6 nodes.

(Here we may deﬁne (cid:107)Θ(cid:107)1 as the sum of absolute values of all entries  or only of off-diagonal en-
tries; both variants are common). The (cid:96)1 penalty promotes zeros in the estimated inverse covariance
matrix ˆΘρ; increasing the penalty yields an increase in sparsity. The ‘glasso path’  that is  the set
of models recovered over the full range of penalties ρ ∈ [0 ∞)  gives a small set of models which 
roughly  include the ‘best’ models at various levels of sparsity. We may therefore apply the EBIC to
this manageably small set of models (without further restriction to decomposable models). Consis-
tency results on the graphical lasso require the penalty ρ to satisfy bounds that involve measures of
regularity in the unknown matrix Θ0; see [7]. Minimizing the EBIC can be viewed as a data-driven
method of tuning ρ  one that does not require creation of test data.
While cross-validation does not generally have consistency properties for model selection (see [9]) 
it is nevertheless interesting to compare our method to cross-validation. For the considered simulated
data  we start with the set of models from the ‘glasso path’  as before  and then perform 100-fold
cross-validation. For each model and each choice of training set and test set  we ﬁt the model to
the training set and then evaluate its performance on each sample in the test set  by measuring error
in predicting each individual node conditional on the other nodes and then taking the sum of the
squared errors. We note that this method is computationally much more intensive than the BIC or
EBIC  because models need to be ﬁtted many more times.

3.1 Design

In our simulations  we examine the EBIC as applied to the case where the graph is a chain with node
j being connected to nodes j−1  j+1  and to the ‘double chain’  where node j is connected to nodes
j − 2  j − 1  j + 1  j + 2. Figure 1 shows examples of the two types of graphs  which have on the
order of p and 2p edges  respectively. For both the chain and the double chain  we investigate four
different scaling scenarios  with the exponent κ selected from {0.5  0.9  1  1.1}. In each scenario 
we test n = 100  200  400  800  and deﬁne p ∝ nκ with the constant of proportionality chosen such
that p = 10 when n = 100 for better comparability.
In the case of a chain  the true inverse covariance matrix Θ0 is tridiagonal with all diagonal entries
(Θ0)j j set equal to 1  and the entries (Θ0)j j+1 = (Θ0)j+1 j that are next to the main diagonal
equal to 0.3. For the double chain  Θ0 has all diagonal entries equal to 1  the entries next to the main
diagonal are (Θ0)j j+1 = (Θ0)j+1 j = 0.2 and the remaining non-zero entries are (Θ0)j j+2 =
(Θ0)j+2 j = 0.1. In both cases  the choices result in values for θ0  σ2
max and λmax that are bounded
uniformly in the matrix size p.
For each data set generated from N(0  Θ−1
0 )  we use the ‘glasso’ package [2] in R to compute the
‘glasso path’. We choose 100 penalty values ρ which are logarithmically evenly spaced between
ρmax (the smallest value which will result in a no-edge model) and ρmax/100. At each penalty
value ρ  we compute ˆΘρ from (4) and deﬁne the model Eρ based on this estimate’s support. The R
routine also allows us to compute the unpenalized maximum likelihood estimate ˆΘ(Eρ). We may
then readily compute the EBIC from (1). There is no guarantee that this procedure will ﬁnd the
model with the lowest EBIC along the full ‘glasso path’  let alone among the space of all possible
models of size ≤ q. Nonetheless  it serves as a fast way to select a model without any manual tuning.

3.2 Results

Chain graph: The results for the chain graph are displayed in Figure 2. The ﬁgure shows the positive
selection rate (PSR) and false discovery rate (FDR) in the four scaling scenarios. We observe that 
for the larger sample sizes  the recovery of the non-zero coefﬁcients is perfect or nearly perfect for all
three values of γ; however  the FDR rate is noticeably better for the positive values of γ  especially

4

for higher scaling exponents κ. Therefore  for moderately large n  the EBIC with γ = 0.5 or γ = 1
performs very well  while the ordinary BIC0 produces a non-trivial amount of false positives. For
100-fold cross-validation  while the PSR is initially slightly higher  the growing FDR demonstrates
the extreme inconsistency of this method in the given setting.
Double chain graph: The results for the double chain graph are displayed in Figure 3. In each
of the four scaling scenarios for this case  we see a noticeable decline in the PSR as γ increases.
Nonetheless  for each value of γ  the PSR increases as n and p grow. Furthermore  the FDR for the
ordinary BIC0 is again noticeably higher than for the positive values of γ  and in the scaling scenar-
ios κ ≥ 0.9  the FDR for BIC0 is actually increasing as n and p grow  suggesting that asymptotic
consistency may not hold in these cases  as is supported by our theoretical results. 100-fold cross-
validation shows signiﬁcantly better PSR than the BIC and EBIC methods  but the FDR is again
extremely high and increases quickly as the model grows  which shows the unreliability of cross-
validation in this setting. Similarly to what Chen and Chen [3] conclude for the regression case 
it appears that the EBIC with parameter γ = 0.5 performs well. Although the PSR is necessarily
lower than with γ = 0  the FDR is quite low and decreasing as n and p grow  as desired.
For both types of simulations  the results demonstrate the trade-off inherent in choosing γ in the ﬁnite
(non-asymptotic) setting. For low values of γ  we are more likely to obtain a good (high) positive
selection rate. For higher values of γ  we are more likely to obtain a good (low) false discovery
rate. (In the Appendix  this corresponds to assumptions (5) and (6)). However  asymptotically  the
conditions (3) guarantee consistency  meaning that the trade-off becomes irrelevant for large n and
p. In the ﬁnite case  γ = 0.5 seems to be a good compromise in simulations  but the question of
determining the best value of γ in general settings is an open question. Nonetheless  this method
offers guaranteed asymptotic consistency for (known) values of γ depending only on n and p.

4 Discussion

We have proposed the use of an extended Bayesian information criterion for multivariate data gener-
ated by sparse graphical models. Our main result gives a speciﬁc scaling for the number of variables
p  the sample size n  the bound on the number of edges q  and other technical quantities relating to
the true model  which will ensure asymptotic consistency. Our simulation study demonstrates the
the practical potential of the extended BIC  particularly as a way to tune the graphical lasso. The
results show that the extended BIC with positive γ gives strong improvement in false discovery rate
over the classical BIC  and even more so over cross-validation  while showing comparable positive
selection rate for the chain  where all the signals are fairly strong  and noticeably lower  but steadily
increasing  positive selection rate for the double chain with a large number of weaker signals.

5 Appendix

We now sketch proofs of non-asymptotic versions of Theorems 1 and 2  which are formulated as
Theorems 3 and 4. We also give a non-asymptotic formulation of the Main Theorem; see Theorem 5.
In the non-asymptotic approach  we treat all quantities as ﬁxed (e.g. n  p  q  etc.) and state precise
assumptions on those quantities  and then give an explicit lower bound on the probability of the
extended BIC recovering the model E0 exactly. We do this to give an intuition for the magnitude
of the sample size n necessary for a good chance of exact recovery in a given setting but due to the
proof techniques  the resulting implications about sample size are extremely conservative.

5.1 Preliminaries

We begin by stating two lemmas that are used in the proof of the main result  but are also more
generally interesting as tools for precise bounds on Gaussian and chi-square distributions. First  Cai
[10  Lemma 4] proves the following chi-square bound. For any n ≥ 1  λ > 0 

P{χ2

n > n(1 + λ)} ≤ 1
√
λ

πn

e− n

2 (λ−log(1+λ)).

We can give an analagous left-tail upper bound. The proof is similar to Cai’s proof and omitted here.
We will refer to these two bounds together as (CSB).

5

Figure 2: Simulation results when the true graph is a chain.

Lemma 1. For any λ > 0  for n such that n ≥ 4λ−2 + 1 

P{χ2

n < n(1 − λ)} ≤

n−1
2 (λ+log(1−λ)).

e

λ(cid:112)π(n − 1)

1

Second  we give a distributional result about the sample correlation when sampling from a bivariate
normal distribution.
Lemma 2. Suppose (X1  Y1)  . . .   (Xn  Yn) are independent draws from a bivariate normal distri-
bution with zero mean  variances equal to one and covariance ρ. Then the following distributional
equivalence holds  where A and B are independent χ2

n(cid:88)

i=1

(XiYi − ρ)

D
=

1 + ρ

2

(B − n).

Proof. Let A1  B1  A2  B2  . . .   An  Bn be independent standard normal random variables. Deﬁne:

Xi =

(cid:114)1 − ρ

(cid:114)1 + ρ
(cid:114)1 + ρ
2 Ai −
n variables. The claim follows from writing(cid:80)

2 Bi; Yi =

2 Ai +

Then the variables X1  Y1  X2  Y2  . . .   Xn  Yn have the desired joint distribution  and A  B are in-
dependent χ2

i XiYi in terms of A and B.

n(cid:88)

n(cid:88)

2 Bi; A =

A2

i ; B =

B2
i .

i=1

i=1

n variables:
(A − n) − 1 − ρ
2
(cid:114)1 − ρ

6

Figure 3: Simulation results when the true graph is a ‘double chain’.

5.2 Non-asymptotic versions of the theorems
We assume the following two conditions  where 0  1 > 0  C ≥ σ2
γ0 = γ − (1 − 1

4κ):

(p + 2q) log p

× λ2
max
θ2
0

n

2((cid:112)1 + γ0 − 1) − log log p + log(4

≤

√

2 log p

3200 max{1 + γ0 (cid:0)1 + 1

1

(cid:1) C 2}

2

1 + γ0) + 1

≥ 0

Theorem 3. Suppose assumption (5) holds. Then with probability at least 1 −
E (cid:54)⊃ E0 with |E| ≤ q 

ln(Θ0) − ln( ˆΘ(E)) > 2q(log p)(1 + γ0).

maxλmax  κ = logn p  and

(5)

(6)
π log p p−1  for all
1√

Proof. We sketch a proof along the lines of the proof of Theorem 2 in [6]  using Taylor series
centered at the true Θ0 to approximate the likelihood at ˆΘ(E). The score and the negative Hessian
of the log-likelihood function in (2) are

sn(Θ) = d

dΘ ln(Θ) = n
2

Hn(Θ) = − d

dΘ sn(Θ) = n
2

Θ−1 ⊗ Θ−1.

Here  the symbol ⊗ denotes the Kronecker product of matrices. Note that  while we require Θ to be
symmetric positive deﬁnite  this is not reﬂected in the derivatives above. We adopt this convention
for the notational convenience in the sequel.

(cid:0)Θ−1 − S(cid:1)  

7

Next  observe that ˆΘ(E) has support on ∆∪ E0 ∪ E  and that by deﬁnition of θ0  we have the lower
bound | ˆΘ(E) − Θ0|F ≥ θ0 in terms of the Frobenius norm. By concavity of the log-likelihood
function  it sufﬁces to show that the desired inequality holds for all Θ with support on ∆ ∪ E0 ∪ E
with |Θ − Θ0|F = θ0. By Taylor expansion  for some ˜Θ on the path from Θ0 to Θ  we have:
vec(Θ − Θ0)T Hn( ˜Θ)vec(Θ − Θ0).

ln(Θ) − ln(Θ0) = vec(Θ − Θ0)T sn(Θ0) − 1
2

Next  by (CSB) and Lemma 2  with probability at least 1 −
π log p e−1 log p  the following bound
1√
holds for all edges e in the complete graph (we omit the details):

(sn(Θ0))2

e ≤ 6σ4

max(2 + 1)n log p.

Now assume that this bound holds for all edges. Fix some E as above  and ﬁx Θ with support on
∆ ∪ E0 ∪ E  with |Θ − Θ0| = θ0. Note that the support has at most (p + 2q) entries. Therefore 

|vec(Θ − Θ0)T sn(Θ0)|2 ≤ θ2

0(p + 2q) × 6σ4

max(2 + 1)n log p.

ln(Θ) − ln(Θ0) ≤(cid:113)

Furthermore  the eigenvalues of Θ are bounded by λmax + θ0 ≤ 2λmax  and so by properties of
2 (2λmax)−2. We conclude that
Kronecker products  the minimum eigenvalue of Hn( ˜Θ) is at least n

0(p + 2q) × 6σ4
θ2

0 × n
2
Combining this bound with our assumptions above  we obtain the desired result.
Theorem 4. Suppose additionally that assumption (6) holds (in particular  this implies that γ >
1 − 1
p−0
1−p−0   for all decomposable models E such
that E (cid:41) E0 and |E| ≤ q 

4κ ). Then with probability at least 1 −

max(2 + 1)n log p − 1
2 θ2

(2λmax)−2.

√
4

π log p

1

ln( ˆΘ(E)) − ln( ˆΘ(E0)) < 2(1 + γ0)(|E| − |E0|) log p.

2 log ((cid:81)m
Proof. First  ﬁx a single such model E  and deﬁne m = |E| − |E0|. By [8  11]  ln( ˆΘ(E)) −
ln( ˆΘ(E0)) is distributed as − n
2) are independent random
  1
graph given by model E  implying ci ≤ √
variables and the constants c1  . . .   cm are bounded by 1 less than the maximal clique size of the
2q for each i. Also shown in [8] is the stochastic inequality
− log(Bi) ≤

i=1 Bi)  where Bi ∼ Beta( n−ci

1

2

n−ci−1 χ2

1. It follows that  stochastically 
ln( ˆΘ(E)) − ln( ˆΘ(E0)) ≤ n
×
2

n − √

1
2q − 1 χ2
m.

Finally  combining the assumptions on n  p  q and the (CSB) inequalities  we obtain:

P{ln( ˆΘ(E)) − ln( ˆΘ(E0)) ≥ 2(1 + γ0)m log(p)} ≤

√

1
π log p

4

e− m

2 (4(1+ 0

2 ) log p).

Next  note that the number of models |E| with E ⊃ E0 and |E| − |E0| = m is bounded by p2m.
Taking the union bound over all choices of m and all choices of E with that given m  we obtain that
the desired result holds with the desired probability.

We are now ready to give a non-asymptotic version of the Main Theorem. For its proof apply the
union bound to the statements in Theorems 3 and 4  as in the asymptotic proof given in section 2.
Theorem 5. Suppose assumptions (5) and (6) hold. Let E be the set of subsets E of edges be-
tween the p nodes  satisfying |E| ≤ q and representing a decomposable model. Then it holds with
probability at least 1 −

1

√
4

π log p

1−p−0 −
p−0

π log p p−1 that
1√
E∈E BICγ(E).

E0 = arg min

That is  the extended BIC with parameter γ selects the smallest true model.

Finally  we note that translating the above to the asymptotic version of the result is simple. If the
conditions (3) hold  then for sufﬁciently large n (and thus sufﬁciently large p)  assumptions (5) and
(6) hold. Furthermore  although we may not have the exact equality κ = logn p  we will have
logn p → κ; this limit will be sufﬁcient for the necessary inequalities to hold for sufﬁciently large
n. The proofs then follow from the non-asymptotic results.

8

References

[1] Steffen L. Lauritzen. Graphical models  volume 17 of Oxford Statistical Science Series. The

Clarendon Press Oxford University Press  New York  1996. Oxford Science Publications.

[2] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Sparse inverse covariance estimation

with the graphical lasso. Biostatistics  9(3):432–441  2008.

[3] Jiahua Chen and Zehua Chen. Extended Bayesian information criterion for model selection

with large model space. Biometrika  95:759–771  2008.

[4] Gideon Schwarz. Estimating the dimension of a model. Ann. Statist.  6(2):461–464  1978.
[5] Malgorzata Bogdan  Jayanta K. Ghosh  and R. W. Doerge. Modifying the Schwarz Bayesian
information criterion to locate multiple interacting quantitative trait loci. Genetics  167:989–
999  2004.

[6] Jiahua Chen and Zehua Chen. Extended BIC for small-n-large-p sparse GLM. Preprint.
[7] Pradeep Ravikumar  Martin J. Wainwright  Garvesh Raskutti  and Bin Yu.

High-
dimensional covariance estimation by minimizing (cid:96)1-penalized log-determinant divergence.
arXiv:0811.3628  2008.

[8] B. T. Porteous. Stochastic inequalities relating a class of log-likelihood ratio statistics to their

asymptotic χ2 distribution. Ann. Statist.  17(4):1723–1734  1989.

[9] Jun Shao. Linear model selection by cross-validation. J. Amer. Statist. Assoc.  88(422):486–

494  1993.

[10] T. Tony Cai. On block thresholding in wavelet regression: adaptivity  block size  and threshold

level. Statist. Sinica  12(4):1241–1273  2002.

[11] P. Svante Eriksen. Tests in covariance selection models. Scand. J. Statist.  23(3):275–284 

1996.

9

,Fabian Pedregosa
Rémi Leblond
Simon Lacoste-Julien
Aravind Rajeswaran
Chelsea Finn
Sham Kakade
Sergey Levine