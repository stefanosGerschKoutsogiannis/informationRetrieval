2014,Bayesian Inference for Structured Spike and Slab Priors,Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation  the structured spike and slab prior  which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus  prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore  we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data  we demonstrate the benefits of the model.,Bayesian Inference for Structured Spike and

Slab Priors

Michael Riis Andersen  Ole Winther & Lars Kai Hansen

DTU Compute  Technical University of Denmark

DK-2800 Kgs. Lyngby  Denmark

{miri  olwi  lkh}@dtu.dk

Abstract

Sparse signal recovery addresses the problem of solving underdetermined
linear inverse problems subject to a sparsity constraint. We propose a novel
prior formulation  the structured spike and slab prior  which allows to in-
corporate a priori knowledge of the sparsity pattern by imposing a spatial
Gaussian process on the spike and slab probabilities. Thus  prior informa-
tion on the structure of the sparsity pattern can be encoded using generic
covariance functions. Furthermore  we provide a Bayesian inference scheme
for the proposed model based on the expectation propagation framework.
Using numerical experiments on synthetic data  we demonstrate the bene-
ﬁts of the model.

1 Introduction

Consider a linear inverse problem of the form:

y = Ax + e 

(1)
where A ∈ RN×D is the forward matrix  y ∈ RN is the measurement vector  x ∈ RD
is the desired solution and e ∈ RN is a vector of corruptive noise. The ﬁeld of sparse
signal recovery deals with the task of reconstructing the sparse solution x from (A  y) in
the ill-posed regime where N < D.
In many applications it is beneﬁcial to encourage a
structured sparsity pattern rather than independent sparsity. In this paper we consider a
model for exploiting a priori information on the sparsity pattern  which has applications
in many diﬀerent ﬁelds  e.g.  structured sparse PCA [1]  background subtraction [2] and
neuroimaging [3].

In the framework of probabilistic modelling sparsity can be enforced using so-called sparsity
promoting priors  which conventionally has the following form

p(x(cid:12)(cid:12)λ) =

D(cid:89)

i=1

(cid:12)(cid:12)λ) 

p(xi

(cid:12)(cid:12)λ) is the marginal prior on xi and λ is a ﬁxed hyperparameter controlling the

where p(xi
degree of sparsity. Examples of such sparsity promoting priors include the Laplace prior
(LASSO [4])  and the Bernoulli-Gaussian prior (the spike and slab model [5]). The main
advantage of this formulation is that the inference schemes become relatively simple due to
the fact that the prior factorizes over the variables xi. However  this fact also implies that
the models cannot encode any prior knowledge of the structure of the sparsity pattern.

(2)

One approach to model a richer sparsity structure is the so-called group sparsity ap-
proach  where the set of variables x has been partitioned into groups beforehand. This

1

approach has been extensively developed for the (cid:96)1 minimization community  i.e. group
LASSO  sparse group LASSO [6] and graph LASSO [7]. Let G be a partition of the set of
variables into G groups. A Bayesian equivalent of group sparsity is the group spike and
slab model [8]  which takes the form

p(x(cid:12)(cid:12)z) =

G(cid:89)

(cid:2)(1 − zg) δ (xg) + zgN(cid:0)xg

(cid:12)(cid:12)0  τ Ig

(cid:3)  

p(z(cid:12)(cid:12)λ(cid:1) =

G(cid:89)

Bernoulli(cid:0)zg

(cid:12)(cid:12)λg

(cid:1)  

(3)

g=1

g=1

where z ∈ [0  1]G are binary support variables indicating whether the variables in diﬀerent
groups are active or not. Other relevant work includes [9  10  11] and [12]. Another more
ﬂexible approach is to use a Markov random ﬁeld (MRF) as prior for the binary variables
[2].

Related to the MRF-formulation  we propose a novel model called the Structured Spike and
Slab model. This model allows us to encode a priori information of the sparsity pattern into
the model using generic covariance functions rather than through clique potentials as for
the MRF-formulation [2]. Furthermore  we provide a Bayesian inference scheme based on
expectation propagation for the proposed model.

2 The structured spike and slab prior

We propose a hierarchical prior of the following form:

p(x(cid:12)(cid:12)γ) =

D(cid:89)

i=1

(cid:12)(cid:12)g(γi)) 

p(xi

p(γ) = N(cid:0)γ(cid:12)(cid:12)µ0  Σ0

(cid:1)  

(4)

where g : R → R is a suitable injective transformation. That is  we impose a Gaussian
process [13] as a prior on the parameters γi. Using this parametrization  prior knowledge
of the structure of the sparsity pattern can be encoded using µ0 and Σ0. The mean value
µ0 controls the prior belief of the support and the covariance matrix determines the prior
correlation of the support. In the remainder of this paper we restrict p(xi|g(γi)) to be a
spike and slab model  i.e.

(cid:1)  

(cid:12)(cid:12)zi) = (1 − zi)δ(xi) + ziN(cid:0)xi
p(zi = 1(cid:12)(cid:12)γi)p(γi)dγi =

(cid:12)(cid:12)0  τ0
φ(γi)N(cid:0)γi

(cid:90)

p(xi

(cid:90)

p(zi = 1) =

zi ∼ Ber (g(γi)) .

(5)

(cid:12)(cid:12)µi  Σii

(cid:1) dγi = φ

(cid:18)

(cid:19)

µi√
1 + Σii

.

(6)

This formulation clearly ﬁts into eq. (4) when zi is marginalized out. Furthermore  we will
assume that g is the standard Normal CDF  i.e. g(x) = φ(x). Using this formulation  the
marginal prior probability of the i’th weight being active is given by:

This implies that the probability of zi = 1 is 0.5 when µi = 0 as expected. In contrast
to the (cid:96)1-based methods and the MRF-priors  the Gaussian process formulation makes
it easy to generate samples from the model. Figures 1(a)  1(b) each show three real-
izations of the support from the prior using a squared exponential kernel of the form:
Σij = 50 exp(− (i − j)2 /2s2) and µi is ﬁxed such that the expected level of sparsity is
10%. It is seen that when the scale  s  is small  the support consists of scattered spikes.
As the scale increases  the support of the signals becomes more contiguous and clustered 
where the sizes of the clusters increase with the scale.

To gain insight into the relationship between γ and z  we consider the two dimensional
system with µi = 0 and the following covariance structure

(cid:20)1 ρ

(cid:21)

ρ 1

Σ0 = κ

 

κ > 0.

(7)

The correlation between z1 and z2 is then computed as a function of ρ and κ by sampling.
The resulting curves in Figure 1(c) show that the desired correlation is an increasing function
of ρ as expected. However  the ﬁgure also reveals that for ρ = 1  i.e. 100% correlation
between the γ parameters  does not imply 100% correlation of the support variables z. This

2

(a) Scale s = 0.1

(b) Scale s = 5

(c) Correlation of support

 

Figure 1: (a b) Realizations of the support z from the prior distribution using a squared
exponential covariance function for γ  i.e. Σij = 50 exp(−(i − j)2/2s2) and µ is ﬁxed to
match an expected sparsity rate K/D of 10%. (c) Correlation of z1 and z2 as a function
of ρ for 5 diﬀerent values of A obtained by sampling. This prior mean function is ﬁxed at
µi = 0 for all i.

is due to the fact that there are two levels of uncertainty in the prior distribution of the
support. That is  ﬁrst we sample γ  and then we sample the support z conditioned on γ.

The proposed prior formulation extends easily to the multiple measurement vector (MMV)
formulation [14  15  16]  in which multiple linear inverse problems are solved simultaneously.
The most straightforward way is to assume all problem instances share the same support
variable  commonly known as joint sparsity [16]

t=1

i=1

(8)

p(zi

(cid:12)(cid:12)0  τ(cid:1)(cid:3)  

i) + ziN(cid:0)xt

D(cid:89)
T(cid:89)
p(cid:0)X(cid:12)(cid:12)z(cid:1) =
(cid:2)(1 − zi)δ(xt
(cid:12)(cid:12)φ(γi)(cid:1)  
(cid:12)(cid:12)γi) = Ber(cid:0)zi
p(γ) = N(cid:0)γ(cid:12)(cid:12)µ0  Σ0
(cid:1)  
. . . xT(cid:3) ∈ RD×T . The model can also be extended to problems  where
D(cid:89)
T(cid:89)
p(cid:0)X(cid:12)(cid:12)z(cid:1) =
(cid:2)(1 − zt
(cid:12)(cid:12)φ(γt
(cid:12)(cid:12)γt
i )(cid:1)  
i ) = Ber(cid:0)zt
(cid:1) T(cid:89)
(cid:12)(cid:12)µ0  Σ0
p(γ1  ...  γT ) = N(cid:0)γ1

iN(cid:0)xt
(cid:12)(cid:12)(1 − α)µ0 + αγt−1  βΣ0

(cid:12)(cid:12)0  τ(cid:1)(cid:3)  

N(cid:0)γt

i) + zt

(cid:1)  

i )δ(xt

p(zt
i

(11)

(12)

(13)

(10)

(9)

t=1

i=1

i

i

i

where X =(cid:2)x1

the sparsity pattern changes in time

where the parameters 0 ≤ α ≤ 1 and β ≥ 0 controls the temporal dynamics of the support.

t=2

3 Bayesian inference using expectation propagation

distribution of interest thus becomes

In this section we combine the structured spike and slab prior as given in eq.
(5) with
an isotropic Gaussian noise model and derive an inference algorithm based on expectation

propagation. The likelihood function is p(y(cid:12)(cid:12)x) = N(cid:0)y(cid:12)(cid:12)Ax  σ2
0I(cid:1) and the joint posterior
p(x  z  γ(cid:12)(cid:12)y) =
p(y(cid:12)(cid:12)x)p(x(cid:12)(cid:12)z)p(z(cid:12)(cid:12)γ)p(γ)
D(cid:89)
D(cid:89)
N(cid:0)γ(cid:12)(cid:12)µ0  Σ0
N(cid:0)y(cid:12)(cid:12)Ax  σ2
(cid:12)(cid:12)φ (γi)(cid:1)
(cid:1)
0I(cid:1)
Ber(cid:0)zi
(cid:124)
(cid:124)
(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:124)
(cid:123)(cid:122)
(cid:125)

(cid:2)(1 − zi)δ(xi) + ziN(cid:0)xi

(cid:12)(cid:12)0  τ0

(cid:1)(cid:3)
(cid:125)

(14)

1
Z

1
Z

i=1

i=1

=

f4

f1

 

f2

f3

3

00.20.40.60.8100.20.40.60.81ρ = Correlation of γ1 and γ2Correlation of z1 and z2 κ = 1.0κ = 10.0κ = 10000.0where Z is the normalization constant independent of x  z and γ. Unfortunately  the true
posterior is intractable and therefore we have to settle for an approximation. In particular 
we apply the framework of expectation propagation (EP) [17  18]  which is an iterative
deterministic framework for approximating probability distributions using distributions from
the exponential family. The algorithm proposed here can be seen as an extension of the
work in [8].

As shown in eq. (14)  the true posterior is a composition of 4 factors  i.e. fa for a = 1  ..  4.
The terms f2 and f3 are further decomposed into D conditionally independent factors

D(cid:89)
D(cid:89)

i=1

f2(x  z) =

f3(z  γ) =

f2 i(xi  zi) =

f3 i(zi  γi) =

i=1

i=1

(cid:12)(cid:12)0  τ0

(cid:1)(cid:3)  

D(cid:89)
D(cid:89)

i=1

(cid:12)(cid:12)φ (γi)(cid:1)

(cid:2)(1 − zi)δ(xi) + ziN(cid:0)xi
Ber(cid:0)zi
4(cid:89)

˜fa (x  z  γ) .

Q (x  z  γ) =

1

ZEP

a=1

The idea is then to approximate each term in the true posterior density  i.e. fa  by simpler
terms  i.e. ˜fa for a = 1  ..  4. The resulting approximation Q (x  z  γ) then becomes

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(cid:12)(cid:12)φ (˜γ2 i)(cid:1)  
(cid:12)(cid:12)φ (˜γ2 i)(cid:1)  

˜m1 = 1

The terms ˜f1 and ˜f4 can be computed exact. In fact  ˜f4 is simply equal to the prior over
γ and ˜f1 is a multivariate Gaussian distribution with mean ˜m1 and covariance matrix
˜V1 determined by ˜V −1
σ2 AT A. Therefore  we only have to
approximate the factors ˜f2 and ˜f3 using EP. Note that the exact term f1 is a distribution
of y conditioned on x  whereas the approximate term ˜f1 is a function of x that depends
on y through ˜m1 and ˜V1 etc. In order to take full advantage of the structure of the true
posterior distribution  we will further assume that the terms ˜f2 and ˜f3 also are decomposed
into D independent factors.

σ2 AT y and ˜V −1

1 = 1

1

The EP scheme provides great ﬂexibility in the choice of the approximating factors. This
choice is a trade-oﬀ between analytical tractability and suﬃcient ﬂexibility for capturing the
important characteristics of the true density. Due to the product over the binary support
variables {zi} for i = 1  ..  D  the true density is highly multimodal. Finally  f2 couples the
variables x and z  while f3 couples the variables z and γ. Based on these observations  we
choose ˜f2 and ˜f3 to have the following forms

where ˜m2 = [ ˜m2 1  ..  ˜m2 D]T   ˜V2 = diag (˜v2 1  ...  ˜v2 D) and analogously for ˜µ3 and ˜Σ3.
These choices lead to a joint variational approximation Q(x  z  γ) of the form

˜f2 (x  z) ∝ D(cid:89)
˜f3 (z  γ) ∝ D(cid:89)

i=1

i=1

i=1

i=1

i=1

(cid:12)(cid:12)φ (˜γ2 i)(cid:1) = N(cid:16)
(cid:17) D(cid:89)
(cid:1) D(cid:89)
x(cid:12)(cid:12) ˜m2  ˜V2
(cid:12)(cid:12) ˜m2 i  ˜v2 i
Ber(cid:0)zi
Ber(cid:0)zi
N(cid:0)xi
(cid:17) D(cid:89)
(cid:1) = N(cid:16)
(cid:12)(cid:12)φ (˜γ3 i)(cid:1) D(cid:89)
γ(cid:12)(cid:12) ˜µ3  ˜Σ3
(cid:12)(cid:12)˜µ3 i  ˜σ3 i
Ber(cid:0)zi
N(cid:0)γi
Ber(cid:0)zi
Q (x  z  γ) = N(cid:16)
(cid:17) D(cid:89)
(cid:12)(cid:12)g (˜γi)(cid:1)N(cid:16)
(cid:17)
x(cid:12)(cid:12) ˜m  ˜V
γ(cid:12)(cid:12) ˜µ  ˜Σ
Ber(cid:0)zi
(cid:16) ˜V −1
(cid:16) ˜V −1
(cid:17)−1
(cid:17)
(cid:17)
(cid:16) ˜Σ−1
(cid:17)−1
(cid:16) ˜Σ−1
1 + ˜V −1
(cid:34)(cid:18) (1 − φ(˜γ2 j)) (1 − φ(˜γ3 j))
(cid:19)−1(cid:35)
3 + ˜Σ−1

4 ˜µ4
∀j ∈ {1  ..  D} .

3 ˜µ3 + ˜Σ−1

˜m1 + ˜V −1

˜m = ˜V

˜µ = ˜Σ

˜m2

+ 1

i=1

i=1

2

4

 

 

1

2

 

 

φ(˜γ2 j)φ(˜γ3 j)

˜V =

˜Σ =

˜γj = φ−1

where the joint parameters are given by

where φ−1(x) is the probit function. The function in eq. (21) amounts to computing the
product of two Bernoulli densities parametrized using φ (·).

4

• Initialize approximation terms ˜fa for a = 1  2  3  4 and Q
• Repeat until stopping criteria

– For each ˜f2 i:

∗ Compute cavity distribution: Q\2 i ∝ Q
˜f2 i

∗ Minimize: KL(cid:0)f2 iQ\2 i(cid:12)(cid:12)(cid:12)(cid:12)Q2 new(cid:1) w.r.t. Qnew

∗ Compute: ˜f2 i ∝ Q2 new

Q\2 i to update parameters ˜m2 i  ˜v2 i and ˜γ2 i.

– Update joint approximation parameters: ˜m  ˜V and ˜γ
– For each ˜f3 i:

∗ Compute cavity distribution: Q\3 i ∝ Q
˜f3 i

∗ Minimize: KL(cid:0)f3 iQ\3 i(cid:12)(cid:12)(cid:12)(cid:12)Q3 new(cid:1) w.r.t. Qnew

∗ Compute: ˜f3 i ∝ Q3 new

Q\3 i to update parameters ˜µ3 i  ˜σ3 i and ˜γ3 i

– Update joint approximation parameters: ˜µ  ˜Σ and ˜γ

Figure 2: Proposed algorithm for approximating the joint posterior distribution over x  z
and γ.

3.1 The EP algorithm

Consider the update of the term ˜fa i for a given a and a given i  where ˜fa =(cid:81)

˜fa i. This
update is performed by ﬁrst removing the contribution of ˜fa i from the joint approximation
by forming the so-called cavity distribution

i

Q\a i ∝ Q
˜fa i

(22)

followed by the minimization of the Kullbach-Leibler [19] divergence between fa iQ\a i and
Qa new w.r.t. Qa new. For distributions within the exponential family  minimizing this form
of KL divergence amounts to matching moments between fa iQ\a i and Qa new [17]. Finally 
the new update of ˜fa i is given by

˜fa i ∝ Qa new
Q\a i

.

(23)

After all the individual approximation terms ˜fa i for a = 1  2 and i = 1  ..  D have been
updated  the joint approximation is updated using eq. (19)-(21). To minimize the compu-
tational load  we use parallel updates of ˜f2 i [8] followed by parallel updates of ˜f3 i rather
than the conventional sequential update scheme. Furthermore  due to the fact that ˜f2 and
˜f3 factorizes  we only need the marginals of the cavity distributions Q\a i and the marginals
of the updated joint distributions Qa new for a = 2  3.

Computing the cavity distributions and matching the moments are tedious  but straight-
forward. The moments of fa iQ\2 i require evaluation of the zeroth  ﬁrst and second order

moment of the distributions of the form φ(γi)N(cid:0)γi

(cid:1). Derivation of analytical ex-

(cid:12)(cid:12)µi  Σii

pressions for these moments can be found in [13]. See the supplementary material for more
details. The proposed algorithm is summarized in ﬁgure 2. Note  that the EP framework
also provides an approximation of the marginal likelihood [13]  which can be useful for
learning the hyperparameters of the model. Furthermore  the proposed inference scheme
can easily be extended to the MMV formulation eq. (8)-(10) by introducing a ˜f t
2 i for each
time step t = 1  ..  T .

5

3.2 Computational details

Most linear inverse problems of practical interest are high dimensional  i.e. D is large. It is
therefore of interest to simplify the computational complexity of the algorithm as much as
possible. The dominating operations in this algorithm are the inversions of the two D × D

covariance matrices in eq. (19) and eq. (20)  and therefore the algorithm scales as O(cid:0)D3(cid:1).

But ˜V1 has low rank and ˜V2 is diagonal  and therefore we can apply the Woodbury matrix
identity [20] to eq. (19) to get

˜V = ˜V2 − ˜V2AT(cid:16)

oI + A ˜V2AT(cid:17)−1

(cid:16)

For N < D  this scales as O(cid:0)N D2(cid:1)  where N is the number of observations. Unfortunately 

A ˜V2.

(24)

σ2

we cannot apply the same identity to the inversion in eq. (20) since ˜Σ4 has full rank and
is non-diagonal in general. The eigenvalue spectrum of many prior covariance structures of
interest  i.e. simple neighbourhoods etc.  decay relatively fast. Therefore  we can approx-
imate Σ0 with a low rank approximation Σ0 ≈ P ΛP T   where Λ ∈ RR×R is a diagonal
matrix of the R largest eigenvalues and P ∈ RD×R is the corresponding eigenvectors. Using
the R-rank approximation  we can now invoke the Woodbury matrix identity again to get:

Similarly  for R < D  this scales as O(cid:0)RD2(cid:1). Another better approach that preserves the

˜Σ = ˜Σ3 − ˜Σ3P

Λ + P T ˜Σ3P

P T ˜Σ3.

(25)

total variance would be to use probabilistic PCA [21] to approximate Σ0. A third alternative
is to consider other structures for Σ0  which facilitate fast matrix inversions such as block
structures and Toeplitz structures. Numerical issues can arise in EP implementations and
in order to avoid this  we use the same precautions as described in [8].

4 Numerical experiments

This section describes a series of numerical experiments that have been designed and con-
ducted in order to investigate the properties of the proposed algorithm.

4.1 Experiment 1

(cid:17)−1

The ﬁrst experiment compares the proposed method to the LARS algorithm [22] and to
the BG-AMP method [23]  which is an approximate message passing-based method for the
spike and slab model. We also compare the method to an ”oracle least squares estimator”
that knows the true support of the solutions. We generate 100 problem instances from
y = Ax0 + e  where the solutions vectors have been sampled from the proposed prior using
the kernel Σi j = 50 exp(−||i − j||2
2/(2 · 102))  but constrained to have a ﬁxed sparsity level
of the K/D = 0.25. That is  each solution x0 has the same number of non-zero entries 
but diﬀerent sparsity patterns. We vary the degree of undersampling from N/D = 0.05 to
N/D = 0.95. The elements of A ∈ RN×250 are i.i.d Gaussian and the columns of A have
been scaled to unit (cid:96)2-norm. The SNR is ﬁxed at 20dB. We apply the four methods to each
of the 100 problems  and for each solution we compute the Normalized Mean Square Error
(NMSE) between the true signal x0 and the estimated signal ˆx as well as the F -measure:

NMSE =

||x0 − ˆx||2
||x0||2

F = 2

precision · recall
precision + recall

 

(26)

where precision and recall are computed using a MAP estimate of the support. For the
structured spike and slab method  we consider three diﬀerent covariance structures: Σij =
κ · δ(i − j)  Σij = κ exp(−||i − j||2/s) and Σij = κ exp(−||i − j||2
2/(2s2)) with parameters
κ = 50 and s = 10. In each case  we use a R = 50 rank approximation of Σ. The average
results are shown in ﬁgures 3(a)-(f). Figure (a) shows an example of one of the sampled
vectors x0 and ﬁgure (b) shows the three covariance functions.

From ﬁgure 3(c)-(d)  it is seen that the two EP methods with neighbour correlation are
able to improve the phase transition point. That is  in order to obtain a reconstruction

6

(a) Example signal

(b) Covariance functions

(c) NMSE

(d) F-measure

(e) Run times

(f) Iterations

Figure 3: Illustration of the beneﬁt of modelling the additional structure of the sparsity
pattern. 100 problem instances are generated using the linear measurement model y =
Ax + e  where elements of A ∈ RN×250 are i.i.d Gaussian and the columns are scaled to
unit (cid:96)2-norm. The solutions x0 are sampled from the prior in eq. (5) with hyperparameters

Σij = 50 exp(cid:2)−||i − j||2 /(cid:0)2 · 102(cid:1)(cid:3) and a ﬁxed level of sparsity of K/D = 0.25. For EP

methods  the Σ0 matrix is approximated using a rank 50 matrix. SNR is ﬁxed at 20dB.

of the signal such that F ≈ 0.8  EP with diagonal covariance and BG-AMP need an un-
dersamplingratio of N/D ≈ 0.55  while the EP methods with neighbour correlation only
need N/D ≈ 0.35 to achieve F ≈ 0.8. For this speciﬁc problem  this means that utilizing
the neighbourhood structure allows us to reconstruct the signal with 50 fewer observations.
Note that  the reconstruction using the exponential covariance function does also improve
the result even if the true underlying covariance structure corresponds to a squared exponen-
tial function. Furthermore  we see similar performance of BG-AMP and EP with a diagonal
covariance matrix. This is expected for problems where Aij is drawn iid as assumed in
BG-AMP. However  the price of the improved phase transition is clear from ﬁgure 3(e). The
proposed algorithm has signiﬁcantly higher computational complexity than BG-AMP and
LARS. Figure 4(a) shows the posterior mean of z for the signal shown in ﬁgure 3(a). Here
it is seen that the two models with neighbour correlation provide a better approximation
to the posterior activation probabilities. Figure 4(b) shows the posterior mean of γ for the
model with the squared exponential kernel along with ± one standard deviation.

4.2 Experiment 2

In this experiment we consider an application of the MMV formulation as given in eq. (8)-
(10)  namely EEG source localization with synthetic sources [24]. Here we are interested in
localizing the active sources within a speciﬁc region of interest on the cortical surface (grey
area on ﬁgure 5(a)). To do this  we now generate a problem instance of Y = AEEGX0 +
E using the procedure as described in experiment 1  where AEEG ∈ R128×800 is now a
submatrix of a real EEG forward matrix corresponding to the grey area on the ﬁgure. The
condition number of AEEG is ≈ 8·1015. The true sources X0 ∈ R800×20 are sampled from the
structured spike and slab prior in eq. (8) using a squared exponential kernel with parameters
A = 50  s = 10 and T = 20. The number of active sources is 46  i.e. x has 46 non-zero
rows. SNR is ﬁxed to 20dB. The true sources are shown in ﬁgure 5(a). We now use the EP
algorithm to recover the sources using the true prior  i.e. squared exponential kernel and

7

050100150200250−3−2−10123Signal domainSignal Example signal x−50−40−30−20−10010203040500102030405060||i−j||2cov(||i−j||2) DiagonalExponentialSq. exponential00.20.40.60.8100.20.40.60.81Undersamplingsratio N/DNMSE Oracle LSLARSBG−AMPEP  DiagonalEP  ExponentialEP  Sq. exponential00.20.40.60.8100.20.40.60.81Undersamplingsratio N/DF Oracle LSLARSBG−AMPEP  DiagonalEP  ExponentialEP  Sq. exponential00.20.40.60.8100.511.522.533.5Undersamplingsratio N/DSecond Oracle LSLARSBG−AMPEP  DiagonalEP  ExponentialEP  Sq. exponential00.20.40.60.81050100150200250300Undersamplingsratio N/DIterations EP  DiagonalEP  ExponentialEP  Sq. exponential(a)

(b)

Figure 4: (a) Marginal posterior means over z obtained using the structured spike and slab
model for the signal in ﬁgure 3(a). The experiment set-up is the as described in ﬁgure
3  except the undersamplingsratio is ﬁxed to N/D = 0.5. (b) The posterior mean of γ
superimposed with ± one standard deviation. The green dots indicate the true support.

(a) True sources

(b) EP  Sq. exponential

(c) EP  Diagonal

Figure 5: Source localization using synthetic sources. The A ∈ R128×800 is a submatrix
(grey area) of a real EEG forward matrix. (a) True sources. (b) Reconstruction using the
true prior   Fsq = 0.78. (c) Reconstruction using a diagonal covariance matrix  Fdiag = 0.34.

the results are shown in ﬁgure 5(b). We see that the algorithm detects most of the sources
correctly  even the small blob on the right hand side. However  it also introduces a small
number of false positives in the neighbourhood of the true active sources. The resulting
F -measure is Fsq = 0.78. Figure 5(c) shows the result of reconstructing the sources using a
diagonal covariance matrix  where Fdiag = 0.34. Here the BG-AMP algorithm is expected
to perform poorly due to the heavy violation of the assumption of Aij being Gaussian iid.

4.3 Experiment 3

We have also recreated the Shepp-Logan Phantom experiment from [2] with D = 104 un-
knowns  K = 1723 non-zero weights  N = 2K observations and SNR = 10dB (see sup-
plementary material for more details). The EP method yields Fsq = 0.994 and NMSEsq
= 0.336 for this experiment  whereas BG-AMP yields F = 0.624 and NMSE = 0.717. For
reference  the oracle estimator yields NMSE = 0.326.

5 Conclusion and outlook

We introduced the structured spike and slab model  which allows incorporation of a priori
knowledge of the sparsity pattern. We developed an expectation propagation-based algo-
rithm for Bayesian inference under the proposed model. Future work includes developing
a scheme for learning the structure of the sparsity pattern and extending the algorithm to
the multiple measurement vector formulation with slowly changing support.

8

2040608010012014016018020022024000.10.20.30.40.50.60.70.80.91Signal indexp(zi = 1|y) True supportEP  DiagEP  Exp.EP  Sq. exp50100150200250−10−505Signal indexγi|y ± 1 standard deviationPosterior mean of γ for sq. exp.References

[1] R. Jenatton  G. Obozinski  and F. Bach. Structured sparse principal component analysis. In

AISTATS  pages 366–373  2010.

[2] V. Cevher  M. F. Duarte  C. Hegde  and R. G. Baraniuk. Sparse signal recovery using markov

random ﬁelds. In NIPS  Vancouver  B.C.  Canada  8–11 December 2008.

[3] M. Pontil  L. Baldassarre  and J. Mouro-Miranda. Structured sparsity models for brain decod-
ing from fMRI data. Proceedings - 2012 2nd International Workshop on Pattern Recognition
in NeuroImaging  PRNI 2012  pages 5–8  2012.

[4] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the royal statistical

society series b-methodological  58(1):267–288  1996.

[5] T. J. Mitchell and J. Beauchamp. Bayesian variable selection in linear-regression. Journal of

the American Statistical Association  83(404):1023–1032  1988.

[6] N. Simon  J. Friedman  T. Hastie  and R. Tibshirani. A sparse-group lasso. Journal Of

Computational And Graphical Statistics  22(2):231–245  2013.

[7] G. Obozinski  J. P. Vert  and L. Jacob. Group lasso with overlap and graph lasso. ACM

International Conference Proceeding Series  382:–  2009.

[8] D. Hernandez-Lobato  J. Hernandez-Lobato  and P. Dupont. Generalized spike-and-slab pri-
ors for bayesian group feature selection using expectation propagation. Journal Of Machine
Learning Research  14:1891–1945  2013.

[9] L. Yu  H. Sun  J. P. Barbot  and G. Zheng. Bayesian compressive sensing for cluster structured

sparse signals. Signal Processing  92(1):259 – 269  2012.

[10] M. Van Gerven  B. Cseke  R. Oostenveld  and T. Heskes. Bayesian source localization with the
multivariate laplace prior. In Y. Bengio  D. Schuurmans  J.D. Laﬀerty  C.K.I. Williams  and
A. Culotta  editors  Advances in Neural Information Processing Systems 22  pages 1901–1909.
Curran Associates  Inc.  2009.

[11] J. M. Hernndez-Lobato  D. Hernndez-Lobato  and A. Surez. Network-based sparse bayesian

classiﬁcation. Pattern Recognition  44(4):886–900  2011.

[12] D. Hern´andez-Lobato and J. M. Hern´andez-Lobato. Learning feature selection dependencies
in multi-task learning. In C.J.C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.Q.
Weinberger  editors  Advances in Neural Information Processing Systems 26  pages 746–754.
Curran Associates  Inc.  2013.

[13] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT Press 

2006.

[14] S. F. Cotter  B. D. Rao  K. Engan  and K. Kreutz-delgado. Sparse solutions to linear inverse
problems with multiple measurement vectors. IEEE Trans. Signal Processing  pages 2477–2488 
2005.

[15] D. P. Wipf and B. D. Rao. An empirical bayesian strategy for solving the  simultaneous sparse

approximation problem. IEEE Transactions On Signal Processing  55(7):3704–3716  2007.

[16] J. Ziniel and P. Schniter. Dynamic compressive sensing of time-varying signals via approximate

message passing. IEEE Transactions On Signal Processing  61(21):5270–5284  2013.

[17] T. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the
Seventeenth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-01) 
pages 362–369  San Francisco  CA  2001. Morgan Kaufmann.

[18] M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean-ﬁeld algorithms. Neural

Computation  12(11):2655–2684  2000.

[19] C. M. Bishop. Pattern recognition and machine learning. Springer  2006.
[20] K. B. Petersen and M. S. Pedersen. The matrix cookbook. 2012.
[21] M. E Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the

Royal Statistical Society  Series B  61:611–622  1999.

[22] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of

Statistics  32:407–499  2004.

[23] P. Schniter and J. Vila. Expectation-maximization gaussian-mixture approximate message
passing. 2012 46th Annual Conference on Information Sciences and Systems  CISS 2012 
pages –  2012.

[24] S. Baillet  J. C. Mosher  and R. M. Leahy. Electromagnetic brain mapping.

IEEE Signal

Processing Magazine  18(6):14–30  2001.

9

,Francesco Orabona
Michael Andersen
Ole Winther
Lars Hansen
Corinna Cortes
Giulia DeSalvo
Mehryar Mohri
Bo-Jian Hou
Lijun Zhang
Zhi-Hua Zhou
Horia Mania
Aurelia Guy
Benjamin Recht
Rui Li