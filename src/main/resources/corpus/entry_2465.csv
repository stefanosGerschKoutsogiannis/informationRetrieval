2019,High-Quality Self-Supervised Deep Image Denoising,We describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images  or explicit pairs of corrupted images  and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a "blind spot" in the receptive field  and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise  and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data.,High-Quality Self-Supervised Deep Image Denoising

Samuli Laine
NVIDIA∗

Tero Karras

NVIDIA

Jaakko Lehtinen

NVIDIA  Aalto University

Timo Aila
NVIDIA

Abstract

We describe a novel method for training high-quality image denoising models
based on unorganized collections of corrupted images. The training does not need
access to clean reference images  or explicit pairs of corrupted images  and can
thus be applied in situations where such data is unacceptably expensive or im-
possible to acquire. We build on a recent technique that removes the need for
reference data by employing networks with a “blind spot” in the receptive ﬁeld 
and signiﬁcantly improve two key aspects: image quality and training efﬁciency.
Our result quality is on par with state-of-the-art neural network denoisers in the
case of i.i.d. additive Gaussian noise  and not far behind with Poisson and impulse
noise. We also successfully handle cases where parameters of the noise model are
variable and/or unknown in both training and evaluation data.

1

Introduction

Denoising  the removal of noise from images  is a major application of deep learning. Several
architectures have been proposed for general-purpose image restoration tasks  e.g.  U-Nets [23] 
hierarchical residual networks [20]  and residual dense networks [31]. Traditionally  the models are
trained in a supervised fashion with corrupted images as inputs and clean images as targets  so that
the network learns to remove the corruption.
Lehtinen et al. [17] introduced NOISE2NOISE training  where pairs of corrupted images are used as
training data. They observe that when certain statistical conditions are met  a network faced with
the impossible task of mapping corrupted images to corrupted images learns  loosely speaking  to
output the “average” image. For a large class of image corruptions  the clean image is a simple
per-pixel statistic — such as mean  median  or mode — over the stochastic corruption process  and
hence the restoration model can be supervised using corrupted data by choosing the appropriate loss
function to recover the statistic of interest.
While removing the need for clean training images  NOISE2NOISE training still requires at least two
independent realizations of the corruption for each training image. While this eases data collection
signiﬁcantly compared to noisy-clean pairs  large collections of (single) poor images are still much
more widespread. This motivates investigation of self-supervised training: how much can we learn
from just looking at corrupted data? While foregoing supervision would lead to the expectation of
some regression in performance  can we make up for it by making stronger assumptions about the
corruption process? In this paper  we show that for several noise models that are i.i.d. between pixels
(Gaussian  Poisson  impulse)  only minor concessions in denoising performance are necessary. We
furthermore show that the parameters of the noise models do not need to be known in advance.
We draw inspiration from the recent NOISE2VOID training technique of Krull et al. [14]. The
algorithm needs no image pairs  and uses just individual noisy images as training data  assuming
that the corruption is zero-mean and independent between pixels. The method is based on blind-
spot networks where the receptive ﬁeld of the network does not include the center pixel. This

∗{slaine  tkarras  jlehtinen  taila}@nvidia.com

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Top: In our blind-spot network architecture  we effectively construct four denoiser net-
work branches  each having its receptive ﬁeld restricted to a different direction. A single-pixel offset
at the end of each branch separates the receptive ﬁeld from the center pixel. The results are then
combined by 1×1 convolutions. Bottom: In practice  we run four rotated versions of each input im-
age through a single receptive ﬁeld -restricted branch  yielding a simpler architecture that performs
the same function. This also implicitly shares the convolution kernels between the branches and thus
avoids the four-fold increase in the number of trainable weights.

allows using the same noisy image as both training input and training target — because the network
cannot see the correct answer  using the same image as target is equivalent to using a different noisy
realization. This approach is self-supervised in the sense that the surrounding context is used to
predict the value of the output pixel without a separate reference image [8].
The networks used by Krull et al. [14] do not have a blind spot by design  but are trained to ignore
the center pixel using a masking scheme where only a few output pixels can contribute to the loss
function  reducing training efﬁciency considerably. We remedy this with a novel architecture that
allows efﬁcient training without masking. Furthermore  the existence of the blind spot leads to poor
denoising quality. We derive a scheme for combining the network output with data in the blind
spot  bringing the denoising quality on par with  or at least much closer to  conventionally trained
networks.

2 Convolutional blind-spot network architectures

Our convolutional blind-spot networks are designed by combining multiple branches that each have
their receptive ﬁeld restricted to a half-plane (Figure 1) that does not contain the center pixel. We
combine the four branches with a series of 1×1 convolutions to obtain a receptive ﬁeld that can
extend arbitrarily far in every direction but does not contain the center pixel. The principle of
limiting the receptive ﬁeld has been previously used in PixelCNN [29  28  24] image synthesis
networks  where only pixels synthesized before the current pixel are allowed in the receptive ﬁeld.2
The beneﬁt of our architecture compared to the masking-based training of Krull et al. [14] is that all
output pixels can contribute to the loss function as in conventional training.
In order to transform a restoration network into one with a restricted receptive ﬁeld  we modify
each individual layer so that its receptive ﬁeld is fully contained within one half-plane  including
the center row/column. The receptive ﬁeld of the resulting network includes the center pixel  so we
offset the feature maps by one pixel before combining them. Layers that do not extend the receptive
ﬁeld  e.g.  concatenation  summation  1×1 convolution  etc.  can be used without modiﬁcations.
Convolution layers To restrict the receptive ﬁeld of a zero-padding convolution layer to extend
only  say  upwards  the easiest solution is to offset the feature maps downwards when performing
the convolution operation. For an h × w kernel size  a downwards offset of k = (cid:98)h/2(cid:99) pixels is
equivalent to using a kernel that is shifted upwards so that all weights below the center row are zero.
Speciﬁcally  we ﬁrst append k rows of zeros to the top of input tensor  then perform the convolution 
and ﬁnally crop out the k bottom rows of the output.

2Regrettably the term “blind spot” has a slightly different meaning in PixelCNN literature: van den Oord et
al. [28] use it to denote valid input pixels that the network in question fails to see due to poor design  whereas
we follow the naming convention of Krull et al. [14] so that a blind spot is always intentional.

2

CCCCCCC111CCCCCCC111RR-1Downsampling and upsampling layers Many image restoration networks involve downsampling
and upsampling layers  and by default  these extend the receptive ﬁeld in all directions. Consider 
e.g.  a 2 × 2 average downsampling step followed immediately by a nearest-neighbor 2 × 2 upsam-
pling step. The contents of every 2 × 2 pixel block in the output now correspond to the average of
this block in the input  i.e.  information has been transferred in every direction within the block. We
ﬁx this problem by again applying an offset to the data. It is sufﬁcient to restrict the receptive ﬁeld
for the pair of downsampling and upsampling layers  which means that only one of the layers needs
to be modiﬁed  and we have chosen to attach the offsets to the downsampling layers. For a 2 × 2
average downsampling layer  we can restrict the receptive ﬁeld to extend upwards only by padding
the input tensor with one row of zeros at top and cropping out the bottom row before performing the
actual downsampling operation.

3 Self-supervised Bayesian denoising with blind-spot networks

Consider the prediction of the clean value x for a noisy pixel y. As the pixels in an image are
not independent  all denoising algorithms assume the clean value depends not only on the noisy
measurement y  but also on the context of neighboring (noisy) pixels that we denote by Ωy. For our
convolutional networks  the context corresponds to the receptive ﬁeld sans the central pixel. From
this point of view  denoising can be thought of as statistical inference on the probability distribution
p(x|y  Ωy) over the clean pixel value x conditioned with both the context Ωy and the measurement
y. Concretely  a standard supervised regression model trained with corrupted-clean pairs and L2
loss will return an estimate of Ex[p(x|y  Ωy)]  i.e.  the mean over all possible clean pixel values
given the noisy pixel and its context.
Assuming the noise is independent between pixels and independent of the context  the blind-spot
network introduced by Krull et al. [14] predicts the clean value based purely on the context  using the
noisy measurement y as a training target  drawing on the NOISE2NOISE approach [17]. Concretely 
their regressor learns to estimate Ex[p(x|Ωy)]  i.e.  the mean of all potential clean values consistent
with the context. Batson and Royer [1] present an elegant general formulation for self-supervised
models like this. However  methods that ignore the corrupted measurement y at test-time clearly
leave useful information unused  potentially leading to reduced performance.
We bring in extra information in the form of an explicit model of the corruption  provided as a
likelihood p(y|x) of the observation given the clean value  which we assume to be independent of
the context and i.i.d. between pixels. This allows us to connect the observed marginal distribution
of the noisy training data to the unobserved distribution of clean data:

(cid:90)

p(y|Ωy)

(cid:124) (cid:123)(cid:122) (cid:125)

Training data

=

p(y|x)

(cid:124) (cid:123)(cid:122) (cid:125)

Noise model

p(x|Ωy)

(cid:124) (cid:123)(cid:122) (cid:125)

Unobserved

dx

(1)

This functional relationship suggests that even though we only observe corrupted training data  the
known noise model should help us learn to predict a parametric model for the distribution p(x|Ωy).
Speciﬁcally  we model p(x|Ωy) as a multivariate Gaussian N (µx  Σx) over color components. For
many noise models  the marginal likelihood p(y|Ωy) can then be computed in closed form  allowing
us to train a neural network to map the context Ωy to the mean µx and covariance Σx by maximizing
the likelihood of the data under Equation (1).
The approximate distribution p(x|Ωy) allows us to now apply Bayesian reasoning to include infor-
mation from y at test-time. Speciﬁcally  the (unnormalized) posterior probability of the clean value
x given observations of both the noisy pixel y and its context is given by Bayes’ rule as follows:

(cid:124)

(cid:123)(cid:122)

p(x|y  Ωy)

Posterior

(cid:125)

∝ p(y|x)

(cid:124) (cid:123)(cid:122) (cid:125)

Noise model

p(x|Ωy)

(cid:124) (cid:123)(cid:122) (cid:125)

Prior

(2)

From this point of view  the distribution p(x|Ωy) takes the role of the prior  encoding our beliefs on
the possible xs before observing y. (Note that even though we represent the prior as a Gaussian 
the posterior is generally not Gaussian due to the multiplication with the noise likelihood.) With the
posterior at hand  standard Bayesian inference tools become available: for instance  a maximum a
posteriori (MAP) estimate would pick the x that maximizes the posterior; we use the posterior mean
Ex[p(x|y  Ωy)] for all denoising results as it minimizes MSE and consequently maximizes PSNR.
To summarize  our approach consists of (1) standard training phase and (2) two-step testing phase:

3

(1) Train a neural network to map the context Ωy to the mean µx and variance Σx of a Gaussian

approximation to the prior p(x|Ωy).
posterior mean Ex[p(x|y  Ωy)] by closed-form analytic integration.

(2) At test time  ﬁrst feed context Ωy to neural network to yield µx and Σx; then compute

Looping back to the beginning of this section  we note that the estimate found by standard supervised
training with the L2 loss is precisely the same posterior mean Ex[p(x|y  Ωy)] we seek. Unfortu-
nately  this does not imply that our self-supervised technique would be guaranteed to ﬁnd the same
optimum: we approximate the prior distribution with a Gaussian  whereas standard supervised train-
ing corresponds to a Gaussian approximation of the posterior. However  benign noise models  such
as additive Gaussian noise or Poisson noise  interact with the prior in a way that the result is almost
as good  as demonstrated below.
In concurrent work  Krull at al. [15] describe a similar algorithm for monochromatic data. Instead
of an analytical solution  they use a sampling-based method to describe the prior and posterior  and
represent an arbitrary noise model as a discretized two-dimensional histogram.

4 Practical experiments

In this section  we detail the implementation of our denoising scheme in Gaussian  Poisson  and
impulse noise. In all our experiments  we use a modiﬁed version of the ﬁve-level U-Net [23] archi-
tecture used by Lehtinen et al. [17]  to which we append three 1×1 convolution layers. We construct
our convolutional blind-spot networks based on this same architecture. Details regarding network
architecture  training  and evaluation are provided in the supplement. Our training data comes from
the 50k images in the ILSVRC2012 (Imagenet) validation set  and our test datasets are the commonly
used KODAK (24 images)  BSD300 validation set (100 images)  and SET14 (14 images).

4.1 Additive Gaussian noise

Let us now realize the scheme outlined in Section 3 in the context of additive Gaussian noise. We will
cover the general case of color images only  but the method simpliﬁes trivially to monochromatic
images by replacing all matrices and vectors with scalar values.
The blind-spot network outputs the parameters of a multivariate Gaussian N (µx  Σx) = p(x|Ωy)
representing the distribution of the clean signal. We parameterize the covariance matrix as
TAx where Ax is an upper triangular matrix. This ensures that Σx is a valid covariance
Σx = Ax
matrix  i.e.  symmetric and positive semideﬁnite. Thus we have a total of nine output components
per pixel for RGB images: the three-component mean µx and the six nonzero elements of Ax.
Modeling the corruption process is particularly simple with additive zero-mean Gaussian noise. In
this case  Eq. 1 performs a convolution of two mutually independent Gaussians  and the covariance
of the result is simply the sum of the constituents [2]. Therefore 

µy = µx

and Σy = Σx + σ2I 

(3)
where σ is the standard deviation of the Gaussian noise. We can either assume σ to be known for
each training and validation image  or we can learn to estimate it during training. For a constant 
unknown σ  we add σ as one of the trainable parameters. For variable and unknown σ  we learn an
auxiliary neural network for predicting it during training. The architecture of this auxiliary network
is the same as in the baseline networks except that only one scalar per pixel is produced  and the σ
for the entire image is obtained by taking the mean over the output. It is quite likely that a simpler
network would have sufﬁced for the task  but we did not attempt to optimize its architecture. Note
that the σ estimation network is not trained with a known noise level as a target  but it learns to
predict it as a part of the training process.
To ﬁt N (µy  Σy) to the observed noisy training data  we minimize the corresponding negative log-
likelihood loss during training [22  16  13]:

loss(y  µy  Σy) = − log f (y; µy  Σy) = 1

2

[(y − µy)TΣ−1

y (y − µy)] + 1

2

log |Σy| + C 

(4)

where C subsumes additive constant terms that can be discarded  and f (y; µy  Σy) denotes the
probability density of a multivariate Gaussian distribution N (µy  Σy) at pixel value y. In cases

4

Table 1: Image quality results for Gaussian noise. Values of σ are shown in 8-bit units.

Noise type Method

Gaussian
σ = 25

Gaussian
σ ∈ [5  50]

Baseline  N2C
Baseline  N2N
Our
Our
Our ablated  diag. Σ
Our ablated  diag. Σ
Our ablated  µ only
CBM3D
CBM3D
Baseline  N2C
Baseline  N2N
Our
Our
Our ablated  diag. Σ
Our ablated  diag. Σ
Our ablated  µ only
CBM3D
CBM3D

σ known? KODAK
32.46
32.45
32.45
32.44
31.60
31.55
30.64
31.82
31.81
32.57
32.57
32.47
32.46
31.59
31.58
30.54
31.99
31.99

no
no
yes
no
yes
no
no
yes
no
no
no
yes
no
yes
no
no
yes
no

BSD300

31.08
31.07
31.03
31.02
29.91
29.87
28.65
30.40
30.40
31.29
31.29
31.19
31.18
30.06
30.05
28.56
30.67
30.67

SET14 Average
31.60
31.26
31.58
31.23
31.25
31.57
31.56
31.22
30.70
30.58
30.65
30.53
29.62
29.57
30.68
30.96
30.96
30.66
31.71
31.27
31.70
31.26
31.62
31.21
31.13
31.59
30.73
30.54
30.69
30.45
29.50
29.41
31.15
30.78
30.72
31.13

where σ is unknown and needs to be estimated  we add a small regularization term of −0.1σ to the
loss. This encourages explaining the observed noise as corruption instead of uncertainty about the
clean signal. As long as the regularization is gentle enough  the estimated σ does not overshoot — if
it did  Σy = Σx + σ2I would become too large to ﬁt the observed data in easy-to-denoise regions.
At test time  we compute the mean of the posterior distribution. With additive Gaussian noise the
product involves two Gaussians  and because both distributions are functions of x  we have

p(y|x) p(x|Ωy) = f (x; y  σ2I) f (x; µx  Σx) 

(5)

where we have exploited the symmetry of Gaussian distribution in the ﬁrst term to swap x and y. A
product of two Gaussian functions is an unnormalized Gaussian function  whose mean [2] coincides
with the desired posterior mean:

Ex[p(x|y  Ωy)] = (Σ−1

x + σ−2I)−1(Σ−1

x µx + σ−2y).

(6)

Note that we do not need to evaluate the normalizing constant (marginal likelihood)  as scalar mul-
tiplication does not change the mean of a Gaussian.
Informally  the formula can be seen to “mix in” some of the observed noisy pixel color y into the
estimated mean µx. When the network is certain about the clean signal (Σx is small)  the estimated
mean µx dominates the result. Conversely  the larger the uncertainty of the clean signal is compared
to σ  the more of the noisy observed signal is included in the result.
Comparisons and ablations Table 1 shows the output image quality for the various methods and
ablations tested. Example result images are shown in Figure 2. All methods are evaluated using the
same corrupted input data  and thus the only sources of randomness are the network initialization
and training data shufﬂing during training. Denoiser networks seem to be fairly robust to these
effects  e.g. [17] reports ±0.02 dB variation in the averaged results. We expect the same bounds to
hold for our results as well.
Let us ﬁrst consider the case where the amount of noise is ﬁxed (top half of the table). The N2C
baseline is trained with clean reference images as training targets  and unsurprisingly produces the
best results that can be reached with a given network architecture. N2N [17] matches the results.
Our method with a convolutional blind-spot network and posterior mean estimation is virtually as
good as the baseline methods. This holds even when the amount of noise is unknown and needs
to be estimated as part of the learning process. However  when we ablate our method by forcing
the covariance matrix Σx to be diagonal  the quality of the results suffers considerably. This setup
corresponds to treating each color component of the prior as a univariate  independent distribution 
and the bad result quality highlights the need to treat the signal as a true multivariate distribution.

5

Test image
KODAK-6

Noisy input
20.41 dB

N2C (baseline)

31.17 dB

Our (full)
31.17 dB

Our (diag Σ)

30.06 dB

Our (µ only)

29.04 dB

CBM3D
30.59 dB

Figure 2: Example result images for methods corresponding to Table 1: Gaussian noise σ = 25
(σ not known). PSNRs refer to the individual images. The supplement gives additional result im-
ages  and the full images are included as PNG ﬁles in the supplementary material.

Table 2: Average output quality for Gaussian noise (σ = 25  known) with smaller training sets.

Method
Baseline  N2C
Our
Baseline  N2C + rotation aug.
Our
+ rotation aug.

all

31.60
31.57
31.60
31.58

10 000
31.59
31.58
31.60
31.58

1000
31.53
31.53
31.57
31.53

500
31.44
31.48
31.54
31.47

Training images

300
31.35
31.40
31.48
31.42

200
31.21
31.29
31.38
31.32

100
30.84
31.03
31.21
31.08

We can ablate the setup even further by having our blind-spot network architecture predict only the
mean µ using standard L2 loss  and using this predicted mean directly as the denoiser output. This
corresponds to the setup of Krull et al. [14] in the sense that the center pixel is ignored. As expected 
the image quality suffers greatly due to the inability to extract information from the center pixel.
Since we do not perform posterior mean estimation in this setup  noise level σ does not appear in
the calculations and knowing it would be of no use.
Finally  we denoise the same test images using the ofﬁcial implementation of CBM3D [6]  a state-
of-the-art non-learned image denoising algorithm.3 It uses no training data and relies on the contents
of each individual test image for recovering the clean signal. With both known and automatically
estimated (using the method of Chen et al. [5]) noise parameters  CBM3D outperforms our ablated
setups but remains far from the quality of our full method and the baseline methods.
The lower half of Table 1 presents the same metrics in the case of variable Gaussian noise  i.e. 
when the noise parameters are chosen randomly within the speciﬁed range for each training and
test image. The relative ordering of the methods remains the same as with a ﬁxed amount of noise 
although our method concedes 0.1dB relative to the baseline. Knowing the noise level in advance
does not change the results.
Table 2 illustrates the relationship between output quality and training set size. Without dataset
augmentation  our method performs roughly on par with the baseline and surpasses it for very small
datasets (<1000 images). For the smaller training sets  rotation augmentation becomes beneﬁcial
for the baseline method  whereas for our method it only improves the training of 1×1 combination
layers. With rotation augmentation enabled  our method therefore loses to the baseline method for
very small datasets  although not by much. No other training runs in this paper use augmentation 
as it provides no beneﬁt when using the full training set.
Comparison to masking-based training Our “µ only” ablations illustrate the beneﬁts of
Bayesian training and posterior mean estimation compared to ignoring the center pixel as in the
original NOISE2VOID method. Here  we shall separately estimate the advantages of having an ar-
chitectural blind spot instead of masking-based training [14]. We trained several networks with our
baseline architecture using masking. As recommended by Krull et al.  we chose 64 pixels to be
masked in each input crop using stratiﬁed sampling. Two masking strategies were evaluated: copy-
ing from another pixel in a 5×5 neighborhood (denoted COPY) as advocated in [14]  and overwriting
the pixel with a random color in [0  1]3 (denoted RANDOM)  as done by Batson and Royer [1].

3Even though (grayscale) WNNM [9] has been shown to be superior to (grayscale) BM3D [7]  our experi-
ments with the ofﬁcial implementation of MCWNNM [30]  a multi-channel version of WNNM  indicated that
CBM3D performs better on our test data where all color channels have the same amount of noise.

6

training

comparison 

Figure 3:
Relative train-
ing costs for Gaussian noise
(σ = 25  known) denoisers using
the posterior mean estimation.
For
a
convolutional blind-spot network
for 0.5M minibatches achieves
32.39 dB in KODAK.
For the
masking-based methods  the hor-
izontal axis takes into account
the approximately 4× cheaper
training compared to our convolutional blind-spot networks. For example  at x-axis position marked “1” they
have been trained for 2M minibatches compared to 0.5M minibatches for our method.

Our tests conﬁrmed that the COPY strategy gave better results when the center pixel was ignored 
but the RANDOM strategy gave consistently better results in the Bayesian setting. COPY probably
leads to the network learning to leak some of the center pixel value into the output  which may help
by sharpening the output a bit even when done in such an ad hoc fashion. However  our Bayesian
approach assumes that no such information leaking occurs  and therefore does not tolerate it.
Focusing on the highest-quality setup with posterior mean estimation and RANDOM masking strat-
egy  we estimate that training to a quality matching 0.5M minibatches with our convolutional blind-
spot architecture would require at least 20–100× as much computation due to the loss function
sparsity. This is based on a 10× longer masking-based training run still not reaching comparable
output quality  see Figure 3.

4.2 Poisson noise

In our second experiment we consider Poisson noise which is an interesting practical case as it can
be used to model the photon noise in imaging sensors. We denote the maximum event count as λ
and implement the noise as yi = Poisson(λxi)/λ where i is the color channel and xi ∈ [0  1] is the
clean color component. For denoising  we follow the common approach of approximating Poisson
noise as signal-dependent Gaussian noise [11].
In this setup  the resulting standard deviation is

σi =(cid:112)xi/λ and the corruption model is thus

µy = µx

and Σy = Σx + λ−1diag(µx).

(7)

Note that there is a second approximation in this approach — the marginalization over x (Eq. 1) is
treated as a convolution with a ﬁxed Gaussian even though p(y|x) should be different for each x.
In the formula above  we implicitly take this term to be p(y|µx) which is a good approximation in
the common case of Σx being small. Aside from a different corruption model  both training and
denoising are equivalent to the Gaussian case (Section 4.1). For cases where the noise parameters
are unknown  we treat λ−1 as the unknown parameter that is either learned directly or estimated via
the auxiliary network  depending on whether the amount of noise is ﬁxed or variable  respectively.
Comparisons Table 3  top half  shows the image quality results with Poisson noise  and Figure 4 
top  shows example result images. Note that even though we internally model the noise as signal-
dependent Gaussian noise  we apply true Poisson noise to training and test data. In the case of
ﬁxed amount of noise  our method is within 0.1–0.2 dB from the N2C baseline. Curiously  the case
where the λ is unknown performs slightly better than the case where it is supplied. This is probably a
consequence of the approximations discussed above  and the network may be able to ﬁt the observed
noisy distribution better when it is free to choose a different ratio between variance and mean.
In the case of variable noise  our method remains roughly as good when the noise parameters are
known  but starts to have trouble when they need to be estimated from data. However  it appears that
the problems are mainly concentrated to SET14 where there is a 1.2 dB drop whereas the other test
sets suffer by only ∼0.1 dB. The lone culprit for this drop is the POWERPOINT clip art image  where
our method fails to estimate the noise level correctly  suffering a hefty 13dB penalty. Nonethe-
less  comparing to the “µ only” ablation with L2 loss  i.e.  ignoring the center pixel  shows that
our method with posterior mean estimation still produces much higher output quality. Anscombe
transform [19] is a classical non-learned baseline for denoising Poisson noise  and for reference we
include the results for this method as reported in [17].

7

33.032.532.031.531.030.530.0PSNR (dB)32.391Relative training costOur methodMasking (RANDOM)Masking (COPY)2345678910Table 3: Image quality results for Poisson and impulse noise.

Noise type Method

Poisson
λ = 30

Poisson
λ ∈ [5  50]

Impulse
α = 0.5

Impulse
α ∈ [0  1]

Baseline  N2C
Baseline  N2N
Our
Our
Our ablated  µ only
Anscombe [19] (from [17])
Baseline  N2C
Baseline  N2N
Our
Our
Our ablated  µ only
Baseline  N2C
Baseline  N2N
Our
Our
Our ablated  µ only
Baseline  N2C
Baseline  N2N
Our
Our
Our ablated  µ only

λ/α known? KODAK
31.81
31.80
31.65
31.70
30.22
29.15
31.33
31.32
31.16
31.02
29.88
33.32
32.88
32.98
32.93
30.82
31.69
31.53
31.36
31.40
27.16

no
no
yes
no
no
yes
no
no
yes
no
no
no
no
yes
no
no
no
no
yes
no
no

BSD300

30.40
30.39
30.25
30.28
28.27
27.56
29.91
29.90
29.75
29.69
27.95
31.20
30.85
30.78
30.71
28.52
30.27
30.11
30.00
29.98
25.55

SET14 Average
30.89
30.45
30.88
30.44
30.29
30.73
30.78
30.35
29.17
29.03
28.62
28.36
30.40
29.96
29.96
30.39
30.24
29.82
29.79
28.65
28.84
28.67
31.42
31.98
31.56
30.94
31.61
31.06
31.57
31.09
29.46
29.05
29.77
30.58
30.38
29.51
30.28
29.47
30.29
29.51
25.56
26.09

4.3

Impulse noise

Our last example involves impulse noise where each pixel is  with probability α  replaced by an
uniformly sampled random color in [0  1]3. This corruption process is more complex than in the
previous cases  as both mean and covariance are modiﬁed  and there is a Dirac peak at the clean
color value. To derive the training loss  we again approximate p(y|Ωy) with a Gaussian  and match
its ﬁrst and second raw moments to the data during training. Because the marginal likelihood is
a mixture distribution  its raw moments are obtained by linearly interpolating  with parameter α 
between the raw moments of p(x|Ωy) and the raw moments of the uniform random distribution.
The resulting mean and covariance are

(cid:34)1

(cid:35)

1
1

(cid:34)4

3
3

3
4
3

(cid:35)

3
3
4

µy =

α
2

+ (1 − α)µx

and Σy =

α
12

+ (1 − α)(Σx + µxµx

T) − µyµy
T.

(8)

This deﬁnes the approximate p(y|Ωy) needed for training the denoiser network. As with previous
noise types  in setups where parameter α is unknown  we add it as a learned parameter or estimate
it via a simultaneously trained auxiliary network. The unnormalized posterior is

p(y|x) p(x|Ωy) =(cid:0)α + (1 − α)δ(y − x)(cid:1) f (x; µx  Σx)

= αf (x; µx  Σx) + (1 − α)δ(y − x)f (x; µx  Σx)

from which we obtain the posterior mean:
Ex[p(x|y  Ωy)] =

αµx + (1 − α)f (y; µx  Σx)y
α + (1 − α)f (y; µx  Σx)

.

Looking at the formula  we can see that the result is a linear interpolation between the mean µx
predicted by the network and the potentially corrupted observed pixel value y.
Informally  we
can reason that the less likely the observed value y is to be drawn from the predicted distribution
N (µx  Σx)  the more likely it is to be corrupted  and therefore its weight is low compared to the
predicted mean µx. On the other hand  when the observed pixel value is consistent with the network
prediction  it is weighted more heavily in the output color.
Comparisons Table 3  bottom half  shows the image quality results  and example result images
are shown in Figure 4  bottom. The N2N baseline has more trouble with impulse noise than with

8

(9)

(10)

Test image
KODAK-14

Noisy input
19.48 dB

N2C (baseline)

30.33 dB

Our (full)
30.24 dB

Our (µ only)

28.64 dB

Test image
KODAK-20

Noisy input

9.30 dB

N2C (baseline)

34.90 dB

Our (full)
34.55 dB

Our (µ only)

32.13 dB

Figure 4: Example result images for Poisson (top) and Impulse noise (bottom). PSNRs refer to the
individual images. The supplement gives additional result images  and the full images are included
as PNG ﬁles in the supplementary material.

Gaussian or Poisson noise — note that it cannot be trained with standard L2 loss because the noise
is not zero-mean. Lehtinen et al. [17] recommend annealing from L2 loss to L0 loss in these cases.
We experimented with several loss function schedules for N2N  and obtained the best results by
annealing the loss exponent from 2 to 0.5 during the ﬁrst 75% of training and holding it there for
the remaining training time. Our method loses to the N2C baseline by ∼0.4 dB in the case of ﬁxed
noise  and by ∼0.3 dB with the more difﬁcult variable noise. Notably  our method does not suffer
from not knowing the noise parameter α in either case. The ablated “µ only” setups were trained
with the same loss schedules as the corresponding N2N baselines and lose to the other methods by
multiple dB  highlighting the usefulness of the information in the center pixel in this type of noise.

5 Discussion and future work

Applying Bayesian statistics to denoising has a long history. Non-local means [3]  BM3D [7]  and
WNNM [9] identify a group of similar pixel neighborhoods and estimate the center pixel’s color
from those. Deep image prior [27] seeks a representation for the input image that is easiest to model
with a convolutional network  often encountering a reasonable noise-free representation along the
way. As with self-supervised training  these methods need only the noisy images  but while the
explicit block-based methods determine a small number of neighborhoods from the input image
alone  a deep denoising model may implicitly identify and regress an arbitrarily large number of
neighborhoods from a collection of noisy training data.
Stein’s unbiased risk estimator has been used for training deep denoisers for Gaussian noise [26  21] 
but compared to our work these methods leave a larger quality gap compared to supervised training.
Jena [12] corrupts noisy training data further  and trains a network to reduce the amount of noise
to the original level. This network can then iteratively restore images with the original amount of
noise. Unfortunately  no comparisons against supervised training are given. Finally  FC-AIDE [4]
features an interesting combination of supervised and unsupervised training  where a traditionally
trained denoiser network is ﬁne-tuned in an unsupervised fashion for each test image individually.
We have shown  for the ﬁrst time  that deep denoising models trained in a self-supervised fashion
can reach similar quality as comparable models trained using clean reference data  as long as the
drawbacks imposed by self-supervision are appropriately remedied. Our method assumes pixel-
wise independent noise with a known analytic likelihood model  although we have demonstrated
that individual parameters of the corruption model can also be successfully deducted from the noisy
data. Real corrupted images rarely follow theoretical models exactly [10  18  25]  and an important
avenue for future work will be to learn as much of the noise model from the data as possible. By
basing the learning exclusively on the dataset of interest  we should also be able to alleviate the
concern that the training data (e.g.  natural images) deviates from the intended use (e.g.  medical
images). Experiments with such real life data will be valuable next steps.
Acknowledgements We thank Arno Solin and Samuel Kaski for helpful comments  and Janne
Hellsten and Tero Kuosmanen for the compute infrastructure.

9

References

[1] J. Batson and L. Royer. Noise2Self: Blind denoising by self-supervision. In Proc. International

Conference on Machine Learning (ICML)  pages 524–533  2019. 3  6

[2] P. A. Bromiley. Products and convolutions of Gaussian distributions. Technical Report 2003-

003  www.tina-vision.net  2003. 4  5

[3] A. Buades  B. Coll  and J.-M. Morel. A non-local algorithm for image denoising. In Proc.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 60–65  2005. 9
CoRR 

Fully convolutional pixel adaptive image denoiser.

[4] S. Cha and T. Moon.
abs/1807.07569  2018. 9

[5] G. Chen  F. Zhu  and P. Ann Heng. An efﬁcient statistical method for image noise level
estimation. In Proc. IEEE International Conference on Computer Vision (ICCV)  pages 477–
485  2015. 6

[6] K. Dabov  A. Foi  V. Katkovnik  and K. Egiazarian. Color image denoising via sparse 3D
In Proc.

collaborative ﬁltering with grouping constraint in luminance-chrominance space.
IEEE International Conference on Image Processing  pages 313–316  2007. 6

[7] K. Dabov  A. Foi  V. Katkovnik  and K. Egiazarian. Image denoising by sparse 3-D transform-
IEEE Transactions on Image Processing  16(8):2080–2095 

domain collaborative ﬁltering.
2007. 6  9

[8] C. Doersch  A. Gupta  and A. A. Efros. Unsupervised visual representation learning by context
prediction. In Proc. International Conference on Computer Vision (ICCV)  pages 1422–1430 
2015. 2

[9] S. Gu  L. Zhang  W. Zuo  and X. Feng. Weighted nuclear norm minimization with application
to image denoising. In Proc. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 2862–2869  2014. 6  9

[10] S. Guo  Z. Yan  K. Zhang  W. Zuo  and L. Zhang. Toward convolutional blind denoising of real
photographs. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 1712–1722  2019. 9

[11] S. W. Hasinoff. Photon  Poisson noise. In K. Ikeuchi  editor  Computer Vision: A Reference

Guide  pages 608–610. Springer US  2014. 7

[12] R. Jena. An approach to image denoising using manifold approximation without clean images.

CoRR  abs/1904.12323  2019. 9

[13] A. Kendall and Y. Gal. What uncertainties do we need in Bayesian deep learning for computer
vision? In Advances in Neural Information Processing Systems 30 (Proc. NIPS)  pages 5574–
5584. 2017. 4

[14] A. Krull  T.-O. Buchholz  and F. Jug. Noise2Void – Learning denoising from single noisy
In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 

images.
pages 2129–2137  2019. 1  2  3  6

[15] A. Krull  T. Vicar  and F. Jug. Probabilistic Noise2Void: Unsupervised content-aware denois-

ing. CoRR  abs/1906.00651  2019. 4

[16] Q. V. Le  A. J. Smola  and S. Canu. Heteroscedastic Gaussian process regression. In Proc.

International Conference on Machine Learning (ICML)  pages 489–496  2005. 4

[17] J. Lehtinen  J. Munkberg  J. Hasselgren  S. Laine  T. Karras  M. Aittala  and T. Aila.
In Proc. International Con-

Noise2Noise: Learning image restoration without clean data.
ference on Machine Learning (ICML)  2018. 1  3  4  5  7  8  9

[18] B. Liu  X. Shu  and X. Wu. Deep learning with inaccurate training data for image restoration.

CoRR  abs/1811.07268  2018. 9

[19] M. M¨akitalo and A. Foi. Optimal inversion of the Anscombe transformation in low-count
Poisson image denoising. IEEE Transactions on Image Processing  20(1):99–109  2011. 7  8
Image restoration using very deep convolutional encoder-
decoder networks with symmetric skip connections. In Advances in Neural Information Pro-
cessing Systems 29 (Proc. NIPS)  pages 2802–2810. 2016. 1

[20] X. Mao  C. Shen  and Y. Yang.

[21] C. A. Metzler  A. Mousavi  R. Heckel  and R. G. Baraniuk. Unsupervised learning with Stein’s

unbiased risk estimator. CoRR  abs/1805.10531  2018. 9

[22] D. A. Nix and A. S. Weigend. Estimating the mean and variance of the target probability
distribution. Proc. IEEE International Conference on Neural Networks (ICNN)  pages 55–60 
1994. 4

10

[23] O. Ronneberger  P. Fischer  and T. Brox. U-Net: Convolutional networks for biomedical im-
age segmentation. Medical Image Computing and Computer-Assisted Intervention (MICCAI) 
9351:234–241  2015. 1  4

[24] T. Salimans  A. Karpathy  X. Chen  and D. P. Kingma. PixelCNN++: Improving the PixelCNN
In Proc. International

with discretized logistic mixture likelihood and other modiﬁcations.
Conference on Learning Representations (ICLR)  2017. 2

[25] A. Shocher  N. Cohen  and M. Irani. “Zero-shot” super-resolution using deep internal learning.
In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3118–
3126  2018. 9

[26] S. Soltanayev and S. Y. Chun. Training deep learning based denoisers without ground truth
data. In Advances in Neural Information Processing Systems 31 (Proc. NeurIPS)  pages 3257–
3267. 2018. 9

[27] D. Ulyanov  A. Vedaldi  and V. S. Lempitsky. Deep image prior. In Proc. IEEE Conference on

Computer Vision and Pattern Recognition (CVPR)  pages 9446–9454  2018. 9

CoRR  abs/1812.10477  2018. 1

[28] A. van den Oord  N. Kalchbrenner  L. Espeholt  K. Kavukcuoglu  O. Vinyals  and A. Graves.
Conditional image generation with PixelCNN decoders. In Advances in Neural Information
Processing Systems 29 (Proc. NIPS)  pages 4790–4798. 2016. 2

[29] A. van den Oord  N. Kalchbrenner  and K. Kavukcuoglu. Pixel recurrent neural networks. In

Proc. International Conference on Machine Learning (ICML)  pages 1747–1756  2016. 2

[30] J. Xu  L. Zhang  D. Zhang  and X. Feng. Multi-channel weighted nuclear norm minimization
for real color image denoising. In Proc. IEEE International Conference on Computer Vision
(ICCV)  pages 1105–1113  2017. 6

[31] Y. Zhang  Y. Tian  Y. Kong  B. Zhong  and Y. Fu. Residual dense network for image restoration.

11

,Samuli Laine
Tero Karras
Jaakko Lehtinen
Timo Aila