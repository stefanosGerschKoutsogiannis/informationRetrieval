2017,VAE Learning via Stein Variational Gradient Descent,A new method for learning variational autoencoders (VAEs) is developed  based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems  including semi-supervised analysis of the ImageNet data  demonstrating the scalability of the model to large datasets.,VAE Learning via Stein Variational Gradient Descent

Yunchen Pu  Zhe Gan  Ricardo Henao  Chunyuan Li  Shaobo Han  Lawrence Carin
{yp42  zg27  r.henao  cl319  shaobo.han  lcarin}@duke.edu

Department of Electrical and Computer Engineering  Duke University

Abstract

A new method for learning variational autoencoders (VAEs) is developed  based
on Stein variational gradient descent. A key advantage of this approach is that
one need not make parametric assumptions about the form of the encoder distri-
bution. Performance is further enhanced by integrating the proposed encoder with
importance sampling. Excellent performance is demonstrated across multiple un-
supervised and semi-supervised problems  including semi-supervised analysis of
the ImageNet data  demonstrating the scalability of the model to large datasets.

1

Introduction

There has been signiﬁcant recent interest in the variational autoencoder (VAE) [11]  a generalization
of the original autoencoder [33]. VAEs are typically trained by maximizing a variational lower
bound of the data log-likelihood [2  10  11  12  18  21  22  23  30  34  35]. To compute the variational
expression  one must be able to explicitly evaluate the associated distribution of latent features  i.e. 
the stochastic encoder must have an explicit analytic form. This requirement has motivated design
of encoders in which a neural network maps input data to the parameters of a simple distribution 
e.g.  Gaussian distributions have been widely utilized [1  11  27  25].
The Gaussian assumption may be too restrictive in some cases [28]. Consequently  recent work has
considered normalizing ﬂows [28]  in which random variables from (for example) a Gaussian distri-
bution are fed through a series of nonlinear functions to increase the complexity and representational
power of the encoder. However  because of the need to explicitly evaluate the distribution within the
variational expression used when learning  these nonlinear functions must be relatively simple  e.g. 
planar ﬂows. Further  one may require many layers to achieve the desired representational power.
We present a new approach for training a VAE. We recognize that the need for an explicit form for
the encoder distribution is only a consequence of the fact that learning is performed based on the
variational lower bound. For inference (e.g.  at test time)  we do not need an explicit form for the
distribution of latent features  we only require fast sampling from the encoder. Consequently  rather
than directly employing the traditional variational lower bound  we seek to minimize the Kullback-
Leibler (KL) distance between the true posterior of model and latent parameters. Learning then
becomes a novel application of Stein variational gradient descent (SVGD) [15]  constituting its ﬁrst
application to training VAEs. We extend SVGD with importance sampling [1]  and also demonstrate
its novel use in semi-supervised VAE learning.
The concepts developed here are demonstrated on a wide range of unsupervised and semi-supervised
learning problems  including a large-scale semi-supervised analysis of the ImageNet dataset. These
experimental results illustrate the advantage of SVGD-based VAE training  relative to traditional
approaches. Moreover  the results demonstrate further improvements realized by integrating SVGD
with importance sampling.
Independent work by [3  6] proposed the similar models  in which the aurthers incorporated SVGD
with VAEs [3] and importance sampling [6] for unsupervised learning tasks.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

2 Stein Learning of Variational Autoencoder (Stein VAE)

2.1 Review of VAE and Motivation for Use of SVGD
n=1  where xn are modeled via decoder xn|zn ∼ p(x|zn; θ). A prior
Consider data D = {xn}N
(cid:80)N
p(z) is placed on the latent codes. To learn parameters θ  one typically is interested in maximizing
n=1 log p(xn; θ). A variational lower bound is often
the empirical expected log-likelihood  1
(cid:105)
(cid:104) p(x|z; θ)p(z)
N
employed:

(1)
with log p(x; θ) ≥ L(θ  φ; x)  and where Ez|x;φ[·] is approximated by averaging over a ﬁnite
number of samples drawn from encoder q(z|x; φ). Parameters θ and φ are typically iteratively

optimized via stochastic gradient descent [11]  seeking to maximize(cid:80)N

= −KL(q(z|x; φ)(cid:107)p(z|x; θ)) + log p(x; θ)  

L(θ  φ; x) = Ez|x;φ log

q(z|x; φ)

n=1 L(θ  φ; xn).

To evaluate the variational expression in (1)  we require the ability to sample efﬁciently from
q(z|x; φ)  to approximate the expectation. We also require a closed form for this encoder  to evalu-
ate log[p(x|z; θ)p(z)/q(z|x; φ)]. In the proposed VAE learning framework  rather than maximiz-
ing the variational lower bound explicitly  we focus on the term KL(q(z|x; φ)(cid:107)p(z|x; θ))  which
we seek to minimize. This can be achieved by leveraging Stein variational gradient descent (SVGD)
[15]. Importantly  for SVGD we need only be able to sample from q(z|x; φ)  and we need not
possess its explicit functional form.
In the above discussion  θ is treated as a parameter; below we treat it as a random variable  as
was considered in the Appendix of [11]. Treatment of θ as a random variable allows for model
averaging  and a point estimate of θ is revealed as a special case of the proposed method.
The set of codes associated with all xn ∈ D is represented Z = {zn}N

n=1. The prior on {θ Z} is
n=1 p(zn). We desire the posterior p(θ Z|D). Consider the

here represented as p(θ Z) = p(θ)(cid:81)N

revised variational expression
L1(q;D) = Eq(θ Z) log
(2)
where p(D;M) is the evidence for the underlying model M. Learning q(θ Z) such that L1 is
maximized is equivalent to seeking q(θ Z) that minimizes KL(q(θ Z)(cid:107)p(θ Z|D)). By leveraging
and generalizing SVGD  we will perform the latter.

= −KL(q(θ Z)(cid:107)p(θ Z|D)) + log p(D;M)  

(cid:104) p(D|Z  θ)p(θ Z)

q(θ Z)

(cid:105)

2.2 Stein Variational Gradient Descent (SVGD)
Rather than explicitly specifying a form for p(θ Z|D)  we sequentially reﬁne samples of θ and Z 
such that they are better matched to p(θ Z|D). We alternate between updating the samples of θ and
samples of Z  analogous to how θ and φ are updated alternatively in traditional VAE optimization
of (1). We ﬁrst consider updating samples of θ  with the samples of Z held ﬁxed. Speciﬁcally 
assume we have samples {θj}M
j=1 drawn from
distribution q(Z). We wish to transform {θj}M
j=1 by feeding them through a function  and the
corresponding (implicit) transformed distribution from which they are drawn is denoted as qT (θ). It
is desired that  in a KL sense  qT (θ)q(Z) is closer to p(θ Z|D) than was q(θ)q(Z). The following
theorem is useful for deﬁning how to best update {θj}M
Theorem 1 Assume θ and Z are Random Variables (RVs) drawn from distributions q(θ) and q(Z) 
respectively. Consider the transformation T (θ) = θ + ψ(θ;D) and let qT (θ) represent the distri-
bution of θ

j=1 drawn from distribution q(θ)  and samples {zjn}M

= T (θ). We have

j=1.

(cid:48)

(cid:17)|=0 = −Eθ∼q(θ)

(cid:0)trace(Ap(θ;D))(cid:1)  

(cid:16)

∇

KL(qT(cid:107)p)

(3)
where qT = qT (θ)q(Z)  p = p(θ Z|D)  Ap(θ;D) = ∇θ log ˜p(θ;D)ψ(θ;D)T + ∇θψ(θ;D) 
log ˜p(θ;D) = EZ∼q(Z)[log p(D Z  θ)]  and p(D Z  θ) = p(D|Z  θ)p(θ Z).
The proof is provided in Appendix A. Following [15]  we assume ψ(θ;D) lives in a reproducing
kernel Hilbert space (RKHS) with kernel k(· ·). Under this assumption  the solution for ψ(θ;D)

2

that maximizes the decrease in the KL distance (3) is

ψ∗(·;D) = Eq(θ)[k(θ ·)∇θ log ˜p(θ;D) + ∇θk(θ ·)] .

(4)
Theorem 1 concerns updating samples from q(θ) assuming ﬁxed q(Z). Similarly  to update q(Z)
with q(θ) ﬁxed  we employ a complementary form of Theorem 1 (omitted for brevity). In that case 
we consider transformation T (Z) = Z + ψ(Z;D)  with Z ∼ q(Z)  and function ψ(Z;D) is also
assumed to be in a RKHS.
The expectations in (3) and (4) are approximated by samples θ(t+1)

= θ(t)

j + ∆θ(t)

j   with

j

(cid:21)

 

(cid:80)M

(cid:20)
(cid:80)N
(cid:20)

M

∆θ(t)

j ≈ 1

M

j )∇θ(t)

j(cid:48)

j(cid:48) ;D) + ∇θ(t)

j(cid:48)

(5)

M

n=1

j(cid:48)=1

j(cid:48)=1

j ))

∆z(t)

kθ(θ(t)

kθ(θ(t)

kz(z(t)

j(cid:48)   θ(t)

jn = 1
M

log ˜p(θ(t)

with ∇θ log ˜p(θ;D) ≈ 1
manifested for the latent variables z(t+1)

j(cid:48)   θ(t)
(cid:80)M
j=1 ∇θ log p(xn|zjn  θ)p(θ). A similar update of samples is
jn = z(t)
jn )∇z(t)

jn + ∆z(t)
(cid:80)M
jn:
j(cid:48)n;D) + ∇z(t)
where ∇zn log ˜p(zn;D) ≈ 1
(cid:48)
j)p(zn). The kernels used to update sam-
ples of θ and zn are in general different  denoted respectively kθ(· ·) and kz(· ·)  and  is a small
step size. For notational simplicity  M is the same in (5) and (6)  but in practice a different number
of samples may be used for θ and Z.
If M = 1 for parameter θ  indices j and j(cid:48) are removed in (5). Learning then reduces to gradient
descent and a point estimate for θ  identical to the optimization procedure used for the traditional
VAE expression in (1)  but with the (multiple) samples associated with Z sequentially transformed
via SVGD (and  importantly  without the need to assume a form for q(z|x; φ)). Therefore  if only a
point estimate of θ is desired  (1) can be optimized wrt θ  while for updating Z SVGD is applied.

j(cid:48)n  z(t)
(cid:80)M
j=1 ∇zn log p(xn|zn  θ

j(cid:48)n  z(t)
jn )

log ˜p(z(t)

kz(z(t)

(cid:21)

 

(6)

j(cid:48) n

j(cid:48) n

2.3 Efﬁcient Stochastic Encoder

jn}M

At iteration t of the above learning procedure  we realize a set of latent-variable (code) samples
j=1 for each xn ∈ D under analysis. For large N  training may be computationally expensive.
{z(t)
Further  the need to evolve (learn) samples {zj∗}M
j=1 for each new test sample  x∗  is undesirable.
We therefore develop a recognition model that efﬁciently computes samples of latent codes for a data
sample of interest. The recognition model draws samples via zjn = f η(xn  ξjn) with ξjn ∼ q0(ξ).
Distribution q0(ξ) is selected such that it may be easily sampled  e.g.  isotropic Gaussian.
After each iteration of updating the samples of Z  we reﬁne recognition model f η(x  ξ) to mimic
the Stein sample dynamics. Assume recognition-model parameters η(t) have been learned thus far.
jn = f η(t) (xn  ξjn)  with ξjn ∼ q0(ξ).
Using η(t)  latent codes for iteration t are constituted as z(t)
These codes are computed for all data xn ∈ Bt  where Bt ⊂ D is the minibatch of data at iteration
t. The change in the codes is ∆z(t)
jn  as deﬁned in (6). We then update η to match the reﬁned codes 
as

(cid:80)

(cid:80)M
j=1 (cid:107)f η(xn  ξjn) − z(t+1)

jn

(cid:107)2 .

(7)

η(t k−1) − δ(cid:80)

η(t+1) = arg minη

xn∈Bt

(cid:80)M
j=1 ∆η(t k−1)

jn

xn∈Bt

  where ∆η(t k−1)

The analytic solution of (7) is intractable. We update η with K steps of gradient descent as η(t k) =
= ∂ηf η(xn  ξjn)(f η(xn  ξjn) −
)|η=η(t k−1)  δ is a small step size  η(t) = η(t 0)  η(t+1) = η(t K)  and ∂ηf η(xn  ξjn) is
z(t+1)
jn
the transpose of the Jacobian of f η(xn  ξjn) wrt η. Note that the use of minibatches mitigates
challenges of training with large training sets  D.
The function f η(x  ξ) plays a role analogous to q(z|x; φ) in (1)  in that it yields a means of efﬁ-
ciently drawing samples of latent codes z  given observed x; however  we do not impose an explicit
functional form for the distribution of these samples.

jn

3

3 Stein Variational Importance Weighted Autoencoder (Stein VIWAE)

3.1 Multi-sample importance-weighted KL divergence

(cid:104)

(cid:104)

(cid:80)k

(cid:80)k

(cid:105)

(cid:105)

Recall the variational expression in (1) employed in conventional VAE learning. Recently  [1  19]
showed that the multi-sample (k samples) importance-weighted estimator

Lk(x) = Ez1 ... zk∼q(z|x)

log 1
k

p(x zi)
q(zi|x)

i=1

 

(8)

provides a tighter lower bound and a better proxy for the log-likelihood  where z1  . . .   zk are ran-
dom variables sampled independently from q(z|x). Recall from (3) that the KL divergence played
a key role in the Stein-based learning of Section 2. Equation (8) motivates replacement of the KL
objective function with the multi-sample importance-weighted KL divergence

q p(Θ;D) (cid:44) −E

KLk

(9)
where Θ = (θ Z) and Θ1:k = Θ1  . . .   Θk are independent samples from q(θ Z). Note that the
special case of k = 1 recovers the standard KL divergence. Inspired by [1]  the following theorem
(proved in Appendix A) shows that increasing the number of samples k is guaranteed to reduce the
KL divergence and provide a better approximation of target distribution.

Θ1:k∼q(Θ)

log 1
k

i=1

 

p(Θi|D)
q(Θi)

q p(Θ;D) ≥ KLk+1

q p (Θ;D) ≥ 0  and if

Theorem 2 For any natural number k  we have KLk
q(Θ)/p(Θ|D) is bounded  then limk→∞ KLk
We minimize (9) with a sample transformation based on a generalization of SVGD and the recogni-
tion model (encoder) is trained in the same way as in Section 2.3. Speciﬁcally  we ﬁrst draw samples
j=1 from a simple distribution q0(·)  and convert these to approximate draws
{θ1:k
from p(θ1:k Z 1:k|D) by minimizing the multi-sample importance weighted KL divergence via non-
linear functional transformation.

q p(Θ;D) = 0.

j=1 and {z1:k

jn }M

j }M

3.2

Importance-weighted SVGD for VAEs

The following theorem generalizes Theorem 1 to multi-sample weighted KL divergence.

q p(Θ D) is the
Theorem 3 Let Θ1:k be RVs drawn independently from distribution q(Θ) and KLk
multi-sample importance weighted KL divergence in (9). Let T (Θ) = Θ + ψ(Θ;D) and qT (Θ)
represent the distribution of Θ(cid:48) = T (Θ). We have

(cid:16)

(cid:17)|=0 = −E

∇

q p(Θ(cid:48);D)

KLk

Θ1:k∼q(Θ)(Ak

p(Θ1:k;D)) .

(10)

(cid:16)
 Z) . We have
∇
p(θ1:k;D) = 1

The proof and detailed deﬁnition is provided in Appendix A. The following corollaries generalize
Theorem 1 and (4) via use of importance sampling  respectively.
Corollary 3.1 θ1:k and Z 1:k are RVs drawn independently from distributions q(θ) and q(Z)  re-
spectively. Let T (θ) = θ + ψ(θ;D)  qT (θ) represent the distribution of θ
= T (θ)  and
Θ(cid:48) = (θ

(cid:48)

(cid:48)

KLk

θ1:k∼q(θ)(Ak
qT  p(Θ(cid:48);D)
(cid:80)k
i=1 ωiAp(θi;D)  ωi = EZ i∼q(Z)

p(θ1:k;D))  

(cid:104) p(θi Z i D)

(cid:105)

  ˜ω = (cid:80)k

where Ak
Ap(θ;D) and log ˜p(θ;D) are as deﬁned in Theorem 1.
Corollary 3.2 Assume ψ(θ;D) lives in a reproducing kernel Hilbert space (RKHS) with kernel
kθ(· ·). The solution for ψ(θ;D) that maximizes the decrease in the KL distance (11) is

q(θi)q(Z i)

i=1 ωi;

˜ω

(11)

(cid:104) 1

˜ω

(cid:80)k

i=1 ωi

(cid:0)∇θi kθ(θi ·) + kθ(θi ·)∇θi log ˜p(θi;D)(cid:1)(cid:105)

ψ∗(·;D) = E

θ1:k∼q(θ)

.

(12)

(cid:17)|=0 = −E

4

Corollary 3.1 and Corollary 3.2 provide a means of updating multiple samples {θ1:k
j=1 from q(θ)
via T (θi) = θi + ψ(θi;D). The expectation wrt q(Z) is approximated via samples drawn from
q(Z). Similarly  we can employ a complementary form of Corollary 3.1 and Corollary 3.2 to update
j=1 from q(Z). This suggests an importance-weighted learning procedure
multiple samples {Z 1:k
that alternates between update of particles {θ1:k
j }M
j=1  which is similar to the one in
Section 2.2. Detailed update equations are provided in Appendix B.

j=1 and {Z 1:k

j }M

j }M

j }M

4 Semi-Supervised Learning with Stein VAE
n=1  where the label yn ∈ {1  . . .   C} and the de-
Consider labeled data as pairs Dl = {xn  yn}Nl
coder is modeled as (xn  yn|zn) ∼ p(x  y|zn; θ  ˜θ) = p(x|zn; θ)p(y|zn; ˜θ)  where ˜θ represents
the parameters of the decoder for labels. The set of codes associated with all labeled data are repre-
sented as Zl = {zn}Nl
n=1. We desire to approximate the posterior distribution on the entire dataset
p(θ  ˜θ Z Zl|D Dl) via samples  where D represents the unlabeled data  and Z is the set of codes
associated with D. In the following  we will only discuss how to update the samples of θ  ˜θ and Zl.
Updating samples Z is the same as discussed in Sections 2 and 3.2 for Stein VAE and Stein VIWAE 
respectively.
Assume {θj}M
{zjn}M
and (4)  which is useful for deﬁning how to best update {θj}M
Corollary 3.3 Assume θ  ˜θ  Z and Zl are RVs drawn from distributions q(θ)  q(˜θ)  q(Z) and
q(Zl)  respectively. Consider the transformation T (θ) = θ + ψ(θ;D Dl) where ψ(θ;D Dl)
lives in a RKHS with kernel kθ(· ·). Let qT (θ) represent the distribution of θ
= T (θ). For
qT = qT (θ)q(Z)q(˜θ) and p = p(θ  ˜θ Z|D Dl)  we have

j=1 drawn from distribution q(˜θ)  and samples
j=1 drawn from (distinct) distribution q(Zl). The following corollary generalizes Theorem 1

j=1 drawn from distribution q(θ)  {˜θj}M

j=1.

(cid:48)

(cid:16)

(cid:17)|=0 = −Eθ∼q(θ)(Ap(θ;D Dl))  

∇

KL(qT(cid:107)p)

(13)
where Ap(θ;D Dl) = ∇θψ(θ;D Dl) + ∇θ log ˜p(θ;D Dl)ψ(θ;D Dl)T   log ˜p(θ;D Dl) =
EZ∼q(Z)[log p(D|Z  θ)] + EZl∼q(Zl)[log p(Dl|Zl  θ)]  and the solution for ψ(θ;D Dl) that maxi-
mizes the change in the KL distance (13) is

ψ∗(·;D Dl) = Eq(θ)[k(θ ·)∇θ log ˜p(θ;D Dl) + ∇θk(θ ·)] .

(14)

Further details are provided in Appendix C.

h(cid:107)x − x(cid:48)(cid:107)2

5 Experiments
For all experiments  we use a radial basis-function (RBF) kernel as in [15]  i.e.  k(x  x(cid:48)) =
exp(− 1
2)  where the bandwidth  h  is the median of pairwise distances between cur-
rent samples. q0(θ) and q0(ξ) are set to isotropic Gaussian distributions. We share the samples of ξ
across data points  i.e.  ξjn = ξj  for n = 1  . . .   N (this is not necessary  but it saves computation).
The samples of θ and z  and parameters of the recognition model  η  are optimized via Adam [9]
with learning rate 0.0002. We do not perform any dataset-speciﬁc tuning or regularization other
than dropout [32] and early stopping on validation sets. We set M = 100 and k = 50  and use
minibatches of size 64 for all experiments  unless otherwise speciﬁed.

5.1 Expressive power of Stein recognition model
Gaussian Mixture Model We synthesize data by (i) drawing zn ∼ 1

where µ1 = [5  5]T   µ2 = [−5 −5]T ; (ii) drawing xn ∼ N (θzn  σ2I)  where θ = (cid:2) 2 −1

2N (µ1  I) + 1

(cid:3) and

2N (µ2  I) 
1 −2

σ = 0.1. The recognition model fη(xn  ξj) is speciﬁed as a multi-layer perceptron (MLP) with
100 hidden units  by ﬁrst concatenating ξj and xn into a long vector. The dimension of ξj is set
to 2. The recognition model for standard VAE is also an MLP with 100 hidden units  and with the
assumption of a Gaussian distribution for the latent codes [11].

5

Figure 1: Approximation of posterior distribution: Stein VAE vs. VAE. The ﬁgures represent differ-
ent samples of Stein VAE. (left) 10 samples  (center) 50 samples  and (right) 100 samples.
We generate N = 10  000 data points for training and 10 data points for testing. The analytic form
of true posterior distribution is provided in Appendix D. Figure 1 shows the performance of Stein
VAE approximations for the true posterior; other similar examples are provided in Appendix F. The
Stein recognition model is able to capture the multi-modal posterior and produce accurate density
approximation.

+

Poisson Factor Analysis Given a discrete vector
xn ∈ ZP
+  Poisson factor analysis [36] assumes xn
is a weighted combination of V latent factors xn ∼
Pois(θzn)  where θ ∈ RP×V
is the factor loadings
matrix and zn ∈ RV
+ is the vector of factor scores.
We consider topic modeling with Dirichlet priors
on θv (v-th column of θ) and gamma priors on each
component of zn.
We evaluate our model on the 20 Newsgroups
dataset containing N = 18  845 documents with a
vocabulary of P = 2  000. The data are partitioned
into 10 314 training  1 000 validation and 7 531 test
documents. The number of factors (topics) is set to
V = 128. θ is ﬁrst learned by Markov chain Monte
Carlo (MCMC) [4]. We then ﬁx θ at its MAP value 
and only learn the recognition model η using stan-
dard VAE and Stein VAE; this is done  as in the
previous example  to examine the accuracy of the
recognition model to estimate the posterior of the
latent factors  isolated from estimation of θ. The
recognition model is an MLP with 100 hidden units.
An analytic form of
the true posterior distribution
p(zn|xn) is intractable for this problem. Consequently 
we employ samples collected from MCMC as ground
truth. With θ ﬁxed  we sample zn via Gibbs sampling  us-
ing 2 000 burn-in iterations followed by 2 500 collection
draws  retaining every 10th collection sample. We show
the marginal and pairwise posterior of one test data point
in Figure 2. Additional results are provided in Appendix
F. Stein VAE leads to a more accurate approximation than
standard VAE  compared to the MCMC samples. Consid-
ering Figure 2  note that VAE signiﬁcantly underestimates
the variance of the posterior (examining the marginals)  a
well-known problem of variational Bayesian analysis [7].
In sharp contrast  Stein VAE yields highly accurate ap-
proximations to the true posterior.
5.2 Density estimation
Data We consider ﬁve benchmark datasets: MNIST and four text corpora: 20 Newsgroups
(20News)  New York Times (NYT)  Science and RCV1-v2 (RCV2). For MNIST  we used the stan-
dard split of 50K training  10K validation and 10K test examples. The latter three text corpora

Table 1: Negative log-likelihood (NLL) on
MNIST. †Trained with VAE and tested with
IWAE. ‡Trained and tested with IWAE.
NLL
89.90
Normalizing ﬂow [28]
85.10
VAE + IWAE [1]†
86.76
IWAE + IWAE [1]‡
84.78
85.21
Stein VAE + ELBO
84.98
Stein VAE + S-ELBO
Stein VIWAE + ELBO
83.01
Stein VIWAE + S-ELBO 82.88

Figure 2: Univariate marginals and pairwise pos-
teriors. Purple  red and green represent the distribu-
tion inferred from MCMC  standard VAE and Stein
VAE  respectively.

Method

DGLM [27]

6

consist of 133K  166K and 794K documents. These three datasets are split into 1K validation  10K
testing and the rest for training.

Evaluation Given new data x∗ (testing data)  the marginal log-likelihood/perplexity values are
estimated by the variational evidence lower bound (ELBO) while integrating the decoder parame-
ters θ out  log p(x∗) ≥ Eq(z∗)[log p(x∗  z∗)] + H(q(z∗)) = ELBO(q(z∗))  where p(x∗  z∗) =
Eq(θ)[log p(x∗  θ  z∗)] and H(q(·)) = −Eq(log q(·)) is the entropy. The expectation is approxi-
mated with samples {θj}M
j=1 with z∗j = f η(x∗  ξj)  ξj ∼ q0(ξ). Directly evaluating
q(z∗) is intractable  thus it is estimated via density transformation q(z) = q0(ξ)
.

j=1 and {z∗j}M

(cid:12)(cid:12)(cid:12)det ∂f η(x ξ)

(cid:12)(cid:12)(cid:12)−1

∂ξ

DocNADE [14]

DEF [24]
NVDM [17]

Table 2: Test perplexities on four text corpora.
Method

We further estimate the marginal log-
likelihood/perplexity values via the
stochastic variational lower bound  as
the mean of 5K-sample importance
weighting estimate [1]. Therefore  for
each dataset  we report four results: (i)
Stein VAE + ELBO  (ii) Stein VAE + S-
ELBO  (iii) Stein VIWAE + ELBO and
(iv) Stein VIWAE + S-ELBO; the ﬁrst
term denotes the training procedure is
employed as Stein VAE in Section 2 or Stein VIWAE in Section 3; the second term denotes the
testing log-likelihood/perplexity is estimated by the ELBO or the stochastic variational lower bound 
S-ELBO [1].

Science RCV2
1725
742
—-
1576
550
—-
549
1499
544
1497
1453
523
518
1421

20News NYT
2496
2416
—-
2402
2401
2315
2277

Stein VAE + ELBO
Stein VAE + S-ELBO
Stein VIWAE + ELBO
Stein VIWAE + S-ELBO

896
—-
852
849
845
837
829

Model For MNIST  we train the model with one stochastic layer  zn  with 50 hidden units and
two deterministic layers  each with 200 units. The nonlinearity is set as tanh. The visible layer 
xn  follows a Bernoulli distribution. For the text corpora  we build a three-layer deep Poisson
network [24]. The sizes of hidden units are 200  200 and 50 for the ﬁrst  second and third layer 
respectively (see [24] for detailed architectures).

Results The log-likelihood/perplexity results
are summarized in Tables 1 and 2. On MNIST 
our Stein VAE achieves a variational lower bound
of -85.21 nats  which outperforms standard VAE
with the same model architecture. Our Stein VI-
WAE achieves a log-likelihood of -82.88 nats 
exceeding normalizing ﬂow (-85.1 nats) and im-
portance weighted autoencoder (-84.78 nats) 
which is the best prior result obtained by feed-
forward neural network (FNN). DRAW [5] and
PixelRNN [20]  which exploit spatial structure 
achieved log-likelihoods of around -80 nats. Our
model can also be applied on these models  but
this is left as interesting future work. To further illustrate the beneﬁt of model averaging  we vary
the number of samples for θ (while retaining 100 samples for Z) and show the results associated
with training/testing time in Figure 3. When M = 1 for θ  our model reduces to a point estimate
for that parameter. Increasing the number of samples of θ (model averaging) improves the negative
log-likelihood (NLL). The testing time of using 100 samples of θ is around 0.12 ms per image.

Figure 3: NLL vs. Training/Testing time on MNIST
with various numbers of samples for θ.

5.3 Semi-supervised Classiﬁcation

We consider semi-supervised classiﬁcation on MNIST and ImageNet [29] data. For each dataset 
we report the results obtained by (i) VAE  (ii) Stein VAE  and (iii) Stein VIWAE.

MNIST We randomly split the training set into a labeled and unlabeled set  and the number of
labeled samples in each category varies from 10 to 300. We perform testing on the standard test
set with 20 different training-set splits. The decoder for labels is implemented as p(yn|zn  ˜θ) =
softmax(˜θzn). We consider two types of decoders for images p(xn|zn  θ) and encoder f η(x  ξ):

7

123456858687881510204060100200300Number of Samples (M)Time (s)Negative Log−likelihood (nats)Negative Log−likelihoodTesting Time for Entire DatasetTraining Time for Each EpochNρ

10
60
100
300

VAE§

3.33 ± 0.14
2.59 ±0.05
2.40 ±0.02
2.18 ±0.04

VAE

this extension as future work.

Stein VIWAE
1.90 ± 0.05
1.41 ± 0.02
0.99 ± 0.02
0.86 ± 0.01

Stein VIWAE
2.67 ± 0.09
2.09 ± 0.03
1.88 ± 0.01
1.75 ± 0.01

VAE†

2.44 ± 0.17
1.88 ±0.05
1.47 ±0.02
0.98 ±0.02

FNN

Stein VAE
2.78 ± 0.24
2.13 ± 0.08
1.92 ± 0.05
1.77 ± 0.03

CNN

Stein VAE
1.94 ± 0.24
1.44 ± 0.04
1.01 ± 0.03
0.89 ± 0.03

Table 4: Semi-supervised classiﬁcation accuracy (%) on ImageNet.

indicating the former produces more robust parameter estimates.

State-of-
results [26] are achieved by the Ladder network  which can be employed with

Table 3: Semi-supervised classiﬁcation error (%) on MNIST. Nρ is the number
of labeled images per class. §[12]; †our implementation.

(i) FNN: Following [12]  we use a 50-dimensional latent variables zn and two hidden layers  each
with 600 hidden units  for both encoder and decoder; softplus is employed as the nonlinear activation
function. (ii) All convolutional nets (CNN): Inspired by [31]  we replace the two hidden layers with
32 and 64 kernels of size 5 × 5 and a stride of 2. A fully connected layer is stacked on the CNN to
produce a 50-dimensional latent variables zn. We use the leaky rectiﬁed activation [16]. The input
of the encoder is formed by spatially aligning and stacking xn and ξ  while the output of decoder is
the image itself.
Table 3 shows the classi-
ﬁcation results. Our Stein
VAE and Stein VIWAE
consistently achieve bet-
ter performance than the
VAE. We further observe
that the variance of Stein
VIWAE results is much
smaller than that of Stein
VAE results on small la-
beled data 
the-art
our Stein-based approach  however  we will consider
ImageNet
2012 We
consider scalability of our
model to large datasets.
We split the 1.3 million
training images into an
unlabeled and labeled set 
and vary the proportion
of labeled images from
1% to 40%. The classes
are balanced to ensure
that no particular class
is over-represented  i.e.  the ratio of labeled and unlabeled images is the same for each class. We
repeat the training process 10 times for the training setting with labeled images ranging from 1% to
10%   and 5 times for the the training setting with labeled images ranging from 20% to 40%. Each
time we utilize different sets of images as the unlabeled ones.
We employ an all convolutional net [31] for both the encoder and decoder  which replaces determin-
istic pooling (e.g.  max-pooling) with stridden convolutions. Residual connections [8] are incorpo-
rated to encourage gradient ﬂow. The model architecture is detailed in Appendix E. Following [13] 
images are resized to 256 × 256. A 224 × 224 crop is randomly sampled from the images or its
horizontal ﬂip with the mean subtracted [13]. We set M = 20 and k = 10.
Table 4 shows classiﬁcation results indicating that Stein VAE and Stein IVWAE outperform VAE
in all the experiments  demonstrating the effectiveness of our approach for semi-supervised classi-
ﬁcation. When the proportion of labeled examples is too small (< 10%)  DGDN [21] outperforms
all the VAE-based models  which is not surprising provided that our models are deeper  thus have
considerably more parameters than DGDN [21].
6 Conclusion
We have employed SVGD to develop a new method for learning a variational autoencoder  in which
we need not specify an a priori form for the encoder distribution. Fast inference is manifested
by learning a recognition model that mimics the manner in which the inferred code samples are
manifested. The method is further generalized and improved by performing importance sampling.
An extensive set of results  for unsupervised and semi-supervised learning  demonstrate excellent
performance and scaling to large datasets.

Stein VIWAE DGDN [21]
43.98± 1.15
36.91 ± 0.98
46.92± 1.11
42.57 ± 0.84
46.20 ± 0.52
47.36± 0.91
48.67 ± 0.31
48.41± 0.76
51.77 ± 0.12
51.51± 0.28
54.14± 0.12
55.45 ± 0.11
58.21 ± 0.12
57.34± 0.18

1 % 35.92± 1.91
2 % 40.15± 1.52
5 % 44.27± 1.47
10 % 46.92± 1.02
20 % 50.43± 0.41
30 % 53.24± 0.33
40 % 56.89± 0.11

Stein VAE
36.44 ± 1.66
41.71 ± 1.14
46.14 ± 1.02
47.83 ± 0.88
51.62 ± 0.24
55.02 ± 0.22
58.17 ± 0.16

Acknowledgements

This research was supported in part by ARO  DARPA  DOE  NGA  ONR and NSF.

8

References
[1] Y. Burda  R. Grosse  and R. Salakhutdinov.

2016.

Importance weighted autoencoders.

In ICLR 

[2] L. Chen  S. Dai  Y. Pu  C. Li  and Q. Su Lawrence Carin. Symmetric variational autoencoder

and connections to adversarial learning. In arXiv  2017.

[3] Y. Feng  D. Wang  and Q. Liu. Learning to draw samples with amortized stein variational

gradient descent. In UAI  2017.

[4] Z. Gan  C. Chen  R. Henao  D. Carlson  and L. Carin. Scalable deep poisson factor analysis

for topic modeling. In ICML  2015.

[5] K. Gregor  I. Danihelka  A. Graves  and D. Wierstra. Draw: A recurrent neural network for

image generation. In ICML  2015.

[6] J. Han and Q. Liu. Stein variational adaptive importance sampling. In UAI  2017.

[7] S. Han  X. Liao  D.B. Dunson  and L. Carin. Variational gaussian copula inference. In AIS-

TATS  2016.

[8] K. He  X. Zhang  S. Ren  and Sun J. Deep residual learning for image recognition. In CVPR 

2016.

[9] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR  2015.

[10] D. P. Kingma  T. Salimans  R. Jozefowicz  X.i Chen  I. Sutskever  and M. Welling. Improving

variational inference with inverse autoregressive ﬂow. In NIPS  2016.

[11] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR  2014.

[12] D.P. Kingma  D.J. Rezende  S. Mohamed  and M. Welling. Semi-supervised learning with

deep generative models. In NIPS  2014.

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS  2012.

[14] H. Larochelle and S. Laulyi. A neural autoregressive topic model. In NIPS  2012.

[15] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference

algorithm. In NIPS  2016.

[16] A. L. Maas  A. Y. Hannun  and A. Y. Ng. Rectiﬁer nonlinearities improve neural network

acoustic models. In ICML  2013.

[17] Y. Miao  L. Yu  and Phil Blunsomi. Neural variational inference for text processing. In ICML 

2016.

[18] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML 

2014.

[19] A. Mnih and D. J. Rezende. Variational inference for monte carlo objectives. In ICML  2016.

[20] A. Oord  N. Kalchbrenner  and K. Kavukcuoglu. Pixel recurrent neural network. In ICML 

2016.

[21] Y. Pu  Z. Gan  R. Henao  X. Yuan  C. Li  A. Stevens  and L. Carin. Variational autoencoder for

deep learning of images  labels and captions. In NIPS  2016.

[22] Y. Pu  X. Yuan  and L. Carin. Generative deep deconvolutional learning. In ICLR workshop 

2015.

[23] Y. Pu  X. Yuan  A. Stevens  C. Li  and L. Carin. A deep generative deconvolutional image

model. Artiﬁcial Intelligence and Statistics (AISTATS)  2016.

9

[24] R. Ranganath  L. Tang  L. Charlin  and D. M.Blei. Deep exponential families. In AISTATS 

2015.

[25] R. Ranganath  D. Tran  and D. M. Blei. Hierarchical variational models. In ICML  2016.

[26] A. Rasmus  M. Berglund  M. Honkala  H. Valpola  and T. Raiko. Semi-supervised learning

with ladder networks. In NIPS  2015.

[27] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate

inference in deep generative models. In ICML  2014.

[28] D.J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In ICML  2015.

[29] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy 
A. Khosla  M. Bernstein  A. C. Berg  and L. Fei-fei. Imagenet large scale visual recognition
challenge. IJCV  2014.

[30] D. Shen  Y. Zhang  R. Henao  Q. Su  and L. Carin. Deconvolutional latent-variable model for

text sequence matching. In arXiv  2017.

[31] J. T. Springenberg  A. Dosovitskiy  T. Brox  and M. Riedmiller. Striving for simplicity: The

all convolutional net. In ICLR workshop  2015.

[32] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: A

simple way to prevent neural networks from overﬁtting. JMLR  2014.

[33] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P.-A. Manzagol. Stacked denoising au-
toencoders: Learning useful representations in a deep network with a local denoising criterion.
JMLR  2010.

[34] Y. Pu W. Wang  R. Henao  L. Chen  Z. Gan  C. Li  and Lawrence Carin. Adversarial symmetric

variational autoencoder. In NIPS  2017.

[35] Y. Zhang  D. Shen  G. Wang  Z. Gan  R. Henao  and L. Carin. Deconvolutional paragraph

representation learning. In NIPS  2017.

[36] M. Zhou  L. Hannah  D. Dunson  and L. Carin. Beta-negative binomial process and Poisson

factor analysis. In AISTATS  2012.

10

,Yuchen Pu
Zhe Gan
Ricardo Henao
Chunyuan Li
Shaobo Han
Lawrence Carin