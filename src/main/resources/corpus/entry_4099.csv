2019,Optimal Best Markovian Arm Identification with Fixed Confidence,We give a complete characterization of the sampling complexity
of best Markovian arm identification in one-parameter Markovian bandit models. We derive instance specific nonasymptotic and asymptotic lower bounds which generalize those of the IID setting.
We analyze the Track-and-Stop strategy  initially proposed for the IID setting  and we prove that asymptotically it is at most a factor of four apart from the lower bound. Our one-parameter Markovian bandit model is based on the notion of an exponential family of stochastic matrices for which we establish many useful properties. For the analysis of the Track-and-Stop strategy we derive a novel and optimal concentration inequality for Markov chains that may be of interest in its own right.,Optimal Best Markovian Arm Identiﬁcation with

Fixed Conﬁdence

Vrettos Moulos

University of California Berkeley

vrettos@berkeley.edu

Abstract

We give a complete characterization of the sampling complexity of best Markovian
arm identiﬁcation in one-parameter Markovian bandit models. We derive instance
speciﬁc nonasymptotic and asymptotic lower bounds which generalize those of
the IID setting. We analyze the Track-and-Stop strategy  initially proposed for the
IID setting  and we prove that asymptotically it is at most a factor of four apart
from the lower bound. Our one-parameter Markovian bandit model is based on the
notion of an exponential family of stochastic matrices for which we establish many
useful properties. For the analysis of the Track-and-Stop strategy we derive a novel
concentration inequality for Markov chains that may be of interest in its own right.

1

Introduction

This paper is about optimal best Markovian arm identiﬁcation with ﬁxed conﬁdence. There are K
independent options which are referred to as arms. Each arm a is associated with a discrete time
stochastic process  which is characterized by a parameter θa and it’s governed by the probability law
Pθa. At each round we select one arm  without any prior knowledge of the statistics of the stochastic
processes. The stochastic process that corresponds to the selected arm evolves by one time step  and
we observe this evolution through a reward function  while the stochastic processes for the rest of the
arms stay still. A conﬁdence level δ ∈ (0  1) is prescribed  and our goal is to identify the arm that
corresponds to the process with the highest stationary mean with probability at least 1 − δ  and using
as few samples as possible.

1.1 Contributions

In the work of Garivier and Kaufmann (2016) the discrete time stochastic process associated with
each arm a is assumed to be an IID process. Here we go one step further and we study more
complicated dependent processes  which allow us to use more expressive models in the stochastic
multi-armed bandits framework. More speciﬁcally we consider the case that each Pθa is the law of an
irreducible ﬁnite state Markov chain associated with a stationary mean µ(θa). We establish a lower
bound (Theorem 1) for the expected sample complexity  as well as an analysis of the Track-and-Stop
strategy  proposed for the IID setting in Garivier and Kaufmann (2016)  which shows (Theorem 3)
that asymptotically the Track-and-Stop strategy in the Markovian dependence setting attains a sample
complexity which is at most a factor of four apart from our asymptotic lower bound. Both our lower
and upper bounds extend the work of Garivier and Kaufmann (2016) in the more complicated and
more general Markovian dependence setting.
The abstract framework of multi-armed bandits has numerous applications in areas like clinical trials 
ad placement  adaptive routing  resource allocation  gambling etc. For more context we refer the
interested reader to the survey of Bubeck and Cesa-Bianchi (2012). Here we generalize this model to
allow for the presence of Markovian dependence  enabling this way the practitioner to use richer and

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

more expressive models for the various applications. In particular  Markovian dependence allows
models where the distribution of next sample depends on the sample just observed. This way one can
model for instance the evolution of a rigged slot machine  which as soon as it generates a big reward
for the gambler  it changes the reward distribution to a distribution which is skewed towards smaller
rewards.
Our key technical contributions stem from the large deviations theory for Markov chains Miller
(1961); Donsker and Varadhan (1975); Ellis (1984); Dembo and Zeitouni (1998). In particular we
utilize the concept of an exponential family of stochastic matrices  ﬁrst introduced in Miller (1961) 
in order to model our one-parameter Markovian bandit model. Many properties of the family are
established which are then used for our analysis of the Track-and-Stop strategy. The most important
one is an optimal concentration inequality for the empirical means of Markov chains (Theorem 2).
We are able to establish this inequality for a large class of Markov chains  including those that all the
transitions have positive probability. Prior work on the topic  Gillman (1993); Dinwoodie (1995);
Lezaud (1998); León and Perron (2004)  fails to capture the optimal exponential decay  or introduces
a polynomial prefactor  Davisson et al. (1981)  as opposed to our constant prefactor. This result may
be of independent interest due to the wide applicability of Markov chains in many aspects of learning
theory such as various aspects of reinforcement learning  Markov chain Monte Carlo and others.

1.2 Related Work

The cornerstone of stochastic multi-armed bandits is the seminal work of Lai and Robbins (1985).
They considered K IID process with the objective being to maximize the expected value of the sum of
the observed rewards  or equivalently to minimize the so called regret. In the same spirit Anantharam
et al. (1987a b) examine the generalization where one is allowed to collect multiple rewards at each
time step  ﬁrst in the case that processes are IID Anantharam et al. (1987a)  and then in the case that
the processes are irreducible and aperiodic Markov chains Anantharam et al. (1987a). A survey of
the regret minimization literature is contained in Bubeck and Cesa-Bianchi (2012).
An alternative objective is the one of identifying the process with the highest stationary mean as fast
as and as accurately as possible  notions which are made precise in Subsection 2.1. In the IID setting 
Even-Dar et al. (2006) establish an elimination based algorithm in order to ﬁnd an approximate best
arm  and Mannor and Tsitsiklis (0304) provide a matching lower bound. Jamieson et al. (2014)
propose an upper conﬁdence strategy  inspired by the law of iterated logarithm  for exact best arm
identiﬁcation given some ﬁxed level of conﬁdence. In the asymptotic high conﬁdence regime  the
problem is settled by the work of Garivier and Kaufmann (2016)  who provide instance speciﬁc
matching lower and upper bounds. For their upper bound they propose the Track-and-Stop strategy
which is further explored in the work of Kaufmann and Koolen (2018).
The earliest reference for the exponential family of stochastic matrices which is being used to model
the Markovian arms can be found in the work of Miller (1961). Exponential families of stochastic
matrices lie in the heart of the theory of large deviations for Markov processes  which was popularized
with the pioneering work of Donsker and Varadhan (1975). A comprehensive overview of the theory
can be found in the book Dembo and Zeitouni (1998). Naturally they also show up when one
conditions on the second order empirical distribution of a Markov chain  see the work of Csiszár
et al. (1987) about conditional limit theorems. A variant of the exponential family that we are
going to discuss has been developed in the context of hypothesis testing in Nakagawa and Kanaya
(1993). A more recent development by Nagaoka (2005) gives an information geometry perspective
to this concept  and the work Hayashi and Watanabe (2016) examines parameter estimation for the
exponential family. Our development of the exponential family of stochastic matrices tries to parallel
the development of simple exponential families of probability distributions of Wainwright and Jordan
(2008).
Regarding concentration inequalities for Markov chains one of the earliest works Davisson et al.
(1981) is based on counting  and is able to capture the optimal rate of exponential decay dictated by
the theory of large deviations  but has a suboptimal polynomial prefactor. More recent approaches
follow the line of work started by Gillman (1993)  who used matrix perturbation theory to derive a
bound for reversible Markov chains. This bound attains a constant prefactor but with a suboptimal
rate of exponential decay which depends on the spectral gap of the transition matrix. This work
was later extended by Dinwoodie (1995); Lezaud (1998) but still with a sub-optimal rate. The work
of León and Perron (2004) reduces the problem to a two state Markov chain  and attains the optimal

2

rate only for the case of a two state Markov chain. Chung et al. (2012) obtain rates that depend on the
mixing time of the chain rather than the spectral gap  but which are still suboptimal.

2 Problem Formulation

2.1 One-parameter family of Markov Chains

In order to model the problem we will use a one-parameter family of Markov chains on a ﬁnite state
space S. Each Markov chain in the family corresponds to a parameter θ ∈ Θ  where Θ ⊆ R is the
parameter space  and is completely characterized by an initial distribution qθ = [qθ(x)]x∈S  and a
stochastic transition matrix Pθ = [Pθ(x  y)]x y∈S  which satisfy the following conditions.

Pθ is irreducible for all θ ∈ Θ.

qθ(x) > 0 ⇒ qλ(x)  for all θ  λ ∈ Θ  x ∈ S.

Pθ(x  y) > 0 ⇒ Pλ(x  y) > 0  for all θ  λ ∈ Θ  x  y ∈ S.

(1)
(2)
(3)
There are K Markovian arms with parameters θθθ = (θ1  . . .   θK) ∈ ΘK  and each arm a ∈ [K] =
{1  . . .   K} evolves as a Markov chain with parameter θa which we denote by {X a
n}x∈Z≥0 . A
non-constant real valued reward function f : S → R is applied at each state and produces the reward
process {Y a
n). We can only observe the reward process but not the
internal Markov chain. Note that the reward process is a function of the Markov chain and so in
general it will have more complicated dependencies than the Markov chain. The reward process is a
Markov chain if and only if f is injective. For each θ ∈ Θ there is a unique stationary distribution
πθ = [πθ(x)]x∈S associated with the stochastic matrix Pθ  due to (1). This allows us to deﬁne the
x f (x)πθ(x).
We will assume that among the K Markovian arms there exists precisely one that possess the highest
stationary mean  and we will denote this arm by a∗(θθθ)  so in particular

stationary reward of the Markov chain corresponding to the parameter θ as µ(θ) =(cid:80)

n }n∈Z≥0 given by Y a

n = f (X a

The set of all parameter conﬁgurations that possess a unique highest mean is denoted by

{a∗(θθθ)} = arg max
a∈[K]

µ(θa).

(cid:40)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)arg max

a∈[K]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 1
(cid:41)

.

ΘΘΘ =

θθθ ∈ ΘK :

µ(θa)

The Kullback-Leibler divergence rate characterizes the sample complexity of the Markovian identiﬁ-
cation problem that we are about to study. For two Markov chains of the one-parameter family that
are indexed by θ and λ respectively it is given by 

(cid:88)

x y∈S

D (θ (cid:107) λ) =

log

Pθ(x  y)
Pλ(x  y)

πθ(x)Pθ(x  y) 

where we use the standard notational conventions log 0 = ∞ 
0 = ∞ if α > 0  and 0 log 0 =
0 = 0. It is always nonnegative  D (θ (cid:107) λ) ≥ 0  with equality occurring if and only if Pθ = Pλ 
0 ln 0
and so µ(θ) (cid:54)= µ(λ) yields that D (θ (cid:107) λ) > 0. Furthermore  D (θ (cid:107) λ) < ∞ due to (2).
With some abuse of notation we will also write D (P (cid:107) Q) for the Kullback-Leibler divergence
between two probability measures P and Q on the same measurable space  which is deﬁned as

log α

(cid:105)

(cid:104)

(cid:40)EP

∞ 

D (P (cid:107) Q) =

log d P
d Q

 

if P (cid:28) Q
otherwise 

where P (cid:28) Q means that P is absolutely continuous with respect to Q  and in that case d P
the Radon-Nikodym derivative of P with respect to Q.

d Q denotes

2.2 Best Markovian Arm Identiﬁcation with Fixed Conﬁdence
Let θθθ ∈ ΘΘΘ be an unknown parameter conﬁguration for the K Markovian arms. Let δ ∈ (0  1)
be a given conﬁdence level. Our goal is to identify a∗(θθθ) with probability at least 1 − δ using

3

Na(t) =(cid:80)t

as few samples as possible. At each time t we select a single arm At and we observe the next
n }n∈Z≥0   while all the other reward processes stay still. Let
sample from the reward process {Y At
s=1 I{As=a}− 1 be the number of transitions of the Markovian arm a up to time t. Let Ft
n }NK (t)
n=0 .

be the σ-ﬁeld generated by our choices A1  . . .   At and the observations {Y 1
A sampling strategy  Aδ  is a triple Aδ = ((At)t∈Z>0   τδ  ˆaτδ ) consisting of:

n=0   . . .  {Y a

n }N1(t)

mines which arm At+1 we should sample next  so At+1 is Ft-measurable;
with respect to the ﬁltration (Ft)t∈Z>0   such that EAδ

• a sampling rule (At)t∈Z>0   which based on the past decisions and observations Ft  deter-
• a stopping rule τδ  which denotes the end of the data collection phase and is a stopping time
• a decision rule ˆaτδ  which is Fτδ-measurable  and determines the arm that we estimate to be

λλλ [τδ] < ∞ for all λλλ ∈ ΘΘΘ;

the best one.

Sampling strategies need to perform well across all possible parameter conﬁgurations in ΘΘΘ  therefore
we need to restrict our strategies to a class of uniformly accurate strategies. This motivates the
following standard deﬁnition.
Deﬁnition 1 (δ-PC). Given a conﬁdence level δ ∈ (0  1)  a sampling strategy Aδ =
((At)t∈Z>0   τδ  ˆaτδ ) is called δ-PC (Probably Correct) if 

λλλ (ˆaτδ (cid:54)= a∗(λλλ)) ≤ δ  for all λλλ ∈ ΘΘΘ.
PAδ

Therefore our goal is to study the quantity 

inf

Aδ:δ−P C

EAδ
θθθ [τδ] 

both in terms of ﬁnding a lower bound  i.e. establishing that no δ-PC strategy can have expected
sample complexity less than our lower bound  and also in terms of ﬁnding an upper bound  i.e. a
δ-PC strategy with very small expected sample complexity. We will do so in the high conﬁdence
regime of δ → 0  by establishing instance speciﬁc lower and upper bounds which differ just by a
factor of four.

3 Lower Bound on the Sample Complexity

Deriving lower bounds in the multi-armed bandits setting is a task performed by change of measure
arguments initial introduced by Lai and Robbins (1985). Those change of measure arguments capture
the simple idea that in order to identify the best arm we should at least be able to differentiate between
two bandit models that exhibit different best arms but are statistically similar. Fix θ ∈ ΘΘΘ  and deﬁne
the set of parameter conﬁgurations that exhibit as best arm an arm different than a∗(θθθ) by

Alt(θθθ) = {λλλ ∈ ΘΘΘ : a∗(λλλ) (cid:54)= a∗(θθθ)}.

(cid:32)

log

=

I{Na(t)≥0} log

K(cid:88)

a=1

Then we consider an alternative parametrization λλλ ∈ Alt(θθθ) and we write their log-likelihood ratio
up to time t

where Na(x  y  0  t) = (cid:80)t−1

|Ft
λλλ |Ft
K(cid:88)
(cid:88)
s+1 = y}. The log-likelihood ratio enables us to
s=0 1{X a
perform changes of measure for ﬁxed times t  and more generally for stopping times τ with respect
to (Ft)t∈Z>0   which are PAδ
and PAδ
λλλ -a.s. ﬁnite  through the following change of measure formula 
λλλ (E) = EAδ
PAδ

  for any E ∈ Fτ .

x y
s = x  X a

Pθa (x  y)
Pλa (x  y)

qθa (X a
0 )
qλa (X a
0 )

Na(x  y  0  t) log

IE

θθθ

d PAδ
d PAδ

θθθ

+

a=1

(cid:33)

(cid:20)

(4)

(5)

θθθ

 

(cid:21)

d Pλλλ |Fτ
d Pθθθ |Fτ

In order to derive our lower bound we use a technique developed for the IID case by Garivier and
Kaufmann (2016) which combines several changes of measure at once. To make this technique work
in the Markovian setting we need the following inequality which we derive in Appendix A using a
renewal argument for Markov chains.

4

D

(cid:13)(cid:13)(cid:13) PAδ

Lemma 1. Let θθθ ∈ ΘΘΘ and λλλ ∈ Alt(θθθ) be two parameter conﬁgurations. Let τ be a stopping time
with respect to (Ft)t∈Z>0   with EAδ
λλλ [τ ] < ∞. Then
θθθ [τ ]  EAδ
λλλ |Fτ
K(cid:88)
0}] < ∞  the ﬁrst summand is ﬁnite due to (3)  and the

(cid:17)
D (qθa (cid:107) qλa ) +

(cid:16)EAδ

(cid:16)PAδ
≤ K(cid:88)

|Fτ

θθθ

D (θa (cid:107) λa) 

θθθ [Na(τ )] + Rθa

where Rθa = Eθa [inf{n > 0 : X a
second summand is ﬁnite due to (2).

n = X a

(cid:17)

a=1

a=1

Combining those ingredients with the data processing inequality we derive our instance speciﬁc lower
bound for the Markovian bandit identiﬁcation problem in Appendix A.
Theorem 1. Assume that the one-parameter family of Markov chains on the ﬁnite state space S
satisﬁes conditions (1)  (2)  and (3). Fix δ ∈ (0  1)  let f : S → R be a nonconstant reward function 
let Aδ be a δ-PC sampling strategy  and ﬁx a parameter conﬁguration θθθ ∈ ΘΘΘ. Then

T ∗(θθθ) ≤ lim inf
δ→0

EAδ
θθθ [τδ]
log 1
δ

 

where

T ∗(θθθ)−1 =

sup

www∈M1([K])

inf

λλλ∈Alt(θθθ)

waD (θa (cid:107) λa) 

and M1 ([K]) denotes the set of all probability distributions on [K].
As noted in Garivier and Kaufmann (2016) the sup in the deﬁnition of T ∗(θθθ) is actually attained
uniquely  and therefore we can deﬁne www∗(θθθ) as the unique maximizer 

K(cid:88)

a=1

K(cid:88)

a=1

{www∗(θθθ)} = arg max
www∈M1([K])

inf

λλλ∈Alt(θθθ)

waD (θa (cid:107) λa).

4 One-Parameter Exponential Family of Markov Chains

4.1 Deﬁnition and Basic Properties

In this section we instantiate the abstract one-parameter family of Markov chains from Subsection 2.1 
with the one-parameter exponential family of Markov chains. Given the ﬁnite state space S  and the
nonconstant reward function f : S → R  we deﬁne M = maxx f (x) and m = minx f (x). Based on
f we construct two subsets of state space  SM = {x ∈ S : f (x) = M} and Sm = {x ∈ S : f (x) =
m}  corresponding to states of maximum and minimum f-value respectively. Our goal is to create a
family of Markov chains which can realize any stationary mean in the interval (m  M )  which will be
later used in order to model the Markovian arms. Towards this goal we use as a generator for our
family  an irreducible stochastic matrix P which satisﬁes the following properties.
The submatrix of P with rows and columns in SM is irreducible.
For every x ∈ S − SM   there is a y ∈ SM such that P (x  y) > 0.
The submatrix of P with rows and columns in Sm is irreducible.
For every x ∈ S − Sm  there is a y ∈ Sm such that P (x  y) > 0.

(6)
(7)
(8)
(9)
For example  a positive stochastic matrix  i.e. one where all the transition probabilities are positive 
satisﬁes all those properties. Note that in practice this can always be attained by substituting zero
transition probabilities with  transition probabilities  where  ∈ (0  1) is some small constant.
Our parameter space will be the whole real line  Θ = R. Given a parameter θ ∈ Θ  we pick an
arbitrary initial distribution qθ ∈ M1 (S) such that qθ(x) > 0 for all x ∈ S  and we tilt exponentially
all the the transitions of P by constructing the matrix ˜Pθ(x  y) = P (x  y)eθf (y). Note that ˜Pθ is
not a stochastic matrix  but we can normalize it and turn it into a stochastic matrix by invoking the
Perron-Frobenius theory. Let ρ(θ) be the spectral radius of ˜Pθ. From the Perron-Frobenius theory

5

with unique left and right eigenvectors uθ  vθ such that they are both positive (cid:80)
(cid:80)

we know that ρ(θ) is a simple eigenvalue of ˜Pθ  called the Perron-Frobenius eigenvalue  associated
x uθ(x) = 1  and
x uθ(x)vθ(x) = 1  see for instance Theorem 8.4.4 in the book Horn and Johnson (2013). Let
A(θ) = log ρ(θ) be the log-Perron-Frobenius eigenvalue  a quantity which plays a role similar to that
of a log-moment-generating function. From ˜Pθ we can construct an irreducible nonnegative matrix

Pθ(x  y) =

˜Pθ(x  y)vθ(y)

ρ(θ)vθ(x)

=

vθ(y)
vθ(x)

eθφ(y)−A(θ)P (x  y) 

which is stochastic  since(cid:88)

y

Pθ(x  y) =

1

ρ(θ)vθ(x)

˜Pθ(x  y)vθ(y) = 1.

·(cid:88)

y

In addition its stationary distributions is given by

since

(cid:88)

x

πθ(x)Pθ(x  y) =

vθ(y)
ρ(θ)

uθ(x) ˜Pθ(x  y) = uθ(y)vθ(y) = πθ(y).

πθ(x) = uθ(x)vθ(x) 

·(cid:88)

x

Note that the generator stochastic matrix P   is the member of the family that corresponds to θ = 0 
i.e. P = P0  ρ(0) = 1  and A(0) = 0.
The following lemma  whose proof is presented in Appendix B  suggests that the family can be
reparametrized using the mean parameters µ(θ). More speciﬁcally ˙A is a strictly increasing bijection
between the set Θ of canonical parameters and the set M = {µ ∈ (m  M ) : µ(θ) = µ  for some θ ∈
Θ} of mean parameters. Therefore with some abuse of notation  we will write uµ  vµ  Pµ  πµ for
u ˙A−1(µ)  v ˙A−1(µ)  P ˙A−1(µ)  π ˙A−1(µ)  and D (µ1 (cid:107) µ2) for D
Lemma 2. Let P be an irreducible stochastic matrix stochastic matrix on a ﬁnite state space S which
combined with a real-valued function f : S → R satisﬁes (6)  (7)  (8) and (9). Then the following
properties hold true for the exponential family of stochastic matrices generated by P and f.

(cid:13)(cid:13)(cid:13) ˙A−1(µ2)
(cid:17)

(cid:16) ˙A−1(µ1)

.

(a) ρ(θ)  A(θ)  uθ and vθ are analytic functions of θ on Θ = R.
(b)

˙A(θ) = µ(θ)  for all θ ∈ Θ.
˙A(θ) is strictly increasing.

(c)
(d) M = (m  M ).

4.2 Concentration for Markov Chains
For a Markov chain {Xn}n∈Z≥0   driven by an irreducible transition matrix P and an initial
distribution q  the large deviations theory  Miller (1961); Donsker and Varadhan (1975); Ellis
(1984); Dembo and Zeitouni (1998)  suggests that the probability of the large deviation event
{f (X1) + . . . + f (Xn) ≥ nµ}  when µ is greater or equal than the stationary mean µ(0)  asymptoti-
cally is an exponential decay with the rate of the decay given by a Kullback-Leibler divergence rate.
In particular Theorem 3.1.2. from Dembo and Zeitouni (1998) in our context can be written as

lim
n→∞

1
n

log P0(f (X1) + . . . + f (Xn) ≥ nµ) = −A∗(µ)  for any µ ≥ µ(0) 

where A∗(µ) = supθ∈R{θµ− A(θ)} is the convex conjugate of the log-Perron-Frobenius eigenvalue
and represents a Kullback-Leibler divergence rate as we illustrate in Lemma 10.
In the following theorem we present a concentration inequality for Markov chains which attains the
rate of exponential decay prescribed from the large deviations theory  as well as a constant prefactor
which is independent from µ.
Theorem 2. Let S be a ﬁnite state space  and let P be an irreducible stochastic matrix on S  which
combined with a function f : S → R satisﬁes (6)  (7)  (8)  and (9). Fix θ ∈ R  and let {Xn}n∈Z≥0

6

be a Markov chain on S  which is driven by Pθ  the stochastic matrix from the exponential family
which corresponds to the parameter θ and has stationary mean µ(θ). Then

Pθ (f (X1) + . . . + f (Xn) ≥ nµ) ≤ C 2e−nD(µ (cid:107) µ(θ))  for µ ∈ [µ(θ)  M ] 

where C = C(P  f ) is a constant depending only on the generator stochastic matrix P and the
P (y z)
function f. In particular  if P is a positive stochastic matrix then we can take C = maxx y z
P (x z) .

We note that in the special case that the process is an IID process the constant C(P  φ) can be taken to
be 1  and thus Theorem 2 generalizes the classic Cramer-Chernoff bound  Chernoff (1952). Observe
also that Theorem 2 has a straightforward counterpart for the lower tail as well.
Moreover our inequality is optimal up to the constant prefactor  since the exponential decay is
unimprovable due to the large deviations theory  while with respect to the prefactor we can not expect
anything better than a constant because otherwise we would contradict the central limit theorem for
Markov chains. In particular  when our conditions on P and f are met  our bound dominates similar
bounds given by Davisson et al. (1981); Gillman (1993); Dinwoodie (1995); Lezaud (1998); León
and Perron (2004).
We give a proof of Theorem 2 in Appendix C  where the main techniques involved are a uniform
upper bound on the ratio of the entries of the right Perron-Frobenius eigenvector  as well as an
approximation of the log-Perron-Frobenius eigenvalue using the log-moment-generating function.

5 Upper Bound on the Sample Complexity: the (α  δ)
(α  δ)-Track-and-Stop
(α  δ)

Strategy

The (α  δ)-Track-and-Stop strategy  which was proposed in Garivier and Kaufmann (2016) in order to
tackle the IID setting  tries to track the optimal weights w∗
a(θθθ). In the sequel we will also write www∗(µµµ) 
with µµµ = (µ(θ1)  . . .   µ(θK))  to denote www∗(θθθ). Not having access to µµµ  the (α  δ)-Track-and-Stop
strategy tries to approximate µµµ using sample means. Let ˆµµµ(t) = (ˆµ1(N1(t))  . . .   ˆµK(NK(t))) be
the sample means of the K Markov chains when t samples have been observed overall and the
calculation of the very ﬁrst sample from each Markov chain is excluded from the calculation of its
sample mean  i.e.

Na(t)(cid:88)

ˆµa(t) =

1

Y a
s .

Na(t)

s=1

By imposing sufﬁcient exploration the law of large numbers for Markov chains will kick in and the
sample means ˆµµµ(t) will almost surely converge to the true means µµµ  as t → ∞.
We proceed by brieﬂy describing the three components of the (α  δ)-Track-and-Stop strategy.

5.1 Sampling Rule: Tracking the Optimal Proportions

For initialization reasons the ﬁrst 2K samples that we are going to observe are Y 1
After that  for t ≥ 2K we let Ut = {a : Na(t) <

1   . . .   Y K
t − K/2} and we follow the tracking rule:

0   Y 1

1 .
0   Y K

√

arg min

arg max
a=1 ... K

a∈Ut

At+1 ∈

(cid:26)

Na(t) 
a(ˆµµµ(t)) − Na(t)
w∗

t

(cid:27)

if Ut (cid:54)= ∅

(forced exploration) 

  otherwise

(direct tracking).

The forced exploration step is there to ensure that ˆµµµ(t) a.s.→ µµµ as t → ∞. Then the continuity of
µµµ (cid:55)→ www∗(µµµ)  combined with the direct tracking step guarantees that almost surely the frequencies

Na(t)

t

converge to the optimal weights w∗

a(µµµ) for all a = 1  . . .   K.

7

5.2 Stopping Rule: (α  δ)
(α  δ)-Chernoff’s Stopping Rule
(α  δ)

For the stopping rule we will need the following statistics. For any two distinct arms a  b if
ˆµa(Na(t)) ≥ ˆµb(Nb(t))  we deﬁne

Za b(t) =

Na(t)

Na(t) + Nb(t)

Nb(t)

Na(t) + Nb(t)

D (ˆµa(Na(t)) (cid:107) ˆµa b(Na(t)  Nb(t)))+

D (ˆµb(Nb(t)) (cid:107) ˆµa b(Na(t)  Nb(t))) 

while if ˆµa(Na(t)) < ˆµb(Nb(t))  we deﬁne Za b(t) = −Zb a(t)  where
Nb(t)

Na(t)

ˆµa b(Na(t)  Nb(t)) =

Na(t) + Nb(t)

Na(t) + Nb(t)

ˆµa(Na(t)) +

ˆµb(Nb(t)).

Note that the statistics Za b(t) do not arise as the closed form solutions of the Generalized Likelihood
Ratio statistics for Markov chains  as it is the case in the IID bandits setting.
For a conﬁdence level δ ∈ (0  1)  and a convergence parameter α > 1 we deﬁne the (α  δ)-Chernoff
stopping rule following Garivier and Kaufmann (2016)

τα δ = inf {t ∈ Z>0 : ∃a ∈ {1  . . .   K} ∀b (cid:54)= a  Za b(t) > (0 ∨ βα δ(t))}  

2αKC 2
α − 1

δ   D =

where βα δ(t) = 2 log Dtα
  and C = C(P  f ) is the constant from Lemma 11. In
the special case that P is a positive stochastic matrix we can explicitly set C = maxx y z
P (x z). It
is important to notice that the constant C = C(P  f ) does not depend on the bandit instance θθθ or the
conﬁdence level δ  but only on the generator stochastic matrix P and the reward function f. In other
words it is a characteristic of the exponential family of Markov chains and not of the particular bandit
instance  θθθ  under consideration.

P (y z)

5.3 Decision Rule: Best Sample Mean

For a ﬁxed arm a it is clear that  minb(cid:54)=a Za b(t) > 0 if and only if ˆµa(Na(t)) > ˆµb(Nb(t)) for all
b (cid:54)= a. Hence the following simple decision rule is well deﬁned when used in conjunction with the
(α  δ)-Chernoff stopping rule:

{ˆaτα δ} = arg max

a=1 ... K

ˆµa(Na(ττα δ )).

5.4 Sample Complexity Analysis

In this section we establish that the (α  δ)-Track-and-Stop strategy is δ-PC  and we upper bound its
expected sample complexity. In order to do this we use our Markovian concentration bound Theo-
rem 2.
We ﬁrst use it in order to establish the following uniform deviation bound.
Lemma 3. Let θθθ ∈ ΘΘΘ  δ ∈ (0  1)  and α > 1. Let Aδ be a sampling strategy that uses an arbitrary
sampling rule  the (α  δ)-Chernoff’s stopping rule and the best sample mean decision rule. Then  for
any arm a 

PAδ

θθθ

(∃t ∈ Z>0 : Na(t)D (ˆµa(Na(t)) (cid:107) µa) ≥ βα δ(t)/2) ≤ δ
K

.

With this in our possession we are able to prove in Appendix D that the (α  δ)-Track-and-Stop strategy
is δ-PC.
Proposition 1. Let δ ∈ (0  1)  and α ∈ (1  e/4]. The (α  δ)-Track-and-Stop strategy is δ-PC.
Finally  we obtain that in the high conﬁdence regime  δ → 0  the (α  δ)-Track-and-Stop strategy
has a sample complexity which is at most 4α times the asymptotic lower bound that we established
in Theorem 1.
Theorem 3. Let θθθ ∈ ΘΘΘ  and α ∈ (1  e/4]. The (α  δ)-Track-and-Stop strategy  denoted here by Aδ 
has its asymptotic expected sample complexity upper bounded by 
≤ 4αT ∗(θθθ).

lim sup

EAδ
θθθ [τα δ]
log 1
δ

δ→0

8

Acknowledgements

We would like to thank Venkat Anantharam  Jim Pitman and Satish Rao for many helpful discussions.
This research was supported in part by the NSF grant CCF-1816861.

References
Anantharam  V.  Varaiya  P.  and Walrand  J. (1987a). Asymptotically efﬁcient allocation rules for the
multiarmed bandit problem with multiple plays. I. I.I.D. rewards. IEEE Trans. Automat. Control 
32(11):968–976.

Anantharam  V.  Varaiya  P.  and Walrand  J. (1987b). Asymptotically efﬁcient allocation rules for
the multiarmed bandit problem with multiple plays. II. Markovian rewards. IEEE Trans. Automat.
Control  32(11):977–982.

Bubeck  S. and Cesa-Bianchi  N. (2012). Regret Analysis of Stochastic and Nonstochastic Multi-

armed Bandit Problems. Foundations and Trends in Machine Learning  5(1):1–122.

Chernoff  H. (1952). A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum

of observations. Ann. Math. Statistics  23:493–507.

Chung  K.-M.  Lam  H.  Liu  Z.  and Mitzenmacher  M. (2012). Chernoff-Hoeffding Bounds for

Markov Chains: Generalized and Simpliﬁed. In STACS.

Cover  T. M. and Thomas  J. A. (2006). Elements of information theory. Wiley-Interscience [John

Wiley & Sons]  Hoboken  NJ  second edition.

Csiszár  I.  Cover  T. M.  and Choi  B. S. (1987). Conditional limit theorems under Markov condition-

ing. IEEE Trans. Inform. Theory  33(6):788–801.

Davisson  L. D.  Longo  G.  and Sgarro  A. (1981). The error exponent for the noiseless encoding of

ﬁnite ergodic Markov sources. IEEE Trans. Inform. Theory  27(4):431–438.

Dembo  A. and Zeitouni  O. (1998). Large deviations techniques and applications  volume 38 of

Applications of Mathematics (New York). Springer-Verlag  New York  second edition.

Dinwoodie  I. H. (1995). A probability inequality for the occupation measure of a reversible Markov

chain. Ann. Appl. Probab.  5(1):37–43.

Donsker  M. D. and Varadhan  S. R. S. (1975). Asymptotic evaluation of certain Markov process

expectations for large time. I. II. Comm. Pure Appl. Math.  28:1–47; ibid. 28 (1975)  279–301.

Durrett  R. (2010). Probability: theory and examples  volume 31 of Cambridge Series in Statistical

and Probabilistic Mathematics. Cambridge University Press  Cambridge  fourth edition.

Ellis  R. S. (1984). Large deviations for a general class of random vectors. Ann. Probab.  12(1):1–12.

Even-Dar  E.  Mannor  S.  and Mansour  Y. (2006). Action elimination and stopping conditions for
the multi-armed bandit and reinforcement learning problems. J. Mach. Learn. Res.  7:1079–1105.

Garivier  A. and Kaufmann  E. (2016). Optimal best arm identiﬁcation with ﬁxed conﬁdence.

Proceedings of the 29th Conference On Learning Theory  49:1–30.

Gillman  D. (1993). A Chernoff bound for random walks on expander graphs. In 34th Annual
Symposium on Foundations of Computer Science (Palo Alto  CA  1993)  pages 680–691. IEEE
Comput. Soc. Press  Los Alamitos  CA.

Hayashi  M. and Watanabe  S. (2016). Information geometry approach to parameter estimation in

Markov chains. Ann. Statist.  44(4):1495–1535.

Horn  R. A. and Johnson  C. R. (2013). Matrix analysis. Cambridge University Press  Cambridge 

second edition.

9

Jamieson  K. G.  Malloy  M.  Nowak  R. D.  and Bubeck  S. (2014).

lil’ UCB : An Optimal
Exploration Algorithm for Multi-Armed Bandits. In COLT  volume 35 of JMLR Workshop and
Conference Proceedings  pages 423–439.

Kaufmann  E. and Koolen  W. (2018). Mixture martingales revisited with applications to sequential

tests and conﬁdence intervals.

Lai  T. L. and Robbins  H. (1985). Asymptotically efﬁcient adaptive allocation rules. Adv. in Appl.

Math.  6(1):4–22.

Lax  P. D. (2007). Linear algebra and its applications. Pure and Applied Mathematics (Hoboken).

Wiley-Interscience [John Wiley & Sons]  Hoboken  NJ  second edition.

León  C. A. and Perron  F. (2004). Optimal Hoeffding bounds for discrete reversible Markov chains.

Ann. Appl. Probab.  14(2):958–970.

Lezaud  P. (1998). Chernoff-type bound for ﬁnite Markov chains. Ann. Appl. Probab.  8(3):849–867.

Mannor  S. and Tsitsiklis  J. N. (2003/04). The sample complexity of exploration in the multi-armed

bandit problem. J. Mach. Learn. Res.  5:623–648.

Miller  H. D. (1961). A convexity property in the theory of random variables deﬁned on a ﬁnite

Markov chain. Ann. Math. Statist.  32:1260–1270.

Nagaoka  H. (2005). The exponential family of Markov chains and its information geometry. In
Proceedings of The 28th Symposium on Information Theory and Its Applications (SITA2005)  pages
1091–1095  Okinawa  Japan.

Nakagawa  K. and Kanaya  F. (1993). On the converse theorem in statistical hypothesis testing for

Markov chains. IEEE Trans. Inform. Theory  39(2):629–633.

Ortega  J. M. (1990). Numerical analysis  volume 3 of Classics in Applied Mathematics. Society for
Industrial and Applied Mathematics (SIAM)  Philadelphia  PA  second edition. A second course.

Wainwright  M. J. and Jordan  M. I. (2008). Graphical Models  Exponential Families  and Variational

Inference. Found. Trends Mach. Learn.  1(1-2):1–305.

10

,Ahmet Alacaoglu
Quoc Tran Dinh
Olivier Fercoq
Volkan Cevher
Vrettos Moulos