2012,Symbolic Dynamic Programming for Continuous State and Observation POMDPs,Partially-observable Markov decision processes (POMDPs) provide a powerful model for real-world sequential decision-making problems. In recent years  point- based value iteration methods have proven to be extremely effective techniques for ﬁnding (approximately) optimal dynamic programming solutions to POMDPs when an initial set of belief states is known. However  no point-based work has provided exact point-based backups for both continuous state and observation spaces  which we tackle in this paper. Our key insight is that while there may be an inﬁnite number of possible observations  there are only a ﬁnite number of observation partitionings that are relevant for optimal decision-making when a ﬁnite  ﬁxed set of reachable belief states is known. To this end  we make two important contributions: (1) we show how previous exact symbolic dynamic pro- gramming solutions for continuous state MDPs can be generalized to continu- ous state POMDPs with discrete observations  and (2) we show how this solution can be further extended via recently developed symbolic methods to continuous state and observations to derive the minimal relevant observation partitioning for potentially correlated  multivariate observation spaces. We demonstrate proof-of- concept results on uni- and multi-variate state and observation steam plant control.,Symbolic Dynamic Programming for Continuous

State and Observation POMDPs

Zahra Zamani
ANU & NICTA

Canberra  Australia

Scott Sanner
NICTA & ANU

Canberra  Australia

zahra.zamani@anu.edu.au

scott.sanner@nicta.com.au

Pascal Poupart
U. of Waterloo
Waterloo  Canada

Kristian Kersting

Fraunhofer IAIS & U. of Bonn

Bonn  Germany

ppoupart@uwaterloo.ca

kristian.kersting@iais.fraunhofer.de

Abstract

Point-based value iteration (PBVI) methods have proven extremely effective for
ﬁnding (approximately) optimal dynamic programming solutions to partially-
observable Markov decision processes (POMDPs) when a set of initial belief
states is known. However  no PBVI work has provided exact point-based back-
ups for both continuous state and observation spaces  which we tackle in this
paper. Our key insight is that while there may be an inﬁnite number of observa-
tions  there are only a ﬁnite number of continuous observation partitionings that
are relevant for optimal decision-making when a ﬁnite  ﬁxed set of reachable be-
lief states is considered. To this end  we make two important contributions: (1) we
show how previous exact symbolic dynamic programming solutions for continu-
ous state MDPs can be generalized to continuous state POMDPs with discrete ob-
servations  and (2) we show how recently developed symbolic integration methods
allow this solution to be extended to PBVI for continuous state and observation
POMDPs with potentially correlated  multivariate continuous observation spaces.

1

Introduction

Partially-observable Markov decision processes (POMDPs) are a powerful modeling formalism for
real-world sequential decision-making problems [3]. In recent years  point-based value iteration
methods (PBVI) [5  10  11  7] have proved extremely successful at scaling (approximately) optimal
POMDP solutions to large state spaces when a set of initial belief states is known.
While PBVI has been extended to both continuous state and continuous observation spaces  no prior
work has tackled both jointly without sampling. [6] provides exact point-based backups for contin-
uous state and discrete observation problems (with approximate sample-based extensions to contin-
uous actions and observations)  while [2] provides exact point-based backups (PBBs) for discrete
state and continuous observation problems (where multivariate observations must be conditionally
independent). While restricted to discrete states  [2] provides an important insight that we exploit in
this work: only a ﬁnite number of partitionings of the observation space are required to distinguish
between the optimal conditional policy over a ﬁnite set of belief states.
We propose two major contributions: First  we extend symbolic dynamic programming for con-
tinuous state MDPs [9] to POMDPs with discrete observations  arbitrary continuous reward and
transitions with discrete noise (i.e.  a ﬁnite mixture of deterministic transitions). Second  we extend
this symbolic dynamic programming algorithm to PBVI and the case of continuous observations

1

(while restricting transition dynamics to be piecewise linear with discrete noise  rewards to be piece-
wise constant  and observation probabilities and beliefs to be uniform) by building on [2] to derive
relevant observation partitions for potentially correlated  multivariate continuous observations.

2 Hybrid POMDP Model

A hybrid (discrete and continuous) partially observable MDP (H-POMDP)
is a tuple
(cid:104)S A O T  R Z  γ  h(cid:105). States S are given by vector (ds  xs) = (ds1  . . .   dsn   xs1  . . .   xsm )
where each dsi ∈ {0  1} (1 ≤ i ≤ n) is boolean and each xsj ∈ R (1 ≤ j ≤ m) is continuous. We
assume a ﬁnite  discrete action space A = {a1  . . .   ar}. Observations O are given by the vector
(do  xo) = (do1   . . .   dop   xo1  . . .   xoq ) where each doi ∈ {0  1} (1 ≤ i ≤ p) is boolean and each
xoj ∈ R (1 ≤ j ≤ q) is continuous.
Three functions are required for modeling H-POMDPs: (1) T : S × A × S → [0  1] a Markovian
transition model deﬁned as the probability of the next state given the action and previous state; (2)
R : S × A → R a reward function which returns the immediate reward of taking an action in
some state; and (3) an observation function deﬁned as Z : S × A × O → [0  1] which gives the
probability of an observation given the outcome of a state after executing an action. A discount
factor γ  0 ≤ γ ≤ 1 is used to discount rewards t time steps into the future by γt.
We use a dynamic Bayes net (DBN)1 to compactly represent the transition model T over the factored
state variables and we use a two-layer Bayes net to represent the observation model Z:

T : p(x

s|xs ds  a) =
(cid:48)
(cid:48)
s d

Z : p(xo do|x

(cid:48)
(cid:48)
s d
s  a) =

sj|xs ds  d
(cid:48)
p(x

(cid:48)
s  a).

p(xoj|x

(cid:48)
(cid:48)
s d
s  a).

(1)

(2)

i=1

p(d

si|xs ds  a)
(cid:48)

m(cid:89)
n(cid:89)
q(cid:89)
p(cid:89)
|xs ds a) and p(doi|x(cid:48)

p(doi|x

(cid:48)
(cid:48)
s d
s  a)

j=1

j=1

i=1

si

sj

s d(cid:48)

|xs ds d(cid:48)

s. Observation probabilities over continuous variables p(xoj|x(cid:48)

Probabilities over discrete variables p(d(cid:48)
s a) may condition on both dis-
crete variables and (nonlinear) inequalities of continuous variables; this is further restricted to
linear inequalities in the case of continuous observations. Transitions over continuous variables
p(x(cid:48)
s a) must be deterministic (but arbitrary nonlinear) piecewise functions; in the case of
continuous observations they are further restricted to be piecewise linear; this permits discrete noise
in the continuous transitions since they may condition on stochastically sampled discrete next-state
variables d(cid:48)
s a) only occur in the
case of continuous observations and are required to be piecewise constant (a mixture of uniform dis-
tributions); the same restriction holds for belief state representations. The reward R(d  x  a) may be
an arbitrary (nonlinear) piecewise function in the case of deterministic observations and a piecewise
constant function in the case of continuous observations. We now provide concrete examples.
Example (Power Plant) [1] The steam generation system of a power plant evaporates feed-water
under restricted pressure and temperature conditions to turn a steam turbine. A reward is obtained
when electricity is generated from the turbine and the steam pressure and temperature are within safe
ranges. Mixing water and steam makes the respective pressure and temperature observations po ∈ R
and to ∈ R on the underlying state ps ∈ R and ts ∈ R highly uncertain. Actions A = {open  close}
control temperature and pressure by means of a pressure valve.
We initially present two H-POMDP variants labeled 1D-Power Plant using a single temperature
state variable ts. The transition and reward are common to both — temperature increments (decre-
ments) with a closed (opened) valve  a large negative reward is given for a closed valve with ts
exceeding critical threshold 15  and positive reward is given for a safe  electricity-producing state:

s d(cid:48)

(cid:34)

(cid:40)

(cid:35)

s|ts  a) = δ
(cid:48)

p(t

s −
(cid:48)

t

(a = open)
(a = close)

: ts − 5
: ts + 7

R(ts  a) =

(a = open)

(a = close) ∧ (ts > 15)
(a = close) ∧ ¬(ts > 15)

: −1
: −1000
: 100

(3)

Next we introduce the Discrete Obs. 1D-Power Plant variant where we deﬁne an observation space
with a single discrete binary variable o ∈ O = {high  low}:

1We disallow general synchronic arcs for simplicity of exposition but note their inclusion only places re-

strictions on the variable elimination ordering used during the dynamic programming backup operation.

2

(cid:40)

(cid:40)

Figure 1: (left) Example conditional plan βh for discrete observations; (right) example α-function for βh over
state b ∈ {0  1}  x ∈ R in decision diagram form: the true (1) branch is solid  the false (0) branch is dashed.

p(o = high|t

(cid:48)
s  a = open) =

s ≤ 15 : 0.9
t(cid:48)
t(cid:48)
s > 15 : 0.1

p(o = high|t

(cid:48)
s  a = close) =

s ≤ 15 : 0.7
t(cid:48)
t(cid:48)
s > 15 : 0.3

(4)

Finally we introduce the Cont. Obs. 1D-Power Plant variant where we deﬁne an observation space
with a single continuous variable to uniformly distributed on an interval of 10 units centered at t(cid:48)
s.

p(to|t

(cid:48)
s  a = open) = U (to; t

s − 5  t
(cid:48)

(cid:48)
s + 5) =

(to > t(cid:48)
(to ≤ t(cid:48)

s − 5) ∧ (to < t(cid:48)
s − 5) ∨ (to ≥ t(cid:48)

s + 5)
s + 5)

: 0.1
: 0

(5)

(cid:40)

While simple  we note no prior method could perform exact point-based backups for either problem.
3 Value Iteration for Hybrid POMDPs

In an H-POMDP  the agent does not directly observe the states and thus must maintain a belief state
b(xs ds) = p(xs ds). For a given belief state b = b(xs ds)  a POMDP policy π can be represented
by a tree corresponding to a conditional plan β. An h-step conditional plan βh can be deﬁned
recursively in terms of (h − 1)-step conditional plans as shown in Fig. 1 (left). Our goal is to ﬁnd a
policy π that maximizes the value function  deﬁned as the sum of expected discounted rewards over
horizon h starting from initial belief state b:

(cid:104)(cid:88)h

(cid:12)(cid:12)(cid:12)b0 = b

(cid:105)

V h
π (b) = Eπ

γt · rt

t=0

where rt is the reward obtained at time t and b0 is the belief state at t = 0. For ﬁnite h and belief
state b  the optimal policy π is given by an h-step conditional plan βh. For h = ∞  the optimal
discounted (γ < 1) value can be approximated arbitrarily closely by a sufﬁciently large h [3].
Even when the state is continuous (but the actions and observations are discrete)  the optimal
POMDP value function for ﬁnite horizon h is a piecewise linear and convex function of the be-
lief state b [6]  hence V h is given by a maximum over a ﬁnite set of “α-functions” αh
i :

V h(b) = max
i ∈Γh
αh

(cid:104)αh

i   b(cid:105) = max
i ∈Γh
αh

i (xs ds) · b(xs ds) dxs
αh

(7)

(cid:90)

(cid:88)

xs

ds

Later on when we tackle continuous state and observations  we note that we will dynamically derive
an optimal  ﬁnite partitioning of the observation space for a given belief state and hence reduce the
continuous observation problem back to a discrete observation problem at every horizon.
The Γh in this optimal h-stage-to-go value function can be computed via Monahan’s dynamic pro-
1}  and assuming
gramming approach to value iteration (VI) [4]. Initializing α0
discrete observations o ∈ Oh  Γh is obtained from Γh−1 as follows:2
(cid:48)
(x
s d

1 = 0  Γ0 = {α0

s|xs ds  a)αh−1
(cid:48)
(cid:48)
s d

s)dxs(cid:48) ; ∀αh−1
(cid:48)

gh
a o j(xs ds) =

(cid:48)
(cid:48)
s d
s  a)p(x

(cid:88)

∈ Γh−1

p(o|x

(cid:90)

j

j

(6)

(8)

(9)

xs(cid:48)

ds(cid:48)

a = R(xs ds  a) + γ(cid:1)o∈O
Γh
Γh =

(cid:91)

(cid:110)

gh
a o j(xs ds)

(cid:111)

j

(10)
2The (cid:1) of sets is deﬁned as (cid:1)j∈{1 ... n}Sj = S1(cid:1)···(cid:1)Sn where the pairwise cross-sum P(cid:1)Q =

a

Γh
a

{p + q|p ∈ P  q ∈ Q}.

3

121 + (3 * x)(1 * x) >= 50(1 * x) <= 39234 + (1.5 * x)197 + (2 * x)250b (1 * x) >= 150(1 * x) <= 139// Derive relevant observation partitions Oh
(cid:104)Oh

s  a)(cid:105) := GenRelObs(Γh−1

i   p(Oh

i |x(cid:48)

s d(cid:48)

i for belief bi
P BV I   a  bi)

else

// Discrete observations and model already known
Oh
i := {do}; p(Oh
foreach o ∈ Oh
i do
foreach αh−1
P BV I do
:= Prime(αh−1

i |x(cid:48)
∈ Γh−1

) // ∀di: di → d(cid:48)

s  a) := see Eq (2)

s d(cid:48)

j

i and ∀xi: xi → x(cid:48)

i

begin

P BV I := ∅

P BV I = {α0
1}

foreach a ∈ A do

V 0 := 0  h := 0  Γ0
while h < H do

h := h + 1  Γh := ∅  Γh
foreach bi ∈ B do

a := ∅
Γh
if (continuous observations: q > 0) then

Algorithm 1: PBVI(H-POMDP  H  B = {bi}) −→ (cid:104)V h(cid:105)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

bi := arg maxαj∈Γh αj · bi
αh
P BV I := Γh
Γh

// Terminate if early convergence
if Γh

αh−1
j
j
a o j := see Eq (8)
gh

a := see Eq (9)
Γh
Γh := Γh ∪ Γh

P BV I = Γh−1
break

P BV I ∪ αh

P BV I then

return ΓP BV I

end

a

bi

// Retain only α-functions optimal at each belief point
foreach bi ∈ B do

Point-based value iteration (PBVI) [5  11] computes the value function only for a set of belief
states {bi} where bi := p(xs ds). The idea is straightforward and the main modiﬁcation needed to
Monahan’s VI approach in Algorithm 1 is the loop from lines 23–25 where only α-functions optimal
at some belief state are retained for subsequent iterations. In the case of continuous observation
variables (q > 0)  we will need to derive a relevant set of observations on line 10  a key contribution
of this work as described in Section 4.3. Otherwise if the observations are only discrete (q = 0) 
then a ﬁnite set of observations is already known and the observation function as given in Eq (2).
We remark that Algorithm 1 is a generic framework that can be used for both PBVI and other
variants of approximate VI. If used for PBVI  an efﬁcient direct backup computation of the optimal
α-function for belief state bi should be used in line 17 that is linear in the number of observations [5 
11] and which obviates the need for lines 23–25. However  for an alternate version of approximate
value iteration that will often produce more accurate values for belief states other than those in B 
one may instead retain the full cross-sum backup of line 17  but omit lines 23–25 — this yields
an approximate VI approach (using discretized observations relevant only to a chosen set of belief
states B if continuous observations are present) that is not restricted to alpha-functions only optimal
at B  hence allowing greater ﬂexibility in approximating the value function over all belief states.
Whereas PBVI is optimal if all reachable belief states within horizon H are enumerated in B  in
the H-POMDP setting  the generation of continuous observations will most often lead to an inﬁnite
number of reachable belief states  even with ﬁnite horizon — this makes it quite difﬁcult to provide
optimality guarantees in the general case of PBVI for continuous observation settings. Nonethe-
less  PBVI has been quite successful in practice without exhaustive enumeration of all reachable
beliefs [5  10  11  7]  which motivates our use of PBVI in this work.

4

4 Symbolic Dynamic Programming

In this section we take a symbolic dynamic programming (SDP) approach to implementing VI and
PBVI as deﬁned in the last section. To do this  we need only show that all required operations can
be computed efﬁciently and in closed-form  which we do next  building on SDP for MDPs [9].

4.1 Case Representation and Extended ADDs

The previous Power Plant examples represented all functions in case form  generally deﬁned as



f =

φ1 :

...
φk :

f1

...
fk

and this is the form we use to represent all functions in an H-POMDP. The φi are disjoint logical
formulae deﬁned over xs ds and/or xo do with logical (∧ ∨ ¬) combinations of boolean variables
and inequalities (≥  > ≤  <) over continuous variables. For discrete observation H-POMDPs  the
fi and inequalities may use any function (e.g.  sin(x1) > log(x2)· x3); for continuous observations 
they are restricted to linear inequalities and linear or piecewise constant fi as described in Section 2.
For unary operations such as scalar multiplication c · f (for some constant c ∈ R) or negation
−f on case statements is simply to apply the operation on each case partition fi (1 ≤ i ≤ k). A
binary operation on two case statements  takes the cross-product of the logical partitions of each
case statement and performs the corresponding operation on the resulting paired partitions. The
cross-sum ⊕ of two cases is deﬁned as the following:

Likewise (cid:9) and ⊗ are deﬁned by subtracting or multiplying partition values. Inconsistent partitions
can be discarded when they are irrelevant to the function value. A symbolic case maximization is
deﬁned as below:

(cid:40)

(cid:40)

φ1 :
φ2 :

⊕

f1
f2

ψ1 :
ψ2 :

g1
g2

=

(cid:32)(cid:40)

(cid:40)

(cid:33)

casemax

φ1 : f1
φ2 : f2

 

ψ1 : g1
ψ2 : g2

=




φ1 ∧ ψ1 :
φ1 ∧ ψ2 :
φ2 ∧ ψ1 :
φ2 ∧ ψ2 :

f1 + g1
f1 + g2
f2 + g1
f2 + g2

φ1 ∧ ψ1 ∧ f1 > g1 : f1
φ1 ∧ ψ1 ∧ f1 ≤ g1 : g1
φ1 ∧ ψ2 ∧ f1 > g2 : f1
φ1 ∧ ψ2 ∧ f1 ≤ g2 : g2
...
...

The following SDP operations on case statements require more detail than can be provided here 
hence we refer the reader to the relevant literature:

• Integration(cid:82)

• Substitution f σ: Takes a set σ of variables and their substitutions (which may be case

statements themselves)  and carries out all variable substitutions [9].

x1

f dx1: There are two forms: If x1 is involved in a δ-function (cf.

the
transition in Eq (3)) then the integral is equivalent to a symbolic substitution and can be
applied to any case statement (cf. [9]). Otherwise  if f is in linearly constrained polynomial
case form  then the approach of [8] can be applied to yield a result in the same form.

Case operations yield a combinatorial explosion in size if na¨ıvely implemented  hence we use the
data structure of the extended algebraic decision diagram (XADD) [9] as shown in Figure 1 (right)
to compactly represent case statements and efﬁciently support the above case operations with them.

4.2 VI for Hybrid State and Discrete Observations
For H-POMDPs with only discrete observations o ∈ O and observation function p(o|x(cid:48)
s  a) as in
the form of Eq (4)  we introduce a symbolic version of Monahan’s VI algorithm. In brief  we note
that all VI operations needed in Section 3 apply directly to H-POMDPs  e.g.  rewriting Eq (8):
(cid:48)
s)

(cid:33)
si|xs ds a)
(cid:48)

sj|xs ds  d
(cid:48)
(cid:48)
p(x
s a)

(cid:32) m(cid:79)

p(o|x
(cid:48)
s d

s a)⊗
(cid:48)

(cid:32) n(cid:79)

(cid:34)
(cid:77)

gh
a o j(xs ds) =

⊗αh−1

j

(cid:48)
s d

(x

s d(cid:48)

(cid:33)

(cid:35)

(cid:90)

p(d

⊗

dxs(cid:48)

xs(cid:48)

ds(cid:48)

i=1

j=1

(11)

5

s d(cid:48)

s  a)(cid:105)

// Perform exact 1-step DP backup of α-functions at horizon h − 1
αa

s  a) ⊗ p(x(cid:48)

s d(cid:48)

s d(cid:48)

s|xs ds  a) ⊗ αj(x(cid:48)

s d(cid:48)

s) dx(cid:48)

s

s

s

d(cid:48)

s d(cid:48)

begin

(cid:76)

foreach αa

p(xo do|x(cid:48)

foreach αj(x(cid:48)

s) ∈ Γh−1 and a ∈ A do

x(cid:48)
j (xs ds  xo do) do

j (xs ds  xo do) :=(cid:82)
j (xo do) :=(cid:82)
(cid:76)

Algorithm 2: GenRelObs(Γh−1  a  bi) −→ (cid:104)Oh  p(Oh|x(cid:48)
1
2
3
4
5
6
7
8
9
10
11
12
13
14

bi(xs ds) ⊗ αa

s  a)(cid:105)

s d(cid:48)

end

ds

xs

// Generate value of each α-vector at belief point bi(xs ds) as a function of observations
δa

j (xs ds  xo do) dxs

// Using casemax  generate observation partitions relevant to each policy – see text for details
Oh := extract-partition-constraints[casemax(δa1
1 (xo do)  δa2
j (xo do))]
foreach ok ∈ Oh do
// Let φok be the partition constraints for observation ok ∈ Oh
p(Oh = ok|x(cid:48)
s d(cid:48)
s  a)I[φok ]dxo
return (cid:104)Oh  p(Oh|x(cid:48)

s  a) :=(cid:82)

1 (xo do)  . . .   δar

p(xo do|x(cid:48)

(cid:76)

s d(cid:48)

do

xo

Figure 2: (left) Beliefs b1  b2 for Cont. 1D-Power Plant; (right) derived observation partitions for b2 at h = 2.

Crucially we note since the continuous transition cpfs p(x(cid:48)

deﬁned with Dirac δ’s (e.g.  Eq 3) as described in Section 2  the integral(cid:82)

s a) are deterministic and hence
xs(cid:48) can always be computed
in closed case form as discussed in Section 4.1. In short  nothing additional is required for PBVI on
H-POMDPs in this case — the key insight is simply that α-functions are now represented by case
statements and can “grow” with the horizon as they partition the state space more and more ﬁnely.

|xs ds d(cid:48)

sj

4.3 PBVI for Hybrid State and Hybrid Observations

In general  it would be impossible to apply standard VI to H-POMDPs with continuous observations
since the number of observations is inﬁnite. However  building on ideas in [2]  in the case of PBVI 
it is possible to derive a ﬁnite set of continuous observation partitions that permit exact point-based
backups at a belief point. This additional operation (GenRelObs) appears on line 10 of PBVI in
Algorithm 1 in the case of continuous observations and is formally deﬁned in Algorithm 2.
To demonstrate the generation of relevant continuous observation partitions  we use the second
iteration of the Cont. Obs. 1D-Power Plant along with two belief points represented as uniform
distributions: b1 : U (ts; 2  6) and b2 : U (ts; 6  11) as shown in Figure 2 (left). Letting h = 2  we will
assume simply for expository purposes that |Γ1| = 1 (i.e.  it contains only one α-function) and that
in lines 2–4 of Algorithm 2 we have computed the following two α-functions for a ∈ {open  close}:

(ts < 15) ∧ (ts−10 < to < ts) : 10

(ts ≥ 15) ∧ (ts−10 < to < ts) : −100
¬(ts−10 < to < ts)

: 0

αclose

1

(ts  to) =

(cid:40)

αopen

1

(ts  to) =

(ts−10 < to < ts)
: 0.1
¬(ts−10 < to < ts) : 0

We now need the α-vectors as a function of the observation space for a particular belief state  thus
next we marginalize out xs ds in lines 5–7. The resulting δ-functions are shown as follows where
for brevity from this point forward  0 partitions are suppressed in the cases:

6

tsP(t  )0.250.226111b2bs18t57.5-72.5ocloseopenP(o ) =0.0127P(o ) =0.983210414815-750.15.1(t  )o(14 < to < 18)

(8 < to < 14)
(4 < to < 8)

δclose
1

(to) =

: 0.025to − 0.45
: −0.1
: −0.025to − 0.1

δopen
1

(to) =



(15 < to < 18)
(14 < to < 15)
(8 < to < 14)
(5 < to < 8)
(4 < to < 5)

: 25to − 450
: −2.5to − 37.5
: −72.5
: −25to + 127.5
: 2.5to − 10

1

1

(to) and δopen

(to) are drawn graphically in Figure 2 (right). These observation-
Both δclose
dependent δ’s divide the observation space into regions which can yield the optimal policy according
to the belief state b2. Following [2]  we need to ﬁnd the optimal boundaries or partitions of the ob-
servation space; in their work  numerical solutions are proposed to ﬁnd these boundaries in one
dimension (multiple observations are handled through an independence assumption). Instead  here
we leverage the symbolic power of the casemax operator deﬁned in Section 4.1 to ﬁnd all the parti-
tions where each potentially correlated  multivariate observation δ is optimal. For the two δ’s above 
the following partitions of the observation space are derived by the casemax operator in line 9:

(cid:17)



o1 : (14 < to ≤ 18)
o1 : (8 < to ≤ 14)
o1 : (5.1 < to ≤ 8)
o2 : (5 < to ≤ 5.1)
o2 : (4 < to ≤ 5)

: 0.025to − 0.45
: −0.1
: −0.025to − 0.1
: −25to + 127.5
: 2.5to − 10

(cid:16)

casemax

δclose
1

(to)  δopen

1

(to)

=

1

1

s d(cid:48)
(cid:90)

is maximal and with o2 the observations
Here we have labeled with o1 the observations where δclose
where δopen
is maximal. What we really care about though are just the constraints identifying o1
and o2 and this is the task of extract-partition-constraints in line 9. This would associate with o1
the partition constraint φo1 ≡ (5.1 < to ≤ 8) ∨ (8 < to ≤ 14) ∨ (14 < to ≤ 18) and with o2 the
partition constraint φo2 ≡ (4 < to ≤ 5) ∨ (5 < to ≤ 5.1) — taking into account the 0 partitions
and the 1D nature of this example  we can further simplify φo1 ≡ (to > 5.1) and φo2 ≡ (to ≤ 5.1).
Given these relevant observation partitons  our ﬁnal task in lines 10-12 is to compute the probabil-
ities of each observation partition φok. This is simply done by marginalizing over the observation
function p(Oh|x(cid:48)
s  a) within each region deﬁned by φok (achieved by multiplying by an indicator
function I[φok ] over these constraints). To better understand what is computed here  we can compute
(cid:90)
the probability p(ok|bi  a) of each observation for a particular belief  calculated as follows:
p(ok|bi  a) :=
s)⊗bi(xs ds) dx
(cid:48)
(cid:48)
(cid:48)
sdxs (12)
s d
Speciﬁcally  for b2  we obtain p(o1|b2  a = close) = 0.0127 and p(o2|b2  a = close) = 0.933 as
shown in Figure 2 (right).
In summary  in this section we have shown how we can extend the exact dynamic programming
algorithm for the continuous state  discrete observation POMDP setting from Section 4.2 to com-
pute exact 1-step point-based backups in the continuous observation setting; this was accomplished
through the crucial insight that despite the inﬁnite number of observations  using Algorithm 2 we
can symbolically derive a set of relevant observations for each belief point that distinguish the op-
timal policy and hence value as graphically illustrated in Figure 2 (right). Next we present some
empirical results for 1- and 2-dimensional continuous state and observation spaces.

s|xs ds  a)⊗αj(x
(cid:48)

s  a)⊗p(x
(cid:48)
(cid:48)
s d

(cid:77)

(cid:77)

p(ok|x

(cid:48)
s d

d(cid:48)

x(cid:48)

s

ds

s

xs

5 Empirical Results

We evaluated our continuous POMDP solution using XADDs on the 1D-Power Plant example and
another variant of this problem with two variables  described below.3
2D-Power Plant: We consider the more complex model of the power plant similar to [1] where the
pressure inside the water tank must be controlled to avoid mixing water into the steam (leading to
explosion of the tank). We model an observable pressure reading po as a function of the underlying
pressure state ps. Again we have two actions for opening and closing a pressure valve. The close
action has transition

(cid:34)

(cid:40)

(cid:35)

s|ts  a = close) = δ(cid:2)t

(cid:48)

s − (ts + 10)(cid:3)

(cid:48)

p(t

s|ps  a = close) = δ
(cid:48)

p(p

s −
(cid:48)

p

(p + 10 > 20)
¬(p + 10 > 20)

: 20
: ps + 10

3Full problem speciﬁcations and Java code to reproduce these experiments are available online in Google

Code: http://code.google.com/p/cpomdp .

7

Figure 3: (left) time vs. horizon  and (right) space (total # XADD nodes in α-functions) vs. horizon.

and yields high reward for staying within the safe temperature and pressure range:



(5 ≤ ps ≤ 15) ∧ (95 ≤ ts ≤ 105)
(5 ≤ ps ≤ 15) ∧ (ts ≤ 95)
(ps ≥ 15)
else

: 50
: −1
: −5
: −3

R(ts  ps  a = close) =

Alternately  for the open action  the transition functions reduce the temperature by 5 units and the
pressure by 10 units as long as the pressure stays above zero. For the open reward function  we
assume that there is always a small constant penalty (-1) since no electricity is produced.
Observations are distributed uniformly within a region depending on their underlying state:

(cid:40)

(cid:40)

p(to|t

(cid:48)
s) =

(ts + 80 < to < ts + 105)
¬(ts + 80 < to < ts + 105)

: 0.04
: 0

p(po|p

(cid:48)
s) =

(ps < po < ps + 10)
¬(ps < po < ps + 10)

: 0.1
: 0

Finally for PBVI  we deﬁne two uniform beliefs as follows: b1 : U [ts; 90  100] ∗ U [ps; 0  10] and
b2 : U [ts; 90  130] ∗ U [ps; 10  30]
In Figure 3  a time and space analysis of the two versions of Power Plant have been performed for
up to horizon h = 6. This experimental evaluation relies on one additional approximation over the
PBVI approach of Algorithm 1 in that it substitutes p(Oh|b  a) in place of p(Oh|x(cid:48)
s  a) — while
this yields correct observation probabilities for a point-based backup at a particular belief state b 
the resulting α-functions represent an approximation for other belief states. In general  the PBVI
framework in this paper does not require this approximation  although when appropriate  using it
should increase computational efﬁciency.
Figure 3 shows that the computation time required per iteration generally increases since more com-
plex α-functions lead to a larger number of observation partitions and thus a more expensive backup
operation. While an order of magnitude more time is required to double the number of state and
observation variables  one can see that the PBVI approach leads to a fairly constant amount of com-
putation time per horizon  which indicates that long horizons should be computable for any problem
for which at least one horizon can be computed in an acceptable amount of time.

s d(cid:48)

6 Conclusion

We presented the ﬁrst exact symbolic operations for PBVI in an expressive subset of H-POMDPs
with continuous state and observations. Unlike related work that has extended to the continuous
state and observation setting [6]  we do not approach the problem by sampling. Rather  follow-
ing [2]  the key contribution of this work was to deﬁne a discrete set of observation partitions on
the multivariate continuous observation space via symbolic maximization techniques and derive the
related probabilities using symbolic integration. An important avenue for future work is to extend
these techniques to the case of continuous state  observation  and action H-POMDPs.

Acknowledgments

NICTA is funded by the Australian Government as represented by the Department of Broadband  Communi-
cations and the Digital Economy and the ARC through the ICT Centre of Excellence program. This work was
supported by the Fraunhofer ATTRACT fellowship STREAM and by the EC  FP7-248258-First-MM.

8

123456102103104105HorizonTime(ms)Power Plant  1 state & 1 observ var2 state & 2 observ vars123456010203040506070HorizonNumber of NodesPower Plant  1 state & 1 observ var2 state & 2 observ varsReferences
[1] Mario Agueda and Pablo Ibarguengoytia. An architecture for planning in uncertain domains.

In Proceedings of the ICTAI 2002 Conference  Dallas Texas  2002.

[2] Jesse Hoey and Pascal Poupart. Solving pomdps with continuous or large discrete observation
spaces. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence (IJCAI) 
Edinburgh  Scotland  2005.

[3] Leslie P. Kaelbling  Michael L. Littman  and Anthony R. Cassandra. Planning and acting in

partially observable stochastic domains. Artiﬁcial Intelligence  101:99–134  1998.

[4] G. E. Monahan. Survey of partially observable markov decision processes: Theory  models 

and algorithms. Management Science  28(1):1–16  1982.

[5] Joelle Pineau  Geoffrey J. Gordon  and Sebastian Thrun. Anytime point-based approximations

for large pomdps. J. Artif. Intell. Res. (JAIR)  27:335–380  2006.

[6] J. M. Porta  N. Vlassis  M.T.J. Spaan  and P. Poupart. Point-based value iteration for continuous

pomdps. Journal of Machine Learning Research  7:195220  2006.

[7] Pascal Poupart  Kee-Eung Kim  and Dongho Kim. Closing the gap: Improved bounds on
optimal pomdp solutions. In In Proceedings of the 21st International Conference on Automated
Planning and Scheduling (ICAPS-11)  2011.

[8] Scott Sanner and Ehsan Abbasnejad. Symbolic variable elimination for discrete and continuous
graphical models. In In Proceedings of the 26th AAAI Conference on Artiﬁcial Intelligence
(AAAI-12)  Toronto  Canada  2012.

[9] Scott Sanner  Karina Valdivia Delgado  and Leliane Nunes de Barros. Symbolic dynamic
programming for discrete and continuous state mdps. In Proceedings of the 27th Conference
on Uncertainty in AI (UAI-2011)  Barcelona  2011.

[10] Trey Smith and Reid G. Simmons. Point-based POMDP algorithms: Improved analysis and

implementation. In Proc. Int. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI)  2005.

[11] M. Spaan and N. Vlassis. Perseus: Randomized point-based value iteration for pomdps. Jour-

nal of Articial Intelligence Research (JAIR)  page 195220  2005.

9

,Shashank Singh
Barnabas Poczos