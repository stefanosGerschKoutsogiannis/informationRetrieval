2009,Variational Inference for the Nested Chinese Restaurant Process,The nested Chinese restaurant process (nCRP) is a powerful nonparametric Bayesian model for learning tree-based hierarchies from data. Since its posterior distribution is intractable  current inference methods have all relied on MCMC sampling. In this paper  we develop an alternative inference technique based on variational methods. To employ variational methods  we derive a tree-based stick-breaking construction of the nCRP mixture model  and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures. We demonstrate the use of this approach for text and hand written digits modeling  where we show we can adapt the nCRP to continuous data as well.,Variational Inference for the

Nested Chinese Restaurant Process

Chong Wang

Computer Science Department

Princeton University

David M. Blei

Computer Science Department

Princeton University

chongw@cs.princeton.edu

blei@cs.princeton.edu

Abstract

The nested Chinese restaurant process (nCRP) is a powerful nonparametric
Bayesian model for learning tree-based hierarchies from data. Since its poste-
rior distribution is intractable  current inference methods have all relied on MCMC
sampling. In this paper  we develop an alternative inference technique based
on variational methods. To employ variational methods  we derive a tree-based
stick-breaking construction of the nCRP mixture model  and a novel variational
algorithm that efﬁciently explores a posterior over a large set of combinatorial
structures. We demonstrate the use of this approach for text and hand written digits
modeling  where we show we can adapt the nCRP to continuous data as well.

1 Introduction

For many application areas  such as text analysis and image analysis  learning a tree-based hierarchy
is an appealing approach to illuminate the internal structure of the data. In such settings  however 
the combinatoric space of tree structures makes model selection unusually daunting. Traditional
techniques  such as cross-validation  require us to enumerate all possible model structures; this kind
of methodology quickly becomes infeasible in the face of the set of all trees.
The nested Chinese restaurant process (nCRP) [1] addresses this problem by specifying a generative
probabilistic model for tree structures. This model can then be used to discover structure from data
using Bayesian posterior computation. The nCRP has been applied to several problems  such as
ﬁtting hierarchical topic models [1] and discovering taxonomies of images [2  3].
The nCRP is based on the Chinese restaurant process (CRP) [4]  which is closely linked to the
Dirichlet process in its application to mixture models [5]. As a complicated Bayesian nonparametric
model  posterior inference in an nCRP-based model is intractable  and previous approaches all rely
Gibbs sampling [1  2  3]. While powerful and ﬂexible  Gibbs sampling can be slow to converge and
it is difﬁcult to assess the convergence [6  7]. Here  we develop an alternative for posterior inference
for nCRP-based models.
Our solution is to use the optimization-based variational methods [8]. The idea behind variational
methods is to posit a simple distribution over the latent variables  and then to ﬁt this distribution to
be close to the posterior of interest. Variational methods have been successfully applied to several
Bayesian nonparametric models  such as Dirichlet process (DP) mixtures [9  10  11]  hierarchical
Dirichlet processes (HDP) [12]  Pitman-Yor processes [13] and Indian buffet processes (IBP) [14].
The work presented here is unique in that our optimization of the variational distribution searches the
combinatorial space of trees. Similar to Gibbs sampling  our method includes an exploration of a
latent structure associated with the free parameters in addition to their values. First  we describe the
tree-based stick-breaking construction of nCRP  which is needed for variational inference. Second 
we develop our variational inference algorithm  which explores the inﬁnite tree space associated with
the nCRP. Finally  we study the performance of our algorithm on discrete and continuous data sets.

1

2 Nested Chinese restaurant process mixtures

The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions [1]. It
generalizes the Chinese restaurant process (CRP)  which is a distribution over partitions. The CRP
can be described by the following metaphor. Imagine a restaurant with an inﬁnite number of tables 
and imagine customers entering the restaurant in sequence. The dth customer sits at a table according
to the following distribution 

(cid:26) mk

γ

p(cd = k|c1:(d−1)) ∝

if k is previous occupied
if k is a new table 

(1)

where mk is the number of previous customers sitting at table k and γ is a positive scalar. After D
customers have sat down  their seating plan describes a partition of D items.
In the nested CRP  imagine now that tables are organized in a hierarchy: there is one table at the ﬁrst
level; it is associated with an inﬁnite number of tables at the second level; each second-level table
is associated with an inﬁnite number of tables at the third level; and so on until the Lth level. Each
customer enters at the ﬁrst level and comes out at the Lth level  generating a path with L tables as
she sits in each restaurant. Moving from a table at level (cid:96) to one of its subtables at level (cid:96) + 1  the
customer draws following the CRP using Equation 1. (This description is slightly different from the
metaphor in [1]  but leads to the same distribution.)
The nCRP mixture model can be derived by analogy to the CRP mixture model [15]. (From now
on  we will use the term “nodes” instead of “tables.”) Each node is associated with a parameter w 
where w ∼ G0 and G0 is called the base distribution. Each data point is drawn by ﬁrst choosing a
path in the tree according to the nCRP  and then choosing its value from a distribution that depends
on the parameters in that path. An additional hidden variable x represents other latent quantities
that can be used in this distribution. This is a generalization of the model described in [1]. For data
D = {tn}N

n=1  the nCRP mixture assumes that the nth data point tn is drawn as follows:

1. Draw a path cn|c1:(n−1) ∼ nCRP(γ  c1:(n−1))  which contains L nodes from the tree.
2. Draw a latent variable xn ∼ p(xn|λ).
3. Draw an observation tn ∼ p(tn|Wcn   xn  τ).

The parameters λ and τ are associated with the latent variables x and data generating distribution 
respectively. Note that Wcn contains the wis selected by the path cn. Speciﬁc applications of the
nCRP mixture depend on the particular forms of p(w)  p(x) and p(t|Wc  x).
The corresponding posterior of the latent variables decomposes the data into a collection of paths  and
provides distributions of the parameters attached to each node in those paths. Even though the nCRP
assumes an “inﬁnite” tree  the paths associated with the data will only populate a portion of that tree.
Through this posterior  the nCRP mixture can be used as a ﬂexible tree-based mixture model that
does not assume a particular tree structure in advance of the data.

Hierarchical topic models. The nCRP mixture described above includes the hierarchical topic
model of [1] as a special case. In that model  observed data are documents  i.e.  a list of N words
from a ﬁxed vocabulary. The nodes of the tree are associated with distributions over words (“topics”) 
and each document is associated with both a path in the tree and with a vector of proportions over its
levels. Given a path  a document is generated by repeatedly generating level assignments from the
proportions and then words from the corresponding topics. In the notation above  p(w) is a Dirichlet
distribution over the vocabulary simplex  p(x) is a joint distribution of level proportions (from a
Dirichlet) and level assignments (N draws from the proportions)  and p(t|Wc  x) are the N draws
from the topics (for each word) associated with x.
Tree-based hierarchical component analysis. For continuous data  if p(w)  p(x) and p(t|Wc  x)
are appropriate Gaussian distributions  we obtain hierarchical component analysis  a generalization
of probabilistic principal component analysis (PPCA) [16  17]. In this model  w is the component
parameter for the node it belongs to. Each path c can be thought as a PPCA model with factor loading
Wc speciﬁed by that path. Then each data point chooses a path (also a PPCA model speciﬁed by that
path) and draw the factors x. This model can also be thought as an inﬁnite mixtures of PPCA model 

2

Figure 1: Left. A possible tree structure in a 3-level nCRP. Right. The tree-based stick-breaking
construction of a 3-level nCRP.

where each PPCA can share components. In addition  we can incorporate the general exponential
family PCA [18  19] into the nCRP framework.1

2.1 Tree-based stick-breaking construction

i=1 πiδwi 

vi ∼ Beta(1  γ)  πi = vi

i=1 πi = 1 almost surely. This representation also illuminates

CRP mixtures can be equivalently formulated using the Dirichlet process (DP) as a distribution over
the distribution of each data point’s random parameter [21  4]. An advantage of expressing the CRP
mixture with a DP is that the draw from the DP can be explicitly represented using the stick-breaking
construction [22]. The DP bundles the scaling parameter γ and base distribution G0. A draw from a
DP(γ  G0) is described as

(cid:81)i−1
j=1(1 − vj)  wi ∼ G0  i ∈ {1  2 ···}  G =(cid:80)∞
where π are the stick lengths  and(cid:80)∞
(cid:81)i−1
j=1(1 − v1j) for i = {1  2 ···  ∞} and(cid:80)∞

the discreteness of a distribution drawn from a DP.
For the nCRP  we develop a similar stick-breaking construction. At the ﬁrst level  the root node’s
stick length is π1 = v1 ≡ 1. For all the nodes at the second level  their stick lengths are constructed
as for the DP  i.e.  π1i = π1v1i
i=1 π1i = π1 =
1. The stick-breaking construction is then applied to each of these stick segments at the second
level. For example  the π11 portion of the stick is divided up into an inﬁnite number of pieces
according to the stick-breaking process. For the segment π1k  the stick lengths of its children are
π1ki = π1kv1ki
i=1 π1ki = π1k. The whole process
continues for L levels. This construction is best understood by Figure 1 (Right).
Although this stick represents an inﬁnite tree  the nodes are countable and each node is uniquely
identiﬁed by a sequence of L numbers. We will denote all Beta draws as V   each of which are
independent draws from Beta(1  γ) (except for the root v1  which is equal to one).
The tree-based stick-breaking construction lets us calculate the conditional probability of a path given
V . Let the path c = [1  c2 ···   cL] 

(cid:81)i−1
j=1(1 − v1kj)  for i = {1  2 ···  ∞} and(cid:80)∞

p(c|V ) =(cid:81)L

(cid:96)=1 π1 c2 ···  c(cid:96) =(cid:81)L

(cid:81)c(cid:96)−1
j=1 (1 − v1 c2 ···  j).

(2)
By integrating out V in Equation 2  we recover the nCRP. Given Equation 2  the joint probability of
a data set under the nCRP mixture is

p(t1:N   x1:N   c1:N   V   W ) = p(V )p(W )(cid:81)N

n=1 p(cn|V )p(xn)p(tn|Wcn   xn).

(cid:96)=1 v1 c2 ···  c(cid:96)

(3)

This representation is the basis for variational inference.

3 Variational inference for the nCRP mixture

The central computational problem in Bayesian modeling is posterior inference: Given data  what is
the conditional distribution of the latent variables in the model? In the nCRP mixture  these latent
variables provide the tree structure and node parameters.

1We note that Bach and Jordan [20] studied tree-dependent component analysis  a generalization of inde-
pendent component analysis where the components are organized in a tree. This model expresses a different
philosophy: Their tree reﬂects the actual conditional dependencies among the components. Data are not
generated by choosing a path ﬁrst  but by a linear transformation of all components in the tree.

3

Posterior inference in an nCRP mixture has previously relied on Gibbs sampling  in which we sample
from a Markov chain whose stationary distribution is the posterior [1  2  3]. Variational inference
provides an alternative methodology: Posit a simple (e.g.  factorized) family of distributions over
the latent variables indexed by free parameters (called “variational parameters”). Then ﬁt those
parameters to be close in KL divergence to the true posterior of interest [8  23].
Variational inference for Bayesian nonparametric models uses a truncated stick-breaking represen-
tation in the variational distribution [9] – free variational parameters are allowed only up to the
truncation level. If the truncation is too large  the variational algorithm will still isolate only a subset
of components; if the truncation is too small  methods have been developed to expand the truncated
stick as part of the variational algorithm [10]. In the nCRP mixture  however  the challenge is that the
tree structure is too large even to effectively truncate. We will address this by deﬁning search criteria
for adaptively adjusting the structure of the variational distribution  searching over the set of trees to
best accommodate the data.

3.1 Variational inference based on the tree-based stick-breaking construction

We ﬁrst address the problem of variational inference with a truncated tree of ﬁxed structure. Suppose
that we have a truncated tree T and let MT be the set of all nodes in T . Our family of variational
distributions is deﬁned as follows 

q(W   V   x1:N   c1:N ) =(cid:81)

q(wi)q(vi)(cid:81)

p(wi)p(vi)(cid:81)N

i∈MT

i /∈MT

n=1 q(cn)q(xn) 

(4)
where: (1) Distributions p(wi) and p(vi) for i /∈ MT   are the prior distributions  containing
no variational parameters; (2) Distributions q(wi) and q(vi) for i ∈ MT contain the variational
parameters that we want to optimize for the truncated tree T ; (3) Distribution q(cn) is the variational
multinomial distribution over all the possible paths  not just those in the truncated tree T . Note that
there are inﬁnite number of paths. We will address this issue below; (4) Distribution q(xn) is the
variational distribution for the latent variable xn and it is in the same family of distribution  as p(xn).
In summary  this family of distributions retains the inﬁnite tree structure. Moreover  this family is
nested [10  11]: If a truncated tree T1 is a subtree of a truncated tree T2 then variational distributions
deﬁned over T1 are a special case of those deﬁned over T2. Theoretically  the solution found using
T2 is at least as good as the one found using T1. This allows us to use greedy search to ﬁnd a better
tree structure.
With the variational distributions (Equation 4) and the joint distributions (Equation 3)  we turn to the
details of posterior inference. Equivalent to minimizing KL is tightening the bound on the likelihood
of the observations D = {tn}N
log p(t1:N ) ≥ Eq [log p(t1:N   V   W   x1:N   c1:N )] − Eq [log q(V   W   x1:N   c1:N )]

n=1 given by Jensen’s inequality [8] 

(cid:105)

+(cid:80)N

(cid:104)

(cid:105)

+(cid:80)N

(cid:104)

=(cid:80)

i∈MT

Eq

log p(wi)p(vi)
q(wi)q(vi)

n=1

Eq

log p(xn)
q(xn)

n=1

Eq

log p(tn|xn Wcn )p(cn|V )

q(cn)

(cid:44) L(q).
We optimize L(q) using coordinate ascent. First we isolate the terms that only contain q(cn) 

(cid:104)

(cid:105)

(5)

(6)

L (q(cn)) = Eq [log p(tn|xn  Wcn)p(cn|V )] − Eq [log q(cn)] .

Then we ﬁnd the optimal solution for q(cn) by setting the gradient to zero:

q(cn = c) ∝ Sn c (cid:44) exp{Eq [log p(cn = c|V )] + Eq [log p(tn|xn  Wc)]} .

(7)
Since the values of q(cn = c) is inﬁnite  operating coordinate ascent over q(cn = c) is difﬁcult. We
plug the optimal q(cn) (Equation 7) into Equation 6 to obtain the lower bound

to ﬁnd an efﬁcient way to manipulate this. 2) the lower bound log(cid:80)

(8)
Two issues arise: 1) the variational distribution q(cn) has inﬁnite number of values  and we need
c Sn c (Equation 8) contains
inﬁnite sum  which pose a problem in evaluation. In the appendix  we show that all the operations
can be done only via the truncated tree T . We summarize the results as follows. Let ¯c be a path in
T   either an inner path (a path ending at an inner node) or a full path (a path ending at a leaf node).
Note that the inner path is only deﬁned for the truncated tree T . The number of such ¯c is ﬁnite. In the

c Sn c.

L (q(cn)) = log(cid:80)

4

nCRP tree  denote child(¯c) as the set of all full paths that are not in T but include ¯c as a sub path.
As a special case  if ¯c is a full path  child(¯c) just contains itself. As shown in the appendix  we can
compute these quantities efﬁciently:

q(cn = ¯c) (cid:44)(cid:80)

c:c∈child(¯c) q(cn = c) and Sn ¯c (cid:44)(cid:80)

c:c∈child(¯c) Sn c.

(9)

Consequently iterating over the truncated tree T using ¯c is the same as iterating all the full paths in
the nCRP tree. And these are all we need for doing variational inference.
Next  we move to optimize q(vi|ai  bi) for i ∈ MT   where ai and bi are variational parameters for
Beta distribution q(vi). Let the path containing vi be [1  c2 ···   c(cid:96)0]  where (cid:96)0 ≤ L. We isolate the
term that only contains vi from the lower bound (Equation 5) 

(cid:80)
c q(cn = c) log p(cn = c|V ).

n=1

(10)

After plugging Equation 2 into 10 and setting the gradient to be zero  we obtain the optimal q(vi) 

L (q(vi)) = Eq [log p(vi) − log q(vi)] +(cid:80)N
i = 1 +(cid:80)N
i = γ +(cid:80)N

i −1 
c(cid:96)0+1 ···  cL
j c(cid:96)0+1 ···  cL:j>c(cid:96)0

(1 − vi)b∗

(cid:80)
(cid:80)

n=1

n=1

a∗
b∗

q(vi) ∝ va∗

i −1

i

q(cn = [1  c2 ···   c(cid:96)0  c(cid:96)0+1 ···   cL]) 

q(cn = [1  c2 ···   c(cid:96)0−1  j  c(cid:96)0+1 ···   cL]) 

(11)

where the inﬁnite sum involved can be solved using Equations 9.
The variational update functions for W and x depend on the actual distributions we use  and deriving
them is straightforward. If they include an inﬁnite sum then we apply similar techniques as we did
for q(vi).

3.2 Reﬁning the tree structure during variational inference

Since our variational distribution is nested  a larger truncated tree will always (theoretically) achieve
a lower bound at least as tight as a smaller truncated tree. This allows us to search the inﬁnite tree
space until a certain criterion is satisﬁed (e.g.  relative change of the lower bound). To achieve this 
we present several heuristics to guide us to do so. All these operations are performed on the truncated
tree T .

Grow. This operation is similar to what Gibbs sampling does in searching the tree space. We
implement two heuristics: 1) Randomly choose several data points  and for each of them sample
a path ¯c according to q(cn = ¯c). If it is an inner path  expand it a full path; 2) For every inner
n=1 q(cn = ¯c). Then sample an inner path (say ¯c∗)

path in T   ﬁrst compute the quantity g(¯c) =(cid:80)N
this path – for path c  the criterion is(cid:80)N

according to g(¯c)  and expand it to full path.

Prune.

If a certain path gets very little probability assignments from all data points  we eliminate
n=1 q(cn = c) < δ  where δ is a small number. We use
δ = 10−6). This mimics Gibbs sampling in the sense that for nCRP (or CRP)  if a certain path (table)
gets no assignments in the sampling process  it will never get any assignment any more according to
Equation 1.

If paths i and j give almost equal posterior distributions  merging these two paths is
i Pj/|Pi||Pj|  where Pi = [q(c1 = i) ···   q(cN =

Merge.
employed [24]. The measure is J(i  j) = P T
i)]T . We use 0.95 as the threshold in our experiments.
In theory  Prune and Merge may decrease the lower bound. Empirically  we found even sometime
it does  the effect is negligible. (but reduced the size of the tree). For continuous data settings  we
additionally implement the Split method used in [24].

4 Experiments

In this section  we demonstrate variational inference for the nCRP. We analyze both discrete and
continuous data using the two applications discussed in Section 2.

5

Method

Per-word test set likelihood
JACM

Psy. Review

PNAS

−5.3922 ± 0.0052 −5.7834 ± 0.0149 −6.4961 ± 0.0068
Gibbs sampling
−5.4331 ± 0.0100 −5.8430 ± 0.0153 −6.5736 ± 0.0050
Var. inference
Var. inference (G) −5.4495 ± 0.0118 −5.8593 ± 0.0157 −6.5996 ± 0.0153

Table 1: Test set likelihood comparison on three datasets. Var. inference (G): variational inference
initialized from the initialization of Gibbs sampling. Variational inference can give competitive
performance on test set likelihood.

4.1 Hierarchical topic modeling

For discrete data  we compare variational inference compared with Gibbs sampling for hierarchical
topic modeling. Three corpora are used in the experiments: (1) JACM: a collection of 536 abstracts
from the Journal of the ACM from years 1987 to 2004 with a vocabulary size of 1 539 and around
68K words; (2) Psy. Review: a collection of 1 272 psychology abstracts from Psychological Review
from years 1967 to 2003  with a vocabulary size of 1 971 and around 137K words; (3) PNAS: a
collection of 5 000 abstracts from the Proceedings of the National Academy of Sciences from years
1991 to 2001  with a vocabulary size of 7762 and around 895K words. Those terms occurring in
fewer than 5 documents were removed.
Local maxima can be a problem for both Gibbs sampling and variational inference. To avoid them in
Gibbs sampling  we randomly restart the sampler 200 times and take the trajectory with the highest
average posterior likelihood. We run the Gibbs sampling for 10000 iterations and collect the results
for post analysis. For variational inference  we use two types of initializations 1) similar to Gibbs
sampling  we gradually add data points during the variational inference as well – add a new path for
each document in the initialization; 2) we initialize the variational inference from the initialization
for Gibbs sampling – using the MAP estimate using one Gibbs sample. We set L = 3 for all the
experiments and use the same hyperparameters in both algorithms. Speciﬁcally  the stick-breaking
prior parameter γ is set to 1.0; the symmetric Dirichlet prior parameter for the topics is set to 1.0; the
prior for level proportions is skewed to favor high levels (50  20  10). (This is suggested in [1].) We
run the variational inference until the relative change of log-likelihood is less than 0.001.

Per-word test set likelihood. We use test set likelihood as a measure of performance. The proce-
dure is to divide the corpus into a training set Dtrain and a test set Dtest  and approximate the likelihood
of Dtest given Dtrain. We use the same method in Teh et al. [12] to approximate it. Speciﬁcally  we
use posterior means ˆθ and ˆβ to represent the estimated topic mixture proportions over L levels and
topic multinomial parameters. For the variational method  we use

p({t1 ···   tN}test) =(cid:81)N
p({t1 ···   tN}test) =(cid:81)N

n=1

(cid:80)
c q(cn = c)(cid:81)
(cid:80)
(cid:80)S
(cid:81)

j

(cid:80)
(cid:80)

c δcs

n

1
S

ˆθn (cid:96)

ˆβc(cid:96) tnj  

n (cid:96)

where ˆθ and ˆβ are estimated using mean values from the variational distributions. For Gibbs sampling 
we use S samples and compute

s=1

n=1

10 after a 200-sample burn-in for a document in test set. Actually  1/S(cid:80)S

where ˆθs and ˆβs are estimated using sample s [25  12]. We use 30 samples collected at a lag of
n gives the
empirical estimation of p(cn)  where in variational inference  we approximate it using q(cn). Table 1
shows the test likelihood comparison using ﬁve-fold cross validation. This shows our model can give
competitive performance in term of the test set likelihood. This discrepancy is similar to that in [12]
when variational inference is compared the collapsed Gibbs sampling for HDP.

(cid:80)

c δcs

s=1

j

n (cid:96)

n (cid:96)

ˆθs

ˆβs
c(cid:96) tnj

 

Topic visualizations. Figures 2 and 3 show the tree-based topic visualizations from JACM and
PNAS datasets. These are quite similar to those obtained by Gibbs sampling (see [1]).

4.2 Modeling handwritten digits using hierarchical component analysis

For continuous data  we use the hierarchical component analysis for modeling handwritten digits
(http://archive.ics.uci.edu/ml). This dataset contains 3823 handwritten digits as a training set and

6

Figure 2: A sub network discovered on JACM dataset  each topic represented by top 5 terms. The
whole tree has 30 nodes  with an average branching factor 2.64.

Figure 3: A sub network discovered on PNAS dataset  each topic represented by top 5 terms. The
whole tree has 45 nodes  with an average branching factor 2.93.

1797 as a testing set. Each digit contains 64 integer attributes  ranging from 0-16. As described in
section 2  we use PPCA [16] as the basic model for each path. We use a global mean parameter µ for
all paths  although a model with an individual mean parameter for each path can be similarly derived.
We put broad priors over the parameters similar to those in variational Bayesian PCA [17]. The
stick-breaking prior parameter γ = 1 is set to be 1.0; for each node  w ∼ N (0  103); µ ∼ N (0  103);
the inverse of the variance for the noise model in PPCA is τ and τ ∼ Gamma(10−3  10−3). Again 
we run the variational inference until the relative change of log-likelihood is less than 0.001.
We compare the reconstruction error with PCA. To compute the reconstruction error for our model 
we ﬁrst select the path for each data point using its MAP estimation by ˆcn = arg maxc q(cn = c).
Then we use the similar approach [26  24] to reconstruct tn 

ˆtn = Wˆcn(Wˆcn

T Wˆcn)−1Wˆcn

T (tn − ˆµ) + ˆµ.

We test our model using depth L = 2  3  4  5. All of our models run within 2 minutes. The
reconstruction errors for both the training and testing set are shown in Table 2. Our model gives lower
reconstruction errors than PCA.

5 Conclusions

In this paper  we presented the variational inference algorithm for the nested Chinese restaurant
process based on its tree-based stick-breaking construction. Our result indicates that the variational

7

of a andis innetworksnetworkdistributedparallelprocessorsprogramslogicrulesresolutionprogramqueriesformulascomplexityqueryclassesroutingnetworkcommunicationsortingdistributedqueuingclosedtreesspanningproductformlogarithmicimprovesuponworstcaseoatomicconcurrentwaitfreecontrolsharedmethodstreedecompositioncompressiongreedyfunctionspolynomialbooleancompressioninputbuildingedgesdesiredefficiencytogetherplanargraphsmaximumcomponentessentiallythe in anda todnarnareplicationstrandrecombinationspeciesevolutionbasedtimevisualcardiacmiceerarheartraddnatelomerasebrcarecombinationhotracoxpcnaspotgirkchannelsgagexchangercurrentsdyecouplingtnfplusgapspeciespopulationsyearspopulationgeneticsleepfaorfsmaizehaplotypeleptinghagemicecardiacfkdimerizationerythropoietinreversiblyinterleukinReconstruction error on handwritten digits

#Depth HCA (tr)

PCA (tr) HCA (te)

PCA (te)

878.5
727.7
633.0
564.2

2(9)
3(14)
4(18)
5(22)

631.6
559.8
463.4
384.8

863.0
722.3
621.0
553.0

699.4
585.6
506.1
461.8

Table 2: Reconstruction error comparison (Tr: train; Te: test). HCA stands for hierarchical component
analysis. PCA uses L largest components. In the ﬁrst column  2(9) means L = 2 with 9 nodes
inferred using our model. Others are similarly deﬁned. HCA gives lower reconstruction errors.

inference is a powerful alternative method for the widely used Gibbs sampling. We also adapt the
nCRP to model continuous data  e.g. in hierarchical component analysis.

Acknowledgements. We thank anonymous reviewers for insightful suggestions. David M. Blei is
supported by ONR 175-6343  NSF CAREER 0745520  and grants from Google and Microsoft.

Appendix: efﬁciently manipulating Sn c and q(cn = c)
Case 1: All nodes of the path are in T   c ⊂ MT . Let Z0 (cid:44) Eq [log p(tn|xn  Wc)]. We have

(cid:104)(cid:80)L
(cid:96)=1(log(v1 c2 ···  c(cid:96)) +(cid:80)c(cid:96)−1

(cid:105)
j=1 log(1 − v1 c2 ···  j))

Sn c = exp

+ Z0

(cid:110)

(cid:111)

Eq

.

(12)

c∈child(¯c) Sn c

Case 2: At least one node is not in T   c (cid:54)⊂ MT . Although c (cid:54)⊂ MT   c must have some nodes
in MT . Then c can be written as c = [¯c  c(cid:96)0+1 ···   cL]  where ¯c (cid:44) [1  c2 ···   c(cid:96)0] ⊂ MT and
[¯c  c(cid:96)0+1 ···   c(cid:96)] (cid:54)⊂ MT for any (cid:96) > (cid:96)0. In the truncated tree T   let j0 be the maximum index
for the child node whose parent path is ¯c  then we know if c(cid:96)0+1 > j0  [¯c  c(cid:96)0+1 ···   cL] (cid:54)⊂ MT .
Now we ﬁx the sub path ¯c and let [c(cid:96)0+1 ···   cL] vary (satisfying c(cid:96)0+1 > j0). All these possible
paths constitute a set: child(¯c) (cid:44) {[¯c  c(cid:96)0+1 ···   cL] : c(cid:96)0+1 > j0}. According to Equation 4  for
any c ∈ child(¯c)   Z0 (cid:44) Eq [log p(tn|xn  Wc)] is constant  since the variational distribution for w
(cid:80)
outside the truncated tree is the same prior distribution. We have
=(cid:80)

(cid:105)(cid:111)
j=1 log(1 − v1 c2 ···  j))

(cid:104)(cid:80)L
(cid:96)=1(log(v1 ···  c(cid:96)) +(cid:80)c(cid:96)−1
(cid:110)
(cid:104)(cid:80)(cid:96)0
(cid:96)=1(log(v1 c2 ···  c(cid:96)) +(cid:80)c(cid:96)−1
(cid:105)(cid:17)
j=1 log(1 − v1 c2 ···  c(cid:96)0  j)

Z0 + Eq
= exp(Z0+(L−(cid:96)0)Ep[log(v)])
× exp

(13)
where v ∼ Beta(1  γ). Such cases contain all inner nodes in the truncated tree T . Note that Case 1
c Sn c can be computed efﬁciently.

is a special case of Case 2 by setting (cid:96)0 = L. Given all these (cid:80)
c∈child(¯c) q(cn = c) ∝(cid:80)

Furthermore  given Equations 13 and Equation 7  we deﬁne

q(cn = ¯c) (cid:44)(cid:80)

(cid:105)(cid:111)
j=1 log(1 − v1 c2 ···  j))

(1−exp(Ep[log(1−v)]))L−(cid:96)0 exp

(cid:104)(cid:80)j0

c∈child(¯c) exp

c∈child(¯c) Sn c 

(cid:110)

(cid:16)

(14)

Eq

Eq

 

which corresponds the sum of probabilities from all paths in child(¯c). We note that this organization
only depends on the truncated tree T and is sufﬁcient for variational inference.

8

References

[1] Blei  D. M.  T. L. Grifﬁths  M. I. Jordan  et al. Hierarchical topic models and the nested Chinese restaurant

process. In NIPS. 2003.

[2] Bart  E.  I. Porteous  P. Perona  et al. Unsupervised learning of visual taxonomies. In CVPR. 2008.
[3] Sivic  J.  B. C. Russell  A. Zisserman  et al. Unsupervised discovery of visual object class hierarchies. In

CVPR. 2008.

[4] Aldous  D. Exchangeability and related topics. In Ecole d’Ete de Probabilities de Saint-Flour XIII 1983 

pages 1–198. Springer  1985.

[5] Ferguson  T. S. A Bayesian analysis of some nonparametric problems. The Annals of Statistics  1(2):209–

230  1973.

[6] Neal  R. Probabilistic inference using Markov chain Monte Carlo methods. Tech. Rep. CRG-TR-93-1 

Department of Computer Science  University of Toronto  1993.

[7] Robert  C.  G. Casella. Monte Carlo Statistical Methods. Springer-Verlag  New York  NY  2004.
[8] Jordan  M. I.  Z. Ghahramani  T. S. Jaakkola  et al. An introduction to variational methods for graphical

models. Learning in Graphical Models  1999.

[9] Blei  D. M.  M. I. Jordan. Variational methods for the Dirichlet process. In ICML. 2004.
[10] Kurihara  K.  M. Welling  N. A. Vlassis. Accelerated variational Dirichlet process mixtures. In NIPS.

2006.

[11] Kurihara  K.  M. Welling  Y. W. Teh. Collapsed variational Dirichlet process mixture models. In IJCAI.

2007.

[12] Teh  Y. W.  K. Kurihara  M. Welling. Collapsed variational inference for HDP. In NIPS. 2008.
[13] Sudderth  E. B.  M. I. Jordan. Shared segmentation of natural scenes using dependent Pitman-Yor processes.

In NIPS. 2008.

[14] Doshi  F.  K. T. Miller  J. Van Gael  et al. Variational inference for the Indian buffet process. In AISTATS 

vol. 12. 2009.

[15] Escobar  M. D.  M. West. Bayesian density estimation and inference using mixtures. Journal of the

American Statistical Association  90:577–588  1995.

[16] Tipping  M. E.  C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical

Society  Series B  61:611–622  1999.

[17] Bishop  C. M. Variational principal components. In ICANN. 1999.
[18] Collins  M.  S. Dasgupta  R. E. Schapire. A generalization of principal components analysis to the

exponential family. In NIPS. 2001.

[19] Mohamed  S.  K. A. Heller  Z. Ghahramani. Bayesian exponential family PCA. In NIPS. 2008.
[20] Bach  F. R.  M. I. Jordan. Beyond independent components: Trees and clusters. JMLR  4:1205–1233 

2003.

[21] Antoniak  C. E. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.

The Annals of Statistics  2(6):1152–1174  1974.

[22] Sethuraman  J. A constructive deﬁnition of Dirichlet priors. Statistica Sinica  4:639–650  1994.
[23] Wainwright  M.  M. Jordan. Variational inference in graphical models: The view from the marginal

polytope. In Allerton Conference on Control  Communication and Computation. 2003.

[24] Ueda  N.  R. Nakano  Z. Ghahramani  et al. SMEM algorithm for mixture models. Neural Computation 

12(9):2109–2128  2000.

[25] Grifﬁths  T. L.  M. Steyvers. Finding scientiﬁc topics. Proc Natl Acad Sci USA  101 Suppl 1:5228–5235 

2004.

[26] Tipping  M. E.  C. M. Bishop. Mixtures of probabilistic principal component analysers. Neural Computa-

tion  11(2):443–482  1999.

9

,Isik Fidaner
Taylan Cemgil