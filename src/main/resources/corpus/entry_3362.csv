2012,Matrix reconstruction with the local max norm,We introduce a new family of matrix norms  the ''local max'' norms  generalizing existing methods such as the max norm  the trace norm (nuclear norm)  and the weighted or smoothed weighted trace norms  which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netflix and MovieLens ratings data  and find improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms.,Matrix reconstruction with the local max norm

Rina Foygel

Department of Statistics

Stanford University

rinafb@stanford.edu

Nathan Srebro

Toyota Technological Institute at Chicago

nati@ttic.edu

Ruslan Salakhutdinov

Dept. of Statistics and Dept. of Computer Science University of Toronto

rsalakhu@utstat.toronto.edu

Abstract

We introduce a new family of matrix norms  the “local max” norms  generalizing
existing methods such as the max norm  the trace norm (nuclear norm)  and the
weighted or smoothed weighted trace norms  which have been extensively used in
the literature as regularizers for matrix reconstruction problems. We show that this
new family can be used to interpolate between the (weighted or unweighted) trace
norm and the more conservative max norm. We test this interpolation on simulated
data and on the large-scale Netﬂix and MovieLens ratings data  and ﬁnd improved
accuracy relative to the existing matrix norms. We also provide theoretical results
showing learning guarantees for some of the new norms.

Introduction

1
In the matrix reconstruction problem  we are given a matrix Y ∈ Rn×m whose entries are only partly
observed  and would like to reconstruct the unobserved entries as accurately as possible. Matrix
reconstruction arises in many modern applications  including the areas of collaborative ﬁltering
(e.g. the Netﬂix prize)  image and video data  and others. This problem has often been approached
using regularization with matrix norms that promote low-rank or approximately-low-rank solutions 
including the trace norm (also known as the nuclear norm) and the max norm  as well as several
adaptations of the trace norm described below.
In this paper  we introduce a unifying family of norms that generalizes these existing matrix norms 
and that can be used to interpolate between the trace and max norms. We show that this family
includes new norms  lying strictly between the trace and max norms  that give empirical and theo-
retical improvements over the existing norms. We give results allowing for large-scale optimization
with norms from the new family. Some proofs are deferred to the Supplementary Materials.
Notation Without loss of generality we take n ≥ m. We let R+ denote the nonnegative real
numbers. For any n ∈ N  let [n] = {1  . . .   n}  and deﬁne the simplex on [n] as ∆[n] =

i ri = 1(cid:9). We analyze situations where the locations of observed entries are sampled

(cid:8)r ∈ Rn
+ :(cid:80)
i.i.d. according to some distribution p on [n] × [m]. We write pi• =(cid:80)

j pij to denote the marginal
probability of row i  and prow = (p1•  . . .   pn•) ∈ ∆[n] to denote the marginal row distribution.
We deﬁne p•j and pcol similarly for the columns. For any matrix M  M(i) denotes its ith row.

1.1 Trace norm and max norm

A common regularizer used in matrix reconstruction  and other matrix problems  is the trace norm
(cid:107)X(cid:107)tr  equal to the sum of the singular values of X. This norm can also be deﬁned via a factorization

1

of X [1]:

1√
nm

(cid:107)X(cid:107)tr =

1
2

min

AB(cid:62)=X

 1

n

(cid:88)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

i

+

1
m

  

(cid:88)

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

j

(1)

√
where the minimum is taken over factorizations of X of arbitrary dimension—that is  the number of
columns in A and B is unbounded. Note that we choose to scale the trace norm by 1/
nm in order
to emphasize that we are averaging the squared row norms of A and B.
Regularization with the trace norm gives good theoretical and empirical results  as long as the loca-
tions of observed entries are sampled uniformly (i.e. when p is the uniform distribution on [n]×[m]) 
and  under this assumption  can also be used to guarantee approximate recovery of an underlying
low-rank matrix [1  2  3  4].
The factorized deﬁnition of the trace norm (1) allows for an intuitive comparison with the max norm 
deﬁned as [1]:

(cid:18)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

(cid:19)

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

2

(cid:107)X(cid:107)max =

1
2

min

AB(cid:62)=X

sup

i

2 + sup
j

.

(2)

We see that the max norm measures the largest row norms in the factorization  while the rescaled
trace norm instead considers the average row norms. The max norm is therefore an upper bound
on the rescaled trace norm  and can be viewed as a more conservative regularizer. For the more
general setting where p may not be uniform  Foygel and Srebro [4] show that the max norm is still
an effective regularizer (in particular  bounds on error for the max norm are not affected by p). On
the other hand  Salakhutdinov and Srebro [5] show that the trace norm is not robust to non-uniform
sampling—regularizing with the trace norm may yield large error due to over-ﬁtting on the rows and
columns with high marginals. They obtain improved empirical results by placing more penalization
on these over-represented rows and columns  described next.

1.2 The weighted trace norm

To reduce overﬁtting on the rows and columns with high marginal probabilities under the distribution
p  Salakhutdinov and Srebro propose regularizing with the p-weighted trace norm 

(cid:13)(cid:13)(cid:13)diag(prow)1/2 · X · diag(pcol)1/2(cid:13)(cid:13)(cid:13)tr

.

(cid:107)X(cid:107)tr(p) :=

If the row and the column of entries to be observed are sampled independently (i.e. p = prow ·
pcol is a product distribution)  then the p-weighted trace norm can be used to obtain good learning
guarantees even when prow and pcol are non-uniform [3  6]. However  for non-uniform non-product
sampling distributions  even the p-weighted trace norm can yield poor generalization performance.
To correct for this  Foygel et al. [6] suggest adding in some “smoothing” to avoid under-penalizing
the rows and columns with low marginal probabilities  and obtain improved empirical and theoretical
results for matrix reconstruction using the smoothed weighted trace norm:

(cid:13)(cid:13)(cid:13)diag((cid:101)prow)1/2 · X · diag((cid:101)pcol)1/2(cid:13)(cid:13)(cid:13)tr
where(cid:101)prow and(cid:101)pcol denote smoothed row and column marginals  given by

(cid:101)prow = (1 − ζ) · prow + ζ · 1/n and(cid:101)pcol = (1 − ζ) · pcol + ζ · 1/m  

(cid:107)X(cid:107)tr((cid:101)p) :=

 

total # observations

(3)
for some choice of smoothing parameter ζ which may be selected with cross-validation1. The
smoothed empirically-weighted trace norm is also studied in [6]  where pi• is replaced with

  the empirical marginal probability of row i (and same for (cid:98)p•j). Using

(cid:98)pi• = # observations in row i

empirical rather than “true” weights yielded lower error in experiments in [6]  even when the true
sampling distribution was uniform.
More generally  for any weight vectors r ∈ ∆[n] and c ∈ ∆[m] and a matrix X ∈ Rn×m  the
(r  c)-weighted trace norm is given by
(cid:107)X(cid:107)tr(r c) =

(cid:13)(cid:13)(cid:13)diag(r)1/2 · X · diag(c)1/2(cid:13)(cid:13)(cid:13)tr

.

1Our ζ parameter here is equivalent to 1 − α in [6].

2

Of course  we can easily obtain the existing methods of the uniform trace norm  (empirically)
weighted trace norm  and smoothed (empirically) weighted trace norm as special cases of this for-
mulation. Furthermore  the max norm is equal to a supremum over all possible weightings [7]:

(cid:107)X(cid:107)max =

sup

r∈∆[n] c∈∆[m]

(cid:107)X(cid:107)tr(r c) .

2 The local max norm

We consider a generalization of these norms  which lies “in between” the trace norm and max norm.
For any R ⊆ ∆[n] and C ⊆ ∆[m]  we deﬁne the (R C)-norm of X:
(cid:107)X(cid:107)tr(r c) .

(cid:107)X(cid:107)(R C) = sup
r∈R c∈C

This gives a norm on matrices  except in the trivial case where  for some i or some j  ri = 0 for all
r ∈ R or cj = 0 for all c ∈ C.
We now show some existing and novel norms that can be obtained using local max norms.

2.1 Trace norm and max norm
We can obtain the max norm by taking the largest possible R and C  i.e. (cid:107)X(cid:107)max = (cid:107)X(cid:107)(∆[n] ∆[m]) 
and similarly we can obtain the (r  c)-weighted trace norm by taking the singleton sets R = {r}
and C = {c}. As discussed above  this includes the standard trace norm (when r and c are uniform) 
as well as the weighted  empirically weighted  and smoothed weighted trace norm.

2.2 Arbitrary smoothing

When using the smoothed weighted max norm  we need to choose the amount of smoothing to
apply to the marginals  that is  we need to choose ζ in our deﬁnition of the smoothed row and
column weights  as given in (3). Alternately  we could regularize simultaneously over all possible
amounts of smoothing by considering the local max norm with

R = {(1 − ζ) · prow + ζ · 1/n : any ζ ∈ [0  1]}  

and same for C. That is  R and C are line segments in the simplex—they are larger than any single
point as for the uniform or weighted trace norm (or smoothed weighted trace norm for a ﬁxed amount
of smoothing)  but smaller than the entire simplex as for the max norm.

2.3 Connection to (β  τ )-decomposability

Hazan et al. [8] introduce a class of matrices deﬁned by a property of (β  τ )-decomposability: a
matrix X satisﬁes this property if there exists a factorization X = AB(cid:62) (where A and B may have
an arbitrary number of columns) such that2

(cid:27)

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

2

≤ 2β 

(cid:88)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

2 +

(cid:88)

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

i

j

2 ≤ τ .

(cid:26)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

max

max

i

2   max

j

Comparing with (1) and (2)  we see that the β and τ parameters essentially correspond to the max
norm and trace norm  with the max norm being the minimal 2β∗ such that the matrix is (β∗  τ )-
decomposable for some τ  and the trace norm being the minimal τ∗/2 such that the matrix is
(β  τ∗)-decomposable for some β. However  Hazan et al. go beyond these two extremes  and rely
on balancing both β and τ: they establish learning guarantees (in an adversarial online model  and
β · τ. It may therefore be
thus also under an arbitrary sampling distribution p) which scale with
useful to consider a penalty function of the form:

√


(cid:114)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

(cid:115)(cid:88)

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

2 ·

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

2 +

(cid:88)

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

2

i

j

 .

Penalty(β τ )(X) = min

X=AB(cid:62)

max

i

2 + max

j

(4)
2Hazan et al. state the property differently  but equivalently  in terms of a semideﬁnite matrix decomposition.

3

(cid:110)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

(cid:111)

(cid:13)(cid:13)A(i)

(cid:13)(cid:13)2

(cid:13)(cid:13)B(j)

(cid:13)(cid:13)2

maxi

2   maxj

is replaced with maxi

(Note that max
2 + maxj
√
for later convenience. This affects the value of the penalty function by at most a factor of
This penalty function does not appear to be convex in X. However  the proposition below (proved in
the Supplementary Materials) shows that we can use a (convex) local max norm penalty to compute
a solution to any objective function with a penalty function of the form (4):

Proposition 1. Let (cid:98)X be the minimizer of a penalized loss function with this modiﬁed penalty 

2.)

2 

2

where λ ≥ 0 is some penalty parameter and Loss(·) is any convex function. Then  for some penalty
parameter µ ≥ 0 and some t ∈ [0  1] 

X

(cid:110)
(cid:111)
(cid:98)X := arg min
Loss(X) + λ · Penalty(β τ )(X)
(cid:110)
(cid:98)X = arg min
(cid:27)
Loss(X) + µ · (cid:107)X(cid:107)(R C)
∀i

c ∈ ∆[m] : cj ≥

and C =

(cid:26)

(cid:111)

X

t

  where

 

(cid:27)

.

t

1 + (m − 1)t

∀j

(cid:26)

R =

r ∈ ∆[n] : ri ≥

1 + (n − 1)t

the unknown solution (cid:98)X.

We note that µ and t cannot be determined based on λ alone—they will depend on the properties of

Here the sets R and C impose a lower bound on each of the weights  and this lower bound can be
used to interpolate between the max and trace norms: when t = 1  each ri is lower bounded by
1/n (and similarly for cj)  i.e. R and C are singletons containing only the uniform weights and we
obtain the trace norm. On the other hand  when t = 0  the weights are lower-bounded by zero  and
so any weight vector is allowed  i.e. R and C are each the entire simplex and we obtain the max
norm. Intermediate values of t interpolate between the trace norm and max norm and correspond to
different balances between β and τ.

2.4

Interpolating between trace norm and max norm

We next turn to an interpolation which relies on an upper bound  rather than a lower bound  on the
weights. Consider

R =(cid:8)r ∈ ∆[n] : ri ≤  ∀i(cid:9) and Cδ =(cid:8)c ∈ ∆[n] : cj ≤ δ ∀j(cid:9)  

(5)
for some  ∈ [1/n  1] and δ ∈ [1/m  1]. The (R Cδ)-norm is then equal to the (rescaled) trace norm
when we choose  = 1/n and δ = 1/m  and is equal to the max norm when we choose  = δ = 1.
Allowing  and δ to take intermediate values gives a smooth interpolation between these two familiar
norms  and may be useful in situations where we want more ﬂexibility in the type of regularization.
We can generalize this to an interpolation between the max norm and a smoothed weighted trace
norm  which we will use in our experimental results. We consider two generalizations—for each
one  we state a deﬁnition of R  with C deﬁned analogously. The ﬁrst is multiplicative:

ζ γ :=(cid:8)r ∈ ∆[n] : ri ≤ γ · ((1 − ζ) · pi• + ζ · 1/n) ∀i(cid:9)  

(6)
where γ = 1 corresponds to choosing the singleton set R×
ζ γ = {(1 − ζ) · prow + ζ · 1/n} (i.e. the
smoothed weighted trace norm)  while γ = ∞ corresponds to the max norm (for any choice of ζ)
since we would get R×
(cid:110)
(cid:111)
The second option for an interpolation is instead deﬁned with an exponent:
r ∈ ∆[n] : ri ≤ ((1 − ζ) · pi• + ζ · 1/n)1−τ ∀i

ζ γ = ∆[n].

Rζ τ :=

R×

(7)

.

Here τ = 0 will yield the singleton set corresponding to the smoothed weighted trace norm  while
τ = 1 will yield Rζ τ = ∆[n]  i.e. the max norm  for any choice of ζ.
We ﬁnd the second (exponent) option to be more natural  because each of the row marginal bounds
will reach 1 simultaneously when τ = 1  and hence we use this version in our experiments. On
the other hand  the multiplicative version is easier to work with theoretically  and we use this in our
learning guarantee in Section 4.2. If all of the row and column marginals satisfy some loose upper
bound  then the two options will not be highly different.

4

3 Optimization with the local max norm

One appeal of both the trace norm and the max norm is that they are both SDP representable [9  10] 
and thus easily optimizable  at least in small scale problems. In the Supplementary Materials we
show that the local max norm is also SDP representable  as long as the sets R and C can be written
in terms of linear or semi-deﬁnite constraints—this includes all the examples we mention  where in
all of them the sets R and C are speciﬁed in terms of simple linear constraints.
However  for large scale problems  it is not practical to directly use SDP optimization approaches.
Instead  and especially for very large scale problems  an effective optimization approach for both
the trace norm and the max norm is to use the factorized versions of the norms  given in (1) and (2) 
and to optimize the factorization directly (typically  only factorizations of some truncated dimen-
sionality are used) [11  12  7]. As we show in Theorem 1 below  a similar factorization-optimization
approach is also possible for any local max norm with convex R and C. We further give a simpliﬁed
representation which is applicable when R and C are speciﬁed through element-wise upper bounds
R ∈ Rn

R = {r ∈ ∆[n] : ri ≤ Ri ∀i} and C = {c ∈ ∆[m] : cj ≤ Cj ∀j}  

i Ri ≥ 1  0 ≤ Cj ≤ 1 (cid:80)
(cid:88)
(cid:13)(cid:13)A(i)
(cid:16)(cid:13)(cid:13)A(i)
(cid:13)(cid:13)2

(8)
j Cj ≥ 1 to avoid triviality. This includes the
interpolation norms of Section 2.4.
Theorem 1. If R and C are convex  then the (R C)-norm can be calculated with the factorization
(9)
In the special case when R and C are deﬁned by (8)  writing (x)+ := max{0  x}  this simpliﬁes to
(cid:107)X(cid:107)(R C) =
Proof sketch for Theorem 1. For convenience we will write r1/2 to mean diag(r)1/2  and same for c.
Using the trace norm factorization identity (1)  we have

+ and C ∈ Rm

with 0 ≤ Ri ≤ 1 (cid:80)

(cid:13)(cid:13)2
(cid:17)
2 − a

(cid:88)
(cid:88)

(cid:107)X(cid:107)(R C) =

+   respectively:

AB(cid:62)=X;a b∈R

2 + sup
c∈C

2 − b

(cid:13)(cid:13)2

(cid:111)

+ b +

(cid:17)

1
2

inf

cj

ri

+

+

.

.

j

j

i

2(cid:107)X(cid:107)(R C) = 2

= sup

r∈R c∈C

inf

AB(cid:62)=X

sup

r∈R c∈C

(cid:18)(cid:13)(cid:13)(cid:13)r1/2 · A

inf

CD(cid:62)=r1/2·X·c1/2

(cid:19)

(cid:13)(cid:13)(cid:13)2

F

= sup

r∈R c∈C
≤ inf

AB(cid:62)=X

(cid:18)

sup
r∈R

(cid:13)(cid:13)(cid:13)r1/2A

inf

1
2

a +

sup
r∈R

AB(cid:62)=X

(cid:16)
(cid:110)
(cid:88)
(cid:13)(cid:13)(cid:13)r1/2 · X · c1/2(cid:13)(cid:13)(cid:13)tr
(cid:13)(cid:13)(cid:13)c1/2 · B
(cid:13)(cid:13)(cid:13)2

Ri

+

i

F

2

Cj

(cid:17)
(cid:13)(cid:13)2
(cid:13)(cid:13)B(j)
(cid:16)(cid:13)(cid:13)B(j)
(cid:16)(cid:107)C(cid:107)2
(cid:13)(cid:13)(cid:13)2

F

+ sup
c∈C

F + (cid:107)D(cid:107)2

(cid:17)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)c1/2B

F

F

(cid:19)

 

where for the next-to-last step we set C = r1/2A and D = c1/2B  and the last step follows because
sup inf ≤ inf sup always (weak duality). The reverse inequality holds as well (strong duality)  and
is proved in the Supplementary Materials  where we also prove the special-case result.

4 An approximate convex hull and a learning guarantee

In this section  we look for theoretical bounds on error for the problem of estimating unobserved
entries in a matrix Y that is approximately low-rank. Our results apply for either uniform or non-
uniform sampling of entries from the matrix. We begin with a result comparing the (R C)-norm unit
ball to a convex hull of rank-1 matrices  which will be useful for proving our learning guarantee.

4.1 Convex hull
To gain a better theoretical understanding of the (R C) norm  we ﬁrst need to deﬁne corresponding
vector norms on Rn and Rm. For any u ∈ Rn  let

(cid:115)

(cid:88)

i

(cid:107)u(cid:107)R :=

sup
r∈R

riu2

i = sup
r∈R

(cid:13)(cid:13)(cid:13)diag(r)1/2 · u
(cid:13)(cid:13)(cid:13)2

.

We can think of this norm as a way to interpolate between the (cid:96)2 and (cid:96)∞ vector norms. For example 
if we choose R = R as deﬁned in (5)  then (cid:107)u(cid:107)R is equal to the root-mean-square of the −1
largest entries of u whenever −1 is an integer. Deﬁning (cid:107)v(cid:107)C analogously for v ∈ Rm  we can now
relate these vector norms to the (R C)-norm on matrices.

5

Theorem 2. For any convex R ⊆ ∆[n] and C ⊆ ∆[m]  the (R C)-norm unit ball is bounded above
and below by a convex hull as:

(cid:111) ⊆ KG·Conv(cid:8)uv(cid:62):(cid:107)u(cid:107)R = (cid:107)v(cid:107)C = 1(cid:9)  

Conv(cid:8)uv(cid:62):(cid:107)u(cid:107)R = (cid:107)v(cid:107)C = 1(cid:9) ⊆(cid:110)

X :(cid:107)X(cid:107)(R C) ≤ 1

where KG ≤ 1.79 is Grothendieck’s constant  and implicitly u ∈ Rn  v ∈ Rm.
This result is a nontrivial extension of Srebro and Shraibman [1]’s analysis for the max norm and
the trace norm. They show that the statement holds for the max norm  i.e. when R = ∆[n] and
C = ∆[m]  and that the trace norm unit ball is exactly equal to the corresponding convex hull (see
Corollary 2 and Section 3.2 in their paper  respectively).
Proof sketch for Theorem 2. To prove the ﬁrst inclusion  given any X = uv(cid:62) with (cid:107)u(cid:107)R = (cid:107)v(cid:107)C =
1  we apply the factorization result Theorem 1 to see that (cid:107)X(cid:107)(R C) ≤ 1. Since the (R C)-norm
unit ball is convex  this is sufﬁcient. For the second inclusion  we state a weighted version of
Grothendieck’s Inequality (proof in the Supplementary Materials):

sup(cid:8)(cid:104)Y  U V (cid:62)(cid:105) : U ∈ Rn×k  V ∈ Rm×k (cid:13)(cid:13)U(i)

= KG · sup(cid:8)(cid:104)Y  uv(cid:62)(cid:105) : u ∈ Rn  v ∈ Rm |ui| ≤ ai ∀i  |vj| ≤ bj ∀j(cid:9) .

(cid:13)(cid:13)2 ≤ ai ∀i  (cid:13)(cid:13)V(j)

(cid:13)(cid:13)2 ≤ bj ∀j(cid:9)

We then apply this weighted inequality to the dual norm of the (R C)-norm to prove the desired
inclusion  as in Srebro and Shraibman [1]’s work for the max norm case (see Corollary 2 in their
paper). Details are given in the Supplementary Materials.

4.2 Learning guarantee

k

ij

 

inf

k

kn
s

·

1 +

pij

ij

Approximation error

+O

(cid:124)

(cid:125)

(cid:19)
(cid:125)

log(n)√
γ

(cid:12)(cid:12)(cid:12)Yij − (cid:98)Xij

(cid:12)(cid:12)(cid:12) ≤

pij |Yij − Xij|

We now give our main matrix reconstruction result  which provides error bounds for a family of
norms interpolating between the max norm and the smoothed weighted trace norm.
Theorem 3. Let p be any distribution on [n] × [m]. Suppose that  for some γ ≥ 1  R ⊇
1/2 γ and C ⊇ C×
R×
1/2 γ  where these two sets are deﬁned in (6). Let S = {(it  jt) : t = 1  . . .   s} be
a random sample of locations in the matrix drawn i.i.d. from p  where s ≥ n. Then  in expectation
(cid:88)
over the sample S 

(cid:32)(cid:114)

(cid:33)

Proof sketch for Theorem 3. The main idea is to use the convex hull formulation from Theorem 2
k  there exists a decomposition X = X(cid:48) + X(cid:48)(cid:48) with

(cid:88)
(cid:123)(cid:122)
(cid:124)
(cid:107)X(cid:107)(R C)≤√
(cid:80)s
t=1 |Yitjt − Xitjt|. Additionally  if we assume that s ≥

(cid:18)
(cid:123)(cid:122)
where (cid:98)X = arg min(cid:107)X(cid:107)(R C)≤√
n log(n)  then in the excess risk bound  we can reduce the term log(n) to(cid:112)log(n).
k) and (cid:107)X(cid:48)(cid:48)(cid:107)tr((cid:101)p) ≤ O((cid:112)k/γ)  where(cid:101)p represents the smoothed marginals with
to show that  for any X with (cid:107)X(cid:107)(R C) ≤ √
(cid:107)X(cid:48)(cid:107)max ≤ O(
k(cid:9). This then yields a learning guarantee by
Rademacher complexity of (cid:8)X : (cid:107)X(cid:107)(R C) ≤ √
smoothing parameter ζ = 1/2 as in (3). We then apply known bounds on the Rademacher complexity
of the max norm unit ball [1] and the smoothed weighted trace norm unit ball [6]  to bound the
weighted trace norm. Speciﬁcally  choosing γ = ∞ gives us an excess error term of order(cid:112)kn/s
(cid:112)kn log(n)/s for the smoothed weighted trace norm as long as s ≥ n log(n)  as shown in [6].

As special cases of this theorem  we can re-derive the existing results for the max norm and smoothed

for the max norm  previously shown by [1]  while setting γ = 1 yields an excess error term of order

Theorem 8 of Bartlett and Mendelson [13]. Details are given in the Supplementary Materials.

Excess error

√

What advantage does this new result offer over the existing results for the max norm and for the
smoothed weighted trace norm? To simplify the comparison  suppose we choose γ = log2(n)  and
deﬁne R = R×
1/2 γ. Then  comparing to the max norm result (when γ = ∞)  we see

1/2 γ and C = C×

6

Table 1: Matrix ﬁtting for the ﬁve methods used in experiments.

(Uniform) trace norm

Empirically-weighted trace norm

Arbitrarily-smoothed emp.-wtd. trace norm

Norm

Max norm

Local max norm

Fixed parameters
ζ arbitrary; τ = 1

ζ = 1; τ = 0
ζ = 0; τ = 0

τ = 0

—

Free parameters

λ
λ
λ
ζ; λ
ζ; τ; λ

Figure 1: Simulation results for matrix reconstruction with a rank-2 (left) or rank-4 (right) signal  corrupted by
noise. The plot shows per-entry squared error averaged over 50 trials  with standard error bars. For the rank-4
experiment  max norm error exceeded 0.20 for each n = 60  120  240 and is not displayed in the plot.

that the excess error term is the same in both cases (up to a constant)  but the approximation error
term may in general be much lower for the local max norm than for the max norm. Comparing next
to the weighted trace norm (when γ = 1)  we see that the excess error term is lower by a factor of
log(n) for the local max norm. This may come at a cost of increasing the approximation error  but in
general this increase will be very small. In particular  the local max norm result allows us to give a
meaningful guarantee for a sample size s = Θ (kn)  rather than requiring s ≥ Θ (kn log(n)) as for
any trace norm result  but with a hypothesis class signiﬁcantly richer than the max norm constrained
class (though not as rich as the trace norm constrained class).

5 Experiments

We test the local max norm on simulated and real matrix reconstruction tasks  and compare its
performance to the max norm  the uniform and empirically-weighted trace norms  and the smoothed
empirically-weighted trace norm.
5.1 Simulations
We simulate n × n noisy matrices for n = 30  60  120  240  where the underlying signal has rank
k = 2 or k = 4  and we observe s = 3kn entries (chosen uniformly without replacement). We
performed 50 trials for each of the 8 combinations of (n  k).
Data For each trial  we randomly draw a matrix U ∈ Rn×k by drawing each row uniformly at
random from the unit sphere in Rn. We generate V ∈ Rm×k similarly. We set Y = U V (cid:62) + σ · Z 
where the noise matrix Z has i.i.d. standard normal entries and σ = 0.3 is a moderate noise level.
We also divide the n2 entries of the matrix into sets S0 (cid:116) S1 (cid:116) S2 which consist of s = 3kn training
entries  s validation entries  and n2 − 2s test entries  respectively  chosen uniformly at random.
Methods We use the two-parameter family of norms deﬁned in (7)  but replacing the true
and each penalty parameter value λ ∈ {21  22  . . .   210}  we compute the ﬁtted matrix

marginals pi• and p•j with the empirical marginals(cid:98)pi• and(cid:98)p•j. For each ζ  τ ∈ {0  0.1  . . .   0.9  1}

(i j)∈S0 (Yij − Xij)2 + λ · (cid:107)X(cid:107)(Rζ τ  Cζ τ )

(10)
(In fact  we use a rank-8 approximation to this optimization problem  as described in Section 3.) For
each method  we use S1 to select the best ζ  τ  and λ  with restrictions on ζ and/or τ as speciﬁed by
the deﬁnition of the method (see Table 1)  then report the error of the resulting ﬁtted matrix on S2.
Results The results for these simulations are displayed in Figure 1. We see that the local max
norm results in lower error than any of the tested existing norms  across all the settings used.

(cid:98)X = arg min(cid:8)(cid:88)

(cid:9) .

7

Matrix dimension nMean sq. error per entryllllllllllllllllllll0.160.200.240.283060120240TraceEmp. traceSmth. traceMaxLocal maxMatrix dimension nMean sq. error per entrylllllllllllllllll0.110.130.153060120240Table 2: Root mean squared error (RMSE) results for estimating movie ratings on Netﬂix and MovieLens data
using a rank 30 model. Setting τ = 0 corresponds to the uniform or weighted or smoothed weighted trace
norm (depending on ζ)  while τ = 1 corresponds to the max norm for any ζ value.

ζ \ τ
0.00
0.05
0.10
0.15
0.20
1.00

0.00
0.7852
0.7836
0.7831
0.7833
0.7842
0.7997

MovieLens

0.05
0.7827
0.7822
0.7837
0.7842
0.7853

0.10
0.7838
0.7842
0.7846
0.7854
0.7866

1.00
0.7918

—
—
—
—

ζ \ τ
0.00
0.05
0.10
0.15
0.20
1.00

0.00
0.9107
0.9095
0.9096
0.9102
0.9126
0.9235

Netﬂix
0.05
0.9092
0.9090
0.9098
0.9111
0.9344

0.10
0.9094
0.9107
0.9122
0.9131
0.9153

1.00
0.9131

—
—
—
—

5.2 Movie ratings data
We next compare several different matrix norms on two collaborative ﬁltering movie ratings datasets 
the Netﬂix [14] and MovieLens [15] datasets. The sizes of the data sets  and the split of the ratings
into training  validation and test sets3  are:

Dataset
Netﬂix

MovieLens

# users
480 189
71 567

# movies
17 770
10 681

Training set
100 380 507
8 900 054

Validation set

100 000
100 000

Test set
1 408 395
1 000 000

We test the local max norm given in (7) with ζ ∈ {0  0.05  0.1  0.15  0.2} and τ ∈ {0  0.05  0.1}.
We also test τ = 1 (the max norm—here ζ is arbitrary) and ζ = 1  τ = 0 (the uniform trace norm).
We follow the test protocol of [6]  with a rank-30 approximation to the optimization problem (10).
Table 2 shows root mean squared error (RMSE) for the experiments. For both the MovieLens and
Netﬂix data  the local max norm with τ = 0.05 and ζ = 0.05 gives strictly better accuracy than any
previously-known norm studied in this setting. (In practice  we can use a validation set to reliably
select good values for the τ and ζ parameters4.) For the MovieLens data  the local max norm
achieves RMSE of 0.7822  compared to 0.7831 achieved by the smoothed empirically-weighted
trace norm with ζ = 0.10  which gives the best result among the previously-known norms. For the
Netﬂix dataset the local max norm achieves RMSE of 0.9090  improving upon the previous best
result of 0.9095 achieved by the smoothed empirically-weighted trace norm [6].

6 Summary

In this paper  we introduce a unifying family of matrix norms  called the “local max” norms  that
generalizes existing methods for matrix reconstruction  such as the max norm and trace norm. We
examine some interesting sub-families of local max norms  and consider several different options
for interpolating between the trace (or smoothed weighted trace) and max norms. We ﬁnd norms
lying strictly between the trace norm and the max norm that give improved accuracy in matrix
reconstruction for both simulated data and real movie ratings data. We show that regularizing with
any local max norm is fairly simple to optimize  and give a theoretical result suggesting improved
matrix reconstruction using new norms in this family.

Acknowledgements

R.F. is supported by NSF grant DMS-1203762. R.S. is supported by NSERC and Early Researcher
Award.

3For Netﬂix  the test set we use is their “qualiﬁcation set”  designed for a more uniform distribution of
ratings across users relative to the training set. For MovieLens  we choose our test set at random from the
available data.

4To check this  we subsample half of the test data at random  and use it as a validation set to choose (ζ  τ )
for each method (as speciﬁed in Table 1). We then evaluate error on the remaining half of the test data. For
MovieLens  the local max norm gives an RMSE of 0.7820 with selected parameter values ζ = τ = 0.05  as
compared to an RMSE of 0.7829 with selected smoothing parameter ζ = 0.10 for the smoothed weighted trace
norm. For Netﬂix  the local max norm gives an RMSE of 0.9093 with ζ = τ = 0.05  while the smoothed
weighted trace norm gives an RMSE of 0.9098 with ζ = 0.05. The other tested methods give higher error on
both datasets.

8

References
[1] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. 18th Annual Conference on

Learning Theory (COLT)  pages 545–560  2005.

[2] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. Journal of

Machine Learning Research  11:2057–2078  2010.

[3] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion:

Optimal bounds with noise. arXiv:1009.2118  2010.

[4] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction.

24th Annual Conference on Learning Theory (COLT)  2011.

[5] R. Salakhutdinov and N. Srebro. Collaborative Filtering in a Non-Uniform World: Learning
with the Weighted Trace Norm. Advances in Neural Information Processing Systems  23  2010.
[6] R. Foygel  R. Salakhutdinov  O. Shamir  and N. Srebro. Learning with the weighted trace-norm
under arbitrary sampling distributions. Advances in Neural Information Processing Systems 
24  2011.

[7] J. Lee  B. Recht  R. Salakhutdinov  N. Srebro  and J. Tropp. Practical Large-Scale Optimization
for Max-Norm Regularization. Advances in Neural Information Processing Systems  23  2010.
[8] E. Hazan  S. Kale  and S. Shalev-Shwartz. Near-optimal algorithms for online matrix predic-

tion. 25th Annual Conference on Learning Theory (COLT)  2012.

[9] M. Fazel  H. Hindi  and S. Boyd. A rank minimization heuristic with application to mini-
mum order system approximation. In Proceedings of the 2001 American Control Conference 
volume 6  pages 4734–4739  2002.

[10] N. Srebro  J.D.M. Rennie  and T.S. Jaakkola. Maximum-margin matrix factorization. Advances

in Neural Information Processing Systems  18  2005.

[11] J.D.M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd international conference on Machine learning  pages
713–719. ACM  2005.

[12] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Advances in neural infor-

mation processing systems  20:1257–1264  2008.

[13] P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and struc-

tural results. Journal of Machine Learning Research  3:463–482  2002.

[14] J. Bennett and S. Lanning. The netﬂix prize.

volume 2007  page 35. Citeseer  2007.

In Proceedings of KDD Cup and Workshop 

[15] MovieLens Dataset. Available at http://www.grouplens.org/node/73. 2006.

9

,Niki Kilbertus
Mateo Rojas Carulla
Giambattista Parascandolo
Moritz Hardt
Dominik Janzing
Bernhard Schölkopf
Yangyan Li
Rui Bu
Mingchao Sun
Wei Wu
Xinhan Di
Baoquan Chen