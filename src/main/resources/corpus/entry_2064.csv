2016,Fast learning rates with heavy-tailed losses,We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses  we introduce two new conditions: (i)  the envelope function $\sup_{f \in \mathcal{F}}|\ell \circ f|$  where $\ell$ is the loss function and $\mathcal{F}$ is the hypothesis class  exists and is $L^r$-integrable  and (ii) $\ell$ satisfies the multi-scale Bernstein's condition on $\mathcal{F}$. Under these assumptions  we prove that learning rate faster than $O(n^{-1/2})$ can be obtained and  depending on $r$ and the multi-scale Bernstein's powers  can be arbitrarily close to $O(n^{-1})$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.,Fast learning rates with heavy-tailed losses

Vu Dinh1 Lam Si Tung Ho2 Duy Nguyen3 Binh T. Nguyen4

1Program in Computational Biology  Fred Hutchinson Cancer Research Center

2Department of Biostatistics  University of California  Los Angeles

3Department of Statistics  University of Wisconsin-Madison

4Department of Computer Science  University of Science  Vietnam

Abstract

We study fast learning rates when the losses are not necessarily bounded and may
have a distribution with heavy tails. To enable such analyses  we introduce two
new conditions: (i) the envelope function supf∈F |(cid:96) ◦ f|  where (cid:96) is the loss func-
tion and F is the hypothesis class  exists and is Lr-integrable  and (ii) (cid:96) satisﬁes
the multi-scale Bernstein’s condition on F. Under these assumptions  we prove
that learning rate faster than O(n−1/2) can be obtained and  depending on r and
the multi-scale Bernstein’s powers  can be arbitrarily close to O(n−1). We then
verify these assumptions and derive fast learning rates for the problem of vector
quantization by k-means clustering with heavy-tailed distributions. The analy-
ses enable us to obtain novel learning rates that extend and complement existing
results in the literature from both theoretical and practical viewpoints.

1

Introduction

The rate with which a learning algorithm converges as more data comes in play a central role in
machine learning. Recent progress has reﬁned our theoretical understanding about setting under
which fast learning rates are possible  leading to the development of robust algorithms that can
automatically adapt to data with hidden structures and achieve faster rates whenever possible. The
literature  however  has mainly focused on bounded losses and little has been known about rates of
learning in the unbounded cases  especially in cases when the distribution of the loss has heavy tails
[van Erven et al.  2015].
Most of previous work about learning rate for unbounded losses are done in the context of density
estimation [van Erven et al.  2015  Zhang  2006a b]  of which the proofs of fast rates implicitly
employ the central condition [Gr¨unwald  2012] and cannot be extended to address losses with poly-
nomial tails [van Erven et al.  2015]. Efforts to resolve this issue include Brownlees et al. [2015] 
which proposes using some robust mean estimators to replace empirical means  and Cortes et al.
[2013]  which derives relative deviation and generalization bounds for unbounded losses with the
assumption that Lr-diameter of the hypothesis class is bounded. However  results about fast learning
rates were not obtained in both approaches. Fast learning rates are derived in Lecu´e and Mendel-
son [2013] for sub-Gaussian losses and in Lecu´e and Mendelson [2012] for hypothesis classes that
have sub-exponential envelope functions. To the best of our knowledge  no previous work about fast
learning rates for heavy-tailed losses has been done in the literature.
The goal of this research is to study fast learning rates for the empirical risk minimizer when the
losses are not necessarily bounded and may have a distribution with heavy tails. We recall that
heavy-tailed distributions are probability distributions whose tails are not exponentially bounded:
that is  they have heavier tails than the exponential distribution. To enable the analyses of fast
rates with heavy-tailed losses  two new assumptions are introduced. First  we assume the existence
and the Lr-integrability of the envelope function F = supf∈F |f| of the hypothesis class F for

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

some value of r ≥ 2  which enables us to use the results of Lederer and van de Geer [2014] on
concentration inequalities for suprema of empirical unbounded processes. Second  we assume that
the loss function satisﬁes the multi-scale Bernstein’s condition  a generalization of the standard
Bernstein’s condition for unbounded losses  which enables derivation of fast learning rates.
Building upon this framework  we prove that if the loss has ﬁnite moments up to order r large
enough and if the hypothesis class satisﬁes the regularity conditions described above  then learning
rate faster than O(n−1/2) can be obtained. Moreover  depending on r and the multi-scale Bernstein’s
powers  the learning rate can be arbitrarily close to the optimal rate O(n−1). We then verify these
assumptions and derive fast learning rates for the k-mean clustering algorithm and prove that if the
distribution of observations has ﬁnite moments up to order r and satisﬁes the Pollard’s regularity
conditions  then fast learning rate can be derived. The result can be viewed as an extension of
the result from Antos et al. [2005] and Levrard [2013] to cases when the source distribution has
unbounded support  and produces a more favorable convergence rate than that of Telgarsky and
Dasgupta [2013] under similar settings.

2 Mathematical framework
Let the hypothesis class F be a class of functions deﬁned on some measurable space X with values
in R. Let Z = (X  Y ) be a random variable taking values in Z = X ×Y with probability distribution
P where Y ⊂ R. The loss (cid:96) : Z ×F → R+ is a non-negative function. For a hypothesis f ∈ F and
n iid samples {Z1  Z2  . . .   Zn} of Z  we deﬁne

P (cid:96)(f ) = EZ∼P [(cid:96)(Z  f )]

and

Pn(cid:96)(f ) =

1
n

(cid:96)(Zi  f ).

n(cid:88)

i=1

For unsupervised learning frameworks  there is no output (Y = ∅) and the loss has the form (cid:96)(X  f )
depending on applications. Nevertheless  P (cid:96)(f ) and Pn(cid:96)(f ) can be deﬁned in a similar manner. We
will abuse the notation to denote the losses (cid:96)(Z  f ) by (cid:96)(f ). We also denote the optimal hypothesis
f∗ be any function for which P (cid:96)(f∗) = inf f∈F P (cid:96)(f ) := P ∗ and consider the empirical risk
minimizer (ERM) estimator ˆfn = arg minf∈F Pn(cid:96)(f ).
We recall that heavy-tailed distributions are probability distributions whose tails are not exponen-
tially bounded. Rigorously  the distribution of a random variable V is said to have a heavy right
tail if limv→∞ eλvP[V > v] = ∞ for all λ > 0 and the deﬁnition is similar for heavy left tail. A
learning problem is said to be with heavy-tailed loss if the distribution of (cid:96)(f ) has heavy tails from
some or all hypotheses f ∈ F.
For a pseudo-metric space (G  d) and  > 0  we denote by N (  G  d) the covering number of (G  d);
that is  N (  G  d) is the minimal number of balls of radius  needed to cover G. The universal metric
entropy of G is deﬁned by H(  G) = supQ log N (  G  L2(Q))  where the supremum is taken over
the set of all probability measures Q concentrated on some ﬁnite subset of G. For convenience  we
deﬁne G = (cid:96) ◦ F the class of all functions g such that g = (cid:96)(f ) for some f ∈ F and denote by F a
ﬁnite subset of F such that G is contained in the union of balls of radius  with centers in G = (cid:96)◦F.
We refer to F and G as an -net of F and G  respectively.
To enable the analyses of fast rates for learning problems with heavy-tailed losses  throughout the
paper  we impose the following regularity conditions on F and (cid:96).
Assumption 2.1 (Multi-scale Bernstein’s condition). Deﬁne F∗ = arg minF P (cid:96)(f ). There exist a
ﬁnite partition of F = ∪i∈IFi  positive constants B = {Bi}i∈I  constants γ = {γi}i∈I in (0  1] 
and f∗ = {f∗
i )])γi for all i ∈ I and
f ∈ Fi.
Assumption 2.2 (Entropy bounds). The hypothesis class F is separable and there exist C ≥ 1 
K ≥ 1 such that ∀ ∈ (0  K]  the L2(P )-covering numbers and the universal metric entropies of G
are bounded as log N ( G  L2(P )) ≤ C log(K/) and H( G) ≤ C log(K/).
Assumption 2.3 (Integrability of the envelope function). There exists W > 0  r ≥ C + 1 such that

i }i∈I ⊂ F∗ such that E[((cid:96)(f ) − (cid:96)(f∗

i ))2] ≤ Bi (E[(cid:96)(f ) − (cid:96)(f∗

(cid:0)E supg∈G |g|r(cid:1)1/r ≤ W .

The multi-scale Bernstein’s condition is more general than the Bernstein’s condition. This entails
that the multi-scale Bernstein’s condition holds whenever the Bernstein’s condition does  thus al-

2

lows us to consider a larger class of problems. In other words  our results are also valid with the
Bernstein’s condition. The multi-scale Bernstein’s condition is more proper to study unbounded
losses since it is able to separately consider the behaviors of the risk function on microscopic and
macroscopic scales  for which the distinction can only be observed in an unbounded setting.
We also recall that if G has ﬁnite VC-dimension  then Assumption 2.2 is satisﬁed [Boucheron et al. 
2013  Bousquet et al.  2004]. Both Bernstein’s condition and the assumption of separable parametric
hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic
settings. A review about the Bernstein’s condition and its applications is Mendelson [2008]  while
fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were pre-
viously studied in Mehta and Williamson [2014] under the stochastic mixability condition. Fast
learning rate for hypothesis classes with envelope functions were studied in Lecu´e and Mendelson
[2012]  but under a much stronger assumption that the envelope function is sub-exponential.
Under these assumptions  we illustrate that fast rates for heavy-tailed losses can be obtained.
Throughout the analyses  two recurrent analytical techniques are worth mentioning. The ﬁrst comes
from the simple observation that in the standard derivation of fast learning rates for bounded losses 
the boundedness assumption is used in multiple places only to provide reverse-Holder-type inequali-
ties  where the L2-norm are upper bounded by the L1-norm. This use of the boundedness assumption
can be simply relieved by the assumption that the Lr-norm of the loss is bounded  which implies

(cid:107)u(cid:107)L2 ≤ (cid:107)u(cid:107)(r−2)/(2r−2)

L1

(cid:107)u(cid:107)r/(2r−2)

Lr

.

The second technique relies on the following results of Lederer and van de Geer [2014] on concen-
tration inequalities for suprema of empirical unbounded processes.
Lemma 2.1. If {Vk : k ∈ K} is a countable family of non-negative functions such that

|Vk|r ≤ M r

E sup
k∈K

then for all ζ  x > 0  we have

σ2 = sup
k∈K

EV 2

k

and

P[V ≥ (1 + ζ)EV + x] ≤ min
1≤l≤r

(1/x)l

V := sup
k∈K

PnVk 

(cid:20)(cid:16)

(cid:17)l(cid:21)
64/ζ + ζ + 7) (l/n)1−l/r M + 4σ(cid:112)l/n

.

An important notice from this result is that the failure probability is a polynomial in the deviation
x. As we will see later  for a given level of conﬁdence δ  this makes the constant in the convergence
rate a polynomial function of (1/δ) instead of log(1/δ) as in sub-exponential cases. Thus  more
careful examinations of the order of the failure probability are required for the derivation of any
generalization bound with heavy-tailed losses.

3 Fast learning rates with heavy-tailed losses

The derivation of fast learning rate with heavy tailed losses proceeds as follows. First  we will use
the assumption of integrable envelope function to prove a localization-based result that allows us to
reduce the analyses from the separable parametric classes F to its ﬁnite -net F. The multi-scale
Bernstein’s condition is then employed to derive a fast-rate inequality that helps distinguish the
optimal hypothesis from alternative hypotheses in F. The two results are then combined to obtain
fast learning rates.

3.1 Preliminaries
Throughout this section  let G be an -net for G in the L2(P )-norm  with  = n−β for some
1 ≥ β > 0. Denote by π : G → G an L2(P )-metric projection from G to G. For any g0 ∈ G  we
denote K(g0) = {|g0 − g| : g ∈ π−1(g0)}. We have

(i) the constant zero function is an element of K(g0) 
(ii) E[supu∈K(g0) |u|r] ≤ (2W )r; and supu∈K(g0) (cid:107)u(cid:107)L2(P ) ≤  
(iii) N (t K(g0)  L2(P )) ≤ (K/t)C for all t > 0.

3

(cid:17) r−2

ED(KZ) ≤(cid:16)

Given a sample Z = (Z1  . . .   Zn)  we denote by KZ the projection of K(g0) onto the sample Z
and by D(KZ) half of the radius of (KZ (cid:107) · (cid:107)2)  that is D(KZ) = supu v∈KZ (cid:107)u − v(cid:107)/4. We have
the following preliminary lemma  for which the proofs are provided in the Appendix.

2√
n

 + E supu∈K(g0) (Pn − P )u

Lemma 3.1.
Lemma 3.2. Given 0 < ν < 1  there exist constant C1  C2 > 0 depending only on ν such that for
all x > 0  if x ≤ axν + b then x ≤ C1a1/(1−ν) + C2b.
Lemma 3.3. Deﬁne

A(l  r  β C  α) = max(cid:8)l2/r − (1 − β)l + βC  [β (1 − α/2) − 1/2] l + βC(cid:9) .

2(r−1) (2W )

2(r−1) .

r

Assuming that r ≥ 4C and α ≤ 1  if we choose l = r (1 − β) /2 and

0 < β < (1 − 2(cid:112)C/r)/(2 − α) 

then 1 ≤ l ≤ r and A(l  r  β C  α) < 0. This also holds if α ≥ 1 and 0 < β < 1 − 2(cid:112)C/r.

(3.1)

(3.2)

3.2 Local analysis of the empirical loss
The preliminary lemmas enable us to locally bound E supu∈K(g0) (Pn − P )u as follows:
Lemma 3.4. If β < (r − 1)/r  there exists c1 > 0 such that E supu∈K(g0) (Pn − P )u ≤ c1n−β for
all n.

Proof. Without loss of generality  we assume that K(g0) is countable. The arguments to extend
the bound from countable classes to separable classes are standard (see  for example  Lemma 12
of Mehta and Williamson [2014]). Denote ¯Z = supu∈K(g0) (Pn − P )u and let  = 1/nβ  R =
(R1  R2  . . .   Rn) be iid Rademacher random variables  using standard results about symmetrization
and chaining of Rademacher process (see  for example  Corollary 13.2 in Boucheron et al. [2013]) 
we have

n(cid:88)

ER sup
(cid:112)log N (t KX  (cid:107) · (cid:107)2)dt ≤ 24E

u∈K(g0)

j=1

Rju(Xj)


(cid:90) D(KX )∨

nE sup
u∈K(g0)

≤ 24E

(Pn − P )g ≤ 2E

(cid:90) D(KX )∨

0

0

where ER denotes the expected value with respect to the random variables R1  R2  . . .   Rn. By
Assumption 2.2  we deduce that

√

n K(g0)(cid:1)dt 

(cid:113)H(cid:0)t/
C0 = O((cid:112)log n).
2(r−1) /2 = O((cid:112)log n/

r

√

n) 

nE ¯Z ≤ C0(K  n  σ C)( + ED(KX ))

where

If we deﬁne

x =  + E ¯Z  b = C0/n = O((cid:112)log n/nβ+1)  a = C0n−1/2(2W )

then by Lemma 3.1  we have x ≤ ax(r−2)/(2r−2) + b + . Using lemma 3.2  we have

x ≤ C1a2(r−1)/r + C2(b + ) ≤ C3n−β 

which completes the proof.

Lemma 3.5. Assuming that r ≥ 4C  if β < 1 − 2(cid:112)C/r  there exist c1  c2 > 0 such that for all n

Pnu ≤(cid:16)

9c1 + (c2/δ)1/[r(1−β)](cid:17)

n−β

∀g0 ∈ G

and δ > 0

sup

u∈K(g0)

with probability at least 1 − δ.

4

Applying Lemma 2.1 for ζ = 8 and x = y/nβ for ¯Z  using the facts that

Z = sup

u∈K(g0)

Pnu ≤ ¯Z + sup
u∈K(g0)

(cid:112)E[u(X)]2 ≤  = 1/nβ 
(cid:20)(cid:16)

y−l

1≤l≤r

σ = sup
u∈Kg0

P(cid:2) ¯Z ≥ 9E ¯Z + y/nβ(cid:3) ≤ min

we have

P u ≤ ¯Z + sup
u∈K(g0)

(cid:107)u(cid:107)L2(P ) = ¯Z + .

|u|r] ≤ (2W )r 

and

E[ sup
u∈Kg0

(cid:17)l(cid:21)
46 (l/n)1−l/r nβW + 4(cid:112)l/n

:= φ(y  n).

Proof. Denote Z = supu∈K(g0) Pnu and ¯Z = supu∈K(g0) (Pn − P )u. We have

To provide a union bound for all g0 ∈ G  we want the total failure probability φ(y  n)(nβK)C ≤ δ.
This failure probability  as a function of n  is of order A(l  r  β C  α) (as deﬁne in Lemma 3.3) with
such that φ(y  n)(nβK)C ≤ c2/(nc3yl) ≤ c2/yr(1−β)/2. The proof is completed by choosing

α = 2 . By choosing l = r(1 − β)/2 and β < 1 − 2(cid:112)C/r  we deduce that there exist c2  c3 > 0
y = (c2/δ)2/[r(1−β)] and using the fact that E ¯Z ≤ c1/nβ (note that 1 − 2(cid:112)C/r ≤ (r − 1)/r
in the L2(P )-norm  with  = n−β where β < 1 − 2(cid:112)C/r. Then there exist c1  c2 > 0 such that for

A direct consequence of this Lemma is the following localization-based result.
Theorem 3.1 (Local analysis). Under Assumptions 2.1  2.2 and 2.3  let G be a minimal -net for G

and we can apply Lemma 3.4 to get the bound).

Png ≥ Pn(π(g)) −(cid:16)

9c1 + (c2/δ)2/[r(1−β)](cid:17)

n−β

∀g ∈ G

all δ > 0 

with probability at least 1 − δ.

3.3 Fast learning rates with heavy-tailed losses

Theorem 3.2. Given a0  δ > 0. Under the multi-scale (B  γ  I)-Bernstein’s condition and the
assumption that r ≥ 4C  consider

0 < β < (1 − 2(cid:112)C/r)/(2 − γi)

∀i ∈ I.

Then there exist Na0 δ r B γ > 0 such that ∀f ∈ F and n ≥ Na0 δ r B γ  we have

P (cid:96)(f ) − P ∗ ≥ a0/nβ
with probability at least 1 − δ.
Proof. Deﬁne a = [P (cid:96)(f ) − P ∗]nβ. Assuming that f ∈ Fi  applying Lemma 2.1 for ζ = 1/2 and
x = a/4nβ for a single hypothesis f  we have

∃f∗ ∈ F∗ : Pn(cid:96)(f ) − Pn(cid:96)(f∗) ≥ a0/(4nβ)

implies

(3.3)

where

P [Pn(cid:96)(f ) − Pn(cid:96)(f∗

(4/a)l(cid:16)

i ) ≤ (P (cid:96)(f ) − P (cid:96)(f∗

(cid:17)l
50nβ (l/n)1−l/r W + 4nβBiaγi/2/nβγi/2(cid:112)l/n

i ))/4] ≤ h(a  n)

h(a  n  i) = min
1≤l≤r

using the fact that σ2 = E[(cid:96)(f )−(cid:96)(f∗
γi ≤ 1  h(a  n  i) is a non-increasing function in a. Thus 
P [Pn(cid:96)(f ) − Pn(cid:96)(f∗

i )]2 ≤ Bi [E((cid:96)(f ) − (cid:96)(f∗
i ) ≤ (P (cid:96)(f ) − P (cid:96)(f∗

i ))]γi = Biaγi/nβγi if f ∈ Fi. Since
i ))/4] ≤ h(a0  n  i).

To provide a union bound for all f ∈ F such that P (cid:96)(f ) − P (cid:96)(f∗
i ) ≥ a0/nβ  we want the total
failure probability to be small. This is guaranteed if h(a0  n  i)(nβK)C ≤ δ. This failure probability 
as a function of n  is of order A(l  r  β C  γi) as deﬁned in equation (3.1). By choosing r  l as in
Lemma 3.3 and β as in equation (3.3)  we have 1 ≤ l ≤ r and A(l  r  β C  γi) < 0 for all i. Thus 
there exists c4  c5  c6 > 0 such that

h(a0  n  i)(nβK)C ≤ c6a

−c5(1−γi/2)
0

Hence  when n ≥ Na δ r B γ =
min{γ}1{a0<1}  we have: ∀f ∈ F  P (cid:96)(f ) − P ∗ ≥ a0/nβ
Pn(cid:96)(f∗) ≥ a0/(4nβ) with probability at least 1 − δ.

−c5(1−˜γ/2)
c6δa
0

(cid:16)

n−c4

(cid:17)1/c4 where ˜γ = max{γ}1{a0≥1} +

∀n  i.

implies

∃f∗ ∈ F∗  Pn(cid:96)(f ) −

5

Theorem 3.3. Under Assumptions 2.1  2.2 and 2.3  consider β as in equation (3.3) and c1  c2 as in
previous theorems. For all δ > 0  there exists Nδ r B γ such that if n ≥ Nδ r B γ  then

(cid:16)

36c1 + 1 + 4 (2c2/δ)2/[r(1−β)](cid:17)

n−β

P (cid:96)( ˆfz) ≤ P (cid:96)(f∗) +

with probability at least 1 − δ.
Proof of Theorem 3.3. Let F by an -net of F with  = 1/nβ such that f∗ ∈ F. We denote the
projection of ˆfz to F by f1 = π( ˆfz). For a given δ > 0  deﬁne

(cid:110)∃f ∈ F : Pnf ≤ Pn(π(f )) −(cid:16)

9c1 + (c3/δ)2/[r(1−β)](cid:17)

A2 =(cid:8)∃f ∈ F : Pn(cid:96)(π(f )) − Pn(cid:96)(f∗) ≤ a0/(4nβ) and P (cid:96)(π(f )) − P (cid:96)(f∗) ≥ a0/nβ(cid:9)  

A1 =

 

n−β(cid:111)

where c1  c2 is deﬁned as in previous theorem  a0/4 = 9c1 + (c3/δ)2/[r(1−β)] and n ≥ Na0 δ r γ.
We deduce that A1 and A2 happen with probability at most δ. On the other hand  under the event
that A1 and A2 do not happen  we have

Pn(cid:96)(f1) ≤ Pn(cid:96)( ˆfz) +

n−β ≤ Pn(cid:96)(f∗) + a0/(4nβ).

(cid:16)

9c1 + (c3/δ)2/[r(1−β)](cid:17)

By deﬁnition of F  we have P (cid:96)( ˆfz) ≤ P (cid:96)(f1) +  ≤ P (cid:96)(f∗) + (a0 + 1)/nβ.

3.4 Verifying the multi-scale Bernstein’s condition

In practice  the most difﬁcult condition to verify for fast learning rates is the multi-scale Bernstein’s
condition. We derive in this section some approaches to verify the condition. We ﬁrst extend the re-
sult of Mendelson [2008] to prove that the (standard) Bernstein’s condition is automatically satisﬁed
for functions that are relatively far way from f∗ under the integrability condition of the envelope
function (proof in the Appendix). We recall that R(f ) = E(cid:96)(f ) is referred to as the risk function.
Lemma 3.6. Under Assumption 2.3  we deﬁne M = W r/(r−2) and γ = (r − 2)/(r − 1). Then  if
α > M and R(f ) ≥ α/(α − M )R(f∗)  then E((cid:96)(f ) − (cid:96)(f∗))2 ≤ 2αγE((cid:96)(f ) − (cid:96)(f∗))γ.
This allows us to derive the following result  for which the proof is provided in the Appendix.
Lemma 3.7. If F is a subset of a vector space with metric d and the risk function R(f ) = E(cid:96)(f )
has a unique minimizer on F at f∗ in the interior of F and

(i) There exists L > 0 such that E((cid:96)(f ) − (cid:96)(g))2 ≤ Ld(f  g)2 for all f  g ∈ F.
(ii) There exists m ≥ 2  c > 0 and a neighborhood U around f∗ such that

R(f ) − R(f∗) ≥ cd(f  f∗)m for all f ∈ U.

Then the multi-scale Bernstein’s condition holds for γ = ((r − 2)/(r − 1)  2/m).
Corollary 3.1. Suppose that (F  d) is a pseudo-metric space  (cid:96) satisﬁes condition (i) in Lemma 3.7
and the risk function is strongly convex with respect to d  then the Bernstein’s condition holds with
γ = 1.
Remark 3.1. If the risk function is analytic at f∗  then condition (ii) in Lemma 3.7 holds. Similarly 
if the risk function is continuously differentiable up to order 2 and the Hessian of R(f ) is positive
deﬁnite at f∗  then condition (ii) is valid with m = 2.
Corollary 3.2. If the risk function R(f ) = E(cid:96)(f ) has a ﬁnite number of global minimizers
f1  f2  . . .   fk  (cid:96) satisﬁes condition (i) in Lemma 3.7 and there exists mi ≥ 2  ci > 0 and neighbor-
hoods Ui around fi such that R(f ) − R(fi) ≥ cid(f  fi)mi for all f ∈ Ui  i = 1  . . .   k  then the
multi-scale Bernstein’s condition holds for γ = ((r − 2)/(r − 1)  2/m1  . . .   2/mk).

3.5 Comparison to related work

Theorem 3.3 dictates that under our settings  the problem of learning with heavy-tailed losses can
obtain convergence rates up to order

O(cid:16)

√
n−(1−2

C/r)/(2−min{γ})(cid:17)

(3.4)

6

where γ is the multi-scale Bernstein’s order and r is the degree of integrability of the loss. We
recall that convergence rate of O(n−1/(2−γ)) is obtained in Mehta and Williamson [2014] under the
same setting but for bounded losses. (The analysis there was done under the γ-weakly stochastic
mixability condition  which is equivalent with the standard γ-Bernstein’s condition for bounded
losses [van Erven et al.  2015]). We note that if the loss is bounded  r = ∞ and (3.4) reduces to the
convergence rate obtained in Mehta and Williamson [2014].
Fast learning rates for unbounded loses are previously derived in Lecu´e and Mendelson [2013]
for sub-Gaussian losses and in Lecu´e and Mendelson [2012] for hypothesis classes that have sub-
exponential envelope functions. In Lecu´e and Mendelson [2013]  the Bernstein’s condition is not
directly imposed  but is replaced by condition (ii) of Lemma 3.7 with m = 2 on the whole hypothesis
class  while the assumption of sub-Gaussian hypothesis class validates condition (i). This implies
the standard Bernstein’s condition with γ = 1 and makes the convergence rate O(n−1) consistent
with our result (note that for sub-Gaussian losses  r can be chosen arbitrary large). The analysis of
Lecu´e and Mendelson [2012] concerns about non-exact oracle inequalities (rather than the sharp
oracle inequalities we investigate in this paper) and can not be directly compared with our results.

4 Application: k-means clustering with heavy-tailed source distributions
k-means clustering is a method of vector quantization aiming to partition n observations into k ≥ 2
clusters in which each observation belongs to the cluster with the nearest mean. Formally  let X be
a random vector taking values in Rd with distribution P . Given a codebook (set of k cluster centers)
C = {yi} ∈ (Rd)k  the distortion (loss) on an instant x is deﬁned as (cid:96)(C  x) = minyi∈C (cid:107)x −
yi(cid:107)2 and k-means clustering method aims at ﬁnding a minimizer C∗ of R((cid:96)(C)) = P (cid:96)(C) via
minimizing the empirical distortion Pn(cid:96)(C).
The rate of convergence of k-means clustering has drawn considerable attention in the statistics and
machine learning literatures [Pollard  1982  Bartlett et al.  1998  Linder et al.  1994  Ben-David 
2007]. Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al.
[2005] in the case when the source distribution is supported on a ﬁnite set of points  and by Lev-
rard [2013] under the assumptions that the source distribution has bounded support and satisﬁes the
so-called Pollard’s regularity condition  which dictates that P has a continuous density with respect
to the Lebesgue measure and the Hessian matrix of the mapping C → R(C) is positive deﬁnite
at C∗. Little is known about the ﬁnite-sample performance of empirically designed quantizers un-
der possibly heavy-tailed distributions. In Telgarsky and Dasgupta [2013]  a convergence rate of
O(n−1/2+2/r) are derived  where r is the number of moments of X that are assumed to be ﬁnite.
Brownlees et al. [2015] uses some robust mean estimators to replace empirical means and derives a
convergence rate of O(n−1/2) assuming only that the variance of X is ﬁnite.
The results from previous sections enable us to prove that with proper setting  the convergence
rate of k-means clustering for heavy-tailed source distributions can be arbitrarily close to O(1/n).
Following the framework of Brownlees et al. [2015]  we consider

G = {(cid:96)(C  x) = min
yi∈C

(cid:107)x − yi(cid:107)2  C ∈ F = (−ρ  ρ)d×k}

for some ρ > 0 with the regular Euclidean metric. We let C∗  ˆCn be deﬁned as in the previous
sections.
Theorem 4.1. If X has ﬁnite moments up to order r ≥ 4k(d + 1)  P has a continuous density with
respect to the Lebesgue measure  the risk function has a ﬁnite number of global minimizers and the
Hessian matrix of C → R(C) is positive deﬁnite at the every optimal C∗ in the interior of F  then
for all β that satisﬁes

there exists c1  c2 > 0 such that for all δ > 0  with probability at least 1 − δ  we have

r − 1
r

0 < β <

R( ˆCn) − R(C∗) ≤(cid:16)

(1 − 2(cid:112)k(d + 1)/r) 
c1 + 4 (c2/δ)2/r(cid:17)

n−β

Moreover  when r → ∞  β can be chosen arbitrarily close to 1.

7

Proof. We have

(cid:18)

E sup
C∈F

(cid:96)(C  X)r

(cid:19)1/r ≤

(cid:18) 1

2r

(cid:19)1/r ≤

(cid:18) 1

2

E(cid:107)X(cid:107)2r +

1
2

ρ2r

(cid:19)1/r ≤ W < ∞ 

E[(cid:107)X(cid:107)2 + ρ2]r

while standard results about VC-dimension of k-means clustering hypothesis class guarantees that
C ≤ k(d + 1) [Linder et al.  1994]. On the other hand  we can verify that

E[(cid:96)(C  X) − (cid:96)(C(cid:48)  X)]2 ≤ Lρ(cid:107)C − C(cid:48)(cid:107)2
2 

which validates condition (i) in Lemma 3.7. The fact that the Hessian matrix of C → R(C) is
positive deﬁnite at C∗ prompts R( ˆCn)−R(C∗) ≥ c(cid:107) ˆCn−C∗(cid:107)2 for some c > 0 in a neighborhood U
around any optimal codebook C∗. Thus  Lemma 3.6 conﬁrms the multi-scale Bernstein’s condition
with γ = ((r − 2)/(r − 1)  1  . . .   1). The inequality is then obtained from Theorem 3.3.

5 Discussion and future work

We have shown that fast learning rates for heavy-tailed losses can be obtained for hypothesis classes
with an integrable envelope when the loss satisﬁes the multi-scale Bernstein’s condition. We then
verify those conditions and obtain new convergence rates for k-means clustering with heavy-tailed
losses. The analyses extend and complement existing results in the literature from both theoretical
and practical points of view. We also introduce a new fast-rate assumption  the multi-scale Bern-
stein’s condition  and provide a clear path to verify the assumption in practice. We believe that the
multi-scale Bernstein’s condition is the proper assumption to study fast rates for unbounded losses 
for its ability to separate the behaviors of the risk function on microscopic and macroscopic scales 
for which the distinction can only be observed in an unbounded setting.
There are several avenues for improvement. First  we would like to consider hypothesis class with
polynomial entropy bounds. Similarly  the condition of independent and identically distributed ob-
servations can be replaced with mixing properties [Steinwart and Christmann  2009  Hang and Stein-
wart  2014  Dinh et al.  2015]. While the condition of integrable envelope is an improvement from
the condition of sub-exponential envelope previously investigated in the literature  it would be in-
teresting to see if the rates retain under weaker conditions  for example  the assumption that the
Lr-diameter of the hypothesis class is bounded [Cortes et al.  2013]. Finally  the recent work of
Brownlees et al. [2015]  Hsu and Sabato [2016] about robust estimators as alternatives of ERM to
study heavy-tailed losses has yielded more favorable learning rates under weaker conditions  and we
would like to extend the result in this paper to study such estimators.

Acknowledgement

Vu Dinh was supported by DMS-1223057 and CISE-1564137 from the National Science Foundation
and U54GM111274 from the National Institutes of Health. Lam Si Tung Ho was supported by NSF
grant IIS 1251151.

8

References
Andr´as Antos  L´aszl´o Gy¨orﬁ  and Andr´as Gy¨orgy. Individual convergence rates in empirical vector quantizer

design. IEEE Transactions on Information Theory  51(11):4013–4022  2005.

Peter L Bartlett  Tam´as Linder  and G´abor Lugosi. The minimax distortion redundancy in empirical quantizer

design. IEEE Transactions on Information Theory  44(5):1802–1813  1998.

Shai Ben-David. A framework for statistical clustering with constant time approximation algorithms for k-

median and k-means clustering. Machine Learning  66(2):243–257  2007.

St´ephane Boucheron  G´abor Lugosi  and Pascal Massart. Concentration inequalities: A nonasymptotic theory

of independence. OUP Oxford  2013.

Olivier Bousquet  St´ephane Boucheron  and G´abor Lugosi.

Advanced lectures on machine learning  pages 169–207. Springer  2004.

Introduction to statistical learning theory.

In

Christian Brownlees  Emilien Joly  and G´abor Lugosi. Empirical risk minimization for heavy-tailed losses. The

Annals of Statistics  43(6):2507–2536  2015.

Corinna Cortes  Spencer Greenberg  and Mehryar Mohri. Relative deviation learning bounds and generalization

with unbounded loss functions. arXiv:1310.5796  2013.

Vu Dinh  Lam Si Tung Ho  Nguyen Viet Cuong  Duy Nguyen  and Binh T Nguyen. Learning from non-iid
data: Fast rates for the one-vs-all multiclass plug-in classiﬁers. In Theory and Applications of Models of
Computation  pages 375–387. Springer  2015.

Peter Gr¨unwald. The safe Bayesian: learning the learning rate via the mixability gap. In Proceedings of the

23rd international conference on Algorithmic Learning Theory  pages 169–183. Springer-Verlag  2012.

Hanyuan Hang and Ingo Steinwart. Fast learning from α-mixing observations. Journal of Multivariate Analysis 

127:184–199  2014.

Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. Journal of Machine

Learning Research  17(18):1–40  2016.

Guillaume Lecu´e and Shahar Mendelson. General nonexact oracle inequalities for classes with a sub-

exponential envelope. The Annals of Statistics  40(2):832–860  2012.

Guillaume Lecu´e and Shahar Mendelson. Learning sub-Gaussian classes: Upper and minimax bounds.

arXiv:1305.4825  2013.

Johannes Lederer and Sara van de Geer. New concentration inequalities for suprema of empirical processes.

Bernoulli  20(4):2020–2038  2014.

Cl´ement Levrard. Fast rates for empirical vector quantization. Electronic Journal of Statistics  7:1716–1746 

2013.

Tam´as Linder  G´abor Lugosi  and Kenneth Zeger. Rates of convergence in the source coding theorem  in
empirical quantizer design  and in universal lossy source coding. IEEE Transactions on Information Theory 
40(6):1728–1740  1994.

Nishant A Mehta and Robert C Williamson. From stochastic mixability to fast rates. In Advances in Neural

Information Processing Systems  pages 1197–1205  2014.

Shahar Mendelson. Obtaining fast error rates in nonconvex situations. Journal of Complexity  24(3):380–397 

2008.

David Pollard. A central limit theorem for k-means clustering. The Annals of Probability  pages 919–926 

1982.

Ingo Steinwart and Andreas Christmann. Fast learning from non-iid observations.

Information Processing Systems  pages 1768–1776  2009.

In Advances in Neural

Matus J Telgarsky and Sanjoy Dasgupta. Moment-based uniform deviation bounds for k-means and friends. In

Advances in Neural Information Processing Systems  pages 2940–2948  2013.

Tim van Erven  Peter D Gr¨unwald  Nishant A Mehta  Mark D Reid  and Robert C Williamson. Fast rates in

statistical and online learning. Journal of Machine Learning Research  16:1793–1861  2015.

Tong Zhang. From -entropy to KL-entropy: Analysis of minimum information complexity density estimation.

The Annals of Statistics  34(5):2180–2210  2006a.

Tong Zhang. Information-theoretic upper and lower bounds for statistical estimation. IEEE Transactions on

Information Theory  52(4):1307–1321  2006b.

9

,Alaa Saade
Florent Krzakala
Lenka Zdeborová
Vu Dinh
Lam Ho
Binh Nguyen
Duy Nguyen