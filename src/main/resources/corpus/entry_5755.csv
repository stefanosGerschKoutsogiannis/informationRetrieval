2015,Unlocking neural population non-stationarities using hierarchical dynamics models,Neural population activity often exhibits rich variability. This variability is thought to arise from single-neuron stochasticity  neural dynamics on short time-scales  as well as  from modulations of neural firing properties on long time-scales  often referred to as non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing  we introduce a  hierarchical dynamics model that is able to capture inter-trial modulations in firing rates  as well as neural population dynamics. We derive an algorithm for Bayesian Laplace propagation for fast posterior inference  and demonstrate that our model provides a better account of the structure of neural firing than existing stationary dynamics models  when applied to neural population recordings from primary visual cortex.,Unlocking neural population non-stationarity

using a hierarchical dynamics model

Mijung Park1  Gergo Bohner1  Jakob H. Macke2

1 Gatsby Computational Neuroscience Unit  University College London
2 Research Center caesar  an associate of the Max Planck Society  Bonn

Max Planck Institute for Biological Cybernetics 

Bernstein Center for Computational Neuroscience T¨ubingen

{mijung  gbohner}@gatsby.ucl.ac.uk  jakob.macke@caesar.de

Abstract

Neural population activity often exhibits rich variability. This variability can arise
from single-neuron stochasticity  neural dynamics on short time-scales  as well as
from modulations of neural ﬁring properties on long time-scales  often referred
to as neural non-stationarity. To better understand the nature of co-variability in
neural circuits and their impact on cortical information processing  we introduce
a hierarchical dynamics model that is able to capture both slow inter-trial modula-
tions in ﬁring rates as well as neural population dynamics. We derive a Bayesian
Laplace propagation algorithm for joint inference of parameters and population
states. On neural population recordings from primary visual cortex  we demon-
strate that our model provides a better account of the structure of neural ﬁring than
stationary dynamics models.

1

Introduction

Neural spiking activity recorded from populations of cortical neurons can exhibit substantial vari-
ability in response to repeated presentations of a sensory stimulus [1]. This variability is thought to
arise both from dynamics generated endogenously within the circuit [2] as well as from variations in
internal and behavioural states [3  4  5  6  7]. An understanding of how the interplay between sensory
inputs and endogenous dynamics shapes neural activity patterns is essential for our understanding
of how information is processed by neuronal populations. Multiple statistical [8  9  10  11  12  13]
and mechanistic [14] models for characterising neuronal population dynamics have been developed.
In addition to these dynamics which take place on fast time-scales (milliseconds up to few seconds) 
there are also processes modulating neural ﬁring activity which take place on much slower time-
scales (seconds to hours). Slow drifts in rates across an experiment can be caused by ﬂuctuations in
arousal  anaesthesia level or other physiological properties of the experimental preparation [15  16 
17]. Furthermore  processes such as learning and short-term plasticity can lead to slow changes in
neural ﬁring properties [18]. The statistical structure of these slow ﬂuctuations has been modelled
using state-space models and related techniques [19  20  21  22  23]. Recent experimental ﬁndings
have shown that slow  multiplicative ﬂuctuations in neural excitability are a dominant source of
neural covariability in extracellular multi-cell recordings from cortical circuits [5  17  24].
To accurately capture the the structure of neural dynamics and to disentangle the contributions of
slow and fast modulatory processes to neural variability and co-variability  it is therefore important
to develop models that can capture neural dynamics both on fast (i.e.  within experimental trials) and
slow (i.e.  across trials) time-scales. Few such models exist: Czanner et al. [25] presented a statistical
model of single-neuron ﬁring in which within-trial dynamics are modelled by (generalised) linear
coupling from the recent spiking history of each neuron onto its instantaneous ﬁring rate  and across-
trial dynamics were modelled by deﬁning a random walk model over parameters. More recently 

1

Mangion et al [26] presented a latent linear dynamical system model with Poisson observations
(PLDS  [8  11  13]) with a one-dimensional latent space  and used a heuristic ﬁltering approach
for tracking parameters  again based on a random-walk model. Rabinowitz et al [27] presented
a technique for identifying slow modulatory inputs from the recordings of single neurons using a
Gaussian Process model and an efﬁcient inference technique using evidence optimisation.
Here  we present a hierarchical model that consists of a latent dynamical system with Poisson ob-
servations (PLDS) to model neural population dynamics  combined with a Gaussian process (GP)
[28] to model modulations in ﬁring rates or model-parameters across experimental trials. The use
of an exponential nonlinearity implies that latent modulations have a multiplicative effect on neural
ﬁring rates. Compared to previous models using random walks over parameters  using a GP is a
more ﬂexible and powerful way of modelling the statistical structure of non-stationarity  and makes
it possible to use hyper-parameters that model the variability and smoothness of parameter-changes
across time.
In this paper  we focus on a concrete variant of this general model: We introduce a new set of
variables which control neural ﬁring rate on each trial to capture non-stationarity in ﬁring rates.
We derive a Bayesian Laplace propagation method for inferring the posterior distributions over the
latent variables and the parameters from population recordings of spiking activity. Our approach
generalises the 1-dimensional latent states in [26] to models with multi-dimensional states  as well
as to a Bayesian treatment of non-stationarity based on Gaussian Process priors. The paper is or-
ganised as follows: In Sec. 2  we introduce our framework for constructing non-stationary neural
population models  as well as the concrete model we will use for analyses. In Sec. 3  we derive
the Bayesian Laplace propagation algorithm. In Sec. 4  we show applications to simulated data and
neural population recordings from visual cortex.

2 Hierarchical non-stationary models of neural population dynamics

We start by introducing a hierarchical model for capturing short time-scale population dynamics as
well as long time-scale non-stationarities in ﬁring rates. Although we use the term “non-stationary”
to mean that the system is best described by parameters that change over time (which is how the term
is often used in the context of neural data analysis)  we note that the distribution over parameters
can be described by a stochastic process which might be strictly stationary in the statistical sense1.
Modelling framework We assume that the neural population activity of p neurons yt ∈ Rp de-
pends on a k-dimensional latent state xt ∈ Rk and a modulatory factor h(i) ∈ Rk which is different
for each trial i = {1  . . .   r}. The latent state x models short-term co-variability of spiking activity
and the modulatory factor h models slowly varying mean ﬁring rates across experimental trials.
We model neural spiking activity as conditionally Poisson given the latent state xt and a modulator
h(i)  with a log ﬁring rate which is linear in parameters and latent factors 

yt|xt  C  h(i)  d ∼ Poiss(yt| exp(C(xt + h(i)) + d)) 

where the loading matrix C ∈ Rp×k speciﬁes how each neuron is related to the latent state and the
modulator  d ∈ Rp is an offset term that controls the mean ﬁring rate of each cell  and Poiss(yt|w)
means that the ith entry of yt is drawn independently from Poisson distribution with mean wi (the
ith entry of w). Because of the use of an exponential ﬁring-rate nonlinearity  latent factors have a
multiplicative effect on neural ﬁring rates  as has been observed experimentally [17  5].
Following [11  13  26]  we assume that the latent dynamics evolve according to a ﬁrst-order autore-
gressive process with Gaussian innovations 

xt|xt−1  A  B  Q ∼ N (xt|Axt−1 + But  Q).

Here  we allow for sensory stimuli (or experimental covariates)  ut ∈ Rd to inﬂuence the latent
states linearly. The dynamics matrix A ∈ Rk×k determines the state evolution  B ∈ Rk×d models
the dependence of latent states on external inputs  and Q ∈ Rk×k is the covariance of the innovation
noise. We set Q to be the identity matrix  Q = Ik as in [29]  and we assume x(i)

0 ∼ N (0  Ik).

1A stochastic process is strict-sense stationary if its joint distribution over any two time-points t and s only

depends on the elapsed time t − s.

2

Figure 1:
Schematic of hierarchical non-
stationary Poisson observation Latent Dynam-
ical System (N-PLDS)
for capturing non-
stationarity in mean ﬁring rates. The parameter
h slowly varies across trials and leads to ﬂuc-
tuations in mean ﬁring rates.

The parameters in this model are θ = {A  B  C  d  h(1:r)}. We refer to this general model as non-
stationary PLDS (N-PLDS). Different variants of N-PLDS can be constructed by placing priors on
individual parameters which allow them to vary across trials (in which case they would then depend
on the trial index i) or by omitting different components of the model2.
For the modulator h  we assume that it varies across trials according to a GP with mean mh and
(modiﬁed) squared exponential kernel  h(i) ∼ GP(mh  K(i  j))  where the (i  j)th block of K (size

k × k) is given by K(i  j) = (σ2 + δi j) exp(cid:0)− 1

2τ 2 (i − j)2(cid:1) Ik. Here  we assume the independent

noise-variance on the diagonal () to be constant and small as in [30]. When σ2 =  = 0  the
modulator vanishes  which corresponds to the conventional PLDS model with ﬁxed parameters [11 
13]. When σ2 > 0  the mean ﬁring rates vary across trials  and the parameter τ determines the time-
scale (in units of ‘trials’) of these ﬂuctuations. We impose ridge priors on the model parameters (see
Appendix for details)  so that the total set of hyperparameters of the model is Φ = {mh  σ2  τ 2  φ} 
where φ is the set of ridge parameters.

3 Bayesian Laplace propagation

Our goal is to infer parameters and latent variables in the model. The exact posterior distribution
is analytically intractable due to the use of a Poisson likelihood  and we therefore assume the joint
posterior over the latent variables and parameters to be factorising 

p(θ  x(1:r)

1:T |y(1:r)

1:T   Φ) ∝ p(y(1:r)

1:T |x(1:r)

1:T   θ)p(x(1:r)

1:T |θ  Φ)p(θ|Φ) ≈ q(θ  x(1:r)

1:T ) = qθ(θ)

qx(x(i)

0:T ).

r(cid:89)

i=1

This factorisation simpliﬁes computing the integrals involved in calculating a bound on the marginal
likelihood of the observations 

log p(y(1:r)

1:T |Φ) = log
(cid:90)

≥

dθ dx(1:r)

1:T p(θ  x(1:r)

1:T   y(1:r)

1:T |Φ) 
p(θ  x(1:r)

1:T |Φ)
1:T   y(1:r)
q(θ  x(1:r)
1:T )

dθ dx(1:r)

1:T q(θ  x(1:r)

1:T ) log

.

(1)

(cid:90)

(cid:20)(cid:90)

(cid:21)
1:T |θ)

(cid:21)
1:T |θ)

Similar to variational Bayesian expectation maximization (VBEM) algorithm [29]  our inference
procedure consists of the following three steps: (1) we compute the approximate posterior over
latent variables qx(x(1:r)

0:T ) by integrating out the parameters

0:T ) ∝ exp
(cid:20)(cid:90)

qx(x(1:r)

dθqθ(θ) log p(x(1:r)

1:T   y(1:r)

 

(2)

which is performed by forward-backward message passing relying on the order-1 dependency in
latent states. Then  (2) we compute the approximate posterior over parameters qθ(θ) by integrating
out the latent variables 

qθ(θ) ∝ p(θ) exp

dx(1:r)

0:T qx(x(1:r)

0:T ) log p(x(1:r)

0:T   y(1:r)

 

(3)

and (3) we update the hyperparameters by computing the gradients of the bound on the eq. 1 after
integrating out both latent variables and parameters. We iterate the three steps until convergence.
Unfortunately  the integrals in both eq. 2 and eq. 3 are not analytically tractable  even with the Gaus-
sian distributions for qx(x(1:r)
0:T ) and qθ(θ). For tractability and fast computation of messages in
2A second variant of the model  in which the dynamics matrix determining the spatio-temporal correlations

in the population varies across trials  is described in the Appendix.

3

recording 1recording rthe forward-backward algorithm for eq. 2  we utilise the so-called Laplace propagation or Laplace
expectation propagation (Laplace-EP) [31  32  33]  which makes a Gaussian approximation to each
message based on Laplace approximation  then propagates the messages forward and backward.
While Laplace propagation in the prior work is commonly coupled with point estimates of parame-
ters  we consider the posterior distribution over parameters. For this reason  we refer to our inference
method as Bayesian Laplace propagation. The use of approximate message passing in the Laplace
propagation implies that there is no longer a guarantee that the lower bound will increase monoton-
ically in each iteration  which is the main difference between our method and the VBEM algorithm.
We therefore monitored the convergence of our algorithm by computing one-step ahead prediction
scores [13]. The algorithm proceeds by iterating the following three steps:

(cid:90)

(1) Approximating the posterior over latent states: Using the ﬁrst-order dependency in latent
states  we derive a sequential forward/backward algorithm to obtain qx(x(1:r)
0:T )  generalising the
approach of [26] to multi-dimensional latent states. Since this step decouples across trials  it is easy
to parallelize  and we omit the trial-indices for clarity. We note that computation of the approximate
posterior in this step is not more expensive than Bayesian inference of the latent state in a ‘ﬁxed
parameter’ PLDS. The forward message α(xt) at time t is given by

dxt−1α(xt−1) exp(cid:2)(cid:104)log(p(xt|xt−1)p(yt|xt))(cid:105)qθ (θ)

(4)
Assuming that the forward message at time t− 1 denoted by α(xt−1) is Gaussian  the Poisson
likelihood term will render the forward message at time t non-Gaussian  but we will approximate
α(xt) as a Gaussian using the ﬁrst and second derivatives of the right-hand side of eq. 4 with respect
to xt.
Similarly  the backward message at time t − 1 is given by

α(xt) ∝

(cid:3) .

(cid:90)

dxtβ(xt) exp(cid:0)(cid:104)log(p(xt|xt−1)p(yt|xt))(cid:105)qθ (θ)

(cid:1)  

β(xt−1) ∝

(5)

which we also approximate to a Gaussian for tractability in computing backward messages.
Using the forward/backward messages  we compute the posterior marginal distribution over latent
variables (See Appendix). We need to compute the cross-covariance between neighbouring latent
variables to obtain the sufﬁcient statistics of latent variables (which we will need for updating the
posterior over parameters). The pairwise marginals of latent variables are given by

p(xt  xt+1|y1:T ) ∝ β(xt+1) exp(cid:0)(cid:104)log(p(yt+1|xt+1)p(xt+1|xt))(cid:105)qθ (θ)

(6)
which we approximate as a joint Gaussian distribution by using the ﬁrst/second derivatives of eq. 6
and extracting the cross-covariance term from the joint covariance matrix.

(cid:1) α(xt) 

(2) Approximating the posterior over parameters: After inferring the posterior over latent
states  we update the posterior distribution over the parameters. The posterior over parameters fac-
torizes as

qθ(θ) = qa b(a  b) qc d h(c  d  h(1:r)) 

(7)
where used the vectorized notations b = vec(B(cid:62)) and c = vec(C(cid:62)). We set c  d to the maximum
likelihood estimates ˆc  ˆd for simplicity in inference. The computational cost of this algorithm is
dominated by the cost of calculating the posterior distribution over h(1:r)  which involves manipula-
tion of a rk-dimensional Gaussian. While this was still tractable without further approximations for
the data-set sizes used in our analyses below (hundreds of trials)  a variety of approximate methods
for GP-inference exist which could be used to improve efﬁciency of this computation. In particular 
we will typically be dealing with systems in which τ (cid:29) 1  which means that the kernel-matrix is
smooth and could be approximated using low-rank representations [28].

(3) Estimating hyperparameters:
Finally  after obtaining the the approximate posterior
q(θ  x(1:r)
0:T )  we update the hyperparameters of the prior by maximizing the lower bound with re-
spect to the hyperparameters. The variational lower bound simpliﬁes to (see Ch.5 in [29] for details 
note that the usage of Gaussian approximate posteriors ensures that this step is analogous to hyper
parameter updating in a fully Gaussian LDS)

log p(y(1:r)

1:T |Φ) ≥ −KL(Φ) + c 

(8)

4

Figure 2: Illustration of non-stationarity in ﬁring rates (simulated data). A  B Spike rates of 40
neurons are inﬂuenced by two slowly varying ﬁring rate modulators. The log mean ﬁring rates of the
two groups of neurons are z1(red  group 1) and z2(blue  group 2) across 100 trials. C  D Raster plots
show the extreme cases  i.e. trials 25 and 75. The traces show the posterior mean of z estimated
by N-PLDS (light blue for z2  light red for z1)  independent PLDSs (ﬁt a PLDS to each trial data
individually  dark gray)  and PLDS (light gray). E Total and conditional (on each trial) covariance of
recovered neural responses from each model (averaged across all neuron pairs  and then normalised
for visualisation). The covariances recovered by our model (red) well match the true ones (black) 
while those by independent PLDSs (gray) and a single PLDS (light gray) do not.

2Tr(cid:2)Σ−1

Φ Σ(cid:3) + 1

where c is a constant. Here  the KL divergence between the prior and posterior over parameters 
denoted by N (µΦ  ΣΦ) and N (µ  Σ)  respectively  is given by

KL(Φ) = − 1

2 log |Σ−1

Φ Σ| + 1

(9)
where the prior mean and covariance depend on the hyperparameters. We update the hyperparame-
ters by taking the derivative of KL w.r.t. each hyper parameter. For the prior mean  the ﬁrst derivative
expression provides a closed-form update. For τ (time scale of inter-trial ﬂuctuations in ﬁring rates)
and σ2 (variance of inter-trial ﬂuctuations)  their derivative expressions do not provide a closed form
update  in which case we compute the KL divergence on the grid deﬁned in each hyperparameter
space and choose the value that minimises KL.

2 (µ − µΦ)(cid:62)Σ−1

Φ (µ − µΦ) + c 

Predictive distributions for test data.
In our model  different trials are no longer considered to
be independent  so we can predict parameters for held-out trials. Using the GP model on h and our
approximations  we have Gaussian predictive distributions on h∗ for test data D∗ given training data
D:

p(h∗|D D∗) = N (mh + K∗K−1(µh − mh)  K∗∗ − K∗(K + H−1

(10)
where K is the prior covariance matrix on D and K∗∗ is on D∗  and K∗ is their prior cross-
covariance as introduced in Ch.2 of [28]  and the negative Hessian Hh is deﬁned as

h )−1K∗(cid:62)) 

r(cid:88)

(cid:90)

[

Hh = − ∂2

∂2h(1:h)

T(cid:88)

dx(i)

0:T q(x(i)
0:T )

log p(y(i)
t

|x(i)

t

  ˆc  ˆd  h(i))].

(11)

i=1

t=1

In the applications to simulated and neurophysiological data described in the following  we used this
approach to predict the properties of neural dynamics on held-out trials.

4 Applications

Simulated data: We ﬁrst illustrate the performance of N-PLDS on a simulated population record-
ing from 40 neurons consisting of 100 trials of length T = 200 time steps each. We used a
4-dimensional latent state and assumed that the population consisted of two homogeneous sub-
populations of size 20 each  with one modulatory input controlling rate ﬂuctuations in each group
(See Fig. 2 A). In addition  we assumed that for half of each trial  there was a time-varying stimulus
(‘drifting grating’)  represented by a 3-dimensional vector which consisted of the sine and cosine

5

true z1A group 1−2−10102030405060708090100−2−1trialslog mean firing rateB group 2N-PLDSPLDSIndep-PLDSneuronstrial # 25true z2N-PLDSPLDSIndep-PLDSlog mean firing rate010strial # 75010sneuronsneuronstrial # 25010strial # 75010sneurons−0.200.20.40.60.81-5s-2.5s02.5s5stotal cov (z)condi cov (z)N-PLDSIndep-PLDSPLDStrueC group 1 population activityD group 2 population activityE covariance estimation0102030405060708090100trialsFigure 3: Non-stationary ﬁring rates in a population of V1 neurons. A: Mean ﬁring rates of
neurons (black trace) across trials. Left: The 5 most non-stationary neurons. Right: The 5 most
stationary neurons. The ﬁtted (solid line) and the predicted (circles) mean ﬁring rates are also shown
for N-PLDS (in red) and PLDS (in gray). B Left: The RMSE in predicting single neuron ﬁring rates
across 5 most non-stationary neurons for varying latent dimensionalities k  where N-PLDS achieves
signiﬁcantly lower RMSE. Middle: RMSE for the 5 most stationary neurons  where there is no
difference between two methods (apart from an outlier at k=8). Right: RMSE for the all 64 neurons.

of the time-varying phase of the stimulus (frequency 0.4 Hz) as well as an additional binary term
which indicated whether the stimulus was active.

We ﬁt N-PLDS to the data  and found that it successfully captures the non-stationarity in (log) mean
ﬁring rates  deﬁned by z = C(x + h) + d  as shown in Fig. 2  and recovers the total and trial-
conditioned covariances (the across-trial mean of the single-trial covariances of z). For comparison 
we also ﬁt 100 separate PLDSs to the data from each trial  as well as a single PLDS to the entire
data. The naive approach of ﬁtting an individual PLDS to each trial can  in principle  follow the
modulation. However  as each model is only ﬁt to one trial  the parameter-estimates are very noisy
since they are not sufﬁciently constrained by the data from each trial.

We note that a single PLDS with ﬁxed parameters (as is conventionally used in neural data analysis)
is able to track the modulations in ﬁring rates in the posterior mean here– however  a single PLDS
would not be able to extrapolate ﬁring rates for unseen trials (as we will demonstrate in our analyses
on neural data below). In addition  it will also fail to separate ‘slow’ and ‘fast’ modulations into
different parameters. By comparing the total covariance of the data (averaged across neuron pairs) to
the ‘trial-conditioned’ covariance (calculated by estimating the covariance on each trial individually 
and averaging covariances) one can calculate how much of the cross-neuron co-variability can be
explained by across-trials ﬂuctuations in ﬁring rates (see e.g.  [17]). In this simulation shown in
Fig. 2 (which illustrates an extreme case dominated by strong across-trial effects)  the conditional
covariance is much smaller than the full covariance.

6

Trial0255075100cell#1cell#2cell#3Acell#4cell#5cell#6Mean firing rate (Hz)Bk12345678RMSE 0.10.2N-PLDSPLDS0.010.02k123456785 most non-stationary neurons5 most stationary neuronsN-PLDSPLDSdatak123456780.050.07all neurons (64)5 most non-stationary neurons5 most stationary neurons01051501051501002cell#702cell#802cell#902cell#1002Trial0255075100Neurophysiological data: How big are non-stationarities in neural population recordings  and
can our model successfully capture them? To address these questions  we analyzed a population
recording from anaesthetized macaque primary visual cortex consisting of 64 neurons stimulated by
sine grating stimuli. The details of data collection are described in [5]  but our data-set also included
units not used in the original study. We binned the spikes recorded during 100 trials of length 4s
(stimulus was on for 2s) of the same orientation using 50ms bins  resulting in trials of length T = 80
bins. Analogously to the simulated dataset above  we parameterised the stimulus as a 3-dimensional
vector of the sine and cosine with the same temporal frequency of the drifting grating  as well as an
indicator that speciﬁes whether there is a stimulus or not.
We used 10-fold cross validation to evaluate performance of the model  i.e. repeatedly divided the
data into test data (10 trials) and training data (the remaining 90 trials). We ﬁt the model on each
training set  and using the estimated parameters from the training data  we made predictions on the
modulator h on test data by using the mean of the predictive distribution over h. We note that  in
contrast to conventional applications of cross-validation which assume i.i.d. trials  our model here
also takes into correlations in ﬁring rates across trials– therefore  we had to keep the trial-indices
in order to compute predictive distributions for test data using formulas in eq. 10. Using these
parameters  we drew samples for spikes for the entire trials to compute the mean ﬁring rates of each
neuron at each trial. For comparison  we also ﬁt a single PLDS to the data. As this model does not
allow for across-trial modulations of ﬁring rates  we simply kept the parameters estimated from the
training data. For visualisation of results  we quantiﬁed the ‘non-stationarity’ of each neuron by ﬁrst
smoothing its ﬁring rate across trials (using a kernel of size 10 trials)  calculating the variance of the
smoothed ﬁring rate estimate  and displaying ﬁring rates for the 5 most non-stationary neurons in
the population (Fig. 3A  left) as well as 5 most stationary neurons (Fig. 3A  right). Importantly  the
ﬁring-rates were also correctly interpolated for held out trials (circles in Fig. 3A).
To evaluate whether the additional parameters in N-PLDS result in a superior model compared to
conventional PLDS [13]  we tested the model with different latent dimensionalities ranging from
k = 1 to k = 8  and compared each model against a ‘ﬁxed’ PLDS of matched dimensionality
(Fig. 3B). We estimated predicted ﬁring rates on held out trials by sampling 1000 replicate trials
from the predictive distribution for both models and compared the median (across samples) of the
mean ﬁring rates of each neuron to those of the data. The shown RMSE values are the errors of
predicted ﬁring rate (in Hz) per neuron per held out trial (population mean across all neurons and
trials is 4.54 Hz). We found that N-PLDS outperformed PLDS provided that we had sufﬁciently
many latent states  at least k > 3. For large latent dimensionalities (k > 8) performance degraded
again  which could be a consequence of overﬁtting. Furthermore  we show that for non-stationary
neurons there is a large gain in predictive power (Fig. 3B  left)  whereas for stationary neurons PLDS
and N-PLDS have similar prediction accuracy (Fig. 3B  middle). The RMSE on ﬁring rates for all
neurons (Fig. 3B  right) suggests that our model correctly identiﬁed the ﬂuctuation in ﬁring rates.
We also wanted to gain insights into the temporal scale of the underlying non-stationarities. We ﬁrst
looked at the recovered time-scales τ of the latent modulators  and found them to be highly preserved
across multiple training folds  and  importantly  across different values of the latent dimensionalities 
consistently peaked near 10 trials (Fig. 4 A). We made sure that the peak near 10 trials is not merely
a consequence of parameter initialization– parameters were initialised by ﬁtting a Gaussian Process
with a exponentiated quadratic one-dimensional kernel to each neuron’s mean ﬁring rate over trials
individually  then taking the mean time-scale over neurons as the initial global time-scale for our
kernel. The initial values were 8.12 ± 0.01  differing slightly between training sets. Similarly  we
checked that the parameters of the ﬁnal model (after 30 iterations of Bayesian Laplace propagation) 
were indeed superior to the initial values  by monitoring the prediction error on held-out trials.
Furthermore  due to introducing a smooth change with the correct time scale in the latent space
(e.g.  the posterior mean of h across trials shown in Fig. 4B)  we ﬁnd that N-PLDS recovers more
of the time-lagged covariance of neurons compared to the ﬁxed PLDS model (Fig. 4C).

5 Discussion

Non-stationarities are ubiquitous in neural data: Slow modulations in ﬁring properties can result
from diverse processes such as plasticity and learning  ﬂuctuations in arousal  cortical reorganisation
after injury as well as development and aging. In addition  non-stationarities in neural data can also
be a consequence of experimental artifacts  and can be caused by ﬂuctuations in anaesthesia level 

7

Figure 4: Non-stationary ﬁring rates in a population of V1 neurons (continued). A: Histogram
of time-constants across different latent dimensionalities and training sets. Mean at 10.4 is indicated
by the vertical red line. B: Estimated 7-dimensional modulator (the posterior mean of h). The
modulator with an estimated length scale of approximately 10 trials is smoothly varying across
trials. C: Comparison of normalized mean auto-covariance across neurons.

stability of the physiological preparation or electrode drift. Whatever the origins of non-stationarities
are  it is important to have statistical models which can identify them and disentangle their effects
from correlations and dynamics on faster time-scales [16].
We here presented a hierarchical model for neural population dynamics in the presence of non-
stationarity. Speciﬁcally  we concentrated on a variant of this model which focuses on non-
stationarity in ﬁring rates. Recent experimental studies have shown that slow ﬂuctuations in neural
excitability which have a multiplicative effect on neural ﬁring rates are a dominant source of noise
correlations in anaesthetized visual cortex [17  5  24]. Because of the exponential spiking nonlin-
earity employed in our model  the latent additive ﬂuctuations in the modulator-variables also have
a multiplicative effect on ﬁring rates. Applied to a data-set of neurophysiological recordings  we
demonstrated that this modelling approach can successfully capture non-stationarities in neurophys-
iological recordings from primary visual cortex.
In our model  both neural dynamics and latent modulators are mediated by the same low-dimensional
subspace (parameterised by C). We note  however  that this assumption does not imply that neurons
with strong short-term correlations will also have strong long-term correlations  as different dimen-
sions of this subspace (as long as it is chosen big enough) could be occupied by short and long term
correlations  respectively. In our applications to neural data  we found that the latent state had to be
at least three-dimensional for the non-stationary model to outperform a stationary dynamics model 
and it might be the case that at least three dimensions are necessary to capture both fast and slow
correlations. It is an open question of how correlations on fast and slow timescales are related [17] 
and the techniques presented have the potential to be of use for mapping out their relationships.
There are limitations to the current study: (1) We did not address the question of how to select
amongst multiple different models which could be used to model neural non-stationarity for a given
dataset; (2) we did not present numerical techniques for how to scale up the current algorithm for
larger trial numbers (e.g.  using low-rank approximations to the covariance matrix) or large neural
populations; and (3) we did not address the question of how to overcome the slow convergence
properties of GP kernel parameter estimation [34]. (4) While Laplace propagation is ﬂexible  it is
an approximate inference technique  and the quality of its approximations might vary for different
models of tasks. We believe that extending our method to address these questions provides an
exciting direction for future research  and will result in a powerful set of statistical methods for
investigating how neural systems operate in the presence of non-stationarity.

Acknowledgments

We thank Alexander Ecker and the lab of Andreas Tolias for sharing their data with us [5] (see
http://toliaslab.org/publications/ecker-et-al-2014/)  and for allowing us
to use it in this publication  as well as Maneesh Sahani and Alexander Ecker for valuable comments.
This work was funded by the Gatsby Charitable Foundation (MP and GB) and the German Federal
Ministry of Education and Research (MP and JHM) through BMBF; FKZ:01GQ1002 (Bernstein
Center T¨ubingen). Code available at http://www.mackelab.org/code.

8

AN-PLDSPLDSdata−50510Trial index0100−10Estimated Modulators51015200Time-scale estimatesCountB10515Normalized mean autocovarianceCTime-scale (trials)Time lag (ms)-500050000.20.40.60.81References
[1] A. Renart and C. K. Machens. Variability in neural activity and behavior. Curr Opin Neurobiol  25:211–

[2] A. Destexhe. Intracellular and computational evidence for a dominant role of internal network activity in

cortical computations. Curr Opin Neurobiol  21(5):717–725  2011.

[3] G. Maimon. Modulation of visual physiology by behavioral state in monkeys  mice  and ﬂies. Curr Opin

Neurobiol  21(4):559–64  2011.

[4] K. D. Harris and A. Thiele. Cortical state and attention. Nat Rev Neurosci  12(9):509–523  2011.
[5] Ecker et al. State dependence of noise correlations in macaque primary visual cortex. Neuron  82(1):235–

20  2014.

48  2014.

[6] Ralf M Haefner  Pietro Berkes  and J´ozsef Fiser. Perceptual decision-making as probabilistic inference

by neural sampling. arXiv preprint arXiv:1409.0257  2014.

[7] Alexander S Ecker  George H Denﬁeld  Matthias Bethge  and Andreas S Tolias. On the structure of

population activity under ﬂuctuations in attentional state. bioRxiv  page 018226  2015.

[8] A. C. Smith and E. N. Brown. Estimating a state-space model from point process observations. Neural

Comput  15(5):965–91  2003.

[9] U. T. Eden  L. M. Frank  R. Barbieri  V. Solo  and E. N. Brown. Dynamic analysis of neural encoding by

point process adaptive ﬁltering. Neural Comput  16(5):971–98  2004.

[10] B. M. Yu  A. Afshar  G. Santhanam  S. I. Ryu  K. Shenoy  and M. Sahani. Extracting dynamical structure

embedded in neural activity. In NIPS 18  pages 1545–1552. MIT Press  Cambridge  MA  2006.

[11] J. E. Kulkarni and L. Paninski. Common-input models for multiple neural spike-train data. Network 

18(4):375–407  2007.

[12] W. Truccolo  L. R. Hochberg  and J. P. Donoghue. Collective dynamics in human and monkey sensori-

motor cortex: predicting single neuron spikes. Nat Neurosci  13(1):105–111  2010.

[13] J. H. Macke  L. Buesing  J. P. Cunningham  B. M. Yu  K. V. Shenoy  and M. Sahani. Empirical models of

spiking in neural populations. In NIPS  pages 1350–1358  2011.

[14] C. van Vreeswijk and H. Sompolinsky. Chaos in neuronal networks with balanced excitatory and in-

hibitory activity. Science  274(5293):1724–6  1996.

[15] G. J. Tomko and D. R. Crapper. Neuronal variability: non-stationary responses to identical visual stimuli.

Brain Res  79(3):405–18  1974.

[16] C. D. Brody. Correlations without synchrony. Neural Comput  11(7):1537–51  1999.
[17] R. L. T. Goris  J. A. Movshon  and E. P. Simoncelli. Partitioning neuronal variability. Nat Neurosci 

17(6):858–65  2014.

[18] C. D. Gilbert and W. Li. Adult visual cortical plasticity. Neuron  75(2):250–64  2012.
[19] E. N. Brown  D. P. Nguyen  L. M. Frank  M. A. Wilson  and V. Solo. An analysis of neural receptive ﬁeld

plasticity by point process adaptive ﬁltering. Proc Natl Acad Sci U S A  98(21):12261–6  2001.

[20] Frank et al. Contrasting patterns of receptive ﬁeld plasticity in the hippocampus and the entorhinal cortex:

an adaptive ﬁltering approach. J Neurosci  22(9):3817–30  2002.

[21] N. A. Lesica and G. B. Stanley. Improved tracking of time-varying encoding properties of visual neurons

by extended recursive least-squares. IEEE Trans Neural Syst Rehabil Eng  13(2):194–200  2005.

[22] V. Ventura  C. Cai  and R.E. Kass. Trial-to-Trial Variability and Its Effect on Time-Varying Dependency

Between Two Neurons  2005.

[23] C. S. Quiroga-Lombard  J. Hass  and D. Durstewitz. Method for stationarity-segmentation of spike train

data with application to the pearson cross-correlation. J Neurophysiol  110(2):562–72  2013.

[24] Sch¨olvinck et al. Cortical state determines global variability and correlations in visual cortex. J Neurosci 

35(1):170–8  2015.

[25] Gabriela C.  Uri T. E.  Sylvia W.  Marianna Y.  Wendy A. S.  and Emery N. B. Analysis of between-trial

and within-trial neural spiking dynamics. Journal of Neurophysiology  99(5):2672–2693  2008.

[26] Mangion et al. Online variational inference for state-space models with point-process observations. Neu-

ral Comput  23(8):1967–1999  2011.

[27] Neil C Rabinowitz  Robbe LT Goris  Johannes Ball´e  and Eero P Simoncelli. A model of sensory neural

responses in the presence of unknown modulatory inputs. arXiv preprint arXiv:1507.01497  2015.

[28] C.E. Rasmussen and C.K.I. Williams. Gaussian processes for machine learning. MIT Press Cambridge 

[29] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis  Gatsby Unit  Uni-

[30] Yu et al. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population

MA  USA  2006.

versity College London  2003.

activity. 102(1):614–635  2009.

[31] A. J. Smola  V. Vishwanathan  and E. Eskin. Laplace propagation. In Sebastian Thrun  Lawrence K. Saul 

and Bernhard Sch¨olkopf  editors  NIPS  pages 441–448. MIT Press  2003.

[32] A. Ypma and T. Heskes. Novel approximations for inference in nonlinear dynamical systems using

expectation propagation. Neurocomput.  69(1-3):85–99  2005.

[33] K. V. Shenoy B. M. Yu and M. Sahani. Expectation propagation for inference in non-linear dynamical
models with poisson observations. In Proc IEEE Nonlinear Statistical Signal Processing Workshop  2006.
[34] I. Murray and R. P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models. In

NIPS 23  pages 1723–1731. 2010.

9

,Mijung Park
Gergo Bohner
Jakob Macke
Kwang-Sung Jun
Aniruddha Bhargava
Robert Nowak
Rebecca Willett
Chaosheng Dong
Yiran Chen
Bo Zeng