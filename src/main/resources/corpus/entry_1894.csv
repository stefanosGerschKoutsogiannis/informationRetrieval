2012,Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential _1-Minimization,We consider the problem of recovering a sequence of vectors  $(x_k)_{k=0}^K$  for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$)  based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$  where $A_k$ and $e_k$ denote the measurement matrix and noise  respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program  which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints  recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense  and yet we can achieve exact recovery. In the presence of bounded noise  we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.,Exact and Stable Recovery of Sequences of Signals

with Sparse Increments via Differential

ℓ1-Minimization

Demba Ba1 2  Behtash Babadi1 2  Patrick Purdon2 and Emery Brown1 2

1MIT Department of BCS  Cambridge  MA 02139

2MGH Department of Anesthesia  Critical Care and Pain Medicine

55 Fruit st  GRJ 4  Boston  MA 02114

demba@mit.edu  {behtash patrickp}@nmr.mgh.harvard.edu

enb@neurostat.mit.edu

Abstract

We consider the problem of recovering a sequence of vectors  (xk)K
k=0  for which
the increments xk− xk−1 are Sk-sparse (with Sk typically smaller than S1)  based
k=1  where Ak and ek denote the mea-
on linear measurements (yk = Akxk + ek)K
surement matrix and noise  respectively. Assuming each Ak obeys the restricted
isometry property (RIP) of a certain order—depending only on Sk—we show that
in the absence of noise a convex program  which minimizes the weighted sum
of the ℓ1-norm of successive differences subject to the linear measurement con-
k=1 exactly. This is an interesting result be-
straints  recovers the sequence (xk)K
cause this convex program is equivalent to a standard compressive sensing prob-
lem with a highly-structured aggregate measurement matrix which does not satisfy
the RIP requirements in the standard sense  and yet we can achieve exact recov-
ery. In the presence of bounded noise  we propose a quadratically-constrained
convex program for recovery and derive bounds on the reconstruction error of the
sequence. We supplement our theoretical analysis with simulations and an ap-
plication to real video data. These further support the validity of the proposed
approach for acquisition and recovery of signals with time-varying sparsity.

1 Introduction

In the ﬁeld of theoretical signal processing  compressive sensing (CS) has arguably been one of the
major developments of the past decade. This claim is supported in part by the deluge of research
efforts (see for example Rice University’s CS repository [1]) which has followed the inception of
this ﬁeld [2  3  4]. CS considers the problem of acquiring and recovering signals that are sparse
(or compressible) in a given basis using non-adaptive linear measurements  at a rate smaller than
what the Shannon-Nyquist theorem would require. The work [2  4] derived conditions under which
a sparse signal can be recovered exactly from a small set of non-adaptive linear measurements.
In [3]  the authors propose a recovery algorithm for the case of measurements contaminated by
bounded noise. They show that this algorithm is stable  that is  within a constant of the noise
tolerance. Recovery of these sparse or compressible signals is performed using convex optimization
techniques.

The classic CS setting does not take into account the structure  e.g.
temporal or spatial  of the
underlying high-dimensional sparse signals of interest. In recent years  the attention has shifted to
formulations which incorporate the signal structure into the CS framework. A number of problems
and applications of interest deal with time-varying signals which may not only be sparse at any
given instant  but may also exhibit sparse changes from one instant to the next. For example  a video

1

of a natural scene consists of a sequence of natural images (compressible signals) which exhibits
sparse changes from one frame to the next. It is thus reasonable to hope that one would be able to
get away with far fewer measurements than prescribed by conventional CS theory to acquire and
recover such time-varying signals as videos. The problem of recovering signals with time-varying
sparsity has been referred to in the literature as dynamic CS. A number of empirically-motivated
algorithms to solve the dynamic CS problem have been proposed  e.g. [5  6]. To our knowledge 
no recovery guarantees have been proved for these algorithms  which typically assume that the
support of the signal and/or the amplitudes of the coefﬁcients change smoothly with time. In [5] 
for instance  the authors propose message-passing algorithms for tracking and smoothing of signals
with time-varying sparsity. Simulation results show the superiority of the algorithms compared to
one based on applying conventional CS principles at each time instant. Dynamic CS algorithms
have potential applications to video processing [7]  estimation of sources of brain activity from
MEG time-series [8]  medical imaging [7]  and estimation of time-varying networks [9].

To the best of our knowledge  the dynamic CS problem has not received rigorous  theoretical
scrutiny. In this paper  we develop rigorous results for dynamic CS both in the absence and in
the presence of noise. More speciﬁcally  in the absence of noise  we show that one can exactly re-
cover a sequence (xk)K
k=0 of vectors  for which the increments xk − xk−1 are Sk-sparse  based on
linear measurements yk = Akxk and under certain regularity conditions on (Ak)K
k=1  by solving a
convex program which minimizes the weighted sum of the ℓ1-norms of successive differences. In
the presence of noise  we derive error bounds for a quadratically-constrained convex program for
recovery of the sequence (xk)K
In the following section  we formulate the problem of interest and introduce our notation. In Sec-
tion 3  we present our main theoretical results  which we supplement with simulated experiments
and an application to real video data in Section 4. In this latter section  we introduce probability-of-
recovery surfaces for the dynamic CS problem  which generalize the traditional recovery curves of
CS. We give concluding remarks in Section 5.

k=0.

2 Problem Formulation and Notation

We denote the support of a vector x ∈ Rp by supp(x) = {j : xj 6= 0}. We say that a vector
x ∈ Rp is S-sparse if ||x||0 ≤ S  where ||x||0 := |supp(x)|. We consider the problem of recovering
a sequence (xk)K
k=0 of Rp vectors such that xk − xk−1 is Sk-sparse based on linear measurements
of the form yk = Akxk + ek. Here  Ak ∈ Rnk×p  ek ∈ Rnk and yk ∈ Rnk denote the measurement
matrix  measurement noise  and the observation vector  respectively. Typically  Sk < nk ≪ p 
which accounts for the compressive nature of the measurements. For convenience  we let x0 be the
Rp vector of all zeros.

For the rest of our treatment  it will be useful to introduce some notation. We will be dealing with
sequences (of sets  matrices  vectors)  as such we let the index k denote the kth element of any such
sequence. Let J be the set of indices {1  2 ···   p}. For each k  we denote by {akj : j ∈ J}  the
columns of the matrix Ak and by Hk the Hilbert space spanned by these vectors.
For two matrices A1 ∈ Rn1×p and A2 ∈ Rn2×p  n2 ≤ n1  we say that A2 ⊂ A1 if the rows of A2
are distinct and each row of A2 coincides with a row of A1.
We say that the matrix A ∈ Rn×p satisﬁes the restricted isometry property (RIP) or order S if  for
all S-sparse x ∈ Rp  we have

(1 − δS)||x||2

2 ≤ ||Ax||2

2 ≤ (1 + δS)||x||2
2  

where δS ∈ (0  1) is the smallest constant for which Equation 1 is satisﬁed [2].
Consider the following convex optimization programs

(1)

min

x1 x2 ···  xK

K

X

k=1

||xk − xk−1||1

√Sk

s.t.

yk = Akxk 

k = 1  2 ···   K.

min

x1 x2 ···  xK

K

X

k=1

kxk − xk−1k1

√Sk

s.t. kyk − Akxkk2 ≤ ǫk 

k = 1  2 ···   K.

2

(P1)

(P2)

What theoretical guarantees can we provide on the performance of the above programs for recovery
of sequences of signals with sparse increments  respectively in the absence (P1) and in the presence
(P2) of noise?

3 Theoretical Results

We ﬁrst present a lemma giving sufﬁcient conditions for the uniqueness of sequences of vectors with
sparse increments given linear measurements in the absence of noise. Then  we prove a theorem
which shows that  by strengthening the conditions of this lemma  program (P1) can exactly recover
every sequence of vectors with sparse increments. Finally  we derive error bounds for program (P2)
in the context of recovery of sequences of vectors with sparse increments in the presence of noise.
Lemma 1 (Uniqueness of Sequences of Vectors with Sparse Increments).
Suppose (Sk)K
k=0 is such that S0 = 0  and for each k  Sk ≥ 1. Let Ak satisfy the RIP of order 2Sk.
Let xk ∈ Rp supported on Tk ⊆ J be such that ||xk − xk−1||0 ≤ Sk  for k = 1  2 ···   K. Suppose
T0 = ∅ without loss of generality (w.l.o.g.). Then  given Ak and yk = Akxk  the sequence of sets
(Tk)K

k=1  and consequently the sequence of coefﬁcients (xk)K

k=1  can be reconstructed uniquely.

Proof. For brevity  and w.l.o.g.  we prove the lemma for K = 2. We prove that there is a unique
choice of x1 and x2 such that ||x1 − x0||0 ≤ S1  ||x2 − x1||0 ≤ S2 and obeying y1 = A1x1  y2 =
2 6= x2 supported
A2x2. We proceed by contradiction   and assume that there exist x′
1 − x0||0 ≤ S1 
2  respectively  such that y1 = A1x1 = A1x′
1 and T ′
on T ′
1||0 ≤ S2. Then ||A1(x1 − x′
and ||x′
1)||2 = 0. Using the lower bound in the RIP of A1 and the
2 − x′
1||2
1  thus contradicting our assumption
2 = 0  i.e. x1 = x′
fact that δ2S1 < 1  this leads to ||x1 − x′
1. Now consider the case of x2 and x′
that x1 6= x′
0 = A2(x2 − x′

2) = A2(x2 − x1 + x1 − x′

1 6= x1 and x′
2  ||x′

2) = A2(x2 − x1 + x′

1  y2 = A2x2 = A2x′

2. We have

(2)

1 − x′
2).
that δ2S2 < 1 

2||2

1 − x′

2 − x′

1  which implies x′

2 = 0  i.e. x2 − x1 = x′

this leads to
2 = x2  thus contradict-

Using the lower bound in the RIP of A2 and the fact
||x2 − x1 + x′
ing our assumption that x2 6= x′
2.
As in Cand`es and Tao’s work [2]  this lemma only suggests what may be possible in terms of
k=1 through a combinatorial  brute-force approach. By imposing stricter conditions
recovery of (xk)K
on (δ2Sk )K
k=1 by solving a convex program. This is summarized in the
following theorem.
Theorem 2 (Exact Recovery in the Absence of Noise).
k=1 ∈ Rp be a sequence of Rp vectors such that  for each k  ||¯xk − ¯xk−1||0 ≤ Sk for some
Let (¯xk)K
Sk < p/2. Suppose that the measurements yk = Ak ¯xk ∈ Rnk are given  such that nk < p  A1 ⊃
A2  Ak = A2 for k = 3 ···   K and (Ak)K
k=1 satisﬁes δSk + δ2Sk + δ3Sk < 1 for k = 1  2 ···   K.
Then  the sequence (¯xk)K

k=1 is the unique minimizer to the program (P1).

k=1  we can recover (xk)K

Proof. As before  we consider the case K = 2. The proof easily generalizes to the case of arbitrary
K. We can re-write the program as follows:

min
x1 x2

||x1||1√S1

+ ||x2 − x1||1

√S2

s.t.

A1x1 = A1 ¯x1  A2(x2 − x1) = A2(¯x2 − ¯x1) 

(3)

1 and x∗

2 be the solutions to the above program. Let T1 = supp(¯x1) and ∆T2 = supp(¯x2− ¯x1).

where we have used the fact that A1 ⊃ A2: A2x2 − A1x1 = A2 ¯x2 − A1 ¯x1  which implies A2(x2 −
x1) = A2(¯x2 − ¯x1).
Let x∗
Assume |T1| ≤ S1 and |∆T2| ≤ S2.
Key element of the proof: The key element of the proof is the existence of vectors u1  u2 sat-
isfying the exact reconstruction property (ERP) [10  11].
It has been shown in [10] that given
δSk + δ2Sk + δ3Sk < 1 for k = 1  2:

3

1. hu1  a1ji = sgn(x1 j )  for all j ∈ T1  and hu2  a2ji = sgn(x2 j )  for all j ∈ ∆T2.
2. |hu1  a1ji| < 1  for all j ∈ T c

1   and |hu2  a2ji| < 1  for all j ∈ ∆T c
2 .

Since ¯x1 and ¯x2 − ¯x1 are feasible  we have
2 − x∗
√S2

||x∗
1||1√S1

+ ||x∗

1||1

≤ ||¯x1||1√S1

+ ||¯x2 − ¯x1||1

√S2

.

(4)

||x∗
1||1√S1

+ ||x∗

1||1

=

1

√S1 X

j∈T1

|¯x1 j + (x∗

|¯x2 j − ¯x1 j + (cid:0)x∗

2 j − x∗

|x∗
2 j − x∗
1 j|

1

2 − x∗
√S2
√S2 X
√S1 X

1

j∈T1

j∈∆T2

1

√S2 X

j∈∆T2

j∈∆T c
2

1

√S2 X
√S1 X

1

j∈T1

1

√S2 X

j∈∆T2

+

≥

+

+

=

+

1

1 j − ¯x1 j )| +

√S1 X
1 j − (¯x2 j − ¯x1 j )(cid:1)| +

j∈T c
1

|x∗
1 j|
1
√S2 X

j∈∆T c
2

1

√S1 X

j∈T c
1

x∗
1 jhu1  a1ji

2 j − x∗

1 j − (¯x2 j − ¯x1 j)))

sgn(¯x1 j )
}
|

hu1 a1j i

(¯x1 j + (x∗

1 j − ¯x1 j)) +

{z
(¯x2 j − ¯x1 j + (x∗
sgn(¯x2 j − ¯x1 j)
|
}
(x∗

hu2 a2j i

{z
2 j − x∗
1 j)hu2  a2ji
1
√S1hu1 X
}
|
√S2hu2 X
|

x∗
1 ja1j

{z

A1x∗
1

j∈J

j∈J

1

|¯x2 j − ¯x1 j| +

|¯x1 j| +

¯x1 ja1j

j∈T1

− X
{z
|
(x∗
2 j − x∗

A1 ¯x1

i
}
1 j)a2j

A2(x∗

2 −x∗
1)

{z

− X
|

j∈∆T2

i
(¯x2 j − ¯x1 j )a2j
}
(5)

A2(¯x2−¯x1)

{z

}

This implies that all of the inequalities in the derivation above must in fact be equalities. In particular 

= ||¯x1||1√S1

+ ||¯x2 − ¯x1||1

.

√S2

1

√S1 X

j∈T c
1

|x∗
1 j| +

=

≤

j∈∆T c
2

1

1

√S2 X
√S1 X
√S1 X

j∈T c
1

1

j∈T c
1

x∗
1 jhu1  a1ji +

(x∗

2 j − x∗

1 j )hu2  a2ji

|x∗
2 j − x∗
1 j|

1

√S2 X
√S2 X

j∈∆T c
2
1

+

j∈∆T c
2

|x∗
1 j||hu1  a1ji|
}

|
1 j = 0 ∀j ∈ ∆T c

{z

<1

2 j − x∗
|x∗

1 j||hu2  a2ji|
}

{z

|

<1

.

Therefore  x∗
A1 and A2 leads to

1 j = 0 ∀j ∈ T c

0 = ||A2(x∗
1 = ¯x1  and x∗

so that x∗

2 . Using the lower bounds in the RIP of

1   and x∗

2 j − x∗
1 − ¯x1)||2 ≥ (1 − δ2S1)||x∗
1 − (¯x2 − ¯x1))||2 ≥ (1 − δ2S2)||x∗

0 = ||A1(x∗
2 − x∗
2 = ¯x2. Uniqueness follows from simple convexity arguments.

1 − ¯x1||2
2 − x∗

1 − (¯x2 − ¯x1)||2  

(6)
(7)

A few remarks are in order. First  Theorem 2 effectively asserts that the program (P1) is equivalent
0 the vector
to sequentially solving (i.e. for k = 1  2 ···   K) the following program  starting with x∗
of all zeros in Rp:

min

xk (cid:12)(cid:12)(cid:12)(cid:12)xk − x∗

k−1(cid:12)(cid:12)(cid:12)(cid:12)1

s.t.

yk − Akx∗

k−1 = Ak(xk − x∗

k−1) 

k = 1  2 ···   K.

(8)

4

Second  it is interesting and surprising that Theorem 2 would hold  if one naively applies standard
CS principles to our problem. To see this  if we let wk = xk − xk−1  then program (P1) becomes

min

w1 ···  wK

K

X

k=1

||wk||1√Sk

s.t.

y = Aw 

(9)

where w = (w′

1 ···   w′

1 ···   y′

K )′ ∈ RK×p  y = (y′
A1
0
0
0
A2 A2
...
...
...
AK AK ··· AK

K)′ ∈ RPK
k=1 nk and A is given by

···
···

...




A =

.

As K grows large  the columns of A become increasingly correlated or coherent  which intuitively
means that A would be far from satisfying RIP of any order. Yet  we get exact recovery. This is an
important reminder that the RIP is a sufﬁcient  but not necessary condition for recovery.
Third  the assumption that A1 ⊃ A2  Ak = A2 for k = 3 ···   K makes practical sense as it
allows one to avoid the prohibitive storage and computational cost of generating several distinct
measurement matrices. Note that if a random A1 satisﬁes the RIP of some order and A1 ⊃ A2  then
A2 also satisﬁes the RIP (of lower order).
Lastly  the key advantage of dynamic CS recovery (P1) is the smaller number of measurements
required compared to the classical approach [2] which would solve K separate ℓ1-minimization
problems. For each k = 1 ···   K  one would require nk ≥ CSk log(p/Sk) measurements for
dynamic recovery  compared to nk ≥ CS1 log(p/S1) for classical recovery. Due to the hypothesis
of Sk ≤ S1 ≪ p  i.e.  the sparse increments are small  we conclude that there are less number of
measurements required for dynamic CS.

We now move to the case where the measurements are perturbed by bounded noise. More specif-
ically  we derive error bounds for a quadratically-constrained convex program for recovery of se-
quences of vectors with sparse increments in the presence of noise.
Theorem 3 (Conditionally Stable Recovery in Presence of Noise).
Let (¯xk)K
the measurements yk = Akxk + ek ∈ Rnk are given such that ||ek||2 ≤ ǫk and (Ak)K
δ3Sk + 3δ4Sk < 2  for each k. Let (x∗
(x∗
Then  we have:

k=1 ∈ Rp be as stated in Theorem 2  and x0 be the vector of all zeros in Rp. Suppose that
k=1 satisfy
k=1 be the solution to the program (P2). Finally  let hk :=
0 := 0 ∈ Rp.

k−1) − (¯xk − ¯xk−1)  for k = 1  2 ···   K  with the convention that ¯x0 := x∗

k − x∗

k)K

K

X

k=1

khkk2 ≤

K

X

k=1

2CSk ǫk +

K

X

k=2

CSk

Ak X

ℓ<k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
hℓ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where  for each k = 1  2 ···   K  CSk is only a function of δ3Sk and δ4Sk.

(10)

Proof sketch. Cand`es et al.’s proof for stable recovery in the presence of bounded noise relies on
the so-called tube and cone constraints [3]. Our proof for Theorem 3 relies on generalization of
these two constraints. We omit some of the algebraic details of the proof as they can be ﬁlled in by
following the proof of [3] for the time-invariant case.

Generalized tube constraint: Let ¯wk = ¯xk − ¯xk−1  w∗
generalized tube constraints are obtained using a simple application of the triangle inequality:

k−1  for k = 1 ···   K. The

k − x∗

k = x∗

||A1( ¯w1 − w∗
||A2( ¯w2 − w∗
||Ak( ¯wk − w∗

1)||2 ≤ 2ǫ1
2)||2 ≤ 2ǫ2 + ||A2h1||2 and more generally 

k)||2 ≤ 2ǫk + (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Ak X

ℓ<k

hℓ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

  for k = 2 ···   K.

5

(11)
(12)

(13)

Generalized cone constraint: To obtain a generalization of the cone constraint in [3]  we need
k=1 (may) have different support sizes. The
to account for the fact that the increments (xk − xk−1)K
resulting generalized cone constraint is as follows:

K

X

k=1

k(cid:12)(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)(cid:12)hk∆T c
√Sk

≤

K

X

k=1

||hk∆Tk||1

√Sk

 

(14)

where ∆Tk = supp(¯xk − ¯xk−1). The proof proceeds along the lines of that presented in [3]  with
CSk =

1+√1/3

.

√1−δ4Sk −q 1+δ3Sk

3

Equation (10) is an implicit bound: the second term in the inequality reﬂects the fact that  for a
given k  the error x∗
k − ¯xk depends on previous errors. Our bound proves a form of stability that
is conditional on the stability of previous estimates.The appeal of dynamic CS comes from the fact
that one may pick the constants CSk in the bound above to be much smaller that those from the
corresponding conventional CS bound [3] (Equation (10) without the second term). This ensures
that the errors do not propagate in an unbounded manner. One may obtain sharper bounds using
techniques as in [12]. In the next section  we use simulations to compare explicitly the average
mean-squared error (MSE) of conventional CS and our algorithm.

4 Experiments/Simulations

We ran a series of numerical experiments to assess the ability of the convex programs introduced
to recover signals with time-varying sparsity. In the absence of noise  the experiments result in
probability-of-recovery surfaces for the dynamic CS problem  which generalize the traditional re-
covery curves of CS. In the presence of noise  we compare dynamic CS to conventional CS in terms
of their reconstruction error as a function of signal-to-noise-ratio (SNR). We also show an applica-
tion to real video data. All optimization problems were solved using CVX  a package for specifying
and solving convex programs [13  14].

4.1 Simulated noiseless data

Experimental set-up:

independent Gaussian entries  for k = 1  2 ···   K.

1. Select nk  for k = 1 ···   K  and p  so that the Ak’s are nk × p matrices; sample Ak with
2. Select S1 = ⌈s1 · p⌉  s1 ∈ (0  1)  and Sk = ⌈s2 · p⌉  s2 ∈ (0  1)  for k = 2 ···   K.
3. Select T1 of size S1 uniformly at random and set ¯x1 j = 1 for all j ∈ T1  and 0 otherwise;
for k = 2 ···   K  select ∆Tk = supp(¯xk − ¯xk−1) of size Sk uniformly at random and set
¯xk j − ¯xk−1 j = 1 for all j ∈ ∆Tk  and 0 otherwise.

4. Make yk = Ak ¯xk  for k = 1  2 ···   K; solve the program (P1) to obtain (x∗
5. Compare (¯xk)K
6. Repeat 100 times for each (s1  s2).

k=1 to (x∗

k=1.

k)K

k)K

k=1.

We compare dynamic CS to conventional CS applied independently at each k. Figure 1 shows
results for nk = 100  p = 200  and K = 2. We can infer the expected behavior for larger values of
K from the case K = 2 and from the theory developed above (see remarks below).
The probability of recovery for conventional CS is 1 on the set {(s1  s2) : s1 + (K − 1)s2 ≤
s∗}  and 0 on its complement  where s∗ is the sparsity level at which a phase transition occurs in
the conventional CS problem [2]. The ﬁgure shows that  when the measurement matrices Ak  for
k = 2 ···   K are derived from A1 as assumed in Theorem 1  dynamic CS (DCS 1) outperforms
conventional CS (CCS). However  when we used different measurement matrices (DCS 2)  we see
that there is an asymmetry between s1 and s2  which is not predicted by our Theorem 1. Intuitively 
this is because for small s2  the program (P1) operates in a regime where we have not only one
but multiple measurements to recover a given sparse vector [15]. Program (P1) is equivalent to
sequential CS. Therefore  we expect the behavior of conventional CS to persist for larger K.

6

CC S

DC S 1

DC S 2

1
s

0.4

0.3

0.2

0.1

 0

0.1 0.2 0.3 0.4

s2

1
s

0.4

0.3

0.2

0.1

 0

0.1 0.2 0.3 0.4

s2

1
s

0.4

0.3

0.2

0.1

 0

0.1 0.2 0.3 0.4

s2

Figure 1: Probability of recovery maps as a function of s1 and s2.

4.2 Simulated noisy data

The experimental set-up differs slightly from the one of the noiseless case. In Step 2  we ﬁx constant
values for S1 and Sk  k = 2 ···   K. Moreover  in Step 4  we form yk = Akxk + ek  where the ek’s
are drawn uniformly in (−α  α). In Step 6  we repeat the experiment 100 times for each α. In our
experiments  we used n1 = 100  S1 = 5  n2 = 20  Sk = 1  for k = 2 ···   K  and p = 200. We
report results for K = 2 and K = 10  and choose values of α resulting in SNRs in the range [5  30]
dB  in increments of 5 dB.
Figure 2 displays the average MSE given by 10 · log10( 1
2) of conventional CS
and dynamic CS as a function of SNR. The Figure shows that the proposed algorithm outperforms
conventional CS  and is robust to noise.

k=1 ||¯xk − x∗

K PK

k||2

)
B
d
(
E
S
M

4

2

0

−2

−4

−6

−8
 
5

Average MSE  K = 2

 

Conventional CS
Dynamic CS

10

15
20
SNR (dB)

25

30

)
B
d
(
E
S
M

6

4

2

0

−2

−4

−6
5

Average MSE  K = 10

10

15
20
SNR (dB)

25

30

Figure 2: Average MSE as a function of SNR.

4.3 Real video data

We consider the problem of recovering the ﬁrst 10 frames of a real video using our dynamic CS
algorithm  and conventional CS applied to each frame separately. In both cases  we assume the
absence of noise. We use a video portraying a close-up of a woman engaged in a telephonic conver-
sation [16]. The video has a frame rate of 12 Hz and a total of 150 frames  each of size 176 × 144.
Due to computational constraints  we downsampled each frame by a factor of 3 in each dimension.
We obtained measurements in the wavelet domain by performing a two-level decomposition of each
frame using Daubechies-1 wavelet.
k||2
In Table 1  we report the negative of the normalized MSE given by −10· log10( 1
)
in dB for various (n1  n2) measurement pairs (nk = n2  for k = 3 ···   10). Larger numbers indi-
cate better reconstruction accuracy. The table shows that  for all (n1  n2) considered  dynamic CS
outperforms conventional CS. The average performance gap across (n1  n2) pairs is approximately
7 dB. Interestingly  for sufﬁcient number of measurements  dynamic CS improves as the video pro-
gresses. We observed this phenomenon in the small-s2 regime of the simulations. Figure 3 shows
the reconstructed frames highlighted in Table 1. The frames reconstructed using dynamic CS are
more appealing visually than their conventional CS counterparts.

10 P10

||¯xk−x∗
||¯xk||2
2

k=1

2

7

Table 1: Normalized negated MSE in dB for frames 1  5  10  and average over all 10 frames. Each
frame consist of ≈ 3000 pixels. Each row of the table corresponds to a different (n1  n2) pair (refer
to text). Larger numbers indicate better reconstruction accuracy.

Frame 1

CCS DCS
27.8
27.8
22.4
22.4
27.8
27.8
19.1
19.1
19.2
19.2

Frame 5

CCS DCS
38
28.5
31.3
22.3
15.2
24.2
25
18.9
8.4
17.6

Frame 10

CCS DCS
41.1
28
35.6
22.9
14.8
25.4
29.7
19.8
9.3
16.7

Avg. (10 frames)
CCS
28.2
22.8
15.9
19.1
8.4

DCS
35
28.9
25.5
24.1
17.8

(2400 2400)
(2000 2000)
(2400 1200)
(1600 1600)
(1600 800)

Frame 1

Frame 5

Frame 10

l

a
n
g
i
r

i

O

S
C
C

S
C
D

Figure 3: Comparison of frames reconstructed using dynamic CS and conventional CS  (n1  n2) =
(2000  2000).

5 Discussion

In this paper  we proved rigorous guarantees for convex programs for recovery of sequences of vec-
tors with sparse increments  both in the absence and in the presence of noise. Our formulation of
the dynamic CS problem is more general than the empirically-motivated solutions proposed in the
literature  e.g. [5  6]. Indeed  we only require that x1 is sparse  as well as the increments. Therefore 
there may exist values of k such that xk is not a sparse vector. We supplemented our theoretical
analysis with simulation experiments and an application to real video data. In the noiseless case  we
introduced probability-of-recovery surfaces which generalize traditional CS recovery curves. The
recovery surface showed that dynamic CS signiﬁcantly outperforms conventional CS  especially for
large sequences (large K). In the noisy case  simulations showed that dynamic CS also outperforms
conventional CS for SNR values ranging from 5 to 30 dB. Our results on real video data demon-
strated that dynamic CS outperforms conventional CS in terms of visual appeal of the reconstructed
frames  and by an average MSE gap of 7dB.

8

References

[1] Compressive

sensing

resources 

http://dsp.rice/edu/cs/.

rice

university.

Rice

University 

[2] E.J. Cand`es and T. Tao. Decoding by linear programming. Information Theory  IEEE Trans-

actions on  51(12):4203–4215  2005.

[3] E.J. Cand`es  J.K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate

measurements. Communications on pure and applied mathematics  59(8):1207–1223  2006.

[4] D.L. Donoho. Compressed sensing. Information Theory  IEEE Transactions on  52(4):1289–

1306  2006.

[5] J. Ziniel  L.C. Potter  and P. Schniter. Tracking and smoothing of time-varying sparse signals
via approximate belief propagation. In Signals  Systems and Computers (ASILOMAR)  2010
Conference Record of the Forty Fourth Asilomar Conference on  pages 808–812. IEEE  2010.
[6] M. Salman Asif and J. Romberg. Dynamic updating for ℓ1-minimization. Selected Topics in

Signal Processing  IEEE Journal of  4(2):421–434  2010.

[7] H. Jung and J.C. Ye. Motion estimated and compensated compressed sensing dynamic mag-
netic resonance imaging: What we can learn from video compression techniques. International
Journal of Imaging Systems and Technology  20(2):81–98  2010.

[8] J.W. Phillips  R.M. Leahy  and J.C. Mosher. Meg-based imaging of focal neuronal current

sources. Medical Imaging  IEEE Transactions on  16(3):338–348  1997.

[9] M. Kolar  L. Song  A. Ahmed  and E.P. Xing. Estimating time-varying networks. The Annals

of Applied Statistics  4(1):94–123  2010.

[10] E. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. IEEE Trans. Inform. Theory  June 2004.
Submitted.

[11] E. Cand`es and T. Tao. Near optimal signal recovery from random projections: Universal

encoding strategies? IEEE Trans. Inform. Theory  October 2004. Submitted.

[12] E.J. Cand`es. The restricted isometry property and its implications for compressed sensing.

Comptes Rendus Mathematique  346(9):589–592  2008.

[13] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming  version

1.22. http://cvxr.com/cvx  May 2012.

[14] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In V. Blondel 
S. Boyd  and H. Kimura  editors  Recent Advances in Learning and Control  Lecture Notes in
Control and Information Sciences  pages 95–110. Springer-Verlag Limited  2008.

[15] S.F. Cotter  B.D. Rao  K. Engan  and K. Kreutz-Delgado. Sparse solutions to linear in-
verse problems with multiple measurement vectors. Signal Processing  IEEE Transactions
on  53(7):2477–2488  2005.

[16] Softage

video

Softage 
http:www.softage.ru/products/video-codec/uncompressed/suzie.avi.

download

codec

demo

page.

9

,David Eigen
Rob Fergus
Kevin Roth
Aurelien Lucchi
Sebastian Nowozin
Thomas Hofmann
Kuang Xu