2016,More Supervision  Less Computation: Statistical-Computational Tradeoffs in Weakly Supervised Learning,We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability $1-\alpha$. Although there exist numerous algorithms for this problem  it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision  which is quantified by $\alpha$. In this paper  we characterize the effect of $\alpha$ by establishing the information-theoretic and computational boundaries  namely  the minimax-optimal statistical accuracy that can be achieved by all algorithms  and polynomial-time algorithms under an oracle computational model. For small $\alpha$  our result shows a gap between these two boundaries  which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly  we also show that this gap narrows as $\alpha$ increases. In other words  having more supervision  i.e.  more correct labels  not only improves the optimal statistical accuracy as expected  but also enhances the computational efficiency for achieving such accuracy.,More Supervision  Less Computation:

Statistical-Computational Tradeoffs in Weakly

Supervised Learning

Xinyang Yi†∗ Zhaoran Wang‡∗ Zhuoran Yang‡∗ Constantine Caramanis† Han Liu‡

†The University of Texas at Austin

‡Princeton University

†{yixy constantine}@utexas.edu

‡{zhaoran zy6 hanliu}@princeton.edu

{∗: equal contribution}

Abstract

We consider the weakly supervised binary classiﬁcation problem where the labels
are randomly ﬂipped with probability 1 − α. Although there exist numerous al-
gorithms for this problem  it remains theoretically unexplored how the statistical
accuracies and computational efﬁciency of these algorithms depend on the degree
of supervision  which is quantiﬁed by α. In this paper  we characterize the effect of
α by establishing the information-theoretic and computational boundaries  namely 
the minimax-optimal statistical accuracy that can be achieved by all algorithms 
and polynomial-time algorithms under an oracle computational model. For small
α  our result shows a gap between these two boundaries  which represents the com-
putational price of achieving the information-theoretic boundary due to the lack of
supervision. Interestingly  we also show that this gap narrows as α increases. In
other words  having more supervision  i.e.  more correct labels  not only improves
the optimal statistical accuracy as expected  but also enhances the computational
efﬁciency for achieving such accuracy.

Introduction

i=1  we observe {(xi  yi)}n

1
Practical classiﬁcation problems usually involve corrupted labels. Speciﬁcally  let {(xi  zi)}n
i=1 be
n independent data points  where xi ∈ Rd is the covariate vector and zi ∈ {0  1} is the uncorrupted
label. Instead of observing {(xi  zi)}n
i=1 in which yi is the corrupted label.
In detail  with probability (1− α)  yi is chosen uniformly at random over {0  1}  and with probability
α  yi = zi. Here α ∈ [0  1] quantiﬁes the degree of supervision: a larger α indicates more supervision
since we have more uncorrupted labels in this case. In this paper  we are particularly interested in the
effect of α on the statistical accuracy and computational efﬁciency for parameter estimation in this
problem  particularly in the high dimensional settings where the dimension d is much larger than the
sample size n.
There exists a vast body of literature on binary classiﬁcation problems with corrupted labels. In
particular  the study of randomly perturbed labels dates back to [1] in the context of random clas-
siﬁcation noise model. See  e.g.  [12  20] for a survey. Also  classiﬁcation problems with missing
labels are also extensively studied in the context of semi-supervised or weakly supervised learning
by [14  17  21]  among others. Despite the extensive study on this problem  its information-theoretic
and computational boundaries remain unexplored in terms of theory. In a nutshell  the information-
theoretic boundary refers to the optimal statistical accuracy achievable by any algorithms  while the
computational boundary refers to the optimal statistical accuracy achievable by the algorithms under
a computational budget that has a polynomial dependence on the problem scale (d  n). Moreover 
it remains unclear how these two boundaries vary along with α. One interesting question to ask is
29th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

how the degree of supervision affects the fundamental statistical and computational difﬁculties of
this problem  especially in the high dimensional regime.
In this paper  we sharply characterize both the information-theoretic and computational boundaries
of the weakly supervised binary classiﬁcation problems under the minimax framework. Speciﬁcally 
we consider the Gaussian generative model where X|Z = z ∼ N (µz  Σ) and z ∈ {0  1} is the
true label. Suppose {(xi  zi)}n
i=1 are
generated from {zi}n
i=1 in the aforementioned manner. We focus on the high dimensional regime 
where d � n and µ1 − µ0 is s-sparse  i.e.  µ1 − µ0 has s nonzero entires. We are interested in
estimating µ1 − µ0 from the observed samples {(xi  yi)}n
i=1. By a standard reduction argument [24] 
the fundamental limits of this estimation task are captured by a hypothesis testing problem  namely 
H0 : µ1 − µ0 = 0 versus H1 : µ1 − µ0 is s-sparse and

i=1 are n independent samples of (X  Z). We assume that {yi}n

(1.1)
where γn denotes the signal strength that scales with n. Consequently  we focus on studying the
fundamental limits of γn for solving this hypothesis testing problem.

(µ1 − µ0)�Σ−1(µ1 − µ0) := γn > 0 

s log d

2n � 

�

�

�n = o�� s2
�n = ��� s log d

n

n

�

s log d

2n �

�

�n

E�cient

�n = ��� s2

n

�

s log d

2n �

�

Intractable

Impossible

�n = o�� s log d

n

�

0

�

1

s log d

2n �

�

Figure 1: Computational-statistical phase transitions for weakly supervised binary classiﬁcation. Here
α denotes the degree of supervision  i.e.  the label is corrupted to be uniformly random with probabil-
ity 1 − α  and γn is the signal strength  which is deﬁned in (1.1). Here a ∧ b denotes min{a  b}.
Our main results are illustrated in Figure 1. Speciﬁcally  we identify the impossible  intractable  and
efﬁcient regimes for the statistical-computational phase transitions under certain regularity condi-
tions.

solving the hypothesis testing problem.

oracle complexity that is asymptotically powerful in solving the testing problem.

tractable algorithm that has a polynomial oracle complexity under an extension of the statistical query
model [18] is asymptotically powerless. We will rigorously deﬁne the computational model in §2.

(i) For γn = o[�s log d/n ∧ (1/α2 · s log d/n)]  any algorithm is asymptotically powerless in
(ii) For γn = Ω[�s log d/n ∧ (1/α2 · s log d/n)] and γn = o[�s2/n ∧ (1/α2 · s log d/n)]  any
(iii) For γn = Ω[�s2/n ∧ (1/α2 · s log d/n)]  there is an efﬁcient algorithm with a polynomial
Here �s log d/n ∧ (1/α2 · s log d/n) gives the information-theoretic boundary  while �s2/n ∧
(1/α2 · s log d/n) gives the computational boundary. Moreover  by a reduction from the estimation
problem to the testing problem  these boundaries for testing imply the ones for estimating µ2 − µ1
as well.
Consequently  there exists a signiﬁcant gap between the computational and information-theoretic
boundaries for small α. In other word  to achieve the information-theoretic boundary  one has to
pay the price of intractable computation. As α tends to one  this gap between computational and
information-theoretic boundaries narrows and eventually vanishes. This indicates that  having more
supervision not only improves the statistical accuracy  as shown by the decay of information-theoretic
boundary in Figure 1  but more importantly  enhances the computational efﬁciency by reducing
the computational price for attaining information-theoretic optimality. This phenomenon — “more
supervision  less computation” — is observed for the ﬁrst time in this paper.

1.1 More Related Work  Our Contribution  and Notation

Besides the aforementioned literature on weakly supervised learning and label corruption  our work
is also connected to a recent line of work on statistical-computational tradeoffs [2–5  8  13  15  19 
26–28]. In comparison  we quantify the statistical-computational tradeoffs for weakly supervised
learning for the ﬁrst time. Furthermore  our results are built on an oracle computational model

2

in [8] that slightly extends the statistical query model [18]  and hence do not hinge on unproven
conjectures on computational hardness like planted clique. Compared with our work  [8] focuses
on the computational hardness of learning heterogeneous models  whereas we consider the interplay
between supervision and statistical-computational tradeoffs. A similar computational model is used
in [27] to study structural normal mean model and principal component analysis  which exhibit
different statistical-computational phase transitions. In addition  our work is related to sparse linear
discriminant analysis and two-sample testing of sparse means  which correspond to our special cases
of α = 1 and α = 0  respectively. See  e.g.  [7  23] for details. In contrast with their results  our
results capture the effects of α on statistical and computational tradeoffs.
In summary  the contribution of our work is two-fold:

(i) We characterize the computational and statistical boundaries of the weakly supervised binary
classiﬁcation problem for the ﬁrst time. Compared with existing results for other models  our results
do not rely on unproven conjectures.
(ii) Based on our theoretical characterization  we propose the “more supervision  less computation”
phenomenon  which is observed for the ﬁrst time.

Notation. We denote the χ2-divergence between two distributions P  Q by Dχ2 (P  Q). For two
nonnegative sequences an  bn indexed by n  we use an = o(bn) as a shorthand for limn→∞ an/bn =
0. We say an = Ω(bn) if an/bn ≥ c for some absolute constant c > 0 when n is sufﬁciently large.
We use a ∨ b and a ∧ b to denote max{a  b} and min{a  b}  respectively. For any positive integer k 
we denote {1  2  . . .   k} by [k]. For v ∈ Rd  we denote by �v�p the �p-norm of v. In addition  we
denote the operator norm of a matrix A by |||A|||2.
2 Background

In this section  we formally deﬁne the statistical model for weakly supervised binary classiﬁcation.
Then we follow it with the statistical query model that connects computational complexity and
statistical optimality.

2.1 Problem Setup

Consider the following Gaussian generative model for binary classiﬁcation. For a random vector

X ∈ Rd and a binary random variable Z ∈ {0  1}  we assume

X|Z = 0 ∼ N (µ0  Σ)  X|Z = 1 ∼ N (µ1  Σ) 

(2.1)

where P(Z = 0) = P(Z = 1) = 1/2. Under this model  the optimal classiﬁer by Bayes rule
corresponds to the Fisher’s linear discriminative analysis (LDA) classiﬁer. In this paper  we focus
on the noisy label setting where true label Z is replaced by a uniformly random label in {0  1} with
probability 1− α. Hence  α characterizes the degree of supervision in the model. In speciﬁc  if α = 0 
we observe the true label Z  thus the problem belongs to supervised learning. Whereas if α = 1 
the observed label is completely random  which contains no information of the model in (2.1). This
setting is thus equivalent to learning a Gaussian mixture model  which is an unsupervised problem.
In the general setting with noisy labels  we denote the observed label by Y   which is linked to the
true label Z via

(2.2)
We consider the hypothesis testing problem of detecting whether µ0 �= µ1 given n i.i.d. samples
{yi  xi}n

P(Y = Z) = (1 + α)/2  P(Y = 1 − Z) = (1 − α)/2.

i=1 of (Y  X)  namely

H0 : µ0 = µ1 versus H1 : µ0 �= µ1.

(2.3)
We focus on the high dimensional and sparse regime  where d � n and µ0 − µ1 is s-sparse  i.e. 
µ0 − µ1 ∈ B0(s)  where B0(s) := {µ ∈ Rd : �µ�0 ≤ s}. Throughout this paper  use the sample
size n to drive the asymptotics. We introduce a shorthand notation θ := (µ0  µ1  Σ  α) to represent
the parameters of the aforementioned model. Let Pθ be the joint distribution of (Y  X) under our
statistical model with parameter θ  and Pn
θ be the product distribution of n i.i.d. samples accordingly.
We denote the parameter spaces of the null and alternative hypotheses by G0 and G1 respectively. For
any test function φ : {(yi  xi)}n
i=1 → {0  1}  the classical testing risk is deﬁned as the summation of

3

type-I and type-II errors  namely

Rn(φ;G0 G1) := sup

θ∈G0

Pn

θ (φ = 1) + sup
θ∈G1

Pn

θ (φ = 0).

The minimax risk is deﬁned as the smallest testing risk of all possible test functions  that is 

R∗
n(G0 G1) := inf

φ

Rn(φ;G0 G1) 

(2.4)

where the inﬁmum is taken over all measurable test functions.
Intuitively  the separation between two Gaussian components under H1 and the covariance matrix Σ
together determine the hardness of detection. To characterize such dependence  we deﬁne the signal-

to-noise ratio (SNR) as ρ(θ) := (µ0− µ1)�Σ−1(µ0− µ1). For any nonnegative sequence {γn}n≥1 
let G1(γn) := {θ : ρ(θ) ≥ γn} be a sequence of alternative parameter spaces with minimum
separation γn. The following minimax rate characterizes the information-theoretic limits of the
detection problem.

Deﬁnition 2.1 (Minimax rate). We say a sequence {γ∗
• For any sequence {γn}n≥1 satisfying γn = o(γ∗
• For any sequence {γn}n≥1 satisfying γn = Ω(γ∗

n}n≥1 is a minimax rate if

n)  we have limn→∞ R∗
n)  we have limn→∞ R∗

n[G0 G1(γn)] = 1;
n[G0 G1(γn)] = 0.

The minimax rate in Deﬁnition 2.1 characterizes the statistical difﬁculty of the testing problem. How-
ever  it fails to shed light on the computational efﬁciency of possible testing algorithms. The reason
is that this concept does not make any computational restriction on the test functions. The minimax
risk in (2.4) might be attained only by test functions that have exponential computational complex-
ities. This limitation of Deﬁnition 2.1 motivates us to study statistical limits under computational
constraints.

2.2 Computational Model

Statistical query models [8–11  18  27] capture computational complexity by characterizing the total
number of rounds an algorithm interacts with data. In this paper  we consider the following statistical
query model  which admits bounded query functions but allows the responses of query functions to
be unbounded.

Deﬁnition 2.2 (Statistical query model). In the statistical query model  an algorithm A is allowed
to query an oracle T rounds  but not to access data {(yi  xi)}n
i=1 directly. At each round  A queries
the oracle r with a query function q ∈ QA   in which QA ⊆ {q : {0  1}× Rd → [−M  M ]} denotes
the query space of A . The oracle r outputs a realization of a random variable Zq ∈ R satisfying

P� �q∈QA�|Zq − E[q(Y  X)]| ≤ τq�� ≥ 1 − 2ξ  where
τq = [η(QA ) + log(1/ξ)] · M/n��2[η(QA ) + log(1/ξ)] · (M 2 − {E[q(Y  X)]}2)�n.

(2.5)
Here τq > 0 is the tolerance parameter and ξ ∈ [0  1) is the tail probability. The quantity η(QA ) ≥ 0
in τq measures the capacity of QA in logarithmic scale  e.g.  for countable QA   η(QA ) = log(|QA |).
The number T is deﬁned as the oracle complexity. We denote by R[ξ  n  T  η(QA )] the set of oracles
satisfying (2.5)  and by A(T ) the family of algorithms that queries an oracle no more than T rounds.
This version of statistical query model is used in [8]  and reduces to the VSTAT model proposed in
[9–11] by the transformation �q(y  x) = q(y  x)/(2M ) + 1/2 for any q ∈ QA . The computational

model in Deﬁnition 2.2 enables us to handle query functions that are bounded by an unknown and
ﬁxed number M . Note that that by incorporating the tail probability ξ  the response Zq is allowed to
be unbounded. To understand the intuition behind Deﬁnition 2.2  we remark that (2.5) resembles the
Bernstein’s inequality for bounded random variables [25]

1
n

n�i=1

P�����

q(Yi  Xi) − E[q(Y  X)]���� ≥ t� ≤ 2 exp�

2Var[q(Y  X)] + M t�.

t2

(2.6)

We ﬁrst replace Var [q(Y  X)] by its upper bound M 2 − {E[q(Y  X)]}2  which is tight when q takes
values in {−M  M}. Then inequality (2.5) is obtained by replacing n−1�n
i=1 q(Yi  Xi) in (2.6) by
Zq and then bounding the suprema over the query space QA . In the deﬁnition of τq in (2.5)  we

4

incorporate the effect of uniform concentration over the query space QA by adding the quantity
η(QA )  which measures the capacity of QA . In addition  under the Deﬁnition 2.2  the algorithm
A does not interact directly with data. Such an restriction characterizes the fact that in statistical
problems  the effectiveness of an algorithm only depends on the global statistical properties  not the
information of individual data points. For instance  algorithms that only rely on the convergence of
the empirical distribution to the population distribution are contained in the statistical query model;
whereas algorithms that hinge on the ﬁrst data point (y1  x1) is not allowed. This restriction captures
a vast family of algorithms in statistics and machine learning  including applying gradient method to
maximize likelihood function  matrix factorization algorithms  expectation-maximization algorithms 
and sampling algorithms [9].
Based on the statistical query model  we study the minimax risk under oracle complexity constraints.
For the testing problem (2.3)  let A(Tn) be a class of testing algorithms under the statistical query
model with query complexity no more than Tn  with {Tn}n≥1 being a sequence of positive integers
depending on the sample size n. For any A ∈ A(Tn) and any oracle r ∈ R[ξ  n  Tn  η(QA )] that
responds to A   let H(A   r) be the set of test functions that deterministically depend on A ’s queries
to the oracle r and the corresponding responses. We use Pθ to denote the distribution of the random
variables returned by oracle r when the model parameter is θ.
For a general hypothesis testing problem  namely  H0 : θ ∈ G0 versus H1 : θ ∈ G1  the minimax test-
ing risk with respect to an algorithm A and a statistical oracle r ∈ R[ξ  n  Tn  η(QA )] is deﬁned as
(2.7)

∗

R

n(G0 G1; A   r) := inf

φ∈H(A  r)� sup

θ∈G0

Pθ(φ = 1) + sup
θ∈G1

Pθ(φ = 0)�.

Compared with the classical minimax risk in (2.4)  the new notion in (2.7) incorporates the computa-
tional budgets via oracle complexity. In speciﬁc  we only consider the test functions obtained by an
algorithm with at most Tn queries to a statistical oracle. If Tn is a polynomial of the dimensionality
d  (2.7) characterizes the statistical optimality of computational efﬁcient algorithms. This motivates
us to deﬁne the computationally tractable minimax rate  which contrasts with Deﬁnition 2.1.

Deﬁnition 2.3 (Computationally tractable minimax rate). Let G1(γn) := {θ : ρ(θ) ≥ γn} be a
sequence of model spaces with minimum separation γn  where ρ(θ) is the SNR. A sequence {γ∗
n}n≥1
is called a computationally tractable minimax rate if
n)  any constant η > 0  and any A ∈ A(dη) 
• For any sequence {γn}n≥1 satisfying γn = o(γ∗
there exists an oracle r ∈ R[ξ  n  Tn  η(QA )] such that limn→∞ R
• For any sequence {γn}n≥1 satisfying γn = Ω(γ∗
n)  there exist a constant η > 0 and an algorithm
A ∈ A(dη) such that  for any r ∈ R[ξ  n  Tn  η(QA )]  we have limn→∞ R
n[G0 G1(γn); A   r] = 0.
3 Main Results

n[G0 G1(γn); A   r] = 1;

∗

∗

Throughout this paper  we assume that the covariance matrix Σ in (2.1) is known. Speciﬁcally  for

some positive deﬁnite Σ ∈ Rd×d  the parameter spaces of the null and alternative hypotheses are

deﬁned as

G0(Σ) := {θ = (µ  µ  Σ  α) : µ ∈ Rd} 

G1(Σ; γn) := {θ = (µ0  µ1  Σ  α) : µ0  µ1 ∈ Rd  µ0 − µ1 ∈ B0(s)  ρ(θ) ≥ γn}.

Accordingly  the testing problem of detecting whether µ0 �= µ1 is to distinguish

H0 : θ ∈ G0(Σ) versus H1 : θ ∈ G1(Σ; γn).

In §3.1  we present the minimax rate of the detection problem from an information-theoretic perspec-
tive. In §3.2  under the statistical query model introduced in §2.2  we provide a computational lower
bound and a nearly matching upper bound that is achieved by an efﬁcient testing algorithm.

(3.1)

(3.2)

(3.3)

(3.4)

3.1

Information-theoretic Limits

Now we turn to characterize the minimax rate given in Deﬁnition 2.1. For parameter spaces (3.1) and

(3.2) with known Σ  we show that in highly sparse setting where s = o(√d)  we have

γ∗

n =�s log d/n ∧ (1/α2 · s log d/n) 

5

To prove (3.4)  we ﬁrst present a lower bound which shows that the hypothesis testing problem in
(3.3) is impossible if γn = o(γ∗

n).

Theorem 3.1. For the hypothesis testing problem in (3.3) with known Σ  we assume that there exists
a small constant δ > 0 such that s = o(d1/2−δ). Let γ∗
n be deﬁned in (3.4). For any sequence
{γn}n≥1 such that γn = o(γ∗

n)  any hypothesis test is asymptotically powerless  namely 

lim
n→∞

sup

Σ

R∗
n[G0(Σ) G1(Σ; γn)] = 1.

By Theorem 3.1  we observe a phase transition in the necessary SNR for powerful detection when α
decreases from one to zero. Starting with rate s log d/n in the supervised setting where α = 1  the
required SNR gradually increases as label qualities decrease. Finally  when α reaches zero  which

corresponds to the unsupervised setting  powerful detection requires the SNR to be Ω(�s log d/n).

It is worth noting that when α = (s log d/n)1/4  we still have (n3s log d)1/4 uncorrupted labels.
However  our lower bound (along with the upper bound shown in Theorem 3.2) indicates that the
information contained in these uncorrupted labels are buried in the noise  and cannot essentially
improve the detection quality compared with the unsupervised setting.
Next we establish a matching upper bound for the detection problem in (3.3). We denote the condition
number of the covariance matrix Σ by κ  i.e.  κ := λmax(Σ)/λmin(Σ)  where λmax(Σ) and λmin(Σ)
are the largest and smallest eigenvalues of Σ  repectively. Note that marginally Y is uniformly
distributed over {0  1}. For ease of presentation  we assume that the sample size is 2n and each class
contains exactly n data points. Note that we can always discard some samples in the larger class to
make the sample sizes of both classes to be equal. Due to the law of large numbers  this trick will not
affect the analysis of sample complexity in the sense of order wise.

Given 2n i.i.d. samples {(yi  xi)}2n

i=1 of (Y  X) ∈ {0  1} × Rd  we deﬁne

wi = Σ−1/2(x2i − x2i−1)  for all i ∈ [n].

In addition  we split the dataset {(yi  xi)}2n
ui = x(1)

and deﬁne

i=1 into two disjoint parts {(0  x(0)
i − x(0)

  for all i ∈ [n].

i

i

(3.5)

i=1 and {(1  x(1)
)}n

i

)}n
i=1 

(3.6)

We note that computing sample differences in (3.5) and (3.6) is critical for our problem because we
focus on detecting the difference between µ0 and µ1  and computing differences can avoid estimating
EPθ (X) that might be dense. For any integer s ∈ [d]  we deﬁne B2(s) := B0(s) ∩ Sd−1 as the set
of s-sparse vectors on the unit sphere in Rd. With {wi}n
i=1  we introduce two test

functions

φ1 := 1� sup
φ2 := 1� sup

v∈B2(1)

v∈B2(s)

(v�Σ−1wi)2

i=1 and {ui}n
2v�Σ−1v ≥ 1 + τ1� 
n�i=1
�v  diag(Σ)−1/2ui� ≥ τ2� 
n�i=1

1
n

1
n

(3.7)

(3.8)

where τ1  τ2 > 0 are algorithmic parameters that will be speciﬁed later. To provide some intuitions 
we consider the case where Σ = I. Test function φ1 seeks a sparse direction that explains the most
variance of wi. Therefore  such a test is closely related to the sparse principal component detection
problem [3]. Test function φ2 simply selects the coordinate of n−1�n
ui that has the largest
magnitude and compares it with τ2. This test is closely related to detecting sparse normal mean
in high dimensions [16]. Based on these two ingredients  we construct our ﬁnal testing function φ
as φ = φ1 ∨ φ2  i.e.  if any of φ1 and φ2 is true  then φ rejects the null. The following theorem
establishes a sufﬁcient condition for test function φ to be asymptotically powerful.

i=1

Theorem 3.2. Consider the testing problem (3.3) where Σ is known and has condition number κ.
For test functions φ1 and φ2 deﬁned in (3.7) and (3.8) with parameters τ1 and τ2 given by

We deﬁne the ultimate test function as φ = φ1 ∨ φ2. We assume that s ≤ C · d for some absolute
constant Cs and n ≥ 64 · s log(ed/s). Then if

τ1 = κ�s log(ed/s)/n  τ2 =�8 log d/n.
γn ≥ C �κ · [�s log(ed/s)/n ∧ (1/α2 · s log d/n)] 

(3.9)

6

where C � is an absolute constant  then test function φ is asymptotically powerful. In speciﬁc  we have

sup

Pn

θ (φ = 1) +

sup

θ∈G0(Σ)

θ∈G1(Σ;γn)

Pn
θ (φ = 0) ≤ 20/d.

(3.10)

Theorem 3.2 provides a non-asymptotic guarantee. When n goes to inﬁnity  (3.10) implies that
the test function φ is asymptotically powerful. When s = o(√d) and κ is a constant  (3.9) yields
γn = Ω[�s log d/n∧(1/α2·s log d/n)]  which matches the lower bound given in Theorem 3.1. Thus
we conclude that γ∗
n deﬁned in (3.4) is the minimax rate of testing problem in (3.3). We remark that
when s = Ω(d)  α = 1  i.e.  the standard (low-dimensional) setting of two sample testing  the bound
provided in (3.9) is sub-optimal as [22] shows that SNR rate √d/n is sufﬁcient for asymptotically
powerful detection when n = Ω(√d). It is thus worth noting that we focus on the highly sparse
setting s = o(√d) and provided sharp minimax rate for this regime. In the deﬁnition of φ1 in
(3.7)  we search over the set B2(s). Since B2(s) contains�d
s� distinct sets of supports  computing φ1

requires exponential running time.

3.2 Computational Limits

In this section  we characterize the computationally tractable minimax rate γ∗
n given in Deﬁnition 2.3.
Moreover  we focus on the setting where Σ is known a priori and the parameter spaces for the null
and alternative hypotheses are deﬁned in (3.1) and (3.2)  respectively. The main result is that  in

highly sparse setting where s = o(√d)  we have

We ﬁrst present the lower bound in the next result.

γ∗

n =�s2/n ∧ (1/α2 · s log d/n).

(3.11)

(3.12)

Theorem 3.3. For the testing problem in (3.3) with Σ known a priori  we make the same assumptions
as in Theorem 3.1. For any sequence {γn}n≥1 such that

γn = o�γ∗

n ∨��s2/n ∧ (1/α2 · s/n)��  

where γ∗
n is deﬁned in (3.4)  any computationally tractable test is asymptotically powerless under the
statistical query model. That is  for any constant η > 0 and any A ∈ A(dη)  there exists an oracle
r ∈ R[ξ  n  Tn  η(QA )] such that limn→∞ R
We remark that the lower bound in (3.12) differs from γ∗
�1/n ≤ α2 ≤�s log d/n. We expect this gap to be eliminated by more delicate analysis under the

statistical query model.
Now putting Theorems 3.1 and 3.3 together  we describe the “more supervision  less computation”
phenomenon as follows.

n[G0(Σ) G1(Σ  γn); A   r] = 1.

n in (3.11) by a logarithmic term when

∗

n and γ∗

n remains the same.

(i) When 0 ≤ α ≤ (log2 d/n)1/4  the computational lower bound implies that the uncorrupted
labels are unable to improve the quality of computationally tractable detection compared with the
unsupervised setting. In addition  in this region  the gap between γ∗
(ii) When (log2 d/n)1/4 < α ≤ (s log d/n)1/4  the information-theoretic lower bound shows that

the uncorrupted labels cannot improve the quality of detection compared with unsupervised setting.
However  more uncorrupted labels improve the statistical performances of hypothesis tests that are
computationally tractable by shrinking the gap between γ∗
(iii) When (s log d/n)1/4 < α ≤ 1  having more uncorrupted labels improves both statistical
n and γ∗
optimality and the computational efﬁciency. In speciﬁc  in this case  the gap between γ∗
n
vanishes and we have γ∗
Now we derive a nearly matching upper bound under the statistical query model  which establishes
the computationally tractable minimax rate together with Theorem 3.3. We construct a computation-
ally efﬁcient testing procedure that combines two test functions which yields the two parts in γ∗
n
respectively. Similar to φ1 deﬁned in (3.7)  the ﬁrst test function discards the information of labels 
which works for the purely unsupervised setting where α = 0. For j ∈ [d]  we denote by σj the j-th
diagonal element of Σ. Under the statistical query model  we consider the 2d query functions

n = 1/α2 · s log d/n.

n and γ∗
n.

n = γ∗

qj(y  x) := xj/√σj · 1{|xj/√σj| ≤ R ·�log d} 
�qj(y  x) := (x2

j /σj − 1) · 1{|xj/√σj| ≤ R ·�log d}  for all j ∈ [d] 

(3.13)

(3.14)

7

where R > 0 is an absolute constant. Here we apply truncation to the query functions to obtain
bounded queries  which is speciﬁed by the statistical query model in Deﬁnition 2.2. We denote by zqj
and z�qj the realizations of the random variables output by the statistical oracle for query functions qj

and�qj   respectively. As for the second test function  similar to (3.8)  we consider
for all v ∈ B2(1). We denote by Zqv
function qv. With these 4d query functions  we introduce test functions

qv(y  x) = (2y − 1) · v�diag(Σ)−1/2x · 1�|v�diag(Σ)−1/2x| ≤ R ·�log d�
zqv ≥ 2τ 2� 

qj ) ≥ Cτ 1�  φ2 := 1� sup

φ1 := 1� sup

(z�qj − z2

v∈B2(1)

j∈[d]

the output of the statistical oracle corresponding to query

(3.15)

(3.16)

where τ 1 and τ2 are positive parameters that will be speciﬁed later and C is an absolute constant.

Theorem 3.4. For the test functions φ1 and φ2 deﬁned in (3.16)   we deﬁne the ultimate test function
as φ = φ1 ∨ φ2. We set

τ 1 = R2 log d ·�log(4d/ξ)/n  τ 2 = R�log d ·�log(4d/ξ)/n 

(3.17)
where ξ = o(1). For the hypothesis testing problem in (3.3)  we further assume that �µ0�∞ ∨
�µ1�∞ ≤ C0 for some constant C0 > 0. Under the assumption that

(µ0 j − µ1 j)2/σj = Ω��1/α2 · log2 d · log(d/ξ)/n� ∧ log d ·�log(d/ξ)/n�  

sup
j∈[d]

(3.18)

n(φ) = supθ∈G0(Σ) Pθ(φ = 1) + supθ∈G1(Σ γn) Pθ(φ = 0) ≤ 5ξ. Here
the risk of φ satisﬁes that R
we denote by µ0 j and µ1 j the j-th entry of µ0 and µ1  respectively.
If we set the tail probability of the statistical query model to be ξ = 1/d  (3.18) shows that φ
is asymptotically powerful if supj∈[d](µ0 j − µ1 j)2/σj = Ω[(1/α2 · log3 d/n) ∧ (log3 d/n)1/2].
When the energy of µ0 − µ1 is spread over its support  �µ0 − µ1�∞ and �µ0 − µ1�2/√s are close.
Under the assumption that the condition number κ of Σ is a constant  (3.18) is implied by

∗

γn � (s2 log3 d/n)1/2 ∧ (1/α2 · s log3 d/n).
Compared with Theorem 3.3  the above upper bound matches the computational lower bound up to
n is between�s2/n ∧ (1/α2 · s log d/n) and (s2 log3 d/n)1/2 ∧ (1/α2 ·
a logarithmic factor and γ∗
s log3 d/n). Note that the truncation on query functions in (3.13) and (3.14) yields an additional
logarithmic term  which could be reduced to (s2 log d/n)1/2∧ (1/α2· s log d/n) using more delicate
analysis. Moreover  the test function φ1 is essentially based on a diagonal thresholding algorithm
performed on the covariance matrix of X. The work in [6] provides a more delicate analysis of
this algorithm which establishes the�s2/n rate. Their algorithm can also be formulated into the
more sophicated proof techinique  it can be shown that�s2/n ∧ (1/α2 · s log d/n) is the critical

statistical query model; we use the simpler version in (3.16) for ease of presentation. Therefore  with

threshold for asymptotically powerful detection with computational efﬁciency.

3.3

Implication for Estimation

Our aforementioned phase transition in the detection problems directly implies the statistical and
computational trade-offs in the problem of estimation. We consider the problem of estimating the
parameter ∆µ = µ0 − µ1 of the binary classiﬁcation model in (2.1) and (2.2)  where ∆µ is s-sparse
and Σ is known a priori. We assume that the signal to noise ratio is ρ(θ) = ∆µ�Σ−1∆µ ≥ γn =
n). For any constant η > 0 and any A ∈ A(T ) with T = O(dη)  suppose we obtain an estimator
o(γ∗
∆�µ of ∆µ by algorithm A under the statistical query model. If ∆�µ converges to ∆µ in the sense that
we have |∆�µ�Σ−1∆�µ − ∆µ�Σ−1∆µ| = o(γn). Thus the test function φ = 1{∆�µ�Σ∆�µ ≥
γn/2} is asymptotically powerful  which contradicts the computational lower bound in Theorem 3.3.
Therefore  there exists a constant C such that (∆�µ − ∆µ)�Σ−1(∆�µ − ∆µ) ≥ Cγ2
n/ρ(θ) for any
estimator ∆�µ constructed from polynomial number of queries.

(∆�µ − ∆µ)�Σ−1(∆�µ − ∆µ) = o[γ2

Acknowledgments

n/ρ(θ)] 

We would like to thank Vitaly Feldman for valuable discussions.

8

References

[1] Angluin  D. and Laird  P. (1988). Learning from noisy examples. Machine Learning  2 343–370.
[2] Berthet  Q. and Rigollet  P. (2013). Computational lower bounds for sparse PCA.

In Conference on

Learning Theory.

[3] Berthet  Q. and Rigollet  P. (2013). Optimal detection of sparse principal components in high dimension.

The Annals of Statistics  41 1780–1815.

[4] Chandrasekaran  V. and Jordan  M. I. (2013). Computational and statistical tradeoffs via convex relaxation.

Proceedings of the National Academy of Sciences  110 1181–1190.

[5] Chen  Y. and Xu  J. (2014). Statistical-computational tradeoffs in planted problems and submatrix local-

ization with a growing number of clusters and submatrices. arXiv preprint arXiv:1402.1267.

[6] Deshpande  Y. and Montanari  A. (2014). Sparse PCA via covariance thresholding. In Advances in Neural

Information Processing Systems.

[7] Fan  J.  Feng  Y. and Tong  X. (2012). A road to classiﬁcation in high dimensional space: The regularized

optimal afﬁne discriminant. Journal of the Royal Statistical Society: Series B  74 745–771.

[8] Fan  J.  Liu  H.  Wang  Z. and Yang  Z. (2016). Curse of heterogeneity: Computational barriers in sparse

mixture models and phase retrieval. Manuscript.

[9] Feldman  V.  Grigorescu  E.  Reyzin  L.  Vempala  S. and Xiao  Y. (2013). Statistical algorithms and a

lower bound for detecting planted cliques. In ACM Symposium on Theory of Computing.

[10] Feldman  V.  Guzman  C. and Vempala  S. (2015). Statistical query algorithms for stochastic convex

optimization. arXiv preprint arXiv:1512.09170.

[11] Feldman  V.  Perkins  W. and Vempala  S. (2015). On the complexity of random satisﬁability problems

with planted solutions. In ACM Symposium on Theory of Computing.

[12] Frénay  B. and Verleysen  M. (2014). Classiﬁcation in the presence of label noise: A survey.

IEEE

Transactions on Neural Networks and Learning Systems  25 845–869.

[13] Gao  C.  Ma  Z. and Zhou  H. H. (2014). Sparse CCA: Adaptive estimation and computational barriers.

arXiv preprint arXiv:1409.8565.

[14] Garcıa-Garcıa  D. and Williamson  R. C. (2011). Degrees of supervision. In Advances in Neural Informa-

tion Processing Systems.

[15] Hajek  B.  Wu  Y. and Xu  J. (2014). Computational lower bounds for community detection on random

graphs. arXiv preprint arXiv:1406.6625.

[16] Johnstone  I. M. (1994). On minimax estimation of a sparse normal mean vector. The Annals of Statistics 

22 271–289.

[17] Joulin  A. and Bach  F. R. (2012). A convex relaxation for weakly supervised classiﬁers. In International

Conference on Machine Learning.

[18] Kearns  M. (1993). Efﬁcient noise-tolerant learning from statistical queries. In ACM Symposium on Theory

of Computing.

[19] Ma  Z. and Wu  Y. (2014). Computational barriers in minimax submatrix detection. The Annals of

Statistics  43 1089–1116.

[20] Nettleton  D. F.  Orriols-Puig  A. and Fornells  A. (2010). A study of the effect of different types of noise

on the precision of supervised learning techniques. Artiﬁcial Intelligence Review  33 275–306.

[21] Patrini  G.  Nielsen  F.  Nock  R. and Carioni  M. (2016). Loss factorization  weakly supervised learning

and label noise robustness. arXiv preprint arXiv:1602.02450.

[22] Ramdas  A.  Singh  A. and Wasserman  L. (2016). Classiﬁcation accuracy as a proxy for two sample

testing. arXiv preprint arXiv:1602.02210.

[23] Tony Cai  T.  Liu  W. and Xia  Y. (2014). Two-sample test of high dimensional means under dependence.

Journal of the Royal Statistical Society: Series B  76 349–372.

[24] Tsybakov  A. B. (2008). Introduction to nonparametric estimation. Springer.
[25] Vershynin  R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint

arXiv:1011.3027.

[26] Wang  T.  Berthet  Q. and Samworth  R. J. (2014). Statistical and computational trade-offs in estimation

of sparse principal components. arXiv preprint arXiv:1408.5369.

[27] Wang  Z.  Gu  Q. and Liu  H. (2015). Sharp computational-statistical phase transitions via oracle compu-

tational model. arXiv preprint arXiv:1512.08861.

[28] Zhang  Y.  Wainwright  M. J. and Jordan  M. I. (2014). Lower bounds on the performance of polynomial-

time algorithms for sparse linear regression. In Conference on Learning Theory.

9

,Xinyang Yi
Zhaoran Wang
Zhuoran Yang
Constantine Caramanis
Han Liu