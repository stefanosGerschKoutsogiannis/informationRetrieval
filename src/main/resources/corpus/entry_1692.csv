2017,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification,Sequence classification algorithms  such as SVM  require a definition of distance (similarity) measure between two sequences. A commonly used notion of similarity is the number of matches between k-mers (k-length subsequences) in the two sequences. Extending this definition  by considering two k-mers to match if their distance is at most m  yields better classification performance. This  however  makes the problem computationally much more complex. Known algorithms to compute this similarity have computational complexity that render them applicable only for small values of k and m. In this work  we develop novel techniques to efficiently and accurately estimate the pairwise similarity score  which enables us to use much larger values of k and m  and get higher predictive accuracy. This opens up a broad avenue of applying this classification approach to audio  images  and text sequences. Our algorithm achieves excellent approximation performance with theoretical guarantees. In the process we solve an open combinatorial problem  which was posed as a major hindrance to the scalability of existing solutions. We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.,Efﬁcient Approximation Algorithms for String Kernel

Based Sequence Classiﬁcation

Muhammad Farhan

Department of Computer Science
School of Science and Engineering

Juvaria Tariq

Department of Mathematics

School of Science and Engineering

Lahore University of Management Sciences

Lahore University of Management Sciences

Lahore  Pakistan

14030031@lums.edu.pk

Lahore  Pakistan

jtariq@emory.edu

Arif Zaman

Department of Computer Science
School of Science and Engineering

Lahore University of Management Sciences

Lahore  Pakistan

arifz@lums.edu.pk

Mudassir Shabbir

Department of Computer Science
Information Technology University

Lahore  Pakistan

mudassir.shabbir@itu.edu.pk

Imdad Ullah Khan

Department of Computer Science
School of Science and Engineering

Lahore University of Management Sciences

Lahore  Pakistan

imdad.khan@lums.edu.pk

Abstract

Sequence classiﬁcation algorithms  such as SVM  require a deﬁnition of distance
(similarity) measure between two sequences. A commonly used notion of similarity
is the number of matches between k-mers (k-length subsequences) in the two
sequences. Extending this deﬁnition  by considering two k-mers to match if their
distance is at most m  yields better classiﬁcation performance. This  however 
makes the problem computationally much more complex. Known algorithms to
compute this similarity have computational complexity that render them applicable
only for small values of k and m. In this work  we develop novel techniques to
efﬁciently and accurately estimate the pairwise similarity score  which enables us
to use much larger values of k and m  and get higher predictive accuracy. This
opens up a broad avenue of applying this classiﬁcation approach to audio  images 
and text sequences. Our algorithm achieves excellent approximation performance
with theoretical guarantees. In the process we solve an open combinatorial problem 
which was posed as a major hindrance to the scalability of existing solutions. We
give analytical bounds on quality and runtime of our algorithm and report its
empirical performance on real world biological and music sequences datasets.

1

Introduction

Sequence classiﬁcation is a fundamental task in pattern recognition  machine learning  and data
mining with numerous applications in bioinformatics  text mining  and natural language processing.
Detecting proteins homology (shared ancestry measured from similarity of their sequences of amino

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

acids) and predicting proteins fold (functional three dimensional structure) are essential tasks in
bioinformatics. Sequence classiﬁcation algorithms have been applied to both of these problems with
great success [3  10  13  18  19  20  25]. Music data  a real valued signal when discretized using vector
quantization of MFCC features is another ﬂavor of sequential data [26]. Sequence classiﬁcation
has been used for recognizing genres of music sequences with no annotation and identifying artists
from albums [12  13  14]. Text documents can also be considered as sequences of words from a
language lexicon. Categorizing texts into classes based on their topics is another application domain
of sequence classiﬁcation [11  15].
While general purpose classiﬁcation methods may be applicable to sequence classiﬁcation  huge
lengths of sequences  large alphabet sizes  and large scale datasets prove to be rather challenging for
such techniques. Furthermore  we cannot directly apply classiﬁcation algorithms devised for vectors
in metric spaces because in almost all practical scenarios sequences have varying lengths unless
some mapping is done beforehand. In one of the more successful approaches  the variable-length
sequences are represented as ﬁxed dimensional feature vectors. A feature vector typically is the
spectra (counts) of all k-length substrings (k-mers) present exactly [18] or inexactly (with up to m
mismatches) [19] within a sequence. A kernel function is then deﬁned that takes as input a pair of
feature vectors and returns a real-valued similarity score between the pair (typically inner-product of
the respective spectra’s). The matrix of pairwise similarity scores (the kernel matrix) thus computed
is used as input to a standard support vector machine (SVM) [5  27] classiﬁer resulting in excellent
classiﬁcation performance in many applications [19]. In this setting k (the length of substrings used
as bases of feature map) and m (the mismatch parameter) are independent variables directly related
to classiﬁcation accuracy and time complexity of the algorithm. It has been established that using
larger values of k and m improve classiﬁcation performance [11  13]. On the other hand  the runtime
of kernel computation by the efﬁcient trie-based algorithm [19  24] is O(km+1|Σ|m(|X| + |Y |)) for
two sequences X and Y over alphabet Σ.
Computation of mismatch kernel between two sequences X and Y reduces to the following two
problems. i) Given two k-mers α and β that are at Hamming distance d from each other  determine
the size of intersection of m-mismatch neighborhoods of α and β (k-mers that are at distance at
most m from both of them). ii) For 0 ≤ d ≤ min{2m  k} determine the number of pairs of k-mers
(α  β) ∈ X × Y such that Hamming distance between α and β is d. In the best known algorithm [13]
the former problem is addressed by precomputing the intersection size in constant time for m ≤ 2
only. While a sorting and enumeration based technique is proposed for the latter problem that has
computational complexity O(2k(|X| + |Y |)  which makes it applicable for moderately large values
of k (of course limited to m ≤ 2 only).
In this paper  we completely resolve the combinatorial problem (problem i) for all values of m. We
prove a closed form expression for the size of intersection of m-mismatch neighborhoods that lets us
precompute these values in O(m3) time (independent of |Σ|  k  lengths and number of sequences).
For the latter problem we devise an efﬁcient approximation scheme inspired by the theory of locality
sensitive hashing to accurately estimate the number of k-mer pairs between the two sequences that
are at distance d. Combining the above two we design a polynomial time approximation algorithm for
kernel computation. We provide probabilistic guarantees on the quality of our algorithm and analytical
bounds on its runtime. Furthermore  we test our algorithm on several real world datasets with large
values of k and m to demonstrate that we achieve excellent predictive performance. Note that string
kernel based sequence classiﬁcation was previously not feasible for this range of parameters.

2 Related Work

In the computational biology community pairwise alignment similarity scores were used traditionally
as basis for classiﬁcation  like the local and global alignment [5  29]. String kernel based classiﬁcation
was introduced in [30  9]. Extending this idea  [30] deﬁned the gappy n-gram kernel and used it
in conjunction with SVM [27] for text classiﬁcation. The main drawback of this approach is that
runtime for kernel evaluations depends quadratically on lengths of the sequences.
An alternative model of string kernels represents sequences as ﬁxed dimensional vectors of counts of
occurrences of k-mers in them. These include k-spectrum [18] and substring [28] kernels. This notion
is extended to count inexact occurrences of patterns in sequences as in mismatch [19] and proﬁle
[10] kernels. In this transformed feature space SVM is used to learn class boundaries. This approach

2

yields excellent classiﬁcation accuracies [13] but computational complexity of kernel evaluation
remains a daunting challenge [11].
The exponential dimensions (|Σ|k) of the feature space for both the k-spectrum kernel and k  m-
mismatch kernel make explicit transformation of strings computationally prohibitive. SVM does not
require the feature vectors explicitly; it only uses pairwise dot products between them. A trie-based
strategy to implicitly compute kernel values for pairs of sequences was proposed in [18] and [19].
A (k  m)-mismatch tree is introduced which is a rooted |Σ|-ary tree of depth k  where each internal
node has a child corresponding to each symbol in Σ and every leaf corresponds to a k-mer in Σk.
The runtime for computing the k  m mismatch kernel value between two sequences X and Y   under
this trie-based framework  is O((|X| + |Y |)km+1|Σ|m)  where |X| and |Y | are lengths of sequences.
This makes the algorithm only feasible for small alphabet sizes and very small number of allowed
mismatches.
The k-mer based kernel framework has been extended in several ways by deﬁning different string
kernels such as restricted gappy kernel  substitution kernel  wildcard kernel [20]  cluster kernel [32] 
sparse spatial kernel [12]  abstraction-augmented kernel [16]  and generalized similarity kernel [14].
For literature on large scale kernel learning and kernel approximation see [34  1  7  22  23  33] and
references therein.

3 Algorithm for Kernel Computation

In this section we formulate the problem  describe our algorithm and analyze it’s runtime and quality.
k-spectrum and k  m-mismatch kernel: Given a sequence X over alphabet Σ  the k  m-mismatch
spectrum of X is a |Σ|k-dimensional vector  Φk m(X) of number of times each possible k-mer occurs
in X with at most m mismatches. Formally 

Φk m(X) = (Φk m(X)[γ])γ∈Σk =

Im(α  γ)

 

γ∈Σk

(1)

where Im(α  γ) = 1  if α belongs to the set of k-mers that differ from γ by at most m mismatches 
the Hamming distance between α and γ  d(α  γ) ≤ m. Note that for m = 0  it is known
i.e.
as k-spectrum of X. The k  m-mismatch kernel value for two sequences X and Y (the mismatch
spectrum similarity score) [19] is deﬁned as:

K(X  Y |k  m) = (cid:104)Φk m(X)  Φk m(Y )(cid:105) =

Φk m(X)[γ]Φk m(Y )[γ]

(cid:33)

(cid:32)(cid:88)

α∈X

(cid:88)
(cid:88)

γ∈Σk

(cid:88)

(cid:88)

α∈X

β∈Y

γ∈Σk

(cid:88)

(cid:88)

γ∈Σk

α∈X

(cid:88)

β∈Y

=

Im(α  γ)

Im(β  γ) =

Im(α  γ)Im(β  γ).

(2)

For a k-mer α  let Nk m(α) = {γ ∈ Σk : d(α  γ) ≤ m} be the m-mutational neighborhood of α.
Then for a pair of sequences X and Y   the k  m-mismatch kernel given in eq (2) can be equivalently
computed as follows [13]:

K(X  Y |k  m) =

(cid:88)

(cid:88)
(cid:88)

α∈X

(cid:88)
(cid:88)

β∈Y

Im(α  γ)Im(β  γ)

γ∈Σk
|Nk m(α) ∩ Nk m(β)| =

(cid:88)

(cid:88)

=

β∈Y

α∈X

Im(α  β) 

(3)
where Im(α  β) = |Nk m(α) ∩ Nk m(β)| is the size of intersection of m-mutational neighborhoods
of α and β. We use the following two facts.
Fact 3.1. Im(α  β)  the size of the intersection of m-mismatch neighborhoods of α and β  is a
function of k  m  |Σ| and d(α  β) and is independent of the actual k-mers α and β or the actual
positions where they differ. (See section 3.1)
Fact 3.2. If d(α  β) > 2m  then Im(α  β) = 0.
In view of the above two facts we can rewrite the kernel value (3) as

α∈X

β∈Y

K(X  Y |k  m) =

Mi · Ii 

(4)

(cid:88)

(cid:88)

α∈X

β∈Y

Im(α  β) =

3

min{2m k}(cid:88)

i=0

where Ii = Im(α  β) when d(α  β) = i and Mi is the number of pairs of k-mers (α  β) such that
d(α  β) = i  where α ∈ X and β ∈ Y . Note that bounds on the last summation follows from Fact
3.2 and the fact that the Hamming distance between two k-mers is at most k. Hence the problem of
kernel evaluation is reduced to computing Mi’s and evaluating Ii’s.

3.1 Closed form for Intersection Size

Let Nk m(α  β) be the intersection of m-mismatch neighborhoods of α and β i.e.

Nk m(α  β) = Nk m(α) ∩ Nk m(β).

As deﬁned earlier |Nk m(α  β)| = Im(α  β). Let Nq(α) = {γ ∈ Σk : d(α  γ) = q} be the set of
k-mers that differ with α in exactly q indices. Note that Nq(α) ∩ Nr(α) = ∅ for all q (cid:54)= r. Using
this and deﬁning nqr(α  β) = |Nq(α) ∩ Nr(β)| 

m(cid:91)

m(cid:91)

Nk m(α  β) =

Nq(α) ∩ Nr(β)

and Im(α  β) =

nqr(α  β).

q=0

r=0

q=0

r=0

Hence we give a formula to compute nij(α  β). Let s = |Σ|.
Theorem 3.3. Given two k-mers α and β such that d(α  β) = d  we have that

(cid:18)2d − i − j + 2t

(cid:19)(cid:18)

d − (i − t)

2(cid:88)

i+j−d

t=0

nij(α  β) =

(cid:19)

d

i + j − 2t − d

(s − 2)i+j−2t−d

(s − 1)t.

m(cid:88)

m(cid:88)

(cid:18)k − d
(cid:19)

t

Proof. nij(α  β) can be interpreted as the number of ways to make i changes in α and j changes
in β to get the same string. For clarity  we ﬁrst deal with the case when we have d(α  β) = 0  i.e
both strings are identical. We wish to ﬁnd nij(α  β) = |Ni(α) ∩ Nj(β)|. It is clear that in this
case i = j  otherwise making i and j changes to the same string will not result in the same string.

(cid:1)(s − 1)i. Second we consider α  β such that d(α  β) = k. Clearly k ≥ i and k ≥ j.

Hence nij =(cid:0)k

Moreover  since both strings do not agree at any index  character at every index has to be changed in
at least one of α or β. This gives k ≤ i + j.
Now for a particular index p  α[p] and β[p] can go through any one of the following three changes.
Let α[p] = x  β[p] = y. (I) Both α[p] and β[p] may change from x and y respectively to some
character z. Let l1 be the count of indices going through this type of change. (II) α[p] changes from
x to y  call the count of these l2. (III) β[p] changes from y to x  let this count be l3. It follows that

i

 

i = l1 + l2

j = l1 + l3 

change  we have s − 2 character choices for each such index and(cid:0)
(cid:0)2k−i−j

This results in l1 = i + j − k. Since l1 is the count of indices at which characters of both strings
indices for l1. From the remaining l2 + l3 = 2k − i − j indices  we choose l2 = k − j indices in

(cid:1) possible combinations of
(cid:1) ways and change the characters at these indices of α to characters of β at respective indices.

l1 + l2 + l3 = k.

k

i+j−k

 

Finally  we are left with only l3 remaining indices and we change them according to the deﬁnition of
l3. Thus the total number of strings we get after making i changes in α and j changes in β is

k−j

(cid:18)

(cid:19)(cid:18)2k − i − j

(cid:19)

k − j

.

(s − 2)i+j−k

k

i + j − k

Now we consider general strings α and β of length k with d(α  β) = d. Without loss of generality
assume that they differ in the ﬁrst d indices. We parameterize the system in terms of the number of
changes that occur in the last k − d indices of the strings i.e let t be the number of indices that go
through a change in last k − d indices. Number of possible such changes is

(s − 1)t.

(5)

(cid:18)k − d
(cid:19)

t

Lets call the ﬁrst d-length substrings of both strings α(cid:48) and β(cid:48). There are i − t characters to be
changed in α(cid:48) and j − t in β(cid:48). As reasoned above  we have d ≤ (i − t) + (j − t) =⇒ t ≤ i+j−d
.

2

4

In this setup we get i − t = l1 + l2  j − t = l1 + l3  l1 + l2 + l3 = d and l1 = (i − t) + (j − t) − d.
We immediately get that for a ﬁxed t  the total number of resultant strings after making i − t changes
in α(cid:48) and j − t changes in β(cid:48) is

(cid:19)

(cid:18)2d − (i − t) − (j − t)
(cid:19)(cid:18)

d − (i − t)

d

(i − t) + (j − t) − d

(s − 2)(i−t)+(j−t)−d.

(6)

For a ﬁxed t  every substring counted in (5)  every substring counted in (6) gives a required string
obtained after i and j changes in α and β respectively. The statement of the theorem follows.
Corollary 3.4. Runtime of computing Id is O(m3)  independent of k and |Σ|.
This is so  because if d(α  β) = d  Id =

nqr(α  β) and nqr(α  β) can be computed in O(m).

m(cid:80)

m(cid:80)

3.2 Computing Mi

q=0

r=0

Recall that given two sequences X and Y   Mi is the number of pairs of k-mers (α  β) such that
d(α  β) = i  where α ∈ X and β ∈ Y . Formally  the problem of computing Mi is as follows:
Problem 3.5. Given k  m  and two sets of k-mers SX and SY (set of k-mers extracted from the
sequences X and Y respectively) with |SX| = nX and |SY | = nY . Compute

Mi = |{(α  β) ∈ SX × SY : d(α  β) = i}|

for 0 ≤ i ≤ min{2m  k}.

Note that the brute force approach to compute Mi requires O(nX · nY · k) comparisons. Let Qk(j)
denote the set of all j-sets of {1  . . .   k} (subsets of indices). For θ ∈ Qk(j) and a k-mer α  let α|θ
be the j-mer obtained by selecting the characters at the j indices in θ. Let fθ(X  Y ) be the number
of pairs of k-mers in SX × SY as follows;

fθ(X  Y ) = |{(α  β) ∈ SX × SY : d(α|θ  β|θ) = 0}|.

We use the following important observations about fθ.
Fact 3.6. For 0 ≤ i ≤ k and θ ∈ Qk(k − i)  if d(α|θ  β|θ) = 0  then d(α  β) ≤ i.
Fact 3.7. For 0 ≤ i ≤ k and θ ∈ Qk(k − i)  fθ(X  Y ) can be computed in O(kn log n) time.
This can be done by ﬁrst lexicographically sorting the k-mers in each of SX and SY by the indices
in θ. The pairs in SX × SY that are the same at indices in θ can then be enumerated in one linear
scan over the sorted lists. Let n = nX + nY   runtime of this computation is O(k(n + |Σ|)) if we
use counting sort (as in [13]) or O(kn log n) for mergesort (since θ has O(k) indices.) Since this
procedure is repeated many times  we refer to this as the SORT-ENUMERATE subroutine. We deﬁne

Lemma 3.8.

Fi(X  Y ) =

Proof. Let (α  β) be a pair that contributes to Mj  i.e. d(α  β) = j. Then for every θ ∈ Qk(k − i)
that has all indices within the k − j positions where α and β agree  the pair (α  β) is counted in

k−i

the required equality.

fθ(X  Y ). The number of such θ’s are(cid:0)k−j
(cid:1)  hence Mj is counted(cid:0)k−j
Corollary 3.9. Mi can readily be computed as: Mi = Fi(X  Y ) − i−1(cid:80)
By deﬁnition  Fi(X  Y ) can be computed with(cid:0) k
(cid:33)

(cid:1) =(cid:0)k

k−i

k−i

(cid:32) t(cid:88)

(cid:18)k

(cid:19)

(cid:1) times in Fi(X  Y )  yielding
(cid:0)k−j

(cid:1)Mj.

k−i

(cid:1) fθ computations. Let t = min{2m  k}.

j=0

K(X  Y |k  m) can be evaluated by (4) after computing Mi (by (8)) and Ii (by Corollary 3.4) for
0 ≤ i ≤ t. The overall complexity of this strategy thus is

i

(k − i)(n log n + n)

+ O(n) = O(k · 2k−1 · (n log n)).

i

i=0

5

Fi(X  Y ) =

fθ(X  Y ).

(cid:88)
(cid:18)k − j
i(cid:88)

θ∈Qk(k−i)

(cid:19)

k − i

Mj.

j=0

(7)

(8)

δ

(cid:1))

Algorithm 1 : Approximate-Kernel(SX SY  k m  δ B)
1: I  M(cid:48) ← ZEROS(t + 1)
2: σ ←  · √
3: Populate I using Corollary 3.4
4: for i = 0 to t do
5:
6:
7:
8:
9:

µF ← 0
iter ← 1
varF ← ∞
while varF > σ2 ∧ iter < B do

θ ← RANDOM((cid:0) k
µF ← µF · (iter − 1) + SORT-ENUMERATE(SX   SY   k  θ)
F (cid:48)[i] ← µF ·(cid:0) k
(cid:1)
iter
varF ← VARIANCE(µF   varF   iter)
iter ← iter + 1
k−i
M(cid:48)[i] ← M(cid:48)[i] −(cid:0)k−j

10:
11:
12:
13:
14:
15:
16:
k−i
17: K(cid:48) ← SUMPRODUCT(M(cid:48) I)
18: return K(cid:48)

M(cid:48)[i] ← F (cid:48)[i]
for j = 0 to i − 1 do

(cid:1) · M(cid:48)[j]

k−i

(cid:46) Application of Fact 3.7
(cid:46) Compute online variance

(cid:46) Application of Corollary 3.9

(cid:46) Applying Equation (4)

We give our algorithm to approximate K(X  Y |k  m)  it’s explanation followed by it’s analysis.
Algorithm 1 takes   δ ∈ (0  1)  and B ∈ Z+ as input parameters; the ﬁrst two controls the accuracy
of estimate while B is an upper bound on the sample size. We use (7) to estimate Fi = Fi(X  Y ) with
an online sampling algorithm  where we choose θ ∈ Qk(k − i) uniformly at random and compute the
online mean and variance of the estimate for Fi. We continue to sample until the variance is below the
threshold (σ2 = 2δ) or the sample size reaches the upper bound B. We scale up our estimate by the
population size and use it to compute M(cid:48)
i ’s together
with the precomputed exact values of Ii’s are used to compute our estimate  K(cid:48)(X  Y |k  m  σ  δ  B) 
for the kernel value using (4). First we give an analytical bound on the runtime of Algorithm 1 then
we provide guarantees on it’s performance.
Theorem 3.10. Runtime of Algorithm 1 is bounded above by O(k2n log n).

i (estimates of Mi) using Corollary 3.9. These M(cid:48)

Proof. Observe that throughout the execution of the algorithm there are at most tB computations of
fθ  which by Fact 3.7 needs O(kn log n) time. Since B is an absolute constant and t ≤ k  we get
that the total runtime of the algorithm is O(k2n log n). Note that in practice the while loop in line 8
is rarely executed for B iterations; the deviation is within the desired range much earlier.
Let K(cid:48) = K(cid:48)(X  Y |k  m    δ  B) be our estimate (output of Algorithm 1) for K = K(X  Y |k  m).
Theorem 3.11. K(cid:48) is an unbiased estimator of the true kernel value  i.e. E(K(cid:48)) = K.

Proof. For this we need the following result  whose proof is deferred.
Lemma 3.12. E(M(cid:48)

By Line 17 of Algorithm 1  E(K(cid:48)) = E((cid:80)t

i ) = Mi.

i=0 IiM(cid:48)

Lemma 3.12 we get that

t(cid:88)

E(K(cid:48)) =

IiE(M(cid:48)

i ) =

i ). Using the fact that Ii’s are constants and
min{2m k}(cid:88)

IiMi = K.

i=0

i=0

Theorem 3.13. For any 0 <   δ < 1  Algorithm 1 is an (Imax  δ)−additive approximation
algorithm  i.e. P r(|K − K(cid:48)| ≥ Imax) < δ  where Imax = maxi{Ii}.

6

Note that these are very loose bounds  in practice we get approximation far better than these bounds.
Furthermore  though Imax could be large  but it is only a fraction of one of the terms in summation
for the kernel value K(X  Y |k  m).

i be our estimate for Fi (X  Y ) = Fi. We use the following bound on the variance of

Proof. Let F (cid:48)
K(cid:48) that is proved later.
Lemma 3.14. V ar(K(cid:48)) ≤ δ( · Imax)2.
By Lemma 3.12 we have E(K(cid:48)) = K  hence by Lemma 3.14  P r[|K(cid:48) − K|] ≥ Imax is equivalent
to P r[|K(cid:48) − E(K(cid:48))|] ≥ 1√
most δ. Therefore  Algorithm 1 is an (Imax  δ)−additive approximation algorithm.

(cid:112)V ar(K(cid:48)). By the Chebychev’s inequality  this latter probability is at

δ

Proof. (Proof of Lemma 3.12) We prove it by induction on i. The base case (i = 0) is true as we
j) = Mj for 0 ≤ j ≤ i − 1. Let iter be
compute M(cid:48)[0] exactly  i.e. M(cid:48)[0] = M [0]. Suppose E(M(cid:48)
the number of iterations for i  after execution of Line 10 we get

(cid:80)iter

(cid:18) k

(cid:19)

k − i

=

r=1 fθr (X  Y )

iter

(cid:18) k

(cid:19)

k − i

 

F (cid:48)[i] = µF

(cid:18) k

where θr is the random (k − i)-set chosen in the rth iteration of the while loop. Since θr is chosen
uniformly at random we get that

(cid:1) (cid:18) k
(cid:0) k
(cid:0)k−j
After the loop on Line 15 is executed we get that E(M(cid:48)[i]) = Fi(X  Y ) − i−1(cid:80)

(cid:19)
(cid:1)E(M(cid:48)

E(F (cid:48)[i]) = E(µF )

(cid:18) k

= E(fθr (X  Y ))

Fi(X  Y )

k − i

k − i

k − i

(cid:19)

(cid:19)

k−i

(9)

j). Using

=

.

k−i

E(M(cid:48)

j) = Mj (inductive hypothesis) in (8) we get that E(M(cid:48)

i ) = Mi.

j=0

(cid:1)M(cid:48)

Proof. (Proof of Lemma 3.14) After execution of the while loop in Algorithm 1  we have F (cid:48)

(cid:0)k−j
i(cid:80)
Fact 3.15. Suppose X0  . . .   Xt are random variables and let S =(cid:80)t

j. We use the following fact that follows from basic calculations.

k−i

i=0 aiXi  where a0  . . .   at

i =

j=0

are constants. Then

t(cid:88)

t(cid:88)

t(cid:88)

V ar(S) =

a2
i V ar(Xi) + 2

aiajCov(Xi  Xj).

i=0

i=0

j=i+1

Using fact 3.15 and deﬁnitions of Imax and σ we get that

V ar(K(cid:48)) =

2V ar(M(cid:48)

IiIjCov(M(cid:48)

i   M(cid:48)
j)

V ar(M(cid:48)

Cov(M(cid:48)

maxV ar(F (cid:48)

maxσ2 = δ(·Imax)2.

The last inequality follows from the following relation derived from deﬁnition of F (cid:48)

i and Fact 3.15.

V ar(F (cid:48)

t ) =

V ar(M(cid:48)

i ) + 2

Cov(M(cid:48)

i   M(cid:48)
j).

(10)

t ) ≤ I 2
(cid:19)

(cid:19)(cid:18)k − j

k − t

 t(cid:88)

i=0

≤ I 2

max

i=0

Ii

t(cid:88)
t(cid:88)
t(cid:88)
(cid:19)2
(cid:18)k − i

i ) + 2

i=0

j=i+1

k − t

t(cid:88)

i=0

i ) + 2

i=0

j=i+1

t(cid:88)
t(cid:88)
 ≤ I 2
i   M(cid:48)
j)
(cid:18)k − i
t(cid:88)
t(cid:88)

k − t

i=0

j=i+1

7

4 Evaluation

We study the performance of our algorithm in terms of runtime  quality of kernel estimates and pre-
dictive accuracies on standard benchmark sequences datasets (Table 1) . For the range of parameters
feasible for existing solutions  we generated kernel matrices both by algorithm of [13] (exact) and
our algorithm (approximate). These experiments are performed on an Intel Xeon machine with (8
Cores  2.1 GHz and 32 GB RAM) using the same experimental settings as in [13  15  17]. Since
our algorithm is applicable for signiﬁcantly wider range of k and m  we also report classiﬁcation
performance with large k and m. For our algorithm we used B ∈ {300  500} and σ ∈ {0.25  0.5}
with no signiﬁcant difference in results as implied by the theoretical analysis. In all reported results
B = 300 and σ = 0.5. In order to perform comparisons  for a few combinations of parameters
we generated exact kernel matrices of each dataset on a much more powerful machine (a cluster of
20 nodes  each having 24 CPU’s with 2.5 GHz speed and 128GB RAM). Sources for datasets and
source code are available at 1.

Table 1: Datasets description

Name
Ding-Dubchak [6]
SCOP [4  31]
Music [21  26]
Artist20 [8  17]
ISMIR [17]

Task
protein fold recognition
protein homology detection
music genre recognition
artist identiﬁcation
music genre recognition

Classes
27
54
10
20
6

Seq.
694
7329
1000
1413
729

Av.Len. Evaluation
10-fold CV
169
54 binary class.
308
5-fold CV
2368
6-fold CV
9854
5-fold CV
10137

Running Times: We report difference in running times for kernels generation in Figure 1. Exact
kernels are generated using code provided by authors of [13  14] for 8 ≤ k ≤ 16 and m = 2 only.
We achieve signiﬁcant speedups for large values of k (for k = 16 we get one order of magnitude
gains in computational efﬁciency on all datasets). The running times for these algorithms are O(2kn)
and O(k2n log n) respectively. We can use larger values of k without an exponential penalty  which
is visible in the fact that in all graphs  as k increases the growth of running time of the exact algorithm
is linear (on the log-scale)  while that of our algorithm tends to taper off.

Figure 1: Log scaled plot of running time of approximate and exact kernel generation for m = 2

Kernel Error Analysis: We show that despite reduction in runtimes  we get excellent approximation
of kernel matrices. In Table 2 we report point-to-point error analysis of the approximate kernel
matrices. We compare our estimates with exact kernels for m = 2. For m > 2 we report statistical
error analyses. More precisely  we evaluate differences with principal submatrices of the exact kernel
matrix. These principal submatrices are selected by randomly sampling 50 sequences and computing
their pairwise kernel values. We report errors for four datasets; the ﬁfth one  not included for space
reasons  showed no difference in error. From Table 2 it is evident that our empirical performance is
signiﬁcantly more precise than the theoretical bounds proved on errors in our estimates.

1https://github.com/mufarhan/sequence_class_NIPS_2017

8

110100100010000810121416810121416810121416810121416810121416DingDubchakSCOPMusicGenreISMIR2004Artist20k Running Time (sec) ExactApproximateTable 2: Mean absolute error (MAE) and root mean squared error (RMSE) of approximate kernels.
For m > 2 we report average MAE and RMSE of three random principal submatrices of size 50 × 50

(k  m)
(10  2)
(12  2)
(14  2)
(16  2)
(12  6)

Music Genre

RMSE

MAE

ISMIR

RMSE

MAE

Artist20

RMSE

MAE

0
0

2.0E−8
1.3E−8
1.97E−5

0
0
0
0

8.5E−7

0
0

2.0E−8
4.0E−8

0
0
0

3.3E−9

0
0

3.3E−8

0
0

1.3E−8

SCOP

RMSE
1.3E−6
1.4E−6
2.9E−6
2.9E−6
2.4E−4

MAE
9.0E−8
1.0E−8
1.3E−8
1.0E−8
1.8E−5

Prediction Accuracies: We compare the outputs of SVM on the exact and approximate kernels
using the publicly available SVM implementation LIBSVM [2]. We computed exact kernel matrices
by brute force algorithm for a few combinations of parameters for each dataset on the much more
powerful machine. Generating these kernels took days; we only generated to compare classiﬁcation
performance of our algorithm with the exact one. We demonstrate that our predictive accuracies
are sufﬁciently close to that with exact kernels in Table 3 (bio-sequences) and Table 4 (music). The
parameters used for reporting classiﬁcation performance are chosen in order to maintain comparability
with previous studies. Similarly all measurements are made as in [13  14]  for instance for music
genre classiﬁcation we report results of 10-fold cross-validation (see Table 1). For our algorithm we
used B = 300 and σ = 0.5 and we take an average of performances over three independent runs.
Table 3: Classiﬁcation performance comparisons on SCOP (ROC) and Ding-Dubchak (Accuracy)

SCOP

Exact

Approx

k  m ROC ROC50 ROC ROC50
38.60
8  2
10  2
26.72
11.04
12  2
6.66
14  2
5.76
16  2
10  5
54.1
48.44
10  7
12  8
52.08

38.71
28.18
23.27
7.78
6.89
53.77
48.18
50.54

88.09
81.65
71.31
67.91
64.45
91.60
90.27
91.44

88.05
80.56
66.93
63.67
61.64
91.67
90.30
90.97

Ding-Dubchak
Exact Approx

Accuracy

34.01
28.1
27.23
25.5
25.94
45.1
58.21
58.21

31.65
26.9
26.66
25.5
25.03
43.80
57.20
57.83

Table 4: Classiﬁcation error comparisons on music datasets exact and estimated kernels

Music Genre

k  m Exact
10  2
14  2
16  2
10  7
12  6
12  8

61.30 ± 3.3
71.70 ± 3.0
73.90 ± 1.9
37.00 ± 3.5
54.20 ± 2.7
43.70 ± 3.2

Estimate
61.30 ± 3.3
71.70 ± 3.0
73.90 ± 1.9
37.00 ± 3.5
54.13 ± 2.9
44.20 ± 3.2

ISMIR

Exact
54.32 ± 1.6
55.14 ± 1.1
54.73 ± 1.5
52.12 ± 2.0
47.03 ± 2.6

Estimate
54.32 ± 1.6
55.14 ± 1.1
54.73 ± 1.5
27.16 ± 1.6
52.08 ± 1.5
47.41 ± 2.4

Artist20

Exact
82.10 ± 2.2
86.84 ± 1.8
87.56 ± 1.8
55.75 ± 4.7
79.57 ± 2.4

Estimate
82.10 ± 2.2
86.84 ± 1.8
87.56 ± 1.8
55.75 ± 4.7
80.00 ± 2.6
67.57 ± 3.6

5 Conclusion

In this work we devised an efﬁcient algorithm for evaluation of string kernels based on inexact
matching of subsequences (k-mers). We derived a closed form expression for the size of intersection
of m-mismatch neighborhoods of two k-mers. Another signiﬁcant contribution of this work is a novel
statistical estimate of the number of k-mer pairs at a ﬁxed distance between two sequences. Although
large values of the parameters k and m were known to yield better classiﬁcation results  known
algorithms are not feasible even for moderately large values. Using the two above mentioned results
our algorithm efﬁciently approximate kernel matrices with probabilistic bounds on the accuracy.
Evaluation on several challenging benchmark datasets for large k and m  show that we achieve state
of the art classiﬁcation performance  with an order of magnitude speedup over existing solutions.

9

References
[1] F. R. Bach and M. I. Jordan. Predictive low-rank decomposition for kernel methods.

International Conference on Machine Learning  ICML  pages 33–40  2005.

In

[2] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology  2:27:1–27:27  2011.

[3] J. Cheng and P. Baldi. A machine learning information retrieval approach to protein fold

recognition. Bioinformatics  22(12):1456–1463  2006.

[4] L. Conte  B. Ailey  T. Hubbard  S. Brenner  A. Murzin  and C. Chothia. Scop: A structural

classiﬁcation of proteins database. Nucleic Acids Research  28(1):257–259  2000.

[5] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines and other

kernel-based learning methods. Cambridge university press  2000.

[6] C. Ding and I. Dubchak. Multi-class protein fold recognition using support vector machines

and neural networks. Bioinformatics  17(4):349–358  2001.

[7] P. Drineas and M. W. Mahoney. On the nyström method for approximating a gram matrix for
improved kernel-based learning. The Journal of Machine Learning Research  6:2153–2175 
2005.

[8] D. P. Ellis. Classifying music audio with timbral and chroma features. In ISMIR  volume 7 

pages 339–340  2007.

[9] D. Haussler. Convolution kernels on discrete structures. Technical Report UCS-CRL-99-10 

University of California at Santa Cruz  1999.

[10] R. Kuang  E. Ie  K. Wang  K. Wang  M. Siddiqi  Y. Freund  and C. Leslie. Proﬁle-based string
kernels for remote homology detection and motif extraction. Journal of Bioinformatics and
Computational Biology  3(3):527–550  2005.

[11] P. Kuksa. Scalable kernel methods and algorithms for general sequence analysis. PhD thesis 

Department of Computer Science  Rutgers  The State University of New Jersey  2011.

[12] P. Kuksa  P.-H. Huang  and V. Pavlovic. Fast protein homology and fold detection with sparse
spatial sample kernels. In 19th International Conference on Pattern Recognition  ICPR  pages
1–4. IEEE  2008.

[13] P. Kuksa  P.-H. Huang  and V. Pavlovic. Scalable algorithms for string kernels with inexact
matching. In Advances in Neural Information Processing Systems  NIPS  pages 881–888. MIT
Press  2009.

[14] P. Kuksa  I. Khan  and V. Pavlovic. Generalized similarity kernels for efﬁcient sequence
classiﬁcation. In SIAM International Conference on Data Mining  SDM  pages 873–882. SIAM 
2012.

[15] P. Kuksa and V. Pavlovic. Spatial representation for efﬁcient sequence classiﬁcation. In 20th

International Conference on Pattern Recognition  ICPR  pages 3320–3323. IEEE  2010.

[16] P. Kuksa  Y. Qi  B. Bai  R. Collobert  J. Weston  V. Pavlovic  and X. Ning. Semi-supervised
abstraction-augmented string kernel for multi-level bio-relation extraction. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases  ECML-PKDD 
pages 128–144. Springer  2010.

[17] P. P. Kuksa. Efﬁcient multivariate sequence classiﬁcation. In CoRR abs/1409.8211  2013.

[18] C. Leslie  E. Eskin  and W. Noble. The spectrum kernel: A string kernel for svm protein
classiﬁcation. In Paciﬁc Symposium on Biocomputing  volume 7 of PSB  pages 566–575  2002.

[19] C. Leslie  E. Eskin  J. Weston  and W. Noble. Mismatch string kernels for svm protein
classiﬁcation. In Advances in Neural Information Processing Systems  NIPS  pages 1441–1448.
MIT Press  2003.

10

[20] C. Leslie and R. Kuang. Fast string kernels using inexact matching for protein sequences.

Journal of Machine Learning Research  5:1435–1455  2004.

[21] T. Li  M. Ogihara  and Q. Li. A comparative study on content-based music genre classiﬁca-
tion. In 26th Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval  ACM/SIGIR  pages 282–289. ACM  2003.

[22] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

Neural Information Processing Systems  NIPS  pages 1177–1184  2007.

[23] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems  NIPS 
pages 1313–1320  2008.

[24] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge university

press  2004.

[25] S. Sonnenburg  G. Rätsch  and B. Schölkopf. Large scale genomic sequence svm classiﬁers. In

22nd International Conference on Machine Learning  ICML  pages 848–855. ACM  2005.

[26] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of audio signals. IEEE Transactions on

Speech and Audio Processing  10(5):293–302  2002.

[27] V. Vapnik. Statistical learning theory  volume 1. Wiley New York  1998.

[28] S. Vishwanathan and A. Smola. Fast kernels for string and tree matching. In Advances in

Neural Information Processing Systems  NIPS  pages 585–592  2002.

[29] M. Waterman  J. Joyce  and M. Eggert. Computer alignment of sequences. Phylogenetic

analysis of DNA sequences  pages 59–72.

[30] C. Watkins. Dynamic alignment kernels. In Advances in Large Margin Classiﬁers  pages 39–50.

MIT Press  1999.

[31] J. Weston  C. Leslie  E. Ie  D. Zhou  A. Elisseeff  and W. Noble. Semi-supervised protein

classiﬁcation using cluster kernels. Bioinformatics  21(15):3241–3247  2005.

[32] J. Weston  C. Leslie  D. Zhou  A. Elisseeff  and W. Noble. Semi-supervised protein classiﬁcation
using cluster kernels. In Advances in Neural Information Processing Systems  NIPS  pages
595–602. MIT Press  2004.

[33] C. K. I. Williams and M. Seeger. Using the nyström method to speed up kernel machines. In

Advances in Neural Information Processing Systems  NIPS  pages 661–667  2000.

[34] T. Yang  Y.-F. Li  M. Mahdavi  R. Jin  and Z.-H. Zhou. Nyström method vs random fourier
features: A theoretical and empirical comparison. In Advances in Neural Information Processing
Systems  NIPS  pages 476–484  2012.

11

,Muhammad Farhan
Juvaria Tariq
Arif Zaman
Mudassir Shabbir
Imdad Ullah Khan