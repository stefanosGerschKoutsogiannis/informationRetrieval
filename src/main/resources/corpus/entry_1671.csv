2019,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup,Deep neural networks achieve stellar generalisation even when they have enough
parameters to easily fit all their training data. We study this phenomenon by
analysing the dynamics and the performance of over-parameterised two-layer
neural networks in the teacher-student setup  where one network  the student 
is trained on data generated by another network  called the teacher. We show
how the dynamics of stochastic gradient descent (SGD) is captured by a set of
differential equations and prove that this description is asymptotically exact
in the limit of large inputs. Using this framework  we calculate the final
generalisation error of student networks that have more parameters than their
teachers. We find that the final generalisation error of the student increases
with network size when training only the first layer  but stays constant or
even decreases with size when training both layers. We show that these
different behaviours have their root in the different solutions SGD finds for
different activation functions. Our results indicate that achieving good
generalisation in neural networks goes beyond the properties of SGD alone and
depends on the interplay of at least the algorithm  the model architecture 
and the data set.,Dynamics of stochastic gradient descent for two-layer

neural networks in the teacher-student setup

Sebastian Goldt1  Madhu S. Advani2  Andrew M. Saxe3

Florent Krzakala4  Lenka Zdeborová1

1 Institut de Physique Théorique  CNRS  CEA  Université Paris-Saclay  Saclay  France

2 Center for Brain Science  Harvard University  Cambridge  MA 02138  USA

3 Department of Experimental Psychology  University of Oxford  Oxford  United Kingdom

4 Laboratoire de Physique Statistique  Sorbonne Universités 

Université Pierre et Marie Curie Paris 6  Ecole Normale Supérieure  75005 Paris  France

Abstract

Deep neural networks achieve stellar generalisation even when they have enough
parameters to easily ﬁt all their training data. We study this phenomenon by
analysing the dynamics and the performance of over-parameterised two-layer
neural networks in the teacher-student setup  where one network  the student  is
trained on data generated by another network  called the teacher. We show how the
dynamics of stochastic gradient descent (SGD) is captured by a set of differential
equations and prove that this description is asymptotically exact in the limit of
large inputs. Using this framework  we calculate the ﬁnal generalisation error of
student networks that have more parameters than their teachers. We ﬁnd that the
ﬁnal generalisation error of the student increases with network size when training
only the ﬁrst layer  but stays constant or even decreases with size when training
both layers. We show that these different behaviours have their root in the different
solutions SGD ﬁnds for different activation functions. Our results indicate that
achieving good generalisation in neural networks goes beyond the properties of
SGD alone and depends on the interplay of at least the algorithm  the model
architecture  and the data set.

Deep neural networks behind state-of-the-art results in image classiﬁcation and other domains
have one thing in common: their size. In many applications  the free parameters of these models
outnumber the samples in their training set by up to two orders of magnitude 1 2. Statistical learning
theory suggests that such heavily over-parameterised networks generalise poorly without further
regularisation 3–9  yet empirical studies consistently ﬁnd that increasing the size of networks to
the point where they can easily ﬁt their training data and beyond does not impede their ability to
generalise well  even without any explicit regularisation 10–12. Resolving this paradox is arguably one
of the big challenges in the theory of deep learning.
One tentative explanation for the success of large networks has focused on the properties of stochastic
gradient descent (SGD)  the algorithm routinely used to train these networks. In particular  it has
been proposed that SGD has an implicit regularisation mechanism that ensures that solutions found
by SGD generalise well irrespective of the number of parameters involved  for models as diverse as
(over-parameterised) neural networks 10 13  logistic regression 14 and matrix factorisation models 15 16.
In this paper  we analyse the dynamics of one-pass (or online) SGD in two-layer neural networks. We
focus in particular on the inﬂuence of over-parameterisation on the ﬁnal generalisation error. We use
the teacher-student framework 17 18  where a training data set is generated by feeding random inputs
through a two-layer neural network with M hidden units called the teacher. Another neural network 
the student  is then trained using SGD on that data set. The generalisation error is deﬁned as the mean

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

squared error between teacher and student outputs  averaged over all of input space. We will focus on
student networks that have a larger number of hidden units K ≥ M than their teacher. This means
that the student can express much more complex functions than the teacher function they have to
learn; the students are thus over-parameterised with respect to the generative model of the training
data in a way that is simple to quantify. We ﬁnd this deﬁnition of over-parameterisation cleaner in our
setting than the oft-used comparison of the number of parameters in the model with the number of
samples in the training set  which is not well justiﬁed for non-linear functions. Furthermore  these two
numbers surely cannot fully capture the complexity of the function learned in practical applications.
The teacher-student framework is also interesting in the wake of the need to understand the ef-
fectiveness of neural networks and the limitations of the classical approaches to generalisation 11.
Traditional approaches to learning and generalisation are data agnostic and seek worst-case type
bounds 19. On the other hand  there has been a considerable body of theoretical work calculating
the generalisation ability of neural networks for data arising from a probabilistic model  particularly
within the framework of statistical mechanics 17 18 20–22. Revisiting and extending the results that have
emerged from this perspective is currently experiencing a surge of interest 23–28.
In this work we consider two-layer networks with a large input layer and a ﬁnite  but arbitrary  number
of hidden neurons. Other limits of two-layer neural networks have received a lot of attention recently.
A series of papers 29–32 studied the mean-ﬁeld limit of two-layer networks  where the number of
neurons in the hidden layer is very large  and proved various general properties of SGD based on
a description in terms of a limiting partial differential equation. Another set of works  operating in
a different limit  have shown that inﬁnitely wide over-parameterised neural networks trained with
gradient-based methods effectively solve a kernel regression 33–38  without any feature learning. Both
the mean-ﬁeld and the kernel regime crucially rely on having an inﬁnite number of nodes in the
hidden layer  and the performance of the networks strongly depends on the detailed scaling used 38 39.
Furthermore  a very wide hidden layer makes it hard to have a student that is larger than the teacher
in a quantiﬁable way. This leads us to consider the opposite limit of large input dimension and ﬁnite
number of hidden units.
Our main contributions are as follows:
(i) The dynamics of SGD (online) learning by two-layer neural networks in the teacher-student setup
was studied in a series of classic papers 40–44 from the statistical physics community  leading to a
heuristic derivation of a set of coupled ordinary differential equations (ODE) that describe the typical
time-evolution of the generalisation error. We provide a rigorous foundation of the ODE approach to
analysing the generalisation dynamics in the limit of large input size by proving their correctness.
(ii) These works focused on training only the ﬁrst layer  mainly in the case where the teacher network
has the same number of hidden units and the student network  K = M. We generalise their analysis
to the case where the student’s expressivity is considerably larger than that of the teacher in order to
investigate the over-parameterised regime K > M.
(iii) We provide a detailed analysis of the dynamics of learning and of the generalisation when only
the ﬁrst layer is trained. We derive a reduced set of coupled ODE that describes the generalisation
dynamics for any K ≥ M and obtain analytical expressions for the asymptotic generalisation error
of networks with linear and sigmoidal activation functions. Crucially  we ﬁnd that with all other
parameters equal  the ﬁnal generalisation error increases with the size of the student network. In this
case  SGD alone thus does not seem to be enough to regularise larger student networks.
(iv) We ﬁnally analyse the dynamics when learning both layers. We give an analytical expression for
the ﬁnal generalisation error of sigmoidal networks and ﬁnd evidence that suggests that SGD ﬁnds
solutions which amount to performing an effective model average  thus improving the generalisation
error upon over-parameterisation. In linear and ReLU networks  we experimentally ﬁnd that the
generalisation error does change as a function of K when training both layers. However  there exist
student networks with better performance that are ﬁxed points of the SGD dynamics  but are not
reached when starting SGD from initial conditions with small  random weights.
Crucially  we ﬁnd this range of different behaviours while keeping the training algorithm (SGD)
the same  changing only the activation functions of the networks and the parts of the network that
are trained. Our results clearly indicate that the implicit regularisation of neural networks in our
setting goes beyond the properties of SGD alone. Instead  a full understanding of the generalisation
properties of even very simple neural networks requires taking into account the interplay of at least

2

the algorithm  the network architecture  and the data set used for training  setting up a formidable
research programme for the future.
Reproducibility — We have packaged the implementation of our experiments and our ODE integrator
into a user-friendly library with example programs at https://github.com/sgoldt/nn2pp. All
plots were generated with these programs  and we give the necessary parameter values beneath each
plot.

1 Online learning in teacher-student neural networks
We consider a supervised regression problem with training set D = {(xµ  yµ)} with µ = 1  . . .   P .
The components of the inputs xµ ∈ RN are i.i.d. draws from the standard normal distribution N (0  1).
The scalar labels yµ are given by the output of a network with M hidden units  a non-linear activation
function g : R → R and ﬁxed weights θ∗ = (v∗ ∈ RM   w∗ ∈ RM×N ) with an additive output noise
ζ µ ∼ N (0  1)  called the teacher (see also Fig. 1a):

yµ ≡ φ(xµ  θ

∗

) + σζ µ 

where φ(x  θ

∗

) =

v

∗

mg(cid:18) w∗

N (cid:19) =

mx√

M(cid:88)m=1

M(cid:88)m

∗
mg(ρm)  

v

(1)
√

m is the mth row of w∗  and the local ﬁeld of the mth teacher node is ρm ≡ w∗

where w∗
We will analyse three different network types: sigmoidal with g(x) = erf(x/
g(x) = max(x  0)  and linear networks where g(x) = x.
A second two-layer network with K hidden units and weights θ = (v ∈ RK  w ∈ RK×N )  called
µ=1 [φ(xµ  θ) − yµ]2.
We emphasise that the student network may have a larger number of hidden units K ≥ M than the
teacher and thus be over-parameterised with respect to the generative model of its training data.
The SGD algorithm deﬁnes a Markov process X µ ≡ [v∗  w∗  vµ  wµ] with update rule given by the
coupled SGD recursion relations

the student  is then trained using SGD on the quadratic training loss E(θ) ∝(cid:80)P

N.
2)  ReLU with

mx/

√

vµ
k g

(cid:48)

(λµ

k )∆µxµ 

wµ+1
k = wµ

vµ+1
k = vµ

k − ηw√
k − ηv

N

N
g(λµ

k )∆µ.

(2)

(3)

and we deﬁned the error term ∆µ ≡(cid:80)k vµ

We can choose different learning rates ηv and ηw for the two layers and denote by g(cid:48)(λµ
of the activation function evaluated at the local ﬁeld of the student’s kth hidden unit λµ

√
k ) the derivative
k ≡ wkxµ/
N 
m) − σζ µ. We will use the indices
√
i  j  k  . . . to refer to student nodes  and n  m  . . . to denote teacher nodes. We take initial weights at
random from N (0  1) for sigmoidal networks  while initial weights have variance 1/
N for ReLU
and linear networks.
The key quantity in our approach is the generalisation error of the student with respect to the teacher:

k ) −(cid:80)m v∗

k g (λµ

mg(ρµ

g(θ  θ

(4)
where the angled brackets (cid:104)·(cid:105) denote an average over the input distribution. We can make progress by
realising that g(θ∗  θ) can be expressed as a function of a set of macroscopic variables  called order
parameters in statistical physics  21 40 41

2(cid:68)[φ(x  θ) − φ(x  θ

∗

)]2(cid:69)  

∗

) ≡ 1

ik ≡ wµ
Qµ

i wµ
N

k

in ≡ wµ

i w∗
N

and Tnm ≡ w∗

nw∗
N

n

  Rµ

(5)
together with the second-layer weights v∗ and vµ. Intuitively  the teacher-student overlaps Rµ =
[Rµ
in] measure the similarity between the weights of the ith student node and the nth teacher node.
The matrix Qik quantiﬁes the overlap of the weights of different student nodes with each other  and
the corresponding overlap of the teacher nodes are collected in the matrix Tnm. We will ﬁnd it
convenient to collect all order parameters in a single vector
∗

m

 

mµ ≡ (Rµ  Qµ  T  v

  vµ) 

(6)

3

and we write the full expression for g(mµ) in the SM  Eq. (S31).
In a series of classic papers  Biehl  Schwarze  Saad  Solla and Riegler 40–44 derived a closed set of
ordinary differential equations for the time evolution of the order parameters m (see SM Sec. B).
Together with the expression for the generalisation error g(mµ)  these equations give a complete
description of the generalisation dynamics of the student  which they analysed for the special case
K = M when only the ﬁrst layer is trained 42 44. Our ﬁrst contribution is to provide a rigorous
foundation for these results under the following assumptions:

(A1) Both the sequences xµ and ζ µ  µ = 1  2  . . .  are i.i.d. random variables; xµ is drawn from a
normal distribution with mean 0 and covariance matrix IN   while ζ µ is a Gaussian random
variable with mean zero and unity variance;

(A2) The function g(x) is bounded and its derivatives up to and including the second order exist and

are bounded  too;

(A3) The initial macroscopic state m0 is deterministic and bounded by a constant;
(A4) The constants σ  K  M  ηw and ηv are all ﬁnite.

The correctness of the ODE description is then established by the following theorem:
Theorem 1.1. Choose T > 0 and deﬁne α ≡ µ/N. Under assumptions (A1) – (A4)  and for any
α > 0  the macroscopic state mµ satisﬁes

max

0≤µ≤N T

E ||mµ − m(α)|| ≤ C(T )√
N

 

(7)

where C(T ) is a constant depending on T   but not on N  and m(α) is the unique solution of the
ODE

with initial condition m∗. In particular  we have
(λi)ρn(cid:105)  
(λi)λk(cid:105) + ηvk(cid:104)∆g

≡ fR(m(α)) = ηvi(cid:104)∆g
≡ fQ(m(α)) = ηvi(cid:104)∆g

dRin
dα
dQik
dα

(cid:48)

(λk)λi(cid:105)

m(α) = f (m(α))

d
dt

(cid:48)

(cid:48)

+ η2vivk(cid:104)∆2g

(cid:48)

(cid:48)

(λk)(cid:105) + η2vivkσ2(cid:104)g

(cid:48)

(λi)g

(cid:48)

(λk)(cid:105)  

(λi)g

≡ fv(m(α)) = ηv(cid:104)∆g(λi)(cid:105).

dvi
dα

(8)

(9a)

(9b)

(9c)

where all f (m(α)) are uniformly Lipschitz continuous in m(α). We are able to close the equations
because we can express averages in Eq. (9) in terms of only m(α).

We prove Theorem 1.1 using the theory of convergence of stochastic processes and a coupling trick
introduced recently by Wang et al. 45 in Sec. A of the SM. The content of the theorem is illustrated in
Fig. 1b  where we plot g(α) obtained by numerically integrating (9) (solid) and from a single run of
SGD (2) (crosses) for sigmoidal students and varying K  which are in very good agreement.
Given a set of non-linear  coupled ODE such as Eqns. (9)  ﬁnding the asymptotic ﬁxed points
analytically to compute the generalisation error would seem to be impossible. In the following  we
will therefore focus on analysing the asymptotic ﬁxed points found by numerically integrating the
equations of motion. The form of these ﬁxed points will reveal a drastically different dependence of
the test error on the over-parameterisation of neural networks with different activation functions in
the different setups we consider  despite them all being trained by SGD. This highlights the fact that
good generalisation goes beyond the properties of just the algorithm. Second  knowledge of these
ﬁxed points allows us to make analytical and quantitative predictions for the asymptotic performance
of the networks which agree well with experiments. We also note that several recent theorems 29–31
about the global convergence of SGD do not apply in our setting because we have a ﬁnite number of
hidden units.

4

Figure 1: The analytical description of the generalisation dynamics of sigmoidal networks
matches experiments. (a) We consider two-layer neural networks with a very large input layer.
(b) We plot the learning dynamics g(α) obtained by integration of the ODEs (9) (solid) and from a
single run of SGD (2) (crosses) for students with different numbers of hidden units K. The insets
show the values of the teacher-student overlaps Rin (5) for a student with K = 4 at the two times
indicated by the arrows. N = 784  M = 4  η = 0.2.

2 Asymptotic generalisation error of Soft Committee machines
We will ﬁrst study networks where the second layer weights are ﬁxed at v∗
m = vk = 1. These networks
are called a Soft Committee Machine (SCM) in the statistical physics literature 18 27 40–42 44. One
notable feature of g(α) in SCMs is the existence of a long plateau with sub-optimal generalisation
error during training. During this period  all student nodes have roughly the same overlap with all
the teacher nodes  Rin = const. (left inset in Fig. 1b). As training continues  the student nodes
“specialise” and each of them becomes strongly correlated with a single teacher node (right inset) 
leading to a sharp decrease in g. This effect is well-known for both batch and online learning 18 and
will be key for our analysis.
Let us now use the equations of motion (9) to analyse the asymptotic generalisation error of neural
g after training has converged and in particular its scaling with L = K − M. Our ﬁrst
networks ∗
contribution is to reduce the remaining K(K + M ) equations of motion to a set of eight coupled
differential equations for any combination of K and M in Sec. C. This enables us to obtain a
closed-form expression for ∗
In the absence of output noise (σ = 0)  the generalisation error of a student with K ≥ M will
asymptotically tend to zero as α → ∞. On the level of the order parameters  this corresponds to
reaching a stable ﬁxed point of (9) with g = 0. In the presence of small output noise σ > 0  this
ﬁxed point becomes unstable and the order parameters instead converge to another  nearby ﬁxed
point m∗ with g(m∗) > 0. The values of the order parameters at that ﬁxed point can be obtained by
perturbing Eqns. (9) to ﬁrst order in σ  and the corresponding generalisation error g(m∗) turns out
to be in excellent agreement with the generalisation error obtained when training a neural network
using (2) from random initial conditions  which we show in Fig. 2a.

g as follows.

√

Sigmoidal networks. We have performed this calculation for teacher and student networks with
2). We relegate the details to Sec. C.2  and content us here to state the asymptotic
g(x) = erf(x/
value of the generalisation error to ﬁrst order in σ2 

∗
g =



σ2η
2π

f (M  L  η) + O(σ3) 

(10)

where f (M  L  η) is a lengthy rational function of its variables. We plot our result in Fig. 2a together
with the ﬁnal generalisation error obtained in a single run of SGD (2) for a neural network with initial
weights drawn i.i.d. from N (0  1) and ﬁnd excellent agreement  which we conﬁrmed for a range of
values for η  σ  and L.
One notable feature of Fig. 2a is that with all else being equal  SGD alone fails to regularise the
student networks of increasing size in our setup  instead yielding students whose generalisation error
increases linearly with L. One might be tempted to mitigate this effect by simultaneously decreasing
the learning rate η for larger students. However  lowering the learning rate incurs longer training

5

...(a)xwgK...g2g1gk=g(wkx)vφφ=PkvkgkFigure 2: The asymptotic generalisation error of Soft Committee Machines increases with the
network size. N = 784  η = 0.05  σ = 0.01. (a) Our theoretical prediction for ∗
g/σ2 for sigmoidal
(solid) and linear (dashed)  Eqns. (10) and (12)  agree perfectly with the result obtained from a single
run of SGD (2) starting from random initial weights (crosses). (b) The ﬁnal overlap matrices Q and
R (5) at the end of an experiment with M = 2  K = 5. Networks with sigmoidal activation function
(top) show clear signs of specialisation as described in Sec. 2. ReLU networks (bottom) instead
converge to solutions where all of the student’s nodes have ﬁnite overlap with teacher nodes.

times  which requires more data for online learning. This trade-off is also found in statistical learning
theory  where models with more parameters (higher L) and thus a higher complexity class (e.g. VC
dimension or Rademacher complexity 4) generalise just as well as smaller ones when given more data.
In practice  however  more data might not be readily available  and we show in Fig. S2 of the SM that
even when choosing η = 1/K  the generalisation error still increases with L before plateauing at a
constant value.
We can gain some intuition for the scaling of ∗
g by considering the asymptotic overlap matrices Q
and R shown in the left half of Fig. 2b. In the over-parameterised case  L = K − M student nodes
are effectively trying to specialise to teacher nodes which do not exist  or equivalently  have weights
zero. These L student nodes do not carry any information about the teachers output  but they pick up
ﬂuctuations from output noise and thus increase ∗
g. This intuition is borne out by an expansion of ∗
g
in the limit of small learning rate η  which yields

∗
g =



σ2η

2π (cid:18)L +

M√

3(cid:19) + O(η2) 

(11)

which is indeed the sum of the error of M independent hidden units that are specialised to a single
teacher hidden unit  and L = K − M superﬂuous units contributing each the error of a hidden unit
that is “learning” from a hidden unit with zero weights w∗
g ∼ L in sigmoidal networks may
Linear networks. Two possible explanations for the scaling ∗
be the specialisation of the hidden units or the fact that teacher and student network can implement
functions of different range if K (cid:54)= M. To test these hypotheses  we calculated ∗
g for linear neural
networks 46 47 with g(x) = x. Linear networks lack a specialisation transition 27 and their output
range is set by the magnitude of their weights  rather than their number of hidden units. Following
the same steps as before  a perturbative calculation in the limit of small noise variance σ2 yields

m = 0 (see also Sec. D of the SM).

∗
g =



ησ2(L + M )
4 − 2η(L + M )

+ O(σ3).

(12)

This result is again in perfect agreement with experiments  as we demonstrate in Fig. 2a. In the limit
of small learning rates η  Eq. (10) simpliﬁes to yield the same scaling as for sigmoidal networks 

(13)
g ∼ L is not just a consequence of either specialisation or the mismatched
This shows that the scaling ∗
range of the networks’ output functions. The optimal number of hidden units for linear networks
is K = 1 for all M  because linear networks implement an effective linear transformation with an

ησ2(L + M ) + O(cid:0)η2(cid:1) .

∗
g =

1
4



6

051015L101*g/2M=4M=8i n n i (b)Figure 3: The performance of sigmoidal networks improves with network size when training
both layers with SGD. (a) Generalisation dynamics observed experimentally for students with
increasing K  with all other parameters being equal. (N = 500  M = 2  η = 0.05  σ = 0.01  v∗ = 4).
(b) Overlap matrices Q  R  and second layer weights vk of the student at the end of the run with
g (solid) against ∗
K = 5 shown in (a). (c) Theoretical prediction for ∗
g observed after integration of
the ODE until convergence (crosses) (9) (σ = 0.01  η = 0.2  v∗ = 2).

effective matrix W =(cid:80)k wk. Adding hidden units to a linear network hence does not augment the

class of functions it can implement  but it adds redundant parameters which pick up ﬂuctuations from
the teacher’s output noise  increasing g.
ReLU networks. The analytical calculation of ∗
g  described above  for ReLU networks poses some
additional technical challenges  so we resort to experiments to investigate this case. We found that
the asymptotic generalisation error of a ReLU student learning from a ReLU teacher has the same
scaling as the one we found analytically for networks with sigmoidal and linear activation functions:
g ∼ ησ2L (see Fig. S3). Looking at the ﬁnal overlap matrices Q and R for ReLU networks in the
∗
bottom half of Fig. 2b  we see that instead of the one-to-one specialisation of sigmoidal networks  all
student nodes have a ﬁnite overlap with some teacher node. This is a consequence of the fact that it is
much simpler to re-express the sum of M ReLU units with K (cid:54)= M ReLU units. However  there
are still a lot of redundant degrees of freedom in the student  which all pick up ﬂuctuations from the
teacher’s output noise and increase ∗
g.

Discussion. The key result of this section has been that the generalisation error of SCMs scales as

g ∼ ησ2L.
∗



(14)

Before moving on the full two-layer network  we discuss a number of experiments that we performed
to check the robustness of this result (Details can be found in Sec. G of the SM). A standard
regularisation method is adding weight decay to the SGD updates (2). However  we did not ﬁnd a
scenario in our experiments where weight decay improved the performance of a student with L > 0.
We also made sure that our results persist when performing SGD with mini-batches. We investigated
the impact of higher-order correlations in the inputs by replacing Gaussian inputs with MNIST
images  with all other aspects of our setup the same  and the same g-L curve as for Gaussian inputs.
Finally  we analysed the impact of having a ﬁnite training set. The behaviour of linear networks and
of non-linear networks with large but ﬁnite training sets did not change qualitatively. However  as
we reduce the size of the training set  we found that the lowest asymptotic generalisation error was
obtained with networks that have K > M.

3 Training both layers: Asymptotic generalisation error of a neural network

We now study the performance of two-layer neural networks when both layers are trained according
to the SGD updates (2) and (3). We set all the teacher weights equal to a constant value  v∗
m = v∗ 
to ensure comparability between experiments. However  we train all K second-layer weights of the
student independently and do not rely on the fact that all second-layer teacher weights have the same
value. Note that learning the second layer is not needed from the point of view of statistical learning:
the networks from the previous section are already expressive enough to capture the students  and
we are thus slightly increasing the over-parameterisation even further. Yet  we will see that the
generalisation properties will be signiﬁcantly enhanced.

7

101101103105steps / K / N105104103102101100101g(a)K=1K=2K=3K=4K=5K=6(b)123Z=K/M101*g/2(c)M=1M=2M=3Sigmoidal networks. We plot the generalisation dynamics of students with increasing K trained
on a teacher with M = 2 in Fig. 3a. Our ﬁrst observation is that increasing the student size K ≥ M
decreases the asymptotic generalisation error ∗
g  with all other parameters being equal  in stark
contrast to the SCMs of the previous section.
A look at the order parameters after convergence in the experiments from Fig. 3a reveals the intriguing
pattern of specialisation of the student’s hidden units behind this behaviour  shown for K = 5 in
Fig. 3b. First  note that all the hidden units of the student have non-negligible weights (Qii > 0). Two
student nodes (k = 1  2) have specialised to the ﬁrst teacher node  i.e. their weights are very close to
the weights of the ﬁrst teacher node (R10 ≈ R20 ≈ 0.85). The corresponding second-layer weights
approximately fulﬁl v1 + v3 ≈ v∗. Summing the output of these two student hidden units is thus
approximately equivalent to an empirical average of two estimates of the output of the teacher node.
The remaining three student nodes all specialised to the second teacher node  and their outgoing
weights approximately sum to v∗. This pattern suggests that SGD has found a set of weights for
both layers where the student’s output is a weighted average of several estimates of the output of the
teacher’s nodes. We call this the denoising solution and note that it resembles the solutions found in
the mean-ﬁeld limit of an inﬁnite hidden layer 29 31 where the neurons become redundant and follow
a distribution dynamics (in our case  a simple one with few peaks  as e.g. Fig. 1 in 31).
We conﬁrmed this intuition by using an ansatz for the order parameters that corresponds to a denoising
solution to solve the equations of motion (9) perturbatively in the limit of small noise to calculate
∗
g for sigmoidal networks after training both layers  similarly to the approach in Sec. 2. While this
approach can be extended to any K and M  we focused on the case where K = ZM to obtain
manageable expressions; see Sec. E of the SM for details on the derivation. While the ﬁnal expression
is again too long to be given here  we plot it with solid lines in Fig. 3c. The crosses in the same plot
are the asymptotic generalisation error obtained by integration of the ODE (9) starting from random
initial conditions  and show very good agreement.
While our result holds for any M  we note from Fig. 3c that the curves for different M are qualitatively
similar. We ﬁnd a particular simple result for M = 1 in the limit of small learning rates  where:

∗
g =



η(σv∗)2
√
3Kπ
2

+ O(ησ2) .

(15)

This result should be contrasted with the g ∼ K behaviour found for SCM.
Experimentally  we robustly observed that training both layers of the network yields better per-
formance than training only the ﬁrst layer with the second layer weights ﬁxed to v∗. However 
convergence to the denoising solution can be difﬁcult for large students which might get stuck on a
long plateau where their nodes are not evenly distributed among the teacher nodes. While it is easy to
check that such a network has a higher value of g than the denoising solution  the difference is small 
and hence the driving force that pushes the student out of the corresponding plateaus is small  too.
These observations demonstrate that in our setup  SGD does not always ﬁnd the solution with the
lowest generalisation error in ﬁnite time.

ReLU and linear networks. We found experimentally that ∗
g remains constant with increasing K
in ReLU and in linear networks when training both layers. We plot a typical learning curve in green
for linear networks in Fig. 4  but note that the ﬁgure shows qualitatively similar features for ReLU
networks (Fig. S4). This behaviour was also observed in linear networks trained by batch gradient
descent  starting from small initial weights 48. While this scaling of ∗
g with K is an improvement
over its increase with K for the SCM  (blue curve)  this is not the 1/K decay that we observed for
sigmoidal networks. A possible explanation is the lack of specialisation in linear and ReLU networks
(see Sec. 2)  without which the denoising solution found in sigmoidal networks is not possible. We
also considered normalised SCM  where we train only the ﬁrst layer and ﬁx the second-layer weights
at v∗
m = 1/M and vk = 1/K. The asymptotic error of normalised SCM decreases with K (orange
curve in Fig. 4)  because the second-layer weights vk = 1/K effectively reduce the learning rate 
as can be easily seen from the SGD updates (2)  and we know from our analysis of linear SCM in
Sec. 2 that g ∼ η. In SM Sec. F we show analytically how imbalance in the norms of the ﬁrst and
second layer weights can lead to a larger effective learning rate. Normalised SCM also beat the
performance students where we trained both layers  starting from small initial weights in both cases.
This is surprising because we checked experimentally that the weights of a normalised SCM after

8

training are a ﬁxed point of the SGD dynamics when training both layers. However  we conﬁrmed
experimentally that SGD does not ﬁnd this ﬁxed point when starting with random initial weights.

Discussion. The qualitative difference be-
tween training both or only the ﬁrst layer of
neural networks is particularly striking for lin-
ear networks  where ﬁxing one layer does not
change the class of functions the model can im-
plement  but makes a dramatic difference for
their asymptotic performance. This observation
highlights two important points: ﬁrst  the per-
formance of a network is not just determined by
the number of additional parameters  but also
by how the additional parameters are arranged
in the model. Second  the non-linear dynamics
of SGD means that changing which weights are
trainable can alter the training dynamics in un-
expected ways. We saw this for two-layer linear
networks  where SGD did not ﬁnd the optimal
ﬁxed point  and in the non-linear sigmoidal net-
works  where training the second layer allowed
the student to decrease its ﬁnal error with every
additional hidden unit instead of increasing it
like in the SCM.

Acknowledgements

Figure 4: Asymptotic performance of linear two
layer network. Error bars indicate one standard de-
viation over ﬁve runs. Parameters: N = 100  M =
4  v∗ = 1  η = 0.01  σ = 0.01.

SG and LZ acknowledge funding from the ERC under the European Union’s Horizon 2020 Research
and Innovation Programme Grant Agreement 714608-SMiLe. MA thanks the Swartz Program in
Theoretical Neuroscience at Harvard University for support. AS acknowledges funding by the
European Research Council  grant 725937 NEUROABSTRACTION. FK acknowledges support from
“Chaire de recherche sur les modèles et sciences des données”  Fondation CFM pour la Recherche-
ENS  and from the French National Research Agency (ANR) grant PAIL.

References
[1] Y. LeCun  Y. Bengio  and G.E. Hinton. Deep learning. Nature  521(7553):436–444  2015.

[2] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image

Recognition. In International Conference on Learning Representations  2015.

[3] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(3):463–482  2003.

[4] M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of Machine Learning. MIT Press 

2012.

[5] B. Neyshabur  R. Tomioka  and N. Srebro. Norm-Based Capacity Control in Neural Networks.

In Conference on Learning Theory  2015.

[6] N. Golowich  A. Rakhlin  and O. Shamir. Size-independent sample complexity of neural

networks. Information and Inference: A Journal of the IMA  2019.

[7] G.K. Dziugaite and D.M. Roy. Computing Nonvacuous Generalization Bounds for Deep
(Stochastic) Neural Networks with Many More Parameters than Training Data. In Proceedings
of the Thirty-Third Conference on Uncertainty in Artiﬁcial Intelligence  2017.

[8] S. Arora  R. Ge  B. Neyshabur  and Y. Zhang. Stronger generalization bounds for deep nets via
a compression approach. In 35th International Conference on Machine Learning  ICML 2018 
pages 390–418  2018.

9

101102103K104103102*g/2SCMNormalisedBoth[9] Z. Allen-Zhu  Y. Li  and Y. Liang. Learning and Generalization in Overparameterized Neural

Networks  Going Beyond Two Layers. arXiv:1811.04918  2018.

[10] B. Neyshabur  R. Tomioka  and N. Srebro. In search of the real inductive bias: On the role of

implicit regularization in deep learning. In ICLR  2015.

[11] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires

rethinking generalization. In ICLR  2017.

[12] D. Arpit  S. Jastrz  M.S. Kanwal  T. Maharaj  A. Fischer  A. Courville  and Y. Bengio. A Closer
Look at Memorization in Deep Networks. In Proceedings of the 34th International Conference
on Machine Learning  2017.

[13] P. Chaudhari and S. Soatto. On the inductive bias of stochastic gradient descent. In International

Conference on Learning Representations  2018.

[14] D. Soudry  E. Hoffer  and N. Srebro. The implicit bias of gradient descent on separable data. In

International Conference on Learning Representations  2018.

[15] S. Gunasekar  B. Woodworth  S. Bhojanapalli  B. Neyshabur  and N. Srebro. Implicit Regu-
larization in Matrix Factorization. In Advances in Neural Information Processing Systems 30 
pages 6151–6159  2017.

[16] Y. Li  T. Ma  and H. Zhang. Algorithmic Regularization in Over-parameterized Matrix Sensing
and Neural Networks with Quadratic Activations. In Conference on Learning Theory  pages
2–47  2018.

[17] H. S. Seung  H. Sompolinsky  and N. Tishby. Statistical mechanics of learning from examples.

Physical Review A  45(8):6056–6091  1992.

[18] A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University

Press  2001.

[19] V. Vapnik. Statistical learning theory. New York  pages 156–160  1998.

[20] E. Gardner and B. Derrida. Three unﬁnished works on the optimal storage capacity of networks.

Journal of Physics A: Mathematical and General  22(12):1983–1994  1989.

[21] W. Kinzel  P. Ruján  and P. Rujan. Improving a Network Generalization Ability by Selecting

Examples. EPL (Europhysics Letters)  13(5):473–477  1990.

[22] T.L.H. Watkin  A. Rau  and M. Biehl. The statistical mechanics of learning a rule. Reviews of

Modern Physics  65(2):499–556  1993.

[23] L. Zdeborová and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Adv.

Phys.  65(5):453–552  2016.

[24] M.S. Advani and S. Ganguli. Statistical mechanics of optimal convex inference in high

dimensions. Physical Review X  6(3):1–16  2016.

[25] P. Chaudhari  A. Choromanska  S. Soatto  Y. LeCun  C. Baldassi  C. Borgs  J. Chayes  L. Sagun 
and R. Zecchina. Entropy-SGD: Biasing Gradient Descent Into Wide Valleys. In ICLR  2017.

[26] M.S. Advani and A.M. Saxe. High-dimensional dynamics of generalization error in neural

networks. arXiv:1710.03667  2017.

[27] B. Aubin  A. Maillard  J. Barbier  F. Krzakala  N. Macris  and L. Zdeborová. The committee
machine: Computational to statistical gaps in learning a two-layers neural network. In Advances
in Neural Information Processing Systems 31  pages 3227–3238  2018.

[28] M. Baity-Jesi  L. Sagun  M. Geiger  S. Spigler  G.B. Arous  C. Cammarota  Y. LeCun  M. Wyart 
In

and G. Biroli. Comparing Dynamics: Deep Neural Networks versus Glassy Systems.
Proceedings of the 35th International Conference on Machine Learning  2018.

10

[29] S. Mei  A. Montanari  and P. Nguyen. A mean ﬁeld view of the landscape of two-layer neural

networks. Proceedings of the National Academy of Sciences  115(33):E7665–E7671  2018.

[30] G.M. Rotskoff and E. Vanden-Eijnden. Parameters as interacting particles: long time conver-
gence and asymptotic error scaling of neural networks. In Advances in Neural Information
Processing Systems 31  pages 7146–7155  2018.

[31] L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in Neural Information Processing Systems 31 
pages 3040–3050  2018.

[32] J. Sirignano and K. Spiliopoulos. Mean ﬁeld analysis of neural networks: A central limit

theorem. Stochastic Processes and their Applications  2019.

[33] A. Jacot  F. Gabriel  and C. Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In Advances in Neural Information Processing Systems 32  pages 8571–8580 
2018.

[34] S.S. Du  X. Zhai  B. Poczos  and A. Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations 
2019.

[35] Z. Allen-Zhu  Y. Li  and Z. Song. A convergence theory for deep learning via over-

parameterization. arXiv preprint arXiv:1811.03962  2018.

[36] Y. Li and Y. Liang. Learning Overparameterized Neural Networks via Stochastic Gradient
Descent on Structured Data. In Advances in Neural Information Processing Systems 31  2018.

[37] D. Zou  Y. Cao  D. Zhou  and Q. Gu. Stochastic gradient descent optimizes over-parameterized

deep relu networks. Machine Learning  pages 1–26  2019.

[38] L. Chizat  E. Oyallon  and F. Bach. On lazy training in differentiable programming. In Advances

in Neural Information Processing Systems 33  page forthcoming  2019.

[39] S. Mei  T. Misiakiewicz  and A. Montanari. Mean-ﬁeld theory of two-layers neural networks:

dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015  2019.

[40] M. Biehl and H. Schwarze. Learning by on-line gradient descent. J. Phys. A. Math. Gen. 

28(3):643–656  1995.

[41] D. Saad and S.A. Solla. Exact Solution for On-Line Learning in Multilayer Neural Networks.

Phys. Rev. Lett.  74(21):4337–4340  1995.

[42] D. Saad and S.A. Solla. On-line learning in soft committee machines. Phys. Rev. E  52(4):4225–

4243  1995.

[43] P. Riegler and M. Biehl. On-line backpropagation in two-layered neural networks. Journal of

Physics A: Mathematical and General  28(20)  1995.

[44] D. Saad and S.A. Solla. Learning with Noise and Regularizers Multilayer Neural Networks. In

Advances in Neural Information Processing Systems 9  pages 260–266  1997.

[45] C. Wang  Hong Hu  and Yue M. Lu. A Solvable High-Dimensional Model of GAN.

arXiv:1805.08349  2018.

[46] A. Krogh and J. A. Hertz. Generalization in a linear perceptron in the presence of noise. Journal

of Physics A: Mathematical and General  25(5):1135–1147  1992.

[47] A.M. Saxe  James L. McClelland  and S. Ganguli. Exact solutions to the nonlinear dynamics of

learning in deep linear neural networks. In ICLR  2014.

[48] A.K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In International Conference on Learning Representations 
2019.

11

,Sebastian Goldt
Madhu Advani
Andrew Saxe
Florent Krzakala
Lenka Zdeborová