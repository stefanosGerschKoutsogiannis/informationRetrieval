2012,Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing,Statistical features of neuronal spike trains are known to be non-Poisson. Here  we investigate the extent to which the non-Poissonian feature affects the efficiency of transmitting information on fluctuating firing rates. For this purpose  we introduce the Kullbuck-Leibler (KL) divergence as a measure of the efficiency of information encoding  and assume that spike trains are generated by time-rescaled renewal processes. We show that the KL divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data. We also show that the KL divergence  as well as the lower bound  depends not only on the variability of spikes in terms of the coefficient of variation  but also significantly on the higher-order moments of interspike interval (ISI) distributions. We examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma  inverse Gaussian (IG) and lognormal ISI distributions)  and find that the time-rescaled renewal process with the IG distribution achieves the largest KL divergence  followed by the lognormal and gamma distributions.,Coding efﬁciency and detectability of rate ﬂuctuations

with non-Poisson neuronal ﬁring

Department of Statistical Modeling
The Institute of Statistical Mathematics

10-3 Midori-cho  Tachikawa  Tokyo 190-8562  Japan

Shinsuke Koyama∗

skoyama@ism.ac.jp

Abstract

Statistical features of neuronal spike trains are known to be non-Poisson. Here  we
investigate the extent to which the non-Poissonian feature affects the efﬁciency of
transmitting information on ﬂuctuating ﬁring rates. For this purpose  we introduce
the Kullback-Leibler (KL) divergence as a measure of the efﬁciency of informa-
tion encoding  and assume that spike trains are generatedby time-rescaled renewal
processes. We show that the KL divergence determines the lower bound of the de-
gree of rate ﬂuctuations below which the temporal variation of the ﬁring rates is
undetectablefrom sparse data. We also show that the KL divergence as well as the
lower bound  depends not only on the variability of spikes in terms of the coefﬁ-
cient of variation  but also signiﬁcantly on the higher-order moments of interspike
interval (ISI) distributions. We examine three speciﬁc models that are commonly
used for describing the stochastic nature of spikes (the gamma  inverse Gaussian
(IG) and lognormal ISI distributions)  and ﬁnd that the time-rescaled renewal pro-
cess with the IG distribution achieves the largest KL divergence  followed by the
lognormal and gamma distributions.

1 Introduction
Characterizing the statistical features of spike time sequences in the brain is important for under-
standing how the brain represents information about stimuli or actions in the sequences of spikes.
Although the spike trains recorded from in vivo cortical neurons are known to be highly irregu-
lar [20  24]  a recent non-stationary analysis has revealed that individual neurons signal with non-
Poisson ﬁring  the characteristics of which are strongly correlated with the function of the cortical
area [21].
This raises the question of what the neural coding advantages of non-Poisson spiking are. It could be
that the precise timing of spikes carries additional information about the stimuli or actions [6  15]. It
is also possible that the efﬁciency of transmitting ﬂuctuatingrates might be enhancedby non-Poisson
ﬁring [5  17]. Here  we explore the latter possibility.
In the problem of estimating ﬁring rates  there is a minimum degree of rate ﬂuctuation below which
a rate estimator cannot detect the temporal variation of the ﬁring rate [23]. If  for instance  the degree
of temporal variation of the rate is on the same order as that of the noise  a constant rate might be
chosen as the most likely estimate for a given spike train. It is  therefore  interesting to see how the
minimum degree of rate ﬂuctuation depends on the non-Poissonian feature of spike trains.
In this study  we investigate the extent to which the non-Poissonian feature of spike trains affects
the encoding efﬁciency of rate ﬂuctuations. In addition  we address the question of how the de-

∗http://skoyama.blogspot.jp

1

tectability of rate ﬂuctuations depends on the encoding efﬁciency. For this purpose  we introduce
the Kullback-Leibler (KL) divergence to measure the encoding efﬁciency  and assume that spike se-
quences are generated by time-rescaled renewal processes. With the aid of analytical and numerical
studies  we suggest that the lower bound of detectable rate ﬂuctuations  below which the empirical
Bayes decoder cannot detect the rate ﬂuctuations  is uniquely determined by the KL divergence. By
examining three speciﬁc models (the time-rescaled renewal process with the gamma  inverse Gaus-
sian (IG) and lognormal interspike interval (ISI) distributions)  it is shown that the KL divergence 
as well as the lower bound  depends not only on the ﬁrst- and second-order moments  but also sig-
niﬁcantly on the higher-order moments of the ISI distributions. We also ﬁnd that among the three
ISI distributions  the IG distribution achieves the highest efﬁciency of coding information on rate
ﬂuctuations.

2 Encoding rate ﬂuctuations using time-rescaled renewal processes
Deﬁnitions of time-rescaled renewal processes and KL divergence
We introduce time-rescaled renewal processes for a model of neuronal spike trains constructed in the
following way. Let fκ(y) be a family of ISI distributions with the unit mean (i.e. ! ∞
0 yfκ(y)dy =
1)  where κ controls the shape of the distribution  and λ(t) be a ﬂuctuating ﬁring rate. A sequence of
spikes {ti} := {t1  t2  . . .   tn} is generated in the following steps: (i) Derive ISIs {y 1  y2  . . .   yn}
independently from fκ(y)  and arrange the ISIs sequentially to form a spike train of the unit rate;
ith spike is given by summing the previous ISIs as s i ="i
j=1 yj. (ii) Transform {s1  s2  . . .   sn}
to {t1  t2  . . .   tn} according to ti =Λ −1(si)  where Λ−1(si) is the inverse of the function Λ(t) =
! t
0 λ(u)du. This transformationensures that the instantaneousﬁring rate of {t i} correspondsto λ(t) 
while the shape of the ISI distribution f κ(y)  which characterizes the ﬁring irregularity is unchanged
in time. This is in agreement with the empirical fact that the degree of irregularity in neuronal ﬁring
is generally maintained in cortical processing [21  22]  while the ﬁring rate λ(t) changes in time.
The probability density of the occurrence of spikes at {t i} is  then  given by
λ(ti)fκ(Λ(ti) − Λ(ti−1)).

pκ({ti}|{λ(t)}) =

(1)

where t0 = 0.
We next introduce the KL divergence for measuring the encoding efﬁciency of ﬂuctuating rates. For
this purpose  we assume that λ(t) is ergodic with a stationary distribution p(λ)  the mean of which
is given by µ:

"λ#λ :=$ ∞

0

(2)
Consider a probability density of a renewal process that has the same ISI density f κ(x) and the
constant rate µ:

λp(λ)dλ = lim
T→∞

λ(t)dt = µ.

1

T $ T

0

n#i=1

(3)

(4)

The KL divergence between pκ({ti}|{λ(t)}) and pκ({ti}|µ) is  then  deﬁned as

pκ({ti}|µ) =

µfκ(µ(ti − ti−1)).

n#n=1
T $ T
t1 ···$ T
0 $ T
∞%n=0
× log pκ({ti}|{λ(t)})
pκ({ti}|µ)

1

tn−1

dt1dt2 ··· dtn.

Dκ(λ(t)||µ)

:= lim
T→∞

pκ({ti}|{λ(t)})

Since it is deﬁned as the entropy of a renewal process with the ﬂuctuating rate λ(t) relative to that
with the constant rate µ  Dκ(λ(t)||µ) can be interpreted as the amount of information on the rate
ﬂuctuations encoded into spike trains. Note that a similar quantity has been introduced in [3]  where
the quantity was computed only under a Poisson model.

2

Substituting Eqs. (1) and (3) into Eq. (4) and further assuming ergodicity of spike trains  the KL
divergence can be expressed as

Dκ(λ(t)||µ) = lim
n→∞

1

tn − t0

1

tn − t0

pκ({ti}|µ)

log pκ({ti}|{λ(t)})
n%i=1& log λ(ti) + log fκ(Λ(ti) − Λ(ti−1))

= lim
n→∞

− log µ − log fκ(µ(ti − ti−1))’.

(5)
This expression can be used for computing the KL divergence numerically by simulating a large
number of spikes n % 1.
Three ISI distributions and their KL divergence
In order to examine the behavior of the KL divergence  we use the three speciﬁc ISI distributions
for fκ(y) (the gamma  inverse Gaussian (IG) and lognormal distributions)  which have been used to
describe the stochastic nature of ISIs [9  10  14]. These distributions and their coefﬁcient of variation

gamma :

(CV =(V ar(X)/E(X)) are given by
fκ(y) = κκyκ−1e−κy/Γ(κ)  CV = 1/√κ 
2πy3 exp* −
fκ(y) =) κ
exp* −
y√2πκ

κ(y − 1)2
(log y + κ

lognormal

fκ(y) =

IG :

2 )2

2κ

2y

1

:

+  CV = 1/√κ 
+  CV = √eκ − 1 

where Γ(κ) =! ∞

0 xκ−1e−xdx is the gamma function. Figure 1a illustrates the shape of the three
distributions with three different values of C V .
The KL divergence for the three models is analytically solvable when the rate ﬂuctuation has a long
time scale relative to the mean ISI. Here  we show the derivation for the gamma distribution. (The
derivations for the IG and lognormal distributions are essentially the same.) Inserting Eq. (6) into
Eq. (5) leads to

(6)
(7)

(8)

1

tn − t0

Dκ(λ(t)||µ) = lim
n→∞

n%i=1& log λ(ti) + (κ − 1) log[Λ(ti) − Λ(ti−1)]
− (κ − 1) log(ti − ti−1)’ − κµ log µ 
(9)
tn−t0 → µ as n → ∞. By introducing the “averaged”
where we used
λ(t)dt → µ and
  we obtain log[Λ(ti) − Λ(ti−1)] = log ¯λi + log(ti −
ﬁring rate in the ith ISI: ¯λi := Λ(ti)−Λ(ti−1)
ti−ti−1
ti−1). Assuming that the time scale of the rate ﬂuctuation is longer than the mean ISI so that ¯λi is
approximated to λ(ti)  Eq. (9) becomes

tn−t0! tn

t0

n

1

Dκ(λ(t)||µ) = κ lim
n→∞
= κ  lim

T→∞

tn − t0

1

n%i=1
T $ T
0 %i

1

log λ(ti) − κµ log µ
δ(t − ti) log λ(t)dt − µ log µ-.

(10)

The ﬂuctuation in the apparent spike count is given by the variance to mean ratio as represented
by the Fano factor [8]. For the renewal process in which ISIs are drawn from a given distribution
function  it is proven that the Fano factor is related to the ISI variability with F ≈ C 2
V [4]. Thus  for
a long range time scale in which a serial correlation of spikes is negligible  the spike train in Eq. (10)
can be approximated to

n%i=1

δ(t − ti) ≈ λ(t) +(λ(t)/κξ(t) 

3

(11)

0

1

where ξ(t) is a ﬂuctuating process such that "ξ(t)# = 0 and "ξ(t)ξ(t%)# = δ(t − t%). Using this  the
ﬁrst term on the rhs of (10) can be evaluated as
1

(12)
where the second term on the lhs has vanished due to a property of stochastic integrals. Therefore 
the KL divergence of the gamma distribution is obtained as

T $ T
0 (λ(t)/κ log λ(t)ξ(t)dt = "λ log λ#λ 

λ(t) log λ(t)dt + lim
T→∞

T $ T

lim
T→∞

(13)
In the same way  the KL divergence for the IG and lognormal distributions are  respectively  derived
as

Dκ(λ(t)||µ) = κ&"λ log λ#λ − µ log µ’.
2"λ log λ#λ + κ + 1

Dκ(λ(t)||µ) = µ
2

2µ "(λ − µ)2#λ 

log µ −

1

and

(14)

(15)

Dκ(λ(t)||µ) = µ
2κ

(log µ)2 −

log µ
κ "λ log λ#λ +

1
2κ"λ(log λ)2#λ.

See the supplementary material for the details of their derivations.
Results
We compute the KL divergence for the three models  in which the rate ﬂuctuates according to the
Ornstein-Uhlenbeck process. Formally  the rate process is given by λ(t) = [x(t)] +  where [·]+ is
the rectiﬁcation function:
(16)

otherwise
and x(t) is derived from the Ornstein-Uhlenbeck process:

[x]+ =  x  x > 0

0 

dx(t)

dt

x(t) − µ

τ

= −

+ σ) 2

τ

ξ(t) 

(17)

where ξ(t) is the Gaussian white noise.
Figure 1b depicts the KL divergence as a function of σ for C V =0.6  1 and 1.5. The analytical results
(the solid lines) are in good agreementwith the numerical results (the error bars). The KL divergence
for the three models increases as σ is increased and as C V is decreased  which is rather obvious
since larger σ and smaller CV imply lower noise entropy of spike trains. One nontrivial result is
that  even if the three models share the same values of σ and C V   the KL divergence of each model
signiﬁcantly differs from that of the others: the IG distribution achieves the largest KL divergence 
followed by the lognormal and gamma distributions. The difference in the KL divergence among
the three models becomes larger as CV grows larger. Since the three models share the same ﬁring
rate λ(t) and CV   it can be concluded that the higher-order (more than second-order) moments of
ISI distributions strongly affect the KL divergence.
In order to conﬁrm this result for another rate process  we examine a sinusoidal rate process  λ(t) =
µ + σ sin t/τ  and observe the same behavior as the Ornstein-Uhlenbeck rate process (Figure 1c).
3 Decoding ﬂuctuating rates using the empirical Bayes method
In this section  we show that the KL divergence (4) determines the lower bound of the degree of rate
ﬂuctuation below which the empirical Bayes estimator cannot detect rate ﬂuctuations.
The empirical Bayes method
We consider decoding a ﬂuctuation rate λ(t) from a given spike train {t i} := {t1 . . .   tn} in an
observation interval [0  T ] by the empirical Bayes method. Let x(t) ∈ R be a latent variable that

4

(a)

(b)
0.15

e
c
n
e
g
r
e
v
d

i

 

L
K

0.1

0.05

CV=0.6

CV=1

CV=1.5

gamma
IG
lognormal

CV=0.6

CV=1

CV=1.5

CV=0.6

CV=1

CV=1.5

(c)
0.4

e
c
n
e
g
r
e
v
d

i

 

L
K

0.3

0.2

0.1

0

0

0

0

0.1

0.2
σ

0.3

0.2

0.4

σ

0.6

0.8

Figure 1: (a) The gamma (blue)  IG (green) and lognormal (red) ISI distribution functions for
CV =0.6  1 and 1.5. (b) The KL divergence as a function of σ for C V =0.6  1 and 1.5  when the rate
ﬂuctuates accordingto the Ornstein-Uhlenbeckprocess (17)with µ = 1 and τ = 10. The blue  green
and red indicate the KL divergence for the gamma  IG and lognormal distribution  respectively. The
lines represent the theoretical values obtained by Eqs. (13)  (14) and (15)  and the error bars repre-
sent the average and standard deviation numerically computed accordingto Eq. (5) with n = 50  000
and 10 trials. (c) The KL divergence for the sinusoidally modulated rate  λ(t) = µ + σ sin t/τ  with
µ = 1 and τ = 10.

is transformed from λ(t) via the log-link function x(t) = log λ(t). For the inference of λ(t) from
{ti}  we use a prior distribution of x(t)  such that the large gradient of x(t) is controlled by

pγ({x(t)}) ∝ exp. −

1

2γ2$ T

0 / dx(t)
dt 02

dt1 

(18)

(19)

where the hyperparameter γ controls the roughness of the latent process x(t): with the small γ  the
model requires a constant latent process  and vice versa. By inverting the conditional probability
distribution with the Bayes’ theorem  the posterior distribution of {x(t)} is obtained as

pκ γ({x(t)}|{ti}) = pκ({ti}|{x(t)})pγ({x(t)})

.

pκ γ({ti})

The hyperparameters  γ and κ  which represent the roughness of the latent process and the shape of
the ISI density function  can be determined by maximizing the marginal likelihood [16] deﬁned by
(20)

pκ γ({ti}) =$ pκ({ti}|{x(t)})pγ({x(t)})D{x(t)} 

where! D{x(t)} represents the integration over all possible latent process paths. Under a set of

hyperparameters ˆγ and ˆκ that are determined by the marginal likelihood maximization  we can
determine the maximum a posteriori (MAP) estimate of the latent process ˆx(t). The method for
implementing the empirical Bayes analysis is summarized in the Appendix.
Detectability of rate ﬂuctuations
We ﬁrst examine the gamma distribution (6). For synthetic spike trains (n = 1  000) generated
by the time-rescaled renewal process with the gamma ISI distribution  in which the rate ﬂuctuates
according to the Ornstein-Uhlenbeck process (17) with µ = 1 and τ = 10  we attempt to decode
λ(t) using the empirical Bayes decoder. Depending on the amplitude of the rate ﬂuctuation σ and
CV of fκ(y)  the empirical Bayes decoder provides qualitatively two distinct rate estimations: (I)
a ﬂuctuating rate estimation (ˆγ> 0) for large σ and small C V   or (II) a constant rate estimation
(ˆγ = 0) for small σ and large CV (Figure 2a). When σ is increased or CV is decreased  the

5

empirical Bayes estimator exhibits a phase transition corresponding to the switch of the most likely
rate estimation from (II) to (I) (Figure 2b). Note that below the critical point of this phase transition 
the empirical Bayes method provides a constant rate as the most likely estimation even if the true
rate process ﬂuctuates. The critical point  thus  gives the lower bound for the degree of detectable
rate ﬂuctuations. It is also conﬁrmed  using numerical simulations  that the phase transition occurs
not only with the gamma distribution  but also with the IG and lognormal distributions (Figure 2c d).
For the time-rescaled renewal process with the gamma ISI distribution  we could analytically derive
the formula that the lower bound satisﬁes as:

Dκ(λ(t)||µ) =

 

(21)

φ(0)
0 φ(u)e−ηudu

4 maxη! ∞

(See supplementary material for the derivation.)
where φ(u) is the correlation function of λ(t).
Eq. (21) is in good agreement with the simulation result for the entire parameter space (the solid line
in Figure 2a).
The expression of Eq. (21) itself does not depend on the gamma distribution. We investigated if this
formula is also applicable to the IG and lognormal distributions  and found that the theoretical lower
bounds (the solid lines in Figure 2c d) indeed do correspond to those obtained by the numerical
simulations; this result implies that Eq. (21) is applicable to more general time-rescaled renewal
processes.
Figure 2e compares the lower bounds among the three distributions. The lower bound of the IG
distribution is the lowest  followed by the lognormal and gamma distributions  which is expected
from the result in Figure 1b  as the lower bound is identically determined by the KL divergence via
Eq. (21).
We also examined the sinusoidally modulated rate  λ(t) = µ + σ sin t/τ; the qualitative result
remains the same (Figure 2f-h).

4 Discussion
In this study  we ﬁrst examined the extent to which spike trains derived from time-rescaled renewal
processes encode information on ﬂuctuating rates. The encoding efﬁciency is measured by the KL
divergence between two renewal processes with ﬂuctuating and constant rates. We showed that the
KL divergence signiﬁcantly differs among the gamma  IG and lognormal ISI distributions  even if
these three processes share the same rate ﬂuctuation λ(t) and C V (Figure 1b). This suggests that the
higher-order moments of ISIs play an important role in encoding information on ﬂuctuating rates.
Among the three distributions  the IG distribution achieves the largest KL divergence  followed by
the lognormal and gamma distributions. A similar result has been reported for stationary renewal
processes [12].
Since the KL divergence gives the distance between two probability distributions  Eq. (4) is natu-
rally related to the ability to discriminate between a ﬂuctuating rate and a constant rate. In fact 
the lower bound of the degree of rate ﬂuctuation  below which the empirical Bayes decoder cannot
discriminate the underlying ﬂuctuating rate from a constant rate  satisﬁes the formula (21). There
commonly exists a lower bound below which the underlying rate ﬂuctuations are undetectable  not
only in the empirical Bayes method with the above prior distribution (18)  but also with other prior
distributions  and in other rate estimators such as a time-histogram. The lower bound in these meth-
ods has been derived for inhomogeneous Poisson processes as τσ 2/µ ∼ O(1)  where τ  σ and µ
are the time scale  amplitude and mean of the rate ﬂuctuation  respectively [23]. Thus  Eq. (21) 
or equivalently τD κ(λ(t)||µ) ∼ O(1) is regarded as a generalization to the non-Poisson processes.
Here  the crucial step for this generalization is incorporating the KL divergence into the formula.
Note that the formula (21) was derived analytically under the assumption of the gamma ISI dis-
tribution  and then was shown to hold for the IG and lognormal ISI distributions with numerical
simulations. The analytical tractability of the gamma family lies in the fact that it is the only scale
family that admits the mean as a sufﬁcient statistic. We conjecture  from our results with the three
speciﬁc models  that Eq. (21) is applicable to more general time-rescaled renewal processes (even
to “non-renewal” processes)  which is open to future research.

6

(a)

0.3

σ

0.2

0.1

0

(c)

0.3

σ

(f)

σ

0.2

0.1

0

0.6

0.4

0.2

0

(I)

(II)

(d)

0.3

σ

0.2

0.1

0

(g)

0.6

σ

0.4

0.2

0

γ>0^

γ=0^

gamma

1.5

0.5

1
CV

IG
1.5

0.5

1
CV

gamma

1.5

0.5

1
CV

(b)

0.1

0.05

γ^

(I)

(II)

0.1

σ

0.2

0.3

0

0
(e)

0.3

σ

0.2

0.1

0

(h)

0.6

σ

0.4

0.2

0

0.5

lognormal
1
CV

1.5

IG
1.5

0.5

1
CV

0.5

1
CV

1.5

0.5

lognormal
1
CV

1.5

Figure 2: (a) Left: the phase diagram for sequences generated by the time-rescaled renewal process
with the gamma ISI distribution. The ordinate represents the amplitude of rate ﬂuctuation σ  and
abscissa represents CV of the gamma ISI distribution. The dots represent the result of numerical
simulations in which the empirical Bayes decoder provides a ﬂuctuating rate estimation (ˆγ> 0).
Each dot is plotted if ˆγ> 0 in more than 20 out of 40 identical trials. The solid line represents
the theoretical lower bound obtained by the formula (21). Right: raster plots of sample spike trains
and the estimated rates. The dotted lines and the solid lines represent the underlying rates and the
estimated rates  respectively. The parameters (CV  σ ) of top (ˆγ> 0) and bottom (ˆγ = 0) are
(0.6  0.3) and (1.5  0.15)  respectively. (b) The optimal hyperparameter ˆγ as a function of σ for
CV = 0.6. The solid line represents the theoretical value  and the error bars represent the average
and standard deviation of ˆγ determined by applying the empirical Bayes algorithm to 40 trials. (c 
d) The phase diagrams for the IG and lognormal ISI distributions. (e) Comparison of the lower
bounds among the three models. (f-h) The phase diagrams for the gamma  IG and lognormal ISI
distributions  when the rate process is given by λ(t) = µ + σ sin t/τ with µ = 1 and τ = 10.

A recent non-stationary analysis has revealed that individual neurons in the cortex signal with non-
Poisson ﬁring  which has empirically been characterized by measures based on the second-order
moment of ISIs  such as CV and LV [21  22]. Our results  however  suggest that it may be important
to take into account the higher-order moments of ISIs for characterizing “irregularity” of cortical
ﬁring  in order to gain information on ﬂuctuating ﬁring rates. It has also been demonstrated that
using non-Poisson spiking models enhances the performance of neural decoding [2  11  19]. Our
results provide theoretical support for this as well.

Appendix: Implementation of the empirical Bayes method
Discretization
To construct a practical algorithm for performing empirical Bayes decoding  we ﬁrst divide the
time axis into a set of intervals (ti−1  ti] (i = 1  . . .   n). We assume that the ﬁring rate within
each interval (ti−1  ti] does not change drastically (which is a reasonable assumption in practice) 
so that it can be approximated to a constant value λ i. Letting Ti = ti − ti−1 be the ith ISI  the
probability density of {Ti}≡{ T1  T2  . . .   Tn}  given the rate process {λi}≡{ λ1 λ 2 . . .  λ n}

7

1

i=2 pγ(xi|xi−1)  where
pγ(xi|xi−1) =

is obtained from Eq. (1) as pκ({Ti}|{λi}) = 2n
i=1 λifκ(λiTi). The rate process is linked with
the latent process via xi = log λi. With the same time-discretization  the prior distribution of the
latent process {xi}≡{ x1  x2  . . .   xn}  which corresponds to Eq. (18)  is derived as p γ({xi}) =
p(x1)2n
(22)

γ2(Ti + Ti−1)+ 
(xi − xi−1)2
and p(x1) is the probability density function of the initial latent rate variable.
p({Ti}|{λi}) and pγ({xi}) deﬁne a discrete-time state space model. We note that this provides a
good approximation to the original continuous-time model if the timescale of the rate ﬂuctuation is
larger than the mean ISI.
EM algorithm
We assume that the ISI density function can be rewritten in the form of exponential family distribu-
tions with respect to the shape parameter κ:

(πγ2(Ti + Ti−1)

exp* −

pκ(Ti|φi) := λifκ(λiTi) = exp[κS(Ti φ i) − ϕ(κ) + c(Ti φ i)] 

(23)
with an appropriate parameter representation φ i = φ(λi κ ). Here  κ is the natural parameter of
the exponential family and S(T i φ i) is its sufﬁcient statistic. Suppose that the potential ϕ(κ) is a
convex function. The expectation of S(T i φ i) is then given by

η =$ S(Ti φ i)pκ(Ti|φi)dTi = dϕ(κ)

(24)
Since ϕ(κ) is convex  there is one-to-one correspondence between κ and η  and thus η provides
alternative parametrization to κ [1]. The gamma (6)  IG (7) and lognormal (8) distributions are
included in this family.
With the parameterization η  the EM algorithm for the state space model is derived as follows.
Supposethat we have estimations ˆη(m) and ˆγ(m) at the mth iteration. The estimations at the (m+1)th
iteration are given by

dκ

.

(25)

and

ˆη(m+1) =

1
n

n%i=1
"S(Ti φ (xi))#(m) 
n%i=2
2
n − 1

"(xi − xi−1)2#(m)

 

ˆγ2
(m+1) =

Ti + Ti−1

(26)
where " #(m) denotes the expectation with respect to the posterior probability of {x i}  given {Ti} 
ˆη(m) and ˆγ(m). The posterior probability is computed by the Laplace approximation  introduced
below. We update ˆη and ˆγ until the estimations converge. The estimation of κ is then transformed
from ˆη with Eq. (24).
Laplace approximation
We employ Laplace’s method to compute an approximate posterior distribution of {x i}. Let x =
(x1  x2  . . .   xn)t be the column vector of the latent process  ( ) t being the transpose of a vector.
The MAP estimate of the latent process is obtained by maximizing the log posterior distribution:

n%i=2

n%i=1

l(x) = log p(x1) +

log pγ(xi|xi−1) +

log pκ(Ti|xi) + const. 

(27)

with respect to x. We use a diffuse prior for p(x1) so that its contribution vanishes [7].
If
pγ(xi|xi−1) is log-concave in xi and xi−1  and the pκ(Ti|xi) is also log-concave in xi  comput-
ing the MAP estimate is a concave optimization problem [18]  which can be solved efﬁciently by
a Newton method. Due to the Markovian Structure of the state-space model  the Hessian matrix 
J(x) ≡ ∇∇xl(x)  becomes a tridiagonal matrix  which allows us to compute the Newton step in
O(n) time [13]. Let ˆx denote the MAP estimation of the posterior probability. The posterior proba-
bility is then approximated to a Gaussian whose mean vector and covariance matrix are given by ˆx
and −J(ˆx)−1  respectively.

8

Acknowledgments
This work was supported by JSPS KAKENHI Grant Number 24700287.
References
[1] S. Amari and H. Nagaoka. Methods of Information Geometry. Oxford University Press  2000.
[2] R. Barbieri  M. C. Quirk  L. M. Frank  M. A. Wilson  and E. N. Brown. Construction and analysis
of non-poisson stimulus-response models of neural spiking activity. Journal of Neuroscience Methods 
105:25–37  2001.

[3] N. Brenner  S. P. Strong  R. Koberle  and W. Bialek. Synergy in a neural code. Neural Computation 

12:1531–1552  2000.

[4] D. R. Cox. Renewal Theory. Chapman and Hal  1962.
[5] J. P. Cunningham  B. M. Yu  K. V. Shenoy  and M. Sahani. Inferring neural ﬁring rates from spike trains
using Gaussian processes. In Neural Information Processing Systems  volume 20  pages 329–336  2008.
[6] R. M. Davies  G. L. Gerstein  and S. N. Baker. Measurement of time-dependent changes in the irregularity

of neural spiking. Journal of Neurophysiology  96:906–918  2006.

[7] J. Durbin and S. J. Koopman. Time Series Analysis by State Space Methods. Oxford University Press 

[8] U. Fano.

Ionization yield of radiations. ii. the ﬂuctuations of the number of ions. Physical Review 

2001.

72:26–29  1947.

1748  2009.

[9] G. L. Gerstein and B. Mandelbrot. Random walk models for the spike activity of a single neuron. Bio-

physical Journal  4:41–68  1964.

[10] S. Ikeda and J. H. Manton. Capacity of a single spiking neuron channel. Neural Computation  21:1714–

[11] A. L. Jacobs  G. Fridman  R. M. Douglas  N. M. Alam  P. E. Latham  G. T. Prusky  and S. Nirenberg.
Ruling out and ruling in neural codes. Proceedings of the National Academy of Sciences  106:5936–5941 
2009.

[12] K. Kang and S. Amari. Discrimination with spike times and ISI distributions. Neural Computation 

20:1411–1426  2008.

[13] S. Koyama and L. Paninski. Efﬁcient computation of the maximum a posteriori path and parameter estima-
tion in integrate-and-ﬁre and more general state-space models. Journal of Computational Neuroscience 
29:89–105  2009.

[14] M. W. Levine. The distribution of the intervals between neural impulses in the maintained discharges of

retinal ganglion cells. Biological Cybernetics  65:459–467  1991.

[15] B. N. Lundstrom and A. L. Fairhall. Decoding stimulus variance from a distributional neural code of

interspike intervals. Journal of Neuroscience  26:9030–9037  2006.

[16] D. J. C. MacKay. Bayesian interpolation. Neural Computation  4:415–447  1992.
[17] T. Omi and S. Shinomoto. Optimizing time histograms for non-Poisson spike trains. Neural Computation 

23:3125–3144  2011.

[18] L. Paninski. Log-concavity results on gaussian process methods for supervised and unsupervised learning.

In Neural Information Processing Systems  volume 17  pages 1025–1032  2005.

[19] J. W. Pillow  L. Paninski  V. J. Uzzell  E. P. Simoncelli  and E. J. Chichilnisky. Prediction and decoding
of retinal ganglion cell responses with a probabilistic spiking model. Journal of Neuroscience  23:11003–
11013  2005.

[20] M. N. Shadlen and W. T. Newsome. The variable discharge of cortical neurons: Implications for connec-

tivity  computation  and information coding. Journal of Neuroscience  18:3870–3896  1998.

[21] S. Shinomoto  H. Kim  T. Shimokawa  N. Matsuno  S. Funahashi  K. Shima  I. Fujita  H. Tamura  T. Doi 
K. Kawano  N. Inaba  K. Fukushima  S. Kurkin  K. Kurata  M. Taira  K. Tsutsui  H. Komatsu  T. Ogawa 
K. Koida  J. Tanji  and K. Toyama. Relating neuronal ﬁring patterns to functional differentiation of
cerebral cortex. PLoS Computational Biology  5:e1000433  2009.

[22] S. Shinomoto  K. Shima  and J. Tanji. Differences in spiking patterns among cortical neurons. Neural

Computation  15:2823–2842  2003.

Physical Review E  85:041139  2012.

[23] T. Shintani and S. Shinomoto. Detection limit for rate ﬂuctuations in inhomogeneous poisson processes.

[24] W. R. Softky and C. Koch. The highly irregular ﬁring of cortical cells is inconsistent with temporal

integration of random EPSPs. Journal of Neuroscience  13:334–350  1993.

9

,Sefi Bell-Kligler
Assaf Shocher
Michal Irani