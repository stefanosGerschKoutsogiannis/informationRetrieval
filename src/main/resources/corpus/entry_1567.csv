2014,Distributed Estimation  Information Loss and Exponential Families,Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets  and then combines the local MLEs to achieve the best possible approximation to the global MLE  based on the whole dataset jointly. We study the statistical properties of this framework  showing that the loss of efficiency compared to the global setting relates to how much the underlying distribution families deviate from full exponential families  drawing connection to the theory of information loss by Fisher  Rao and Efron. We show that the full-exponential-family-ness" represents the lower bound of the error rate of arbitrary combinations of local MLEs  and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of the KL and linear combination methods  showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification  non-convexity  and heterogeneous data partitions.",Distributed Estimation  Information Loss and

Exponential Families

Qiang Liu

Alexander Ihler

Department of Computer Science  University of California  Irvine

qliu1@uci.edu

ihler@ics.uci.edu

Abstract

Distributed learning of probabilistic models from multiple data repositories
with minimum communication is increasingly important. We study a simple
communication-efﬁcient learning framework that ﬁrst calculates the local maxi-
mum likelihood estimates (MLE) based on the data subsets  and then combines
the local MLEs to achieve the best possible approximation to the global MLE
given the whole dataset. We study this framework’s statistical properties  showing
that the efﬁciency loss compared to the global setting relates to how much the un-
derlying distribution families deviate from full exponential families  drawing con-
nection to the theory of information loss by Fisher  Rao and Efron. We show that
the “full-exponential-family-ness” represents the lower bound of the error rate of
arbitrary combinations of local MLEs  and is achieved by a KL-divergence-based
combination method but not by a more common linear combination method. We
also study the empirical properties of both methods  showing that the KL method
signiﬁcantly outperforms linear combination in practical settings with issues such
as model misspeciﬁcation  non-convexity  and heterogeneous data partitions.

1

Introduction

Modern data-science applications increasingly require distributed learning algorithms to extract in-
formation from many data repositories stored at different locations with minimal interaction. Such
distributed settings are created due to high communication costs (for example in sensor networks) 
or privacy and ownership issues (such as sensitive medical or ﬁnancial data). Traditional algorithms
often require access to the entire dataset simultaneously  and are not suitable for distributed settings.
We consider a straightforward two-step procedure for distributed learning that follows a “divide and
conquer” strategy: (i) local learning  which involves learning probabilistic models based on the local
data repositories separately  and (ii) model combination  where the local models are transmitted
to a central node (the “fusion center”)  and combined to form a global model that integrates the
information in the local repositories. This framework only requires transmitting the local model
parameters to the fusion center once  yielding signiﬁcant advantages in terms of both communication
and privacy constraints. However  the two-step procedure may not fully extract all the information in
the data  and may be less (statistically) efﬁcient than a corresponding centralized learning algorithm
that operates globally on the whole dataset. This raises important challenges in understanding the
fundamental statistical limits of the local learning framework  and proposing optimal combination
methods to best approximate the global learning algorithm.
In this work  we study these problems in the setting of estimating generative model parameters
from a distribution family via the maximum likelihood estimator (MLE). We show that the loss of
statistical efﬁciency caused by using the local learning framework is related to how much the under-
lying distribution families deviate from full exponential families: local learning can be as efﬁcient
as (in fact exactly equivalent to) global learning on full exponential families  but is less efﬁcient
on non-exponential families  depending on how nearly “full exponential family” they are. The

1

“full-exponential-family-ness” is formally captured by the statistical curvature originally deﬁned
by Efron (1975)  and is a measure of the minimum loss of Fisher information when summarizing
the data using ﬁrst order efﬁcient estimators (e.g.  Fisher  1925  Rao  1963). Speciﬁcally  we show
that arbitrary combinations of the local MLEs on the local datasets can approximate the global MLE
on the whole dataset at most up to an asymptotic error rate proportional to the square of the sta-
tistical curvature. In addition  a KL-divergence-based combination of the local MLEs achieves this
minimum error rate in general  and exactly recovers the global MLE on full exponential families.
In contrast  a more widely-used linear combination method does not achieve the optimal error rate 
and makes mistakes even on full exponential families. We also study the two methods empirically 
examining their robustness against practical issues such as model mis-speciﬁcation  heterogeneous
data partitions  and the existence of hidden variables (e.g.  in the Gaussian mixture model). These
issues often cause the likelihood to have multiple local optima  and can easily degrade the linear
combination method. On the other hand  the KL method remains robust in these practical settings.
Related Work. Our work is related to Zhang et al. (2013a)  which includes a theoretical analysis
for linear combination. Merugu and Ghosh (2003  2006) proposed the KL combination method in
the setting of Gaussian mixtures  but without theoretical analysis. There are many recent theoretical
works on distributed learning (e.g.  Predd et al.  2007  Balcan et al.  2012  Zhang et al.  2013b 
Shamir  2013)  but most focus on discrimination tasks like classiﬁcation and regression. There are
also many works on distributed clustering (e.g.  Merugu and Ghosh  2003  Forero et al.  2011  Liang
et al.  2013) and distributed MCMC (e.g.  Scott et al.  2013  Wang and Dunson  2013  Neiswanger
et al.  2013). An orthogonal setting of distributed learning is when the data is split across the variable
dimensions  instead of the data instances; see e.g.  Liu and Ihler (2012)  Meng et al. (2013).

2 Problem Setting

Assume we have an i.i.d. sample X = {xi∶ i = 1  . . .   n}  partitioned into d sub-samples X k =
{xi∶ i∈ αk} that are stored in different locations  where∪d
k=1αk=[n]. For simplicity  we assume
the data are equally partitioned  so that each group has n~d instances; extensions to the more general
a distribution family{p(xθ)∶ θ∈ Θ}. Let θ
∗ be the true unknown parameter. We are interested in
i∈[n] log p(xiθ).
ˆθmle= arg max
Q
θ∈Θ

∗ via the maximum likelihood estimator (MLE) based on the whole sample 

case is straightforward. Assume X is drawn i.i.d. from a distribution with an unknown density from

estimating θ

However  directly calculating the global MLE often requires distributed optimization algorithms
(such as ADMM (Boyd et al.  2011)) that need iterative communication between the local reposito-
ries and the fusion center  which can signiﬁcantly slow down the algorithm regardless of the amount
of information communicated at each iteration. We instead approximate the global MLE by a two-
stage procedure that calculates the local MLEs separately for each sub-sample  then sends the local
MLEs to the fusion center and combines them. Speciﬁcally  the k-th sub-sample’s local MLE is

and we want to construct a combination function f(ˆθ1  . . .   ˆθd)→ ˆθf to form the best approximation

to the global MLE ˆθmle. Perhaps the most straightforward combination is the linear average 

ˆθk= arg max
θ∈Θ

Q
i∈αk
ˆθlinear= 1

log p(xiθ) 
Q

ˆθk.

d

k

Linear-Averaging:

However  this method is obviously limited to continuous and additive parameters; in the sequel  we
illustrate it also tends to degenerate in the presence of practical issues such as non-convexity and
non-i.i.d. data partitions. A better combination method is to average the models w.r.t. some distance
metric  instead of the parameters. In particular  we consider a KL-divergence based averaging 

ˆθKL= arg min
θ∈Θ

Q

k

KL(p(xˆθk) p(xθ)).

from each local model p(xˆθk)  and then estimates a global MLE based on all the combined

The estimate ˆθKL can also be motivated by a parametric bootstrap procedure that ﬁrst draws sample
X k

′

KL-Averaging:

(1)

2

′={X k

′∶ k∈[d]}. We can readily show that this reduces to ˆθKL as the size

bootstrap samples X
of the bootstrapped samples X k
distance metrics are also possible  but may not have a similarly natural interpretation.

grows to inﬁnity. Other combination methods based on different

′

3 Exactness on Full Exponential Families

θ∈ Θ≡{θ∈ Rm∶S

In this section  we analyze the KL and linear combination methods on full exponential families.
We show that the KL combination of the local MLEs exactly equals the global MLE  while the
linear average does not in general  but can be made exact by using a special parameterization. This
suggests that distributed learning is in some sense “easy” on full exponential families.
Deﬁnition 3.1. (1). A family of distributions is said to be a full exponential family if its density can
be represented in a canonical form (up to one-to-one transforms of the parameters) 

exp(θT φ(x))dH(x)<∞}.
p(xθ)= exp(θT φ(x)− log Z(θ)) 
where θ=[θ1  . . . θm]T and φ(x)=[φ1(x)  . . . φm(x)]T are called the natural parameters and the
natural sufﬁcient statistics  respectively. The quantity Z(θ) is the normalization constant  and H(x)
is the reference measure. An exponential family is said to be minimal if[1  φ1(x)  . . . φm(x)]T is
linearly independent  that is  there is no non-zero constant vector α  such that αT φ(x)= 0 for all x.
Theorem 3.2. IfP={p(xθ)∶ θ∈ Θ} is a full exponential family  then the KL-average ˆθKL always
exactly recovers the global MLE  that is  ˆθKL= ˆθmle. Further  ifP is minimal  we have
where µ∶ θ Eθ[φ(x)] is the one-to-one map from the natural parameters to the moment param-

−1 is the inverse map of µ. Note that we have µ(θ)= ∂log Z(θ)~∂θ.

−1 µ(ˆθ1)++ µ(ˆθd)

ˆθKL= µ

eters  and µ

  

(2)

d

x

Proof. Directly verify that the KL objective in (1) equals the global negative log-likelihood.

which then exactly maps to the global MLE.

The nonlinear average in (2) gives an intuitive interpretation of why ˆθKL equals ˆθmle on full expo-

nential families: it ﬁrst calculates the local empirical moment parameters µ(ˆθk)= d~n∑i∈αk φ(xk);
averaging them gives the empirical moment parameter on the whole data ˆµn = 1~n∑i∈[n] φ(xk) 
Eq (2) also suggests that ˆθlinear would be exact only if µ(⋅) is an identity map. Therefore  one may
make ˆθlinear exact by using the special parameterization ϑ= µ(θ). In contrast  KL-averaging will
distribution. Let ˆsk = (d~n)∑i∈αk(xi)2 be the empirical variance on the k-th sub-sample and
ˆs = ∑k ˆsk~d the overall empirical variance. Then  ˆθlinear would correspond to different power

make this reparameterization automatically (µ is different on different exponential families). Note
that both KL-averaging and global MLE are invariant w.r.t. one-to-one transforms of the parameter
θ  but linear averaging is not.
Example 3.3 (Variance Estimation). Consider estimating the variance σ2 of a zero-mean Gaussian

means on ˆsk  depending on the choice of parameterization  e.g. 

1
d

ˆθlinear

where only the linear average of ˆsk (when θ= σ2) matches the overall empirical variance ˆs and

equals the global MLE. In contrast  ˆθKL always corresponds to a linear average of ˆsk  equaling the
global MLE  regardless of the parameterization.

1
d

1
d

3

θ= σ2 (variance)
∑k ˆsk

θ= σ (standard deviation)

θ= σ

∑k

(ˆsk)1~2

−2 (precision)
(ˆsk)−1
∑k

4

Information Loss in Distributed Learning

The exactness of ˆθKL in Theorem 3.2 is due to the beauty (or simplicity) of exponential families.
Following Efron’s intuition  full exponential families can be viewed as “straight lines” or “linear
subspaces” in the space of distributions  while other distribution families correspond to “curved” sets
of distributions  whose deviation from full exponential families can be measured by their statistical
curvatures as deﬁned by Efron (1975). That work shows that statistical curvature is closely related
to Fisher and Rao’s theory of second order efﬁciency (Fisher  1925  Rao  1963)  and represents
the minimum information loss when summarizing the data using ﬁrst order efﬁcient estimators. In
this section  we connect this classical theory with the local learning framework  and show that the
statistical curvature also represents the minimum asymptotic deviation of arbitrary combinations of
the local MLEs to the global MLE  and that this is achieved by the KL combination method  but not
in general by the simpler linear combination method.

4.1 Curved Exponential Families and Statistical Curvature

(3)

its density can be represented as

Following Kass and Vos (2011)  we assume some regularity conditions for our asymptotic analysis.

p(xθ)= exp(η(θ)T φ(x)− log Z(η(θ))) 

We follow the convention in Efron (1975)  and illustrate the idea of statistical curvature using curved
exponential families  which are smooth sub-families of full exponential families. The theory can be
naturally extended to more general families (see e.g.  Efron  1975  Kass and Vos  2011).

Deﬁnition 4.1. A family of distributions{p(xθ)∶ θ∈ Θ} is said to be a curved exponential family if
where the dimension of θ=[θ1  . . .   θq] is assumed to be smaller than that of η=[η1  . . .   ηm] and
φ=[φ1  . . .   φm]  that is q< m.
Assume Θ is an open set in Rq  and the mapping η∶ Θ→ η(Θ) is one-to-one and inﬁnitely differ-
entiable  and of rank q  meaning that the q× m matrix ˙η(θ) has rank q everywhere. In addition 
if a sequence{η(θi)∈ N0} converges to a point η(θ0)  then{ηi∈ Θ} must converge to φ(η0). In
geometric terminology  such a map η∶ Θ→ η(Θ) is called a q-dimensional embedding in Rm.
p(xη)= exp(ηT φ(x)− log Z(η))  with η constrained in η(Θ). If η(θ) is a linear function  then
otherwise  η(θ) is a curved subset in the η-space  whose curvature – its deviation from planes or
Consider the case when θ is a scalar  and hence η(θ) is a curve; the ge-
ometric curvature γθ of η(θ) at point θ is deﬁned to be the reciprocal of
the radius of the circle that ﬁts best to η(θ) locally at θ. Therefore  the
curvature of a circle of radius r is a constant 1~r. In general  elementary
θ ˙ηθ)2). The statis-
Deﬁnition 4.2 (Statistical Curvature). Consider a curved exponential familyP={p(xθ)∶ θ∈ Θ} 
whose parameter θ is a scalar (q= 1). Let Σθ= covθ[φ(x)] be the m× m Fisher information on
the corresponding full exponential family p(xη). The statistical curvature ofP at θ is deﬁned as

calculus shows that γ2
θ
tical curvature of a curved exponential family is deﬁned similarly  except
equipped with an inner product deﬁned via its Fisher information metric.

Obviously  a curved exponential family can be treated as a smooth subset of a full exponential family

the curved exponential family can be rewritten into a full exponential family in lower dimensions;

straight lines – represents its deviation from full exponential families.

=( ˙ηT
θ ˙ηθ)−3(¨ηT

θ ¨ηθ⋅ ˙ηT

θ ˙ηθ−(¨ηT

θ Σθ ˙ηθ)−3(¨ηT
=( ˙ηT

θ Σθ ¨ηθ)⋅( ˙ηT

θ Σθ ˙ηθ)−(¨ηT

θ Σθ ˙ηθ)2(cid:6).

γ2
θ

The deﬁnition can be extended to general multi-dimensional parameters  but requires involved no-
tation. We give the full deﬁnition and our general results in the appendix.
Example 4.3 (Bivariate Normal on Ellipse). Consider a bivariate normal distribution with diagonal

covariance matrix and mean vector restricted on an ellipse η(θ)=[a cos(θ)  b sin(θ)]  that is 
θ∈(−π  π)  x∈ R2.
geometric curvature of the ellipse in the Euclidian space  γθ= ab(a2 sin2(θ)+ b2 cos2(θ))−3~2.

We have that Σθ equals the identity matrix in this case  and the statistical curvature equals the

)+ a cos θ x1+ b sin θ x2)(cid:6) 

p(xθ)∝ exp− 1

+ x2

(x2

2

1

2

4

1/✓⌘(✓)The statistical curvature was originally deﬁned by Efron (1975) as the minimum amount of informa-
tion loss when summarizing the sample using ﬁrst order efﬁcient estimators. Efron (1975) showed
that  extending the result of Fisher (1925) and Rao (1963) 

n→∞[I X

θ∗−I ˆθmle
θ∗

lim

]= γ2
θ∗ Iθ∗  

(4)

γ2

4.2 Lower Bound

results in the sequel closely match this intuition.

θ∗  in contrast with the global MLE  which only loses γ2

where Iθ∗ is the Fisher information (per data instance) of the distribution p(xθ) at the true parameter
∗  andI X
θ∗= nIθ∗ is the total information included in a sample X of size n  andI ˆθmle
θ∗
is the Fisher
θ
θ∗ units of Fisher
as the effective number of data instances lost in MLE  easily seen from rewritingI ˆθmle
≈ (n−
information included in ˆθmle based on X. Intuitively speaking  we lose about γ2
θ∗
θ∗)Iθ∗  as compared toI X
θ∗ = nIθ∗. Moreover  this is the minimum possible information loss
information when summarizing the data using the ML estimator. Fisher (1925) also interpreted γ2
θ∗
in the class of “ﬁrst order efﬁcient” estimators T(X)  those which satisfy the weaker condition
limn→∞Iθ∗~I T
θ∗= 1. Rao coined the term “second order efﬁciency” for this property of the MLE.
only through{ˆθk}  each of which summarizes the data with a loss of γ2
total information loss is d⋅ γ2
The intuition here has direct implications for our distributed setting  since ˆθf depends on the data
θ∗ units of information. The
Therefore  the additional loss due to the distributed setting is(d− 1)⋅ γ2
θ∗ overall.
θ∗. We will see that our
The extra information loss(d− 1)γ2
error rate n2Eθ∗[Iθ∗(ˆθf− ˆθmle)2] for any arbitrary combination function f(ˆθ1  . . .   ˆθd).
θ∗ turns out to be the asymptotic lower bound of the mean square
Theorem 4.4 (Lower Bound). For an arbitrary measurable function ˆθf=f(ˆθ1  . . .   ˆθd)  we have
n→+∞ n2 Eθ∗[f(ˆθ1  . . .   ˆθd)− ˆθmle2]≥(d− 1)γ2
Eθ∗[ˆθf− ˆθmle2]= Eθ∗[ˆθf− Eθ∗(ˆθmleˆθ1  . . .   ˆθd)2]+ Eθ∗[ˆθmle− Eθ∗(ˆθmleˆθ1  . . .   ˆθd)2]
≥ Eθ∗[ˆθmle− Eθ∗(ˆθmleˆθ1  . . .   ˆθd)2]
= Eθ∗[varθ∗(ˆθmleˆθ1  . . .   ˆθd)] 
where the lower bound is achieved when ˆθf = Eθ∗(ˆθmleˆθ1  . . .   ˆθd). The conclusion follows by
showing that limn→+∞ Eθ∗[varθ∗(ˆθmleˆθ1  . . .   ˆθd)]=(d− 1)γ2
−1
θ∗ ; this requires involved asymp-
θ∗ I
jection of random variables (e.g.  Van der Vaart  2000). LetF be
the set of all random variables in the form of f(ˆθ1  . . .   ˆθd). The op-
timal consensus function should be the projection of ˆθmle ontoF 
andF. The conditional expectation ˆθf = Eθ∗(ˆθmleˆθ1  . . .   ˆθd) is the exact projection and ideally
∗. We show in the sequel that ˆθKL gives an efﬁcient approximation and

the best combination function; however  this is intractable to calculate due to the dependence on the
unknown true parameter θ
achieves the same asymptotic lower bound.

and the minimum mean square error is the distance between ˆθmle

The proof above highlights a geometric interpretation via the pro-

totic analysis  and is presented in the Appendix.

lim inf

Sketch of Proof . Note that

−1
θ∗ .

θ∗I

4.3 General Consistent Combination

We now analyze the performance of a general class of ˆθf   which includes both the KL average ˆθKL
and the linear average ˆθlinear; we show that ˆθKL matches the lower bound in Theorem 4.4  while
ˆθlinear is not optimal even on full exponential families. We start by deﬁning conditions which any

“reasonable” f(ˆθ1  . . .   ˆθd) should satisfy.

5

(⇡1n2(d1)·2ˆ✓1ˆ✓df(ˆ✓1 ... ˆ✓d)ˆ✓mleDeﬁnition 4.5. (1). We say f(⋅) is consistent  if for ∀θ ∈ Θ  θk → θ  ∀k ∈ [d] implies
f(θ1  . . .   θd)→ θ.
(2). f(⋅) is symmetric if f(ˆθ1  . . .   ˆθd)= f(ˆθσ(1)
be consistent. The symmetry is also straightforward due to the symmetry of the data partition{X k}.
In fact  if f(⋅) is not symmetric  one can always construct a symmetric version that performs better
Theorem 4.6. (1). Consider a consistent and symmetric ˆθf = f(ˆθ1  . . .   ˆθd) as in Deﬁnition 4.5 

  . . .   ˆθσ(d))  for any permutation σ on[d].

The consistency condition guarantees that if all the ˆθk are consistent estimators  then ˆθf should also

or at least the same (see Appendix for details). We are now ready to present the main result.

whose ﬁrst three orders of derivatives exist. Then  for curved exponential families in Deﬁnition 4.1 

Eθ∗[ˆθf− ˆθmle] = d− 1
Eθ∗[ˆθf− ˆθmle2] = d− 1

βf

n

θ∗+ o(n
−1) 
⋅[γ2
θ∗ +(d+ 1)(βf
−1
θ∗ I

θ∗)2]+ o(n

n2

where βf
square error is consistent with the lower bound in Theorem 4.4  and is tight if βf

θ∗ is a term that depends on the choice of the combination function f(⋅). Note that the mean
θ∗= 0  and hence achieves the minimum bias and mean square error 
Eθ∗[ˆθKL− ˆθmle] = o(n
−1) 
θ∗≠ 0.

In particular  note that the bias of ˆθKL is smaller in magnitude than that of general ˆθf with βf

(2). The KL average ˆθKL has βf

−2).

n2

(4). The linear averaging ˆθlinear  however  does not achieve the lower bound in general. We have

−2) 
θ∗= 0.
θ∗ + o(n
⋅ γ2
−1
θ∗ I

θ∗
βlinear

Eθ∗[ˆθKL− ˆθmle2] = d− 1
∗)
Eθ∗ ∂3 log p(xθ
−2∗ (¨ηT
= I
θ∗ Σθ∗ ˙ηθ∗+ 1
∗2] + d− 1
⋅ γ2
θ∗ + o(n
−2).
−1
∗2] + d− 1
θ∗ I
θ∗ + 2(βlinear
⋅[γ2
−1
θ∗
θ∗I

(cid:6)) 

∂θ3

n2

2

n2

∗  by

)2]+ o(n

−2).

which is in general non-zero even for full exponential families.

(5). The MSE w.r.t. the global MLE ˆθmle can be related to the MSE w.r.t. the true parameter θ

Eθ∗[ˆθKL− θ
Eθ∗[ˆθlinear− θ

∗2] = Eθ∗[ˆθmle− θ
∗2] = Eθ∗[ˆθmle− θ
∗ = Op(1~√

Proof. See Appendix for the proof and the general results for multi-dimensional parameters.

Theorem 4.6 suggests that ˆθf− ˆθmle= Op(1~n) for any consistent f(⋅)  which is smaller in mag-
nitude than ˆθmle− θ
n). Therefore  any consistent ˆθf is ﬁrst order efﬁcient  in that
its difference from the global MLE ˆθmle is negligible compared to ˆθmle− θ
∗ asymptotically. This
∗. However  we need to treat this claim with caution  because  as

also suggests that KL and the linear methods perform roughly the same asymptotically in terms of
recovering the true parameter θ
we demonstrate empirically  the linear method may signiﬁcantly degenerate in the non-asymptotic
region or when the conditions in Theorem 4.6 do not hold.

5 Experiments and Practical Issues

We present numerical experiments to demonstrate the correctness of our theoretical analysis. More
importantly  we also study empirical properties of the linear and KL combination methods that
are not enlightened by the asymptotic analysis. We ﬁnd that the linear average tends to degrade
signiﬁcantly when its local models (ˆθk) are not already close  for example due to small sample
sizes  heterogenous data partitions  or non-convex likelihoods (so that different local models ﬁnd
different local optima). In contrast  the KL combination is much more robust in practice.

6

(b).E(θf− ˆθmle)

(a). E(θf− ˆθmle2)

∗2)
∗  respectively. The y-axes are shown on logarithmic (base 10) scales.

Figure 1: Result on the toy model in Example 4.3. (a)-(d) The mean square errors and biases of the
linear average ˆθlinear and the KL average ˆθKL w.r.t. to the global MLE ˆθmle and the true parameter
θ

(c). E(θf− θ

(d).E(θf− θ

∗)

5.1 Bivariate Normal on Ellipse

We start with the toy model in Example 4.3 to verify our theoretical results. We draw samples from
the true model (assuming θ

∗= π~4  a= 1  b= 5)  and partition the samples randomly into 10 sub-
groups (d= 10). Fig. 1 shows that the empirical biases and MSEs match closely with the theoretical
predictions when the sample size is large (e.g.  n≥ 250)  and ˆθKL is consistently better than ˆθlinear
∗  but
linear average degrades signiﬁcantly in the non-asymptotic region (e.g.  n< 250).

in terms of recovering both the global MLE and the true parameters. Fig. 1(b) shows that the bias
of ˆθKL decreases faster than that of ˆθlinear  as predicted in Theorem 4.6 (2). Fig. 1(c) shows that
all algorithms perform similarly in terms of the asymptotic MSE w.r.t. the true parameters θ

Model Misspeciﬁcation. Model misspeciﬁcation is unavoidable in prac-
tice  and may create multiple local modes in the likelihood objective 
leading to poor behavior from the linear average. We illustrate this phe-
nomenon using the toy model in Example 4.3  assuming the true model

illustrated in the ﬁgure at right  where the ellipse represents the paramet-
ric family  and the black square denotes the true model. The MLE will concentrate on the projection

isN([0  1~2]  12×2)  outside of the assumed parametric family. This is
of the true model to the ellipse  in one of two locations (θ=±π~2) indicated by the two red circles.
two values; see Fig. 2(a). Given a sufﬁcient number of samples (n> 250)  the probability that the
MLE is at θ≈−π~2 (the less favorable mode) goes to zero. Fig. 2(b) shows KL averaging mimics the

Depending on the random data sample  the global MLE will concentrate on one or the other of these

bi-modal distribution of the global MLE across data samples; the less likely mode vanishes slightly
slower. In contrast  the linear average takes the arithmetic average of local models from both of
these two local modes  giving unreasonable parameter estimates that are close to neither (Fig. 2(c)).

(n= 10)

(n= 10)

(n= 10)

(a). Global MLE ˆθmle

(b). KL Average ˆθKL

(c). Linear Average ˆθlinear

Figure 2: Result on the toy model in Example 4.3 with model misspeciﬁcation: scatter plots of the
estimated parameters vs. the total sample size n (with 10 000 random trials for each ﬁxed n). The

inside ﬁgures are the densities of the estimated parameters with ﬁxed n= 10. Both global MLE and
KL-average concentrate on two locations(±π~2)  and the less favorable(−π~2) vanishes when the
sample sizes are large (e.g.  n> 250). In contrast  the linear approach averages local MLEs from the

two modes  giving unreasonable estimates spread across the full interval.

7

1502505001000−8−6−4Total Sample Size (n) Linear−AvgKL−AvgTheoretical1502505001000−6−5−4−3−2Total Sample Size (n)1502505001000−4−3.5−3Total Sample Size (n) Linear−AvgKL−AvgGlobal MLE1502505001000−4−3−2Total Sample Size (n)⇡/2⇡/20✓101001000−π/20π/2Total Sample Size (n)Estimted Parameter−π/20π/2101001000−π/20π/2Total Sample Size (n)Estimted Parameter−π/20π/2101001000−π/20π/2Total Sample Size (n)Estimted Parameter−π/20π/2(a) Training LL
(random partition)

(b) Test LL

(random partition)

(label-wise partition)
Figure 3: Learning Gaussian mixture models on MNIST: training and test log-likelihoods of dif-
ferent methods with varying training size n. In (a)-(b)  the data are partitioned into 10 sub-groups
uniformly at random (ensuring sub-samples are i.i.d.); in (c)-(d) the data are partitioned according
to their digit labels. The number of mixture components is ﬁxed to be 10.

(c) Training LL
(label-wise partition)

(d) Test LL

Figure 4: Learning Gaussian mixture models
on the YearPredictionMSD data set. The data
are randomly partitioned into 10 sub-groups 
and we use 10 mixture components.

(a) Training log-likelihood

(b) Test log-likelihood

5.2 Gaussian Mixture Models on Real Datasets

We next consider learning Gaussian mixture models. Because component indexes may be arbitrarily
switched  na¨ıve linear averaging is problematic; we consider a matched linear average that ﬁrst
matches indices by minimizing the sum of the symmetric KL divergences of the different mixture
components. The KL average is also difﬁcult to calculate exactly  since the KL divergence between
Gaussian mixtures is intractable. We approximate the KL average using Monte Carlo sampling (with
500 samples per local model)  corresponding to the parametric bootstrap discussed in Section 2.

We experiment on the MNIST dataset and the YearPredictionMSD dataset in the UCI repository 
where the training data is partitioned into 10 sub-groups randomly and evenly. In both cases  we use
the original training/test split; we use the full testing set  and vary the number of training examples
n by randomly sub-sampling from the full training set (averaging over 100 trials). We take the ﬁrst
100 principal components when using MNIST. Fig. 3(a)-(b) and 4(a)-(b) show the training and test
likelihoods. As a baseline  we also show the average of the log-likelihoods of the local models
(marked as local MLEs in the ﬁgures); this corresponds to randomly selecting a local model as
the combined model. We see that the KL average tends to perform as well as the global MLE  and
remains stable even with small sample sizes. The na¨ıve linear average performs badly even with
large sample sizes. The matched linear average performs as badly as the na¨ıve linear average when
the sample size is small  but improves towards to the global MLE as sample size increases.

For MNIST  we also consider a severely heterogenous data partition by splitting the images into 10
groups according to their digit labels. In this setup  each partition learns a local model only over
its own digit  with no information about the other digits. Fig. 3(c)-(d) shows the KL average still
performs as well as the global MLE  but both the na¨ıve and matched linear average are much worse
even with large sample sizes  due to the dissimilarity in the local models.

6 Conclusion and Future Directions

We study communication-efﬁcient algorithms for learning generative models with distributed data.
Analyzing both a common linear averaging technique and a less common KL-averaging technique
provides both theoretical and empirical insights. Our analysis opens many important future direc-
tions  including extensions to high dimensional inference and efﬁcient approximations for complex
machine learning models  such as LDA and neural networks.

8

500500050000−630−625−620−615Total Sample Size (n)500500050000−635−630−625−620Total Sample Size (n) Local MLEsGlobal MLELinear−Avg−MatchedLinear−AvgKL−Avg500500050000−650−640−630−620Total Sample Size (n)500500050000−650−640−630−620Total Sample Size (n)100010000100000−140−120−100Training Sample Size (n) Local MLEsGlobal MLELinear−Avg−MatchedLinear−AvgKL−Avg100010000100000−140−120−100Training Sample Size (n)Acknowledgements. This work sponsored in part by NSF grants IIS-1065618 and IIS-1254071 
and the US Air Force under Contract No. FA8750-14-C-0011 under DARPA’s PPAML program.

References
Bradley Efron. Deﬁning the curvature of a statistical problem (with applications to second order

efﬁciency). The Annals of Statistics  pages 1189–1242  1975.

Ronald Aylmer Fisher. Theory of statistical estimation. In Mathematical Proceedings of the Cam-

bridge Philosophical Society  volume 22  pages 700–725. Cambridge Univ Press  1925.

C Radhakrishna Rao. Criteria of estimation in large samples. Sankhy¯a: The Indian Journal of

Statistics  Series A  pages 189–206  1963.

Yuchen Zhang  John C Duchi  and Martin J Wainwright. Communication-efﬁcient algorithms for

statistical optimization. Journal of Machine Learning Research  14:3321–3363  2013a.

Srujana Merugu and Joydeep Ghosh. Privacy-preserving distributed clustering using generative

models. In IEEE Int’l Conf. on Data Mining (ICDM)  pages 211–218. IEEE  2003.

Srujana Merugu and Joydeep Ghosh. Distributed learning using generative models. PhD thesis 

University of Texas at Austin  2006.

Joel B Predd  Sanjeev R Kulkarni  and H Vincent Poor. Distributed learning in wireless sensor

networks. John Wiley & Sons: Chichester  UK  2007.

Maria-Florina Balcan  Avrim Blum  Shai Fine  and Yishay Mansour. Distributed learning  commu-

nication complexity and privacy. arXiv preprint arXiv:1204.3514  2012.

Yuchen Zhang  John Duchi  Michael Jordan  and Martin J Wainwright. Information-theoretic lower
bounds for distributed statistical estimation with communication constraints. In Advances in Neu-
ral Information Processing Systems (NIPS)  pages 2328–2336  2013b.

Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and

estimation. arXiv preprint arXiv:1311.3494  2013.

Pedro A Forero  Alfonso Cano  and Georgios B Giannakis. Distributed clustering using wireless

sensor networks. IEEE Journal of Selected Topics in Signal Processing  5(4):707–724  2011.

Yingyu Liang  Maria-Florina Balcan  and Vandana Kanchanapally. Distributed PCA and k-means

clustering. In Big Learning Workshop  NIPS  2013.

Steven L Scott  Alexander W Blocker  Fernando V Bonassi  Hugh A Chipman  Edward I George 
and Robert E McCulloch. Bayes and big data: The consensus Monte Carlo algorithm. In EFaB-
Bayes 250 conference  volume 16  2013.

Xiangyu Wang and David B Dunson. Parallel MCMC via Weierstrass sampler. arXiv preprint

arXiv:1312.4605  2013.

Willie Neiswanger  Chong Wang  and Eric Xing. Asymptotically exact  embarrassingly parallel

MCMC. arXiv preprint arXiv:1311.4780  2013.

Qiang Liu and Alexander Ihler. Distributed parameter estimation via pseudo-likelihood. In Interna-

tional Conference on Machine Learning (ICML)  pages 1487–1494. July 2012.

Z. Meng  D. Wei  A. Wiesel  and A.O. Hero III. Distributed learning of Gaussian graphical models
via marginal likelihoods. In Int’l Conf. on Artiﬁcial Intelligence and Statistics (AISTATS)  2013.
Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
Trends® in Machine Learning  3(1):1–122  2011.

Robert E Kass and Paul W Vos. Geometrical foundations of asymptotic inference  volume 908. John

Wiley & Sons  2011.

Aad W Van der Vaart. Asymptotic statistics  volume 3. Cambridge university press  2000.

9

,Qiang Liu
Alexander Ihler
Kristof Schütt
Pieter-Jan Kindermans
Huziel Enoc Sauceda Felix
Stefan Chmiela
Alexandre Tkatchenko
Klaus-Robert Müller