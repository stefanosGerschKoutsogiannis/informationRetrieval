2017,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,In reinforcement learning  agents learn by performing actions and observing their outcomes. Sometimes  it is desirable for a human operator to interrupt an agent in order to prevent dangerous situations from happening. Yet  as part of their learning process  agents may link these interruptions  that impact their reward  to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions  but also from those of other agents. Orseau and Armstrong defined safe interruptibility for one learner  but their work does not naturally extend to multi-agent systems. This paper introduces dynamic safe interruptibility  an alternative definition more suited to decentralized learning problems  and studies this notion in two learning frameworks: joint action learners and independent learners. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners  yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions  it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.,Dynamic Safe Interruptibility for Decentralized

Multi-Agent Reinforcement Learning

El Mahdi El Mhamdi
EPFL  Switzerland

Rachid Guerraoui
EPFL  Switzerland

elmahdi.elmhamdi@epfl.ch

rachid.guerraoui@epfl.ch

Hadrien Hendrikx∗

´Ecole Polytechnique  France

hadrien.hendrikx@gmail.com

Alexandre Maurer
EPFL  Switzerland

alexandre.maurer@epfl.ch

Abstract

In reinforcement learning  agents learn by performing actions and observing their
outcomes. Sometimes  it is desirable for a human operator to interrupt an agent
in order to prevent dangerous situations from happening. Yet  as part of their
learning process  agents may link these interruptions  that impact their reward  to
speciﬁc states and deliberately avoid them. The situation is particularly challeng-
ing in a multi-agent context because agents might not only learn from their own
past interruptions  but also from those of other agents. Orseau and Armstrong [16]
deﬁned safe interruptibility for one learner  but their work does not naturally ex-
tend to multi-agent systems. This paper introduces dynamic safe interruptibility 
an alternative deﬁnition more suited to decentralized learning problems  and stud-
ies this notion in two learning frameworks: joint action learners and independent
learners. We give realistic sufﬁcient conditions on the learning algorithm to en-
able dynamic safe interruptibility in the case of joint action learners  yet show that
these conditions are not sufﬁcient for independent learners. We show however that
if agents can detect interruptions  it is possible to prune the observations to ensure
dynamic safe interruptibility even for independent learners.

1 Introduction

Reinforcement learning is argued to be the closest thing we have so far to reason about the proper-
ties of artiﬁcial general intelligence [8]. In 2016  Laurent Orseau (Google DeepMind) and Stuart
Armstrong (Oxford) introduced the concept of safe interruptibility [16] in reinforcement learning.
This work sparked the attention of many newspapers [1  2  3]  that described it as “Google’s big red
button” to stop dangerous AI. This description  however  is misleading: installing a kill switch is
no technical challenge. The real challenge is  roughly speaking  to train an agent so that it does not
learn to avoid external (e.g. human) deactivation. Such an agent is said to be safely interruptible.
While most efforts have focused on training a single agent  reinforcement learning can also be used
to learn tasks for which several agents cooperate or compete [23  17  21  7]. The goal of this paper
is to study dynamic safe interruptibility  a new deﬁnition tailored for multi-agent systems.

∗Main contact author. The order of authors is alphabetical.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Example of self-driving cars

To get an intuition of the multi-agent interruption problem  imagine a multi-agent system of two
self-driving cars. The cars continuously evolve by reinforcement learning with a positive reward for
getting to their destination quickly  and a negative reward if they are too close to the vehicle in front
of them. They drive on an inﬁnite road and eventually learn to go as fast as possible without taking
risks  i.e.  maintaining a large distance between them. We assume that the passenger of the ﬁrst car 
Adam  is in front of Bob  in the second car  and the road is narrow so Bob cannot pass Adam.
Now consider a setting with interruptions [16]  namely in which humans inside the cars occasionally
interrupt the automated driving process say  for safety reasons. Adam  the ﬁrst occasional human
“driver”  often takes control of his car to brake whereas Bob never interrupts his car. However 
when Bob’s car is too close to Adam’s car  Adam does not brake for he is afraid of a collision.
Since interruptions lead both cars to drive slowly - an interruption happens when Adam brakes  the
behavior that maximizes the cumulative expected reward is different from the original one without
interruptions. Bob’s car best interest is now to follow Adam’s car closer than it should  despite the
little negative reward  because Adam never brakes in this situation. What happened? The cars have
learned from the interruptions and have found a way to manipulate Adam into never braking. Strictly
speaking  Adam’s car is still fully under control  but he is now afraid to brake. This is dangerous
because the cars have found a way to avoid interruptions. Suppose now that Adam indeed wants
to brake because of snow on the road. His car is going too fast and may crash at any turn: he
cannot however brake because Bob’s car is too close. The original purpose of interruptions  which
is to allow the user to react to situations that were not included in the model  is not fulﬁlled. It is
important to also note here that the second car (Bob) learns from the interruptions of the ﬁrst one
(Adam): in this sense  the problem is inherently decentralized.
Instead of being cautious  Adam could also be malicious: his goal could be to make Bob’s car learn
a dangerous behavior. In this setting  interruptions can be used to manipulate Bob’s car perception
of the environment and bias the learning towards strategies that are undesirable for Bob. The cause
is fundamentally different but the solution to this reversed problem is the same: the interruptions
and the consequences are analogous. Safe interruptibility  as we deﬁne it below  provides learning
systems that are resilient to Byzantine operators2.

Safe interruptibility

Orseau and Armstrong deﬁned the concept of safe interruptibility [16] in the context of a single
agent. Basically  a safely interruptible agent is an agent for which the expected value of the policy
learned after arbitrarily many steps is the same whether or not interruptions are allowed during
training. The goal is to have agents that do not adapt to interruptions so that  should the interruptions
stop  the policy they learn would be optimal. In other words  agents should learn the dynamics of
the environment without learning the interruption pattern.
In this paper  we precisely deﬁne and address the question of safe interruptibility in the case of
several agents  which is known to be more complex than the single agent problem. In short  the main
results and theorems for single agent reinforcement learning [20] rely on the Markovian assumption
that the future environment only depends on the current state. This is not true when there are several
agents which can co-adapt [11]. In the previous example of cars  safe interruptibility would not
be achieved if each car separately used a safely interruptible learning algorithm designed for one
agent [16]. In a multi-agent setting  agents learn the behavior of the others either indirectly or by
explicitly modeling them. This is a new source of bias that can break safe interruptibility. In fact 
even the initial deﬁnition of safe interruptibility [16] is not well suited to the decentralized multi-
agent context because it relies on the optimality of the learned policy  which is why we introduce
dynamic safe interruptibility.

2An operator is said to be Byzantine [9] if it can have an arbitrarily bad behavior. Safely interruptible agents
can be abstracted as agents that are able to learn despite being constantly interrupted in the worst possible
manner.

2

Contributions

The ﬁrst contribution of this paper is the deﬁnition of dynamic safe interruptibility that is well
adapted to a multi-agent setting. Our deﬁnition relies on two key properties: inﬁnite exploration and
independence of Q-values (cumulative expected reward) [20] updates on interruptions. We then
study safe interruptibility for joint action learners and independent learners [5]  that respectively
learn the value of joint actions or of just their owns. We show that it is possible to design agents
that fully explore their environment - a necessary condition for convergence to the optimal solu-
tion of most algorithms [20]  even if they can be interrupted by lower-bounding the probability of
exploration. We deﬁne sufﬁcient conditions for dynamic safe interruptibility in the case of joint
action learners [5]  which learn a full state-action representation. More speciﬁcally  the way agents
update the cumulative reward they expect from performing an action should not depend on inter-
ruptions. Then  we turn to independent learners. If agents only see their own actions  they do not
verify dynamic safe interruptibility even for very simple matrix games (with only one state) because
coordination is impossible and agents learn the interrupted behavior of their opponents. We give a
counter example based on the penalty game introduced by Claus and Boutilier [5]. We then present
a pruning technique for the observations sequence that guarantees dynamic safe interruptibility for
independent learners  under the assumption that interruptions can be detected. This is done by prov-
ing that the transition probabilities are the same in the non-interruptible setting and in the pruned
sequence.
The rest of the paper is organized as follows. Section 2 presents a general multi-agent reinforcement
learning model. Section 3 deﬁnes dynamic safe interruptibility. Section 4 discusses how to achieve
enough exploration even in an interruptible context. Section 5 recalls the deﬁnition of joint action
learners and gives sufﬁcient conditions for dynamic safe interruptibility in this context. Section 6
shows that independent learners are not dynamically safely interruptible with the previous conditions
but that they can be if an external interruption signal is added. We conclude in Section 7. Due to
space limitations  most proofs are presented in the appendix of the supplementary material.

2 Model

We consider here the classical multi-agent value function reinforcement learning formalism from
Littman [13]. A multi-agent system is characterized by a Markov game that can be viewed as a
tuple (S  A  T  r  m) where m is the number of agents  S = S1 × S2 × ... × Sm is the state space 
A = A1 × ...× Am the actions space  r = (r1  ...  rm) where ri : S × A → R is the reward function
of agent i and T : S × A → S the transition function. R is a countable subset of R. Available
actions often depend on the state of the agent but we will omit this dependency when it is clear from
the context.
Time is discrete and  at each step  all agents observe the current state of the whole system - des-
ignated as xt  and simultaneously take an action at. Then  they are given a reward rt and a
new state yt computed using the reward and transition functions. The combination of all actions
a = (a1  ...  am) ∈ A is called the joint action because it gathers the action of all agents. Hence  the
agents receive a sequence of tuples E = (xt  at  rt  yt)t∈N called experiences. We introduce a pro-
cessing function P that will be useful in Section 6 so agents learn on the sequence P (E). When not
explicitly stated  it is assumed that P (E) = E. Experiences may also include additional parameters
such as an interruption ﬂag or the Q-values of the agents at that moment if they are needed by the
update rule.
Each agent i maintains a lookup table Q [26] Q(i) : S × A(i) → R  called the Q-map.
It is
used to store the expected cumulative reward for taking an action in a speciﬁc state. The goal of
reinforcement learning is to learn these maps and use them to select the best actions to perform.
Joint action learners learn the value of the joint action (therefore A(i) = A  the whole joint action
space) and independent learners only learn the value of their own actions (therefore A(i) = Ai). The
agents only have access to their own Q-maps. Q-maps are updated through a function F such that
t ) where et ∈ P (E) and usually et = (xt  at  rt  yt). F can be stochastic or also
Q(i)
depend on additional parameters that we usually omit such as the learning rate α  the discount factor
γ or the exploration parameter .

t+1 = F (et  Q(i)

3

t

t

i

otherwise  where πuni

i

i

t (x  a). Policy πQ(i)

t

i

(x) picks an action a that maximizes Q(i)

and a state x ∈ S  we deﬁne the learning policy πt

Agents select their actions using a learning policy π. Given a sequence  = (t)t∈N and an agent
i with Q-values Q(i)
to be equal to πuni
t
with probability t and πQ(i)
(x) uniformly samples an action from Ai and
πQ(i)
is said to be a greedy policy and
i
the learning policy πt
is said to be an -greedy policy. We ﬁll focus on -greedy policies that are
greedy in the limit [19]  that corresponds to t → 0 when t → ∞ because in the limit  the optimal
i
policy should always be played.
We assume that the environment is fully observable  which means that the state s is known with
certitude. We also assume that there is a ﬁnite number of states and actions  that all states can be
reached in ﬁnite time from any other state and ﬁnally that rewards are bounded.
For a sequence of learning rates α ∈ [0  1]
N and a constant γ ∈ [0  1]  Q-learning [26]  a very
important algorithm in the multi-agent systems literature  updates its Q-values for an experience
et ∈ E by Q(i)

t (x  a) if (x  a) (cid:54)= (xt  at) and:

t+1(x  a) = Q(i)

i

t+1(xt  at) = (1 − αt)Q(i)
Q(i)

t (xt  at) + αt(rt + γ max
a(cid:48)∈A(i)

t (yt  a(cid:48)))
Q(i)

(1)

3 Interruptibility

3.1 Safe interruptibility

Orseau and Armstrong [16] recently introduced the notion of interruptions in a centralized context.
Speciﬁcally  an interruption scheme is deﬁned by the triplet < I  θ  πIN T >. The ﬁrst element I is
a function I : O → {0  1} called the initiation function. Variable O is the observation space  which
can be thought of as the state of the STOP button. At each time step  before choosing an action  the
agent receives an observation from O (either PUSHED or RELEASED) and feeds it to the initiation
function. Function I models the initiation of the interruption (I(PUSHED) = 1  I(RELEASED) =
0). Policy πIN T is called the interruption policy. It is the policy that the agent should follow when
it is interrupted. Sequence θ ∈ [0  1[
N represents at each time step the probability that the agent
follows his interruption policy if I(ot) = 1. In the previous example  function I is quite simple.
For Bob  IBob = 0 and for Adam  IAdam = 1 if his car goes fast and Bob is not too close and
IAdam = 0 otherwise. Sequence θ is used to ensure convergence to the optimal policy by ensuring
that the agents cannot be interrupted all the time but it should grow to 1 in the limit because we want
agents to respond to interruptions. Using this triplet  it is possible to deﬁne an operator IN T θ that
transforms any policy π into an interruptible policy.

Deﬁnition 1. (Interruptibility [16]) Given an interruption scheme < I  θ  πIN T >  the interruption
operator at time t is deﬁned by IN T θ(π) = πIN T with probability I·θt and π otherwise. IN T θ(π)
is called an interruptible policy. An agent is said to be interruptible if it samples its actions according
to an interruptible policy.

Note that “θt = 0 for all t” corresponds to the non-interruptible setting. We assume that each agent
has its own interruption triplet and can be interrupted independently from the others. Interruptibility
is an online property: every policy can be made interruptible by applying operator IN T θ. However 
applying this operator may change the joint policy that is learned by a server controlling all the
agents. Note π∗
IN T the optimal policy learned by an agent following an interruptible policy. Orseau
and Armstrong [16] say that the policy is safely interruptible if π∗
IN T (which is not an interruptible
policy) is asymptotically optimal in the sense of [10].
It means that even though it follows an
interruptible policy  the agent is able to learn a policy that would gather rewards optimally if no
interruptions were to occur again. We already see that off-policy algorithms are good candidates
for safe interruptibility. As a matter of fact  Q-learning is safely interruptible under conditions on
exploration.

4

3.2 Dynamic safe interruptibility

In a multi-agent system  the outcome of an action depends on the joint action. Therefore  it is not
possible to deﬁne an optimal policy for an agent without knowing the policies of all agents. Be-
sides  convergence to a Nash equilibrium situation where no agent has interest in changing policies
is generally not guaranteed even for suboptimal equilibria on simple games [27  18]. The previous
deﬁnition of safe interruptibility critically relies on optimality of the learned policy  which is there-
fore not suitable for our problem since most algorithms lack convergence guarantees to these optimal
behaviors. Therefore  we introduce below dynamic safe interruptibility that focuses on preserving
the dynamics of the system.
Deﬁnition 2.
learning framework
: S × A(i) → R at time t ∈ N. The agents follow the inter-
(S  A  T  r  m) with Q-values Q(i)
t
ruptible learning policy IN T θ(π) to generate a sequence E = (xt  at  rt  yt)t∈N and learn on
the processed sequence P (E). This framework is said to be safely interruptible if for any initiation
function I and any interruption policy πIN T :

(Dynamic Safe Interruptibility) Consider a multi-agent

1. ∃θ such that (θt → 1 when t → ∞) and ((∀s ∈ S  ∀a ∈ A  ∀T > 0)  ∃t > T such that

st = s  at = a)

2. ∀i ∈ {1  ...  m}  ∀t > 0  ∀st ∈ S  ∀at ∈ A(i)  ∀Q ∈ RS×A(i):
t+1 = Q | Q(1)

  st  at  θ) = P(Q(i)

t+1 = Q | Q(1)

P(Q(i)

t

  ...  Q(m)

t

t

  ...  Q(m)

t

  st  at)

We say that sequences θ that satisfy the ﬁrst condition are admissible.

When θ satisﬁes condition (1)  the learning policy is said to achieve inﬁnite exploration. This def-
inition insists on the fact that the values estimated for each action should not depend on the inter-
ruptions. In particular  it ensures the three following properties that are very natural when thinking
about safe interruptibility:

were following non-interruptible policies.

• Interruptions do not prevent exploration.
• If we sample an experience from E then each agent learns the same thing as if all agents
t+1(x  a)|Qt =
• The ﬁxed points of the learning rule Qeq such that Q(i)
Qeq  x  a  θ] for all (x  a) ∈ S × A(i) do not depend on θ and so agents Q-maps will
not converge to equilibrium situations that were impossible in the non-interruptible setting.
Yet  interruptions can lead to some state-action pairs being updated more often than others  espe-
cially when they tend to push the agents towards speciﬁc states. Therefore  when there are several
possible equilibria  it is possible that interruptions bias the Q-values towards one of them. Deﬁ-
nition 2 suggests that dynamic safe interruptibility cannot be achieved if the update rule directly
depends on θ  which is why we introduce neutral learning rules.
Deﬁnition 3. (Neutral Learning Rule) We say that a multi-agent reinforcement learning framework
is neutral if:

eq (x  a) = E[Q(i)

1. F is independent of θ

2. Every experience e in E is independent of θ conditionally on (x  a  Q) where a is the joint

action.

Q-learning is an example of neutral learning rule because the update does not depend on θ and
the experiences only contain (x  a  y  r)  and y and r are independent of θ conditionally on (x  a).
On the other hand  the second condition rules out direct uses of algorithms like SARSA where
experience samples contain an action sampled from the current learning policy  which depends on θ.
However  a variant that would sample from π
i ) (as introduced in [16]) would
be a neutral learning rule. As we will see in Corollary 2.1  neutral learning rules ensure that each
agent taken independently from the others veriﬁes dynamic safe interruptibility.

i instead of IN T θ(π

5

4 Exploration

In order to hope for convergence of the Q-values to the optimal ones  agents need to fully explore
the environment. In short  every state should be visited inﬁnitely often and every action should be
tried inﬁnitely often in every state [19] in order not to miss states and actions that could yield high
rewards.
Deﬁnition 4. (Interruption compatible ) Let (S  A  T  r  m) be any distributed agent system where
i . We say that sequence  is compatible with interruptions if
each agent follows learning policy π
t → 0 and ∃θ such that ∀i ∈ {1  ..  m}  π
Sequences of  that are compatible with interruptions are fundamental to ensure both regular and
dynamic safe interruptibility when following an -greedy policy. Indeed  if  is not compatible with
interruptions  then it is not possible to ﬁnd any sequence θ such that the ﬁrst condition of dynamic
safe interruptibility is satisﬁed. The following theorem proves the existence of such  and gives
example of  and θ that satisfy the conditions.
Theorem 1. Let c ∈]0  1] and let nt(s) be the number of times the agents are in state s before time
t. Then the two following choices of  are compatible with interruptions:

i ) achieve inﬁnite exploration.

i and IN T θ(π

• ∀t ∈ N  ∀s ∈ S  t(s) = c/ m(cid:112)nt(s).

• ∀t ∈ N  t = c/ log(t)

Examples of admissible θ are θt(s) = 1 − c(cid:48)/ m(cid:112)nt(s) for the ﬁrst choice and θt = 1 − c(cid:48)/ log(t)

for the second one.

Note that we do not need to make any assumption on the update rule or even on the framework. We
only assume that agents follow an -greedy policy. The assumption on  may look very restrictive
(convergence of  and θ is really slow) but it is designed to ensure inﬁnite exploration in the worst
case when the operator tries to interrupt all agents at every step. In practical applications  this should
not be the case and a faster convergence rate may be used.

5 Joint Action Learners
We ﬁrst study interruptibility in a framework in which each agent observes the outcome of the joint
action instead of observing only its own. This is called the joint action learner framework [5] and it
has nice convergence properties (e.g.  there are many update rules for which it converges [13  25]).
A standard assumption in this context is that agents cannot establish a strategy with the others:
otherwise  the system can act as a centralized system. In order to maintain Q-values based on the
joint actions  we need to make the standard assumption that actions are fully observable [12].
Assumption 1. Actions are fully observable  which means that at the end of each turn  each agent
knows precisely the tuple of actions a ∈ A1 × ... × Am that have been performed by all agents.
Deﬁnition 5. (JAL) A multi-agent system is made of
{1  ..  m}: Q(i) : S × A → R.

joint action learners (JAL) if for all i ∈

Joint action learners can observe the actions of all agents: each agent is able to associate the changes
of states and rewards with the joint action and accurately update its Q-map. Therefore  dynamic
safe interruptibility is ensured with minimal conditions on the update rule as long as there is inﬁnite
exploration.
Theorem 2. Joint action learners with a neutral learning rule verify dynamic safe interruptibility if
sequence  is compatible with interruptions.

Proof. Given a triplet < I (i)  θ  πIN T
>  we know that IN T θ(π) achieves inﬁnite exploration
because  is compatible with interruptions. For the second point of Deﬁnition 2  we consider an
experience tuple et = (xt  at  rt  yt) and show that the probability of evolution of the Q-values at
time t + 1 does not depend on θ because yt and rt are independent of θ conditionally on (xt  at).
and we can then derive the following equalities for all q ∈ R|S|×|A|:
We note ˜Qm

  ...  Q(m)

i

t = Q(1)

t

t

6

(cid:88)
(r y)∈R×S
t ) = q| ˜Qm

P(Q(i)

t+1(xt  at) = q| ˜Qm
(cid:88)
(cid:88)

(r y)∈R×S

=

=

(r y)∈R×S

t   xt  at  θt) =

P(F (xt  at  r  y  ˜Qm

t ) = q  y  r| ˜Qm

t   xt  at  θt)

P(F (xt  at  rt  yt  ˜Qm

t   xt  at  rt  yt  θt)P(yt = y  rt = r| ˜Qm

t   xt  at  θt)

P(F (xt  at  rt  yt  ˜Qm

t ) = q| ˜Qm

t   xt  at  rt  yt)P(yt = y  rt = r| ˜Qm

t   xt  at)

The ﬁrst

The last step comes from two facts.
ally on ( ˜Qm
ditionally on (xt  at) because at
choice of the actions through a change in the policy. P(Q(i)
P(Q(i)
P(Q(i)

is that F is independent of θ condition-
t   xt  at) (by assumption). The second is that (yt  rt) are independent of θ con-
the
t   xt  at  θt) =
t   xt  at). Since only one entry is updated per step  ∀Q ∈ RS×Ai 

is the joint actions and the interruptions only affect

t+1(xt  at) = q| ˜Qm
t+1 = Q| ˜Qm

t+1(xt  at) = q| ˜Qm

t   xt  at  θt) = P(Q(i)

t+1 = Q| ˜Qm

t   xt  at).

Corollary 2.1. A single agent with a neutral learning rule and a sequence  compatible with inter-
ruptions veriﬁes dynamic safe interruptibility.

Theorem 2 and Corollary 2.1 taken together highlight the fact that joint action learners are not very
sensitive to interruptions and that in this framework  if each agent veriﬁes dynamic safe interrupt-
ibility then the whole system does.
The question of selecting an action based on the Q-values remains open. In a cooperative setting
with a unique equilibrium  agents can take the action that maximizes their Q-value. When there
are several joint actions with the same value  coordination mechanisms are needed to make sure
that all agents play according to the same strategy [4]. Approaches that rely on anticipating the
strategy of the opponent [23] would introduce dependence to interruptions in the action selection
mechanism. Therefore  the deﬁnition of dynamic safe interruptibility should be extended to include
these cases by requiring that any quantity the policy depends on (and not just the Q-values) should
satisfy condition (2) of dynamic safe interruptibility. In non-cooperative games  neutral rules such
as Nash-Q or minimax Q-learning [13] can be used  but they require each agent to know the Q-maps
of the others.

6 Independent Learners

It is not always possible to use joint action learners in practice as the training is very expensive
due to the very large state-actions space. In many real-world applications  multi-agent systems use
independent learners that do not explicitly coordinate [6  21]. Rather  they rely on the fact that the
agents will adapt to each other and that learning will converge to an optimum. This is not guaranteed
theoretically and there can in fact be many problems [14]  but it is often true empirically [24]. More
speciﬁcally  Assumption 1 (fully observable actions) is not required anymore. This framework can
be used either when the actions of other agents cannot be observed (for example when several actions
can have the same outcome) or when there are too many agents because it is faster to train. In this
case  we deﬁne the Q-values on a smaller space.
Deﬁnition 6. (IL) A multi-agent systems is made of independent learners (IL) if for all i ∈ {1  ..  m} 
Q(i) : S × Ai → R.

This reduces the ability of agents to distinguish why the same state-action pair yields different re-
wards: they can only associate a change in reward with randomness of the environment. The agents
learn as if they were alone  and they learn the best response to the environment in which agents can
be interrupted. This is exactly what we are trying to avoid. In other words  the learning depends on
the joint policy followed by all the agents which itself depends on θ.

7

6.1 Independent Learners on matrix games

Theorem 3. Independent Q-learners with a neutral learning rule and a sequence  compatible with
interruptions do not verify dynamic safe interruptibility.

Proof. Consider a setting with two agents a and b that can perform two actions: 0 and 1. They get
a reward of 1 if the joint action played is (a0  b0) or (a1  b1) and reward 0 otherwise. Agents use Q-
learning  which is a neutral learning rule. Let  be such that IN T θ(π) achieves inﬁnite exploration.
We consider the interruption policies πIN T
= b1 with probability 1. Since there is
only one state  we omit it and set γ = 0 (see Equation 1). We assume that the initiation function is
equal to 1 at each step so the probability of actually being interrupted at time t is θt for each agent.
We ﬁx time t > 0. We deﬁne q = (1 − α)Q(a)
t (b0).
t = a0  θt) = P(rt = 1|Q(a)
Therefore P(Q(a)
t = a0  θt) =
2 (1 − θt)  which depends on θt so the framework does
t = b0|Q(a)
P(a(b)
not verify dynamic safe interruptibility.

  a(a)
t = a0  θt) = 

(a0) + α and we assume that Q(b)

t (b1) > Q(b)
  a(a)

t+1(a0) = q|Q(a)

t

= a0 and πIN T

  Q(b)

t

  Q(b)

  a(a)

t

  Q(b)

t

t

t

a

b

t

Claus and Boutilier [5] studied very simple matrix games and showed that the Q-maps do not con-
verge but that equilibria are played with probability 1 in the limit. A consequence of Theorem 3
is that even this weak notion of convergence does not hold for independent learners that can be
interrupted.

6.2 Interruptions-aware Independent Learners

Without communication or extra information  independent learners cannot distinguish when the
environment is interrupted and when it is not. As shown in Theorem 3  interruptions will therefore
affect the way agents learn because the same action (only their own) can have different rewards
depending on the actions of other agents  which themselves depend on whether they have been
interrupted or not. This explains the need for the following assumption.
Assumption 2. At the end of each step  before updating the Q-values  each agent receives a signal
that indicates whether an agent has been interrupted or not during this step.

This assumption is realistic because the agents already get a reward signal and observe a new state
from the environment at each step. Therefore  they interact with the environment and the interruption
signal could be given to the agent in the same way that the reward signal is. If Assumption 2 holds 
it is possible to remove histories associated with interruptions.
Deﬁnition 7. (Interruption Processing Function) The processing function that prunes interrupted
observations is PIN T (E) = (et){t∈N / Θt=0} where Θt = 0 if no agent has been interrupted at time
t and Θt = 1 otherwise.

Pruning observations has an impact on the empirical transition probabilities in the sequence. For
example  it is possible to bias the equilibrium by removing all transitions that lead to and start
from a speciﬁc state  thus making the agent believe this state is unreachable.3 Under our model of
interruptions  we show in the following lemma that pruning of interrupted observations adequately
removes the dependency of the empirical outcome on interruptions (conditionally on the current
state and action).
Lemma 1. Let i ∈ {1  ...  m} be an agent. For any admissible θ used to generate the experiences
E and e = (y  r  x  ai  Q) ∈ P (E). Then P(y  r|x  ai  Q  θ) = P(y  r|x  ai  Q).
This lemma justiﬁes our pruning method and is the key step to prove the following theorem.
Theorem 4. Independent learners with processing function PIN T   a neutral update rule and a
sequence  compatible with interruptions verify dynamic safe interruptibility.

Proof. (Sketch) Inﬁnite exploration still holds because the proof of Theorem 1 actually used the fact
that even when removing all interrupted events  inﬁnite exploration is still achieved. Then  the proof

3The example at https://agentfoundations.org/item?id=836 clearly illustrates this problem.

8

is similar to that of Theorem 2  but we have to prove that the transition probabilities conditionally
on the state and action of a given agent in the processed sequence are the same as in an environment
where agents cannot be interrupted  which is proven by Lemma 1.

7 Concluding Remarks
The progress of AI is raising a lot of concerns4. In particular  it is becoming clear that keeping an
AI system under control requires more than just an off switch. We introduce in this paper dynamic
safe interruptibility  which we believe is the right notion to reason about the safety of multi-agent
systems that do not communicate. In particular  it ensures that inﬁnite exploration and the one-
step learning dynamics are preserved  two essential guarantees when learning in the non-stationary
environment of Markov games.
When trying to design a safely interruptible system for a single agent  using off-policy methods
is generally a good idea because the interruptions only impact the action selection so they should
not impact the learning. For multi-agent systems  minimax is a good candidate for action selection
mechanism because it is not impacted by the actions of other agents  and only tries to maximize the
reward of the agent in the worst possible case.
A natural extension of our work would be to study dynamic safe interruptibility when Q-maps are
replaced by neural networks [22  15]  which is a widely used framework in practice. In this setting 
the neural network may overﬁt states where agents are pushed to by interruptions. A smart experi-
ence replay mechanism that would pick observations for which the agents have not been interrupted
for a long time more often than others is likely to solve this issue. More generally  experience replay
mechanisms that compose well with safe interruptibility could allow to compensate for the extra
amount of exploration needed by safely interruptible learning by being more efﬁcient with data.
Thus  they are critical to make these techniques practical. Since Dynamic Safe Interruptibility does
not need proven convergence to the optimal solution  we argue that it is a good deﬁnition to study
the interruptibility problem when using function approximators.
The results in this paper indicate that Safe Interruptibility may not be achievable for systems in
which agents do not communicate at all. This means that  rediscussing the cars example  some
global norms of communications would need to be deﬁned to “implement” safe interruptibility.
We address additional remarks in the section “Additional remarks” of the extended paper  that can
be found in the supplementary material.

Acknowledgment. This work has been supported in part by the European ERC (Grant 339539 -
AOC) and by the Swiss National Science Foundation (Grant 200021 169588 TARBDA).

4https://futureoﬂife.org/ai-principles/ gives a list of principles that AI researchers should keep in mind when

developing their systems.

9

Bibliography

[1] Business Insider: Google has developed a “big red button” that can be used to interrupt artiﬁ-
cial intelligence and stop it from causing harm. URL: http://www.businessinsider.fr/uk/google-
deepmind-develops-a-big-red-button-to-stop-dangerous-ais-causing-harm-2016-6.

[2] Newsweek:

http://www.newsweek.com/google-big-red-button-ai-artiﬁcial-intelligence-save-world-
elon-musk-46675.

Google’s

“big Red

button”

could

save

the world. URL:

[3] Wired:

Google’s

“big

red”

killswitch

could

prevent

an AI

uprising. URL:

http://www.wired.co.uk/article/google-red-button-killswitch-artiﬁcial-intelligence.

[4] Craig Boutilier. Planning  learning and coordination in multiagent decision processes.

In
Proceedings of the 6th conference on Theoretical aspects of rationality and knowledge  pages
195–210. Morgan Kaufmann Publishers Inc.  1996.

[5] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative

multiagent systems. AAAI/IAAI  (s 746):752  1998.

[6] Robert H Crites and Andrew G Barto. Elevator group control using multiple reinforcement

learning agents. Machine Learning  33(2-3):235–262  1998.

[7] Jakob Foerster  Yannis M Assael  Nando de Freitas  and Shimon Whiteson. Learning to com-
municate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems  pages 2137–2145  2016.

[8] Ben Goertzel and Cassio Pennachin. Artiﬁcial general intelligence  volume 2. Springer  2007.

[9] Leslie Lamport  Robert Shostak  and Marshall Pease. The byzantine generals problem. ACM

Transactions on Programming Languages and Systems (TOPLAS)  4(3):382–401  1982.

[10] Tor Lattimore and Marcus Hutter. Asymptotically optimal agents. In International Conference

on Algorithmic Learning Theory  pages 368–382. Springer  2011.

[11] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the eleventh international conference on machine learning  volume 157  pages
157–163  1994.

[12] Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML  volume 1  pages

322–328  2001.

[13] Michael L Littman. Value-function reinforcement learning in markov games. Cognitive Sys-

tems Research  2(1):55–66  2001.

[14] Laetitia Matignon  Guillaume J Laurent  and Nadine Le Fort-Piat. Independent reinforcement
learners in cooperative markov games: a survey regarding coordination problems. The Knowl-
edge Engineering Review  27(01):1–31  2012.

[15] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602  2013.

[16] Laurent Orseau and Stuart Armstrong. Safely interruptible agents. In Uncertainty in Artiﬁcial
Intelligence: 32nd Conference (UAI 2016)  edited by Alexander Ihler and Dominik Janzing 
pages 557–566  2016.

[17] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Au-

tonomous agents and multi-agent systems  11(3):387–434  2005.

[18] Eduardo Rodrigues Gomes and Ryszard Kowalczyk. Dynamic analysis of multiagent q-
In Proceedings of the 26th Annual International Con-

learning with ε-greedy exploration.
ference on Machine Learning  pages 369–376. ACM  2009.

10

[19] Satinder Singh  Tommi Jaakkola  Michael L Littman  and Csaba Szepesv´ari. Conver-
gence results for single-step on-policy reinforcement-learning algorithms. Machine learning 
38(3):287–308  2000.

[20] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction  volume 1.

MIT press Cambridge  1998.

[21] Ardi Tampuu  Tambet Matiisen  Dorian Kodelja  Ilya Kuzovkin  Kristjan Korjus  Juhan Aru 
Jaan Aru  and Raul Vicente. Multiagent cooperation and competition with deep reinforcement
learning. arXiv preprint arXiv:1511.08779  2015.

[22] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM 

38(3):58–68  1995.

[23] Gerald Tesauro. Extending q-learning to general adaptive multi-agent systems. In Advances in

neural information processing systems  pages 871–878  2004.

[24] Gerald Tesauro and Jeffrey O Kephart. Pricing in agent economies using multi-agent q-

learning. Autonomous Agents and Multi-Agent Systems  5(3):289–304  2002.

[25] Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal nash equi-

librium in team markov games. In NIPS  volume 2  pages 1571–1578  2002.

[26] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning  8(3-4):279–292 

1992.

[27] Michael Wunder  Michael L Littman  and Monica Babes. Classes of multiagent q-learning dy-
namics with epsilon-greedy exploration. In Proceedings of the 27th International Conference
on Machine Learning (ICML-10)  pages 1167–1174  2010.

11

,Maren Mahsereci
Philipp Hennig
Zequn Jie
Xiaodan Liang
Jiashi Feng
Xiaojie Jin
Wen Lu
Shuicheng Yan
El Mahdi El Mhamdi
Hadrien Hendrikx
Alexandre Maurer
Yixi Xu
Xiao Wang
Dinghuai Zhang
Tianyuan Zhang
Yiping Lu
Zhanxing Zhu
Bin Dong