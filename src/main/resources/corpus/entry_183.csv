2010,A Reduction from Apprenticeship Learning to Classification,We provide new theoretical results for apprenticeship learning  a variant of reinforcement learning in which the true reward function is unknown  and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice  it has been subject to very little formal analysis. We prove that  if the learned classifier has error rate $\eps$  the difference between the value of the apprentice's policy and the expert's policy is $O(\sqrt{\eps})$. Further  we prove that this difference is only $O(\eps)$ when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy  but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good  but demonstrations are expensive or difficult to obtain.,A Reduction from Apprenticeship Learning to

Classiﬁcation

Department of Computer and Information Science

Department of Computer Science

Robert E. Schapire

Princeton University
Princeton  NJ 08540

Umar Syed∗

University of Pennsylvania

Philadelphia  PA 19104

usyed@cis.upenn.edu

schapire@cs.princeton.edu

Abstract

We provide new theoretical results for apprenticeship learning  a variant of rein-
forcement learning in which the true reward function is unknown  and the goal
is to perform well relative to an observed expert. We study a common approach
to learning from expert demonstrations: using a classiﬁcation algorithm to learn
to imitate the expert’s behavior. Although this straightforward learning strategy
is widely-used in practice  it has been subject to very little formal analysis. We
prove that  if the learned classiﬁer has error rate ǫ  the difference between the
value of the apprentice’s policy and the expert’s policy is O(√ǫ). Further  we
prove that this difference is only O(ǫ) when the expert’s policy is close to optimal.
This latter result has an important practical consequence: Not only does imitating
a near-optimal expert result in a better policy  but far fewer demonstrations are
required to successfully imitate such an expert. This suggests an opportunity for
substantial savings whenever the expert is known to be good  but demonstrations
are expensive or difﬁcult to obtain.

1

Introduction

Apprenticeship learning is a variant of reinforcement learning  ﬁrst introduced by Abbeel & Ng [1]
(see also [2  3  4  5  6])  designed to address the difﬁculty of correctly specifying the reward function
in many reinforcement learning problems. The basic idea underlying apprenticeship learning is that
a learning agent  called the apprentice  is able to observe another agent  called the expert  behaving
in a Markov Decision Process (MDP). The goal of the apprentice is to learn a policy that is at least
as good as the expert’s policy  relative to an unknown reward function. This is a weaker requirement
than the usual goal in reinforcement learning  which is to ﬁnd a policy that maximizes reward.
The development of the apprenticeship learning framework was prompted by the observation that 
although reward functions are often difﬁcult to specify  demonstrations of good behavior by an
expert are usually available. Therefore  by observing such a expert  one can infer information about
the true reward function without needing to specify it.

Existing apprenticeship learning algorithms have a number of limitations. For one  they typically
assume that the true reward function can be expressed as a linear combination of a set of known fea-
tures. However  there may be cases where the apprentice is unwilling or unable to assume that the
rewards have this structure. Additionally  most formulations of apprenticeship learning are actually
harder than reinforcement learning; apprenticeship learning algorithms typically invoke reinforce-
ment learning algorithms as subroutines  and their performance guarantees depend strongly on the
quality of these subroutines. Consequently  these apprenticeship learning algorithms suffer from the
same challenges of large state spaces  exploration vs. exploitation trade-offs  etc.  as reinforcement

∗Work done while the author was a student at Princeton University.

1

learning algorithms. This fact is somewhat contrary to the intuition that demonstrations from an
expert — especially a good expert — should make the problem easier  not harder.

Another approach to using expert demonstrations that has received attention primarily in the empir-
ical literature is to passively imitate the expert using a classiﬁcation algorithm (see [7  Section 4] for
a comprehensive survey). Classiﬁcation is the most well-studied machine learning problem  and it
is sensible to leverage our knowledge about this “easier” problem in order to solve a more “difﬁcult”
one. However  there has been little formal analysis of this straightforward learning strategy (the
main recent example is Ross & Bagnell [8]  discussed below). In this paper  we consider a setting
in which an apprentice uses a classiﬁcation algorithm to passively imitate an observed expert in an
MDP  and we bound the difference between the value of the apprentice’s policy and the value of
the expert’s policy in terms of the accuracy of the learned classiﬁer. Put differently  we show that
apprenticeship learning can be reduced to classiﬁcation. The idea of reducing one learning problem
to another was ﬁrst proposed by Zadrozny & Langford [9].

Our main contributions in this paper are a pair of theoretical results. First  we show that the differ-

ence between the value of the apprentice’s policy and the expert’s policy is O(√ǫ) 1 where ǫ ∈ (0  1]

is the error of the learned classiﬁer. Secondly  and perhaps more interestingly  we extend our ﬁrst
result to prove that the difference in policy values is only O(ǫ) when the expert’s policy is close to
optimal. Of course  if one could perfectly imitate the expert  then naturally a near-optimal expert
policy is preferred. But our result implies something further: that near-optimal experts are actually
easier to imitate  in the sense that fewer demonstration are required to achieve the same performance
guarantee. This has important practical consequences. If one is certain a priori that the expert is
demonstrating good behavior  then our result implies that many fewer demonstrations need to be col-
lected than if this were not the case. This can yield substantial savings when expert demonstrations
are expensive or difﬁcult to obtain.

2 Related Work

Several authors have reduced reinforcement learning to simpler problems. Bagnell et al [10] de-
scribed an algorithm for constructing a good nonstationary policy from a sequence of good “one-
step” policies. These policies are only concerned with maximizing reward collected in a single
time step  and are learned with the help of observations from an expert. Langford & Zadrozny
[11] reduced reinforcement learning to a sequence of classiﬁcation problems (see also Blatt & Hero
[12])  but these problems have an unusual structure  and the authors are only able to provide a small
amount of guidance as to how data for these problems can be collected. Kakade & Langford [13]
reduced reinforcement learning to regression  but required additional assumptions about how easily
a learning algorithm can access the entire state space. Importantly  all this work makes the standard
reinforcement learning assumptions that the true rewards are known  and that a learning algorithm
is able to interact directly with the environment. In this paper we are interested in settings where
the reward function is not known  and where the learning algorithm is limited to passively observing
an expert. Concurrently to this work  Ross & Bagnell [8] have described an approach to reducing
imitation learning to classiﬁcation  and some of their analysis resembles ours. However  their frame-
work requires somewhat more than passive observation of the expert  and is focused on improving
the sensitivity of the reduction to the horizon length  not the classiﬁcation error. They also assume
that the expert follows a deterministic policy  and assumption we do not make.

3 Preliminaries

We consider a ﬁnite-horizon MDP  with horizon H. We will allow the state space S to be inﬁnite 
but assume that the action space A is ﬁnite. Let α be the initial state distribution  and θ the transition
function  where θ(s  a ·) speciﬁes the next-state distribution from state s ∈ S under action a ∈ A.
The only assumption we make about the unknown reward function R is that 0 ≤ R(s) ≤ Rmax for
all states s ∈ S  where Rmax is a ﬁnite upper bound on the reward of any state.

1The big-O notation is concealing a polynomial dependence on other problem parameters. We give exact

bounds in the body of the paper.

2

We introduce some notation and deﬁnitions regarding policies. A policy π is stationary if it is a
mapping from states to distributions over actions. In this case  π(s  a) denotes the probability of
taking action a in state s. Let Π be the set of all stationary policies. A policy π is nonstationary if it
belongs to the set ΠH = Π × ··· (H times)··· × Π . In this case  πt(s  a) denotes the probability of
taking action a in state s at time t. Also  if π is nonstationary  then πt refers to the stationary policy
that is equal to the tth component of π. A (stationary or nonstationary) policy π is deterministic if
each one of its action distributions is concentrated on a single action. If a deterministic policy π is
stationary  then π(s) is the action taken in state s  and if π is nonstationary  the πt(s) is the action
taken in state s at time t.

We deﬁne the value function V π
manner:

t (s) for a nonstationary policy π at time t as follows in the usual

V π

t (s)   E" H
Xt′=t

R(st′)(cid:12)(cid:12)(cid:12)

st = s  at′ ∼ πt′ (st′  ·)  st′+1 ∼ θ(st′   at′  ·)# .

So V π
t (s) is the expected cumulative reward for following policy π when starting at state s and time
step t. Note that there are several value functions per nonstationary policy  one for each time step t.
The value of a policy is deﬁned to be V (π)   E[V π
1 (s) | s ∼ α(·)]  and an optimal policy π∗ is one
that satisﬁes π∗   arg maxπ V (π).
We write πE to denote the (possibly nonstationary) expert policy  and V E
V πE
t
that the values of these policies are with respect to the unknown reward function.

t (s) as an abbreviation for
(s). Our goal is to ﬁnd a nonstationary apprentice policy πA such that V (πA) ≥ V (πE). Note

t be the distribution on state-action pairs at time t under policy π. In other words  a sample
t by ﬁrst drawing s1 ∼ α(·)  then following policy π for time steps 1 through
t as
. In a minor abuse of notation  we write s ∼ Dπ
t to mean: draw state-action

Let Dπ
(s  a) is drawn from Dπ
t  which generates a trajectory (s1  a1  . . .   st  at)  and then letting (s  a) = (st  at). We write DE
an abbreviation for DπE
pair (s  a) ∼ Dπ
4 Details and Justiﬁcation of the Reduction

t   and discard a.

t

Our goal is to reduce apprenticeship learning to classiﬁcation  so let us describe exactly how this
reduction is deﬁned  and also justify the utility of such a reduction.
In a classiﬁcation problem  a learning algorithm is given a training set h(x1  y1)  . . .   (xm  ym)i 
where each labeled example (xi  yi) ∈ X ×Y is drawn independently from a distribution D on X ×
Y. Here X is the example space and Y is the ﬁnite set of labels. The learning algorithm is also given
the deﬁnition of a hypothesis class H  which is a set of functions mapping X to Y. The objective
of the learning algorithm is to ﬁnd a hypothesis h ∈ H such that the error Pr(x y)∼D(h(x) 6= y) is
small.
For our purposes  the hypothesis class H is said to be PAC-learnable if there exists a learning
algorithm A such that  whenever A is given a training set of size m = poly( 1
ǫ )  the algorithm
ǫ ) steps and outputs a hypothesis ˆh ∈ H such that  with probability at least 1 − δ 
runs for poly( 1
we have Pr(x y)∼D(cid:16)ˆh(x) 6= y(cid:17) ≤ ǫ∗
H D = inf h∈H Pr(x y)∼D(h(x) 6= y) is the
error of the best hypothesis in H. The expression poly( 1
ǫ ) will typically also depend on other
quantities  such as the number of labels |Y| and the VC-dimension of H [14]  but this dependence is
not germane to our discussion.

H D + ǫ. Here ǫ∗

δ   1

δ   1

δ   1

The existence of PAC-learnable hypothesis classes is the reason that reducing apprenticeship learn-
ing to classiﬁcation is a sensible endeavor. Suppose that the apprentice observes m independent

t  ai

H(cid:1).
trajectories from the expert’s policy πE  where the ith trajectory is a sequence(cid:0)si
t) can be viewed as an independent sample from the distribution
The key is to note that each (si
t . Now consider a PAC-learnable hypothesis class H  where H contains a set of functions map-
DE
ping the state space S to the ﬁnite action space A. If m = poly( 1
ǫ )  then for each time step
t  the apprentice can use a PAC learning algorithm for H to learn a hypothesis ˆht ∈ H such that 
t (cid:16)ˆht(s) 6= a(cid:17) ≤ ǫ∗
with probability at least 1 − 1
+ ǫ. And by the union

Hδ   we have Pr(s a)∼DE

1  . . .   si

Hδ   1

H   ai

1  ai

H DE
t

3

bound  this inequality holds for all t with probability at least 1− δ. If each ǫ∗
natural choice for the apprentice’s policy πA is to set πA
classiﬁers to imitate the behavior of the expert.

+ ǫ is small  then a
t = ˆht for all t. This policy uses the learned

H DE
t

In light of the preceding discussion  throughout the remainder of this paper we make the following
assumption about the apprentice’s policy.
Assumption 1. The apprentice policy πA is a deterministic policy
Pr(s a)∼DE

t (s) 6= a) ≤ ǫ for some ǫ > 0 and all time steps t.

satisﬁes

(πA

that

t

As we have shown  an apprentice policy satisfying Assumption 1 with small ǫ can be found with
high probability  provided that expert’s policy is well-approximated by a PAC-learnable hypothesis
class and that the apprentice is given enough trajectories from the expert. A reasonable intuition is
that the value of the policy πA in Assumption 1 is nearly as high as the value of the policy πE; the
remainder of this paper is devoted to conﬁrming this intuition.

5 Guarantee for Any Expert

If the error rate ǫ in Assumption 1 is small  then the apprentice’s policy πA closely imitates the
expert’s policy πE  and we might hope that this implies that V (πA) is not much less than V (πE).
This is indeed the case  as the next theorem shows.

Theorem 1. If Assumption 1 holds  then V (πA) ≥ V (πE) − 2√ǫH 2Rmax.

In a typical classiﬁcation problem  it is assumed that the training and test examples are drawn from
the same distribution. The main challenge in proving Theorem 1 is that this assumption does not hold
for the classiﬁcation problems to which we have reduced the apprenticeship learning problem. This
t) appearing in an expert trajectory is distributed
is because  although each state-action pair (si
according to DE
t   a state-action pair (st  at) visited by the apprentice’s policy may not follow this
distribution  since the behavior of the apprentice prior to time step t may not exactly match the
expert’s behavior. So our strategy for proving Theorem 1 will be to show that these differences do
not cause the value of the apprentice policy to degrade too much relative to the value of the expert’s
policy.

t  ai

Before proceeding  we will show that Assumption 1 implies a condition that is  for our purposes 
more convenient.
Lemma 1. Let ˆπ be a deterministic nonstationary policy. If Pr(s a)∼DE
all ǫ1 ∈ (0  1] we have Prs∼DE

(ˆπt(s) 6= a) ≤ ǫ  then for

ǫ1

t

t (cid:0)πE

t (s  ˆπt(s)) ≥ 1 − ǫ1(cid:1) ≥ 1 − ǫ

Proof. Fix any ǫ1 ∈ (0  1]  and suppose for contradiction that Prs∼DE
1 − ǫ

. Say that a state s is good if πE

t (s  ˆπt(s)) ≥ 1 − ǫ1  and that s is bad otherwise. Then

t (s  ˆπt(s)) ≥ 1 − ǫ1(cid:1) <

t (cid:0)πE

ǫ1

Pr(s a)∼DE

t

(ˆπt(s) = a) = Prs∼DE

t

(s is good) · Pr(s a)∼DE
(s is bad) · Pr(s a)∼DE
(s is good) · 1 + (1 − Prs∼DE

t

t

t

t

(ˆπt(s) = a | s is good)
(ˆπt(s) = a | s is bad)

(s is good)) · (1 − ǫ1)

+ Prs∼DE

t

≤ Prs∼DE
= 1 − ǫ1(1 − Prs∼DE
< 1 − ǫ

t

(s is good))

where the ﬁrst inequality holds because Pr(s a)∼DE
inequality holds because Prs∼DE
(s is good) < 1− ǫ
the assumption of the lemma.

t

t

ǫ1

(ˆπt(s) = a | s is bad) ≤ 1 − ǫ1  and the second
. This chain of inequalities clearly contradicts

The next two lemmas are the main tools used to prove Theorem 1. In the proofs of these lemmas  we
write sa to denote a trajectory  where sa = (¯s1  ¯a1  . . .   ¯sH   ¯aH ) ∈ (S ×A)H. Also  let dPπ denote
the probability measure induced on trajectories by following policy π  and let R(sa) =PH
t=1 R(¯st)

4

denote the sum of the rewards of the states in trajectory sa. Importantly  using these deﬁnitions we
have

V (π) =Zsa

R(sa)dPπ.

The next lemma proves that if a deterministic policy “almost” agrees with the expert’s policy πE in
every state and time step  then its value is not much worse the value of πE.
Lemma 2. Let ˆπ be a deterministic nonstationary policy. If for all states s and time steps t we have
t (s  ˆπt(s)) ≥ 1 − ǫ then V (ˆπ) ≥ V (πE) − ǫH 2Rmax.
πE
Proof. Say a trajectory sa is good if it is “consistent” with ˆπ — that is  ˆπ(¯st) = ¯at for all time steps
t — and that sa is bad otherwise. We have

V (πE) =Zsa

R(sa)dPπE

=Zsa good
≤Zsa good
≤Zsa good

R(sa)dPπE +Zsa bad

R(sa)dPπE

R(sa)dPπE + ǫH 2Rmax

R(sa)dPˆπ + ǫH 2Rmax

= V (ˆπ) + ǫH 2Rmax

where the ﬁrst inequality holds because  by the union bound  PπE assigns at most an ǫH fraction
of its measure to bad trajectories  and the maximum reward of a trajectory is HRmax. The second
inequality holds because good trajectories are assigned at least as much measure by Pˆπ as by PπE  
because ˆπ is deterministic.

The next lemma proves a slightly different statement than Lemma 2: If a policy exactly agrees with
the expert’s policy πE in “almost” every state and time step  then its value is not much worse the
value of πE.
Lemma 3. Let ˆπ be a nonstationary policy.
Prs∼DE

t we have

time steps

for all

t (cid:0)ˆπt(s ·) = πE

t (s ·)(cid:1) ≥ 1 − ǫ then V (ˆπ) ≥ V (πE) − ǫH 2Rmax.

Proof. Say a trajectory sa is good if πE
otherwise. We have

t (¯st ·) = ˆπt(¯st ·) for all time steps t  and that sa is bad

If

V (ˆπ) =Zsa

R(sa)dPˆπ

R(sa)dPˆπ

R(sa)dPˆπ

R(sa)dPˆπ +Zsa bad
R(sa)dPπE +Zsa bad

=Zsa good
=Zsa good
=Zsa
≥ V (πE) − ǫH 2Rmax +Zsa bad
≥ V (πE) − ǫH 2Rmax.

R(sa)dPπE −Zsa bad

R(sa)dPπE +Zsa bad

R(sa)dPˆπ

R(sa)dPˆπ

The ﬁrst inequality holds because  by the union bound  PπE assigns at most an ǫH fraction of
its measure to bad trajectories  and the maximum reward of a trajectory is HRmax. The second
inequality holds by our assumption that all rewards are nonnegative.

We are now ready to combine the previous lemmas and prove Theorem 1.

5

Proof of Theorem 1. Since the apprentice’s policy πA satisﬁes Assumption 1  by Lemma 1 we can
choose any ǫ1 ∈ (0  1] and have

Now construct a “dummy” policy ˆπ as follows: For all time steps t  let ˆπt(s ·) = πE
state s where πE

t (s)) = 1. By Lemma 2

t (s ·) for any

t (s  πA

and by Lemma 3

Combining these inequalities yields

.

ǫ1

Prs∼DE

t (s  πA

t (s)) ≥ 1 − ǫ1(cid:1) ≥ 1 − ǫ
t (s)) ≥ 1 − ǫ1. On all other states  let ˆπt(s  πA

t (cid:0)πE
V (πA) ≥ V (ˆπ) − ǫ1H 2Rmax
H 2Rmax.
V (ˆπ) ≥ V (πE) −
V (πA) ≥ V (πE) −(cid:18)ǫ1 +

ǫ1(cid:19) H 2Rmax.

ǫ
ǫ1

ǫ

Since ǫ1 was chosen arbitrarily  we set ǫ1 = √ǫ  which maximizes this lower bound.

6 Guarantee for Good Expert

Theorem 1 makes no assumptions about the value of the expert’s policy. However  in many cases it
may be reasonable to assume that the expert is following a near-optimal policy (indeed  if she is not 
then we should question the decision to select her as an expert). The next theorem shows that the
dependence of V (πA) on the classiﬁcation error ǫ is signiﬁcantly better when the expert is following
a near-optimal policy.

Theorem 2. If Assumption 1 holds  then V (πA) ≥ V (πE) −(cid:0)4ǫH 3Rmax + ∆πE(cid:1)  where ∆πE  
V (π∗) − V (πE) is the suboptimality of the expert’s policy πE.
Note that the bound in Theorem 2 varies with ǫ and not with √ǫ. We can interpret this bound as
follows: If our goal is to learn an apprentice policy whose value is within ∆πE of the expert policy’s
value  we can double our progress towards that goal by halving the classiﬁcation error rate. On the
other hand  Theorem 2 suggests that the error rate must be reduced by a factor of four.

To see why a near-optimal expert policy should yield a weaker dependence on ǫ  consider an expert
policy πE that is an optimal policy  but in every state s ∈ S selects one of two actions as
1 and
2 uniformly at random. A deterministic apprentice policy πA that closely imitates the expert will
as
2  but in either case the classiﬁcation error will not be less than
either set πA(s) = as
1
2 . However  since πE is optimal  both actions as
2 must be optimal actions for state s  and so
the apprentice policy πA will be optimal as well.

1 or πA(s) = as

1 and as

Our strategy for proving Theorem 2 is to replace Lemma 2 with a different result — namely  Lemma
6 below — that has a much weaker dependence on the classiﬁcation error ǫ when ∆πE is small.
To help us prove Lemma 6  we will ﬁrst need to deﬁne several useful policies. The next several
deﬁnitions will be with respect to an arbitrary nonstationary base policy πB; in the proof of Theorem
2  we will make a particular choice for the base policy.

Fix a deterministic nonstationary policy πB ǫ that satisﬁes

t (s  πB ǫ
πB

t

(s)) ≥ 1 − ǫ

for some ǫ ∈ (0  1] and all states s and time steps t. Such a policy always exists by letting ǫ = 1  but
if ǫ is close to zero  then πB ǫ is a deterministic policy that “almost” agrees with πB in every state
and time step. Of course  depending on the choice of πB  a policy πB ǫ may not exist for small ǫ 
but let us set aside that concern for the moment; in the proof of Theorem 2  the base policy πB will
be chosen so that ǫ can be as small as we like.
Having thus deﬁned πB ǫ  we deﬁne πB\ǫ as follows: For all states s ∈ S and time steps t  if
t (s  πB ǫ(s)) < 1  then let
πB

πB\ǫ
t

(s  a) =


0

if πB ǫ

t

(s) = a

πB

t (s  a)
(s) πB

t (s  a′)

Pa′6=πB ǫ

t

6

otherwise

t

for all actions a ∈ A  and otherwise let πB\ǫ
state s and time step t  the distribution πB\ǫ
probability assigned to action πB ǫ
where πB
the proof of Lemma 4  it is actually immaterial how the distribution πB\ǫ
cases; we choose the uniform distribution for deﬁniteness.

(s  a) = 1
|A| for all a ∈ A. In other words  in each
(s ·) is obtained by proportionally redistributing the
t (s ·) to all other actions. The case
(s) is treated specially  but as will be clear from
(s ·) is deﬁned in these

t (s ·) assigns all probability to action πB ǫ

(s) by the distribution πB

t

t

t

t

Let πB+ be a deterministic policy deﬁned by

πB+
t

(s) = arg max

a

t+1(s′)(cid:12)(cid:12)(cid:12)
EhV πB

t

s′ ∼ θ(s  a ·)i

.

or πi

t = πB+

t

t = πB ǫ

t

t = πB ǫ

t

(s) is the best action in state s at time t 

for all states s ∈ S and time steps t. In other words  πB+
assuming that the policy πB is followed thereafter.
The next deﬁnition requires the use of mixed policies. A mixed policy consists of a ﬁnite set of
deterministic nonstationary policies  along with a distribution over those policies; the mixed policy
is followed by drawing a single policy according to the distribution in the initial time step  and
following that policy exclusively thereafter. More formally  a mixed policy is deﬁned by a set of
i=1 for some ﬁnite N  where each component policy πi is a deterministic
ordered pairs {(πi  λ(i))}N
nonstationary policy PN
i=1 λ(i) = 1 and λ(i) ≥ 0 for all i ∈ [N ].

We deﬁne a mixed policy ˜πB ǫ + as follows: For each component policy πi and each time step t 
either πi
. There is one component policy for each possible choice; this yields
N = 2|H| component policies. And the probability λ(i) assigned to each component policy πi is
λ(i) = (1 − ǫ)k(i)ǫH−k(i)  where k(i) is the number of times steps t for which πi
Having established these deﬁnitions  we are now ready to prove several lemmas that will help us
prove Theorem 2.
Lemma 4. V (˜πB ǫ +) ≥ V (πB).
Proof. The proof will be by backwards induction on t. Clearly V ˜πB ǫ +
s  since the value function V π
for induction that V ˜πB ǫ +

H (s) for all states
H for any policy π depends only on the reward function R. Now suppose
(s) ≥ V πB
(s′)(cid:12)(cid:12)(cid:12)
(s) = R(s) + EhV ˜πB ǫ +
(s ·)  s′ ∼ θ(s  a′ ·)i
t+1(s′)(cid:12)(cid:12)(cid:12)
≥ R(s) + EhV πB
(s ·)  s′ ∼ θ(s  a′ ·)i
t+1(s′)(cid:12)(cid:12)(cid:12)
= R(s) + (1 − ǫ)EhV πB
t+1(s′)(cid:12)(cid:12)(cid:12)
(s)) · EhV πB
≥ R(s) + πB
t+1(s′)(cid:12)(cid:12)(cid:12)
+(cid:16)1 − πB
(s))(cid:17) · EhV πB
t+1(s′)(cid:12)(cid:12)(cid:12)
(s)) · EhV πB
≥ R(s) + πB
t+1(s′)(cid:12)(cid:12)(cid:12)
+(cid:16)1 − πB
(s))(cid:17) · EhV πB
= R(s) + EhV πB
t (s)  s′ ∼ θ(s  a′ ·)i
a′ ∼ πB

t+1(s′)(cid:12)(cid:12)(cid:12)
(s) ·)i + ǫEhV πB
(s) ·)i
(s) ·)i
s′ ∼ θ(s  πB+
(s) ·)i
(s ·)  s′ ∼ θ(s  a′ ·)i

s′ ∼ θ(s  πB ǫ
a′ ∼ πB\ǫ

t+1(s) for all states s. Then for all states s

s′ ∼ θ(s  πB ǫ

s′ ∼ θ(s  πB ǫ

s′ ∼ θ(s  πB+

a′ ∼ ˜πB ǫ +

a′ ∼ ˜πB ǫ +

(s) = V πB

t (s  πB ǫ

t (s  πB ǫ

t (s  πB ǫ

t (s  πB ǫ

V ˜πB ǫ +
t

t+1

t+1

t

t

t

t

t

t

H

t

t

t

t

= V πB

t

(s).

t+1(s′)(cid:12)(cid:12)(cid:12)

The ﬁrst equality holds for all policies π  and follows straightforwardly from the deﬁnition of V π
t .
The rest of the derivation uses  in order: the inductive hypothesis; the deﬁnition of ˜πB ǫ +; property
of πB ǫ and the fact that πB+
(s) is the
best action with respect to V πB
Lemma 5. V (˜πB ǫ +) ≤ (1 − ǫH)V (πB ǫ) + ǫHV (π∗).

(s) is the best action with respect to V πB
t+1; the deﬁnition of πB\ǫ; the deﬁnition of V πB

t+1; the fact that πB+

(s).

t

t

t

t

t

(s) ·)i

7

Proof. Since ˜πB ǫ + is a mixed policy  by the linearity of expectation we have

N

where each πi is a component policy of ˜πB ǫ + and λ(i) is its associated probability. Therefore

V (˜πB ǫ +) =

λ(i)V (πi)

Xi=1

V (˜πB ǫ +) =Xi

λ(i)V (πi)

≤ (1 − ǫ)H V (πB ǫ) + (1 − (1 − ǫ)H )V (π∗)
≤ (1 − ǫH)V (πB ǫ) + ǫHV (π∗).

Here we used the fact that probability (1 − ǫ)H ≥ 1 − ǫH is assigned to a component policy that is
identical to πB ǫ  and the value of any component policy is at most V (π∗).

Lemma 6. If ǫ < 1

H   then V (πB ǫ) ≥ V (πB) − ǫH

1−ǫH ∆πB .

Proof. Combining Lemmas 4 and 5 yields

And via algebraic manipulation we have

(1 − ǫH)V (πB ǫ) + ǫHV (π∗) ≥ V (πB).

(1 − ǫH)V (πB ǫ) + ǫHV (π∗) ≥ V (πB)

⇒ (1 − ǫH)V (πB ǫ) ≥ (1 − ǫH)V (πB) + ǫHV (πB) − ǫHV (π∗)
⇒ (1 − ǫH)V (πB ǫ) ≥ (1 − ǫH)V (πB) − ǫH∆πB
⇒ V (πB ǫ) ≥ V (πB) −

∆πB .

ǫH

1 − ǫH

In the last line  we were able to divide by (1 − ǫH) without changing the direction of the inequality
because of our assumption that ǫ < 1
H .

We are now ready to combine the previous lemmas and prove Theorem 2.

Proof of Theorem 2. Since the apprentice’s policy πA satisﬁes Assumption 1  by Lemma 1 we can
choose any ǫ1 ∈ (0  1

H ) and have

Prs∼DE

t (s  πA

t (cid:0)πE

t (s)) ≥ 1 − ǫ1(cid:1) ≥ 1 − ǫ

ǫ1

.

As in the proof of Theorem 1  let us construct a “dummy” policy ˆπ as follows: For all time steps
t  let ˆπt(s ·) = πE
t (s)) ≥ 1 − ǫ1. On all other states  let
ˆπt(s  πA

t (s ·) for any state s where πE

t (s)) = 1. By Lemma 3 we have

t (s  πA

V (ˆπ) ≥ V (πE) −

ǫ
ǫ1

H 2Rmax.

Substituting V (πE) = V (π∗) − ∆πE and V (ˆπ) = V (π∗) − ∆ˆπ and rearranging yields

∆ˆπ ≤ ∆πE +

ǫ
ǫ1

H 2Rmax.

(1)

(2)

Now observe that  if we set the base policy πB = ˆπ  then by deﬁnition πA is a valid choice for
πB ǫ1. And since ǫ1 < 1

H we have
V (πA) ≥ V (ˆπ) −

ǫ1H

∆ˆπ

ǫ1H

1 − ǫ1H
1 − ǫ1H (cid:18)∆πE +
H 2Rmax −

ǫ
ǫ1

≥ V (ˆπ) −
≥ V (πE) −

ǫ
ǫ1
ǫ1H

H 2Rmax(cid:19)
1 − ǫ1H (cid:18)∆πE +

ǫ
ǫ1

H 2Rmax(cid:19)

(3)

where we used Lemma 6  (2) and (1)  in that order. Letting ǫ1 = 1

2H proves the theorem.

8

References
[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In

Proceedings of the 21st International Conference on Machine Learning  2004.

[2] P Abbeel and A Y Ng. Exploration and apprenticeship learning in reinforcement learning. In

Proceedings of the 22nd International Conference on Machine Learning  2005.

[3] Nathan D. Ratliff  J. Andrew Bagnell  and Martin A. Zinkevich. Maximum margin planning.

In Proceedings of the 23rd International Conference on Machine Learning  2006.

[4] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In

Advances in Neural Information Processing Systems 20  2008.

[5] J. Zico Kolter  Pieter Abbeel  and Andrew Ng. Hierarchical apprenticeship learning with ap-
plication to quadruped locomotion. In Advances in Neural Information Processing Systems 20 
2008.

[6] Umar Syed and Robert E. Schapire. Apprenticeship learning using linear programming. In

Proceedings of the 25th International Conference on Machine Learning  2008.

[7] Brenna D. Argall  Sonia Chernova  Manuela Veloso  and Brett Browning. A survey of robot

learning from demonstration. Robotics and Autonomous Systems  57(5):469–483  2009.

[8] St´ephane Ross and J. Andrew Bagnell. Efﬁcient reduction for imitation learning. In AISTATS 

2010.

[9] Bianca Zadrozny  John Langford  and Naoki Abe.

Cost-sensitive learning by cost-
proportionate example weighting. In Proceedings of the Third IEEE International Conference
on Data Mining  2003.

[10] J. Andrew Bagnell  Sham Kakade  Andrew Y. Ng  and Jeff Schneider. Policy search by dy-

namic programming. In Advances in Neural Information Processing Systems 15  2003.

[11] John Langford and Bianca Zadrozny. Relating reinforcement learning performance to classiﬁ-
cation performance. In Proceedings of the 22nd International Conference on Machine Learn-
ing  2005.

[12] Doron Blatt and Alfred Hero. From weighted classiﬁcation to policy search. In Advances in

Neural Information Processing Systems 18  pages 139–146  2006.

[13] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learn-

ing. In Proceedings 19th International Conference on Machine Learning  2002.

[14] V. N. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of

events to their probabilities. Theory of Probability and Its Applications  16:264–280  1971.

9

,Qichao Que
Mikhail Belkin