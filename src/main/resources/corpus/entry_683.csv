2016,Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition,We develop a Bayesian model for decision-making under time pressure with endogenous information acquisition. In our model  the decision-maker decides when to observe (costly) information by sampling an underlying continuous-time stochastic process (time series) that conveys information about the potential occurrence/non-occurrence of an adverse event which will terminate the decision-making process. In her attempt to predict the occurrence of the adverse event  the decision-maker follows a policy that determines when to acquire information from the time series (continuation)  and when to stop acquiring information and make a final prediction (stopping). We show that the optimal policy has a "rendezvous" structure  i.e. a structure in which whenever a new information sample is gathered from the time series  the optimal "date" for acquiring the next sample becomes computable. The optimal interval between two information samples balances a trade-off between the decision maker’s "surprise"  i.e. the drift in her posterior belief after observing new information  and "suspense"  i.e. the probability that the adverse event occurs in the time interval between two information samples. Moreover  we characterize the continuation and stopping regions in the decision-maker’s state-space  and show that they depend not only on the decision-maker’s beliefs  but also on the "context"  i.e. the current realization of the time series.,Balancing Suspense and Surprise: Timely Decision
Making with Endogenous Information Acquisition

Ahmed M. Alaa

Electrical Engineering Department

University of California  Los Angeles

Mihaela van der Schaar

Electrical Engineering Department

University of California  Los Angeles

Abstract

We develop a Bayesian model for decision-making under time pressure with en-
dogenous information acquisition.
In our model  the decision-maker decides
when to observe (costly) information by sampling an underlying continuous-
time stochastic process (time series) that conveys information about the potential
occurrence/non-occurrence of an adverse event which will terminate the decision-
making process. In her attempt to predict the occurrence of the adverse event  the
decision-maker follows a policy that determines when to acquire information from
the time series (continuation)  and when to stop acquiring information and make
a ﬁnal prediction (stopping). We show that the optimal policy has a "rendezvous"
structure  i.e. a structure in which whenever a new information sample is gathered
from the time series  the optimal "date" for acquiring the next sample becomes
computable. The optimal interval between two information samples balances a
trade-off between the decision maker’s "surprise"  i.e.
the drift in her posterior
belief after observing new information  and "suspense"  i.e. the probability that
the adverse event occurs in the time interval between two information samples.
Moreover  we characterize the continuation and stopping regions in the decision-
maker’s state-space  and show that they depend not only on the decision-maker’s
beliefs  but also on the "context"  i.e. the current realization of the time series.

1

Introduction

The problem of timely risk assessment and decision-making based on a sequentially observed time
series is ubiquitous  with applications in ﬁnance  medicine  cognitive science and signal processing
[1-7]. A common setting that arises in all these domains is that a decision-maker  provided with
sequential observations of a time series  needs to decide whether or not an adverse event (e.g. ﬁnan-
cial crisis  clinical acuity for ward patients  etc) will take place in the future. The decision-maker’s
recognition of a forthcoming adverse event needs to be timely  for that a delayed decision may hin-
der effective intervention (e.g. delayed admission of clinically acute patients to intensive care units
can lead to mortality [5]). In the context of cognitive science  this decision-making task is known
as the two-alternative forced choice (2AFC) task [15]. Insightful structural solutions for the optimal
Bayesian 2AFC decision-making policies have been derived in [9-16]  most of which are inspired
by the classical work of Wald on sequential probability ratio tests (SPRT) [8].
In this paper  we present a Bayesian decision-making model in which a decision-maker adaptively
decides when to gather (costly) information from an underlying time series in order to accumulate
evidence on the occurrence/non-occurrence of an adverse event. The decision-maker operates under
time pressure: occurrence of the adverse event terminates the decision-making process. Our abstract
model is motivated and inspired by many practical decision-making tasks such as: constructing tem-
poral patterns for gathering sensory information in perceptual decision-making [1]  scheduling lab

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

tests for ward patients in order to predict clinical deterioration in a timely manner [3  5]  designing
breast cancer screening programs for early tumor detection [7]  etc.
We characterize the structure of the optimal decision-making policy that prescribes when should
the decision-maker acquire new information  and when should she stop acquiring information and
issue a ﬁnal prediction. We show that the decision-maker’s posterior belief process  based on which
policies are prescribed  is a supermartingale that reﬂects the decision-maker’s tendency to deny
the occurrence of an adverse event in the future as she observes the survival of the time series for
longer time periods. Moreover  the information acquisition policy has a "rendezvous" structure;
the optimal "date" for acquiring the next information sample can be computed given the current
sample. The optimal schedule for gathering information over time balances the information gain
(surprise) obtained from acquiring new samples  and the probability of survival for the underlying
stochastic process (suspense). Finally  we characterize the continuation and stopping regions in the
decision-maker’s state-space and show that  unlike previous models  they depend on the time series
"context" and not just the decision-maker’s beliefs.

Related Works Mathematical models and analyses for perceptual decision-making based on
sequential hypothesis testing have been developed in [9-17]. Most of these models use tools
from sequential analysis developed by Wald [8] and Shiryaev [21  22].
In [9 13 14]  optimal
decision-making policies for the 2AFC task were computed by modelling the decision-maker’s
sensory evidence using diffusion processes [20]. These models assume an inﬁnite time horizon for
the decision-making policy  and an exogenous supply of sensory information.
The assumption of an inﬁnite time horizon was relaxed in [10] and [15]  where decision-making is
assumed to be performed under the pressure of a stochastic deadline; however  these deadlines were
considered to be drawn from known distributions that are independent of the hypothesis and the
realized sensory evidence  and the assumption of an exogenous information supply was maintained.
In practical settings  the deadlines would naturally be dependent on the realized sensory information
(e.g. patients’ acuity events are correlated with their physiological information [5])  which induces
more complex dynamics in the decision-making process. Context-based decision-making models
were introduced in [17]  but assuming an exogenous information supply and an inﬁnite time horizon.
The notions of “suspense" and “surprise" in Bayesian decision-making have also been recently intro-
duced in the economics literature (see [18] and the references therein). These models use measures
for Bayesian surprise  originally introduced in the context of sensory neuroscience [19]  in order
to model the explicit preference of a decision-maker to non-instrumental information. The goal
there is to design information disclosure policies that are suspense-optimal or surprise-optimal. Un-
like our model  such models impose suspense (and/or surprise) as a (behavioral) preference of the
decision-maker  and hence they do not emerge endogenously by virtue of rational decision making.

2 Timely Decision Making with Endogenous Information Gathering

Time Series Model The decision-maker has access to a time-series X(t) modeled as a continuous-
time stochastic process that takes values in R  and is deﬁned over the time domain t ∈ R+  with an
underlying ﬁltered probability space (Ω  F  {Ft}t∈R+   P). The process X(t) is naturally adapted
to {Ft}t∈R+  and hence the ﬁltration Ft abstracts the information conveyed in the time series real-
ization up to time t. The decision-maker extracts information from X(t) to guide her actions over
time.
We assume that X(t) is a stationary Markov process1  with a stationary transition kernel
Pθ (X(t) ∈ A|Fs) = Pθ (X(t) ∈ A|X(s))   ∀A ⊂ R  ∀s < t ∈ R+  where θ is a realization
of a latent Bernoulli random variable Θ ∈ {0  1} (unobservable by the decision-maker)  with
P(Θ = 1) = p. The distributional properties of the paths of X(t) are determined by θ  since
the realization of θ decides which Markov kernel (Po or P1) generates X(t). If the realization θ is
equal to 1  then an adverse event occurs almost surely at a (ﬁnite) random time τ   the distribution of
which is dependent on the realization of the path (X(t))0≤t≤τ .

1Most of the insights distilled from our results would hold for more general dependency structures. However 
we keep this assumption to simplify the exposition and maintain the tractability and interpretability of the
results.

2

0.1

0.05

)
t
(

X

0

−0.05

−0.1

 
0

Pt = {0  0.1  0.15  0.325  0.4  0.45  0.475  0.5  0.65  0.7}

Information at t = 0.2:
1) σ(X(0)  X(0.1)  X(0.15))
2) S0.2: survival up to t = 0.2

Adverse event

 

Stopping time τ

Continuous-path X(t)
Partitioned path X(Pt)

0.1

0.2

0.3

0.4

0.5

Time t

0.6

0.7

0.8

0.9

1

Figure 1: An exemplary stopped sample path for X τ (t)|Θ = 1  with an exemplary partition Pt.

The decision-maker’s ultimate goal is to sequentially observe X(t)  and infer θ before the adverse
event happens; inference is obsolete if it is declared after τ . Since Θ is latent  the decision-maker is
unaware whether the adverse event will occur or not  i.e. whether her access to X(t) is temporary
(τ < ∞ for θ = 1) or permanent (τ = ∞ for θ = 0). In order to model the occurrence of the
adverse event; we deﬁne τ as an F-stopping time for the process X(t)  for which we assume the
following:

• The stopping time τ |Θ = 1 is ﬁnite almost surely  whereas τ |Θ = 0 is inﬁnite almost

surely  i.e. P (τ < ∞ |Θ = 1 ) = 1  and P (τ = ∞ |Θ = 0 ) = 1.

• The stopping time τ |Θ = 1 is accessible2  with a Markovian dependency on history  i.e.
P ( τ < t| Fs) = P ( τ < t| X(s))   ∀s < t  where P ( τ < t| X(s)) is an injective map from
R to [0  1] and P ( τ < t| X(s)) is non-decreasing in X(s).

Thus  unlike the stochastic deadline models in [10] and [15]  the decision deadline in our model (i.e.
occurrence of the adverse event) is context-dependent as it depends on the time series realization
(i.e. P ( τ < t| X(s)) is not independent of X(t) as in [15]). We use the notation X τ (t) = X(t ∧ τ ) 
where t ∧ τ = min{t  τ } to denote the stopped process to which the decision-maker has ac-
cess. Throughout the paper  the measures Po and P1 assign probability measures to the paths
X τ (t)|Θ = 0 and X τ (t)|Θ = 1 respectively  and we assume that Po << P1

3.

i=0

Information The decision-maker can only observe a set of (costly) samples of X τ (t) rather
than the full continuous path. The samples observed by the decision-maker are captured by
partitioning X(t) over speciﬁc time intervals: we deﬁne Pt = {to  t1  . . .  tN (Pt)−1}  with
0 ≤ to < t1 < . . . < tN (Pt)−1 ≤ t  as a size-N (Pt) partition of X τ (t) over the interval [0  t] 
where N (Pt) is the total number of samples in the partition Pt. The decision-maker observes the
values that X τ (t) takes at the time instances in Pt; thus the sequence of observations is given by the
X(ti)δti   where δti is the Dirac measure. The space of all partitions
over the interval [0  t] is denoted by Pt = [0  t]N. We denote the probability measures for partitioned
paths generated under Θ = 0 and 1 with a partition Pt as ˜Po(Pt) and ˜P1(Pt) respectively.
Since the decision-maker observes X τ (t) through the partition Pt  her information at time t is
conveyed in the σ-algebra σ(X τ (Pt)) ⊂ Ft. The stopping event is observable by the decision-

process X(Pt) = PN (Pt)−1

maker even if τ /∈ Pτ . We denote the σ-algebra generated by the stopping event as St = σ(cid:0)1{t≥τ }(cid:1).

Thus  the information that the decision-maker has at time t is expressed by the ﬁltration ˜Ft =
σ(X τ (Pt)) ∨ St. Hence  any decision-making policy needs to be ˜Ft-measurable.
Figure 1 depicts a Brownian path (a sample path of a Wiener process  which satisﬁes all the
assumptions of our model)4  with an exemplary partition Pt over the time interval [0  1]. The
decision-maker observes the samples in X(Pt) sequentially  and reasons about the realization of
the latent variable Θ based on these samples and the process survival  i.e. at t = 0.2  the decision-
maker’s information resides in the σ-algebra σ(X(0)  X(0.1)  X(0.15)) generated by the samples

2Our analyses hold if the stopping time is totally inaccessible.
3The absolute continuity of Po with respect to P1 means that no sample path of X τ (t)|Θ = 0 should be

fully revealing of the realization of Θ.

4In Figure 1  the stopping event was simulated as a totally inaccessible ﬁrst jump of a Poisson process.

3

in P0.2 = {0  0.1  0.15}  and the σ-algebra generated by the process’ survival S0.2 = σ(1{τ >0.2}).

Policies and Risks The decision-maker’s goal is to come up with a (timely) decision ˆθ ∈ {0  1} 
that reﬂects her prediction for whether the actual realization θ is 0 or 1  before the process X τ (t)
potentially stops at the unknown time τ . The decision-maker follows a policy: a (continuous-time)
mapping from the observations gathered up to every time instance t to two types of actions:

• A sensing action δt ∈ {0  1}: if δt = 1  then the decision-maker decides to observe a new

sample from the running process X τ (t) at time t.

• A continuation/stopping action ˆθt ∈ {∅  0  1}: if ˆθt ∈ {0  1}  then the decision-maker
decides to stop gathering samples from X τ (t)  and declares a ﬁnal decision (estimate) for
θ. Whenever ˆθt = ∅  the decision-maker continues observing X τ (t) and postpones her
declaration for the estimate of θ.

A policy π = (πt)t∈R+ is a ( ˜Ft-measurable) mapping rule that maps the information in ˜Ft to an
action tuple πt = (δt  ˆθt) at every time instance t. We assume that every single observation that the
decision-maker draws from X τ (t) entails a ﬁxed cost  hence the process (δt)t∈R+ has to be a point
process under any optimal policy5. We denote the space of all such policies by Π.
A policy π generates the following random quantities as a function of the paths X τ (t) on the proba-
bility space (Ω  F  {Ft}t∈R+   P):
1- A stopping time Tπ: The ﬁrst time at which the decision-maker declares its estimate for θ  i.e.
Tπ = inf{t ∈ R+ : ˆθt ∈ {0  1}}.
2- A decision (estimate of θ) ˆθπ: Given by ˆθπ = ˆθTπ∧τ .
3- A random partition P π
Tπ : A realization of the point process (δt)t∈R+  comprising a ﬁnite set of
strictly increasing F-stopping times at which the decision-maker decides to sample the path X τ (t).

A loss function is associated with every realization of the policy π  representing the overall cost
incurred when following that policy for a speciﬁc path X τ (t). The loss function is given by

|

{z

Type I error

Type II error

+ Cd Tπ

) 1{Tπ≤τ }+ Cr 1{Tπ >τ }

+ Co 1{ˆθπ=1 θ=0}

ℓ (π; Θ)   (C1 1{ˆθπ=0 θ=1}

Tπ∧τ )
Information
(1)
where C1 is the cost of type I error (failure to anticipate the adverse event)  Co is the cost of type II
error (falsely predicting that an adverse event will occur)  Cd is the cost of the delay in declaring the
estimate ˆθπ  Cr is the cost incurred when the adverse event occurs before an estimate ˆθπ is declared
(cost of missing the deadline)  and Cs is the cost of every observation sample (cost of information).
The risk of each policy π is deﬁned as its expected loss

| {z }Delay

}

|

+ CsN (P π

{z

}

Deadline missed

{z

}

|

|

{z

 

}

where the expectation is taken over the paths of X τ (t). In the next section  we characterize the
structure of the optimal policy π∗ = arg infπ∈ΠR(π).

R(π)   E [ℓ (π; Θ)]  

(2)

3 Structure of the Optimal Policy

Since the decision-maker’s posterior belief at time t  deﬁned as µt = P( Θ = 1| ˜Ft)  is an impor-
tant statistic for designing sequential policies [10  21-22]  we start our characterization for π∗ by
investigating the belief process (µt)t∈R+.

3.1 The Posterior Belief Process

Recall that the decision-maker distills information from two types of observations: the realization
of the partitioned time series X τ (Pt) (i.e. the information in σ(X τ (Pt)))  and 2) the survival of the

5Note that the cost of observing any local continuous path is inﬁnite  hence any optimal policy must have

(δt)t∈R+ being a point process to keep the number of observed samples ﬁnite.

4

t

µ

s
s
e
c
o
r
p

f
e
i
l
e
b

r
o
i
r
e
t
s
o
P

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

 
0

Suspense phase (risk bearing)

0.65

0.6

0.55

0.5

500

600

700

800

Surprise phase (risk assessment)

Information gain
It1(t2 − t1) = µt2

− µt1

Policy π1 with partition P π1
Policy π2  with P π1 ⊂ P π2
Wait-and-watch policy

 

Stopping time τ

200

400

600

800

Time t

1000

1200

1400

1600

Figure 2: Depiction for exemplary belief paths of different policies under Θ = 1.

process up to time t (i.e. the information in St). In the following Theorem  we study the evolution
of the decision-maker’s beliefs as she integrates these pieces of information over time6.
Theorem 1 (Information and beliefs). Every posterior belief trajectory (µt)t∈R+ associated with
a policy π ∈ Π that creates a partition P π

t ∈ Pt of X τ (t) is a càdlàg path given by

µt = 1{t≥τ } + 1{0≤t<τ } · 1 +

1 − p

p

d˜Po(P π
t )
d˜P1(P π

t )!−1

 

·

where d˜Po(P π
t )
d˜P1(P π
t )
and is given by the following elementary predictable process

is the Radon–Nikodym derivative7 of the measure ˜Po(P π

t ) with respect to ˜P1(P π

t ) 

P(τ > t|σ(X(P π

t )  Θ = 1)

1{P π

t (k)≤t≤P π

t (k+1)} 

Survival probability

{z

}

1

d˜Po(P π
t )
d˜P1(P π
t )

=

N (P π

t )−1

Xk=1

P(X(P π
P(X(P π

t )|Θ = 1)
t )|Θ = 0)

Likelihood ratio

|

{z

|

}

for t ≥ P π
N (P π

t (1)  and p P(τ > t|Θ = 1) for t < P π

Tπ∧τ ) + 1{τ <∞} jumps at the time indexes in P π

t∧τ ∪ {τ }.

t (k). Moreover  the path (µt)t∈R+ has exactly
(cid:3)

Theorem 1 says that every belief path is right-continuous with left limits  and has jumps at the time
t   whereas between each two jumps  the paths (µt)t∈[t1 t2)  t1  t2 ∈ P π
indexes in the partition P π
t
are predictable (i.e.
they are known ahead of time once we know the magnitudes of the jumps
preceding them). This means that the decision-maker obtains "active" information by probing
the time series to observe new samples (i.e. the information in σ(X τ (Pt)))  inducing jumps that
revive her beliefs  whereas the progression of time without witnessing a stopping event offers the
decision-maker "passive information" that is distilled just from the costless observation of process
survival information. Both sources of information manifest themselves in terms of the likelihood
ratio  and the survival probability in the expression of d˜Po(P π
t )
d˜P1(P π
t )

above.

In Figure 2  we plot the càdlàg belief paths for policies π1 and π2  where P π1 ⊂ P π2 (i.e. policy
π1 observe a subset of the samples observed by π2). We also plot the (predictable) belief path of
a wait-and-watch policy that observes no samples. We can see that π2  which has more jumps of
"active information"  copes faster with the truthful belief over time. Between each two jumps  the
belief process exhibits a non-increasing predictable path until fed with a new piece of information.
The wait-and-watch policy has its belief drifting away from the prior p = 0.5 towards the wrong
belief µt = 0 since it only distills information from the process survival  which favors the hypothesis
Θ = 0. This discussion motivates the introduction of the following key quantities.

Information gain (surprise) It(∆t): The amount of drift in the decision-maker’s belief at time
t + ∆t with respect to her belief at time t  given the information available up to time t  i.e.
It(∆t) = (µt+∆t − µt) | ˜Ft.

6All proofs are provided in the supplementary material
7Since we impose the condition Po << P1 and ﬁx a partition Pt  then the Radon–Nikodym derivative

exists.

5

Posterior survival function (suspense) St(∆t): The probability that a process generated
with Θ = 1 survives up to time t + ∆t given the information observed up to time t 
i.e.
St(∆t) = P(τ > t + ∆t| ˜Ft  Θ = 1). The function St(∆t) is a non-increasing function in ∆t  i.e.
∂St(∆t)

∂∆t ≤ 0.

That is  the information gain is the amount of “surprise" that the decision-maker experiences in
response to a new information sample expressed in terms of the change in here belief  i.e. the jumps
in µt  whereas the survival probability (suspense) is her assessment for the risk of having the adverse
event taking places in the next ∆t time interval. As we will see in the next subsection  the optimal
policy would balance the two quantities when scheduling the times to sense X τ (t).
We conclude our analysis for the process µt by noting that lack of information samples creates bias
towards the belief that Θ = 0 (e.g. see the belief path of the wait-and-watch policy in Figure 2). We
formally express this behavior in the following Corollary.
Corollary 1 (Leaning towards denial). For every policy π ∈ Π  the posterior belief process µt is
a supermartingale with respect to ˜Ft  where

E[µt+∆t| ˜Ft] = µt − µ2

t St(∆t)(1 − St(∆t)) ≤ µt  ∀∆t ∈ R+.

(cid:3)

Thus  unlike classical Bayesian learning models with a belief martingale [18  21-23]  the belief
process in our model is a supermartingale that leans toward decreasing over time. The reason for
this is that in our model  time conveys information. That is  unlike [10] and [15] where the decision
deadline is hypothesis-independent and is almost surely occurring in ﬁnite time for any path  in our
model the occurrence of the adverse event is itself a hypothesis  hence observing the survival of the
process is informative and contributes to the evolution of the belief. The informativeness of both the
acquired information samples and process survival can be disentangled using Doob decomposition 
by writing µt as µt = ˜µt + A(µt  St(∆t))  where ˜µt is a martingale  capturing the information gain
from the acquired samples  and A(µt  St(∆t)) is a predictable compensator process [23]  capturing
information extracted from the process survival.

3.2 The Optimal Policy

The optimal policy π∗ minimizes the expected risk as deﬁned in (1) and (2) by generating the tuple
of random processes (Tπ  ˆθπ  P π
t ) in response to the paths of X τ (t) on (Ω  F  {Ft}t∈R+  P) in a
way that "shapes" a belief process µt that maximizes informativeness  maintains timeliness and
controls cost. In the following  we introduce the notion of a "rendezvous policy"  then in Theorem
2  we show that the optimal policy π∗ complies with this deﬁnition.

Rendezvous policies We say that a policy π is a rendezvous policy  if the random partition P π
Tπ
t )t∈[0 Tπ]  is a point process with predictable
constructed by the sequence of sensing actions (δπ
jumps  where for every two consecutive jumps at times t and t
Tπ   we
have that t

′ is ˜Ft-measurable.

> t and t  t

′  with t

′

′

∈ P π

t )t∈[0 Tπ]  such that
That is  a rendezvous policy is a policy that constructs a sensing schedule (δπ
′ at which the decision-maker acquires information is actually computable using the
every time t
information available up to time t  the previous time instance at which information was gathered.
Hence  the decision-maker can decide the next "date" in which she will gather information directly
after she senses a new information sample. This structure is a natural consequence of the informa-
tion structure in Theorem 1  since the belief paths between every two jumps are predictable  then
they convey no "actionable" information  i.e. if the decision-maker was to respond to a predictable
belief path  say by sensing or making a stopping decision  then she should have taken that decision
right before the predictable path starts  which leads her to better off by saving the delay cost Cd.
We denote the space of all rendezvous policies by Πr. In the following Theorem  we establish that
the rendezvous structure is optimal.

Theorem 2 (Rendezvous). The optimal policy π∗ is a rendezvous policy (π∗ ∈ Πr).

(cid:3)

6

A direct implication of Theorem 2 is that the time variable can now be viewed as a state
variable  whereas the problem is virtually solved in "discrete-time" since the decision-maker
effectively jumps from one time instance to another in a discrete manner. Hence  we alter the
deﬁnition of the action δt from an indicator variable that indicates sensing the time series at time t 
to a "rendezvous action" that takes real values  and speciﬁes the time after which the decision-maker
would sense a new sample  i.e. if δt = ∆t  then the decision-maker gathers the new sample at t+∆t.
This transformation restricts our policy design problem to the space of rendezvous policies Πr 
which we know from Theorem 2 that it contains the optimal policy (i.e. π∗ = arg infπ∈Πr R(π)).
Having established the result in Theorem 2  in the following Theorem  we characterize the optimal
policy π∗ in terms of the random process (Tπ∗  ˆθπ∗  P π∗
) using discrete-time Bellman optimality
conditions [24].

t

Theorem 3 (The optimal policy). The optimal policy π∗ is a sequence of actions (ˆθπ∗
t
resulting in a random process (ˆθπ∗  Tπ∗  P π∗

Tπ∗ ) with the following properties:

  δπ∗

t )t∈R+  

(Continuation and stopping)

1. The process (t  µt  ¯X(P π∗

t

(ˆθπ∗  Tπ∗   P π∗
¯X(P π∗

Tπ∗ )  where ¯X(P π∗
) = X(t∗)  t∗ = max P π∗

t

t

))t∈R+ is a Markov sufﬁcient statistic for the distribution of
  i.e.

) is the most recent sample in the partition P π∗
.

t

t

2. The policy π∗ recommends continuation  i.e. ˆθπ∗

C(t  ¯X(P π∗
the following properties: C(t

))  where C(t  ¯X(P π∗

t

t

′

t = ∅  as long as the belief µt ∈
))  is a time and context-dependent continuation set with
> X.

> t  and C(t  X

) ⊂ C(t  X)  ∀X

  X) ⊂ C(t  X)  ∀t

′

′

′

(Rendezvous and decisions)

1. Whenever µt ∈ C(t  ¯X(P π∗

t

))  and t ∈ P π∗

Tπ∗   then the rendezvous δπ∗

t

is set as follows

where f (E[It(δ)]  St(δ)) is decreasing in E[It(δ)] and St(δ).

δπ∗
t = arg infδ∈R+f (E[It(δ)]  St(δ)) 

2. Whenever µt /∈ C(t  ¯X(P π∗

))  then a decision ˆθπ∗
on a belief threshold as follows: ˆθπ∗ = 1nµt≥
Tπ∗ = inf{t ∈ R+ : µt /∈ C(t  ¯X(P π∗

))}.

t

t

t = ˆθπ∗ ∈ {0  1} is issued  and is based
Co+C1 o. The stopping time is given by

C1

(cid:3)

t

Theorem 3 establishes the structure of the optimal policy and its prescribed actions in the decision-
maker’s state-space. The ﬁrst part of the Theorem says that in order to generate the random
tuple (Tπ∗  ˆθπ∗  P π∗
) optimally  we only need to keep track of the realization of the process
(t  µt  ¯X(Pt))t∈R+ in every time instance. That is  an optimal policy maps the current belief  the
current time  and the most recently observed realization of the time series to an action tuple (ˆθπ
t ) 
t   δπ
i.e. a decision on whether to stop and declare an estimate for θ or sense a new sample. Hence  the
process (t  µt  ¯X(Pt))t∈R+ represents the "state" of the decision-maker  and the decision-maker’s
actions can partially inﬂuence the state through the belief process  i.e. a decision on when to ac-
quire the next sample affects the distributional properties of the posterior belief. The remaining state
variables t and X(t) are beyond the decision-maker’s control.
We note that unlike the previous models in [9-16]  with the exception of [17]  a policy in our model
is context-dependent. That is  since the state is (t  µt  ¯X(P π
t )) and not just the time-belief tuple
(t  µt)  a policy π can recommend different actions for the same belief and at the same time but for
a different context. This is because  while µt captures what the decision-maker learned from the
history  ¯X(P π
t ) captures her foresightedness into the future  i.e. it can be that the belief µt is not
decisive (e.g. µt ≈ p)  but the context is "risky" (i.e. ¯X(P π
t ) is large)  which means that a potential
forthcoming adverse event is likely to happen in the near future  hence the decision-maker would be
more eager to make a stopping decision and declare an estimate ˆθπ. This is manifested through the
dependence of the continuation set C(t  ¯X(P π
t )) on both time and context; the continuation set is
monotonically decreasing in time due to the deadline pressure  and is also monotonically decreasing
in ¯X(P π

t ) due to the dependence of the deadline on the time series realization.

7

µt

Sample path 1

Sample path 2

¯µ

X(t)

Policy π:
Continue sampling X τ (t)

¯t

t

ˆθπ = 1

Policy π:
Stop and declare ˆθπ

Figure 3: Context-dependence of the policy π.

The context dependence of the optimal policy is pictorially depicted in Figure 3 where we show
two exemplary trajectories for the decision-maker’s state  and the actions recommended by a policy
π for the same time and belief  but a different context  i.e. a stopping action recommended when
X(t) is large since it corresponds to a low survival probability  whereas for the same belief and
time  a continuation action can be recommended if X(t) is low since it is safer to keep observing
the process for that the survival probability is high. Such a prescription speciﬁes optimal decision-
making in context-driven settings such as clinical decision-making in critical care environment [3-5] 
where a combination of a patient’s length of hospital stay (i.e. t)  clinical risk score (i.e. µt) and
current physiological test measurements (i.e. ¯X(P π
t )) determine the decision on whether or not a
patient should be admitted to an intensive care unit.
The second part of Theorem 3 says that whenever the optimal policy decides to stop gathering
information and issue a conclusive decision  it imposes a threshold on the posterior belief  based on
which it issues the estimate ˆθπ∗; the threshold is
  and hence weights the estimates by their
respective risks. When the policy favors continuation  it issues a rendezvous action  i.e. the next time
instance at which information will be gathered. This rendezvous balances surprise and suspense:
the decision-maker prefers maximizing surprise in order to draw the maximum informativeness
from the costly sample it will acquire; this is captured in terms of the expected information gain
E[It(δ)]. Maximizing surprise may increase suspense  i.e. the probability of process termination 
which is controlled by the survival function St(δ)  and hence it can be that harvesting the maximum
informativeness entails a survival risk when Cr is high. Therefore  the optimal policy selects a
rendezvous δπ∗
that optimizes a combination of the survival risk survival  captured by the cost Cr
and the survival function St(∆t)  and the value of information  captured by the costs Co  C1 and the
expected information gain E[It(δ)].

Co+C1

C1

t

4 Conclusions

We developed a model for decision-making with endogenous information acquisition under time
pressure  where a decision-maker needs to issue a conclusive decision before an adverse event (po-
tentially) takes place. We have shown that the optimal policy has a "rendezvous" structure  i.e. the
optimal policy sets a "date" for gathering a new sample whenever the current information sample is
observed. The optimal policy selects the time between two information samples such that it balances
the information gain (surprise) with the survival probability (suspense). Moreover  we characterized
the optimal policy’s continuation and stopping conditions  and showed that they depend on the con-
text and not just on beliefs. Our model can help understanding the nature of optimal decision-making
in settings where timely risk assessment and information gathering is essential.

5 Acknowledgments

This work was supported by the ONR and the NSF (Grant number: ECCS 1462245).

8

References

[1] Balci  F.  Freestone  D.  Simen  P.  de Souza  L.  Cohen  J. D.  & Holmes  P. (2011) Optimal temporal risk
assessment  Frontiers in Integrative Neuroscience  5(56)  1-15.

[2] Banerjee  T. & Veeravalli  V. V. (2012) Data-efﬁcient quickest change detection with on–off observation
control  Sequential Analysis  31(1)  40-77.

[3] Wiens  J.  Horvitz  E.  & Guttag  J. V. (2012) Patient risk stratiﬁcation for hospital-associated c. diff as a
time-series classiﬁcation task  In Advances in Neural Information Processing Systems  pp. 467-475.

[4] Schulam  P.  & Saria  S. (2015) A Framework for Individualizing Predictions of Disease Trajectories by
Exploiting Multi-resolution Structure  In Advances in Neural Information Processing Systems  pp. 748-756.

[5] Chalﬁn  D. B.  Trzeciak  S.  Likourezos  A.  Baumann  B. M.  Dellinger  R. P.  & DELAY-ED study group.
(2007) Impact of delayed transfer of critically ill patients from the emergency department to the intensive care
unit  Critical care medicine  35(6)  pp. 1477-1483.

[6] Bortfeld  T.  Ramakrishnan  J.  Tsitsiklis  J. N.  & Unkelbach  J. (2015) Optimization of radiation therapy
fractionation schedules in the presence of tumor repopulation  INFORMS Journal on Computing  27(4)  pp.
788-803.

[7] Shapiro  S.  et al.  (1998) Breast cancer screening programmes in 22 countries: current policies  administra-
tion and guidelines  International journal of epidemiology  27(5)  pp. 735-742.

[8] Wald  A.  Sequential analysis  Courier Corporation  1973.

[9] Khalvati  K.  & Rao  R. P. (2015) A Bayesian Framework for Modeling Conﬁdence in Perceptual Decision
Making  In Advances in neural information processing systems  pp. 2404-2412.

[10] Dayanik  S.  & Angela  J. Y. (2013) Reward-Rate Maximization in Sequential Identiﬁcation under a
Stochastic Deadline  SIAM J. Control Optim.  51(4)  pp. 2922–2948.

[11] Zhang  S.  & Angela  J.Y. (2013) Forgetful Bayes and myopic planning: Human learning and decision-
making in a bandit setting  In Advances in neural information processing systems  pp. 2607-2615.

[12] Shenoy  P.  & Angela  J.Y. (2012) Strategic impatience in Go/NoGo versus forced-choice decision-making 
In Advances in neural information processing systems  pp. 2123-2131.

[13] Drugowitsch  J.  Moreno-Bote  R.  & Pouget  A. (2014) Optimal decision-making with time-varying
evidence reliability  In Advances in neural information processing systems  pp. 748-756.

[14] Yu  A. J.  Dayan  P.  & Cohen  J. D. (2009) Dynamics of attentional selection under conﬂict: toward a
rational Bayesian account  Journal of Experimental Psychology: Human Perception and Performance  35(3) 
700.

[15] Frazier  P. & Angela  J. Y. (2007) Sequential hypothesis testing under stochastic deadlines  In Advances in
Neural Information Processing Systems  pp. 465-472.

[16] Drugowitsch  J.  Moreno-Bote  R.  Churchland  A. K.  Shadlen  M. N.  & Pouget  A. (2012) The cost of
accumulating evidence in perceptual decision making  The Journal of Neuroscience  32(11)  3612-3628.

[17] Shvartsman  M.  Srivastava  V.  & Cohen J. D. (2015) A Theory of Decision Making Under Dynamic
Context  In Advances in Neural Information Processing Systems  pp. 2476-2484. 2015.

[18] Ely  J.  Frankel  A.  & Kamenica  E. (2015) Suspense and surprise  Journal of Political Economy  123(1) 
pp. 215-260.

[19] Itti  L.  & Baldi  P. (2005) Bayesian Surprise Attracts Human Attention  In Advances in Neural Information
Processing Systems  pp. 547-554.

[20] Bogacz  R.  Brown  E.  Moehlis  J.  Holmes  P. J.  & Cohen J. D. (2006) The physics of optimal decision
making: A formal analysis of models of performance in two-alternative forced-choice tasks  Psychological
Review  113(4)  pp. 700–765.

[21] Peskir  G.  & Shiryaev  A. (2006) Optimal stopping and free-boundary problems  Birkhäuser Basel.

[22] Shiryaev  A. N. (2007) Optimal stopping rules (Vol. 8). Springer Science & Business Media.

[23] Shreve  Steven E. (2004) Stochastic calculus for ﬁnance II: Continuous-time models (Vol. 11)  Springer
Science & Business Media  2004.

[24] Bertsekas  D. P.  & Shreve  S. E. Stochastic optimal control: The discrete time case (Vol. 23)  New York:
Academic Press  1978.

9

,Emile Richard
Georges Goetz
E.J. Chichilnisky
Ahmed Alaa
Mihaela van der Schaar