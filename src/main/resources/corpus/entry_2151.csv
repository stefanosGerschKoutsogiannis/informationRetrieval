2017,Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation,Recent work has shown that state-of-the-art classifiers are quite brittle  in the sense that a small adversarial change of an originally with high confidence correctly classified  input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific \emph{lower bounds} on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.,Formal Guarantees on the Robustness of a
Classiﬁer against Adversarial Manipulation

Matthias Hein and Maksym Andriushchenko
Department of Mathematics and Computer Science

Saarland University  Saarbrücken Informatics Campus  Germany

Abstract

Recent work has shown that state-of-the-art classiﬁers are quite brittle 
in the sense that a small adversarial change of an originally with high
conﬁdence correctly classiﬁed input leads to a wrong classiﬁcation again
with high conﬁdence. This raises concerns that such classiﬁers are vulnerable
to attacks and calls into question their usage in safety-critical systems. We
show in this paper for the ﬁrst time formal guarantees on the robustness
of a classiﬁer by giving instance-speciﬁc lower bounds on the norm of the
input manipulation required to change the classiﬁer decision. Based on
this analysis we propose the Cross-Lipschitz regularization functional. We
show that using this form of regularization in kernel methods resp. neural
networks improves the robustness of the classiﬁer with no or small loss in
prediction performance.

Introduction

1
The problem of adversarial manipulation of classiﬁers has been addressed initially in the area
of spam email detection  see e.g. [5  16]. The goal of the spammer is to manipulate the spam
email (the input of the classiﬁer) in such a way that it is not detected by the classiﬁer. In deep
learning the problem was brought up in the seminal paper by [24]. They showed for state-of-
the-art deep neural networks  that one can manipulate an originally correctly classiﬁed input
image with a non-perceivable small transformation so that the classiﬁer now misclassiﬁes
this image with high conﬁdence  see [7] or Figure 3 for an illustration. This property calls
into question the usage of neural networks and other classiﬁers showing this behavior in
safety critical systems  as they are vulnerable to attacks. On the other hand this also shows
that the concepts learned by a classiﬁer are still quite far away from the visual perception
of humans. Subsequent research has found fast ways to generate adversarial samples with
high probability [7  12  19] and suggested to use them during training as a form of data
augmentation to gain more robustness. However  it turns out that the so-called adversarial
training does not settle the problem as one can yet again construct adversarial examples
for the ﬁnal classiﬁer. Interestingly  it has recently been shown that there exist universal
adversarial changes which when applied lead  for every image  to a wrong classiﬁcation with
high probability [17]. While one needs access to the neural network model for the generation
of adversarial changes  it has been shown that adversarial manipulations generalize across
neural networks [18  15  14]  which means that neural network classiﬁers can be attacked
even as a black-box method. The most extreme case has been shown recently [15]  where
they attack the commercial system Clarifai  which is a black-box system as neither the
underlying classiﬁer nor the training data are known. Nevertheless  they could successfully
generate adversarial images with an existing network and fool this commercial system. This
emphasizes that there are indeed severe security issues with modern neural networks. While
countermeasures have been proposed [8  7  26  18  12  2]  none of them provides a guarantee

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

of preventing this behavior [3]. One might think that generative adversarial neural networks
should be resistant to this problem  but it has recently been shown [13] that they can also
be attacked by adversarial manipulation of input images.
In this paper we show for the ﬁrst time instance-speciﬁc formal guarantees on the robustness
of a classiﬁer against adversarial manipulation. That means we provide lower bounds on the
norm of the change of the input required to alter the classiﬁer decision or said otherwise: we
provide a guarantee that the classiﬁer decision does not change in a certain ball around the
considered instance. We exemplify our technique for two widely used family of classiﬁers:
kernel methods and neural networks. Based on the analysis we propose a new regularization
functional  which we call Cross-Lipschitz Regularization. This regularization functional
can be used in kernel methods and neural networks. We show that using Cross-Lipschitz
regularization improves both the formal guarantees of the resulting classiﬁer (lower bounds)
as well as the change required for adversarial manipulation (upper bounds) while maintaining
similar prediction performance achievable with other forms of regularization. While there
exist fast ways to generate adversarial samples [7  12  19] without constraints  we provide
algorithms based on the ﬁrst order approximation of the classiﬁer which generate adversarial
samples satisfying box constraints in O(d log d)  where d is the input dimension.

2 Formal Robustness Guarantees for Classiﬁers
In the following we consider the multi-class setting for K classes and d features where one
has a classiﬁer f : Rd → RK and a point x is classiﬁed via c = arg max
fj(x). We call a
j=1 ... K
classiﬁer robust at x if small changes of the input do not alter the decision. Formally  the
problem can be described as follows [24]. Suppose that the classiﬁer outputs class c for
input x  that is fc(x) > fj(x) for j 6= c (we assume the decision is unique). The problem of
generating an input x + δ such that the classiﬁer decision changes  can be formulated as

fl(x + δ) ≥ fc(x + δ) and x + δ ∈ C 

(1)

min
δ∈Rd

kδkp  

s.th. max
l6=c

where C is a constraint set specifying certain requirements on the generated input x + δ  e.g. 
an image has to be in [0  1]d. Typically  the optimization problem (1) is non-convex and thus
intractable. The so generated points x + δ are called adversarial samples. Depending on
the p-norm the perturbations have diﬀerent characteristics: for p = ∞ the perturbations are
small and aﬀect all features  whereas for p = 1 one gets sparse solutions up to the extreme
case that only a single feature is changed. In [24] they used p = 2 which leads to more spread
but still localized perturbations. The striking result of [24  7] was that for most instances
in computer vision datasets  the change δ necessary to alter the decision is astonishingly
small and thus clearly the label should not change. However  we will see later that our new
regularizer leads to robust classiﬁers in the sense that the required adversarial change is so
large that now also the class label changes (we have found the correct decision boundary) 
see Fig 3. Already in [24] it is suggested to add the generated adversarial samples as a form
of data augmentation during the training of neural networks in order to achieve robustness.
This is denoted as adversarial training. Later on fast ways to approximately solve (1) were
proposed in order to speed up the adversarial training process [7  12  19]. However  in this
fj(x + δ) 6= c  one gets just
way  given that the approximation is successful  that is arg max
upper bounds on the perturbation necessary to change the classiﬁer decision. Also it was
noted early on  that the ﬁnal classiﬁer achieved by adversarial training is again vulnerable
to adversarial samples [7]. Robust optimization has been suggested as a measure against
adversarial manipulation [12  21] which eﬀectively boils down to adversarial training in
practice. It is thus fair to say that up to date no mechanism exists which prevents the
generation of adversarial samples nor can defend against it [3].
In this paper we focus instead on robustness guarantees  that is we show that the classiﬁer
decision does not change in a small ball around the instance. Thus our guarantees hold for
any method to generate adversarial samples or input transformations due to noise or sensor
failure etc. Such formal guarantees are in our point of view absolutely necessary when a
classiﬁer becomes part of a safety-critical technical system such as autonomous driving. In
the following we will ﬁrst show how one can achieve such a guarantee and then explicitly

j

2

derive bounds for kernel methods and neural networks. We think that such formal guarantees
on robustness should be investigated further and it should become standard to report them
for diﬀerent classiﬁers alongside the usual performance measures.

2.1 Formal Robustness Guarantee against Adversarial Manipulation
The following guarantee holds for any classiﬁer which is continuously diﬀerentiable with
respect to the input in each output component. It is instance-speciﬁc and depends to some
extent on the conﬁdence in the decision  at least if we measure conﬁdence by the relative
diﬀerence fc(x)− maxj6=c fj(x) as it is typical for the cross-entropy loss and other multi-class
losses. In the following we use the notation Bp(x  R) = {y ∈ Rd | kx − ykp ≤ R}.
Theorem 2.1. Let x ∈ Rd and f : Rd → RK be a multi-class classiﬁer with continuously
diﬀerentiable components and let c = arg max
fj(x) be the class which f predicts for x. Let
j=1 ... K
q ∈ R be deﬁned as 1

q = 1  then for all δ ∈ Rd with

p + 1

 := α 

min

j6=c

kδkp ≤ max
R>0

min

fc(x) − fj(x)
k∇fc(y) − ∇fj(y)kq

max

y∈Bp(x R)

  R

it holds c = arg max
j=1 ... K

fj(x + δ)  that is the classiﬁer decision does not change on Bp(x  α).

Note that the bound requires in the denominator a bound on the local Lipschitz constant
of all cross terms fc − fj  which we call local cross-Lipschitz constant in the following.
However  we do not require to have a global bound. The problem with a global bound is
that the ideal robust classiﬁer is basically piecewise constant on larger regions with sharp
transitions between the classes. However  the global Lipschitz constant would then just be
inﬂuenced by the sharp transition zones and would not yield a good bound  whereas the
local bound can adapt to regions where the classiﬁer is approximately constant and then
yields good guarantees. In [24  4] they suggest to study the global Lipschitz constant1 of
each fj  j = 1  . . .   K. A small global Lipschitz constant for all fj implies a good bound as
(2)
but the converse does not hold. As discussed below it turns out that our local estimates are
signiﬁcantly better than the suggested global estimates which implies also better robustness
guarantees. In turn we want to emphasize that our bound is tight  that is the bound is
attained  for linear classiﬁers fj(x) = hwj  xi  j = 1  . . .   K. It holds

k∇fj(y) − ∇fc(y)kq ≤ k∇fj(y)kq + k∇fc(y)kq  

kδkp = min
j6=c

hwc − wj  xi
kwc − wjkq

.

In Section 4 we reﬁne this result for the case when the input is constrained to [0  1]d. In
general  it is possible to integrate constraints on the input by simply doing the maximum
over the intersection of Bp(x  R) with the constraint set e.g. [0  1]d for gray-scale images.

2.2 Evaluation of the Bound for Kernel Methods
Next  we discuss how the bound can be evaluated for diﬀerent classiﬁer models. For simplicity
we restrict ourselves to the case p = 2 (which implies q = 2) and leave the other cases to
future work. We consider the class of kernel methods  that is the classiﬁer has the form

nX

fj(x) =

αjrk(xr  x) 

r=1

r=1 are the n training points  k : Rd ×Rd → R is a positive deﬁnite kernel function
where (xr)n
and α ∈ RK×n are the trained parameters e.g. of a SVM. The goal is to upper bound the
1The Lipschitz constant L wrt to p-norm of a piecewise continuously diﬀerentiable function is

given as L = supx∈Rd k∇f(x)kq. Then it holds  |f(x) − f(y)| ≤ Lkx − ykp.

3

term maxy∈B2(x R) k∇fj(y) − ∇fc(y)k2 for this classiﬁer model. A simple calculation shows

0 ≤ k∇fj(y) − ∇fc(y)k2

2 =

(αjr − αcr)(αjs − αcs)h∇yk(xr  y) ∇yk(xs  y)i

(3)

nX

r s=1

It has been reported that kernel methods with a Gaussian kernel are robust to noise. Thus
we specialize now to this class  that is k(x  y) = e−γkx−yk2

2. In this case

o

2

  R

h∇yk(xr  y) ∇yk(xs  y)i = 4γ2 hy − xr  y − xsi e−γkxr−yk2

2 e−γkxs−yk2
2 .

We derive the following bound

and S = k2x − xr − xsk2. Then

maxy∈B2(x R) k∇fj(y) − ∇fc(y)k2 ≤ 2γ

Proposition 2.1. Let βr = αjr − αcr  r = 1  . . .   n and deﬁne M = minnk2x−xr−xsk2
 nX
2−2M S+2M 2(cid:1)
2+2RS+2R2(cid:1)i
2+2RS+2R2(cid:1)
2−2M S+2M 2(cid:1)i! 1

h max{hx − xr  x − xsi + RS + R2  0}e
h max{hx − xr  x − xsi − M S + M 2  0}e

−γ(cid:0)kx−xrk2
−γ(cid:0)kx−xrk2
−γ(cid:0)kx−xrk2
−γ(cid:0)kx−xrk2

+ min{hx − xr  x − xsi + RS + R2  0}e

+ min{hx − xr  x − xsi − M S + M 2  0}e

2+kx−xsk2

2+kx−xsk2

+

βrβs

2+kx−xsk2

r s=1
βrβs≥0

nX

r s=1
βrβs<0

2+kx−xsk2

βrβs

2

While the bound leads to non-trivial estimates as seen in Section 5  the bound is not very
tight. The reason is that the sum is bounded elementwise  which is quite pessimistic. We
think that better bounds are possible but have to postpone this to future work.

2.3 Evaluation of the Bound for Neural Networks
We derive the bound for a neural network with one hidden layer. In principle  the technique
we apply below can be used for arbitrary layers but the computational complexity increases
rapidly. The problem is that in the directed network topology one has to consider almost
each path separately to derive the bound. Let U be the number of hidden units and w  u
are the weight matrices of the output resp.
input layer. We assume that the activation
function σ is continuously diﬀerentiable and assume that the derivative σ0 is monotonically
increasing. Our prototype activation function we have in mind and which we use later
on in the experiment is the diﬀerentiable approximation  σα(x) = 1
α log(1 + eαx) of the
ReLU activation function σReLU(x) = max{0  x}. Note that limα→∞ σα(x) = σReLU(x) and
σ0
α(x) =
1+e−αx . The output of the neural network can be written as

1

UX

(cid:16) dX

r=1

s=1

(cid:17)

fj(x) =

wjr σ

ursxs

 

j = 1  . . .   K 

UX

dX

where for simplicity we omit any bias terms  but it is straightforward to consider also models
with bias. A direct computation shows that

k∇fj(y) − ∇fc(y)k2

2 =

(wjr − wcr)(wjm − wcm)σ0(hur  yi)σ0(hum  yi)

urluml  (4)

r m=1

l=1

where ur ∈ Rd is the r-th row of the weight matrix u ∈ RU×d. The resulting bound is given
in the following proposition.

4

Proposition 2.2. Let σ be a continuously diﬀerentiable activation function with σ0 mono-

maxy∈B2(x R) k∇fj(y) − ∇fc(y)k2

tonically increasing. Deﬁne βrm = (wjr − wcr)(wjm − wcm)Pd
max{βrm  0}σ0(cid:0)hur  xi + R kurk2
+ min{βrm  0}σ0(cid:0)hur  xi − R kurk2

≤h UX

r m=1

l=1 urluml. Then

(cid:1)σ0(cid:0)hum  xi + R kumk2
(cid:1)σ0(cid:0)hum  xi − R kumk2

(cid:1)
(cid:1)i 1

2

As discussed above the global Lipschitz bounds of the individual classiﬁer outputs  see (2) 
lead to an upper bound of our desired local cross-Lipschitz constant. In the experiments
below our local bounds on the Lipschitz constant are up to 8 times smaller  than what one
would achieve via the global Lipschitz bounds of [24]. This shows that their global approach
is much too rough to get meaningful robustness guarantees.

3 The Cross-Lipschitz Regularization Functional
We have seen in Section 2 that if
max
j6=c

k∇fc(y) − ∇fj(y)kq  

(5)
is small and fc(x) − fj(x) is large  then we get good robustness guarantees. The latter
property is typically already optimized in a multi-class loss function. We consider for all
methods in this paper the cross-entropy loss so that the diﬀerences in the results only come
from the chosen function class (kernel methods versus neural networks) and the chosen
regularization functional. The cross-entropy loss L : {1  . . .   K} × RK → R is given as

y∈Bp(x R)

max

L(y  f(x)) = − log(cid:16)

PK

efy(x)
k=1 efk(x)

(cid:17) = log(cid:16)1 +

KX

k6=y

efk(x)−fy(x)(cid:17)

.

In the latter formulation it becomes apparent that the loss tries to make the diﬀerence
fy(x) − fk(x) as large as possible for all k = 1  . . .   K.
As our goal are good robustness guarantees it is natural to consider a proxy of the quantity
in (5) for regularization. We deﬁne the Cross-Lipschitz Regularization functional as

Ω(f) = 1
nK2

nX

KX

i=1

l m=1

k∇fl(xi) − ∇fm(xi)k2
2  

(6)

i=1 are the training points. The goal of this regularization functional is to
where the (xi)n
make the diﬀerences of the classiﬁer functions at the data points as constant as possible. In
total by minimizing

L(cid:0)yi  f(xi)(cid:1) + λΩ(f) 

nX

i=1

1
n

(7)

over some function class we thus try to maximize fc(xi) − fj(xi) and at the same time
keep k∇fl(xi) − ∇fm(xi)k2
2 small uniformly over all classes. This automatically enforces
robustness of the resulting classiﬁer. It is important to note that this regularization functional
is coherent with the loss as it shares the same degrees of freedom  that is adding the same
function g to all outputs: f0
j(x) = fj(x) + g(x) leaves loss and regularization functional
invariant. This is the main diﬀerence to [4]  where they enforce the global Lipschitz constant
to be smaller than one.

3.1 Cross-Lipschitz Regularization in Kernel Methods
In kernel methods one uses typically the regularization functional induced by the kernel which
i=1 αik(xi  x)  in the corresponding

is given as the squared norm of the function  f(x) =Pn

5

= Pn

reproducing kernel Hilbert space Hk  kfk2
i j=1 αiαjk(xi  xj). In particular  for
translation invariant kernels one can make directly a connection to penalization of derivatives
of the function f via the Fourier transform  see [20]. However  penalizing higher-order
derivatives is irrelevant for achieving robustness. Given the kernel expansion of f  one can
write the Cross-Lipschitz regularization function as

Hk

(αlr − αmr)(αls − αms)h∇yk(xr  xi) ∇yk(xs  xi)i

Ω(f) = 1
nK2

nX

KX

nX

i j=1

l m=1

r s=1

Ω is convex in α ∈ RK×n as k0(xr  xs) = h∇yk(xr  xi) ∇yk(xs  xi)i is a positive deﬁnite
kernel for any xi and with the convex cross-entropy loss the learning problem in (7) is convex.

3.2 Cross-Lipschitz Regularization in Neural Networks
The standard way to regularize neural networks is weight decay; that is  the squared
Euclidean norm of all weights is added to the objective. More recently dropout [22]  which
can be seen as a form of stochastic regularization  has been introduced. Dropout can
also be interpreted as a form of regularization of the weights [22  10].
It is interesting
to note that classical regularization functionals which penalize derivatives of the resulting
classiﬁer function are not typically used in deep learning  but see [6  11]. As noted above
we restrict ourselves to one hidden layer neural networks to simplify notation  that is 
j = 1  . . .   K. Then we can write the Cross-Lipschitz

fj(x) =PU
regularization as
Ω(f) = 2
nK2
which leads to an expression which can be fast evaluated using vectorization. Obviously  one
can also implement the Cross-Lipschitz Regularization also for all standard deep networks.

r=1 wjr σ(cid:0)Pd
(cid:16) KX
UX

(cid:1) 
wlrwls − KX

σ0(hur  xii)σ0(hus  xii)

(cid:17) nX

KX

dX

s=1 ursxs

l=1

m=1

wlr

wms

r s=1

l=1

i j=1

urlusl

l=1

4 Box Constrained Adversarial Sample Generation
The main emphasis of this paper are robustness guarantees without resorting to particular
ways how to generate adversarial samples. On the other hand while Theorem 2.1 gives
lower bounds on the required input transformation  eﬃcient ways to approximately solve
the adversarial sample generation in (1) are helpful to get upper bounds on the required
change. Upper bounds allow us to check how tight our derived lower bounds are. As all of
our experiments will be concerned with images  it is reasonable that our adversarial samples
are also images. However  up to our knowledge  the current main techniques to generate
adversarial samples [7  12  19] integrate box constraints by clipping the results to [0  1]d. We
provide in the following fast algorithms to generate adversarial samples which lie in [0  1]d.
The strategy is similar to [12]  where they use a linear approximation of the classiﬁer to
derive adversarial samples with respect to diﬀerent norms. Formally 
j = 1  . . .   K.

fj(x + δ) ≈ fj(x) + h∇fj(x)  δi  

Assuming that the linear approximation holds  the optimization problem (1) integrating box
constraints for changing class c into j becomes

minδ∈Rd kδkp
sbj. to: fj(x) − fc(x) ≥ h∇fc(x) − ∇fj(x)  δi

0 ≤ xj + δj ≤ 1

(8)

In order to get the minimal adversarial sample we have to solve this for all j 6= c and take
the one with minimal kδkp. This yields the minimal adversarial change for linear classiifers.
Note that (8) is a convex optimization problem  which can be reduced to a one-parameter
problem in the dual. This allows to derive the following result (proofs and algorithms are in
the supplement).
Proposition 4.1. Let p ∈ {1  2 ∞}  then (8) can be solved in O(d log d) time.
For nonlinear classiﬁers a change of the decision is not guaranteed and thus we use later on
a binary search with a variable c instead of fc(x) − fj(x).

6

5 Experiments
The goal of the experiments is the evaluation of the robustness of the resulting classiﬁers
and not necessarily state-of-the-art results in terms of test error. In all cases we compute the
robustness guarantees from Theorem 2.1 (lower bound on the norm of the minimal change
required to change the classiﬁer decision)  where we optimize over R using binary search 
and adversarial samples with the algorithm for the 2-norm from Section 4 (upper bound
on the norm of the minimal change required to change the classiﬁer decision)  where we do
a binary search in the classiﬁer output diﬀerence in order to ﬁnd a point on the decision
boundary. Additional experiments can be found in the supplementary material.

ρ2
KNN40

Kernel methods: We optimize the cross-entropy loss once with the standard regularization
(Kernel-LogReg) and with Cross-Lipschitz regularization (Kernel-CL). Both are convex
optimization problems and we use L-BFGS to solve them. We use the Gaussian kernel
k(x  y) = e−γkx−yk2 where γ = α
and ρKNN40 is the mean of the 40 nearest neighbor
distances on the training set and α ∈ {0.5  1  2  4}. We show the results for MNIST (60000
training and 10000 test samples). However  we have checked that parameter selection
using a subset of 50000 images from the training set and evaluating on the rest yields
indeed the parameters which give the best test errors when trained on the full set. The
regularization parameter is chosen in λ ∈ {10−k|k ∈ {5  6  7  8}} for Kernel-SVM and
λ ∈ {10−k | k ∈ {0  1  2  3}} for our Kernel-CL. The results of the optimal parameters are
given in the following table and the performance of all parameters is shown in Figure 1. Note
that due to the high computational complexity we could evaluate the robustness guarantees
only for the optimal parameters.

avg. k·k2
adv.
samples

test
error
2.23% 2.39

No Reg.
(λ = 0)
K-SVM 1.48% 1.91
K-CL
1.44% 3.12

avg.k·k2
rob.
guar.
0.037

0.058
0.045

Figure 1: Kernel Methods: Cross-Lipschitz regularization achieves both better test error and robustness against
adversarial samples (upper bounds  larger is better) compared to the standard regularization. The robustness
guarantee is weaker than for neural networks but this is most likely due to the relatively loose bound.

Neural Networks: Before we demonstrate how upper and lower bounds improve using
cross-Lipschitz regularization  we ﬁrst want to highlight the importance of the usage of the
local cross-Lipschitz constant in Theorem 2.1 for our robustness guarantee.

Local versus global Cross-Lipschitz constant: While no robustness guarantee has
been proven before  it has been discussed in [24] that penalization of the global Lipschitz
constant should improve robustness  see also [4]. For that purpose they derive the Lipschitz
constants of several diﬀerent layers and use the fact that the Lipschitz constant of a
composition of functions is upper bounded by the product of the Lipschitz constants of
the functions. In analogy  this would mean that the term supy∈B(x R) k∇fc(y) − ∇fj(y)k2 
which we have upper bounded in Proposition 2.2  in the denominator in Theorem 2.1 could
be replaced2 by the global Lipschitz constant of g(x) := fc(x) − fj(x). which is given as
supy∈Rd k∇g(x)k2 = supx6=y
. We have with kUk2 2 being the largest singular value
of U 

|g(x)−g(y)|
kx−yk2

|g(x) − g(y)| = hwc − wj  σ(U x) − σ(U y)i ≤ kwc − wjk2 kσ(U x) − σ(U y)k2

≤ kwc − wjk2 kU(x − y)k2 ≤ kwc − wjk2 kUk2 2 kx − yk2  

where we used that σ is contractive as σ0(z) =

1+e−αz and thus we get
k∇fc(x) − ∇fj(x)k2 ≤ kwc − wjk2 kUk2 2 .

1

sup
y∈Rd

2Note that then the optimization of R in Theorem 2.1 would be unnecessary.

7

MNIST (plain)

CIFAR10 (plain)

0.48

None Dropout Weight Dec. Cross Lip. None Dropout Weight Dec. Cross Lip.
0.69

0.21
Table 1: We show the average ratio αglobal
of the robustness guarantees αglobal  αlocal from Theorem 2.1 on
αlocal
the test data for MNIST and CIFAR10 and diﬀerent regularizers. The guarantees using the local Cross-Lipschitz
constant are up to eight times better than with the global one.

0.68

0.22

0.13

0.24

0.17

The advantage is clearly that this global Cross-Lipschitz constant can just be computed
once and by using it in Theorem 2.1 one can evaluate the guarantees very quickly. However 
it turns out that one gets signiﬁcantly better robustness guarantees by using the local
Cross-Lipschitz constant in terms of the bound derived in Proposition 2.2 instead of the just
derived global Lipschitz constant. Note that the optimization over R in Theorem 2.1 is done
using a binary search  noting that the bound of the local Lipschitz constant in Proposition
2.2 is monotonically decreasing in R. We have the following comparison in Table 1. We
want to highlight that the robustness guarantee with the global Cross-Lipschitz constant
was always worse than when using the local Cross-Lipschitz constant across all regularizers
and data sets. Table 1 shows that the guarantees using the local Cross-Lipschitz can be up
to eight times better than for the global one. As these are just one hidden layer networks  it
is obvious that robustness guarantees for deep neural networks based on the global Lipschitz
constants will be too coarse to be useful.

Experiments: We use a one hidden layer network with 1024 hidden units and the softplus
activation function with α = 10. Thus the resulting classiﬁer is continuously diﬀerentiable.
We compare three diﬀerent regularization techniques: weight decay  dropout and our Cross-
Lipschitz regularization. Training is done with SGD. For each method we have adapted
the learning rate (two per method) and regularization parameters (4 per method) so that
all methods achieve good performance. We do experiments for MNIST and CIFAR10
in three settings: plain  data augmentation and adversarial training. The exact settings
of the parameters and the augmentation techniques are described in the supplementary
material.The results for MNIST are shown in Figure 2 and the results for CIFAR10 are
in the supplementary material.For MNIST there is a clear trend that our Cross-Lipschitz
regularization improves the robustness of the resulting classiﬁer while having competitive
resp. better test error.
It is surprising that data augmentation does not lead to more
robust models. However  adversarial training improves the guarantees as well as adversarial
resistance. For CIFAR10 the picture is mixed  our CL-Regularization performs well for
the augmented task in test error and upper bounds but is not signiﬁcantly better in the
robustness guarantees. The problem might be that the overall bad performance due to the
simple model is preventing a better behavior. Data augmentation leads to better test error
but the robustness properties (upper and lower bounds) are basically unchanged. Adversarial
training slightly improves performance compared to the plain setting and improves upper
and lower bounds in terms of robustness. We want to highlight that our guarantees (lower
bounds) and the upper bounds from the adversarial samples are not too far away.

Illustration of adversarial samples: we take one test image from MNIST and apply the
adversarial generation from Section 4 wrt to the 2-norm to generate the adversarial samples for
the diﬀerent kernel methods and neural networks (plain setting)  where we use for each method
the parameters leading to best test performance. All classiﬁers change their originally correct
decision to a “wrong” one. It is interesting to note that for Cross-Lipschitz regularization
(both kernel method and neural network) the “adversarial” sample is really at the decision
boundary between 1 and 8 (as predicted) and thus the new decision is actually correct.
This eﬀect is strongest for our Kernel-CL  which also requires the strongest modiﬁcation to
generate the adversarial sample. The situation is diﬀerent for neural networks  where the
classiﬁers obtained from the two standard regularization techniques are still vulnerable  as
the adversarial sample is still clearly a 1 for dropout and weight decay.

Outlook Formal guarantees on machine learning systems are becoming increasingly more
important as they are used in safety-critical systems. We think that there should be more

8

Adversarial Resistance (Upper Bound)

wrt to L2-norm

Robustness Guarantee (Lower Bound)

wrt to L2-norm

Figure 2: Neural Networks  Left: Adversarial resistance wrt to L2-norm on MNIST. Right: Average ro-
bustness guarantee wrt to L2-norm on MNIST for diﬀerent neural networks (one hidden layer  1024 HU) and
hyperparameters. The Cross-Lipschitz regularization leads to better robustness with similar or better prediction
performance. Top row: plain MNIST  Middle: Data Augmentation  Bottom: Adv. Training

research on robustness guarantees (lower bounds)  whereas current research is focused on
new attacks (upper bounds). We have argued that our instance-speciﬁc guarantees using our
local Cross-Lipschitz constant is more eﬀective than using a global one and leads to lower
bounds which are up to 8 times better. A major open problem is to come up with tight
lower bounds for deep networks.

Original  Class 1

K-SVM  Pred:7  kδk2 = 1.2

K-CL  Pred:8  kδk2 = 3.5

NN-WD  Pred:8  kδk2 = 1.2 NN-DO  Pred:7  kδk2 = 1.1 NN-CL  Pred:8  kδk2 = 2.6
Figure 3: Top left: original test image  for each classiﬁer we generate the corresponding adversarial sample which
changes the classiﬁer decision (denoted as Pred). Note that for Cross-Lipschitz regularization this new decision
makes (often) sense  whereas for the neural network models (weight decay/dropout) the change is so small that
the new decision is clearly wrong.

9

References
[1] M. Abadi  A. Agarwal  P. Barham  E. Brevdo  Z. Chen  C. Citro  G. S. Corrado  A. Davis 
J. Dean  M. Devin  S. Ghemawat  I. J. Goodfellow  A. Harp  G. Irving  M. Isard  Y. Jia 
R. Józefowicz  L. Kaiser  M. Kudlur  J. Levenberg  D. Mané  R. Monga  S. Moore  D. G.
Murray  C. Olah  M. Schuster  J. Shlens  B. Steiner  I. Sutskever  K. Talwar  P. A. Tucker 
V. Vanhoucke  V. Vasudevan  F. B. Viégas  O. Vinyals  P. Warden  M. Wattenberg  M. Wicke 
Y. Yu  and X. Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous distributed
systems  2016.

[2] O. Bastani  Y. Ioannou  L. Lampropoulos  D. Vytiniotis  A. Nori  and A. Criminisi. Measuring

neural net robustness with constraints. In NIPS  2016.

[3] N. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten

detection methods. In ACM Workshop on Artiﬁcial Intelligence and Security  2017.

[4] M. Cisse  P. Bojanowksi  E. Grave  Y. Dauphin  and N. Usunier. Parseval networks: Improving

robustness to adversarial examples. In ICML  2017.

[5] N. Dalvi  P. Domingos  Mausam  S. Sanghai  and D. Verma. Adversarial classiﬁcation. In KDD 

2004.

[6] H. Drucker and Y. Le Cun. Double backpropagation increasing generalization performance. In

IJCNN  1992.

[7] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples.

In ICLR  2015.

[8] S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples.

In ICLR Workshop  2015.

[9] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

pages 770–778  2016.

[10] D. P. Helmbold and P. Long. On the inductive bias of dropout. Journal of Machine Learning

Research  16:3403–3454  2015.

[11] S. Hochreiter and J. Schmidhuber. Simplifying neural nets by discovering ﬂat minima. In NIPS 

1995.

[12] R. Huang  B. Xu  D. Schuurmans  and C. Szepesvari. Learning with a strong adversary. In

ICLR  2016.

[13] J. Kos  I. Fischer  and D. Song. Adversarial examples for generative models. In ICLR Workshop 

2017.

[14] A. Kurakin  I. J. Goodfellow  and S. Bengio. Adversarial examples in the physical world. In

ICLR Workshop  2017.

[15] Y. Liu  X. Chen  C. Liu  and D. Song. Delving into transferable adversarial examples and

black-box attacks. In ICLR  2017.

[16] D. Lowd and C. Meek. Adversarial learning. In KDD  2005.
[17] S.M. Moosavi-Dezfooli  A. Fawzi  O. Fawzi  and P. Frossard. Universal adversarial perturbations.

In CVPR  2017.

[18] N. Papernot  P. McDonald  X. Wu  S. Jha  and A. Swami. Distillation as a defense to adversarial

perturbations against deep networks. In IEEE Symposium on Security & Privacy  2016.

[19] P. Frossard S.-M. Moosavi-Dezfooli  A. Fawzi. Deepfool: a simple and accurate method to fool

deep neural networks. In CVPR  pages 2574–2582  2016.

[20] B. Schölkopf and A. J. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[21] U. Shaham  Y. Yamada  and S. Negahban. Understanding adversarial training: Increasing local

stability of neural nets through robust optimization. In NIPS  2016.

[22] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: A
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research 
15:1929–1958  2014.

10

[23] J. Stallkamp  M. Schlipsing  J. Salmen  and C. Igel. Man vs. computer: Benchmarking machine

learning algorithms for traﬃc sign recognition. Neural Networks  32:323–332  2012.

[24] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.

Intriguing properties of neural networks. In ICLR  pages 2503–2511  2014.

[25] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC  pages 87.1–87.12.
[26] S. Zheng  Y. Song  T. Leung  and I. J. Goodfellow. Improving the robustness of deep neural

networks via stability training. In CVPR  2016.

11

,Matthew Lawlor
Steven Zucker
Matthias Hein
Maksym Andriushchenko
Amir Dezfouli
Hassan Ashtiani
Omar Ghattas
Richard Nock
Peter Dayan
Cheng Soon Ong