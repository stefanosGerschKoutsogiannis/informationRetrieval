2019,Finite-Sample Analysis for SARSA with Linear Function Approximation,SARSA is an on-policy algorithm to learn a Markov decision process policy in reinforcement learning. We investigate the SARSA algorithm with linear function approximation under the non-i.i.d.\ setting  where a single sample trajectory is available. With a Lipschitz continuous policy improvement operator that is smooth enough  SARSA has been shown to converge asymptotically. However  its non-asymptotic analysis is challenging and remains unsolved due to the non-i.i.d.  samples  and the fact that the behavior policy changes dynamically with time. In this paper  we develop a novel technique to explicitly characterize the stochastic bias of a type of stochastic approximation procedures with time-varying Markov transition kernels. Our approach enables non-asymptotic convergence analyses of this type of stochastic approximation algorithms  which may be of independent interest. Using  our bias characterization technique and a  gradient descent type of analysis  we further provide the finite-sample analysis on the  mean square error of the SARSA algorithm.  In the end  we  present a fitted SARSA algorithm  which includes the original SARSA algorithm and its variant as special cases. This fitted SARSA algorithm provides a framework for \textit{iterative} on-policy fitted policy iteration  which is more memory and computationally efficient. For this fitted SARSA algorithm  we also present its finite-sample analysis.,Finite-Sample Analysis for SARSA with Linear

Function Approximation

Department of Electrical Engineering

University at Buffalo  The State University of New York

Shaofeng Zou

Buffalo  NY 14228
szou3@buffalo.edu

Tengyu Xu

Department of ECE

The Ohio State University

Columbus  OH 43210
xu.3260@osu.edu

Yingbin Liang

Department of ECE

The Ohio State University

Columbus  OH 43210
liang.889@osu.edu

Abstract

SARSA is an on-policy algorithm to learn a Markov decision process policy in
reinforcement learning. We investigate the SARSA algorithm with linear func-
tion approximation under the non-i.i.d. data  where a single sample trajectory is
available. With a Lipschitz continuous policy improvement operator that is smooth
enough  SARSA has been shown to converge asymptotically [28  23]. However  its
non-asymptotic analysis is challenging and remains unsolved due to the non-i.i.d.
samples and the fact that the behavior policy changes dynamically with time. In
this paper  we develop a novel technique to explicitly characterize the stochastic
bias of a type of stochastic approximation procedures with time-varying Markov
transition kernels. Our approach enables non-asymptotic convergence analyses of
this type of stochastic approximation algorithms  which may be of independent
interest. Using our bias characterization technique and a gradient descent type
of analysis  we provide the ﬁnite-sample analysis on the mean square error of
the SARSA algorithm. We then further study a ﬁtted SARSA algorithm  which
includes the original SARSA algorithm and its variant in [28] as special cases. This
ﬁtted SARSA algorithm provides a more general framework for iterative on-policy
ﬁtted policy iteration  which is more memory and computationally efﬁcient. For
this ﬁtted SARSA algorithm  we also provide its ﬁnite-sample analysis.

1

Introduction

SARSA  originally proposed in [31]  is an on-policy reinforcement learning algorithm  which
continuously updates the behavior policy towards attaining as large an accumulated reward as
possible over time. Speciﬁcally  SARSA is initialized with a state and a policy. At each time instance 
it takes an action based on the current policy  observes the next state  and receives a reward. Using
the newly observed information  it ﬁrst updates the estimate of the action-value function  and then
improves the behavior policy by applying a policy improvement operator  e.g.  ✏-greedy  to the
estimated action-value function. Such a process is iteratively taken until it converges (see Algorithm
1 for a precise description of the SARSA algorithm).
With the tabular approach that stores the action-value function  the convergence of SARSA has been
established in [33]. However  the tabular approach may not be applicable when the state space is

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

large or continuous. For this purpose  SARSA that incorporates parametrized function approximation
is commonly used  and is more efﬁcient and scalable. With the function approximation approach 
SARSA is not guaranteed to converge in general when the ✏-greedy or softmax policy improvement
operators are used [13  10]. However  under certain conditions  its convergence can be established.
For example  a variant of SARSA with linear function approximation was constructed in [28]  where
between two policy improvements  a temporal difference (TD) learning algorithm is applied to learn
the action-value function till its convergence. The convergence of this algorithm was established
in [28] using a contraction argument under the condition that the policy improvement operator is
Lipschitz continuous and the Lipschitz constant is not too large. The convergence of the original
SARSA algorithm under the same Lipschitz condition was later established using an O.D.E. approach
in [23].
Previous studies on SARSA in [28  23] mainly focused on the asymptotic convergence analysis 
which does not suggest how fast SARSA converges and how the accuracy of the solution depends
on the number of samples  i.e.  sample complexity. The goal of this paper is to provide such a
non-asymptotic ﬁnite-sample analysis of SARSA and to further understand how the parameters of
the underlying Markov process and the algorithm affect the convergence rate. Technically  such an
analysis does not follow directly from the existing ﬁnite-sample analysis for time difference (TD)
learning [4  34] and Q-learning [32]  where samples are taken by a Markov process with a ﬁxed
transition kernel. The analysis of SARSA necessarily needs to deal with samples taken from a Markov
decision process with a time-varying transition kernel  and in this paper  we develop novel techniques
to explicitly characterize the stochastic bias for a Markov decision process with a time-varying
transition kernel  which may be of independent interest.

1.1 Contributions

In this paper  we design a novel approach to analyze SARSA and a more general ﬁtted SARSA
algorithm  and develop the corresponding ﬁnite-sample error bounds. In particular  we consider the
on-line setting where a single sample trajectory with Markovian noise is available  i.e.  samples are
not identical and independently distributed (i.i.d.).
Bias characterization for time-varying Markov process. One major challenge in our analysis is
due to the fact that the estimate of the “gradient” is biased with non-i.i.d. Markovian noise. Existing
studies mostly focus on the case where the samples are generated according to a Markov process
with a ﬁxed transition kernel  e.g.  TD learning [4  34] and Q-learning with nearest neighbors [32]  so
that the uniform ergodicity of the Markov process can be exploited to decouple the dependency on
the Markovian noise  and then to explicitly bound the stochastic bias. For Markov processes with a
time-varying transition kernel  such a property of uniform ergodicity does not hold in general. In this
paper  we develop a novel approach to explicitly characterize the stochastic bias induced by non-i.i.d.
samples generated from Markov processes with time-varying transition kernels. The central idea of
our approach is to construct auxiliary Markov chains  which are uniformly ergodic  to approximate
the dynamically changing Markov process to facilitate the analysis. Our approach can also be applied
more generally to analyze stochastic approximation (SA) algorithms with time-varying Markov
transition kernels  which may be of independent interest.
Finite-sample analysis for on-policy SARSA. For the on-policy SARSA algorithm  as the estimate
of the action-value function changes with time  the behavior policy also changes. By a gradient
descent type of analysis [4] and our bias characterization technique for analyzing time-varying
Markov processes  we develop the ﬁnite-sample analysis for the on-policy SARSA algorithm with a
continuous state space and linear function approximation. Our analysis is for the on-line case with a
single sample trajectory and non-i.i.d. data. To the best of our knowledge  this is the ﬁrst ﬁnite-sample
analysis for this type of on-policy algorithm with time-varying behavior policy.
Fitted SARSA algorithm. We propose a more general on-line ﬁtted SARSA algorithm  where
between two policy improvements  a “ﬁtted” step is taken to obtain a more accurate estimate of the
action-value function of the corresponding behavior policy via multiple iterations rather than taking
only a single iteration as in the original SARSA. In particular  it includes the variant of SARSA in [28]
as a special case  in which each ﬁtted step is required to converge before doing policy improvement.
We provide a non-asymptotic analysis for the convergence of the proposed algorithm. Interestingly 
our analysis indicates that the ﬁtted step can stop at any time (not necessarily until convergence)
without affecting the overall convergence of the ﬁtted SARSA algorithm.

2

1.2 Related Work
Finite-sample analysis for TD learning. The asymptotic convergence of the TD algorithm was
established in [36]. The ﬁnite-sample analysis of the TD algorithm was provided in [9  19] under the
i.i.d. setting and in [4  34] recently under the non-i.i.d. setting  where a single sample trajectory is
available. The ﬁnite sample analysis for the two-time scale methods for TD learning was also studied
very recently under i.i.d. setting in [8]  under non-i.i.d. setting with constant step sizes in [15]  and
under non-i.i.d. setting with diminishing step sizes in [38]. Differently from TD  the goal of which
is to estimate the value function of a ﬁxed policy  SARSA aims to continuously update its estimate
of the action-value function to obtain an optimal policy. While samples of the TD algorithm are
generated by following a time-invariant behavior policy  the behavior policy that generates samples
in SARSA follows from an instantaneous estimate of the action-value function  which changes over
time.
Q-learning with function approximation. The asymptotic convergence of Q-learning with linear
function approximation was established in [23] under certain conditions. An approach based on a
combination of Q-learning and kernel-based nearest neighbor regression was proposed in [32] which
ﬁrst discretize the entire state space  and then use the nearest neighbor regression method to estimate
the action-value function. Such an approach was shown to converge  and a ﬁnite-sample analysis of
the convergence rate was further provided. Q-learning algorithms in [23  32] are off-policy algorithms 
where a ﬁxed behavior policy is used to collect samples  whereas SARSA is an on-policy algorithm
with a time-varying behavior policy. Moreover  differently from the nearest neighbor approach  we
consider SARSA with linear function approximation. These differences require different techniques
to characterize the non-asymptotic convergence rate.
On-policy SARSA algorithm. SARSA was originally proposed in [31]  and using the tabular
approach its convergence was established in [33]. With function approximation  SARSA is not
guaranteed to converge if ✏-greedy and softmax are used. With a smooth enough Lipschitz continuous
policy improvement operator  the asymptotic convergence of SARSA was shown in [23  28]. In this
paper  we further develop the non-asymptotic ﬁnite-sample analysis for SARSA under the Lipschitz
continuous condition.
Fitted value/policy iteration algorithms. The least-squares temporal difference learning (LSTD)
algorithms have been extensively studied in [6  5  25  20  12  29  30  35  37] and references therein 
where in each iteration a least square regression problem based on a batch data is solved. Approximate
(ﬁtted) policy iteration (API) algorithms further extend ﬁtted value iteration with policy improvement.
Several variants were studied  which adopt different objective functions  including least-squares
policy iteration (LSPI) algorithms in [18  21  39]  ﬁtted policy iteration based on Bellman residual
minimization (BRM) in [1  11]  and classiﬁcation-based policy iteration algorithm in [22]. The ﬁtted
SARSA algorithm in this paper uses an iterative way (TD(0) algorithm) to estimate the action-value
function between two policy improvements  which is more memory and computationally efﬁcient
than the batch method. Differently from [28]  we do not require a convergent TD(0) run for each
ﬁtted step. For this algorithm  we provide its non-asymptotic convergence analysis.

2 Preliminaries

2.1 Markov Decision Process
Consider a general reinforcement learning setting  where an agent interacts with a stochastic environ-
ment  which is modeled as a Markov decision process (MDP). Speciﬁcally  we consider a MDP that
consists of (X  A  P  r  )  where X is a continuous state space X⇢ Rd  and A is a ﬁnite action set.
We further let Xt 2X denote the state at time t  and At 2A denote the action at time t. Then  the
measure P deﬁnes the action dependent transition kernel for the underlying Markov chain {Xt}t0:
P(Xt+1 2 U|Xt = x  At = a) =RU P(dy|x  a)  for any measurable set U ✓X . The one-stage
reward at time t is given by r(Xt  At)  where r : X⇥A! R is the reward function  and is assumed
to be uniformly bounded  i.e.  r(x  a) 2 [0  rmax]  for any (x  a) 2X⇥A . Finally   denotes the
discount factor.
A stationary policy maps a state x 2X to a probability distribution ⇡(·|x) over A  which
does not depend on time. For a policy ⇡  the corresponding value function V ⇡ : X! R
is deﬁned as the expected total discounted reward obtained by actions executed according to

3

⇡: V ⇡ (x0) = E[P1t=0 tr(Xt  At)|X0 = x0]. The action-value function Q⇡ : X⇥A! R
is deﬁned as Q⇡(x  a) = r(x  a) + RX
P(dy|x  a)V ⇡(y). The goal is to ﬁnd an optimal pol-
icy that maximizes the value function from any initial state. The optimal value function is
deﬁned as V ⇤(x) = sup⇡ V ⇡(x)  8x 2X . The optimal action-value function is deﬁned as
. The optimal policy ⇡⇤ is then greedy with re-
Q⇤(x  a) = sup⇡ Q⇡(x  a)  8(x  a) 2X⇥A
It can be veriﬁed that Q⇤ = Q⇡⇤. The Bellman operator H is deﬁned as
spect to Q⇤.
(HQ)(x  a) = r(x  a) + RX
maxb2A Q(y  b)P(dy|x  a). It is clear that H is contraction in the
sup norm deﬁned as kQksup = sup(x a)2X⇥A |Q(x  a)|  and the optimal action-value function Q⇤ is
the ﬁxed point of H [3].

2.2 Linear Function Approximation
Let Q = {Q✓ : ✓ 2 RN} be a family of real-valued functions deﬁned on X⇥A . We consider the
problem where any function in Q is a linear combination of a set of N ﬁxed functions i : X⇥A! R
for i = 1  . . .   N. Speciﬁcally  for ✓ 2 RN  Q✓(x  a) =PN
i=1 ✓ii(x  a) = T (x  a)✓. We assume
that k(x  a)k2  1  8(x  a) 2X⇥A   which can be ensured by normalizing {i}N
i=1. The goal is
to ﬁnd a Q✓ with a compact representation in ✓ to approximate the optimal action-value function Q⇤
with a continuous state space.

3 Finite-Sample Analysis for SARSA

3.1 SARSA with Linear Function Approximation

We consider a ✓-dependent behavior policy  which changes with time. Speciﬁcally  the behavior policy
⇡✓t is given by (T (x  a)✓t)  where  is a policy improvement operator  e.g.  greedy  ✏-greedy 
softmax and mellowmax [2]. Suppose that {xt  at  rt}t0 is a sample trajectory of states  actions and
rewards obtained from the MDP following the time dependent behavior policy ⇡✓t (see Algorithm 1).
The projected SARSA with linear function approximation updates as follows:

(1)
where gt(✓t) = r✓Q✓(xt  at)t = (xt  at)t  t denotes the temporal difference at time t:
t = r(xt  at)+T (xt+1  at+1)✓tT (xt  at)✓t  and proj2 R(✓) := arg min✓0:k✓0k2R k✓✓0k2.
In this paper  we refer to gt as "gradient"  although it is not a gradient of any function.

✓t+1 = proj2 R(✓t + ↵tgt(✓t)) 

Algorithm 1 SARSA

Initialization:
✓0  x0  R  i  for i = 1  2  ...  N
Method:
⇡✓0 (T ✓0)
Choose a0 according to ⇡✓0
for t = 1  2  ... do

Observe xt and r(xt1  at1)
Choose at according to ⇡✓t1
✓t proj2 R(✓t1 + ↵t1gt1(✓t1))
Policy improvement: ⇡✓t (T ✓t)

end for

Here  the projection step is to control the norm of the gradient gt(✓t)  which is a commonly used
technique to control the gradient bias [4  16  17  7  26]. With a small step size ↵t and a bounded
gradient  ✓t does not change too fast. We note that [14] showed that SARSA converges to a bounded
region  and thus ✓t is bounded for all t  0. This implies that our analysis still holds without
the projection step. We further note that even without exploiting the fact that ✓t is bounded  the
ﬁnite-sample analysis for SARSA can still be obtained by combining our approach of analyzing the
stochastic bias with an extension of the approach in [34]. However  to convey the central idea of
characterizing the stochastic bias of a MDP with dynamically changing transition kernel  we focus on
the projected SARSA in this paper.

4

We consider the following Lipschitz continuous policy improvement operator  as in [28  23]. For
any ✓ 2 RN  the behavior policy ⇡✓ = (T ✓) is Lipschitz with respect to ✓: 8(x  a) 2X⇥A  

|⇡✓1(a|x)  ⇡✓2(a|x)| Ck✓1  ✓2k2 

(2)
where C > 0 is the Lipschitz constant. Further discussion about this assumption and its impact on
the convergence is provided in Section 5. We further assume that for any ﬁxed ✓ 2 RN  the Markov
chain {Xt}t0 induced by the behavior policy ⇡✓ and the transition kernel P is uniformly ergodic
with the invariant measure denoted by P✓  and satisﬁes the following assumption.
Assumption 1. There are constants m > 0 and ⇢ 2 (0  1) such that

dT V (P(Xt 2·| X0 = x)  P✓)  m⇢t 8t  0 

sup
x2X

where dT V (P  Q) denotes the total-variation distance between the probability measures P and Q.

We denote by µ✓ the probability measure induced by the invariant measure P✓ and the behavior
policy ⇡✓. We assume that the N base functions i’s are linearly independent in the Hilbert space
L2(X⇥A   µ✓⇤)  where ✓⇤ is the limit point of Algorithm 1  which will be deﬁned in the next section.
For the space L2(X⇥A   µ✓⇤)  two measurable functions on X⇥A are equivalent if they are identical
except on a set of µ✓⇤-measure zero.

3.2 Finite-Sample Analysis
We ﬁrst deﬁne A✓ = E✓[(X  A)(T (Y  B)  T (X  A))]  and b✓ = E✓[(X  A)r(X  A)]  where
E✓ denotes the expectation where X follows the invariant probability measure P✓  A is generated by
the behavior policy ⇡✓(A = ·|X)  Y is the subsequent state of X following action A  i.e.  Y follows
from the transition kernel P(Y 2·| X  A)  and B is generated by the behavior policy ⇡✓(B = ·|Y ).
It was shown in [23] that the algorithm in (1) converges to a unique point ✓⇤  which satisﬁes the
following relation: A✓⇤✓⇤ + b✓⇤ = 0  if the Lipschitz constant C is not so large that (A✓⇤ + CI ) is
negative deﬁnite1.
1⇢ ). Recall in (2) that the policy ⇡✓ is Lipschitz
Let G = rmax + 2R and  = G|A|(2 +dlog⇢
with respect to ✓ with Lipschitz constant C. We then make the following assumption [28  23].
Assumption 2. The Lipschitz constant C is not so large that (A✓⇤ + CI ) is negative deﬁnite  and
denote the largest eigenvalue of 1

me + 1

1

2(A✓⇤ + CI ) + (A✓⇤ + CI )T by ws < 0.

2 

G2(4C|A|G⌧ 2

0 + (12 + 2C)⌧0 + 1)(log T + 1)

The following theorems present the ﬁnite-sample bound on the convergence of SARSA with dimin-
ishing and constant step sizes.
Theorem 1. Consider SARSA with linear function approximation in Algorithm 1 with k✓⇤k2  R.
Consider a decaying step size ↵t =
2w(t+1) for t  0  where w  ws. Under Assumptions 1 and 2 
we have that
Ek✓T  ✓⇤k2
 
where ⌧0 = min{t  0 : m⇢t  ↵T}. For large T   ⌧0 ⇠ log T   and hence Ek✓T  ✓⇤k2
T ⌘ . Thus  to guarantee the accuracy E[k✓T  ✓⇤k2
O⇣ log3 T
complexity is given by O( 1
Theorem 1 indicates that SARSA has a faster convergence rate than the existing ﬁnite-sample bound
for Q-learning with nearest neighbors [32].
Theorem 2. Consider SARSA with linear function approximation in Algorithm 1 with k✓⇤k2  R.
Under Assumptions 1 and 2 and with a constant step size ↵t = ↵0 < 1
2ws

2 
2]   for a small   the overall sample

for t > 0  we have that

2G2(⌧0w + w + ⇢1)

 log3 1
 ).

4w2T

+

w2T

(3)

1

Ek✓T  ✓⇤k2

2 e2↵0wsT Ek✓0  ✓⇤k2

2 +

↵0G2((12 + 2C)⌧0 + 4GC|A|⌧ 2

0 + 8/⇢ + 1)

2ws

 

(4)

where ⌧0 = min{t  0 : m⇢t  ↵0}.

1It can be shown that if i’s are linearly independent in L2(X⇥A   µ✓⇤ )  then A✓⇤ is negative deﬁnite

[28  36].

5

If ↵0 is small enough  and T is large enough  then the algorithm converges to a small neighborhood

of ✓⇤. For example  if ↵t = 1/pT   the upper bound converges to zero as T ! 1. The proof of this
theorem is a straightforward extension of that for Theorem 1.
In order for Theorems 1 and 2 to hold  the projection radius R shall be chosen such that k✓⇤k2  R.
However  ✓⇤ is unknown in advance. We next provide an upper bound on k✓⇤k2  which can be
estimated in practice [4].
Lemma 1. For the projected SARSA algorithm in (1)  the limit point ✓⇤ satisﬁes that k✓⇤k2  rmax
 
|wl|
where wl < 0 is the largest eigenvalue of 1

2 (A✓⇤ + AT

✓⇤).

3.3 Outline of Technical Proof of Theorem 1
The major challenge in the ﬁnite-sample analysis of SARSA lies in analyzing the stochastic bias in
gradient  which are two-folds: (1) non-i.i.d. samples; and (2) dynamically changing behavior policy.
First  as per the updating rule in (1)  there is a strong coupling between the sample path and {✓t}t0 
because the samples are used to compute the gradient gt and then ✓t+1  which introduces a strong
dependency between {✓t}t0  and {Xt  At}t0  and thus the bias in gt. Moreover  differently from
TD learning and Q-learning  ✓t is further used (as in the policy ⇡✓t) to generate the subsequent actions 
which makes the dependency even stronger. Although the convergence can still be established using
the O.D.E. approach [23]  in order to derive a ﬁnite-sample analysis  the stochastic bias in the gradient
needs to be explicitly characterized  which makes the problem challenging.
Second  as ✓t updates  the transition kernel for the state-action pair (Xt  At) changes with time.
Previous analyses  e.g.  [4]  rely on the facts that the behavior policy is ﬁxed and that the underlying
Markov process is uniformly ergodic  so that the Markov process reaches its stationary distribution
quickly. In [28]  a variant of SARSA was studied  where between two policy improvements  the
behavior policy is ﬁxed  and a TD method is used to estimate its action-value function until con-
vergence. The behavior policy is then improved using a Lipschitz continuous policy improvement
operator. In this way  for each given behavior policy  the induced Markov process can reach its
stationary distribution quickly so that the analysis can be conducted. The SARSA algorithm studied
in this paper does not possess these nice properties. The behavior policy of the SARSA algorithm
changes at each time step  and the underlying Markov process does not necessarily reach a stationary
distribution due to lack of uniform ergodicity.
To provide a ﬁnite-sample analysis  our major technical novelty lies in the design of auxiliary Markov
chains  which are uniformly ergodic and   to approximate the original Markov chain induced by the
SARSA algorithm  and a careful decomposition of the stochastic bias. Using such an approach  the
gradient bias can be explicitly characterized. Then together with a gradient descent type of analysis 
we derive the ﬁnite-sample analysis for the SARSA algorithm.
To illustrate the main idea of the proof  we provide a sketch. We note that Step 3 contains our major
technical contributions of bias characterization for time-varying Markov processes.

Proof sketch. We ﬁrst introduce some notations. For any ﬁxed ✓ 2 RN  deﬁne ¯g(✓) = E✓[gt(✓)] 
where Xt follows the stationary distribution P✓  and (At  Xt+1  At+1) are subsequent actions and
states generated according to the policy ⇡✓ and the transition kernel P. Here  ¯g(✓) can be interpreted
as the noiseless gradient at ✓. We then deﬁne

Thus  ⇤t(✓t) measures the bias caused by using non-i.i.d. samples to estimate the gradient.
Step 1. Error decomposition. The error at each time step can be decomposed recursively as follows:

⇤t(✓) = h✓  ✓⇤  gt(✓)  ¯g(✓)i.

(5)

E[k✓t+1  ✓⇤k2

2] E[k✓t  ✓⇤k2

2] + 2↵tE[h✓t  ✓⇤  ¯g(✓t)  ¯g(✓⇤)i]

+ ↵2

(6)
Step 2. Gradient descent type analysis. The ﬁrst three terms in (6) mimic the analysis of the gradient
descent algorithm without noise  because the accurate gradient ¯gt at ✓t is used.
Due to the projection step in (1)  kgt(✓t)k2 is upper bounded by G. It can also be shown that

2] + 2↵tE[⇤t(✓t)].

t E[kgt(✓t)k2

E[h✓t  ✓⇤  ¯g(✓t)  ¯g(✓⇤)i]  (✓t  ✓⇤)T (A✓⇤ + CI )(✓t  ✓⇤).

(7)

6

For a not so large C  i.e.  ⇡✓ is smooth enough with respect to ✓  (A✓⇤ + CI ) is negative deﬁnite.
Then  we have

E[h✓t  ✓⇤  ¯g(✓t)  ¯g(✓⇤)i]  wsE[k✓t  ✓⇤k2
2].

(8)

Step 3. Stochastic bias analysis. This step consists of our major technical developments. The last term
in (6) is the bias caused by using a single sample path with non-i.i.d. data and time-varying behavior
policy. For convenience  we rewrite ⇤t(✓t) as ⇤t(✓t  Ot)  where Ot = (Xt  At  Xt+1  At+1).
Bounding this term is challenging due to the strong dependency between ✓t and Ot.
We ﬁrst show that ⇤t(✓  Ot) is Lipschitz in ✓. Due to the projection step  ✓t changes slowly with t.
Combining the two facts  we can show that for any ⌧> 0 

⇤t(✓t  Ot)  ⇤t(✓t⌧   Ot) + (6 + C)G2

↵i.

(9)

t1Xi=t⌧

Such a step is intended to decouple the dependency between Ot and ✓t by considering Ot and ✓t⌧ .
If the Markov chain {(Xt  At ✓ t)}t0 induced by SARSA was uniformly ergodic  and satisﬁed
Assumption 1  then for any ✓t⌧   Ot would reach its stationary distribution quickly for large ⌧.
However  such an argument is not necessarily true  since ✓t changes with time and thus the transition
kernel of the Markov chain changes with time.
Our idea is to construct an auxiliary Markov chain to assist our proof. Consider the following new
Markov chain. Before time t  ⌧ + 1  the states and actions are generated according to the SARSA
algorithm  but after time t  ⌧ + 1  the behavior policy is kept ﬁxed as ⇡✓t⌧ to generate all the
subsequent actions. We then denote by ˜Ot = ( ˜Xt  ˜At  ˜Xt+1  ˜At+1) the observations of the new
Markov chain at time t and time t + 1. For this new Markov chain  for large ⌧  ˜Ot reaches the
stationary distribution induced by ⇡✓t⌧ and P. It then can be shown that

E[⇤t(✓t⌧   ˜Ot)]  4G2m⇢⌧1.

(10)

The next step is to bound the difference between the Markov chain generated by the SARSA algorithm
and the auxiliary Markov chain that we construct. Since the behavior policy changes slowly  due to
its Lipschitz property and the small step size ↵t  the two Markov chains should not deviate from each
other too much. It can be shown that for the case with diminishing step size (similar argument can be
obtained for the case with constant step size) 

E[⇤t(✓t⌧   Ot)]  E[⇤t(✓t⌧   ˜Ot)] 

w
Combining (9)  (10) and (11) yields an upper bound on E[⇤t(✓t)].
Step 4. Putting the ﬁrst three steps together and recursively applying Step 1 complete the proof.

C|A|G3⌧

log

t
t  ⌧

.

(11)

4 Finite-sample Analysis for Fitted SARSA Algorithm
In this section  we introduce a more general on-policy ﬁtted SARSA algorithm (see Algorithm 2) 
which provides a general framework for on-policy ﬁtted policy iteration. Speciﬁcally  after each
policy improvement  we perform a “ﬁtted” step that consists of B TD(0) iterations to estimate the
action-value function of the current policy. This more general ﬁtted SARSA algorithm contains the
original SARSA algorithm [31] as a special case with B = 1 and the algorithm in [28] as another
special case with B = 1 (i.e.  until TD(0) converges). Moreover  the entire algorithm uses only
one single Markov trajectory  instead of restarting from state x0 after each policy improvement [28].
Differently from most existing ﬁtted policy iteration algorithms  where a regression problem for
model ﬁtting is solved between two policy improvements  our ﬁtted SARSA algorithm does not
require a convergent TD iteration process between policy improvements. As will be shown  the
on-policy ﬁtted SARSA algorithm is guaranteed to converge for an arbitrary B. The overall sample
complexity for this ﬁtted algorithm will be provided.
In fact  there is no need for the number B of TD iterations in the ﬁtted step to be the same. More
generally  by setting the number of TD iterations differently  we can control the estimation accuracy of

7

Algorithm 2 General Fitted SARSA

Initialization:
✓0  x0  R  i  for i = 1  2  ...  N
Method:
⇡✓0 (T ✓0)
Choose a0 according to ⇡✓0
for t = 0  1  2  ... do

TD learning of policy ⇡✓tB:
for j = 1  ...  B do

Observe xtB+j and r(xtB+j1  atB+j1)
Choose atB+j according to ⇡✓tB
✓tB+j proj2 R(✓tB+j1 + ↵tB+j1gtB+j1(✓tB+j1))

end for
Policy improvement: ⇡✓(t+1)B (T ✓(t+1)B)

end for

the action-value function between policy improvements using the ﬁnite-sample bound of TD [4]. Our
analysis can be extended to this general scenario in a straightforward manner  but the mathematical
expressions get more involved. Thus we focus on the simple case with the same B to convey the
central idea.
The following theorem provides the ﬁnite-sample bound on the convergence of the ﬁtted SARSA
algorithm.
Theorem 3. Consider the ﬁtted SARSA algorithm with linear function approximation as in Algorithm
2. Suppose that Assumptions 1 and 2 hold.
(1) With a decaying step size ↵t = 1

2tw for t  1 and w  ws  we have that

E[k✓T B  ✓⇤k2
2]
⇣4G2(⌧0 + B)w + (log T + 1)((6 + C)G2⌧0 + (6.5 + C)G2B
+ C|A|G3⌧ 2
T ⌘ . For any given B  to guarantee the accuracy E[k✓T B  ✓⇤k2

0 ) + 4G2/⇢ + 0.5BG2⌘w2BT 

where ⌧0 = inf{nB : m⇢nB  ↵T B}. For sufﬁciently large T   ⌧0 ⇠ log T   and hence Ek✓T 
2 O ⇣ log3 T
2]   for a small  
✓⇤k2
the overall sample complexity is given by O( 1
(2) With a constant step size ↵t = ↵0 < 1
E[k✓T B  ✓⇤k2
2]
 e2wsB↵0T k✓0  ✓⇤k2

↵0(BG2 + 2(6 + C)G2(⌧0 + B) + 8G2/⇢ + 2|A|G3⌧ 2
0 )

2wsB for t  0  we have that

 log3 1
 ).

  (13)

(12)

2 +

2ws

where ⌧0 = inf{nB : m⇢nB  ↵0}.
The item (2) of Theorem 3 indicates that with a small enough constant step size and a large enough
T   the ﬁtted SARSA algorithm converges to a small neighborhood of ✓⇤.
Theorem 3 further implies that the ﬁtted step can take any number of TD iterations (not necessarily
to converge) without affecting the overall convergence and sample complexity of the ﬁtted SARSA
algorithm. In particular  the comparison between the original SARSA and the ﬁtted SARSA algo-
rithms indicates that they have the same overall sample complexity. On the other hand  the ﬁtted
SARSA algorithm is more computationally efﬁcient due to the following two facts: (a) with the same
number of samples n0  the general ﬁtted SARSA algorithm uses a fewer number n0/B of policy
improvement operators; and (b) to apply the policy improvement operator  an inner product between
 and ✓tB needs to be computed  the complexity of which scales linearly with the size of the action
space |A|.

8

5 Discussion of Lipschitz Continuity Assumption

In this section  we discuss the Lipschitz continuity assumption on the policy improvement operator  
which plays an important role in the convergence of SARSA.
Using a tabular approach that stores the action-values  the convergence of the SARSA algorithm
was established in [33]. However  an example given in [13] shows that SARSA with function
approximation and ✏-greedy policy improvement operator is chattering  and does not converge. Later 
[14] showed that SARSA converges to a bounded region  although this region may be large  and
does not diverge as Q-learning with linear function approximation. One possible explanation of this
non-convergent behavior of the SARSA algorithm with ✏-greedy and softmax policy improvement
operators is the discontinuity in the action selection strategies [27  10]. More speciﬁcally  a slight
change in the estimate of the action-value function may result in a big change in the behavior policy 
which thus yields a completely different estimate of the action-value function.
Toward further understanding the convergence of SARSA  [10] showed that the approximate value
iteration with soft-max policy improvement is guaranteed to have ﬁxed points  which however may
not be unique  and [27] later showed that for any continuous policy improvement operator  ﬁxed
points of SARSA are guaranteed to exist. Then [28] developed a convergent form of SARSA by using
a Lipschitz continuous policy improvement operator  and demonstrated its convergence to the unique
limit point when the Lipschitz constant is not too large. As discussed in [27]  the non-convergence
example in [13] does not contradict the convergence result in [28]  because the example does not
satisfy the Lipschitz continuity condition of the policy improvement operator  which is essential to
guarantee the convergence of SARSA. In this paper  we follow this line of reasoning  and consider
Lipschitz continuous policy improvement operators.
As discussed in [28]  the Lipschitz constant C shall be chosen not so large to ensure the convergence
of the SARSA algorithm. However  to ensure exploitation  one generally prefers a large Lipschitz
constant C so that the agent can choose actions with higher estimated action-values. In [28]  an
adaptive approach to choose a policy improvement operator with a proper C was proposed. It was
also noted in [28] that it is possible that the convergence could be obtained with a much larger C than
the one suggested by Theorems 1  2 and 3.
However  an important open problem for the SARSA algorithms with Lipschitz continuous operator
(also for other algorithms with continuous action selection [10]) is that there is no theoretical
performance characterization of the solutions this type of algorithms produce. It is thus of future
interest to further investigate the performance of the policy generated by the SARSA algorithm with
Lipschitz continuous operator.

6 Conclusion

In this paper  we presented the ﬁrst ﬁnite-sample analysis for the SARSA algorithm with continuous
state space and linear function approximation. Our analysis is applicable to the on-line case with a
single sample path and non-i.i.d. data. In particular  we developed a novel technique to handle the
stochastic bias for dynamically changing behavior policies  which enables non-asymptotic analysis
of this type of stochastic approximation algorithms. We also presented a ﬁtted SARSA algorithm 
which provides a general framework for iterative on-policy ﬁtted policy iterations. We also presented
the ﬁnite-sample analysis for such a ﬁtted SARSA algorithm.

Acknowledgement

We would like to thank the anonymous reviewer and the Area Chair for their valuable comments. The
work of T. Xu and Y. Liang was supported in partby the U.S. National Science Foundation under
Grants CCF-1761506  ECCS-1818904  and CCF-1801855.

9

References
[1] A. Antos  C. Szepesvari  and R. Munos. Learning near-optimal policies with Bellman-residual
minimization based ﬁtted policy iteration and a single sample path. Machine Learning  71(1):89–
129  2008.

[2] K. Asadi and M. L. Littman. An alternative softmax operator for reinforcement learning. In

Proc. International Conference on Machine Learning (ICML)  2016.

[3] D. P. Bertsekas. Dynamic Programming and Optimal Control  volume 2. Athena Scientiﬁc  3rd

edition  2012.

[4] J. Bhandari  D. Russo  and R. Singal. A ﬁnite time analysis of temporal difference learning with

linear function approximation. arXiv preprint arXiv:1806.02450  2018.

[5] J. A. Boyan. Technical update: Least-squares temporal difference learning. Machine Learning 

49:233–246  2002.

[6] S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning.

Machine Learning  22:33–57  1996.

[7] S. Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R

in Machine Learning  8(3-4):231–357  2015.

[8] G. Dalal  B. Szorenyi  G. Thoppe  and S. Mannor. Finite sample analysis of two-timescale
stochastic approximation with applications to reinforcement learning. In Proc. Conference on
Learning Theory (COLT)  2018.

[9] G. Dalal  B. Szrnyi  G. Thoppe  and S. Mannor. Finite sample analyses for TD(0) with function

approximation. In Proc. AAAI Conference on Artiﬁcial Intelligence (AAAI)  2018.

[10] D. P. De Farias and B. Van Roy. On the existence of ﬁxed points for approximate value
iteration and temporal-difference learning. Journal of Optimization theory and Applications 
105(3):589–608  2000.

[11] A.-M. Farahmand  C. Szepesvari  and R. Munos. Error propagation for approximate policy and

value iteration. In Proc. Advances in Neural Information Processing Systems (NIPS)  2010.

[12] M. Ghavamzadeh  A. Lazaric  O. Maillard  and R. Munos. LSTD with random projections. In

Proc. Advances in Neural Information Processing Systems (NIPS)  2010.

[13] G. J. Gordon. Chattering in SARSA ()-a CMU learning lab internal report. 1996.
[14] G. J. Gordon. Reinforcement learning with function approximation converges to a region. In
Proc. Advances in Neural Information Processing Systems (NeurIPS)  pages 1040–1046  2001.
[15] H. Gupta  R. Srikant  and L. Ying. Finite-time performance bounds and adaptive learning rate
selection for two time-scale reinforcement learning. To appear in Proc. Advances in Neural
Information Processing Systems (NeurIPS)  2019.

[16] H. Kushner. Stochastic approximation: a survey. Wiley Interdisciplinary Reviews: Computa-

tional Statistics  2(1):87–96  2010.

[17] S. Lacoste-Julien  M. Schmidt  and F. Bach. A simpler approach to obtaining an o(1/t) conver-
gence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002 
2012.

[18] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning

Research  4:1107–1149  2003.

[19] C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does
constant step-size and iterate averaging go? In Proc. International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS)  2018.

[20] A. Lazaric  M. Ghavamzadeh  and R. Munos. Finite-sample analysis of lstd. In Proc. Interna-

tional Conference on Machine Learning (ICML)  2010.

10

[21] A. Lazaric  M. Ghavamzadeh  and R. Munos. Finite-sample analysis of least-squares policy

iteration. Journal of Machine Learning Research  13:3041–3074  2012.

[22] A. Lazaric  M. Ghavamzadeh  and R. Munos. Analysis of classiﬁcation-based policy iteration

algorithms. Journal of Machine Learning Research  17:583–612  2016.

[23] F. S. Melo  S. P. Meyn  and M. I. Ribeiro. An analysis of reinforcement learning with function
approximation. In Proc. International Conference on Machine Learning (ICML)  pages 664–671.
ACM  2008.

[24] A. Y. Mitrophanov. Sensitivity and convergence of uniformly ergodic markov chains. Journal

of Applied Probability  42(4):1003–1014  2005.

[25] R. Munos and C. Szepesvari. Finite-time bounds for ﬁtted value iteration. Journal of Machine

Learning Research  9:815–857  May 2008.

[26] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[27] T. J. Perkins and M. D. Pendrith. On the existence of ﬁxed points for Q-learning and Sarsa in
partially observable domains. In Proc. International Conference on Machine Learning (ICML) 
pages 490–497  2002.

[28] T. J. Perkins and D. Precup. A convergent form of approximate policy iteration. In Proc.

Advances in Neural Information Processing Systems (NeurIPS)  pages 1627–1634  2003.

[29] B. A. Pires and C. Szepesvari. Statistical linear estimation with penalized estimators: An
application to reinforcement learning. In Proc. International Conference on Machine Learning
(ICML)  2012.

[30] L. Prashanth  N. Korda  and R. Munos. Fast LSTD using stochastic approximation: Finite time
analysis and application to trafﬁc control. In Proc. Joint European Conference on Machine
Learning and Knowledge Discovery in Databases  2013.

[31] G. A. Rummery and M. Niranjan. Online Q-learning using connectionist systems. Technical

Report  Cambridge University Engineering Department  Sept. 1994.

[32] D. Shah and Q. Xie. Q-learning with nearest neighbors. In Proc. Advances in Neural Information

Processing Systems (NeurIPS)  2018.

[33] S. Singh  T. Jaakkola  M. L. Littman  and C. Szepesvári. Convergence results for single-step

on-policy reinforcement-learning algorithms. Machine Learning  38(3):287–308  2000.

[34] R. Srikant and L. Ying. Finite-time error bounds for linear stochastic approximation and TD

learning. In Proc. Annual Conference on Learning Theory (CoLT)  2019.

[35] M. Tagorti and B. Scherrer. On the rate of convergence and error bounds for LSTD (). In Proc.

International Conference on Machine Learning (ICML)  2015.

[36] J. N. Tsitsiklis and B. Roy. An analysis of temporal-difference learning with function approxi-

mation. IEEE Transactions on Automatic Control  42(5):674–690  May 1997.

[37] S. Tu and B. Recht. Least-squares temporal difference learning for the linear quadratic regulator.

In Proc. International Conference on Machine Learning (ICML)  2018.

[38] T. Xu and Y. Zou  S.and Liang. Two time-scale off-policy TD learning: Non-asymptotic analysis
over Markovian samples. To appear in Proc. Advances in Neural Information Processing
Systems (NeurIPS)  2019.

[39] Z. Yang  Y. Xie  and Z. Wang. A theoretical analysis of deep Q-learning. ArXiv: 1901.00137 

Jan. 2019.

11

,Shaofeng Zou
Tengyu Xu
Yingbin Liang