2018,Diffusion Maps for Textual Network Embedding,Textual network embedding leverages rich text information associated with the network to learn low-dimensional vectorial representations of vertices.
Rather than using typical natural language processing (NLP) approaches  recent research exploits the relationship of texts on the same edge to graphically embed text. However  these models neglect to measure the complete level of connectivity between any two texts in the graph. We present diffusion maps for textual network embedding (DMTE)  integrating global structural information of the graph to capture the semantic relatedness between texts  with a diffusion-convolution operation applied on the text inputs. In addition  a new objective function is designed to efficiently preserve the high-order proximity using the graph diffusion. Experimental results show that the proposed approach outperforms state-of-the-art methods on the vertex-classification and link-prediction tasks.,Diffusion Maps for Textual Network Embedding

Xinyuan Zhang  Yitong Li  Dinghan Shen  Lawrence Carin

Department of Electrical and Computer Engineering

{xy.zhang  yitong.li  dinghan.shen  lcarin}@duke.edu

Duke University

Durham  NC 27707

Abstract

Textual network embedding leverages rich text information associated with the
network to learn low-dimensional vectorial representations of vertices. Rather
than using typical natural language processing (NLP) approaches  recent research
exploits the relationship of texts on the same edge to graphically embed text. How-
ever  these models neglect to measure the complete level of connectivity between
any two texts in the graph. We present diffusion maps for textual network embed-
ding (DMTE)  integrating global structural information of the graph to capture
the semantic relatedness between texts  with a diffusion-convolution operation
applied on the text inputs. In addition  a new objective function is designed to efﬁ-
ciently preserve the high-order proximity using the graph diffusion. Experimental
results show that the proposed approach outperforms state-of-the-art methods on
the vertex-classiﬁcation and link-prediction tasks.

1

Introduction

Learning effective vectorial embeddings to rep-
resent text can lead to improvements in many
natural language processing (NLP) tasks. How-
ever  most text embedding models do not em-
bed the semantic relatedness between different
texts. Graphical text networks address this prob-
lem by adding edges between correlated text
vertices. For example  paper citation networks
contain rich textual information and the citation
relationships provide structural information to
reﬂect the similarity between papers. Graphical
text embedding naturally extends the problem
to network embedding (NE)  mapping vertices
of a graph into a low-dimensional space. The
learned representations containing structure and textual information can be used as features for
network tasks  such as vertex classiﬁcation [22]  link prediction [14]  and tag recommendation [31].
Learning network embeddings is a challenging research problem  due to the sparsity  non-linearity
and high dimensionality of the graph data.
In order to exploit textual information associated with each vertex  some NE models [13  33  19  26]
embed texts with a variety of NLP approaches  ranging from bag-of-words models to deep neural
models. However these text embedding methods fail to consider the semantic distance indicated
from the graph. In [30  24] it was recently proposed to simultaneously embed two texts on the same
edge using a mutual-attention mechanism. But in real-world sparse networks  it is intuitive that two
connected vertices do not necessarily share more similarities than two unconnected vertices. Figure 1

Figure 1: Three sentences from the DBLP dataset. Ver-
tices A and C are second neighbors  i.e.  two vertices that
are not on the same edge but share at lease one common
neighbor (vertex B). The alignment words are colored.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

The K-D-B-Tree: A Search Structure For Large Multidimensional Dynamic Indexes.Segment Indexes: DynamicIndexingTechniques for Multi-DimensionalInterval Data.Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases.ABCCitationCitationpresents three examples from the DBLP dataset. By aligning dynamic index and multi-dimensional 
the sentences of vertex A and vertex C are closer than the sentence of their common ﬁrst neighbor 
vertex B. The relatedness between two vertices that are not linked by an edge cannot be preserved by
only capturing the local pairwise proximity.
We propose a ﬂexible approach for textual network embedding  including global structural information
without increasing model complexity. Global structure information serves to capture the long-distance
relationship between two texts  incorporating connection paths within different steps. The diffusion-
convolution operation [2] is employed to build a latent representation of the graph-structured text
inputs  by scanning a diffusion map across each vertex. The graph diffusion  comprised of a
normalized adjacency matrix and its power series  provides the probability of random walks from
one vertex to another within a certain number of steps in the graph. The idea is to measure the level
of connectivity between any two texts when considering all paths between them. In this study  we
consider text-based information networks  but our model can be ﬂexibly extended to other types of
content.
We further use the graph diffusion to redesign the objective function  capturing high-order proximity.
Unlike some NE models [27]  that calculate the probability of vertex vi being generated by vj  we
preserve high-order proximity by calculating the probability of vertex vi given the diffusion map
of vj. Compared to GraRep [5]  the proposed objective function is more computationally efﬁcient 
especially for large-scale networks  because it does not need matrix factorization during training.
This objective function is able to scale to directed or undirected  and weighted or unweighted graphs.
To demonstrate the effectiveness of our model  we focus on two common tasks in analysis of textual
information networks: (i) multi-label classiﬁcation  where we predict the labels of each text; and (ii)
link prediction  where we predict the existence of an edge given a pair of vertices. The experiments
are conducted on several real-world datasets of information networks. Experimental results show
that the DMTE model outperforms all other methods considered. The superiority of the proposed
approach indicates that the diffusion process helps to incorporate long-distance relationship between
texts and thus to achieve more informative textual network embeddings.

2 Related Work

Text Embedding Many existing methods embed text messages into a vector space for various NLP
tasks. Early approaches include bag-of-words models or topic models [4]. The Skip-gram model [16] 
which learns distributed word vectors by utilizing word co-occurrences in a local context  has been
further extended to the document level via a paragraph vector [13] to learn text latent representations.
To exploit the internal structure of text  more-complicated text embedding models have emerged 
adopting deep neural network architectures. For example  convolutional neural networks (CNNs)
[10  6  34] have been considered to apply a convolution kernel over different positions of the text 
followed by max-pooling to obtain a ﬁxed-length vectorial representation. Recursive neural tensor
networks (RNTNs) [25] have applied a tensor-based composition function over parse trees to obtain
sentence representations. LSTM-based recurrent neural networks (RNNs) [12] capture long-term
dependencies in the text  using long short-term memory cells. However  deep neural architectures
usually assume the availability of a large dataset  unrealistic for many information networks. When
the data size is small  some methods [18  9] avoid over-ﬁtting by simply averaging embeddings of
each word in the text  achieving competitive empirical results.

Network Embedding Earlier works including IsoMap [29]  LLE [21]  and Laplacian Eigenmaps
[3] transform feature vectors of vertices into an afﬁnity graph  and then solve for the leading
eigenvectors as the embedding. Recent NE models focus on learning the vectorial representation of
existing networks. For example  DeepWalk [20] uses the Skip-gram model [16] on vertex sequences
generated by truncated random walks  learning vertex embeddings. In node2vec [8]  the random walk
strategy of DeepWalk is modiﬁed for multi-scale representation learning. To exploit the distance
between vertices  LINE [27] designed objective functions to preserve the ﬁrst-order and second-order
proximity  while [5] integrates global structure information by expanding the proximity into k-order.
In [32] deep models are employed to capture the nonlinear network structure. However  all these
methods only consider structural information of the network  without leveraging rich heterogeneous
information associated with vertices; this may result in less informative representations  especially
when the edges are sparse.

2

To address this issue  some recent works combine structure and content information to learn better
embeddings. For example  TADW [33] shows that DeepWalk is equivalent to matrix factorization  and
text features can be incorporated into the framework. TriDNR [19] uses information from structure 
content and labels in a coupled neural network architecture  to learn the vertex representation. CENE
[26] integrates text modeling and structure modeling by regarding the content information as a special
kind of vertex. CANE [30] learns two embedding vectors for each vertex where the context-aware
text embedding is obtained using a mutual attention mechanism. However  none of these methods
takes into account the similarities of context inﬂuenced by global structural information.

3 Problem Deﬁnition
Deﬁnition 1. A textual information network is G = (V  E  T )  where V = {vi}i=1 ···  N is the
set of vertices  E = {ei j}N
i j=1 is the set of edges  and T = {ti}i=1 ···  N is the set of texts associated
with vertices. Each edge ei j has a weight si j representing the relationship between vertices vi and
vj. If vi and vj are not linked  si j = 0. If there exists an edge between vi and vj  si j = 1 for an
unweighted graph  and si j > 0 for a weighted graph. A path is a sequence of edges that connect two
vertices. The text of vertex vi  ti  is comprised of a word sequence < w1 ···   w|ti| >.
Deﬁnition 2. Let S ∈ RN×N be the adjacency matrix of a graph whose entry si j ≥ 0 is the weight
of edge ei j. The transition matrix P ∈ RN×N is obtained by normalizing rows of S to sum to one 
with pi j representing the transition probability from vertex vi to vertex vj within one step. Then an
h-step transition matrix can be computed with P to the h-th power  i.e.  Ph. The entry ph
i j refers to
the transition probability from vertex vi to vertex vj within exactly h steps.

Deﬁnition 3. A network embedding aims to learn a low-dimensional vector vi ∈ Rd for vertex
vi ∈ V   where d (cid:28) |V | is the dimension of the embedding. The embedding matrix V for the
complete graph is the concatenation of {v1  v2 ···   vN}. The distance between vertices on the
graph and context similarity should be preserved in the representation space.

Deﬁnition 4. The diffusion map of vertex vi is ui  the i-th row of the diffusion embedding matrix
U  which maps from vertices and their embeddings to the results of a diffusion process that begins at
vertex vi. U is computed by

U =

λhPhV 

(1)

H−1(cid:88)h=0

where λh is the importance coefﬁcient that typically decreases as the value of h increases. The
high-order proximity in the network is preserved in diffusion maps.

4 Method

We employ a diffusion process to build long-distance semantic relatedness in text embeddings  and
global structural information in the objective function. To incorporate both the structure and textual
information of the network  we adopt two types of embeddings vs
i for each vi vertex  as
proposed in [30]. The structure-based embedding vector vs
i is obtained by feeding the i-th row
of a learned structure embedding table Es ∈ RN×ds into a function. The text-based embedding
vector vt
i is obtained by applying the diffusion convolutional operation on the text inputs (see Section
4.2). Here dimensions of the structure embedding and the text embedding satisfy ds + dt = d. The
embedding of vertex vi is simply the concatenation of vt
i . In this work  vi
is learned by an unsupervised approach  and it can be used directly as a feature vector of vertex vi for
various tasks. The objective function consists of four parts  which measure both the structure and text
embeddings. The high-order proximity is preserved during training without increasing computational
complexity. The entire framework for textual network embedding is illustrated in Figure 3 where
each vertex is associated with a text.

i   i.e.  vi = vt

i ⊕ vs

i and vt

i and vs

4.1 Diffusion Process

3

Initially the network only has a few active ver-
tices  due to sparsity. Through the diffusion
process  information is delivered from active ver-
tices to inactive ones by ﬁlling information gaps
between vertices [1]; vertices may be connected
by indirect  multi-step paths. This process is
the same as the molecular diffusion in a ﬂuid 
where particles move from high-concentration
areas to low-concentration areas. We introduce
the transition matrix P and its power series for
the diffusion process. The directed graph with
four vertices and normalized weights in Figure 2
shows the smoothing effect of the high order of
P in diffusion process. The original graph only
has edges e1 2  e1 3  e3 4 and e1 4  while the in-
formation gaps between other vertices are not depicted. The diffusion process can smooth the whole
graph with the higher order of P  so that indirect relationships  such as (n2  n4)  can be connected
(via a multi-step diffusion process). As we can see from Figure 2(b)  the fourth-order diffusion
graph is fully connected. The number associated with each edge represents the transition probability
from one vertex to another within exactly 4 steps. The network will be stable when information is
eventually evenly distributed.

Figure 2: A simple example of diffusion process
in a directed graph.

4.2 Text Embedding
A word sequence t =< w1 ···   w|t| > is mapped into a set of dt-dimensional real-valued vectors
< w1 ···   w|t| > by looking up the word embedding matrix Ew. Here Ew ∈ R|w|×dt is initialized
randomly  and learned during training  and |w| is the vocabulary size of the dataset. We can obtain a
simple text representation xi ∈ Rdt of vertex vi by taking the average of word vectors. Although
the word order is not preserved in such a representation  taking the average of word embeddings
can avoid over-ﬁtting efﬁciently  especially when the data size is small [23]. Given the ﬁxed-length
vectors of each text  the input texts can be represented by matrix X ∈ RN×dt  where the i-th row is
xi.

wi 

X = x1 ⊕ x2 ⊕ ··· ⊕ xN .

(2)

Alternatively  we can use the bi-directional LSTM [7] which processes a text from both directions to
capture long-term dependencies. Text inputs are represented by the mean of all hidden states.

x =

1
|t|

|t|(cid:88)i=1

−→h i = LST M (wi  hi−1) 
(−→h i ⊕ ←−h i) 

x =

1
|t|

|t|(cid:88)i=1

←−h i = LST M (wi  hi+1)

X = x1 ⊕ x2 ⊕ ··· ⊕ xN .

(3)

(4)

However  in this text representation matrix for both approaches  the embeddings are completely
independent  without leveraging the semantic relatedness indicated from the graph. To address this
issue  we employ the diffusion convolutional operator [2] to measure the level of connectivity between
any of two texts in the network.
Let P∗ ∈ RN×H×N be a tensor containing H hops of power series of P  i.e.  the concatenation of
{P0  P1 ···   PH−1}. V∗t ∈ RN×H×d is the tensor version of the text embedding representation 
after the diffusion convolutional operation. The activation V∗(i j k)
for vertex i  hop j  and feature k
is given by

t

V∗(i j k)

t

= f (W(j k) ·

N(cid:88)n=1

P∗(i j n)X(n k)) 

(5)

where W ∈ RH×d is the weight matrix and f is a nonlinear differentiable function. The activations
can be expressed equivalently using tensor notation
(6)

V∗t = f (W (cid:12) P∗X) 

4

12341.00.420.380.620.890.110.080.5(a)Originalgraph12340.050.110.540.020.320.100.510.330.420.070.400.110.100.230.070.59(b)Forthorderdiffusiongraph.Figure2:(Left)Originalgraphonlyhaveconnectededgee1 2 e1 3 e3 4ande1 4.Hereweplotitasdirectedgraphbecausewenormalizetheoutgoingedgesweight.(Right)Forthpowerdiffusiongraph.onthegraph.Figure2givesanexampleofthesmoothingeffectofdiffusiongraph.Thisexampleonly137containsfournodes.Theedgesarenormalizedsothegraphbecomesdirected.Theoriginalgraph138onlyhaveedgepaire1 2 e1 3 e3 4ande1 4.However theindirectrelationshipbetweenotheredge139pairsarenotconsidered.Diffusiongraphcansmoothingthewholegraphwithhigherorder.Thus140thoseindirectrelationships like(n2 n4) canalsobeconsidered.Aswecanseefromﬁgure2(b) the141forthorderdiffusiongraphbecomesfullyconnected.Whentheordergoestoinﬁnity itcorresponds142totheconvergencepointofarandomwalk.1434.2TextEmbedding144Awordsequencet=<w1 ··· w|t|>ismappedintoasetofdt-dimensionalreal-valuedvectors145<w1 ··· w|t|>bylookingupthewordembeddingmatrixEw.HereEw∈R|w|×dtisrandomly146initializedandfurtherlearnedduringtrainingand|w|isthevocabularysizeofthedataset.Wecan147obtainasimpletextrepresentationxi∈Rdtofverticevibytakingtheaverageofwordvectors.148Althoughthewordorderisnotpreservedinsuchrepresentation [5]hasshownthatwordembedding149averagemodelscanperformsurprisinglywellandavoidover-ﬁttingefﬁcientlyinmanyNLPtasks.150Giventheﬁxed-lengthvectorsofeachtext theinputtextscanberepresentedbymatrixX∈RN×dt151wherethei-throwisxi.152x=1c|t|(cid:2)i=1wi X=x1⊕x2⊕···⊕xN.However inthistextrepresentationmatrixeachembeddingiscompletelyindependentwithout153leveragingthesemanticrelatednessindicatedfromthegraph.Toaddressthisissue weemploy154diffusionconvolutionaloperator[1]tomeasurethelevelofconnectivitybetweenanyoftwotextsin155thenetwrok.156LetP∗∈RN×H×NbeatensorcontainingHhopsofpowerseriesofP i.e. theconcatenationof157{P0 P1 ··· PH−1}.V∗t∈RN×H×disthetensorversionoftextembeddingrepresentionafter158diffusionconvolutionaloperation.TheactivationV∗(i j k)tfornodei hopj andfeaturekisgivenby159V∗(i j k)t=f(W(j k)·N(cid:2)n=1P∗(i j n)X(n k))(2)whereW∈RH×distheweightmatrixandfisanon-lineardifferentiablefunction.Theactivations160canbeexpressedequavalentlyusingtensornotations.161V∗t=f(W(cid:4)P∗X)(3)where(cid:4)representselement-wisemultiplication.Thistensorrepresentationconsidersallpaths162betweentwotextsinthenetworkandthusincludeslong-distancesemanticrelationship.Withlonger1634Figure 3: An illustration of our framework for textual network embedding.

where (cid:12) represents element-wise multiplication. This tensor representation considers all paths
between two texts in the network  and thus includes long-distance semantic relationship. With longer
paths discounted more than shorter paths  the text embedding matrix Vt is given by

Vt =

λhV∗(: h :)

t

.

(7)

H−1(cid:88)h=0

Through the diffusion process  text representations  i.e.  rows of Vt are not embedded independently.
With the whole graph being smoothed  indirect relationships between texts that are not on the same
edge can be considered to learn embeddings.

4.3 Objective Function

Given the set of edges E  the goal of DMTE is to maximize the following overall objective function:

αttLtt(e) + αssLss(e) + αstLst(e) + αtsLts(e)

(8)

L =(cid:88)e∈E

L(e) =(cid:88)e∈E

where αtt  αss  αst  and αts control the weight of corresponding objectives. The overall objective
consists of four parts: Ltt(e) denotes the objective for text embeddings  Lss(e) denotes the objective
for structure embeddings  Lst(e) and Lts(e) denote the objectives that consider both structure and
text embeddings to map them into the same representation space. We assume the network is directed 
since the undirected edge can be considered as two opposite-directed edges with equal weights. Then
each objective is to measure the log-likelihood of generating vi conditioned on vj  where vi and vj
are on the same directed edge:

Ltt(e) = si j log p(vt

j) = si j log

Lss(e) = si j log p(vs

j) = si j log

Lst(e) = si j log p(vs

j) = si j log

Lts(e) = si j log p(vt

j) = si j log

i|vt

i|us

i|vt

i|us

exp(vt

k∈Vt
exp(vs

k∈Vs
exp(vs

k∈Vs
exp(vt

 

i · vt
j)
k · vt
exp(vt
j)
i · us
j)
exp(vs
k · us
j)
i · vt
j)
exp(vs
k · vt
j)
i · us
j)
exp(vt
k · us
j)

 

 

.

(cid:80)vt
(cid:80)vs
(cid:80)vs
(cid:80)vt

k∈Vt

(9)

(10)

(11)

(12)

j) computes the probability conditioned on the diffusion map of vertex vj  and p(·|vt
Note that p(·|us
j)
computes the probability conditioned on the text embedding of vertex vj. Compared to using vs
j to
compute the conditional probability  the diffusion map us
j utilizes both local information and global
relations of vertex vj in the graph. We use vt
j because the global
structural information is included during text embedding  with the diffusion convolutional operation.
Moreover the high-order proximity is preserved without using matrix factorization  which may be
computationally inefﬁcient for large-scale networks.

j instead of the diffusion map ut

5

1234…𝑷𝑷𝟎𝟎𝑷𝑷𝟏𝟏𝑷𝑷𝑯𝑯−𝟏𝟏*Structure Embedding Table 𝑬𝑬𝒔𝒔Word Embedding Table 𝑬𝑬𝒘𝒘𝒙𝒙1𝒙𝒙2𝒙𝒙3𝒙𝒙4𝜆𝜆0𝜆𝜆1𝜆𝜆𝐻𝐻−1𝒗𝒗𝟏𝟏𝒕𝒕𝒗𝒗𝟐𝟐𝒕𝒕𝒗𝒗𝟑𝟑𝒕𝒕𝒗𝒗𝟒𝟒𝒕𝒕𝒗𝒗𝟏𝟏𝒔𝒔𝒗𝒗𝟐𝟐𝒔𝒔𝒗𝒗𝟒𝟒𝒔𝒔𝒗𝒗𝟑𝟑𝒔𝒔4.4 Optimization

Optimizing (8) is computationally expensive  since the conditional probability requires the summation
over the entire vertex set. In [17] negative sampling was proposed to solve this problem. For each
edge ei j  we sample multiple negative edges according to some noisy distribution. Then during
training the conditional function p(vi|vj) can be replaced by

log σ(vi · vj) +

K(cid:88)k=1

Evk∼Pn(v)[log σ(−vk · vj)] 

(13)

where σ(·) is the sigmoid function  K is the number of negative samples  and Pn(v) ∝ d3/4
is the
distribution of vertices with dv being the out-degree of vertex v. All parameters are jointly trained.
Adam [11] is adopted for stochastic optimization. In each step  Adam samples a mini-batch of edges
and then updates the model parameters.

v

5 Experiments

We evaluate the proposed method for the multi-label classiﬁcation and link prediction tasks. We
design four versions of DMTE in our experiments: (i) DMTE without diffusion process; (ii) DMTE
with text embedding only; (iii) DMTE with bidirectional LSTM (Bi-LSTM); (iv) DMTE with
word average embedding (WAvg). In DMTE without diffusion process  the diffusion convolutional
operation is not added on top of the text inputs  i.e.  the text embedding matrix Vt is directly replaced
by X in Eq. 2. In DMTE with text embedding only  the embedding of vertex vi is only vt
i instead of
the concatenation of vt
i . In DMTE with Bi-LSTM  the input texts embedding matrix Xt is
obtained using Eq. 4. In DMTE with WAvg  the input texts embedding matrix Xt is obtained using
Eq. 2. We compare the four versions of DMTE model with seven competitive network embedding
algorithms. Experimental results for multi-label classiﬁcation are evaluated by Macro F1 scores and
experimental results for link prediction are evaluated by Area Under the Curve (AUC).

i and vs

Datasets We conduct experiments on three real-world datasets: DBLP  Cora  and Zhihu.
• DBLP [28] is a citation network that consists of bibliography data in computer science. In our
experiments  60744 papers are collected in 4 research areas: database  data mining  artiﬁcial in-
telligence  and computer vision. The network has 52890 edges indicating the citation relationship
between papers.

network has 5214 edges indicating the citation relationship between papers.

• Cora [15] is a citation network that consists of 2277 machine learning papers in 7 classes. The
• Zhihu [26] is a Q&A based community social network in China. In our experiments  10000
active users are collected as vertices and 43894 edges indicating the relationship. The description
of their interested topics are used as text information.

Baselines The following baselines are compared with our DMTE model:
• Structure-Based Methods: DeepWalk [20]  LINE [27]  node2vec [8].
• Structure and Text Combined Methods: TADW [33]  Tri-DNR [19]  CENE [26]  CANE [30].
Evaluation and Parameter Settings For link prediction  we evaluate the performance with AUC 
which is widely used for a ranking list. Since the testing set only contains existing edges as positive
instances  we randomly sample the same number of non-existing edges as negative instances. Positive
and negative edges are ranked according to a prediction function and AUC is employed to measure
the probability that vertices on a positive edge are more similar than those on a negative edge. The
experiment for each training ratio is executed 10 times and the mean AUC scores are reported  where
the higher value indicates a better performance.
For multi-label classiﬁcation  we evaluate the performance with Macro-F1 scores. We ﬁrst learn
embeddings with all edges and vertices in an unsupervised way. Once the vertex embeddings are
obtained  we feed them into a classiﬁer. The experiment for each training ratio is executed 10 times
and the mean Macro-F1 scores are reported where the higher value indicates a better performance.

6

Table 1: AUC scores for link prediction on Cora.

% of edges
Deep Walk
LINE
node2vec
TADW
TriDNR
CENE
CANE
DMTE (w/o diffusion)
DMTE (text only)
DMTE (Bi-LSTM)
DMTE (WAvg)

15% 25% 35% 45% 55% 65% 75% 85% 95%
90.3
56.0
89.3
55.0
88.2
55.9
86.6
92.7
93.7
85.9
95.9
72.1
97.7
86.8
96.7
87.4
82.6
94.2
98.1
86.3
91.3
98.8

80.1
77.6
78.7
90.0
91.3
89.4
94.6
93.9
89.1
94.1
96.0

85.3
85.6
85.9
91.0
93.0
93.9
95.6
95.5
92.0
96.0
97.4

87.8
88.4
87.3
93.4
93.6
95.0
96.6
95.9
92.9
97.3
98.2

70.2
66.4
66.1
90.2
90.5
84.6
92.2
92.0
85.7
90.7
93.7

63.0
58.6
62.4
88.2
88.6
86.5
91.5
91.2
84.0
88.2
93.1

85.2
82.8
81.6
93.0
92.4
89.2
94.9
94.6
91.1
94.8
97.1

75.5
73.0
75.0
90.8
91.2
88.1
93.9
93.2
87.3
92.7
95.0

Table 2: AUC scores for link prediction on Zhihu.

% of edges
Deep Walk
LINE
node2vec
TADW
TriDNR
CENE
CANE
DMTE (w/o diffusion)
DMTE (text only)
DMTE (Bi-LSTM)
DMTE (WAvg)

15% 25% 35% 45% 55% 65% 75% 85% 95%
67.8
56.6
71.1
52.3
54.2
68.5
69.0
52.3
70.3
53.8
73.8
56.2
75.4
56.8
56.2
75.1
74.1
55.9
82.2
56.3
58.4
81.5

61.8
64.3
58.7
60.8
63.0
66.3
68.9
68.5
65.3
73.2
74.0

58.1
55.9
57.1
54.2
55.7
57.4
59.3
58.4
57.2
60.3
63.2

60.1
59.9
57.3
55.6
57.9
60.3
62.9
61.3
58.8
64.9
67.5

60.0
60.9
58.3
57.3
59.5
63.0
64.5
64.0
61.6
69.8
71.6

61.9
66.0
62.5
62.4
64.6
66.0
70.4
69.7
67.6
76.4
76.7

63.7
69.3
67.6
63.8
67.5
69.8
73.6
73.3
71.0
80.3
79.8

63.3
67.7
66.2
65.2
66.0
70.2
71.4
71.5
69.5
78.7
78.5

We set the embedding of dimension d to 200 with ds and dt both equal to 100. The number of hops
H is set to 4 and the importance coefﬁcients λh’s are tuned for different datasets and different tasks
with λ0 > λ1 > ··· > λH. αtt  αss  αts  and αst are set to 1  1  0.3 and 0.3 respectively. The
number of negative samples K is set to 1 to speed up the training process. The word embedding
matrix Ew  the structure embedding table Es and the diffusion weight matrix W are all randomly
initialized with a truncated Gaussian distribution. All models are implemented in Tensorﬂow using a
NVIDIA Titan X GPU with 12 GB memory.

5.1 Link Prediction

Given a pair of vertices  link prediction
seeks to predict the existence of an unob-
served edge using the trained representa-
tions. We use Cora and Zhihu datasets for
link prediction. We randomly hold out a
portion of edges (%e) for training in an un-
supervised way with the rest of edges for
testing.
Tables 1 and 2 show the AUC scores of dif-
ferent models for %e from 15% to 95% on
Cora and Zhihu. The best performance is
highlighted in bold. As can be seen from
both tables  our proposed method performs
better than all other baseline methods. The
AUC gains of DMTE model over the state-
of-the-art CANE model can be as much as
4.5 and 6.8 on Cora and Zhihu respectively.
These results demonstrate the effectiveness
of the learned embeddings using the pro-
posed method on link prediction task. We observe that baselines incorporating both structure and text

Figure 4: Performance over H.

7

H=1H=2H=3H=4H=5H=60.870.880.890.90.910.92AUC15%H=1H=2H=3H=4H=5H=60.920.9250.930.9350.94AUC35%H=1H=2H=3H=4H=5H=60.9350.940.9450.950.9550.960.965AUC55%H=1H=2H=3H=4H=5H=60.9550.960.9650.970.9750.98AUC75%Table 3: Top-5 similar vertex search based on embeddings learned by DMTE.

Query: The K-D-B-Tree: A Search Structure For Large Multidimensional Dynamic Indexes.
1. The R+-Tree: A Dynamic Index for Multi-Dimensional Objects.
2. The SR-tree: An Index Structure for High-Dimensional Nearest Neighbor Queries.
3. Segment Indexes: Dynamic Indexing Techniques for Multi-Dimensional Interval Data.
4. Generalized Search Trees for Database Systems.
5. High Performance Clustering Based on the Similarity Join.

information perform better than those only utilizes structure information  which indicates that text
associated with each vertex helps to achieve more informative embeddings. The proposed approach
shows ﬂexibility and robustness in various training ratios. As the portion of training edges gets larger 
the performance of our DMTE model steadily increases while other approaches suffer under either
low training ratio (such as CENE) or high training ratio (such as TADW).
Comparing the four versions of DMTE  DMTE with word embedding average as the text inputs has
the best performance on Cora at all training ratios and on Zhihu at low training ratios  while DMTE
with bidirectional LSTM as the text inputs has the best performance on Zhihu at high training ratios.
This is because when the training data is limited  the model with less parameters can successfully
avoid over-ﬁtting and thus achieve better results. For larger networks like Zhihu with high training
data ratios  deep models (such as Bi-LSTM) with more parameters can be a good choice to encode
input texts. The model with the diffusion convolutional operation applied on text inputs performs
better than the model without the diffusion process  verifying our assumption that the diffusion process
can help include long-distance semantic relationship and thus achieves better embeddings. We also
observe that DMTE with text embeddings only performs better than some baseline methods but
worse than the other three DMTE variations  demonstrating the effectiveness of text embeddings and
the necessity of adding structure embeddings. Furthermore  DMTE with only the word-embedding
average as the text representation has comparable performance over baselines  demonstrating the
effectiveness of the redesigned objective function  which calculates the conditional probability of
generating vi given the diffusion map of vj.

Parameter Sensitivity Figure 4 shows the link prediction results w.r.t. the number of hops H
at different training ratios. The model we use here is DMTE(WAvg). Note that when H = 1 the
model is equivalent to DMTE without diffusion precess. As H gets larger  the performance of DMTE
increases initially then stops increasing when H is big enough. This observation indicates that the
diffusion process can help exploit the relatedness of any two vertices in the graph  however this
relatedness is neglectable when the distance between two vertices is too long.

5.2 Multi-Label Classiﬁcation

Multi-label classiﬁcation seeks to classify each ver-
tex into a set of labels using the learned vertex
representation as features. We use DBLP dataset
for multi-label classiﬁcation. Here DMTE refers to
DMTE(WAvg). To maximally reduce the impact of
complicated learning approaches on the classiﬁcation
performance  a linear SVM is employed instead of
a sophisticated deep classiﬁer. We randomly sam-
ple a portion of labeled vertices with embeddings
(%l = {10%  30%  50%  70%}) to train the classiﬁer
with the rest vertices for testing.
Figure 5 shows the AUC scores of different models on
DBLP. Compared to baselines  the proposed DMTE
model consistently achieves performance improve-
ment at all training ratios  demonstrating that DMTE
learns high-quality embeddings which can be used di-
rectly as features for multi-label vertex classiﬁcation.
The F1-Macro score gains of DMTE over baseline

8

Figure 5: F1-Macro scores for multi-label
classiﬁcation on DBLP.

10%30%50%70%Label Percentage0.30.40.50.60.70.80.9F1-Macro ScoreDeepWalkLINETADWTriDNRCANEDMTECANE indicates that the embeddings learned using global structure information is more informative
than only considering local pairwise proximity. We also observe that structure-based methods perform
much worse than methods based on structure and text combined  which further shows the importance
of integrating both structure and text information in textual network embeddings.

5.3 Case Study

To visualize the effectiveness of the learned embeddings  we retrieve the most similar vertices and
their corresponding texts for a given query vertex. The distance is evaluated by cosine similarity
based on the vectorial representations learned by DMTE. Table 3 shows the texts of the top 5 closest
vertex embeddings of a query paper in DBLP dataset. In the graph  vertices 1  2  4  and 5 are all
neighbors of the query while vertex 3 is not directly connected with the query vertex. As observed 
direct neighbors vertices 1 and 2 are not only structurally but also textually similar to the query vertex
with multiple words aligned such as tree  index and multi-dimensional. Although vertex 3 is not
on the same edge with the query vertex  the semantic relatedness makes it closer than the query’s
direct neighbors such as vertex 4 and 5. This is an illustration that the embeddings learned by DMTE
successfully incorporate both structure and text information  helping to explain the quality of the
aforementioned results.

6 Conclusions

We have proposed a new DMTE model for textual network embedding. Unlike existing embedding
methods  that neglect semantic relatedness between texts or only exploit local pairwise relationship 
the proposed method integrates global structural information of the graph to capture the level of
connectivity between any two texts  by applying a diffusion convolutional operation on the text
inputs. Furthermore  we designed a new objective that preserves high-order proximity  by including a
diffusion map in the conditional probability. We conducted experiments on three real-word networks
for multi-label classiﬁcation and link prediction  and the associated results demonstrate the superiority
of the proposed DMTE model.

Acknowledgments
The authors would like to thank the anonymous reviewers for their insightful comments. This research
was supported in part by DARPA  DOE  NIH  ONR and NSF.

References
[1] E. Abrahamson and L. Rosenkopf. Social network effects on the extent of innovation diffusion:

A computer simulation. Organization science  1997.

[2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In NIPS  2016.

[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and

clustering. In Advances in neural information processing systems  2002.

[4] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent dirichlet allocation. Journal of machine Learning

research  2003.

[5] S. Cao  W. Lu  and Q. Xu. Grarep: Learning graph representations with global structural
information. In Proceedings of the 24th ACM International on Conference on Information and
Knowledge Management. ACM  2015.

[6] Z. Gan  Y. Pu  R. Henao  C. Li  X. He  and L. Carin. Learning generic sentence representations
using convolutional neural networks. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing  2017.

[7] A. Graves  N. Jaitly  and A.-r. Mohamed. Hybrid speech recognition with deep bidirectional
lstm. In Automatic Speech Recognition and Understanding (ASRU)  2013 IEEE Workshop on.
IEEE  2013.

9

[8] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining.
ACM  2016.

[9] M. Iyyer  V. Manjunatha  J. Boyd-Graber  and H. Daumé III. Deep unordered composition
rivals syntactic methods for text classiﬁcation. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)  volume 1  2015.

[10] N. Kalchbrenner  E. Grefenstette  and P. Blunsom. A convolutional neural network for modelling

sentences. arXiv preprint arXiv:1404.2188  2014.

[11] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[12] R. Kiros  Y. Zhu  R. R. Salakhutdinov  R. Zemel  R. Urtasun  A. Torralba  and S. Fidler.

Skip-thought vectors. In Advances in neural information processing systems  2015.

[13] Q. Le and T. Mikolov. Distributed representations of sentences and documents. In International

Conference on Machine Learning  2014.

[14] L. Lü and T. Zhou. Link prediction in complex networks: A survey. Physica A: statistical

mechanics and its applications  2011.

[15] A. K. McCallum  K. Nigam  J. Rennie  and K. Seymore. Automating the construction of internet

portals with machine learning. Information Retrieval  2000.

[16] T. Mikolov  K. Chen  G. Corrado  and J. Dean. Efﬁcient estimation of word representations in

vector space. arXiv preprint arXiv:1301.3781  2013.

[17] T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations of
words and phrases and their compositionality. In Advances in neural information processing
systems  2013.

[18] J. Mitchell and M. Lapata. Composition in distributional models of semantics. Cognitive

science  2010.

[19] S. Pan  J. Wu  X. Zhu  C. Zhang  and Y. Wang. Tri-party deep network representation. Network 

2016.

[20] B. Perozzi  R. Al-Rfou  and S. Skiena. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining. ACM  2014.

[21] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.

science  2000.

[22] P. Sen  G. Namata  M. Bilgic  L. Getoor  B. Galligher  and T. Eliassi-Rad. Collective classiﬁca-

tion in network data. AI magazine  2008.

[23] D. Shen  G. Wang  W. Wang  M. Renqiang Min  Q. Su  Y. Zhang  C. Li  R. Henao  and L. Carin.
Baseline needs more love: On simple word-embedding-based models and associated pooling
mechanisms. In ACL  2018.

[24] D. Shen  X. Zhang  R. Henao  and L. Carin. Improved semantic-aware network embedding

with ﬁne-grained word alignment. arXiv preprint arXiv:1808.09633  2018.

[25] R. Socher  A. Perelygin  J. Wu  J. Chuang  C. D. Manning  A. Ng  and C. Potts. Recursive deep
models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013
conference on empirical methods in natural language processing  2013.

[26] X. Sun  J. Guo  X. Ding  and T. Liu. A general framework for content-enhanced network

representation learning. arXiv preprint arXiv:1610.02906  2016.

10

[27] J. Tang  M. Qu  M. Wang  M. Zhang  J. Yan  and Q. Mei. Line: Large-scale information
network embedding. In Proceedings of the 24th International Conference on World Wide Web.
International World Wide Web Conferences Steering Committee  2015.

[28] J. Tang  J. Zhang  L. Yao  J. Li  L. Zhang  and Z. Su. Arnetminer: extraction and mining of
academic social networks. In Proceedings of the 14th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM  2008.

[29] J. B. Tenenbaum  V. De Silva  and J. C. Langford. A global geometric framework for nonlinear

dimensionality reduction. science  2000.

[30] C. Tu  H. Liu  Z. Liu  and M. Sun. Cane: Context-aware network embedding for relation
modeling. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)  volume 1  2017.

[31] C. Tu  Z. Liu  and M. Sun. Inferring correspondences from multiple sources for microblog user

tags. In Chinese National Conference on Social Media Processing. Springer  2014.

[32] D. Wang  P. Cui  and W. Zhu. Structural deep network embedding. In Proceedings of the 22nd
ACM SIGKDD international conference on Knowledge discovery and data mining. ACM  2016.

[33] C. Yang  Z. Liu  D. Zhao  M. Sun  and E. Y. Chang. Network representation learning with rich

text information. In IJCAI  2015.

[34] X. Zhang  R. Henao  Z. Gan  Y. Li  and L. Carin. Multi-label learning from medical plain text

with convolutional residual models. arXiv preprint arXiv:1801.05062  2018.

11

,Xinyuan Zhang
Yitong Li
Dinghan Shen
Lawrence Carin