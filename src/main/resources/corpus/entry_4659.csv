2016,Regularized Nonlinear Acceleration,We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system  whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm  providing improved estimates of the solution on the fly  while the original optimization method is running. Numerical experiments are detailed on classical classification problems.,Regularized Nonlinear Acceleration

Damien Scieur

INRIA & D.I.  UMR 8548 

Alexandre d’Aspremont
CNRS & D.I.  UMR 8548 

École Normale Supérieure  Paris  France.

damien.scieur@inria.fr

École Normale Supérieure  Paris  France.

aspremon@di.ens.fr

Francis Bach

INRIA & D.I.  UMR 8548 

École Normale Supérieure  Paris  France.

francis.bach@inria.fr

Abstract

We describe a convergence acceleration technique for generic optimization prob-
lems. Our scheme computes estimates of the optimum from a nonlinear average
of the iterates produced by any optimization method. The weights in this average
are computed via a simple and small linear system  whose solution can be updated
online. This acceleration scheme runs in parallel to the base algorithm  provid-
ing improved estimates of the solution on the ﬂy  while the original optimization
method is running. Numerical experiments are detailed on classical classiﬁcation
problems.

1

Introduction

Suppose we want to solve the following optimization problem

(1)
in the variable x ∈ Rn  where f (x) is strongly convex with respect to the Euclidean norm with
parameter µ  and has a Lipschitz continuous gradient with parameter L with respect to the same norm.
This class of function is often encountered  for example in regression where f (x) is of the form

min
x∈Rn

f (x)

f (x) = L(x) + Ω(x) 

where L(x) is a smooth convex loss function and Ω(x) is a smooth strongly convex penalty function.
Assume we solve this problem using an iterative algorithm of the form

xi+1 = g(xi) 

for i = 1  ...  k 

(2)
where xi ∈ Rn and k the number of iterations. Here  we will focus on the problem of estimating the
solution to (1) by tracking only the sequence of iterates xi produced by an optimization algorithm.
This will lead to an acceleration of the speed of convergence  since we will be able to extrapolate
more accurate solutions without any calls to the oracle g(x).
Since the publication of Nesterov’s optimal ﬁrst-order smooth convex minimization algorithm [1]  a
signiﬁcant effort has been focused on either providing more explicit interpretable views on current
acceleration techniques  or on replicating these complexity gains using different  more intuitive
schemes. Early efforts were focused on directly extending the original acceleration result in [1] to
broader function classes [2]  allow for generic metrics  line searches or simpler proofs [5  6]  produce
adaptive accelerated algorithms [7]  etc. More recently however  several authors [8  9] have started

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

using classical results from control theory to obtain numerical bounds on convergence rates that
match the optimal rates. Others have studied the second order ODEs obtained as the limit for small
step sizes of classical accelerated schemes  to better understand their convergence [10  11]. Finally 
recent results have also shown how to wrap classical algorithms in an outer optimization loop  to
accelerate convergence [12] and reach optimal complexity bounds.
Here  we take a signiﬁcantly different approach to convergence acceleration stemming from classical
results in numerical analysis. We use the iterates produced by any (converging) optimization algorithm 
and estimate the solution directly from this sequence  assuming only some regularity conditions on the
function to minimize. Our scheme is based on the idea behind Aitken’s ∆2 algorithm [13]  generalized
as the Shanks transform [14]  whose recursive formulation is known as the ε-algorithm [15] (see e.g.
[16  17] for a survey). In a nutshell  these methods ﬁt geometrical models to linearly converging
sequences  then extrapolate their limit from the ﬁtted model.
In a sense  this approach is more statistical in nature. It assumes an approximately linear model
holds for iterations near the optimum  and estimates this model using the iterates. In fact  Wynn’s
algorithm [15] is directly connected to the Levinson-Durbin algorithm [18  19] used to solve Toeplitz
systems recursively and ﬁt autoregressive models (the Shanks transform solves Hankel systems  but
this is essentially the same problem [20]). The key difference here is that estimating the autocovariance
operator is not required  as we only focus on the limit. Moreover  the method presents strong links
with the conjugate gradient when applied to unconstrained quadratic optimization.
We start from a slightly different formulation of these techniques known as minimal polynomial
extrapolation (MPE) [17  21] which uses the minimal polynomial of the linear operator driving
iterations to estimate the optimum by nonlinear averaging (i.e.  using weights in the average which are
nonlinear functions of the iterates). So far  for all the techniques cited above  no proofs of convergence
of these estimates were given in the case where the iterates made the estimation process unstable.
Our contribution here is to add a regularization in order to produce explicit bounds on the distance to
optimality by controlling the stability through the regularization parameter  thus explicitly quanti-
fying the acceleration provided by these techniques. We show in several numerical examples that
these stabilized estimates often speed up convergence by an order of magnitude. Furthermore this
acceleration scheme thus runs in parallel to the original algorithm  providing improved estimates of
the solution on the ﬂy  while the original method is progressing.
The paper is organized as follows. In section 2.1 we recall basic results behind MPE for linear
iterations and we will introduce in section 2.2 a formulation of the approximate version of MPE and
make a link with the conjugate gradient method. Then  in section 2.3  we generalize these results to
generic nonlinear iterations and show  in section 2.4  how to fully control the impact of nonlinearity.
We use these results to derive explicit bounds on the acceleration performance of our estimates.

2 Approximate Minimal Polynomial Extrapolation

In what follows  we recall the key arguments behind minimal polynomial extrapolation (MPE) as
derived in [22] or also [21]. We also explain a variant called approximate minimal polynomial
extrapolation (AMPE) which allows to control the number of iterates used in the extrapolation  hence
reduces its computational complexity. We begin by a simple description of the method for linear
iterations  then extend these results to the generic nonlinear case. Finally  we fully characterize the
acceleration factor provided by a regularized version of AMPE  using regularity properties of the
function f (x)  and the result of a Chebyshev-like  tractable polynomial optimization problem.

2.1 Linear Iterations

Here  we assume that the iterative algorithm in (2) is in fact linear  with

xi = A(xi−1 − x∗) + x∗ 

(3)
where A ∈ Rn×n (not necessarily symmetric) and x∗ ∈ Rn. We assume that 1 is not an eigenvalue
of A  implying that (3) admits a unique ﬁxed point x∗. Moreover  if we assume that (cid:107)A(cid:107)2 < 1  then
xk converge to x∗ at rate (cid:107)xk − x∗(cid:107)2 ≤ (cid:107)A(cid:107)k
2(cid:107)x0 − x∗(cid:107). We now recall the minimal polynomial
extrapolation (MPE) method as described in [21]  starting with the following deﬁnition.

2

Deﬁnition 2.1 Given A ∈ Rn×n  s.t. 1 is not an eigenvalue of A and v ∈ Rn  the minimal polynomial
of A with respect to the vector v is the lowest degree polynomial p(x) such that

p(A)v = 0 

p(1) = 1.

Note that the degree of p(x) is always less than n and the condition p(1) = 1 makes p unique. Notice
that because we assumed that 1 is not an eigenvalue of A  having p(1) = 1 is not restrictive since
we can normalize each minimal polynomial with the sum of its coefﬁcients (see Lemma A.1 in
the supplementary material). Given an initial iterate x0  MPE starts by forming a matrix U whose
columns are the increments xi+1 − xi  with

ui = xi+1 − xi = (A − I)(xi − x∗) = (A − I)Ai(x0 − x∗).

(4)
Now  let p be the minimal polynomial of A with respect to the vector u0 (where p has coefﬁcients ci
and degree d)  and U = [u0  u1  ...  ud]. So

p(1) =(cid:80)d

computed exactly as follows

(cid:80)d
i=0 ciui =(cid:80)d
We can thus solve the system U c = 0 (cid:80)
i=0 ciAiu0 = (cid:80)d
0 =(cid:80)d
= (A − I)(cid:80)d
(A − I)(cid:80)d
i=0 ci(xi − x∗) = 0 ⇔ (cid:80)d

i=0 ciAiu0 = p(A)u0 = 0  

(5)
i ci = 1 to ﬁnd p. In this case  the ﬁxed point x∗ can be

i=0 ci = 1.

i=0 ciAi(A − I)(x0 − x∗)

i=0 ciAi(x0 − x∗) = (A − I)(cid:80)d
i=0 ci(xi − x∗) = 0 ⇔ (cid:80)d

i=0 ci(xi − x∗).

i=0 cixi = x∗.

Hence  using the fact that 1 is not an eigenvalue of A and p(1) = 1 

This means that x∗ is obtained by averaging iterates using the coefﬁcients in c. The averaging in this
case is called nonlinear  since the coefﬁcients of c vary with the iterates themselves.

2.2 Approximate Minimal Polynomial Extrapolation (AMPE)

Suppose now that we only compute a fraction of the iterates xi used in the MPE procedure. While the
number of iterates k might be smaller than the degree of the minimal polynomial of A with respect
to u0  we can still try to make the quantity pk(A)u0 small  where pk(x) is now a polynomial of degree
at most k. The corresponding difference matrix U = [u0  u1  ...  uk] ∈ Rn×(k+1) is rectangular.
This is also known as the Eddy-Mešina method [3  4] or reduced rank extrapolation with arbitrary k
(see [21  §10]). The objective here is similar to (5)  but the system is now overdetermined because
k < deg(P ). We will thus choose c to make (cid:107)U c(cid:107)2 = (cid:107)p(A)u0(cid:107)2  for some polynomial p such that
p(1) = 1  as small as possible  which means solving for

c∗ (cid:44) argmin (cid:107)U c(cid:107)2

(AMPE)
in the variable c ∈ Rk+1. The optimal value (cid:107)U c∗(cid:107)2 of this problem is decreasing with k  satis-
ﬁes (cid:107)U c∗(cid:107)2 = 0 when k is greater than the degree of the minimal polynomial  and controls the
approximation error in x∗ with equation (4). Setting ui = (A − I)(xi − x∗)  we have

s.t. 1T c = 1

(cid:107)(cid:80)k

i=0 c∗

i xi − x∗(cid:107)2 = (cid:107)(I − A)−1(cid:80)k

≤ (cid:13)(cid:13)(I − A)−1(cid:13)(cid:13)2 (cid:107)U c∗(cid:107)2.

i=0 c∗

i ui(cid:107)2

We can get a crude bound on (cid:107)U c∗(cid:107)2 from Chebyshev polynomials  using only an assumption on
the range of the spectrum of the matrix A. Assume A symmetric  0 (cid:22) A (cid:22) σI ≺ I and deg(p) ≤ k.
Indeed 

(cid:107)U c∗(cid:107)2 = (cid:107)p∗(A)u0(cid:107)2 ≤ (cid:107)u0(cid:107)2 min

p:p(1)=1

(cid:107)p(A)(cid:107)2 ≤ (cid:107)u0(cid:107)2 min

p:p(1)=1

max

A:0(cid:22)A(cid:22)σI

(cid:107)p(A)(cid:107)2 

(6)

where p∗ is the polynomial with coefﬁcients c∗. Since A is symmetric  we have A = QΛQT where
Q is unitary. We can thus simplify the objective function:
(cid:107)p(Λ)(cid:107)2 = max

(cid:107)p(A)(cid:107)2 = max

|p(λ)|.

max

|p(λi)| = max
λ:0≤λ≤σ

max

A:0(cid:22)A(cid:22)σI

Λ:0(cid:22)Λ(cid:22)σI

i

Λ:0(cid:22)Λ(cid:22)σI

3

We thus have

(cid:107)U c∗(cid:107)2 ≤ (cid:107)u0(cid:107)2 min

p:p(1)=1

|p(λ)|.

max

λ:0≤λ≤σ

So we must ﬁnd a polynomial which takes small values in the interval [0  σ]. However  Chebyshev
polynomials are known to be polynomials for which the maximal value in the interval [0  1] is
the smallest. Let Ck be the Chebyshev polynomial of degree k. By deﬁnition  Ck(x) is a monic
polynomial1 which solves

Ck(x) = argmin
p:p is monic

max

x:x∈[−1 1]

|p(x)|.

We can thus use a variant of Ck(x) in order to solve the minimax problem

min

p:p(1)=1

max

λ:0≤λ≤σ

|p(λ)|.

The solution of this problem is given in [23] and admits an explicit formulation:

T (x) =

Ck(t(x))
Ck(t(1))

 

t(x) =

2x − σ

.

σ

Note that t(x) is simply a linear mapping from interval [0  σ] to [−1  1]. Moreover 

min

p:p(1)=1

max

λ:0≤λ≤σ

|p(λ)| = max
λ:0≤λ≤σ

|Tk(λ)| = |Tk(σ)| =

2ζ k
1 + ζ 2k  

where ζ is

ζ = (1 − √

1 − σ)/(1 +

√

1 − σ) < σ.

Since (cid:107)u0(cid:107)2 = (cid:107)(A − I)(x0 − x∗)(cid:107)2 ≤ (cid:107)A − I(cid:107)2(cid:107)x0 − x∗(cid:107)  we can bound (6) by

(cid:107)U c∗(cid:107)2 ≤ (cid:107)u0(cid:107)2 min

p:p(1)=1

max

λ:0≤λ≤σ

|p(λ)| ≤ (cid:107)A − I(cid:107)2

2ζ k

1 + ζ 2k (cid:107)x0 − x∗(cid:107)2.

(7)

(8)

(9)

This leads to the following proposition.
Proposition 2.2 Let A be symmetric  0 (cid:22) A (cid:22) σI ≺ I and ci be the solution of (AMPE). Then

≤ κ(A − I) 2ζk

1+ζ2k (cid:107)x0 − x∗(cid:107)2 

(10)

(cid:13)(cid:13)(cid:13)(cid:80)k

i=0 c∗

i xi − x∗(cid:13)(cid:13)(cid:13)2

√

where κ(A − I) is the condition number of the matrix A − I and ζ is deﬁned in (9).
Note that  when solving quadratic optimization problems  the rate in this bound matches that obtained
using the optimal method in [6]. Also  the bound on the rate of convergence of this method is exactly
the one of the conjugate gradient with an additional factor κ(A − I).

This method presents a strong link with the conjugate gradient. Denote (cid:107)v(cid:107)B =

Remark:
vT Bv
the norm induced by the deﬁnite positive matrix B. By deﬁnition  at the k-th iteration  the conjugate
gradient computes an approximation s of x∗ which follows

s = argmin(cid:107)x − x∗(cid:107)A s.t. x ∈ Kk  

is a linear combination of the element in Kk  so x = (cid:80)k

where Kk = {Ax∗  A2x∗  ...  Akx∗} is called a Krylov subspace. Since x ∈ Kk  we have that x
i=1 ciAix∗ = q(A)x∗  where q(x) is a

polynomial of degree k and q(0) = 0. So conjugate gradient solves

s = argminq:q(0)=0 (cid:107)q(A)x∗ − x∗(cid:107)A = argminˆq:ˆq(0)=0 (cid:107)ˆq(A)x∗(cid:107)A 

which is very similar to equation (AMPE). However  while conjugate gradient has access to an oracle
which gives the result of the product between matrix A and any vector v  the AMPE procedure can
only use the iterations produced by (3) (meaning that the AMPE procedure does not need to know A).
Moreover  we analyze the convergence of AMPE in another norm ((cid:107) · (cid:107)2 instead of (cid:107) · (cid:107)A). These
two reasons explain why a condition number appears in the rate of convergence of AMPE (10).

1A monic polynomial is a univariate polynomial in which the coefﬁcient of highest degree is equal to 1.

4

2.3 Nonlinear Iterations

We now go back to the general case where the iterative algorithm is nonlinear  with

˜xi+1 = g(˜xi) 

for i = 1  ...  k 

(11)
where ˜xi ∈ Rn and the function g has a symmetric Jacobian at point x∗. We also assume that the
method has a unique ﬁxed point written x∗ and linearize these iterations around x∗  to get

˜xi − x∗ = A(˜xi−1 − x∗) + ei 
(12)
where A is now the Jacobian matrix (i.e.  the ﬁrst derivative) of g taken at the ﬁxed point x∗ and
ei ∈ Rn is a second order error term (cid:107)ei(cid:107)2 = O((cid:107)˜xi−1 − x∗(cid:107)2
2). Note that  by construction  the
linear and nonlinear models share the same ﬁxed point x∗. We write xi the iterates that would be
obtained using the asymptotic linear model (starting at x0)

xi − x∗ = A(xi−1 − x∗).

Running the algorithm described in (11)  we thus observe the iterates ˜xi and build ˜U from their
differences. As in (AMPE) we then compute ˜c using matrix ˜U and ﬁnally estimate

i=0 ˜ci ˜xi.

˜x∗ =(cid:80)k
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:80)k

+

In this case  our estimate for x∗ is based on the coefﬁcient ˜c  computed using the iterates ˜xi. We will
now decompose the error made by the estimation by comparing it with the estimation which comes
from the linear model:

(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:80)k
i=0 cixi − x∗(cid:13)(cid:13)(cid:13)2

. (13)

(cid:13)(cid:13)(cid:13)(cid:80)k
i=0 ˜ci ˜xi − x∗(cid:13)(cid:13)(cid:13)2

≤(cid:13)(cid:13)(cid:13)(cid:80)k

i=0(˜ci − ci)xi

i=0 ˜ci(˜xi − xi)

+

This expression shows us that the precision is comparable to the precision of the AMPE process in
the linear case (third term) with some perturbation. Also  if (cid:107)ei(cid:107)2 is small then (cid:107)xi − ˜xi(cid:107)2 is small
as well. But we need more information about (cid:107)c(cid:107)2 and (cid:107)˜c − c(cid:107)2 if we want to go further.
We now show the following proposition computing the perturbation ∆c = (˜c∗ − c∗) of the optimal
solution of (AMPE)  c∗  induced by E = ˜U − U. It will allow us to bound the ﬁrst term on the
right-hand side of (13) (see proof A.2 in the Appendix). For simplicity  we will use P = ˜U T ˜U−U T U.
Proposition 2.3 Let c∗ be the optimal solution to (AMPE)
(cid:107)U c(cid:107)2

c∗ = argmin

for some matrix U ∈ Rn×k. Suppose U becomes ˜U = U + E and write c∗ + ∆c the perturbed
solution to (AMPE). Let M = ˜U T ˜U and the perturbation matrix P = ˜U T ˜U − U T U. Then 

∆c = −

M−1P c∗.

(14)

1T c=1

(cid:18)
I − M−111T
1T M−11

(cid:19)

We see here that the perturbation can be potentially large. Even if (cid:107)c∗(cid:107)2 and (cid:107)P(cid:107)2 can be potentially
small  (cid:107)M−1(cid:107)2 is huge in general. It can be shown that U T U (the square of a Krylov-like matrix)
presents an exponential condition number (see [24]) because the minimal eigenvalue decays very fast.
Moreover  the eigenvalues are perturbed by P   leading to a potential huge perturbation ∆c  especially
if (cid:107)P(cid:107)2 is comparable (or bigger) to λmin(U T U ).

2.4 Regularized AMPE

The condition number of the matrix U T U in problem (AMPE) can be arbitrary large. Indeed  this
condition number is related to the one of Krylov matrices which has been proved in [24] to be
exponential in k. By consequence  this conditioning problem coupled with nonlinear errors lead to
highly unstable solutions c∗ (which we observe in our experiments). We thus study a regularized
formulation of problem (AMPE)  which reads
minimize
subject to 1T c = 1

cT (U T U + λI)c

(RMPE)

The solution of this problem may be computed with a linear system  and the regularization parameter
controls the norm of the solution  as shown in the following Lemma (see proof A.3 in Appendix).

5

Lemma 2.4 Let c∗
λ be the optimal solution of problem (RMPE). Then
c∗
λ =

(U T U + λI)−11
1T (U T U + λI)−11

(cid:107)c∗
λ(cid:107)2 ≤

and

(cid:114)

λ + (cid:107)U(cid:107)2

2

kλ

.

(15)

This allows us to obtain the following corollary extending Proposition 2.3 to the regularized AMPE
problem in (RMPE)  showing that the perturbation of c is now controlled by the regulaization
parameter λ.
Corollary 2.5 Let c∗
problem (RMPE) for the perturbed matrix ˜U = U + E is given by c∗
(cid:107)∆c∗

λ  deﬁned in (15)  be the solution of problem (RMPE). Then the solution of

(cid:16)
λ = −M−1
λ W T P c∗
I − M
where Mλ = (U T U + P + λI) and W =

∆cλ = −W M−1

λ + ∆cλ where
λ(cid:107)2 ≤ (cid:107)P(cid:107)2
λ(cid:107)2 
λ (cid:107)c∗
is a projector of rank k − 1.

λ P c∗

(cid:17)

and

λ
−1
λ 11T
−1
λ 1

1T M

These results lead us to the following simple algorithm.

Algorithm 1 Regularized Approximate Minimal Polynomial Extrapolation (RMPE)
Input: Sequence {x0  x1  ...  xk+1}  parameter λ > 0

Compute U = [x1 − x0  ...  xk+1 − xk]
Solve the linear system (U T U + λI)z = 1
Set c = z/(zT 1)

Output: (cid:80)k

i=0 cixi  the approximation of the ﬁxed point x∗

The computational complexity (with online updates or in batch mode) of the algorithm is O(nk2)
and some strategies (batch and online) are discussed in the Appendix A.3. Note that the algorithm
never calls the oracle g(x). It means that  in an optimization context  the acceleration does not require
f (x) or f(cid:48)(x) to compute the extrapolation. Moreover  it does not need a priori information on the
function  for example L and µ (unlike Nesterov’s method).

2.5 Convergence Bounds on Regularized AMPE

on the right-hand side of (13)  namely (cid:107)(cid:80)k

To fully characterize the convergence of our estimate sequence  we still need to bound the last term
i=0 cixi − x∗(cid:107)2. A coarse bound can be provided using
Chebyshev polynomials  however the norm of the Chebyshev’s coefﬁcients grows exponentially as k
grows. Here we reﬁne this bound to better control the quality of our estimate.
(cid:26)
Let g(x∗) (cid:22) σI. Consider the following Chebyshev-like optimization problem  written

{q∈Rk[x]: q(1)=1}

(16)
where Rk[x] is the ring of polynomials of degree at most k and q ∈ Rk+1 is the vector of coefﬁcients
of the polynomial q(x). This problem can be solved exactly using a semi-deﬁnite solver because it
can be reduced to a SDP program (see Appendix A.4 for the details of the reduction). Our main result
below shows how S(k  α) bounds the error between our estimate of the optimum constructed using
the iterates ˜xi in (RMPE) and the optimum x∗ of problem (1).
Proposition 2.6 Let matrices X = [x0  x1  ...  xk]  ˜X = [x0  ˜x1  ...  ˜xk]  E = (X − ˜X) and scalar
κ = (cid:107)(A − I)−1(cid:107)2. Suppose ˜c∗

((1 − x)q(x))2 + α(cid:107)q(cid:107)2

S(k  α) (cid:44)

max
x∈[0 σ]

(cid:27)

min

2

 

minimize
subject to 1T c = 1

(17)
in the variable c ∈ Rk+1  with parameters ˜U ∈ Rn×(k+1). Assume A symmetric with 0 (cid:22) A ≺ I.
Then
(cid:107) ˜X ˜c∗

(cid:19)2(cid:33)1
2(cid:0)S(k  λ/(cid:107)x0 − x∗(cid:107)2
2)(cid:1) 1

λ−x∗(cid:107)2 ≤

2(cid:107)x0−x∗(cid:107)2 

(cid:19)2(cid:18)

(cid:107)E(cid:107)2 + κ

(cid:32)

(cid:18)

κ2 +

λ =

1 +

( ˜U T ˜U + λI)−11
1T ( ˜U T ˜U + λI)−11

λ solves problem (RMPE)
cT ( ˜U T ˜U + λI)c
⇒ ˜c∗

1
λ

(cid:107)P(cid:107)2
λ

(cid:107)P(cid:107)2
√
λ
2

with P is deﬁned in Corollary 2.5 and S(k  α) is deﬁned in (16).

6

We have that S(k  λ/(cid:107)x0 − x∗(cid:107)2
2 is similar to the value Tk(σ) (see (8)) so our algorithm achieves a
2) 1
rate similar to the Chebyshev’s acceleration up to some multiplicative scalar. We thus need to choose
λ so that this multiplicative scalar is not too high (while keeping S(k  λ/(cid:107)x0 − x∗(cid:107)2
2) 1
We can analyze the behavior of the bound if we start close to the optimum. Assume

2 small).

(cid:107)E(cid:107)2 = O((cid:107)x0 − x∗(cid:107)2
2) 

(cid:107)U(cid:107)2 = O((cid:107)x0 − x∗(cid:107)2) ⇒ (cid:107)P(cid:107)2 = O((cid:107)x0 − x∗(cid:107)3
2).

This case is encountered when minimizing a smooth strongly convex function with Lipchitz-
continuous Hessian using ﬁxed-step gradient method (this case is discussed in details in the Appendix 
section A.6). Also  let λ = β(cid:107)P(cid:107)2 for β > 0 and (cid:107)x0 − x∗(cid:107) small. We can thus approximate the
right parenthesis of the bound by
(cid:107)E(cid:107)2 + κ

κ(cid:112)(cid:107)P(cid:107)2

(cid:112)(cid:107)P(cid:107)2

(cid:107)E(cid:107)2 + κ

(cid:33)

(cid:18)

(cid:19)

√

=

=

.

lim(cid:107)x−x∗(cid:107)2→0

lim(cid:107)x−x∗(cid:107)2→0

2

β

√
2

β

Therefore  the bound on the precision of the extrapolation is approximately equal to

(cid:107)P(cid:107)2
√
λ
2

(cid:32)

(cid:32)
(cid:33)1/2(cid:115)

(cid:18)

S

k 

(cid:19)

β(cid:107)P(cid:107)2

(cid:107)x0 − x∗(cid:107)2

2

(cid:107)x0 − x∗(cid:107)2

(cid:107) ˜X ˜c∗

λ − x∗(cid:107)2 (cid:46) κ

1 +

β )2

(1 + 1
4β2

Also  if we use equation (8)  it is easy to see that

(cid:112)S (k  0) ≤

{q∈Rk[x]: q(1)=1} max
x∈[0 σ1]

min

|q(x)| = Tk(t(σ)) =

2ζ k
1 + ζ 2k  

where ζ is deﬁned in (9). So  when (cid:107)x0 − x∗(cid:107)2 is close to zero  the regularized version of AMPE
tends to converge as fast as AMPE (see equation (10)) up to a small constant.

3 Numerical Experiments

We test our methods on a regularized logistic regression problem written
2(cid:107)w(cid:107)2
2 

i=1 log(cid:0)1 + exp(−yiξT

f (w) =(cid:80)m

i w)(cid:1) + τ

where Ξ = [ξ1  ...  ξm]T ∈ Rm×n is the design matrix and y is a {−1  1}n vector of labels. We used
the Madelon UCI dataset  setting τ = 102 (in order to have a ratio L/τ equal to 109). We solve this
problem using several algorithms  the ﬁxed-step gradient method for strongly convex functions [6 
Th. 2.1.15] using stepsize 2/(L + µ)  where L = (cid:107)Ξ(cid:107)2
2/4 + τ and µ = τ  the accelerated gradient
method for strongly convex functions [6  Th. 2.2.3] and our nonlinear acceleration of the gradient
method iterates using RMPE in Proposition 2.6 with restarts.
This last algorithm is implemented as follows. We do k steps (in the numerical experiments  k is
typically equal to 5) of the gradient method  then extrapolate a solution ˜X ˜c∗
λ is computed
by solving the RMPE problem (17) on the gradient iterates ˜X  with regularization parameter λ
determined by a grid search. Then  this extrapolation becomes the new starting point of the gradient
method. We consider it as one iteration of RMPEk using k gradient oracle calls. We also analyze the
impact of an inexact line-search (described in Appendix A.7) performed after this procedure.
The results are reported in Figure 1. Using very few iterates  the solution computed using our estimate
(a nonlinear average of the gradient iterates) are markedly better than those produced by the Nesterov-
accelerated method. This is only partially reﬂected by the theoretical bound from Proposition 2.6
which shows signiﬁcant speedup in some regions but remains highly conservative (see Figure 3 in
section A.6). Also  Figure 2 shows us the impact of regularization. The AMPE process becomes
unstable because of the condition number of matrix M  which impacts the precision of the estimate.

λ where ˜c∗

4 Conclusion and Perspectives

In this paper  we developed a method which is able to accelerate  under some regularity conditions 
the convergence of a sequence {xi} without any knowledge of the algorithm which generates this
sequence. The regularization parameter present in the acceleration method can be computed easily
using some inexact line-search.

7

Figure 1: Solving logistic regression on UCI Madelon dataset (500 features  2000 data points)
using the gradient method  Nesterov’s accelerated method and RMPE with k = 5 (with and without
line search over the stepsize)  with penalty parameter τ equal to 102 (Condition number is equal
to 1.2 · 109). Here  we see that our algorithm has a similar behavior to the conjugate gradient:
unlike the Nesterov’s method  where we need to provide parameters µ and L  the RMPE algorithm
adapts himself in function of the spectrum of g(x∗) (so it can exploit the good local strong convexity
parameter)  without any prior speciﬁcation. We can  for example  observe this behavior when the
global strong convexity parameter is bad but not the local one.

Figure 2: Logistic regression on Madelon UCI Dataset  solved using Gradient method  Nesterov’s
method and AMPE (i.e. RMPE with λ = 0). The condition number is equal to 1.2 · 109. We see that
without regularization  AMPE is unstable because (cid:107)( ˜U T ˜U )−1(cid:107)2 is huge (see Proposition 2.3).

The algorithm itself is simple. By solving only a small linear system we are able to compute a good
estimate of the limits of the sequence {xi}. Also  we showed (using the gradient method on logistic
regression) that the strategy which consists in alternating the algorithm and the extrapolation method
can lead to impressive results  improving signiﬁcantly the rate of convergence.
Future work will consist in improving the performance of the algorithm by exploiting the structure of
the noise matrix E in some cases (for example  using the gradient method  the norm of the column
Ek in the matrix E is decreasing when k grows)  extending the algorithm to the constrained case  the
stochastic case and to the non-symmetric case. We will also try to reﬁne the term (16) present in the
theoretical bound.

Acknowledgment. The research leading to these results has received funding from the European
Union’s Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under grant agreement no 607290
SpaRTaN  as well as support from ERC SIPA and the chaire Économie des nouvelles données with
the data science joint research initiative with the fonds AXA pour la recherche.

8

0246810×10410-5100f(xk)−f(x∗)GradientoraclecallsGradientNesterovNest.+backtrackRMPE5RMPE5+LS05001000150010-5100CPUTime(sec.)GradientNesterovNest.+backtrackRMPE5RMPE5+LS0246810x 10410−2100102 f(xk)−f(x∗)GradientoraclecallsGradientNesterovRMPE5AMPE5References
[1] Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). Soviet

Mathematics Doklady  27(2):372–376  1983.

[2] AS Nemirovskii and Y. E Nesterov. Optimal methods of smooth convex minimization. USSR Computational

Mathematics and Mathematical Physics  25(2):21–30  1985.

[3] Mešina  M. [1977]  ‘Convergence acceleration for the iterative solution of the equations x = ax + f’ 

Computer Methods in Applied Mechanics and Engineering 10(2)  165–173.

[4] Eddy  R. [1979]  ‘Extrapolating to the limit of a vector sequence’  Information linkage between applied

mathematics and industry pp. 387–396.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] Y. Nesterov. Introductory Lectures on Convex Optimization. Springer  2003.
[7] Y. Nesterov. Universal gradient methods for convex optimization problems. Mathematical Programming 

152(1-2):381–404  2015.

[8] Yoel Drori and Marc Teboulle. Performance of ﬁrst-order methods for smooth convex minimization: a

novel approach. Mathematical Programming  145(1-2):451–482  2014.

[9] Laurent Lessard  Benjamin Recht  and Andrew Packard. Analysis and design of optimization algorithms

via integral quadratic constraints. SIAM Journal on Optimization  26(1):57–95  2016.

[10] Weijie Su  Stephen Boyd  and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems 
pages 2510–2518  2014.

[11] Andre Wibisono and Ashia C Wilson. On accelerated methods in optimization. Technical report  2015.
[12] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization. In

Advances in Neural Information Processing Systems  pages 3366–3374  2015.

[13] Alexander Craig Aitken. On Bernoulli’s numerical solution of algebraic equations. Proceedings of the

Royal Society of Edinburgh  46:289–305  1927.

[14] Daniel Shanks. Non-linear transformations of divergent and slowly convergent sequences. Journal of

Mathematics and Physics  34(1):1–42  1955.

[15] Peter Wynn. On a device for computing the em(sn) transformation. Mathematical Tables and Other Aids

to Computation  10(54):91–96  1956.

[16] C Brezinski. Accélération de la convergence en analyse numérique. Lecture notes in mathematics  (584) 

1977.

[17] Avram Sidi  William F Ford  and David A Smith. Acceleration of convergence of vector sequences. SIAM

Journal on Numerical Analysis  23(1):178–196  1986.

[18] N Levinson. The Wiener RMS error criterion in ﬁlter design and prediction  appendix b of wiener  n.(1949).

Extrapolation  Interpolation  and Smoothing of Stationary Time Series  1949.

[19] James Durbin. The ﬁtting of time-series models. Revue de l’Institut International de Statistique  pages

233–244  1960.

[20] Georg Heinig and Karla Rost. Fast algorithms for Toeplitz and Hankel matrices. Linear Algebra and its

Applications  435(1):1–59  2011.

[21] David A Smith  William F Ford  and Avram Sidi. Extrapolation methods for vector sequences. SIAM

review  29(2):199–233  1987.

[22] Stan Cabay and LW Jackson. A polynomial extrapolation method for ﬁnding limits and antilimits of vector

sequences. SIAM Journal on Numerical Analysis  13(5):734–752  1976.

[23] Gene H Golub and Richard S Varga. Chebyshev semi-iterative methods  successive overrelaxation iterative
methods  and second order richardson iterative methods. Numerische Mathematik  3(1):157–168  1961.
[24] Evgenij E Tyrtyshnikov. How bad are Hankel matrices? Numerische Mathematik  67(2):261–269  1994.
[25] Y. Nesterov. Squared functional systems and optimization problems. In High performance optimization 

pages 405–440. Springer  2000.

[26] J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on

Optimization  11(3):796–817  2001.

[27] P. Parrilo. Structured Semideﬁnite Programs and Semialgebraic Geometry Methods in Robustness and

Optimization. PhD thesis  California Institute of Technology  2000.

[28] A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization : analysis  algorithms  and

engineering applications. MPS-SIAM series on optimization. SIAM  2001.

9

,Damien Scieur
Alexandre d'Aspremont
Francis Bach
Gengshan Yang
Deva Ramanan