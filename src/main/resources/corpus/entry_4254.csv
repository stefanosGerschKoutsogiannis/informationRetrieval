2007,People Tracking with the Laplacian Eigenmaps Latent Variable Model,Reliably recovering 3D human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions. We define priors for people tracking using a Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable models---definining a multimodal probability density for latent and observed variables  and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction---with those of spectral manifold learning methods---no local optima  ability to unfold highly nonlinear manifolds  and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient  simple to learn from sparse training data  and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM provides sufficient constraints for robust operation in the presence of missing  noisy and ambiguous image measurements.,People Tracking with the Laplacian Eigenmaps

Latent Variable Model

Zhengdong Lu

CSEE  OGI  OHSU
zhengdon@csee.ogi.edu

Miguel ´A. Carreira-Perpi˜n´an

EECS  UC Merced

Cristian Sminchisescu

University of Bonn

http://eecs.ucmerced.edu

sminchisescu.ins.uni-bonn.de

Abstract

Reliably recovering 3D human pose from monocular video requires models that
bias the estimates towards typical human poses and motions. We construct pri-
ors for people tracking using the Laplacian Eigenmaps Latent Variable Model
(LELVM). LELVM is a recently introduced probabilistic dimensionality reduc-
tion model that combines the advantages of latent variable models—a multimodal
probability density for latent and observed variables  and globally differentiable
nonlinear mappings for reconstruction and dimensionality reduction—with those
of spectral manifold learning methods—no local optima  ability to unfold highly
nonlinear manifolds  and good practical scaling to latent spaces of high dimen-
sion. LELVM is computationally efﬁcient  simple to learn from sparse training
data  and compatible with standard probabilistic trackers such as particle ﬁlters.
We analyze the performance of a LELVM-based probabilistic sigma point mixture
tracker in several real and synthetic human motion sequences and demonstrate that
LELVM not only provides sufﬁcient constraints for robust operation in the pres-
ence of missing  noisy and ambiguous image measurements  but also compares
favorably with alternative trackers based on PCA or GPLVM priors.

Recent research in reconstructing articulated human motion has focused on methods that can exploit
available prior knowledge on typical human poses or motions in an attempt to build more reliable
algorithms. The high-dimensionality of human ambient pose space—between 30-60 joint angles
or joint positions depending on the desired accuracy level  makes exhaustive search prohibitively
expensive. This has negative impact on existing trackers  which are often not sufﬁciently reliable at
reconstructing human-like poses  self-initializing or recovering from failure. Such difﬁculties have
stimulated research in algorithms and models that reduce the effective working space  either us-
ing generic search focusing methods (annealing  state space decomposition  covariance scaling) or
by exploiting speciﬁc problem structure (e.g. kinematic jumps). Experience with these procedures
has nevertheless shown that any search strategy  no matter how effective  can be made signiﬁcantly
more reliable if restricted to low-dimensional state spaces. This permits a more thorough explo-
ration of the typical solution space  for a given  comparatively similar computational effort as a
high-dimensional method. The argument correlates well with the belief that the human pose space 
although high-dimensional in its natural ambient parameterization  has a signiﬁcantly lower percep-
tual (latent or intrinsic) dimensionality  at least in a practical sense—many poses that are possible
are so improbable in many real-world situations that it pays off to encode them with low accuracy.

A perceptual representation has to be powerful enough to capture the diversity of human poses in a
sufﬁciently broad domain of applicability (the task domain)  yet compact and analytically tractable
for search and optimization. This justiﬁes the use of models that are nonlinear and low-dimensional
(able to unfold highly nonlinear manifolds with low distortion)  yet probabilistically motivated and
globally continuous for efﬁcient optimization. Reducing dimensionality is not the only goal: per-
ceptual representations have to preserve critical properties of the ambient space. Reliable tracking
needs locality: nearby regions in ambient space have to be mapped to nearby regions in latent space.
If this does not hold  the tracker is forced to make unrealistically large  and difﬁcult to predict jumps
in latent space in order to follow smooth trajectories in the joint angle ambient space.

1

In this paper we propose to model priors for articulated motion using a recently introduced proba-
bilistic dimensionality reduction method  the Laplacian Eigenmaps Latent Variable Model (LELVM)
[1]. Section 1 discusses the requirements of priors for articulated motion in the context of proba-
bilistic and spectral methods for manifold learning  and section 2 describes LELVM and shows how
it combines both types of methods in a principled way. Section 3 describes our tracking frame-
work (using a particle ﬁlter) and section 4 shows experiments with synthetic and real human motion
sequences using LELVM priors learned from motion-capture data.
Related work: There is signiﬁcant work in human tracking  using both generative and discrimina-
tive methods. Due to space limitations  we will focus on the more restricted class of 3D generative
algorithms based on learned state priors  and not aim at a full literature review. Deriving com-
pact prior representations for tracking people or other articulated objects is an active research ﬁeld 
steadily growing with the increased availability of human motion capture data. Howe et al. and
Sidenbladh et al. [2] propose Gaussian mixture representations of short human motion fragments
(snippets) and integrate them in a Bayesian MAP estimation framework that uses 2D human joint
measurements  independently tracked by scaled prismatic models [3]. Brand [4] models the human
pose manifold using a Gaussian mixture and uses an HMM to infer the mixture component index
based on a temporal sequence of human silhouettes. Sidenbladh et al. [5] use similar dynamic priors
and exploit ideas in texture synthesis—efﬁcient nearest-neighbor search for similar motion frag-
ments at runtime—in order to build a particle-ﬁlter tracker with observation model based on contour
and image intensity measurements. Sminchisescu and Jepson [6] propose a low-dimensional proba-
bilistic model based on ﬁtting a parametric reconstruction mapping (sparse radial basis function) and
a parametric latent density (Gaussian mixture) to the embedding produced with a spectral method.
They track humans walking and involved in conversations using a Bayesian multiple hypotheses
framework that fuses contour and intensity measurements. Urtasun et al. [7] use a dynamic MAP
estimation framework based on a GPLVM and 2D human joint correspondences obtained from an
independent image-based tracker. Li et al. [8] use a coordinated mixture of factor analyzers within a
particle ﬁltering framework  in order to reconstruct human motion in multiple views using chamfer
matching to score different conﬁguration. Wang et al. [9] learn a latent space with associated dy-
namics where both the dynamics and observation mapping are Gaussian processes  and Urtasun et
al. [10] use it for tracking. Taylor et al. [11] also learn a binary latent space with dynamics (using
an energy-based model) but apply it to synthesis  not tracking. Our work learns a static  generative
low-dimensional model of poses and integrates it into a particle ﬁlter for tracking. We show its
ability to work with real or partially missing data and to track multiple activities.

1 Priors for articulated human pose

We consider the problem of learning a probabilistic low-dimensional model of human articulated
motion. Call y ∈ RD the representation in ambient space of the articulated pose of a person. In this
paper  y contains the 3D locations of anywhere between 10 and 60 markers located on the person’s
joints (other representations such as joint angles are also possible). The values of y have been
normalised for translation and rotation in order to remove rigid motion and leave only the articulated
motion (see section 3 for how we track the rigid motion). While y is high-dimensional  the motion
pattern lives in a low-dimensional manifold because most values of y yield poses that violate body
constraints or are simply atypical for the motion type considered. Thus we want to model y in terms
of a small number of latent variables x given a collection of poses {yn}N
n=1 (recorded from a human
with motion-capture technology). The model should satisfy the following: (1) It should deﬁne a
probability density for x and y  to be able to deal with noise (in the image or marker measurements)
and uncertainty (from missing data due to occlusion or markers that drop)  and to allow integration
in a sequential Bayesian estimation framework. The density model should also be ﬂexible enough
to represent multimodal densities. (2) It should deﬁne mappings for dimensionality reduction F :
y → x and reconstruction f : x → y that apply to any value of x and y (not just those in the
training set); and such mappings should be deﬁned on a global coordinate system  be continuous
(to avoid physically impossible discontinuities) and differentiable (to allow efﬁcient optimisation
when tracking)  yet ﬂexible enough to represent the highly nonlinear manifold of articulated poses.
From a statistical machine learning point of view  this is precisely what latent variable models
(LVMs) do; for example  factor analysis deﬁnes linear mappings and Gaussian densities  while the
generative topographic mapping (GTM; [12]) deﬁnes nonlinear mappings and a Gaussian-mixture
density in ambient space. However  factor analysis is too limited to be of practical use  and GTM—

2

while ﬂexible—has two important practical problems: (1) the latent space must be discretised to
allow tractable learning and inference  which limits it to very low (2–3) latent dimensions; (2) the
parameter estimation is prone to bad local optima that result in highly distorted mappings.

Another dimensionality reduction method recently introduced  GPLVM [13]  which uses a Gauss-
ian process mapping f (x)  partly improves this situation by deﬁning a tunable parameter xn for
each data point yn. While still prone to local optima  this allows the use of a better initialisation
n=1 (obtained from a spectral method  see later). This has prompted the application of
for {xn}N
GPLVM for tracking human motion [7]. However  GPLVM has some disadvantages: its training is
very costly (each step of the gradient iteration is cubic on the number of training points N  though
approximations based on using few points exist); unlike true LVMs  it deﬁnes neither a posterior
distribution p(x|y) in latent space nor a dimensionality reduction mapping E {x|y}; and the latent
representation it obtains is not ideal. For example  for periodic motions such as running or walking 
repeated periods (identical up to small noise) can be mapped apart from each other in latent space
because nothing constrains xn and xm to be close even when yn = ym (see ﬁg. 3 and [10]).
There exists a different type of dimensionality reduction methods  spectral methods (such as Isomap 
LLE or Laplacian eigenmaps [14])  that have advantages and disadvantages complementary to those
of LVMs. They deﬁne neither mappings nor densities but just a correspondence (xn  yn) between
points in latent space xn and ambient space yn. However  the training is efﬁcient (a sparse eigen-
value problem) and has no local optima  and often yields a correspondence that successfully models
highly nonlinear  convoluted manifolds such as the Swiss roll. While these attractive properties have
spurred recent research in spectral methods  their lack of mappings and densities has limited their
applicability in people tracking. However  a new model that combines the advantages of LVMs and
spectral methods in a principled way has been recently proposed [1]  which we brieﬂy describe next.

2 The Laplacian Eigenmaps Latent Variable Model (LELVM)

min tr(cid:0)XLX⊤(cid:1)

LELVM is based on a natural way of deﬁning an out-of-sample mapping for Laplacian eigenmaps
(LE) which  in addition  results in a density model. In LE  typically we ﬁrst deﬁne a k-nearest-
n=1 and weigh each edge yn ∼ ym by a Gaussian afﬁnity
neighbour graph on the sample data {yn}N
2 k(yn − ym)/σk2). Then the latent points X result from:
function K(yn  ym) = wnm = exp (− 1
(1)
where we deﬁne the matrix XL×N = (x1  . . .   xN )  the symmetric afﬁnity matrix WN ×N   the de-
gree matrix D = diag (PN
n=1 wnm)  the graph Laplacian matrix L = D−W  and 1 = (1  . . .   1)⊤.
The constraints eliminate the two trivial solutions X = 0 (by ﬁxing an arbitrary scale) and
x1 = · · · = xN (by removing 1  which is an eigenvector of L associated with a zero eigenvalue).
The solution is given by the leading u2  . . .   uL+1 eigenvectors of the normalised afﬁnity matrix
N = D− 1
2 with VN ×L = (v2  . . .   vL+1) (an a posteriori trans-
lated  rotated or uniformly scaled X is equally valid).

s.t. X ∈ RL×N   XDX⊤ = I  XD1 = 0

2   namely X = V⊤D− 1

2 WD− 1

Following [1]  we now deﬁne an out-of-sample mapping F(y) = x for a new point y as a semi-
supervised learning problem  by recomputing the embedding as in (1) (i.e.  augmenting the graph
Laplacian with the new point)  but keeping the old embedding ﬁxed:
K(y)⊤ 1⊤K(y)(cid:17)(cid:16) X⊤

(2)
2 k(y − yn)/σk2) for n = 1  . . .   N is the kernel induced by
where Kn(y) = K(y  yn) = exp (− 1
the Gaussian afﬁnity (applied only to the k nearest neighbours of y  i.e.  Kn(y) = 0 if y ≁ yn).
This is one natural way of adding a new point to the embedding by keeping existing embedded
points ﬁxed. We need not use the constraints from (1) because they would trivially determine x  and
the uninteresting solutions X = 0 and X = constant were already removed in the old embedding
anyway. The solution yields an out-of-sample dimensionality reduction mapping x = F(y):

tr(cid:16)( X x )(cid:16) L

x⊤ (cid:17)(cid:17)

min
x∈RL

K(y)

x = F(y) = X K(y)

1⊤K(y) = PN

n=1

K(y yn)

PN

n′=1 K(y yn′ )

xn

(3)

N PN

3

applicable to any point y (new or old). This mapping is formally identical to a Nadaraya-Watson
estimator (kernel regression; [15]) using as data {(xn  yn)}N
n=1 and the kernel K. We can take this
a step further by deﬁning a LVM that has as joint distribution a kernel density estimate (KDE):
p(x  y) = 1

n=1 Ky(y  yn)Kx(x  xn) p(y) = 1

n=1 Ky(y  yn) p(x) = 1

N PN

N PN

n=1 Kx(x  xn)

where Ky is proportional to K so it integrates to 1  and Kx is a pdf kernel in x–space. Consequently 
the marginals in observed and latent space are also KDEs  and the dimensionality reduction and
reconstruction mappings are given by kernel regression (the conditional means E {y|x}  E {x|y}):

F(y) = PN
allow the

yn = PN
n=1 p(n|y)xn
bandwidths
ambient
and
latent
2 k(y − yn)/σyk2).
2 k(x − xn)/σxk2) and Ky(y  yn) ∝ exp (− 1

We
Kx(x  xn) ∝ exp (− 1
may be tuned to control the smoothness of the mappings and densities [1].

f (x) = PN
be

different

Kx(x xn)

n′=1 Kx(x xn′ )
in

the

n=1

PN

to

n=1 p(n|x)yn.

(4)

spaces:
They

Thus  LELVM naturally extends a LE embedding (efﬁciently obtained as a sparse eigenvalue prob-
lem with a cost O(N 2)) to global  continuous  differentiable mappings (NW estimators) and po-
tentially multimodal densities having the form of a Gaussian KDE. This allows easy computation
of posterior probabilities such as p(x|y) (unlike GPLVM). It can use a continuous latent space of
arbitrary dimension L (unlike GTM) by simply choosing L eigenvectors in the LE embedding. It
has no local optima since it is based on the LE embedding. LELVM can learn convoluted mappings
(e.g. the Swiss roll) and deﬁne maps and densities for them [1]. The only parameters to set are the
graph parameters (number of neighbours k  afﬁnity width σ) and the smoothing bandwidths σx  σy.

3 Tracking framework

We follow the sequential Bayesian estimation framework  where for state variables s and observation
variables z we have the recursive prediction and correction equations:

p(st|z0:t−1) = R p(st|st−1) p(st−1|z0:t−1) dst−1

p(st|z0:t) ∝ p(zt|st) p(st|z0:t−1).

(5)

We deﬁne the state variables as s = (x  d) where x ∈ RL is the low-dim. latent space (for pose)
and d ∈ R3 is the centre-of-mass location of the body (in the experiments our state also includes
the orientation of the body  but for simplicity here we describe only the translation). The observed
variables z consist of image features or the perspective projection of the markers on the camera
plane. The mapping from state to observations is (for the markers’ case  assuming M markers):

x ∈ RL
d ∈ R3

f−−−−→ y ∈ R3M −−→ ⊕ P−−−−−→ z ∈ R2M

(6)

where f is the LELVM reconstruction mapping (learnt from mocap data); ⊕ shifts each 3D marker
by d; and P is the perspective projection (pinhole camera)  applied to each 3D point separately. Here
we use a simple observation model p(zt|st): Gaussian with mean given by the transformation (6)
and isotropic covariance (set by the user to control the inﬂuence of measurements in the tracking).
We assume known correspondences and observations that are obtained either from the 3D markers
(for tracking synthetic data) or 2D tracks obtained from a 2D tracker. Our dynamics model is

p(st|st−1) ∝ pd(dt|dt−1) px(xt|xt−1) p(xt)

(7)
where both dynamics models for d and x are random walks: Gaussians centred at the previous
step value dt−1 and xt−1  respectively  with isotropic covariance (set by the user to control the
inﬂuence of dynamics in the tracking); and p(xt) is the LELVM prior. Thus the overall dynamics
predicts states that are both near the previous state and yield feasible poses. Of course  more complex
dynamics models could be used if e.g. the speed and direction of movement are known.

As tracker we use the Gaussian mixture Sigma-point particle ﬁlter (GMSPPF) [16]. This is a par-
ticle ﬁlter that uses a Gaussian mixture representation for the posterior distribution in state space
and updates it with a Sigma-point Kalman ﬁlter. This Gaussian mixture will be used as proposal
distribution to draw the particles. As in other particle ﬁlter implementations  the prediction step
is carried out by approximating the integral (5) with particles and updating the particles’ weights.
Then  a new Gaussian mixture is ﬁtted with a weighted EM algorithm to these particles. This re-
places the resampling stage needed by many particle ﬁlters and mitigates the problem of sample
depletion while also preventing the number of components in the Gaussian mixture from growing
over time. The choice of this particular tracker is not critical; we use it to illustrate the fact that
LELVM can be introduced in any probabilistic tracker for nonlinear  nongaussian models. Given the
corrected distribution p(st|z0:t)  we choose its mean as recovered state (pose and location). It is also
possible to choose instead the mode closest to the state at t − 1  which could be found by mean-shift
or Newton algorithms [17] since we are using a Gaussian-mixture representation in state space.

4

4 Experiments

We demonstrate our low-dimensional tracker on image sequences of people walking and running 
both synthetic (ﬁg. 1) and real (ﬁg. 2–3). Fig. 1 shows the model copes well with persistent partial
occlusion and severely subsampled training data (A B)  and quantitatively evaluates temporal recon-
struction (C). For all our experiments  the LELVM parameters (number of neighbors k  Gaussian
afﬁnity σ  and bandwidths σx and σy) were set manually. We mainly considered 2D latent spaces
(for pose  plus 6D for rigid motion)  which were expressive enough for our experiments. More
complex  higher-dimensional models are straightforward to construct. The initial state distribution
p(s0) was chosen a broad Gaussian  the dynamics and observation covariance were set manually to
control the tracking smoothness  and the GMSPPF tracker used a 5-component Gaussian mixture
in latent space (and in the state space of rigid motion) and a small set of 500 particles. The 3D
representation we use is a 102-D vector obtained by concatenating the 3D markers coordinates of all
the body joints. These would be highly unconstrained if estimated independently  but we only use
them as intermediate representation; tracking actually occurs in the latent space  tightly controlled
using the LELVM prior. For the synthetic experiments and some of the real experiments (ﬁgs. 2–3)
the camera parameters and the body proportions were known (for the latter  we used the 2D outputs
of [6]). For the CMU mocap video (ﬁg. 2B) we roughly guessed. We used mocap data from several
sources (CMU  OSU). As observations we always use 2D marker positions  which  depending on
the analyzed sequence were either known (the synthetic case)  or provided by an existing tracker
[6] or speciﬁed manually (ﬁg. 2B). Alternatively 2D point trackers similar to the ones of [7] can be
used. The forward generative model is obtained by combining the latent to ambient space mapping
(this provides the position of the 3D markers) with a perspective projection transformation. The
observation model is a product of Gaussians  each measuring the probability of a particular marker
position given its corresponding image point track.
Experiments with synthetic data: we analyze the performance of our tracker in controlled condi-
tions (noise perturbed synthetically generated image tracks) both under regular circumstances (rea-
sonable sampling of training data) and more severe conditions with subsampled training points and
persistent partial occlusion (the man running behind a fence  with many of the 2D marker tracks
obstructed). Fig. 1B C shows both the posterior (ﬁltered) latent space distribution obtained from
our tracker  and its mean (we do not show the distribution of the global rigid body motion; in all
experiments this is tracked with good accuracy). In the latent space plot shown in ﬁg. 1B  the onset
of running (two cycles were used) appears as a separate region external to the main loop. It does not
appear in the subsampled training set in ﬁg. 1B  where only one running cycle was used for training
and the onset of running was removed. In each case  one can see that the model is able to track quite
competently  with a modest decrease in its temporal accuracy  shown in ﬁg. 1C  where the averages
are computed per 3D joint (normalised wrt body height). Subsampling causes some ambiguity in
the estimate  e.g. see the bimodality in the right plot in ﬁg. 1C. In another set of experiments (not
shown) we also tracked using different subsets of 3D markers. The estimates were accurate even
when about 30% of the markers were dropped.
Experiments with real images: this shows our tracker’s ability to work with real motions of differ-
ent people  with different body proportions  not in its latent variable model training set (ﬁgs. 2–3).
We study walking  running and turns. In all cases  tracking and 3D reconstruction are reasonably ac-
curate. We have also run comparisons against low-dimensional models based on PCA and GPLVM
(ﬁg. 3). It is important to note that  for LELVM  errors in the pose estimates are primarily caused
by mismatches between the mocap data used to learn the LELVM prior and the body proportions of
the person in the video. For example  the body proportions of the OSU motion captured walker are
quite different from those of the image in ﬁg. 2–3 (e.g. note how the legs of the stick man are shorter
relative to the trunk). Likewise  the style of the runner from the OSU data (e.g. the swinging of the
arms) is quite different from that of the video. Finally  the interest points tracked by the 2D tracker
do not entirely correspond either in number or location to the motion capture markers  and are noisy
and sometimes missing. In future work  we plan to include an optimization step to also estimate the
body proportions. This would be complicated for a general  unconstrained model because the di-
mensions of the body couple with the pose  so either one or the other can be changed to improve the
tracking error (the observation likelihood can also become singular). But for dedicated prior pose
models like ours these difﬁculties should be signiﬁcantly reduced. The model simply cannot assume
highly unlikely stances—these are either not representable at all  or have reduced probability—and
thus avoids compensatory  unrealistic body proportion estimates.

5

B

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

n = 15

n = 40

n = 65

n = 90

n = 115

n = 140

A

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

n = 1

n = 13

n = 25

n = 37

n = 49

n = 60

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

C

E
S
M
R

0.1

0.08

0.06

0.04

0.02

0

0

0.13

0.12

0.11

0.1

0.09

0.08

0.07

0.06

E
S
M
R

150

0.05

0

10

50

100

time step n

20

30

40

50

60

time step n

OSU running man motion capture data. A: we use 217 datapoints for training LELVM
Figure 1:
(with added noise) and for tracking. Row 1: tracking in the 2D latent space. The contours (very tight
in this sequence) are the posterior probability. Row 2: perspective-projection-based observations
with occlusions. Row 3: each quadruplet (a  a′  b  b′) show the true pose of the running man from
a front and side views (a  b)  and the reconstructed pose by tracking with our model (a′  b′). B: we
use the ﬁrst running cycle for training LELVM and the second cycle for tracking. C: RMSE errors
for each frame  for the tracking of A (left plot) and B (middle plot)  normalised so that 1 equals the
j=1 kynj − ˆynjk2(cid:1)−1/2 for all 3D locations of the M
height of the stick man. RMSE(n) = (cid:0) 1
markers  i.e.  comparison of reconstructed stick man ˆyn with ground-truth stick man yn. Right plot:
multimodal posterior distribution in pose space for the model of A (frame 42).

M PM

Comparison with PCA and GPLVM (ﬁg. 3): for these models  the tracker uses the same GMSPPF
setting as for LELVM (number of particles  initialisation  random-walk dynamics  etc.) but with the
mapping y = f (x) provided by GPLVM or PCA  and with a uniform prior p(x) in latent space
(since neither GPLVM nor the non-probabilistic PCA provide one). The LELVM-tracker uses both
its f (x) and latent space prior p(x)  as discussed. All methods use a 2D latent space. We ensured
the best possible training of GPLVM by model selection based on multiple runs. For PCA  the
latent space looks deceptively good  showing non-intersecting loops. However  (1) individual loops
do not collect together as they should (for LELVM they do); (2) worse still  the mapping from 2D
to pose space yields a poor observation model. The reason is that the loop in 102-D pose space
is nonlinearly bent and a plane can at best intersect it at a few points  so the tracker often stays
put at one of those (typically an “average” standing position)  since leaving it would increase the
error a lot. Using more latent dimensions would improve this  but as LELVM shows  this is not
necessary. For GPLVM  we found high sensitivity to ﬁlter initialisation: the estimates have high
variance across runs and are inaccurate ≈ 80% of the time. When it fails  the GPLVM tracker often
freezes in latent space  like PCA. When it does succeed  it produces results that are comparable
with LELVM  although somewhat less accurate visually. However  even then GPLVM’s latent space
consists of continuous chunks spread apart and offset from each other; GPLVM has no incentive to
place nearby two xs mapping to the same y. This effect  combined with the lack of a data-sensitive 
realistic latent space density p(x)  makes GPLVM jump erratically from chunk to chunk  in contrast
with LELVM  which smoothly follows the 1D loop. Some GPLVM problems might be alleviated
using higher-order dynamics  but our experiments suggest that such modeling sophistication is less

6

n = 1

n = 15

n = 29

n = 43

n = 55

n = 69

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

n = 4

n = 9

n = 14

n = 19

n = 24

n = 29

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

A

B

20

40

60

80

100

120

140

160

180

200

220

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

A: tracking of a video from [6] (turning & walking). We use 220 datapoints (3 full walking
Figure 2:
cycles) for training LELVM. Row 1: tracking in the 2D latent space. The contours are the estimated
posterior probability. Row 2: tracking based on markers. The red dots are the 2D tracks and the
green stick man is the 3D reconstruction obtained using our model. Row 3: our 3D reconstruction
from a different viewpoint. B: tracking of a person running straight towards the camera. Notice the
scale changes and possible forward-backward ambiguities in the 3D estimates. We train the LELVM
using 180 datapoints (2.5 running cycles); 2D tracks were obtained by manually marking the video.
In both A–B the mocap training data was for a person different from the video’s (with different body
proportions and motions)  and no ground-truth estimate was available for favourable initialisation.

LELVM

GPLVM

PCA

tracking in latent space

38
0.99

0.02

0.015

0.01

0.005

0

−0.005

−0.01

−0.015

−0.02

−0.025

−0.025

−0.02

−0.015

−0.01

−0.005

0

0.005

0.01

0.015

0.02

0.025

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

tracking in latent space

38

−2

−1

0

1

2

3

30

20

10

0

−10

−20

−30

−80

tracking in latent space

38

−60

−40

−20

0

20

40

60

80

frame 38.

Figure 3: Method compari-
son 
PCA and
GPLVM map consecutive walk-
ing cycles to spatially distinct
latent space regions.
Com-
pounded by a data independent
latent prior  the resulting tracker
gets easily confused:
it jumps
across loops and/or remains put 
trapped in local optima. In con-
trast  LELVM is stable and fol-
lows tightly a 1D manifold (see
videos).

crucial if locality constraints are correctly modeled (as in LELVM). We conclude that  compared to
LELVM  GPLVM is signiﬁcantly less robust for tracking  has much higher training overhead and
lacks some operations (e.g. computing latent conditionals based on partly missing ambient data).

7

5 Conclusion and future work

We have proposed the use of priors based on the Laplacian Eigenmaps Latent Variable Model
(LELVM) for people tracking. LELVM is a probabilistic dim. red. method that combines the advan-
tages of latent variable models and spectral manifold learning algorithms: a multimodal probability
density over latent and ambient variables  globally differentiable nonlinear mappings for reconstruc-
tion and dimensionality reduction  no local optima  ability to unfold highly nonlinear manifolds  and
good practical scaling to latent spaces of high dimension. LELVM is computationally efﬁcient  sim-
ple to learn from sparse training data  and compatible with standard probabilistic trackers such as
particle ﬁlters. Our results using a LELVM-based probabilistic sigma point mixture tracker with sev-
eral real and synthetic human motion sequences show that LELVM provides sufﬁcient constraints
for robust operation in the presence of missing  noisy and ambiguous image measurements. Com-
parisons with PCA and GPLVM show LELVM is superior in terms of accuracy  robustness and
computation time. The objective of this paper was to demonstrate the ability of the LELVM prior
in a simple setting using 2D tracks obtained automatically or manually  and single-type motions
(running  walking). Future work will explore more complex observation models such as silhouettes;
the combination of different motion types in the same latent space (whose dimension will exceed 2);
and the exploration of multimodal posterior distributions in latent space caused by ambiguities.

Acknowledgments

This work was partially supported by NSF CAREER award IIS–0546857 (MACP)  NSF IIS–0535140
and EC MCEXT–025481 (CS). CMU data: http://mocap.cs.cmu.edu (created with fund-
ing from NSF EIA–0196217). OSU data: http://accad.osu.edu/research/mocap/mocap
data.htm.

References
[1] M. ´A. Carreira-Perpi˜n´an and Z. Lu. The Laplacian Eigenmaps Latent Variable Model. In AISTATS  2007.
[2] N. R. Howe  M. E. Leventon  and W. T. Freeman. Bayesian reconstruction of 3D human motion from

single-camera video. In NIPS  volume 12  pages 820–826  2000.

[3] T.-J. Cham and J. M. Rehg. A multiple hypothesis approach to ﬁgure tracking. In CVPR  1999.
[4] M. Brand. Shadow puppetry. In ICCV  pages 1237–1244  1999.
[5] H. Sidenbladh  M. J. Black  and L. Sigal. Implicit probabilistic models of human motion for synthesis

and tracking. In ECCV  volume 1  pages 784–800  2002.

[6] C. Sminchisescu and A. Jepson. Generative modeling for continuous non-linearly embedded visual infer-

ence. In ICML  pages 759–766  2004.

[7] R. Urtasun  D. J. Fleet  A. Hertzmann  and P. Fua. Priors for people tracking from small training sets. In

ICCV  pages 403–410  2005.

[8] R. Li  M.-H. Yang  S. Sclaroff  and T.-P. Tian. Monocular tracking of 3D human motion with a coordinated

mixture of factor analyzers. In ECCV  volume 2  pages 137–150  2006.

[9] J. M. Wang  D. Fleet  and A. Hertzmann. Gaussian process dynamical models. In NIPS  volume 18  2006.
[10] R. Urtasun  D. J. Fleet  and P. Fua. Gaussian process dynamical models for 3D people tracking. In CVPR 

pages 238–245  2006.

[11] G. W. Taylor  G. E. Hinton  and S. Roweis. Modeling human motion using binary latent variables. In

NIPS  volume 19  2007.

[12] C. M. Bishop  M. Svens´en  and C. K. I. Williams. GTM: The generative topographic mapping. Neural

Computation  10(1):215–234  January 1998.

[13] N. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable

models. Journal of Machine Learning Research  6:1783–1816  November 2005.

[14] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural Computation  15(6):1373–1396  June 2003.

[15] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall  1986.
[16] R. van der Merwe and E. A. Wan. Gaussian mixture sigma-point particle ﬁlters for sequential probabilistic

inference in dynamic state-space models. In ICASSP  volume 6  pages 701–704  2003.

[17] M. ´A. Carreira-Perpi˜n´an. Acceleration strategies for Gaussian mean-shift image segmentation. In CVPR 

pages 1160–1167  2006.

8

,Zhisheng Zhong
Tiancheng Shen
Yibo Yang
Zhouchen Lin
Chao Zhang