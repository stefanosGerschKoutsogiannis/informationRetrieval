2017,First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization,This paper studies empirical risk minimization (ERM) problems for large-scale datasets and incorporates the idea of adaptive sample size methods to improve the guaranteed convergence bounds for first-order stochastic and deterministic methods. In contrast to traditional methods that attempt to solve the ERM problem corresponding to the full dataset directly  adaptive sample size schemes start with a small number of samples and solve the corresponding ERM problem to its statistical accuracy. The sample size is then grown geometrically -- e.g.  scaling by a factor of two -- and use the solution of the previous ERM as a warm start for the new ERM. Theoretical analyses show that the use of adaptive sample size methods reduces the overall computational cost of achieving the statistical accuracy of the whole dataset for a broad range of deterministic and stochastic first-order methods. The gains are specific to the choice of method. When particularized to  e.g.  accelerated gradient descent and stochastic variance reduce gradient  the computational cost advantage is a logarithm of the number of training samples. Numerical experiments on various datasets confirm theoretical claims and showcase the gains of using the proposed adaptive sample size scheme.,First-Order Adaptive Sample Size Methods to

Reduce Complexity of Empirical Risk Minimization

Aryan Mokhtari

University of Pennsylvania
aryanm@seas.upenn.edu

Alejandro Ribeiro

University of Pennsylvania
aribeiro@seas.upenn.edu

Abstract

This paper studies empirical risk minimization (ERM) problems for large-scale
datasets and incorporates the idea of adaptive sample size methods to improve the
guaranteed convergence bounds for ﬁrst-order stochastic and deterministic meth-
ods. In contrast to traditional methods that attempt to solve the ERM problem
corresponding to the full dataset directly  adaptive sample size schemes start with
a small number of samples and solve the corresponding ERM problem to its sta-
tistical accuracy. The sample size is then grown geometrically – e.g.  scaling by a
factor of two – and use the solution of the previous ERM as a warm start for the
new ERM. Theoretical analyses show that the use of adaptive sample size methods
reduces the overall computational cost of achieving the statistical accuracy of the
whole dataset for a broad range of deterministic and stochastic ﬁrst-order meth-
ods. The gains are speciﬁc to the choice of method. When particularized to  e.g. 
accelerated gradient descent and stochastic variance reduce gradient  the computa-
tional cost advantage is a logarithm of the number of training samples. Numerical
experiments on various datasets conﬁrm theoretical claims and showcase the gains
of using the proposed adaptive sample size scheme.

1

Introduction

Finite sum minimization (FSM) problems involve objectives that are expressed as the sum of a
typically large number of component functions. Since evaluating descent directions is costly  it is
customary to utilize stochastic descent methods that access only one of the functions at each itera-
tion. When considering ﬁrst order methods  a ﬁtting measure of complexity is the total number of
gradient evaluations that are needed to achieve optimality of order ✏. The paradigmatic deterministic
gradient descent (GD) method serves as a naive complexity upper bound and has long been known
to obtain an ✏-suboptimal solution with O(N log(1/✏)) gradient evaluations for an FSM problem
with N component functions and condition number  [13]. Accelerated gradient descent (AGD) [14]
improves the computational complexity of GD to O(Np log(1/✏))  which is known to be the opti-
mal bound for deterministic ﬁrst-order methods [13]. In terms of stochastic optimization  it has been
only recently that linearly convergent methods have been proposed. Stochastic averaging gradient
[15  8]  stochastic variance reduction [10]  and stochastic dual coordinate ascent [17  18]  have all
been shown to converge to ✏-accuracy at a cost of O((N +) log(1/✏)) gradient evaluations. The ac-
celerating catalyst framework in [11] further reduces complexity to O((N +pN ) log() log(1/✏))
and the works in [1] and [7] to O((N + pN ) log(1/✏)). The latter matches the upper bound on

the complexity of stochastic methods [20].
Perhaps the main motivation for studying FSM is the solution of empirical risk minimization (ERM)
problems associated with a large training set. ERM problems are particular cases of FSM  but they
do have two speciﬁc qualities that come from the fact that ERM is a proxy for statistical loss min-
imization. The ﬁrst property is that since the empirical risk and the statistical loss have different

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

minimizers  there is no reason to solve ERM beyond the expected difference between the two objec-
tives. This so-called statistical accuracy takes the place of ✏ in the complexity orders of the previous
paragraph and is a constant of order O(1/N ↵) where ↵ is a constant from the interval [0.5  1] de-
pending on the regularity of the loss function; see Section 2. The second important property of
ERM is that the component functions are drawn from a common distribution. This implies that if
we consider subsets of the training set  the respective empirical risk functions are not that different
from each other and  indeed  their differences are related to the statistical accuracy of the subset.
The relationship of ERM to statistical loss minimization suggests that ERM problems have more
structure than FSM problems. This is not exploited by most existing methods which  albeit used for
ERM  are in fact designed for FSM. The goal of this paper is to exploit the relationship between ERM
and statistical loss minimization to achieve lower overall computational complexity for a broad class
of ﬁrst-order methods applied to ERM. The technique we propose uses subsamples of the training
set containing n  N component functions that we grow geometrically. In particular  we start by a
small number of samples and minimize the corresponding empirical risk added by a regularization
term of order Vn up to its statistical accuracy. Note that  based on the ﬁrst property of ERM  the
added adaptive regularization term does not modify the required accuracy while it makes the problem
strongly convex and improves the problem condition number. After solving the subproblem  we
double the size of the training set and use the solution of the problem with n samples as a warm
start for the problem with 2n samples. This is a reasonable initialization since based on the second
property of ERM the functions are drawn from a joint distribution  and  therefore  the optimal values
of the ERM problems with n and 2n functions are not that different from each other. The proposed
approach succeeds in exploiting the two properties of ERM problems to improve complexity bounds
of ﬁrst-order methods. In particular  we show that to reach the statistical accuracy of the full training
set the adaptive sample size scheme reduces the overall computational complexity of a broad range
of ﬁrst-order methods by a factor of log(N ↵). For instance  the overall computational complexity
of adaptive sample size AGD to reach the statistical accuracy of the full training set is of order
O(Np) which is lower than O((Np) log(N ↵)) complexity of AGD.
Related work. The adaptive sample size approach was used in [6] to improve the performance of
the SAGA method [8] for solving ERM problems. In the dynamic SAGA (DynaSAGA) method in
[6]  the size of training set grows at each iteration by adding two new samples  and the iterates are
updated by a single step of SAGA. Although DynaSAGA succeeds in improving the performance of
SAGA for solving ERM problems  it does not use an adaptive regularization term to tune the problem
condition number. Moreover  DynaSAGA only works for strongly convex functions  while in our
proposed scheme the functions are convex (not necessarily strongly convex). The work in [12] is
the most similar work to this manuscript. The Ada Newton method introduced in [12] aims to solve
each subproblem within its statistical accuracy with a single update of Newton’s method by ensuring
that iterates always stay in the quadratic convergence region of Newton’s method. Ada Newton
reaches the statistical accuracy of the full training in almost two passes over the dataset; however  its
computational complexity is prohibitive since it requires computing the objective function Hessian
and its inverse at each iteration.

2 Problem Formulation
Consider a decision vector w 2 Rp  a random variable Z with realizations z and a convex loss
function f (w; z). We aim to ﬁnd the optimal argument that minimizes the optimization problem

w⇤ := argmin

w

L(w) = argmin

w

EZ[f (w  Z)] = argmin

f (w  Z)P (dz) 

(1)

w ZZ

where L(w) := EZ[f (w  Z)] is deﬁned as the expected loss  and P is the probability distribution
of the random variable Z. The optimization problem in (1) cannot be solved since the distribution
P is unknown. However  we have access to a training set T = {z1  . . .   zN} containing N indepen-
dent samples z1  . . .   zN drawn from P   and  therefore  we attempt to minimize the empirical loss
associated with the training set T = {z1  . . .   zN}  which is equivalent to minimizing the problem
(2)

w†n := argmin

f (w  zi) 

nXi=1
for n = N. Note that in (2) we deﬁned Ln(w) := (1/n)Pn

Ln(w) = argmin

1
n

w

w

i=1 f (w  zi) as the empirical loss.

2

There is a rich literature on bounds for the difference between the expected loss L and the empirical
loss Ln which is also referred to as estimation error [4  3]. We assume here that there exists a
constant Vn  which depends on the number of samples n  that upper bounds the difference between
the expected and empirical losses for all w 2 Rp

E sup
w2Rp |L(w)  Ln(w)|  Vn 

(3)

where the expectation is with respect to the choice of the training set. The celebrated work of Vapnik

in [19  Section 3.4] provides the upper bound Vn = O(p(1/n) log(1/n)) which can be improved
to Vn = O(p1/n) using the chaining technique (see  e.g.  [5]). Bounds of the order Vn = O(1/n)

have been derived more recently under stronger regularity conditions that are not uncommon in
practice  [2  9  4]. In this paper  we report our results using the general bound Vn = O(1/n↵) where
↵ can be any constant form the interval [0.5  1].
The observation that the optimal values of the expected loss and empirical loss are within a Vn
distance of each other implies that there is no gain in improving the optimization error of minimizing
Ln beyond the constant Vn. In other words  if we ﬁnd an approximate solution wn such that the
optimization error is bounded by Ln(wn) Ln(w†n)  Vn  then ﬁnding a more accurate solution to
reduce the optimization error is not beneﬁcial since the overall error  i.e.  the sum of estimation and
optimization errors  does not become smaller than Vn. Throughout the paper we say that wn solves
the ERM problem in (2) to within its statistical accuracy if it satisﬁes Ln(wn)  Ln(w†n)  Vn.
We can further leverage the estimation error to add a regularization term of the form (cVn/2)kwk2 to
the empirical loss to ensure that the problem is strongly convex. To do so  we deﬁne the regularized
empirical risk Rn(w) := Ln(w) + (cVn/2)kwk2 and the corresponding optimal argument

w⇤n := argmin

w

Rn(w) = argmin

w

Ln(w) +

cVn
2 kwk2 

(4)

and attempt to minimize Rn with accuracy Vn. Since the regularization in (4) is of order Vn and
(3) holds  the difference between Rn(w⇤n) and L(w⇤) is also of order Vn – this is not immediate
as it seems; see [16]. Thus  the variable wn solves the ERM problem in (2) to within its statistical
accuracy if it satisﬁes Rn(wn)  Rn(w⇤n)  Vn. It follows that by solving the problem in (4) for
n = N we ﬁnd w⇤N that solves the expected risk minimization in (1) up to the statistical accuracy
VN of the full training set T . In the following section we introduce a class of methods that solve
problem (4) up to its statistical accuracy faster than traditional deterministic and stochastic descent
methods.

3 Adaptive Sample Size Methods

The empirical risk minimization (ERM) problem in (4) can be solved using state-of-the-art methods
for minimizing strongly convex functions. However  these methods never exploit the particular
property of ERM that the functions are drawn from the same distribution. In this section  we propose
an adaptive sample size scheme which exploits this property of ERM to improve the convergence
guarantees for traditional optimization method to reach the statistical accuracy of the full training
set. In the proposed adaptive sample size scheme  we start by a small number of samples and solve
its corresponding ERM problem with a speciﬁc accuracy. Then  we double the size of the training
set and use the solution of the previous ERM problem – with half samples – as a warm start for the
new ERM problem. This procedure keeps going until the training set becomes identical to the given
training set T which contains N samples.
Consider the training set Sm with m samples as a subset of the full training T   i.e.  Sm ⇢T . As-
sume that we have solved the ERM problem corresponding to the set Sm such that the approximate
solution wm satisﬁes the condition E[Rm(wm)  Rm(w⇤m)]  m. Now the next step in the pro-
posed adaptive sample size scheme is to double the size of the current training set Sm and solve the
ERM problem corresponding to the set Sn which has n = 2m samples and contains the previous
set  i.e.  Sm ⇢S n ⇢T .
We use wm which is a proper approximate for the optimal solution of Rm as the initial iterate for the
optimization method that we use to minimize the risk Rn. This is a reasonable choice if the optimal
arguments of Rm and Rn are close to each other  which is the case since samples are drawn from

3

Algorithm 1 Adaptive Sample Size Mechanism
1: Input: Initial sample size n = m0 and argument wn = wm0 with krRn(wn)k  (p2c)Vn
2: while n  N do {main loop}
3:
4:
5:
6:
7:
8:
9:
10: end while

Update argument and index: wm = wn and m = n.
Increase sample size: n = min{2m  N}.
Set the initial variable: ˜w = wm.
while krRn( ˜w)k > (p2c)Vn do
Update the variable ˜w: Compute ˜w = Update( ˜w rRn( ˜w))
end while
Set wn = ˜w.

a ﬁxed distribution P. Starting with wm  we can use ﬁrst-order descent methods to minimize the
empirical risk Rn. Depending on the iterative method that we use for solving each ERM problem
we might need different number of iterations to ﬁnd an approximate solution wn which satisﬁes the
condition E[Rn(wn)  Rn(w⇤n)]  n. To design a comprehensive routine we need to come up
with a proper condition for the required accuracy n at each phase.
In the following proposition we derive an upper bound for the expected suboptimality of the variable
wm for the risk Rn based on the accuracy of wm for the previous risk Rm associated with the
training set Sm. This upper bound allows us to choose the accuracy m efﬁciently.
Proposition 1. Consider the sets Sm and Sn as subsets of the training set T such that Sm⇢S n⇢T  
where the number of samples in the sets Sm and Sn are m and n  respectively. Further  deﬁne wm as
an m optimal solution of the risk Rm in expectation  i.e.  E[Rm(wm)  R⇤m]  m  and recall Vn
as the statistical accuracy of the training set Sn. Then the empirical risk error Rn(wm)  Rn(w⇤n)
of the variable wm corresponding to the set Sn in expectation is bounded above by
E[Rn(wm)Rn(w⇤n)]  m+
Proof. See Section 7.1 in the supplementary material.

(Vnm + Vm)+2 (Vm  Vn)+

kw⇤k2. (5)

c(Vm  Vn)

2(n  m)

n

2

The result in Proposition 1 characterizes the sub-optimality of the variable wm  which is an m
sub-optimal solution for the risk Rm  with respect to the empirical risk Rn associated with the set
Sn. If we assume that the statistical accuracy Vn is of the order O(1/n↵) and we double the size of
the training set at each step  i.e.  n = 2m  then the inequality in (5) can be simpliﬁed to

E[Rn(wm)  Rn(w⇤n)]  m +2 +✓1 

1

2↵◆⇣2 +

c

2kw⇤k2⌘ Vm.

The expression in (6) formalizes the reason that there is no need to solve the sub-problem Rm
beyond its statistical accuracy Vm. In other words  even if m is zero the expected sub-optimality
will be of the order O(Vm)  i.e.  E[Rn(wm)  Rn(w⇤n)] = O(Vm). Based on this observation  The
required precision m for solving the sub-problem Rm should be of the order m = O(Vm).
The steps of the proposed adaptive sample size scheme is summarized in Algorithm 1. Note that
since computation of the sub-optimality Rn(wn)Rn(w⇤n) requires access to the minimizer w⇤n  we
replace the condition Rn(wn)  Rn(w⇤n)  Vn by a bound on the norm of gradient krRn(wn)k2.
The risk Rn is strongly convex  and we can bound the suboptimality Rn(wn)  Rn(w⇤n) as

(6)

(7)

Rn(wn)  Rn(w⇤n) 

1
2cVnkrRn(wn)k2.

Hence  at each stage  we stop updating the variable if the condition krRn(wn)k  (p2c)Vn holds
which implies Rn(wn)  Rn(w⇤n)  Vn. The intermediate variable ˜w can be updated in Step 7
using any ﬁrst-order method. We will discuss this procedure for accelerated gradient descent (AGD)
and stochastic variance reduced gradient (SVRG) methods in Sections 4.1 and 4.2  respectively.

4

4 Complexity Analysis

In this section  we aim to characterize the number of required iterations sn at each stage to solve
the subproblems within their statistical accuracy. We derive this result for all linearly convergent
ﬁrst-order deterministic and stochastic methods.
The inequality in (6) not only leads to an efﬁcient policy for the required precision m at each
step  but also provides an upper bound for the sub-optimality of the initial iterate  i.e.  wm  for
minimizing the risk Rn. Using this upper bound  depending on the iterative method of choice  we
can characterize the number of required iterations sn to ensure that the updated variable is within
the statistical accuracy of the risk Rn. To formally characterize the number of required iterations
sn  we ﬁrst assume the following conditions are satisﬁed.
Assumption 1. The loss functions f (w  z) are convex with respect to w for all values of z. More-
over  their gradients rf (w  z) are Lipschitz continuous with constant M

krf (w  z)  rf (w0  z)k  Mkw  w0k 

for all z.

(8)

The conditions in Assumption 1 imply that the average loss L(w) and the empirical loss Ln(w)
are convex and their gradients are Lipschitz continuous with constant M. Thus  the empirical risk
Rn(w) is strongly convex with constant cVn and its gradients rRn(w) are Lipschitz continuous
with parameter M + cVn.
So far we have concluded that each subproblem should be solved up to its statistical accuracy.
This observation leads to an upper bound for the number of iterations needed at each step to solve
each subproblem. Indeed various descent methods can be executed for solving the sub-problem.
Here we intend to come up with a general result that contains all descent methods that have a
linear convergence rate when the objective function is strongly convex and smooth. In the following
theorem  we derive a lower bound for the number of required iterations sn to ensure that the variable
wn  which is the outcome of updating wm by sn iterations of the method of interest  is within the
statistical accuracy of the risk Rn for any linearly convergent method.
Theorem 2. Consider the variable wm as a Vm-suboptimal solution of the risk Rm in expectation 
i.e.  E[Rm(wm)  Rm(w⇤m)]  Vm  where Vm = O(1/m↵). Consider the sets Sm ⇢S n ⇢T
such that n = 2m  and suppose Assumption 1 holds. Further  deﬁne 0  ⇢n < 1 as the linear
convergence factor of the descent method used for updating the iterates. Then  the variable wn
generated based on the adaptive sample size mechanism satisﬁes E[Rn(wn)  Rn(w⇤n)]  Vn if
the number of iterations sn at the n-th stage is larger than

sn  

log⇥3 ⇥ 2↵ + (2↵  1)2 + c

log ⇢n

2kw⇤k2⇤

.

(9)

Proof. See Section 7.2 in the supplementary material.

The result in Theorem 2 characterizes the number of required iterations at each phase. Depending
on the linear convergence factor ⇢n and the parameter ↵ for the order of statistical accuracy  the
number of required iterations might be different. Note that the parameter ⇢n might depend on the
size of the training set directly or through the dependency of the problem condition number on
n. It is worth mentioning that the result in (9) shows a lower bound for the number of required

iteration which means that sn = b( log⇥3 ⇥ 2↵ + (2↵  1)2 + (c/2)kw⇤k2⇤/log ⇢n)c + 1 is
the exact number of iterations needed when minimizing Rn  where bac indicates the ﬂoor of a.
To characterize the overall computational complexity of the proposed adaptive sample size scheme 
the exact expression for the linear convergence constant ⇢n is required. In the following section 
we focus on two deterministic and stochastic methods and characterize their overall computational
complexity to reach the statistical accuracy of the full training set T .
4.1 Adaptive Sample Size Accelerated Gradient (Ada AGD)
The accelerated gradient descent (AGD) method  also called as Nesterov’s method  is a long-
established descent method which achieves the optimal convergence rate for ﬁrst-order determin-
istic methods. In this section  we aim to combine the update of AGD with the adaptive sample size
scheme in Section 3 to improve convergence guarantees of AGD for solving ERM problems. This

5

and

can be done by using AGD for updating the iterates in step 7 of Algorithm 1. Given an iterate wm
within the statistical accuracy of the set Sm  the adaptive sample size accelerated gradient descent
method (Ada AGD) requires sn iterations of AGD to ensure that the resulted iterate wn lies in the
statistical accuracy of Sn. In particular  if we initialize the sequences ˜w and ˜y as ˜w0 = ˜y0 = wm 
the approximate solution wn for the risk Rn is the outcome of the updates
(10)

˜wk+1 = ˜yk  ⌘nrRn(˜yk) 

˜yk+1 = ˜wk+1 + n( ˜wk+1  ˜wk)

(11)
after sn iterations  i.e.  wn = ˜wsn. The parameters ⌘n and n are indexed by n since they depend on
the number of samples. We use the convergence rate of AGD to characterize the number of required
iterations sn to guarantee that the outcome of the recursive updates in (10) and (11) is within the
statistical accuracy of Rn.
Theorem 3. Consider the variable wm as a Vm-optimal solution of the risk Rm in expectation  i.e. 
E[Rm(wm)  Rm(w⇤m)]  Vm  where Vm = /m↵. Consider the sets Sm ⇢S n ⇢T such that
n = 2m  and suppose Assumption 1 holds. Further  set the parameters ⌘n and n as

⌘n =

1

cVn + M

and

n =

pcVn + M  pcVn
pcVn + M + pcVn

.

(12)

Then  the variable wn generated based on the update of Ada AGD in (10)-(11) satisﬁes E[Rn(wn)
Rn(w⇤n)]  Vn if the number of iterations sn is larger than

sn s n↵M + c

c

log⇥6 ⇥ 2↵ + (2↵  1)4 + ckw⇤k2⇤ .
Moreover  if we deﬁne m0 as the size of the ﬁrst training set  to reach the statistical accuracy VN of
the full training set T the overall computational complexity of Ada GD is given by

(13)

N"1 + log2✓ N

m0◆ + p2↵

p2↵  1!s N ↵M

c

# log⇥6 ⇥ 2↵ + (2↵  1)4 + ckw⇤k2⇤ .

(14)

Proof. See Section 7.3 in the supplementary material.

The result in Theorem 3 characterizes the number of required iterations sn to achieve the statistical
accuracy of Rn. Moreover  it shows that to reach the accuracy VN = O(1/N ↵) for the risk RN
accosiated to the full training set T   the total computational complexity of Ada AGD is of the order
ON (1+↵/2). Indeed  this complexity is lower than the overall computational complexity of AGD
for reaching the same target which is given by ONpN log(N ↵) = ON (1+↵/2) log(N ↵).
Note that this bound holds for AGD since the condition number N := (M + cVN )/(cVN ) of the
risk RN is of the order O(1/VN ) = O(N ↵).
4.2 Adaptive Sample Size SVRG (Ada SVRG)
For the adaptive sample size mechanism presented in Section 3  we can also use linearly conver-
gent stochastic methods such as stochastic variance reduced gradient (SVRG) in [10] to update the
iterates. The SVRG method succeeds in reducing the computational complexity of deterministic
ﬁrst-order methods by computing a single gradient per iteration and using a delayed version of the
average gradient to update the iterates. Indeed  we can exploit the idea of SVRG to develop low
computational complexity adaptive sample size methods to improve the performance of determin-
istic adaptive sample size algorithms. Moreover  the adaptive sample size variant of SVRG (Ada
SVRG) enhances the proven bounds for SVRG to solve ERM problems.
We proceed to extend the idea of adaptive sample size scheme to the SVRG algorithm. To do so 
consider wm as an iterate within the statistical accuracy  E[Rm(wm)  Rm(w⇤m)]  Vm  for a set
Sm which contains m samples. Consider sn and qn as the numbers of outer and inner loops for the
update of SVRG  respectively  when the size of the training set is n. Further  consider ˜w and ˆw as
the sequences of iterates for the outer and inner loops of SVRG  respectively. In the adaptive sample

6

size SVRG (Ada SVRG) method to minimize the risk Rn  we set the approximate solution wm for
the previous ERM problem as the initial iterate for the outer loop  i.e.  ˜w0 = wm. Then  the outer
loop update which contains gradient computation is deﬁned as

rRn( ˜wk) =

k = 0  . . .   sn  1 
and the inner loop for the k-th outer loop contains qn iterations of the following update

rf ( ˜wk  zi) + cVn ˜wk

for

1
n

nXi=1

(15)

ˆwt+1 k = ˆwt k  ⌘n (rf ( ˆwt k  zit) + cVn ˆwt k  rf ( ˜wk  zit)  cVn ˜wk + rRn( ˜wk))  

(16)
for t = 0  . . .   qn  1  where the iterates for the inner loop at step k are initialized as ˆw0 k = ˜wk 
and it is index of the function which is chosen unﬁrmly at random from the set {1  . . .   n} at the
inner iterate t. The outcome of each inner loop ˆwqn k is used as the variable for the next outer loop 
i.e.  ˜wk+1 = ˆwqn k. We deﬁne the outcome of sn outer loops ˜wsn as the approximate solution for
the risk Rn  i.e.  wn = ˜wsn.
In the following theorem we derive a bound on the number of required outer loops sn to ensure that
the variable wn generated by the updates in (15) and (16) will be in the statistical accuracy of Rn in
expectation  i.e.  E[Rn(wn)  Rn(w⇤n)]  Vn. To reach the smallest possible lower bound for sn 
we properly choose the number of inner loop iterations qn and the learning rate ⌘n.
Theorem 4. Consider the variable wm as a Vm-optimal solution of the risk Rm  i.e.  a solution such
that E[Rm(wm)  Rm(w⇤m)]  Vm  where Vm = O(1/m↵). Consider the sets Sm ⇢S n ⇢T
such that n = 2m  and suppose Assumption 1 holds. Further  set the number of inner loop iterations
as qn = n and the learning rate as ⌘n = 0.1/(M + cVn). Then  the variable wn generated based
on the update of Ada SVRG in (15)-(16) satisﬁes E[Rn(wn)  Rn(w⇤n)]  Vn if the number of
iterations sn is larger than

(17)
Moreover  to reach the statistical accuracy VN of the full training set T the overall computational
complexity of Ada SVRG is given by

c

sn  log2h3 ⇥ 2↵ + (2↵  1)⇣2 +
4N log2h3 ⇥ 2↵ + (2↵  1)⇣2 +

2kw⇤k2⌘i.
2kw⇤k2⌘i .

c

(18)

Proof. See Section 7.4.

The result in (17) shows that the minimum number of outer loop iterations for Ada SVRG is equal to
sn = blog2[3 ⇥ 2↵ + (2↵  1)(2 + (c/2)kw⇤k2)]c+1. This bound leads to the result in (18) which
shows that the overall computational complexity of Ada SVRG to reach the statistical accuracy of
the full training set T is of the order O(N ). This bound not only improves the bound O(N 1+↵/2)
for Ada AGD  but also enhances the complexity of SVRG for reaching the same target accuracy
which is given by O((N + ) log(N ↵)) = O(N log(N ↵)).
5 Experiments

In this section  we compare the adaptive sample size versions of a group of ﬁrst-order methods  in-
cluding gradient descent (GD)  accelerated gradient descent (AGD)  and stochastic variance reduced
gradient (SVRG) with their standard (ﬁxed sample size) versions. In the main paper  we only use
the RCV1 dataset. Further numerical experiments on MNIST dataset can be found in Section 7.5 in
the supplementary material. We use N = 10  000 samples of the RCV1 dataset for the training set
and the remaining 10  242 as the test set. The number of features in each sample is p = 47  236. In
our experiments  we use logistic loss. The constant c should be within the order of gradients Lips-
chitz continuity constant M  and  therefore  we set it as c = 1 since the samples are normalized and
M = 1. The size of the initial training set for adaptive methods is m0 = 400. In our experiments
we assume ↵ = 0.5 and therefore the added regularization term is (1/pn)kwk2.
The plots in Figure 1 compare the suboptimality of GD  AGD  and SVRG with their adaptive sample
size versions. As our theoretical results suggested  we observe that the adaptive sample size scheme
reduces the overall computational complexity of all of the considered linearly convergent ﬁrst-order

7

2

10

1

10

0

10

y
t
i
l
a
m

i
t
p
o
b
u
S

-1

10

-2

10

0

GD
Ada GD

20

40

60

80

100

2

10

1

10

0

10

y
t
i
l
a
m

i
t
p
o
b
u
S

-1

10

-2

10

0

AGD
Ada AGD

20

40

60

80

100

2

10

1

10

0

10

-1

10

-2

10

y
t
i
l
a
m

i
t
p
o
b
u
S

-3

10

0

SVRG
Ada SVRG

1

2

3

4

5

6

Number of eﬀective passes

Number of eﬀective passes

Number of eﬀective passes

Figure 1: Suboptimality vs. number of effective passes for RCV1 dataset with regularization of O(1/pn).

r
o
r
r
e

t
s
e
T

50%

45%

40%

35%

30%

25%

20%

15%

10%

 5%

0

r
o
r
r
e

t
s
e
T

50%

45%

40%

35%

30%

25%

20%

15%

10%

 5%

0

GD
Ada GD

20

40

60

80

100

AGD
Ada AGD

20

40

60

80

100

r
o
r
r
e

t
s
e
T

50%

45%

40%

35%

30%

25%

20%

15%

10%

 5%

0

SVRG
Ada SVRG

1

2

3

4

5

6

Number of eﬀective passes

Number of eﬀective passes

Number of eﬀective passes

Figure 2: Test error vs. number of effective passes for RCV1 dataset with regularization of O(1/pn).

methods. If we compare the test errors of GD  AGD  and SVRG with their adaptive sample size
variants  we reach the same conclusion that the adaptive sample size scheme reduces the overall
computational complexity to reach the statistical accuracy of the full training set. In particular  the
left plot in Figure 2 shows that Ada GD approaches the minimum test error of 8% after 55 effective
passes  while GD can not improve the test error even after 100 passes. Indeed  GD will reach lower
test error if we run it for more iterations. The central plot in Figure 2 showcases that Ada AGD
reaches 8% test error about 5 times faster than AGD. This is as predicted by log(N ↵) = log(100) =
4.6. The right plot in Figure 2 illustrates a similar improvement for Ada SVRG. We have observed
similar performances for other datasets such as MNIST – see Section 7.5 in supplementary material.

6 Discussions

We presented an adaptive sample size scheme to improve the convergence guarantees for a class
of ﬁrst-order methods which have linear convergence rates under strong convexity and smoothness
assumptions. The logic behind the proposed adaptive sample size scheme is to replace the solution
of a relatively hard problem – the ERM problem for the full training set – by a sequence of relatively
easier problems – ERM problems corresponding to a subset of samples. Indeed  whenever m < n 
solving the ERM problems in (4) for loss Rm is simpler than the one for loss Rn because:

(i) The adaptive regularization term of order Vm makes the condition number of Rm smaller

than the condition number of Rn – which uses a regularizer of order Vn.

(ii) The approximate solution wm that we need to ﬁnd for Rm is less accurate than the approx-

imate solution wn we need to ﬁnd for Rn.

(iii) The computation cost of an iteration for Rm – e.g.  the cost of evaluating a gradient – is

lower than the cost of an iteration for Rn.

Properties (i)-(iii) combined with the ability to grow the sample size geometrically  reduce the over-
all computational complexity for reaching the statistical accuracy of the full training set. We par-
ticularized our results to develop adaptive (Ada) versions of AGD and SVRG. In both methods we
found a computational complexity reduction of order O(log(1/VN )) = O(log(N ↵)) which was
corroborated in numerical experiments. The idea and analysis of adaptive ﬁrst order methods apply
generically to any other approach with linear convergence rate (Theorem 2). The development of
sample size adaptation for sublinear methods is left for future research.

Acknowledgments
This research was supported by NSF CCF 1717120 and ARO W911NF1710438.

8

References
[1] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC 

2017.

[2] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[3] L´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-

STAT’2010  pages 177–186. Springer  2010.

[4] L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural Informa-
tion Processing Systems 20  Vancouver  British Columbia  Canada  December 3-6  2007  pages 161–168 
2007.

[5] Olivier Bousquet. Concentration inequalities and empirical processes theory applied to the analysis of

learning algorithms. PhD thesis  Ecole Polytechnique  2002.

[6] Hadi Daneshmand  Aur´elien Lucchi  and Thomas Hofmann. Starting small - learning with adaptive
sample sizes. In Proceedings of the 33nd International Conference on Machine Learning  ICML 2016 
New York City  NY  USA  pages 1463–1471  2016.

[7] Aaron Defazio. A simple practical accelerated method for ﬁnite sums. In Advances In Neural Information

Processing Systems  pages 676–684  2016.

[8] Aaron Defazio  Francis R. Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information Process-
ing Systems 27  Montreal  Quebec  Canada  pages 1646–1654  2014.

[9] Roy Frostig  Rong Ge  Sham M. Kakade  and Aaron Sidford. Competing with the empirical risk mini-
mizer in a single pass. In Proceedings of The 28th Conference on Learning Theory  COLT 2015  Paris 
France  July 3-6  2015  pages 728–763  2015.

[10] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduc-
tion. In Advances in Neural Information Processing Systems 26. Lake Tahoe  Nevada  United States. 
pages 315–323  2013.

[11] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization. In

Advances in Neural Information Processing Systems  pages 3384–3392  2015.

[12] Aryan Mokhtari  Hadi Daneshmand  Aur´elien Lucchi  Thomas Hofmann  and Alejandro Ribeiro. Adap-
tive Newton method for empirical risk minimization to statistical accuracy. In Advances in Neural Infor-
mation Processing Systems 29. Barcelona  Spain  pages 4062–4070  2016.

[13] Yurii Nesterov.

Introductory lectures on convex optimization: A basic course  volume 87. Springer

Science & Business Media  2013.

[14] Yurii Nesterov et al. Gradient methods for minimizing composite objective function. 2007.

[15] Nicolas Le Roux  Mark W. Schmidt  and Francis R. Bach. A stochastic gradient method with an expo-
nential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems
25. Lake Tahoe  Nevada  United States.  pages 2672–2680  2012.

[16] Shai Shalev-Shwartz  Ohad Shamir  Nathan Srebro  and Karthik Sridharan. Learnability  stability and

uniform convergence. The Journal of Machine Learning Research  11:2635–2670  2010.

[17] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss.

The Journal of Machine Learning Research  14:567–599  2013.

[18] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regu-

larized loss minimization. Mathematical Programming  155(1-2):105–145  2016.

[19] Vladimir Vapnik. The nature of statistical learning theory. Springer Science & Business Media  2013.

[20] Blake E Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. In

Advances in Neural Information Processing Systems  pages 3639–3647  2016.

9

,Vincent Vu
Juhee Cho
Jing Lei
Karl Rohe
Aryan Mokhtari
Alejandro Ribeiro