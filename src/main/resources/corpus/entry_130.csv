2019,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation,Reinforcement learning is effective in optimizing policies for recommender systems. Current solutions mostly focus on model-free approaches  which require frequent interactions with a real environment  and thus are expensive in model learning. Offline evaluation methods  such as importance sampling  can alleviate such limitations  but usually request a large amount of logged data and do not work well when the action space is large. In this work  we propose a model-based reinforcement learning solution which models the user-agent interaction for offline policy learning via a generative adversarial network. To reduce bias in the learnt policy  we use the discriminator to evaluate the quality of generated sequences and rescale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in identifying patterns from given offline data and learning policies based on the offline and generated data.,Model-Based Reinforcement Learning with

Adversarial Training for Online Recommendation

Xueying Baiâˆ—â€¡  Jian Guanâˆ—Â§  Hongning Wangâ€ 

Â§ Department of Computer Science and Technology  Tsinghua University

â€¡Department of Computer Science  Stony Brook University
â€  Department of Computer Science  University of Virginia

xubai@cs.stonybrook.edu  j-guan19@mails.tsinghua.edu.cn

hw5x@virginia.edu

Abstract

Reinforcement learning is well suited for optimizing policies of recommender
systems. Current solutions mostly focus on model-free approaches  which require
frequent interactions with the real environment  and thus are expensive in model
learning. Ofï¬‚ine evaluation methods  such as importance sampling  can alleviate
such limitations  but usually request a large amount of logged data and do not
work well when the action space is large. In this work  we propose a model-based
reinforcement learning solution which models user-agent interaction for ofï¬‚ine
policy learning via a generative adversarial network. To reduce bias in the learned
model and policy  we use a discriminator to evaluate the quality of generated data
and scale the generated rewards. Our theoretical analysis and empirical evaluations
demonstrate the effectiveness of our solution in learning policies from the ofï¬‚ine
and generated data.

Introduction

1
Recommender systems have been successful in connecting users with their most interested content in
a variety of application domains. However  because of usersâ€™ diverse interest and behavior patterns 
only a small fraction of items are presented to each user  with even less feedback recorded. This gives
relatively little information on user-system interactions for such a large state and action space [2]  and
thus brings considerable challenges to construct a useful recommendation policy based on historical
interactions. It is important to develop solutions to learn usersâ€™ preferences from sparse user feedback
such as clicks and purchases [11  13] to further improve the utility of recommender systems.
Usersâ€™ interests can be short-term or long-term and reï¬‚ected by different types of feedback [35]. For
example  clicks are generally considered as short-term feedback which reï¬‚ects usersâ€™ immediate
interests during the interaction  while purchase reveals usersâ€™ long-term interests which usually
happen after several clicks. Considering both usersâ€™ short-term and long-term interests  we frame the
recommender system as a reinforcement learning (RL) agent  which aims to maximize usersâ€™ overall
long-term satisfaction without sacriï¬cing the recommendationsâ€™ short-term utility [28].
Classical model-free RL methods require collecting large quantities of data by interacting with
the environment  e.g.  a population of users. Therefore  without interacting with real users  a
recommender cannot easily probe for reward in previously unexplored regions in the state and action
space. However  it is prohibitively expensive for a recommender to interact with users for reward
and model updates  because bad recommendations (e.g.  for exploration) hurt user satisfaction and
increase the risk of user drop out. In this case  it is preferred for a recommender to learn a policy by
fully utilizing the logged data that is acquired from other policies (e.g.  previously deployed systems)

âˆ—Both authors contributed equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

instead of direct interactions with users. For this purpose  we take a model-based learning approach
in this work  in which we estimate a model of user behavior from the ofï¬‚ine data and use it to interact
with our learning agent to obtain an improved policy simultaneously.
Model-based RL has a strong advantage of being sample efï¬cient and helping reduce noise in ofï¬‚ine
data. However  such an advantage can easily diminish due to the inherent bias in its model approxi-
mation of the real environment. Moreover  dramatic changes in subsequent policy updates impose
the risk of decreased user satisfaction  i.e.  inconsistent recommendations across model updates.
To address these issues  we introduce adversarial training into a recommenderâ€™s policy learning
from ofï¬‚ine data. The discriminator is trained to differentiate simulated interaction trajectories from
real ones so as to debias the user behavior model and improve policy learning. To the best of our
knowledge  this is the ï¬rst work to explore adversarial training over a model-based RL framework for
recommendation. We theoretically and empirically demonstrate the value of our proposed solution in
policy evaluation. Together  the main contributions of our work are as follows:
â€¢ To avoid the high interaction cost  we propose a uniï¬ed solution to more effectively utilize
the logged ofï¬‚ine data with model-based RL algorithms  integrated via adversarial training.
It enables robust recommendation policy learning.

â€¢ The proposed model is veriï¬ed through theoretical analysis and extensive empirical eval-
uations. Experiment results demonstrate our solutionâ€™s better sample efï¬ciency over the
state-of-the-art baselines 2

2 Related Work
Deep RL for recommendation There have been studies utilizing deep RL solutions in news  music
and video recommendations [17  15  38]. However  most of the existing solutions are model-free
methods and thus do not explicitly model the agent-user interactions. In these methods  value-
based approaches  such as deep Q-learning [20]  present unique advantages such as seamless off-
policy learning  but are prone to instability with function approximation [30  19]. And the policyâ€™s
convergence in these algorithms is not well-studied. In contrast  policy-based methods such as policy
gradient [14] remain stable but suffer from data bias without real-time interactive control due to
learning and infrastructure constraints. Oftentimes  importance sampling [22] is adopted to address
the bias but instead results in huge variance [2]. In this work  we rely on a policy gradient based RL
approach  in particular  REINFORCE [34]; but we simultaneously estimate a user behavior model to
provide a reliable environment estimate so as to update our agent on policy.
Model-based RL Model-based RL algorithms incorporate a model of the environment to predict
rewards for unseen state-action pairs. It is known in general to outperform model-free solutions in
terms of sample complexity [7]  and has been applied successfully to control robotic systems both in
simulation and real world [5  18  21  6]. Furthermore  Dyna-Q [29  24] integrates model-free and
model-based RL to generate samples for learning in addition to the real interaction data. Gu et al.
[10] extended these ideas to neural network models  and Peng et al. [24] further apply the method on
task-completion dialogue policy learning. However  the most efï¬cient model-based algorithms have
used relatively simple function approximations  which actually have difï¬culties in high-dimensional
space with nonlinear dynamics and thus lead to huge approximation bias.
Ofï¬‚ine evaluation The problems of off-policy learning [22  25  26] and ofï¬‚ine policy evaluation are
generally pervasive and challenging in RL  and in recommender systems in particular. As a policy
evolves  so does the distribution under which the expectation of gradient is computed. Especially
in the scenario of recommender systems  where item catalogues and user behavior change rapidly 
substantial policy changes are required; and therefore it is not feasible to take the classic approaches
[27  1] to constrain the policy updates before new data is collected under an updated policy. Multiple
off-policy estimators leveraging inverse-propensity scores  capped inverse-propensity scores and
various variance control measures have been developed [33  32  31  8] for this purpose.
RL with adversarial training Yu et al. [36] propose SeqGAN to extend GANs with an RL-like gen-
erator for the sequence generation problem  where the reward signal is provided by the discriminator
at the end of each episode via a Monte Carlo sampling approach. The generator takes sequential
actions and learns the policy using estimated cumulative rewards. In our solution  the generator
consists of two components  i.e.  our recommendation agent and the user behavior model  and we

2Our implementation is available at https://github.com/JianGuanTHU/IRecGAN.

2

model the interactive process via adversarial training and policy gradient. Different from the sequence
generation task which only aims to generate sequences similar to the given observations  we leverage
adversarial training to help reduce bias in the user model and further reduce the variance in training
our agent. The agent learns from both the interactions with the user behavior model and those stored
in the logged ofï¬‚ine data. To the best of our knowledge  this is the ï¬rst work that utilizes adversarial
training for improving both model approximation and policy learning on ofï¬‚ine data.

3 Problem Statement

The problem is to learn a policy from ofï¬‚ine data such that when deployed online it maximizes
cumulative rewards collected from interactions with users. We address this problem with a model-
based reinforcement learning solution  which explicitly model usersâ€™ behavior patterns from data.
Problem A recommender is formed as a learning agent to generate actions under a policy  where
each action gives a recommendation list of k items. Every time through interactions between the
agent and the environment (i.e.  users of the system)  a set â„¦ of n sequences â„¦ = {Ï„1  ...  Ï„n}
is recorded  where Ï„i is the i-th sequence containing agent actions  user behaviors and rewards:
Ï„i = {(ai
t (e.g.  make a purchase) 
and ci
t (e.g.  click on a recommended
item). For simplicity  in the rest of paper  we drop the superscript i to represent a general sequence Ï„.
Based on the observed sequences  a policy Ï€ is learnt to maximize the expected cumulative reward

t is the associated user behavior corresponding to agentâ€™s action ai

t represents the reward on ai

1)  ...  (ai

t)}  ri

0)  (ai

0  ri

1  ci

1  ri

t  ci

t  ri

0  ci

EÏ„âˆ¼Ï€[(cid:80)T

t=0 rt]  where T is the end time of Ï„.

Assumption To narrow the scope of our discussion  we study a typical type of user behavior  i.e. 
clicks  and make following assumptions: 1) at each time a user must click on one item from the
recommendation list; 2) items not clicked in the recommendation list will not inï¬‚uence the userâ€™s
future behaviors; 3) rewards only relate to clicked items. For example  when taking the userâ€™s
purchase as reward  purchases can only happen in the clicked items.
Learning framework In a Markov Decision Process  an environment consists of a state set S  an
action set A  a state transition distribution P : S Ã— A Ã— S  and a reward function fr : S Ã— A â†’ R 
which maps a state-action pair to a real-valued scalar. In this paper  the environment is modeled as a
user behavior model U  and learnt from ofï¬‚ine log data. S is reï¬‚ected by the interaction history before
time t  and P captures the transition of user behaviors. In the meanwhile  based on the assumptions
mentioned above  at each time t  the environment generates userâ€™s click ct on items recommended by
an agent A in at based on his/her click probabilities under the current state; and the reward function
fr generates reward rt for the clicked item ct.
Our recommendation policy is learnt from both ofï¬‚ine data and data sampled from the learnt user
behavior model  i.e.  a model-based RL solution. We incorporate adversarial training in our model-
based policy learning to: 1) improve the user model to ensure the sampled data is close to true data
distribution; 2) utilize the discriminator to scale rewards from generated sequences to further reduce
bias in value estimation. Our proposed solution contains an interactive model constructed by U and
A  and an adversarial policy learning approach. We name the solution as Interactive Recommender
GAN  or IRecGAN in short. The overview of our proposed solution is shown in Figure 1.

4

Interactive Modeling for Recommendation

We present our interactive model for recommendation  which consists of two components: 1) the
user behavior model U that generates user clicks over the recommended items with corresponding
rewards; and 2) the agent A which generates recommendations according to its policy. U and A
interact with each other to generate user behavior sequences for adversarial policy learning.
User behavior model Given usersâ€™ click observations {c0  c1  ...  ctâˆ’1}  the user behavior model
U ï¬rst projects the clicked item into an embedding vector eu at each time 3. The state su
t can be
represented as a summary of click history  i.e.  su
tâˆ’1). We use a recurrent neural
network to model the state transition P on the user side  thus for the state su

t = hu(eu

1   ...eu

0   eu

t we have 

su
t = hu(su

tâˆ’1  eu

tâˆ’1) 

3As we can use different embeddings on the user side and agent side  we use the superscript u and a to

denote this difference accordingly.

3

Figure 1: Model overview of IRecGAN. A  U and D denote the agent model  user behavior model 
and discriminator  respectively. In IRecGAN  A and U interact with each other to generate recom-
mendation sequences that are close to the true data distribution  so as to jointly reduce bias in U and
improve the recommendation quality in A.

(cid:88)|at|

hu(Â· Â·) can be functions in the RNN family like GRU [4] and LSTM [12] cells. Given the action
at = {at(1)  ...at(k)}  i.e.  the top-k recommendations at time t  we compute the probability of click
among the recommended items via a softmax function 

t + bc)(cid:62)Eu

t   p(ct|su

j=1

i )/

t   Eu

exp(Vc
j)

Vc = (Wcsu

t   at) = exp(Vc

(1)
where Vc âˆˆ Rk is a transformed vector indicating the evaluated quality of each recommended item
at(i) under state su
t is the embedding matrix of recommended items  Wc is the click weight
matrix  and bc is the corresponding bias term. Under the assumption that target rewards only relate
to clicked items  the reward rt for (su
rt(su

(2)
where Wr is the reward weight matrix  br is the corresponding bias term  and fr is the reward
mapping function and can be set according to the reward deï¬nition in speciï¬c recommender systems.
For example  if we make rt the purchase of a clicked item ct  where rt = 1 if it is purchased and
rt = 0 otherwise  fr can be realized by a Sigmoid function with binary output.
Based on Eq (1) and (2)  taking categorical reward  the user behavior model U can be estimated from
the ofï¬‚ine data â„¦ via maximum likelihood estimation:

t   at) is calculated by:
t   at) = fr

(cid:0)(Wrsu

t + br)(cid:62)eu

(cid:1) 

t

log p(ci

t   ai

t) + Î»p log p(ri

t|sui

t|sui

t   ci

t) 

(3)

(cid:88)

(cid:88)Ti

Ï„iâˆˆâ„¦

t=0

max

where Î»p is a parameter balancing the loss between click prediction and reward prediction  and Ti is
the length of the observation sequence Ï„i. With a learnt user behavior model  user clicks and reward
on the recommendation list can be sampled from Eq (1) and (2) accordingly.
Agent The agent should take actions based on the environmentâ€™s provided states. However  in
practice  usersâ€™ states are not observable in a recommender system. Besides  as discussed in [23] 
the states for the agent to take actions may be different from those for users to generate clicks and
rewards. As a result  we build a different state model on the agent side in A to learn its states. Similar
tâˆ’1}  we model states on the
to that on the user side  given the projected click vectors {ea
t denotes the state maintained by the agent at time t 
agent side by sa
ha(Â· Â·) is the chosen RNN cell. The initial state sa
0 for the ï¬rst recommendation is drawn from a
distribution Ï. We simply denote it as s0 in the rest of our paper. We should note that although the
agent also models states based on usersâ€™ click history  it might create different state sequences than
those on the user side.
Based on the current state sa
items as its action at. The probability of item i to be included in at under the policy Ï€ is:

t   the agent generates a size-k recommendation list out of the entire set of

tâˆ’1)  where sa

t = ha(sa

tâˆ’1  ea

2  ...ea

0  ea

Ï€(i âˆˆ at|sa

t ) =

i sa
exp(Wa
j=1 exp(Wa

t + ba
i )
j sa

t + ba
j )

 

(4)

(cid:80)|C|

4

ğ¬1ğ‘¢ğ¬1ğ‘ğ‘0MLPğ‘0ğ‘1ğ¬2ğ‘¢ğ¬2ğ‘ğ‘1MLPğ‘1ğ‘2MCSearchğ’ŸDiscriminatorScoreğ¬0ğ‘¢ğ¬0ğ‘MLPğ‘0ğ’°ğ’œEnvironmentRewardğ’Ÿ(a) Interactive modeling and adversarial policy learning with a discriminator Real Offline Datağ‰Generated Dataà·œğ‰(b) Training of the discriminatorğ‘0ğ‘1ğ‘2ğ‘Ÿ0ğ‘Ÿ1â€¦â€¦â€¦â€¦i is the i-th row of the action weight matrix Wa  C is the entire set of recommendation
where Wa
candidates  and ba
i is the corresponding bias term. Following [2]  we generate at by sampling without
replacement according to Eq (4). Unlike [3]  we do not consider the combinatorial effect among the
k items by simply assuming the users will evaluate them independently (as indicated in Eq (1)).

0:tâˆ’1) by Eq (4)  Ë†ct = Uc(Ë†Ï„ c

the generated and ofï¬‚ine data. When generating Ë†Ï„0:t =(cid:8)(Ë†a0  Ë†c0  Ë†r0)  ...  (Ë†at  Ë†ct  Ë†rt)(cid:9) for t > 0  we

5 Adversarial Policy Learning
We use the policy gradient method REINFORCE [34] for the agentâ€™s policy learning  based on both
obtain Ë†at = A(Ë†Ï„ c
0:tâˆ’1  Ë†ct) by Eq (1).
Ï„ c represents clicks in the sequence Ï„ and (Ë†a0  Ë†c0  Ë†r0) is generated by sa
0 accordingly. The
generation of a sequence ends at the time t if Ë†ct = cend  where cend is a stopping symbol. The
distributions of generated and ofï¬‚ine data are denoted as g and data respectively. In the following
discussions  we do not explicitly differentiate Ï„ and Ë†Ï„ when the distribution of them is speciï¬ed.
Since we start the training of U from ofï¬‚ine data  it introduces inherent bias from the observations
and our speciï¬c modeling choices. The bias affects the sequence generation and thus may cause
biased value estimation. To reduce the effect of bias  we apply adversarial training to control the
training of both U and A. The discriminator is also used to rescale the generated rewards Ë†r for policy
learning. Therefore  the learning of agent A considers both sequence generation and target rewards.

0:tâˆ’1  Ë†at) by Eq (2)  and Ë†rt = Ur(Ë†Ï„ c
0 and su

5.1 Adversarial training

We leverage adversarial training to encourage our IRecGAN model to generate high-quality sequences
that capture intrinsic patterns in the real data distribution. A discriminator D is used to evaluate a given
sequence Ï„  where D(Ï„ ) represents the probability that Ï„ is generated from the real recommendation
environment. The discriminator can be estimated by minimizing the objective function:

âˆ’EÏ„âˆ¼data log(cid:0)D(Ï„ )(cid:1) âˆ’ EÏ„âˆ¼g log(cid:0)1 âˆ’ D(Ï„ )(cid:1).
(cid:26) 1

(5)
However  D only evaluates a completed sequence  and hence it cannot directly evaluate a partially
generated sequence at a particular time step t. Inspired by [36]  we utilize the Monte-Carlo tree
search algorithm with the roll-out policy constructed by U and A to get sequence generation score at
each time. At time t  the sequence generation score qD of Ï„0:t is deï¬ned as:
âˆˆ M CU  A(Ï„0:t; N )

(6)
where M CU  A(Ï„0:t; N ) is the set of N sequences sampled from the interaction between U and A.
Given the observations in ofï¬‚ine data  U should generate clicks and rewards that reï¬‚ect intrinsic
patterns of the real data distribution. Therefore  U should maximize the sequence generation objective
0   a0) Â· qD(Ï„0:0)]  which is the expected discriminator score for gen-
Esu
erating a sequence from the initial state. U may not generate clicks and rewards exactly the same as
those in ofï¬‚ine data  but the similarity of its generated data to ofï¬‚ine data is still an informative signal
to evaluate its sequence generation quality. By setting qD(Ï„0:t) = 1 at any time t for ofï¬‚ine data  we
extend this objective to include ofï¬‚ine data (it becomes the data likelihood function on ofï¬‚ine data).
Following [36]  based on Eq (1) and Eq (2)  the gradient of Uâ€™s objective can be derived as 

(cid:80)N
n=1 D(Ï„ n
D(Ï„0:T )

(a0 c0 r0)âˆ¼g U(c0  r0|su

0 âˆ¼Ï[(cid:80)

qD(Ï„0:t) =

t < T
t = T

)  Ï„ n

0:T

0:T

N

EÏ„âˆ¼{g data}

qD(Ï„0:t)âˆ‡Î˜u

t   at) + Î»p log pÎ˜u (rt|su

 

t=0

(7)
where Î˜u denotes the parameters of U and Î˜a denotes those of A. Based on our assumption 
even when U can already capture usersâ€™ true behavior patterns  it still depends on A to provide
appropriate recommendations to generate clicks and rewards that the discriminator will treat as
authentic. Hence  A and U are coupled in this adversarial training. To encourage A to provide
needed recommendations  we include qD(Ï„0:t) as a sequence generation reward for A at time t as
well. As qD(Ï„0:t) evaluates the overall generation quality of Ï„0:t  it ignores sequence generations
after t. To evaluate the quality of a whole sequence  we require A to maximize the cumulative

sequence generation reward EÏ„âˆ¼{g data}(cid:2)(cid:80)T

the observations in the interaction sequence  we approximate âˆ‡Î˜a
calculating the gradients. Putting these together  the gradient derived from sequence generations for
A is estimated as 

t=0 qD(Ï„0:t)(cid:3). Because A does not directly generate
(cid:0)(cid:80)T
t=0 qD(Ï„0:t)(cid:1) as 0 when
Î³t(cid:48)âˆ’tqD(Ï„0:t)(cid:1)âˆ‡Î˜a log Ï€Î˜a (ct âˆˆ at|sa
t )(cid:3).

EÏ„âˆ¼{g data}(cid:2)(cid:88)T

(cid:0)(cid:88)T

(8)

(cid:0) log pÎ˜u (ct|su

(cid:104)(cid:88)T

t   ct)(cid:1)(cid:105)

t=0

t(cid:48)=t

5

Based on our assumption that only the clicked items inï¬‚uence user behaviors  and U only generates
rewards on the clicked items  we use Ï€Î˜a (ct âˆˆ at|sa
t )  i.e.  A should
promote ct in its recommendation at time t. In practice  we add a discount factor Î³ < 1 when
calculating the cumulative rewards to reduce estimation variance [2].

t ) as an estimation of Ï€Î˜a (at|sa

5.2 Policy learning

cumulative reward EÏ„âˆ¼{g data}[RT ]  where RT = (cid:80)T

Because our adversarial training encourages IRecGAN to generate clicks and rewards with similar
patterns as ofï¬‚ine data  and we assume rewards only relate to the clicked items  we use ofï¬‚ine data
as well as generated data for policy learning. Given data Ï„0:T = {(a0  c0  r0)  ...  (aT   cT   rT )} 
including both ofï¬‚ine and generated data  the objective of the agent is to maximize the expected
In the generated data  due to the
difference in distributions of the generated and ofï¬‚ine sequences  the generated reward Ë†rt calculated
by Eq (2) might be biased. To reduce such bias  we utilize the sequence generation score in Eq (6)
to rescale the generated rewards: rs
t = qD(Ï„0:t)Ë†rt  and treat it as the reward for generated data. The
gradient of the objective is thus estimated by:

t=0 rt.

Rtâˆ‡Î˜a log Ï€Î˜a (ct âˆˆ at|sa

t=0

(9)
Rt is an approximation of RT with the discount factor Î³. Overall  the user behavior model U is
updated only by the sequence generation objective deï¬ned in Eq (7) on both ofï¬‚ine and generated
data; but the agent A is updated by both sequence generation and target rewards. Hence  the overall
reward for A at time t is qD(Ï„0:t)(1 + Î»rrt)  where Î»r is the weight for cumulative target rewards.
The overall gradient for A is thus:

t(cid:48)=t

Î³t(cid:48)âˆ’tqD(Ï„0:t)rt

EÏ„âˆ¼{g data}(cid:2)(cid:88)T

t )(cid:3)  Rt =

(cid:88)T

t âˆ‡Î˜a log Ï€Î˜a (ct âˆˆ at|sa
Ra

Î³t(cid:48)âˆ’tqD(Ï„0:t)(1 + Î»rrt) (10)

EÏ„âˆ¼{g data}(cid:2)(cid:88)T

t=0

t )(cid:3)  Ra

t =

(cid:88)T

t(cid:48)=t

6 Theoretical Analysis
For one iteration of policy learning in IRecGAN  we ï¬rst train the discriminator D with ofï¬‚ine data 
which follows Pdata and was generated by an unknown logging policy  and the data generated by
IRecGAN under Ï€Î˜a with the distribution of g. When Î˜u and Î˜a are learnt  for a given sequence Ï„ 
by proposition 1 in [9]  the optimal discriminator D is Dâˆ—(Ï„ ) =
Sequence generation Both A and U contribute to the sequence generation in IRecGAN. U is updated
by the gradient in Eq (7) to maximize the sequence generation objective. At time t  the expected se-
quence generation reward for A on the generated data is: EÏ„0:tâˆ¼g[qD(Ï„0:t)] = EÏ„0:tâˆ¼g[D(Ï„0:T|Ï„0:t)].
The expected value on Ï„0:t is: EÏ„âˆ¼g[Vg] = EÏ„âˆ¼g
Given the optimal Dâˆ—  the sequence generation value can be written as:
Pdata(Ï„0:T|Ï„0:t)

(cid:2)(cid:80)T
t=0 qD(Ï„0:t)(cid:3) =(cid:80)T

(cid:2)D(Ï„0:T|Ï„0:t)(cid:3).

Pdata(Ï„ )+Pg(Ï„ ).

(cid:88)T

EÏ„0:tâˆ¼g

Pdata(Ï„ )

(cid:105)

(cid:104)

t=0

EÏ„âˆ¼g[Vg] =

EÏ„0:tâˆ¼g

t=0

Pdata(Ï„0:T|Ï„0:t) + Pg(Ï„0:T|Ï„0:t)

.

(11)

Maximizing each term in the summation of Eq (11) is an objective for the generator at time t in
GAN. According to [9]  the optimal solution for all such terms is Pg(Ï„0:T|s0) = Pdata(Ï„0:T|s0). It
means A can maximize the sequence generation value when it helps to generate sequences with the
same distribution as data. Besides the global optimal  Eq (11) also encourages A to reward each
Pg(Ï„0:T|Ï„0:t) = Pdata(Ï„0:T|Ï„0:t)  even if Ï„0:t is less likely to be generated from Pg. This prevents
IRecGAN to recommend items only considering usersâ€™ immediate preferences.
Value estimation The agent A should also be updated to maximize the expected value of target
rewards Va. To achieve this  we use discriminator D to rescale the estimation of Va on the generated
sequences  and we also combine ofï¬‚ine data to evaluate Va for policy Ï€Î˜a:

EÏ„0:tâˆ¼g

EÏ„âˆ¼Ï€Î˜a

[Va] = Î»1

(12)
where Ë†rt is the generated reward by U at time t and rt is the true reward. Î»1 and Î»2 represent the
ratio of generated data and ofï¬‚ine data during model training  and we require Î»1 + Î»2 = 1. Here we
simplify P (Ï„0:T|Ï„0:t) as P (Ï„0:t). As a result  there are three sources of biases in this value estimation:

Pdata(Ï„0:t) + Pg(Ï„0:t)

Ë†rt + Î»2

t=0

t=0

EÏ„0:tâˆ¼datart 

Pdata(Ï„0:t)

âˆ† = Ë†rt âˆ’ rt 

Î´1 = 1 âˆ’ PÏ€Î˜a

Î´2 = 1 âˆ’ PÏ€Î˜a

(Ï„0:t)/Pdata(Ï„0:t).

(cid:88)T

(cid:88)T

(Ï„0:t)/Pg(Ï„0:t) 

6

Based on different sources of biases  the expected value estimation in Eq (12) is:

T(cid:88)

(cid:16) PÏ€Î˜a

(Ï„0:t)
Pdata(Ï„0:t)

(cid:17)

rt

+ Î´2

EÏ„0:tâˆ¼Ï€Î˜a

(Î»1 âˆ’ wt)rt 

EÏ„âˆ¼Ï€Î˜a

[Va] =Î»1

T(cid:88)

t=0

EÏ„0:tâˆ¼g

T(cid:88)

(Ï„0:t)
PÏ€Î˜a
Pg(Ï„0:t)

âˆ† + rt

2 âˆ’ (Î´1 + Î´2)

T(cid:88)

EÏ„0:tâˆ¼data

+ Î»2

EÏ„0:tâˆ¼dataÎ»2Î´2rt âˆ’ T(cid:88)

t=0

=V

Ï€Î˜a
a

+

EÏ„0:tâˆ¼Ï€Î˜a

wtâˆ† +

t=0

t=0

t=0

Î»1

2âˆ’(Î´1+Î´2). âˆ† and Î´1 come from the bias of user behavior model U. Because the
where wt =
adversarial training helps improve U to capture real data patterns  it decreases âˆ† and Î´2. Because we
can adjust the sampling ratio Î»1 to reduce wt  wtâˆ† can be small. The sequence generation rewards
for agent A encourage distribution g to be close to data. Because Î´2 = 1 âˆ’ PÏ€Î˜a
Â· Pg(Ï„0:t)
Pdata(Ï„0:t) 
the bias Î´2 can also be reduced. It shows our method has a bias controlling effect.

Pg(Ï„0:t)

(Ï„0:t)

7 Experiments

In our theoretical analysis  we can ï¬nd that reducing the model bias improves value estimation 
and therefore improves policy learning. In this section  we conduct empirical evaluations on both
real-world and synthetic datasets to demonstrate that our solution can effectively model the pattern of
data for better recommendations  compared with state-of-the-art solutions.

7.1 Simulated Online Test

Subject to the difï¬culty of deploying a recommender system with real users for online evaluation  we
use simulation-based studies to ï¬rst investigate the effectiveness of our approach following [37  3].
Simulated Environment We synthesize an MDP to simulate an online recommendation environment.
It has m states and n items for recommendation  with a randomly initialized transition probability
matrix P (s âˆˆ S|aj âˆˆ A  si âˆˆ S). Under each state si  an item ajâ€™s reward r(aj âˆˆ A|si âˆˆ S) is
uniformly sampled from the range of 0 to 1. During the interaction  given a recommendation list
including k items selected from the whole item set by an agent  the simulator ï¬rst samples an item
proportional to its ground-truth reward under the current state si as the click candidate. Denote
the sampled item as aj  a Bernoulli experiment is performed on this item with r(aj) as the success
probability; then the simulator moves to the next state according to the state transition probability
p(s|aj  si). A special state s0 is used to initialize all the sessions  which do not stop until the Bernoulli
experiment fails. The immediate reward is 1 if the session continues to the next step; otherwise 0. In
our experiment  m  n and k are set to 10  50 and 10 respectively.
Ofï¬‚ine Data Generation We generate ofï¬‚ine recommendation logs denoted by doff with the simula-
tor. The bias and variance in doff are especially controlled by changing the logging policy and the size
of doff. We adopt three different logging policies: 1) uniformly random policy Ï€random  2) maximum
reward policy Ï€max  3) mixed reward policy Ï€mix. Speciï¬cally  Ï€max recommends the top k items with
the highest ground-truth reward under the current simulator state at each step  while Ï€mix randomly
selects k items with either the top 20%-50% ground-truth reward or the highest ground-truth reward
under a given state. In the meanwhile  we vary the size of data in doff from 200 to 10 000.
Baselines We compared our IRecGAN with the following baselines: 1) LSTM: only the user
behavior model trained on ofï¬‚ine data; 2) PG: only the agent model trained by policy gradient on
ofï¬‚ine data; 3) LSTMD: the user behavior model in IRecGAN  updated by adversarial training.
Experiment Settings The hyper-parameters in all models are set as follows: the item embedding
dimension is set to 50  the discount factor Î³ in value calculation is set to 0.9  the scale factors Î»r and
Î»p are set to 3 and 1. We use 2-layer LSTM units with 512-dimension hidden states. The ratio of
generated training samples and ofï¬‚ine data for each training epoch is set to 1:10. We use an RNN
based discriminator in all experiments with details provided in the appendix.
Online Evaluation After training our models and baselines on doff  we deploy the learned policy to
interact with the simulator for online evaluation. We calculated coverage@r to measure the proportion
of the true top r relevant items that are ranked in the top k recommended items by a model across
all time steps (details in the appendix). The results of coverage@r under different conï¬gurations
of ofï¬‚ine data generation are reported in Figure 2. Under Ï€random  coverage@r of all algorithms are
relatively low when r is large and the difference in overall performance between behavior and agent

7

(a) Ï€random(200)

(b) Ï€random(10  000)

(c) Ï€max(10  000)

(d) Ï€mix(10  000)

Figure 2: Online evaluation results of coverage@r and cumulative rewards.

Figure 3: Online learning results of coverage@1 and coverage@10.

models is not very large. This suggests the difï¬culty of recognizing high reward items under Ï€random 
because every item has an equal chance to be observed (i.e.  full exploration) especially with a small
size of ofï¬‚ine data. However  under Ï€max and Ï€mix  when the high reward items can be sufï¬ciently
learned  user behavior models (LSTM  LSTMD) fail to capture the overall preferred items while agent
models (PG  IRecGAN) are stable to the change of r. IRecGAN shows its advantage especially under
Ï€mix  which requires a model to differentiate top relevant items from those with moderate reward. It
has close coverage@r to LSTM when r is small and better captures usersâ€™ overall preferences when
user behavior models fail seriously. When rewards can not be sufï¬ciently learned (Fig 2(a))  our
mechanism can strengthen the inï¬‚uence of truly learned rewards (LSTMD outperforms LSTM when
r is small) but may also underestimate some bias. However  when it is feasible to estimate the reward
generation (Fig 2(b)(c)(d))  both LSTMD and IRecGAN outperform baselines in coverage@r under
the help of generating samples via adversarial training.
The average cumulative rewards are also reported in the rightmost bars of Figure 2. They are calculated
by generating 1000 sequences with the environment and take the average of their cumulative rewards.
IRecGAN has a larger average cumulative reward than other methods under all conï¬gurations except
Ï€random with 10 000 ofï¬‚ine sequences. Under Ï€random(10  000) IRecGAN outperforms PG but not
LSTMD. The low cumulative reward of PG under Ï€random indicates that the transition probabilities
conditioned on high rewarded items may not be sufï¬ciently learned under the random ofï¬‚ine policy.
Online Learning To evaluate our modelâ€™s effectiveness in a more practical setting  we execute online
and ofï¬‚ine learning alternately. Speciï¬cally  we separate the learning into two stages: ï¬rst  the
agents can directly interact with the simulator to update their policies  and we only allow them to
generate 200 sequences in this stage; then they turn to the ofï¬‚ine stage to reuse their generated data
for ofï¬‚ine learning. We iterate the two stages and record their performance in the online learning
stage. We compare with the following baselines: 1) PG-online with only online learning  2) PG-
online&ofï¬‚ine with online learning and reusing the generated data via policy gradient for ofï¬‚ine

8

12345678910rewardr0.30.40.50.6coverage@rLSTMLSTMDPGIRecGAN2.93.03.13.23.33.43.53.6reward12345678910rewardr0.400.450.500.550.600.65coverage@rLSTMLSTMDPGIRecGAN3.03.23.43.63.84.04.2reward12345678910rewardr0.30.40.50.60.7coverage@rLSTMLSTMDPGIRecGAN3.03.23.43.63.84.04.2reward12345678910rewardr0.30.40.50.6coverage@rLSTMLSTMDPGIRecGAN2.93.03.13.23.33.43.53.6reward05101520iteration0.150.200.250.300.350.40coverage@1LSTM-offlinePG-onlinePG-online&offlineIRecGAN-online&offline05101520iteration0.150.200.250.300.35coverage@10LSTM-offlinePG-onlinePG-online&offlineIRecGAN-online&offlinelearning  and 3) LSTM-ofï¬‚ine with only ofï¬‚ine learning. We train all the models from scratch and
report the performance of coverage@1 and coverage@10 over 20 iterations in Figure 3. We can
observe that LSTM-ofï¬‚ine performs worse than other RL methods with ofï¬‚ine learning  especially
in the later stage  due to its lack of exploration. PG-online improves slowly as it does not reuse the
generated data. Compared with PG-online&ofï¬‚ine  IRecGAN has better convergence and coverage
because of its reduced value estimation bias. We also ï¬nd that coverage@10 is harder to improve.
The key reason is that as the model identiï¬es the items with high rewards  it tends to recommend
them more often. This gives less relevant items less chance to be explored  which is similar to our
online evaluation experiments under Ï€max and Ï€mix. Our model-based RL training alleviates this
bias to a certain extent by generating more training sequences  but it cannot totally alleviate it. This
reminds us to focus on explore-exploit trade-off in model-based RL in our future work.

7.2 Real-world Data Ofï¬‚ine Test
We use a large-scale real-world recommendation dataset from CIKM Cup 2016 to evaluate the
effectiveness of our proposed solution for ofï¬‚ine reranking. Sessions of length 1 or longer than 40
and items that have never been clicked are ï¬ltered out. We selected the top 40 000 most popular
items into the recommendation candidate set  and randomly selected 65 284/1 718/1 720 sessions
for training/validation/testing. The average length of sessions is 2.81/2.80/2.77 respectively; and the
ratio of clicks which lead to purchases is 2.31%/2.46%/2.45%. We followed the same model setting
as in our simulation-based study in this experiment. To understand of the effect of different data
separation strategies on RL model training and test  we also provide a comparison of performances
under different data separation strategies in the appendix.
Baselines In addition to the baselines we compared in our simulation-based study  we also include
the following state-of-the-art solutions for recommendation: 1). PGIS: the agent model estimated
with importance sampling on ofï¬‚ine data to reduce bias; 2). AC: an LSTM model whose setting is
the same as our agent model but trained with actor-critic algorithm [16] to reduce variance; 3). PGU:
the agent model trained using ofï¬‚ine and generated data  without adversarial training; 4). ACU: AC
model trained with both ofï¬‚ine and generated data  without adversarial training.
Evaluation Metrics All the models were applied to rerank the given recommendation list at each
step of testing sessions in ofï¬‚ine data. We used Precision@k (P@1 and P@10) to compare different
modelsâ€™ recommendation performance  where we deï¬ne the clicked items as relevant. Because the
logged recommendation list was not ordered  we cannot assess the logging policyâ€™s performance here.

Model

P@10 (%)
P@1 (%)

Table 1: Rerank evaluation on real-world dataset with random splitting.
LSTM
ACU
32.43Â±0.22
32.89Â±0.50
6.63Â± 0.29
8.20Â±0.65

LSTMD
33.42Â±0.40
8.55Â±0.63

31.93Â±0.17
6.54Â±0.19

PGIS

28.13Â±0.45
4.61Â±0.73

PGU

34.12Â±0.52
6.44Â±0.56

PG

33.28Â±0.71
6.25Â±0.14

AC

IRecGAN
35.06Â±0.48
6.79Â±0.44

Results The results of the ofï¬‚ine rerank evaluation are reported in Table 1. With the help of
adversarial training  IRecGAN achieved encouraging P@10 improvement against all baselines. This
veriï¬es the effectiveness of our model-based reinforcement learning  especially its adversarial training
strategy for utilizing the ofï¬‚ine data with reduced bias. Speciï¬cally  PGIS did not perform as well
as PG partially because of the high variance introduced by importance sampling. PGU was able
to ï¬t the given data more accurately than PG by learning from the generated data  since there are
many items for recommendation and the collected data is limited. However  PGU performed worse
than IRecGAN because of the biased user behavior model. And with the help of the discriminator 
IRecGAN reduces the bias in the user behavior model to improve value estimation and policy learning.
This is also reï¬‚ected on its improved user behavior model: LSTMD outperformed LSTM  given both
of them are for user behavior modeling.

8 Conclusion
In this work  we developed a practical solution for utilizing ofï¬‚ine data to build a model-based
reinforcement learning solution for recommendation. We introduce adversarial training for joint user
behavior model learning and policy update. Our theoretical analysis shows our solutionâ€™s promise in
reducing bias; our empirical evaluations in both synthetic and real-world recommendation datasets
verify the effectiveness of our solution. Several directions left open in our work  including balancing
explore-exploit in policy learning with ofï¬‚ine data  incorporating richer structures in user behavior
modeling  and exploring the applicability of our solution in other off-policy learning scenarios  such
as conversational systems.

9

References
[1] Joshua Achiam  David Held  Aviv Tamar  and Pieter Abbeel. Constrained policy optimization.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
22â€“31. JMLR. org  2017.

[2] Minmin Chen  Alex Beutel  Paul Covington  Sagar Jain  Francois Belletti  and Ed H Chi. Top-k
off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining  pages 456â€“464. ACM  2019.

[3] Xinshi Chen  Shuang Li  Hui Li  Shaohua Jiang  Yuan Qi  and Le Song. Generative adversarial
user model for reinforcement learning based recommendation system. In Proceedings of the
36th International Conference on Machine Learning  volume 97  pages 1052â€“1061  2019.

[4] Junyoung Chung  Caglar Gulcehre  KyungHyun Cho  and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 
2014.

[5] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efï¬cient approach
to policy search. In Proceedings of the 28th International Conference on machine learning
(ICML-11)  pages 465â€“472  2011.

[6] Marc Peter Deisenroth  Carl Edward Rasmussen  and Dieter Fox. Learning to control a low-cost

manipulator using data-efï¬cient reinforcement learning. 2011.

[7] Marc Peter Deisenroth  Gerhard Neumann  Jan Peters  et al. A survey on policy search for

robotics. Foundations and Trends R(cid:13) in Robotics  2(1â€“2):1â€“142  2013.

[8] Alexandre Gilotte  ClÃ©ment CalauzÃ¨nes  Thomas Nedelec  Alexandre Abraham  and Simon
DollÃ©. Ofï¬‚ine a/b testing for recommender systems. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining  pages 198â€“206. ACM  2018.

[9] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672â€“2680  2014.

[10] Shixiang Gu  Timothy Lillicrap  Ilya Sutskever  and Sergey Levine. Continuous deep q-learning
In International Conference on Machine Learning  pages

with model-based acceleration.
2829â€“2838  2016.

[11] Xiangnan He  Hanwang Zhang  Min-Yen Kan  and Tat-Seng Chua. Fast matrix factorization
for online recommendation with implicit feedback. In Proceedings of the 39th International
ACM SIGIR conference on Research and Development in Information Retrieval  pages 549â€“558.
ACM  2016.

[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation  9(8):

1735â€“1780  1997.

[13] Yehuda Koren  Robert Bell  and Chris Volinsky. Matrix factorization techniques for recom-

mender systems. Computer  (8):30â€“37  2009.

[14] Reinforcement Learning. An introduction  richard s. sutton and andrew g. barto  1998.

[15] Elad Liebman  Maytal Saar-Tsechansky  and Peter Stone. Dj-mc: A reinforcement-learning
agent for music playlist recommendation. In Proceedings of the 2015 International Conference
on Autonomous Agents and Multiagent Systems  pages 591â€“599. International Foundation for
Autonomous Agents and Multiagent Systems  2015.

[16] Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa 
David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971  2015.

[17] Zhongqi Lu and Qiang Yang. Partially observable markov decision process for recommender

systems. arXiv preprint arXiv:1608.07793  2016.

10

[18] David Meger  Juan Camilo Gamboa Higuera  Anqi Xu  Philippe Giguere  and Gregory Dudek.
Learning legged swimming gaits from experience. In 2015 IEEE International Conference on
Robotics and Automation (ICRA)  pages 2332â€“2338. IEEE  2015.

[19] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602  2013.

[20] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[21] Jun Morimoto and Christopher G Atkeson. Minimax differential dynamic programming: An
application to robust biped walking. In Advances in neural information processing systems 
pages 1563â€“1570  2003.

[22] RÃ©mi Munos  Tom Stepleton  Anna Harutyunyan  and Marc Bellemare. Safe and efï¬cient
off-policy reinforcement learning. In Advances in Neural Information Processing Systems 
pages 1054â€“1062  2016.

[23] Junhyuk Oh  Satinder Singh  and Honglak Lee. Value prediction network. In Advances in

Neural Information Processing Systems  pages 6118â€“6128  2017.

[24] Baolin Peng  Xiujun Li  Jianfeng Gao  Jingjing Liu  Kam-Fai Wong  and Shang-Yu Su. Deep
dyna-q: Integrating planning for task-completion dialogue policy learning. arXiv preprint
arXiv:1801.06176  2018.

[25] Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department

Faculty Publication Series  page 80  2000.

[26] Doina Precup  Richard S Sutton  and Sanjoy Dasgupta. Off-policy temporal-difference learning

with function approximation. In ICML  pages 417â€“424  2001.

[27] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889â€“1897 
2015.

[28] Guy Shani  David Heckerman  and Ronen I Brafman. An mdp-based recommender system.

Journal of Machine Learning Research  6(Sep):1265â€“1295  2005.

[29] Richard S Sutton.

Integrated architectures for learning  planning  and reacting based on
approximating dynamic programming. In Machine Learning Proceedings 1990  pages 216â€“224.
Elsevier  1990.

[30] Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour. Policy gradient
In Advances in neural

methods for reinforcement learning with function approximation.
information processing systems  pages 1057â€“1063  2000.

[31] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization. Journal of Machine Learning Research  16(1):
1731â€“1755  2015.

[32] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual

learning. In advances in neural information processing systems  pages 3231â€“3239  2015.

[33] Philip Thomas and Emma Brunskill. Data-efï¬cient off-policy policy evaluation for reinforce-

ment learning. In International Conference on Machine Learning  pages 2139â€“2148  2016.

[34] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine learning  8(3-4):229â€“256  1992.

[35] Qingyun Wu  Hongning Wang  Liangjie Hong  and Yue Shi. Returning is believing: Optimizing
long-term user engagement in recommender systems. In Proceedings of the 2017 ACM on
Conference on Information and Knowledge Management  pages 1927â€“1936. ACM  2017.

11

[36] Lantao Yu  Weinan Zhang  Jun Wang  and Yong Yu. Seqgan: Sequence generative adversarial

nets with policy gradient. In Thirty-First AAAI Conference on Artiï¬cial Intelligence  2017.

[37] Xiangyu Zhao  Long Xia  Yihong Zhao  Dawei Yin  and Jiliang Tang. Model-based reinforce-

ment learning for whole-chain recommendations. arXiv preprint arXiv:1902.03987  2019.

[38] Guanjie Zheng  Fuzheng Zhang  Zihan Zheng  Yang Xiang  Nicholas Jing Yuan  Xing Xie 
and Zhenhui Li. Drn: A deep reinforcement learning framework for news recommendation.
In Proceedings of the 2018 World Wide Web Conference on World Wide Web  pages 167â€“176.
International World Wide Web Conferences Steering Committee  2018.

12

,Xueying Bai
Jian Guan
Hongning Wang