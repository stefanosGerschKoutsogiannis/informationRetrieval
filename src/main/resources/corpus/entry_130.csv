2019,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation,Reinforcement learning is effective in optimizing policies for recommender systems. Current solutions mostly focus on model-free approaches  which require frequent interactions with a real environment  and thus are expensive in model learning. Offline evaluation methods  such as importance sampling  can alleviate such limitations  but usually request a large amount of logged data and do not work well when the action space is large. In this work  we propose a model-based reinforcement learning solution which models the user-agent interaction for offline policy learning via a generative adversarial network. To reduce bias in the learnt policy  we use the discriminator to evaluate the quality of generated sequences and rescale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in identifying patterns from given offline data and learning policies based on the offline and generated data.,Model-Based Reinforcement Learning with

Adversarial Training for Online Recommendation

Xueying Bai∗‡  Jian Guan∗§  Hongning Wang†

§ Department of Computer Science and Technology  Tsinghua University

‡Department of Computer Science  Stony Brook University
† Department of Computer Science  University of Virginia

xubai@cs.stonybrook.edu  j-guan19@mails.tsinghua.edu.cn

hw5x@virginia.edu

Abstract

Reinforcement learning is well suited for optimizing policies of recommender
systems. Current solutions mostly focus on model-free approaches  which require
frequent interactions with the real environment  and thus are expensive in model
learning. Ofﬂine evaluation methods  such as importance sampling  can alleviate
such limitations  but usually request a large amount of logged data and do not
work well when the action space is large. In this work  we propose a model-based
reinforcement learning solution which models user-agent interaction for ofﬂine
policy learning via a generative adversarial network. To reduce bias in the learned
model and policy  we use a discriminator to evaluate the quality of generated data
and scale the generated rewards. Our theoretical analysis and empirical evaluations
demonstrate the effectiveness of our solution in learning policies from the ofﬂine
and generated data.

Introduction

1
Recommender systems have been successful in connecting users with their most interested content in
a variety of application domains. However  because of users’ diverse interest and behavior patterns 
only a small fraction of items are presented to each user  with even less feedback recorded. This gives
relatively little information on user-system interactions for such a large state and action space [2]  and
thus brings considerable challenges to construct a useful recommendation policy based on historical
interactions. It is important to develop solutions to learn users’ preferences from sparse user feedback
such as clicks and purchases [11  13] to further improve the utility of recommender systems.
Users’ interests can be short-term or long-term and reﬂected by different types of feedback [35]. For
example  clicks are generally considered as short-term feedback which reﬂects users’ immediate
interests during the interaction  while purchase reveals users’ long-term interests which usually
happen after several clicks. Considering both users’ short-term and long-term interests  we frame the
recommender system as a reinforcement learning (RL) agent  which aims to maximize users’ overall
long-term satisfaction without sacriﬁcing the recommendations’ short-term utility [28].
Classical model-free RL methods require collecting large quantities of data by interacting with
the environment  e.g.  a population of users. Therefore  without interacting with real users  a
recommender cannot easily probe for reward in previously unexplored regions in the state and action
space. However  it is prohibitively expensive for a recommender to interact with users for reward
and model updates  because bad recommendations (e.g.  for exploration) hurt user satisfaction and
increase the risk of user drop out. In this case  it is preferred for a recommender to learn a policy by
fully utilizing the logged data that is acquired from other policies (e.g.  previously deployed systems)

∗Both authors contributed equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

instead of direct interactions with users. For this purpose  we take a model-based learning approach
in this work  in which we estimate a model of user behavior from the ofﬂine data and use it to interact
with our learning agent to obtain an improved policy simultaneously.
Model-based RL has a strong advantage of being sample efﬁcient and helping reduce noise in ofﬂine
data. However  such an advantage can easily diminish due to the inherent bias in its model approxi-
mation of the real environment. Moreover  dramatic changes in subsequent policy updates impose
the risk of decreased user satisfaction  i.e.  inconsistent recommendations across model updates.
To address these issues  we introduce adversarial training into a recommender’s policy learning
from ofﬂine data. The discriminator is trained to differentiate simulated interaction trajectories from
real ones so as to debias the user behavior model and improve policy learning. To the best of our
knowledge  this is the ﬁrst work to explore adversarial training over a model-based RL framework for
recommendation. We theoretically and empirically demonstrate the value of our proposed solution in
policy evaluation. Together  the main contributions of our work are as follows:
• To avoid the high interaction cost  we propose a uniﬁed solution to more effectively utilize
the logged ofﬂine data with model-based RL algorithms  integrated via adversarial training.
It enables robust recommendation policy learning.

• The proposed model is veriﬁed through theoretical analysis and extensive empirical eval-
uations. Experiment results demonstrate our solution’s better sample efﬁciency over the
state-of-the-art baselines 2

2 Related Work
Deep RL for recommendation There have been studies utilizing deep RL solutions in news  music
and video recommendations [17  15  38]. However  most of the existing solutions are model-free
methods and thus do not explicitly model the agent-user interactions. In these methods  value-
based approaches  such as deep Q-learning [20]  present unique advantages such as seamless off-
policy learning  but are prone to instability with function approximation [30  19]. And the policy’s
convergence in these algorithms is not well-studied. In contrast  policy-based methods such as policy
gradient [14] remain stable but suffer from data bias without real-time interactive control due to
learning and infrastructure constraints. Oftentimes  importance sampling [22] is adopted to address
the bias but instead results in huge variance [2]. In this work  we rely on a policy gradient based RL
approach  in particular  REINFORCE [34]; but we simultaneously estimate a user behavior model to
provide a reliable environment estimate so as to update our agent on policy.
Model-based RL Model-based RL algorithms incorporate a model of the environment to predict
rewards for unseen state-action pairs. It is known in general to outperform model-free solutions in
terms of sample complexity [7]  and has been applied successfully to control robotic systems both in
simulation and real world [5  18  21  6]. Furthermore  Dyna-Q [29  24] integrates model-free and
model-based RL to generate samples for learning in addition to the real interaction data. Gu et al.
[10] extended these ideas to neural network models  and Peng et al. [24] further apply the method on
task-completion dialogue policy learning. However  the most efﬁcient model-based algorithms have
used relatively simple function approximations  which actually have difﬁculties in high-dimensional
space with nonlinear dynamics and thus lead to huge approximation bias.
Ofﬂine evaluation The problems of off-policy learning [22  25  26] and ofﬂine policy evaluation are
generally pervasive and challenging in RL  and in recommender systems in particular. As a policy
evolves  so does the distribution under which the expectation of gradient is computed. Especially
in the scenario of recommender systems  where item catalogues and user behavior change rapidly 
substantial policy changes are required; and therefore it is not feasible to take the classic approaches
[27  1] to constrain the policy updates before new data is collected under an updated policy. Multiple
off-policy estimators leveraging inverse-propensity scores  capped inverse-propensity scores and
various variance control measures have been developed [33  32  31  8] for this purpose.
RL with adversarial training Yu et al. [36] propose SeqGAN to extend GANs with an RL-like gen-
erator for the sequence generation problem  where the reward signal is provided by the discriminator
at the end of each episode via a Monte Carlo sampling approach. The generator takes sequential
actions and learns the policy using estimated cumulative rewards. In our solution  the generator
consists of two components  i.e.  our recommendation agent and the user behavior model  and we

2Our implementation is available at https://github.com/JianGuanTHU/IRecGAN.

2

model the interactive process via adversarial training and policy gradient. Different from the sequence
generation task which only aims to generate sequences similar to the given observations  we leverage
adversarial training to help reduce bias in the user model and further reduce the variance in training
our agent. The agent learns from both the interactions with the user behavior model and those stored
in the logged ofﬂine data. To the best of our knowledge  this is the ﬁrst work that utilizes adversarial
training for improving both model approximation and policy learning on ofﬂine data.

3 Problem Statement

The problem is to learn a policy from ofﬂine data such that when deployed online it maximizes
cumulative rewards collected from interactions with users. We address this problem with a model-
based reinforcement learning solution  which explicitly model users’ behavior patterns from data.
Problem A recommender is formed as a learning agent to generate actions under a policy  where
each action gives a recommendation list of k items. Every time through interactions between the
agent and the environment (i.e.  users of the system)  a set Ω of n sequences Ω = {τ1  ...  τn}
is recorded  where τi is the i-th sequence containing agent actions  user behaviors and rewards:
τi = {(ai
t (e.g.  make a purchase) 
and ci
t (e.g.  click on a recommended
item). For simplicity  in the rest of paper  we drop the superscript i to represent a general sequence τ.
Based on the observed sequences  a policy π is learnt to maximize the expected cumulative reward

t is the associated user behavior corresponding to agent’s action ai

t represents the reward on ai

1)  ...  (ai

t)}  ri

0)  (ai

0  ri

1  ci

1  ri

t  ci

t  ri

0  ci

Eτ∼π[(cid:80)T

t=0 rt]  where T is the end time of τ.

Assumption To narrow the scope of our discussion  we study a typical type of user behavior  i.e. 
clicks  and make following assumptions: 1) at each time a user must click on one item from the
recommendation list; 2) items not clicked in the recommendation list will not inﬂuence the user’s
future behaviors; 3) rewards only relate to clicked items. For example  when taking the user’s
purchase as reward  purchases can only happen in the clicked items.
Learning framework In a Markov Decision Process  an environment consists of a state set S  an
action set A  a state transition distribution P : S × A × S  and a reward function fr : S × A → R 
which maps a state-action pair to a real-valued scalar. In this paper  the environment is modeled as a
user behavior model U  and learnt from ofﬂine log data. S is reﬂected by the interaction history before
time t  and P captures the transition of user behaviors. In the meanwhile  based on the assumptions
mentioned above  at each time t  the environment generates user’s click ct on items recommended by
an agent A in at based on his/her click probabilities under the current state; and the reward function
fr generates reward rt for the clicked item ct.
Our recommendation policy is learnt from both ofﬂine data and data sampled from the learnt user
behavior model  i.e.  a model-based RL solution. We incorporate adversarial training in our model-
based policy learning to: 1) improve the user model to ensure the sampled data is close to true data
distribution; 2) utilize the discriminator to scale rewards from generated sequences to further reduce
bias in value estimation. Our proposed solution contains an interactive model constructed by U and
A  and an adversarial policy learning approach. We name the solution as Interactive Recommender
GAN  or IRecGAN in short. The overview of our proposed solution is shown in Figure 1.

4

Interactive Modeling for Recommendation

We present our interactive model for recommendation  which consists of two components: 1) the
user behavior model U that generates user clicks over the recommended items with corresponding
rewards; and 2) the agent A which generates recommendations according to its policy. U and A
interact with each other to generate user behavior sequences for adversarial policy learning.
User behavior model Given users’ click observations {c0  c1  ...  ct−1}  the user behavior model
U ﬁrst projects the clicked item into an embedding vector eu at each time 3. The state su
t can be
represented as a summary of click history  i.e.  su
t−1). We use a recurrent neural
network to model the state transition P on the user side  thus for the state su

t = hu(eu

1   ...eu

0   eu

t we have 

su
t = hu(su

t−1  eu

t−1) 

3As we can use different embeddings on the user side and agent side  we use the superscript u and a to

denote this difference accordingly.

3

Figure 1: Model overview of IRecGAN. A  U and D denote the agent model  user behavior model 
and discriminator  respectively. In IRecGAN  A and U interact with each other to generate recom-
mendation sequences that are close to the true data distribution  so as to jointly reduce bias in U and
improve the recommendation quality in A.

(cid:88)|at|

hu(· ·) can be functions in the RNN family like GRU [4] and LSTM [12] cells. Given the action
at = {at(1)  ...at(k)}  i.e.  the top-k recommendations at time t  we compute the probability of click
among the recommended items via a softmax function 

t + bc)(cid:62)Eu

t   p(ct|su

j=1

i )/

t   Eu

exp(Vc
j)

Vc = (Wcsu

t   at) = exp(Vc

(1)
where Vc ∈ Rk is a transformed vector indicating the evaluated quality of each recommended item
at(i) under state su
t is the embedding matrix of recommended items  Wc is the click weight
matrix  and bc is the corresponding bias term. Under the assumption that target rewards only relate
to clicked items  the reward rt for (su
rt(su

(2)
where Wr is the reward weight matrix  br is the corresponding bias term  and fr is the reward
mapping function and can be set according to the reward deﬁnition in speciﬁc recommender systems.
For example  if we make rt the purchase of a clicked item ct  where rt = 1 if it is purchased and
rt = 0 otherwise  fr can be realized by a Sigmoid function with binary output.
Based on Eq (1) and (2)  taking categorical reward  the user behavior model U can be estimated from
the ofﬂine data Ω via maximum likelihood estimation:

t   at) is calculated by:
t   at) = fr

(cid:0)(Wrsu

t + br)(cid:62)eu

(cid:1) 

t

log p(ci

t   ai

t) + λp log p(ri

t|sui

t|sui

t   ci

t) 

(3)

(cid:88)

(cid:88)Ti

τi∈Ω

t=0

max

where λp is a parameter balancing the loss between click prediction and reward prediction  and Ti is
the length of the observation sequence τi. With a learnt user behavior model  user clicks and reward
on the recommendation list can be sampled from Eq (1) and (2) accordingly.
Agent The agent should take actions based on the environment’s provided states. However  in
practice  users’ states are not observable in a recommender system. Besides  as discussed in [23] 
the states for the agent to take actions may be different from those for users to generate clicks and
rewards. As a result  we build a different state model on the agent side in A to learn its states. Similar
t−1}  we model states on the
to that on the user side  given the projected click vectors {ea
t denotes the state maintained by the agent at time t 
agent side by sa
ha(· ·) is the chosen RNN cell. The initial state sa
0 for the ﬁrst recommendation is drawn from a
distribution ρ. We simply denote it as s0 in the rest of our paper. We should note that although the
agent also models states based on users’ click history  it might create different state sequences than
those on the user side.
Based on the current state sa
items as its action at. The probability of item i to be included in at under the policy π is:

t   the agent generates a size-k recommendation list out of the entire set of

t−1)  where sa

t = ha(sa

t−1  ea

2  ...ea

0  ea

π(i ∈ at|sa

t ) =

i sa
exp(Wa
j=1 exp(Wa

t + ba
i )
j sa

t + ba
j )

 

(4)

(cid:80)|C|

4

𝐬1𝑢𝐬1𝑎𝑐0MLP𝑐0𝑐1𝐬2𝑢𝐬2𝑎𝑐1MLP𝑐1𝑐2MCSearch𝒟DiscriminatorScore𝐬0𝑢𝐬0𝑎MLP𝑐0𝒰𝒜EnvironmentReward𝒟(a) Interactive modeling and adversarial policy learning with a discriminator Real Offline Data𝝉Generated Dataො𝝉(b) Training of the discriminator𝑎0𝑎1𝑎2𝑟0𝑟1…………i is the i-th row of the action weight matrix Wa  C is the entire set of recommendation
where Wa
candidates  and ba
i is the corresponding bias term. Following [2]  we generate at by sampling without
replacement according to Eq (4). Unlike [3]  we do not consider the combinatorial effect among the
k items by simply assuming the users will evaluate them independently (as indicated in Eq (1)).

0:t−1) by Eq (4)  ˆct = Uc(ˆτ c

the generated and ofﬂine data. When generating ˆτ0:t =(cid:8)(ˆa0  ˆc0  ˆr0)  ...  (ˆat  ˆct  ˆrt)(cid:9) for t > 0  we

5 Adversarial Policy Learning
We use the policy gradient method REINFORCE [34] for the agent’s policy learning  based on both
obtain ˆat = A(ˆτ c
0:t−1  ˆct) by Eq (1).
τ c represents clicks in the sequence τ and (ˆa0  ˆc0  ˆr0) is generated by sa
0 accordingly. The
generation of a sequence ends at the time t if ˆct = cend  where cend is a stopping symbol. The
distributions of generated and ofﬂine data are denoted as g and data respectively. In the following
discussions  we do not explicitly differentiate τ and ˆτ when the distribution of them is speciﬁed.
Since we start the training of U from ofﬂine data  it introduces inherent bias from the observations
and our speciﬁc modeling choices. The bias affects the sequence generation and thus may cause
biased value estimation. To reduce the effect of bias  we apply adversarial training to control the
training of both U and A. The discriminator is also used to rescale the generated rewards ˆr for policy
learning. Therefore  the learning of agent A considers both sequence generation and target rewards.

0:t−1  ˆat) by Eq (2)  and ˆrt = Ur(ˆτ c
0 and su

5.1 Adversarial training

We leverage adversarial training to encourage our IRecGAN model to generate high-quality sequences
that capture intrinsic patterns in the real data distribution. A discriminator D is used to evaluate a given
sequence τ  where D(τ ) represents the probability that τ is generated from the real recommendation
environment. The discriminator can be estimated by minimizing the objective function:

−Eτ∼data log(cid:0)D(τ )(cid:1) − Eτ∼g log(cid:0)1 − D(τ )(cid:1).
(cid:26) 1

(5)
However  D only evaluates a completed sequence  and hence it cannot directly evaluate a partially
generated sequence at a particular time step t. Inspired by [36]  we utilize the Monte-Carlo tree
search algorithm with the roll-out policy constructed by U and A to get sequence generation score at
each time. At time t  the sequence generation score qD of τ0:t is deﬁned as:
∈ M CU  A(τ0:t; N )

(6)
where M CU  A(τ0:t; N ) is the set of N sequences sampled from the interaction between U and A.
Given the observations in ofﬂine data  U should generate clicks and rewards that reﬂect intrinsic
patterns of the real data distribution. Therefore  U should maximize the sequence generation objective
0   a0) · qD(τ0:0)]  which is the expected discriminator score for gen-
Esu
erating a sequence from the initial state. U may not generate clicks and rewards exactly the same as
those in ofﬂine data  but the similarity of its generated data to ofﬂine data is still an informative signal
to evaluate its sequence generation quality. By setting qD(τ0:t) = 1 at any time t for ofﬂine data  we
extend this objective to include ofﬂine data (it becomes the data likelihood function on ofﬂine data).
Following [36]  based on Eq (1) and Eq (2)  the gradient of U’s objective can be derived as 

(cid:80)N
n=1 D(τ n
D(τ0:T )

(a0 c0 r0)∼g U(c0  r0|su

0 ∼ρ[(cid:80)

qD(τ0:t) =

t < T
t = T

)  τ n

0:T

0:T

N

Eτ∼{g data}

qD(τ0:t)∇Θu

t   at) + λp log pΘu (rt|su

 

t=0

(7)
where Θu denotes the parameters of U and Θa denotes those of A. Based on our assumption 
even when U can already capture users’ true behavior patterns  it still depends on A to provide
appropriate recommendations to generate clicks and rewards that the discriminator will treat as
authentic. Hence  A and U are coupled in this adversarial training. To encourage A to provide
needed recommendations  we include qD(τ0:t) as a sequence generation reward for A at time t as
well. As qD(τ0:t) evaluates the overall generation quality of τ0:t  it ignores sequence generations
after t. To evaluate the quality of a whole sequence  we require A to maximize the cumulative

sequence generation reward Eτ∼{g data}(cid:2)(cid:80)T

the observations in the interaction sequence  we approximate ∇Θa
calculating the gradients. Putting these together  the gradient derived from sequence generations for
A is estimated as 

t=0 qD(τ0:t)(cid:3). Because A does not directly generate
(cid:0)(cid:80)T
t=0 qD(τ0:t)(cid:1) as 0 when
γt(cid:48)−tqD(τ0:t)(cid:1)∇Θa log πΘa (ct ∈ at|sa
t )(cid:3).

Eτ∼{g data}(cid:2)(cid:88)T

(cid:0)(cid:88)T

(8)

(cid:0) log pΘu (ct|su

(cid:104)(cid:88)T

t   ct)(cid:1)(cid:105)

t=0

t(cid:48)=t

5

Based on our assumption that only the clicked items inﬂuence user behaviors  and U only generates
rewards on the clicked items  we use πΘa (ct ∈ at|sa
t )  i.e.  A should
promote ct in its recommendation at time t. In practice  we add a discount factor γ < 1 when
calculating the cumulative rewards to reduce estimation variance [2].

t ) as an estimation of πΘa (at|sa

5.2 Policy learning

cumulative reward Eτ∼{g data}[RT ]  where RT = (cid:80)T

Because our adversarial training encourages IRecGAN to generate clicks and rewards with similar
patterns as ofﬂine data  and we assume rewards only relate to the clicked items  we use ofﬂine data
as well as generated data for policy learning. Given data τ0:T = {(a0  c0  r0)  ...  (aT   cT   rT )} 
including both ofﬂine and generated data  the objective of the agent is to maximize the expected
In the generated data  due to the
difference in distributions of the generated and ofﬂine sequences  the generated reward ˆrt calculated
by Eq (2) might be biased. To reduce such bias  we utilize the sequence generation score in Eq (6)
to rescale the generated rewards: rs
t = qD(τ0:t)ˆrt  and treat it as the reward for generated data. The
gradient of the objective is thus estimated by:

t=0 rt.

Rt∇Θa log πΘa (ct ∈ at|sa

t=0

(9)
Rt is an approximation of RT with the discount factor γ. Overall  the user behavior model U is
updated only by the sequence generation objective deﬁned in Eq (7) on both ofﬂine and generated
data; but the agent A is updated by both sequence generation and target rewards. Hence  the overall
reward for A at time t is qD(τ0:t)(1 + λrrt)  where λr is the weight for cumulative target rewards.
The overall gradient for A is thus:

t(cid:48)=t

γt(cid:48)−tqD(τ0:t)rt

Eτ∼{g data}(cid:2)(cid:88)T

t )(cid:3)  Rt =

(cid:88)T

t ∇Θa log πΘa (ct ∈ at|sa
Ra

γt(cid:48)−tqD(τ0:t)(1 + λrrt) (10)

Eτ∼{g data}(cid:2)(cid:88)T

t=0

t )(cid:3)  Ra

t =

(cid:88)T

t(cid:48)=t

6 Theoretical Analysis
For one iteration of policy learning in IRecGAN  we ﬁrst train the discriminator D with ofﬂine data 
which follows Pdata and was generated by an unknown logging policy  and the data generated by
IRecGAN under πΘa with the distribution of g. When Θu and Θa are learnt  for a given sequence τ 
by proposition 1 in [9]  the optimal discriminator D is D∗(τ ) =
Sequence generation Both A and U contribute to the sequence generation in IRecGAN. U is updated
by the gradient in Eq (7) to maximize the sequence generation objective. At time t  the expected se-
quence generation reward for A on the generated data is: Eτ0:t∼g[qD(τ0:t)] = Eτ0:t∼g[D(τ0:T|τ0:t)].
The expected value on τ0:t is: Eτ∼g[Vg] = Eτ∼g
Given the optimal D∗  the sequence generation value can be written as:
Pdata(τ0:T|τ0:t)

(cid:2)(cid:80)T
t=0 qD(τ0:t)(cid:3) =(cid:80)T

(cid:2)D(τ0:T|τ0:t)(cid:3).

Pdata(τ )+Pg(τ ).

(cid:88)T

Eτ0:t∼g

Pdata(τ )

(cid:105)

(cid:104)

t=0

Eτ∼g[Vg] =

Eτ0:t∼g

t=0

Pdata(τ0:T|τ0:t) + Pg(τ0:T|τ0:t)

.

(11)

Maximizing each term in the summation of Eq (11) is an objective for the generator at time t in
GAN. According to [9]  the optimal solution for all such terms is Pg(τ0:T|s0) = Pdata(τ0:T|s0). It
means A can maximize the sequence generation value when it helps to generate sequences with the
same distribution as data. Besides the global optimal  Eq (11) also encourages A to reward each
Pg(τ0:T|τ0:t) = Pdata(τ0:T|τ0:t)  even if τ0:t is less likely to be generated from Pg. This prevents
IRecGAN to recommend items only considering users’ immediate preferences.
Value estimation The agent A should also be updated to maximize the expected value of target
rewards Va. To achieve this  we use discriminator D to rescale the estimation of Va on the generated
sequences  and we also combine ofﬂine data to evaluate Va for policy πΘa:

Eτ0:t∼g

Eτ∼πΘa

[Va] = λ1

(12)
where ˆrt is the generated reward by U at time t and rt is the true reward. λ1 and λ2 represent the
ratio of generated data and ofﬂine data during model training  and we require λ1 + λ2 = 1. Here we
simplify P (τ0:T|τ0:t) as P (τ0:t). As a result  there are three sources of biases in this value estimation:

Pdata(τ0:t) + Pg(τ0:t)

ˆrt + λ2

t=0

t=0

Eτ0:t∼datart 

Pdata(τ0:t)

∆ = ˆrt − rt 

δ1 = 1 − PπΘa

δ2 = 1 − PπΘa

(τ0:t)/Pdata(τ0:t).

(cid:88)T

(cid:88)T

(τ0:t)/Pg(τ0:t) 

6

Based on different sources of biases  the expected value estimation in Eq (12) is:

T(cid:88)

(cid:16) PπΘa

(τ0:t)
Pdata(τ0:t)

(cid:17)

rt

+ δ2

Eτ0:t∼πΘa

(λ1 − wt)rt 

Eτ∼πΘa

[Va] =λ1

T(cid:88)

t=0

Eτ0:t∼g

T(cid:88)

(τ0:t)
PπΘa
Pg(τ0:t)

∆ + rt

2 − (δ1 + δ2)

T(cid:88)

Eτ0:t∼data

+ λ2

Eτ0:t∼dataλ2δ2rt − T(cid:88)

t=0

=V

πΘa
a

+

Eτ0:t∼πΘa

wt∆ +

t=0

t=0

t=0

λ1

2−(δ1+δ2). ∆ and δ1 come from the bias of user behavior model U. Because the
where wt =
adversarial training helps improve U to capture real data patterns  it decreases ∆ and δ2. Because we
can adjust the sampling ratio λ1 to reduce wt  wt∆ can be small. The sequence generation rewards
for agent A encourage distribution g to be close to data. Because δ2 = 1 − PπΘa
· Pg(τ0:t)
Pdata(τ0:t) 
the bias δ2 can also be reduced. It shows our method has a bias controlling effect.

Pg(τ0:t)

(τ0:t)

7 Experiments

In our theoretical analysis  we can ﬁnd that reducing the model bias improves value estimation 
and therefore improves policy learning. In this section  we conduct empirical evaluations on both
real-world and synthetic datasets to demonstrate that our solution can effectively model the pattern of
data for better recommendations  compared with state-of-the-art solutions.

7.1 Simulated Online Test

Subject to the difﬁculty of deploying a recommender system with real users for online evaluation  we
use simulation-based studies to ﬁrst investigate the effectiveness of our approach following [37  3].
Simulated Environment We synthesize an MDP to simulate an online recommendation environment.
It has m states and n items for recommendation  with a randomly initialized transition probability
matrix P (s ∈ S|aj ∈ A  si ∈ S). Under each state si  an item aj’s reward r(aj ∈ A|si ∈ S) is
uniformly sampled from the range of 0 to 1. During the interaction  given a recommendation list
including k items selected from the whole item set by an agent  the simulator ﬁrst samples an item
proportional to its ground-truth reward under the current state si as the click candidate. Denote
the sampled item as aj  a Bernoulli experiment is performed on this item with r(aj) as the success
probability; then the simulator moves to the next state according to the state transition probability
p(s|aj  si). A special state s0 is used to initialize all the sessions  which do not stop until the Bernoulli
experiment fails. The immediate reward is 1 if the session continues to the next step; otherwise 0. In
our experiment  m  n and k are set to 10  50 and 10 respectively.
Ofﬂine Data Generation We generate ofﬂine recommendation logs denoted by doff with the simula-
tor. The bias and variance in doff are especially controlled by changing the logging policy and the size
of doff. We adopt three different logging policies: 1) uniformly random policy πrandom  2) maximum
reward policy πmax  3) mixed reward policy πmix. Speciﬁcally  πmax recommends the top k items with
the highest ground-truth reward under the current simulator state at each step  while πmix randomly
selects k items with either the top 20%-50% ground-truth reward or the highest ground-truth reward
under a given state. In the meanwhile  we vary the size of data in doff from 200 to 10 000.
Baselines We compared our IRecGAN with the following baselines: 1) LSTM: only the user
behavior model trained on ofﬂine data; 2) PG: only the agent model trained by policy gradient on
ofﬂine data; 3) LSTMD: the user behavior model in IRecGAN  updated by adversarial training.
Experiment Settings The hyper-parameters in all models are set as follows: the item embedding
dimension is set to 50  the discount factor γ in value calculation is set to 0.9  the scale factors λr and
λp are set to 3 and 1. We use 2-layer LSTM units with 512-dimension hidden states. The ratio of
generated training samples and ofﬂine data for each training epoch is set to 1:10. We use an RNN
based discriminator in all experiments with details provided in the appendix.
Online Evaluation After training our models and baselines on doff  we deploy the learned policy to
interact with the simulator for online evaluation. We calculated coverage@r to measure the proportion
of the true top r relevant items that are ranked in the top k recommended items by a model across
all time steps (details in the appendix). The results of coverage@r under different conﬁgurations
of ofﬂine data generation are reported in Figure 2. Under πrandom  coverage@r of all algorithms are
relatively low when r is large and the difference in overall performance between behavior and agent

7

(a) πrandom(200)

(b) πrandom(10  000)

(c) πmax(10  000)

(d) πmix(10  000)

Figure 2: Online evaluation results of coverage@r and cumulative rewards.

Figure 3: Online learning results of coverage@1 and coverage@10.

models is not very large. This suggests the difﬁculty of recognizing high reward items under πrandom 
because every item has an equal chance to be observed (i.e.  full exploration) especially with a small
size of ofﬂine data. However  under πmax and πmix  when the high reward items can be sufﬁciently
learned  user behavior models (LSTM  LSTMD) fail to capture the overall preferred items while agent
models (PG  IRecGAN) are stable to the change of r. IRecGAN shows its advantage especially under
πmix  which requires a model to differentiate top relevant items from those with moderate reward. It
has close coverage@r to LSTM when r is small and better captures users’ overall preferences when
user behavior models fail seriously. When rewards can not be sufﬁciently learned (Fig 2(a))  our
mechanism can strengthen the inﬂuence of truly learned rewards (LSTMD outperforms LSTM when
r is small) but may also underestimate some bias. However  when it is feasible to estimate the reward
generation (Fig 2(b)(c)(d))  both LSTMD and IRecGAN outperform baselines in coverage@r under
the help of generating samples via adversarial training.
The average cumulative rewards are also reported in the rightmost bars of Figure 2. They are calculated
by generating 1000 sequences with the environment and take the average of their cumulative rewards.
IRecGAN has a larger average cumulative reward than other methods under all conﬁgurations except
πrandom with 10 000 ofﬂine sequences. Under πrandom(10  000) IRecGAN outperforms PG but not
LSTMD. The low cumulative reward of PG under πrandom indicates that the transition probabilities
conditioned on high rewarded items may not be sufﬁciently learned under the random ofﬂine policy.
Online Learning To evaluate our model’s effectiveness in a more practical setting  we execute online
and ofﬂine learning alternately. Speciﬁcally  we separate the learning into two stages: ﬁrst  the
agents can directly interact with the simulator to update their policies  and we only allow them to
generate 200 sequences in this stage; then they turn to the ofﬂine stage to reuse their generated data
for ofﬂine learning. We iterate the two stages and record their performance in the online learning
stage. We compare with the following baselines: 1) PG-online with only online learning  2) PG-
online&ofﬂine with online learning and reusing the generated data via policy gradient for ofﬂine

8

12345678910rewardr0.30.40.50.6coverage@rLSTMLSTMDPGIRecGAN2.93.03.13.23.33.43.53.6reward12345678910rewardr0.400.450.500.550.600.65coverage@rLSTMLSTMDPGIRecGAN3.03.23.43.63.84.04.2reward12345678910rewardr0.30.40.50.60.7coverage@rLSTMLSTMDPGIRecGAN3.03.23.43.63.84.04.2reward12345678910rewardr0.30.40.50.6coverage@rLSTMLSTMDPGIRecGAN2.93.03.13.23.33.43.53.6reward05101520iteration0.150.200.250.300.350.40coverage@1LSTM-offlinePG-onlinePG-online&offlineIRecGAN-online&offline05101520iteration0.150.200.250.300.35coverage@10LSTM-offlinePG-onlinePG-online&offlineIRecGAN-online&offlinelearning  and 3) LSTM-ofﬂine with only ofﬂine learning. We train all the models from scratch and
report the performance of coverage@1 and coverage@10 over 20 iterations in Figure 3. We can
observe that LSTM-ofﬂine performs worse than other RL methods with ofﬂine learning  especially
in the later stage  due to its lack of exploration. PG-online improves slowly as it does not reuse the
generated data. Compared with PG-online&ofﬂine  IRecGAN has better convergence and coverage
because of its reduced value estimation bias. We also ﬁnd that coverage@10 is harder to improve.
The key reason is that as the model identiﬁes the items with high rewards  it tends to recommend
them more often. This gives less relevant items less chance to be explored  which is similar to our
online evaluation experiments under πmax and πmix. Our model-based RL training alleviates this
bias to a certain extent by generating more training sequences  but it cannot totally alleviate it. This
reminds us to focus on explore-exploit trade-off in model-based RL in our future work.

7.2 Real-world Data Ofﬂine Test
We use a large-scale real-world recommendation dataset from CIKM Cup 2016 to evaluate the
effectiveness of our proposed solution for ofﬂine reranking. Sessions of length 1 or longer than 40
and items that have never been clicked are ﬁltered out. We selected the top 40 000 most popular
items into the recommendation candidate set  and randomly selected 65 284/1 718/1 720 sessions
for training/validation/testing. The average length of sessions is 2.81/2.80/2.77 respectively; and the
ratio of clicks which lead to purchases is 2.31%/2.46%/2.45%. We followed the same model setting
as in our simulation-based study in this experiment. To understand of the effect of different data
separation strategies on RL model training and test  we also provide a comparison of performances
under different data separation strategies in the appendix.
Baselines In addition to the baselines we compared in our simulation-based study  we also include
the following state-of-the-art solutions for recommendation: 1). PGIS: the agent model estimated
with importance sampling on ofﬂine data to reduce bias; 2). AC: an LSTM model whose setting is
the same as our agent model but trained with actor-critic algorithm [16] to reduce variance; 3). PGU:
the agent model trained using ofﬂine and generated data  without adversarial training; 4). ACU: AC
model trained with both ofﬂine and generated data  without adversarial training.
Evaluation Metrics All the models were applied to rerank the given recommendation list at each
step of testing sessions in ofﬂine data. We used Precision@k (P@1 and P@10) to compare different
models’ recommendation performance  where we deﬁne the clicked items as relevant. Because the
logged recommendation list was not ordered  we cannot assess the logging policy’s performance here.

Model

P@10 (%)
P@1 (%)

Table 1: Rerank evaluation on real-world dataset with random splitting.
LSTM
ACU
32.43±0.22
32.89±0.50
6.63± 0.29
8.20±0.65

LSTMD
33.42±0.40
8.55±0.63

31.93±0.17
6.54±0.19

PGIS

28.13±0.45
4.61±0.73

PGU

34.12±0.52
6.44±0.56

PG

33.28±0.71
6.25±0.14

AC

IRecGAN
35.06±0.48
6.79±0.44

Results The results of the ofﬂine rerank evaluation are reported in Table 1. With the help of
adversarial training  IRecGAN achieved encouraging P@10 improvement against all baselines. This
veriﬁes the effectiveness of our model-based reinforcement learning  especially its adversarial training
strategy for utilizing the ofﬂine data with reduced bias. Speciﬁcally  PGIS did not perform as well
as PG partially because of the high variance introduced by importance sampling. PGU was able
to ﬁt the given data more accurately than PG by learning from the generated data  since there are
many items for recommendation and the collected data is limited. However  PGU performed worse
than IRecGAN because of the biased user behavior model. And with the help of the discriminator 
IRecGAN reduces the bias in the user behavior model to improve value estimation and policy learning.
This is also reﬂected on its improved user behavior model: LSTMD outperformed LSTM  given both
of them are for user behavior modeling.

8 Conclusion
In this work  we developed a practical solution for utilizing ofﬂine data to build a model-based
reinforcement learning solution for recommendation. We introduce adversarial training for joint user
behavior model learning and policy update. Our theoretical analysis shows our solution’s promise in
reducing bias; our empirical evaluations in both synthetic and real-world recommendation datasets
verify the effectiveness of our solution. Several directions left open in our work  including balancing
explore-exploit in policy learning with ofﬂine data  incorporating richer structures in user behavior
modeling  and exploring the applicability of our solution in other off-policy learning scenarios  such
as conversational systems.

9

References
[1] Joshua Achiam  David Held  Aviv Tamar  and Pieter Abbeel. Constrained policy optimization.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
22–31. JMLR. org  2017.

[2] Minmin Chen  Alex Beutel  Paul Covington  Sagar Jain  Francois Belletti  and Ed H Chi. Top-k
off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining  pages 456–464. ACM  2019.

[3] Xinshi Chen  Shuang Li  Hui Li  Shaohua Jiang  Yuan Qi  and Le Song. Generative adversarial
user model for reinforcement learning based recommendation system. In Proceedings of the
36th International Conference on Machine Learning  volume 97  pages 1052–1061  2019.

[4] Junyoung Chung  Caglar Gulcehre  KyungHyun Cho  and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 
2014.

[5] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efﬁcient approach
to policy search. In Proceedings of the 28th International Conference on machine learning
(ICML-11)  pages 465–472  2011.

[6] Marc Peter Deisenroth  Carl Edward Rasmussen  and Dieter Fox. Learning to control a low-cost

manipulator using data-efﬁcient reinforcement learning. 2011.

[7] Marc Peter Deisenroth  Gerhard Neumann  Jan Peters  et al. A survey on policy search for

robotics. Foundations and Trends R(cid:13) in Robotics  2(1–2):1–142  2013.

[8] Alexandre Gilotte  Clément Calauzènes  Thomas Nedelec  Alexandre Abraham  and Simon
Dollé. Ofﬂine a/b testing for recommender systems. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining  pages 198–206. ACM  2018.

[9] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[10] Shixiang Gu  Timothy Lillicrap  Ilya Sutskever  and Sergey Levine. Continuous deep q-learning
In International Conference on Machine Learning  pages

with model-based acceleration.
2829–2838  2016.

[11] Xiangnan He  Hanwang Zhang  Min-Yen Kan  and Tat-Seng Chua. Fast matrix factorization
for online recommendation with implicit feedback. In Proceedings of the 39th International
ACM SIGIR conference on Research and Development in Information Retrieval  pages 549–558.
ACM  2016.

[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997.

[13] Yehuda Koren  Robert Bell  and Chris Volinsky. Matrix factorization techniques for recom-

mender systems. Computer  (8):30–37  2009.

[14] Reinforcement Learning. An introduction  richard s. sutton and andrew g. barto  1998.

[15] Elad Liebman  Maytal Saar-Tsechansky  and Peter Stone. Dj-mc: A reinforcement-learning
agent for music playlist recommendation. In Proceedings of the 2015 International Conference
on Autonomous Agents and Multiagent Systems  pages 591–599. International Foundation for
Autonomous Agents and Multiagent Systems  2015.

[16] Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa 
David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971  2015.

[17] Zhongqi Lu and Qiang Yang. Partially observable markov decision process for recommender

systems. arXiv preprint arXiv:1608.07793  2016.

10

[18] David Meger  Juan Camilo Gamboa Higuera  Anqi Xu  Philippe Giguere  and Gregory Dudek.
Learning legged swimming gaits from experience. In 2015 IEEE International Conference on
Robotics and Automation (ICRA)  pages 2332–2338. IEEE  2015.

[19] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602  2013.

[20] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[21] Jun Morimoto and Christopher G Atkeson. Minimax differential dynamic programming: An
application to robust biped walking. In Advances in neural information processing systems 
pages 1563–1570  2003.

[22] Rémi Munos  Tom Stepleton  Anna Harutyunyan  and Marc Bellemare. Safe and efﬁcient
off-policy reinforcement learning. In Advances in Neural Information Processing Systems 
pages 1054–1062  2016.

[23] Junhyuk Oh  Satinder Singh  and Honglak Lee. Value prediction network. In Advances in

Neural Information Processing Systems  pages 6118–6128  2017.

[24] Baolin Peng  Xiujun Li  Jianfeng Gao  Jingjing Liu  Kam-Fai Wong  and Shang-Yu Su. Deep
dyna-q: Integrating planning for task-completion dialogue policy learning. arXiv preprint
arXiv:1801.06176  2018.

[25] Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department

Faculty Publication Series  page 80  2000.

[26] Doina Precup  Richard S Sutton  and Sanjoy Dasgupta. Off-policy temporal-difference learning

with function approximation. In ICML  pages 417–424  2001.

[27] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889–1897 
2015.

[28] Guy Shani  David Heckerman  and Ronen I Brafman. An mdp-based recommender system.

Journal of Machine Learning Research  6(Sep):1265–1295  2005.

[29] Richard S Sutton.

Integrated architectures for learning  planning  and reacting based on
approximating dynamic programming. In Machine Learning Proceedings 1990  pages 216–224.
Elsevier  1990.

[30] Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour. Policy gradient
In Advances in neural

methods for reinforcement learning with function approximation.
information processing systems  pages 1057–1063  2000.

[31] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization. Journal of Machine Learning Research  16(1):
1731–1755  2015.

[32] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual

learning. In advances in neural information processing systems  pages 3231–3239  2015.

[33] Philip Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforce-

ment learning. In International Conference on Machine Learning  pages 2139–2148  2016.

[34] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine learning  8(3-4):229–256  1992.

[35] Qingyun Wu  Hongning Wang  Liangjie Hong  and Yue Shi. Returning is believing: Optimizing
long-term user engagement in recommender systems. In Proceedings of the 2017 ACM on
Conference on Information and Knowledge Management  pages 1927–1936. ACM  2017.

11

[36] Lantao Yu  Weinan Zhang  Jun Wang  and Yong Yu. Seqgan: Sequence generative adversarial

nets with policy gradient. In Thirty-First AAAI Conference on Artiﬁcial Intelligence  2017.

[37] Xiangyu Zhao  Long Xia  Yihong Zhao  Dawei Yin  and Jiliang Tang. Model-based reinforce-

ment learning for whole-chain recommendations. arXiv preprint arXiv:1902.03987  2019.

[38] Guanjie Zheng  Fuzheng Zhang  Zihan Zheng  Yang Xiang  Nicholas Jing Yuan  Xing Xie 
and Zhenhui Li. Drn: A deep reinforcement learning framework for news recommendation.
In Proceedings of the 2018 World Wide Web Conference on World Wide Web  pages 167–176.
International World Wide Web Conferences Steering Committee  2018.

12

,Xueying Bai
Jian Guan
Hongning Wang