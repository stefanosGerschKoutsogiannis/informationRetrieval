2019,How degenerate is the parametrization of neural networks with the ReLU activation function?,Neural network training is usually accomplished by solving a non-convex optimization problem using stochastic gradient descent. Although one optimizes over the networks parameters  the main loss function generally only depends on the realization of the neural network  i.e. the function it computes. Studying the optimization problem over the space of realizations opens up new ways to understand neural network training. In particular  usual loss functions like mean squared error and categorical cross entropy are convex on spaces of neural network realizations  which themselves are non-convex. Approximation capabilities of neural networks can be used to deal with the latter non-convexity  which allows us to establish that for sufficiently large networks local minima of a regularized optimization problem on the realization space are almost optimal. Note  however  that each realization has many different  possibly degenerate  parametrizations. In particular  a local minimum in the parametrization space needs not correspond to a local minimum in the realization space. To establish such a connection  inverse stability of the realization map is required  meaning that proximity of realizations must imply proximity of corresponding parametrizations. We present pathologies which prevent inverse stability in general  and  for shallow networks  proceed to establish a restricted space of parametrizations on which we have inverse stability w.r.t. to a Sobolev norm. Furthermore  we show that by optimizing over such restricted sets  it is still possible to learn any function which can be learned by optimization over unrestricted sets.,How degenerate is the parametrization of neural

networks with the ReLU activation function?

Julius Berner

Faculty of Mathematics  University of Vienna

Oskar-Morgenstern-Platz 1  1090 Vienna  Austria

julius.berner@univie.ac.at

Dennis Elbrächter

Faculty of Mathematics  University of Vienna

Oskar-Morgenstern-Platz 1  1090 Vienna  Austria

dennis.elbraechter@univie.ac.at

Faculty of Mathematics and Research Platform DataScience@UniVienna  University of Vienna

Philipp Grohs

Oskar-Morgenstern-Platz 1  1090 Vienna  Austria

philipp.grohs@univie.ac.at

Abstract

Neural network training is usually accomplished by solving a non-convex opti-
mization problem using stochastic gradient descent. Although one optimizes over
the networks parameters  the main loss function generally only depends on the
realization of the neural network  i.e. the function it computes. Studying the opti-
mization problem over the space of realizations opens up new ways to understand
neural network training. In particular  usual loss functions like mean squared error
and categorical cross entropy are convex on spaces of neural network realizations 
which themselves are non-convex. Approximation capabilities of neural networks
can be used to deal with the latter non-convexity  which allows us to establish
that for sufﬁciently large networks local minima of a regularized optimization
problem on the realization space are almost optimal. Note  however  that each
realization has many different  possibly degenerate  parametrizations. In particular 
a local minimum in the parametrization space needs not correspond to a local
minimum in the realization space. To establish such a connection  inverse stability
of the realization map is required  meaning that proximity of realizations must
imply proximity of corresponding parametrizations. We present pathologies which
prevent inverse stability in general  and  for shallow networks  proceed to establish
a restricted space of parametrizations on which we have inverse stability w.r.t. to a
Sobolev norm. Furthermore  we show that by optimizing over such restricted sets 
it is still possible to learn any function which can be learned by optimization over
unrestricted sets.

1

Introduction and Motivation

In recent years much effort has been invested into explaining and understanding the overwhelming
success of deep learning based methods. On the theoretical side  impressive approximation capa-
bilities of neural networks have been established [9  10  16  20  32  33  37  39]. No less important
are recent results on the generalization of neural networks  which deal with the question of how

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

well networks  trained on limited samples  perform on unseen data [2  3  5–7  17  29]. Last but
not least  the optimization error  which quantiﬁes how well a neural network can be trained by
applying stochastic gradient descent to an optimization problem  has been analyzed in different
scenarios [1  11  13  22  24  25  27  38]. While there are many interesting approaches to the latter
question  they tend to require very strong assumptions (e.g. (almost) linearity  convexity  or extreme
over-parametrization). Thus a satisfying explanation for the success of stochastic gradient descent for
a non-smooth  non-convex problem remains elusive.
In the present paper we intend to pave the way for a functional perspective on the optimization
problem. This allows for new mathematical approaches towards understanding the training of neural
networks  some of which are demonstrated in Section 1.2. To this end we examine degenerate
parametrizations with undesirable properties in Section 2. These can be roughly classiﬁed as

C.1 unbalanced magnitudes of the parameters
C.2 weight vectors with the same direction
C.3 weight vectors with directly opposite directions.

Under conditions designed to avoid these degeneracies  Theorem 3.1 establishes inverse stability
for shallow networks with ReLU activation function. This is accomplished by a reﬁned analysis
of the behavior of ReLU networks near a discontinuity of their derivative. Proposition 1.2 shows
how inverse stability connects the loss surface of the parametrized minimization problem to the loss
surface of the realization space problem. In Theorem 1.3 we showcase a novel result on almost
optimality of local minima of the parametrized problem obtained by analyzing the realization space
problem. Note that this approach of analyzing the loss surface is conceptually different from previous
approaches as in [11  18  23  30  31  36].

1.1

Inverse Stability of Neural Networks

We will focus on neural networks with the ReLU activation function ρ(x) := x+  and adapt the
mathematically convenient notation from [33]  which distinguishes between the parametrization of a
neural network and its realization. Let us deﬁne the set AL of all network architectures with depth
L ∈ N  input dimension d ∈ N  and output dimension D ∈ N by

AL := {(N0  . . .   NL) ∈ NL+1 : N0 = d  NL = D}.

(1)
The architecture N ∈ AL simply speciﬁes the number of neurons Nl in each of the L layers. We can
then deﬁne the space PN of parametrizations with architecture N ∈ AL as

L(cid:89)

(cid:0)RN(cid:96)×N(cid:96)−1 × RN(cid:96)(cid:1)  

PN :=

(2)

the set P =(cid:83)

N∈AL

PN of all parametrizations with architecture in AL  and the realization map

(cid:96)=1

R : P → C(Rd  RD)
(cid:96)=1 (cid:55)→ R(Θ) := WL ◦ ρ ◦ WL−1 . . . ρ ◦ W1 

Θ = ((A(cid:96)  b(cid:96)))L

(3)

where W(cid:96)(x) := A(cid:96)x + b(cid:96) and ρ is applied component-wise. We refer to A(cid:96) and b(cid:96) as the weights
and biases in the (cid:96)-th layer.
Note that a parametrization Θ ∈ Ω ⊆ P uniquely induces a realization R(Θ) in the realization space
R(Ω)  while in general there can be multiple non-trivially different parametrizations with the same
realization. To put it in mathematical terms  the realization map is not injective. Consider the basic
counterexample

and Γ =(cid:0)(B1  c1)  . . .   (BL−1  cL−1)  (0  0)(cid:1) (4)

Θ =(cid:0)(A1  b1)  . . .   (AL−1  bL−1)  (0  0)(cid:1)

from [34] where regardless of A(cid:96)  B(cid:96)  b(cid:96) and c(cid:96) both realizations coincide with R(Θ) = R(Γ) = 0.
However  it it is well-known that the realization map is locally Lipschitz continuous  meaning that
close1 parametrizations in PN induce realizations which are close in the uniform norm on compact
1On the ﬁnite dimensional vector space PN all norms are equivalent and we take w.l.o.g. the maximum norm

(cid:107)Θ(cid:107)∞  i.e. the maximum of the absolute values of the entries of the A(cid:96) and b(cid:96).

2

sets  see e.g. [2  Lemma 14.6]  [7  Theorem 4.2]  and [34  Proposition 5.1].
We will shed light upon the inverse question. Given realizations R(Γ) and R(Θ) that are close  do
the parametrizations Γ and Θ have to be close? In an abstract setting we measure the proximity of
realizations in the norm (cid:107) · (cid:107) of a Banach space B with R(P) ⊆ B  while concrete Banach spaces of
interest will be speciﬁed later. In view of the above counterexample we will  at the very least  need to
allow for the reparametrization of one of the networks  i.e. we arrive at the following question.

Given R(Γ) and R(Θ) that are close  does there exist a parametrization Φ with
R(Φ) = R(Θ) such that Γ and Φ are close?

As we will see in Section 2  this question is fundamentally connected to understanding the redundan-
cies and degeneracies of the way that neural networks are parametrized. By suitable regularization  i.e.
considering a subspace Ω ⊆ PN of parametrizations  we can avoid these pathologies and establish a
positive answer to the question above. For such a property the term inverse stability was introduced
in [34]  which constitutes the only other research conducted in this area  as far as we are aware.
Deﬁnition 1.1 (Inverse stability). Let s  α > 0  N ∈ AL  and Ω ⊆ PN . We say that the realization
map is (s  α) inverse stable on Ω w.r.t. (cid:107) · (cid:107)  if for all Γ ∈ Ω and g ∈ R(Ω) there exists Φ ∈ Ω with
(5)

and (cid:107)Φ − Γ(cid:107)∞ ≤ s(cid:107)g − R(Γ)(cid:107)α.

R(Φ) = g

In Section 2 we will see why inverse stability fails w.r.t. the uniform norm. Therefore  we consider
a norm which takes into account not only the maximum error of the function values but also of
the gradients. In mathematical terms  we make use of the Sobolev norm (cid:107) · (cid:107)W 1 ∞(U ) (on some
domain U ⊆ Rd) deﬁned for every (locally) Lipschitz continuous function g : Rd → RD by
(cid:107)g(cid:107)W 1 ∞(U ) := max{(cid:107)g(cid:107)L∞(U ) |g|W 1 ∞(U )} with the Sobolev semi-norm | · |W 1 ∞(U ) given by

|g|W 1 ∞(U ) := (cid:107)Dg(cid:107)L∞(U ) = ess sup
x∈U

(cid:107)Dg(x)(cid:107)∞.

(6)

See [15] for further information on Sobolev norms  and [8] for further information on the derivative
of ReLU networks.

1.2

Implications of inverse stability for neural network optimization

We proceed by demonstrating how inverse stability opens up new perspectives on the optimiza-
tion problem which arises in neural network training. Speciﬁcally  consider a loss function
L : C(Rd  RD) → [0 ∞) on the space of continuous functions. For illustration  we take the com-
i=1 ∈ (Rd × RD)n  is given
monly used mean squared error (MSE) which  for training data ((xi  yi))n
by

(7)
Typically  the optimization problem is solved over some subspace of parametrizations Ω ⊆ PN   i.e.

i=1

(cid:107)g(xi) − yi(cid:107)2
2 

for g ∈ C(Rd  RD).

L(g) = 1

n

n(cid:88)

n(cid:88)

i=1

n(cid:88)

i=1

L(R(Γ)) = min
Γ∈Ω

1
n

min
Γ∈Ω

(cid:107)R(Γ)(xi) − yi(cid:107)2
2.

(8)

From an abstract point of view  by writing g = R(Γ) ∈ R(Ω)  this is equivalent to the corresponding
optimization problem over the space of realizations R(Ω)  i.e.

min
g∈R(Ω)

L(g) = min
g∈R(Ω)

1
n

(cid:107)g(xi) − yi(cid:107)2
2.

(9)

However  the loss landscape of the optimization problem (8) is only properly connected to the loss
landscape of the optimization problem (9) if the realization map is inverse stable on Ω. Otherwise
a realization g ∈ R(PN ) can be arbitrarily close to a global minimum in the realization space but
every parametrization Φ with R(Φ) = g is far away from the corresponding global minimum in the
parametrization space. Moreover  local minima of (8) in the parametrization space must correspond
to local minima of (9) in the realization space if and only if we have inverse stability.

3

Proposition 1.2 (Parametrization minimum ⇒ realization minimum). Let N ∈ AL  Ω ⊆ PN and
let the realization map be (s  α) inverse stable on Ω w.r.t. (cid:107) · (cid:107). Let Γ∗ ∈ Ω be a local minimum of
L ◦ R on Ω with radius r > 0  i.e. for all Φ ∈ Ω with (cid:107)Φ − Γ∗(cid:107)∞ ≤ r it holds that

L(R(Γ∗)) ≤ L(R(Φ)).

Then R(Γ∗) is a local minimum of L on R(Ω) with radius ( r
(cid:107)g − R(Γ∗)(cid:107) ≤ ( r

s )1/α it holds that

L(R(Γ∗)) ≤ L(g).

(10)
s )1/α  i.e. for all g ∈ R(Ω) with

See Appendix A.1.2 for a proof and Example A.1 for a counterexample in the case that inverse
stability is not given. Note that in (9) we consider a problem with convex loss function but non-convex
feasible set  see [34  Section 3.2]. This opens up new avenues of investigation using tools from
functional analysis and allows utilizing recent results [19  34] exploring the topological properties of
neural network realization spaces.
As a concrete demonstration we provide with Theorem A.2 a strong result obtained on the realization
space  which estimates the quality of a local minimum based on its radius and the approximation
capabilities of the chosen architecture for a class of functions S. Speciﬁcally let C > 0  let
Λ : B → [0 ∞) be a quasi-convex regularizer  and deﬁne

S := {f ∈ B : Λ(f ) ≤ C}.

We denote the sets of regularized parametrizations by

ΩN := {Φ ∈ PN : Λ(R(Φ)) ≤ C}

(13)
and assume that the loss function L is convex and c-Lipschitz continuous on S. Note that virtually
all relevant loss functions are convex and locally Lipschitz continuous on C(Rd  RD). Employing
Proposition 1.2  inverse stability can then be used to derive the following result for the practically
relevant parametrized problem  showing that for sufﬁciently large architectures local minima of a
regularized neural network optimization problem are almost optimal.
Theorem 1.3 (Almost optimality of local parameter minima). Assume that S is compact in the
(cid:107) · (cid:107)-closure of R(P) and that for every N ∈ AL the realization map is (s  α) inverse stable on
ΩN w.r.t. (cid:107) · (cid:107) . Then for all ε  r > 0 there exists n(ε  r) ∈ AL such that for every N ∈ AL with
N1 ≥ n1(ε  r)  . . .   NL−1 ≥ nL−1(ε  r) the following holds:
Every local minimum Γ∗ with radius at least r of minΓ∈ΩN L(R(Γ)) satisﬁes

L(R(Γ∗)) ≤ min
Γ∈ΩN

L(R(Γ)) + ε.

(11)

(12)

(14)

See Appendix A.1.2 for a proof and note that here it is important to have an inverse stability result 
where the parameters (s  α) do not depend on the size of the architecture  which we achieve for
L = 2 and B = W 1 ∞. Suitable Λ would be Besov norms which constitute a common regularizer in
image and signal processing. Moreover  note that the required size of the architecture in Theorem 1.3
can be quantiﬁed  if one has approximation rates for S. In particular  this approach allows the use of
approximation results in order to explain the success of neural network optimization and enables a
combined study of these two aspects  which  to the best of our knowledge  has not been done before.
Unlike in recent literature  our result needs no assumptions on the sample set (incorporated in the loss
function  see (7))  in particular we do not require “overparametrization” with respect to the sample
size. Here the required size of the architecture only depends on the complexity of S  i.e. the class of
functions one wants to approximate  the radius of the local minima of interest  the Lipschitz constant
of the loss function  and the parameters of the inverse stability.
In the following we restrict ourselves to two-layer ReLU networks without biases  where we present
a proof for (4  1/2) inverse stability w.r.t. the Sobolev semi-norm on a suitably regularized space of
parametrizations. Both the regularizations as well as the stronger norm (compared to the uniform
norm) will shown to be necessary in Section 2. We now present  in an informal way  a collection
of our main results. A short proof making the connection to the formal results can be found in
Appendix A.1.2.
Corollary 1.4 (Inverse stability and implications - colloquial). Suppose we are given data
i=1 ∈ (Rd × RD)n and want to solve a typical minimization problem for ReLU networks
((xi  yi))n
with shallow architecture N = (d  N1  D)  i.e.

min
Γ∈PN

1
n

(cid:107)R(Γ)(xi) − yi)(cid:107)2
2.

(15)

n(cid:88)

i=1

4

First we augment the architecture to ˜N = (d + 2  N1 + 1  D)  while omitting the biases  and augment
the samples to ˜xi = (xi

d  1 −1). Moreover  we assume that the parametrizations

1  . . .   xi

Φ =(cid:0)(cid:0)[a1| . . .|aN1+1]T   0(cid:1)  ([c1| . . .|cN1+1]  0)(cid:1) ∈ Ω ⊆ P ˜N

(16)

(17)

(18)

are regularized such that

C.1 the network is balanced  i.e. (cid:107)ai(cid:107)∞ = (cid:107)ci(cid:107)∞ 
C.2 no non-zero weight vectors in the ﬁrst layer are redundant  i.e. ai (cid:54)(cid:107) aj 
C.3 the last two coordinates of each weight vector ai are strictly positive.

Then for the new minimization problem

min
Φ∈Ω

1
n

n(cid:88)

i=1

(cid:107)R(Φ)(˜xi) − yi(cid:107)2

2

the following holds:

1. If Φ∗ is a local minimum of (17) with radius r  then R(Φ∗) is a local minimum of

ming∈R(Ω)

1
n

2 with radius at least r2

2. The global minimum of (17) is at least as good as the global minimum of (15)  i.e.

(cid:80)n
i=1 (cid:107)g(˜xi) − yi(cid:107)2
n(cid:88)

min
Φ∈Ω

1
n

i=1

(cid:107)R(Φ)(˜xi) − yi(cid:107)2

2 ≤ min
Γ∈PN

1
n

16 w.r.t. | · |W 1 ∞.
n(cid:88)

(cid:107)R(Γ)(xi) − yi(cid:107)2
2.

i=1

3. By further regularizing (17) in the sense of Theorem 1.3  we can estimate the quality of its

local minima.

This argument is not limited to the MSE loss function but works for any loss function based on
evaluating the realization. The omission of bias weights is standard in neural network optimization
literature [11  13  22  24]. While this severely limits the functions that can be realized with a given
architecture  it is sufﬁcient to augment the problem by one dimension in order to recover the full
range of functions that can be learned [1]. Here we augment by two dimensions  so that the third
regularization condition C.3 can be fulﬁlled without loosing range. Moreover  note that  for simplicity
of presentation  the regularization assumptions stated above are stricter than necessary and possible
relaxations are discussed in Section 3.

2 Obstacles to inverse stability - degeneracies of ReLU parametrizations

In the remainder of this paper we focus on shallow ReLU networks without biases and deﬁne the cor-
responding space of parametrizations with architecture N = (d  m  D) as NN := Rm×d × RD×m.

The realization map2 R is  for every Θ = (A  C) =(cid:0)[a1| . . .|am]T   [c1| . . .|cm](cid:1) ∈ NN   given by

Rd (cid:51) x (cid:55)→ R(Θ)(x) = Cρ(Ax) =

ciρ((cid:104)ai  x(cid:105)).

(19)

m(cid:88)

i=1

Note that each function x (cid:55)→ ciρ((cid:104)ai  x(cid:105)) represents a so-called ridge function which is zero on the
half-space {x ∈ Rd : (cid:104)ai  x(cid:105) ≤ 0} and linear with constant derivative ciaT
i ∈ RD × Rd on the other
half-space. Thus  the ai are the normal vectors of the separating hyperplanes {x ∈ Rd : (cid:104)ai  x(cid:105) = 0}
and consequently we refer to the weight vectors ai also as the directions of Θ. Moreover  for Θ ∈ NN
it holds that R(Θ)(0) = 0 and  as long as the domain of interest U ⊆ Rd contains the origin  the
Sobolev norm (cid:107) · (cid:107)W 1 ∞(U ) is equivalent to its semi-norm  since

(20)
2This is a slight abuse of notation  justiﬁed by the the fact that R acts the same on PN with zero biases b1  b2

(cid:107)R(Θ)(cid:107)L∞(U ) ≤

d diam(U )|R(Θ)|W 1 ∞  

and weights A1 = A and A2 = C.

√

5

Figure 1: The ﬁgure shows gk for k = 1  2.

see also inequalities of Poincaré-Friedrichs type [14  Subsection 5.8.1]. Therefore  in the rest of the
paper we will only consider the Sobolev semi-norm3

|R(Θ)|W 1 ∞(U ) = ess sup
x∈U

(cid:13)(cid:13)(cid:13) (cid:88)

i∈[m] : (cid:104)ai x(cid:105)>0

(cid:13)(cid:13)(cid:13)∞.

ciaT
i

(21)

(22)

(23)

In (21) one can see that in our setting | · |W 1 ∞(U ) is independent of U (as long as U contains a
neighbourhood of the origin) and will thus be abbreviated by | · |W 1 ∞.

2.1 Failure of inverse stability w.r.t. uniform norm

All proofs for this section can be found in Appendix A.2.2. We start by showing that inverse stability
fails w.r.t. the uniform norm. This example is adapted from [34  Theorem 5.2] and represents  to the
best of our knowledge  the only degeneracy which has already been observed before.
Example 2.1 (Failure due to exploding gradient). Let Γ := (0  0) ∈ N(2 2 1) and gk ∈ R(N(2 2 1))
be given by (see Figure 1)

k ∈ N.
Then for every sequence (Φk)k∈N ⊆ N(2 2 1) with R(Φk) = gk it holds that

gk(x) := kρ((cid:104)(k  0)  x(cid:105)) − kρ((cid:104)(k − 1

k2 )  x(cid:105)) 

k→∞(cid:107)R(Φk) − R(Γ)(cid:107)L∞((−1 1)2) = 0 and

lim

k→∞(cid:107)Φk − Γ(cid:107)∞ = ∞.

lim

In particular  note that inverse stability fails here even for a non-degenerate parametrization of the
zero function Γ = (0  0). However  for this type of counterexample the magnitude of the gradient of
R(Φk) needs to go to inﬁnity  which is our motivation for looking at inverse stability w.r.t. | · |W 1 ∞.

Example 2.2 (Failure due to complete unbalancedness). Let r > 0  Γ :=(cid:0)(r  0)  0(cid:1) ∈ N(2 1 1) and

2.2 Failure of inverse stability w.r.t. Sobolev norm
In this section we present four degenerate cases where inverse stability fails w.r.t. | · |W 1 ∞. This
collection of counterexamples is complete in the sense that we can establish inverse stability under
assumptions which are designed to exclude these four pathologies.
gk ∈ R(N(2 1 1)) be given by (see Figure 2)
gk(x) = 1

k ∈ N.

k ρ((cid:104)(0  1)  x(cid:105)) 

(24)

Then for every k ∈ N and Φk ∈ N(2 1 1) with R(Φk) = gk it holds that

|R(Φk) − R(Γ)|W 1 ∞ = 1

(cid:107)Φk − Γ(cid:107)∞ ≥ r.

(25)
This is a very simple example of a degenerate parametrization of the zero function  since R(Γ) = 0
regardless of choice of r. The issue here is that we can have a weight pair  i.e. ((r  0)  0)  where the
product is independent of the value of one of the parameters. Note that in Example A.4 one can see a

slightly more subtle version of this pathology by considering Γk :=(cid:0)(k  0)  1

(cid:1) ∈ N(2 1 1) instead.

In that case one could still get an inverse stability estimate for each ﬁxed k; the parameters of inverse

and

k2

k

3For m ∈ N we abbreviate [m] := {1  . . .   m}.

6

Figure 2: Shows R(Γ) (r = 0.5) and g3.

Figure 3: Shows R(Γ) and g2.

stability (s  α) would however deteriorate with increasing k. In particular this demonstrates the need
for some sort of balancedness of the parametrization  i.e. control over (cid:107)ci(cid:107)∞ and (cid:107)ai(cid:107)∞ individually
relative to (cid:107)ci(cid:107)∞(cid:107)ai(cid:107)∞.
Inverse stability is also prevented by redundant directions as the following example illustrates.
Example 2.3 (Failure due to redundant directions). Let

(cid:18)(cid:20)1

1

(cid:21)

0
0

(cid:19)

Γ :=

  (1  1)

∈ N(2 2 1)

(26)

(27)

(28)

and gk ∈ R(N(2 2 1)) be given by (see Figure 3)

Then for every k ∈ N and Φk ∈ N(2 2 1) with R(Φk) = gk it holds that

gk(x) := 2ρ((cid:104)(1  0)  x(cid:105)) + 1

k ρ((cid:104)(0  1)  x(cid:105)) 

k ∈ N.

|R(Φk) − R(Γ)|W 1 ∞ = 1

k

and

(cid:107)Φk − Γ(cid:107)∞ ≥ 1.

The next example shows that not only redundant weight vectors can cause issues  but also weight
vectors of opposite direction  as they would allow for a (balanced) degenerate parametrization of the
zero function.
Example 2.4 (Failure due to opposite weight vectors 1). Let ai ∈ Rd  i ∈ [m]  be pairwise linearly

independent with (cid:107)ai(cid:107)∞ = 1 and(cid:80)m

Γ :=(cid:0)[a1| . . .|am| − a1| . . .| − am]T  (cid:0)1  . . .   1 −1  . . .  −1(cid:1)(cid:1) ∈ N(d 2m 1).

(29)
Now let v ∈ Rd with (cid:107)v(cid:107)∞ = 1 be linearly independent to each ai  i ∈ [m]  and let gk ∈
R(N(d 2m 1)) be given by (see Figure 4)

i=1 ai = 0. We deﬁne

(30)
Then there exists a constant C > 0 such that for every k ∈ N and every Φk ∈ N(d 2m 1) with
R(Φk) = gk it holds that

gk(x) = 1

k ρ((cid:104)v  x(cid:105)) 

k ∈ N.

|R(Φk) − R(Γ)|W 1 ∞ = 1

k

and

(cid:107)Φk − Γ(cid:107)∞ ≥ C.

(31)

Thus we will need an assumption which prevents each individual Γ in our restricted set from having
pairwise linearly dependent weight vectors  i.e. coinciding hyperplanes of non-differentiability. This 
however  does not sufﬁce as is demonstrated by the next example  which shows that the relation
between the hyperplanes of the two realizations matters.
Example 2.5 (Failure due to opposite weight vectors 2). We deﬁne the weight vectors
√
ck = (k  k 

√

2k)

2k 

(32)

) 

ak
1 = (k  k  1

1√
2k

and consider the parametrizations (see Figure 5)

(cid:16)(cid:2) − ak

1

k ) 

(cid:12)(cid:12) − ak

2

3 = (0 −
ak

2 = (−k  k  1
ak
k ) 
(cid:3)T

  ck(cid:17) ∈ N(3 3 1)  Θk :=

(cid:12)(cid:12) − ak

3

Γk :=

(cid:16)(cid:2)ak

1

(cid:12)(cid:12)ak

2

(cid:12)(cid:12)ak

3

(cid:3)T

  ck(cid:17) ∈ N(3 3 1).

Then for every k ∈ N and every Φk ∈ N(3 3 1) with R(Φk) = R(Θk) it holds that

|R(Φk) − R(Γk)|W 1 ∞ = 3 and

(cid:107)Φk − Γk(cid:107)∞ ≥ k.

(33)

(34)

Note that Γ and Θ need to have multiple exactly opposite weight vectors which add to something
small (compared to the size of the individual vectors)  but not zero  since otherwise reparametrization
would be possible (see Lemma A.5).

7

Figure 4: Shows R(Γ) and g3 (a1 = (1 − 1
2 ) 
a2 = (−1 − 1
2 )  a3 = (0  1)  v = (1  0)).

Figure 5: Shows the weight vectors of Θ2
(grey) and Γ2 (black).

3

Inverse stability for two-layer ReLU Networks

We now establish an inverse stability result using assumptions designed to exclude the pathologies
from the previous section. First we present a rather technical theorem for output dimension one
which considers a parametrization Γ in the unrestricted parametrization space NN and a function g
in the the corresponding function space R(NN ). The aim is to use assumptions which are as weak as
possible  while allowing us to ﬁnd a parametrization Φ of g  whose distance to Γ can be bounded
relative to |g − R(Γ)|W 1 ∞. We then continue by deﬁning a restricted parametrization space N ∗
N   for
which we get uniform inverse stability (meaning that we get the same estimate for every Γ ∈ N ∗
N ).
Theorem 3.1 (Inverse stability at Γ ∈ NN ). Let d  m ∈ N  N := (d  m  1)  β ∈ [0 ∞)  let
Γ =
Assume that the following conditions are satisﬁed:

  cΓ(cid:17) ∈ NN   g ∈ R(NN )  and let I Γ := {i ∈ [m] : aΓ

(cid:12)(cid:12) . . .(cid:12)(cid:12)aΓ

(cid:16)(cid:2)aΓ

i (cid:54)= 0}.

(cid:3)T

m

1

C.1 It holds for all i ∈ [m] with (cid:107)cΓ
C.2 It holds for all i  j ∈ I Γ with i (cid:54)= j that

i aΓ

C.3 There exists a parametrization Θ =

i (cid:107)∞ ≤ 2|g − R(Γ)|W 1 ∞ that |cΓ

i | (cid:107)aΓ

i (cid:107)∞ ≤ β.

(cid:16)(cid:2)aΘ

1

  cΘ(cid:17) ∈ NN such that R(Θ) = g and

j (cid:107)∞ (cid:54)= aΓ
aΓ
(cid:12)(cid:12) . . .(cid:12)(cid:12)aΘ
(cid:3)T
i (cid:107)∞ .
j(cid:107)aΓ
i(cid:107)aΓ
j (cid:107)∞ (cid:54)= − aΓ
i (cid:107)∞ and for all i  j ∈ I Θ with
aΓ
j(cid:107)aΓ
i(cid:107)aΓ

m

(a) it holds for all i  j ∈ I Γ with i (cid:54)= j that

i (cid:54)= j that

j (cid:107)∞ (cid:54)= − aΘ
aΘ
i (cid:107)∞  
j(cid:107)aΘ
i(cid:107)aΘ

(b) it holds for all i ∈ I Γ  j ∈ I Θ that
where I Θ := {i ∈ [m] : aΘ

(cid:54)= 0}.

i

i (cid:107)∞ (cid:54)= − aΘ
aΓ
j(cid:107)aΘ
i(cid:107)aΓ
j (cid:107)∞

Then there exists a parametrization Φ ∈ NN with

R(Φ) = g

and

(cid:107)Φ − Γ(cid:107)∞ ≤ β + 2|g − R(Γ)| 1

2

W 1 ∞ .

(35)

The proof can be found in Appendix A.3.2. Note that each of the conditions in the theorem above
corresponds directly to one of the pathologies in Section 2.2. Condition C.1  which deals with
unbalancedness  only imposes an restriction on the weight pairs whose product is small compared
to the distance of R(Γ) and g. As can be guessed from Example 2.2 and seen in the proof of
Theorem 3.1  such a balancedness assumption is in fact only needed to deal with degenerate cases 
where R(Γ) and g have parts with mismatching directions of negligible magnitude. Otherwise a
matching reparametrization is always possible. Note that a balanced Γ (i.e. |cΓ
i (cid:107)∞) satisﬁes
Condition C.1 with β = (2|g − R(Γ)|W 1 ∞ )1/2.
i | and (cid:107)Γi(cid:107)∞ to be close
It is also possible to relax the balancedness assumption by only requiring |cΓ
to (cid:107)cΓ
i (cid:107)1/2∞   which would still give a similar estimate but with a worse exponent. In order to see that
requiring balancedness does not restrict the space of realizations  observe that the ReLU is positively
homogeneous (i.e. ρ(λx) = λρ(x) for all λ ≥ 0  x ∈ R). Thus balancedness can always be achieved
simply by rescaling.
Condition C.2 requires Γ to have no redundant directions  the necessity of which is demonstrated by
Example 2.3. Note that prohibiting redundant directions does not restrict the space of realizations 

i | = (cid:107)aΓ

i aΓ

8

see (87) in the appendix for details. From a practical point of view  enforcing this condition could
be achieved by a regularization term using a barrier function. Alternatively on could employ a
non-standard approach of combining such redundant neurons by changing one of them according
to (87) and either setting the other one to zero or removing it entirely4.
From a theoretical perspective the ﬁrst two conditions are rather mild  in the sense that they only
restrict the space of parametrizations and not the corresponding space of realizations. Speciﬁcally we
can deﬁne the restricted parametrization space

i (cid:107)∞ = (cid:107)aΓ

(d m D) := {Γ ∈ N(d m D) : (cid:107)cΓ
N (cid:48)

i (cid:107)∞ for all i ∈ [m] and Γ satisﬁes C.2}

(36)
for which we have R(N (cid:48)
N ) = R(NN ). Note that the above deﬁnition as well as the following
deﬁnition and theorem are for networks with arbitrary output dimensions  as the balancedness
condition makes this extension rather straightforward.
In order to satisfy Conditions C.3a and C.3b we need to restrict the parametrization space in a way
which also restricts the corresponding space of realizations. One possibility to do so is the following
approach  which also incorporates the previous restrictions as well as the transition to networks
without biases.
Deﬁnition 3.2 (Restricted parametrization space). Let N = (d  m  D) ∈ N3. We deﬁne

N :=(cid:8)Γ ∈ N (cid:48)

N ∗

i )d > 0 for all i ∈ [m](cid:9) .

While we no longer have R(N ∗
exists Γ ∈ N ∗

(d+2 m+1 D) such that for all x ∈ Rd it holds that

N : (aΓ

i )d−1  (aΓ

(37)
N ) = R(NN )  Lemma A.6 shows that for every Θ ∈ P(d m D) there

R(Γ)(x1  . . .   xd  1 −1) = R(Θ)(x1  . . .   xd).

(38)
In particular  this means that for any optimization problem over an unrestricted parametrization
space P(d m D)  there is a corresponding optimization problem over the parametrization space
N ∗
(d+2 m+1 D) whose solution is at least as good (see Corollary 1.4). Our main result now states that
for such a restricted parametrization space we have uniform (4  1/2) inverse stability w.r.t. | · |W 1 ∞ 
a proof of which can be found in Appendix A.3.2.
Theorem 3.3 (Inverse stability on N ∗
a parametrization Φ ∈ N ∗

N ). Let N ∈ N3. For all Γ ∈ N ∗

N and g ∈ R(N ∗

N ) there exists

N with
R(Φ) = g

and

(cid:107)Φ − Γ(cid:107)∞ ≤ 4|g − R(Γ)| 1

2

W 1 ∞.

(39)

4 Outlook

This contribution investigates the potential insights which may be gained from studying the optimiza-
tion problem over the space of realizations  as well as the difﬁculties encountered when trying to
connect it to the parametrized problem. While Theorem 1.3 and Theorem 3.3 offer some compelling
preliminary answers  there are multiple ways in which they can be extended.
To obtain our inverse stability result for shallow ReLU networks we studied sums of ridge functions.
Extending this result to deep ReLU networks requires understanding their behaviour under com-
position. In particular  we have ridge functions which vanish on some half space  i.e. colloquially
speaking each neuron may “discard half the information” it receives from the previous layer. This
introduces a new type of degeneracy  which one will have to deal with.
Another interesting direction is an extension to inverse stability w.r.t. some weaker norm like (cid:107)·(cid:107)L∞ or
a fractional Sobolev norm under stronger restrictions on the space of parametrizations (see Lemma A.7
for a simple approach using very strong restrictions).
Lastly  note that Theorem 1.3 is not speciﬁc to the ReLU activation function and thus also incentivizes
the study of inverse stability for any other activation function.
From an applied point of view  Conditions C.1-C.3 motivate the implementation of corresponding
regularization (i.e. penalizing unbalancedness and redundancy in the sense of parallel weight vectors)
in state-of-the-art networks  in order to explore whether preventing inverse stability leads to improved
performance in practice. Note that there already are results using  e.g. cosine similarity  as regularizer
to prevent parallel weight vectors [4  35] as well as approaches  called Sobolev Training  reporting
better generalization and data-efﬁciency by employing a Sobolev norm based loss [12].

4This could be of interest in the design of dynamic network architectures [26  28  40] and is also closely

related to the co-adaption of neurons  to counteract which  dropout was invented [21].

9

Acknowledgment

The research of JB and DE was supported by the Austrian Science Fund (FWF) under grants
I3403-N32 and P 30148. The authors would like to thank Pavol Harár for helpful comments.

References
[1] Z. Allen-Zhu  Y. Li  and Z. Song. A Convergence Theory for Deep Learning via Over-

Parameterization. arXiv:1811.03962  2018.

[2] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge

University Press  2009.

[3] S. Arora  R. Ge  B. Neyshabur  and Y. Zhang. Stronger generalization bounds for deep nets via
a compression approach. In International Conference on Machine Learning  pages 254–263 
2018.

[4] N. Bansal  X. Chen  and Z. Wang. Can we gain more from orthogonality regularizations in
training deep networks? In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-
Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages
4261–4271. Curran Associates  Inc.  2018.

[5] P. L. Bartlett  D. J. Foster  and M. Telgarsky. Spectrally-normalized margin bounds for neural

networks. arXiv:1706.08498  2017.

[6] P. L. Bartlett  N. Harvey  C. Liaw  and A. Mehrabian. Nearly-tight VC-dimension and pseudodi-

mension bounds for piecewise linear neural networks. arXiv:1703.02930  2017.

[7] J. Berner  P. Grohs  and A. Jentzen. Analysis of the generalization error: Empirical risk
minimization over deep artiﬁcial neural networks overcomes the curse of dimensionality in the
numerical approximation of Black-Scholes partial differential equations. arXiv:1809.03062 
2018.

[8] J. Berner  D. Elbrächter  P. Grohs  and A. Jentzen. Towards a regularity theory for ReLU

networks–chain rule and global error estimates. arXiv:1905.04992  2019.

[9] H. Bölcskei  P. Grohs  G. Kutyniok  and P. Petersen. Optimal approximation with sparsely

connected deep neural networks. arXiv:1705.01714  2017.

[10] M. Burger and A. Neubauer. Error Bounds for Approximation with Neural Networks . Journal

of Approximation Theory  112(2):235–250  2001.

[11] A. Choromanska  M. Henaff  M. Mathieu  G. B. Arous  and Y. LeCun. The loss surfaces of

multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204  2015.

[12] W. M. Czarnecki  S. Osindero  M. Jaderberg  G. Swirszcz  and R. Pascanu. Sobolev training for
neural networks. In Advances in Neural Information Processing Systems  pages 4278–4287 
2017.

[13] S. S. Du  J. D. Lee  H. Li  L. Wang  and X. Zhai. Gradient Descent Finds Global Minima of

Deep Neural Networks. arXiv:1811.03804  2018.

[14] L. C. Evans. Partial Differential Equations (second edition). Graduate studies in mathematics.

American Mathematical Society  2010.

[15] L. C. Evans and R. F. Gariepy. Measure Theory and Fine Properties of Functions  Revised

Edition. Textbooks in Mathematics. CRC Press  2015.

[16] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks.

Neural Networks  2(3):183–192  1989.

[17] N. Golowich  A. Rakhlin  and O. Shamir. Size-independent sample complexity of neural

networks. arXiv:1712.06541  2017.

10

[18] I. J. Goodfellow  O. Vinyals  and A. M. Saxe. Qualitatively characterizing neural network

optimization problems. arXiv:1412.6544  2014.

[19] R. Gribonval  G. Kutyniok  M. Nielsen  and F. Voigtlaender. Approximation spaces of deep

neural networks. arXiv: 1905.01208  2019.

[20] I. Gühring  G. Kutyniok  and P. Petersen. Error bounds for approximations with deep ReLU

neural networks in W s p norms. arXiv:1902.07896  2019.

[21] G. E. Hinton  N. Srivastava  A. Krizhevsky  I. Sutskever  and R. R. Salakhutdinov. Improving

neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580  2012.

[22] K. Kawaguchi. Deep learning without poor local minima. In Advances in neural information

processing systems  pages 586–594  2016.

[23] H. Li  Z. Xu  G. Taylor  C. Studer  and T. Goldstein. Visualizing the loss landscape of neural

nets. In Advances in Neural Information Processing Systems  pages 6389–6399  2018.

[24] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent
on structured data. In Advances in Neural Information Processing Systems  pages 8157–8166 
2018.

[25] Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with ReLU activation.

In Advances in Neural Information Processing Systems  pages 597–607  2017.

[26] H. Liu  K. Simonyan  and Y. Yang. Darts: Differentiable architecture search. arXiv:1806.09055 

2018.

[27] S. Mei  A. Montanari  and P.-M. Nguyen. A mean ﬁeld view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences  115(33):E7665–E7671 
2018.

[28] R. Miikkulainen  J. Liang  E. Meyerson  A. Rawal  D. Fink  O. Francon  B. Raju  H. Shahrzad 
A. Navruzyan  N. Duffy  and B. Hodjat. Chapter 15 - evolving deep neural networks. In
R. Kozma  C. Alippi  Y. Choe  and F. C. Morabito  editors  Artiﬁcial Intelligence in the Age of
Neural Networks and Brain Computing  pages 293 – 312. Academic Press  2019.

[29] B. Neyshabur  S. Bhojanapalli  D. McAllester  and N. Srebro. Exploring generalization in deep

learning. In Advances in Neural Information Processing Systems  pages 5947–5956  2017.

[30] Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning - Volume 70  ICML’17  pages
2603–2612. JMLR.org  2017.

[31] J. Pennington and Y. Bahri. Geometry of neural network loss surfaces via random matrix
theory. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 
ICML’17  pages 2798–2806. JMLR.org  2017.

[32] D. Perekrestenko  P. Grohs  D. Elbrächter  and H. Bölcskei. The universal approximation power

of ﬁnite-width deep ReLU networks. arXiv:1806.01528  2018.

[33] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using

deep ReLU neural networks. arXiv:1709.05289  2017.

[34] P. Petersen  M. Raslan  and F. Voigtlaender. Topological properties of the set of functions

generated by neural networks of ﬁxed size. arXiv:1806.08459  2018.

[35] P. Rodríguez  J. Gonzalez  G. Cucurull  J. M. Gonfaus  and X. Roca. Regularizing cnns with

locally constrained decorrelations. arXiv:1611.01967  2016.

[36] I. Safran and O. Shamir. On the quality of the initial basin in overspeciﬁed neural networks. In

International Conference on Machine Learning  pages 774–782  2016.

[37] U. Shaham  A. Cloninger  and R. R. Coifman. Provable approximation properties for deep

neural networks. Applied and Computational Harmonic Analysis  44(3):537 – 557  2018.

11

[38] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Convergence
results and optimal averaging schemes. In International Conference on Machine Learning 
pages 71–79  2013.

[39] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks  94:

103–114  2017.

[40] B. Zoph  V. Vasudevan  J. Shlens  and Q. V. Le. Learning transferable architectures for scalable
image recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition  pages 8697–8710  2018.

12

,Dennis Maximilian Elbrächter
Julius Berner
Philipp Grohs