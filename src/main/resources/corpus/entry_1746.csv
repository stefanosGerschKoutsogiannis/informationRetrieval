2019,Efficient Pure Exploration in Adaptive Round model,In the adaptive setting  many multi-armed bandit applications allow the learner to adaptively draw samples and adjust sampling strategy in rounds. In many real applications  not only the query complexity but also the round complexity need to be optimized. In this paper  we study both PAC and exact top-$k$ arm identification problems and design efficient algorithms considering both round complexity and query complexity. For PAC problem  we achieve optimal query complexity and use only $O(\log_{\frac{k}{\delta}}^*(n))$ rounds  which matches the lower bound of round complexity  while most of existing works need $\Theta(\log \frac{n}{k})$ rounds. For exact top-$k$ arm identification  we improve the round complexity factor from $\log n$ to $\log_{\frac{1}{\delta}}^*(n)$  and achieve near optimal query complexity.  In experiments  our algorithms conduct far fewer rounds  and outperform state of the art by orders of magnitude with respect to query cost.,Efﬁcient Pure Exploration in Adaptive Round model

Tianyuan Jin†  Jieming Shi‡  Xiaokui Xiao‡  Enhong Chen†∗

†School of Computer Science and Technology  University of Science and Technology of China

‡School of Computing  National University of Singapore

† jty123@mail.ustc.edu.cn  cheneh@ustc.edu.cn  ‡{shijm  xkxiao}@nus.edu.sg

Abstract

In the adaptive setting  many multi-armed bandit applications allow the learner
to adaptively draw samples and adjust sampling strategy in rounds.
In many
real applications  not only the query complexity but also the round complexity
need to be optimized. In this paper  we study both PAC and exact top-k arm
identiﬁcation problems and design efﬁcient algorithms considering both round
complexity and query complexity. For PAC problem  we achieve optimal query

complexity and use only O(log∗
of round complexity  while most of existing works need Θ(log n

δ(n)) rounds  which matches the lower bound
k) rounds. For

k

1

δ(n)  and achieve near optimal query complexity.

exact top-k arm identiﬁcation  we improve the round complexity factor from log n
to log∗
In experiments  our
algorithms conduct far fewer rounds  and outperform state of the art by orders of
magnitude with respect to query cost.

1

Introduction

Mutli-armed bandit (MAB) problems are classic decision problems with numerous applications such
as medical trials [1]  online advertisement [2]  and crowdsourcing [3]. These problems typically
consider a bandit with a set of arms  each of which has an unknown reward distribution with an
unknown mean  and the objective is either to (i) identify the top-k arms with the maximum reward
means or (ii) maximize the expected total reward under some constrains on the costs of arm pulling.

This paper studies the problem of top-k arms identiﬁcation in the adaptive setting  which allows the
leaner to draw samples from the arms adaptively in rounds to estimate their means  and to adjust the

sampling strategy for the i-th round based on the observations from the ﬁrst i− 1 rounds. Following

previous work [4]  we assume that in each round  the learner is allowed to query an arbitrary number
of arms for an arbitrary number of times  but the query results would only be revealed at the end of
the round. We aim to minimize the number of rounds performed  as well as achieving best possible
query complexity. In addition  our proposed algorithms exhibit superior practical performance due to
our small constant factors. Existing top-k algorithms mainly focus on query complexity and most of
them are not efﬁcient due to their large constants [5  6  3  7  8] or inferior query complexities [9  10].
Adaptive round setting of MAB has many real applications  as described below.

Medical trials. In medical trials [11]  to identify the best drug for a disease  one can conduct tests in
rounds  such that each round involves testing multiple candidate drugs on multiple clinical subjects
(e.g.  mice) simultaneously. However  after each round of testing  there is typically a waiting time
(e.g.  days) before the effects of drugs become observable to guide the design of the next round of
testing. It is important to minimize not only the total number of tests on clinical subjects (i.e.  query
complexity) but also the number of rounds  to identify the best drug within the shortest time frame.

∗Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Online advertisement. In online advertisement [12]  an advertiser may push ads to the users of
candidate websites  so as to identify the top-k websites that have the highest click-through rates and
match some clients advertising requirements. The pushing of ads could be conducted in rounds  and
each round may involve multiple ads and multiple users. However  in each round  it takes time to
observe users’ responses to the ads  and to decide which websites are unpromising and should be
pruned in the next round. In this application  there is usually a tight time frame to offer a solution to
the clients  so as to ensure the timeliness of the ads.

Crowdsourcing. Workers on crowdsourcing platforms often vary signiﬁcantly in terms of the answer
quality. As an effective strategy to identify the most reliable workers for a speciﬁc task  one may test
each worker with a sequence of questions with ground-truths  and then select workers based on the
accuracy of their answers. Note that for such tests  workers need some time to answer the questions 
and need to be rewarded upon the completion of the questions. To minimize the time and monetary
cots  it is crucial to have an algorithm to identify the most reliable workers that minimizes the number
of tests (i.e.  query complexities) within a limited number of rounds  where our proposals ﬁt.

1.1 Problem Formulation

Under the standard setting of stochastic multi-armed bandit selection  there is a set S of n arms 

such that each arm i is associated with an unknown reward distributionDi supported on[0  1] with

unknown mean θi. Let i∗ be the arm with ith largest mean. We aim to identify the k arms with the
largest means by pulling (i.e.  sampling from) the arms in rounds. In each round  we can pull any
number of arms for any number of times  such that (i) each pull of an arm i returns a reward that is an

i.i.d. sample fromDi  and (ii) the reward is only revealed at the end of the round.

For PAC subset selection  we study two problems: (i) Problem 1 (PAC-top-k): PAC Top-k Arm
Selection with Adaptive Rounds  and (ii) Problem 2 (RL-top-k): Top-k Arm with a Round Limit R.

In both problems  the goal is to identify a set V ⊆ S of k arms  such that for all i ∈ [1  k]  the ith
largest arm in V has mean larger than θi∗ − ǫ with probability at least 1− δ  where ǫ and δ are given

constants. Speciﬁcally  for PAC-top-k  we aim to minimize the number of rounds performed  while
achieving the best possible query complexity; for RL-top-k  we expose a upper limit on the number
of rounds that can perform  R  and aim to minimize the query complexity within R rounds.

For exact top-k arm identiﬁcation  denoted as Problem 3 (exact-top-k)  we aim to minimize the
number of rounds required as well as the query cost  for identifying the top-k arms with the largest

means. We assume θk∗ > θ(k+1)∗   in order to ensure the uniqueness of the solution.

1.2 State of the Art

To the best of our knowledge  Agarwal et al.’s work [4] is the only one that studies the top-k arms
problem while taking into account the round complexity. In particular  [4] studies the identiﬁcation
of exact top-k arms with adaptive rounds  and presents a method that takes ∆k as input and returns

δ and round
the exact top-k arms with at least 1− δ probability  with query complexity O n
complexity2 log∗(n)   where ∆k denotes the difference between the means of the kth and(k + 1)th
largest arms  and log∗(n) denotes the iterated logarithm of n  i.e. 
if n > 1

log∗(n)=1+ log∗(log n) 

otherwise

⋅ log k

∆2
k

(1)

0 

⋅(log k

the logarithm function on n for R times  i.e. 

n before the result is no more than 1. Furthermore  [4] also studies the problem where the round limit
R is given. Their algorithm identiﬁes the exact top-k arms with at least 1 − δ probability  with a query

In other words  log∗(n) equals the number of times that we need to apply the logarithm function on
+ ilog(R)(n))  where ilog(R)(n) is the result of iteratively applying
complexity of O n
ilog(r)(x)=ilog(r−1)(log(x)) 
if x> 1
if x≤ 1.
With respect to lower bound  Agarwal et al. show that a round complexity of log∗(n) is near
optimal  since for constants k and δ  any algorithm with O( n
k) query complexity requires at least

∆2
k

(2)

∆2

1 

δ

2All logarithms(e.g.  log∗

b(n)) in this paper are to base b.

2

k = 1
All k ∈ [n]

Algorithm

[5]

[6  16  14]

This paper (Algorithm 1)

Number of Rounds Query Complexity

Θ(log n)
Θ(log n
k)
δ (n)
2 log∗

k

O n
O n
O n

δ
ǫ2 ⋅ log 1
δ
ǫ2 ⋅ log k
δ
ǫ2 ⋅ log k

Table 1: Summary of algorithms for Problem 1: Top-k arms with adaptive rounds.

Algorithm

Bound

Query Complexity

All k ∈ [n]

[4]  assuming ∆k is known

This paper (Algorithm 2)

exact top-k O n
O n

(ǫ  δ)

⋅(log k
ǫ2 ⋅(log k

∆2
k

δ

δ

+ ilog(R)(n))
δ (n))
+ ilog(R)

k

Table 2: Summary of algorithms for Problem 2: Top-k arms with a round limit R

log∗(n)− log∗(Θ(log∗(n))) rounds. Besides  Agarwal et al. prove that identifying the exact top-k
arms with at least 3~4 probability using R rounds must use Ω n

kR4 ⋅ ilog(r) n

k samples.

∆2

.

Agarwal et al.’s algorithm suffers from a major deﬁciency that it requires ∆k to be known in advance 
which is unrealistic in most practical applications as the mean of each arm is unknown. In addition 
the algorithm cannot be extended to address PAC-top-k and RL-top-k by replacing ∆k with ǫ  since
the algorithm strongly relies on the assumption that there is exact k arms whose means are larger
than θk∗ − ∆k  where k∗ is the arm with the kth largest mean. (This assumption does not hold in

general if we replace ∆k with any ǫ > ∆k.) Further  the algorithm cannot be used to get instance-
on{θi}n

dependent query complexity (where the query complexity not only depends on ∆k but also depends
i=1)  since all Exponential-Gap-Elimination algorithms [8  13  14  15] need a PAC algorithm

as a subroutine.

There also exists a number of techniques [16  6  13  10  14  8] for both PAC and exact top-k arm
identiﬁcation problems that optimizes the query complexity  without considering the round complexity.
The query complexity achieved by these technique is near optimal. However  all of these incur log n
factor on round complexity  signiﬁcantly worse than the round complexity of [4].

1.3 Our Results

In this paper  we present three algorithms for the top-k arm selection problems in adaptive round
model. Below summarizes our results.
Theorem 1. There is an algorithm that computes ǫ-top-k arms with probability at least 1 − δ  pulls

ǫ2 ⋅ log k
ǫ2 ⋅(ilog(R)

Theorem 2. There is an algorithm that computes ǫ-top-k arms with probability at least 1 − δ  pulls

the arms at most O( n
the arms at most O( n
special cases of PAC-top-k and RL-top-k with ǫ← ∆k  the round complexity of our algorithm for

δ) times and runs in at most 2 log∗
δ (n) + log k

Since (i) the solution in [4] is proved to be near-optimal  and (ii) the problems studied in [4] are

δ) times and runs within R rounds.

δ(n) expected rounds.

PAC-top-k and the query complexity of our algorithm for RL-top-k are near-optimal.

k

k

Compared with the solution in [4]  our algorithms do not require any prior knowledge of ∆k  and

allow us to choose an error parameter ǫ ∈ (0  1) to strike a trade-off between the accuracy and

efﬁciency of the algorithm  which is much more practical. Further  our PAC version can be used to
get instance-dependent query complexity while [4] can not.
Theorem 3. There is an algorithm that computes exact top-k arms with probability at least 1 − δ 

pulls the arms at most O∑n

Compared with the previous exact top-k arm algorithms [14  8  13]  we improve the factor on round
complexity from log n to log∗
summarize our results and those of the state-of-the-art methods.

k ) rounds.
δ(n)  while achieving the same query complexity. Tables 1  2 and 3

 times and runs in O(log∗

log k⋅log ∆−1

n ⋅ log ∆−1

i=1 ∆−2

1
δ

δ

i

i

1

2 PAC Subset Selection

We present our algorithms for the PAC top-k arms selection problems  i.e.  PAC-top-k and RL-top-k.

3

k = 1

All k ∈ [n]

Algorithm

[8]

[17]

[10]

[13]

This paper

O∑n
O∑n

Round Complexity

i

i

δ

log ∆−1

k )
O(log n ⋅ log ∆−1
i=1 ∆−2

⋅ log
i=1 ∆−1
⋅ log ∑n
i=1 ∆−2
k )
O(log n ⋅ log ∆−1
k )
O(log∗
n ⋅ log ∆−1

1
δ

δ

i

i

O∑n
O∑n

 O∑n
O∑n
O∑n







Query Complexity
i=1 ∆−2

⋅ log

i

δ

log ∆−1

i

i=1 ∆−2

i

i=1 ∆−2

i

log ∆−1

i

δ

i=1 ∆−1

i

⋅ log
⋅ log ∑n

i=1 ∆−2

i

i=1 ∆−2

i

⋅ log

⋅ log

k⋅log ∆−1

k⋅log ∆−1

δ

δ

δ

i

i

Table 3: Summary of algorithms for Problem 3: Exact top-k arm identiﬁcation. (For i ≤ k  ∆i
denotes the difference between the means of the ith and (k+ 1)th arms. For i > k  ∆i denotes the

difference between the means of the kth and ith arms.)

2.1 Top-k δ-Elimination

k

k

100(1− δ

k-δE (Algorithm 1) can identify the top-k arms for PAC-top-k  with query complexity O( n

and at most 2 log∗
e.g.  [16  6]  which only eliminate half of the candidates in each round  k-δE can eliminate at least

δ)
ǫ2 log k
δ(n) expected rounds. Compared with Median Elimination based top-k algorithms 
k) percent of candidate arms every other round  which is far better. We go through the

δ(n). Without ambiguity  r means iterations in Algorithm 1  but means rounds in Algorithm 2.

algorithm ﬁrst and then explain why. Note that in each while iteration (Line 5-17)  k-δE performs
two separate rounds of pulling (Line 5 and Line 8)  since the pulls at Line 8 are dependent on
the empirical results obtained at Line 5. This corresponds to the 2 factor in our round complexity
2 log∗
Algorithm 1 takes as input S  Q  k  ǫ  δ  where S is the set of all the arms and Q = c
ǫ2 (c is an constant
factor determined in Lemma 1). An empty set S′ (Line 3) is initialized for the storage of the arms and
their empirical means obtained later in the algorithm. In each iteration  we pull every arm in Sr by
Qr times  and sort them by their empirical means (Line 5). At Line 7-8  we double test the empirical
k′ (in order to keep the estimation unbiased) and keep it in S′. Then we update
mean of each arm in Sr
Sr to Sr+1 by only keeping the arms with empirical means 3~4ǫ greater than the kth largest mean in

S′  and also excluding the arms in Sr
k′ (Line 10). From Line 11 to 15  we update βr and δr  which
can make Qr exponentially decrease in next iteration. This is critical to keep the total number of pulls
linear to n. The whole process continues until Sr is empty  then the top-k arms in S′ are returned.

to sample O((1~ǫ2

Median Elimination (ME) methods can only allow ǫr regret in each iteration(∑r ǫr ≤ ǫ)  in order to

guarantee ǫ error bound even when the best arm is mistakenly eliminated. On the other hand  k-δE
allows ǫ loss in each iteration with the help of S′ and double test  which allows us to perform fewer
pulls and eliminate more than half of the arms per iteration. During a iteration r  ME methods need

r) log(k~δr)) times per arm  much larger compared to O((1~ǫ2) log(k~δr)). It is

even worse when r increases (i.e.  ǫr decreases)  leading to the large constant factors in ME methods.
Speciﬁcally  in Algorithm 1  S′ stores randomly chosen arms that are eliminated. It holds that the
top ith(i ≤ k) arm stored in S′ is at most ǫ smaller than ith eliminated arm (see the proof details of
Lemma 1). If k-δE has eliminated the ith largest arm  then with high probability the ith largest arm
stored in S′ must be the ǫ-approximate of ith largest arm. Hence  k-δE allows ǫ loss per iteration.

Moreover  k-δE uses a more aggressive indicator to eliminate arms  compared to the median indicator
used in Median Elimination based algorithms. We use as our indicator  the kth largest empirical mean

of the randomly chosen top arms stored in S′  plus 3~4ǫ (Line 10 of Algorithm 1). However  directly

using such indicator without double test  the indicator may be positively biased. And then all the
ǫ-top-k arms might be eliminated with such indicator  which leads to wrong results. To deal with this 
we use double-test strategy to re-sample another Qr times at Line 8 before using the indicator at Line

10 in Algorithm 1  to keep the indicator unbiased. Further  3~4ǫ increment is added to the indicator to

eliminate more arms safely  proved in Lemma 1. k-δE runs in 2 log∗

k

δ(n) expected rounds.

Compared to [4]  our Algorithm 1 and Algorithm 2 are fundamentally different. We assume no prior
knowledge of the arms  e.g.  ∆k. Given ∆k  Agarwal et al.’s algorithm can compute an optimal
indicator to eliminate the arms deﬁnitely not in top-k. Our indicator (Line 10) is set with the help of
S′ and double test  which gives our algorithm near-optimal round complexity 2 log∗

k

δ(n).

4

k′   double test by re-sampling it Qr times and insert its new empirical mean into S′;

− 1] sorted arms as set Sr
k′ ;

δr) times; sort them decreasingly by empirical means ˆθi;

Algorithm 1 Top-k δ-Elimination (k-δE)
1: Input: S  Q  k  ε and δ.

2: Initialize r ← 1  β1 ← 1  δ1 ← δ~4  S1 ← S.
3: Initialize S′ ←∅.
4: while Sr ≠∅ do

5:

6:
7:
8:
9:
10:
11:

Sample each arm i ∈ Sr for Qr ← βr ⋅ Q ⋅ log( k
k′ ← min{k Sr};
Uniformly sample k′ arms from the top-[⌈(δr~k)βr ⋅Sr~2⌉ + k′
For each arm i ∈ Sr
Get the k-th largest mean in S′ as S′(k);
Set Sr+1 ← {i ∈ Sr ∶ ˆθi ≥ S′(k) + 3ǫ~4} and Sr+1 ← Sr+1ƒSr
if Sr+1≤ 2δ
βr+1 ← βr
βr+1 ← βr
Sr 
Sr+1 ;
δr+1 ← δ~(2 ⋅ 2r);
r ← r + 1;

12:
13:
14:
15:
16:
17:
18: end while
19: Return: Top-k arms in S′.

k Sr then
Sr 
2Sr+1 ;

end if

else

k′ ;

2.2 Bounding the Regret  Query Complexity  and Round Number of k-δE

2

(ǫ−ǫ1)2  

We bound the regret in k-δE and give its query and round complexity. The proofs are in Appendix B.

Algorithm 1 still works with the (ǫ  δ) guarantee.

Lemma 1. Given a n-arm set  S  parameter ǫ∈(0  1)  and δ ∈(0  1~4)  it sufﬁces to run Algorithm 1
with Q ≥ 32
ǫ2 in order to obtain a k-sized subset V ⊆ S  such that with probability at least 1− δ 
the ith largest arm in V has mean larger than θi∗ − ǫ  for all i ∈ [1  k]. Additionally  if we change
the parameter 3~4ǫ (Line 10 in Algorithm 1) to ǫ1  where ǫ1 ∈ (0  ǫ)  then by setting Q ≥
Lemma 1 provides the (ǫ  δ) guarantee of algorithm k-δE. Lemma 2 shows that  w.h.p.  Sr+1 is
(δ~k)−βr times smaller than Sr  which is used in Lemma 3 to bound the round complexity.
ǫ2 and δ ∈(0  1~4)  then at iteration r  with probability at least 1− 2δr Sr+1≤
Lemma 2. If Q≥ 57
⌈2 ⋅(δr~k)βrSr⌉ − 1.
Lemma 3. For Q≥ 57~ǫ2 and δ ∈ (0  1~4)  with probability at least 1 − δ  the number of rounds R′
used in k-δE satisﬁes: R′ ≤ 2 log∗
8] present a lower bound of Ω( n
Lemma 4. Let N be the number of arms pulled by Algorithm 1. For Q ≥ 57
least 1 − δ  N ≤ 7n ⋅ Q ⋅ log(4k~δ); and E[N]≤ 7(n + 1) ⋅ Q ⋅ log(4k~δ).

Hence  up to a small constant factor  our query complexity is optimal. Combining Lemma 1 3  and 4 
Theorem 1 follows.

δ) for PAC version (the Explore-k metric  see Section 5).

Next  we provide the query complexity of k-δE in Lemma 4. Kalyanakrishnan et al. [10  Theorem

δ (n)  and E[R′]≤ 2(1 + 2δ) log∗

ǫ2   with probability at

Remark 1. In previous work [16]  as the theoretical analysis is rather pessimistic due to the extensive
usage of the union bound  the constant to achieve regret bound are far from tight. The constant can

In k-δE  (i) our constants (Lemma 4) are much smaller; (ii) our constant factor is adjustable according

be even up to 105(i.e.  the bound is N ≥ 105
to Lemma 1 with the ǫ regret bound still guaranteed (for instance  the 3~4ǫ factor in Line 10 of
Algorithm 1 can be changed to 1~2ǫ  then setting Q≥ 8

ǫ2 still guarantees the PAC bound); (iii) our
algorithm can stop as soon as it is conﬁdent to ﬁnd the correct arm  reducing the practical query cost.

δ in previous works).

δ(n).

ǫ2 log 1

ǫ2 log k

k

k

2.3 Top-k Arm Selection with a Round Limit

In this section  we propose k-δER (Algorithm 2) to solve Problem 2  top-k arm selection with a
round limit R. Our proposal can report correct result within R rounds  with almost optimal query
complexity. Compared to k-δE  k-δER only requires one round (Line 4) per iteration  rather than

5

Algorithm 2 Top-k δ-Elimination with Limited Rounds (k-δER)
1: Input: S  R  k Q  ǫ and δ.

2: Initialize r ← 1  δ1 ← δ~4  β1 ← 1+ i log(R)
3: for r ≤ R− 1 do

k

δ (n)  S1 ← S  S′ ←∅.

4:
5:
6:
7:
8:
9:
10:

Sample each arm in Sr by Qr ← βr ⋅ Q ⋅ log(k~δr) times  and sort decreasingly by their empirical ˆθi;
k′ ← min{k Sr}.
Uniformly sample k′ arms from the top-[⌈(δr~k)βr ⋅Sr~2⌉ + k′
k′ into S′;
Add Sr
Let Sr+1 be set containing all the top-[⌈2 ⋅(δr~k)βrSr⌉ + k′
Sr+1 ← Sr+1ƒSr
k′ ;
βr+1 ← βr
Sr 
2Sr+1 ;
δr+1 ← δ~(2 ⋅ 2r);
r ← r + 1;

− 1] sorted arms as set Sr

− 1] sorted arms in Sr;

k′ ;

11:
12:
13: end for

14: Return: US(S′  SR  Q  βR  δ  k).

Algorithm 3 Uniformly Sampling (US)
1: Input: S′  SR  Q  βR  δ  k.

3: Let SR

2: Sample each arm i ∈ SR by Q ⋅ βR ⋅ log 2k⋅2R
4: Sample each arm i ∈ S′ by Q log 4S′

k be the set of all the top-min{k SR} arms;

5: Return: Top-k arms in SR

δ

δ

k ⋃ S′.

times and sort decreasingly by their empirical ˆθi;

times  and let ˆθi be its empirical mean;

two rounds per iteration. According to Lemma 2  in each iteration  we can bound the total number of
arms in Sr+1 without double test. Thus  rather than performing the double test immediately (Line 8 
Algorithm 1)  k-δER delays all the double-tests of all the iterations until the ﬁnal round (Line 14 
Algorithm 2)  and conducts all the double-tests in this round  using uniform sampling (Algorithm 3).

Algorithm 2 shows the pseudo-code of k-δER. It takes as input one more parameter  R  the round

limit. S′ stores all the arms delayed for double test in the ﬁrst R − 1 rounds (Line 7). At Line

14  Algorithm 3 is called to sample all the arms in both S′ and SR in one round  and then the the
top-k arms are reported. Note that in Algorithm 3  the samples in Line 2 and 4 can be submitted
simultaneously  so this only cost one round. Compared to Median Elimination algorithms  k-δER has
similar advantages as k-δE analyzed in Section 3.1. With the help of S′ and double test  k-δER can

eliminate more arms in each round  while still provides(ǫ  δ) guarantee  as follows.
δ(n)  it
Lemma 5. Given a n-arm set  S  parameters k  ǫ ∈ (0  1)  δ ∈ (0  1~4)  and 1 ≤ R ≤ log∗
ǫ2 in order to obtain a k-sized subset V ⊆ S  such that with
sufﬁces to run Algorithm 2 with Q ≥ 57
probability at least 1− δ  the ith largest arm in V has mean larger than θi∗ − ǫ  for all i∈[1  k].
R ≥ log∗
Lemma 6. If Q ≥ 57
O n

δ(n)  our algorithm can achieve the optimal query complexity using just log∗

δ(n) rounds.
δ(n)  Algorithm 2 uses

ǫ2   with target number of rounds 1 ≤ R ≤ log∗

Details of the proof are in Appendix B. Lemma 6 bounds the query complexity of k-δER. When

δ (n)+ log(k~δ) samples.

Combining Lemma 5 and 6  Theorem 2 follows.

ǫ2 ilog(R)

k

k

k

k

k

3 Exact Top-k Arm Identiﬁcation

Here we solve exact-top-k to identify exact top-k arms. Our algorithm uses the Exponential-Gap-
Elimination algorithm(e.g.  [13  14  8]) as a framework  and uses Algorithm 1 as a component.
Speciﬁcally  we replace the Median Elimination Algorithm used in [13  14  8] by Algorithm 1 and
then prove the newly algorithms satisﬁes Theorem 3. Here  we use [13] as an example. In [13]  it
has three subroutines  called PAC-Best-k  EstMean-Large  EstMean-Small. We replace all of these
subroutines by Algorithm 1  to get our algorithm for exact-top-k  denoted as Algorithm 4. In [13] 

6

12

10

10

10

8

10

t
s
o
C

E

ME

ER

12

10

10

10

8

10

t
s
o
C

E

ME

ER

12

10

10

10

8

10

t
s
o
C

E

ME

ER

6

10

0.01

0.03

0.05

0.07

0.09

6

10

0.01

0.03

0.05

0.07

0.09

6

10

0.01

0.03

0.05

0.07

0.09

(a) Uniform Dataset.

(b) Normal Dataset.

(c) Segment Dataset.

Figure 1: Query cost of PAC best arm selection.

12

10

10

10

t
s
o
C

8

10

k- E

ME-AS

k- ER

12

10

10

10

t
s
o
C

8

10

k- E

ME-AS

k- ER

12

10

10

10

t
s
o
C

8

10

k- E

ME-AS

k- ER

6

10

0.01

0.03

0.05

0.07

0.09

6

10

0.01

0.03

0.05

0.07

0.09

6

10

0.01

0.03

0.05

0.07

0.09

(a) Uniform Dataset.

(b) Normal Dataset.

(c) Segment Dataset.

Figure 2: Query cost of PAC top-k arm selection.

it is proved that the query complexity is no worse than O∑n

Lemma 7 and 8  we get Theorem 3. The proofs are in Appendix B.

i=1 ∆−2

i

⋅ log k⋅log ∆−1

i

δ

. Combining

Lemma 7 ([13]  Theorem 1.2). Algorithm 4 returns the correct answer with probability at least 1 − δ

and takes O∑n
⋅ log k⋅log ∆−1
Lemma 8. Algorithm 4 runs in O(log∗

 samples.
δ(n) ⋅ log ∆−1

i=1 ∆−2

δ

i

i

k

k ) rounds.

4 Experiments

4.1 Experimental Results for PAC Top-k Identiﬁcation

For PAC top-k arms  we compare k-δE and k-δER with median elimination method ME-AS [16].

arm algorithm ME [5]. We do not experimentally compare to [4] since there is no prior knowledge
of ∆k in this paper. Note that ME-AS is designed for relative error. To make a fair comparison 

When k = 1  we denote k-δE and k-δER  as δE and δER respectively  and compare them with the best
given the absolute error bound ǫ  we transform it to ǫ~θ1  where θ1 is the largest mean in the given
bandit. ǫ~θ1 is used as the equivalent relative error bound in ME-AS. As proved in Lemma 1  without
compromising correctness  we can adjust the elimination indicator in k-δE (Line 10 in Algorithm 1).
We change 3~4ǫ to 1~2ǫ and set Q to be 8
ǫ2 in our implementation  to gain even better performance.
Without loss of generality  we test our algorithms and competitors on arms following independent
Bernoulli distributions with various means. We set the number of total arms to be n = 2000. We test

the methods on three synthetic datasets  as follows:

• Uniform: θi ∼ Unif[0  1]. The mean of arms  θi  are uniformly distributed in[0  1].
• Normal: θi ∼ T N(0.5  0.2). Each θi is generated from a truncated normal distribution with
mean 0.5  the standard deviation 0.2 and the support[0  1].
• Segment: θi = 0.5 for i = 1  ⋯  k and θi = 0.4 for i = k + 1  ⋯  n.

Default parameter values are set as: δ = 0.1  and R = 2. For each setting  the results are averaged over

100 repeated runs. As shown later  ME-AS can be very costly and takes too long time to obtain their
average performance over 100 runs  so we terminate them when time is up and report the average
obtained. We vary ǫ from 0.01 to 0.1  while keeping other parameters unchanged.

7

Algorithm Uniform Normal

Segment

k = 1

k = 20

ME
δE
δER

ME-AS

k-δE
k-δER

11
2.2
2
6
2.1
2

11
3.4
2
6
3.0
2

11
3.9
2
6
3.8
2

Table 4: Number of rounds performed.

Dataset

Algorithm Rounds

Query Cost

21
36

Normal

Uniform

Segment

EG-δE

[8]
[17]

EG-δE

[8]
[17]

EG-δE

[8]
[17]

1.4× 108
6.7× 109
0.9× 108
2.8× 109
1.2× 1011
2.4× 109
5.6× 107
1.3× 1010
2.2× 108

27
59

0.9× 108
2.4× 109
2.2× 108

6
24

Table 5: Exact top-k arms: rounds and query cost.

For the best arm selection (k = 1)  Figure 1 reports the query cost (i.e.  total number of pulls) for

δE  δER  and ME on the three datasets. Both δE and δER outperform ME signiﬁcantly for all the ǫ
values on the three datasets. δE is about 100 times faster than ME  while δER is about 10 times faster

than ME. δE is faster than δER since δER has hard round limit R = 2  while δE does not has such

constraint. The ﬁrst row of Table 4 shows the number of rounds actually used by each method. δER
strictly uses only two rounds limited by R  and δE needs slightly more rounds  while ME requires 11

rounds that is several times more. For the top-k arm selection (k = 20)  Figure 2 reports the query

cost for k-δE  k-δER  and ME-AS when varying ǫ. k-δER is about 100 times faster than ME-AS 
and k-δE is about 1000 times faster. The second row of Table 4 shows the number of rounds used
per method. Our methods use far fewer rounds. In summary  our k-δE  k-δER outperform ME and
ME-AS with a huge performance gap.

4.2 Experimental Results for Exact Top-k Arm Identiﬁcation

We evaluate our algorithm for exact top-k arm identiﬁcation. Our algorithm choose [8] as framework 
since [14  13] only focus on theory part and have big constants. We call our algorithm EG-δE
(Exponential-Gap + δE)  and compare it with Elimination based [8] and UCB based [17] algorithms.

Default parameter is set as: δ = 0.1. We set [17]’s parameters following their experimental setting.

Other experimental settings are same as the PAC-top-k problems.

Table 5 reports the query and round cost for different methods. Compare with [8]  EG-δE uses fewer
rounds and is up to 250 times faster than [8] with respect to query cost. Compare with [17]  EG-δE
uses signiﬁcantly fewer rounds while keeps the query cost on same order.

5 Related Work

Instance-independent arm selection. Top-k arm selection is ﬁrst studied under the setting of
instance-independent. All such existing works [5  18  6  16  3  14] are designed for worst case query

complexity  and need Θ(log n
k) rounds  which is inferior to ours. Median Elimination [5] ﬁnds the
best arm (when k = 1) with query complexity O( n
δ) under PAC bound  matching the lower
bound in [18]. Our top-k algorithms can be easily handle best arm selection by simply setting k = 1.
ith-top arm has mean greater than θi∗ − ǫ  for all i ∈[1  k]  where θi∗ is ith largest mean in the whole
greater than θk∗ − ǫ  where θk∗ is kth largest mean in the whole bandit. Our metric is tighter than

bandit. Explore-k metric is studied in [6]: with high probability  all the k selected arms have mean

We use the same top-k arm deﬁnition as [16]  which requires that  with high probability  the selected

ǫ2 log 1

8

Explore-k metric  and thus our algorithms can also apply to solve Explore-k problem. Another metric
was considered in [3]  where the identiﬁed k arms can have at most kǫ regret in total. [14] studies
multi-armed bandit problem under matroid constraints. All these works are elimination based.

Instance-dependent arm selection. The query complexity of instance-optimal algorithms (e.g. 
[4  7  13  15  8  10  9]) is closely tied to the bandit instance and is better than the worst case
complexity for ‘easy’ bandit instances. Some of them [7  13  15  8] are elimination-based  and use
instance-independent algorithms like [6] and [5] as a sub-procedure to eliminate the arms. Due to
the usage of instance-independent algorithms  in the worst case  each iteration of these instance-

dependent algorithms needs log n rounds. Thus the total round complexity is O(log n ⋅ log ∆−1
k ).

Another instance-dependent approach is based on upper or lower conﬁdence bounds (UCB or LUCB) 
e.g.  [10] [9]. With respect to query complexity  UCB methods require a log n factor  while it is log k
in the lower bound. For round complexity  UCB methods need a huge number of rounds since their
round complexity is proportional to the query complexity due to their nature of fully adaptiveness.

Variant settings on limited rounds. Under the delayed feedback setting  the reward of pulling an
arm in round τ is delayed to be shown in later round τ + t [19  20]. Our methods can simulate this
setting when taking an appropriately high value of t. Most of the existing works focus on regret
minimization rather than top-k arms. Some works [11  21  22] investigate the batches arm problem.
[11] only considers the regret minimization. [21] only allows to pull an arm once per round; the

number of rounds required is Ω(log n). In [22]  within a round  there are limits for both the number
of total pulls and the number of pulls per arm. Its rounds in the worst case is at least Ω(log n).

6 Conclusion

We study the problems of top-k arm selection in adaptive round model  and propose algorithms
that achieve the near-optimal query complexity and match the lower bound of round complexity. In
practice  our algorithms outperform existing methods in terms of query cost and round complexity.

Acknowledgement

This research was supported by National Natural Science Foundation of China (No.U1605251) and
by the National University of Singapore under SUG grant R-252-000-686-133.

References

[1] William R Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[2] Dimitris Bertsimas and Adam J Mersereau. A learning approach for interactive marketing to a

customer segment. Operations Research  55(6):1120–1135  2007.

[3] Yuan Zhou  Xi Chen  and Jian Li. Optimal pac multiple arm identiﬁcation with applications to

crowdsourcing. In International Conference on Machine Learning  pages 217–225  2014.

[4] Arpit Agarwal  Shivani Agarwal  Sepehr Assadi  and Sanjeev Khanna. Learning with limited
rounds of adaptivity: Coin tossing  multi-armed bandits  and ranking from pairwise comparisons.
In Conference on Learning Theory  pages 39–75  2017.

[5] Eyal Even-Dar  Shie Mannor  and Yishay Mansour. Pac bounds for multi-armed bandit and
markov decision processes. In International Conference on Computational Learning Theory 
pages 255–270. Springer  2002.

[6] Shivaram Kalyanakrishnan and Peter Stone. Efﬁcient selection of multiple bandit arms: theory

and practice. In ICML  pages 511–518  2010.

[7] Jiecao Chen  Xi Chen  Qin Zhang  and Yuan Zhou. Adaptive multiple-arm identiﬁcation. In

International Conference on Machine Learning  pages 722–730  2017.

[8] Zohar Karnin  Tomer Koren  and Oren Somekh. Almost optimal exploration in multi-armed

bandits. In International Conference on Machine Learning  pages 1238–1246  2013.

9

[9] Shouyuan Chen  Tian Lin  Irwin King  Michael R Lyu  and Wei Chen. Combinatorial pure
exploration of multi-armed bandits. In Advances in Neural Information Processing Systems 
pages 379–387  2014.

[10] Shivaram Kalyanakrishnan  Ambuj Tewari  Peter Auer  and Peter Stone. Pac subset selection in

stochastic multi-armed bandits. In ICML  volume 12  pages 655–662  2012.

[11] Vianney Perchet  Philippe Rigollet  Sylvain Chassang  Erik Snowberg  et al. Batched bandit

problems. The Annals of Statistics  44(2):660–681  2016.

[12] Eric M Schwartz  Eric T Bradlow  and Peter S Fader. Customer acquisition via display
advertising using multi-armed bandit experiments. Marketing Science  36(4):500–522  2017.

[13] Lijie Chen  Jian Li  and Mingda Qiao. Nearly instance optimal sample complexity bounds for

top-k arm selection. In Artiﬁcial Intelligence and Statistics  pages 101–110  2017.

[14] Lijie Chen  Anupam Gupta  and Jian Li. Pure exploration of multi-armed bandit under matroid

constraints. In Conference on Learning Theory  pages 647–669  2016.

[15] Lijie Chen  Jian Li  and Mingda Qiao. Towards instance optimal bounds for best arm identiﬁca-

tion. In Conference on Learning Theory  pages 535–592  2017.

[16] Wei Cao  Jian Li  Yufei Tao  and Zhize Li. On top-k selection in multi-armed bandits and hidden
bipartite graphs. In Advances in Neural Information Processing Systems  pages 1036–1044 
2015.

[17] Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sébastien Bubeck. lil’ucb: An optimal
exploration algorithm for multi-armed bandits. In Conference on Learning Theory  pages
423–439  2014.

[18] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed

bandit problem. Journal of Machine Learning Research  5(Jun):623–648  2004.

[19] Pooria Joulani  Andras Gyorgy  and Csaba Szepesvári. Online learning under delayed feedback.

In International Conference on Machine Learning  pages 1453–1461  2013.

[20] Thomas Desautels  Andreas Krause  and Joel W Burdick. Parallelizing exploration-exploitation
tradeoffs in gaussian process bandit optimization. The Journal of Machine Learning Research 
15(1):3873–3923  2014.

[21] Yifan Wu  Andras Gyorgy  and Csaba Szepesvari. On identifying good options under combi-
natorially structured feedback in ﬁnite noisy environments. In International Conference on
Machine Learning  pages 1283–1291  2015.

[22] Kwang-Sung Jun  Kevin Jamieson  Robert Nowak  and Xiaojin Zhu. Top arm identiﬁcation
in multi-armed bandits with batch arm pulls. In Artiﬁcial Intelligence and Statistics  pages
139–148  2016.

[23] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of

the American statistical association  58(301):13–30  1963.

[24] Lijie Chen and Jian Li. On the optimal sample complexity for best arm identiﬁcation. CoRR 

abs/1511.03774  2015.

10

,Tianyuan Jin
Jieming SHI
Xiaokui Xiao
Enhong Chen