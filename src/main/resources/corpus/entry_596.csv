2018,Are ResNets Provably Better than Linear Predictors?,A residual network (or ResNet) is a standard deep neural net architecture  with state-of-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer's output and the target output. Thus  we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However  due to the non-convexity of the optimization problem  it is not at all clear that ResNets indeed achieve this behavior  rather than getting stuck at some arbitrarily poor local minimum. In this paper  we rigorously prove that arbitrarily deep  nonlinear residual units indeed exhibit this behavior  in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably  we show this under minimal or no assumptions on the precise network architecture  data distribution  or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally  we show that with a certain tweak to the architecture  training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor.,Are ResNets Provably Better than Linear Predictors?

Department of Computer Science and Applied Mathematics

Ohad Shamir

Weizmann Institute of Science

Rehovot  Israel

ohad.shamir@weizmann.ac.il

Abstract

A residual network (or ResNet) is a standard deep neural net architecture  with state-
of-the-art performance across numerous applications. The main premise of ResNets
is that they allow the training of each layer to focus on ﬁtting just the residual of
the previous layer’s output and the target output. Thus  we should expect that the
trained network is no worse than what we can obtain if we remove the residual
layers and train a shallower network instead. However  due to the non-convexity
of the optimization problem  it is not at all clear that ResNets indeed achieve this
behavior  rather than getting stuck at some arbitrarily poor local minimum. In this
paper  we rigorously prove that arbitrarily deep  nonlinear residual units indeed
exhibit this behavior  in the sense that the optimization landscape contains no local
minima with value above what can be obtained with a linear predictor (namely
a 1-layer network). Notably  we show this under minimal or no assumptions on
the precise network architecture  data distribution  or loss function used. We also
provide a quantitative analysis of approximate stationary points for this problem.
Finally  we show that with a certain tweak to the architecture  training the network
with standard stochastic gradient descent achieves an objective value close or better
than any linear predictor.

1

Introduction

Residual networks (or ResNets) are a popular class of artiﬁcial neural networks  providing state-of-
the-art performance across numerous applications [He et al.  2016a b  Kim et al.  2016  Xie et al. 
2017  Xiong et al.  2017]. Unlike vanilla feedforward neural networks  ResNets are characterized by
skip connections  in which the output of one layer is directly added to the output of some following
layer. Mathematically  whereas feedforward neural networks can be expressed as stacking layers of
the form

y = gΦ(x)  

(where (x  y) is the input-output pair and Φ are the tunable parameters of the function gΦ)  ResNets
are built from “residual units” of the form y = f (h(x) + gΦ(x))  where f  h are ﬁxed functions. In
fact  it is common to let f  h be the identity [He et al.  2016b]  in which case each unit takes the form
(1)
Intuitively  this means that in each layer  the training of fΦ can focus on ﬁtting just the “residual”
of the target y given x  rather than y itself. In particular  adding more depth should not harm
performance  since we can effectively eliminate layers by tuning Φ such that gΦ is the zero function.
Due to this property  residual networks have proven to be very effective in training extremely deep
networks  with hundreds of layers or more.
Despite their widespread empirical success  our rigorous theoretical understanding of training residual
networks is very limited. Most recent theoretical works on optimization in deep learning (e.g.

y = x + gΦ(x) .

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Soltanolkotabi et al. [2017]  Yun et al. [2018]  Soudry and Hoffer [2017]  Brutzkus et al. [2017] 
Ge et al. [2017]  Safran and Shamir [2017]  Du and Lee [2018] to name just a few examples)
have focused on simpler  feedforward architectures  which do not capture the properties of residual
networks. Some recent results do consider residual-like elements (see discussion of related work
below)  but generally do not apply to standard architectures. In particular  we are not aware of any
theoretical justiﬁcation for the basic premise of ResNets: Namely  that their architecture allows
adding layers without harming performance. The problem is that training neural networks involves
solving a highly non-convex problem using local search procedures. Thus  even though deeper
residual networks can express shallower ones  it is not at all clear that the training process will indeed
converge to such a network (or a better one). Perhaps  when we attempt to train the residual network
using gradient-based methods  we might hit some poor local minimum  with a worse error than what
can be obtained with a shallower network? This question is the main motivation to our work.
A secondary motivation are several recent results (e.g. Yun et al. [2018]  Safran and Shamir [2017] 
Du et al. [2017]  Liang et al. [2018]) which demonstrate how spurious local minima (with value
larger than the global minima) do exist in general when training neural networks  even under fairly
strong assumptions. Thus  instead of aiming for a result demonstrating that no such minima exist 
which might be too good to be true on realistic networks  we can perhaps consider a more modest
goal  showing that no such minima exist above a certain (non-trivial) level set. This level set can
correspond  for instance  to the optimal value attainable by shallower networks  without the additional
residual layers.
In this paper  we study these questions by considering the competitiveness of a simple residual network
(composed of an arbitrarily deep  nonlinear residual unit and a linear output layer) with respect to
linear predictors (or equivalently  1-layer networks). Speciﬁcally  we consider the optimization
problem associated with training such a residual network  which is in general non-convex and can
have a complicated structure. Nevertheless  we prove that the optimization landscape has no local
minima with a value higher than what can be achieved with a linear predictor on the same data. In
other words  if we run a local search procedure and reach a local minimum  we are assured that the
solution is no worse than the best obtainable with a linear predictor. Importantly  we show this under
fairly minimal assumptions on the residual unit  no assumptions on the data distribution (such as
linear separability)  and no assumption on the loss function used besides smoothness and convexity
in the network’s output (which is satisﬁed for losses used in practice). In addition  we provide a
quantitative analysis  which shows how every point which is -close to being stationary in certain
directions (see Sec. 2 for a precise deﬁnition) can’t be more than poly() worse than any ﬁxed linear
predictor.
The results above are geometric in nature. As we explain later on  they do not necessarily imply that
standard gradient-based methods will indeed converge to such desirable solutions (for example  since
the iterates might diverge). Nevertheless  we also provide an algorithmic result  showing that if the
residual architecture is changed a bit  then a standard stochastic gradient descent (SGD) procedure
will result in a predictor similar or better than the best linear predictor. This result relies on a simple 
but perhaps unexpected reduction to the setting of online learning  and might be of independent
interest.
The supplementary material to this paper contains most proofs (Appendix A) and a discussion of how
some of our results can be generalized to vector-valued outputs (Appendix B).

Related Work

As far as we know  existing rigorous theoretical results on residual networks all pertain to linear
networks  which combine linear residual units of the form

y = x + W x = (I + W )x .

Although such networks are not used in practice  they capture important aspects of the non-convexity
associated with training residual networks. In particular  Hardt and Ma [2016] showed that linear
residual networks with the squared loss have no spurious local minima (namely  every local minimum
is also a global one). More recently  Bartlett et al. [2018] proved convergence results for gradient
descent on such problems  assuming the inputs are isotropic and the target linear mapping is symmetric
and positive deﬁnite. Showing similar results for non-linear networks is mentioned in Hardt and Ma
[2016] as a major open problem. In our paper  we focus on non-linear residual units  but consider
only local minima above some level set.

2

In terms of the setting  perhaps the work closest to ours is Liang et al. [2018]  which considers
networks which can be written as x (cid:55)→ fS(x) + fD(x)  where fS is a one-hidden-layer network  and
fD is an arbitrary  possibly deeper network. Under technical assumptions on the data distribution 
activations used  network size  and assuming certain classiﬁcation losses  the authors prove that
the training objective is benign  in the sense that the network corresponding to any local minimum
has zero classiﬁcation error. However  as the authors point out  their architecture is different than
standard ResNets (which would require a ﬁnal tunable layer to combine the outputs of fS  fD)  and
their results provably do not hold under such an architecture. Moreover  the technical assumptions
are non-trivial  do not apply as-is to standard activations and losses (such as the ReLU activation and
the logistic loss)  and require speciﬁc conditions on the data  such as linear separability or a certain
low-rank structure. In contrast  we study a more standard residual unit  and make minimal or no
assumptions on the network  data distribution  and loss used. On the ﬂip side  we only prove results
for local minima above a certain level set  rather than all such points.
Finally  the idea of studying stationary points in non-convex optimization problems  which are above
or below some reference level set  has also been explored in some other works (e.g. Ge and Ma
[2017])  but under settings quite different than ours.

2 Setting and Preliminaries

We start with a few words about basic notation and terminology. We generally use bold-faced letters
to denote vectors (assumed to be in column form)  and capital letters to denote matrices or functions.
(cid:107)·(cid:107) refers to the Euclidean norm for vectors and spectral norm for matrices  unless speciﬁed otherwise.
(cid:107) · (cid:107)F r for matrices denotes the Frobenius norm (which always upper bounds the spectral norm).
For a matrix M  vec(M ) refers to the entries of M written as one long vector (according to some
canonical order). Given a function g on Euclidean space  ∇g denotes its gradient and ∇2g denotes
its Hessian. A point x in the domain of a function g is a local minimum  if g(x) ≤ g(x(cid:48)) for any x(cid:48)
in some open neighborhood of x. Finally  we use standard O(·) and Θ(·) notation to hide constants 
and let poly(x1  . . .   xr) referto an expression which is polynomial in x1  . . .   xr.
We consider a residual network architecture  consisting of a residual unit as in Eq. (1)  composed
with a linear output layer  with scalar output1:

x (cid:55)→ w(cid:62) (x + gΦ(x)) .

We will make no assumptions on the structure of each gΦ  nor on the overall depth of the network
which computes it  except that it’s last layer is a tunable linear transformation (namely  that gΦ(x) =
V fθ(x) for some matrix V   not necessarily a square one  and parameters θ). This condition follows
the “full pre-activation” structure proposed in He et al. [2016b]  which was empirically found to be
the best-performing residual unit architecture  and is commonly used in practice (e.g. in TensorFlow).
We depart from that structure only in that V is fully tunable rather than a convolution  to facilitate
and simplify our theoretical study. Under this assumption  we have that given x  the network outputs

x (cid:55)→ w(cid:62) (x + V fθ(x))  

parameterized by a vector w  a matrix V   and with some (possibly complicated) function fθ parame-
terized by θ.
Remark 1 (Biases). We note that this model can easily incorporate biases  namely predictors of the
form x (cid:55)→ w(cid:62) (x + V fθ(x) + a) + a for some tunable a  a  by the standard trick of augmenting x
with an additional coordinate whose value is always 1  and assuming that fθ(x) outputs a vector
with an additional coordinate of value 1. Since our results do not depend on the data geometry or
speciﬁcs of fθ  they would not be affected by such modiﬁcations.

We assume that our network is trained with respect to some data distribution (e.g. an average over
some training set {xi  yi})  using a loss function (cid:96)(p  y)  where p is the network’s prediction and y is
the target value. Thus  we consider the optimization problem

(2)

F (w  V  θ) := Ex y

min
w V θ

(cid:2)(cid:96)(w(cid:62)(x + V fθ(x)); y)(cid:3)  

1See Appendix B for a discussion of how some of our results can be generalized to networks with vector-

valued outputs.

3

where w  V  θ are unconstrained. This objective will be the main focus of our paper. In general  this
objective is not convex in (w  V  θ)  and can easily have spurious local minima and saddle points.
In our results  we will make no explicit assumptions on the distribution of (x  y)  nor on the structure
of fθ. As to the loss  we will assume throughout the paper the following:
Assumption 1. For any y  the loss (cid:96)(p  y) is twice differentiable and convex in p.

This assumption is mild  and is satisﬁed for standard losses such as the logistic loss  squared loss 
smoothed hinge loss etc. Note that under this assumption  F (w  V  θ) is twice-differentiable with
respect to w  V   and in particular the function deﬁned as

Fθ(w  V ) := F (w  V  θ)

(for any ﬁxed θ) is twice-differentiable. We emphasize that throughout the paper  we will not
assume that F is necessarily differentiable with respect to θ (indeed  if fθ represents a network with
non-differentiable operators such as ReLU or the max function  we cannot expect that F will be
differentiable everywhere). When considering derivatives of Fθ  we think of the input as one long
vector in Euclidean space (in order speciﬁed by vec())  so ∇Fθ is a vector and ∇2Fθ is a matrix.
As discussed in the introduction  we wish to compare our objective value to that obtained by linear
predictors. Speciﬁcally  we will use the notation

Flin(w) := F (w  0  θ) = Ex y

(cid:2)(cid:96)(w(cid:62)x; y)(cid:3)

to denote the expected loss of a linear predictor parameterized by the vector w. By Assumption 1 
this function is convex and twice-differentiable.
Finally  we introduce the following class of points  which behave approximately like local minima of
F with respect to (w  V )  in terms of its ﬁrst two derivatives:
Deﬁnition 1 (-SOPSP). Let M be an open subset of the domain of F (w  V  θ)  on which
∇2Fθ(w  V ) is µ2-Lipschitz in (w  V ). Then (w  V  θ) ∈ M is an -second-order partial stationary
point (-SOPSP) of F on M  if

(cid:107)∇Fθ(w  V )(cid:107) ≤  and λmin(∇2Fθ(w  V )) ≥ −√

µ2 .

Importantly  note that any local minimum (w  V  θ) of F must be a 0-SOPSP: This is because
(w  V ) is a local minimum of the (differentiable) function Fθ  hence (cid:107)∇Fθ(w  V )(cid:107) = 0 and
λmin(∇2Fθ(w  V )) ≥ 0. Our deﬁnition above directly generalizes the well-known notion of -
second-order stationary points (or -SOSP) [McCormick  1977  Nesterov and Polyak  2006  Jin
et al.  2017]  which are deﬁned for functions which are twice-differentiable in all of their parameters.
In fact  our deﬁnition of -SOPSP is equivalent to requiring that (w  V ) is an -SOSP of Fθ. We
need to use this more general deﬁnition  because we are not assuming that F is differentiable in θ.
Interestingly  -SOSP is one of the most general classes of points in non-convex optimization  to
which gradient-based methods can be shown to converge in poly(1/) iterations.

3 Competitiveness with Linear Predictors

Our main results are Thm. 3 and Corollary 1 below  which are proven in two stages: First  we show
that at any point such that w (cid:54)= 0  (cid:107)∇Fθ(w  V )(cid:107) is lower bounded in terms of the suboptimality with
respect to the best linear predictor (Thm. 1). We then consider the case w = 0  and show that for such
points  if they are suboptimal with respect to the best linear predictor  then either (cid:107)∇Fθ(w  V )(cid:107) is
strictly positive  or λmin(∇2Fθ(w  V )) is strictly negative (Thm. 2). Thus  building on the deﬁnition
of -SOPSP from the previous section  we can show that no point which is suboptimal (compared to
a linear predictor) can be a local minimum of F .
Theorem 1. At any point (w  V  θ) such that w (cid:54)= 0  and for any vector w∗ of the same dimension
as w 

(cid:107)∇Fθ(w  V )(cid:107) ≥

(cid:114)

F (w  V  θ) − Flin(w∗)
2(cid:107)w(cid:107)2 + (cid:107)w∗(cid:107)2

2 +

(cid:107)V (cid:107)2
(cid:107)w(cid:107)2

(cid:16)

(cid:17) .

4

The theorem implies that for any point (w  V  θ) for which the objective value F (w  V  θ) is larger
than that of some linear predictor Flin(w∗)  and unless w = 0  its partial derivative with respect to
(w  V ) (namely ∇Fθ(w  V )) is non-zero  so it cannot be a stationary point with respect to w  V   nor
a local minimum of F .
The proof of the theorem appears in the supplementary material  but relies on the following key
lemma  which we shall state and roughly sketch its proof here:
Lemma 1. Fix some w  V (where w (cid:54)= 0) and a vector w∗ of the same size as w. Deﬁne the matrix

G =

w − w∗ ;

1

(cid:107)w(cid:107)2 w(w∗)(cid:62)V

.

Then

(cid:104)vec(G) ∇Fθ(w  V )(cid:105) ≥ F (w  V  θ) − Flin(w∗) .

In other words  the inner product of the gradient with some carefully-chosen vector is lower bounded
by the suboptimality of F (w  V  θ) compared to a linear predictor (and in particular  if the point is
suboptimal  the gradient cannot be zero).

(cid:18)

(cid:28)
(cid:28)

+

vec

(cid:19)

(cid:29)
(cid:19)

Proof Sketch of Lemma 1. We have

(cid:104)vec(G) ∇Fθ(w  V )(cid:105) =

w − w∗  

F (w  V  θ)

∂
∂w

(cid:18) 1
(cid:107)w(cid:107)2 w(w∗)(cid:62)V

(cid:18) ∂

∂V

(cid:19)(cid:29)

F (w  V  θ)

.

  vec

Let d(cid:96) = ∂
above equals

∂p (cid:96)(p; y)|p=w(cid:62)(x+V fθ(x)). A careful technical calculation reveals that the expression

(cid:2)d(cid:96) (w∗)(cid:62)V fθ(x)(cid:3) + Ex y

(cid:2)d(cid:96)(w − w∗)(cid:62)(x + V fθ(x))(cid:3) .

Ex y

This in turn equals

(cid:2)d(cid:96)

(cid:0)w(cid:62)(x + V fθ(x)) − (w∗)(cid:62)x(cid:1)(cid:3) .

Ex y

Recalling the deﬁnition of d(cid:96)  and noting that by convexity of (cid:96)  ∂
for all p  ˜p  it follows that the above is lower bounded by

(cid:2)(cid:96)(w(cid:62)(x + V fθ(x); y)) − (cid:96)((w∗)(cid:62)x; y)(cid:3) = F (w  V  θ) − Flin(w∗) .

Ex y

∂p (cid:96)(p; y)(p − ˜p) ≥ (cid:96)(p; y) − (cid:96)(˜p; y)

To analyze the case w = 0  we have the following result:
Theorem 2. For any V  θ  w∗ 

(cid:0)∇2Fθ(0  V )(cid:1) ≤ 0
(cid:13)(cid:13)(cid:13)(cid:13) ∂2

∂w2 Fθ(0  V )

λmin

(cid:115)

|λmin (∇2Fθ(0  V ))| ·
≥ F (0  V  θ) − Flin(w∗)

 

(cid:107)w∗(cid:107)

(cid:13)(cid:13)(cid:13)(cid:13) + λmin (∇2Fθ(0  V ))2

and

(cid:107)∇Fθ(0  V )(cid:107) + (cid:107)V (cid:107)

where λmin(M ) denotes the minimal eigenvalue of a symmetric matrix M.

Combining the two theorems above  we can show the following main result:
Theorem 3. Fix some positive b  r  µ0  µ1  µ2 and  ≥ 0  and suppose M is some convex open subset
of the domain of F (w  V  θ) in which

• max{(cid:107)w(cid:107) (cid:107)V (cid:107)} ≤ b
• Fθ(w  V )  ∇Fθ(w  V ) and ∇2Fθ(w  V ) are µ0-Lipschitz  µ1-Lipschitz  and µ2-Lipschitz

in (w  V ) respectively.

5

• For any (w  V  θ) ∈ W  we have (0  V  θ) ∈ W and (cid:107)∇2Fθ(0  V )(cid:107) ≤ µ1.

Then for any (w  V  θ) ∈ M which is an -SOPSP of F on M 

F (w  V  θ) ≤ min
w:(cid:107)w(cid:107)≤r

√
Flin(w) + ( + 4

) · poly(b  r  µ0  µ1  µ2).

We note that the poly(b  r  µ0  µ1  µ2) term hides only dependencies which are at most linear in the
individual factors (see the proof in the supplementary material for the exact expression).
As discussed in Sec. 2  any local minima of F must correspond to a 0-SOPSP. Hence  the theorem
above implies that for such a point  F (w  V  θ) ≤ minw:(cid:107)w(cid:107)≤r Flin(w) (as long as F satisﬁes the
Lipschitz continuity assumptions for some ﬁnite µ0  µ1  µ2 on any bounded subset of the domain).
Since this holds for any r  we have arrived at the following corollary:
Corollary 1. Suppose that on any bounded subset of
it holds that
Fθ(w  V ) ∇Fθ(w  V ) and ∇2Fθ(w  V ) are all Lipschitz continuous in (w  V ). Then every lo-
cal minimum (w  V  θ) of F satisﬁes

the domain of F  

F (w  V  θ) ≤ inf

w

Flin(w) .

In other words  the objective F has no spurious local minima with value above the smallest attainable
with a linear predictor.
Remark 2 (Generalization to vector-valued outputs). One can consider a generalization of our
setting to networks with vector-valued outputs  namely x (cid:55)→ W (x + V fθ(x))  where W is a matrix 
and with losses (cid:96)(p  y) taking vector-valued arguments and convex in p (e.g. the cross-entropy loss).
In this more general setting  it is possible to prove a variant of Thm. 1 using a similar proof technique
(see Appendix B). However  it is not clear to us how to prove an analog of Thm. 2 and hence Thm. 3.
We leave this as a question for future research.

4 Effects of Norm and Regularization

Thm. 3 implies that any -SOPSP must have a value not much worse than that obtained by a linear
predictor. Moreover  as discussed in Sec. 2  such points are closely related to second-order stationary
points  and gradient-based methods are known to converge quickly to such points (e.g. Jin et al.
[2017]). Thus  it is tempting to claim that such methods will indeed result in a network competitive
with linear predictors. Unfortunately  there is a fundamental catch: The bound of Thm. 3 depends on
the norm of the point (via (cid:107)w(cid:107) (cid:107)V (cid:107))  and can be arbitrarily bad if the norm is sufﬁciently large. In
other words  Thm. 3 guarantees that a point which is -SOPSP is only “good” as long as it is not too
far away from the origin.
If the dynamics of the gradient method are such that the iterates remain in some bounded domain (or
at least have a sufﬁciently slowly increasing norm)  then this would not be an issue. However  we are
not a-priori guaranteed that this would be the case: Since the optimization problem is unconstrained 
and we are not assuming anything on the structure of fθ  it could be that the parameters w  V diverge 
and no meaningful algorithmic result can be derived from Thm. 3.
Of course  one option is that this dependence on (cid:107)w(cid:107) (cid:107)V (cid:107) is an artifact of the analysis  and any
-SOPSP of F is competitive with a linear predictor  regardless of the norms. However  the following
example shows that this is not the case:
Example 1. Fix some  > 0. Suppose x  w  V  w∗ are all scalars  w∗ = 1  fθ(x) = x (with no
2 (p − y)2 is the squared loss  and x = y = 1 w.p. 1. Then
dependence on a parameter θ)  (cid:96)(p; y) = 1
the objective can be equivalently written as

(see leftmost plot in Figure 1). The gradient and Hessian of F (w  v) equal

(cid:18)(w − 1 + wv)(1 + v)
(cid:19)

(w − 1 + wv)w

1
2

(cid:18)

6

F (w  v) =

(w(1 + v) − 1)2

and

(1 + v)2

(2w + 2wv − 1)

(2w + 2wv − 1)

2w2

(cid:19)

Figure 1: From left to right: Contour plots of (a) F (w  v) = (w(1+v)−1)2  (b) F (w  v)+ 1
4 (w2+v2) 
and (c) F (w  v) superimposed with the constraint (cid:107)(w  v)(cid:107) ≤ 2 (inside the circle). The x-axis
corresponds to w  and the y-axis corresponds to v. Both (b) and (c) exhibit a spurious local minima
in the bottom left quadrant of the domain. Best viewed in color.

In particular  at (w  v) = (0 −1/)  the gradient is 0 and the Hessian equals
  which is arbitrarily close to 0 if  is small enough. However  the objective value

(cid:19)

(cid:18) 0 −

respectively.
−
at that point equals

0

(cid:18)

(cid:19)

F

0 − 1


=

1
2

> 0 = Flin(1).

Remark 3. In the example above  F does not have gradients and Hessians with a uniformly bounded
Lipschitz constant (over all of Euclidean space). However  for any  > 0  the Lipschitz constants are
bounded by a numerical constant over (w  v) ∈ [−2/  2/]2 (which includes the stationary point
studied in the construction). This indicates that the problem indeed lies with the norm of (w  v) being
unbounded  and not with the Lipschitz constants of the derivatives of F .

One standard approach to ensure that the iterates remain bounded is to add regularization  namely
optimize

min
w V θ

F (w  V  θ) + R(w  V  θ)  

where R is a regularization term penalizing large norms of w  V  θ. Unfortunately  not only does
this alter the objective  it might also introduce new spurious local minima that did not exist in
F (w  V  θ). This is graphically illustrated in Figure 1  which plots F (w  v) from Example 1 (when
 = 1)  with and without regularization of the form R(w  v) = λ
2 (w2 + v2) where λ = 1/2. Whereas
the stationary points of F (w  v) are either global minima (along two valleys  corresponding to
{(w  v) : w(1 + v) = 1}) or a saddle point (at (w  v) = (1 −1/))  the regularization created a new
spurious local minimum around (w  v) ≈ (−1 −1.6). Intuitively  this is because the regularization
makes the objective value increase well before the valley of global minima of F . Other regularization
choices can also lead to the same phenomenon. A similar issue can also occur if we impose a hard
constraint  namely optimize

w V θ:(w V θ)∈M F (w  V  θ)

min

for some constrained domain M. Again  as Figure 1 illustrates  this optimization problem can have
spurious local minima inside its constrained domain  using the same F as before.
Of course  one way to ﬁx this issue is by making the regularization parameter λ sufﬁciently small (or
the domain M sufﬁciently large)  so that the regularization only comes into effect when (cid:107)(w  v)(cid:107)
is sufﬁciently large. However  the correct choice of λ and M depends on   and here we run into a
problem: If fθ is not simply some ﬁxed  (as in the example above)  but changes over time  then
we have no a-priori guarantee on how λ or M should be chosen. Thus  it is not clear that any ﬁxed
choice of regularization would work  and lead a gradient-based method to a good local minimum.

5 Success of SGD Assuming a Skip Connection to the Output

Having discussed the challenges of getting an algorithmic result in the previous section  we now show
how such a result is possible  assuming the architecture of our network is changed a bit.

7

Concretely  instead of the network architecture x (cid:55)→ w(cid:62)(x + V fθ(x))  we consider the architecture

parameterized by vectors w  v and θ  so our new objective can be written as

x (cid:55)→ w(cid:62)x + v(cid:62)fθ(x) 

(cid:2)(cid:96)(cid:0)w(cid:62)x + v(cid:62)fθ(x); y(cid:1)(cid:3) .

F (w  v  θ) = Ex y

This architecture corresponds to having a skip connection directly to the network’s output  rather than
to a ﬁnal linear output layer. It is similar in spirit to the skip-connection studied in Liang et al. [2018] 
except that they had a two-layer nonlinear network instead of our linear w(cid:62)x component.
In what follows  we consider a standard stochastic gradient descent (SGD) algorithm to train our
network: Fixing a step size η and some convex parameter domain M  we

1. Initialize (w1  v1  θ1) at some point in M
2. For t = 1  2  . . .   T   we randomly sample a data point (xt  yt) from the underlying data

distribution  and perform

(wt+1  vt+1  θt+1) = ΠM ((wt  vt  θt) − η∇ht(wt  vt  θt))  

where

and ΠM denote an Euclidean projection on the set M.

ht(w  v  θ) := (cid:96)(w(cid:62)xt + v(cid:62)fθ(xt); yt)

Note that ht(w  v  θ) is always differentiable with respect to w  v  and in the above  we assume for
simplicity that it is also differentiable with respect to θ (if not  one can simply deﬁne ∇ht(w  v  θ)

above to be(cid:0) ∂

∂w ht(w  v  θ)  ∂

∂v ht(w  v  θ)  rt w v θ

result below can still be easily veriﬁed to hold).
As before  we use the notation

Flin(w) = Ex y

(cid:1) for some arbitrary vector rt w v θ  and the
(cid:2)(cid:96)(cid:0)w(cid:62)x; y(cid:1)(cid:3)

to denote the expected loss of a linear predictor parameterized by w. The following theorem
establishes that under mild conditions  running stochastic gradient descent with sufﬁciently many
iterations results in a network competitive with any ﬁxed linear predictor:
Theorem 4. Suppose the domain M satisﬁes the following for some positive constants b  r  l:

• M = {(w  v  θ) : (w  v) ∈ M1  θ ∈ M2} for some closed convex sets M1 M2 in Eu-

clidean spaces (namely  M is a Cartesian product of M1 M2).

• For any (x  y) in the support of the data distribution  and any θ ∈ M2  (cid:96)(w(cid:62)x +

v(cid:62)fθ(x); y) is l-Lipschitz in (w  v) over M1  and bounded in absolute value by r.

• For any (w  v) ∈ M1 (cid:112)(cid:107)w(cid:107)2 + (cid:107)v(cid:107)2 ≤ b.

Suppose we perform T iterations of stochastic gradient descent as described above  with any step size
t=1 satisﬁes
η = Θ(b/(l

T )). Then with probability at least 1 − δ  one of the iterates {(wt  vt  θt)}T

√

F (wt  vt  θt) ≤

min

u:(u 0)∈M1

Flin(u) + O

√

T

(cid:32)

bl + r(cid:112)log(1/δ)

(cid:33)

.

The proof relies on a technically straightforward – but perhaps unexpected – reduction to adversarial
online learning  and appears in the supplementary material. Roughly speaking  the idea is that
our stochastic gradient descent procedure over (w  v  θ) is equivalent to online gradient descent on
(w  v)  with respect to a sequence of functions deﬁned by the iterates θ1  θ2  . . .. Even though these
iterates can change in unexpected and complicated ways  the strong guarantees of online learning
(which allow the sequence of functions to be rather arbitrary) allow us to obtain the theorem above.
Acknowledgements. We thank the anonymous NIPS 2018 reviewers for their helpful comments.
This research is supported in part by European Research Council (ERC) grant 754705.

8

References
Peter L Bartlett  David P Helmbold  and Philip M Long. Gradient descent with identity initialization
efﬁciently learns positive deﬁnite linear transformations by deep residual networks. arXiv preprint
arXiv:1802.06093  2018.

Alon Brutzkus  Amir Globerson  Eran Malach  and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174  2017.

Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic

activation. arXiv preprint arXiv:1803.01206  2018.

Simon S Du  Jason D Lee  Yuandong Tian  Barnabas Poczos  and Aarti Singh. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779 
2017.

Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In Advances in

Neural Information Processing Systems  pages 3656–3666  2017.

Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with landscape

design. arXiv preprint arXiv:1711.00501  2017.

Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231 

2016.

Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization 

2(3-4):157–325  2016.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016a.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual

networks. In European Conference on Computer Vision  pages 630–645. Springer  2016b.

Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to escape saddle

points efﬁciently. arXiv preprint arXiv:1703.00887  2017.

Jiwon Kim  Jung Kwon Lee  and Kyoung Mu Lee. Accurate image super-resolution using very deep
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 1646–1654  2016.

Shiyu Liang  Ruoyu Sun  Yixuan Li  and R Srikant. Understanding the loss surface of neural networks

for binary classiﬁcation. arXiv preprint arXiv:1803.00909  2018.

Garth P McCormick. A modiﬁcation of armijo’s step-size rule for negative curvature. Mathematical

Programming  13(1):111–115  1977.

Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.

Mathematical Programming  108(1):177–205  2006.

Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.

arXiv preprint arXiv:1712.08968  2017.

Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R(cid:13) in

Machine Learning  4(2):107–194  2012.

Mahdi Soltanolkotabi  Adel Javanmard  and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926  2017.

Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer

neural networks. arXiv preprint arXiv:1702.05777  2017.

9

Saining Xie  Ross Girshick  Piotr Dollár  Zhuowen Tu  and Kaiming He. Aggregated residual
transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR) 
2017 IEEE Conference on  pages 5987–5995. IEEE  2017.

Wayne Xiong  Jasha Droppo  Xuedong Huang  Frank Seide  Mike Seltzer  Andreas Stolcke  Dong
In
Yu  and Geoffrey Zweig. The microsoft 2016 conversational speech recognition system.
Acoustics  Speech and Signal Processing (ICASSP)  2017 IEEE International Conference on  pages
5255–5259. IEEE  2017.

Chulhee Yun  Suvrit Sra  and Ali Jadbabaie. A critical view of global optimality in deep learning.

arXiv preprint arXiv:1802.03487  2018.

Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages 928–936 
2003.

10

,Ohad Shamir