2010,Active Estimation of F-Measures,We address the problem of estimating the F-measure of a given model as accurately as possible on a fixed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance  when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution. In this case  new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of F-measures are more accurate than estimates based on instances sampled from the test distribution.,Active Estimation of F-Measures

Christoph Sawade  Niels Landwehr  and Tobias Scheffer

August-Bebel-Strasse 89  14482 Potsdam  Germany

{sawade  landwehr  scheffer}@cs.uni-potsdam.de

University of Potsdam

Department of Computer Science

Abstract

We address the problem of estimating the Fα-measure of a given model as accu-
rately as possible on a ﬁxed labeling budget. This problem occurs whenever an
estimate cannot be obtained from held-out training data; for instance  when data
that have been used to train the model are held back for reasons of privacy or do
not reﬂect the test distribution. In this case  new test instances have to be drawn
and labeled at a cost. An active estimation procedure selects instances according
to an instrumental sampling distribution. An analysis of the sources of estimation
error leads to an optimal sampling distribution that minimizes estimator variance.
We explore conditions under which active estimates of Fα-measures are more ac-
curate than estimates based on instances sampled from the test distribution.

1

Introduction

This paper addresses the problem of evaluating a given model in terms of its predictive performance.
In practice  it is not always possible to evaluate a model on held-out training data; consider  for
instance  the following scenarios. When a readily trained model is shipped and deployed  training
data may be held back for reasons of privacy. Secondly  training data may have been created under
laboratory conditions and may not entirely reﬂect the test distribution. Finally  when a model has
been trained actively  the labeled data is biased towards small-margin instances which would incur
a pessimistic bias on any cross-validation estimate.
This problem has recently been studied for risks—i.e.  for performance measures which are integrals
of a loss function over an instance space [7]. However  several performance measures cannot be
expressed as a risk. Perhaps the most prominent such measure is the Fα-measure [10]. For a given
binary classiﬁer and sample of size n  let ntp and nf p denote the number of true and false positives 
respectively  and nf n the number of false negatives. Then the classiﬁer’s Fα-measure on the sample
is deﬁned as

Fα =

α(ntp + nf p) + (1 − α)(ntp + nf n)

ntp

.

(1)

Precision and recall are special cases for α = 1 and α = 0  respectively. The Fα-measure is
deﬁned as an estimator in terms of empirical quantities. This is unintuitive from a statistical point of
view and raises the question which quantity of the underlying distribution the F -measure actually
estimates. We will now introduce the class of generalized risk functionals that we study in this paper.
We will then show that Fα is a consistent estimate of a quantity that falls into this class.
Let X denote the feature space and Y the label space. An unknown test distribution p(x  y) is deﬁned
over X × Y. Let p(y|x; θ) be a given θ-parameterized model of p(y|x) and let fθ : X → Y with
fθ(x) = arg maxy p(y|x; θ) be the corresponding hypothesis.
Like any risk functional  the generalized risk is parameterized with a function (cid:96) : Y × Y → R
determining either the loss or—alternatively—the gain that is incurred for a pair of predicted and

1

true label. In addition  the generalized risk is parameterized with a function w that assigns a weight
w(x  y  fθ) to each instance. For instance  precision sums over instances with fθ(x) = 1 with
weight 1 and gives no consideration to other instances. Equation 2 deﬁnes the generalized risk:

(cid:82)(cid:82) (cid:96)(fθ(x)  y)w(x  y  fθ)p(x  y)dydx

(cid:82)(cid:82) w(x  y  fθ)p(x  y)dydx

G =

(2)
The integral over Y is replaced by a sum in the case of a discrete label space Y. Note that the
generalized risk (Equation 2) reduces to the regular risk for w(x  y  fθ) = 1. On a sample of size
n  a consistent estimator can be obtained by replacing the cumulative distribution function with the
empirical distribution function.
Proposition 1. Let (x1  y1)  . . .   (xn  yn) be drawn iid according to p(x  y). The quantity

.

(cid:80)n

(cid:80)n

ˆGn =

i=1 (cid:96)(fθ(xi)  yi)w(xi  yi  fθ)

i=1 w(xi  yi  fθ)

(3)

is a consistent estimate of the generalized risk G deﬁned by Equation 2.

Proof. The proposition follows from Slutsky’s theorem [3] applied to the numerator and denomina-
tor of Equation 3.

Consistency means asymptotical unbiasedness; that is  the expected value of the estimate ˆGn con-
verges in distribution to the true risk G for n → ∞. We now observe that Fα-measures—including
precision and recall—are consistent empirical estimates of generalized risks for appropriately cho-
sen functions w.
Corollary 1. Fα is a consistent estimate of the generalized risk with Y = {0  1}  w(x  y  fθ) =
αfθ(x) + (1 − α)y and (cid:96) = 1 − (cid:96)0/1  where (cid:96)0/1 denotes the zero-one loss.

Proof. The claim follows from Proposition 1 since

(cid:80)n
(cid:80)n
i=1(1 − (cid:96)0/1(fθ(xi)  yi)) (αfθ(xi) + (1 − α)yi)
(cid:80)n
i=1 (αfθ(xi) + (1 − α)yi)
α(cid:80)n
i=1 fθ(xi) + (1 − α)(cid:80)n
i=1 fθ(xi)yi

i=1 yi

=

ˆGn =

=

α (ntp + nf p) + (1 − α) (ntp + nf n)

.

ntp

Having established and motivated the generalized risk functional  we now turn towards the problem
of acquiring a consistent estimate with minimal estimation error on a ﬁxed labeling budget n. Test
instances x1  ...  xn need not necessarily be drawn according to the distribution p. Instead  we study
an active estimation process that selects test instances according to an instrumental distribution q.
When instances are sampled from q  an estimator of the generalized risk can be deﬁned as

p(xi)
q(xi) (cid:96)(fθ(xi)  yi)w(xi  yi  fθ)

i=1

(4)

i=1

p(xi)
q(xi) w(xi  yi  fθ)
where (xi  yi) are drawn from q(x)p(y|x). Weighting factors p(xi)
q(xi) compensate for the discrepancy
between test and instrumental distributions. Because of the weighting factors  Slutsky’s Theorem
again implies that Equation 4 deﬁnes a consistent estimator for G  under the precondition that for all
x ∈ X with p(x) > 0 it holds that q(x) > 0. Note that Equation 3 is a special case of Equation 4 
using the instrumental distribution q = p.
The estimate ˆGn q given by Equation 4 depends on the selected instances (xi  yi)  which are drawn
according to the distribution q(x)p(y|x). Thus  ˆGn q is a random variable whose distribution de-
pends on q. Our overall goal is to determine the instrumental distribution q such that the expected
deviation from the generalized risk is minimal for ﬁxed labeling costs n:

(cid:80)n

(cid:80)n

ˆGn q =

(cid:20)(cid:16) ˆGn q − G

(cid:17)2(cid:21)

.

q∗ = arg min

q

E

2

2 Active Estimation through Variance Minimization

The bias-variance decomposition expresses the estimation error as a sum of a squared bias and a
variance term [5]:

E(cid:104)

( ˆGn q − G)2(cid:105)

(cid:16)E(cid:104) ˆGn q

(cid:105) − G
(cid:17)2

=

+ E

(cid:20)(cid:16) ˆGn q − E(cid:104) ˆGn q

(cid:105)(cid:17)2(cid:21)

(5)

= Bias2[ ˆGn q] + Var[ ˆGn q].

(6)
Because ˆGn q is consistent  both Bias2[ ˆGn q] and Var[ ˆGn q] vanish for n → ∞. More speciﬁcally 
Lemma 1 shows that Bias2[ ˆGn q] is of order 1
n2 .
Lemma 1 (Bias of Estimator). Let ˆGn q be as deﬁned in Equation 4. Then there exists C ≥ 0 with

(cid:12)(cid:12)(cid:12)E(cid:104) ˆGn q

(cid:105) − G

(cid:12)(cid:12)(cid:12) ≤ C

n

.

(7)

The proof can be found in the online appendix. Lemma 2 states that the active risk estimator ˆGn q
is asymptotically normally distributed  and characterizes its variance in the limit.
Lemma 2 (Asymptotic Distribution of Estimator). Let ˆGn q be deﬁned as in Equation 4. Then 

(cid:16) ˆGn q − G

(cid:17) n→∞−→ N(cid:0)0  σ2

(cid:1)

q

√

n

with asymptotic variance

(cid:90) p(x)

(cid:18)(cid:90)

q(x)

σ2
q =

where n→∞−→ denotes convergence in distribution.

w(x  y  fθ)2 ((cid:96)(fθ(x)  y) − G)2 p(y|x)dy

(cid:19)

p(x)dx

(8)

(9)

A proof of Lemma 2 can be found in the appendix. Taking the variance of Equation 8  we obtain

(cid:104) ˆGn q

(cid:105) n→∞−→ σ2

q  

n Var

(10)

thus Var[ ˆGn q] is of order 1
n2   the expected estimation error
E[( ˆGn q − G)2] will be dominated by Var[ ˆGn q]. Moreover  Equation 10 indicates that Var[ ˆGn q]
can be approximately minimized by minimizing σ2
q. In the following  we will consequently derive a
sampling distribution q∗ that minimizes the asymptotic variance σ2

n. As the bias term vanishes with 1

q of the estimator ˆGn q.

2.1 Optimal Sampling Distribution

The following theorem derives the sampling distribution that minimizes the asymptotic variance σ2
q:
Theorem 1 (Optimal Sampling Distribution). The instrumental distribution that minimizes the
asymptotic variance σ2

q of the generalized risk estimator ˆGn q is given by

q∗(x) ∝ p(x)

w(x  y  fθ)2 ((cid:96)(fθ(x)  y) − G)2 p(y|x)dy.

(11)

(cid:115)(cid:90)

A proof of Theorem 1 is given in the appendix. Since F -measures are estimators of generalized
risks according to Corollary 1  we can now derive their variance-minimizing sampling distributions.
Corollary 2 (Optimal Sampling for Fα). The sampling distribution that minimizes the asymptotic
variance of the Fα-estimator resolves to

(cid:26)p(x)(cid:112)p(fθ(x)|x)(1 − G)2 + α2(1 − p(fθ(x)|x))G2
p(x)(1 − α)(cid:112)(1 − p(fθ(x)|x))G2

: f (x) = 1
: f (x) = 0

(12)

q∗(x) ∝

3

Algorithm 1 Active Estimation of Fα-Measures
input Model parameters θ  pool D  labeling costs n.
output Generalized risk estimate ˆGn q∗.
1: Compute optimal sampling distribution q∗ according to Corollary 2  3  or 4  respectively.
2: for i = 1  . . .   n do
3:
4:
5: end for
6: return

Draw xi ∼ q∗(x) from D with replacement.
Query label yi ∼ p(y|xi) from oracle.

q(xi) (cid:96)(fθ(xi) yi)w(xi yi fθ)

(cid:80)n

i=1

(cid:80)n

1

i=1

1

q(xi) w(xi yi fθ)

Proof. According to Corollary 1  Fα estimates a generalized risk with Y = {0  1}  w(x  y  fθ) =
αfθ(x) + (1 − α)y and (cid:96) = 1 − (cid:96)0/1. Starting from Theorem 1  we derive

(αfθ(x) + (1 − α)y)2(cid:0)1 − (cid:96)0/1(fθ(x)  y) − G(cid:1)2

q∗(x) ∝ p(x)

(cid:115) (cid:88)
(cid:16)

y∈{0 1}
α2fθ(x) ((1 − fθ(x)) − G)2 p(y = 0|x)
= p(x)
+ (1 − α(1 − fθ(x)))2 (fθ(x) − G)2 p(y = 1|x)

(cid:17) 1

2

p(y|x)

(13)

(14)

The claim follows by case differentiation according to the value of fθ(x).
Corollary 3 (Optimal Sampling for Recall). The sampling distribution that minimizes σ2
resolves to

q for recall

(cid:26)p(x)(cid:112)p(fθ(x)|x)(1 − G)2
p(x)(cid:112)(1 − p(fθ(x)|x))G2

q∗(x) ∝

: f (x) = 1
: f (x) = 0.

(15)

Corollary 4 (Optimal Sampling for Precision). The sampling distribution that minimizes σ2
precision resolves to

q∗(x) ∝ p(x)fθ(x)(cid:112)(1 − 2G)p(fθ(x)|x) + G2.

q for

(16)

Corollaries 3 and 4 directly follow from Corollary 2 for α = 0 and α = 1. Note that for standard
risks (that is  w = 1) Theorem 1 coincides with the optimal sampling distribution derived in [7].

2.2 Empirical Sampling Distribution

m for all x ∈ D.

Theorem 1 and Corollaries 2  3  and 4 depend on the unknown test distribution p(x). We now turn
towards a setting in which a large pool D of unlabeled test instances is available. Instances from this
pool can be sampled and then labeled at a cost. Drawing instances from the pool replaces generating
them under the test distribution; that is  p(x) = 1
Theorem 1 and its corollaries also depend on the true conditional p(y|x). To implement the method 
we have to approximate the true conditional p(y|x); we use the model p(y|x; θ). This approximation
constitutes an analogy to active learning: In active learning  the model-based output probability
p(y|x; θ) serves as the basis on which the least conﬁdent instances are selected. Note that as long as
p(x) > 0 implies q(x) > 0  the weighting factors ensure that such approximations do not introduce
an asymptotic bias in our estimator (Equation 4). Finally  Theorem 1 and its corollaries depend on
the true generalized risk G. G is replaced by an intrinsic generalized risk calculated from Equation 2 
m  and p(y|x) ≈ p(y|x; θ).
where the integral over X is replaced by a sum over the pool  p(x) = 1
Algorithm 1 summarizes the procedure for active estimation of F -measures. A special case occurs
when the labeling process is deterministic. Since instances are sampled with replacement  elements
may be drawn more than once. In this case  labels can be looked up rather than be queried from the
deterministic labeling oracle repeatedly. The loop may then be continued until the labeling budget
is exhausted. Note that F -measures are undeﬁned when the denominator is zero which is the case
when all drawn examples have a weight w of zero. For instance  precision is undeﬁned when no
positive examples have been drawn.

4

n(cid:88)

(cid:18) p(xi)

(cid:19)2

w(xi  yi  fθ)2(cid:16)

p(xi)
q(xi)

i=1

q(xi)

(cid:17)2

(cid:96)(fθ(xi)  yi) − ˆGn q

.

1(cid:80)n
(cid:1) Sn q√

i=1

S2

n q =

(cid:0)1 − ρ

2.3 Conﬁdence Intervals

Lemma 2 shows that the estimator ˆGn q is asymptotically normally distributed and character-
izes its asymptotic variance. A consistent estimate of σ2
q is obtained from the labeled sample
(x1  y1)  . . .   (xn  yn) drawn from the distribution q(x)p(y|x) by computing empirical variance

A two-sided conﬁdence interval [ ˆGn q − z  ˆGn q + z] with coverage 1 − ρ is now given by
z = F −1
n is the inverse cumulative distribution function of the Student’s t
distribution. As in the standard case of drawing test instances xi from the original distribution p 
such conﬁdence intervals are approximate for ﬁnite n  but become exact for n → ∞.

n where F −1

n

2

3 Empirical Studies

We compare active estimation of Fα-measures according to Algorithm 1 (denoted activeF ) to esti-
mation based on a sample of instances drawn uniformly from the pool (denoted passive). We also
consider the active estimator for risks presented in [7]. Instances are drawn according to the opti-
mal sampling distribution q∗
0/1 for zero-one risk (Derivation 1 in [7]); the Fα-measure is computed
according to Equation 4 using q = q∗

0/1 (denoted activeerr).

3.1 Experimental Setting and Domains

For each experimental domain  data is split into a training set and a pool of test instances. We
train a kernelized regularized logistic regression model p(y|x; θ) (using the implementation of Ya-
mada [11]). All methods operate on identical labeling budgets n. The evaluation process is averaged
over 1 000 repetitions. In case one of the repetitions results in an undeﬁned estimate  the entire ex-
periment is discarded (i.e.  there is no data point for the method in the corresponding diagram).
Spam ﬁltering domain. Spammers impose a shift on the distribution over time as they implement
new templates and generators. In our experiments  a ﬁlter trained in the past has to be evaluated
with respect to a present distribution of emails. We collect 169 612 emails from an email service
provider between June 2007 and April 2010; of these  42 165 emails received by February 2008 are
used for training. Emails are represented by 541 713 binary bag-of-word features. Approximately
5% of all emails fall into the positive class non-spam.
Text classiﬁcation domain. The Reuters-21578 text classiﬁcation task [4] allows us to study the
effect of class skew  and serves as a prototypical domain for active learning. We experiment on the
ten most frequently occurring topics. We employ an active learner that always queries the example
with minimal functional margin p(fθ(x)|x; θ) − maxy(cid:54)=fθ(x) p(y|x; θ) [9]. The learning process is
initialized with one labeled training instance from each class  another 200 class labels are queried.
Digit recognition domain. We also study a digit recognition domain in which training and test data
originate from different sources. A detailed description is included in the online appendix.

3.2 Empirical Results

We study the performance of active and passive estimates as a function of (a) the precision-recall
trade-off parameter α  (b) the discrepancy between training and test distribution  and (c) class skew
in the test distribution. Point (b) is of interest because active estimates require the approximation
p(y|x) ≈ p(y|x; θ); this assumption is violated when training and test distributions differ.
Effect of the trade-off parameter α. For the spam ﬁltering domain  Figure 1 shows the average
absolute estimation error for F0 (recall)  F0.5  and F1 (precision) estimates on a test set of 33 296
emails received between February 2008 and October 2008. The active generalized risk estimate
activeF signiﬁcantly outperforms the passive estimate passive for all three measures.
In order
to reach the estimation accuracy of passive with a labeling budget of n = 800  activeF requires
fewer than 150 (recall)  200 (F0.5)  or 100 (precision) labeled test instances. Estimates obtained from

5

Figure 1: Spam ﬁltering: Estimation error over labeling costs. Error bars indicate the standard error.

Figure 2: Spam ﬁltering: Optimal sampling distribution q∗ for Fα over log-odds (left). Ratio of
passive and active estimation error  error bars indicate standard deviation (center). Estimation error
over class ratio  logarithmic scale  error bars indicate standard errors (right).

activeF are at least as accurate as those of activeerr  and more accurate for high α values. Results
obtained in the digit recognition domain are consistent with these ﬁndings (see online appendix).
Figure 2 (left) shows the sampling distribution q∗(x) for recall  precision and F0.5-measure in the
spam ﬁltering domain as a function of the classiﬁer’s conﬁdence  characterized by the log-odds ratio
log p(y=1|x;θ)
p(y=0|x;θ). The ﬁgure also shows the optimal sampling distribution for zero-one risk as used
in activeerr (denoted “0/1-Risk”). We observe that the precision estimator dismisses all examples
with fθ(x) = 0; this is intuitive because precision is a function of true-positive and false-positive
examples only. By contrast  the recall estimator selects examples on both sides of the decision
boundary  as it has to estimate both the true positive and the false negative rate. The optimal sampling
distribution for zero-one risk is symmetric  it prefers instances close to the decision boundary.
Effect of discrepancy between training and test distribution. We keep the training set of emails
ﬁxed and move the time interval from which test instances are drawn increasingly further away
into the future  thereby creating a growing gap between training and test distribution. Speciﬁcally 
we divide 127 447 emails received between February 2008 and April 2010 into ten different test
sets spanning approximately 2.5 months each. Figure 2 (center  red curve) shows the discrepancy
between training and test distribution measured in terms of the exponentiated average log-likelihood
of the test labels given the model parameters θ. The likelihood at ﬁrst continually decreases. It grows
again for the two most recent batches; this coincides with a recent wave of text-based vintage spam.
| ˆGn−G|
Figure 2 (center  blue curve) also shows the ratio of passive-to-active estimation errors
| ˆGn q∗−G|. A
value above one indicates that the active estimate is more accurate than a passive estimate. The active
estimate consistently outperforms the passive estimate; its advantage diminishes when training and
test distributions diverge and the assumption of p(y|x) ≈ p(y|x; θ) becomes less accurate.
Effect of class skew. In the spam ﬁltering domain we artiﬁcially sub-sampled data to different ratios
of spam and non-spam emails. Figure 2 (right) shows the performance of activeF   passive  and
activeerr for F0.5 estimation as a function of class skew. We observe that activeF outperforms
passive consistently. Furthermore  activeF outperforms activeerr for imbalanced classes  while
the approaches perform comparably when classes are balanced. This ﬁnding is consistent with the
intuition that accuracy and F -measure diverge more strongly for imbalanced classes.

6

02004006008000.010.0150.020.0250.030.0350.04Recalllabeling costs nestimation error (absolute)  passiveactiveFactiveerr02004006008000.050.10.150.20.25F0.5−measurelabeling costs nestimation error (absolute)  passiveactiveFactiveerr02004006008000.050.10.150.20.250.30.35Precisionlabeling costs nestimation error (absolute)  passiveactiveFactiveerr−10−505100510152025Optimal Sampling Distribution (class ratio: 5/95)log(p(y=1|x)/p(y=0|x))m q*(x)  RecallF0.5Precision0/1−Risk05/200801/200908/200904/20101.522.53dateratio of estimation errors  likelihoodTime Shift vs. Ratio of Estimation Errors0.60.70.80.91ratiolikelihood0.030.10.320.010.1F0.5−measure (n=500)positive class fractionestimation error (absolute)passiveactiveFactiveerrFigure 3: Text classiﬁcation: Estimation error over number of labeled data for infrequent (left) and
frequent (center) class. Estimation error over class ratio for all ten classes  logarithmic scale (right).
Error bars indicate the standard error.

In the text classiﬁcation domain we estimate the F0.5-measure for ten one-versus-rest classiﬁers.
Figure 3 shows the estimation error of activeF   passive  and activeerr for an infrequent class
(“crude”  4.41%  left) and a frequent class (“earn”  51.0%  center). These results are representative
for other frequent and infrequent classes  all results are included in the online appendix. Figure 3
(right) shows the estimation error of activeF   passive  and activeerr on all ten one-versus-rest
problems as a function of the problem’s class skew. We again observe that activeF outperforms
passive consistently  and activeF outperforms activeerr for strongly skewed class distributions.

4 Related Work

Sawade et al. [7] derive a variance-minimizing sampling distribution for risks. Their result does not
cover F -measures. Our experimental ﬁndings show that for estimating F -measures their variance-
minimizing sampling distribution performs worse than the sampling distributions characterized by
Theorem 1  especially for skewed class distributions.
Active estimation of generalized risks can be considered to be a dual problem of active learning; in
active learning  the goal of the selection process is to minimize the variance of the predictions or
the variance of the model parameters  while in active evaluation the variance of the risk estimate
is reduced. The variance-minimizing sampling distribution derived in Section 2.1 depends on the
unknown conditional distribution p(y|x). We use the model itself to approximate this distribution
and decide on instances whose class labels are queried. This is analogous to many active learning
algorithms. Speciﬁcally  Bach derives a sampling distribution for active learning under the assump-
tion that the current model gives a good approximation to the conditional probability p(y|x) [1]. To
compensate for the bias incurred by the instrumental distribution  several active learning algorithms
use importance weighting: for regression [8]  exponential family models [1]  or SVMs [2].
Finally  the proposed active estimation approach can be considered an instance of the general prin-
ciple of importance sampling [6]  which we employ in the context of generalized risk estimation.

5 Conclusions

Fα-measures are deﬁned as empirical estimates; we have shown that they are consistent estimates
of a generalized risk functional which Proposition 1 identiﬁes. Generalized risks can be estimated
actively by sampling test instances from an instrumental distribution q. An analysis of the sources
of estimation error leads to an instrumental distribution q∗ that minimizes estimator variance. The
optimal sampling distribution depends on the unknown conditional p(y|x); the active generalized
risk estimator approximates this conditional by the model to be evaluated.
Our empirical study supports the conclusion that the advantage of active over passive evaluation
is particularly strong for skewed classes. The advantage of active evaluation is also correlated
to the quality of the model as measured by the model-based likelihood of the test labels. In our
experiments  active evaluation consistently outperformed passive evaluation  even for the greatest
divergence between training and test distribution that we could observe.

7

02004006008000.020.040.060.080.1F0.5−measure (class fraction: 4.4%)labeling costs nestimation error (absolute)  passiveactiveFactiveerr02004006008000.0050.010.0150.02F0.5−measure (class fraction: 51.0%)labeling costs nestimation error (absolute)  passiveactiveFactiveerr0.010.030.10.320.010.1F0.5−measure (n=800)positive class fractionestimation error (absolute)  passiveactiveFactiveerrAppendix

(cid:80)n

i=1 viwi with vi = p(xi)

Proof of Lemma 2
Let (x1  y1)  ...  (xn  yn) be drawn according to q(x)p(y|x). Let ˆG0

i=1 vi(cid:96)iwi and Wn =
=
nG E [wi] and E [Wn] = n E [wi]. The random variables w1v1  . . .   wnvn and w1(cid:96)1v1  . . .   wn(cid:96)nvn
are iid  therefore the central limit theorem implies that 1
n Wn are asymptotically normally
n
distributed with

q(xi)   wi = w(xi  yi  fθ) and (cid:96)i = (cid:96)(fθ(xi)  yi). We note that E(cid:104) ˆG0
(cid:18) 1

n q and 1

(cid:105)

ˆG0

√

n q

n q =(cid:80)n

(17)

(cid:19)
(cid:19)
n q − G E [wi]
ˆG0
Wn − E [wi]

(cid:18) 1

n

n
√

n→∞−→ N (0  Var[wi(cid:96)ivi])
n→∞−→ N (0  Var[wivi])

(18)
where n→∞−→ denotes convergence in distribution. Application of the delta method to the func-
tion f (x  y) = x

n

n

(cid:32) 1

y yields
ˆG0
n q
n
1
n Wn

√

n

− G

(cid:33)
n→∞−→ N (0 ∇f (G E [wi]   E [wi])T Σ∇f (G E [wi]   E [wi]))
(cid:18)

(cid:19)

Var[wi(cid:96)ivi]

Cov[wi(cid:96)ivi  wivi]

Σ =

Cov[wi(cid:96)ivi  wivi]

Var[wivi]

.

where ∇f denotes the gradient of f and Σ is the asymptotic covariance matrix of the input arguments

Furthermore 

∇f (G E [wi]   E [wi])T Σ∇f (G E [wi]   E [wi])

= Var [wi(cid:96)ivi] − 2G Cov [wivi  wi(cid:96)ivi] + G2 Var [wivi]

(cid:3) + G2 E(cid:2)w2

i v2
i

(cid:3)

i (cid:96)iv2
i

= E(cid:2)w2
(cid:90)(cid:90) (cid:18) p(x)

i v2
i

i (cid:96)2

(cid:3) − 2G E(cid:2)w2
(cid:19)2

=

q(x)

w(x  y  fθ)2 ((cid:96)(fθ(x)  y) − G)2 p(y|x)q(x)dydx.

From this  the claim follows by canceling q(x).

Proof of Theorem 1

To minimize the variance with respect

straint(cid:82) q(x)dx = 1 we deﬁne the Lagrangian with Lagrange multiplier β
(cid:123)(cid:122)

(cid:90) c(x)
(cid:124)

(cid:90) c(x)

q(x)dx − 1

L [q  β] =

(cid:18)(cid:90)

dx + β

(cid:19)

q(x)

q(x)

=

(cid:125)

where c(x) = p(x)2(cid:82) w(x  y  fθ)2 ((cid:96)(fθ(x)  y) − G)2 p(y|x)dy. The optimal function for the con-

=K(q(x) x)

to the function q under the the normalization con-

+ βq(x)

dx − β 

(19)

∂q(x) = − c(x)

q(x)2 + β = 0. A solution for this

strained problem satisﬁes the Euler-Lagrange equation ∂K
Equation under the side condition is given by
q∗(x) =

(cid:112)c(x)
(cid:82)(cid:112)c(x)dx

.

(20)

Note that we dismiss the negative solution  since q(x) is a probability density function. Resubstitu-
tion of c in Equation 20 implies the theorem.

Acknowledgments

We gratefully acknowledge that this work was supported by a Google Research Award. We wish to
thank Michael Br¨uckner for his help with the experiments on spam data.

8

References
[1] F. Bach. Active learning for misspeciﬁed generalized linear models. In Advances in Neural

Information Processing Systems  2007.

[2] A. Beygelzimer  S. Dasgupta  and J. Langford. Importance weighted active learning. In Pro-

ceedings of the International Conference on Machine Learning  2009.

[3] H Cram´er. Mathematical Methods of Statistics  chapter 20. Princeton University Press  1946.
[4] A. Frank and A. Asuncion. UCI machine learning repository  2010.
[5] S. Geman  E. Bienenstock  and R. Doursat. Neural networks and the bias/variance dilemma.

Neural Computation  4:1–58  1992.

[6] J. Hammersley and D. Handscomb. Monte carlo methods. Taylor & Francis  1964.
[7] C. Sawade  N. Landwehr  S. Bickel  and T. Scheffer. Active risk estimation. In Proceedings of

the 27th International Conference on Machine Learning  2010.

[8] M. Sugiyama. Active learning in approximately linear regression based on conditional expec-

tation of generalization error. Journal of Machine Learning Research  7:141–166  2006.

[9] S. Tong and D. Koller. Support vector machine active learning with applications to text classi-

ﬁcation. Journal of Machine Learning Research  pages 45–66  2002.

[10] C. van Rijsbergen. Information Retrieval. Butterworths  2nd edition  1979.
[11] M. Yamada  M. Sugiyama  and T. Matsui. Semi-supervised speaker identiﬁcation under co-

variate shift. Signal Processing  90(8):2353–2361  2010.

9

,Hedi Hadiji