2018,Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression,In this paper  we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix $\mat{A} \in \R^{n \times d}$ where every row $a \in \R^d$ has $\|a\|_2^2 \leq L$ and numerical sparsity $\leq s$  i.e. $\|a\|_1^2 / \|a\|_2^2 \leq s$  we provide faster algorithms for these problems for many parameter settings.

For top eigenvector computation  when $\gap > 0$ is the relative gap between the top two eigenvectors of $\mat{A}^\top \mat{A}$ and $r$ is the stable rank of $\mat{A}$ we obtain a running time of $\otilde(nd + r(s + \sqrt{r s}) / \gap^2)$ improving upon the previous best unaccelerated running time of $O(nd + r d / \gap^2)$. As $r \leq d$ and $s \leq d$ our algorithm everywhere improves or matches the previous bounds for all parameter settings.

For regression  when $\mu > 0$ is the smallest eigenvalue of $\mat{A}^\top \mat{A}$ we obtain a running time of $\otilde(nd + (nL / \mu) \sqrt{s nL / \mu})$ improving upon the previous best unaccelerated running time of $\otilde(nd + n L d / \mu)$. This result expands when regression can be solved in nearly linear time from when $L/\mu = \otilde(1)$ to when $L / \mu = \otilde(d^{2/3} / (sn)^{1/3})$.

Furthermore  we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point \cite{frostig2015regularizing} / catalyst \cite{lin2015universal}. Our running times depend only on the size of the input and natural numerical measures of the matrix  i.e. eigenvalues and $\ell_p$ norms  making progress on a key open problem regarding optimal running times for efficient large-scale learning.,Exploiting Numerical Sparsity for Efﬁcient Learning :

Faster Eigenvector Computation and Regression

Department of Computer Science

Department of Management Science and Engineering

Aaron Sidford

Stanford University
Stanford  CA USA

sidford@stanford.edu

Neha Gupta

Stanford University
Stanford  CA USA

nehagupta@cs.stanford.edu

Abstract

1/(cid:107)a(cid:107)2

In this paper  we obtain improved running times for regression and top eigenvector
computation for numerically sparse matrices. Given a data matrix A ∈ Rn×d
where every row a ∈ Rd has (cid:107)a(cid:107)2
2 ≤ L and numerical sparsity at most s  i.e.
(cid:107)a(cid:107)2
2 ≤ s  we provide faster algorithms for these problems in many parameter
settings.
√
For top eigenvector computation  we obtain a running time of ˜O(nd + r(s +
rs)/gap2) where gap > 0 is the relative gap between the top two eigenvectors of
A(cid:62)A and r is the stable rank of A. This running time improves upon the previous
best unaccelerated running time of O(nd + rd/gap2) as r ≤ d and s ≤ d.

For regression  we obtain a running time of ˜O(nd + (nL/µ)(cid:112)snL/µ) where

µ > 0 is the smallest eigenvalue of A(cid:62)A. This running time improves upon
the previous best unaccelerated running time of ˜O(nd + nLd/µ). This result
expands the regimes where regression can be solved in nearly linear time from
when L/µ = ˜O(1) to when L/µ = ˜O(d2/3/(sn)1/3).
Furthermore  we obtain similar improvements even when row norms and numerical
sparsities are non-uniform and we show how to achieve even faster running times
by accelerating using approximate proximal point [9] / catalyst [15]. Our running
times depend only on the size of the input and natural numerical measures of the
matrix  i.e. eigenvalues and (cid:96)p norms  making progress on a key open problem
regarding optimal running times for efﬁcient large-scale learning.

1

Introduction

Regression and top eigenvector computation are two of the most fundamental problems in learning 
optimization  and numerical linear algebra. They are central tools for data analysis and of the simplest
problems in a hierarchy of complex machine learning computational problems. Consequently 
developing provably faster algorithms for these problems is often a ﬁrst step towards deriving new
theoretically motivated algorithms for large scale data analysis.
Both regression and top eigenvector computation are known to be efﬁciently reducible [10] to the
more general and prevalent ﬁnite sum optimization problem of minimizing a convex function f decom-
posed into the sum of m functions f1  ...  fm  i.e. minx∈Rn f (x) where f (x) = 1
i∈[m] fi(x).
m
This optimization problem encapsulates a variety of learning tasks where we have data points
{(a1  b1)  (a2  b2) ···   (an  bn)} corresponding to feature vectors ai  labels bi  and we wish to ﬁnd
the predictor x that minimizes the average loss of predicting bi from ai using x  denoted by fi(x).

(cid:80)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Given the centrality of this problem to machine learning and optimization  over the past few years
there have been extensive research efforts to design new provably efﬁcient methods for solving
this problem [12  9  13  6  20]. Using a variety of sampling techniques  impressive running time
improvements have been achieved. The emphasis in this line of work has been on improving the
dependence on the number of gradient evaluations of the fi that need to be performed  i.e. improving
dependence on m  as well as improving the dependence on other problem parameters.
Much less studied is the question of what structural assumptions on fi allow even faster running
times to be achieved. A natural and fundamental question in this space  is when can we achieve
faster running times by computing the gradients of fi approximately  thereby decreasing iteration
costs. While there has been work on combining coordinate descent methods with these stochastic
methods [13]  in the simple cases of regression and top eigenvector computation these methods do not
yield any improvement in iteration cost. More broadly we are unaware of previous work on linearly
convergent algorithms with faster running times for ﬁnite sum problems through this approach.
In this paper  we advance our understanding of the computational power of subsampling gradients of
the fi for the problems of top eigenvector computation and regression. In particular  we show that
under assumptions of numerical sparsity of the input matrix we can achieve provably faster algorithms
and new nearly linear time algorithms for a broad range of parameters. We achieve our result by
applying coordinate sampling techniques to Stochastic Variance Reduced Gradient Descent (SVRG)
[13  12]  a popular tool for ﬁnite sum optimization  along with linear algebraic data structures (in the
case of eigenvector computation) that we believe may be of independent interest.
The results in this paper constitute an important step towards resolving a key gap in our understanding
of optimal iterative methods for top eigenvector computation and regression. Ideally running times of
these problems would depend only on the size of the input  e.g. the number of non-zero entries in the
input data matrix  row norms  eigenvalues  etc. However  this is not the case for the current fastest
regression algorithms as these methods work by picking rows of the matrix non-uniformly yielding
expected iteration costs that depend on brittle weighted sparsity measures (which for simplicity are
typically instead stated in terms of the maximum sparsity among all rows  see Section 1.4.1). This
causes particularly unusual running times for related problems like nuclear norm estimation [17].
This paper takes an important step towards resolving this problem by providing running times for
top eigenvector computation and regression that depend only on the size of the input and natural
numerical quantities like eigenvalues  (cid:96)1-norms  (cid:96)2-norms  etc. While our running times do not
strictly dominate those based on the sparsity structure of the input (and it is unclear if such running
times are possible)  they improve upon the previous work in many settings. Ultimately  we hope this
paper provides useful tools for even faster algorithms for solving large scale learning problems.

1.1 The Problems
Throughout this paper we let A ∈ Rn×d denote a data matrix with rows a1  ...  an ∈ Rd. We
let sr(A) def= (cid:107)A(cid:107)2
2 denote the stable rank of A and we let nnz(A) denote the number of
non-zero entries in A. For symmetric M ∈ Rd×d we let λ1(M ) ≥ λ2(M ) ≥ ... ≥ λd(M )
M = x(cid:62)M x and we let gap(M ) def= (λ1(M ) − λ2(M ))/λ1(M )
denote its eigenvalues  (cid:107)x(cid:107)2
def= λ1(A(cid:62)A) 
denote its (relative) eigenvalue gap. For convenience we let gap def= gap(A(cid:62)A)  λ1
def= λ/µ. With this
µ def= λmin
notation  we consider the following two optimization problems.
Deﬁnition 1 (Top Eigenvector Problem) Find v∗ ∈ Rd such that

def= λd(A(cid:62)A)  sr def= sr(A)  nnz(A) def= nnz  κ def= (cid:107)A(cid:107)2

F /(cid:107)A(cid:107)2

F /µ and κmax

v∗ = argmax
x∈Rd (cid:107)x(cid:107)2=1

x(cid:62)A(cid:62)Ax

We call v an -approximate solution to the problem if (cid:107)v(cid:107)2 = 1 and v(cid:62)A(cid:62)Av ≥ (1 − )λ1(A(cid:62)A) .
Deﬁnition 2 (Regression Problem) Given b ∈ Rn ﬁnd x∗ ∈ Rd such that

x∗ = argmin
x∈Rd

(cid:107)Ax − b(cid:107)2

2

Given initial x0 ∈ Rd  we call x an -approximate solution if (cid:107)x − x∗(cid:107)A(cid:62)A ≤ (cid:107)x0 − x∗(cid:107)A(cid:62)A.

2

Each of these are known to be reducible to the ﬁnite sum optimization problem. The regression
i x − bi)2 and the top
problem is equivalent to the ﬁnite sum problem with fi(x) def= (m/2)(a(cid:62)
eigenvector problem is reducible with only polylogarithmic overhead to the ﬁnite sum problem with
fi(x) def= λ(cid:107)x − x0(cid:107)2

i x for carefully chosen λ and x0 [10].

i (x − x0))2 + b(cid:62)

2 − (m/2)(a(cid:62)

1.2 Our Results

In this paper  we provide improved iterative methods for top eigenvector computation and regression
that depend only on regularity parameters and not the speciﬁc sparsity structure of the input. Rather
than assuming uniform row sparsity as in previous work our running times depend on the numerical
sparsity of rows of A  i.e. si
2  which is at most the row sparsity  but may be smaller.
Note that our results  as stated  are worse as compared to the previous running times which depend on
the (cid:96)0 sparsity in some parameter regimes. For simplicity  we are stating our results in terms of only
the numerical sparsity. However  when the number of zero entries in a row is small  we can always
choose that row completely and not do sampling on it. This would lead to our results always as good
as the previous results and stricly better in some parameter regimes.

def= (cid:107)ai(cid:107)2

1/(cid:107)ai(cid:107)2

1.2.1 Top Eigenvector Computation

1/(gap2)(cid:80)

si +(cid:112)sr(A))

√

√

2/λ1)(

√
2/λ1)(

√
i((cid:107)ai(cid:107)2

i((cid:107)ai(cid:107)2
√
4 /

gap)((cid:80)

si +(cid:112)sr(A))

For top eigenvector computation  we give an unaccelerated running time of ˜O(nnz(A) +
si) and an accelerated running time of ˜O(nnz(A) +
(nnz(A) 3
4 ) as compared to the previous unacceler-
ated running time of ˜O(nnz(A) + maxi nnz(ai)sr(A)/gap2) and accelerated iterative methods of
˜O(nnz(A)3/4(maxi nnz(ai)(sr(A)/gap2))1/4) respectively.
In the simpler case of uniform row norms (cid:107)ai(cid:107)2

time (unaccelerated) becomes ˜O(nnz(A) + (sr(A)/gap2)(s +(cid:112)sr(A) · s)). To understand the
although the number of nnz(ai) = d  then our running time ˜O(nnz(A) + (sr(A)/gap2)(cid:112)sr(A))

relative strength of our results  we give an example of one parameter regime where our running times
are strictly better than the previous running times. When the rows are numerically sparse i.e. s = O(1)

2 and uniform row sparsity si = s  our running

2 = (cid:107)a(cid:107)2

si) 1

gives signiﬁcant improvement over the previous best running time of ˜O(nnz(A) + d(sr(A)/gap2))
since sr(A) ≤ d.

κ(cid:80)

1.2.2 Regression

i

√

√

√

i∈[n]

si(cid:107)ai(cid:107)2

(A(cid:62)A)−1.

si(cid:107)ai(cid:107)2
2/µ)
√
i((cid:107)ai(cid:107)2/

For regression we give an unaccelerated running time of ˜O(nnz(A) +

accelerated running time of ˜O(nnz(A)2/3κ1/6((cid:80)
2/µ) and an
1/3). Our methods improve upon
iterative methods of ˜O(nnz(A) + d maxi nnz(ai) +(cid:80)
the previous best unaccelerated iterative methods of ˜O(nnz(A) + κ maxi nnz(ai)) and accelerated
√
σi(A) maxi nnz(ai)) where
σi = (cid:107)ai(cid:107)2
2 = (cid:107)a(cid:107)2
In the simpler case of uniform row norms (cid:107)ai(cid:107)2
(unaccelerated) running time becomes ˜O(nnz(A) + κ3/2√
To understand the relative strength of our results  we give an example of one parameter regime
where our running times are strictly better than the previous running times. Consider the case where
κ = o(d2) and the rows are numerically sparse i.e. s = O(1) but maxi nnz(ai) = d  consider the
particular case of κ = d1.5  then our running time is ˜O(nnz(A) + d2.25) whereas the SVRG running
time for regression will be ˜O(nnz(A) + d2.5) and our running time is better in this case.

2 and uniform row sparsity si = s  our
si).

µ)

1.3 Overview of Our Approach

We achieve these results by carefully modifying known techniques for ﬁnite sum optimization
problem to our setting. The starting point for our algorithms is Stochastic Variance Reduced Gradient
Descent (SVRG) [12] a popular method for ﬁnite sum optimization. This method takes steps in the

3

direction of negative gradient in expectation and its convergence depends on a measure of variance of
the steps.
We apply SVRG to our problems where we carefully subsample the entries of the rows of the data
matrix so that we can compute steps that are the negative gradient in expectation in time possibly
sublinear in the size of the row. There is an inherent issue in such a procedure  in that this can change
the shape of variance. Previous sampling methods for regression ensure that the variance can be
directly related to the function error  whereas here such sampling methods give (cid:96)2 error  the bounding
of which in terms of function error can be expensive.
It is unclear how to completely avoid this issue and we leave this as future work. Instead  to mitigate
this issue we provide several techniques for subsampling that ensure we can obtain signiﬁcant
decrease in this (cid:96)2 error for small increases in the number of samples we take per row (See Section 3).
Here we crucially use that we have bounds on the numerical sparsity of rows of the data matrix and
prove that we can use this to quantify this decrease.
Formally  the sampling problem we have for each row is as follows. For each row ai at any point
we may receive some vector x and need to compute a random vector g with E[g] = aia(cid:62)
i x and with
E(cid:107)g(cid:107)2
i x) for some value of α  as
previous methods do. However  instead we settle for a bound of the form E(cid:107)g(cid:107)2
(cid:62)x)+β(cid:107)x(cid:107)2
2.
Our sampling schemes for this problem works as follows: For the outer ai  we sample from the
coordinates with probability proportional to the coordinate’s absolute value  we take a few (more than
(cid:62)x  we always take the dot
1) samples to control the variance (Lemma 4). For the approximation of ai
product of x with large coordinates of ai and we sample from the rest with probability proportional
to the squared value of the coordinate of ai and take more than one sample to control the variance
(Lemma 5).
Carefully controlling the number of samples we take per row and picking the right distribution over
rows gives our bounds for regression. For eigenvector computation the same broad techniques work
but to keep the iteration costs down but a little more care needs to be taken due to the structure of
fi(x) def= λ(cid:107)x − x0(cid:107)2
i x. Interestingly  for eigenvector computation the
penalty from (cid:96)2 error is in some sense smaller due to the structure of the objective.

2 sufﬁciently bounded. Ideally  we would have that E(cid:107)g(cid:107)2

2 ≤ α(a(cid:62)

2 ≤ α(ai

2 − (m/2)(a(cid:62)

i (x − x0))2 + b(cid:62)

1.4 Previous Results

Here we brieﬂy cover previous work on regression and eigenvector computation (Section 1.4.1) 
sparse ﬁnite sum optimization (Section 1.4.2)  and matrix entrywise sparsiﬁcation (Section 1.4.3).

1.4.1 Regression and Eigenvector Algorithms

There is an extensive amount of work on regression  eigenvector computation  and ﬁnite sum
optimization with far too many results to state but we have tried to include the algorithms with the
best known running times. The results for top eigenvector computation are stated in Table 1 and the
results for regression are stated in Table 2. The algorithms work according to the weighted (cid:96)0 sparsity
measure of all rows and do not take into account the numerical sparsity which is a natural parameter
to state the running times in and is strictly better than the (cid:96)0 sparsity.

1.4.2 Sparsity Structure

There has been some prior work on attempting to improve for sparsity structure. Particularly relevant
is the work of [13] on combining coordinate descent and sampling schemes. This paper picks
unbiased estimates of the gradient at each step by ﬁrst picking a function and then picking a random
coordinate whose variance decreases as time increases. Unfortunately  for regression and eigenvector
computation computing a partial derivative is as expensive as computing the gradient and hence  this
method does not give improved running times for regression and top eigenvector computation.

1.4.3 Entrywise Sparsiﬁcation

Another natural approach to yielding the results of this paper would be to simply subsample the
entries of A beforehand and use this as a preconditioner to solve the problem. There have been
multiple works on such entrywise sparsiﬁcation and in Table 3 we provide them. If we optimistically

4

Algorithm

Table 1: Previous results for computing -approximate top eigenvector (Deﬁnition 1).
(cid:17)
(cid:17)

(cid:17)
(cid:17)

Runtime

sparsity

gap

gap

˜O

˜O

Runtime with uniform row norms and

Power Method
Lanczos Method

(cid:16) nnz
(cid:16) nnz√

gap

˜O

(cid:16) nd
(cid:16) nd√

gap

˜O

(cid:19)

(cid:18)

˜O

Fast subspace embeddings +

Lanczos method [7]

SVRG (assuming bounded row

norms and warm start) [21]
Shift & Invert Power method

with SVRG [10]

Shift & Invert Power method
with Accelerated SVRG [10]

This paper
This paper

(cid:18)

˜O

nnz +

d·sr

˜O

˜O

(cid:19)
(cid:19)

(cid:18)
max {gap2.5  2.5}
nnz + d·sr2
(cid:18)
gap2
nnz + d·sr
gap2
(cid:0)√
(cid:80)

(cid:19)

gap

nnz + nnz3/4(d·sr)1/4

√

√
i (cid:107)ai(cid:107)2
√
(cid:107)ai(cid:107)2
(
λ1

2

2

si +
√

si +

sr)

(cid:18)

˜O

˜O(nnz +

˜O(nnz+ nnz

1

gap2λ1
3
4√

gap ((cid:80)

i

sr(cid:1) √

si)

(cid:19)

d·sr

nd +

˜O

(cid:18)
max {gap2.5  2.5}
nd + d·sr2
(cid:18)
gap2
nd + d·sr
gap2

(cid:19)
(cid:19)

˜O

(cid:18)
nd + (nd)3/4(d·sr)1/4
√

√
√

√

gap

(cid:19)

˜O

˜O(nd + sr

gap2 (

s +

s)

sr)
√

√

1
4 )

si))

˜O(nd + (nd)

3
4√

gap sr

1
4 (s +

sr · s)

1
4

)

Table 2: Previous results for solving approximate regression (Deﬁnition 2).

Algorithm

Runtime

Gradient Descent

Conjugate Gradient Descent

SVRG [12]

Accelerated SVRG [4  9  15]

Accelerated SVRG with

leverage score sampling [3]

This paper

This paper

˜O(nnz · κmax)
√
˜O(nnz
κmax)
˜O(nnz + κd)
√
nκd)

˜O(nnz +

˜O(nnz + d maxi nnz(ai) +(cid:80)
κ(cid:80)
˜O(nnz2/3κ1/6((cid:80)

√
i
σi(A) maxi nnz(ai))
√
si)
√

˜O(nnz +

µ
(cid:107)ai(cid:107)2

(cid:107)ai(cid:107)2

√

2

2

i

si)

i∈[n]

µ

µ ·
(cid:107)ai(cid:107)2√

Runtime with

uniform row norms

and sparsity
˜O(ndκmax)
√
˜O(nd
κmax)
˜O(nd + κd)
√
nκd)
√
κ· d3/2)
˜O(nd + d2 +
√
√
κ3

˜O(nd +

˜O(nd +

s)

1/3

)

˜O((nd)2/3κ1/2s1/6)

(cid:80)

√

F /λ3
F

(cid:80)
min) [14] and ˜O(nnz(A) +
i si(cid:107)ai(cid:107)2

compare them to our approach  by supposing that their sparsity bounds are uniform (i.e. every row
has the same sparsity) and bound its quality as a preconditioner the best of these would give bounds
of ˜O(nnz(A) + λmax(cid:107)A(cid:107)4
min)
[5] and ˜O(nnz(A) + (cid:107)A(cid:107)2
min) [1] for regression. Bound obtained by [14]
depends on the the condition number square and does not depend on the numerical sparsity struc-
ture of the matrix. Bound obtained by [5] is worse as compared to our bound when compared
with matrices having equal row norms and uniform sparsity. Our running time for regression is
˜O(nnz(A) +
√
si). Our results are not always comparable to that by [1]. Assuming
uniform sparsity and row norms  we get that Our runtime/Runtime by [1] = (λminn)/(
κ).
Depending on the values of the particular parameters  the ratio can be both greater or less than 1 and
hence  the results are incomparable. Our results are always better than that obtained by [14].

√
si(cid:107)ai(cid:107)2/

λmax(cid:107)A(cid:107)2

κ(cid:80)

2λmax/nλ3

i((cid:107)ai(cid:107)2

sλmax

2/µ)

nλ2

√

√

√

√

F

i

2 Notation

Vector Properties: For a ∈ Rd  let s(a) = (cid:107)a(cid:107)2
2 denote the numerical sparsity. For c ∈
{1  2  . . .   d}  let (Πc(a))i = ai if i ∈ S where S is a set of the c largest coordinates of a in absolute
value and 0 otherwise and ¯Πc(a) = a − Πc(a). Let Ic(a) denote the set of indices with the c largest
coordinates of a in absolute value and ¯Ic(a) = [d]\ Ic(a) i.e. everything except the top c co-ordinates.
Let ˆej denote the ith basis vector i.e. (ˆej)i = 1 if i = j and 0 otherwise.
Other: Let [d] denote the set {1  2  . . .   d}. We use ˜O notation to hide polylogarithmic factors in the
input parameters and error rates. Refer to Section 1.1 for other deﬁnitions.

1/(cid:107)a(cid:107)2

5

3 Sampling techniques
In this section we provide our key tools for sampling from a matrix for both regression and eigenvector
computation. First  we provide a technical lemma on numerical sparsity that we will use throughout
our analysis. Then  we provide and analyze the sampling distribution we use to sample from our
matrix for SVRG. We use the same distribution for both the applications  regression and eigenvector
computation and provide some of the analysis of properties of this distribution. All proofs in this
section are differed to Appendix B.1.
We begin with a lemma at the core of the proofs of our sampling techniques. The lemma essentially
states that for a numerically sparse vector  most of the (cid:96)2-mass of the vector is concentrated in its
top few coordinates. Consequently  if a vector is numerically sparse then we can remove a few big
coordinates from it and reduce its (cid:96)2 norm considerably. Later  in our sampling schemes  we will use
this lemma to bound the variance of sampling a vector.
Lemma 3 (Numerical Sparsity) For a ∈ Rd and c ∈ [d]  we have (cid:107) ¯Πc(a)(cid:107)2

2 ≤ s(a)(cid:107)a(cid:107)2

2/c.

The following lemmas state the sampling distribution that we use for sampling the gradient function
2 xA(cid:62)Ax − b(cid:62)x i.e.
in SVRG. Basically  since we want to approximate the gradient of f (x) = 1

A(cid:62)Ax − b  we would like to sample A(cid:62)Ax =(cid:80)

i∈[n] aia(cid:62)
i x.

We show how to perform this sampling and analyze it in several steps.
In Lemma 4 we
show how to sample from a and then in Lemma 5 we show how to sample from a(cid:62)x.
In
Lemma 6 we put these together to sample from aa(cid:62)x and in Lemma 7 we put it all to-
gether to sample from A(cid:62)A. The distributions and our guarantees on them are stated below.

Algorithm 1: Samplevec(a  c)
1: for t = 1 . . . c (i.i.d. trials) do
2:
3:
4: end for
5: Output 1
c

randomly sample indices jt with
Pr(jt = j) = pj =

(cid:80)c

|aj|
(cid:107)a(cid:107)1

ˆejt

t=1

ajt
pjt

∀j ∈ [d]

Algorithm 3: Samplerankonemat(a  x  c)

1: ((cid:98)a)c = Samplevec(a  c)
2: ((cid:100)a(cid:62)x)c = Sampledotproduct(a  x  c)
3: Output ((cid:98)a)c((cid:100)a(cid:62)x)c

Algorithm 2: Sampledotproduct(a  x  c)
1: for t = 1 . . . c (i.i.d. trials) do
randomly sample indices jt with
2:
3:
Pr(jt = j) = pj =
4: end for
5: Output Πc(a)(cid:62)x + 1

(cid:80)c

(cid:107) ¯Πc(a)(cid:107)2

ajt xjt

a2
j

2

c

t=1

pjt

∀j ∈ ¯Ic(a)

√

Algorithm 4: Samplemat(A  x  k)
1: ci =

2: M =(cid:80)

si · k ∀i ∈ [n]
i (cid:107)ai(cid:107)2
2(1 + si
ci

)

3: Select a row index i with probability

pi =
(cid:92)
aia(cid:62)

4: (
5: Output 1
pi

(cid:107)ai(cid:107)2
M (1 + si
ci

2

)

(cid:92)
aia(cid:62)

i x)ci

(

i x)ci = Samplerankonemat(ai  x  ci)

Lemma 4 (Stochastic Approximation of a) Let a ∈ Rd and c ∈ N and let our estimator (ˆa)c =
Samplevec(a  x) (Algorithm 1) Then 

E[(ˆa)c] = a and E(cid:2)(cid:107)(ˆa)c(cid:107)2

2

(cid:3) ≤ (cid:107)a(cid:107)2

2

(cid:18)

(cid:19)

1 +

s(a)

c

Lemma 5 (Stochastic Approximation of a(cid:62)x) Let a  x ∈ Rd and c ∈ [d]  and let our estimator be

deﬁned as ((cid:100)a(cid:62)x)c = Sampledotproduct(a  x  c) (Algorithm 2) Then 
(cid:105) ≤ (a(cid:62)x)2 +

E[((cid:100)a(cid:62)x)c] = a(cid:62)x and E(cid:104)

((cid:100)a(cid:62)x)2

c

(cid:107) ¯Πc(a)(cid:107)2

2(cid:107)x(cid:107)2

2

1
c

6

Lemma 6 (Stochastic Approximation of aa(cid:62)x) Let a  x ∈ Rd and c ∈ [d]  and the estimator be
deﬁned as (
(cid:91)

(cid:91)
aa(cid:62)x)c = Samplerankonemat(a  x  c) (Algorithm 3) Then 

aa(cid:62)x)c] = aa(cid:62)x and E(cid:104)(cid:107)(

(cid:91)
aa(cid:62)x)c(cid:107)2

(a(cid:62)x)2 +

(cid:19)(cid:18)

(cid:18)

(cid:19)

E[(

s(a)

s(a)

1 +

2(cid:107)x(cid:107)2

2

2

c2 (cid:107)a(cid:107)2

c

2

(cid:105) ≤ (cid:107)a(cid:107)2
(cid:13)(cid:13)(cid:13)(cid:13)2

2

Lemma 7 (Stochastic Approximation of A(cid:62)Ax ) Let A ∈ Rn×d with rows a1  a2  . . .   an and
(cid:92)
x ∈ Rd and let (
A(cid:62)Ax)k = Samplemat(A  x  k) (Algorithm 4) where k is some parameter. Then 
(cid:92)
A(cid:62)Ax)k
(

= A(cid:62)Ax and E

(cid:92)
A(cid:62)Ax)k

(cid:107)Ax(cid:107)2

≤ M

(cid:18)

(cid:19)

(cid:35)

(cid:20)

(cid:21)

F(cid:107)x(cid:107)2

2 +

E

2

.

1

k2(cid:107)A(cid:107)2

(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(

4 Applications

Using the framework of SVRG deﬁned in Theorem 14 and the sampling techniques presented in
Section 3  we now state how do we solve our problems of regression and top eigenvector computation.

4.1 Eigenvector computation

1

= 1

λ−λ1

λ−λ1

)/ 1

− 1
λ−λ2

The classic method to estimate the top eigenvector of a matrix is to apply power method which
involves starting with an initial vector x0 and repeatedly multiplying the vector by A(cid:62)A which
eventually leads to convergence of the vector to the top eigenvector of the matrix A(cid:62)A if top
eigenvalue of the matrix is well separated from the other eigenvalues i.e. gap is large enough. The
number of iterations required for convergence is O(log( d
 )/gap). However  this method can be very
slow when the gap is small. If the gap is small  one thing that can be done to improve convergence rate
is running the power method on a matrix B−1 where B = λI − A(cid:62)A. B−1 has the same largest
2 if λ ≈ (1 + gap)λ1 and
eigenvector as A(cid:62)A and the eigenvalue gap is (
thus we get a constant eigenvalue gap. Hence  if we have a rough estimate of the largest eigenvalue
of the matrix  we can get the gap parameter as roughly constant. Section 6 of [10] shows how we
can get such an estimate based on the gap free eigenvalue estimation algorithm by [16] in running
time dependent on the linear system solver of B ignoring some additional polylogarithmic factors.
However  doing power iteration on B−1 requires solving linear systems on B whose condition
number now depends on 1/gap and thus  solving linear system on B would become expensive. [10]
showed how we can solve the linear systems in B faster by using SVRG [12] and achieved a better
overall running time for top eigenvector computation. The formal theorem statement is differed to
Theorem 17 in the appendix.
We simply use this framework for solving the eigenvector problem using SVRG and on the top of that 
give different sampling scheme for SVRG for B−1 which reduces the runtime for numerically sparse
matrices. Basically  we use the sampling scheme presented in Lemma 7. The following lemma states
the variance bound that we get for the gradient updates for SVRG for the top eigenvector computation
problem. This will be used to obtain a bound on the solving of linear systems in B = λI − A(cid:62)A
which will be ultimately used in solving the approximate topeigen vector problem.
Lemma 8 (Variance bound for eigenvector computation) Let ∇g(x) = λx − (
(cid:92)
(

A(cid:62)Ax)k is the estimator of A(cid:62)Ax deﬁned in Lemma 7  and k =(cid:112)sr(A)  then we get
(cid:3) ≤ (f (x) − f (x∗))8M/gap
E[∇g(x)] = (λI − A(cid:62)A)x and E(cid:2)(cid:107)∇g(x) − ∇g(x∗)(cid:107)2
(cid:16)
(cid:17)
si +(cid:112)sisr(A)
with average time taken in calculating ∇g(x)  T =(cid:80)
(cid:18)
(cid:80)
i (cid:107)ai(cid:107)2

(cid:92)
A(cid:62)Ax)k where

2 x(cid:62)Bx − b(cid:62)x

(cid:113) si

/M where M =

and f (x) = 1

i (cid:107)ai(cid:107)2

(cid:19)

1 +

sr(A)

2

2

2

Now  using the variance of the gradient estimators and per iteration running time T obtained in
Lemma 8 along with the framework of SVRG [12] (deﬁned in Theorem 14)  we can get constant
multiplicative decrease in the error in solving linear systems in B = λI − A(cid:62)A in total running time

7

(cid:80)

√
i (cid:107)ai(cid:107)2
2(

si +(cid:112)sr(A))

√

2

gap2λ1(A(cid:62)A)

si) assuming we have a crude approximation
O(nnz(A)+
to the top eigenvector and eigenvalue which we have already discussed we can get. The formal
theorem statement (Theorem 18) and proof are differed to the appendix. Now  using the linear system
solver descibed above along with the shift and invert algorithmic framework  we get the following
running time for top eigenvector computation problem. The proof appears in Appendix B.2.
Theorem 9 (Numerically Sparse Top Eigenvector Computation Runtime) Linear system solver
from Theorem 18 combined with the shift and invert framework from [10] stated in Theorem 17 gives
an algorithm which computes -approximate top eigenvector (Deﬁnition 1) in total running time
O

(cid:17)√
si +(cid:112)sr(A)

log2(cid:16) d

+ log(cid:0) 1

(cid:80)
i (cid:107)ai(cid:107)2

(cid:17) ·(cid:16)

nnz(A) + 1

(cid:1)(cid:17)(cid:17)

(cid:16)√

(cid:16)(cid:16)

(cid:17)

si

2

gap



gap2λ1

Similarly  using the acceleration framework of [9] mentioned in Theorem 15 in the appendix along
with the linear system solver runtime  we get the following accelerated running time for top eigenvec-
tor computation and the proof appears in Appendix B.2.
Theorem 10 (Numerically Sparse Accelerated Top Eigenvector Computation Runtime)
Linear system solver from Theorem 18 combined with acceleration framework from [9] men-
tioned in Theorem 15 and shift and invert framework from [10] stated in Theorem 17 gives an
algorithm which computes -approximate top eigenvector (Deﬁnition 1) in total running time
where ˜O hides a factor of
˜O

(cid:17)√
si +(cid:112)sr(A)

nnz(A) + nnz(A)3/4

(cid:17)1/4(cid:19)

(cid:16)√

si)

(cid:18)
log2(cid:16) d

(cid:16)

gap

(cid:16)(cid:80)
(cid:16) d

i

log

(cid:107)ai(cid:107)2
λ1

2

(cid:17)

gap

.

(cid:17)

√

+ log(cid:0) 1

gap



(cid:1)(cid:17)

4.2 Linear Regression

(cid:80)
i x(cid:62)aiai

2(cid:107)Ax − b(cid:107)2

2

2 which is equivalent to minimizing
In linear regression  we want to minimize 1
2 x(cid:62)A(cid:62)Ax − x(cid:62)A(cid:62)b = 1
(cid:62)x − x(cid:62)A(cid:62)b and hence  we can apply the framework
1
of SVRG [12] (stated in Theorem 14) for solving it. However  instead of selecting the complete
row for calculating the gradient  we only select a few entries from the row to achieve lower cost
per iteration. In particular  we use the distribution deﬁned in Lemma 7. Note that the sampling
probabilities depend on λd and we need to know a constant factor approximation of λd for the scheme
to work. For most of the ridge regression problems  we know a lower bound on the value of λd and we
can get an approximation by doing a binary search over all the values and paying an extra logarithmic
factor. The following lemma states the sampling distribution which we use for approximating the true
gradient and the corresponding variance that we obtain. The proof of this appears in Appendix B.2.
(cid:92)
Lemma 11 (Variance Bound for Regression) Let ∇g(x) = (
A(cid:62)Ax)k is the
estimator for A(cid:62)Ax deﬁned in Lemma 7 and k =

κ  assuming κ ≤ d2 we get

(cid:92)
A(cid:62)Ax)k where (

√

E[∇g(x)] = A(cid:62)Ax and E(cid:104)(cid:107)∇g(x) − ∇g(x∗)(cid:107)2
(cid:0)1 +(cid:112) si

(cid:1) where f (x) = 1

with average time taken in calculating ∇g(x)  T =

(cid:80)
i (cid:107)ai(cid:107)2

2(cid:107)Ax − b(cid:107)2

2
√

κ

2

2

κ
M

(cid:105) ≤ M (f (x) − f (x∗))
(cid:80)

i∈[n] (cid:107)ai(cid:107)2

√

2

si where M =

Using the variance bound obtained in Lemma 11 and the framework of SVRG stated in Theorem
14 for solving approximate linear systems  we show how we can obtain an algorithm for solving
approximate regression in time which is faster in certain regimes when the corresponding matrix is
numerically sparse. The proof of this appears in Appendix B.2.
Theorem 12 (Numerically Sparse Regression Runtime) For solving -approximate regression
(Deﬁnition 2)  if κ ≤ d2  SVRG framework from Theorem 14 and the variance bound from Lemma 11
gives an algorithm with running time O

log(cid:0) 1

κ(cid:80)

(cid:16)(cid:16)

(cid:1)(cid:17)

nnz(A) +

(cid:107)ai(cid:107)2

(cid:17)

√

√

.

2

i∈[n]

µ

si



Combined with the additional acceleration framework mentione in Theorem 15  we can get an
accelerated algorithm for solving regression. The proof of this appears in Appendix B.2.2.
Theorem 13 (Numerically Sparse Accelerated Regression Runtime) For solving -approximate
regression (Deﬁnition 2) if κ ≤ d2  SVRG framework from Theorem 14  acceleration framework

8

from Theorem 15 and the variance bound from Lemma 11 gives an algorithm with running time

(cid:18)

nnz(A)2/3κ1/6(cid:16)(cid:80)

O

(cid:107)ai(cid:107)2

2

µ

i∈[n]

√

si

(cid:17)1/3

log(κ) log(cid:0) 1



(cid:1)(cid:19)

Acknowledgments

We would like to thank the anonymous reviewers who helped improve the readability and presentation
of this draft by providing many helpful comments.

References
[1] Dimitris Achlioptas  Zohar S Karnin  and Edo Liberty. Near-optimal entrywise sampling for
data matrices. In Advances in Neural Information Processing Systems  pages 1565–1573  2013.

[2] Dimitris Achlioptas and Frank McSherry. Fast computation of low-rank matrix approximations.

Journal of the ACM (JACM)  54(2):9  2007.

[3] Naman Agarwal  Sham Kakade  Rahul Kidambi  Yin Tat Lee  Praneeth Netrapalli  and Aaron
Sidford. Leverage score sampling for faster accelerated regression and erm. arXiv preprint
arXiv:1711.08426  2017.

[4] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
1200–1205. ACM  2017.

[5] Sanjeev Arora  Elad Hazan  and Satyen Kale. A fast random sampling algorithm for sparsifying
matrices. In Approximation  Randomization  and Combinatorial Optimization. Algorithms and
Techniques  pages 272–279. Springer  2006.

[6] Léon Bottou and Yann L Cun. Large scale online learning. In Advances in neural information

processing systems  pages 217–224  2004.

[7] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input
sparsity time. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing 
pages 81–90. ACM  2013.

[8] Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsiﬁcation via a

matrix-valued bernstein inequality. Information Processing Letters  111(8):385–389  2011.

[9] Roy Frostig  Rong Ge  Sham Kakade  and Aaron Sidford. Un-regularizing: approximate
proximal point and faster stochastic algorithms for empirical risk minimization. In International
Conference on Machine Learning  pages 2540–2548  2015.

[10] Dan Garber  Elad Hazan  Chi Jin  Cameron Musco  Praneeth Netrapalli  Aaron Sidford  et al.
Faster eigenvector computation via shift-and-invert preconditioning. In International Conference
on Machine Learning  pages 2626–2634  2016.

[11] Alex Gittens and Joel A Tropp. Error bounds for random matrix approximation schemes. arXiv

preprint arXiv:0911.4108  2009.

[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[13] Jakub Koneˇcn`y  Zheng Qu  and Peter Richtárik. Semi-stochastic coordinate descent. Optimiza-

tion Methods and Software  32(5):993–1005  2017.

[14] Abhisek Kundu and Petros Drineas. A note on randomized element-wise matrix sparsiﬁcation.

arXiv preprint arXiv:1404.0320  2014.

[15] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimiza-

tion. In Advances in Neural Information Processing Systems  pages 3384–3392  2015.

9

[16] Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and
faster approximate singular value decomposition. In Advances in Neural Information Processing
Systems  pages 1396–1404  2015.

[17] Cameron Musco  Praneeth Netrapalli  Aaron Sidford  Shashanka Ubaru  and David P Woodruff.
Spectrum approximation beyond fast matrix multiplication: Algorithms and hardness. arXiv
preprint arXiv:1704.04163  2017.

[18] Nam H Nguyen  Petros Drineas  and Trac D Tran. Tensor sparsiﬁcation via a bound on the

spectral norm of random tensors. arXiv preprint arXiv:1005.4732  2010.

[19] NH Nguyen  Petros Drineas  and TD Tran. Matrix sparsiﬁcation via the khintchine inequality.

2009.

[20] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  14(Feb):567–599  2013.

[21] Ohad Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In

International Conference on Machine Learning  pages 144–152  2015.

10

,Neha Gupta
Aaron Sidford