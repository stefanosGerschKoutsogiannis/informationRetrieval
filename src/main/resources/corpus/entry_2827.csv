2015,Latent Bayesian melding for integrating individual and population models,In many statistical problems  a more coarse-grained model may be suitable for population-level behaviour   whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching  in that they allow matchingexpectations between two models  but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding  which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation  which is a type of single-channel blind source separation problem  we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.,Latent Bayesian melding for integrating individual

and population models

Mingjun Zhong  Nigel Goddard  Charles Sutton

{mzhong nigel.goddard csutton}@inf.ed.ac.uk

School of Informatics
University of Edinburgh

United Kingdom

Abstract

In many statistical problems  a more coarse-grained model may be suitable for
population-level behaviour  whereas a more detailed model is appropriate for ac-
curate modelling of individual behaviour. This raises the question of how to in-
tegrate both types of models. Methods such as posterior regularization follow
the idea of generalized moment matching  in that they allow matching expec-
tations between two models  but sometimes both models are most conveniently
expressed as latent variable models. We propose latent Bayesian melding  which
is motivated by averaging the distributions over populations statistics of both the
individual-level and the population-level models under a logarithmic opinion pool
framework. In a case study on electricity disaggregation  which is a type of single-
channel blind source separation problem  we show that latent Bayesian melding
leads to signiﬁcantly more accurate predictions than an approach based solely on
generalized moment matching.

1

Introduction

Good statistical models of populations are often very different from good models of individuals.
As an illustration  the population distribution over human height might be approximately normal 
but to model an individual’s height  we might use a more detailed discriminative model based on
many features of the individual’s genotype. As another example  in social network analysis  simple
models like the preferential attachment model [3] replicate aggregate network statistics such as
degree distributions  whereas to predict whether two individuals have a link  a social networking
web site might well use a classiﬁer with many features of each person’s previous history. Of course
every model of an individual implies a model of the population  but models whose goal is to model
individuals tend to be necessarily more detailed.
These two styles of modelling represent different types of information  so it is natural to want to
combine them. A recent line of research in machine learning has explored the idea of incorporating
constraints into Bayesian models that are difﬁcult to encode in standard prior distributions. These
methods  which include posterior regularization [9]  learning with measurements [16]  and the gen-
eralized expectation criterion [18]  tend to follow a moment matching idea  in which expectations of
the distribution of one model are encouraged to match values based on prior information.
Interestingly  these ideas have precursors in the statistical literature on simulation models. In partic-
ular  Bayesian melding [21] considers applications in which there is a computer simulation M that
maps from model parameters θ to a quantity φ = M(θ). For example  M might summarize the
output of a deterministic simulation of population dynamics or some other physical phenomenon.
Bayesian melding considers the case in which we can build meaningful prior distributions over both
θ and φ. These two prior distributions need to be merged because of the deterministic relationship;

1

this is done using a logarithmic opinion pool [5]. We show that there is a close connection between
Bayesian melding and the later work on posterior regularization  which does not seem to have been
recognized in the machine learning literature. We also show that Bayesian melding has the addi-
tional advantage that it can be conveniently applied when both individual-level and population-level
models contain latent variables  as would commonly be the case  e.g.  if they were mixture models
or hierarchical Bayesian models. We call this approach latent Bayesian melding.
We present a detailed case study of latent Bayesian melding in the domain of energy disaggregation
[11  20]  which is a particular type of blind source separation (BSS) problem. The goal of the
electricity disaggregation problem is to separate the total electricity usage of a building into a sum of
source signals that describe the energy usage of individual appliances. This problem is hard because
the source signals are not identiﬁable  which motivates work that adds additional prior information
into the model [14  15  20  25  26  8]. We show that the latent Bayesian melding approach allows
incorporation of new types of constraints into standard models for this problem  yielding a strong
improvement in performance  in some cases amounting to a 50% error reduction over a moment
matching approach.

2 The Bayesian melding approach

We brieﬂy describe the Bayesian melding approach to integrating prior information in deterministic
simulation models [21]  which has seen wide application [1  6  23].
In the Bayesian modelling
context  denote Y as the observation data  and suppose that the model includes unknown variables
S  which could include model parameters and latent variables. We are then interested in the posterior

p(S|Y ) = p(Y )−1p(Y |S)pS(S).

t=1 ST

τ =(cid:80)T

(1)
However  in some situations  the variables S may be related to a new random variable τ by a de-
terministic simulation function f(·) such that τ = f(S). We call S and τ input and output vari-
ables. For example  in the energy disaggregation problem  the total energy consumption variable
t µ where St are the state variables of a hidden Markov model (one-hot encoding) and
µ is a vector containing the mean energy consumption of each state (see Section 5.2). Both τ and
S are random variables  and so in the Bayesian context  the modellers usually choose appropriate
priors pτ (τ) and pS(S) based on prior knowledge. However  given pS(S)  the map f naturally
introduces another prior for τ  which is an induced prior denoted by p∗
τ (τ). Therefore  there are
two different priors for the same variable τ from different sources  which might not be consistent.
In the energy disaggregation example  p∗
τ is induced by the state variables St of the hidden Markov
model which is the individual model of a speciﬁc household  and pτ could be modelled by using
population information  e.g. from a national survey — we can think of this as a population model
since it combines information from many households. The Bayesian melding approach combines
the two priors into one by using the logarithmic pooling method so that the logarithmically pooled

τ (τ)αpτ (τ)1−α where 0 ≤ α ≤ 1. The prior (cid:101)pτ melds the prior information

prior is (cid:101)pτ (τ) ∝ p∗

of both S and τ. In the model (1)  the prior pS does not include information about τ. Thus it is
required to derive a melded prior for S. If f is invertible  the prior for S can be obtained by using
the change-of-variable technique. If f is not invertible  Poole and Raftery [21] heuristically derived
a melded prior

(cid:101)pS(S) = cαpS(S)

(cid:18) pτ (f(S))

(cid:19)1−α

where cα is a constant given α such that(cid:82) (cid:101)pS(S)dS = 1. This gives a new posterior (cid:101)p(S|Y ) =
(cid:101)p(Y )−1p(Y |S)(cid:101)pS(S). Note that it is interesting to infer α [22  7]  however we use a ﬁxed value in

this paper. So far we have been assuming there are no latent variables in pτ . We now consider the
situation when τ is generated by some latent variables.

p∗
τ (f(S))

(2)

3 The latent Bayesian melding approach

It is common that the variable τ is modelled by a latent variable ξ  see the examples in Section 5.2.
So we could assume that we have a conditional distribution p(τ|ξ) and a prior distribution pξ(ξ).

This deﬁnes a marginal distribution pτ (τ) = (cid:82) pξ(ξ)p(τ|ξ)dξ. This could be used to produce the

2

melded prior (2) of the Bayesian melding approach

(cid:101)pS(S) = cαpS(S)

(cid:18)(cid:82) pτ (f(S)|ξ)pξ(ξ)dξ

p∗
τ (f(S))

(cid:19)1−α

.

(3)

(cid:18) pτ (f(S)|ξ)pξ(ξ)

ξ (cid:101)pS ξ(S  ξ) = max

The integration in (3) is generally intractable. We could employ the Monte Carlo method to approx-
imate it for a ﬁxed τ. However  importantly we are also interested in inferring the latent variable ξ
which is meaningful for example in the energy disaggregation problem. When we are interested in

ﬁnding the maximum a posteriori (MAP) value of the posterior where(cid:101)pS(S) was used as the prior 
we propose to use a rough approximation(cid:82) pξ(ξ)pτ (τ|ξ)dξ ≈ maxξ pξ(ξ)pτ (τ|ξ). This leads to an
approximate prior(cid:101)pS(S) ≈ max
To obtain this approximate prior for S  the joint prior(cid:101)pS ξ(S  ξ) has to exist  and so we show that it
does exist under certain conditions by the following theorem. We assume that S and ξ are continuous
τ and pτ are positive and share the same support. Also  EpS (S)[·]
random variables  and that both p∗
(cid:105)
denotes the expectation with respect to pS.
(cid:82) (cid:101)pS ξ(S  ξ)dξdS = 1  for any ﬁxed α ∈ [0  1].
Theorem 1.
imate joint prior(cid:101)pS ξ. Interestingly  if ξ and S are independent conditional on τ  we can show as
follows that(cid:101)pS ξ is a limit distribution derived from a joint distribution of ξ and S induced by τ. To

The proof can be found in the supplementary materials. In (4) we heuristically derived an approx-

then a constant cα < ∞ exists such that

(cid:104) pτ (f (S))

(cid:19)1−α

p∗
τ (f(S))

If EpS (S)

< ∞ 

cαpS(S)

p∗
τ (f (S))

(4)

.

ξ

see this  we derive a joint prior for S and ξ 
p(S  ξ|τ)pτ (τ)dτ =

pS ξ(S  ξ) =

(cid:90)
(cid:90) p(τ|S)pS(S)

p∗
τ (τ)

=

p(τ|ξ)pξ(ξ)

pτ (τ)

(cid:90)

p(S|τ)p(ξ|τ)pτ (τ)dτ

pτ (τ)dτ = pS(S)pξ(ξ)

(cid:90)

p(τ|S) p(τ|ξ)

τ (τ) dτ.
p∗

is then denoted by pδ(τ|S). The marginal distribution is pδ(τ) = (cid:82) pδ(τ|S)pS(S)dS. Denote

For a deterministic simulation τ = f(S)  the distribution p(τ|S) = p(τ|S  τ = f(S)) is ill-deﬁned
due to the Borel’s paradox [24]. The distribution p(τ|S) depends on the parameterization. We
assume that τ is uniform on [f(S) − δ  f(S) + δ] conditional on S and δ > 0  and the distribution
g(τ) = p(τ|ξ)
Theorem 2. If limδ→0 pδ(τ) = p∗
limδ→0

τ (τ ) and gδ(τ) = p(τ|ξ)
(cid:82) pδ(τ|S)gδ(τ)dτ = g(f(S)).
p∗

τ (τ)  and gδ(τ) has bounded derivatives in any order  then

pδ(τ ) . Then we have the following theorem.

See the supplementary materials for the proof. Under this parameterization  we denote ˆpS ξ(S  ξ) =
pS(S)pξ(ξ) limδ→0
τ (f (S)) . By applying the logarithmic pool-
p∗
ing method  we have a joint prior

(cid:82) pδ(τ|S)gδ(τ)dτ = pS(S)pξ(ξ) p(f (S)|ξ)
(cid:101)pS ξ(S  ξ) = cα (pS(S))α (ˆpS ξ(S  ξ))1−α = cαpS(S)

(cid:18) pτ (f(S)|ξ)pξ(ξ)

Since the joint prior blends the variable S and the latent variable ξ  we call this approxi-

mation the latent Bayesian melding (LBM) approach  which gives the posterior (cid:101)p(S  ξ|Y ) =
(cid:101)p(Y )−1p(Y |S)(cid:101)pS ξ(S  ξ). Note that if there are no latent variables  then latent Bayesian meld-

ing collapses to the Bayesian melding approach. In section 6 we will apply this method to an energy
disaggregation problem for integrating population information with an individual model.

(cid:19)1−α

p∗
τ (f(S))

.

4 Related methods

We now discuss possible connections between Bayesian melding (BM) and other related methods.
Recently in machine learning  moment matching methods have been proposed  e.g.  posterior regu-
larization (PR) [9]  learning with measurements [16] and the generalized expectation criterion [18].

3

These methods share the common idea that the Bayesian models (or posterior distributions) are con-
strained by some observations or measurements to obtain a least-biased distribution. The idea is
that the system we are modelling is too complex and unobservable  and thus we have limited prior
information. To alleviate this problem  we assume we can obtain some observations of the system
in some way  e.g.  by experiments  for example those observations could be the mean values of
the functions of the variables. Those observations could then guide the modelling of the system.
Interestingly  a very similar idea has been employed in the bias correction method in information
theory and statistics [12  10  19]  where the least-biased distribution is obtained by optimizing the
Kullback-Leibler divergence subject to the moment constraints. Note that the bias correction method
in [17] is different to others where the bias of a consistent estimator was corrected when the bias
function could be estimated.
We now consider the posteriors derived by PR and BM. In general  given a function f(S) and values
bi  PR solves the constrained problem
minimize

KL((cid:101)p(S)||p(S|Y ))
subject to Eep (mi(f(S))) − bi ≤ δi ||δi|| ≤ ; i = 1  2 ···   I.
ep
(cid:101)pP R(S) = Z(λ)−1p(Y |S)p(S)(cid:81)I
where mi could be any function such as a power function. This gives an optimal posterior
i=1 exp(−λimi(f(S))) where Z(λ) is the normalizing con-
(cid:16) pτ (f (S))
stant. BM has a deterministic simulation f(S) = τ where τ ∼ pτ . The posterior is then
(cid:101)pBM (S) = Z(α)−1p(Y |S)p(S)
the last factor which is derived from the constraints or the deterministic simulation. (cid:101)pP R and(cid:101)pBM
are identical  if −(cid:80)I
. They have a similar form and the key difference is

i=1 λimi(f(S)) = (1 − α) log pτ (f (S))
τ (f (S)).
p∗

(cid:17)1−α

p∗
τ (f (S))

The difference between BM and LBM is the latent variable ξ. We could perform BM by integrating
out ξ in (3)  but this is computationally expensive. Instead  LBM jointly models S and ξ allowing
possibly joint inference  which is an advantage over BM.

5 The energy disaggregation problem

signals so that Yt = (cid:80)I

In energy disaggregation  we are given a time series of energy consumption readings from a sensor.
We consider the energy measured in watt hours as read from a household’s electricity meter  which is
denoted by Y = (Y1  Y2 ···   YT ) where Yt ∈ R+. The recorded energy signal Y is assumed to be
the aggregation of the consumption of individual appliances in the household. Suppose there are I
appliances  and the energy consumption of each appliance is denoted by Xi = (Xi1  Xi2 ···   XiT )
where Xit ∈ R+. The observed aggregate signal is assumed to be the sum of the component
i=1 Xit + t where t ∼ N (0  σ2). Given Y   the task is to infer the
unknown component signals Xi. This is essentially the single-channel BSS problem  for which
there is no unique solution. It can also be useful to add an extra component U = (U1  U2 ···   UT )
to model the unknown appliances to make the model more robust as proposed in [15]. The prior
of Ut is deﬁned as p(U) =
. The model then has a new
i=1 Xit + Ut + t. A natural way to represent this model is as an additive factorial
hidden Markov model (AFHMM) where the appliances are treated as HMMs [15  20  26]; this is
now described.

t=1 |Ut+1 − Ut|(cid:111)
(cid:80)T−1

form Yt = (cid:80)I

(cid:110)− 1

v2(T −1) exp

1

2v2

5.1 The additive factorial hidden Markov model

HMM  the initial probabilities are πik = P (Zi1 = k) (k = 1  2 ···   Ki) where(cid:80)Ki

In the AFHMM  each component signal Xi is represented by a HMM. We suppose there are Ki
states for each Xit  and so the state variable is denoted by Zit ∈ {1  2 ···   Ki}. Since Xi is a
k=1 πik = 1;
the mean values are µi = {µ1  µ2 ···   µKi} such that Xit ∈ µi; the transition probabilities are
P (i) = (p(i)
jk = 1. We denote all these
parameters {πi  µi  P (i)} by θ. We assume they are known and can be learned from the training
data. Instead of using Z  we could use a binary vector Sit = (Sit1  Sit2 ···   SitKi)T to represent
the variable Z such that Sitk = 1 when Zit = k and for all Sitj = 0 when j (cid:54)= k. Then we are
interested in inferring the states Sit instead of inferring Xit directly  since Xit = ST
it µi. Therefore

jk = P (Zit = j|Zi t−1 = k) and(cid:80)Ki

jk ) where p(i)

j=1 p(i)

4

we want to make inference over the posterior distribution

(cid:81)Ki

the states P (S|θ) ∝ (cid:81)I
(cid:17)2(cid:27)
(cid:16)
Yt −(cid:80)I

(cid:80)T

(cid:26)

P (S  U  σ2|Y  θ) ∝ p(Y |S  U  σ2)P (S|θ)p(U)p(σ2)

(cid:81)I

where the HMM deﬁnes

(cid:17)SitkSi t−1 j   the inverse noise variance is assumed to be a Gamma dis-
(cid:81)T
tribution p(σ−2) ∝ (σ−2)α−1 exp(cid:8)−βσ−2(cid:9)  and the data likelihood has the Gaussian form

the prior of

k=1 πSi1k

(cid:81)

(cid:16)

p(i)
kj

×

t=2

i=1

ik

i=1

k j

2 exp

− 1

p(Y |S  U  σ2  θ) = |2πσ2|− T
. To make the MAP
inference over S  we relax the binary variable Sitk to be continuous in the range [0  1] as in [15  26].
It has been shown that incorporating domain knowledge into AFHMM can help to reduce the iden-
tiﬁability problem [15  20  26]. The domain knowledge we will incorporate using LBM is the
summary statistics.

it µi − Ut

i=1 ST

2σ2

t=1

5.2 Population modelling of summary statistics

t=1

t=1 ST

(cid:80)Ki

(cid:80)Ki

In energy disaggregation  it is useful to provide a summaries of energy consumption to the users.
For example  it would be useful to show the householders the total energy they had consumed in
one day for their appliances  the duration that each appliance was in use  and the number of times
that they had used these appliances. Since there already exists data about typical usage of different
appliances [4]  we can employ these data to model the distributions of those summary statistics.
We denote those desired statistics by τ = {τi}I
i=1  where i denotes the appliances. For appliance
i  we assume we have measured some time series from different houses for many days. This is
always possible because we can collect them from public data sets  e.g.  the data reviewed in [4].
We can then empirically obtain the distributions of those statistics. The distribution is represented by
pm(τim|Γim  ηim) where Γim represents the empirical quantities of the statistic m of the appliance
i which can be obtained from data and ηim are the latent variables which might not be known. Since
ηim are variables  we can employ a prior distribution p(ηim).
We now give some examples of those statistics. Total energy consumption: The total energy
consumption of an appliance can be represented as a function of the states of HMM such that τi =
it µi. Duration of appliance usage: The duration of using the appliance i can also be
k=2 Sitk where ∆t represents the sampling
duration for a data point of the appliances  and we assume that Sit1 represents the off state which
means the appliance was turned off. Number of cycles: The number of cycles (the number of times
an appliance is used) can be counted by computing the number of alterations from OFF state to ON

(cid:80)T
represented as a function of states τi = ∆t(cid:80)T
such that τi =(cid:80)T
1 means that the appliance i had been used c cycles  and(cid:80)Ci
data. One approach is to assume a Gaussian mixture density such that p(τi|ξi) = (cid:80)Ci
1)pc(τi|Γi)  where(cid:80)Ci
would be a linear regression model such that τi = (cid:80)Ci
When τi represents the number of cycles for appliance i  we can use τi = (cid:80)Ci
employ a noise model such that τi =(cid:80)Ci
discrete distribution such that P (ξi) =(cid:81)Ci

Let the binary vector ξi = (ξi1  ξi2 ···   ξic ···   ξiCi) represent the number of cycles  where ξic =
c=1 ξic = 1. (Note ξi is an example of ηi
in this case.) To model these statistics in our LBM framework  the latent variable that we use is the
number of cycles ξ. The distributions of τi could be empirically modelled by using the observation
c=1 p(ξic =
c=1 p(ξic = 1) = 1 and pc is the Gaussian component density. Using the
mixture Gaussian  we basically assume that  for an appliance  given the number of cycles the total
energy consumption is modelled by a Gaussian with mean µic and variance σ2
ic. A simpler model
i ). This
model assumes that given the number of cycles the total energy consumption is close to the mean
µic. The mixture model is more appropriate than the regression model  but the inference is more
difﬁcult.

c=1 cicξic where cic
represents the number of cycles. When the state variables Si are relaxed to [0  1]  we can then
i ). We model ξi with a
ic where pic represents the prior probability of the
number of cycles for the appliance i  which can be obtained from the training data. We now show
that how to use the LBM to integrate the AFHMM with these population distributions.

c=1 cicξic + i where  ∼ N (0  σ2
c=1 pξic

c=1 ξicµic + i where i ∼ N (0  σ2

k=2 I(Sitk = 1  Si t−1 1 = 0).

t=2

5

6 The latent Bayesian melding approach to energy disaggregation

over S and ξ such that (cid:101)pS ξ(S  ξ) = cαpS(S)

We have shown that the summary statistics τ can be represented as a deterministic function of the
state variable of HMMs S such that τ = f(S)  which means that the τ itself can be represented as
a latent variable model. We could then straightforwardly employ the LBM to produce a joint prior
. Since in our model f is not
p∗
τ (f (S))
invertible  we need to generate a proper density for p∗
τ . One possible way is to generate N random
samples {S(n)}N
τ can be modelled by using
kernel density estimation. However  this will make the inference difﬁcult. Instead  we employ a
Gaussian density p∗
n=1.
The new posterior distribution of LBM thus has the form

n=1 from the prior pS(S) which is a HMM  and then p∗

im are computed from {S(n)}N

(cid:16) pτ (f (S)|ξ)p(ξ)

(τim) = N (ˆµim  ˆσ2

im) where ˆµim and ˆσ2

(cid:17)1−α

τim

p(S  U  Σ|Y  θ) ∝ p(Σ)p(U)(cid:101)pS ξ(S  ξ)p(Y |S  U  σ2)
(cid:18) pτ (f(S)|ξ)p(ξ)

(cid:19)1−α

= p(Σ)p(U)cαpS(S)

p∗
τ (f(S))

p(Y |S  U  σ2)

where Σ represents the collection of all the noise variances. All the inverse noise variances employ
the Gamma distribution as the prior. We are interested in inferring the MAP values. Since the vari-
ables S and ξ are binary  we have to solve a combinatorial optimization problem which is intractable 
so we solve a relaxed problem as in [15  26]. Since log pS(S) is not convex  we employ the relax-
ation method of [15]. So a new Ki×Ki variable matrix H it = (hit
jk = 1
jk = 0. Under these constraints  we then obtain
when Si t−1 k = 1 and Sitj = 1 and otherwise hit
(cid:111)
jk log p(i)
jk ; this is now linear. We optimize
i t k j hit
the log-posterior which is denoted by L(S  H  U  Σ  ξ). The constraints for those variables are repre-
(cid:111)
sented as sets QS =
  Qξ =
k=1 Sitk = 1  Sitk ∈ [0  1] ∀i  t
c=1 ξic = 1  ξic ∈ [0  1] ∀i
 
QH S
jk ∈ [0  1] ∀i  t

log pS(S) = log p(S  H) =(cid:80)I
(cid:110)(cid:80)Ki
(cid:111)
(cid:110)(cid:80)Ki
im ∀i  m(cid:9). Denote Q = QS ∪ Qξ ∪ QH S ∪ QU Σ. The relaxed
(cid:8)U ≥ 0  Σ ≥ 0  σ2

i1 log πi +(cid:80)
i t−1 (cid:80)Ki

jk) is introduced such that hit

(cid:110)(cid:80)Ci

and QU Σ

.l = Sit  hit

im < ˆσ2

l. = ST

l=1 H it

l=1 H it

i=1 ST

=

 

=

optimization problem is then

maximize
S H U Σ ξ

L(S  H  U  Σ  ξ)

subject to Q.

We oberved that every term in L is either quadratic or linear when Σ are ﬁxed  and the solutions
for Σ are deterministic when the other variables are ﬁxed. The constraints are all linear. Therefore 
we optimize Σ while ﬁxing all the other variables  and then optimize all the other variables simulta-
neously while ﬁxing Σ. This optimization problem is then a convex quadratic program (CQP)  for
which we use MOSEK [2]. We denote this method by AFHMM+LBM.

7 Experimental results

We have incorporated population information into the AFHMM by employing the latent Bayesian
melding approach. In this section  we apply the proposed model to the disaggregation problem. We
will compare the new approach with the AFHMM+PR [26] using the set of statistics τ described
in Section 5.2. The key difference between our method AFHMM+LBM and AFHMM+PR is that
AFHMM+LBM models the statistics τ conditional on the number of cycles ξ.

7.1 The HES data

We apply AFHMM  AFHMM+PR and AFHMM+LBM to the Household Electricity Survey (HES)
data1. This data set was gathered in a recent study commissioned by the UK Department of Food and
Rural Affairs. The study monitored 251 households  selected to be representative of the population 
across England from May 2010 to July 2011 [27]. Individual appliances were monitored  and in
some households the overall electricity consumption was also monitored. The data were monitored

1The HES dataset and information on how the raw data was cleaned can be found from

https://www.gov.uk/government/publications/household-electricity-survey.

6

Table 1: Normalized disaggregation error (NDE)  signal aggregate error (SAE)  duration aggregate
error (DAE)  and cycle aggregate error (CAE) by AFHMM+PR and AFHMM+LBM on synthetic
mains in HES data.
METHODS
AFHMM

NDE

DAE

CAE

SAE

1.45± 0.88
0.87± 0.21
AFHMM+PR
AFHMM+LBM 0.89± 0.49

1.42± 0.39
0.86± 0.39
0.87± 0.37

1.56±0.23
0.83±0.53
0.76±0.32

1.41±0.31
1.57±0.66
0.79±0.35

TIME (S)
179.3±1.9
195.4±3.2
198.1±3.1

Table 2: Normalized disaggregation error (NDE)  signal aggregate error (SAE)  duration aggregate
error (DAE)  and cycle aggregate error (CAE) by AFHMM+PR and AFHMM+LBM on mains in
HES data.

NDE

METHODS
AFHMM

1.90±1.16
0.91±0.11
AFHMM+PR
AFHMM+LBM 0.77±0.23

SAE

2.26±0.86
0.67± 0.07
0.68± 0.19

DAE

1.91±0.67
0.68± 0.18
0.61± 0.22

CAE

1.12 ±0.17
1.65 ±0.49
0.98±0.32

TIME (S)
170.8±33.3
214.2±38.1
224.8±34.8

every 2 or 10 minutes for different houses. We used only the 2-minute data. We then used the
individual appliances to train the model parameters θ of the AFHMM  which will be used as the
input to the models for disaggregation. Note that we assumed the HMMs have 3 states for all the
appliances. This number of states is widely applied in energy disaggregation problems  though our
method could easily be applied to larger state spaces. In the HES data  in some houses the overall
electricity consumption (the mains) was monitored. However  in most houses  only a subset of
individual appliances were monitored  and the total electricity readings were not recorded.
Generating the population information: Most of the houses in HES did not monitor the mains
readings. They all recorded the individual appliances consumption. We used a subset of the houses
to generate the population information of the individual appliances. We used the population infor-
mation of total energy consumption  duration of appliance usage and the number of cycles in a time
period. In our experiments  the time period was one day. We modelled the distributions of these
summary statistics by using the methods described in the Section 5.2  where the distributions were
Gaussian. All the required quantities for modelling these distributions were generated by using the
samples of the individual appliances.
Houses without mains readings: In this experiment  we randomly selected one hundred house-
holds  and one day’s usage was used as test data for each household. Since no mains readings were
monitored in these houses  we added up the appliance readings to generate synthetic mains read-
ings. We then applied the AFHMM  AFHMM+PR and AFHMM+LBM to these synthetic mains to
predict the individual appliance usage. To compare these three methods  we employed four error
measures. Denote ˆxi as the inferred signal for the appliance usage xi. One measure is the normal-
. This measures how well the method predicts the
ized disaggregation error (NDE):
energy consumption at every time point. However  the householders might be more interested in the
summaries of the appliance usage. For example  in a particular time period  e.g  one day  people
are interested in the total energy consumption of the appliances  the total time they have been using
as the
those appliances and how many times they have used them. We thus employ 1
I
signal aggregate error (SAE)  the duration aggregate error (DAE) or the cycle aggregate error (CAE) 
where ri represents the total energy consumption  the duration or the number of cycles  respectively 
and ˆri represents the predicted summary statistics.
All the methods were applied to the synthetic data. Table 1 shows the overall error computed by
these methods. We see that both the methods using prior information improved over the base line
method AFHMM. The AFHMM+PR and AFHMM+LBM performed similarly in terms of NDE and
SAE  but AFHMM+LBM improved over AFHMM+PR in terms of DAE (8%) and CAE (50%).
Houses with mains readings: We also applied those methods to 6 houses which have mains read-
ings. We used 10 days data for each house  and the recorded mains readings were used as the input
to the models. All the methods were used to predict the appliance consumption. Table 2 shows the

P
P
it(xit−ˆxit)2

|ˆri−ri|P

(cid:80)I

it x2

it

i=1

i ri

7

Table 3: Normalized disaggregation error (NDE)  signal aggregate error (SAE)  duration aggregate
error (DAE)  and cycle aggregate error (CAE) by AFHMM+PR and AFHMM+LBM on UK-DALE
data.

NDE

METHODS
AFHMM

1.57±1.16
0.83±0.27
AFHMM+PR
AFHMM+LBM 0.84±0.25

SAE

1.99±0.52
0.82± 0.38
0.89± 0.38

DAE

2.81±0.79
1.68± 1.21
0.49± 0.33

CAE

1.37 ± 0.28
1.90 ±0.52
0.59±0.21

TIME (S)
118.6±23.1
120.4±25.3
123.1±25.8

error of each house and also the overall errors. This experiment is more realistic than the synthetic
mains readings  since the real mains readings were used as the input. We see that both the meth-
ods incorporating prior information have improved over the AFHMM in terms of NDE  SAE and
DAE. The AFHMM+PR and AFHMM+LBM have the similar results for SAE. AFHMM+LBM is
improved over AFHMM+PR for NDE (15%)  DAE (10%) and CAE (40%).

7.2 UK-DALE data

In the previous section we have trained the model using the HES data  and applied the models to
different houses of the same data set. A more realistic situation is to train the model in one data set 
and apply the model to a different data set  because it is unrealistic to expect to obtain appliance-
level data from every household on which the system will be deployed. In this section  we use the
HES data to train the model parameters of the AFHMM  and model the distribution of the summary
statistics. We then apply the models to the UK-DALE dataset [13]  which was also gathered from
UK households  to make the predictions. There are ﬁve houses in UK-DALE  and all of them have
mains readings and as well as the individual appliance readings. All the mains meters were sampled
every 6 seconds and some of them also sampled at a higher rate  details of the data and how to access
it can be found in [13]. We employ three of the houses for analysis in our experiments (houses 1  2
& 5 in the data). The other two houses were excluded because the correlation between the sum of
submeters and mains is very low  which suggests that there might be recording errors in the meters.
We selected 7 appliances for disaggregation  based on those that typically use the most energy. Since
the sample rate of the submeters in the HES data is 2 minutes  we downsampled the signal from 6
seconds to 2 minutes for the UK-DALE data. For each house  we randomly selected a month for
analysis. All the four methods were applied to the mains readings. For comparison purposes  we
computed the NDE  SAE  DAE and CAE errors of all three methods  averaged over 30 days. Table 3
shows the results. The results are consistent with the results of the HES data. Both the AFHMM+PR
and AFHMM+LBM improve over the basic AFHMM  except that AFHMM+PR did not improve the
CAE. As for HES testing data  AFHMM+PR and AFHMM+LBM have similar results on NDE and
SAE. And AFHMM+LBM again improved over AFHMM+PR in DAE (70%) and CAE (68%).
These results are consistent in suggesting that incorporating population information into the model
can help to reduce the identiﬁability problem in single-channel BSS problems.

8 Conclusions

We have proposed a latent Bayesian melding approach for incorporating population information
with latent variables into individual models  and have applied the approach to energy disaggregation
problems. The new approach has been evaluated by applying it to two real-world electricity data sets.
The latent Bayesian melding approach has been compared to the posterior regularization approach
(a case of the Bayesian melding approach) and AFHMM. Both the LBM and PR have signiﬁcantly
lower error than the base line method. LBM improves over PR in predicting the duration and the
number of cycles. Both methods were similar in NDE and the SAE errors.

Acknowledgments

This work is supported by the Engineering and Physical Sciences Research Council  UK (grant
numbers EP/K002732/1 and EP/M008223/1).

8

References

[1] Leontine Alkema  Adrian E Raftery  and Samuel J Clark. Probabilistic projections of HIV prevalence

using Bayesian melding. The Annals of Applied Statistics  pages 229–248  2007.

[2] MOSEK ApS. The MOSEK optimization toolbox for Python manual. Version 7.1 (Revision 28)  2015.
[3] Albert-Laszlo Barabasi and Reka Albert.

Emergence of scaling in random networks.

Science 

286(5439):509–512  1999.

[4] N. Batra et al. Nilmtk: An open source toolkit for non-intrusive load monitoring. In Proceedings of the

5th International Conference on Future Energy Systems  pages 265–276  New York  NY  USA  2014.

[5] Robert F. Bordley. A multiplicative formula for aggregating probability assessments. Management Sci-

ence  28(10):1137–1148  1982.

[6] Grace S Chiu and Joshua M Gould. Statistical inference for food webs with emphasis on ecological

networks via Bayesian melding. Environmetrics  21(7-8):728–740  2010.

[7] Luiz Max F de Carvalhoa  Daniel AM Villelaa  Flavio Coelhoc  and Leonardo S Bastosa. On the choice

of the weights for the logarithmic pooling of probability distributions. September 24  2015.

[8] E. Elhamifar and S. Sastry. Energy disaggregation via learning powerlets and sparse coding. In Proceed-

ings of the Twenty-Ninth Conference on Artiﬁcial Intelligence (AAAI)  pages 629–635  2015.

[9] K. Ganchev  J. Grac¸a  J. Gillenwater  and B. Taskar. Posterior regularization for structured latent variable

models. Journal of Machine Learning Research  11:2001–2049  2010.

[10] A. Gifﬁn and A. Caticha. Updating probabilities with data and moments. The 27th Int. Workshop on

Bayesian Inference and Maximum Entropy Methods in Science and Engineering  NY  July 8-13 2007.

[11] G.W. Hart. Nonintrusive appliance load monitoring. Proceedings of the IEEE  80(12):1870 –1891  1992.
[12] Edwin T Jaynes. Information theory and statistical mechanics. Physical review  106(4):620  1957.
[13] Jack Kelly and William Knottenbelt. The UK-DALE dataset  domestic appliance-level electricity demand

and whole-house demand from ﬁve UK homes. 2(150007)  2015.

[14] H. Kim  M. Marwah  M. Arlitt  G. Lyon  and J. Han. Unsupervised disaggregation of low frequency

power measurements. In Proceedings of the SIAM Conference on Data Mining  pages 747–758  2011.

[15] J. Z. Kolter and T. Jaakkola. Approximate inference in additive factorial HMMs with application to energy

disaggregation. In Proceedings of AISTATS  volume 22  pages 1472–1482  2012.

[16] P. Liang  M.I. Jordan  and D. Klein. Learning from measurements in exponential families. In The 26th

Annual International Conference on Machine Learning  pages 641–648  2009.

[17] James G MacKinnon and Anthony A Smith. Approximate bias correction in econometrics. Journal of

Econometrics  85(2):205–230  1998.

[18] G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of conditional

random ﬁelds. In Proceedings of ACL  pages 870–878  Columbus  Ohio  June 2008.

[19] Keith Myerscough  Jason Frank  and Benedict Leimkuhler. Least-biased correction of extended dynami-

cal systems using observational data. arXiv preprint arXiv:1411.6011  2014.

[20] O. Parson  S. Ghosh  M. Weal  and A. Rogers. Non-intrusive load monitoring using prior models of

general appliance types. In Proceedings of AAAI  pages 356–362  July 2012.

[21] David Poole and Adrian E. Raftery. Inference for deterministic simulation models: The Bayesian melding

approach. Journal of the American Statistical Association  pages 1244–1255  2000.

[22] MJ Rufo  J Mart´ın  CJ P´erez  et al. Log-linear pool to combine prior distributions: A suggestion for a

calibration-based approach. Bayesian Analysis  7(2):411–438  2012.

[23] H. ˇSevˇc´ıkov´a  A. Raftery  and P. Waddell. Uncertain beneﬁts: Application of Bayesian melding to the
Alaskan way viaduct in Seattle. Transportation Research Part A: Policy and Practice  45:540–553  2011.
[24] Robert L Wolpert. Comment on “Inference from a deterministic population dynamics model for bowhead

whales”. Journal of the American Statistical Association  90(430):426–427  1995.

[25] M. Wytock and J. Zico Kolter. Contextually supervised source separation with application to energy

disaggregation. In Proceedings of AAAI  pages 486–492  2014.

[26] M. Zhong  N. Goddard  and C. Sutton. Signal aggregate constraints in additive factorial HMMs  with

application to energy disaggregation. In NIPS  pages 3590–3598  2014.

[27] J.-P. Zimmermann  M. Evans  J. Griggs  N. King  L. Harding  P. Roberts  and C. Evans. Household

electricity survey  2012.

9

,Mingjun Zhong
Charles Sutton
Navdeep Jaitly
Quoc Le
Oriol Vinyals
Ilya Sutskever
David Sussillo
Samy Bengio
Yunus Saatci
Andrew Wilson
Kevin Bello
Jean Honorio