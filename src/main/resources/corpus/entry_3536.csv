2017,Monte-Carlo Tree Search by Best Arm Identification,Recent advances in bandit tools and techniques for sequential learning are steadily enabling new applications and are promising the resolution of a range of challenging related problems. We study the game tree search problem  where the goal is to quickly identify the optimal move in a given game tree by sequentially sampling its stochastic payoffs. We develop new algorithms for trees of arbitrary depth  that operate by summarizing all deeper levels of the tree into confidence intervals at depth one  and applying a best arm identification procedure at the root. We prove new sample complexity guarantees with a refined dependence on the problem instance. We show experimentally that our algorithms outperform existing elimination-based algorithms and match  previous special-purpose methods for depth-two trees.,Monte-Carlo Tree Search by Best Arm Identiﬁcation

CNRS & Univ. Lille  UMR 9189 (CRIStAL)  Inria SequeL

Emilie Kaufmann

Lille  France

emilie.kaufmann@univ-lille1.fr

Wouter M. Koolen

Centrum Wiskunde & Informatica 

Science Park 123  1098 XG Amsterdam  The Netherlands

wmkoolen@cwi.nl

Abstract

Recent advances in bandit tools and techniques for sequential learning are steadily
enabling new applications and are promising the resolution of a range of challeng-
ing related problems. We study the game tree search problem  where the goal is to
quickly identify the optimal move in a given game tree by sequentially sampling its
stochastic payoffs. We develop new algorithms for trees of arbitrary depth  that op-
erate by summarizing all deeper levels of the tree into conﬁdence intervals at depth
one  and applying a best arm identiﬁcation procedure at the root. We prove new
sample complexity guarantees with a reﬁned dependence on the problem instance.
We show experimentally that our algorithms outperform existing elimination-based
algorithms and match previous special-purpose methods for depth-two trees.

1

Introduction

sive moves is represented by a maximin game treeT . This tree models the possible actions sequences

We consider two-player zero-sum turn-based interactions  in which the sequence of possible succes-

by a collection of MAX nodes  that correspond to states in the game in which player A should take
action  MIN nodes  for states in the game in which player B should take action  and leaves which
specify the payoff for player A. The goal is to determine the best action at the root for player A. For
deterministic payoffs this search problem is primarily algorithmic  with several powerful pruning
strategies available [20]. We look at problems with stochastic payoffs  which in addition present a
major statistical challenge.
Sequential identiﬁcation questions in game trees with stochastic payoffs arise naturally as robust
versions of bandit problems. They are also a core component of Monte Carlo tree search (MCTS)
approaches for solving intractably large deterministic tree search problems  where an entire sub-tree
is represented by a stochastic leaf in which randomized play-out and/or evaluations are performed [4].
A play-out consists in ﬁnishing the game with some simple  typically random  policy and observing
the outcome for player A.
For example  MCTS is used within the AlphaGo system [21]  and the evaluation of a leaf position
combines supervised learning and (smart) play-outs. While MCTS algorithms for Go have now
reached expert human level  such algorithms remain very costly  in that many (expensive) leaf
evaluations or play-outs are necessary to output the next action to be taken by the player. In this
paper  we focus on the sample complexity of Monte-Carlo Tree Search methods  about which very
little is known. For this purpose  we work under a simpliﬁed model for MCTS already studied by
[22]  and that generalizes the depth-two framework of [10].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1.1 A simple model for Monte-Carlo Tree Search

play-out performed when this leaf is reached by an MCTS algorithm. In this model  we do not try

We start by ﬁxing a game treeT   in which the root is a MAX node. LettingL be the set of leaves
of this tree  for each (cid:96)∈L we introduce a stochastic oracleO(cid:96) that represents the leaf evaluation or
to optimize the evaluation or play-out strategy  but we rather assume that the oracleO(cid:96) produces
O(cid:96) is a Bernoulli distribution with unknown mean µ(cid:96) (the probability of player A winning the game
distribution bounded in[0  1].
For each node s in the tree  we denote byC(s) the set of its children and byP(s) its parent. The root
is denoted by s0. The value (for player A) of any node s is recursively deﬁned by V(cid:96)= µ(cid:96) if (cid:96)∈L and

i.i.d. samples from an unknown distribution whose mean µ(cid:96) is the value of the position (cid:96). To ease the
presentation  we focus on binary oracles (indicating the win or loss of a play-out)  in which the oracle

in the corresponding state). Our algorithms can be used without modiﬁcation in case the oracle is a

Vs= maxc∈C(s) Vc
minc∈C(s) Vc

if s is a MAX node 
if s is a MIN node.

∗= argmax
s∈C(s0) Vs.

s

The best move is the action at the root with highest value 

as [22  10]  in which the strategy also requires a stopping rule  after which leaves are no longer

To identify s∗ (or an -close move)  an MCTS algorithm sequentially selects paths in the game tree
and calls the corresponding leaf oracle. At round t  a leaf Lt∈L is chosen by this adaptive sampling
rule  after which a sample Xt∼OLt is collected. We consider here the same PAC learning framework
evaluated  and a recommendation rule that outputs upon stopping a guess ˆsτ ∈C(s0) for the best
Given a risk level δ and some accuracy parameter ≥ 0 our goal is have a recommendation ˆsτ∈C(s0)
whose value is within  of the value of the best move  with probability larger than 1− δ  that is
An algorithm satisfying this property is called(  δ)-correct. The main challenge is to design
(  δ)-correct algorithms that use as few leaf evaluations τ as possible.

P(V(s0)− V(ˆsτ)≤ )≥ 1− δ.

move of player A.

Related work The model we introduce for Monte-Carlo Tree Search is very reminiscent of a
stochastic bandit model. In those  an agent repeatedly selects one out of several probability distri-
butions  called arms  and draws a sample from the chosen distribution. Bandits models have been
studied since the 1930s [23]  mostly with a focus on regret minimization  where the agent aims to
maximize the sum of the samples collected  which are viewed as rewards [18]. In the context of
MCTS  a sample corresponds to a win or a loss in one play-out  and maximizing the number of
successful play-outs (that correspond to simulated games) may be at odds with identifying quickly
the next best action to take at the root. In that  our best action identiﬁcation problem is closer to a
so-called Best Arm Identiﬁcation (BAI) problem.
The goal in the standard BAI problem is to ﬁnd quickly and accurately the arm with highest mean.
The BAI problem in the ﬁxed-conﬁdence setting [7] is the special case of our simple model for a tree
of depth one. For deeper trees  rather than ﬁnding the best arm (i.e. leaf)  we are interested in ﬁnding
the best action at the root. As the best root action is a function of the means of all leaves  this is a
more structured problem.
Bandit algorithms  and more recently BAI algorithms have been successfully adapted to tree search.
Building on the UCB algorithm [2]  a regret minimizing algorithm  variants of the UCT algorithm
[17] have been used for MCTS in growing trees  leading to successful AIs for games. However  there
are only very weak theoretical guarantees for UCT. Moreover  observing that maximizing the number
of successful play-outs is not the target  recent work rather tried to leverage tools from the BAI
literature. In [19  6] Sequential Halving [14] is used for exploring game trees. The latter algorithm is
a state-of-the-art algorithm for the ﬁxed-budget BAI problem [1]  in which the goal is to identify the
best arm with the smallest probability of error based on a given budget of draws. The proposed SHOT
(Sequential Halving applied tO Trees) algorithm [6] is compared empirically to the UCT approach
of [17]  showing improvements in some cases. A hybrid approach mixing SHOT and UCT is also
studied [19]  still without sample complexity guarantees.

2

In the ﬁxed-conﬁdence setting  [22] develop the ﬁrst sample complexity guarantees in the model we
consider. The proposed algorithm  FindTopWinner is based on uniform sampling and eliminations 
an approach that may be related to the Successive Eliminations algorithm [7] for ﬁxed-conﬁdence
BAI in bandit models. FindTopWinner proceeds in rounds  in which the leaves that have not been
eliminated are sampled repeatedly until the precision of their estimates doubled. Then the tree is
pruned of every node whose estimated value differs signiﬁcantly from the estimated value of its
parent  which leads to the possible elimination of several leaves. For depth-two trees  [10] propose
an elimination procedure that is not round-based. In this simpler setting  an algorithm that exploits
conﬁdence intervals is also developed  inspired by the LUCB algorithm for ﬁxed-conﬁdence BAI
[13]. Some variants of the proposed M-LUCB algorithm appear to perform better in simulations than
elimination based algorithms. We now investigate this trend further in deeper trees  both in theory
and in practice.

Our Contribution.
In this paper  we propose a generic architecture  called BAI-MCTS  that builds
on a Best Arm Identiﬁcation (BAI) algorithm and on conﬁdence intervals on the node values in order
to solve the best action identiﬁcation problem in a tree of arbitrary depth. In particular  we study two
speciﬁc instances  UGapE-MCTS and LUCB-MCTS  that rely on conﬁdence-based BAI algorithms
[8  13]. We prove that these are (  δ)-correct and give a high-probability upper bound on their
sample complexity. Both our theoretical and empirical results improve over the elimination-based
state-of-the-art algorithm  FindTopWinner [22].

2 BAI-MCTS algorithms

We present a generic class of algorithms  called BAI-MCTS  that combines a BAI algorithm with
an exploration of the tree based on conﬁdence intervals on the node values. Before introducing the
algorithm and two particular instances  we ﬁrst explain how to build such conﬁdence intervals  and
also introduce the central notion of representative child and representative leaf.

2.1 Conﬁdence intervals and representative nodes

I(cid:96)(t)=[L(cid:96)(t)  U(cid:96)(t)] 

For each leaf (cid:96)∈L  using the past observations from this leaf we may build a conﬁdence interval
where U(cid:96)(t) (resp. L(cid:96)(t)) is an Upper Conﬁdence Bound (resp. a Lower Conﬁdence Bound) on the
value V((cid:96))= µ(cid:96). The speciﬁc conﬁdence interval we shall use will be discussed later.
For each internal node s  we recursively deﬁne Is(t) = [Ls(t)  Us(t)] with
Ls(t)= maxc∈C(s) Lc(t)
minc∈C(s) Lc(t)

These conﬁdence intervals are then propagated upwards in the tree using the following con-
struction.

Us(t)= maxc∈C(s) Uc(t)
minc∈C(s) Uc(t)

for a MAX node s 
for a MIN node s 

for a MAX node s 
for a MIN node s.
Note that these intervals are the tightest possible on the parent under the sole assumption that the
child conﬁdence intervals are all valid. A similar construction was used in the OMS algorithm of [3]
in a different context. It is easy to convince oneself (or prove by induction  see Appendix B.1) that
the accuracy of the conﬁdence intervals is preserved under this construction  as stated below.

Proposition 1. Let t∈ N. One has(cid:96)∈L(µ(cid:96)∈I(cid:96)(t)) ⇒ s∈T(Vs∈Is(t)).
We now deﬁne the representative child cs(t) of an internal node s as
and the representative leaf (cid:96)s(t) of a node s∈T   which is the leaf obtained when going down the

cs(t) =  argmaxc∈C(s) Uc(t)
argminc∈C(s) Lc(t)
(cid:96)s(t)= s if s∈L 

(cid:96)s(t)= (cid:96)cs(t)(t) otherwise.

tree by always selecting the representative child:

if s is a MAX node 
if s is a MIN node 

The conﬁdence intervals in the tree represent the statistically plausible values in each node  hence the
representative child can be interpreted as an “optimistic move” in a MAX node and a “pessimistic
move” in a MIN node (assuming we play against the best possible adversary). This is reminiscent of
the behavior of the UCT algorithm [17]. The construction of the conﬁdence intervals and associated
representative children are illustrated in Figure 1.

3

Input: a BAI algorithm

Initialization: t= 0.
while not BAIStop({s∈C(s0)}) do
Rt+1= BAIStep({s∈C(s0)})
Lt+1= (cid:96)Rt+1(t)
t= t+ 1.
Output: BAIReco({s∈C(s0)})

Sample the representative leaf

end

Update the information about the arms.

Figure 2: The BAI-MCTS architecture

(a) Children

(b) Parent

Figure 1: Construction of conﬁdence in-
terval and representative child (in red)
for a MAX node.

2.2 The BAI-MCTS architecture

In this section we present the generic BAI-MCTS algorithm  whose sampling rule combines two
ingredients: a best arm identiﬁcation step which selects an action at the root  followed by a conﬁdence
based exploration step  that goes down the tree starting from this depth-one node in order to select
the representative leaf for evaluation.
The structure of a BAI-MCTS algorithm is presented in Figure 2. The algorithm depends on a Best
Arm Identiﬁcation (BAI) algorithm  and uses the three components of this algorithm:

• the sampling rule BAIStep(S) selects an arm in the setS
• the stopping rule BAIStop(S) returns True if the algorithm decides to stop
• the recommendation rule BAIReco(S) selects an arm as a candidate for the best arm

In BAI-MCTS  the arms are the depth-one nodes  hence the information needed by the BAI algorithm
to make a decision (e.g. BAIStep for choosing an arm  or BAIStop for stopping) is information
about depth-one nodes  that has to be updated at the end of each round (last line in the while loop).
Different BAI algorithms may require different information  and we now present two instances that
rely on conﬁdence intervals (and empirical estimates) for the value of the depth-one nodes.

2.3 UGapE-MCTS and LUCB-MCTS

Several Best Arm Identiﬁcation algorithms may be used within BAI-MCTS  and we now present
two variants  that are respectively based on the UGapE [8] and the LUCB [13] algorithms. These
two algorithms are very similar in that they exploit conﬁdence intervals and use the same stopping
rule  however the LUCB algorithm additionally uses the empirical means of the arms  which within

BAI-MCTS requires deﬁning an estimate ˆVs(t) of the value of the depth-one nodes.
The generic structure of the two algorithms is similar. At round t+ 1 two promising depth-one nodes

are computed  that we denote by bt and ct. Among these two candidates  the node whose conﬁdence
interval is the largest (that is  the most uncertain node) is selected:

Rt+1= argmax

i∈{bt ct}[Ui(t)− Li(t)] .
(t)<   

of the two promising arms overlap by less than :

down the tree) is sampled: Lt+1= (cid:96)Rt+1(t). The algorithm stops whenever the conﬁdence intervals
Then  following the BAI-MCTS architecture  the representative leaf of Rt+1 (computed by going
and it recommends ˆsτ= bτ .

τ= inft∈ N∶ Uct

(t)− Lbt

In both algorithms that we detail below bt represents a guess for the best depth-one node  while ct is
an “optimistic” challenger  that has the maximal possible value among the other depth-one nodes.
Both nodes need to be explored enough in order to discover the best depth-one action quickly.

4

UGapE-MCTS.

In UGapE-MCTS  introducing for each depth-one node the index

the promising depth-one nodes are deﬁned as

Bs(t)= max
s′∈C(s0)(cid:131){s} Us′(t)− Ls(t) 
bt= argmin
a∈C(s0) Ba(t) and ct= argmax
bt= argmax
a∈C(s0) ˆVa(t) and ct= argmax

b∈C(s0)(cid:131){bt} Ub(t).
b∈C(s0)(cid:131){bt} Ub(t) 

LUCB-MCTS.

In LUCB-MCTS  the promising depth-one nodes are deﬁned as

where ˆVs(t)= ˆµ(cid:96)s(t)(t) is the empirical mean of the reprentative leaf of node s. Note that several
alternative deﬁnitions of ˆVs(t) may be proposed (such as the middle of the conﬁdence intervalIs(t) 
or maxa∈C(s) ˆVa(t))  but our choice is crucial for the analysis of LUCB-MCTS  given in Appendix C.

3 Analysis of UGapE-MCTS

In this section we ﬁrst prove that UGapE-MCTS and LUCB-MCTS are both (  δ)-correct. Then we
give in Theorem 3 a high-probability upper bound on the number of samples used by UGapE-MCTS.
A similar upper bound is obtained for LUCB-MCTS in Theorem 9  stated in Appendix C.

3.1 Choosing the Conﬁdence Intervals

the following lemma  whose proof can be found in Appendix B.2

From now on  we assume that the conﬁdence intervals on the leaves are of the form

and U(cid:96)(t)= ˆµ(cid:96)(t)+¿``(cid:192) β(N(cid:96)(t)  δ)
2N(cid:96)(t)

L(cid:96)(t) = ˆµ(cid:96)(t)−¿``(cid:192) β(N(cid:96)(t)  δ)
2N(cid:96)(t)
β(s  δ) is some exploration function  that can be tuned to have a δ-PAC algorithm  as expressed in
Lemma 2. If δ≤ max(0.1L  1)  for the choice
β(s  δ)= ln(L~δ)+ 3 ln ln(L~δ)+(3~2) ln(ln s+ 1)
both UGapE-MCTS and LUCB-MCTS satisfy P(V(s∗)− V(ˆsτ)≤ )≥ 1− δ.
number of draws N(cid:96)(t)  whereas most of the BAI algorithms use exploration functions that depend
Moreover  β(s  δ) scales with ln(ln(s))  and not ln(s)  leveraging some tools recently introduced to
obtain tighter conﬁdence intervals [12  15]. The union bound overL (that may be an artifact of our
and in practice  we recommend the use of β(s  δ)= ln(ln(es)~δ).

on the number of rounds t. Hence the only conﬁdence intervals that need to be updated at round t are
those of the ancestors of the selected leaf  which can be done recursively.

current analysis) however makes the exploration function of Lemma 2 still a bit over-conservative

An interesting practical feature of these conﬁdence intervals is that they only depend on the local

.

(1)

(2)

Finally  similar correctness results (with slightly larger exploration functions) may be obtained for
conﬁdence intervals based on the Kullback-Leibler divergence (see [5])  which are known to lead to
better performance in standard best arm identiﬁcation problems [16] and also depth-two tree search
problems [10]. However  the sample complexity analysis is much more intricate  hence we stick to
the above Hoeffding-based conﬁdence intervals for the next section.

3.2 Complexity term and sample complexity guarantees

We ﬁrst introduce some notation. Recall that s∗ is the optimal action at the root  identiﬁed with
the depth-one node satisfying V(s∗)= V(s0)  and deﬁne the second-best depth-one node as s∗
2=

5

argmaxs∈C(s0)(cid:131){s∗} Vs. RecallP(s) denotes the parent of a node s different from the root. Introducing
furthermore the set Anc(s) of all the ancestors of a node s  we deﬁne the complexity term by
∶= V(s∗)− V(s∗
2)
∶= maxs∈Anc((cid:96))(cid:131){s0}Vs− V(P(s))

  where ∆∗

(µ)∶=Q
∗
(cid:96)∈L

(cid:96)∨ ∆2∗∨ 2

(3)

∆2

∆(cid:96)

H

1

The intuition behind these squared terms in the denominator is the following. We will sample a leaf (cid:96)
until we either prune it (by determining that it or one of its ancestors is a bad move)  prune everyone
else (this happens for leaves below the optimal arm) or reach the required precision .

Theorem 3. Let δ≤ min(1  0.1L). UGapE-MCTS using the exploration function (2) is such that 
with probability larger than 1− δ (V(s∗)− V(ˆsτ)< ) and  letting ∆(cid:96) = ∆(cid:96)∨ ∆∗∨  
+ 1.

L
τ ≤ 8H
+Q
(µ) ln
∗
(µ)3 ln ln
+ 8H
∗

Remark 4. If β(Na(t)  δ) is changed to β(t  δ)  one can still prove(  δ) correctness and further-

+ 2 ln ln8e ln

+ 24e ln ln

more upper bound the expectation of τ. However the algorithm becomes less efﬁcient to implement 
since after each leaf observation  ALL the conﬁdence intervals have to be updated. In practice  this
change lowers the probability of error but does not effect signiﬁcantly the number of play-outs used.

L

L

L

16
2
(cid:96) 

1
2
(cid:96) 

ln ln

∆

∆

δ

δ

δ

δ

(cid:96)

3.3 Comparison with previous work

To the best of our knowledge1  the FindTopWinner algorithm [22] is the only algorithm from the
literature designed to solve the best action identiﬁcation problem in any-depth trees. The number of
play-outs of this algorithm is upper bounded with high probability by

ln

ln

2

δ

∆2
(cid:96)

∆(cid:96)δ

small [12]. The most interesting improvement is in the control of the number of draws of 2-optimal

and second best value. Moreover  unlike FindTopWinner and M-LUCB [10] in the depth two case 

One can ﬁrst note the improvement in the constant in front of the leading term in ln(1~δ)  as well as
the presence of the ln ln(1~∆(cid:96) 2) second order  that is unavoidable in a regime in which the gaps are
leaves (such that ∆(cid:96)≤ 2). In UGapE-MCTS  the number of draws of such leaves is at most of order
(∨ ∆2∗)−1 ln(1~δ)  which may be signiﬁcantly smaller than −1 ln(1~δ) if there is a gap in the best
UGapE-MCTS can also be used when = 0  with provable guarantees.
structure  ﬁrst computing the representative leaf for each depth-one node:∀s∈C(s0)  Rs t= (cid:96)s(t)
and then performing a BAI step over the representative leaves: ˜Lt+1= BAIStep(Rs t  s∈C(s0)).
global time β(t  δ) and that bt is the empirical maximin arm (which can be different from the arm

This alternative architecture can also be generalized to deeper trees  and was found to have empirical
performance similar to BAI-MCTS. M-LUCB  which will be used as a benchmark in Section 4  also
distinguish itself from LUCB-MCTS by the fact that it uses an exploration rate that depends on the

Regarding the algorithms themselves  one can note that M-LUCB  an extension of LUCB suited for
depth-two tree  does not belong to the class of BAI-MCTS algorithms. Indeed  it has a “reversed”

maximizing ˆVs). This alternative choice is not yet supported by theoretical guarantees in deeper trees.
Finally  the exploration step of BAI-MCTS algorithm bears some similarity with the UCT algorithm
[17]  as it goes down the tree choosing alternatively the move that yields the highest UCB or the
lowest LCB. However  the behavior of BAI-MCTS is very different at the root  where the ﬁrst move is
selected using a BAI algorithm. Another key difference is that BAI-MCTS relies on exact conﬁdence

1In a recent paper  [11] independently proposed the LUCBMinMax algorithm  that differs from UGapE-
MCTS and LUCB-MCTS only by the way the best guess bt is picked. The analysis is very similar to ours 
but features some reﬁned complexity measure  in which ∆(cid:96) (that is the maximal distance between consecutive
ancestors of the leaf  see (3)) is replaced by the maximal distance between any ancestors of that leaf. Similar
results could be obtained for our two algorithms following the same lines.

6

 32

Q
(cid:96)∶∆(cid:96)>2

16L

+ 1+ Q
(cid:96)∶∆(cid:96)≤2

 8

8L

+ 1

intervals: each intervalIs(t) is shown to contain with high probability the corresponding value Vs 

whereas UCT uses more heuristic conﬁdence intervals  based on the number of visits of the parent
node  and aggregating all the samples from descendant nodes. Using UCT in our setting is not obvious
as it would require to deﬁne a suitable stopping rule  hence we don’t include a comparison with this
algorithm in Section 4. A hybrid comparison between UCT and FindTopWinner is proposed in
[22]  providing UCT with the random number of samples used by the the ﬁxed-conﬁdence algorithm.
It is shown that FindTopWinner has the advantage for hard trees that require many samples. Our
experiments show that our algorithms in turn always dominate FindTopWinner.

∆2

2

∆2

7

3.4 Proof of Theorem 3.

using the following key result  which is proved in Appendix D.

An intuition behind this result is the following. First  using that the selected leaf (cid:96) is a representative

Another useful tool is the following lemma  that will allow to leverage the particular form of the

2 ln(1+ ln(s)) and deﬁne S= sup{s≥ 1∶ aβ(s)≥ s}. Then

LettingEt=(cid:96)∈L(µ(cid:96)∈I(cid:96)(t)) and E =t∈NEt  we upper bound τ assuming the eventE holds 
Lemma 5. Let t∈ N.Et∩(τ> t)∩(Lt+1= (cid:96)) ⇒ N(cid:96)(t)≤ 8β(N(cid:96)(t) δ)
(cid:96)∨∆2∗∨2 .
leaf  it can be seen that the conﬁdence intervals from sD= (cid:96) to s0 are nested (Lemma 11). Hence if
Et holds  V(sk)∈I(cid:96)(t) for all k= 1  . . .   D  which permits to lower bound the width of this interval
(and thus upper bound N(cid:96)(t)) as a function of the V(sk) (Lemma 12). Then Lemma 13 exploits the
mechanism of UGapE to further relate this width to ∆∗ and .
exploration function β to obtain an explicit upper bound on N(cid:96)(τ).
Lemma 6. Let β(s)= C+ 3
S≤ aC+ 2a ln(1+ ln(aC)).
This result is a consequence of Theorem 16 stated in Appendix F  that uses the fact that for C≥
− ln(0.1) and a≥ 8  it holds that
C(1+ ln(aC))
C(1+ ln(aC))− 3
On the eventE  letting τ(cid:96) be the last instant before τ at which the leaf (cid:96) has been played before
stopping  one has N(cid:96)(τ− 1)= N(cid:96)(τ(cid:96)) that satisﬁes by Lemma 5
N(cid:96)(τ(cid:96))≤ 8β(N(cid:96)(τ(cid:96))  δ)
(cid:96)∨ ∆2∗∨ 2
(cid:96)∨∆2∗∨2 and C = ln
+ 3 ln ln
Applying Lemma 6 with a= a(cid:96)=
L
L
N(cid:96)(τ− 1) ≤ a(cid:96)(C+ 2 ln(1+ ln(a(cid:96)C))) .
δ leads to
Letting ∆(cid:96) = ∆(cid:96)∨ ∆∗∨  and summing over arms  we ﬁnd
τ = 1+Q
ïïï8e
ïï
ïï
+ 3 ln ln
L
L
≤ 1+Q
L
+ 2 ln ln8e ln
= 1+Q
(µ)3 ln ln
∗
onE  V(s∗)− V(ˆsτ)<  and thatE holds with probability larger than 1− δ.

To conclude the proof  we remark that from the proof of Lemma 2 (see Appendix B.2) it follows that

≤ 1.7995564 ≤ 2.

+ 2 ln ln
ïï+ 8H

N(cid:96)(τ− 1)
ïïïln
L
ïïïln
L

8
2
(cid:96) 

8
2
(cid:96) 

∆

∆

δ

δ

+ 3 ln ln
+ 2 ln ln

+ 24e ln ln

1
2
(cid:96) 

∆

L

L

δ

L

δ

8

∆2

∆

2
(cid:96) 

ln

δ

δ

.

δ

(cid:96)

(cid:96)

(cid:96)

3
2

δ

 .

δ

4 Experimental Validation

L

δ

In this section we evaluate the performance of our algorithms in three experiments. We evaluate
on the depth-two benchmark tree from [10]  a new depth-three tree and the random tree ensemble
from [22]. We compare to the FindTopWinner algorithm from [22] in all experiments  and in the
depth-two experiment we include the M-LUCB algorithm from [10]. Its relation to BAI-MCTS is
discussed in Section 3.3. For our BAI-MCTS algorithms and for M-LUCB we use the exploration

rate β(s  δ)= ln
+ ln(ln(s)+ 1) (a stylized version of Lemma 2 that works well in practice)  and
supply all algorithms with δ= 0.1 and = 0.01. For comparing with [10] we run all algorithms with
= 0 and δ= 0.1L (undoing the conservative union bound over leaves. This excessive choice  which
might even exceed one  does not cause a problem  as the algorithms depend on δL= 0.1). In none of

we use the KL reﬁnement of the conﬁdence intervals (1). To replicate the experiment from [22]  we

our experiments the observed error rate exceeds 0.1.
Figure 3 shows the benchmark tree from [10  Section 5] and the performance of four algorithms on it.
We see that the special-purpose depth-two M-LUCB performs best  very closely followed by both
our new arbitrary-depth LUCB-MCTS and UGapE-MCTS methods. All three use signiﬁcantly fewer
samples than FindTopWinner. Figure 4 (displayed in Appendix A for the sake of readability) shows

a full 3-way tree of depth 3 with leafs drawn uniformly from[0  1]. Again our algorithms outperform
from[0  1] the average numbers of samples are: LUCB-MCTS 141811  UGapE-MCTS 142953 and
over leaves to all algorithms  which are run with = 0.01 and δ= 0.1. We did not observe any error

the previous state of the art by an order of magnitude. Finally  we replicate the experiment from
[22  Section 4]. To make the comparison as fair as possible  we use the proven exploration rate from
(2). On 10K full 10-ary trees of depth 3 with Bernoulli leaf parameters drawn uniformly at random

FindTopWinner 2254560. To closely follow the original experiment  we do apply the union bound

from any algorithm (even though we allow 10%). Our BAI-MCTS algorithms deliver an impressive
15-fold reduction in samples.

Figure 3: The 3× 3 tree of depth 2 that is the benchmark in [10]. Shown below the leaves are the
averages over 10K repetitions with = 0 and δ= 0.1⋅ 9.

average numbers of pulls for 4 algorithms: LUCB-MCTS (0.89% errors  2460 samples)  UGapE-
MCTS (0.94%  2419)  FindTopWinner (0%  17097) and M-LUCB (0.14%  2399). All counts are

5 Lower bounds and discussion

Given a treeT   a MCTS model is parameterized by the leaf values  µ∶=(µ(cid:96))(cid:96)∈L  which determine the
best root action: s∗= s∗(µ). For µ∈[0  1]L  We deﬁne Alt(µ)={λ∈[0  1]L∶ s∗(λ)≠ s∗(µ)}.

Using the same technique as [9] for the classic best arm identiﬁcation problem  one can establish the
following (non explicit) lower bound. The proof is given in Appendix E.

8

0.450.450.350.300.450.500.559058752941798199200293121281822498920.350.400.60629630293275228727929302481717418220.300.470.52197193114021012312373944202056621binary Kullback-Leibler divergence.

and

T

min

w1 aµ1 a+ wi 1µi 1
w1 a+ wi 1

λ∈Alt(µ)Q

(cid:96)∈L w(cid:96)d(µ(cid:96)  λ(cid:96))

inf

(4)

This result is however not directly amenable for comparison with our upper bounds  as the optimiza-

Eµ[τ]≥ T

∗(µ)d(δ  1− δ)  where T

our upper bounds have the right dependency in δ. For depth-two trees with K (resp. M) actions for
player A (resp. B)  we can moreover prove the following result  that suggests an intriguing behavior.

˜ΣK M∶={w∈ ΣK×M∶ wi j= 0 if i≥ 2 and j≥ 2}
w1 adµ1 a 
+wi 1dµi 1 
i=2 ... K
a=1 ... M

Theorem 7. Assume = 0. Any δ-correct algorithm satisﬁes
∗(µ)−1∶= sup
w∈ΣL
i=1 wi= 1} and d(x  y)= x ln(x~y)+(1− x) ln((1− x)~(1− y)) is the
with Σk={w∈[0  1]i∶∑k
tion problem deﬁned in Lemma 7 is not easy to solve. Note that d(δ  1− δ)≥ ln(1~(2.4δ)) [15]  thus
Lemma 8. Assume = 0 and consider a tree of depth two with µ=(µi j)1≤i≤K 1≤j≤M such that
∀(i  j)  µ1 1> µi 1  µi 1< µi j. The supremum in the deﬁnition of T∗(µ)−1 can be restricted to
w1 aµ1 a+ wi 1µi 1
∗(µ)−1= max
 .
w1 a+ wi 1
w∈ ˜ΣK M
It can be extracted from the proof of Theorem 7 (see Appendix E) that the vector w∗(µ) that attains
should draw many of the leaves much less than O(ln(1~δ)) times. This hints at the exciting prospect
of optimal stochastic pruning  at least in the asymptotic regime δ→ 0.
∗ = (0.3633  0.1057  0.0532) (0.3738  0  0) (0.1040  0  0).
With δ= 0.1 we ﬁnd kl(δ  1− δ)= 1.76 and the lower bound is Eµ[τ]≥ 456.9. We see that there is a
developed by [9]. It maintains the empirical proportions of draws close to w∗( ˆµ)  adding forced
exploration to ensure ˆµ→ µ. We believe that developing this line of ideas for MCTS would result in
pattern evolves for deeper trees  let alone how to compute w∗(µ).

a major advance in the quality of tree search algorithms. The main challenge is developing efﬁcient
solvers for the general optimization problem (4). For now  even the sparsity pattern revealed by
Lemma 8 for depth two does not give rise to efﬁcient solvers. We also do not know how this sparsity

As an example  we numerically solve the lower bound optimization problem (which is a concave
maximization problem) for µ corresponding to the benchmark tree displayed in Figure 3 to obtain

the supremum in (4) represents the average proportions of selections of leaves by any algorithm
matching the lower bound. Hence  the sparsity pattern of Lemma 8 suggests that matching algorithms

Future directions An (asymptotically) optimal algorithm for BAI called Track-and-Stop was

∗(µ)= 259.9

T

and

w

potential improvement of at least a factor 4.

Acknowledgments. Emilie Kaufmann acknowledges the support of the French Agence Nationale
de la Recherche (ANR)  under grant ANR-16-CE40-0002 (project BADASS). Wouter Koolen ac-
knowledges support from the Netherlands Organization for Scientiﬁc Research (NWO) under Veni
grant 639.021.439.

References
[1] J-Y. Audibert  S. Bubeck  and R. Munos. Best Arm Identiﬁcation in Multi-armed Bandits. In

Proceedings of the 23rd Conference on Learning Theory  2010.

[2] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2):235–256  2002.

[3] L. Borsoniu  R. Munos  and E. Páll. An analysis of optimistic  best-ﬁrst search for minimax

sequential decision making. In ADPRL14  2014.

[4] C. Browne  E. Powley  D. Whitehouse  S. Lucas  P. Cowling  P. Rohlfshagen  S. Tavener 
D. Perez  S. Samothrakis  and S. Colton. A survey of monte carlo tree search methods. IEEE
Transactions on Computational Intelligence and AI in games   4(1):1–49  2012.

9

[5] O. Cappé  A. Garivier  O-A. Maillard  R. Munos  and G. Stoltz. Kullback-Leibler upper
conﬁdence bounds for optimal sequential allocation. Annals of Statistics  41(3):1516–1541 
2013.

[6] T. Cazenave. Sequential halving applied to trees.

Intelligence and AI in Games  7(1):102–105  2015.

IEEE Transactions on Computational

[7] E. Even-Dar  S. Mannor  and Y. Mansour. Action Elimination and Stopping Conditions for
the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning
Research  7:1079–1105  2006.

[8] V. Gabillon  M. Ghavamzadeh  and A. Lazaric. Best Arm Identiﬁcation: A Uniﬁed Approach to
Fixed Budget and Fixed Conﬁdence. In Advances in Neural Information Processing Systems 
2012.

[9] A. Garivier and E. Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence.

Proceedings of the 29th Conference On Learning Theory (COLT)  2016.

In

[10] A. Garivier  E. Kaufmann  and W.M. Koolen. Maximin action identiﬁcation: A new bandit

framework for games. In Proceedings of the 29th Conference On Learning Theory  2016.

[11] Ruitong Huang  Mohammad M. Ajallooeian  Csaba Szepesvári  and Martin Müller. Structured
best arm identiﬁcation with ﬁxed conﬁdence. In 28th International Conference on Algorithmic
Learning Theory (ALT)  2017.

[12] K. Jamieson  M. Malloy  R. Nowak  and S. Bubeck. lil’UCB: an Optimal Exploration Algorithm
for Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory  2014.

[13] S. Kalyanakrishnan  A. Tewari  P. Auer  and P. Stone. PAC subset selection in stochastic

multi-armed bandits. In International Conference on Machine Learning (ICML)  2012.

[14] Z. Karnin  T. Koren  and O. Somekh. Almost optimal Exploration in multi-armed bandits. In

International Conference on Machine Learning (ICML)  2013.

[15] E. Kaufmann  O. Cappé  and A. Garivier. On the Complexity of Best Arm Identiﬁcation in

Multi-Armed Bandit Models. Journal of Machine Learning Research  17(1):1–42  2016.

[16] E. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In

Proceeding of the 26th Conference On Learning Theory.  2013.

[17] L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In Proceedings of the 17th
European Conference on Machine Learning  ECML’06  pages 282–293  Berlin  Heidelberg 
2006. Springer-Verlag.

[18] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied

Mathematics  6(1):4–22  1985.

[19] T. Pepels  T. Cazenave  M. Winands  and M. Lanctot. Minimizing simple and cumulative regret

in monte-carlo tree search. In Computer Games Workshop  ECAI  2014.

[20] Aske Plaat  Jonathan Schaeffer  Wim Pijls  and Arie de Bruin. Best-ﬁrst ﬁxed-depth minimax

algorithms. Artiﬁcial Intelligence  87(1):255 – 293  1996.

[21] David Silver  Aja Huang  Chris J. Maddison  Arthur Guez  Laurent Sifre  George van den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  Sander
Dieleman  Dominik Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy Lillicrap 
Madeleine Leach  Koray Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature  529:484–489  2016.

[22] K. Teraoka  K. Hatano  and E. Takimoto. Efﬁcient sampling method for monte carlo tree search

problem. IEICE Transactions on Infomation and Systems  pages 392–398  2014.

[23] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25:285–294  1933.

10

,Emilie Kaufmann
Wouter Koolen