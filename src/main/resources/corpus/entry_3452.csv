2013,Minimax Optimal Algorithms for Unconstrained Linear Optimization,We design and analyze minimax-optimal algorithms for online linear   optimization games where the player's choice is unconstrained.  The   player strives to minimize regret  the difference between his loss   and the loss of a post-hoc benchmark strategy.  The standard   benchmark is the loss of the best strategy chosen from a bounded   comparator set  whereas we consider a broad range of benchmark   functions. We consider the problem as a sequential multi-stage   zero-sum game  and we give a thorough analysis of the minimax   behavior of the game  providing characterizations for the value of   the game  as well as both the player's and the adversary's optimal   strategy.  We show how these objects can be computed efficiently   under certain circumstances  and by selecting an appropriate   benchmark  we construct a novel hedging strategy for an   unconstrained betting game.,Minimax Optimal Algorithms

for Unconstrained Linear Optimization

H. Brendan McMahan

Google Reasearch

Seattle  WA

mcmahan@google.com

Jacob Abernethy⇤

Computer Science and Engineering

University of Michigan

jabernet@umich.edu

Abstract

We design and analyze minimax-optimal algorithms for online linear optimization
games where the player’s choice is unconstrained. The player strives to minimize
regret  the difference between his loss and the loss of a post-hoc benchmark strat-
egy. While the standard benchmark is the loss of the best strategy chosen from a
bounded comparator set  we consider a very broad range of benchmark functions.
The problem is cast as a sequential multi-stage zero-sum game  and we give a
thorough analysis of the minimax behavior of the game  providing characteriza-
tions for the value of the game  as well as both the player’s and the adversary’s
optimal strategy. We show how these objects can be computed efﬁciently under
certain circumstances  and by selecting an appropriate benchmark  we construct a
novel hedging strategy for an unconstrained betting game.

1

Introduction

Minimax analysis has recently been shown to be a powerful tool for the construction of online
learning algorithms [Rakhlin et al.  2012]. Generally  these results use bounds on the value of
the game (often based on the sequential Rademacher complexity) in order to construct efﬁcient
algorithms. In this work  we show that when the learner is unconstrained  it is often possible to
efﬁciently compute an exact minimax strategy for both the player and nature. Moreover  with our
tools we can analyze a much broader range of problems than have been previously considered.
We consider a game where on each round t = 1  . . .   T   ﬁrst the learner selects xt 2 Rn  and then
an adversary chooses gt 2G⇢ Rn  and the learner suffers loss gt · xt. The goal of the learner is to
minimize regret  that is  loss in excess of that achieved by a post-hoc benchmark strategy. We deﬁne

TXt=1

Regret = Loss  (Benchmark Loss) =

gt · xt  L(g1  . . .   gT )

(1)

as the regret with respect to benchmark performance L (the L intended will be clear from context).
The standard deﬁnition of regret arises from the choice
g1:T · x = inf
x2Rn

L(g1  . . .   gT ) = inf
x2X

g1:T · x + I(x 2X ) 

(2)

where I(condition) is the indicator function: it returns 0 when condition holds  and returns
1 otherwise. The above choice of L represents the loss of the best ﬁxed point x in the bounded
convex set X . Throughout we shall write g1:t =Pt
s=1 gs for a sum of scalars or vectors. When L
depends only on the sum G ⌘ g1:T we write L(G).

⇤Work performed while the author was in the CIS Department at the University of Pennsylvania and funded

by a Simons Postdoctoral Fellowship

1

In the present work we shall consider a broad notion of regret in which  for example  L is deﬁned not
in terms of a “best in hindsight” comparator but instead in terms of a “penalized best in hindsight”
objective. Let be some penalty function  and consider

L(G) = min

x

G · x + ( x).

(3)

This is a direct generalization of the usual comparator notion which takes (x) = I(x 2X ).
We view this interaction as a sequential zero-sum game played over T rounds  where the player
strives to minimize Eq. (1)  and the adversary attempts to maximize it. We study the value of this
game  deﬁned as

gT 2G TXt=1

sup

gt · xt  L(g1  . . .   gT )! .

(4)

V T ⌘ inf
x12Rn

sup
g12G

. . .

inf
xT 2Rn

With this in mind  we can describe the primary contributions of the present paper:

1. We provide a characterization of the value of the game Eq. (4) in terms of the supremum
over the expected value of a function of a martingale difference sequence. This will be
made more explicit in Section 2.

2. We provide a method for computing the player’s minimax optimal (deterministic) strategy
in terms of a “discrete derivative.” Similarly  we show how to describe the adversary’s
optimal randomized strategy in terms of martingale differences.

3. For “coordinate-decomposable” games we give a natural and efﬁciently computable de-

scription of the value of the game and the player’s optimal strategy.

4. In Section 3  we consider several benchmark functions L  deﬁned in Eq. (3) via a penalty
function   which lead to interesting and surprising optimal algorithms; we also exactly
compute the values of these games. Figure 1 summarizes these applications. In particular 
we show that constant-step-size gradient descent is minimax optimal for a quadratic   and
an exponential L leads to a bounded-loss hedging algorithm that can still yield exponential
reward on “easy” sequences.

Applications The primary contributions of this paper are to the theory. Nevertheless  it is worth
pausing to emphasize that the framework of “unconstrained online optimization” is a fundamental
template for (and strongly motivated by) several online learning settings  and the results we develop
are applicable to a wide range of commonly studied algorithmic problems. The classic algorithm
for linear pattern recognition  the Perceptron  can be seen as an algorithm for unconstrained linear
optimization. Methods for training a linear SVM or a logistic regression model  such as stochastic
gradient descent or the Pegasos algorithm [Shalev-Shwartz et al.  2011]  are unconstrained opti-
mization algorithms. Finally  there has been recent work in the pricing of options and other ﬁnancial
derivatives [DeMarzo et al.  2006  Abernethy et al.  2012] that can be described exactly in terms of
a repeated game which ﬁts nicely into our framework.
We also wish to emphasize that the algorithm of Section 3.2 is both practical and easily imple-
mentable: for a multi-dimensional problem one needs to only track the sum of gradients for each
coordinate (similar to Dual Averaging)  and compute Eq. (12) for each coordinate to derive the
appropriate strategy. The algorithm provides us with a tool for making potentially unconstrained
bets/investments  but as we discuss it also leads to interesting regret bounds.

Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-
Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative
notions of regret is also not new. Vovk [2001] gives bounds relative to benchmarks similar to Eq. (3) 
though for different problems and not in the minimax setting. In the expert setting  there has been
much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen
et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online
convex optimization framework. This notion can be expressed by an appropriate L(g1  . . .   gT )  but
now the order of the gradients matters. Merhav et al. [2006] and Dekel et al. [2012] consider the
stronger notion of policy regret in the online experts and bandit settings  respectively. Stoltz [2011]
also considers some alternative notions of regret. For investing scenarios  Agarwal et al. [2006]

2

setting
soft feasible set

(

minimax value
T
2

update
xt+1 = 1

 g1:t

L(G) 
 G2
|G|

2



x)
2 x2
I(|x| 1)

standard regret

!q 2
bounded-loss betting  exp(G/pT ) pT x log(pT x) + pT x ! pe
Figure 1: Summary of speciﬁc online linear games considered in Section 3. Results are stated for
the one-dimensional problem where gt 2 [1  1]; Corollary 5 gives an extension to n dimensions.
The benchmark L is given as a function of G = g1:T . The standard notion of regret corresponds
to the L(G) = minx2[1 1] g1:t · x = |G|. The benchmark functions can alternatively be derived
from a suitable penalty on comparator points x  so L(G) = minx Gx + ( x).

Eq. (14)
Eq. (12)

⇡ T

and Hazan and Kale [2009] consider regret with respect to the best constant-rebalanced portfolio.
Our algorithm in Section 3.2 applies to similar problems  but does not require a “no junk bonds”
assumption  and is in fact minimax optimal for a natural benchmark.
Existing algorithms do offer bounds for unconstrained problems  generally of the form kx⇤k/⌘ +
⌘Pt gtxt. However  such bounds can only guarantee no-regret when an upper bound R on kx⇤k is
known in advance and used to tune the parameter ⌘. If one knows such a R  however  the problem
is no longer truly unconstrained. The only algorithms we know that avoid this problem are those of
Streeter and McMahan [2012]  and the minimax-optimal algorithm we introduce in Sec 3.2; these
algorithms guarantee guarantee Regret ORpT log((1 + R)T ) for any R > 0.

The ﬁeld has seen a number of minimax approaches to online learning. Abernethy and Warmuth
[2010] and Abernethy et al. [2008b] give the optimal behavior for several zero-sum games against
a budgeted adversary. Section 3.3 studies the online linear game of Abernethy et al. [2008a] under
different assumptions  and we adapt some techniques from Abernethy et al. [2009  2012]; the latter
work also involves analyzing an unconstrained player. Rakhlin et al. [2012] utilizes powerful tools
for non-constructive analysis of online learning as a technique to design algorithms; our work differs
in that we focus on cases where the exact minimax strategy can be computed.

Notions of Regret The standard notion of regret corresponds to a hard penalty (x) = I(x 2
X ). Such a deﬁnition makes sense when the player by deﬁnition must select a strategy from some
bounded set  for example a probability from the n-dimensional simplex  or a distribution on paths
in a graph. However  in contexts such as machine learning where any x 2 Rn corresponds to a valid
model  such a hard constraint is difﬁcult to justify; while any x 2 Rn is technically feasible  in order
to prove regret bounds we compare to a much more restrictive set. As an alternative  in Sections 3.1
and 3.2 we propose soft penalty functions that encode the belief that points near the origin are more
likely to be optimal (we can always re-center the problem to match our beliefs in this regard)  but do
not rule out any x 2 Rn a priori.
Thus  one of our contributions is showing that interesting results can be obtained by choosing L
differently than in Eq. (2). The player cannot do well in terms of the absolute lossPt gt · xt for

all sequences g1  . . .   gT   but she can do better on some sequences at the expense of doing worse on
others. The benchmark L makes this notion precise: sequences for which L(g1  . . .   gT ) is large and
negative are those on which the player desires good performance  at the expense of allowing more
loss (in absolute terms) on sequences where L(g1  . . .   gT ) is large and positive. The value of the
game V T tells us to what extent any online algorithm can hope to match the benchmark L.

2 General Unconstrained Linear Optimization

In this section we develop general results on the unconstrained linear optimization problem. We start
by analyzing (4) in greater detail  and give tools for computing the regret value V T in such games.
We show that in certain cases the computation of the minimax value can be greatly simpliﬁed.
Throughout we will assume that the function L is concave in each of its arguments (thought not
necessarily jointly concave) and bounded on GT . We also include the following assumptions on the

3

set G. First  we assume that either G is a polytope or  more generally  that ConvexHull(G) is a full-
rank polytope in Rn. This is not strictly necessary but is convenient for the analysis; any bounded
convex set in Rn can be approximated to arbitrary precision with a polytope. We also make the
necessary assumption that the ConvexHull(G) contains the origin in its interior. We let G0 be the set
of “corners” of G  that is G0 = {g1  . . .   gm} and hence ConvexHull(G) = ConvexHull(G0).
We are also concerned with the conditional value of the game  Vt  given x1  . . . xt and g1  . . . gt have
already been played. That is  the Regret when we ﬁx the plays on the ﬁrst t rounds  and then assume
minimax optimal play for rounds t+1 through T . However  following the approach of Rakhlin et al.
s=1 xs · gs from Eq. (4). We can view this as cost that the learner has
already payed  and neither that cost nor the speciﬁc previous plays of the learner impact the value of
the remaining terms in Eq. (1). Thus  we deﬁne

[2012]  we omit the termsPt

Vt(g1  . . .   gt) = inf

xt+12Rn

sup
gt+12G

. . .

inf
xT 2Rn

gT 2G TXs=t+1

sup

gs · xs  L(g1  . . .   gT )! .

(5)

Note the conditional value of the game before anything has been played  V0()  is exactly V T .

The martingale characterization of the game The fundamental tool used in the rest of the paper
is the following characterization of the conditional value of the game:
Theorem 1. For every t and every sequence g1  . . .   gt 2G   we can write the conditional value of
the game as

Vt(g1  . . .   gt) =

max

G2(G0) E[G]=0

E[Vt+1(g1  . . .   gt  G)] 

where (G0) is the set of random variables on G0. Moreover  for all t the function Vt is convex in
each of its coordinates and bounded.

All proofs omitted from the body of the paper can be found in the appendix or the extended version
of this paper.
Let MT (G) be the set of T -length martingale difference sequences on G0  that is the set of
all sequences of random variables (G1  . . .   GT )  with Gt taking values in G0  which satisfy
E[Gt|G1  . . .   Gt1] = 0 for all t = 1  . . .   T . Then  we immediately have the following:
Corollary 2. We can write

V T =

max

(G1 ... GT )2MT (G0)

E[L(G1  . . .   GT )] 

with the analogous expression holding for the conditional value of the game.

Characterization of optimal strategies The result above gives a nice expression for the value
of the game V T but unfortunately it does not lead directly to a strategy for the player. We now
dig a bit deeper and produce a characterization of the optimal player behavior. This is achieved by
analyzing a simple one-round zero-sum game. As before  we assume G is a bounded subset of Rn
whose convex hull is a polytope whose interior contains the the origin 0. Assume we are given some
convex function f deﬁned and bounded on all of ConvexHull(G). We consider the following:

V = inf
x2Rn

sup
g2G

x · g + f (g).

(6)

i=1 ↵igi = 0  such that V =Pn+1

Theorem 3. There exists a set of n + 1 distinct points {g1  . . .   gn+1}⇢G whose convex hull is of
full rank  and a distribution ~↵ 2 n+1 satisfyingPn+1
i=1 ↵if (gi).
Moreover  an optimal choice for the inﬁmum in (6) is the gradient of the unique linear interpolation
of the pairs {(g1 f (g1))  . . .   (gn+1 f (gn+1))}.
The theorem makes a useful point about determining the player’s optimal strategy for games of this
form. If the player can determine a full-rank set of “best responses” {g1  . . .   gn+1} to his optimal
x⇤  each of which should be a corner of the polytope G  then we know that x⇤ must be a “discrete
gradient” of the function f around 0. That is  if the size of G is small relative to the curvature of
f  then an approximation to rf (0) is the linear interpolation of f at a set of points around 0.
An optimal x⇤ will be exactly this interpolation.

4

This result also tells us how to analyze the general T -round game. We can express (5)  the condi-
tional value of the game Vt1  in recursive form as
sup
gt2G

Vt1(g1  . . .   gt1) = inf
xt2Rn

gt · xt + Vt(g1  . . .   gt1  gt).

(7)

Hence by setting f (gt) = Vt(g1  . . .   gt1  gt)  noting that the latter is convex in gt by Theorem 1 
we see we have an immediate use of Theorem 3.

i=1 Li(gi).

3 Minimax Optimal Algorithms for Coordinate-Decomposable Games
In this section  we consider games where G consists of axis-aligned constraints  and L decomposes
so L(g) = Pn
In order to solve such games  it is generally sufﬁcient to consider n
independent one-dimensional problems. We study such games ﬁrst:
Theorem 4. Consider the one-dimensional unconstrained game where the player selects xt 2 R
and the adversary chooses gt 2G = [1  1]  and L is concave in each of its arguments and bounded
on GT . Then  V T = Egt⇠{1 1}⇥  L(g1  . . .   gT )⇤ where the expectation is over each gt chosen
independently and uniformly from {1  1} (that is  the gt are Rademacher random variables). Fur-
ther  the conditional value of the game is
(8)

Vt(g1  . . .   gt) =

E

gt+1 ... gT ⇠{1 1}⇥  L(g1  . . .   gt  gt+1  . . . gT )⇤.

The proof is immediate from Corollary 2  since the only possible martingale that both plays from
the corners of G and has expectation 0 on each round is the sequence of independent Rademacher
random variables.1 Given Theorem 4  and the fact that the functions L of interest will generally
depend only on g1:T   it will be useful to deﬁne BT to be the distribution of g1:T when each gt is
drawn independently and uniformly from {1  1}.
Theorem 4 can immediately be extended to coordinate-decomposable games as follows:
Corollary 5. Consider the game where the player chooses xt 2 Rn  the adversary chooses gt 2
[1  1]n  and the payoff isPT
t=1 gt · xt Pn
i=1 L(g1:T i) for concave L. Then the value V T and
the conditional value Vt(·) can be written as
G⇠BT⇥  L(G)⇤

Gi⇠BTt⇥  L(g1:t i + Gi)⇤.

and Vt(g1  . . .   gt) =

V T = n E

nXi=1

E

The proof follows by noting the constraints on both players’ strategies and the value of the game
fully decompose on a per-coordinate basis.

A recipe for minimax optimal algorithms in one dimension Since Eq. (5) gives the minimax
value of the game if both players play optimally from round t + 1 forward  a minimax strategy for
the learner on round t + 1 must be xt+1 = arg minx2R maxg2{1 1} g · x + Vt+1(g1  . . .   gt  g).
Now  we can apply Theorem 3  and note that unique strategy for the adversary is to play g = 1
or g = 1 with equal probability. Thus  the player strategy is just the interpolation of the points
(1 f (1)) and (1 f (1))  where we take f = Vt+1  giving us

xt+1 =

1

2Vt+1(g1  . . .   gt 1)  Vt+1(g1  . . .   gt  +1).

Thus  if we can derive a closed form for Vt(g1  . . .   gt)  we will have an efﬁcient minimax-optimal
algorithm. Note that for any function L 

(9)

(10)

[L(G)] =

1
2T

E
G⇠BT

TXi=0✓T

i◆L(2i  T ) 

i is the binomial probability of getting exactly i gradients of +1 over T rounds  which
since 2TT
implies Ti gradients of 1  so G = i(Ti) = 2iT . Using Theorem 4  and Eqs (9) and (10)  in

1However  is easy to extend this to the case where G = [a  b]  which leads to different random variables.

5

the following sections we exactly compute the game values and unique minimax optimal strategies
for a variety of interesting coordinate-decomposable games. Even when such exact computations
are not possible  any coordinate-decomposable game where L depends only on G = g1:T can be
solved numerically in polynomial time. If ⌧ = T  t  the number of rounds remaining  then we can
compute Vt exactly by using the appropriate binomial probabilities (following Eq. (8) and Eq. (10)) 
requiring only a sum over O(⌧ ) values. If ⌧ is large enough  then using an approximation to the
binomial (e.g.  the Gaussian approximation) may be sufﬁcient.
We can also immediately provide a characterization of the potentially optimal player strategies in
terms of the subgradients of L. For simplicity  we write @L(g) instead of @(L(g)).
Theorem 6. Let G = [a  b]  with a < 0 < b  and L : R ! R is bounded and concave. Then  on
every round  the unique minimax optimal x⇤t satisﬁes x⇤t 2L where L = [w2R  @L(w).
Proof. Following Theorem 3  we know the minimax xt+1 interpolates (a f (a)) and (b f (b)) 
where we take f (g) = Vt+1(g1  . . .   gt  g). In one dimension  this implies xt+1 2 @f (g) for some
g 2G . It remains to show @f (g) ✓L . From Theorem 1 we have f (g) = E[L(g1:t + g + B)] 
where the E is with respect to mean-zero random variable B ⇠B ⌧   ⌧ = T  t. For each possible
value b that B can take on  @gL(g1:t +g +bi) ✓L by deﬁnition  so @f (g) is a convex combination
of these sets (e.g.  Rockafellar [1997  Thm. 23.8]). The result follows as L is convex.
Note that for standard regret  L(g) = inf x2X gx  we have @L(g) ✓X   indicating that (in 1 dimen-
sion at least)  the player never needs to play outside the comparator set X . We will see additional
consequences of this theorem in the following sections.

3.1 Constant step-size gradient descent can be minimax optimal

Suppose we use a “soft” feasible set for the benchmark via a quadratic penalty 

L(G) = min

x

Gx +


2

x2 = 

1
2

G2 

(11)

for a constant > 0. Does a no-regret algorithm against this comparison class exist? Unfortunately 
the general answer is no  as shown in the next theorem. Recalling gt 2 [1  1] 
Theorem 7. The value of this game is V T = EG⇠BTh 1

2 G2i = T

Thus  for a ﬁxed   we cannot have a no regret algorithm with respect to this L. But this does not
mean the minimax algorithm will be uninteresting. To derive the minimax optimal algorithm  we
compute conditional values (using similar techniques to Theorem 7) 

2 .

Vt(g1  . . .   gt) = E

G⇠BTth 1

2

(g1:t + G)2i =

1

2(g1:t)2 + (T  t) 

and so following Eq. (9) the minimax-optimal algorithm must use

xt+1 =

1

4(g1:t  1)2 + (T  t  1)  ((g1:t + 1)2 + (T  t  1)) =

Thus  a minimax-optimal algorithm is simply constant-learning-rate gradient descent with learning
rate 1
 . Note that for a ﬁxed   this is the optimal algorithm independent of T ; this is atypical  as
usually the minimax optimal algorithm depends on the horizon (as we will see in the next two cases).
Note that the set L = R (from Theorem 6)  and indeed the player could eventually play an arbitrary
point in R (given large enough T ).

1
4

(4g1:t) = 

1


g1:t

3.2 Non-stochastic betting with exponential upside and bounded worst-case loss

A major advantage of the regret minimization framework is that the guarantees we can achieve are
typically robust to arbitrary input sequences. But on the downside the model is very pessimistic: we
measure performance in the worst case. One might aim to perform not too badly in the worst case
yet extremely well under certain conditions.

6

We now show how the results in the present paper can lead to a very optimistic guarantee  particu-
larly in the case of a sequential betting game. On each round t  the world offers the player a betting
opportunity on a coin toss  i.e. a binary outcome gt 2 {1  1}. The player may take either side of
the bet  and selects a wager amount xt  where xt > 0 implies a bet on tails (gt = 1) and xt < 0 a
bet on heads (gt = 1). The world then announces whether the bet was won or lost  revealing gt. The
player’s wealth changes (additively) by gtxt (that is  the player strives to minimize loss gtxt). We
assume that the player begins with some initial capital ↵> 0  and at any time period the wager |xt|
must not exceed ↵ Pt1
With the beneﬁt of hindsight  the gambler can see G =PT
t=1 gt  the total number of heads minus the
total number of heads. Let us imagine that the number of heads signiﬁcantly exceeded the number of
tails  or vice versa; that is  |G| was much larger than 0. Without loss of generality let us assume that
G is positive. Let us imagine that the gambler  with the beneﬁt of hindsight  considers what could
have happened had he always bet a constant fraction  of his wealth on heads. A simple exercise
shows that his wealth would become

s=1 gsxs  the initial capital plus the money earned thus far.

TYt=1

(1 + gt) = (1 + )

T +G

2 (1  )

TG

2

.

This is optimized at  = G

maximum wealth in hindsight  exp⇣T · KL⇣ 1+G/T

2

T   which gives a simple expression in terms of KL-divergence for the

2⌘⌘  and the former is well-approximated
| | 1

by exp(O(G2/T )) when G is not too large relative to T . In other words  with knowledge of the ﬁnal
G  a na¨ıve betting strategy could have earned the gambler exponentially large winnings starting with
constant capital. Note that this is essentially a Kelly betting scheme [Kelly Jr  1956]  expressed in
terms of G. We ask: does there exist an adaptive betting strategy that can compete with this hindsight
benchmark  even if the gt are chosen fully adversarially?
Indeed we show we can get reasonably close. Our aim will be to compete with a slightly weaker

absolute value  so the player only aims for exponential wealth growth for large positive G. It is not
hard to develop a two-sided algorithm as a result  which we soon discuss.

benchmark L(G) =  exp(|G|/pT ). We present a solution for the one-sided game  without the
Theorem 8. Consider the game where G = [1  1] with benchmark L(G) =  exp(G/pT ). Then

with the bound tight as T ! 1. Let ⌧ = T  t and Gt = g1:t  then the conditional value of the
game is Vt(Gt) =⇣cosh 1pT⌘⌧

V T =⇣cosh 1pT⌘T
exp⇣ GtpT⌘ and the player’s minimax optimal strategy is:

 pe

xt+1 =  exp✓ GtpT◆ sinh 1pT ⇣cosh 1pT⌘⌧1

Recall that the value of the game can be thought of as the largest possible difference between the

payoff of the benchmark function exp(G/pT ) and the winnings of the player P gtxt  when the

player uses an optimal betting strategy. That the value of the game here is of constant order is
critical  since it says that we can always achieve a payoff that is exponential in GpT at a cost of no
more than pe = O(1). Notice we have said nothing thus far regarding the nature of our betting
strategy; in particular we have not proved that the strategy satisﬁes the required condition that the
gambler cannot bet more than ↵ plus the earnings thus far. We now give a general result showing
that this condition can be satisﬁed:
Theorem 9. Consider a one dimensional game with G = [1  1] with benchmark function L non-
positive on GT . Then for the optimal betting strategy we have that |xt|  Pt
s=1 gsxs + V T   and
further V T Pt
In other words  the player’s cumulative loss at any time is always bounded from below by V T . This
implies that the starting capital ↵ required to “replicate” the payoff function is exactly the value2 of
the game V T . Indeed  to replicate exp(G/pT ) we would require no more than ↵ = $1.65.

s=1 gsxs for any t and any sequence g1  . . .   gt.

(12)

2This idea has a long history in ﬁnance and was a key tool in Abernethy et al. [2012]  DeMarzo et al. [2006] 

and other works.

7

It is worth noting an alternative characterization of the benchmark function L used here. For a  0 
minx2R (Gx  ax log(ax) + ax) =  exp G
a. Thus  if we take (x) = ax log(ax) +
ax + I(x  0)  we have minx2R g1:T x + ( x) =  exp G
a . Since this algorithm needs large
Reward when G is large and positive  we might expect that the minimax optimal algorithm only
plays xt  0. Another intuition for this is that the algorithm should not need to play any point x to
which assigns an inﬁnite penalty. This intuition can be conﬁrmed immediately via Theorem 6.
We now sketch how to derive an algorithm for the “two-sided” game. To do this  we let LC(G) ⌘
L(G) + L(G)   exp(|G|/pT ). We can construct a minimax optimal algorithm for LC(G) by
running two copies of the one-sided minimax algorithm simultaneously  switching the signs of the
gradients and plays of the second copy. We formalize this in Appendix B.
This same benchmark and algorithm can be used in the setting introduced by Streeter and McMa-
han [2012].
In that work  the goal was to prove bounds on standard regret like Regret 
O(RpT log ((1 + R)T )) simultaneously for any comparator x⇤ with |x⇤| = R. Stating their The-
orem 1 in terms of losses  this traditional regret bound is achieved by any algorithm that guarantees

Loss =

TXt=1

gtxt   exp✓ |G|pT◆ + O(1).

(13)

The symmetric algorithm (Appendix B) satisﬁes

Loss   exp✓ G

pT◆  exp✓G

pT◆ + 2pe   exp✓ |G|pT◆ + 2pe 

and so we also achieve a standard regret bound of the form given above.

3.3 Optimal regret against hypercube adversaries

Perhaps the simplest and best studied learning games are those that restrict both the player and
adversary to a norm ball  and use the standard notion of regret. We can derive results for the game
where the adversary has an L1 constraint  the comparator set is also the L1 ball  and the player is
unconstrained. Corollary 5 implies it is sufﬁcient to study the one-dimensional case.
Theorem 10. Consider the game between an adversary who chooses losses gt 2 [1  1]  and a
player who chooses xt 2 R. For a given sequence of plays  x1  g1  x2  g2  . . .   xT   gT   the value to
the adversary isPT
t=1 gtxt | g1:T|. Then  when T is even with T = 2M  the minimax value of this
game is given by
Further  as T ! 1  VT !q 2T

⇡ . Let B be a random variable drawn from BTt. Then the minimax

optimal strategy for the player given the adversary has played Gt = g1:t is given by

(T  M )!M ! r 2T

VT = 2T

2M T !

.

⇡

xt+1 = Pr(B < Gt)  Pr(B > Gt) = 1  2 Pr(B > Gt) 2 [1  1].

(14)

The fact that the limiting value of this game isp2T /⇡ was previously known  e.g.  see a mention
in Abernethy et al. [2009]; however  we believe this explicit form for the optimal player strategy is
new. This strategy can be efﬁciently computed numerically  e.g  by using the regularized incomplete
beta function for the CDF of the binomial distribution. It also follows from this expression that even
though we allow the player to select xt+1 2 R  the minimax optimal algorithm always selects points
from [1  1]  so our result applies to the case where the player is constrained to play from X .
Abernethy et al. [2008a] shows that for the linear game with n  3 where both the learner and
adversary select vectors from the unit sphere  the minimax value is exactly pT . Interestingly  in the
n = 1 case (where L2 and L1 coincide)  the value of the game is lower  about 0.8pT rather than
pT . This indicates a fundamental difference in the geometry of the n = 1 space and n  3. We
conjecture the minimax value for the L2 game with n = 2 lies somewhere in between.

8

References
Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries. In NIPS 

2010.

Jacob Abernethy  Peter L. Bartlett  Alexander Rakhlin  and Ambuj Tewari. Optimal strategies and

minimax lower bounds for online convex games. In COLT  2008a.

Jacob Abernethy  Manfred K Warmuth  and Joel Yellin. Optimal strategies from random walks.
In Proceedings of The 21st Annual Conference on Learning Theory  pages 437–446. Citeseer 
2008b.

Jacob Abernethy  Alekh Agarwal  Peter Bartlett  and Alexander Rakhlin. A stochastic view of

optimal regret through minimax duality. In COLT  2009.

Jacob Abernethy  Rafael M. Frongillo  and Andre Wibisono. Minimax option pricing meets black-

scholes in the limit. In STOC  2012.

Amit Agarwal  Elad Hazan  Satyen Kale  and Robert E. Schapire. Algorithms for portfolio manage-

ment based on the Newton method. In ICML  2006.

Nicol`o Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge University

Press  2006.

A. de Moivre. The Doctrine of Chances: or  A Method of Calculating the Probabilities of Events in

Play. 1718.

Ofer Dekel  Ambuj Tewari  and Raman Arora. Online bandit learning against an adaptive adversary:

from regret to policy regret. In ICML  2012.

Peter DeMarzo  Ilan Kremer  and Yishay Mansour. Online trading algorithms and robust option
In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing 

pricing.
pages 477–486. ACM  2006.

Persi Diaconis and Sandy Zabell. Closed form summation for classical distributions: Variations on

a theme of de Moivre. Statistical Science  6(3)  1991.

Elad Hazan and Satyen Kale. On stochastic and worst-case models for investing. In NIPS. 2009.
J. L. Kelly Jr. A new interpretation of information rate. Bell System Technical Journal  1956.
Wouter Koolen  Dmitry Adamskiy  and Manfred Warmuth. Putting bayes to sleep. In NIPS. 2012.
N. Merhav  E. Ordentlich  G. Seroussi  and M. J. Weinberger. On sequential strategies for loss

functions with memory. IEEE Trans. Inf. Theor.  48(7)  September 2006.

Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Relax and randomize: From value to

algorithms. In NIPS  2012.

Ralph T. Rockafellar. Convex Analysis (Princeton Landmarks in Mathematics and Physics). Prince-

ton University Press  1997.

Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in

Machine Learning  2012.

Shai Shalev-Shwartz  Yoram Singer  Nathan Srebro  and Andrew Cotter. Pegasos: Primal estimated

sub-gradient solver for svm. Mathematical Programming  127(1):3–30  2011.

Gilles Stoltz. Contributions to the sequential prediction of arbitrary sequences: applications to the
theory of repeated games and empirical studies of the performance of the aggregation of experts.
Habilitation `a diriger des recherches  Universit´e Paris-Sud  2011.

Matthew Streeter and H. Brendan McMahan. No-regret algorithms for unconstrained online convex

optimization. In NIPS  2012.

Volodya Vovk. Competitive on-line statistics. International Statistical Review  69  2001.
Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In

ICML  2003.

9

,Brendan McMahan
Jacob Abernethy
Stephan Mandt
David Blei
Shakir Mohamed
Danilo Jimenez Rezende
Stéphanie van der Pas
Veronika Ročková
Faidra Georgia Monachou
Itai Ashlagi