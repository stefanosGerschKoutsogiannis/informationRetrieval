2013,Correlated random features for fast semi-supervised learning,This paper presents Correlated Nystrom Views (XNV)  a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First  it generates two views consisting of computationally inexpensive random features. Second  multiview regression  using Canonical Correlation Analysis (CCA) on unlabeled data  biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression  implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets  whilst also reducing runtime by orders of magnitude.,Correlated random features for
fast semi-supervised learning

Brian McWilliams

ETH Z¨urich  Switzerland

brian.mcwilliams@inf.ethz.ch

David Balduzzi

ETH Z¨urich  Switzerland

david.balduzzi@inf.ethz.ch

Joachim M. Buhmann
ETH Z¨urich  Switzerland

jbuhmann@inf.ethz.ch

Abstract

This paper presents Correlated Nystr¨om Views (XNV)  a fast semi-supervised al-
gorithm for regression and classiﬁcation. The algorithm draws on two main ideas.
First  it generates two views consisting of computationally inexpensive random
features. Second  multiview regression  using Canonical Correlation Analysis
(CCA) on unlabeled data  biases the regression towards useful features. It has
been shown that CCA regression can substantially reduce variance with a mini-
mal increase in bias if the views contains accurate estimators. Recent theoretical
and empirical work shows that regression with random features closely approxi-
mates kernel regression  implying that the accuracy requirement holds for random
views. We show that XNV consistently outperforms a state-of-the-art algorithm
for semi-supervised learning: substantially improving predictive performance and
reducing the variability of performance on a wide variety of real-world datasets 
whilst also reducing runtime by orders of magnitude.

1

Introduction

As the volume of data collected in the social and natural sciences increases  the computational cost
of learning from large datasets has become an important consideration. For learning non-linear
relationships  kernel methods achieve excellent performance but na¨ıvely require operations cubic in
the number of training points.
Randomization has recently been considered as an alternative to optimization that  surprisingly  can
yield comparable generalization performance at a fraction of the computational cost [1  2]. Ran-
dom features have been introduced to approximate kernel machines when the number of training
examples is very large  rendering exact kernel computation intractable. Among several different
approaches  the Nystr¨om method for low-rank kernel approximation [1] exhibits good theoretical
properties and empirical performance [3–5].
A second problem arising with large datasets concerns obtaining labels  which often requires a do-
main expert to manually assign a label to each instance which can be very expensive – requiring sig-
niﬁcant investments of both time and money – as the size of the dataset increases. Semi-supervised
learning aims to improve prediction by extracting useful structure from the unlabeled data points
and using this in conjunction with a function learned on a small number of labeled points.

Contribution. This paper proposes a new semi-supervised algorithm for regression and classiﬁ-
cation  Correlated Nystr¨om Views (XNV)  that addresses both problems simultaneously. The method

1

consists in essentially two steps. First  we construct two “views” using random features. We in-
vestigate two ways of doing so: one based on the Nystr¨om method and another based on random
Fourier features (so-called kitchen sinks) [2  6]. It turns out that the Nystr¨om method almost always
outperforms Fourier features by a quite large margin  so we only report these results in the main
text.
The second step  following [7]  uses Canonical Correlation Analysis (CCA  [8  9]) to bias the opti-
mization procedure towards features that are correlated across the views. Intuitively  if both views
contain accurate estimators  then penalizing uncorrelated features reduces variance without increas-
ing the bias by much. Recent theoretical work by Bach [5] shows that Nystr¨om views can be ex-
pected to contain accurate estimators.
We perform an extensive evaluation of XNV on 18 real-world datasets  comparing against a modiﬁed
version of the SSSL (simple semi-supervised learning) algorithm introduced in [10]. We ﬁnd that
XNV outperforms SSSL by around 10-15% on average  depending on the number of labeled points
available  see §3. We also ﬁnd that the performance of XNV exhibits dramatically less variability
than SSSL  with a typical reduction of 30%.
We chose SSSL since it was shown in [10] to outperform a state of the art algorithm  Laplacian
Regularized Least Squares [11]. However  since SSSL does not scale up to large sets of unlabeled
data  we modify SSSL by introducing a Nystr¨om approximation to improve runtime performance.
This reduces runtime by a factor of ⇥1000 on N = 10  000 points  with further improvements as N
increases. Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50%
on the 18 datasets on average  in line with the results reported in [10]  suggesting that we lose little
by replacing the exact SSSL with our approximate implementation.

Related work. Multiple view learning was ﬁrst introduced in the co-training method of [12] and
has also recently been extended to unsupervised settings [13 14]. Our algorithm builds on an elegant
proposal for multi-view regression introduced in [7]. Surprisingly  despite guaranteeing improved
prediction performance under a relatively weak assumption on the views  CCA regression has not
been widely used since its proposal – to the best of our knowledge this is ﬁrst empirical evaluation
of multi-view regression’s performance. A possible reason for this is the difﬁculty in obtaining
naturally occurring data equipped with multiple views that can be shown to satisfy the multi-view
assumption. We overcome this problem by constructing random views that satisfy the assumption
by design.

2 Method

This section introduces XNV  our semi-supervised learning method. The method builds on two
main ideas. First  given two equally useful but sufﬁciently different views on a dataset  penalizing
regression using the canonical norm (computed via CCA)  can substantially improve performance
[7]. The second is the Nystr¨om method for constructing random features [1]  which we use to
construct the views.

2.1 Multi-view regression

Suppose we have data T =(x1  y1)  . . .   (xn  yn) for xi 2 RD and yi 2 R  sampled according to

joint distribution P (x  y). Further suppose we have two views on the data

z(⌫) : RD ! H(⌫) = RM : x 7! z(⌫)(x) =: z(⌫)

for ⌫ 2{ 1  2}.

We make the following assumption about linear regressors which can be learned on these views.
Assumption 1 (Multi-view assumption [7]). Deﬁne mean-squared error loss function `(g  x  y) =
(g(x)  y)2 and let loss(g) := EP `(g(x)  y). Further let L(Z) denote the space of linear maps
from a linear space Z to the reals  and deﬁne:

f (⌫) := argmin
g2L(H(⌫))

The multi-view assumption is that

loss(g) for ⌫ 2{ 1  2}
loss⇣f (⌫)⌘  loss(f )  ✏

2

and f :=

argmin

loss(g).

g2L(H(1)H(2))

for ⌫ 2{ 1  2}.

(1)

In short  the best predictor in each view is within ✏ of the best overall predictor.

Canonical correlation analysis. Canonical correlation analysis [8  9] extends principal compo-
nent analysis (PCA) from one to two sets of variables. CCA ﬁnds bases for the two sets of variables
such that the correlation between projections onto the bases are maximized.

The ﬁrst pair of canonical basis vectors ⇣b(1)

1   b(2)

1 ⌘ is found by solving:
corr⇣b(1)>z(1)  b(2)>z(2)⌘ .

argmax

b(1) b(2)2RM

1. Orthogonality: ET⇥¯z(⌫)>j
2. Correlation: ET⇥¯z(1)>j

Subsequent pairs are found by maximizing correlations subject to being orthogonal to previously

found pairs. The result of performing CCA is two sets of bases  B(⌫) = hb(⌫)
⌫ 2{ 1  2}  such that the projection of z(⌫) onto B(⌫) which we denote ¯z(⌫) satisﬁes
¯z(⌫)
k ] = jk  where jk is the Kronecker delta  and

1   . . .   b(⌫)

Mi for

¯z(2)

j is referred to as the jth canonical correlation coefﬁcient.
Deﬁnition 1 (canonical norm). Given vector ¯z(⌫) in the canonical basis  deﬁne its canonical norm
as

k ⇤ = j · jk where w.l.o.g. we assume 1  1  2 ··· 0.
k¯z(⌫)kCCA :=vuut

⇣¯z(⌫)
j ⌘2

1  j
j

DXj=1

Canonical ridge regression. Assume we observe n pairs of views coupled with real valued labels

.

nz(1)

i

  yion

i=1

  z(2)

i

  canonical ridge regression ﬁnds coefﬁcientsb
nXi=1⇣yi  (⌫) >¯z(⌫)
i ⌘2
b

+ k(⌫)k2
The resulting estimator  referred to as the canonical shrinkage estimator  is

:= argmin

1
n

(⌫)



(⌫)

=hb(⌫)
1   . . .  b(⌫)

M i> such that

CCA.

(2)

(3)

(4)

j
n

nXi=1

¯z(⌫)
i j yi.

j =

b(⌫)

Penalizing with the canonical norm biases the optimization towards features that are highly cor-
related across the views. Good regressors exist in both views by Assumption 1. Thus  intuitively 
penalizing uncorrelated features signiﬁcantly reduces variance  without increasing the bias by much.
More formally:
Theorem 1 (canonical ridge regression  [7]). Assume E[y2|x]  1 and that Assumption 1 holds. Let
f (⌫)
denote the estimator constructed with the canonical shrinkage estimator  Eq. (4)  on training
b
set T   and let f denote the best linear predictor across both views. For ⌫ 2{ 1  2} we have

)]  loss(f )  5✏ +PM

j=1 2
j
n

ET [loss(f (⌫)
b

where the expectation is with respect to training sets T sampled from P (x  y).

the variance. TheP 2

The ﬁrst term  5✏  bounds the bias of the canonical estimator  whereas the second  1

j bounds
j can be thought of as a measure of the “intrinsic dimensionality” of the
unlabeled data  which controls the rate of convergence.
If the canonical correlation coefﬁcients
decay sufﬁciently rapidly  then the increase in bias is more than made up for by the decrease in
variance.

nP 2

3

2.2 Constructing random views

We construct two views satisfying Assumption 1 in expectation  see Theorem 3 below. To ensure our
method scales to large sets of unlabeled data  we use random features generated using the Nystr¨om
method [1].
Suppose we have data {xi}N
i=1. When N is very large  constructing and manipulating the N ⇥ N
Gram matrix [K]ii0 = h(xi)  (xi0)i = (xi  xi0) is computationally expensive. Where here  (x)
deﬁnes a mapping from RD to a high dimensional feature space and (· ·) is a positive semi-deﬁnite
kernel function.
The idea behind random features is to instead deﬁne a lower-dimensional mapping  z(xi) : RD !
RM through a random sampling scheme such that [K]ii0 ⇡ z(xi)>z(xi0) [6  15]. Thus  using
random features  non-linear functions in x can be learned as linear functions in z(x) leading to
signiﬁcant computational speed-ups. Here we give a brief overview of the Nystr¨om method  which
uses random subsampling to approximate the Gram matrix.

NXi=1

K ⇡ ˜K :=

The Nystr¨om method. Fix an M ⌧ N and randomly (uniformly) sample a subset M = {ˆxi}M
i=1. Let bK denote the Gram matrix [bK]ii0 where i  i0 2M . The
of M points from the data {xi}N
Nystr¨om method [1  3] constructs a low-rank approximation to the Gram matrix as
NXi0=1
[(xi  ˆx1)  . . .   (xi  ˆxM )]bK† [(xi0  ˆx1)  . . .   (xi0  ˆxM )]>  
z(xi) = bD1/2bV> [(xi  ˆx1)  . . .   (xi  ˆxM )]>  

where bK† 2 RM⇥M is the pseudo-inverse of bK. Vectors of random features can be constructed as
where the columns of bV are the eigenvectors of bK with bD the diagonal matrix whose entries are

the corresponding eigenvalues. Constructing features in this way reduces the time complexity of
learning a non-linear prediction function from O(N 3) to O(N ) [15].
An alternative perspective on the Nystr¨om approximation  that will be useful below  is as follows.
Consider integral operators

i=1

(5)

LN [f ](·) :=

1
N

(xi ·)f (xi)

and LM [f ](·) :=

1
M

(xi ·)f (xi) 

(6)

MXi=1

NXi=1

and introduce Hilbert space ˆH = span{ ˆ'1  . . .   ˆ'r} where r is the rank of bK and the ˆ'i are the ﬁrst
r eigenfunctions of LM. Then the following proposition shows that using the Nystr¨om approxima-
tion is equivalent to performing linear regression in the feature space (“view”) z : X! ˆH spanned
by the eigenfunctions of linear operator LM in Eq. (6):
Proposition 2 (random Nystr¨om view  [3]). Solving

is equivalent to solving

1
N

min
w2Rr

1
N

min
f2 ˆH

NXi=1
NXi=1

`(w>z(xi)  yi) +


2kwk2

2

`(f (xi)  yi) +


2kfk2

H.

(7)

(8)

2.3 The proposed algorithm: Correlated Nystr¨om Views (XNV)

Algorithm 1 details our approach to semi-supervised learning based on generating two views consist-
ing of Nystr¨om random features and penalizing features which are weakly correlated across views.
i=n+1.
The setting is that we have labeled data {xi  yi}n
Step 1 generates a set of random features. The next two steps implement multi-view regression using
the randomly generated views z(1)(x) and z(2)(x). Eq. (9) yields a solution for which unimportant

i=1 and a large amount of unlabeled data {xi}N

4

Algorithm 1 Correlated Nystr¨om Views (XNV).
Input: Labeled data: {xi  yi}n
i=1 and unlabeled data: {xi}N
1: Generate features. Sample ˆx1  . . .   ˆx2M uniformly from the dataset  compute the eigendecom-
positions of the sub-sampled kernel matrices ˆK(1) and ˆK(2) which are constructed from the
samples 1  . . .   M and M + 1  . . .   2M respectively  and featurize the input:

i=n+1

z(⌫)(xi) ˆD(⌫) 1/2 ˆV(⌫)> [(xi  ˆx1)  . . .   (xi  ˆxM )]> for ⌫ 2{ 1  2}.

2: Unlabeled data. Compute CCA bases B(1)  B(2) and canonical correlations 1  . . .   M for the

`⇣>¯zi  yi⌘ + kk2

CCA + kk2
2 .

(9)

3: Labeled data. Solve

two views and set ¯zi B(1)z(1)(xi).
nXi=1

b = argmin

1
n



Output: b

features are heavily downweighted in the CCA basis without introducing an additional tuning pa-
rameter. The further penalty on the `2 norm (in the CCA basis) is introduced as a practical measure

to control the variance of the estimatorb which can become large if there are many highly correlated
j ⇡ 0 for large j). In practice most of the shrinkage is due to the CCA

features (i.e. the ratio 1j
norm: cross-validation obtains optimal values of  in the range [0.00001  0.1].

Computational complexity. XNV is extremely fast. Nystr¨om sampling  step 1  reduces the O(N 3)
operations required for kernel learning to O(N ). Computing the CCA basis  step 2  using standard
algorithms is in O(N M 2). However  we reduce the runtime to O(N M ) by applying a recently
proposed randomized CCA algorithm of [16]. Finally  step 3 is a computationally cheap linear
program on n samples and M features.

Performance guarantees. The quality of the kernel approximation in (5) has been the subject of
detailed study in recent years leading to a number of strong empirical and theoretical results [3–5 
15]. Recent work of Bach [5] provides theoretical guarantees on the quality of Nystr¨om estimates in
the ﬁxed design setting that are relevant to our approach.1
Theorem 3 (Nystr¨om generalization bound  [5]). Let ⇠ 2 RN be a random vector with ﬁnite
variance and zero mean  y = [y1  . . .   yN ]>  and deﬁne smoothed estimate ˆykernel
:= (K +
N I)1K(y + ⇠) and smoothed Nystr¨om estimate ˆyNystr¨om := ( ˜K + N I)1 ˜K(y + ⇠)  both
computed by minimizing the MSE with ridge penalty . Let ⌘ 2 (0  1). For sufﬁciently large M
(depending on ⌘  see [5])  we have

EME⇠⇥ky  ˆyNystr¨omk2

2⇤  (1 + 4⌘) · E⇠⇥ky  ˆykernelk2
2⇤

where EM refers to the expectation over subsampled columns used to construct ˜K.
In short  the best smoothed estimators in the Nystr¨om views are close to the optimal smoothed
estimator. Since the kernel estimate is consistent  loss(f ) ! 0 as n ! 1. Thus  Assumption 1
holds in expectation and the generalization performance of XNV is controlled by Theorem 1.

Random Fourier Features. An alternative approach to constructing random views is to use
Fourier features instead of Nystr¨om features in Step 1. We refer to this approach as Correlated
Kitchen Sinks (XKS) after [2]. It turns out that the performance of XKS is consistently worse than
XNV  in line with the detailed comparison presented in [3]. We therefore do not discuss Fourier
features in the main text  see §SI.3 for details on implementation and experimental results.

1Extending to a random design requires techniques from [17].

5

Task

Set Name

Table 1: Datasets used for evaluation.
Task
1 abalone2
C
2 adult2
C
3 ailerons4
R
4 bank84
C
5 bank324
C
6 cal housing4 R
7 census2
R
8 CPU2
R
9 CT2
R

D Set Name
N
6
2  089
14
32  561
40
7  154
8
4  096
32
4  096
10  320
8
18  186 119
6  554
21
30  000 385

10 elevators4 R
11 HIVa3
C
12 house4
R
13 ibn Sina3
C
14 orange3
C
15 sarcos 15
R
16 sarcos 55
R
17 sarcos 75
R
18 sylva3
C

D
N
8  752
18
21  339 1  617
11  392
16
92
10  361
230
25  000
21
44  484
21
44  484
44  484
21
216
72  626

2.4 A fast approximation to SSSL

The SSSL (simple semi-supervised learning) algorithm proposed in [10] ﬁnds the ﬁrst s eigenfunc-
tions i of the integral operator LN in Eq. (6) and then solves

argmin
w2Rs

nXi=1

0@
sXj=1

wjk(xi)  yi1A

2

 

(10)

where s is set by the user. SSSL outperforms Laplacian Regularized Least Squares [11]  a state of
the art semi-supervised learning method  see [10]. It also has good generalization guarantees under
reasonable assumptions on the distribution of eigenvalues of LN. However  since SSSL requires
computing the full N ⇥ N Gram matrix  it is extremely computationally intensive for large N.
Moreover  tuning s is difﬁcult since it is discrete.
We therefore propose SSSLM  an approximation to SSSL. First  instead of constructing the full
Gram matrix  we construct a Nystr¨om approximation by sampling M points from the labeled and
unlabeled training set. Second  instead of thresholding eigenfunctions  we use the easier to tune
ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18].
As justiﬁcation  note that Proposition 2 states that the Nystr¨om approximation to kernel regression
actually solves a ridge regression problem in the span of the eigenfunctions of ˆLM. As M increases 
the span of ˆLM tends towards that of LN [15]. We will also refer to the Nystr¨om approximation to
SSSL using 2M features as SSSL2M. See experiments below for further discussion of the quality
of the approximation.

3 Experiments

Setup. We evaluate the performance of XNV on 18 real-world datasets  see Table 1. The datasets
cover a variety of regression (denoted by R) and two-class classiﬁcation (C) problems. The sarcos
dataset involves predicting the joint position of a robot arm; following convention we report results
on the 1st  5th and 7th joint positions.
The SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semi-
supervised methods in scenarios where few labeled training examples are available [10]. How-
ever  as discussed in §2.2  due to its computational cost we compare the performance of XNV to the
Nystr¨om approximations SSSLM and SSSL2M.
We used a Gaussian kernel for all datasets. We set the kernel width   and the `2 regularisation
strength    for each method using 5-fold cross validation with 1000 labeled training examples. We
trained all methods using a squared error loss function  `(f (xi)  yi) = (f (xi)yi)2  with M = 200
random features  and n = 100  150  200  . . .   1000 randomly selected training examples.

2Taken from the UCI repository http://archive.ics.uci.edu/ml/datasets.html
3Taken from http://www.causality.inf.ethz.ch/activelearning.php
4Taken from http://www.dcc.fc.up.pt/˜ltorgo/Regression/DataSets.html
5Taken from http://www.gaussianprocess.org/gpml/data/

6

Runtime performance. The SSSL algorithm of [10] is not computationally feasible on large
datasets  since it has time complexity O(N 3). For illustrative purposes  we report run times6 in
seconds of the SSSL algorithm against SSSLM and XNV on three datasets of different sizes.

runtimes bank8 cal housing sylva
-
SSSL
24s
SSSL2M
26s
XNV

2300s
0.6s
1.3s

72s
0.3s
0.9s

For the cal housing dataset  XNV exhibits an almost 1800⇥ speed up over SSSL. For the largest
dataset  sylva  exact SSSL is computationally intractable. Importantly  the computational over-
head of XNV over SSSL2M is small.

Generalization performance. We report on the prediction performance averaged over 100 experi-
ments. For regression tasks we report on the mean squared error (MSE) on the testing set normalized
by the variance of the test output. For classiﬁcation tasks we report the percentage of the test set that
was misclassiﬁed.
The table below shows the improvement in performance of XNV over SSSLM and SSSL2M (taking
whichever performs better out of M or 2M on each dataset)  averaged over all 18 datasets. Observe
that XNV is considerably more accurate and more robust than SSSLM.

XNV vs SSSLM/2M
Avg reduction in error
Avg reduction in std err

n = 100
11%
15%

n = 200
16%
30%

n = 300
15%
31%

n = 400
12%
33%

n = 500
9%
30%

The reduced variability is to be expected from Theorem 1.

0.24

0.23

0.22

0.21

0.2

0.19

0.18

0.17

0.16

r
o
r
r
e
n
o

 

i

i
t
c
d
e
r
p

 

SSSL

M

SSSL

2M

XNV

1

0.9

0.8

0.7

0.6

0.5

0.4

r
o
r
r
e

 

n
o

i

i
t
c
d
e
r
p

 

SSSL

M

SSSL

2M

XNV

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r
e
n
o

 

i

i
t
c
d
e
r
p

 

SSSL

M

SSSL

2M

XNV

0.15

 
100

200

300

400

500

600

700

800

900

1000

number of labeled training points

 
100

200

300

400

500

600

700

800

900

1000

number of labeled training points

0

 
100

200

300

400

500

600

700

800

900

1000

number of labeled training points

(a) adult

(b) cal housing

(c) census

0.8

0.7

0.6

0.5

0.4

0.3

0.2

r
o
r
r
e

 

n
o

i

i
t
c
d
e
r
p

 

SSSL

M

SSSL

2M

XNV

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

r
o
r
r
e
n
o

 

i

i
t
c
d
e
r
p

SSSL

M

SSSL

2M

XNV

 

0.4

r
o
r
r
e
n
o

 

i

i
t
c
d
e
r
p

0.35

0.3

0.25

0.2

0.15

0.1

 

SSSL

M

SSSL

2M

XNV

0.1

 
100

200

300

400

500

600

700

800

900

1000

0.04

 
100

number of labeled training points

(d) elevators

200

300

400

500

600

700

800

900

1000

number of labeled training points

0.05

 
100

200

300

400

500

600

700

800

900

1000

number of labeled training points

(e) ibn Sina

(f) sarcos 5

Figure 1: Comparison of mean prediction error and standard deviation on a selection of datasets.

Table 2 presents more detailed comparison of performance for individual datasets when n =
200  400. The plots in Figure 1 shows a representative comparison of mean prediction errors for
several datasets when n = 100  . . .   1000. Error bars represent one standard deviation. Observe that
XNV almost always improves prediction accuracy and reduces variance compared with SSSLM and
SSSL2M when the labeled training set contains between 100 and 500 labeled points. A complete
set of results is provided in §SI.1.
Discussion of SSSLM. Our experiments show that going from M to 2M does not improve gener-
alization performance in practice. This suggests that when there are few labeled points  obtaining a

6Computed in Matlab 7.14 on a Core i5 with 4GB memory.

7

more accurate estimate of the eigenfunctions of the kernel does not necessarily improve predictive
performance. Indeed  when more random features are added  stronger regularization is required to
reduce the inﬂuence of uninformative features  this also has the effect of downweighting informative
features. This suggests that the low rank approximation SSSLM to SSSL sufﬁces.
Finally  §SI.2 compares the performance of SSSLM and XNV to fully supervised kernel ridge reg-
ression (KRR). We observe dramatic improvements  between 48% and 63%  consistent with the
results observed in [10] for the exact SSSL algorithm.

Random Fourier features. Nystr¨om features signiﬁcantly outperform Fourier features  in line
with observations in [3]. The table below shows the relative improvement of XNV over XKS:

XNV vs XKS
Avg reduction in error
Avg reduction in std err

n = 100
30%
36%

n = 200
28%
44%

n = 300
26%
34%

n = 400
25%
37%

n = 500
24%
36%

Further results and discussion for XKS are included in the supplementary material.

XNV

set

SSSLM

XNV

SSSLM

SSSL2M

0.054 (0.005) 0.055 (0.006) 0.053 (0.004) 10
0.198 (0.014) 0.184 (0.010) 0.175 (0.010) 11
0.218 (0.016) 0.231 (0.020) 0.213 (0.016) 12 0.761 (0.075) 0.787 (0.091)
0.561 (0.030) 13
0.558 (0.027) 0.567 (0.029)
0.058 (0.004) 0.060 (0.005) 0.055 (0.003) 14
0.567 (0.081) 0.634 (0.103) 0.459 (0.045) 15
0.020 (0.012) 0.022 (0.014) 0.019 (0.005) 16
0.395 (0.395) 0.463 (0.414) 0.263 (0.352) 17
0.437 (0.096) 0.367 (0.060) 0.222 (0.015) 18

0.309 (0.059) 0.358 (0.077) 0.226 (0.020)
0.146 (0.048) 0.072 (0.024) 0.036 (0.001)
0.792 (0.100)
0.109 (0.017) 0.109 (0.017) 0.068 (0.010)
0.019 (0.001) 0.019 (0.001) 0.019 (0.000)
0.076 (0.008) 0.078 (0.009) 0.071 (0.006)
0.172 (0.032) 0.192 (0.036) 0.119 (0.014)
0.041 (0.004) 0.043 (0.005) 0.040 (0.004)
0.036 (0.007) 0.039 (0.007) 0.028 (0.009)

SSSL2M

Table 2: Performance (normalized MSE/classiﬁcation error rate). Standard errors in parentheses.
set
n = 200
1
2
3
4
5
6
7
8
9
n = 400
1
2
3
4
5
6
7
8
9

0.051 (0.003) 0.052 (0.003) 0.050 (0.002) 10
0.218 (0.022) 0.233 (0.027) 0.192 (0.010)
0.177 (0.008) 0.172 (0.006) 0.167 (0.005) 11
0.051 (0.009) 0.122 (0.031) 0.036 (0.001)
0.199 (0.011) 0.209 (0.013) 0.193 (0.010) 12 0.691 (0.040) 0.701 (0.051)
0.709 (0.058)
0.517 (0.018) 0.527 (0.019) 0.510 (0.016) 13
0.070 (0.009) 0.072 (0.008) 0.054 (0.004)
0.050 (0.003) 0.051 (0.003) 0.050 (0.002) 14
0.019 (0.001) 0.019 (0.001) 0.019 (0.000)
0.513 (0.055) 0.555 (0.063) 0.432 (0.036) 15
0.059 (0.004) 0.060 (0.005) 0.057 (0.003)
0.019 (0.010) 0.021 (0.012) 0.014 (0.003) 16
0.105 (0.014) 0.106 (0.014) 0.090 (0.007)
0.209 (0.171) 0.286 (0.248) 0.110 (0.107) 17 0.032 (0.002) 0.033 (0.003) 0.032 (0.002)
0.249 (0.024) 0.304 (0.037) 0.201 (0.013) 18
0.029 (0.006) 0.032 (0.005) 0.023 (0.006)

4 Conclusion

We have introduced the XNV algorithm for semi-supervised learning. By combining two randomly
generated views of Nystr¨om features via an efﬁcient implementation of CCA  XNV outperforms the
prior state-of-the-art  SSSL  by 10-15% (depending on the number of labeled points) on average
over 18 datasets. Furthermore  XNV is over 3 orders of magnitude faster than SSSL on medium
sized datasets (N = 10  000) with further gains as N increases. An interesting research direction
is to investigate using the recently developed deep CCA algorithm  which extracts higher order
correlations between views [19]  as a preprocessing step.
In this work we use a uniform sampling scheme for the Nystr¨om method for computational reasons
since it has been shown to perform well empirically relative to more expensive schemes [20]. Since
CCA gives us a criterion by which to measure the important of random features  in the future we
aim to investigate active sampling schemes based on canonical correlations which may yield better
performance by selecting the most informative indices to sample.

Acknowledgements. We thank Haim Avron for help with implementing randomized CCA and
Patrick Pletscher for drawing our attention to the Nystr¨om method.

8

References
[1] Williams C  Seeger M: Using the Nystr¨om method to speed up kernel machines. In NIPS 2001.
[2] Rahimi A  Recht B: Weighted sums of random kitchen sinks: Replacing minimization with random-

ization in learning. In Adv in Neural Information Processing Systems (NIPS) 2008.

[3] Yang T  Li YF  Mahdavi M  Jin R  Zhou ZH: Nystr¨om Method vs Random Fourier Features: A Theo-

retical and Empirical Comparison. In NIPS 2012.

[4] Gittens A  Mahoney MW: Revisiting the Nystr¨om method for improved large-scale machine learning.

In ICML 2013.

[5] Bach F: Sharp analysis of low-rank kernel approximations. In COLT 2013.
[6] Rahimi A  Recht B: Random Features for Large-Scale Kernel Machines. In Adv in Neural Information

Processing Systems 2007.

[7] Kakade S  Foster DP: Multi-view Regression Via Canonical Correlation Analysis. In Computational

Learning Theory (COLT) 2007.

[8] Hotelling H: Relations between two sets of variates. Biometrika 1936  28:312–377.
[9] Hardoon DR  Szedmak S  Shawe-Taylor J: Canonical Correlation Analysis: An Overview with Appli-

cation to Learning Methods. Neural Comp 2004  16(12):2639–2664.

[10] Ji M  Yang T  Lin B  Jin R  Han J: A Simple Algorithm for Semi-supervised Learning with Improved

Generalization Error Bound. In ICML 2012.

[11] Belkin M  Niyogi P  Sindhwani V: Manifold regularization: A geometric framework for learning

from labeled and unlabeled examples. JMLR 2006  7:2399–2434.

[12] Blum A  Mitchell T: Combining labeled and unlabeled data with co-training. In COLT 1998.
[13] Chaudhuri K  Kakade SM  Livescu K  Sridharan K: Multiview clustering via Canonical Correlation

Analysis. In ICML 2009.

[14] McWilliams B  Montana G: Multi-view predictive partitioning in high dimensions. Statistical Analysis

and Data Mining 2012  5:304–321.

[15] Drineas P  Mahoney MW: On the Nystr¨om Method for Approximating a Gram Matrix for Improved

Kernel-Based Learning. JMLR 2005  6:2153–2175.

[16] Avron H  Boutsidis C  Toledo S  Zouzias A: Efﬁcient Dimensionality Reduction for Canonical Corre-

lation Analysis. In ICML 2013.

[17] Hsu D  Kakade S  Zhang T: An Analysis of Random Design Linear Regression. In COLT 2012.
[18] Dhillon PS  Foster DP  Kakade SM  Ungar LH: A Risk Comparison of Ordinary Least Squares vs

Ridge Regression. Journal of Machine Learning Research 2013  14:1505–1511.

[19] Andrew G  Arora R  Bilmes J  Livescu K: Deep Canonical Correlation Analysis. In ICML 2013.
[20] Kumar S  Mohri M  Talwalkar A: Sampling methods for the Nystr¨om method. JMLR 2012  13:981–

1006.

9

,Brian McWilliams
David Balduzzi
Joachim Buhmann
Arthur Guez
Nicolas Heess
David Silver
Peter Dayan
Luca Bertinetto
João Henriques
Jack Valmadre
Philip Torr
Andrea Vedaldi