2013,The Fast Convergence of Incremental PCA,We prove the first finite-sample convergence rates for any incremental PCA algorithm using sub-quadratic time and memory per iteration. The algorithm analyzed is Oja's learning rule  an efficient and well-known scheme for estimating the top principal component. Our analysis of this non-convex problem yields expected and high-probability convergence rates of $\tilde{O}(1/n)$ through a novel technique. We relate our guarantees to existing rates for stochastic gradient descent on strongly convex functions  and extend those results. We also include experiments which demonstrate convergence behaviors predicted by our analysis.,The Fast Convergence of Incremental PCA

Akshay Balsubramani

UC San Diego

abalsubr@cs.ucsd.edu

Sanjoy Dasgupta

UC San Diego

dasgupta@cs.ucsd.edu

Yoav Freund
UC San Diego

yfreund@cs.ucsd.edu

Abstract

We consider a situation in which we see samples Xn ∈ Rd drawn i.i.d. from some
distribution with mean zero and unknown covariance A. We wish to compute the
top eigenvector of A in an incremental fashion - with an algorithm that maintains
an estimate of the top eigenvector in O(d) space  and incrementally adjusts the
estimate with each new data point that arrives. Two classical such schemes are
due to Krasulina (1969) and Oja (1983). We give ﬁnite-sample convergence rates
for both.

1

Introduction

Principal component analysis (PCA) is a popular form of dimensionality reduction that projects a
data set on the top eigenvector(s) of its covariance matrix. The default method for computing these
eigenvectors uses O(d2) space for data in Rd  which can be prohibitive in practice. It is therefore
of interest to study incremental schemes that take one data point at a time  updating their estimates
of the desired eigenvectors with each new point. For computing one eigenvector  such methods use
O(d) space.
For the case of the top eigenvector  this problem has long been studied  and two elegant solutions
were obtained by Krasulina [7] and Oja [9]. Their methods are closely related. At time n − 1  they
have some estimate Vn−1 ∈ Rd of the top eigenvector. Upon seeing the next data point  Xn  they
update this estimate as follows:

(cid:18)

(cid:19)

Vn = Vn−1 + γn

XnX T

Vn =

Vn−1 + γnXnX T
(cid:107)Vn−1 + γnXnX T

n Vn−1

n−1XnX T
(cid:107)Vn−1(cid:107)2

Id

n − V T
n Vn−1
n Vn−1(cid:107)

Vn−1

(Krasulina)

(Oja)

Here γn is a “learning rate” that is typically proportional to 1/n.
Suppose the points X1  X2  . . . are drawn i.i.d. from a distribution on Rd with mean zero and co-
variance matrix A. The original papers proved that these estimators converge almost surely to the
top eigenvector of A (call it v∗) under mild conditions:

• (cid:80)

n γn = ∞ while(cid:80)

n < ∞.

n γ2

• If λ1  λ2 denote the top two eigenvalues of A  then λ1 > λ2.
• E(cid:107)Xn(cid:107)k < ∞ for some suitable k (for instance  k = 8 works).

There are also other incremental estimators for which convergence has not been established; see  for
instance  [12] and [16].
In this paper  we analyze the rate of convergence of the Krasulina and Oja estimators. They can
be treated in a common framework  as stochastic approximation algorithms for maximizing the

1

Rayleigh quotient

G(v) =

The maximum value of this function is λ1  and is achieved at v∗ (or any nonzero multiple thereof).
The gradient is

vT Av
vT v

.

(cid:18)

∇G(v) =

2
(cid:107)v(cid:107)2

A − vT Av
vT v

(cid:19)

Id

v.

n = A  we see that Krasulina’s method is stochastic gradient descent. The Oja proce-

Since EXnX T
dure is closely related: as pointed out in [10]  the two are identical to within second-order terms.
Recently  there has been a lot of work on rates of convergence for stochastic gradient descent (for in-
stance  [11])  but this has typically been limited to convex cost functions. These results do not apply
to the non-convex Rayleigh quotient  except at the very end  when the system is near convergence.
Most of our analysis focuses on the buildup to this ﬁnale.
We measure the quality of the solution Vn at time n using the potential function

Ψn = 1 − (Vn · v∗)2
(cid:107)Vn(cid:107)2

 

where v∗ is taken to have unit norm. This quantity lies in the range [0  1]  and we are interested in
the rate at which it approaches zero. The result  in brief  is that E[Ψn] = O(1/n)  under conditions
that are similar to those above  but stronger. In particular  we require that γn be proportional to 1/n
and that (cid:107)Xn(cid:107) be bounded.

1.1 The algorithm

We analyze the following procedure.

1. Set starting time. Set the clock to time no.
2. Initialization. Initialize Vno uniformly at random from the unit sphere in Rd.
3. For time n = no + 1  no + 2  . . .:

(a) Receive the next data point  Xn.
(b) Update step. Perform either the Krasulina or Oja update  with γn = c/n.

The ﬁrst step is similar to using a learning rate of the form γn = c/(n + no)  as is often done in
stochastic gradient descent implementations [1]. We have adopted it because the initial sequence of
updates is highly noisy: during this phase Vn moves around wildly  and cannot be shown to make
progress. It becomes better behaved when the step size γn becomes smaller  that is to say when n
gets larger than some suitable no. By setting the start time to no  we can simply fast-forward the
analysis to this moment.

1.2

Initialization

One possible initialization is to set Vno to the ﬁrst data point that arrives  or to the average of a few
data points. This seems sensible enough  but can fail dramatically in some situations.
Here is an example. Suppose X can take on just 2d possible values: ±e1 ±σe2  . . .  ±σed  where
the ei are coordinate directions and 0 < σ < 1 is a small constant. Suppose further that the
distribution of X is speciﬁed by a single positive number p < 1:

Pr(X = e1) = Pr(X = −e1) =
Pr(X = σei) = Pr(X = −σei) =

p
2
1 − p
2(d − 1)

for i > 1

Then X has mean zero and covariance diag(p  σ2(1 − p)/(d − 1)  . . .   σ2(1 − p)/(d − 1)). We will
assume that p and σ are chosen so that p > σ2(1 − p)/(d − 1); in our notation  the top eigenvalues
are then λ1 = p and λ2 = σ2(1 − p)/(d − 1)  and the target vector is v∗ = e1.

2

If Vn is ever orthogonal to some ei  it will remain so forever. This is because both the Krasulina and
Oja updates have the following properties:

Vn−1 · Xn = 0 =⇒ Vn = Vn−1
Vn−1 · Xn (cid:54)= 0 =⇒ Vn ∈ span(Vn−1  Xn).

If Vno is initialized to a random data point  then with probability 1 − p  it will be assigned to some
ei with i > 1  and will converge to a multiple of that same ei rather than to e1. Likewise  if it is
initialized to the average of ≤ 1/p data points  then with constant probability it will be orthogonal
to e1 and remain so always.
Setting Vno to a random unit vector avoids this problem. However  there are doubtless cases  for
instance when the data has intrinsic dimension (cid:28) d  in which a better initializer is possible.

1.3 The setting of the learning rate

In order to get a sense of what rates of convergence we might expect  let’s return to the example of a
random vector X with 2d possible values. In the Oja update Vn = Vn−1 + γnXnX T
n Vn−1  we can
ignore normalization if we are merely interested in the progress of the potential function Ψn. Since
the Xn correspond to coordinate directions  each update changes just one coordinate of V :

Xn = ±e1 =⇒ Vn 1 = Vn−1 1(1 + γn)
Xn = ±σei =⇒ Vn i = Vn−1 i(1 + σ2γn)

Recall that we initialize Vno to a random vector from the unit sphere. For simplicity  let’s just
suppose that no = 0 and that this initial value is the all-ones vector (again  we don’t have to worry
about normalization). On each iteration the ﬁrst coordinate is updated with probability exactly
p = λ1  and thus

E[Vn 1] = (1 + λ1γ1)(1 + λ1γ2)··· (1 + λ1γn) ∼ exp(λ1(γ1 + ··· + γn)) ∼ ncλ1

since γn = c/n. Likewise  for i > 1 

E[Vn i] = (1 + λ2γ1)(1 + λ2γ2)··· (1 + λ2γn) ∼ ncλ2 .

If all goes according to expectation  then at time n 

Ψn = 1 − V 2

n 1

(cid:107)Vn(cid:107)2 ∼ 1 −

n2cλ1

n2cλ1 + (d − 1)n2cλ2

∼ d − 1
n2c(λ1−λ2)

.

(This is all very rough  but can be made precise by obtaining concentration bounds for ln Vn i.)
From this  we can see that it is not possible to achieve a O(1/n) rate unless c ≥ 1/(2(λ1 − λ2)).
Therefore  we will assume this when stating our ﬁnal results  although most of our analysis is in
terms of general γn. An interesting practical question  to which we do not have an answer  is how
one would empirically set c without prior knowledge of the eigenvalue gap.

1.4 Nested sample spaces
For n ≥ no  let Fn denote the sigma-ﬁeld of all outcomes up to and including time n: Fn =
σ(Vno  Xno+1  . . .   Xn). We start by showing that

E[Ψn|Fn−1] ≤ Ψn−1(1 − 2γn(λ1 − λ2)(1 − Ψn−1)) + O(γ2
n).

Initially Ψn is likely to be close to 1. For instance  if the initial Vno is picked uniformly at random
from the surface of the unit sphere in Rd  then we’d expect Ψno ≈ 1 − 1/d. This means that the
initial rate of decrease is very small  because of the (1 − Ψn−1) term.
To deal with this  we divide the analysis into epochs: the ﬁrst takes Ψn from 1− 1/d to 1− 2/d  the
second from 1−2/d to 1−4/d  and so on until Ψn ﬁnally drops below 1/2. We use martingale large
deviation bounds to bound the length of each epoch  and also to argue that Ψn does not regress. In
particular  we establish a sequence of times nj such that (with high probability)

Ψn ≤ 1 − 2j
d

.

sup
n≥nj

3

(1)

The analysis of each epoch uses martingale arguments  but at the same time  assumes that Ψn re-
mains bounded above. Combining the two requires a careful speciﬁcation of the sample space at
each step. Let Ω denote the sample space of all realizations (vno  xno+1  xno+2  . . .)  and P the
probability distribution on these sequences. For any δ > 0  we deﬁne a nested sequence of spaces
Ω ⊃ Ω(cid:48)
n) ≥ 1 − δ 
and moreover consists exclusively of realizations ω ∈ Ω that satisfy the constraints (1) up to and
including time n − 1. We can then build martingale arguments by restricting attention to Ω(cid:48)
n when
computing the conditional expectations of quantities at time n.

n is Fn−1-measurable  has probability P (Ω(cid:48)

no+1 ⊃ ··· such that each Ω(cid:48)

⊃ Ω(cid:48)

no

1.5 Main result

We make the following assumptions:

(A1) The Xn ∈ Rd are i.i.d. with mean zero and covariance A.
(A2) There is a constant B such that (cid:107)Xn(cid:107)2 ≤ B.
(A3) The eigenvalues λ1 ≥ λ2 ≥ ··· ≥ λd of A satisfy λ1 > λ2.
(A4) The step sizes are of the form γn = c/n.

En

≥ 1 −

− A1

Under these conditions  we get the following rate of convergence for the Krasulina update.
Theorem 1.1. There are absolute constants Ao  A1 > 0 and 1 < a < 4 for which the following
holds. Pick any 0 < δ < 1  and any co > 2. Set the step sizes to γn = c/n  where c = co/(2(λ1 −
λ2))  and set the starting time to no ≥ (AoB2c2d2/δ4) ln(1/δ). Then there is a nested sequence of
subsets of the sample space Ω ⊃ Ω(cid:48)

P (Ω(cid:48)

(cid:20) (Vn · v∗)2

no

⊃ Ω(cid:48)
(cid:21)
n) ≥ 1 − δ and

(cid:18) c2B2eco/no

no+1 ⊃ ··· such that for any n ≥ no  we have:
(cid:19)co/2

(cid:19)a(cid:18) no + 1

(cid:19) 1

(cid:18) d

n + 1

(cid:107)Vn(cid:107)2

2(co − 2)
where En denotes expectation restricted to Ω(cid:48)
n.
Since co > 2  this bound is of the form En[Ψn] = O(1/n).
The result above also holds for the Oja update up to absolute constants.
We also remark that a small modiﬁcation to the ﬁnal step in the proof of the above yields a rate
of En [Ψn] = O(n−a)  with an identical deﬁnition of En [Ψn]. The details are in the proof  in
Appendix D.2.

n + 1

δ2

 

1.6 Related work

There is an extensive line of work analyzing PCA from the statistical perspective  in which the con-
vergence of various estimators is characterized under certain conditions  including generative models
of the data [5] and various assumptions on the covariance matrix spectrum [14  4] and eigenvalue
spacing [17]. Such works do provide ﬁnite-sample guarantees  but they apply only to the batch case
and/or are computationally intensive  rather than considering an efﬁcient incremental algorithm.
Among incremental algorithms  the work of Warmuth and Kuzmin [15] describes and analyzes
worst-case online PCA  using an experts-setting algorithm with a super-quadratic per-iteration cost.
More efﬁcient general-purpose incremental PCA algorithms have lacked ﬁnite-sample analyses [2].
There have been recent attempts to remedy this situation by relaxing the nonconvexity inherent in
the problem [3] or making generative assumptions [8]. The present paper directly analyzes the oldest
known incremental PCA algorithms under relatively mild assumptions.

2 Outline of proof

We now sketch the proof of Theorem 1.1; almost all the details are relegated to the appendix.
Recall that for n ≥ no  we take Fn to be the sigma-ﬁeld of all outcomes up to and including time n 
that is  Fn = σ(Vno  Xno+1  . . .   Xn).

4

An additional piece of notation: we will use(cid:98)u to denote u/(cid:107)u(cid:107)  the unit vector in the direction of
u ∈ Rd. Thus  for instance  the Rayleigh quotient can be written G(v) =(cid:98)vT A(cid:98)v.

2.1 Expected per-step change in potential

We ﬁrst bound the expected improvement in Ψn in each step of the Krasulina or Oja algorithms.
Theorem 2.1. For any n > no  we can write Ψn ≤ Ψn−1 + βn − Zn  where

(cid:26) γ2

βn =

nB2/4
nB2 + 2γ3
5γ2

nB3

(Krasulina)
(Oja)

and where Zn is a Fn-measurable random variable with the following properties:

• E[Zn|Fn−1] = 2γn((cid:98)Vn−1 · v∗)2(λ1 − G(Vn−1)) ≥ 2γn(λ1 − λ2)Ψn−1(1 − Ψn−1) ≥ 0.

• |Zn| ≤ 4γnB.

The theorem follows from Lemmas ?? and ?? in the appendix.
Its characterization of the two
estimators is almost identical  and for simplicity we will henceforth deal only with Krasulina’s
estimator. All the subsequent results hold also for Oja’s method  up to constants.

2.2 A large deviation bound for Ψn
We know from Theorem 2.1 that Ψn ≤ Ψn−1 + βn − Zn  where βn is non-stochastic and Zn is
a quantity of positive expected value. Thus  in expectation  and modulo a small additive term  Ψn
decreases monotonically. However  the amount of decrease at the nth time step can be arbitrarily
small when Ψn is close to 1. Thus  we need to show that Ψn is eventually bounded away from 1 
i.e. there exists some o > 0 and some time no such that for any n ≥ no  we have Ψn ≤ 1 − o.
Recall from the algorithm speciﬁcation that we advance the clock so as to skip the pre-no phase.
√
Given this  what can we expect o to be? If the initial estimate Vno is a random unit vector  then
E[Ψno] = 1 − 1/d and  roughly speaking  Pr(Ψno > 1 − /d) = O(
). If no is sufﬁciently large 
then Ψn may subsequently increase a little bit  but not by very much. In this section  we establish
the following bound.
Theorem 2.2. Suppose the initial estimate Vno is chosen uniformly at random from the surface of
the unit sphere in Rd. Assume also that the step sizes are of the form γn = c/n  for some constant
c > 0. Then for any 0 <  < 1  if no ≥ 2B2c2d2/2  we have
√
≤

(cid:18)

(cid:19)

2e.

Pr

Ψn ≥ 1 − 
d

sup
n≥no

To prove this  we start with a simple recurrence for the moment-generating function of Ψn.
Lemma 2.3. Consider a ﬁltration (Fn) and random variables Yn  Zn ∈ Fn such that there are two
sequences of nonnegative constants  (βn) and (ζn)  for which:

• Yn ≤ Yn−1 + βn − Zn.
• Each Zn takes values in an interval of length ζn.

Then for any t > 0  we have E[etYn|Fn−1] ≤ exp(t(Yn−1 − E[Zn|Fn−1] + βn + tζ 2
This relation shows how to deﬁne a supermartingale based on etYn  from which we can derive a
large deviation bound on Yn.
Lemma 2.4. Assume the conditions of Lemma 2.3  and also that E[Zn|Fn−1] ≥ 0. Then  for any
integer m and any ∆  t > 0 

n/8)).

(cid:19)

≤ E[etYm ] exp(cid:0) − t(cid:0)∆ −(cid:88)

(cid:96) /8)(cid:1)(cid:1).

(β(cid:96) + tζ 2

(cid:18)

Pr

sup
n≥m

Yn ≥ ∆

(cid:96)>m

5

In order to apply this to the sequence (Ψn)  we need to ﬁrst calculate the moment-generating func-
tion of its starting value Ψno.
Lemma 2.5. Suppose a vector V is picked uniformly at random from the surface of the unit sphere
in Rd  where d ≥ 3. Deﬁne Y = 1 − (V 2

(cid:114)

1 )/(cid:107)V (cid:107)2. Then  for any t > 0 
EetY ≤ et

.

d − 1
2t

Putting these pieces together yields Theorem 2.2.

Intermediate epochs of improvement

2.3
We have seen that  for suitable  and no  it is likely that Ψn ≤ 1 − /d for all n ≥ no. We now
deﬁne a series of epochs in which 1 − Ψn successively doubles  until Ψn ﬁnally drops below 1/2.
To do this  we specify intermediate goals (no  o)  (n1  1)  (n2  2)  . . .   (nJ   J )  where no < n1 <
··· < nJ and o < 1 < ··· < J = 1/2  with the intention that:

For all 0 ≤ j ≤ J  we have

Ψn ≤ 1 − j.

sup
n≥nj

(2)

Of course  this can only hold with a certain probability.
Let Ω denote the sample space of all realizations (vno  xno+1  xno+2  . . .)  and P the probability
distribution on these sequences. We will show that  for a certain choice of {(nj  j)}  all J + 1
constraints (2) can be met by excluding just a small portion of Ω.
We consider a speciﬁc realization ω ∈ Ω to be good if it satisﬁes (2). Call this set Ω(cid:48):

Ω(cid:48) = {ω ∈ Ω : sup
n≥nj

Ψn(ω) ≤ 1 − j for all 0 ≤ j ≤ J}.

For technical reasons  we also need to look at realizations that are good up to time n−1. Speciﬁcally 
for each n  deﬁne

Crucially  this is Fn−1-measurable. Also note that Ω(cid:48) =(cid:84)

n = {ω ∈ Ω :
Ω(cid:48)

nj≤(cid:96)<n

sup

Ψ(cid:96)(ω) ≤ 1 − j for all 0 ≤ j ≤ J}.

Ω(cid:48)
n.

n>no

We can talk about expectations under the distribution P restricted to subsets of Ω. In particular  let
Pn be the restriction of P to Ω(cid:48)
n). As
for expectations with respect to Pn  for any function f : Ω → R  we deﬁne

n; that is  for any A ⊂ Ω  we have Pn(A) = P (A ∩ Ω(cid:48)

n)/P (Ω(cid:48)

(cid:90)

Enf =

1
P (Ω(cid:48)
n)

Ω(cid:48)

n

f (ω)P (dω).

Here is the main result of this section.
Theorem 2.6. Assume that γn = c/n  where c = co/(2(λ1 − λ2)) and co > 0. Pick any 0 < δ < 1
and select a schedule (no  o)  . . .   (nJ   J ) that satisﬁes the conditions

2 j ≤ j+1 ≤ 2j for 0 ≤ j < J  and J−1 ≤ 1

4

8ed   and 3

o = δ2
(nj+1 + 1) ≥ e5/co(nj + 1) for 0 ≤ j < J
o) ln(4/δ). Then Pr(Ω(cid:48)) ≥ 1 − δ.

as well as no ≥ (20c2B2/2
The ﬁrst step towards proving this theorem is bounding the moment-generating function of Ψn in
terms of that of Ψn−1.
Lemma 2.7. Suppose n > nj. Suppose also that γn = c/n  where c = co/(2(λ1 − λ2)). Then for
any t > 0 

(3)

(cid:104)

(cid:16)

(cid:16)

En[etΨn ] ≤ En

exp

tΨn−1

(cid:17)(cid:17)(cid:105)

exp

(cid:18) c2B2t(1 + 32t)

(cid:19)

4n2

.

1 − coj
n

6

We would like to use this result to bound En[Ψn] in terms of Em[Ψm] for m < n. The shift in
sample spaces is easily handled using the following observation.
Lemma 2.8. If g : R → R is nondecreasing  then En[g(Ψn−1)] ≤ En−1[g(Ψn−1)] for any n > no.

A repeated application of Lemmas 2.7 and 2.8 yields the following.
Lemma 2.9. Suppose that conditions (3) hold. Then for 0 ≤ j < J and any t > 0 
− 1
nj+1

Enj+1[etΨnj+1 ] ≤ exp

t(1 − j+1) − tj +

tc2B2(1 + 32t)

(cid:18) 1

nj

(cid:18)

4

(cid:19)(cid:19)

.

Now that we have bounds on the moment-generating functions of intermediate Ψn  we can apply
martingale deviation bounds  as in Lemma 2.4  to obtain the following  from which Theorem 2.6
ensues.
Lemma 2.10. Assume conditions (3) hold. Pick any 0 < δ < 1  and set no ≥ (20c2B2/2
Then

o) ln(4/δ).

(cid:33)

(cid:32)

J(cid:88)

Pnj

sup
n≥nj

Ψn > 1 − j

≤ δ
2

.

j=1

2.4 The ﬁnal epoch

Recall the deﬁnition of the intermediate goals (nj  j) in (2)  (3). The ﬁnal epoch is the period
n ≥ nJ  at which point Ψn ≤ 1/2. The following consequence of Lemmas ?? and 2.8 captures the
rate at which Ψ decreases during this phase.
Lemma 2.11. For all n > nJ 

En[Ψn] ≤ (1 − αn)En−1[Ψn−1] + βn 

where αn = (λ1 − λ2)γn and βn = (B2/4)γ2
n.
By solving this recurrence relation  and piecing together the various epochs  we get the overall
convergence result of Theorem 1.1.
Note that Lemma 2.11 closely resembles the recurrence relation followed by the squared L2 distance
from the optimum of stochastic gradient descent (SGD) on a strongly convex function [11]. As
Ψn → 0  the incremental PCA algorithms we study have convergence rates of the same form as
SGD in this scenario.

3 Experiments

When performing PCA in practice with massive d and a large/growing dataset  an incremental
method like that of Krasulina or Oja remains practically viable  even as quadratic-time and -memory
algorithms become increasingly impractical. Arora et al. [2] have a more complete discussion of
the empirical necessity of incremental PCA algorithms  including a version of Oja’s method which
is shown to be extremely competitive in practice.
Since the efﬁciency beneﬁts of these types of algorithms are well understood  we now instead focus
on the effect of the learning rate on the performance of Oja’s algorithm (results for Krasulina’s are
extremely similar). We use the CMU PIE faces [13]  consisting of 11554 images of size 32 × 32 
as a prototypical example of a dataset with most of its variance captured by a few PCs  as shown in
Fig. 1. We set n0 = 0.
We expect from Theorem 1.1 and the discussion in the introduction that varying c (the constant in
the learning rate) will inﬂuence the overall rate of convergence. In particular  if c is low  then halving
it can be expected to halve the exponent of n  and the slope of the log-log convergence graph (ref.
the remark after Thm. 1.1). This is exactly what occurs in practice  as illustrated in Fig. 2. The
dotted line in that ﬁgure is a convergence rate of 1/n  drawn as a guide.

7

Figures 1 and 2.

4 Open problems

Several fundamental questions remain unanswered. First  the convergence rates of the two incre-
mental schemes depend on the multiplier c in the learning rate γn. If it is too low  convergence will
be slower than O(1/n). If it is too high  the constant in the rate of convergence will be large. Is
there a simple and practical scheme for setting c?
Second  what can be said about incrementally estimating the top p eigenvectors  for p > 1? Both
methods we consider extend easily to this case [10]; the estimate at time n is a d × p matrix Vn
n Vn = Ip always maintained.
whose columns correspond to the eigenvectors  with the invariant V T
In Oja’s algorithm  for instance  when a new data point Xn ∈ Rd arrives  the following update is
performed:

Wn = Vn−1 + γnXnX T
Vn = orth(Wn)

n Vn−1

where the second step orthonormalizes the columns  for instance by Gram-Schmidt. It would be
interesting to characterize the rate of convergence of this scheme.
Finally  our analysis applies to a modiﬁed procedure in which the starting time no is artiﬁcially set
to a large constant. This seems unnecessary in practice  and it would be useful to extend the analysis
to the case where no = 0.

Acknowledgments

The authors are grateful to the National Science Foundation for support under grant IIS-1162581.

References
[1] A. Agarwal  O. Chapelle  M. Dud´ık  and J. Langford. A reliable effective terascale linear

learning system. CoRR  abs/1110.4198  2011.

[2] R. Arora  A. Cotter  K. Livescu  and N. Srebro. Stochastic optimization for PCA and PLS.
In 50th Annual Allerton Conference on Communication  Control  and Computing  pages 861–
868. 2012.

[3] R. Arora  A. Cotter  and N. Srebro. Stochastic optimization of PCA with capped MSG. In

Advances in Neural Information Processing Systems  2013.

[4] G. Blanchard  O. Bousquet  and L. Zwald. Statistical properties of kernel principal component

analysis. Machine Learning  66(2-3):259–294  2007.

[5] T. T. Cai  Z. Ma  and Y. Wu. Sparse PCA: Optimal rates and adaptive estimation. CoRR 

abs/1211.1309  2012.

8

0510152025300500100015002000250030003500400045005000Component NumberEigenvaluePIE Dataset Covariance Spectrum10010110210310410510−610−510−410−310−210−1100Iteration NumberReconstruction ErrorOja Subspace Rule Dependence on c  c=6c=3c=1.5c=1c=0.666c=0.444c=0.296[6] R. Durrett. Probability: Theory and Examples. Duxbury  second edition  1995.
[7] T.P. Krasulina. A method of stochastic approximation for the determination of the least eigen-
value of a symmetrical matrix. USSR Computational Mathematics and Mathematical Physics 
9(6):189–195  1969.

[8] I. Mitliagkas  C. Caramanis  and P. Jain. Memory limited  streaming PCA. In Advances in

Neural Information Processing Systems  2013.

[9] E. Oja. Subspace Methods of Pattern Recognition. Research Studies Press  1983.
[10] E. Oja and J. Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of
the expectation of a random matrix. Journal of Math. Analysis and Applications  106:69–84 
1985.

[11] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex

stochastic optimization. In International Conference on Machine Learning  2012.

[12] S. Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing

Systems  1997.

[13] T. Sim  S. Baker  and M. Bsat. The CMU pose  illumination  and expression database. IEEE

Transactions on Pattern Analysis and Machine Intelligence  25(12):1615–1618  2003.

[14] V.Q. Vu and J. Lei. Minimax rates of estimation for sparse PCA in high dimensions. Journal

of Machine Learning Research - Proceedings Track  22:1278–1286  2012.

[15] M.K. Warmuth and D. Kuzmin. Randomized PCA algorithms with regret bounds that are

logarithmic in the dimension. In Advances in Neural Information Processing Systems. 2007.

[16] J. Weng  Y. Zhang  and W.-S. Hwang. Candid covariance-free incremental principal compo-
nent analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence  25(8):1034–
1040  2003.

[17] L. Zwald and G. Blanchard. On the convergence of eigenspaces in kernel principal component

analysis. In Advances in Neural Information Processing Systems  2005.

9

,Akshay Balsubramani
Sanjoy Dasgupta
Yoav Freund
Mahdi Milani Fard
Kevin Canini
Andrew Cotter
Jan Pfeifer
Maya Gupta