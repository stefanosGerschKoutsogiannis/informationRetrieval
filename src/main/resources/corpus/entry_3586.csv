2015,Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms,This paper studies the generalization performance of multi-class classification algorithms  for which we obtain  for the first time  a data-dependent generalization error bound with a logarithmic dependence on the class size  substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on lp-norm regularization  where the parameter p controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.,Multi-class SVMs: From Tighter Data-Dependent

Generalization Bounds to Novel Algorithms

Yunwen Lei

Department of Mathematics
City University of Hong Kong
yunwelei@cityu.edu.hk

¨Ur¨un Dogan

Microsoft Research

Cambridge CB1 2FB  UK

udogan@microsoft.com

Alexander Binder

ISTD Pillar

Singapore University of Technology and Design

Machine Learning Group  TU Berlin

alexander binder@sutd.edu.sg

Marius Kloft

Department of Computer Science
Humboldt University of Berlin

kloft@hu-berlin.de

Abstract

This paper studies the generalization performance of multi-class classiﬁcation al-
gorithms  for which we obtain—for the ﬁrst time—a data-dependent generaliza-
tion error bound with a logarithmic dependence on the class size  substantially
improving the state-of-the-art linear dependence in the existing data-dependent
generalization analysis. The theoretical analysis motivates us to introduce a new
multi-class classiﬁcation machine based on (cid:96)p-norm regularization  where the pa-
rameter p controls the complexity of the corresponding bounds. We derive an
efﬁcient optimization algorithm based on Fenchel duality theory. Benchmarks on
several real-world datasets show that the proposed algorithm can achieve signiﬁ-
cant accuracy gains over the state of the art.

1

Introduction

Typical multi-class application domains such as natural language processing [1]  information re-
trieval [2]  image annotation [3] and web advertising [4] involve tens or hundreds of thousands of
classes  and yet these datasets are still growing [5]. To handle such learning tasks  it is essential
to build algorithms that scale favorably with respect to the number of classes. Over the past years 
much progress in this respect has been achieved on the algorithmic side [4–7]  including efﬁcient
stochastic gradient optimization strategies [8].
Although also theoretical properties such as consistency [9–11] and ﬁnite-sample behavior [1  12–
15] have been studied  there still is a discrepancy between algorithms and theory in the sense that the
corresponding theoretical bounds do often not scale well with respect to the number of classes. This
discrepancy occurs the most strongly in research on data-dependent generalization bounds  that is 
bounds that can measure generalization performance of prediction models purely from the training
samples  and which thus are very appealing in model selection [16]. A crucial advantage of these
bounds is that they can better capture the properties of the distribution that has generated the data 
which can lead to tighter estimates [17] than conservative data-independent bounds.
To our best knowledge  for multi-class classiﬁcation  the ﬁrst data-dependent error bounds were
given by [14]. These bounds exhibit a quadratic dependence on the class size and were used by [12]
and [18] to derive bounds for kernel-based multi-class classiﬁcation and multiple kernel learning
(MKL) problems  respectively. More recently  [13] improve the quadratic dependence to a linear
dependence by introducing a novel surrogate for the multi-class margin that is independent on the
true realization of the class label.

1

However  a heavy dependence on the class size  such as linear or quadratic  implies a poor gen-
eralization guarantee for large-scale multi-class classiﬁcation problems with a massive number of
classes. In this paper  we show data-dependent generalization bounds for multi-class classiﬁcation
problems that—for the ﬁrst time—exhibit a sublinear dependence on the number of classes. Choos-
ing appropriate regularization  this dependence can be as mild as logarithmic. We achieve these
improved bounds via the use of Gaussian complexities  while previous bounds are based on a well-
known structural result on Rademacher complexities for classes induced by the maximum operator.
The proposed proof technique based on Gaussian complexities exploits potential coupling among
different components of the multi-class classiﬁer  while this fact is ignored by previous analyses.
The result shows that the generalization ability is strongly impacted by the employed regularization.
Which motivates us to propose a new learning machine performing block-norm regularization over
the multi-class components. As a natural choice we investigate here the application of the proven (cid:96)p
norm [19]. This results in a novel (cid:96)p-norm multi-class support vector machine (MC-SVM)  which
contains the classical model by Crammer & Singer [20] as a special case for p = 2. The bounds
indicate that the parameter p crucially controls the complexity of the resulting prediction models.
We develop an efﬁcient optimization algorithm for the proposed method based on its Fenchel dual
representation. We empirically evaluate its effectiveness on several standard benchmarks for multi-
class classiﬁcation taken from various domains  where the proposed approach signiﬁcantly outper-
forms the state-of-the-art method of [20].
The remainder of this paper is structured as follows. Section 2 introduces the problem setting and
presents the main theoretical results. Motivated by which we propose a new multi-class classiﬁcation
model in Section 3 and give an efﬁcient optimization algorithm based on Fenchel duality theory. In
Section 4 we evaluate the approach for the application of visual image recognition and on several
standard benchmark datasets taken from various application domains. Section 5 concludes.

2 Theory
2.1 Problem Setting and Notations
This paper considers multi-class classiﬁcation problems with c ≥ 2 classes. Let X denote the input
space and Y = {1  2  . . .   c} denote the output space. Assume that we are given a sequence of ex-
amples S = {(x1  y1)  . . .   (xn  yn)} ∈ (X × Y)n  independently drawn according to a probability
measure P deﬁned on the sample space Z = X × Y. Based on the training examples S  we wish to
learn a prediction rule h from a space H of hypotheses mapping from Z to R and use the mapping
x → arg maxy∈Y h(x  y) to predict (ties are broken by favoring classes with a lower index  for
which our loss function deﬁned below always counts an error). For any hypothesis h ∈ H  the mar-
gin ρh(x  y) of the function h at a labeled example (x  y) is ρh(x  y) := h(x  y)− maxy(cid:48)(cid:54)=y h(x  y(cid:48)).
The prediction rule h makes an error at (x  y) if ρh(x  y) ≤ 0 and thus the expected risk incurred
from using h for prediction is R(h) := E[1ρh(x y)≤0].
(h1  . . .   hc) with hj(x) = h(x  j) ∀j = 1  . . .   c. We denote by (cid:101)H := {ρh : h ∈ H} the class
Any function h : X × Y → R can be equivalently represented by the vector-valued function
of margin functions associated to H. Let k : X × X → R be a Mercer kernel with φ being the
associated feature map  i.e.  k(x  ˜x) = (cid:104)φ(x)  φ(˜x)(cid:105) for all x  ˜x ∈ X . We denote by (cid:107) · (cid:107)∗ the dual
norm of (cid:107) · (cid:107)  i.e.  (cid:107)w(cid:107)∗ := sup(cid:107) ¯w(cid:107)≤1(cid:104)w  ¯w(cid:105). For a convex function f  we denote by f∗ its Fenchel
by (cid:107)w(cid:107)2 p := [(cid:80)c
conjugate  i.e.  f∗(v) := supw[(cid:104)w  v(cid:105) − f (w)]. For any w = (w1  . . .   wc) we deﬁne the (cid:96)2 p-norm
2]1/p. For any p ≥ 1  we denote by p∗ the dual exponent of p satisfying
1/p + 1/p∗ = 1 and ¯p := p(2 − p)−1. We require the following deﬁnitions.
Deﬁnition 1 (Strong Convexity). A function f : X → R is said to be β-strongly convex w.r.t. a
norm (cid:107) · (cid:107) iff ∀x  y ∈ X and ∀α ∈ (0  1)  we have

j=1 (cid:107)wj(cid:107)p

f (αx + (1 − α)y) ≤ αf (x) + (1 − α)f (y) − β
2

α(1 − α)(cid:107)x − y(cid:107)2.

Deﬁnition 2 (Regular Loss). We call (cid:96) a L-regular loss if it satisﬁes the following properties:
(i) (cid:96)(t) bounds the 0-1 loss from above: (cid:96)(t) ≥ 1t≤0;
(ii) (cid:96) is L-Lipschitz in the sense |(cid:96)(t1) − (cid:96)(t2)| ≤ L|t1 − t2|;

2

(iii) (cid:96)(t) is decreasing and it has a zero point c(cid:96)  i.e.  (cid:96)(c(cid:96)) = 0.
Some examples of L-regular loss functions include the hinge (cid:96)h(t) = (1 − t)+ and the margin loss
(1)

(cid:96)ρ(t) = 1t≤0 + (1 − tρ−1)10<t≤ρ 

ρ > 0.

2.2 Main results
Our discussion on data-dependent generalization error bounds is based on the established method-
ology of Rademacher and Gaussian complexities [21].
Deﬁnition 3 (Rademacher and Gaussian Complexity). Let H be a family of real-valued functions
deﬁned on Z and S = (z1  . . .   zn) a ﬁxed sample of size n with elements in Z. Then  the empirical
Rademacher and Gaussian complexities of H with respect to the sample S are deﬁned by

(cid:2) sup

h∈H

1
n

n(cid:88)

i=1

σih(zi)(cid:3)  GS(H) = Eg

(cid:2) sup

h∈H

1
n

n(cid:88)

i=1

gih(zi)(cid:3) 

RS(H) = Eσ

where σ1  . . .   σn are independent random variables with equal probability taking values +1 or −1 
and g1  . . .   gn are independent N (0  1) random variables.

Note that we have the following comparison inequality relating Rademacher and Gaussian complex-
ities (Cf. Section 4.2 in [22]):

Existing work on data-dependent generalization bounds for multi-class classiﬁers [12–14  18] builds
on the following structural result on Rademacher complexities (e.g.  [12]  Lemma 8.1):

(cid:114) π

(cid:114) π

RS(H) ≤

GS(H) ≤ 3

(cid:112)log nRS(H).
RS(max{h1  . . .   hc} : hj ∈ Hj  j = 1  . . .   c) ≤ c(cid:88)

2

2

RS(Hj) 

j=1

(2)

(3)

where H1  . . .   Hc are c hypothesis sets. This result is crucial for the standard generalization analysis
of multi-class classiﬁcation since the margin ρh involves the maximum operator  which is removed
by (3)  but at the expense of a linear dependency on the class size. In the following we show that this
linear dependency is suboptimal because (3) does not take into account the coupling among different
classes. For example  a common regularizer used in multi-class learning algorithms is r(h) =
2 [20]  for which the components h1  . . .   hc are correlated via a (cid:107) · (cid:107)2 2 regularizer  and

(cid:80)c
j=1 (cid:107)hj(cid:107)2

the bound (3) ignoring this correlation would not be effective in this case [12–14  18].
As a remedy  we here introduce a new structural complexity result on function classes induced
by general classes via the maximum operator  while allowing to preserve the correlations among
different components meanwhile.
Instead of considering the Rademacher complexity  Lemma 4
concerns the structural relationship of Gaussian complexities since it is based on a comparison result
among different Gaussian processes.
Lemma 4 (Structural result on Gaussian complexity). Let H be a class of functions deﬁned on
X × Y with Y = {1  . . .   c}. Let g1  . . .   gnc be independent N (0  1) distributed random variables.
Then  for any sample S = {x1  . . .   xn} of size n  we have

(cid:0){max{h1  . . .   hc} : h = (h1  . . .   hc) ∈ H}(cid:1) ≤ 1

GS

Eg

sup

h=(h1 ... hc)∈H

n

n(cid:88)

c(cid:88)

i=1

j=1

g(j−1)n+ihj(xi) 

(4)

where Eg denotes the expectation w.r.t. to the Gaussian variables g1  . . .   gnc.

The proof of Lemma 4 is given in Supplementary Material A. Equipped with Lemma 4  we are
now able to present a general data-dependent margin-based generalization bound. The proof of the
following results (Theorem 5  Theorem 7 and Corollary 8) is given in Supplementary Material B.
Theorem 5 (Data-dependent generalization bound for multi-class classiﬁcation). Let H ⊂ RX×Y
be a hypothesis class with Y = {1  . . .   c}. Let (cid:96) be a L-regular loss function and denote B(cid:96) :=
sup(x y) h (cid:96)(ρh(x  y)). Suppose that the examples S = {(x1  y1)  . . .   (xn  yn)} are independently

3

√
2L

n(cid:88)

c(cid:88)

i=1

j=1

n(cid:88)

i=1

n(cid:88)

(cid:115)

log 2
δ
2n

 

(cid:115)

drawn from a probability measure deﬁned on X × Y. Then  for any δ > 0  with probability at least
1 − δ  the following multi-class classiﬁcation generalization bound holds for any h ∈ H:

R(h) ≤ 1
n

(cid:96)(ρh(xi  yi)) +

2π

Eg

sup

h=(h1 ... hc)∈H

n

g(j−1)n+ihj(xi) + 3B(cid:96)

where g1  . . .   gnc are independent N (0  1) distributed random variables.
Remark 6. Under the same condition of Theorem 5  [12] derive the following data-dependent
generalization bound (Cf. Corollary 8.1 in [12]):

R(h) ≤ 1
n

(cid:96)(ρh(xi  yi)) +

4Lc

RS({x → h(x  y) : y ∈ Y  h ∈ H}) + 3B(cid:96)

n

i=1

dependence on c is governed by the term(cid:80)n

This linear dependence on c is due to the use of (3). For comparison  Theorem 5 implies that the
j=1 g(j−1)n+ihj(xi)  an advantage of which is that
the components h1  . . .   hc are jointly coupled. As we will see  this allows us to derive an improved
result with a favorable dependence on c  when a constraint is imposed on (h1  . . .   hc).

(cid:80)c

i=1

log 2
δ
2n

.

The following Theorem 7 applies the general result in Theorem 5 to kernel-based methods. The
hypothesis space is deﬁned by imposing a constraint with a general strongly convex function.
Theorem 7 (Data-dependent generalization bound for kernel-based multi-class learning algorithms
and MC-SVMs). Suppose that the hypothesis space is deﬁned by

H := Hf Λ = {hw = ((cid:104)w1  φ(x)(cid:105)  . . .  (cid:104)wc  φ(x)(cid:105)) : f (w) ≤ Λ} 

where f is a β-strongly convex function w.r.t. a norm (cid:107)·(cid:107) deﬁned on H satisfying f∗(0) = 0. Let (cid:96) be
a L-regular loss function and denote B(cid:96) := sup(x y) h (cid:96)(ρh(x  y)). Let g1  . . .   gnc be independent
N (0  1) distributed random variables. Then  for any δ > 0  with probability at least 1 − δ we have

(cid:118)(cid:117)(cid:117)(cid:116) πΛ

β

n(cid:88)

i=1

Eg

(cid:13)(cid:13)(cid:13)(cid:0)g(j−1)n+iφ(xi)(cid:1)

(cid:115)

(cid:13)(cid:13)(cid:13)2

j=1 ... c

∗ + 3B(cid:96)

log 2
δ
2n

.

R(hw) ≤ 1
n

(cid:96)(ρhw (xi  yi)) +

4L
n

n(cid:88)

i=1

We now consider the following speciﬁc hypothesis spaces using a (cid:107) · (cid:107)2 p constraint:

Hp Λ := {hw = ((cid:104)w1  φ(x)(cid:105)  . . .  (cid:104)wc  φ(x)(cid:105)) : (cid:107)w(cid:107)2 p ≤ Λ} 

(5)
Corollary 8 ((cid:96)p-norm MC-SVM generalization bound). Let (cid:96) be a L-regular loss function and
denote B(cid:96) := sup(x y) h (cid:96)(ρh(x  y)). Then  with probability at least 1 − δ  for any hw ∈ Hp Λ the
generalization error R(hw) can be upper bounded by:
n(cid:88)

1 ≤ p ≤ 2.

(cid:115)

2LΛ

2 log c   if p∗ ≥ 2 log c 
e(4 log c)1+ 1
p∗  

otherwise.

p∗

c

1

(cid:40)√
(cid:0)2p∗(cid:1)1+ 1

log 2
δ
2n

+

n

(cid:96)(ρhw (xi  yi))+3B(cid:96)

k(xi  xi) ×

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

1
n

i=1

Remark 9. The bounds in Corollary 8 enjoy a mild dependence on the number of classes. The
dependence is polynomial with exponent 1/p∗ for 2 < p∗ < 2 log c and becomes logarithmic if p∗ ≥
2 log c. Even in the theoretically unfavorable case of p = 2 [20]  the bounds still exhibit a radical
dependence on the number of classes  which is substantially milder than the quadratic dependence
established in [12  14  18] and the linear dependence established in [13]. Our generalization bound
is data-dependent and shows clearly how the margin would affect the generalization performance
(when (cid:96) is the margin loss (cid:96)ρ): a large margin ρ would increase the empirical error while decrease
the model’s complexity  and vice versa.

2.3 Comparison of the Achieved Bounds to the State of the Art
Related work on data-independent bounds. The large body of theoretical work on multi-class
learning considers data-independent bounds. Based on the (cid:96)∞-norm covering number bound of
linear operators  [15] obtain a generalization bound exhibiting a linear dependence on the class size 
√
which is improved by [9] to a radical dependence of the form O(n− 1
ρ ). Under conditions

2 (log

2 n)

c

3

4

analogous to Corollary 8  [23] derive a class-size independent generalization guarantee. However 
their bound is based on a delicate deﬁnition of margin  which is why it is commonly not used in the
mainstream multi-class literature. [1] derive the following generalization bound

(cid:16)

E(cid:104) 1

p

log

1 +

ep(ρ−(cid:104) ˆwy− ˆw ˜y φ(x)(cid:105))(cid:17)(cid:105) ≤ inf

w∈H

E(cid:104) 1

p

(cid:88)

˜y(cid:54)=y

(cid:16)

(cid:88)

log

λn

1 +

˜y(cid:54)=y
(cid:107)w(cid:107)2

2 2

+

2(n + 1)

ep(ρ−(cid:104)wy−w ˜y φ(x)(cid:105))(cid:17)
(cid:105)

2 supx∈X k(x  x)

+

λn

 

(6)

(cid:80)n

where ρ is a margin condition  p > 0 a scaling factor  and λ a regularization parameter. Eq. (6) is
class-size independent  yet Corollary 8 shows superiority in the following aspects: ﬁrst  for SVMs
(i.e.  margin loss (cid:96)ρ)  our bound consists of an empirical error ( 1
i=1 (cid:96)ρ(ρhw (xi  yi))) and a com-
n
plexity term divided by the margin value (note that L = 1/ρ in Corollary 8). When the margin
is large (which is often desirable) [14]  the last term in the bound given by Corollary 8 becomes
small  while—on the contrary—-the bound (6) is an increasing function of ρ  which is undesirable.
Secondly  Theorem 7 applies to general loss functions  expressed through a strongly convex func-
tion over a general hypothesis space  while the bound (6) only applies to a speciﬁc regularization
algorithm. Lastly  all the above mentioned results are conservative data-independent estimates.
Related work on data-dependent bounds. The techniques used in above mentioned papers do not
straightforwardly translate to data-dependent bounds  which is the type of bounds in the focus of
the present work. The investigation of these was initiated  to our best knowledge  by [14]: with the
structural complexity bound (3) for function classes induced via the maximal operator  [14] derive a
margin bound admitting a quadratic dependency on the number of classes. [12] use these results in
[14] to study the generalization performance of MC-SVMs  where the components h1  . . .   hc are
coupled with an (cid:107) · (cid:107)2 p  p ≥ 1 constraint. Due to the usage of the suboptimal Eq. (3)  [12] obtain
a margin bound growing quadratically w.r.t. the number of classes. [18] develop a new multi-class
classiﬁcation algorithm based on a natural notion called the multi-class margin of a kernel. [18]
also present a novel multi-class Rademacher complexity margin bound based on Eq. (3)  and the
bound also depends quadratically on the class size. More recently  [13] give a reﬁned Rademacher
complexity bound with a linear dependence on the class size. The key reason for this improvement
is the introduction of ρθ h := miny(cid:48)∈Y [h(x  y)−h(x  y(cid:48))+θ1y(cid:48)=y] bounding margin ρh from below 
and since the maximum operation in ρθ h is applied to the set Y rather than the subset Y − {yi} for
ρh  one needs not to consider the random realization of yi. We also use this trick in our proof of
Theorem 5. However  [13] fail to improve this linear dependence to a logarithmic dependence  as
we achieved in Corollary 8  due to the use of the suboptimal structural result (3).

3 Algorithms
Motivated by the generalization analysis given in Section 2  we now present a new multi-class
learning algorithm  based on performing empirical risk minimization in the hypothesis space (5).
This corresponds to the following (cid:96)p-norm MC-SVM (1 ≤ p ≤ 2):
Problem 10 (Primal problem: (cid:96)p-norm MC-SVM).

(cid:105) 2

p

(cid:104) c(cid:88)

n(cid:88)
min
s.t. ti = (cid:104)wyi  φ(xi)(cid:105) − max
y(cid:54)=yi

(cid:107)wj(cid:107)p

+ C

1
2

j=1

i=1

w

2

(cid:96)(ti) 
(cid:104)wy  φ(xi)(cid:105) 

(P)

For p = 2 we recover the seminal multi-class algorithm by Crammer & Singer [20] (CS)  which is
thus a special case of the proposed formulation. An advantage of the proposed approach over [20]
can be that  as shown in Corollary 8  the dependence of the generalization performance on the class
size becomes milder as p decreases to 1.

3.1 Dual problems
Since the optimization problem (P) is convex  we can derive the associated dual problem for the
construction of efﬁcient optimization algorithms. The derivation of the following dual problem is
deferred to Supplementary Material C. For a matrix α ∈ Rn×c  we denote by αi the i-th row.
Denote by ej the j-th unit vector in Rc and 1 the vector in Rc with all components being zero.

5

(D)

Problem 11 (Completely dualized problem for general loss). The Lagrangian dual of (10) is:

(cid:104) c(cid:88)

(cid:13)(cid:13) n(cid:88)

αijφ(xi)(cid:13)(cid:13) p

p−1
2

(cid:105) 2(p−1)

p − C

n(cid:88)

i=1

j=1

i=1

s.t. αij ≤ 0 ∧ αi · 1 = 0 

∀j (cid:54)= yi  i = 1  . . .   n.

sup

α∈Rn×c

− 1
2

(cid:96)∗(− αiyi
C

)

Theorem 12 (REPRESENTER THEOREM). For any dual variable α ∈ Rn×c  the associated primal
variable w = (w1  . . .   wc) minimizing the Lagrangian saddle problem can be represented by:

wj =(cid:2) c(cid:88)

(cid:107) n(cid:88)

p∗ −1(cid:13)(cid:13) n(cid:88)
(cid:3) 2

αijφ(xi)(cid:13)(cid:13)p∗−2

2

(cid:2) n(cid:88)

αijφ(xi)(cid:3).

αi˜jφ(xi)(cid:107)p∗

2

˜j=1

i=1

i=1

i=1

For the hinge loss (cid:96)h(t) = (1 − t)+  we know its Fenchel-Legendre conjugate is (cid:96)∗
−1 ≤ t ≤ 0 and ∞ elsewise. Hence (cid:96)∗
we have the following dual problem for the hinge loss function:
Problem 13 (Completely dualized problem for the hinge loss ((cid:96)p-norm MC-SVM)).

h(t) = t if
C if −1 ≤ − αiyi
C ≤ 0 and ∞ elsewise. Now
n(cid:88)

h(− αiyi
(cid:104) c(cid:88)

C ) = − αiyi

(cid:105) 2(p−1)

p

(cid:13)(cid:13) n(cid:88)

αijφ(xi)(cid:13)(cid:13) p

p−1
2

αiyi

+

sup

α∈Rn×c

− 1
2

(7)

j=1

i=1

s.t. αi ≤ eyi · C ∧ αi · 1 = 0 

i=1

∀i = 1  . . .   n.

3.2 Optimization Algorithms
The dual problems (D) and (7) are not quadratic programs for p (cid:54)= 2  and thus generally not easy to
solve. To circumvent this difﬁculty  we rewrite Problem 10 as the following equivalent problem:

n(cid:88)

c(cid:88)
min
w β
s.t. ti ≤ (cid:104)wyi   φ(xi)(cid:105) − (cid:104)wy  φ(xi)(cid:105) 

(cid:107)wj(cid:107)2
2βj

(cid:96)(ti)

+ C

j=1

i=1

2

(cid:107)β(cid:107) ¯p ≤ 1  ¯p = p(2 − p)−1  βj ≥ 0.

y (cid:54)= yi  i = 1  . . .   n 

(8)

The class weights β1  . . .   βc in Eq. (8) play a similar role as the kernel weights in (cid:96)p-norm MKL
algorithms [19]. The equivalence between problem (P) and Eq. (8) follows directly from Lemma 26
in [24]  which shows that the optimal β = (β1  . . .   βc) in Eq. (8) can be explicitly represented in
closed form. Motivated by the recent work on (cid:96)p-norm MKL  we propose to solve the problem (8)
via alternately optimizing w and β. As we will show  given temporarily ﬁxed β  the optimization
of w reduces to a standard multi-class classiﬁcation problem. Furthermore  the update of β  given
ﬁxed w  can be achieved via an analytic formula.
Problem 14 (Partially dualized problem for a general loss). For ﬁxed β  the partial dual problem
for the sub-optimization problem (8) w.r.t. w is

c(cid:88)

(cid:13)(cid:13) n(cid:88)

n(cid:88)

sup

2 − C
(cid:96)∗(− αiyi
)
C
∀j (cid:54)= yi  i = 1  . . .   n.
The primal variable w minimizing the associated Lagrangian saddle problem is

s.t. αij ≤ 0 ∧ αi · 1 = 0 

− 1
2

α∈Rn×c

βj

j=1

i=1

i=1

αijφ(xi)(cid:13)(cid:13)2
n(cid:88)

i=1

wj = βj

αijφ(xi).

We defer the proof to Supplementary Material C. Analogous to Problem 13  we have the following
partial dual problem for the hinge loss.
Problem 15 (Partially dualized problem for the hinge loss ((cid:96)p-norm MC-SVM)).

sup

α∈Rn×c

f (α) := − 1
2

s.t. αi ≤ eyi · C ∧ αi · 1 = 0 

c(cid:88)

j=1

(cid:13)(cid:13) n(cid:88)

i=1

βj

αijφ(xi)(cid:13)(cid:13)2

n(cid:88)

2 +

αiyi

i=1

∀i = 1  . . .   n.

6

(9)

(10)

(11)

The Problems 14 and 15 are quadratic  so we can use the dual coordinate ascent algorithm [25] to
very efﬁciently solve them for the case of linear kernels. To this end  we need to compute the gradient
and solve the restricted problem of optimizing only one αi ∀i  keeping all other dual variables
ﬁxed [25]. The gradient of f can be exactly represented by w:

α˜ijk(xi  x˜i) + 1yi=j = 1yi=j − (cid:104)wj  φ(xi)(cid:105).

(12)

n(cid:88)

˜i=1

∂f
∂αij

= −βj

Suppose the additive change to be applied to the current αi is δαi  then from (12) we have

c(cid:88)

j=1

∂f
∂αij

δαij − 1
2

c(cid:88)

j=1

f (α1  . . .   αi−1  αi + δαi  αi+1  . . .   αn) =

Therefore  the sub-problem of optimizing δαi is given by

c(cid:88)

c(cid:88)
max
δαi
s.t. δαi ≤ eyi · C − αi ∧ δαi · 1 = 0.

βjk(xi  xi)[δαij]2 +

− 1
2

j=1

j=1

∂f
∂αij

βjk(xi  xi)[δαij]2 + const.

δαij

(13)

(cid:18) c(cid:88)

(cid:19) p−2

p

We now consider the subproblem of updating class weights β with temporarily ﬁxed w  for which
we have the following analytic solution. The proof is deferred to the Supplementary Material C.1.
Proposition 16. (Solving the subproblem with respect to the class weights) Given ﬁxed wj  the
minimal βj optimizing the problem (8) is attained at

βj = (cid:107)wj(cid:107)2−p

2

(cid:107)w˜j(cid:107)p

2

.

(14)

˜j=1

The update of βj based on Eq. (14) requires calculating (cid:107)wj(cid:107)2
recalling the representation established in Eq. (10).
The resulting training algorithm for the proposed (cid:96)p-norm MC-SVM is given in Algorithm 1. The
algorithm alternates between solving a MC-SVM problem for ﬁxed class weights (Line 3) and up-
dating the class weights in a closed-form manner (Line 5). Recall that Problem 11 establishes a
completely dualized problem  which can be used as a sound stopping criterion for Algorithm 1.

2  which can be easily fulﬁlled by

Algorithm 1: Training algorithm for (cid:96)p-norm MC-SVM.
input: examples {(xi  yi)n

initialize βj = ¯p(cid:112)1/c  wj = 0 for all j = 1  . . .   c

i=1} and the kernel k.

while Optimality conditions are not satisﬁed do

optimize the multi-class classiﬁcation problem (9)
compute (cid:107)wj(cid:107)2
update βj for all j = 1  . . .   c  according to Eq. (14)

2 for all j = 1  . . .   c  according to Eq. (10)

end

4 Empirical Analysis

We implemented the proposed (cid:96)p-norm MC-SVM algorithm (Algorithm 1) in C++ and solved the
involved MC-SVM problem using dual coordinate ascent [25]. We experiment on six benchmark
datasets: the Sector dataset studied in [26]  the News 20 dataset collected by [27]  the Rcv1 dataset
collected by [28]  the Birds 15  Birds 50 as a part from [29] and the Caltech 256 collected by
grifﬁn2007caltech. We used fc6 features from the BVLC reference caffenet from [30]. Table 1
gives an information on these datasets.
We compare with the classical CS in [20]  which constitutes a strong baseline for these datasets
[25]. We employ a 5-fold cross validation on the training set to tune the regularization parameter
C by grid search over the set {2−12  2−11  . . .   212} and p from 1.1 to 2 with 10 equidistant points.
We repeat the experiments 10 times  and report in Table 2 on the average accuracy and standard
deviations attained on the test set.

7

Dataset
Sector
News 20

Rcv1

Birds 15
Birds 50

Caltech 256

No. of Classes No. of Training Examples No. of Test Examples No. of Attributes

105
20
53
200
200
256

6  412
15  935
15  564
3  000
9  958
12  800

3  207
3  993

518  571

8  788
1  830
16  980

55  197
62  060
47  236
4  096
4  096
4  096

Table 1: Description of datasets used in the experiments.

Sector

Method / Dataset
Caltech 256
(cid:96)p-norm MC-SVM 94.20±0.34 86.19±0.12 85.74±0.71 13.73±1.4 27.86±0.2 56.00±1.2
Crammer & Singer 93.89±0.27
54.96±1.1
Table 2: Accuracies achieved by CS and the proposed (cid:96)p-norm MC-SVM on the benchmark datasets.

News 20
85.12±0.29

Birds 50
26.28±0.3

Birds 15
12.53±1.6

Rcv1

85.21±0.32

We observe that the proposed (cid:96)p-norm MC-SVM consistently outperforms CS [20] on all considered
datasets. Speciﬁcally  our method attains 0.31% accuracy gain on Sector  1.07% accuracy gain on
News 20  0.53% accuracy gain on Rcv1  1.2% accuracy gain on Birds 15  1.58% accuracy gain on
Birds 50  and 1.04% accuracy gain on Birds 15. We perform a Wilcoxon signed rank test between
the accuracies of CS and our method on the benchmark datasets  and the p-value is 0.03  which
means our method is signiﬁcantly better than CS at the signiﬁcance level of 0.05. These promising
results indicate that the proposed (cid:96)p-norm MC-SVM could further lift the state of the art in multi-
class classiﬁcation  even in real-world applications beyond the ones studied in this paper.

5 Conclusion

Motivated by the ever growing size of multi-class datasets in real-world applications such as im-
age annotation and web advertising  which involve tens or hundreds of thousands of classes  we
studied the inﬂuence of the class size on the generalization behavior of multi-class classiﬁers. We
focus here on data-dependent generalization bounds enjoying the ability to capture the properties of
the distribution that has generated the data. Of independent interest  for hypothesis classes that are
given as a maximum over base classes  we developed a new structural result on Gaussian complex-
ities that is able to preserve the coupling among different components  while the existing structural
results ignore this coupling and may yield suboptimal generalization bounds. We applied the new
structural result to study learning rates for multi-class classiﬁers  and derived  for the ﬁrst time  a
data-dependent bound with a logarithmic dependence on the class size  which substantially outper-
forms the linear dependence in the state-of-the-art data-dependent generalization bounds.
Motivated by the theoretical analysis  we proposed a novel (cid:96)p-norm MC-SVM  where the parameter
p controls the complexity of the corresponding bounds. This class of algorithms contains the classi-
cal CS [20] as a special case for p = 2. We developed an effective optimization algorithm based on
the Fenchel dual representation. For several standard benchmarks taken from various domains  the
proposed approach surpassed the state-of-the-art method of CS [20] by up to 1.5%.
A future direction will be to derive a data-dependent bound that is completely independent of the
class size (even overcoming the mild logarithmic dependence here). To this end  we will study more
powerful structural results than Lemma 4 for controlling complexities of function classes induced
via the maximum operator. As a good starting point  we will consider (cid:96)∞-norm covering numbers.

Acknowledgments

We thank Mehryar Mohri for helpful discussions. This work was partly funded by the German
Research Foundation (DFG) award KL 2698/2-1.

References
[1] T. Zhang  “Class-size independent generalization analsysis of some discriminative multi-category classi-

ﬁcation ” in Advances in Neural Information Processing Systems  pp. 1625–1632  2004.

[2] T. Hofmann  L. Cai  and M. Ciaramita  “Learning with taxonomies: Classifying documents and words ”

in NIPS workshop on syntax  semantics  and statistics  2003.

8

[3] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei  “Imagenet: A large-scale hierarchical
image database ” in Computer Vision and Pattern Recognition  2009. CVPR 2009. IEEE Conference on 
pp. 248–255  IEEE  2009.

[4] A. Beygelzimer  J. Langford  Y. Lifshits  G. Sorkin  and A. Strehl  “Conditional probability tree estimation

analysis and algorithms ” in Proceedings of UAI  pp. 51–58  AUAI Press  2009.

[5] S. Bengio  J. Weston  and D. Grangier  “Label embedding trees for large multi-class tasks ” in Advances

in Neural Information Processing Systems  pp. 163–171  2010.

[6] P. Jain and A. Kapoor  “Active learning for large multi-class problems ” in Computer Vision and Pattern

Recognition  2009. CVPR 2009. IEEE Conference on  pp. 762–769  IEEE  2009.

[7] O. Dekel and O. Shamir  “Multiclass-multilabel classiﬁcation with more classes than examples ” in Inter-

national Conference on Artiﬁcial Intelligence and Statistics  pp. 137–144  2010.

[8] M. R. Gupta  S. Bengio  and J. Weston  “Training highly multiclass classiﬁers ” The Journal of Machine

Learning Research  vol. 15  no. 1  pp. 1461–1492  2014.

[9] T. Zhang  “Statistical analysis of some multi-category large margin classiﬁcation methods ” The Journal

of Machine Learning Research  vol. 5  pp. 1225–1251  2004.

[10] A. Tewari and P. L. Bartlett  “On the consistency of multiclass classiﬁcation methods ” The Journal of

Machine Learning Research  vol. 8  pp. 1007–1025  2007.

[11] T. Glasmachers  “Universal consistency of multi-class support vector classiﬁcation ” in Advances in Neu-

ral Information Processing Systems  pp. 739–747  2010.

[12] M. Mohri  A. Rostamizadeh  and A. Talwalkar  Foundations of machine learning. MIT press  2012.
[13] V. Kuznetsov  M. Mohri  and U. Syed  “Multi-class deep boosting ” in Advances in Neural Information

Processing Systems  pp. 2501–2509  2014.

[14] V. Koltchinskii and D. Panchenko  “Empirical margin distributions and bounding the generalization error

of combined classiﬁers ” Annals of Statistics  pp. 1–50  2002.

[15] Y. Guermeur  “Combining discriminant models with new multi-class svms ” Pattern Analysis & Applica-

tions  vol. 5  no. 2  pp. 168–179  2002.

[16] L. Oneto  D. Anguita  A. Ghio  and S. Ridella  “The impact of unlabeled patterns in rademacher com-
plexity theory for kernel classiﬁers ” in Advances in Neural Information Processing Systems  pp. 585–593 
2011.

[17] V. Koltchinskii and D. Panchenko  “Rademacher processes and bounding the risk of function learning ”

in High Dimensional Probability II  pp. 443–457  Springer  2000.

[18] C. Cortes  M. Mohri  and A. Rostamizadeh  “Multi-class classiﬁcation with maximum margin multiple

kernel ” in ICML-13  pp. 46–54  2013.

[19] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien  “Lp-norm multiple kernel learning ” The Journal of

Machine Learning Research  vol. 12  pp. 953–997  2011.

[20] K. Crammer and Y. Singer  “On the algorithmic implementation of multiclass kernel-based vector ma-

chines ” The Journal of Machine Learning Research  vol. 2  pp. 265–292  2002.

[21] P. L. Bartlett and S. Mendelson  “Rademacher and gaussian complexities: Risk bounds and structural

results ” J. Mach. Learn. Res.  vol. 3  pp. 463–482  2002.

[22] M. Ledoux and M. Talagrand  Probability in Banach Spaces: isoperimetry and processes  vol. 23. Berlin:

Springer  1991.

Res.(JAIR)  vol. 30  pp. 525–564  2007.

Learning Research  pp. 1099–1125  2005.

[23] S. I. Hill and A. Doucet  “A framework for kernel-based multi-category classiﬁcation. ” J. Artif. Intell.

[24] C. A. Micchelli and M. Pontil  “Learning the kernel function via regularization ” Journal of Machine

[25] S. S. Keerthi  S. Sundararajan  K.-W. Chang  C.-J. Hsieh  and C.-J. Lin  “A sequential dual method for

large scale multi-class linear svms ” in 14th ACM SIGKDD  pp. 408–416  ACM  2008.

[26] J. D. Rennie and R. Rifkin  “Improving multiclass text classiﬁcation with the support vector machine ”

[27] K. Lang  “Newsweeder: Learning to ﬁlter netnews ” in Proceedings of the 12th international conference

tech. rep.  AIM-2001-026  MIT  2001.

on machine learning  pp. 331–339  1995.

[28] D. D. Lewis  Y. Yang  T. G. Rose  and F. Li  “Rcv1: A new benchmark collection for text categorization

research ” The Journal of Machine Learning Research  vol. 5  pp. 361–397  2004.

[29] P. Welinder  S. Branson  T. Mita  C. Wah  F. Schroff  S. Belongie  and P. Perona  “Caltech-UCSD Birds

200 ” Tech. Rep. CNS-TR-2010-001  California Institute of Technology  2010.

[30] Y. Jia  E. Shelhamer  J. Donahue  S. Karayev  J. Long  R. Girshick  S. Guadarrama  and T. Darrell  “Caffe:

Convolutional architecture for fast feature embedding ” arXiv preprint arXiv:1408.5093  2014.

9

,Yunwen Lei
Urun Dogan
Alexander Binder
Marius Kloft
Wenlin Wang
Chenyang Tao
Zhe Gan
Guoyin Wang
Liqun Chen
Xinyuan Zhang
Ruiyi Zhang
Qian Yang
Ricardo Henao
Lawrence Carin