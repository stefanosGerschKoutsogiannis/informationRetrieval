2019,Multivariate Distributionally Robust Convex Regression under Absolute Error Loss,This paper proposes a novel non-parametric multidimensional convex
regression estimator which is designed to be robust to adversarial
perturbations in the empirical measure. We minimize over convex functions
the maximum (over Wasserstein perturbations of the empirical measure) of the
absolute regression errors. The inner maximization is solved in closed form
resulting in a regularization penalty involves the norm of the gradient. We
show consistency of our estimator and a rate of convergence of order $
\widetilde{O}\left( n^{-1/d}\right) $  matching the bounds of alternative
estimators based on square-loss minimization. Contrary to all of the existing results  our convergence rates hold  without imposing compactness on the underlying domain and with no a priori bounds on the underlying convex function or its gradient norm.,Multivariate Distributionally Robust Convex

Regression under Absolute Error Loss

Jose Blanchet
Stanford MS&E

jose.blanchet@stanford.edu

Jun Yan

Stanford Statistics

junyan65@stanford.edu

Peter W. Glynn
Stanford MS&E

glynn@stanford.edu

Zhengqing Zhou

Stanford Mathematics
zqzhou@stanford.edu

Abstract

This paper proposes a novel non-parametric multidimensional convex regression
estimator which is designed to be robust to adversarial perturbations in the empirical
measure. We minimize over convex functions the maximum (over Wasserstein
perturbations of the empirical measure) of the absolute regression errors. The
inner maximization is solved in closed form resulting in a regularization penalty
involves the norm of the gradient. We show consistency of our estimator and a rate

of convergence of order (cid:101)O(cid:0)n−1/d(cid:1)  matching the bounds of alternative estimators

based on square-loss minimization. Contrary to all of the existing results  our
convergence rates hold without imposing compactness on the underlying domain
and with no a priori bounds on the underlying convex function or its gradient
norm.

1

Introduction

Convex regression estimation arises in a wide range of learning applications  for example  when
ﬁtting demand functions  production curves or utility functions  see [14  22  23]. Economic theory
often dictates that demand functions are concave  [2]. In ﬁnancial engineering  stock option prices
often exhibit convexity restrictions [1]. This paper introduces a novel convex regression estimator
which  by design  enjoys enhanced robustness properties. This estimator requires no a priori uniform
bounds on the underlying convex function or its Lipschitz constant  nor does our estimator require
that the domain of the convex function be compact  in contrast to existing convex function estimators
that have known convergence rate guarantees. Furthermore  our numerical experiments show that
our estimator exhibits good empirical performance  in comparison with existing estimators  and is a
promising alternative to existing methods.
Let X be a d-dimensional random vector and let Y be a scalar random variable. Given a sample
(X1  Y1) ···   (Xn  Yn) of i.i.d. copies of (X  Y )  we adopt the convex regression model

Yi = f∗(Xi) + Ei 

(1)
where f∗ : Rd → R is a (unknown) convex function and Ei is a zero-median random variable
independent of Xi  satisfying mild regularity conditions indicated in the sequel. Unlike the existing
literature on convex regression (or  more generally  shape-based regression)  we base our estimation
methodology not on minimizing the squared error loss  but on minimizing mean absolute error loss.
We adopt this viewpoint as a means of reducing the sensitivity of our regression estimator to outliers
in the data.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We further wish to regularize our estimator. One vehicle towards accomplishing this goal in a
principled fashion is to consider a distributionally robust formulation in which we robustify over a
Wasserstein ball around the data  using a diameter that is driven by consistency and convergence rate
considerations. When we do this  we arrive at a computationally tractable formulation of the problem
that can be solved as a linear program. This is to be contrasted against the quadratic program that
arises when minimizing squared error loss. Furthermore  the form of regularization that appears in
this problem involves a novel gradient-based penalization term  to be described in more detail later in
this Introduction.
In order to introduce our Wasserstein-based distributionally robust optimization formulation  we ﬁrst
recall how the Wasserstein distance is deﬁned.
First  let P(Rm × Rm) be the space of Borel probability measures deﬁned on Rm × Rm. Let Π (µ  ν)
be the subspace of P(Rm × Rm) with ﬁxed marginals given by µ and v  respectively. That is  if
U ∈ Rm  V ∈ Rm are random vectors with joint distribution π ∈ P(Rm × Rm)  then π ∈ Π (µ  ν) 
if the marginal distribution of U  πU   equals µ and the marginal distribution of V   πV   equals ν. The
Wasserstein distance between µ and ν is given by

D(µ  ν) := inf

Eπ [c (U  V )] : π ∈ P(Rm × Rm)  πU = µ  πV = ν

 

(cid:26)

(cid:27)

where c : Rm × Rm → [0 ∞] is a metric. In our setting  we have m = d + 1  and we will choose as
our metric

c ((x  y)   (x(cid:48)  y(cid:48))) = (cid:107)x − x(cid:48)(cid:107)1

1 (y = y(cid:48)) + ∞1 (y (cid:54)= y(cid:48)) .

(2)

We take the view here that distributional uncertainty is incorporated only in terms of the predictors
and not the responses  since the responses already include a measurement error (in the term E). This
type of cost function has been used in the literature  [6]  to exactly recover regularized estimators
such as sqrt-Lasso  among others. It is possible to add distributional uncertainty in the response. The
methods that we propose allow for adding distributional uncertainty in the response with only a small
variation in the form of the estimator and without any change in the learning rates or the assumptions
that we impose. Since the challenge here arises from the multidimensional aspect of the predictor
variable  we decided to mostly impose the distributional robustness on the predictors.
Now  consider a loss function l(y  z) : R × R → R  which is assumed to be convex and uniformly
Lipschitz. Our distributionally robust convex regression (DRCR) formulation takes the form 

inf
f∈F

sup

P∈P(Rd+1):D(P Pn)≤δ

EP [l(Y  f (X))]  

(3)

where F represents the class of convex and Lipschitz functions (formally deﬁned in Section 2.3)  the
parameter δ := δn > 0 is the uncertainty radius. This radius will be judiciously chosen as a function
of n to obtain consistency and suitable rates of convergence. The notation Pn encodes the empirical
distribution of the observations (X1  Y1) ···   (Xn  Yn)  namely 

Pn(dx  dy) :=

1
n

δ{(Xi Yi)}(dx  dy).

n(cid:88)

i=1

Distributionally robust optimization formulations such as (3) have been used in a wide range of
settings in the operations research literature and these formulations have become increasingly popular
in machine learning and statistics.
Our main contributions in this paper are as follows.

i) We provide a tractable formulation of (3)  in particular  we will show that

inf
f∈F

sup

P∈P(Rd+1):D(P Pn)≤δ

EP [l(Y  f (X))] = inf

f∈F {δL(cid:107)∇f(cid:107)∞ + EPn l(Y  f (X))}  

(4)

where (cid:107)∇f(cid:107)∞ is the largest l∞-norm of all subgradients of f (x) for all x  and similarly  L :=
sup(y z)∈R×R |∇zl(y  z)| (see Theorem 1). Note the penalty term is expressed in terms of the
norm of the gradient of the estimator. The appearence of the l∞-norm is intimately connected to
the choice of the l1 cost function given in (2).

2

conditions on the residuals (see Theorem 2) 

ii) Assuming that l (y  f (x)) = |y − f (x)|  we provide statistical guarantees for the rate of conver-
gence of the estimators obtained in (4)  improving upon the results obtained using a quadratic loss
. In particular  we show that if (cid:107)X(cid:107)γ∞ has a ﬁnite moment generating function in a neighborhood

of the origin for some γ > 0 and if δn is chosen to be (cid:101)O(cid:0)n−2/d(cid:1)  then  under suitable regularity
in a suitable sense  where (cid:98)fn δn ∈ arg inf f∈F {δnL(cid:107)∇f(cid:107)∞ + EPn l(Y  f (X))} and the notation
(cid:101)O(cid:0)n−1/d(cid:1) ignores poly-log factors in n. In contrast to the current results in the literature  our rate

(cid:98)fn δn = f∗ + (cid:101)O

(cid:16)

n−1/d(cid:17)

 

of convergence does not require X to have compact support  nor do we need to build an apriori
bound on the size of the gradient of f into our estimator in order to obtain convergence rate result.

Our contributions have several signiﬁcant features. First  it is not difﬁcult to see that choosing the
absolute error loss l (y  f (x)) = |y − f (x)| makes (4) equivalent to a linear programming problem.
In fact  since Pn is ﬁnitely supported  the problem becomes a ﬁnite dimensional linear programming
problem. Hence  this problem is  in principle  easier to solve than the standard quadratic problem that
arises in typical non-parametric convex regression formulations  which arise when minimizing the
squared error loss.
Second  our estimator is naturally endowed with desirable out-of-sample features due to the presence
of the inner maximization  which explores the impact on the loss function due to statistical variations
in the data. This interpretation follows from the left hand side of (4). The right hand side of (4)  on
the other hand  shows a direct connection to regularization in terms of the norm of the gradient of f 
and the resulting norm is the dual transportation cost. This regularization term  as we shall see  allows
us to construct an estimator that are free of a priori bounds imposed on the size of the gradient of
f  which typically are required in order to obtain statistical guarantees. We now provide a literature
review in the scientiﬁc areas touched by our contribution  namely  convex regression estimation and
distributionally robust optimization.

1.1 Related Literature

In the context of convex regression  the overwhelming majority of the literature focuses on empirical
least-squares estimators (leading to a quadratic programming formulation of the same size as the
linear programming formulation that we offer). In one dimension  the work of [11] proves the
consistency of the least squares estimator  and provides a rate of convergence of order O(n−2/5) and
an asymptotic distribution for this estimator; a matching upper and lower bounds for the min-max
risk (in terms of quadratic loss) was obtained in [12]  also with the same rate of order O(n−2/5) up
to a logarithmic factor. The ﬁrst consistency results in higher dimensional problems were obtained
in [16  19]. Associated rates of convergence have only been derived recently  in [3  13  15]  all of
which assume that the predictor takes values on a compact set. It is shown in these papers that a phase
transition occurs at d = 4. When d ≤ 4  the least squares estimator achieves the convergence rate of
n−2/(d+4)  which matches the optimal convergence rate in the non-parametric setting (when f∗ is a
twice continuously differentiable and the data is restricted to lie on a compact set). However  when
d > 4  the convergence rate of the least squares estimator deteriorates to O(n−1/d). Moreover  the
results in [15] and [3] require apriori knowledge on (cid:107)∇f∗(cid:107)∞ in the construction of their estimator 
while [13] requires knowledge of (cid:107)f∗(cid:107)∞. The work of [13] shows that under additional smoothness
assumptions  the optimal min-max risk is of order n−2/(d+4)  although  interestingly  no explicit
estimator was given to recover such a rate in dimensions larger than four.
In connection to optimization  our formulation connects to an area which has been active in operations
research for many years  namely  robust and distributionally robust optimization [5]. Distributionally
robust optimization (DRO) problems informed by optimal transport costs  as in this paper’s formula-
tion  have become popular in recent years not only in operations research but also in the machine
learning community. The work of [20] is the ﬁrst one to show a connection to regularized estimators 
in the context of logistic regression. The paper [6] provides an exact recovery of sqrt-Lasso and
support vector machines. The work in [6] uses the DRO formulation to deﬁne a statistical criterion to
optimally choose the uncertainty size δ. This criterion  when applied to linear regression problems 
recovers the scalings both in dimension and sample size obtained in the high-dimensional statistics

3

literature (see  for example  [4]). Applications in training of deep neural networks are given in [21] 
and additional representations of other estimators are given in [8  10  18]  among others. A key step
involved in obtaining these representations involves a duality result  which is given in [7].

1.2 Organization

The rest of this paper is organized as follows. In Section 2.1  we state and prove a strong duality
result for the DRCR formulation in (6). Section 2.2 provides an explicit construction of the DRCR

estimator  and in Section 2.3  we show that the convergence rate of this estimator is at most (cid:101)O(n−1/d).

Finally we run a simulation study showing that the DRCR estimator can outperform the standard
LSE or kernel based estimator. The proof of Theorem 2  as well as the main lemmas  is deferred to
the supplementary materials.

2 Main Results

We ﬁrst discuss our main result corresponding to the ﬁrst contribution stated in the Introduction.
We later turn to the second contribution. In order to state the strong duality result  we introduce
some notations as follows. Let x = (x1 ···   xd)  denoted by ∂f (x) the subdifferential of f
at x  and we deﬁne ∂xif (x) to be the partial subdifferential of f at x with respect to xi. we
deﬁne (cid:107)∇f(cid:107)∞ := supx∈Rd max{(cid:107)g(cid:107)∞ : g ∈ ∂f (x)}  and |∇xif (x)| := max{|g| : g ∈ ∂xif (x)}.
Finally  let ∇f (x) denotes one of the solutions in arg max{(cid:107)g(cid:107)∞ : g ∈ ∂f (x)}.

2.1 Dual formulation of DRCR

In this section  we establish the strong duality result for the DRCR problem (3)  which plays an
important role in the construction of our estimator and the analysis of rate of convergence.
Theorem 1 (Strong Duality). Suppose l(y  z) : R × R → R is a convex and Lipschitz function  such
that l(y  z) = l(−y −z). Deﬁne

Then  for any δ ≥ 0 

L := sup

(y z)∈R×R

|∇zl(y  z)|.

inf
f∈F

sup

P∈P(Rd+1):D(P Pn)≤δ

EP [l(Y  f (X))] = inf
f∈F

(cid:40)

δL(cid:107)∇f(cid:107)∞ +

n(cid:88)

i=1

1
n

(cid:41)

l(Yi  f (Xi))

.

By the above theorem  we see that the DRCR (3) problem is essentially equivalent to a regularized
empirical loss  where the supremum norm of ∇f is penalized.

Proof of Theorem 1. To begin  we invoke the following lemma
Lemma 1 ([7]). Given any probability distribution µ ∈ P(Rd)  for any upper semi-continuous
function f ∈ L1(dµ) and any cost function c  the following strong duality holds:

(cid:40)

(cid:34)

sup

ν∈P(Rd):D(µ ν)≤δ

Eνf (X) = inf
λ≥0

λδ + Eµ

{f (y) − λc(X  y)}

sup
y∈Rd

(cid:35)(cid:41)

.

(cid:35)(cid:41)

As a direct consequence of Lemma 1  we have for any f ∈ F that

sup

P∈Rd+1:D(P Pn)≤δ

EP [l(Y  f (X))]

(cid:40)
(cid:40)

(cid:34)
n(cid:88)

i=1

= inf
λ≥0

λδ + EPn

sup

(x y)∈Rd×R

{l(y  f (x)) − λc ((X  Y )  (x  y))}

(cid:41)

= inf
λ≥0

λδ +

1
n

sup
x∈Rd

{l(Yi  f (x)) − λ(cid:107)x − Xi(cid:107)1}

.

(5)

4

For simplicity  let ∇if (x) denotes the ith coordinate of ∇f (x)  (1 ≤ i ≤ d). Suppose λ < L(cid:107)∇f(cid:107)∞
  then there exists y0 ∈ R  z0 ∈ R  x0 ∈ Rd and i0 ∈ {1  . . .   d}  such that λ < |∇zl(y0  z0)| ·
|∇i0 f (x0)|. Without lost of generality  we may assume that ∇zl(y0  z0)∇i0f (x0) > 0. Otherwise 
we consider (−y0 −z0). We may consider the case that both ∇zl(y0  z0) ∇i0 f (x0) > 0  since the
case in which both of them are negative is similar. Let {ei}d
i=1 be the canonical basis of Rd  if
xt := x0 + t· ei0 ∈ Rd  then f (xt) is a convex function of t. Moreover  under the above assumptions 
we have f (xt) → +∞ as t → +∞. Hence  together with the convexity of l  for t > 0 sufﬁciently
large 

l(Yi.f (xt)) − λ(cid:107)xt − Xi(cid:107)1

≥ l(y0  f (xt)) − λ(cid:107)xt − x0(cid:107)1 − L0|y0 − Yi| − λ(cid:107)x0 − Xi(cid:107)
≥ l(y0  z0) + ∇zl(y0  z0) · (f (xt) − z0) − λt − L0|y0 − Yi| − λ(cid:107)x0 − Xi(cid:107)
≥ (∇zl(y0  z0)∇i0f (x0) − λ)t + ∇zl(y0  z0) · (f (x0) − z0) + l(y0  z0) − L0|y0 − Yi|

− λ(cid:107)x0 − Xi(cid:107) 

where L0 := sup(y z)∈R×R |∇yl(y  z)| < ∞. By taking the supremum over t  we have

{l(Yi  f (x)) − λ(cid:107)x − Xi(cid:107)1} = ∞.

sup
x∈Rd

On the other hand  if λ ≥ L(cid:107)∇f(cid:107)∞  we have for any x ∈ Rd that

l(Yi  f (x)) − l(Yi  f (Xi)) ≤ L(cid:107)∇f(cid:107)∞(cid:107)x − Xi(cid:107)1 ≤ λ(cid:107)x − Xi(cid:107)1 

where the equality holds if x = Xi. Hence

{l(Yi  f (x)) − λ(cid:107)x − Xi(cid:107)1} = l(Yi  f (Xi)).

sup
x∈Rd

Now  we can rewrite the equation (5) as

sup

ν∈P(Rd):D(µ ν)≤δ

Eνf (X) =

inf

λ≥L(cid:107)∇f(cid:107)∞

(cid:41)

n(cid:88)

i=1

l(Yi  f (Xi))

(cid:40)

λδ +

1
n

n(cid:88)

i=1

= δL(cid:107)∇f(cid:107)∞ +

1
n

l(Yi  f (Xi)).

2.2 Construction of the DRCR Estimator
To construct the DRCR estimator  we focus now on the absolute error loss l(y  f (x)) = |y − f (x)|.
Consider the following class of convex and Lipschitz functions:

Fn := {f : f is convex (cid:107)∇f(cid:107)∞ ≤ log n}.

It can be checked directly that the loss function l satisﬁes the requirements in Theorem 1 with the
constant L = 1  so  we can rewrite the DRCR problem (3) as follows:

Now we construct an estimator (cid:98)fn δ that solve the problem (6). Consider the following ﬁnite

i=1

l(Yi  f (Xi))

.

inf
f∈Fn

(6)

δ(cid:107)∇f(cid:107)∞ +

1
n

dimensional linear programming (LP)

n(cid:88)

(cid:41)

Let ((cid:98)g1 (cid:98)ξ1) ···   ((cid:98)gn (cid:98)ξn) be any solution of problem (7). Then  we can deﬁne the DRCR estimator

i )  1 ≤ i ≤ n.

by

min
gi ξi

s.t.

(cid:107)ξi(cid:107)∞.

i=1

1
l(Yi  gi) + δ max
1≤i≤n
n
gj ≥ gi + (cid:104)ξi  Xj − Xi(cid:105) 
i | ≤ log n  where ξi = (ξ1
|ξk
(cid:98)fn δ(x) := max

1≤i≤n

(cid:16)(cid:98)gi + (cid:104)(cid:98)ξi  x − Xi(cid:105)(cid:17)

1 ≤ i  j ≤ n.
i  ···   ξd

 

(8)

(7)

(cid:40)

n(cid:88)

5

where (cid:104)· ·(cid:105) is the standard inner product. Next  we show that (cid:98)fn δ also solves the problem (6). In fact 
(cid:98)fn δ is a solution to the problem
(cid:40)

(cid:41)

inf
f∈Fn

δ sup
1≤i≤n

(cid:107)∇f (Xi)(cid:107)∞ +

1
n

l(Yi  f (Xi))

 

where the objective value certainly serves as a lower bound for that of (6). Moreover  observe that

(cid:107)∇(cid:98)fn δ(cid:107)∞ = max1≤i≤n (cid:107)(cid:98)ξi(cid:107)∞ = sup1≤i≤n (cid:107)∇f (Xi)(cid:107)∞  hence (cid:98)fn δ is also a solution of (6).

i=1

n(cid:88)

2.3 Rate of Convergence

In order to state our rate of convergence result  corresponding the second contribution stated in the
Introduction  we need to impose some assumptions and state some deﬁnitions.
Let P(Rn) denote the set of all probability measures supported on Rn. Given a metric space
(X   ρ) and any subset G ⊂ X   the ε−covering number M (G  ε; ρ) is deﬁned as the small-
est number of balls with radius ε whose union contains G  and let Aε denotes any corre-
sponding ε-covering set. We say a random variable W is σ-sub-Gaussian if its Orlicz norm

(cid:107)W(cid:107)ψ2 := supk≥1 k−1/2(cid:0)E|W − EW|k(cid:1)1/k ≤ σ  which is equivalent to the standard deﬁni-
lim supn→∞ an/bn < ∞  an = Θ(bn) iff an = O(bn) and bn = O(an)  and an = (cid:101)O(bn) iff for

tion of sub-Gaussian random variable  see [24]. Furthermore  we use standard Landau’s asymp-
totic notations as follows: for two non-negative sequences {an} and {bn}  let an = O(bn) iff

some an = O(bn) up to a poly-log factor of bn.
We assume that the data {(Xi  Yi)}n
i=1 are i.i.d samples from P . To analyze the asymptotic behavior
of the DRCR estimator  we shall impose the following assumptions on the distribution of X and the
random variable E in (1).
Assumption 1. There exists some α  γ > 0 such that

E exp (α(cid:107)X(cid:107)γ∞) < ∞.
(9)
Assumption 2. The distribution of E is σ-sub-Gaussian for some σ > 0  symmetric about zero  and
has a continuous positive density pE (·) in a neighborhood of 0.
Remark 1. Assumption 1 allows the study of random variables (such as Weibull random variables)
exhibiting heavy tail behavior [9].
Remark 2. The assumptions on the symmetry and the density  ensure that 0 is the unique median of
E. As is standard in statistical formulations involving absolute error minimization  this assumption is
needed to guarantee the consistency of our estimator.

In the rest of this section  we study the convergence rate of the DRCR estimator (cid:98)fn δn introduced in

Section 2.2. We consider the general question of convergence rate for robustiﬁed estimators of the
form

We will show that by a suitable choice of δn  the convergence rate of(cid:98)gn δn to f∗ under the empirical
l1 loss is of order (cid:101)O(cid:0)n−1/d(cid:1)  where the empirical l1 loss of any two functions f  g is deﬁned as

P∈P(Rd+1):Dc(P Pn)≤δn

f∈Fn

sup

.

(10)

EP [l(Y  f (X))]

(cid:98)gn δn(x) ∈ arg min

(cid:40)

(cid:41)

n(cid:88)

i=1

l1(f  g) :=

1
n

|f (Xi) − g(Xi)|.

d (log n)1+ 3

Now we state our main theorem. The proof details are deferred to the supplementary materials
(Appendix A).
Theorem 2. If (cid:107)∇f∗(cid:107)∞ < ∞ and d > 4  and Assumption 1 and 2 hold  we can pick a δn of order
Θ(n− 2
that

γ ) so that for any(cid:98)gn δn (·) deﬁned via (10)  there exists some constant C > 0 such
P(cid:16)
In particular  the DRCR estimator (cid:98)fn δn deﬁned in (8) also enjoys the rate of (cid:101)O(n−1/d)  which is

l1((cid:98)gn δn   f∗) > Cn− 1

(cid:17) → 0

the best known rate so far (compare to [3  13  15]). In contrast to prior work  the estimation are not
deﬁned in terms of a priori bounds on (cid:107)f∗(cid:107)∞ and (cid:107)∇f∗(cid:107)∞.

as n → ∞.

d (log n)

(11)

γ+3

2γ

6

3 Numerical Experiments

3.1 Synthetic datasets

In this section we investigate the performance of our estimator (cid:98)fn δ  and compare it with the least

squares estimator (LSE) of convex regression in [15]  as well as the kernel smoothing estimator.
We conduct the experiments in the following setting. For each d and n  we generate i.i.d. random
variables Xi ∈ Rd  i = 1 . . . n such that each coordinate of Xi are i.i.d. from N (0  1)  or a standard
Student’s t-distribution with 10 degrees of freedom. We include this heavy-tailed speciﬁcation to
empirically test the impact of Assumption 1 in our estimator. The results suggest that even if such
assumption is violated  our estimator still performs remarkably well.
Let f∗ : Rd → R such that

d(cid:88)

i=1

f∗(x) =

|xi| 

x = (x1  . . .   xd).

1
n

(cid:40)

(cid:41)

.

n(cid:88)

i=1

n c = arg min

We generate Yi  i = 1 . . . d by Yi = f∗(Xi) + Ei  where the noises Ei are sampled i.i.d. from
N (0  σ2).

line with the setting in [3  15]  let c be any numerical constant greater than (cid:107)∇f∗(cid:107)∞  and we consider
the class of functions

We construct our DRCR estimator (cid:98)fn δn by taking δn = n−2/d. For the LSE of convex regression  in
Let (cid:98)f LS

n c be the least squares convex regression estimator  namely 

Fc := {f : f is convex (cid:107)∇f(cid:107)∞ ≤ c}.

(cid:98)f LS
In [3  15] it is shown that (cid:98)f LS
n c converges to f∗ for any c > (cid:107)∇f∗(cid:107)∞. Given that (cid:107)∇f∗(cid:107)∞ = 1  we
set c = 10 or 0.8  since in practice we typically do not have a tight bound for (cid:107)∇f∗(cid:107)∞ (we may
overestimate/underestimate (cid:107)∇f∗(cid:107)∞).
Next we construct the kernel regression estimator. Although not required to be convex  the
kernel estimator is a good benchmark comparison choice  in the non-parametric setting. For
)  where K : Rd → R denotes the Gaussian kernel with
2 e−(cid:107)x(cid:107)2/2. We then choose the best bandwidth hn via cross validation. To be
d+4   and then optimize the choice C via line search. That is  for each
i=1 i(cid:54)=j YiK( x−Xi
(cid:16)
) and we select C to be the
n(cid:88)

some bandwidth hn > 0  we deﬁne the kernel regression estimator (cid:98)kn hn by (cid:98)kn hn (x) =
)/(cid:80)n
(cid:80)n
i=1 YiK( x−Xi
K(x) = (2π)− d
(x) =(cid:80)n
1 ≤ j ≤ n  let(cid:98)k(−j)
speciﬁc  we pick hn = Cn− 1

i=1 i(cid:54)=j K( x−Xi

(Yi − f (Xi))2

i=1 K( x−Xi

hn

minimizer of

)/(cid:80)n
Yi −(cid:98)k(−i)

(cid:17)2

n Cn−1/(d+4)(Xi)

.

f∈Fc

n hn

hn

hn

hn

min

C∈{j/100 1≤j≤100}

Deﬁne the empirical l2 loss of any two functions f  g as

l2(f  g) :=

|f (Xi) − g(Xi)|2

(cid:33) 1

2

.

(cid:32)

i=1

n(cid:88)

i=1

1
n

the performance of (cid:98)fn δn  (cid:98)f LS

In the experiments  we set d = 5  n ∈ {50  100  150  200  250  300  350} and σ = 0.2. We compare

n 10 and(cid:98)kn hn under both the empirical l1 and l2 losses. For each

n 0.8  (cid:98)f LS

choice of n and d  we repeat the simulation 100 times and calculate their average.
We ﬁrst sample i.i.d. Xi ∼ N (0  Id) for the light tail case that satisfying Assumption 1. To compare 
we also sample i.i.d. heavy tail random variable Xi such that coordinates of Xi are i.i.d. from the
t-distribution with parameter 10. The results of the experiment follow.

7

(a) Light tail covariates  l1 loss

(b) Light tail covariates  l2 loss

(c) Heavy tail covariates  l1 loss

(d) Heavy tail covariates  l2 loss

Figure 1: In the above plots  the blue solid line stands for the estimator (cid:98)fn δ  the black dotted line stands for
(cid:98)f LS
n 0.8  the red dash-dot line stands for the estimator (cid:98)f LS
estimator(cid:98)kn hn.
n 10 and(cid:98)kn hn
From the Figure 1 in above  we observed that our estimator (cid:98)fn δ outperforms (cid:98)f LS

n 10  and the green dashed line stands for the kernel

n 0.8  (cid:98)f LS

in both l1 and l2 losses  and the performance of the least squares estimator is highly sensitive to
the choice of the constant c  the a priori bound on (cid:107)∇f∗(cid:107)∞. We believe that a key factor in the
performance of our estimator is the regularization penalty introduced in the DRCR formulation.

3.2 Real dataset

We consider a public dataset from United States Environmental Protection Agency  which was
suggested by [17]. The dataset consists of 600 air market data of California in the ﬁrst quarter of
2019. The response was the amount of heat input with the covariates corresponding to the amounts
of emissions of SO2  NOx  CO2 (in tons) and the NOX rate. Empirical evidence suggests that
relationship between the response and the log transformation of each individual covariate can be
modeled well by a convex ﬁt  so we do the log transformation on covariates and then standardize the
data. Since we never know f∗ in real data  we can not evaluate our method in the same way as the
submitted paper. Instead  we randomly split the dataset into a training set with 400 data and a test set
with 200 data  and we implement three different approaches: DRCR  LSE and LR (linear regression).
We repeat the experiment 10 times and then compare the average training l1 loss and average test l1
error.

Method Training loss Test error
DRCR
0.1294
0.1516
LSE
LR
0.1692

0.1238
0.1485
0.1691

We summarize the results in the above table. It is clear that our method outperforms both LSE and
LR.

8

References
[1] Yacine Ait-Sahalia and Jefferson Duarte. Nonparametric option pricing under shape restrictions.

Journal of Econometrics  116(1-2):9–47  2003.

[2] Gad Allon  Michael Beenstock  Steven Hackman  Ury Passy  and Alexander Shapiro. Nonpara-
metric estimation of concave production technologies by entropic methods. Journal of Applied
Econometrics  22(4):795–816  2007.

[3] Gabor Balazs  András György  and Csaba Szepesvari. Near-optimal max-afﬁne estimators for
convex regression. In Proceedings of the Eighteenth International Conference on Artiﬁcial
Intelligence and Statistics  volume 38 of Proceedings of Machine Learning Research  pages
56–64  San Diego  California  USA  09–12 May 2015. PMLR.

[4] A. Belloni  V. Chernozhukov  and L. Wang. Square-root lasso: pivotal recovery of sparse signals

via conic programming. Biometrika  98(4):791–806  12 2011.

[5] Aharon Ben-Tal and Arkadiaei Semenovich Nemirovskiaei. Lectures on Modern Convex
Optimization: Analysis  Algorithms  and Engineering Applications. Society for Industrial and
Applied Mathematics  Philadelphia  PA  USA  2001.

[6] Jose Blanchet  Yang Kang  and Karthyek Murthy. Robust wasserstein proﬁle inference and

applications to machine learning. arXiv e-prints  page arXiv:1610.05627  Oct 2016.

[7] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.

Mathematics of Operations Research  2019.

[8] Jose H. Blanchet and Yang Kang. Distributionally robust groupwise regularization estimator. In
ACML  volume 77 of Proceedings of Machine Learning Research  pages 97–112. PMLR  2017.

[9] Paul Embrechts  Thomas Mikosch  and Claudia Klüppelberg. Modelling extremal events: for

insurance and ﬁnance. Springer-Verlag  Berlin  Heidelberg  1997.

[10] Rui Gao and Anton J. Kleywegt. Distributionally robust stochastic optimization with wasserstein

distance. arXiv e-prints  page arXiv:1604.02199  Apr 2016.

[11] Piet Groeneboom  Geurt Jongbloed  and Jon A. Wellner. Estimation of a convex function:

Characterizations and asymptotic theory. Ann. Statist.  29(6):1653–1698  12 2001.

[12] Adityanand Guntuboyina and Bodhisattva Sen. Global risk bounds and adaptation in univariate

convex regression. Probability Theory and Related Fields  163(1):379–411  Oct 2015.

[13] Qiyang Han and Jon A. Wellner. Multivariate convex regression: global risk bounds and

adaptation. arXiv e-prints  page arXiv:1601.06844  Jan 2016.

[14] Lauren A. Hannah and David B. Dunson. Multivariate convex regression with adaptive parti-

tioning. J. Mach. Learn. Res.  14(1):3261–3294  January 2013.

[15] Eunji Lim. On convergence rates of convex regression in multiple dimensions. INFORMS

Journal on Computing  26(3):616–628  2014.

[16] Eunji Lim and Peter W. Glynn. Consistency of multidimensional convex regression. Operations

Research  60(1):196–208  2012.

[17] Rahul Mazumder  Arkopal Choudhury  Garud Iyengar  and Bodhisattva Sen. A computational
framework for multivariate convex regression and its variants. Journal of the American Statistical
Association  114(525):318–331  2019.

[18] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: performance guarantees and tractable reformulations. Mathemati-
cal Programming  171(1):115–166  Sep 2018.

[19] Emilio Seijo and Bodhisattva Sen. Nonparametric least squares estimation of a multivariate

convex regression function. Ann. Statist.  39(3):1633–1657  06 2011.

9

[20] Soroosh Shaﬁeezadeh Abadeh  Peyman Mohajerin Mohajerin Esfahani  and Daniel Kuhn.
Distributionally robust logistic regression. In Advances in Neural Information Processing
Systems 28  pages 1576–1584. Curran Associates  Inc.  2015.

[21] Aman Sinha  Hongseok Namkoong  and John Duchi. Certifying some distributional robustness

with principled adversarial training. arXiv preprint arXiv:1710.10571  2017.

[22] Hal Varian. The nonparametric approach to demand analysis. Econometrica  50(4):945–73 

1982.

[23] Hal Varian. The nonparametric approach to production analysis. Econometrica  52(3):579–97 

1984.

[24] Roman Vershynin.

Introduction to the non-asymptotic analysis of random matrices  page

210–268. Cambridge University Press  2012.

10

,Jose Blanchet
Peter Glynn
Jun Yan
Zhengqing Zhou