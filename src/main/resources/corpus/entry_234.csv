2008,A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning,In this paper we present two transductive bounds on the risk of the majority vote estimated over partially labeled training sets. Our first bound is tight when the additional unlabeled training data are used in the cases where the voted classifier makes its errors on low margin observations and where the errors of the associated Gibbs classifier can accurately be estimated. In semi-supervised learning  considering the margin as an indicator of confidence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. In this case  we propose a second bound on the joint probability that the voted classifier makes an error over an example having its margin over a fixed threshold. As an application we are interested on self-learning algorithms which assign iteratively pseudo-labels to unlabeled training examples having margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is fixed manually.,A Transductive Bound for the Voted Classiﬁer with an

Application to Semi-supervised Learning

Massih R. Amini

Laboratoire d’Informatique de Paris 6

Universit´e Pierre et Marie Curie  Paris  France

massih-reza.amini@lip6.fr

Franc¸ois Laviolette

Universit´e Laval

Qu´ebec (QC)  Canada

francois.laviolette@ift.ulaval.ca

Nicolas Usunier

Laboratoire d’Informatique de Paris 6

Universit´e Pierre et Marie Curie  Paris  France

nicolas.usunier@lip6.fr

Abstract

We propose two transductive bounds on the risk of majority votes that are estimated over
partially labeled training sets. The ﬁrst one involves the margin distribution of the clas-
siﬁer and a risk bound on its associate Gibbs classiﬁer. The bound is tight when so is
the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a
zone of low margin. In semi-supervised learning  considering the margin as an indicator
of conﬁdence constitutes the working hypothesis of algorithms which search the decision
boundary on low density regions. Following this assumption  we propose to bound the er-
ror probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed
threshold. As an application  we propose a self-learning algorithm which iteratively as-
signs pseudo-labels to the set of unlabeled training examples that have their margin above
a threshold obtained from this bound. Empirical results on different datasets show the
effectiveness of our approach compared to the same algorithm and the TSVM in which
the threshold is ﬁxed manually.

1 Introduction

Ensemble methods [5] return a weighted vote of baseline classiﬁers. It is well known that under the PAC-
Bayes framework [9]  one can obtain an estimation of the generalization error (also called risk) of such
majority votes (referred as Bayes classiﬁer). Unfortunately  those bounds are generally not tight  mainly
because they are indirectly obtain via a bound on a randomized combination of the baseline classiﬁers
(called the Gibbs classiﬁer). Although the PAC-Bayes theorem gives tight risk bounds of Gibbs classiﬁers 
the bounds of their associate Bayes classiﬁers come at a cost of worse risk (trivially a factor of 2  or under
some margin assumption  a factor of 1+). In practice the Bayes risk is often smaller than the Gibbs risk.
In this paper we present a transductive bound over the Bayes risk. This bound is also based on the risk of
the associated Gibbs classiﬁer  but it takes as an additional information the exact knowledge of the margin
distribution of unlabeled data. This bound is obtained by analytically solving a linear program. The intuitive
idea here is that given the risk of the Gibbs classiﬁer and the margin distribution  the risk of the majority
vote classiﬁer is maximized when all its errors are located on low margin examples. We show that our
bound is tight when the associated Gibbs risk can accurately be estimated and when the Bayes classiﬁer
makes most of its errors on low margin examples.
The proof of this transductive bound makes use of the (joint) probability over an unlabeled data set that the
majority vote classiﬁer makes an error and the margin is above a given threshold. This second result natu-
rally leads to consider the conditional probability that the majority vote classiﬁer makes an error knowing
that the margin is above a given threshold.

This conditional probability is related to the concept that the margin is an indicator of conﬁdence which is
recurrent in semi-supervised self-learning algorithms [3 6 10 11 12]. These methods ﬁrst train a classiﬁer
on the labeled training examples. The classiﬁer outputs serve then to assign pseudo-class labels to unlabeled
data having margin above a given threshold. The supervised method is retrained using the initial labeled set
and its previous predictions on unlabeled data as additional labeled examples. Practical algorithms almost
ﬁx the margin threshold manually.
In the second part of the paper  we propose to ﬁnd this margin threshold by minimizing the bound on
the conditional probability. Empirical results on different datasets show the effectiveness of our approach
compared to TSVM [7] and the same algorithm but with a manually ﬁxed threshold as in [11]
In the remainder of the paper  we present  in section 2  our transductive bounds and show their outcomes
in terms of sufﬁcient conditions under which unlabeled data may be of help in the learning process and a
linear programming method to estimate these bounds. In section 4  we present experimental results obtained
with a self-learning algorithm on different datasets in which we use the bound presented in section 2.2 for
choosing the threshold which serve in the label assignment step of the algorithm. Finally  in section 5 we
discuss the outcomes of this study and give some pointers to further research.

i=1 ∈ Z l and an unlabeled set XU = (x(cid:48)

2 Transductive Bounds on the Risk of the Voted Classiﬁer
We are interested in the study of binary classiﬁcation problems where the input space X is a subset of Rd
and the output space is Y = {−1  +1}. We furthermore suppose that the training set is composed of a
i=l+1 ∈ X u  where Z represents the
labeled set Z(cid:96) = ((xi  yi))l
i)l+u
set of X × Y. We suppose that each pair (x  y) ∈ Z(cid:96) is drawn i.i.d. with respect to a ﬁxed  but unknown 
probability distribution D over X × Y and we denote the marginal distribution over X by DX .
To simplify the notation and the proofs  we restrict ourselves to the deterministic labeling case  that is  for
each x(cid:48) ∈ XU   there is exactly one possible label that we will denote by y(cid:48).1
In this study  we consider learning algorithms that work in a ﬁxed hypothesis space H of binary classiﬁers
(deﬁned without reference to the training data). After observing the training set S = Z(cid:96) ∪ XU   the task of
the learner is to choose a posterior distribution Q over H such that the Q-weighted majority vote classiﬁer
BQ (also called the Bayes classiﬁer) will have the smallest possible risk on examples of XU . Recall that the
Bayes classiﬁer is deﬁned by
(1)
where  sgn(x)=+1 if the real number x > 0 and −1 otherwise. We further denote by GQ the associated
Gibbs classiﬁer which for classifying any example x ∈ X chooses randomly a classiﬁer h according to the
distribution Q. We accordingly deﬁne the transductive risk of GQ over an unlabeled set by:

BQ(x) = sgn [Eh∼Qh(x)]

∀x ∈ X .

Ru(GQ) def=

(2)
Where  [[π]] = 1 if predicate π holds and 0 otherwise  and for every unlabeled example x(cid:48) ∈ XU we refer to
y(cid:48) as its true unknown class label. In section 2.1 we show that if we consider the margin as an indicator of
u(GQ) of the risk of GQ which holds with probability
conﬁdence and that we dispose a tight upper bound Rδ
1− δ over the random choice of Z(cid:96) and XU (for example using Theorem 17 or 18 of Derbelo et al. [4])  we
are then able to accurately bound the transductive risk of the Bayes classiﬁer:

x(cid:48)∈XU

Eh∼Q[[h(x(cid:48)) (cid:54)= y(cid:48)]]

(cid:88)

1
u

Ru(BQ) def=

[[BQ(x(cid:48)) (cid:54)= y(cid:48)]]

(3)

(cid:88)

1
u

(cid:88)

x(cid:48)∈XU
This result follows from a bound on the joint Bayes risk:

1
u

[[BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) > θ]]

x(cid:48)∈XU

Ru∧θ(BQ) def=

(4)
Where mQ(·) = |Eh∼Qh(·)| denotes the unsigned margin function. One of the practical issues that arises
from this result is the possibility to deﬁne a threshold θ for which the bound is optimal and that we use
in a self-learning algorithm by iteratively assigning pseudo-labels to unlabeled examples having margin
above this threshold. We ﬁnally denote by Euz the expectation of a random variable z with respect to
the uniform distribution over XU and for notation convenience we equivalently deﬁne Pu the uniform
probability distribution over XU i.e. For any subset A  P (A) = 1

1The proofs can be inferred to the more general noisy case  but one has to replace the summation(cid:80)
(cid:80)
(x(cid:48) y(cid:48))∈XU×{−1 +1}. P(x(cid:48) y(cid:48))∼D(y(cid:48)|x(cid:48)) in the deﬁnitions of equations (3) and (4).

u card(A).

x(cid:48)∈XU by

2.1 Main Result

≤
Q (θ) − M <
(cid:27)

(cid:26)

Our main result is the following theorem which provides two bounds on the transductive risks of the Bayes
classiﬁer (3) and the joint Bayes risk (4).
Theorem 1 Suppose that BQ is as in (1). Then for all Q and all δ ∈ (0  1] with probability at least 1 − δ:

u(Q) = Rδ

Where K δ
and (cid:98).(cid:99)+ denotes the positive part (i.e. (cid:98)x(cid:99)+ = [[x > 0]]x).
More generally  with probability at least 1 − δ  for all Q and all θ ≥ 0:

u(GQ) + 1

Ru(BQ) ≤ inf
γ∈(0 1]
2 (EumQ(x(cid:48)) − 1)  M
(cid:26)

Pu(mQ(x(cid:48)) < γ) +

u(Q) − M <

(5)
(cid:67)
Q (t) = EumQ(x(cid:48))[[mQ(x(cid:48)) (cid:67) t]] for (cid:67) being < or ≤

+

Ru∧θ(BQ) ≤ inf
γ∈(θ 1]

Pu(θ < mQ(x(cid:48)) < γ) +

K δ

u(Q) + M

(cid:107)

(cid:27)

(6)

Q (γ)

+

1
γ

(cid:106)

1
γ

(cid:27)

Q (γ)(cid:5)

(cid:4)K δ

In section 2.2 we will prove that the bound (5) simply follows from (6). In order to better understand the
u(Q) the right hand side of equation (5):
former bound on the risk of the Bayes classiﬁer  denote by F δ
1
Pu(mQ(x(cid:48)) < γ) +
γ

Q (γ)(cid:5)

u(Q) def= inf
F δ

u(Q) − M <

(cid:4)K δ

γ∈(0 1]

+

(cid:26)

and consider the following special case where the classiﬁer makes most of its errors on unlabeled examples
with low margin. Proposition 2  together with the explanations that follow  makes this idea clearer.
Proposition 2 Assume that ∀x ∈ XU   mQ(x) > 0 and that ∃C ∈ (0  1] such that ∀γ > 0:

Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) = γ) (cid:54)= 0 ⇒ Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) < γ) ≥ C · Pu (mQ(x(cid:48)) < γ)
Then  with probability at least 1 − δ:

u(Q) − Ru(BQ) ≤ 1 − C

F δ

Ru(BQ) + Rδ

u(GQ) − Ru(GQ)

γ∗

C

(7)

Where γ∗ = sup{γ|Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) = γ) (cid:54)= 0}
Now  suppose that the margin is an indicator of conﬁdence. Then  a Bayes classiﬁer that makes its error
mostly on low margin regions will admit a coefﬁcient C in inequality (7) close to 1 and the bound of (5)
u(GQ) ). In the next section we provide proofs
becomes tight (provided we have an accurate upper bound Rδ
of all the statements above and show in lemma 4 a simple way to compute the best margin threshold for
which the general bound on the joint Bayes risk is the lowest.

2.2 Proofs

All our proofs are based on the relationship between Ru(GQ) and Ru(BQ) and the following lemma:

Lemma 3 Let (γ1  ..  γN ) be the ordered sequence of the different strictly positive values of the margin on
XU   that is {γi  i = 1..N} = {mQ(x(cid:48))|x(cid:48) ∈ XU ∧ mQ(x(cid:48)) > 0} and ∀i ∈ {1  . . .   N − 1}  γi < γi+1.
Denote moreover bi = Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) = γi) for i ∈ {1  . . .   N}. Then 

N(cid:88)

Ru(GQ) =

biγi +

i=1

∀θ ∈ [0  1]  Ru∧θ(BQ) =

(1 − EumQ(x(cid:48)))

1
2

bi with k = max{i|γi ≤ θ}

(8)

(9)

N(cid:88)

i=k+1

Proof Equation (9) follows the deﬁnition Ru∧θ(BQ) = Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) > θ).
Equation (8) is obtained from the deﬁnition of the margin mQ which writes as

∀x(cid:48) ∈ XU   mQ(x(cid:48)) = |Eh∼Q[[h(x(cid:48)) = 1]] − Eh∼Q[[h(x(cid:48)) = −1]]| = |1 − 2Eh∼Q[[h(x(cid:48)) (cid:54)= y(cid:48)]]|

By noticing that for all x(cid:48) ∈ XU the condition Eh∼Q[[h(x(cid:48)) (cid:54)= y(cid:48)]] > 1
y(cid:48)Eh∼Qh(x(cid:48)) < 0 or BQ(x(cid:48)) (cid:54)= y(cid:48)  we can rewrite mQ without absolute values and hence get:

2 is equivalent to the statement

Eh∼Q[[h(x(cid:48)) (cid:54)= y(cid:48)]] =

(1 + mQ(x(cid:48)))[[BQ(x(cid:48)) (cid:54)= y(cid:48)]] +

(10)
Finally equation (8) yields by taking the mean over x(cid:48) ∈ XU and by reorganizing the equation using the
notations of bi and γi. Recall that the values the x(cid:48) for which mQ(x(cid:48)) = 0 counts for 0 in the sum that
deﬁned the Gibbs risk (see equation 2 and the deﬁnition of mQ). (cid:3)

(1 − mQ(x(cid:48)))[[BQ(x(cid:48)) = y(cid:48)]]

1
2

1
2

Proof of Theorem 1 First  we notice that equation (5) follows equation (6) from the fact that M
and the following inequality:

≤
Q (0) = 0

Ru(BQ) = Ru∧0(BQ) + Pu(BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) = 0) ≤ Ru∧0(BQ) + Pu(mQ(x(cid:48)) = 0)

For proving equation (6)  we know from lemma 3 that for a ﬁx θ ∈ [0  1] there exist (b1  . . .   bN ) such that
0 ≤ bi ≤ Pu (mQ(x(cid:48)) = γi) and which satisfy equations (8) and (9).
Let k = max{i | γi ≤ θ}  assuming now that we can obtain an upper bound Rδ
u(GQ) of Ru(GQ) which
holds with probability 1 − δ over the random choices of Z(cid:96) and XU   from the deﬁnition (4) of Ru∧θ(BQ)
with probability 1 − δ we have then

b1 .. bN

bi u.c. ∀i  0 ≤ bi ≤ Pu (mQ(x(cid:48)) = γi) and

N(cid:88)
2 (1 − EumQ(x(cid:48))). It turns out that the right hand side in equation (11) is the
u(GQ)− 1
(cid:32)

biγi ≤ K δ

if i ≤ k 

N(cid:88)

(cid:33)

u(Q)

(11)

i=k+1

i=1

(cid:23)

Where K δ
solution of a linear program that can be solved analytically and which is attained for:

u(Q) = Rδ

Ru∧θ(BQ) ≤ max

k<j<i γj Pu(mQ(x(cid:48))=γj)

γi

+

elsewhere.

(12)

0

bi =

(cid:22) Kδ
u(Q)−(cid:80)
Using the notations deﬁned in Theorem 1  we rewrite(cid:80)

Pu (mQ(x(cid:48)) = γi)  

min

(cid:110)

i|K δ

u(Q) + M
We further deﬁne I = max
from equations (11) and (12) with bI = Kδ
bound on Ru∧θ(BQ):

(cid:111)

which implies(cid:80)N

k<j<i γjPu (mQ(x(cid:48)) = γj) as M <
Q (γi) > 0
Q (γI )

i=k+1 bi =(cid:80)I

Q (γi) − M

≤
Q (θ).
i=k+1 bi

. From this inequality we hence obtain a

≤
Q (θ)−M <
γI

≤
Q (θ) − M <

u(Q)+M

For clarity  we defer the proof of equation (12) to lemma 4  and continue the proof of equation (6).

Ru∧θ(BQ) ≤ Pu (θ < mQ(x(cid:48)) < γI) +

K δ

u(Q) + M

≤
Q (θ) − M <
γI

Q (γI)

(13)

The proof of the second point in theorem 1 is just a rewriting of this result as from the deﬁnition of γI  for any
γ > γI  the right-hand side of equation (6) is equal to Pu (mQ(x(cid:48)) < γ)  which is greater than the right-hand
side of equation (13). Moreover  for γ < γI  we notice that γ (cid:55)→ Pu (mQ(x(cid:48)) < γ)+ Kδ
Q (γ)
decreases. (cid:3)
Lemma 4 (equation (12)) Let gi  i = 1...N be such that 0 < gi < gi+1  pi ≥ 0  i = 1...N  B ≥ 0 and
k ∈ {1  . . .   N}. Then  the optimal value of the linear program:

≤
Q (θ)−M <

u(Q)+M

γ

max
q1 ... qN

N(cid:88)

N(cid:88)
qigi ≤ B
(cid:16)
pi (cid:98) B−(cid:80)
Proof Deﬁne O = {0}k ×(cid:81)N
in O  and that this solution is q∗. In the rest of the proof  we denote F (q) =(cid:80)N

is attained for q∗ deﬁned by: ∀i ≤ k : q∗

u.c. ∀i  0 ≤ qi ≤ pi and

i = 0 and ∀i > k  q∗

i = min

i=k+1

i=1

qi

i=k+1 qi.

(14)

(cid:17)

(cid:99)+

j<i q∗
gi

j gj

i=k+1[0  pi]. We will show that problem (14) has a unique optimal solution

qopt ∈(cid:81)N

i

i

i

= qopt

i=1 q∗

necessarily feasible and(cid:80)N

i=1[0  pi]. Deﬁne qopt O by qopt O

First  the problem is convex  feasible (take ∀i  qi = 0) and bounded. Therefore there is an optimal solution
= 0 otherwise. Then  qopt O ∈ O 
it is clearly feasible  and F (qopt O) = F (qopt). Therefore  there is an optimal solution in O.
Now  for (q  q(cid:48)) ∈ RN × RN   deﬁne I(q  q(cid:48)) = {i|qi > q(cid:48)

i}  and consider the lexicographic order (cid:23):
∀(q  q(cid:48)) ∈ RN × RN   q (cid:23) q(cid:48) ⇔ I(q(cid:48)  q) = ∅ or (I(q  q(cid:48)) (cid:54)= ∅ and min I(q  q(cid:48)) < min I(q(cid:48)  q))

if i > k and qopt O

i < pi} and M = I(q  q∗).

i = B. To see this result let M be the set {i > k| : q∗

which yields the same result; and ﬁnally  if M ≥ K  we have(cid:80)N

The crucial point is that q∗ is the greatest feasible solution in O for (cid:23).
Indeed  notice ﬁrst that q∗ is
i < pi}  we then have
two possibilities to consider. (a) M = ∅. In this case q∗ is simply the maximal element for (cid:23) in O. (b)
M (cid:54)= ∅. In this case  let K = min{i > k|q∗
We claim that there are no feasible q ∈ RN such that q (cid:31) q∗. By way of contradiction  suppose such a q
exists. Then  if M ≤ k  we have qM > 0  and therefore q is not in O; if k < M < K  we have qM > pM  
i = B  and q is not
feasible.
We now show that if q ∈ O is feasible and q∗ (cid:31) q  then q is not optimal (which is equivalent to show that
an optimal solution in O must be the greatest feasible solution for (cid:23)).
Let q ∈ O be a feasible solution such that q∗ (cid:31) q. Since q (cid:31) q∗  I(q∗  q) is not empty. If I(q  q∗) = ∅  we
have F (q∗) > F (q)  and therefore q is not optimal. We now treat the case where I(q  q∗) (cid:54)= ∅.
Let K = min I(q∗  q) and M = min I(q  q∗). We have qM > 0 by deﬁnition  and K < M because q∗ (cid:31) q
and q ∈ O. Let λ = min

i=1 qi > (cid:80)N

i=1 q∗

(cid:17)

(cid:16)

qM   gM
gK

i = qi if i (cid:54)∈ {K  M}  
q(cid:48)

We can see that q(cid:48) is feasible by the deﬁnition of λ  that it satisﬁes the box constraints  and(cid:80)
(cid:80)

λ ∗ gK − λ ∗ gM = (cid:80)

i qigi ≤ B. Moreover F (q(cid:48)) = F (q) + λ( gM

M = qM − λ.
q(cid:48)

i qigi + gM
gK

and

gK

igi =
− 1) > F (q) since

gK < gM and λ > 0. Thus  q is not optimal.
In summary  we have shown that there is an optimal solution in O  and that a feasible solution in O must be
the greatest feasible solution for the lexicographic order in O to be optimal and which is q∗. (cid:3)
Proof of Proposition 2 First let us claim that

i q(cid:48)

(pK − qK)
and deﬁne q(cid:48) by:
q(cid:48)
K = qK + gM
gK

λ

Ru(BQ) ≥ Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) < γ∗) +

1
γ∗

where γ∗ = sup{γ|Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) = γ) (cid:54)= 0} and Ku(Q) = Ru(GQ) + 1
Indeed  assume for now that equation (15) is true. Then  by assumption we have:

Q (γ∗)(cid:5)
(cid:4)Ku(Q) − M <
Q (γ∗)(cid:5)

(cid:4)Ku(Q) − M <

+

+

(15)
2 (EumQ(x(cid:48)) − 1).

(16)

Ru(BQ) ≥ C · Pu (mQ(x(cid:48)) < γ∗) +

(cid:106)

1
(cid:107)
γ∗
Q (γ∗)

+

Since F δ

u(Q) ≤ Pu (mQ(x(cid:48)) < γ∗) + 1
γ∗

u(Q) − M <
K δ

  with probability at least 1− δ we obtain:

u(GQ) − Ru(GQ)

u(Q) − M <

u(Q) − Ru(BQ) ≤ (1 − C)Pu (mQ(x(cid:48)) < γ∗) + Rδ
F δ

Q (γ∗)(cid:99)+ − (cid:98)Ku(Q) − M <

γ∗
Q (γ∗)(cid:99)+ ≤ Rδ

(17)
This is due to the fact that (cid:98)K δ
u(GQ) − Ru(GQ) when
u(GQ) ≥ Ru(GQ). Taking once again equation (16)  we have Pu (mQ(x(cid:48)) < γ∗) ≤ 1
C Ru(BQ). Plug-
Rδ
ging back this result in equation (17) yields Proposition 2.
Now  let us prove the claim (equation (15)). Since ∀x(cid:48) ∈ XU   mQ(x(cid:48)) > 0  we have Ru(BQ) = Ru∧0(BQ).
of lemma 3 that Ru(GQ) =(cid:80)K
Using the notations of lemma 3  denote K the index such that γK = γ∗. Then  it follows from equation (8)
Ku(Q)−(cid:80)K−1
2 (1 − EumQ(x(cid:48))). Solving for bK in this equality yields bK =
Pu (mQ(x(cid:48)) = γi). Finally  from equation (9)  we have Ru(BQ) =(cid:80)K
Q (γ∗)(cid:99)+ since bK ≥ 0 and ∀i  bi ≤
by using the lower bound on bK and the fact that(cid:80)K−1
i=1 bi  which implies equation (15)

i=1 bi = Pu (BQ(x(cid:48)) (cid:54)= y(cid:48) ∧ mQ(x(cid:48)) < γ∗). (cid:3)

and we therefore have bK ≥ 1

(cid:98)Ku(Q) − M <

i=1 biγi + 1

i=1 biγi

γK

γK

In general  good PAC-Bayesian approximations of Ru(GQ) are difﬁcult to carry out in supervised learning
[4] mostly due to the huge number of needed instances to obtain accurate approximations of the distribution
of the absolute values of the margin. In this section we have shown that the transductive setting allows for
high precision on the bounds from the risk Ru(GQ) of the Gibbs classiﬁer to the risk Ru(BQ) if we suppose
that the Bayes classiﬁer makes its errors mostly on low margin regions.

3 Relationship with margin-based self-learning algorithms

In Proposition 2 we have considered the hypothesis that the margin is an indicator of conﬁdence as one of
the sufﬁcient conditions which leads to a tight approximation of the risk of the Bayes classiﬁer  Ru(BQ).
This assumption constitutes the working hypothesis of margin-based self-learning algorithms in which a
classiﬁer is ﬁrst built on the labeled training set. The output of the learner can then be used to assign
pseudo-labels to unlabeled examples having a margin above a ﬁxed threshold (denoted by the set ZU\ in
what follows) and the supervised method is repeatedly retrained upon the set of the initial labeled and
unlabeled examples that have been classiﬁed in the previous steps. The idea behind this pseudo-labeling
is that unlabeled examples having a margin above a threshold are less subject to error prone labels  or
equivalently  are those which have a small conditional Bayes error deﬁned as:

Ru|θ(BQ) def= Pu(BQ(x(cid:48)) (cid:54)= y(cid:48) | mQ(x(cid:48)) > θ) =

Ru∧θ(BQ)

Pu(mQ(x(cid:48)) > θ)

(18)

Indeed 

to push away the decision boundary from the unlabeled data.

Input: Labeled and Unlabeled training sets: Z(cid:96)  XU
Initialize
(1) Train a classiﬁer H on Z(cid:96)
(2) Set ZU\ ← ∅
repeat

In this case the label assignation of unlabeled examples upon a margin criterion has the ef-
fect
This strategy follows the
cluster assumption [10] used in the design of some semi-supervised learning algorithms where
the decision boundary is supposed to pass through a region of low pattern density.
Though
their learning phase is nearly re-
margin-based self-learning algorithms are inductive in essence 
lated to transductive learning which predicts the labels of a given unlabeled set.
in both
cases the pseudo class-label assignation of unlabeled examples is interrelated to their margin.
For all these algorithms the choice
of the threshold is a crucial point 
as with a low threshold the risk
to assign false labels to exam-
ples is high and a higher value of
the threshold would not provide
enough examples to enhance the
current decision function. In order
to examine the effect of ﬁxing the
threshold or computing it automat-
ically we considered the margin-
based self-training algorithm pro-
posed by T¨ur et al. [10  Figure 6]
(referred as SLA in the following) 
in which unlabeled examples hav-
ing margin above a ﬁxed threshold
are iteratively added to the labeled
set and are not considered in next rounds for label distribution. In our approach  the best threshold mini-
mizing the conditional Bayes error (18) from equation (6) of theorem 1 is computed at each round of the
algorithm (line 3  ﬁgure 1 - SLA∗) while the threshold is kept ﬁxed in [10  Figure 6] (line 3 is outside of the
Q(G)  of the risk of the Gibbs classiﬁer which is involved in the computation of
repeat loop). The bound Rδ
the threshold in equation (18) was ﬁxed to its worst value 0.5.

(3) Compute the margin threshold θ∗ minimizing (18) from (6)
(4) S ← {(x(cid:48)  y(cid:48)) | x(cid:48) ∈ XU; mQ(x(cid:48)) ≥ θ∗ ∧ y(cid:48) = sgn(H(x(cid:48)))}
(5) ZU\ ← ZU\ ∪ S  XU = XU\S
(6) Learn a classiﬁer H by optimizing a global loss function on
Z(cid:96) and ZU\

until XU is empty or that there are no adds to ZU\ ;
Output The ﬁnal classiﬁer H

Figure 1: Self-learning algorithm (SLA∗)

4 Experiments and Results

In our experiments  we employed a Boosting algorithm optimizing the following exponential loss2 as the
baseline learner (line (6)  ﬁgure 1):

e−y(cid:48)H(x(cid:48))

(19)

(cid:88)

x∈Z(cid:96)

1
l

(cid:88)

x(cid:48)∈ZU\

1
|ZU\|

Lc(H  Z(cid:96)  ZU\) =
(cid:80)

e−yH(x) +
(cid:80)

2Bennett et al. [1] have shown that the minimization of (19) allows to reach a local minima of the margin loss

function LM (H  Z(cid:96)  ZU\) = 1

l

x∈Z(cid:96)

e−yH(x) + 1|ZU\|

x(cid:48)∈ZU\ e|H(x(cid:48))|.

Where H = (cid:80)

input feature jt ∈ {1  . . .   d} and a threshold λt as:

t αtht is a linear weighted sum of decision stumps ht which are uniquely deﬁned by an

ht(x) = 2[[ϕjt(x) > λt]] − 1

With ϕj(x) the jth feature characteristic of x. Within this setting  the Gibbs classiﬁer is deﬁned as a
t=1 according to Q such that ∀t  PQ(ht) = |αt|(cid:80)
random choice from the set of baseline classiﬁers {ht}T
t |αt|.
Accordingly the Bayes classiﬁer is simply the weighted voting classiﬁer BQ = sign(H). Although the
self-learning model (SLA∗) is an inductive algorithm we carried out experiments in a transductive setting
in order to compare results with the transductive SVM of Joachims [7] and the self-learning algorithm
(SLA) described in [11  Figure 6]. For the latter  after training a classiﬁer H on Z(cid:96) (ﬁgure 1  step 1) we
ﬁxed different margin thresholds considering the lowest and the highest output values of H over the labeled
training examples. We evaluated the performance of the algorithms on 4 collections from the benchmark
data sets3 used in [3] as well as 2 data sets from the UCI repository [2]. In this case  we chose sets large
enough for reasonable labeled/unlabeled partitioning  and that represent binary classiﬁcation problems.
Each experiment was repeated 20 times by partitioning  at each time  the data set into two random labeled
and unlabeled training sets.

Table 1: Means and standard deviations of the classiﬁcation error on unlabeled training data over the 20
trials for each data set. d denotes the dimension  l and u refer respectively to the number of labeled and
unlabeled examples in each data set.

Dataset
COIL2
DIGIT
G241c
USPS
PIMA 8
WDBC 30

d
l
241 1500 10
241 1500 10
241 1500 10
241 1500 10
10
10

l + u

768
569

SLA

.302↓±.042
.201↓±.038
.314↓±.037
.342↓±.024
.379↓±.026
.168↓±.016

SLA∗
.255±.019
.149±.012
.248±.018
.278↓±.022
.305±.021
.124±.011

TSVM
.286↓±.031
.156±.014
.252±.021
.261±.019
.318↓±.018
.141↓±.016

SLA

l
100 .148↓±.015
.091↓±.01
100
100 .201↓±.017
100 .114↓±.012
.284↓±.019
50
.112↓±.011
50

SLA∗
.134±.011
.071±.005
.191±.014
.112↓±.012
.266±.018
.079±.007

TSVM
.152↓±.016
.087↓±.009
.196±.022
.103±.011
.276±.021
.108↓±.01

For each data set  means and standard deviations of the classiﬁcation error on unlabeled training data over
the 20 trials are shown in Table 1 for 2 different splits of the labeled and unlabeled sets. The symbol ↓
indicates that performance is signiﬁcantly worse than the best result  according to a Wilcoxon rank sum test
used at a p-value threshold of 0.01 [8]. In addition  we show in ﬁgure 2 the evolutions on the COIL2  DIGIT
and USPS data sets of the classiﬁcation and both risks of the Gibbs classiﬁer (on the labeled and unlabeled
training sets) for different number of rounds in the SLA∗ algorithm. These ﬁgures are obtained from one of
the 20 trials that we ran for these collections.
The most important conclusion from these empirical results is that for all data sets  the self-learning al-
gorithm becomes competitive when the margin threshold is found automatically rather than if it is ﬁxed
manually. The augmented self-learning algorithm achieves performance statistically better or equivalent to
that of TSVM in most cases  while it outperforms the initial method over all runs.

Figure 2: Classiﬁcation error  train and test Gibbs errors with respect to the iterations of the SLA∗ algorithm
for a ﬁxed number of labeled training data l = 10.

3http://www.kyb.tuebingen.mpg.de/ssl-book/benchmarks.html

In SLA∗ the automatic choice of the margin-threshold has the effect to select  at the ﬁrst rounds of the
algorithm  many unlabeled examples for which their class labels can be predicted with high conﬁdence by
the voted classiﬁer. The exponential fall of the classiﬁcation rate in ﬁgure 2 can be explained by the addition
of these highly informative pseudo-labeled examples at the ﬁrst steps of the learning process (ﬁgure 1).
After this fall  few examples are added because the learning algorithm does not increase the margin on
unlabeled data. Hence  the number of additional pseudo-labeled examples decreases resulting in a plateau
in the classiﬁcation error curves in ﬁgure 2. We further notice that the error of the Gibbs classiﬁer on labeled
data increases fastly to a stationary error point and that on the unlabeled examples does not vary in time.

5 Conclusions

The contribution of this paper is two fold. First  we proposed a bound on the risk of the voted classiﬁer
using the margin distribution of unlabeled examples and an estimation of the risk of the Gibbs classiﬁer.
We have shown that our bound is a good approximation of the true risk when the errors of the associated
Gibbs classiﬁer can accurately be estimated and that the voted classiﬁer makes most its errors on low margin
examples.
The proof of the bound passed through a second bound on the joint probability that the voted classiﬁer
makes an error and that the margin is above a given threshold. This tool led to the conditional probability
that the voted classiﬁer makes an error knowing that the margin is above a given threshold. We showed that
the search of a margin threshold minimizing this conditional probability can be obtained by analytically
solving a linear program.
This resolution conducted to our second contribution which is to ﬁnd automatically the margin threshold in
a self-learning algorithm. Empirical results on a number of data sets have shown that the adaptive threshold
allows to enhance the performance of a self-learning algorithm.

References

In First International Workshop on Multiple

[1] Bennett  K.  Demiriz  A. & Maclin  R. (2002) Expoliting unlabeled data in ensemble methods. In Proc. ACM Int.
Conf. Knowledge Discovery and Data Mining  289-296.
[2] Blake  C.  Keogh  E. & Merz  C.J. (1998) UCI repository of machine learning databases. University of California 
Irvine. [on-line] http://www.ics.uci.edu/ mlearn/MLRepository.html
[3] Chapelle  O.  Sch¨olkopf  B. & Zien  A. (2006) Semi-supervised learning. MA: MIT Press.
[4] Derbeko  P.  El-Yaniv  R. & Meir  R. (2004) Explicit learning curves for transduction and application to clustering
and compression algorithms. Journal of Artiﬁcial Intelligence Research 22:117-142.
[5] Dietterich  T.G. (2000) Ensemble Methods in Machine Learning.
Classiﬁer Systems  1-15.
[6] Grandvalet  Y. & Bengio  Y. (2005) Semi-supervised learning by entropy minimization. In Advances in Neural
Information Processing Systems 17  529-536. Cambridge  MA: MIT Press.
[7] Joachims  T. (1999) Transductive Inference for Text Classiﬁcation using Support Vector Machines. In Proceedings
of the 16th International Conference on Machine Learning  200-209.
[8] Lehmann  E.L. (1975) Nonparametric Statistical Methods Based on Ranks. McGraw-Hill  New York.
[9] McAllester  D. (2003) Simpliﬁed PAC-Bayesian margin bounds.
Learning Theory  Lecture Notes in Artiﬁcial Intelligence  203-215.
[10] Seeger  M. (2002) Learning with labeled and unlabeled data. Technical report  Institute for Adaptive and Neural
Computation  University of Edinburgh.
[11] T¨ur  G.  Hakkani-T¨ur  D.Z. & Schapire  R.E. (2005) Combining active and semi-supervised learning for spoken
language understanding. Journal of Speech Communication 45(2):171-186.
[12] Vittaut  J.-N.  Amini  M.-R. & Gallinari  P. (2002) Learning Classiﬁcation with Both Labeled and Unlabeled Data.
In European Conference on Machine Learning  468-476.

In Proc. od the 16th Annual Conference on

,Kwang-Sung Jun
Jerry Zhu
Timothy Rogers
Zhuoran Yang
ming yuan
Abel Gonzalez-Garcia
Joost van de Weijer
Yoshua Bengio
Fengxiang He
Tongliang Liu
Dacheng Tao