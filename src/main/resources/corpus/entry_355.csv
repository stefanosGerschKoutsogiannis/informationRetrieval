2010,Active Instance Sampling via Matrix Partition,Recently  batch-mode active learning has attracted a lot of attention. In this paper  we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural form of mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework  this mutual information based instance selection problem can be formulated as a matrix partition problem. Although the matrix partition is an NP-hard combinatorial optimization problem  we show a good local solution can be obtained by exploiting an effective local optimization technique on the relaxed continuous optimization problem. The proposed active learning approach is independent of employed classification models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods.,Active Instance Sampling via Matrix Partition

Department of Computer & Information Sciences

Yuhong Guo

Temple University

Philadelphia  PA 19122
yuhong@temple.edu

Abstract

Recently  batch-mode active learning has attracted a lot of attention. In this pa-
per  we propose a novel batch-mode active learning approach that selects a batch
of queries in each iteration by maximizing a natural mutual information criterion
between the labeled and unlabeled instances. By employing a Gaussian process
framework  this mutual information based instance selection problem can be for-
mulated as a matrix partition problem. Although matrix partition is an NP-hard
combinatorial optimization problem  we show that a good local solution can be
obtained by exploiting an effective local optimization technique on a relaxed con-
tinuous optimization problem. The proposed active learning approach is indepen-
dent of employed classiﬁcation models. Our empirical studies show this approach
can achieve comparable or superior performance to discriminative batch-mode ac-
tive learning methods.

1

Introduction

Active learning is well-motivated in many supervised learning scenarios where unlabeled instances
are abundant and easy to retrieve but labels are difﬁcult  time-consuming  or expensive to obtain.
For example  it is easy to gather large amounts of unlabeled documents or images from the Inter-
net  whereas labeling them requires manual effort from experienced human annotators. Randomly
selecting unlabeled instances for labeling is inefﬁcient in many situations  since non-informative or
redundant instances might be selected. Aiming to reduce labeling effort  active learning (i.e.  selec-
tive sampling) methods have been adopted to control the labeling process in many areas of machine
learning. Given a large pool of unlabeled instances  active learning provides a way to iteratively
select the most informative unlabeled instances—the queries—from the pool to label.

Many researchers have addressed the active learning problem in various ways [13]. Most have fo-
cused on selecting a single most informative unlabeled instance to query each time. The ultimate
goal for most such approaches is to select instances that could lead to a classiﬁer with low gener-
alization error. Towards this  a few variants of a mutual information criterion have been employed
in the literature to guide the active instance sampling process. The approaches in [4][10] select the
instance to maximize the increase of mutual information and the mutual information  respectively 
between the selected set of instances and the remainder based on Gaussian process models. The
approach proposed in [5] seeks the instance whose optimistic label provides maximum mutual in-
formation about the labels of the remaining unlabeled instances. The mutual information measure
used is discriminative  computed using their trained classiﬁer at that point. This approach implicitly
exploits the clustering information contained in the unlabeled data in an optimistic way.

The single instance selection active learning methods require tedious retraining with each single in-
stance being labeled. When the learning task is sufﬁciently complex  the retraining process between
queries can become very slow. This may make highly interactive learning inefﬁcient or imprac-
tical. Furthermore  if a parallel labeling system is available  e.g.  multiple annotators working on

1

different labeling workstations at the same time on a network  a single instance selection system
can make wasteful use of the resource. Thus  a batch-mode active learning strategy that selects
multiple instances each time is more appropriate under these circumstances. The challenge in batch-
mode active learning is how to properly assemble the optimal query batch. Simply using a single
instance selection strategy to select a batch of queries in each iteration does not work well  since
it fails to take the information overlap between the multiple instances into account. Principles for
batch mode active learning need to be developed to address the multi-instance selection speciﬁcally.
Several sophisticated batch-mode active learning methods have been proposed for classiﬁcation.
Most of these approaches use greedy heuristics to ensure the overall informativeness of the batch by
taking both the individual informativeness and the diversity of the selected instances into account.
Schohn and Cohn [12] select instances according to their proximity to the dividing hyperplane for
a linear SVM. Brinker [2] considers an approach for SVMs that explicitly takes the diversity of the
selected instances into account  in addition to individual informativeness. Xu et al. [14] propose a
representative sampling approach for SVM active learning  which also incorporates a diversity mea-
sure. Speciﬁcally  they query cluster centroids for instances that lie close to the decision boundary.
Hoi et al. [7  8] extend the Fisher information framework to the batch-mode setting for binary logistic
regression. Hoi et al. [9] propose a novel batch-mode active learning scheme on SVMs that exploits
semi-supervised kernel learning. In particular  a kernel function is ﬁrst learned from a mixture of
labeled and unlabeled examples  and then is used to effectively identify the informative and diverse
instances via a min-max framework. Instead of using heuristic measures  Guo and Schuurmans [6]
treat batch construction for logistic regression as a discriminative optimization problem  and at-
tempt to construct the most informative batch directly. Overall  these batch-mode active learning
approaches all make batch selection decisions directly based on the classiﬁers employed.

In this paper  we propose a novel batch-mode active learning approach that makes query selection
decisions independent of the classiﬁcation model employed. The idea is to select a batch of queries
in each iteration by maximizing a general mutual information measure between the labeled instances
and the unlabeled instances. By employing a Gaussian process framework  this mutual information
maximization problem can be further formulated as a matrix partition problem. Although the matrix
partition problem is an NP-hard combinatorial optimization  it can ﬁrst be relaxed into a continuous
optimization problem and then a good local solution can be obtained by exploiting an effective local
optimization. The local optimization method we use is developed by combining a local lineariza-
tion of the objective function based on its ﬁrst-order Taylor series expansion  and a straightforward
backtracking line search. Unlike most active learning methods studied in the literature  our query
selection method does not require knowledge of the employed classiﬁer. Our empirical studies show
that the proposed batch-mode active learning approach can achieve superior or comparable perfor-
mance to discriminative batch-mode active learning methods that have been optimized on speciﬁc
classiﬁers.

The remainder of the paper is organized as follows. Section 2 provides preliminaries on Gaussian
processes. Section 3 introduces the proposed matrix partition approach for batch-mode active learn-
ing. Empirical studies are presented in Section 4  and Section 5 concludes this work.

2 Gaussian Processes

A Gaussian process is a generalization of the Gaussian probability distribution. Although Gaussian
processes have a long history in statistics  their potential has only become widely appreciated in the
machine learning community during the past decade [11]. In this section  we provide an overview of
Gaussian processes and some of their important properties which we will exploit later to construct
our active learning approach.

2.1 Multivariate Gaussian Distribution

The Gaussian  also known as the normal distribution  is a widely used model for the distribution
of continuous variables. In the case of multiple random variables  the joint multivariate Gaussian
distribution for a d × 1 vector x is given in the form

P (x) =

(2π)d/2|Σ|1/2 exp(cid:18)−

1

(x − µ)>Σ−1(x − µ)(cid:19)

1
2

2

where µ is a d-dimensional mean vector  Σ is a d × d covariance matrix  and |Σ| denotes the
determinant of Σ. When d = 1  we obtain the standard one-variable Gaussian distribution.

2.2 Gaussian Processes

A Gaussian process is a generalization of a multivariate Gaussian distribution over a ﬁnite vector
space to a function space of inﬁnite dimension. Given a set of instances X = [x>
t ] 
2 ; · · · ; x>
a data modeling function f (·) can be viewed as a single sample from a Gaussian distribution with
a mean function µ(·)  and a covariance function C(·  ·). In particular  µ(xi) denotes the mean of
the function variable f (xi) at point xi  and C(xi  xj) expresses the expected covariance between
functions f at point xi and xj. A Gaussian process is deﬁned as a Gaussian distribution on a space
of functions f which can be written in the form

1 ; x>

P (f (x)) =

exp(cid:18)−

(f (x)−µ(x))>Σ−1(f (x)−µ(x))(cid:19)

1
2

1
Z

where µ(x) is the mean function  Σ is deﬁned using the covariance function C  and Z denotes the
normalization factor. One typical choice for the covariance function C is a symmetric positive-
deﬁnite kernel function K  e.g. a Gaussian kernel

K(xi  xj) = exp(cid:18)−

(kxi − xjk2

τ 2

(cid:19)

(1)

One important property of Gaussian processes is that for every ﬁnite set (or subset) of instances XQ
with indices Q  the joint distribution over the corresponding random function variables fQ = f (XQ)
is a multivariate Gaussian distribution with a mean vector µQ = µ(XQ) and a covariance matrix
ΣQQ  where each entry Σi j is deﬁned using the covariance kernel function K(xi  xj)

P (fQ) =

exp(cid:18)−

1
2

1
Z

(fQ −µQ)>Σ−1

QQ(fQ −µQ)(cid:19)

(2)

Here Z = (2π)q/2|ΣQQ|1/2  and q is the size of set Q. We can assume the the mean function
µ(·) = 0. Nevertheless  it is irrelevant in this paper.

3 Batch-mode Active Learning via Matrix Partition

Given a small set of labeled instances {(xi  yi)}i∈L and a large set of unlabeled instances {xj}j∈U  
our task is to iteratively select the most informative set of b instances from U and add them into
the labeled set L after querying their labels from a labeling system. In this section  we propose to
conduct instance selective sampling using a maximum mutual information strategy which can then
be formulated into a matrix partition problem.

3.1 Maximum Mutual Information Instance Selection

Since the ultimate goal of active learning is to achieve a classiﬁer with good generalization perfor-
mance on unseen test data  it makes sense to select instances that can produce a labeled set that is
most informative about the unseen test instances. Apparently it is not possible to access the unseen
test data. Nevertheless  in active learning setting  we have a large number of unlabeled instances
available that come from the same distribution as the future test instances. Thus we can select in-
stances that lead to a labeled set which is most informative about the large set of unlabeled instances
instead. We propose to use a mutual information criterion to measure the informativeness of the
labeled set L over the unlabeled set U

I(XL  XU ) = H(XL) + H(XU ) − H(XL  XU )

(3)

where XL and XU denotes the labeled set of instances and the unlabeled set of instances respec-
tively  H(·) denotes the entropy term.
Both the mutual information measure and the entropy measure are deﬁned on probability distribu-
tions [3]. We thus employ a Gaussian process framework (introduced in the previous section) to

3

model the joint probability distribution over all the instances. We ﬁrst associate each instance xi
with a random variable fi. Then the joint distribution over a ﬁnite number of instances XQ can be
represented using the joint multivariate Gaussian distribution over variables fQ  which is given in
(2). Thus the entropy term H(XQ) = H(fQ) can be computed using a closed-form solution

H(fQ) =

1
2

ln(cid:0)(2πe)m|ΣQQ|(cid:1)

where m is the number of variables  i.e.  the size of Q; ΣQQ is the covariance matrix computed over
XQ using a kernel function K given in (1). Within this Gaussian process framework  the mutual
information criterion in (3) can be rewritten as

I(XL  XU ) = H(fL) + H(fU ) − H(fL  fU )

(5)

=

1
2

ln(cid:0)(2πe)l|ΣLL|(cid:1) +

1
2

ln(cid:0)(2πe)u|ΣU U |(cid:1) −

1
2

ln(cid:0)(2πe)t|ΣV V |(cid:1)

where V is the union of L and U; l  u  t denote the sizes of L  U  V respectively such that l + u = t.
Note that for a given data set  the overall number of instances does not change during the active
learning process. We simply move b instances from the unlabeled set U into the labeled set L
in each iteration. Thus the set V and the entropy term H(fL  fU ) are irrelevant to the instance
selection. Based on this observation  our maximum mutual information instance selection strategy
can be formulated as

Q∗ = arg max
|Q|=b Q⊆U

I(XL∪Q  X

U \Q) = arg max
|Q|=b Q⊆U

ln |ΣL0L0| + ln |ΣU 0U 0|

(6)

where L0 = L∪Q and U 0 = U \Q. This also suggests the mutual information criterion depends only
on the covariance matrices computed using the kernel functions over the instances. Our maximum
mutual information strategy attempts to select the batch of b instances from the unlabeled set U to
label  to maximize the log determinants of the covariance matrices over the produced sets L0 and U 0.

3.2 Matrix Partition

Let Σ be the covariance matrix over all the instances indexed by V = L ∪ U = L0 ∪ U 0. Then
the covariance matrices ΣLL  ΣU U   ΣL0L0 and ΣU 0U 0 are all submatrices of Σ. Without losing any
generality  we assume the instances are arranged in the order of [U  L]  such that

(4)

(7)

(8)

(9)

ΣLU ΣLL (cid:21)
Σ =(cid:20) ΣU U ΣU L

The instance selection problem formulated in (6) selects a subset of b instances indexed by Q from U
and moves them into the labeled set L. This problem is actually equivalent to partitioning matrix Σ
into submatrices ΣL0L0  ΣU 0U 0  ΣL0U 0 and ΣU 0L0 by reordering the instances in U. Since L is ﬁxed 
the actual matrix partition is conduct on covariance matrix ΣU U . Now we deﬁne a permutation
matrix M ∈ {0  1}u×u such that

where 1 denotes a vector of all 1 entries. We let M˜b denote the ﬁrst u − b rows of M  and Mb denote
the last b rows of M  such that

M 1 = 1  M >1 = 1

Obviously Mb selects b instances from U to form Q. Let

M˜bΣU U M >

˜b = ΣU 0U 0   MbΣU U M >

b = ΣQQ

T =(cid:2)M˜b O(u−b)×l(cid:3)   B =(cid:20) Mb

Ol×u

Ob×l
Il

(cid:21)

where Om×n denotes a m × n matrix with all 0 entries  and Il denotes a l × l identity matrix.
According to (8) we then have

(10)
Finally  the maximum mutual information problem given in (6) can be equivalently formulated into
the following matrix partition problem

ΣU 0U 0 = T ΣT >  ΣL0L0 = BΣB>

max

M
s.t.

ln |BΣB>| + ln |T ΣT >|

(11)

M ∈ {0  1}u×u  M 1 = 1  M >1 = 1

4

After solving this problem to obtain an optimal M ∗  the instance selection can be determined from
the last b rows of M ∗  i.e.  M ∗
b .
However  the optimization problem (11) is an NP-hard combinatorial optimization problem over an
integer matrix M. To facilitate a convenient optimization procedure  we relax the integer optimiza-
tion problem (11) into the following upper bound optimization problem

max

M
s.t.

ln |BΣB>| + ln |T ΣT >|

0 ≤ M ≤ 1  M 1 = 1  M >1 = 1

(12)

(13)

Note a determinant is a log concave function on positive deﬁnite matrices [1]. Thus ln |X| is concave
in X. However  the quadratic matrix function X = BΣB> is matrix convex given the matrix Σ
is positive deﬁnite. Thus the composition function ln |BΣB>| is neither convex nor concave  but
differentiable.
In general  this type of problems are difﬁcult global optimization problems. We
develop an efﬁcient local optimization technique to solve for a reasonable local solution instead.

3.3 First-order Local Optimization

The target optimization (12) is an optimization problem over a u × u matrix M  subject to the
linear inequality and equality constraints (13). Here u is the number of unlabeled instances  and
we typically assume it is a large number. Therefore a second-order optimization approach will be
space demanding. We develop a ﬁrst-order local maximization algorithm to conduct optimization 
which combines a gradient direction ﬁnding method with a straightforward backtracking line search
technique. This local optimization algorithm produced promising results in our experiments.

The algorithm is an iterative procedure  starting from an initial matrix M (0). Let M (k) denote the
optimization variable values returned from the the kth iteration. At the (k + 1)th iteration  we
approximate the objective function in (12) using its ﬁrst-order Taylor series expansion at point M (k)

g(M ) = ln |BΣB>| + ln |T ΣT >|

≈ ln |B(k)ΣB(k)>| + ln |T (k)ΣT (k)>| + Tr(cid:16)G(M (k))>(M − M (k))(cid:17)

(14)

Where B(k) and T (k) denote the corresponding B and T matrices with their M submatrices ﬁxed to
values given by M (k); Tr denotes the trace operator; G(M (k)) denotes the gradient matrix value at
point M (k). The gradient of the objective function g(M ) can be calculated using the matrix calculus 
which gives the following results

G(M˜b) =

G(Mb) =

dg(M )
dM˜b
dg(M )
dMb

= 2(cid:2)(T ΣT >)−1T Σ(cid:3)1:(u−b) 1:u
= 2(cid:2)(BΣB>)−1BΣ(cid:3)1:b 1:u

G(M ) = (cid:2)G(M˜b)>  G(Mb)>(cid:3)>

Note here we use notations in the matlab format where [X]i:j m:n denotes the (j−i+1)×(n−m+1)
submatrix of X formed by entries between the ith to the jth rows and the mth to the nth columns.

Given the gradient at point M (k)  we maximize the local linearization (14) to seek a gradient direc-
tion regarding the constraints. This leads to a convex linear optimization

(15)

(16)

(17)

(18)

(19)

fM = arg max

M
s.t.

Tr(cid:16)G(M (k))>M(cid:17)

0 ≤ M ≤ 1  M 1 = 1  M >1 = 1

The gradient direction for the (k + 1)th iteration can be determined as

We then employ a backtracking line search to seek the optimal value M (k+1) to improve the
original objective function g(M ) with g(M (k+1)) > g(M (k)). The line search procedure 

D = fM − M (k).

5

Algorithm 1 Matrix Partition

Input: l: the number of labeled instances; u the number of unlabeled instances;

Σ: covariance matrix given in form of (7); b: batch size;
M (0);  < 1e − 8.

Output: M ∗
Initialize k = 0  N oChange = f alse.
repeat

Set T and B according to equations (9) using the current M (k).
Compute gradient G(M (k)) at point M (k) according to equations (15)  (16) and (17).

Solve the local linear optimization (18) for the given gradient to get fM.

Compute the gradient ascend direction D using the equation (19).
Compute M (k+1) = linesearch(D  M (k)).
if kM (k+1) − M (k)k2 <  then NoChange=true. end if
k = k+1.

until N oChange is true or maximum iteration number is reached.
M ∗ = M (k).

Algorithm 2 Heuristic Greedy Rounding Procedure

Input: b  M ∈ (0  1)b×u for b < u.

for k = 1 to b do

Output: cM   Q.
Initialize Let Q = ∅  set cM as a b × u matrix with all 0 entries.
Set Q = Q ∪ {j}  cM (i  j) = 1  M (i  :) = −Inf  M (:  j) = −Inf.

Identify the largest value v = max(M (:)).
Identify the indices (i  j) of v in M.

end for

linesearch(D  M (k))  seeks an optimal step size  0 ≤ s < 1  to update the M (k) in the ascending
direction D given in (19)  i.e. M (k+1) = M (k) + sD  guaranteeing the returned M (k+1) satisﬁes
the linear constraints in (13)  and leads to an objective value no worse than before.

The overall algorithm for optimizing the matrix partition problem (12) is given in Algorithm 1.
In our implementation  the constrained linear optimization (18) can be efﬁciently solved using an
optimization software package CPLEX. When the number of unlabeled instances  u  is large  com-
puting the log-determinant of the (u − b) × (u − b) matrix  T ΣT >  is likely to run into overﬂow
or underﬂow. Instead of computing the log-determinant directly  we choose to compute it in an
alternative efﬁcient way. The key idea is based on the mathematical fact that the determinant of a
triangular matrix equals the product of its diagonal elements. Hence  the matrix’s log-determinant
is equal to the sum of their logarithm values. By keeping all computations in log-scale  the problem
of underﬂow/overﬂow caused by product of many numbers can be effectively circumvented. For
positive deﬁnite matrices  such as the matrices we have  one can use Cholesky factorization to ﬁrst
produce a triangular matrix and then compute the log-determinant of the original matrix using the
logarithms of the diagonal values of the triangular matrix. The computation of log-determinants or
matrix inverse in our algorithm are all conducted on matrices assumed to be positive deﬁnite. How-
ever  in order to increase the robustness of the algorithm and avoid numerical problems  we can add
an additional δI term to the matrices to guarantee the positive deﬁnite property. Here δ is a very
small value and I is an identity matrix.
By solving the matrix partition problem in (12) using Algorithm 1  an optimal matrix M ∗ is returned.
However  this M ∗ contains continuous values. In order to determine which set of b instances to

select  we need to round M ∗ to a {0 1}-valued dM ∗  while maintaining the permutation constraints
dM ∗1 = 1 anddM ∗

1 = 1. We use a simple heuristic greedy procedure to conduct the rounding. In
this procedure  we focused on rounding the last b rows  M ∗
b   since they are the ones used to pick b
instances for labeling. The procedure is described in Algorithm 2  which returns the indices of the
selected b instances as well.

>

6

4 Experiments

To investigate the empirical performance of the proposed batch-mode active learning algorithm  we
conducted two sets of experiments on a few UCI datasets and the 20 newsgroups dataset. Note the
proposed active learning method is in general independent of the speciﬁc classiﬁcation model em-
ployed. For the experiments in this section  we used logistic regression as its classiﬁcation model to
evaluate the informativeness of the selected labeled instances. We compared the proposed approach 
denoted as Matrix  with three discriminative batch-mode active learning methods proposed in the
literature: svmD  an approach that incorporates diversity in active learning with SVMs [2]; Fisher 
an approach that uses Fisher information matrix based on logistic regression classiﬁers for instance
selection [8]; Discriminative  a discriminative optimization approach based on logistic regression
classiﬁers [6]. We have also compared our approach to one transductive experimental design method
which is formulated from regression problems and whose instance selection process is independent
of evaluation classiﬁcation models [15]. We used the sequential design code downloaded from the
authors’ webpage and denote this method as Design.

First  we conducted experiments on seven UCI datasets. We consider a hard case of active learning 
where we start active learning from only a few labeled instances. In each experiment  we start with
two randomly selected labeled instances  one in each class. We then randomly select 2/3 of the
remaining instances as the unlabeled set  using all the other instances for testing. All the algorithms
start with the same initial labeled set  unlabeled set and testing set. For a ﬁxed batch size b  each
algorithm repeatedly select b instances to label each time and evaluate the produced classiﬁer on
testing data after each new labeling  with maximum 110 instances to select in total. The experiments
were repeated 20 times. In Table 1  we report the experimental results with b = 10  comparing the
proposed Matrix algorithm with each of the three batch-mode alternatives. With b = 10  there are
totally 11 evaluation points  with 20 results on each of them. We therefore run a 2-sided paired t-test
at each evaluation point to compare the performance of each pair of algorithms. The “win%” denotes
the percentage of evaluation points where the Matrix algorithm outperforms the speciﬁed algorithm
using a 2-sided paired t-test at the level of p<0.05; the “lose%” denotes the percentage of evaluation
points where the speciﬁed algorithm outperforms the Matrix algorithm. The “overall” nevertheless
show the comparison results using a single 2-sided paired t-test on all 220 results. These results
show that the proposed active learning method  Matrix  overperformed svmD  Fisher and Design on
most data sets  except an overall lose to svmD on pima  a tie with Fisher and Design on hepatitis  and
a tie with Design on ﬂare. Matrix is mostly tied with Discriminative on all data sets  with a slight
pointwise win on crx and a slight overall lose on german. Although Matrix and Discriminative
demonstrated similar performance  the proposed Matrix is more efﬁcient regarding running time on
relatively big data sets. The comparison in running times over 20 repeats are reported in Table 2.

Table 1: Comparison of the active learning algorithms on UCI data with batch size = 10. These
results are based on 2-sided paired t-test at the level of p< 0.05.

Data set

Matrix vs svmD

Matrix vs Fisher Matrix vs Discriminative Matrix vs Design

win% lose% overall win% lose% overall win% lose% overall win% lose% overall

63.6
cleve
27.3
crx
ﬂare
54.5
81.8
german
63.6
heart
hepatitis 100.0
0
pima

0 win
0 win
0 win
0 win
0 win
0 win
0
lose

45.5
9.1
100.0
9.1
36.4
33.3
100.0

0 win
0 win
0 win
0 win
0 win
0
tie
0 win

0
9.1
0
0
0
0
0

0
0
0
0
0
0
0

tie
tie
tie
lose
tie
tie
tie

90.9
90.9
36.4
72.7
100.0
0
81.8

9.1

0 win
0 win
tie
0 win
0 win
0
tie
0 win

Method
Matrix
Discriminative

Table 2: Average running time (in minutes)
cleve
8.37
3.33

german
22.08
285.65

ﬂare
9.53
220.12

crx
6.14
61.44

heart
5.68
2.40

hepatitis
0.12
0.08

pima
60.11
68.27

7

Table 3: Comparison of the active learning algorithms on Newsgroup data with batch size = 20.
These results are based on 2-sided paired t-test at the level of p< 0.05.

Data set

Matrix vs svmD

Matrix vs Fisher Matrix vs Random Matrix vs Design

win% lose% overall win% lose% overall win% lose% overall win% lose% overall

Autos
86.7
Hardware 100.0
86.7
Sport

0 win
0 win
6.6 win

20.0
0
20.0

6.6
0
13.3

tie
tie
tie

73.3
13.3
46.7

6.6 win
0 win
0 win

80.0
86.7
80.0

6.7 win
0 win
6.7 win

Next we conducted experiments on 20 newsgroups dataset for document categorization. We build
three binary classiﬁcation tasks: (1) Autos: rec.autos (987 documents) vs. rec.motorcycles (993 doc-
uments); (2) Hardware: comp.sys.ibm.pc.hardware (979 documents) vs. comp.sys.mac.hardware
(958 documents); (3) Sport: rec.sport.baseball (991 documents) vs. rec.sport.hockey (997 docu-
ments). Each document is ﬁrst minimally processed into a “tf.idf” vector. We then select the top
400 features to use according to their total “tf.idf” frequencies in all the documents for the consid-
ered task. In each experiment  we start with four randomly selected labeled instances  two in each
class. We then randomly select 1000 instances (500 from each class) from the remaining ones as the
unlabeled set  using all the other instances for testing. All the algorithms start with the same initial
labeled set  unlabeled set and testing set. For a ﬁxed batch size b  each algorithm repeatedly select b
instances to label each time with maximum 300 instances to select in total. In this section  we report
the experimental results with b = 20 averaged over 20 times repetitions. There are 300/20 = 15
evaluation points in this case.

Note the unlabeled sets used for this set of experiments are much larger than the ones used for
experiments on UCI datasets. This substantially increases the searching space of instance selection.
One consequence in our experiments is that the Discriminative algorithm becomes very slow. Thus
we were not able to produce comparison results for this algorithm. The proposed Matrix method
was affected as well. However  we coped with this problem using a subsampling assisted method 
where we ﬁrst select a subset of 400 instances from the unlabeled set and then restrain our instance
selection to this subset. This is equivalent to solving the matrix partition optimization in (12) with
additional constraints on Mb  such that the columns of Mb corresponding to instances outside of
this subset of 400 instances are all set to 0. For the experiments  we chose the 400 instances as
the ones with top entropy terms under the current classiﬁcation model. The same subsampling was
used for the method Design as well. Table 3 shows the comparison results on the three document
categorization tasks  comparing Matrix to svmD  Fisher  Design and a baseline random selection 
Random. These results show the proposed Matrix outperformed svmD  Design and Random. It tied
with Fisher regarding overall measure  but had a slight win regarding pointwise measure.

These empirical results suggest that selecting unlabeled instances independent of the classiﬁcation
model using the proposed matrix partition method can achieve reasonable performance  which is
better than a transductive experimental design method and comparable to the discriminative batch-
mode active learning approaches. However  our approach can offer certain conveniences in some
circumstances where one does not know the classiﬁcation model to be employed for classiﬁcation.

5 Conclusions

In this paper  we propose a novel batch-mode active learning approach that makes query selection
decisions independent of the classiﬁcation model employed. The proposed approach is based on a
general maximum mutual information principle. It is formulated as a matrix partition optimization
problem under a Gaussian process framework. To tackle the formulated combinatorial optimization
problem  we developed an effective local optimization technique. Our empirical studies show the
proposed ﬂexible batch-mode active learning approach can achieve comparable or superior perfor-
mance to discriminative batch-mode active learning methods that have been optimized on speciﬁc
classiﬁers. A future extension for this work is to consider batch-mode active learning with structured
data by exploiting different kernel functions.

8

References
[1] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[2] K. Brinker. Incorporating diversity in active learning with support vector machines. In Pro-

ceedings of International Conference on Machine learning  2003.

[3] T. Cover and J. Thomas. Elements of Information Theory. John Wiley & sons  1991.
[4] C. Guestrin  A. Krause  and A. Singh. Near-optimal sensor placements in Gaussian processes.

In Proceedings of International Conference on Machine Learning  2005.

[5] Y. Guo and R. Greiner. Optimistic active learning using mutual information. In Proceedings

of International Joint Conference on Artiﬁcial Intelligence  2007.

[6] Y. Guo and D. Schuurmans. Discriminative batch mode active learning. In Proceedings of

Neural Information Processing Systems  2007.

[7] S. Hoi  R. Jin  and M. Lyu. Large-scale text categorization by batch mode active learning. In

Proceedings of the International World Wide Web Conference  2006.

[8] S. Hoi  R. Jin  J. Zhu  and M. Lyu. Batch mode active learning and its application to medical
image classiﬁcation. In Proceedings of International Conference on Machine Learning  2006.
[9] S. Hoi  R. Jin  J. Zhu  and M. Lyu. Semi-supervised SVM batch mode active learning for
image retrieval. In Proceedings of IEEE Computer Society Conference on Computer Vision
and Pattern Recognition  2008.

[10] A. Krause  C. Guestrin  A. Gupta  and J. Kleinberg. Near-optimal sensor placements: Max-
imizing information while minimizing communication cost. In International Symposium on
Information Processing in Sensor Networks  2006.

[11] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press  2006.
[12] G. Schohn and D. Cohn. Less is more: Active learning with support vector machines.
In

Proceedings of International Conference on Machine Learning  2000.

[13] B. Settles. Active learning literature survey. Computer Sciences Technical Report 1648  Uni-

versity of Wisconsin–Madison  2009.

[14] Z. Xu  K. Yu  V. Tresp  X. Xu  and J. Wang. Representative sampling for text classiﬁcation

using support vector machines. In European Conference on Information Retrieval  2003.

[15] K. Yu and J. Bi. Active learning via transductive experimental design. In In Proceedings of the

International Conference on Machine Learning  2006.

9

,Deanna Needell
Rachel Ward
Nati Srebro