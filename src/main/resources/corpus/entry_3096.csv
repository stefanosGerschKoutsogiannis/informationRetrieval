2018,Learning sparse neural networks via sensitivity-driven regularization,The ever-increasing number of parameters in deep neural networks poses challenges for memory-limited applications. Regularize-and-prune methods aim at meeting these challenges by sparsifying the network weights. In this context we quantify the output sensitivity to the parameters (i.e. their relevance to the network output) and introduce a regularization term that gradually lowers the absolute value of parameters with low sensitivity.  Thus  a very large fraction of the parameters approach zero and are eventually set to zero by simple thresholding. Our method surpasses most of the recent techniques both in terms of sparsity and error rates. In some cases  the method reaches twice the sparsity obtained by other techniques at equal error rates.,Learning Sparse Neural Networks
via Sensitivity-Driven Regularization

Enzo Tartaglione
Politecnico di Torino

Torino  Italy

tartaglioneenzo@gmail.com

Skjalg Lepsøy

Nuance Communications

Torino  Italy

Attilio Fiandrotti

Politecnico di Torino  Torino  Italy
Télécom ParisTech  Paris  France

Gianluca Francini

Telecom Italia
Torino  Italy

Abstract

The ever-increasing number of parameters in deep neural networks poses challenges
for memory-limited applications. Regularize-and-prune methods aim at meeting
these challenges by sparsifying the network weights. In this context we quantify
the output sensitivity to the parameters (i.e. their relevance to the network output)
and introduce a regularization term that gradually lowers the absolute value of
parameters with low sensitivity. Thus  a very large fraction of the parameters
approach zero and are eventually set to zero by simple thresholding. Our method
surpasses most of the recent techniques both in terms of sparsity and error rates. In
some cases  the method reaches twice the sparsity obtained by other techniques at
equal error rates.

1

Introduction

Deep neural networks achieve state-of-the-art performance in a number of tasks by means of complex
architectures. Let us deﬁne the complexity of a neural network as the number of its learnable
parameters. The complexity of architectures such as VGGNet [1] and the SENet-154 [2] lies in the
order of 108 parameters  hindering their deployability on portable and embedded devices  where
storage  memory and bandwidth resources are limited.
The complexity of a neural network can be reduced by promoting sparse interconnection structures.
Empirical evidence shows that deep architectures often require to be over-parametrized (having
more parameters than training examples) in order to be successfully trained [3  4  5]. However  once
input-output relations are properly represented by a complex network  such a network may form a
starting point in order to ﬁnd a simpler  sparser  but sufﬁcient architecture [4  5].
Recently  regularization has been proposed as a principle for promoting sparsity during training. In
general  regularization replaces unstable (ill-posed) problems with nearby and stable (well-posed)
ones by introducing additional information about what a solution should be like [6]. This is often
done by adding a term R to the original objective function L. Letting θ denote the network parameters
and λ the regularization factor  the problem

is recasted as

minimize L(θ) with respect to θ

minimize L(θ) + λR(θ) with respect to θ.

(1)

(2)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Stability and generalization are strongly related or even equivalent  as shown by Mukherjee et al. [7].
Regularization therefore also helps ensure that a properly trained network generalizes well on unseen
data.
Several known methods aim at reaching sparsity via regularization terms that are more or less
speciﬁcally designed for the goal. Examples are found in [8  9  10].
The original contribution of this work is a regularization and pruning method that takes advantage of
output sensitivity to each parameter. This measure quantiﬁes the change in network output that is
brought about by a change in the parameter. The proposed method gradually moves the less sensitive
parameters towards zero  avoiding harmful modiﬁcations to sensitive  therefore important  parameters.
When a parameter value approaches zero and drops below a threshold  the parameter is set to zero 
yielding the desired sparsity of interconnections.
Furthermore  our method implies minimal computational overhead  since the sensitivity is a simple
function of a by-product of back-propagation. Image classiﬁcation experiments show that our method
improves sparsity with respect to competing state-of-the-art techniques. According to our evidence 
the method also improves generalization.
The rest of this paper is organized as follows. In Sec. 2 we review the relevant literature concerning
sparse neural architectures. Next  in Sec. 3 we describe our supervised method for training a neural
network such that its interconnection matrix is sparse. Then  in Sec. 4 we experiment with our
proposed training scheme over different network architectures. The experiments show that our
proposed method achieves a tenfold reduction in the network complexity while leaving the network
performance unaffected. Finally  Sec. 5 draws the conclusions while providing further directions for
future research.

2 Related work

Sparse neural architectures have been the focus of intense research recently due the advantages they
entail. For example  Zhu et al. [11]  have shown that a large sparse architecture improves the network
generalization ability in a number of different scenarios. A number of approaches towards sparse
interconnection matrices have been proposed. For example  Liu et al. [12] propose to recast multi-
dimensional convolutional operations into bidimensional equivalents  resulting in a ﬁnal reduction of
the required parameters. Another approach involves the design of an object function to minimize the
number of features in the convolutional layers. Wen et al. [8] propose a regularizer based on group
lasso whose task is to cluster ﬁlters. However  such approaches are speciﬁc for convolutional layers 
whereas the bulk of network complexity often lies in the fully connected layers.
A direct strategy to introduce sparsity in neural networks is l0 regularization  which entails however
solving a highly complex optimization problem (e.g.  Louizos et al. [13]).
Recently  a technique based on soft weight sharing has been proposed to reduce the memory footprint
of whole networks (Ullrich et al. [10]). However  it limits the number of the possible parameters
values  resulting in sub-optimal network performance.
Another approach involves making input signals sparse in order to use smaller architectures. Inserting
autoencoder layers at the begin of the neural network (Ranzato et al. [14]) or modeling of ‘receptive
ﬁelds’ to preprocess input signals for image classiﬁcation (Culurciello et al. [15]) are two clear
examples of how a sparse  properly-correlated input signal can make the learning problem easier.
In the last few years  dropout techniques have also been employed to ease sparsiﬁcation.
Molchanov et al. [16] propose variational dropout to promote sparsity. This approach also pro-
vides a bayesian interpretation of gaussian dropout. A similar but differently principled approach
was proposed by Theis et al. [17]. However  such a technique does not achieve in fully-connected
architectures state-of-the-art test error.
The proposal of Han et al. [9] consists of steps that are similar to those of our method. It is a three-
staged approach in which ﬁrst  a network learns a coarse measurement of each connection importance 
minimizing some target loss function; second  all the connections less important than a threshold
are pruned; third and ﬁnally  the resulting network is retrained with standard backpropagation to
learn the actual weights. An application of such a technique can be found in [18]. Their experiments

2

show reduced complexity for partially better performance achieved by avoiding network over-
parametrization.
In this work  we propose to selectively prune each network parameter using the knowledge of
sensitivity. Engelbrecht et al. [19] and Mrazova et al. [20  21] previously proposed sensitivity-based
strategies for learning sparse architectures. In their work  the sensitivity is however deﬁned as the
variation of the network output with respect to a variation of the network inputs. Conversely  in our
work we deﬁne the sensitivity of a parameter as the variation of the network output with respect to
the parameter  pruning parameters with low sensitivity as they contribute little to the network output.

3 Sensitivity-based regularization

In this section  we ﬁrst formulate the sensitivity of a network with respect to a single network
parameter. Next  we insert a sensitivity-based term in the update rule. Then  we derive a per-
parameter general formulation of a regularization term based on sensitivity  having as particular case
ReLU-activated neural networks. Finally  we propose a general training procedure aiming for sparsity.
As we will experimentally see in Sec. 4  our technique not only sparsiﬁes the network  but improves
its generalization ability as well.

3.1 Some notation

Here we introduce the terminology and the notation used in the rest of this work. Let a feed-forward 
acyclic  multi-layer artiﬁcial neural network be composed of N layers  with xn−1 being the input of
the n-th network layer and xn its output  n ∈ [1  N ] integer. We identify with n=0 the input layer 
n = N the output layer  and other n values indicate the hidden layers. The n-th layer has learnable
parameters  indicated by wn (which can be biases or weights).1 In order to identify the i-th parameter
at layer n  we write wn i.
The output of the n-th layer can be described as

xn = fn [gn (xn−1  wn)]  

(3)
where gn(·) is usually some afﬁne function and fn(·) is the activation function at layer n. In the
following  x0 indicates the network input. Let us indicate the output of the network as y = xN ∈ RC 
with C ∈ N. Similarly  y(cid:63) indicates the target (expected) network output associated to x0.

(a)

Figure 1: Generic layer representation (Fig. 1a) and the case of a fully connected layer in detail
(Fig. 1b  here we have wn ∈ Rm×p). Biases may also be included.

(b)

1According to our notation  θ = ∪N

n=1wn

3

3.2 Sensitivity deﬁnition

We are interested in evaluating how inﬂuential a generic parameter wn i is to determine the k-th
output of the network (given an input of the network).
Let the weight wn i vary by a small amount ∆wn i  such that the output varies by ∆y. For small
∆wn i  we have  for each element 

∆yk ≈ ∆wn i

∂yk
∂wn i

by a Taylor series expansion. A weighted sum of the variation in all output elements is then

C(cid:88)

αk |∆yk| = |∆wn i| C(cid:88)

k=1

k=1

(cid:12)(cid:12)(cid:12)(cid:12) ∂yk

∂wn i

(cid:12)(cid:12)(cid:12)(cid:12)

αk

where αk > 0. The sum on the right-hand side is a key quantity for the regularization  so we deﬁne it
as the sensitivity:

Deﬁnition 1 (Sensitivity) The sensitivity of the network output with respect to the (n  i)-th network
parameter is

C(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12) ∂yk

∂wn i

(cid:12)(cid:12)(cid:12)(cid:12)  

S(y  wn i) =

αk

where the coefﬁcients αk are positive and constant.

k=1

(4)

(5)

(6)

(8)

The choice of coefﬁcients αk will depend on the application at hand. In Subsec. 3.5 we propose two
choices of coefﬁcients that will be used in the experiments.
If the sensitivity with respect to a given parameter is small  then a small change of that parameter
towards zero causes a very small change in the network output. After such a change  and if the
sensitivity computed at the new value still is small  then the parameter may be moved towards zero
again. Such an operation can be paired naturally with a procedure like gradient descent  as we propose
below. Towards this end  we introduce the insensitivity function ¯S

¯S(y  wn i) = 1 − S(y  wn i)

(7)
The range of such a function is (−∞; 1] and the lower it is the more the parameter is relevant. We
observe that having ¯S < 0 ⇔ S > 1 means that a weight change brings about an output change
that is bigger than the weight change itself (5). In this case we say the output is super-sensitive
to the weight. In our framework we are not interested in promoting the sparsity for such a class
of parameters; on the contrary  they are very relevant for generating the output. We want to focus
our attention towards all those parameters whose variation is not signiﬁcantly felt by the output
k αk|∆yk| < ∆w)  for which the output is sub-sensitive to them. Hence  we introduce a bounded

((cid:80)

insensitivity

¯Sb(y  wn i) = max(cid:2)0  ¯S (y  wn i)(cid:3)

having ¯Sb ∈ [0  1].

3.3 The update rule

As already hinted at  a parameter with small sensitivity may safely be moved some way towards zero.
This can be done by subtracting a product of the parameter itself and its insensitivity measure (recall
that ¯Sb is between 0 and 1)  appropriately scaled by some small factor λ.
Such a subtraction may be carried out simultaneously with the step towards steepest descent  effec-
tively modifying SGD to incorporate the push of less ‘important’ parameters towards small values.

4

This brings us to the operation at the core of our method – the rule for updating each weight. At the
t-th update iteration  the i-th weight in the n-th layer will be updated as
¯Sb(y  wt−1
n i )

n i := wt−1
wt

− λwt−1

n i − η

(9)

n i

∂L
∂wt−1

n i

where L is a loss function  as in (1) and (2). Here we see why the bounded insensitivity is not allowed
to become negative: this would allow to push some weights (the super-sensitive ones) away from
zero.
Below we show that each of the two correction terms dominates over the other in different phases of
the training. The supplementary material treats this matter in more detail.
The derivative of the ﬁrst correction term in (9) wrt. to the weight (disregarding η) can be factorized
as

which is a scalar product of two vectors: the derivative of the loss with respect to the output elements
and the derivative of the output elements with respect to the parameter in question. By the Hölder
inequality  we have that

.

(11)

(10)

(12)

∂L
∂wn i

=

∂L
∂y

∂y
∂wn i

(cid:12)(cid:12)(cid:12)(cid:12) ∂L

∂wn i

(cid:13)(cid:13)(cid:13)(cid:13)1

∂wn i

k

∂yk

(cid:12)(cid:12)(cid:12)(cid:12) ≤ max
(cid:12)(cid:12)(cid:12)(cid:12) ∂L
(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)(cid:13)(cid:13) ∂y
(cid:13)(cid:13)(cid:13)(cid:13) ∂y
(cid:12)(cid:12)(cid:12)(cid:12) ∂L
(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:12)(cid:12)(cid:12)(cid:12) ≤

∂wn i

∂wn i

.

Furthermore  if the loss function L is the composition of the cross-entropy and the softmax function 
the derivative of L with respect to any yk cannot exceed 1 in absolute value. The inequality in eq.11
then simpliﬁes to

We note that the l1 norm on the right is proportional to the sensitivity of (6)  provided that all
coefﬁcients αk are equal (as in (17) in a later section). Otherwise the l1 norm is equivalent to the
sensitivity. For the following  we think of the l1 norm on the right in eq.12 as a multiple of the
sensitivity.
By (7)  the insensitivity is complementary to the sensitivity. The bounded insensitivity is simply a
restriction of the insensitivity to non-negative values (8).
Now we return to the two correction terms in the update rule of (9). If the ﬁrst correction term is large 
then by (12) also the sensitivity must be large. A large sensitivity implies a small (or zero) bounded
insensitivity. Therefore a large ﬁrst correction term implies a small or zero second correction term.
This typically happens in early phases of training  when the loss can be greatly reduced by changing
a weight  i.e. when ∂L
∂wn i

is large.

Conversely  if the loss function is near a minimum  then the ﬁrst correction term is very small. In
this situation  the above equations do not imply anything about the magnitude of the sensitivity. The
bounded insensitivity may be near 1 for some weights  thus the second correction term will dominate.
These weights will be moved towards zero in proportion to λ. Sec. 4 shows that this indeed happens
for a large number of weights.
The parameter cannot be moved all the way to zero in one update  as the insensitivity may change
when wn i changes; it must be recomputed at each new updated value of the parameter. The factor λ
should therefore be (much) smaller than 1.

3.4 Cost function formulation

The update rule of (9) does provide the “additional information” typical of regularization methods.
Indeed  this method amounts to the addition of a regularization function R to an original loss
function  as in (1). Since (9) speciﬁes how a parameter is updated through the derivative of R  an
integration of the update term will ‘restore’ the regularization term. The result is readily interpreted
for ReLU-activated networks [3].

5

Towards this end  we deﬁne the overall regularization term as a sum over all parameters

R (θ) =

Rn i (wn i)

and integrate each term over wn i

(cid:88)

n

i

(cid:88)
(cid:90)
(cid:34)
1 − C(cid:88)

k=1

(13)

(14)

(cid:35)

(17)

(18)

Rn i (wn i) =

wn i ¯Sb(y  wn i)dwn i.

If we solve (14) we ﬁnd

Rn i (wn i) = H(cid:2) ¯S(y  wn i)(cid:3) w2

n i
2

·

(cid:18) ∂yk

(cid:19) ∞(cid:88)

∂wn i

m=1

αksign

−1m+1 wm−1

(m + 1)!

∂myk
∂wm
n i

(15)
where H(·) is the Heaviside (one-step) function. (15) holds for any feedforward neural network
having any activation function.
Now suppose that all activation functions are rectiﬁed linear units. Its derivative is the step function;
the higher order derivatives are therefore zero. This results in dropping all the m > 1 terms in (15).
Thus  the regularization term for ReLU-activated networks reduces to

Rn i (wn i) =

w2
n i
2

¯S(y  wn i)

(16)

The ﬁrst factor in this expression is the square of the weight  showing the relation to Tikhonov
regularization. The other factor is a selection and damping mechanism. Only the sub-sensitive
weights are inﬂuenced by the regularization – in proportion to their insensitivity.

3.5 Types of sensitivity

In general  (6) allows for different kinds of sensitivity  depending on the value assumed by αk. This
freedom permits some adaptation to the learning problem in question.
If all the k outputs assume the same “relevance” (all αk = 1
formulation

C ) we say we have an unspeciﬁc

Sunspec(y  wn i) =

1
C

(cid:12)(cid:12)(cid:12)(cid:12) ∂yk

∂wn i

(cid:12)(cid:12)(cid:12)(cid:12)

C(cid:88)

k=1

This formulation does not require any information about the training examples.
Another possibility  applicable to classiﬁcation problems  does take into account some extra informa-
tion. In this formulation we let only one term count  namely the one that corresponds to the desired
output class for the given input x0. The factors αk are therefore taken as the elements in the one-hot
encoding for the desired output y∗. In this case we speak of speciﬁc sensitivity:

Sspec(y  y∗  wn i) =

C(cid:88)

k=1

y∗

k

(cid:12)(cid:12)(cid:12)(cid:12) ∂yk

∂wn i

(cid:12)(cid:12)(cid:12)(cid:12)

The experiments in Sec. 4 regard classiﬁcation problems  and we apply both of the above types of
sensitivity.

3.6 Training procedure

Our technique ideally aims to put to zero a great number of parameters. However  according to our
update rule (9)  less sensitive parameters approach zero but seldom reach it exactly. For this reason 
we introduce a threshold T . If

|wn i| < T

the method will prune it. According to this  the threshold in the very early stages must be kept to very
low values (or must be introduced afterwards). Our training procedure is divided into two different
steps:

6

Table 1: LeNet300 network trained over the MNIST dataset

Han et al. [9]

Proposed (Sunspec)
Proposed (Sspec)
Louizos et al. [13]

SWS[10]

Sparse VD[16]

DNS[24]

Proposed (Sunspec)
Proposed (Sspec)

Remaining parameters

FC2
9%

FC1
8%

FC3
Total
26% 21.76k
2.25% 11.93% 69.3% 9.55k
4.78% 24.75% 73.8% 19.39k
33% 26.64k
9.95% 9.68%
11.19k
N/A
N/A
N/A
1.1%
2.7%
38%
3.71k
5.5% 4.72k
1.8%
1.8%
0.93% 1.12% 5.9%
2.53k
1.12% 1.88% 13.4% 3.26k

Memory
footprint
87.04kB
34.2kB
77.56kB
106.57kB
44.76kB
14.84kB
18.88kB
10.12kB
13.06kB

|θ|
|θ(cid:54)=0|
12.2x
27.87x
13.73x
12.2x
23x
68x
56x
103x
80x

Top-1
error
1.6%
1.65%
1.56%
1.8%
1.94%
1.92%
1.99%
1.95%
1.96%

1. Reaching the performance: in this phase we train the network in order to get to the target
performance. Here  any training procedure may be adopted: this makes our method suitable
also for pre-trained models and  unlike other state-of-the-art techniques  can be applied at
any moment of training.

2. Sparsify: thresholding is introduced and applied to the network. The learning process
still advances but in the end of every training epoch all the weights of the network are
thresholded. The procedure is stopped when the network performance drops below a given
target performance.

4 Results

In this section we experiment with our regularization method in different supervised image classiﬁca-
tion tasks. Namely  we experiment training a number of well-known neural network architectures
and over a number of different image datasets. For each trained network we measure the sparsity
with layer granularity and the corresponding memory footprint assuming single precision ﬂoat rep-
resentation of each parameter. Our method is implemented in Julia language and experiments are
performed using the Knet package [22].

4.1 LeNet300 and LeNet5 on MNIST

To start with  we experiment training the fully connected LeNet300 and the convolutional LeNet5
over the standard MNIST dataset [23] (60k training images and 10k test images). We use SGD with a
learning parameter η = 0.1  a regularization factor λ = 10−5 and a thresholding value T = 10−3
unless otherwise speciﬁed. No other sparsity-promoting method (dropout  batch normalization) is
used.
Table 1 reports the results of the experiments over the LeNet300 network in two successive moments
during the training procedure.2 The top-half of the table refers to the network trained up to the point
where the error decreases to 1.6%  the best error reported in [9]. Our method achieves twice the
sparsity of [9] (27.8x vs. 12.2x compression ratio) for comparable error. The bottom-half of the table
refers to the network further trained up to the point where the error settles around 1.95%  the mean
best error reported in [10  16  24]. Also in this case  our method shows almost doubled sparsity over
the nearest competitor for similar error (103x vs. 68x compression ratio of [16]).
Table 2 shows the corresponding results for LeNet-5 trained until the Top-1 error reaches about 0.77%
(best error reported by [9]).
In this case  when compared to the work of Han et al.  our method achieves far better sparsity (51.07x
vs. 11.87x compression ratio) for a comparable error. We observe how in all the previous experiments
the largest gains stem from the ﬁrst fully connected layer  where most of the network parameters lie.
However  if we compare our results to other state-of-the-art sparsiﬁers  we see that our technique does

2

|θ|
|θ(cid:54)=0| is the compression ratio  i.e.

the ratio between number of parameters in the original network

(cardinality of θ) and number of remaining parameters after sparsiﬁcation (the higher  the better).

7

FC1
8%

Table 2: LeNet5 network trained over the MNIST dataset
|θ|
Memory
|θ(cid:54)=0|
footprint
145.12kB 11.9x
33.72kB
51.1x
41.9x
41.12kB
70x
24.6kB
200x
8.6kB
6.16kB
280x
111x
15.52kB

Remaining parameters
FC2
Total
Conv1 Conv2
19% 36.28k
66% 12%
67.6% 11.8% 0.9% 31.0% 8.43k
72.6% 12.0% 1.7% 37.4% 10.28k
6.15k
45%
2.15k
N/A
1.54k
33%
4% 3.88k
14%

0.4%
N/A

5%
36%
N/A
N/A
2% 0.2% 5%
3%

0.7%

Top-1
error
0.77%
0.78%
0.8%
1.0%
0.97%
0.75%
0.91%

Han et al. [9]
Prop. (Sunspec)
Prop. (Sspec)

Louizos et al. [13]

SWS [10]

Sparse VD [16]

DNS [24]

Figure 2: Loss on test set across epochs for LeNet300 trained on MNIST with different regularizers
(without thresholding): our method enables improved generalization over l2-regularization.

not achieve the highest compression rates. Most prominently  Sparse VD obtains higher compression
at better performance compression rates. as is the case of convolutional layers.
Last  we investigate how our sensitivity-based regularization term affects the network generalization
ability  which is the ultimate goal of regularization. As we focus on the effects of the regularization
term  no thresholding or pruning is applied and we consider the unspeciﬁc sensitivity formulation in
(17). We experiment over four formulations of the regularization term R(θ): no regularizer (λ = 0) 
weight decay (Tikhonov  l2 regularization)  l1 regularization  and our sensitivity-based regularizer.
Fig. 2 shows the value of the loss function L (cross-entropy) during training. Without regularization 
the loss increases after some epochs  indicating sharp overﬁtting. With the l1-regularization  some
overﬁtting cannot be avoided  whereas l2-regularization prevents overﬁtting. However  our sensitivity-
based regularizer is even more effective than l2-regularization  achieving lower error. As seen from
(16)  our regularization factor can be interpreted as an improved l2 term with an additional factor
promoting sparsity proportionally to each parameter’s insensitivity.

4.2 VGG-16 on ImageNet

Finally  we experiment on the far more complex VGG-16 [1] network over the larger ImageNet [25]
dataset. VGG-16 is a 13 convolutional  3 fully connected layers deep network having more than
100M parameters while ImageNet consists of 224x224 24-bit colour images of 1000 different types

8

 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0 200 400 600 800 1000Loss testepochSGDSGD+L2SGD+SensitivitySGD+L1of objects. In this case  we skip the initial training step as we used the open-source keras pretrained
model [1]. For the sparsity step we have used SGD with η = 10−3 and λ = 10−5 for the speciﬁc
sensitivity  λ = 10−6 for the unspeciﬁc sensitivity.
As previous experiment revealed our method enables improved sparsiﬁcation for comparable error 
here we train the network up to the point where the Top-1 error is minimized. In this case our method
enables an 1.08% reduction in error (9.80% vs 10.88%) for comparable sparsiﬁcation  supporting the
ﬁnding that our method improves a network ability to generalize as shown in Fig. 2.

Table 3: VGG16 network trained on the ImageNet dataset

FC

Total

Remaining parameters

|θ|
Memory
|θ(cid:54)=0|
footprint
Conv
32.77% 4.61% 10.35M 41.4 MB
13.33x
64.73% 2.9% 11.34M 45.36 MB 12.17x
56.49% 2.56% 9.77M 39.08 MB 14.12x

Top-5
Top-1
error
error
31.34% 10.88%
29.29% 9.80%
30.92% 10.06%

Han et al. [9]
Prop. (Sunspec)
Prop. (Sspec)

5 Conclusions

In this work we have proposed a sensitivity-based regularize-and-prune method for the supervised
learning of sparse network topologies. Namely  we have introduced a regularization term that
selectively drives towards zero parameters that are less sensitive  i.e. have little importance on the
network output  and thus can be pruned without affecting the network performance. The regularization
derivation is completely general and applicable to any optimization problem  plus it is efﬁciency-
friendly  introducing a minimum computation overhead as it makes use of the Jacobian matrices
computed during backpropagation.
Our proposed method enables more effective sparsiﬁcation than other regularization-based methods
for both the speciﬁc and the unspeciﬁc formulation of the sensitivity in fully-connected architectures.
It was empirically observed that for the experiments on MNIST Sunspec reaches higher sparsity than
Sspec  while on ImageNet and on a deeper neural network (VGG16) Sspec is able to reach the highest
sparsity.
Moreover  our regularization seems to have a beneﬁcial impact on the generalization of the network.
However  in convolutional architectures the proposed technique is surpassed by one sparsifying
technique. This might be explained from the fact that our sensitivity term does not take into account
shared parameters.
Future work involves an investigation into the observed improvement of generalization  a study of the
trade-offs between speciﬁc and unspeciﬁc sensitivity  and the extension of the sensitivity term to the
case of shared parameters.

Acknowledgments

The authors would like to thank the anonymous reviewers for their valuable comments and suggestions.
This work was done at the Joint Open Lab Cognitive Computing and was supported by a fellowship
from TIM.

9

References
[1] Karen Simonyan and Andrew Zisserman  “Very deep convolutional networks for large-scale

image recognition ” arXiv preprint arXiv:1409.1556  2014.

[2] Jie Hu  Li Shen  and Gang Sun  “Squeeze-and-excitation networks ” in Conference on Computer

Vision and Pattern Recognition  CVPR  2018.

[3] Xavier Glorot  Antoine Bordes  and Yoshua Bengio  “Deep sparse rectiﬁer neural networks ”
in Proceedings of the 14th International Conference on Artiicial Intelligence and Statistics
(AISTATS)  2011  pp. 315–323.

[4] Alon Brutzkus  Amir Globerson  Eran Malach  and Shai Shalev-Shwartz  “SGD learns over-
parameterized networks that provably generalize on linearly separable data ” arXiv preprint
arXiv:1710.10174  2017.

[5] Hrushikesh N Mhaskar and Tomaso Poggio  “Deep vs. shallow networks: An approximation

theory perspective ” Analysis and Applications  vol. 14  no. 06  pp. 829–848  2016.

[6] Charles W. Groetsch  Inverse Problems in the Mathematical Sciences  Vieweg  1993.
[7] Sayan Mukherjee  Partha Niyogic  Tomaso Poggio  and Ryan Rifkin  “Learning theory: stability
is sufﬁcient for generalization and necessary and sufﬁcient for consistency of empirical risk
minimization ” Advances in Computational Mathematics  vol. 25  pp. 161–193  2006.

[8] Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li  “Learning structured sparsity
in deep neural networks ” in Advances in Neural Information Processing Systems  2016  pp.
2074–2082.

[9] Song Han  Jeff Pool  John Tran  and William Dally  “Learning both weights and connections
for efﬁcient neural network ” in Advances in Neural Information Processing Systems  2015  pp.
1135–1143.

[10] Karen Ullrich  Edward Meeds  and Max Welling  “Soft weight-sharing for neural network

compression ” arXiv preprint arXiv:1702.04008  2017.

[11] Michael Zhu and Suyog Gupta  “To prune  or not to prune: exploring the efﬁcacy of pruning

for model compression ” arXiv preprint arXiv:1710.01878  2017.

[12] Baoyuan Liu  Min Wang  Hassan Foroosh  Marshall Tappen  and Marianna Pensky  “Sparse
convolutional neural networks ” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  2015  pp. 806–814.

[13] Christos Louizos  Max Welling  and Diederik P Kingma  “Learning sparse neural networks

through l_0 regularization ” arXiv preprint arXiv:1712.01312  2017.

[14] Y-lan Boureau  Yann L Cun  et al.  “Sparse feature learning for deep belief networks ” in

Advances in neural information processing systems  2008  pp. 1185–1192.

[15] Eugenio Culurciello  Ralph Etienne-Cummings  and Kwabena A Boahen  “A biomorphic digital

image sensor ” IEEE Journal of Solid-State Circuits  vol. 38  no. 2  pp. 281–294  2003.

[16] Dmitry Molchanov  Arsenii Ashukha  and Dmitry Vetrov  “Variational dropout sparsiﬁes deep

neural networks ” arXiv preprint arXiv:1701.05369  2017.

[17] Lucas Theis  Iryna Korshunova  Alykhan Tejani  and Ferenc Huszár  “Faster gaze prediction

with dense networks and ﬁsher pruning ” arXiv preprint arXiv:1801.05787  2018.

[18] Song Han  Huizi Mao  and William J Dally  “Deep compression: Compressing deep neural net-
works with pruning  trained quantization and huffman coding ” arXiv preprint arXiv:1510.00149 
2015.

[19] Andries P Engelbrecht and Ian Cloete  “A sensitivity analysis algorithm for pruning feedforward
neural networks ” in Neural Networks  1996.  IEEE International Conference on. IEEE  1996 
vol. 2  pp. 1274–1278.

[20] Iveta Mrázová and Zuzana Reitermanová  “A new sensitivity-based pruning technique for
feed-forward neural networks that improves generalization ” in Neural Networks (IJCNN)  The
2011 International Joint Conference on. IEEE  2011  pp. 2143–2150.

[21] Iveta Mrazova and Marek Kukacka  “Can deep neural networks discover meaningful pattern

features? ” Procedia Computer Science  vol. 12  pp. 194–199  2012.

10

[22] Deniz Yuret  “Knet: beginning deep learning with 100 lines of julia ” in Machine Learning

Systems Workshop at NIPS 2016  2016.

[23] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document

recognition ” Proceedings of the IEEE  vol. 86  no. 11  pp. 2278 – 2324  Nov. 1998.

[24] Yiwen Guo  Anbang Yao  and Yurong Chen  “Dynamic network surgery for efﬁcient dnns ” in

Advances In Neural Information Processing Systems  2016  pp. 1379–1387.

[25] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei 
“Imagenet large scale visual recognition challenge ” International Journal of Computer Vision 
vol. 115  no. 3  pp. 211–252  Dec. 2015.

11

,Michaël Defferrard
Xavier Bresson
Pierre Vandergheynst
Enzo Tartaglione
Skjalg Lepsøy
Attilio Fiandrotti
Gianluca Francini
Kamalika Chaudhuri
Jacob Imola
Ashwin Machanavajjhala