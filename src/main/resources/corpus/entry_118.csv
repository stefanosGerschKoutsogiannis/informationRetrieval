2015,A Market Framework for Eliciting Private Data,We propose a mechanism for purchasing information from a sequence of participants.The participants may simply hold data points they wish to sell  or may have more sophisticated information; either way  they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism's future prediction on a test set.The mechanism  which draws on the principles of prediction markets  has a bounded budget and minimizes generalization error for Bregman divergence loss functions.We then show how to modify this mechanism to preserve the privacy of participants' information: At any given time  the current prices and predictions of the mechanism reveal almost no information about any one participant  yet in total over all participants  information is accurately aggregated.,A Market Framework for Eliciting Private Data

Bo Waggoner
Harvard SEAS

bwaggoner@fas.harvard.edu

Rafael Frongillo

University of Colorado
raf@colorado.edu

Jacob Abernethy

University of Michigan

jabernet@umich.edu

Abstract

We propose a mechanism for purchasing information from a sequence of partici-
pants. The participants may simply hold data points they wish to sell  or may have
more sophisticated information; either way  they are incentivized to participate as
long as they believe their data points are representative or their information will
improve the mechanism’s future prediction on a test set. The mechanism  which
draws on the principles of prediction markets  has a bounded budget and mini-
mizes generalization error for Bregman divergence loss functions. We then show
how to modify this mechanism to preserve the privacy of participants’ informa-
tion: At any given time  the current prices and predictions of the mechanism reveal
almost no information about any one participant  yet in total over all participants 
information is accurately aggregated.

1

Introduction

E
x y

A ﬁrm that relies on the ability to make difﬁcult predictions can gain a lot from a large collection
of data. The goal is often to estimate values y ∈ Y given observations x ∈ X according to an
appropriate class of hypotheses F describing the relationship between x and y (for example  y = a·
x + b for linear regression). In classic statistical learning theory  the goal is formalized as attempting
to approximately solve

min
f∈F

Loss(f ; (x  y))

(1)
where Loss(·) is an appropriate inutility function and (x  y) is drawn from an unknown distribution.
In the present paper we are concerned with the case in which the data are not drawn or held by a
central authority but are instead inherently distributed. By this we mean that the data is (disjointly)
partitioned across a set of agents  with agent i privately possessing some portion of the dataset Si 
and agents have no obvious incentive to reveal this data to the ﬁrm seeking it. The vast swaths of data
available in our personal email accounts could provide massive beneﬁts to a range of companies  for
example  but users are typically loathe to provide account credentials  even when asked politely.
We will be concerned with the design of ﬁnancial mechanisms that provide a community of agents 
each holding a private set of data  an incentive to contribute to the solution of a large learning or
prediction task. Here we use the term ‘mechanism’ to mean an algorithmic interface that can receive
and answer queries  as well as engage in monetary exchange (deposits and payouts). Our aim will
be to design such a mechanism that satisﬁes the following three properties:

1. The mechanism is efﬁcient in that it approaches a solution to (1) as the amount of data and

participation grows while spending a constant  ﬁxed total budget.

2. The mechanism is incentive-compatible in the sense that agents are rewarded when their
contributions provide marginal value in terms of improved hypotheses  and are not re-
warded for bad or misleading information.
3. The mechanism provides reasonable privacy guarantees  so that no agent j (or outside
observer) can manipulate the mechanism in order to infer the contributions of agent i (cid:54)= j.

1

Ultimately we would like our mechanism to approach the performance of a learning algorithm that
had direct access to all the data  while only spending a constant budget to acquire data and improve
predictions and while protecting participants’ privacy.
Our construction relies on the recent surge in literature on prediction markets [13  14  19  20] 
popular for some time in the ﬁeld of economics and recently studied in great detail in computer
science [8  16  6  15  18  1]. A prediction market is a mechanism designed for the purpose of
information aggregation  particularly when there is some underlying future event about which many
members of the population may have private and useful information. For instance  it may elicit
predictions about which team will win an upcoming sporting event  or which candidate will win an
election. These predictions are eventually scored on the actual outcome of the event.
Applying these prediction market techniques allows participants to essentially “trade in a market”
based on their data. (This approach is similar to prior work on crowdsourcing contests [3].) Members
of the population have private information  just as with prediction markets — in this case  data points
or beliefs — and the goal is to incentivize them to reveal and aggregate that information into a ﬁnal
hypothesis or prediction. Their ﬁnal proﬁts are tied to the outcome of a test set of data  with each
participant being paid in accordance with how much their information improved the performance
on the test set. Our techniques depart from the framework of [3] in two signiﬁcant aspects: (a) we
focus on the particular problem of data aggregation  and most of our results take advantage of kernel
methods; and (b) our mechanisms are the ﬁrst to combine differential privacy guarantees with data
aggregation in a prediction-market framework.
This framework will provide efﬁciency and truthfulness. We will also show how to achieve privacy
in many scenarios. We will give mechanisms where the prices and predictions published satisfy
(  δ)-differential privacy [10] with respect to each participant’s data. The mechanism’s output can
still give reasonable predictions while no observer can infer much about any participant’s input data.

2 Mechanisms for Eliciting and Aggregating Data

We now give a broad description of the mechanism we will study. In brief  we imagine a central
authority (the mechanism  or market) maintaining a hypothesis f t representing the current aggrega-
tion of all the contributions made thus far. A new (or returning) participant may query f t at no cost 
perhaps evaluating the quality of the predictions on a privately-held dataset  and can then propose an
update df t+1 to f t that possibly requires an investment (a “bet”). Bets are evaluated at the close of
the market when a true data sample is generated (analogous to a test set)  and payouts are distributed
according to the quality of the updates.
After describing this initial framework as Mechanism 1  which is based loosely on the setting of
[3]  we turn our attention to the special case in which our hypotheses must lie in a Reproducing
Kernel Hilbert Space (RKHS) [17] for a given kernel k(· ·). This kernel-based “nonparametric
mechanism” is particularly well-suited for the problem of data aggregation  as the betting space of
the participants consists essentially of updates of the form df t = αtk(zt ·)  where zt is the data
object offered by the participant and αt ∈ R is the “magnitude” of the bet.
A drawback of Mechanism 1 is the lack of privacy guarantees associated with the betting protocol:
utilizing one’s data to make bets or investments in the mechanism can lead to a loss of privacy for
the owner of that data. When a participant submits a bet of the form df t = αtk(zt ·)  where zt
could contain sensitive personal information  another participant may be able to infer zt by querying
the mechanism. One of the primary contributions of the present work  detailed in Section 3  is a
technique to allow for productive participation in the mechanism while maintaining a guarantee on
the privacy of the data submitted.

2.1 The General Template
There is a space of examples X ×Y  where x ∈ X are features and y ∈ Y are labels. The mechanism
designer chooses a function space F consisting of f : X × Y → R  and assumed to have Hilbert
space structure; one may view F as either the hypothesis class or the associated loss class  that is
where fh(x  y) measures the loss/performance of hypothesis h on observation x and label y. In each
case we will refer to f ∈ F as a hypothesis  eliding the distinction between fh and h.

2

The pricing scheme of the mechanism relies on a convex cost function Cx(·) : F → R which is
parameterized by elements x ∈ X but whose domain is the set of hypotheses F. The cost function
is publicly available and determined in advance. The interaction with the mechanism is a sequential
process of querying and betting. On round t − 1 the mechanism publishes a hypothesis f t−1  the
“state” of the market  which participants may query. Each participant arrives sequentially  and on
round t a participant may place a “bet” df t ∈ F  also called a “trade” or “update”  modifying the
hypothesis f t−1 → f t = f t−1 + df t. Finally participation ends and the mechanism samples (or
reveals) a test example1 (x  y) from the underlying distribution and pays (or charges) each participant
according to the relative performance of their marginal contributions. Precisely  the total reward for
participant t’s bet df t is the value df t(x  y) minus the cost Cx(f t) − Cx(f t−1).

Mechanism 1: The Market Template
MARKET announces f 0 ∈ F
for t = 1  2  . . .   T do

PARTICIPANT may query functions ∇f Cx(f t−1) and f t−1(x  y) for examples (x  y)
PARTICIPANT t may submit a bet df t ∈ F to MARKET
MARKET updates state f t = f t−1 + df t

MARKET observes a true sample (x  y)
for t = 1  2  . . .   T do

PARTICIPANT t receives payment df t(x  y) + Cx(f t−1) − Cx(f t)

The design of cost-function prediction markets has been an area of active research over the past
several years  starting with [8] and many further reﬁnements and generalizations [1  6  15]. The
general idea is that the mechanism can efﬁciently provide price quotes via a function C(·) which
acts as a potential on the space of outstandings shares; see [1] for a thorough review. In the present
work we have added an additional twist which is that the function Cx(·) is given an additional
parameterization of the observation x. We will not dive too deeply into the theoretical aspects of
this generalization  but this is a straightforward extension of existing theory.

uation is given by fθ(x  y) = (cid:104)θ  φ(x  y)(cid:105). We let Cx(f ) := log(cid:82)
Cx(fθ) = log(cid:82)

Key special case: exponential family mechanism. For those more familiar with statistics and
machine learning  there is a natural and canonical family of problems that can be cast within the
general framework of Mechanism 1  which we will call the exponential family prediction mech-
anism following [2]. Assume that F can be parameterized as F = {fθ : θ ∈ Rd}  that we
are given a sufﬁcient statistics summary function φ : X × Y → Rd  and that function eval-
Y exp(f (x  y))dy so that
Y exp((cid:104)θ  φ(x  y)(cid:105)dy. In other words  we have chosen our mechanism to encode
a particular exponential family model  with Cx(·) chosen as the conditional log partition function
over the distribution on y given x. If the market has settled on a function fθ  then one may interpret
that as the aggregate market belief on the distribution of X × Y is
X×Y exp((cid:104)θ  φ(x  y)(cid:105)) dx dy.
pθ(x  y) = exp((cid:104)θ  φ(x  y)(cid:105) − A(θ))
How may we view this as a “market aggregate” belief? Notice that if a trader observes the market
state of fθ and she is considering a bet of the form df = fθ − fθ(cid:48)  the eventual proﬁt will be

A(θ) = log(cid:82)

where

fθ(cid:48)(x  y) − fθ(x  y) + Cx(fθ) − Cx(fθ(cid:48)) = log

pθ(cid:48)(y|x)
pθ(y|x)

.

I.e.  the proﬁt is precisely the conditional log likelihood ratio of the update θ → θ(cid:48).
Example: Logistic regression. Let X = Rk  Y = {−1  1}  and take F to be the set of func-
tions fθ(x  y) = y · (θ(cid:62)x) for θ ∈ Rk. Then by our construction  Cx(f ) = log(exp(f (x  1)) +
exp(f (x −1))) = log(exp(θ(cid:62)x) + exp(−θ(cid:62)x))  and we let f 0 = f0 ≡ 0. The payoff of a
participant placing a bet which moves the market state to f 1 = fθ  upon outcome (x  y)  is:
fθ(x  y) + Cx(f0) − Cx(fθ) = yθ(cid:62)x + log(2) − log(exp(θ(cid:62)x) + exp(−θ(cid:62)x))

= log(2) − log(1 + exp(−2yθ(cid:62)x))  

1This can easily be extended to a test set by taking the average performance over the test set.

3

which is simply negative logistic loss of the parameter choice 2θ. A participant wishing to maximize
proﬁt under a belief distribution p(x  y) should therefore choose θ via logistic regression 

(cid:2)log(1 − exp(2yθ(cid:62)x))(cid:3) .

(2)

θ∗ = arg min

E

(x y)∼p

2.2 Properties of the Market

θ

We next describe two nice properties of Mechanism 1: incentive-compatibility and bounded bud-
get. Recall that  for the exponential family markets discussed above  a trader moving the market
hypothesis from f t−1 to f t was compensated according to the conditional log-likelihood ratio of
f t−1 and f t on the test data point. The implication is that traders are incentivized to minimize a
KL divergence between the market’s estimate of the distribution and the true underlying distribu-
tion. We refer to this property as incentive-compatibility because traders’ interests are aligned with
the mechanism designer’s. This property indeed holds generally for Mechanism 1  where the KL
divergence is replaced with a general Bregman divergence corresponding to the Fenchel conjugate
of Cx(·); see Proposition 1 in the appendix for details.
Given that the mechanism must make a sequence of (possibly negative) payments to traders  a natural
question is whether there is the potential for large downside for the mechanism in terms of total
payment (budget).
In the context of the exponential family mechanism  this question is easy to
answer: after a sequence of bets moving the market state parameter θ0 → θ1 → . . . → θﬁnal  the
total loss to the mechanism corresponds to the total payouts made to traders 

(cid:88)

i

fθi+1(x  y) − fθi (x  y) + Cx(fθi) − Cx(fθi+1) = log

pθﬁnal(y|x)
pθ0 (y|x)

;

that is  the worst-case loss is exactly the worst-case conditional log-likelihood ratio. In the context
of logistic regression this quantity can always be guaranteed to be no more than log 2 as long as
the initial parameter is set to θ = 0. For Mechanism 1 more generally  one has tight bounds on
the worst-case loss following from such results from prediction markets [1  8]  and we give a more
detailed statement in Proposition 2 in the appendix.

In choosing the cost function family C = {Cx : x ∈ X}  an
Price sensitivity parameter λC.
important consideration is the “scale” of each Cx  or how quickly changes in the market hypothesis
f t translate to changes in the “instantaneous prices” ∇Cx(f t) (which give the marginal cost for an
inﬁnitesimal bet df t+1). Formally  this is captured by the price sensitivity λC  deﬁned as the upper
bound on the operator norm (with respect to the L1 norm) of the Hessian of the cost function Cx
(over all x). A choice of small λC translates to a small worst-case budget required by the mechanism.
However  it means that the market prices are sensitive in that the same update df t changes the prices
much more quickly. When we consider protecting the privacy of trader updates in Section 3  we will
see that privacy imposes restrictions on the price sensitivity.

2.3 A Nonparametric Mechanism via Kernel Methods
The framework we have discussed thus far has involved a general function space F as the “state”
of the mechanism  and the contributions by participants are in the form of modiﬁcations to these
functions. One of the downsides of this generic template is that participants may not be able to reason
about F  and they may have information about the optimal f only through their own privately-held
dataset S ⊂ X ×Y. A more speciﬁc class of functions would be those parameterized by actual data.
This brings us to a well-studied type of non-parametric hypothesis class  namely the reproducing
kernel Hilbert space (RKHS). We can design a market based on an RKHS  which we will refer to
as a kernel market  that brings together a number of ideas including recent work of [21] as well as
kernel exponential families [4].
We have a positive semideﬁnite kernel k : Z × Z → R and associated reproducing kernel Hilbert
space F  with basis {fz(·) = k(z ·) : z ∈ Z}. The reproducing property is that for all f ∈ F 
s αsk(zs ·) for
some collection of points {(αs  zs)}.
The kernel approach has several nice properties. One is a natural extension of the exponential family
mechanism using an RKHS as a building block of the class of exponential family distributions [4]. A

(cid:104)f  k(z ·)(cid:105) = f (z). Now each hypothesis f ∈ F can be expressed as f (·) = (cid:80)

4

yy(cid:48)k(x  x(cid:48)). Under certain conditions [4]  we again can take Cx(f ) = log(cid:82)

key assumption in the exponential family mechanism is that evaluating f can be viewed as an inner
product in some feature space; this is precisely what one has given a kernel framework. Speciﬁcally 
assume we have some PSD kernel k : X × X → R  where Y = {−1  1}. Then we can deﬁne the
associated classiﬁcation kernel ˆk : (X × Y) × (X × Y) → R according to ˆk((x  y)  (x(cid:48)  y(cid:48))) :=
Y exp(f (x  y))dy  and
for any f in the RKHS associated to ˆk  we have an associated distribution of the form pf (x  y) ∝
exp(f (x  y)). And again  a participant updating the market from f t−1 to f t is rewarded by the
conditional log-likelihood ratio of f t−1 and f t on the test data.
The second nice property mirrors one of standard kernel learning methods  namely that under cer-
tain conditions one need only search the subset of the RKHS spanned by the basis {k((xi  yi) ·) :
(xi  yk) ∈ S}  where S is the set of available data; this is a direct result of the Representer Theo-
rem [17]. In the context of the kernel market  this suggests that participants need only interact with
the mechanism by pushing updates that lie in the span of their own data. In other words  we only
need to consider updates of the form df = αk((x  y) ·). This naturally suggests the idea of directly
purchasing data points from traders.

Buying Data Points. So far  we have supposed that a participant knows what trade df t she prefers
to make. But what if she simply has a data point (x  y) drawn from the underlying distribution?
We would like to give this trader a “simple” trading interface in which she can sell her data to the
mechanism without having to reason about the correct df t for this data point.
Our proposal is to mimic the behavior of natural learning algorithms  such as stochastic gradient
descent  when presented with (x  y). The market can offer the trader the purchase bundle corre-
sponding to the update of the learning algorithm on this data point. In principle  this approach can
be used with any online learning algorithm. In particular  stochastic gradient descent gives a clean
update rule  which we now describe. The expected proﬁt (which is the negative of expected loss)
for trade df t is Ex
gradient is −∇f t−1Cx + δx y (where δx y is the indicator on data point x  y). This suggests that the

(cid:2)Cx(f t−1 + df t) − Cx(f t−1) − Ey|x[df t(x  y)](cid:3). Given a draw (x  y)  the loss
function on which to take a gradient step is −(cid:0)Cx(f t−1 + df t) − Cx(f t−1) − df t(x  y)(cid:1)  whose
(cid:1)  where  can be chosen arbitrarily
market offer the participant the trade df t = (cid:0)∇f t−1Cx − δx y

as a “learning rate”. This can be interpreted as buying a unit of shares in the participant’s data point
(x  y)  then “hedging” by selling a small amount of all other shares in proportion to their current
prices (recall that the current prices are given by ∇f t Cx).
In the kernel setting  the choice of stochastic gradient descent may be somewhat problematic  be-
cause it can result in non-sparse share purchases. It may instead be desirable to use algorithms that
guarantee sparse updates—a modern discussion of such approaches can be found in [22  23].
Given this framework  participants with access to a private set of samples from the true underlying
distribution can simply opt for this “standard bundle” corresponding to their data point  which is
precisely a stochastic gradient descent update. With a small enough learning rate  and assuming
that the data point is truly independent of the current hypothesis (i.e. (x  y) has not been previously
incorporated)  the trade is guaranteed to make at least some positive proﬁt in expectation. More
sophisticated alternative strategies are also possible of course  but even the proposed simple bet type
has earning potential.

3 Protecting Participants’ Privacy

We now extend the mechanism to protect privacy of the participants: An adversary observing the
hypotheses and prices of the mechanism  and even controlling the trades of other participants  should
not be able to infer too much about any one trader’s update df t. This is especially relevant when
participants sell data to the mechanism and this data can be sensitive  e.g. medical data.
Here  privacy is formalized by (  δ)-differential privacy  to be deﬁned shortly. One intuitive charac-
terization is that  for any prior distribution some adversary has about a trader’s data  the adversary’s
posterior belief after observing the mechanism would be approximately the same even if the trader
did not participate at all. The idea is that  rather than posting the exact prices and trades made in the
market  we will publish noisy versions  with the random noise giving the above guarantee.

5

A naive approach would be to add independent noise to each participant’s trade. However  this would
require a prohibitively-large amount of noise; the ﬁnal market hypothesis would be determined by
the random noise just as much as by the data and trades. The central challenge is to add carefully
correlated noise that is large enough to hide the effects of any one participant’s data point  but not
so large that the prices (equivalently  hypothesis) become meaningless. We show this is possible
by adjusting the “price sensitivity” λC of the mechanism  a measure of how fast prices change
in response to trades deﬁned in 2.2.
It will turn out to sufﬁce to set the price sensitivity to be
O(1/polylog T ) when there are T participants. This can roughly be interpreted as saying that any
one participant does not move the market price noticeably (so their privacy is protected)  but just
O(polylog T ) traders together can move the prices completely.
We now formally deﬁne differential privacy and discuss two useful tools at our disposal.

3.1 Differential Privacy and Tools

Differential privacy in our context is deﬁned as follows. Consider a randomized function M op-
erating on inputs of the form (cid:126)f = (df 1  . . .   df T ) and having outputs of the form s. Then M is
(  δ)-differentially private if  for any coordinate t of the vector  any two distinct df t
2  and any
1  df t
2) ∈ S] + δ. The
1) ∈ S)] ≤ e Pr[M (f−t  df t
(measurable) set of outputs S  we have Pr[M (f−t  df t
notation f−t means the vector (cid:126)f with the tth entry removed.
Intuitively  M is private if modifying the tth entry in the vector to a different entry does not change
the distribution on outputs too much. In our case  the data to be protected will be the trade df t of each
participant t  and the space of outputs will be the entire sequence of prices/predictions published by
the mechanism.
To preserve privacy  each trade must have a bounded size (e.g. consist only of one data point). To
enforce this  we deﬁne the following parameter chosen by the mechanism designer:

(cid:112)(cid:104)df  df(cid:105) 
df = αk(z ·)  then we would have ∆ = maxα z α(cid:112)k(z  z).

∆ = max

allowed df

We next describe the two tools we require.

where the maximum is over all trades df allowed by the mechanism. That is  ∆ is a scalar capturing
the maximum allowed size of any one trade. For instance  if all trades are restricted to be of the form

(3)

(4)

Tool 1: Private functions via Gaussian processes. Given a current market state f t = f 0 + df 1 +
··· + df t  where f t lies in a RKHS  we construct a “private” version ˆf t such that queries to ˆf t are
“accurate” — close to the outputs of f t — but also private with respect to each df j. In fact  it will
become convenient to privately output partial sums of trades  so we wish to output a ˆft1:t2 that is
df j. This is accomplished by the following construction

private and approximates ft1:t2 =(cid:80)t2

due to [11].
Theorem 1 ([11]  Corollary 9). Let G be the sample path of a Gaussian process with mean zero and
whose covariance is given by the kernel function k.2 Then

j=t1

√

ˆft1:t2 = ft1:t2 + ∆



2 ln(2/δ)

G .

is (  δ)-differentially private with respect to each df j for j ∈ {t1  . . .   t2}.
In general  ˆft1:t2 may be an inﬁnite-dimensional object and thus impossible to ﬁnitely represent.
In this case  the theorem implies that releasing the results of any number of queries ˆft1:t2(z) is
differentially private. (Of course  the more queries that are released  the larger the chance of high
error on some query.) This is computationally feasible as each sample G(z) is simply a sample from
a Gaussian having known covariance with the previous samples drawn.
Unfortunately  it would not be sufﬁcient to independently release ˆf1:t at each time t  because the
amount of noise required would be prohibitive. This leads us to our next tool.

2Formally  each G(z) is a random variable and  for any ﬁnite subset of Z  the corresponding variables are

distributed as a multivariate normal with covariance given by k.

6

•
df 0

•
df 1

•
df 2

•
df 3

•
df 4

•
df 5

•
df 6

•
df 7

•
df 8

•
df 9

•
df 10

•
df 11

•
df 12

•
df 13

•
df 14

•
df 15

•
df 16

point sold to the market). The goal is to release  at each time step t  a noisy version of f t =(cid:80)t

Figure 1: Picturing the continual observation technique for preserving privacy. Each df t is a trade (e.g. a data
j=1 df j. To do
so  start at t and follow the arrow back to s(t). Take the partial sum of df j for j from s(t) to t and add some
random noise. Trace the next arrow from s(t) to s(s(t)) to get another partial sum and add noise to that sum
as well. Repeat until 0 is reached  then add together all the noisy partial sums to get the output at time t  which
will equal f t plus noise. The key point is that we can re-use many of the noisy partial sums in many different
time steps. For instance  the noisy partial sum from 0 to 8 can be re-used when releasing all of f 9  . . .   f 15.
Meanwhile  each df t participates in few noisy partial sums (the number of arrows passing above it).

construct ˆf t =(cid:80)t

Tool 2: Continual observation technique. The idea of this technique  pioneered by [9  5]  is to
j=0 df t by adding together noisy partial sums of the form ˆft1:t2 as constructed in
Equation 4. The idea for choosing these partial sums is pictured in Figure 1: For a function s(t) that
returns an integer smaller than t  we take ˆf t = ˆf s(t)+1:t + ˆf s(s(t))+1:s(t) + ··· + ˆf 0:0. Speciﬁcally 
s(t) is determined by writing t in binary  then ﬂipping the rightmost “one” bit to zero. This is
pictured in Figure 1. The intuition behind why this technique helps is twofold. First  the total noise
in ˆf t is the sum of noises of its partial sums  and it turns out that there are at most (cid:100)log T(cid:101) terms.
Second  the total noise we need to add to protect privacy is governed by how many different partial
sums each df j participates in  and it turns out that this number is also at most (cid:100)log T(cid:101). This allows
for much better privacy and accuracy guarantees than naively treating each step independently.

3.2 Mechanism and Results

Combining our market template in Mechanism 1 with the above privacy tools  we obtain Mecha-
nism 2. There are some key differences. First  we have a bound Q on the total number of queries.
(Each query x returns the instantaneous prices in the market for x.) This is because each query
reveals information about the participants  so intuitively  allowing too many queries must sacriﬁce
either privacy or accuracy. Fortunately  this bound Q can be an arbitrarily large polynomial in the
number of traders without affecting the quality of the results. Second  we have PAC-style guaran-
tees on accuracy: with probability 1 − γ  all price queries return values within α of their true prices.
Third  it is no longer straightforward to compute and represent the market prices ∇Cx( ˆf t) unless Y
is ﬁnite. We leave the more general analysis of Mechanism 2 to future work.
Either exactly or approximately  Mechanism 2 inherits the desirable properties of Mechanism 1  such
as bounded budget and incentive-compatitibility (that is  participants are incentivized to minimize
the risk of the market hypothesis). In addition  we show that it preserves privacy while maintaining
accuracy  for an appropriate choice of the price sensitivity λC.
Theorem 2. Consider Mechanism 2  where ∆ is the maximimum trade size (Equation 3) and d =
|Y|. Then Mechanism 2 is (  δ) differentially private and  with T traders and Q price queries 
has the following accuracy guarantee: with probability 1 − γ  for each query x the returned prices
satisfy (cid:107)∇Cx( ˆf t) − ∇Cx(f t)(cid:107)∞ ≤ α by setting
(cid:113)

λC =

α

.

2d∆2

ln Qd

γ ln 2 log T

δ

log(T )3

If one for example takes δ  γ = exp [−polylog(Q  T )]  then except for a superpolynomially low fail-
ure probability  Mechanism 2 answers all queries to within accuracy α by setting the price sensitivity
to be λC = O (α/polylog(Q  T )). We note  however  that this is a somewhat weaker guarantee
than is usually desired in the differential privacy literature  where ideally δ is exponentially small.

7

Mechanism 2: Privacy Protected Market
Parameters:   δ (privacy)  α  γ (accuracy)  k (kernel)  ∆ (trade size 3)  Q (#queries)  T (#traders)
MARKET announces ˆf 0 = f 0  sets r = 0  sets C with λC = λC(  δ  α  γ  ∆  Q  T ) (Theorem 2)
for t = 1  2  . . .   T do

PARTICIPANT t proposes a bet df t
MARKET updates true position f t = f t−1 + df t
MARKET instantiates ˆf s(t)+1 t as deﬁned in Equation 4
while r ≤ Q and some OBSERVER wishes to make a query do

OBSERVER r submits pricing query on x
MARKET returns prices ∇Cx( ˆf t)  where ˆf t = ˆf s(t)+1:t + ˆf s(s(t))+1:s(t) + ··· + ˆf 0:0
MARKET sets r ← r + 1

MARKET observes a true sample (x  y)
for t = 1  2  . . .   T do

PARTICIPANT receives payment f t−1(x  y) − f t(x  y) − Cx( ˆf t−1 + df t) + Cx( ˆf t−1)

when C comes from an exponential family  so that Cx(f ) = log(cid:82)

Computing ∇Cx( ˆf t). We have already discussed limiting to ﬁnite |Y| in order to efﬁciently com-
pute the marginal prices ∇Cx( ˆf t). However  it is still not immediately clear how to compute these
prices  and hence how to implement Mechanism 2. Here  we show that the problem can be solved
Y exp [f (x  y)] dy. In this case 
the marginal prices given by the gradient of C have a nice exponential-weights form  namely the
(cid:80)
y∈Y ef (x y) . Thus evaluating the prices can be
price of shares in (x  y) is pt
done by evaluating f t(x  y) for each y ∈ Y.
We also note that the worst-case bound used here could be greatly improved by taking into account
the structure of the kernel. For “smooth” cases such as the Gaussian kernel  querying a second
point very close to the ﬁrst one requires very little additional randomness and builds up very little
additional error. We gave only a worst-case bound that holds for all kernels.
Adding a transaction fee.
Adding a small Θ(α) fee sufﬁces to deter arbitrage opportunities introduced by noisy pricing.

In the appendix  we discuss the potential need for transaction fees.

x(y) = ∇yCx(f t) =

ef (x y)

Discussion

The main contribution of this work was to bring together several tools to construct a mechanism
for incentivized data aggregation with “contest-like” incentive properties  privacy guarantees  and
limited downside for the mechanism.
Our proposed mechanisms are also extensions of the prediction market literature. Building upon the
work of Abernethy et al. [1] we introduce the following innovations:
• Conditional markets. Our framework of Mechanism 1 can be interpreted as a prediction market
for conditional predictions p(y|x) rather than a classic market which would elicit the joint dis-
tribution p(x  y)  or just the marginals. (This is similar to decision markets [12  7]  but without
out the associated incentive problems.) Naturally then  we couple conditional predictions with
restricted hypothesis spaces  allowing F to capture  e.g.  a linear relationship between x and y.
• Nonparametric securities. We also extend to nonparametric hypothesis spaces using kernels 
• Privacy guarantees. We provide the ﬁrst private prediction market (to our knowledge)  showing
that information about individual trades is not revealed. Our approach for preserving privacy also
holds in the classic prediction market setting with similar privacy and accuracy guarantees.

following the kernel-based scoring rules of [21].

Many directions remain for future work. These mechanisms could be made more practical and
perhaps even better privacy guarantees derived  especially in nonparametric settings. One could also
explore the connections to similar settings  such as when agents have costs for acquiring data.
Acknoledgements J. Abernethy acknowledges the generous support of the US National Science
Foundation under CAREER Grant IIS-1453304 and Grant IIS-1421391.

8

References
[1] Jacob Abernethy  Yiling Chen  and Jennifer Wortman Vaughan. Efﬁcient market making via convex
optimization  and a connection to online learning. ACM Transactions on Economics and Computation 
1(2)  May 2013.

[2] Jacob Abernethy  Sindhu Kutty  S´ebastien Lahaie  and Rahul Sami. Information aggregation in expo-
nential family markets. In Proceedings of the ﬁfteenth ACM conference on Economics and computation 
pages 395–412. ACM  2014.

[3] Jacob D Abernethy and Rafael M Frongillo. A collaborative mechanism for crowdsourcing prediction

problems. In Advances in Neural Information Processing Systems  pages 2600–2608  2011.

[4] St´ephane Canu and Alex Smola. Kernel methods and the exponential family. Neurocomputing  69(7):714–

720  2006.

[5] T-H Hubert Chan  Elaine Shi  and Dawn Song. Private and continual release of statistics. ACM Transac-

tions on Information and System Security (TISSEC)  14(3):26  2011.

[6] Y. Chen and J.W. Vaughan. A new understanding of prediction markets via no-regret learning. In Pro-

ceedings of the 11th ACM Conference on Electronic Commerce (EC)  pages 189–198  2010.

[7] Yiling Chen  Ian Kash  Mike Ruberry  and Victor Shnayder. Decision markets with good incentives. In

Internet and Network Economics  pages 72–83. Springer  2011.

[8] Yiling Chen and David M. Pennock. A utility framework for bounded-loss market makers. In In Proceed-

ings of the 23rd Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 49–56  2007.

[9] Cynthia Dwork  Moni Naor  Toniann Pitassi  and Guy N Rothblum. Differential privacy under continual
observation. In Proceedings of the forty-second ACM symposium on Theory of computing  pages 715–724.
ACM  2010.

[10] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and

Trends in Theoretical Computer Science  2014.

[11] Rob Hall  Alessandro Rinaldo  and Larry Wasserman. Differential privacy for functions and functional

data. The Journal of Machine Learning Research  14(1):703–727  2013.

[12] R Hanson. Decision markets. Entrepreneurial Economics: Bright Ideas from the Dismal Science  pages

79–85  2002.

[13] R. Hanson. Combinatorial information market design.

2003.

Information Systems Frontiers  5(1):105–119 

[14] R. Hanson. Logarithmic market scoring rules for modular combinatorial information aggregation. Journal

of Prediction Markets  1(1):3–15  2007.

[15] Abraham Othman and Tuomas Sandholm. Automated market makers that enable new settings: extending
constant-utility cost functions. In Proceedings of the Second Conference on Auctions  Market Mechanisms
and their Applications (AMMA)  pages 19–30  2011.

[16] David M. Pennock and Rahul Sami. Computational aspects of prediction markets.

In Noam Nisan 
Tim Roughgarden  Eva Tardos  and Vijay V. Vazirani  editors  Algorithmic Game Theory  chapter 26.
Cambridge University Press  2007.

[17] Bernhard Sch¨olkopf and Alexander J Smola. Learning with kernels: Support vector machines  regular-

ization  optimization  and beyond. MIT press  2002.

[18] Amos J. Storkey. Machine learning markets. In Proceedings of AI and Statistics (AISTATS)  pages 716–

724  2011.

[19] J. Wolfers and E. Zitzewitz. Prediction markets. Journal of Economic Perspectives  18(2):107–126  2004.
[20] Justin Wolfers and Eric Zitzewitz. Interpreting prediction market prices as probabilities. Technical report 

National Bureau of Economic Research  2006.

[21] Erik Zawadzki and S´ebastien Lahaie. Nonparametric scoring rules. In Proceedings of the Twenty-Ninth

AAAI Conference on Artiﬁcial Intelligence  2015.

[22] Lijun Zhang  Rong Jin  Chun Chen  Jiajun Bu  and Xiaofei He. Efﬁcient online learning for large-scale

sparse kernel logistic regression. In AAAI  2012.

[23] Lijun Zhang  Jinfeng Yi  Rong Jin  Ming Lin  and Xiaofei He. Online kernel learning with a near optimal
sparsity bound. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) 
pages 621–629  2013.

9

,Bo Waggoner
Rafael Frongillo
Jacob Abernethy
Lionel Gueguen
Alex Sergeev
Ben Kadlec
Rosanne Liu
Jason Yosinski