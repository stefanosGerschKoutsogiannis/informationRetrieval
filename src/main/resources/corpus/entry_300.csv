2018,A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization,We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance  they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results  we show that a class of non-differentiable nonconvex optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used $\ell_1$ norm to avoid outliers in nonconvex optimization.,A theory on the absence of spurious solutions for

nonconvex and nonsmooth optimization

C. Josz

EECS  UC Berkeley

cedric.josz@gmail.com

Y. Ouyang

IEOR  UC Berkeley

ouyangyii@gmail.com

R. Y. Zhang

IEOR  UC Berkeley
ryz@berkeley.edu

J. Lavaei

IEOR  UC Berkeley

lavaei@berkeley.edu

S. Sojoudi

EECS  UC Berkeley

sojoudi@berkeley.edu

Abstract

We study the set of continuous functions that admit no spurious local optima
(i.e. local minima that are not global minima) which we term global functions.
They satisfy various powerful properties for analyzing nonconvex and nonsmooth
optimization problems. For instance  they satisfy a theorem akin to the fundamental
uniform limit theorem in the analysis regarding continuous functions. Global
functions are also endowed with useful properties regarding the composition of
functions and change of variables. Using these new results  we show that a class of
nonconvex and nonsmooth optimization problems arising in tensor decomposition
applications are global functions. This is the ﬁrst result concerning nonconvex
methods for nonsmooth objective functions. Our result provides a theoretical
guarantee for the widely-used (cid:96)1 norm to avoid outliers in nonconvex optimization.

1

Introduction

A recent branch of research in optimization and machine learning consists in proving that simple
and practical algorithms can solve nonconvex optimization problems. Applications include  but are
not limited to  neural networks [40  44]  dictionary learning [1  2]  deep learning [39  50]  mixed
linear regression [49  43]  and phase retrieval [46  21]. In this paper  we focus our attention on
matrix completion/sensing [30  24  38] and tensor recovery/decomposition [5  4  31  35]. Matrix
completion/sensing aims to recover an unknown positive semideﬁnite matrix M of known size n
and rank r from a ﬁnite number of linear measurements modeled by the expression (cid:104)Ai  M(cid:105) :=
trace(AiM )  i = 1  . . .   m  where the symmetric matrices A1  . . .   Am of size n are known. It is
assumed that the measurements contain noise which can modeled as bi := (cid:104)Ai  M(cid:105) + i where i is
a realization of a random variable. When the noise is Gaussian  the maximum likelihood estimate of
M can be recast as the nonconvex optimization problem

((cid:104)Ai  M(cid:105) − bi)2

i=1

inf
M(cid:60)0

(1)
where M (cid:60) 0 stands for positive semideﬁnite. One can remove the rank constraint and obtain a
convex relaxation. It can then be solved via semideﬁnite programming after the reformulation of the
objective function in a linear way. However  the computational complexity of the resulting problem
is high  which makes it impractical for large-scale problems. A popular alternative is due to Burer
and Monteiro [18  12]:

rank(M ) = r

subject to

m(cid:88)

(cid:0)(cid:104)Ai  XX T(cid:105) − bi

(cid:1)2

m(cid:88)

i=1

inf

X∈Rn×r

(2)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

This nonlinear Least-Squares (LS) problem can be solved efﬁciently and on a large-scale with the
Gauss-Newton method for instance. It has received a lot of attention recently due to the discovery
in [30  10] stating that the problem admits no spurious local minima (i.e. local minima that are
not global minima) under certain conditions. These require adding a regularizer and satisfying the
restricted isometry property (RIP) [20]. We raise the question of whether this also holds in the case of
Laplacian noise  which is a better model to account for outliers in the data. The maximum likelihood
estimate of M can be converted to the Least-Absolute Value (LAV) optimization problem

(cid:12)(cid:12)(cid:104)Ai  XX T(cid:105) − bi

(cid:12)(cid:12) .

m(cid:88)

i=1

inf

X∈Rn×r

(3)

The nonlinear problem can be solved efﬁciently using nonconvex methods (for some recent work 
see [36]). For example  one may adopt the famous reformulation technique for converting (cid:96)1 norms
to linear functions subject to linear inequalities to cast the above problem as a smooth nonconvex
quadratically-constrained quadratic program [13]. However  the analysis of this result has not been
addressed in the literature - all ensuing papers (e.g. [29  52  8]) on matrix completion since the
aforementioned discovery deal with smooth objective functions.
Consider y ∈ Rn and assume r = 1. On the one hand  in the fully observable case1 with M = yyT  
the above nonconvex LS problem (2) consists in solving

(xixj − yiyj − i j)2

(4)

for which there are no spurious local minima with high probability when i j are i.i.d. Gaussian
variables [30]. On the other hand  in the full observable case  the LAV problem (3) aims to solve

n(cid:88)

i j=1

inf
x∈Rn

n(cid:88)

i j=1

inf
x∈Rn

|xixj − yiyj − i j|.

(5)

Although the LS problem has nice properties with Gaussian noise  we observe that stochastic gradient
descent (SGD) fails to recover the matrix M = yyT in the presence of large but sparse noise. In
contrast  SGD can perfectly recover the matrix by solving the LAV problem even when the sparse
noise i j has a large amplitude. Figures 1a and 1b show our experiments for n = 20 and n = 50 with
the number of noisy elements ranging from 0 to n2. See Appendix 5.1 for our experiment settings.

(a) n = 20

(b) n = 50

Figure 1: Experiments with sparse noise

Upon this LAV formulation hinges the potential of nonconvex methods to cope with sparse noise
and with Laplacian noise. There is no result on the analysis of the local solutions of this nons-
mooth problem in the literature even for the noiseless case. This could be due to the fact that the
optimality conditions for the smooth reformulated version of this problem in the form of quadratically-
constrained quadratic program are highly nonlinear and lead to an exponential number of scenarios.

1This corresponds to the case where the sensing matrices A1  . . .   An2 have all zeros terms apart from one

element which is equal to 1.

2

As such  the goal of this paper is to prove the following proposition  which as the reader will see  is a
signiﬁcant hurdle. It addresses the matrix noiseless case and more generally the case of a tensor of
order d ∈ N.
Proposition 1.1. The function f1 : Rn −→ R deﬁned as

n(cid:88)

f1(x) :=

|xi1 . . . xid − yi1 . . . yid|

(6)

has no spurious local minima.

i1 ... id=1

A direct consequence of Proposition 1.1 is that one can perform the rank-one tensor decomposition
by minimizing the function in Proposition 1.1 using a local search algorithm (e.g. [19]). Whenever
the algorithm reaches a local minimum  it is a globally optimal solution leading to the desired
decomposition. Existing proof techniques  e.g. [29  30  24  38  5  4  31  35]  are not directly useful
for the analysis of the nonconvex and nonsmooth optimization problem stated above. In particular 
results on the absence of spurious local minima neural networks with a Rectiﬁed Linear Unit (ReLU)
activation function pertain to smooth objective functions (e.g. [48  14]). The Clarke derivative [22  23]
provides valuable insight (see Lemma 3.1) but it is not conclusive. In order to pursue the proof
(see Lemma 3.2)  we propose the new notion of global function. Unlike the previous approaches  it
does not require one to exhibit a direction of descent. After some successive transformations  we
reduce the problem to a linear program. It is then obvious that there are no spurious local minima.
Incidentally  global functions provide a far simpler and shorter proof to a slightly weaker result  that
is to say  the absence of spurious strict local minima. It eschews the Clarke derivative all together
and instead considers a sequence of converging differentiable functions that have no spurious local
minima (see Proposition 3.1). In fact  this technique also applies if we substitute the (cid:96)1 norm with the
(cid:96)∞ norm (see Proposition 3.2).
The paper is organized as follows. Global functions are examined in Section 2 and their application
to tensor decomposition is discussed in Section 3. Section 4 concludes our work. The proofs may be
found in the supplementary material (Section 5 of the supplementary material).

(cid:115) n(cid:80)

i=1

2 Notion of global function

Given an integer n  consider the Euclidian space Rn with norm (cid:107)x(cid:107)2 :=
S ⊂ Rn. The next two deﬁnitions are classical.
Deﬁnition 2.1. We say that x ∈ S is a global minimum of f : S −→ R if for all y ∈ S \ {x}  it
holds that f (x) (cid:54) f (y).
Deﬁnition 2.2. We say that x ∈ S is a local minimum (respectively  strict local minimum) of
f : S −→ R if there exists  > 0 such that for all y ∈ S \ {x} satisfying (cid:107)x − y(cid:107)2 (cid:54)   it holds that
f (x) (cid:54) f (y) (respectively  f (x) < f (y)).

i along with a subset
x2

We introduce the notion of global functions below.
Deﬁnition 2.3. We say that f : S −→ R is a global function if it is continuous and its local minima
are all global minima. Deﬁne G(S) as the set of all global functions on S.
In the following  we compare global functions with other classes of functions in the literature 
particularly those that seek to generalize convex functions.
When the domain S is convex  two important proper subsets of G(S) are the sets of convex and strict
quasiconvex functions. Convex functions (respectively  strict quasiconvex [27  26]) are such that
f (λx + (1− λ)y) (cid:54) λf (x) + (1− λ)f (y) (respectively  f (λx + (1− λ)y) < max{f (x)  f (y)}) for
all x  y ∈ S (with x (cid:54)= y) and 0 < λ < 1. To see why these are proper subsets  notice that the cosine
function on [0  4π] is a global function that is neither convex nor strict quasiconvex. In dimension
one  global and strict quasiconvex functions are very closely related. Indeed  when the domain is
convex and compact (i.e. an interval [a  b] where a  b ∈ R)  it can be shown that a function is strict
quasiconvex if and only if it is global and has a unique global minimum. However  this is not true in
higher dimensions  as can be seen in Figure 4b in Appendix 5.2  or in the existing literature  i.e. in

3

[25] or in [9  Figure 1.1.10]. It is also not true in dimension one if we remove the assumption that the
domain is compact (consider f (x) := (x2 + x4)/(1 + x4) deﬁned on R and illustrated in Figure 4a
in Appendix 5.2).
When the domain S is not necessarily convex  a proper subset of G(S) is the set of star-convex
functions. For a star-convex function f  there exists x ∈ S such that f (λx+(1−λ)y) (cid:54) λf (x)+(1−
λ)f (y) for all y ∈ S \{x} and 0 < λ < 1. Again  the cosinus function on [0  4π] is a global function
that is not star-convex. Another interesting proper subset of G(S) is the set of functions for which 
informally  given any point  there exists a strictly decreasing path from that point to a global minimum.
This property is discussed in [47  P.1] (see also [28]) to study the landscape of loss functions of
neural networks. Formally  the property is that for all x ∈ S such that f (x) > inf y∈S f (y)  there
exists a continuous function g : [0  1] −→ S such that g(0) = x  g(1) ∈ argmin{f (y) | y ∈ S}  and
t ∈ [0  1] (cid:55)−→ f (g(t)) is strictly decreasing (i.e. f (g(t1)) > f (g(t2)) if 0 (cid:54) t1 < t2 (cid:54) 1). Not all
global functions satisfy this property  as illustrated by the function in Figure 4a. For instance  there
exists no strictly decreasing path from x = −3 to the global minimizer 0. However  in the funtion in
Figure 4b in Appendix 5.2  there exists a strictly decreasing path from any point to the unique global
minimizer. One could thus think that if S is compact  or if f is coercive  then one should always
be able to ﬁnd a strictly decreasing path. However  there need not exist a strictly decreasing path in
general. Consider for example the function deﬁned on ([−1  1] \ {0}) × [−1  1] as follows



4|x1|3(cid:16)

f (x1  x2) :=

+ 1

(cid:17)

(cid:17)
(cid:16)− 1|x1|
(cid:16)
(cid:17) − 2
(cid:17)
(cid:111)
(cid:16)− 1|x1|
(cid:17) − 3
(cid:17)
(cid:111)
(cid:16)− 1|x1|
(cid:17)
(cid:16)− 1|x1|
x2 − 4|x1|3(cid:16)

x2
2 +

x3
2 +

+ 1

+ 1

sin

−4|x1|3(1 − x2)

sin

(cid:110)
12|x1|3(cid:16)
(cid:110)
20|x1|3(cid:16)
(cid:17)
(cid:16)− 1|x1|

sin

sin

sin

+ 1

if

0 (cid:54) x2 (cid:54) 1 

if − 1 (cid:54) x2 < 0.

(cid:17)

(cid:17)

+ 1

10

1)(x2 − 4x2

The function and its differential can readily be extended continuously to [−1  1] × [−1  1]. This
is illustrated in Figure 6a in Appendix 5.2. This yields a smooth2 global function for which there
exists no strictly decreasing path from the point x = (0  1/2) to a global minimizer (i.e. any point
in [−1  1] × {−1}). We ﬁnd this to be rather counter-intuitive. To the best of our knowledge  no
such function has been presented in past literature. Hestenes [32] considered the function deﬁned on
[−1  1] × [−1  1] by f (x1  x2) := (x2 − x2
1) (see also [9  Figure 1.1.18]). It is a global
function for which the point x = (0  0) (which is not a global minimizer) admits no direction of
descent  i.e. d ∈ R2 such that t ∈ [0  1] (cid:55)−→ f (x + td) is strictly decreasing. However  it does
√
admit a strictly decreasing path to a global minimizer  i.e. t ∈ [0  1] (cid:55)−→ (
4 t  t2)  along which
the objective equals − 9
16 t4. This is unlike the function exhibited in Figure 6a. As a byproduct  our
function shows that the generalization of quasiconvexity to non-convex domains described in [6 
Chapter 9] is a proper subset of global functions. This generalization was proposed in [41] and further
investigated in [7  33  34  15  16  17]. It consists in replacing the segment used to deﬁne convexity
and quasiconvexity by a continuous path.
Finally  we note that there exists a characterization of functions whose local minima are global 
without requiring continuity as in global functions. It is based on a certain notion of continuity
of sublevel sets  namely lower-semicontinuity of point-to-set mappings [51  Theorem 3.3]. We
will see below that continuity is a key ingredient for obtaining our results. We do not require
more regularity precisely because one of our goals is to study nonsmooth functions. Speaking of
which  observe that global functions can be nowhere differentiable  contrary to convex functions [11 
Theorems 2.1.2 and 2.5.1]. Consider for example the global function deﬁned on ]0  1[ × ]0  1[ by
n=0 s(2nx1)/2n where s(x) := minn∈N |x − n| is the distance to nearest
integer. For any ﬁxed x2 (cid:54)= 0  the function x1 ∈ [0  1] (cid:55)−→ f (x1  x2)/|x2| is the Takagi curve
[45  3  37] which is nowhere differentiable. It can easily be deduced that the bivariate function is
nowhere differentiable. This is illustrated in Figure 6b.
In the following  we review some of the properties of global functions. Their proofs can be found in
the appendix. We begin by investigating the composition operation.

f (x1  x2) := |2x2 − 1|(cid:80)+∞

2In fact  one could make it inﬁnitely differentiable by using the exponential function in the construction  but

it is more cumbersome.

4

Proposition 2.1 (Composition of functions). Consider f : S −→ R. Let φ : f (S) −→ R denote
a strictly increasing function where f (S) is the range of f. It holds that f ∈ G(S) if and only if
φ ◦ f ∈ G(S).
However  the set of global functions is not closed under composition of functions in general. For
instance  f (x) := |x| and g(x) := max(−1 |x|− 2) are global functions on R  but f ◦ g is not global
function on R.
Proposition 2.2 (Change of variables). Consider f : S −→ R  a subset S(cid:48) ⊂ Rn  and a homeomor-
phism ϕ : S −→ S(cid:48) (i.e. continuous bijection with continuous inverse). It holds that f ∈ G(S) if and
only if f ◦ ϕ−1 ∈ G(S(cid:48)).
Next  we consider what happens if we have a sequence of global functions. Figure 2a shows that the
sequence of global functions (red dotted curves) pointwise converges to a function with a spurious
local minimum (blue curve). Figure 2b shows that uniform convergence also does not preserve the
property of being a global function: all points on the middle part of the limit function (blue curve) are
spurious local minima. However  it suggests that uniform convergence preserves a slightly weaker
property than being a global function. Intuitively  the limit should behave like a global function
except that it may have “ﬂat” parts. We next formalize this intuition. To do so  we consider the
notions of global minimum  local minimum  and strict local minimum (Deﬁnition 2.1 and Deﬁnition
2.2)  which apply to points in Rn  and generalize them to subsets of Rn. We will borrow the notion
of neighborhood of a set (uniform neighborhood to be precise  see Deﬁnition 2.5).

(a) Pointwise convergence

(b) Uniform convergence

Figure 2: Convergence of a sequence of global functions

Deﬁnition 2.4. We say that a subset X ⊂ S is a global minimum of f : S −→ R if inf X f (cid:54)
inf S\X f.

We note in passing the following two propositions. We will use them repeatedly in the next section.
The proofs are omitted as they follow directly from the deﬁnitions.
Proposition 2.3. Assume that the following statements are true:

1. X ⊂ S is a global minimum of f;
2. f ∈ G(X);
3. f does not have any local minima on S \ X.

Then  f ∈ G(S).
Note that the ﬁrst assumption is needed; otherwise the function may not be global because it could
take a smaller value at a non local min outside X (possible when S is unbounded).
Proposition 2.4. If f : S −→ R is a global function on global minima (Xα)α∈A for some index set

A  then it is a global function on(cid:83)

α∈A Xα.

We proceed to generalize the deﬁnition of local minimum.

5

Deﬁnition 2.5. We say that a compact subset X ⊂ S is local minimum (respectively  strict local
minimum) of f : S −→ R if there exists  > 0 such that for all x ∈ X and for all y ∈ S \ X
satisfying (cid:107)x − y(cid:107)2 (cid:54)   it holds that f (x) (cid:54) f (y) (respectively  f (x) < f (y)).3
The above deﬁnitions are distinct from the notion of valley proposed in [47  Deﬁnition 1]. The latter
is deﬁned as a connected component4 of a sublevel set (i.e. {x ∈ S | f (x) (cid:54) α} for some α ∈ R).
Local minima and strict local minima need not be valleys  and vice-versa. One may easily check
that when the set is a point  i.e. X = {x} with x ∈ S  the two deﬁnitions above are the same as the
previous deﬁnitions of minimum (Deﬁnition 2.1 and Deﬁnition 2.2). They are therefore consistent. It
turns out that the notion of global function (Deﬁnition 2.3) does not change when we interpret it in
the sense of sets. We next verify this claim.
Proposition 2.5 (Consistency of Deﬁnition 2.3). Let f : S −→ R denote a continuous function. All
local minima are global minima in the sense of points if only if all local minima are global minima in
the sense of sets.

We are ready to deﬁne a slightly weaker notion than being a global function.
Deﬁnition 2.6. We say that f : S −→ R is a weakly global function if it is continuous and if all strict
local minima are global minima in the sense of sets.

The generalization from points to sets in the deﬁnition of a minimum is justiﬁed here  as can be seen
in Figure 3. All strict local minima are global minima in the sense of points. However  X = [a  b]
with a ≈ −2.6 and b = −1 is a strict local minimum that is not a global minimum. Indeed 
inf X f = 6 > 1 = infR\X f. Hence  the function is not weakly global.

Figure 3: All strict local minima are global minima in the sense of points but not in the sense of sets.

We next make the link with the intuition regarding the ﬂat part in Figure 2b.
Proposition 2.6. If f : S −→ R is a weakly global function  then it is constant on all local minima
that are not global minima.
We are interested in functions that are potentially deﬁned on all of Rn (i.e. unconstrained optimization)
or on subsets S ⊂ Rn that are not necessarily compact (i.e. general constrained optimization). We
therefore need to borrow a slightly more general notion than uniform convergence [42  page 95 
Section 3].
Deﬁnition 2.7. We say that a sequence of continuous functions fk : S −→ R  k = 1  2  . . .  
converges compactly towards f : S −→ R if for all compact subsets K ⊂ S  the restrictions of fk to
K converge uniformly towards the restriction of f to K.

We are now ready to state a result regarding the convergence of a sequence of global functions and an
important property that is preserved in the process.

3Note that the neighborhood of a compact set is always uniform.
4A subset C ⊂ S is connected if it is not equal to the union of two disjoint nonempty closed subsets of S. A

maximal connected subset (ordered by inclusion) of S is called a connected component.

6

Proposition 2.7 (Compact convergence). Consider a sequence of functions (fk)k∈N and a function
f  all from S ⊂ Rn to R. If

fk −→ f compactly

and if fk are global functions on S  then f is a weakly global function on S.

Note that the proofs in this section are not valid if we replace the Euclidian space by an inﬁnite-
dimensional metric space. Indeed  we have implicitely used the fact that the unit ball is compact in
order for the uniform neighborhood of a minimum to be compact.

3 Application to tensor decomposition

Global functions can be used to prove the following two signiﬁcant results on nonlinear functions
involving (cid:96)1 norm and (cid:96)∞ norm  as explained below.
Proposition 3.1. The function f1 : Rn −→ R deﬁned as

(7)

(8)

(9)

n(cid:88)

f1(x) :=

|xi1 . . . xid − yi1 . . . yid|

i1 ... id=1

is a weakly global function; in particular  it has no spurious strict local minima.

Proof. The functions

n(cid:88)

fp(x) :=

|xi1 . . . xid − yi1 . . . yid|p

i1 ... id=1

for p −→ 1 with p > 1 form a set of global functions that converge compactly towards the function
f1. This is illustrated in Figure 5 in Appendix 5.2 for n = d = 2 and y = (1 −3/4). The desired
result then follows from Proposition 2.7. To see why each fp is a global function  observe that fp is
differentiable with the ﬁrst-order optimality condition as follows:

xi1 . . . xid−1(xi1 . . . xid−1 xi − yi1 . . . yid−1 yi)|xi1 . . . xid−1 xi − yi1 . . . yid−1 yi|p−2 = 0

n(cid:88)

i1 ... id−1=1

for all i ∈ {1  . . .   n}. Note that each term in the sum converges towards zero if the expression inside
the absolute value converges towards zero  so that the equation is well-deﬁned. Consider a local
minimum x ∈ Rn; then  x must satisfy the above ﬁrst-order optimality condition. If yi = 0  then the
above equation readily yields xi = 0. This reduces the problem dimension from n variables to n − 1
variables  so without loss of generality we may assume that yi (cid:54)= 0  i = 1  . . .   m. After a division 
observe that the following equation is satisﬁed

(cid:18) xi1 . . . xid−1

yi1 . . . yid−1

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) xi1 . . . xid−1

yi1 . . . yid−1

t − 1

(cid:12)(cid:12)(cid:12)(cid:12)p−2

t − 1

= 0

n(cid:88)

|yi1 . . . yid−1|p xi1 . . . xid−1
yi1 . . . yid−1

i1 ... id−1=1

for all t ∈ {x1/y1  . . .   xn/yn}. Each term with xi1 . . . xid−1 (cid:54)= 0 in the above sum is a strictly
increasing function of t ∈ R since it is the derivative of the strictly convex function

g(t) = |xi1 . . . xid−1 t − yi1 . . . yid−1|p.

(10)
The point x = 0 is not a local minimum (y is a direction of descent of fp at 0)  and thus x (cid:54)= 0. As a
result  the above sum is a strictly increasing function of t ∈ R. Hence  it has at most one root  that is
to say t = x1/y1 = ··· = xn/yn. Plugging in  we ﬁnd that td = 1. If d is odd  then x = y and if d
is even  then x = ±y. To conclude  any local minimum x is a global minimum of fp.
Proposition 3.2. f∞ : Rn −→ R deﬁned as
max

|xi1 . . . xid − yi1 . . . yid|

f∞(x) :=

(11)

1(cid:54)i1 ... id(cid:54)n

is a weakly global function; in particular  it has no spurious strict local minima.

7

(cid:32) n(cid:80)

(cid:33) 1

p

Proof. The functions hp(x) :=

for p −→ +∞ form a set
of global functions that converge compactly towards the function f∞. We know that each hp is a
global function by applying Proposition 2.1 to (9) with the fact that (·)
p is increasing for nonnegative
arguments.

|xi1 . . . xid − yi1 . . . yid|p

i1 ... id=1

1

Note that the functions in Proposition 3.1 and Proposition 3.2 are a priori utterly different  yet both
proofs are essentially the same. This highlights the usefulness of the new notion of global functions.
Remark 3.1. The notion of weakly global functions explains that one can perform tensor decomposi-
tion by minimizing the nonconvex and nonsmooth functions in Proposition 3.1 and Proposition 3.2
with a local search algorithm. Whenever the algorithm reports a strict local minimum  it is a globally
optimal solution.

In order to strengthen the conclusion in Proposition 3.1 and to establish the absence of spurious local
minima  we propose the following two lemmas. Using Proposition 2.3 and these two lemmas  we
arrive at the stronger result stated in Proposition 1.1.
Lemma 3.1. If x ∈ Rn is a ﬁrst-order stationary point of f1 in the sense of the Clarke derivative 
then the following statements hold:

1. If yi = 0 for some i ∈ {1  . . .   n}  then xi = 0;
2. For all i1  . . .   id ∈ {1  . . .   n}  it holds that xi1 ...xid
yi1 ...yid

(cid:54) 1.

Proof. Similar in spirit to the proof of Proposition 3.1  the ratios t ∈ {x1/y1  . . .   xn/yn} for a
ﬁrst-order stationary point must all be the roots of an increasing (set-valued) “staircase function". We
then obtain the results by analyzing the relation between the roots and the jump points of the staircase
function. See Appendix 5.8 for the complete proof.

Note that the above lemma only uses the ﬁrst-order optimality condition (in the sense of Clarke
derivative) without any direction of decent.
Remark 3.2. One cannot show that there are no spurious local minima with only the ﬁrst-order
optimality condition (in the Clarke derivative sense). In fact  any x ∈ Rn satisfying
= 0
(cid:54) 1 for all i1  . . .   id ∈ {1  . . .   n}  is a ﬁrst-order stationary point  but is not a local
and xi1 ...xid
yi1 ...yid
minimum.
Lemma 3.2. If y1 . . . yn (cid:54)= 0  deﬁne the set

|yi| xi

n(cid:80)

i=1

yi

(cid:27)

(cid:54) 1  

∀ i1  . . .   id ∈ {1  . . .   n}

.

(12)

(cid:26)

S :=

x ∈ Rn

(cid:12)(cid:12)(cid:12)(cid:12) xi1 . . . xid

yi1 . . . yid

Then  f1 ∈ G(S).

|yi|

(cid:19)d

(cid:18) n(cid:80)

Proof. We provide a sketch here  and the complete proof is deferred to Appendix 5.9. The

(cid:19)d −
(cid:18) n(cid:80)
that f1 is a global function on S if and only if fodd(x) = −(cid:80)n
is an even number  f is a global function if and only if feven(x) = − ((cid:80)n
we divide the set S(cid:48) into two subsets: S(cid:48) ∩ {x|(cid:80)n

objective function on S is equal to f1(x) =
. Deﬁne the set
(cid:54) 1   ∀ i1  . . .   id ∈ {1  . . .   n} }. When d is an odd number  the
S(cid:48) := { x ∈ Rn | xi1 . . . xid
composition and change of variables properties of global functions (Propositions 2.1 and 2.2) imply
i=1 |yi|xi ∈ G(S(cid:48)). Similarly  when d
i=1 |yi|xi)2 ∈ G(S(cid:48)). For the
case when d is odd  we apply the Karush-Kuhn-Tucker conditions to restrict attention to the positive
orthant and conclude by showing its association with a linear program. For the case when d is even 
i=1 |yi|xi ≤ 0}.
Observe that feven(x) is a global function on each of the subset by associating each subset with a
linear program. Then  Proposition 2.3 establishes the result.

i=1 |yi|xi ≥ 0} and S(cid:48) ∩ {x|(cid:80)n

|yi| xi

i=1

i=1

yi

The two previous lemmas prove Proposition 1.1; the notion of global function is used to the prove the
latter.

8

4 Conclusion

Nonconvex optimization appears in many applications  such as matrix completion/sensing  tensor
recovery/decomposition  and training of neural networks. For a general nonconvex function  a
local search algorithm may become stuck at a local minimum that is arbitrarily worse than a global
minimum. We develop a new notion of global functions for which all local minima are global
minima. Using certain properties of global functions  we show that the set of these functions include
a class of nonconvex and nonsmooth functions that arise in matrix completion/sensing and tensor
recovery/decomposition with Laplacian noise. This paper offers a new mathematical technique for
the analysis of nonconvex and nonsmooth functions such as those involving (cid:96)1 norm and (cid:96)∞ norm.

Acknowledgments

This work was supported by the ONR Awards N00014-17-1-2933 ONR and N00014-18-1-2526 
NSF Award 1808859  DARPA Award D16AP00002  and AFOSR Award FA9550- 17-1-0163. We
wish to thank the anonymous reviewers for their valuable feedback  as well as Chris Dock for fruitful
discussions.

9

,Aaron van den Oord
Sander Dieleman
Benjamin Schrauwen
Qiang Liu
Dilin Wang
Cedric Josz
Yi Ouyang
Richard Zhang
Javad Lavaei
Somayeh Sojoudi