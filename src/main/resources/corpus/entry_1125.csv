2009,Construction of Nonparametric Bayesian Models from Parametric Bayes Equations,We consider the general problem of constructing nonparametric   Bayesian models on infinite-dimensional random objects  such   as functions  infinite graphs or infinite permutations.   The problem has generated much interest in machine learning    where it is treated heuristically  but has not been    studied in full generality in nonparametric Bayesian statistics  which tends to   focus on models over probability distributions.    Our approach applies a standard tool of stochastic process   theory  the construction of stochastic processes from their   finite-dimensional marginal distributions.    The main contribution of the paper is a generalization   of the classic Kolmogorov extension theorem to conditional   probabilities.   This extension allows a rigorous construction of nonparametric Bayesian models   from systems of finite-dimensional  parametric Bayes equations.   Using this approach  we show (i)   how existence of a conjugate posterior for    the nonparametric model can be guaranteed by choosing   conjugate finite-dimensional models in the construction  (ii) how the   mapping to the posterior parameters of the nonparametric   model can be explicitly determined  and (iii) that   the construction of conjugate models in essence requires the   finite-dimensional models to be in the exponential family.   As an application of our constructive framework     we derive a model on infinite   permutations  the nonparametric Bayesian analogue of a model   recently proposed for the analysis of rank data.,Construction of Nonparametric Bayesian Models

from Parametric Bayes Equations

Peter Orbanz

University of Cambridge and ETH Zurich

p.orbanz@eng.cam.ac.uk

Abstract

We consider the general problem of constructing nonparametric Bayesian models
on inﬁnite-dimensional random objects  such as functions  inﬁnite graphs or inﬁ-
nite permutations. The problem has generated much interest in machine learning 
where it is treated heuristically  but has not been studied in full generality in non-
parametric Bayesian statistics  which tends to focus on models over probability
distributions. Our approach applies a standard tool of stochastic process theory 
the construction of stochastic processes from their ﬁnite-dimensional marginal
distributions. The main contribution of the paper is a generalization of the classic
Kolmogorov extension theorem to conditional probabilities. This extension allows
a rigorous construction of nonparametric Bayesian models from systems of ﬁnite-
dimensional  parametric Bayes equations. Using this approach  we show (i) how
existence of a conjugate posterior for the nonparametric model can be guaranteed
by choosing conjugate ﬁnite-dimensional models in the construction  (ii) how the
mapping to the posterior parameters of the nonparametric model can be explicitly
determined  and (iii) that the construction of conjugate models in essence requires
the ﬁnite-dimensional models to be in the exponential family. As an application
of our constructive framework  we derive a model on inﬁnite permutations  the
nonparametric Bayesian analogue of a model recently proposed for the analysis
of rank data.

1

Introduction

Nonparametric Bayesian models are now widely used in machine learning. Common models  in
particular the Gaussian process (GP) and the Dirichlet process (DP)  were originally imported from
statistics  but the nonparametric Bayesian idea has since been adapted to the needs of machine
learning. As a result  the scope of Bayesian nonparametrics has expanded signiﬁcantly: Whereas
traditional nonparametric Bayesian statistics mostly focuses on models on probability distributions 
machine learning researchers are interested in a variety of inﬁnite-dimensional objects  such as func-
tions  kernels  or inﬁnite graphs. Initially  existing DP and GP approaches were modiﬁed and com-
bined to derive new models  including the Inﬁnite Hidden Markov Model [2] or the Hierarchical
Dirichlet Process [15]. More recently  novel stochastic process models have been deﬁned from
scratch  such as the Indian Buffet Process (IBP) [8] and the Mondrian Process [13]. This paper
studies the construction of new nonparametric Bayesian models from ﬁnite-dimensional distribu-
tions: To construct a model on a given type of inﬁnite-dimensional object (for example  an inﬁnite
graph)  we start out from available probability models on the ﬁnite-dimensional counterparts (prob-
ability models on ﬁnite graphs)  and translate them into a model on inﬁnite-dimensional objects
using methods of stochastic process theory. We then ask whether interesting statistical properties of
the ﬁnite-dimensional models used in the constructions  such as conjugacy of priors and posteriors 
carry over to the stochastic process model.

1

In general  the term nonparametric Bayesian model refers to a Bayesian model on an inﬁnite-
dimensional parameter space. Unlike parametric models  for which the number of parameters is
constantly bounded w.r.t. sample size  nonparametric models allow the number of parameters to
grow with the number of observations. To accommodate a variable and asymptotically unbounded
number of parameters within a single parameter space  the dimension of the space has to be inﬁnite 
and nonparametric models can be deﬁned as statistical models with inﬁnite-dimensional parameter
spaces [17]. For a given sample of ﬁnite size  the model will typically select a ﬁnite subset of the
available parameters to explain the observations. A Bayesian nonparametric model places a prior
distribution on the inﬁnite-dimensional parameter space.

Many nonparametric Bayesian models are deﬁned in terms of their ﬁnite-dimensional marginals:
For example  the Gaussian process and Dirichlet process are characterized by the fact that their
ﬁnite-dimensional marginals are  respectively  Gaussian and Dirichlet distributions [11  5]. The
probability-theoretic construction result underlying such deﬁnitions is the Kolmogorov extension
theorem [1]  described in Sec. 2 below. In stochastic process theory  the theorem is used to study
the properties of a process in terms of its marginals  and hence by studying the properties of ﬁnite-
dimensional distributions. Can the statistical properties of a nonparametric Bayesian model  i.e. of
a parameterized family of distributions  be treated in a similar manner  by considering the model’s
marginals? For example  can a nonparametric Bayesian model be guaranteed to be conjugate if
the marginals used in its construction are conjugate? Techniques such as the Kolmogorov theo-
rem construct individual distributions  whereas statistical properties are properties of parameterized
families of distributions. In Bayesian estimation  such families take the form of conditional prob-
abilities. The treatment of the statistical properties of nonparametric Bayesian models in terms of
ﬁnite-dimensional Bayes equations therefore requires an extension result similar to the Kolmogorov
theorem that is applicable to conditional distributions. The main contribution of this paper is to
provide such a result.

We present an analogue of the Kolmogorov theorem for conditional probabilities  which permits the
direct construction of conditional stochastic process models on countable-dimensional spaces from
ﬁnite-dimensional conditional probabilities. Application to conjugate models shows how a conju-
gate nonparametric Bayesian model can be constructed from conjugate ﬁnite-dimensional Bayes
equations – including the mapping to the posterior parameters. The converse is also true: To con-
struct a conjugate nonparametric Bayesian model  the ﬁnite-dimensional models used in the con-
struction all have to be conjugate. The construction of stochastic process models from exponential
family marginals is almost generic: The model is completely described by the mapping to the poste-
rior parameters  which has a generic form as a function of the inﬁnite-dimensional counterpart of the
model’s sufﬁcient statistic. We discuss how existing models ﬁt into the framework  and derive the
nonparametric Bayesian version of a model on inﬁnite permutations suggested by [9]. By essentially
providing a construction recipe for conjugate models of countable dimension  our theoretical results
have clear practical implications for the derivation of novel nonparametric Bayesian models.

2 Formal Setup and Notation

Inﬁnite-dimensional probability models cannot generally be described with densities and therefore
require some basic notions of measure-theoretic probability. In this paper  required concepts will
be measures on product spaces and abstract conditional probabilities (see e.g. [3] or [1] for general
introductions). Randomness is described by means of an abstract probability space (Ω A  P). Here 
Ω is a space of points ω  which represent atomic random events  A is a σ-algebra of events on Ω 
and P a probability measure deﬁned on the σ-algebra. A random variable is a measurable mapping
from Ω into some space of observed values  such as X : Ω → Ωx. The distribution of X is the
image measure PX := X(P) = P ◦ X−1. Roughly speaking  the events ω ∈ Ω represent abstract
states of nature  i.e. knowing the value of ω completely describes all probabilistic aspects of the
model universe  and all random aspects are described by the probability measure P. However  Ω  A
and P are never known explicitly  but rather constitute the modeling assumption that any explicitly
known distribution PX is derived from one and the same probability measure P through some random
variable X.

Multiple dimensions of random variables are formalized by product spaces. We will generally deal
with an inﬁnite-dimensional space such as ΩE
x is the E-

x  were E is an inﬁnite index set and ΩE

2

X] := P J

X](AI) = P J

X(π-1

x. Each product space ΩI

X] is the marginal of the measure P J

structure  such as X I =(cid:78)

x of different dimensions are linked by a projection operator πJI  which
x to xI  the subset of entries of xJ that are indexed by I ⊂ J. For a set
JI AI under projection is called a cylinder set with base AI. The projection
JI   so for an I-dimensional event AI ∈ BI
X ◦ π-1
x 
JI AI). In other words  a probability is assigned to the I-dimensional
X to the cylinder with base AI. The projection
X on the lower-

fold product of Ωx with itself. The set of ﬁnite subsets of E will be denoted F(E)  such that
x with I ∈ F(E) is a ﬁnite-dimensional subspace of ΩE
ΩI
x is equipped
with the product Borel σ-algebra BI
x. Random variables with values on these spaces have product
i∈I X{i}. Note that this does not imply that the corresponding measure
X := X I(P) is a product measure; the individual components of X I may be dependent. The
P I
elements of the inﬁnite-dimensional product space ΩE
x can be thought of as functions of the form
E → Ωx. For example  the space RR contains all real-valued functions on the line.
x ⊂ ΩJ
Product spaces ΩI
restricts a vector xJ ∈ ΩJ
AI ⊂ ΩI
x  the preimage π-1
operator can be applied to measures as [πJIP J
we have [πJIP J
set AI by applying the J-dimensional measure P J
of a measure is just its marginal  that is  [πJIP J
dimensional subspace ΩI
x.
We denote observation variables (data) by X I  parameters by ΘI and hyperparameters by ΨI. The
corresponding measures and spaces are indexed accordingly  as PX  PΘ  Ωθ etc. The likelihoods and
posteriors that occur in Bayesian estimation are conditional probability distributions. Since densities
are not generally applicable in inﬁnite-dimensional spaces  the formulation of Bayesian models on
such spaces draws on the abstract conditional probabilities of measure-theoretic probability  which
are derived from Kolmogorov’s implicit formulation of conditional expectations [3]. We will write
e.g. PX(X|Θ) for the conditional probability of X given Θ. For the reader familiar with the theory 
we note that all spaces considered here are Borel spaces  such that regular versions of conditionals
always exist  and we hence assume all conditionals to be regular conditional probabilities (Markov
kernels). Introducing abstract conditional probabilities here is far beyond the possible scope of this
paper. A reader not familiar with the theory should simply read PX(X|Θ) as a conditional distribu-
tion  but take into account that these abstract objects are only uniquely deﬁned almost everywhere.
That is  the probability PX(X|Θ = θ) can be changed arbitrarily for those values of θ within some
set of exceptions  provided that this set has measure zero. While not essential for understanding
most of our results  this fact is the principal reason that limits the results to countable dimensions.
X (X E|ΘE) is to represent a Gaussian process with ﬁxed covariance
Example: GP. Assume that P E
function. Then X E is function-valued  and if for example E := R+ and Ωx := R  the product space
x = RR+ contains all functions xE of the form xE : R+ → R. Each axis label i ∈ E in the product
ΩE
space is a point on the real line  and a ﬁnite index set I ∈ F(E) is a ﬁnite collection of points I =
x is then the vector xI := (xE(i1)  . . .   xE(im))
(i1  . . .   im). The projection πEIxE of a function in ΩE
of function values at the points in I. The parameter variable ΘE represents the mean function of the
x = RR+.
process  and so we would choose ΩE
X (X E|ΘE) is a Dirichlet process  the variable X E takes values xE in the set of
Example: DP. If P E
probability measures over a given domain  such as R. A probability measure on R (with its Borel
algebra B(R)) is in particular a set function B(R) → [0  1]  so we could choose E = B(R) and
Ωx = [0  1]. The parameters of a Dirichlet process DP(α  G0) are a scalar concentration parameter
α ∈ R+  and a probability measure G0 with the same domain as the randomly drawn measure xE.
The parameter space would therefore be chosen as R+ × [0  1]B(R).

θ := ΩE

2.1 Construction of Stochastic Processes from their Marginals

Suppose that a family P I
dimensional measure P E
subspace ΩI
x of ΩE
each other as well:

X of probability measures are the ﬁnite-dimensional marginals of an inﬁnite-
X (a “stochastic process”). Each measure P I
X lives on the ﬁnite-dimensional
x. As marginals of one and the same measure  the measures must be marginals of

X ◦ π-1

JI

X = P J
P I

(1)
Any family of probability measures satisfying (1) is called a projective family. The marginals of
a stochastic process measure are always projective. A famous theorem by Kolmogorov states that
the converse is also true: Any projective family on the ﬁnite-dimensional subspaces of an inﬁnite-
dimensional product space ΩE
x [1]. The only
assumption required is that the “axes” Ωx of the product space are so-called Polish spaces  i.e.

x uniquely deﬁnes a stochastic process on the space ΩE

whenever I ⊂ J .

3

x BI

X on ΩE

X as its marginals.

X|I ∈ F(E)} be a family of probability measures on the spaces (ΩI

topological spaces that are complete  separable and metrizable. Examples include Euclidean spaces 
separable Banach or Hilbert spaces  countable discrete spaces  and countable products of spaces that
are themselves Polish.
Theorem 1 (Kolmogorov Extension Theorem). Let E be an arbitrary inﬁnite set. Let Ωx be a Polish
space  and let {P I
x). If the
family is projective  there exists a uniquely deﬁned probability measure P E
x with the measures
P I
The inﬁnite-dimensional measure P E
X constructed in Theorem 1 is called the projective limit of the
family P I
X. Intuitively  the theorem is a regularity result: The marginals determine the values of
P E
X on a subset of events (namely on those events involving only a ﬁnite subset of the random
variables  which are just the cylinder sets with ﬁnite-dimensional base). The theorem then states that
a probability measure is such a regular object that knowledge of these values determines the measure
completely  in a similar manner as continuous functions on the line are completely determined by
their values on a countable dense subset. The statement of the Kolmogorov theorem is deceptive in
its generality: It holds for any index set E  but if E is not countable  the constructed measure P E
X is
essentially useless – even though the theorem still holds  and the measure is still uniquely deﬁned.
The problem is that the measure P E
x  but on the
σ-algebra BE
x). If E is uncountable  this σ-algebra is too coarse to
resolve events of interest1. In particular  it does not contain the singletons (one-point sets)  such that
the measure P E

x (the product σ-algebra on ΩE
X is incapable of assigning a probability to an event of the form {X E = xE}.

X   as a set function  is not deﬁned on the space ΩE

3 Extension of Conditional and Bayesian Models

According to the Kolmogorov extension theorem  the properties of a stochastic process can be an-
alyzed by studying its marginals. Can we  analogously  use a set of ﬁnite-dimensional Bayes equa-
tions to represent a nonparametric Bayesian model? The components of a Bayesian model are condi-
tional distributions. Even though these conditionals are probability measures for (almost) each value
of the condition variable  the Kolmogorov theorem cannot simply be applied to extend conditional
models: Conditional probabilities are functions of two arguments  and have to satisfy a measurabil-
ity requirement in the second argument (the condition). Application of the extension theorem to each
value of the condition need not yield a proper conditional distribution on the inﬁnite-dimensional
space  as it disregards the properties of the second argument. But since the second argument takes
the role of a parameter in statistical estimation  these properties determine the statistical properties of
the model  such as sufﬁciency  identiﬁability  or conjugacy. In order to analyze the properties of an
inﬁnite-dimensional Bayesian model in terms of ﬁnite-dimensional marginals  we need a theorem
that establishes a correspondence between the ﬁnite-dimensional and inﬁnite-dimensional condi-
tional distributions. Though a number of extension theorems based on conditional distributions is
available in the literature  these results focus on the construction of sequential stochastic processes
from a sequence of conditionals (see [10] for an overview). Theorem 2 below provides a result that 
like the Kolmogorov theorem  is applicable on product spaces.

To formulate the result  the projector used to deﬁne the marginals has to be generalized from mea-
X(X J|ΘJ) is a conditional
sures to conditionals. The natural way to do so is the following: If P J
probability on the product space ΩJ  and I ⊂ J  deﬁne
X]( .|ΘJ) := P J

JI .|ΘJ) .

[πJIP J

X(π-1

(2)

This deﬁnition is consistent with that of the projector above  in the sense that it coincides with the
X( .|ΘJ = θJ) for any ﬁxed value θJ of the parameter. As
standard projector applied to the measure P J
with projective families of measures  we then deﬁne projective families of conditional probabilities.
X(X I|ΘI) be a family of regu-
Deﬁnition 1 (Conditionally Projective Probability Models). Let P I
x  for all I ∈ F(E). The family will be called
lar conditional probabilities on product spaces ΩI
conditionally projective if [πJIP J
As conditional probabilities are unique almost everywhere  the equality is only required to hold al-
most everywhere as well. In the jargon of abstract conditional probabilities  the deﬁnition requires

X( .|ΘI) whenever I ⊂ J.

X]( .|ΘJ) =a.e. P I

1This problem is unfortunately often neglected in the statistics literature  and measures in uncountable
dimensions are “constructed” by means of the extension theorem (such as in the original paper [5] on the
Dirichlet process). See e.g. [1] for theoretical background  and [7] for a rigorous construction of the DP.

4

X( .|ΘI) is a version of the projection of P J

X( .|ΘJ). Theorem 2 states that a conditional prob-
that P I
ability on a countably-dimensional product space is uniquely deﬁned (up to a.e.-equivalence) by a
conditionally projective family of marginals. In particular  if we can deﬁne a parametric model on
x for I ∈ F(E) such that these models are conditionally projective 
each ﬁnite-dimensional space ΩI
the models determine an inﬁnite-dimensional parametric model (a “nonparametric” model) on the
overall space ΩE
x.
X(X I|ΘI)
Theorem 2 (Extension of Conditional Probabilities). Let E be a countable index set. Let P I
be a family of regular conditional probabilities on the product space ΩI
x. Then if the family is
X (X E|CE) on the inﬁnite-
conditionally projective  there exists a regular conditional probability P E
X (X E|CE) is measurable
X(X I|ΘI) as its conditional marginals. P E
dimensional space ΩE
with respect to the σ-algebra CE := σ(∪I∈F (E)σ(ΘI)). In particular  if the parameter variables
X (X E|ΘE)
satisfy πJIΘJ = ΘI  then P E

X (X E|CE) can be interpreted as the conditional probability P E

x with the P I

with ΘE :=(cid:78)

i∈E Θ{i}.

Proof Sketch2. We ﬁrst apply the Kolmogorov theorem separately for each setting of the parameters
X(X I|ΘI = θI) projective. For any given ω ∈ Ω (the abstract probability
that makes the measures P I
space)  projectiveness holds if θI = ΘI(ω) for all I ∈ F(E). However  for any conditionally
projective family  there is a set N ⊂ Ω of possible exceptions (for which projectiveness need not
hold)  due to the fact that conditional probabilities and conditional projections are only unique almost
everywhere. Using the countability of the dimension set E  we can argue that N is always a null set;
the resulting set of constructed inﬁnite-dimensional measures is still a valid candidate for a regular
conditional probability. We then show that if this set of measures is assembled into a function of the
parameter  it satisﬁes the measurability conditions of a regular conditional probability: We ﬁrst use
the properties of the marginals to show measurability on the subset of events which are preimages
under projection of ﬁnite-dimensional events (the cylinder sets)  and then use the π-λ theorem [3]
to extend measurability to all events.

4 Conjugacy

The posterior of a Dirichlet process is again a Dirichlet process  and the posterior parameters can be
computed as a function of the data and the prior parameters. This property is known as conjugacy 
in analogy to conjugacy in parametric Bayesian models  and makes Dirichlet process inference
tractable. Virtually all known nonparametric Bayesian models  including Gaussian processes  P´olya
trees  and neutral-to-the-right processes are conjugate [16]. In the Bayesian and exponential family
literature  conjugacy is often deﬁned as “closure under sampling”  i.e. for a given likelihood and a
given class of priors  the posterior is again an element of the prior class [12]. This deﬁnition does not
imply tractability of the posterior: In particular  the set of all probability measures (used as priors)
is conjugate for any possible likelihood  but obviously this does not facilitate computation of the
posterior. In the following  we call a prior and a likelihood of a Bayesian model conjugate if the
posterior (i) is parameterized and (ii) there is a measurable mapping T from the data x and the prior
parameter ψ to the parameter ψ(cid:48) = T (x  ψ) which speciﬁes the corresponding posterior. In the
deﬁnition below  the conditional probability k represents the parametric form of the posterior. The
deﬁnition is applicable to “nonparametric” models  in which case the parameter simply becomes
inﬁnite-dimensional.
Deﬁnition 2 (Conjugacy and Posterior Index). Let PX(X|Θ) and PΘ(Θ|Ψ) be regular conditional
probabilities. Let PΘ(Θ|X  Ψ) be the posterior of the model PX(X|Θ) under prior PΘ(Θ|Ψ). Model
and prior are called conjugate if there exists a regular conditional probability k : Bθ × Ωt → [0  1] 
parameterized on a measurable Polish space (Ωt Bt)  and a measurable map T : Ωx × Ωψ → Ωt 
such that

PΘ(A|X = x  Ψ = ψ) = k(A  T (x  ψ))
The mapping T is called the posterior index of the model.
The deﬁnition becomes trivial for Ωt = Ωx × Ωψ and T chosen as the identity mapping; it is mean-
ingful if T is reasonably simple to evaluate  and its complexity does not increase with sample size.
Theorem 3 below shows that  under suitable conditions  the structure of the posterior index carries

for all A ∈ Bθ .

(3)

2Complete proofs for both theorems in this paper are provided as supplementary material.

5

over to the projective limit model: If the ﬁnite-dimensional marginals admit a tractable posterior
index  then so does the projective limit model.
(Posterior Indices in Exponential Families) Suppose that PX(X|Θ) is an exponential
Example.
family model with sufﬁcient statistic S and density p(x|θ) = exp((cid:104)S(x)  θ(cid:105)− γ(x)− φ(θ)). Choose
PΘ(Θ|Ψ) as the “natural conjugate prior” with parameters ψ = (α  y). Its density  w.r.t. a suitable
measure νΘ on parameter space  is of the form q(θ|α  y) = K(α  y)−1 exp((cid:104)θ  y(cid:105) − αφ(θ)). The
posterior PΘ(Θ|X  Ψ) is conjugate in the sense of Def. 2  and its density is q(θ|α + 1  y + S(x)).
A q(θ|t1  t2)dνΘ(θ)  and the posterior index

The probability kernel k is given by k(A  (t1  t2)) :=(cid:82)

is T (x  (α  y)) := (α + 1  y + S(x)).
The main result of this section is Theorem 3  which explains how conjugacy carries over from
the ﬁnite-dimensional to the inﬁnite-dimensional case  and vice versa. Both extension theorems
discussed so far require a projection condition on the measures and models involved. A similar
condition is now required for the mappings T I: The preimages T I -1 of the posterior indices T I must
commute with the preimage under projection 

(πEI ◦ T E)-1 = (T I ◦ πEI)-1

for all I ∈ F(E) .

(4)

x and ΩE

x → ΩE

Θ(ΘE)  P E

X (X E|ΘE) and P E

X (X E|ΘE) with prior P E

Θ(ΘE)  and the following holds:

(i) Assume that each ﬁnite-dimensional posterior P I

The posterior indices of all well-known exponential family models  such as Gaussians and Dirich-
lets  satisfy this condition. The following theorem states that (i) stochastic process Bayesian models
that are constructed from conjugate marginals are conjugate if the projection equation (4) is satisﬁed 
and that (ii) such conjugate models can only be constructed from conjugate marginals.
Theorem 3 (Functional Conjugacy of Projective Limit Models). Let E be a countable index set
and ΩE
θ be Polish product spaces. Assume that there is a Bayesian model on each ﬁnite-
dimensional subspace ΩI
x  such that the families of all priors  all observation models and all poste-
Θ(ΘE|X E) denote the respective
riors are conditionally projective. Let P E
Θ(ΘE|X E) is a posterior for the inﬁnite-dimensional Bayesian model de-
projective limits. Then P E
ﬁned by P E
Θ(ΘI|X I) is conjugate w.r.t. its respective
Bayesian model  with posterior index T I and probability kernel kI. Then if there is a mea-
surable mapping T : ΩE
t satisfying the projection condition (4)  the projective limit
posterior P E
Θ(ΘE|X E) is conjugate with posterior
Θ(ΘI|X I) is conjugate 
EI. The corresponding probability kernels kI are

index T E and probability kernel kE  then each marginal posterior P I
with posterior index T I := πEI ◦ T E ◦ π-1
given by
kI(AI  tI) := kE(π-1

(5)
The theorem is not stated here in full generality  but under two simplifying assumptions: We have
omitted the use of hyperparameters  such that the posterior indices depend only on the data  and all
involved spaces (observation space  parameter space etc) are assumed to have the same dimension
for each Bayesian model. Generalizing the theorem beyond both assumptions is technically not dif-
ﬁcult  but the additional parameters and notation for book-keeping on dimensions reduce readability.

Θ(ΘE|X E) is conjugate with posterior index T .

(ii) Conversely  if the inﬁnite-dimensional posterior P E

EIAI  t)

for any t ∈ π-1

EItI .

Proof Sketch2. Part (i): We deﬁne a candidate for the probability kernel kE representing the projec-
tive limit posterior  and then verify that it makes the model conjugate when combined with the map-
Θ(ΘI|T I) 
ping T given by assumption. To do so  we ﬁrst construct the conditional probabilities P I
show that they form a conditionally projective family  and take their conditional projective limit
using Theorem 2. This projective limit is used as a candidate for kE. To show that kE indeed repre-
sents the posterior  we show that the two coincide on the cylinder sets (events which are preimages
under projection of ﬁnite-dimensional events). From this  equality for all events follows by the
Caratheodory theorem [1].
Part (ii): We only have to verify that the mappings T I and probability kernels kI indeed satisfy the
deﬁnition of conjugacy  which is a straightforward computation.

5 Construction of Nonparametric Bayesian Models

Theorem 3(ii) states that conjugate models have conjugate marginals.
in the ﬁnite-
dimensional case  conjugate Bayesian models are essentially limited to exponential families and

Since 

6

their natural conjugate priors3  a consequence of the theorem is that we can only expect a non-
parametric Bayesian model to be conjugate if it is constructed from exponential family marginals –
assuming that the construction is based on a product space approach.

When an exponential family model and its conjugate prior are used in the construction  the form
of the resulting model becomes generic: The posterior index T of a conjugate exponential fam-
ily Bayesian model is always given by the sufﬁcient statistic S in the form T (x  (α  y)) :=
(α + 1  y + S(x)). Addition commutes with projection  and hence the posterior indices T I of a
family of such models over all dimensions I ∈ F(E) satisfy the projection condition (4) if and
only if the same condition is satisﬁed by the sufﬁcient statistics SI of the marginals. Accord-
ingly  the inﬁnite-dimensional posterior index T E in Theorem 3 exists if and only if there is an
inﬁnite-dimensional “extension” SE of the sufﬁcient statistics SI satisfying (4). If that is the case 
T E(xE  (α  yE)) := (α + 1  yE + SE(xE)) is a posterior index for the inﬁnite-dimensional projective
limit model. In the case of countable dimensions  Theorem 3 therefore implies a construction recipe
for nonparametric Bayesian models from exponential family marginals; constructing the model boils
down to checking whether the models selected as ﬁnite-dimensional marginals are conditionally
projective  and whether the sufﬁcient statistics satisfy the projection condition. An example con-
struction  for a model on inﬁnite permutations  is given in below. The following table summarizes
some stochastic process models from the conjugate extension point of view:

Marginals (d-dim)

Bernoulli/Beta
Multin./Dirichlet
Gaussian/Gaussian
Mallows/conjugate

Projective limit model

Beta process; IBP

DP; CRP
GP/GP

Example below

Observations (limit)

Binary arrays

Discrete distributions
(continuous) functions
Bijections N → N

l=j+1

A Construction Example. The analysis of preference data  in which preferences are represented
as permutations  has motivated the deﬁnition of distributions on permutations of an inﬁnite number
of items [9]. A ﬁnite permutation on r items always implies a question such as “rank your favorite
movies out of r movies”. A nonparametric approach can generalize the question to “rank your
favorite movies”. Meila and Bao [9] derived a model on inﬁnite permutations  that is  on bijections of
the set N. We construct a nonparametric Bayesian model on bijections  with a likelihood component
X (X E|ΘE) equivalent to the model of Meila and Bao.
P E
Choice of marginals. The ﬁnite-dimensional marginals are probability models of rankings of a ﬁnite
number of items  introduced by Fligner and Verducci [6]. For permutations τ ∈ Sr of length r 

the model is deﬁned by the exponential family density p(τ|σ  θ) := Z(θ)−1 exp((cid:10)S(τ σ−1)  θ(cid:11)) 
(cid:80)r

where the sufﬁcient statistic is the vector Sr(τ) := (S1(τ)  . . .   Sr(τ)) with components Sj(τ) :=
I{τ−1(j) > τ−1(l)}. Roughly speaking  the model is a location-scale model  and the
permutation σ deﬁnes the distribution’s mean. If all entries of θ are chosen identical as some con-
stant  this constant acts as a concentration parameter  and the scalar product is equivalent to the
Kendall metric on permutations. This metric measures distance between permutations as the min-
imum number of adjacent transpositions (i.e. swaps of neighboring entries) required to transform
one permutation into the other. If the entries of θ differ  they can be regarded as weights specifying
the relevance of each position in the ranking [6].
Deﬁnition of marginals. In the product space context  each ﬁnite set I ∈ F(E) of axis labels is a
Θ(τ I|σI  θI) is a model on the corresponding ﬁnite
set of items to be permuted  and the marginal P I
permutation group SI on the elements of I. The sufﬁcient statistics SI maps each permutation to a
vector of integers  and thus embeds the group SI into RI. The mapping is one-to-one [6]. Projections 
i.e. restrictions  on the group mean deletion of elements. A permutation τ J is restricted to a subset
I ⊂ J of indices by deleting all items indexed by J \ I  producing the restriction τ J|I. We overload
notation and write πJI for both the restriction in the group SI and axes-parallel projection in the
Euclidean space RI  into which the sufﬁcient statistic SI embeds SI. It follows from the deﬁnition
of SI that  whenever πJIτ J = τ I  then πJISJ(τ J) = SI(τ I). In other words  πJI ◦ SJ = SI ◦ πJI 
which is a stronger form of the projection condition SJ -1 ◦ π-1
JI ◦ SI -1 given in Eq. 4. We
will deﬁne a nonparametric Bayesian model that puts a prior on the inﬁnite-dimensional analogue

JI = π-1

3Mixtures of conjugate priors are conjugate in the sense of closure under sampling [4]  but the posterior
index in Def. 2 has to be evaluated for each mixture component individually. An example of a conjugate model
not in the exponential family is the uniform distribution on [0  θ] with a Pareto prior [12].

7

given by the density pI(τ I|σI  θI) := Z I(θI)−1 exp((cid:10)SI(τ I(σI)−1)  θI(cid:11)). The corresponding natural

of θ  i.e. on the weight function θE. For I ∈ F(N)  the marginal of the likelihood component is
conjugate prior on θI has density qI(θI|α  yI) ∝ exp((cid:104)θI  yI(cid:105) − α log Z I(θI)). Since the model is an
exponential family model  the posterior index is of the form T I((α  yI)  τ I) = (α + 1  yI + SI(τ I)) 
and since SI is projective in the sense of Eq. 4  so is T I. The prior and likelihood densities above
deﬁne two families P I(X I|ΘI) and P I(ΘI|Ψ) of measures over all ﬁnite dimensions I ∈ F(E). It
is reasonably straightforward to show that both families are conditionally projective  and so is the
family of the corresponding posteriors. Each therefore has a projective limit  and the projective limit
of the posteriors is the posterior of the projective limit P E(X E|ΘE) under prior P E(ΘE).
Posterior index. The posterior index of the inﬁnite-dimensional model can be derived by means
of Theorem 3: To get rid of the hyperparameters  we ﬁrst ﬁx a value ψE := (α  yE) of the
inﬁnite-dimensional hyperparameter  and only consider the corresponding inﬁnite-dimensional prior
Θ(ΘE|ΨE = ψE)  with its marginals P I
Θ(ΘI|ΨI = πEIψE). Now deﬁne a function SE on the
(cid:80)∞
P E
bijections of N as follows. For each bijection τ : N → N  and each j ∈ N  set SE
j (τ) :=
I{τ−1(j) > τ−1(l)}. Since τ−1(j) is a ﬁnite number for any j ∈ N  the indicator function
is non-zero only for a ﬁnite number of indices l  such that the entries of SE are always ﬁnite. Then
EISI -1 for all I ∈ F(E). As candidate posterior
SE satisﬁes the projection condition SE -1 ◦ π-1
index  we deﬁne the function T E((α  yE)  τ E) = (α + 1  yE + SE(τ E)) for yE ∈ ΩN
θ . Then T E also
satisﬁes the projection condition (4) for any I ∈ F(E). By Theorem 3  this makes T E a posterior
index for the projective limit model.

l=j+1

EI = π-1

6 Discussion and Conclusion
We have shown how nonparametric Bayesian models can be constructed from ﬁnite-dimensional
Bayes equations  and how conjugacy properties of the ﬁnite-dimensional models carry over to
the inﬁnite-dimensional  nonparametric case. We also have argued that conjugate nonparametric
Bayesian models arise from exponential families.

A number of interesting questions could not be addressed within the scope of this paper  including
(1) the extension to model properties other than conjugacy and (2) the generalization to uncountable
dimensions. For example  a model property which is closely related to conjugacy is sufﬁciency [14].
In this case  we would ask whether the existence of sufﬁcient statistics for the ﬁnite-dimensional
marginals implies the existence of a sufﬁcient statistic for the nonparametric Bayesian model  and
whether the inﬁnite-dimensional sufﬁcient statistic can be explicitly constructed. Second  the results
presented here are restricted to the case of countable dimensions. This restriction is inconvenient 
since the natural product space representations of  for example  Gaussian and Dirichlet processes
on the real line have uncountable dimensions. The GP (on continuous functions) and the DP are
within the scope of our results  as both can be derived by means of countable-dimensional surrogate
constructions: Since continuous functions on R are completely determined by their values on Q  a
GP can be constructed on the countable-dimensional product space RQ. Analogous constructions
have been proposed for the DP [7]. The drawback of this approach is that the actual random draw is
just a partial version of the object of interest  and formally has to be completed e.g. into a continuous
function or a probability measure after it is sampled. On the other hand  uncountable product space
constructions are subject to all the subtleties of stochastic process theory  many of which do not
occur in countable dimensions. The application of construction methods to conditional probabilities
also becomes more complicated (roughly speaking  the point-wise application of the Kolmogorov
theorem in the proof of Theorem 2 is not possible if the dimension is uncountable).

Product space constructions are by far not the only way to deﬁne nonparametric Bayesian models. A
P´olya tree model [7]  for example  is much more intuitive to construct by means of a binary partition
argument than from marginals in product space. As far as characterization results  such as which
models can be conjugate  are concerned  our results are still applicable  since the set of Poly´a trees
can be embedded into a product space. However  the marginals may then not be the marginals in
terms of which we “naturally” think about the model. Nonetheless  we have hopefully demonstrated
that the theoretical results are applicable for the construction of an interesting and practical range of
nonparametric Bayesian models.
Acknowledgments. I am grateful to Joachim M. Buhmann  Zoubin Ghaharamani  Finale Doshi-
Velez and the reviewers for helpful comments. This work was in part supported by EPSRC grant
EP/F028628/1.

8

References

[1] H. Bauer. Probability Theory. W. de Gruyter  1996.
[2] M. J. Beal  Z. Ghahramani  and C. E. Rasmussen. The inﬁnite hidden Markov model.

Advances in Neural Information Processing Systems  2001.

In

[3] P. Billingsley. Probability and measure  1995.
[4] S. R. Dalal and W. J. Hall. Approximating priors by mixtures of natural conjugate priors.

Annals of Statistics  45(2):278–286  1983.

[5] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics 

1(2)  1973.

[6] M. A. Fligner and J. S. Verducci. Distance based ranking models. Journal of the Royal Statis-

tical Society B  48(3):359–369  1986.

[7] J. K. Ghosh and R. V. Ramamoorthi. Bayesian Nonparametrics. Springer  2002.
[8] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In Advances in Neural Information Processing Systems  2005.

[9] M. Meil˘a and L. Bao. Estimation and clustering with inﬁnite rankings.

Artiﬁcial Intelligence  2008.

In Uncertainty in

[10] M. M. Rao. Conditional Measures and Applications. Chapman & Hall  second edition  2005.
[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[12] C. P. Robert. The Bayesian Choice. Springer  1994.
[13] D. M. Roy and Y. W. Teh. The Mondrian process. In Advances in Neural Information Pro-

cessing Systems  2009.

[14] M. J. Schervish. Theory of Statistics. Springer  1995.
[15] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal

of the American Statistical Association  (476):1566–1581  2006.

[16] S. G. Walker  P. Damien  P. W. Laud  and A. F. M. Smith. Bayesian nonparametric inference
for random distributions and related functions. Journal of the Royal Statistical Society B 
61(3):485–527  1999.

[17] L. Wasserman. All of Nonparametric Statistics. Springer  2006.

9

,Troy Lee
Adi Shraibman
Xu Chen
Xiuyuan Cheng
Stephane Mallat
Benjamin Cowley
Ryan Williamson
Katerina Clemens
Matthew Smith
Byron Yu