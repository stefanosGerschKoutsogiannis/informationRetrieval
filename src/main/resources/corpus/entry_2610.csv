2019,Incremental Few-Shot Learning with Attention Attractor Networks,Machine learning classifiers are often trained to recognize a set of pre-defined classes. However  in many applications  it is often desirable to have the flexibility of learning additional concepts  with limited data and without re-training on the full training set. This paper addresses this problem  incremental few-shot learning  where a regular classification network has already been trained to recognize a set of base classes  and several extra novel classes are being considered  each with only a few labeled examples. After learning the novel classes  the model is then evaluated on the overall classification performance on both base and novel classes. To this end  we propose a meta-learning model  the Attention Attractor Network  which regularizes the learning of novel classes. In each episode  we train a set of new weights to recognize novel classes until they converge  and we show that the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of these parameters. We demonstrate that the learned attractor network can help recognize novel classes while remembering old classes without the need to review the original training set  outperforming various baselines.,Incremental Few-Shot Learning with Attention

Attractor Networks

Mengye Ren1 2 3  Renjie Liao1 2 3  Ethan Fetaya1 2  Richard S. Zemel1 2

1University of Toronto  2Vector Institute  3Uber ATG
{mren  rjliao  ethanf  zemel}@cs.toronto.edu

Abstract

Machine learning classiﬁers are often trained to recognize a set of pre-deﬁned
classes. However  in many applications  it is often desirable to have the ﬂexibility
of learning additional concepts  with limited data and without re-training on the
full training set. This paper addresses this problem  incremental few-shot learning 
where a regular classiﬁcation network has already been trained to recognize a
set of base classes  and several extra novel classes are being considered  each
with only a few labeled examples. After learning the novel classes  the model is
then evaluated on the overall classiﬁcation performance on both base and novel
classes. To this end  we propose a meta-learning model  the Attention Attractor
Network  which regularizes the learning of novel classes. In each episode  we
train a set of new weights to recognize novel classes until they converge  and
we show that the technique of recurrent back-propagation can back-propagate
through the optimization process and facilitate the learning of these parameters.
We demonstrate that the learned attractor network can help recognize novel classes
while remembering old classes without the need to review the original training set 
outperforming various baselines.

1

Introduction

The availability of large scale datasets with detailed annotation  such as ImageNet [30]  played a
signiﬁcant role in the recent success of deep learning. The need for such a large dataset is however a
limitation  since its collection requires intensive human labor. This is also strikingly different from
human learning  where new concepts can be learned from very few examples. One line of work
that attempts to bridge this gap is few-shot learning [16  36  33]  where a model learns to output a
classiﬁer given only a few labeled examples of the unseen classes. While this is a promising line
of work  its practical usability is a concern  because few-shot models only focus on learning novel
classes  ignoring the fact that many common classes are readily available in large datasets.
An approach that aims to enjoy the best of both worlds  the ability to learn from large datasets for
common classes with the ﬂexibility of few-shot learning for others  is incremental few-shot learning
[9]. This combines incremental learning where we want to add new classes without catastrophic
forgetting [20]  with few-shot learning when the new classes  unlike the base classes  only have a
small amount of examples. One use case to illustrate the problem is a visual aid system. Most objects
of interest are common to all users  e.g.  cars  pedestrian signals; however  users would also like to
augment the system with additional personalized items or important landmarks in their area. Such a
system needs to be able to learn new classes from few examples  without harming the performance
on the original classes and typically without access to the dataset used to train the original classes.
In this work we present a novel method for incremental few-shot learning where during meta-learning
we optimize a regularizer that reduces catastrophic forgetting from the incremental few-shot learning.
Our proposed regularizer is inspired by attractor networks [42] and can be thought of as a memory of
the base classes  adapted to the new classes. We also show how this regularizer can be optimized 
using recurrent back-propagation [18  1  25] to back-propagate through the few-shot optimization

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Our proposed attention attractor network for incremental few-shot learning. During
pretraining we learn the base class weights Wa and the feature extractor CNN backbone. In the
meta-learning stage  a few-shot episode is presented. The support set only contains novel classes 
whereas the query set contains both base and novel classes. We learn an episodic classiﬁer network
through an iterative solver  to minimize cross entropy plus an additional regularization term predicted
by the attention attractor network by attending to the base classes. The attention attractor network is
meta-learned to minimize the expected query loss. During testing an episodic classiﬁer is learned in
the same way.

stage. Finally  we show empirically that our proposed method can produce state-of-the-art results in
incremental few-shot learning on mini-ImageNet [36] and tiered-ImageNet [29] tasks.

2 Related Work

Recently  there has been a surge in interest in few-shot learning [16  36  33  17]  where a model
for novel classes is learned with only a few labeled examples. One family of approaches for few-
shot learning  including Deep Siamese Networks [16]  Matching Networks [36] and Prototypical
Networks [33]  follows the line of metric learning. In particular  these approaches use deep neural
networks to learn a function that maps the input space to the embedding space where examples
belonging to the same category are close and those belonging to different categories are far apart.
Recently  [8] proposes a graph neural networks based method which captures the information
propagation from the labeled support set to the query set. [29] extends Prototypical Networks to
leverage unlabeled examples while doing few-shot learning. Despite their simplicity  these methods
are very effective and often competitive with the state-of-the-art.
Another class of approaches aims to learn models which can adapt to the episodic tasks. In particular 
[27] treats the long short-term memory (LSTM) as a meta learner such that it can learn to predict
the parameter update of a base learner  e.g.  a convolutional neural network (CNN). MAML [7]
instead learns the hyperparameters or the initial parameters of the base learner by back-propagating
through the gradient descent steps. [31] uses a read/write augmented memory  and [21] combines
soft attention with temporal convolutions which enables retrieval of information from past episodes.
Methods described above belong to the general class of meta-learning models. First proposed in
[32  23  35]  meta-learning is a machine learning paradigm where the meta-learner tries to improve
the base learner using the learning experiences from multiple tasks. Meta-learning methods typically
learn the update policy yet lack an overall learning objective in the few-shot episodes. Furthermore 
they could potentially suffer from short-horizon bias [41]  if at test time the model is trained for

2

BackboneCNNFew-Shot Episodic NetworkPretrained BackboneAttention AttractorNetworkBase Class Weights!"Support Loss#Attractor Regularizerdogcatplanetrain…#(%&;()%&Fast weights+Minimization%+Joint Classificationdogcatplanetrain…fishshipFew-Shot EpisodeQuerySupportAttentionPretraining StageWhen are parameters being learned?Meta-Learning StageEvery Few-Shot Episode (()longer steps. To address this problem  [4] proposes to use fast convergent models like logistic
regression (LR)  which can be back-propagated via a closed form update rule. Compared to [4]  our
proposed method using recurrent back-propagation [18  1  25] is more general as it does not require a
closed-form update  and the inner loop solver can employ any existing continuous optimizers.
Our work is also related to incremental learning  a setting where information is arriving continuously
while prior knowledge needs to be transferred. A key challenge is catastrophic forgetting [20  19]  i.e. 
the model forgets the learned knowledge. Various memory-based models have since been proposed 
which store training examples explicitly [28  34  5  24]  regularize the parameter updates [15]  or
learn a generative model [13]. However  in these studies  incremental learning typically starts from
scratch  and usually performs worse than a regular model that is trained with all available classes
together since it needs to learned a good representation while dealing with catastrophic forgetting.
Incremental few-shot learning is also known as low-shot learning. To leverage a good representation 
[10  37  9] starts off with a pre-trained network on a set of base classes  and tries to augment the
classiﬁer with a batch of new classes that has not been seen during training. [10] proposes the squared
gradient magnitude loss  which makes the learned classiﬁer from the low-shot examples have a smaller
gradient value when learning on all examples. [37] propose the prototypical matching networks 
a combination of prototypical network and matching network. The paper also adds hallucination 
which generates new examples. [9] proposes an attention based model which generates weights for
novel categories. They also promote the use of cosine similarity between feature representations and
weight vectors to classify images.
In contrast  during each few-shot episode  we directly learn a classiﬁer network that is randomly
initialized and solved till convergence  unlike [9] which directly output the prediction. Since the
model cannot see base class data within the support set of each few-shot learning episode  it is
challenging to learn a classiﬁer that jointly classiﬁes both base and novel categories. Towards this
end  we propose to add a learned regularizer  which is predicted by a meta-network  the “attention
attractor network”. The network is learned by differentiating through few-shot learning optimization
iterations. We found that using an iterative solver with the learned regularizer signiﬁcantly improves
the classiﬁer model on the task of incremental few-shot learning.

3 Model

In this section  we ﬁrst deﬁne the setup of incremental few-shot learning  and then we introduce our
new model  the Attention Attractor Network  which attends to the set of base classes according to
the few-shot training data by using the attractor regularizing term. Figure 1 illustrates the high-level
model diagram of our method.

3.1

Incremental Few-Shot Learning

The outline of our meta-learning approach to incremental few-shot learning is: (1) We learn a
ﬁxed feature representation and a classiﬁer on a set of base classes; (2) In each training and testing
episode we train a novel-class classiﬁer with our meta-learned regularizer; (3) We optimize our
meta-learned regularizer on combined novel and base classes classiﬁcation  adapting it to perform
well in conjunction with the base classiﬁer. Details of these stages follow.

Pretraining Stage: We learn a base model for the regular supervised classiﬁcation task on
dataset {(xa i  ya i)}Na
i=1 where xa i is the i-th example from dataset Da and its labeled class
ya i ∈ {1  2  ...  K}. The purpose of this stage is to learn both a good base classiﬁer and a good
representation. The parameters of the base classiﬁer are learned in this stage and will be ﬁxed
after pretraining. We denote the parameters of the top fully connected layer of the base classiﬁer
Wa ∈ RD×K where D is the dimension of our learned representation.
Incremental Few-Shot Episodes: A few-shot dataset Db is presented  from which we can sample
few-shot learning episodes E. Note that this can be the same data source as the pretraining dataset Da 
but sampled episodically. For each N-shot K(cid:48)-way episode  there are K(cid:48) novel classes disjoint from
the base classes. Each novel class has N and M images from the support set Sb and the query set
Qb respectively. Therefore  we have E = (Sb  Qb)  Sb = (xS
b i)M×K(cid:48)

b i)N×K(cid:48)

b i  yS

i=1

  Qb = (xQ

b i  yQ

i=1

3

where yb i ∈ {K + 1  ...  K + K(cid:48)}. Sb and Qb can be regarded as this episodes training and validation
sets. Each episode we learn a classiﬁer on the support set Sb whose learnable parameters Wb are
called the fast weights as they are only used during this episode. To evaluate the performance on
a joint prediction of both base and novel classes  i.e.  a (K + K(cid:48))-way classiﬁcation  a mini-batch
Qa = {(xa i  ya i)}M×K
sampled from Da is also added to Qb to form Qa+b = Qa ∪ Qb. This
means that the learning algorithm  which only has access to samples from the novel classes Sb  is
evaluated on the joint query set Qa+b.

i=1

In meta-training  we iteratively sample few-shot episodes E and try to learn
Meta-Learning Stage:
the meta-parameters in order to minimize the joint prediction loss on Qa+b. In particular  we design a
regularizer R(·  θ) such that the fast weights are learned via minimizing the loss (cid:96)(Wb  Sb)+R(Wb  θ)
where (cid:96)(Wb  Sb) is typically cross-entropy loss for few-shot classiﬁcation. The meta-learner tries to
learn meta-parameters θ such that the optimal fast weights W ∗
b w.r.t. the above loss function performs
well on Qa+b. In our model  meta-parameters θ are encapsulated in our attention attractor network 
which produces regularizers for the fast weights in the few-shot learning objective.

Joint Prediction on Base and Novel Classes: We now introduce the details of our joint prediction
framework performed in each few-shot episode. First  we construct an episodic classiﬁer  e.g.  a
logistic regression (LR) model or a multi-layer perceptron (MLP)  which takes the learned image
features as inputs and classiﬁes them according to the few-shot classes.
During training on the support set Sb  we learn the fast weights Wb via minimizing the following
regularized cross-entropy objective  which we call the episodic objective:

LS(Wb  θ) = − 1
N K(cid:48)

yS
b i c log ˆyS

b i c + R(Wb  θ).

This is a general formulation and the speciﬁc functional form of the regularization term R(Wb  θ) will
be speciﬁed later. The predicted output ˆyS
where h(xb i) is our classiﬁcation network and Wb is the fast weights in the network. In the case of
LR  h is a linear model: h(xb i; Wb) = W (cid:62)
b xb i. h can also be an MLP for more expressive power.
During testing on the query set Qa+b  in order to predict both base and novel classes  we directly
augment the softmax with the ﬁxed base class weights Wa  ˆyQ
where W ∗

b are the optimal parameters that minimize the regularized classiﬁcation objective in Eq. (1).

b i = softmax((cid:2)W (cid:62)
i = softmax((cid:2)W (cid:62)

a xb i  h(xb i; W ∗

b i is obtained via  ˆyS

a xi  h(xi; W ∗

(1)

b )(cid:3)) 
b )(cid:3)) 

N K(cid:48)(cid:88)

K+K(cid:48)(cid:88)

i=1

c=K+1

3.2 Attention Attractor Networks

K(cid:48)(cid:88)

k(cid:48)=1

Directly learning the few-shot episode  e.g.  by setting R(Wb  θ) to be zero or simple weight decay 
can cause catastrophic forgetting on the base classes. This is because Wb which is trained to maximize
the correct novel class probability can dominate the base classes in the joint prediction. In this section 
we introduce the Attention Attractor Network to address this problem. The key feature of our attractor
network is the regularization term R(Wb  θ):

R(Wb  θ) =

(Wb k(cid:48) − uk(cid:48))(cid:62)diag(exp(γ))(Wb k(cid:48) − uk(cid:48)) 

(2)

where uk(cid:48) is the so-called attractor and Wb k(cid:48) is the k(cid:48)-th column of Wb. This sum of squared
Mahalanobis distances from the attractors adds a bias to the learning signal arriving solely from
novel classes. Note that for a classiﬁer such as an MLP  one can extend this regularization term
in a layer-wise manner. Speciﬁcally  one can have separate attractors per layer  and the number of
attractors equals the number of output dimension of that layer.
To ensure that the model performs well on base classes  the attractors uk(cid:48) must contain some
information about examples from base classes. Since we can not directly access these base examples 
we propose to use the slow weights to encode such information. Speciﬁcally  each base class has
a learned attractor vector Uk stored in the memory matrix U = [U1  ...  UK]. It is computed as 
Uk = fφ(Wa k)  where f is a MLP of which the learnable parameters are φ. For each novel class k(cid:48)
its classiﬁer is regularized towards its attractor uk(cid:48) which is a weighted sum of Uk vectors. Intuitively

4

(cid:16)

the weighting is an attention mechanism where each novel class attends to the base classes according
to the level of interference  i.e. how prediction of new class k(cid:48) causes the forgetting of base class k.
(cid:17)
For each class in the support set  we compute the cosine similarity between the average representation
(cid:80)
of the class and base weights Wa then normalize using a softmax function
(cid:80)
j hj 1[yb j = k(cid:48)]  Wa k)
j hj 1[yb j = k(cid:48)]  Wa k)
sum of entries in the memory matrix U  uk(cid:48) =(cid:80)

where A is the cosine similarity function  hj are the representations of the inputs in the support set Sb
and τ is a learnable temperature scalar. ak(cid:48) k encodes a normalized pairwise attention matrix between
the novel classes and the base classes. The attention vector is then used to compute a linear weighted
k ak(cid:48) kUk + U0  where U0 is an embedding vector

(cid:80)

(cid:17)  

τ A( 1
N

τ A( 1
N

ak(cid:48) k =

k exp

(cid:16)

exp

(3)

b   yQ

b   yS
a+b  yQ

b )} {(xQ
b )} ← GetEpisode(Db);
a+b} ← GetMiniBatch(Da) ∪ {(xQ

Algorithm 1 Meta Learning for Incremental Few-Shot
Learning
Require: θ0  Da  Db  h
Ensure: θ
1: θ ← θ0;
2: for t = 1 ... T do
{(xS
3:
{xQ
4:
5:
6:
7:
8:
9:
10:
11:
12:

LS ← 1
Wb ← OptimizerStep(Wb ∇Wb LS);
until Wb converges
a+b j ← softmax([W (cid:62)
ˆyQ
LQ ← 1

N K(cid:48)(cid:80)
2N K(cid:48)(cid:80)

b i + R(Wb; θ);

a+b j; Wb)]);

a+b j log ˆyQ

a+b j  h(xQ

b i log ˆyS

b )};

repeat

a+b j;

b   yQ

a xQ

j yQ

i yS

b ← Wb − α∇Wb LS;

// Backprop through the above optimization via RBP
// A dummy gradient descent step
J ← ∂W (cid:48)
repeat

; g ← v;
v ← J(cid:62)v − v; g ← g + v;

; v ← ∂LQ

b
∂Wb

∂Wb

until g converges
θ ← OptimizerStep(θ  g(cid:62) ∂W (cid:48)
∂θ )

b

13: W (cid:48)
14:
15:
16:
17:
18:
19:
20: end for

yj c log ˆyj c(θ  Sb)

(4)

  
b (θ  Sb))(cid:3)(cid:1).

and serves as a bias for the attractor.
Our design takes inspiration from attractor
networks [22  42]  where for each base
class one learns an “attractor” that stores
the relevant memory regarding that class.
We call our full model “dynamic attractors”
as they may vary with each episode even
after meta-learning. In contrast if we only
have the bias term U0  i.e. a single attractor
which is shared by all novel classes  it will
not change after meta-learning from one
episode to the other. We call this model
variant the “static attractor”.
In summary  our meta parameters θ include
φ  U0  γ and τ  which is on the same
scale as as the number of paramters in Wa.
It is important to note that R(Wb  θ) is
convex w.r.t. Wb. Therefore  if we use
the LR model as the classiﬁer  the overall
training objective on episodes in Eq. (1)
is convex which implies that the optimum
W ∗
b (θ  Sb) is guaranteed to be unique and
achievable. Here we emphasize that the
optimal parameters W ∗
b are functions of
parameters θ and few-shot samples Sb.
During meta-learning  θ are updated to
minimize an expected loss of the query
set Qa+b which contains both base and
novel classes  averaging over all few-shot
learning episodes 

M (K+K(cid:48))(cid:88)
K+K(cid:48)(cid:88)
where the predicted class is ˆyj(θ  Sb) = softmax(cid:0)(cid:2)W (cid:62)

(cid:2)LQ(θ  Sb)(cid:3) = E

min

E
E

j=1

c=1

E

θ

a xj  h (xj; W ∗

3.3 Learning via Recurrent Back-Propagation

As there is no closed-form solution to the episodic objective (the optimization problem in Eq. 1)  in
each episode we need to minimize LS to obtain W ∗
b through an iterative optimizer. The question is
how to efﬁciently compute ∂W ∗
∂θ   i.e.  back-propagating through the optimization. One option is to
unroll the iterative optimization process in the computation graph and use back-propagation through
time (BPTT) [38]. However  the number of iterations for a gradient-based optimizer to converge can
be on the order of thousands  and BPTT can be computationally prohibitive. Another way is to use

b

5

Table 1: Comparison of our proposed model with other methods

Method

Few-shot learner

Episodic objective

Attention mechanism

Imprint [26]
LwoF [9]
Ours

Prototypes
N/A
Prototypes + base classes N/A
A fully trained classiﬁer

Cross entropy on
novel classes

N/A
Attention on base classes
Attention on learned attractors

b ) = W (t)

b − F (W (t)

b )  where F (W (t)

b ) = W (t+1)

b

the truncated BPTT [39] (T-BPTT) which optimizes for T steps of gradient-based optimization  and
is commonly used in meta-learning problems. However  when T is small the training objective could
be signiﬁcantly biased.
Alternatively  the recurrent back-propagation (RBP) algorithm [1  25  18] allows us to back-propagate
through the ﬁxed point efﬁciently without unrolling the computation graph and storing intermediate
activations. Consider a vanilla gradient descent process on Wb with step size α. The difference
between two steps Φ can be written as Φ(W (t)
=
b − α∇LS(W (t)
b ). Since Φ(W ∗
W (t)
b (θ)) is identically zero as a function of θ  using the implicit
function theorem we have ∂W ∗
denotes the Jacobian matrix
of the mapping F evaluated at W ∗
b . Algorithm 1 outlines the key steps for learning the episodic
objective using RBP in the incremental few-shot learning setting. Note that the RBP algorithm
implicitly inverts (I − J(cid:62)) by computing the matrix inverse vector product  and has the same time
complexity compared to truncated BPTT given the same number of unrolled steps  but meanwhile
RBP does not have to store intermediate activations.
Damped Neumann RBP To compute the matrix-inverse vector product (I−J(cid:62))−1v  [18] propose
n=0 v(n). Note that J(cid:62)v can be
computed by standard back-propagation. However  directly applying the Neumann RBP algorithm
sometimes leads to numerical instability. Therefore  we propose to add a damping term 0 <  < 1
to I − J(cid:62). This results in the following update: ˜v(n) = (J(cid:62) − I)nv. In practice  we found the
damping term with  = 0.1 helps alleviate the issue signiﬁcantly.

to use the Neumann series: (I − J(cid:62))−1v =(cid:80)∞

n=0(J(cid:62))nv ≡(cid:80)∞

∂θ = (I − J(cid:62)

∂θ   where JF W ∗

b

b

)−1 ∂F

F W ∗

b

4 Experiments

We experiment on two few-shot classiﬁcation datasets  mini-ImageNet and tiered-ImageNet. Both
are subsets of ImageNet [30]  with images sizes reduced to 84 × 84 pixels. We also modiﬁed the
datasets to accommodate the incremental few-shot learning settings. 1

4.1 Datasets
• mini-ImageNet Proposed by [36]  mini-ImageNet contains 100 object classes and 60 000 images.
We used the splits proposed by [27]  where training  validation  and testing have 64  16 and 20
classes respectively.

• tiered-ImageNet Proposed by [29]  tiered-ImageNet is a larger subset of ILSVRC-12. It features
a categorical split among training  validation  and testing subsets. The categorical split means that
classes that belong to the same high-level category  e.g. “working dog” and ”terrier” or some other
dog breed  are not split between training  validation and test. This is a harder task  but one that
more strictly evaluates generalization to new classes. It is also an order of magnitude larger than
mini-ImageNet.

4.2 Experiment setup

We use a standard ResNet backbone [11] to learn the feature representation through supervised
training. For mini-ImageNet experiments  we follow [21] and use a modiﬁed version of ResNet-10.

1Code released at: https://github.com/renmengye/inc-few-shot-attractor-public

6

Table 2: mini-ImageNet 64+5-way results

Table 3: tiered-ImageNet 200+5-way results

Model

ProtoNet [33]
Imprint [26]
LwoF [9]

Ours

1-shot

Acc. ↑

42.73 ± 0.15
41.10 ± 0.20
52.37 ± 0.20
54.95 ± 0.30

∆ ↓
-20.21
-22.49
-13.65
-11.84

5-shot

Acc. ↑

57.05 ± 0.10
44.68 ± 0.23
59.90 ± 0.20
63.04 ± 0.30

∆ ↓
-31.72
-27.68
-14.18
-10.66

Model

ProtoNet [33]
Imprint [26]
LwoF [9]

Ours

1-shot

Acc. ↑

30.04 ± 0.21
39.13 ± 0.15
52.40 ± 0.33
56.11 ± 0.33

∆ ↓
-29.54
-22.26
-8.27
-6.11

5-shot

Acc. ↑

41.38 ± 0.28
53.60 ± 0.18
62.63 ± 0.31
65.52 ± 0.31

∆ ↓
-26.39
-16.35
-6.72
-4.48
2 (∆a + ∆b))

∆ = average decrease in acc. caused by joint prediction within base and novel classes (∆ = 1

↑ (↓) represents higher (lower) is better.

For tiered-ImageNet  we use the standard ResNet-18 [11]  but replace all batch normalization [12]
layers with group normalization [40]  as there is a large distributional shift from training to testing in
tiered-ImageNet due to categorical splits. We used standard data augmentation  with random crops
and horizonal ﬂips. We use the same pretrained checkpoint as the starting point for meta-learning.
In the meta-learning stage as well as the ﬁnal evaluation  we sample a few-shot episode from the
Db  together with a regular mini-batch from the Da. The base class images are added to the query
set of the few-shot episode. The base and novel classes are maintained in equal proportion in our
experiments. For all the experiments  we consider 5-way classiﬁcation with 1 or 5 support examples
(i.e. shots). In the experiments  we use a query set of size 25×2 =50.
We use L-BFGS [43] to solve the inner loop of our models to make sure Wb converges. We use the
ADAM [14] optimizer for meta-learning with a learning rate of 1e-3  which decays by a factor of 10
after 4 000 steps  for a total of 8 000 steps. We ﬁx recurrent backpropagation to 20 iterations and
 = 0.1.
We study two variants of the classiﬁer network. The ﬁrst is a logistic regression model with a single
weight matrix Wb. The second is a 2-layer fully connected MLP model with 40 hidden units in the
middle and tanh non-linearity. To make training more efﬁcient  we also add a shortcut connection in
our MLP  which directly links the input to the output. In the second stage of training  we keep all
backbone weights frozen and only train the meta-parameters θ.

4.3 Evaluation metrics

We consider the following evaluation metrics: 1) overall accuracy on individual query sets and the
joint query set (“Base”  “Novel”  and “Both”); and 2) decrease in performance caused by joint
prediction within the base and novel classes  considered separately (“∆a” and “∆b”). Finally we take
the average ∆ = 1

2 (∆a + ∆b) as a key measure of the overall decrease in accuracy.

4.4 Comparisons

We implemented and compared to three methods. First  we adapted Prototypical Networks [33]
to incremental few-shot settings. For each base class we store a base representation  which is the
average representation (prototype) over all images belonging to the base class. During the few-shot
learning stage  we again average the representation of the few-shot classes and add them to the bank
of base representations. Finally  we retrieve the nearest neighbor by comparing the representation of
a test image with entries in the representation store. In summary  both Wa and Wb are stored as the
average representation of all images seen so far that belong to a certain class. We also compare to the
following methods:
• Weights Imprinting (“Imprint”) [26]:
the base weights Wa are learned regularly through
• Learning without Forgetting (“LwoF”) [9]: Similar to [26]  Wb are computed using prototypical
averaging. In addition  Wa is ﬁnetuned during episodic meta-learning. We implemented the most
advanced variants proposed in the paper  which involves a class-wise attention mechanism. This
model is the previous state-of-the-art method on incremental few-shot learning  and has better
performance compared to other low-shot models [37  10].

supervised pre-training  and Wb are computed using prototypical averaging.

4.5 Results

We ﬁrst evaluate our vanilla approach on the standard few-shot classiﬁcation benchmark where no
base classes are present in the query set. Our vanilla model consists of a pretrained CNN and a
single-layer logistic regression with weight decay learned from scratch; this model performs on-par

7

Table 4: Ablation studies on mini-ImageNet

Table 5: Ablation studies on tiered-ImageNet

1-shot

Acc. ↑

5-shot

Acc. ↑

1-shot

LR

52.74 ± 0.24
53.63 ± 0.30
LR +S
55.31 ± 0.32
LR +A
49.36 ± 0.29
MLP
54.46 ± 0.31
MLP +S
MLP +A 54.95 ± 0.30

Acc. ↑

∆ ↓
-13.95
-12.53
-11.72
-16.78
-11.74
-11.84

∆ ↓
-10.44
-6.88
-6.07
-10.61
-6.28
6.11
“+S” stands for static attractors  and “+A” for attention attractors.

48.84 ± 0.23
55.36 ± 0.32
LR +S
55.98 ± 0.32
LR +A
41.22 ± 0.35
MLP
56.16 ± 0.32
MLP +S
MLP +A 56.11 ± 0.33

60.34 ± 0.20
62.50 ± 0.30
63.00 ± 0.29
60.85 ± 0.29
62.79 ± 0.31
63.04 ± 0.30

∆ ↓
-13.60
-11.29
-10.80
-12.62
-10.77
-10.66

LR

5-shot

Acc. ↑

62.08 ± 0.20
65.53 ± 0.30
65.58 ± 0.29
62.70 ± 0.31
65.80 ± 0.31
65.52 ± 0.31

∆ ↓
-8.00
-4.68
-4.39
-7.44
-4.58
-4.48

Figure 2: Learning the proposed model using truncated BPTT vs. RBP. Models are evaluated with
1-shot (left) and 5-shot (right) 64+5-way episodes  with various number of gradient descent steps.

with other competitive meta-learning approaches (1-shot 55.40 ± 0.51  5-shot 70.17 ± 0.46). Note
that our model uses the same backbone architecture as [21] and [9]  and is directly comparable
with their results. Similar ﬁndings of strong results using simple logistic regression on few-shot
classiﬁcation benchmarks are also recently reported in [6]. Our full model has similar performance
as the vanilla model on pure few-shot benchmarks  and the full table is available in Supp. Materials.
Next  we compare our models to other methods on incremental few-shot learning benchmarks in
Tables 2 and 3. On both benchmarks  our best performing model shows a signiﬁcant margin over
the prior works that predict the prototype representation without using an iterative optimization
[33  26  9].

4.6 Ablation studies

with a weight decay regularizer.

To understand the effectiveness of each part of the proposed model  we consider the following
variants:
• Vanilla (“LR  MLP”) optimizes a logistic regression or an MLP network at each few-shot episode 
• Static attractor (“+S”) learns a ﬁxed attractor center u and attractor slope γ for all classes.
• Attention attractor (“+A”) learns the full attention attractor model. For MLP models  the weights
below the ﬁnal layer are controlled by attractors predicted by the average representation across all
the episodes. fφ is an MLP with one hidden layer of 50 units.

Tables 4 and 5 shows the ablation experiment results. In all cases  the learned regularization function
shows better performance than a manually set weight decay constant on the classiﬁer network  in
terms of both jointly predicting base and novel classes  as well as less degradation from individual
prediction. On mini-ImageNet  our attention attractors have a clear advantage over static attractors.
Formulating the classiﬁer as an MLP network is slightly better than the linear models in our
experiments. Although the ﬁnal performance is similar  our RBP-based algorithm have the ﬂexibility
of adding the fast episodic model with more capacity. Unlike [4]  we do not rely on an analytic form
of the gradients of the optimization process.

Comparison to truncated BPTT (T-BPTT) An alternative way to learn the regularizer is to unroll
the inner optimization for a ﬁxed number of steps in a differentiable computation graph  and then
back-propagate through time. Truncated BPTT is a popular learning algorithm in many recent
meta-learning approaches [2  27  7  34  3]. Shown in Figure 2  the performance of T-BPTT learned

8

0255075100125150175200Episodic Training Steps48.050.052.054.056.01-Shot Acc. (%)T-BPTT 20 StepsT-BPTT 50 StepsRBP0255075100125150175200Episodic Training Steps56.058.060.062.064.066.05-Shot Acc. (%)(a) Ours

(b) LwoF [9]

Figure 3: Visualization of a 5-shot 64+5-way episode using PCA. Left: Our attractor model learns to
“pull” prototypes (large colored circles) towards base class weights (white circles). We visualize the
trajectories during episodic training; Right: Dynamic few-shot learning without forgetting [9].

models are comparable to ours; however  when solved to convergence at test time  the performance
of T-BPTT models drops signiﬁcantly. This is expected as they are only guaranteed to work well for
a certain number of steps  and failed to learn a good regularizer. While an early-stopped T-BPTT
model can do equally well  in practice it is hard to tell when to stop; whereas for the RBP model 
doing the full episodic training is very fast since the number of support examples is small.

Visualization of attractor dynamics We
visualize attractor dynamics in Figure 3. Our
learned attractors pulled the fast weights close
towards the base class weights. In comparison 
[9] only modiﬁes the prototypes slightly.

Varying the number of base classes While
the framework proposed in this paper cannot be
directly applied on class-incremental continual
learning  as there is no module for memory
consolidation  we can simulate the continual
learning process by varying the number of base
classes  to see how the proposed models are
affected by different stages of continual learning.
Figure 4 shows that the learned regularizers
consistently improve over baselines with weight
decay only. The overall accuracy increases from
50 to 150 classes due to better representations on the backbone network  and drops at 200 classes due
to a more challenging classiﬁcation task.

Figure 4: Results on tiered-ImageNet with {50 
100  150  200} base classes.

5 Conclusion

Incremental few-shot learning  the ability to jointly predict based on a set of pre-deﬁned concepts
as well as additional novel concepts  is an important step towards making machine learning models
more ﬂexible and usable in everyday life. In this work  we propose an attention attractor model 
which regulates a per-episode training objective by attending to the set of base classes. We show that
our iterative model that solves the few-shot objective till convergence is better than baselines that do
one-step inference  and that recurrent back-propagation is an effective and modular tool for learning
in a general meta-learning setting  whereas truncated back-propagation through time fails to learn
functions that converge well. Future directions of this work include sequential iterative learning of
few-shot novel concepts  and hierarchical memory organization.

Acknowledgment Supported by NSERC and the Intelligence Advanced Research Projects
Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number
D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors and should not be interpreted as necessarily
representing the ofﬁcial policies or endorsements  either expressed or implied  of IARPA  DoI/IBC 
or the U.S. Government.

9

fast weightsattractorsbase class weightsprototypesmodified prototypesattended base classesbase class weightsprototypes50100150200Number of Base Classes485052545658Accuracy (%)LRLR +SLR +AMLPMLP +SMLP +AReferences
[1] L. B. Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial
environment. In Proceedings of the 1st International Conference on Neural Networks  volume 2 
pages 609–618. IEEE  1987.

[2] M. Andrychowicz  M. Denil  S. Gomez  M. W. Hoffman  D. Pfau  T. Schaul  and N. de Freitas.
Learning to learn by gradient descent by gradient descent. In Advances in Neural Information
Processing Systems 29 (NIPS)  2016.

[3] Y. Balaji  S. Sankaranarayanan  and R. Chellappa. Metareg: Towards domain generalization
using meta-regularization. In Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems (NeurIPS)  2018.

[4] L. Bertinetto  J. F. Henriques  P. H. S. Torr  and A. Vedaldi. Meta-learning with differentiable

closed-form solvers. CoRR  abs/1805.08136  2018.

[5] F. M. Castro  M. Mar´ın-Jim´enez  N. Guil  C. Schmid  and K. Alahari. End-to-end incremental

learning. In European Conference on Computer Vision (ECCV)  2018.

[6] W. Chen  Y. Liu  Z. Kira  Y. F. Wang  and J. Huang. A closer look at few-shot classiﬁcation. In

Proceedings of the 7th International Conference on Learning Representations (ICLR)  2019.

[7] C. Finn  P. Abbeel  and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning (ICML) 
2017.

[8] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In Proceedings of the

6th International Conference on Learning Representations (ICLR)  2018.

[9] S. Gidaris and N. Komodakis. Dynamic few-shot visual learning without forgetting.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2018.

[10] B. Hariharan and R. B. Girshick. Low-shot visual recognition by shrinking and hallucinating
features. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) 
2017.

[11] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 770–778  2016.

[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning (ICML)  2015.

[13] R. Kemker and C. Kanan. Fearnet: Brain-inspired model for incremental learning.
Proceedings of 6th International Conference on Learning Representations (ICLR)  2018.

In

[14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the

3rd International Conference on Learning Representations (ICLR)  2015.

[15] J. Kirkpatrick  R. Pascanu  N. Rabinowitz  J. Veness  G. Desjardins  A. A. Rusu  K. Milan 
J. Quan  T. Ramalho  A. Grabska-Barwinska  et al. Overcoming catastrophic forgetting in neural
networks. Proceedings of the national academy of sciences (PNAS)  page 201611835  2017.

[16] G. Koch  R. Zemel  and R. Salakhutdinov. Siamese neural networks for one-shot image

recognition. In ICML Deep Learning Workshop  volume 2  2015.

[17] B. M. Lake  R. Salakhutdinov  J. Gross  and J. B. Tenenbaum. One shot learning of simple
visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science Society
(CogSci)  2011.

[18] R. Liao  Y. Xiong  E. Fetaya  L. Zhang  K. Yoon  X. Pitkow  R. Urtasun  and R. S. Zemel.
Reviving and improving recurrent back-propagation. In Proceedings of the 35th International
Conference on Machine Learning (ICML)  2018.

[19] J. L. McClelland  B. L. McNaughton  and R. C. O’reilly. Why there are complementary
learning systems in the hippocampus and neocortex: insights from the successes and failures of
connectionist models of learning and memory. Psychological review  102(3):419  1995.

[20] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation  volume 24  pages
109–165. Elsevier  1989.

[21] N. Mishra  M. Rohaninejad  X. Chen  and P. Abbeel. A simple neural attentive meta-learner. In

Proceedings of the 6th International Conference on Learning Representations (ICLR)  2018.

10

[22] M. C. Mozer. Attractor networks. The Oxford companion to consciousness  pages 86–89  2009.
[23] D. K. Naik and R. Mammone. Meta-neural networks that learn by learning. In Proceedings of

the IEEE International Joint Conference on Neural Networks (IJCNN)  1992.

[24] C. V. Nguyen  Y. Li  T. D. Bui  and R. E. Turner. Variational continual learning. In Proceedings

of the 6th International Conference on Learning Representations (ICLR)  2018.

[25] F. J. Pineda. Generalization of back-propagation to recurrent neural networks. Physical review

letters  59(19):2229  1987.

[26] H. Qi  M. Brown  and D. G. Lowe. Low-shot learning with imprinted weights. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[27] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proceedings of

the 5th International Conference on Learning Representations (ICLR)  2017.

[28] S. Rebufﬁ  A. Kolesnikov  G. Sperl  and C. H. Lampert.

icarl: Incremental classiﬁer and
In Proceedings of the IEEE Conference on Computer Vision and

representation learning.
Pattern Recognition (CVPR)  2017.

[29] M. Ren  E. Triantaﬁllou  S. Ravi  J. Snell  K. Swersky  J. B. Tenenbaum  H. Larochelle  and
R. S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. In Proceedings of 6th
International Conference on Learning Representations (ICLR)  2018.

[30] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy 
A. Khosla  M. Bernstein  et al. Imagenet large scale visual recognition challenge. International
Journal of Computer Vision  115(3):211–252  2015.

[31] A. Santoro  S. Bartunov  M. Botvinick  D. Wierstra  and T. Lillicrap. One-shot learning with
memory-augmented neural networks. In Proceedings of the 33rd International Conference on
Machine Learning (ICML)  2016.

[32] J. Schmidhuber. Evolutionary principles in self-referential learning  or on learning how to
learn: The meta-meta-... hook. Diplomarbeit  Technische Universit¨at M¨unchen  M¨unchen 
1987.

[33] J. Snell  K. Swersky  and R. S. Zemel. Prototypical networks for few-shot learning. In Advances

in Neural Information Processing Systems 30 (NIPS)  2017.

[34] P. Sprechmann  S. M. Jayakumar  J. W. Rae  A. Pritzel  A. P. Badia  B. Uria  O. Vinyals 
D. Hassabis  R. Pascanu  and C. Blundell. Memory-based parameter adaptation. In Proceedings
of 6th International Conference on Learning Representations (ICLR)  2018.

[35] S. Thrun. Lifelong learning algorithms. In Learning to learn  pages 181–209. Springer  1998.
[36] O. Vinyals  C. Blundell  T. Lillicrap  K. Kavukcuoglu  and D. Wierstra. Matching networks for

one shot learning. In Advances in Neural Information Processing Systems 29 (NIPS)  2016.

[37] Y. Wang  R. B. Girshick  M. Hebert  and B. Hariharan. Low-shot learning from imaginary data.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2018.

[38] P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE  78(10):1550–1560  1990.

[39] R. J. Williams and J. Peng. An efﬁcient gradient-based algorithm for on-line training of recurrent

network trajectories. Neural computation  2(4):490–501  1990.

[40] Y. Wu and K. He. Group normalization. In European Conference on Computer Vision (ECCV) 

2018.

[41] Y. Wu  M. Ren  R. Liao  and R. B. Grosse. Understanding short-horizon bias in stochastic meta-
optimization. In Proceedings of the 6th International Conference on Learning Representations
(ICLR)  2018.

[42] R. S. Zemel and M. C. Mozer. Localist attractor networks. Neural Computation  13(5):1045–

1064  2001.

[43] C. Zhu  R. H. Byrd  P. Lu  and J. Nocedal. Algorithm 778: L-bfgs-b: Fortran subroutines
for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software
(TOMS)  23(4):550–560  1997.

11

,Mengye Ren
Renjie Liao
Ethan Fetaya
Richard Zemel