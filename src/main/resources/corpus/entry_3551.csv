2017,Multi-Armed Bandits with Metric Movement Costs,We consider the non-stochastic Multi-Armed Bandit problem in a setting where there is a fixed and known metric on the action space that determines a cost for switching between any pair of actions. The loss of the online learner has two components: the first is the usual loss of the selected actions  and the second is an additional loss due to switching between actions.  Our main contribution gives a tight characterization of the expected minimax regret in this setting  in terms of a complexity measure $\mathcal{C}$ of the underlying metric which depends on its covering numbers. In finite metric spaces with $k$ actions  we give an efficient algorithm that achieves regret of the form $\widetilde(\max\set{\mathcal{C}^{1/3}T^{2/3} \sqrt{kT}})$  and show that this is the best possible. Our regret bound generalizes previous known regret bounds for some special cases: (i) the unit-switching cost regret $\widetilde{\Theta}(\max\set{k^{1/3}T^{2/3} \sqrt{kT}})$ where $\mathcal{C}=\Theta(k)$  and (ii) the interval metric with regret $\widetilde{\Theta}(\max\set{T^{2/3} \sqrt{kT}})$ where $\mathcal{C}=\Theta(1)$.  For infinite metrics spaces with Lipschitz loss functions  we derive a tight regret bound of $\widetilde{\Theta}(T^{\frac{d+1}{d+2}})$ where $d \ge 1$ is the Minkowski dimension of the space  which is known to be tight even when there are no switching costs.,Multi-Armed Bandits with Metric Movement Costs

Tomer Koren
Google Brain

Roi Livni

Princeton University

Yishay Mansour

Tel Aviv University and Google

tkoren@google.com

rlivni@cs.princeton.edu

mansour@cs.tau.ac.il

Abstract

We consider the non-stochastic Multi-Armed Bandit problem in a setting where
there is a ﬁxed and known metric on the action space that determines a cost for
switching between any pair of actions. The loss of the online learner has two
components: the ﬁrst is the usual loss of the selected actions  and the second is an
additional loss due to switching between actions. Our main contribution gives a
tight characterization of the expected minimax regret in this setting  in terms of
a complexity measure C of the underlying metric which depends on its covering
numbers. In ﬁnite metric spaces with k actions  we give an eﬃcient algorithm
kT})  and show that this is
the best possible. Our regret bound generalizes previous known regret bounds
kT})
kT}) where
C = Θ(1). For inﬁnite metrics spaces with Lipschitz loss functions  we derive a
d+2) where d ≥ 1 is the Minkowski dimension of the
space  which is known to be tight even when there are no switching costs.

that achieves regret of the form (cid:101)O(max{C1/3T2/3
for some special cases: (i) the unit-switching cost regret(cid:101)Θ(max{k1/3T2/3
where C = Θ(k)  and (ii) the interval metric with regret(cid:101)Θ(max{T2/3
tight regret bound of(cid:101)Θ(T d+1

√

√

 

√

 

 

t=1 (cid:96)t(i).

Introduction

the expected diﬀerence between her loss (cid:80)T
mini∈K(cid:80)T

1
Multi-Armed Bandit (MAB) is perhaps one of the most well studied model for learning that allows to
incorporate settings with limited feedback. In its simplest form  MAB can be thought of as a game
between a learner and an adversary: At ﬁrst  the adversary chooses an arbitrary sequence of losses
(cid:96)1  . . .   (cid:96)T (possibly adversarially). Then  at each round the learner chooses an action it from a ﬁnite
set of actions K. At the end of each round  the learner gets to observe her loss (cid:96)t(it)  and only the loss
of her chosen action. The objective of the learner is to minimize her (external) regret  deﬁned as
t=1 (cid:96)t(it)  and the loss of the best action in hindsight  i.e. 
One simpliﬁcation of the MAB is that it assumes that the learner can switch between actions without
any cost  this is in contrast to online algorithms that maintain a state and have a cost of switching
between states. One simple intermediate solution is to add further costs to the learner that penalize
movements between actions. (Since we compare the learner to the single best action  the adversary
has no movement and hence no movement cost.) This approach has been studied in the MAB with
unit switching costs [2  12]  where the learner is not only penalized for her loss but also pays a
unit cost for any time she switches between actions. This simple penalty implicitly advocates the
construction of algorithms that avoid frequent ﬂuctuation in their decisions. Regulating switching has
been successfully applied to many interesting instances such as buﬀering problems [16]  limited-delay
lossy coding [19] and dynamic pricing with patient buyers [15].
The unit switching cost assumes that any pair of actions have the same cost  which in many scenarios
is far from true. For example  consider an ice-cream vendor on a beach  where his actions are to select
a location and price. Clearly  changing location comes at a cost  while changing prices might come
with no cost. In this case we can deﬁne a interval metric (the coast line) and the movement cost is the
distance. A more involved case is a hot-dog vendor in Manhattan  which needs to select a location

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

√

 

and price. Again  it makes sense to charge a switching cost between locations according to their
distance  and in this case the Manhattan-distance seems the most appropriate. Such settings are at the
core of our model for MAB with movement cost. The authors of [24] considered a MAB problem
equipped with an interval metric  i.e  the actions are [0  1] and the movement cost is the distance
between the actions. They proposed a new online algorithm  called the Slowly Moving Bandit (SMB)
algorithm  that achieves optimal regret bound for this setting  and applied it to a dynamic pricing
problem with patient buyers to achieve a new tight regret bound.
The objective of this paper is to handle general metric spaces  both ﬁnite and inﬁnite. We show
how to generalize the SMB algorithm and its analysis to design optimal moving-cost algorithms
for any metric space over ﬁnite decision space. Our main result identiﬁes an intrinsic complexity
measure of the metric space  which we call the covering/packing complexity  and give a tight
characterization of the expected movement regret in terms of the complexity of the underlying metric.
In particular  in ﬁnite metric spaces of complexity C with k actions  we give a regret bound of the
kT}) and present an eﬃcient algorithm that achieves it. We also give a
 

form (cid:101)O(max{C1/3T2/3
matching(cid:101)Ω(max{C1/3T2/3

kT}) lower bound that applies to any metric with complexity C.

√

inﬁnite case. Speciﬁcally  we give an upper bound on the regret of(cid:101)O(T d+1

We extend out results to general continuous metric spaces. For such a settings we clearly have to make
some assumption about the losses  and we make the rather standard assumption that the losses are
Lipchitz with respect to the underlying metric. In this setting our results depend on a quite diﬀerent
complexity measures: the upper and lower Minkowski dimensions of the space  thus exhibiting a
phase transition between the ﬁnite case (that corresponds to Minkowski dimension zero) and the
d+2) where d ≥ 1 is the upper
Minkowski dimension. When the upper and lower Minkowski dimensions coincide—which is the
case in many natural spaces  such as normed vector spaces—the latter bound matches a lower bound
of [10] that holds even when there are no switching costs. Thus  a surprising implication of our result
is that in inﬁnite actions spaces (of bounded Minkowski dimension)  adding movement costs do not
add to the complexity of the MAB problem!
Our approach extends the techniques of [24] for the SMB algorithm  which was designed to optimize
over an interval metric  which is equivalent to a complete binary Hierarchally well-Separated Tree
(HST) metric space. By carefully balancing and regulating its sampling distributions  the SMB
algorithm avoids switching between far-apart nodes in the tree and possibly incurring large movement
costs with respect to the associated metric. We show that the SMB regret guarantees are much more
general than just binary balanced trees  and give an analysis of the SMB algorithm when applied to
general HSTs. As a second step  we show that a rich class of trees  on which the SMB algorithm can
be applied  can be used to upper-bound any general metric. Finally  we reduce the case of an inﬁnite
metric space to the ﬁnite case via simple discretization  and show that this reduction gives rise to
the Minkowski dimension as a natural complexity measure. All of these contractions turn out to be
optimal (up to logarithmic factors)  as demonstrated by our matching lower bounds.

1.1 Related Work

that guarantee a regret of(cid:101)O(√
(cid:101)Ω(T) switches between actions are expected (see [12]).

Perhaps the most well known classical algorithm for non-stochastic bandit is the Exp3 Algorithm [4]
kT) without movement costs. However  for general MAB algorithms
there are no guarantees for slow movement between actions. In fact  it is known that in a worst case

A simple case of MAB with movement cost is the uniform metric  i.e.  when the distance between any
two actions is the same. This setting has seen intensive study  both in terms of analyzing optimal
regret rates [2  12]  as well as applications [16  19  15]. Our main technical tools for achieving lower
bounds is through the lower bound of Dekel et al. [12] that achieve such bound for this special case.
The general problem of bandits with movement costs has been ﬁrst introduced in [24]  where the
authors gave an eﬃcient algorithm for a 2-HST binary balanced tree metric  as well as for evenly
spaced points on the interval. The main contribution of this paper is a generalization of these results
to general metric spaces.
There is a vast and vigorous study of MAB in continuous spaces [23  11  5  10  32]. These works
relate the change in the payoﬀ to the change in the action. Speciﬁcally  there has been a vast research
on Lipschitz MAB with stochastic payoﬀs [22  29  30  21  26]  where  roughly  the expected reward
is Lipschitz. For applying our results in continuous spaces we too need to assume Lipschitz losses 

2

however  our metric deﬁnes also the movement cost between actions and not only relates the losses of
similar actions. Our general ﬁndings is that in Euclidean spaces  one can achieve the same regret
bounds when movement cost is applied. Thus  the SMB algorithm can achieve the optimal regret rate.
One can model our problem as a deterministic Markov Decision Process (MDP)  where the states
are the MAB actions and in every state there is an action to move the MDP to a given state (which
correspond to switching actions). The payoﬀ would be the payoﬀ of the MAB action associated with
the state plus the movement cost to the next state. The work of Ortner [28] studies deterministic MDP
where the payoﬀs are stochastic  and also allows for a ﬁxed uniform switching cost. The work of
Even-Dar et al. [13] and it extensions [27  33] studies a MDP where the payoﬀs are adversarial but
there is full information of the payoﬀs. Latter this work was extended to the bandit model by Neu et al.
[27]. This line of works imposes various assumptions regarding the MDP and the benchmark policies 
speciﬁcally  that the MDP is “mixing” and that the policies considered has full support stationary
distributions  assumptions that clearly fail in our very speciﬁc setting.
Bayesian MAB  such as in the Gittins index (see [17])  assume that the payoﬀs are from some
stochastic process. It is known that when there are switching costs then the existence of an optimal
index policy is not guaranteed [6]. There have been some works on special cases with a ﬁxed uniform
switching cost [1  3]. The most relevant work is that of Guha and Munagala [18] which for a general
metric over the actions gives a constant approximation oﬀ-line algorithm. For a survey of switching
costs in this context see [20].
The MAB problem with movement costs is related to the literature on online algorithms and the
competitive analysis framework [8]. A prototypical online problem is the Metrical Task System
(MTS) presented by Borodin et al. [9]. In a metrical task system there are a collection of states and
a metric over the states. Similar to MAB  the online algorithm at each time step moves to a state 
incurs a movement cost according to the metric  and suﬀers a loss that corresponds to that state.
However  unlike MAB  in an MTS the online algorithm is given the loss prior to selecting the new
state. Furthermore  competitive analysis has a much more stringent benchmark: the best sequence of
actions in retrospect. Like most of the regret minimization literature  we use the best single action in
hindsight as a benchmark  aiming for a vanishing average regret.
One of our main technical tools is an approximation from above of a metric via a Metric Tree (i.e. 
2-HST). k-HST metrics have been vastly studied in the online algorithms starting with [7]. The main
goal is to derive a simpler metric representation (using randomized trees) that will both upper and
lower bound the given metric. The main result is to show a bound of O(log n) on the expected stretch
of any edge  and this is also the best possible [14]. It is noteworthy that for bandit learning  and in
contrast with these works  an upper bound over the metric suﬃces to achieve optimal regret rate. This
is since in online learning we compete against the best static action in hindsight  which does not move
at all and hence has zero movement cost. In contrast  in a MTS  where one compete against the best
dynamic sequence of actions  one needs both an upper a lower bound on the metric.

2 Problem Setup and Background
In this section we recall the setting of Multi-armed Bandit with Movement Costs introduced in [24] 
and review the necessary background required to state our main results.

2.1 Multi-armed Bandits with Movement Costs
In the Multi-armed Bandits (MAB) with Movement Costs problem  we consider a game between an
online learner and an adversary continuing for T rounds. There is a set K  possibly inﬁnite  of actions
(or “arms”) that the learner can choose from. The set of actions is equipped with a ﬁxed and known
metric ∆ that determines a cost ∆(i  j) ∈ [0  1] for moving between any pair of actions i  j ∈ K.
Before the game begins  an adversary ﬁxes a sequence (cid:96)1  . . .   (cid:96)T : K (cid:55)→ [0  1] of loss functions
assigning loss values in [0  1] to actions in K (in particular  we assume an oblivious adversary). Then 
on each round t = 1  . . .   T  the learner picks an action it ∈ K  possibly at random. At the end of each
round t  the learner gets to observe her loss (namely  (cid:96)t(it)) and nothing else. In contrast with the
standard MAB setting  in addition to the loss (cid:96)t(it) the learner suﬀers an additional cost due to her
movement between actions  which is determined by the metric and is equal to ∆(it  it−1). Thus  the
total cost at round t is given by (cid:96)t(it) + ∆(it−1  it).

3

The goal of the learner  over the course of T rounds of the game  is to minimize her expected
movement-regret  which is deﬁned as the diﬀerence between her (expected) total costs and the total
costs of the best ﬁxed action in hindsight (that incurs no movement costs); namely  the movement
regret with respect to a sequence (cid:96)1:T of loss vectors and a metric ∆ equals

RegretMC((cid:96)1:T  ∆) = (cid:69)

(cid:96)t(it) +

∆(it  it−1)

(cid:34) T(cid:88)

t=1

T(cid:88)

t=2

(cid:35)

T(cid:88)

t=1

− min
i∈K

(cid:96)t(i) .

Here  the expectation is taken with respect to the learner’s randomization in choosing the actions
i1  . . .   iT ; notice that  as we assume an oblivious adversary  the loss functions (cid:96)t are deterministic and
cannot depend on the learner’s randomization.

2.2 Basic Deﬁnitions in Metric Spaces
We recall basic notions in metric space that govern the regret in the MAB with movement costs
setting. Throughout we assume a bounded metric space (K  ∆)  where for normalization we assume
∆(i  j) ∈ [0  1] for all i  j ∈ K. Given a point i ∈ K we will denote by B(i) = { j ∈ K : ∆(i  j) ≤ }
the ball of radius  around i.
The following deﬁnitions are standard.
Deﬁnition 1 (Packing numbers). A subset P ⊂ K in a metric space (K  ∆) is an -packing if the sets
{B(i)}i∈P are disjoint sets. The -packing number of ∆  denoted N
 (∆)  is the maximum cardinality
of any -packing of K.
Deﬁnition 2 (Covering numbers). A subset C ⊂ K in a metric space (K  ∆) is an -covering if
K ⊆ ∪i∈C B(i). The -covering number of K  denoted Nc
(∆)  is the minimum cardinality of any
-covering of K.

p

Tree metrics and HSTs. We recall the notion of a tree metric  and in particular  a metric induced
by an Hierarchically well-Separated (HST) Tree; see [7] for more details. Any weighted tree deﬁnes
a metric over the vertices  by considering the shortest path between each two nodes. An HST tree
(2-HST tree  to be precise) is a rooted weighted tree such that: 1) the edge weight from any node
to each of its children is the same and 2) the edge weight along any path from the root to a leaf are
decreasing by a factor 2 per edge. We will also assume that all leaves are of the same depth in the tree
(this does not imply that the tree is complete).
Given a tree T we let depth(T) denote its height  which is the maximal length of a path from any leaf
to the root. Let level(v) be the level of a node v ∈ T   where the level of the leaves is 0 and the level of
the root is depth(T). Given nodes u  v ∈ T   let LCA(u  v) be their least common ancestor node in T .
The metric which we next deﬁne is equivalent (up to a constant factor) to standard tree–metric induced
over the leaves by an HST. By a slight abuse of terminology  we will call it HST metric:
Deﬁnition 3 (HST metric). Let K be a ﬁnite set and let T be a tree whose leaves are at the same
depth and are indexed by elements of K. Then the HST metric ∆T over K induced by the tree T is
deﬁned as follows:

∆T(i  j) =

2level(LCA(i  j))

2depth(T)

∀ i  j ∈ K.

For a HST metric ∆T  observe that the packing number and covering number are simple to characterize:
for all 0 ≤ h < depth(T) we have that for  = 2h−H 

 (∆T) =(cid:12)(cid:12){v ∈ T : level(v) = h}(cid:12)(cid:12).

p

(∆T) = N
Nc

Complexity measures for ﬁnite metric spaces. We next deﬁne the two notions of complexity that 
as we will later see  governs the complexity of MAB with metric movement costs.
Deﬁnition 4 (covering complexity). The covering complexity of a metric space (K  ∆) denoted Cc(∆)
is given by

Cc(∆) = sup
0< <1

·Nc

(∆).

4

Deﬁnition 5 (packing complexity). The packing complexity of a metric space (K  ∆) denoted Cp(∆)
is given by

Cp(∆) = sup
0< <1

·N

p

 (∆).

For a HST metric  the two complexity measures coincide as its packing and covering numbers are the
same. Therefore  for a HST metric ∆T we will simply denote the complexity of (K  ∆T) as C(T). In
/2(∆) for all  > 0. Thus  for a general
fact  it is known that in any metric space N
metric space we obtain that

 (∆) ≤ Nc

p

(∆) ≤ Np
Cp(∆) ≤ Cc(∆) ≤ 2Cp(∆).

(1)

Complexity measures for inﬁnite metric spaces. For inﬁnite metric spaces  we require the
following deﬁnition.
Deﬁnition 6 (Minkowski dimensions). Let (K  ∆) be a bounded metric space. The upper Minkowski
dimension of (K  ∆)  denoted D(∆)  is deﬁned as
p
log N
log(1/) = lim sup
→0

D(∆) = lim sup
→0

(∆)
log Nc
log(1/) .

 (∆)

Similarly  the lower Minkowski dimension is denoted by D(∆) and is deﬁned as

D(∆) = lim inf
→0

p

 (∆)

log N
log(1/) = lim inf
→0

(∆)
log Nc
log(1/) .

We refer to [31] for more background on the Minkowski dimensions and related notions in metric
spaces theory.

3 Main Results
We now state the main results of the paper  which give a complete characterization of the expected
regret in the MAB with movement costs problem.

3.1 Finite Metric Spaces
The following are the main results of the paper.
Theorem 7 (Upper Bound). Let (K  ∆) be a ﬁnite metric space over |K| = k elements with diameter
≤ 1 and covering complexity Cc = Cc(∆). There exists an algorithm such that for any sequence of
loss functions (cid:96)1  . . .   (cid:96)T guarantees that

Theorem 8 (Lower Bound). Let (K  ∆) be a ﬁnite metric space over |K| = k elements with diameter
≥ 1 and packing complexity Cp = Cp(∆). For any algorithm there exists a sequence (cid:96)1  . . .   (cid:96)T of loss
functions such that

RegretMC((cid:96)1:T  ∆) =(cid:101)O(cid:0) max(cid:8)C1/3
RegretMC((cid:96)1:T  ∆) =(cid:101)Ω(cid:0) max(cid:8)C1/3

c T2/3

 

p T2/3

 

kT(cid:9)(cid:1).
kT(cid:9)(cid:1).

√

√

For the detailed proofs  see the full version of the paper [25]. Recalling Eq. (1)  we see that the regret
bounds obtained in Theorems 7 and 8 are matching up to logarithmic factors. Notice that the tightness
is achieved per instance; namely  for any given metric we are able to fully characterize the regret’s
rate of growth as a function of the intrinsic properties of the metric. (In particular  this is substantially
stronger than demonstrating a speciﬁc metric for which the upper bound cannot be improved.) Note
that for the lower bound statement in Theorem 8 we require that the diameter of K is bounded away
from zero  where for simplicity we assume a constant bound of 1. Such an assumption is necessary
to avoid degenerate metrics. Indeed  when the diameter is very small  the problem reduces to the
standard MAB setting without any additional costs and we obtain a regret rate of Ω(√
Notice how the above results extend known instances of the problem from previous work: for uniform
movement costs (i.e.  unit switching costs) over K = {1  . . .   k} we have Cc = Θ(k)  so that the

kT).

5

obtain bound is(cid:101)Θ(max{k1/3T2/3
balanced tree with k leaves  we have Cc = Θ(1) and the resulting bound is(cid:101)Θ(max{T2/3

kT})  which recovers the results in [2  12]; and for a 2-HST binary
kT})  which

is identical to the bound proved in [24].
The 2-HST regret bound in [24] was primarily used to obtain regret bounds for the action space
K = [0  1]. In the next section we show how this technique is extended for inﬁnite metric space to
obtain regret bounds that depend on the dimensionality of the action space.

√

√

 

 

Inﬁnite Metric Spaces

3.2
When (K  ∆) is an inﬁnite metric space  without additional constraints on the loss functions  the
problem becomes ill-posed with a linear regret rate  even without movement costs. Therefore  one
has to make additional assumptions on the loss functions in order to achieve sublinear regret. One
natural assumption  which is common in previous work  is to assume that the loss functions (cid:96)1  . . .   (cid:96)T
are all 1-Lipschitz with respect to the metric ∆. Under this assumption  we have the following result.
Theorem 9. Let (K  ∆) be a metric space with diameter ≤ 1 and upper Minkowski dimension
d = D(∆)  such that d ≥ 1. There exists a strategy such that for any sequence of loss functions
(cid:96)1  . . .   (cid:96)T   which are all 1-Lipschitz with respect to ∆  guarantees that

d+2(cid:1).
RegretMC((cid:96)1:T  ∆) =(cid:101)O(cid:0)T d+1

We refer the full version of the paper [25] for a proof of the theorem. Again  we observe that the above
result extend the case of K = [0  1] where d = 1. Indeed  for Lipschitz functions over the interval a
d+2) is known for MAB in metric spaces with Lipschitz cost
functions—even without movement costs—where d = D(∆) is the lower Minkowski dimension.
Theorem 10 (Bubeck et al. [10]). Let (K  ∆) be a metric space with diameter ≤ 1 and lower Minkowski
dimension d = D(∆)  such that d ≥ 1. Then for any learning algorithm  there exists a sequence
of loss function (cid:96)1  . . .   (cid:96)T   which are all 1-Lipschitz with respect to ∆  such that the regret (without

tight regret bound of(cid:101)Θ(T2/3) was achieved in [24]  which is exactly the bound we obtain above.
We mention that a lower bound of(cid:101)Ω(T d+1
movement costs) is(cid:101)Ω(cid:0)T d+1
d+2(cid:1).

In many natural metric spaces in which the upper and lower Minkowski dimensions coincide (e.g. 
normed spaces)  the bound of Theorem 9 is tight up to logarithmic factors in T. In particular  and
quite surprisingly  we see that the movement costs do not add to the regret of the problem!
It is important to note that Theorem 9 holds only for metric spaces whose (upper) Minkowski
demonstrated in Section 3.1 above  a O(√
dimension is at least 1. Indeed  ﬁnite metric spaces are of Minkowski dimension zero  and as we
T) regret bound is not achievable. Finite matric spaces are
associated with a complexity measure which is very diﬀerent from the Minkowski dimension (i.e. 
the covering/packing complexity). In other words  we exhibit a phase transition between dimension
d = 0 and d ≥ 1 in the rate of growth of the regret induced by the metric.

4 Algorithms
In this section we turn to prove Theorem 7. Our strategy is much inspired by the approach in [24] 
and we employ a two-step approach: First  we consider the case that the metric is a HST metric; we
then turn to deal with general metrics  and show how to upper-bound any metric with a HST metric.

4.1 Tree Metrics: The Slowly-Moving Bandit Algorithm
In this section we analyze the simplest case of the problem  in which the metric ∆ = ∆T is induced
by a HST tree T (whose leaves are associated with actions in K). In this case  our main tool is the
Slowly-Moving Bandit (SMB) algorithm [24]: we demonstrate how it can be applied to general tree
metrics  and analyze its performance in terms of intrinsic properties of the metric.
We begin by reviewing the SMB algorithm. In order to present the algorithm we require few additional
notations. The algorithm receives as input a tree structure over the set of actions K  and its operation
depends on the tree structure. We ﬁx a HST tree T and let H = depth(T). For any level 0 ≤ h ≤ H
and action i ∈ K  let Ah(i) be the set of leaves of T that share a common ancestor with i at level h

6

(recall that level h = 0 is the bottom–most level corresponding to the singletons). In terms of the tree
metric we have that Ah(i) = { j : ∆T(i  j) ≤ 2−H+h}.
The SMB algorithm is presented in Algorithm 1. The algorithm is based on the multiplicative update
method  in the spirit of Exp3 algorithms [4]. Similarly to Exp3  the algorithm computes at each round

t an estimator(cid:101)(cid:96)t to the loss vector (cid:96)t using the single loss value (cid:96)t(it) observed. In addition to being
an (almost) unbiased estimate for the true loss vector  the estimator(cid:101)(cid:96)t used by SMB has the additional

property of inducing slowly-changing sampling distributions pt: This is done by choosing at random
a level ht of the tree to be rebalanced (in terms of the weights maintained by the algorithm): As a
result  the marginal probabilities pt+1(Aht(i)) are not changed at round t.
In turn  and in contrast with Exp3  the algorithm choice of action at round t + 1 is not purely sampled
from pt  but rather conditioned on our last choice of level ht. This is informally justiﬁed by the fact
that pt and pt+1 agree on the marginal distribution of Aht(it)  hence we can think of the level drawn
at round t as if it were drawn subject to pt+1(Aht) = pt(Aht).

Input: A tree T with a set of ﬁnite leaves K  η > 0.
Initialize: H = depth(T)  Ah(i) = B2−H +h(i)  ∀i ∈ K  0 ≤ h ≤ H
Initialize p1 = unif(K)  h0 = H and i0 ∼ p1
For t = 1  . . .   T:

(1) Choose action it ∼ pt(· | Aht−1(it−1))  observe loss (cid:96)t(it)
(2) Choose σt 0  . . .   σt  H−1 ∈ {±1} uniformly at random;
let ht = min{0 ≤ h ≤ H : σt h < 0} where σt  H = −1
(3) Compute vectors ¯(cid:96)t 0  . . .   ¯(cid:96)t  H−1 recursively via

1{it = i}
¯(cid:96)t 0(i) =
pt(i)
and for all h ≥ 1:
¯(cid:96)t h(i) = −1
(cid:101)(cid:96)t =

ln

η

(cid:96)t(it) 

(cid:33)

j∈ Ah(i)

(cid:32) (cid:88)
pt(j)
pt(Ah(i)) e−η(1+σt  h−1) ¯(cid:96)t  h−1(j)
(cid:40) 0
¯(cid:96)t 0 +(cid:80)H−1
h=0 σt h ¯(cid:96)t h
pt(i) e−η(cid:101)(cid:96)t(i)
(cid:80)k
j=1 pt(j) e−η(cid:101)(cid:96)t(j)

if it ∈ Et;
otherwise

∀ i ∈ K

(4) Deﬁne Et = {i : pt(Ah(i)) < 2hη for some 0 ≤ h < H} and set:

(5) Update:

pt+1(i) =

Algorithm 1: The SMB algorithm.

A key observation is that by directly applying SMB to the metric ∆T  we can achieve the following
regret bound:
Theorem 11. Let (K  ∆T) be a metric space deﬁned by a 2-HST T with depth(T) = H and complexity
C(T) = C. Using SMB algorithm we can achieve the following regret bound:

(cid:16)

(cid:112)2HTClog C + H2−HT

(cid:17)

RegretMC((cid:96)1:T  ∆T) = O

H

.

(2)

To show Theorem 11  we adapt the analysis of [24] (that applies only to complete binary HSTs) to
handle more general HSTs. We defer this part of our analysis to the full version of the paper [25] 
since it follows from a technical modiﬁcation of the original proof.
For a tree that is either too deep or too shallow  Eq. (2) may not necessarily lead to a sublinear regret
bound  let alone optimal. The main idea behind achieving optimal regret bound for a general tree  is
to modify it until one of two things happen: Either we have optimized the depth so that the two terms
in the left-hand side of Eq. (2) are of same order: In that case  we will show that one can achieve

7

kT).

(2) One of the following is true:

(b) 2−(H−1)(H − 1)T ≥(cid:112)2H−1(H − 1)CT.

(a) 2HC ≤ k;

2H HCT;

regret rate of order O(C(T)1/3T2/3). If we fail to do that  we show that the ﬁrst term in the left-hand
side is the dominant one  and it will be of order O(√
For trees that are in some sense “well behaved" we have the following Corollary of Theorem 11.
Corollary 12. Let (K  ∆T) be a metric space deﬁned by a tree T over |K| = k leaves with
depth(T) = H and complexity C(T) = C. Assume that T satisﬁes the following:
(1) 2−H HT ≤ √
Then  the SMB algorithm can be used to attain RegretMC((cid:96)1:T  ∆T) =(cid:101)O(cid:0) max(cid:8)C1/3T2/3
SMB to the metric space (K  ∆T(cid:48)) leads to RegretMC((cid:96)1:T  ∆T) =(cid:101)O(cid:0) max(cid:8)C(T)1/3T2/3

The following establishes Theorem 7 for the special case of tree metrics.
Lemma 13. For any tree T and time horizon T  there exists a tree T (cid:48) (over the same set K of k leaves)
that satisﬁes the conditions of Corollary 12  such that ∆T(cid:48) ≥ ∆T and C(T (cid:48)) = C(T). Furthermore 
T (cid:48) can be constructed eﬃciently from T (i.e.  in time polynomial in |K| and T). Hence  applying

kT(cid:9)(cid:1).
kT(cid:9)(cid:1).

√

 

√

 

We refer to [25] for the proofs of both results.

4.2 General Finite Metrics
Finally  we obtain the general ﬁnite case as a corollary of the following.
Lemma 14. Let (K  ∆) be a ﬁnite metric space. There exists a tree metric ∆T over K (with
|K| = k) such that 4∆T  dominates ∆ (i.e.  such that 4∆T(i  j) ≥ ∆(i  j) for all i  j ∈ K) for which
C(T) = O(Cc(∆) log k). Furthermore  T can be constructed eﬃciently.
Proof. Let H be such that the minimal distance in ∆ is larger than 2−H. For each r = 2−1
  . . .   2−H
we let {Br(i{1 r})  . . .   Br(i{mr r})} = Br be a covering of K of size Nc
r(T) log k using balls of radius r.
Note that ﬁnding a minimal set of balls of radius r that covers K is exactly the set cover problem.
Hence  we can eﬃciently approximate it (to within a O(log k) factor) and construct the sets Br.
We now construct a tree graph  whose nodes are associated with the cover balls: The leaves correspond
to singleton balls  hence correspond to the action space. For each leaf i we ﬁnd an action a1(i) ∈ K
such that: i ∈ B2−H +1(a1(i)) ∈ B2−H +1 . If there is more than one  we arbitrarily choose one  and we
connect an edge between i and B2−H +1(a1(i)). We continue in this manner inductively to deﬁne ar(i) for
every a and r < 1: given ar−1(i) we ﬁnd an action ar(i) such that ar−1(i) ∈ B2−H +r(ar(i)) ∈ B2−H +r  
and we connect an edge from B2−H +r−1(ar−1(i)) and B2−H +r(ar(i)).
We now claim that the metric induced by the tree graph dominates up to factor 4 the original metric.
Let i  j ∈ K such that ∆T(i  j) < 2−H+r then by construction there are i  a1(i)  a2(i)  . . . ar(i) and
j  a1(j)  a2(j)  . . . ar(j)  such that ar(i) = ar(j) and for which it holds that ∆(as(i)  as−1(i)) ≤ 2−H+s
and similarly ∆(as(j)  as−1(j)) ≤ 2−H+s for every s ≤ r. Denoting a0(i) = i and a0(j) = j  we have
that

  2−2

∆(as−1(i)  as(i)) +

∆(as−1(j)  as(j))

2−H+s ≤ 2·2−H ·2r+1 ≤ 4∆T(i  j).

(cid:3)

∆(i  j) ≤ r(cid:88)
≤ 2 r(cid:88)

s=1

s=1

r(cid:88)

s=1

Inﬁnite Metric Spaces

4.3
Finally  we address inﬁnite spaces by discretizing the space K and reducing to the ﬁnite case. Recall
that in this case we also assume that the loss functions are Lipschitz.
Proof of Theorem 9. Given the deﬁnition of the covering dimension d = D(∆) ≥ 1  it is straightfor-
r(∆) ≤ Cr−d for
ward that for some constant C > 0 (that might depend on the metric ∆) it holds that Nc

8

all r > 0. Fix some  > 0  and take a minimal 2-covering K(cid:48) of K of size |K(cid:48)| ≤ C(2)−d ≤ C−d.
Observe that by restricting the algorithm to pick actions from K(cid:48)  we might lose at most O(T) in the
regret. Also  since K(cid:48) is minimal  the distance between any two elements in K(cid:48) is at least   thus the
covering complexity of the space has
Cc(∆) = sup
r ≥

r(∆) ≤ C sup
r ≥

r−d+1 ≤ C

as we assume that d ≥ 1. Hence  by Theorem 7 and the Lipschitz assumption  there exists an
algorithm for which

−d+1
 

r·Nc

RegretMC((cid:96)1:T  ∆) =(cid:101)O

1

A simple computation reveals that  = Θ(T− 1
movement regret.

(cid:16)max(cid:8)
d+2) optimizes the above bound  and leads to(cid:101)O(T d+1

2   T(cid:9)(cid:17)

− d2 T

− d−1

2
3   

3 T

.

d+2)
(cid:3)

Acknowledgements
RL is supported in funds by the Eric and Wendy Schmidt Foundation for strategic innovations. YM is
supported in part by a grant from the Israel Science Foundation  a grant from the United States-Israel
Binational Science Foundation (BSF)  and the Israeli Centers of Research Excellence (I-CORE)
program (Center No. 4/11).

References
[1] R. Agrawal  M. V. Hegde  and D. Teneketzis. Asymptotically eﬃcient adaptive allocation rules
for the multiarmed bandit problem with switching costs. IEEE Transactions on Optimal Control 
33(10):899–906  1988.

[2] R. Arora  O. Dekel  and A. Tewari. Online bandit learning against an adaptive adversary:
from regret to policy regret. In Proceedings of the 29th International Conference on Machine
Learning (ICML-12)  pages 1503–1510  2012.

[3] M. Asawa and D. Teneketzis. Multi-armed bandits with switching penalties. IEEE Transactions

on Automatic Control  41(3):328–348  1996.

[4] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[5] P. Auer  R. Ortner  and C. Szepesvári. Improved rates for the stochastic continuum-armed bandit
problem. Proceedings of the 20th Annual Conference on Learning Theory  pages 454–468 
2007.

[6] J. S. Banks and R. K. Sundaram. Switching costs and the gittins index. Econometrica  62:

687–694  1994.

[7] Y. Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In
37th Annual Symposium on Foundations of Computer Science  FOCS ’96  Burlington  Vermont 
USA  14-16 October  1996  pages 184–193  1996.

[8] A. Borodin and R. El-Yaniv. Online Computation and Competitive Analysis. Cambridge

University Press  1998.

[9] A. Borodin  N. Linial  and M. E. Saks. An optimal on-line algorithm for metrical task system.

Journal of the ACM (JACM)  39(4):745–763  1992.

[10] S. Bubeck  R. Munos  G. Stoltz  and C. Szepesvári. X-armed bandits. Journal of Machine

Learning Research  12:1587–1627  2011.

[11] E. Cope. Regret and convergence bounds for a class of continuum-armed bandit problems. IEEE

Transactions on Automatic Control  54(6):1243–1253  2009.

9

[12] O. Dekel  J. Ding  T. Koren  and Y. Peres. Bandits with switching costs: T2/3 regret. In
Proceedings of the 46th Annual ACM Symposium on Theory of Computing  pages 459–467.
ACM  2014.

[13] E. Even-Dar  S. M. Kakade  and Y. Mansour. Online markov decision processes. Math. Oper.

Res.  34(3):726–736  2009.

[14] J. Fakcharoenphol  S. Rao  and K. Talwar. A tight bound on approximating arbitrary metrics by

tree metrics. J. Comput. Syst. Sci.  69(3):485–497  2004.

[15] M. Feldman  T. Koren  R. Livni  Y. Mansour  and A. Zohar. Online pricing with strategic and

patient buyers. In Annual Conference on Neural Information Processing Systems  2016.

[16] S. Geulen  B. Vöcking  and M. Winkler. Regret minimization for online buﬀering problems

using the weighted majority algorithm. In COLT  pages 132–143  2010.

[17] J. Gittins  K. Glazebrook  and R. Weber. Multi-Armed Bandit Allocation Indices  2nd Edition.

John Wiley  2011.

[18] S. Guha and K. Munagala. Multi-armed bandits with metric switching costs. In International

Colloquium on Automata  Languages  and Programming  pages 496–507. Springer  2009.

[19] A. Gyorgy and G. Neu. Near-optimal rates for limited-delay universal lossy source coding.

IEEE Transactions on Information Theory  60(5):2823–2834  2014.

[20] T. Jun. A survey on the bandit problem with switching costs. De Economist  152(4):513–541 

2004.

[21] R. Kleinberg and A. Slivkins. Sharp dichotomies for regret minimization in metric spaces. In
Proceedings of the twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms  pages
827–846. Society for Industrial and Applied Mathematics  2010.

[22] R. Kleinberg  A. Slivkins  and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings
of the fortieth annual ACM symposium on Theory of computing  pages 681–690. ACM  2008.
[23] R. D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in

Neural Information Processing Systems  pages 697–704  2004.

[24] T. Koren  R. Livni  and Y. Mansour. Bandits with movement costs and adaptive pricing. In

COLT  2017.

[25] T. Koren  R. Livni  and Y. Mansour. Multi-armed bandits with metric movement costs. arXiv

preprint arXiv:1710.08997  2017.

[26] S. Magureanu  R. Combes  and A. Proutiere. Lipschitz bandits: Regret lower bound and optimal

algorithms. In COLT  pages 975–999  2014.

[27] G. Neu  A. György  C. Szepesvári  and A. Antos. Online markov decision processes under

bandit feedback. IEEE Trans. Automat. Contr.  59(3):676–691  2014.

[28] R. Ortner. Online regret bounds for markov decision processes with deterministic transitions.

Theor. Comput. Sci.  411(29-30):2684–2695  2010.

[29] A. Slivkins. Multi-armed bandits on implicit metric spaces. In Advances in Neural Information

Processing Systems  pages 1602–1610  2011.

[30] A. Slivkins  F. Radlinski  and S. Gollapudi. Ranked bandits in metric spaces: learning diverse
rankings over large document collections. Journal of Machine Learning Research  14(Feb):
399–436  2013.

[31] T. Tao. 245c  notes 5: Hausdorﬀ dimension. http://terrytao.wordpress.com/2009/05/

19/245c-notes-5-hausdorff-dimension-optional/  2009.

[32] J. Yu and S. Mannor. Unimodal bandits. In Proceedings of the 28th International Conference

on Machine Learning  2011.

[33] J. Y. Yu  S. Mannor  and N. Shimkin. Markov decision processes with arbitrary reward processes.

Math. Oper. Res.  34(3):737–757  Aug. 2009. ISSN 0364-765X.

10

,Tomer Koren
Roi Livni
Yishay Mansour