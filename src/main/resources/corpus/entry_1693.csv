2019,Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds,Deep neural networks have revolutionized many real world applications  due to their flexibility in data fitting and accurate predictions for unseen data. A line of research reveals that neural networks can approximate certain classes of functions with an arbitrary accuracy  while the size of the network scales exponentially with respect to the data dimension. Empirical results  however  suggest that networks of moderate size already yield appealing performance. To explain such a gap  a common belief is that many data sets exhibit low dimensional structures  and can be modeled as samples near a low dimensional manifold. In this paper  we prove that neural networks can efficiently approximate functions supported on low dimensional manifolds. The network size scales exponentially in the approximation error  with an exponent depending on the intrinsic dimension of the data and the smoothness of the function. Our result shows that exploiting low dimensional data structures can greatly enhance the efficiency in function approximation by neural networks. We also implement a sub-network that assigns input data to their corresponding local neighborhoods  which may be of independent interest.,Efﬁcient Approximation of Deep ReLU Networks for

Functions on Low Dimensional Manifolds

Minshuo Chen Haoming Jiang Wenjing Liao

Tuo Zhao

{mchen393  jianghm  wliao60  tourzhao}@gatech.edu

Georgia Institute of Technology

Abstract

Deep neural networks have revolutionized many real world applications  due to
their ﬂexibility in data ﬁtting and accurate predictions for unseen data. A line of
research reveals that neural networks can approximate certain classes of functions
with an arbitrary accuracy  while the size of the network scales exponentially with
respect to the data dimension. Empirical results  however  suggest that networks
of moderate size already yield appealing performance. To explain such a gap 
a common belief is that many data sets exhibit low dimensional structures  and
can be modeled as samples near a low dimensional manifold. In this paper  we
prove that neural networks can efﬁciently approximate functions supported on low
dimensional manifolds. The network size scales exponentially in the approximation
error  with an exponent depending on the intrinsic dimension of the data and the
smoothness of the function. Our result shows that exploiting low dimensional
data structures can greatly enhance the efﬁciency in function approximation by
neural networks. We also implement a sub-network that assigns input data to their
corresponding local neighborhoods  which may be of independent interest.

1

Introduction

In the past decade  neural networks have made astonishing breakthroughs in many real world
applications  such as computer vision (Krizhevsky et al.  2012; Goodfellow et al.  2014; Long et al. 
2015)  natural language processing (Graves et al.  2013; Bahdanau et al.  2014; Young et al.  2018) 
healthcare (Miotto et al.  2017; Jiang et al.  2017)  robotics (Gu et al.  2017)  etc.
Although data sets in these applications are highly complex  neural networks have achieved over-
whelming successes. For image classiﬁcation  the winner of the 2017 ImageNet challenge retained a
top-5 error rate of 2.25% (Hu et al.  2018)  while the data set consists of about 1.2 million labeled
high resolution images in 1000 categories. For speech recognition  Amodei et al. (2016) reported that
deep neural networks outperformed humans with a 5.15% word error rate on the LibriSpeech corpus
constructed from audio books (Panayotov et al.  2015). Such a data set consists of approximately
1000 hours of 16kHz read English speech from 8000 audio books. These empirical results suggest
that neural networks can well approximate complex distributions and functions on data.
A line of research attempts to explain the success of neural networks through the lens of expressivity
— neural networks can effectively approximate various classes of functions. Among existing works 
the most well-known results are the universal approximation theorems  see Irie and Miyake (1988);
Funahashi (1989); Cybenko (1989); Hornik (1991); Chui and Li (1992); Leshno et al. (1993).
Speciﬁcally  Cybenko (1989) showed that neural networks with one single hidden layer and continuous
sigmoidal1 activations can approximate continuous functions in a unit cube with arbitrary accuracy.
Later  Hornik (1991) extended the universal approximation theorem to general feed-forward networks

1A function σ(x) is sigmoidal  if σ(x) → 0 as x → −∞  and σ(x) → 1 as x → ∞.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

with a single hidden layer  while the width of the network has to be exponentially large. Speciﬁc
approximation rates of shallow networks (with one hidden layer) with smooth activation functions
were given in Barron (1993) and Mhaskar (1996). Recently  Lu et al. (2017) proved the universal
approximation theorem for width-bounded deep neural networks  and Hanin (2017) improved the
result with ReLU (Rectiﬁed Linear Units) activations  i.e. ReLU(x) = max{0  x}. Yarotsky (2017)
further showed that deep ReLU networks can uniformly approximate functions in Sobolev spaces 
while the network size scales exponentially in the approximation error with an exponent depending
on the data dimension. Moreover  the network size in Yarotsky (2017) matches its lower bound.
The network size considered in applications  however  is signiﬁcantly smaller than what is predicted
by the theory above. In the ImageNet challenge  data are RGB images with a resolution of 224 × 224.
The theory above suggests that  to achieve a  uniform approximation error  the number of neurons
has to scale as −224×224×3/2 (Barron  1993). Setting  = 0.1 already gives rise to 10224×224×3/2
neurons. However  the AlexNet (Krizhevsky et al.  2012) only consists of 650000 neurons and 60
million parameters to beat the state-of-the-art. To boost the performance on the ImageNet  several
more sophisticated network structures were proposed later  such as VGG16 (Simonyan and Zisserman 
2014) which consists of about 138 million parameters. The size of both networks remains extremely
small compared to 10224×224×3/2. Why is there a tremendous gap between theory and practice?
A common belief is that real world data sets often exhibit low dimensional structures. Many
images consist of projections of 3-dimensional objects followed by some transformations  such as
rotation  translation  and skeleton. Such a generating mechanism induces a small number of intrinsic
parameters. Speech data are composed of words and sentences following the grammar  and therefore
have a small degree of freedom. More broadly  visual  acoustic  textual  and many other types of
data all have low dimensional structures due to rich local regularities  global symmetries  repetitive
patterns  or redundant sampling. It is plausible to model these data as samples near a low dimensional
manifold (Tenenbaum et al.  2000; Roweis and Saul  2000). Then a natural question is:
Can deep neural networks efﬁciently approximate functions supported on low dimensional manifolds?
Function approximation on manifolds has been well studied using local polynomials (Bickel et al. 
2007) and wavelets (Coifman and Maggioni  2006). However  studies using neural networks are
very limited. Two noticeable works are Chui and Mhaskar (2016) and Shaham et al. (2018). In Chui
and Mhaskar (2016)  high order differentiable functions on manifolds are approximated by neural
networks with smooth activations  e.g.  sigmoid activations and rectiﬁed quadratic unit functions
(σ(x) = (max{0  x})2). These smooth activations  however  are rarely used in the mainstream
applications such as computer vision (Krizhevsky et al.  2012; Long et al.  2015; Hu et al.  2018).
In Shaham et al. (2018)  a 4-layer network with ReLU activations was proposed to approximate
C 2 functions on low dimensional manifolds that have absolutely summable wavelet coefﬁcients.
However  this theory does not cover arbitrarily smooth functions  and the analysis is built upon
a restrictive assumption — there exists a linear transformation that maps the input data to sparse
coordinates  but such transformation is not explicitly given.
In this paper  we propose a framework to construct deep neural networks with nonsmooth activations
to approximate functions supported on a d-dimensional smooth manifold isometrically embedded
in RD. We prove that  in order to achieve a ﬁxed approximation error  the network size scales
exponentially with respect to the intrinsic dimension d  instead of the ambient dimension D. Our
framework is ﬂexible: 1). It applies to nonsmooth activations  e.g.  ReLU and leaky ReLU activations;
2). It applies to a wide class of functions  such as Sobolev and Hölder classes which are typical
examples in nonparametric statistics (Györﬁ et al.  2006); 3). It exploits high order smoothness of
functions for making the approximation as efﬁcient as possible.
Theorem (informal). Let M be a d-dimensional compact Riemannian manifold isometrically em-
bedded in RD with d (cid:28) D. Assume M satisﬁes some mild regularity conditions. Given any
 ∈ (0  1)  there exists a ReLU neural network structure such that  for any C n (n ≥ 1) function
f : M → R  if the weight parameters are properly chosen  the network yields a function (cid:98)f sat-
isfying (cid:107)f − (cid:98)f(cid:107)∞ ≤ . Such a network has no more than c1(cid:0)log 1
 + log D(cid:1) layers  and at most
 + D log D(cid:1) neurons and weight parameters  where c1  c2 depend on d  n 
c2(cid:0)−d/n log 1
f  and M.
Our network size scales like −d/n and only weakly depends on the ambient dimension D. This is
consistent with empirical observations  and partially justiﬁes why the networks of moderate size have

 + D log 1

2

achieved a great success on aforementioned learning tasks. Moreover  we show that our network size
matches its lower bound up to a logarithmic factor (see Theorem 2).
Our theory applies to general C n functions and leverages the beneﬁts of exploiting high order
smoothness. Our result improves Shaham et al. (2018) for C n functions with n > 2. In this case  our
network size scales like −d/n  which is signiﬁcantly smaller than the one in Shaham et al. (2018) in
the order of −d/2.
Here we state the theorem for C n functions for simplicity  and similar results hold for Hölder
functions (see Theorem 1). Our framework can be easily applied to leaky ReLU activations  since
leaky ReLU can be implemented by the difference of two ReLU functions.
The high level idea of our framework is to partition the low dimensional manifold into a collection
of open sets  and then use Taylor expansions to approximate the function in each neighborhood. A
new technique is developed to implement a sub-network that assigns the input to its corresponding
neighborhood on the manifold  which may be of independent interest.
Notations: We use bold-faced letters to denote vectors  and normal font letters with a subscript
to denote its coordinate  e.g.  x ∈ Rd and xk being the k-th coordinate of x. Given a vector
i=1 ni! and |n| =(cid:80)d
n = [n1  . . .   nd](cid:62)
.
i=1 xni
Given a function f : Rd (cid:55)→ R  we denote its derivative as Dnf =
  and its (cid:96)∞ norm as
(cid:107)f(cid:107)∞ = maxx |f (x)|. We use ◦ to denote the composition operator.
2 Preliminaries

i=1 ni. We deﬁne xn =(cid:81)d

∈ Nd  we deﬁne n! =(cid:81)d

i

∂|n|f
1 ...∂x

∂xn1

nd
d

We brieﬂy review manifolds  partition of unity  and function spaces deﬁned on smooth manifolds.
Details can be found in Tu (2010) and Lee (2006).
Let M be a d-dimensional Riemannian manifold isometrically embedded in RD.
Deﬁnition 1 (Chart). A chart for M is a pair (U  φ) such that U ⊂ M is open and φ : U (cid:55)→ Rd 
where φ is a homeomorphism (i.e.  bijective  φ and φ−1 are both continuous).
The open set U is called a coordinate neighborhood  and φ is called a coordinate system on U. A
chart essentially deﬁnes a local coordinate system on M. We say two charts (U  φ) and (V  ψ) on
M are C k compatible if and only if the transition functions  φ ◦ ψ−1 : ψ(U ∩ V ) (cid:55)→ φ(U ∩ V ) and
ψ ◦ φ−1 : φ(U ∩ V ) (cid:55)→ ψ(U ∩ V ) are both C k. Then we give the deﬁnition of an atlas.
Deﬁnition 2 (C k Atlas). An atlas for M is a collection {(Uα  φα)}α∈A of pairwise C k compatible
charts such that(cid:83)α∈A Uα = M.
Deﬁnition 3 (Smooth Manifold). A smooth manifold is a manifold M together with a C∞ atlas.
Classical examples of smooth manifolds are the Euclidean space RD  the torus  and the unit sphere.
The existence of an atlas on M allows us to deﬁne differentiable functions.
Deﬁnition 4 (C n Functions on M). Let M be a smooth manifold in RD. A function f : M (cid:55)→ R is
C n if for any chart (U  φ)  the composition f ◦ φ−1 : φ(U ) (cid:55)→ R is continuously differentiable up to
order n.
Remark 1. The deﬁnition of C n functions is independent of the choice of the chart (U  φ). Suppose
(V  ψ) is another chart and V (cid:84) U (cid:54)= ∅. Then we have f ◦ ψ−1 = (f ◦ φ−1) ◦ (φ ◦ ψ−1). Since M
is a smooth manifold  (U  φ) and (V  ψ) are C∞ compatible. Thus  f ◦ φ−1 is C n and φ ◦ ψ−1 is
C∞  and their composition is C n.
We next introduce partition of unity  which plays a crucial role in our construction of neural networks.
Deﬁnition 5 (Partition of Unity). A C∞ partition of unity on a manifold M is a collection of
nonnegative C∞ functions ρα : M (cid:55)→ R+ for α ∈ A such that 1).
the collection of supports 
{supp(ρα)}α∈A is locally ﬁnite2; 2).(cid:80) ρα = 1.
For a smooth manifold  a C∞ partition of unity always exisits.

2A collection {Aα} is locally ﬁnite if every point has a neighborhood that meets only ﬁnitely many of Aα’s.

3

Proposition 1 (Existence of a C∞ partition of unity). Let {Uα}α∈A be an open cover of a smooth
manifold M. Then there is a C∞ partition of unity {ρi}
∞
i=1 with every ρi having a compact support
such that supp(ρi) ⊂ Uα for some α ∈ A.
Proposition 1 gives rise to the decomposition f =(cid:80)∞
i=1 fi with fi = f ρi. Note that the fi’s have the
same regularity as f  since fi ◦ φ−1 = (f ◦ φ−1)× (ρi ◦ φ−1) for a chart (U  φ). This decomposition
has the advantage that every fi is only supported in a single chart. Then the approximation of f boils
down to the approximations of the fi’s  which are localized and have the same regularity as f.
To characterize the curvature of a manifold  we adopt the following geometric concept.
Deﬁnition 6 (Reach  Deﬁnition 2.1 in Aamari et al. (2019)). Denote C(M) = {x ∈ RD : ∃p (cid:54)= q ∈
M (cid:107)p − x(cid:107)2 = (cid:107)q − x(cid:107)2 = inf y∈M (cid:107)y − x(cid:107)2} as the set of points that have at least two nearest
neighbors on M. Then the reach τ > 0 is deﬁned as τ = inf x∈M y∈C(M) (cid:107)x − y(cid:107)2 .

Figure 1: Manifolds with large and small reach.

Reach has a straightforward geometrical interpretation: At each point x ∈ M  the radius of the
osculating circle is greater or equal to τ. A large reach for M essentially requires the manifold M
not to change “rapidly” as shown in Figure 1.
Reach determines a proper choice of an atlas for M. In Section 4  we choose each chart Uα contained
in a ball of radius less than τ /2. For smooth manifolds with a small τ  we need a large number of
charts. Therefore  the reach of a smooth manifold reﬂects the difﬁculty of function approximations
on M.
3 Main Result
We next present how to construct a ReLU network to approximate f : M (cid:55)→ R with error   under
certain assumptions on M and f.
Assumption 1. M is a d-dimensional compact Riemannian manifold isometrically embedded in RD.
There exists a constant B such that for any point x ∈ M  we have |xi| ≤ B for all i = 1  . . .   D.
Assumption 2. The reach of M is τ > 0.
Assumption 3. f : M (cid:55)→ R belongs to the Hölder space H n α with a positive integer n and
α ∈ (0  1]  in the sense that f ∈ C n−1 and for any chart (U  φ) and |n| = n  we have

−1)(cid:12)(cid:12)φ(x1) − Dn(f ◦ φ

−1)(cid:12)(cid:12)φ(x2)(cid:12)(cid:12)(cid:12) ≤ (cid:107)φ(x1) − φ(x2)(cid:107)α

(1)
Assumption 3 says that all n-th order derivatives of f ◦ φ−1 are Hölder continuous. Here Hölder
functions are deﬁned on manifolds. We recover the standard Hölder class on Euclidean spaces by
taking φ as the identity map. We also note that Assumption 3 does not depend on the choice of charts.
We now formally state our main result. Extensions to functions in Sobolev spaces are straightforward.
Theorem 1. Suppose Assumptions 1 and 2 hold. Given any  ∈ (0  1)  there exists a ReLU network
structure such that  for any f : M → R satisfying Assumption 3  if the weight parameters are
properly chosen  the network yields a function (cid:98)f satisfying (cid:107)(cid:98)f − f(cid:107)∞ ≤ . Such a network has no
more than c1(log 1
 + D log D) neurons and
weight parameters  where c1  c2 depend on d  n  f  τ  and the surface area of M.
The network structure identiﬁed by Theorem 1 consists of three sub-networks as shown in Figure 2:
• Chart determination sub-network  which assigns the input to its corresponding neighborhoods;
• Taylor approximation sub-network  which approximates f by polynomials in each neighborhood;

 + log D) layers  and at most c2(

n+α log 1

 + D log 1

(cid:12)(cid:12)(cid:12)Dn(f ◦ φ

2  

∀x1  x2 ∈ U.

− d

4

Large⌧<latexit sha1_base64="fKNHLI5C5ExbR/gZzZNpO6xHyvs=">AAACB3icbVDLSsNAFL2pr1pfVZduBlvBVUnqwi4Lbly4qGAf0IQymUzaoZNJmJkIJfQD/AG3+gfuxK2f4Q/4HU7aLLT1wIXDOffF8RPOlLbtL6u0sbm1vVPereztHxweVY9PeipOJaFdEvNYDnysKGeCdjXTnA4SSXHkc9r3pze533+kUrFYPOhZQr0IjwULGcHaSO4dlmOK6q7GaX1UrdkNewG0TpyC1KBAZ1T9doOYpBEVmnCs1NCxE+1lWGpGOJ1X3FTRBJMpHtOhoQJHVHnZ4uc5ujBKgMJYmhIaLdTfExmOlJpFvumMsJ6oVS8X//UClS9cua7DlpcxkaSaCrI8HqYc6RjloaCASUo0nxmCiWTmf0QmWGKiTXQVE4yzGsM66TUbzlWjed+stVtFRGU4g3O4BAeuoQ230IEuEEjgGV7g1Xqy3qx362PZWrKKmVP4A+vzB2KXmWA=</latexit>Small⌧<latexit sha1_base64="vBdcAHOfpTIU6FPoZZYSLqeYSbM=">AAACB3icbVDLSsNAFL2pr1pfVZduBlvBVUnqwi4LblxWtA9oQplMJu3QySTMTIQS+gH+gFv9A3fi1s/wB/wOJ20W2nrgwuGc++L4CWdK2/aXVdrY3NreKe9W9vYPDo+qxyc9FaeS0C6JeSwHPlaUM0G7mmlOB4mkOPI57fvTm9zvP1KpWCwe9CyhXoTHgoWMYG0k9z7CnKO6q3FaH1VrdsNeAK0TpyA1KNAZVb/dICZpRIUmHCs1dOxEexmWmhFO5xU3VTTBZIrHdGiowBFVXrb4eY4ujBKgMJamhEYL9fdEhiOlZpFvOiOsJ2rVy8V/vUDlC1eu67DlZUwkqaaCLI+HKUc6RnkoKGCSEs1nhmAimfkfkQmWmGgTXcUE46zGsE56zYZz1WjeNWvtVhFRGc7gHC7BgWtowy10oAsEEniGF3i1nqw36936WLaWrGLmFP7A+vwBeWyZbg==</latexit>RapidChange<latexit sha1_base64="mXjvclsmbbGt3EGkxmEEuCvFg6k=">AAACB3icbVDLSsNAFJ34rPVVdelmsAiuSlIXdlnoxmUV+4A2lMnkph06mQwzE6GEfoA/4Fb/wJ249TP8Ab/DSZuFth64cDjnvjiB5Ewb1/1yNja3tnd2S3vl/YPDo+PKyWlXJ6mi0KEJT1Q/IBo4E9AxzHDoSwUkDjj0gmkr93uPoDRLxIOZSfBjMhYsYpQYKw3viWQhbk2IGMOoUnVr7gJ4nXgFqaIC7VHlexgmNI1BGMqJ1gPPlcbPiDKMcpiXh6kGSeiUjGFgqSAxaD9b/DzHl1YJcZQoW8Lghfp7IiOx1rM4sJ0xMRO96uXiv16o84Ur103U8DMmZGpA0OXxKOXYJDgPBYdMATV8Zgmhitn/MZ0QRaix0ZVtMN5qDOukW69517X6Xb3abBQRldA5ukBXyEM3qIluURt1EEUSPaMX9Oo8OW/Ou/OxbN1wipkz9AfO5w/3ppm9</latexit>SlowChange<latexit sha1_base64="i1c8ZnWMheX2Fx9Xq/Ai3mUCXLc=">AAACBnicbVC7TgJBFJ3FF+ILtbSZSEysyC4WUpLQWGKURwIbMjt7gQmzM5uZWQ3Z0PsDtvoHdsbW3/AH/A5nYQsFT3KTk3PuKyeIOdPGdb+cwsbm1vZOcbe0t39weFQ+PulomSgKbSq5VL2AaOBMQNsww6EXKyBRwKEbTJuZ330ApZkU92YWgx+RsWAjRomxUv+Oy0fcnBAxhmG54lbdBfA68XJSQTlaw/L3IJQ0iUAYyonWfc+NjZ8SZRjlMC8NEg0xoVMyhr6lgkSg/XTx8hxfWCXEI6lsCYMX6u+JlERaz6LAdkbETPSql4n/eqHOFq5cN6O6nzIRJwYEXR4fJRwbibNMcMgUUMNnlhCqmP0f0wlRhBqbXMkG463GsE46tap3Va3d1iqNeh5REZ2hc3SJPHSNGugGtVAbUSTRM3pBr86T8+a8Ox/L1oKTz5yiP3A+fwBXkZlo</latexit>Figure 2: The ReLU network identiﬁed by Theorem 1.

• Pairing sub-network  which yields multiplications of the proper pairs of outputs from the chart
determination and the Taylor approximation sub-networks.
Speciﬁcally  we partition the manifold as M =(cid:83)CM
i=1 Ui  where the Ui’s are open sets contained in a
Euclidean ball of radius less than τ /2. CM depends on the reach τ  the surface area of M  and the
dimension d (see Section 4 for an explicit characterization). For each chart  the chart determination
sub-network computes an approximation of the indicator function on Ui. The Taylor approximation
sub-network provides a local polynomial approximation of f on Ui. Then the pairing sub-network

approximates the product for the proper pairs of outputs in the previous two sub-networks. Finally  (cid:98)f

is obtained by taking a sum over CM outputs from the pairing sub-network.
The size of our ReLU network matches its lower bound up to a logarithmic factor for the approxima-
tion of functions in Hölder spaces. Denote F n d as functions deﬁned on [0  1]d in the Hölder space
H n−1 1. We state a lower bound due to DeVore et al. (1989).
Theorem 2. Fix d and n. Let W be a positive integer and κ : RW (cid:55)→ C([0  1]d) be any mapping.
Suppose there is a continuous map Θ : F n d (cid:55)→ RW such that (cid:107)f − κ(Θ(f ))(cid:107)∞ ≤  for any
f ∈ F n d. Then W ≥ c− d
We take RW as the parameter space of a ReLU network  and κ as the network structure. Then
to approximate any f ∈ F n d  the ReLU network has at least c− d
n weight parameters. Although
Theorem 2 holds for functions on [0  1]d  our network size remains in the same order up to a
logarithmic factor even when the function is supported on a manifold of dimension d.

n with c depending on n only.

4 Proof of the Main Result

Due to limited space  we present a sketch of the proof for Theorem 1. Before we proceed  we show
how to approximate the multiplication operation using ReLU networks. This operation is heavily
used in the Taylor approximation sub-network  since Taylor polynomials involve sum of products.
We ﬁrst show ReLU networks can approximate quadratic functions.
Lemma 1 (Proposition 2 in Yarotsky (2017)). The function f (x) = x2 with x ∈ [0  1] can be
approximated by a ReLU network with any error  > 0. The network has depth and the number of
neurons and weight parameters no more than c log(1/) with an absolute constant c.

This lemma is proved in Appendix A.1. The idea is to approximate quadratic functions using a
weighted sum of a series of sawtooth functions. Those sawtooth functions are obtained by compositing
the triangular function

g(x) = 2ReLU(x) − 4ReLU(x − 1/2) + 2ReLU(x − 1) 

which can be implemented by a single layer ReLU network.
We then approximate the multiplication operation by invoking the identity ab = 1
4 ((a+b)2−(a−b)2)
where the two squares can be approximated by ReLU networks in Lemma 1.
Corollary 1 (Proposition 3 in Yarotsky (2017)). Given a constant C > 0 and  ∈ (0  C 2)  there

is a ReLU network which implements a function (cid:98)× : R2 (cid:55)→ R such that: 1). For all inputs x and

5

P<latexit sha1_base64="PVOqlyFnedxpBSr3+6iiXLUv7WU=">AAACAXicbVC7TgJBFL2LL8QXamkzEUysyC4WUpLYWGIijwQ2ZHZ2FkZmZjczsyaEUPkDtvoHdsbWL/EH/A5nYQsFT3KTk3PuKydIONPGdb+cwsbm1vZOcbe0t39weFQ+PunoOFWEtknMY9ULsKacSdo2zHDaSxTFIuC0G0xuMr/7SJVmsbw304T6Ao8kixjBxkqd6kCnojosV9yauwBaJ15OKpCjNSx/D8KYpIJKQzjWuu+5ifFnWBlGOJ2XBqmmCSYTPKJ9SyUWVPuzxbdzdGGVEEWxsiUNWqi/J2ZYaD0Vge0U2Iz1qpeJ/3qhzhauXDdRw58xmaSGSrI8HqUcmRhlcaCQKUoMn1qCiWL2f0TGWGFibGglG4y3GsM66dRr3lXNvatXmo08oiKcwTlcggfX0IRbaEEbCDzAM7zAq/PkvDnvzseyteDkM6fwB87nD0HflyI=</latexit>Inputx<latexit sha1_base64="dL0dMPLsW2Fg2clRUpJX4ZR3PQA=">AAACD3icbVDLTsJAFL3FF+Kr4tLNRDBxRVpc6JLEje4wkUcCDZkOU5gwnTYzUwNp+Ah/wK3+gTvj1k/wB/wOp9CFgieZ5OTc15njx5wp7ThfVmFjc2t7p7hb2ts/ODyyj8ttFSWS0BaJeCS7PlaUM0FbmmlOu7GkOPQ57fiTm6zeeaRSsUg86FlMvRCPBAsYwdpIA7t8J+JEo2o/xHrsB+l0Xh3YFafmLIDWiZuTCuRoDuzv/jAiSUiFJhwr1XOdWHsplpoRTuelfqJojMkEj2jPUIFDqrx04X2Ozo0yREEkzRMaLdTfEykOlZqFvunMLKrVWib+WxuqbOHKdR1ceynLvkwFWR4PEo50hLJw0JBJSjSfGYKJZMY/ImMsMdEmwpIJxl2NYZ206zX3subc1yuNeh5REU7hDC7AhStowC00oQUEpvAML/BqPVlv1rv1sWwtWPnMCfyB9fkDODWcgg==</latexit>ChartDetermination<latexit sha1_base64="eYW/DtD3JxkNK4qnf8s9n3kGubQ=">AAACEHicbVDLSsNAFJ3UV62vapduBovgqiR1YZeFunBZwT6gDWUyuWmHTiZhZiKE0J/wB9zqH7gTt/6BP+B3OGmz0NYDFw7n3BfHizlT2ra/rNLW9s7uXnm/cnB4dHxSPT3rqyiRFHo04pEcekQBZwJ6mmkOw1gCCT0OA2/eyf3BI0jFIvGg0xjckEwFCxgl2kiTaq0zI1LjW9AgQyYKtW437CXwJnEKUkcFupPq99iPaBKC0JQTpUaOHWs3M4sZ5bCojBMFMaFzMoWRoYKEoNxs+fwCXxrFx0EkTQmNl+rviYyESqWhZzpDomdq3cvFfz1f5QvXruug5WZMxIkGQVfHg4RjHeE8HewzCVTz1BBCJTP/Y2oCItTEoyomGGc9hk3Sbzac60bzvllvt4qIyugcXaAr5KAb1EZ3qIt6iKIUPaMX9Go9WW/Wu/Wxai1ZxUwN/YH1+QN9/51D</latexit>TaylorApproximation<latexit sha1_base64="XO7DmYmkw5sml1IVBEt9AA149Z8=">AAACEXicbVA7TgMxFPSGXwi/8OloLCIkqmg3FKQMoqEMUn5Ssoq8Xm9ixWtbthexrHIKLkALN6BDtJyAC3AOnGQLSHjVaGbee6MJJKPauO6XU1hb39jcKm6Xdnb39g/Kh0cdLRKFSRsLJlQvQJowyknbUMNITyqC4oCRbjC5mende6I0FbxlUkn8GI04jShGxlLD8kkLpdYDr6VU4oHGOV1xq+584CrwclAB+TSH5e9BKHASE24wQ1r3PVcaP0PKUMzItDRINJEIT9CI9C3kKCbaz+bpp/DcMiGMbIhIcAPn7O+NDMVap3FgnTbeWC9rM/JfLdSzg0vfTVT3M8plYgjHi+dRwqARcFYPDKki2LDUAoQVtfkhHiOFsLEllmwx3nINq6BTq3qX1dpdrdKo5xUVwSk4AxfAA1egAW5BE7QBBo/gGbyAV+fJeXPenY+FteDkO8fgzzifP66Tne4=</latexit>Pairing<latexit sha1_base64="SEMrlTJf4KpMunKRiogQz4XSGQc=">AAACAnicbVDLSsNAFL3xWeur6tLNYBFclaQu7LLgxmUF+4A2lMlk2g6dTMLMjVBCd/6AW/0Dd+LWH/EH/A4nbRbaeuDC4Zz74gSJFAZd98vZ2Nza3tkt7ZX3Dw6Pjisnpx0Tp5rxNotlrHsBNVwKxdsoUPJeojmNAsm7wfQ297uPXBsRqwecJdyP6FiJkWAUrdRtUaGFGg8rVbfmLkDWiVeQKhRoDSvfgzBmacQVMkmN6Xtugn5GNQom+bw8SA1PKJvSMe9bqmjEjZ8t3p2TS6uEZBRrWwrJQv09kdHImFkU2M6I4sSsern4rxeafOHKdRw1/EyoJEWu2PL4KJUEY5LnQUKhOUM5s4QyLez/hE2opgxtamUbjLcawzrp1Gveda1+X682G0VEJTiHC7gCD26gCXfQgjYwmMIzvMCr8+S8Oe/Ox7J1wylmzuAPnM8ft9eX/w==</latexit>1<latexit sha1_base64="F3h1zoHDXmmgsvSwmSaFr/3F4xA=">AAACAXicbVDLSsNAFL1TX7W+qi7dDBbBVUmqYJcFNy4r2FZoQ5lMJu3YySTMTIQSuvIH3OofuBO3fok/4Hc4abPQ1gMXDufcF8dPBNfGcb5QaW19Y3OrvF3Z2d3bP6geHnV1nCrKOjQWsbr3iWaCS9Yx3Ah2nyhGIl+wnj+5zv3eI1Oax/LOTBPmRWQkecgpMVbqDpIxH7rDas2pO3PgVeIWpAYF2sPq9yCIaRoxaaggWvddJzFeRpThVLBZZZBqlhA6ISPWt1SSiGkvm387w2dWCXAYK1vS4Ln6eyIjkdbTyLedETFjvezl4r9eoPOFS9dN2PQyLpPUMEkXx8NUYBPjPA4ccMWoEVNLCFXc/o/pmChCjQ2tYoNxl2NYJd1G3b2oN24va61mEVEZTuAUzsGFK2jBDbShAxQe4Ble4BU9oTf0jj4WrSVUzBzDH6DPH5ckl1o=</latexit>CM<latexit sha1_base64="QDp9fSJqzOGvREr19Mu0d/Xrn48=">AAACEXicbVDLSsNAFJ3UV62v+Ni5GSyCq5JUwS4L3bgRKtgHNCFMJpN26GQSZiZCDfkKf8Ct/oE7cesX+AN+h5M2C209MHA45965h+MnjEplWV9GZW19Y3Orul3b2d3bPzAPj/oyTgUmPRyzWAx9JAmjnPQUVYwME0FQ5DMy8Kedwh88ECFpzO/VLCFuhMachhQjpSXPPHGSCfWyjudESE0wYtltnntm3WpYc8BVYpekDkp0PfPbCWKcRoQrzJCUI9tKlJshoShmJK85qSQJwlM0JiNNOYqIdLN5+hyeayWAYSz04wrO1d8bGYqknEW+niwyymWvEP/1All8uHRdhS03ozxJFeF4cTxMGVQxLOqBARUEKzbTBGFBdX6IJ0ggrHSJNV2MvVzDKuk3G/Zlo3l3VW+3yoqq4BScgQtgg2vQBjegC3oAg0fwDF7Aq/FkvBnvxsditGKUO8fgD4zPH8Gpnfs=</latexit>ˆd21<latexit sha1_base64="/nKRVIb+wwcGaNuyb9PPB7nyDzA=">AAACBnicbVDLSsNAFJ3UV62vqks3g0VwVZIq2GXBjcsKthXaWCaTSTt0MhNmboQSuvcH3OofuBO3/oY/4Hc4abPQ1gMXDufcFydIBDfgul9OaW19Y3OrvF3Z2d3bP6geHnWNSjVlHaqE0vcBMUxwyTrAQbD7RDMSB4L1gsl17vcemTZcyTuYJsyPyUjyiFMCVuoPxgSycDb0HhrDas2tu3PgVeIVpIYKtIfV70GoaBozCVQQY/qem4CfEQ2cCjarDFLDEkInZMT6lkoSM+Nn85dn+MwqIY6UtiUBz9XfExmJjZnGge2MCYzNspeL/3qhyRcuXYeo6WdcJikwSRfHo1RgUDjPBIdcMwpiagmhmtv/MR0TTSjY5Co2GG85hlXSbdS9i3rj9rLWahYRldEJOkXnyENXqIVuUBt1EEUKPaMX9Oo8OW/Ou/OxaC05xcwx+gPn8wdqZJl0</latexit>ˆd2CM<latexit sha1_base64="B/N0upGd5GE5FqlQtZ78az+zfRo=">AAACFnicbVDLSsNAFJ34rPUVdSVuBovgqiRVsMtCN26ECvYBTQyTyaQdOpmEmYlQQvA3/AG3+gfuxK1bf8DvcNJmoa0HBg7n3NccP2FUKsv6MlZW19Y3Nitb1e2d3b198+CwJ+NUYNLFMYvFwEeSMMpJV1HFyCARBEU+I31/0i78/gMRksb8Tk0T4kZoxGlIMVJa8sxjZ4xUFuRe1vacCKkxRiy7yfP7hmfWrLo1A1wmdklqoETHM7+dIMZpRLjCDEk5tK1EuRkSimJG8qqTSpIgPEEjMtSUo4hIN5t9IYdnWglgGAv9uIIz9XdHhiIpp5GvK4sr5aJXiP96gSwGLmxXYdPNKE9SRTieLw9TBlUMi4xgQAXBik01QVhQfT/EYyQQVjrJqg7GXoxhmfQadfui3ri9rLWaZUQVcAJOwTmwwRVogWvQAV2AwSN4Bi/g1Xgy3ox342NeumKUPUfgD4zPH7SooBU=</latexit>b1<latexit sha1_base64="gDkYxWJ3p16Y/ECTdAJ4ayGkePM=">AAACGnicbVDLSsNAFJ34rPVVdSlCsBVclaQu7LKgC5cV7AOaECaT23bo5MHMjVJCV/6GP+BW/8CduHXjD/gdTtoutPXAwOGc+5rjJ4IrtKwvY2V1bX1js7BV3N7Z3dsvHRy2VZxKBi0Wi1h2fapA8AhayFFAN5FAQ19Axx9d5X7nHqTicXSH4wTckA4i3ueMopa80knFeeABDClmTkhxGKjMnkw85xoE0opXKltVawpzmdhzUiZzNL3StxPELA0hQiaoUj3bStDNqETOBEyKTqogoWxEB9DTNKIhKDebfmNinmklMPux1C9Cc6r+7shoqNQ49HVlfqpa9HLxXy9Q+cCF7divuxmPkhQhYrPl/VSYGJt5TmbAJTAUY00ok1zfb7IhlZShTrOog7EXY1gm7VrVvqjWbmvlRn0eUYEck1NyTmxySRrkhjRJizDySJ7JC3k1now34934mJWuGPOeI/IHxucPWJqhbw==</latexit>b1<latexit sha1_base64="gDkYxWJ3p16Y/ECTdAJ4ayGkePM=">AAACGnicbVDLSsNAFJ34rPVVdSlCsBVclaQu7LKgC5cV7AOaECaT23bo5MHMjVJCV/6GP+BW/8CduHXjD/gdTtoutPXAwOGc+5rjJ4IrtKwvY2V1bX1js7BV3N7Z3dsvHRy2VZxKBi0Wi1h2fapA8AhayFFAN5FAQ19Axx9d5X7nHqTicXSH4wTckA4i3ueMopa80knFeeABDClmTkhxGKjMnkw85xoE0opXKltVawpzmdhzUiZzNL3StxPELA0hQiaoUj3bStDNqETOBEyKTqogoWxEB9DTNKIhKDebfmNinmklMPux1C9Cc6r+7shoqNQ49HVlfqpa9HLxXy9Q+cCF7divuxmPkhQhYrPl/VSYGJt5TmbAJTAUY00ok1zfb7IhlZShTrOog7EXY1gm7VrVvqjWbmvlRn0eUYEck1NyTmxySRrkhjRJizDySJ7JC3k1now34934mJWuGPOeI/IHxucPWJqhbw==</latexit>b⇥<latexit sha1_base64="5rKBV9G1ak+LnP3Hp02BoVHsI7A=">AAACD3icbVBLTsMwFHTKr5RfKEs2ES0SqyopC1hWsGFZJPqRmqhyHKe16jiR/QJUUQ/BBdjCDdghthyBC3AOnDYLaBnJ0mjmPc/T+AlnCmz7yyitrW9sbpW3Kzu7e/sH5mG1q+JUEtohMY9l38eKciZoBxhw2k8kxZHPac+fXOd+755KxWJxB9OEehEeCRYygkFLQ7Nadx9YQMcYMhdYRNWsPjRrdsOew1olTkFqqEB7aH67QUzSiAogHCs1cOwEvAxLYITTWcVNFU0wmeARHWgqsI7xsvntM+tUK4EVxlI/AdZc/b2R4UipaeTryQjDWC17ufivF6j8w6V0CC+9jIkkBSrIIjxMuQWxlZdjBUxSAnyqCSaS6fstMsYSE9AVVnQxznINq6TbbDjnjeZts9a6Kioqo2N0gs6Qgy5QC92gNuoggh7RM3pBr8aT8Wa8Gx+L0ZJR7ByhPzA+fwC18Zze</latexit>bf(x)<latexit sha1_base64="LY5VQBt2Xs9WRrZ7EQ5DF/pfpSU=">AAACFnicbVC7TsMwFHV4lvIKMCEWixapLFVSBhgrWBiLRB9SE1WO47RWHSeyHaCKIn6DH2CFP2BDrKz8AN+B02aAliNZOjrnvny8mFGpLOvLWFpeWV1bL22UN7e2d3bNvf2OjBKBSRtHLBI9D0nCKCdtRRUjvVgQFHqMdL3xVe5374iQNOK3ahITN0RDTgOKkdLSwDysOvfUJyOk0iCrOSFSIy9IH7LT6sCsWHVrCrhI7IJUQIHWwPx2/AgnIeEKMyRl37Zi5aZIKIoZycpOIkmM8BgNSV9TjkIi3XT6hQyeaMWHQST04wpO1d8dKQqlnISersxvlPNeLv7r+TIfOLddBRduSnmcKMLxbHmQMKgimGcEfSoIVmyiCcKC6vshHiGBsNJJlnUw9nwMi6TTqNtn9cZNo9K8LCIqgSNwDGrABuegCa5BC7QBBo/gGbyAV+PJeDPejY9Z6ZJR9ByAPzA+fwDg2p+b</latexit>ef1<latexit sha1_base64="GsA17F4fN5nHel44YuT3aQTycoU=">AAACDHicbVDLSsNAFJ3UV62PRl26GSyCq5JUwS4LblxWsA9oQ5lMbtqhk0mYmSgl9Bf8Abf6B+7Erf/gD/gdTtostPXAhcM598XxE86Udpwvq7SxubW9U96t7O0fHFbto+OuilNJoUNjHsu+TxRwJqCjmebQTySQyOfQ86c3ud97AKlYLO71LAEvImPBQkaJNtLIrg4fWQCa8QCycD5yR3bNqTsL4HXiFqSGCrRH9vcwiGkagdCUE6UGrpNoLyNSM8phXhmmChJCp2QMA0MFiUB52eLxOT43SoDDWJoSGi/U3xMZiZSaRb7pjIieqFUvF//1ApUvXLmuw6aXMZGkGgRdHg9TjnWM82RwwCRQzWeGECqZ+R/TCZGEapNfxQTjrsawTrqNuntZb9xd1VrNIqIyOkVn6AK56Bq10C1qow6iKEXP6AW9Wk/Wm/VufSxbS1Yxc4L+wPr8AaS3m70=</latexit>efCM<latexit sha1_base64="D7+S+hctqX0vuLu8b8jOjuEFUzk=">AAACGnicbVDLSsNAFJ34rPUVdSlCsAiuSlIFuyx040aoYB/QhDCZ3LRDJw9mJkoJWfkb/oBb/QN34taNP+B3OGmz0NYDA4dz7muOlzAqpGl+aSura+sbm5Wt6vbO7t6+fnDYE3HKCXRJzGI+8LAARiPoSioZDBIOOPQY9L1Ju/D798AFjaM7OU3ACfEoogElWCrJ1U/sB+qDpMyHLMjdrO3aIZZjgll2k+euXjPr5gzGMrFKUkMlOq7+bfsxSUOIJGFYiKFlJtLJMJeUMMirdiogwWSCRzBUNMIhCCebfSM3zpTiG0HM1YukMVN/d2Q4FGIaeqqyuFEseoX4r+eLYuDCdhk0nYxGSSohIvPlQcoMGRtFToZPORDJpopgwqm63yBjzDGRKs2qCsZajGGZ9Bp166LeuL2stZplRBV0jE7RObLQFWqha9RBXUTQI3pGL+hVe9LetHftY166opU9R+gPtM8fjP6iLQ==</latexit>b⇥⇣(efii) (b1bd2i)⌘<latexit sha1_base64="3/H7GEKCN7N78z+GWJf39FkbtaM=">AAACcHicbVHbattAEF2pN8e9xG1fAn3Itk7BDsFI7kPzGGge+phCnASyrlitRtaQ1YXdUYsR+tD8QD4gP9CuHIe2TgcWDmfO2dk5G1caLQXBtec/evzk6bPeVv/5i5evtgev35zZsjYKZqrUpbmIpQWNBcwIScNFZUDmsYbz+OpL1z//AcZiWZzSsoJ5LhcFpqgkOSoa2D3xExPIJDWCMAfbcqEhpdFoxRPqBJq0jZALhUZxUWUY4fiAj/74cklZYpuwbSNxDJrkvfZekTj/9+lYGFxkNN6LBsNgEqyKPwThGgzZuk6iwY1ISlXnUJDS0trLMKho3khDqDS0fVFbqKS6kgu4dLCQbo95swqn5R8dk/C0NO4UxFfs345G5tYu89gpu0XsZq8j/9tLbHfhxnRKD+cNFlVNUKi74WmtOZW8S58naECRXjoglUH3fq4yaaQi90d9F0y4GcNDcDadhJ8m02/T4dHhOqIee8c+sBEL2Wd2xL6yEzZjil2zX17P2/Ju/R1/139/J/W9tect+6f8/d+/jb57</latexit>...<latexit sha1_base64="/4tyr2CRglXNS7iGErVDHQ3QOrY=">AAACDHicbVDLSsNAFJ3UV62PRl26GSyCq5JUQZdFNy4r2Ac0pUwmk3boZBJmbgol9Bf8Abf6B+7Erf/gD/gdTtostPXAwOGce+cejp8IrsFxvqzSxubW9k55t7K3f3BYtY+OOzpOFWVtGotY9XyimeCStYGDYL1EMRL5gnX9yV3ud6dMaR7LR5glbBCRkeQhpwSMNLSrXkRg7IeZNw1i0POhXXPqzgJ4nbgFqaECraH97QUxTSMmgQqidd91EhhkRAGngs0rXqpZQuiEjFjfUEkipgfZIvgcnxslwGGszJOAF+rvjYxEWs8i30zmMfWql4v/eoHOP1y5DuHNIOMySYFJujwepgJDjPNmcMAVoyBmhhCquMmP6ZgoQsH0VzHFuKs1rJNOo+5e1hsPV7XmbVFRGZ2iM3SBXHSNmugetVAbUZSiZ/SCXq0n6816tz6WoyWr2DlBf2B9/gAiAZwU</latexit>...<latexit sha1_base64="/4tyr2CRglXNS7iGErVDHQ3QOrY=">AAACDHicbVDLSsNAFJ3UV62PRl26GSyCq5JUQZdFNy4r2Ac0pUwmk3boZBJmbgol9Bf8Abf6B+7Erf/gD/gdTtostPXAwOGce+cejp8IrsFxvqzSxubW9k55t7K3f3BYtY+OOzpOFWVtGotY9XyimeCStYGDYL1EMRL5gnX9yV3ud6dMaR7LR5glbBCRkeQhpwSMNLSrXkRg7IeZNw1i0POhXXPqzgJ4nbgFqaECraH97QUxTSMmgQqidd91EhhkRAGngs0rXqpZQuiEjFjfUEkipgfZIvgcnxslwGGszJOAF+rvjYxEWs8i30zmMfWql4v/eoHOP1y5DuHNIOMySYFJujwepgJDjPNmcMAVoyBmhhCquMmP6ZgoQsH0VzHFuKs1rJNOo+5e1hsPV7XmbVFRGZ2iM3SBXHSNmugetVAbUZSiZ/SCXq0n6816tz6WoyWr2DlBf2B9/gAiAZwU</latexit>...<latexit sha1_base64="/4tyr2CRglXNS7iGErVDHQ3QOrY=">AAACDHicbVDLSsNAFJ3UV62PRl26GSyCq5JUQZdFNy4r2Ac0pUwmk3boZBJmbgol9Bf8Abf6B+7Erf/gD/gdTtostPXAwOGce+cejp8IrsFxvqzSxubW9k55t7K3f3BYtY+OOzpOFWVtGotY9XyimeCStYGDYL1EMRL5gnX9yV3ud6dMaR7LR5glbBCRkeQhpwSMNLSrXkRg7IeZNw1i0POhXXPqzgJ4nbgFqaECraH97QUxTSMmgQqidd91EhhkRAGngs0rXqpZQuiEjFjfUEkipgfZIvgcnxslwGGszJOAF+rvjYxEWs8i30zmMfWql4v/eoHOP1y5DuHNIOMySYFJujwepgJDjPNmcMAVoyBmhhCquMmP6ZgoQsH0VzHFuKs1rJNOo+5e1hsPV7XmbVFRGZ2iM3SBXHSNmugetVAbUZSiZ/SCXq0n6816tz6WoyWr2DlBf2B9/gAiAZwU</latexit>...<latexit sha1_base64="/4tyr2CRglXNS7iGErVDHQ3QOrY=">AAACDHicbVDLSsNAFJ3UV62PRl26GSyCq5JUQZdFNy4r2Ac0pUwmk3boZBJmbgol9Bf8Abf6B+7Erf/gD/gdTtostPXAwOGce+cejp8IrsFxvqzSxubW9k55t7K3f3BYtY+OOzpOFWVtGotY9XyimeCStYGDYL1EMRL5gnX9yV3ud6dMaR7LR5glbBCRkeQhpwSMNLSrXkRg7IeZNw1i0POhXXPqzgJ4nbgFqaECraH97QUxTSMmgQqidd91EhhkRAGngs0rXqpZQuiEjFjfUEkipgfZIvgcnxslwGGszJOAF+rvjYxEWs8i30zmMfWql4v/eoHOP1y5DuHNIOMySYFJujwepgJDjPNmcMAVoyBmhhCquMmP6ZgoQsH0VzHFuKs1rJNOo+5e1hsPV7XmbVFRGZ2iM3SBXHSNmugetVAbUZSiZ/SCXq0n6816tz6WoyWr2DlBf2B9/gAiAZwU</latexit>y satisfying |x| ≤ C and |y| ≤ C  we have |(cid:98)×(x  y) − xy| ≤ ; 2). The depth and the weight

parameters of the network is no more than c log C2

 with an absolute constant c.

The ReLU network in Theorem 1 is constructed in the following 5 steps.
Step 1. Construction of an atlas. Denote the open Euclidean ball with center c and radius r in RD
by B(c  r). For any r  the collection {B(x  r)}x∈M is an open cover of M. Since M is compact 
there exists a ﬁnite collection of points ci for i = 1  . . .   CM such that M ⊂(cid:83)i B(ci  r).
Now we pick the radius r < τ /2 so that Ui = M∩B(ci  r) is diffeomorphic3 to a ball in Rd (Niyogi
et al.  2008). Let {(Ui  φi)}CMi=1 be an atlas on M  where φi is to be deﬁned in Step 2. The number
of charts CM is upper bounded by

CM ≤(cid:24) SA(M)

rd

Td(cid:25)  

e

(cid:46) Td ≤ d log d + d log log d + 5d.

where SA(M ) is the surface area of M  and Td is the thickness4 of the Ui’s.
Remark 2. The thickness Td scales approximately linear in d. As shown in Conway et al. (1987) 
√
there exists covering with d
e
Step 2. Projection with rescaling and translation. We denote the tangent space at ci as Tci(M) =
span(vi1  . . .   vid)  where {vi1  . . .   vid} form an orthonormal basis. We obtain the matrix Vi =
[vi1  . . .   vid] ∈ RD×d by concatenating vij’s as column vectors.
Deﬁne φi(x) = bi(V (cid:62)
i (x − ci) + si) ∈ [0  1]d for any x ∈ Ui  where bi ∈ (0  1] is a scaling factor
and si is a translation vector. Since Ui is bounded  we can choose proper bi and si to guarantee
φi(x) ∈ [0  1]d. We rescale and translate the projection to ease the notation for the development of
local Taylor approximations in Step 4. We also remark that each φi is a linear function  and can be
realized by a single-layer linear network.
Step 3. Chart determination. This step is to locate the charts that a given input x belongs to. This
avoids projecting x using unmatched charts (i.e.  x (cid:54)∈ Uj for some j) as illustrated in Figure 3.
Proper charts5 can be determined by compositing an indi-
cator function and the squared Euclidean distance d2
i (x) =
(cid:107)x − ci(cid:107)2
j=1(xj − ci j)2 for i = 1  . . .   CM. The
squared distance d2
i (x) is a sum of univariate quadratic
functions  thus  we can apply Lemma 1 to approximate
i (x) by ReLU networks. Denote(cid:98)hsq as an approximation
d2
of the quadratic function x2 on [0  1] with an approxima-
tion error ν. Then we deﬁne

2 =(cid:80)D

i − d2

xj − ci j

is (cid:107)(cid:98)d2

j=1(cid:98)hsq(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)

2B (cid:12)(cid:12)(cid:12)(cid:12)(cid:19) .

as an approximation of d2

i (x) = 4B2(cid:88)D
(cid:98)d2
i (x). The approximation error
i(cid:107)∞ ≤ 4B2Dν  by the triangle inequality. We
next consider an approximation of the indicator function
of an interval as in Figure 4:
(cid:98)1∆(a) =

∆−8B2mν a + r2−4B2mν
∆−8B2mν

1
−
0

1

Figure 3: Projecting xj using a matched
chart (blue) (Uj  φj)  and an unmatched
chart (green) (Ui  φi).

a ≤ r2 − ∆ + 4B2mν
a ∈ [r2 − ∆ + 4B2mν  r2 − 4B2mν]
a > r2 − 4B2mν

 

(2)

where ∆ (∆ ≥ 8B2mν) will be chosen later according to the accuracy . Note that(cid:98)1∆ can be
∆−8B2mν ReLU(−a + r2 −
implemented exactly by a single layer ReLU network: (cid:98)1∆(a) =
3P is diffeomorphic to Q if there is a mapping Γ : P (cid:55)→ Q bijective  C∞  and its inverse also being C∞.
4Thickness is the average number of Ui’s that contain a point on M (Conway et al.  1987).
5Note that an input x can belong to multiple charts. Accordingly  the chart determination sub-network

1

determines all these charts.

6

UnmatchedUj<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>Ui<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>M<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>xi<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>xj<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>i(xj)<latexit sha1_base64="2eflpXWEkbK8brDfMMTRWCnJH98=">AAACEHicbVC7TsMwFHV4lvIqMLJEtIiyVEkZYKzEwlgk+pCaKnLcm9bUeci+Qa2ifAILv8LCAEKsjGz8De5jgJYjWTo65177+Hix4Aot69tYWV1b39jMbeW3d3b39gsHh00VJZJBg0Uikm2PKhA8hAZyFNCOJdDAE9DyhtcTv/UAUvEovMNxDN2A9kPuc0ZRS27hzEEY4fSeVEIvS0tOPOAuLzsBxYHnp6PMvT8vZW6haFWsKcxlYs9JkcxRdwtfTi9iSQAhMkGV6thWjN2USuRMQJZ3EgUxZUPah46mIQ1AddNpkMw81UrP9COpT4jmVP29kdJAqXHg6clJTLXoTcT/vE6C/lU35WGcIIRs9pCfCBMjc9KO2eMSGIqxJpRJrrOabEAlZag7zOsS7MUvL5NmtWJfVKzbarFWnteRI8fkhJSJTS5JjdyQOmkQRh7JM3klb8aT8WK8Gx+z0RVjvnNE/sD4/AG4rJ2H</latexit>i(xi)<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>j(xj)<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>1

4B2mν) −
function on Ui: if x (cid:54)∈ Ui  i.e.  d2
we have(cid:98)1∆ ◦(cid:98)d2

∆−8B2mν ReLU(−a + r2 − ∆ + 4B2mν). We use(cid:98)1∆ ◦(cid:98)d2

i (x) ≥ r2  we have(cid:98)1∆ ◦(cid:98)d2

i (x) = 1.

i (x) = 0; if x ∈ Ui and d2

i to approximate the indicator
i (x) ≤ r2 − ∆ 

Figure 4: The approximation of the indicator function(cid:98)1∆ in (2).

2  

−1
i

−1
i

−1
i

−1
i

−1
i

(cid:12)(cid:12)(cid:12)Dn(fi ◦ φ

Step 4. Taylor approximation. In each chart (Ui  φi)  we locally approximate f using Taylor
polynomials of order n. Speciﬁcally  we decompose f as f =(cid:80)CM
i=1 fi with fi = f ρi where ρi is
an element in a C∞ partition of unity on M which is supported inside Ui. The existence of such a
partition of unity is guaranteed by Proposition 1. Since M is compact and ρi is C∞  fi preserves the
regularity (smoothness) of f such that fi ∈ H n α for i = 1  . . .   CM.
Lemma 2. Suppose Assumption 3 holds. For i = 1  . . .   CM  the function fi belongs to H n α: there
exists a Hölder coefﬁcient Li depending on d  fi  and φi such that for any |n| = n  we have

)(cid:12)(cid:12)φi(x1) − Dn(fi ◦ φ

)(cid:12)(cid:12)φi(x2)(cid:12)(cid:12)(cid:12) ≤ Li (cid:107)φi(x1) − φi(x2)(cid:107)α
∀x1  x2 ∈ Ui.
Proof Sketch. We provide a sketch here. Details can be found in Appendix B.1. Denote g1 = f ◦ φ
) = Dn(g1 × g2) =(cid:80)|p|+|q|=n(cid:0) n|p|(cid:1)Dpg1Dqg2  by the
and g2 = ρi ◦ φ
Leibniz rule. Consider each term in the sum: for any x1  x2 ∈ Ui 
(cid:12)(cid:12)Dpg1Dqg2|φi(x1) − Dpg1Dqg2|φi(x2)(cid:12)(cid:12)
≤ |Dpg1(φi(x1))|(cid:12)(cid:12)Dqg2|φi(x1) − Dqg2|φi(x2)(cid:12)(cid:12) + |Dqg2(φi(x2))|(cid:12)(cid:12)Dpg1|φi(x1) − Dpg1|φi(x2)(cid:12)(cid:12)
(cid:12)(cid:12)Dqg2|φi(x1) − Dqg2|φi(x2)(cid:12)(cid:12) ≤
=√dµi (cid:107)φi(x1) − φi(x2)(cid:107)1−α

≤ λiθi α (cid:107)φi(x1) − φi(x2)(cid:107)α
Here λi and µi are uniform upper bounds on the derivatives of g1 and g2 with order up to n 
respectively. The last inequality above is derived as follows: by the mean value theorem  we have

√dµi (cid:107)φi(x1) − φi(x2)(cid:107)2
(cid:107)φi(x1) − φi(x2)(cid:107)α

√dµi(2r)1−α (cid:107)φi(x1) − φi(x2)(cid:107)α

2 + µiβi α (cid:107)φi(x1) − φi(x2)(cid:107)α
2 .

. We have Dn(fi ◦ φ

where the last inequality is due to the fact that (cid:107)φi(x1) − φi(x2)(cid:107)2 ≤ bi (cid:107)Vi(cid:107)(cid:107)x1 − x2(cid:107)2 ≤ 2r.
Then we set θi α = √dµi(2r)1−α and by a similar argument  we set βi α = √dλi(2r)1−α. We
complete the proof by taking Li = 2n+1√dλiµi(2r)1−α.

2 ≤

2

2  

−1
i

by Taylor
Lemma 2 is crucial for the error estimation in the local approximation of fi ◦ φ
polynomials. This error estimate is given in the following theorem  where some of the proof
techniques are from Theorem 1 in Yarotsky (2017).
Theorem 3. Let fi = f ρi as in Step 4. For any δ ∈ (0  1)  there exists a ReLU network structure
that  if the weight parameters are properly chosen  the network yields an approximation of fi ◦
δ + 1(cid:1) layers  and at most
−1
φ
i
− d
c(cid:48)δ

uniformly with error δ. Such a network has no more than c(cid:0)log 1
n+α(cid:0)log 1
Proof Sketch. The detailed proof is provided in Appendix B.2. The proof consists of two steps: 1).
using a weighted sum of Taylor polynomials; 2). Implement the weighted sum
Approximate fi ◦ φ
of Taylor polynomials using ReLU networks. Speciﬁcally  we set up a uniform grid and divide [0  1]d

δ + 1(cid:1) neurons and weight parameters with c  c(cid:48) depending on n  d  fi ◦ φ

−1
i

−1
i

.

7

b1(·)<latexit sha1_base64="HXftc2Cui61ctbquLpD0UV3zHxc=">AAACI3icbVC7TgJBFJ31ifhCLW02ggYbsouFlCRaWGIij4QlZHb2AhNmH5m5qyGb/QN/wx+w1T+wMzYWtn6Hs0Ch4EkmOTnnvua4keAKLevTWFldW9/YzG3lt3d29/YLB4ctFcaSQZOFIpQdlyoQPIAmchTQiSRQ3xXQdsdXmd++B6l4GNzhJIKeT4cBH3BGUUv9wlnJeeAejCgmjk9x5KnETtN+4lyDQJqWHeaFeF7qF4pWxZrCXCb2nBTJHI1+4dvxQhb7ECATVKmubUXYS6hEzgSkeSdWEFE2pkPoahpQH1Qvmf4nNU+14pmDUOoXoDlVf3ck1Fdq4ru6MrtZLXqZ+K/nqWzgwnYc1HoJD6IYIWCz5YNYmBiaWWCmxyUwFBNNKJNc32+yEZWUoY41r4OxF2NYJq1qxb6oWLfVYr02jyhHjskJKRObXJI6uSEN0iSMPJJn8kJejSfjzXg3PmalK8a854j8gfH1A2ObpRY=</latexit>r2+4B2m⌫<latexit sha1_base64="H83lc/OQg9dVQdKA3cIukMDZq6A=">AAACFXicbVDdSsMwGE3n35x/VW8Eb4JDEMTRzoG7HOqFlxPcD6x1pGm6haVpSVJhlPkavoC3+gbeibde+wI+h+nWC908EDg55/tJjhczKpVlfRmFpeWV1bXiemljc2t7x9zda8soEZi0cMQi0fWQJIxy0lJUMdKNBUGhx0jHG11lfueBCEkjfqfGMXFDNOA0oBgpLfXNA3FfhWfONWEKwVNYu9TXEDo86Ztlq2JNAReJnZMyyNHsm9+OH+EkJFxhhqTs2Vas3BQJRTEjk5KTSBIjPEID0tOUo5BIN53+YAKPteLDIBL6cAWn6u+OFIVSjkNPV4ZIDeW8l4n/er7MBs5tV0HdTSmPE0U4ni0PEgZVBLOIoE8FwYqNNUFYUP1+iIdIIKx0kCUdjD0fwyJpVyv2eaV6Wys36nlERXAIjsAJsMEFaIAb0AQtgMEjeAYv4NV4Mt6Md+NjVlow8p598AfG5w+Rl50E</latexit>r24B2m⌫<latexit sha1_base64="N+JfnS3IlLvCk2A5cqFdOtjsqvI=">AAACDXicbVDLSsNAFL3xWesr6tLNYBHcWJJasMuiG5cV7APaWCaTSTt0Mgkzk0Ip/QZ/wK3+gTtx6zf4A36HkzYLbT0wzOGc++L4CWdKO86Xtba+sbm1Xdgp7u7tHxzaR8ctFaeS0CaJeSw7PlaUM0GbmmlOO4mkOPI5bfuj28xvj6lULBYPepJQL8IDwUJGsDZS37blYwVdouqN+SLUE2nfLjllZw60StyclCBHo29/94KYpBEVmnCsVNd1Eu1NsdSMcDor9lJFE0xGeEC7hgocUeVN55fP0LlRAhTG0jyh0Vz93THFkVKTyDeVEdZDtexl4r9eoLKBS9t1WPOmTCSppoIslocpRzpGWTQoYJISzSeGYCKZuR+RIZaYaBNg0QTjLsewSlqVsntVrtxXS/VaHlEBTuEMLsCFa6jDHTSgCQTG8Awv8Go9WW/Wu/WxKF2z8p4T+APr8wcfnpoj</latexit>1<latexit sha1_base64="yDly7JVMcgv3HqfblGFqmD3rPTw=">AAAB/HicbVDLSsNAFL3xWeur6tJNsAiuSlIFXbgouHHZgn1AG8pkctMOnUzCzEQoof6AW/0Dd+LWf/EH/A4nbRbaeuDC4Zz74vgJZ0o7zpe1tr6xubVd2inv7u0fHFaOjjsqTiXFNo15LHs+UciZwLZmmmMvkUgin2PXn9zlfvcRpWKxeNDTBL2IjAQLGSXaSC13WKk6NWcOe5W4BalCgeaw8j0IYppGKDTlRKm+6yTay4jUjHKclQepwoTQCRlh31BBIlReNn90Zp8bJbDDWJoS2p6rvycyEik1jXzTGRE9VsteLv7rBSpfuHRdhzdexkSSahR0cTxMua1jO0/CDphEqvnUEEIlM//bdEwkodrkVTbBuMsxrJJOveZe1uqtq2rjtoioBKdwBhfgwjU04B6a0AYKCM/wAq/Wk/VmvVsfi9Y1q5g5gT+wPn8Ar5uVMA==</latexit>0<latexit sha1_base64="vj9MW1UuRA9PfcrXAcSg3Nd1Cig=">AAAB/HicbVDLSsNAFL3xWeur6tJNsAiuSlIFXbgouHHZgn1AG8pkctMOnUzCzEQoof6AW/0Dd+LWf/EH/A4nbRbaeuDC4Zz74vgJZ0o7zpe1tr6xubVd2inv7u0fHFaOjjsqTiXFNo15LHs+UciZwLZmmmMvkUgin2PXn9zlfvcRpWKxeNDTBL2IjAQLGSXaSC1nWKk6NWcOe5W4BalCgeaw8j0IYppGKDTlRKm+6yTay4jUjHKclQepwoTQCRlh31BBIlReNn90Zp8bJbDDWJoS2p6rvycyEik1jXzTGRE9VsteLv7rBSpfuHRdhzdexkSSahR0cTxMua1jO0/CDphEqvnUEEIlM//bdEwkodrkVTbBuMsxrJJOveZe1uqtq2rjtoioBKdwBhfgwjU04B6a0AYKCM/wAq/Wk/VmvVsfi9Y1q5g5gT+wPn8ArgOVLw==</latexit>−1
i

by its n-th order Taylor polynomial in each cube. To

into small cubes  and then approximate fi ◦ φ
implement such polynomials by ReLU networks  we recursively apply the multiplication(cid:98)× operator
in Corollary 1  since these polynomials are sums of the products of different variables.
Step 5. Estimating the total error. We have collected all the ingredients to implement the entire
ReLU network to approximate f on M. Recall that the network structure consists of 3 main sub-
networks as demonstrated in Figure 2. Let(cid:98)× be an approximation to the multiplication operator in
the pairing sub-network with error η. Accordingly  the function given by the whole network is

using Taylor polynomials in Theorem 3. The total error

i=1 (Ai 1 + Ai 2 + Ai 3)  where

(cid:98)f =

CM(cid:88)i=1(cid:98)×((cid:98)fi (cid:98)1∆ ◦(cid:98)d2

i ) with (cid:98)fi = (cid:101)fi ◦ φi 
where (cid:101)fi is the approximation of fi ◦ φ
Theorem 4. For any i = 1  . . .   CM  we have (cid:107)(cid:98)f − f(cid:107)∞ ≤(cid:80)CM

can be decomposed to three components according to the following theorem.

−1
i

Ai 1 =(cid:13)(cid:13)(cid:98)×((cid:98)fi (cid:98)1∆ ◦(cid:98)d2
Ai 2 =(cid:13)(cid:13)(cid:98)fi × ((cid:98)1∆ ◦(cid:98)d2
Ai 3 =(cid:13)(cid:13)fi × ((cid:98)1∆ ◦(cid:98)d2

i )(cid:13)(cid:13)∞ ≤ η 
i ) − (cid:98)fi × ((cid:98)1∆ ◦(cid:98)d2
i )(cid:13)(cid:13)∞ ≤ δ 
i ) − fi × ((cid:98)1∆ ◦(cid:98)d2
i ) − fi × 1(x ∈ Ui)(cid:13)(cid:13)∞ ≤

c(π + 1)
r(1 − r/τ )

∆ for some constant c.

in the Taylor approximation sub-network  and Ai 3 is the error from the chart determination sub-

Here 1(x ∈ Ui) is the indicator function on Ui. Theorem 4 is proved in Appendix B.3. In order
to achieve an  total approximation error  i.e.  (cid:107)f − (cid:98)f(cid:107)∞ ≤   we need to control the errors in
the three sub-networks. In other words  we need to decide ν for (cid:98)d2
i   ∆ for(cid:98)1∆  δ for (cid:101)fi  and η
for (cid:98)×. Note that Ai 1 is the error from the pairing sub-network  Ai 2 is the approximation error
network. The error bounds on Ai 1  Ai 2 are straightforward from the constructions of(cid:98)× and (cid:98)fi. The
estimate of Ai 3 involves some technical analysis since (cid:107)(cid:98)1∆ ◦ (cid:98)d2
i − 1(x ∈ Ui)(cid:107)∞ = 1. Note that
(cid:98)1∆ ◦ (cid:98)d2
2 > r2  so we only need
to prove that |fi(x)| is sufﬁciently small in the region Ki deﬁned below.
Lemma 3. For any i = 1  . . .   CM  denote Ki = {x ∈ M : r2 − ∆ ≤ (cid:107)x − ci(cid:107)2
2 ≤ r2}. Then
there exists a constant c depending on fi’s and φi’s such that
c(π + 1)
r(1 − r/τ )

i (x) − 1(x ∈ Ui) = 0 whenever (cid:107)x − ci(cid:107)2

2 < r2 − ∆ or (cid:107)x − ci(cid:107)2

x∈Ki |fi(x)| ≤
max

∆.

Proof Sketch. The detailed proof is in Appendix B.4. The function fi ◦ φ
is deﬁned on φi(Ui) ⊂
[0  1]d. We extend fi ◦ φ
(x) = 0 for x ∈ [0  1]d \ φi(Ui). It is easy to
verify that such an extension preserves the regularity of fi ◦ φ
  since supp(fi) is a compact subset
of Ui. By the mean value theorem  for any x  y ∈ Ki  there exists z = βφi(x) + (1 − β)φi(y) for
some β ∈ (0  1) such that

to [0  1]d by letting fi ◦ φ

−1
i

−1
i

−1
i

|fi(x) − fi(y)| ≤ (cid:107)∇fi ◦ φ

−1
i

(z)(cid:107)2(cid:107)φi(x) − φi(y)(cid:107)2 ≤ (cid:107)∇fi ◦ φ

(z)(cid:107)2bi(cid:107)Vi(cid:107)2(cid:107)x − y(cid:107)2.

−1
i

−1
i

−1
i

We pick y ∈ ∂Ui (the boundary of Ui) so that fi(y) = 0. Since fi ∈ H n α and M is compact 
(cid:13)(cid:13)∇fi ◦ φ
(z)(cid:13)(cid:13)2 bi (cid:107)Vi(cid:107)2 ≤ c for some c > 0. To bound |fi(x)|  the key is to estimate (cid:107)x − y(cid:107)2.
We next prove that  for any x ∈ Ki  there exists y ∈ ∂Ui satisfying (cid:107)x − y(cid:107)2 ≤ π+1
The idea is to consider a geodesic6 γ(t) parameterized by the arc length from x to ∂Ui in Figure
5. Denote y = ∂Ui(cid:84) γ. Without loss of generality  we shift the center ci to 0 in the following
analysis. To utilize polar coordinates  we deﬁne two auxiliary quantities: θ(t) = γ(t)(cid:62) ˙γ(t)/(cid:107)γ(t)(cid:107)2
and (cid:96)(t) = (cid:107)γ(t)(cid:107)2  where ˙γ denotes the derivative of γ.

r(1−r/τ ) ∆.

6A geodesic is the shortest path between two points on the manifold. We refer readers to Chapter 6 in Lee

(2006) for a formal introduction.

8

We show that there exists a geodesic γ(t) satisfying inf t ˙(cid:96)(t) ≥ 1−r/τ
π+1 > 0. This implies that the
geodesic continuously moves away from the center. Denote T such that γ(T ) = y. By the deﬁnition
of geodesic  T is the arc length of γ(t) between x and y. We have T inf t ˙(cid:96)(t) ≤ (cid:96)(T ) − (cid:96)(0) ≤
r − √r2 − ∆ ≤ ∆

r . Therefore  (cid:107)x − y(cid:107)2 ≤ T ≤

∆

r inf t ˙(cid:96)(t) ≤ π+1

r(1−r/τ ) ∆.

Given Theorem 4  we choose

η = δ =



3CM

and ∆ =

r(1 − r/τ )
3c(π + 1)CM

(3)

so that the approximation error is bounded by . Moreover 
16B2D to guarantee ∆ > 8B2Dν so that the
we choose ν = ∆

deﬁnition of(cid:98)1∆ is valid.

Finally we quantify the size of the ReLU network. Recall that
the chart determination sub-network has c1 log 1
ν layers  the
Taylor approximation sub-network has c2 log 1
δ layers  and the
pairing sub-network has c3 log 1
η layers. Here c2 depends on
d  n  f  and c1  c3 are absolute constants. Combining these with
(3) yields the depth in Theorem 1. By a similar argument  we
can obtain the number of neurons and weight parameters. A
detailed analysis is given in Appendix B.5.

Figure 5: A geometric illustration
of θ and (cid:96).

5 Discussions

ReLU activations. We consider neural networks with ReLU activations for a practical concern
— ReLU activations are widely used in deep networks. Moreover  ReLU networks are easier to
train compared with sigmoid or hyperbolic tangent activations  which are known for their notorious
vanishing gradient problem (Goodfellow et al.  2016; Glorot et al.  2011).
Low Dimensional Manifolds. The low dimensional manifold model plays a vital role to reduce the
network size. As shown in Theorem 2  to approximate functions in F n D with accuracy   the minimal
number of weight parameters is O(− D
n ). This lower bound is huge  and can not be improved without
low dimensional structures of data.
Existence vs. Learnability and Generalization. Our Theorem 1 shows the existence of a ReLU
network structure that gives efﬁcient approximations of functions on low dimensional manifolds  if
the weight parameters are properly chosen. In practice  it is observed that larger neural networks
are easier to train and yield better generalization performances (Li et al.  2018; Zhang et al.  2016;
Arora et al.  2018). This is referred to as overparameterization. Establishing the connection between
learnability and generalization is an important future direction.
Convolutional Filters. Convolutional neural networks (CNNs  Krizhevsky et al. (2012)) are widely
used in computer vision  language modeling  etc. Empirical results reveal that different convolutional
ﬁlters can capture various patterns in images  e.g.  edge detection ﬁlters. An interesting question is
whether convolutional ﬁlters serve as charts in our framework.
Equivalent Networks. The ReLU network identiﬁed in Theorem 1 is sparsely connected. Several
other network structures can yield the same function as our ReLU network. It is interesting to
investigate whether these network structures also possess the universal approximation property.

6 Acknowledgements

This work is supported by NSF grants DMS 1818751 and III 1717916. The authors would like to
thank Ryan Tibshirani for his helpful discussions and insightful comments.

References
AAMARI  E.  KIM  J.  CHAZAL  F.  MICHEL  B.  RINALDO  A.  WASSERMAN  L. ET AL. (2019).

Estimating the reach of a manifold. Electronic Journal of Statistics  13 1359–1399.

9

ci<latexit sha1_base64="KamUmlzpOAAMpq5bo1PKbKGaNcU=">AAACB3icbVDLSsNAFL2pr1pfVZduBovgqiRVsMuCG5cV7AOaUCaTSTt0MgkzE6GEfoA/4Fb/wJ249TP8Ab/DSZqFth4YOJxz79zD8RPOlLbtL6uysbm1vVPdre3tHxwe1Y9P+ipOJaE9EvNYDn2sKGeC9jTTnA4TSXHkczrwZ7e5P3ikUrFYPOh5Qr0ITwQLGcHaSK4bYT31w4wsxmxcb9hNuwBaJ05JGlCiO65/u0FM0ogKTThWauTYifYyLDUjnC5qbqpogskMT+jIUIEjqrysyLxAF0YJUBhL84RGhfp7I8ORUvPIN5N5RrXq5eK/XqDyD1eu67DtZUwkqaaCLI+HKUc6RnkpKGCSEs3nhmAimcmPyBRLTLSprmaKcVZrWCf9VtO5arburxuddllRFc7gHC7BgRvowB10oQcEEniGF3i1nqw36936WI5WrHLnFP7A+vwB8tOaWg==</latexit>x<latexit sha1_base64="SRl7GnPpXhXST9rQC7XJ9WdvmDg=">AAACBXicbVDLSsNAFL2pr1pfVZduBovgqiRVsMuCG5cV7APbUCaTSTt0MgkzE7GErv0Bt/oH7sSt3+EP+B1O0iy09cDA4Zx75x6OF3OmtG1/WaW19Y3NrfJ2ZWd3b/+genjUVVEiCe2QiEey72FFORO0o5nmtB9LikOP0543vc783gOVikXiTs9i6oZ4LFjACNZGuh+GWE+8IH2cj6o1u27nQKvEKUgNCrRH1e+hH5EkpEITjpUaOHas3RRLzQin88owUTTGZIrHdGCowCFVbponnqMzo/goiKR5QqNc/b2R4lCpWeiZySyhWvYy8V/PV9mHS9d10HRTJuJEU0EWx4OEIx2hrBLkM0mJ5jNDMJHM5EdkgiUm2hRXMcU4yzWskm6j7lzUG7eXtVazqKgMJ3AK5+DAFbTgBtrQAQICnuEFXq0n6816tz4WoyWr2DmGP7A+fwCCypmT</latexit>y<latexit sha1_base64="wuTK6wRB5HjPTrVohu+9hGMAcfY=">AAACBXicbVDLSsNAFL2pr1pfVZduBovgqiS1YJcFNy4r2Ae2oUwmk3boZBJmJkIIXfsDbvUP3Ilbv8Mf8DuctFlo64GBwzn3zj0cL+ZMadv+skobm1vbO+Xdyt7+weFR9fikp6JEEtolEY/kwMOKciZoVzPN6SCWFIcep31vdpP7/UcqFYvEvU5j6oZ4IljACNZGehiFWE+9IEvn42rNrtsLoHXiFKQGBTrj6vfIj0gSUqEJx0oNHTvWboalZoTTeWWUKBpjMsMTOjRU4JAqN1sknqMLo/goiKR5QqOF+nsjw6FSaeiZyTyhWvVy8V/PV/mHK9d10HIzJuJEU0GWx4OEIx2hvBLkM0mJ5qkhmEhm8iMyxRITbYqrmGKc1RrWSa9Rd67qjbtmrd0qKirDGZzDJThwDW24hQ50gYCAZ3iBV+vJerPerY/laMkqdk7hD6zPH4RjmZQ=</latexit>(t)<latexit sha1_base64="Szqth62rUyn1jeubuKXYPXh1SkU=">AAACBHicbVDLTgIxFL2DL8QX6tJNIzHBDZlBE1mSuHGJiYAGJqRTOtDQdiZtx4RM2PoDbvUP3Bm3/oc/4HfYgVkoeJKbnJxzXzlBzJk2rvvlFNbWNza3itulnd29/YPy4VFHR4kitE0iHqn7AGvKmaRtwwyn97GiWAScdoPJdeZ3H6nSLJJ3ZhpTX+CRZCEj2FjpoT/CQuCqOR+UK27NnQOtEi8nFcjRGpS/+8OIJIJKQzjWuue5sfFTrAwjnM5K/UTTGJMJHtGepRILqv10/vAMnVlliMJI2ZIGzdXfEykWWk9FYDsFNmO97GXiv95QZwuXrpuw4adMxomhkiyOhwlHJkJZImjIFCWGTy3BRDH7PyJjrDAxNreSDcZbjmGVdOo176JWv72sNBt5REU4gVOoggdX0IQbaEEbCAh4hhd4dZ6cN+fd+Vi0Fpx85hj+wPn8AZe0mG8=</latexit>`(t)<latexit sha1_base64="lsLN4nrfHXemYZOEAbnu/H09rQY=">AAACAnicbVC7SgNBFL0bXzG+opY2g0GITdiNgikDNpYRzAOSJczO3k2GzD6YmRVCSOcP2Oof2ImtP+IP+B3OJlto4oELh3Pui+Mlgitt219WYWNza3unuFva2z84PCofn3RUnEqGbRaLWPY8qlDwCNuaa4G9RCINPYFdb3Kb+d1HlIrH0YOeJuiGdBTxgDOqjdQdoBBVfTksV+yavQBZJ05OKpCjNSx/D/yYpSFGmgmqVN+xE+3OqNScCZyXBqnChLIJHWHf0IiGqNzZ4t05uTCKT4JYmoo0Wai/J2Y0VGoaeqYzpHqsVr1M/NfzVbZw5boOGu6MR0mqMWLL40EqiI5JlgfxuUSmxdQQyiQ3/xM2ppIybVIrmWCc1RjWSadec65q9fvrSrORR1SEMziHKjhwA024gxa0gcEEnuEFXq0n6816tz6WrQUrnzmFP7A+fwAMMZeV</latexit>˙(t)<latexit sha1_base64="ryt9nYc0+rCuTWiJJj1UezMFWCw=">AAACDHicbVDLSsNAFJ34rPXRqEs3g0Wom5JUwS4LblxWsA9oQ5lMJu3QmSTM3Agl9Bf8Abf6B+7Erf/gD/gdTtostPXAhcM598XxE8E1OM6XtbG5tb2zW9or7x8cHlXs45OujlNFWYfGIlZ9n2gmeMQ6wEGwfqIYkb5gPX96m/u9R6Y0j6MHmCXMk2Qc8ZBTAkYa2ZVhEEM2HBMpybwGlyO76tSdBfA6cQtSRQXaI/vbbKCpZBFQQbQeuE4CXkYUcCrYvDxMNUsInZIxGxgaEcm0ly0en+MLowQ4jJWpCPBC/T2REan1TPqmUxKY6FUvF//1Ap0vXLkOYdPLeJSkwCK6PB6mAkOM82RwwBWjIGaGEKq4+R/TCVGEgsmvbIJxV2NYJ91G3b2qN+6vq61mEVEJnaFzVEMuukEtdIfaqIMoStEzekGv1pP1Zr1bH8vWDauYOUV/YH3+ADR1m3c=</latexit>✓(t)<latexit sha1_base64="IMzOhk8YSXLp/KDA5NVxEY3OvHk=">AAACBHicbVDLTgIxFO3gC/GFunTTSExwQ2bQRJYkblxiIqCBCemUDjS0nUl7x4RM2PoDbvUP3Bm3/oc/4HfYgVkoeJKbnJxzXzlBLLgB1/1yCmvrG5tbxe3Szu7e/kH58KhjokRT1qaRiPR9QAwTXLE2cBDsPtaMyECwbjC5zvzuI9OGR+oOpjHzJRkpHnJKwEoPfRgzIFU4H5Qrbs2dA68SLycVlKM1KH/3hxFNJFNABTGm57kx+CnRwKlgs1I/MSwmdEJGrGepIpIZP50/PMNnVhniMNK2FOC5+nsiJdKYqQxspyQwNsteJv7rDU22cOk6hA0/5SpOgCm6OB4mAkOEs0TwkGtGQUwtIVRz+z+mY6IJBZtbyQbjLcewSjr1mndRq99eVpqNPKIiOkGnqIo8dIWa6Aa1UBtRJNEzekGvzpPz5rw7H4vWgpPPHKM/cD5/ALZ1mII=</latexit>r<latexit sha1_base64="cVLdqOIMLzJ5kor4GghebSD0Oik=">AAAB/HicbVDLSsNAFL3xWeur6tLNYBFclaQKdllw47IF+4A2lMnkph06eTAzEUqoP+BW/8CduPVf/AG/w0mbhbYeuHA45744XiK40rb9ZW1sbm3v7Jb2yvsHh0fHlZPTropTybDDYhHLvkcVCh5hR3MtsJ9IpKEnsOdN73K/94hS8Th60LME3ZCOIx5wRrWR2nJUqdo1ewGyTpyCVKFAa1T5HvoxS0OMNBNUqYFjJ9rNqNScCZyXh6nChLIpHePA0IiGqNxs8eicXBrFJ0EsTUWaLNTfExkNlZqFnukMqZ6oVS8X//V8lS9cua6DhpvxKEk1Rmx5PEgF0THJkyA+l8i0mBlCmeTmf8ImVFKmTV5lE4yzGsM66dZrznWt3r6pNhtFRCU4hwu4AgduoQn30IIOMEB4hhd4tZ6sN+vd+li2bljFzBn8gfX5AxYOlW0=</latexit>pr2<latexit sha1_base64="Cg7KzyJpP54mbiO9Md+oZaWRLXY=">AAACEHicbVDLSsNAFJ34rPUV7dLNYBHcWJIq2GVBFy4r2Ac0sUwmk3bo5OHMjVBCf8IfcKt/4E7c+gf+gN/hpM1CWw9cOJxzXxwvEVyBZX0ZK6tr6xubpa3y9s7u3r55cNhRcSopa9NYxLLnEcUEj1gbOAjWSyQjoSdY1xtf5X73kUnF4+gOJglzQzKMeMApAS0NzIqjHiRk8r6Oz7BzzQSQ6cCsWjVrBrxM7IJUUYHWwPx2/JimIYuACqJU37YScDMigVPBpmUnVSwhdEyGrK9pREKm3Gz2/BSfaMXHQSx1RYBn6u+JjIRKTUJPd4YERmrRy8V/PV/lCxeuQ9BwMx4lKbCIzo8HqcAQ4zwd7HPJKIiJJoRKrv/HdEQkoaAzLOtg7MUYlkmnXrPPa/Xbi2qzUURUQkfoGJ0iG12iJrpBLdRGFE3QM3pBr8aT8Wa8Gx/z1hWjmKmgPzA+fwCVc5yz</latexit>AMODEI  D.  ANANTHANARAYANAN  S.  ANUBHAI  R.  BAI  J.  BATTENBERG  E.  CASE  C. 
CASPER  J.  CATANZARO  B.  CHENG  Q.  CHEN  G. ET AL. (2016). Deep speech 2: End-to-end
speech recognition in english and mandarin. In International conference on machine learning.

ARORA  S.  COHEN  N. and HAZAN  E. (2018). On the optimization of deep networks: Implicit

acceleration by overparameterization. arXiv preprint arXiv:1802.06509.

BAHDANAU  D.  CHO  K. and BENGIO  Y. (2014). Neural machine translation by jointly learning to

align and translate. arXiv preprint arXiv:1409.0473.

BARRON  A. R. (1993). Universal approximation bounds for superpositions of a sigmoidal function.

IEEE Transactions on Information theory  39 930–945.

BICKEL  P. J.  LI  B. ET AL. (2007). Local polynomial regression on unknown manifolds. In

Complex datasets and inverse problems. Institute of Mathematical Statistics  177–186.

CHUI  C. K. and LI  X. (1992). Approximation by ridge functions and neural networks with one

hidden layer. Journal of Approximation Theory  70 131–141.

CHUI  C. K. and MHASKAR  H. N. (2016). Deep nets for local manifold learning. arXiv preprint

arXiv:1607.07110.

COIFMAN  R. R. and MAGGIONI  M. (2006). Diffusion wavelets. Applied and Computational

Harmonic Analysis  21 53–94.

CONWAY  J. H.  SLOANE  N. J. A. and BANNAI  E. (1987). Sphere-packings  Lattices  and Groups.

Springer-Verlag  Berlin  Heidelberg.

CYBENKO  G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of

control  signals and systems  2 303–314.

DEVORE  R. A.  HOWARD  R. and MICCHELLI  C. (1989). Optimal nonlinear approximation.

Manuscripta mathematica  63 469–478.

FUNAHASHI  K.-I. (1989). On the approximate realization of continuous mappings by neural

networks. Neural networks  2 183–192.

GLOROT  X.  BORDES  A. and BENGIO  Y. (2011). Deep sparse rectiﬁer neural networks. In

Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics.

GOODFELLOW  I.  BENGIO  Y. and COURVILLE  A. (2016). Deep Learning. MIT Press. http:

//www.deeplearningbook.org.

GOODFELLOW  I.  POUGET-ABADIE  J.  MIRZA  M.  XU  B.  WARDE-FARLEY  D.  OZAIR  S. 
COURVILLE  A. and BENGIO  Y. (2014). Generative adversarial nets. In Advances in neural
information processing systems.

GRAVES  A.  MOHAMED  A.-R. and HINTON  G. (2013). Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics  speech and signal processing.
IEEE.

GU  S.  HOLLY  E.  LILLICRAP  T. and LEVINE  S. (2017). Deep reinforcement learning for robotic
manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on
robotics and automation (ICRA). IEEE.

GYÖRFI  L.  KOHLER  M.  KRZYZAK  A. and WALK  H. (2006). A distribution-free theory of

nonparametric regression. Springer Science & Business Media.

HANIN  B. (2017). Universal function approximation by deep neural nets with bounded width and

relu activations. arXiv preprint arXiv:1708.02691.

HORNIK  K. (1991). Approximation capabilities of multilayer feedforward networks. Neural

networks  4 251–257.

10

HU  J.  SHEN  L. and SUN  G. (2018). Squeeze-and-excitation networks. In Proceedings of the

IEEE conference on computer vision and pattern recognition.

IRIE  B. and MIYAKE  S. (1988). Capabilities of three-layered perceptrons. In IEEE International

Conference on Neural Networks  vol. 1.

JIANG  F.  JIANG  Y.  ZHI  H.  DONG  Y.  LI  H.  MA  S.  WANG  Y.  DONG  Q.  SHEN  H.
and WANG  Y. (2017). Artiﬁcial intelligence in healthcare: past  present and future. Stroke and
vascular neurology  2 230–243.

KRIZHEVSKY  A.  SUTSKEVER  I. and HINTON  G. E. (2012). Imagenet classiﬁcation with deep

convolutional neural networks. In Advances in neural information processing systems.

LEE  J. M. (2006). Riemannian manifolds: an introduction to curvature  vol. 176. Springer Science

& Business Media.

LESHNO  M.  LIN  V. Y.  PINKUS  A. and SCHOCKEN  S. (1993). Multilayer feedforward networks
with a nonpolynomial activation function can approximate any function. Neural networks  6
861–867.

LI  H.  XU  Z.  TAYLOR  G.  STUDER  C. and GOLDSTEIN  T. (2018). Visualizing the loss

landscape of neural nets. In Advances in Neural Information Processing Systems.

LONG  J.  SHELHAMER  E. and DARRELL  T. (2015). Fully convolutional networks for semantic

segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

LU  Z.  PU  H.  WANG  F.  HU  Z. and WANG  L. (2017). The expressive power of neural networks:

A view from the width. In Advances in Neural Information Processing Systems.

MHASKAR  H. N. (1996). Neural networks for optimal approximation of smooth and analytic

functions. Neural computation  8 164–177.

MIOTTO  R.  WANG  F.  WANG  S.  JIANG  X. and DUDLEY  J. T. (2017). Deep learning for

healthcare: review  opportunities and challenges. Brieﬁngs in bioinformatics  19 1236–1246.

NIYOGI  P.  SMALE  S. and WEINBERGER  S. (2008). Finding the homology of submanifolds with

high conﬁdence from random samples. Discrete & Computational Geometry  39 419–441.

PANAYOTOV  V.  CHEN  G.  POVEY  D. and KHUDANPUR  S. (2015). Librispeech: an asr corpus
based on public domain audio books. In 2015 IEEE International Conference on Acoustics  Speech
and Signal Processing (ICASSP). IEEE.

ROWEIS  S. T. and SAUL  L. K. (2000). Nonlinear dimensionality reduction by locally linear

embedding. science  290 2323–2326.

SHAHAM  U.  CLONINGER  A. and COIFMAN  R. R. (2018). Provable approximation properties for

deep neural networks. Applied and Computational Harmonic Analysis  44 537–557.

SIMONYAN  K. and ZISSERMAN  A. (2014). Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556.

TENENBAUM  J. B.  DE SILVA  V. and LANGFORD  J. C. (2000). A global geometric framework

for nonlinear dimensionality reduction. Science  290 2319–2323.

TU  L. (2010). An Introduction to Manifolds. Universitext  Springer New York.

https://books.google.com/books?id=br1KngEACAAJ

YAROTSKY  D. (2017). Error bounds for approximations with deep relu networks. Neural Networks 

94 103–114.

YOUNG  T.  HAZARIKA  D.  PORIA  S. and CAMBRIA  E. (2018). Recent trends in deep learning

based natural language processing. ieee Computational intelligenCe magazine  13 55–75.

ZHANG  C.  BENGIO  S.  HARDT  M.  RECHT  B. and VINYALS  O. (2016). Understanding deep

learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.

11

,Minshuo Chen
Haoming Jiang
Wenjing Liao
Tuo Zhao