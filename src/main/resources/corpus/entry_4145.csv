2014,Optimistic Planning in Markov Decision Processes Using a Generative Model,We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing  with probability $1-\delta$  an $\epsilon$-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm  called StOP (for Stochastic-Optimistic Planning)  based on the optimism in the face of uncertainty" principle. StOP can be used in the general setting  requires only a generative model  and enjoys a complexity bound that only depends on the local structure of the MDP.",Optimistic planning in Markov decision processes

using a generative model

Bal´azs Sz¨or´enyi

INRIA Lille - Nord Europe 

SequeL project  France /

MTA-SZTE Research Group on
Artiﬁcial Intelligence  Hungary

Gunnar Kedenburg

INRIA Lille - Nord Europe 

SequeL project  France

gunnar.kedenburg@inria.fr

balazs.szorenyi@inria.fr

Remi Munos∗

INRIA Lille - Nord Europe 

SequeL project  France

remi.munos@inria.fr

Abstract

We consider the problem of online planning in a Markov decision process with
discounted rewards for any given initial state. We consider the PAC sample com-
plexity problem of computing  with probability 1−δ  an �-optimal action using the
smallest possible number of calls to the generative model (which provides reward
and next-state samples). We design an algorithm  called StOP (for Stochastic-
Optimistic Planning)  based on the “optimism in the face of uncertainty” princi-
ple. StOP can be used in the general setting  requires only a generative model  and
enjoys a complexity bound that only depends on the local structure of the MDP.

1

Introduction

1.1 Problem formulation

In a Markov decision process (MDP)  an agent navigates in a state space X by making decisions
from some action set U. The dynamics of the system are determined by transition probabilities
P : X × U × X → [0  1] and reward probabilities R : X × U × [0  1] → [0  1]  as follows: when
the agent chooses action u in state x  then  with probability R(x  u  r)  it receives reward r  and with
probability P (x  u  x�) it makes a transition to a next state x�. This happens independently of all
previous actions  states and rewards—that is  the system possesses the Markov property. See [20  2]
for a general introduction to MDPs. We do not assume that the transition or reward probabilities
are fully known. Instead  we assume access to the MDP via a generative model (e.g. simulation
software)  which  for a state-action (x  u)  returns a reward sample r ∼ R(x  u ·) and a next-state
sample x� ∼ P (x  u ·). We also assume the number of possible next-states to be bounded by N ∈ N.
We would like to ﬁnd an agent that implements a policy which maximizes the expected cumulative
discounted reward E[�∞t=0 γtrt]  which we will also refer to as the return. Here  rt is the reward
received at time t and γ ∈ (0  1) is the discount factor. Further  we take an online planning approach 
where at each time step  the agent uses the generative model to perform a simulated search (planning)
in the set of policies  starting from the current state. As a result of this search  the agent takes a single
action. An expensive global search for the optimal policy in the whole MDP is avoided.

∗Current afﬁliation: Google DeepMind

1

To quantify the performance of our algorithm  we consider a PAC (Probably Approximately Correct)
setting  where  given � > 0 and δ ∈ (0  1)  our algorithm returns  with probability 1−δ  an �-optimal
action (i.e. such that the loss of performing this action and then following an optimal policy instead
of following an optimal policy from the beginning is at most �). The number of calls to the generative
model required by the planning algorithm is referred to as its sample complexity. The sample and
computational complexities of the planning algorithm introduced here depend on local properties
of the MDP  such as the quantity of near-optimal policies starting from the initial state  rather than
global features like the MDP’s size.

1.2 Related work

The online planning approach and  in particular  its ability to get rid of the dependency on the global
features of the MDP in the complexity bounds (mentioned above  and detailed further below) is
the driving force behind the Monte Carlo Tree Search algorithms [16  8  11  18]. 1 The theoreti-
cal analysis of this approach is still far from complete. Some of the earlier algorithms use strong
assumptions  others are applicable only in restricted cases  or don’t adapt to the complexity of the
problem. In this paper we build on ideas used in previous works  and aim at ﬁxing these issues.
A ﬁrst related work is the sparse sampling algorithm of [14]. It builds a uniform look-ahead tree of a
given depth (which depends on the precision �)  using for each transition a ﬁnite number of samples
obtained from a generative model. An estimate of the value function is then built using empirical
averaging instead of expectations in the dynamic programming back-up scheme. This results in an

1

(1−γ)3�� log K+log[1/(�(1−γ)2 )])

log(1/γ)

�2(1−γ)4 .

algorithm with (problem-independent) sample complexity of order�

(neglecting some poly-logarithmic dependence)  where K is the number of actions. In terms of � 
this bound scales as exp(O([log(1/�)]2))  which is non-polynomial in 1/�. 2 Another disadvantage
of the algorithm is that the expansion of the look-ahead tree is uniform; it does not adapt to the MDP.
An algorithm which addresses this appears in [21]. It avoids evaluating some unnecessary branches
of the look-ahead tree of the sparse sampling algorithm. However  the provided sample bound does
not improve on the one in [14]  and it is possible to show that the bound is tight (for both algorithms).
In fact  the sample complexity turns out to be super-polynomial even in the pure Monte Carlo setting
(i.e.  when K = 1): 1/�2+(log C)/ log(1/γ)  with C ≥
Close to our contribution are the planning algorithms [13  3  5  15] (see also the survey [18]) that
follow the so-called “optimism in the face of uncertainty” principle for online planning. This prin-
ciple has been extensively investigated in the multi-armed bandit literature (see e.g. [17  1  4]). In
the planning problem  this approach translates to prioritizing the most promising part of the policy
space during exploration. In [13  3  5]  the sample complexity depends on a measure of the quantity
of near-optimal policies  which gives a better understanding of the real hardness of the problem than
the uniform bound in [14].
The case of deterministic dynamics and rewards is considered in [13]. The proposed algorithm has
log(1/γ)   where κ ∈ [1  K] measures (as a branching factor) the
sample complexity of order (1/�)
quantity of nodes of the planning tree that belong to near-optimal policies. If all policies are very
good  many nodes need to be explored in order to distinguish the optimal policies from the rest  and
log(1/γ) .
therefore  κ is close to the number of actions K  resulting in the minimax bound of (1/�)
Now if there is structure in the rewards (e.g. when sub-optimal policies can be eliminated by ob-
serving the ﬁrst rewards along the sequence)  then the proportion of near-optimal policies is low 
so κ can be small and the bound is much better. In [3]  the case of stochastic rewards have been
considered. However  in that work the performance is not compared to the optimal (closed-loop)
policy  but to the best open-loop policy (i.e. which does not depends on the state but only on the
sequence of actions). In that situation  the sample complexity is of order (1/�)max(2  log(κ)
The deterministic and open-loop settings are relatively simple  since any policy can be identiﬁed with
a sequence of actions. In the general MDP case however  a policy corresponds to an exponentially

log(1/γ) ).

log K

1

log κ

1A similar planning approach has been considered in the control literature  such as the model-predictive

control [6] or in the AI community  such as the A∗ heuristic search [19] and the AO∗ variant [12].

2A problem-independent lower bound for the sample complexity  of order (1/�)1/ log(1/γ)  is provided too.

2

wide tree  where several branches need to be explored. The closest work to ours in this respect is
[5]. However  it makes the (strong) assumption that a full model of the rewards and transitions is
log(1/γ)   but where κ ∈ (1  N K] is deﬁned
as the branching factor of the set of nodes that simultaneously (1) belong to near-optimal policies 
and (2) whose “contribution” to the value function at the initial state is non-negligible.

available. The sample complexity achieved is again�1/�� log(κ)

1.3 The main results of the paper

Our main contribution is a planning algorithm  called StOP (for Stochastic Optimistic Planning)
that achieves a polynomial sample complexity in terms of � (which can be regarded as the leading
parameter in this problem)  and which is  in terms of this complexity  competitive to other algorithms
that can exploit more speciﬁcs of their respective domains.
It beneﬁts from possible reward or
transition probability structures  and does not require any special restriction or knowledge about the
MDP besides having access to a generative model. The sample complexity bound is more involved
than in previous works  but can be upper-bounded by:

(1/�)2+ log κ

log(1/γ) +o(1)

(1)
The important quantity κ ∈ [1  KN ] plays the role of a branching factor of the set of important
states S � ∗ (deﬁned precisely later) that “contribute” in a signiﬁcant way to near-optimal policies.
These states have a non-negligible probability to be reached when following some near-optimal
policy. This measure is similar (but with some differences illustrated below) to the κ introduced in
the analysis of OP-MDP in [5]. Comparing the two  (1) contains an additional constant of 2 in the
exponent. This is a consequence of the fact that the rewards are random and that we do not have
access to the true probabilities  only to a generative model generating transition and reward samples.
In order to provide intuition about the bound  let us consider several speciﬁc cases (the derivation of
these bounds can be found in Section E):

• Worst-case. When there is no structure at all  then S � ∗ may potentially be the set of
all possible reachable nodes (up to some depth which depends on �)  and its branching
factor is κ = KN. The sample complexity is thus of order (neglecting logarithmic fac-
tors) (1/�)2+ log(KN )
log(1/γ) . This is the same complexity that uniform planning algorithm would
achieve. Indeed  uniform planning would build a tree of depth h with branching factor KN
where from each state-action one would generate m rewards and next-state samples. Then 
dynamic programming would be used with the empirical Bellman operator built from the
samples. Using Chernoff-Hoeffding bound  the estimation error is of the order (neglecting

logarithms and (1− γ) dependence) of 1/√m. So for a desired error � we need to choose h
of order log(1/�)/ log(1/γ)  and m of order 1/�2 leading to a sample complexity of order
m(KN )h = (1/�)2+ log(KN )
log(1/γ) . (See also [15]) Note that in the worst-case sense there is no
uniformly better strategy than a uniform planning  which is achieved by StOP. However 
StOP can also do much better in speciﬁc settings  as illustrated next.

• Case with K0 > 1 actions at the initial state  K1 = 1 actions for all other states  and
arbitrary transition probabilities. Now each branch corresponds to a single policy. In
that case one has κ = 1 (even though N > 1) and the sample complexity of StOP is of
order ˜O(log(1/δ)/�2) with high probability3. This is the same rate as a Monte-Carlo eval-
uation strategy would achieve  by sampling O(log(1/δ)/�2) random trajectories of length
log(1/�)/ log(1/γ). Notice that this result is surprisingly different from OP-MDP which
has a complexity of order (1/�)
log(1/γ) (in the case when κ = N  i.e.  when all transitions
are uniform). Indeed  in the case of uniform transition probabilities  OP-MDP would sam-
ple the nodes in breadth-ﬁrst search way  thus achieving this minimax-optimal complexity.
This does not contradict the ˜O(log(1/δ)/�2) bound for StOP (and Monte-Carlo) since this
bound applies to an individual problem and holds in high probability  whereas the bound
for OP-MDP is deterministic and holds uniformly over all problems of this type.

log N

3We emphasize the dependence on δ here since we want to compare this high-probability bound to the

deterministic bound of OP-MDP.

3

Here we see the potential beneﬁt of using StOP instead of OP-MDP  even though StOP
only uses a generative model of the MDP whereas OP-MDP requires a full model.

• Highly structured policies. This situation holds when there is a substantial gap between
near optimal policies and other sub-optimal policies. For example if along an optimal
policy  all immediate rewards are 1  whereas as soon as one deviates from it  all rewards
are < 1. Then only a small proportion of the nodes (the ones that contribute to near-optimal
policies) will be expanded by the algorithm. In such cases  κ is very close to 1 and in the
limit  we recover the previous case when K = 1 and the sample complexity is O(1/�)2.

• Deterministic MDPs. Here N = 1 and we have that κ ∈ [1  K]. When there is structure in
the rewards (like in the previous case)  then κ = 1 and we obtain a rate ˜O(1/�2). Now when
the MDP is almost deterministic  in the sense that N > 1 but from any state-action  there
is one next-state probability which is close to 1  then we have almost the same complexity
as in the deterministic case (since the nodes that have a small probability to be reached will
not contribute to the set of important nodes S � ∗  which characterizes κ).
[9] for the PAC setting.

• Multi-armed bandit we essentially recover the result of the Action Elimination algorithm

Thus we see that in the worst case StOP is minimax-optimal  and in addition  StOP is able to beneﬁt
from situations when there is some structure either in the rewards or in the transition probabilities.
We stress that StOP achieves the above mentioned results having no knowledge about κ.

1.4 The structure of the paper

Section 2 describes the algorithm  and introduces all the necessary notions. Section 3 presents the
consistency and sample complexity results. Section 4 discusses run time efﬁciency  and in Section 5
we make some concluding remarks. Finally  the supplementary material provides the missing proofs 
the analysis of the special cases  and the necessary ﬁxes for the issues with the run-time complexity.

2 StOP: Stochastic Optimistic Planning

Recall that N ∈ N denotes the number of possible next states. That is  for each state x ∈ X and each
action u available at x  it holds that P (x  u  x�) = 0 for all but at most N states x� ∈ X. Throughout
this section  the state of interest is denoted by x0  the requested accuracy by �  and the conﬁdence
parameter by δ0. That is  the problem to be solved is to output an action u which is  with probability
at least (1 − δ0)  at least �-optimal in x0.
The algorithm and the analysis make use of the notion of an (inﬁnite) planning tree  policies and
trajectories. These notions are introduced in the next subsection.

2.1 Planning trees and trajectories

The inﬁnite planning tree Π∞ for a given MDP is a rooted and labeled inﬁnite tree. Its root is
denoted s0 and is labeled by the state of interest  x0 ∈ X. Nodes on even levels are called action
nodes (the root is an action node)  and have Kd children each on the d-th level of action nodes: each
action u is represented by exactly one child  labeled u. Nodes on odd levels are called transition
nodes and have N children each: if the label of the parent (action) node is x  and the label of the
transition node itself is u  then for each x� ∈ X with P (x  u  x�) > 0 there is a corresponding child 
labeled x�. There may be children with probability zero  but no duplicates.
An inﬁnite policy is a subtree of Π∞ with the same root  where each action node has exactly one
child and each transition node has N children. It corresponds to an agent having ﬁxed all its possible
future actions. A (partial) policy Π is a ﬁnite subtree of Π∞  again with the same root  but where
the action nodes have at most one child  each transition node has N children  and all leaves 4 are
on the same level. The number of transition nodes on any path from the root to a leaf is denoted
d(Π) and is called the depth of Π. A partial policy corresponds to the agent having its possible
future actions planned for d(Π) steps. There is a natural partial order over these policies: a policy

4Note that leaves are  by deﬁnition  always action nodes.

4

Π� is called descendant policy of a policy Π if Π is a subtree of Π�. If  additionally  it holds that
d(Π�) = d(Π) + 1  then Π is called the parent policy of Π�  and Π� the child policy of Π.
A (random) trajectory  or rollout  for some policy Π is a realization τ := (xt  ut  rt)T
t=0 of the
stochastic process that belongs to the policy. A random path is generated from the root by always
following  from a non-leaf action node with label xt  its unique child in Π  then setting ut to the
label of this node  from where  drawing ﬁrst a label xt+1 from P (xt  ut ·)  one follows the child
with label xt+1. The reward rt is drawn from the distribution determined by R(xt  ut ·). The value
of the rollout τ (also called return or payoff in the literature) is v(τ ) :=�T
t=0 rtγt  and the value of
the policy Π is v(Π) := E[v(τ )] = E[�T
t=0 rtγt]. For an action u available at x0  denote by v(u)
the maximum of the values of the policies having u as the label of the child of root s0. Denote by v∗
the maximum of these v(u) values. Using this notation  the task of the algorithm is to return  with
high probability  an action u with v(u) ≥ v∗ − �.
2.2 The algorithm

StOP (Algorithm 1  see Figure 1 in the supplementary material for an illustration) maintains for each
action u available at x0 a set of active policies Active(u). Initially  it holds that Active(u) = {Πu} 
where Πu is the shallowest partial policy with the child of the root being labeled u. Also  for each
policy Π that becomes a member of an active set  the algorithm maintains high conﬁdence lower and
upper bounds for the value v(Π) of the policy  denoted ν(Π) and b(Π)  respectively.
In each round t  an optimistic policy Π†t u := argmaxΠ∈Active(u) b(Π) is determined for each ac-
tion u. Based on this  the current optimistic action u†t := argmaxu b(Π†t u) and secondary action
u††t
b(Π†t u) are computed. A policy Πt to explore is then chosen: if the policy
that belongs to the secondary action is at least as deeply developed as the policy that belongs to
the optimistic action  the optimistic one is chosen for exploration  otherwise the secondary one.
Note that a smaller depth is equivalent to a larger gap between lower and upper bound  and vice
versa5. The set Active(ut) is then updated by replacing the policy Πt by its child policies. Accord-
ingly  the upper and lower bounds for these policies are computed. The algorithm terminates when
b(Π†t u)–that is  when  with high conﬁdence  no policies starting with an
ν(Π†t ) + � ≥ maxu�=u†t
action different from u†t have the potential to have signiﬁcantly higher value.

:= argmaxu�=u†t

2.2.1 Number and length of trajectories needed for one partial policy

Fix some integer d > 0 and let Π be a partial policy of depth d. Let  furthermore  Π� be an inﬁnite
policy that is a descendant of Π. Note that

0 ≤ v(Π�) − v(Π) ≤ γd
1−γ .

(2)

The value of Π is a γd
1−γ -accurate approximation of the value of Π�. On the other hand  having m
trajectories for Π  their average reward ˆv(Π) can be used as an estimate of the value v(Π) of Π. From
1−γ � ln(1/δ)
the Hoeffding bound  this estimate has  with probability at least (1 − δ)  accuracy 1−γd
2m .
With m := m(d  δ) := � ln(1/δ)
2m holds  so with prob-
ability at least (1 − δ)  b(Π) := ˆv(Π) + γd
1−γ and ν(Π) :=
ˆv(Π)− 1−γd
1−γ bound v(Π�) from above and below  respectively. This choice
balances the inaccuracy of estimating v(Π�) based on v(Π) and the inaccuracy of estimating v(Π).
1−γ ≤ �/2. Note
Let d∗ := d∗(�  γ) := �(ln
that if d(Π) = d∗ for any given policy Π  then b(Π) − ν(Π) ≤ �/2. Because of this  it follows
(see Lemma 3 in the supplementary material) that d∗ is the maximal length the algorithm ever has
to develop a policy.

(1−γ)� )/ ln(1/γ)�  the smallest integer satisfying 3 γd∗

1−γ � ln(1/δ)
2m ≤ ˆv(Π) + 2 γd

γd )2� trajectories  γd
( 1−γd
1−γ + 1−γd

1−γ ≥ 1−γd
1−γ � ln(1/δ)

1−γ � ln(1/δ)

2m ≥ ˆv(Π)− γd

2

6

5This approach of using secondary actions is based on the UGapE algorithm [10].

5

Algorithm 1 StOP(s0  δ0  �  γ)
1: for all u available from x0 do
Πu := smallest policy with the child of s0 labeled u
2:
δ1 := (δ0/d∗) · (K0)−1
3:
4:
(ν(Πu)  b(Πu)) := BoundValue(Πu  δ1)
5:
Active(u) := {Πu}
6: for round t=1  2  . . . do
for all u available at x0 do
7:
8:
9:

Π†t u := argmaxΠ∈Active(u) b(Π)

b(Π†t u) 

t u††t

Π†t := Π†
  where u†t := argmaxu b(Π†t u) 
t u†t
Π††t
:= Π†
  where u††t
if ν(Π†t ) + � ≥ maxu�=u†t
if d(Π††t ) ≥ d(Π†t ) then
ut := u†t and Πt := Π†t
else

:= argmaxu�=u†t
b(Π†t u) then

return u†t

ut := u††t and Πt := Π††t

Active(ut) := Active(ut) \ {Πt}
(K�)−N �
for all child policy Π� of Πt do

δ := (δ0/d∗) ·�d(Πt)−1
(ν(Π)  b(Π)) := BoundValue(Π�  δ)
Active(ut) := Active(ut) ∪ {Π�}

�=0

� �d−1

�=0 (K�)N �

10:

11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

� initialize

� d(Πu) = 1

� the set of active policies that follow u in s0

� optimistic action and policy
� secondary action and policy
� termination criterion

� select the policy to evaluate

� action and policy to explore

= # of policies of depth at most d

2.2.2 Samples and sample trees

Algorithm StOP aims to aggressively reuse every sample for each transition node and every sample
for each state-action pair  in order to keep the sample complexity as low as possible. Each time the
value of a partial policy is evaluated  all samples that are available for any part of it from previous
rounds are reused. That is  if m trajectories are necessary for assessing the value of some policy
Π  and there are m� complete trajectories available and m�� that end at some inner node of Π  then
StOP (more precisely  another algorithm  Sample  called from StOP) samples rewards (using
SampleReward) and transitions (SampleTransition) to generate continuations for the m��
incomplete trajectories and to generate (m− m� − m��) new trajectories  as described in Section 2.1 
where

• SampleReward(s) for some action node s samples a reward from the distribution
R(x  u ·)  where u is the label of the parent of s and x is the label of the grandparent
of s  and
• SampleTransition(s) for some transition node s samples a next state from the distri-

bution P (x  u ·)  where u is the label of s and x is the label of the parent of s.

To compensate for the sharing of the samples  the conﬁdences of the estimates are increased  so that
with probability at least (1− δ0)  all of them are valid6. The samples are organized as a collection of
sample trees  where a sample tree T is a (ﬁnite) subtree of Π∞ with the property that each transition
node has exactly one child  and that each action node s is associated with some reward rT (s). Note
that the intersection of a policy Π and a sample tree T is always a path. Denote this path by τ (T   Π)
and note that it necessarily starts from the root and ends either in a leaf or in an internal node of Π. In
the former case  this path can be interpreted as a complete trajectory for Π  and in the latter case  as
an initial segment. Accordingly  when the value of a new policy Π needs to be estimated/bounded  it
is computed as ˆv(Π) := 1
i=1 v(τ (Ti  Π)) (see Algorithm 2: BoundValue)  where T1  . . .  Tm
are sample trees constructed by the algorithm. For terseness  these are considered to be global
variables  and are constructed and maintained using algorithm Sample (Algorithm 3).

m�m

�=0 K−N �
divided by the number of policies of depth at most d  and by the largest possible depth—see section 2.2.1.

6In particular  the conﬁdence is set to 1 − δd(Π) for policy Π  where δd = (δ0/d∗)�d−1

�

is δ0

6

2

Algorithm 2 BoundValue(Π  δ)
Ensure: with probability at least (1 − δ)  interval [ν(Π)  b(Π)] contains v(Π)
1: m :=� ln(1/δ)
m�m

γd(Π) �2�
� 1−γd(Π)
2: Sample(Π  s0  m)
3: ˆv(Π) := 1
1−γ � ln(1/δ)
4: ν(Π) := ˆv(Π) − 1−γd(Π)
5: b(Π) := ˆv(Π) + γd(Π)
1−γ + 1−γd(Π)
6: return (ν(Π)  b(Π))

1−γ � ln(1/δ)

i=1 v(τ (Ti  Π))

2m

2m

� Ensure that at least m trajectories exist for Π
� empirical estimate of v(Π)
� Hoeffding bound

� . . . and (2)

ends in a leaf of Π for i = 1  . . .   m)

Algorithm 3 Sample(Π  s  m)
Ensure: there are m sample trees T1  . . .  Tm that contain a complete trajectory for Π (i.e. τ (Ti  Π)
1: for i := 1  . . .   m do
2:
3:
4:
5:
6:
7:
8:
9:
10:

let s� be the child of s in Π and add it to T as a new child of s
s�� := SampleTransition(s�) 
add s�� to T as a new child of s�
s := s��
rT (s��) := SampleReward(s��)

if sample tree Ti does not yet exist then
let Ti be a new sample tree of depth 0
let s be the last node of τ (Ti  Π)
while s is not a leaf of Π do

� s� is a transition node

� s is an action node

3 Analysis

Recall that v∗ denotes the maximal value of any (possibly inﬁnite) policy tree. The following theo-
rem formalizes the consistency result for StOP (see the proof in Section C).
Theorem 1. With probability at least (1 − δ0)  StOP returns an action with value at least v∗ − �.
Before stating the sample complexity result  some further notation needs to be introduced.
Let u∗ denote an optimal action available at state x0. That is  v(u∗) = v∗. Deﬁne for u �= u∗

u :=�Π : Π follows u from s0 and v(Π) + 3 γd(Π)
P �
and also deﬁne
u∗ :=�Π : Π follows u∗ from s0  v(Π) + 3 γd(Π)
P �
Then P � := P �
in order to determine an �-optimal action. (See also Lemma 8 in the supplementary material.)
Let now p(s) denote the product of the probabilities of the transitions on the path from s0 to s. That
is  for any policy tree Π containing s  a trajectory for Π goes through s with probability p(s). When
estimating the value of some policy Π of depth d  the expected number of trajectories going through
some nodes s of it is p(s)m(d  δd). The sample complexity therefore has to take into consideration
for each node s (at least for the ones with “high” p(s) value) the maximum �(s) = max{d(Π) : Π ∈
P � contains s} of the depth of the relevant policies it is included in. Therefore  the expected number
of trajectories going through s in a given run of StOP is

u is the set of “important” policies that potentially need to be evaluated

1−γ ≥ v∗ and v(Π) − 6 γd(Π)

u∗ ∪�u�=u∗ P �

1−γ + ��  

1−γ ≥ v∗ − 3 γd(Π)

1−γ + � ≤ max
u�=u∗

v(u)� .

p(s) · m(�(s)  δ�(s)) = p(s)� ln(1/δ�(s))

2

γ�(s) �2�
� 1−γ�(s)

(3)

If (3) is “large” for some s  it can be used to deduce a high conﬁdence upper bound on the number of
times s gets sampled. To this end  let S � denote the set of nodes of the trees in P �  let N � denote the

7

smallest positive integer N satisfying N ≥���s ∈ S � : p(s) · m(�(s)  δ�(s)) ≥ (8/3) ln(2N /δ0)���

(obviously N � ≤ |S �|)  and deﬁne

S � ∗ :=�s ∈ S � : p(s) · m(�(s)  δ�(s)) ≥ (8/3) ln(2N �/δ0)� .

2 )  as N � = |S � ∗|. See also Appendix D.)

S � is the set of “important” nodes (P � is the set of “important” policies)  and S � ∗ consists of the
important nodes that  with high probability  are not sampled more than twice as often as expected.
(This high probability is 1 − δ0
2N � according to the Bernstein bound  so these upper bounds hold
jointly with probability at least (1 − δ0
For s� ∈ S � \ S � ∗  the number of times s� gets sampled has a variance that is too high compared
to its expected value (3)  so in this case  a different approach is needed in order to derive high
conﬁdence upper bounds. To this end  for a transition node s  let p◦(s) := p◦(s  �) := �{p(s�) :
s� is a child of s with p(s�) · m(�(s�)  δ�(s�)) < (8/3) ln(2N �/δ0)}  and deﬁne
B(s) := B(s  �) :=�0 
if p◦(s) ≤
otherwise.
As it will be shown in the proof of Theorem 2 (in Section D)  this is a high conﬁdence upper bound
on the number of trajectories that go through some child s� ∈ S � \ S � ∗ of some s� ∈ S � ∗.
Theorem 2. With probability at least (1 − 2δ)  StOP outputs a policy of value at least (v∗ − �)  af-
�=d(s)+1 K�� samples 

ter generating at most�s∈S � ∗�2p(s)m(�(s)  δ�(s)) + B(s)��(s)
d=d(s)+1�d
where d(s) = min{d(Π) : s appears in policy Π} is the depth of node s.
Finally  the bound discussed in Section 1 is obtained by setting κ := lim sup�→0 max(κ1  κ2) 
ln(1/δ0) 2p(s)m(�(s)  δ�(s))�1/d∗
where κ1 := κ1(�  δ0  γ) :=��s∈S � ∗
and κ2 := κ2(�  δ0  γ) :=
�=d(s) K��1/d∗
� �2(1−γ)2
ln(1/δ0) �s∈S � ∗ B(s)��(s)
d=d(s)�d

2N �m(�(s) δ�(s))

max(6 ln( 2N �
δ0

)  2p◦(s)m(�(s)  δ�(s)))

�2(1−γ)2

.

δ

4 Efﬁciency

StOP  as presented in Algorithm 1  is not efﬁciently executable. First of all  whenever it evaluates
an optimistic policy  it enumerates all its child policies  which typically has exponential time com-
plexity. Besides that  the sample trees are also treated in an inefﬁcient way. An efﬁcient version of
StOP with all these issues ﬁxed is presented in Appendix F of the supplementary material.

5 Concluding remarks

In this work  we have presented and analyzed our algorithm  StOP. To the best of our knowledge 
StOP is currently the only algorithm for optimal (i.e. closed loop) online planning with a generative
model that provably beneﬁts from local structure both in reward as well as in transition probabilities.
It assumes no knowledge about this structure other than access to the generative model  and does
not impose any restrictions on the system dynamics.
One should note though that the current version of StOP does not support domains with inﬁnite
N. The sparse sampling algorithm in [14] can easily handle such problems (at the cost of a non-
polynomial (in 1/�) sample complexity)  however  StOP has much better sample complexity in case
of ﬁnite N. An interesting problem for future research is to design adaptive planning algorithms
with sample complexity independent of N ([21] presents such an algorithm  but the complexity
bound provided there is the same as the one in [14]).

Acknowledgments

This work was supported by the French Ministry of Higher Education and Research  and by the
European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement
no 270327 (project CompLACS). Author two would like to acknowledge the support of the BMBF
project ALICE (01IB10003B).

8

References
[1] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine

Learning Journal  47(2-3):235–256  2002.

[2] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc  2001.
[3] S. Bubeck and R. Munos. Open loop optimistic planning. In Conference on Learning Theory  2010.
[4] S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[5] Lucian Bus¸oniu and R´emi Munos. Optimistic planning for markov decision processes. In Proceedings
15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS-12)  pages 182–189 
2012.

[6] E. F. Camacho and C. Bordons. Model Predictive Control. Springer-Verlag  2004.
[7] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge University Press 

New York  NY  USA  2006.

[8] R´emi Coulom. Efﬁcient selectivity and backup operators in Monte-Carlo tree search. In Proceedings

Computers and Games 2006. Springer-Verlag  2006.

[9] E. Even-Dar  S. Mannor  and Y. Mansour. Action elimination and stopping conditions for reinforcement
learning. In T. Fawcett and N. Mishra  editors  Proceedings of the Twentieth International Conference on
Machine Learning (ICML-2003)  pages 162–169  2003.

[10] Victor Gabillon  Mohammad Ghavamzadeh  and Alessandro Lazaric. Best arm identiﬁcation: A uniﬁed
approach to ﬁxed budget and ﬁxed conﬁdence. In Peter L. Bartlett  Fernando C. N. Pereira  Christopher
J. C. Burges  Lon Bottou  and Kilian Q. Weinberger  editors  NIPS  pages 3221–3229  2012.

[11] Sylvain Gelly  Yizao Wang  R´emi Munos  and Olivier Teytaud. Modiﬁcation of UCT with Patterns in

Monte-Carlo Go. Rapport de recherche RR-6062  INRIA  2006.

[12] Eric A. Hansen and Shlomo Zilberstein. A heuristic search algorithm for Markov decision problems. In
Proceedings Bar-Ilan Symposium on the Foundation of Artiﬁcial Intelligence  Ramat Gan  Israel  23–25
June 1999.

[13] J-F. Hren and R. Munos. Optimistic planning of deterministic systems. In Recent Advances in Reinforce-
ment Learning  pages 151–164. Springer LNAI 5323  European Workshop on Reinforcement Learning 
2008.

[14] M. Kearns  Y. Mansour  and A.Y. Ng. A sparse sampling algorithm for near-optimal planning in large

Markovian decision processes. In Machine Learning  volume 49  pages 193–208  2002.

[15] Gunnar Kedenburg  Raphael Fonteneau  and Remi Munos. Aggregating optimistic planning trees for
solving markov decision processes. In C.J.C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.Q.
Weinberger  editors  Advances in Neural Information Processing Systems 26  pages 2382–2390. Curran
Associates  Inc.  2013.

[16] Levente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In In: ECML-06. Number

4212 in LNCS  pages 282–293. Springer  2006.

[17] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Math-

ematics  6:4–22  1985.

[18] R´emi Munos. From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization

and planning. Foundation and Trends in Machine Learning  7(1):1–129  2014.

[19] N.J. Nilsson. Principles of Artiﬁcial Intelligence. Tioga Publishing  1980.
[20] M.L. Puterman. Markov Decision Processes — Discrete Stochastic Dynamic Programming. John Wiley

& Sons  Inc.  New York  NY  1994.

[21] Thomas J. Walsh  Sergiu Goschin  and Michael L. Littman.

Integrating sample-based planning and
model-based reinforcement learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artiﬁcial
Intelligence  pages 612–617. AAAI Press  2010.

9

,Adel Javanmard
Andrea Montanari
Balázs Szörényi
Remi Munos
Kevin Ellis
Armando Solar-Lezama
Josh Tenenbaum
Shuangfei Zhai
Yu Cheng
Zhongfei (Mark) Zhang
Weining Lu