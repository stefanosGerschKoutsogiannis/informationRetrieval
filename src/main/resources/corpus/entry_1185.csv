2014,Convex Optimization Procedure for Clustering: Theoretical Revisit,In this paper  we present theoretical analysis of SON~--~a convex optimization procedure for clustering using a sum-of-norms (SON) regularization recently proposed in \cite{ICML2011Hocking_419 SON  Lindsten650707  pelckmans2005convex}. In particular  we show if the samples are drawn from two cubes  each being one cluster  then SON can provably identify the cluster membership provided that the distance between the two cubes is larger than a threshold which (linearly) depends on the size of the cube and the ratio of numbers of samples in each cluster. To the best of our knowledge  this paper is the first to provide a rigorous analysis to understand why and when SON works. We believe this may provide important insights to develop novel convex optimization based algorithms for clustering.,Convex Optimization Procedure for Clustering:

Theoretical Revisit

Department of Electrical and Computer Engineering

Changbo Zhu

Department of Mathematics

National University of Singapore

elezhuc@nus.edu.sg

Huan Xu

Department of Mechanical Engineering

National University of Singapore

mpexuh@nus.edu.sg

Chenlei Leng

Department of Statistics
University of Warwick

c.leng@warwick.ac.uk

Shuicheng Yan

Department of Electrical and Computer Engineering

National University of Singapore

eleyans@nus.edu.sg

Abstract

In this paper  we present theoretical analysis of SON – a convex optimization
procedure for clustering using a sum-of-norms (SON) regularization recently pro-
posed in [8  10  11  17]. In particular  we show if the samples are drawn from two
cubes  each being one cluster  then SON can provably identify the cluster mem-
bership provided that the distance between the two cubes is larger than a threshold
which (linearly) depends on the size of the cube and the ratio of numbers of sam-
ples in each cluster. To the best of our knowledge  this paper is the ﬁrst to provide
a rigorous analysis to understand why and when SON works. We believe this may
provide important insights to develop novel convex optimization based algorithms
for clustering.

1

Introduction

Clustering is an important problem in unsupervised learning that deals with grouping observations
(data points) appropriately based on their similarities or distances [20]. Many clustering algo-
rithms have been proposed in literature  including K-means  spectral clustering  Gaussian mix-
ture models and hierarchical clustering  to solve problems with respect to a wide range of cluster
shapes. However  much research has pointed out that these methods all suffer from instabilities
[3  20  16  15  13  19]. Taking K-means as an example  the formulation of K-means is NP-hard and
the typical way to solve it is the Lloyd’s method  which requires randomly initializing the clusters.
However  different initialization may lead to signiﬁcantly different ﬁnal cluster results.

1.1 A Convex Optimization Procedure for Clustering

[10  11]  Hocking et al.

Recently  Lindsten et al.
[17] proposed the
following convex optimization procedure for clustering  which is termed as SON by Lindsten et al.
[11] (Also called Clusterpath by Hocking et al. [8]) 
(cid:107)A − X(cid:107)2

[8] and Pelckmans et al.

ˆX = arg min

(cid:107)Xi· − Xj·(cid:107)2.

(1)

F + α

X∈Rn×p

(cid:88)

i<j

Here A is a given data matrix of dimension n × p where each row is a data point  α is a tunable
parameter to determine the number of clusters  (cid:107) · (cid:107)F denotes the Frobenius norm and Xi· denotes
the ith row of X.

1

The main idea of the algorithm is that if the i-th sample and the j-th sample belong to the same
cluster  then ˆXi· and ˆXj· should be equal. Intuitively  this is due to the fact that the second term is a
regularization term that enforces the rows of ˆX to be the same  and can be seen as a generalization
of the fused Lasso penalty [18]. In particular  this penalty seeks to fuse the rows of ˆX. From another
point of view  the regularization term can be seen as an (cid:96)1 2 norm  i.e.  the sum of (cid:96)2 norm. Such a
norm is known to encourage block sparse (in this case row-sparse) solutions [1]. Thus  it is expected
that for many (i  j) pairs  ˆXi· − ˆXj· = 0.
Mathematically  given c disjoint clusters {C1  C2 ···   Cc} with Ci ⊆ Rp for i = 1  2 ···   c  we
deﬁne the Cluster Membership Matrix of a given data matrix A as the following.
Deﬁnition 1. Given a data matrix A of dimension n × p  for j = 1  2 ···   c  set Ij = {i | Ai· ∈
Cj  1 ≤ i ≤ n}. We say that a matrix X of dimension n × p is a Cluster Membership Matrix of A
if

if i ∈ Ik  j ∈ Ik and 1 ≤ k ≤ c
if i ∈ Im  j ∈ Il  1 ≤ m ≤ c  1 ≤ l ≤ c and m (cid:54)= l.

(cid:26) Xi· = Xj·

Xi· (cid:54)= Xj·

Given a data matrix A  if the optimal solution ˆX of Problem (1) is a Cluster Membership Matrix
of A  then we can determine the cluster membership by simply grouping the identical rows of ˆX
together. We say that SON successfully recovers the cluster membership of A in this case.
Notice that unlike previous approaches  SON does not suffer from the instability issue since it is a
strictly convex optimization problem and the solution is ﬁxed once a data matrix A is given. More-
over  SON can easily be adapted to incorporate a priori knowledge of the clustering membership.
For example  if we have prior knowledge about which points are more likely to be in the same
cluster  we can appropriately weight the regularization term  i.e.  change the regularization term to

i<j γij(cid:107)Xi· − Xj·(cid:107)2 for some γij > 0.

α(cid:80)

The main contribution of this paper is to provide theoretic analysis of SON  in particular to derive
sufﬁcient conditions when SON successfully recovers the clustering membership. We show that
if there are two clusters  each of which is a cube  then SON succeeds provided that the distance
between the cubes is larger than a threshold value that depends on the cube size and the ratio of
number of samples drawn in each cluster. Thus  the intuitive argument about why SON works is
made rigorous and mathematically solid. To the best of our knowledge  this is the ﬁrst attempt to
theoretically quantify why and when SON succeeds.
Related Work: we brieﬂy review the related works on SON. Hocking et al. [8] proposed SON 
arguing that it can be seen as a generalization of hierarchical clustering  and presented via numerical
simulations several situations in which SON works while K-means and average linkage hierarchical
clustering fail. They also developed R package called “clusterpath” which can be used to solve
Problem (1). Independently  Lindsten et al.
[10  11] derived SON as a convex relaxation of K-
means clustering. In the algorithmic aspect  Chi et al. [6] developed two methods to solve Problem
(1)  namely  Alternating Direction Method of Multipliers (ADMM) and alternating minimization
algorithm (AMA). Marchetti et al. [14] generalized SON to the high-dimensional and noisy cases.
Yet  in all these works  no attempt has been made to study rigorously why and when SON succeeds.
Notation: in this paper  matrices are denoted by upper case boldface letters (e.g. A  B)  sets are
denoted by blackboard bold characters (e.g. R  I  C) and operators are denoted by Fraktur characters
(e.g. D  M). Given a matrix A  we use Ai· to denote its ith row  and A·j to denote its jth column.
Its (i  j)th entry is denoted by Ai j. Two norms are used: we use (cid:107) · (cid:107)F to denote the Frobenius
norm and (cid:107) · (cid:107)2 to denote the l2 norm of a vector. The space spanned by the rows of A is denoted
by Row(A). Moreover  given a matrix A of dimension n × p and a function f : Rp (cid:55)→ Rq  we use
the notation f (A) to denote the matrix whose ith row is f (Ai·).

2 Main Result

In this section we present our main theoretic result – a provable guarantee when SON succeeds in
identifying cluster membership.

2

2.1 Preliminaries

We ﬁrst deﬁne some operators that will be frequently used in the remainder of the paper.
Deﬁnition 2. Given any two matrices E of dimension n1 × p and F of dimension n2 × p  deﬁne
the difference operator D1 on E  D2 on the two matrices E  F and D on the matrix constructed by
concatenating E and F vertically as



E1· − F1·
E1· − F2·

E1· − Fn2·
E2· − F1·

E2· − Fn2·

...

...
...

En1· − Fn2·



(cid:18)E
(cid:19)

F

and D(

(cid:32) D1(E)

(cid:33)

D1(F)

D2(E  F)

.

) =



E1· − E2·
E1· − E3·

E1· − En1·
E2· − E3·

E2· − En1·

...

...
...

E(n1−1)· − En1·



D1(E) =

  D2(E  F) =

In words  the operator D1 calculates the difference between every two rows of a matrix and lists the
results in the order indicated in the deﬁnition. Similarly  given two matrices E and F  the operator
D2(E  F) calculates the difference of any two rows between E and F  one from E and the other from
F. We also deﬁne the following average operation which calculates the mean of the row vectors.
Deﬁnition 3. Given any matrix E of dimension n × p  deﬁne the average operator on E as

n(cid:88)

i=1

M(E) =

1
n

(

Ei·).

Deﬁnition 4. A matrix E is called column centered if M(E) = 0.

2.2 Theoretical Guarantees

Our main result essentially says that when there are two clusters  each of which is a cube  and
they are reasonably separated away from each other  then SON successfully recovers the cluster
membership. We now make this formal. For i = 1  2  suppose Ci ⊆ Rp is a cube with center
(µi1  µi2 ···   µip) and edge length si = 2(σi1  σi2 ···   σip)   i.e. 

Ci = [µi1 − σi1  µi1 + σi1] × ··· × [µip − σip  µip + σip].

Deﬁnition 5. The distance d1 2 between cubes C1 and C2 is

d1 2 (cid:44) inf{(cid:107)x − y(cid:107)2 | x ∈ C1  y ∈ C2}.

Deﬁnition 6. The weighted size w1 2 with respect to C1  C2  n1 and n2 is deﬁned as
(cid:107)s2(cid:107)2

w1 2 = max

(cid:107)s1(cid:107)2 

(cid:26)(cid:18) 2n2(n1 − 1)

(cid:18) 2n1(n2 − 1)

(cid:19)

(cid:19)

+ 1

+ 1

(cid:27)

.

n2
1

n2
2

Theorem 1. Given a column centered data matrix A of dimension n × p  where each row is ar-
bitrarily picked from either cube C1 or cube C2 and there are totally ni rows chosen from Ci for
i = 1  2  if w1 2 < d1 2  then by choosing the parameter α ∈ R such that w1 2 < n
2 α < d1 2  we
have the following:

1. SON can correctly determine the cluster membership of A;

2. Rearrange the rows of A such that

A =

(cid:18)A1

A2

(cid:19)

and Ai =

3

  

 Ai

1·
Ai
2·
...
Ai
ni·

(2)

where for i = 1  2 and j = 1  2 ···   ni  Ai
optimal solution ˆX of Problem (1) is given by

(cid:16)

 n2

n1+n2

− n1

n1+n2

(cid:16)

1 −
1 −

ˆXi· =

nα

2(cid:107)M(D2(A1 A2))(cid:107)2

nα

2(cid:107)M(D2(A1 A2))(cid:107)2

(cid:17)

j· = (Ai

j 1  Ai

M(cid:0)D2(A1  A2)(cid:1)  
(cid:17)
M(cid:0)D2(A1  A2)(cid:1)  

if Ai· ∈ C1;
if Ai· ∈ C2.

j 2 ···   Ai

j p) ∈ Ci. Then  the

The theorem essentially states that we need d1 2 to be large and w1 2 to be small for correct deter-
mination of the cluster membership of A. This is indeed intuitive. Notice that d1 2 is the distance
between the cubes and w1 2 is a constant that depends on the size of the cube as well as the ratio
between the samples in each cube. Obviously  if the cubes are too close with each other  i.e.  d1 2 is
small  or if the sizes of the clusters are too big compared to their distance  it is difﬁcult to determine
the cluster membership correctly. Moreover  when n1 (cid:28) n2 or n1 (cid:29) n2  w1 2 is large  and the
theorem states that it is difﬁcult to determine the cluster membership. This is also well expected 
since in this case one cluster will be overwhelmed by the other  and hence determining where the
data points are chosen from becomes problematic.
The assumption in Theorem 1 that the data matrix A is column centered can be easily relaxed  using
the following proposition which states that the result of SON is invariant to any isometry operation.
Deﬁnition 7. An isometry of Rn is a function f : Rn → Rn that preserves the distance between
vectors  i.e. 
Proposition 1. (Isometry Invariance) Given a data matrix A of dimension n × p where each row
is chosen from some cluster Ci  i = 1  2 ···   c  and f (·) an isometry of Rp  we have

(cid:107)f (u) − f (w)(cid:107)2 = (cid:107)u − w(cid:107)2 ∀ u  w ∈ Rn.

(cid:88)

ˆX = arg min

X∈Rn×p
⇐⇒f ( ˆX) = arg min
X∈Rn×p

(cid:107)A − X(cid:107)2

F + α

(cid:107)Xi· − Xj·(cid:107)2

i<j

(cid:107)f (A) − X(cid:107)2

F + α

(cid:107)Xi· − Xj·(cid:107)2.

(cid:88)

i<j

This further implies that if SON successfully determines the cluster membership of A  then it also
successfully determines the cluster membership of f (A).

3 Kernelization

SON can be easily kernelized as we show in this section. In the kernel clustering setup  instead
of clustering {Ai·} such that points within a cluster are closer in the original space  we want to
cluster {Ai·} such that points within a cluster are closer in the feature space. Mathematically  this
means we map Ai· to a Hilbert space H (the feature space) by the feature mapping function φ(·)
and perform clustering on {φ(Ai·)}.
Notice that we can write Problem (1) in terms of the inner product (cid:104)Ai·  Aj·(cid:105)  (cid:104)Ai·  Xj·(cid:105) and
(cid:104)Xi·  Xj·(cid:105). Thus  for SON in the feature space  we only need to replace all these inner products
by (cid:104)φ(Ai·)  φ(Aj·)(cid:105)  (cid:104)φ(Ai·)  Xj·(cid:105) and (cid:104)Xi·  Xj·(cid:105). Thus  SON in the feature space can be formu-
lated as

ˆX = arg min

X∈Rn×q

((cid:104)φ(Ai·)  φ(Ai·)(cid:105) − 2(cid:104)φ(Ai·)  Xi·(cid:105) + (cid:104)Xi·  Xi·(cid:105))

(cid:113)(cid:104)Xi·  Xi·(cid:105) − 2(cid:104)Xi·  Xj·(cid:105) + (cid:104)Xj·  Xj·(cid:105).

(cid:88)

i=1

+α

(3)

We have the following representation theorem about the optimal solution of (3).
Theorem 2. (Representation Theorem) Each row of the optimal solution of Problem (3) can be
written as a linear combination of rows of A  i.e. 

i<j

n(cid:88)

j=1

4

ˆXi· =

aijφ(Aj·).

n(cid:88)

(cid:32)

n(cid:88)

n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

k=1

n(cid:88)

n(cid:88)

n(cid:88)

(cid:33)

Thus  to solve SON in the feature space reduces to ﬁnding the optimal weight {aij}. Deﬁne the
kernel function as K(x  y) = (cid:104)φ(x)  φ(y)(cid:105). Then Problem (3) is equivalent to

min{aij}

K(Ai·  Ai·) − 2

i=1

+α

(cid:88)

aikK(Ai·  Ak·) +

aikailK(Ak·  Al·)

k=1

l=1

(4)

K(Ak·  Al·)(aikail − 2aikajl + ajkajl) 

i<j

k=1

l=1

which is a second order cone program since the kernel is positive semi-deﬁnite. Notice that this
implies that solving SON in the feature space only requires knowing the kernel function rather than
the feature mapping φ(·).

4 Proof

We sketch the proof of Theorem 1 here. The detailed proof is given in the supplementary material.

4.1 Preliminaries

We ﬁrst introduce some notations useful in the proof. We use In to denote an identity matrix of
dimension n × n and use 1m×nto denote a matrix of dimension m × n with all entries being 1.
Similarly  we use 0m×n to denote a matrix of dimension m × n with all entries being 0.
We now deﬁne some special matrices. Let Hn denote a matrix of dimension (n − 1) × n which
is constructed by concatenating 1(n−1)×1 and −In−1 horizontally  i.e.  Hn = (1(n−1)×1 −
In−1). For i = 1  2 ···   n − 2  we ﬁrst concatenate matrices Hn−i and 0(n−1−i)×i hor-
izontally to form a matrix (0(n−1−i)×i Hn−i). Then  we construct Rn by concatenating
{Hn  (0(n−2)×1 Hn−1) ···   (01×(n−2) H2)} vertically  i.e. 



Hn

0(n−2)×1 Hn−1
0(n−3)×2 Hn−2

Rn (cid:44)

...

01×(n−2) H2

 .

We concatenate m copies of −In vertically to form a new matrix and denote it by Wmn×n. Let
Gm n i denote an m × n dimensional matrix where the entries of the ith column all equal 1 and
all the other entries equal 0  i.e.  Gm n i (cid:44) (0m×(i−1) 1m×1 0m×(n−i)). Then  we concatenate
{Gm n 1  Gm n 2 ···   Gm n n} vertically and denote it by Smn×n  i.e. 

   Smn×n (cid:44)

Gm n 1

Gm n 2

...

 .

−In−In

...−In

Wmn×n (cid:44)



Finally  set Ω (cid:44)
Rn1−1
2 )×(n1−1)

0(n2

I(n1−1
2 )
2 )×(n1−1
2 )
S(n1−1)n2×(n1−1) 0(n1−1)n2×(n1−1

0(n2

Gm n n

0(n1−1
2 )×n2
Rn2

0(n1−1
2 )×(n2
2 )
I(n2
2 )

2 ) 0(n1−1)n2×(n2

2 ) W(n1−1)n2×n2

 .

0(n1−1
0(n2

2 )×(n1−1)n2
2 )×(n1−1)n2
I(n1−1)n2

4.2 Proof sketch of Theorem 1

The proof of Theorem 1 is based on the idea of “lifting”. That is  we project Problem (1) into a
higher dimensional space (in particular  from n rows to n(n − 1)/2 rows)  which then allows us to
separate the regularization term into the sum of l2 norm of each row. Although this brings additional

5

linear constraints to the formulation  it facilitates the analysis. In the following  we divide the proof
into 3 steps and explain the main idea of each step.
Step 1: In this step  we derive an equivalent form of Problem (1) and give optimality conditions.
For convenience  set B(1 2) = D2(A1  A2)  B1 = D1(A1)  B2 = D1(A2) and V = {y ∈
2) | Ωy = 0}. The following lemmas show that we can lift the original problem into an equivalent
R(n
problem that is easier to analyze.
Lemma 1. If the data matrix A is column centered  then the optimal solution ˆX of problem (1) is
also column centered. Further more  set B = D(A) and ˆY = D( ˆX)  we have

(cid:107)A − ˆX(cid:107)2

F =

(cid:107)Bi· − ˆYi·(cid:107)2
2.

1
n

Lemma 2. Given a column centered data matrix A  set B = D(A) and S = {Z ∈
R(n

2)×p | ΩZ·j = 0  1 ≤ j ≤ p}. Then  ˆX is the optimal solution to Problem (1) iff

2(cid:88)

n(n−1)

i=1

2(cid:88)

n(n−1)

i=1

D( ˆX) = arg min
Y∈S

(cid:107)Bi· − Yi·(cid:107)2

2 + α(cid:107)Yi·(cid:107)2).

(

1
n

(5)

Thus  we can determine whether ˆX is the membership matrix of A by solving Problem (5). Com-
pared to Problem (1)  Problem (5) is more amenable to analyze as it is the sum of separable equa-
tions. That is  for i = 1  2 ···   n(n−1)
2 + α(cid:107)Yi·(cid:107)2 individ-
ually with the additional constraint ΩY = 0. Following standard convex analysis (Page 303 of [2]) 
ˆY and ˆΛ are an optimal primal and dual solution pair of Problem (5) if and only if

  we can minimize each 1

n(cid:107)Bi· − Yi·(cid:107)2

2

ˆY·j ∈ V  ( ˆΛ·j)T ∈ V⊥  j = 1  2 ···   p 

and

ˆYi· ∈ arg min
y∈Rp

(

1
n

(cid:107)Bi· − y(cid:107)2

2 + α(cid:107)y(cid:107)2 − y ˆΛT

i· )  i = 1  2 ···  

(cid:18)n

(cid:19)

2

.

(6)

(7)

Step 2: In this step  we construct ˆΛ. Since A is constructed by concatenating matrices A1 and
A2 vertically  we also expect ˆX to be concatenated by two matrices vertically. Due to the fact that
ˆY = D( ˆX)  for 1 ≤ l ≤ p  we write ˆY and ˆΛ as the following

 and ˆY·l =

 ˆΛ1·l

ˆΛ2·l
ˆΛ(1 2)
·l



 ˆY1·l

ˆY2·l
ˆY(1 2)
·l

ˆΛ·l =

where ˆΛi·l  ˆYi·l ∈ R(ni
By the structure of Ω  after some algebraic operations  it can be shown that ( ˆΛ·l)T ∈ V⊥ is equiva-
lent to the following equalities that hold 

∈ Rn1n2  which are determined below.

2 ) for i = 1  2 and ˆΛ(1 2)

  ˆY(1 2)

·l

·l

RT
n1

ˆΛ1·l = −ST

n1n2×n1

ˆΛ(1 2)

·l

  RT
n2

n1n2×n2

ˆΛ(1 2)

·l

.

We now construct ˆΛ(1 2). Set

ˆΛ(1 2)

m· =

(cid:16)

(cid:16)

2
n

M

B(1 2)(cid:17) − B(1 2)

m·

ˆΛ2·l = −WT
(cid:17)

  1 ≤ m ≤ n1n2.

(8)

(9)

Since ˆΛ(1 2) is now ﬁxed  we can bound the right hand sides of the two equalities in (8). In order to
bound the entries of ˆΛ1·l and ˆΛ2·l  we need the following lemma.
Lemma 3. Given cn ∈ Rn  i.e.  cn = (c1  c2 ···   cn)T   such that
then ∃x ∈ R n(n−1)

ci = 0 and ∃b ∈ R |ci| ≤ b 

  such that (cid:107)x(cid:107)∞ ≤ 2

n(cid:80)

n x = cn.

n b and RT

i=1

2

6

n1 = n2 = 25

n1 = 25 & n2 = 50

n1 = 25 & n2 = 75

30

20

10

0

Theoretical bounds w1 2

Empirical performance ¯d1 2

2

4

6
(cid:107)s(cid:107)2

8

10

40

30

20

10

0

Theoretical bounds w1 2

Empirical performance ¯d1 2

2

4

6
(cid:107)s(cid:107)2

8

10

60

40

20

0

Theoretical bounds w1 2

Empirical performance ¯d1 2

2

4

6
(cid:107)s(cid:107)2

8

10

Figure 1: Theoretical bounds and empirical performance. This ﬁgure illustrates the case in which
n1  n2 are constants and (cid:107)s(cid:107)2 is increasing.

Then  because we can bound the right hand sides of the two equalities of (8)  by using Lemma 3  we
can show that there exist ˆΛ1·l  ˆΛ2·l satisfying (8) such that the following holds

(n2 − 1)

(n1)

(cid:1) × p such that

n2
2

2

(4σ2l).

(10)

(n1 − 1)

n2
1

(n2)

(cid:107) ˆΛ1·l(cid:107)∞ ≤ 2
n

(4σ1l) and (cid:107) ˆΛ2·l(cid:107)∞ ≤ 2
n

To summarize this step  we have constructed ˆΛ of dimension(cid:0)n
 ˆΛ1·l  ˆΛ2·l satisﬁes (10)  1 ≤ l ≤ p 
(cid:17)
B(1 2)(cid:17) − B(1 2)
(cid:32)
(cid:33)(cid:16)
(cid:16)

ˆY1·l = ˆY2·l = 0  1 ≤ l ≤ p 
nα

Step 3: Finally  we construct ˆY. Set

m· =

ˆΛ(1 2)

(cid:16)

(cid:16)

2
n

M

ˆY(1 2)

m· =

1 −

2(cid:107)M(cid:0)B(1 2)(cid:1)(cid:107)2



m·

  1 ≤ m ≤ n1n2.

B(1 2)(cid:17)(cid:17)

M

  1 ≤ m ≤ n1n2.

Choosing w1 2 < n
(6) and (7) are satisﬁed. So ˆΛ and ˆY are an optimal primal and dual solution pair of Problem (5).

2 α < d1 2  according to ˆΛ and ˆY constructed  it is easy to checked that conditions

5 Experiments

We now report some numerical experimental results. The empirical performance of SON has been
reported in numerous works [8  10  11]. It has been shown that SON outperforms traditional cluster-
ing methods like K-means in many situations. As such  we do not reproduce such results. Instead 
we conduct experiments to validate our theoretic results.
Recall that Theorem 1 states that when samples are drawn from two cubes  SON guarantees to
successfully recover the cluster membership if the distance between cubes is larger than a threshold
which is linear to the cube size (cid:107)si(cid:107) and the ratio between n1 and n2. To validate this  we randomly
draw a data matrix A where each row belongs to one of the two cubes  and ﬁnd numerically the
largest distance ¯d1 2 between the cubes where the cluster membership is not correctly recovered.
Clearly  ¯d1 2 provides an empirical estimator of the minimal distance needed to successfully recover
the cluster membership. We compare the theoretic bound w1 2 with the empirical performance ¯d1 2
to validate our theorem. The speciﬁc procedures of the experiments are as follows.

1. Choose two cubes C1 and C2 from space Rp with size s1 = 2(σ11  σ12 ···   σ1p) and

s2 = 2(σ21  σ22 ···   σ2p)  and the distance between C1 and C2 is d.

2. Choose arbitrarily n1 points from C1 and n2 points from C2 and form the data matrix Ad

of dimension n × p. Repeat and sample m data matrices {Ad

2 ···   Ad

m}.

1  Ad

7

(cid:107)s1(cid:107)2 = (cid:107)s2(cid:107)2 = 1

(cid:107)s1(cid:107)2 = (cid:107)s2(cid:107)2 = 2

(cid:107)s1(cid:107)2 = (cid:107)s2(cid:107)2 = 3

20

15

10

5

0

Theoretical bounds w1 2

Empirical performance ¯d1 2

2

4

n2
n1

6

8

30

20

10

0

Theoretical bounds w1 2

Empirical performance ¯d1 2

2

4

n2
n1

6

8

50

40

30

20

10

0

Theoretical bounds w1 2

Empirical performance ¯d1 2

2

4

n2
n1

6

8

Figure 2: Theoretical bounds and empirical performance. This ﬁgure illustrates the case in which
(cid:107)s1(cid:107)2 (cid:107)s2(cid:107)2 are constants and the ratio n2

is increasing.

n1

3. Repeat for different d. Set

¯d1 2 = max{d|∃1 ≤ j ≤ m s.t. SON fails to determine the cluster membership of Ad
j}.

4. Repeat for different cube sizes (cid:107)s1(cid:107)2 and (cid:107)s2(cid:107)2.
5. Repeat for different sample numbers n1 and n2.

In the experiments  we focus on the samples chosen from R2  i.e.  p = 2  and use synthetic data to
obtain the empirical performance. The results are shown in Figure 1 and 2. Figure 1 presents the
situation where n1 and n2 are ﬁxed and the cube sizes are increasing. In particular  the two cubes are
both of size l × l  i.e.  both with edge length (l  l). Thus we have (cid:107)s(cid:107)2 =
2l. Clearly  we can see
that the empirical performance and the theoretical bounds are both linearly increasing with respect
to (cid:107)s(cid:107)2  which implies that our theoretical results correctly predict how the performance of SON
depends on (cid:107)s(cid:107)2. Figure 2 presents the situation in which (cid:107)s(cid:107)1 and (cid:107)s(cid:107)2 are ﬁxed  while the ratio
is changing. Again  we observe that both the empirical performance and the theoretical bounds
  which implies that our theoretical bounds w1 2 predict the

n2
n1
are linearly increasing with respect to n2
n1
correct relation between the performance of SON and n2
n1

√

.

6 Conclusion

In this paper  we provided theoretical analysis for the recently presented convex optimization pro-
cedure for clustering  which we term as SON. We showed that if all samples are drawn from two
clusters  each being a cube  then SON is guaranteed to successfully recover the cluster membership
provided that the distance between the two cubes is greater than the “weighted size” – a term that
linearly depends on the cube size and the ratio between the numbers of the samples in each cluster.
Such linear dependence is also observed in our numerical experiment  which demonstrates (at least
qualitatively) the validity of our results.
The main thrust of this paper is to explore using techniques from high-dimensional statistics  in
particular regularization methods that extract low-dimensional structures such as sparsity or low-
rankness  to tackle clustering problems. These techniques have recently been successfully applied to
graph clustering and subspace clustering [4  7  12  5  9]  but not so much to distance-based clustering
tasks with the only exception of SON  to the best of our knowledge. This paper is the ﬁrst attempt
to provide a rigorous analysis to derive sufﬁcient conditions when SON succeeds. We believe this
not only helps to understand why SON works in practice as shown in previous works [8  10  11]  but
also provides important insights to develop novel algorithms based on high-dimensional statistics
tools for clustering tasks.

Acknowledgments

The work of H. Xu was partially supported by the Ministry of Education of Singapore through
AcRF Tier Two grant R-265-000-443-112. This work is also partially supported by the grant from
Microsoft Research Asia with grant number R-263-000-B13-597.

8

References
[1] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  73(3):243–272  2008. 2

[2] Dimitri P Bertsekas. Convex Optimization Theory. Universities Press. 6
[3] S´ebastien Bubeck  Marina Meila  and Ulrike von Luxburg. How the initialization affects the

stability of the k-means algorithm. arXiv preprint arXiv:0907.5494  2009. 1

[4] Yudong Chen  Shiau Hong Lim  and Huan Xu. Weighted graph clustering with non-uniform
In Proceedings of The 31st International Conference on Machine Learning 

uncertainties.
2014. 8

[5] Yudong Chen  Sujay Sanghavi  and Huan Xu. Clustering sparse graphs. In NIPS  pages 2213–

2221  2012. 8

[6] E. C. Chi and K. Lange. Splitting Methods for Convex Clustering. ArXiv e-prints  April 2013.

2

[7] Ehsan Elhamifar and Ren´e Vidal. Sparse subspace clustering. In Computer Vision and Pattern

Recognition  2009. CVPR 2009. IEEE Conference on  pages 2790–2797. IEEE  2009. 8

[8] Toby Hocking  Jean-Philippe Vert  Francis Bach  and Armand Joulin. Clusterpath: an algo-
rithm for clustering using convex fusion penalties. In Lise Getoor and Tobias Scheffer  editors 
Proceedings of the 28th International Conference on Machine Learning (ICML-11)  ICML
’11  pages 745–752  New York  NY  USA  June 2011. ACM. 1  2  7  8

[9] Ali Jalali  Yudong Chen  Sujay Sanghavi  and Huan Xu. Clustering partially observed graphs
In Proceedings of the 28th International Conference on Machine

via convex optimization.
Learning (ICML-11)  pages 1001–1008  2011. 8

[10] F. Lindsten  H. Ohlsson  and L. Ljung. Clustering using sum-of-norms regularization: With
In Statistical Signal Processing Workshop

application to particle ﬁlter output computation.
(SSP)  2011 IEEE  pages 201–204  2011. 1  2  7  8

[11] Fredrik Lindsten  Henrik Ohlsson  and Lennart Ljung. Just relax and come clustering! : A
convexiﬁcation of k-means clustering. Technical Report 2992  Linkping University  Automatic
Control  2011. 1  2  7  8

[12] Guangcan Liu  Zhouchen Lin  and Yong Yu. Robust subspace segmentation by low-rank repre-
sentation. In Proceedings of the 27th International Conference on Machine Learning (ICML-
10)  pages 663–670  2010. 8

[13] Ulrike Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416 

dec 2007. 1

[14] Yuliya Marchetti and Qing Zhou. Solution path clustering with adaptive concave penalty. arXiv

preprint arXiv:1404.6289  2014. 2

[15] Geoffrey McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions  volume

382. John Wiley & Sons  2007. 1

[16] Geoffrey J McLachlan  Thriyambakam Krishnan  and See Ket Ng. The em algorithm. Techni-
cal report  Papers/Humboldt-Universit¨at Berlin  Center for Applied Statistics and Economics
(CASE)  2004. 1

[17] Kristiaan Pelckmans  Joseph De Brabanter  JAK Suykens  and B De Moor. Convex clustering
shrinkage. In PASCAL Workshop on Statistics and Optimization of Clustering Workshop  2005.
1

[18] Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. Sparsity and
smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  67(1):91–108  2005. 2

[19] Spaans Alexander M. J. Heiser van der Kloot  Willem A. and Willem J. Instability of hierar-
chical cluster analysis due to input order of the data: The permucluster solution. Psychological
Methods  10(4):468–476  2005. 1

[20] Rui Xu and II Wunsch  D. Survey of clustering algorithms. Neural Networks  IEEE Transac-

tions on  16(3):645–678  May 2005. 1

9

,Changbo Zhu
Huan Xu
Chenlei Leng
Shuicheng Yan
Ricardo Silva
Edouard Pauwels
Francis Bach
Jean-Philippe Vert