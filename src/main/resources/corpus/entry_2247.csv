2019,Generalization Error Analysis of Quantized Compressive Learning,Compressive learning is an effective method to deal with very high dimensional datasets by applying learning algorithms in a randomly projected lower dimensional space. In this paper  we consider the learning problem where the projected data is further compressed by scalar quantization  which is called quantized compressive learning. Generalization error bounds are derived for three models: nearest neighbor (NN) classifier  linear classifier and least squares regression. Besides studying finite sample setting  our asymptotic analysis shows that the inner product estimators have deep connection with NN and linear classification problem through the variance of their debiased counterparts. By analyzing the extra error term brought by quantization  our results provide useful implications to the choice of quantizers in applications involving different learning tasks. Empirical study is also conducted to validate our theoretical findings.,Generalization Error Analysis of Quantized

Compressive Learning

Xiaoyun Li

Department of Statistics

Rutgers University

Piscataway  NJ 08854  USA
xiaoyun.li@rutgers.edu

Ping Li

Cognitive Computing Lab

Baidu Research

Bellevue  WA 98004  USA

liping11@baidu.com

Abstract

Compressive1 learning is an effective method to deal with very high dimensional
datasets by applying learning algorithms in a randomly projected lower dimensional
space. In this paper  we consider the learning problem where the projected data
is further compressed by scalar quantization  which is called quantized compres-
sive learning. Generalization error bounds are derived for three models: nearest
neighbor (NN) classiﬁer  linear classiﬁer and least squares regression. Besides
studying ﬁnite sample setting  our asymptotic analysis shows that the inner product
estimators have deep connection with NN and linear classiﬁcation problem through
the variance of their debiased counterparts. By analyzing the extra error term
brought by quantization  our results provide useful implications to the choice of
quantizers in applications involving different learning tasks. Empirical study is
also conducted to validate our theoretical ﬁndings.

Introduction

1
Random projection (RP) method [36] has become a popular tool for dimensionality reduction in
many machine learning and database applications  e.g.  [14  2  11  4  38  9]  including classiﬁcation 
matrix sketching  compressive sensing  regression  bioinformatics  matrix factorization  etc. The
great success of random projection lies in the favorable distance preserving property with fairly
elegant statement given by the famous Johnson-Lindenstrauss Lemma [19  10]. In short  under some
conditions we can always project a set of n points X ∈ Rn×d in a high-dimensional space onto a lower
k-dimensional space such that pair-wise distances are approximately preserved  with high probability.
Here k (cid:28) d is the number of random projections. This nice theoretical guarantee has originated
the study of generalization performance of learning in the reduced dimensional space instead of the
original space. This line of work is called compressive learning [16  3  30  13  20  34  35  21].
In many cases  it is useful to further condense the projected data  due to storage saving  privacy con-
sideration  etc. Consequently  research on quantized random projections (QRP) has been conducted
for a while  i.e.  [24  25]. QRP itself has been developed into many promising ﬁelds in computer
science  such as 1-bit compressive sensing  simhash and so on [32  1  6  23]. More recently  it is
shown that quantized random projection is also very convenient for cosine estimation and similarity
search [27  26  28]. However  to the best of our knowledge  theoretical analysis of QRP in learning
mechanisms is still missing in literature. In this paper  we investigate the generalization error bounds
of applying QRP in three models: nearest neighbor classiﬁer  linear classiﬁer and least squares
regression. Apart from ﬁnite k analysis  we also consider the case where k is asymptotically large.

Contributions. An important implication of our analysis is to answer the following question—The
generalization performance using quantized random projections is determined by what factors of a
quantizer? Our theoretical analysis illustrates that for nearest neighbor and linear classiﬁcation  the

1The work of Xiaoyun Li was conducted during the internship at Baidu Research.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

extra loss of quantization decreases as k gets large  and the learning performance is determined by
the variance of debiased inner product estimator when data samples are allocated on the unit sphere.
For regression problems  the distortion of a quantizer becomes crucial. Our theoretical ﬁndings are
validated by empirical study. Practically  our results also suggest appropriate quantizing strategies for
different learning models  which would be helpful for various applications.

2 Preliminaries
Problem setting. Assume dataset X  Y ∼ Dn with X = [x1  ...  xn]T ∈ Rn×d  and xi  i =
1  ...  n are i.i.d. drawn from some marginal distribution X . Throughout this paper  we assume that
every sample in X is standardized to have unit Euclidean norm2. Therefore  the domain of X is
the unit Euclidean sphere Sd  which allows us to call “inner product” and ”cosine” interchangeably.
For classiﬁcation problems  Y ∈ [0  1]n  while in regression model Y ∈ Rn. We will focus on the
Gaussian random projection matrix R = [r1  ...  rk] ∈ Rd×k with i.i.d. standard normal entries.
Random projection is realized by XR = 1√
is for the ease of presentation.
Quantized RP’s. An M-level scalar quantizer Q(·) : A → C is speciﬁed by M + 1 decision borders
t0 < t1 < ··· < tM and M reconstruction levels (or codes) µi  i = 1  ...  M. Given a signal v  the
quantizing operator is deﬁned as Qb(v) = µi ∈ C  such that ti−1 < v ≤ ti. Here  A is the domain of
the original signal and C is the set of codes. The number of bits is deﬁned as b = log2 M ≥ 1. We
note that t0 and tM can be either ﬁnite or inﬁnite depending on the support of signal. For generality 
in this paper we do not restrict our analysis to any speciﬁc quantizer  but cast basic assumption of
increasing and bounded codes  i.e.  µ1 < ··· < µM and ti−1 < µi < ti for all i = 1  ...  M.
Deﬁnition 1. (Maximal gap) For an M-level quantizer Q deﬁned above and an interval [a  b] 
denote α = {i : ti−1 < a ≤ ti} and β = {i : ti < b ≤ ti+1}. The maximal gap on a
interval [a  b] is deﬁned as the largest distance between any two nearby borders in [a  b]  gQ(a  b) =
max{ max

|ti+1 − ti| |tα − a| |b − tβ|}  if tα ∈ [a  b]  and gQ(a  b) = |b − a| otherwise.

XR  where the factor 1√

k

k

i:α≤i≤β−1

In a random signal model  v is assumed to be generated from a probability density V ∼ f. In this
case  the following is an important quantity measuring the information loss of a quantizer.
Deﬁnition 2. (Distortion) The distortion of a b-bit quantizer Qb with respect to distribution f is

Db = EV ∼f [(V − Qb(v))2] =

(v − Qb(v))2f (v)dv.

(1)

(cid:90)

Q(XR). We are interested in using XQ for learning problems instead of X.

Uniform quantizer is the most simple quantizer  whose partitions are equal size bins with length (cid:52)
(i.e.  ti+1 − ti = (cid:52) ∀i with ﬁnite ti  ti+1) and the reconstruction levels are simply the mid points of
the bins. Lloyd-Max (LM) quantizer [29  31] is designed to minimize the distortion with respect to
a given distribution. In this present paper  we optimize LM quantizer with respect to standard normal
i x with i = 1  ..  k  x ∈ X is marginally N (0  1) under Gaussian RP’s. Now
distribution  since any rT
suppose Q is a quantizing function that operates element-wise on matrix. The quantized RP is deﬁned
as XQ = 1√
The inner product estimate. It is easy to show that for x1  x2 ∼ X with ρ12 = cos(x1  x2)  the
projections (RT x1  RT x2) consist of k i.i.d. samples from N
. One important
application is to use the projections to estimate ρ12. It is well-known that the inner product of two
projected vectors is an unbiased estimator of ρ12  i.e.  E[ˆρR] = E[ xT
] = ρ12. This estimator is
called the full-precision estimator. For quantized RP’s  we analogously deﬁne the quantized estimator
as ˆρQ = Q(RT x1)T Q(RT x2)
  whose statistical property is studied in [27  28]. In most cases  ˆρQ is
biased. The following analytical concept is considered in [28]  which is also helpful in our analysis.
Deﬁnition 3. (Debiased variance) Denote the space of expectation of estimator ˆρQ as E. If there
exists a map g : [−1  1] → E  the debiased estimator is deﬁned by applying the inverse map
Q = g−1(ˆρ) to correct for the bias. The variance of ˆρdb
ˆρdb
2Instance normalization is a standard data preprocessing step for many learning models. In this paper  this
assumption is mainly for convenience. Our analysis can be modiﬁed for scenarios without data normalization.

Q is called the debiased variance.

(cid:19)
(cid:18)(cid:18)0

(cid:18)1 ρ

(cid:19)(cid:19)

 

0

1 RRT x2

k

ρ 1

k

k

2

3 Quantized Compressive Nearest Neighbor Classiﬁcation

We ﬁrst look at the generalization error incurred by learning using XQ instead of X on nearest
neighbor (NN) classiﬁcation problem  which is a simple but powerful non-parametric algorithm
that is popular in practice. Given a dataset S = (X  Y ) and a test sample (x  y) ∼ D where
y is unknown  the algorithm ﬁnds the nearest neighbors of x in X  denoted by (x(1)  y(1))  and
classiﬁes x as ˆy = y(1). We denote the classiﬁer of NN as hS(x) = y(1)  in the original sample
space. Denote the conditional distribution of y given x ∼ X as η(x) = P (y = 1|x). A Bayes
classiﬁer  h∗(x) = 1{η(x) > 1/2}  is well known as the optimal solution in minimizing the risk
L(h(x)) = Ex[1{h(x) (cid:54)= y}] over all hypothesis. [8] showed that the risk of NN classiﬁer converges
to 2L(h∗(x)) as sample size n → ∞. See additional asymptotic analysis in [15  37  18]. In ﬁnite
n case  [33  17  7] studied the error bounds and convergence rate of NN classiﬁer  all of which
require the sample size n increases exponentially in dimensionality d  under some Lipschitz-type
assumptions on the conditional probability function η(x). As discussed in [33  21]  by the celebrated
No-Free-Lunch Theorem [39]  this exponential sample complexity comes from the nature of this
problem and cannot be reduced in general.

Classical ﬁnite sample analysis. Yet  the work [21] demonstrates that when data has small “metric
size” measured by metric entropy integral γ (which will be deﬁned later)  it is possible to reduce
the sample complexity from O(ed) to O(eγ) by working in the randomly projected space using XR.
This is called compressive NN classiﬁcation. The following deﬁnitions are necessary for our analysis.
Deﬁnition 4. Let (T  (cid:107) · (cid:107)) be a totally bounded metric space  and α > 0. T is α-separated if
∀a − b ∈ T   a (cid:54)= b  (cid:107)a − b(cid:107) ≥ α holds. The α-packing number of T is N(cid:107)·(cid:107)(α T ) = max{|T (cid:48)| :
T (cid:48) is α-separable T (cid:48) ⊂ T }.
Deﬁnition 5. The α-entropy of T is deﬁned as Z(α T ) = log N (α T )  and function Z(· T ) is
called the metric entropy of T .
Theorem 1. [22]. Let X ⊂ Rd  and R ∈ Rd×k a random matrix with i.i.d. Gaussian or Rademacher
(cid:107)a−b(cid:107) : a  b ∈ X} be the set of all pair-wise normalized
entries with mean 0 and variance σ2. T = { a−b
constant c  such that ∀ω  δ ∈ (0  1)  if k ≥ cω−2(γ(T )2 + log(2/δ))  then with probability at least
1 − δ  R is ω-isometry on X   namely 

chords. Deﬁne metric entropy integral as γ(T ) =(cid:82) 1

(cid:112)Z(α T )dα  then there exists an absolute

0

(1 − ω)kσ2(cid:107)x − y(cid:107)2 ≤ (cid:107)RT x − RT y(cid:107)2 ≤ (1 + ω)kσ2(cid:107)x − y(cid:107)2 ∀x  y ∈ X .

Theorem 1 is a generalization of Johnson-Lindenstrauss Lemma  which characterizes the probability
of getting a “good” projection matrix with nice isometry property. By a careful analysis under a
slightly different assumption on the domain X   we present the generalization bound on compressive
NN classiﬁer (learning with XR) in [21] as follows.
Theorem 2. X ∼ X n  Y ∼ {0  1}n with X = [x1  ...  xn]T ∈ Rn×d  x is on the unit sphere. Assume
that η(x) = P r(y = 1|x) is L-Lipschitz. Let R ∈d×k  k < d a random matrix with i.i.d. Gaussian
R ) ∈ (X  Y )
entries following N (0  1). (x  y) is a test sample with unknown y. Denote (x(1)
the training sample such that 1√
RT x in the projected space 
k
R . Denote L(h∗) the risk of Bayes classiﬁer. Then
and the compressive NN classiﬁer hR(x) = y(1)
∀ω  δ ∈ (0  1)  if k = O(ω−2(γ(T )2 + log(2/δ))  with probability 1 − δ we have the risk of
compressive NN classiﬁer

R is the nearest neighbor of 1√

R   y(1)

RT x(1)

k

EX Y [L(hR(x))] ≤ 2L(h∗(x)) + 2

2(L

k

k+1 (ne)− 1

k+1

)

k.

(2)

√

(cid:114) 1 + ω

1 − ω

√

Equipped with above tools  we are now ready to state our ﬁrst result on the risk of uniformly quantized
compressive nearest neighbor classiﬁer  with ﬁnite n and k.
Theorem 3. Let X  Y  R and η(x) be the same as in Theorem 2. Q is a b-bit uniform quantizer with
bin width (cid:52). Suppose (x  y) is a test sample with unknown y. Denote (x(1)
Q ) ∈ (X  Y ) the
training sample such that 1√
Q(RT x) in the quantized
k
Q . Let L(h∗) be the risk of Bayes
space  and the quantized compressive NN classiﬁer hQ(x) = y(1)

Q ) is the nearest neighbor of 1√

Q(RT x(1)

Q   y(1)

k

3

rule. Then ∀ω  δ ∈ (0  1)  if k = O(ω−2(γ(T )2 + log(2/δ))  [−√
the maximal gap gQ (cid:44) gQ(−√

√
1 + ω) < 2

1 + ω] ⊂ [t0  t2b ] and
1 + ω  then with probability 1−δ over random

1 + ω 

1 + ω 

√

√

draws of R  the risk of quantized compressive NN classiﬁer is bounded by

(cid:114) 1 + ω

1 − ω

√

L(cid:52)
gQ

2(

√

k +

2L(cid:52)√
√
k
1 − ω

.

(3)

EX Y [L(hQ(x))] ≤ 2L(h∗(x)) + 2

k

k+1 (ne)− 1

k+1

)

Remark. The assumption that Q is uniform quantizer is only for the ease of presentation. For an
arbitrary quantizer  the bound also holds with (cid:52) replaced by a more complicated term.
The proof involves two interleaving covers of the projected space  which  by Theorem 1  has bounded
diameter with high probability. Now we compare Theorem 3 with Theorem 2. Denote the second
term in (3) as the random projection error and the last term as quantization error. We observe: 1) The
bound preserves sample complexity of O(ek)  which is favorable. 2) The extra quantization error
decreases with smaller bin length (cid:52)  which is reasonable since small (cid:52) implies better approximation
to the full-precision RP’s in general. 3) When (cid:52) → 0 which means no quantization applied  we have
gQ = (cid:52) and the bound reduces to (2) in Theorem 2. Note that  although the factor
k in quantization
error term also appears in the RP error term  it implies that the error incurred by quantization becomes
larger as k increases. Intuitively  however  large k provides better estimation of the pair-wise angle and
thus pair-wise distance (since X has domain Sd)  which should actually reduce the extra loss  since
nearest neighbors would be more accurately estimated. This unsatisfactory pattern of quantization
error in Theorem 3 comes from the ﬁnite sample setting and proof methodology  since the bound is a
worst case bound with n and k both ﬁnite. Thus  this bound is less meaningful for practical purposes.

√

Asymptotic analysis. Notice that the key difference between NN classiﬁer  compressive NN and
quantized compressive NN is simply the space in which we look for the neighbors. More importantly 
this procedure essentially depends on the distance estimation. Given that X is deﬁned on the unit
sphere  ﬁnding NN in projected or quantized space is identical to ﬁnding xi ∈ X that has largest
estimated cosine between test example x. In this case  we do not need to care about the speciﬁc space
from which we derive the estimator  while the statistical property becomes the major concern.
Theorem 4. (Asymptotic k). Let data X  Y and projection matrix R be same as Theorem 3. Let
(x  y) be a test sample with unknown y. Q is any arbitrary quantizer with increasing reconstruction
levels. We estimate the cosine between any two points s  t ∈ X with (cid:104)s  t(cid:105) = ρs t in the quantized
. Assume that ∀s  t ∼ X   E[ˆρQ(s  t)] = αρs t for some α > 0.
space by ˆρQ(s  t) = Q(RT s)T Q(RT t)
Denote (x(1)
Q ) is the nearest neighbor of
Q . Then we have as k → ∞ 
1√
k

Q(RT x(1)
Q(RT x)  and the quantized compressive NN classiﬁer hQ(x) = y(1)
EX Y R[L(hQ(x))] ≤ EX Y [L(hS(x))] + rk 

Q ) ∈ (X  Y ) the training sample such that 1√

Q   y(1)

k

k

(cid:88)

i:xi∈G

Φ(cid:0)

(cid:113)

√

k(cos(x  xi) − cos(x  x(1)))

ξ2
x xi

+ ξ2

x x(1) − 2Corr(ˆρQ(x  xi)  ˆρQ(x  x(1)))ξx xiξx x(1)

(cid:1)] 

where rk = EX x[

x y/k the debiased variance of ˆρQ(x  y) and G = X/x(1). L(hS(x)) is the risk of data space
with ξ2
NN classiﬁer  hS(x) = y(1) with (x(1)  y(1)) the nearest neighbor of x. Φ(·) is the CDF of N (0  1).
Remark. We express the bound in terms of EX Y [L(hS(x))] to highlight the extra quantization error.
The assumption that ˆρQ has expectation linear in ρ is mainly for the ease of analytical consideration.
Similar result also holds in general situations  under additional minor assumptions.

The bound is intuitive  in the sense that the quantization error term rk represents the probability of
picking different nearest neighbor in data space and quantized space. The beneﬁt of Theorem 4 is
that  we factor out L(hS(x))  instead of L(hR(x)) as in Theorem 3. Conceptually  we get rid of the
error incurred by using the projected space as an intermediate step. The quantization error term rk
is interesting—Note that for ∀i ∈ G = X/x(1)  cos(x  xi) − cos(x  x(1)) < 0 holds. Consequently 
when k → ∞  all the Φ(·) terms in rk would decrease towards 0 (since Φ(t) → 0 as t → −∞).
Therefore  we derive a well behaving quantization error term in the asymptotic case: the quantization
error indeed decreases with k and converges to that of the data space nearest neighbor classiﬁer.

4

Choice of Q. It can be shown that under some mild conditions  small debiased variance (ξx xi and
ξx x(1)) reduces the quantization error rk in Theorem 3. In addition  by the asymptotic normality
of ˆρQ  given a large k and a query x  points near x(1) (i.e.  with small | cos(x  xi) − cos(x  x(1))|)
tend to affect the quantization error more substantially due to the light tail of Gaussian distribution.
Hence  for 1-NN classiﬁcation  we should ideally choose quantizers with low debiased variance
around ρ∗ = cos(x  x(1)
)  provided that it can be known (or estimated) a priori. In particular  if a
quantized estimator has lower debiased variance than the full-precision estimator  then learning with
XQ would outperform learning with XR in 1-NN classiﬁcation.
Is there a way to reduce the debiased variance of inner product estimates  for better generalization in
NN classiﬁcation? Recent progress on quantized random projections [28] shows that normalizing the
randomly projected vectors (i.e.  RT xi  i = 1  ...  n) can provide smaller debiased variance  especially
in high similarity region (large |ρ|). This is exactly the situation for most of the NN classiﬁcations
where ρ∗ = cos(x  x(1)

) is high. More speciﬁcally  we can use the estimator

i

i

ˆρQ n =

Q(RT x1)T Q(RT x2)
(cid:107)Q(RT x1)(cid:107)(cid:107)Q(RT x2)(cid:107)  

(4)

to estimate ρ(x1  x2)  instead of the simple inner product estimator ˆρQ = Q(RT x1)T Q(RT x2)
Theorem 4. We refer interested readers to [28] for more detailed discussions on this topic.
In the following  we derive a corollary regarding the error of compressive NN classiﬁer hR(x) by
noticing that the full-precision RP corresponds to applying quantization with inﬁnite bits.
  ∀x1  x2 ∈ X .
Lemma 1. Let full-precision linear estimator ˆρR be deﬁned as ˆρR(x1  x2) = xT
Suppose x  y  z ∈ Rd are three data points on a unit sphere with inner products ρxy  ρxz and ρyz
respectively. Then the covariance

used in

1 RRT x2

k

k

Cov(ˆρR(x  y)  ˆρR(x  z)) =

(ρyz + ρxyρxz).

1
k

Corollary 1. Let the data (X  Y )  (x  y) and projection matrix R be same as Theorem 3  with Q a
quantizer with increasing reconstruction levels. We estimate the cosine between any two points s  t ∈
X with (cid:104)s  t(cid:105) = ρs t in the projected space by ˆρR(s  t) = sT RRT t
R ) ∈ (X  Y ) the
training sample such that RT x(1)
Q is the nearest neighbor of RT x in the projected space  and the NN
classiﬁer hR(x) = y(1)

. Denote (x(1)

R   y(1)

k

R . Then as k → ∞ 
√

i:xi∈G Φ(cid:0)

√

EX Y R[L(hR(x))] ≤ EX Y [L(hS(x))] + rk 

k(cos(x xi)−cos(x x(1)))

(cos(x xi)−cos(x x(1)))2+2(1−cos(xi x(1)))

(cid:1)]  with G = X/x(1).

where rk = EX x[(cid:80)

4 Quantized Compressive Linear Classiﬁcation with (0 1)-loss

In this section  we consider the generalization error for binary linear classiﬁers  which include some
of the most popular learning models  e.g.  logistic regression  linear SVM  etc. Let H be a hypothesis
class of functions on X → {0  1}. For original data  we assume that a function H ∈ H separates S
by a hyperplane  and classify each side as a distinct class. Hence  for a test data point x  the predicted
label returned by H is

H(x) = 1{hT x > 0} 

where h is a vector in Rd and orthogonal to the separating plane. Since all xi’s are normalized to
unit norm  we may assume that h also lies on the unit sphere passing though the origin. The optimal
classiﬁer  ˆH  is the minimizer of (0 1)-loss  deﬁned as

ˆL(0 1)(S  h) =

1
n

n(cid:88)

i=1

L(0 1)(H(xi)  yi)  L(0 1)(H(xi)  yi) =

(5)

We denote ˆh ∈ Rd the learned vector associated with ˆH. ( ˆH  ˆh) is called the empirical risk
minimization (ERM) classiﬁer. In projected space and quantized space  the ERM classiﬁers are
denoted by similar notation with corresponding subscripts as ˆHR  ˆhR ∈ Rk  ˆHQ and ˆhQ ∈ Rk 

ˆH(x) = 1{ˆhT x > 0} 

ˆHR(x) = 1{ˆhT

RRT x > 0} 

ˆHQ(x) = 1{ˆhT

QQ(RT x) > 0}.

(6)

(cid:26)0  if H(xi) = yi 

1  otherwise.

5

Figure 1: Sign ﬂipping in quantized space. Points on the right of black decision boundary are
classiﬁed as 1  and 0 otherwise. Green dashed lines are boarders of the quantizer. Left: data space
classiﬁer predicts 1. Right: quantized space prediction (using Q(ˆhT R) as predictor) changes to 0.

Now suppose x is a test sample with unknown class y  we are interested in the probability of making
a wrong prediction by training the classiﬁer in the quantized space 
P r[ ˆHQ(x)) (cid:54)= y] = E[L( ˆHQ(x)  y)].

Existing results on such compressive linear classiﬁer have studied bounds on the same type of
objective in the projected space  under ﬁnite k setting [13]. Here  we look at this problem in the
asymptotic domain. When studying the error incurred by learning in the projected space  an important
tool is the following deﬁnition.
Deﬁnition 6. Let ˆh  x ∈ Rd be deﬁned above  (cid:107)ˆh(cid:107) = (cid:107)x(cid:107) = 1. Let (cid:104)ˆh  x(cid:105) = cos(ˆh  x) = ρ > 0 
. R ∈ Rd×k an i.i.d. standard Gaussian random matrix. The ﬂipping probability
and ˆρR = ˆhT RRT x
is deﬁned as

k

fk(ρ) = P r[ρR < 0|ρ > 0].

(7)

Intuitively  this quantity measures the probability of changed prediction with the compressed model
which learns in the space projected by R when RT ˆh is the classiﬁer. [13] gives the exact formula of
this quantity  which reads as

fk(ρ) =

Γ(k)

Γ(k/2)2

0

z(k−2)/2
(1 + z)k dz = Fk k(

1 − ρ
1 + ρ

) 

(8)

(cid:90) 1−ρ

1+ρ

where F is the cumulative distribution function (CDF) of F-distribution with (k  k) degrees of
freedom. This formula also holds for ρ < 0 by simply plugging in ρ = −ρ. By symmetry  it
sufﬁces to consider ρ > 0. As it is well-known that E[ˆρR] = ρ and V ar[ˆρR] = 1+ρ2
k   ˆρR should
asymptotically follow N (ρ  1+ρ2

k ) as k → ∞. So the asymptotic ﬂipping probability should be

˜fk(ρ) = Φ(−

√

kρ(cid:112)1 + ρ2

).

(9)

The following proposition conﬁrms this asymptotic convergence.
Proposition 1. As k → ∞  we have fk(ρ) → ˜fk(ρ) for ρ > 0.
For quantized compressive classiﬁer  sign ﬂipping may also happen (an illustrative example is given
in Figure 1). By analyzing this event  in the following we state the asymptotic generalization error
(cid:80)n
bound for linear classiﬁers when working in quantized space instead of data space.
Theorem 5. Let the (0 1)-loss and ERM classiﬁer be deﬁned as (5) and (6). R ∈ Rd×k is i.i.d
standard normal random matrix. Let ˆL(0 1)(S  ˆh) = 1
i=1 L(0 1)(H(xi)  yi) be the empirical loss
in the data space. Q is a quantizer and the quantized estimator ˆρQ = Q(RT s)T Q(RT t)
has mean αρ 

n

k

6

α > 0  and debiased variance ξ2
unknown  when k → ∞  with probability at least 1 − 2δ we have

ρ/k at ρ = cos(s  t)  ∀s  t ∼ X . Given (x  y) a test sample with y

(cid:115)

(k + 1) log 2en

k+1 + log 1
n

δ

(cid:26)(cid:114)

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

1
δ

n(cid:88)

i=1

(cid:27)

fk Q(ρi)

 

n(cid:88)

i=1

1 − δ
δn

P r[ ˆHQ(x) (cid:54)= y] ≤ ˆL(0 1)(S  ˆh) + 2

n(cid:88)

i=1

+

1
n

fk Q(ρi) + min

3 log

fk Q(ρi) 

where the ﬂipping probability fk Q(ρi) = Φ(−
xi and ERM classiﬁer ˆh in the data space.

√

k|ρi|
ξρi

)  with ρi the cosine between training sample

In Theorem 5  the ﬁrst term is the empirical loss in the data space  and the second term is the generic
sample complexity in learning theory. The last two terms are called the quantization error. When
b → ∞ (full-precision RP’s)  the bound reduces to that derived in [13] for compressive linear
classiﬁer  according to Proposition 1. One important observation is that the quantization error again
depends on the debiased variance of the quantized inner product estimator  at different ρi  i = 1  ...  n.
This result provides some insights on the inﬂuence of quantization for linear classiﬁcation.

Choice of Q. Unlike NN classiﬁer  the extra generalization error depends more on the region near
0 for linear classiﬁer. To see this  we notice that the ﬂipping probabilities (8) and (9) decrease as ρ
increases. Intuitively  label ﬂipping is much more likely to occur for the points near the boundary (i.e. 
with small ˆhT x). As a result  one may choose a quantizer with small debiased variance around ρ = 0
for linear classiﬁcation. In fact  by the analysis and results from [28]  one can show that Lloyd-Max
(LM) quantizer gives minimal debiased variance of ˆρQ at ρ = 0  among all quantizers with same bits.
Hence  LM quantization is recommended for linear classiﬁcation problems.

5 Quantized Compressive Least Squares Regression

Compressive least squares (CLS) regression has been studied in several papers  e.g.  [30  20]. [34]
shows that in many cases  CLS can match the performance of principle component regression (PCR)
but runs faster by avoiding large scale SVD or optimization  especially on high-dimensional data. In
CLS  the projected design matrix XR  instead of the original X  is used for ordinary least squares
(OLS) regression. We are interested in the extra error brought by further quantizing the projections 
where XQ is used as the new design matrix. We call this approach QCLS. In particular  we consider
a ﬁx design problem where data X ∈ Rn×d is determinant and Y ∈ Rn are treated as random. OLS
regression with Gaussian error is modeled by

(10)
with β ∈ Rd and  ∈ Rn contains i.i.d. Gaussian noise with mean 0 and variance γ. For projected
data and quantized data  we also ﬁt a OLS with same response Y   while the predictors becomes
1√
k

Q(RT xi) respectively. Furthermore  deﬁne the expected squared losses as

RT xi and 1√

Y = Xβ +  

k

L(β) =

1
n

EY [(cid:107)Y −Xβ(cid:107)2]  LR(βR) =
EY |R[(cid:107)Y − 1√
k

LQ(βQ) =

1
n

XRβR(cid:107)2] 

EY |R[(cid:107)Y − 1√
1
n
k
Q(XR)βQ(cid:107)2].

(11)

Note that in the above the expectation is taken w.r.t. Y   and R is given. Denote the true minimizers
of above losses as β∗  β∗
Q  respectively. The risk of an estimator in the data space is deﬁned
as r(w) = L(w) − L(β∗)  and analogues rR(wR) and rQ(wQ) can be also deﬁned in projected and
quantized spaces. On the other hand  we have the empirical losses

R and β∗

ˆL(β) =

(cid:107)Y −Xβ(cid:107)2 

1
n

ˆLR(βR) =

1
n

(cid:107)Y − 1√
k

XRβR(cid:107)2 

ˆLQ(βQ) =

1
n

(cid:107)Y − 1√
k

Q(XR)βQ(cid:107)2 
(12)

7

ˆβ∗ = argmin
β∈Rd

ˆL(β) 

ˆLR(β) 

ˆβ∗
R = argmin
β∈Rk
R and ˆβ∗

ˆβ∗
Q = argmin
β∈Rk

ˆLQ(β).

(13)

which are computed from the data. The least squares estimates minimize the empirical losses in a
given space  namely 

In particular  ˆβ is the OLS estimator  and ˆβ∗
estimator  respectively. The following result bounds the expected loss of ˆβ∗
Theorem 6. Let the regression problems be deﬁned as in (10)  (11)  (12) and (13)  with γ being the
variance of Gaussian noise. Suppose all samples in X has unit norm  Σ = X T X/n  and R ∈ Rd×k
are i.i.d. standard normal with k < n. Q is a Lloyd-Max quantizer with distortion DQ w.r.t.
standard Gaussian. Further deﬁne ξ2 2 = E[Q(x)2x2] with x ∼ N (0  1). Then  the expected QCLS
risk over loss of data space learner is bounded by

Q are called the CLS estimator and QCLS

Q  over Y and R.

EY R[LQ( ˆβ∗

Q)] − L(β∗) ≤ γ
k
√
n
Id  with (cid:107)w(cid:107)Ω =
wT Ωw the Mahalanobis norm and Id the

(1−DQ)2 − 1]Σ + 1
1−DQ

where Ω = [ ξ2 2−1+DQ
identity matrix with rank d.
Remark. Lloyd-Max quantizer is considered for the ease of presentation. Similar result can be
derived for general quantizers under extra technical assumptions. When DQ = 0 (no quantization is
applied)  ξ2 2 = 3 holds and the bound reduces to the classical bound [20] for CLS.

(14)

+

(cid:107)β∗(cid:107)2
Ω 

1
k

Choice of Q.
In Theorem 6  we see that the distortion in general controls the excess risk. In
particular  smaller DQ would reduce expected loss. Although the bound is for LM quantizer  we
expect similar results for other quantizers  since smaller distortion in general implies less deviation
from the compressed signals. Hence  with a ﬁx number of bits  Lloyd-Max (LM) quantizer  which is
built naturally for the purpose of distortion minimization  should be the ﬁrst choice for QCLS.

6 Numerical study
In this section  we validate the theoretical ﬁndings through experiments on real-world datasets from
UCI repository [12]. Table 1 provides summary statistics  where mean ρ is the average pair-wise
cosine of all pairs of samples. Mean 1-NN ρ is the average cosine of each point to its nearest neighbor.

Table 1: Summary statistics of datasets  all standardized to unit norm.

Dataset
arcene

BASEHOCK
orlraws10P

# samples

200
1993
100

# features

10000
4862
10304

# classes Mean ρ Mean 1-NN ρ

2
2
10

0.63
0.33
0.80

0.86
0.59
0.89

Classiﬁcation setup. We test three quantizers: 1-bit
Lloyd-Max quantizer  3-bit Lloyd-Max quantizer and 3-
bit uniform quantizer. LM quantizers are optimized w.r.t.
standard normal distribution  and the uniform quantizer
is symmetric about 0 with (cid:52) = 1  and cut-off points
x = −3.5 if x < −3; x = 3.5 if x > 3. As discussed
in [28]  the debiased variance of ˆρQ = Q(XR)T Q(XR)
cannot be computed exactly. Here we approximate it by
simulation as in Figure 2. For 1-NN classiﬁcation  we take
each data point as test sample and the rest as training data
over all the examples  and report the mean test accuracy.
For linear classiﬁer  we feed the inner product estimation
matrix XQX T
Q as the kernel matrix into a linear SVM
solver [5]. We randomly split the data to 60% for training and 40% for testing  and the best test
accuracy among all hyper-parameter C is reported  averaged over 5 repetition’s.

Figure 2: Empirical debiased variance.
To be divided by k.

k

Linear SVM. At ρ = 0  Figure 2 shows that the debiased variances of estimators using different
quantizers admit the order 1-bit LM>3-bit uniform>3-bit LM>full-precision. Therefore  following
the discussion in Theorem 5  we expect test error in the same order  which is conﬁrmed by Figure 5.

8

0.20.40.60.810123Debiased VarianceFull-precisionLM b=1LM b=3Uniform b=3Figure 4: Test accuracy of quantized compressive nearest neighbor classiﬁcation.

Figure 5: Test accuracy of quantized compressive linear SVM.

Figure 3: Test MSE of QCLS.

NN classiﬁcation. Theorem 4 states that small debi-
ased variance around the “mean 1-NN ρ” should be
beneﬁcial for 1-NN classiﬁcation. BASEHOCK dataset
has mean 1-NN ρ = 0.59  the point at which the debi-
ased variance is compared as 1-bit LM > 3-bit uniform
> full-precision ≈ 3-bit LM. Hence  we see in Figure 4
that the NN classiﬁcation error is in the same sequence
on this dataset. On the other hand  the mean 1-NN ρ
of arcene and orlraws10P is high (around 0.9). At this
point  1-bit LM quantizer has much smaller debiased
variance than others. Therefore  we expect 1-bit LM
to provide highest test accuracy on these two datasets 
which is again consistent with Figure 4. In conclusion 
our empirical observations validate the theoretical results and analysis in Theorem 4 and Theorem 5
on the inﬂuence of debiased estimator variance on NN and linear classiﬁers  at different ρ level.
Simulated QCLS. We simulate data X ∈ R3000×1200 and β both following i.i.d. N (0  1)  and
noise  ∼ N (0  0.2). We compare LM quantizers with equal-bit uniform quantizers  for b = 3  4  5.
The distortion is (0.035  0.009  0.002) for LM quantizers and (0.043  0.026  0.019) for uniform
quantizers. In Figure 3  we see that the order of test MSE perfectly agrees with the order of distortion
from high to low  and LM quantizers always outperform uniform quantizers with same bits. As the
distortion gets smaller  the performance of QCLS approaches that of CLS. These observations verify
the conclusion in Theorem 6 that quantizers with smaller distortion generalize better for QCLS.
7 Concluding Remarks
This paper studies the generalization error of various quantized compressive learning models  includ-
ing nearest neighbor classiﬁer  linear classiﬁer and linear regression. Our theoretical results provide
useful guidance for choosing appropriate quantizers for different models  which in particular depicts
an interesting connection between debiased variance of inner product estimates and the generalization
performance on classiﬁcation tasks. Quantizers with small debiased variance are favorable for NN
classiﬁer and linear classiﬁer  in high similarity region and around ρ = 0  respectively. For linear
regression  quantizers with smaller distortion tend to perform better. As a consequence  Lloyd-Max
(LM) quantizer is recommended for both linear classiﬁcation and regression  and normalizing the
projections may help with nearest neighbor classiﬁcation. Our work contributes to understanding
the underlying statistical aspects of learning with quantized random projections  and provides useful
implications to various machine learning applications where data compression is useful.

9

 26 27 28 29210211212Number of Projections65%70%75%80%85%90%Test AccuracyarceneFull-precisionLM b=1LM b=3Uniform b=3 26 27 28 29210211212Number of Projections80%85%90%95%100%Test AccuracyBASEHOCKFull-precisionLM b=1LM b=3Uniform b=3 26 27 28 29210211212Number of Projections40%60%80%100%Test Accuracyorlraws10PFull-precisionLM b=1LM b=3Uniform b=3 26 27 28 29210211212Number of Projections65%70%75%80%85%90%Test AccuracyarceneFull-precisionLM b=1LM b=3Uniform b=3 26 27 28 29210211212Number of Projections70%80%90%100%Test AccuracyBASEHOCKFull-precisionLM b=1LM b=3Uniform b=3 26 27 28 29210211212Number of Projections60%70%80%90%100%Test Accuracyorlraws10PFull-precisionLM b=1LM b=3Uniform b=32004006008001000Number of Projections0.60.70.80.911.1Test MSEReferences
[1] Petros Boufounos and Richard G. Baraniuk. 1-bit compressive sensing.

In 42nd Annual

Conference on Information Sciences and Systems (CISS)  pages 16–21  Princeton  NJ  2008.

[2] Jeremy Buhler. Efﬁcient large-scale sequence comparison by locality-sensitive hashing. Bioin-

formatics  17(5):419–428  2001.

[3] Robert Calderbank  Sina Jafarpour  and Robert Schapire. Compressed learning: Universal

sparse dimensionality reduction and learning in the measurement domain. preprint  2009.

[4] Emmanuel J. Candès and Terence Tao. Near-optimal signal recovery from random projections:

Universal encoding strategies? IEEE Trans. Information Theory  52(12):5406–5425  2006.

[5] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM

Transactions on Intelligent Systems and Technology  2:27:1–27:27  2011.

[6] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings
on 34th Annual ACM Symposium on Theory of Computing (STOC)  pages 380–388  Montreal 
Canada  2002.

[7] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor clas-
siﬁcation. In Advances in Neural Information Processing Systems (NIPS)  pages 3437–3445 
Montreal  Canada  2014.

[8] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Trans.

Information Theory  13(1):21–27  1967.

[9] George E. Dahl  Jack W. Stokes  Li Deng  and Dong Yu. Large-scale malware classiﬁcation
using random projections and neural networks. In IEEE International Conference on Acoustics 
Speech and Signal Processing (ICASSP)  pages 3422–3426  Vancouver  Canada  2013.

[10] Sanjoy Dasgupta and Anupam Gupta.

An elementary proof of a theorem of

Johnson and Lindenstrauss. Random Structures and Algorithms  22(1):60 – 65  2003.

[11] David L. Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–

1306  April 2006.

[12] Dheeru Dua and Casey Graff. UCI machine learning repository  2017.

[13] Robert J. Durrant and Ata Kabán. Sharp generalization error bounds for randomly-projected

classiﬁers. In ICML  pages 693–701  2013.

[14] Ronald Fagin  Ravi Kumar  and D. Sivakumar. Efﬁcient similarity search and classiﬁcation
via rank aggregation. In Proceedings of the 2003 ACM SIGMOD International Conference on
Management of Data (SIGMOD)  pages 301–312  San Diego  CA  2003.

[15] Jozsef Fritz. Distribution-free exponential error bound for nearest neighbor pattern classiﬁcation.

IEEE Trans. on Information Theory  21(5):552–557  1975.

[16] Ashutosh Garg  Sariel Har-Peled  and Dan Roth. On generalization bounds  projection proﬁle 

and margin distribution. In ICML  pages 171–178  2002.

[17] Lee-Ad Gottlieb  Aryeh Kontorovich  and Pinhas Nisnevitch. Near-optimal sample compression
for nearest neighbors. In Advances in Neural Information Processing Systems (NIPS)  pages
370–378  Montreal  Canada  2014.

[18] László Györﬁ and Zoltán Györﬁ. An upper bound on the asymptotic error probability on
the k-nearest neighbor rule for multiple classes (corresp.). IEEE Trans. Information Theory 
24(4):512–514  1978.

[19] William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mapping into Hilbert

space. Contemporary Mathematics  26:189–206  1984.

10

[20] Ata Kabán. New bounds on compressive linear least squares regression. In Proceedings of
the 17th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages
448–456  Reykjavik  Iceland  2014.

[21] Ata Kabán. A new look at nearest neighbours: Identifying benign input geometries via random
projections. In Proceedings of The 7th Asian Conference on Machine Learning (ACML)  pages
65–80  Hong Kong  China  2015.

[22] B Klartag and Shahar Mendelson. Empirical processes and random projections. Journal of

Functional Analysis  225(1):229–245  2005.

[23] Ping Li. One scan 1-bit compressed sensing. In Proceedings of the 19th International Conference

on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1515–1523  Cadiz  Spain  2016.

[24] Ping Li. Binary and multi-bit coding for stable random projections. In Proceedings of the 20th
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1430–1438 
Fort Lauderdale  FL  2017.

[25] Ping Li  Michael Mitzenmacher  and Anshumali Shrivastava. Coding for random projections.
In Proceedings of the 31th International Conference on Machine Learning (ICML)  Beijing 
China  2014.

[26] Ping Li  Michael Mitzenmacher  and Martin Slawski. Quantized random projections and non-
linear estimation of cosine similarity. In Advances in Neural Information Processing Systems
(NIPS)  pages 2748–2756  Barcelona  Spain  2016.

[27] Ping Li and Martin Slawski. Simple strategies for recovering inner products from coarsely
quantized random projections. In Advances in Neural Information Processing Systems (NIPS) 
pages 4567–4576  Long Beach  CA  2017.

[28] Xiaoyun Li and Ping Li. Random projections with asymmetric quantization. In Advances in

Neural Information Processing Systems (NeurIPS)  Vancouver  Canada  2019.

[29] Stuart P. Lloyd. Least squares quantization in PCM. IEEE Trans. Information Theory  28(2):129–

136  1982.

[30] Odalric-Ambrym Maillard and Rémi Munos. Compressed least-squares regression. In Advances

in Neural Information Processing Systems (NIPS)  pages 1213–1221  2009.

[31] Joel Max. Quantizing for minimum distortion. IRE Trans. Information Theory  6(1):7–12  1960.

[32] Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic regres-
sion: A convex programming approach. IEEE Transactions on Information Theory  59(1):482–
494  2013.

[33] Shai Shalev-Shwartz and Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[34] Martin Slawski. Compressed least squares regression revisited. In Proceedings of the 20th
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1207–1215 
Fort Lauderdale  FL  2017.

[35] Gian-Andrea Thanei  Christina Heinze  and Nicolai Meinshausen. Random projections for

large-scale regression. In Big and complex data analysis  pages 51–68. Springer  2017.

[36] Santosh S. Vempala. The Random Projection Method. American Mathematical Society  2004.
[37] Terry J. Wagner. Convergence of the nearest neighbor rule. IEEE Trans. Information Theory 

17(5):566–571  1971.

[38] Fei Wang and Ping Li. Efﬁcient nonnegative matrix factorization with random projections. In
Proceedings of the SIAM International Conference on Data Mining (SDM)  pages 281–292 
Columbus  Ohio  2010.

[39] David H. Wolpert and William G. Macready. No free lunch theorems for optimization. IEEE

Trans. Evolutionary Computation  1(1):67–82  1997.

11

,Xiaoyun Li
Ping Li