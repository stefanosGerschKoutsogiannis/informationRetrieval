2014,QUIC & DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models,In this paper  we develop a family of algorithms for optimizing superposition-structured” or “dirty” statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods  including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers  and provide convergence guarantees for our algorithm. Empirically  we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems  our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.",QUIC & DIRTY: A Quadratic Approximation

Approach for Dirty Statistical Models

Cho-Jui Hsieh  Inderjit S. Dhillon  Pradeep Ravikumar

University of Texas at Austin

Austin  TX 78712 USA

{cjhsieh inderjit pradeepr}@cs.utexas.edu

Stephen Becker

University of Colorado at Boulder

Boulder  CO 80309 USA

stephen.becker@colorado.edu

Peder A. Olsen

IBM T.J. Watson Research Center
Yorktown Heights  NY 10598 USA

pederao@us.ibm.com

Abstract

In this paper  we develop a family of algorithms for optimizing “superposition-
structured” or “dirty” statistical estimators for high-dimensional problems involv-
ing the minimization of the sum of a smooth loss function with a hybrid reg-
ularization. Most of the current approaches are ﬁrst-order methods  including
proximal gradient or Alternating Direction Method of Multipliers (ADMM). We
propose a new family of second-order methods where we approximate the loss
function using quadratic approximation. The superposition structured regularizer
then leads to a subproblem that can be efﬁciently solved by alternating minimiza-
tion. We propose a general active subspace selection approach to speed up the
solver by utilizing the low-dimensional structure given by the regularizers  and
provide convergence guarantees for our algorithm. Empirically  we show that our
approach is more than 10 times faster than state-of-the-art ﬁrst-order approaches
for the latent variable graphical model selection problems and multi-task learning
problems when there is more than one regularizer. For these problems  our ap-
proach appears to be the ﬁrst algorithm that can extend active subspace ideas to
multiple regularizers.

1

Introduction

From the considerable amount of recent research on high-dimensional statistical estimation  it has
now become well understood that it is vital to impose structural constraints upon the statistical model
parameters for their statistically consistent estimation. These structural constraints take the form of
sparsity  group-sparsity  and low-rank structure  among others; see [18] for uniﬁed statistical views
of such structural constraints.
In recent years  such “clean” structural constraints are frequently
proving insufﬁcient  and accordingly there has been a line of work on “superposition-structured” or
“dirty model” constraints  where the model parameter is expressed as the sum of a number of param-
eter components  each of which have their own structure. For instance  [4  6] consider the estimation
of a matrix that is neither low-rank nor sparse  but which can be decomposed into the sum of a low-
rank matrix and a sparse outlier matrix (this corresponds to robust PCA when the matrix-structured
parameter corresponds to a covariance matrix). [5] use such matrix decomposition to estimate the
structure of latent-variable Gaussian graphical models. [15] in turn use a superposition of sparse
and group-sparse structure for multi-task learning. For other recent work on such superposition-
structured models  see [1  7  14]. For a uniﬁed statistical view of such superposition-structured
models  and the resulting classes of M-estimators  please see [27].

Consider a general superposition-structured parameter ¯θ := (cid:80)k

r=1 are the
parameter-components  each with their own structure. Let {R(r)(·)}k
r=1 be regularization functions
suited to the respective parameter components  and let L(·) be a (typically non-linear) loss function

r=1 θ(r)  where {θ(r)}k

1

that measures the goodness of ﬁt of the superposition-structured parameter ¯θ to the data. We now
have the notation to consider a popular class of M-estimators studied in the papers above for these
superposition-structured models:
L

+(cid:88)

(cid:18)(cid:88)

λrR(r)(θ(r))

:= F (θ) 

(cid:26)

(cid:27)

(cid:19)

(1)

min
{θ(r)}k

r=1

θ(r)

r

r

on the sum ¯θ :=(cid:80)k

where {λr}k
r=1 are regularization penalties. In (1)  the overall regularization contribution is sepa-
rable in the individual parameter components  but the loss function term itself is not  and depends
r=1 θ(r). Throughout the paper  we use ¯θ to denote the overall superposition-

structured parameter  and θ =[θ(1) . . .  θ(k)] to denote the concatenation of all the parameters.
Due to the wide applicability of this class of M-estimators in (1)  there has been a line of work on
developing efﬁcient optimization methods for solving special instances of this class of M-estimators
[14  26]  in addition to the papers listed above. In particular  due to the superposition-structure in
(1) and the high-dimensionality of the problem  this class seems naturally amenable to a proximal
gradient descent approach or the ADMM method [2  17]; note that these are ﬁrst-order methods and
are thus very scalable.
In this paper  we consider instead a proximal Newton framework to minimize the M-estimation ob-
jective in (1). Speciﬁcally  we use iterative quadratic approximations  and for each of the quadratic
subproblems  we use an alternating minimization approach to individually update each of the pa-
rameter components comprising the superposition-structure. Note that the Hessian of the loss might
be structured  as for instance with the logdet loss for inverse covariance estimation and the logistic
loss  which allows us to develop very efﬁcient second-order methods. Even given this structure 
solving the regularized quadratic problem in order to obtain the proximal Newton direction is too
expensive due to the high dimensional setting. The key algorithmic contribution of this paper is in
developing a general active subspace selection framework for general decomposable norms  which
allows us to solve the proximal Newton steps over a signiﬁcantly reduced search space. We are
able to do so by leveraging the structural properties of decomposable regularization functions in the
M-estimator in (1).
Our other key contribution is theoretical. While recent works [16  21] have analyzed the conver-
gence of proximal Newton methods  the superposition-structure here poses a key caveat: since the
loss function term only depends on the sum of the individual parameter components  the Hessian is
not positive-deﬁnite  as is required in previous analyses of proximal Newton methods. The theoret-
ical analysis [9] relaxes this assumption by instead assuming the loss is self-concordant but again
allows at most one regularizer. Another key theoretical difﬁculty is our use of active subspace se-
lection  where we do not solve for the vanilla proximal Newton direction  but solve the proximal
Newton step subproblem only over a restricted subspace  which moreover varies with each step. We
deal with these issues and show super-linear convergence of the algorithm when the sub-problems
are solved exactly. We apply our algorithm to two real world applications: latent Gaussian Markov
random ﬁeld (GMRF) structure learning (with low-rank + sparse structure)  and multitask learning
(with sparse + group sparse structure)  and demonstrate that our algorithm is more than ten times
faster than state-of-the-art methods.
Overall  our algorithmic and theoretical developments open up the state of the art but forbidding
class of M-estimators in (1) to very large-scale problems.
Outline of the paper. We begin by introducing some background in Section 2. In Section 3 
we propose our quadratic approximation framework with active subspace selection for general dirty
statistical models. We derive the convergence guarantees of our algorithm in Section 4. Finally  in
Section 5  we apply our model to solve two real applications  and show experimental comparisons
with other state-of-the-art methods.
2 Background and Applications
Decomposable norms. We consider the case where all the regularizers {R(r)}k
r=1 are decompos-
able norms (cid:107) · (cid:107)Ar. A norm (cid:107) · (cid:107) is decomposable at x if there is a subspace T and a vector e ∈ T
such that the sub differential at x has the following form:

∂(cid:107)x(cid:107)r = {ρ ∈ Rn | ΠT (ρ) = e and (cid:107)ΠT ⊥(ρ)(cid:107)∗
Ar

(2)
where ΠT (·) is the orthogonal projection onto T   and (cid:107)x(cid:107)∗ := sup(cid:107)a(cid:107)≤1(cid:104)x  a(cid:105) is the dual norm of
(cid:107) · (cid:107). The decomposable norm was deﬁned in [3  18]  and many interesting regularizers belong to
this category  including:

≤ 1} 

2

groups  say G = {G1  . . .   GNG}  and deﬁne the (1 α)-group norm by (cid:107)x(cid:107)1 α :=(cid:80)NG

• Sparse vectors: for the (cid:96)1 regularizer  T is the span of all points with the same support as x.
• Group sparse vectors: suppose that the index set can be partitioned into a set of NG disjoint
t=1 (cid:107)xGt(cid:107)α. If
SG denotes the subset of groups where xGt (cid:54)= 0  then the subgradient has the following form:

∂(cid:107)x(cid:107)1 α := {ρ | ρ = (cid:88)

t∈SG

xGt/(cid:107)xGt(cid:107)∗

mt} 

α + (cid:88)

t /∈SG

where (cid:107)mt(cid:107)∗

α ≤ 1 for all t /∈ SG. Therefore  the group sparse norm is also decomposable with

(3)
• Low-rank matrices: for the nuclear norm regularizer (cid:107) · (cid:107)∗  which is deﬁned to be the sum of
singular values  the subgradient can be written as

T := {x | xGt = 0 for all t /∈ SG}.

∂(cid:107)X(cid:107)∗ = {U V T + W | U T W = 0  W V = 0 (cid:107)W(cid:107)2 ≤ 1} 

j }k

i j=1) where {ui}k

where (cid:107) · (cid:107)2 is the matrix 2 norm and U  V are the left/right singular vectors of X corresponding to
non-zero singular values. The above subgradient can also be written in the decomposable form (2) 
where T is deﬁned to be span({uivT
i=1 are the columns of U and V .
Applications. Next we discuss some widely used applications of superposition-structured models 
and the corresponding instances of the class of M-estimators in (1).
• Gaussian graphical model with latent variables: let Θ denote the precision matrix with corre-
sponding covariance matrix Σ = Θ−1. [5] showed that the precision matrix will have a low rank
+ sparse structure when some random variables are hidden  thus Θ = S − L can be estimated by
solving the following regularized MLE problem:

i=1 {vi}k

− log det(S − L) + (cid:104)S − L  Σ(cid:105) + λS(cid:107)S(cid:107)1 + λL trace(L).

(4)

min

S L:L(cid:23)0 S−L(cid:31)0

While proximal Newton methods have recently become a dominant technique for solving the (cid:96)1-
regularized log-determinant problems [12  10  13  19]  our development is the ﬁrst to apply proximal
Newton methods to solve log-determinant problems with sparse and low rank regularizers.
• Multi-task learning: given k tasks  each with sample matrix X (r) ∈ Rnr×d (nr samples in the
r-th task) and labels y(r)  [15] proposes minimizing the following objective:
(cid:96)(y(r)  X (r)(S(r) + B(r))) + λS(cid:107)S(cid:107)1 + λB(cid:107)B(cid:107)1 ∞ 

k(cid:88)

(5)

r=1

where (cid:96)(·) is the loss function and S(r) is the r-th column of S.
• Noisy PCA: to recover a covariance matrix corrupted with sparse noise  a widely used technique
is to solve the matrix decomposition problem [6]. In contrast to the squared loss above  an exponen-
tial PCA problem [8] would use a Bregman divergence for the loss function.
3 Our proposed framework

To perform a Newton-like step  we iteratively form quadratic approximations of the smooth loss
function. Generally the quadratic subproblem will have a large number of variables and will be hard
to solve. Therefore we propose a general active subspace selection technique to reduce the problem
size by exploiting the structure of the regularizers R1  . . .  Rk.

3.1 Quadratic Approximation
Given k sets of variables θ = [θ(1)  . . .   θ(k)]  and each θ(r) ∈ Rn  let ∆(r) denote perturbation of
r=1 θ(r)) = L(¯θ) to be the loss function 
r=1 R(r)(θ(r)) to be the regularization. Given the current estimate θ  we form the

θ(r)  and ∆ = [∆(1)  . . .   ∆(k)]. We deﬁne g(θ) := L((cid:80)k
and h(θ) :=(cid:80)k

quadratic approximation of the smooth loss function:

¯g(θ + ∆) = g(θ) +

(6)
where G = ∇L(¯θ) is the gradient of L and H is the Hessian matrix of g(θ). Note that ∇¯θL(¯θ) =
∇θ(r)L(¯θ) for all r so we simply write ∇ and refer to the gradient at ¯θ as G (and similarly for ∇2).
By the chain rule  we can show that

r=1

(cid:104)∆(r)  G(cid:105) +

∆TH∆ 

1
2

k(cid:88)

3

Lemma 1. The Hessian matrix of g(θ) is

H := ∇2g(θ) =

H ··· H

...
...
H ··· H

...

   H := ∇2L(¯θ).

(7)

In this paper we focus on the case where H is positive deﬁnite. When it is not  we add a small
constant  to the diagonal of H to ensure that each block is positive deﬁnite.
Note that the full Hessian  H  will in general  not be positive deﬁnite (in fact rank(H) = rank(H)).
However  based on its special structure  we can still give convergence guarantees (along with rate of
convergence) for our algorithm. The Newton direction d is deﬁned to be:

[d(1)  . . .   d(k)] = argmin

¯g(θ + ∆) +

∆(1) ... ∆(k)

r=1

λr(cid:107)θ(r) + ∆(r)(cid:107)Ar := QH(∆; θ).

(8)

The quadratic subproblem (8) cannot be directly separated into k parts because the Hessian matrix
(7) is not a block-diagonal matrix. Also  each set of parameters has its own regularizer  so it is hard
to solve them all together. Therefore  to solve (8)  we propose a block coordinate descent method.
At each iteration  we pick a variable set ∆(r) where r ∈ {1  2  . . .   k} by a cyclic (or random) order 
and update the parameter set ∆(r) while keeping other parameters ﬁxed. Assume ∆ is the current
solution (for all the variable sets)  then the subproblem with respect to ∆(r) can be written as

k(cid:88)

∆(r) ← argmin
d∈Rn

2 dT Hd + (cid:104)d  G + (cid:88)

1

t:r(cid:54)=t

H∆(t)(cid:105) + λr(cid:107)θ(r) + d(cid:107)Ar .

(9)

The subproblem (9) is just a typical quadratic problem with a speciﬁc regularizer  so there already
exist efﬁcient algorithms for solving it for different choices of (cid:107) · (cid:107)A. For the (cid:96)1 norm regularizer 
coordinate descent methods can be applied to solve (9) efﬁciently as used in [12  21]; (accelerated)
proximal gradient descent or projected Newton’s method can also be used  as shown in [19]. For a
general atomic norm where there might be inﬁnitely many atoms (coordinates)  a greedy coordinate
descent approach can be applied  as shown in [22].

To iterate between different groups of parameters  we have to maintain the term(cid:80)k

r=1 H∆(r) during
the Newton iteration. Directly computing H∆(r) requires O(n2) ﬂops; however  the Hessian matrix
often has a special structure so that H∆(r) can be computed efﬁciently. For example  in the inverse
covariance estimation problem H = Θ−1 ⊗ Θ−1 where Θ−1 is the current estimate of covariance 
and in the empirical risk minimization problem H = XDX T where X is the data matrix and D is
diagonal.
After solving the subproblem (8)  we have to search for a suitable stepsize. We apply an Armijo
rule for line search [24]  where we test the step size α = 20  2−1  ... until the following sufﬁcient
decrease condition is satisﬁed for a pre-speciﬁed σ ∈ (0  1) (typically σ = 10−4):

k(cid:88)

λr(cid:107)Θr + α∆(r)(cid:107)Ar − k(cid:88)

r=1

r=1

λr(cid:107)θ(r)(cid:107)Ar .

(10)

F (θ + α∆) ≤ F (θ) + ασδ  δ = (cid:104)G  ∆(cid:105) +

3.2 Active Subspace Selection

Since the quadratic subproblem (8) contains a large number of variables  directly applying the above
quadratic approximation framework is not efﬁcient. In this subsection  we provide a general active
subspace selection technique  which dramatically reduces the size of variables by exploiting the
structure of regularizers. A similar method has been discussed in [12] for the (cid:96)1 norm and in [11]
for the nuclear norm  but it has not been generalized to all decomposable norms. Furthermore  a key
point to note is that in this paper our active subspace selection is not only a heuristic  but comes with
strong convergence guarantees that we derive in Section 4.
Given the current θ  our subspace selection approach partitions each θ(r) into S (r)
(S (r)
ﬁxed)⊥ and then restricts the search space of the Newton direction in (8) within S (r)
the following quadratic approximation problem:

ﬁxed and S (r)
free =
free  which yields

[d(1)  . . .   d(k)] =

∆(1)∈S(1)

argmin
free  ... ∆(k)∈S(k)

free

¯g(θ + ∆) +

4

k(cid:88)

r=1

λr(cid:107)θ(r) + ∆(r)(cid:107)Ar .

(11)

Each group of parameter has its own ﬁxed/free subspace  so we now focus on a single parameter
component θ(r). An ideal subspace selection procedure would satisfy:

Property (I). Given the current iterate θ  any updates along directions in the ﬁxed set  for instance

as θ(r) ← θ(r) + a  a ∈ S (r)

Property (II). The subspace Sfree converges to the support of the ﬁnal solution in a ﬁnite number of

ﬁxed  does not improve the objective function value.

iterations.

Suppose given the current iterate  we ﬁrst do updates along directions in the ﬁxed set  and then do
updates along directions in the free set. Property (I) ensures that this is equivalent to ignoring updates
along directions in the ﬁxed set in this current iteration  and focusing on updates along the free set.
As we will show in the next section  this property would sufﬁce to ensure global convergence of our
procedure. Property (II) will be used to derive the asymptotic quadratic convergence rate.
We will now discuss our active subspace selection strategy which will satisfy both properties above.
Consider the parameter component θ(r)  and its corresponding regularizer (cid:107) · (cid:107)Ar. Based on the
deﬁnition of decomposable norm in (2)  there exists a subspace Tr where ΠTr(ρ) is a ﬁxed vector
for any subgradient of (cid:107) · (cid:107)Ar. The following proposition explores some properties of the sub-
differential of the overall objective F (θ) in (1).
Proposition 1. Consider any unit-norm vector a  with (cid:107)a(cid:107)Ar = 1  such that a ∈ T ⊥
r .

(a) The inner-product of the sub-differential ∂θ(r) F (θ) with a satisﬁes:
(cid:104)a  ∂θ(r) F (θ)(cid:105) ∈ [(cid:104)a  G(cid:105) − λr (cid:104)a  G(cid:105) + λr].

(12)

(b) Suppose |(cid:104)a  G(cid:105)| ≤ λr. Then  0 ∈ argminσ F (θ + σa).

See Appendix 7.8 for the proof. Note that G = ∇L(¯θ) denotes the gradient of L. The proposition
thus implies that if |(cid:104)a  G(cid:105)| ≤ λr and S (r)
then Property (I) immediately follows. The
difﬁculty is that the set {a | |(cid:104)a  G(cid:105)| ≤ λr} is possibly hard to characterize  and even if we could
characterize this set  it may not be amenable enough for the optimization solvers to leverage in order
to provide a speedup. Therefore  we propose an alternative characterization of the ﬁxed subspace:
Deﬁnition 1. Let θ(r) be the current iterate  prox(r)

ﬁxed ⊂ T ⊥

r

prox(r)

λ (x) = argmin

λ be the proximal operator deﬁned by
1
(cid:107)y − x(cid:107)2 + λ(cid:107)y(cid:107)Ar  
2

and Tr(x) be the subspace for the decomposable norm (2) (cid:107) · (cid:107)Ar at point x. We can deﬁne the
ﬁxed/free subset at θ(r) as:

y

S (r)
ﬁxed := [T (θ(r))]⊥ ∩ [T (prox(r)

(G))]⊥  S (r)

free = S (r)

ﬁxed

λr

⊥

.

(13)

It can be shown that from the deﬁnition of the proximal operator  and Deﬁnition 1  it holds that
|(cid:104)a  G(cid:105)| < λr  so that we would have local optimality in the direction a as before. We have the
following proposition:
Proposition 2. Let S (r)

ﬁxed be the ﬁxed subspace deﬁned in Deﬁnition 1. We then have:

0 = argmin
∆(r)∈S(r)

ﬁxed

QH([0  . . .   0  ∆(r)  0  . . .   0]; θ).

We will prove that Sfree as deﬁned above converges to the ﬁnal support in Section 4  as required in
Property (II) above. We will now detail some examples of the ﬁxed/free subsets deﬁned above.
• For (cid:96)1 regularization: Sﬁxed = span{ei | θi = 0 and |∇iL(¯θ)| ≤ λ} where ei is the ith canonical
vector.
• For nuclear norm regularization: the selection scheme can be written as

Sfree = {UAM V T

A | M ∈ Rk×k} 

(14)
where UA = span(U  Ug)  VA = span(V  Vg)  with Θ = UΣV T is the thin SVD of Θ and Ug  Vg
are the left and right singular vectors of proxλ(Θ−∇L(Θ)). The proximal operator proxλ(·) in this
case corresponds to singular-value soft-thresholding  and can be computed by randomized SVD or
the Lanczos algorithm.

5

• For group sparse regularization: in the (1  2)-group norm case  let SG be the nonzero groups 
then the ﬁxed groups FG can be deﬁned by FG := {i | i /∈ SG and (cid:107)∇LGi(¯θ)(cid:107) ≤ λ}  and the free
subspace will be
(15)
In Figure 3 (in the appendix) that the active subspace selection can signiﬁcantly improve the speed
for the block coordinate descent algorithm [20].
Algorithm 1: QUIC & DIRTY: Quadratic Approximation Framework for Dirty Statistical Mod-
els

Sfree = {θ | θi = 0 ∀i ∈ FG}.

: Loss function L(·)  regularizers λr(cid:107) · (cid:107)Ar for r = 1  . . .   k  and initial iterate θ0.

Input
Output: Sequence {θt} such that {¯θt} converges to ¯θ(cid:63).

r=1 θ(r)

t

.

Compute ¯θt ←(cid:80)k

1 for t = 0  1  . . . do
2
3
4
5
6
7
8

Compute ∇L(¯θt).
Compute Sfree by (13).
for sweep = 1  . . .   Touter do
for r = 1  . . .   k do

Update(cid:80)k

Solve the subproblem (9) within S (r)
free.

r=1 ∇2L(¯θt)∆(r).

9
10

Find the step size α by (10).
θ(r) ← θ(r) + α∆(r) for all r = 1  . . .   k.

4 Convergence
The recently developed theoretical analysis of proximal Newton methods [16  21] cannot be directly
applied because (1) we have the active subspace selection step  and (2) the Hessian matrix for each
quadratic subproblem is not positive deﬁnite. We ﬁrst prove the global convergence of our algorithm
when the quadratic approximation subproblem (11) is solved exactly. Interestingly  in our proof
we show that the active subspace selection can be modeled within the framework of the Block
Coordinate Gradient Descent algorithm [24] with a carefully designed Hessian approximation  and
by making this connection we are able to prove global convergence.
Theorem 1. Suppose L(·) is convex (may not be strongly convex)  and the quadratic subproblem
(8) at each iteration is solved exactly  Algorithm 1 converges to the optimal solution.
The proof is in Appendix 7.1. Next we consider the case that L(¯θ) is strongly convex. Note that
r=1 θ(r)) will not be strongly convex in θ
(if k > 1) and there may exist more than one optimal solution. However  we show that all solutions

even when L(¯θ) is strongly convex with respect to ¯θ  L((cid:80)k
give the same ¯θ :=(cid:80)k
r=1 x(r) =(cid:80)k
(1)  then(cid:80)k

Lemma 2. Assume L(·) is strongly convex  and {x(r)}k

r=1 are two optimal solutions of

r=1 {y(r)}k

r=1 y(r).

r=1 θ(r).

The proof is in Appendix 7.2. Next  we show that S (r)
free (from Deﬁnition 1) will converge to the ﬁnal
support ¯T (r) for each parameter set r = 1  . . .   k. Let ¯θ(cid:63) be the global minimizer (which is unique
as shown in Lemma 2)  and assume that we have

(16)
This is the generalization of the assumption used in earlier literature [12] where only (cid:96)1 regulariza-
tion was considered. The condition is similar to strict complementary in linear programming.
Theorem 2. If L(·) is strongly convex and assumption (16) holds  then there exists a ﬁnite T > 0
such that S (r)

f ree = ¯T (r) ∀r = 1  . . .   k after t > T iterations.

Ar

< λr ∀r = 1  . . .   k.

The proof is in Appendix 7.3. Next we show that our algorithm has an asymptotic quadratic conver-
gence rate (the proof is in Appendix 7.4).
Theorem 3. Assume that ∇2L(·) is Lipschitz continuous  and assumption (16) holds. If at each iter-
ation the quadratic subproblem (8) is solved exactly  and L(·) is strongly convex  then our algorithm
converges with asymptotic quadratic convergence rate.

6

(cid:107)Π( ¯T (r))⊥(cid:0)∇L(¯θ(cid:63))(cid:1)(cid:107)∗

5 Applications
We demonstrate that our algorithm is extremely efﬁcient for two applications: Gaussian Markov
Random Fields (GMRF) with latent variables (with sparse + low rank structure) and multi-task
learning problems (with sparse + group sparse structure).

5.1 GMRF with Latent Variables

We ﬁrst apply our algorithm to solve the latent feature GMRF structure learning problem in eq (4) 
where S ∈ Rp×p is the sparse part  L ∈ Rp×p is the low-rank part  and we require L = LT (cid:23)
0  S = ST and Y = S − L (cid:31) 0 (i.e. θ(2) = −L). In this case  L(Y ) = − log det(Y ) + (cid:104)Σ  Y (cid:105) 
hence

∇2L(Y ) = Y −1 ⊗ Y −1  and ∇L(Y ) = Σ − Y −1.

(17)
For the sparse part  the free subspace is a subset of indices {(i  j) | Sij (cid:54)=
Active Subspace.
0 or |∇ijL(Y )| ≥ λ}. For the low-rank part  the free subspace can be presented as {UAM V T
A |
M ∈ Rk×k} where UA and VA are deﬁned in (14).
Updating ∆L. To solve the quadratic subproblem (11)  ﬁrst we discuss how to update ∆L using
subspace selection. The subproblem is

min

trace(∆LY −1∆LY −1)+trace((Y −1−Σ−Y −1∆SY −1)∆L)+λL(cid:107)L+∆L(cid:107)∗ 

1
2

1
2

A so that we can write ∆L =

∆L=U ∆DU T :L+∆L(cid:23)0
and since ∆L is constrained to be a perturbation of L = UAM U T
UA∆M U T
A   and the subproblem becomes
min

1
2

A Y −1UA and ¯Σ := U T

trace( ¯Y ∆M ¯Y ∆M ) + trace(¯Σ∆M ) + λL trace(M + ∆M ) := q(∆M ) 

(18)
∆M :M +∆M(cid:23)0
A (Y −1 − Σ − Y −1∆SY −1)UA. Therefore the subproblem
where ¯Y := U T
(18) becomes a k × k dimensional problem where k (cid:28) p.
To solve (18)  we ﬁrst check if the closed form solution exists. Note that ∇q(∆M ) = ¯Y ∆M ¯Y +
¯Σ + λLI  thus the minimizer is ∆M = − ¯Y −1(¯Σ + λLI) ¯Y −1 if M + ∆M (cid:23) 0. If not  we solve the
subproblem by the projected gradient descent method  where each step only requires O(k2) time.
Updating ∆S. The subproblem with respect to ∆S can be written as
vec(∆S)T (Y −1⊗Y −1) vec(∆S)+trace((Σ−Y −1−Y −1(∆L)Y −1)∆S)+λS(cid:107)S +∆S(cid:107)1 
min
∆S
In our implementation we apply the same coordinate descent procedure proposed in QUIC [12] to
solve this subproblem.
Results. We compare our algorithm with two state-of-the-art software packages. The LogdetPPA
algorithm was proposed in [26] and used in [5] to solve (4). The PGALM algorithm was proposed
in [17]. We run our algorithm on three gene expression datasets: the ER dataset (p = 692)  the
Leukemia dataset (p = 1255)  and a subset of the Rosetta dataset (p = 2000)1 For the parameters  we
use λS = 0.5  λL = 50 for the ER and Leukemia datasets  which give us low-rank and sparse results.
For the Rosetta dataset  we use the parameters suggested in LogdetPPA  with λS = 0.0313  λL =
0.1565. The results in Figure 1 shows that our algorithm is more than 10 times faster than other
algorithms. Note that in the beginning PGALM tends to produce infeasible solutions (L or S − L is
not positive deﬁnite)  which is not plotted in the ﬁgures.
Our proximal Newton framework has two algorithmic components: the quadratic approximation 
and our active subspace selection. From Figure 1 we can observe that although our algorithm is
a Newton-like method  the time cost for each iteration is similar or even cheaper than other ﬁrst
order methods. The reason is (1) we take advantage from active selection  and (2) the problem has
a special structure of the Hessian (17)  where computing it is no more expensive than the gradient.
To delineate the contribution of the quadratic approximation to the gain in speed of convergence  we
further compare our algorithm to an alternating minimization approach for solving (4)  together with
our active subspace selection. Such an alternating minimization approach would iteratively ﬁx one
of S  L  and update the other; we defer detailed algorithmic and implementation details to Appendix
7.6 for reasons of space. The results show that by using the quadratic approximation  we get a much
faster convergence rate (see Figure 2 in Appendix 7.6).

1The full dataset has p = 6316 but the other methods cannot solve this size problem.

7

(a) ER dataset

(b) Leukemia dataset

(c) Rosetta dataset

Figure 1: Comparison of algorithms on the latent feature GMRF problem using gene expression
datasets. Our algorithm is much faster than PGALM and LogdetPPA.

Table 1: The comparisons on multi-task problems.

Dirty Models (sparse + group sparse)

dataset

USPS

RCV1

number of
training data

100
100
400
400
1000
1000
5000
5000

relative
error
10−1
10−4
10−1
10−4
10−1
10−4
10−1
10−4

QUIC & DIRTY
8.3% / 0.42s
7.47% / 0.75s
2.92% / 1.01s
2.5% / 1.55s
18.91% / 10.5s
18.45% / 23.1s
10.54% / 42s
10.27% / 87s

proximal gradient
8.5% / 1.8s
7.49% / 10.8s
2.9% / 9.4s
2.5% / 35.8
18.5%/47s
18.49% / 430.8s
10.8% / 541s
10.27% / 2254s

ADMM
8.3% / 1.3
7.47% / 4.5s
3.0% / 3.6s
2.5% / 11.0s
18.9% / 23.8s
18.5% / 259s
10.6% / 281s
10.27% / 1191s

Other Models

Lasso Group Lasso
8.36%

10.27%

4.87%

22.67%

13.67%

2.93%

20.8%

12.25%

5.2 Multiple-task learning with superposition-structured regularizers
Next we solve the multi-task learning problem (5) where the parameter is a sparse matrix S ∈ Rd×k
and a group sparse matrix B ∈ Rd×k. Instead of using the square loss (as in [15])  we consider the
logistic loss (cid:96)logistic(y  a) = log(1 + e−ya)  which gives better performance as seen by comparing
Table 1 to results in [15]. Here the Hessian matrix has a special structure again: H = XDX T where
X is the data matrix and D is the diagonal matrix  and in Appendix 7.7 we have a detail description
of how to applying our algorithm to solve this problem.
Results. We follow [15] and transform multi-class problems into multi-task problems. For a
multiclass dataset with k classes and n samples  for each r = 1  . . .   k  we generate yr ∈ {0  1}n
to be the vector such that y(k)
i = 1 if and only if the i-th sample is in class r. Our ﬁrst dataset is the
USPS dataset which was ﬁrst collected in [25] and subsequently widely used in multi-task papers.
On this dataset  the use of several regularizers is crucial for good performance. For example  [15]
demonstrates that on USPS  using lasso and group lasso regularizations together outperforms models
with a single regularizer. However  they only consider the squared loss in their paper  whereas we
consider a logistic loss which leads to better performance. For example  we get 7.47% error rate
using 100 samples in USPS dataset  while using the squared loss the error rate is 10.8% [15]. Our
second dataset is a larger document dataset RCV1 downloaded from LIBSVM Data  which has 53
classes and 47 236 features. We show that our algorithm is much faster than other algorithms on both
datasets  especially on RCV1 where we are more than 20 times faster than proximal gradient descent.
Here our subspace selection techniques works well because we expect that the active subspace at the
true solution is small.

6 Acknowledgements

This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H also acknowl-
edges support from an IBM PhD fellowship. P.R. acknowledges the support of ARO via W911NF-
12-1-0390 and NSF via IIS-1149803  IIS-1447574  and DMS-1264033. S.R.B. was supported by
an IBM Research Goldstine Postdoctoral Fellowship while the work was performed.

8

0501001509001000110012001300time (sec)Objective value Quic & DirtyPGALMLogdetPPM .01002003004005001500200025003000time (sec)Objective value Quic & DirtyPGALMLogdetPPM .0200400600−2000−1500−1000−500time (sec)Objective value Quic & DirtyPGALMLogdetPPM .References
[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Noisy matrix decomposition via convex relaxation:

Optimal rates in high dimensions. Annals if Statistics  40(2):1171–1197  2012.

[2] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning  3(1):1–
122  2011.

[3] E. Candes and B. Recht. Simple bounds for recovering low-complexity models. Mathemetical Program-

ming  2012.

[4] E. J. Candes  X. Li  Y. Ma  and J. Wright. Robust principal component analysis?

Mach.  58(3):1–37  2011.

J. Assoc. Comput.

[5] V. Chandrasekaran  P. A. Parrilo  and A. S. Willsky. Latent variable graphical model selection via convex

optimization. The Annals of Statistics  2012.

[6] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Rank-sparsity incoherence for matrix

decomposition. Siam J. Optim  21(2):572–596  2011.

[7] Y. Chen  A. Jalali  S. Sanghavi  and C. Caramanis. Low-rank matrix recovery from errors and erasures.

IEEE Transactions on Information Theory  59(7):4324–4337  2013.

[8] M. Collins  S. Dasgupta  and R. E. Schapire. A generalization of principal component analysis to the

exponential family. In NIPS  2012.

[9] Q. T. Dinh  A. Kyrillidis  and V. Cevher. An inexact proximal path-following algorithm for constrained

convex minimization. arxiv:1311.1756  2013.

[10] C.-J. Hsieh  I. S. Dhillon  P. Ravikumar  and A. Banerjee. A divide-and-conquer method for sparse inverse

covariance estimation. In NIPS  2012.

[11] C.-J. Hsieh and P. A. Olsen. Nuclear norm minimization via active subspace selection. In ICML  2014.
[12] C.-J. Hsieh  M. A. Sustik  I. S. Dhillon  and P. Ravikumar. Sparse inverse covariance matrix estimation

using quadratic approximation. In NIPS  2011.

[13] C.-J. Hsieh  M. A. Sustik  I. S. Dhillon  P. Ravikumar  and R. A. Poldrack. BIG & QUIC: Sparse inverse

covariance estimation for a million variables. In NIPS  2013.

[14] D. Hsu  S. M. Kakade  and T. Zhang. Robust matrix decomposition with sparse corruptions. IEEE Trans.

Inform. Theory  57:7221–7234  2011.

[15] A. Jalali  P. Ravikumar  S. Sanghavi  and C. Ruan. A dirty model for multi-task learning. In NIPS  2010.
[16] J. D. Lee  Y. Sun  and M. A. Saunders. Proximal Newton-type methods for convex optimization. In NIPS 

2012.

[17] S. Ma  L. Xue  and H. Zou. Alternating direction methods for latent variable Gaussian graphical model

selection. Neural Computation  25(8):2172–2198  2013.

[18] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Statistical Science  27(4):538–557  2012.

[19] P. Olsen  F. Oztoprak  J. Nocedal  and S. Rennie. Newton-like methods for sparse inverse covariance

estimation. In NIPS  2012.

[20] Z. Qin  K. Scheinberg  and D. Goldfarb. Efﬁcient block-coordinate descent algorithm for the group lasso.

Mathematical Programming Computation  2013.

[21] K. Scheinberg and X. Tang. Practical inexact proximal quasi-newton method with global complexity

analysis. arxiv:1311.6547  2014.

[22] A. Tewari  P. Ravikumar  and I. Dhillon. Greedy algorithms for structurally constrained high dimensional

problems. In NIPS  2011.

[23] K.-C. Toh  P. Tseng  and S. Yun. A block coordinate gradient descent method for regularized convex

separable optimization and covariance selection. Mathemetical Programming  129:331–355  2011.

[24] P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable minimization.

Mathematical Programming  117:387–423  2007.

[25] M. van Breukelen  R. P. W. Duin  D. M. J. Tax  and J. E. den Hartog. Handwritten digit recognition by

combined classiﬁers. Kybernetika  34(4):381–386  1998.

[26] C. Wang  D. Sun  and K.-C. Toh. Solving log-determinant optimization problems by a Newton-CG primal

proximal point algorithm. SIAM J. Optimization  20:2994–3013  2010.

[27] E. Yang and P. Ravikumar. Dirty statistical models. In NIPS  2013.
[28] E.-H. Yen  C.-J. Hsieh  P. Ravikumar  and I. S. Dhillon. Constant nullspace strong convexity and fast

convergence of proximal methods under high-dimensional settings. In NIPS  2014.

[29] G.-X. Yuan  C.-H. Ho  and C.-J. Lin. An improved GLMNET for L1-regularized logistic regression.

JMLR  13:1999–2030  2012.

9

,Cho-Jui Hsieh
Inderjit Dhillon
Pradeep Ravikumar
Stephen Becker
Peder Olsen
Alain Durmus
Umut Simsekli
Eric Moulines
Roland Badeau
Gaël RICHARD