2019,Momentum-Based Variance Reduction in Non-Convex SGD,Variance reduction has emerged in recent years as a strong competitor to stochastic gradient descent in non-convex problems  providing the first algorithms to improve upon the converge rate of stochastic gradient descent for finding first-order critical points. However  variance reduction techniques typically require carefully tuned learning rates and willingness to use excessively large "mega-batches" in order to achieve their improved results. We present a new algorithm  STORM  that does not require any batches and makes use of adaptive learning rates  enabling simpler implementation and less hyperparameter tuning. Our technique for removing the batches uses a variant of momentum to achieve variance reduction in non-convex optimization. On smooth losses $F$  STORM finds a point $x$ with $\mathbb{E}[\|\nabla F(x)\|]\le O(1/\sqrt{T}+\sigma^{1/3}/T^{1/3})$ in $T$ iterations with $\sigma^2$ variance in the gradients  matching the best-known rate but without requiring knowledge of $\sigma$.,Momentum-BasedVarianceReductioninNon-ConvexSGDAshokCutkoskyGoogleResearchMountainView CA USAashok@cutkosky.comFrancescoOrabonaBostonUniversityBoston MA USAfrancesco@orabona.comAbstractVariancereductionhasemergedinrecentyearsasastrongcompetitortostochasticgradientdescentinnon-convexproblems providingtheﬁrstalgorithmstoimproveupontheconvergerateofstochasticgradientdescentforﬁndingﬁrst-ordercriticalpoints.However variancereductiontechniquestypicallyrequirecarefullytunedlearningratesandwillingnesstouseexcessivelylarge“mega-batches”inordertoachievetheirimprovedresults.Wepresentanewalgorithm STORM thatdoesnotrequireanybatchesandmakesuseofadaptivelearningrates enablingsimplerimplementationandlesshyperparametertuning.Ourtechniqueforremovingthebatchesusesavariantofmomentumtoachievevariancereductioninnon-convexoptimization.OnsmoothlossesF STORMﬁndsapointxwithE[k∇F(x)k]≤O(1/√T+σ1/3/T1/3)inTiterationswithσ2varianceinthegradients matchingtheoptimalrateandwithoutrequiringknowledgeofσ.1IntroductionThispaperaddressestheclassicstochasticoptimizationproblem inwhichwearegivenafunctionF:Rd→R andwishtoﬁndx∈RdsuchthatF(x)isassmallaspossible.Unfortunately ouraccesstoFislimitedtoastochasticfunctionoracle:wecanobtainsamplefunctionsf(· ξ)whereξrepresentssomesamplevariable(e.g.aminibatchindex)suchthatE[f(· ξ)]=F(·).Stochasticoptimizationproblemsarefoundthroughoutmachinelearning.Forexample insupervisedlearning xrepresentstheparametersofamodel(saytheweightsofaneuralnetwork) ξrepresentsanexample f(x ξ)representsthelossonanexample andFrepresentsthetraininglossofthemodel.Wedonotassumeconvexity soingeneraltheproblemofﬁndingatrueminimumofFmaybeNP-hard.Hence werelaxtheproblemtoﬁndingacriticalpointofF–thatisapointsuchthat∇F(x)=0.Also weassumeaccessonlytostochasticgradientsevaluatedonarbitrarypoints ratherthanHessiansorotherinformation.Inthissetting thestandardalgorithmisstochasticgradientdescent(SGD).SGDproducesasequenceofiteratesx1 ... xTusingtherecursionxt+1=xt−ηtgt (1)wheregt=∇f(xt ξt) f(· ξ1) ... f(· ξT)arei.i.d.samplesfromadistributionD andη1 ...ηT∈Rareasequenceoflearningratesthatmustbecarefullytunedtoensuregoodperfor-mance.Assumingtheηtareselectedproperly SGDguaranteesthatarandomlyselectediteratextsatisﬁesE[k∇F(xt)k]≤O(1/T1/4)[9].Recently variancereductionhasemergedasanimprovedtechniqueforﬁndingcriticalpointsinnon-convexoptimizationproblems.Stochasticvariance-reducedgradient(SVRG)algorithmsalsoproduceiteratesx1 ... xTaccordingtotheupdateformula(1) butnowgtisavariancereducedestimateof∇F(xt).Overthelastfewyears SVRGalgorithmshaveimprovedtheconvergenceratetocriticalpointsofnon-convexSGDfromO(1/T1/4)toO(1/T3/10)[2 21]toO(1/T1/3)[8 31].Despite33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.thisimprovement SVRGhasnotseenasmuchsuccessinpracticeinnon-convexmachinelearningproblems[5].Manyreasonsmaycontributetothisphenomenon buttwopotentialissuesweaddresshereareSVRG’suseofnon-adaptivelearningratesandrelianceongiantbatchsizestoconstructvariancereducedgradientsthroughtheuseoflow-noisegradientscalculatedata“checkpoint”.Inparticular fornon-convexlossesSVRGanalysestypicallyinvolvecarefullyselectinglearningrates thenumberofsamplestoconstructthegradientonthecheckpointpoints andthefrequencyofupdateofthecheckpointpoints.Theoptimalsettingsbalancevariousunknownproblemparametersexactlyinordertoobtainimprovedperformance makingitespeciallyimportant andespeciallydifﬁcult totunethem.Inthispaper weaddressbothoftheseissues.WepresentanewalgorithmcalledSTOchasticRecursiveMomentum(STORM)thatachievesvariancereductionthroughtheuseofavariantofthemomentumterm similartothepopularRMSProporAdammomentumheuristics[24 13].Hence ouralgorithmdoesnotrequireagiganticbatchtocomputecheckpointgradients–infact ouralgorithmdoesnotrequireanybatchesatallbecauseitneverneedstocomputeacheckpointgradient.STORMachievestheoptimalconvergencerateofO(1/T1/3)[3] anditusesanadaptivelearningrateschedulethatwillautomaticallyadjusttothevariancevaluesof∇f(xt ξt).Overall weconsiderouralgorithmasigniﬁcantqualitativedeparturefromtheusualparadigmforvariancereduction andwehopeouranalysismayprovideinsightintothevalueofmomentuminnon-convexoptimization.Therestofthepaperisorganizedasfollows.Thenextsectiondiscussestherelatedworkonvariancereductionandadaptivelearningratesinnon-convexSGD.Section3formallyintroducesournotationandassumptions.WepresentourbasicupdateruleanditsconnectiontoSGDwithmomentuminSection4 andouralgorithminSection5.Finally wepresentsomeempiricalresultsinSection6andconcludeswithadiscussioninSection7.2RelatedWorkVariance-reductionmethodswereproposedindependentlybythreegroupsatthesameconference:JohnsonandZhang[12] Zhangetal.[30] Mahdavietal.[17] andWangetal.[27].Theﬁrstapplicationofvariance-reductionmethodtonon-convexSGDisduetoAllen-ZhuandHazan[2].Usingvariancereductionmethods Fangetal.[8] Zhouetal.[31]haveobtainedmuchbetterconvergenceratesforcriticalpointsinnon-convexSGD.Thesemethodsareverydifferentfromourapproachbecausetheyrequirethecalculationofgradientsatcheckpoints.Infact inordertocomputethevariancereducedgradientestimatesgt thealgorithmmustperiodicallystopproducingiteratesxtandinsteadgenerateaverylarge“mega-batch”ofsamplesξ1 ... ξNwhichisusedtocomputeacheckpointgradient1NPNi=1∇f(v ξi)foranappropriatecheckpointpointv.Dependingonthealgorithm NmaybeaslargeasO(T) andtypicallynosmallerthanO(T2/3).TheonlyexceptionsweareawareofareSARAH[18 19]andiSARAH[20].However theirguaranteesdonotimproveovertheonesofplainSGD andtheystillrequireatleastonecheckpointgradient.Independentlyandsimultaneouslywiththiswork [25]haveproposedanewalgorithmthatdoesimproveoverSGDtomatchoursameconvergencerate althoughitdoesstillrequireonecheckpointgradient.Interestingly theirupdateformulaisverysimilartoours althoughtheanalysisisratherdifferent.Wearenotawareofpriorworksfornon-convexoptimizationwithreducedvariancemethodsthatcompletelyavoidusinggiantbatches.Ontheotherhand adaptivelearning-rateschemes thatchoosethevaluesηtinsomedata-dependentwaysoastoreducetheneedfortuningthevaluesofηtmanually havebeenintroducedbyDuchietal.[7]andpopularizedbytheheuristicmethodslikeRMSPropandAdam[24 13].Inthenon-convexsetting adaptivelearningratescanbeshowntoimprovetheconvergencerateofSGDtoO(1/√T+(σ2/T)1/4) whereσ2isaboundonthevarianceof∇f(xt)[16 28 22].Hence theseadaptivealgorithmsobtainmuchbetterconvergenceguaranteeswhentheproblemis“easy” andhavebecomeextremelypopularinpractice.Incontrast theonlyvariance-reducedalgorithmweareawareofthatusesadaptivelearningratesis[4] buttheirtechniquesapplyonlytoconvexlosses.3NotationandAssumptionsInthefollowing wewillwritevectorswithboldlettersandwewilldenotetheinnerproductbetweenvectorsaandbbya·b.2Throughoutthepaperwewillmakethefollowingassumptions.Weassumeaccesstoastreamofindependentrandomvariablesξ1 ... ξT∈Ξandafunctionfsuchthatforalltandforallx E[f(x ξt)|x]=F(x).Notethatweaccesstwogradientsonthesameξtontwodifferentpointsineachupdate likeinstandardvariance-reducedmethods.Inpractice ξtmaydenoteani.i.d.trainingexample oranindexintoatrainingsetwhilef(x ξt)indicatesthelossonthetrainingexampleusingthemodelparameterx.Weassumethereissomeσ2thatupperboundsthenoiseongradients:E[k∇f(x ξt)−∇F(x)k2]≤σ2.WedeﬁneF?=infxF(x)andwewillassumethatF?>−∞.Wewillalsoneedsomeassumptionsonthefunctionsf(x ξt).Deﬁneadifferentiablefunctionf:Rd→RtobeG-Lipschitziffk∇f(x)k≤Gforallx andftobeL-smoothiffk∇f(x)−∇f(y)k≤Lkx−ykforallxandy.Weassumethatf(x ξt)isdifferentiable andL-smoothasafunctionofxwithprobability1.Wewillalsoassumethatf(x ξt)isG-Lipschitzforouradaptiveanalysis.WeshowinappendixBthatthisassumptioncanbeliftedattheexpenseofadaptivitytoσ.4MomentumandVarianceReductionBeforedescribingouralgorithmindetails webrieﬂyexploretheconnectionbetweenSGDwithmomentumandvariancereduction.Thestochasticgradientdescentwithmomentumalgorithmistypicallyimplementedasdt=(1−a)dt−1+a∇f(xt ξt)xt+1=xt−ηdt whereaissmall i.e.a=0.1.Inwords insteadofusingthecurrentgradient∇F(xt)intheupdateofxt weuseanexponentialaverageofthepastobservedgradients.WhileSGDwithmomentumanditsvariantshavebeensuccessfullyusedinmanymachinelearningapplications[13] itiswellknownthatthepresenceofnoiseinthestochasticgradientscannullifythetheoreticalgainofthemomentumterm[e.g.29].Asaresult itisunclearhowandwhyusingmomentumcanbebetterthanplainSGD.AlthoughrecentworkshaveprovedthatavariantofSGDwithmomentumimprovesthenon-dominanttermsintheconvergencerateonconvexstochasticleastsquareproblems[6 11] itisstilluncleariftheactualconvergenceratecanbeimproved.Here wetakeadifferentroute.InsteadofshowingthatmomentuminSGDworksinthesamewayasinthenoiselesscase i.e.givingacceleratedrates weshowthatavariantofmomentumcanprovablyreducethevarianceofthegradients.Initssimplestform thevariantweproposeis:dt=(1−a)dt−1+a∇f(xt ξt)+(1−a)(∇f(xt ξt)−∇f(xt−1 ξt))(2)xt+1=xt−ηdt.(3)Theonlydifferenceisthethatweaddtheterm(1−a)(∇f(xt ξt)−∇f(xt−1 ξt))totheupdate.Asinstandardvariance-reducedmethods weusetwogradientsineachstep.However wedonotneedtousethegradientcalculatedatanycheckpointpoints.Notethatifxt≈xt−1 thenourupdatebecomesapproximatelythemomentumone.Thesetwotermswillbesimilaraslongasthealgorithmisactuallyconvergingtosomepoint andsowecanexpectthealgorithmtobehaveexactlyliketheclassicmomentumSGDtowardstheendoftheoptimizationprocess.Tounderstandwhytheaboveupdatesdeliversavariancereduction considerthe“errorindt”whichwedenoteast:t:=dt−∇F(xt).Thistermmeasurestheerrorweincurbyusingdtasupdatedirectioninsteadofthecorrectbutunknowndirection ∇F(xt).TheequivalentterminSGDwouldbeE[k∇f(xt ξt)−∇F(xt)k2]≤σ2.So ifE[ktk2]decreasesovertime wehaverealizedavariancereductioneffect.OurtechnicalresultthatweusetoshowthisdecreaseisprovidedinLemma2 butletustakeamomentheretoappreciatewhythisshouldbeexpectedintuitively.Consideringtheupdatewrittenin(2) wecanobtainarecursiveexpressionfortbysubtracting∇F(xt)frombothsides:t=(1−a)t−1+a(∇f(xt ξt)−∇F(xt))+(1−a)(∇f(xt ξt)−∇f(xt−1 ξt)−(∇F(xt)−∇F(xt−1))).3Algorithm1STORM:STOchasticRecursiveMomentum1:Input:Parametersk w c initialpointx12:Sampleξ13:G1←k∇f(x1 ξ1)k4:d1←∇f(x1 ξ1)5:η0←kw1/36:fort=1toTdo7:ηt←k(w+Pti=1G2t)1/38:xt+1←xt−ηtdt9:at+1←cη2t10:Sampleξt+111:Gt+1←k∇f(xt+1 ξt+1)k12:dt+1←∇f(xt+1 ξt+1)+(1−at+1)(dt−∇f(xt ξt+1))13:endfor14:Chooseˆxuniformlyatrandomfromx1 ... xT.(Inpractice setˆx=xT).15:returnˆxNow noticethatthereisgoodreasontoexpectthesecondandthirdtermsoftheRHSabovetobesmall:wecancontrola(∇f(xt ξt)−∇F(xt))simplybychoosingsmallenoughvaluesa andfromsmoothnessweexpect(∇f(xt ξt)−∇f(xt−1 ξt)−(∇F(xt)−∇F(xt−1))tobeoftheorderofO(kxt−xt−1k)=O(ηdt−1).Therefore bychoosingsmallenoughηanda weobtainktk=(1−a)kt−1k+ZwhereZissomesmallvalue.Thus intuitivelyktkwilldecreaseuntilitreachesZ/a.Thishighlightsatrade-offinsettingηandainordertodecreasethenumeratorofZ/awhilekeepingthedenominatorsufﬁcientlylarge.Ourcentralchallengeisshowingthatitispossibletoachieveafavorabletrade-offinwhichZ/aisverysmall resultinginsmallerrort.5STORM:STOchasticRecursiveMomentumWenowdescribeourstochasticoptimizationalgorithm whichwecallSTOchasticRecursiveMo-mentum(STORM).ThepseudocodeisinAlgorithm1.Asdescribedintheprevioussection itsbasicupdateisoftheformof(2)and(3).However inordertoachieveadaptivitytothenoiseinthegradients boththestepsizeandthemomentumtermwilldependonthepastgradients àlaAdaGrad[7].TheconvergenceguaranteeofSTORMispresentedinTheorem1below.Theorem1.UndertheassumptionsinSection3 foranyb>0 wewritek=bG23L.Setc=28L2+G2/(7Lk3)=L2(28+1/(7b3))andw=max(cid:16)(4Lk)3 2G2 (cid:0)ck4L(cid:1)3(cid:17)=G2max(cid:0)(4b)3 2 (28b+17b2)3/64(cid:1).Then STORMsatisﬁesE[k∇F(ˆx)k]=E"1TTXt=1k∇F(xt)k#≤w1/6√2M+2M3/4√T+2σ1/3T1/3 whereM=8k(F(x1)−F?)+w1/3σ24L2k2+k2c22L2ln(T+2).Inwords Theorem1guaranteesthatSTORMwillmakethenormofthegradientsconvergeto0atarateofO(lnT√T)ifthereisnonoise andinexpectationatarateof2σ1/3T1/3inthestochasticcase.Weremarkthatweachievebothratesautomatically withouttheneedtoknowthenoiselevelnortheneedtotunestepsizes.Notethattheratewhenσ6=0matchestheoptimalrate[3] whichwaspreviouslyonlyobtainedbySVRG-basedalgorithmsthatrequirea“mega-batch”[8 31].ThedependenceonGinthisbounddeservessomediscussion-atﬁrstblushitappearsthatifG→0 theboundwillgotoinﬁnitybecausethedenominatorinMgoestozero.Fortunately thisisnotso:theresolutionistoobservethatF(x1)−F?=O(G)andσ=O(G) sothatthenumeratorsofMactuallygotozeroatleastasfastasthedenominator.ThedependenceonLmaybesimilarlynon-intuitive:asL→0 M→∞.Inthiscasethisisactuallytobeexpected:ifL=0 thenthere4arenocriticalpoints(becausethegradientsareallthesame!)andsowecannotactuallyﬁndone.Ingeneral MshouldberegardedasanO(log(T))termwheretheconstantindicatessomeinherenthardnesslevelintheproblem.Finally notethathereweassumedthateachf(x ξ)isG-Lipschitzinx.Priorvariancereductionresults(e.g.[18 8 25])donotmakeuseofthisassumption.However weweshowinAppendixBthatsimplyreplacingallinstancesofGorGtintheparametersofSTORMwithanoracle-tunedvalueofσallowsustodispensewiththisassumptionwhilestillavoidingallcheckpointgradients.Alsonotethat asinsimilarworkonstochasticminimizationofnon-convexfunctions Theorem1onlyboundsthegradientofarandomlyselectediterate[9].However inpracticalimplementationsweexpectthelastiteratetoperformequallywell.OuranalysisformalizestheintuitiondevelopedintheprevioussectionthroughaLyapunovpotentialfunction.OurLyapunovfunctionissomewhatnon-standard:forsmoothnon-convexfunctions theLyapunovfunctionistypicallyoftheformΦt=F(xt) butweproposetousethefunctionΦt=F(xt)+ztktk2foratime-varyingzt∝η−1t−1 wheretistheerrorintheupdateintroducedintheprevioussection.Theuseoftime-varyingztappearstobecriticalforustoavoidusinganycheckpoints:withconstantztitseemsthatonealwaysneedsatleastonecheckpointgradient.Potentialfunctionsofthisformhavebeenusedtoanalyzemomentumalgorithmsinordertoproveasymptoticguarantees see e.g. RuszczynskiandSyski[23].However asfarasweknow thisuseofapotentialissomewhatdifferentthanmostvariancereductionanalyses andsomayprovideavenuesforfurtherdevelopment.WenowproceedtotheproofofTheorem1.5.1ProofofTheorem1First weconsideragenericSGD-styleanalysis.MostSGDanalysesassumethatthegradientestimatesusedbythealgorithmareunbiasedof∇F(xt) butunfortunatelydtbiased.Asaresult weneedthefollowingslightlydifferentanalysis.Forlackofspace theproofofthisLemmaandthenextoneareintheAppendix.Lemma1.Supposeηt≤14Lforallt.ThenE[F(xt+1)−F(xt)]≤E(cid:2)−ηt/4k∇F(xt)k2+3ηt/4ktk2(cid:3).ThefollowingtechnicalobservationiskeytoouranalysisofSTORM:itprovidesarecurrencethatenablesustoboundthevarianceoftheestimatesdt.Lemma2.WiththenotationinAlgorithm1 wehaveE(cid:2)ktk2/ηt−1(cid:3)≤E(cid:2)2c2η3t−1G2t+(1−at)2(1+4L2η2t−1)kt−1k2/ηt−1+4(1−at)2L2ηt−1k∇F(xt−1)k2(cid:3).Lemma2exhibitsasomewhatinvolvedalgebraicidentity soletustrytobuildsomeintuitionforwhatitmeansandhowitcanhelpus.First multiplybothsidesbyηt−1.Technicallytheexpectationsmakethisaforbiddenoperation butweignorethisdetailfornow.Next observethatPTt=1G2tisroughlyΘ(T)(sincethethevariancepreventskgtk2fromgoingtozeroevenwhenk∇F(xt)kdoes).ThereforeηtisroughlyO(1/t1/3) andatisroughlyO(1/t2/3).Discardingallconstants andobservingthat(1−at)2≤(1−at) theaboveLemmaisthensayingthatE[ktk2]≤E(cid:2)η4t−1+(1−at)kt−1k2+η2t−1k∇F(xt−1)k2(cid:3)=Eht−4/3+(cid:16)1−t−2/3(cid:17)kt−1k2+t−1/3k∇F(xt−1)k2i.Wecanusethisrecurrencetocomputeakindof“equilibriumvalue”forE[ktk2]:setE[ktk2]=E[kt−1k2]andsolvetoobtainktk2isO(1/t2/3+k∇F(xt)k2).Thisinturnsuggeststhat wheneverk∇F(xt)k2isgreaterthan1/t2/3 thegradientestimatedt=∇F(xt)+twillbeaverygoodapproximationof∇F(xt)sothatgradientdescentshouldmakeveryfastprogress.Therefore weexpectthe“equilibriumvalue”fork∇F(xt)k2tobeO(1/T2/3) sincethisisthepointatwhichtheestimatedtbecomesdominatedbytheerror.WeformalizethisintuitionusingaLyapunovfunctionoftheformΦt=F(xt)+ztktk2intheproofofTheorem1below.5ProofofTheorem1.ConsiderthepotentialΦt=F(xt)+132L2ηt−1ktk2.WewillupperboundΦt+1−Φtforeacht whichwillallowustoboundΦTintermsofΦ1bysummingovert.First observethatsincew≥(4Lk)3 wehaveηt≤14L.Further sinceat+1=cη2t wehaveat+1≤ck4Lw1/3≤1forallt.Then weﬁrstconsiderη−1tkt+1k2−η−1t−1ktk2.UsingLemma2 weobtainE(cid:2)η−1tkt+1k2−η−1t−1ktk2(cid:3)≤E(cid:20)2c2η3tG2t+1+(1−at+1)2(1+4L2η2t)ktk2ηt+4(1−at+1)2L2ηtk∇F(xt)k2−ktk2ηt−1(cid:21)≤E2c2η3tG2t+1|{z}At+(cid:0)η−1t(1−at+1)(1+4L2η2t)−η−1t−1(cid:1)ktk2|{z}Bt+4L2ηtk∇F(xt)k2|{z}Ct.Letusfocusonthetermsofthisexpressionindividually.Fortheﬁrstterm At observethatw≥2G2≥G2+G2t+1toobtain:TXt=1At=TXt=12c2η3tG2t+1=TXt=12k3c2G2t+1w+Pti=1G2i≤TXt=12k3c2G2t+1G2+Pt+1i=1G2i≤2k3c2ln 1+T+1Xt=1G2tG2!≤2k3c2ln(T+2) whereinthesecondtolastinequalityweusedLemma4intheAppendix.ForthesecondtermBt wehaveBt≤(η−1t−η−1t−1+η−1t(4L2η2t−at+1))ktk2=(cid:0)η−1t−η−1t−1+ηt(4L2−c)(cid:1)ktk2.Letusfocuson1ηt−1ηt−1foraminute.Usingtheconcavityofx1/3 wehave(x+y)1/3≤x1/3+yx−2/3/3.Therefore:1ηt−1ηt−1=1k w+tXi=1G2i!1/3− w+t−1Xi=1G2i!1/3≤G2t3k(w+Pt−1i=1G2i)2/3≤G2t3k(w−G2+Pti=1G2i)2/3≤G2t3k(w/2+Pti=1G2i)2/3≤22/3G2t3k(w+Pti=1G2i)2/3≤22/3G23k3η2t≤22/3G212Lk3ηt≤G27Lk3ηt wherewehaveusedthatthatw≥(4Lk)3tohaveηt≤14L.Further sincec=28L2+G2/(7Lk3) wehaveηt(4L2−c)≤−24L2ηt−G2ηt/(7Lk3).Thus weobtainBt≤−24L2ηtktk2.Puttingallthistogetheryields:132L2TXt=1(cid:18)kt+1k2ηt−ktk2ηt−1(cid:19)≤k3c216L2ln(T+2)+TXt=1(cid:20)ηt8k∇F(xt)k2−3ηt4ktk2(cid:21).(4)Now wearereadytoanalyzethepotentialΦt.Sinceηt≤14L wecanuseLemma1toobtainE[Φt+1−Φt]≤E(cid:20)−ηt4k∇F(xt)k2+3ηt4ktk2+132L2ηtkt+1k2−132L2ηt−1ktk2(cid:21).Summingovertandusing(4) weobtainE[ΦT+1−Φ1]≤TXt=1E(cid:20)−ηt4k∇F(xt)k2+3ηt4ktk2+132L2ηtkt+1k2−132L2ηt−1ktk2(cid:21)≤E"k3c216L2ln(T+2)−TXt=1ηt8k∇F(xt)k2#.6Reorderingtheterms wehaveE"TXt=1ηtk∇F(xt)k2#≤E(cid:2)8(Φ1−ΦT+1)+k3c2/(2L2)ln(T+2)(cid:3)≤8(F(x1)−F?)+E[k1k2]/(4L2η0)+k3c2/(2L2)ln(T+2)≤8(F(x1)−F?)+w1/3σ2/(4L2k)+k3c2/(2L2)ln(T+2) wherethelastinequalityisgivenbythedeﬁnitionofd1andη0inthealgorithm.Now werelateEhPTt=1ηtk∇F(xt)k2itoEhPTt=1k∇F(xt)k2i.First sinceηtisdecreasing E"TXt=1ηtk∇F(xt)k2#≥E"ηTTXt=1k∇F(xt)k2#.Now fromCauchy-Schwarzinequality foranyrandomvariablesAandBwehaveE[A2]E[B2]≥E[AB]2.Hence settingA=qηTPT−1t=1k∇F(xt)k2andB=p1/ηT weobtainE[1/ηT]E"ηTTXt=1k∇F(xt)k2#≥EvuutTXt=1k∇F(xt)k22.Therefore ifwesetM=1kh8(F(x1)−F?)+w1/3σ24L2k+k3c22L2ln(T+2)i togetEvuutTXt=1k∇F(xt)k22≤E"8(F(x1)−F?)+w1/3σ24L2k+k3c22L2ln(T+2)ηT#=E(cid:20)kMηT(cid:21)≤EM w+TXt=1G2t!1/3.Deﬁneζt=∇f(xt ξt)−∇F(xt) sothatE[kζtk2]≤σ2.Then wehaveG2t=k∇F(xt)+ζtk2≤2k∇F(xt)k2+2kζtk2.Pluggingthisinandusing(a+b)1/3≤a1/3+b1/3weobtain:EvuutTXt=1k∇F(xt)k22≤EM w+2TXt=1kζtk2!1/3+M21/3 TXt=1k∇F(xt)k2!1/3≤M(w+2Tσ2)1/3+E21/3MvuutTXt=1k∇F(xt)k22/3≤M(w+2Tσ2)1/3+21/3MEvuutTXt=1k∇F(xt)k22/3 wherewehaveusedtheconcavityofx7→xaforalla≤1tomoveexpectationsinsidetheexponents.Now deﬁneX=qPTt=1k∇F(xt)k2.Thentheabovecanberewrittenas:(E[X])2≤M(w+2Tσ2)1/3+21/3M(E[X])2/3.Notethatthisimpliesthateither(E[X])2≤2M(w+Tσ2)1/3 or(E[X])2≤2·21/3M(E[X])2/3.SolvingforE[X]inthesetwocases weobtainE[X]≤√2M(w+2Tσ2)1/6+2M3/4.Finally observethatbyCauchy-SchwarzwehavePTt=1k∇F(xt)k/T≤X/√TsothatE"TXt=1k∇F(xt)kT#≤√2M(w+2Tσ2)1/6+2M3/4√T≤w1/6√2M+2M3/4√T+2σ1/3T1/3 whereweused(a+b)1/3≤a1/3+b1/3inthelastinequality.70123Steps1e510−410−310−210−1100Train loss (log scale)AdamAdagradStorm(a)TrainLossvsIterations0123Steps1e50.800.850.900.951.00Train accuracyAdamAdagradStorm(b)TrainAccuracyvsIterations0246Steps1e50.40.50.60.70.80.9Test accuracyAdamAdagradStorm(c)TestAccuracyvsIterationsFigure1:ExperimentsonCIFAR-10withResNet-32Network.6EmpiricalValidationInordertoconﬁrmthatouradvancesdoindeedyieldanalgorithmthatperformswellandrequireslittletuning weimplementedSTORMinTensorFlow[1]andtesteditsperformanceontheCIFAR-10imagerecognitionbenchmark[14]usingaResNetmodel[10] asimplementedbytheTensor2Tensorpackage[26]1.WecompareSTORMtoAdaGradandAdam whicharebothverypopularandsuccessfuloptimizationalgorithms.ThelearningratesforAdaGradandAdamweresweptoveralogarithmicallyspacedgrid.ForSTORM wesetw=k=0.1asadefault2andsweptcoveralogarithmicallyspacedgrid sothatallalgorithmsinvolvedonlyoneparametertotune.Noregularizationwasemployed.Werecordtrainloss(cross-entropy) andaccuracyonboththetrainandtestsets(seeFigure1).Theseresultsshowthat whileSTORMisonlymarginallybetterthanAdaGradontestaccuracy onbothtraininglossandaccuracySTORMappearstobesomewhatfasterintermsofnumberofiterations.Wenotethattheconvergenceproofweprovideactuallyonlyappliestothetrainingloss(sincewearemakingmultiplepassesoverthedataset).Weleaveforthefuturewhetherappropriateregularizationcantrade-offSTORM’sbettertraininglossperformancetoobtainbettertestperformance.7ConclusionWehaveintroducedanewvariance-reduction-basedalgorithm STORM thatﬁndscriticalpointsinstochastic smooth non-convexproblems.Ouralgorithmimprovesuponprioralgorithmsbyvirtueofremovingtheneedforcheckpointgradients andincorporatingadaptivelearningrates.TheseimprovementsmeanthatSTORMissubstantiallyeasiertotune:itdoesnotrequirechoosingthesizeofthecheckpoints norhowoftentocomputethecheckpoints(becausetherearenocheckpoints) andbyusingadaptivelearningratesthealgorithmenjoysthesamerobustnesstolearningratetuningaspopularalgorithmslikeAdaGradorAdam.STORMobtainstheoptimalconvergenceguarantee adaptingtothelevelofnoiseintheproblemwithoutknowledgeofthisparameter.WeveriﬁedthatonCIFAR-10withaResNetarchitecture STORMindeedseemstobeoptimizingtheobjectiveinfeweriterationsthanbaselinealgorithms.Additionally wepointoutthatSTORM’supdateformulaisstrikinglysimilartothestandardSGDwithmomentumheuristicemployedinpractice.Toourknowledge notheoreticalresultactuallyestablishesanadvantageofaddingmomentumtoSGDinstochasticproblems creatinganintriguingmystery.WhileouralgorithmisnotpreciselythesameastheSGDwithmomentum wefeelthatitprovidesstrongintuitiveevidencethatmomentumisperformingsomekindofvariancereduction.Wethereforehopethatsomeoftheanalysistechniquesusedinthispapermayprovideapathtowardsexplainingtheadvantagesofmomentum.1https://github.com/google-research/google-research/tree/master/storm_optimizer2Wepickedthesedefaultsbytuningoveralogarithmicgridonthemuch-simplerMNISTdataset[15].wandkwerenottunedonCIFAR10.8References[1]M.Abadi A.Agarwal P.Barham E.Brevdo Z.Chen C.Citro G.S.Corrado A.Davis J.Dean M.Devin S.Ghemawat I.Goodfellow A.Harp G.Irving M.Isard Y.Jia R.Joze-fowicz L.Kaiser M.Kudlur J.Levenberg D.Mané R.Monga S.Moore D.Murray C.Olah M.Schuster J.Shlens B.Steiner I.Sutskever K.Talwar P.Tucker V.Vanhoucke V.Va-sudevan F.Viégas O.Vinyals P.Warden M.Wattenberg M.Wicke Y.Yu andX.Zheng.TensorFlow:Large-scalemachinelearningonheterogeneoussystems 2015.[2]Z.Allen-ZhuandE.Hazan.Variancereductionforfasternon-convexoptimization.InInterna-tionalconferenceonmachinelearning pages699–707 2016.[3]Y.Arjevani Y.Carmon J.C.Duchi D.J.Foster N.Srebro andB.Woodworth.Lowerboundsfornon-convexstochasticoptimization.arXivpreprintarXiv:1912.02365 2019.[4]A.CutkoskyandR.Busa-Fekete.DistributedstochasticoptimizationviaadaptiveSGD.InAdvancesinNeuralInformationProcessingSystems pages1910–1919 2018.[5]A.DefazioandL.Bottou.Ontheineffectivenessofvariancereducedoptimizationfordeeplearning.arXivpreprintarXiv:1812.04529 2018.[6]A.Dieuleveut N.Flammarion andF.Bach.Harder better faster strongerconvergenceratesforleast-squaresregression.J.Mach.Learn.Res. 18(1):3520–3570 January2017.[7]J.C.Duchi E.Hazan andY.Singer.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.JournalofMachineLearningResearch 12:2121–2159 2011.[8]C.Fang C.J.Li Z.Lin andT.Zhang.Spider:Near-optimalnon-convexoptimizationviastochasticpath-integrateddifferentialestimator.InAdvancesinNeuralInformationProcessingSystems pages689–699 2018.[9]S.GhadimiandG.Lan.Stochasticﬁrst-andzeroth-ordermethodsfornonconvexstochasticprogramming.SIAMJournalonOptimization 23(4):2341–2368 2013.[10]K.He X.Zhang S.Ren andJ.Sun.Deepresiduallearningforimagerecognition.InProc.oftheIEEEconferenceoncomputervisionandpatternrecognition pages770–778 2016.[11]P.Jain S.M.Kakade R.Kidambi P.Netrapalli andA.Sidford.Acceleratingstochasticgradientdescentforleastsquaresregression.InS.Bubeck V.Perchet andP.Rigollet editors Proceedingsofthe31stConferenceOnLearningTheory volume75ofProceedingsofMachineLearningResearch pages545–604.PMLR 06–09Jul2018.[12]R.JohnsonandT.Zhang.Acceleratingstochasticgradientdescentusingpredictivevariancereduction.InAdvancesinneuralinformationprocessingsystems pages315–323 2013.[13]D.P.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.InInternationalConferenceonLearningRepresentations(ICLR) 2015.[14]A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.Technicalreport UniversityofToronto 2009.[15]Y.Lecun L.Bottou Y.Bengio andP.Haffner.Gradient-basedlearningappliedtodocumentrecognition.ProceedingsoftheIEEE 86(11):2278–2324 November1998.[16]X.LiandF.Orabona.Ontheconvergenceofstochasticgradientdescentwithadaptivestepsizes.InProc.ofthe22ndInternationalConferenceonArtiﬁcialIntelligenceandStatistics AISTATS 2019.[17]M.Mahdavi L.Zhang andR.Jin.Mixedoptimizationforsmoothfunctions.InAdvancesinneuralinformationprocessingsystems pages674–682 2013.[18]L.M.Nguyen J.Liu K.Scheinberg andM.Takáˇc.SARAH:Anovelmethodformachinelearningproblemsusingstochasticrecursivegradient.InProc.ofthe34thInternationalConferenceonMachineLearning-Volume70 pages2613–2621.JMLR.org 2017.[19]L.M.Nguyen J.Liu K.Scheinberg andM.Takáˇc.Stochasticrecursivegradientalgorithmfornonconvexoptimization.arXivpreprintarXiv:1705.07261 2017.[20]L.M.Nguyen K.Scheinberg andM.Takáˇc.InexactSARAHalgorithmforstochasticoptimization.arXivpreprintarXiv:1811.10105 2018.9[21]S.J.Reddi A.Hefny S.Sra B.Poczos andA.Smola.Stochasticvariancereductionfornonconvexoptimization.InInternationalconferenceonmachinelearning pages314–323 2016.[22]S.J.Reddi S.Kale andS.Kumar.OntheconvergenceofAdamandbeyond.InInternationalConferenceonLearningRepresentations 2018.[23]A.RuszczynskiandW.Syski.Stochasticapproximationmethodwithgradientaveragingforunconstrainedproblems.IEEETransactionsonAutomaticControl 28(12):1097–1105 December1983.[24]T.TielemanandG.Hinton.Lecture6.5-rmsprop:Dividethegradientbyarunningaverageofitsrecentmagnitude.COURSERA:NeuralNetworksforMachineLearning 2012.[25]QuocTran-Dinh NhanHPham DzungTPhan andLamMNguyen.Hybridstochasticgradientdescentalgorithmsforstochasticnonconvexoptimization.arXivpreprintarXiv:1905.05920 2019.[26]A.Vaswani S.Bengio E.Brevdo F.Chollet A.N.Gomez S.Gouws L.Jones Ł.Kaiser N.Kalchbrenner N.Parmar R.Sepassi N.Shazeer andJ.Uszkoreit.Tensor2tensorforneuralmachinetranslation.CoRR abs/1803.07416 2018.[27]C.Wang X.Chen A.J.Smola andE.P.Xing.Variancereductionforstochasticgradientoptimization.InC.J.C.Burges L.Bottou M.Welling Z.Ghahramani andK.Q.Wein-berger editors AdvancesinNeuralInformationProcessingSystems26 pages181–189.CurranAssociates Inc. 2013.[28]R.Ward X.Wu andL.Bottou.AdaGradstepsizes:Sharpconvergenceovernonconvexlandscapes fromanyinitialization.arXivpreprintarXiv:1806.01811 2018.[29]K.Yuan B.Ying andA.H.Sayed.Ontheinﬂuenceofmomentumaccelerationononlinelearning.JournalofMachineLearningResearch 17(192):1–66 2016.[30]L.Zhang M.Mahdavi andR.Jin.Linearconvergencewithconditionnumberindependentaccessoffullgradients.InC.J.C.Burges L.Bottou M.Welling Z.Ghahramani andK.Q.Weinberger editors AdvancesinNeuralInformationProcessingSystems26 pages980–988.CurranAssociates Inc. 2013.[31]D.Zhou P.Xu andQ.Gu.Stochasticnestedvariancereducedgradientdescentfornonconvexoptimization.InAdvancesinNeuralInformationProcessingSystems pages3921–3932 2018.10,Ashok Cutkosky
Francesco Orabona