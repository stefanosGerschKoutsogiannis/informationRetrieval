2019,A First-Order Algorithmic Framework for Distributionally Robust Logistic Regression,Wasserstein distance-based distributionally robust optimization (DRO) has received much attention lately due to its ability to provide a robustness interpretation of various learning models. Moreover  many of the DRO problems that arise in the learning context admits exact convex reformulations and hence can be tackled by off-the-shelf solvers. Nevertheless  the use of such solvers severely limits the applicability of DRO in large-scale learning problems  as they often rely on general purpose interior-point algorithms. On the other hand  there are very few works that attempt to develop fast iterative methods to solve these DRO problems  which typically possess complicated structures. In this paper  we take a first step towards resolving the above difficulty by developing a first-order algorithmic framework for tackling a class of Wasserstein distance-based distributionally robust logistic regression (DRLR) problem. Specifically  we propose a novel linearized proximal ADMM to solve the DRLR problem  whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms. We prove that our method enjoys a sublinear convergence rate. Furthermore  we conduct three different experiments to show its superb performance on both synthetic and real-world datasets. In particular  our method can achieve the same accuracy up to 800+ times faster than the standard off-the-shelf solver.,A First-Order Algorithmic Framework for Wasserstein

Distributionally Robust Logistic Regression

Jiajin Li  Sen Huang  Anthony Man-Cho So

Department of Systems Engineering & Engineering Management

The Chinese University of Hong Kong

Shatin  N. T.  Hong Kong

{jjli hsen manchoso}@se.cuhk.edu.hk

Abstract

Wasserstein distance-based distributionally robust optimization (DRO) has received
much attention lately due to its ability to provide a robustness interpretation of
various learning models. Moreover  many of the DRO problems that arise in the
learning context admits exact convex reformulations and hence can be tackled
by off-the-shelf solvers. Nevertheless  the use of such solvers severely limits the
applicability of DRO in large-scale learning problems  as they often rely on general
purpose interior-point algorithms. On the other hand  there are very few works
that attempt to develop fast iterative methods to solve these DRO problems  which
typically possess complicated structures. In this paper  we take a ﬁrst step towards
resolving the above difﬁculty by developing a ﬁrst-order algorithmic framework
for tackling a class of Wasserstein distance-based distributionally robust logistic
regression (DRLR) problem. Speciﬁcally  we propose a novel linearized proximal
ADMM to solve the DRLR problem  whose objective is convex but consists of a
smooth term plus two non-separable non-smooth terms. We prove that our method
enjoys a sublinear convergence rate. Furthermore  we conduct three different
experiments to show its superb performance on both synthetic and real-world
datasets. In particular  our method can achieve the same accuracy up to 800+ times
faster than the standard off-the-shelf solver.

1

Introduction

One of the basic principles for dealing with the overﬁtting phenomenon in statistical learning is
regularization [23]. Recently  there has been a ﬂurry of works that aim to interpret regularization
from a distributionally robust optimization (DRO) perspective; see  e.g.  [19  1  7  20  18] and the
references therein. The results in these works not only provide a probabilistic justiﬁcation of existing
regularization techniques but also offer a powerful alternative approach to tackle risk minimization
problems. Indeed  it has been shown that the DRO formulations of various statistical learning
problems admit polynomial-time solvable and exact convex reformulations [19  7  2  21  20]  which
can be tackled by off-the-shelf solvers (e.g.  YALMIP). Nevertheless  the use of such solvers severely
limits the applicability of the DRO approach in large-scale learning problems  as they often rely on
general-purpose interior-point algorithms. On the other hand  there are very few works that address
the design of fast iterative methods for solving the convex reformulations of DRO problems. This is
in part due to the complicated structures that are often possessed by such reformulations. In fact  it is
only recently that researchers have proposed stochastic gradient descent (SGD) algorithms for DRO
with f-divergence-based ambiguity sets [17]. However  f-divergence measures can only compare
distributions with the same support  while the Wasserstein distances do not have such a restriction.
On another front  the works [15  11] propose cutting-surface methods to deal with Wasserstein
distance-based DRO problems. However  they tend to suffer a large computational burden.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper  we take a ﬁrst step towards bridging the above-mentioned gap by proposing a new
ﬁrst-order algorithmic framework for solving the class of Wasserstein distance-based distributionally
robust logistic regression (DRLR) problems considered in [19]. The starting point of our investigation
is the following reformulation result; see Theorem 1 and Remark 2 in [19]:

(cid:0)(cid:96)β(ˆxi  ˆyi) + max{ˆyiβT ˆxi − λκ  0}(cid:1)

N(cid:88)

i=1

(1.1)

inf
β

sup

Q∈B(ˆPN )

E(x y)∼Q[(cid:96)β(x  y)] (cid:44) inf

β  λ

λ +

1
N

s.t. (cid:107)β(cid:107)∗ ≤ λ.

 

N

inf

Θ×Θ

(cid:26)(cid:90)

Π∈P(Θ×Θ)

(cid:80)N

Here  x ∈ Rn denotes a feature vector and y ∈ {−1  +1} its associated label to be predicted;
(cid:96)β(x  y) = log(1 + exp(−yβT x)) is the log-loss associated with the feature-label pair (x  y) and
regression parameter β ∈ Rn; {(ˆxi  ˆyi)}N
i=1 are N training samples drawn from an unknown
underlying distribution P∗ on the feature-label space Θ = Rn × {−1  +1}; ˆPN = 1
i=1 δ(ˆxi ˆyi)
denotes the empirical distribution associated with the training samples {(ˆxi  ˆyi)}N
i=1; B(ˆPN ) =
{Q ∈ P(Θ) : W (Q  ˆPN ) ≤ } is the ball in the space P(Θ) of probability distributions on Θ that is
(cid:27)
centered at the empirical distribution ˆPN and has radius  with respect to the Wasserstein distance
d(ξ  ξ(cid:48))Π(dξ  dξ(cid:48)) : Π(dξ  Θ) = Q(dξ)  Π(Θ  dξ(cid:48)) = ˆPN(dξ(cid:48))
W (Q  ˆPN ) =
where ξ = (x  y) ∈ Θ  d(ξ  ξ(cid:48)) = (cid:107)x − x(cid:48)(cid:107) + κ
2|y − y(cid:48)| is the transport cost between two data
points ξ  ξ(cid:48) ∈ Θ induced by a generic norm (cid:107) · (cid:107) on Rn with (cid:107) · (cid:107)∗ being its dual norm  and κ > 0
is a parameter that represents the reliability of the label measurements (the larger the κ  the more
reliable are the measurements; when κ = ∞  the measurements are error-free). The formulation
on the left-hand side of (1.1) is motivated by the desire to construct an ambiguity set around the
empirical distribution ˆPN that contains the true distribution P∗  so that the resulting classiﬁer has
good out-of-sample performance. We refer the reader to [19] for a more detailed discussion.
A natural question that arises from (1.1) is how to solve the convex optimization problem on the
right-hand side (RHS) efﬁciently. When κ = ∞  the RHS of (1.1) reduces to a classic regularized
logistic regression problem [19  Remark 1]. As such  a host of practically efﬁcient ﬁrst-order methods
(such as proximal gradient-type methods or stochastic (variance-reduced) gradient methods) with
provable convergence guarantees (see  e.g.  [22  24  28  2]) can be applied. However  the algorithmic
aspects of the practically more relevant case where κ < ∞ have not been well explored. Our proposed
framework for tackling this case consists of two steps. First  by considering the optimality conditions
of the RHS of (1.1)  we can derive an upper bound λU on the optimal λ∗. This suggests that we can
ﬁrst initialize λ to a value in [0  λU ] and solve the resulting problem that involves only the variable β
(the β-subproblem)  then apply golden-section search to update λ  and then repeat the whole process
until we ﬁnd the optimal solution to (1.1). Second  which is the core step of our framework  is to
design a fast iterative method for solving the β-subproblem. By treating λ as a constant  the RHS
of (1.1) is equivalent to

N(cid:88)

i=1

inf

(cid:107)β(cid:107)∗≤λ

1
N

(cid:0)h(ˆyiβT ˆxi) + max{ˆyiβT ˆxi − λκ  0}(cid:1)

with h(u) = log(1 + exp(−u)). Although (1.2) has a relatively simple norm-ball constraint 
its objective is non-smooth and non-separable. As such  most existing ﬁrst-order methods (e.g. 
projected/proximal subgradient methods) are ill-suited for tackling it. To proceed  we apply the
operator splitting technique to reformulate (1.2) as

(1.2)

(1.3)

N(cid:88)

1
N

(h(µi) + max{µi − λκ  0})

inf
β  µ
s.t. Zβ − µ = 0  (cid:107)β(cid:107)∗ ≤ λ 

i=1

where Z is the N × n matrix whose i-th row is ˆyi ˆxT
i   and propose a new linearized proximal
alternating direction method of multipliers (LP-ADMM) to fully exploit the structure of (1.3). In
particular  our method differs substantially from the commonly used ADMM-variants in the literature
[27  14  6  12  8  9  25] in the updates of the variables. For the β-update  we solve a norm-constrained

2

quadratic optimization problem. Since such a problem can be rather ill-conditioned  we provide
three different types of solvers to handle this task  namely  the accelerated projected gradient descent 
coordinate minimization [10]  and active set conjugate gradient methods [5]. For the µ-update 
observing that the coupling matrix for µ in the linear equality constraint is the identity  the augmented
Lagrangian function is already locally strongly convex in µ. Hence  instead of using a quadratic
approximation of h(·) as in the vanilla proximal ADMM  we use a ﬁrst-order approximation without
(cid:41)
step size selection; i.e. 

i )µi + max{µi − λκ  0}(cid:17) − (wk)T (Zβk+1 − µ) +

(cid:107)µ − Zβk+1(cid:107)2

N(cid:88)

(cid:40)

(cid:16)

(µk

h

(cid:48)

 

2

µk+1 = arg min

µ

ρ
2

1
N

i=1

where w ∈ RN is the dual variable associated with the linear equality constraint in (1.3) and ρ > 0
is the penalty parameter in the augmented Lagrangian function. On the theoretical side  we prove
that our proposed LP-ADMM enjoys an O( 1
K ) convergence rate under standard assumptions. On the
numerical side  we demonstrate via extensive experiments that our proposed method can be sped up
substantially by adopting a geometrically increasing step size strategy. In particular  our method can
achieve a hundred-fold speedup over the standard solver (which is the only other method that has
been used so far to solve (1.1)) on both synthetic and real-world datasets without the need to tune
an optimal penalty parameter in every iteration. To the best of our knowledge  our work is the ﬁrst
to propose a ﬁrst-order algorithmic framework for solving the Wasserstein distance-based DRLR
problem (1.1) for any κ > 0. Moreover  the proposed framework is sufﬁciently general that it can
potentially be applied to other DRO problems  which could be of independent interest.

2 Preliminaries
Let us introduce some basic deﬁnitions and concepts. To allow for greater generality  consider the
following problem:

F (x  y) = f (y) + P (y) + g(x)

x y

minimize
subject to Ax − y = 0.

(2.1)
Here  f : RN → R is a closed convex function that is continuously differentiable on int(dom(f ))
with linear operator A ∈ RN×n; P : RN → R ∪ {+∞} is a closed proper convex function; g(x) is
the indicator function of a norm ball. It should be clear that problem (2.1) includes the β-subproblem
(1.3) as a special case. Indeed  the latter can be written as

F (µ  β) = f (µ) + P (µ) + g(β)

(cid:80)N

µ β

minimize
subject to Zβ − µ = 0 

(cid:8)log(1 + exp(−µi)) + 1

2 (µi − λκ)(cid:9)  P (µ) = 1

(cid:80)N
i=1 |µi − λκ|  and

where f (µ) = 1
N
g(β) = I{(cid:107)β(cid:107)∗≤λ}. Now  the augmented Lagrangian function associated with (2.1) is given by

i=1

2N

Lρ(x  y; w) = f (y) + P (y) + g(x) − wT (Ax − y) +

(2.2)
where w is the multiplier. We use (X ∗ Y∗) to denote the solution set of (2.1). A point (x∗  y∗) is
optimal for (2.1) if there exists a w∗ such that the following KKT conditions are satisﬁed:

(cid:107)Ax − y(cid:107)2
2 

ρ
2

 AT w∗ ∈ ∂g(x∗) 

− w∗ ∈ ∇f (y∗) + ∂P (y∗) 
Ax∗ − y∗ = 0.

(2.3)

Assumption 2.1. There exists a point (x∗  y∗  w∗) satisfying the KKT conditions in (2.3).
Assumption 2.2. The gradient of the function f is Lipschitz continuous; i.e.  there exists a constant
Lf > 0 such that
Deﬁnition 2.3 (Bregman Divergence). Let f : Ω → R be a function that is a) strictly convex  b)
continuously differentiable  and c) deﬁned on a closed convex set Ω. The Bregman divergence with
respect to f is deﬁned as

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ Lf(cid:107)x − y(cid:107) ∀x  y.

Bf (x  y) = f (x) − f (y) − (cid:104)∇f (y)  x − y(cid:105).

3

3 First-Order Algorithmic Framework
In this section  we present our ﬁrst-order algorithmic framework for solving the DRLR problem. For
concreteness’ sake  we take (cid:107) · (cid:107) in the transport cost to be the (cid:96)1-norm in this paper. However  it
should be mentioned that our framework is general enough to handle other norms as well.

N(cid:88)

1
N

min
β  µ
s.t. Zβ − µ = 0 
(cid:107)β(cid:107)∞ ≤ λ.

i=1

(h(µi) + max{µi − λκ  0})

β Exact Update

(cid:13)(cid:13)(cid:13)(cid:13)Zβ − µk − wk

ρ

(cid:107)β(cid:107)∞ ≤ λ.

(cid:13)(cid:13)(cid:13)(cid:13)2

2

(C)

min
β
s.t.

β-subproblem (LP-ADMM)

(B)

Fix λ

min
β  λ

s.t.

min
β  s  λ

s.t.

N(cid:80)

i=1

λ + 1
(h(ˆyiβT ˆxi)+
N
max{ˆyiβT ˆxi − λκ  0})
(cid:107)β(cid:107)∞ ≤ λ.

Exact Reformulation

N(cid:88)

1
N

si

i=1

λ +
(cid:96)β(ˆxi  ˆyi) ≤ si  i ∈ [N ] 
(cid:96)β(ˆxi −ˆyi) − λκ ≤ si i ∈ [N ] 
(cid:107)β(cid:107)∞ ≤ λ.

(A)

First-Order Algorithmic Framework

DRLR Problem

Figure 1: First-order algorithmic framework for Wasserstein DRLR with (cid:96)1-induced transport cost

We summarize the key components of our ﬁrst-order algorithmic framework in Figure 1. As shown
in [19]  the original DRLR problem (i.e.  LHS of (1.1)) can be reformulated as the convex program (A)
using strong duality. A standard approach to tackling problem (A) is to use an off-the-shelf solver
(e.g.  YALMIP). To develop an efﬁcient algorithmic framework  we focus on the RHS of (1.1) and
proceed in two steps. Motivated by the structure of the RHS of (1.1)  a natural ﬁrst step is to ﬁx λ to
a certain value to obtain the problem (1.2)  which involves only the variable β and will be referred to
as the β-subproblem in the sequel. The second  which is also the core step of our framework  is to
design a fast iterative algorithm to tackle the β-subproblem (1.2). The main difﬁculty of problem (1.2)
comes from the two non-smooth non-separable terms. To overcome this difﬁculty  we introduce the
auxiliary variable µi = ˆyiβT ˆxi to split the non-separable non-smooth term max{ˆyiβT ˆxi − λκ  0} 
thus leading to problem (B). Then  we propose a novel linearized proximal ADMM (LP-ADMM)
algorithm to solve it efﬁciently. As will be shown in Section 4  the proposed LP-ADMM will
converge at the rate O(1/K) when applied to the β-subproblem (B). In each iteration of our LP-
ADMM algorithm  we perform an exact minimization for the β-update  which entails solving the
box-constrained quadratic optimization problem (C) (here  wk denotes the corresponding Lagrange
multiplier). Towards that end  we provide three alternative solvers for problem (C)  which target three
different settings. Speciﬁcally  we use accelerated projected gradient descent in the well-conditioned
case; coordinate minimization [10] in the high-dimensional case N (cid:28) d; active set conjugate gradient
method [5] in the ill-conditioned case. The details are given in Appendix B.
To implement the above framework  let us ﬁrst show that there is a ﬁnite upper bound λU on the
optimal λ∗ to problem (A). Observe that the objective function in the RHS of (1.1) takes the form

Ω(λ  β) = λ +

1
N

(cid:0)h(ˆyiβT ˆxi) + max{ˆyiβT ˆxi − λκ  0}(cid:1) + I{(cid:107)β(cid:107)∞≤λ}.

N(cid:88)

i=1

Now  let q(λ) = inf β Ω(λ  β). As the function Ω(· ·) is jointly convex  we can conclude that q(·) is
a convex (and hence unimodal) function on R. Furthermore  the DRLR problem (A) satisﬁes the
Mangasarian-Fromovitz constraint qualiﬁcation (MFCQ)  which implies that its KKT conditions are
necessary and sufﬁcient for optimality. As the following proposition shows  we can use the KKT
system of problem (A) to derive the desired upper bound on λ∗:

4

1
N

λ +

min
si
β  s  λ
s.t. (cid:96)β(ˆxi  ˆyi) ≤ si 

i=1

N(cid:88)

i ∈ [N ] 

i ∈ [N ] 

(cid:96)β(ˆxi −ˆyi) − λκ ≤ si 

⇒

i β ≤ λ  i ∈ [N ] 
eT
−eT
i β ≤ λ  i ∈ [N ]

ai1∇β(cid:96)β(ˆxi  ˆyi) + ai2∇β(cid:96)β(ˆxi −ˆyi) + (ai3 − ai4)ei = 0 

κai2 + ai3 + ai4 =  

  i ∈ [N ] 

1
N

ai1 + ai2 =
ai1((cid:96)β(ˆxi  ˆyi) − si) = 0  i ∈ [N ] 
ai2((cid:96)β(ˆxi −ˆyi) − λκ − si) = 0  i ∈ [N ] 
ai3(eT
ai4(eT
aij ≥ 0  i ∈ [N ]  j ∈ [4].

i β − λ) = 0  i ∈ [N ] 
i β + λ) = 0  i ∈ [N ] 

N(cid:88)
N(cid:88)

i=1

i=1



N(cid:88)

i=1

Proposition 3.1. Suppose that (β∗  λ∗  s∗) is an optimal solution to problem (A) in Figure 1. Then 
we have λ∗ ≤ λU = 0.2785
Proof. Using aij  where i = 1  . . .   N and j = 1  . . .   4 to denote the multipliers associated with the
constraints in problem (A)  we can write down the KKT conditions of problem (A) as follows:

.



After some elementary manipulations (see Appendix A for details)  we obtain

λ ≤ 1
N 

ˆyiβT ˆxi exp(−ˆyiβT ˆxi)
1 + exp(−ˆyiβT ˆxi)

≤ 0.2785



 

as desired.
Remark 3.2. Although Proposition 3.1 applies to the case where the transport cost is induced by the
(cid:96)1-norm  the techniques used to prove it can also carry over to the (cid:96)2 and (cid:96)∞ cases. All we need to
do is to modify the parts highlighted in blue above. Indeed  when the transport cost is induced by the
(cid:96)2-norm  the norm constraint in problem (A) becomes (cid:107)β(cid:107)2 ≤ λ  which is equivalent to (cid:107)β(cid:107)2
2 ≤ λ2.
On the other hand  when the transport cost is induced by the (cid:96)∞-norm  the norm constraint becomes
(cid:107)β(cid:107)1 ≤ λ  which can be expressed as Bβ ≤ λe2n with B being the 2n × n matrix whose rows are
all the possible arrangements of +1’s and −1(cid:48)s.
Proposition 3.1  together with the unimodality of q(·)  suggests the following natural strategy for
ﬁnding an optimal solution (β∗  λ∗  s∗) to problem (A): initialize λ in (A) to a value in [0  λU ]  solve
the resulting β-subproblem (B)  apply golden-section search to update λ  and repeat. The pseudo-code
for the golden-section search on λ can be found in Appendix B. The β-subproblem (B) will be solved
by our proposed LP-ADMM  which we present next.
4 LP-ADMM for the β-Subproblem and Its Convergence Analysis
To simplify notation  we consider the prototypical form (2.1) of the β-subproblem here. It can be
shown that the β-subproblem (B) satisﬁes Assumptions 2.1 and 2.2 in Section 2. Now  we present
our proposed LP-ADMM in Algorithm 1.
The x-update is standard in ADMM-type algorithms and leads to a box-constrained quadratic
optimization problem. The crux of our algorithm lies in the local model used to perform the y-
update. To understand the local model  observe that since the coupling matrix for y in the constraint
Ax − y = 0 is the identity  the augmented Lagrangian function Lρ(· ·;·) in (2.2) is strongly convex
in y. Thus  instead of using the quadratic approximation of f (·) as in the vanilla proximal ADMM 
we can use the ﬁrst-order approximation y (cid:55)→ ˆf (y; yk) = f (yk) + ∇f (yk)T (y − yk) at the current
iterate yk. This leads to the y-update

(cid:110) ˆf (y; yk) − (cid:104)wk  Axk+1 − y(cid:105) +

(cid:111)

(cid:107)y − Axk+1(cid:107)2

2 + P (y)

 

ρ
2

yk+1 = arg min

y

which  as can be easily veriﬁed  is equivalent to the update given in Algorithm 1. In fact  using
such a ﬁrst-order local model not only makes the resulting algorithm converge faster in practice but
also eliminates the need to perform step size selection. The latter makes our algorithmic framework
numerically more robust in general.

5

Algorithm 1: Linearized Proximal ADMM (LP-ADMM) for Solving (2.1)
Input: Choose initial point (x0  y0  w0) ∈ Rn × RN × RN and number of iterations K;
Initialized the penalty parameter ρ0 and shrinking parameter γ ≥ 1;
Output: {(xk  yk  wk)}K
1 for each iteration do
2

k=1 and {F (xk  yk)}K

k=1;

xk+1 = arg min
x∈Rn

(cid:41)

+ g(x)

;

(cid:40)
(cid:40)

ρk
2

ρk
2

(cid:13)(cid:13)(cid:13)(cid:13)2

2

(cid:13)(cid:13)(cid:13)(cid:13)Ax − yk − wk
(cid:13)(cid:13)(cid:13)(cid:13)y −
(cid:18)

ρk

Axk+1 − wk + ∇f (yk)

yk+1 = arg min
y∈RN
wk+1 = wk − ρk(Axk+1 − yk+1);
ρk+1 = γρk (in particular  if γ = 1  then ρk = ρk+1 = ρ);

ρk

(cid:41)

+ P (y)

;

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2

2

3 end

Next  let us analyze the convergence behavior of the LP-ADMM. Based on the deﬁnition of the
augmented Lagrangian function in (2.2)  the optimality conditions of the subproblems in Algorithm 1
can be written as follows:

(cid:18)

(cid:19)

0 ∈ ρAT

Axk+1 − yk − wk
ρ

+ ∂g(xk+1) 

0 ∈ ∇f (yk) + ρ

yk+1 − Axk+1 +

wk
ρ

+ ∂P (yk+1).

(cid:18)

(cid:19)

(4.1)

(4.2)

Using (4.1) and (4.2)  we can establish the following basic properties concerning the iterates of our
proposed LP-ADMM. The proofs can be found in Appendix A.
√
Proposition 4.1. Suppose that we use a constant penalty parameter ρ that satisﬁes ρ > (
3 + 1)Lf .
Let {(xk  yk  wk)}k≥0 be the sequence generated by the LP-ADMM and (x∗  y∗  w∗) be a point
satisfying the KKT conditions (2.3) with x∗ ∈ X   y∗ ∈ Y. Then  the following hold:
(a) For all k ≥ 1  (cid:107)Axk+1 − yk(cid:107)2
(b) For all k ≥ 0 and (x  y) satisfying Ax−y = 0  we have F (xk+1  yk+1)−F (x  y) ≤ 1

ρ2 (cid:107)yk − yk−1(cid:107)2
2.

2(cid:107)yk+1 − yk(cid:107)2

2 − Lf

2 ≥ 1

2

2−(cid:107)yk+1− y(cid:107)2

2) + c((cid:107)yk − yk−1(cid:107)2

2−(cid:107)yk+1− yk(cid:107)2

2ρ ((cid:107)wk(cid:107)2
2−
2) + (Bf (y  yk+1)−

2) + ρ

2 ((cid:107)yk − y(cid:107)2

(cid:107)wk+1(cid:107)2
Bf (y  yk))  where c = ρ−2Lf
2ρ(cid:107)wk − w∗(cid:107)2

(c) The sequence { 1

4

.

below.

2 + ρ

2(cid:107)yk − y∗(cid:107)2

2 − Bf (y∗  yk)}k≥0 is non-increasing and bounded

(cid:80)K

(cid:80)K

Armed with Proposition 4.1  we can prove the main convergence theorem for LP-ADMM.
Theorem 4.2. Consider the setting of Proposition 4.1. Set ¯xK = 1
K
1
K

k=1 yk. Then  the following hold:

k=1 xk and ¯yK =

(a) The sequence {(xk  yk  wk)}k≥0 converges to a KKT point of problem (2.1).
(b) The sequence of function values converges at the rate O( 1

K ):

2 + (ρ/2)(cid:107)y∗ − y0(cid:107)2

F (¯xK  ¯yK) − F (x∗  y∗) ≤ (1/2ρ)(cid:107)w0(cid:107)2

= O(
2(cid:107)y− yk(cid:107)2 
Remark 4.3. The standard linearized ADMM in [14  26  25] involves the quadratic term η
where η needs to satisfy η > Lf . Our LP-ADMM can be regarded as a linearized ADMM with η = 0.
Using the ﬁrst-order local model  the LP-ADMM achieves the fastest single-step update. Moreover  it
is worth noting that the adaptive penalty strategy works well in practice  especially the geometrical
increasing one (i.e.  the blue line in Algorithm 1).

2 + c(cid:107)y0 − y1(cid:107)2

1
K

K

).

2

6

5 Experiment Results
In this section  we present numerical results to demonstrate the effectiveness and efﬁciency of the
different components in our proposed algorithmic framework. All experiments were conducted using
MATLAB R2018a on a computer running Windows 10 with Intel R(cid:13) CoreTM i5-8600 CPU (3.10 GHz)
and 16 GB RAM. We conducted three different experiments to validate our theoretical results and
show the high efﬁciency of our implementation of the proposed ﬁrst-order algorithmic framework.
To begin  we compare the CPU time of our framework with the YALMIP solver used by [19] on both
synthetic and real datasets. Then  we present an empirical comparison of our LP-ADMM with other
baseline ﬁrst-order algorithms  including Projected SubGradient Method (SubGradient)  Primal-Dual
Hybrid Gradient (PDHG)  Linearized-ADMM and Standard ADMM  on the β-subproblem. Lastly 
we show the test data performance of the DRLR model on real datasets. We use the active set
conjugate gradient method to solve the box-constrained quadratic optimization problem (C) in this
section. Our code is available at https://github.com/gerrili1996/DRLR_NIPS2019_exp.

5.1 CPU Time Comparison with the YALMIP Solver
Our setup for the synthetic experiments is as follows. We ﬁrst generate β from the standard n-
dimensional Gaussian distribution N (0  In) and normalize it to obtain the ground truth β∗ = β/(cid:107)β(cid:107).
Next  we generate the feature vectors {ˆxi}N
i=1 independently and identically (i.i.d) from N (0  In) and
the noisy measurements {zi}N
i=1 i.i.d from the uniform distribution over [0  1]. Lastly  we compute
1+exp(−βT∗ ˆxi) ) − 1. We set the DRLR model
the ground truth labels {ˆyi}N
i=1 via ˆyi = 2 × int(zi <
parameters to be κ = 1   = 0.1 and the default parameters of our Adaptive LP-ADMM to be
ρ0 = 0.001  γ = 1.05. All the experiment results reported here were averaged over 30 independent
trials over random seeds. Table 1 summarizes the comparison of CPU times on different scales in the
synthetic setting. Our experiment results indicate that the proposed LP-ADMM with adaptive penalty
strategy can be over 800 times faster than YALMIP  a state-of-the-art optimization solver  and the
performance gap grows considerably with problem size.

1

Table 1: CPU time comparison: LP-ADMM vs. YALMIP (used in [19]) in the synthetic setting

(N  d)
(10 3)
(100 3)
(100 10)
(500 10)
(500 50)
(1000 50)
(1000 100)
(3000 50)
(3000 100)
(5000 100)
(10000 10)
(10000 100)

YALMIP (s)
2.40 ± 0.18
3.29 ± 0.05
3.34 ± 0.03
7.92 ± 0.17
8.53 ± 0.17
16.44 ± 0.44
19.16 ± 0.48
65.87 ± 1.54
113.94 ± 2.05
287.67 ± 2.67
283.25 ± 18.98
1165.40 ± 26.52

Non-Adaptive (s) Adaptive (s) Ratio
37
54
44
55
36
67
51
206
243
451
563
852

0.06 ± 0.02
0.14 ± 0.03
0.21 ± 0.03
0.58 ± 0.16
0.60 ± 0.03
0.96 ± 0.07
1.69 ± 0.11
2.40 ± 0.15
3.84 ± 0.20
7.08 ± 0.66
5.03 ± 0.76
19.75 ± 3.74

0.07 ± 0.02
0.06 ± 0.01
0.08 ± 0.01
0.14 ± 0.01
0.24 ± 0.01
0.25 ± 0.02
0.38 ± 0.01
0.32 ± 0.01
0.47 ± 0.04
0.64 ± 0.03
0.50 ± 0.02
1.37 ± 0.12

We also tested our proposed method on the real datasets a1a-a9a downloaded from LIBSVM1. Note
that the data matrices from these datasets are ill-conditioned and highly sparse  which should be
contrasted with the well-conditioned and dense ones in the synthetic setting. Table 2 shows the
comparison of CPU times on the real datasets. We observe that our methods work exceptionally well 
especially in the large-scale case (i.e.  a9a).
As standard ADMM-type algorithms are very sensitive to the choice of penalty parameters  it is hard
for us to tune the optimal penalty parameter for each subproblem with different λ. Thus  we use a
constant penalty parameter for the non-adaptive case. In fact  since we do not need to perform a
careful penalty parameter selection in our method  we can achieve an even greater speed by using an
adaptive penalty strategy. Moreover  it is worth noticing that our approaches achieve higher-accuracy
solutions compared with the YALMIP solver in all the experiments.

1https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html

7

Table 2: CPU time comparison: LP-ADMM vs. YALMIP (used in [19]) on UCI adult datasets

Dataset

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a

Samples

1605
2265
3185
4781
6414
11220
16100
22696
32561

Data statistics

CPU time (s)

Features YALMIP Ours
2.93
3.53
4.26
4.56
4.39
4.68
5.41
5.81
7.08

25.63
39.20
57.79
105.32
155.42
413.65
738.12
1396.45
2993.30

123
123
123
123
123
123
123
123
123

Ratio

9
11
14
23
35
88
137
240
423

Figure 2: CPU time comparison with YALMIP using interior point algorithm

5.2 Efﬁciency of LP-ADMM for β-Subproblem
To further demonstrate the efﬁcacy of our proposed LP-ADMM on the β-subproblem  we present an
empirical comparison of our algorithm with other ﬁrst-order methods in the synthetic setting. The
implementation details are given as follows: λ is regarded as a constant (i.e.  λ = 0.1)  the DRLR
model parameters are the same as in Section 5.1  and the ﬁrst-order methods used include

(a) Two-block Standard ADMM (SADMM) [3]: for both β- and µ-updates  we perform exact
minimization  which are done using accelerated projected gradient descent and semi-smooth
Newton method [13]  respectively (pseudo-codes are given in Appendix B);

(b) Primal-Dual Hybrid Gradient (PDHG) [4];
(c) Linearized-ADMM (LADMM): all the ingredients are the same as LP-ADMM  except that the

µ-update involves the classic quadratic term;
(d) Projected Subgradient Method (SubGradient).

The convergence curves for various synthetic cases are shown in Figure 3. The performance of our
methods signiﬁcantly dominates those of other methods  which agrees with our theoretical ﬁndings in
Section 4. Compared with LADMM  we show practical advantages of the ﬁrst-order local model for
the µ-update. In addition  LP-ADMM and Adaptive LP-ADMM have similar performance in small
instances  but the latter has better performance in large instances. In summary  we have demonstrated
that the usefulness and efﬁciency of all the components in our ﬁrst-order algorithmic framework. In
particular  as the data matrices in the real datasets are ill-conditioned  all the baseline approaches
cannot achieve a high accuracy but our proposed LP-ADMM can.

5.3 Test Data Performance of the DRLR Model

(cid:80)N
In this subsection  we compare the test data performance of the DRLR model with two classic models 
namely  Logistic Regression (LR) and Regularized Logistic Regression (RLR). The latter refers to
i=1 (cid:96)β(ˆxi  ˆyi) + (cid:107)β(cid:107)∞. If the training data labels are error-free (which corresponds to
min 1
κ = ∞)  then the DRLR model reduces to RLR [19]. We use grid search with cross-validation to
N

8

123456789101112Various synthetic settings in order of scale 020040060080010001200CPU time(s)0100200300400500600700800900RatioThe CPU time Comparision on Synthetic DataYAMLIPOur Method (Adaptive)Ratio0.511.522.53Number of Samples104050010001500200025003000CPU time(s)050100150200250300350400450RatioThe CPU time Comparision on UCI Adult DatasetYAMLIPOur MethodRatioFigure 3: Comparison of LP-ADMM with other ﬁrst-order methods on β-subproblem: y-axis is the
sub-optimality gap log(F k − F ∗); “total iterations” refers to that taken by Adaptive ADMM

select the parameters of the DRLR model (i.e.   = 0.3  κ = 7). In addition  we randomly select
60% of the data to train the models and the rest to test the performance. As before  all the results
reported here were averaged over 30 independent trials. Table 3 shows the average classiﬁcation
accuracy on the test data. We observe that the DRLR model consistently outperforms the two classic
models over all datasets. Thus  the distributionally robust optimization approach opens a new door
for ameliorating the poor test performance in practice.

Table 3: Average classiﬁcation accuracy on test datasets

Dataset

a1a
a2a

MNIST(0 vs 3)
MNIST(0 vs 4)
MNIST(0 vs 6)
MNIST(2 vs 3)
MNIST(2 vs 5)
MNIST(5 vs 8)
MNIST(5 vs 9)
MNIST(6 vs 9)

LR

83.13%
83.68%
99.15%
99.39%
97.88%
96.87%
96.67%
94.80%
97.21%
99.54%

RLR (κ = ∞) DRLR (κ = 7)

83.82%
83.93%
99.45%
99.54%
98.86%
97.31%
97.45%
94.91%
98.11%
99.59%

84.01%
84.24%
99.55%
99.75%
98.92%
97.40%
97.77%
95.18%
98.47%
99.80%

6 Conclusion and Future Work

In this paper  we have proposed a ﬁrst-order algorithmic framework to solve a class of Wasserstein
distance-based distributionally robust logistic regression (DRLR) problem. The core step of our
framework is the efﬁcient solution of the β-subproblem. Towards that end  we have developed a
novel ADMM-type algorithm (the LP-ADMM) and established its sublinear rate of convergence. We
have also conducted extensive experiments to verify the practicality of our framework. It is worth
noting that problem (1.2) actually enjoys the Luo-Tseng error bound property when the transport
cost is induced either by the (cid:96)1-norm or (cid:96)∞-norm [16]. However  this does not immediately imply
the linear convergence of our proposed LP-ADMM  as the method involves both primal and dual
updates. Thus  it is interesting to see whether our proposed LP-ADMM or some other practically
efﬁcient ﬁrst-order methods can provably achieve a linear rate of convergence when applied to the
β-subproblem.

9

References
[1] Jose Blanchet  Yang Kang  and Karthyek Murthy. Robust Wasserstein Proﬁle Inference and

Applications to Machine Learning. Journal of Applied Probability  56(3):830–857  2019.

[2] Jose Blanchet  Karthyek Murthy  and Fan Zhang. Optimal Transport Based Distributional-
ly Robust Optimization: Structural Properties and Iterative Schemes. arXiv preprint arX-
iv:1810.02403  2018.

[3] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed
Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.
Foundations and Trends R(cid:13) in Machine Learning  3(1):1–122  2011.

[4] Antonin Chambolle and Thomas Pock. A First-Order Primal-Dual Algorithm for Convex
Problems with Applications to Imaging. Journal of Mathematical Imaging and Vision  40(1):120–
145  2011.

[5] Wanyou Cheng  Qunfeng Liu  and Donghui Li. An Accurate Active Set Conjugate Gradient
Algorithm with Project Search for Bound Constrained Optimization. Optimization Letters 
8(2):763–776  2014.

[6] Wei Deng and Wotao Yin. On the Global and Linear Convergence of the Generalized Alternating

Direction Method of Multipliers. Journal of Scientiﬁc Computing  66(3):889–916  2016.

[7] Rui Gao  Xi Chen  and Anton J Kleywegt. Wasserstein Distributional Robustness and Regular-

ization in Statistical Learning. arXiv preprint arXiv:1712.06050  2017.

[8] Xiang Gao and Shu-Zhong Zhang. First-Order Algorithms for Convex Optimization with
Nonseparable Objective and Coupled Constraints. Journal of the Operations Research Society
of China  5(2):131–159  2017.

[9] Mingyi Hong and Zhi-Quan Luo. On the Linear Convergence of the Alternating Direction

Method of Multipliers. Mathematical Programming  162(1-2):165–199  2017.

[10] Cho-Jui Hsieh  Kai-Wei Chang  Chih-Jen Lin  S. Sathiya Keerthi  and S. Sundararajan. A
Dual Coordinate Descent Method for Large–Scale Linear SVM. In Proceedings of the 25th
International Conference on Machine Learning (ICML 2008)  pages 408–415  2008.

[11] Changhyeok Lee and Sanjay Mehrotra. A Distributionally-Robust Approach for Finding Support
Vector Machines. Manuscript  available at http://www.optimization-online.org/DB_
HTML/2015/06/4965.html  2015.

[12] Min Li  Defeng Sun  and Kim-Chuan Toh. A Majorized ADMM with Indeﬁnite Proximal Terms
for Linearly Constrained Convex Composite Optimization. SIAM Journal on Optimization 
26(2):922–950  2016.

[13] Xudong Li  Defeng Sun  and Kim-Chuan Toh. A Highly Efﬁcient Semismooth Newton
Augmented Lagrangian Method for Solving Lasso Problems. SIAM Journal on Optimization 
28(1):433–458  2018.

[14] Zhouchen Lin  Risheng Liu  and Zhixun Su. Linearized Alternating Direction Method with
Adaptive Penalty for Low-Rank Representation. In Advances in Neural Information Processing
Systems  pages 612–620  2011.

[15] Fengqiao Luo and Sanjay Mehrotra. Decomposition Algorithm for Distributionally Robust
Optimization Using Wasserstein Metric with an Application to a Class of Regression Models.
European Journal of Operational Research  278(1):20–35  2019.

[16] Zhi-Quan Luo and Paul Tseng. Error Bounds and Convergence Analysis of Feasible Descent

Methods: A General Approach. Annals of Operations Research  46(1):157–178  1993.

[17] Hongseok Namkoong and John C Duchi. Stochastic Gradient Methods for Distributionally
In Advances in Neural Information Processing

Robust Optimization with f-Divergences.
Systems  pages 2208–2216  2016.

10

[18] Hongseok Namkoong and John C Duchi. Variance-based Regularization with Convex Objectives.

In Advances in Neural Information Processing Systems  pages 2971–2980  2017.

[19] Soroosh Shaﬁeezadeh-Abadeh  Peyman Mohajerin Esfahani  and Daniel Kuhn. Distributionally
Robust Logistic Regression. In Advances in Neural Information Processing Systems  pages
1576–1584  2015.

[20] Soroosh Shaﬁeezadeh-Abadeh  Daniel Kuhn  and Peyman Mohajerin Esfahani. Regularization

via Mass Transportation. Journal of Machine Learning Research  20(103):1–68  2019.

[21] Aman Sinha  Hongseok Namkoong  and John Duchi. Certifying Some Distributional Robustness

with Principled Adversarial Training. arXiv preprint arXiv:1710.10571  2017.

[22] Suvrit Sra  Sebastian Nowozin  and Stephen J. Wright  editors. Optimization for Machine
Learning. Neural Information Processing Series. MIT Press  Cambridge  Massachusetts  2012.

[23] Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Statistics for Engineering and

Information Science. Springer Science+Business Media  LLC  second edition  2000.

[24] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance

Reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[25] Yangyang Xu. Accelerated First-Order Primal-Dual Proximal Methods for Linearly Constrained

Composite Convex Programming. SIAM Journal on Optimization  27(3):1459–1484  2017.

[26] Junfeng Yang and Xiaoming Yuan. Linearized Augmented Lagrangian and Alternating Direction
Methods for Nuclear Norm Minimization. Mathematics of computation  82(281):301–329 
2013.

[27] Wotao Yin. Analysis and Generalizations of the Linearized Bregman Method. SIAM Journal on

Imaging Sciences  3(4):856–877  2010.

[28] Zirui Zhou and Anthony Man-Cho So. A Uniﬁed Approach to Error Bounds for Structured
Convex Optimization Problems. Mathematical Programming: Series A and B  165(2):689–728 
2017.

11

,JIAJIN LI
SEN HUANG
Anthony Man-Cho So