2016,Learning a Metric Embedding  for Face Recognition using the Multibatch Method,This work is motivated by the engineering task of achieving a near state-of-the-art face recognition on a minimal computing budget running on an embedded system.  Our main technical contribution centers around a novel training method  called Multibatch  for similarity learning  i.e.  for the task of generating an invariant ``face signature'' through training pairs of ``same'' and ``not-same'' face images. The Multibatch method first generates signatures for a mini-batch of $k$ face images and then constructs an unbiased estimate of the full gradient by relying on all $k^2-k$ pairs from the mini-batch. We prove that the variance of the Multibatch estimator is bounded by $O(1/k^2)$  under some mild conditions. In contrast  the standard gradient estimator that relies on random $k/2$ pairs has a variance of order $1/k$. The smaller variance of the Multibatch estimator significantly speeds up the convergence rate of stochastic gradient descent.  Using the Multibatch method we train a deep convolutional neural network that achieves an accuracy of $98.2\%$ on the LFW benchmark  while its prediction runtime takes only $30$msec on a single ARM Cortex A9 core. Furthermore  the entire training process took only 12 hours on a single Titan X GPU.,A Appendix: Proof of Theorem 1

We ﬁrst show that the estimate is unbiased.
E⇡ `⇡(i) ⇡(j)(z). Therefore 

L(z) =

1

k2  k Xi6=j2[k]

L(z) =

Indeed  for every i 6= j we can rewrite L(z) as
k2  k Xi6=j2[k]

`⇡(i) ⇡(j)(z) = E⇡

L⇡(z)  

E⇡

1

which proves that the multibatch estimate is unbiased.
Next  we turn to analyze the variance of the multibatch estimate. let I ⇢ [k]4 be all the indices
i  j  s  t s.t. i 6= j  s 6= t  and we partition I to I1 [ I2 [ I3  where I1 is the set where i = s and j = t 
I2 is when all indices are different  and I3 is when i = s and j 6= t or i 6= s and j = t. Then:
E⇡ krL⇡(z)  rL(z)k2 =
dXr=1

(rr`⇡(i) ⇡(j)(z)  rrL(z)) (rr`⇡(s) ⇡(t)(z)  rrL(z))

(k2  k)2 E⇡ X(i j s t)2I

3Xq=1 X(i j s t)2Iq

(r`⇡(i) ⇡(j)(z)  rL(z)) · (r`⇡(s) ⇡(t)(z)  rL(z))

(k2  k)2

E⇡

=

1

1

For every r  denote by A(r) the matrix with A(r)
Ei6=j A(r)

i j = 0  and that

i j = rr`i j(z)  rrL(z). Observe that for every r 

Xr

E
i6=j

(A(r)

i j )2 = E

i6=j kr`i j(z)  rL(z)k2.

Therefore 

E⇡ krL⇡(z)  rL(z)k2 =

dXr=1

1

(k2  k)2

3Xq=1 X(i j s t)2Iq

E⇡

A(r)
⇡(i) ⇡(j)A(r)

⇡(s) ⇡(t)

Let us momentarily ﬁx r and omit the superscript from A(r). We consider the value of
E⇡ A⇡(i) ⇡(j)A⇡(s) ⇡(t) according to the value of q.

• For q = 1: we obtain E⇡ A2

⇡(i) ⇡(j) which is the variance of the random variable rr`i j(z)

rrL(z).
• For q = 2: When we ﬁx i  j  s  t which are all different  and take expectation over ⇡ 
then all products of off-diagonal elements of A appear the same number of times in
E⇡ A⇡(i) ⇡(j)A⇡(s) ⇡(t). Therefore  this quantity is proportional toPp6=r vpvr  where v
is the vector of all non-diagonal entries of A. SincePp vp = 0  we obtain (using Lemma 1)
thatPp6=r vpvr  0  which means that the entire sum for this case is non-positive.
• For q = 3: Let us consider the case when i = s and j 6= t  and the derivation for the case
when i 6= s and j = t is analogous. The expression we obtain is E⇡ A⇡(i) ⇡(j)A⇡(i) ⇡(t).
This is like ﬁrst sampling a row and then sampling  without replacement  two indices from
the row (while not allowing to take the diagonal element). So  we can rewrite the expression
as:

A⇡(i) ⇡(j)A⇡(s) ⇡(t) = E

E⇡

E

Ai jAi t

j t2[m]\{i}:j6=t

i⇠[m]
i⇠[m]✓ E
 E

j6=i

Ai j◆2

( ¯Ai)2  

= E

i⇠[m]

(5)

where we denote ¯Ai = Ej6=i Ai j and in the inequality we used again Lemma 1.

Finally  the bound on the variance follows by observing that the number of summands in I1 is k2  k
and the number of summands in I3 is O(k3). This concludes our proof.

10

[vi])2

[vsvt]  (E

i

Proof For simplicity  we use E[v] for Ei[vi] and E[v2] for Ei[v2

Lemma 1 Let v 2 Rn be any vector. Then 
E
s6=t
In particular  if Ei[vi] = 0 thenPs6=t vsvt  0.
nXs=1
nXt=1
nXs=1
nXt=1
vt 
n2  n
n2  n
n
n2  n E[v]2 
n2  n E[v2]
=
n2  n
 0 + E[v]2

(E[v]2  E[v2]) +

n2  n

n2  n

vsvt 

n2

n

vsvt =

E
s6=t

=

=

i ]. Then:

1

1

v2
s

v2
s

nXs=1
nXs=1

n2  n
n2  n E[v]2

1

1

vs

11

,Robert Lindsey
Mohammad Khajah
Michael Mozer
Oren Tadmor
Tal Rosenwein
Shai Shalev-Shwartz
Yonatan Wexler
Amnon Shashua
Jianwei Yang
Zhile Ren
Chuang Gan
Hongyuan Zhu
Devi Parikh