2019,Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization,Communication overhead is one of the key challenges that hinders the scalability of distributed optimization algorithms. In this paper  we study local distributed SGD  where data is partitioned among computation nodes  and the computation nodes perform local updates with periodically exchanging the model among the workers to perform averaging. While local SGD is empirically shown to provide promising results  a theoretical understanding of its performance remains open. In this paper  we strengthen convergence analysis for local SGD  and show that local SGD can be far less expensive and applied far more generally than current theory suggests. Specifically  we show that for loss functions that satisfy the Polyak-Kojasiewicz condition  $O((pT)^{1/3})$ rounds of communication suffice to achieve a linear speed up  that is  an error of $O(1/pT)$  where $T$ is the total number of model updates at each worker. This is in contrast with previous work which required higher number of communication rounds  as well as was limited to strongly convex loss functions  for a similar asymptotic performance. We also develop an adaptive synchronization scheme that provides a general condition for linear speed up. Finally  we validate the theory with experimental results  running over AWS EC2 clouds and an internal GPUs cluster.,Local SGD with Periodic Averaging:

Tighter Analysis and Adaptive Synchronization

Farzin Haddadpour

Penn State

fxh18@psu.edu

Mohammad Mahdi Kamani

Penn State

mqk5591@psu.edu

Mehrdad Mahdavi

Penn State

mzm616@psu.edu

Viveck R. Cadambe

Penn State

vxc12@psu.edu

Abstract

Communication overhead is one of the key challenges that hinders the scalability
of distributed optimization algorithms. In this paper  we study local distributed
SGD  where data is partitioned among computation nodes  and the computation
nodes perform local updates with periodically exchanging the model among the
workers to perform averaging. While local SGD is empirically shown to pro-
vide promising results  a theoretical understanding of its performance remains
open. We strengthen convergence analysis for local SGD  and show that local
SGD can be far less expensive and applied far more generally than current theory
suggests. Speciﬁcally  we show that for loss functions that satisfy the Polyak-
Łojasiewicz condition  O((pT )1/3) rounds of communication sufﬁce to achieve
a linear speed up  that is  an error of O(1/pT )  where T is the total number of
model updates at each worker. This is in contrast with previous work which re-
quired higher number of communication rounds  as well as was limited to strongly
convex loss functions  for a similar asymptotic performance. We also develop an
adaptive synchronization scheme that provides a general condition for linear speed
up. Finally  we validate the theory with experimental results  running over AWS
EC2 clouds and an internal GPU cluster.

1

Introduction

We consider the problem of distributed empirical risk minimization  where a set of p machines  each
with access to a different local shard of training examples Di  i = 1  2    . . .   p  attempt to jointly
solve the following optimization problem over entire data set D = D1 ∪ . . . ∪D p in parallel:

f (x;Di) 

(1)

F (x) ! 1
p

min
x∈Rd

p

!i=1

where f (·;Di) is the training loss over the data shard Di. The predominant optimization methodol-
ogy to solve the above optimization problem is stochastic gradient descent (SGD)  where the model
parameters are iteratively updated by

(2)
where x(t) and x(t+1) are solutions at the tth and (t + 1)th iterations  respectively  and ˜g(t) is a
stochastic gradient of the cost function evaluated on a small mini-batch of all data.

x(t+1) = x(t) − η˜g(t) 

In this paper  we are particularly interested in synchronous distributed stochastic gradient descent
algorithms for non-convex optimization problems mainly due to their recent successes and popu-
larity in deep learning models [26  29  46  47]. Parallelizing the updating rule in Eq. (2) can be

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Comparison of different local-SGD with periodic averaging based algorithms.

Strategy

Convergence Rate Communication Rounds (T /τ )

Extra Assumption

Bounded Gradients

No

Setting

Non-convex

Non-convex

[43]

[38]

[33]

This Paper

O" G2
√pT#
O" 1√pT#
pT#
O" G2
pT#
O" 1

4 T 3
2 T 1
2 T 1

O"p 3
O"p 3
O"p 1
O"p

4#
2#
2#
3#

1

1

3 T

Bounded Gradients

Strongly Convex

No

Non-convex under PL Condition

done simply by replacing ˜g(t) with the average of partial gradients of each worker over a random
mini-batch sample of its own data shard. In fully synchronous SGD  the computation nodes  after
evaluation of gradients over the sampled mini-batch  exchange their updates in every iteration to
ensure that all nodes have the same updated model. Despite its ease of implementation  updating the
model in fully synchronous SGD incurs signiﬁcant amount of communication in terms of number
of rounds and amount of data exchanged per communication round. The communication cost is 
in fact  among the primary obstacles towards scaling distributed SGD to large scale deep learning
applications [30  4  44  21]. A central idea that has emerged recently to reduce the communication
overhead of vanilla distributed SGD  while preserving the linear speedup  is local SGD  which is
the focus of our work. In local SGD  the idea is to perform local updates with periodic averaging 
wherein machines update their own local models  and the models of the different nodes are averaged
periodically [43  38  51  23  45  49  33]. Because of local updates  the model averaging approach
reduces the number of communication rounds in training and can  therefore  be much faster in prac-
tice. However  as the model for every iteration is not updated based on the entire data  it suffers
from a residual error with respect to fully synchronous SGD; but it can be shown that if the averag-
ing period is chosen properly the residual error can be compensated. For instance  in [33] it has been
shown that for strongly convex loss functions  with a ﬁxed mini-batch size with local updates and
periodic averaging  when T model update iterations are performed at each node  the linear speedup

of the parallel SGD is attainable only with O$√pT% rounds of communication  with each node per-
forming τ = O(&T /p) local updates for every round. If p < T   this is a signiﬁcant improvement

than the naive parallel SGD which requires T rounds of communication. This motivates us to study
the following key question: Can we reduce the number of communication rounds even more  and yet
achieve linear speedup?

In this paper  we give an afﬁrmative answer to this question by providing a tighter analysis of local
SGD via model averaging [43  38  51  23  45  49]. By focusing on possibly non-convex loss func-
tions that satisfy smoothness and the Polyak-Łojasiewicz condition [18]  and performing a careful
convergence analysis  we demonstrate that O((pT )1/3) rounds of communication sufﬁce to achieve
linear speed up for local SGD. To the best of our knowledge  this is the ﬁrst work that presents

bounds better than O$√pT% on the communication complexity of local SGD with ﬁxed minibatch

sizes - our results are summarized in Table 1.

The convergence analysis of periodic averaging  where the models are averaged across nodes after
every τ local updates was shown at [49]  but it did not prove a linear speed up. For non-convex

averaging achieves linear speedup. As a further improvement  [38] shows that even by removing

optimization [43] shows that by choosing the number of local updates τ = O"T 1
4#  model
bounded gradient assumption and the choice of τ = O"T 1
2#  linear speedup can be achieved
2# linear speedup can
be achieved by O$√pT% rounds of communication. The present work can be considered as a

tightening of the aforementioned known results. In summary  the main contributions of this paper
are highlighted as follows:

[33] shows that by setting τ = O"T 1

for non-convex optimization.

2 /p 3

4 /p 3

2 /p 1

• We improve the upper bound over the number of local updates in [33] by establishing a lin-
ear speedup O (1/pT ) for non-convex optimization problems under Polyak-Łojasiewicz
3# communication
condition with τ = O"T 2
rounds are sufﬁcient  in contrast to previous work that showed a sufﬁciency of O$√pT%.

Importantly  our analysis does not require boundedness assumption for stochastic gradients
unlike [33].

3#. Therefore  we show that O"p 1

3 /p 1

3 T 1

2

• We introduce an adaptive scheme for choosing the communication frequency and elaborate
on conditions that linear speedup can be achieved. We also empirically verify that the
adaptive scheme outperforms ﬁx periodic averaging scheme.

• Finally  we complement our theoretical results with experimental results on Amazon EC2

cluster and an internal GPU cluster.

2 Other Related Work

Asynchronous parallel SGD. For large scale machine learning optimization problems parallel mini-
batch SGD suffers from synchronization delay due to a few slow machines  slowing down entire
computation. To mitigate synchronization delay  asynchronous SGD method are studied in [28  8 
19]. These methods  though faster than synchronized methods  lead to convergence error issues due
to stale gradients. [2] shows that limited amount of delay can be tolerated while preserving linear
speedup for convex optimization problems. Furthermore  [50] indicates that even polynomially
growing delays can be tolerated by utilizing a quasilinear step-size sequence  but without achieving
linear speedup.

Gradient compression based schemes. A popular approach to reduce the communication cost is
to decrease the number of transmitted bits at each iteration via gradient compression. Limiting the
number of bits in the ﬂoating point representation is studied at [8  13  25]. In [4  40  44]  random
quantization schemes are studied. Gradient vector sparsiﬁcation is another approach analyzed in
[4  40  39  5  30  34  9  3  36  21  32].

Periodic model averaging. The one shot averaging  which can be seen as an extreme case of
model averaging  was introduced in [51  23]. In these works  it is shown empirically that one-shot
averaging works well in a number of optimization problems. However  it is still an open problem
whether the one-shot averaging can achieve the linear speed-up with respect to the number of work-
ers. In fact  [45] shows that one-shot averaging can yield inaccurate solutions for certain non-convex
optimization problems. As a potential solution  [45] suggests that more frequent averaging in the
beginning can improve the performance. [48  31  11  16] represent statistical convergence analy-
sis with only one-pass over the training data which usually is not enough for the training error to
converge. Advantages of model averaging have been studied from an empirical point of view in
[27  7  24  35  17  20]. Speciﬁcally  they show that model averaging performs well empirically in
terms of reducing communication cost for a given accuracy. Furthermore  for the case of T = τ the
work [16] provides speedup with respect to bias and variance for the quadratic square optimization
problems. There is another line of research which aims to reduce communication cost by adding data
redundancy. For instance  reference [15] shows that by adding a controlled amount of redundancy
through coding theoretic means  linear regression can be solved through one round of communica-
tion. Additionally  [14] shows an interesting trade-off between the amount of data redundancy and
the accuracy of local SGD for general non-convex optimization.

Parallel SGD with varying minbatch sizes.
References [10  6] show  for strongly convex
stochastic minimization  that SGD with expo-
nentially increasing batch sizes can achieve lin-
ear convergence rate on a single machine. Re-
cently  [42] has shown that remarkably  with ex-
ponentially growing mini-batch size it is pos-
sible to achieve linear speed up (i.e.  error of
O(1/pT )) with only log T iterations of the al-
gorithm  and thereby  when implemented in a
distributed setting  this corresponds to log T
rounds of communication. The result of [42]
implies that SGD with exponentially increasing
batch sizes has a similar convergence behavior
as the full-ﬂedged (non-stochastic) gradient de-
scent. While the algorithm of [42] provides
a different way of reducing communication in
distributed setting  for a large number of iter-
ations  their algorithm will require large mini-

Figure 1: Running SyncSGD for different number
of mini-batches on Epsilon dataset with logistic
regression. Increasing mini-batches can result in
divergence as it is the case here for mini-batch size
of 1024 comparing to mini-batch size of 512. For
experiment setup please refer to Section 6. A sim-
ilar observation can be found in [20].

3

Algorithm 1 LUPA-SGD(τ ): Local updates with periodic averaging.

Inputs: x(0) as an initial global model and τ as averaging period.
for t = 1  2  . . .   T do

parallel for j = 1  2  . . .   p do
j-th machine uniformly and independently samples a mini-batch ξ(t)
Evaluates stochastic gradient over a mini-batch  ˜g(t)
j
if t divides τ do

as in (3)

j ⊂D at iteration t.

1:
2:
3:

4:

5:
6:
7:

x(t+1)
j

= 1

else do

= x(t)

x(t+1)
j

8:
9:
10:
11:
12: end
13: Output: ¯x(T ) = 1

end if
end parallel for

j )
j − ηt ˜g(t)

j=1(x(t)
p’p
j − ηt ˜g(t)

j

j=1 x(T )

j

p’p

batches  and washes away the computational beneﬁts of the stochastic gradient descent algorithm
over its deterministic counter part. Furthermore certain real-world data sets  it is well known that
larger minibatches also lead to poor generalization and gradient saturation that lead to signiﬁcant
performance gaps between the ideal and practical speed up [12  22  41  20]. Our own experiments
also reveal this (See Fig. 1 that illustrates this for a logistic regression and a ﬁxed learning rate). Our
work is complementary to the approach of [42]  as we focus on approaches that use local updates
with a ﬁxed minibatch size  which in our experiments  is a hyperparameter that is tuned to the data
set.

3 Local SGD with Periodic Averaging

In this section  we introduce the local SGD with model averaging algorithm and state the main
assumptions we make to derive the convergence rates.
SGD with Local Updates and Periodic Averaging. Consider a setting with training data as D  loss
functions fi : Rd → R for each data point indexed as i ∈ 1  2  . . .  |D|  and p distributed machines
. Without loss of generality  it will be notationally convenient to assume D = {1  2  . . .  |D|} in the
sequel. For any subset S⊆D   we denote f (x S) = ’i∈S
p f (x D). Let ξ
denote a 2|D| × 1 random vector that encodes a subset of D of cardinality B  or equivalently  ξ is a
random vector of Hamming weight B. In our local updates with periodic averaging SGD algorithm 
denoted by LUPA-SGD(τ ) where τ represents the number of local updates  at iteration t the jth
machine samples mini-batches ξ(t)
  j = 1  2  . . .   p  t = 1  2  . . .  τ are independent
j
realizations of ξ. The samples are then used to calculate stochastic gradient as follows:

fi(x) and F (x) = 1

  where ξ(t)
j

˜g(t)
j

! 1

B∇f (x(t)

j  ξ (t)
j )

Next  each machine  updates its own local version of the model x(t)

j using:

x(t+1)
j

= x(t)

j − ηt ˜g(t)

j

(3)

(4)

After every τ iterations  we do the model averaging  where we average local versions of the model in
all p machines. The pseudocode of the algorithm is shown in Algorithm 1. The algorithm proceeds
for T iterations alternating between τ local updates followed by a communication round where the
local solutions of all p machines are aggregated to update the global parameters. We note that unlike
parallel SGD that the machines are always in sync through frequent communication  in local SGD
the local solutions are aggregated every τ iterations.

Assumptions. Our convergence analysis is based on the following standard assumptions. We use
the notations g(x) ! ∇F (x D) and ˜g(x) ! 1
B∇f (x ξ ) below. We drop the dependence of these
functions on x when it is clear from context.

4

Assumption 1 (Unbiased estimation). The stochastic gradient evaluated on a mini-batch ξ ⊂D
and at any point x is an unbiased estimator of the partial full gradient  i.e. E [˜g(x)] = g(x) for all
x.
Assumption 2 (Bounded variance [6]). The variance of stochastic gradients evaluated on a mini-
batch of size B from D is bounded as

where C1 and σ are non-negative constants.

E*∥˜g − g∥2+ ≤ C1∥g∥2 +

σ2
B

(5)

Note that the bounded variance assumption (see [6]) is a stronger form of the above with C1 = 0.
Assumption 3 (L-smoothness  µ-Polyak-Łojasiewicz (PL)). The objective function F (x) is differ-
entiable and L-smooth: ∥∇F (x) − ∇F (y)∥ ≤ L∥x − y∥  ∀x  y ∈ Rd  and it satisﬁes the Polyak-
Łojasiewicz condition with constant µ: 1
2 ≥ µ$F (x) − F (x∗)%  ∀x ∈ Rd with x∗ is an
optimal solution  that is  F (x) ≥ F (x∗) ∀x.
Remark 1. Note that the PL condition does not require convexity. For instance  simple functions
4 x2 + sin2(2x) are not convex  but are µ-PL. The PL condition is a generalization
such as f (x) = 1
of strong convexity  and the property of µ-strong convexity implies µ-Polyak-Łojasiewicz (PL)  e.g. 
see [18] for more details. Therefore  any result based on µ-PL assumption also applies assuming
µ-strong convexity. It is noteworthy that while many popular convex optimization problems such as
logistic regression and least-squares are often not strongly convex  but satisfy µ-PL condition [18].

2∥∇F (x)∥2

4 Convergence Analysis

In this section  we present the convergence analysis of the LUPA-SGD(τ ) algorithm. All the proofs
are deferred to the appendix. We deﬁne an auxiliary variable ¯x(t) = 1
j   which is the
average model across p different machines at iteration t. Using the deﬁnition of ¯x(t)  the update rule
in Algorithm 1  can be written as:

p’p

j=1 x(t)

¯x(t+1) = ¯x(t) − η( 1

p

p

!j=1

˜g(t)

j ) 

(6)

which is equivalent to

¯x(t+1) = ¯x(t) − η∇F (¯x(t)) + η(∇F (¯x(t)) −
j=1 ˜g(t)

1
p

p

!j=1

˜g(t)

j ) 

thus establishing a connection between our algorithm and the perturbed SGD with deviation

p’p

$∇F (¯x(t)) − 1

j %. We show that by i.i.d. assumption and averaging with properly chosen

number of local updates  we can reduce the variance of unbiased gradients to obtain the desired
convergence rates with linear speed up. The convergence rate of LUPA-SGD(τ ) algorithm as stated
below:
Theorem 1. For LUPA-SGD(τ ) with τ local updates  under Assumptions 1 - 3  if we choose the
learning rate as ηt = 4
α ) <
p )  and initialize all local model parameters at the same point ¯x(0)  for τ sufﬁciently
µ C1(τ −

µ(t+a) where a = ατ + 4 with α being a constant satisfying α exp (− 2

(τ − 1)τ (a + 1)τ−2  32L2

κ 192( p+1
large to ensure that1 that 4(a − 3)τ−1L(C1 + p) ≤ 64L2(p+1)
1)(a + 1)τ−2 ≤ 64L2
τ ≥ "( p+1

µ (τ − 1)τ (a + 1)τ−2 and
α + 6α# +-"( p+1
2"( p+1

α + 6α#2
α − α2#

p )192κ2e 4
p )192κ2e 4

p )192κ2e 4

µp

1Note that this is a mild condition: if we choose τ as an increasing function of T   e.g.  Corollary 1  this

condition holds. Note also that C1 can be tuned to be small enough if required via appropriate sampling.

5

+ 20"( p+1

p )192κ2e 4

α − α2#

(7)

after T iterations we have:

E!F (¯x

(T )) − F ∗" ≤

µBp(T + a)3
where F ∗ is the global minimum and κ = L/µ is the condition number.

µBpa3E!F (¯x

(0)) − F ∗" + 4κσ2T (T + 2a) + 256κ2σ2T (τ − 1)

 

(8)

An immediate result of above theorem is the following:

Corollary 1. In Theorem 1 choosing τ = O. T

p

1
3 B

2
3

1

3/ leads to the following error bound:

E(F (¯x(T )) − F ∗) ≤ O0 Bp(ατ + 4)3 + T 2

1 = O. 1
pBT/  
Therefore  for large number of iterations T the convergence rate becomes O" 1
implication of Theorem 1 is that by proper choice of τ   i.e.  O"T 2

Bp(T + a)3

3 /p 1

pBT#  thus achieving
3#  and periodically averaging

the local models it is possible to reduce the variance of stochastic gradients as discussed before.
Furthermore  as µ-strong convexity implies µ-PL condition [18]  Theorem 1 holds for µ-strongly
convex cost functions as well.

a linear speed up with respect to the mini-batch size B and the number of machines p. A direct

4.1 Comparison with existing algorithms

Noting that the number of communication rounds is T /τ   for general non-convex optimization  [38]
improves the number of communication rounds in [43] from O(p 3
2 ). In [33]  by
exploiting bounded variance and bounded gradient assumptions  it has been shown that for strongly

linear speed up can be achieved. In comparison to [33]  we show that using the weaker Assump-

convex functions with τ = O(&T /p)  or equivalently T /τ = O(√pT ) communication rounds 
3# or equivalently
tion 3  for non-convex cost functions under PL condition with τ = O"T 2
3# communication rounds  linear speed up can be achieved. All these results are
T /τ = O"(pT )

summarized in Table 1.

4 ) to O(p 3

3 /p 1

4 T 3

2 T 1

1

The detailed proof of Theorem 1 will be provided in appendix  but here we discuss how a tighter
convergence rate compared to [33] is obtainable. In particular  the main reason behind improve-
ment of the LUPA-SGD over [33] is due to the difference in Assumption 3 and a novel technique
introduced to prove the convergence rate. The convergence rate analysis of [33] is based on the uni-
B  
which leads to the following bound on the difference between local solutions and their average at
tth iteration:

formly bounded gradient assumption  E*∥˜gj∥2

2+ ≤ G2  and bounded variance  E*∥˜gj − gj∥2

2+ ≤ σ2

(9)

p

1
p

!j=1

E*∥¯x(t) − x(t)
j ∥2
2+ ≤ 4η2

t G2τ 2.

p

convergence bound which determines the maximum allowable size of the local updates without hurt-

µT 2# in their
In [33] it is shown that weighted averaging over the term (9) results in the term O" κτ 2
ing optimal convergence rate. However  our analysis based on the assumption Eξj*∥˜gj − gj∥2+ ≤
C1∥gj∥2 + σ2
τ ⌋τ ):
p / τ η2
!j=1

B   implies the following bound (see Lemma 3 in appendix with tc ! ⌊ t
)∥2 + 2. p + 1

j ∥2≤2. p + 1

(10)
Note that we obtain (10) using the non-increasing property of ηt from Lemma 3 by careful analysis
of the effect of ﬁrst term in (10) and the weighted averaging. In particular  in our analysis we show
that the second term in (10) can be reduced to 256κ2σ2T (τ−1)
in Theorem 1; hence resulting in
improved upper bound over the number of local updates.

p / [C1 + τ ]

E∥¯x(t) − x(t)

∥ ∇F (x(k)

!k=tc

!j=1

µpB(T +a)3

σ2
B

t−1

η2
k

tc

p

.

j

6

5 Adaptive LUPA-SGD

The convergence results discussed so far are indicated based on a ﬁxed number of local updates  τ .
Recently  [45] and [20] have shown empirically that more frequent communication in the beginning
leads to improved performance over ﬁxed communication period.

The main idea behind adaptive variant of LUPA-SGD stems from the following observation. Let us
consider the convergence error of LUPA-SGD algorithm as stated in (8). A careful investigation of

the obtained rate O" 1
where α being a constant  or equivalently τ = O.T 2

pT# reveals that we need to have a3E(F (¯x(0))−F ∗) = O$T 2% for a = ατ +4
3/. Therefore  the

number of local updates τ can be chosen proportional to the distance of objective at initial model 
¯x(0)  to the objective at optimal solution  x∗. Inspired by this observation  we can think of the ith
communication period as if machines restarting training at a new initial point ¯x(iτ0)  where τ0 is the
number of initial local updates  and propose the following strategy to adaptively decide the number
of local updates before averaging the models:

3(F (¯x(0)) − F ∗)

3 /p 1

1

τi = ⌈"

F (¯x(0))

F (¯x(iτ0)) − F ∗#

1
3

⌉τ0

➀

→ τi = ⌈" F (¯x(0))
F (¯x(iτ0))#

1
3

⌉τ0 

(11)

where’E
i=1 τi = T and E is the total number of synchronizations  and ➀ comes from the fact that
F (x(t)) ≥ F ∗ and as a result we can simply drop the unknown global minimum value F ∗ from the
denominator of (11). Note that (11) generates increasing sequence of number of local updates. A
variation of this choice to decide on τi is discussed in Section 6. We denote the adaptive algorithm
by ADA-LUPA-SGD(τ1  . . .  τ E) for an arbitrary (not necessarily increasing) sequence of positive
integers. Following theorem analyzes the convergence rate of adaptive algorithm  ADA-LUPA-SGD
(τ1  . . .  τ E)).

Theorem 2. For ADA-LUPA-SGD (τ1  . . .  τ E) with local updates  under Assumptions 1 to 3  if we
choose the learning rate as ηt = 4
µ(t+c) and all local model parameters are initialized at the same
point  for τi  1 ≤ i ≤ E sufﬁciently large to ensure that 4(c − 3)τi−1L(C1 + p) ≤ 64L2(p+1)
(τi −
1)τi(c + 1)τi−2  32L2
µ (τi − 1)τi(c + 1)τi−2  and τi  i = 1  2  . . .   E
satisﬁes the condition in (7)  then after T =’E
(0)) − F ∗" +
E!F (¯x

µ C1(τi − 1)(c + 1)τi−2 ≤ 64L2

i=1 τi iterations we have:

256κ2σ2 #E

(T )) − F ∗" ≤

4κσ2T (T + 2c)
µBp(T + c)3 +

c3

(T + c)3 E!F (¯x

µBp(T + c)3

µp

i=1(τi − 1)τi

. (12)

where c = α max1≤i≤E τi + 4  α is a constant satisfying α exp (− 2
minimum and κ = L/µ is the condition number.

α ) <κ $192( p+1

p )  F ∗ is the global

We emphasize that Algorithm 1 with sequence of local updates τ1  . . .  τ E  preserves linear speed up
i=1 τi(τi−1) = O(T 2) 
pB#. Note that exponentially increasing τi that results in a total of

as long as the following three conditions are satisﬁed: i)’E
iii) (max1≤i≤E τi)3 = O" T 2

O(log T ) communication rounds  does not satisfy these three conditions. Thus our result sheds
some theoretical insight of ADA-LUPA algorithm on how big we can choose τi- under our setup
and convergence techniques while preserving linear speed up - although  we note that impossibility
results need to be derived in future work to cement this insight.

i=1 τi = T   ii)’E

Additionally  the result of [37] is based on minimizing convergence error with respect to the wall-
clock time using an adaptive synchronization scheme  while our focus is on reducing the number of
communication rounds for a ﬁxed number of model updates. Given a model for wall clock time  our
analysis can be readily extended to further ﬁne-tune the communication-computation complexity of
[37].

7

Figure 2: Comparison of the convergence rate of SyncSGD with LUPA-SGD with τ = 5 [33] 
τ = 91 (ours) and one-shot (with only one communication round).

6 Experiments

To validate the proposed algorithm compared to existing work and algorithms  we conduct experi-
ments on Epsilon dataset2  using logistic regression model  which satisﬁes PL condition. Epsilon
dataset  a popular large scale binary dataset  consists of 400  000 training samples and 100  000 test
samples with feature dimension of 2000.

Experiment setting. We run our experiments on two different settings implemented with different
libraries to show its efﬁcacy on different platforms. Most of the experiments will be run on Ama-
zon EC2 cluster with 5 p2.xlarge instances. In this environment we use PyTorch [26] to implement
LUPA-SGD as well as the baseline SyncSGD. We also use an internal high performance computing
(HPC) cluster equipped with NVIDIA Tesla V100 GPUs. In this environment we use Tensorﬂow [1]
to implement both SyncSGD and LUPA-SGD. The performance on both settings shows the superi-
ority of the algorithm in both time and convergence3.

Implementations and setups. To run our algorithm  as we stated  we will use logistic regression.
The learning rate and regularization parameter are 0.01 and 1 × 10−4  respectively  and the mini-
batch size is 128 unless otherwise stated. We use mpi4py library from OpenMPI4 as the MPI library
for distributed training.

Normal training. The ﬁrst experiment is to do a normal training on epsilon dataset. As it
was stated  epsilon dataset has 400  000 training samples  and if we want to run the experi-
ment for 7 epochs on 5 machines with mini-batch size of 128 (T = 21875)  based on Ta-
ble 1  we can calculate the given value for τ which for our LUPA-SGD is T 2
3 ≈ 91.
If we follow the τ in [33] we would have to set τ as &T /pb ≈ 5 for this experiment.

3 /(pb) 1

We also include the results for one-shot learn-
ing  which is local SGD with only having one
round of communication at the end. The re-
sults are depicted in Figure 2  shows that LUPA-
SGD with higher τ   can indeed  converges to
the same level as SyncSGD with faster rate in
terms of wall clock time.

Speedup. To show that LUPA-SGD with
greater number of local updates can still ben-
eﬁts from linear speedup with increasing the
number of machines  we run our experiment on
different number of machines. Then  we report
the time that each of them reaches to a certain
error level  say ϵ = 0.35. The results are the
average of 5 repeats.

Adaptive LUPA SGD. To show how ADA
LUPA-SGD works  we run two experiments 

Figure 3: Changing the number of machines and
calculate time to reach certain level of error rate
(ϵ = 0.35).
It indicates that LUPA-SGD with
τ = 91 can beneﬁt from linear speedup by in-
creasing the number of machines. The experiment
is repeated 5 times and the average is reported.

2https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html
3The implementation code is available at https://github.com/mmkamani7/LUPA-SGD.
4https://www.open-mpi.org/

8

Figure 4: Comparison of the convergence rate of LUPA-SGD with ADA-LUPA-SGD with τ = 91
for LUPA-SGD  and τ0 = 91 and τi = (1 + iα)τ0  with α = 1.09 for ADA-LUPA-SGD to have
10 rounds of communication. The results show that ADA-LUPA-SGD can reach the same level of
error rate as LUPA-SGD  with less number of communication.

ﬁrst with constant τ = 91 and the other with increasing number of local updates starting with
τ0 = 91 and τi = (1 + iα)τ0  with α ≥ 0. We set α in a way to have certain number of communica-
tions. This experiment has been run on Tensorﬂow setting described before.

We note that having access to the function F (x(t)) is only for theoretical analysis purposes and is
not necessary in practice as long as the choice of τi satisﬁes the conditions in the statement of the
theorem. In fact as explained in our experiments  we do NOT use the function value oracle and
increase τi within each communication period linearly (please see Figure 4) which demonstrates
improvement over keeping τi constant.

7 Conclusion and Future Work

In this paper  we strengthen the theory of local updates with periodic averaging for distributed non-
convex optimization. We improve the previously known bound on the number of local updates  while
preserving the linear speed up  and validate our results through experiments. We also presented an
adaptive algorithm to decide the number of local updates as algorithm proceeds.

Our work opens few interesting directions as future work. First  it is still unclear if we can preserve
linear speed up with larger local updates (e.g.  τ = O (T /log T ) to require O (log T ) communi-
cations). Recent studies have demonstrated remarkable observations about using large mini-bath
sizes from practical standpoint: [41] demonstrated that the maximum allowable mini-batch size is
bounded by gradient diversity quantity  and [42] showed that using larger mini-batch sizes can lead
to superior training error convergence. These observations raise an interesting question that is wor-
thy of investigation. In particular  an interesting direction motivated by our work and the contrasting
views of these works would be exploring the maximum allowable τ for which performance does not
decay with ﬁxed bound on the mini-batch size. Finally  obtaining lower bounds on the number of
local updates for a ﬁxed mini-bath size to achieve linear speedup is an interesting research question.

Acknowledgement

This work was partially supported by the NSF CCF 1553248 and NSF CCF 1763657 grants.

9

References

[1] Martín Abadi  Paul Barham  Jianmin Chen  Zhifeng Chen  Andy Davis  Jeffrey Dean  Matthieu
Devin  Sanjay Ghemawat  Geoffrey Irving  Michael Isard  et al. Tensorﬂow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design
and Implementation ({OSDI} 16)  pages 265–283  2016.

[2] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances

in Neural Information Processing Systems  pages 873–881  2011.

[3] Alham Fikri Aji and Kenneth Heaﬁeld. Sparse communication for distributed gradient descent.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing 
pages 440–445  2017.

[4] Dan Alistarh  Demjan Grubic  Jerry Li  Ryota Tomioka  and Milan Vojnovic. Qsgd:
Communication-efﬁcient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems  pages 1709–1720  2017.

[5] Jeremy Bernstein  Yu-Xiang Wang  Kamyar Azizzadenesheli  and Anima Anandku-
arXiv preprint

signsgd: Compressed optimisation for non-convex problems.

mar.
arXiv:1802.04434  2018.

[6] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale ma-

chine learning. Siam Review  60(2):223–311  2018.

[7] Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block
training with intra-block parallel optimization and blockwise model-update ﬁltering. In 2016
ieee international conference on acoustics  speech and signal processing (icassp)  pages 5880–
5884. IEEE  2016.

[8] Christopher M De Sa  Ce Zhang  Kunle Olukotun  and Christopher Ré. Taming the wild: A
uniﬁed analysis of hogwild-style algorithms. In Advances in neural information processing
systems  pages 2674–2682  2015.

[9] Nikoli Dryden  Tim Moon  Sam Ade Jacobs  and Brian Van Essen. Communication quanti-
zation for data-parallel training of deep neural networks. In 2016 2nd Workshop on Machine
Learning in HPC Environments (MLHPC)  pages 1–8. IEEE  2016.

[10] Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data

ﬁtting. Technical report  2011.

[11] Antoine Godichon-Baggioni and Soﬁane Saadane. On the rates of convergence of parallelized

averaged stochastic gradient algorithms. arXiv preprint arXiv:1710.07926  2017.

[12] Noah Golmant  Nikita Vemuri  Zhewei Yao  Vladimir Feinberg  Amir Gholami  Kai Rothauge 
Michael W Mahoney  and Joseph Gonzalez. On the computational inefﬁciency of large batch
sizes for stochastic gradient descent. arXiv preprint arXiv:1811.12941  2018.

[13] Suyog Gupta  Ankur Agrawal  Kailash Gopalakrishnan  and Pritish Narayanan. Deep learning
with limited numerical precision. In International Conference on Machine Learning  pages
1737–1746  2015.

[14] Farzin Haddadpour  Mohammad Mahdi Kamani  Mehrdad Mahdavi  and Viveck Cadambe.
Trading redundancy for communication: Speeding up distributed sgd for non-convex optimiza-
tion. In International Conference on Machine Learning  pages 2545–2554  2019.

[15] Farzin Haddadpour  Yaoqing Yang  Viveck Cadambe  and Pulkit Grover. Cross-iteration coded
computing. In 2018 56th Annual Allerton Conference on Communication  Control  and Com-
puting (Allerton)  pages 196–203. IEEE  2018.

[16] Prateek Jain  Sham M Kakade  Rahul Kidambi  Praneeth Netrapalli  and Aaron Sidford. Paral-
lelizing stochastic gradient descent for least squares regression: mini-batching  averaging  and
model misspeciﬁcation. Journal of Machine Learning Research  18(223):1–42  2018.

10

[17] Michael Kamp  Linara Adilova  Joachim Sicking  Fabian Hüger  Peter Schlicht  Tim Wirtz 
and Stefan Wrobel. Efﬁcient decentralized deep learning by dynamic model averaging.
In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases 
pages 393–409. Springer  2018.

[18] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases  pages 795–811. Springer  2016.

[19] Xiangru Lian  Yijun Huang  Yuncheng Li  and Ji Liu. Asynchronous parallel stochastic gradi-
ent for nonconvex optimization. In Advances in Neural Information Processing Systems  pages
2737–2745  2015.

[20] Tao Lin  Sebastian U Stich  and Martin Jaggi. Don’t use large mini-batches  use local sgd.

arXiv preprint arXiv:1808.07217  2018.

[21] Yujun Lin  Song Han  Huizi Mao  Yu Wang  and William J Dally. Deep gradient com-
pression: Reducing the communication bandwidth for distributed training. arXiv preprint
arXiv:1712.01887  2017.

[22] Siyuan Ma  Raef Bassily  and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. arXiv preprint arXiv:1712.06559 
2017.

[23] Ryan McDonald  Keith Hall  and Gideon Mann. Distributed training strategies for the struc-
tured perceptron.
In Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics  pages 456–464.
Association for Computational Linguistics  2010.

[24] Brendan McMahan  Eider Moore  Daniel Ramage  Seth Hampson  and Blaise Aguera y Ar-
cas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial
Intelligence and Statistics  pages 1273–1282  2017.

[25] Taesik Na  Jong Hwan Ko  Jaeha Kung  and Saibal Mukhopadhyay. On-chip training of recur-
rent neural networks with limited numerical precision. In 2017 International Joint Conference
on Neural Networks (IJCNN)  pages 3716–3723. IEEE  2017.

[26] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[27] Daniel Povey  Xiaohui Zhang  and Sanjeev Khudanpur. Parallel training of dnns with natural

gradient and parameter averaging. arXiv preprint arXiv:1410.7455  2014.

[28] Benjamin Recht  Christopher Re  Stephen Wright  and Feng Niu. Hogwild: A lock-free ap-
proach to parallelizing stochastic gradient descent. In Advances in neural information process-
ing systems  pages 693–701  2011.

[29] Frank Seide and Amit Agarwal. Cntk: Microsoft’s open-source deep-learning toolkit. In KDD 

2016.

[30] Frank Seide  Hao Fu  Jasha Droppo  Gang Li  and Dong Yu. 1-bit stochastic gradient descent
In Fifteenth Annual

and its application to data-parallel distributed training of speech dnns.
Conference of the International Speech Communication Association  2014.

[31] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014
52nd Annual Allerton Conference on Communication  Control  and Computing (Allerton) 
pages 850–857. IEEE  2014.

[32] Sebastian U Stich  Jean-Baptiste Cordonnier  and Martin Jaggi. Sparsiﬁed sgd with memory.

In Advances in Neural Information Processing Systems  pages 4447–4458  2018.

[33] Sebastian Urban Stich. Local sgd converges fast and communicates little. In ICLR 2019 ICLR

2019 International Conference on Learning Representations  number CONF  2019.

11

[34] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In

Sixteenth Annual Conference of the International Speech Communication Association  2015.

[35] Hang Su and Haoyu Chen. Experiments on parallel training of deep neural network using

model averaging. arXiv preprint arXiv:1507.01239  2015.

[36] Xu Sun  Xuancheng Ren  Shuming Ma  and Houfeng Wang. meprop: Sparsiﬁed back propa-
gation for accelerated deep learning with reduced overﬁtting. In Proceedings of the 34th Inter-
national Conference on Machine Learning-Volume 70  pages 3299–3308. JMLR. org  2017.

[37] Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-

runtime trade-off in local-update sgd. arXiv preprint arXiv:1810.08313  2018.

[38] Jianyu Wang and Gauri Joshi. Cooperative sgd: A uniﬁed framework for the design and
analysis of communication-efﬁcient sgd algorithms. arXiv preprint arXiv:1808.07576  2018.

[39] Jianqiao Wangni  Jialei Wang  Ji Liu  and Tong Zhang.

Gradient sparsiﬁcation for
communication-efﬁcient distributed optimization. In Advances in Neural Information Process-
ing Systems  pages 1299–1309  2018.

[40] Wei Wen  Cong Xu  Feng Yan  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Tern-
grad: Ternary gradients to reduce communication in distributed deep learning. In Advances in
neural information processing systems  pages 1509–1519  2017.

[41] Dong Yin  Ashwin Pananjady  Max Lam  Dimitris Papailiopoulos  Kannan Ramchandran  and
Peter Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. arXiv
preprint arXiv:1706.05699  2017.

[42] Hao Yu and Rong Jin. On the computation and communication complexity of parallel sgd with
dynamic batch sizes for stochastic non-convex optimization. In International Conference on
Machine Learning  pages 7174–7183  2019.

[43] Hao Yu  Sen Yang  and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence  volume 33  pages 5693–5700  2019.

[44] Hantian Zhang  Jerry Li  Kaan Kara  Dan Alistarh  Ji Liu  and Ce Zhang. Zipml: Training
linear models with end-to-end low precision  and a little bit of deep learning. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70  pages 4035–4043. JMLR.
org  2017.

[45] Jian Zhang  Christopher De Sa  Ioannis Mitliagkas  and Christopher Ré. Parallel sgd: When

does averaging help? arXiv preprint arXiv:1606.07365  2016.

[46] X. Zhang  M. M. Khalili  and M. Liu. Recycled admm: Improving the privacy and accuracy of
distributed algorithms. IEEE Transactions on Information Forensics and Security  pages 1–1 
2019.

[47] Xueru Zhang  Mohammad Mahdi Khalili  and Mingyan Liu. Improving the privacy and ac-
curacy of ADMM-based distributed algorithms. In Jennifer Dy and Andreas Krause  editors 
Proceedings of the 35th International Conference on Machine Learning  volume 80 of Pro-
ceedings of Machine Learning Research  pages 5796–5805. PMLR  10–15 Jul 2018.

[48] Yuchen Zhang  Martin J Wainwright  and John C Duchi. Communication-efﬁcient algorithms
In Advances in Neural Information Processing Systems  pages

for statistical optimization.
1502–1510  2012.

[49] Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic
gradient descent algorithm for nonconvex optimization. In Proceedings of the 27th Interna-
tional Joint Conference on Artiﬁcial Intelligence  pages 3219–3227. AAAI Press  2018.

[50] Zhengyuan Zhou  Panayotis Mertikopoulos  Nicholas Bambos  Peter W Glynn  Yinyu Ye  Li-
Jia Li  and Fei-Fei Li. Distributed asynchronous optimization with unbounded delays: How
slow can you go? In ICML 2018-35th International Conference on Machine Learning  pages
1–10  2018.

12

[51] Martin Zinkevich  Markus Weimer  Lihong Li  and Alex J Smola. Parallelized stochastic
In Advances in neural information processing systems  pages 2595–2603 

gradient descent.
2010.

13

,Gabriele Farina
Andrea Celli
Nicola Gatti
Tuomas Sandholm
Farzin Haddadpour
Mohammad Mahdi Kamani
Mehrdad Mahdavi
Viveck Cadambe