2019,Invariance and identifiability issues for word embeddings,Word embeddings are commonly obtained as optimisers of a criterion function f of a text corpus  but assessed on word-task performance using a different evaluation function g of the test data. We contend that a possible source of disparity in performance on tasks is the incompatibility between classes of transformations that leave f and g invariant. In particular  word embeddings defined by f are not unique; they are defined only up to a class of transformations to which f is invariant  and this class is larger than the class to which g is invariant.  One implication of this is that the apparent superiority of one word embedding over another  as measured by word task performance  may largely be a consequence of the arbitrary elements selected from the respective solution sets. We provide a formal treatment of the above identifiability issue  present some numerical examples  and discuss possible resolutions.,Invariance and identiﬁability issues for word

embeddings

Rachel Carrington

Karthik Bharath

Simon Preston

School of Mathematical Sciences  University of Nottingham

{rachel.carrington  karthik.bharath  simon.preston}@nottingham.ac.uk

Abstract

Word embeddings are commonly obtained as optimizers of a criterion function f of
a text corpus  but assessed on word-task performance using a different evaluation
function g of the test data. We contend that a possible source of disparity in
performance on tasks is the incompatibility between classes of transformations that
leave f and g invariant. In particular  word embeddings deﬁned by f are not unique;
they are deﬁned only up to a class of transformations to which f is invariant  and
this class is larger than the class to which g is invariant. One implication of this is
that the apparent superiority of one word embedding over another  as measured by
word task performance  may largely be a consequence of the arbitrary elements
selected from the respective solution sets. We provide a formal treatment of the
above identiﬁability issue  present some numerical examples  and discuss possible
resolutions.

1

Introduction

Word embeddings map a text corpus  say X  to a collection of vectors V = (v1  ...  vp) where each
vj ∈ Rd  for a prescribed embedding dimension d  represents one of p words in the corpus. Different
word embedding models can be cast as the solution of an optimisation

arg min

F (X  U  V ) = arg min

f (X  U V ) 

U V

U V

(1)

for particular corpus representation X and objective function f  where U = (u1  . . .   un)T are
vectors in Rn representing contexts  typically not of main interest. The setup subsumes some popular
embedding techniques such as Latent Semantic Analysis (LSA) [Deerwester et al.  1990]  word2vec
[Mikolov et al.  2013b a]  and GloVe [Pennington et al.  2014]  wherein the matrices U and V appear
in a suitably chosen f only through their product U V .
Once a word embedding V is constructed by solving (1)  the embedding is evaluated on its perfor-
mance in tasks  including identifying word similarity (given word a  identify words with similar
meanings)  and word analogy (for the statement "a is to b what c is to x"  given a  b and c  identify
x). Similarities or analogies can be computed from V   then performance evaluated against a test data
set D containing human-assigned judgements as

(2)
for some function g. Constructing word embeddings is "unsupervised" with respect to the evaluation
task in the sense that V is determined from (1) independently of the choice of g and the data D in (2) 
although f typically entails free parameters that may  consciously or not  be chosen to optimize (2)
[Levy et al.  2015].

g(D  V ) 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Different word embedding models  identiﬁed as different f in (1)  are often compared based on
performance in word tasks in the sense of g in (2). But there are several reasons why comparing
performance in this way is difﬁcult. First: performance may be affected less by the structure of
model f  and more by the number of free parameters it entails and how well they have been tuned
[Levy et al.  2015]. Second: for many embeddings  solving (1) entails a Monte Carlo optimisation 
so different runs with identical f will result in different realisations of V and hence different values
of g(D  V ). Third  more subtle and often conﬂated with the ﬁrst and second: for most embedding
models f  (1) does not uniquely identify V — V is said to be non-identiﬁable — and different
solutions  V   each equally optimal with respect to (1)  correspond to different values of g(D  V ).
This raises the disconcerting question: can apparent differences in performances in word tasks
as evaluated with g be substantially attributed to the arbitrary selection of a solution V from the
set of solutions of f? In this paper we explore the non-identiﬁability of V   particularly with
respect to the class of non-singular transformations C for which f (X  U V ) = f (X  U C−1CV )
but g(D  V ) (cid:54)= g(D  CV )  and the consequences for constructing and evaluating word embeddings.
Speciﬁcally  our contributions are as follows.

1. For g deﬁned using inner products of embedded word vectors (e.g. Cosine similarity) in d di-
mensions  we characterise the subset Fd contained in the set of non-singular transformations
to which g is not invariant.

2. We study a widely used strategy for constructing word embeddings that involves multiplying
a "base" embedding by a powered matrix of singular values  and show that this amounts to
exploring a one-dimensional subset of the optimal solutions.

3. We discuss resolutions to the non-identiﬁability  including (i) constraining the set of solutions
of f to ensure compatibility with invariances of g  and (ii) optimizing over the solutions of
f with respect to g in a supervised learning sense.

2 Non-identiﬁability of word embedding V

The issue of non-identiﬁability is most transparent in word embedding models explicitly involving
matrix factorisation. LSA assumes X is an n × p context-word matrix and seeks V as

arg min

f (X  U V ) := arg min

(3)
where (cid:107) · (cid:107) is the Frobenius norm  and U is an n × d matrix of contexts to be estimated. For any
particular solution {U∗  V ∗} of (3) {U∗C−1  CV ∗} is also a solution  where C is any d×d invertible
matrix. The solution of (3) for V is hence a set

U V

U V

(cid:107)X − U V (cid:107) 

{CV ∗ : C ∈ GL(d)}

(4)

where GL(d) denotes the general linear group of d × d invertible matrices.
One way to ﬁnd an element of the solution set (4) is by using the singular value decomposition (SVD)
of X. The SVD decomposes X as X = AΣBT where A and B have orthogonal columns and Σ is a
diagonal matrix with the singular values in decreasing order on the diagonal. Then a rank d matrix
that minimizes (cid:107)X − Xd(cid:107) is Xd = AdΣdBT
d where Ad and Bd are matrices containing the ﬁrst d
columns of A and B respectively  and Σd is the d × d upper left part of Σ [Eckart and Young  1936].
Hence a solution to (3) is obtained by taking
U∗ = Ad 

(5)
called by Bullinaria and Levy [2012] the "simple SVD" solution. Bullinaria and Levy [2012] and
Turney [2013] have investigated the word embedding V ∗ = Σ1−α
d which generalises V ∗ in (5)
by introducing a tunable parameter α ∈ R  motivated by empirical evidence that α (cid:54)= 0 often leads to
better performance on word tasks. Such an embedding is perfectly justiﬁed  however  as an alternative
solution
to (3)  for any α ∈ R. We can hence interpret the tuning parameter α as indexing different elements
of the solution set (4)  each optimal with respect to the embedding model f  with α free to be chosen
so that the word-task performance g is maximized.

V ∗ = ΣdBT
d  

U∗ = AdΣα
d  

V ∗ = Σ1−α

d BT
d  

d BT

2

Indeed  by choosing the particular solution V ∗ in (5)  and setting C = Σ−α
d   we see that tuning α
d ∈ GL(d)  a one-dimensional
amounts to optimising over the one-parameter subgroup γ(α) := Σ−α
subset of the d2-dimensional group GL(d) to which V is non-identiﬁable. The motivation for
restricting the optimisation to this particular subset is unclear  however. In fact  it is not clear that
choice of the matrix of singular values Σd in the subgroup γ necessarily leads to better performance
with g; Figure 2 in Section 4.2  demonstrates superior performance for alternate (but arbitrary)
diagonal matrices for certain values of α.
Yin and Shen [2018] (see also references therein) recognise "unitary [equivalently orthogonal]
invariance" of word embeddings  explaining that "two embeddings are essentially identical if one
can be obtained from the other by performing a unitary [orthogonal] operation." Here "essentially
identical" appears to mean with respect to the performance evaluation  our g in this paper. We
emphasise the distinction between this and the non-identiﬁability of V   which refers to the invariance
of f to a (typically larger) class of transformations. The distinction was similarly made by Mu
et al. [2019] who suggested modifying the embedding model f such that the class of invariant
transformations of f and g match. We brieﬂy discuss further their approach later.
Remark 1. The foregoing discussion focuses on the LSA embedding model  f in (3)  in which the
optimal embedding V arises clearly from a matrix factorisation X ≈ U V with respect to Frobenius
norm  and the non-identiﬁability is transparent. But other embedding models  including word2vec
and GloVe  are deﬁned by different f yet share the same property that V is non-identiﬁable  i.e. that
the solution is deﬁned as the set (4). Levy et al. [2015] have shown that word2vec and GloVe both
amount to solving implicit matrix factorisation problems each with respect to a particular corpus
representation X and metric. To see this  and the consequent non-identiﬁability  it is sufﬁcient to
observe  as with the objective of LSA  that the objective functions of word2vec and GloVe involve
matrices U and V appearing only as the product U V .

3 Effect of non-identiﬁability of embeddings on g

The word embeddings are evaluated on tasks on the test data D using the function g  which typically
is based on cosine similarity between elements of Rd. Our focus will hence be on functions g that
depend on V only through the cosine similarity between its columns.
The set of invariances associated with such g consists of the group cO(d) := {cQ ∈ GL(d) : c ∈
R  Q ∈ O(d)}  where O(d) is the subset of orthogonal matrices {Q ∈ GL(d) : QT Q = QQT = Id}.
This set also contains the set of scale transformations cI := {cId : c ∈ R − {0}}. O(d) relates to
transformations that leave (cid:104)v1  v2(cid:105) invariant; the scale transformation preserves the angle between v1
and v2.
Figure 1 (left) illustrates the incompatibility between invariances of f and g. For embedding
dimension d = 2  vi and vj are 2D embeddings of words i and j obtained from solving f with respect
to coordinate vectors {e1  e2}. For Q ∈ O(d)  with respect to orthogonally transformed coordinates
{Qe1  Qe2}  Qvi and Qvj are also viable solutions of f. A g that depends only on cos (vi  vj) has
the same value for cos (Qvi  Qvj). On the other hand  equally valid solutions Cvi and Cvj of f with
respect to nonsingularly transformed coordinates {Ce1  Ce2} for C ∈ GL(d) lead to a different value
of g since cos (vi  vi) (cid:54)= cos (Cvi  Cvj) unless C ∈ cO(d).
Thus with respect to the evaluation function g  each solution from the set {CV ∗ : C ∈ cO(d)} is
equally good (or bad). However  since cO(d) ⊂ GL(d)  there still exist embeddings CV ∗ which solve
f with g(·  CV ∗) (cid:54)= g(·  V ∗). Such C are precisely those which characterise the incompatibility
between invariances of f and g. One such example is the set of C given by the one-parameter
subgroup R (cid:51) α (cid:55)→ Λα  where Λ is a d-dimensional diagonal matrix with positive elements. This
generalises the subgroup γ(α) discussed in §2  which is the special case with Λ = Σd. Figure 1
(right) illustrates the solution set and 1D subsets {ΛαV ∗} for different Λ and particular solutions V ∗.
The discussion above is summarised through the following Proposition.
Proposition 1. Let V ∗ be a solution of (1). Then g is not invariant to non-singular transforms
V ∗ (cid:55)→ ΛαV ∗ for any α ∈ R unless Λ ∈ cI for some c ∈ R.
The key message from Proposition 1 is: for α1  α2 ∈ R  comparison of performances of embeddings
Λα1 V ∗ and Λα2V ∗ using g depends on the (arbitrary) choice of the orthogonal coordinates of Rd.
Note however that the choice of the orthogonal coordinates does not have any bearing on f  and

3

Figure 1: Left: For d = 2  orthogonally transformed coordinates {Qe1  Qe2} (blue) with Q ∈ O(d)  and
nonsingularly transformed {Ce1  Ce2} (green) with C ∈ GL(d)  where {e1  e2} (red) are standard coordinates.
Distances between two embedding vectors vi and vj are preserved in the coordinates {Qe1  Qe2}  but altered in
the coordinates {Ce1  Ce2}. However  {vi  vj} {Qvi  Qvj} and {Cvi  Cvj} are valid solutions to (1). Right:
Illustration of the solution set and one-dimensional subsets ΛαV ∗ parameterised by α for two choices of Λ and
two particular solutions V ∗.

hence Λα1V ∗ and Λα2V ∗ are both solutions of f. The ﬁrst step towards addressing identiﬁability
issues pertaining to f and g is to isolate and understand the structure of the set Fd of transformations
in GL(d) which leave f invariant but not g.
3.1 Structure of the set Fd
What is the dimension of the set Fd ⊂ GL(d)? The dimension of GL(d) is d2 and that of O(d) is
d(d−1)/2. Since cI is one-dimensional  the dimension of Fd is d2−d(d−1)/2−1 = d(d+1)/2−1.
Figure 1 (right) clariﬁes the implication of the result of Proposition 1: given a solution V ∗  tuning α
explores only a one-dimensional set within {CV ∗ : C ∈ Fd} (yellow) within the overall solution set
{CV ∗ : C ∈ GL(d)} (green).
A group-theoretic formalism is useful in precisely identifying Fd. Since O(d) is a subgroup of GL(d) 
we are interested in those elements of GL(d) that cannot be related by an orthogonal transformation.
Such elements can be identiﬁed as the (right) coset GL(d) \ O(d) of O(d) in GL(d): equivalence
classes [C] := {QC : Q ∈ O(d)} for C ∈ GL(d)  known as orbits  under the equivalence relation
M ∼ N if there exists Q ∈ O(d) such that M = QN. The set of orbits {[C] : C ∈ GL(d)} forms a
partition of GL(d): each nonsingular transformation C ∈ GL(d) is associated with its [C]  elements
of which are orthogonally equivalent.
From the deﬁnition of GL(d) \ O(d)  we can represent Fd as Fd = ˜Fd − cI  where ˜Fd represents
what is left behind in GL(d) once O(d) has been ‘removed’  and − denotes the set difference.
Proposition 2. The set ˜Fd can be identiﬁed with the subgroup UT(d) of upper triangular matrices
within GL(d) with positive diagonal entries.
Proof. The proof is based on identifying a set S ⊂ GL(d) that is in bijection with the orbits in
GL(d) \ O(d). Such a subset S is known as a cross section of the coset GL(d) \ O(d)  and intersects
each orbit [C] at a single point. Since O(d) is a subgroup of GL(d)  no two members of Fd belong
to the same orbit [C] of any C ∈ GL(d). Thus Fd can be identiﬁed with any cross section of
GL(d) \ O(d).
The map GL(d) (cid:51) C (cid:55)→ h(C) := C T C is invariant to the action of O(d) since h(QC) =
(QC)T QC = C T C. This implies that h is constant within each orbit [C]. To show that h is maximal
invariant  we need to show that h(C1) = h(C2) if and only if there is a Q ∈ O(d) with C1 = QC2.
2 C2  and let v1  ...  vd be a basis for Rd. Let xi = C1vi and
To see this  suppose that C T
yi = C2vi. Then (cid:104)xi  xj(cid:105) = (cid:104)C1vi  C1vj(cid:105) = (cid:104)vi  C T
2 C2vj(cid:105) = (cid:104)C2vi  C2vj(cid:105) =
(cid:104)yi  yj(cid:105). There thus exists a linear isometry  say Q  such that Qyi = xi for i = 1  ...  d. This implies
that QC2vi = C1vi for i = 1  ...  d  and since v1  ...  vd is a basis for Rd  QC2 = C1 with Q ∈ O(d).
Thus the range of h is in bijection with the orbits in GL(d) \ O(d)  and constitutes a cross section.

1 C1vj(cid:105) = (cid:104)vi  C T

1 C1 = C T

4

For any C ∈ GL(d) consider its unique QR decomposition C = QR  where Q ∈ O(d) and
R ∈ UT(d)  made possible since R is assumed to have positive diagonal elements. Clearly then
h(C) = h(QR) = RT R  and its range h(GL(d)) can be identiﬁed with the set UT(d).

Remark 2. The result of Proposition 2 can be distilled to the existence of a unique QR decomposition
of C ∈ GL(d): C = QR  where Q ∈ O(d) and R ∈ UT(d). There is no loss of generality in
assuming that R has positive entries along the diagonal  since this amounts to multiplying by another
orthogonal matrix which changes signs accordingly. Thus the map GL(d) (cid:51) C (cid:55)→ {UT(d) − cI}
uniquely identiﬁes an element of Fd.
The map GL(d) (cid:51) C (cid:55)→ h(C) = C T C is referred to as a maximal invariant function  and indexes the
elements of GL(d) \ O(d)  and hence UT(d). This offers veriﬁcation of the fact that the dimension of
Fd is d(d + 1)/2 − 1 since it is one fewer than the dimension of the subgroup UT(d). Another way
to arrive at the conclusion is to notice that any d × d upper triangular matrix R can be represented
as R = D(Id + L)  where Id is the identity  L is an upper triangular matrix with zeroes along the
diagonal  and D is a diagonal matrix. The dimension of the set of L is d(d − 1)/2 and that of the set
of D is d  resulting in d + d(d − 1)/2 = d(d + 1)/2 as the dimension of the set of R.

4 Resolving the problem of non-identiﬁability
From the preceding discussion we gather that {CV ∗ : C ∈ Fd} comprises the set of solutions of
f which do not leave g invariant. We explore two resolutions: (i) imposing additional constraints
on V in (1) to identify solutions up to C ∈ O(d) (Theorem 1)  and uniquely (Corollary 1); and
(ii) considering C as a free parameter. In (i) the identiﬁed solution is chosen in a way that is
mathematically natural  but need not be necessarily optimal with respect to g. In (ii)  where C is
considered as a free parameter  it may be chosen to optimize performance in tasks  i.e.  by optimising
g(D  CV ∗) over C ∈ UT(d).

4.1 Constraining the solution set

Redeﬁne (1) as a constrained optimisation

arg min
U V :V ∈Cv

f (X  U V ) 

(6)

over a subset Cv of possible values of V which ensures that the only possible solutions are of the form
{CV ∗ : C ∈ O(d)} for any solution V ∗. The set of possible U is unconstrained. From Proposition
2 and the QR decomposition of an element of GL(d)  this is tantamount to ensuring that CV ∗ for
C ∈ UT(d) is a solution of (6) if and only if C = Id  the identity matrix. Theorem below identiﬁes
the set Cv for any solution of U.
Theorem 1. Let Cv = {V ∈ Rd×p : V V T = Id}. Then for any solution V ∗ to the constrained
problem (6)  any other solution of the form CV ∗ for C ∈ GL(d) satisﬁes g(D  CV ∗) = g(D  V ∗)
for a given test data D.
Proof. Let { ¯U   ¯V } be a solution to the unconstrained problem. The proof rests on the simultaneous
diagonalisation of ¯V ¯V T and ¯U T ¯U. Since ¯V ¯V T is positive deﬁnite there exists M ∈ GL(d) such
that ¯V ¯V T = M T M. Then M−T ( ¯U T ¯U )M−1 is symmetric  and there exists Q ∈ O(d) such that
QT M−T ( ¯U T ¯U )M−1Q = Λ  where Λ is diagonal. Setting C = M−1Q results in C T ¯V ¯V T C =
QT M−T ( ¯V ¯V T )M−1Q = Id.
We thus arrive at the conclusion that there exists a C ∈ GL(d) such that C T ¯V ¯V T C =
and C T ¯U T ¯U C = Λ. The elements of Λ solve the generalised eigenvalue problem
Id 
det( ¯U T ¯U − λ ¯V ¯V T ). Evidently then C ∈ GL(d) is orthogonal if ¯V ¯V T = Id.

An obvious but important corollary to the above Theorem is that any two solutions from Cv are
related through an orthogonal transformation (not necessarily unique).
Corollary 1. For any solutions V1 and V2 of (6) in C there exists an Q ∈ O(d) such that QV1 = V2.
In other words  O(d) acts transitively on C.

5

Remark 3. Optimisation over the constrained set Cv results in a reduction of the invariance transfor-
mations of f from GL(d) to O(d). This can be understood as choosing CV ∗ for a ﬁxed solution V ∗
and arbitrary C ∈ GL(d)  performing a Gram–Schmidt procedure to obtain QRV ∗ for an Q ∈ O(d)
and R ∈ UT(d)  and discarding R. Topologically then  the set of solutions {QV ∗ : Q ∈ O(d)} is ho-
motopically equivalent to the set {CV ∗ : C ∈ GL(d)}. This is because the inclusion O(d) (cid:44)→ GL(d)
is a homotopy equivalence  as it is well-known that the Gram Schmidt process GL(d) → O(d) is a
(strong) deformation retraction.

A unique solution for V can be identiﬁed by imposing additional constraints on U as follows.
Corollary 2. Denote by Cu the set of all U ∈ Rn×d which satisfy the following conditions: (i) The
columns of U are orthogonal; (ii) the diagonal elements of U T U are arranged in descending order;
(iii) ﬁrst non-zero element of each column of U is positive. Then  any solution to the optimisation
problem in (1) over the constrained set (U  V ) ∈ Cu × Cv is unique.
Proof. We need to show that on the constrained space Cu × Cv  the orthogonal C obtained by
optimising (6) reduces to the identity.
On the set Cv  from the proof of Theorem 1  we note that there exists a C ∈ O(d) such that
C T ¯U T ¯U C = Λ for a diagonal Λ containing the eigenvalues of U T U with respect to V V T obtained
a solution of det( ¯U T ¯U − λ ¯V ¯V T ).
In addition to being orthogonal  condition (i) forces C to be a matrix with each column and row
containing one non-zero element assuming values ±1. In other words  C is forced to be a monomial
matrix with entries equal to ±1. This implies that the diagonal C T U T U C contains the same elements
as U T U  but possibly in a different order. Condition (ii) then ﬁxes a particular order  and condition
(iii) ensures that each diagonal element is +1. We thus end up with C = Id.

The idea to modify the optimisation so that the solution is unique up to transformations in O(d)  but
not necessarily GL(d)  is also used by Mu et al. [2019]. Rather than place constraints on V   as above 
they modiﬁed the objective f to include Frobenius norm penalties on U and V   which achieves the
same outcome  although the relationship between the solutions of the penalised and unpenalised
problems is not transparent.

4.1.1 Exploiting symmetry of X

If the corpus representation X is a symmetric matrix  for example involving counts of word-word
co-occurrences  then the rows of U and the columns of V both have the same interpretation as word
embeddings. In such cases the symmetry motivates the imposition U T = V . For example  in LSA
(3) and its solution (5)  this is achieved by taking α = 1/2  since Ad = Bd owing to the symmetry.
This identiﬁes a solution up to sign changes and permutations of the word vectors  transformations
which are contained within O(d) and hence are of no consequence to g.
In GloVe  Pennington et al. [2014] observe that when X is symmetric the U T and V are equivalent
but differ in practise "as a result of their random initializations". It seems likely that different runs
involve the optimisation routine converging to different elements of the solution set  and not in
general to solutions with U T = V . For a given run Pennington et al seek to treat solutions U∗T
and V ∗ symmetrically by taking the word embedding to be V = U∗T + V ∗  which is not itself in
general optimal with respect to the GloVe objective function  f (although they report that using it
over V = V ∗ typically confers a small performance advantage). A different approach is to take the
embedding to be V = CV ∗ where C ∈ GL(d) is the solution to the equation C−T U∗T
= CV ∗
which more directly identiﬁes an element of the solution set for which U T = V   and hence avoids
taking the ﬁnal embedding to be one that is non-optimal with respect to criterion f. The same strategy
is also appropriate to other word embedding models  e.g. word2vec.
4.2 Optimizing over Fd
To what extent can we optimize word-task performance g(D  V ) by choosing an appropriate element
V of the solution set (4)? The set of transformations Fd has dimension d(d + 1)/2 − 1  typically
much larger than the number of cases in d  so care is needed to avoid overﬁtting. In particular 
if the embeddings generated are to be regarded as a predictive model  then it is necessary to use
cross-validation rather than just optimising the embeddings with respect to a particular test set. One

6

V ∗ = ΣdBT
Pearson

d

V ∗ = ΣdBT
Spearman

d

V ∗ = BT
Pearson

d

V ∗ = BT
d
Spearman

Figure 2: Plots showing word task evaluation scores g(D  V ) corresponding to the WordSim-353 task [Finkel-
stein et al.  2002] (located at http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)
which provides a set of word pairs with human-assigned similarity scores. The embeddings are evaluated by
calculating the cosine similarities between the word pairs and using either Pearson or Spearman correlation (each
invariant to O(d) ∪ cI) to score correspondence between embedding and human-assigned similarity values.
The embedding is from model (3)  with X taken to be a document–term matrix computed from the Corpus of
Historical American English [Davies  2012]  and the plotted lines show how performance varies with different
elements of the solution set  namely V = ΛαV ∗ for V ∗ as indicated and different Λ = diag(λ1  ...  λd) as
follows: Λ = Σd (red lines); λi = i (green); λi ∼ U (0  1) (blue); and λi ∼ |N (0  1)| (purple). Performance
for Λ = Σd  which is widely used  is not obviously superior to performance of the other completely arbitrary
choices for Λ.

approach is to restrict the dimension of the optimisation  for example as earlier by considering
solutions V = ΛαV ∗ for a particular solution V ∗ and diagonal matrix Λ. A widely used approach
corresponds to choosing Λ = Σd  a matrix containing the dominant singular values of X; Figure 2
shows how g varies with α for this Λ and some other choices of Λ chosen quite arbitrarily (details
in the caption). There is clearly substantial variability in g with α  but performance with Λ = Σd is
only on a par with the other arbitrary choices.
Figure 3 shows the distribution of g for V = RV ∗ where V ∗ is a GloVe embedding  and R is a
random element of Fd  which is either upper triangular or diagonal  with its non-zero elements taken
from the distribution |N (0  1)|  and g measures the performance of the embeddings on two similarity
test sets. (More details in caption.) The histograms shows substantial variance in the scores for
different R. The score for the base embedding V ∗ is at the higher end of the distribution  though for
some instances of random R the performance of V is superior. It is also noticeable that there is a
much greater range of scores when R is sampled from the set of diagonal matrices than when it is
sampled from the set of upper triangular matrices. We hypothesize that this is because when R is
diagonal  there is a possibility of very small elements on the diagonal which will essentially wipe out
whole rows of V   which could have a signiﬁcant impact on the results.
Table 1 shows scores that result from optimising g(D  V ) for V = ΛV ∗ with respect to the elements
of Λ = diag(λ1  ...  λd)  using R’s optim implementation of the Nelder–Mead method  where V ∗
are 300-dimensional embeddings generated using GloVe and word2vec. The results show that there
exists a transformed embedding ΛV ∗ that performs substantially better than the base embedding. In
practice  in order to use this optimization method to generate embeddings  it would be necessary to
use cross-validation  as embeddings which achieve optimal performance with respect to one test set
may do less well on others. Our aim here is merely to point out that it is possible to improve the test
scores by optimizing over elements of Λ.

5 Conclusions

We summarise our conclusions as follows.

1. Examining word embeddings — including LSA  word2vec  GloVe — through the relation-
ship with low-rank matrix factorisations with respect to a criterion f makes it clear that

7

−2−10120.00.20.40.6a−2−10120.00.20.40.6a−2−10120.00.20.40.6a−2−10120.00.20.40.6aCorrelation coefficient(a)

(b)

(c)

(d)

Figure 3: For the same type of task as in Fig. 2  histograms of Spearman correlation scores for embeddings
V = RV ∗ where V ∗ is a GloVe embedding1 with d = 300 trained on Wikipedia 2014 + Gigaword 5 corpus 
evaluated on the WordSim-353 test set in (a) and (b)  and on the SimLex-999 test set [Hill et al.  2015] in
(c) and (d). R ∈ Fd is a random matrix  taken to be diagonal in (a) and (c) and upper-triangular in (b) and
(d)  in each case with the non-zero elements each distributed as |N (0  1)|. The number of runs in each case
was 1000. The red line on each graph shows the score for the original embedding in each case. 1Source:
https://nlp.stanford.edu/projects/glove/

(a)

(b)

(c)

(d)

Figure 4: Histograms showing the performance of word2vec embeddings trained on the 100-billion word
Google News corpus  where d = 300 (downloaded from https://code.google.com/archive/p/word2vec). As for
Figure 3  the test set used is the WordSim-353 test set in (a) and (b)  and SimLex-999 in (c) and (d)  with the test
score being calculated using the Spearman correlation coefﬁcient. In graphs (a) and (c) R is sampled from the
set of diagonal matrices  and in (b) and (d) it is taken to be upper triangular.

the solution V is non-identiﬁable: for a particular solution V ∗  CV ∗ for any C ∈ GL(d) is
also a solution. Different elements of the d2-dimensional solution set perform differently in
evaluations  g  of word task performance.

2. An important implication is that the disparity in performance between word embeddings
on tasks g maybe due to the particular elements selected from the solution sets. In word
embeddings for which the f is optimized numerically with some randomness  for example
in the initializations  the optimisation may converge to different elements of the solution
set. An embedding chosen based on the best performance in g over repeated runs of the
optimisation can essentially be viewed as a Monte Carlo optimisation over the solution set.
3. The evaluation function g is usually only invariant to orthogonal (O(d)) and scale-type (cI)
transformations. Thus for an embedding dimension d  the effective dimension of the solution
set after accounting for the orthogonal transformations  and scaled versions of the identity  is
d(d + 1)/2 − 1. Conclusions from evaluations with large d must hence be interpreted with
some care  especially if the V is optimized with respect to the incompatible transformations
Fd directly or indirectly  for example as in point 2 above.

4. These considerations have a bearing on the interpretation of the performance of the popular
embedding approach of taking V = ΛαV ∗ where α is a tuning parameter and Λ is a diagonal

8

rDensity0.20.30.40.50.602468rDensity0.500.540.580.6205101520rDensity0.150.250.350246810rDensity0.300.340.3805102030rDensity0.550.65051015rDensity0.580.620.660.7005101520rDensity0.360.400.440510152025rDensity0.380.400.420.4405152535Test set

WordSim-353

SimLex-999

Spearman
0.658
0.601
0.641
0.679
0.700
0.645
0.797
0.371
0.402
0.560
0.441
0.475
0.583

Pearson

0.603
0.637
0.760
0.652
0.588
0.838
0.389
0.421
0.582
0.453
0.480
0.617

GloVe
GloVe with Equation 6 imposed
GloVe optimized over Λ = diag(λ1  ...  λd)
w2v
w2v with Equation 6 imposed

Embeddings
GloVe vectors reported in [Pennington et al.  2014]
GloVe embedding V ∗
GloVe embedding V ∗
V = ΛV ∗
word2vec embedding V ∗
word2vec embedding V ∗
V = ΛV ∗
GloVe embedding V ∗
GloVe embedding V ∗
V = ΛV ∗
word2vec embedding V ∗
word2vec embedding V ∗
V = ΛV ∗

w2v optimized over Λ = diag(λ1  ...  λd)

GloVe
GloVe with Equation 6 imposed
GloVe optimized over Λ = diag(λ1  ...  λd)
w2v
w2v with Equation 6 imposed

w2v optimized over Λ = diag(λ1  ...  λd)

Table 1: Evaluation task scores g(D  V ) corresponding to WordSim-353 [Finkelstein et al.  2002] and SimLex-
999 [Hill et al.  2015] test sets. The base GloVe embedding V ∗ is as described in the caption of Figure 3; the
word2vec embedding is as described in the caption of Figure 4.

In the ﬁrst row we note for reference the performance reported in [Pennington et al.  2014]. The
results indicate substantial scope for improving performance scores via an appropriate choice of Λ.

matrix taken  for example  to contain the singular values of X. This amounts to providing a
way to perform a search over a one-dimensional subset of the (d(d + 1)/2− 1)-dimensional
solution set. Our numerical results suggest there is nothing special about this particular
choice of Λ (or the corresponding one-dimensional subset being searched over)  nor is there
a clear rationale for restricting to a one-dimensional subset.

Acknowledgments

The authors gratefully acknowledge support for this work from grants NSF DMS 1613054 and NIH
RO1 CA214955 (KB)  a Bloomberg Data Science Research Grant (KB & SP)  and an EPSRC PhD
studentship (RC).

References
John A Bullinaria and Joseph P Levy. Extracting semantic representations from word co-occurrence

statistics: stop-lists  stemming  and svd. Behavior research methods  44(3):890–907  2012.

Mark Davies. Expanding horizons in historical linguistics with the 400-million word corpus of

historical american english. Corpora  7(2):121–157  2012.

Scott Deerwester  Susan T Dumais  George W Furnas  Thomas K Landauer  and Richard Harshman.
Indexing by latent semantic analysis. Journal of the American society for information science  41
(6):391–407  1990.

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychome-

trika  1(3):211–218  1936.

Lev Finkelstein  Evgeniy Gabrilovich  Yossi Matias  Ehud Rivlin  Zach Solan  Gadi Wolfman  and
Eytan Ruppin. Placing search in context: The concept revisited. ACM Transactions on information
systems  20(1):116–131  2002.

Felix Hill  Roi Reichart  and Anna Korhonen. Simlex-999: Evaluating semantic models with

(genuine) similarity estimation. Computational Linguistics  41(4):665–695  2015.

Omer Levy  Yoav Goldberg  and Ido Dagan. Improving distributional similarity with lessons learned
from word embeddings. Transactions of the Association for Computational Linguistics  3:211–225 
2015.

Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word representa-

tions in vector space. arXiv preprint arXiv:1301.3781  2013a.

9

Tomas Mikolov  Ilya Sutskever  Kai Chen  Greg Corrado  and Jeffrey Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in neural information
processing systems  pages 3111–3119  2013b.

Cun Mu  Guang Yang  and Zheng Yan. Revisiting skip-gram negative sampling model with rectiﬁca-

tion. arXiv preprint arXiv:1804.00306v2  2019.

Jeffrey Pennington  Richard Socher  and Christopher D. Manning. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing  pages 1532–1543  2014.

Peter D Turney. Distributional semantics beyond words: Supervised learning of analogy and

paraphrase. Transactions of the Association for Computational Linguistics  1:353–366  2013.

Zi Yin and Yuanyuan Shen. On the dimensionality of word embedding. In Advances in Neural

Information Processing Systems  pages 887–898  2018.

10

,Rachel Carrington
Karthik Bharath
Simon Preston