2019,Dual Variational Generation for Low Shot Heterogeneous Face Recognition,Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem  and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise  for the sake of reducing the domain gap of HFR. Specifically  we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then  in order to ensure the identity consistency of the generated paired heterogeneous images  we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover  the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. When using the generated paired images for training  our method gains more than 18\% True Positive Rate improvements over the baseline model when False Positive Rate is at $10^{-5}$.,Dual Variational Generation for Low Shot

Heterogeneous Face Recognition

Chaoyou Fu1 2∗  Xiang Wu1∗  Yibo Hu1  Huaibo Huang1  Ran He1 2 3†

1NLPR & CRIPAC  CASIA

2University of Chinese Academy of Sciences

3Center for Excellence in Brain Science and Intelligence Technology  CAS
{chaoyou.fu  rhe}@nlpr.ia.ac.cn  alfredxiangwu@gmail.com

{yibo.hu  huaibo.huang}@cripac.ia.ac.cn

Abstract

Heterogeneous Face Recognition (HFR) is a challenging issue because of the large
domain discrepancy and a lack of heterogeneous data. This paper considers HFR
as a dual generation problem  and proposes a novel Dual Variational Generation
(DVG) framework. It generates large-scale new paired heterogeneous images with
the same identity from noise  for the sake of reducing the domain gap of HFR.
Speciﬁcally  we ﬁrst introduce a dual variational autoencoder to represent a joint
distribution of paired heterogeneous images. Then  in order to ensure the identity
consistency of the generated paired heterogeneous images  we impose a distribution
alignment in the latent space and a pairwise identity preserving in the image space.
Moreover  the HFR network reduces the domain discrepancy by constraining the
pairwise feature distances between the generated paired heterogeneous images. Ex-
tensive experiments on four HFR databases show that our method can signiﬁcantly
improve state-of-the-art results.

1

Introduction

With the development of deep learning  face recognition has made signiﬁcant progress [34  2] in recent
years. However  in many real-world applications  such as video surveillance  facial authentication on
mobile devices and computer forensics  it is still a great challenge to match heterogeneous face images
in different modalities  including sketch images [37]  near infrared images [24] and polarimetric
thermal images [36]. Heterogeneous face recognition (HFR) has attracted much attention in the face
recognition community. Due to the large domain gap  one challenge is that the face recognition model
trained on VIS data often degrades signiﬁcantly for HFR. Therefore  lots of cross domain feature
matching methods [10] are introduced to reduce the large domain gap between heterogeneous face
images. However  since it is expensive and time-consuming to collect a large number of heterogeneous
face images  there is no public large-scale heterogeneous face database. With the limited training
data  CNNs trained for HFR often tend to be overﬁtting.
Recently  the great progress of high-quality face synthesis [38  5  33  39] has made “recognition via
generation” possible. TP-GAN [16] and CAPG-GAN [13] introduce face synthesis to improve the
quantitative performance of large pose face recognition. For HFR  [32] proposes a two-path model
to synthesize VIS images from NIR images. [36] utilizes a GAN based multi-stream feature fusion
technique to generate VIS images from polarimetric thermal faces. However  all these methods are
based on conditional image-to-image translation framework  leading to two potential challenges: 1)

∗Equal Contribution
†Corresponding Author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: The diversity comparisons between the conditional image-to-image translation [32] (left
part  the above is the input NIR image and the below is the corresponding translated VIS image) and
our unconditional DVG (right part  all paired heterogeneous images are generated via noise). For the
conditional image-to-image translation methods  given one NIR image  a generator only synthesizes
one new VIS image with same attributes (e.g.  the pose and the expression) except for the spectral
information. Differently  DVG generates massive new paired images with rich intra-class diversity
from noise.

Diversity: Given one image  a generator only synthesizes one new image of the target domain [32].
It means such conditional image-to-image translation methods can only generate limited number
of images. In addition  as shown in the left part of Fig. 1  two images before and after translation
have same attributes (e.g.  the pose and the expression) except for the spectral information  which
means it is difﬁcult for such conditional image-to-image translation methods to promote intra-class
diversity. In particular  these problems will be very prominent in the low-shot heterogeneous face
recognition  i.e.  learning from few heterogeneous data. 2) Consistency: When generating large-scale
samples  it is challenging to guarantee that the synthesized face images belong to the same identity of
the input images. Although identity preserving loss [13] constrains the distances between features of
the input and synthesized images  it does not constraint the intra-class and inter-class distances of the
embedding space.
To tackle the above challenges  we propose a novel unconditional Dual Variational Generation (DVG)
framework (shown in Fig. 3) that generates large-scale paired heterogeneous images with the same
identity from noise. Unconditional generative models can generate new images (generate single
image per time) from noise [21]  but since these images do not have identity labels  it is difﬁcult
to use these images for recognition networks. DVG makes use of the property of generating new
images of the unconditional generative model [21]  and adopts a dual generation manner to get
paired heterogeneous images with the same identity every time. This enables DVG to generate
large-scale images  and make the generated images can be used to optimize recognition networks.
Meanwhile  DVG also absorbs the various intra-class changes of the training database  leading to
the generated paired images have abundant intra-class diversity. For instance  as presented in the
right part of Fig. 1  the ﬁrst four paired images have different poses  and the ﬁfth paired images
have different expressions. Furthermore  DVG only pays attention to the identity consistency of
the paired heterogeneous images rather than the identity whom the paired heterogeneous images
belong to  which avoids the consistency problem of previous methods. Speciﬁcally  we introduce a
dual variational autoencoder to learn a joint distribution of paired heterogeneous images. In order to
constrain the generated paired images to belong to the same identity  we impose both a distribution
alignment in the latent space and a pairwise identity preserving in the image space. New paired
images are generated by sampling and copying a noise from a standard Gaussian distribution  as
displayed in the left part of Fig. 3. These generated paired images are used to optimize the HFR
network by a pairwise distance constraint  aiming at reducing the domain discrepancy.
In summary  the main contributions are as follows:

• We provide a new insight into the problems of HFR. That is  we consider HFR as a dual
generation problem  and propose a novel dual variational generation framework. This
framework generates new paired heterogeneous images with abundant intra-class diversity
to reduce the domain gap of HFR.

• In order to guarantee that the generated paired images belong to the same identity  we
constrain the consistency of paired images in both latent space and image space. These
allow new images sampled from the noise can be used for recognition networks.

2

Conditional translationUnconditional dual variational generationTranslationFigure 2: The dual generation results via noise (256 × 256 resolution). For each pair  the left is NIR
and the right is the paired VIS image.

• We can sample large-scale diverse paired heterogeneous images from noise. By constraining
the pairwise feature distances of the generated paired images in the HFR network  the
domain discrepancy is effectively reduced.

• Experiments on the CASIA NIR-VIS 2.0  the Oulu-CASIA NIR-VIS  the BUAA-VisNir and
the IIIT-D Viewed Sketch databases demonstrate that our method can generate photo-realistic
images  and signiﬁcantly improve the performance of recognition.

2 Background and Related Work

2.1 Heterogeneous Face Recognition

Lots of researchers pay their attention to Heterogeneous Face Recognition (HFR). For the feature-level
learning  [22] employs HOG features with sparse representation for HFR. [7] utilizes LBP histogram
with Linear Discriminant Analysis to obtain domain-invariant features. [10] proposes Invariant Deep
Representation (IDR) to disentangle representations into two orthogonal subspaces for NIR-VIS HFR.
Further  [11] extends IDR by introducing Wasserstein distance to obtain domain invariant features
for HFR. For the image-level learning  the common idea is to transform heterogeneous face images
from one modality into another one via image synthesis. [19] utilizes joint dictionary learning to
reconstruct face images for boosting the performance of face matching. [23] proposes a cross-spectral
hallucination and low-rank embedding to synthesize a heterogeneous image in a patch way.

2.2 Generative Models

Variational autoencoders (VAEs) [21] and generative adversarial networks (GANs) [6] are the most
prominent generative models. VAEs consist of an encoder network qφ(z|x) and a decoder network
pθ(x|z). qφ(z|x) maps input images x to the latent variables z that match to a prior p(z)  and pθ(x|z)
samples images x from the latent variables z. The evidence lower bound objective (ELBO) of VAEs:

log pθ(x) ≥ Eqφ(z|x) log pθ(x|z) − DKL(qφ(z|x)||p(z)).

(1)

The two parts in ELBO are a reconstruction error and a Kullback-Leibler divergence  respectively.
Differently  GANs adopt a generator G and a discriminator D to play a min-max game. G generates
images from a prior p(z) to confuse D  and D is trained to distinguish between generated data and
real data. This adversarial rule takes the form:

min

G

max

D

Ex∼pdata(x) [log D(x)] + Ez∼pz(z) [log(1 − D(G(z)))] .

(2)

They have achieved remarkable success in various applications  such as unconditional image genera-
tion that generates images from noise [20  15]  and conditional image generation that synthesizes
images according to the given condition [32  16]. According to [15]  VAEs have nice manifold
representations  while GANs are better at generating sharper images.
Another work to address the similar problem of our method is CoGAN [25]  which uses a weight-
sharing manner to generate paired images in two different modalities. However  CoGAN neither
explicitly constrains the identity consistency of paired images in the latent space nor in the image
space. It is challenging for the weight-sharing manner of CoGAN to generate paired images with the
same identity  as shown in Fig. 4.

3

Figure 3: The purpose (left part) and training model (right part) of our unconditional DVG framework.
DVG generates large-scale new paired heterogeneous images with the same identity from standard
Gaussian noise  aiming at reducing the domain discrepancy for HFR. In order to achieve this purpose 
we elaborately design a dual variational autoencoder. Given a pair of heterogeneous images from
the same identity  the dual variational autoencoder learns a joint distribution in the latent space. In
order to guarantee the identity consistency of the generated paired images  we impose a distribution
alignment in the latent space and a pairwise identity preserving in the image space.

3 Proposed Method

In this section  we will introduce our method in detail  including the dual variational generation and
heterogeneous face recognition. Note that we speciﬁcally discuss the NIR-VIS images for better
presentation. Other heterogeneous images are also applicable.

3.1 Dual Variational Generation

As shown in the right part of Fig. 3  DVG consists of a feature extractor Fip  and a dual variational
autoencoder: two encoder networks and a decoder network  all of which play the same roles of
VAEs [21]. Speciﬁcally  Fip extracts the semantic information of the generated images to preserve
the identity information. The encoder network EN maps NIR images xN to a latent space zN =
qφN (zN|xN ) by a reparameterization trick: zN = uN + σN (cid:12)   where uN and σN denote mean
and standard deviation of NIR images  respectively. In addition   is sampled from a multi-variate
standard Gaussian and (cid:12) denotes the Hadamard product. The encoder network EV has the same
manner as EN : zV = qφV (zV |xV )  which is for VIS images xV . After obtaining the two independent
distributions  we concatenate zN and zV to get the joint distribution zI.

Distribution Learning We utilize VAEs to learn the joint distribution of the paired NIR-VIS images.
Given a pair of NIR-VIS images {xN   xV }  we constrain the posterior distribution qφN (zN|xN ) and
qφV (zV |xV ) by the Kullback-Leibler divergence:

Lkl = DKL(qφN (zN|xN )||p(zN )) + DKL(qφV (zV |xV )||p(zV )) 

(3)

where the prior distributions p(zN ) and p(zV ) are both the multi-variate standard Gaussian dis-
tributions. Like the original VAEs  we require the decoder network pθ(xN   xV |zI ) to be able to
reconstruct the input images xN and xV from the learned distribution:

Lrec = −EqφN (zN|xN )∪qφV (zV |xV ) log pθ(xN   xV |zI ).

(4)

Distribution Alignment We expect a pair of NIR-VIS images {xN   xV } to be projected into a
common latent space by the encoders EN and EV   i.e.  the NIR distribution p(z(i)
N ) is the same as
the VIS distribution p(z(i)
V )  where i denotes the identity information. That means we maintain the
identity consistency of the generated paired images in the latent space. Explicitly  we align the NIR
and VIS distributions by minimizing the Wasserstein distance between the two distributions. Given
two Gaussian distributions p(z(i)
)  the 2-Wasserstein
distance between p(z(i)

N ) = N (u(i)

V ) = N (u(i)

) and p(z(i)

V ) is simpliﬁed [10] as:

N   σ(i)

N

V   σ(i)

V

2

N − u(i)

V ||2

2 + ||σ(i)

N − σ(i)

V ||2

2

.

(5)

N ) and p(z(i)
Ldist =
1
2

(cid:104)||u(i)

(cid:105)

2

4

!"CopyStandard Gaussian Noise#̂"HFR NetDomain GapReductionz%&'&#&#(Distribution Alignment)&*&)(*(%(!"#"Concat+ -PairwiseIdentityPreserving'(ReconstructedGeneratedPairwise Identity Preserving
In previous image-to-image translation works [16  13]  identity
preserving is usually introduced to maintain identity information. The traditional approach uses a
pre-trained feature extractor to enforce the features of the generated images to be close to the target
ones. However  since the lack of intra-class and inter-class constraints  it is challenge to guarantee the
synthesized images to belong to the speciﬁc categories of the target images. Considering that DVG
generates a pair of heterogeneous images per time  we only need to consider the identity consistency
of the paired images.
Speciﬁcally  we adopt Light CNN [34] as the feature extractor Fip to constrain the feature distance
between the reconstructed paired images:

Lip-pair = ||Fip(ˆxN ) − Fip(ˆxV )||2
2 

(6)
where Fip(·) means the normalized output of the last fully connected layer of Fip. In addition  we
also use Fip to make the features of the reconstructed images and the original input images close
enough as previous works [16  13]:

Lip-rec = ||Fip(ˆxN ) − Fip(xN )||2

2 + ||Fip(ˆxV ) − Fip(xV )||2
2 

(7)

where ˆxN and ˆxV denote the reconstructions of the input paired images xN and xV   respectively.

Diversity Constraint
In order to further increase the diversity of the generated images  we also
introduce a diversity loss [27]. In the sampling stage  when two sampled noise zI1 are zI2 are
close  the generated images xI1 and xI2 are going to be similar. We maximize the following loss to
encourage the decoder DI to generate more diverse images:

Ldiv = max

DI

|Fip(xI1) − Fip(xI2)|

|zI1 − zI2|

.

(8)

Overall Loss Moreover  in order to increase the sharpness of our generated images  we also adopt
an adversarial loss Ladv as [31]. Hence  the overall loss to optimize the dual variational autoencoder
can be formulated as

Lgen = Lrec + Lkl + Ladv + λ1Ldist + λ2Lip-pair + λ3Lip-rec + λ4Ldiv 

(9)

where λ1  λ2  λ3 and λ4 are the trade-off parameters.

3.2 Heterogeneous Face Recognition

For the heterogeneous face recognition  our training data contains the original limited labeled data
xi(i ∈ {N  V }) and the large-scale generated unlabeled paired NIR-VIS data ˜xi(i ∈ {N  V }). Here 
we deﬁne a heterogeneous face recognition network F to extract features fi = F (xi; Θ)  where
i ∈ {N  V } and Θ is the parameters of F . For the original labeled NIR and VIS images  we utilize a
softmax loss:

softmax(F (xi; Θ)  y) 

(10)

(cid:88)

i∈{N V }

Lcls =

where y is the label of identity.
For the generated paired heterogeneous images  since they are generated from noise  there are no
speciﬁc classes for the paired images. But as mentioned in section 3.1  DVG ensures that the generated
paired images belong to the same identity. Therefore  a pairwise distance loss between the paired
heterogeneous samples is formulated as follows:

Lpair = ||F (˜xN ; Θ) − F (˜xV ; Θ)||2
2 

(11)

In this way  we can efﬁciently minimize the domain discrepancy by generating large-scale unlabeled
paired heterogeneous images. As stated above  the ﬁnal loss to optimize for the heterogeneous face
recognition network can be written as

Lhfr = Lcls + α1Lpair 

(12)

where α1 is the trade-off parameter.

5

Method MD
CoGAN 0.61
VAE
0.54
DVG
0.24

FID Rank-1
95.2
10.6
94.6
8.2
7.1
99.2

Method
w/o Ldist
w/o Lip-pair
w/o Ldiv
DVG

Rank-1
94.3
96.1
98.5
99.2

(a)

(b)

Table 1: Experimental analyses on the CASIA NIR-VIS 2.0 database. The backbone is LightCNN-9.
(a) The quantitative comparisons of different methods. MD (lower is better) means the mean feature
distance between the generated paired NIR and VIS images. FID (lower is better) is measured based
on the features of LightCNN-9  instead of the traditional Inception model. (b) The ablation study.

4 Experiments

4.1 Databases and Protocols

Three NIR-VIS heterogeneous face databases and one Sketch-Photo heterogeneous face database
are used to evaluate our proposed method. For the NIR-VIS face recognition  following [35]  we
report Rank-1 accuracy and veriﬁcation rate (VR)@false accept rate (FAR) for the CASIA NIR-VIS
2.0 [24]  the Oulu-CASIA NIR-VIS [18] and the BUAA-VisNir Face [14] databases. Note that  for
the Oulu-CASIA NIR-VIS database  there are only 20 subjects are selected as the training set. In
addition  the IIIT-D Viewed Sketch database [1] is employed for the Sketch-Photo face recognition.
Due to the few number of images in the IIIT-D Viewed Sketch database  following the protocols
of [3]  we use the CUHK Face Sketch FERET (CUFSF) [37] as the training set and report the Rank-1
accuracy and VR@FAR=1% for comparisons.

4.2 Experimental Details

For the dual variational generation  the architectures of the encoder and decoder networks are the
same as [15]  and the architecture of our discriminator is the same as [31]. These networks are
trained using Adam optimizer with a ﬁxed rate of 0.0002. Other parameters λ1  λ2  λ3 and λ4 in
Eq. (9) are set to 50  5  1000 and 0.2  respectively. For the heterogeneous face recognition  we
utilize both LightCNN-9 and LightCNN-29 [34] as the backbones. The models are pre-trained on the
MS-Celeb-1M database [9] and ﬁne-tuned on the HFR training sets. All the face images are aligned
to 144× 144 and randomly cropped to 128× 128 as the input for training. Stochastic gradient descent
(SGD) is used as the optimizer  where the momentum is set to 0.9 and weight decay is set to 5e-4.
The learning rate is set to 1e-3 initially and reduced to 5e-4 gradually. The batch size is set to 64 and
the dropout ratio is 0.5. The trade-off parameters α1 in Eq. (12) is set to 0.001 during training.

4.3 Experimental Analyses

In this section  we analyze three metrics  including identity consistency  distribution consistency and
visual quality  to demonstrate the effectiveness of DVG. The compared methods include CoGAN [25]
and VAE [21]. For VAE model  the input is the concatenated NIR-VIS images.

Identity Consistency.
In order to analyze the identity consistency  we measure the feature distance
between the generated paired images on the CASIA NIR-VIS 2.0 database. Speciﬁcally  we ﬁrst use
a pre-trained Light CNN-9 [34] to extract features and then measure the mean distance (MD) of the
paired images. The results are reported in Table 1a. MD is computed from 50K generated image pairs
and the MD value of the original database is 0.26. We can clearly see that the MD value of DVG
is even smaller than the original database  which means that our method can effectively guarantee
the identity consistency of the generated paired images. The recognition performance of different
methods is also reported in Table 1a. We can see that DVG correspondingly achieves the best results.

Distribution Consistency. On the CASIA NIR-VIS 2.0 database  we take Fr´echet Inception Dis-
tance (FID) [12] to measure the Fr´echet distance of two distributions in the feature space  reﬂecting
the distribution consistency. We ﬁrst measure the FID between the generated VIS images and the real
VIS images  and the FID between the generated NIR images and the real NIR images  respectively.

6

Figure 4: Visual comparisons of dual image generation results on the CASIA NIR-VIS 2.0 database.
The generated paired images of DVG are more similar than those of CoGAN and VAE.

Figure 5: The dual generation results on the Oulu-CASIA NIR-VIS  the BUAA-VisNir and the
CUHK Face Sketch FERET (CUFSF) databases.

Then we calculate the mean FID as the ﬁnal results  which are reported in Table 1a. Considering
that the face recognition network can better extract features of face images  we use a LightCNN-9 to
extract features for calculating FID instead of the traditional Inception model. Similarly  FID results
are computed from 50K generated image pairs. As shown in Table 1a  DVG achieves best results 
demonstrating that DVG has really learned the distributions of two modalities.

In Fig. 4  we compare the dual generation results (128 × 128 resolution) of different
Visual Quality.
methods on the CASIA NIR-VIS 2.0 database. Our visual results are obviously better than CoGAN
and VAE. Moreover  we can observe that the generated paired images of VAE and CoGAN are not
similar  which leads to worse Rank-1 accuracy during optimizing HFR network (see Table 1a). More
dual generation results of DVG are shown in Fig. 2 (256 × 256 resolution) and Fig. 5.

Ablation Study. Table 1b presents the comparison results of our DVG and its three variants on
the CASIA NIR-VIS 2.0 database. We observe that the recognition performance will decrease if
one component is not adopted. Particularly  the accuracy drops signiﬁcantly when the distribution
alignment loss Ldist or the pairwise identity preserving loss Lip-pair are not used. These results suggest
that every component is crucial in our model.
Moreover  we analyze how the number of generated samples inﬂuence the HFR network on the
Oulu-CASIA NIR-VIS database that only contains 20 identities with about 1 000 images for training.
We generate 1K  5K  10K and 50K pairs of heterogeneous images via DVG  and we obtain 68.7% 
85.9%  89.5% and 89.4% on VR@FAR=0.1% by LightCNN-9  respectively. The results have been
signiﬁcantly improved with the increasing number of the generated pairs  suggesting that DVG can
boost the performance of the low-shot heterogeneous face recognition.

4.4 Comparisons with State-of-the-art Methods

The recognition performance of our proposed DVG is demonstrated in this section on four heteroge-
neous face recognition databases. The performance of state-of-the-art methods  such as IDNet [29] 
HFR-CNN [30]  Hallucination [23]  DLFace [28]  TRIVET [26]  IDR [10]  W-CNN [11]  RCN [4] 
MC-CNN [3] and DVR [35] is compared in Table 2. In addition  LightCNN-9 and LightCNN-29 are
our baseline methods.

7

DVGCoGANVAEOuluBUAACUFSFFAR=0.1% Rank-1

FAR=1% FAR=0.1% Rank-1

Oulu-CASIA NIR-VIS

BUAA-VisNir
FAR=1% FAR=0.1% Rank-1

IIIT-D Viewed Sketch
FAR=1%

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

98.68

Method

IDNet [29]

74.5
78.0

-
-

DLFace [28]
TRIVET [26]

IDR [10]

W-CNN [11]

HFR-CNN [30]
Hallucination [23]

CASIA NIR-VIS 2.0
Rank-1
87.1 ± 0.9
85.9 ± 0.9
89.6 ± 0.9
95.7 ± 0.5
97.3 ± 0.4
98.7 ± 0.3
99.7 ± 0.1
99.3 ± 0.2
99.4 ± 0.1
97.1 ± 0.7
99.2 ± 0.3
LightCNN-9 + DVG
98.1 ± 0.4
LightCNN-29 + DVG 99.8 ± 0.1

91.0 ± 1.3
95.7 ± 0.7
98.4 ± 0.4
99.6 ± 0.3
98.7 ± 0.2
99.3 ± 0.1
93.7 ± 0.8
98.8 ± 0.3
97.4 ± 0.5
99.8 ± 0.1

MC-CNN [3]
LightCNN-9

DVR [35]
RCN [4]

LightCNN-29

92.2
94.3
98.0
100.0

-
-

93.8
100.0
99.0
100.0

67.9
73.4
81.5
97.2

-
-

80.4
97.6
93.1
98.5

33.6
46.2
54.6
84.9

-
-

43.8
89.5
68.3
92.9

93.9
94.3
97.4
99.2

-
-

94.8
98.0
96.8
99.3

93.0
93.4
96.0
98.5

-
-

94.3
97.1
97.0
98.5

80.9
84.7
91.9
96.9

-
-

83.5
93.1
89.4
97.3

-
-
-
-
-
-
-
-

90.34
87.40
84.07
86.65
83.24
96.99

-
-
-
-
-
-
-
-
-
-

75.30
92.24
81.04
97.86

Table 2: Comparisons with other state-of-the-art deep HFR methods on the CASIA NIR-VIS 2.0  the
Oulu-CASIA NIR-VIS  the BUAA-VisNir and the IIIT-D Viewed Sketch databases.

Figure 6: The ROC curves on the CASIA NIR-VIS 2.0  the Oulu-CASIA NIR-VIS and the BUAA-
VisNir databases  respectively

For the most challenging CASIA NIR-VIS 2.0 database  it is obvious that DVG outperforms other
state-of-the-art methods. We ﬁrst employ LightCNN-9 as the backbone to perform DVG  which
obtains 99.2% on Rank-1 accuracy and 98.8% on VR@FAR=0.1%. Further  when backbone
changed to more powerful LightCNN-29  DVG obtains 99.8% on Rank-1 accuracy and 99.8%
on VR@FAR=0.1%. Moreover  for BUAA-VisNir Face database  DVG obtains 99.3% on Rank-1
accuracy and 97.3% on VR@FAR=0.1%  which outperforms our baseline LightCNN-29 and other
state-of-the-art methods.
To further analyze the effectiveness of the proposed DVG for low-shot heterogeneous face recognition 
we evaluate DVG on the Oulu-CASIA NIR-VIS and the IIIT-D Viewed Sketch Face databases. As
mentioned in section 4.1  there are fewer identities or images in these two databases. Table 2 presents
the performance of DVG on these two challenging low-shot HFR databases. For the Oulu-CASIA
NIR-VIS database  we observe that DVG with LightCNN-29 signiﬁcantly boosts the performance
from 84.9% [35] to 92.9% on VR@FAR=0.1%. Besides  for the IIIT-D Viewed Sketch Face database 
DVG also obtains 96.99% on Rank-1 accuracy and 97.86% on VR@FAR=1%  which signiﬁcantly
outperform our baseline lightCNN-29 and state-of-the-art methods including RCN and MC-CNN by
a large margin.
Fig. 6 presents the ROC curves  including TRIVET  IDR  W-CNN  DVR  and the proposed DVG.
To better demonstrate the results  we only perform ROC curves of DVG trained on LightCNN-
29. It is obvious that DVG outperforms other state-of-the-art methods  especially on the low shot
heterogeneous databases such as the Oulu-CASIA NIR-VIS database.
Expect for the above commonly used NIR-VIS and Sketch-Photo  we further explore other potential
applications  including the face recognition under different resolutions on the NJU-ID database [17]
and different poses on the Multi-PIE database [8]. The NJU-ID database consists of 256 identities
with one ID card image (102 × 126 resolution) and one camera image (640 × 480 resolution) per
identity. Considering the few number of images in the NJU-ID database  we use our collected
ID-Photo database (1000 identities) as the training set and the NJU-ID database as the testing set.
The Multi-PIE database contains 337 subjects with different poses. We use proﬁles (±75o ±90o)

8

105104103102101100False Positive Rate0.8000.8250.8500.8750.9000.9250.9500.9751.000True Positive Rate(a) CASIA NIR-VIS 2.0 ROCDVGDVRTRIVETIDRWCNN105104103102101100False Positive Rate0.30.40.50.60.70.80.91.0True Positive Rate(b) Oulu-CASIA NIR-VIS ROCDVGDVRTRIVETIDRWCNN105104103102101100False Positive Rate0.8000.8250.8500.8750.9000.9250.9500.9751.000True Positive Rate(c) BUAA-VisNir ROCDVGDVRTRIVETIDRWCNNand frontal faces as different modalities. 200 persons are used as the training set and the rest 137
persons are the testing set (Setting 2 of [13]). On the NJU-ID database  we improve Rank-1 by 5.5%
(DVG 96.8% - Baseline 91.3%) and VR@FAR=1% by 6.2% (DVG 96.7% - Baseline 90.5%) over
the baseline LightCNN-29. On the Multi-PIE database  the Rank-1 of ±90o and ±75o is increased
by 18.5% (DVG 83.9% - Baseline 65.4%) and 4.3% (DVG 97.3% - Baseline 93.0%)  respectively.
We will continue to explore more applications in our future work.

5 Conclusion

This paper has developed a novel dual variational generation framework that generates large-scale
new paired heterogeneous images with abundant intra-class diversity from noise  providing a new
insight into the problems of HFR. A dual variational autoencoder is ﬁrst proposed to learn a joint
distribution of paired heterogeneous images. Then  both the distribution alignment in the latent space
and the pairwise distance constraint in the image space are utilized to ensure the identity consistency
of the generated image pairs. Finally  DVG generates diverse paired heterogeneous images with the
same identity from noise to boost HFR network. Extensive qualitative and quantitative experimental
results on four databases have shown the superiority of our method.

Acknowledgments

This work is funded by the National Natural Science Foundation of China (Grants No. 61622310)
and Beijing Natural Science Foundation (Grants No. JQ18017).

References
[1] Himanshu S Bhatt  Samarth Bharadwaj  Richa Singh  and Mayank Vatsa. Memetic approach for matching

sketches with digital face images. Technical report  2012.

[2] Jiankang Deng  Jia Guo  Xue Niannan  and Stefanos Zafeiriou. Arcface: Additive angular margin loss for

deep face recognition. In CVPR  2019.

[3] Zhongying Deng  Xiaojiang Peng  Zhifeng Li  and Yu Qiao. Mutual component convolutional neural

networks for heterogeneous face recognition. TIP  2019.

[4] Zhongying Deng  Xiaojiang Peng  and Yu Qiao. Residual compensation networks for heterogeneous face

recognition. In AAAI  2019.

[5] Chaoyou Fu  Yibo Hu  Xiang Wu  Guoli Wang  Qian Zhang  and Ran He. High ﬁdelity face manipulation

with extreme pose and expression. arXiv:1903.12003  2019.

[6] Ian J. Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair 

Aaron C. Courville  and Yoshua Bengio. Generative adversarial networks. In NeurIPS  2014.

[7] Debaditya Goswami  Chi-Ho Chan  David Windridge  and Josef Kittler. Evaluation of face recognition

system in heterogeneous environments (visible vs nir). In ICCV Workshops  2011.

[8] Ralph Gross  Iain Matthews  Jeffrey Cohn  Takeo Kanade  and Simon Baker. Multi-pie. Image and Vision

Computing  2010.

[9] Yandong Guo  Lei Zhang  Yuxiao Hu  Xiaodong He  and Jianfeng Gao. Ms-celeb-1m: A dataset and

benchmark for large-scale face recognition. In ECCV  2016.

[10] Ran He  Xiang Wu  Zhenan Sun  and Tieniu Tan. Learning invariant deep representation for NIR-VIS face

[11] Ran He  Xiang Wu  Zhenan Sun  and Tieniu Tan. Wasserstein CNN: learning invariant features for

recognition. In AAAI  2017.

NIR-VIS face recognition. TPAMI  2018.

[12] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  Günter Klambauer  and Sepp
Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. In NeurIPS 
2017.

[13] Yibo Hu  Xiang Wu  Bing Yu  Ran He  and Zhenan Sun. Pose-guided photorealistic face rotation. In

CVPR  2018.

2012.

[14] Di Huang  Jia Sun  and Yunhong Wang. The BUAA-VisNir face database instructions. Technical report 

[15] Huaibo Huang  Zhihang Li  Ran He  Zhenan Sun  and Tieniu Tan. Introvae: Introspective variational

autoencoders for photographic image synthesis. In NeurIPS  2018.

[16] Rui Huang  Shu Zhang  Tianyu Li  and Ran He. Beyond face rotation: Global and local perception gan for

photorealistic and identity preserving frontal view synthesis. In ICCV  2017.

[17] Jing Huo  Yang Gao  Yinghuan Shi  Wanqi Yang  and Hujun Yin. Heterogeneous face recognition by

margin-based cross-modality metric learning. IEEE Transactions on Cybernetics  2017.

[18] Jie Chen  D. Yi  Jimei Yang  Guoying Zhao  S. Z. Li  and M. Pietikainen. Learning mappings for face

synthesis from near infrared to visual light images. In CVPR  2009.

9

[19] Felix Juefei-Xu  Dipan K. Pal  and Marios Savvides. Nir-vis heterogeneous face recognition via cross-

spectral joint dictionary learning and reconstruction. In CVPR Workshops  2015.

[20] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive growing of gans for improved

quality  stability  and variation. In ICLR  2018.

[21] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR  2014.
[22] Brendan Klare  Zhifeng Li  and Anil K. Jain. Matching forensic sketches to mug shot photos. TPAMI 

2011.

2013.

[23] José Lezama  Qiang Qiu  and Guillermo Sapiro. Not afraid of the dark: Nir-vis face recognition via

cross-spectral hallucination and low-rank embedding. In CVPR  2017.

[24] Stan Z. Li  Dong Yi  Zhen Lei  and Shengcai Liao. The casia nir-vis 2.0 face database. In CVPR Workshops 

[25] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In NeurIPS  2016.
[26] Xiaoxiang Liu  Lingxiao Song  Xiang Wu  and Tieniu Tan. Transferring deep representation for nir-vis

heterogeneous face recognition. In ICB  2016.

[27] Qi Mao  Hsin-Ying Lee  Hung-Yu Tseng  Siwei Ma  and Ming-Hsuan Yang. Mode seeking generative

adversarial networks for diverse image synthesis. In CVPR  2019.

[28] Chunlei Peng  Nannan Wang  Jie Li  and Xinbo Gao. Dlface: Deep local descriptor for cross-modality face

recognition. PR  2019.

[29] Christopher Reale  Nasser M. Nasrabadi  Heesung Kwon  and Rama Chellappa. Seeing the forest from the

trees: A holistic approach to near-infrared heterogeneous face recognition. In CVPR Workshops  2016.

[30] Shreyas Saxena and Jakob Verbeek. Heterogeneous face recognition with cnns. In ECCV Workshops  2016.
[31] Zhixin Shu  Mihir Sahasrabudhe  Rıza Alp Güler  Dimitris Samaras  Nikos Paragios  and Iasonas Kokkinos.

Deforming autoencoders: Unsupervised disentangling of shape and appearance. In ECCV  2018.

[32] Lingxiao Song  Man Zhang  Xiang Wu  and Ran He. Adversarial discriminative heterogeneous face

recognition. In AAAI  2018.

recognition. In CVPR  2017.

TIFS  2018.

[33] Luan Tran  Xi Yin  and Xiaoming Liu. Disentangled representation learning gan for pose-invariant face

[34] Xiang Wu  Ran He  Zhenan Sun  and Tieniu Tan. A light cnn for deep face representation with noisy labels.

[35] Xiang Wu  Huaibo Huang  Vishal M. Patel  Ran He  and Zhenan Sun. Disentangled variational representa-

tion for heterogeneous face recognition. In AAAI  2019.

[36] He Zhang  Benjamin S. Riggan  Shuowen Hu  Nathaniel J. Short  and Vishal M. Patel. Synthesis of
high-quality visible faces from polarimetric thermal faces using generative adversarial networks. IJCV 
2019.

[37] Wei Zhang  Xiaogang Wang  and Xiaoou Tang. Coupled information-theoretic encoding for face photo-

sketch recognition. In CVPR  2011.

[38] Jian Zhao  Yu Cheng  Yan Xu  Lin Xiong  Jianshu Li  Fang Zhao  Karlekar Jayashree  Sugiri Pranata 
Shengmei Shen  Junliang Xing  Shuicheng Yan  and Jiashi Feng. Towards pose invariant face recognition
in the wild. In CVPR  2018.

[39] Jian Zhao  Lin Xiong  Yu Cheng  Yi Cheng  Jianshu Li  Li Zhou  Yan Xu  Jayashree Karlekar  Sugiri
Pranata  Shengmei Shen  Junliang Xing  Shuicheng Yan  and Jiashi Feng. 3d-aided deep pose-invariant
face recognition. In IJCAI  2018.

10

,Bowei Yan
Purnamrita Sarkar
Chaoyou Fu
Xiang Wu
Yibo Hu
Huaibo Huang
Ran He