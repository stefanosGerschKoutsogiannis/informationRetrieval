2017,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting,In reinforcement learning (RL)  one of the key components is policy evaluation  which aims to estimate the value function (i.e.  expected long-term accumulated reward) of a policy. With a good policy evaluation method  the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous \emph{Gradient-based Temporal Difference(GTD)} policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming  a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated  previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting  them into  convex-concave saddle point problems. However  it is well-known that  the data are generated from Markov processes rather than i.i.d in RL problems.. In this paper  in the realistic Markov setting  we derive the finite sample bounds for the general convex-concave saddle point problems  and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size  GTD algorithms converge. (2) The convergence rate is determined by the step size  with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix  the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process.  To the best of our knowledge  our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.,Finite Sample Analysis of the GTD Policy Evaluation

Algorithms in Markov Setting

Yue Wang ∗
School of Science

Beijing Jiaotong University
11271012@bjtu.edu.cn

Wei Chen

Microsoft Research
wche@microsoft.com

Yuting Liu

School of Science

Beijing Jiaotong University

ytliu@bjtu.edu.cn

Zhi-Ming Ma

Academy of Mathematics and Systems Science

Chinese Academy of Sciences

mazm@amt.ac.cn

Tie-Yan Liu

Microsoft Research

Tie-Yan.Liu@microsoft.com

Abstract

In reinforcement learning (RL)   one of the key components is policy evaluation 
which aims to estimate the value function (i.e.  expected long-term accumulated
reward) of a policy. With a good policy evaluation method  the RL algorithms
will estimate the value function more accurately and ﬁnd a better policy. When
the state space is large or continuous Gradient-based Temporal Difference(GTD)
policy evaluation algorithms with linear function approximation are widely used.
Considering that the collection of the evaluation data is both time and reward
consuming  a clear understanding of the ﬁnite sample performance of the policy
evaluation algorithms is very important to reinforcement learning. Under the
assumption that data are i.i.d. generated  previous work provided the ﬁnite sample
analysis of the GTD algorithms with constant step size by converting them into
convex-concave saddle point problems. However  it is well-known that  the data
are generated from Markov processes rather than i.i.d. in RL problems.. In this
paper  in the realistic Markov setting  we derive the ﬁnite sample bounds for the
general convex-concave saddle point problems  and hence for the GTD algorithms.
We have the following discussions based on our bounds. (1) With variants of step
size  GTD algorithms converge. (2) The convergence rate is determined by the
step size  with the mixing time of the Markov process as the coefﬁcient. The faster
the Markov processes mix  the faster the convergence. (3) We explain that the
experience replay trick is effective by improving the mixing property of the Markov
process. To the best of our knowledge  our analysis is the ﬁrst to provide ﬁnite
sample bounds for the GTD algorithms in Markov setting.

1

Introduction

Reinforcement Learning (RL) (Sutton and Barto [1998]) technologies are very powerful to learn how
to interact with environments  and has variants of important applications  such as robotics  computer
games and so on (Kober et al. [2013]  Mnih et al. [2015]  Silver et al. [2016]  Bahdanau et al. [2016]).
In RL problem  an agent observes the current state  takes an action following a policy at the current
state  receives a reward from the environment  and the environment transits to the next state in Markov 
and again repeats these steps. The goal of the RL algorithms is to ﬁnd the optimal policy which

∗This work was done when the ﬁrst author was visiting Microsoft Research Asia.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

leads to the maximum long-term reward. The value function of a ﬁxed policy for a state is deﬁned as
the expected long-term accumulated reward the agent would receive by following the ﬁxed policy
starting from this state. Policy evaluation aims to accurately estimate the value of all states under a
given policy  which is a key component in RL (Sutton and Barto [1998]  Dann et al. [2014]). A better
policy evaluation method will help us to better improve the current policy and ﬁnd the optimal policy.
When the state space is large or continuous  it is inefﬁcient to represent the value function over all the
states by a look-up table. A common approach is to extract features for states and use parameterized
function over the feature space to approximate the value function. In applications  there are linear
approximation and non-linear approximation (e.g. neural networks) to the value function. In this
paper  we will focus on the linear approximation (Sutton et al. [2009a] Sutton et al. [2009b] Liu et al.
[2015]). Leveraging the localization technique in Bhatnagar et al. [2009]  the results can be generated
into non-linear cases with extra efforts. We leave it as future work.
In policy evaluation with linear approximation  there were substantial work for the temporal-difference
(TD) method  which uses the Bellman equation to update the value function during the learning
process (Sutton [1988] Tsitsiklis et al. [1997]). Recently  Sutton et al. [2009a] Sutton et al. [2009b]
have proposed Gradient-based Temporal Difference (GTD) algorithms which use gradient information
of the error from the Bellman equation to update the value function. It is shown that  GTD algorithms
have achieved the lower-bound of the storage and computation complexity  making them powerful
to handle high dimensional big data. Therefore  now GTD algorithms are widely used in policy
evaluation problems and the policy evaluation step in practical RL algorithms (Bhatnagar et al.
[2009] Silver et al. [2014]).
However  we don’t have sufﬁcient theory to tell us about the ﬁnite sample performance of the GTD
algorithms. To be speciﬁc  will the evaluation process converge with the increasing of the number
of the samples? If yes  how many samples we need to get a target evaluation error? Will the step
size in GTD algorithms inﬂuence the ﬁnite sample error? How to explain the effectiveness of the
practical tricks  such as experience replay? Considering that the collection of the evaluation data is
very likely to be both time and reward consuming  to get a clear understanding of the ﬁnite sample
performance of the GTD algorithms is very important to the efﬁciency of policy evaluation and the
entire RL algorithms.
Previous work (Liu et al. [2015]) converted the objective function of GTD algorithms into a convex-
concave saddle problem and conducted the ﬁnite sample analysis for GTD with constant step size
under the assumption that data are i.i.d. generated. However  in RL problem  the date are generated
from an agent who interacts with the environment step by step  and the state will transit in Markov as
introduced previously. As a result  the data are generated from a Markov process but not i.i.d.. In
addition  the work did not study the decreasing step size  which are also commonly-used in many
gradient based algorithms(Sutton et al. [2009a] Sutton et al. [2009b] Yu [2015]). Thus  the results
from previous work cannot provide satisfactory answers to the above questions for the ﬁnite sample
performance of the GTD algorithms.
In this paper  we perform the ﬁnite sample analysis for the GTD algorithms in the more realistic
Markov setting. To achieve the goal  ﬁrst of all  same with Liu et al. [2015]  we consider the stochastic
gradient descent algorithms of the general convex-concave saddle point problems  which include
the GTD algorithms. The optimality of the solution is measured by the primal-dual gap (Liu et al.
[2015]  Nemirovski et al. [2009]). The ﬁnite sample analysis for convex-concave optimization in
Markov setting is challenging. On one hand  in Markov setting  the non-i.i.d. sampled gradients
are no longer unbiased estimation of the gradients. Thus  the proof technique for the convergence
of convex-concave problem in i.i.d. setting cannot be applied. On the other hand  although SGD
converge for convex optimization problem with the Markov gradients  it is much more difﬁcult to
obtain the same results in the more complex convex-concave optimization problem.
To overcome the challenge  we design a novel decomposition of the error function (i.e. Eqn (3.1)).
The intuition of the decomposition and key techniques are as follows: (1) Although samples are not
i.i.d.  for large enough τ  the sample at time t + τ is "nearly independent" of the sample at time t 
and its distribution is "very close" to the stationary distribution. (2) We split the random variables in
the objective related to E operator and the variables related to max operator into different terms in
order to control them respectively. It is non-trivial  and we construct a sequence of auxiliary random
variables to do so. (3) All constructions above need to be carefully considered the measurable issues

2

in the Markov setting. (4) We construct new martingale difference sequences and apply Azuma’s
inequality to derive the high-probability bound from the in-expectation bound.
By using the above techniques  we prove a novel ﬁnite sample bound for the convex-concave
saddle point problem. Considering the GTD algorithms are speciﬁc convex-concave saddle point
optimization methods  we ﬁnally obtained the ﬁnite sample bounds for the GTD algorithms  in the
realistic Markov setting for RL. To the best of our knowledge  our analysis is the ﬁrst to provide ﬁnite
sample bounds for the GTD algorithms in Markov setting.
We have the following discussions based on our ﬁnite sample bounds.

1. GTD algorithms do converge  under a ﬂexible condition on the step size  i.e.(cid:80)T

t=1 αt →
< ∞  as T → ∞  where αt is the step size. Most of step sizes used in practice

(cid:80)T
(cid:80)T

∞ 
satisfy this condition.

t=1 α2
t
t=1 αt

(cid:32)(cid:114)

2. The convergence rate is O

(1 + τ (η))

(cid:113)

(cid:80)T
(cid:80)T

t=1 α2
t
t=1 αt

+

τ (η) log( τ (η)

t=1 α2
t

δ )(cid:80)T

t=1 αt

(cid:80)T

(cid:33)

  where τ (η) is

the mixing time of the Markov process  and η is a constant. Different step sizes will lead to
different convergence rates.

3. The experience replay trick is effective  since it can improve the mixing property of the

Markov process.

Finally  we conduct simulation experiments to verify our theoretical ﬁnding. All the conclusions
from the analysis are consistent with our empirical observations.

2 Preliminaries

In this section  we brieﬂy introduce the GTD algorithms and related works.

2.1 Gradient-based TD algorithms
Consider the reinforcement learning problem with Markov decision process(MDP) (S A  P  R  γ) 
where S is the state space  A is the action space  P = {P a
s s(cid:48); s  s(cid:48) ∈ S  a ∈ A} is the transition
s s(cid:48) is the transition probability from state s to state s(cid:48) after taking action a  R =
matrix and P a
{R(s  a); s ∈ S  a ∈ A is the reward function and R(s  a) is the reward received at state s if
taking action a  and 0 < γ < 1 is the discount factor. A policy function µ : A × S → [0  1]
indicates the probability to take each action at each state. Value function for policy µ is deﬁned as:

V µ(s) (cid:44) E [(cid:80)∞

t=0 γtR(st  at)|s0 = s  µ].

In order to perform policy evaluation in a large state space  states are represented by a feature vector
φ(s) ∈ Rd  and a linear function ˆv(s) = φ(s)(cid:62)θ is used to approximate the value function. The
evaluation error is deﬁned as (cid:107)V (s) − ˆv(s)(cid:107)s∼π  which can be decomposed into approximation
error and estimation error. In this paper  we will focus on the estimation error with linear function
approximation.
As we know  the value function in RL satisﬁes the following Bellman equation: V µ(s) =
Eµ P [R(st  at) + γV µ(st+1)|st = s] (cid:44) T µV µ(s)  where T µ is called Bellman operator for policy
µ. Gradient-based TD (GTD) algorithms (including GTD and GTD2) proposed by Sutton et al.
[2009a] and Sutton et al. [2009b] update the approximated value function by minimizing the objective
function related to Bellman equation errors  i.e.  the norm of the expected TD update (NEU) and
mean-square projected Bellman error (MSPBE) respectively(Maei [2011] Liu et al. [2015])  

GT D :

GT D2 :

JN EU (θ) = (cid:107)Φ(cid:62)K(T µˆv − ˆv)(cid:107)2
JM SP BE(θ) = (cid:107)ˆv − PT µˆv(cid:107) = (cid:107)Φ(cid:62)K(T µˆv − ˆv)(cid:107)2

where K is a diagonal matrix whose elements are π(s)  C = Eπ(φiφ(cid:62)
the state space S.
Actually  the two objective functions in GTD and GTD2 can be uniﬁed as below

(2.1)
(2.2)
i )  and π is a distribution over

C−1

J(θ) = (cid:107)b − Aθ(cid:107)2

M−1  

(2.3)

3

Algorithm 1 GTD Algorithms
1: for t = 1  . . .   T do
2:
(cid:80)T
3: end for
(cid:80)T
Output:

Update parameters:

(cid:16)
yt + αt(ˆbt − ˆAtθt − ˆMtyt)
yt+1 = PXy
(cid:80)T
(cid:80)T

˜xT =

˜yT =

(cid:17)

t=1 αtxt
t=1 αt

t=1 αtyt
t=1 αt

(cid:16)

xt+1 = PXx

xt + αt ˆA(cid:62)

t yt

(cid:17)

where M = I in GTD  M = C  in GTD2  A = Eπ[ρ(s  a)φ(s)(φ(s) − γφ(s(cid:48)))(cid:62)]  b =
Eπ[ρ(s  a)φ(s)r]  ρ(s  a) = µ(a|s)/µb(a|s) is the importance weighting factor. Since the underlying
distribution is unknown  we use the data D = {ξi = (si  ai  ri  s(cid:48)
i=1 to estimate the value function
by minimizing the empirical estimation error  i.e. 

i)}n

ˆJ(θ) = 1/T

(cid:107)ˆb − ˆAθ(cid:107)2

ˆM−1

T(cid:88)

i=1

where ˆAi = ρ(si  ai)φ(si)(φ(si) − γφ(s(cid:48)
Liu et al. [2015] derived that the GTD algorithms to minimize (2.3) is equivalent to the stochastic
(cid:18)
gradient algorithms to solve the following convex-concave saddle point problem

i))(cid:62)  ˆbi = ρ(si  ai)φ(si)ri  ˆCi = φ(si)φ(si)(cid:62).

(cid:19)

min

x

max

y

L(x  y) = (cid:104)b − Ax  y(cid:105) − 1
2

(cid:107)y(cid:107)2

M

 

(2.4)

with x as the parameter θ in the value function  y as the auxiliary variable used in GTD algorithms.
Therefore  we consider the general convex-concave stochastic saddle point problem as below

{φ(x  y) = Eξ[Φ(x  y  ξ)]} 

max
y∈Xy

min
x∈Xx

(2.5)
where Xx ⊂ Rn and Xy ⊂ Rm are bounded closed convex sets  ξ ∈ Ξ is random variable and its
distribution is Π(ξ)  and the expected function φ(x  y) is convex in x and concave in y. Denote
z = (x  y) ∈ Xx × Xy (cid:44) X   the gradient of φ(z) as g(z)  and the gradient of Φ(z  ξ) as G(z  ξ).
In the stochastic gradient algorithm  the model is updated as: zt+1 = PX (zt − αt(G(zt  ξt))) 
where PX is the projection onto X and αt is the step size. After T iterations  we get the model
˜zT
1 =

1 is measured by the primal-dual gap error

. The error of the model ˜zT

(cid:80)T
(cid:80)T

t=1 αtzt
t=1 αt

Errφ(˜zT

1 ) = max
y∈Xy

φ(˜xT

1   y) − min
x∈Xx

φ(x  ˜yT

1 ).

(2.6)

Liu et al. [2015] proved that the estimation error of the GTD algorithms can be upper bounded by
their corresponding primal-dual gap error multiply a factor. Therefore  we are going to derive the
ﬁnite sample primal-dual gap error bound for the convex-concave saddle point problem ﬁrstly  and
then extend it to the ﬁnite sample estimation error bound for the GTD algorithms.
Details of GTD algorithms used to optimize (2.4) are placed in Algorithm 1( Liu et al. [2015]).

2.2 Related work

The TD algorithms for policy evaluation can be divided into two categories: gradient based methods
and least-square(LS) based methods(Dann et al. [2014]). Since LS based algorithms need O(d2)
storage and computational complexity while GTD algorithms are both of O(d) complexity  gradient
based algorithms are more commonly used when the feature dimension is large. Thus  in this paper 
we focus on GTD algorithms.
Sutton et al. [2009a] proposed the gradient-based temporal difference (GTD) algorithm for off-policy
policy evaluation problem with linear function approximation. Sutton et al. [2009b] proposed GTD2
algorithm which shows a faster convergence in practice. Liu et al. [2015] connected GTD algorithms
to a convex-concave saddle point problem and derive a ﬁnite sample bound in both on-policy and
off-policy cases for constant step size in i.i.d. setting.
In the realistic Markov setting  although the ﬁnite sample bounds for LS-based algorithms have been
proved (Lazaric et al. [2012] Tagorti and Scherrer [2015]) LSTD(λ)  to the best of our knowledge 
there is no previous ﬁnite sample analysis work for GTD algorithms.

4

3 Main Theorems

In this section  we will present our main results. In Theorem 1  we present our ﬁnite sample bound
for the general convex-concave saddle point problem; in Theorem 2  we provide the ﬁnite sample
bounds for GTD algorithms in both on-policy and off-policy cases. Please refer the complete proofs
in the supplementary materials.
Our results are derived based on the following common assumptions(Nemirovski [2004]  Duchi et al.
[2012]  Liu et al. [2015]). Please note that  the bounded-data property in assumption 4 in RL can
guarantee the Lipschitz and smooth properties in assumption 5-6 (Please see Propsition 1 ).
Assumption 1 (Bounded parameter). There exists D > 0  such that (cid:107)z − z(cid:48)(cid:107) ≤ D  f or ∀z  z(cid:48) ∈ X .
Assumption 2 (Step size). The step size αt is non-increasing.
Assumption 3 (Problem solvable). The matrix A and C in Problem 2.4 are non-singular.
Assumption 4 (Bounded data). Features are bounded by L  rewards are bounded by Rmax and
importance weights are bounded by ρmax.
Assumption 5 (Lipschitz). For Π-almost every ξ  the function Φ(x  y  ξ) is Lipschitz for both x and
y  with ﬁnite constant L1x  L1y  respectively. We Denote L1 (cid:44) √
1y.
for both x and y with ﬁnite constant L2x  L2y respectively. We denote L2 (cid:44) √

Assumption 6 (Smooth). For Π-almost every ξ  the partial gradient function of Φ(x  y  ξ) is Lipschitz

1x + L2

(cid:113)

(cid:113)

L2

L2

2

2

2x + L2

2y.

the initial

[s](A) and the corresponding probability density as pt

(cid:110)
∆ : t ∈ N (cid:82) |pt+∆

t sample Ft = σ(ξ1  . . .   ξt) is deﬁned as:

For Markov process  the mixing time characterizes how fast the process converge to its stationary
distribution. Following the notation of Duchi et al. [2012]  we denote the conditional probability
distribution P (ξt ∈ A|Fs) as P t
[s]. Similarly  we
denote the stationary distribution of the data generating stochastic process as Π and its density as π.
Deﬁnition 1. The mixing time τ (P[t]  η) of
the sampling distribution P conditioned on
the σ−ﬁeld of
τ (P[t]  η) (cid:44)
is the conditional probability density
inf
at time t + ∆  given Ft.
Assumption 7 (Mixing time). The mixing times of the stochastic process {ξt} are uniform. i.e.  there
exists uniform mixing times τ (P  η) ≤ ∞ such that  with probability 1  we have τ (P[s]  η) ≤ τ (P  η)
for all η > 0 and s ∈ N.
Please note that  any time-homogeneous Markov chain with ﬁnite state-space and any uniformly
ergodic Markov chains with general state space satisfy the above assumption(Meyn and Tweedie
[2012]). For simplicity and without of confusion  we will denote τ (P  η) as τ (η).

(ξ) − π(ξ)|d(ξ) ≤ η

  where pt+∆

(cid:111)

[t]

[t]

3.1 Finite Sample Bound for Convex-concave Saddle Point Problem
Theorem 1. Consider the convex-concave problem in Eqn (2.5). Suppose Assumption 1 2 5 6 hold.
Then for the gradient algorithm optimizing the convex-concave saddle point problem in (2.5)  for
∀δ > 0 and ∀η > 0 such that τ (η) ≤ T /2  with probability at least 1 − δ  we have

Errφ(˜zT

1 ) ≤ 1

A + B

(cid:34)

T(cid:80)

t=1

T(cid:88)

T(cid:88)

αt

t=1

+ 8DL1

α2

t + Cτ (η)

(cid:118)(cid:117)(cid:117)(cid:116)2τ (η) log

α2

t + F η

t=1

τ (η)

δ

(cid:32) T(cid:88)

t=1

T(cid:88)

t=1

αt + Hτ (η)

(cid:33)(cid:35)

α2

t + τ (η)α0

where : A = D2

B =

5
2

L2
1

C = 6L2

1 + 2L1L2D

F = 2L1D

H = 6L1Dα0

Proof Sketch of Theorem 1. By the deﬁnition of the error function in (2.6) and the property that
φ(x  y) is convex for x and concave for y  the expected error can be bounded as below

Errφ(˜zT

1 ) ≤ max

z

(zt − z)

(cid:62)

αt

g(zt)

.

(cid:105)

T(cid:88)

(cid:104)

1(cid:80)T

t=1 αt

t=1

5

(cid:104)

T(cid:88)

t=1

z

{vt}t≥1 which is measurable with respect to Ft−1 vt+1 = PX(cid:0)vt − αt(g(zt)− G(zt  ξt))(cid:1). We have

(cid:44) G(zt  ξt+τ )−G(zt  ξt). Constructing

Denote δt (cid:44) g(zt)−G(zt  ξt)  δ(cid:48)

(cid:44) g(zt)−G(zt  ξt+τ )  δ(cid:48)(cid:48)

the following key decomposition to the right hand side in the above inequality  the initiation and the
explanation for such decomposition is placed in supplementary materials. For ∀τ ≥ 0:

t

t

(cid:105)

(cid:20)

z

(cid:34)T−τ(cid:88)
(cid:123)(cid:122)

t=1

(c)

(d)

(a)

(cid:62)

.

(cid:62)

+

(b)

αt

αt

(cid:62)

δt

αt

(cid:48)
δ
t

(cid:125)

(cid:125)

(cid:124)

(cid:48)(cid:48)
δ
t

(cid:104)

max

(cid:21)

(cid:35)

g(zt)

g(zt)

(3.1)

= max

(cid:105)
(cid:125)

(cid:123)(cid:122)

(cid:123)(cid:122)

G(zt  ξt)

t=T−τ +1

(cid:125)
(cid:124)

(zt − z)

(zt − z)

(zt − z)
(cid:62)

+ (vt − z)

(cid:124)
T(cid:88)

+ (zt − vt)
(cid:62)

+ (zt − vt)
(cid:62)

(cid:125)
(cid:123)(cid:122)

For term(a)  we split G(zt  ξt) into three terms by the deﬁnition of L2-norm and the iteration formula

(cid:124)
(cid:123)(cid:122)
(cid:124)
(cid:0)(cid:107)αtG(zt  ξt)(cid:107)2 + (cid:107)zt − z(cid:107)2 − (cid:107)zt+1 − z(cid:107)2(cid:1).
of zt  and then we bound its summation by(cid:80)T−τ
terms. Swap the max and(cid:80) operators and use the Lipschitz Assumption 5  the ﬁrst term can be

Actually  in the summation  the last two terms will be eliminated except for their ﬁrst and the last
bounded. Term (c) includes the sum of G(zt  ξt+τ ) − G(zt  ξt)  which is might be large in Markov
setting. We reformulate it into the sum of G(zt−τ   ξt) − G(zt  ξt) and use the smooth Assumption 6
to bound it. Term (d) is similar to term (a) except that g(zt) − G(zt  ξt) is the gradient that used to
update vt. We can bound it similarly with term (a). Term(e) is a constant that does not change much
with T → ∞  and we can bound it directly through upper bound of each of its own terms. Finally  we
combine all the upper bounds to each term  use the mixing time Assumption 7 to choose τ = τ (η)
and obtain the error bound in Theorem 1.
We decompose Term(b) into a martingale part and an expectation part.By constructing a martingale
difference sequence and using the Azuma’s inequality together with the Assumption 7  we can bound
Term (b) and ﬁnally obtain the high probability error bound.

t=1

(e)

(cid:105)

t=1 α2
t
t=1 αt

(cid:104)
A + B(cid:80)T

Remark: (1) With T → ∞  the error bound approaches 0 in order O(
). (2) The mixing
time τ (η) will inﬂuence the convergence rate. If the Markov process has better mixing property
with smaller τ (η)  the algorithm converge faster. (3) If the data are i.i.d. generated (the mixing time
τ (η) = 0 ∀η) and the step size is set to the constant
1 ) ≤
1(cid:80)T
)  which is identical to previous work with constant step size in
i.i.d. setting (Liu et al. [2015] Nemirovski et al. [2009]). (4) The high probability bound is similar to
the expectation bound in the following Lemma 1 except for the last term. This is because we consider
the deviation of the data around its expectation to derive the high probability bound.
Lemma 1. Consider the convex-concave problem (2.5)  under the same as Theorem 1  we have
 ∀η > 0 

  our bound will reduce to Errφ(˜zT

= O( L1√

ED[Errφ(˜zT

T(cid:88)

T(cid:88)

T(cid:88)

αt + Hτ (η)

t=1 α2
t

A + B

t + Cτ (η)

1 )] ≤

t + F η

t=1 αt

(cid:35)

(cid:34)

α2

α2

√
c

L1

T

T

1(cid:80)T

t=1 αt

t=1

t=1

t=1

(cid:80)T
(cid:80)T

Proof Sketch of Lemma 1. We start from the key decomposition (3.1)  and bound each term with
expectation this time. We can easily bound each term as previously except for Term (b). For term
(b)  since (zt − vt) is not related to max operator and it is measurable with respect to Ft−1  we can
bound Term (b) through the deﬁnition of mixing time and ﬁnally obtain the expectation bound.

3.2 Finite Sample Bounds for GTD Algorithms

As a speciﬁc convex-concave saddle point problem  the error bounds in Theorem 1&2 can also
provide the error bounds for GTD with the following speciﬁcations for the Lipschitz constants.
Proposition 1. Suppose Assumption 1-4 hold  then the objective function in GTD algorithms is
Lipschitz and smooth with the following coefﬁcients:

√
√

L1 ≤
L2 ≤

2(2D(1 + γ)ρmaxL2d + ρmaxLRmax + λM )
2(2(1 + γ)ρmaxL2d + λM )

6

L

O

√

(cid:18)

L4d2λM πmax

L4d3λM πmax(1+τ (η))πmaxo1(T )

In on-policy case 

(cid:19)
then we have the following ﬁnite sam-
1 (cid:107)π in GTD algorithms:
the
(cid:33)(cid:33)
and with probability 1 − δ is

where λM is the largest singular value of M.
Theorem 2. Suppose assumptions 1-4 hold 
ple bounds for the error (cid:107)V − ˜vT
(cid:32)√
bound in expectation is O
(cid:32)√
O
  where νC  ν(AT M−1A) is
the smallest eigenvalue of the C and AT M−1A respectively  λC is the largest singular value
of C  o1(T ) = (

the
o2(T )
(cid:33)(cid:33)
and with probability 1 − δ is

(cid:32)(cid:115)
(cid:32)(cid:114)
(cid:80)T
(cid:80)T

bound in expectation is O

In off-policy case 

L4d2(1 + τ (η))o1(T ) +

(1 + τ (η))L2do1(T ) +

2λC λM πmax
(AT M−1A)
ν

(cid:18) L2d

√(cid:80)T
(cid:80)T

(cid:16) τ (η)

2λC λM πmax(1+τ (η))o1(T )

τ (η) log ( τ (η)

)  o2(T ) = (

(cid:17)
(cid:19)

(AT M−1A)
ν

δ )o2(T )

(cid:114)

(cid:113)

τ (η) log

√

νC

νC

δ

;

t=1 α2
t
t=1 αt

t=1 α2
t
t=1 αt

).

t

t

.

(cid:33)

(cid:113)

t=1 α2

t=1 α2

δ )o2(T )

t=1 α2
t
t=1 αt

(cid:80)T
(cid:80)T

τ (η) log( τ (η)

(1 + τ (η))o1(T ) +

)  αt = O( 1

t < 1  o2(T ) dominates.

)  the convergence rate is O( ln(T )√

t=1 αt → ∞ 
t )  αt = c = O( 1√

the bound in expectation is O(cid:16)(cid:112)(1 + τ (η))o1(T )
(cid:17)
We would like to make the following discussions for Theorem 2.
The GTD algorithms do converge in the realistic Markov setting.
As in Theorem
(cid:32)(cid:114)
and with probability 1 − δ is
2 
If the step size αt makes o1(T ) → 0 and
O
bound  if(cid:80)T
t > 1  then o1(T ) dominates the order  if(cid:80)T
o2(T ) → 0  as T → ∞  the GTD algorithms will converge. Additionally  in high probability
to 0 if the step size satisﬁes(cid:80)T

The setup of the step size can be ﬂexible. Our ﬁnite sample bounds for GTD algorithms converge
< ∞  as T → ∞. This condition on step size
is much weaker than the constant step size in previous work Liu et al. [2015]  and the common-used
step size αt = O( 1√
) all satisfy the condition. To be speciﬁc  for
αt = O( 1√
ln(T ) ) 
for the constant step size  the optimal setup is αt = O( 1√
) considering the trade off between o1(T )
and o2(T )  and the convergence rate is O( 1√
The mixing time matters.
If the data are generated from a Markov process with smaller mixing
time  the error bound will be smaller  and we just need fewer samples to achieve a ﬁxed estimation
error. This ﬁnding can explain why the experience replay trick (Lin [1993]) works. With experience
replay  we store the agent’s experiences (or data samples) at each step  and randomly sample one from
the pool of stored samples to update the policy function. By Theorem 1.19 - 1.23 of Durrett [2016]  it
can be proved that  for arbitrary η > 0  there exists t0  such that ∀t > t0 maxi | Nt(i)
t − π(i)| ≤ η.
That is to say  when the size of the stored samples is larger than t0  the mixing time of the new data
process with experience replay is 0. Thus  the experience replay trick improves the mixing property
of the data process  and hence improves the convergence rate.
Other factors that inﬂuence the ﬁnite sample bound: (1) With the increasing of the feature norm
L  the ﬁnite sample bound increase. This is consistent with the empirical ﬁnding by Dann et al.
[2014] that the normalization of features is crucial for the estimation quality of GTD algorithms. (2)
With the increasing of the feature dimension d  the bound increase. Intuitively  we need more samples
for a linear approximation in a higher dimension feature space.

t )  the convergence rate is O(

); for αt = O( 1

).

T

T

T

T

1

4 Experiments

In this section  we report our simulation results to validate our theoretical ﬁndings. We consider the
general convex-concave saddle problem 

min

x

max

y

L(x  y) = (cid:104)b − Ax  y(cid:105) +

1
2

(cid:107)x(cid:107)2 − 1
2

(cid:107)y(cid:107)2

(4.1)

(cid:18)

7

(cid:19)

t

t ) = 0.03

t

) = 0.015√
t

and αt = O( 1

where A is a n× n matrix  b is a n× 1 vector  Here we set n = 10. We conduct three experiment and
set the step size to αt = c = 0.001  αt = O( 1√
respectively. In
each experiment we sample the data ˆA  ˆb three ways: sample from two Markov chains with different
mixing time but share the same stationary distribution or sample from stationary distribution i.i.d.
directly. We sample ˆA and ˆb from Markov chain by using MCMC Metropolis-Hastings algorithms.
Speciﬁcally  notice that the mixing time of a Markov chain is positive correlation with the second
largest eigenvalue of its transition probability matrix (Levin et al. [2009])  we ﬁrstly conduct two
transition probability matrix with different second largest eigenvalues( both with 1001 state and the
second largest eigenvalue are 0.634 and 0.31 respectively)  then using Metropolis-Hastings algorithms
construct two Markov chain with same stationary distribution.
We run the gradient algorithm for the objective in (4.1) based on the simulation data  without and
with experience replay trick. The primal-dual gap error curves are plotted in Figure 1.
We have the following observations. (1) The error curves converge in Markov setting with all the
three setups of the step size. (2) The error curves with the data generated from the process which
has small mixing time converge faster. The error curve for i.i.d. generated data converge fastest. (3)
The error curve for different step size convergence at different rate. (4) With experience replay trick 
the error curves in the Markov settings converge faster than previously. All these observations are
consistent with our theoretical ﬁndings.

(a) αt = c

(b) αt = O( 1√

t

)

(c) αt = O( 1
t )

(d) αt = c with trick

(e) αt = O( 1√

t

) with trick

(f) αt = O( 1

t ) with trick

Figure 1: Experimental Results

5 Conclusion

In this paper  in the more realistic Markov setting  we proved the ﬁnite sample bound for the convex-
concave saddle problems with high probability and in expectation. Then  we obtain the ﬁnite sample
bound for GTD algorithms both in on-policy and off-policy  considering that the GTD algorithms
are speciﬁc convex-concave saddle point problems. Our ﬁnite sample bounds provide important
theoretical guarantee to the GTD algorithms  and also insights to improve them  including how to
setup the step size and we need to improve the mixing property of the data like experience replay.
In the future  we will study the ﬁnite sample bounds for policy evaluation with nonlinear function
approximation.

Acknowledgment

This work was supported by A Foundation for the Author of National Excellent Doctoral Dissertation
of RP China (FANEDD 201312) and National Center for Mathematics and Interdisciplinary Sciences
of CAS.

8

References
Dzmitry Bahdanau  Philemon Brakel  Kelvin Xu  Anirudh Goyal  Ryan Lowe  Joelle Pineau  Aaron
Courville  and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086  2016.

Shalabh Bhatnagar  Doina Precup  David Silver  Richard S Sutton  Hamid R Maei  and Csaba
Szepesvári. Convergent temporal-difference learning with arbitrary smooth function approximation.
In Advances in Neural Information Processing Systems  pages 1204–1212  2009.

Christoph Dann  Gerhard Neumann  and Jan Peters. Policy evaluation with temporal differences: a

survey and comparison. Journal of Machine Learning Research  15(1):809–883  2014.

John C Duchi  Alekh Agarwal  Mikael Johansson  and Michael I Jordan. Ergodic mirror descent.

SIAM Journal on Optimization  22(4):1549–1578  2012.

Richard Durrett. Poisson processes. In Essentials of Stochastic Processes  pages 95–124. Springer 

2016.

Jens Kober  J Andrew Bagnell  and Jan Peters. Reinforcement learning in robotics: A survey. The

International Journal of Robotics Research  32(11):1238–1274  2013.

Alessandro Lazaric  Mohammad Ghavamzadeh  and Rémi Munos. Finite-sample analysis of least-

squares policy iteration. Journal of Machine Learning Research  13(1):3041–3074  2012.

David Asher Levin  Yuval Peres  and Elizabeth Lee Wilmer. Markov chains and mixing times.

American Mathematical Soc.  2009.

Long-Ji Lin. Reinforcement learning for robots using neural networks. PhD thesis  Fujitsu Laborato-

ries Ltd  1993.

Bo Liu  Ji Liu  Mohammad Ghavamzadeh  Sridhar Mahadevan  and Marek Petrik. Finite-sample

analysis of proximal gradient td algorithms. In UAI  pages 504–513. Citeseer  2015.

Hamid Reza Maei. Gradient temporal-difference learning algorithms. PhD thesis  University of

Alberta  2011.

Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Springer Science &

Business Media  2012.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G Bellemare 
Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al. Human-level control
through deep reinforcement learning. Nature  518(7540):529–533  2015.

Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM
Journal on Optimization  15(1):229–251  2004.

Arkadi Nemirovski  Anatoli Juditsky  Guanghui Lan  and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization  19(4):1574–
1609  2009.

David Silver  Guy Lever  Nicolas Heess  Thomas Degris  Daan Wierstra  and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
Machine Learning  pages 387–395  2014.

David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driessche 
Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al. Mastering
the game of go with deep neural networks and tree search. Nature  529(7587):484–489  2016.

Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning  3

(1):9–44  1988.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press

Cambridge  1998.

9

Richard S Sutton  Hamid R Maei  and Csaba Szepesvári. A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in Neural
Information Processing Systems  pages 1609–1616  2009a.

Richard S Sutton  Hamid Reza Maei  Doina Precup  Shalabh Bhatnagar  David Silver  Csaba
Szepesvári  and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learn-
ing with linear function approximation. In Proceedings of the 26th International Conference on
Machine Learning  pages 993–1000  2009b.

Manel Tagorti and Bruno Scherrer. On the rate of convergence and error bounds for lstd ( lambda).
In Proceedings of the 32nd International Conference on Machine Learning  pages 1521–1529 
2015.

John N Tsitsiklis  Benjamin Van Roy  et al. An analysis of temporal-difference learning with function

approximation. IEEE transactions on automatic control  42(5):674–690  1997.

H Yu. On convergence of emphatic temporal-difference learning.

Conference on Learning Theory  pages 1724–1751  2015.

In Proceedings of The 28th

10

,Yue Wang
Wei Chen
Yuting Liu
Tie-Yan Liu