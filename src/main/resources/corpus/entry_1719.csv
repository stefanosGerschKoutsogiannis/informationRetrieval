2011,Efficient Methods for Overlapping Group Lasso,The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation  where groups of features are given  potentially with overlaps between the groups. The resulting optimization is  however  much more challenging to solve due to the group overlaps. In this paper  we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso  and compute the proximal operator by solving the smooth and convex dual problem  which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set  which consists of 8 141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms.,Efﬁcient Methods for Overlapping Group Lasso

Lei Yuan

Arizona State University

Tempe  AZ  85287

Lei.Yuan@asu.edu

Jun Liu

Arizona State University

Tempe  AZ  85287
j.liu@asu.edu

Jieping Ye

Arizona State University

Tempe  AZ  85287

jieping.ye@asu.edu

Abstract

The group Lasso is an extension of the Lasso for feature selection on (predeﬁned)
non-overlapping groups of features. The non-overlapping group structure limits
its applicability in practice. There have been several recent attempts to study a
more general formulation  where groups of features are given  potentially with
overlaps between the groups. The resulting optimization is  however  much more
challenging to solve due to the group overlaps. In this paper  we consider the efﬁ-
cient optimization of the overlapping group Lasso penalized problem. We reveal
several key properties of the proximal operator associated with the overlapping
group Lasso  and compute the proximal operator by solving the smooth and con-
vex dual problem  which allows the use of the gradient descent type of algorithms
for the optimization. We have performed empirical evaluations using both syn-
thetic and the breast cancer gene expression data set  which consists of 8 141
genes organized into (overlapping) gene sets. Experimental results show that the
proposed algorithm is more efﬁcient than existing state-of-the-art algorithms.

Introduction

1
Problems with high dimensionality have become common over the recent years. The high dimen-
sionality poses signiﬁcant challenges in building interpretable models with high prediction accuracy.
Regularization has been commonly employed to obtain more stable and interpretable models. A
well-known example is the penalization of the ℓ1 norm of the estimator  known as Lasso [25]. The
ℓ1 norm regularization has achieved great success in many applications. However  in some appli-
cations [28]  we are interested in ﬁnding important explanatory factors in predicting the response
variable  where each explanatory factor is represented by a group of input features. In such cases 
the selection of important features corresponds to the selection of groups of features. As an exten-
sion of Lasso  group Lasso [28] based on the combination of the ℓ1 norm and the ℓ2 norm has been
proposed for group feature selection  and quite a few efﬁcient algorithms [16  17  19] have been
proposed for efﬁcient optimization. However  the non-overlapping group structure in group Lasso
limits its applicability in practice. For example  in microarray gene expression data analysis  genes
may form overlapping groups as each gene may participate in multiple pathways [12].

Several recent work [3  12  15  18  29] studies the overlapping group Lasso  where groups of features
are given  potentially with overlaps between the groups. The resulting optimization is  however 
much more challenging to solve due to the group overlaps. When optimizing the overlapping group
Lasso problem  one can reformulate it as a second order cone program and solve it by a generic
toolbox  which  however  does not scale well. Jenatton et al. [13] proposed an alternating algorithm
called SLasso for solving the equivalent reformulation. However  SLasso involves an expensive
matrix inversion at each alternating iteration  and there is no known global convergence rate for
such an alternating procedure. A reformulation [5] was also proposed such that the original problem
can be solved by the Alternating Direction Method of Multipliers (ADMM)  which involves solving
a linear system at each iteration  and may not scale well for high dimensional problems. Argyriou
et al.
[1] adopted the proximal gradient method for solving the overlapping group lasso  and a
ﬁxed point method was developed to compute the proximal operator. Chen et al. [6] employed a

1

smoothing technique to solve the overlapping group Lasso problem. Mairal [18] proposed to solve
the proximal operator associated with the overlapping group Lasso deﬁned as the sum of the ℓ∞
norms  which  however  is not applicable to the overlapping group Lasso deﬁned as the sum of the
ℓ2 norms considered in this paper.
In this paper  we develop an efﬁcient algorithm for the overlapping group Lasso penalized problem
via the accelerated gradient descent method. The accelerated gradient descent method has recently
received increasing attention in machine learning due to the fast convergence rate even for non-
smooth convex problems. One of the key operations is the computation of the proximal operator
associated with the penalty. We reveal several key properties of the proximal operator associated
with the overlapping group Lasso penalty  and proposed several possible reformulations that can
be solved efﬁciently. The main contributions of this paper include: (1) we develop a low cost
prepossessing procedure to identify (and then remove) zero groups in the proximal operator  which
dramatically reduces the size of the problem to be solved; (2) we propose one dual formulation
and two proximal splitting formulations for the proximal operator; (3) for the dual formulation  we
further derive the duality gap which can be used to check the quality of the solution and determine
the convergence of the algorithm. We have performed empirical evaluations using both synthetic
data and the breast cancer gene expression data set  which consists of 8 141 genes organized into
(overlapping) gene sets. Experimental results demonstrate the efﬁciency of the proposed algorithm
in comparison with existing state-of-the-art algorithms.
Notations: k · k denotes the Euclidean norm  and 0 denotes a vector of zeros. SGN(·) and sgn(·)
are deﬁned in a component wise fashion as: 1) if t = 0  then SGN(t) = [−1  1] and sgn(t) = 0; 2)
if t > 0  then SGN(t) = {1} and sgn(t) = 1; and 3) if t < 0  SGN(t) = {−1} and sgn(t) = −1.
Gi ⊆ {1  2  . . .   p} denotes an index set  and xGi denote a sub-vector of x restricted to Gi.
2 The Overlapping Group Lasso
We consider the following overlapping group Lasso penalized problem:

(1)

(2)

min
x∈Rp

f (x) = l(x) + φλ1
λ2

(x)

where l(·) is a smooth convex loss function  e.g.  the least squares loss 

g

φλ1
λ2

(x) = λ1kxk1 + λ2

wikxGik

Xi=1

is the overlapping group Lasso penalty  λ1 ≥ 0 and λ2 ≥ 0 are regularization parameters 
wi > 0  i = 1  2  . . .   g  Gi ⊆ {1  2  . . .   p} contains the indices corresponding to the i-th group
of features  and k · k denotes the Euclidean norm. Note that the ﬁrst term in (2) can be absorbed
into the second term  which however will introduce p additional groups. The g groups of features
are pre-speciﬁed  and they may overlap. The penalty in (2) is a special case of the more general
Composite Absolute Penalty (CAP) family [29]. When the groups are disjoint with λ1 = 0 and
λ2 > 0  the model in (1) reduces to the group Lasso [28]. If λ1 > 0 and λ2 = 0  then the model in
(1) reduces to the standard Lasso [25].

In this paper  we propose to make use of the accelerated gradient descent (AGD) [2  21  22] for
solving (1)  due to its fast convergence rate. The algorithm is called “FoGLasso”  which stands for
Fast overlapping Group Lasso. One of the key steps in the proposed FoGLasso algorithm is the
computation of the proximal operator associated with the penalty in (2); and we present an efﬁcient
algorithm for the computation in the next section.
In FoGLasso  we ﬁrst construct a model for approximating f (·) at the point x as:
L
2 ky − xk2 

fL x(y) = [l(x) + hl′(x)  y − xi] + φλ2

(y) +

(3)

λ1

where L > 0. The model fL x(y) consists of the ﬁrst-order Taylor expansion of the smooth function
l(·) at the point x  the non-smooth penalty φλ2
2 ky − xk2. Next  a
sequence of approximate solutions {xi} is computed as follows: xi+1 = arg miny fLi si (y)  where
the search point si is an afﬁne combination of xi−1 and xi as si = xi +βi(xi− xi−1)  for a properly
chosen coefﬁcient βi  Li is determined by the line search according to the Armijo-Goldstein rule

(x)  and a regularization term L

λ1

2

so that Li should be appropriate for si  i.e.  f (xi+1) ≤ fLi si (xi+1). A key building block in
FoGLasso is the minimization of (3)  whose solution is known as the proximal operator [20]. The
computation of the proximal operator is the main technical contribution of this paper. The pseudo-
code of FoGLasso is summarized in Algorithm 1  where the proximal operator π(·) is deﬁned in
(4). In practice  we can terminate Algorithm 1 if the change of the function values corresponding to
adjacent iterations is within a small value  say 10−5.

Algorithm 1 The FoGLasso Algorithm
Input: L0 > 0  x0  k
Output: xk+1
1: Initialize x1 = x0  α−1 = 0  α0 = 1  and L = L0.
2: for i = 1 to k do
3:
4:

λ2/L(si − 1
Set Li = L and αi+1 =

L l′(si))

1+√1+4α2

i

2

5:
6: end for

Set βi = αi−2−1
αi−1
Find the smallest L = 2jLi−1  j = 0  1  . . . such that f (xi+1) ≤ fL si (xi+1) holds  where
xi+1 = πλ1/L

  si = xi + βi(xi − xi−1)

3 The Associated Proximal Operator and Its Efﬁcient Computation
The proximal operator associated with the overlapping group Lasso penalty is deﬁned as follows:

(4)

πλ1
λ2

x∈Rp(cid:26)gλ1

λ2

1
2kx − vk2 + φλ1

λ2

(x)(cid:27)  

Li

(v) = arg min

(x) ≡
which is a special case of (1) by setting l(x) = 1
2kx − vk2. It can be veriﬁed that the approximate
solution xi+1 = arg miny fLi si (y) is given by xi+1 = πλ1/Li
(si − 1
l′(si)). Recently  it has
λ2/Li
been shown in [14] that  the efﬁcient computation of the proximal operator is key to many sparse
learning algorithms. Next  we focus on the efﬁcient computation of πλ1
(v) in (4) for a given v.
λ2
The rest of this section is organized as follows. In Section 3.1  we discuss some key properties of
the proximal operator  based on which we propose a pre-processing technique that will signiﬁcantly
reduce the size of the problem. We then proposed to solve it via the dual formulation in Section 3.2 
and the duality gap is also derived. Several alternative methods for solving the proximal operator
via proximal splitting methods are discussed in Section 3.3.
3.1 Key Properties of the Proximal Operator
We ﬁrst reveal several basic properties of the proximal operator πλ1
λ2
Lemma 1. Suppose that λ1  λ2 ≥ 0  and wi > 0  for i = 1  2  . . .   g. Let x∗ = πλ1
following holds: 1) if vi > 0  then 0 ≤ x∗
i = 0; 4) SGN(v) ⊆ SGN(x∗); and 5) πλ1
x∗
Proof. When λ1  λ2 ≥ 0  and wi ≥ 0  for i = 1  2  . . .   g  the objective function gλ1
(·) is strictly
i > vi 
convex  thus x∗ is the unique minimizer. We ﬁrst show if vi > 0  then 0 ≤ x∗
i < 0  then
then we can construct a ˆx as follows: ˆxj = x∗
j   j 6= i and ˆxi = vi. Similarly  if x∗
j   j 6= i and ˆxi = 0. It is easy to verify that ˆx achieves
we can construct a ˆx as follows: ˆxj = x∗
a lower objective function value than x∗ in both cases. We can prove the second and the third
properties using similar arguments. Finally  we can prove the fourth and the ﬁfth properties using
the deﬁnition of SGN(·) and the ﬁrst three properties.
Next  we show that πλ1
(·) by soft-thresholding. Thus  we only
λ2
need to focus on the case when λ1 = 0. This simpliﬁes the optimization in (4). It is an extension of
the result for Fused Lasso in [10].
Theorem 1. Let u = sgn(v) ⊙ max(|v| − λ1  0)  and

(v). The
i ≤ 0; 3) if vi = 0  then

(·) can be directly derived from π0

i ≤ vi; 2) if vi < 0  then vi ≤ x∗

(v) = sgn(v) ⊙ πλ1

λ2

i ≤ vi. If x∗

(|v|).

(v).

λ2

λ2

λ2

λ2

π0
λ2 (u) = arg min

x∈Rp(hλ2(x) ≡

Then  the following holds: πλ1
λ2

(v) = π0
λ2

(u).

wikxGik) .

g

Xi=1

(5)

1
2kx − uk2 + λ2

3

Proof. Denote the unique minimizer of hλ2(·) as x∗. The sufﬁcient and necessary condition for the
optimality of x∗ is:
(6)

0 ∈ ∂hλ2(x∗) = x∗ − u + ∂φ0

λ2(x∗) 

(x) are the sub-differential sets of hλ2(·) and φ0

λ2

(·) at x  respectively.

where ∂hλ2(x) and ∂φ0
λ2
Next  we need to show 0 ∈ ∂gλ1

λ2

(x∗). The sub-differential of gλ1
λ2

(·) at x∗ is given by
λ2(x∗).

λ2

∂gλ1
λ2

(x∗) = x∗ − v + ∂φλ1

u ∈ v − λ1SGN(x∗).
(x∗).

(x∗) = x∗ − v + λ1SGN(x∗) + ∂φ0

(7)
It follows from the deﬁnition of u that u ∈ v − λ1SGN(u). Using the fourth property in Lemma 1 
we have SGN(u) ⊆ SGN(x∗). Thus 
(8)
It follows from (6)-(8) that 0 ∈ ∂gλ1
It follows from Theorem 1 that  we only need to focus on the optimization of (5) in the following
discussion. The difﬁculty in the optimization of (5) lies in the large number of groups that may
overlap. In practice  many groups will be zero  thus achieving a sparse solution (a sparse solution
is desirable in many applications). However  the zero groups are not known in advance. The key
question we aim to address is how we can identify as many zero groups as possible to reduce the
complexity of the optimization. Next  we present a sufﬁcient condition for a group to be zero.
Lemma 2. Denote the minimizer of hλ2(·) in (5) by x∗. If the i-th group satisﬁes kuGik ≤ λ2wi 
then x∗
Gi

= 0  i.e.  the i-th group is zero.

λ2

Proof. We decompose hλ2(x) into two parts as follows:

1

2kxGi − uGik2 + λ2wikxGik(cid:19) +


hλ2 (x) =(cid:18) 1
where Gi = {1  2  . . .   p} − Gi is the complementary set of Gi. We consider the minimization of
is ﬁxed. It can be veriﬁed that if kuGik ≤ λ2wi  then
hλ2(x) in terms of xGi when xGi
x∗
Gi
Lemma 2 may not identify many true zero groups due to the strong condition imposed. The lemma
below weakens the condition in Lemma 2. Intuitively  for a group Gi  we ﬁrst identify all existing
zero groups that overlap with Gi  and then compute the overlapping index subset Si of Gi as:

= 0 minimizes both terms in (9) simultaneously. Thus we have x∗
Gi

2kxGi − uGik2 + λ2Xj6=i

wjkxGjk
  

= x∗
Gi

= 0.

(9)

Si = [j6=i x∗

Gj

(Gj ∩ Gi).

=0

(10)

= 0 if kuGi−Sik ≤ λ2wi is satisﬁed. Note that this condition is much weaker
We can show that x∗
Gi
than the condition in Lemma 2  which requires that kuGik ≤ λ2wi.
Lemma 3. Denote the minimizer of hλ2(·) by x∗. Let Si  a subset of Gi  be deﬁned in (10). If
kuGi−Sik ≤ λ2wi holds  then x∗
Proof. Suppose that we have identiﬁed a collection of zero groups. By removing these groups  the
original problem (5) can then be reduced to:

= 0.

Gi

1

Gi

min

x(I1)∈R|I1|

wikxGi−Sik

2kx(I1) − u(I1)k2 + λ2 Xi∈G1
where I1 is the reduced index set  i.e.  I1 = {1  2  . . .   p} −Si:x∗
Gi 6= 0}
is the index set of the remaining non-zero groups. Note that ∀i ∈ G1  Gi − Si ∈ I1. By applying
Lemma 2 again  we show that if kuGi−Sik ≤ λ2wi holds  then x∗
Lemma 3 naturally leads to an iterative procedure for identifying the zero groups: For each group
Gi  if kuGik ≤ λ2wi  then we set uGi = 0; we cycle through all groups repeatedly until u does not
change. Let p′ = |{ui : ui 6= 0}| be the number of nonzero elements in u  g′ = |{uGi : uGi 6= 0}|
It follows from
be the number of the nonzero groups  and x∗ denote the minimizer of hλ2(·).
Lemma 3 and Lemma 1 that  if ui = 0  then x∗
i = 0. Therefore  by applying the above iterative
procedure  we can ﬁnd the minimizer of (5) by solving a reduced problem that has p′ ≤ p variables
and g′ ≤ g groups. With some abuse of notation  we still use (5) to denote the resulting reduced
problem. In addition  from Lemma 1  we only focus on u > 0 in the following discussion  and the
analysis can be easily generalized to the general u.

=0 Gi  and G1 = {i : x∗
= 0.

= 0. Thus  x∗
Gi

Gi−Si

4

3.2 Reformulation as an Equivalent Smooth Convex Optimization Problem
It follows from the ﬁrst two properties of Lemma 1 that  we can rewrite (5) as:

π0
λ2 (u) = arg min
x∈Rp
0(cid:22)x(cid:22)u

hλ2 (x) 

(11)

where (cid:22) denotes the element-wise inequality  and

hλ2(x) =

1
2kx − uk2 + λ2

g

Xi=1

wikxGik 

and the minimizer of hλ2(·) is constrained to be non-negative due to u > 0 (refer to the discussion
at the end of Section 3.1).
Making use of the dual norm of the Euclidean norm k · k  we can rewrite hλ2(x) as:

hλ2(x) = max
Y ∈Ω

1
2kx − uk2 +

where Ω is deﬁned as follows:

g

hx  Y ii 

Xi=1

(12)

Ω = {Y ∈ Rp×g : Y i

Gi

= 0 kY ik ≤ λ2wi  i = 1  2  . . .   g} 

Gi is the complementary set of Gi  Y is a sparse matrix satisfying Yij = 0 if the i-th feature does
not belong to the j-th group  i.e.  i 6∈ Gj  and Y i denotes the i-th column of Y . As a result  we can
reformulate (11) as the following min-max problem:

min
x∈Rp
0(cid:22)x(cid:22)u

max

Y ∈Ω(cid:26)ψ(x  Y ) =

1

2kx − uk2 + hx  Y ei(cid:27)  

(13)

where e ∈ Rg is a vector of ones. It is easy to verify that ψ(x  Y ) is convex in x and concave in Y  
and the constraint sets are closed convex for both x and Y . Thus  (13) has a saddle point  and the
min-max can be exchanged.

It is easy to verify that for a given Y   the optimal x minimizing ψ(x  Y ) in (13) is given by

Plugging (14) into (13)  we obtain the following minimization problem with regard to Y :

x = max(u − Y e  0).

min

Y ∈Rp×g:Y ∈Ω{ω(Y ) = −ψ(max(u − Y e  0)  Y )} .

(14)

(15)

Our methodology for minimizing hλ2(·) deﬁned in (5) is to ﬁrst solve (15)  and then construct the
solution to hλ2 (·) via (14). Using standard optimization techniques  we can show that the function
ω(·) is continuously differentiable with Lipschitz continuous gradient. We include the detailed proof
in the supplemental material for completeness. Therefore  we convert the non-smooth problem (11)
to the smooth problem (15)  making the smooth convex optimization tools applicable. In this paper 
we employ the accelerated gradient descent to solve (15)  due to its fast convergence property. Note
that  the Euclidean projection onto the set Ω can be computed in closed form. We would like to
emphasize here that  the problem (15) may have a much smaller size than (4).

3.2.1 Computing the Duality Gap
We show how to estimate the duality gap of the min-max problem (13)  which can be used to check
the quality of the solution and determine the convergence of the algorithm.
For any given approximate solution ˜Y ∈ Ω for ω(Y )  we can construct the approximate solution
˜x = max(u − ˜Y e  0) for hλ2(x). The duality gap for the min-max problem (13) at the point (˜x  ˜Y )
can be computed as:
(16)

ψ(x  ˜Y ).

gap( ˜Y ) = max
Y ∈Ω

ψ(˜x  Y ) − min

x∈Rp
0(cid:22)x(cid:22)u

The main result of this subsection is summarized in the following theorem:

5

Theorem 2. Let gap( ˜Y ) be the duality gap deﬁned in (16). Then  the following holds:

In addition  we have

gap( ˜Y ) = λ2

g

(wik˜xGik − h˜xGi   ˜Y i
Xi=1

Gii).

ω( ˜Y ) − ω(Y ∗) ≤ gap( ˜Y ) 
h(˜x) − h(x∗) ≤ gap( ˜Y ).

Proof. Denote (x∗  Y ∗) as the optimal solution to the problem (13). From (12)-(15)  we have

− ω( ˜Y ) = ψ(˜x  ˜Y ) = min

x∈Rp
0(cid:22)x(cid:22)u

ψ(x  ˜Y ) ≤ ψ(x∗  ˜Y ) 

ψ(x∗  ˜Y ) ≤ max
hλ2(x∗) = ψ(x∗  Y ∗) = min
x∈Rp
0(cid:22)x(cid:22)u

ψ(x∗  Y ) = ψ(x∗  Y ∗) = −ω(Y ∗) 
ψ(x  Y ∗) ≤ ψ(˜x  Y ∗) 

Y ∈Ω

(17)

(18)
(19)

(20)

(21)

(22)

ψ(˜x  Y ∗) ≤ max
Y ∈Ω
Incorporating (11)  (20)-(23)  we prove (17)-(19).
In our experiments  we terminate the algorithm when the estimated duality gap is less than 10−10.

ψ(˜x  Y ) = hλ2(˜x).

(23)

3.3 Proximal Splitting Methods

Recently  a family of proximal splitting methods [8] have been proposed for converting a challenging
optimization problem into a series of sub-problems with a closed form solution. We consider two
reformulations of the proximal operator (4)  based on the Dykstra-like Proximal Splitting Method
and the alternating direction method of multipliers (ADMM). The efﬁciency of these two methods
for overlapping Group Lasso will be demonstrated in the next section.

In [5]  Boyd et al. suggested that the original overlapping group lasso problem (1) can be reformu-
lated and solved by ADMM directly. We include the implementation of ADMM for our comparative
study. We provide the details of all three reformulations in the supplemental materials.

4 Experiments

In this section  extensive experiments are performed to demonstrate the efﬁciency of our proposed
methods. We use both synthetic data sets and a real world data set and the evaluation is done in
various problem size and precision settings. The proposed algorithms are mainly implemented in
Matlab  with the proximal operator implemented in standard C for improved efﬁciency. Several
state-of-the-art methods are also included for comparison purpose  including SLasso algorithm de-
veloped by Jenatton et al. [13]  the ADMM reformulation [5]  the Prox-Grad method by Chen et
al. [6] and the Picard-Nesterov algorithm by Argyriou et al. [1].

4.1 Synthetic Data

In the ﬁrst set of simulation we consider only the key component of our algorithm  the proximal
operator. The group indices are predeﬁned such that G1 = {1  2  . . .   10}  G2 = {6  7  . . .   20}  . . . 
with each group overlapping half of the previous group. 100 examples are generated for each set of
ﬁxed problem size p and group size g  and the results are summarized in Figure 1. As we can observe
from the ﬁgure  the dual formulation yields the best performance  followed closely by ADMM and
then the Dykstra method. We can also observe that our method scales very well to high dimensional
problems  since even with p = 106  the proximal operator can be computed in a few seconds. It
is also not surprising that Dykstra method is much more sensitive to the number of groups  which
equals to the number of projections in one Dykstra step.

To illustrate the effectiveness of our pre-processing technique  we repeat the previous experiment by
removing the pre-processing step. The results are shown in the right plot of Figure 1. As we can ob-
serve from the ﬁgure  the proposed pre-processing technique effectively reduces the computational

6

 

Dual
ADMM
Dykstra

102

100

10−2

i

e
m
T
U
P
C

 

10−4

 

1e3

1e4

p

1e5

1e6

i

e
m
T
U
P
C

 

101

100

10−1

10−2

10−3
 
10

Dual
ADMM
Dykstra

 

102

i

e
m
T
U
P
C

 

100

10−2

20

50
g

100

200

10−4

 

1e3

Dual
ADMM
Dual−no−pre
ADMM−no−pre

 

1e4

p

1e5

1e6

Figure 1: Time comparison for computing the proximal operators. The group number is ﬁxed in the
left ﬁgure and the problem size is ﬁxed in the middle ﬁgure. In the right ﬁgure  the effectiveness of
the pre-processing technique is illustrated.

time. As is evident from Figure 1  the dual formulation proposed in Section 3.2 consistently outper-
forms other proximal splitting methods. In the following experiments  only the dual method will be
used for computing the proximal operator  and our method will then be called as “FoGLasso”.

4.2 Gene Expression Data

1

1 = kATbk∞ (the zero point is a solution to (1) if λ1 ≥ λmax

We have also conducted experiments to evaluate the efﬁciency of the proposed algorithm using the
breast cancer gene expression data set [26]  which consists of 8 141 genes in 295 breast cancer
tumors (78 metastatic and 217 non-metastatic). For the sake of analyzing microarrays in terms
of biologically meaningful gene sets  different approaches have been used to organize the genes
into (overlapping) gene sets. In our experiments  we follow [12] and employ the following two
approaches for generating the overlapping gene sets (groups): pathways [24] and edges [7]. For
pathways  the canonical pathways from the Molecular Signatures Database (MSigDB) [24] are used.
It contains 639 groups of genes  of which 637 groups involve the genes in our study. The statistics of
the 637 gene groups are summarized as follows: the average number of genes in each group is 23.7 
the largest gene group has 213 genes  and 3 510 genes appear in these 637 groups with an average
appearance frequency of about 4. For edges  the network built in [7] will be used  and we follow [12]
to extract 42 594 edges from the network  leading to 42 594 overlapping gene sets of size 2. All
8 141 genes appear in the 42 594 groups with an average appearance frequency of about 10. The
experimental settings are as follows: we solve (1) with the least squares loss l(x) = 1
2kAx − bk2 
and we set wi = p|Gi|  and λ1 = λ2 = γ × λmax
  where |Gi| denotes the size of the i-th group
)  and γ is chosen from the
Gi  λmax
set {5 × 10−1  2 × 10−1  1 × 10−1  5 × 10−2  2 × 10−2  1 × 10−2  5 × 10−3  2 × 10−3  1 × 10−3}.
Comparison with SLasso  Prox-Grad and ADMM We ﬁrst compare our proposed FoGLasso
with the SLasso algorithm [13]  ADMM [5] and Prox-Grad [6]. The comparisons are based on
the computational time  since all these methods have efﬁcient Matlab implementations with key
components written in C. For a given γ  we ﬁrst run SLasso till a certain precision level is reached 
and then run the others until they achieve an objective function value smaller than or equal to that
of SLasso. Different precision levels of the solutions are evaluated such that a fair comparison can
be made. We vary the number of genes involved  and report the total computational time (seconds)
including all nine regularization parameters in Figure 2. We can observe that: 1) for all precision
levels  our proposed FoGLasso is much more efﬁcient than SLasso  ADMM and Prox-Grad; 2) the
advantage of FoGLasso over other three methods in efﬁciency grows with the increasing number of
genes (variables). For example  with the grouping by pathways  FoGLasso is about 25 and 70 times
faster than SLasso for 1000 and 2000 genes  respectively; and 3) the efﬁciency on edges is inferior
to that on pathways  due to the larger number of overlapping groups. Additional scalability study of
our proposed method using larger problem size can be found in the supplemental materials.
Comparison with Picard-Nesterov Since the code acquired for Picard-Nesterov is implemented
purely in Matlab  a computational time comparison might not be fair. Therefore  only the number
of iterations required for convergence is reported  as both methods adopt the ﬁrst order method.
We use edges to generate the groups  and vary the problem size from 100 to 400  using the same
set of regularization parameters. For each problem  we record both the number of outer iterations
(the gradient steps) and the total number of inner iterations (the steps required for computing the

1

7

i

e
m
T
U
P
C

 

i

e
m
T
U
P
C

 

104

103

102

101

100

10−1

 

103

102

101

100

10−1

 

Edges with Precision 1e−02

 

FoGLasso
ADMM
SLasso
Prox−Grad

100 200 300 400 500 1000 1500 2000

Number of involved genes

Pathways with Precision 1e−02

 

FoGLasso
ADMM
SLasso
Prox−Grad

100 200 300 400 500 1000 1500 2000

Number of involved genes

i

e
m
T
U
P
C

 

i

e
m
T
U
P
C

 

104

103

102

101

100

10−1

 

104

103

102

101

100

10−1

 

Edges with Precision 1e−04

 

FoGLasso
ADMM
SLasso
Prox−Grad

100 200 300 400 500 1000 1500 2000

Number of involved genes

Pathways with Precision 1e−04

 

FoGLasso
ADMM
SLasso
Prox−Grad

100 200 300 400 500 1000 1500 2000

Number of involved genes

i

e
m
T
U
P
C

 

i

e
m
T
U
P
C

 

104

103

102

101

100

10−1

 

104

103

102

101

100

 

Edges with Precision 1e−06

 

FoGLasso
ADMM
SLasso
Prox−Grad

100 200 300 400 500 1000 1500 2000

Number of involved genes

Pathways with Precision 1e−06

 

FoGLasso
ADMM
SLasso
Prox−Grad

100 200 300 400 500 1000 1500 2000

Number of involved genes

Figure 2: Comparison of SLasso [13]  ADMM [5]  Prox-Grad [6] and our proposed FoGLasso
algorithm in terms of computational time (in seconds and in the logarithmic scale) when different
numbers of genes (variables) are involved. Different precision levels are used for comparison.

Table 1: Comparison of FoGLasso and Picard-Nesterov using different numbers (p) of genes and
various precision levels. For each particular method  the ﬁrst row denotes the number of outer itera-
tions required for convergence  while the second row represents the total number of inner iterations.

Precision Level
p
FoGLasso

Picard-Nesterov

10−2
200
189
401
176
6.8e4

100
81
288
78
8271

400
353
921
325
2.2e5

100
192
404
181
2.6e4

10−4
200
371
590
304
1.0e5

400
1299
1912
1028
7.8e5

100
334
547
318
5.1e4

10−6
200
507
727
504
1.3e5

400
1796
2387
1431
1.1e6

proximal operators). The average number of iterations among all the regularization parameters are
summarized in Table 1. As we can observe from the table  though Picard-Nesterov method often
takes less outer iterations to converge  it takes a lot more inner iterations to compute the proximal
operator. It is straight forward to verify that the inner iterations in Picard-Nesterov method and our
proposed method have the same complexity of O(pg).

5 Conclusion

In this paper  we consider the efﬁcient optimization of the overlapping group Lasso penalized prob-
lem based on the accelerated gradient descent method. We reveal several key properties of the
proximal operator associated with the overlapping group Lasso  and compute the proximal operator
via solving the smooth and convex dual problem. Numerical experiments on both synthetic and
the breast cancer data set demonstrate the efﬁciency of the proposed algorithm. Although with an
inexact proximal operator  the optimal convergence rate of the accelerated gradient descent might
not be guaranteed [23  11]  the algorithm performs quite well empirically. A theoretical analysis on
the convergence property will be an interesting future direction. In the future  we also plan to apply
the proposed algorithm to other real-world applications involving overlapping groups.

Acknowledgments

This work was supported by NSF IIS-0812551  IIS-0953662  MCB-1026710  CCF-1025177  NGA
HM1582-08-1-0016  and NSFC 60905035  61035003.

8

References
[1] A. Argyriou  C.A. Micchelli  M. Pontil  L. Shen  and Y. Xu. Efﬁcient ﬁrst order methods for linear

composite regularizers. Arxiv preprint arXiv:1104.1436  2011.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[3] H. D. Bondell and B. J. Reich. Simultaneous regression shrinkage  variable selection and clustering of

predictors with oscar. Biometrics  64:115–123  2008.

[4] J. F. Bonnans and A. Shapiro. Optimization problems with perturbations: A guided tour. SIAM Review 

40(2):228–264  1998.

[5] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning

via the alternating direction method of multipliers. 2010.

[6] X. Chen  Q. Lin  S. Kim  J.G. Carbonell  and E.P. Xing. An efﬁcient proximal gradient method for general

structured sparse learning. Arxiv preprint arXiv:1005.4717  2010.

[7] H. Y. Chuang  E. Lee  Y. T. Liu  D. Lee  and T. Ideker. Network-based classiﬁcation of breast cancer

metastasis. Molecular Systems Biology  3(140)  2007.

[8] P.L. Combettes and J.C. Pesquet. Proximal splitting methods in signal processing. Arxiv preprint

arXiv:0912.3522  2009.

[9] J. M. Danskin. The theory of max-min and its applications to weapons allocation problems. Springer-

Verlag  New York  1967.

[10] J. Friedman  T. Hastie  H. H¨oﬂing  and R. Tibshirani. Pathwise coordinate optimization. Annals of Applied

Statistics  1(2):302–332  2007.

[11] B. He and X. Yuan. An accelerated inexact proximal point algorithm for convex minimization. 2010.
[12] L. Jacob  G. Obozinski  and J. Vert. Group lasso with overlap and graph lasso. In ICML  2009.
[13] R. Jenatton  J.-Y. Audibert  and F. Bach. Structured variable selection with sparsity-inducing norms.

Technical report  arXiv:0904.3523  2009.

[14] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for sparse hierarchical dictionary

learning. In ICML  2010.

[15] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In

ICML  2010.

[16] H. Liu  M. Palatucci  and J. Zhang. Blockwise coordinate descent procedures for the multi-task lasso 

with applications to neural semantic basis discovery. In ICML  2009.

[17] J. Liu  S. Ji  and J. Ye. Multi-task feature learning via efﬁcient ℓ2
[18] J. Mairal  R. Jenatton  G. Obozinski  and F. Bach. Network ﬂow algorithms for structured sparsity. In

1-norm minimization. In UAI  2009.

 

NIPS. 2010.

[19] L. Meier  S. Geer  and P. B¨uhlmann. The group lasso for logistic regression. Journal of the Royal

Statistical Society: Series B  70:53–71  2008.

[20] J.-J. Moreau. Proximit´e et dualit´e dans un espace hilbertien. Bull. Soc. Math. France  93:273–299  1965.
[21] A. Nemirovski. Efﬁcient methods in convex programming. Lecture Notes  1994.
[22] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publish-

ers  2004.

[23] R.T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and

Optimization  14:877  1976.

[24] A. Subramanian and et al. Gene set enrichment analysis: A knowledge-based approach for interpreting
genome-wide expression proﬁles. Proceedings of the National Academy of Sciences  102(43):15545–
15550  2005.

[25] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society

Series B  58(1):267–288  1996.

[26] M. J. Van de Vijver and et al. A gene-expression signature as a predictor of survival in breast cancer. The

New England Journal of Medicine  347(25):1999–2009  2002.

[27] Y. Ying  C. Campbell  and M. Girolami. Analysis of svm with indeﬁnite kernels. In NIPS. 2009.
[28] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal Of

The Royal Statistical Society Series B  68(1):49–67  2006.

[29] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and hierarchical

variable selection. Annals of Statistics  37(6A):3468–3497  2009.

9

,Simon Du
Yining Wang
Xiyu Zhai
Sivaraman Balakrishnan
Russ Salakhutdinov
Aarti Singh