2009,Sufficient Conditions for Agnostic Active Learnable,We study pool-based active learning in the presence of noise  i.e. the agnostic setting. Previous works have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful  it is also easy to construct examples that no active learning algorithm can have advantage. In this paper  we propose intuitively reasonable sufficient conditions under which agnostic active learning algorithm is strictly superior to passive supervised learning. We show that under some noise condition  if the classification boundary and the underlying distribution are smooth to a finite order  active learning achieves polynomial improvement in the label complexity; if the boundary and the distribution are infinitely smooth  the improvement is exponential.,Sufﬁcient Conditions for Agnostic Active Learnable

Key Laboratory of Machine Perception  MOE 

School of Electronics Engineering and Computer Science 

Liwei Wang

Peking University 

wanglw@cis.pku.edu.cn

Abstract

We study pool-based active learning in the presence of noise  i.e. the agnostic set-
ting. Previous works have shown that the effectiveness of agnostic active learning
depends on the learning problem and the hypothesis space. Although there are
many cases on which active learning is very useful  it is also easy to construct
examples that no active learning algorithm can have advantage. In this paper  we
propose intuitively reasonable sufﬁcient conditions under which agnostic active
learning algorithm is strictly superior to passive supervised learning. We show
that under some noise condition  if the Bayesian classiﬁcation boundary and the
underlying distribution are smooth to a ﬁnite order  active learning achieves poly-
nomial improvement in the label complexity; if the boundary and the distribution
are inﬁnitely smooth  the improvement is exponential.

1 Introduction

Active learning addresses the problem that the algorithm is given a pool of unlabeled data drawn
i.i.d. from some underlying distribution. The algorithm can then pay for the label of any example
in the pool. The goal is to learn an accurate classiﬁer by requesting as few labels as possible. This
is in contrast with the standard passive supervised learning  where the labeled examples are chosen
randomly.

The simplest example that demonstrates the potential of active learning is to learn the optimal thresh-
old on an interval. If there exists a perfect threshold separating the two classes (i.e. there is no noise) 
 ) labels to learn an -accurate classiﬁer  while passive learn-
then binary search only needs O(ln 1
ing requires O( 1
 ) labels. Another encouraging example is to learn a homogeneous linear separator
for data uniformly distributed on the unit sphere of Rd. In this case active learning can still give
exponential savings in the label complexity [Das05].

However  there are also very simple problems that active learning does not help at all. Suppose the
instances are uniformly distributed on [0  1]  and the positive class could be any interval on [0  1].
Any active learning algorithms needs O( 1
 ) label requests to learn an -accurate classiﬁer [Han07].
There is no improvement over passive learning. All above are noise-free (realizable) problems. Of
more interest and more realistic is the agnostic setting  where the class labels can be noisy so that
the best classiﬁer in the hypothesis space has a non-zero error ν. For agnostic active learning  there
is no active learning algorithm that can always reduce label requests due to a lower bound Ω( ν2
2 ) for
the label complexity [Kaa06].

It is known that whether active learning helps or not depends on the distribution of the instance-label
pairs and the hypothesis space. Thus a natural question would be that under what conditions is active
learning guaranteed to require fewer labels than passive learning.

1

In this paper we propose intuitively reasonable sufﬁcient conditions under which active learning
achieves lower label complexity than that of passive learning. Speciﬁcally  we focus on the A2 al-
gorithm [BAL06] which works in the agnostic setting. Earlier work has discovered that the label
complexity of A2 can be upper bounded by a parameter of the hypothesis space and the data dis-
tribution called disagreement coefﬁcient [Han07]. This parameter often characterizes the intrinsic
difﬁculty of the learning problem. By an analysis of the disagreement coefﬁcient we show that  un-
der some noise condition  if the Bayesian classiﬁcation boundary and the underlying distribution are
smooth to a ﬁnite order  then A2 gives polynomial savings in the label complexity; if the boundary
and the distribution are inﬁnitely smooth  A2 gives exponential savings.

1.1 Related Works

Our work is closely related to [CN07]  in which the authors proved sample complexity bounds for
problems with smooth classiﬁcation boundary under Tsybakov’s noise condition [Tsy04]. They also
assumed that the distribution of the instances is bounded from above and below. The main difference
to our work is that their analysis is for the membership-query setting [Ang88]  in which the learning
algorithm can choose any point in the instance space and ask for its label; while the pool-based
model analyzed here assumes the algorithm can only request labels of the instances it observes.

Another related work is due to Friedman [Fri09]. He introduced a different notion of smoothness
and showed that this guarantees exponential improvement for active learning. But his work focused
on the realizable case and does not apply to the agnostic setting studied here.
Soon after A2  Dasgupta  Hsu and Monteleoni [DHM07] proposed an elegant agnostic active learn-
ing algorithm. It reduces active learning to a series of supervised learning problems. If the hy-
pothesis space has a ﬁnite VC dimension  it has a better label complexity than A2. However  this
algorithm relies on the normalized uniform convergence bound for the VC class. It is not known
whether it holds for more general hypothesis space such as the smooth boundary class analyzed in
this paper. (For recent advances on this topic  see [GKW03].) It is left as an open problem whether
our results apply to this algorithm by reﬁned analysis of the normalized bounds.

2 Preliminaries
Let X be an instance space  D a distribution over X × {−1  1}. Let H be the hypothesis space  a
set of classiﬁers from X to {±1}. Denote DX the marginal of D over X . In our active learning
model  the algorithm has access to a pool of unlabeled examples from DX . For any unlabeled point
(cid:80)
x  the algorithm can ask for its label y  which is generated from the conditional distribution at x.
The error of a hypothesis h according to D is erD(h) = Pr(x y)∼D(h(x) (cid:54)= y). The empirical error
on a ﬁnite sample S is erS(h) = 1|S|
(x y)∈S I[h(x) (cid:54)= y]  where I is the indicator function. We
use h∗ denote the best classiﬁer in H. That is  h∗ = arg minh∈H erD(h). Let ν = erD(h∗). Our
goal is to learn a ˆh ∈ H with error rate at most ν +   where  is a predeﬁned parameter.
A2 is the ﬁrst rigorous agnostic active learning algorithm. A description of the algorithm is given
in Fig.1. It was shown that A2 is never much worse than passive learning in terms of the label
complexity. The key observation that A2 can be superior to passive learning is that  since our goal is
to choose an ˆh such that erD(ˆh) ≤ erD(h∗) +   we only need to compare the errors of hypotheses.
Therefore we can just request labels of those x on which the hypotheses under consideration have
disagreement.
To do this  the algorithm keeps track of two spaces. One is the current version space Vi  consisting
of hypotheses that with statistical conﬁdence are not too bad compared to h∗. To achieve such a
statistical guarantee  the algorithm must be provided with a uniform convergence bound over the
hypothesis space. That is  with probability at least 1 − δ over the draw of sample S according to D 
hold simultaneously for all h ∈ H  where the lower bound LB(S  h  δ) and upper bound
U B(S  h  δ) can be computed from the empirical error erS(h). The other space is the region of
disagreement DIS(Vi)  which is the set of all x ∈ X for which there are hypotheses in Vi that
disagree on x. Formally  for any V ⊂ H 

LB(S  h  δ) ≤ erD(h) ≤ U B(S  h  δ) 

DIS(V ) = {x ∈ X : ∃h  h(cid:48) ∈ V  h(x) (cid:54)= h(cid:48)(x)}.

2

Input: concept space H  accuracy parameter  ∈ (0  1)  conﬁdence parameter δ ∈ (0  1);
Output: classiﬁer ˆh ∈ H;
(λ depends on H and the problem  see Theorem 5) ;
Let ˆn = 2(2 log2
 + ln 1
Let δ(cid:48) = δ/ˆn ;
V0 ← H  S0 ← ∅  i ←0  j1 ←0  k ←1 ;
while ∆(Vi)(minh∈Vi U B(Si  h  δ(cid:48)) − minh∈Vi LB(Si  h  δ(cid:48))) >  do

δ ) log2

2


λ

Vi+1 ← {h ∈ Vi : LB(Si  h  δ(cid:48)) ≤ minh(cid:48)∈Vi U B(Si  h(cid:48)  δ(cid:48))};
i ←i+1;
if ∆(Vi) < 1

2∆(Vjk) then

k ← k + 1; jk ← i;
end
i ← Rejection sample 2i−jk samples x from D satisfying x ∈ DIS(Vi);
S(cid:48)
Si ← {(x  y = label(x)) : x ∈ S(cid:48)
i};
end
Return ˆh= argminh∈ViU B(Si  h  δ(cid:48)).

Algorithm 1: The A2 algorithm (this is the version in [Han07])

The volume of DIS(V ) is denoted by ∆(V ) = PrX∼DX (X ∈ DIS(V )). Requesting labels of
the instances from DIS(Vi) allows A2 require fewer labels than passive learning. Hence the key
issue is how fast ∆(Vi) reduces. This process  and in turn the label complexity of A2  are nicely
characterized by the disagreement coefﬁcient θ introduced in [Han07].

Deﬁnition 1 Let ρ(· ·) be the pseudo-metric on a hypothesis space H induced by DX . That is  for
h  h(cid:48) ∈ H  ρ(h  h(cid:48)) = PrX∼DX (h(X) (cid:54)= h(cid:48)(X)). Let B(h  r) = {h(cid:48) ∈ H: ρ(h  h(cid:48)) ≤ r}. The
disagreement coefﬁcient θ() is

θ() = sup
r≥

PrX∼DX (X ∈ DIS(B(h∗  r)))

r

 

(1)

where h∗ = arg minh∈H erD(h).

Note that θ depends on H and D  and 1 ≤ θ() ≤ 1
 .

3 Main Results

As mentioned earlier  whether active learning helps or not depends on the distribution and the hy-
pothesis space. There are simple examples such as learning intervals for which active learning has
no advantage. However  these negative examples are more or less “artiﬁcial”. It is important to
understand whether problems with practical interest are actively learnable or not. In this section we
provide intuitively reasonable conditions under which the A2 algorithm is strictly superior to passive
learning. Our main results (Theorem 11 and Theorem 12) show that if the learning problem has a
smooth Bayes classiﬁcation boundary  and the distribution DX has a density bounded by a smooth
function  then under some noise condition A2 saves label requests. It is a polynomial improvement
for ﬁnite smoothness  and exponential for inﬁnite smoothness.

In Section 3.1 we formally deﬁne the smoothness and introduce the hypothesis space  which contains
smooth classiﬁers. We show a uniform convergence bound of order O(n−1/2) for this hypothesis
space. This bound determines U B(S  h  δ) and LB(S  h  δ) in A2. Section 3.2 is the main technical
part  where we give upper bounds for the disagreement coefﬁcient of smooth problems. In Section
3.3 we show that under some noise condition  there is a sharper bound for the label complexity in
terms of the disagreement coefﬁcient. These lead to our main results.

3

3.1 Smoothness
Let f be a function deﬁned on Ω ⊂ Rd. For any vector k = (k1 ···   kd) of d nonnegative integers 
let |k| =

(cid:80)d

i=1 ki. Deﬁne the K-norm as
(cid:107)f(cid:107)K := max
|k|≤K−1

sup
x∈Ω

|Dkf(x)| + max
|k|=K−1

Dkf(x) − Dkf(x(cid:48))

(cid:107)x − x(cid:48)(cid:107)

 

sup
x x(cid:48)∈Ω

(2)

where

is the differential operator.

Dk =

∂|k|

∂k1x1 ··· ∂kdxd

 

Deﬁnition 2 (Finite Smooth Functions) A function f is said to be Kth order smooth with respect
to a constant C  if (cid:107)f(cid:107)K ≤ C. The set of Kth order smooth functions is deﬁned as

(3)
Thus Kth order smooth functions have uniformly bounded partial derivatives up to order K −1  and
the K − 1th order partial derivatives are Lipschitz.

C := {f : (cid:107)f(cid:107)K ≤ C}.
F K

Deﬁnition 3 (Inﬁnitely Smooth Functions) A function f is said to be inﬁnitely smooth with respect
to a constant C  if (cid:107)f(cid:107)K ≤ C for all nonnegative integers K. The set of inﬁnitely smooth functions
is denoted by F ∞
C .

With the deﬁnitions of smoothness  we introduce the hypothesis space we use in the A2 algorithm.
Deﬁnition 4 (Hypotheses with Smooth Boundaries) A set of hypotheses HK
C deﬁned on [0  1]d+1
is said to have Kth order smooth boundaries  if for every h ∈ HK
C   the classiﬁcation boundary is
a Kth order smooth function on [0  1]d. To be precise  let x = (x1  x2  . . .   xd+1) ∈ [0  1]d+1. The
classiﬁcation boundary is the graph of function xd+1 = f(x1  . . .   xd)  where f ∈ F K
C . Similarly 
a hypothesis space H∞
C the
classiﬁcation boundary is the graph an inﬁnitely smooth function on [0  1]d.

C is said to have inﬁnitely smooth boundaries  if for every h ∈ H∞

Previous results on the label complexity of A2 assumes the hypothesis space has ﬁnite VC di-
mension. The goal is to ensure a O(n−1/2) uniform convergence bound so that U B(S  h  δ) −
LB(S  h  δ) = O(n−1/2). The hypothesis space HK
C do not have ﬁnite VC dimensions.
Compared with the VC class  HK
C are exponentially larger in terms of the covering num-
bers [vdVW96]. But uniform convergence bound still holds for HK
C under a broad class of
distributions. The following theorem is a consequence of some known results in empirical processes.
Theorem 5 For any distribution D over [0  1]d+1 × {−1  1}  whose marginal distribution DX on
[0  1]d+1 has a density upper bounded by a constant M  and any 0 < δ ≤ δ0 (δ0 is a constant)  with
probability at least 1 − δ over the draw of the training set S of n examples 

C and H∞

C and H∞

C and H∞

(cid:115)

log 1
δ

 

|erD(h) − erS(h)| ≤ λ
(4)
C provided K > d (or K = ∞). Here λ is a constant depending

holds simultaneously for all h ∈ HK
only on d  K  C and M.
Proof It can be seen  from Corollary 2.7.3 in [vdVW96] that the bracketing numbers N[ ] of HK
C
satisﬁes log N[ ]( HK
K ). Since K > d  then there exist constants c1  c2 such
that

(cid:195)
C   L2(DX )) = O(( 1

(cid:33)

 ) 2d

n

(cid:181)

(cid:182)

PD

sup
h∈HK

C

|er(h) − erS(h)| ≥ t

≤ c1 exp

− nt2
c2

for all nt2 ≥ t0  where t0 is some constant (see Theorem 5.11 and Lemma 5.10 of [vdG00]). Let
δ = c1 exp

  the theorem follows.

− nt2

(cid:179)

(cid:180)

c2

4

(cid:113)

(cid:113)

Now we can determine U B(S  h  δ) and LB(S  h  δ) for A2 by simply letting U B(S  h  δ) =
erS(h) + λ

n and LB(S  h  δ) = erS(h) − λ

n   where S is of size n.

ln 1
δ

ln 1
δ

3.2 Disagreement Coefﬁcient

The disagreement coefﬁcient θ plays an important role for the label complexity of active learning
algorithms. In fact previous negative examples for which active learning does not work are all the
results of large θ. For instance the interval learning problem  θ() = 1
   which leads to the same
label complexity as passive learning. In the following two theorems we show that the disagreement
coefﬁcient θ() for smooth problems is small.
C . If the distribution DX has a density p(x1  . . .   xd+1)
Theorem 6 Let the hypothesis space be HK
such that there exists a Kth order smooth function g(x1  . . .   xd+1) and two constants 0 < α ≤
β such that αg(x1  . . .   xd+1) ≤ p(x1  . . .   xd+1) ≤ βg(x1  . . .   xd+1) for all (x1  . . .   xd+1) ∈
[0  1]d+1  then θ() = O

(cid:162) d

(cid:179)(cid:161)

(cid:180)

K+d

.

1


Theorem 7 Let the hypothesis space be H∞
C . If the distribution DX has a density p(x1  . . .   xd+1)
such that there exist an inﬁnitely smooth function g(x1  . . .   xd) and two constants 0 < α ≤ β such
that αg(x1  . . .   xd) ≤ p(x1  . . .   xd+1) ≤ βg(x1  . . .   xd) for all (x1  . . .   xd+1) ∈ [0  1]d+1  then
θ() = O(logd( 1

 )).

The key points in the theorems are: the classiﬁcation boundaries are smooth; and the density is
bounded from above and below by constants times a smooth function. These two conditions include
a large class of learning problems. Note that the density itself is not necessarily smooth. We just
require the density does not change too rapidly.
The intuition behind the two theorems above is as follows. Let fh∗(x) and fh(x) be the classiﬁcation
boundaries of h∗ and h  and suppose ρ(h  h∗) is small  where ρ(h  h∗) = Prx∼DX (h(x) (cid:54)= h∗(x))
is the pseudo metric. If the classiﬁcation boundaries and the density are all smooth  then the two
boundaries have to be close to each other everywhere. That is  |fh(x) − ff∗(x)| is small uniformly
for all x. Hence only the points close to the classiﬁcation boundary of h∗ can be in DIS(B(h∗  )) 
which leads to a small disagreement coefﬁcient.

The proofs of Theorem 6 and Theorem 7 rely on the following two lemmas.

[0 1]d |Φ(x)|dx ≤ r. If there exists a Kth
Lemma 8 Let Φ be a function deﬁned on [0  1]d and
order smooth function ˜Φ and 0 < α ≤ β such that α|˜Φ(x)| ≤ |Φ(x)| ≤ β|˜Φ(x)| for all x ∈ [0  1]d 
then (cid:107)Φ(cid:107)∞ = O(r

K+d )  where (cid:107)Φ(cid:107)∞ = supx∈[0 1]d |Φ(x)|.

K+d ) = O(r · ( 1

K

r ) d

[0 1]d |Φ(x)|dx ≤ r. If there exists an inﬁnitely
Lemma 9 Let Φ be a function deﬁned on [0  1]d and
smooth function ˜Φ and 0 < α ≤ β such that α|˜Φ(x)| ≤ |Φ(x)| ≤ β|˜Φ(x)| for all x ∈ [0  1]d  then
(cid:107)Φ(cid:107)∞ = O(r · logd( 1
r ))

(cid:82)

(cid:82)

We will brieﬂy describe the ideas of the proofs of these two lemmas in the Appendix. The formal
proofs are given in the supplementary ﬁle.
Proof of Theorem 6 First of all  since we focus on binary classiﬁcation  DIS(B(h∗  r)) can be
written equivalently as

DIS(B(h∗  r)) = {x ∈ X   ∃h ∈ B(h∗  r)  s.t. h(x) (cid:54)= h∗(x)}.

Consider any h ∈ B(h∗  r). Let fh  fh∗ ∈ F K
and h∗ respectively. If r is sufﬁciently small  we must have

C be the corresponding classiﬁcation boundaries of h

ρ(h  h∗) = Pr
X∼DX

(h(X) (cid:54)= h∗(X)) =

p(x1  . . .   xd+1)dxd+1

(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)

(cid:90) fh(x1 ... xd)

fh∗ (x1 ... xd)

(cid:90)

dx1 . . . dxd

[0 1]d

5

(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) .

Denote

Φh(x1  . . .   xd) =

p(x1  . . .   xd+1)dxd+1.

(cid:90) fh(x1 ... xd)

fh∗ (x1 ... xd)

We assert that there is a Kth order smooth function ˜Φh(x1  . . .   xd) and two constants 0 < u ≤ v
such that u|˜Φh| ≤ |Φh| ≤ v|˜Φh|. To see this  remember that fh and fh∗ are Kth order smooth
functions; and the density p is upper and lower bounded by constants times a Kth order smooth
function g(x1  . . .   xd+1); and note that ˜Φh(x1  . . .   xd) =
fh∗ (x1 ... xd) g(x1  . . .   xd+1)dxd+1 is
a Kth order smooth function. The latter is easy to check by taking derivatives. By Lemma 8  we have
(cid:107)Φh(cid:107)∞ = O(r · ( 1
we have suph∈B(h∗ r) (cid:107)Φh(cid:107)∞ = O(r · ( 1
Now consider the region of disagreement of B(h∗  r). Clearly DIS(B(h∗  r)) = ∪h∈B(h∗ r){x :
h(x) (cid:54)= h∗(x)}. Hence

(cid:82) |Φh| = ρ(h  h∗) ≤ r. Because this holds for all h ∈ B(h∗  r) 

(cid:82) fh(x1 ... xd)

K+d )  since

r ) d

r ) d

K+d ).

(cid:161)

x ∈ ∪h∈B(h∗ r){x : h(x) (cid:54)= h∗(x)}(cid:162)
(cid:195)

(cid:33)

(cid:182) d

(cid:181)

Pr
X∼DX

(x ∈ DIS(B(h∗  r))) = Pr
X∼DX

(cid:90)

≤ 2

sup

h∈B(h∗ r)

[0 1]d

(cid:107)Φh(cid:107)∞dx1 . . . dxd = O

r ·

The theorem follows by the deﬁnition of θ().
Theorem 7 can be proved similarly by using Lemma 9.

1
r

K+d

.

(cid:182)

3.3 Label Complexity

(cid:181)

(cid:181)

(cid:182)
ν2
2 + 1

(cid:181)

(cid:182)

O

θ2

It was shown in [Han07] that the label complexity of A2 is
1


(5)
where ν = minh∈H erD(h). When  ≥ ν  our previous results on the disagreement coefﬁcient
already imply polynomial or exponential improvements for A2. However  when  < ν  the label
complexity becomes O( 1
2 )  the same as passive learning whatever θ is. In fact  without any as-
sumption on the noise  the O( 1
2 ) lower bound of agnostic
active learning [Kaa06].

2 ) result is inevitable due to the Ω( ν2

polylog

1
δ

ln

 

Recently  there has been considerable interest in how noise affects the learning rate. A remarkable
notion is due to Tsybakov [Tsy04]  which was ﬁrst introduced for passive learning. Let η(x) =
P (Y = 1|X = x). Tsybakov’s noise condition assumes that for some c > 0  0 < α ≤ ∞

(6)
for all 0 < t ≤ t0  where t0 is some constant. (6) implies a connection between the pseudo distance
ρ(h  h∗) and the excess risk erD(h) − erD(h∗):

Pr
X∼DX

(|η(X) − 1/2| ≤ t) ≤ ct−α 

ρ(h  h∗) ≤ c(cid:48) (erD(h) − erD(h∗))1/κ  

(7)
α ≥ 1 is called the noise
where h∗ is the Bayes classiﬁer  c(cid:48) is some ﬁnite constant. Here κ = 1+α
exponent. κ = 1 is the optimal case  where the problem has bounded noise; κ > 1 correspond to
unbounded noise.

Castro and Nowak [CN07] noticed that Tsybakov’s noise condition is also important in active learn-
ing. They proved label complexity bounds in terms of κ for the membership-query setting. A notable
fact is that ˜O(( 1
κ ) (κ > 1) is both an upper and a lower bound for membership-query in the
minimax sense. It is important to point out that the lower bound automatically applies to pool-based
model  since pool makes weaker assumptions than membership-query. Hence for large κ  active
learning has very limited improvement over passive learning whatever other factors are.

 ) 2κ−2

Recently  Hanneke [Han09] obtained similar label complexity for pool-based model. He showed the
δ ) for the bounded noise case  i.e. κ = 1. Here we slightly
labels requested by A2 is O(θ2 ln 1

 ln 1

6

generalize Hanneke’s result to unbounded noise by introducing the following noise condition. We
assume there exist c1  c2 > 0 and T0 > 0 such that

(|η(X) − 1/2| ≤ 1
(cid:181)
T
for all T ≥ T0. It is not difﬁcult to show that (8) implies
(er(h) − er(h∗)) ln

ρ(h  h∗) = O

Pr
X∼DX

) ≤ c1e−c2T  

1

(er(h) − er(h∗))

(cid:182)

.

(8)

(9)

 )).

δ · polylog( 1

This condition assumes unbounded noise. Under this noise condition  A2 has a better label com-
plexity.
Theorem 10 Assume that the learning problem satisﬁes the noise condition (8) and DX has a den-
sity upper bounded by a constant M. For any hypothesis space H that has a O(n−1/2) uniform
convergence bound  if the Bayes classiﬁer h∗ is in H  then with probability at least 1− δ  A2 outputs
ˆh ∈ H with erD(ˆh) ≤ erD(h∗) +   and the number of labels requested by the algorithm is at most
O(θ2() · ln 1
Proof As the proof of [Han07]  one can show that with probability 1 − δ we never remove h∗ from
Vi  and for any h  h(cid:48) ∈ Vi we must have ∆(Vi)(eri(h) − eri(h(cid:48))) = erD(h) − erD(h(cid:48))  where
eri(h) is the error rate of h conditioned on DIS(Vi). These guarantees erD(ˆh) ≤ erD(h∗) + .
If ∆(Vi) ≤ 2θ()  due to the O(n−1/2) uniform convergence bound  O(θ2() ln 1
δ ) labels sufﬁces
to make ∆(Vi)(U B(Si  h  δ(cid:48)) − LB(Si  h  δ(cid:48))) ≤  for all h ∈ DIS(Vi) and the algorithm stops.
 ) times ∆(Vi) <
Hence we next consider ∆(Vi) > 2θ(). Note that there are at most O(ln 1
2∆(Vjk) occurs. So below we bound the number of labels needed to make ∆(Vi) < 1
2∆(Vjk)
1
occurs. By the deﬁnition of θ()  if ρ(h  h∗) ≤ ∆(Vjk )
2∆(Vjk). Let
γ(h) = erD(h) − erD(h∗). By the noise assumption (9) we have that if

for all h ∈ Vi  then ∆(Vi) < 1

2θ()

γ(h) ln

1

γ(h)

≤ c

∆(Vjk)
2θ()  

(10)

)

  and in turn if γ(h) ≤ c ∆(Vjk )
θ() ln 1


then ∆(Vi) < 1
2∆(Vjk). Here and below  c is appropriate constant but may be different from
line to line. Note that (10) holds if γ(h) ≤ c ∆(Vjk )
since
θ() ln θ()
∆(Vjk
∆(Vjk) ≥ ∆(Vi) > 2θ(). But to have the last inequality  the algorithm only needs to label
O(θ2() ln2 1
δ ) instances from DIS(Vi). So the total number of labels requested by A2 is
 ln 1
O(θ2() ln 1
 )
δ ln3 1
Now we give our main label complexity bounds for agnostic active learning.
Theorem 11 Let the instance space be [0  1]d+1. Let the Hypothesis space be HK
C   where K > d.
Assume that the Bayes classiﬁer h∗ of the learning problem is in HK
C ; the noise condition (8) holds;
and DX has a density bounded by a Kth order smooth function as in Theorem 6. Then the A2
algorithm outputs ˆh with error rate erD(ˆh) ≤ erD(h∗) +  and the number of labels requested is at
most ˜O
Proof Note that the density DX is upper bounded by a smooth function implies that it is also upper
bounded by a constant M. Combining Theorem 5  6 and 10 the theorem follows.

  where in ˜O we hide the polylog

(cid:162) 2d

K+d ln 1
δ

(cid:179)(cid:161)

term.

(cid:180)

(cid:161)

(cid:162)

1


1


Combining Theorem 5  7 and 10 we can show the following theorem.
Theorem 12 Let the instance space be [0  1]d+1. Let the Hypothesis space be H∞
C . Assume that
the Bayes classiﬁer h∗ of the learning problem is in H∞
C ; the noise condition (8) holds; and DX
has a density bounded by an inﬁnitely smooth function as in Theorem 7. Then the A2 algorithm
outputs ˆh with error rate erD(ˆh) ≤ erD(h∗) +  and the number of labels requested is at most
O

polylog

(cid:162)

(cid:162)

(cid:161)

(cid:161)

.

1


ln 1
δ

7

4 Conclusion

We show that if the Bayesian classiﬁcation boundary is smooth and the distribution is bounded by a
smooth function  then under some noise condition active learning achieves polynomial or exponen-
tial improvement in the label complexity than passive supervised learning according to whether the
smoothness is of ﬁnite order or inﬁnite.

Although we assume that the classiﬁcation boundary is the graph of a function  our results can be
generalized to the case that the boundaries are a ﬁnite number of functions. To be precise  consider
N functions f1(x) ≤ ··· ≤ fN (x)  for all x ∈ [0  1]d. Let f0(x) ≡ 0  fN +1(x) ≡ 1. The positive (or
negative) set deﬁned by these functions is {(x  xd+1) : f2i(x) ≤ x ≤ f2i+1(x)  i = 0  1  . . .   N
2 }.
Our theorems still hold in this case. In addition  by techniques in [Dud99] (page 259)  our results
may generalize to problems which have intrinsic smooth boundaries (not only graphs of functions).

Appendix

In this appendix we describe very brieﬂy the ideas to prove Lemma 8 and Lemma 9. The formal
proofs can be found in the supplementary ﬁle.
Ideas to Prove Lemma 8 First consider the d = 1 case. Note that if f ∈ F K
C   then |f (K−1)(x) −
f (K−1)(x(cid:48))| ≤ C|x − x(cid:48)| for all x  x(cid:48) ∈ [0  1]. It is not difﬁcult to see that we only need to show for
any f such that |f (K−1)(x)−f (K−1)(x(cid:48))| ≤ C|x−x(cid:48)|  if
K+1 ).
To show this  note that in order that (cid:107)f(cid:107)∞ achieves the maximum while
of f must be as large as possible. Indeed  it can be shown that (one of) the optimal f is of the form

(cid:82) 1
0 |f(x)|dx = r  then (cid:107)f(cid:107)∞ = O(r

(cid:82) |f| = r  the derivatives

K

 C

f(x) =

K!|x − ξ|K

0

0 ≤ x ≤ ξ 
ξ < x ≤ 1.

That is  |f (K−1)(x) − f (K−1)(x(cid:48))| = C|x − x(cid:48)| (i.e. the K − 1 order derivatives reaches the upper
bound of the Lipschitz constant.) for all x  x(cid:48) ∈ [0  ξ]  where ξ is determined by
0 f(x)dx = r. It
is then easy to check that (cid:107)f(cid:107)∞ = O(r
For the general d > 1 case  we relax the constraint. Note that all K − 1th order partial derivatives
are Lipschitz implies that all K −1th order directional derivatives are Lipschitz too. Under the latter
constraint  (one of) the optimal f has the form

K+1 ).

K

(11)

(cid:82) 1

 C

f(x) =

(cid:82)

K!|(cid:107)x(cid:107) − ξ|K

0

0 ≤ (cid:107)x(cid:107) ≤ ξ 
ξ < (cid:107)x(cid:107).

(cid:82)

where ξ is determined by

[0 1]d |f(x)|dx = r. This implies (cid:107)f(cid:107)∞ = O(r

K

K+d ).

C   if

[0 1]d |f(x)|dx = r  then (cid:107)f(cid:107)∞ = O(r · logd( 1

Ideas to Prove Lemma 9 Similar to the proof of Lemma 8  we only need to show that for any
f ∈ F ∞
Since f is inﬁnitely smooth  we can choose K large and depending on r. For the d = 1 case  let
K + 1 = log 1
. We know that the optimal f is of the form of Eq.(11). (Actually this choice of K
log log 1
r
is approximately the largest K such that Eq.(11) is still the optimal form. If K is larger than this  ξ
will be out of [0  1].) Since
K! ξK. Note

(cid:82) 1
0 |f(x)| = r  we have ξK+1 = (K+1)!

. Now  (cid:107)f(cid:107)∞ = C

r )).

C

r

log log 1
r

log 1

r )K+1 = ( 1
r )

that ( 1
For the d > 1 case  let K + d = log 1
log log 1
r

r = log 1

r

r . By Stirling’s formula we can show (cid:107)f(cid:107)∞ = O(r · log 1
r ).

. By similar arguments we can show (cid:107)f(cid:107)∞ = O(r·logd 1
r ).

Acknowledgement

This work was supported by NSFC(60775005).

8

References

[Ang88]
[BAL06] M.-F. Balcan  A.Beygelzimer  and J. Langford. Agnostic active learning. In 23th International

D. Angluin. Queries and concept learning. Machine Learning  2:319–342  1988.

[CN07]

[Das05]

Conference on Machine Learning  2006.
R. Castro and R. Nowak. Minimax bounds for active learning.
Learning Theory  2007.
S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Infor-
mation Processing Systems  2005.

In 20th Annual Conference on

[DHM07] S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In Advances

in Neural Information Processing Systems  2007.
R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press  1999.
E. Friedman. Active learning for smooth problems.
Theory  2009.

In 22th Annual Conference on Learning

[Dud99]
[Fri09]

[Han07]

[Han09]

[GKW03] V.E. Gine  V.I. Koltchinskii  and J. Wellner. Ratio limit theorems for empirical processes. Stochas-

tic Inequalities and Applications  56:249–278  2003.
S. Hanneke. A bound on the label complexity of agnostic active learning. In 24th International
Conference on Machine Learning  2007.
S. Hanneke. Adaptive rates of convergence in active learning.
Learning Theory  2009.

In 22th Annual Conference on

[Kaa06] M. Kaariainen. Active learning in the non-realizable case. In 17th International Conference on

[Tsy04]

Algorithmic Learning Theory  2006.
A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics 
32:135–166  2004.
S. van de Geer. Applications of Empirical Process Theory. Cambridge University Press  2000.

[vdG00]
[vdVW96] A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes with Application to

Statistics. Springer Verlag  1996.

9

,Congchao Wang
Yizhi Wang
Yinxue Wang
Chiung-Ting Wu
Guoqiang Yu