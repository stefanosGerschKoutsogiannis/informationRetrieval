2009,Locality-sensitive binary codes from shift-invariant kernels,This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections  such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g.  a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme  and report favorable experimental performance as compared to a recent state-of-the-art method  spectral hashing.,Locality-Sensitive Binary Codes

from Shift-Invariant Kernels

Maxim Raginsky
Duke University

Durham  NC 27708
m.raginsky@duke.edu

Svetlana Lazebnik
UNC Chapel Hill

Chapel Hill  NC 27599
lazebnik@cs.unc.edu

Abstract

This paper addresses the problem of designing binary codes for high-dimensional
data such that vectors that are similar in the original space map to similar bi-
nary strings. We introduce a simple distribution-free encoding scheme based on
random projections  such that the expected Hamming distance between the bi-
nary codes of two vectors is related to the value of a shift-invariant kernel (e.g.  a
Gaussian kernel) between the vectors. We present a full theoretical analysis of the
convergence properties of the proposed scheme  and report favorable experimental
performance as compared to a recent state-of-the-art method  spectral hashing.

1 Introduction

Recently  there has been a lot of interest in the problem of designing compact binary codes for
reducing storage requirements and accelerating search and retrieval in large collections of high-
dimensional vector data [11  13  15]. A desirable property of such coding schemes is that they
should map similar data points to similar binary strings  i.e.  strings with a low Hamming distance.
Hamming distances can be computed very efﬁciently in hardware  resulting in very fast retrieval of
strings similar to a given query  even for brute-force search in a database consisting of millions of
data points [11  13]. Moreover  if code strings can be effectively used as hash keys  then similarity
searches can be carried out in sublinear time. In some existing schemes  e.g. [11  13]  the notion of
similarity between data points comes from supervisory information  e.g.  two documents are similar
if they focus on the same topic or two images are similar if they contain the same objects. The
binary encoder is then trained to reproduce this “semantic” similarity measure. In this paper  we are
more interested in unsupervised schemes  where the similarity is given by Euclidean distance or by
a kernel deﬁned on the original feature space. Weiss et al. [15] have recently proposed a spectral
hashing approach motivated by the idea that a good encoding scheme should minimize the sum of
Hamming distances between pairs of code strings weighted by the value of a Gaussian kernel be-
tween the corresponding feature vectors. With appropriate heuristic simpliﬁcations  this objective
can be shown to yield a very efﬁcient encoding rule  where each bit of the code is given by the sign
of a sine function applied to a one-dimensional projection of the feature vector. Spectral hashing
shows promising experimental results  but its behavior is not easy to characterize theoretically. In
particular  it is not clear whether the Hamming distance between spectral hashing code strings con-
verges to any function of the Euclidean distance or the kernel value between the original vectors as
the number of bits in the code increases.

In this paper  we propose a coding method that is similar to spectral hashing computationally  but
is derived from completely different considerations  is amenable to full theoretical analysis  and
shows better practical behavior as a function of code size. We start with a low-dimensional mapping
of the original data that is guaranteed to preserve the value of a shift-invariant kernel (speciﬁcally 
the random Fourier features of Rahimi and Recht [8])  and convert this mapping to a binary one
with similar guarantees. In particular  we show that the normalized Hamming distance (i.e.  Ham-

ming distance divided by the number of bits in the code) between any two embedded points sharply
concentrates around a well-deﬁned continuous function of the kernel value. This leads to a Johnson–
Lindenstrauss type result [4] which says that a set of any N points in a Euclidean feature space can
be embedded in a binary cube of dimension O(log N ) in a similarity-preserving way: with high
probability  the binary encodings of any two points that are similar (as measured by the kernel) are
nearly identical  while those of any two points that are dissimilar differ in a constant fraction of their
bits. Using entropy bounds from the theory of empirical processes  we also prove a stronger result
of this type that holds for any compact domain of RD  provided the number of bits is proportional
to the intrinsic dimension of the domain. Our scheme is completely distribution-free with respect to
the data: its structure depends only on the underlying kernel. In this  it is similar to locality sensitive
hashing (LSH) [1]  which is a family of methods for deriving low-dimensional discrete represen-
tations of the data for sublinear near-neighbor search. However  our scheme differs from LSH in
that we obtain both upper and lower bounds on the normalized Hamming distance between any two
embedded points  while in LSH the goal is only to preserve nearest neighbors (see [6] for further dis-
cussion of the distinction between LSH and more general similarity-preserving embeddings). To the
best of our knowledge  our scheme is among the ﬁrst random projection methods for constructing a
similarity-preserving embedding into a binary cube. In addition to presenting a thorough theoretical
analysis  we have evaluated our approach on both synthetic and real data (images from the LabelMe
database [10] represented by high-dimensional GIST descriptors [7]) and compared its performance
to that of spectral hashing. Despite the simplicity and distribution-free nature of our scheme  we
have been able to obtain very encouraging experimental results.

2 Binary codes for shift-invariant kernels

Consider a Mercer kernel K(· ·) on RD that satisﬁes the following for all points x  y ∈ RD:

(K1) It is translation-invariant (or shift-invariant)  i.e.  K(x  y) = K(x − y).
(K2) It is normalized  i.e.  K(x − y) ≤ 1 and K(x − x) ≡ K(0) = 1.
(K3) For any real number α ≥ 1  K(αx − αy) ≤ K(x − y).

The Gaussian kernel K(x  y) = exp(−γkx − yk2/2) or the Laplacian kernel K(x  y) =
exp(−γkx − yk1) are two well-known examples. We would like to construct an embedding F n of
RD into the binary cube {0  1}n such that for any pair x  y the normalized Hamming distance

1
n

dH (F n(x)  F n(y)) △=

1
n

1{Fi(x)6=Fi(y)}

n

Xi=1

between F n(x) = (F1(x)  . . .   Fn(x)) and F n(y) = (F1(y)  . . .   Fn(y)) behaves like

h1(K(x − y)) ≤

1
n

dH (F n(x)  F n(y)) ≤ h2(K(x − y))

where h1  h2 : [0  1] → R+ are continuous decreasing functions  and h1(1) = h2(1) = 0 and
h1(0) = h2(0) = c > 0. In other words  we would like to map D-dimensional real vectors into
n-bit binary strings in a locality-sensitive manner  where the notion of locality is induced by the
kernel K. We will achieve this goal by drawing F n appropriately at random.

Random Fourier features. Recently  Rahimi and Recht [8] gave a scheme that takes a Mercer
kernel satisfying (K1) and (K2) and produces a random mapping Φn : RD → Rn  such that 
with high probability  the inner product of any two transformed points approximates the kernel:
Φn(x)·Φn(y) ≈ K(x−y) for all x  y. Their scheme exploits Bochner’s theorem [9]  a fundamental
result in harmonic analysis which says that any such K is a Fourier transform of a uniquely deﬁned
probability measure PK on RD. They deﬁne the random Fourier features (RFF) via

(1)
where ω ∼ PK and b ∼ Unif[0  2π]. For example  for the Gaussian kernel K(s) = e−γksk2/2  we
take ω ∼ Normal(0  γID×D). With these features  we have E[Φω b(x)Φω b(y)] = K(x − y).
The scheme of [8] is as follows: draw an i.i.d. sample ((ω1  b1)  . . .   (ωn  bn))  where each

Φω b(x)

△

= √2 cos(ω · x + b) 

ωi ∼ PK and bi ∼ Unif[0  2π]  and deﬁne a mapping Φn : RD → Rn via Φn(x)
1√n(cid:0)Φω1 b1 (x)  . . .   Φωn bn (x)(cid:1) for x ∈ X . Then E[Φn(x) · Φn(y)] = K(x − y) for all x  y.
From random Fourier features to random binary codes. We will compose the RFFs with
random binary quantizers. Draw a random threshold t ∼ Unif[−1  1] and deﬁne the quantizer
Qt : [−1  1] → {−1  +1} via Qt(u) △= sgn(u + t)  where we let sgn(u) = −1 if u < 0 and
sgn(u) = +1 if u ≥ 0. We note the following basic fact (we omit the easy proof):
Lemma 2.1 For any u  v ∈ [−1  1]  Pt {Qt(u) 6= Qt(v)} = |u − v|/2.
Now  given a kernel K  we deﬁne a random map Ft ω b : RD → {0  1} through

=

△

Ft ω b(x)

△

=

[1 + Qt (cos(ω · x + b))]  

(2)

1
2

where t ∼ Unif[−1  1]  ω ∼ PK  and b ∼ Unif[0  2π] are independent of one another. From now
on  we will often omit the subscripts t  ω  b and just write F for the sake of brevity. We have:

Lemma 2.2

8
π2

∞

Xm=0

1 − K(mx − my)

 

E 1{F (x)6=F (y)} = hK(x − y) △=

4m2 − 1
Proof: Using Lemma 2.1  we can show E 1{F (x)6=F (y)} = 1
Eω b | cos(ω· x + b)− cos(ω· y + b)|.
Using trigonometric identities and the independence of ω and b  we can express this expectation as
sin(cid:18) ω · (x − y)

Eb ω |cos(ω · x + b) − cos(ω · y + b)| =

We now make use of the Fourier series representation of the full rectiﬁed sine wave g(τ ) = | sin(τ )|:

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

4
π

2

2

.

∀x  y

(3)

g(τ ) =

2
π

+

4
π

∞

Xm=1

1

1 − 4m2 cos(mτ ) =

1 − cos(2mτ )

4m2 − 1

.

Eω(cid:12)(cid:12)(cid:12)(cid:12)
Xm=1

4
π

∞

Using this together with the fact that Eω cos(ω · s) = K(s) for any s ∈ RD [8]  we obtain (3). (cid:4)
Lemma 2.2 shows that the probability that F (x) 6= F (y) is a well-deﬁned continuous function of
x− y. The inﬁnite series in (3) can  of course  be computed numerically to any desired precision. In
addition  we have the following upper and lower bounds solely in terms of the kernel value K(x−y):
Lemma 2.3 Deﬁne the functions

△

△

4

2

△

=

and

h2(u)

h1(u)

√1 − u 

4
π2 (1 − u)

π2 (1 − 2u/3)(cid:27) 

= min(cid:26) 1
where u ∈ [0  1]. Note that h1(0) = h2(0) = 4/π2 ≈ 0.405 and that h1(1) = h2(1) = 0. Then
h1(K(x − y)) ≤ hK(x − y) ≤ h2(K(x − y)) for all x  y.
√E ∆2 (the last step
Proof: Let ∆
uses concavity of the square root). Using the properties of the RFF  E ∆2 = (1/2) E[(Φω b(x) −
Φω b(y))2] = 1 − K(x − y). Therefore  E 1{F (x)6=F (y)} = (1/2) E|∆| ≤ (1/2)p1 − K(x − y).
We also have
π2(cid:0)1− 2K(x− y)/3(cid:1).
E 1{F (x)6=F (y)} =

= cos(ω · x + b) − cos(ω · y + b). Then E|∆| = E√∆2 ≤

8
3π2 K(x− y) =

This proves the upper bound in the lemma. On the other hand  since K satisﬁes (K3) 

K(mx − my)

4m2 − 1

Xm=1

4
π2 −

4
π2 −

8
π2

≤

∞

4

hK(x − y) ≥(cid:0)1 − K(x − y)(cid:1) ·

π2(cid:0)1 − K(x − y)(cid:1) 
because the mth term of the series in (3) is not smaller than(cid:0)1 − K(x − y)(cid:1)/(4m2 − 1).

(cid:4)
Fig. 1 shows a comparison of the kernel approximation properties of the RFFs [8] with our scheme
for the Gaussian kernel.

4m2 − 1

=

1

4

8
π2

∞

Xm=1

(a)

(b)

(c)

Figure 1: (a) Approximating the Gaussian kernel by random features (green) and random signs (red). (b) Rela-
tionship of normalized Hamming distance between random signs to functions of the kernel. The scatter plots in
(a) and (b) are obtained from a synthetic set of 500 uniformly distributed 2D points with n = 5000. (c) Bounds
for normalized Hamming distance in Lemmas 2.2 and 2.3 vs. the Euclidean distance.

Now we concatenate several mappings of the form Ft ω b to construct an embedding of X into the
binary cube {0  1}n. Speciﬁcally  we draw n i.i.d. triples (t1  ω1  b1)  . . .   (tn  ωn  bn) and deﬁne

F n(x)

△

=(cid:0)F1(x)  . . .   Fn(y)(cid:1) 

where Fi(x) ≡ Fti ωi bi (x)  i = 1  . . .   n

As we will show next  this construction ensures that  for any two points x and y  the fraction of the
bits where the binary strings F n(x) and F n(y) disagree sharply concentrates around hK(x − y) 
provided n is large enough. Using the results proved above  we conclude that  for any two points
x and y that are “similar ” i.e.  K(x − y) ∼ 1  most of the bits of F n(x) and F n(y) will agree 
whereas for any two points x and y that are “dissimilar ” i.e.  K(x − y) ∼ 0  F n(x) and F n(y)
will disagree in about 40% or more of their bits.

Analysis of performance. We ﬁrst prove a Johnson–Lindenstrauss type result which says that 
for any ﬁnite subset of RD  the normalized Hamming distance respects the similarities between
points. It should be pointed out that the analogy with Johnson–Lindenstrauss is only qualitative:
our embedding is highly nonlinear  in contrast to random linear projections used there [4]  and the
resulting distortion of the neighborhood structure  although controllable  does not amount to a mere
rescaling by constants.

Theorem 2.4 Fix ǫ  δ ∈ (0  1). For any ﬁnite data set D = {x1  . . .   xN} ⊂ RD  F n is such that
(4)

hK(xj − xk) − δ ≤

1
n
1
n

dH (F n(xj)  F n(xk)) ≤ hK(xj − xk) + δ
dH (F n(xj)  F n(xk)) ≤ h2(K(xj − xk)) + δ

h1(K(xj − xk)) − δ ≤

(5)
for all j  k with probability ≥ 1 − N 2e−2nδ2. Moreover  the events (4) and (5) will hold with
probability ≥ 1 − ǫ if n ≥ (1/2δ2) log(N 2/ǫ). Thus  any N -point subset of RD can be embedded 
with high probability  into the binary cube of dimension O(log N ) in a similarity-preserving way.

The proof (omitted) is by a standard argument using Hoeffding’s inequality and the union bound  as
well as the bounds of Lemma 2.3. We also prove a much stronger result: any compact subset X ⊂
RD can be embedded into a binary cube whose dimension depends only on the intrinsic dimension
and the diameter of X and on the second moment of PK  such that the normalized Hamming distance
behaves in a similarity-preserving way for all pairs of points in X simultaneously. We make use of
the following [5]:
Deﬁnition 2.5 The Assouad dimension of X ⊂ RD  denoted by dX   is the smallest integer k  such
that  for any ball B ⊂ RD  the set B ∩ X can be covered by 2k balls of half the radius of B.
The Assouad dimension is a widely used measure of the intrinsic dimension [2  6  3]. For example 
if X is an ℓp ball in RD  then dX = O(D); if X is a d-dimensional hyperplane in RD  then
dX = O(d) [2]. Moreover  if X is a d-dimensional Riemannian submanifold of RD with a suitably
bounded curvature  then dX = O(d) [3]. We now have the following result:

Theorem 2.6 Suppose that the kernel K is such that LK
exists a constant C > 0 independent of D and K  such that the following holds. Fix any ǫ  δ > 0. If

△

n ≥ max(cid:26) CLKdX diamX

δ2

 

= pEω∼PK kωk2 < +∞. Then there
δ2 log(cid:18) 2

ǫ(cid:19)(cid:27)  

2

then  with probability at least 1 − ǫ  the mapping F n is such that  for every pair x  y ∈ X  

hK(x − y) − δ ≤

dH (F n(x)  F n(y)) ≤ hK(x − y) + δ

1
n

(6)

Proof: For every pair x  y ∈ X   let Ax y be the set of all θ ≡ (t  ω  b)  such that Ft ω b(x) 6=
Ft ω b(y)  and let A = {Ax y : x  y ∈ X}. Then we can write
Xi=1

dH (F n(x)  F n(y)) =

1{θi∈Ax y}.

For any sequence θn = (θ1  . . .   θn)  deﬁne the uniform deviation

1
n

1
n

n

∆(θn)

△

= sup

1
n

n

Xi=1

x y∈X(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1{θi∈Ax y} − E 1{Ft ω b(x)6=Ft ω b(y)}(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.

For every 1 ≤ i ≤ n and an arbitrary θ′i  let θn
Then |∆(θn) − ∆(θn

(7)
Now we need to bound Eθn ∆(θn). Using a standard symmetrization technique [14]  we can write

(i))| ≤ 1/n for any i and any θ′i. Hence  by McDiarmid’s inequality 
P {|∆(θn) − Eθn ∆(θn)| > β} ≤ 2e−2nβ 2

(i) denote θn with the ith component replaced by θ′i.

∀β > 0.

 

Eθn ∆(θn) ≤ 2R(A)

△

x y∈X(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= 2 Eθn σn" sup

1
n

n

Xi=1

σi1{θi∈Ax y}(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

# 

(8)

2

′  y

′  y

′(cid:13)(cid:13)

R(A) ≤

C0√nZ ∞

where σn = (σ1  . . .   σn) is an i.i.d. Rademacher sequence  P{σi = −1} = P(σi = +1} = 1/2.
The quantity R(A) can be bounded by the Dudley entropy integral [14]
0 qlog N (ǫ A k · kL2(µ))dǫ 

(9)
where C0 > 0 is a universal constant  and N (ǫ A k · kL2(µ)) is the ǫ-covering number of the
function class {θ 7→ 1{θ∈A} : A ∈ A} with respect to the L2(µ) norm  where µ is the distribution
of θ ≡ (t  ω  b). We will bound these covering numbers by the covering numbers of X with respect
to the Euclidean norm on RD. It can be shown that  for any four points x  x′  y  y′ ∈ X  
L2(µ) =Z (cid:0)1{θ∈Ax y} − 1{θ∈Ax
′}(cid:1)2
(cid:13)(cid:13)1Ax y − 1Ax
dµ(θ) ≤ µ(Bx△Bx′ ) + µ(By△By′ ) 
△= {(t  ω  b) : Qt(cos(ω · x + b)) = +1}
where △ denotes symmetric difference of sets  and Bx
(details omitted for lack of space). Now 
2µ (Bx△Bx′ ) = 2 Eω bh Pt(cid:8)Qt(cos(ω · x + b)) 6= Qt(cos(ω · y + b))(cid:9)i
= Eω b |cos(ω · x + b) − cos(ω · x′ + b)| ≤ Eω |ω · (x − x′)| ≤ LKkx − x′k.
Then µ (Bx△Bx′) + µ (By△By′ ) ≤ LK
This implies that
2 (kx − x′k + ky − y′k).
N (ǫ A k · kL2(µ)) ≤ N (ǫ2/LK X  k·k)2  where N (δ X  k·k) are the covering numbers of X w.r.t.
the Euclidean norm k·k. By deﬁnition of the Assouad dimension  N (δ X  k·k) ≤ (2 diamX /δ)dX  
so N (ǫ A k · kL2(µ)) ≤(cid:0) 2LK diam X
for some constant C1 > 0. From (10) and (8)  we obtain Eθn ∆(θn) ≤ C2q LK dX diam X
C2 = 2C1. Using this and (7) with β = δ/2  we obtain (6) with C = 16C2
2.
For example  with the Gaussian kernel K(s) = e−γksk2/2 on RD  we have LK = √Dγ. The kernel
bandwidth γ is often chosen as γ ∝ 1/[D(diamX )2] (see  e.g.  [12  Sec. 7.8]); with this setting 
the number of bits needed to guarantee the bound (6) is n = Ω((dX /δ2) log(1/ǫ)). It is possible 
in principle  to construct a dimension-reducing embedding of X into a binary cube  provided the
number of bits in the embedding is larger than the intrinsic dimension of X .

(cid:1)2dX . We can now estimate the integral in (9) by
R(A) ≤ C1r LKdX diamX

  where

(10)

n

(cid:4)

ǫ2

n

 

Our method

Spectral hashing

(a)

(c)

(e)

(b)

(d)

(f)

Figure 2: Synthetic results. First row: scatter plots of normalized Hamming distance vs. Euclidean distance
for our method (a) and spectral hashing (b) with code size 32 bits. Green indicates pairs of data points that
are considered true “neighbors” for the purpose of retrieval. Second row: scatter plots for our method (c) and
spectral hashing (d) with code size 512 bits. Third row: recall-precision plots for our method (e) and spectral
hashing (f) for code sizes from 8 to 512 bits (best viewed in color).

3 Empirical Evaluation

In this section  we present the results of our scheme with a Gaussian kernel  and compare our perfor-
mance to spectral hashing [15].1 Spectral hashing is a recently introduced  state-of-the-art approach
that has been reported to obtain better results than several other well-known methods  including
LSH [1] and restricted Boltzmann machines [11]. Unlike our method  spectral hashing chooses
code parameters in a deterministic  data-dependent way  motivated by results on convergence of

1We use the code made available by the authors of [15] at http://www.cs.huji.ac.il/˜yweiss/SpectralHashing/.

Our method

Spectral hashing

Figure 3: Recall-precision curves for the LabelMe database for our method (left) and for spectral hashing
(right). Best viewed in color.

eigenvectors of graph Laplacians to Laplacian eigenfunctions on manifolds. Though spectral hash-
ing is derived from completely different considerations than our method  its encoding scheme is
similar to ours in terms of basic computation. Namely  each bit of a spectral hashing code is given
by sgn(cos(k ω · x))  where ω is a principal direction of the data (instead of a randomly sampled
direction  as in our method) and k is a weight that is deterministically chosen according to the ana-
lytical form of certain kinds of Laplacian eigenfunctions. The structural similarity between spectral
hashing and our method makes comparison between them appropriate.

To demonstrate the basic behavior of our method  we ﬁrst report results for two-dimensional syn-
thetic data using a protocol similar to [15] (we have also conducted tests on higher-dimensional
synthetic data  with very similar results). We sample 10 000 “database” and 1 000 “query” points
from a uniform distribution deﬁned on a 2d rectangle with aspect ratio 0.5. To distinguish true posi-
tives from false positives for evaluating retrieval performance  we select a “nominal” neighborhood
radius so that each query point on average has 50 neighbors in the database. Next  we rescale the
data so that this radius is 1  and set the bandwidth of the kernel to γ = 1. Fig. 2 (a c) shows scatter
plots of normalized Hamming distance vs. Euclidean distance for each query point paired with each
database point for 32-bit and 512-bit codes. As more bits are added to our code  the variance of the
scatter plots decreases  and the points cluster tighter around the theoretically expected curve (Eq. (3) 
Fig. 1). The scatter plots for spectral hashing are shown in Fig. 2 (b d). As the number of bits in the
spectral hashing code is increased  normalized Hamming distance does not appear to converge to any
clear function of the Euclidean distance. Because the derivation of spectral hashing in [15] includes
several heuristic steps  the behavior of the resulting scheme appears to be difﬁcult to analyze  and
shows some undesirable effects as the code size increases. Figure 2 (e f) compares recall-precision
curves for both methods using a range of code sizes. Since the normalized Hamming distance for
our method converges to a monotonic function of the Euclidean distance  its performance keeps
improving as a function of code size. On the other hand  spectral hashing starts out with promising
performance for very short codes (up to 32 bits)  but then deteriorates for higher numbers of bits.

Next  we present retrieval results for 14 871 images taken from the LabelMe database [10]. The
images are represented by 320-dimensional GIST descriptors [7]  which have proven to be effective
at capturing perceptual similarity between scenes. For this experiment  we randomly select 1 000
images to serve as queries  and the rest make up the “database.” As with the synthetic experiments  a
nominal threshold of the average distance to the 50th nearest neighbor is used to determine whether
a database point returned for a given query is considered a true positive. Figure 3 shows precision-
recall curves for code sizes ranging from 16 bits to 1024 bits. As in the synthetic experiments 
spectral hashing appears to have an advantage over our method for extremely small code sizes  up to
about 32 bits. However  this low bit regime may not be very useful in practice  since below 32 bits 
neither method achieves performance levels that would be satisfactory for real-world applications.
For larger code sizes  our method begins to dominate. For example  with a 128-bit code (which is
equivalent to just two double-precision ﬂoating point numbers)  our scheme achieves 0.8 precision

Euclidean neighbors

32 bit code

512 bit code

Precision: 0.81

Precision: 1.00

Precision: 0.38

Precision: 0.96

Figure 4: Examples of retrieval for two query images on the LabelMe database. The left column shows top
48 neighbors for each query according to Euclidean distance (the query image is in the top left of the collage).
The middle (resp. right) column shows nearest neighbors according to normalized Hamming distance with a
32-bit (resp. 512-bit) code. The precision of retrieval is evaluated as the proportion of top Hamming neighbors
that are also Euclidean neighbors within the “nominal” radius. Incorrectly retrieved images in the middle and
right columns are shown with a red border. Best viewed in color.

at 0.2 recall  whereas spectral hashing only achieves about 0.5 precision at the same recall. More-
over  the performance of spectral hashing actually begins to decrease for code sizes above 256 bits.
Finally  Figure 4 shows retrieval results for our method on a couple of representative query images.

In addition to being completely distribution-free and exhibiting more desirable behavior as a func-
tion of code size  our scheme has one more practical advantage. Unlike spectral hashing  we retain
the kernel bandwidth γ as a “free parameter ” which gives us ﬂexibility in terms of adapting to target
neighborhood size  or setting a target Hamming distance for neighbors at a given Euclidean dis-
tance. This can be especially useful for making sure that a signiﬁcant fraction of neighbors for each
query are mapped to strings whose Hamming distance from the query is no greater than 2. This is a
necesary condition for being able to use binary codes for hashing as opposed to brute-force search
(although  as demonstrated in [11  13]  even brute-force search with binary codes can already be
quite fast). To ensure high recall within a low Hamming radius  we can progressively increase the
kernel bandwidth γ as the code size increases  thus counteracting the increase in unnormalized Ham-
ming distance that inevitably accompanies larger code sizes. Preliminary results (omitted for lack of
space) show that this strategy can indeed increase recall for low Hamming radius while sacriﬁcing
some precision. In the future  we will evaluate this tradeoff more extensively  and test our method
on datasets consisting of millions of data points. At present  our promising initial results  combined
with our comprehensive theoretical analysis  convincingly demonstrate the potential usefulness of
our scheme for large-scale indexing and search applications.

Acknowledgments

This work was supported by NSF CAREER Award No. IIS 0845629.

References

[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high

dimensions. Commun. ACM  51(1):117–122  2008.

[2] K. Clarkson. Nearest-neighbor searching and metric space dimensions. In Nearest-Neighbor Methods for

Learning and Vision: Theory and Practice  pages 15–59. MIT Press  2006.

[3] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. In STOC  2008.
[4] S. Dasgupta and A. Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random

Struct. Alg.  22(1):60–65  2003.

[5] J. Heinonen. Lectures on Analysis on Metric Spaces. Springer  New York  2001.
[6] P. Indyk and A. Naor. Nearest-neighbor-preserving embeddings. ACM Trans. Algorithms  3(3):Art. 31 

2007.

[7] A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial enve-

lope. Int. J. Computer Vision  42(3):145–175  2001.

[8] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  2007.
[9] M. Reed and B. Simon. Methods of Modern Mathematical Physics II: Fourier Analysis  Self-Adjointness.

Academic Press  1975.

[10] B. Russell  A. Torralba  K. Murphy  and W. T. Freeman. LabelMe: a database and web-based tool for

image annotation. Int. J. Computer Vision  77:157–173  2008.

[11] R. Salakhutdinov and G. Hinton. Semantic hashing. In SIGIR Workshop on Inf. Retrieval and App. of

Graphical Models  2007.

[12] B. Sch¨olkopf and A. J. Smola. Learning With Kernels. MIT Press  2002.
[13] A. Torralba  R. Fergus  and Y. Weiss. Small codes and large databases for recognition. In CVPR  2008.
[14] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer  1996.
[15] Y. Weiss  A. Torralba  and R. Fergus. Spectral hashing. In NIPS  2008.

,Leon Gatys
Alexander Ecker
Matthias Bethge