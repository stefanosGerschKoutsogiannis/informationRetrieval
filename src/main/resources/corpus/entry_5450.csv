2017,The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities,Non-parametric multivariate density estimation faces strong statistical and computational bottlenecks  and the more practical approaches impose near-parametric assumptions on the form of the density functions. In this paper  we leverage recent developments to propose a class of non-parametric models which have very attractive computational and statistical properties. Our approach relies on the simple function space assumption that the conditional distribution of each variable conditioned on the other variables has a non-parametric exponential family form.,The Expxorcist: Nonparametric Graphical Models

Via Conditional Exponential Densities

Arun Sai Suggala ∗

Carnegie Mellon University

Pittsburgh  PA 15213

Mladen Kolar †

University of Chicago

Chicago  IL 60637

Pradeep Ravikumar ‡
Carnegie Mellon University

Pittsburgh  PA 15213

Abstract

Non-parametric multivariate density estimation faces strong statistical and com-
putational bottlenecks  and the more practical approaches impose near-parametric
assumptions on the form of the density functions. In this paper  we leverage re-
cent developments to propose a class of non-parametric models which have very
attractive computational and statistical properties. Our approach relies on the
simple function space assumption that the conditional distribution of each variable
conditioned on the other variables has a non-parametric exponential family form.

1

Introduction

Let X = (X1  . . .   Xp) be a p-dimensional random vector. Let G = (V  E) be the graph that encodes
conditional independence assumptions underlying the distribution of X  that is  each node of the
graph corresponds to a component of vector X and (a  b) ∈ E if and only if Xa (cid:54)⊥⊥ Xb | X¬ab with
X¬ab := {Xc | c ∈ V \{a  b}}. The graphical model represented by G is then the set of distributions
over X that satisfy the conditional independence assumptions speciﬁed by the graph G.
There has been a considerable line of work on learning parametric families of such graphical model
distributions from data [22  20  13  28]  where the distribution is indexed by a ﬁnite-dimensional
parameter vector. The goal of this paper  however  is on specifying and learning nonparametric
families of graphical model distributions  indexed by inﬁnite-dimensional parameters  and for which
there has been comparatively limited work. Non-parametric multivariate density estimation broadly 
even without the graphical model constraint  has not proved as popular in practical machine learning
contexts  for both statistical and computational reasons. Loosely  estimating a non-parametric
multivariate density  with mild assumptions  typically requires the number of samples to scale
exponentially in the dimension p of the data  which is infeasible even in the big-data era when n is
very large. And the resulting estimators are typically computationally expensive or intractable  for
instance requiring repeated computations of multivariate integrals.
We present a review of multivariate density estimation  that is necessarily incomplete but sets up
our proposed approach. A common approach dating back to [15] uses the logistic density transform
X0 ∈ X or(cid:82)
(cid:82)
to satisfy the unity and positivity constraints for densities  and considers densities of the form
f (X) = exp(η(X))
X exp(η(x))dx  with some constraints on η for identiﬁability such as η(X0) = 0 for some

X η(x)dx = 0.

With the logistic density transform  differing approaches for non-parametric density estimation can
be contrasted in part by their assumptions on the inﬁnite-dimensional function space domain of η(·).
An early approach [8] considered function spaces of functions with bounded “roughness” functionals.
The predominant line of work however has focused on the setting where η(·) lies in a Reproducing
Kernel Hilbert Space (RKHS)  dating back to [21]. Consider the estimation of these logistic density

∗asuggala@cs.cmu.edu

†mkolar@chicagobooth.edu

‡pradeepr@cs.cmu.edu

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

i∈[n] η(X (i)) + log(cid:82) exp(η(x))dx + λ pen(η)  for
(cid:80)

transforms η(X) given n i.i.d. samples Xn = {X (i)}n
i=1 drawn from fη(X). A natural loss
functional is penalized log likelihood  with a penalty functional that ensures a smooth ﬁt with respect
to the function space domain: (cid:96)(η; Xn) := − 1
functions η(·) that lie in an RKHS H  and where pen(η) = (cid:107)η(cid:107)2H is the squared RKHS norm. This
was studied by many [21  11  6]. A crucial caveat is that the representer theorem for RKHSs does not
hold. Nonetheless  one can consider ﬁnite-dimensional function space approximations consisting
of the linear span of kernel functions evaluated at the sample points [12]. Computationally this still
scales poorly with the dimension due to the need to compute multidimensional integrals of the form

(cid:82) exp(η(x)dx which do not  in general  decompose. These approximations also do not come with

n

s=1 ηs(Xs) +(cid:80)p

s=1

(cid:80)p

pairwise terms: η(X) =(cid:80)p

i∈[n] exp(−η(X (i)))+(cid:82) η(x)ρ(x)dx+λpen(η)  where ρ(X) is some ﬁxed known
(cid:80)

strong statistical guarantees.
We brieﬂy note that the function space assumption that η(·) lies in an RKHS could also be viewed
from the lens of an inﬁnite-dimensional exponential family [4]. Speciﬁcally  let H be a Repro-
ducing Kernel Hilbert Space with reproducing kernel k(· ·)  and inner product (cid:104)· ·(cid:105)H. Then
η(X) = (cid:104)θ(·)  k(X ·)(cid:105)H  so that the density f (X) can in turn be viewed as a member of an
inﬁnite-dimensional exponential family with sufﬁcient statistics k(X ·) : X (cid:55)→ H  and natural
parameter θ(·) ∈ H. Following this viewpoint  [4] propose estimators via linear span approximations
similar to [11].
Due to the computational caveat with exact likelihood based functionals  a line of approaches
have focused on penalized surrogate likelihoods instead. [14] study the following loss functional:
(cid:96)(η; Xn) := 1
density with the same support as the unknown density f (X). While this estimation procedure is
much more computationally amenable than minimizing the exact penalized likelihood  the caveat 
however  is that for a general RKHS this requires solving higher order integrals. The next level of
simpliﬁcation has thus focused on the form of the logistic transform function itself. There has been a
line of work on an ANOVA type decomposition of the logistic density function into node-wise and
t=s+1 ηst(Xs  Xt). A line of work has coupled
such a decomposition with the assumption that each of the terms lie in an RKHS. This does not
immediately provide a computational beneﬁt: with penalized likelihood based loss functionals  the
loss functional does not necessarily decompose into such node and pairwise terms. [24] thus couple
this ANOVA type pairwise decomposition with a score matching based objective. [10] use the above
decomposition with the surrogate loss functional of [14] discussed above  but note that this still
requires the aforementioned function space approximation as a linear span of kernel evaluations  as
well as two-dimensional integrals.
A line of recent work has thus focused on further stringent assumptions on the density function space 
by assuming some components of the logistic transform to be ﬁnite-dimensional. [30] use an ANOVA
decomposition but assume the terms belong to ﬁnite-dimensional function spaces instead of RKHSs 
speciﬁed by a pre-deﬁned ﬁnite set of basis functions. [29] consider logistic transform functions η(·)
that have the pairwise decomposition above  with a speciﬁc class of parametric pairwise functions
βstXsXt  and non-parametric node-wise functions. [17  16] consider the problem of estimating
monotonic node-wise functions such that the transformed random vector is multivariate Gaussian;
which could also be viewed as estimating a Gaussian copula density.
To summarize the (necessarily incomplete) review above  non-parametric density estimation faces
strong statistical and computational bottlenecks  and the more practical approaches impose stringent
near-parametric assumptions on the form of the (logistic transform of the) density functions. In this
paper  we leverage recent developments to propose a very computationally simple non-parametric
density estimation algorithm  that still comes with strong statistical guarantees. Moreover  the
density could be viewed as a graphical model distribution  with a corresponding sparse conditional
independence graph.
Our approach relies on the following simple function space assumption: that the conditional distri-
bution of each variable conditioned on the other variables has a non-parametric exponential family
form. As we show  for there to exist a consistent joint density  the logistic density transform with
respect to a particular base measure necessarily decomposes into the following semi-parametric
t=s+1 θst Bs(Xs) Bt(Xt) in the pairwise case  with
both a parametric component {θs : s = 1  . . .   p} {θst : s < t; s  t = 1  . . .   p}  as well as
non-parametric components {Bs : s = 1  . . .   p}. We call this class of models the “expxorcist”  fol-

s=1 θsBs(Xs) +(cid:80)p

form: η(X) = (cid:80)p

n

(cid:80)p

s=1

2

lowing other “ghostbusting” semi-parametric models such as the nonparanormal and nonparanormal
skeptic [17  16].
Since the conditional distributions are exponential families  we show that there exist computationally
amenable estimators  even in our more general non-parametric setting  where the sufﬁcient statistics
have to be estimated as well. The statistical analysis in our non-parametric setting however is more
subtle  due in part to non-convexity and in part to the non-parametric setting. We also show how the
Expxorcist class of densities is closely related to a semi-parametric exponential family copula density
that generalizes the Gaussian copula density of [17  16]. We corroborate the applicability of our class
of models with experiments on synthetic and real data sets.

2 Multivariate Density Speciﬁcation via Conditional Densities

(cid:82)

Xs

(cid:82)

We are interested in the approach of estimating a multivariate density by estimating node-conditional
densities. Since node-conditional densities focus on the density of a single variable  though condi-
tioned on the rest of the variables  estimating these is potentially a simpler problem  both statistically
and computationally  than estimating the entire joint density itself. Let us consider the general
non-parametric conditional density estimation problem. Given the general multivariate density
f (X) = exp(η(X))
X exp(η(x))dx  the conditional density of a variable Xs given the rest of the variables X−s
is given by f (Xs | X−s) = exp(η((Xs X−s)))
exp(η((x X−s)))dx  which does not have a multi-dimensional integral 
but otherwise does not have a computationally amenable form. There has been a line of work on such
conditional density estimation  mirroring developments in multivariate density estimation [9  18  23] 
but unlike parametric settings  there are no large sample complexity gains with non-parametric
conditional density estimation under general settings. There have also been efforts to use ANOVA
decompositions in a conditional density context [31  26].
In addition to computational and sample complexity caveats  recall that in our context  we would
like to use conditional density estimates to infer a joint multivariate density. A crucial caveat with
using the above estimates to do so is that it is not clear when the estimated node-conditional densities
would be consistent with a joint multivariate density. There has been a line of work on this question
(of when conditional densities are consistent with a joint density) for parametric densities; see [1] for
an overview  with more recent results in [27  5  2  25]. Overall  while estimating node-conditional
densities could be viewed as surrogate estimation of a joint density  arbitrary node-conditional
distributions need not be consistent in general with any joint density. There has however been a line
of work in recent years [3  28]  where it was shown that when the node-conditional distributions
belong to an exponential family  then under certain conditions on their parameterization  there do
exist multivariate densities consistent with the node-conditional densities. In the next section  we
leverage these results towards non-parametric estimation of conditional densities.

3 Conditional Densities of an Exponential Family Form

We ﬁrst recall the deﬁnition of an exponential family in the context of a conditional density.
Deﬁnition 1. A conditional density of a random variable Y ∈ Y given covariates Z :=
(Z1  . . .   Zm) ∈ Z is said to have an exponential family form if it can be written as f (Y | Z) =
exp(B(Y )T E(Z) + C(Y ) + D(Z))  for some functions B : Y (cid:55)→ Rk (for some ﬁnite integer k > 0) 
E : Z (cid:55)→ Rk  C : Y (cid:55)→ R and D : Z (cid:55)→ R.
Thus  f (Y | Z) belongs to a ﬁnite-dimensional exponential family with sufﬁcient statistics B(Y ) 
base measure exp(C(Y ))  and with natural parameter E(Z) and where −D(Z) is the log-partition
function. Contrast this with a general conditional density f (Y | Z) = exp(h(Y  Z) + C(Y ) + D(Z))
with respect to the base measure exp(C(Y )) and −D(Z) being the log-normalization constant  and it
can be seen that a conditional density of the exponential family form has its logistic density transform
h(Y  Z) that factorizes as B(Y )T E(Z).
Consider the case where the sufﬁcient statistic function is real-valued. The non-parametric estimation
problem of a conditional density of exponential form then reduces to the estimation of the sufﬁcient
statistics function B(·)  the exponential natural parameter function E(·)  assuming the base measure
C(·) is given. But when would such estimated conditional densities be consistent with a joint density?

3

To answer this question  we draw upon developments in [28]. Suppose that the node-conditional
distributions of each random variable Xs conditioned on the rest of random variables have the
exponential family form as in Deﬁnition 1  so that for each s ∈ V

P(Xs | X−s) ∝ exp{Es(X−s)Bs(Xs) + Cs(Xs)}  

(1)
for some arbitrary functions Es(·)  Bs(·)  Cs(·) that specify a valid conditional density. Then [28]
show that these node-conditional densities are consistent with a unique joint density over the random
(cid:81)
vector X  that moreover factors according to a set of cliques C in the graph G  if and only if
the functions {Es(·)}s∈V specifying the node-conditional distributions have the form Es(X−s) =
t∈C t(cid:54)=s Bt(Xt)  where {θs} ∪ {θC}C∈C is a set of parameters. Moreover  the
(cid:110)(cid:88)

corresponding consistent joint distribution has the following form

θs +(cid:80)

C∈C:s∈C θC

(cid:88)

(cid:88)

(cid:89)

(cid:111)

θsBs(Xs) +

s∈V

C∈C θC

Bs(Xs) +

s∈C

s∈V

Cs(Xs)

.

(2)

P(X) ∝ exp

joint density is with respect to a given product base measure(cid:81)

In this paper  we are interested in the non-parametric estimation of the Expxorcist class of densities
in (2)  where we estimate both the ﬁnite-dimensional parameters {θs} ∪ {θC}C∈C  as well as the
functions {Bs(Xs)}s∈V . We assume we are given the base measures {Cs(Xs)}s∈V   so that the
s∈V exp(Cs(XS))  as is common
in the multivariate density estimation literature. Note that this is not a very restrictive assumption.
In practice the base measure at each node can be well approximated using the empirical univariate
marginal density of that node. We could also extend our algorithm  which we present next  to estimate
the base measures along with sufﬁcient statistic functions.

4 Regularized Conditional Likelihood Estimation for Exponential Family

Form Densities

We consider the nonparametric estimation problem of estimating a joint density of the form in (2) 
focusing on the pairwise case where the factors have size at most k = 2  so that the joint density
takes the form

(cid:26)(cid:88)

P(X) ∝ exp

(cid:88)

(cid:17)

(cid:27)

(cid:27)

(cid:88)

(cid:16)

(cid:26)

(cid:88)

θsBs(Xs) +

s∈V

(s t)∈E

θstBs(Xs) Bt(Xt) +

Cs(Xs)

.

(3)

s∈V

As detailed in the previous section  estimating this joint density can be reduced to estimating its
node-conditional densities  which take the form

P(Xs | X−s) ∝ exp

.

Bs(Xs)

θs +

t∈NG(s)

+ Cs(Xs)

θstBt(Xt)

(4)
We now introduce some notation which we use in the sequel. Let Θ = {θs}s∈V ∪ {θst}s(cid:54)=t and
Θs = θs ∪ {θst}t∈V \{s}. Let B = {Bs}s∈V be the set of sufﬁcient statistics. Let Xs be the domain
of Xs  which we assume is bounded and L2(Xs) be the Hilbert space of square integrable functions
over Xs with respect to Lebesgue measure. We assume that the sufﬁcient statistics Bs(·) ∈ L2(Xs).
Note that the model in Equation (3) is unidentiﬁable. To overcome this issue we impose additional
Bs(X)dX = 0 

constraints on its parameters. Speciﬁcally  we require Bs(Xs) to satisfy (cid:82)
(cid:82)

Xs
Optimization objective: Let Xn = {X (1)  . . . X (n)} be n i.i.d. samples drawn from a joint density
of the form in Equation (3)  with parameters Θ∗  B∗. And let Ls(Θs  B; Xn) be the node conditional
negative log likelihood at node s
Ls(Θs  B; Xn) =

Bs(X)2dX = 1 and θs ≥ 0  ∀s ∈ V .

(cid:88)n

+ A(X (i)−s; Θs  B)

−Bs(X (i)
s )

θstBt(X (i)
t )

(cid:88)

(cid:26)

(cid:18)

(cid:27)

 

(cid:19)

θs +

Xs

1
n

i=1

t∈V \s

where A(X−s; Θs  B) is the log partition function. To estimate the unknown parameters  we solve
the following regularized node conditional log-likelihood estimation problem at each node s ∈ V

s.t. θs ≥ 0 (cid:82)

min
Θs B

Bt(X)dX = 0 (cid:82)

Xt

Ls(Θs  B; Xn) + λn(cid:107)Θs(cid:107)1

Bt(X)2dX = 1 ∀t ∈ V.

Xt

(5)

4

The equality constraints on the norm of functions Bt(·) makes the above optimization problem a
difﬁcult one to solve. While the norm constraints on Bt(·) ∀t ∈ V \ s can be handled through re-
parametrization  the constraint on Bs(·) can not be handled efﬁciently. To make the optimization more
amenable for numerical optimization techniques  we solve a closely related optimization problem.
At each node s ∈ V   we consider the following re-parametrization of B: Bs(Xs) ← θsBs(Xs) 
Bt(Xt) ← (θst/θs)Bt(Xt) ∀t ∈ V \ {s}. With a slight abuse of notation we redeﬁne Ls using this
re-parametrization as
Ls(B; Xn) =

(cid:88)n

−Bs(X (i)
s )

+ A(X (i)−s; B)

Bt(X (i)
t )

(cid:26)

(cid:27)

(cid:18)

(cid:19)

1 +

 

(6)

1
n

i=1

where A(X−s; B) is the log partition function. We solve the following optimization problem  which
is closely related to the original optimization in Equation (5)

t∈V \s

(cid:88)
(cid:113)(cid:82)

(cid:80)

min

B

Ls(B; Xn) + λn

s.t. (cid:82)

Xt

t∈V

Bt(X)2dX

Xt

Bt(X)dX = 0 ∀t ∈ V.

(7)

For more details on the relation between (5) and (7)  please refer to Appendix.
Algorithm: We now present our algorithm for optimization of (7). In the sequel  for simplicity 
we assume that the domains Xt of random variables Xt are all the same and equal to X . In order to
estimate functions Bt  we expand them over a uniformly bounded  orthonormal basis {φk(·)}∞
k=0 of
L2(X ) with φ0(·) ∝ 1. Expansion of the functions Bt(·) over this basis yields
αt kφk(X)+ρt m(X) where ρt m(X) = αt 0φ0(X)+

(cid:88)∞

Note that the constraint(cid:82)
basis expansion to the top m terms and approximate Bt(·) as(cid:80)m

X Bt(X)dX = 0 in Equation (7)  translates to αt 0 = 0. To convert the
inﬁnite dimensional optimization problem in (7) into a ﬁnite dimensional problem  we truncate the
k=1 αt kφk(·). The optimization

(cid:88)m

αt kφk(X).

Bt(X) =

k=m+1

k=1

problem in Equation (7) can then be rewritten as

(cid:88)

t∈V

Ls m(αm; Xn) + λn

(cid:107)αt m(cid:107)2 

(8)

where αt m = {αt k}m

Ls m(αm; Xn) =

1
n

i=1

min
αm

− m(cid:88)

k=1

k=1  αm = {αt m}t∈V and Ls m is deﬁned as
n(cid:88)
m(cid:88)

(cid:88)

αs kφk(X (i)
s )

1 +

αt lφl(X (i)
t )

t∈V \{s}

l=1

 .
 + A(X (i)−s; αm)

Iterative minimization of (8): Note that the objective in (8) is non-convex. In this work  we use
a simple alternating minimization technique for its optimization. In this technique  we alternately
minimize αs m  {αt m}t∈V \s while ﬁxing the other parameters. The resulting optimization problem
in each of the alternating steps is convex. We use Proximal Gradient Descent to optimize these
sub-problems. To compute the objective and its gradients  we need to numerically evaluate the
one-dimensional integrals in the log partition function. To do this  we choose a uniform grid of points
over the domain and use quadrature rules to approximate the integrals.

Convergence: Although (8) is non-convex  we can show that under certain conditions on the
objective function  the alternating minimization procedure converges to the global minimum. In a
recent work [32] analyze alternating minimization for low rank matrix factorization problems and
show that it converges to a global minimum if the sequence of convex problems are strongly convex
and satisfy certain other regularity condition. The analysis of [32] can be extended to show global
convergence of alternating minimization for (8).

5 Statistical Properties

In this section we provide parameter estimation error rates for the node conditional estimator in
Equation (8). Note that these rates are for the re-parameterized model described in Equation (6) and
can be easily translated to guarantees on the original model described in Equations (3)  (4).

5

t be the coefﬁcients of B∗

k=0 by αt  which is an inﬁnite dimensional vector and let α∗

Note that(cid:82) Bt(X)2dX = (cid:107)αt(cid:107)2
zeros. Finally  we deﬁne the norm R(·) as R(αm) =(cid:80)

Notation: Let B2(x  r) = {y : (cid:107)y − x(cid:107)2 ≤ r} be the (cid:96)2 ball with center x and radius r. Let
t (·)}t∈V be the true functions of the re-parametrized model  which we would like to estimate
{B∗
from the data. Denote the basis expansion coefﬁcients of Bt(·) with respect to orthonormal basis
{φk(·)}∞
t (·).
And let αt m be the coefﬁcients corresponding to the top m basis in the basis expansion of Bt(·).
2. Let α = {αt}t∈V and αm = {αt m}t∈V . Let ¯Ls m(αm) =
E [Ls m(αm; Xn)] be the population version of the sample loss deﬁned in Equation (8). We will often
omit Xn from Ls m(αm; Xn) when clear from the context. We let (αt − αt m) be the difference
between inﬁnite dimensional vector αt and the vector obtained by appropriately padding αt m with
t∈V (cid:107)αt m(cid:107)2 and its dual as R∗(αm) =
supt∈V (cid:107)αt m(cid:107)2. The norms on inﬁnite dimensional vector α are similarly deﬁned.
We now state our key assumption on the loss function Ls m(·). This assumption imposes strong
curvature condition on Ls m along certain directions in a ball around α∗
m.
Assumption 1. There exists rm > 0 and constants c  κ > 0 such that for any ∆m ∈ B2(0  rm) the
m)  ∆m(cid:105) ≥ κ(cid:107)∆m(cid:107)2
2−
gradient of the sample loss Ls m satisﬁes: (cid:104)∇Ls m(α∗
c

m + ∆m) − ∇Ls m(α∗

(cid:113) m log(p)

n R(∆m).

Similar assumptions are increasingly common in analysis of non-convex estimators  see [19] and
references therein. We are now ready to state our results which give the parameter estimation error
rates  the proofs of which can be found in Appendix. We ﬁrst provide a deterministic bound on
the error (cid:107)αm − α∗
m)). We derive probabilistic
results in the subsequent corollaries.
Theorem 2. Let Ns be the true neighborhood of node s  with |Ns| = d. Suppose Ls m satisﬁes
Assumption 1. If the regularization parameter λn is chosen such that λn ≥ 2R∗(∇Ls m(α∗
m)) +
2c

m(cid:107)2 in terms of the random quantity R∗(∇Ls m(α∗

(cid:113) m log(p)

  then any stationary point ˆαm of (8) in B2(α∗
m  rm) satisﬁes:
√

n

(cid:107)ˆαm − α∗

√
m(cid:107)2 ≤ 6

2

κ

dλn.

We now provide a set of sufﬁcient conditions under which the random quantity R∗(∇Ls m(α∗
m))
can be bounded.
Assumption 2. There exists a constant L > 0 such that the gradient of the population loss ¯Ls m at
m satisﬁes: R∗(∇ ¯Ls m(α∗
α∗
Corollary 3. Suppose the conditions in Theorem 2 are satisﬁed. Moreover 

supi∈N X∈X |φi(X)| and τm = supt∈V X∈X |(cid:80)m

m)) ≤ LR∗(α∗ − α∗

m).
i=1 α∗

2. If the regularization parameter λn is chosen such that λn ≥ 2LR∗(α∗− α∗
m) + cγτm
then then with probability at least 1− 2m/p2 any stationary point ˆαm of (8) in B2(α∗

let γ =
t iφi(X)|. Suppose Ls m satisﬁes Assumption
 
m  rm) satisﬁes:

(cid:113) md2 log(p)

n

(cid:107)ˆαm − α∗

√
m(cid:107)2 ≤ 6

κ

√

2

dλn.

Theorem 2 and Corollary 3 bound the error of the estimated coefﬁcients in the truncated expansion.
The approximation error of the truncated expansion itself depends on the function space assumption 
as well as the basis chosen  but can be simply combined with the statement of the above corollary to
derive the overall error. As an instance  we present a corollary below for the speciﬁc case of Sobolev
space of order two  and the trigonometric basis.
Corollary 4. Suppose the conditions in Corollary 3 are satisﬁed. Moreover  suppose the true functions
k=0 be the trigonometric basis of L2(X ). If the
t (·) lie in a Sobolev space of order two. Let {φk}∞
B∗
optimization problem (8) is solved with λn = c1(d2 log(p)/n)2/5 and m = c2(n/d2 log(p))1/5  then
with probability at least 1 − 2m/p2 any stationary point ˆαm of (8) in B2(α∗

m  rm) satisﬁes:

(cid:107)ˆαm − α∗(cid:107)2 ≤ c3

where c1  c2  c3 depend on L  κ  γ  τm.

(cid:18) d13/4 log(p)

(cid:19)2/5

 

n

6

(cid:10)∇ ¯Ls m(α∗

Discussion on Assumption 1: We now provide a set of sufﬁcient conditions which ensure the
restricted strong convexity (RSC) condition. Suppose the population risk ¯Ls m(·) is strongly convex
in a ball of radius rm around α∗

m

2 ∀∆m ∈ B2(0  rm).
Moreover  suppose the empirical gradients converge uniformly to the population gradients

m + ∆m) − ∇ ¯Ls m(α∗

(cid:11) ≥ κ(cid:107)∆m(cid:107)2
(cid:114)
R∗(cid:0)∇Ls m(αm) − ∇ ¯Ls m(αm)(cid:1) ≤ c

m)  ∆m

(9)

(10)

m log p

n

.

sup
αm∈B2(α∗

m rm)

For example  this condition holds with high probability when the gradient of Ls m(αm) w.r.t
αt m  for any t ∈ [p] is a sub-Gaussian process. Equations (9) (10) are easier to check and en-
sure that Ls m(αm) satisﬁes the RSC property in Assumption 1.

6 Connections to Exponential Family MRF Copulas

(cid:110)(cid:80)
s∈V θsBs(Xs) +(cid:80)

(s t)∈E(G) θstBs(Xs) Bt(Xt) +(cid:80)

The Expxorcist class of models could be viewed as being closely related to an exponential fam-
ily MRF [28] copula density. Consider the parametric exponential family MRF joint density in
(3): PMRF;θ(X) ∝ exp
 
s∈V Cs(Xs)
where the distribution is indexed by the ﬁnite-dimensional parameters {θs}s∈V  {θst}(s t)∈E  and
where in contrast to the previous sections  we assume we are given the sufﬁcient statistics functions
{Bs(·)}s∈V as well as the nodewise base measures {Cs(·)}s∈V . Now consider the following non-
parametric problem. Given a random vector X  suppose we are interested in estimating monotonic
node-wise functions {fs(Xs)}s∈V such that (f1(X1)  . . .   fp(Xp)) follows PMRF;θ for some θ. Let-
ting f(X) = (f1(X1)  . . .   fp(Xp))  we have that P(f(X)) = PMRF;θ(f(X))  so that the density of
s(Xs). This is now a semi-parametric estimation
problem  where the unknowns are the functions {fs(Xs)}s∈V as well as the ﬁnite-dimensional pa-
rameters θ. To simplify this density  suppose we assume that the given node-wise sufﬁcient statistics
are linear  so that Bs(z) = z  for all s ∈ V   so that density reduces to

X can be written as P(X) ∝ P(f(X))(cid:81)

s∈V f(cid:48)

(cid:111)

(cid:88)

s∈V

(cid:88)

(s t)∈E(G)

(cid:88)

s∈V

(cid:88)

(s t)∈E(G)

 .

(cid:48)
s(Xs))

 .

(11)

(12)

(cid:88)

s∈V

(cid:88)

s∈V

P(X) ∝ exp

θsfs(Xs) +

θstfs(Xs) ft(Xt) +

(Cs(fs(Xs)) + log f

In contrast  the Expxorcist nonparametric exponential family graphical model takes the form

P(X) ∝ exp

θsfs(Xs) +

θstfs(Xs) ft(Xt) +

Cs(Xs)

It can be seen that the two densities have very similar forms  except that the density in (11) has a
more complex base measure that depends on the unknown functions {fs}s∈V and importantly the
functions {fs}s∈V in (11) are monotonic.
The class of densities in (11) can be cast as an exponential family MRF copula density. Suppose
we denote the CDF of the parametric exponential family MRF joint density by FMRF;θ(X)  with
nodewise marginal CDFs FMRF;θ s(Xs). Then the marginal CDF of the density (11) can be written
as Fs(xs) = P[Xs ≤ xs] = P[fs(Xs) ≤ fs(xs)] = FMRF;θ s(fs(xs))  so that

(cid:16)

MRF;θ s(Fs(xs)).

(cid:17)
(cid:16)
MRF;θ 1(F1(X1))  . . .   F −1
F −1
MRF;θ p(Fp(Xp))
F −1
MRF;θ 1(U1)  . . .   F −1

It then follows that: F (X) = FMRF;θ
is the CDF of density (11). By letting FCOP;θ(U ) = FMRF;θ
MRF;θ p(Up)
be the exponential family MRF copula density function  we see that the CDF of X is precisely:
F (X) = FCOP;θ(F1(X1)  . . .   Fp(Xp))  which is speciﬁed by the marginal CDFs {Fs(Xs)}s∈V and
the copula density FCOP;θ corresponding to the exponential family MRF density. In other words  the
non-parametric extension in (11) of the exponential family MRF densities is precisely an exponential
family MRF copula density. This development thus generalizes the non-parametric extension of
Gaussian MRF densities via the Gaussian copula nonparanormal densities [17]. The caveats with the
copula density however are two-fold: the node-wise functions are restricted to be monotonic  but

  where F (X)

fs(xs) = F −1

(13)

(cid:17)

7

also the estimation of these as in (13) requires the estimation of inverses of marginal CDFs of an
exponential family MRF  which is intractable in general. Thus  minor differences in the expressions
of the Expxorcist density (12) and an exponential family MRF copula density (11) nonetheless have
seemingly large consequences for tractable estimation of these densities from data.

7 Experiments

We present experimental results on both synthetic and real datasets. We compare our estimator 
Expxorcist  with the Nonparanormal model of [17] and Gaussian Graphical Model (GGM). We use
glasso [7] to estimate GGM and the two step estimator of [17] to estimate Nonparanormal model.

7.1 Synthetic Experiments

(cid:2)exp(cid:0)−20(X − 0.5)2(cid:1) + exp(cid:0)−20(X + 0.5)2(cid:1) − 1(cid:3) and picked the log base measure Cs(X) to

Data: We generated synthetic data from the Expxorcist model with chain and grid graph structures.
For both the graph structures  we set θs = 1 ∀s ∈ V  θst = 1 ∀(s  t) ∈ E and ﬁx the domain
X to [−1  1]. We experimented with two choices for sufﬁcient statistics Bs(X): sin(4πX) and
be 0. The grid graph we considered has a 10 × (p/10) structure. We used Gibbs sampling to sample
data from these models. We also generated data from Gaussian distribution with chain and grid graph
structures. To generate this data we set the off diagonal non-zero entries of inverse covariance matrix
to 0.49 for chain graph and 0.25 for grid graph and diagonal entries to 1.
Evaluation Metric: We compared the performance of Expxorcist against baselines  on graph
structure recovery  using ROC curves. The ROC curve plots the true positive rate (TPR) against false
positive rate (FPR) over different choices of regularization parameter  where TPR is the fraction of
correctly detected edges and FPR is the fraction of mis-identiﬁed non edges.
Experiment Settings: For this experiment we set p = 50 and n ∈ {100  200  500} and varied the
regularization parameter λ from 10−2 to 1. To ﬁt the data to the non parametric model (3)  we used
cosine basis and truncated the basis expansion to top 30 terms. In practice  one could choose the
number of basis (m) based on domain knowledge (e.g. “smooth” functions)  or in the absence of
which  one could use hold-out validation/cross validation. Given ˆN (s)  the estimated neighborhood
for node s  we estimated the overall graph structure as: ∪s∈V ∪t∈ ˆN (s){(s  t)}. To reduce the variance
in the ROC plots  we averaged results over 10 repetitions.
Results: Figure 1 shows the ROC plots obtained from this experiment. Due to the lack of space 
we present more experimental results in Appendix. It can be seen that Expxorcist has much better
performance on non-Gaussian data. On these datasets  even at n = 500 the baselines chose edges
at random. This suggests that in the presence of multiple modes and fat tails  Expxorcist is a better
model. Expxorcist has slightly poor performance than baselines on Gaussian data. However  this is
expected because it learns a broader family of distributions than Nonparanormal.

7.2 Futures Intraday Data

We now present our analysis on the Futures price returns. This dataset was downloaded from
http://www.kibot.com/. We focus on the Top-26 most liquid instruments being traded at the
Chicago Mercantile Exchange (CME). The instruments span different sectors like Energy  Agriculture 
Currencies  Equity Indices  Metals and Interest Rates. We focus on the hours of maximum liquidity
(9am Eastern to 3pm Eastern) and look at the 1 minute price returns. The return distribution is a
mixture of 1 minute returns with the overnight return. Since overnight returns tend to be bigger than
the 1 minute return within the day  the return distribution is multimodal and fat-tailed. We treat each
instrument as a random variable and the 1 minute returns as independent samples drawn from these
random variables. We use the data collected in February 2010 as training data and data from March
2010 as held out data for tuning parameter selection. After removing samples with missing entries
we are left with 894 training and 650 held out data samples. We ﬁt Expxorcist and baselines on this
data with the same parameter settings described above. For each of these models  we select the best
tuning parameter through log likelihood on held out data. However  this criteria resulted in complete
graphs for Nonparanormal and GGM (325 edges) and a relatively sparser graph for Expxorcist (168
edges). So for a better comparison of these models  we selected tuning parameters for each of the
models such that the resulting graphs have almost the same number of edges. Figure 2 shows the

8

Figure 1: ROC plots from synthetic experiments. Top and bottom rows show plots for chain and grid graphs
respectively. Left column shows plots for data generated from our non-parametric model with Bs(X) = sin(X) 
n = 500 and center column shows plots for the other choice of sufﬁcient statistic with n = 500. Right column
shows plots for Gaussian data with n = 200.

(a) Nonparanormal

(b) Expxorcist

Figure 2: Graph Structures learned for the Futures Intraday Data. The Expxorcist graph shown here was
obtained by selecting λ = 0.1. Nodes are colored based on their categories. Edge thickness is proportional to
the magnitude of the interaction.
learned graphs for one such choice of tuning parameters  which resulted in ∼ 52 edges in the graphs.
Nonparanormal and GGM resulted in very similar graphs  so we only present Nonparanormal here. It
can be seen that Expxorcist is able to identify the clusters better than Nonparanormal. More detailed
graphs and comparison with GGM can be found in Appendix.

8 Conclusion

In this work we considered the problem of non-parametric density estimation and introduced Expx-
orcist  a new family of non-parametric graphical models. Our approach relies on a simple function
space assumption that the conditional distribution of each variable conditioned on the other variables
has a non-parametric exponential family form. We proposed an estimator for Expxorcist that is
computationally efﬁcient and comes with statistical guarantees. Our empirical results suggest that  in
the presence of multiple modes and fat tails in the data  our non-parametric model is a better choice
than the Nonparanormal model of [17].

9 Acknowledgement

A.S. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803 
IIS-1447574  DMS-1264033  and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS
Initiative to Support Research at the Interface of the Biological and Mathematical Sciences. M. K.
acknowledges support by an IBM Corporation Faculty Research Fund at the University of Chicago
Booth School of Business.

9

00.20.40.60.8100.20.40.60.81Gaussian(n = 200)ExpxorcistGGMNonparanormal00.20.40.60.8100.20.40.60.81TPRSine(n = 500)00.20.40.60.8100.20.40.60.81Exp (n = 500)00.20.40.60.81FPR00.20.40.60.8100.20.40.60.81FPR00.20.40.60.81TPR00.20.40.60.81FPR00.20.40.60.81 References
[1] Barry C. Arnold  Enrique Castillo  and José María Sarabia. Conditionally speciﬁed distributions: an

introduction. Stat. Sci.  16(3):249–274  2001. With comments and a rejoinder by the authors.

[2] Patrizia Berti  Emanuela Dreassi  and Pietro Rigo. Compatibility results for conditional distributions. J.

Multivar. Anal.  125:190–203  2014.

[3] Julian Besag. Spatial interaction and the statistical analysis of lattice systems. J. R. Stat. Soc. B  pages

192–236  1974.

[4] Stéphane Canu and Alex Smola. Kernel methods and the exponential family. Neurocomputing  69(7-

9):714–720  Mar 2006.

[5] Hua Yun Chen. Compatibility of conditionally speciﬁed models. Statist. Probab. Lett.  80(7-8):670–677 

2010.

[6] Ronaldo Dias. Density estimation via hybrid splines. J. Statist. Comput. Simulation  60(4):277–293  1998.
[7] Jerome H. Friedman  Trevor J. Hastie  and Robert J. Tibshirani. Sparse inverse covariance estimation with

the graphical lasso. Biostatistics  9(3):432–441  2008.

[8] I. J. Good and R. A. Gaskins. Nonparametric roughness penalties for probability densities. Biometrika 

58:255–277  1971.

[9] Chong Gu. Smoothing spline density estimation: conditional distribution. Stat. Sinica  5(2):709–726 

1995.

[10] Chong Gu  Yongho Jeon  and Yi Lin. Nonparametric density estimation in high-dimensions. Stat. Sinica 

23:1131–1153  2013.

[11] Chong Gu and Chunfu Qiu. Smoothing spline density estimation: theory. Ann. Stat.  21(1):217–234  1993.
[12] Chong Gu and Jingyuan Wang. Penalized likelihood density estimation: direct cross-validation and scalable

approximation. Stat. Sinica  13(3):811–826  2003.

[13] Ali Jalali  Pradeep Ravikumar  Vishvas Vasuki  and Sujay Sanghavi. On learning discrete graphical models

using group-sparse regularization. In AISTATS  pages 378–387  2011.

[14] Yongho Jeon and Yi Lin. An effective method for high-dimensional log-density anova estimation  with

application to nonparametric graphical model building. Stat. Sinica  16(2):353–374  2006.

[15] Tom Leonard. Density estimation  stochastic processes and prior information. J. R. Stat. Soc. B  40(2):113–

146  1978. With discussion.

[16] Han Liu  Fang Han  Ming Yuan  John D. Lafferty  and Larry A. Wasserman. High-dimensional semipara-

metric Gaussian copula graphical models. Ann. Stat.  40(4):2293–2326  2012.

[17] Han Liu  John D. Lafferty  and Larry A. Wasserman. The nonparanormal: Semiparametric estimation of

high dimensional undirected graphs. J. Mach. Learn. Res.  10:2295–2328  2009.

[18] Benoît R. Mâsse and Young K. Truong. Conditional logspline density estimation. Canad. J. Statist. 

27(4):819–832  1999.

[19] Song Mei  Yu Bai  and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv

preprint arXiv:1607.06534  2016.

[20] Pradeep Ravikumar  Martin J Wainwright  John D Lafferty  et al. High-dimensional ising model selection

using l1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319  2010.

[21] B. W. Silverman. On the estimation of a probability density function by the maximum penalized likelihood

method. Ann. Stat.  10(3):795–810  1982.

[22] TP Speed and HT Kiiveri. Gaussian markov distributions over ﬁnite graphs. The Annals of Statistics  pages

138–150  1986.

[23] Charles J. Stone  Mark H. Hansen  Charles Kooperberg  and Young K. Truong. Polynomial splines and
their tensor products in extended linear modeling. Ann. Stat.  25(4):1371–1470  1997. With discussion and
a rejoinder by the authors and Jianhua Z. Huang.

[24] Siqi Sun  Jinbo Xu  and Mladen Kolar. Learning structured densities via inﬁnite dimensional exponential

families. In Advances in Neural Information Processing Systems  pages 2287–2295  2015.

[25] Cristiano Varin  Nancy Reid  and David Firth. An overview of composite likelihood methods. Stat. Sinica 

21(1):5–42  2011.

[26] Arend Voorman  Ali Shojaie  and Daniela M. Witten. Graph estimation with joint additive models.

Biometrika  101(1):85–101  Mar 2014.

[27] Yuchung J. Wang and Edward H. Ip. Conditionally speciﬁed continuous distributions. Biometrika 

95(3):735–746  2008.

[28] Eunho Yang  Pradeep Ravikumar  Genevera I Allen  and Zhandong Liu. Graphical models via univariate

exponential family distributions. Journal of Machine Learning Research  16(1):3813–3847  2015.

[29] Zhuoran Yang  Yang Ning  and Han Liu. On semiparametric exponential family graphical models. arXiv

preprint arXiv:1412.8697  2014.

10

[30] Xiaotong Yuan  Ping Li  Tong Zhang  Qingshan Liu  and Guangcan Liu. Learning additive exponential
family graphical models via (cid:96)_{2  1}-norm regularized m-estimation. In Advances in Neural Information
Processing Systems  pages 4367–4375  2016.

[31] Hao Helen Zhang and Yi Lin. Component selection and smoothing for nonparametric regression in

exponential families. Stat. Sinica  16(3):1021–1041  2006.

[32] Tuo Zhao  Zhaoran Wang  and Han Liu. Nonconvex low rank matrix factorization via inexact ﬁrst order

oracle. Advances in Neural Information Processing Systems  2015.

11

,Ruiyu Li
Jiaya Jia
Arun Suggala
Mladen Kolar
Pradeep Ravikumar