2015,Optimal Ridge Detection using Coverage Risk,We introduce the concept of coverage risk as an error measure for density ridge estimation.The coverage risk generalizes the mean integrated square error to set estimation.We propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk.We study the rate of convergence for coverage risk and prove consistency of the risk estimators.We apply our method to three simulated datasets and to cosmology data.In all the examples  the proposed method successfully recover the underlying density structure.,Optimal Ridge Detection using Coverage Risk

Yen-Chi Chen

Department of Statistics

Carnegie Mellon University

yenchic@andrew.cmu.edu

Shirley Ho

Department of Physics

Carnegie Mellon University

shirleyh@andrew.cmu.edu

Christopher R. Genovese
Department of Statistics

Carnegie Mellon University

genovese@stat.cmu.edu

Larry Wasserman

Department of Statistics

Carnegie Mellon University
larry@stat.cmu.edu

Abstract

We introduce the concept of coverage risk as an error measure for density ridge
estimation. The coverage risk generalizes the mean integrated square error to set
estimation. We propose two risk estimators for the coverage risk and we show that
we can select tuning parameters by minimizing the estimated risk. We study the
rate of convergence for coverage risk and prove consistency of the risk estimators.
We apply our method to three simulated datasets and to cosmology data. In all
the examples  the proposed method successfully recover the underlying density
structure.

1

Introduction

Density ridges [10  22  15  6] are one-dimensional curve-like structures that characterize high den-
sity regions. Density ridges have been applied to computer vision [2]  remote sensing [21]  biomedi-
cal imaging [1]  and cosmology [5  7]. Density ridges are similar to the principal curves [17  18  27].
Figure 1 provides an example for applying density ridges to learn the structure of our Universe.

To detect density ridges from data  [22] proposed the ‘Subspace Constrained Mean Shift (SCMS)’
algorithm. SCMS is a modiﬁcation of usual mean shift algorithm [14  8] to adapt to the local
geometry. Unlike mean shift that pushes every mesh point to a local mode  SCMS moves the meshes
along a projected gradient until arriving at nearby ridges. Essentially  the SCMS algorithm detects
the ridges of the kernel density estimator (KDE). Therefore  the SCMS algorithm requires a pre-
selected parameter h  which acts as the role of smoothing bandwidth in the kernel density estimator.

Despite the wide application of the SCMS algorithm  the choice of h remains an unsolved problem.
Similar to the density estimation problem  a poor choice of h results in over-smoothing or under-
smoothing for the density ridges. See the second row of Figure 1.

In this paper  we introduce the concept of coverage risk which is a generalization of the mean
integrated expected error from function estimation. We then show that one can consistently estimate
the coverage risk by using data splitting or the smoothed bootstrap. This leads us to a data-driven
selection rule for choosing the parameter h for the SCMS algorithm. We apply the proposed method
to several famous datasets including the spiral dataset  the three spirals dataset  and the NIPS dataset.
In all simulations  our selection rule allows the SCMS algorithm to detect the underlying structure
of the data.

1

Figure 1: The cosmic web. This is a slice of the observed Universe from the Sloan Digital Sky
Survey. We apply the density ridge method to detect ﬁlaments [7]. The top row is one example
for the detected ﬁlaments. The bottom row shows the effect of smoothing. Bottom-Left: optimal
smoothing. Bottom-Middle: under-smoothing. Bottom-Right: over-smoothing. Under optimal
smoothing  we detect an intricate ﬁlament network. If we under-smooth or over-smooth the dataset 
we cannot ﬁnd the structure.

1.1 Density Ridges

Density ridges are deﬁned as follows. Assume X1 ···   Xn are independently and identically dis-
tributed from a smooth probability density function p with compact support K. The density ridges
[10  15  6] are deﬁned as

R = {x ∈ K : V (x)V (x)T∇p(x) = 0  λ2(x) < 0} 

where V (x) = [v2(x) ··· vd(x)] with vj(x) being the eigenvector associated with the ordered
eigenvalue λj(x) (λ1(x) ≥ ··· ≥ λd(x)) for Hessian matrix H(x) = ∇∇p(x). That is  R is
the collection of points whose projected gradient V (x)V (x)T∇p(x) = 0. It can be shown that
(under appropriate conditions)  R is a collection of 1-dimensional smooth curves (1-dimensional
manifolds) in Rd.

The SCMS algorithm is a plug-in estimate for R by using

(cid:110)
(cid:111)
x ∈ K : (cid:98)Vn(x)(cid:98)Vn(x)T∇(cid:98)pn(x) = 0 (cid:98)λ2(x) < 0
(cid:98)Rn =
i=1 K(cid:0) x−Xi
(cid:80)n

(cid:1) is the KDE and(cid:98)Vn and(cid:98)λ2 are the associated quantities deﬁned
where(cid:98)pn(x) = 1
by(cid:98)pn. Hence  one can clearly see that the parameter h in the SCMS algorithm plays the same role

 

nhd

h

of smoothing bandwidth for the KDE.

2

2 Coverage Risk

r.

(1)

  R).

Wn = d(UR (cid:98)Rn)  (cid:102)Wn = d(U(cid:98)Rn

Before we introduce the coverage risk  we ﬁrst deﬁne some geometric concepts. Let µ(cid:96) be the (cid:96)-
dimensional Hausdorff measure [13]. Namely  µ1(A) is the length of set A and µ2(A) is the area
as

Let Haus(A  B) = inf{r : A ⊂ B ⊕ r  B ⊂ A ⊕ r} be the Hausdorff distance between A and B
where A ⊕ r = {x : d(x  A) ≤ r}. The following lemma gives some useful properties about Wn

of A. Let d(x  A) be the projection distance from point x to a set A. We deﬁne UR and U(cid:98)Rn
random variables uniformly distributed over the true density ridges R and the ridge estimator (cid:98)Rn
respectively. Assuming R and (cid:98)Rn are given  we deﬁne the following two random variables
are random variables while R (cid:98)Rn are sets. Wn is the distance from a randomly
Note that UR  U(cid:98)Rn
selected point on R to the estimator (cid:98)Rn and(cid:102)Wn is the distance from a random point on (cid:98)Rn to R.
and(cid:102)Wn.
Lemma 1 Both random variables Wn and(cid:102)Wn are bounded by Haus((cid:99)Mn  M ). Namely 
The cumulative distribution function (CDF) for Wn and(cid:102)Wn are
(cid:16)(cid:98)Rn ∩ (R ⊕ r)
(cid:17)
(cid:16)(cid:98)Rn
(cid:17)

0 ≤(cid:102)Wn ≤ Haus((cid:98)Rn  R).
  P((cid:102)Wn ≤ r|(cid:98)Rn) =

0 ≤ Wn ≤ Haus((cid:98)Rn  R) 
(cid:17)
R ∩ ((cid:98)Rn ⊕ r)

P(Wn ≤ r|(cid:98)Rn) =

This lemma follows trivially by deﬁnition so that we omit its proof. Lemma 1 links the random

Thus  P(Wn ≤ r|(cid:98)Rn) is the ratio of R being covered by padding the regions around (cid:98)Rn at distance
variables Wn and(cid:102)Wn to the Hausdorff distance and the coverage for R and (cid:98)Rn. Thus  we call them
coverage random variables. Now we deﬁne the L1 and L2 coverage risk for estimating R by (cid:98)Rn as
That is  Risk1 n (and Risk2 n) is the expected (square) projected distance between R and (cid:98)Rn. Note
that the expectation in (4) applies to both (cid:98)Rn and UR. One can view Risk2 n as a generalized mean

E(Wn +(cid:102)Wn)

n +(cid:102)W 2

  Risk2 n =

Risk1 n =

E(W 2

µ1 (R)

n)

.

(cid:16)

.

(3)

µ1

µ1

(4)

(2)

µ1

2

2

integrated square errors (MISE) for sets.

A nice property of Risk1 n and Risk2 n is that they are not sensitive to outliers of R in the sense that
a small perturbation of R will not change the risk too much. On the contrary  the Hausdorff distance
is very sensitive to outliers.

2.1 Selection for Tuning Parameters Based on Risk Minimization

In this section  we will show how to choose h by minimizing an estimate of the risk.

We propose two risk estimators. The ﬁrst estimator is based on the smoothed bootstrap [25]. We
sample X∗
n. The we estimate the risk by

n from the KDE(cid:98)pn and recompute the estimator (cid:98)R∗
n +(cid:102)W ∗
n) and(cid:102)W ∗
 (cid:98)R∗

  (cid:100)Risk2 n =
 (cid:98)Rn).

n|X1 ···   Xn)
2
n = d(U(cid:98)R∗

E(W ∗2

E(W ∗

1  ··· X∗
(cid:100)Risk1 n =
n = d(U(cid:98)Rn

n

n +(cid:102)W ∗2

n |X1 ···   Xn)
2

 

(5)

where W ∗

3

†
1m and
†
2m  assuming n is even and 2m = n. We compute the estimated manifolds by using

The second approach is to use data splitting. We randomly split the data into X
half of the data  which we denote as (cid:98)R
†
21 ···   X
X
(cid:100)Risk

1 n and (cid:98)R
†
2 n|X1 ···   Xn)
2

†2
2 n|X1 ···   Xn)
2

†
2 n. Then we compute

†
11 ···   X

†2
1 n + W

†
1 n + W

E(W

†

†
1 n =
†

E(W
1 n = d(U(cid:98)R

 (cid:98)R

†
1 n

where W

†
2 n) and W

Having estimated the risk  we select h by

†
2 n =
†
1 n).

†

  (cid:100)Risk
 (cid:98)R
2 n = d(U(cid:98)R
(cid:100)Risk

h∗ = argmin
h≤¯hn

†
2 n

†
1 n 

 

(6)

(7)

where ¯hn is an upper bound by the normal reference rule [26] (which is known to oversmooth  so
that we only select h below this rule). Moreover  one can choose h by minimizing L2 risk as well.

In [11]  they consider selecting the smoothing bandwidth for local principal curves by self-coverage.
This criterion is a different from ours. The self-coverage counts data points. The self-coverage is
a monotonic increasing function and they propose to select the bandwidth such that the derivative
is highest. Our coverage risk yields a simple trade-off curve and one can easily pick the optimal
bandwidth by minimizing the estimated risk.

3 Manifold Comparison by Coverage

The concepts of coverage in previous section can be generalized to investigate the difference between
two manifolds. Let M1 and M2 be an (cid:96)1-dimensional and an (cid:96)2-dimensional manifolds ((cid:96)1 and (cid:96)2
are not necessarily the same). We deﬁne the coverage random variables

.

(9)

µ(cid:96)2 (M1)

  P(W21 ≤ r) =

W12 = d(UM1  M2)  W21 = d(UM2  M1).

(8)
Then by Lemma 1  the CDF for W12 and W21 contains information about how M1 and M2 are
different from each other:
P(W12 ≤ r) =

µ(cid:96)2 (M2 ∩ (M1 ⊕ r))

µ(cid:96)1 (M1 ∩ (M2 ⊕ r))

µr2 (M1)
P(W12 ≤ r) is the coverage on M1 by padding regions with distance r around M2.
We call the plots of the CDF of W12 and W21 coverage diagrams since they are linked to the
coverage over M1 and M2. The coverage diagram allows us to study how two manifolds are different
from each other. When (cid:96)1 = (cid:96)2  the coverage diagram can be used as a similarity measure for two
manifolds. When (cid:96)1 (cid:54)= (cid:96)2  the coverage diagram serves as a measure for quality of representing high
dimensional objects by low dimensional ones. A nice property for coverage diagram is that we can
approximate the CDF for W12 and W21 by a mesh of points (or points uniformly distributed) over
M1 and M2. In Figure 2 we consider a Helix dataset whose support has dimension d = 3 and we
compare two curves  a spiral curve (green) and a straight line (orange)  to represent the Helix dataset.
As can be seen from the coverage diagram (right panel)  the green curve has better coverage at each
distance (compared to the orange curve) so that the spiral curve provides a better representation for
the Helix dataset.
In addition to the coverage diagram  we can also use the following L1 and L2 losses as summary for
the difference:

Loss1(M1  M2) =

 

Loss2(M1  M2) =

E(W12 + W21)

2

E(W 2

12 + W 2

21)

2

.

(10)

The expectation is take over UM1 and UM2 and both M1 and M2 here are ﬁxed. The risks in (4) are
the expected losses:

Loss1((cid:99)Mn  M )

(cid:17)

  Risk2 n = E(cid:16)

(cid:17)
Loss2((cid:99)Mn  M )

Risk1 n = E(cid:16)

.

(11)

4

Figure 2: The Helix dataset. The original support for the Helix dataset (black dots) are a 3-
dimensional regions. We can use green spiral curves (d = 1) to represent the regions. Note that
we also provide a bad representation using a straight line (orange). The coverage plot reveals the
quality for representation. Left: the original data. Dashed line is coverage from data points (black
dots) over green/orange curves in the left panel and solid line is coverage from green/orange curves
on data points. Right: the coverage plot for the spiral curve (green) versus a straight line (orange).

4 Theoretical Analysis

In this section  we analyze the asymptotic behavior for the coverage risk and prove the consistency
for estimating the coverage risk by the proposed method. In particular  we derive the asymptotic
properties for the density ridges. We only focus on L2 risk since by Jensen’s inequality  the L2 risk
can be bounded by the L1 risk.

Before we state our assumption  we ﬁrst deﬁne the orientation of density ridges. Recall that the
density ridge R is a collection of one dimensional curves. Thus  for each point x ∈ R  we can
associate a unit vector e(x) that represent the orientation of R at x. The explicit formula for e(x)
can be found in Lemma 1 of [6].

Assumptions.

(R) There exist β0  β1  β2  δR > 0 such that for all x ∈ R ⊕ δR 

λ2(x) ≤ −β1 

λ1(x) ≥ β0 − β1 

(12)
where (cid:107)p(3)(x)(cid:107)max is the element wise norm to the third derivative. And for each x ∈ R 
|e(x)T∇p(x)| ≥

(cid:107)∇p(x)(cid:107)(cid:107)p(3)(x)(cid:107)max ≤ β0(β1 − β2) 

λ1(x)

λ1(x)−λ2(x).

(K1) The kernel function K is three times bounded differetiable and is symmetric  non-negative

(cid:90) (cid:16)

(cid:17)2

x2K (α)(x)dx < ∞ 

K (α)(x)

dx < ∞

and

let

for all α = 0  1  2  3.

(cid:90)

(cid:26)

(K2) The kernel function K and its partial derivative satisﬁes condition K1 in [16]. Speciﬁcally 

K =

y (cid:55)→ K (α)

: x ∈ Rd  h > 0 |α| = 0  1  2

(cid:19)

(cid:18) x − y
N(cid:0)K  L2(P )  (cid:107)F(cid:107)L2(P )

h

(cid:1) ≤

(cid:18) A

(cid:19)v



(cid:27)

We require that K satisﬁes

sup
P

(13)

(14)

5

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0rCoveragefor some positive number A  v  where N (T  d  ) denotes the -covering number of the
metric space (T  d) and F is the envelope function of K and the supreme is taken over
the whole Rd. The A and v are usually called the VC characteristics of K. The norm
(cid:107)F(cid:107)L2(P ) = supP

(cid:82) |F (x)|2dP (x).

Assumption (R) appears in [6] and is very mild. The ﬁrst two inequality in (12) are just the bound
on eigenvalues. The last inequality requires the density around ridges to be smooth. The latter part
of (R) requires the direction of ridges to be similar to the gradient direction. Assumption (K1) is
the common condition for kernel density estimator see e.g. [28] and [24]. Assumption (K2) is to
regularize the classes of kernel functions that is widely assumed [12  15  4]; any bounded kernel
function with compact support satisﬁes this condition. Both (K1) and (K2) hold for the Gaussian
kernel.
Under the above condition  we derive the rate of convergence for the L2 risk.
Theorem 2 Let Risk2 n be the L2 coverage risk for estimating the density ridges and level sets.
Assume (K1–2) and (R) and p is at least four times bounded differentiable. Then as n → ∞  h → 0
and log n

nhd+6 → 0

Risk2 n = B2

Rh4 +

σ2
nhd+2 + o(h4) + o
R

(cid:18) 1

nhd+2

(cid:19)

 

for some BR and σ2

R that depends only on the density p and the kernel function K.

The rate in Theorem 2 shows a bias-variance decomposition. The ﬁrst term involving h4 is the
bias term while the latter term is the variance part. Thanks to the Jensen’s inequality  the rate of
convergence for L1 risk is the square root of the rate Theorem 2. Note that we require the smoothing
nhd+6 → 0. This constraint comes from the uniform bound
parameter h to decay slowly to 0 by log n
for estimating third derivatives for p. We need this constraint since we need the smoothness for
estimated ridges to converge to the smoothness for the true ridges. Similar result for density level
set appears in [3  20].
By Lemma 1  we can upper bound the L2 risk by expected square of the Hausdorff distance which
gives the rate

Risk2 n ≤ E(cid:16)

(cid:17)
Haus2((cid:98)Rn  R)

= O(h4) + O

(15)

(cid:18) log n

(cid:19)

nhd+2

The rate under Hausdorff distance for density ridges can be found in [6] and the rate for density
ridges appears in [9]. The rate induced by Theorem 2 agrees with the bound from the Hausdorff
distance and has a slightly better rate for variance (without a log-n factor). This phenomena is
similar to the MISE and L∞ error for nonparametric estimation for functions. The MISE converges
slightly faster (by a log-n factor) than square to the L∞ error.

Now we prove the consistency of the risk estimators. In particular  we prove the consistency for the
smoothed bootstrap. The case of data splitting can be proved in the similar way.
Theorem 3 Let Risk2 n be the L2 coverage risk for estimating the density ridges and level sets. Let
p is at least four times bounded differentiable. Then as n → ∞  h → 0 and log n

(cid:100)Risk2 n be the corresponding risk estimator by the smoothed bootstrap. Assume (K1–2) and (R) and

nhd+6 → 0 

(cid:100)Risk2 n − Risk2 n

Risk2 n

P→ 0.

Theorem 3 proves the consistency for risk estimation using the smoothed bootstrap. This also leads
to the consistency for data splitting.

6

Figure 3: Three different simulation datasets. Top row: the spiral dataset. Middle row: the three
spirals dataset. Bottom row: NIPS character dataset. For each row  the leftmost panel shows the
estimated L1 coverage risk using data splitting; the red straight line indicates the bandwidth selected
by least square cross validation [19]  which is either undersmooth or oversmooth. Then the rest three
panels  are the result using different smoothing parameters. From left to right  we show the result
for under-smoothing  optimal smoothing (using the coverage risk)  and over-smoothing. Note that
the second minimum in the coverage risk at the three spirals dataset (middle row) corresponds to a
phase transition when the estimator becomes a big circle; this is also a locally stable structure.

5 Applications

5.1 Simulation Data

We now apply the data splitting technique (7) to choose the smoothing bandwidth for density ridge
estimation. Note that we use data splitting over smooth bootstrap since in practice  data splitting
works better. The density ridge estimation can be done by the subspace constrain mean shift al-
gorithm [22]. We consider three famous datasets: the spiral dataset  the three spirals dataset and a
‘NIPS’ dataset.

Figure 3 shows the result for the three simulation datasets. The top row is the spiral dataset; the
middle row is the three spirals dataset; the bottom row is the NIPS character dataset. For each row 
from left to right the ﬁrst panel is the estimated L1 risk by using data splitting. Note that there is
no practical difference between L1 and L2 risk. The second to fourth panels are under-smoothing 
optimal smoothing  and over-smoothing. Note that we also remove the ridges whose density is

below 0.05 × maxx(cid:98)pn(x) since they behave like random noise. As can be seen easily  the optimal

bandwidth allows the density ridges to capture the underlying structures in every dataset. On the
contrary  the under-smoothing and the over-smoothing does not capture the structure and have a
higher risk.

7

0.00.51.01.52.00.51.01.52.02.53.0Smoothing Parameter(Estimated) L1 Coverage Riskllllllllllllllllllll0.00.51.01.52.00.20.40.60.81.01.2Smoothing Parameter(Estimated) L1 Coverage Riskllllllllllllllllllll0.00.51.01.52.00.030.040.050.06Smoothing Parameter(Estimated) L1 Coverage RiskllllllllllllllllllllFigure 4: Another slice for the cosmic web data from the Sloan Digital Sky Survey. The leftmost
panel shows the (estimated) L1 coverage risk (right panel) for estimating density ridges under dif-
ferent smoothing parameters. We estimated the L1 coverage risk by using data splitting. For the
rest panels  from left to right  we display the case for under-smoothing  optimal smoothing  and
over-smoothing. As can be seen easily  the optimal smoothing method allows the SCMS algorithm
to detect the intricate cosmic network structure.

5.2 Cosmic Web

Now we apply our technique to the Sloan Digital Sky Survey  a huge dataset that contains millions
of galaxies. In our data  each point is an observed galaxy with three features:

• z: the redshift  which is the distance from the galaxy to Earth.
• RA: the right ascension  which is the longitude of the Universe.
• dec: the declination  which is the latitude of the Universe.

These three features (z  RA  dec) uniquely determine the location of a given galaxy.

To demonstrate the effectiveness of our method  we select a 2-D slice of our Universe at redshift
z = 0.050 − 0.055 with (RA  dec) ∈ [200  240] × [0  40]. Since the redshift difference is very tiny 
we ignore the redshift value of the galaxies within this region and treat them as a 2-D data points.
Thus  we only use RA and dec. Then we apply the SCMS algorithm (version of [7]) with data
splitting method introduced in section 2.1 to select the smoothing parameter h. The result is given in
Figure 4. The left panel provides the estimated coverage risk at different smoothing bandwidth. The
rest panels give the result for under-smoothing (second panel)  optimal smoothing (third panel) and
over-smoothing (right most panel). In the third panel of Figure 4  we see that the SCMS algorithm
detects the ﬁlament structure in the data.

6 Discussion

In this paper  we propose a method using coverage risk  a generalization of mean integrated square
error  to select the smoothing parameter for the density ridge estimation problem. We show that
the coverage risk can be estimated using data splitting or smoothed bootstrap and we derive the
statistical consistency for risk estimators. Both simulation and real data analysis show that the
proposed bandwidth selector works very well in practice.

The concept of coverage risk is not limited to density ridges; instead  it can be easily generalized to
other manifold learning technique. Thus  we can use data splitting to estimate the risk and use the
risk estimator to select the tuning parameters. This is related to the so-called stability selection [23] 
which allows us to select tuning parameters even in an unsupervised learning settings.

8

0.00.20.40.60.81.00.91.01.11.21.31.41.5Smoothing Parameter(Estimated) L1 Coverage RiskllllllllllllllllllllReferences

[1] E. Bas  N. Ghadarghadar  and D. Erdogmus. Automated extraction of blood vessel networks from 3d
microscopy image stacks via multi-scale principal curve tracing. In Biomedical Imaging: From Nano to
Macro  2011 IEEE International Symposium on  pages 1358–1361. IEEE  2011.

[2] E. Bas  D. Erdogmus  R. Draft  and J. W. Lichtman. Local tracing of curvilinear structures in volumet-
ric color images: application to the brainbow analysis. Journal of Visual Communication and Image
Representation  23(8):1260–1271  2012.

[3] B. Cadre. Kernel estimation of density level sets. Journal of multivariate analysis  2006.
[4] Y.-C. Chen  C. R. Genovese  R. J. Tibshirani  and L. Wasserman. Nonparametric modal regression. arXiv

preprint arXiv:1412.1716  2014.

[5] Y.-C. Chen  C. R. Genovese  and L. Wasserman. Generalized mode and ridge estimation.

1406.1803  June 2014.

arXiv:

[6] Y.-C. Chen  C. R. Genovese  and L. Wasserman. Asymptotic theory for density ridges. arXiv preprint

arXiv:1406.5663  2014.

[7] Y.-C. Chen  S. Ho  P. E. Freeman  C. R. Genovese  and L. Wasserman. Cosmic web reconstruction

through density ridges: Method and algorithm. arXiv preprint arXiv:1501.05303  2015.

[8] Y. Cheng. Mean shift  mode seeking  and clustering. Pattern Analysis and Machine Intelligence  IEEE

Transactions on  17(8):790–799  1995.

[9] A. Cuevas  W. Gonzalez-Manteiga  and A. Rodriguez-Casal. Plug-in estimation of general level sets.

Aust. N. Z. J. Stat.  2006.

[10] D. Eberly. Ridges in Image and Data Analysis. Springer  1996.
[11] J. Einbeck. Bandwidth selection for mean-shift based unsupervised learning techniques: a uniﬁed ap-

proach via self-coverage. Journal of pattern recognition research.  6(2):175–192  2011.

[12] U. Einmahl and D. M. Mason. Uniform in bandwidth consistency for kernel-type function estimators.

The Annals of Statistics  2005.

[13] L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions  volume 5. CRC press 

1991.

[14] K. Fukunaga and L. Hostetler. The estimation of the gradient of a density function  with applications in

pattern recognition. Information Theory  IEEE Transactions on  21(1):32–40  1975.

[15] C. R. Genovese  M. Perone-Paciﬁco  I. Verdinelli  and L. Wasserman. Nonparametric ridge estimation.

The Annals of Statistics  42(4):1511–1545  2014.

[16] E. Gine and A. Guillou. Rates of strong uniform consistency for multivariate kernel density estimators.

In Annales de l’Institut Henri Poincare (B) Probability and Statistics  2002.

[17] T. Hastie. Principal curves and surfaces. Technical report  DTIC Document  1984.
[18] T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association  84(406):

502–516  1989.

[19] M. C. Jones  J. S. Marron  and S. J. Sheather. A brief survey of bandwidth selection for density estimation.

Journal of the American Statistical Association  91(433):401–407  1996.

[20] D. M. Mason  W. Polonik  et al. Asymptotic normality of plug-in level set estimates. The Annals of

Applied Probability  19(3):1108–1142  2009.

[21] Z. Miao  B. Wang  W. Shi  and H. Wu. A method for accurate road centerline extraction from a classiﬁed

image. 2014.

[22] U. Ozertem and D. Erdogmus. Locally deﬁned principal curves and surfaces. Journal of Machine Learn-

ing Research  2011.

[23] A. Rinaldo and L. Wasserman. Generalized density clustering. The Annals of Statistics  2010.
[24] D. W. Scott. Multivariate density estimation: theory  practice  and visualization  volume 383. John Wiley

& Sons  2009.

[25] B. Silverman and G. Young. The bootstrap: To smooth or not to smooth? Biometrika  74(3):469–479 

1987.

[26] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall  1986.
[27] R. Tibshirani. Principal curves revisited. Statistics and Computing  2(4):183–190  1992.
[28] L. Wasserman. All of Nonparametric Statistics. Springer-Verlag New York  Inc.  2006.

—

9

,Yen-Chi Chen
Christopher Genovese
Shirley Ho
Larry Wasserman
Wenqi Ren
Jiawei Zhang
Lin Ma
Jinshan Pan
Xiaochun Cao
Wangmeng Zuo
Wei Liu
Ming-Hsuan Yang