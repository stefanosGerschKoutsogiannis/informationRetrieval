2016,LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain,We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently  a few breakthroughs have been discovered on k-SVD: Musco and Musco [1] proved the first gap-free convergence result using the block Krylov method  Shamir [2] discovered the first variance-reduction stochastic method  and Bhojanapalli et al. [3] provided the fastest $O(\mathsf{nnz}(A) + \mathsf{poly}(1/\varepsilon))$-time algorithm using alternating minimization.

In this paper  we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [1]  and the first accelerated and stochastic method outperforming [2]. In the $O(\mathsf{nnz}(A) + \mathsf{poly}(1/\varepsilon))$ running-time regime  LazySVD outperforms [3] in certain parameter regimes without even using alternating minimization.,LazySVD: Even Faster SVD Decomposition

Yet Without Agonizing Pain∗

Zeyuan Allen-Zhu

zeyuan@csail.mit.edu

Institute for Advanced Study

& Princeton University

Yuanzhi Li

yuanzhil@cs.princeton.edu

Princeton University

Abstract

We study k-SVD that is to obtain the ﬁrst k singular vectors of a matrix A.
Recently  a few breakthroughs have been discovered on k-SVD: Musco and
Musco [19] proved the ﬁrst gap-free convergence result using the block Krylov
method  Shamir [21] discovered the ﬁrst variance-reduction stochastic method  and
Bhojanapalli et al. [7] provided the fastest O(nnz(A) + poly(1/ε))-time algorithm
using alternating minimization.
In this paper  we put forward a new and simple LazySVD framework to improve
the above breakthroughs. This framework leads to a faster gap-free method outper-
forming [19]  and the ﬁrst accelerated and stochastic method outperforming [21].
In the O(nnz(A) + poly(1/ε)) running-time regime  LazySVD outperforms [7] in
certain parameter regimes without even using alternating minimization.

Introduction

1
The singular value decomposition (SVD) of a rank-r matrix A ∈ Rd×n corresponds to decomposing
A = V ΣU(cid:62) where V ∈ Rd×r  U ∈ Rn×r are two column orthonormal matrices  and Σ =
diag{σ1  . . .   σr} ∈ Rr×r is a non-negative diagonal matrix with σ1 ≥ σ2 ≥ ··· ≥ σr ≥ 0. The
columns of V (resp. U) are called the left (resp. right) singular vectors of A and the diagonal entries
of Σ are called the singular values of A. SVD is one of the most fundamental tools used in machine
learning  computer vision  statistics  and operations research  and is essentially equivalent to principal
component analysis (PCA) up to column averaging.
A rank k partial SVD  or k-SVD for short  is to ﬁnd the top k left singular vectors of A  or equivalently 
the ﬁrst k columns of V . Denoting by Vk ∈ Rd×k the ﬁrst k columns of V   and Uk the ﬁrst k
k where Σk = diag{σ1  . . .   σk}. Under
columns of U  one can deﬁne A∗
k is the the best rank-k approximation of matrix A in terms of minimizing (cid:107)A − Ak(cid:107)
this notation  A∗
among all rank k matrices Ak. Here  the norm can be any Schatten-q norm for q ∈ [1 ∞]  including
spectral norm (q = ∞) and Frobenius norm (q = 2)  therefore making k-SVD a very powerful tool
for information retrieval  data de-noising  or even data compression.
Traditional algorithms to compute SVD essentially run in time O(nd min{d  n})  which is usually
very expensive for big-data scenarios. As for k-SVD  deﬁning gap := (σk − σk+1)/(σk) to be the
relative k-th eigengap of matrix A  the famous subspace power method or block Krylov method [14]
solves k-SVD in time O(gap−1·k·nnz(A)·log(1/ε)) or O(gap−0.5·k·nnz(A)·log(1/ε)) respectively
if ignoring lower-order terms. Here  nnz(A) is the number of non-zero elements in matrix A  and the
more precise running times are stated in Table 1.
Recently  there are breakthroughs to compute k-SVD faster  from three distinct perspectives.

k A = VkΣkU(cid:62)

k := VkV (cid:62)

∗The full version of this paper can be found on https://arxiv.org/abs/1607.03463. This paper is partially

supported by a Microsoft Research Award  no. 0518584  and an NSF grant  no. CCF-1412958.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Paper

subspace PM [19]

block Krylov [19]

LazySVD

Corollary 4.3 and 4.4

Shamir [21]

LazySVD

Corollary 4.3 and 4.4

ε3/2

ε + k3
gap + k3

gap3/2

ε

gap

(cid:1)
(cid:1)

Running time

ε + k2d
gap + k2d
ε1/2 + k2d
gap1/2 + k2d
ε1/2 + k2d
gap1/2 + k2d

(cid:101)O(cid:0) knnz(A)
(cid:101)O(cid:0) knnz(A)
(cid:101)O(cid:0) knnz(A)
(cid:101)O(cid:0) knnz(A)
(cid:101)O(cid:0) knnz(A)
(cid:1)
(cid:101)O(cid:0) knnz(A)
(cid:101)O(cid:0)knd + k4d
(cid:1)
(cid:101)O(cid:0)knd + kn3/4d
(cid:101)O(cid:0)knd + kn3/4d

1/2
k ε1/2

σ4
kgap2

gap1/2

ε1/2

σ

σ

1/2
k gap1/2

(cid:1)
(cid:1)

(× for being outperformed) GF?
× yes
× no
× yes
× no

(cid:1)

(cid:1)

Stoc?

Acc?

no

no

no

yes

(local convergence only)

(cid:16)
always ≤ (cid:101)O(cid:0)knd + kd
(cid:1)(cid:16)
always ≤ (cid:101)O(cid:0)knd + kd

σ2
kε2

(cid:1)(cid:17)
(cid:1)(cid:17)

σ2
kgap2

no
× no

yes

no

yes

no

yes

yes

yes

no

yes

All GF results above provide (1 + ε)(cid:107)∆(cid:107)2 spectral and (1 + ε)(cid:107)∆(cid:107)F Frobenius guarantees

Table 1: Performance comparison among direct methods. Deﬁne gap = (σk − σk+1)/σk ∈ [0  1]. GF = Gap
Free; Stoc = Stochastic; Acc = Accelerted. Stochastic results in this table are assuming (cid:107)ai(cid:107)2 ≤ 1
following (1.1).

The ﬁrst breakthrough is the work of Musco and Musco [19] for proving a running time for k-
SVD that does not depend on singular value gaps (or any other properties) of A. As highlighted
in [19]  providing gap-free results was an open question for decades and is a more reliable goal
for practical purposes. Speciﬁcally  they proved that the block Krylov method converges in time

(cid:1)  where ε is the multiplicative approximation error.2

(cid:101)O(cid:0) knnz(A)

ε1/2 + k2d

ε + k3

ε3/2

i

n

(cid:80)n
i=1 aia(cid:62)

i and each ai ∈ Rd has norm at most 1 .

The second breakthrough is the work of Shamir [21] for providing a fast stochastic k-SVD algorithm.
In a stochastic setting  one assumes3
A is given in form AA(cid:62) = 1

(1.1)
Instead of repeatedly multiplying matrix AA(cid:62) to a vector in the (subspace) power method  Shamir
proposed to use a random rank-1 copy aia(cid:62)
to approximate such multiplications. When equipped
with very ad-hoc variance-reduction techniques  Shamir showed that the algorithm has a better (local)
performance than power method (see Table 1). Unfortunately  Shamir’s result is (1) not gap-free; (2)
not accelerated (i.e.  does not match the gap−0.5 dependence comparing to block Krylov); and (3)
requires a very accurate warm-start that in principle can take a very long time to compute.

The third breakthrough is in obtaining running times of the form (cid:101)O(nnz(A) + poly(k  1/ε) · (n +

d)) [7  8]  see Table 2. We call them NNZ results. To obtain NNZ results  one needs sub-sampling
on the matrix and this incurs a poor dependence on ε. For this reason  the polynomial dependence
on 1/ε is usually considered as the most important factor. In 2015  Bhojanapalli et al. [7] obtained
a 1/ε2-rate NNZ result using alternating minimization. Since 1/ε2 also shows up in the sampling
complexity  we believe the quadratic dependence on ε is tight among NNZ types of algorithms.
All the cited results rely on ad-hoc non-convex optimization techniques together with matrix algebra 
which make the ﬁnal proofs complicated. Furthermore  Shamir’s result [21] only works if a 1/poly(d)-
accurate warm start is given  and the time needed to ﬁnd a warm start is unclear.
In this paper  we develop a new algorithmic framework to solve k-SVD. It not only improves the
aforementioned breakthroughs  but also relies only on simple convex analysis.

2In this paper  we use (cid:101)O notations to hide possible logarithmic factors on 1/gap  1/ε  n  d  k and potentially

3This normalization follows the tradition of stochastic k-SVD or 1-SVD literatures [12  20  21] in order to

also on σ1/σk+1.

state results more cleanly.

2

Paper

[8]

[7]

LazySVD

Theorem 5.1

Running time

ε4 (n + d) + k3

O(nnz(A)) + O(cid:0) k2
(cid:1)
(n + d)(cid:1)
O(nnz(A)) + (cid:101)O(cid:0) k5(σ1/σk)2
d(cid:1)
O(nnz(A)) + (cid:101)O(cid:0) k2(σ1/σk+1)4
(n + d)(cid:1)
O(nnz(A)) + (cid:101)O(cid:0) k2(σ1/σk+1)2
O(nnz(A)) + (cid:101)O(cid:0) k4(σ1/σk+1)4.5
d(cid:1)

ε2

ε5

ε2

ε2.5

ε2

Frobenius norm
(1 + ε)(cid:107)∆(cid:107)F
(1 + ε)(cid:107)∆(cid:107)F

N/A

N/A

(1 + ε)(cid:107)∆(cid:107)2

Spectral norm
(1 + ε)(cid:107)∆(cid:107)F
(cid:107)∆(cid:107)2 + ε(cid:107)∆(cid:107)F
(cid:107)∆(cid:107)2 + ε(cid:107)∆(cid:107)F
(cid:107)∆(cid:107)2 + ε(cid:107)∆(cid:107)F
(cid:107)∆(cid:107)2 + ε(cid:107)∆(cid:107)F

Table 2: Performance comparison among O(nnz(A) + poly(1/ε)) type of algorithms. Remark: we have not

tried hard to improve the dependency with respect to k or (σ1/σk+1). See Remark 5.2.

1.1 Our Results and the Settlement of an Open Question
We propose to use an extremely simple framework that we call LazySVD to solve k-SVD:

LazySVD: perform 1-SVD repeatedly  k times in total.

More speciﬁcally  in this framework we ﬁrst compute the leading singular vector v of A  and then
left-project (I − vv(cid:62))A and repeat this procedure k times. Quite surprisingly 

This seemingly “most-intuitive” approach was widely considered as “not a good idea.”

In textbooks and research papers  one typically states that LazySVD has a running time that inversely
depends on all the intermediate singular value gaps σ1−σ2  . . .   σk−σk+1 [18  21]. This dependence
makes the algorithm useless if some singular values are close  and is even thought to be necessary [18].
For this reason  textbooks describe only block methods (such as block power method  block Krylov 
alternating minimization) which ﬁnd the top k singular vectors together. Musco and Musco [19]
stated as an open question to design “single-vector” methods without running time dependence on all
the intermediate singular value gaps.
In this paper  we fully answer this open question with novel analyses on this LazySVD framework.
In particular  the resulting running time either
• depends on gap−0.5 where gap is the relative singular value gap only between σk and σk+1  or
• depends on ε−0.5 where ε is the approximation ratio (so is gap-free).
Such dependency matches the best known dependency for block methods.
More surprisingly  by making different choices of the 1-SVD subroutine in this LazySVD framework 
we obtain multiple algorithms for different needs (see Table 1 and 2):
• If accelerated gradient descent or Lanczos algorithm is used for 1-SVD  we obtain a faster k-SVD
• If a variance-reduction stochastic method is used for 1-SVD  we obtain the ﬁrst accelerated

algorithm than block Krylov [19].

• If one sub-samples A before applying LazySVD  the running time becomes (cid:101)O(nnz(A) +

stochastic algorithm for k-SVD  and this outperforms Shamir [21].
ε−2poly(k)· d). This improves upon [7] in certain (but sufﬁciently interesting) parameter regimes 
but completely avoids the use of alternating minimization.

Finally  besides the running time advantages above  our analysis is completely based on convex
optimization because 1-SVD is solvable using convex techniques. LazySVD also works when k is
not known to the algorithm  as opposed to block methods which need to know k in advance.

Other Related Work. Some authors focus on the streaming or online model of 1-SVD [4  15  17] or
k-SVD [3]. These algorithms are slower than ofﬂine methods. Unlike k-SVD  accelerated stochastic
methods were previously known for 1-SVD [12  13]. After this paper is published  LazySVD has
been generalized to also solve canonical component analysis and generalized PCA by the same
authors [1]. If one is only interested in projecting a vector to the top k-eigenspace without computing
the top k eigenvectors like we do in this paper  this can also be done in an accelerated manner [2].

3

2 Preliminaries
Given a matrix A we denote by (cid:107)A(cid:107)2 and (cid:107)A(cid:107)F respectively the spectral and Frobenius norms of A.
For q ≥ 1  we denote by (cid:107)A(cid:107)Sq the Schatten q-norm of A. We write A (cid:23) B if A  B are symmetric
and A − B is positive semi-deﬁnite (PSD). We denote by λk(M ) the k-th largest eigenvalue of a
symmetric matrix M  and σk(A) the k-th largest singular value of a rectangular matrix A.
Since λk(AA(cid:62)) = λk(A(cid:62)A) = (σk(A))2 

solving k-SVD for A is the same as solving k-PCA for M = AA(cid:62).

k the best rank-k approximation of A.

We denote by σ1 ≥ ··· σd ≥ 0 the singular values of A ∈ Rd×n  by λ1 ≥ ··· λd ≥ 0 the eigenvalues
of M = AA(cid:62) ∈ Rd×d. (Although A may have fewer than d singular values for instance when n < d 
if this happens  we append zeros.) We denote by A∗
We use ⊥ to denote the orthogonal complement of a matrix. More speciﬁcally  given a column
orthonormal matrix U ∈ Rd×k  we deﬁne U⊥ := {x ∈ Rd | U(cid:62)x = 0}. For notational simplicity 
we sometimes also denote U⊥ as a d × (d − k) matrix consisting of some basis of U⊥.
Theorem 2.1 (approximate matrix inverse). Given d × d matrix M (cid:23) 0 and constants λ  δ > 0
satisfying λI − M (cid:23) δI  one can minimize the quadratic f (x) := x(cid:62)(λI − M )x − b(cid:62)x in order to

invert (λI − M )−1b. Suppose the desired accuracy is(cid:13)(cid:13)x − (λI − M )−1b(cid:13)(cid:13) ≤ ε. Then 
• Accelerated gradient descent (AGD) produces such an output x in O(cid:0) λ1/2
instance [5]) produces such an output x in time O(cid:0) max{nd  n3/4dλ1/4

(cid:1) iterations  each
(cid:1).

i and (cid:107)ai(cid:107)2 ≤ 1  then accelerated SVRG (see for

requiring O(d) time plus the time needed to multiply M with a vector.

• If M is given in the form M = 1

(cid:80)n
i=1 aia(cid:62)

(cid:9) log λ

δ1/2 log λ

εδ

n

δ1/2

εδ

3 A Speciﬁc 1-SVD Algorithm: Shift-and-Inverse Revisited
In this section  we study a speciﬁc 1-PCA algorithm AppxPCA (recall 1-PCA equals 1-SVD). It is a
(multiplicative-)approximate algorithm for computing the leading eigenvector of a symmetric matrix.
We emphasize that  in principle  most known 1-PCA algorithms (e.g.  power method  Lanczos
method) are suitable for our LazySVD framework. We choose AppxPCA solely because it provides
the maximum ﬂexibility in obtaining all stochastic / NNZ running time results at once.
Our AppxPCA uses the shift-and-inverse routine [12  13]  and our pseudo-code in Algorithm 1 is
a modiﬁcation of Algorithm 5 that appeared in [12]. Since we need a more reﬁned running time
statement with a multiplicative error guarantee  and since the stated proof in [12] is anyways only a
sketched one  we choose to carefully reprove a similar result of [12] and state the following theorem:
Theorem 3.1 (AppxPCA). Let M ∈ Rd×d be a symmetric matrix with eigenvalues 1 ≥ λ1 ≥ ··· ≥
λd ≥ 0 and corresponding eigenvectors u1  . . .   ud. With probability at least 1 − p  AppxPCA

(w(cid:62)ui)2 ≤ ε and w(cid:62)M w ≥ (1 − δ×)(1 − ε)λ1 .

produces an output w satisfying(cid:88)

i∈[d] λi≤(1−δ×)λ1

λ(s)

δ× and

λmin(λ(s)I−M ) ≤ 12

Furthermore  the total number of oracle calls to A is O(log(1/δ×)m1 + m2)  and each time we call
A we have
Since AppxPCA reduces 1-PCA to oracle calls of a matrix inversion subroutine A  the stated conditions
λmin(λ(s)I−M ) ≤ 12
in Theorem 3.1  together with complexity results
for matrix inversions (see Theorem 2.1)  imply the following running times for AppxPCA:
Corollary 3.2.

λmin(λ(s)I−M ) ≤ 12

λmin(λ(s)I−M ) ≤ 12

δ× and

δ×λ1

δ×λ1

λ(s)

.

1

1

(cid:1) multiplied with O(d) plus the time needed

• If A is AGD  the running time of AppxPCA is (cid:101)O(cid:0) 1
time of AppxPCA is (cid:101)O(cid:0) max{nd  n3/4d

(cid:80)n
i=1 aia(cid:62)

to multiply M with a vector.

• If M = 1

(cid:9)(cid:1).

n

δ1/2×

λ1/4

1

δ1/2×

i where each (cid:107)ai(cid:107)2 ≤ 1  and A is accelerated SVRG  then the total running

4

Algorithm 1 AppxPCA(A  M  δ×  ε  p)
practitioners  feel free to use your favorite 1-PCA algorithm such as Lanczos to replace AppxPCA.)
Input: A  an approximate matrix inversion method; M ∈ Rd×d  a symmetric matrix satisfying
0 (cid:22) M (cid:22) I; δ× ∈ (0  0.5]  a multiplicative error; ε ∈ (0  1)  a numerical accuracy parameter;
and p ∈ (0  1)  a conﬁdence parameter. (cid:5) running time only logarithmically depends on 1/ε and 1/p.

(cid:5) (only for proving our theoretical results; for

(cid:5) m1 = T PM(8  1/32  p) and m2 = T PM(2  ε/4  p) using deﬁnition in Lemma A.1

6

;

p2

8m2

6

p2ε

log

64m1

4 log

s ← s + 1;
for t = 1 to m1 do

(cid:17)(cid:109)
(cid:16) 36d
(cid:1)m2
(cid:0) δ×

1: m1 ←(cid:108)
2: (cid:101)ε1 ← 1
3: (cid:98)w0 ← a random unit vector; s ← 0; λ(0) ← 1 + δ×;

(cid:16) 288d
  m2 ←(cid:108)
(cid:17)(cid:109)
(cid:0) δ×
(cid:1)m1 and(cid:101)ε2 ← ε
Apply A to ﬁnd (cid:98)wt satisfying(cid:13)(cid:13)(cid:98)wt − (λ(s−1)I − M )−1(cid:98)wt−1
w ← (cid:98)wm1/(cid:107)(cid:98)wm1(cid:107);
Apply A to ﬁnd v satisfying(cid:13)(cid:13)v − (λ(s−1)I − M )−1w(cid:13)(cid:13) ≤(cid:101)ε1;
w(cid:62)v−(cid:101)ε1
(cid:13)(cid:13) ≤(cid:101)ε2;
Apply A to ﬁnd (cid:98)wt satisfying(cid:13)(cid:13)(cid:98)wt − (λ(f )I − M )−1(cid:98)wt−1

4: repeat
5:
6:
7:
8:
9:
10:
11: until ∆(s) ≤ δ×λ(s)
12: f ← s;
13: for t = 1 to m2 do
14:

and λ(s) ← λ(s−1) − ∆(s)
2 ;

∆(s) ← 1
2 ·

15: return w := (cid:98)wm2/(cid:107)(cid:98)wm2(cid:107).

1

3

(cid:13)(cid:13) ≤(cid:101)ε1;

Algorithm 2 LazySVD(A  M  k  δ×  εpca  p)
Input: A  an approximate matrix inversion method; M ∈ Rd×d  a matrix satisfying 0 (cid:22) M (cid:22) I;
k ∈ [d]  the desired rank; δ× ∈ (0  1)  a multiplicative error; εpca ∈ (0  1)  a numerical accuracy
parameter; and p ∈ (0  1)  a conﬁdence parameter.

1: M0 ← M and V0 ← [];
2: for s = 1 to k do
3:

vs ←(cid:0)(I − Vs−1V (cid:62)

Vs ← [Vs−1  vs];
Ms ← (I − vsv(cid:62)

4:
5:
6:
7: end for
8: return Vk.

s ← AppxPCA(A  Ms−1  δ×/2  εpca  p/k);
v(cid:48)

(cid:1)/(cid:13)(cid:13)(I − Vs−1V (cid:62)

(cid:13)(cid:13);

s−1)v(cid:48)

(cid:5) to practitioners: use your favorite 1-PCA algorithm such as Lanczos to compute v(cid:48)
s
s to V ⊥
s−1
s )M (I − VsV (cid:62)
s )

(cid:5) we also have Ms = (I − VsV (cid:62)

s )Ms−1(I − vsv(cid:62)
s )

(cid:5) project v(cid:48)

s−1)v(cid:48)

s

s

4 Main Algorithm and Theorems
Our algorithm LazySVD is stated in Algorithm 2. It starts with M0 = M  and repeatedly applies
k times AppxPCA. In the s-th iteration  it computes an approximate leading eigenvector of matrix
Ms−1 using AppxPCA with a multiplicative error δ×/2  projects Ms−1 to the orthogonal space of this
vector  and then calls it matrix Ms.
In this stated form  LazySVD ﬁnds approximately the top k eigenvectors of a symmetric matrix
M ∈ Rd×d. If M is given as M = AA(cid:62)  then LazySVD automatically ﬁnds the k-SVD of A.

4.1 Our Core Theorems

We state our approximation and running time core theorems of LazySVD below  and then provide
corollaries to translate them into gap-dependent and gap-free theorems on k-SVD.
Theorem 4.1 (approximation). Let M ∈ Rd×d be a symmetric matrix with eigenvalues 1 ≥ λ1 ≥
··· λd ≥ 0 and corresponding eigenvectors u1  . . .   ud. Let k ∈ [d]  let δ×  p ∈ (0  1)  and let εpca ≤

5

k )M (I − VkV (cid:62)

poly(cid:0)ε  δ×  1

(cid:1).4 Then  LazySVD outputs a (column) orthonormal matrix Vk = (v1  . . .   vk) ∈

d   λ1
λk+1
Rd×k which  with probability at least 1 − p  satisﬁes all of the following properties. (Denote by
Mk = (I − VkV (cid:62)
(a) Core lemma: (cid:107)V (cid:62)
(b) Spectral norm guarantee: λk+1 ≤ (cid:107)Mk(cid:107)2 ≤ λk+1
1−δ× .
(c) Rayleigh quotient guarantee: (1 − δ×)λk ≤ v(cid:62)
k M vk ≤ 1
(d) Schatten-q norm guarantee: for every q ≥ 1  we have (cid:107)Mk(cid:107)Sq ≤ (1+δ×)2
(1−δ×)2

k U(cid:107)2 ≤ ε  where U = (uj  . . .   ud) is the (column) orthonormal matrix and j
(cid:17)1/q

is the smallest index satisfying λj ≤ (1 − δ×)(cid:107)Mk−1(cid:107)2.

(cid:16)(cid:80)d

1−δ× λk.

i=k+1 λq

k ).)

.

i

δ1/2×

We defer the proof of Theorem 4.1 to the full version  and we also have a section in the full version
to highlight the technical ideas behind the proof. Below we state the running time of LazySVD.
Theorem 4.2 (running time). LazySVD can be implemented to run in time

• (cid:101)O(cid:0) knnz(M )+k2d
• (cid:101)O(cid:0) knnz(A)+k2d
• (cid:101)O(cid:0)knd + kn3/4d
i where each (cid:107)ai(cid:107)2 ≤ 1.
Above  the (cid:101)O notation hides logarithmic factors with respect to k  d  1/δ×  1/p  1/λ1  λ1/λk.

(cid:1) if A is AGD and M ∈ Rd×d is given explicitly;
(cid:1) if A is AGD and M is given as M = AA(cid:62) where A ∈ Rd×n; or
(cid:1) if A is accelerated SVRG and M = 1
(cid:80)n
i=1 aia(cid:62)

λ1/4
k δ1/2×

δ1/2×

n

s−1)M (I − Vs−1V (cid:62)

s )ai for each vector ai  and feed the new a(cid:48)

s−1. This proves the ﬁrst two running times using Corollary 3.2.

Proof of Theorem 4.2. We call k times AppxPCA  and each time we can feed Ms−1 = (I −
Vs−1V (cid:62)
s−1) implicitly into AppxPCA thus the time needed to multiply Ms−1 with
a d-dimensional vector is O(dk + nnz(M )) or O(dk + nnz(A)). Here  the O(dk) overhead is due to
the projection of a vector into V ⊥
i ← (I −
To obtain the third running time  when we compute Ms from Ms−1  we explicitly project a(cid:48)
vsv(cid:62)
n into AppxPCA. Now the running time follows
from the second part of Corollary 3.2 together with the fact that (cid:107)Ms−1(cid:107)2 ≥ (cid:107)Mk−1(cid:107)2 ≥ λk. (cid:3)
4.2 Our Main Results for k-SVD
Our main theorems imply the following corollaries (proved in full version of this paper).
Corollary 4.3 (Gap-dependent k-SVD). Let A ∈ Rd×n be a matrix with singular values 1 ≥ σ1 ≥
··· σd ≥ 0 and the corresponding left singular vectors u1  . . .   ud ∈ Rd. Let gap = σk−σk+1
be the
relative gap. For ﬁxed ε  p > 0  consider the output

1  . . .   a(cid:48)

.
Then  deﬁning W = (uk+1  . . .   ud)  we have with probability at least 1 − p:

Vk is a rank-k (column) orthonormal matrix with (cid:107)V (cid:62)

Our running time is (cid:101)O(cid:0) knnz(A)+k2d

√

gap

σk

(cid:16)A  AA(cid:62)  k  gap  O(cid:0) ε4·gap2
(cid:1)  or time (cid:101)O(cid:0)knd + kn3/4d

(cid:17)
(cid:1)  p
(cid:1) in the stochastic setting (1.1).

k W(cid:107)2 ≤ ε .

k4(σ1/σk)4

√

σ1/2

k

gap

Vk ← LazySVD

Above  both running times depend only poly-logarithmically on 1/ε.
Corollary 4.4 (Gap-free k-SVD). Let A ∈ Rd×n be a matrix with singular values 1 ≥ σ1 ≥
··· σd ≥ 0. For ﬁxed ε  p > 0  consider the output

(v1  . . .   vk) = Vk ← LazySVD

3   O

ε6

k4d4(σ1/σk+1)12

  p

.

(cid:16)A  AA(cid:62)  k  ε

(cid:16)

(cid:17)

(cid:17)

Then  deﬁning Ak = VkV (cid:62)

k A which is a rank k matrix  we have with probability at least 1 − p:

4The detailed speciﬁcations of εpca can be found in the appendix where we restate the theorem more formally.
To provide the simplest proof  we have not tightened the polynomial factors in the theoretical upper bound of
εpca because the running time depends only logarithmic on 1/εpca.

6

1. Spectral norm guarantee: (cid:107)A − Ak(cid:107)2 ≤ (1 + ε)(cid:107)A − A∗
k(cid:107)2;
k(cid:107)F ; and
2. Frobenius norm guarantee: (cid:107)A − Ak(cid:107)F ≤ (1 + ε)(cid:107)A − A∗

3. Rayleigh quotient guarantee: ∀i ∈ [k] (cid:12)(cid:12)v(cid:62)
Running time is (cid:101)O(cid:0) knnz(A)+k2d

(cid:1)  or time (cid:101)O(cid:0)knd + kn3/4d

i AA(cid:62)vi − σ2
√

(cid:12)(cid:12) ≤ εσ2
(cid:1) in the stochastic setting (1.1).

i .

i

√

ε

σ1/2

k

ε

Remark 4.5. The spectral and Frobenius guarantees are standard. The spectral guarantee is more
desirable than the Frobenius one in practice [19]. In fact  our algorithm implies for all q ≥ 1 
(cid:107)A − Ak(cid:107)Sq ≤ (1 + ε)(cid:107)A − A∗
k(cid:107)Sq where (cid:107) · (cid:107)Sq is the Schatten-q norm. Rayleigh-quotient
guarantee was introduced by Musco and Musco [19] for a more reﬁned comparison. They showed
that the block Krylov method satisﬁes |v(cid:62)
k+1  which is slightly stronger than ours.
However  these two guarantees are not much different in practice as we evidenced in experiments.

i AA(cid:62)vi−σ2

i | ≤ εσ2

k(cid:107)2 + ε(cid:107)A − A∗
k(cid:107)F .

5 NNZ Running Time
In this section  we translate our results in the previous section into the O(nnz(A) + poly(k  1/ε)(n +
d)) running-time statements. The idea is surprisingly simple: we sample either random columns
of A  or random entries of A  and then apply LazySVD to compute the k-SVD. Such translation
directly gives either 1/ε2.5 results if AGD is used as the convex subroutine and either column or entry
sampling is used  or a 1/ε2 result if accelerated SVRG and column sampling are used together.
We only informally state our theorem and defer all the details to the full paper.
Theorem 5.1 (informal). Let A ∈ Rd×n be a matrix with singular values σ1 ≥ ··· ≥ σd ≥ 0.
For every ε ∈ (0  1/2)  one can apply LazySVD with appropriately chosen δ× on a “carefully
sub-sampled version” of A. Then  the resulting matrix V ∈ Rd×k can satisfy
• spectral norm guarantee: (cid:107)A − V V (cid:62)A(cid:107)2 ≤ (cid:107)A − A∗
k(cid:107)F ;5
• Frobenius norm guarantee: (cid:107)A − V V (cid:62)A(cid:107)F ≤ (1 + ε)(cid:107)A − A∗
The total running time depends on (1) whether column or entry sampling is used  (2) which matrix
inversion routine A is used  and (3) whether spectral or Frobenius guarantee is needed. We list our
deduced results in Table 2 and the formal statements can be found in the full version of this paper.
Remark 5.2. The main purpose of our NNZ results is to demonstrate the strength of LazySVD
framework in terms of improving the ε dependency to 1/ε2. Since the 1/ε2 rate matches sampling
complexity  it is very challenging have an NNZ result with 1/ε2 dependency.6 We have not tried
hard  and believe it possible  to improve the polynomial dependence with respect to k or (σ1/σk+1).
6 Experiments
We demonstrate the practicality of our LazySVD framework  and compare it to block power method
or block Krylov method. We emphasize that in theory  the best worse-cast complexity for 1-PCA
is obtained by AppxPCA on top of accelerated SVRG. However  for the size of our chosen datasets 
Lanczos method runs faster than AppxPCA and therefore we adopt Lanczos method as the 1-PCA
method for our LazySVD framework.7
Datasets. We use datasets SNAP/amazon0302  SNAP/email-enron  and news20 that were also
used by Musco and Musco [19]  as well as an additional but famous dataset RCV1. The ﬁrst two can
be found on the SNAP website [16] and the last two can be found on the LibSVM website [11]. The
four datasets give rise sparse matrices of dimensions 257570×262111  35600×16507  11269×53975 
and 20242 × 47236 respectively.

5This is the best known spectral guarantee one can obtain using NNZ running time [7]. It is an open question

whether the stricter (cid:107)A − V V (cid:62)A(cid:107)2 ≤ (1 + ε)(cid:107)A − A∗

k(cid:107)2 type of spectral guarantee is possible.

6On one hand  one can use dimension reduction such as [9] to reduce the problem size to O(k/ε2); to the best
of our knowledge  it is impossible to obtain any NNZ result faster than 1/ε3 using solely dimension reduction.
On the other hand  obtaining 1/ε2 dependency was the main contribution of [7]: they relied on alternating
minimization but we have avoided it in our paper.

7Our LazySVD framework turns every 1-PCA method satisfying Theorem 3.1 (including Lanczos method)
into a k-SVD solver. However  our theoretical results (esp. stochastic and NNZ) rely on AppxPCA because
Lanczos is not a stochastic method.

7

(a) amazon  k = 20  spectral

(b) news  k = 20  spectral

(c) news  k = 20  rayleigh

(d) email  k = 10  Fnorm

(e) rcv1  k = 30  Fnorm

(f) rcv1  k = 30  rayleigh(last)

Figure 1: Selected performance plots. Relative error (y-axis) vs. running time (x-axis).

Implemented Algorithms. For the block Krylov method  it is a well-known issue that the Lanczos
type of three-term recurrence update is numerically unstable. This is why Musco and Musco [19]
only used the stable variant of block Krylov which requires an orthogonalization of each n× k matrix
with respect to all previously obtained n × k matrices. This greatly improves the numerical stability
albeit sacriﬁcing running time. We implement both these algorithms. In sum  we have implemented:
• PM: block power method for T iterations.
• Krylov: stable block Krylov method for T iterations [19].
• Krylov(unstable): the three-term recurrence implementation of block Krylov for T iterations.
• LazySVD: k calls of the vanilla Lanczos method  and each call runs T iterations.
A Fair Running-Time Comparison. For a ﬁxed integer T   the four methods go through the dataset
(in terms of multiplying A with column vectors) the same number of times. However  since LazySVD
does not need block orthogonalization (as needed in PM and Krylov) and does not need a (T k)-
dimensional SVD computation in the end (as needed in Krylov)  the running time of LazySVD is
clearly much faster for a ﬁxed value T . We therefore compare the performances of the four methods
in terms of running time rather than T .
We programmed the four algorithms using the same programming language with the same sparse-
matrix implementation. We tested them single-threaded on the same Intel i7-3770 3.40GHz personal
computer. As for the ﬁnal low-dimensional SVD decomposition step at the end of the PM or Krylov
method (which is not needed for our LazySVD)  we used a third-party library that is built upon the
x64 Intel Math Kernel Library so the time needed for such SVD is maximally reduced.
Performance Metrics. We compute four metrics on the output V = (v1  . . .   vk) ∈ Rn×k:
k(cid:107)F )/(cid:107)A − A∗
• Fnorm: relative Frobenius norm error: ((cid:107)A − V V (cid:62)A(cid:107)F − (cid:107)A − A∗
k(cid:107)F .
• spectral: relative spectral norm error: ((cid:107)A − V V (cid:62)A(cid:107)2 − (cid:107)A − A∗
k(cid:107)2.
k(cid:107)2)/(cid:107)A − A∗
j − v(cid:62)
• rayleigh(last): Rayleigh quotient error relative to σk+1: maxk
j AA(cid:62)vj
• rayleigh: relative Rayleigh quotient error: maxk
j − v(cid:62)
j .
The ﬁrst three metrics were also used by Musco and Musco [19]. We added the fourth one because
our theory only predicted convergence with respect to the fourth but not the third metric. However 
we observe that in practice they are not much different from each other.
Our Results. We study four datasets each with k = 10  20  30 and with the four performance
metrics  totaling 48 plots. Due to space limitation  we only select six representative plots out of 48
and include them in Figure 1. (The full plots can be found in Figure 2  3  4 and 5 in the appendix.)
We make the following observations:
• LazySVD outperforms its three competitors almost universally.
• Krylov(unstable) outperforms Krylov for small value T ; however  it is less useful for obtaining
accurate solutions due to its instability. (The dotted green curves even go up if T is large.)
• Subspace power method performs the slowest unsurprisingly due to its lack of acceleration.

(cid:12)(cid:12)σ2
(cid:12)(cid:12)/σ2

(cid:12)(cid:12)/σ2

j=1

j AA(cid:62)vj

(cid:12)(cid:12)σ2

j=1

k+1.

8

1E-31E-21E-11E+0010203040this paperKrylov(unstable)KrylovPM1E-71E-51E-31E-1051015202530this paperKrylov(unstable)KrylovPM1E-81E-61E-41E-21E+0051015202530this paperKrylov(unstable)KrylovPM1E-71E-51E-31E-100.511.522.53this paperKrylov(unstable)KrylovPM1E-71E-51E-31E-10102030405060this paperKrylov(unstable)KrylovPM1E-31E-21E-11E+00102030405060this paperKrylov(unstable)KrylovPMReferences
[1] Zeyuan Allen-Zhu and Yuanzhi Li. Doubly Accelerated Methods for Faster CCA and General-

ized Eigendecomposition. ArXiv e-prints  abs/1607.06017  July 2016.

[2] Zeyuan Allen-Zhu and Yuanzhi Li. Faster Principal Component Regression via Optimal

Polynomial Approximation to sgn(x). ArXiv e-prints  abs/1608.04773  August 2016.

[3] Zeyuan Allen-Zhu and Yuanzhi Li. First Efﬁcient Convergence for Streaming k-PCA: a Global 

Gap-Free  and Near-Optimal Rate. ArXiv e-prints  abs/1607.07837  July 2016.

[4] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the Compressed Leader: Faster Algorithm for Matrix

Multiplicative Weight Updates. ArXiv e-prints  abs/1701.01722  January 2017.

[5] Zeyuan Allen-Zhu and Yang Yuan. Improved SVRG for Non-Strongly-Convex or Sum-of-Non-

Convex Objectives. In ICML  2016.

[6] Sanjeev Arora  Satish Rao  and Umesh V. Vazirani. Expander ﬂows  geometric embeddings and

graph partitioning. Journal of the ACM  56(2)  2009.

[7] Srinadh Bhojanapalli  Prateek Jain  and Sujay Sanghavi. Tighter Low-rank Approximation via

Sampling the Leveraged Element. In SODA  pages 902–920  2015.

[8] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input

sparsity time. In STOC  pages 81–90  2013.

[9] Michael B. Cohen  Sam Elder  Cameron Musco  Christopher Musco  and Madalina Persu.
Dimensionality reduction for k-means clustering and low rank approximation. In STOC  pages
163–172. ACM  2015.

[10] Petros Drineas and Anastasios Zouzias. A Note on Element-wise Matrix Sparsiﬁcation via a

Matrix-valued Bernstein Inequality. ArXiv e-prints  abs/1006.0407  January 2011.

[11] Rong-En Fan and Chih-Jen Lin. LIBSVM Data: Classiﬁcation  Regression and Multi-label.

Accessed: 2015-06.

[12] Dan Garber and Elad Hazan. Fast and simple PCA via convex optimization. ArXiv e-prints 

September 2015.

[13] Dan Garber  Elad Hazan  Chi Jin  Sham M. Kakade  Cameron Musco  Praneeth Netrapalli 
and Aaron Sidford. Robust shift-and-invert preconditioning: Faster and more sample efﬁcient
algorithms for eigenvector computation. In ICML  2016.

[14] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The JHU Press  4th edition 

2012.

[15] Prateek Jain  Chi Jin  Sham M. Kakade  Praneeth Netrapalli  and Aaron Sidford. Streaming PCA:
Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja’s Algorithm.
In COLT  2016.

[16] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection.

http://snap.stanford.edu/data  June 2014.

[17] Chris J. Li  Mengdi Wang  Han Liu  and Tong Zhang. Near-Optimal Stochastic Approximation

for Online Principal Component Estimation. ArXiv e-prints  abs/1603.05305  March 2016.

[18] Ren-Cang Li and Lei-Hong Zhang. Convergence of the block lanczos method for eigenvalue

clusters. Numerische Mathematik  131(1):83–113  2015.

[19] Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and

faster approximate singular value decomposition. In NIPS  pages 1396–1404  2015.

[20] Ohad Shamir. A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate.

In ICML  pages 144—-153  2015.

[21] Ohad Shamir. Fast stochastic algorithms for svd and pca: Convergence properties and convexity.

In ICML  2016.

[22] Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. ArXiv e-prints 

abs/1501.01571  January 2015.

9

,Zeyuan Allen-Zhu
Yuanzhi Li