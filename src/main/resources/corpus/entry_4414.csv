2019,Private Learning Implies Online Learning: An Efficient Reduction,We study the relationship between the notions of differentially private learning and online learning. Several recent works have shown that differentially private learning implies online learning  but an open problem of Neel  Roth  and Wu \cite{NeelAaronRoth2018} asks whether this implication is {\it efficient}. 
Specifically  does an efficient differentially private learner imply an efficient online learner? 

In this paper we resolve this open question in the context of pure differential privacy.
We derive an efficient black-box reduction from differentially private learning to online learning from expert advice.,Private Learning Implies Online Learning:

An Efﬁcient Reduction

Alon Gonen

Elad Hazan

University of California San Diego

Princeton University and Google AI Princeton

algonen@cs.ucsd.edu

ehazan@princeton.edu

Shay Moran

Google AI Princeton

shaymoran1@gmail.com

Abstract

We study the relationship between the notions of differentially private learning
and online learning in games. Several recent works have shown that differentially
private learning implies online learning  but an open problem of Neel  Roth  and
Wu [27] asks whether this implication is efﬁcient. Speciﬁcally  does an efﬁcient
differentially private learner imply an efﬁcient online learner?
In this paper we resolve this open question in the context of pure differential pri-
vacy. We derive an efﬁcient black-box reduction from differentially private learn-
ing to online learning from expert advice.

1

Introduction

Differential Private Learning and Online Learning are two well-studied areas in machine learning.
While at a ﬁrst glance these two subjects may seem disparate  recent works gathered a growing
amount of evidence which suggests otherwise. For example  Adaptive Data Analysis [15  14  24 
19  3] shares strong similarities with adversarial frameworks studied in online learning  and on the
other hand exploits ideas and tools from differential privacy. A more formal relation between private
and online learning is manifested by the following fact:

Every privately learnable class is online learnable.

This implication and variants of it were derived by several recent works [20  9  1] (see the related
work section for more details). One caveat of the latter results is that they are non-constructive:
they show that every privately learnable class has a ﬁnite Littlestone dimension. Then  since the
Littlestone dimension is known to capture online learnability [26  5]  it follows that privately learn-
able classes are indeed online learnable. Consequently  the implied online learner is not necessarily
efﬁcient  even if the assumed private learner is. Thus  the following question emerges:

Does efﬁcient differentially private learning imply efﬁcient online learning?

This question was explicitly raised by Neel  Roth and Wu [27]. In this work we resolve this question
afﬁrmatively under the assumption that the given private learner satisﬁes Pure Differential Privacy
(the case of Approximate Differential Privacy remains open: see Section 4 for a short discussion).
We give an efﬁcient black-box reduction which transforms an efﬁcient pure private learner to an
efﬁcient online learner. Our reduction exploits a characterization of private learning due to [4] 
together with tools from online boosting [6]  and a lemma which converts oblivious online learning
to adaptive online learning. The latter lemma is novel and may be of independent interest.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Main result
Theorem 1. Let A be a differentially private learning algorithm for an hypothesis class H in the
realizable setting. Denote its sample complexity by m(· ·) and denote by m0 := m(1/4  1/2). Then 
Algorithm 3 is an efﬁcient online learner for H in the realizable setting which attains an expected

regret of at most O((cid:112)ln(T )).

The (standard) notation used in the theorem statement is detailed in Section 2.

Agnostic versus Realizable.
It is natural to ask whether Theorem 1 can be generalized to the ag-
nostic setting  namely  whether Algorithm 3 can be extended to an (efﬁcient) online learner which
achieves a sublinear regret against arbitrary adversaries. It turns out  that the answer is no  at least
if one is willing to assume certain customary complexity theoretical assumptions and consider a
non-uniform1 model of computation. Speciﬁcally  consider the class of all halfspaces over the do-
main {0  1}n ⊆ Rn whose margin is at least poly(n). This class satisﬁes: (i) it is efﬁciently learnable
by a pure differentially private algorithm [7  18  28]. (ii) Conditioned on certain average case hard-
ness assumptions  there is no efﬁcient online learner2 for this class which achieves sublinear regret
against arbitrary adversaries [11]. We note that this argument only invalidates the possibility of re-
ducing agnostic online learning to realizable private learning. The question of whether there exists
an efﬁcient reduction from agnostic online learning to agnostic private learning remains open.

Proof overview. Here is a short outline of the proof. A characterization of differentially private
learning due to [4] implies that if H is privately learnable in the pure setting  then the representation
dimension of H is ﬁnite. Roughly  this means that for any ﬁxed distribution D over labeled examples 
by repeatedly sampling the (random) outputs of the algorithm A on a “dummy" input sample  we
eventually get an hypothesis that performs well with respect to D. In more detail  if one samples
(roughly) exp(1/α) random hypotheses  then with high probability one of them will have excess
population loss ≤ α with respect to D. This suggests the following approach: sample exp(1/α)
random hypotheses (α will be speciﬁed later) and treat them an a class of experts  denoted by Hα;

then  use Multiplicative Weights to online learn Hα with regret (roughly)(cid:112)T log|Hα| ≈ (cid:112)T /α 

and thus the total regret will be

α · T +(cid:112)T /α 

which is at most T 2/3 if we set α = T −1/3.
There are two caveats with this approach: i) the number of experts in Hα is exp(T 1/3)  which is
too large for applying Multiplicative Weights efﬁciently. ii) A more subtle issue is that the above
regret analysis only applies in the oblivious setting: an adaptive adversary may “learn” the random
class Hα from the responses of our online learner  and eventually produce a (non-typical) sequence
of examples for which it is no longer the case that the best expert in Hα has loss ≤ α. To handle
the ﬁrst obstacle we only require a constant accuracy of α = 1/4  which we later reduce using
online boosting from [6]. As for the second obstacle  to cope with adaptive adversaries we propose
a general reduction from the adaptive setting which might be of independent interest.

1.2 Related work

Online and private learning Feldman and Xiao [20] exploited techniques from communication
complexity to show that every pure differentially private (DP) learnable class has a ﬁnite Littlestone
dimension (and hence is online learnable). Their work actually proved that pure private learning
is strictly more difﬁcult than online learning. That is  there exist classes with a ﬁnite Littlestone
dimension which are not pure-DP learnable. More recently  Alon et al. [9  1] extended the former
result to approximate differential privacy  showing that every approximate-DP learnable class has a
ﬁnite Littlestone dimension. It remains open whether the converse holds.

1Complexity theory distinguishes between uniform and non-uniform models  such as Turing machines vs.
arithmetic circuits. In this paper we consider the uniform model. However  the lower bound we sketch applies
to non-uniform computation.

2The result in [11] is in fact stronger: it shows that there exists no efﬁcient agnostic PAC learner for this

class (see Theorem 1.4 in it).

2

Another line of work by [27  8] exploit online learning techniques to derive results in differential
privacy related to sanitization and uniform convergence.

Adaptive data analysis. A growing area which intersects both ﬁelds of online learning and pri-
vate learning is adaptive data analysis ([15]  [14] [24] [19] [3]). This framework studies scenarios
in which a data analyst wishes to test multiple hypotheses on a ﬁnite sample in an adaptive manner.
The adaptive nature of this setting resembles scenarios that are traditionally studied in online learn-
ing  and the connection with differential privacy is manifested in the technical tools used to study
adaptive data analysis  many of which were developed in differential privacy (e.g. composition the-
orems).

Oracle complexity of online learning. One feature of our algorithm is that it uses an oracle access
to a private learner. Several works studied online learning in oracle model ([23  25  13]). This
framework is natural in scenarios in which it is computationally hard to achieve sublinear regret in
the worst case  but the online learner has access to an ofﬂine optimization and/or learning oracle.
Our results fall into the same paradigm  where the oracle is a differentially private learner.

2 Deﬁnitions and Preliminaries

2.1 PAC learning
Let X be an instance space  Y = {−1  1} be a label set  and let D be an (unknown) distribution
over X × Y. An “X → Y” function is called a concept/hypothesis. The goal here is to design a
learning algorithm  which given a large enough input sample S = ((x1  y1))  . . .   (xm  ym)) drawn
i.i.d. from D  outputs an hypothesis h : X → Y whose expected risk

LD(h) := E(x y)∼D[(cid:96)(h(x)  y)]

(cid:96)(a  b) = 1a(cid:54)=b

(cid:80)m

is small compared to the best hypothesis in a hypothesis class H  which is a ﬁxed and known to the
algorithm.
The distribution D is said to be realizable with respect to H if there exists h(cid:63) ∈ H such that
LD(h) = 0. We also deﬁne the empirical risk of an hypothesis h with respect to a sample S =
((x1  y1)  . . .   (xm  ym)) as LS(h) = 1
m
Deﬁnition 2. (PAC learning) An hypothesis class H is PAC learnable with sample complexity
m(α  β) if there exists an algorithm A such that for any distribution D over X   an accuracy and
conﬁdence parameters α  β ∈ (0  1)  if A is given an input sample S = ((x1  ym)  . . .   (xm  ym)) ∼
Dm such that m ≥ m(α  β)  then it outputs an hypothesis h : X → Y satisfying LD(h) ≤ α
with probability at least 1 − β. The class H is efﬁciently PAC learnable if the runtime of A (and
thus its sample complexity) are polynomial in 1/α and 1/β. If the above holds only for realizable
distributions then we say that H is PAC learnable in the realizable setting.

i=1 (cid:96)(h(xi)  yi).

2.2 Differentially private PAC learning

In some important learning tasks (e.g. medical analysis  social networks  ﬁnancial records  etc.) the
input sample consists of sensitive data that should be kept private. Differential privacy ([12  16]) is
a by-now standard formalism that captures such requirements.
The deﬁnition of differentially private algorithms is as follows. Two samples S(cid:48)  S(cid:48)(cid:48) ∈ (X × Y)m
are called neighbors if there exists at most one i ∈ [m] such that the i’th example in S(cid:48) differs from
the i’th example in S(cid:48)(cid:48).
Deﬁnition 3. (Differentially private learning) A learning algorithm A is said to be -differentially
private3 (DP) if for any two neighboring samples and for any measurable subset F ∈ YX  

P r[A(S) ∈ F] ≤ exp()P r[A(S(cid:48)) ∈ F] and
P r[A(S(cid:48)) ∈ F] ≤ exp()P r[A(S) ∈ F]

3The algorithm is said to be (  δ)-approximate differentially private if the above inequality holds up to an

additive factor δ. In this work we focus on the so-called pure case where δ = 0.

3

Group privacy is a simple extension of the above deﬁnition [17]: Two samples S  S(cid:48) are q-neighbors
if they differ in at most q of their pairs.
Lemma 4. Let A be a DP learner. Then for any q ∈ N and any two q-neighboring samples S  S(cid:48)
and any subset F ∈ YX ∩ range(A)  P r[A(S) ∈ F] ≤ exp(q)P r[A(S(cid:48)) ∈ F]
Combining the requirements of PAC and DP learnability yields the deﬁnition of private PAC (PPAC)
learner.
Deﬁnition 5. (PPAC Learning) A concept class H is differentially private PAC learnable with
sample complexity m(α  β) if it is PAC learnable with sample complexity m(α  β) by an algorithm
A which is an  = 0.1-differentially private.

Remark. Setting  = 0.1 is without loss of generality; the reason is that there are efﬁcient methods
to boost the value of  to arbitrarily small constants  see [30] and references within.

2.3 Online Learning
The online model can be seen as a repeated game between a learner A and an environment (a.k.a.
adversary) E. Let T be a (known4) horizon parameter. On each round t ∈ [T ] the adversary decides
on a pair (xt  yt) ∈ X × Y  and the learner decides on a prediction rule ht : X → {0  1}. Then 
the learner suffers the loss |yt − ˆyt|  where ˆyt = h(xt). Both players may base their decisions
on the entire history and may use randomness. Unlike in the statistical setting  the adversary E
can generate the examples in an adaptive manner. In this work we focus on the realizable setting
where it is assumed that the labels are realized by some target concept c ∈ H  i.e.  for all t ∈ [T ] 
yt = c(xt).5 The measure of success is the expected number of mistakes done by the learner:

E[MA] = E(cid:2) T(cid:88)

(cid:96)(ˆyt  yt)(cid:3) 

where the expectation is taken over the randomness of the learner and the adversary. An algo-
rithm A is a (strong) online learner if for any horizon parameter T and any realizable sequence
((x1  y1)  . . .   (xT   yT ))  the expected number of mistakes made by A is sublinear in T .

t=1

2.3.1 Weak Online Learning

We describe an extension due to [6] of the boosting framework ([29]) (from the statistical setting) to
the online.
Deﬁnition 6. (Weak online learning) An online learner A is called a weak online learner for a
class H with an edge parameter γ ∈ (0  1/2) and excess loss parameter T0 > 0 if for any horizon
parameter T and every sequence ((x1  y1)  . . .   (xT   yT )) realized by some target concept c ∈ H 
the expected number of mistakes done by A satisﬁes
− γ

(cid:18) 1

E[MA] ≤

(cid:19)

T + T0 .

2

2.3.2 Oblivious vs. Non-oblivious Adversaries

The general adversary considered in this paper is adaptive in the sense that it can choose the pair
(xt  yt) based on the actual predictions ˆy1  . . .   ˆyt−1 made by the learner on rounds 1  . . .   t− 1. An
adversary is called oblivious if it chooses the entire sequence ((x1  y1)  . . .   (xT   yT )) in advance.
We will ﬁrst develop an online weak online learner for the oblivious setting and then extend it to the
adaptive setting.

2.3.3 Regret bounds using Multiplicative Weights

Although we focus our attention on the realizable setting  our development also requires working
in the so-called agnostic setting  where the sequence ((x1  y1)  . . .   (xT   yT )) is not assumed to be

4Standard doubling techniques allow the learner to cope with scenarios where T is not known.
5However  the adversary does not need to decide on the identity of c in advance.

4

realized by some c ∈ H. The standard measure of success in this setting is the expected regret
deﬁned as

E[RegretT ] = E

(cid:96)(ˆyt  yt) − inf
h∈H

(cid:96)(h(xt)  yt).

T(cid:88)

t=1

T(cid:88)

t=1

Accordingly  an online learner in this context needs to achieve a sublinear regret in terms of the
horizon parameter T .
When the class H is ﬁnite  there is a well-known algorithm named Multiplicative Weights (MW)
which maintains a weight wt j for each hypothesis (a.k.a. expert in the online model) hj according
to

w1 j = 1   wt+1 j = wt j exp(−η(cid:96)(hj(xt)  yt)))

where η > 0 is a step-size parameter. At each time t  MW predicts with ˆyt = hj(xt) with probability
proportional to wt j. We refer to [2] for an extensive survey on Multiplicative Weights and its many
applications. The following theorem establishes an upper bound on the regret of MW.
Theorem 7. (Regret of MW) If the class H is ﬁnite then the expected regret of MW with step size

parameter η =(cid:112)log(|H|)/T is at most(cid:112)2T log |H|.

3 The Reduction and its Analysis

In this section we formally present our efﬁcient reduction from online learning to private PAC learn-
ing. Our reduction only requires a black-box oracle access to the the private learner. The reduction
can be roughly partitioned into 3 parts: (i) We ﬁrst use this oracle to construct an efﬁcient weak
online learner against oblivious adversaries. (ii) Then  we transform this learner so it also handles
adaptive adversaries. This step is based on a general reduction which may be of independent interest.
(iii) Finally  we boost the weak online learner to a strong one using online boosting.

3.1 A Weak Online Learner in the Oblivious Setting
Let Ap be a PPAC algorithm with sample complexity m(α  β) for H and denote by m0 :=
m(1/4  1/2) = Θ(1). We only assume an oracle access to Ap  and in the ﬁrst part we use it to
construct a distribution over hypotheses/experts. Speciﬁcally  let S0 be a dummy sample consist-
ing of m occurrences of the pair (¯x  0) where ¯x is an arbitrary instance from X . Note that the
hypothesis/expert Ap(S0) is random.6
Deﬁnition 8. Let P0 be the distribution over hypotheses/experts induced by applying Ap on the
input sample S0.
Lemma 9. For any realizable distribution D over X × Y  with probability at least 15/16 over the
draw of N = Θ(exp(m0)) = Θ(1) i.i.d. hypothesis h1  . . .   hN ∼ P0   there exists i ∈ [N ] such
that LD(hi) ≤ 1/4.
Proof. Let c ∈ H be such that LD(c) = 0  and denote by

H(D) = {h ∈ range(A) : LD(h) ≤ 1/4} .

By assumption  if we feed the PPAC algorithm A with a sample S drawn according to Dm and
labeled by c  then with probability at least 1/4 over both the internal randomness of A and the draw
of S  the output of A belongs to H(D). It follows that there exists at least one sample  which we
denote by ¯S  such that with probability at least 1/2 over the randomness of A  the output h = A( ¯S)
belongs to H(D). Since A is differentially private and ( ¯S  S0) are m-neighbors  we obtain that

P r[A(S0) ∈ H(D)] ≥ 1
2

exp(−0.1m0) .

Consequently if we draw N = Θ(exp(m0)) hypotheses hj ∼ P0 then with probability at least
15/16  at least one of the hj’s belongs to H(D). This completes the proof.

Armed with this lemma  we proceed by applying the Multiplicative Weights method to the random
class H produced by the PPAC learner Ap. The algorithm is detailed as Algorithm 1. The next
lemma establishes its weak learnability in the oblivious setting.

6The deﬁnition of differential privacy implies that every private algorithm is randomized (ignoring triviali-

ties).

5

Algorithm 1 Weak online learner for oblivious adversaries

Oracle access: Let P0 denote the distribution from Deﬁnition 8  and let m0 = m(1/4  1/2) 
where m(α  β) is the sample complexity of the private learner Ap.
Set: N = Θ(exp(m0))  η =
for j = 1 to N do

T .

(cid:113) log N

∀j ∈ [N ]

(cid:46) Initializing MW w.r.t. h1  . . .   hN

hj ∼ P0
w1 j = 1

end for
for t = 1 to T do

Predict ˆyt = hj(xt) with probability wt j/(cid:80)N

Receive an instance xt

Receive the true label yt
wt+1 j = wt j exp(−η|yt − hj(xt)|)

k=1 wt k

end for

made by Algorithm 1 is at most O(cid:0)√

T m0 + T
4

(cid:1). In particular  the algorithm is a weak online

Lemma 10. For any oblivious adversary and horizon parameter T   the expected number of mistakes

learner with an edge parameter 1/8 and excess loss T0 = O(1).

Proof. Since the adversary is oblivious  it chooses the (realizable) sequence (x1  y1) . . .   (xT   yT )
in advance. In particular  these choices do not depend on the hypotheses h1  . . .   hN drawn from
P0. Deﬁne a distribution D over X × Y by

D[{(x  y)}] =

|{t ∈ [T ] : (xt  yt) = (x  y)}|

.

T

By the previous lemma we have that with probability at least 15/16  there exists j ∈ [N ] such that

T(cid:88)

t=1

1
T

(cid:96)(hj(xt)  yt) = LD(hi) ≤ 1/4.

2(cid:112)T log N +

2(cid:112)T log N +

T
4

+

T
16

T
4

+ T /16.

≤(cid:16) 1

2

(cid:17)

− 1
8

T + T0.

Using the standard regret bound of Multiplicative Weights (Lemma 7)  we obtain that the expected
number of mistakes done by our algorithm is at most

(The T /16 factor is because the success probability of Ap is 15/16  see Lemma 9). In particular  set
T0 = C · log N = O(m0) for a sufﬁciently large constant C such that 

This concludes the proof.

3.2 General reduction from adaptive to oblivious environments

In this part we describe a simple general-purpose extension from the oblivious setting to the adaptive
setting. Let Ao be an online learner for H that handles oblivious adversaries. We may assume
that Ao is random since otherwise any guarantee with respect to oblivious adversary holds also
with respect to adaptive adversary. Given an horizon parameter T   we initialize T instances of this
algorithm (each of with an independent random seed of its own). Finally  on round t we follow the
prediction of the t-th instance  A(t)
o .
Lemma 11. Suppose that Ao is an online learner for a class H in the oblivious setting whose
expected regret is upper bounded by R(T ). Then  the expected regret of Algorithm 2 is also upper
bounded by R(T ).

Proof. The proof relies on a lemma by [10] which provides a reduction from the adaptive to the
oblivious setting given a certain condition on the responses of the online learner. Since this lemma

6

Algorithm 2 Reduction from Oblivious to Adaptive Setting

Oracle access: Online algorithm Ao for the oblivious setting.
Initialize T independent instances of Ao  denoted A(1)
for t = 1 to T do

o   . . .   A(T )

o

.

:= prediction of A(j)

ˆy(j)
t
Predict ˆyt = ˆy(t)
t

o   j = 1  . . .   T .

end for

is somewhat technical  we defer the proof of the stated bound to the appendix (Section A)  and prove
here a slightly weaker bound  which is off by a factor of log T . This weaker bound however follows
from elementary arguments in a self contained manner.
Note that the algorithms A(j)’s for j = 1 . . . T are i.i.d. (i.e. have independent internal randomness).
Therefore  the sequence of examples chosen by the adversary up to time t is independent of the
predictions of A(j)
in the
oblivious setting:

o whenever j ≥ t  and thus we can use the assumed guarantee for A(j)

o

where ˆ(cid:96)(j)

i = (cid:96)(yi  ˆy(j)

i

i=1
). Similarly  it follows that

E[ˆ(cid:96)t] = E[ˆ(cid:96)(t)

t

] = E[ˆ(cid:96)(t+1)

t

] = . . . = E[ˆ(cid:96)(T )

t

1

T − t + 1

Therefore 

(cid:105)

ˆ(cid:96)t

E(cid:104) T(cid:88)

t=1

(1)

(2)

 .

ˆ(cid:96)(j)
t

T(cid:88)

j=t

(by Equation 2)

(∀j ≥ t) : E(cid:104) t(cid:88)

ˆ(cid:96)(j)
i

(cid:105) ≤ R(T ) 


] = E

1

T − t + 1

T(cid:88)

j=t

(cid:105)

ˆ(cid:96)(j)
t

(cid:105)
(cid:105)

ˆ(cid:96)(j)
t

T − t + 1

ˆ(cid:96)(j)
t

T − j + 1

t=1

t=1

j=1

= E(cid:104) T(cid:88)
= E(cid:104) T(cid:88)
j(cid:88)
≤ E(cid:104) T(cid:88)
j(cid:88)
E[(cid:80)j
T(cid:88)
≤ T(cid:88)

j=1

j=1

t=1

=

R(T )

T − j + 1

j=1

≤ R(T ) log T.

ˆ(cid:96)(j)
t
T − j + 1

t=1

]

(by Equation 1)

3.3 Applying Online Boosting

In this part we apply an online boosting algorithm due to [6] to improve the accuracy of our weak
learner. The algorithm is named Online Boosting-by-Majority (online BBM). We start by brieﬂy
describing online BBM and stating an upper bound on its expected regret.
The Online BBM can be seen as an extension of Boosting-by-Majority algorithm due to [21]. Let
WL be a weak learner with an edge parameter γ ∈ (0  1/2) and excessive loss T0. The online BBM
algorithm maintains N copies WL  denoted by WL(1)  . . .   WL(N ). On each round t it uses a simple

7

(unweighted) majority vote over WL(1)  . . .   WL(N ) to perform a prediction ˆyt. The pair (xt  yt) is
passed to the weak learner WL(j) with probability that depends on the accuracy of the majority vote
based on the weak learners WL(1)  . . .   WL(j−1) with respect to (xt  yt). Similarly to the well-known
AdaBoost algorithm by [22]  the worse is the accuracy of the previous weak learners  the larger is
the probability that (xt  yt) is passed to WL(j) (see Algorithm 1 in [6]).
Theorem 12. ([6]) For any T and any N  the expected number of mistakes made by the Online
Boosting-by-Majority Algorithm is bounded by7

(cid:18)

exp

− 1
2

N γ2

(cid:19)

1
γ

)

N (T0 +

(cid:19)

T + ˜O

(cid:18)√
O(T  +(cid:112)ln(1/))

In particular  if γ and T0 are constants then for any  > 0  it sufﬁces to pick N = Θ(ln(1/)) weak
learners to obtain an upper bound of

(3)

on the expected number of mistakes.

We have collected all the pieces of our algorithm.

Algorithm 3 Online Learning using a Private Oracle

Horizon parameter: T
 := 1/T
Weak learner WL: Algorithm 2 applied to Algorithm 1
Apply online BBM using N = Θ(ln(1/)) = Θ(ln T ) instances of WL

Proof. (of Theorem 1) Combining Lemma 10 and Lemma 11  we obtain that WL is a weak online
learner with an edge parameter γ = 1/8 and constant excessive loss. Plugging  = 1/T in the
accuracy parameter in Theorem 12 (Equation 3) yields the stated bound.

4 Discussion

We have considered online learning in the presence of a private learning oracle  and gave an efﬁcient
reduction from online learning to private learning.
We conclude with two questions for future research.

• Can our result can be extended to the approximate case? That is  does an efﬁcient ap-
proximately differentially private learner for a class H imply an efﬁcient online algorithm
with sublinear regret? Can the online learner be derived using only an oracle access to the
private learner?
• Can our result be extended to the agnostic setting? That is  does an efﬁcient agnostic private
learner for a class H implies an efﬁcient agnostic online learner for it?

As for the ﬁrst question  one difference with pure privacy is that Lemma 9 ceases to hold. Recall that
this lemma guarantees that applying a pure private learner on a dummy sample yields an output hy-
pothesis  which is correlated with any realizable target concept  with a non-negligible chance. This
lemma manifests a certain obliviousness of pure private learners which is crucial in our transfor-
mation from the statistical i.i.d setting to the adversarial setting. Approximately private learners do
not share similar obliviousness: in particular  to obtain an output hypothesis which is non-trivially
correlated with the realizable target concept  one must apply it on an i.i.d sample consistent with the
concept (rather than an arbitrary dummy sample).
As for the second question  it is natural to try and implement the approach used in this paper  but
there are several major missing components. Most notably  we would need a boosting algorithm
for agnostic online learning  which is not known to exist. We consider this as a direction for future
research  which is interesting in its own right.

7The bound in [6] is an high probability bound. It is easy to translate it to a bound in expectation.

8

References
[1] Noga Alon  Roi Livni  Maryanthe Malliaris  and Shay Moran. Private PAC learning implies

ﬁnite Littlestone dimension. arXiv preprint arXiv:1806.00949  2018.

[2] Sanjeev Arora  Elad Hazan  and Satyen Kale. The Multiplicative Weights Update Method: A

Meta-Algorithm and Applications. Theory of Computing  8(1):121–164  2012.

[3] Raef Bassily  Kobbi Nissim  Adam Smith  Thomas Steinke  Uri Stemmer  and Jonathan Ull-
man. Algorithmic Stability for Adaptive Data Analysis. In Proceedings of the forty-eighth
annual ACM symposium on Theory of Computing  pages 1046–1059  11 2015.

[4] Amos Beimel  Kobbi Nissim  and Uri Stemmer. Characterizing the Sample Complexity of
Private Learners. In roceedings of the 4th conference on Innovations in Theoretical Computer
Science  pages 97–110  2013.

[5] Shai Ben-David  Dávid Pál  and Shai Shalev-Shwartz. Agnostic online learning. In Conference

on Learning Theory  2009.

[6] Alina Beygelzimer  Satyen Kale  and Haipeng Luo. Optimal and Adaptive Algorithms for
Online Boosting. In International Conference on Machine Learning  pages 2323–2331  2015.

[7] Avrim Blum  Cynthia Dwork  Frank McSherry  and Kobbi Nissim. Practical privacy: the sulq
framework. In Proceedings of the Twenty-fourth ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems  June 13-15  2005  Baltimore  Maryland  USA  pages 128–
138  2005.

[8] Olivier Bousquet  Roi Livni  and Shay Moran. Passing tests without memorizing: Two models

for fooling discriminators. CoRR  abs/1902.03468  2019.

[9] Mark Bun  Kobbi Nissim  Uri Stemmer  and Salil Vadhan. Differentially private release and
In 2015 IEEE 56th Annual Symposium on Foundations of

learning of threshold functions.
Computer Science  2015.

[10] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge uni-

versity press  2006.

[11] Amit Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings of the
48th Annual ACM SIGACT Symposium on Theory of Computing  STOC 2016  Cambridge  MA 
USA  June 18-21  2016  pages 105–117  2016.

[12] Irit Dinur and Kobbi Nissim. Revealing Information while Preserving Privacy. In Proceedings
of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database
systems  pages 202–210  2003.

[13] Miroslav Dudik  Nika Haghtalab  Haipeng Luo  Robert E. Schapire  Vasilis Syrgkanis  and
Jennifer Wortman Vaughan. Oracle-efﬁcient online learning and auction design. In Annual
Symposium on Foundations of Computer Science - Proceedings  2017.

[14] Cynthia Dwork  Vitaly Feldman  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Aaron
Roth. Preserving Statistical Validity in Adaptive Data Analysis. In Proceedings of the forty-
seventh annual ACM symposium on Theory of computing  pages 117–126  2014.

[15] Cynthia Dwork  Vitaly Feldman  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Aaron
Roth. Generalization in Adaptive Data Analysis and Holdout Reuse. In Advances in Neural
Information Processing Systems  pages 2350–2358  2015.

[16] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating Noise to Sen-
In Theory of Cryptography Conference  pages 265–284.

sitivity in Private Data Analysis.
Springer  2006.

[17] Cynthia Dwork  Aaron Roth  and others. The algorithmic foundations of differential privacy.

Foundations and Trends R(cid:13)in Theoretical Computer Science  9(3–4):211–407  2014.

9

[18] Vitaly Feldman  Cristobal Guzman  and Santosh Vempala. Statistical query algorithms for
In Proceedings of the Twenty-
mean vector estimation and stochastic convex optimization.
Eighth Annual ACM-SIAM Symposium on Discrete Algorithms  SODA 2017  Barcelona  Spain 
Hotel Porta Fira  January 16-19  pages 1265–1277  2017.

[19] Vitaly Feldman and Thomas Steinke. Calibrating Noise to Variance in Adaptive Data Analysis.

In Conference On Learning Theory  pages 535–544  2018.

[20] Vitaly Feldman and David Xiao. Sample Complexity Bounds on Differentially Private Learn-
ing via Communication Complexity. In Conference on Learning Theory  pages 1000–1019 
2014.

[21] Yoav Freund. Boosting a Weak Learning Algorithm by Majority.

putaiton  121:256–28  1995.

Information and Com-

[22] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning

and an application to boosting. J. Comput. Syst. Sci.  55(1):119–139  1997.

[23] Alon Gonen and Elad Hazan. Learning in non-convex games with an optimization oracle.

arXiv preprint arXiv:1810.07362  2018.

[24] Moritz Hardt and Jonathan Ullman. Preventing false discovery in interactive data analysis is
hard. In Proceedings - Annual IEEE Symposium on Foundations of Computer Science  FOCS 
pages 454–463  2014.

[25] Elad Hazan and Tomer Koren. A linear-time algorithm for trust region problems. Mathematical

Programming  158(1-2):363–381  2016.

[26] Nick Littlestone and Manfred K Warmuth. Relating Data Compression and Learnability. Tech-

nical report  Technical report  University of California  Santa Cruz  1986.

[27] Seth Neel Aaron Roth and Zhiwei Steven Wu. How to Use Heuristics for Differential Privacy.

Technical report  Technical report  University of California  Santa Cruz  2018.

[28] Huy L. Nguyen  Jonathan Ullman  and Lydia Zakynthinou. Efﬁcient private algorithms for

learning halfspaces. CoRR  abs/1902.09009  2019.

[29] Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. Cambridge

university press  2012.

[30] Salil Vadhan. The Complexity of Differential Privacy.

Cryptography  pages 347–450. Springer  2017.

In Tutorials on the Foundations of

A Proof of Lemma 11
The proof exploits Lemma 4.1 from [10] which we explain next. Let A be a (possibly randomized)
online learner  and let ut denote the response of A in time t ≤ T . Then  since A may be random-
ized  ut is drawn from a random variable Ut that may depend on the entire history: namely  on both
the responses of A as well as of the adversary up to time t. So

Ut = Ut(u1 . . . ut−1  v1 . . . vt−1) 

where ui ∼ Ui denotes the response of A and vi ∼ Vi denotes the response of the (possibly
randomized) adversary on round i < t (in the classiﬁcations setting  vi is the labelled example
(xi  yi)  and ui is the prediction rule hi : X → {0  1} used by A). Lemma 4.1 in [10] asserts that if
Ut is only a function of the vi’s  namely

Ut = Ut(v1 . . . vt−1) 

(4)

then the expected regret of A in the adaptive setting is the same like in the oblivious setting.
The proof now follows by noticing that Algorithm 2 satisﬁes Equation (4). To see this  note that at
each round t  Algorithm 2 uses the response of algorithm A(t)
o which only depends on the responses

10

of the adversary and A(t)
o up to time t. In particular  it does not additionally depend the responses of
Algorithm 2 at times up to t. Putting it differently  given the responses of the adversary z1 . . . zt−1 
one can produce the response of Algorithm 2 at time t by simulating A(t)
Thus  we may assume that the adversary is oblivious  and therefore that the sequence of exam-
ples (x1  y1) . . . (xt  yt) is ﬁxed in advance and independent from the algorithms Aj
o’s. Now 
since A(1)
are i.i.d. (i.e. have independent internal randomness)  the expected loss of
Algorithm 2 at time t satisﬁes

o on this sequence.

o   . . .   A(T )

o

E[ˆ(cid:96)t] = E[ˆ(cid:96)(t)

t

] = E[ˆ(cid:96)(1)

t

] = . . . = E[ˆ(cid:96)(T )

t

] = E

where ˆ(cid:96)(j)

i = (cid:96)(yi  ˆy(j)

i

). Thus  its expected number of mistakes is at most

(cid:34) T(cid:88)

E

(cid:35)

ˆ(cid:96)t

= E

 T(cid:88)

 =

1
T

ˆ(cid:96)(j)
t

T(cid:88)

E

j=1

t=1

t=1

t=1

Therefore  the expected regret satisﬁes

E[RegretT ] =

E[Regret(j)

T ] ≤ R(T ) .

T

 1
T(cid:88)
(cid:34) T(cid:88)

j=1

  

ˆ(cid:96)(j)
t

(cid:35)

.

ˆ(cid:96)(j)
t

T(cid:88)

j=1

T(cid:88)

j=1

1
T

1
T

11

,Alon Gonen
Elad Hazan
Shay Moran