2016,Robustness of classifiers: from adversarial to random noise,Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e.  adversarial) perturbations of the datapoints. On the other hand  it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper  we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime  which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover  we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime  and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally  and more generally offers important insights onto the geometry of high dimensional classification problems.,Robustness of classiﬁers:

from adversarial to random noise

Alhussein Fawzi∗  Seyed-Mohsen Moosavi-Dezfooli∗  Pascal Frossard

École Polytechnique Fédérale de Lausanne

Lausanne  Switzerland

{alhussein.fawzi  seyed.moosavi  pascal.frossard} at epfl.ch

Abstract

Several recent works have shown that state-of-the-art classiﬁers are vulnerable to
worst-case (i.e.  adversarial) perturbations of the datapoints. On the other hand 
it has been empirically observed that these same classiﬁers are relatively robust
to random noise. In this paper  we propose to study a semi-random noise regime
that generalizes both the random and worst-case noise regimes. We propose
the ﬁrst quantitative analysis of the robustness of nonlinear classiﬁers in this
general noise regime. We establish precise theoretical bounds on the robustness of
classiﬁers in this general regime  which depend on the curvature of the classiﬁer’s
decision boundary. Our bounds conﬁrm and quantify the empirical observations that
classiﬁers satisfying curvature constraints are robust to random noise. Moreover 
we quantify the robustness of classiﬁers in terms of the subspace dimension in
the semi-random noise regime  and show that our bounds remarkably interpolate
between the worst-case and random noise regimes. We perform experiments and
show that the derived bounds provide very accurate estimates when applied to
various state-of-the-art deep neural networks and datasets. This result suggests
bounds on the curvature of the classiﬁers’ decision boundaries that we support
experimentally  and more generally offers important insights onto the geometry of
high dimensional classiﬁcation problems.

1

Introduction

State-of-the-art classiﬁers  especially deep networks  have shown impressive classiﬁcation perfor-
mance on many challenging benchmarks in visual tasks [9] and speech processing [7]. An equally
important property of a classiﬁer that is often overlooked is its robustness in noisy regimes  when
data samples are perturbed by noise. The robustness of a classiﬁer is especially fundamental when
it is deployed in real-world  uncontrolled  and possibly hostile environments. In these cases  it
is crucial that classiﬁers exhibit good robustness properties. In other words  a sufﬁciently small
perturbation of a datapoint should ideally not result in altering the estimated label of a classiﬁer.
State-of-the-art deep neural networks have recently been shown to be very unstable to worst-case
perturbations of the data (or equivalently  adversarial perturbations) [17]. In particular  despite
the excellent classiﬁcation performances of these classiﬁers  well-sought perturbations of the data
can easily cause misclassiﬁcation  since data points often lie very close to the decision boundary
of the classiﬁer. Despite the importance of this result  the worst-case noise regime that is studied
in [17] only represents a very speciﬁc type of noise. It furthermore requires the full knowledge of the
classiﬁcation model  which may be a hard assumption in practice.
In this paper  we precisely quantify the robustness of nonlinear classiﬁers in two practical noise
regimes  namely random and semi-random noise regimes. In the random noise regime  datapoints are

∗The ﬁrst two authors contributed equally to this work.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

perturbed by noise with random direction in the input space. The semi-random regime generalizes this
model to random subspaces of arbitrary dimension  where a worst-case perturbation is sought within
the subspace. In both cases  we derive bounds that precisely describe the robustness of classiﬁers in
function of the curvature of the decision boundary. We summarize our contributions as follows:

• In the random regime  we show that the robustness of classiﬁers behaves as √d times the

distance from the datapoint to the classiﬁcation boundary (where d denotes the dimension
of the data) provided the curvature of the decision boundary is sufﬁciently small. This
result highlights the blessing of dimensionality for classiﬁcation tasks  as it implies that
robustness to random noise in high dimensional classiﬁcation problems can be achieved 
even at datapoints that are very close to the decision boundary.

that the robustness precisely behaves as(cid:112)d/m times the distance to boundary  with m the

• This quantiﬁcation notably extends to the general semi-random regime  where we show
dimension of the subspace. This result shows in particular that  even when m is chosen as a
small fraction of the dimension d  it is still possible to ﬁnd small perturbations that cause
data misclassiﬁcation.

• We empirically show that our theoretical estimates are very accurately satisﬁed by state-
of-the-art deep neural networks on various sets of data. This in turn suggests quantitative
insights on the curvature of the decision boundary that we support experimentally through
the visualization and estimation on two-dimensional sections of the boundary.

The robustness of classiﬁers to noise has been the subject of intense research. The robustness proper-
ties of SVM classiﬁers have been studied in [19] for example  and robust optimization approaches for
constructing robust classiﬁers have been proposed to minimize the worst possible empirical error
under noise disturbance [1  10]. More recently  following the recent results on the instability of
deep neural networks to worst-case perturbations [17]  several works have provided explanations of
the phenomenon [3  5  14  18]  and designed more robust networks [6  8  20  13  15  12]. In [18] 
the authors provide an interesting empirical analysis of the adversarial instability  and show that
adversarial examples are not isolated points  but rather occupy dense regions of the pixel space. In
[4]  state-of-the-art classiﬁers are shown to be vulnerable to geometrically constrained adversarial
examples. Our work differs from these works  as we provide a theoretical study of the robustness of
classiﬁers to random and semi-random noise in terms of the robustness to adversarial noise. In [3]  a
formal relation between the robustness to random noise  and the worst-case robustness is established
in the case of linear classiﬁers. Our result therefore generalizes [3] in many aspects  as we study
general nonlinear classiﬁers  and robustness to semi-random noise. Finally  it should be noted that
the authors in [5] conjecture that the “high linearity” of classiﬁcation models explains their instability
to adversarial perturbations. The objective and approach we follow here is however different  as we
study theoretical relations between the robustness to random  semi-random and adversarial noise.

2 Deﬁnitions and notations
Let f : Rd → RL be an L-class classiﬁer. Given a datapoint x0 ∈ Rd  the estimated label is obtained
by ˆk(x0) = argmaxk fk(x0)  where fk(x) is the kth component of f (x) that corresponds to the kth
class. Let S be an arbitrary subspace of Rd of dimension m. Here  we are interested in quantifying the
robustness of f with respect to different noise regimes. To do so  we deﬁne r∗
S to be the perturbation
in S of minimal norm that is required to change the estimated label of f at x0.2

∗
S (x0) = argmin

r

r∈S (cid:107)r(cid:107)2 s.t. ˆk(x0 + r) (cid:54)= ˆk(x0).

(1)

Note that r∗

S (x0) can be equivalently written

r

∗
S (x0) = argmin
When S = Rd  r∗(x0) := r∗
Rd (x0) is the adversarial (or worst-case) perturbation deﬁned in [17] 
which corresponds to the (unconstrained) perturbation of minimal norm that changes the label of the

r∈S (cid:107)r(cid:107)2 s.t. ∃k (cid:54)= ˆk(x0) : fk(x0 + r) ≥ fˆk(x0)(x0 + r).

(2)

2Perturbation vectors sending a datapoint exactly to the boundary are assumed to change the estimated label

of the classiﬁer.

2

datapoint x0. In other words  (cid:107)r∗(x0)(cid:107)2 corresponds to the minimal distance from x0 to the classiﬁer
boundary. In the case where S ⊂ Rd  only perturbations along S are allowed. The robustness of f at
x0 along S is naturally measured by the norm (cid:107)r∗
S (x0)(cid:107)2. Different choices for S permit to study
the robustness of f in two different regimes:

• Random noise regime: This corresponds to the case where S is a one-dimensional subspace
(m = 1) with direction v  where v is a random vector sampled uniformly from the unit
sphere Sd−1. Writing it explicitly  we study in this regime the robustness quantity deﬁned
by mint |t| s.t. ∃k (cid:54)= ˆk(x0)  fk(x0 + tv) ≥ fˆk(x0)(x0 + tv)  where v is a vector sampled
uniformly at random from the unit sphere Sd−1.
• Semi-random noise regime: In this case  the subspace S is chosen randomly  but can be of
arbitrary dimension m.3 We use the semi-random terminology as the subspace is chosen
randomly  and the smallest vector that causes misclassiﬁcation is then sought in the subspace.
It should be noted that the random noise regime is a special case of the semi-random regime
with a subspace of dimension m = 1. We differentiate nevertheless between these two
regimes for clarity.

In the remainder of the paper  the goal is to establish relations between the robustness in the random
and semi-random regimes on the one hand  and the robustness to adversarial perturbations (cid:107)r∗(x0)(cid:107)2
on the other hand. We recall that the latter quantity captures the distance from x0 to the classiﬁer
boundary  and is therefore a key quantity in the analysis of robustness.
In the following analysis  we ﬁx x0 to be a datapoint classiﬁed as ˆk(x0). To simplify the notation 
we remove the explicit dependence on x0 in our notations (e.g.  we use r∗
S (x0) and ˆk
instead of ˆk(x0))  and it should be implicitly understood that all our quantities pertain to the ﬁxed
datapoint x0.

S instead of r∗

3 Robustness of afﬁne classiﬁers
We ﬁrst assume that f is an afﬁne classiﬁer  i.e.  f (x) = W(cid:62)x + b for a given W = [w1 . . . wL]
and b ∈ RL.
The following result shows a precise relation between the robustness to semi-random noise  (cid:107)r∗
S(cid:107)2
and the robustness to adversarial perturbations  (cid:107)r∗
Theorem 1. Let δ > 0  S be a random m-dimensional subspace of Rd  and f be a L-class afﬁne
classiﬁer. Let

(cid:107)2.

(cid:33)−1

 

2 ln(1/δ)

(cid:32)
(cid:18)

1 + 2

max

+

m

ln(1/δ)

(cid:114)
(cid:18)
(1/e)δ2/m  1 −
(cid:112)

ζ2(m  δ) =

ζ1(m  δ) =

(cid:19)(cid:19)−1
The following inequalities hold between the robustness to semi-random noise (cid:107)r∗
(cid:114)
ness to adversarial perturbations (cid:107)r∗
(cid:107)2:
∗
d
m(cid:107)r

2(1 − δ2/m)

∗
S(cid:107)2 ≤

(cid:107)2 ≤ (cid:107)r

d
m(cid:107)r

m

(cid:113)

∗

(cid:107)2 

(cid:114)

(cid:112)

ζ2(m  δ)

ζ1(m  δ)

.

S(cid:107)2  and the robust-

(3)

(4)

(5)

with probability exceeding 1 − 2(L + 1)δ.
The proof can be found in the appendix. Our upper and lower bounds depend on the functions
ζ1(m  δ) and ζ2(m  δ) that control the inequality constants (for m  δ ﬁxed). It should be noted that
ζ1(m  δ) and ζ2(m  δ) are independent of the data dimension d. Fig. 1 shows the plots of ζ1(m  δ)
and ζ2(m  δ) as functions of m  for a ﬁxed δ. It should be noted that for sufﬁciently large m  ζ1(m  δ)
and ζ2(m  δ) are very close to 1 (e.g.  ζ1(m  δ) and ζ2(m  δ) belong to the interval [0.8  1.3] for
m ≥ 250 in the settings of Fig. 1). The interval [ζ1(m  δ)  ζ2(m  δ)] is however (unavoidably) larger
when m = 1.
3A random subspace is deﬁned as the span of m independent vectors drawn uniformly at random from Sd−1.

3

(cid:107)2 by a factor of(cid:112)d/m. Specif-

The result in Theorem 1 shows that in the random and
semi-random noise regimes  the robustness to noise is
precisely related to (cid:107)r∗
ically  in the random noise regime (m = 1)  the mag-
nitude of the noise required to misclassify the datapoint
behaves as Θ(√d(cid:107)r∗
(cid:107)2) with high probability  with con-
stants in the interval [ζ1(1  δ)  ζ2(1  δ)]. Our results there-
fore show that  in high dimensional classiﬁcation set-
tings  afﬁne classiﬁers can be robust to random noise 
even if the datapoint lies very closely to the decision
boundary (i.e.  (cid:107)r∗
(cid:107)2 is small). In the semi-random noise
regime with m sufﬁciently large (e.g.  m ≥ 250)  we have
(cid:107)r∗
(cid:107)2 with high probability  as the con-
S(cid:107)2 ≈
stants ζ1(m  δ) ≈ ζ2(m  δ) ≈ 1 for sufﬁciently large m.
Our bounds therefore “interpolate” between the random
noise regime  which behaves as √d(cid:107)r∗
(cid:107)2. More importantly  the
square root dependence is also notable here  as it shows that the semi-random robustness can remain
small even in regimes where m is chosen to be a very small fraction of d. For example  choosing a
small subspace of dimension m = 0.01d results in semi-random robustness of 10(cid:107)r∗
(cid:107)2 with high
probability  which might still not be perceptible in complex visual tasks. Hence  for semi-random
noise that is mostly random and only mildly adversarial (i.e.  the subspace dimension is small)  afﬁne
classiﬁers remain vulnerable to such noise.

Figure 1: ζ1(m  δ) and ζ2(m  δ) in func-
tion of m [δ = 0.05] .

(cid:107)2  and the worst-case noise (cid:107)r∗

(cid:112)d/m(cid:107)r∗

4 Robustness of general classiﬁers

4.1 Curvature of the decision boundary

S(cid:107)2 and worst-case robustness (cid:107)r∗

We now consider the general case where f is a nonlinear classiﬁer. We derive relations between
the random and semi-random robustness (cid:107)r∗
(cid:107)2 using properties
of the classiﬁer’s boundary. Let i and j be two arbitrary classes; we deﬁne the pairwise boundary
Bi j as the boundary of the binary classiﬁer where only classes i and j are considered. Formally  the
decision boundary is given by Bi j := {x ∈ Rd : fi(x)− fj(x) = 0}. The boundary Bi j separates
between two regions of Rd  namely Ri and Rj  where the estimated label of the binary classiﬁer is
respectively i and j.
We assume for the purpose of this analysis that the boundary Bi j is smooth. We are now interested
in the geometric properties of the boundary  namely its curvature. Many notions of curvature can
be deﬁned on hypersurfaces [11]. In the simple case of a curve in a two-dimensional space  the
curvature is deﬁned as the inverse of the radius of the so-called oscullating circle. One way to deﬁne
curvature for high-dimensional hypersurfaces is by taking normal sections of the hypersurface  and
measuring the curvature of the resulting planar curve (see Fig. 2). We however introduce a notion of
curvature that is speciﬁcally suited to the analysis of the decision boundary of a classiﬁer. Informally 
our curvature captures the global bending of the decision boundary by inscribing balls in the regions
separated by the decision boundary. For a given p ∈ Bi j  we deﬁne qi (cid:107) j(p) to be the radius of the
largest open ball included in the region Ri that intersects with Bi j at p; i.e. 
z∈Rd {(cid:107)z − p(cid:107)2 : B(z (cid:107)z − p(cid:107)2) ⊆ Ri}  

qi (cid:107) j(p) = sup

(6)

where B(z (cid:107)z − p(cid:107)2) is the open ball in Rd of center z and radius (cid:107)z − p(cid:107)2. An illustration
of this quantity in two dimensions is provided in Fig. 2 (b). It is not hard to see that any ball
− p(cid:107)2) centered in z∗ and included in Ri will have its tangent space at p coincide with
B(z∗ (cid:107)z∗
the tangent of the decision boundary at the same point.
It should further be noted that the deﬁnition in Eq. (6) is not symmetric in i and j. We therefore
deﬁne the following symmetric quantity qi j(p)  where the worst-case ball inscribed in any of the
two regions Ri and Rj is considered:

qi j(p) = min(cid:0)qi (cid:107) j(p)  qj (cid:107) i(p)(cid:1) .

4

m0200400600800100010-210-1100101102103104ζ1 (mδ  )ζ2δ (m  )(a)

(b)

Figure 2: (a) Normal section of the boundary Bi j with respect to plane U = span(n  u)  where n is
the normal to the boundary at p  and u is an arbitrary in the tangent space Tp(Bi j). (b) Illustration
of the quantities introduced for the deﬁnition of the curvature of the decision boundary.

To measure the global curvature  the worst-case radius is taken over all points on the decision
boundary  i.e.  q(Bi j) = inf p∈Bi j qi j(p). The curvature κ(Bi j) is then deﬁned as the inverse of
the worst-case radius: κ(Bi j) = 1/q(Bi j ).
In the case of afﬁne classiﬁers  we have κ(Bi j) = 0  as it is possible to inscribe balls of inﬁnite
radius inside each region of the space. When the classiﬁcation boundary is a union of (sufﬁciently
distant) spheres with equal radius R  the curvature κ(Bi j) = 1/R. In general  the quantity κ(Bi j)
provides an intuitive way of describing the nonlinearity of the decision boundary by ﬁtting balls
inside the classiﬁcation regions.

4.2 Robustness to random and semi-random noise

We now establish bounds on the robustness to random and semi-random noise in the binary classiﬁ-
cation case. Let x0 be a datapoint classiﬁed as ˆk = ˆk(x0). We ﬁrst study the binary classiﬁcation
problem  where only classes ˆk and k ∈ {1  . . .   L}\{ˆk} are considered. To simplify the notation 
we let Bk := Bk ˆk be the decision boundary between classes k and ˆk. In the case of the binary
classiﬁcation problem where classes k and ˆk are considered  the semi-random perturbation deﬁned in
Eq. (2) can be re-written as follows:

rkS = argmin

r∈S (cid:107)r(cid:107)2 s.t. fk(x0 + r) ≥ fˆk(x0 + r).

(7)
The worst case perturbation (obtained with S = Rd) is denoted by rk. It should be noted that the
global quantities r∗
S and r∗ are obtained from rkS and rk by taking the vectors with minimum norm
over all classes k.
The following result gives upper and lower bounds on the ratio (cid:107)rkS(cid:107)2
(cid:107)rk(cid:107)2
the boundary separating class k and ˆk.
Theorem 2. Let S be a random m-dimensional subspace of Rd. Let κ := κ(Bk). Assuming that the
curvature satisﬁes

in function of the curvature of

C

ζ2(m  δ)(cid:107)rk(cid:107)2

m
d

 

(8)

the following inequality holds between the semi-random robustness (cid:107)rkS(cid:107)2 and the adversarial
robustness (cid:107)rk(cid:107)2:

(cid:18)

(cid:19)(cid:112)

(cid:114)

κ ≤

(cid:114)

(cid:18)
1 − C1(cid:107)rk(cid:107)2κζ2

d
m

(cid:19)(cid:112)

ζ1

d

m ≤ (cid:107)rkS(cid:107)2
(cid:107)rk(cid:107)2 ≤

1 + C2(cid:107)rk(cid:107)2κζ2

d
m

ζ2

d
m

(9)

with probability larger than 1 − 4δ. We recall that ζ1 = ζ1(m  δ) and ζ2 = ζ2(m  δ) are deﬁned in
Eq. (3  4). The constants are C = 0.2  C1 = 0.625  C2 = 2.25.

The proof can be found in the appendix. This result shows that the bounds relating the robustness to
random and semi-random noise to the worst-case robustness can be extended to nonlinear classiﬁers 

5

UTpBjpγunR1R2p1B1 2p2q12(p1)q21(p2)provided the curvature of the boundary κ(Bk) is sufﬁciently small. In the case of linear classiﬁers 
we have κ(Bk) = 0  and we recover the result for afﬁne classiﬁers from Theorem 1.
To extend this result to multi-class classiﬁcation  special care has to be taken. In particular  if k
denotes a class that has no boundary with class ˆk  (cid:107)rk(cid:107)2 can be very large and the previous curvature
condition is not satisﬁed. It is therefore crucial to exclude such classes that have no boundary in
common with class ˆk  or more generally  boundaries that are far from class ˆk. We deﬁne the set A of
excluded classes k where (cid:107)rk(cid:107)2 is large

(cid:112)
A = {k : (cid:107)rk(cid:107)2 ≥ 1.45

(cid:114)

∗

d
m(cid:107)r

ζ2(m  δ)

(10)
Note that A is independent of S  and depends only on d  m and δ. Moreover  the constants in (10)
were chosen for simplicity of exposition.
Assuming a curvature constraint only on the close enough classes  the following result establishes a
S(cid:107)2 and (cid:107)r∗
simpliﬁed relation between (cid:107)r∗
Corollary 1. Let S be a random m-dimensional subspace of Rd. Assume that  for all k /∈ A  the
(cid:114)
curvature condition in Eq. (8) holds. Then  we have

(cid:114)

(cid:107)2}.

(cid:107)2.

∗

d
m(cid:107)r

(cid:107)2 ≤ (cid:107)r

∗
S(cid:107)2 ≤ 1.45

ζ2(m  δ)

∗

d
m(cid:107)r

(cid:107)2

(11)

(cid:112)

0.875

ζ1(m  δ)

(cid:112)

with probability larger than 1 − 4(L + 2)δ.
Under the curvature condition in (8) on the boundaries between ˆk and classes in Ac  our result
(cid:107)2 by a factor of(cid:112)d/m. In the random regime (m = 1)  this factor
shows that the robustness to random and semi-random noise exhibits the same behavior that has
been observed earlier for linear classiﬁers in Theorem 1. In particular  (cid:107)r∗
S(cid:107)2 is precisely related to
the adversarial robustness (cid:107)r∗
semi-random  the factor is(cid:112)d/m and shows that robustness to semi-random noise might not be
becomes √d  and shows that in high dimensional classiﬁcation problems  classiﬁers with sufﬁciently
ﬂat boundaries are much more robust to random noise than to adversarial noise. However  in the

achieved even if m is chosen to be a tiny fraction of d. In other words  if a classiﬁer is highly
vulnerable to adversarial perturbations  then it is also vulnerable to noise that is overwhelmingly
random and only mildly adversarial.
It is important to note that the curvature condition in Corollary 1 is not an assumption on the curvature
of the global decision boundary  but rather an assumption on the decision boundaries between pairs
of classes. The distinction here is signiﬁcant  as junction points where two decision boundaries meet
might actually have a very large (or inﬁnite) curvature (even in linear classiﬁcation settings)  and the
curvature condition in Corollary 1 typically does not hold for this global curvature deﬁnition. We
refer to our experimental section for a visualization of this phenomenon.
5 Experiments

1|D|

x∈D

(cid:80)

(cid:107)r∗
S (x)(cid:107)2
(cid:107)r∗(x)(cid:107)2

(cid:112)m/d

curvature property precisely behaves as(cid:112)d/m(cid:107)r∗(x)(cid:107)2. We ﬁrst check the accuracy of these results
notes the test set. This quantity provides indication to the accuracy of our(cid:112)d/m(cid:107)r∗(x)(cid:107)2 estimate of

We now evaluate the robustness of different image classiﬁers to random and semi-random pertur-
bations  and assess the accuracy of our bounds on various datasets and state-of-the-art classiﬁers.
Speciﬁcally  our theoretical results show that the robustness (cid:107)r∗
S (x)(cid:107)2 of classiﬁers satisfying the
in different classiﬁcation settings. For a given classiﬁer f and subspace dimension m  we deﬁne
  where S is chosen randomly for each sample x and D de-
β(f ; m) =
the robustness  and should ideally be equal to 1 (for sufﬁciently large m). Since β is a random quantity
(because of S)  we report both its mean and standard deviation for different networks in Table 1.
It should be noted that ﬁnding (cid:107)r∗
(cid:107)2 involves solving the optimization problem in (1).
We have used a similar approach to [13] to ﬁnd subspace minimal perturbations. For each network 
we estimate the expectation by averaging β(f ; m) on 1000 random samples  with S also chosen
randomly for each sample. Observe that β is suprisingly close to 1  even when m is a small fraction
of d. This shows that our quantitative analysis provide very accurate estimates of the robustness to
semi-random noise. We visualize the robustness to random noise  semi-random noise (with m = 10)

S(cid:107)2 and (cid:107)r∗

6

Table 1: β(f ; m) for different classiﬁers f and different subspace dimensions m. The VGG-F and
VGG-19 are respectively introduced in [2  16].

Classiﬁer
LeNet (MNIST)
LeNet (CIFAR-10)
VGG-F (ImageNet)
VGG-19 (ImageNet)

1/4

1/16

m/d
1/36

1/64

1/100

1.00 ± 0.06
1.01 ± 0.03
1.00 ± 0.01
1.00 ± 0.01

1.01 ± 0.12
1.02 ± 0.07
1.02 ± 0.02
1.02 ± 0.03

1.03 ± 0.20
1.04 ± 0.10
1.03 ± 0.04
1.02 ± 0.05

1.01 ± 0.26
1.06 ± 0.14
1.03 ± 0.05
1.03 ± 0.06

1.05 ± 0.34
1.10 ± 0.19
1.04 ± 0.06
1.04 ± 0.08

(a)

(b)

(c)

(d)

Figure 3: (a) Original image classiﬁed as “Cauliﬂower”. Fooling perturbations for VGG-F network:
(b) Random noise  (c) Semi-random perturbation with m = 10  (d) Worst-case perturbation  all
wrongly classiﬁed as “Artichoke”.

and worst-case perturbations on a sample image in Fig. 3. While random noise is clearly perceptible

due to the √d ≈ 400 factor  semi-random noise becomes much less perceptible even with a relatively

√
small value of m = 10  thanks to the 1/
m factor that attenuates the required noise to misclassify
the datapoint. It should be noted that the robustness of neural networks to adversarial perturbations
has previously been observed empirically in [17]  but we provide here a quantitative and generic
explanation for this phenomenon. The high accuracy of our bounds for different state-of-the-art
classiﬁers  and different datasets suggest that the decision boundaries of these classiﬁers have limited
curvature κ(Bk)  as this is a key assumption of our theoretical ﬁndings. To support the validity of this
curvature hypothesis in practice  we visualize two-dimensional sections of the classiﬁers’ boundary
in Fig. 4 in three different settings. Note that we have opted here for a visualization strategy rather
than the numerical estimation of κ(B)  as the latter quantity is difﬁcult to approximate in practice in
high dimensional problems. In Fig. 4  x0 is chosen randomly from the test set for each data set  and
the decision boundaries are shown in the plane spanned by r∗ and r∗
S  where S is a random direction
(i.e.  m = 1). Different colors on the boundary correspond to boundaries with different classes. It
can be observed that the curvature of the boundary is very small except at “junction” points where
the boundary of two different classes intersect. Our curvature assumption  which only assumes a
bound on the curvature of the decision boundary between pairs of classes ˆk(x0) and k (but not on the
global decision boundary that contains junctions with high curvature) is therefore adequate to the
decision boundaries of state-of-the-art classiﬁers according to Fig. 4. Interestingly  the assumption in
Corollary 1 is satisﬁed by taking κ to be an empirical estimate of the curvature of the planar curves in
Fig. 4 (a) for the dimension of the subspace being a very small fraction of d; e.g.  m = 10−3d. While
not reﬂecting the curvature κ(Bk) that drives the assumption of our theoretical analysis  this result
still seems to suggest that the curvature assumption holds in practice.
We now show a simple demonstration of the vulnerability of classiﬁers to semi-random noise in Fig. 5 
where a structured message is hidden in the image and causes data misclassiﬁcation. Speciﬁcally  we
consider S to be the span of random translated and scaled versions of words “NIPS”  “SPAIN” and
“2016” in an image  such that (cid:98)d/m(cid:99) = 228. The resulting perturbations in the subspace are therefore
linear combinations of these words with different intensities.4 The perturbed image x0 + r∗
S shown in

4This example departs somehow from the theoretical framework of this paper  where random subspaces
were considered. However  this empirical example suggests that the theoretical ﬁndings in this paper seem to
approximately hold when the subspace S have statistics that are close to a random subspace.

7

(a) VGG-F (ImageNet)

(b) LeNet (CIFAR)

(c) LeNet (MNIST)

Figure 4: Boundaries of three classiﬁers near randomly chosen samples. Axes are normalized by the
corresponding (cid:107)r∗
(cid:107)2κ.
Note the difference in range between x and y axes. Note also that the range of horizontal axis in (c)
is much smaller than the other two  hence the illustrated boundary is more curved.

(cid:107)2 as our assumption in the theoretical bound depends on the product of (cid:107)r∗

(a) Image of a “Potﬂower”

(b) Perturbation

(c) Classiﬁed as “Pineapple”

Figure 5: A fooling hidden message. S is the span of random translations and scales of the words
“NIPS”  “SPAIN”  and “2016”.

Fig. 5 (c) is clearly indistinguishable from Fig. 5 (a). This shows that imperceptibly small structured
messages can be added to an image causing data misclassiﬁcation.

6 Conclusion

In this work  we precisely characterized the robustness of classiﬁers in a novel semi-random noise
regime that generalizes the random noise regime. Speciﬁcally  our bounds relate the robustness
in this regime to the robustness to adversarial perturbations. Our bounds depend on the curvature
of the decision boundary  the data dimension  and the dimension of the subspace to which the
perturbation belongs. Our results show  in particular  that when the decision boundary has a small
curvature  classiﬁers are robust to random noise in high dimensional classiﬁcation problems (even if
the robustness to adversarial perturbations is relatively small). Moreover  for semi-random noise that
is mostly random and only mildly adversarial (i.e.  the subspace dimension is small)  our results show
that state-of-the-art classiﬁers remain vulnerable to such perturbations. To improve the robustness to
semi-random noise  our analysis encourages to impose geometric constraints on the curvature of the
decision boundary  as we have shown the existence of an intimate relation between the robustness of
classiﬁers and the curvature of the decision boundary.

Acknowledgments

We would like to thank the anonymous reviewers for their helpful comments. We thank Omar Fawzi
and Louis Merlin for the fruitful discussions. We also gratefully acknowledge the support of NVIDIA
Corporation with the donation of the Tesla K40 GPU used for this research. This work has been
partly supported by the Hasler Foundation  Switzerland  in the framework of the CORA project.

8

-100-75-50-250255075100125150-2.5-2-1.5-1-0.500.511.522.5x0B2B1-150-100-50050100150200-2.502.557.51012.5x0B2B1-5-2.502.557.5-10.75-0.50.2500.250.5x0B1B2References
[1] Caramanis  C.  Mannor  S.  and Xu  H. (2012). Robust optimization in machine learning. In Sra  S. 

Nowozin  S.  and Wright  S. J.  editors  Optimization for machine learning  chapter 14. Mit Press.

[2] Chatﬁeld  K.  Simonyan  K.  Vedaldi  A.  and Zisserman  A. (2014). Return of the devil in the details:

Delving deep into convolutional nets. In British Machine Vision Conference.

[3] Fawzi  A.  Fawzi  O.  and Frossard  P. (2015). Analysis of classiﬁers’ robustness to adversarial perturbations.

CoRR  abs/1502.02590.

[4] Fawzi  A. and Frossard  P. (2015). Manitest: Are classiﬁers really invariant? In British Machine Vision

Conference (BMVC)  pages 106.1–106.13.

[5] Goodfellow  I. J.  Shlens  J.  and Szegedy  C. (2015). Explaining and harnessing adversarial examples. In

International Conference on Learning Representations (ICLR).

[6] Gu  S. and Rigazio  L. (2014). Towards deep neural network architectures robust to adversarial examples.

arXiv preprint arXiv:1412.5068.

[7] Hinton  G. E.  Deng  L.  Yu  D.  Dahl  G. E.  Mohamed  A.  Jaitly  N.  Senior  A.  Vanhoucke  V.  Nguyen  P. 
Sainath  T. N.  and Kingsbury  B. (2012). Deep neural networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal Process. Mag.  29(6):82–97.

[8] Huang  R.  Xu  B.  Schuurmans  D.  and Szepesvári  C. (2015). Learning with a strong adversary. CoRR 

abs/1511.03034.

[9] Krizhevsky  A.  Sutskever  I.  and Hinton  G. E. (2012). Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems (NIPS)  pages 1097–1105.

[10] Lanckriet  G.  Ghaoui  L.  Bhattacharyya  C.  and Jordan  M. (2003). A robust minimax approach to

classiﬁcation. The Journal of Machine Learning Research  3:555–582.

[11] Lee  J. M. (2009). Manifolds and differential geometry  volume 107. American Mathematical Society

Providence.

[12] Luo  Y.  Boix  X.  Roig  G.  Poggio  T.  and Zhao  Q. (2015). Foveation-based mechanisms alleviate

adversarial examples. arXiv preprint arXiv:1511.06292.

[13] Moosavi-Dezfooli  S.-M.  Fawzi  A.  and Frossard  P. (2016). Deepfool: a simple and accurate method to

fool deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[14] Sabour  S.  Cao  Y.  Faghri  F.  and Fleet  D. J. (2016). Adversarial manipulation of deep representations.

In International Conference on Learning Representations (ICLR).

[15] Shaham  U.  Yamada  Y.  and Negahban  S. (2015). Understanding adversarial training: Increasing local

stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432.

[16] Simonyan  K. and Zisserman  A. (2014). Very deep convolutional networks for large-scale image recogni-

tion. In International Conference on Learning Representations (ICLR).

[17] Szegedy  C.  Zaremba  W.  Sutskever  I.  Bruna  J.  Erhan  D.  Goodfellow  I.  and Fergus  R. (2014).
Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR).
[18] Tabacof  P. and Valle  E. (2016). Exploring the space of adversarial images. IEEE International Joint

Conference on Neural Networks.

[19] Xu  H.  Caramanis  C.  and Mannor  S. (2009). Robustness and regularization of support vector machines.

The Journal of Machine Learning Research  10:1485–1510.

[20] Zhao  Q. and Grifﬁn  L. D. (2016). Suppressing the unusual: towards robust cnns using symmetric

activation functions. arXiv preprint arXiv:1603.05145.

9

,Daniel Hsu
Aryeh Kontorovich
Csaba Szepesvari
Alhussein Fawzi
Seyed-Mohsen Moosavi-Dezfooli
Pascal Frossard