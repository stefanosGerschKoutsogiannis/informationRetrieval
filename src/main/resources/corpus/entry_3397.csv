2009,Measuring model complexity with the prior predictive,In the last few decades  model complexity has received a lot of press. While many methods have been proposed that jointly measure a model’s descriptive adequacy and its complexity  few measures exist that measure complexity in itself. Moreover  existing measures ignore the parameter prior  which is an inherent part of the model and affects the complexity. This paper presents a stand alone measure for model complexity  that takes the number of parameters  the functional form  the range of the parameters and the parameter prior into account. This Prior Predictive Complexity (PPC) is an intuitive and easy to compute measure. It starts from the observation that model complexity is the property of the model that enables it to fit a wide range of outcomes. The PPC then measures how wide this range exactly is.,Measuring model complexity with the prior predictive

Wolf Vanpaemel ∗

Department of Psychology

University of Leuven

wolf.vanpaemel@psy.kuleuven.be

Belgium.

Abstract

In the last few decades  model complexity has received a lot of press. While many
methods have been proposed that jointly measure a model’s descriptive adequacy
and its complexity  few measures exist that measure complexity in itself. More-
over  existing measures ignore the parameter prior  which is an inherent part of the
model and affects the complexity. This paper presents a stand alone measure for
model complexity  that takes the number of parameters  the functional form  the
range of the parameters and the parameter prior into account. This Prior Predictive
Complexity (PPC) is an intuitive and easy to compute measure. It starts from the
observation that model complexity is the property of the model that enables it to
ﬁt a wide range of outcomes. The PPC then measures how wide this range exactly
is.
keywords: Model Selection & Structure Learning; Model Comparison Methods;
Perception

The recent revolution in model selection methods in the cognitive sciences was driven to a large
extent by the observation that computational models can differ in their complexity. Differences
in complexity put models on unequal footing when their ability to approximate empirical data is
assessed. Therefore  models should be penalized for their complexity when their adequacy is mea-
sured. The balance between descriptive adequacy and complexity has been termed generalizability
[1  2].
Much attention has been devoted to developing  advocating  and comparing different measures of
generalizability (for a recent overview  see [3]). In contrast  measures of complexity have received
relatively little attention. The aim of the current paper is to propose and illustrate a stand alone
measure of model complexity  called the Prior Predictive Complexity (PPC). The PPC is based on
the intuitive idea that a complex model can predict many outcomes and a simple model can predict
a few outcomes only.
First  I discuss existing approaches to measuring model complexity and note some of their limita-
tions. In particular  I argue that currently existing measures ignore one important aspect of a model:
the prior distribution it assumes over the parameters. I then introduce the PPC  which  unlike the
existing measures  is sensitive to the parameter prior. Next  the PPC is illustrated by calculating the
complexities of two popular models of information integration.

1 Previous approaches to measuring model complexity

A ﬁrst approach to assess the (relative) complexity of models relies on simulated data. Simulation-
based methods differ in how these artiﬁcial data are generated. A ﬁrst  atheoretical approach uses
random data [4  5]. In the semi-theoretical approach  the data are generated from some theoretically

∗I am grateful to Michael Lee and Liz Bonawitz.

1

interesting functions  such as the exponential or the logistic function [4]. Using these approaches 
the models under consideration are equally complex if each model provides the best optimal ﬁt to
roughly the same number of data sets. A ﬁnal approach to generating artiﬁcial data is a theoretical
one  in which the data are generated from the models of interest themselves [6  7]. The parameter
sets used in the generation can either be hand-picked by the researcher  estimated from empirical
data or drawn from a previously speciﬁed distribution. If the models under consideration are equally
complex  each model should provide the best optimal ﬁt to self-generated data more often than the
other models under consideration do.
One problem with this simulation-based approach is that it is very labor intensive. It requires gen-
erating a large amount of artiﬁcial data sets  and ﬁtting the models to all these data sets. Further 
it relies on choices that are often made in an arbitrary fashion that nonetheless bias the results. For
example  in the semi-theoretical approach  a crucial choice is which functions to use. Similarly  in
the theoretical approach  results are heavily inﬂuenced by the parameter values used in generating
the data. If they are ﬁxed  on what basis? If they are estimated from empirical data  from which
data? If they are drawn randomly  from which distribution? Further  a simulation study only gives a
rough idea of complexity differences but provides no direct measure reﬂecting the complexity.
A number of proposals have been made to measure model complexity more directly. Consider a
model M with k parameters  summarized in the parameter vector θ = (θ1  θ2  . . .   θk  ) which has a
range indicated by Ω. Let d denote the data and p(d|θ  M) the likelihood. The most straightforward
measure of model complexity is the parametric complexity (PC)  which simply counts the number
of parameters:

PC = k.

(1)

PC is attractive as a measure of model complexity since it is very easy to calculate. Further  it has a
direct and well understood relation toward complexity: the more parameters  the more complex the
model. It is included as the complexity term of several generalizability measures such as AIC [8]
and BIC [9]  and it is at the heart of the Likelihood Ratio Test.
Despite this intuitive appeal  PC is not free from problems. One problem with PC is that it re-
ﬂects only a single aspect of complexity. Also the parameter range and the functional form (the
way the parameters are combined in the model equation) inﬂuence a model’s complexity  but these
dimensions of complexity are ignored in PC [2  6].
A complexity measure that takes these three dimensions into account is provided by the geometric
complexity (GC) measure  which is inspired by differential geometry [10]. In GC  complexity is
conceptualized as the number of distinguishable probability distributions a model can generate. It is
deﬁned by

(cid:112)det I(θ|M)dθ 

GC = k
2

ln n
2π

+ ln

(cid:90)

Ω

(2)

(3)

where n indicates the size of the data sample and I(θ) is the Fisher Information Matrix:

Iij(θ|M) = −Eθ

∂2 ln p(d|θ  M)

∂θi∂θj

.

Note that I(θ|M) is determined by the likelihood function p(d|θ  M)  which is in turn determined
by the model equation. Hence GC is sensitive to the number of parameters (through k)  the func-
tional form (through I)  and the range (through Ω). Quite surprisingly  GC turns out to be equal
to the complexity term used in one version of Minimum Description Length (MDL)  a measure of
generalizability developed within the domain of information theory [2  11  12  13].
GC contrasts favorably with PC  in the sense that it takes three dimensions of complexity into ac-
count rather than a single one. A major drawback of GC is that  unlike PC  it requires considerable
technical sophistication to be computed  as it relies on the second derivative of the likelihood. A
more important limitation of both PC and GC is that these measures are insensitive to yet another
important dimension contributing to model complexity: the prior distribution over the model pa-
rameters. The relation between the parameter prior distribution and model complexity is discussed
next.

2

2 Model complexity and the parameter prior

The growing popularity of Bayesian methods in psychology has not only raised awareness that
model complexity should be taken into account when testing models [6]  it has also drawn attention
to the fact that in many occasions  relevant prior information is available [14]. In Bayesian methods 
there is room to incorporate this information in two different ﬂavors: as a prior distribution over the
models  or as a prior distribution over the parameters. Specifying a model prior is a daunting task  so
almost invariably  the model prior is taken to be uniform (but see [15] for an exception). In contrast 
information regarding the parameter is much easier to include  although still challenging (e.g.  [16]).
There are two ways to formalize prior information about a model’s parameters: using the parameter
prior range (often referred to as simply the range) and using the parameter prior distribution (often
referred to as simply the prior). The prior range indicates which parameter values are allowed
and which are forbidden. The prior distribution indicates which parameter values are likely and
which are unlikely. Models that share the same equation and the same range but differ in the prior
distribution can be considered different models (or at least different model versions)  just like models
that share the same equation but differ in range are different model versions. Like the parameter prior
range  the parameter prior distribution inﬂuences the model complexity. In general  a model with a
vague parameter prior distribution is more complex than a model with a sharply peaked parameter
prior distribution  much as a model with a broad-ranged parameter is more complex than the same
model where the parameter is heavily restricted.
To drive home the point that the parameter prior should be considered when model complexity is
assessed  consider the following “fair coin” model Mf and a “biased coin” model Mb. There is a
clear intuitive complexity difference between these models: Mb is more complex than Mf . The
most straightforward way to formalize these models is as follows  where ph denotes the probability
of observing heads:

for model Mf and the triplet of equations

ph = 1/2 

ph = θ
0 ≤ θ ≤ 1
p(θ) = 1 

(4)

(5)

jointly deﬁne model Mb. The range forbids values smaller than 0 or greater than 1 because ph is a
proportion. As Mf and Mb have a different number of parameters  both PC and GC  being sensitive
to the number of parameters  pick up the difference in model complexity between the models.
Alternatively  model Mf could be deﬁned as follows:

ph = θ
0 ≤ θ ≤ 1
p(θ) = δ(θ − 1
) 
2

(6)

where δ(x) is the Dirac delta. Note that the model formalized in Equation 6 is exactly identical the
model formalized in Equation 4. However  relying on the formulation of model Mf in Equation 6 
PC and GC now judge Mf and Mb to be equally complex: both models share the same model
equation (which implies they have the same number of parameters and the same functional form) and
the same range for the parameter. Hence  PC and GC make an incorrect judgement of the complexity
difference between both models. This misjudgement is a direct result of the insensitivity of these
measures to the parameter prior. As models Mf and Mb have different prior distributions over their
parameter  a measure sensitive to the prior would pick up the complexity difference between these
models. Such a measure is introduced next.

3 The Prior Predictive Complexity

Model complexity refers to the property of the model that enables it to predict a wide range of data
patterns [2]. The idea of the PPC is to measure how wide this range exactly is. A complex model

3

can predict many outcomes  and a simple model can predict a few outcomes only. Model simplicity 
then  refers to the property of placing restrictions on the possible outcomes: the greater restrictions 
the greater the simplicity.
To understand how model complexity is measured in the PPC  it is useful to think about the universal
interval (UI) and the predicted interval (PI). The universal interval is the range of outcomes that could
potentially be observed  irrespective of any model. For example  in an experiment with n binomial
trials  it is impossible to observe less that zero successes  or more than n successes  so the range of
possible outcomes is [0  n] . Similarly  the universal interval for a proportion is [0  1]. The predicted
interval is the interval containing all outcomes the model predicts.
An intuitive way to gauge model complexity is then the cardinality of the predicted interval  relative
to the cardinality of the universal interval  averaged over all m conditions or stimuli:

PPC =

1
m

|PIi|
|UIi| .

(7)

m(cid:88)

i=1

(cid:90)

A key aspect of the PPC is deriving the predicted interval. For a parameterized likelihood-based
model  prediction takes the form of a distribution over all possible outcomes for some future  yet-to-
be-observed data d under some model M. This distribution is called the prior predictive distribution
(ppd) and can be calculated using the law of total probability:

p(d|M) =

p(d|θ  M)p(θ|M)dθ.

(8)

Ω

Predicting the probability of unseen future data d arising under the assumption that model M is true
involves integrating the probability of the data for each of the possible parameter values  p(d|θ  M) 
as weighted by the prior probability of each of these values  p(θ|M).
Note that the ppd relies on the number of parameters (through the number of integrals and the likeli-
hood)  the model equation (through the likelihood)  and the parameter range (through Ω). Therefore 
as GC  the PPC is sensitive to all these aspects. In contrast to GC  however  the ppd  and hence the
PPC  also relies on the parameter prior.
Since predictions are made probabilistically  virtually all outcomes will be assigned some prior
weight. This implies that  in principle  the predicted interval equals the universal interval. However 
for some outcomes the assigned weight will be extremely small. Therefore  it seems reasonable to
restrict the predicted interval to the smallest interval that includes some predetermined amount of the
prior mass. For example  the 95% predictive interval is deﬁned by those outcomes with the highest
prior mass that together make up 95% of the prior mass.
Analytical solutions to the integral deﬁning the ppd are rarely available. Instead  one should rely on
approximations to the ppd by drawing samples from it. In the current study  sampling was performed
using WinBUGS [17  18]  a highly versatile  user friendly  and freely available software package.
It contains sophisticated and relatively general-purpose Markov Chain Monte Carlo (MCMC) algo-
rithms to sample from any distribution of interest.

4 An application example

The PPC is illustrated by comparing the complexity of two popular models of information integra-
tion  which attempt to account for how people merge potentially ambiguous or conﬂicting informa-
tion from various sensorial sources to create subjective experience. These models either assume that
the sources of information are combined additively (the Linear Integration Model; LIM; [19]) or
multiplicatively (the Fuzzy Logical Model of Perception; FLMP; [20  21]).

4.1

Information integration tasks

A typical information integration task exposes participants simultaneously to different sources of
information and requires this combined experience to be identiﬁed in a forced-choice identiﬁcation
task. The presented stimuli are generated from a factorial manipulation of the sources of information
by systematically varying the ambiguity of each of the sources. The relevant empirical data consist

4

of  for each of the presented stimuli  the counts km of the number of times the mth stimulus was
identiﬁed as one of the response alternatives  out of the tm trials on which it was presented.
For example  an experiment in phonemic identiﬁcation could involve two phonemes to be identiﬁed 
/ba/ and /da/ and two sources of information  auditory and visual. Stimuli are created by crossing
different levels of audible speech  varying between /ba/ and /da/  with different levels of visible
speech  also varying between these alternatives. The resulting set of stimuli spans a continuum
between the two syllables. The participant is then asked to listen and to watch the speaker  and
based on this combined audiovisual experience  to identify the syllable as being either /ba/ or /da/.
In the so-called expanded factorial design  not only bimodal stimuli (containing both auditory and
visual information) but also unimodal stimuli (providing only a single source of information) are
presented.

4.2

Information integration models

In what follows  the formal description of the LIM and the FLMP is outlined for a design with two
response alternatives (/da/ or /ba/) and two sources (auditory and visual)  with I and J levels 
respectively. In such a two-choice identiﬁcation task  the counts km follow a Binomial distribution:

km ∼ Binomial(pm  tm) 

(9)

where pm indicates the probability that the mth stimulus is identiﬁed as /da/.

4.2.1 Model equation

The probability for the stimulus constructed with the ith level of the ﬁrst source and the jth level of
the second being identiﬁed as /da/ is computed according to the choice rule:

pij =

s (ij  /da/)

s (ij  /da/) + s (ij  /ba/)  

(10)

where s (ij  /da/) represents the overall degree of support for the stimulus to be /da/.
The sources of information are assumed to be evaluated independently  implying that different pa-
rameters are used for the different modalities. In the present example  the degree of auditory sup-
port for /da/ is denoted by ai (i = 1  . . .   I) and the degree of visual support for /da/ by bj
(j = 1  . . .   J).
When a unimodal stimulus is presented  the overall degree of support for each alternative is given by
s (i∗  /da/) = ai and s (∗j  /da/) = bj  where the asterisk (*) indicates the absence of information 
implying that Equation 10 reduces to

pi∗ = ai

and p∗j = bj.

(11)

When a bimodal stimulus is presented  the overall degree of support for each alternative is based
on the integration or blending of both these sources. Hence  for bimodal stimuli  s (ij  /da/) =
ai
duces to

(cid:78) bj  where the operator(cid:78) denotes the combination of both sources. Hence  Equation 10 re-
The LIM assumes an additive combination  i.e. (cid:78) = +  so Equation 12 becomes
The FLMP  in contrast  assumes a multiplicative combination  i.e. (cid:78) = ×  so Equation 12 becomes

pij = ai + bj

(cid:78) bj + (1 − ai)(cid:78)(1 − bj) .

ai

(cid:78) bj

pij =

ai

(12)

(13)

.

2

pij =

aibj

aibj + (1 − ai)(1 − bj) .

(14)

5

4.2.2 Parameter prior range and distribution

Each level of auditory and visual support for /da/ (i.e.  ai and bj  respectively) is associated with a
free parameter  which implies that the FLMP and the LIM have an equal number of free parameters 
I + J. Each of these parameters is constrained to satisfy 0 ≤ ai  bj ≤ 1.
The original formulations of the LIM and FLMP unfortunately left the parameter priors unspeciﬁed.
However  an implicit assumption that has been commonly used is a uniform prior for each of the
parameters. This assumption implicitly underlies classical and widely adopted methods for model
evaluation using accounted percentage of variance or maximum likelihood.

ai ∼ Uniform(0  1) and bi ∼ Uniform(0  1)

i = 1  . . .   I; j = 1  . . .   J.
The models relying on this set of uniform priors will be referred to as LIMu and FLMPu.
Note that LIMu and FLMPu treat the different parameters as independent. This approach misses
important information. In particular  the experimental design is such that the amount of support for
each level i + 1 is always higher than for level i. Because parameter ai (or bi) corresponds to the
degree of auditory (or visual) support for a unimodal stimulus at the ith level  it seems reasonable to
expect the following orderings among the parameters to hold (see also [6]):

for

(15)

The models relying on this set of ordered priors will be referred to as LIMo and FLMPo.

aj > ai

and bj > bi

for

j > i.

(16)

4.3 Complexity and experimental design

It is tempting to consider model complexity as an inherent characteristic of a model. For some mod-
els and for some measures of complexity this is clearly the case. Consider  for example  model Mb.
In any experimental design (i.e.  a number of coin tosses)  PCMb = 1. However  more generally 
this is not the case. Focusing on the FLMP and the LIM  it is clear that even a simple measure as
PC depends crucially on (some aspects of) the experimental design. In particular  every level corre-
sponds to a new parameter  so PC = I + J . Similarly  GC is dependent on design choices. The PPC
is not different in this respect.
The design sensitivity implies that one can only make sensible conclusions about differences in
model complexity by using different designs. In an information integration task  the design deci-
sions include the type of design (expanded or not)  the number of sources  the number of response
alternatives  the number of levels for each source  and the number of observations for each stimulus
(sample size). The present study focuses on the expanded factorial designs with two sources and two
response alternatives. The additional design features were varied: both a 5 × 5 and a 8 × 2 design
were considered  using three different sample sizes (20  60 and 150  following [2]).

4.4 Results
Figure 1 shows the 99% predicted interval in the 8×2 design with n = 150. Each panel corresponds
to a different model. In each panel  each of the 26 stimuli is displayed on the x-axis. The ﬁrst eight
stimuli correspond to the stimuli with the lowest level of visual support  and are ordered in increasing
order of auditory support. The next eight stimuli correspond to the stimuli with the highest level of
visual support. The next eight stimuli correspond to the unimodal stimuli where only auditory
information is provided (again ranked in increasing order). The ﬁnal two stimuli are the unimodal
visual stimuli.
Panel A shows that the predicted interval of LIMu nearly equals the universal interval  ranging
between 0 and 1. This indicates that almost all outcomes are given a non-negligible prior mass
by LIMu  making it almost maximally complex. FLMPu is even more complex. The predicted
interval  shown in Panel B  virtually equals the universal interval  indicating that the model predicts
virtually every possible outcome. Panels C and D show the dramatic effect of incorporating relevant
prior information in the models. The predicted intervals of both LIMo and FLMPo are much smaller
than their counterparts using the uniform priors.
Focusing on the comparison between LIM and FLMP  the PPC indicates that the latter is more com-
plex than the former. This observation holds irrespective of the model version (assuming uniform

6

A

C

B

D

Figure 1: The 99% predicted interval for each of the 26 stimuli (x-axis) according to LIMu (Panel
A)  FLMPu (Panel B)  LIMo (Panel C)  and FLMPo (Panel D).

Table 1: PPC  based on the 99% predicted interval  for four models across six different designs.

20

0.97
1
0.75
0.83

LIMu
FLMPu
LIMo
FLMPo

5 × 5
60

0.94
1
0.67
0.80

8 × 2
60

0.95
1
0.69
0.82

150

0.94
0.99
0.66
0.81

20

.97
1
0.77
0.86

150

0.93
0.99
0.64
0.78

7

11              21              1*              *1    00.10.20.30.40.50.60.70.80.91Proportion of /da/ responses11              21              1*              *1    00.10.20.30.40.50.60.70.80.91Proportion of /da/ responses11              21              1*              *1    00.10.20.30.40.50.60.70.80.91Proportion of /da/ responses11              21              1*              *1    00.10.20.30.40.50.60.70.80.91Proportion of /da/ responsesvs. ordered priors). The smaller complexity of LIM is in line with previous attempts to measure
the relative complexities of LIM and FLMP  such as the atheoretical simulation-based approach ([4]
but see [5])  the semi-theoretical simulation-based approach [4]  the theoretical simulation-based
approach [2  6  22]  and a direct computation of the GC [2].
The PPC’s for all six designs considered are displayed in Table 1. It shows that the observations
made for the 8 × 2  n = 150 design holds across the ﬁve remaining designs as well: LIM is simpler
than FLMP; and models assuming ordered priors are simpler than models assuming uniform priors.
Note that these conclusions would not have been possible based on PC or GC. For PC  all four
models have the same complexity. GC  in contrast  would detect complexity differences between
LIM and FLMP (i.e.  the ﬁrst conclusion)  but due to its insensitivity to the parameter prior  the
complexity differences between LIMu and LIMo on the one hand  and FLMPu and FLMPo on the
other hand (i.e.  the second conclusion) would have gone unnoticed.

5 Discussion

A theorist deﬁning a model should clearly and explicitly specify at least the three following pieces of
information: the model equation  the parameter prior range  and the parameter prior distribution. If
any of these pieces is missing  the model should be regarded as incomplete  and therefore untestable.
Consequently  any measure of generalizability should be sensitive to all three aspects of the model
deﬁnition. Many currently popular generalizability measures do not satisfy this criterion  including
AIC  BIC and MDL. A measure of generalizability that does take these three aspects of a model into
account is the marginal likelihood [6  7  14  23]. Often  the marginal likelihood is criticized exactly
for its sensitivity to the prior range and distribution (e.g.  [24]). However  in the light of the fact that
the prior is a part of the model deﬁnition  I see the sensitivity of the marginal likelihood to the prior
as an asset rather than a nuisance. It is precisely the measures of generalizability that are insensitive
to the prior that miss an important aspect of the model.
Similarly  any stand alone measure of model complexity should be sensitive to all three aspects of the
model deﬁnition  as all three aspects contribute to the model’s complexity (with the model equation
contributing two factors: the number of parameters and the functional form). Existing measures of
complexity do not satisfy this requirement and are therefore incomplete. PC takes only part of the
model equation into account  whereas GC takes only the model equation and the range into account.
In contrast  the PPC currently proposed is sensitive to all these three aspects. It assesses model
complexity using the predicted interval which contains all possible outcomes a model can generate.
A narrow predicted interval (relative to the universal interval) indicates a simple model; a complex
model is characterized by a wide predicted interval.
There is a tight coupling between the notions of information  knowledge and uncertainty  and the
notion of model complexity. As parameters correspond to unknown variables  having more in-
formation available leads to fewer parameters and hence to a simpler model. Similarly  the more
information there is available  the sharper the parameter prior  implying a simpler model. To put
it differently  the less uncertainty present in a model  the narrower its predicted interval  and the
simpler the model. For example  in model Mb  there is maximal uncertainty. Nothing but the range
is known about θ  so all values of θ are equally likely. In contrast  in model Mf   there is minimal
uncertainty. In fact  ph is known for sure  so only a single value of θ is possible. This difference in
uncertainty is translated in a difference in complexity. The same is true for the information integra-
tion models. Incorporating the order constraints in the priors reduces the uncertainty compared to
the models without these constraints (it tells you  for example  that parameter a1 is smaller than a2).
This reduction in uncertainty is reﬂected by a smaller complexity.
There are many different sources of prior information that can be translated in a range or distribu-
tion. The illustration using the information integration models highlighted that prior information
can reﬂect meaningful information in the design. Alternatively  priors can be informed by previous
applications of similar models in similar settings. Probably the purest form of priors are those that
translate theoretical assumptions made by a model (see [16]). The fact that it is often difﬁcult to for-
malize this prior information may not be used as an excuse to leave the prior unspeciﬁed. Sure it is a
challenging task  but so is translating theoretical assumptions into the model equation. Formalizing
theory  intuitions  and information is what model building is all about.

8

References
[1] Myung  I. J. (2000) The importance of complexity in model selection. Journal of Mathematical Psychol-

ogy  44  190–204.

[2] Pitt  M. A.  Myung  I. J.  and Zhang  S. (2002) Toward a method of selecting among computational models

of cognition. Psychological Review  109  472–491.

[3] Shiffrin  R. M.  Lee  M. D.  Kim  W.  and Wagenmakers  E. J. (2008) A survey of model evaluation

approaches with a tutorial on hierarchical Bayesian methods. Cognitive Science  32  1248–1284.

[4] Cutting  J. E.  Bruno  N.  Brady  N. P.  and Moore  C. (1992) Selectivity  scope  and simplicity of models:
A lesson from ﬁtting judgments of perceived depth. Journal of Experimental Psychology: General  121 
364–381.

[5] Dunn  J. (2000) Model complexity: The ﬁt to random data reconsidered. Psychological Research  63 

174–182.

[6] Myung  I. J. and Pitt  M. A. (1997) Applying Occam’s razor in modeling cognition: A Bayesian approach.

Psychonomic Bulletin & Review  4  79–95.

[7] Vanpaemel  W. and Storms  G. (in press) Abstraction and model evaluation in category learning. Behavior

Research Methods.

[8] Akaike  H. (1973) Information theory and an extension of the maximum likelihood principle. Petrov  B.
and Csaki  B. (eds.)  Second International Symposium on Information Theory  pp. 267–281  Academiai
Kiado.

[9] Schwarz  G. (1978) Estimating the dimension of a model. Annals of Statistics  6  461–464.
[10] Myung  I. J.  Balasubramanian  V.  and Pitt  M. A. (2000) Counting probability distributions: Differential

geometry and model selection. Proceedings of the National Academy of Sciences  97  11170–11175.

[11] Lee  M. D. (2002) Generating additive clustering models with minimal stochastic complexity. Journal of

Classiﬁcation  19  69–85.

[12] Rissanen  J. (1996) Fisher information and stochastic complexity. IEEE Transactions on Information

Theory  42  40–47.

[13] Gr¨unwald  P. (2000) Model selection based on minimum description length. Journal of Mathematical

Psychology  44  133–152.

[14] Lee  M. D. and Wagenmakers  E. J. (2005) Bayesian statistical inference in psychology: Comment on

Traﬁmow (2003). Psychological Review  112  662–668.

[15] Lee  M. D. and Vanpaemel  W. (2008) Exemplars  prototypes  similarities and rules in category represen-

tation: An example of hierarchical Bayesian analysis. Cognitive Science  32  1403–1424.

[16] Vanpaemel  W. and Lee  M. D. (submitted) Using priors to formalize theory: Optimal attention and the

generalized context model.

[17] Lee  M. D. (2008) Three case studies in the Bayesian analysis of cognitive models. Psychonomic Bulletin

& Review  15  1–15.

[18] Spiegelhalter  D.  Thomas  A.  Best  N.  and Lunn  D. (2004) WinBUGS User Manual Version 2.0. Medi-

cal Research Council Biostatistics Unit. Institute of Public Health  Cambridge.

[19] Anderson  N. H. (1981) Foundations of information integration theory. Academic Press.
[20] Oden  G. C. and Massaro  D. W. (1978) Integration of featural information in speech perception. Psycho-

logical Review  85  172–191.

[21] Massaro  D. W. (1998) Perceiving Talking Faces: From Speech Perception to a Behavioral Principle. MIT

Press.

[22] Massaro  D. W.  Cohen  M. M.  Campbell  C. S.  and Rodriguez  T. (2001) Bayes factor of model selection

validates FLMP. Psychonomic Bulletin and Review  8  1–17.

[23] Kass  R. E. and Raftery  A. E. (1995) Bayes factors. Journal of the American Statistical Association  90 

773–795.

[24] Liu  C. C. and Aitkin  M. (2008) Bayes factors: Prior sensitivity and model generalizability. Journal of

Mathematical Psychology  53  362–375.

9

,Christian Szegedy
Alexander Toshev
Dumitru Erhan