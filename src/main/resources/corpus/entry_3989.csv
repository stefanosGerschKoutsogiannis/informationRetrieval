2017,On the Optimization Landscape of Tensor Decompositions,Non-convex optimization with local search heuristics has been widely used in machine learning  achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that ``all local optima are (approximately) global optima''  and thus they can be solved efficiently by local search algorithms. However  establishing such property can be very difficult.   In this paper  we analyze the optimization landscape of the random over-complete  tensor decomposition problem  which has many applications in unsupervised leaning  especially in learning latent variable models. In practice  it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant $\epsilon > 0$  among the set of points with function values $(1+\epsilon)$-factor larger than the expectation of the function  all the local maxima are approximate global maxima. Previously  the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess  the gradient ascent algorithm is guaranteed to solve this problem.   Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge  this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.,On the Optimization Landscape of Tensor

Decompositions

Rong Ge

Duke University

rongge@cs.duke.edu

Tengyu Ma

Facebook AI Research

tengyuma@cs.stanford.edu

Abstract

Non-convex optimization with local search heuristics has been widely used in
machine learning  achieving many state-of-art results. It becomes increasingly
important to understand why they can work for these NP-hard problems on typical
data. The landscape of many objective functions in learning has been conjectured
to have the geometric property that “all local optima are (approximately) global op-
tima”  and thus they can be solved efﬁciently by local search algorithms. However 
establishing such property can be very difﬁcult.
In this paper  we analyze the optimization landscape of the random over-complete
tensor decomposition problem  which has many applications in unsupervised lean-
ing  especially in learning latent variable models. In practice  it can be efﬁciently
solved by gradient ascent on a non-convex objective. We show that for any small
constant ε > 0  among the set of points with function values (1 + ε)-factor larger
than the expectation of the function  all the local maxima are approximate global
maxima. Previously  the best-known result only characterizes the geometry in
small neighborhoods around the true components. Our result implies that even
with an initialization that is barely better than the random guess  the gradient ascent
algorithm is guaranteed to solve this problem.
Our main technique uses Kac-Rice formula and random matrix theory. To our best
knowledge  this is the ﬁrst time when Kac-Rice formula is successfully applied to
counting the number of local optima of a highly-structured random polynomial
with dependent coefﬁcients.

1

Introduction

Non-convex optimization is the dominating algorithmic technique behind many state-of-art results in
machine learning  computer vision  natural language processing and reinforcement learning. Local
search algorithms through stochastic gradient methods are simple  scalable and easy to implement.
Surprisingly  they also return high-quality solutions for practical problems like training deep neural
networks  which are NP-hard in the worst case. It has been conjectured [DPG+14  CHM+15] that
on typical data  the landscape of the training objectives has the nice geometric property that all
local minima are (approximate) global minima. Such property assures the local search algorithms to
converge to global minima [GHJY15  LSJR16  NP06  SQW15]. However  establishing it for concrete
problems can be challenging.
Despite recent progress on understanding the optimization landscape of various machine learning
problems (see [GHJY15  BBV16  BNS16  Kaw16  GLM16  HM16  HMR16] and references therein) 
a comprehensive answer remains elusive. Moreover  all previous techniques fundamentally rely on
the spectral structure of the problems. For example  in [GLM16] allows us to pin down the set of the
critical points (points with vanishing gradients) as approximate eigenvectors of some matrix. Among

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

these eigenvectors we can further identify all the local minima. The heavy dependency on linear
algebraic structure limits the generalization to problems with non-linearity (like neural networks).
Towards developing techniques beyond linear algebra  in this work  we investigate the optimization
landscape of tensor decomposition problems. This is a clean non-convex optimization problem whose
optimization landscape cannot be analyzed by the previous approach. It also connects to the training
of neural networks with many shared properties [NPOV15] . For example  in comparison with the
matrix case where all the global optima reside on a (connected) Grassmannian manifold  for both
tensors and neural networks all the global optima are isolated from each other.
Besides the technical motivations above  tensor decomposition itself is also the key algorithmic tool for
learning many latent variable models  mixture of Gaussians  hidden Markov models  dictionary learn-
ing [Cha96  MR06  HKZ12  AHK12  AFH+12  HK13]  just to name a few. In practice  local search
heuristics such as alternating least squares [CLA09]  gradient descent and power method [KM11] are
popular and successful.
Tensor decomposition also connects to the learning of neural networks [GLM17  JSA15  CS16].
For example  The work [GLM17] shows that the objective of learning one-hidden-layer network is
implicitly decomposing a sequence of tensors with shared components  and uses the intuition from
tensor decomposition to design better objective functions that provably recovers the parameters under
Gaussian inputs.
Concretely  we consider decomposing a random 4-th order tensor T of the rank n of the following
form 

T =

ai ⊗ ai ⊗ ai ⊗ ai .

We are mainly interested in the over-complete regime where n (cid:29) d. This setting is particularly
challenging  but it is crucial for unsupervised learning applications where the hidden representations
have higher dimension than the data [AGMM15  DLCC07]. Previous algorithmic results either
require access to high order tensors [BCMV14  GVX13]  or use complicated techniques such as
FOOBI [DLCC07] or sum-of-squares relaxation [BKS15  GM15  HSSS16  MSS16].
In the worst case  most tensor problems are NP-hard [Hås90  HL13]. Therefore we work in the
average case where vectors ai ∈ Rd are assumed to be drawn i.i.d from Gaussian distribution N (0  I).
We call ai’s the components of the tensor. We are given the entries of tensor T and our goal is to
recover the components a1  . . .   an.
We will analyze the following popular non-convex objective 

n(cid:88)

i=1

(cid:88)

max f (x) =

Ti j k lxixjxkxl =

(cid:104)ai  x(cid:105)4

(1.1)

i j k l∈[d]4

s.t. (cid:107)x(cid:107) = 1.

n(cid:88)

i=1

d

d

a1  . . .  ± 1√

It is known that for n (cid:28) d2  the global maxima of f is close to one of ± 1√
an. Previ-
ously  Ge et al. [GHJY15] show that for the orthogonal case where n ≤ d and all the ai’s are orthogo-
nal  objective function f (·) have only 2n local maxima that are approximately ± 1√
an.
However  the technique heavily uses the orthogonality of the components and is not generalizable to
over-complete case.
Empirically  projected gradient ascent and power methods ﬁnd one of the components ai’s even if n is
signiﬁcantly larger than d. The local geometry for the over-complete case around the true components
is known: in a small neighborhood of each of ± 1√
ai’s  there is a unique local maximum [AGJ15].
Algebraic geometry techniques [CS13  ASS15] can show that f (·) has an exponential number of
other critical points  while these techniques seem difﬁcult to extend to the characterization of local
maxima. It remains a major open question whether there are any other spurious local maxima that
gradient ascent can potentially converge to.

a1  . . .  ± 1√

d

d

d

Main results. We show that there are no spurious local maxima in a large superlevel set that
contains all the points with function values slightly larger than that of the random initialization.

Theorem 1.1. Let ε  ζ ∈ (0  1/3) be two arbitrary constants and d be sufﬁciently large. Suppose
d1+ε < n < d2−ε. Then  with high probability over the randomness of ai’s  we have that in the
superlevel set

there are exactly 2n local maxima with function values (1 ± o(1))d2  each of which is (cid:101)O((cid:112)n/d3)-

L =(cid:8)x ∈ Sd−1 : f (x) ≥ 3(1 + ζ)n(cid:9)  

(1.2)

close to one of ± 1√

a1  . . .  ± 1√

d

an.

d

d

Previously  the best known result [AGJ15] only characterizes the geometry in small neighborhoods
around the true components  that is  there exists one local maximum in each of the small constant
neighborhoods around each of the true components ai’s. (It turns out in such neighborhoods  the
objective function is actually convex.) We signiﬁcantly enlarge this region to the superlevel set L  on
which the function f is not convex and has an exponential number of saddle points  but still doesn’t
have any spurious local maximum.
Note that a random initialization z on the unit sphere has expected function value E[f (z)] = 3n.
Therefore the superlevel set L contains all points that have function values barely larger than that
of the random guess. Hence  Theorem 1.1 implies that with a slightly better initialization than the
random guess  gradient ascent and power method1 are guaranteed to ﬁnd one of the components in
polynomial time. (It is known that after ﬁnding one component  it can be peeled off from the tensor
and the same algorithm can be repeated to ﬁnd all other components.)
Corollary 1.2. In the setting of Theorem 1.1  with high probability over the choice of ai’s  we have
that given any starting point x0 that satisﬁes f (x0) ≥ 3(1 + ζ)n  stochastic projected gradient
descent2 will ﬁnd one of the ± 1√

ai’s up to (cid:101)O((cid:112)n/d3) Euclidean error in polynomial time.

still holds with ζ = O((cid:112)d/n) that is smaller than a constant. Note that the expected value of a

We also strengthen Theorem 1.1 and Corollary 1.2 (see Theorem 3.1) slightly – the same conclusion

random initialization is 3n and we only require an initialization that is slightly better than random
guess in function value. We remark that a uniformly random point x on the unit sphere are not in
the set L with high probability. It’s an intriguing open question to characterize the landscape in the
complement of the set L.
We also conjecture that from random initialization  it sufﬁces to use constant number of projected

gradient descent (with optimal step size) to achieve the function value 3(1 + ζ)n with ζ = O((cid:112)d/n).

dn for a universal constant c).

This conjecture — an interesting question for future work — is based on the hypothesis that the ﬁrst
constant number of steps of gradient descent can make similar improvements as the ﬁrst step does
(which is equal to c
As a comparison  previous works such as [AGJ15] require an initialization with function value
Θ(d2) (cid:29) n. Anandkumar et al. [AGJ16] analyze the dynamics of tensor power method with a
delicate initialization that is independent with the randomness of the tensor. Thus it is not suitable for
the situation where the initialization comes from the result of another algorithm  and it does not have
a direct implication on the landscape of f (·).
We note that the local maximum of f (·) corresponds to the robust eigenvector of the tensor. Using
this language  our theorem says that a robust eigenvector of an over-complete tensor with random
components is either one of those true components or has a small correlation with the tensor in
the sense that (cid:104)T  x⊗4(cid:105) is small. This improves signiﬁcantly upon the understanding of robust
eigenvectors [ASS15] under an interesting random model.
The condition n > d1+ε should be artiﬁcial. The under-complete case (n < d) can be proved
by re-using the proof of [GHJY15] with the observation that local optima are preserved by linear
transformation. The intermediate regime when d < n < d1+ε should be analyzable by Kac-Rice
formula using similar techniques  but our current proof cannot capture it directly. Since the proof in
this paper is already involved  we leave this case to future work. The condition n < d2−ε matches
the best over-completeness level that existing polynomial algorithm can handle [DLCC07  MSS16].

√

1Power method is exactly equivalent to gradient ascent with a properly chosen ﬁnite learning rate
2We note that by stochastic gradient descent we meant the algorithm that is analyzed in [GHJY15]. To get
a global maximum in polynomial time (polynomial in log(1/ε) to get ε precision)  one also needs to slightly
modify stochastic gradient descent in the following way: run SGD until 1/d accuracy and then switch to gradient
descent. Since the problem is locally strongly convex  the local convergence is linear.

Our techniques The proof of Theorem 1.1 uses Kac-Rice formula (see  e.g.  [AT09])  which is
based on a counting argument. To build up the intuition  we tentatively view the unit sphere as a
collection of discrete points  then for each point x one can compute the probability (with respect to
the randomness of the function) that x is a local maximum. Adding up all these probabilities will
give us the expected number of local maxima. In continuous space  such counting argument has
to be more delicate since the local geometry needs to be taken into account. This is formalized by
Kac-Rice formula (see Lemma 2.2).
However  Kac-Rice formula only gives a closed form expression that involves the integration of
the expectation of some complicated random variable. It’s often very challenging to simplify the
expression to obtain interpretable results. Before our work  Aufﬁnger et al. [AA ˇC13  AA+13] have
successfully applied Kac-Rice formula to characterize the landscape of polynomials with random
Gaussian coefﬁcients. The exact expectation of the number of local minima can be computed there 
because the Hessian of a random polynomial is a Gaussian orthogonal ensemble  whose eigenvalue
distribution is well-understood with closed form expression.
Our technical contribution here is successfully applying Kac-Rice formula to structured random
non-convex functions where the formula cannot be exactly evaluated. The Hessian and gradients of
f (·) have much more complicated distributions compared to the Gaussian orthogonal ensemble. As a
result  the Kac-Rice formula is difﬁcult to be evaluated exactly. We instead cut the space Rd into
regions and use different techniques to estimate the number of local maxima. See a proof overview in
Section 3. We believe our techniques can be extended to 3rd order tensors and can shed light on the
analysis of other non-convex problems with structured randomness.
Organization In Section 2 we introduce preliminaries regarding manifold optimization and Kac-Rice
formula. We give a detailed explanation of our proof strategy in Section 3. The technical details are
deferred to the supplementary material. We also note that the supplementary material contains an
extended version of the preliminary and proof overview section below.

2 Notations and Preliminaries
We use Idd to denote the identity matrix of dimension d × d. Let (cid:107) · (cid:107) denote the spectral norm of a
matrix or the Euclidean norm of a vector. Let (cid:107)·(cid:107)F denote the Frobenius norm of a matrix or a tensor.
Gradient  Hessian  and local maxima on manifold We have a constrained optimization problem
over the unit sphere Sd−1  which is a smooth manifold. Thus we deﬁne the local maxima with respect
to the manifold. It’s known that projected gradient descent for Sd−1 behaves pretty much the same
on the manifold as in the usual unconstrained setting [BAC16]. In supplementary material we give a
brief introduction to manifold optimization  and the deﬁnition of gradient and Hessian. We refer the
readers to the book [AMS07] for more backgrounds.
Here we use grad f and Hess f to denote the gradient and the Hessian of f on the manifold Sd−1.
(cid:80)n
We compute them in the following claim.
i=1(cid:104)ai  x(cid:105)4. Let Px = Idd − xx(cid:62). Then the gradient
Claim 2.1. Let f : Sd−1 → R be f (x) := 1
n(cid:88)
and Hessian of f on the sphere can be written as 
(cid:104)ai  x(cid:105)3ai   Hess f (x) = 3

(cid:33)
Let Mf be the set of all local maxima  i.e. Mf =(cid:8)x ∈ Sd−1 : grad f (x) = 0  Hess f (x) (cid:22) 0(cid:9).

A local maximum of a function f on the manifold Sd−1 satisﬁes grad f (x) = 0  and Hess f (x) (cid:22) 0.

(cid:104)ai  x(cid:105)2Pxaia(cid:62)

i Px −

grad f (x) = Px

(cid:32) n(cid:88)

i=1

(cid:104)ai  x(cid:105)4

Px  

n(cid:88)

i=1

4

i=1

Kac-Rice formula Kac-Rice formula is a general tool for computing the expected number of
special points on a manifold. Suppose there are two random functions P (·) : Rd → Rd and
Q(·) : Rd → Rk  and an open set B in Rk. The formula counts the expected number of point x ∈ Rd
that satisﬁes both P (x) = 0 and Q(x) ∈ B.
Suppose we take P = ∇f and Q = ∇2f  and let B be the set of negative semideﬁnite matrices  then
the set of points that satisﬁes P (x) = 0 and Q ∈ B is the set of all local maxima Mf . Moreover 
for any set Z ⊂ Sd−1  we can also augment Q by Q = [∇2f  x] and choose B = {A : A (cid:22) 0} ⊗ Z.

With this choice of P  Q  Kac-Rice formula can count the number of local maxima inside the region
Z. For simplicity  we will only introduce Kac-Rice formula for this setting. We refer the readers
to [AT09  Chapter 11&12] for more backgrounds.
Lemma 2.2 (Informally stated). Let f be a random function deﬁned on the unit sphere Sd−1 and let
Z ⊂ Sd−1. Under certain regularity conditions3 on f and Z  we have

E [|Mf ∩ Z|] =

E [| det(Hess f )| · 1(Hess f (cid:22) 0)1(x ∈ Z) | grad f (x) = 0] pgrad f (x)(0)dx .

(2.1)

x

where dx is the usual surface measure on Sd−1 and pgrad f (x)(0) is the density of grad f (x) at 0.

(cid:90)

n(cid:88)

n(cid:88)

Formula for the number of local maxima In this subsection  we give a concrete formula for
the number of local maxima of our objective function (1.1) inside the superlevel set L (deﬁned
in equation (1.2)). Taking Z = L in Lemma 2.2  it boils down to estimating the quantity on
the right hand side of (2.1). We remark that for the particular function f as deﬁned in (1.1) and
Z = L  the integrand in (2.1) doesn’t depend on the choice of x. This is because for any x ∈ Sd−1 
(Hess f  grad f  1(x ∈ L)) has the same joint distribution  as characterized below:
Lemma 2.3. Let f be the random function deﬁned in (1.1). Let α1  . . .   αn ∈ N (0  1)  and
b1  . . .   bn ∼ N (0  Idd−1) be independent Gaussian random variables. Let

M = (cid:107)α(cid:107)4

4 · Idd−1 − 3

i bib(cid:62)
α2

i

and g =

α3

i bi

(2.2)

Then  we have that for any x ∈ Sd−1  (Hess f  grad f  f ) has the same joint distribution as
(−M  g (cid:107)α(cid:107)4
4).

i=1

i=1

Using Lemma 2.2 (with Z = L) and Lemma 2.3  we derive the following formula for the expectation
of our random variable E [|Mf ∩ L|]. Later we will later use Lemma 2.2 slightly differently with
another choice of Z.
(cid:105)
Lemma 2.4. Using the notation of Lemma 2.3  let pg(·) denote the density of g. Then 
4 ≥ 3(1 + ζ)n) | g = 0

E [|Mf ∩ L|] = Vol(Sd−1) · E(cid:104)|det(M )| 1(M (cid:23) 0)1((cid:107)α(cid:107)4

pg(0) . (2.3)

3 Proof Overview

In this section  we give a high-level overview of the proof of the main Theorem. We will prove a
slightly stronger version of Theorem 1.1.
Let γ be a universal constant that is to be determined later. Deﬁne the set L1 ⊂ Sd−1 as 

Indeed we see that L (deﬁned in (1.2)) is a subset of L1 when n (cid:29) d. We prove that in L1 there are
exactly 2n local maxima.
Theorem 3.1 (main). There exists universal constants γ  β such that the following holds: suppose
d2/ logO(1) ≥ n ≥ βd log2 d and L1 be deﬁned as in (3.1)  then with high probability over the
choice of a1  . . .   an  we have that the number of local maxima in L1 is exactly 2n:

Moreover  each of the local maximum in L1 is (cid:101)O((cid:112)n/d3)-close to one of ± 1√

|Mf ∩ L1| = 2n .

(3.2)

a1  . . .  ± 1√

d

an.

d

In order to count the number of local maxima in L1  we use the Kac-Rice formula (Lemma 2.4).
Recall that what Kac-Rice formula gives an expression that involves the complicated expectation

3We omit the long list of regularity conditions here for simplicity. See more details at [AT09  Theorem

12.1.1]

(cid:40)

n(cid:88)

i=1

(cid:41)

√

L1 :=

x ∈ Sd−1 :

(cid:104)ai  x(cid:105)4 ≥ 3n + γ

nd

.

(3.1)

E(cid:104)|det(M )| 1(M (cid:23) 0)1((cid:107)α(cid:107)4

(cid:105)
4 ≥ 3(1 + ζ)n) | g = 0

. Here the difﬁculty is to deal with the deter-
minant of a random matrix M (deﬁned in Lemma 2.3)  whose eigenvalue distribution does not admit
an analytical form. Moreover  due to the existence of the conditioning and the indicator functions 
it’s almost impossible to compute the RHS of the Kac-Rice formula (equation (2.3)) exactly.
Local vs. global analysis: The key idea to proceed is to divide the superlevel set L1 into two subsets

L1 = (L1 ∩ L2) ∪ Lc
2 

d

a1  . . .   1√

where L2 := {x ∈ Sd−1 : ∀i (cid:107)Pxai(cid:107)2 ≥ (1 − δ)d  and |(cid:104)ai  x(cid:105)|2 ≤ δd} .

(3.3)
2 ⊂ L1
Here δ is a sufﬁciently small universal constant that is to be chosen later. We also note that Lc
and hence L1 = (L1 ∩ L2) ∪ Lc
2.
Intuitively  the set L1 ∩ L2 contains those points that do not have large correlation with any of
the ai’s; the compliment Lc
2 is the union of the neighborhoods around each of the desired vector
an. We will refer to the ﬁrst subset L1 ∩ L2 as the global region  and refer to the Lc
1√
2
d
as the local region.
We will compute the number of local maxima in sets L1 ∩ L2 and Lc
2 separately using different
techniques. We will show that with high probability L1 ∩ L2 contains no local maxima using Kac-
Rice formula (see Theorem 3.2). Then  we show that Lc
2 contains exactly 2n local maxima (see
Theorem 3.3) using a different and more direct approach.
Global analysis. The key beneﬁt of have such division to local and global regions is that for the
global region  we can avoid evaluating the value of the RHS of the Kac-Rice formula. Instead  we only
need to have an estimate: Note that the number of local optima in L1 ∩ L2  namely |Mf ∩ L1 ∩ L2| 
is an integer nonnegative random variable. Thus  if we can show its expectation E [|Mf ∩ L1 ∩ L2|]
is much smaller than 1  then Markov’s inequality implies that with high probability  the number of
local maxima will be exactly zero. Concretely  we will use Lemma 2.2 with Z = L1 ∩ L2  and then
estimate the resulting integral using various techniques in random matrix theory. It remains quite
challenging even if we are only shooting for an estimate. Concretely  we get the following Theorem
Theorem 3.2. Let sets L1  L2 be deﬁned as in equation (3.3) and n ≥ βd log2 d. There exists
universal small constant δ ∈ (0  1) and universal constants γ  β  and a high probability event G0 
such that the expected number of local maxima in L1 ∩ L2 conditioned on G0 is exponentially small:

E(cid:2)|Mf ∩ L1 ∩ L2|(cid:12)(cid:12) G0

(cid:3) ≤ 2−d/2 .

See Section 3.1 for an overview of the analysis. The purpose and deﬁnition of G0 are more technical
and can be found in Section 3 of the supplementary material around equation (3.3) (3 4) and (3.5).
We also prove that G0 is indeed a high probability event in supplementary material. 4
Local analysis. In the local region Lc
2  that is  the neighborhoods of a1  . . .   an  we will show
there are exactly 2n local maxima. As argued above  it’s almost impossible to get exact numbers
out of the Kac-Rice formula since it’s often hard to compute the complicated integral. Moreover 
Kac-Rice formula only gives the expected number but not high probability bounds. However  here the
observation is that the local maxima (and critical points) in the local region are well-structured. Thus 
instead  we show that in these local regions  the gradient and Hessian of a point x are dominated by
the terms corresponding to components {ai}’s that are highly correlated with x. The number of such
terms cannot be very large (by restricted isometry property  see Section B.5 of the supplementary
material). As a result  we can characterize the possible local maxima explicitly  and eventually show
there is exactly one local maximum in each of the local neighborhoods around {± 1√
ai}’s. Similar
(but weaker) analysis was done before in [AGJ15]. We formalize the guarantee for local regions in
the following theorem  which is proved in Section 5 of the supplementary material. In Section 3.2 of
the supplementary material  we also discuss the key ideas of the proof of this Theorem.
Theorem 3.3. Suppose 1/δ2 · d log d ≤ n ≤ d2/ logO(1) d. Then  with high probability over the
choice a1  . . .   an  we have 

d

Moreover  each of the point in L ∩ Lc

|Mf ∩ L1 ∩ Lc

2 is (cid:101)O((cid:112)n/d3)-close to one of ± 1√

2| = 2n .

(3.4)

a1  . . .  ± 1√

d

an.

d

4We note again that the supplementary material contains more details in each section even for sections in the

main text.

The main Theorem 3.1 is a direct consequence of Theorem 3.2 and Theorem 3.3. The formal proof
can be found in Section 3 of the supplementary material.
In the next subsections we sketch the basic ideas behind the proof of Theorem 3.2 and Theorem 3.3.
Theorem 3.2 is the crux of the technical part of the paper.

3.1 Estimating the Kac-Rice formula for the global region

The general plan to prove Theorem 3.2 is to use random matrix theory to estimate the RHS of the
Kac-Rice formula. We begin by applying Kac-Rice formula to our situation. We note that we dropped
the effect of G0 in all of the following discussions since G0 only affects some technicality that
appears in the details of the proof in the supplementary material.
Applying Kac-Rice formula.
The ﬁrst step to apply Kac-Rice formula is to characterize the
joint distribution of the gradient and the Hessian. We use the notation of Lemma 2.3 for ex-
pressing the joint distribution of (Hess f  grad f  1(x ∈ L1 ∩ L2)). For any ﬁx x ∈ Sd−1 
4 · Idd−1 −
let αi = (cid:104)ai  x(cid:105) and bi = Pxai (where Px = Id − xx(cid:62)) and M = (cid:107)α(cid:107)4
In order to apply Kac-Rice for-
mula  we’d like to compute the joint distribution of the gradient and the Hessian. We have that
(Hess f  grad f  1(x ∈ L1 ∩ L2)) has the same distribution as (M  g  1(E1 ∩ E2 ∩ E(cid:48)
2)) where E1
corresponds to the event that x ∈ L1 

and g = (cid:80)n

i bi as deﬁned in (2.2).

3(cid:80)n

i bib(cid:62)

i=1 α2

i=1 α3

i

(cid:111)

√

4 ≥ 3n + γ

nd

 

(cid:110)(cid:107)α(cid:107)4
E2 =(cid:8)(cid:107)α(cid:107)2∞ ≤ δd(cid:9)   and E(cid:48)

E1 =

and events E2 and E(cid:48)
and E(cid:48)

2 depends the randomness of αi’s and bi’s respectively.

2 correspond to the events that x ∈ L2. We separate them out to reﬂect that E2

2 =(cid:8)∀i ∈ [n] (cid:107)bi(cid:107)2 ≥ (1 − δ)d(cid:9) .

Using Kac-Rice formula (Lemma 2.2 with Z = L1 ∩ L2)  we conclude that

E [|Mf ∩ L1 ∩ L2|] = Vol(Sd−1) · E [|det(M )| 1(M (cid:23) 0)1(E1 ∩ E2 ∩ E(cid:48)

2) | g = 0] pg(0) .

(3.5)

Next  towards proving Theorem 3.2 we will estimate the RHS of (3.5) using various techniques.
Conditioning on α. We observe that the distributions of the gradient g and Hessian M on the RHS
of equation 3.5 are fairly complicated. In particular  we need to deal with the interactions of αi’s
(the components along x) and bi’s (the components in the orthogonal subspace of x). Therefore  we
use the law of total expectation to ﬁrst condition on α and take expectation over the randomness of
bi’s  and then take expectation over αi’s. Let pg|α denotes the density of g | α  using the law of total
expectation  we have 

E [|det(M )| 1(M (cid:23) 0)1(E1 ∩ E2 ∩ E(cid:48)

= E(cid:2)E [|det(M )| 1(M (cid:23) 0)1(E(cid:48)

2) | g = 0] pg(0)

2) | g = 0  α] 1(E1)1(E2)pg|α(0)(cid:3) .

(3.6)

Note that the inner expectation of RHS of (3.6) is with respect to the randomness of bi’s and the outer
one is with respect to αi’s.
For notional convenience we deﬁne h(·) : Rn → R as
h(α) := Vol(Sd−1) E [det(M )1(M (cid:23) 0)1(E(cid:48)

2) | g = 0  α] 1(E1)1(E2)pg|α(0) .

Then  using the Kac-Rice formula (equation (2.3))5 and equation (3.5)  we obtain the following
explicit formula for the number of local maxima in L1 ∩ L2.

(3.7)
We note that pg|α(0) has an explicit expression since g | α is Gaussian. For the ease of exposition 
we separate out the hard-to-estimate part from h(α)  which we call W (α):

E [|Mf ∩ L1 ∩ L2|] = E [h(α)] .

W (α) := E [det(M )1(M (cid:23) 0)1(E(cid:48)

2) | g = 0  α] 1(E1)1(E2) .

(3.8)

5In Section C of the supplementary material  we rigorously verify the regularity condition of Kac-Rice

formula.

4 − 3(cid:80) α2

Therefore by deﬁnition  we have that h(α) = Vol(Sd−1)W (α)pg|α(0). Now  since we have condi-
tioned on α  the distributions of the Hessian  namely M | α  is a generalized Wishart matrix which is
slightly easier than before. However there are still several challenges that we need to address in order
to estimate W (α).
How to control det(M )1(M (cid:23) 0)? Recall that M = (cid:107)α(cid:107)4
i   which is a generalized
Wishart matrix whose eigenvalue distribution has no (known) analytical expression. The determinant
itself by deﬁnition is a high-degree polynomial over the entries  and in our case  a complicated
polynomial over the random variables αi’s and vectors bi’s. We also need to properly exploit the
presence of the indicator function 1(M (cid:23) 0)  since otherwise  the desired statement will not be true –
the function f has an exponential number of critical points.
Fortunately  in most of the cases  we can use the following simple claim that bounds the determinant
from above by the trace. The inequality is close to being tight when all the eigenvalues of M are
similar to each other. More importantly  it uses naturally the indicator function 1(M (cid:23) 0)! Later we
will see how to strengthen it when it’s far from tight.
Claim 3.4. We have that

i bib(cid:62)

(cid:18)|tr(M )|

(cid:19)d−1

det(M )1(M (cid:23) 0) ≤

d − 1

1(M (cid:23) 0)

The claim is a direct consequence of AM-GM inequality on the eigenvalue of M. (Note that M is of
dimension (d − 1) × (d − 1). we give a formal proof in Section 3.1 of the supplementary material).
It follows that

(cid:20)|tr(M )|d−1
(d − 1)d−1 | g = 0  α

(cid:21)

W (α) ≤ E

1(E1) .

(3.9)

(cid:1) .

Therefore using equation (3.9) and equation above  we have that

Here we dropped the indicators for events E2 and E(cid:48)
2 since they are not important for the discussion
below. It turns out that |tr(M )| is a random variable that concentrates very well  and thus we have

E(cid:2)|tr(M )|d−1(cid:3) ≈ | E [tr(M )]|d−1. It can be shown that (see Proposition 4.3 in the supplementary

material for the detailed calculation) 

E [tr(M ) | g = 0  α] = (d − 1)(cid:0)(cid:107)α(cid:107)4
W (α) ≤(cid:0)(cid:107)α(cid:107)4
E [h(α)] ≤ Vol(Sd−1) E(cid:104)(cid:0)(cid:107)α(cid:107)4

4 − 3(cid:107)α(cid:107)2 + 3(cid:107)α(cid:107)8

8/(cid:107)α(cid:107)6

6

4 − 3(cid:107)α(cid:107)2 + 3(cid:107)α(cid:107)8

8/(cid:107)α(cid:107)6

6

1(E0)1(E1) .

(cid:1)d−1
(cid:1)d−1 · (2π)−d/2((cid:107)α(cid:107)6

.

6

8/(cid:107)α(cid:107)6

Note that since g | α has Gaussian distribution  we have  pg|α(0) = (2π)−d/2((cid:107)α(cid:107)6
using two equations above  we can bound E [h(α)] by
4 − 3(cid:107)α(cid:107)2 + 3(cid:107)α(cid:107)8

6)−d/2 . Thus
(cid:105)
6)−d/21(E0)1(E1)
(3.10)
Therefore  it sufﬁces to control the RHS of (3.10)  which is much easier than the original Kac-Rice
formula. However  it turns out that RHS of (3.10) is roughly cd for some constant c > 1! Roughly
speaking  this is because the high powers of a random variables is very sensitive to its tail.
Two sub-cases according to max|αi|. We aim to ﬁnd a tighter bond of E[h(α)] by re-using the idea
in equation (3.10). Intuitively we can consider two separate situations events: the event F0 when all
of the αi’s are close to constant and the complementary event F c
0 . Formally  let τ = Kn/d where K

is a universal constant that will be determined later. Let F0 be the event that .F0 =(cid:8)(cid:107)α(cid:107)4∞ ≤ τ(cid:9).

Then we control E [h(α)1(F0)] and E [h(α)1(F c
0 )] separately. For the former  we basically need to
reuse the equation (3.10) with an indicator function inserted inside the expectation. For the latter 
we make use of the large coordinate  which contributes to the −3α2
term in M and makes the
probability of 1(M (cid:23) 0) extremely small. As a result det(M )1(M (cid:23) 0) is almost always 0. We
formalized the two cases as below:
Proposition 3.5. Let K ≥ 2 · 103 be a universal constant. Let τ = Kn/d and let γ  β be sufﬁciently
large constants (depending on K). Then for any n ≥ βd log2 d  we have that

i bib(cid:62)

i

E [h(α)1(F0)] ≤ (0.3)d/2 .

Proposition 3.6. In the setting of Proposition 3.5  we have

E [h(α)1(F c

0 )] ≤ n · (0.3)d/2 .

We see that Theorem 3.2 can be obtained as a direct consequence of Proposition 3.5  Proposition 3.6
and equation (3.7).
Due to space limit  we refer the readers to the supplementary material for an extended version of
proof overview and the full proofs.

4 Conclusion

We analyze the optimization landscape of the random over-complete tensor decomposition problem
using the Kac-Rice formula and random matrix theory. We show that in the superlevel set L that
contains all the points with function values barely larger than the random guess  there are exactly 2n
local maxima that correspond to the true components. This implies that with an initialization slight
better than the random guess  local search algorithms converge to the desired solutions. We believe
our techniques can be extended to 3rd order tensors  or other non-convex problems with structured
randomness.
The immediate open question is whether there is any other spurious local maximum outside this
superlevel set. Answering it seems to involve solving difﬁcult questions in random matrix theory.
Another potential approach to unravel the mystery behind the success of the non-convex methods is
to analyze the early stage of local search algorithms and show that they will enter the superlevel set L
quickly from a good initialization.

References

[AA+13] Antonio Aufﬁnger  Gerard Ben Arous  et al. Complexity of random smooth functions on the

high-dimensional sphere. The Annals of Probability  41(6):4214–4247  2013.

[AA ˇC13] Antonio Aufﬁnger  Gérard Ben Arous  and Jiˇrí ˇCern`y. Random matrices and complexity of spin

glasses. Communications on Pure and Applied Mathematics  66(2):165–201  2013.

[AFH+12] Anima Anandkumar  Dean P. Foster  Daniel Hsu  Sham M. Kakade  and Yi-Kai Liu. A spectral
algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems
25  2012.

[AGJ15] Animashree Anandkumar  Rong Ge  and Majid Janzamin. Learning overcomplete latent variable
models through tensor methods. In Proceedings of the Conference on Learning Theory (COLT) 
Paris  France  2015.

[AGJ16] Anima Anandkumar  Rong Ge  and Majid Janzamin. Analyzing tensor power method dynamics in

overcomplete regime. JMLR  2016.

[AGMM15] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  efﬁcient and neural algorithms

for sparse coding. In Proceedings of The 28th Conference on Learning Theory  2015.

[AHK12] Anima Anandkumar  Daniel Hsu  and Sham M. Kakade. A method of moments for mixture models

and hidden Markov models. In COLT  2012.

[AMS07] P.A. Absil  R. Mahony  and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton

University Press  2007.

[ASS15] H. Abo  A. Seigal  and B. Sturmfels. Eigenconﬁgurations of Tensors. ArXiv e-prints  May 2015.

[AT09] Robert J Adler and Jonathan E Taylor. Random ﬁelds and geometry. Springer Science & Business

Media  2009.

[BAC16] N. Boumal  P.-A. Absil  and C. Cartis. Global rates of convergence for nonconvex optimization on

manifolds. ArXiv e-prints  May 2016.

[BBV16] Afonso S Bandeira  Nicolas Boumal  and Vladislav Voroninski. On the low-rank approach
for semideﬁnite programs arising in synchronization and community detection. arXiv preprint
arXiv:1602.04426  2016.

[BCMV14] Aditya Bhaskara  Moses Charikar  Ankur Moitra  and Aravindan Vijayaraghavan. Smoothed
analysis of tensor decompositions. In Proceedings of the 46th Annual ACM Symposium on Theory
of Computing  pages 594–603. ACM  2014.

[BKS15] Boaz Barak  Jonathan A. Kelner  and David Steurer. Dictionary learning and tensor decomposition
via the sum-of-squares method. In Proceedings of the Forty-Seventh Annual ACM on Symposium
on Theory of Computing  STOC 2015  Portland  OR  USA  June 14-17  2015  pages 143–151  2015.

[BNS16] Srinadh Bhojanapalli  Behnam Neyshabur  and Nathan Srebro. Global optimality of local search

for low rank matrix recovery. arXiv preprint arXiv:1605.07221  2016.

[Cha96] Joseph T. Chang. Full reconstruction of Markov models on evolutionary trees: Identiﬁability and

consistency. Mathematical Biosciences  137:51–73  1996.

[CHM+15] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun. The

loss surfaces of multilayer networks. In AISTATS  2015.

[CLA09] P. Comon  X. Luciani  and A. De Almeida. Tensor decompositions  alternating least squares and

other tales. Journal of Chemometrics  23(7-8):393–405  2009.

[CS13] Dustin Cartwright and Bernd Sturmfels. The number of eigenvalues of a tensor. Linear algebra

and its applications  438(2):942–952  2013.

[CS16] Nadav Cohen and Amnon Shashua. Convolutional rectiﬁer networks as generalized tensor decom-

positions. CoRR  abs/1603.00162  2016.

[DLCC07] L. De Lathauwer  J. Castaing  and J.-F. Cardoso. Fourth-order cumulant-based blind identiﬁcation
of underdetermined mixtures. Signal Processing  IEEE Transactions on  55(6):2965–2973  2007.

[DPG+14] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems  pages 2933–2941  2014.

[GHJY15] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory 
pages 797–842  2015.

[GLM16] Rong Ge  Jason D Lee  and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv

preprint arXiv:1605.07272  2016.

[GLM17] R. Ge  J. D. Lee  and T. Ma. Learning One-hidden-layer Neural Networks with Landscape Design.

ArXiv e-prints  November 2017.

[GM15] Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares

algorithms. arXiv preprint arXiv:1504.05287  2015.

[GVX13] N. Goyal  S. Vempala  and Y. Xiao. Fourier pca. arXiv preprint arXiv:1306.5825  2013.

[Hås90] Johan Håstad. Tensor rank is np-complete. Journal of Algorithms  11(4):644–654  1990.

[HK13] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment methods

and spectral decompositions. In Fourth Innovations in Theoretical Computer Science  2013.

[HKZ12] Daniel Hsu  Sham M. Kakade  and Tong Zhang. A spectral algorithm for learning hidden Markov

models. Journal of Computer and System Sciences  78(5):1460–1480  2012.

[HL13] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM

(JACM)  60(6):45  2013.

[HM16] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. CoRR  abs/1611.04231  2016.

[HMR16] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynamical systems.

CoRR  abs/1609.05191  2016.

[HSSS16] Samuel B. Hopkins  Tselil Schramm  Jonathan Shi  and David Steurer. Fast spectral algorithms
from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In Proceedings of
the 48th Annual ACM SIGACT Symposium on Theory of Computing  STOC 2016  Cambridge  MA 
USA  June 18-21  2016  pages 178–191  2016.

[JSA15] M. Janzamin  H. Sedghi  and A. Anandkumar. Beating the Perils of Non-Convexity: Guaranteed

Training of Neural Networks using Tensor Methods. ArXiv e-prints  June 2015.

[Kaw16] K. Kawaguchi. Deep Learning without Poor Local Minima. ArXiv e-prints  May 2016.

[KM11] Tamara G Kolda and Jackson R Mayo. Shifted power method for computing tensor eigenpairs.

SIAM Journal on Matrix Analysis and Applications  32(4):1095–1124  2011.

[LSJR16] Jason D. Lee  Max Simchowitz  Michael I. Jordan  and Benjamin Recht. Gradient descent only
converges to minimizers. In Proceedings of the 29th Conference on Learning Theory  COLT 2016 
New York  USA  June 23-26  2016  pages 1246–1257  2016.

[MR06] Elchanan Mossel and Sébastian Roch. Learning nonsingular phylogenies and hidden Markov

models. Annals of Applied Probability  16(2):583–614  2006.

[MSS16] Tengyu Ma  Jonathan Shi  and David Steurer. Polynomial-time tensor decompositions with

sum-of-squares. In FOCS 2016  to appear  2016.

[NP06] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global perfor-

mance. Mathematical Programming  108(1):177–205  2006.

[NPOV15] A. Novikov  D. Podoprikhin  A. Osokin  and D. Vetrov. Tensorizing Neural Networks. ArXiv

e-prints  September 2015.

[SQW15] Ju Sun  Qing Qu  and John Wright. When are nonconvex problems not scary? arXiv preprint

arXiv:1510.06096  2015.

,Rong Ge
Tengyu Ma