2017,Selective Classification for Deep Neural Networks,Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time  the classifier rejects instances as needed  to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method  which opens up possibilities to operate DNNs in mission-critical applications. For example  using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%  with almost 60% test coverage.,Selective Classiﬁcation for Deep Neural Networks

Yonatan Geifman

Computer Science Department

Technion – Israel Institute of Technology

yonatan.g@cs.technion.ac.il

Ran El-Yaniv

Computer Science Department

Technion – Israel Institute of Technology

rani@cs.technion.ac.il

Abstract

Selective classiﬁcation techniques (also known as reject option) have not yet been
considered in the context of deep neural networks (DNNs). These techniques
can potentially signiﬁcantly improve DNNs prediction performance by trading-off
coverage. In this paper we propose a method to construct a selective classiﬁer
given a trained neural network. Our method allows a user to set a desired risk
level. At test time  the classiﬁer rejects instances as needed  to grant the desired
risk (with high probability). Empirical results over CIFAR and ImageNet con-
vincingly demonstrate the viability of our method  which opens up possibilities to
operate DNNs in mission-critical applications. For example  using our method an
unprecedented 2% error in top-5 ImageNet classiﬁcation can be guaranteed with
probability 99.9%  and almost 60% test coverage.

1

Introduction

While self-awareness remains an illusive  hard to deﬁne concept  a rudimentary kind of self-awareness 
which is much easier to grasp  is the ability to know what you don’t know  which can make you
smarter. The subﬁeld dealing with such capabilities in machine learning is called selective prediction
(also known as prediction with a reject option)  which has been around for 60 years [1  5]. The main
motivation for selective prediction is to reduce the error rate by abstaining from prediction when in
doubt  while keeping coverage as high as possible. An ultimate manifestation of selective prediction
is a classiﬁer equipped with a “dial” that allows for precise control of the desired true error rate
(which should be guaranteed with high probability)  while keeping the coverage of the classiﬁer as
high as possible.
Many present and future tasks performed by (deep) predictive models can be dramatically enhanced
by high quality selective prediction. Consider  for example  autonomous driving. Since we cannot
rely on the advent of “singularity”  where AI is superhuman  we must manage with standard machine
learning  which sometimes errs. But what if our deep autonomous driving network were capable of
knowing that it doesn’t know how to respond in a certain situation  disengaging itself in advance and
alerting the human driver (hopefully not sleeping at that time) to take over? There are plenty of other
mission-critical applications that would likewise greatly beneﬁt from effective selective prediction.
The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for
various hypothesis classes and learning algorithms  such as SVM  boosting  and nearest-neighbors
[8  13  3]. The reject option has rarely been discussed in the context of neural networks (NNs)  and
so far has not been considered for deep NNs (DNNs). Existing NN works consider a cost-based
rejection model [2  4]  whereby the costs of misclassiﬁcation and abstaining must be speciﬁed  and a
rejection mechanism is optimized for these costs. The proposed mechanism for classiﬁcation is based
on applying a carefully selected threshold on the maximal neuronal response of the softmax layer.
We that call this mechanism softmax response (SR). The cost model can be very useful when we can
quantify the involved costs  but in many applications of interest meaningful costs are hard to reason.
(Imagine trying to set up appropriate rejection/misclassiﬁcation costs for disengaging an autopilot

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

driving system.) Here we consider the alternative risk-coverage view for selective classiﬁcation
discussed in [5].
Ensemble techniques have been considered for selective (and conﬁdence-rated) prediction  where
rejection mechanisms are typically based on the ensemble statistics [18  7]. However  such techniques
are presently hard to realize in the context of DNNs  for which it could be very costly to train
sufﬁciently many ensemble members. Recently  Gal and Ghahramani [9] proposed an ensemble-like
method for measuring uncertainty in DNNs  which bypasses the need to train several ensemble
members. Their method works via sampling multiple dropout applications of the forward pass to
perturb the network prediction randomly. While this Monte-Carlo dropout (MC-dropout) technique
was not mentioned in the context of selective prediction  it can be directly applied as a viable selective
prediction method using a threshold  as we discuss here.
In this paper we consider classiﬁcation tasks  and our goal is to learn a selective classiﬁer (f  g) 
where f is a standard classiﬁer and g is a rejection function. The selective classiﬁer has to allow
full guaranteed control over the true risk. The ideal method should be able to classify samples in
production with any desired level of risk with the optimal coverage rate. It is reasonable to assume
that this optimal performance can only be obtained if the pair (f  g) is trained together. As a ﬁrst step 
however  we consider a simpler setting where a (deep) neural classiﬁer f is already given  and our
goal is to learn a rejection function g that will guarantee with high probability a desired error rate.
To this end  we consider the above two known techniques for rejection (SR and MC-dropout)  and
devise a learning method that chooses an appropriate threshold that ensures the desired risk. For a
given classiﬁer f  conﬁdence level δ  and desired risk r∗  our method outputs a selective classiﬁer
(f  g) whose test error will be no larger than r∗ with probability of at least 1 − δ.
Using the well-known VGG-16 architecture  we apply our method on CIFAR-10  CIFAR-100 and
ImageNet (on ImageNet we also apply the RESNET-50 architecture). We show that both SR and
dropout lead to extremely effective selective classiﬁcation. On both the CIFAR datasets  these two
mechanisms achieve nearly identical results. However  on ImageNet  the simpler SR mechanism
is signiﬁcantly superior. More importantly  we show that almost any desirable risk level can be
guaranteed with a surprisingly high coverage. For example  an unprecedented 2% error in top-5
ImageNet classiﬁcation can be guaranteed with probability 99.9%  and almost 60% test coverage.

2 Problem Setting
We consider a standard multi-class classiﬁcation problem. Let X be some feature space (e.g.  raw
image data) and Y  a ﬁnite label set  Y = {1  2  3  . . .   k}  representing k classes. Let P (X  Y ) be
a distribution over X × Y. A classiﬁer f is a function f : X → Y  and the true risk of f w.r.t. P
is R(f|P ) =∆ EP (X Y )[(cid:96)(f (x)  y)]  where (cid:96) : Y × Y → R+ is a given loss function  for example
(cid:80)m
i=1 ⊆ (X × Y) sampled i.i.d. from P (X  Y )  the
the 0/1 error. Given a labeled set Sm = {(xi  yi)}m
empirical risk of the classiﬁer f is ˆr(f|Sm) =∆ 1
A selective classiﬁer [5] is a pair (f  g)  where f is a classiﬁer  and g : X → {0  1} is a selection
function  which serves as a binary qualiﬁer for f as follows 

i=1 (cid:96)(f (xi)  yi).

m

(cid:26) f (x) 

(f  g)(x) =∆

don’t know 

if g(x) = 1;
if g(x) = 0.

Thus  the selective classiﬁer abstains from prediction at a point x iff g(x) = 0. The performance
of a selective classiﬁer is quantiﬁed using coverage and risk. Fixing P   coverage  deﬁned to be
φ(f  g) =∆ EP [g(x)]  is the probability mass of the non-rejected region in X . The selective risk of
(f  g) is

R(f  g) =∆ EP [(cid:96)(f (x)  y)g(x)]

φ(f  g)

.

(1)

Clearly  the risk of a selective classiﬁer can be traded-off for coverage. The entire performance proﬁle
of such a classiﬁer can be speciﬁed by its risk-coverage curve  deﬁned to be risk as a function of
coverage [5].
Consider the following problem. We are given a classiﬁer f  a training sample Sm  a conﬁdence
parameter δ > 0  and a desired risk target r∗ > 0. Our goal is to use Sm to create a selection function

2

g such that the selective risk of (f  g) satisﬁes

PrSm {R(f  g) > r∗} < δ 

(2)
where the probability is over training samples  Sm  sampled i.i.d. from the unknown underlying
distribution P . Among all classiﬁers satisfying (2)  the best ones are those that maximize the coverage.
For a ﬁxed f  and a given class G (which will be discussed below)  in this paper our goal is to select
g ∈ G such that the selective risk R(f  g) satisﬁes (2) while the coverage Φ(f  g). is maximized.

3 Selection with Guaranteed Risk Control

In this section  we present a general technique for constructing a selection function with guaranteed
performance  based on a given classiﬁer f  and a conﬁdence-rate function κf : X → R+ for f.
We do not assume anything on κf   and the interpretation is that κ can rank in the sense that if
κf (x1) ≥ κf (x2)  for x1  x2 ∈ X   the conﬁdence function κf indicates that the conﬁdence in the
prediction f (x2) is not higher than the conﬁdence in the prediction f (x1). In this section we are not
concerned with the question of what is a good κf (which is discussed in Section 4); our goal is to
generate a selection function g  with guaranteed performance for a given κf .
For the reminder of this paper  the loss function (cid:96) is taken to be the standard 0/1 loss function (unless
explicitly mentioned otherwise). Let Sm = {(xi  yi)}m
i=1 ⊆ (X × Y)m be a training set  assumed
to be sampled i.i.d. from an unknown distribution P (X  Y ). Given also are a conﬁdence parameter
δ > 0  and a desired risk target r∗ > 0. Based on Sm  our goal is to learn a selection function g such
that the selective risk of the classiﬁer (f  g) satisﬁes (2).
For θ > 0  we deﬁne the selection function gθ : X → {0  1} as

gθ(x) = gθ(x|κf ) =∆

if κf (x) ≥ θ;

0  otherwise.

(3)

(cid:26) 1 
(cid:80)m

For any selective classiﬁer (f  g)  we deﬁne its empirical selective risk with respect to the labeled
sample Sm 

ˆr(f  g|Sm) =∆

1
m

i=1 (cid:96)(f (xi)  yi)g(xi)
ˆφ(f  g|Sm)

 

(cid:80)m

m

i=1 g(xi). For any selection function g 

where ˆφ is the empirical coverage  ˆφ(f  g|Sm) =∆ 1
denote by g(Sm) the g-projection of Sm  g(Sm) =∆ {(x  y) ∈ Sm : g(x) = 1}.
The selection with guaranteed risk (SGR) learning algorithm appears in Algorithm 1. The algorithm
receives as input a classiﬁer f  a conﬁdence-rate function κf   a conﬁdence parameter δ > 0  a target
risk r∗1  and a training set Sm. The algorithm performs a binary search to ﬁnd the optimal bound
guaranteeing the required risk with sufﬁcient conﬁdence. The SGR algorithm outputs a selective
classiﬁer (f  g) and a risk bound b∗. In the rest of this section we analyze the SGR algorithm. We
make use of the following lemma  which gives the tightest possible numerical generalization bound
for a single classiﬁer  based on a test over a labeled sample.

Lemma 3.1 (Gascuel and Caraux  1992  [10]) Let P be any distribution and consider a classiﬁer
f whose true error w.r.t. P is R(f|P ). Let 0 < δ < 1 be given and let ˆr(f|Sm) be the empirical
error of f w.r.t. to the labeled set Sm  sampled i.i.d. from P . Let B∗(ˆri  δ  Sm) be the solution b of
the following equation 

m·ˆr(f|Sm)(cid:88)
j
Then  PrSm{R(f|P ) > B∗(ˆri  δ  Sm)} < δ.
We emphasize that the numerical bound of Lemma 3.1 is the tightest possible in this setting. As
discussed in [10]  the analytic bounds derived using  e.g.  Hoeffding inequality (or other concentration
inequalities)  approximate this numerical bound and incur some slack.

bj(1 − b)m−j = δ.

(cid:18)m
(cid:19)

(4)

j=0

1Whenever the triplet Sm  δ and r∗ is infeasible  the algorithm will return a vacuous solution with zero

coverage.

3

that indices reﬂect this

Algorithm 1 Selection with Guaranteed Risk (SGR)
1: SGR(f κf  δ r∗ Sm)
2: Sort Sm according to κf (xi)  xi ∈ Sm (and now assume w.l.o.g.

ordering).

z = (cid:100)(zmin + zmax)/2(cid:101)
θ = κf (xz)
gi = gθ {(see (3))}
ˆri = ˆr(f  gi|Sm)
i = B∗(ˆri  δ/(cid:100)log2 m(cid:101)  gi(Sm)) {see Lemma 3.1 }
b∗
if b∗
i < r∗ then
zmax = z

3: zmin = 1; zmax = m
4: for i = 1 to k =∆ (cid:100)log2 m(cid:101) do
5:
6:
7:
8:
9:
10:
11:
12:
13:
end if
14:
15: end for
16: Output- (f  gk) and the bound b∗
k.

zmin = z

else

For any selection function  g  let Pg(X  Y ) be the projection of P over g; that is  Pg(X  Y ) =∆
P (X  Y |g(X) = 1). The following theorem is a uniform convergence result for the SGR procedure.
Theorem 3.2 (SGR) Let Sm be a given labeled set  sampled i.i.d. from P   and consider an appli-
cation of the SGR procedure. For k =∆ (cid:100)log2 m(cid:101)  let (f  gi) and b∗
i   i = 1  . . .   k  be the selective
classiﬁer and bound computed by SGR in its ith iterations. Then 

PrSm {∃i : R(f|Pgi) > B∗(ˆri  δ/k  gi(Sm))} < δ.

Proof Sketch: For any i = 1  . . .   k  let mi = |gi(Sm)| be the random variable giving the size of
accepted examples from Sm on the ith iteration of SGR. For any ﬁxed value of 0 ≤ mi ≤ m  by
Lemma 3.1  applied with the projected distribution Pgi(X  Y )  and a sample Smi  consisting of mi
examples drawn from the product distribution (Pgi)mi 

PrSmi∼(Pgi )mi {R(f|Pgi) > B∗(ˆri  δ/k  gi(Sm))} < δ/k.

(5)
The sampling distribution of mi labeled examples in SGR is determined by the following process:
sample a set Sm of m examples from the product distribution P m and then use gi to ﬁlter Sm 
resulting in a (randon) number mi of examples. Therefore  the left-hand side of (5) equals

PrSm∼P m {R(f|Pgi) > B∗(ˆri  δ/k  gi(Sm)) |gi(Sm) = mi} .

Clearly 

Therefore 

=

≤ δ
k

n=0

φ(f  g)

= R(f  gi).

[(cid:96)(f (x)  y)] =

EP [(cid:96)(f (x)  y)g(x)]

R(f|Pgi) = EPgi
m(cid:88)
PrSm{R(f  gi) > B∗(ˆri  δ/k  gi(Sm))}
PrSm{R(f  gi) > B∗(ˆri  δ/k  gi(Sm)) | gi(Sm) = n} · Pr{gi(Sm) = n}
m(cid:88)

n=0

Pr{gi(Sm) = n} =

δ
k

.

An application of the union bound completes the proof.

4 Conﬁdence-Rate Functions for Neural Networks

(cid:3)

Consider a classiﬁer f  assumed to be trained for some unknown distribution P . In this section we
consider two conﬁdence-rate functions  κf   based on previous work [9  2]. We note that an ideal

4

conﬁdence-rate function κf (x) for f  should reﬂect true loss monotonicity. Given (x1  y1) ∼ P and
(x2  y2) ∼ P   we would like the following to hold: κf (x1) ≤ κf (x2) if and only if (cid:96)(f (x1)  y1) ≥
(cid:96)(f (x2)  y2). Obviously  one cannot expect to have an ideal κf . Given a conﬁdence-rate functions
κf   a useful way to analyze its effectiveness is to draw the risk-coverage curve of its induced rejection
function  gθ(x|κf )  as deﬁned in (3). This risk-coverage curve shows the relationship between θ
and R(f  gθ). For example  see Figure 2(a) where a two (nearly identical) risk-coverage curves are
plotted. While the conﬁdence-rate functions we consider are not ideal  they will be shown empirically
to be extremely effective. 2
The ﬁrst conﬁdence-rate function we consider has been around in the NN folklore for years  and is
explicitly mentioned by [2  4] in the context of reject option. This function works as follows: given
any neural network classiﬁer f (x) where the last layer is a softmax  we denote by f (x|j) the soft
response output for the jth class. The conﬁdence-rate function is deﬁned as κ =∆ maxj∈Y (f (x|j)).
We call this function softmax response (SR).
Softmax responses are often treated as probabilities (responses are positive and sum to 1)  but some
authors criticize this approach [9]. Noting that  for our purposes  the ideal conﬁdence-rate function
should only provide coherent ranking rather than absolute probability values  softmax responses are
potentially good candidates for relative conﬁdence rates.
We are not familiar with a rigorous explanation for SR  but it can be intuitively motivated by observing
neuron activations. For example  Figure 1 depicts average response values of every neuron in the
second-to-last layer for true positives and false positives for the class ‘8’ in the MNIST dataset (and
qualitatively similar behavior occurs in all MNIST classes). The x-axis corresponds to neuron indices
in that layer (1-128); and the y-axis shows the average responses  where green squares are averages of
true positives  boldface squares highlight strong responses  and red circles correspond to the average
response of false positives. It is evident that the true positive activation response in the active neurons
is much higher than the false positive  which is expected to be reﬂected in the ﬁnal softmax layer
response. Moreover  it can be seen that the large activation values are spread over many neurons 
indicating that the conﬁdence signal arises due to numerous patterns detected by neurons in this layer.
Qualitatively similar behavior can be observed in deeper layers.

Figure 1: Average response values of neuron activations for class "8" on the MNIST dataset; green
squares  true positives  red circles  false negatives

The MC-dropout technique we consider was recently proposed to quantify uncertainty in neural
networks [9]. To estimate uncertainty for a given instance x  we run a number of feed-forward
iterations over x  each applied with dropout in the last fully connected layer. Uncertainty is taken as
the variance in the responses of the neuron corresponding to the most probable class. We consider
minus uncertainty as the MC-dropout conﬁdence rate.

5 Empirical Results

In Section 4 we introduced the SR and MC-dropout conﬁdence-rate function  deﬁned for a given
model f. We trained VGG models [17] for CIFAR-10  CIFAR-100 and ImageNet. For each of these
models f  we considered both the SR and MC-dropout conﬁdence-rate functions  κf   and the induced

2While Theorem 3.2 always holds  we note that if κf is severely skewed (far from ideal)  the bound of the

resulting selective classiﬁer can be far from the target risk.

5

rejection function  gθ(x|κf ). In Figure 2 we present the risk-coverage curves obtained for each of the
three datasets. These curves were obtained by computing a validation risk and coverage for many θ
values. It is evident that the risk-coverage proﬁle for SR and MC-dropout is nearly identical for both
the CIFAR datasets. For the ImageNet set we plot the curves corresponding to top-1 (dashed curves)
and top-5 tasks (solid curves). On this dataset  we see that SR is signiﬁcantly better than MC-dropout
on both tasks. For example  in the top-1 task and 60% coverage  the SR rejection has 10% error while
MC-dropout rejection incurs more than 20% error. But most importantly  these risk-coverage curves
show that selective classiﬁcation can potentially be used to dramatically reduce the error in the three
datasets. Due to the relative advantage of SR  in the rest of our experiments we only focus on the SR
rating.

(a) CIFAR-10

(b) CIFAR-100

(c) Image-Net

Figure 2: Risk coverage curves for (a) cifar-10  (b) cifar-100 and (c) image-net (top-1 task: dashed
curves; top-5 task: solid crves)  SR method in blue and MC-dropout in red.

We now report on experiments with our SGR routine  and apply it on each of the datasets to construct
high probability risk-controlled selective classiﬁers for the three datasets.

Table 1: Risk control results for CIFAR-10 for δ = 0.001

Desired risk (r∗) Train risk Train coverage Test risk Test coverage Risk bound (b∗)
0.01
0.02
0.03
0.04
0.05
0.06

0.0079
0.0160
0.0260
0.0362
0.0454
0.0526

0.7822
0.8482
0.8988
0.9348
0.9610
0.9778

0.0092
0.0149
0.0261
0.0380
0.0486
0.0572

0.7856
0.8466
0.8966
0.9318
0.9596
0.9784

0.0099
0.0199
0.0298
0.0399
0.0491
0.0600

5.1 Selective Guaranteed Risk for CIFAR-10

We now consider CIFAR-10; see [14] for details. We used the VGG-16 architecture [17] and adapted
it to the CIFAR-10 dataset by adding massive dropout  exactly as described in [15]. We used data
augmentation containing horizontal ﬂips  vertical and horizontal shifts  and rotations  and trained
using SGD with momentum of 0.9  initial learning rate of 0.1  and weight decay of 0.0005. We
multiplicatively dropped the learning rate by 0.5 every 25 epochs  and trained for 250 epochs. With
this setting we reached validation accuracy of 93.54  and used the resulting network f10 as the basis
for our selective classiﬁer.
We applied the SGR algorithm on f10 with the SR conﬁdence-rating function  where the training
set for SGR  Sm  was taken as half of the standard CIFAR-10 validation set that was randomly split
to two equal parts. The other half  which was not consumed by SGR for training  was reserved for
testing the resulting bounds. Thus  this training and test sets where each of approximately 5000
samples. We applied the SGR routine with several desired risk values  r∗  and obtained  for each
such r∗  corresponding selective classiﬁer and risk bound b∗. All our applications of the SGR routine

6

(for this dataset and the rest) where with a particularly small conﬁdence level δ = 0.001.3 We then
applied these selective classiﬁers on the reserved test set  and computed  for each selective classiﬁer 
test risk and test coverage. The results are summarized in Table 1  where we also include train risk
and train coverage that were computed  for each selective classiﬁer  over the training set.
Observing the results in Table 1  we see that the risk bound  b∗  is always very close to the target risk 
r∗. Moreover  the test risk is always bounded above by the bound b∗  as required. We compared this
result to a basic baseline in which the threshold is deﬁned to be the value that maximizes coverage
while keeping train error smaller then r∗. For this simple baseline we found that in over 50% of
the cases (1000 random train/test splits)  the bound r∗ was violated over the test set  with a mean
violation of 18% relative to the requested r∗. Finally  we see that it is possible to guarantee with this
method amazingly small 1% error while covering more than 78% of the domain.

5.2 Selective Guaranteed Risk for CIFAR-100

Using the same VGG architechture (now adapted to 100 classes) we trained a model for CIFAR-100
while applying the same data augmentation routine as in the CIFAR-10 experiment. Following
precisly the same experimental design as in the CFAR-10 case  we obtained the results of Table 2

Table 2: Risk control results for CIFAR-100 for δ = 0.001

Desired risk (r∗) Train risk Train coverage Test risk Test coverage Risk bound (b∗)
0.02
0.05
0.10
0.15
0.20
0.25

0.0119
0.0425
0.0927
0.1363
0.1872
0.2380

0.2010
0.4286
0.5736
0.6546
0.7650
0.8716

0.0187
0.0413
0.0938
0.1327
0.1810
0.2395

0.2134
0.4450
0.5952
0.6752
0.7778
0.8826

0.0197
0.0499
0.0998
0.1498
0.1999
0.2499

Here again  SGR generated tight bounds  very close to the desired target risk  and the bounds were
never violated by the true risk. Also  we see again that it is possible to dramatically reduce the risk
with only moderate compromise of the coverage. While the architecture we used is not state-of-the art 
with a coverage of 67%  we easily surpassed the best known result for CIFAR-100  which currently
stands on 18.85% using the wide residual network architecture [19]. It is very likely that by using
ourselves the wide residual network architecture we could obtain signiﬁcantly better results.

5.3 Selective Guaranteed Risk for ImageNet

We used an already trained Image-Net VGG-16 model based on ILSVRC2014 [16]. We repeated the
same experimental design but now the sizes of the training and test set were approximately 25 000.
The SGR results for both the top-1 and top-5 classiﬁcation tasks are summarized in Tables 3 and 4 
respectively. We also implemented the RESNET-50 architecture [12] in order to see if qualitatively
similar results can be obtained with a different architecture. The RESNET-50 results for ImageNet
top-1 and top-5 classiﬁcation tasks are summarized in Tables 5 and 6  respectively.

Table 3: SGR results for Image-Net dataset using VGG-16 top-1 for δ = 0.001

Desired risk (r∗) Train risk Train coverage Test risk Test coverage Risk bound(b∗)
0.02
0.05
0.10
0.15
0.20
0.25

0.0161
0.0462
0.0964
0.1466
0.1937
0.2441

0.0131
0.0446
0.0948
0.1467
0.1949
0.2445

0.2355
0.4292
0.5968
0.7164
0.8131
0.9117

0.0200
0.0500
0.1000
0.1500
0.2000
0.2500

0.2322
0.4276
0.5951
0.7138
0.8154
0.9120

3With this small δ  and small number of reported experiments (6-7 lines in each table) we did not perform a

Bonferroni correction (which can be easily added).

7

Table 4: SGR results for Image-Net dataset using VGG-16 top-5 for δ = 0.001

Desired risk (r∗) Train risk Train coverage Test risk Test coverage Risk bound(b∗)
0.01
0.02
0.03
0.04
0.05
0.06
0.07

0.0080
0.0181
0.0281
0.0381
0.0481
0.0563
0.0663

0.0078
0.0179
0.0290
0.0379
0.0496
0.0577
0.0694

0.3391
0.5360
0.6768
0.7610
0.8263
0.8654
0.9093

0.0100
0.0200
0.0300
0.0400
0.0500
0.0600
0.0700

0.3341
0.5351
0.6735
0.7586
0.8262
0.8668
0.9114

Table 5: SGR results for Image-Net dataset using RESNET50 top-1 for δ = 0.001

Desired risk (r∗) Train risk Train coverage Test risk Test coverage Risk bound (b∗)
0.02
0.05
0.10
0.15
0.20
0.25

0.0161
0.0462
0.0965
0.1466
0.1937
0.2441

0.2613
0.4906
0.6544
0.7711
0.8688
0.9634

0.0164
0.0474
0.0988
0.1475
0.1955
0.2451

0.2585
0.4878
0.6502
0.7676
0.8677
0.9614

0.0199
0.0500
0.1000
0.1500
0.2000
0.2500

These results show that even for the challenging ImageNet  with both the VGG and RESNET archi-
tectures  our selective classiﬁers are extremely effective  and with appropriate coverage compromise 
our classiﬁer easily surpasses the best known results for ImageNet. Not surprisingly  RESNET  which
is known to achieve better results than VGG on this set  preserves its relative advantage relative to
VGG through all r∗ values.

6 Concluding Remarks

We presented an algorithm for learning a selective classiﬁer whose risk can be fully controlled and
guaranteed with high conﬁdence. Our empirical study validated this algorithm on challenging image
classiﬁcation datasets  and showed that guaranteed risk-control is achievable. Our methods can be
immediately used by deep learning practitioners  helping them in coping with mission-critical tasks.
We believe that our work is only the ﬁrst signiﬁcant step in this direction  and many research questions
are left open. The starting point in our approach is a trained neural classiﬁer f (supposedly trained to
optimize risk under full coverage). While the rejection mechanisms we considered were extremely
effective  it might be possible to identify superior mechanisms for a given classiﬁer f. We believe 
however  that the most challenging open question would be to simultaneously train both the classiﬁer
f and the selection function g to optimize coverage for a given risk level. Selective classiﬁcation
is intimately related to active learning in the context of linear classiﬁers [6  11]. It would be very
interesting to explore this potential relationship in the context of (deep) neural classiﬁcation. In this
paper we only studied selective classiﬁcation under the 0/1 loss. It would be of great importance

Table 6: SGR results for Image-Net dataset using RESNET50 top-5 for δ = 0.001

Desired risk (r∗) Train risk Train coverage Test risk Test coverage Risk bound(b∗)
0.01
0.02
0.03
0.04
0.05
0.06
0.07

0.0080
0.0181
0.0281
0.0381
0.0481
0.0581
0.0663

0.0085
0.0189
0.0273
0.0358
0.0464
0.0552
0.0629

0.3796
0.5938
0.7122
0.8180
0.8856
0.9256
0.9508

0.0099
0.0200
0.0300
0.0400
0.0500
0.0600
0.0700

0.3807
0.5935
0.7096
0.8158
0.8846
0.9231
0.9484

8

to extend our techniques to other loss functions and speciﬁcally to regression  and to fully control
false-positive and false-negative rates.
This work has many applications. In general  any classiﬁcation task where a controlled risk is critical
would beneﬁt by using our methods. An obvious example is that of medical applications where
utmost precision is required and rejections should be handled by human experts. In such applications
the existence of performance guarantees  as we propose here  is essential. Financial investment
applications are also obvious  where there are great many opportunities from which one should
cherry-pick the most certain ones. A more futuristic application is that of robotic sales representatives 
where it could extremely harmful if the bot would try to answer questions it does not fully understand.

Acknowledgments

This research was supported by The Israel Science Foundation (grant No. 1890/14)

References
[1] Chao K Chow. An optimum character recognition system using decision functions.

Transactions on Electronic Computers  (4):247–254  1957.

IRE

[2] Luigi Pietro Cordella  Claudio De Stefano  Francesco Tortorella  and Mario Vento. A method
for improving classiﬁcation reliability of multilayer perceptrons. IEEE Transactions on Neural
Networks  6(5):1140–1147  1995.

[3] Corinna Cortes  Giulia DeSalvo  and Mehryar Mohri. Boosting with abstention. In Advances in

Neural Information Processing Systems  pages 1660–1668  2016.

[4] Claudio De Stefano  Carlo Sansone  and Mario Vento. To reject or not to reject: that is the
question-an answer in case of neural classiﬁers. IEEE Transactions on Systems  Man  and
Cybernetics  Part C (Applications and Reviews)  30(1):84–94  2000.

[5] R. El-Yaniv and Y. Wiener. On the foundations of noise-free selective classiﬁcation. Journal of

Machine Learning Research  11:1605–1641  2010.

[6] Ran El-Yaniv and Yair Wiener. Active learning via perfect selective classiﬁcation. Journal of

Machine Learning Research (JMLR)  13(Feb):255–279  2012.

[7] Yoav Freund  Yishay Mansour  and Robert E Schapire. Generalization bounds for averaged

classiﬁers. Annals of Statistics  pages 1698–1722  2004.

[8] Giorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In

Pattern recognition with support vector machines  pages 68–82. Springer  2002.

[9] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: representing model
uncertainty in deep learning. In Proceedings of The 33rd International Conference on Machine
Learning  pages 1050–1059  2016.

[10] O. Gascuel and G. Caraux. Distribution-free performance bounds with the resubstitution error

estimate. Pattern Recognition Letters  13:757–764  1992.

[11] R. Gelbhart and R. El-Yaniv. The Relationship Between Agnostic Selective Classiﬁcation and

Active. ArXiv e-prints  January 2017.

[12] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 770–778  2016.

[13] Martin E Hellman. The nearest neighbor classiﬁcation rule with a reject option. IEEE Transac-

tions on Systems Science and Cybernetics  6(3):179–185  1970.

[14] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

9

[15] Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classiﬁ-
cation using small training sample size. In Pattern Recognition (ACPR)  2015 3rd IAPR Asian
Conference on  pages 730–734. IEEE  2015.

[16] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei.
ImageNet large scale visual recognition challenge. International Journal of Computer Vision
(IJCV)  115(3):211–252  2015.

[17] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[18] Kush R Varshney. A risk bound for ensemble classiﬁcation with a reject option. In Statistical

Signal Processing Workshop (SSP)  2011 IEEE  pages 769–772. IEEE  2011.

[19] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks.

arXiv:1605.07146  2016.

arXiv preprint

10

,Yonatan Geifman
Ran El-Yaniv