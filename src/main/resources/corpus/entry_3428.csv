2016,MetaGrad: Multiple Learning Rates in Online Learning,In online convex optimization it is well known that certain subclasses of objective functions are much easier than arbitrary convex functions. We are interested in designing adaptive methods that can automatically get fast rates in as many such subclasses as possible  without any manual tuning. Previous adaptive methods are able to interpolate between strongly convex and general convex functions. We present a new method  MetaGrad  that adapts to a much broader class of functions  including exp-concave and strongly convex functions  but also various types of stochastic and non-stochastic functions without any curvature. For instance  MetaGrad can achieve logarithmic regret on the unregularized hinge loss  even though it has no curvature  if the data come from a favourable probability distribution. MetaGrad's main feature is that it simultaneously considers multiple learning rates. Unlike all previous methods with provable regret guarantees  however  its learning rates are not monotonically decreasing over time and are not tuned based on a theoretically derived bound on the regret. Instead  they are weighted directly proportional to their empirical performance on the data using a tilted exponential weights master algorithm.,MetaGrad: Multiple Learning Rates

in Online Learning

Tim van Erven
Leiden University

tim@timvanerven.nl

Wouter M. Koolen

Centrum Wiskunde & Informatica

wmkoolen@cwi.nl

Abstract

In online convex optimization it is well known that certain subclasses of objective
functions are much easier than arbitrary convex functions. We are interested in
designing adaptive methods that can automatically get fast rates in as many such
subclasses as possible  without any manual tuning. Previous adaptive methods
are able to interpolate between strongly convex and general convex functions. We
present a new method  MetaGrad  that adapts to a much broader class of functions 
including exp-concave and strongly convex functions  but also various types of
stochastic and non-stochastic functions without any curvature. For instance  Meta-
Grad can achieve logarithmic regret on the unregularized hinge loss  even though
it has no curvature  if the data come from a favourable probability distribution.
MetaGrad’s main feature is that it simultaneously considers multiple learning rates.
Unlike previous methods with provable regret guarantees  however  its learning
rates are not monotonically decreasing over time and are not tuned based on a
theoretically derived bound on the regret. Instead  they are weighted directly
proportional to their empirical performance on the data using a tilted exponential
weights master algorithm.

1

Introduction

Methods for online convex optimization (OCO) [28  12] make it possible to optimize parameters
sequentially  by processing convex functions in a streaming fashion. This is important in time series
prediction where the data are inherently online; but it may also be convenient to process ofﬂine data
sets sequentially  for instance if the data do not all ﬁt into memory at the same time or if parameters
need to be updated quickly when extra data become available.
The difﬁculty of an OCO task depends on the convex functions f1  f2  . . .   fT that need to be
optimized. The argument of these functions is a d-dimensional parameter vector w from a convex
domain U. Although this is abstracted away in the general framework  each function ft usually
measures the loss of the parameters on an underlying example (xt  yt) in a machine learning task.
For example  in classiﬁcation ft might be the hinge loss ft(w) = max{0  1  ythw  xti} or the
logistic loss ft(w) = ln1 + eythw xti  with yt 2 {1  +1}. Thus the difﬁculty depends both on
the choice of loss and on the observed data.
There are different methods for OCO  depending on assumptions that can be made about the functions.
The simplest and most commonly used strategy is online gradient descent (GD)  which does not
require any assumptions beyond convexity. GD updates parameters wt+1 = wt  ⌘trft(wt) by
taking a step in the direction of the negative gradient  where the step size is determined by a parameter
⌘t called the learning rate. For learning rates ⌘t / 1/pt  GD guarantees that the regret over T
rounds  which measures the difference in cumulative loss between the online iterates wt and the best
ofﬂine parameters u  is bounded by O(pT ) [33]. Alternatively  if it is known beforehand that the
functions are of an easier type  then better regret rates are sometimes possible. For instance  if the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

functions are strongly convex  then logarithmic regret O(ln T ) can be achieved by GD with much
smaller learning rates ⌘t / 1/t [14]  and  if they are exp-concave  then logarithmic regret O(d ln T )
can be achieved by the Online Newton Step (ONS) algorithm [14].
This partitions OCO tasks into categories  leaving it to the user to choose the appropriate algorithm
for their setting. Such a strict partition  apart from being a burden on the user  depends on an extensive
cataloguing of all types of easier functions that might occur in practice. (See Section 3 for several
ways in which the existing list of easy functions can be extended.) It also immediately raises the
question of whether there are cases in between logarithmic and square-root regret (there are  see
Theorem 3 in Section 3)  and which algorithm to use then. And  third  it presents the problem that
the appropriate algorithm might depend on (the distribution of) the data (again see Section 3)  which
makes it entirely impossible to select the right algorithm beforehand.
These issues motivate the development of adaptive methods  which are no worse than O(pT ) for
general convex functions  but also automatically take advantage of easier functions whenever possible.
An important step in this direction are the adaptive GD algorithm of Bartlett  Hazan  and Rakhlin
[2] and its proximal improvement by Do  Le  and Foo [8]  which are able to interpolate between
strongly convex and general convex functions if they are provided with a data-dependent strong
convexity parameter in each round  and signiﬁcantly outperform the main non-adaptive method
(i.e. Pegasos  [29]) in the experiments of Do et al. Here we consider a signiﬁcantly richer class of
functions  which includes exp-concave functions  strongly convex functions  general convex functions
that do not change between rounds (even if they have no curvature)  and stochastic functions whose
gradients satisfy the so-called Bernstein condition  which is well-known to enable fast rates in ofﬂine
statistical learning [1  10  19]. The latter group can again include functions without curvature  like
the unregularized hinge loss. All these cases are covered simultaneously by a new adaptive method
we call MetaGrad  for multiple eta gradient algorithm. MetaGrad maintains a covariance matrix of
size d ⇥ d where d is the parameter dimension. In the remainder of the paper we call this version full
MetaGrad. A reference implementation is available from [17]. We also design and analyze a faster
approximation that only maintains the d diagonal elements  called diagonal MetaGrad. Theorem 7
below implies the following:
Theorem 1. Let gt = rft(wt) and V u
is simultaneously bounded by O(pT ln ln T )  and by
TXt=1

t=1 ((u  wt)|gt)2. Then the regret of full MetaGrad
T d ln T + d ln T⌘ for any u 2U . (1)

T =PT

TXt=1

(wt  u)|gt  O⇣pV u

f (wt) 

ft(u) 

TXt=1

Theorem 1 bounds the regret in terms of a measure of variance V u
T that depends on the distance of
the algorithm’s choices wt to the optimum u  and which  in favourable cases  may be signiﬁcantly
smaller than T . Intuitively  this happens  for instance  when there is stable optimum u that the
algorithm’s choices wt converge to. Formal consequences are given in Section 3  which shows that
this bound implies faster than O(pT ) regret rates  often logarithmic in T   for all functions in the rich
class mentioned above. In all cases the dependence on T in the rates matches what we would expect
based on related work in the literature  and in most cases the dependence on the dimension d is also
what we would expect. Only for strongly convex functions is there an extra factor d. It is an open
question whether this is a fundamental obstacle for which an even more general adaptive method is
needed  or whether it is an artefact of our analysis.
The main difﬁculty in achieving the regret guarantee from Theorem 1 is tuning a learning rate
T   but this is not possible using any existing
techniques  because the optimum u is unknown in advance  and tuning in terms of a uniform upper
bound maxu V u
T ruins all desired beneﬁts. MetaGrad therefore runs multiple slave algorithms  each
with a different learning rate  and combines them with a novel master algorithm that learns the
empirically best learning rate for the OCO task in hand. The slaves are instances of exponential
weights on the continuous parameters u with a suitable surrogate loss function  which in particular
causes the exponential weights distributions to be multivariate Gaussians. For the full version of
MetaGrad  the slaves are closely related to the ONS algorithm on the original losses  where each
slave receives the master’s gradients instead of its own. It is shown that d 1
2 log2 Te + 1 slaves sufﬁce 
which is at most 16 as long as T  109  and therefore seems computationally acceptable. If not  then
the number of slaves can be further reduced at the cost of slightly worse constants in the bound.

parameter ⌘. In theory  ⌘ should be roughly 1/pV u

2

Protocol 1: Online Convex Optimization from First-order Information
Input: Convex set U
1: for t = 1  2  . . . do
2:
3:
4:
5: end for

Learner plays wt 2U
Environment reveals convex loss function ft : U! R
Learner incurs loss ft(wt) and observes (sub)gradient gt = rft(wt)

Related Work If we disregard computational efﬁciency  then the result of Theorem 1 can be
achieved by ﬁnely discretizing the domain U and running the Squint algorithm for prediction with
experts with each discretization point as an expert [16]. MetaGrad may therefore also be seen as a
computationally efﬁcient extension of Squint to the OCO setting.
Our focus in this work is on adapting to sequences of functions ft that are easier than general convex
functions. A different direction in which faster rates are possible is by adapting to the domain U. As
we assume U to be ﬁxed  we consider an upper bound D on the norm of the optimum u to be known.
In contrast  Orabona and Pál [24  25] design methods that can adapt to the norm of u. One may also
look at the shape of U. As can be seen in the analysis of the slaves  MetaGrad is based a spherical
Gaussian prior on Rd  which favours u with small `2-norm. This is appropriate for U that are similar
to the Euclidean ball  but less so if U is more like a box (`1-ball). In this case  it would be better
to run a copy of MetaGrad for each dimension separately  similarly to how the diagonal version of
the AdaGrad algorithm [9  21] may be interpreted as running a separate copy of GD with a separate
learning rate for each dimension. AdaGrad further uses an adaptive tuning of the learning rates that is
able to take advantage of sparse gradient vectors  as can happen on data with rarely observed features.
We brieﬂy compare to AdaGrad in some very simple simulations in Appendix A.1.
Another notion of adaptivity is explored in a series of work [13  6  31] obtaining tighter bounds
for linear functions ft that vary little between rounds (as measured either by their deviation from
the mean function or by successive differences). Such bounds imply super fast rates for optimizing
a ﬁxed linear function  but reduce to slow O(pT ) rates in the other cases of easy functions that
we consider. Finally  the way MetaGrad’s slaves maintain a Gaussian distribution on parameters u
is similar in spirit to AROW and related conﬁdence weighted methods  as analyzed by Crammer 
Kulesza  and Dredze [7] in the mistake bound model.

Outline We start with the main deﬁnitions in the next section. Then Section 3 contains an extensive
set of examples where Theorem 1 leads to fast rates  Section 4 presents the MetaGrad algorithm 
and Section 5 provides the analysis leading to Theorem 7  which is a more detailed statement of
Theorem 1 with an improved dependence on the dimension in some particular cases and with exact
constants. The details of the proofs can be found in the appendix.

2 Setup

Let U✓ Rd be a closed convex set  which we assume contains the origin 0 (if not  it can always
be translated). We consider algorithms for Online Convex Optimization over U  which operate
according to the protocol displayed in Protocol 1. Let wt 2U be the iterate produced by the
algorithm in round t  let ft : U! R be the convex loss function produced by the environment and let
gt = rft(wt) be the (sub)gradient  which is the feedback given to the algorithm.1 We abbreviate the
regret with respect to u 2U as Ru
t=1 (ft(wt)  ft(u))  and deﬁne our measure of variance as
T =PT
V u
i=1(uiwt i)2g2
for the diagonal version. By convexity of ft  we always have ft(wt)ft(u)  (wtu)|gt. Deﬁning
T =PT
˜Ru
T . A stronger
requirement than convexity is that a function f is exp-concave  which (for exp-concavity parameter
1) means that ef is concave. Finally  we impose the following standard boundedness assumptions 
distinguishing between the full version of MetaGrad (left column) and the diagonal version (right

t=1Pd
T =PT
t=1 ((u  wt)|gt)2 for the full version of MetaGrad and V u
t=1(wt  u)|gt  this implies the ﬁrst inequality in Theorem 1: Ru
T  ˜Ru

T =PT

t i

1If ft is not differentiable at wt  any choice of subgradient gt 2 @ft(wt) is allowed.

3

column): for all u  v 2U   all dimensions i and all times t 

full

diag

ku  vk  Dfull
kgtk  Gfull

|ui  vi| Ddiag
|gt i| Gdiag.

(2)

Here  and throughout the paper  the norm of a vector (e.g. kgtk) will always refer to the `2-norm.
For the full version of MetaGrad  the Cauchy-Schwarz inequality further implies that (u  v)|gt 
ku  vk·k gtk  DfullGfull.
3 Fast Rate Examples

In this section  we motivate our interest in the adaptive bound (1) by giving a series of examples in
which it provides fast rates. These fast rates are all derived from two general sufﬁcient conditions:
one based on the directional derivative of the functions ft and one for stochastic gradients that satisfy
the Bernstein condition  which is the standard condition for fast rates in off-line statistical learning.
Simple simulations that illustrate the conditions are provided in Appendix A.1 and proofs are also
postponed to Appendix A.

ft(u)  ft(w) + a(u  w)|rft(w) + b ((u  w)|rft(w))2

Directional Derivative Condition In order to control the regret with respect to some point u  the
ﬁrst condition requires a quadratic lower bound on the curvature of the functions ft in the direction
of u:
Theorem 2. Suppose  for a given u 2U   there exist constants a  b > 0 such that the functions ft all
satisfy
(3)
Then any method with regret bound (1) incurs logarithmic regret  Ru
T = O(d ln T )  with respect to u.
The case a = 1 of this condition was introduced by Hazan  Agarwal  and Kale [14]  who show that
it is satisﬁed for all u 2U by exp-concave and strongly convex functions. The rate O(d ln T ) is
also what we would expect by summing the asymptotic ofﬂine rate obtained by ridge regression on
the squared loss [30  Section 5.2]  which is exp-concave. Our extension to a > 1 is technically a
minor step  but it makes the condition much more liberal  because it may then also be satisﬁed by
functions that do not have any curvature. For example  suppose that ft = f is a ﬁxed convex function
that does not change with t. Then  when u⇤ = arg minu f (u) is the ofﬂine minimizer  we have
(u⇤  w)|rf (w) 2 [GfullDfull  0]  so that
DfullGfull ((u⇤  w)|rf (w))2  
f (u⇤)  f (w)  (u⇤  w)|rf (w)  2(u⇤  w)|rf (w) +
where the ﬁrst inequality uses only convexity of f. Thus condition (3) is satisﬁed by any ﬁxed convex
function  even if it does not have any curvature at all  with a = 2 and b = 1/(GfullDfull).

for all w 2U .

1

Bernstein Stochastic Gradients The possibility of getting fast rates even without any curvature
is intriguing  because it goes beyond the usual strong convexity or exp-concavity conditions. In
the online setting  the case of ﬁxed functions ft = f seems rather restricted  however  and may in
fact be handled by ofﬂine optimization methods. We therefore seek to loosen this requirement by
replacing it by a stochastic condition on the distribution of the functions ft. The relation between
variance bounds like Theorem 1 and fast rates in the stochastic setting is studied in depth by Koolen 
Grünwald  and Van Erven [19]  who obtain fast rate results both in expectation and in probability.
Here we provide a direct proof only for the expected regret  which allows a simpliﬁed analysis.
Suppose the functions ft are independent and identically distributed (i.i.d.)  with common distribution
P. Then we say that the gradients satisfy the (B  )-Bernstein condition with respect to the stochastic
optimum u⇤ = arg minu2U Ef⇠P[f (u)] if
(w  u⇤)| E
for all w 2U . (4)
This is an instance of the well-known Bernstein condition from ofﬂine statistical learning [1  10] 
applied to the linearized excess loss (w  u⇤)|rf (w). As shown in Appendix H  imposing the
condition for the linearized excess loss is a weaker requirement than imposing it for the original
excess loss f (w)  f (u⇤).

[rf (w)rf (w)|] (w  u⇤)  B(w  u⇤)| E

[rf (w)]

f

f

4

Algorithm 1: MetaGrad Master
Input: Grid of learning rates
1: for t = 1  2  . . . do
Get prediction w⌘
2:
Play wt = P⌘ ⇡⌘
3:
P⌘ ⇡⌘
Observe gradient gt = rft(wt)
4:
t+1 = ⇡⌘
Update ⇡⌘
5:
P⌘ ⇡⌘
6: end for

⌘
⌘
t (w
t )
⌘
⌘
t (w

2U

t e↵`

t e↵`

t 2U of slave (Algorithm 2) for each ⌘
t ⌘w⌘
t ⌘

t

. Tilted Exponentially Weighted Average

1

5DG  ⌘1  ⌘2  . . . with prior weights ⇡⌘1

1  ⇡ ⌘2

1   . . .

. As in (8)

t ) for all ⌘

. Exponential Weights with surrogate loss (6)

Theorem 3. If the gradients satisfy the (B  )-Bernstein condition for B > 0 and  2 (0  1] with
respect to u⇤ = arg minu2U Ef⇠P[f (u)]  then any method with regret bound (1) incurs expected
regret E[Ru⇤

T ] = O⇣(Bd ln T )1/(2) T (1)/(2) + d ln T⌘.

For  = 1  the rate becomes O(d ln T )  just like for ﬁxed functions  and for smaller  it is in between
logarithmic and O(pdT ). For instance  the hinge loss on the unit ball with i.i.d. data satisﬁes the
Bernstein condition with  = 1  which implies an O(d ln T ) rate. (See Appendix A.4.) It is common
to add `2-regularization to the hinge loss to make it strongly convex  but this example shows that that
is not necessary to get logarithmic regret.

4 MetaGrad Algorithm

In this section we explain the two versions (full and diagonal) of the MetaGrad algorithm. We will
make use of the following deﬁnitions:

full
:= gtg|
:= 1

t

M full
t
↵full

diag
:= diag(g2

M diag
t
↵diag := 1/d.

t 1  . . .   g2

t d)

(5)

Depending on context  wt 2U will refer to the full or diagonal MetaGrad prediction in round t. In
the remainder we will drop the superscript from the letters above  which will always be clear from
context.
MetaGrad will be deﬁned by means of the following surrogate loss `⌘
t (u)  which depends on a
parameter ⌘> 0 that trades off regret compared to u with the square of the scaled directional
derivative towards u (full case) or its approximation (diag case):

t (u) :=  ⌘(wt  u)|gt + ⌘2(u  wt)|Mt(u  wt).
`⌘

(6)
Our surrogate loss consists of a linear and a quadratic part. Using the language of Orabona  Crammer 
and Cesa-Bianchi [26]  the data-dependent quadratic part causes a “time-varying regularizer” and
Duchi  Hazan  and Singer [9] would call it “temporal adaptation of the proximal function”. The sum
of quadratic terms in our surrogate is what appears in the regret bound of Theorem 1.
The MetaGrad algorithm is a two-level hierarchical construction  displayed as Algorithms 1 (master
algorithm that learns the learning rate) and 2 (sub-module  a copy running for each learning rate ⌘
from a ﬁnite grid). Based on our analysis in the next section  we recommend using the grid in (8).

Master The task of the Master Algorithm 1 is to learn the empirically best learning rate ⌘ (parameter
of the surrogate loss `⌘
t )  which is notoriously difﬁcult to track online because the regret is non-
monotonic over rounds and may have multiple local minima as a function of ⌘ (see [18] for a study
in the expert setting). The standard technique is therefore to derive a monotonic upper bound on
the regret and tune the learning rate optimally for the bound. In contrast  our approach  inspired
by the approach for combinatorial games of Koolen and Van Erven [16  Section 4]  is to have our
master aggregate the predictions of a discrete grid of learning rates. Although we provide a formal
analysis of the regret  the master algorithm does not depend on the outcome of this analysis  so any

5

1 = D2I

5DG  domain size D > 0

Algorithm 2: MetaGrad Slave
Input: Learning rate 0 <⌘  1
1: w⌘
2: for t = 1  2  . . . do
3:
4:
5:

t+1 =⇣ 1
ew⌘

1 = 0 and ⌃⌘
Issue w⌘
t to master (Algorithm 1)
Observe gradient gt = rft(wt)
s=1 Ms⌘1
D2 I + 2⌘2Pt
Update ⌃⌘
t+1⌘gt + 2⌘2Mt(w⌘
t+1 = w⌘
t  ⌃⌘
ew⌘
t+1 with projection ⇧⌃
⌃⌘
t+1 =⇧
U
and simplify ew⌘

6: end for
Implementation: For Mt = M diag
t
t gtg|
t ⌃⌘
update ⌃⌘
t
t ⌃⌘
t gt

t  2⌘2⌃⌘
1+2⌘2g|

t+1 = ⌃⌘

w⌘

t+1

t  wt)

. Gradient at master point wt

U (w) = arg min
u2U

(u  w)|⌃1(u  w)

only maintain diagonal of ⌃⌘

t+1 = w⌘

t  ⌘⌃⌘

t . For Mt = M full
t+1gt (1 + 2⌘g|

t

use rank-one
t (w⌘
t  wt)).

slack in our bounds does not feed back into the algorithm. The master is in fact very similar to
the well-known exponential weights method (line 5)  run on the surrogate losses  except that in the
predictions the weights of the slaves are tilted by their learning rates (line 3)  having the effect of
giving a larger weight to larger ⌘. The internal parameter ↵ is set to ↵full from (5) for the full version
of the algorithm  and to ↵diag for the diagonal version.

t (u) yields Gaussian posterior with mean w⌘

Slaves The role of the Slave Algorithm 2 is to guarantee small surrogate regret for a ﬁxed learning
rate ⌘. We consider two versions  corresponding to whether we take rank-one or diagonal matrices
Mt (see (5)) in the surrogate (6). The ﬁrst version maintains a full d ⇥ d covariance matrix and has
the best regret bound. The second version uses only diagonal matrices (with d non-zero entries) 
thus trading off a weaker bound with a better run-time in high dimensions. Algorithm 2 presents
the update equations in a computationally efﬁcient form. Their intuitive motivation is given in the
proof of Lemma 5  where we show that the standard exponential weights method with Gaussian prior
and surrogate losses `⌘
t . The
full version of MetaGrad is closely related to the Online Newton Step algorithm [14] running on the
original losses ft: the differences are that each Slave receives the Master’s gradients gt = rft(wt)
instead of its own rft(w⌘
t  wt) in line 5 adjusts for the
difference between the Slave’s parameters w⌘
t and the Master’s parameters wt. MetaGrad is therefore
a bona ﬁde ﬁrst-order algorithm that only accesses ft through gt. We also note that we have chosen
the Mirror Descent version that iteratively updates and projects (see line 5). One might alternatively
consider the Lazy Projection version (as in [34  23  32]) that forgets past projections when updating
on new data. Since projections are typically computationally expensive  we have opted for the Mirror
Descent version  which we expect to project less often  since a projected point seems less likely to
update to a point outside of the domain than an unprojected point.

t )  and that an additional term 2⌘2Mt(w⌘

t and covariance matrix ⌃⌘

Total run time As mentioned  the running time is dominated by the slaves. Ignoring the projection 
a slave with full covariance matrix takes O(d2) time to update  while slaves with diagonal covariance
matrix take O(d) time. If there are m slaves  this makes the overall computational effort respectively
O(md2) and O(md)  both in time per round and in memory. Our analysis below indicates that
2 log2 Te slaves sufﬁce  so m  16 as long as T  109. In addition  each slave may
m = 1 + d 1
incur the cost of a projection  which depends on the shape of the domain U. To get a sense for the
projection cost we consider a typical example. For the Euclidean ball a diagonal projection can be
performed using a few iterations of Newton’s method to get the desired precision. Each such iteration
costs O(d) time. This is generally considered affordable. For full projections the story is starkly
different. We typically reduce to the diagonal case by a basis transformation  which takes O(d3) to
compute using SVD. Hence here the projection dwarfs the other run time by an order of magnitude.
We refer to [9] for examples of how to compute projections for various domains U. Finally  we
remark that a potential speed-up is possible by running the slaves in parallel.

6

5 Analysis

We conduct the analysis in three parts. We ﬁrst discuss the master  then the slaves and ﬁnally their
composition. The idea is the following. The master guarantees for all ⌘ simultaneously that

0 =

`⌘
t (w⌘

t ) + master regret compared to ⌘-slave.

Then each ⌘-slave takes care of learning u  with regret O(d ln T ):

`⌘
t (wt) 

TXt=1
TXt=1
TXt=1
TXt=1
(wt  u)|gt  ⌘2V u

`⌘
t (w⌘

⌘

TXt=1

t ) 
These two statements combine to

and the overall result follows by optimizing ⌘.

`⌘
t (u) + ⌘-slave regret compared to u.

T = 

TXt=1

`⌘
t (u)  sum of regrets above

(7a)

(7b)

(7c)

t (w⌘

t=1 `⌘

1 e↵PT

t ). In Appendix B  we bound the last factor e↵`⌘

5.1 Master
To show that we can aggregate the slave predictions  we consider the potential T

:=
P⌘ ⇡⌘
T ) above by its tangent at
w⌘
T = wT and obtain an objective that can be shown to be equal to T1 regardless of the gradient
gT if wT is chosen according to the Master algorithm. It follows that the potential is non-increasing:
Lemma 4 (Master combines slaves). The Master Algorithm guarantees 1 = 0  1  . . .  T .
As 0   1
1  this implements step (7a) of our overall proof
t ) + 1
strategy  with master regret 1
1. We further remark that we may view our potential function T
as a game-theoretic supermartingale in the sense of Chernov  Kalnishkan  Zhdanov  and Vovk [5] 
and this lemma as establishing that the MetaGrad Master is the corresponding defensive forecasting
strategy.

↵ ln T  PT

t (w⌘
↵ ln ⇡⌘

↵ ln ⇡⌘

t=1 `⌘

T (w⌘

5.2 Slaves
Next we implement step (7b)  which requires proving an O(d ln T ) regret bound in terms of the
surrogate loss for each MetaGrad slave. In the full case  the surrogate loss is jointly exp-concave  and
in light of the analysis of ONS by Hazan  Agarwal  and Kale [14] such a result is not surprising. For
the diagonal case  the surrogate loss lacks joint exp-concavity  but we can use exp-concavity in each
direction separately  and verify that the projections that tie the dimensions together do not cause any
trouble. In Appendix C we analyze both cases simultaneously  and obtain the following bound on the
regret:
Lemma 5 (Surrogate regret bound). For 0 <⌘  1
in (6) (either the full or the diagonal version). Then the regret of Slave Algorithm 2 is bounded by
TXt=1

ln det I + 2⌘2D2

t (u) be the surrogate losses as deﬁned

1
2D2kuk2 +

for all u 2U .

Mt!

5DG  let `⌘

TXt=1

TXt=1

`⌘
t (u) +

`⌘
t (w⌘

t ) 

1
2

5.3 Composition
To complete the analysis of MetaGrad  we ﬁrst put the regret bounds for the master and slaves together
as in (7c). We then discuss how to choose the grid of ⌘s  and optimize ⌘ over this grid to get our main
result. Proofs are postponed to Appendix D.
Theorem 6 (Grid point regret). The full and diagonal versions of MetaGrad  with corresponding
deﬁnitions from (2) and (5)  guarantee that  for any grid point ⌘ with prior weight ⇡⌘
1 

˜Ru
T  ⌘V u

T +

1

2D2kuk2  1

↵ ln ⇡⌘

1 + 1

2 ln det⇣I + 2⌘2D2PT

t=1 Mt⌘

⌘

for all u 2U .

7

Grid We now specify the grid points and corresponding prior. Theorem 6 above implies that any
two ⌘ that are within a constant factor of each other will guarantee the same bound up to essentially
the same constant factor. We therefore choose an exponentially spaced grid with a heavy tailed prior
(see Appendix E):

⌘i

:=

2i
5DG

and

⇡⌘i
1

:=

C

for i = 0  1  2  . . .  d 1

2 log2 Te 

(8)

(i + 1)(i + 2)
with normalization C = 1 + 1(1 + d 1
2 log2 Te). At the cost of a worse constant factor in the
bounds  the number of slaves can be reduced by using a larger spacing factor  or by omitting some
of the smallest learning rates. The net effect of (8) is that  for any ⌘ 2 [
5DG ] there is an
1  2 ln(i + 2) = O(ln ln(1/⌘i)) = O(ln ln(1/⌘)). As these costs
⌘i 2 [ 1
are independent of T   our regret guarantees still hold if the grid (8) is instantiated with T replaced by
any upper bound.
The ﬁnal step is to apply Theorem 6 to this grid  and to properly select the learning rate ⌘i in the
bound. This leads to our main result:

2 ⌘  ⌘ ]  for which  ln ⇡⌘i

5DGpT

1

2

 

t i.
t=1(ui  wt i)2g2
Then the regret of MetaGrad  with corresponding deﬁnitions from (2) and (5) and with grid and prior
as in (8)  is bounded by

t=1 Mt and V u

T i = PT
CT◆

1
↵

1
↵

˜Ru

where

D2 rk(ST )

T ✓ 1
D2kuk2 +⌅ T +

Theorem 7 (MetaGrad Regret Bound). Let ST = PT
T  s8V u
CT◆ + 5DG✓ 1
ST◆   rk(ST ) ln D2
⌅T  min(ln det✓I +
ln D2PT
for the diagonal version  and CT = 4 ln3 + 1
T vuut8D2 TXt=1

kgtk2!✓ 1

for the full version of the algorithm 

t=1 g2
t i
V u
T i

D2kuk2 +

dXi=1

⌅T =

V u
T

˜Ru

1
↵

V u
T

D2kuk2 +⌅ T +

for all u 2U  

kgtk2!) = O(d ln(D2G2T ))

TXt=1

! = O(d ln(D2G2T ))

2 log2 T = O(ln ln T ) in both cases. Moreover  for
CT◆ + 5DG✓ 1
for all u 2U .

D2kuk2 +

CT◆

1
↵

both versions of the algorithm  the regret is simultaneously bounded by

These two bounds together show that the full version of MetaGrad achieves the new adaptive guarantee
of Theorem 1. The diagonal version behaves like running the full version separately per dimension 
but with a single shared learning rate.

6 Discussion and Future Work

One may consider extending MetaGrad in various directions. In particular it would be interesting
to speed up the method in high dimensions  for instance by sketching [20]. A broader question is
to identify and be adaptive to more types of easy functions that are of practical interest. One may
suspect there to be a price (in regret overhead and in computation) for broader adaptivity  but based
on our results for MetaGrad it does not seem like we are already approaching the point where this
price is no longer worth paying.

Acknowledgments We would like to thank Haipeng Luo and the anonymous reviewers (in par-
ticular Reviewer 6) for valuable comments. Koolen acknowledges support by the Netherlands
Organization for Scientiﬁc Research (NWO  Veni grant 639.021.439).

8

References
[1] P. L. Bartlett and S. Mendelson. Empirical minimization. Probability Theory and Related Fields  135(3):

311–334  2006.

[2] P. L. Bartlett  E. Hazan  and A. Rakhlin. Adaptive online gradient descent. In NIPS 20  pages 65–72  2007.
[3] N. Cesa-Bianchi  Y. Mansour  and G. Stoltz. Improved second-order bounds for prediction with expert

advice. Machine Learning  66(2/3):321–352  2007.

[4] A. V. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Proc. of the 26th

Conf. on Uncertainty in Artiﬁcial Intelligence  pages 117–125  2010.

[5] A. V. Chernov  Y. Kalnishkan  F. Zhdanov  and V. Vovk. Supermartingales in prediction with expert advice.

Theoretical Computer Science  411(29-30):2647–2669  2010.

[6] C.-K. Chiang  T. Yang  C.-J. Le  M. Mahdavi  C.-J. Lu  R. Jin  and S. Zhu. Online optimization with
gradual variations. In Proc. of the 25th Annual Conf. on Learning Theory (COLT)  pages 6.1–6.20  2012.
[7] K. Crammer  A. Kulesza  and M. Dredze. Adaptive regularization of weight vectors. In NIPS 22  pages

414–422  2009.

[8] C. B. Do  Q. V. Le  and C.-S. Foo. Proximal regularization for online and batch learning. In Proc. of the

26th Annual International Conf. on Machine Learning (ICML)  pages 257–264  2009.

[9] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[10] T. van Erven  P. D. Grünwald  N. A. Mehta  M. D. Reid  and R. C. Williamson. Fast rates in statistical and

online learning. Journal of Machine Learning Research  16:1793–1861  2015.

[11] P. Gaillard  G. Stoltz  and T. van Erven. A second-order bound with excess losses. In Proc. of the 27th

Annual Conf. on Learning Theory (COLT)  pages 176–196  2014.

[12] E. Hazan. Introduction to online optimization. Draft  April 10  2016  ocobook.cs.princeton.edu  2016.
[13] E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine

learning  80(2-3):165–188  2010.

Learning  69(2-3):169–192  2007.

[14] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine

[15] S. Ihara. Information Theory for Continuous Systems. World Scientiﬁc  1993.
[16] W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games. In

Proc. of the 28th Annual Conf. on Learning Theory (COLT)  pages 1155–1175  2015.

[17] W. M. Koolen and T. van Erven. MetaGrad open source code. bitbucket.org/wmkoolen/metagrad  2016.
[18] W. M. Koolen  T. van Erven  and P. D. Grünwald. Learning the learning rate for prediction with expert

[19] W. M. Koolen  P. D. Grünwald  and T. van Erven. Combining adversarial guarantees and stochastic fast

advice. In NIPS 27  pages 2294–2302  2014.

rates in online learning. In NIPS 29  2016.

In NIPS 29  2016.

[20] H. Luo  A. Agarwal  N. Cesa-Bianchi  and J. Langford. Efﬁcient second order online learning by sketching.

[21] H. B. McMahan and M. J. Streeter. Adaptive bound optimization for online convex optimization. In Proc.

of the 23rd Annual Conf. on Learning Theory (COLT)  pages 244–256  2010.

[22] T. Mikolov  K. Chen  G. S. Corrado  and J. Dean. Efﬁcient estimation of word representations in vector

space. International Conf. on Learning Representations  2013. Arxiv.org/abs/1301.3781.

[23] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming  120(1):

221–259  2009.

NIPS 27  pages 1116–1124  2014.

[24] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In

[25] F. Orabona and D. Pál. Coin betting and parameter-free online learning. In NIPS 29  2016.
[26] F. Orabona  K. Crammer  and N. Cesa-Bianchi. A generalized online mirror descent with applications to

classiﬁcation and regression. Machine Learning  99(3):411–435  2015.

[27] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks  61:85–117  2015.
[28] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine

Learning  4(2):107–194  2012.

[29] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

SVM. Mathematical Programming  127(1):3–30  2011.

[30] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates.

In NIPS 23  pages

2199–2207  2010.

[31] J. Steinhardt and P. Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In

Proc. of the 31th Annual International Conf. on Machine Learning (ICML)  pages 1593–1601  2014.

[32] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research  11:2543–2596  2010.

[33] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proc. of the

20th Annual International Conf. on Machine Learning (ICML)  pages 928–936  2003.

[34] M. Zinkevich. Theoretical Guarantees for Algorithms in Multi-Agent Settings. PhD thesis  Carnegie

Mellon University  2004.

9

,Tim van Erven
Wouter Koolen
Sai Qian Zhang
Qi Zhang
Jieyu Lin