2012,A new metric on the manifold of kernel matrices with application to matrix geometric means,Symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines  including machine learning and optimization. We consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately  typical non-Euclidean distance measures such as the Riemannian metric $\riem(X Y)=\frob{\log(X\inv{Y})}$  are computationally demanding and also complicated to use. To allay some of these difficulties  we introduce a new metric on spd matrices: this metric not only respects non-Euclidean geometry  it also offers faster computation than $\riem$ while being less complicated to use. We support our claims theoretically via a series of theorems that relate our metric to $\riem(X Y)$  and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances.,A new metric on the manifold of kernel matrices with

application to matrix geometric means

Suvrit Sra

Max Planck Institute for Intelligent Systems

72076 T¨ubigen  Germany

suvrit@tuebingen.mpg.de

Abstract

Symmetric positive deﬁnite (spd) matrices pervade numerous scientiﬁc disci-
plines  including machine learning and optimization. We consider the key task
of measuring distances between two spd matrices; a task that is often nontriv-
ial whenever the distance function must respect the non-Euclidean geometry of
spd matrices. Typical non-Euclidean distance measures such as the Riemannian
metric δR(X  Y ) = (cid:107)log(Y −1/2XY −1/2)(cid:107)F  are computationally demanding and
also complicated to use. To allay some of these difﬁculties  we introduce a new
metric on spd matrices  which not only respects non-Euclidean geometry but also
offers faster computation than δR while being less complicated to use. We sup-
port our claims theoretically by listing a set of theorems that relate our metric to
δR(X  Y )  and experimentally by studying the nonconvex problem of computing
matrix geometric means based on squared distances.

1

Introduction

Symmetric positive deﬁnite (spd) matrices1 are remarkably pervasive in a multitude of areas  espe-
cially machine learning and optimization. Several applications in these areas require an answer to
the fundamental question: how to measure a distance between two spd matrices?
This question arises  for instance  when optimizing over the set of spd matrices. To judge con-
vergence of an optimization procedure or in the design of algorithms we may need to compute
distances between spd matrices [1–3]. As a more concrete example  suppose we wish to retrieve
from a large database of spd matrices the “closest” spd matrix to an input query. The quality of
such a retrieval depends crucially on the distance function used to measure closeness; a choice that
also dramatically impacts the actual search algorithm itself [4  5]. Another familiar setting is that
of computing statistical metrics for multivariate Gaussian distributions [6]  or more recently  quan-
tum statistics [7]. Several other applications depend on being able to effectively measure distances
between spd matrices–see e.g.  [8–10] and references therein.
In many of these domains  viewing spd matrices as members of a Euclidean vector space is insufﬁ-
cient  and the non-Euclidean geometry conferred by a suitable metric is of great importance. Indeed 
the set of (strict) spd matrices forms a differentiable Riemannian manifold [11  10] that is perhaps
the most studied example of a manifold of nonpositive curvature [12; Ch.10]. These matrices also
form a convex cone  and the set of spd matrices in fact serves as a canonical higher-rank symmetric
space [13]. The conic view is of great importance in convex optimization [14–16]  symmetric spaces
are important in algebra and analysis [13  17]  and in optimization [14  18]  while the manifold and
other views are also widely important—see e.g.  [11; Ch.6] for an overview.

1We could equally consider Hermitian matrices  but for simplicity we consider only real matrices.

1

(cid:104)x  Ax(cid:105) > 0

The starting point for this paper is the manifold view. For space reasons  we limit our discussion
to P(n) as a Riemannian manifold  noting that most of the discussion could also be set in terms of
Finsler manifolds. But before we go further  let us ﬁx basic notation.
Notation. Let Sn denote the set of n × n real symmetric matrices. A matrix A ∈ Sn is called
positive (we drop the word “deﬁnite” for brevity) if
for all x (cid:54)= 0;

Frobenius norm of a matrix X ∈ Rm×n is deﬁned as (cid:107)X(cid:107)F =(cid:112)tr(X T X)  while (cid:107)X(cid:107) denotes the

(1)
We denote the set of n × n positive matrices by Pn. If only the non-strict inequality (cid:104)x  Ax(cid:105) ≥ 0
holds (for all x ∈ Rn) we say A is positive semideﬁnite; this is also denoted as A ≥ 0. For two
matrices A  B ∈ Sn  the operator inequality A ≥ B means that the difference A − B ≥ 0. The
standard operator norm. For an analytic function f on C  and a diagonalizable matrix A = U ΛU T  
f (A) := U f (Λ)U T . Let λ(X) denote the vector of eigenvalues of X (in any order) and Eig(X) a
diagonal matrix that has λ(X) as its diagonal. We also use λ↓(X) to denote a sorted (in descending
order) version of λ(X) and λ↑(X) is deﬁned likewise. Finally  we deﬁne Eig↓(X) and Eig↑(X) as
the corresponding diagonal matrices.

also denoted as A > 0.

Background. The set Pn is a canonical higher-rank symmetric space that is actually an open set
within Sn  and thereby a differentiable manifold of dimension n(n + 1)/2. The tangent space at a
point A ∈ Pn can be identiﬁed with Sn  so a suitable inner-product on Sn leads to the Riemannian
distance on Pn [11; Ch.6]. At the point A this metric is induced by the differential form

ds2 = (cid:107)A−1/2dAA−1/2(cid:107)2

(2)
For A  B ∈ Pn  it is known that there is a unique geodesic joining them given by [11; Thm.6.1.6]:
(3)

γ(t) := A(cid:93)tB := A1/2(A−1/2BA−1/2)tA1/2 

0 ≤ t ≤ 1 

F = tr(A−1dAA−1dA).

and its midpoint γ(1/2) is the geometric mean of A and B. The associated Riemannian metric is

δR(A  B) := (cid:107)log(A−1/2BA−1/2)(cid:107)F 

for A  B > 0.

(4)

From deﬁnition (4) it is apparent that computing δR will be computationally demanding  and requires
care. Indeed  to compute (4) we must essentially compute generalized eigenvalues of A and B. For
an application that must repeatedly compute distances between numerous pairs of matrices this
computational burden can be excessive [4]. Driven by such computational concerns  Cherian et al.
[4] introduced a symmetrized “log-det” based matrix divergence:

(cid:16) A+B

2

(cid:17) − 1

J(A  B) = log det

2 log det(AB)

for A  B > 0.

(5)

This divergence was used as a proxy for δR and observed that J(A  B) offers the same level of per-
formance on a difﬁcult nearest neighbor retrieval task as δR  while being many times faster! Among
other reasons  a large part of their speedup was attributed to the avoidance of eigenvalue compu-
tations for obtaining J(A  B) or its derivatives  a luxury the δR does not permit. Independently 
Chebbi and Moahker [2] also introduced a slightly generalized version of (5) and studied some of its
properties  especially computation of “centroids” of positive matrices using their matrix divergence.

Interestingly  Cherian et al. [4] claimed that(cid:112)J(A  B) might not be metric  whereas Chebbi and
Moahker [2] conjectured that(cid:112)J(A  B) is a metric. We resolve this uncertainty and prove that
(cid:112)J(A  B) is indeed a metric  albeit not one that embeds isometrically into a Hilbert space.

Due to space constraints  we only summarily mention several of the properties that this metric sat-
J as a good proxy for the Riemannian
isﬁes  primarily to help develop intuition that motivates
metric δR. We apply these insights to study “matrix geometric means” of set of positive matrices:
a problem also studied in [4  2]. Both cited papers have some gaps in their claims  which we ﬁll
by proving that even though computing the geometric mean is a nonconvex problem  we can still
compute it efﬁciently and optimally.

√

2

2 The δ(cid:96)d metric

The main result of this paper is Theorem 1.
Theorem 1. Let J be as in (5)  and deﬁne δ(cid:96)d :=

√

J. Then  δ(cid:96)d is a metric on Pn.

Our proof of Theorem 1 depends on several key steps. Due to restrictions on space we cannot include
full proofs of all the results  and refer the reader to the longer article [19] instead. We do  however 
provide sketches for the crucial steps in our proof.
Proposition 2. Let A  B  C ∈ Pn. Then  (i) δ(cid:96)d(I  A) = δ(cid:96)d(I  Eig(A)); (ii) for P  Q ∈ GL(n  C) 
δ(cid:96)d(P AQ  P BQ) = δ(cid:96)d(A  B); (iii) for X ∈ GL(n  C)  δ(cid:96)d(X∗AX  X∗BX) = δ(cid:96)d(A  B);
(iv) δ(cid:96)d(A  B) = δ(cid:96)d(A−1  B−1); (v) δ(cid:96)d(A ⊗ B  A ⊗ C) =
nδ(cid:96)d(B  C)  where ⊗ denotes the
Kronecker or tensor product.

√

The ﬁrst crucial result is that for positive scalars  δ(cid:96)d is indeed a metric. To prove this  recall the
notion of negative deﬁnite functions (Def. 3)  and a related classical result of Schoenberg (Thm. 4).
Deﬁnition 3 ([20; Def. 1.1]). Let X be a nonempty set. A function ψ : X × X → R is said to be
negative deﬁnite if for all x  y ∈ X it is symmetric (ψ(x  y) = ψ(y  x))  and satisﬁes the inequality
(6)

cicjψ(xi  xj) ≤ 0 

for all integers n ≥ 2  and subsets {xi}n
Theorem 4 ([20; Prop. 3.2  Chap. 3]). Let ψ : X × X → R be negative deﬁnite. Then  there is a
Hilbert space H ⊆ RX and a mapping x (cid:55)→ ϕ(x) from X → H such that we have the equality

i=1 ci = 0.

(cid:88)n
i=1 ⊆ X   {ci}n

i j=1

i=1 ⊆ R with(cid:80)n

(cid:107)ϕ(x) − ϕ(y)(cid:107)2H = ψ(x  y) − 1

2 (ψ(x  x) + ψ(y  y)).
Moreover  negative deﬁniteness of ψ is necessary for such a mapping to exist.
Theorem 5 (Scalar case). Deﬁne δ2

s (x  y) := log[(x + y)/(2

√

xy)] for scalars x  y > 0. Then 

δs(x  y) ≤ δs(x  z) + δs(y  z)

Proof. We show that ψ(x  y) = log(cid:0) x+y

(8)
s (x  y) = ψ(x  y) −
2 (ψ(x  x)+ψ(y  y))  Thm. 4 then implies the triangle inequality (8). To prove ψ is negative deﬁnite 
by [Thm. 2.2  Chap. 3  20] we may equivalently show that e−βψ(x y) = ((x + y)/2)−β is a positive
deﬁnite function for β > 0  and all x  y > 0. To that end  it sufﬁces to show that the matrix

(cid:1) is negative deﬁnite. Since δ2

for all x  y  z > 0.

1

2

(7)

1 ≤ i  j ≤ n 
is positive deﬁnite for every integer n ≥ 1  and positive numbers {xi}n

i=1. Now  observe that

H = [hij] =(cid:2)(xi + xj)−β(cid:3)  
(cid:90) ∞

1

1

hij =

(xi + xj)β =

Γ(β)

0

where Γ(β) =(cid:82) ∞

e−t(xi+xj )tβ−1dt 

(9)

β−1

2 ∈

L2([0 ∞))  we see that [hij] equals the Gram matrix [(cid:104)fi  fj(cid:105)]  whereby H > 0.

0 e−ttβ−1dt is the well-known Gamma function. Thus  with fi(t) = e−txit

Using Thm. 5 we obtain the following simple but important “Minkowsi” inequality for δs.
Corollary 6. Let x  y  z > 0 be scalars  and let p ≥ 1. Then 

(cid:17)1/p

(cid:16)(cid:88)n

(cid:17)1/p

(cid:16)(cid:88)n

(cid:17)1/p ≤(cid:16)(cid:88)n

δp
s (xi  yi)

i=1

δp
s (xi  zi)

i=1

+

δp
s (yi  zi)

i=1

Corollary 7. Let X  Y  Z > 0 be diagonal matrices. Then 

δ(cid:96)d(X  Y ) ≤ δ(cid:96)d(X  Z) + δ(cid:96)d(Y  Z)

Next  we recall a fundamental determinantal inequality.
Theorem 8 ([21; Exercise VI.7.2]). Let A  B ∈ Pn. Then 

i (B)) ≤ det(A + B) ≤(cid:89)n

↓

(cid:89)n

↓
i (A) + λ
(λ

i=1

↑
↓
i (A) + λ
(λ
i (B)).

i=1

3

.

(10)

(11)

(12)

Corollary 9. Let A  B > 0. Then 

δ(cid:96)d(Eig↓(A)  Eig↓(B)) ≤ δ(cid:96)d(A  B) ≤ δ(cid:96)d(Eig↓(A)  Eig↑(B))

The ﬁnal result that we need is a well-known fact from linear algebra (our own proof is in [19]).
Lemma 10 ([e.g.  22; p.58]). Let A > 0  and let B be Hermitian. There is a matrix P for which

P ∗AP = I 

and P ∗BP = D 

and D is diagonal.

(13)

With all these theorems and lemmas in hand  we are now ﬁnally ready to prove Thm. 1.

Proof. (Theorem 1). We must prove that δ(cid:96)d is symmetric  nonnegative  deﬁnite  and that is satisﬁes
the triangle inequality. Symmetry is immediate from deﬁnition. Nonnegativity and deﬁniteness
follow from the strict log-concavity (on Pn) of the determinant  whereby

det(cid:0) X+Y

2

(cid:1) ≥ det(X)1/2 det(Y )1/2 

which equality iff X = Y   which in turn implies that δ(cid:96)d(X  Y ) ≥ 0 with equality iff X = Y . The
only hard part is to prove the triangle inequality  a result that has eluded previous attempts [4  2].
Let X  Y  Z > 0 be arbitrary. From Lemma 10 we know that there is a matrix P such that P ∗XP =
I and P ∗Y P = D. Since Z > 0 is arbitrary  and congruence preserves positive deﬁniteness  we
may write just Z instead of P ∗ZP . Also  since δ(cid:96)d(P ∗XP  P ∗Y P ) = δ(cid:96)d(X  Y ) (see Prop. 2) 
proving the triangle inequality reduces to showing that

δ(cid:96)d(I  D) ≤ δ(cid:96)d(I  Z) + δ(cid:96)d(D  Z).

(14)

Consider now the diagonal matrices D↓ and Eig↓(Z). Corollary 7 asserts the inequality

δ(cid:96)d(I  D↓) ≤ δ(cid:96)d(I  Eig↓(Z)) + δ(cid:96)d(D↓  Eig↓(Z)).

(15)
Prop. 2(i) implies that δ(cid:96)d(I  D) = δ(cid:96)d(I  D↓) and δ(cid:96)d(I  Z) = δ(cid:96)d(I  Eig↓(Z))  while Cor. 9 shows
that δ(cid:96)d(D↓  Eig↓(Z)) ≤ δ(cid:96)d(D  Z). Combining these inequalities  we obtain (14)  as desired.
Although the metric space (Pn  δ(cid:96)d) has numerous fascinating properties  due to space concerns  we
do not discuss it further. Instead we discuss a connection more important to machine learning and
related areas: kernel functions arising from δ(cid:96)d. Indeed  some of connections (e.g.  Thm. 11) have
already been successfully applied very recently in computer vision [23].

2.1 Hilbert space embedding of δ(cid:96)d

Theorem 1 shows that δ(cid:96)d is a metric and Theorem 5 shows that actually for positive scalars  the
metric space (R++  δs) embeds isometrically into a Hilbert space. It is  therefore  natural to ask
whether (Pn  δ(cid:96)d) also admits such an embedding?
Theorem 4 says that such a kernel exists if and only if δ2

(cid:96)d is negative deﬁnite; equivalently  iff

e−βδ2

(cid:96)d(X Y ) = det(XY )β

det((X+Y )/2)β  

(cid:104)

(cid:105)

Hβ = [hij] :=

1

det(Xi+Xj )β

1 ≤ i  j ≤ m 

 

(16)

(17)

is a positive deﬁnite kernel for all β > 0. To verify this  it sufﬁces to check if the matrix

is positive for every integer m ≥ 1 and arbitrary positive matrices X1  . . .   Xm.
Unfortunately  a numerical experiment (see [19]) reveals that Hβ is not always positive. This implies
that (Pd  δ(cid:96)d) cannot embed isometrically into a Hilbert space. Undeterred  we still ask: For what
choices of β is Hβ positive? Surprisingly  this question admits a complete answer. Theorem 11
characterizes the values of β necessary and sufﬁcient for Hβ to be positive. We note here that the
case β = 1 was essentially treated in [24]  in the context of semigroup kernels on measures.
Theorem 11. Let X1  . . .   Xm ∈ Pn. The matrix Hβ deﬁned by (17) is positive  if and only if

2 : j ∈ N  and 1 ≤ j ≤ (n − 1)(cid:9) ∪(cid:8)γ : γ ∈ R  and γ > 1

2 (n − 1)(cid:9) .

β ∈(cid:8) j

(18)

4

(cid:90)

(cid:90)

Proof. We ﬁrst prove the “if” part. Deﬁne the function fi := 1
fi ∈ L2(Rn)  where the inner-product is given by the Gaussian integral

πn/4 e−xT Xix (for 1 ≤ i ≤ m). Then 

(cid:104)fi  fj(cid:105) :=

1

πd/2

Rn

e−xT (Xi+Xj )xdx =

1

det(Xi+Xj )1/2 .

(19)

From (19) it follows that H1/2 is positive. Since the Schur (elementwise) product of two positive
matrices is again positive  it follows that Hβ > 0 whenever β is an integer multiple of 1/2. To
extend the result to all β covered by (18)  we need a more intricate integral representation  namely
the multivariate Gamma function  deﬁned as [25; §2.1.2]

(cid:90)

(20)
Pn
2 (n − 1). Deﬁne for each i the function fi := ce− tr(AXi)
where the integral converges for β > 1
(c > 0 is a constant). Then  fi ∈ L2(Pn)  which we equip with the inner product

Γn(β) :=

e− tr(A) det(A)β−(n+1)/2dA 

(cid:104)fi  fj(cid:105) := c2

e− tr(A(Xi+Xj )) det(A)β−(n+1)/2dA = det(Xi + Xj)−β 
Pn
2 (n − 1). Consequently  Hβ is positive for all β deﬁned by (18).
and it exists whenever β > 1
The “only if” part follows from deeper results in the rich theory of symmetric spaces.2 Speciﬁcally 
since Pn is a symmetric cone  and 1/ det(X) is a decreasing function on this cone  (i.e.  1/ det(X +
Y ) ≤ 1/ det(X) for all X  Y > 0)  an appeal to [26; VII.3.1] grants our claim.
Remark 12. Readers versed in stochastic processes will recognize that the above result provides a
different perspective on a classical result concerning inﬁnite divisibility of Wishart processes [27] 
where the set (18) also arises as a consequence of Gindikin’s theorem [28].

At this point  it is worth mentioning the following “obvious” result.
Theorem 13. Let X be a set of positive matrices that commute with each other. Then  (X   δ(cid:96)d) can
be isometrically embedded into some Hilbert space.

Proof. The proof follows because a commuting set of matrices can be simultaneously diagonalized 
and for diagonal matrices  δ2
s (Xii  Yii)  which is a nonnegative sum of negative
deﬁnite kernels and is therefore itself negative deﬁnite.

i δ2

(cid:96)d(X  Y ) = (cid:80)

3 Connections between δ(cid:96)d and δR

After showing that δ(cid:96)d is a metric and studying its relation to kernel functions  let us now return to
our original motivation: introducing δ(cid:96)d as a reasonable alternative to the widely used Riemannian
metric δR. We note here that Cherian et al. [4; 29] offer strong experimental evidence supporting
δ(cid:96)d as an alternative; we offer more theoretical results.
Our theoretical results are based around showing that δ(cid:96)d fulﬁlls several properties akin to those
displayed by δR. Due to lack of space  we present only a summary of our results in Table 1  and
cite the corresponding theorems in the longer article [19] for proofs. While the actual proofs are
valuable and instructive  the key message worth noting is: both δR and δ(cid:96)d express the (negatively
curved) non-Euclidean geometry of their respective metric spaces by displaying similar properties.

4 Application: computing geometric means

In this section we turn our attention to an object that perhaps connects δR and δ(cid:96)d most intimately:
the operator geometric mean (GM)  which is given by the midpoint of the geodesic (3)  denoted as

A(cid:93)B := γ(1/2) = A1/2(A−1/2BA−1/2)1/2A1/2.

(21)

2Speciﬁcally  the set (18) is identical to the Wallach set which is important in the study of Hilbert spaces of

holomorphic functions over symmetric domains [26; Ch.XIII].

5

Riemannian metric
Ref.
δR(X∗AX  X∗BX) = δR(A  B)
Prop. 2
δR(A−1  B−1) = δR(A  B)
Prop. 2
δR(At  Bt) ≤ tδR(A  B)
[19; Th.4.6]
δR(As  Bs) ≤ (s/u)δR(Au  Bu)
[19; Th.4.11]
Th.14
δR(A  A(cid:93)B) = δR(B  A(cid:93)B)
[19; Th.4.7]
δR(A  A(cid:93)tB) = tδR(A  B)
δR(A(cid:93)tB  A(cid:93)tC) ≤ tδR(B  C)
[19; Th.4.8]
R(X  B) min(cid:55)→ GM
Th.14
δ2
R(X  A) + δ2
δR(A + X  A + Y ) ≤ δR(X  Y )
[19; Th.4.9]
Table 1: Some of the similarities between δR and δ(cid:96)d. All matrices are assumed to be in Pn. The
scalars t  s  u satisfy 0 < t ≤ 1  1 ≤ s ≤ u < ∞.

δ(cid:96)d-metric
δ(cid:96)d(X∗AX  X∗BX) = δ(cid:96)d(A  B)
δ(cid:96)d(A−1  B−1) = δ(cid:96)d(A  B)
δ(cid:96)d(As  Bs) ≤(cid:112)s/uδ(cid:96)d(Au  Bu)
δ(cid:96)d(At  Bt) ≤ √
tδ(cid:96)d(A  B)
δ(cid:96)d(A  A(cid:93)tB) ≤ √
δ(cid:96)d(A(cid:93)tB  A(cid:93)tC) ≤ √
tδ(cid:96)d(B  C)
(cid:96)d(X  B) min(cid:55)→ GM
δ2
(cid:96)d(X  A) + δ2
δ(cid:96)d(A + X  A + Y ) ≤ δ(cid:96)d(X  Y )

Ref.
[11; Ch.6]
[11; Ch.6]
[11; Ex.6.5.4]
[19; Th.4.11]
Trivial
[11; Th.6.1.6]
[11; Th.6.1.2]
[11; Ch. 6]
[3]

δ(cid:96)d(A  A(cid:93)B) = δ(cid:96)d(B  A(cid:93)B)
tδ(cid:96)d(A  B)

The GM (21) has numerous attractive properties—see for instance [30]—among these  the following
variational characterization is very important [31  32] 

(22)
especially because it generalizes the matrix geometric mean to more than two matrices. Speciﬁcally 
this “natural” generalization is the Karcher mean (Fr´echet mean) [31  32  11]:

A(cid:93)B = argminX>0

δ2
R(A  X) + δ2

R(B  X).

(cid:88)m

i=1

(cid:88)m

GM (A1  . . .   Am) := argminX>0

δ2
R(X  Ai).

(23)

This multivariable generalization is in fact a well-studied difﬁcult problem—see e.g.  [33] for infor-
mation on state-of-the-art. Indeed  its inordinate computational expenses motivated Cherian et al.
[4] to study the alternative mean

GM(cid:96)d(A1  . . .   Am) := argmin

X>0

φ(X) :=

i=1

δ2
(cid:96)d(X  Ai) 

(24)

which has also been more thoroughly studied by Chebbi and Moahker [2].
Although the mean (24) was previously studied in [4  2]  some crucial aspects were missing. Specif-
ically  Cherian et al. [4] only proved their solution to be a stationary point of φ(X); they did not
prove either global or local optimality. Although Chebbi and Moahker [2] showed that (24) has a
unique solution  like [4] they too only proved stationarity  neither global nor local optimality.
We ﬁll these gaps  and we make the following main contributions below:

1. We connect (24) to the Karcher mean more closely  where in Theorem 14 we shows that

for the two matrix case both problems have the same solution;

2. We show that the unique positive solution to (24) is globally optimal; this result is particu-

larly interesting because φ(X) is nonconvex.

We begin by looking at the two variable case of GM(cid:96)d (24).
Theorem 14. Let A  B > 0. Then 

A(cid:93)B = argminX>0

(cid:96)d(X  B).
Moreover  A(cid:93)B is equidistant from A and B  i.e.  δ(cid:96)d(A  A(cid:93)B) = δ(cid:96)d(B  A(cid:93)B).
Proof. If A = B  then clearly X = A minimizes φ(X). Assume therefore  that A (cid:54)= B. Ignoring
the constraint X > 0 momentarily  we see that any stationary point must satisfy ∇φ(X) = 0. Thus 

(cid:96)d(X  A) + δ2

φ(X) := δ2

(25)

∇φ(X) =(cid:0) X+A

2

2 +(cid:0) X+B
(cid:1)−1 1

2

(cid:1)−1 1
2 − X−1 = 0

=⇒ (X + A)X−1(X + B) = 2X + A + B

(26)
The latter equation is a Riccati equation that is known to have a unique  positive deﬁnite solution
given by the matrix GM (21) (see [11; Prop 1.2.13]). All that remains to show is that this GM is in
fact a local minimizer. To that end  we must show that the Hessian ∇2φ(X) > 0 at X = A(cid:93)B; but
this claim is immediate from Theorem 18. So A(cid:93)B is a strict local minimum of (8)  which is actually
a global minimum because it is the unique positive solution to φ(X) = 0. Finally  the equidistance
property follows after some algebraic manipulations; we omit details for brevity [19].

=⇒ B = XA−1X.

6

Let us now turn to the general case (24). The ﬁrst-order optimality condition is

∇φ(X) =

in a convex  compact set speciﬁed by(cid:0) 1

i=1

(cid:88)m

1
2

(cid:1)−1 − 1

(cid:0) X+Ai
(cid:80)m
i=1 A−1

2

i

m

2 mX−1 = 0 

(cid:1)−1 (cid:22) X (cid:22)(cid:0) 1

m

(cid:80)m

i=1 Ai

(cid:1).

From (27) using Lemma 15 it can be inferred that [see also 2  4] that any critical point X of (24) lies

X > 0.

(27)

Lemma 15 ([21; Ch.5]). The map X−1 on Pn is order reversing and operator convex. That is  for
−1 ≤ tX−1 +(1−t)Y −1.
X  Y ∈ Pn  if X ≥ Y   then X−1 ≤ Y −1; for t ∈ [0  1]  (tX + (1 − t)Y )
Lemma 16 ([19]). Let A  B  C  D ∈ Pn  so that A ≥ B and C ≥ D. Then  A ⊗ C ≥ B ⊗ D.
Lemma 17 (Uniqueness [2]). The nonlinear equation (27) has a unique positive solution.

Using the above results  we can ﬁnally prove the main theorem of this section.
Theorem 18. Let X be a matrix satisfying (27). Then  it is the unique global minimizer of (24).

Proof. The objective function φ(X) (24) has only one positive stationary point  which follows from
Lemma 17. Let X be this stationary point satisfying (27). We show that X is actually a local
minimum; global optimality is immediate from uniqueness of X.
To show local optimality  we prove that the Hessian ∇2φ(X) > 0. Ignoring constants  showing
positivity of the Hessian reduces to proving that

Now replace mX−1 in (28) using the condition (27); therewith inequality (28) turns into

2

1
2

i=1

mX−1 ⊗ X−1 −(cid:88)m
(cid:0) X+Ai
(cid:1)−1 ⊗(cid:0) X+Ai
(cid:16)(cid:88)m
(cid:1)−1(cid:17) ⊗ X−1 >
(cid:88)m
(cid:0) X+Ai
(cid:0) X+Ai
⇐⇒ (cid:88)m
(cid:88)m
(cid:0) X+Ai
(cid:0) X+Ai
(cid:1)−1 ⊗ X−1 >
(cid:1)−1 ⊗ (X + Ai)
(cid:1)−1 ⊗ X−1 > (cid:0) X+Ai

i=1

i=1

i=1

i=1

2

2

2

2

2

2

> 0.

(cid:1)−1
(cid:1)−1 ⊗ (X + Ai)
(cid:1)−1 ⊗ (X + Ai)

From Lemma 15 we know that X−1 > (X + Ai)

−1  so that an application of Lemma 16 shows that
−1 for 1 ≤ i ≤ m. Summing up  we obtain (29) 

which implies the desired local (and by uniqueness  global) optimality of X.
Remark 19. It is worth noting that Theorem 18 establishes that solving (27) yields the global
minimum of a nonconvex optimization problem. This result is even more remarkable because unlike
CAT(0)-metrics such as δR  the metric δ(cid:96)d is not geodesically convex.

(cid:0) X+Ai

2

(28)

(29)

−1

−1 .

4.1 Numerical Results

We present a key numerical result to illustrate the large savings in running time when computing with
δ(cid:96)d when compared with δR. To compute the Karcher mean we downloaded the “Matrix Means
Toolbox” of Bini and Iannazzo from http://bezout.dm.unipi.it/software/mmtoolbox/. In particular 
we use the ﬁle called rich.m which implements a state-of-the-art method [33].
The ﬁrst plot in Fig. 1 indicate that δ(cid:96)d can be around 5 times faster than δR2 and up to 50 times
faster than δR1. The second plot shows how expensive it can be to compute GM (23) as opposed
to GM(cid:96)d (24)—up to 1000 times! The former was computed using the method of [33]  while the
latter runs the ﬁxed-point iteration proposed in [2] (the iteration was run until (cid:107)∇φ(X)(cid:107) fell below
10−10). The key point here is not that the ﬁxed-point iteration is faster  but rather that (24) is a much
simpler problem thanks to the convenient eigenvalue free structure of δ(cid:96)d.

5 Conclusions and future work

We presented a new metric on the manifold of positive deﬁnite matrices  and related it to the classical
Riemmannian metric on this manifold. Empirically  our new metric was shown to lead to large
computational gains  while theoretically  a series of theorems demonstrated how it expresses the
negatively curved non-Euclidean geometry in a manner analogous to the Riemannian metric.

7

Figure 1: Running time comparisons between δR and δ(cid:96)d. The left panel shows time (in seconds)
taken to compute δR and δ(cid:96)d  averaged over 10 runs to reduce variance. In the plot  δR1 refers to the
implementation of δR in the matrix means toolbox [33]  while δR2 is our own implementation.

At this point  there are several directions of future work opened by our paper. We mention some of
the most relevant ones below. (i) Study further geometric properties of the metric space (Pn  δ(cid:96)d);
(ii) Further enrich the connections to δR  and to other (Finsler) metrics on Pn; (iii) Study properties
of geometric mean GM(cid:96)d (24)  including faster algorithms to compute it; (iv) Akin to [4]  apply δ(cid:96)d
in where δR has been so far dominant. We plan to tackle some of these problems  and hope that our
paper encourages other researchers in machine learning and optimization to also study them.

References

[1] H. Lee and Y. Lim. Invariant metrics  contractions and nonlinear matrix equations. Nonlinearity  21:

857–878  2008.

[2] Z. Chebbi and M. Moahker. Means of hermitian positive-deﬁnite matrices based on the log-determinant

α-divergence function. Linear Algebra and its Applications  436:1872–1889  2012.

[3] P. Bougerol. Kalman Filtering with Random Coefﬁcients and Contractions. SIAM J. Control Optim.  31

(4):942–959  1993.

[4] A. Cherian  S. Sra  A. Banerjee  and N. Papanikolopoulos. Efﬁcient Similarity Search for Covariance
Matrices via the Jensen-Bregman LogDet Divergence. In International Conference on Computer Vision
(ICCV)  Nov. 2011.

[5] F. Porikli  O. Tuzel  and P. Meer. Covariance Tracking using Model Update Based on Lie Algebra. In

IEEE CVPR  2006.

[6] L. T. Skovgaard. A Riemannian Geometry of the Multivariate Normal Model. Scandinavian Journal of

Statistics  11(4):211–223  1984.

[7] D. Petz. Quantum Information Theory and Quantum Statistics. Springer  2008.
[8] I. Dryden  A. Koloydenko  and D. Zhou. Non-Euclidean statistics for covariance matrices  with applica-

tions to diffusion tensor imaging. Annals of Applied Statistics  3(3):1102–1123  2009.

[9] H. Zhu  H. Zhang  J. G. Ibrahim  and B. S. Peterson. Statistical Analysis of Diffusion Tensors in Diffusion-
Weighted Magnetic Resonance Imaging Data. Journal of the American Statistical Association  102(480):
1085–1102  2007.

[10] F. Hiai and D. Petz. Riemannian metrics on positive deﬁnite matrices related to means. Linear Algebra

and its Applications  430:3105–3130  2009.

[11] R. Bhatia. Positive Deﬁnite Matrices. Princeton University Press  2007.
[12] M. R. Bridson and A. Haeﬂinger. Metric Spaces of Non-Positive Curvature. Springer  1999.
[13] A. Terras. Harmonic Analysis on Symmetric Spaces and Applications  volume II. Springer  1988.
[14] Yu. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Programming. SIAM 

1987.

[15] A. Ben-Tal and A. Nemirovksii. Lectures on modern convex optimization: Analysis  algorithms  and

engineering applications. SIAM  2001.

[16] Yu. Nesterov and M. J. Todd. On the riemannian geometry deﬁned for self-concordant barriers and interior

point methods. Found. Comput. Math.  2:333–361  2002.

8

050010001500200010−410−310−210−1100101102Dimensionality (n) of the matrices usedRunning time (seconds)Time taken to compute δR and δS  δR1δR2δS05010015020010−210−1100101102103Dimensionality (n) of the matrices usedRunning time (seconds)Time taken to compute GM and GMld for 10 matrices  GMGMld[17] S. Helgason. Geometric Analysis on Symmetric Spaces. Number 39 in Mathematical Surveys and Mono-

graphs. AMS  second edition  2008.

[18] H. Wolkowicz  R. Saigal  and L. Vandenberghe  editors. Handbook of Semideﬁnite Programming: Theory 

Algorithms  and Applications. Kluwer Academic  2000.

[19] S. Sra. Positive deﬁnite matrices and the Symmetric Stein Divergence. arXiv: 1110.1773  October 2012.
[20] C. Berg  J. P. R. Christensen  and P. Ressel. Harmonic analysis on semigroups: theory of positive deﬁnite

and related functions  volume 100 of GTM. Springer  1984.

[21] R. Bhatia. Matrix Analysis. Springer  1997.
[22] R. Bellman. Introduction to Matrix Analysis. SIAM  second edition  1970.
[23] M. Harandi  C. Sanderson  R. Hartley  and B. Lovell. Sparse Coding and Dictionary Learning for Sym-
In European Conference on Computer Vision

metric Positive Deﬁnite Matrices: A Kernel Approach.
(ECCV)  2012.

[24] M. Cuturi  K. Fukumizu  and J. P. Vert. Semigroup kernels on measures. JMLR  6:1169–1198  2005.
[25] R. J. Muirhead. Aspects of multivariate statistical theory. Wiley Interscience  1982.
[26] J. Faraut and A. Kor´anyi. Analysis on Symmetric Cones. Clarendon Press  1994.
[27] M.-F. Bru. Wishart Processes. J. Theoretical Probability  4(4)  1991.
[28] S. G. Gindikin. Invariant generalized functions in homogeneous domains. Functional Analysis and its

Applications  9:50–52  1975.

[29] A. Cherian  S. Sra  A. Banerjee  and N. Papanikolopoulos. Jensen-Bregman LogDet Divergence with

Application to Efﬁcient Similarity Search for Covariance Matrices. IEEE TPAMI  2012. Submitted.

[30] T. Ando. Concavity of certain maps on positive deﬁnite matrices and applications to hadamard products.

Linear Algebra and its Applications  26(0):203–241  1979.

[31] R. Bhatia and J. A. R. Holbrook. Riemannian geometry and matrix geometric means. Linear Algebra

Appl.  413:594–618  2006.

[32] M. Moakher. A differential geometric approach to the geometric mean of symmetric positive-deﬁnite

matrices. SIAM J. Matrix Anal. Appl. (SIMAX)  26:735–747  2005.

[33] D. A. Bini and B. Iannazzo. Computing the Karcher mean of symmetric positive deﬁnite matrices. Linear

Algebra and its Applications  Oct. 2011. Available online.

9

,Yao-Liang Yu
Chris Oates
Steven Niederer
Angela Lee
François-Xavier Briol
Mark Girolami
Vladimir Kniaz
Vladimir Knyaz
Fabio Remondino