2013,Mixed Optimization for Smooth Functions,It is well known that the optimal convergence rate for stochastic optimization of smooth functions is $[O(1/\sqrt{T})]$  which is same as stochastic optimization of Lipschitz continuous convex functions. This is in contrast to optimizing smooth functions using full gradients  which yields a convergence rate of $[O(1/T^2)]$. In this work  we consider a new setup for optimizing smooth functions  termed as {\bf Mixed Optimization}  which allows to access  both a stochastic oracle  and a full gradient oracle. Our goal is to significantly improve the convergence rate of stochastic optimization of smooth functions by having an additional small number of accesses to the full gradient oracle. We show that  with an $[O(\ln T)]$ calls to the full gradient oracle  and an $O(T)$ calls to the stochastic oracle  the proposed mixed optimization algorithm is able to achieve an optimization error of $[O(1/T)]$.,Mixed Optimization for Smooth Functions

Department of Computer Science and Engineering  Michigan State University  MI  USA

Rong Jin
Mehrdad Mahdavi
fmahdavim zhanglij rongjing@msu.edu

Lijun Zhang

Abstract

p
It is well known that the optimal convergence rate for stochastic optimization of
smooth functions is O(1=
T )  which is same as stochastic optimization of Lips-
chitz continuous convex functions. This is in contrast to optimizing smooth func-
tions using full gradients  which yields a convergence rate of O(1=T 2). In this
work  we consider a new setup for optimizing smooth functions  termed as Mixed
Optimization  which allows to access both a stochastic oracle and a full gradient
oracle. Our goal is to signiﬁcantly improve the convergence rate of stochastic op-
timization of smooth functions by having an additional small number of accesses
to the full gradient oracle. We show that  with an O(ln T ) calls to the full gradient
oracle and an O(T ) calls to the stochastic oracle  the proposed mixed optimization
algorithm is able to achieve an optimization error of O(1=T ).

1 Introduction

Many machine learning algorithms follow the framework of empirical risk minimization  which
often can be cast into the following generic optimization problem

n∑

min
w2W

G(w) :=

1
n

gi(w);

i=1

(1)

where n is the number of training examples  gi(w) encodes the loss function related to the ith
training example (xi; yi)  and W is a bounded convex domain that is introduced to regularize
the solution w 2 W (i.e.  the smaller the size of W  the stronger the regularization is). In this
study  we focus on the learning problems for which the loss function gi(w) is smooth. Examples of
smooth loss functions include least square with gi(w) = (yi(cid:0)⟨w; xi⟩)2 and logistic regression with
gi(w) = log (1 + exp((cid:0)yi⟨w; xi⟩)). Since the regularization is enforced through the restricted do-
main W  we did not introduce a ℓ2 regularizer (cid:21)∥w∥2=2 into the optimization problem and as a
result  we do not assume the loss function to be strongly convex. We note that a small ℓ2 regularizer
p
does NOT improve the convergence rate of stochastic optimization. More speciﬁcally  the conver-
p
T ) when
gence rate for stochastically optimizing a ℓ2 regularized loss function remains as O(1=
T ) [11  Theorem 1]  a scenario that is often encountered in real-world applications.
(cid:21) = O(1=
A preliminary approach for solving the optimization problem in (1) is the batch gradient descent
(GD) algorithm [16]. It starts with some initial point  and iteratively updates the solution using the
equation wt+1 = (cid:5)W (wt (cid:0) (cid:17)rG(wt))  where (cid:5)W ((cid:1)) is the orthogonal projection onto the convex
domain W. It has been shown that for smooth objective functions  the convergence rate of standard
GD is O(1=T ) [16]  and can be improved to O(1=T 2) by an accelerated GD algorithm [15  16  18].
The main shortcoming of GD method is its high cost in computing the full gradient rG(wt) when
the number of training examples is large. Stochastic gradient descent (SGD) [3  13  21] alleviates
this limitation of GD by sampling one (or a small set of) examples and computing a stochastic
(sub)gradient at each iteration based on the sampled examples. Since the computational cost of
SGD per iteration is independent of the size of the data (i.e.  n)  it is usually appealing for large-
scale learning and optimization.
While SGD enjoys a high computational efﬁciency per iteration  it suffers from a slow convergence
rate for optimizing smooth functions. It has been shown in [14] that the effect of the stochastic noise

1

Setting
Lipschitz
Smooth

Full (GD)

Convergence Os Of

1p
T
1
T 2

1

0
0

T

T

Stochastic (SGD)

Convergence Os Of
0
0

T

T

1p
T
1p
T

Mixed Optimization
Of
Convergence Os
— —

T

log T

—
1
T

Table 1: The convergence rate (O)  number of calls to stochastic oracle (Os)  and number of calls to
full gradient oracle (Of ) for optimizing Lipschitz continuous and smooth convex functions  using
p
full GD  SGD  and mixed optimization methods  measured in the number of iterations T .
cannot be decreased with a better rate than O(1=
T ) which is signiﬁcantly worse than GD that uses
the full gradients for updating the solutions and this limitation is also valid when the target function
is smooth. In addition  as we can see from Table 1  for general Lipschitz functions  SGD exhibits
the same convergence rate as that for the smooth functions  implying that smoothness is essentially
not very useful and can not be exploited in stochastic optimization. The slow convergence rate for
stochastically optimizing smooth loss functions is mostly due to the variance in stochastic gradients:
unlike the full gradient case where the norm of a gradient approaches to zero when the solution is
approaching to the optimal solution  in stochastic optimization  the norm of a stochastic gradient
p
is constant even when the solution is close to the optimal solution. It is the variance in stochastic
T ) unimprovable in smooth setting [14  1].
gradients that makes the convergence rate O(1=

In this study  we are interested in designing an efﬁcient algorithm that is in the same spirit of SGD
but can effectively leverage the smoothness of the loss function to achieve a signiﬁcantly faster
convergence rate. To this end  we consider a new setup for optimization that allows us to interplay
between stochastic and deterministic gradient descent methods. In particular  we assume that the
optimization algorithm has an access to two oracles:
(cid:15) A stochastic oracle Os that returns the loss function gi(w) and its gradient based on the
sampled training example (xi; yi) 2  and
(cid:15) A full gradient oracle Of that returns the gradient rG(w) for any given solution w 2 W.
We refer to this new setting as mixed optimization in order to distinguish it from both stochastic and
full gradient optimization models. The key question we examined in this study is:
functions by having a small number of calls to the full gradient oracle Of ?

Is it possible to improve the convergence rate for stochastic optimization of smooth

We give an afﬁrmative answer to this question. We show that with an additional O(ln T ) accesses
to the full gradient oracle Of   the proposed algorithm  referred to as MIXEDGRAD  can improve
the convergence rate for stochastic optimization of smooth functions to O(1=T )  the same rate for
stochastically optimizing a strongly convex function [11  19  23]. MIXEDGRAD builds off on multi-
stage methods [11] and operates in epochs  but involves novel ingredients so as to obtain an O(1=T )
rate for smooth losses. In particular  we form a sequence of strongly convex objective functions to
be optimized at each epoch and decrease the amount of regularization and shrink the domain as the
algorithm proceeds. The full gradient oracle Of is only called at the beginning of each epoch.
Finally  we would like to distinguish mixed optimization from hybrid methods that use growing
sample-sizes as optimization method proceeds to gradually transform the iterates into the full gra-
dient method [9] and batch gradient with varying sample sizes [6]  which unfortunately make the
iterations to be dependent to the sample size n as opposed to SGD. In contrast  MIXEDGRAD is as
an alternation of deterministic and stochastic gradient steps  with different of frequencies for each
type of steps. Our result for mixed optimization is useful for the scenario when the full gradient
of the objective function can be computed relatively efﬁcient although it is still signiﬁcantly more
expensive than computing a stochastic gradient. An example of such a scenario is distributed com-
puting where the computation of full gradients can be speeded up by having it run in parallel on
many machines with each machine containing a relatively small subset of the entire training data.
Of course  the latency due to the communication between machines will result in an additional cost
for computing the full gradient in a distributed fashion.
Outline The rest of this paper is organized as follows. We begin in Section 2 by brieﬂy reviewing
the literature on deterministic and stochastic optimization. In Section 3  we introduce the necessary
deﬁnitions and discuss the assumptions that underlie our analysis. Section 4 describes the MIXED-
GRAD algorithm and states the main result on its convergence rate. The proof of main result is given
in Section 5. Finally  Section 6 concludes the paper and discusses few open questions.

1The convergence rate can be improved to O(1=T ) when the structure of the objective function is provided.
2We note that the stochastic oracle assumed in our study is slightly stronger than the stochastic gradient

oracle as it returns the sampled function instead of the stochastic gradient.

2

2 More Related Work

Deterministic Smooth Optimization The convergence rate of gradient based methods usually
depends on the analytical properties of the objective function to be optimized. When the objective
function is strongly convex and smooth  it is well known that a simple GD method can achieve a
p
p
linear convergence rate [5]. For a non-smooth Lipschitz-continuous function  the optimal rate for
the ﬁrst order method is only O(1=
T ) [16]. Although O(1=
T ) rate is not improvable in general 
several recent studies are able to improve this rate to O(1=T ) by exploiting the special structure
of the objective function [18  17]. In the full gradient based convex optimization  smoothness is
a highly desirable property. It has been shown that a simple GD achieves a convergence rate of
O(1=T ) when the objective function is smooth  which is further can be improved to O(1=T 2) by
using the accelerated gradient methods [15  18  16].

Stochastic Smooth Optimization Unlike the optimization methods based on full gradients  the
p
smoothness assumption was not exploited by most stochastic optimization methods. In fact  it was
shown in [14] that the O(1=
T ) convergence rate for stochastic optimization cannot be improved
even when the objective function is smooth. This classical result is further conﬁrmed by the recent
studies of composite bounds for the ﬁrst order optimization methods [2  12]. The smoothness of
the objective function is exploited extensively in mini-batch stochastic optimization [7  8]  where
the goal is not to improve the convergence rate but to reduce the variance in stochastic gradients
and consequentially the number of times for updating the solutions [24]. We ﬁnally note that the
smoothness assumption coupled with the strong convexity of function is beneﬁcial in stochastic
setting and yields a geometric convergence in expectation using Stochastic Average Gradient (SAG)
and Stochastic Dual Coordinate Ascent (SDCA) algorithms proposed in [20] and [22]  respectively.

3 Preliminaries

) + ⟨rf (w

′

); w (cid:0) w

′⟩ +

(cid:12)
2

f (w) (cid:20) f (w

′ 2 W  we denote by ⟨w; w
′⟩
We use bold-face letters to denote vectors. For any two vectors w; w
′. Throughout this paper  we only consider the ℓ2-norm. We
the inner product between w and w
assume the objective function G(w) deﬁned in (1) to be the average of n convex loss functions. The
same assumption was made in [20  22]. We assume that G(w) is minimized at some w(cid:3) 2 W. With-
out loss of generality  we assume that W (cid:26) BR  a ball of radius R. Besides convexity of individual
functions  we will also assume that each gi(w) is (cid:12)-smooth as formally deﬁned below [16].
Deﬁnition 1 (Smoothness). A differentiable loss function f (w) is said to be (cid:12)-smooth with respect
to a norm ∥ (cid:1) ∥  if it holds that
′

′∥2;
∥w (cid:0) w
The smoothness assumption also implies that ⟨rf (w) (cid:0) rf (w
′∥2 which
); w (cid:0) w
′
is equivalent to rf (w) being (cid:12)-Lipschitz continuous.
In stochastic ﬁrst-order optimization setting  instead of having direct access to G(w)  we only have
access to a stochastic gradient oracle  which given a solution w 2 W  returns the gradient rgi(w)
where i is sampled uniformly at random from f1; 2;(cid:1)(cid:1)(cid:1) ; ng. The goal of stochastic optimization
to use a bounded number T of oracle calls  and compute some (cid:22)w 2 W such that the optimization
error  G( (cid:22)w) (cid:0) G(w
In the mixed optimization model considered in this study  we ﬁrst relax the stochastic oracle Os by
assuming that it will return a randomly sampled loss function gi(w)  instead of the gradient rgi(w)
for a given solution w 3. Second  we assume that the learner also has an access to the full gradient
oracle Of . Our goal is to signiﬁcantly improve the convergence rate of stochastic gradient descent
(SGD) by making a small number of calls to the full gradient oracle Of . In particular  we show that
by having only O(log T ) accesses to the full gradient oracle and O(T ) accesses to the stochastic
oracle  we can tolerate the noise in stochastic gradients and attain an O(1=T ) convergence rate for
optimizing smooth functions.

)  is as small as possible.

′⟩ (cid:20) (cid:12)∥w (cid:0) w

8 w; w

′ 2 W;

(cid:3)

3The audience may feel that this relaxation of stochastic oracle could provide signiﬁcantly more informa-
tion  and second order methods such as Online Newton [10] may be applied to achieve O(1=T ) convergence.
p
We note (i) the proposed algorithm is a ﬁrst order method  and (ii) although the Online Newton method yields a
regret bound of O(1=T )  its convergence rate for optimization can be as low as O(1=
T ) due to the concentra-
tion bound for Martingales. In addition  the Online Newton method is only applicable to exponential concave
function  not any smooth loss function.

3

Algorithm 1 MIXEDGRAD
Input: step size (cid:17)1  domain size ∆1  the number of iterations T1 for the ﬁrst epoch  the number of
epoches m  regularization parameter (cid:21)1  and shrinking parameter (cid:13) > 1
1: Initialize (cid:22)w1 = 0
2: for k = 1; : : : ; m do
3:
4:
5:
6:
7:
8:
9:
10:

Construct the domain Wk = fw : w + wk 2 W;∥w∥ (cid:20) ∆kg
Call the full gradient oracle Of for rG( (cid:22)wk)
Compute gk = (cid:21)k (cid:22)wk + rG( (cid:22)wk) = (cid:21)k (cid:22)wk + 1
rgi( (cid:22)wk)
Initialize w1
for t = 1; : : : ; Tk do
Call stochastic oracle Os to return a randomly selected loss function git
k + (cid:22)wk) (cid:0) rgit
Compute the stochastic gradient as ^gt
Update the solution by
∥w (cid:0) wt
⟩ +

(w)
( (cid:22)wk)

∑

k = 0

wt+1

n
i=1

(wt

∥2

n

k

k

k + (cid:21)kwt
k

k

1
2

k

k = gk + rgit
k and (cid:22)wk+1 = (cid:22)wk +ewk+1

(cid:17)k⟨w (cid:0) wt

k; ^gt

Set ewk+1 = 1

end for

Tk+1

∑

k = arg max
w2Wk

Tk+1
t=1 wt

Set ∆k+1 = ∆k=(cid:13)  (cid:21)k+1 = (cid:21)k=(cid:13)  (cid:17)k+1 = (cid:17)k=(cid:13)  and Tk+1 = (cid:13)2Tk

11:
12:
13:
14: end for
Return (cid:22)wm+1

The analysis of the proposed algorithm relies on the strong convexity of intermediate loss functions
introduced to facilitate the optimization as given below.
Deﬁnition 2 (Strong convexity). A function f (w) is said to be (cid:11)-strongly convex w.r.t a norm ∥ (cid:1) ∥ 
if there exists a constant (cid:11) > 0 (often called the modulus of strong convexity) such that it holds

f (w) (cid:21) f (w

′

) + ⟨rf (w

′

); w (cid:0) w

′⟩ +

∥w (cid:0) w

′∥2;

8 w; w

′ 2 W

(cid:11)
2

4 Mixed Stochastic/Deterministic Gradient Descent

We now turn to describe the proposed mixed optimization algorithm and state its convergence rate.
The detailed steps of MIXEDGRAD algorithm are shown in Algorithm 1.
It follows the epoch
gradient descent algorithm proposed in [11] for stochastically minimizing strongly convex functions
and divides the optimization process into m epochs  but involves novel ingredients so as to obtain an
O(1=T ) convergence rate. The key idea is to introduce a ℓ2 regularizer into the objective function to
make it strongly convex  and gradually reduce the amount of regularization over the epochs. We also
shrink the domain as the algorithm proceeds. We note that reducing the amount of regularization
over time is closely-related to the classic proximal-point algorithms. Throughout the paper  we will
use the subscript for the index of each epoch  and the superscript for the index of iterations within
each epoch. Below  we describe the key idea behind MIXEDGRAD.
Let (cid:22)wk be the solution obtained before the kth epoch  which is initialized to be 0 for the ﬁrst epoch.
Instead of searching for w(cid:3) at the kth epoch  our goal is to ﬁnd w(cid:3) (cid:0) (cid:22)wk  resulting in the following
optimization problem for the kth epoch
(cid:21)k
2

∥w + (cid:22)wk∥2 +

gi(w + (cid:22)wk);

n∑

min

1
n

(2)

i=1

w + wk 2 W
∥w∥ (cid:20) ∆k

where ∆k speciﬁes the domain size of w and (cid:21)k is the regularization parameter introduced at the
kth epoch. By introducing the ℓ2 regularizer  the objective function in (2) becomes strongly convex 
making it possible to exploit the technique for stochastic optimization of strongly convex function
in order to improve the convergence rate. The domain size ∆k and the regularization parameter (cid:21)k
are initialized to be ∆1 > 0 and (cid:21)1 > 0  respectively  and are reduced by a constant factor (cid:13) > 1
every epoch  i.e.  ∆k = ∆1=(cid:13)k(cid:0)1 and (cid:21)k = (cid:21)1=(cid:13)k(cid:0)1. By removing the constant term (cid:21)k∥ (cid:22)wk∥2=2
from the objective function in (2)  we obtain the following optimization problem for the kth epoch

]

n∑

[
Fk(w) =

min
w2Wk

gi(w + (cid:22)wk)

;

i=1

(3)

∥w∥2 + (cid:21)k⟨w; (cid:22)wk⟩ +

(cid:21)k
2

1
n

4

=

+

i=1

i=1

i=1

1
n

1
n

1
n

1
n

(cid:21)k
2

(cid:21)k
2

i (w)

where

⟨

w; (cid:21)k (cid:22)wk +

n∑

gi(w + (cid:22)wk)

∥w∥2 +

=

(cid:21)k
2

∥w∥2 + ⟨w; gk⟩ +

gi(w + (cid:22)wk) (cid:0) ⟨w;rgi( (cid:22)wk)⟩

⟩
rgi( (cid:22)wk)

where Wk = fw : w + wk 2 W; ∥w∥ (cid:20) ∆kg. We rewrite the objective function Fk(w) as
Fk(w) =

n∑
∥w∥2 + (cid:21)k⟨w; (cid:22)wk⟩ +
n∑
n∑
bgk
rgi( (cid:22)wk) and bgk
(cid:13)(cid:13) = ∥rgi(w + (cid:22)wk) (cid:0) rgi( (cid:22)wk)∥ (cid:20) (cid:12)∥w∥:

n∑
The main reason for usingbgk
the norm ofbgk
i (w) as:(cid:13)(cid:13)rbgk
epochs and consequentially ∥rbgk
As a result  since ∥w∥ (cid:20) ∆k and ∆k shrinks over epochs  then ∥w∥ will approach to zero over
i (w)∥ approaches to zero  which allows us to effectively control
the variance in stochastic gradients  a key to improving the convergence of stochastic optimization
for smooth functions to O(1=T ).
Using Fk(w) in (4)  at the tth iteration of the kth epoch  we call the stochastic oracle Os to randomly
(w) and update the solution by following the standard paradigm of SGD by
select a loss function gik

i (w) instead of gi(w) is to tolerate the variance in the stochastic gradi-
ents. To see this  from the smoothness assumption of gi(w) we obtain the following inequality for

i (w) = gi(w + (cid:22)wk) (cid:0) ⟨w;rgi( (cid:22)wk)⟩:

gk = (cid:21)k (cid:22)wk +

i (w)

1
n

(4)

i=1

i=1

t

(
(

)
k))
k + (cid:22)wk) (cid:0) rgit

k + gk + rbgk
k + gk + rgit

k

it
k

wt
k

wt
k

(wt

wt+1

= (cid:5)w2Wk

= (cid:5)w2Wk

(cid:0) (cid:17)k((cid:21)kwt
(cid:0) (cid:17)k((cid:21)kwt

)
( (cid:22)wk))
At the end of each epoch  we compute the average solution ewk  and update the solution from (cid:22)wk to
where (cid:5)w2Wk (w) projects the solution w into the domain Wk that shrinks over epochs.
(cid:22)wk+1 = (cid:22)wk +ewk. Similar to the epoch gradient descent algorithm [11]  we increase the number of
iterations by a constant (cid:13)2 for every epoch  i.e. Tk = T1(cid:13)2(k(cid:0)1).
In order to perform stochastic gradient updating given in (5)  we need to compute vector gk at the
beginning of the kth epoch  which requires an access to the full gradient oracle Of . It is easy to
count that the number of accesses to the full gradient oracle Of is m  and the number of accesses to
the stochastic oracle Os is

(wt

(5)

;

k

k

m∑

(cid:13)2(i(cid:0)1) =

(cid:13)2m (cid:0) 1
(cid:13)2 (cid:0) 1

T1:

T = T1

i=1

Thus  if the total number of accesses to the stochastic gradient oracle is T   the number of access to
the full gradient oracle required by Algorithm 1 is O(ln T )  consistent with our goal of making a
small number of calls to the full gradient oracle.
The theorem below shows that for smooth objective functions  by having O(ln T ) access to the
full gradient oracle Of and O(T ) access to the stochastic oracle Os  by running MIXEDGRAD
algorithm  we achieve an optimization error of O(1=T ).
Theorem 1. Let (cid:14) (cid:20) e

(cid:0)9=2 be the failure probability. Set (cid:13) = 2  (cid:21)1 = 16(cid:12) and
T1 = 300 ln

; and ∆1 = R:

(cid:17)1 =

;

m
(cid:14)

p
1
3T1

2(cid:12)

Deﬁne T = T1
=3. Let (cid:22)wm+1 be the solution returned by Algorithm 1 after m epochs
with m = O(ln T ) calls to the full gradient oracle Of and T calls to the stochastic oracle Os. Then 
with a probability 1 (cid:0) 2(cid:14)  we have

(

)
22m (cid:0) 1

(

)

(cid:12)
T

:

G( (cid:22)wm+1) (cid:0) min
w2W

G(w) (cid:20) 80(cid:12)R2

22m(cid:0)2 = O

5

5 Convergence Analysis

following theorem.

Now we turn to proving the main theorem. The proof will be given in a series of lemmas and
theorems where the proof of few are given in the Appendix. The proof of main theorem is based

on induction. To this end  let bwk(cid:3) be the optimal solution that minimizes Fk(w) deﬁned in (3).
The key to our analysis is show that when ∥bwk(cid:3)∥ (cid:20) ∆k  with a high probability  it holds that
∥ (cid:20) ∆k=(cid:13)  where bwk+1(cid:3)
∥bwk+1(cid:3)
is the optimal solution that minimizes Fk+1(w)  as revealed by the
Theorem 2. Let bwk(cid:3) and bwk+1(cid:3)
spectively  and ewk+1 be the average solution obtained at the end of kth epoch of MIXEDGRAD
be the optimal solutions that minimize Fk(w) and Fk+1(w)  re-
algorithm. Suppose ∥bwk(cid:3)∥ (cid:20) ∆k. By setting the step size (cid:17)k = 1=
  we have  with a
and Fk(ewk+1) (cid:0) min
∥bwk+1(cid:3)
Tk (cid:21) 300(cid:13)8(cid:12)2

Fk(w) (cid:20) (cid:21)k∆2
k
2(cid:13)4

provided that (cid:14) (cid:20) e

probability 1 (cid:0) 2(cid:14) 

∥ (cid:20) ∆k
(cid:13)

p
3Tk

(cid:0)9=2 and

(

)

2(cid:12)

ln

w

:

1
(cid:14)

(cid:21)2
k

1
n

i=1

(cid:3)

m

2(cid:13)4 =

(cid:21)1∆2
1
2(cid:13)3m+1

be the optimal solution ob-

⟨ewm+1; (cid:22)wm⟩

∥w1(cid:3)∥ = ∥w(cid:3)∥ (cid:20) R := ∆1:

(cid:0) (cid:21)1
(cid:13)m(cid:0)1
(cid:21)1∥ (cid:22)wm∥∆1

∥bwm(cid:3) ∥ (cid:20) ∆1
n∑

Hence by expanding the left hand side and utilizing the smoothness of individual loss functions we
get

tained in the last epoch. Using Theorem 2  with a probability 1 (cid:0) 2m(cid:14)  we have
(cid:21)1∆2
1
2(cid:13)3m+1

Taking this statement as given for the moment  we proceed with the proof of Theorem 1  returning
later to establish the claim stated in Theorem 2.
Proof of Theorem 1. It is easy to check that for the ﬁrst epoch  using the fact W 2 BR  we have
Let wm(cid:3) be the optimal solution that minimizes Fm(w) and let bwm+1
(cid:13)m(cid:0)1 ; Fm(ewm+1) (cid:0) Fm(bwm(cid:3) ) (cid:20) (cid:21)m∆2
gi( (cid:22)wm+1) (cid:20) Fm(bwm(cid:3) ) +
(cid:20) Fm(bwm(cid:3) ) +
where the last step uses the fact ∥bwm+1
∥ (cid:22)wm∥ (cid:20) m∑
jewij (cid:20) m∑
∥ (cid:20) ∆m = ∆1(cid:13)1(cid:0)m. Since
n∑
gi( (cid:22)wm+1) (cid:20) Fm(bwm(cid:3) ) +
n∑

Our ﬁnal goal is to relate Fm(w) to minw G(w). Since bwm(cid:3) minimizes Fm(w)  for any w(cid:3) 2
Fm(wm(cid:3) ) (cid:20) Fm(w(cid:3)) =
(6)
Thus  the key to bound jF(wm(cid:3) ) (cid:0) G(w(cid:3))j is to bound ∥w(cid:3) (cid:0) (cid:22)wm∥. To this end  after the ﬁrst m
will hold deterministically as we deploy the full gradient for updating  i.e.  ∥ewk∥ (cid:20) ∆k for any
epoches  we run Algorithm 1 with full gradients. Let (cid:22)wm+1; (cid:22)wm+2; : : : be the sequence of solutions
generated by Algorithm 1 after the ﬁrst m epochs. For this sequence of solutions  Theorem 2
k (cid:21) m + 1. Since we reduce (cid:21)k exponentially  (cid:21)k will approach to zero and therefore the sequence
f (cid:22)wkg1
k=m+1 will converge to w(cid:3)  one of the optimal solutions that minimize G(w). Since w(cid:3) is the
limit of sequence f (cid:22)wkg1

where in the last step holds under the condition (cid:13) (cid:21) 2. By combining above inequalities  we obtain

(∥w(cid:3) (cid:0) (cid:22)wm∥2 + 2⟨w(cid:3) (cid:0) (cid:22)wm; (cid:22)wm⟩)

1
n
arg minG(w)  we have

∆i (cid:20) (cid:13)∆1
(cid:13) (cid:0) 1

(cid:21)1∆2
1
2(cid:13)3m+1 +

(cid:21)1∆2
1
2(cid:13)3m+1 +

2(cid:21)1∆2
1
(cid:13)2m(cid:0)2 :

(cid:20) 2∆1

gi(w(cid:3)) +

(cid:21)1

2(cid:13)m(cid:0)1

(cid:13)2m(cid:0)2

i=1

i=1

k=m+1 and ∥ (cid:22)wk∥ (cid:20) ∆k for any k (cid:21) m + 1  we have
(cid:13)m(1 (cid:0) (cid:13)(cid:0)1)

jewij (cid:20)

∆k (cid:20)

∆1

∥w(cid:3) (cid:0) (cid:22)wm∥ (cid:20)

(cid:20) 2∆1
(cid:13)m

1
n

i=1

i=1

(cid:3)

1∑

:

1∑

6

i=m+1

k=m+1

where the last step follows from the condition (cid:13) (cid:21) 2. Thus 
4∆2
1
(cid:13)2m +
(cid:0)m

Fm(wm(cid:3) ) (cid:20) 1

gi(w(cid:3)) +

(cid:21)1

n

=

gi(w(cid:3)) +

2 + (cid:13)

n∑
n∑

i=1

(
(

)
) (cid:20) 1

8∆2
1
(cid:13)m

n

n∑

gi(w(cid:3)) +

By combining the bounds in (6) and (7)  we have  with a probability 1 (cid:0) 2m(cid:14) 

i=1

i=1

1
n

n∑

i=1

1
n

where

(cid:13)2m(cid:0)2 = O(1=T )

gi( (cid:22)wm+1) (cid:0) 1
m(cid:0)1∑

n

T = T1

k=0

(cid:13)2k =

1

gi(w(cid:3)) (cid:20) 5(cid:21)1∆2
(
)
(cid:13)2m (cid:0) 1
(cid:13)2 (cid:0) 1

(cid:20) T1
3

T1

(cid:13)2m:

2(cid:13)m(cid:0)1
2(cid:21)1∆2
1
(cid:13)2m(cid:0)1

n∑

i=1

5(cid:21)1∆2
1
(cid:13)2m(cid:0)1

(7)

We complete the proof by plugging in the stated values for (cid:13)  (cid:21)1 and ∆1.

5.1 Proof of Theorem 2

n∑
n∑

i=1

i=1

1
n
′⟩ +

For the convenience of discussion  we drop the subscript k for epoch just to simplify our notation.
Let (cid:21) = (cid:21)k  T = Tk  ∆ = ∆k  g = gk. Let (cid:22)w = (cid:22)wk be the solution obtained before the start of
′
= (cid:22)wk+1 be the solution obtained after running through the kth epoch. We
the epoch k  and let (cid:22)w
denote by F(w) and F′
(w) the objective functions Fk(w) and Fk+1(w). They are given by
F(w) =

∥w∥2 + (cid:21)⟨w; (cid:22)w⟩ +

gi(w + (cid:22)w)

(8)

(cid:21)
2

′

+

2(cid:17)

′

)

(cid:21)
(cid:13)

F′
′

∥w∥2 +

⟨w; (cid:22)w

+

(cid:17)
2

(9)

1
n

2(cid:13)4

⟨

(w) =

gi(w + (cid:22)w

Lemma 1.

(w) over the

; F( (cid:22)w

(cid:3)∥ (cid:20) ∆
′
(cid:13)

) (cid:0) F (bw(cid:3)) (cid:20) (cid:21)∆2

∥rbgit(wt) + (cid:21)wt∥2 + ⟨g; wt (cid:0) wt+1⟩

(cid:21)
2(cid:13)
be the optimal solutions that minimize F(w) and F′

(cid:3) = bwk+1(cid:3)
Let bw(cid:3) = bwk(cid:3) and bw
domain Wk and Wk+1  respectively. Under the assumption that ∥bw(cid:3)∥ (cid:20) ∆  our goal is to show
∥bw
The following lemma bounds F(wt) (cid:0) F (bw(cid:3)) where the proof is deferred to Appendix.
F(wt) (cid:0) F (bw(cid:3)) (cid:20) ∥wt (cid:0)bw(cid:3)∥2

(cid:0) ∥wt+1 (cid:0)bw(cid:3)∥2
⟩
⟨
rbF(bw(cid:3)) (cid:0) rbgit(bw(cid:3)); wt (cid:0)bw(cid:3)
(cid:0)rbgit(wt) + rbgit (bw(cid:3)) (cid:0) rbF(bw(cid:3)) + rbF(wt); wt (cid:0)bw(cid:3)
T∑
F(wt) (cid:0) F(bw(cid:3)) (cid:20) ∥bw(cid:3)∥2
(cid:0) ∥wT +1 (cid:0)bw(cid:3)∥2
T∑
T∑
⟨rbF(bw(cid:3)) (cid:0) rbgit (bw(cid:3)); wt (cid:0)bw(cid:3)⟩
∥rbgit(wt) + (cid:21)wt∥2
}
}
|
|
⟩
⟨
T∑
(cid:0)rbgit(wt) + rbgit (bw(cid:3)) (cid:0) rbF(bw(cid:3)) + rbF(wt); wt (cid:0)bw(cid:3)
|
}
Since g = rF(0) and

By adding the inequality in Lemma 1 over all iterations  using the fact (cid:22)w1 = 0  we have

(cid:0) ⟨g; wT +1⟩
{z

{z

{z

:=BT

:=AT

2(cid:17)

+

+

(cid:17)
2

:=CT

2(cid:17)

+

t=1

+

t=1

2(cid:17)

t=1

t=1

:

F(wT +1) (cid:0) F (0) (cid:20) ⟨rF(0); wT +1⟩ +

∥wT +1∥2 = ⟨g; wT +1⟩ +

∥wT +1∥2

⟩

(cid:12)
2

(cid:12)
2

7

using the fact F(0) (cid:20) F(w(cid:3)) + (cid:12)

and therefore

(cid:0)⟨g; wT +1⟩ (cid:20) F(0) (cid:0) F (wT +1) +
(
F(wt) (cid:0) F (bw(cid:3)) (cid:20) ∆2

T +1∑

1
2(cid:17)

t=1

2

∆2 (cid:20) (cid:12)∆2 (cid:0) (F(wT +1) (cid:0) F(bw(cid:3)))
∥w(cid:3)∥2 and max(∥w(cid:3)∥;∥wT +1∥) (cid:20) ∆  we have
)

(cid:12)
2

+ (cid:12)

+

AT + BT + CT :

(10)

(cid:17)
2

The following lemmas bound AT   BT and CT .
Lemma 2. For AT deﬁned above we have AT (cid:20) 6(cid:12)2∆2T .
The following lemma upper bounds BT and CT . The proof is based on the Bernstein’s inequality
)
for Martingales [4] and is given in the Appendix.
Lemma 3. With a probability 1 (cid:0) 2(cid:14)  we have

(

)

(

√

√

Using Lemmas 2 and 3  by substituting the uppers bounds for AT   BT   and CT in (10)  with a
probability 1 (cid:0) 2(cid:14)  we obtain

ln

+

2T ln

:

1
(cid:14)

)

1
(cid:14)

)
√

1
(cid:14)

+ 3(cid:12)

2T ln

√

√

t=1

1
(cid:14)

1
(cid:14)

1
(cid:14)

ln

+

2T ln

1
(cid:14)

(

1
2(cid:17)

(

+ (cid:12) + 6(cid:12)2(cid:17)T + 3(cid:12) ln

By choosing (cid:17) = 1=[2(cid:12)

and CT (cid:20) 2(cid:12)∆2

p
3T ]  we have

BT (cid:20) (cid:12)∆2
T +1∑
F(wt) (cid:0) F(bw(cid:3)) (cid:20) ∆2
T +1∑
F(wt) (cid:0) F(bw(cid:3)) (cid:20) ∆2
√
F(ew) (cid:0) F (bw(cid:3)) (cid:20) ∆2 5(cid:12)
p
3 ln[1=(cid:14)]
T + 1
b∆2 (cid:20) ∆2
(cid:14) ]=(cid:21)2  we have  with a probability 1 (cid:0) 2(cid:14) 
(cid:13)4 ; and jF(ew) (cid:0) F (bw(cid:3))j (cid:20) (cid:21)
(cid:3)∥ to ∥ew (cid:0)bw(cid:3)∥.
2(cid:13)4 ∆2:
(cid:3)∥ (cid:20) (cid:13)∥ew (cid:0)bw(cid:3)∥.

The next lemma relates ∥bw
Lemma 4. We have ∥bw
Combining the bound in (11) with Lemma 4  we have ∥bw

and using the fact ew =

Thus  when T (cid:21) [300(cid:13)8(cid:12)2 ln 1

p
3T + (cid:12) + 3(cid:12) ln

i=1 wt=(T + 1)  we have

(cid:3)∥ (cid:20) ∆=(cid:13).
′

∑

1
(cid:14)

T +1

2(cid:12)

′

′

t=1

+ 3(cid:12)

2T ln

; and b∆2 = ∥ew (cid:0)bw(cid:3)∥2 (cid:20) ∆2 5(cid:12)

p
3 ln[1=(cid:14)]
T + 1
(cid:21)

:

(11)

6 Conclusions and Open Questions

We presented a new paradigm of optimization  termed as mixed optimization  that aims to improve
the convergence rate of stochastic optimization by making a small number of calls to the full gradient
oracle. We proposed the MIXEDGRAD algorithm and showed that it is able to achieve an O(1=T )
convergence rate by accessing stochastic and full gradient oracles for O(T ) and O(log T ) times 
respectively. We showed that the MIXEDGRAD algorithm is able to exploit the smoothness of the
function  which is believed to be not very useful in stochastic optimization.
In the future  we would like to examine the optimality of our algorithm  namely if it is possible
to achieve a better convergence rate for stochastic optimization of smooth functions using O(ln T )
accesses to the full gradient oracle. Furthermore  to alleviate the computational cost caused by
O(log T ) accesses to the full gradient oracle  it would be interesting to empirically evaluate the
proposed algorithm in a distributed framework by distributing the individual functions among pro-
cessors to parallelize the full gradient computation at the beginning of each epoch which requires
O(log T ) communications between the processors in total. Lastly  it is very interesting to check
whether an O(1=T 2) rate could be achieved by an accelerated method in the mixed optimization
scenario  and whether linear convergence rates could be achieved in the strongly-convex case.
Acknowledgments. The authors would like to thank the anonymous reviewers for their helpful and insight-
ful comments. This work was supported in part by ONR Award N000141210431 and NSF (IIS-1251031).

8

References
[1] A. Agarwal  P. L. Bartlett  P. D. Ravikumar  and M. J. Wainwright.

Information-theoretic
lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions
on Information Theory  58(5):3235–3249  2012.

[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Oper. Res. Lett.  31(3):167–175  2003.

[3] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS  pages 161–168 

2008.

[4] S. Boucheron  G. Lugosi  and O. Bousquet. Concentration inequalities. In Advanced Lectures

on Machine Learning  pages 208–240  2003.

[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[6] R. H. Byrd  G. M. Chin  J. Nocedal  and Y. Wu. Sample size selection in optimization methods

for machine learning. Mathematical programming  134(1):127–155  2012.

[7] A. Cotter  O. Shamir  N. Srebro  and K. Sridharan. Better mini-batch algorithms via accelerated

gradient methods. In NIPS  pages 1647–1655  2011.

[8] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online prediction

using mini-batches. The Journal of Machine Learning Research  13:165–202  2012.

[9] M. P. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting.

SIAM Journal on Scientiﬁc Computing  34(3):A1380–A1405  2012.

[10] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Machine Learning  69(2-3):169–192  2007.

[11] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. Journal of Machine Learning Research - Proceedings
Track  19:421–436  2011.

[12] Q. Lin  X. Chen  and J. Pena. A smoothing stochastic gradient method for composite opti-

mization. arXiv preprint arXiv:1008.5204  2010.

[13] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM J. on Optimization  19:1574–1609  2009.

[14] A. S. Nemirovsky and D. B. Yudin. Problem complexity and method efﬁciency in optimization.

1983.

[15] Y. Nesterov. A method of solving a convex programming problem with convergence rate o

(1/k2). In Soviet Mathematics Doklady  volume 27  pages 372–376  1983.

[16] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Aca-

demic Publishers  2004.

[17] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on

Optimization  16(1):235–249  2005.

[18] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program.  103(1):127–

152  2005.

[19] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex

stochastic optimization. In ICML  2012.

[20] N. L. Roux  M. W. Schmidt  and F. Bach. A stochastic gradient method with an exponential

convergence rate for ﬁnite training sets. In NIPS  pages 2672–2680  2012.

[21] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver

for svm. In ICML  pages 807–814  2007.

[22] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. JMLR  14:567599  2013.

[23] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Conver-

gence results and optimal averaging schemes. ICML  2013.

[24] L. Zhang  T. Yang  R. Jin  and X. He. O(logt) projections for stochastic optimization of smooth

and strongly convex functions. ICML  2013.

9

,Mehrdad Mahdavi
Lijun Zhang
Rong Jin