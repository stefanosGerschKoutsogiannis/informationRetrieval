2018,Banach Wasserstein GAN,Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images  which  induces a notion of distance between probability distributions of images. So far the community has considered $\ell^2$ as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces  allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms  focusing on Sobolev norms. Finally  we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.,Banach Wasserstein GAN

Jonas Adler

Department of Mathematics

KTH - Royal institute of Technology

Research and Physics

Elekta

jonasadl@kth.se

Sebastian Lunz

Department of Applied Mathematics

and Theoretical Physics
University of Cambridge
lunz@math.cam.ac.uk

Abstract

Wasserstein Generative Adversarial Networks (WGANs) can be used to generate
realistic samples from complicated image distributions. The Wasserstein metric
used in WGANs is based on a notion of distance between individual images  which
induces a notion of distance between probability distributions of images. So far
the community has considered (cid:96)2 as the underlying distance. We generalize the
theory of WGAN with gradient penalty to Banach spaces  allowing practitioners to
select the features to emphasize in the generator. We further discuss the effect of
some particular choices of underlying norms  focusing on Sobolev norms. Finally 
we demonstrate a boost in performance for an appropriate choice of norm on
CIFAR-10 and CelebA.

1

Introduction

Generative Adversarial Networks (GANs) are one of the most popular generative models [6]. A
neural network  the generator  learns a map that takes random input noise to samples from a given
distribution. The training involves using a second neural network  the critic  to discriminate between
real samples and the generator output.
In particular  [2  7] introduces a critic built around the Wasserstein distance between the distribution
of true images and generated images. The Wasserstein distance is inherently based on a notion of
distance between images which in all implementations of Wasserstein GANs (WGAN) so far has
been the (cid:96)2 distance. On the other hand  the imaging literature contains a wide range of metrics used
to compare images [4] that each emphasize different features of interest  such as edges or to more
accurately approximate human observer perception of the generated image.
There is hence an untapped potential in selecting a norm beyond simply the classical (cid:96)2 norm. We
could for example select an appropriate Sobolev space to either emphasize edges  or large scale
behavior. In this work we extend the classical WGAN theory to work on these and more general
Banach spaces.
Our contributions are as follows:

gradient penalty (GP) term to any separable complete normed space.

• We introduce Banach Wasserstein GAN (BWGAN)  extending WGAN implemented via a
• We describe how BWGAN can be efﬁciently implemented. The only practical difference
from classical WGAN with gradient penalty is that the (cid:96)2 norm is replaced with a dual norm.
We also give theoretically grounded heuristics for the choice of regularization parameters.
• We compare BWGAN with different norms on the CIFAR-10 and CelebA datasets. Using the
Space L10  which puts strong emphasize on outliers  we achieve an unsupervised inception
score of 8.31 on CIFAR-10  state of the art for non-progressive growing GANs.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(cid:18)

(cid:19)1/p

2 Background

2.1 Generative adversarial networks

Generative Adversarial Networks (GANs) [6] perform generative modeling by learning a map
G : Z → B from a low-dimensional latent space Z to image space B  mapping a ﬁxed noise
distribution PZ to a distribution of generated images PG.
In order to train the generative model G  a second network D is used to discriminate between original
images drawn from a distribution of real images Pr and images drawn from PG. The generator is
trained to output images that are conceived to be realistic by the critic D. The process is iterated 
leading to the famous minimax game [6] between generator G and critic D

min

G

max

D

EX∼Pr [log(D(X))] + EZ∼PZ [log(1 − D(GΘ(Z)))] .

(1)

Assuming the discriminator is perfectly trained  this gives rise to the Jensen–Shannon divergence
(JSD) as distance measure between the distributions PG and Pr [6  Theorem 1].

2.2 Wasserstein metrics

To overcome undesirable behavior of the JSD in the presence of singular measures [1]  in [2] the
Wasserstein metric is introduced to quantify the distance between the distributions PG and Pr. While
the JSD is a strong metric  measuring distances point-wise  the Wasserstein distance is a weak metric 
measuring the cost of transporting one probability distribution to another. This allows it to stay ﬁnite
and provide meaningful gradients to the generator even when the measures are mutually singular.
In a rather general form  the Wasserstein metric takes into account an underlying metric dB :
B × B → R on a Polish (e.g. separable completely metrizable) space B. In its primal formulation 
the Wasserstein-p  p ≥ 1  distance is deﬁned as

Wassp(PG  Pr) :=

inf

E(X1 X2)∼πdB(X1  X2)p

π∈Π(PG Pr)

(2)
where Π(PG  Pr) denotes the set of distributions on B×B with marginals PG and Pr. The Wasserstein
distance is hence highly dependent on the choice of metric dB.
The Kantorovich-Rubinstein duality [19  5.10] provides a way of more efﬁciently computing the
Wasserstein-1 distance (which we will henceforth simply call the Wasserstein distance  Wass =
Wass1) between measures on high dimensional spaces. The duality holds in the general setting of
Polish spaces and states that

 

Wass(PG  Pr) = sup
Lip(f )≤1

EX∼PGf (X) − EX∼Pr f (X).

(3)

The supremum is taken over all Lipschitz continuous functions f : B → R with Lipschitz constant
equal or less than one. We note that in this dual formulation  the dependence of f on the choice of
metric is encoded in the condition of f being 1-Lipschitz and recall that a function f : B → R is
γ-Lipschitz if

|f (x) − f (y)| ≤ γdB(x  y).

In an abstract sense  the Wasserstein metric could be used in GAN training by using a critic D to
approximate the supremum in (3). The generator uses the loss EZ∼PZ D(G(Z)). In the case of a
perfectly trained critic D  this is equivalent to using the Wasserstein loss Wass(PG  Pr) to train G [2 
Theorem 3].

2.3 Wasserstein GAN

Implementing GANs with the Wasserstein metric requires to approximate the supremum in (3) with a
neural network. In order to do so  the Lipschitz constraint has to be enforced on the network. In the
paper Wasserstein GAN [2] this was achieved by restricting all network parameters to lie within a
predeﬁned interval. This technique typically guarantees that the network is γ Lipschitz for some γ
for any metric space. However  it typically reduces the set of admissible functions to a proper subset

2

of all γ Lipschitz functions  hence introducing an uncontrollable additional constraint on the network.
This can lead to training instabilities and artifacts in practice [7].
In [7] strong evidence was presented that the condition can better be enforced by working with another
characterization of 1−Lipschitz functions. In particular  they prove that if B = Rn  d(x  y)B =
(cid:107)x − y(cid:107)2 we have the gradient characterization

f is 1 − Lipschitz ⇐⇒ (cid:107)∇f (x)(cid:107)2 ≤ 1

for all x ∈ Rn.

They softly enforce this condition by adding a penalty term to the loss function of D that takes the
form

(cid:16)(cid:107)∇D( ˆX)(cid:107)2 − 1
(cid:17)2

E ˆX

 

(4)

where the distribution of ˆX is taken to be the uniform distributions on lines connecting points drawn
from PG and Pr.
However  penalizing the (cid:96)2 norm of the gradient corresponds speciﬁcally to choosing the (cid:96)2 norm as
underlying distance measure on image space. Some research has been done on generalizing GAN
theory to other spaces [18  11]  but in its current form WGAN with gradient penalty does not extend
to arbitrary choices of underlying spaces B. We shall give a generalization to a large class of spaces 
the (separable) Banach spaces  but ﬁrst we must introduce some notation.

2.4 Banach spaces of images

A vector space is a collection of objects (vectors) that can be added together and scaled  and can be
seen as a generalization of the Euclidean space Rn. If a vector space B is equipped with a notion of
length  a norm (cid:107) · (cid:107)B : B → R  we call it a normed space. The most commonly used norm is the (cid:96)2
norm deﬁned on Rn  given by

(cid:32) n(cid:88)

(cid:33)1/2

x2
i

.

(cid:107)x(cid:107)2 =

i=1

Such spaces can be used to model images in a very general fashion. In a pixelized model  the image
space B is given by the discrete pixel values  B ∼ Rn×n. Continuous image models that do not rely
on the concept of pixel discretization include the space of square integrable functions over the unit
square. The norm (cid:107)·(cid:107)B gives room for a choice on how distances between images are measured. The
Euclidean distance is a common choice  but many other distance notions are possible that account for
more speciﬁc image features  like the position of edges in Sobolev norms.
A normed space is called a Banach space if it is complete  that is  Cauchy sequences converge.
Finally  a space is separable if there exists some countable dense subset. Completeness is required in
order to ensure that the space is rich enough for us to deﬁne limits whereas separability is necessary
for the usual notions of probability to hold. These technical requirements formally hold in ﬁnite
dimensions but are needed in the inﬁnite dimensional setting. We note that all separable Banach
spaces are Polish spaces and we can hence deﬁne Wasserstein metrics on them using the induced
metric dB(x  y) = (cid:107)x − y(cid:107)B.
For any Banach space B  we can consider the space of all bounded linear functionals B → R  which
we will denote B∗ and call the (topological) dual of B. It can be shown [17] that this space is itself a
Banach space with norm (cid:107) · (cid:107)B∗ : B∗ → R given by
(cid:107)x∗(cid:107)B∗ = sup
x∈B

x∗(x)
(cid:107)x(cid:107)B

(5)

.

In what follows  we will give some examples of Banach spaces along with explicit characterizations
of their duals. We will give the characterizations in continuum  but they are also Banach spaces in
their discretized (ﬁnite dimensional) forms.

Lp-spaces. Let Ω be some domain  for example Ω = [0  1]2 to model square images. The set of
functions x : Ω → R with norm

(cid:107)x(cid:107)Lp =

x(t)pdt

(6)

(cid:19)1/p

(cid:18)(cid:90)

Ω

3

is a Banach space with dual [Lp]∗ = Lq where 1/p + 1/q = 1. In particular  we note that [L2]∗ = L2.
The parameter p controls the emphasis on outliers  with higher values corresponding to a stronger
focus on outliers. In the extreme case p = 1  the norm is known to induce sparsity  ensuring that all
but a small amount of pixels are set to the correct values.
Sobolev spaces. Let Ω be some domain  then the set of functions x : Ω → R with norm

(cid:18)(cid:90)

Ω

(cid:107)x(cid:107)W 1 2 =

x(t)2 + |∇x(t)|2dt

(cid:19)1/2

(cid:18)(cid:90)

(cid:16)F−1(cid:104)

(cid:105)

(cid:17)p

(cid:19)1/p

where ∇x is the spatial gradient  is an example of a Sobolev space. In this space  more emphasis
is put on the edges than in e.g. Lp spaces  since if (cid:107)x1 − x2(cid:107)W 1 2 is small then not only are their
absolute values close  but so are their edges.
Since taking the gradient is equivalent to multiplying with ξ in the Fourier space  the concept of
Sobolev spaces can be generalized to arbitrary (real) derivative orders s if we use the norm

(cid:107)x(cid:107)W s p =

(1 + |ξ|2)s/2Fx

 

Ω

dt

(t)

(7)
where F is the Fourier transform. The tuning parameter s allows to control which frequencies of
an image are emphasized: A negative value of s corresponds to amplifying low frequencies  hence
prioritizing the global structure of the image. On the other hand  high values of s amplify high
frequencies  thus putting emphasis on sharp local structures  like the edges or ridges of an image.
The dual of the Sobolev space  [W s p]∗  is W −s q where q is as above [3]. Under weak assumptions
on Ω  all Sobolev spaces with 1 ≤ p < ∞ are separable. We note that W 0 p = Lp and in particular
we recover as an important special case W 0 2 = L2.
There is a wide range of other norms that can be deﬁned for images  see appendix A and [5  3] for a
further overview of norms and their respective duals.

3 Banach Wasserstein GANs

In this section we generalize the loss (4) to separable Banach spaces  allowing us to effectively train a
Wasserstein GAN using arbitrary norms.
We will show that the characterization of γ-Lipschitz functions via the norm of the differential can
be extended from the (cid:96)2 setting in (4) to arbitrary Banach spaces by considering the gradient as an
element in the dual of B. In particular  for any Banach space B with norm (cid:107) · (cid:107)B  we will derive the
loss function

1
γ

L =

(EX∼PΘD(X) − EX∼Pr D(X)) + λE ˆX

(8)
where λ  γ ∈ R are regularization parameters  and show that a minimizer of this this is an approxima-
tion to the Wasserstein distance on B.

(cid:107)∂D( ˆX)(cid:107)B∗ − 1

γ

 

(cid:18) 1

(cid:19)2

(cid:12)(cid:12)f (x + h) − f (x) −(cid:2)∂f (x)(cid:3)(h)(cid:12)(cid:12) = 0.

3.1 Enforcing the Lipschitz constraint in Banach spaces
Throughout this chapter  let B denote a Banach space with norm (cid:107) · (cid:107)B and f : B → R a continuous
function. We require a more general notion of gradient: The function f is called Fréchet differentiable
at x ∈ B if there is a bounded linear map ∂f (x) : B → R such that

1
(cid:107)h(cid:107)B

(9)
The differential ∂f (x) is hence an element of the dual space B∗. We note that the usual notion of
gradient ∇f (x) in Rn with the standard inner product is connected to the Fréchet derivative via

lim(cid:107)h(cid:107)B→0

(cid:2)∂f (x)(cid:3)(h) = ∇f (x) · h.

The following theorem allows us to characterize all Lipschitz continuous functions according to the
dual norm of the Fréchet derivative.

4

Lemma 1. Assume f : B → R is Fréchet differentiable. Then f is γ-Lipschitz if and only if

(cid:107)∂f (x)(cid:107)B∗ ≤ γ ∀x ∈ B.
Proof. Assume f is γ-Lipschitz. Then for all x  h ∈ B and  > 0
(f (x + h) − f (x)) ≤ lim
→0

(cid:2)∂f (x)(cid:3)(h) = lim

→0

1


hence by the deﬁnition of the dual norm  eq. (5)  we have

γ(cid:107)h(cid:107)B



(cid:2)∂f (x)(cid:3)(h)

(cid:107)h(cid:107)B

≤ sup
h∈B

γ(cid:107)h(cid:107)B
(cid:107)h(cid:107)B

≤ γ.

(cid:107)∂f (x)(cid:107)B∗ = sup
h∈B

(10)

= γ(cid:107)h(cid:107)B 

Now let f satisfy (10) and let x  y ∈ B. Deﬁne the function g : R → R by
x(t) = tx + (1 − t)y 

g(t) = f (x(t)) 

where

As x(t + ∆t) − x(t) = ∆t(x − y)  we see that g is everywhere differentiable and

g(cid:48)(t) =(cid:2)∂f(cid:0)x(t)(cid:1)(cid:3)(x − y).

|g(cid:48)(t)| =(cid:12)(cid:12)(cid:2)∂f(cid:0)x(t)(cid:1)(cid:3)(x − y)(cid:12)(cid:12) ≤ (cid:107)∂f (x(t))(cid:107)B∗(cid:107)x − y(cid:107)B ≤ γ(cid:107)x − y(cid:107)B 

Hence

which gives

|f (x) − f (y)| = |g(1) − g(0)| ≤

|g(cid:48)(t)| dt ≤ γ(cid:107)x − y(cid:107)B 

(cid:90) 1

thus ﬁnishing the proof.

0

Using lemma 1 we see that a γ-Lipschitz requirement in Banach spaces is equivalent to the dual
norm of the Fréchet derivative being less than γ everywhere. In order to enforce this we need to
compute (cid:107)∂f (x)(cid:107)B∗. As shown in section 2.4  the dual norm can be readily computed for a range of
interesting Banach spaces  but we also need to compute ∂f (x)  preferably using readily available
automatic differentiation software. However  such software can typically only compute derivatives in
Rn with the standard norm.
Consider a ﬁnite dimensional Banach space B equipped by any norm (cid:107) · (cid:107)B. By Lemma 1  gradient
norm penalization requires characterizing (e.g. giving a basis for) the dual B∗ of B. This can be a
difﬁcult for inﬁnite dimensional Banach spaces. In a ﬁnite dimensional however setting  there is an
linear continuous bijection ι : Rn → B given by

ι(x)i = xi.

(11)
This isomorphism implicitly relies on the fact that a basis of B can be chosen and can be mapped
to the corresponding dual basis. This does not generalize to the inﬁnite dimensional setting  but we
hope that this is not a very limiting assumption in practice.
We note that we can write f = g ◦ ι where g : Rn → R and automatic differentiation can be used to
compute the derivative ∂g(x) efﬁciently. Further  note that the chain rule yields

∂f (x) = ι∗ (∂g(ι(x)))  

where ι∗ : Rn → B∗ is the adjoint of ι which is readily shown to be as simple as ι  ι∗(x)i = xi.
This shows that computing derivatives in ﬁnite dimensional Banach spaces can be done using
standard automatic differentiation libraries with only some formal mathematical corrections. In an
implementation  the operators ι  ι∗ would be implicit.
In terms of computational costs  the difference between general Banach Wasserstein GANs and the
ones based on the (cid:96)2 metric lies in the computation of the gradient of the dual norm. By the chain
rule  any computational step outside the calculation of this gradient is the same for any choice of
underlying notion of distance. This in particular includes any forward pass or backpropagation step
through the layers of the network used as discriminator. If there is an efﬁcient framework available
to compute the gradient of the dual norm  as in the case of the Fourier transform used for Sobolev
spaces  the computational expenses hence stay essentially the same independent of the choice of
norm.

5

3.2 Regularization parameter choices

The network will be trained by adding the regularization term
(cid:107)∂D( ˆX)(cid:107)B∗ − 1

λE ˆX

(cid:17)2

.

(cid:16) 1

γ

Here  λ is a regularization constant and γ is a scaling factor controlling which norm we compute. In
particular D will approximate γ times the Wasserstein distance. In the original WGAN-GP paper
[7] and most following work λ = 10 and γ = 1  while γ = 750 was used in Progressive GAN [9].
However  it is easy to see that these values are speciﬁc to the (cid:96)2 norm and that we would need to
re-tune them if we change the norm. In order to avoid having to hand-tune these for every choice of
norm  we will derive some heuristic parameter choice rules that work with any norm.
For our heuristic  we will start by assuming that the generator is the zero-generator  always returning
zero. Assuming symmetry of the distribution of true images Pr  the discriminator will then essentially
be decided by a single constant f (x) = c(cid:107)x(cid:107)B  where c solves the optimization problem

min
c∈R

By solving this explicitly we ﬁnd

EX∼Pr

c = γ

1 +

EX∼Pr(cid:107)X(cid:107)B

2λ

(cid:20)
− c(cid:107)X(cid:107)B
(cid:18)

γ

(cid:21)

.

λ(c − γ)2

+

γ2

(cid:19)

.

Since we are trying to approximate γ times the Wasserstein distance  and since the norm has Lipschitz
constant 1  we want c ≈ γ. Hence to get a small relative error we need EX∼Pr(cid:107)X(cid:107)B (cid:28) 2λ. With
this theory to guide us  we can make the heuristic rule

λ ≈ EX∼Pr(cid:107)X(cid:107)B.

In the special case of CIFAR-10 with the (cid:96)2 norm this gives λ ≈ 27  which agrees with earlier
practice (λ = 10) reasonably well.
Further  in order to keep the training stable we assume that the network should be approximately
scale preserving. Since the operation x → ∂D(x) is the deepest part of the network (twice the depth
as the forward evaluation)  we will enforce (cid:107)x(cid:107)B∗ ≈ (cid:107)∂D(x)(cid:107)B∗. Assuming λ was appropriately
chosen  we ﬁnd in general (by lemma 1) (cid:107)∂D(x)(cid:107)B∗ ≈ γ. Hence we want γ ≈ (cid:107)x(cid:107)B∗. We pick the
expected value as a representative and hence we obtain the heuristic

γ ≈ EX∼Pr(cid:107)X(cid:107)B∗

For CIFAR-10 with the (cid:96)2 norm this gives γ = λ ≈ 27 and may explain the improved performance
obtained in [9].
A nice property of the above parameter choice rules is that they can be used with any underlying
norm. By using these parameter choice rules we avoid the issue of hand tuning further parameters
when training using different norms.

4 Computational results

To demonstrate computational feasibility and to show how the choice of norm can impact the trained
generator  we implemented Banach Wasserstein GAN with various Sobolev and Lp norms  applied
to CIFAR-10 and CelebA (64 × 64 pixels). The implementation was done in TensorFlow and
the architecture used was a faithful re-implementation of the residual architecture used in [7]  see
appendix B. For the loss function  we used the loss eq. (8) with parameters according to section 3.2
and the norm chosen according to either the Sobolev norm eq. (7) or the Lp norm eq. (6). In the
case of the Sobolev norm  we selected units such that |ξ| ≤ 5. Following [9]  we add a small
10−5EX∼Pr D(X)2 term to the discriminator loss to stop it from drifting during the training.
For training we used the Adam optimizer [10] with learning rate decaying linearly from 2 · 10−4 to 0
over 100 000 iterations with β1 = 0  β2 = 0.9. We used 5 discriminator updates per generator update.
The batch size used was 64. In order to evaluate the reproducibility of the results on CIFAR-10  we

6

Figure 1: Generated CIFAR-10 samples for some Lp spaces.

(a) p = 1.3

(b) p = 2.0

(c) p = 10.0

Figure 2: FID scores for BWGAN on CIFAR-10.

(a) W s 2

(b) Lp

Figure 3: Inception scores for BWGAN on CIFAR-10.

Figure 4: Inception Scores on CIFAR-10.

Method
DCGAN [16]
EBGAN [21]
WGAN-GP [7]
CT GAN [20]
SNGAN [14]
W − 3
L10-BWGAN
Progressive GAN [9]

2   2-BWGAN

Inception Score
6.16 ± .07
7.07 ± .10
7.86 ± .07
8.12 ± .12
8.22 ± .05
8.26 ± .07
8.31 ± .07
8.80 ± .05

followed this up by training an ensemble of 5 generators using SGD with warm restarts following
[12]. Each warm restart used 10 000 generator steps. Our implementation is available online1.
Some representative samples from the generator on both datasets can be seen in ﬁgs. 1 and 5. See
appendix C for samples from each of the W s 2 and Lp spaces investigated as well as samples from
the corresponding Fréchet derivatives.
For evaluation  we report Fréchet Inception Distance (FID)[8] and Inception scores  both computed
from 50K samples. A high image quality corresponds to high Inception and low FID scores. On
the CIFAR-10 dataset  both FID and inception scores indicate that negative s and large values of p
lead to better image quality. On CelebA  the best FID scores are obtained for values of s between
−1 and 0 and around p = 0  whereas the training become unstable for p = 10. We further compare
our CIFAR-10 results in terms of Inception scores to existing methods  see table 4. To the best of

1https://github.com/adler-j/bwgan

7

2.01.51.00.50.00.51.01.52.0s16171819201.11.31.52.03.04.05.010.0p16171819202.01.51.00.50.00.51.01.52.0s7.757.958.158.351.11.31.52.03.04.05.010.0p7.757.958.158.35Figure 5: Generated CelebA samples for Sobolev spaces W s 2.

(a) s = −2

(b) s = 0

(c) s = 2

Figure 6: FID scores for BWGAN on CelebA.

(a) W s 2

(b) Lp

our knowledge  the inception score of 8.31 ± 0.07  achieved using the L10 space  is state of the art
for non-progressive growing methods. Our FID scores are also highly competitive  for CIFAR-10
we achieve 16.43 using L4. We also note that our result for W 0 2 = (cid:96)2 is slightly better than the
reference implementation  despite using the same network. We suspect that this is due to our improved
parameter choices.

5 How about metric spaces?

Gradient norm penalization according to lemma 1 is only valid in Banach spaces  but a natural
alternative to penalizing gradient norms is to enforce the Lipschitz condition directly (see [15]). This
would potentially allow training Wasserstein GAN on general metric spaces by adding a penalty term
of the form

(12)

(cid:34)(cid:18)|f (X) − f (Y )|

dB(X  Y )

EX Y

(cid:35)

(cid:19)2

− 1

.

+

While theoretically equivalent to gradient norm penalization when the distributions of X and Y are
chosen appropriately  this term is very likely to have considerably higher variance in practice.
For example  if we assume that d is not bounded from below and consider two points x  y ∈ M that
are sufﬁciently close then a penalty term of the Lipschitz quotient as in (12) imposes a condition
on the differential around x and y in the direction (x − y) only  i.e. only |∂f (˜x)(x − y)| ≤ 1 is
ensured. In the case of two distributions that are already close  we will with high probability sample
the difference quotient in a spatial direction that is parallel to the data  hence not exhausting the
Lipschitz constraint  i.e. |∂f (˜x)(x − y)| (cid:28) 1 . Difference quotient penalization (12) then does not
effectively enforce the Lipschitz condition. Gradient norm penalization on the other hand ensures
this condition in all spatial directions simultaneously by considering the dual norm of the differential.

8

2.01.51.00.50.00.51.01.52.0s101112131.11.31.52.03.04.05.010p10111213On the other hand  if d is bounded from below the above argument fails. For example  Wasserstein
GAN over a space equipped with the trivial metric

(cid:26)0

1

dtrivial(x  y) =

if x = y
else

approximates the Total Variation distance [19]. Using the regularizer eq. (12) we get a slight variation
of Least Squares GAN [13]. We do not further investigate this line of reasoning.

6 Conclusion

We analyzed the dependence of Wasserstein GANs (WGANs) on the notion of distance between
images and showed how choosing distances other than the (cid:96)2 metric can be used to make WGANs
focus on particular image features of interest. We introduced a generalization of WGANs with
gradient norm penalization to Banach spaces  allowing to easily implement WGANs for a wide range
of underlying norms on images. This opens up a new degree of freedom to design the algorithm to
account for the image features relevant in a speciﬁc application.
On the CIFAR-10 and CelebA dataset  we demonstrated the impact a change in norm has on model
performance. In particular  we computed FID scores for Banach Wasserstein GANs using different
Sobolev spaces W s p and found a correlation between the values of both s and p with model
performance.
While this work was motivated by images  the theory is general and can be applied to data in any
normed space. In the future  we hope that practitioners take a step back and ask themselves if the (cid:96)2
metric is really the best measure of ﬁt  or if some other metric better emphasize what they want to
achieve with their generative model.

Acknowledgments

The authors would like to acknowledge Peter Maass for brining us together as well as important
support from Ozan Öktem  Axel Ringh  Johan Karlsson  Jens Sjölund  Sam Power and Carola
Schönlieb.
The work by J.A. was supported by the Swedish Foundation of Strategic Research grants AM13-0049 
ID14-0055 and Elekta. The work by S.L. was supported by the EPSRC grant EP/L016516/1 for the
University of Cambridge Centre for Doctoral Training  and the Cambridge Centre for Analysis. We
also acknowledge the support of the Cantab Capital Institute for the Mathematics of Information.

References

[1] Martín Arjovsky and Léon Bottou. Towards Principled Methods for Training Generative
Adversarial Networks. International Conference on Learning Representations (ICLR)  2017.
[2] Martín Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein Generative Adversarial

Networks. International Conference on Machine Learning  ICML  2017.

[3] Haim Brezis. Functional analysis  Sobolev spaces and partial differential equations. Springer

Science & Business Media  2010.

[4] Tony F Chan and Jianhong Jackie Shen. Image processing and analysis: variational  PDE 

wavelet  and stochastic methods  volume 94. Siam  2005.

[5] Michel Marie Deza and Elena Deza. Encyclopedia of Distances. Springer Berlin Heidelberg 

2009.

[6] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. Advances in neural
information processing systems (NIPS)  2014.

[7] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved training of wasserstein gans. Advances in Neural Information Processing Systems
(NIPS)  2017.

9

[8] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon 
U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors 
Advances in Neural Information Processing Systems 30  pages 6626–6637. Curran Associates 
Inc.  2017.

[9] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive Growing of GANs for
Improved Quality  Stability  and Variation. International Conference on Learning Representa-
tions (ICLR)  2018.

[10] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. International

Conference on Learning Representations (ICLR)  2015.

[11] Shuang Liu  Olivier Bousquet  and Kamalika Chaudhuri. Approximation and Convergence

Properties of Generative Adversarial Learning. arXiv  2017.

[12] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient descent with Warm Restarts.

International Conference on Learning Representations (ICLR)  2017.

[13] Xudong Mao  Qing Li  Haoran Xie  Raymond YK Lau  Zhen Wang  and Stephen Paul Smolley.
Least squares generative adversarial networks. IEEE International Conference on Computer
Vision (ICCV)  2017.

[14] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral Normaliza-
tion for Generative Adversarial Networks. International Conference on Learning Representa-
tions (ICLR)  2018.

[15] Henning Petzka  Asja Fischer  and Denis Lukovnikov. On the regularization of Wasserstein

GANs. International Conference on Learning Representations (ICLR)  2018.

[16] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised Representation Learning with
Deep Convolutional Generative Adversarial Networks. International Conference on Learning
Representations (ICLR)  2016.

[17] Walter Rudin. Functional analysis. International series in pure and applied mathematics  1991.
[18] Calvin Seward  Thomas Unterthiner  Urs Bergmann  Nikolay Jetchev  and Sepp Hochreiter.

First Order Generative Adversarial Networks. arXiv  2018.

[19] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business

Media  2008.

[20] Xiang Wei  Zixia Liu  Liqiang Wang  and Boqing Gong. Improving the Improved Training of

Wasserstein GANs. International Conference on Learning Representations (ICLR)  2018.

[21] Junbo Jake Zhao  Michaël Mathieu  and Yann LeCun. Energy-based Generative Adversarial

Network. International Conference on Learning Representations (ICLR)  2017.

10

,Jonas Adler
Sebastian Lunz