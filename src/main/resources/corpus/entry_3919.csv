2019,On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons,This paper studies the problem of finding the exact ranking from noisy comparisons. A noisy comparison over a set of $m$ items produces a noisy outcome about the most preferred item  and reveals some information about the ranking. By repeatedly and adaptively choosing items to compare  we want to fully rank the items with a certain confidence  and use as few comparisons as possible. Different from most previous works  in this paper  we have three main novelties: (i) compared to prior works  our upper bounds (algorithms) and lower bounds on the sample complexity (aka number of comparisons) require the minimal assumptions on the instances  and are not restricted to specific models; (ii) we give lower bounds and upper bounds on instances with \textit{unequal} noise levels; and (iii) this paper aims at the \textit{exact} ranking without knowledge on the instances  while most of the previous works either focus on approximate rankings or study exact ranking but require prior knowledge. We first derive lower bounds for pairwise ranking (i.e.  compare two items each time)  and then propose (nearly) \textit{optimal} pairwise ranking algorithms. We further make extensions to listwise ranking (i.e.  comparing multiple items each time). Numerical results also show our improvements against the state of the art.,On Sample Complexity Upper and Lower Bounds for

Exact Ranking from Noisy Comparisons

Department of Computer Science & Engineering

Department of Computer Science

Wenbo Ren

The Ohio State University

ren.453@osu.edu

Jia Liu

Iowa State University
jialiu@iastate.edu

Department of Electrical & Computer Engineering and Computer Science & Engineering

Ness B. Shroff

The Ohio State University

shroff.11@osu.edu

Abstract

This paper studies the problem of ﬁnding the exact ranking from noisy comparisons.
A noisy comparison over a set of m items produces a noisy outcome about the most
preferred item  and reveals some information about the ranking. By repeatedly
and adaptively choosing items to compare  we want to fully rank the items with
a certain conﬁdence  and use as few comparisons as possible. Different from
most previous works  in this paper  we have three main novelties: (i) compared
to prior works  our upper bounds (algorithms) and lower bounds on the sample
complexity (aka number of comparisons) require the minimal assumptions on the
instances  and are not restricted to speciﬁc models; (ii) we give lower bounds and
upper bounds on instances with unequal noise levels; and (iii) this paper aims at
the exact ranking without knowledge on the instances  while most of the previous
works either focus on approximate rankings or study exact ranking but require prior
knowledge. We ﬁrst derive lower bounds for pairwise ranking (i.e.  compare two
items each time)  and then propose (nearly) optimal pairwise ranking algorithms.
We further make extensions to listwise ranking (i.e.  comparing multiple items each
time). Numerical results also show our improvements against the state of the art.

1

Introduction

Background and motivation: Ranking from noisy comparisons has been a canonical problem in
the machine learning community  and has found applications in various areas such as social choices
[8]  web search [9]  crowd sourcing [4]  and recommendation systems [3]. The main goal of ranking
problems is to recover the full or partial rankings of a set of items from noisy comparisons. The
items can refer to various things  such as products  movies  pages  and advertisements  and the
comparisons refer to tests or queries about the items’ strengths or the users’ preferences. In this paper 
we use words “item”  “comparison” and “preference” for simplicity. A comparison involves two
(i.e.  pairwise) or multiple (i.e.  listwise) items  and returns a noisy result about the most preferred
one  where “noisy” means that the comparison outcome is random and the returned item may not
be the most preferred one. A noisy comparison reveals some information about the ranking of
the items. This information can be used to describe users’ preferences  which helps applications
such as recommendations  decision making  and advertising  etc. One example is e-commerce: A
user’s click or purchase of a product (but not others) is based on a noisy (due to the lack of full
information) comparison between several similar products  and one can rank the products based

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

item as the winner  which makes(cid:80)

on the noisy outcomes of the clicks or the purchases to give better recommendations. Due to the
wide applications  in this paper  we do not focus on speciﬁc applications and regard comparisons as
black-box procedures.
This paper studies the active (or adaptive) ranking  where the learner adaptively chooses items to
compare based on previous comparison results  and returns a ranking when having enough conﬁdence.
Previous works [4  28] have shown that  compared to non-adaptive ranking  active ranking can
signiﬁcantly reduce the number of comparisons needed and achieve a similar conﬁdence or accuracy.
In some applications such as news apps  the servers are able to adaptively choose news to present
to the users and collect feedbacks  by which they can learn the users’ preferences in shorter time
compared to non-adaptive methods and may provide better user experience.
We focus on the active full ranking problem  that is  to ﬁnd the exact full ranking with a certain
conﬁdence level by adaptively choosing the items to compare  and try to use as few comparisons as
possible. The comparisons can be either pairwise (i.e.  comparing two items each time) or listwise
(i.e.  comparing more than two items each time). We are interested in the upper and lower bounds on
the sample complexity (aka number of comparisons needed). We are also interested in understanding
whether using listwise comparisons can reduce the sample complexity.
Models and problem statement: There are n items in total  indexed by 1  2  3  ...  n. Given a
comparison over a set S  each item i ∈ S has pi S probability to be returned as the most preferred
one (also referred to as i “wins” this comparison)  and when a tie happens  we randomly assign one
i∈S pi S = 1 for all set S ⊂ [n]. When |S| = 2  we say this
comparison is pairwise  and when |S| > 2  we say listwise. In this paper  a comparison is said to be
m-wise if it involves exactly m items (i.e.  |S| = m). For m = 2 and a two-sized set S = {i  j}  to
simplify notation  we deﬁne pi j := pi S and pj i := pj S.
Assumptions. In this paper  we make the following assumptions: A1) Comparisons are independent
across items  sets  and time. We note that the assumption of independence is common in the this area
(e.g.  [10  11  12  15  16  22  31  32  34  35]). A2) There is a unique permutation (r1  r2  ...  rn) of [n]
1 such that r1 (cid:31) r2 (cid:31)··· (cid:31) rn  where i (cid:31) j denotes that i ranks higher than j (i.e.  i is more preferred
than j). We refer to this unique permutation as the true ranking or exact ranking  and our goal is to
recover the true ranking; A3) For any set S and item i ∈ S  if i ranks higher than all other items
k of S  then pi S > pk S. For pairwise comparisons  A3 states that i (cid:31) j if and only if pi j > 1/2.
We note that for pairwise comparisons  A3 can be viewed as the weak stochastic transitivity [33].
The three assumptions are necessary to make the exact ranking (i.e.  ﬁnding the unique true ranking)
problem meaningful  and thus  we say our assumptions are minimal. Except for the above three
assumptions  we do not assume any prior knowledge of the pi S values. We note that any comparison
model can be fully described by the comparison probabilities (pi S : i ∈ S  S ⊂ [n]).
We further deﬁne some notations. Two items i and j are said to be adjacent if in the true ranking 
there does not exist an item k such that i (cid:31) k (cid:31) j or j (cid:31) k (cid:31) i. For all items i and j in [n]  deﬁne
∆i j := |pi j − 1/2|  ∆i := minj(cid:54)=i ∆i j  and ˜∆i := min{∆i j : i and j are adjacent}. We adopt the
notion of strong stochastic transitivity (SST) [11]: for all items i  j  and k satisfying i (cid:31) j (cid:31) k  it
holds that pi k ≥ max{pi j  pj k}. Under the SST condition  we have ∆i = ˜∆i for all items i. We
note that this paper is not restricted to the SST condition. Pairwise (listwise) ranking refers to ranking
from pairwise (listwise) comparisons. In this paper  f (cid:22) g means f = O(g)  f (cid:23) g means f = Ω(g) 
and f (cid:39) g means f = Θ(g). The meanings of O(·)  o(·)  Ω(·)  ω(·)  and Θ(·) are standard in the
i j   i (cid:54)= j)). For any
sense of Bachmann-Landau notation with respect to (n  δ−1  −1  ∆−1  η−1  (∆−1
a  b ∈ R  deﬁne a ∧ b := min{a  b} and a ∨ b := max{a  b}.
Problem (Exact ranking). Given δ ∈ (0  1/2) and n items  one wants to determine the true ranking
with probability at least 1 − δ by adaptively choosing sets of items to compare.
Deﬁnition 1 (δ-correct algorithms). An algorithm is said to be δ-correct for a problem if for any
input instance of this problem  it  with probability at least 1 − δ  returns a correct result in ﬁnite time.
Main results: First  for δ-correct pairwise ranking algorithms with no prior knowledge of the
i +
log(n/δ))) 2  which is shown to be tight (up to constant factors) under SST and some mild conditions.

instances  we derive a sample-complexity lower bound of the form Ω((cid:80)

i∈[n] ∆−2

i

(log log ∆−1

1For any positive integer k  deﬁne [k] := {1  2  ...  k} to simplify notation
2All log in this paper  unless explicitly noted  are natural log.

2

Second  for pairwise and listwise ranking under the multinomial logit (MNL) model  we derive a
model-speciﬁc lower bound  which is tight (up to constant factors) under some mild conditions  and
shows that in the worst case  the listwise lower bound is no lower than the pairwise one.
Third  we propose a pairwise ranking algorithm that requires no prior information and minimal
assumptions on the instances  and its sample-complexity upper bound matches the lower bounds
proved in this paper under the SST condition and some mild conditions  implying that both upper and
lower bounds are optimal.

2 Related works

Dating back to 1994  the authors of [14] studied the noisy ranking under the strict constraint that
pi j ≥ 1/2 + ∆ for any i (cid:31) j  where ∆ > 0 is priorly known. They showed that any δ-correct
algorithm needs Θ(n∆−2 log(n/δ)) comparisons for the worst instances. However  in some cases  it
is impossible to either assume the knowledge of ∆ or require pi j ≥ 1/2 + ∆ for any i (cid:31) j. Also 
their bounds only depend on the minimal gap ∆ but not ∆i j’s or ∆i’s  and hence is not tight in most
cases. In contrast  our algorithms require no knowledge on the gaps (i.e.  ∆i j’s)  and we establish
sample-complexity lower bounds and upper bounds that base on unequal gaps  which can be much
tighter when ∆i’s vary a lot.
Another line of research is to explore the probably approximately correct (PAC) ranking (which aims
at ﬁnding a permutation (r1  r2  ...  rn) of [n] such that pri rj ≥ 1/2 −  for all i < j  where  > 0 is
a given error tolerance) under various pairwise comparison models [10  11  12  29  31  32  35]. When
 > 0  the PAC ranking may not be unique. The authors of [10  11  12] proposed algorithms with
O(n−2 log(n/δ)) upper bound for PAC ranking with tolerance  > 0 under SST and the stochastic
triangle inequality3 (STI). When  goes to zero  the PAC ranking reduces to the true ranking. However 
when  > 0  we still need some prior knowledge on (pi j : i  j ∈ [n]) to get the true ranking  as
we need to know a lower bound of the values of ∆i j to ensure that the PAC ranking equals to the
unique true ranking. When  = 0  the algorithms in [10  11  12] do not work. Prior to these works 
the authors of [35] also studied the PAC ranking. In their work  with  = 0  the unique true ranking
)}) comparisons  which is higher than the
can be found by O(n log n · maxi∈[n]{∆−2
lower bound and upper bound proved in this paper by at least a log factor.
In contrast  this paper is focused on recovering the unique true (exact) ranking  and there are three
major motivations. First  in some applications  we prefer to ﬁnd the exact order  especially in
“winner-takes-all” situations. For example  when predicting the winner of an election  we prefer to get
the exact result but not the PAC one  as only a few votes can completely change the result. Second 
analyzing the exact ranking can help us better understand the instance-wise upper and lower bounds
about the ranking problems  while the bounds of PAC ranking (e.g.  in [10  11  12]) may only work
for the worst cases. Third  exact ranking algorithms may better exploit the large gaps (e.g.  ∆i’s) to
achieve lower sample complexities. In fact  when ﬁnding the PAC ranking  we can perform the exact
ranking algorithm and the PAC ranking algorithm parallelly  and return a ranking whenever one of
them returns. By this  when  is large  we can beneﬁt from the PAC upper bounds that depend on −2 
and when  is small  we can beneﬁt from the exact ranking bounds that depend on ∆−2
There are also other interesting active ranking works. Authors of [15  16  22  34] studied active
ranking under the Borda-Score model  where the Borda-Score of item i is deﬁned as
j(cid:54)=i pi j.
We note that the Borda-Score model does not satisfy A2 and A3 and is not comparable with the model
in this paper. There are also many works on best item(s) selection  including [1  5  7  19  26  27  32] 
which are less related to this paper.

log(nδ−1∆−1

(cid:80)

i

i

.

i

1
n−1

3 Lower bound analysis

3.1 Generic lower bound for δ-correct algorithms

In this subsection  we establish a sample-complexity lower bound for pairwise ranking. The lower
bound is for δ-correct algorithms  which have performance guarantee for all input instances. There
are algorithms that work faster than our lower bound but only return correct results with 1 − δ

3Stochastic triangle inequality means that for all items i  j  k with i (cid:31) j (cid:31) k  ∆i k ≤ ∆i j + ∆j k.

3

conﬁdence for a restricted class of instances  which is discussed in Section A.1 of supplementary
material. Theorem 2 states the lower bound  and its full proof is provided in supplementary material.
Here we remind that ˜∆i := min{∆i j : i and j are adjaent}.
Theorem 2 (Lower bound for pairwise ranking). Given δ ∈ (0  1/12) and an instance I with n
items  then the number of comparisons used by a δ-correct algorithm A with no prior knowledge
about the gaps of I is lower bounded by

[ ˜∆−2

(log log ˜∆−1

i

˜∆−2

i

log(1/xi) :

Ω(cid:0)(cid:88)

i∈[n]

If δ (cid:22) 1/poly(n)4  or maxi j∈[n]{ ˜∆i/ ˜∆j} (cid:22) n1/2−p for some constant p > 0  then the lower bound
becomes

i + log(1/δ))] + min{(cid:88)
Ω(cid:0)(cid:88)

(log log ˜∆−1

˜∆−2

i∈[n]

i

i∈[n]

i + log(n/δ))(cid:1).

xi ≤ 1}(cid:1).

(cid:88)

i∈[n]

(1)

(2)

Remark: (i) When the instance satisﬁes the SST condition (the algorithm does not need to know
this information)  the bound in Eq. (2) is tight (up to a constant factor) under the given condition 
which will be shown in Theorem 12 later. (ii) The lower bound in Eq. (1) implies an n log n term in
min{·}  which can be checked by the convexity of log(1/xi) and Jensen’s inequality  which yields
i∈[n] xi) ≥ n log n. (iii) The lower bound in (2) may not hold if the
required conditions do not hold  which will be discussed in Section A.2 of supplementary material.

i∈[n] log(1/xi) ≥ n log(n/(cid:80)
(cid:80)

Proof sketch of Theorem 2. Due to space limitation  we outline the basic idea of the proof here and
refer readers to supplementary material for details. Our ﬁrst step is to use the results in [13  18  25]
to establish a lower bound for ranking two items. Then  it seems straightforward that the lower
bound for ranking n items can be obtained by summing up the lower bounds for ranking {q1  q2} 
{q2  q3} ... {qn−1  qn}  where q1 (cid:31) q2 (cid:31) ··· (cid:31) qn is the true ranking. However  Note that to rank
qi and qj  there may be an algorithm that compares qi and qj with other items like qk  and uses
the comparison outcomes over {qi  qk} and {qj  qk} to determine the order of qi and qj. Since it is
unclear to what degree comparing qi and qj with other items can help to rank qi and qj  the lower
bound for ranking n items cannot be simply obtained by summing up the lower bounds for ranking
2 items. To overcome this challenge  our strategy is to construct two problems: P1 and P2 with
decreasing inﬂuence of this type of comparisons. Then  we prove that P1 reduces to exact ranking
and P2 reduces to P1. Third  we prove a lower bound on δ-correct algorithms for solving P2  which
yields a lower bound for exact ranking. Finally  we use this lower bound to get the desired lower
bounds in Eq. (1) and Eq. (2).

3.2 Model-speciﬁc lower bound

In Section 3.1  we provide a lower bound for δ-correct algorithms that do not require any knowledge
of the instances except assumptions A1 to A3. However  in some applications  people may focus on a
speciﬁc model  and hence  the algorithm may have further knowledge about the instances  such as the
model’s restrictions. Hence  the lower bound in Theorem 2 may not be applicable any more5.
In this paper  we derive a model-speciﬁc lower bound for the MNL model. The MNL model can be
applied to both pairwise and listwise comparisons. For pairwise comparisons  the MNL model is
mathematically equivalent to the Bradley-Terry-Luce (BTL) model [24] and the Plackett-Luce (PL)
model [35]. There have been many prior works that focus on active ranking based on this model (e.g. 
[5  6  7  15  19  27  31  35]).
Under the MNL model  each item holds a real number representing the users’ preference over this
item  where the larger the number  the more preferred the item. Speciﬁcally  each item i holds a
j∈S exp(γj). To simplify
j∈S θj. We name θi as the preference score of
item i. We deﬁne ∆i j := |pi j − 1/2|  ∆i := minj(cid:54)=i ∆i j  and we have ˜∆i = ∆i  i.e.  the MNL
model satisﬁes the SST condition.

parameter γi ∈ R such that for any set S containing i  pi S = exp(γi)/(cid:80)
notation  we let θi = exp(γi)  hence  pi S = θi/(cid:80)

4poly(n) means a polynomial function of n  and δ (cid:22) 1/poly(n) means δ (cid:22) n−p for some constant p > 0.
5For example  under a model with ∆i j = ∆ for any i (cid:54)= j where ∆ > 0 is unknown  one may ﬁrst estimate
a lower bound of ∆  and then perform algorithms in [14]  yielding a sample complexity lower than Theorem 2.

4

Theorem 3. [Lower bound for the MNL model] Let δ ∈ (0  1/12) and given a δ-correct algorithm
A with the knowledge that the input instances satisfy the MNL model  let NA be the number of
comparisons conducted by A  then E[NA] is lower bounded by Eq. (1) with a different hidden
constant factor. When δ (cid:22) 1/poly(n) or maxi j∈[n]{∆i/∆j} (cid:22) n1/2−p for some constant p > 0 
the sample complexity is lower bounded by Eq. (2) with a different hidden constant factor.

Proof sketch. We prove this theorem by Lemmas 4  5 and 6  which could be of independent interest.
Suppose that there are two coins with unknown head probabilities (the probability that a toss produces
a head) λ and µ  respectively  and we want to ﬁnd the more biased one (i.e.  the one with the larger
head probability). Lemma 4 states a lower bound on the number of heads or tails generated for ﬁnding
the more biased coin  which works even if λ and µ go to 0. This is in contrast to the lower bounds on
the number of tosses given by previous works [18  21  25]  which go to inﬁnity as λ and µ go to 0.
Lemma 4 (Lower bound on number of heads). Let λ + µ ≤ 1  ∆ := |λ/(λ + µ) − 1/2|  and
δ ∈ (0  1/2) be given. To ﬁnd the more biased coin with probability 1 − δ  any δ-correct algorithm
for this problem produces Ω(∆−2(log log ∆−1 + log δ−1)) heads in expectation.

i := minj(cid:54)=i ∆c

i j = ∆i j  and ∆i = ˜∆i = ∆c
i .

Now we consider n coins C1  C2  ...  Cn with mean rewards µ1  µ2  ...  µn  respectively  where for any
i ∈ [n]  θi/µi = c for some constant c > 0. Deﬁne the gaps of coins ∆c
i j := |µi/(µi + µj) − 1/2| 
and ∆c
Lemma 5 (Lower bound for arranging coins). For δ < 1/12  to arrange these coins in ascending
order of head probabilities  the number of heads generated by any δ-correct algorithm is lower
bounded by Eq. (1) with a (possibly) different hidden constant factor.

i j. We can check that for all i and j  ∆c

The next lemma shows that any algorithm that solves a ranking problem under the MNL model can
be transformed to solve the pure exploration multi-armed bandit (PEMAB) problem with Bernoulli
rewards(e.g.  [18  20  30]). Previous works [1  15  16] have shown that certain types of pairwise
ranking problems (e.g.  Borda-Score ranking) can also be transformed to PEMAB problems. But in
this paper  we make a reverse connection that bridges these two classes of problems  which may be
of independent interest. We note that in our prior work [29]  we proved a similar result.
Lemma 6 (Reducing PEMAB problems to ranking). If there is a δ-correct algorithm that correctly
ranks [n] with probability 1−δ by M expected number of comparisons  then we can construct another
δ-correct algorithm that correctly arranges the coins C1  C2  ...  Cn in the order of ascending head
probabilities with probability 1 − δ and produces M heads in expectation.

The theorem follows by Lemmas 5 and 6. A full proof can be found in supplementary material.

3.3 Discussions on listwise ranking

A listwise comparison compares m (m > 2) items and returns a noisy result about the most preferred
item. It is an interesting question whether exact ranking from listwise comparisons requires less
comparisons. The answer is “It depends.” When every comparison returns the most preferred
item with high probability (w.h.p.)6  then  by conducting m-wise comparisons  the number of
comparisons needed for exact ranking is Θ(n logm n)  i.e.  there is a log m reduction  which is stated
in Proposition 7. The proof can be found in supplementary material.
Proposition 7 (Listwise ranking with negligible noises). If all comparisons are correct w.h.p.  to
exactly rank n items w.h.p. by using m-wise comparisons  Θ(n logm n) comparisons are needed.
In general  when the “w.h.p. condition” is violated  listwise ranking does not necessarily require less
comparisons than pairwise ranking (in order sense). Here  we give an example. For more general
models  it remains an open problem to identify the theoretical limits  which is left for future studies.
Theorem 8. Under the MNL model  given n items with preference scores θ1  θ2  ...  θn and ∆i j :=
|θi/(θi + θj)− 1/2|  ˜∆i = ∆i := minj(cid:54)=i ∆i j  to correctly rank these n items with probability 1− δ 
even with m-wise comparisons for all m ∈ {2  3  ...  n}  the lower bound is the same as the pairwise
ranking (i.e.  Theorem 3) with (possibly) different hidden constant factors.

6In this paper  “w.h.p.” means with probability at least 1 − n−p  where p > 0 is a sufﬁciently large constant.

5

Theorem 8 gives a minimax lower bound for listwise ranking  which is the same as pairwise ranking.
The proof is given in supplementary material. In [5]  the authors have shown that for top-k item
selection under the MNL model  listwise comparisons can reduce the number of comparisons needed
compared with pairwise comparisons. However  for exact ranking  listwise comparisons cannot.

4 Algorithm and the upper bound for pairwise ranking

In this section  we establish a (nearly) sample-complexity optimal δ-correct algorithm for exact
ranking  where whether the word “nearly” can be deleted depends on the structures of the instances.
The algorithm is based on Binary Search proposed in [14] with upper bound O(n∆−2
min log(n/δ)) 
where ∆min := mini(cid:54)=j ∆i j. Binary Search has two limitations: (i) it requires the knowledge of
∆min a priori to run  and (ii) it does not utilize the unequal noise levels.
In this paper  we propose a technique named Attempting with error prevention and establish a
corresponding insertion subroutine that attempts to insert an item i into a sorted list with a guessing
∆i-value  while preventing errors from happening if the guess is not well chosen. If the guess is small
enough  this subroutine correctly inserts the item with a large probability  and if not  this subroutine
will  with a large probability  not insert the item into a wrong position. By attempting to insert item i
with diminishing guesses of ∆i  this subroutine ﬁnally correctly inserts item i with a large conﬁdence.
To implement the technique “Attempting with error prevention”  we ﬁrst need to construct a useful
subroutine called Attempting-Comparison (ATC)  which attempts to rank two items with   a guess
of ∆i j. Then  by ATC  we establish Attempting-Insertion (ATI)  which also adopts this technique.

(cid:113) 1

2t log π2t2

3δ ; bmax ← (cid:100) 1

Subroutine 1 Attempting-Comparison(i  j    δ) (ATC)
Initialize: ∀t  let bt =
1: for t ← 1 to bmax do
Compare i and j once; Update wi ← wi + 1 if i wins; Update ˆpt
2:
if ˆpt
3:
if ˆpt
4:
5: end for
6: return i if ˆpt

i > 1/2 + bt then return i;
i < 1/2 − bt then return j;
i > 1/2; return j if ˆpt

22 log 2

δ(cid:101); wi ← 0;

i ← wi/t;

i < 1/2; and return a random item if ˆpt

i = 1/2;

Lemma 9 (Theoretical Performance of ATC). ATC terminates after at most bmax = O(−2 log (1/δ))
comparisons and returns the more preferred item with probability at least 1/2. Further  if  ≤ ∆i j 
then ATC returns the more preferred item with probability at least 1 − δ.

Next  to establish insertion subroutine ATI  we introduce preference interval trees [14] (PIT). A PIT
is constructed from a sorted list of items. For a sorted list of items S with size l  without loss of
generality  we assume that r1 (cid:31) r2 (cid:31) ··· (cid:31) rl. We introduce two artiﬁcial items −∞ and +∞ 
where −∞ is such that pi −∞ = 1 for any item i  and +∞ is such that pi +∞ = 0 for any item i.
Preference Interval Tree [14]. A preference interval tree
constructed from the sorted list S satisﬁes the following con-
ditions: (i) It is a binary tree with depth (cid:100)1 + log2(|S| +
1)(cid:101).
(ii) Each node u holds an interval (u.left  u.right)
where u.left  u.right ∈ S ∪ {−∞  +∞}  and if u is non-
leaf  it holds an item u.mid satisfying u.right (cid:31) u.mid (cid:31)
u.left.
(iii) A node i is in the interval (j  k) if and only
if k (cid:31) i (cid:31) j.
(iv) The root node is with interval
(−∞  +∞). From left to right  the leaf nodes are with intervals
(−∞  rl)  (rl  rl−1)  (rl−1  rl−2)  ...  (r2  r1)  (r1  +∞).
(v)
Each non-leaf node u has two children u.lchild and u.rchild
such that u.left = u.lchild.left  u.right = u.rchild.right and
u.mid = u.lchild.right = u.rchild.left.
Based on the notion of PIT  we present insertion subroutine ATI in Subroutine 2. ATI runs a random
walk on the PIT to insert i into S. Let X be the point that moves on the tree. We say a leaf u0 correct

Figure 1: An example of PIT  con-
structed from a sorted list with three
items 3 (cid:31) 2 (cid:31) 1.

6

if the item i belongs to (u0.left  u0.right). Deﬁne d(X) := the distance (i.e.  the number of edges)
between X and u0. At each round of the subroutine  if all comparisons give correct results  we say
this round is correct  otherwise we say incorrect. For each correct round  either d(X) is decreased
by 1 or the counter of u0 is increased by 1. The subroutine inserts i into u0 if u0 is counted for
16 tmax times. Thus  after tmax rounds  the subroutine correctly inserts i into S if the number of
1 + 5
2   where h = (cid:100)1 + log2(|S| + 1)(cid:101) is the depth of the tree.
correct rounds is no less than 21
If guessing  ≤ ∆i  then each round is correct with probability at least q  making the subroutine
correctly insert item i with probability at least 1 − δ.
For all  > 0  each round is incorrect with probability at most 1/2  and thus  by concentration
inequalities  we can also show that with probability at least 1 − δ  i will not be placed into any leaf
node other than u0. That is  if  > ∆i  the subroutine either correctly inserts i or returns unsure with
probability at least 1 − δ. The choice of parameters guarantees the sample complexity. Lemma 10
states its theoretical performance  and the proof is relegated to the supplementary material.

32 tmax + h

3δ + 1 then

else

q) = i then

25 log 2

δ}(cid:101) and q ← 15
16;

if X is the root node then

#i.e.  ATC returns i (cid:31) X.mid

q) = X.right then

cX ← cX + 1;
if cX > bt := 1

Insert i into the corresponding interval of X and return inserted;

2 t +

2 log π2t2
else if cX > 0 then cX ← cX − 1
else X ← X.parent
√
if ATC(i  X.left    1 − 3
X ← X.parent;
√
else if ATC(i  X.mid    1 − 3
else X ← X.lchild;

if ATC(i  X.mid    1 − q) = i then X ← X.right;
else X ← X.left;
else if X is a leaf node then
if ATC(i  X.left    1 − √
q) = i ∧ ATC(i  X.right    1 − √
(cid:113) t

Subroutine 2 Attempting-Insertion(i  S    δ) (ATI).
Initialize: Let T be a PIT constructed from S;
h ← (cid:100)1 + log2(1 + |S|)(cid:101)  the depth of T ;
For all leaf nodes u of T   initialize cu ← 0;
Set tmax ← (cid:100)max{4h  512
1: X ← the root node of T ;
2: for t ← 1 to tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end for
19: if there is a leaf node u with cu ≥ 1 + 5
20:
21: else return unsure;
Lemma 10 (Theoretical performance of ATI). Let δ ∈ (0  1). ATI returns after O(−2 log(|S|/δ))
comparisons and  with probability at least 1 − δ  correctly inserts i or returns unsure. Further  if
 ≤ ∆i  it correctly inserts i with probability at least 1 − δ.
By Lemma 10  we can see that the idea “Attempting with error prevention” is successfully imple-
mented. Thus  by repeatedly attempting to insert an item with diminishing guess  with proper
conﬁdences for the attempts  one can ﬁnally correctly insert i with probability 1 − δ. We use this idea
to establish the insertion subroutine Iterative-Attempting-Insertion (IAI  Subroutine 3)  and then use
it to establish the ranking algorithm Iterative-Insertion-Ranking (IIR  Algorithm 4). Their theoretical
performances are stated in Lemma 11 and Theorem 12  respectively  and their proofs are given in
supplementary material.
Lemma 11 (Theoretical Performance of IAI). With probability at least 1 − δ  IAI correctly inserts i
into S  and conducts at most O(∆−2
Theorem 12 (Theoretical Performance of IIR). With probability at least 1 − δ  IIR returns the exact

√
q) = X.left ∨ ATC(i  X.right    1 − 3

Insert i into the corresponding interval of u and return inserted;

(log log ∆−1

i + log(|S|/δ))) comparisons.

ranking of [n] and conducts at most O((cid:80)

i

i∈[n] ∆−2

i

(log log ∆−1

i + log(n/δ))) comparisons.

q) = i then X ← X.rchild;

16 tmax then

7

π2τ 2 ; t ← 0; F lag ← unsure;
F lag ←ATI(i  S  t  δt);

Algorithm 4 Iterative-Insertion-Ranking (IIR).
Input: S = [n]  and conﬁdence δ > 0;
1: Ans ← the list containing only S[1];
2: for t ← 2 to |S| do
3:
4: end for
5: return Ans;

Subroutine 3 Iterative-Attempting-Insertion (IAI).
Input parameters: (i  S  δ);
Initialize: For all τ ∈ Z+  set τ = 2−(τ +1) and
δτ = 6δ
1: repeat t ← t + 1;
2:
3: until F lag = inserted
Remark: We can see that the upper bounds of IIR depend on the values of (∆i  i ∈ [n]) while the
lower bounds given in Theorem 2 depend on the values of ( ˜∆i  i ∈ [n]). Without SST  it is possible
˜∆i < ∆i  but if SST holds  then our algorithm is optimal up to a constant factor given δ (cid:22) 1/poly(n) 
˜∆i/ ˜∆j (cid:22) O(n1/2−p) for some constant p > 0. According to [10  11  12]  ranking
or maxi j∈[n]
without the SST condition can be much harder than that with SST   and it remains an open problem
whether our upper bound is tight or not when the SST condition does not hold.

IAI(S[t]  Ans  δ/(n − 1));

5 Numerical results

i

log(n∆−1

In this section  we provide numerical results to demonstrate the efﬁcacy of our proposed IIR algorithm.
We compare IIR with: (i) Active-Ranking (AR) [15]  which focuses on the Borda-Score model and
is not directly comparable to our algorithm. We use it as an example to show that although Borda-
Ranking may be the same as exact ranking  for ﬁnding the exact ranking  the performance of
Borda-Score algorithms is not always as good as that for ﬁnding the Borda-Ranking 7; (ii) PLPAC-
AMPR [35]  an algorithm for PAC ranking under the MNL model. By setting the parameter  = 0 
it can ﬁnd the exact ranking with O((n log n) maxi∈[n] ∆−2
i δ−1)) comparisons  higher
than our algorithm by at least a log factor; (iii) UCB + Binary Search of [14]. In the Binary Search
algorithm of [14]  a subroutine that ranks two items with a constant conﬁdence is required. In [14]  it
assumes the value of ∆min = mini∈[n] ∆i is priorly known  and the subroutine is simply comparing
two items for Θ(∆−2
min) times and returns the item that wins more. In this paper  the value of ∆min
is not priorly known  and here  we use UCB algorithms such as LUCB [23] to play the role of the
required subroutine. The UCB algorithms that we use include Hoeffding-LUCB [17  23]  KL-LUCB
[2  23]  and lil’UCB [18]. For Hoeffding-LUCB and KL-LUCB  we choose γ = 2. For lil’UCB  we
choose  = 0.01  β = 1  and λ = ( 2+β
Instances. The experiments are conducted on three different types of instances. To simplify notation 
we use r1 (cid:31) r2 (cid:31) ··· (cid:31) rn to denote the true ranking  and let ∆ = 0.1. (i) Type-Homo: For any
ri (cid:31) rj  pri rj = 1/2 + ∆. (ii) Type-MNL: The preference score of ri (i.e.  θri) is generated by
taking an independent instance of Uniform([0.9 ∗ 1.5n−i  1.1 ∗ 1.5n−i]). By this  for any i  ∆i is
around 0.1. (iii) Type-Random: For any ri (cid:31) rj  pri rj is generated by taking an independent instance
of Uniform([0.5 + 0.8∆  0.5 + 1.5∆]). By this  for any i  ∆i is around 0.1. We let ∆i’s be close
to 0.1 in order to decrease the inﬂuence of ∆i’s on sample complexities and show how the sample
complexities of the algorithms grow with n.
The numerical results for these three types are presented in Figure 2 (a)-(c)  respectively. For all
simulations  we input δ = 0.01. Every point of every ﬁgure is averaged over 100 independent trials.
In every ﬁgure  for the same n-value  the algorithms are tested on an identical input instance.
From Figure 2  we can see that our algorithm signiﬁcantly outperforms the existing algorithms. We
can also see that the sample complexity of IIR scales with n log n  which is consistent with our
theoretical results. There are some insights about the practical performance of IIR. First  in Lines 3
and 4 of ATC and Lines 9 and 10 of ATI  we use LUCB-like [23] designs to allow the algorithms
return before completing all required iterations  which does not improve the theoretical upper bound
but can improve the practical performance. Second  in the theoretical analysis  we only show that

β )2 8.

(cid:80)

7For instance  when pri rj = 1/2 + ∆ for all i < j  the Borda-Score of item ri is

1
n−1

j(cid:54)=i pri rj =

1/2 + n+1−2i

n−1 ∆  and ∆ri = Θ(1/n). Thus  by [15]  the sample complexity of AR is at least O(n3 log n).

8We do not choose the combination ( = 0  β = 1  and λ = 1+10/n) that has a better practical performance

because this combination does not have theoretical guarantee  making the comparison in some sense unfair.

8

(a) Type-Homo.

(b) Type-MNL.

(c) Type-Random.

Figure 2: Comparisons between IIR and existing methods.

ATI correctly inserts an item i with high probability when inputting  ≤ ∆i  but the algorithm may
return before  being that small  making the practical performance better than what the theoretical
upper bound suggests.

6 Conclusion

In this paper  we investigated the theoretical limits of exact ranking with minimal assumptions. We
do not assume any prior knowledge of the comparison probabilities and gaps  and derived the lower
bounds and upper bound for instances with unequal noise levels. We also derived the model-speciﬁc
pairwise and listwise lower bound for the MNL model  which further shows that in the worst case 
listwise ranking is no more efﬁcient than pairwise ranking in terms of sample complexity. The
iterative-insertion-ranking (IIR) algorithm proposed in this paper indicates that our lower bounds are
optimal under strong stochastic transitivity (SST) and some mild conditions. Numerical results also
suggest that our ranking algorithm outperforms existing works in the literature.

Acknowledgments

This work has been supported in part by NSF grants ECCS-1818791  CCF-1758736  CNS-1758757 
CNS-1446582  CNS-1901057; ONR grant N00014-17-1-2417; AFRL grant FA8750-18-1-0107  and
by Institute for Information & communications Technology Promotion (IITP) grant funded by the
Korea government (MSIT)  (2017-0-00692  Transport-aware Streaming Technique Enabling Ultra
Low-Latency AR/VR Services).

9

20406080100n104106108number of comparisonsIIRHoeffdingLUCB + Binary Searchlil'UCB + Binary SearchKL-LUCB + Binary SearchActive Ranking20406080100n104106108number of comparisonsIIRHoeffdingLUCB + Binary Searchlil'UCB + Binary SearchKL-LUCB + Binary SearchActive RankingPLPAC-AMPR20406080100n104106108number of comparisonsIIRHoeffdingLUCB + Binary Searchlil'UCB + Binary SearchKL-LUCB + Binary SearchReferences
[1] Agarwal  A.  Agarwal  S.  Assadi  S.  and Khanna  S. (2017). Learning with limited rounds of
adaptivity: Coin tossing  multi-armed bandits  and ranking from pairwise comparisons. In In
Conference on Learning Theory.

[2] Arratia  R. and Gordon  L. (1989). Tutorial on large deviations for the Binomial distribution.

Bulletin of Mathematical Biology.

[3] Baltrunas  L.  Makcinskas  T.  and Ricci  F. (2010). Group recommendations with rank aggrega-
tion and collaborative ﬁltering. In Proceedings of the fourth ACM conference on Recommender
systems. ACM.

[4] Chen  X.  Bennett  P. N.  Collins-Thompson  K.  and Horvitz  E. (2013). Pairwise ranking
aggregation in a crowdsourced setting. In In ACM Conference on Web Search and Data Mining.
ACM.

[5] Chen  X.  Li  Y.  and Mao  J. (2018). A nearly instance optimal algorithm for top-k ranking under
the multinomial logit model. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium
on Algorithms. SIAM.

[6] Chen  Y.  Fan  J.  Ma  C.  and Wang  K. (2017). Spectral method and regularized MLE are both

optimal for top-k ranking. stat.

[7] Chen  Y. and Suh  C. (2015). Spectral MLE: Top-k rank aggregation from pairwise comparisons.

In International Conference on Machine Learning.

[8] Conitzer  V. and Sandholm  T. (2005). Communication complexity of common voting rules. In

Proceedings of the 6th ACM conference on Electronic commerce. ACM.

[9] Dwork  C.  Kumar  R.  Naor  M.  and Sivakumar  D. (2001). Rank aggregation methods for the

web. In In Proceedings of the 10th international conference on World Wide Web. ACM.

[10] Falahatgar  M.  Hao  Y.  Orlitsky  A.  Pichapati  V.  and Ravindrakumar  V. (2017a). Maxing

and ranking with few assumptions. In In Advances in Neural Information Processing Systems.

[11] Falahatgar  M.  Jain  A.  Orlitsky  A.  Pichapati  V.  and Ravindrakumar  V. (2018). The limits
of maxing  ranking  and preference learning. In Proceedings of the 35th International Conference
on Machine Learning. PMLR.

[12] Falahatgar  M.  Orlitsky  A.  Pichapati  V.  and Suresh  A. T. (2017b). Maximum selection and

ranking under noisy comparisons. In International Conference on Machine Learning.

[13] Farrell  R. H. (1964). Asymptotic behavior of expected sample size in certain one sided tests.

The Annals of Mathematical Statistics.

[14] Feige  U.  Raghavan  P.  Peleg  D.  and Upfal  E. (1994). Computing with noisy information.

SIAM Journal on Computing.

[15] Heckel  R.  Shah  N. B.  Ramchandran  K.  and Wainwright  M. J. (2016). Active rank-
ing from pairwise comparisons and when parametric assumptions don’t help. arXiv preprint
arXiv:1606.08842.

[16] Heckel  R.  Simchowitz  M.  Ramchandran  K.  and Wainwright  M. J. (2018). Approximate
ranking from pairwise comparisons. In International Conference on Artiﬁcial Intelligence and
Statistics.

[17] Hoeffding  W. (1963). Probability inequalities for sums of bounded random variables. Journal

of the American statistical association.

[18] Jamieson  K.  Malloy  M.  Nowak  R.  and Bubeck  S. (2014). lil’UCB: An optimal exploration

algorithm for multi-armed bandits. In In Conference on Learning Theory.

[19] Jang  M.  Kim  S.  Suh  C.  and Oh  S. (2017). Optimal sample complexity of m-wise data for

top-k ranking. In In Advances in Neural Information Processing Systems.

10

[20] Kalyanakrishnan  S. and Stone  P. (2010). Efﬁcient selection of multiple bandit arms: Theory

and practice. In International Conference on Machine Learning.

[21] Kalyanakrishnan  S.  Tewari  A.  Auer  P.  and Stone  P. (2012). PAC subset selection in

stochastic multi-armed bandits. In ICML.

[22] Katariya  S.  Jain  L.  Sengupta  N.  Evans  J.  and Nowak  R. (2018). Adaptive sampling for

coarse ranking. In International Conference on Artiﬁcial Intelligence and Statistics.

[23] Kaufmann  E. and Kalyanakrishnan  S. (2013).

selection. In Conference on Learning Theory.

Information complexity in bandit subset

[24] Luce  R. D. (2012). Individual choice behavior: A theoretical analysis. Courier Corporation.

[25] Mannor  S. and Tsitsiklis  J. N. (2004). The sample complexity of exploration in the multi-armed

bandit problem. Journal of Machine Learning Research.

[26] Mohajer  S. and Suh  C. (2016). Active top-k ranking from noisy comparisons. In Communica-

tion  Control  and Computing (Allerton)  2016 54th Annual Allerton Conference on. IEEE.

[27] Negahban  S.  Oh  S.  and Shah  D. (2016). Rank centrality: Ranking from pairwise comparisons.

In Operations Research.

[28] Pfeiffer  T.  Xi  A.  Gao  A.  Mao  Y.  Chen  and Rand  D. G. (2012). Adaptive polling for

information aggregation. In Twenty-sixth AAAI Conference on Artiﬁcial Intelligence.

[29] Ren  W.  Liu  J.  and Shroff  N. B. (2018). PAC ranking from pairwise and listwise queries:

Lower bounds and upper bounds. arXiv preprint arXiv:1806.02970.

[30] Ren  W.  Liu  J.  and Shroff  N. B. (2019). Exploring k out of top ρ fraction of arms in stochastic

bandits. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics.

[31] Saha  A. and Gopalan  A. (2019a). Active ranking with subset-wise preferences. In Proceedings

of Machine Learning Research  Proceedings of Machine Learning Research. PMLR.

[32] Saha  A. and Gopalan  A. (2019b). From PAC to instance-optimal sample complexity in the

Plackett-Luce model. arXiv preprint arXiv:1903.00558.

[33] Shah  N.  Balakrishnan  S.  Guntuboyina  A.  and Wainwright  M. (2016). Stochastically
transitive models for pairwise comparisons: Statistical and computational issues. In International
Conference on Machine Learning.

[34] Shah  N. B. and Wainwright  M. J. (2017). Simple  robust and optimal ranking from pairwise

comparisons. Journal of machine learning research.

[35] Szörényi  B.  Busa-Fekete  R.  Paul  A.  and Hüllermeier  E. (2015). Online rank elicitation for
Plackett-Luce: A dueling bandits approach. In In Advances in Neural Information Processing
Systems.

11

,Timur Garipov
Pavel Izmailov
Dmitrii Podoprikhin
Dmitry Vetrov
Andrew Wilson
Wenbo Ren
Jia (Kevin) Liu
Ness Shroff