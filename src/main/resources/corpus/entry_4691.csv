2017,Repeated Inverse Reinforcement Learning,We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised  the agent is provided a demonstration of the desired behavior by the human. We formalize this problem  including how the sequence of tasks is chosen  in a few different ways and provide some foundational results.,Repeated Inverse Reinforcement Learning

Kareem Amin∗
Google Research

New York  NY 10011
kamin@google.com

Nan Jiang∗
Satinder Singh
Computer Science & Engineering 

University of Michigan  Ann Arbor  MI 48104

{nanjiang baveja}@umich.edu

Abstract

We introduce a novel repeated Inverse Reinforcement Learning problem: the agent
has to act on behalf of a human in a sequence of tasks and wishes to minimize the
number of tasks that it surprises the human by acting suboptimally with respect to
how the human would have acted. Each time the human is surprised  the agent is
provided a demonstration of the desired behavior by the human. We formalize this
problem  including how the sequence of tasks is chosen  in a few different ways
and provide some foundational results.

1

Introduction

One challenge in building AI agents that learn from experience is how to set their goals or rewards.
In the Reinforcement Learning (RL) setting  one interesting answer to this question is inverse RL
(or IRL) in which the agent infers the rewards of a human by observing the human’s policy in a task
[2]. Unfortunately  the IRL problem is ill-posed for there are typically many reward functions for
which the observed behavior is optimal in a single task [3]. While the use of heuristics to select from
among the set of feasible reward functions has led to successful applications of IRL to the problem
of learning from demonstration [e.g.  4]  not identifying the reward function poses fundamental
challenges to the question of how well and how safely the agent will perform when using the learned
reward function in other tasks.
We formalize multiple variations of a new repeated IRL problem in which the agent and (the same)
human face multiple tasks over time. We separate the reward function into two components  one
which is invariant across tasks and can be viewed as intrinsic to the human  and a second that is
task speciﬁc. As a motivating example  consider a human doing tasks throughout a work day  e.g. 
getting coffee  driving to work  interacting with co-workers  and so on. Each of these tasks has a
task-speciﬁc goal  but the human brings to each task intrinsic goals that correspond to maintaining
health  ﬁnancial well-being  not violating moral and legal principles  etc. In our repeated IRL setting 
the agent presents a policy for each new task that it thinks the human would do. If the agent’s policy
“surprises” the human by being sub-optimal  the human presents the agent with the optimal policy.
The objective of the agent is to minimize the number of surprises to the human  i.e.  to generalize the
human’s behavior to new tasks.
In addition to addressing generalization across tasks  the repeated IRL problem we introduce and
our results are of interest in resolving the question of unidentiﬁability of rewards from observations
in standard IRL. Our results are also of interest to a particular aspect of the concern about how
to make sure that the AI systems we build are safe  or AI safety. Speciﬁcally  the issue of reward
misspeciﬁcation is often mentioned in AI safety articles [e.g.  5  6  7]. These articles mostly discuss
broad ethical concerns and possible research directions  while our paper develops mathematical
formulations and algorithmic solutions to a speciﬁc way of addressing reward misspeciﬁcation.

*This paper extends an unpublished arXiv paper by the authors [1].
∗Equal contribution.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In summary form  our contributions include: (1) an efﬁcient reward-identiﬁcation algorithm when the
agent can choose the tasks in which it observes human behavior; (2) an upper bound on the number of
total surprises when no assumptions are made on the tasks  along with a corresponding lower bound;
(3) an extension to the setting where the human provides sample trajectories instead of complete
behavior; and (4) identiﬁcation guarantees when the agent can only choose the task rewards but is
given a ﬁxed task environment.

value function or long-term utility of π is deﬁned as V π(s) = (1−γ) E[(cid:80)∞
Similarly  the Q-value function is Qπ(s  a) = (1− γ) E[(cid:80)∞

2 Markov Decision Processes (MDPs)
An MDP is speciﬁed by its state space S  action space A  initial state distribution µ ∈ ∆(S)  transition
function (or dynamics) P : S × A → ∆(S)  reward function Y : S → R  and discount factor
γ ∈ [0  1). We assume ﬁnite S and A  and ∆(S) is the space of all distributions over S. A policy π :
S → A describes an agent’s behavior by specifying the action to take in each state. The (normalized)
t=1 γt−1Y (st)|s0 = s; π].2
t=1 γt−1Y (st)|s0 = s  a0 = a; π]. Where
necessary we will use the notation V π
P Y to avoid ambiguity about the dynamics and the reward
function. Let π(cid:63) : S → A be an optimal policy  which maximizes V π and Qπ in all states (and
actions) simultaneously.
Given an initial distribution over states  µ  a scalar value that measures the goodness of π is deﬁned
as Es∼µ[V π(s)]. We introduce some further notation to express Es∼µ[V π(s)] in vector-matrix form.
µ P ∈ R|S| be the normalized state occupancy under initial distribution µ  dynamics P   and
Let ηπ
t=1 γt−1I(st = s)|s0 ∼ µ; π] (I(·) is the indicator function).
This vector can be computed in closed-form as ηπ
  where
P π is an |S| × |S| matrix whose (s  s(cid:48))-th element is P (s(cid:48)|s  π(s))  and I|S| is the |S| × |S| identity
matrix. For convenience we will also treat the reward function Y as a vector in R|S|  and we have
(1)

policy π  whose s-th entry is (1−γ) E[(cid:80)∞

(cid:16)

µ(cid:62)P π (cid:0)I|S| − γP π(cid:1)−1(cid:17)(cid:62)

µ P = (1 − γ)

Es∼µ[V π(s)] = Y (cid:62)ηπ

µ P .

3 Problem setup

Here we deﬁne the repeated IRL problem. The human’s reward function θ(cid:63) captures his/her safety
concerns and intrinsic/general preferences. This θ(cid:63) is unknown to the agent and is the object of
interest herein  i.e.  if θ(cid:63) were known to the agent  the concerns addressed in this paper would be
solved. We assume that the human cannot directly communicate θ(cid:63) to the agent but can evaluate the
agent’s behavior in a task as well as demonstrate optimal behavior. Each task comes with an external
reward function R  and the goal is to maximize the reward with respect to Y := θ(cid:63) + R in each task.
As a concrete example  consider an agent for an autonomous vehicle. In this case  θ(cid:63) represents the
cross-task principles that deﬁne good driving (e.g.  courtesy towards pedestrians and other vehicles) 
which are often difﬁcult to explicitly describe. In contrast  R  the task-speciﬁc reward  could reward
the agent for successfully completing parallel parking. While R is easier to construct  it may not
completely capture what a human deems good driving. (For example  an agent might successfully
parallel park while still boxing in neighboring vehicles.)
More formally  a task is deﬁned by a pair (E  R)  where E = (S A  µ  P  γ) is the task environment
(i.e.  a controlled Markov process) and R is the task-speciﬁc reward function (task reward). We
assume that all tasks share the same S A  γ  with |A| ≥ 2  but may differ in the initial distribution µ 
dynamics P   and task reward R; all of the task-specifying quantities are known to the agent. In any
task  the human’s optimal behavior is always with respect to the reward function Y = θ(cid:63) + R. We
emphasize again that θ(cid:63) is intrinsic to the human and remains the same across all tasks. Our use of
task speciﬁc reward functions R allows for greater generality than the usual IRL setting  and most
of our results apply equally to the case where R ≡ 0.
While θ(cid:63) is private to the human  the agent has some prior knowledge on θ(cid:63)  represented as a set of
possible parameters Θ0 ⊂ R|S| that contains θ(cid:63). Throughout  we assume that the human’s reward
has bounded and normalized magnitude  that is  (cid:107)θ(cid:63)(cid:107)∞ ≤ 1.

2Here we differ (w.l.o.g.) from common IRL literature in assuming that reward occurs after transition.

2

A demonstration in (E  R) reveals π(cid:63)  optimal for Y = θ(cid:63) + R under environment E  to the agent. A
common assumption in the IRL literature is that the full mapping is revealed  which can be unrealistic
if some states are unreachable from the initial distribution. We address the issue by requiring only
the state occupancy vector ηπ∗
µ P . In Section 7 we show that this also allows an easy extension to the
setting where the human only demonstrates trajectories instead of providing a policy.
Under the above framework for repeated IRL  we consider two settings that differ in how the sequence
of tasks are chosen. In both settings  we will want to minimize the number of demonstrations needed.
1. (Section 5) Agent chooses the tasks  observes the human’s behavior in each of them  and infers the
reward function. In this setting where the agent is powerful enough to choose tasks arbitrarily  we
will show that the agent will be able to identify the human’s reward function which of course implies
the ability to generalize to new tasks.
2. (Section 6) Nature chooses the tasks  and the agent proposes a policy in each task. The human
demonstrates a policy only if the agent’s policy is signiﬁcantly suboptimal (i.e.  a mistake). In this
setting we will derive upper and lower bounds on the number of mistakes our agent will make.

4 The challenge of identifying rewards

Note that it is impossible to identify θ(cid:63) from watching human behavior in a single task. This is
because any θ(cid:63) is fundamentally indistinguishable from an inﬁnite set of reward functions that yield
exactly the policy observed in the task. We introduce the idea of behavioral equivalence below to
tease apart two separate issues wrapped up in the challenge of identifying rewards.
Deﬁnition 1. Two reward functions θ  θ(cid:48) ∈ R|S| are behaviorally equivalent in all MDP tasks  if for
any (E  R)  the set of optimal policies for (R + θ) and (R + θ(cid:48)) are the same.
We argue that the task of identifying the reward function should amount only to identifying the
(behavioral) equivalence class to which θ(cid:63) belongs. In particular  identifying the equivalence class
is sufﬁcient to get perfect generalization to new tasks. Any remaining unidentiﬁability is merely
representational and of no real consequence. Next we present a constraint that captures the reward
functions that belong to the same equivalence class.
Proposition 1. Two reward functions θ and θ(cid:48) are behaviorally equivalent in all MDP tasks if and
only if θ − θ(cid:48) = c · 1|S| for some c ∈ R  where 1|S| is an all-1 vector of length |S|.
The proof is elementary and deferred to Appendix A. For any class of θ’s that are equivalent to
each other  we can choose a canonical element to represent this class. For example  we can ﬁx an
arbitrary reference state sref ∈ S  and ﬁx the reward of this state to 0 for θ(cid:63) and all candidate θ’s.
In the rest of the paper  we will always assume such canonicalization in the MDP setting  hence
θ(cid:63) ∈ Θ0 ⊆ {θ ∈ [−1  1]|S| : θ(sref) = 0}.

5 Agent chooses the tasks
In this section  the protocol is that the agent chooses a sequence of tasks {(Et  Rt)}. For each task
(Et  Rt)  the human reveals π(cid:63)
t   which is optimal for environment Et and reward function θ(cid:63) + Rt.
Our goal is to design an algorithm which chooses {(Et  Rt)} and identiﬁes θ(cid:63) to a desired accuracy 
  using as few tasks as possible. Theorem 1 shows that a simple algorithm can identify θ(cid:63) after only
O(log(1/)) tasks  if any tasks may be chosen. Roughly speaking  the algorithm amounts to a binary
search on each component of θ(cid:63) by manipulating the task reward Rt.3 See the proof for the algorithm
speciﬁcation. As noted before  once the agent has identiﬁed θ(cid:63) within an appropriate tolerance  it can
compute a sufﬁciently-near-optimal policy for all tasks  thus completing the generalization objective
through the far stronger identiﬁcation objective in this setting.
Theorem 1. If θ(cid:63) ∈ Θ0 ⊆ {θ ∈ [−1  1]|S| : θ(sref) = 0}  there exists an algorithm that outputs
θ ∈ R|S| that satisﬁes (cid:107)θ − θ(cid:63)(cid:107)∞ ≤  after O(log(1/)) demonstrations.
Proof. The algorithm chooses the following ﬁxed environment in all tasks: for each s ∈ S \ {sref} 
let one action be a self-loop  and the other action transitions to sref. In sref  all actions cause self-loops.
3While we present a proof that manipulates Rt  an only slightly more complex proof applies to the setting

where all the Rt are exactly zero and the manipulation is limited to the environment [1].

3

The initial distribution over states is uniformly at random over S \ {sref}. Each task only differs in
the task reward Rt (where Rt(sref) ≡ 0 always). After observing the state occupancy of the optimal
policy  for each s we check if the occupancy is equal to 0. If so  it means that the demonstrated optimal
policy chooses to go to sref from s in the ﬁrst time step  and θ(cid:63)(s) + Rt(s) ≤ θ(cid:63)(sref) + Rt(sref) = 0;
if not  we have θ(cid:63)(s) + Rt(s) ≥ 0. Consequently  after each task we learn the relationship between
θ(cid:63)(s) and −Rt(s) on each s ∈ S \ {sref}  so conducting a binary search by manipulating Rt(s) will
identify θ(cid:63) to -accuracy after O(log(1/)) tasks.

6 Nature chooses the tasks

While Theorem 1 yields a strong identiﬁcation guarantee  it also relies on a strong assumption  that
{(Et  Rt)} may be chosen by the agent in an arbitrary manner. In this section  we let nature  who is
allowed to be adversarial for the purpose of the analysis  choose {(Et  Rt)}.
Generally speaking  we cannot obtain identiﬁcation guarantees in such an adversarial setup. As an
example  if Rt ≡ 0 and Et remains the same over time  we are essentially back to the classical IRL
setting and suffer from the degeneracy issue. However  generalization to future tasks  which is our
ultimate goal  is easy in this special case: after the initial demonstration  the agent can mimic it to
behave optimally in all subsequent tasks without requiring further demonstrations. More generally  if
nature repeats similar tasks  then the agent obtains little new information  but presumably it knows
how to behave in most cases; if nature chooses a task unfamiliar to the agent  then the agent is likely
to err  but it may learn about θ(cid:63) from the mistake.
To formalize this intuition  we consider the following protocol: the nature chooses a sequence of
tasks {(Et  Rt)} in an arbitrary manner. For every task (Et  Rt)  the agent proposes a policy πt. The
human examines the policy’s value under µt  and if the loss

lt = Es∼µ

V π(cid:63)
Et  θ(cid:63)+Rt

t

(s)

V πt
Et  θ(cid:63)+Rt

(s)

(2)

(cid:104)

(cid:105) − Es∼µ

(cid:104)

(cid:105)

t

t

µt Pt

µt Pt

is revealed to the agent (note that ηπ(cid:63)

is less than some  then the human is satisﬁed and no demonstration is needed; otherwise a mistake is
counted and ηπ(cid:63)
can be computed by the agent if needed
from π∗
t and its knowledge of the task). The main goal of this section is to design an algorithm that
has a provable guarantee on the total number of mistakes.
On human supervision Here we require the human to evaluate the agent’s policies in addition to
providing demonstrations. We argue that this is a reasonable assumption because (1) only a binary
signal I(lt > ) is needed as opposed to the precise value of lt  and (2) if a policy is suboptimal but
the human fails to realize it  arguably it should not be treated as a mistake. Meanwhile  we will also
provide identiﬁcation guarantees in Section 6.4  as the human will be relieved from the supervision
duty once θ(cid:63) is identiﬁed.
Before describing and analyzing our algorithm  we ﬁrst notice that the Equation 2 can be rewritten as

lt = (θ(cid:63) + R)(cid:62)(ηπ(cid:63)

t

− ηπt

(3)
using Equation 1. So effectively  the given environment Et in each round induces a set of state
occupancy vectors {ηπ
: π ∈ (S → A)}  and we want the agent to choose the vector that has
the largest dot product with θ(cid:63) + R. The exponential size of the set will not be a concern because
our main result (Theorem 2) has no dependence on the number of vectors  and only depends on the
dimension of those vectors. The result is enabled by studying the linear bandit version of the problem 
which subsumes the MDP setting for our purpose and is also a model of independent interest.

µt Pt

µt Pt

µt Pt

) 

6.1 The linear bandit setting
In the linear bandit setting  D is a ﬁnite action space with size |D| = K. Each task is denoted as a
pair (X  R)  where R is the task speciﬁc reward function as before. X = [x(1) ··· x(K)] is a d × K
feature matrix  where x(i) is the feature vector for the i-th action  and (cid:107)x(i)(cid:107)1 ≤ 1. When we reduce
MDPs to linear bandits  each element of D corresponds to an MDP policy  and the feature vector is
the state occupancy of that policy.
As before  R  θ(cid:63) ∈ Rd are the task reward and the human’s unknown reward  respectively. The initial
uncertainty set for θ(cid:63) is Θ0 ⊆ [−1  1]d. The value of the i-th action is calculated as (θ(cid:63) + R)(cid:62)x(i) 

4

Algorithm 1 Ellipsoid Algorithm for Repeated Inverse Reinforcement Learning
1: Input: Θ0.
2: Θ1 ← MVEE(Θ0).
3: for t = 1  2  . . . do
4:
5:
6:
7:
end if
8:
9: end for

Nature reveals (Xt  Rt).
Learner plays at = arg maxa∈D c(cid:62)
if lt >  then

t   where ct is the center of Θt. Θt+1 ← Θt.
t ) ≥ 0}).

t . Θt+1 ← MVEE({θ ∈ Θt : (θ − ct)(cid:62)(xa(cid:63)

Human reveals a(cid:63)

t − xat

t xa

t

and a(cid:63) is the action that maximizes this value. Every round the agent proposes an action a ∈ D 
whose loss is deﬁned as

lt = (θ(cid:63) + R)(cid:62)(xa(cid:63) − xa).

As before  a mistake is counted when lt >   in which case the optimal demonstration xa(cid:63) is provided
to the agent. We reiterate here that the agent only receives a binary signal I(lt > ) in addition to the
demonstration. We use the term “linear bandit” to refer to the generative process  but our interaction
protocol differs from those in the standard bandit literature where reward or cost is revealed [8  9].
We now show how to embed the previous MDP setting in the linear bandit setting.
Example 1. Given an MDP problem with variables S A  γ  θ(cid:63)  sref  Θ0 {(Et  Rt)}  we can convert
it into a linear bandit problem as follows: (all variables with prime belong to the linear bandit problem 
and we use v\i to denote the vector v with the i-th coordinate removed)

• D = {π : S → A}  d = |S| − 1  θ(cid:48)
• xπ

  Θ(cid:48)
\sref
t − Rt(sref) · 1d.

)\sref. R(cid:48)

t = (ηπ

t = R

(cid:63) = θ

µt Pt

\sref
(cid:63)

0 = {θ\sref : θ ∈ Θ0}.

  R(cid:48)

Note that there is a more straightforward conversion by letting d = |S|  θ(cid:48)
t =
t = Rt  which also preserves losses. We perform a more succinct conversion in Example 1
ηπ
µt Pt
by canonicalizing both θ(cid:63) (already assumed) and Rt (explicitly done here) and dropping the coordinate
for sref in all relevant vectors.

(cid:63) = θ(cid:63)  Θ(cid:48)

0 = Θ0  xπ

MDPs with linear rewards
In IRL literature  a generalization of the MDP setting is often con-
sidered  that reward is linear in state features φ(s) ∈ Rd [2  3]. In this new setting  θ(cid:63) and R are
reward parameters  and the actual reward is (θ(cid:63) + R)(cid:62)φ(s). This new setting can also be reduced to
the linear bandit setting similarly to Example 1  except that the state occupancy is replaced by the
discounted sum of expected feature values. Our main result  Theorem 2  will still apply automatically 
but now the guarantee will only depend on the dimension of the feature space and has no dependence
on |S|. We include the conversion below but do not further discuss this setting in the rest of the paper.
Example 2. Consider an MDP problem with state features  deﬁned by S A  γ  d ∈ Z+  θ(cid:63) ∈
Rd  Θ0 ⊆ [−1  1]d {(Et  φt ∈ Rd  Rt ∈ Rd)}  where task reward and background reward in state s
(cid:63) φt(s) and R(cid:62)φt(s) respectively  and θ(cid:63) ∈ Θ0. Suppose (cid:107)φt(s)(cid:107)∞ ≤ 1 always holds  then we
are θ(cid:62)
can convert it into a linear bandit problem as follows: D = {π : S → A}. d  θ(cid:63)  and Rt remain the
same. xπ
t is for the
purpose of normalization  so that (cid:107)xπ

h=1 γh−1E[φ(sh)| µt  Pt  π]/d. Note that the division of d in xπ

t = (1 − γ)(cid:80)∞

t (cid:107)1 ≤ (cid:107)φ(cid:107)1/d ≤ (cid:107)φ(cid:107)∞ ≤ 1.

6.2 Ellipsoid Algorithm for Repeated Inverse Reinforcement Learning

We propose Algorithm 1  and provide the mistake bound in the following theorem.
Theorem 2. For Θ0 = [−1  1]d  the number of mistakes made by Algorithm 1 is guaranteed to be
O(d2 log(d/)).

To prove Theorem 2  we quote a result from linear programming literature in Lemma 1  which is
found in standard lecture notes (e.g.  [10]  Theorem 8.8; see also [11]  Lemma 3.1.34).
Lemma 1 (Volume reduction in ellipsoid algorithm). Given any non-degenerate ellipsoid B in Rd
centered at c ∈ Rd  and any non-zero vector v ∈ Rd  let B+ be the minimum-volume enclosing
ellipsoid (MVEE) of {u ∈ B : (u − c)(cid:62)v ≥ 0}. We have vol(B+)/vol(B) ≤ e

2(d+1) .

− 1

5

Proof of Theorem 2. Whenever a mistake is made  we can induce the constraint (Rt + θ(cid:63))(cid:62)(xa(cid:63)
t ) > . Meanwhile  since at is greedy w.r.t. ct  we have (Rt + ct)(cid:62)(xa(cid:63)
xat
the center of Θt as in Line 5. Taking the difference of the two inequalities  we obtain

t −
t ) ≤ 0  where ct is

t − xat

t

t

(θ(cid:63) − ct)(cid:62)(xa(cid:63)

t − xat

t

t ) > .

(4)
Therefore  the update rule on Line 7 of Algorithm 1 preserves θ(cid:63) in Θt+1. Since the update makes
a central cut through the ellipsoid  Lemma 1 applies and the volume shrinks every time a mistake
is made. To prove the theorem  it remains to upper bound the initial volume and lower bound the
terminal volume of Θt. We ﬁrst show that an update never eliminates B∞(θ(cid:63)  /2)  the (cid:96)∞ ball
centered at θ(cid:63) with radius /2. This is because  any eliminated θ satisﬁes (θ + ct)(cid:62)(xa(cid:63)
t ) < 0.
Combining this with Equation 4  we have
t − xat

t ) ≤ (cid:107)θ(cid:63) − θ(cid:107)∞(cid:107)xa(cid:63)

t (cid:107)1 ≤ 2(cid:107)θ(cid:63) − θ(cid:107)∞.

 < (θ(cid:63) − θ)(cid:62)(xa(cid:63)

(cid:84) B∞(θ(cid:63)  /2)  which contains an (cid:96)∞ ball with radius /4 at its smallest (when θ(cid:63) is one of

The last step follows from (cid:107)x(cid:107)1 ≤ 1. We conclude that any eliminated θ should be /2 far
away from θ(cid:63) in (cid:96)∞ distance. Hence  we can lower bound the volume of Θt for any t by that
of Θ0
Θ0’s vertices). To simplify calculation  we relax this lower bound (volume of the (cid:96)∞ ball) to the
volume of the inscribed (cid:96)2 ball.
Finally we put everything together: let MT be the number of mistakes made from round 1 to T   Cd
be the volume of the unit hypersphere in Rd (i.e.  (cid:96)2 ball with radius 1)  and vol(·) denote the volume
of an ellipsoid  we have

t − xat

t − xat

t

t

t

MT

√
≤ log(vol(Θ1)) − log(vol(ΘT +1)) ≤ log(Cd(

d)d) − log(Cd(/4)d) = d log

4

d

.

√



2(d + 1)
So MT ≤ 2d(d + 1) log 4

√
d
 = O(d2 log d

 ).

6.3 Lower bound

In Section 5  we get an O(log(1/)) upper bound on the number of demonstrations  which has no
dependence on |S| (which corresponds to d + 1 in the linear bandit setting). Comparing Theorem 2
to 1  one may wonder whether the polynomial dependence on d is an artifact of the inefﬁciency of
Algorithm 1. We clarify this issue by proving a lower bound  showing that Ω(d log(1/)) mistakes
are inevitable in the worst case when nature chooses the tasks. We provide a proof sketch below  and
the complete proof is deferred to Appendix E.
Theorem 3. For any randomized algorithm4 in the linear bandit setting  there always exists θ(cid:63) ∈
[−1  1]d and an adversarial sequence of {(Xt  Rt)} that potentially adapts to the algorithm’s previous
decisions  such that the expected number of mistakes made by the algorithm is Ω(d log(1/)).
Proof Sketch. We randomize θ(cid:63) by sampling each element i.i.d. from Unif([−1  1]). We will prove
that there exists a strategy of choosing (Xt  Rt) such that any algorithm’s expected number of
mistakes is Ω(d log(1/)  which proves the theorem as max is no less than average.
In our construction  Xt = [0d  ejt]  where jt is some index to be speciﬁed. Hence  every round
the agent is essentially asked to decided whether θ(jt) ≥ −Rt(jt). The adversary’s strategy goes in
phases  and Rt remains the same during each phase. Every phase has d rounds where jt is enumerated
over {1  . . .   d}.
The adversary will use Rt to shift the posterior on θ(jt) + Rt(jt) so that it is centered around the
origin; in this way  the agent has about 1/2 probability to make an error (regardless of the algorithm) 
and the posterior interval will be halved. Overall  the agent makes d/2 mistakes in each phase  and
there will be about log(1/) phases in total  which gives the lower bound.

Applying the lower bound to MDPs The above lower bound is stated for the linear bandit setting. In
principle  we need to prove lower bound for MDPs separately  because linear bandits are more general
than MDPs for our purpose  and the hard instances in linear bandits may not have corresponding

4While our Algorithm 1 is deterministic  randomization is often crucial for online learning in general [12].

6

MDP instances. In Lemma 2 below  we show that a certain type of linear bandit instances can
always be emulated by MDPs with the same number of actions  and the hard instances constructed in
Theorem 3 indeed satisfy the conditions for such a type; in particular  we require the feature vectors
to be non-negative and have (cid:96)1 norm bounded by 1. As a corollary  an Ω(|S| log(1/)) lower bound
for the MDP setting (even with a small action space |A| = 2) follows directly from Theorem 3. The
proof of Lemma 2 is deferred to Appendix B.
Lemma 2 (Linear bandit to MDP conversion). Let (X  R) be a linear bandit task  and K be the
number of actions. If every xa is non-negative and (cid:107)xa(cid:107)1 ≤ 1  then there exists an MDP task
(E  R(cid:48)) with d + 1 states and K actions  such that under some choice of sref  converting (E  R(cid:48)) as
in Example 1 recovers the original problem.

6.4 On identiﬁcation when nature chooses tasks

While Theorem 2 successfully controls the number of total mistakes  it completely avoids the
identiﬁcation problem and does not guarantee to recover θ(cid:63). In this section we explore further
conditions under which we can obtain identiﬁcation guarantees when Nature chooses the tasks.
The ﬁrst condition  stated in Proposition 2  implies that if we have made all the possible mistakes 
then we have indeed identiﬁed the θ(cid:63)  where the identiﬁcation accuracy is determined by the tolerance
parameter  that deﬁnes what is counted as a mistake. Due to space limit  the proof is deferred to
Appendix C.
Proposition 2. Consider the linear bandit setting. If there exists T0 such that for any round t ≥ T0 
no more mistakes can be ever made by the algorithm for any choice of (Et  Rt) and any tie-braking
mechanism  then we have θ(cid:63) ∈ B∞(cT0  ).
While the above proposition shows that identiﬁcation is guaranteed if the agent exhausts the mistakes 
the agent has no ability to actively fulﬁll this condition when nature chooses tasks. For a stronger
identiﬁcation guarantee  we may need to grant the agent some freedom in choosing the tasks.
Identiﬁcation with ﬁxed environment Here we consider a setting that ﬁts in between Section 5
(completely active) and Section 6.1 (completely passive)  where the environment E (hence the
induced feature vectors {x(1)  x(2)  . . .   x(K)}) is given and ﬁxed  and the agent can arbitrarily
choose the task reward Rt. The goal is to obtain identiﬁcation guarantee in this intermediate setting.
Unfortunately  a degenerate case can be easily constructed that prevents the revelation of any in-
formation about θ(cid:63). In particular  if x(1) = x(2) = . . . = x(K)  i.e.  the environment is completely
uncontrolled  then all actions are equally optimal and nothing can be learned. More generally  if
for some v (cid:54)= 0 we have v(cid:62)x(1) = v(cid:62)x(2) = . . . = v(cid:62)x(K)  then we may never recover θ(cid:63) along
the direction of v. In fact  Proposition 1 can be viewed as an instance of this result where v = 1|S|
µ P ≡ 1)  and that is why we have to remove such redundancy in Example 1 in
(recall that 1(cid:62)
order to discuss identiﬁcation in MDPs. Therefore  to guarantee identiﬁcation in a ﬁxed environment 
the feature vectors must have signiﬁcant variation in all directions  and we capture this intuition
by deﬁning a diversity score spread(X) (Deﬁnition 2) and showing that the identiﬁcation accuracy
depends inversely on the score (Theorem 4).

Deﬁnition 2. Given the feature matrix X =(cid:2)x(1) x(2)
spread(X) as the d-th largest singular value of (cid:101)X := X(IK − 1
such that after round T we have (cid:107)cT − θ(cid:63)(cid:107)∞ ≤ (cid:112)(K − 1)/2/spread(X).

Theorem 4. For a ﬁxed feature matrix X  if spread(X) > 0  then there exists a sequence
R1  R2  . . .   RT with T = O(d2 log(d/)) and a sequence of tie-break choices of the algorithm 

|S|ηπ

x(K)(cid:3) whose size is d × K  deﬁne

···

K 1K1(cid:62)
K).

√

The proof is deferred to Appendix D. The
K dependence in Theorem 4 may be of concern as K can
be exponentially large. However  Theorem 4 also holds if we replace X by any matrix that consists
of X’s columns  so we may choose a small yet most diverse set of columns as to optimize the bound.

7 Working with trajectories

In previous sections  we have assumed that the human evaluates the agent’s performance based on the
state occupancy of the agent’s policy  and demonstrates the optimal policy in terms of state occupancy

7

Nature reveals (Et  Rt). Agent rolls-out a trajectory using πt greedily w.r.t. ct + Rt.
if agent takes a in s with Q(cid:63)(s  a) < V (cid:63)(s) −  then

Algorithm 2 Trajectory version of Algorithm 1 for MDPs
1: Input: Θ0  H  n.
2: Θ1 ← MVEE(Θ0)  i ← 0  ¯Z ← 0  ¯Z (cid:63) ← 0.
3: for t = 1  2  . . . do
4:
5: Θt+1 ← Θt.
6:
7:
8:
9:
10:
11:
12:
end if
13:
14: end for

end if

i

Human produces an H-step trajectory from s. Let the empirical state occupancy be ˆz(cid:63) H
i ← i + 1  ¯Z (cid:63) ← ¯Z (cid:63) + ˆz(cid:63) H
Let zi be the state occupancy of πt from initial state s  and ¯Z ← ¯Z + zi.
if i = n then

.

i

.

Θt+1 ← MVEE({θ ∈ Θt : (θ − ct)(cid:62)( ¯Z (cid:63) − ¯Z) ≥ 0}).

i ← 0  ¯Z ← 0  ¯Z (cid:63) ← 0.

as well. In practice  we would like to instead assume that for each task  the agent rolls out a trajectory 
and the human shows an optimal trajectory if he/she ﬁnds the agent’s trajectory unsatisfying. We
are still concerned about upper bounding the number of total mistakes  and aim to provide a parallel
version of Theorem 2.
Unlike in traditional IRL  in our setting the agent is also acting  which gives rise to many subtleties.
First  the total reward on the agent’s single trajectory is a random variable  and may deviate from the
expected value of its policy. Therefore  it is generally impossible to decide if the agent’s policy is
near-optimal  and instead we assume that the human can check if each action that the agent takes
in the trajectory is near-optimal: when the agent takes a at state s  an error is counted if and only if
Q(cid:63)(s  a) < V (cid:63)(s) − . This criterion can be viewed as a noisy version of the one used in previous
sections  as taking expectation of V (cid:63)(s) − Q(cid:63)(s  π(s)) over the occupancy induced by π will recover
Equation 2.
While this resolves the issue on the agent’s side  how should the human provide his/her optimal
trajectory? The most straightforward protocol is that the human rolls out a trajectory from the
initial distribution of the task  µt. We argue that this is not a reasonable protocol for two reasons:
(1) in expectation  the reward collected by the human may be less than that by the agent  because
conditioning on the event that an error is spotted may introduce a selection bias; (2) the human may
not encounter the problematic state in his/her own trajectory  hence the information provided in the
trajectory may be irrelevant.
To resolve this issue  we consider a different protocol where the human rolls out a trajectory using an
optimal policy from the very state where the agent errs.
Now we discuss how we can prove a parallel of Theorem 2 under this new protocol. First  let’s
assume that the demonstration were still given in the form a state occupancy vector starting at the
problematic state. In this case  we can reduce to the setting of Section 6 by changing µt to a point
mass on the problematic state.5 To apply the algorithm and the analysis in Section 6  it remains
to show that the notion of error in this section (a suboptimal action) implies the notion of error in
Section 6 (a suboptimal policy): let s be the problematic state and π be the agent’s policy  we have
V π(s) = Qπ(s  π(s)) ≤ Q(cid:63)(s  π(s)) < V (cid:63)(s) − . So whenever a suboptimal action is spotted in
state s  it indeed implies that the agent’s policy is suboptimal for s as the initial state. Hence  we can
run Algorithm 1 as-is and Theorem 2 immediately applies.
To tackle the remaining issue that the demonstration is in terms of a single trajectory  we will not
update Θt after each mistake as in Algorithm 1  but only make an update after every mini-batch of
mistakes  and aggregate them to form accurate update rules. See Algorithm 2. The formal guarantee
of the algorithm is stated in Theorem 5  whose proof is deferred to Appendix G.

5At the ﬁrst glance this might seem suspicious: the problematic state is random and depends on the learner’s
current policy  but in RL the initial distribution is usually ﬁxed and the learner has no control over it. This
concern is removed thanks to our adversarial setup on (Et  Rt) (of which µt is a component).

8

Theorem 5. ∀δ ∈ (0  1)  with probability at least 1− δ  the number of mistakes made by Algorithm 2
where d = |S| 6
with parameters Θ0 = [−1  1]d  H =

  and n =

4d(d+1) log 6

log(

√


d

)

(cid:108) log(12/)

(cid:109)

1−γ

(cid:38)

δ
322

(cid:39)

is at most ˜O( d2

2 log( d

δ )).7

8 Related work & Conclusions

Most existing work in IRL focused on inferring the reward function8 using data acquired from a ﬁxed
environment [2  3  18  19  20  21  22]. There is prior work on using data collected from multiple —
but exogenously ﬁxed — environments to predict agent behavior [23]. There are also applications
where methods for single-environment MDPs have been adapted to multiple environments [19].
Nevertheless  all these works consider the objective of mimicking an optimal behavior in the presented
environment(s)  and do not aim at generalization to new tasks that is the main contribution of this
paper. Recently  Hadﬁeld-Menell et al. [24] proposed cooperative inverse reinforcement learning 
where the human and the agent act in the same environment  allowing the human to actively resolve
the agent’s uncertainty on the reward function. However  they only consider a single environment (or
task)  and the unidentiﬁability issue of IRL still exists. Combining their interesting framework with
our resolution to unidentiﬁability (by multiple tasks) can be an interesting future direction.

Acknowledgement

This work was supported in part by NSF grant IIS 1319365 (Singh & Jiang) and in part by a Rackham
Predoctoral Fellowship from the University of Michigan (Jiang). Any opinions  ﬁndings  conclusions 
or recommendations expressed here are those of the authors and do not necessarily reﬂect the views
of the sponsors.

References
[1] Kareem Amin and Satinder Singh. Towards resolving unidentiﬁability in inverse reinforcement

learning. arXiv preprint arXiv:1601.06569  2016.

[2] Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceed-

ings of the 17th International Conference on Machine Learning  pages 663–670  2000.

[3] Pieter Abbeel and Andrew Y Ng. Apprenticeship Learning via Inverse Reinforcement Learning.
In Proceedings of the 21st International Conference on Machine learning  page 1. ACM  2004.

[4] Pieter Abbeel  Adam Coates  Morgan Quigley  and Andrew Y Ng. An application of reinforce-
ment learning to aerobatic helicopter ﬂight. Advances in neural information processing systems 
19:1  2007.

[5] Nick Bostrom. Ethical issues in advanced artiﬁcial intelligence. Science Fiction and Philosophy:

From Time Travel to Superintelligence  pages 277–284  2003.

[6] Stuart Russell  Daniel Dewey  and Max Tegmark. Research priorities for robust and beneﬁcial

artiﬁcial intelligence. AI Magazine  36(4):105–114  2015.

[7] Dario Amodei  Chris Olah  Jacob Steinhardt  Paul Christiano  John Schulman  and Dan Mané.

Concrete problems in ai safety. arXiv preprint arXiv:1606.06565  2016.

[8] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3(Nov):397–422  2002.

to d = |S| − 1 by dropping the sref coordinate in all relevant vectors but that complicates presentation.

6Here we use the simpler conversion explained right after Example 1. We can certainly improve the dimension
7A log log(1/) term is suppressed in ˜O(·).
8While we do not discuss it here  in the economics literature  the problem of inferring an agent’s utility from
behavior-queries has long been studied under the heading of utility or preference elicitation [13  14  15  16  17].
While our result in Section 5 uses similar techniques to elicit the reward function  we do so purely by observing
the human’s behavior without external source of information (e.g.  query responses).

9

[9] Varsha Dani  Thomas P Hayes  and Sham M Kakade. Stochastic linear optimization under

bandit feedback. In COLT  pages 355–366  2008.

[10] Ryan O’Donnell. 15-859(E) – linear and semideﬁnite programming: lecture notes. Carnegie
https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/

Mellon University  2011.
class/15859-f11/www/notes/lecture08.pdf.

[11] Martin Grötschel  László Lovász  and Alexander Schrijver. Geometric algorithms and combina-

torial optimization  volume 2. Springer Science & Business Media  2012.

[12] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning  4(2):107–194  2011.

[13] Urszula Chajewska  Daphne Koller  and Ronald Parr. Making rational decisions using adaptive

utility elicitation. In AAAI/IAAI  pages 363–369  2000.

[14] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior (60th

Anniversary Commemorative Edition). Princeton university press  2007.

[15] Kevin Regan and Craig Boutilier. Regret-based reward elicitation for markov decision processes.
In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  pages
444–451. AUAI Press  2009.

[16] Kevin Regan and Craig Boutilier. Eliciting additive reward functions for markov decision
processes. In IJCAI Proceedings-International Joint Conference on Artiﬁcial Intelligence 
volume 22  page 2159  2011.

[17] Constantin A Rothkopf and Christos Dimitrakakis. Preference elicitation and inverse reinforce-
ment learning. In Machine Learning and Knowledge Discovery in Databases  pages 34–48.
Springer  2011.

[18] Adam Coates  Pieter Abbeel  and Andrew Y Ng. Learning for control from multiple demonstra-
tions. In Proceedings of the 25th international conference on Machine learning  pages 144–151.
ACM  2008.

[19] Brian D Ziebart  Andrew L Maas  J Andrew Bagnell  and Anind K Dey. Maximum entropy

inverse reinforcement learning. In AAAI  pages 1433–1438  2008.

[20] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. Urbana 

51:61801  2007.

[21] Umar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning. In

Advances in neural information processing systems  pages 1449–1456  2007.

[22] Kevin Regan and Craig Boutilier. Robust policy computation in reward-uncertain MDPs using

nondominated policies. In AAAI  2010.

[23] Nathan D Ratliff  J Andrew Bagnell  and Martin A Zinkevich. Maximum margin planning. In
Proceedings of the 23rd International Conference on Machine Learning  pages 729–736. ACM 
2006.

[24] Dylan Hadﬁeld-Menell  Stuart J Russell  Pieter Abbeel  and Anca Dragan. Cooperative inverse
reinforcement learning. In Advances in Neural Information Processing Systems  pages 3909–
3917  2016.

10

,Jing Xiang
Seyoung Kim
Eunho Yang
Aurelie Lozano
Pradeep Ravikumar
Kareem Amin
Nan Jiang
Satinder Singh