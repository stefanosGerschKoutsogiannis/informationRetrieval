2019,Distributionally Robust Optimization and Generalization in Kernel Methods,Distributionally robust optimization (DRO) has attracted attention in machine learning due to its connections to regularization  generalization  and robustness. Existing work has considered uncertainty sets based on phi-divergences and Wasserstein distances  each of which have drawbacks. In this paper  we study DRO with uncertainty sets measured via maximum mean discrepancy (MMD). We show that MMD DRO is roughly equivalent to regularization by the Hilbert norm and  as a byproduct  reveal deep connections to classic results in statistical learning. In particular  we obtain an alternative proof of a generalization bound for Gaussian kernel ridge regression via a DRO lense. The proof also suggests a new regularizer. Our results apply beyond kernel methods: we derive a generically applicable approximation of MMD DRO  and show that it generalizes recent work on variance-based regularization.,Distributionally Robust Optimization and

Generalization in Kernel Methods

Matthew Staib

MIT CSAIL

mstaib@mit.edu

Stefanie Jegelka

MIT CSAIL

stefje@csail.mit.edu

Abstract

Distributionally robust optimization (DRO) has attracted attention in machine
learning due to its connections to regularization  generalization  and robustness.
Existing work has considered uncertainty sets based on φ-divergences and Wasser-
stein distances  each of which have drawbacks. In this paper  we study DRO with
uncertainty sets measured via maximum mean discrepancy (MMD). We show that
MMD DRO is roughly equivalent to regularization by the Hilbert norm and  as a
byproduct  reveal deep connections to classic results in statistical learning. In par-
ticular  we obtain an alternative proof of a generalization bound for Gaussian kernel
ridge regression via a DRO lense. The proof also suggests a new regularizer. Our
results apply beyond kernel methods: we derive a generically applicable approxi-
mation of MMD DRO  and show that it generalizes recent work on variance-based
regularization.

1

Introduction

(cid:80)

x∼ˆPn

[(cid:96)f (x)] = 1
n

Distributionally robust optimization (DRO) is an attractive tool for improving machine learning
models. Instead of choosing a model f to minimize empirical risk E
i (cid:96)f (xi) 
an adversary is allowed to perturb the sample distribution within a set U centered around the
empirical distribution ˆPn. DRO seeks a model that performs well regardless of the perturbation:
Ex∼Q[(cid:96)f (x)]. The induced robustness can directly imply generalization: if the data that
inf f supQ∈U
forms ˆPn is drawn from a population distribution P  and U is large enough to contain P  then we
implicitly optimize for P too and the DRO objective value upper bounds out of sample performance.
More broadly  robustness has gained attention due to adversarial examples [17  39  26]; indeed  DRO
generalizes robustness to adversarial examples [35  36].
In machine learning  the DRO uncertainty set U has so far always been deﬁned as a φ-divergence
ball or Wasserstein ball around the empirical distribution ˆPn. These choices are convenient  due
to a number of structural results. For example  DRO with χ2-divergence is roughly equivalent to
regularizing by variance [18  22  31]  and the worst case distribution Q ∈ U can be computed exactly
in O(n log n) [37]. Moreover  DRO with Wasserstein distance is asymptotically equivalent to certain
common norm penalties [15]  and the worst case Q ∈ U can be computed approximately in several
cases [28  14]. These structural results are key  because the most challenging part of DRO is solving
(or bounding) the DRO objective.
However  there are substantial drawbacks to these two types of uncertainty sets. Any φ-divergence
uncertainty set U around ˆPn contains only distributions with the same (ﬁnite) support as ˆPn. Hence 
the population P is typically not in U  and so the DRO objective value cannot directly certify out of
sample performance. Wasserstein uncertainty sets do not suffer from this problem. But  they are
more computationally expensive  and the above key results on equivalences and computation need
nontrivial assumptions on the loss function and the speciﬁc ground distance metric used.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper  we introduce and develop a new class of DRO problems  where the uncertainty set U is
deﬁned with respect to the maximum mean discrepancy (MMD) [19]  a kernel-based distance between
distributions. MMD DRO complements existing approaches and avoids some of their drawbacks 
e.g.  unlike φ-divergences  the uncertainty set U will contain P if the radius is large enough.
First  we show that MMD DRO is roughly equivalent to regularizing by the Hilbert norm (cid:107)(cid:96)f(cid:107)H
of the loss (cid:96)f (not the model f). While  in general  (cid:107)(cid:96)f(cid:107)H may be difﬁcult to compute  we show
settings in which it is tractable. Speciﬁcally  for kernel ridge regression with a Gaussian kernel  we
prove a bound on (cid:107)(cid:96)f(cid:107)H that  as a byproduct  yields generalization bounds that match (up to a small
constant) the standard ones. Second  beyond kernel methods  we show how MMD DRO generalizes
variance-based regularization. Finally  we show how MMD DRO can be efﬁciently approximated
empirically  and in fact generalizes variance-based regularization.
Overall  our results offer deeper insights into the landscape of regularization and robustness ap-
proaches  and a more complete picture of the effects of different divergences for deﬁning robustness.
In short  our contributions are:

1. We prove fundamental structural results for MMD DRO  and its rough equivalence to

penalizing by the Hilbert norm of the loss.

2. We give a new generalization proof for Gaussian kernel ridge regression by way of DRO.
Along the way  we prove bounds on the Hilbert norm of products of functions that may be
of independent interest.

3. Our generalization proof suggests a new regularizer for Gaussian kernel ridge regression.
4. We derive a computationally tractable approximation of MMD DRO  with application to
general learning problems  and we show how the aforementioned approximation generalizes
variance regularization.

2 Background and Related Work

Distributionally robust optimization (DRO) [16  2]  introduced by Scarf [32]  asks to not only perform
well on a ﬁxed problem instance (parameterized by a distribution)  but simultaneously for a range
of problems  each determined by a distribution in an uncertainty set U. This results in more robust
solutions. The uncertainty set plays a key role: it implicitly deﬁnes the induced notion of robustness.
The DRO problem we address asks to learn a model f that solves

inf
f

Ex∼Q[(cid:96)f (x)] 

(1)

(DRO)

(cid:80)n

sup
Q∈U
where (cid:96)f (x) is the loss incurred under prediction f (x).
In this work  we focus on data-driven DRO  where U is centered around an empirical sample
ˆPn = 1
i=1 δxi  and its size is determined in a data-dependent way. Data-driven DRO yields a
natural approach for certifying out-of-sample performance.
Principle 2.1 (DRO Generalization Principle). Fix any model f. Let U be a set of distributions
containing ˆPn. Suppose U is large enough so that  with probability 1 − δ  U contains the population
P. Then with probability 1 − δ  the population loss Ex∼P[(cid:96)f (x)] is bounded by
(2)

n

Ex∼P[(cid:96)f (x)] ≤ sup
Q∈U

Ex∼Q[(cid:96)f (x)].

Essentially  if the uncertainty set U is chosen appropriately  the corresponding DRO problem gives
a high probability bound on population performance. The two key steps in using Principle 2.1 are
1. arguing that U actually contains P with high probability (e.g. via concentration); 2. solving the
DRO problem on the right hand side  or an upper bound thereof.
In practice  U is typically chosen as a ball of radius  around the empirical sample ˆPn: U = {Q :
d(Q  ˆPn) ≤ }. Here  d is a discrepancy between distributions  and is of utmost signiﬁcance: the
choice of d determines how large  must be  and how tractable the DRO problem is.
In machine learning  two choices of the divergence d are prevalent  φ-divergences [1  11  22] 
and Wasserstein distance [28  33  6]. The ﬁrst option  φ-divergences  have the form dφ(P  Q) =

2

(cid:82) φ(dP/dQ) dQ. In particular  they include the χ2-divergence  which makes the DRO problem equiv-
alent to regularizing by variance [18  22  31]. Beyond better generalization  variance regularization
has applications in fairness [20]. However  a major shortcoming of DRO with φ-divergences is that
the ball U = {Q : dφ(Q  P0) ≤ } only contains distributions Q whose support is contained in
the support of P0. If P0 = ˆPn is an empirical distribution on n points  the ball U only contains
distributions with the same ﬁnite support. Hence  the population distribution P typically cannot
belong to U  and it is not possible to certify out-of-sample perfomance by Principle 2.1. Though
(cid:82) g(x  y)p dγ(x  y) :
Principle 2.1 does not apply here  generalization bounds are still possible via other means [31].
The second option  Wasserstein distance  is based on a distance metric g on the data space. The
p-Wasserstein distance Wp between measure µ  ν is given by Wp(µ  ν) = inf{
γ ∈ Π(µ  ν)}1/p  where Π(µ  ν) is the set of couplings of µ and ν [40]. Wasserstein DRO has a key
beneﬁt over φ-divergences: the set U = {Q : Wp(Q  P0) ≤ } contains continuous distributions.
Moreover  concentration results bounding Wp(P  ˆPn) with high probability are available for many
settings  e.g. [13  23  34  41]. However  Wasserstein distance is much harder to work with  and
nontrivial assumptions are needed to derive the necessary structural and algorithmic results for
solving the associated DRO problem. To our knowledge  in all Wasserstein DRO work so far  the
ground metric g is limited to slight variations of either a Euclidean or Mahalanobis metric [7  8].
Such metrics may be a poor ﬁt for complex data such as images or distributions. These assumptions
restrict the extent to which Wasserstein DRO can utilize complex  nonlinear structure in the data.

Maximum Mean Discrepancy (MMD). MMD is a distance metric between distributions that
leverages kernel embeddings. Let H be a reproducing kernel Hilbert space (RKHS) with kernel k
and norm (cid:107)·(cid:107)H. MMD is deﬁned as follows:
Deﬁnition 2.1. The maximum mean discrepancy (MMD) between distributions P and Q is

dMMD(P  Q) :=

(3)
Fact 2.1. Deﬁne the mean embedding µP of the distribution P by µP = Ex∼P[k(x ·)]. Then the
MMD between distributions P and Q can be equivalently written
dMMD(P  Q) = (cid:107)µP − µQ(cid:107)H.
(4)

Ex∼P[f (x)] − Ex∼Q[f (x)].

sup

f∈H:(cid:107)f(cid:107)H≤1

MMD and (more generally) kernel mean embeddings have been used in many applications  particu-
larly in two- and one-sample tests [19  21  25  9] and in generative modeling [12  24  38  5]. We refer
the interested reader to the monograph by Muandet et al. [30]. MMD admits efﬁcient estimation  as
well as fast convergence properties  which are of chief importance in our work.

Further related work. Beyond φ-divergences and Wasserstein distances  work in operations
research has considered DRO problems that capture uncertainty in moments of the distribution 
e.g. [10]. These approaches typically focus on ﬁrst- and second-order moments; in contrast  an MMD
uncertainty set allows high order moments to vary  depending on the choice of kernel.
Robust and adversarial machine learning have strong connections to our work and DRO more
generally. Robustness to adversarial examples [39  17]  where individual inputs to the model are
perturbed in a small ball  can be cast as a robust optimization problem [26]. When the ball is a
norm ball  this robust formulation is a special case of Wasserstein DRO [35  36]. Xu et al. [42]
study the connection between robustness and regularization in SVMs  and perturbations within a
(possibly Hilbert) norm ball. Unlike our work  their results are limited to SVMs instead of general
loss minimization. Moreover  they consider only perturbation of individual data points instead of
shifts in the entire distribution. Bietti et al. [4] show that many regularizers used for neural networks
can also be interpreted in light of an appropriately chosen Hilbert norm [3].

3 Generalization bounds via MMD DRO

The main focus of this paper is Distributionally Robust Optimization where the uncertainty set is
deﬁned via the MMD distance dMMD:

Ex∼Q[(cid:96)f (x)].

(5)

inf
f

sup

Q:dMMD(Q ˆPn)≤

3

One motivation for considering MMD in this setting are its possible implications for Generalization.
Recall that for the DRO Generalization Principle 2.1 to apply  the uncertainty set U must contain the
population distribution with high probability. To ensure this  the radius of U must be large enough.
But  the larger the radius  the more pessimistic is the DRO minimax problem  which may lead to
over-regularization. This radius depends on how quickly dMMD(P  ˆPn) shrinks to zero  i.e.  on the
empirical accuracy of the divergence.
In contrast to Wasserstein distance  which converges at a rate of O(n−1/d) [13]  MMD between the
empirical sample ˆPn and population P shrinks as O(n−1/2):
Lemma 3.1 (Modiﬁed from [30]  Theorem 3.4). Suppose that k(x  x) ≤ M for all x. Let ˆPn be an
n sample empirical approximation to P. Then with probability 1 − δ 

(cid:113)
dMMD(P  ˆPn) ≤ 2

(cid:113)

M
n +

2 log(1/δ)

n

.

(6)

The constant M is dimension-independent for many common universal kernels  e.g. Gaussian 
(cid:112)
(cid:112)
Laplace  and Matern kernels. With Lemma 3.1 in hand  we conclude a simple high probability bound
on out-of-sample performance:
Corollary 3.1. Suppose that k(x  x) ≤ M for all x. Set  = 2
probability 1 − δ  we have the following bound on population risk:
Ex∼Q[(cid:96)f (x)].

2 log(1/δ)/n. Then with

Ex∼P[(cid:96)f (x)] ≤

M/n +

sup

(7)

Q:dMMD(Q ˆPn)≤

We refer to the right hand side as the DRO adversary’s problem. In the next section we develop
results that enable us to bound its value  and consequently bound the DRO problem (5).

3.1 Bounding the DRO adversary’s problem
The DRO adversary’s problem seeks the distribution Q in the MMD ball so that Ex∼Q[(cid:96)f (x)] is as
high as possible. Reasoning about the optimal worst-case Q is the main difﬁculty in DRO. With
MMD  we take two steps for simpliﬁcation. First  instead of directly optimizing over distributions 
we optimize over their mean embeddings in the Hilbert space (described in Fact 2.1). Second  while
the adversary’s problem (7) makes sense for general (cid:96)f   we assume that the loss (cid:96)f is in H. In
case (cid:96)f (cid:54)∈ H  often k is a universal kernel  meaning under mild conditions (cid:96)f can be approximated
arbitrarily well by a member of H [30  Deﬁnition 3.3].
With the additional assumption that (cid:96)f ∈ H  the risk Ex∼P[(cid:96)f (x)] can also be written as (cid:104)(cid:96)f   µP(cid:105)H.
Then we obtain
(8)

sup

Ex∼Q[(cid:96)f (x)] ≤

sup

µQ∈H:(cid:107)µQ−µP(cid:107)H≤(cid:104)(cid:96)f   µQ(cid:105)H 

Q:dMMD(Q P)≤

where we have an inequality because not every function in H is the mean embedding of some
probability distribution. If k is a characteristic kernel [30  Deﬁnition 3.2]  the mapping P (cid:55)→ µP is
injective. In this case  the only looseness in the bound is due to discarding the constraints that Q
integrates to one and is nonnegative. However it is difﬁcult to constrain the mean embedding µQ in
this way as it is a function.
The mean embedding form of the problem is simpler to work with  and leads to further interpretations.
Theorem 3.1. Let (cid:96)f   µP ∈ H. We have the following equality:

µQ∈H:(cid:107)µQ−µP(cid:107)H≤(cid:104)(cid:96)f   µQ(cid:105)H = (cid:104)(cid:96)f   µP(cid:105)H + (cid:107)(cid:96)f(cid:107)H = Ex∼P[(cid:96)f (x)] + (cid:107)(cid:96)f(cid:107)H.

sup

(9)

In particular  the adversary’s optimal solution is µ∗Q = µP + 

(cid:107)(cid:96)(cid:107)H (cid:96)f .

Combining Theorem 3.1 with equation (8) yields our main result for this section:
Corollary 3.2. Let (cid:96)f ∈ H  let P be a probability distribution  and ﬁx  > 0. Then 

sup

Q:dMMD(P Q)≤
Q:dMMD(P Q)≤

sup

inf
f

Ex∼Q[(cid:96)f (x)] ≤ Ex∼P[(cid:96)f (x)] + (cid:107)(cid:96)f(cid:107)H
Ex∼Q[(cid:96)f (x)] ≤ inf

Ex∼P[(cid:96)f (x)] + (cid:107)(cid:96)f(cid:107)H.

f

and therefore

(10)

(11)

4

(cid:80)n
i=1(cid:107)∇x(cid:96)f (xi)(cid:107)q
∗

n

(cid:1)1/q measures a kind of

[(cid:96)f (x)] + (cid:107)∇x(cid:96)f(cid:107)ˆPn q  where (cid:107)∇x(cid:96)f(cid:107)ˆPn q =(cid:0) 1

Combining Corollary 3.2 with Corollary 3.1 shows that minimizing the empirical risk plus a norm
on (cid:96)f leads to a high probability bound on out-of-sample performance. This result is similar to
results that equate Wasserstein DRO to norm regularization. For example  Gao et al. [15] show that
under appropriate assumptions on (cid:96)f   DRO with a p-Wasserstein ball is asymptotically equivalent to
E
x∼ˆPn
q-norm average of (cid:107)∇x(cid:96)f (xi)(cid:107)∗ at each data point xi (here q is such that 1/p + 1/q = 1  and (cid:107) · (cid:107)∗
is the dual norm of the metric deﬁning the Wasserstein distance).
There are a few key differences between our result and that of Gao et al. [15]. First  the norms are
different. Second  their result penalizes only the gradient of (cid:96)f   while ours penalizes (cid:96)f directly.
Third  except for certain special cases  the Wasserstein results cannot serve as a true upper bound;
there are higher order terms that only shrink to zero as  → 0. These higher order terms may not be
so small: in high dimension d  the radius  of the uncertainty set needed so that P ∈ U shrinks very
slowly  as O(n−1/d) [13].
Remark 3.1. Theorem 3.1 and Corollary 3.2 require that (cid:96)f is in the RKHS H. Though this may
seem restrictive  if the kernel k is universal  as is the case for many kernels used in practice such as
Gaussian and Laplace kernels  we can readily extend our results to all bounded continuous functions.
Suppose (cid:96)f is a bounded continuous function on a compact metric space X . By deﬁnition (e.g. [30] 
Deﬁnition 3.3)  if k is a universal kernel on X   then for any  > 0  there is some (cid:96)(cid:48) ∈ H with
supx∈X |(cid:96)f (x) − (cid:96)(cid:48)(x)| < . It follows that for any measure P  we can bound the expectation of
(cid:96)f (x) by that of (cid:96)(cid:48): Ex∼P[(cid:96)f (x)] < Ex∼P[(cid:96)(cid:48)(x)] + . Then  we can apply our results to (cid:96)(cid:48) ∈ H.
4 Connections to kernel ridge regression

After applying Corollary 3.2  we are interested in solving:

inf
f

E
x∼ˆPn

[(cid:96)f (x)] + (cid:107)(cid:96)f(cid:107)H.

(12)

Here  we penalize our model f by (cid:107)(cid:96)f(cid:107)H. This looks similar to but is very different from the usual
penalty (cid:107)f(cid:107)H in kernel methods. In fact  Hilbert norms of function compositions such as (cid:96)f pose
several challenges. For example  f and (cid:96)f may not belong to the same RKHS – it is not hard
to construct counterexamples  even when (cid:96) is merely quadratic. So  the objective (12) is not yet
computational.
Despite these challenges  we next develop tools that will allow us to bound (cid:107)(cid:96)f(cid:107)H and use it as a
regularizer. These tools may be of independent interest to bound RKHS norms of composite functions
(e.g.  for settings as in [4]). Due to the difﬁculty of this task  we specialize to Gaussian kernels
kσ(x  y) = exp(−(cid:107)x − y(cid:107)2/(2σ2)). Since we will need to take care regarding the bandwidth σ  we
explicitly write it out for the inner product (cid:104)· ·(cid:105)σ and norm (cid:107)·(cid:107)σ  of the corresponding RKHS Hσ.
To make the setting concrete  consider kernel ridge regression  with Gaussian kernel kσ. As usual 
we assume there is a simple target function h that ﬁts our data: h(xi) = yi. Then the loss (cid:96)f of f is
(cid:96)f (x) = (f (x) − h(x))2  so we wish to solve

inf
f

E
x∼ˆPn

[(f (x) − h(x))2] + (cid:107)(f − h)2(cid:107)σ.

(13)

4.1 Bounding norms of products
To bound (cid:107)(f − h)2(cid:107)σ  it will sufﬁce to bound RKHS norms of products. The key result for this
subsection is the following deceptively simple-looking bound:
Theorem 4.1. Let f  g ∈ Hσ  that is  the RKHS corresponding to the Gaussian kernel kσ of
bandwidth σ. Then  (cid:107)f g(cid:107)σ/√2 ≤ (cid:107)f(cid:107)σ(cid:107)g(cid:107)σ.
Indeed  there are already subtleties: if f  g ∈ Hσ  then  to discuss the norm of the product f g  we
need to decrease the bandwidth from σ to σ/√2.
We prove Theorem 4.1 via two steps. First  we represent the functions f  g  and f g exactly in terms
of traces of certain matrices. This step is highly dependent on the speciﬁc structure of the Gaussian
kernel. Then  we can apply standard trace inequalities. Proofs of both results are given in Appendix B.

5

j bjkσ(xj ·). For
shorthand denote by zi = φ√2σ(xi) the (possibly inﬁnite) feature expansion of xi in H√2σ. Then 

Proposition 4.1. Let f  g ∈ Hσ have expansions f =(cid:80)
where A =(cid:80)

i and B =(cid:80)

(cid:107)f(cid:107)2
j .
j ajzjzT

σ/√2 = tr(A2B2) 

(cid:107)f g(cid:107)2
i aizizT

σ = tr(A2) 

i aikσ(xi ·) and g =(cid:80)

and

σ = tr(B2) 

(cid:107)g(cid:107)2

Lemma 4.1. Let X  Y be symmetric and positive semideﬁnite. Then tr(XY ) ≤ tr(X) tr(Y ).
With these intermediate results in hand  we can prove the main bound of interest:

Proof of Theorem 4.1. By Proposition 4.1  we may write

where A =(cid:80)

i and B =(cid:80)

(cid:107)f g(cid:107)2
i aizizT

σ/√2 = tr(A2B2) 

(cid:107)f(cid:107)2
j are chosen as described in Proposition 4.1. Since A and
B are each symmetric  it follows that A2 and B2 are each symmetric and positive semideﬁnite. Then
we can apply Lemma 4.1 to conclude that

σ = tr(B2) 

σ = tr(A2) 

j bjzjzT

(cid:107)g(cid:107)2

and

(cid:107)f g(cid:107)2

σ/√2 = tr(A2B2) ≤ tr(A2) tr(B2) = (cid:107)f(cid:107)2

σ(cid:107)g(cid:107)2
σ.

4.2

Implications: kernel ridge regression

With the help of Theorem 4.1  we can develop DRO-based bounds for actual learning problems. In
this section we develop such bounds for Gaussian kernel ridge regression  i.e. problem (13).
For shorthand  we write RQ(f ) = Ex∼Q[(cid:96)f (x)] = Ex∼Q[(f (x) − h(x))2] for the risk of f on a
distribution Q. Generalization amounts to proving that the population risk RP(f ) is not too different
than the empirical risk RˆPn
Theorem 4.2. Assume the target function h satisﬁes (cid:107)h2(cid:107)σ/√2 ≤ Λh2 and (cid:107)h(cid:107)σ ≤ Λh. Then  for
any δ > 0  with probability 1 − δ  the following holds for all functions f satisfying (cid:107)f 2(cid:107)σ/√2 ≤ Λf 2
and (cid:107)f(cid:107)σ ≤ Λf :

(f ).

(cid:113)

(cid:19)(cid:0)Λf 2 + Λh2 + 2Λf Λh

(cid:1) .

RP(f ) ≤ RˆPn

(f ) + 2√n

1 +

log(1/δ)

2

(14)

Proof. We utilize the DRO Generalization Principle 2.1  By Lemma 3.1 we know that with probability
2 log(1/δ))/√n  since kσ(x  x) ≤ M = 1. Note the
1 − δ  dMMD(ˆPn  P) ≤  for  = (2 +
bandwidth σ does not affect the convergence result. As a result of Lemma 3.1  with probability 1 − δ:
(15)

RP(f ) = Ex∼P[(f (x) − h(x))2]

(cid:18)
(cid:112)

(cid:16)

(a)

(b)

≤ E
≤ RˆPn
≤ RˆPn

(c)

x∼ˆPn

[(f (x) − h(x))2] + (cid:107)(f − h)2(cid:107)σ/√2

(cid:107)f 2(cid:107)σ/√2 + (cid:107)h2(cid:107)σ/√2 + 2(cid:107)f h(cid:107)σ/√2

(f ) + 

(f ) + (cid:0)Λf 2 + Λh2 + 2Λf Λh

(cid:1)  

(cid:17)

(16)

(17)

(18)
where (a) is by Corollary 3.2  (b) is by the triangle inequality  and (c) follows from Theorem 4.1 and
our assumptions on f and h. Plugging in the bound on  yields the result.

We placed different bounds on each of f  h  f 2  h2 to emphasize the dependence on each. Since each
is bounded separately  the DRO based bound in Theorem 4.2 allows ﬁner control of the complexity
of the function class than is typical. Since  by Theorem 4.1  the norms of f 2  h2 and f h are bounded
by those of f and h  we may also state Theorem 4.2 just with (cid:107)f(cid:107)σ and (cid:107)h(cid:107)σ.
Corollary 4.1. Assume the target function h satisﬁes (cid:107)h(cid:107)σ ≤ Λ. Then  for any δ > 0  with
probability 1 − δ  the following holds for all functions f satisfying (cid:107)f(cid:107)σ ≤ Λ:

(cid:18)

(cid:113)

(cid:19)

1 +

log(1/δ)

2

.

(19)

RP(f ) ≤ RˆPn

(f ) + 8Λ2
√n

6

Proof. We reduce to Theorem 4.2. By Theorem 4.1  we know that (cid:107)f 2(cid:107)σ/√2 ≤ (cid:107)f(cid:107)2
may be bounded above by Λ2 (and similarly for h). Therefore we can take Λf 2 = Λ2
Λh2 = Λ2

h = Λ in Theorem 4.2. The result follows by bounding

σ  which
f = Λ and

Λf 2 + Λh2 + 2Λf Λh ≤ Λ2 + Λ2 + 2Λ · Λ = 4Λ2.

Generalization bounds for kernel ridge regression are of course not new; we emphasize that the DRO
viewpoint provides an intuitive approach that also grants ﬁner control over the function complexity.
Moreover  our results take essentially the same form as the typical generalization bounds for kernel
ridge regression  reproduced below:
Theorem 4.3 (Specialized from [29]  Theorem 10.7). Assume the target function h satisﬁes (cid:107)h(cid:107)σ ≤
Λ. Then  for any δ > 0  with probability 1 − δ  it holds for all functions f satisfying (cid:107)f(cid:107)σ ≤ Λ that
(20)

(cid:113)

(cid:19)

(cid:18)

log(1/δ)

1 +

.

RP(f ) ≤ RˆPn

(f ) + 8Λ2
√n

1
2

2

Hence  our DRO-based Theorem 4.2 evidently recovers standard results up to a universal constant.

(cid:16)

(cid:17)

4.3 Algorithmic implications
The generalization result in Theorem 4.3 is often used to justify penalizing by the norm (cid:107)f(cid:107)σ  since
it is the only part of the bound (other than the risk RˆPn
(f )) that depends on f. In contrast  our
DRO-based generalization bound in Theorem 4.2 is of the form

RP(f ) − RˆPn

(21)
which depends on f through both norms (cid:107)f(cid:107)σ and (cid:107)f 2(cid:107)σ/√2. This bound motivates the use of both
norms as regularizers in kernel regression  i.e. we would instead solve
(22)

(cid:107)f 2(cid:107)σ/√2 + (cid:107)h2(cid:107)σ/√2 + 2(cid:107)f(cid:107)σ(cid:107)h(cid:107)σ

(f ) ≤ 

[(f (x) − y)2] + λ1(cid:107)f(cid:107)σ + λ2(cid:107)f 2(cid:107)σ/√2.

E
(x y)∼ˆPn

 

to consider only f of the form f =(cid:80)n

inf
f∈Hσ

Given data (xi  yi)n

i=1  for kernel ridge regression  the Representer Theorem implies that it is sufﬁcient
i=1 aikσ(xi ·). Here this is not in general possible due to
the norm of f 2. However  it is possible to evaluate and compute gradients of (cid:107)f 2(cid:107)2
: let K be
the matrix with Kij = k√2σ(xi  xj)  and let D = diag(a). Using Proposition 4.1  we can prove
(cid:107)f 2(cid:107)2
5 Approximation and connections to variance regularization

= tr((DK)4) A complete proof is given in the appendix.

σ/√2

σ/√2

In the previous section we studied bounding the MMD DRO problem (5) via Hilbert norm penalization.
Going beyond kernel methods where we search over f ∈ H  it is even less clear how to evaluate
the Hilbert norm (cid:107)(cid:96)f(cid:107)H. To circumvent this issue  next we approach the DRO problem from a
different angle: we directly search for the adversarial distribution Q. Along the way  we will build
connections to variance regularization [27  18  22  31]  where the empirical risk is regularized by the
empirical variance of (cid:96)f : VarˆPn
[(cid:96)f (x)]2. In particular  we show in
Theorem 5.1 that MMD DRO yields stronger regularization than variance.
Searching over all distributions Q in the MMD ball is intractable  so we restrict our attention to those
i=1 as the empirical sample ˆPn. All such distributions Q can be written
with the same support {xi}n
i=1 wiδxi  where w is in the n-dimensional simplex. By restricting the set of candidate
distributions Q  we make the adversary weaker:

as Q =(cid:80)n

[(cid:96)f (x)2] − E

((cid:96)f ) = E

x∼ˆPn

x∼ˆPn

supQ Ex∼Q[(cid:96)f (x)]
s.t.

dMMD(Q  ˆPn) ≤ 

≥

supw
s.t.

(cid:80)n
dMMD((cid:80)n
(cid:80)n

i=1 wi = 1

i=1 wi(cid:96)f (xi)

wi ≥ 0 ∀i = 1  . . .   n.

i=1 wiδxi  ˆPn) ≤ 

(23)

By restricting the support of Q  it is no longer possible to guarantee out of sample performance  since
it typically will have different support. Yet  as we will see  problem (23) has nice connections.

7

Figure 1: Comparison of the two regularizers (cid:107)f(cid:107)2
(right) settings  across a parameter sweep of λ. The x-axis is shifted to make comparison easier.

σ and (cid:107)f 2(cid:107)σ/√2 in both the easy (left) and hard

The dMMD constraint is a quadratic penalty on v = w − 1
n(cid:88)
deﬁnition of MMD:

(cid:32) n(cid:88)

(cid:33)2

wiδxi  ˆPn

=

dMMD

wik(xi ·) −

1
n

k(xi ·)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

H

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

H

n 1  as one may see via the mean embedding

=

vik(xi ·)

.

(24)

n 1)T K(w− 1

The last term is vT Kv = (w− 1
n 1)  where K is the kernel matrix with Kij = k(xi  xj).
If the radius  of the uncertainty set is small enough  the constraints wi ≥ 0 are inactive  and can be
ignored. By dropping these constraints  we can solve the adversary’s problem in closed form:
Lemma 5.1. Let (cid:126)(cid:96) be the vector with i-th element (cid:96)f (xi). If  is small enough that the constraints wi
(cid:113)
are not active  then the optimal value of problem (23) is given by
(cid:126)(cid:96)T K−1(cid:126)(cid:96) − ((cid:126)(cid:96)T K

−11)2
1T K−11 .

E
x∼ˆPn

[(cid:96)f (x)] + 

(25)

In other words  ﬁtting a model to minimize the support-constrained approximation of MMD DRO
is equivalent to penalizing by the nonconvex regularizer in Lemma 5.1. To better understand this
regularizer  consider  for instance  the case that the kernel matrix K equals the identity I. This will
happen e.g. for a Gaussian kernel as the bandwidth σ approaches zero. Then  the regularizer equals

(cid:113)
(cid:126)(cid:96)T K−1(cid:126)(cid:96) − ((cid:126)(cid:96)T K



−11)2

1T K−11 = 

(cid:113)
(cid:126)(cid:96)T (cid:126)(cid:96) − ((cid:126)(cid:96)T 1)2

1T 1 = √n

(cid:113)

VarˆPn

((cid:96)f ).

(26)

In fact  this equivalence holds a bit more generally:

Lemma 5.2. Let K = aI + b11T . Then 

(cid:113)
(cid:126)(cid:96)T K−1(cid:126)(cid:96) − ((cid:126)(cid:96)T K−11)2

1T K−11 = a−1/2√n

(cid:113)

VarˆPn

((cid:96)f ).

As a consequence  we conclude that with the right choice of kernel k  MMD DRO is a stronger
regularizer than variance:
Theorem 5.1. There exists a kernel k so that MMD DRO bounds the variance regularized problem:

(cid:113)

E
x∼ˆPn

[(cid:96)f (x)] ≤ E

x∼ˆPn

[(cid:96)f (x)] + √n

6 Experiments

VarˆPn

((cid:96)f ) ≤

sup

[(cid:96)f (x)].

Q:dMMD(Q ˆPn)≤

(27)

In subsection 4.3 we proposed an alternate regularizer for kernel ridge regression  speciﬁcally 
penalizing (cid:107)f 2(cid:107)σ/√2 instead of (cid:107)f(cid:107)2
σ. Here we probe the new regularizer on a synthetic problem
where we can precisely compute the population risk RP(f ). Consider the Gaussian kernel kσ with
σ = 1. Fix the ground truth h = kσ(1 ·) − kσ(−1 ·) ∈ Hσ. Sample 104 points from a standard one
dimensional Gaussian  and set this as the population P. Then subsample n points xi = h(xi) + i 

8

100101102103Regularizerstrengthλforkf2kσ/√20.010.020.030.04PopulationRMSEkfk2σkf2kσ/√210−210−1100101Regularizerstrengthλforkfk2σ10−1100101102103Regularizerstrengthλforkf2kσ/√20.30.40.5PopulationRMSEkfk2σkf2kσ/√210−210−1100101102Regularizerstrengthλforkfk2σwhere i are Gaussian. We consider both an easy regime  where n = 103 and Var(i) = 10−2  and a
hard regime where n = 102 and Var(i) = 1. On the empirical data  we ﬁt f ∈ Hσ by minimizing
square loss plus either λ(cid:107)f(cid:107)2
σ (as is typical) or λ(cid:107)f 2(cid:107)σ/√2 (our proposal). We average over 102
resampling trials for the easy case and 103 for the hard case  and report 95% conﬁdence intervals.
Figure 1 shows the result in each case for a parameter sweep over λ. If λ is tuned properly  the
tighter regularizer (cid:107)f 2(cid:107)σ/√2 yields better performance in both cases. It also appears the regularizer
(cid:107)f 2(cid:107)σ/√2 is less sensitive to the choice of λ: performance decays slowly when λ is too low.
7 Conclusion

We introduce MMD DRO  distributionally robust optimization with maximum mean discrepancy
uncertainty sets. We prove fundamental structural results and upper bounds for MMD DRO  and
unearth deep connections  in particular to Gaussian kernel ridge regression and variance regularization.
Several open questions remain. In terms of theory  our MMD DRO approach to generalization bounds
leaves much new ground to explore. In particular  we conjecture that our approach might also work
for ridge regression with non-Gaussian kernels. Practically  there is also much left to do to make
MMD DRO a general purpose tool. We have presented two approximations of MMD DRO  each
with strengths and drawbacks: the upper bound in Corollary 3.2 enables our kernel ridge regression
generalization bound  but is potentially loose  and is difﬁcult to use more generally because the
Hilbert norm is tricky to compute; the discrete approximation in Section 5 is more practical but is
not an upper bound on the MMD DRO problem. Future work could address these drawbacks  or
potentially develop a tractable exact reformulation of the DRO problem.

Acknowledgements

This work was supported by The Defense Advanced Research Projects Agency (grant number YFA17
N66001-17-1-4039). The views  opinions  and/or ﬁndings contained in this article are those of the
author and should not be interpreted as representing the ofﬁcial views or policies  either expressed
or implied  of the Defense Advanced Research Projects Agency or the Department of Defense. We
thank Cameron Musco and Joshua Robinson for helpful conversations  and Marwa El Halabi and
Sebastian Claici for comments on the draft.

References
[1] Aharon Ben-Tal  Dick den Hertog  Anja De Waegenaere  Bertrand Melenberg  and Gijs Rennen. Robust
solutions of optimization problems affected by uncertain probabilities. Management Science  59(2):
341–357  2013.

[2] Dimitris Bertsimas  Vishal Gupta  and Nathan Kallus. Data-driven robust optimization. Mathematical

Programming  167(2):235–292  Feb 2018.

[3] Alberto Bietti and Julien Mairal. Group invariance  stability to deformations  and complexity of deep

convolutional representations. The Journal of Machine Learning Research  20(1):876–924  2019.

[4] Alberto Bietti  Grégoire Mialon  Dexiong Chen  and Julien Mairal. A kernel perspective for regularizing
deep neural networks. In Proceedings of the 36th International Conference on Machine Learning. PMLR 
2019.

[5] Mikołaj Bi´nkowski  Dougal J. Sutherland  Michael Arbel  and Arthur Gretton. Demystifying MMD GANs.

In International Conference on Learning Representations  2018.

[6] Jose Blanchet  Yang Kang  and Karthyek Murthy. Robust wasserstein proﬁle inference and applications to

machine learning. arXiv preprint arXiv:1610.05627  2016.

[7] Jose Blanchet  Yang Kang  Fan Zhang  and Karthyek Murthy. Data-driven optimal transport cost selection

for distributionally robust optimization. arXiv preprint arXiv:1705.07152  2017.

[8] Jose Blanchet  Karthyek Murthy  and Fan Zhang. Optimal transport based distributionally robust optimiza-

tion: Structural properties and iterative schemes. arXiv preprint arXiv:1810.02403  2018.

[9] Kacper Chwialkowski  Heiko Strathmann  and Arthur Gretton. A kernel test of goodness of ﬁt.

In
Maria Florina Balcan and Kilian Q. Weinberger  editors  Proceedings of The 33rd International Conference
on Machine Learning  volume 48 of Proceedings of Machine Learning Research  pages 2606–2615  New
York  New York  USA  20–22 Jun 2016. PMLR.

[10] Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with application

to data-driven problems. Operations Research  58(3):595–612  2010.

[11] John Duchi  Peter Glynn  and Hongseok Namkoong. Statistics of robust optimization: A generalized

empirical likelihood approach. arXiv preprint arXiv:1610.03425  2016.

9

[12] Gintare Karolina Dziugaite  Daniel M. Roy  and Zoubin Ghahramani. Training generative neural networks
via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty
in Artiﬁcial Intelligence  UAI’15  pages 258–267  Arlington  Virginia  United States  2015. AUAI Press.
ISBN 978-0-9966431-0-8.

[13] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical

measure. Probability Theory and Related Fields  162(3):707–738  Aug 2015.

[14] Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance.

arXiv preprint arXiv:1604.02199  2016.

[15] Rui Gao  Xi Chen  and Anton J Kleywegt. Wasserstein distributional robustness and regularization in

statistical learning. arXiv preprint arXiv:1712.06050  2017.

[16] Joel Goh and Melvyn Sim. Distributionally robust optimization and its tractable approximations. Operations

Research  58(4-part-1):902–917  2010.

[17] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial examples.

In International Conference on Learning Representations  2015.

[18] Jun-ya Gotoh  Michael Kim  and Andrew Lim. Robust Empirical Optimization is Almost the Same As

Mean-Variance Optimization. Available at SSRN 2827400  2015.

[19] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Schölkopf  and Alexander Smola. A

kernel two-sample test. Journal of Machine Learning Research  13:723–773  March 2012.

[20] Tatsunori Hashimoto  Megha Srivastava  Hongseok Namkoong  and Percy Liang. Fairness without
demographics in repeated loss minimization. In Jennifer Dy and Andreas Krause  editors  Proceedings of
the 35th International Conference on Machine Learning  volume 80 of Proceedings of Machine Learning
Research  pages 1929–1938  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.

[21] Wittawat Jitkrittum  Wenkai Xu  Zoltan Szabo  Kenji Fukumizu  and Arthur Gretton. A linear-time kernel
goodness-of-ﬁt test. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan 
and R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 262–271. Curran
Associates  Inc.  2017.

[22] Henry Lam. Robust Sensitivity Analysis for Stochastic Systems. Mathematics of Operations Research  41

(4):1248–1275  2016.

[23] Jing Lei. Convergence and concentration of empirical measures under wasserstein distance in unbounded

functional spaces. arXiv preprint arXiv:1804.10556  2018.

[24] Yujia Li  Kevin Swersky  and Rich Zemel. Generative moment matching networks. In Francis Bach and
David Blei  editors  Proceedings of the 32nd International Conference on Machine Learning  volume 37 of
Proceedings of Machine Learning Research  pages 1718–1727  Lille  France  07–09 Jul 2015. PMLR.

[25] Qiang Liu  Jason Lee  and Michael Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests. In
Maria Florina Balcan and Kilian Q. Weinberger  editors  Proceedings of The 33rd International Conference
on Machine Learning  volume 48 of Proceedings of Machine Learning Research  pages 276–284  New
York  New York  USA  20–22 Jun 2016. PMLR.

[26] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learning
Representations  2018.

[27] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample variance penalization.

In Conference on Learning Theory  2009.

[28] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
Wasserstein metric: performance guarantees and tractable reformulations. Mathematical Programming 
171(1):115–166  Sep 2018.

[29] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learning. MIT

press  2018.
[30] Krikamol Muandet  Kenji Fukumizu  Bharath Sriperumbudur  and Bernhard Schölkopf. Kernel mean
embedding of distributions: A review and beyond. Foundations and Trends R(cid:13) in Machine Learning  10
(1-2):1–141  2017.

[31] Hongseok Namkoong and John C. Duchi. Variance-based Regularization with Convex Objectives. In

Advances in Neural Information Processing Systems 30  pages 2975–2984  2017.

[32] Herbert Scarf. A min-max solution of an inventory problem. Studies in the mathematical theory of

inventory and production  1958.

[33] Soroosh Shaﬁeezadeh Abadeh  Peyman Mohajerin Mohajerin Esfahani  and Daniel Kuhn. Distributionally
robust logistic regression. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett  editors 
Advances in Neural Information Processing Systems 28  pages 1576–1584. Curran Associates  Inc.  2015.
[34] Shashank Singh and Barnabás Póczos. Minimax distribution estimation in wasserstein distance. arXiv

preprint arXiv:1802.08855  2018.

[35] Aman Sinha  Hongseok Namkoong  and John Duchi. Certifying some distributional robustness with

principled adversarial training. In International Conference on Learning Representations  2018.

[36] Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of adversarial

training. In NIPS Machine Learning and Computer Security Workshop  2017.

10

[37] Matthew Staib  Bryan Wilder  and Stefanie Jegelka. Distributionally robust submodular maximization.
In Kamalika Chaudhuri and Masashi Sugiyama  editors  Proceedings of the Twenty-Second International
Conference on Artiﬁcial Intelligence and Statistics  volume 89 of Proceedings of Machine Learning
Research  pages 506–516. PMLR  16–18 Apr 2019.

[38] Dougal J Sutherland  Hsiao-Yu Tung  Heiko Strathmann  Soumyajit De  Aaditya Ramdas  Alex Smola 
and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In
International Conference on Learning Representations  2017.

[39] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfellow 
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations  2014.

[40] Cédric Villani. Optimal Transport: Old and New (Grundlehren der mathematischen Wissenschaften).

Springer  2008. ISBN 9788793102132.

[41] Jonathan Weed  Francis Bach  et al. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical

measures in wasserstein distance. Bernoulli  25(4A):2620–2648  2019.

[42] Huan Xu  Constantine Caramanis  and Shie Mannor. Robustness and regularization of support vector

machines. Journal of Machine Learning Research  10(Jul):1485–1510  2009.

11

,Matthew Staib
Stefanie Jegelka