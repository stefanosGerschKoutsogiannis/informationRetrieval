2016,Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation,Estimators of information theoretic measures such as entropy and mutual information from samples are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with bandwidth chosen to be data independent and vanishing sub linearly in the sample size). In this paper we combine both these approaches to design new estimators of entropy and mutual information that strongly outperform all state of the art methods. Our estimator uses bandwidth choice of fixed $k$-NN distances; such a choice is both data dependent and linearly vanishing in the sample size and necessitates a bias cancellation term that  is  universal and independent of the underlying distribution. As a byproduct  we obtain a unified way of obtaining both kernel and NN estimators.  The corresponding theoretical contribution relating the geometry of NN distances to asymptotic order statistics  is of independent mathematical interest.,Breaking the Bandwidth Barrier:

Geometrical Adaptive Entropy Estimation

Weihao Gao∗  Sewoong Oh† 

and Pramod Viswanath∗

{wgao9 swoh pramodv}@illinois.edu

University of Illinois at Urbana-Champaign

Urbana  IL 61801

Abstract

Estimators of information theoretic measures such as entropy and mutual infor-
mation are a basic workhorse for many downstream applications in modern data
science. State of the art approaches have been either geometric (nearest neighbor
(NN) based) or kernel based (with a globally chosen bandwidth). In this paper  we
combine both these approaches to design new estimators of entropy and mutual
information that outperform state of the art methods. Our estimator uses local
bandwidth choices of k-NN distances with a ﬁnite k  independent of the sample
size. Such a local and data dependent choice improves performance in practice  but
the bandwidth is vanishing at a fast rate  leading to a non-vanishing bias. We show
that the asymptotic bias of the proposed estimator is universal; it is independent of
the underlying distribution. Hence  it can be precomputed and subtracted from the
estimate. As a byproduct  we obtain a uniﬁed way of obtaining both kernel and
NN estimators. The corresponding theoretical contribution relating the asymptotic
geometry of nearest neighbors to order statistics is of independent mathematical
interest.

1

Introduction

Unsupervised representation learning is one of the major themes of modern data science; a common
theme among the various approaches is to extract maximally “informative" features via information-
theoretic metrics (entropy  mutual information and their variations) – the primary reason for the
popularity of information theoretic measures is that they are invariant to one-to-one transformations
and that they obey natural axioms such as data processing. Such an approach is evident in many
applications  as varied as computational biology [11]  sociology [20] and information retrieval [17] 
with the citations representing a mere smattering of recent works. Within mainstream machine
learning  a systematic effort at unsupervised clustering and hierarchical information extraction is
conducted in recent works of [25  23]. The basic workhorse in all these methods is the computation
of mutual information (pairwise and multivariate) from i.i.d. samples. Indeed  sample-efﬁcient
estimation of mutual information emerges as the central scientiﬁc question of interest in a variety
of applications  and is also of fundamental interest to statistics  machine learning and information
theory communities.
While these estimation questions have been studied in the past three decades (and summarized in [28]) 
the renewed importance of estimating information theoretic measures in a sample-efﬁcient manner
is persuasively argued in a recent work [2]  where the authors note that existing estimators perform
poorly in several key scenarios of central interest (especially when the high dimensional random
variables are strongly related to each other). The most common estimators (featured in scientiﬁc

∗Coordinated Science Lab and Department of Electrical and Computer Engineering
†Coordinated Science Lab and Department of Industrial and Enterprise Systems Engineering

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

software packages) are nonparametric and involve k nearest neighbor (NN) distances between the
samples. The widely used estimator of mutual information is the one by Kraskov and Stögbauer and
Grassberger [10] and christened the KSG estimator (nomenclature based on the authors  cf. [2]) –
while this estimator works well in practice (and performs much better than other approaches such as
those based on kernel density estimation procedures)  it still suffers in high dimensions. The basic
issue is that the KSG estimator (and the underlying differential entropy estimator based on nearest
neighbor distances by Kozachenko and Leonenko (KL) [9]) does not take advantage of the fact that
the samples could lie in a smaller dimensional subspace (more generally  manifold) despite the high
dimensionality of the data itself. Such lower dimensional structures effectively act as boundaries 
causing the estimator to suffer from what is known as boundary biases.
Ameliorating this deﬁciency is the central theme of recent works [3  2  16]  each of which aims
to improve upon the classical KL (differential) entropy estimator of [9]. A local SVD is used
to heuristically improve the density estimate at each sample point in [2]  while a local Gaussian
density (with empirical mean and covariance weighted by NN distances) is heuristically used for
the same purpose in [16]. Both these approaches  while inspired and intuitive  come with no
theoretical guarantees (even consistency) and from a practical perspective involve delicate choice
of key hyper parameters. An effort towards a systematic study is initiated in [3] which connects the
aforementioned heuristic efforts of [2  16] to the local log-likelihood density estimation methods
[6  15] from theoretical statistics.
The local density estimation method is a strong generalization of the traditional kernel density
estimation methods  but requires a delicate normalization which necessitates the solution of certain
integral equations (cf. Equation (9) of [15]). Indeed  such an elaborate numerical effort is one of the
key impediments for the entropy estimator of [3] to be practically valuable. A second key impediment
is that theoretical guarantees (such as consistency) can only be provided when the bandwidth is chosen
globally (leading to poor sample complexity in practice) and consistency requires the bandwidth h
to be chosen such that nhd → ∞ and h → 0  where n is the sample size and d is the dimension
of the random variable of interest. More generally  it appears that a systematic application of local
log-likelihood methods to estimate functionals of the unknown density from i.i.d. samples is missing
in the theoretical statistics literature (despite local log-likelihood methods for regression and density
estimation being standard textbook fare [29  14]). We resolve each of these deﬁciencies in this paper
by undertaking a comprehensive study of estimating the (differential) entropy and mutual information
from i.i.d. samples using sample dependent bandwidth choices (typically ﬁxed k-NN distances). This
effort allows us to connect disparate threads of ideas from seemingly different arenas: NN methods 
local log-likelihood methods  asymptotic order statistics and sample-dependent heuristic  but inspired 
methods for mutual information estimation suggested in the work of [10].
Main Results: We make the following contributions.

1. Density estimation: Parameterizing the log density by a polynomial of degree p  we derive
simple closed form expressions for the local log-likelihood maximization problem for the
cases of p ≤ 2 for arbitrary dimensions  with Gaussian kernel choices. This derivation  posed
as an exercise in [14  Exercise 5.2]  signiﬁcantly improves the computational efﬁciency
upon similar endeavors in the recent efforts of [3  16  26].

2. Entropy estimation: Using resubstitution of the local density estimate  we derive a simple
closed form estimator of the entropy using a sample dependent bandwidth choice (of k-NN
distance  where k is a ﬁxed small integer independent of the sample size): this estimator
outperforms state of the art entropy estimators in a variety of settings. Since the bandwidth
is data dependent and vanishes too fast (because k is ﬁxed)  the estimator has a bias  which
we derive a closed form expression for and show that it is independent of the underlying
distribution and hence can be easily corrected: this is our main theoretical contribution  and
involves new theorems on asymptotic statistics of nearest neighbors generalizing classical
work in probability theory [19]  which might be of independent mathematical interest.

3. Generalized view: We show that seemingly very different approaches to entropy estimation
– recent works of [2  3  16] and the classical work of ﬁxed k-NN estimator of Kozachenko and
Leonenko [9] – can all be cast in the local log-likelihood framework as speciﬁc kernel and
sample dependent bandwidth choices. This allows for a uniﬁed view  which we theoretically
justify by showing that resubstitution entropy estimation for any kernel choice using ﬁxed
k-NN distances as bandwidth involves a bias term that is independent of the underlying

2

distribution (but depends on the speciﬁc choice of kernel and parametric density family).
Thus our work is a strict mathematical generalization of the classical work of [9].

4. Mutual Information estimation: The inspired work of [10] constructs a mutual information
estimator that subtly altered (in a sample dependent way) the three KL entropy estimation
terms  leading to superior empirical performance. We show that the underlying idea behind
this change can be incorporated in our framework as well  leading to a novel mutual
information estimator that combines the two ideas and outperforms state of the art estimators
in a variety of settings.

In the rest of this paper we describe these main results  the sections organized in roughly the same
order as the enumerated list.

2 Local likelihood density estimation (LLDE)
Given n i.i.d. samples X1  . . .   Xn  estimating the unknown density fX (·) in Rd is a very basic
statistical task. Local likelihood density estimators [15  6] constitute state of the art and are speciﬁed
by a weight function K : Rd → R (also called a kernel)  a degree p ∈ Z+ of the polynomial
approximation  and the bandwidth h ∈ R  and maximizes the local log-likelihood:

n(cid:88)

j=1

K

(cid:18) Xj − x

(cid:19)

h

(cid:90)

(cid:18) u − x

(cid:19)

h

Lx(f ) =

log f (Xj) − n

K

f (u) du  

(1)

where maximization is over an exponential polynomial family  locally approximating f (u) near x:
loge fa x(u) = a0 + (cid:104)a1  u − x(cid:105) + (cid:104)u − x  a2(u − x)(cid:105) + ··· + ap[u − x  u − x  . . .   u − x]  
(2)
parameterized by a = (a0  . . .   ap) ∈ R1×d×d2×···×dp  where (cid:104)· ·(cid:105) denotes the inner-product and
ap[u  . . .   u] the p-th order tensor projection. The local likelihood density estimate (LLDE) is deﬁned

as (cid:98)fn(x) = f(cid:98)a(x) x(x) = e(cid:98)a0(x)  where(cid:98)a(x) ∈ arg maxa Lx(fa x). The maximizer is represented

by a series of nonlinear equations  and does not have a closed form in general. We present below a
few choices of the degrees and the weight functions that admit closed form solutions. Concretely  for
p = 0  it is known that LDDE reduces to the standard Kernel Density Estimator (KDE) [15]:

(cid:18) x − Xi

(cid:19)(cid:46)(cid:90)

h

(cid:18) u − x

(cid:19)

K

h

du .

(3)

(cid:98)fn(x) =

(cid:98)fn(x) =

n(cid:88)

i=1

1
n

K

(cid:80)n

If we choose the step function K(u) = I((cid:107)u(cid:107) ≤ 1) with a local and data-dependent choice of the
bandwidth h = ρk x where ρk x is the k-NN distance from x  then the above estimator recovers the
popular k-NN density estimate as a special case  namely  for Cd = πd/2/Γ(d/2 + 1) 

1
n

I((cid:107)Xi − x(cid:107) ≤ ρk x)

i=1

Vol{u ∈ Rd : (cid:107)u − x(cid:107) ≤ ρk x} =

k

n Cd ρd

k x

.

(4)

For higher degree local likelihood  we provide simple closed form solutions and provide a proof
in Section D. Somewhat surprisingly  this result has eluded prior works [16  26] and [3] which
speciﬁcally attempted the evaluation for p = 2. Part of the subtlety in the result is to critically
use the fact that the parametric family (eg.  the polynomial family in (2)) need not be normalized
themselves; the local log-likelihood maximization ensures that the resulting density estimate is
correctly normalized so that it integrates to 1.
Proposition 2.1. [14  Exercise 5.2] For a degree p ∈ {1  2}  the maximizer of local likelihood (1)
admits a closed form solution  when using the Gaussian kernel K(u) = e− (cid:107)u(cid:107)2
. In case of p = 1 

2

(cid:98)fn(x) =

(cid:107)S1(cid:107)2
where S0 ∈ R and S1 ∈ Rd are deﬁned for given x ∈ Rd and h ∈ R as

n(2π)d/2hd

− 1
2

1
S2
0

exp

S0

S0 ≡ n(cid:88)

− (cid:107)Xj−x(cid:107)2

2h2

e

 

S1 ≡ n(cid:88)

(Xj − x) e

− (cid:107)Xj−x(cid:107)2

2h2

.

1
h

(cid:26)

(cid:27)

 

(5)

(6)

j=1

j=1

3

In case of p = 2  for S0 and S1 deﬁned as above 

(cid:98)fn(x) =

(cid:110) − 1

2

(cid:111)

 

1 Σ−1S1
ST
where |Σ| is the determinant and S2 ∈ Rd×d and Σ ∈ Rd×d are deﬁned as

n(2π)d/2hd|Σ|1/2

1
S2
0

exp

S0

1

h2 (Xj − x)(Xj − x)T e

− (cid:107)Xj−x(cid:107)2

2h2

 

Σ ≡ S0S2 − S1ST

1

S2
0

S2 ≡ n(cid:88)

j=1

(7)

(8)

 

where it follows from Cauchy-Schwarz that Σ is positive semideﬁnite.

One of the major drawbacks of the KDE and k-NN methods is the increased bias near the boundaries.
LLDE provides a principled approach to automatically correct for the boundary bias  which takes
effect only for p ≥ 2 [6  21]. This explains the performance improvement for p = 2 in the ﬁgure
below (left panel)  and the gap increases with the correlation as boundary effect becomes more
prominent. We use the proposed estimators with p ∈ {0  1  2} to estimate the mutual information
between two jointly Gaussian random variables with correlation r  from n = 500 samples  using
resubstitution methods explained in the next sections. Each point is averaged over 100 instances.
In the right panel  we generate i.i.d. samples from a 2-dimensional Gaussian with correlation 0.9  and

found local approximation (cid:98)f (u − x∗) around x∗ denoted by the blue ∗ in the center. Standard k-NN

approach ﬁts a uniform distribution over a circle enclosing k = 20 nearest neighbors (red circle). The
green lines are the contours of the degree-2 polynomial approximation with bandwidth h = ρ20 x.
The ﬁgure illustrates that k-NN method suffers from boundary effect  where it underestimates the
probability by over estimating the volume in (4). However  degree-2 LDDE is able to correctly
capture the local structure of the pdf  correcting for boundary biases.
Despite the advantages of the LLDE  it requires the bandwidth to be data independent and vanishingly
small (sublinearly in sample size) for consistency almost everywhere – both of these are impediments
to practical use since there is no obvious systematic way of choosing these hyperparameters. On the
other hand  if we restrict our focus to functionals of the density  then both these issues are resolved:
this is the focus of the next section where we show that the bandwidth can be chosen to be based on
ﬁxed k-NN distances and the resulting universal bias easily corrected.

]
2
)
(cid:98)I
−
I
(
[
E

X1

(1 − r) where r is correlation

X2

Figure 1: The boundary bias becomes less signiﬁcant and the gap closes as correlation decreases for
estimating the mutual information (left). Local approximation around the blue ∗ in the center. The
degree-2 local likelihood approximation (contours in green) automatically captures the local structure
whereas the standard k-NN approach (uniform distribution in red circle) fails (left).

3 k-LNN Entropy Estimator

We consider resubstitution entropy estimators of the form (cid:98)H(x) = −(1/n)(cid:80)n

i=1 log(cid:98)fn(Xi) and

propose to use the local likelihood density estimator in (7) and a choice of bandwidth that is local

4

 0.001 0.01 0.1 1 100.0000010.00010.0011p=0p=1p=2(cid:41)

(cid:98)H (n)
kLNN(X) = − 1
n

(cid:40)

n(cid:88)

i=1

(varying for each point x) and adaptive (based on the data). Concretely  we choose  for each sample
point Xi  the bandwidth hXi to be the the distance to its k-th nearest neighbor ρk i. Precisely  we
propose the following k-Local Nearest Neighbor (k-LNN) entropy estimator of degree-2:

log

S0 i
k i|Σi|1/2
n(2π)d/2ρd

− 1
2

1
S2
0 i

1 iΣ−1
ST

i S1 i

− Bk d  

(9)

where subtracting Bk d deﬁned in Theorem 1 removes the asymptotic bias  and k ∈ Z+ is the only
hyper parameter determining the bandwidth. In practice k is a small integer ﬁxed to be in the range
4 ∼ 8. We only use the (cid:100)log n(cid:101) nearest subset of samples Ti = {j ∈ [n] : j (cid:54)= i and (cid:107)Xi − Xj(cid:107) ≤
ρ(cid:100)log n(cid:101) i} in computing the quantities below:

S0 i ≡ (cid:88)

− (cid:107)Xj−Xi(cid:107)2

2ρ2

k i

e

 

S1 i ≡ (cid:88)

j∈Ti m
(Xj − Xi)(Xj − Xi)T e

j∈Ti m
− (cid:107)Xj−Xi(cid:107)2

2ρ2

k i

S2 i ≡ (cid:88)

j∈Ti m

1
ρ2
k i

(Xj − Xi)e

1
ρk i

− (cid:107)Xj−Xi(cid:107)2

2ρ2

k i

  Σi ≡ S0 iS2 i − S1 iST

1 i

S2
0 i

 

.

(10)

E[(cid:98)H (n)

The truncation is important for computational efﬁciency  but the analysis works as long as m =
O(n1/(2d)−ε) for any positive ε that can be arbitrarily small. For a larger m  for example of Ω(n) 
those neighbors that are further away have a different asymptotic behavior. We show in Theorem 1
that the asymptotic bias is independent of the underlying distribution and hence can be precomputed
and removed  under mild conditions on a twice continuously differentiable pdf f (x) (cf. Lemma 3.1
below).
Theorem 1. For k ≥ 3 and X1  X2  . . .   Xn ∈ Rd are i.i.d. samples from a twice continuously
differentiable pdf f (x)  then

(11)
where Bk d in (9) is a constant that only depends on k and d. Further  if E[(log f (X))2] < ∞ then

the variance of the proposed estimator is bounded by Var[(cid:98)H (n)

kLNN(X)] = O((log n)2/n).

kLNN(X)] = H(X)  

lim
n→∞

This proves the L1 and L2 consistency of the k-LNN estimator; we relegate the proof to Section F
for ease of reading the main part of the paper. The proof assumes Ansatz 1 (also stated in Section F) 
which states that a certain exchange of limit holds. As noted in [18]  such an assumption is common
in the literature on consistency of k-NN estimators  where it has been implicitly assumed in existing
analyses of entropy estimators including [9  5  12  27]  without explicitly stating that such assumptions
are being made. Our choice of a local adaptive bandwidth hXi = ρk i is crucial in ensuring that
the asymptotic bias Bk d does not depend on the underlying distribution f (x). This relies on a
fundamental connection to the theory of asymptotic order statistics made precise in Lemma 3.1 
which also gives the explicit formula for the bias below.
The main idea is that the empirical quantities used in the estimate (10) converge in large n limit to
similar quantities deﬁned over order statistics. We make this intuition precise in the next section.
We deﬁne order statistics over i.i.d. standard exponential random variables E1  E2  . . .   Em and i.i.d.
random variables ξ1  ξ2  . . .   ξm drawn uniformly (the Haar measure) over the unit sphere in Rd  for
a variable m ∈ Z+. We deﬁne for α ∈ {0  1  2} 

((cid:80)j
((cid:80)k

ξ(α)
j

(cid:96)=1 E(cid:96))α
(cid:96)=1 E(cid:96) )α

exp

(cid:40)
− ((cid:80)j
2((cid:80)k

(cid:96)=1 E(cid:96) )2
(cid:96)=1 E(cid:96) )2

(cid:41)

 

(12)

α ≡ m(cid:88)
(cid:101)Σ = (1/ ˜S0)2( ˜S0 ˜S2 − ˜S1 ˜ST

j = 1  ξ(1)

where ξ(0)

˜S(m)

j=1

Theorem 1) and are directly related to the bias terms in the resubstitution estimator of entropy:

j = ξj ∈ Rd  and ξ(2)
k(cid:88)

and
1 ). We show that the limiting ˜Sα’s are well-deﬁned (in the proof of

j = ξjξT

α

j ∈ Rd×d  and let ˜Sα = limm→∞ ˜S(m)
1(cid:101)Σ−1 ˜S1) ] .

log(cid:12)(cid:12)(cid:101)Σ(cid:12)(cid:12) + (

˜ST

(13)

E(cid:96)) +

log 2π − log Cd − log ˜S0 +

Bk d = E[ log(

1
2

1
2 ˜S2
0

d
2

(cid:96)=1

5

In practice  we propose using a ﬁxed small k such as ﬁve. For k ≤ 3 the estimator has a very large
variance  and numerical evaluation of the corresponding bias also converges slowly. For some typical
choices of k  we provide approximate evaluations below  where 0.0183(±6) indicates empirical mean
µ = 183 × 10−4 with conﬁdence interval 6 × 10−4. In these numerical evaluations  we truncated
the summation at m = 50  000. Although we prove that Bk d converges in m  in practice  one can
choose m based on the number of samples and Bk d can be evaluated for that m.
Theoretical contribution: Our key technical innovation is a fundamental connection between nearest
neighbor statistics and asymptotic order statistics  stated below as Lemma 3.1: we show that the
(normalized) distances ρ(cid:96) i’s jointly converge to the standardized uniform order statistics and the
directions (Xj(cid:96) − Xi)/(cid:107)Xj(cid:96) − Xi(cid:107)’s converge to independent uniform distribution (Haar measure)
over the unit sphere.

k

4

5

6

d

1
2

-0.0183(±6)
-0.1023(±5)

-0.0171(±3)
-0.0401(±3)
Table 1: Numerical evaluation of Bk d  via sampling 1  000  000 instances for each pair (k  d).

-0.0200(±4)
-0.0528(±3)

-0.0181(±4)
-0.0448(±3)

-0.0220(±4)
-0.0628(±4)

-0.0233(±6)
-0.0765(±4)

7

8

9

Conditioned on Xi = x  the proposed estimator uses nearest neighbor statistics on Z(cid:96) i ≡ Xj(cid:96) − x
where Xj(cid:96) is the (cid:96)-th nearest neighbor from x such that Z(cid:96) i = ((Xj(cid:96) − Xi)/(cid:107)Xj(cid:96) − Xi(cid:107))ρ(cid:96) i.
Naturally  all the techniques we develop in this paper generalize to any estimators that depend on the
nearest neighbor statistics {Z(cid:96) i}i (cid:96)∈[n] – and the value of such a general result is demonstrated later
(in Section 4) when we evaluate the bias in similarly inspired entropy estimators [2  3  16  9].
Lemma 3.1. Let E1  E2  . . .   Em be i.i.d. standard exponential random variables and ξ1  ξ2  . . .   ξm
be i.i.d. random variables drawn uniformly over the unit (d − 1)-dimensional sphere in d dimensions 
independent of the Ei’s. Suppose f is twice continuously differentiable and x ∈ Rd satisﬁes that
there exists ε > 0 such that f (a) > 0  (cid:107)∇f (a)(cid:107) = O(1) and (cid:107)Hf (a)(cid:107) = O(1) for any (cid:107)a − x(cid:107) < ε.
Then for any m = O(log n)  we have the following convergence conditioned on Xi = x:

1

(cid:96)=1

(14)

  . . .   ξm(

n→∞ dTV((cdnf (x))1/d( Z1 i  . . .   Zm i )   ( ξ1E1/d
lim

E(cid:96))1/d )) = 0 .
where dTV(· ·) is the total variation and cd is the volume of unit Euclidean ball in Rd.
Empirical contribution: Numerical experiments suggest that the proposed estimator outperforms
state-of-the-art entropy estimators  and the gap increases with correlation. The idea of using k-NN
distance as bandwidth for entropy estimation was originally proposed by Kozachenko and Leonenko
in [9]  and is a special case of the k-LNN method we propose with degree 0 and a step kernel. We
refer to Section 4 for a formal comparison. Another popular resubstitution entropy estimator is to
use KDE in (3) [7]  which is a special case of the k-LNN method with degree 0  and the Gaussian
kernel is used in simulations. As comparison  we also study a new estimator [8] based on von Mises
expansion (as opposed to simple re-substitution) which has an improved convergence rate in the large
sample regime. We relegate simulation results to Section. B in the appendix.

m(cid:88)

4 Universality of the k-LNN approach

In this section  we show that Theorem 1 holds universally for a general family of entropy estimators 
speciﬁed by the choice of k ∈ Z+  degree p ∈ Z+  and a kernel K : Rd → R  thus allowing a uniﬁed
view of several seemingly disparate entropy estimators [9  2  3  16]. The template of the entropy
estimator is the following: given n i.i.d. samples  we ﬁrst compute the local density estimate by
maximizing the local likelihood (1) with bandwidth ρk i  and then resubstitute it to estimate entropy:

k p K(X) = −(1/n)(cid:80)n
(cid:98)H (n)
the solution to the maximization(cid:98)a(x) = arg maxa Lx(fa x) exists for all x ∈ {X1  . . .   Xn}  then

Theorem 2. For the family of estimators described above  under the hypotheses of Theorem 1  if
for any choice of k ≥ p + 1  p ∈ Z+  and K : Rd → R  the asymptotic bias is independent of the
underlying distribution:

i=1 log(cid:98)fn(Xi).

E[(cid:98)H (n)
k p K(X)] = H(X) + (cid:101)Bk p K d  

(15)

lim
n→∞

6

We provide a proof in Section G. Although in general there is no simple analytical characterization of

for some constant (cid:101)Bk d p K that only depends on k  p  K and d.
the asymptotic bias (cid:101)Bk p K d it can be readily numerically computed: since (cid:101)Bk p K d is independent
(cid:98)a(x) = arg maxa Lx(fa x) admits a closed form solution  as is the case with proposed k-LNN  then
(cid:101)Bk p K d can be characterized explicitly in terms of uniform order statistics.

of the underlying distribution  one can run the estimator over i.i.d. samples from any distribution and
numerically approximate the bias for any choice of the parameters. However  when the maximization

This family of estimators is general: for instance  the popular KL estimator is a special case with
p = 0 and a step kernel K(u) = I((cid:107)u(cid:107) ≤ 1). [9] showed (in a remarkable result at the time)
that the asymptotic bias is independent of the dimension d and can be computed exactly to be
log n− ψ(n) + ψ(k)− log k and ψ(k) is the digamma function deﬁned as ψ(x) = Γ−1(x)dΓ(x)/dx.
The dimension independent nature of this asymptotic bias term (of O(n−1/2) for d = 1 in [24 
Theorem 1] and O(n−1/d) for general d in [4]) is special to the choice of p = 0 and the step kernel;
we explain this in detail in Section G  later in the paper. Analogously  the estimator in [2] can be
viewed as a special case with p = 0 and an ellipsoidal step kernel.

5 k-LNN Mutual information estimator

Given an entropy estimator (cid:98)HKL  mutual information can be estimated: (cid:98)I3KL = (cid:98)HKL(X) +
(cid:98)HKL(Y ) − (cid:98)HKL(X  Y ). In [10]  Kraskov and Stögbauer and Grassberger introduced(cid:98)IKSG(X; Y )
by coupling the choices of the bandwidths. The joint entropy is estimated in the usual way  but for the
marginal entropy  instead of using kNN distances from {Xj}  the bandwidth hXi = ρk i(X  Y ) is
(cid:98)I3LNN(X; Y ) = (cid:98)HkLNN(X) + (cid:98)HkLNN(Y ) − (cid:98)HkLNN(X  Y ). Inspired by [10]  we introduce the
chosen  which is the k nearest neighbor distance from (Xi  Yi) for the joint data {(Xj  Yj)}. Consider
following novel mutual information estimator we denote by(cid:98)ILNN−KSG(X; Y ). where for the joint
use the bandwidth hXi = ρk i(X  Y ) coupled to the joint estimator. Empirically  we observe(cid:98)IKSG
outperforms(cid:98)I3KL everywhere  validating the use of correlated bandwidths. However  the performance
of(cid:98)ILNN−KSG is similar to(cid:98)I3LNN–sometimes better and sometimes worse.

(X  Y ) we use the LNN entropy estimator we proposed in (9)  and for the marginal entropy we

Empirical Contribution: Numerical experiments show that for most regimes of correlation  both
3LNN and LNN-KSG outperforms other state-of-the-art estimators  and the gap increases with
correlation r. In the large sample limit  all estimators ﬁnd the correct mutual information  but both
LNN and LNN-KSG are signiﬁcantly more robust compared to other approaches. Mutual information
estimators have been recently proposed in [2  3  16] based on local likelihood maximization. However 
they involve heuristic choices of hyper-parameters or solving elaborate optimization and numerical
integrations  which are far from being easy to implement. Simulation results can be found in
Section. C in the appendix.

6 Breaking the bandwidth barrier

While k-NN distance based bandwidth are routine in practical usage [21]  the main ﬁnding of this work
is that they also turn out to be the “correct" mathematical choice for the purpose of asymptotically

unbiased estimation of an integral functional such as the entropy: −(cid:82) f (x) log f (x); we brieﬂy
thumb  h = 1.06(cid:98)σn−1/5 is suggested when d = 1 where(cid:98)σ is the sample standard deviation [29 
known that resubstitution estimators of the form −(1/n)(cid:80)n
i=1 log(cid:98)f (Xi) achieve variances scaling

discuss the ramiﬁcations below. Traditionally  when the goal is to estimate f (x)  it is well known
that the bandwidth should satisfy h → 0 and nhd → ∞  for KDEs to be consistent. As a rule of

Chapter 6.3]. On the other hand  when estimating entropy  as well as other integral functionals  it is

as O(1/n) independent of the bandwidth [13]. This allows for a bandwidth as small as O(n−1/d).
The bottleneck in choosing such a small bandwidth is the bias  scaling as O(h2 +(nhd)−1 +En) [13] 
where the lower order dependence on n  dubbed En  is generally not known. The barrier in choosing a
global bandwidth of h = O(n−1/d) is the strictly positive bias whose value depends on the unknown
distribution and cannot be subtracted off. However  perhaps surprisingly  the proposed local and

7

adaptive choice of the k-NN distance admits an asymptotic bias that is independent of the unknown
underlying distribution. Manually subtracting off the non-vanishing bias gives an asymptotically
unbiased estimator  with a potentially faster convergence as numerically compared below. Figure 2
illustrates how k-NN based bandwidth signiﬁcantly improves upon  say a rule-of-thumb choice of
O(n−1/(d+4)) explained above and another choice of O(n−1/(d+2)). In the left ﬁgure  we use the
setting from Figure 3 (right) but with correlation r = 0.999. On the right  we generate X ∼ N (0  1)
and U from uniform [0  0.01] and let Y = X + U and estimate I(X; Y ). Following recent advances
in [12  22]  the proposed local estimator has a potential to be extended to  for example  Renyi entropy 
but with a multiplicative bias as opposed to additive.

]
2
)
I
−
(cid:98)I
(
[
E

]
2
)
I
−
(cid:98)I
(
[
E

number of samples n

number of samples n

Figure 2: Local and adaptive bandwidth signiﬁcantly improves over rule-of-thumb ﬁxed bandwidth.

Acknowledgement

This work is supported by NSF SaTC award CNS-1527754  NSF CISE award CCF-1553452  NSF
CISE award CCF-1617745. We thank the anonymous reviewers for their constructive feedback.

References
[1] G. Biau and L. Devroye. Lectures on the Nearest Neighbor Method. Springer  2016.

[2] S. Gao  G. Ver Steeg  and A. Galstyan. Efﬁcient estimation of mutual information for strongly dependent

variables. International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2015.

[3] S. Gao  G. Ver Steeg  and A. Galstyan. Estimating mutual information by local gaussian approximation.

31st Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2015.

[4] W. Gao  S. Oh  and P. Viswanath. Demystifying ﬁxed k-nearest neighbor information estimators. arXiv

preprint arXiv:1604.03006  2016.

[5] M. N. Goria  N. N. Leonenko  V. V. Mergel  and P. L. Novi Inverardi. A new class of random vector entropy
estimators and its applications in testing statistical hypotheses. Nonparametric Statistics  17(3):277–297 
2005.

[6] N. Hjort and M. Jones. Locally parametric nonparametric density estimation. The Annals of Statistics 

pages 1619–1647  1996.

[7] H. Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the Institute of

Statistical Mathematics  41(4):683–697  1989.

[8] K. Kandasamy  A. Krishnamurthy  B. Poczos  and L. Wasserman. Nonparametric von mises estimators for

entropies  divergences and mutual informations. In NIPS  pages 397–405  2015.

[9] L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problemy

Peredachi Informatsii  23(2):9–16  1987.

[10] A. Kraskov  H. Stögbauer  and P. Grassberger. Estimating mutual information. Physical review E 

69(6):066138  2004.

8

 0.0001 0.01 1 100 10000 1x106 1x108 1x1010 1x1012100200400800kNN bandwidthFixed bandwidth N-1/(d+4)Fixed bandwidth N-1/(d+2) 0.0001 0.01 1 100 10000 1x106 1x108 1x1010 1x1012100200400800kNN bandwidthFixed bandwidth N-1/(d+4)Fixed bandwidth N-1/(d+2)[11] S. Krishnaswamy  M. Spitzer  M. Mingueneau  S. Bendall  O. Litvin  E. Stone  D. Peer  and G. Nolan.

Conditional density-based analysis of t cell signaling in single-cell data. Science  346:1250689  2014.

[12] N. Leonenko  L. Pronzato  and V. Savani. A class of rényi information estimators for multidimensional

densities. The Annals of Statistics  36(5):2153–2182  2008.

[13] H. Liu  L. Wasserman  and J. D. Lafferty. Exponential concentration for mutual information estimation

with application to forests. In NIPS  pages 2537–2545  2012.

[14] C. Loader. Local regression and likelihood. Springer Science & Business Media  2006.

[15] C. R. Loader. Local likelihood density estimation. The Annals of Statistics  24(4):1602–1618  1996.

[16] D. Lombardi and S. Pant. Nonparametric k-nearest-neighbor entropy estimator. Physical Review E 

93(1):013310  2016.

[17] C. D. Manning  P. Raghavan  and H. Schütze. Introduction to information retrieval  volume 1. Cambridge

university press Cambridge  2008.

[18] D. Pál  B. Póczos  and C. Szepesvári. Estimation of rényi entropy and mutual information based on
In Advances in Neural Information Processing Systems  pages

generalized nearest-neighbor graphs.
1849–1857  2010.

[19] R-D Reiss. Approximate distributions of order statistics: with applications to nonparametric statistics.

Springer Science & Business Media  2012.

[20] D. Reshef  Y. Reshef  H. Finucane  S. Grossman  G. McVean  P. Turnbaugh  E. Lander  M. Mitzenmacher 

and P. Sabeti. Detecting novel associations in large data sets. science  334(6062):1518–1524  2011.

[21] S. J. Sheather. Density estimation. Statistical Science  19(4):588–597  2004.

[22] S. Singh and B. Póczos. Analysis of k-nearest neighbor distances with application to entropy estimation.

arXiv preprint arXiv:1603.08578  2016.

[23] G. Ver Steeg and A. Galstyan. The information sieve. to appear in ICML  arXiv:1507.02284  2016.

[24] A. B. Tsybakov and E. C. Van der Meulen. Root-n consistent estimators of entropy for densities with

unbounded support. Scandinavian Journal of Statistics  pages 75–83  1996.

[25] G. Ver Steeg and A. Galstyan. Discovering structure in high-dimensional data through correlation

explanation. In Advances in Neural Information Processing Systems  pages 577–585  2014.

[26] P. Vincent and Y. Bengio. Locally weighted full covariance gaussian density estimation. Technical report 

Technical report 1240  2003.

[27] Q. Wang  S. R. Kulkarni  and S. Verdú. Divergence estimation for multidimensional densities via-nearest-

neighbor distances. Information Theory  IEEE Transactions on  55(5):2392–2405  2009.

[28] Q. Wang  S. R. Kulkarni  and S. Verdú. Universal estimation of information measures for analog sources.

Foundations and Trends in Communications and Information Theory  5(3):265–353  2009.

[29] L. Wasserman. All of nonparametric statistics. Springer Science & Business Media  2006.

9

,Ian En-Hsu Yen
Ting-Wei Lin
Shou-De Lin
Pradeep Ravikumar
Inderjit Dhillon
Maria Lomeli
Stefano Favaro
Yee Whye Teh
Weihao Gao
Sewoong Oh
Pramod Viswanath