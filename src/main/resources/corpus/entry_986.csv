2017,Cold-Start Reinforcement Learning with Softmax Policy Gradient,Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures  namely warm-start training and sample variance reduction. In this paper  we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.,Cold-Start Reinforcement Learning with

Softmax Policy Gradient

Nan Ding
Google Inc.

Venice  CA 90291

dingnan@google.com

Radu Soricut
Google Inc.

Venice  CA 90291

rsoricut@google.com

Abstract

Policy-gradient approaches to reinforcement learning have two common and un-
desirable overhead procedures  namely warm-start training and sample variance
reduction. In this paper  we describe a reinforcement learning method based on a
softmax value function that requires neither of these procedures. Our method com-
bines the advantages of policy-gradient methods with the efﬁciency and simplicity
of maximum-likelihood approaches. We apply this new cold-start reinforcement
learning method in training sequence generation models for structured output
prediction problems. Empirical evidence validates this method on automatic sum-
marization and image captioning tasks.

1

Introduction

Reinforcement learning is the study of optimal sequential decision-making in an environment [16]. Its
recent developments underpin a large variety of applications related to robotics [11  5] and games [20].
Policy search in reinforcement learning refers to the search for optimal parameters for a given policy
parameterization [5]. Policy search based on policy-gradient [26  21] has been recently applied to
structured output prediction for sequence generations. These methods alleviate two common problems
that approaches based on training with the Maximum-likelihood Estimation (MLE) objective exhibit 
namely the exposure-bias problem [24  19] and the wrong-objective problem [19  15] (more on this
in Section 2). As a result of addressing these problems  policy-gradient methods achieve improved
performance compared to MLE training in various tasks  including machine translation [19  7]  text
summarization [19]  and image captioning [19  15].
Policy-gradient methods for sequence generation work as follows: ﬁrst the model proposes a sequence 
and the ground-truth target is used to compute a reward for the proposed sequence with respect to
the reward of choice (using metrics known to correlate well with human-rated correctness  such
as ROUGE [13] for summarization  BLEU [18] for machine translation  CIDEr [23] or SPICE [1]
for image captioning  etc.). The reward is used as a weight for the log-likelihood of the proposed
sequence  and learning is done by optimizing the weighted average of the log-likelihood of the
proposed sequences. The policy-gradient approach works around the difﬁculty of differentiating the
reward function (the majority of which are non-differentiable) by using it as a weight. However  since
sequences proposed by the model are also used as the target of the model  they are very noisy and
their initial quality is extremely poor. The difﬁculty of aligning the model output distribution with
the reward distribution over the large search space of possible sequences makes training slow and
inefﬁcient∗. As a result  overhead procedures such as warm-start training with the MLE objective
and sophisticated methods for sample variance reduction are required to train with policy gradient.
∗Search space size is O(V T )  where V is the number of word types in the vocabulary (typically between 104

and 106) and T is the the sequence length (typically between 10 and 50)  hence between 1040 and 10300.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

The fundamental reason for the inefﬁciency of policy-gradient–based reinforcement learning is the
large discrepancy between the model-output distribution and the reward distribution  especially in
the early stages of training. If  instead of generating the target based solely on the model-output
distribution  we generate it based on a proposal distribution that incorporates both the model-output
distribution and the reward distribution  learning would be efﬁcient  and neither warm-start training
nor sample variance reduction would be needed. The outstanding problem is ﬁnding a value function
that induces such a proposal distribution.
In this paper  we describe precisely such a value function  which in turn gives us a Softmax Policy
Gradient (SPG) method. The softmax terminology comes from the equation that deﬁnes this value
function  see Section 3. The gradient of the softmax value function is equal to the average of the
gradient of the log-likelihood of the targets whose proposal distribution combines both model output
distribution and reward distribution. Although this distribution is infeasible to sample exactly  we
show that one can draw samples approximately  based on an efﬁcient forward-pass sampling scheme.
To balance the importance between the model output distribution and the reward distribution  we use
a bang-bang [8] mixture model to combine the two distributions. Such a scheme removes the need of
ﬁne-tuning the weights across different datasets and throughout the learning epochs. In addition to
using a main metric as the task reward (ROUGE  CIDEr  etc.)  we show that one can also incorporate
additional  task-speciﬁc metrics to enforce various properties on the output sequences (Section 4).
We numerically evaluate our method on two sequence generation benchmarks  a headline-generation
task and an image-caption–generation task (Section 5). In both cases  the SPG method signiﬁcantly
improves the accuracy  compared to maximum-likelihood and other competing methods. Finally  it is
worth noting that although the training and inference of the SPG method in the paper is mainly based
on sequence learning  the idea can be extended to other reinforcement learning applications.

2 Limitations of Existing Sequence Learning Regimes

One of the standard approaches to sequence-learning training is Maximum-likelihood Estimation

(MLE). Given a set of inputs X =(cid:8)xi(cid:9) and target sequences Y =(cid:8)yi(cid:9)  the MLE loss function is:
(cid:88)
(cid:9) denote the input and the target sequence of the i-th example 
Here xi and yi = (cid:8)yi

M LE(θ) = − log pθ(yi|xi).

M LE(θ)  where Li
Li

LM LE(θ) =

i
1  . . .   yi
T

(1)

predictions. At training-time  log pθ(yi|xi) = (cid:80)

respectively. For instance  in the image captioning task  xi is the image of the i-th example  and yi is
the groundtruth caption of the i-th example.
Although widely used in many different applications  MLE estimation for sequence learning suffers
from the exposure-bias problem [24  19]. Exposure-bias refers to training procedures that produce
brittle models that have only been exposed to their training data distribution but not to their own
1...t−1)  i.e. the loss of the t-th
word is conditional on the true previous-target tokens yi
1...t−1 are unavailable
1...t−1|xi) yields a signiﬁcant
during inference  replacing them with tokens zi
discrepancy between how the model is used at training time versus inference time. The exposure-bias
problem has recently received attention in neural-network settings with the “data as demonstrator” [24]
and “scheduled sampling” [3] approaches. Although improving model performance in practice  such
proposals have been shown to be statistically inconsistent [10]  and still need to perform MLE-based
warm-start training.
A more general approach to MLE is the Reward Augmented Maximum Likelihood (RAML)
method [17]. RAML makes the correct observation that  under MLE  all alternative outputs are
equally penalized through normalization  regardless of their relationship to the ground-truth target.
Instead  RAML corrects for this shortcoming using an objective of the form:

1...t−1 generated by pθ(zi

1...t−1. However  since yi

t|xi  yi

t log pθ(yi

rR(zi|yi) log pθ(zi|xi).

(2)

RAM L(θ) = −(cid:88)

Li

(cid:80)
where rR(zi|yi) = exp(R(zi|yi)/τ )
zi exp(R(zi|yi)/τ ). This formulation uses R(zi|yi) to denote the value of a
similarity metric R between zi and yi (the reward)  with yi = argmaxzi R(zi|yi); τ is a temperature
hyper-parameter to control the peakiness of this reward distribution. Since the sum over all zi for

zi

2

the reward distribution rR(zi|yi) in Eq. (2) is infeasible to compute  a standard approach is to
draw J samples zij from the reward distribution  and approximate the expectation by Monte Carlo
integration:

RAM L(θ) (cid:39) − 1
Li
J

log pθ(zij|xi).

(3)

J(cid:88)

j=1

P G(θ) = −V i
Li

P G(θ) = −(cid:88)

∂
∂θ

Although a clear improvement over Eq. (1)  the sampling for zij in Eq. (3) is solely based on
rR(zi|yi) and completely ignores the model probability. At the same time  this technique does not
address the exposure bias problem at all.
A different approach  based on reinforcement learning methods  achieves sequence learning following
a policy-gradient method [21]. Its appeal is that it not only solves the exposure-bias problem  but also
directly alleviates the wrong-objective problem [19  15] of MLE approaches. Wrong-objective refers
to the critique that MLE-trained models tend to have suboptimal performance because such models
are trained on a convenient objective (i.e.  maximum likelihood) rather than a desirable objective
(e.g.  a metric known to correlate well with human-rated correctness). The policy-gradient method
uses a value function VP G  which is equivalent to a loss LP G deﬁned as:

P G(θ)  V i

P G(θ) = Epθ(zi|xi)[R(zi|yi)].

(4)

The gradient for Eq. (4) is:

J(cid:88)

Li

(5)
Similar to (3)  one can draw J samples zij from pθ(zi|xi) to approximate the expectation by Monte-
Carlo integration:

zi

pθ(zi|xi)R(zi|yi)

log pθ(zi|xi).

∂
∂θ

∂
∂θ

∂
∂θ

P G(θ) (cid:39) − 1
Li
J

R(zij|yi)

log pθ(zij|xi).

j=1

(6)
However  the large discrepancy between the model prediction distribution pθ(zi|xi) and the reward
R(zi|yi)’s values  which is especially acute during the early training stages  makes the Monte-Carlo
integration extremely inefﬁcient. As a result  this method also requires a warm-start phase in which
the model distribution achieves some local maximum with respect to a reward-metric–free objective
(e.g.  MLE)  followed by a model reﬁnement phase in which reward-metric–based PG updates are
used to reﬁne the model [19  7  15]. Although this combination achieves better results in practice
compared to pure likelihood-based approaches  it is unsatisfactory from a theoretical and modeling
perspective  as well as inefﬁcient from a speed-to-convergence perspective. Both these issues are
addressed by the value function we describe next.

3 Softmax Policy Gradient (SPG) Method
In order to smoothly incorporate both the model distribution pθ(zi|xi) and the reward metric R(zi|yi) 
we replace the value function from Eq. 4 with a Softmax value function for Policy Gradient (SPG) 
VSP G  equivalent to a loss LSP G deﬁned as:

SP G(θ) = log(cid:0)Epθ(zi|xi)[exp(R(zi|yi))](cid:1) .

Softmaxzi(·) = log(cid:80)

(7)
Because the value function for example i is equal to Softmaxzi(log pθ(zi|xi) + R(zi|yi))  where
zi exp(·)  we call it the softmax value function. Note that the softmax value
function from Eq. (7) is the dual of the entropy-regularized policy search (REPS) objective [5  16]
L(q) = Eq[R] + KL(q|pθ). However  our learning and sampling procedures are signiﬁcantly
different from REPS  as shown in what follows.
The gradient for Eq. (7) is:

SP G(θ) = −V i
Li

SP G(θ)  V i

SP G(θ) = −
Li

∂
∂θ

1

(cid:80)
= −(cid:88)
zi pθ(zi|xi) exp(R(zi|yi))
qθ(zi|xi  yi)
(cid:80)
zi pθ(zi|xi) exp(R(zi|yi)) pθ(zi|xi) exp(R(zi|yi)).

log pθ(zi|xi)

∂
∂θ

zi

zi

1

where qθ(zi|xi  yi) =

pθ(zi|xi) exp(R(zi|yi))

log pθ(zi|xi)

∂
∂θ

(cid:33)

(8)

(cid:32)(cid:88)

3

There are several advantages associated with the gra-
dient from Eq. (8).
First  qθ(zi|xi  yi) takes into account both pθ(zi|xi)
and R(zi|yi). As a result  Monte Carlo integration
over qθ-samples approximates Eq. (8) better  and has
smaller variance compared to Eq. (5). This allows
our model to start learning from scratch without the
warm-start and variance-reduction crutches needed
by previously-proposed PG approaches.
Second  as Figure 1 shows  the samples for the SPG
method (pentagons) lie between the ground-truth tar-
get distribution (triangle and circles) and the model
distribution (squares). These targets are both easier
to learn by pθ compared to ground-truth–only targets
like the ones for MLE (triangle) and RAML (circles) 
and also carry more information about the ground-truth target compared to model-only samples (PG
squares). This formulation allows us to directly address the exposure-bias problem  by allowing the
model distribution to learn at training time how to deal with events conditioned on model-generated
tokens  similar with what happens at inference time (more on this in Section 3.2). At the same time 
the updates used for learning rely heavily on the inﬂuence of the reward metric R(zi|yi)  therefore
directly addressing the wrong-objective problem. Together  these properties allow the model to
achieve improved accuracy.
Third  although qθ is infeasible for exact sampling  since both pθ(zi|xi) and exp(R(zi|yi)) are
factorizable across zi
t denotes the t-th word of the i-th output sequence)  we can apply
efﬁcient approximate inference for the SPG method as shown in the next section.

Figure 1: Comparing the target samples for
MLE  RAML (the rR distribution)  PG (the
pθ distribution)  and SPG (the qθ distribution).

t (where zi

3.1

Inference

In order to estimate the gradient from Eq. (8) with Monte-Carlo integration  one needs to be able
to draw samples from qθ(zi|xi  yi). To tackle this problem  we ﬁrst decompose R(zi|yi) along the
t-axis:

R(zi|yi) =

R(zi

(cid:123)(cid:122)
1:t|yi) − R(zi
(cid:44)∆ri
t|yi zi

t(zi

(cid:125)
1:t−1|yi)
 

1:t−1)

T(cid:88)

t=1

(cid:124)

T(cid:89)

where R(zi
increment notation  we can rewrite:

1:t|yi) − R(zi

1:t−1|yi) characterizes the reward increment for zi

t. Using the reward

qθ(zi|xi  yi) =

1

exp(log pθ(zi

1:t−1  xi) + ∆ri

t(zi

t|zi

t|yi  zi

1:t−1))

Zθ(xi  yi)

t=1

where Zθ(xi  yi) is the partition function equal to the sum over all conﬁgurations of zi. Since the
number of such conﬁgurations grows exponentially with respect to the sequence-length T   directly
drawing from qθ(zi|xi  yi) is infeasible. To make the inference efﬁcient  we replace qθ(zi|xi  yi)
with the following approximate distribution:
˜qθ(zi|xi  yi) =

T(cid:89)

t|xi  yi  zi

1:t−1) 

˜qθ(zi

where

˜qθ(zi

t|xi  yi  zi

1:t−1) =

1:t−1)
By replacing qθ in Eq. (8) with ˜qθ  we obtain:

˜Zθ(xi  yi  zi

t=1

∂
∂θ

Li

SP G(θ) = −(cid:88)
(cid:39) −(cid:88)

zi

zi

1

exp(log pθ(zi

1:t−1  xi) + ∆ri

t(zi

t|zi

t|yi  zi

1:t−1)).

log pθ(zi|xi)

log pθ(zi|xi) (cid:44) ∂
∂θ

˜Li

SP G(θ)

(9)

qθ(zi|xi  yi)

˜qθ(zi|xi  yi)

∂
∂θ

∂
∂θ

4

MLE targetRAML targetsPG targetsSPG targets(cid:6925)(cid:7578)(cid:7371)(cid:7578)rR1:t−1) sums over the conﬁgurations of one zi

Compared to Zθ(xi  yi)  ˜Zθ(xi  yi  zi
t only. Therefore 
the cost of drawing one zi from ˜qθ(zi|xi  yi) grows only linearly with respect to T . Furthermore  for
t|yi  zi
common reward metrics such as ROUGE and CIDEr  the computation of ∆ri
1:t−1) can be
t(zi
done in O(T ) instead of O(V ) (where V is the size of the state space for zi
t  i.e.  vocabulary size).
That is because the maximum number of unique words in yi is T   and any words not in yi have the
same reward increment. When we limit ourselves to J = 1 sample for each example in Eq. (9)  the
approximate SPG inference time of each example is similar to the inference time for the gradient of
the MLE objective. Combined with the empirical ﬁndings in Section 5 (Figure 3) where the steps
for convergence are comparable  we conclude that the time for convergence for the SPG method is
similar to the MLE based method.

3.2 Bang-bang Rewarded SPG Method

t|zi

One additional difﬁculty for the SPG method is that
the model’s log-probability values
1:t−1|yi) are not on the
1:t|yi) − R(zi
1:t−1  xi) and the reward-increment values R(zi
log pθ(zi
same scale. In order to balance the impact of these two factors  we need to weigh them appropriately.
t) (cid:44)
Formally  we achieve this by adding a weight wi
t·∆ri
t). The ap-
wi
t)  where

1:t−1) so that the total reward R(zi|yi  wi) =(cid:80)T
proximate proposal distribution becomes ˜qθ(zi|xi  yi  wi) =(cid:81)T

t|yi  zi
1:t−1  wi
1:t−1  wi

t to the reward increments: ∆ri

t|yi  zi

1:t−1  wi

t(zi

˜qθ(zi

t|xi  yi  zi

1:t−1  wi

t) ∝ exp(log pθ(zi

t|zi

1:t−1  xi) + ∆ri

t(zi
t|yi  zi
t(zi
t|xi  yi  zi
t|yi  zi

t=1 ∆ri
t=1 ˜qθ(zi
t(zi

1:t−1  wi

t)).

t|zi

(10)

(11)

The challenge in this case is to choose an appropriate weight wi
heavily for different i  t  as well as across different iterations and tasks.
In order to minimize the efforts for ﬁne-tuning the reward weights  we propose a bang-bang rewarded
softmax value function  equivalent to a loss LBBSP G deﬁned as:

t  because log pθ(zi

1:t−1  xi) varies

Li

BBSP G(θ) = −(cid:88)
BBSP G(θ) = −(cid:88)

wi

wi

p(wi) log(cid:0)Epθ(zi|xi)[exp(R(zi|yi  wi))](cid:1)  
(cid:88)
(cid:125)
(cid:124)

˜qθ(zi|xi  yi  wi)

log pθ(zi|xi)
 

(cid:123)(cid:122)

∂
∂θ

zi

p(wi)

(cid:44)− ∂
t = 0) = pdrop = 1 − p(wi

∂θ

SP G(θ|wi)
˜Li

and ∂
∂θ

˜Li

where p(wi) = (cid:81)

∆ri

t(zi

t|zi

t p(wi

t|yi  zi

t) and p(wi

(cid:54)= argmaxzi

t = W   the term ∆ri

t = W ). Here W is a sufﬁciently
large number (e.g.  10 000)  pdrop is a hyper-parameter in [0  1]. The name bang-bang is borrowed
from control theory [8]  and refers to a system which switches abruptly between two extreme states
(namely W and 0).
When wi
1:t−1  wi
t)
1:t−1  xi)  so the sampling of
overwhelms log pθ(zi
t. It is im-
t is decided by the reward increment of zi
zi
portant to emphasize that in general the groundtruth
t|yi  zi
label yi
1:t−1)  because
t
1:t−1 may not be the same as yi
1:t−1 (see an ex-
zi
ample in Figure 2). The only special case is when
t to always equal W   and
pdrop = 0  which forces wi
implies zi
t (and therefore the
SPG method reduces to the MLE method).
t = 0  by deﬁnition
On the other hand  when wi
t|yi  zi
t) = 0. In this case  the sam-
∆ri
t is based only on the model prediction
pling of zi
distribution pθ(zi
1:t−1  xi)  the same situation we
have at inference time. Furthermore  we have the
following lemma (with the proof provided in the Supplementary Material):

Figure 2: An example of sequence generation
with the bang-bang reward weights. z4 =
”in” is sampled from the model distribution
since w4 = 0. Although w5 = W   z5 =
”the” (cid:54)= y5 because z4 = ”in”.

t is always equal† to yi

1:t−1  wi
t|zi

t(zi

t(zi

t

†This follows from recursively applying R’s property that yi

t = argmaxzi

t

∆ri

t(zi

t|yi  zi

1:t−1 = yi

1:t−1).

5

t1234567ytamanissittingintheparkWWW0W......wtztamanisinthe......argmax (cid:7490)r5(z5|y  z1:4) = ‘the’ ≠ y5 = ‘in’Lemma 1 When wi

t = 0 (cid:88)

˜qθ(zi|xi  yi  wi)

∂
∂θ

log pθ(zi

t|xi  zi

1:t−1) = 0.

zi

t (cid:54)= 0 are included. To see that  using the fact that log pθ(zi|xi) =(cid:80)T

SP G(θ|wi) is very different from traditional PG-method gradients  in that only the zi
˜Li
t
1:t−1) 

t|xi  zi

As a result  ∂
∂θ
with wi

˜qθ(zi|xi  yi  wi)

∂
∂θ

log pθ(zi

t=1 log pθ(zi
t|xi  zi

1:t−1) 

Using the result of Lemma 1  Eq. (12) is equal to:

(cid:88)
(cid:88)

zi

∂
∂θ

∂
∂θ

˜Li

˜Li

SP G(θ|wi) = −(cid:88)
SP G(θ|wi) = − (cid:88)
= −(cid:88)

{t:wi

t

zi

t(cid:54)=0}
˜qθ(zi|xi  yi  wi)

zi

˜qθ(zi|xi  yi  wi)

log pθ(zi

t|xi  zi

1:t−1)

log pθ(zi

t|xi  zi

1:t−1)

∂
∂θ

∂
∂θ

(cid:88)

{t:wi

t(cid:54)=0}

(12)

(13)

Using Monte-Carlo integration  we approximate Eq. (11) by ﬁrst drawing wij from p(wi) and then
t|xi  zi
iteratively drawing zij
from ˜qθ(zi
t ) for t = 1  . . .   T . For larger values of pdrop 
t
the wij sample contains more wij
t = 0 and the resulting zij contains proportionally more samples
from the model prediction distribution (with a direct effect on alleviating the exposure-bias problem).
After zij is obtained  only the log-likelihood of zij

(cid:54)= 0 are included in the loss:

1:t−1  yi  wij

∂
∂θ

BBSP G(θ) (cid:39) − 1
˜Li
J

log pθ(zij

t |xi  zij

1:t−1).

(14)

J(cid:88)

j=1

(cid:110)

t

(cid:88)

t when wij
(cid:111) ∂

∂θ

t:w

ij

t (cid:54)=0

The details about the gradient evaluation for the bang-bang rewarded softmax value function are
described in Algorithm 1 of the Supplementary Material.

4 Additional Reward Functions
Besides the main reward function R(zi|yi)  additional reward functions can be used to enforce
desirable properties for the output sequences. For instance  in summarization  we occasionally ﬁnd
that the decoded output sequence contains repeated words  e.g. "US R&B singer Marie Marie Marie
Marie ...". In this framework  this can be directly ﬁxed by using an additional auxiliary reward
function that simply rewards negatively two consecutive tokens in the generated sequence:

(cid:26)−1

0

DUPi

t =

if zi
t = zi
otherwise.

t−1 

In conjunction with the bang-bang weight scheme  the introduction of such a reward function has the
immediate effect of severely penalizing such “stuttering” in the model output; the decoded sequence
after applying the DUP negative reward becomes: "US R&B singer Marie Christina has ...".
Additionally  we can use the same approach to correct for certain biases in the forward sampling
approximation. For example  the following function negatively rewards the end-of-sentence symbol
when the length of the output sequence is less than that of the ground-truth target sequence |yi|:

EOSi

t =

t = </S> and t < |yi| 

if zi
otherwise.

A more detailed discussion about such reward functions is available in the Supplementary Material.
During training  we linearly combine the main reward function with the auxiliary functions:

∆ri

t(zi

t|yi  zi

1:t−1  wi

t) = wi

1:t|yi) − R(zi

1:t−1|yi) + DUPi

t + EOSi
t

(cid:1)  

with W = 10  000. During testing  since the ground-truth target yi is unavailable  this becomes:

∆ri

t(zi

t|yi  zi

1:t−1  W ) = W · DUPi
t.

6

(cid:26)−1
t ·(cid:0)R(zi

0

5 Experiments

We numerically evaluate the proposed softmax policy gradient (SPG) method on two sequence
generation benchmarks: a document-summarization task for headline generation  and an automatic
image-captioning task. We compare the results of the SPG method against the standard maximum
likelihood estimation (MLE) method  as well as the reward augmented maximum likelihood (RAML)
method [17]. Our experiments indicate that the SPG method outperforms signiﬁcantly the other
approaches on both the summarization and image-captioning tasks.
We implemented all the algorithms using TensorFlow 1.0 [6]. For the RAML method  we used
τ = 0.85 which was the best performer in [17]. For the SPG algorithm  all the results were obtained
using a variant of ROUGE [13] as the main reward metric R  and J = 1 (sample one target for each
example  see Eq. (14)). We report the impact of the pdrop for values in {0.2  0.4  0.6  0.8}.
In addition to using the main reward-metric for sampling targets  we also used it to weight the loss
for target zij   as we found that it improved the performance of the SPG algorithm. We also applied
a naive version of the policy gradient (PG) algorithm (without any variance reduction) by setting
pdrop = 0.0  W → 0  but failed to train any meaningful model with cold-start. When starting from a
pre-trained MLE checkpoint  we found that it was unable to improve the original MLE result. This
result conﬁrms that variance-reduction is a requirement for the PG method to work  whereas our SPG
method is free of such requirements.

5.1 Summarization Task: Headline Generation

Gigaword-10K DUC-2004
22.6 ± 0.6
35.2 ± 0.3
23.1 ± 0.6
36.4 ± 0.2
36.6 ± 0.2
23.5 ± 0.6
37.8 ± 0.2
24.3 ± 0.5
37.4 ± 0.2
24.1 ± 0.5
24.6 ± 0.5
37.3 ± 0.2

Headline generation is a standard text generation task  taking as input a document and generating a
concise summary/headline for it. In our experiments  the supervised data comes from the English
Gigaword [9]  and consists of news-articles paired with their headlines. We use a training set of
about 6 million article-headline pairs  in addition to two randomly-extracted validation and evaluation
sets of 10K examples each. In addition to the Gigaword evaluation set  we also report results on the
standard DUC-2004 test set. The DUC-2004 consists of 500 news articles paired with four different
human-generated groundtruth summaries  capped at 75 bytes.‡ The expected output is a summary of
roughly 14 words  created based on the input article.
We use the sequence-to-sequence recurrent neural net-
work with attention model [2]. For encoding  we use
a three-layer  512-dimensional bidirectional RNN ar-
chitecture  with a Gated Recurrent Unit (GRU) as the
unit-cell [4]; for decoding  we use a similar three-layer 
512-dimensional GRU-based architecture. Both the en-
coder and decoder networks use a shared vocabulary
and embedding matrix for encoding/decoding the word
sequences  with a vocabulary consisting of 220K word
types and a 512-dimensional embedding. We truncate
the encoding sequences to a maximum of 30 tokens  and the decoding sequences to a maximum of
15 tokens. The model is optimized using ADAGRAD with a mini-batch size of 200  a learning rate
of 0.01  and gradient clipping with norm equal to 4. We use 40 workers for computing the updates 
and 10 parameter servers for model storing and (asynchronous and distributed) updating. We run
the training procedure for 10M steps and pick the checkpoint with the best ROUGE-2 score on the
Gigaword validation set.
We report ROUGE-L scores on the Gigaword evaluation set  as well as the DUC-2004 set  in Table 1.
The scores are computed using the standard pyrouge package§  with standard errors computed using
bootstrap resampling [12]. As the numerical values indicate  the maximum performance is achieved
when pdrop is in mid-range  with 37.8 F1 ROUGE-L at pdrop = 0.4 on the large Gigaword evaluation
set (a larger range for pdrop between 0.4 and 0.8 gives comparable scores on the smaller DUC-2004
set). These numbers are signiﬁcantly better compared to RAML (36.4 on Gigaword-10K)  which in
turn is signiﬁcantly better compared to MLE (35.2).

Method
MLE
RAML
SPG 0.2
SPG 0.4
SPG 0.6
SPG 0.8

Table 1: The F1 ROUGE-L scores (with
standard errors) for headline generation.

‡This dataset is available by request at http://duc.nist.gov/data.html.
§Available at pypi.python.org/pypi/pyrouge/0.1.3

7

5.2 Automatic Image-Caption Generation

Method
MLE
RAML
SPG 0.2
SPG 0.4
SPG 0.6
SPG 0.8

Validation-4K

37.7 ± 0.1
38.0 ± 0.1
38.0 ± 0.1
38.1 ± 0.1
38.2 ± 0.1
37.7 ± 0.1

Table 2: The CIDEr (with the coco-caption
package) and ROUGE-L (with the pyrouge
package) scores for image captioning on
MSCOCO.

C40
CIDEr ROUGE-L CIDEr
0.94
0.968
0.997
0.97
0.98
1.001
1.00
1.013
1.01
1.033
1.009
1.00

For the image-captioning task  we use the standard
MSCOCO dataset [14]. The MSCOCO dataset contains
82K training images and 40K validation images  each
with at least 5 groundtruth captions. The results are
reported using the numerical values for the C40 testset
reported by the MSCOCO online evaluation server¶.
Following standard practice  we combine the training
and validation datasets for training our model  and hold
out a subset of 4K images as our validation set.
Our model architecture is simple  following the ap-
proach taken by the Show-and-Tell approach [25]. We
use a one 512-dimensional RNN architecture with an
LSTM unit-cell  with a dropout rate equal of 0.3 ap-
plied to both input and output of the LSTM layer. We use the same vocabulary size of 8 854
word-types as in [25]  with 512-dimensional word-embeddings. We truncate the decoding sequences
to a maximum of 15 tokens. The input image is embedded by ﬁrst passing it through a pretrained
Inception-V3 network [22]  and then projected to a 512-dimensional vector. The model is optimized
using ADAGRAD with a mini-batch size of 25  a learning rate of 0.01  and gradient clipping with
norm equal to 4. We run the training procedure for 4M steps and pick the checkpoint of the best
CIDEr score [23] on our held-out 4K validation set.
We report both CIDEr and ROUGE-L scores on our
4K Validation set  as well as CIDEr scores on the of-
ﬁcial C40 testset as reported by the MSCOCO online
evaluation server  in Table 2. The CIDEr scores are re-
ported using the coco-caption evaluation toolkit(cid:107)  while
ROUGE-L scores are reported using the standard py-
rouge package (note that these ROUGE-L scores are
generally lower than those reported by the coco-caption
toolkit  as it reports an average score over multiple
reference  while the latter reports the maximum).
The evaluation results indicate that the SPG method is
superior to both the MLE and RAML methods. The
maximum score is obtained with pdrop = 0.6  with a
CIDEr score of 1.01 on the C40 testset. In contrast 
on the same testset  the RAML method has a CIDEr
score of 0.97  and the MLE method a score of 0.94. In
Figure 3  we show that the number of steps for SPG to converge is similar to the one for MLE/RAML.
With the per-step inference cost of those methods being similar (see Section 3.1)  the overall conver-
gence time for the SPG method is similar to the MLE and RAML methods.

Figure 3: Number of training steps vs.
CIDEr scores (on Validation-4K) for var-
ious learning regimes.

6 Conclusion

The reinforcement learning method presented in this paper  based on a softmax value function  is
an efﬁcient policy-gradient approach that eliminates the need for warm-start training and sample
variance reduction during policy updates. We show that this approach allows us to tackle sequence
generation tasks by training models that avoid two long-standing issues: the exposure-bias problem
and the wrong-objective problem. Experimental results conﬁrm that the proposed method achieves
superior performance on two different structured output prediction problems  one for text-to-text
(automatic summarization) and one for image-to-text (automatic image captioning). We plan to
explore and exploit the properties of this method for other reinforcement learning problems as well
as the impact of various  more-advanced reward functions on the performance of the learned models.

¶Available at http://mscoco.org/dataset/#captions-eval.
(cid:107)Available at https://github.com/tylin/coco-caption.

8

05000001000000150000020000002500000Steps0.900.920.940.960.981.001.021.04CIDER ScoreMLERAMLSPG 0.6Acknowledgments

We greatly appreciate Sebastian Goodman for his contributions to the experiment code. We would
also like to acknowledge Ning Ye and Zhenhai Zhu for their help with the image captioning model
calibration as well as the anonymous reviewers for their valuable comments.

References
[1] Peter Anderson  Basura Fernando  Mark Johnson  and Stephen Gould. SPICE: semantic

propositional image caption evaluation. In ECCV  2016.

[2] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align

and translate. In Proceedings of ICLR  2015.

[3] Samy Bengio  Oriol Vinyals  Navdeep Jaitly  and Noam Shazeer. Scheduled sampling for
In Advances in Neural Information

sequence prediction with recurrent neural networks.
Processing Systems 28  pages 1171–1179. 2015.

[4] K. Cho  B. van Merrienboer  C. Gülçehre  D. Bahdanau  F. Bougares  H. Schwenk  and
Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine
translation. In Proceedings of EMNLP  pages 1724–1734  2014.

[5] Marc P. Deisenroth  Gerhard Neumann  and Jan Peters. A survey on policy search for robotics.

Foundations and Trends R(cid:13) in Robotics  2(1–2):1–142  2013. ISSN 1935-8253.

[6] M. Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems  2015.

URL http://tensorflow.org/.

[7] Y. Wu et al. Google’s neural machine translation system: Bridging the gap between human and

machine translation. CoRR  abs/1609.08144  2016.

[8] L. C. Evans. An introduction to mathematical optimal control theory. Preprint  version 0.2.

[9] David Graff and Christopher Cieri. English Gigaword Fifth Edition LDC2003T05. In Linguistic

Data Consortium  Philadelphia  2003.

[10] Ferenc Huszar. How (not) to train your generative model: Scheduled sampling  likelihood 

adversary? CoRR  abs/1511.05101  2015.

[11] Jens Kober  J Andrew Bagnell  and Jan Peters. Reinforcement learning in robotics: A survey.

The International Journal of Robotics Research  32(11):1238–1274  2013.

[12] Philipp Koehn. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings

of EMNLP  pages 388—-395  2004.

[13] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using

longest common subsequence and skip-bigram statistics. In Proceedings of ACL  2004.

[14] Tsung-Yi Lin  Michael Maire  Serge J. Belongie  Lubomir D. Bourdev  Ross B. Girshick  James
Hays  Pietro Perona  Deva Ramanan  Piotr Dollár  and C. Lawrence Zitnick. Microsoft COCO:
common objects in context. CoRR  abs/1405.0312  2014.

[15] Siqi Liu  Zhenhai Zhu  Ning Ye  Sergio Guadarrama  and Kevin Murphy. Optimization of image
description metrics using policy gradient methods. In International Conference on Computer
Vision (ICCV)  2017.

[16] Gergely Neu  Anders Jonsson  and Vicenç Gómez. A uniﬁed view of entropy-regularized

markov decision processes. CoRR  abs/1705.07798  2017.

[17] M. Norouzi  S. Bengio  Z. Chen  N. Jaitly  M. Schuster  Y. Wu  and D. Schuurmans. Reward
In Advances in Neural

augmented maximum likelihood for neural structured prediction.
Information Processing Systems 29  pages 1723–1731  2016.

[18] Kishore Papineni  Salim Roukos  Todd Ward  and Wei-Jing Zhu. Bleu: A method for automatic

evaluation of machine translation. In Proceedings of ACL  2002.

9

[19] Marc’Aurelio Ranzato  Sumit Chopra  Michael Auli  and Wojciech Zaremba. Sequence level

training with recurrent neural networks. CoRR  abs/1511.06732  2015.

[20] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al. Mas-
tering the game of Go with deep neural networks and tree search. Nature  529(7587):484–489 
2016.

[21] RS Sutton  D McAllester  S Singh  and Y Mansour. Policy gradient methods for reinforcement

learning with function approximation. In NIPS  1999.

[22] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jonathon Shlens  and Zbigniew Wojna.

Rethinking the inception architecture for computer vision. volume abs/1512.00567  2015.

[23] Ramakrishna Vedantam  C. Lawrence Zitnick  and Devi Parikh. Cider: Consensus-based image
description evaluation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  June 2015.

[24] Arun Venkatraman  Martial Hebert  and J. Andrew Bagnell. Improving multi-step prediction of
learned time series models. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence  pages 3024–3030. AAAI Press  2015.

[25] Oriol Vinyals  Alexander Toshev  Samy Bengio  and Dumitru Erhan. Show and tell: A neural
In Proc. of IEEE Conference on Computer Vision and Pattern

image caption generator.
Recognition (CVPR)  2015.

[26] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine Learning  8(3):229–256  1992.

10

,Nan Ding
Radu Soricut