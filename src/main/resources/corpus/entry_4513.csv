2019,Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection,We study two problems in high-dimensional robust statistics: \emph{robust mean estimation} and \emph{outlier detection}.
In robust mean estimation the goal is to estimate the mean $\mu$ of a distribution on $\mathbb{R}^d$ given $n$ independent samples  an $\epsilon$-fraction of which have been corrupted by a malicious adversary.
In outlier detection the goal is to assign an \emph{outlier score} to each element of a data set such that elements more likely to be outliers are assigned higher scores.
Our algorithms for both problems are based on a new outlier scoring method we call QUE-scoring based on \emph{quantum entropy regularization}.
For robust mean estimation  this yields the first algorithm with optimal error rates and nearly-linear running time $\tilde{O}(nd)$ in all parameters  improving on the previous fastest running time $\tilde{O}(\min(nd/\e^6  nd^2))$.
For outlier detection  we evaluate the performance of QUE-scoring via extensive experiments on synthetic and real data  and demonstrate that it often performs better than previously proposed algorithms.,Quantum Entropy Scoring for Fast Robust Mean

Estimation and Improved Outlier Detection

Yihe Dong

Microsoft Research

yihedong@gmail.com

Samuel B. Hopkins

University of California  Berkeley

hopkins@berkeley.edu

Jerry Li

Microsoft Research

jerrl@microsoft.com

Abstract

We study two problems in high-dimensional robust statistics: robust mean esti-
mation and outlier detection. In robust mean estimation the goal is to estimate
the mean µ of a distribution on Rd given n independent samples  an ε-fraction
of which have been corrupted by a malicious adversary. In outlier detection the
goal is to assign an outlier score to each element of a data set such that elements
more likely to be outliers are assigned higher scores. Our algorithms for both
problems are based on a new outlier scoring method we call QUE-scoring based
on quantum entropy regularization. For robust mean estimation  this yields the

ﬁrst algorithm with optimal error rates and nearly-linear running time (cid:101)O(nd) in all
parameters  improving on the previous fastest running time (cid:101)O(min(nd/ε6  nd2)).

For outlier detection  we evaluate the performance of QUE-scoring via extensive
experiments on synthetic and real data  and demonstrate that it often performs
better than previously proposed algorithms. Code for these experiments is available
at https://github.com/twistedcubic/que-outlier-detection.

1

Introduction

We study outlier-robust statistics in high dimensions  focusing on the question: can theoretically
sound outlier robust algorithms have practical running times for large  high-dimensional data sets?
We address two related problems: robust mean estimation  which is primarily theoretical  and an
applied counterpart  outlier detection.
Robust mean estimation Our main theoretical contribution is the ﬁrst nearly-linear time algorithm
for robust mean estimation with nearly-optimal error. Here the goal is to estimate the mean µ ∈ Rd
of a d-dimensional distribution D given ε-corrupted samples X1  . . .   Xn – that is  i.i.d. samples  an
unknown ε-fraction of which have been maliciously corrupted. Under (for instance) the assumption
√
that the covariance of D is bounded by Id  it has been long known to be possible in exponential time
to estimate µ by ˆµ having (cid:107)µ − ˆµ(cid:107)2 ≤ O(
ε). In particular  this rate of error is independent of d.
Polynomial-time algorithms provably achieving such d-independent error became known only re-
cently  starting with the works [8  15]. Until our work  the running time of algorithms with provably
d-independent error remained suboptimal by polynomial factors in d or ε: the fastest running time

achieved before this work was (cid:101)O(min(nd2  nd/ε6)) [6  8  15  9]. (Here (cid:101)O(·) notation hides logarith-
robust mean estimation with running time (cid:101)O(nd) which achieves error (cid:107)µ − ˆµ(cid:107)2 ≤ O(

mic factors in n and d). While these running times represent a dramatic improvement over previous
exponential-time algorithms  there are still many interesting regimes where the additional runtime
overheads these algorithms incur render them impractically slow. We give the ﬁrst algorithm for
ε). Note
that this running time is nearly-linear in the input size nd. Similar to prior works  our algorithm
has information-theoretically optimal sample complexity and nearly-optimal error rates in both the
bounded-covariance and sub-Gaussian regimes.

√

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Outlier detection Our main applied contribution is a new algorithm for high-dimensional outlier
detection  which we assess via experiments on both synthetic and real data 1. Our goal is to take
a dataset X1  . . .   Xn ∈ Rd and assign to each Xi an outlier score τi ≥ 0  so that higher scores τi
are assigned to points Xi more likely to be outliers. Of course  what constitutes an outlier varies
across applications  so no single algorithm for outlier detection is likely to be the best in all domains.
We show that our method performs well in settings where individual outliers are difﬁcult to pick out
on their own (by  say  their (cid:96)2 norms or their distances to nearby points)  but still collectively bias
empirical statistics such as the mean and covariance.
We compare our method to baselines based on PCA and Euclidean distances  as well as more
sophisticated algorithms from existing literature based on nearest-neighbor distances. Our algorithm
has nearly-linear running time in theory  and simple implementations in practice incur minimal
overhead beyond standard spectral methods  allowing us to run on 1024-dimensional data with no
special optimizations and 8192-dimensional data with a fast approximate implementation. It can
therefore be used in practice to complement existing approaches to outlier detection in exploratory
data analysis.

1.1 What is an outlier and why are they hard to ﬁnd?

For us  an outlier is an element of a data set which was generated according to a different process than
the majority of the data. For instance  we may imagine that our samples X1  . . .   Xn were sampled
i.i.d. from a distribution (1 − ε)D + εN over Rd  where D is the distribution of inliers  N is the
distribution of outliers  and ε > 0 is a small number – that is  we imagine that a constant fraction of
our data may be outliers.
For this discussion  we also informally imagine that N is sufﬁciently distinct from D that the set of
outliers could be approximately identiﬁed by brute-force search over subsets of (1 − ε)n samples  if
given unlimited computational resources. Otherwise  outlier detection is not a meaningful problem 
and robust mean estimation is easy (because the empirical mean will be a good estimator). Under
these circumstances  what makes identifying outliers and estimating the mean in their presence
difﬁcult? Chieﬂy:
Outliers may not be identiﬁable in isolation. On its own  a typical outlier Xi ∼ N may look much
like a typical inlier Xj ∼ D. For instance  it could be (cid:107)Xi(cid:107)2 ≈ (cid:107)Xj(cid:107)2  and Xi  Xj may have similar
distance to the nearest few neighboring samples  especially in high dimensions where samples are far
apart.
Outliers still introduce bias  collectively Even if individual outliers look innocuous  the collective
effect a modiﬁed ε-fraction of samples Xi can still substantially change the empirical distribution of
X1  . . .   Xn. As a result  even simple statistical tasks like estimating the mean or covariance of D
require sophisticated estimators: naively pruning individual outliers and then employing standard
empirical estimators typically leads to far-suboptimal error rates. For example  an ε-fraction of
X1  . . .   Xn which are all slightly biased in a single direction may shift the empirical mean of
X1  . . .   Xn  but this bias will be difﬁcult to detect by looking at small numbers of samples at once.
This also demonstrates that successful outlier detection can require global geometric information
about a high-dimensional dataset  such as whether or not a direction exists in which many (say  εn)
samples are unusually biased.
Outliers may be inhomogeneous. Outliers need not exhibit unusual bias in only one direction  or all
have the same norm  or lie in a single cluster. Rather  if a dataset exhibits several forms of corruption 
there may be as many different-looking kinds of outliers. In the theoretical robust mean estimation
setting  the adversary producing ε-corrupted samples may corrupt εn/10 samples by biasing them in
some direction  another εn/10 samples by unusually enlarging their norms  and so forth.
Since robust mean estimation involves a malicious adversary  all of the above phenomena must be
addressed by our robust mean estimation algorithm. In the empirical section of this paper  we focus
on designing an outlier detection method suited to situations where at least one of them occurs – in
other cases  existing methods (such as those based on Euclidean norms or local neighborhoods of
individual samples [5]) may be more appropriate.

1Code is available at https://github.com/twistedcubic/que-outlier-detection.

2

1.2 QUE: Quantum Entropy Scoring

√

Recent innovations in robust mean estimation [15  8] rely on the following crucial observation about
ε-corrupted samples X1  . . .   Xn from a distribution D with covariance Σ (cid:22) Id. Namely: any subset
S ⊆ {X1  . . .   Xn} of samples which shift the empirical mean by distance more than
ε in some
direction v also introduce an eigenvalue of magnitude greater than 1 to the empirical covariance.
In robust mean estimation  this leads to (amongst others) the ﬁlter algorithm of [8  9]  one of the ﬁrst
to achieve dimension-independent error rates. Roughly speaking  the algorithm iterates the following
until the empirical covariance Σ has small spectral norm: (1) compute the top eigenvector v of the
empirical covariance of Σ  then (2) throw out samples Xi whose projections |(cid:104)Xi − µ  v(cid:105)| (cid:29) 1 is
unusually large  where µ is the empirical mean of the corrupted dataset. For outlier detection this
suggests a natural scoring rule – let the outlier score τi of sample Xi be proportional to |(cid:104)Xi − µ  v(cid:105)|.
The main drawback of these algorithms is that they do not adequately account for inhomogeneity of

outliers. For the ﬁlter  this leads to a worst-case running time of (cid:101)O(nd2)  because the ﬁlter operation
(which can be implemented in (cid:101)O(nd) time) may have to be repeated as many as d times if the

adversary introduces outliers lying in d orthogonal directions. The rule τi = |(cid:104)Xi − µ  v(cid:105)| may miss
outliers causing a large eigenvalue of Σ  but in a direction orthogonal to the top eigenvector v.
In the opposite extreme  if outliers are maximally inhomogeneous – no group of them is unusually
biased in some shared direction v – then the only way they can bias the empirical mean is for the
individual (cid:96)2 norms (cid:107)Xi − µ(cid:107)2 to be larger than typical. This suggests a different scoring rule:
τi = (cid:107)Xi − µ(cid:107)2. This approach  however  breaks down in the situation we started with  that groups
of outliers are biased in a shared direction but they do not have larger norms than good samples.
Our main conceptual contribution is an approach to utilize information about outliers beyond
what is available in the top eigenvector of the empirical covariance Σ and in individual (cid:96)2 norms.
Appropriately adapted to their respective settings  this leads to our algorithms for both robust mean
estimation and outlier detection.
Our ﬁrst observation is that any eigenvalue/eigenvector λ  v – not just the top ones – of the empirical
covariance with λ (cid:29) 1 must be due to outliers. We therefore consider the intermediate goal of ﬁnding
a distribution over directions v ∈ Rd containing information about as many outlier directions as
possible. We formalize this as the following entropy-regularized convex program over d × d positive
semideﬁnite matrices:

α · (cid:104)U  Σ(cid:105) + S(U ) such that U (cid:23) 0  tr(U ) = 1  

the matrix U. If U = (cid:80)d

(1)
where α ≥ 0 is some constant and (cid:104)A  B(cid:105) = tr(AB(cid:62)) denotes the trace inner product of matrices.
Here  S(U ) = −(cid:104)U  log U(cid:105) is the quantum entropy (also known as the von Neumann entropy) of
is the eigendecomposition of U  since it has tr(U ) = 1 we
may interpret it as a distribution over orthonormal vectors v1  . . .   vd with weights µ1  . . .   µd and
hence with entropy S(U ). Under this interpetation  (cid:104)U  Σ(cid:105) = Evi∼µ(cid:104)vi  Σvi(cid:105). As α varies  (1)
trades off optimizing for a distribution supported on many distinct directions for a distribution
supported on eigenvectors of Σ with large eigenvalues. The optimizer of (1) takes the form U =
exp(α · Σ)/tr exp(α · Σ) where exp(·) is the matrix exponential function.
Deﬁnition 1.1. Let U = exp(α · Σ)/tr exp(α · Σ) be the optimizer of (1)  for some data set
X = X1  . . .   Xn ∈ Rd where Σ is the covariance of X . The quantum entropy (QUE) scores with
parameter α are given by τi = (Xi − µ)(cid:62)U (Xi − µ)  where µ is the mean of X .
Intuitively  the QUE scores will penalize any point which is causing a large eigenvalue in any
direction  which should allow us to ﬁnd more outliers than the naive spectral scores presented above.
QUE scores also interpolate between two more naive scoring rules: when α = 0 we have U = Id /d
2 is the (cid:96)2 norm (up to a scaling)  while when α → ∞ we have U → vv(cid:62)
and so τi = 1
where v is the top eigenvector of Σ  recovering naive spectral scoring. In both experiments and theory
we ﬁnd that choosing α strictly between 0 and ∞ outperforms either of the extreme choices.
QUE scores are also appealing from a computational perspective: we show that a list of approximate
i = (1 ± 0.01)τi can be computed from X1  . . .   Xn in nearly-linear time  by appro-
QUE scores τ(cid:48)
priate use of Johnson-Lindenstrauss sketching and efﬁcient computation of the matrix exponential by

d(cid:107)Xi − µ(cid:107)2

max
U∈Rd×d

i=1 µiviv(cid:62)

i

3

series expansion. This is crucial to both the nearly-linear running time of our algorithm for robust
mean estimation and to the scalability of our outlier detection method.
In Section 1.4 we describe reﬁnements of QUE scoring which ﬁt it into the matrix multiplicative
weights framework [3]  leading to our nearly-linear time algorithms for robust mean estimation.
We give two very similar algorithms  one for when the distribution of inliers is only assumed to
have bounded covariance  and one when the inliers are assumed to be subgaussian. The resulting
algorithms are conceptually similar to the following modiﬁcation of the ﬁlter mentioned above: until
(cid:107)Σ(cid:107)2 ≤ O(1)  compute QUE scores τi  throw out data points Xi with τi (cid:29) 1  and repeat. (To obtain
provable guarantees  our ﬁnal algorithms are somewhat more complex: in some iterations we use
QUE scores based on certain reweightings of the data learned in previous iterations.)
In Section 1.5 we describe experiments validating the QUE scoring rule on both synthetic and real
data sets. We show that it performs especially well by comparison to local-neighborhood methods
and to scoring based on only the top eigenvector in data sets where the inliers are close to isotropic
(or can be made so by applying data whitening procedures) and in which there are heterogeneous
outliers.

1.3 Related work

Robust mean estimation: The study of robust statistics and in particular robust mean estimation
began with major works by Anscombe  Huber  Tukey and others in the 1960s [2  25  12  26]. The
literature on polynomial-time algorithms for robust statistics has exploded in recent years  following
works by Diakonikolas et al and Lai  Rao and Vempala giving the ﬁrst polynomial-time algorithms for
robust mean estimation with dimension-independent (or nearly dimension-indepedent) error [8  15].
A full survey is beyond our scope here – see e.g. the recent theses [17  24] for a thorough account.
Particularly relevant to our work is the recent work of Cheng  Diakonikolas  and Ge who design an

algorithm for robust mean estimatin with running time (cid:101)O(nd/ε6) – the ﬁrst to achieve nearly linear

time for constant ε – by appeal to nearly linear time solvers for packing and covering semideﬁnite
programs [6]. Our algorithms carry two advantages over this prior work: ﬁrst  our algorithm runs
in nearly linear time for any choice of ε = ε(n  d)  and second  because we avoid the 1/ε6 scaling
and appeal to semideﬁnite programming  our theoretical ideas lead to a practical method for outlier
detection. The techniques of Diakonikolas et al. were later extended to robust covariance estimation
[7]; it remains an interesting direction to extend our techniques to covariance estimation.
Concurrent work: After this manuscript was initially submitted  we became aware of the concur-
rent work [16]  which also obtains a nearly-linear time algorithm for robust mean estimation of
distributions with bounded covariance. The algorithm of [16] also obtains subgaussian conﬁdence
intervals (see e.g. [19])  which the algorithm in this work does not. By contrast  the algorithms in
our work also obtain improved rates of error with respect to ε when the underlying distribution is
sub-Gaussian  and our method is sufﬁciently practical that we are able to implement parts of it to
run our experiments on outlier detection. (The method of [16] relies on nearly-linear time solvers
for packing/covering semideﬁnite programs  which are not yet practical.) Finally  implicit in the
work [16] is a reduction from arbitrary ε to the case ε = 1/100; we describe this reduction and some
consequences in supplementary material.
Outlier detection Detection of outliers goes back nearly to the beginning of statistics itself [11].
Even restricting to the high dimensional case it has a literature too broad to survey here. Much recent
work has focused on so-called local outlier factor-based methods  which assign outlier scores based
on the local density of other samples near each Xi – see e.g. [13  14] and further references in [5].
We ﬁnd that QUE scoring compares favorably to such local methods in high-dimensional datasets
like we describe in Section 1.1 – see Sections 1.5 and supplementary material for details.

1.4 Robust mean estimation: results and algorithm overview

We turn to our algorithm for robust mean estimation  deferring details to supplementary material.
Deﬁnition 1.2 (ε-corrupted samples). Let D be a distribution on Rd. We say that X1  . . .   Xn are an
ε-corrupted set of samples from D if they are ﬁrst drawn i.i.d. from D  then modiﬁed by an adversary
who may adaptively inspect all the samples  remove εn of them  and replace them with arbitrary
vectors in Rd.

4

Note that ε-corruption is a stronger outlier model than the (1−ε)D +εN mixture model we described
in Section 1; our algorithms also work in this milder mixture model. Our main theoretical result is:
Theorem 1.1. For every n  d ∈ N and ε > 0 there are algorithms QUESCOREFILTER  S.G.-

µ and covariance Σ  given n ε-corrupted samples from D  QUESCOREFILTER produces ˆµ such
√
that (cid:107)ˆµ − µ(cid:107)2 ≤ O(

QUESCOREFILTER with running time (cid:101)O(nd)  such that for every distribution D on Rd with mean
ε) + (cid:101)O((cid:112)d/n) if Σ (cid:22) Id  and S.G.-QUESCOREFILTER produces ˆµ such
that (cid:107)ˆµ − µ(cid:107)2 ≤ O(ε(cid:112)log(1/ε) +(cid:112)d/n) if D is sub-Gaussian with Σ = Id  all with probability
factors. The other term  (cid:101)O((cid:112)d/n)  is information-theoretically optimal up to the logarithmic
factors in the (cid:101)O(·) even without corruptions. For the sub-Gaussian case  the O(ε(cid:112)log 1/ε) term is
such as Tukey median  and the latter is information-theoretically optimal [26]. The(cid:112)d/n term is

believed to be necessary for computationally efﬁcient algorithms (see e.g the statistical-query lower
bound [10])  although that term can be made O(ε) by using computationally-intractable estimators

ε) term information-theoretically optimal up to constant

For the bounded covariance case  the O(

at least 0.99.

√

ε).

√

information-theoretically optimal even without corruptions.
In this section we discuss our algorithm for the bounded-covariance case Σ (cid:22) Id in the setting that
the adversary may not remove samples  leaving technical details and the modiﬁcations necessary to
handle removed samples and sub-Gaussian D to supplementary material.
Deﬁnition 1.3 (Simpliﬁed robust mean estimation). Let S = {X1  . . .   Xn} ⊆ Rd be a dataset with
the property that S partitions into S = Sg ∪ Sb with |Sb| ≤ εn and Ei∼Sg (Xi− µg)(Xi− µg)(cid:62) (cid:22) Id 
where µg = Ei∼Sg Xi. Given S  the goal is to ﬁnd a vector ˆµ with (cid:107)µg − ˆµ(cid:107)2 ≤ O(
Like prior algorithms for robust mean estimation  ours maintains a weight vector w1  . . .   wn ≥ 0
suspected to be outliers that are causing (cid:107)µ(w) − µg(cid:107)2 to be large.2 A key insight of recent work
on robust mean estimation is that it sufﬁces to ﬁnd weights w which place almost as much mass
on Sg as does the uniform weighting and whose empirical covariance is small. This is formalized

with(cid:80) wi ≤ 1  initialized to wi = 1/n. The algorithm iteratively decreases the weight of points
(cid:80) wiXi  and
in the following lemma. For a weight vector w  let |w| = (cid:80) wi  µ(w) = 1|w|
(cid:80) wi(Xi − µ(w))(Xi − µ(w))(cid:62). Let (cid:107)M(cid:107)2 be the spectral norm of a matrix M.

M (w) = 1|w|
Lemma 1.2 (Implicit in prior work). Let S = {X1  . . .   Xn} be as in Deﬁnition 1.3. Suppose
that w is a weight vector such that (cid:107)M (w)(cid:107)2 ≤ O(1) and w is mostly good  by which we mean
| 1
n1Sg − wg| ≤ | 1
n1Sb − wb|  where 1Sg   1Sb are the indicators of Sg  Sb and wg  wb are w restricted
√
to Sg  Sb respectively. (Intuitively  w is mostly good if it results by removing from the uniform
weighting 1S/n more weight from Sb than from Sg.) Then (cid:107)µ(w) − µg(cid:107)2 ≤ O(
w to cause (cid:107)µ(w)−µg(cid:107)2 (cid:29) √
Lemma 1.2 captures the following geometric intuition: if the bad points Sb receive enough weight in
ε  then an O(ε)-fraction of the mass of w is on Xi which are unusually
correlated with the vector µ(w) − µg  which leads to a large maximum eigenvalue in M (w). Prior
works employ a variety of methods to ﬁnd a mostly good weight vector w with (cid:107)M (w)(cid:107)2 ≤ O(1).
Perhaps the simplest is the ﬁlter of [8]  which iterates: While (cid:107)M (w)(cid:107)2 (cid:29) 1  compute its top
eigenvector v and naive spectral scores τi = (cid:104)Xi − µ(w)  v(cid:105)2. Throw out Xi with large τi and repeat.
The ﬁlter ensures that the weight vector it maintains is mostly good because (in an averaged sense)
τi can be large only for Xi which are corrupted. This is because the (weighted) sum of all scores
wiτi ≈
(cid:104) 1
(Xi − µg)(Xi − µg)(cid:62)  vv(cid:62)(cid:105) ≤ 1. (Here we ignore some details about centering Xi at µg

(cid:80) wiτi = (cid:104)M (w)  vv(cid:62)(cid:105) (cid:29) 1  while the contribution to this sum from Sg has(cid:80)
(cid:80)
rather than µ(w).) Thus  the τi from Sb must make up almost all of(cid:80) wiτi. Simple approaches to

removing or downweighting Xi with large τi then remove strictly more weight from Sb than from Sg.
However  ﬁltering based on naive spectral scores alone faces a barrier to achieving nearly-linear
running-time. If the corruptions Sb are split among many orthogonal directions  the naive spectral

i∈Sg

i∈Sg

ε).

n

2Some prior algorithms  e.g. the ﬁlter of [8] instead iteratively throw out points suspected to be outliers.
However  since those algorithms are (necessarily) randomized  they can also be viewed as weighting points 
where the weight of Xi is the probability it has not been thrown out. The algorithm we present here can also be
implemented by throwing out points in a randomized fashion – we discuss further in the appendix.

5

i∈Sb

i∈Sg

of M (w). We show that our modiﬁed QUE scores τi maintain the property that(cid:80)
(cid:80)

ﬁlter will have to ﬁnd those directions one at a time. Thus  it may require Ω(d) iterations (leading to
Ω(nd2) running time) to arrive at w with (cid:107)M (w)(cid:107)2 ≤ O(1).
Our main idea is that by replacing naive spectral scores with slightly modiﬁed QUE scores  each
iteration of the ﬁlter can take into account projections of each sample onto many large eigenvectors
wiτi (cid:29)
wiτi  and so downweighting according to τi removes more mass from Sb than Sg. However 
ﬁltering with QUE scores makes faster progress than with naive spectral scores: roughly speaking 
we show that only O(log d)2 rounds of ﬁltering according to QUE scores are required to ﬁnd a
mostly-good weight vector w with (cid:107)M (w)(cid:107)2 ≤ O(1).
The core of our algorithm is a subroutine  DECREASESPECTRALNORM  to take a mostly good weight
vector w with (cid:107)M (w)(cid:107)2 (cid:29) 1 and in O(log d) rounds of QUE ﬁltering produce another mostly good
w(cid:48) with (cid:107)M (w(cid:48))(cid:107)2 ≤ 3
4(cid:107)M (w)(cid:107)2. Repeating this subroutine O(log d) times and then outputting the
resulting µ(w) yields our main algorithm. An outline of this subroutine is presented as Algorithm 1.
We ﬁrst establish a rigorous sense in which downweighting according to outlier scores τi makes
progress: it decreases the weighted average of the scores while removing more weight from bad
points than good.
Lemma 1.3 (Progress in one round of downweighting  informal). There is a downweighting algorithm
which takes a density matrix U and a mostly good weight vector w and produces a mostly good
weight vector w(cid:48) by downweighting points with large score τi = (cid:104)Xi − µ(w)  U (Xi − µ(w)(cid:105) such

that(cid:80) w(cid:48)
Let us give a geometric interpretation to Lemma 1.3: it establishes that if(cid:80) wiτi = (cid:104)U  M (w)(cid:105) (cid:29) 1

(cid:80) wiτi so long as(cid:80) wiτi (cid:29) 1. Furthermore  M (w(cid:48)) (cid:22) M (w).
(cid:104)M (w(cid:48))  U(cid:105) ≈(cid:88)

then the quadratic form of M (w(cid:48)) decreases in the directions deﬁned by U  since

(cid:104)M (w)  U(cid:105) .

iτi ≤ 1

(cid:88)

(2)

wiτi =

3

iτi ≤ 1
w(cid:48)
3

1
3

This guarantee becomes more meaningful as the entropy S(U ) increases  because it suggests the
quadratic form of M (w) has decreased in more directions. To make this formal  we appeal to the
matrix multiplicative weights framework. DECREASESPECTRALNORM applies downweighting
iteratively using a sequence of entropy-maximizing density matrices U1  . . .   UT chosen according
to the matrix multiplicactive weights update rule  leading to a series of mostly good weight vectors
w1  . . .   wT such that (cid:107)M (wT )(cid:107)2 ≤ 3

(cid:32)

Ut = exp

1

(cid:107)M (w)(cid:107)2

k=0

(cid:32)
4(cid:107)M (w0)(cid:107)2. We choose
t−1(cid:88)

(cid:33)(cid:30)

M (wk)

tr exp

t−1(cid:88)

k=0

1

(cid:107)M (w)(cid:107)2

(cid:33)

M (wk)

 

(3)

where w0 = w is the input weight vector  U0 = Id  and wt results from applying the downweighting
of Lemma 1.3 to wt−1 using Ut (if (cid:104)M (wt−1)  Ut(cid:105) (cid:29) 1). The following lemma is a special case of
the standard (local norm) regret bound for matrix multiplicative weights.
Lemma 1.4 (Special case of Theorem 3.1  [1]). For any w0  . . .   wT   if α ≤ 1/(cid:107)M (wt)(cid:107)2 for all

t ≤ T   then(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)T−1(cid:88)

t=0

≤ T−1(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

T−1(cid:88)

M (wt)

(cid:104)Ut  M (wt)(cid:105) + α

(cid:104)Ut  M (wt)(cid:105) · (cid:107)M (wt)(cid:107)2 +

log d

α

.

(4)

t=0

t=0

Now we sketch the analysis of DECREASESPECTRALNORM.
Claim 1.5 (Informal). If w = w0 is mostly good  with (cid:107)M (w0)(cid:107)2 ≥ 100  then DECREASESPEC-
TRALNORM produces mostly good wT with (cid:107)M (wT )(cid:107)2 ≤ 3
Proof sketch. Since M (wt) (cid:22) M (wt+1) by Lemma 1.3  we have (cid:107)M (wt)(cid:107)2 ≤ (cid:107)M (w0)(cid:107)2 for all t 
and hence α = 1/(cid:107)M (w0)(cid:107)2 ≤ 1/(cid:107)M (wt)(cid:107)2 for all t  so w0  . . .   wT and U0  . . .   UT−1 satisfy the
hypotheses of Lemma 1.4. By our choice of α and M (wT ) (cid:22) M (wt) for all t  (4) implies

4(cid:107)M (w)(cid:107)2.

T · (cid:107)M (wT )(cid:107)2 ≤

M (wt)

≤ 2

(cid:104)Ut  M (wt)(cid:105) + (cid:107)M (w0)(cid:107)2 · log d .

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)T−1(cid:88)

t=0

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

T−1(cid:88)

t=0

6

If (cid:104)Ut  M (wt−1)(cid:105) ≥ (cid:107)M (w0)(cid:107)2/3 (cid:29) 1  then DECREASESPECTRALNORM performs down-
weighting  and by Lemma 1.3 and (2) (which we establish rigorously in supplemental mate-
rial)  (cid:104)M (wt)  Ut(cid:105) ≤ 1
3(cid:107)M (w0)(cid:107). Otherwise  by hypothesis (cid:104)M (wt)  Ut(cid:105) =
(cid:104)M (wt−1)  Ut(cid:105) ≤ (cid:107)M (w0)(cid:107)2/3. Using this bound and dividing by T   we obtain (cid:107)M (wT )(cid:107)2 ≤
3 + log d
( 2

T )(cid:107)M (w0)(cid:107)2. Choosing T ≥ 20 log d completes the proof sketch.

3(cid:104)M (wt−1)  Ut(cid:105) ≤ 1

Running time: Our overall algorithm only requires log(nd)O(1) iterations of DECREASESPECTRAL-
NORM  and the latter only requires O(log(d)) iterations of downweighting  so we just have to
implement downweighting in nearly-linear time. We show in supplemental material that this can be
done by avoiding representing any of the matrices Ut explicitly in memory: instead  we maintain only
low-rank sketches of them. This leads to some approximation error in computing the QUE scores 
but we show that approximations to the QUE scores sufﬁce for all arguments above.
For remaining technical details and full proofs  see Sections 5-9 of supplemental materials.

Algorithm 1 DECREASESPECTRALNORM
1: Input: X1  . . .   Xn as in Deﬁnition 1.3  mostly good weight vector w0.
2: For iteration t = 0  . . .   O(log d)  if (cid:107)M (wt)(cid:107)2 ≤ 3

4(cid:107)M (w0)(cid:107)2  output wt and halt. Otherwise 
3(cid:107)M (w0)(cid:107)2  let wt+1 = wt. Else let wt+1 be the output

let Ut as in (4). If (cid:104)Ut  M (wt−1)(cid:105) ≤ 1
of downweighting from Lemma 1.3 with Ut.

3: Output wT .

1.5 Outlier detection: algorithm and experimental results

In this section  we empirically evaluate outlier detection using QUE scoring. QUE scoring can detect
(some kinds of) spectral outliers. We call X ∈ Rd a spectral outlier with respect to a dataset S if
the list of squared projections ((cid:104)X  v1(cid:105)2  . . .  (cid:104)X  vd(cid:105)2) is atypical by comparison to most Y ∈ S 
where X = X − EY ∼S Y and v1  . . .   vd are the eigenvectors of the covariance matrix of S. The
QUE scoring approach to aggregate the list ((cid:104)X  v1(cid:105)2  . . .  (cid:104)X  vd(cid:105)2) into one number carries (at
least) two distinct advantages: ﬁrst  the QUE scores of a dataset can be computed approximately in
nearly-linear time  and second  the QUE scores weigh (cid:104)X  vi(cid:105)2 more heavily for larger λi  while
still incorporating more information than (cid:104)X  v1(cid:105)2 (which is the naive spectral approach). There
may be many other useful ways to go beyond the naive spectral approach to combine the projections
((cid:104)X  v1(cid:105)2  . . .  (cid:104)X  vd(cid:105)2) into a single outlier score – indeed  by varying α QUE scoring already
provides a tuneable range of methods.
Experimental setup: We must work with data containing well-deﬁned and known inliers and outliers
so that we can compare our results to ground-truth. We generate such data sets in three distinct ways 
leading to three main experiments. (In supplemental material we also study some outlier-detection
data sets appearing in prior work [5].)
Synthetic: We create synthetic data sets in 128 dimensions and 103 − 104 samples with an ε-
fraction of inhomogeneous outliers in k directions by sampling from a mixture of k + 1 Gaus-
e1  . . .   ek are standard basis vectors  with C ≈ 1 and σ (cid:28) 1. The outliers are the samples from

2N (−C(cid:112)k/ε · ei  σ2 Id)]  where
sians (1 − ε)N (0  Id) +(cid:80)k
N (±C(cid:112)k/εei  σ2 Id). By varying ε  k and the distribution ε1  . . .   εk of outlier weights  we de-
in the presence of inhomogeneous outliers. We choose the scaling(cid:112)k/ε · ei because then standard
calculations predict that if εi ≈ ε/k the outliers from N (±C(cid:112)k/εei  σ2 Id) will contribute an

2N (C(cid:112)k/ε · ei  σ2 Id) + 1

mostrate in this simpliﬁed model how max-entropy outlier scoring improves on baseline algorithms

eigenvalue greater than 1 to the overall empirical covariance.
Mixed – word embeddings: We create a data set consisting of word embeddings drawn from several
sources. Inliers are the 100-dimensional GloVe embeddings ([21]) of the words in a random ≈ 103
word long section of a novel (we use Sherlock Holmes) and outliers are embeddings of the ﬁrst
paragraphs of k featured Wikipedia articles from May 2019 [27].
Perturbed – images: We create a data set consisting of CIFAR10 images some of which have
artiﬁcially-introduced dead pixels. Inliers are ≈ 4500 random CIFAR images X ∈ {1  . . .   256}1024

i=1 εi[ 1

7

(restricted to the red color channel). Outliers are ≈ 500 random CIFAR images  partitioned into
groups S1  . . .   Sk  such that for each group i a random coordinate pi ∈ {1  . . .   1024} and a random
value ci ∈ {1  . . .   256} is chosen and for each X ∈ Si we set Xpi = ci.
Metric: All the methods we evaluate produce a vector of scores τ1  . . .   τn ∈ R. We use the standard
ROCAUC metric to compare these scores to a ground-truth partition S = Sg ∪ Sb into inlier and
outlier sets. ROCAUC(τ1  . . .   τn  Sb  Sg) = Pri∼Sb j∼Sg (τi ≥ τj) is simply the probability that a
randomly chosen outlier is scored higher than a random inlier.
Baselines: We compare QUE scoring to the following other scoring rules. (cid:96)2: τi = (cid:107)Xi − µ(cid:107) is the
distance of Xi to the empirical mean; top eigenvector naive spectral: τi = (cid:104)Xi − µ  v(cid:105)2 where v is
the top eigenvector of the empirical covariance; k-nearest neighbors (k-NN) [22  5] and local outlier
factor (LOF) [4  5] methods: τi is a function of the distances to its k nearest neighbors; isolation forest
and elliptic envelope: standard outlier detection methods as implemented in scikit-learn [23  18  20].
Whitening: Scoring methods based on the projection of data points Xi onto large eigenvectors
of the empirical covariance work best when those eigenvectors correspond to directions in which
many outliers lie. In particular  if Σg  the covariance of Sg  itself has large eigenvalues then such
spectral methods perform poorly. We assume access to a whitening transformation W ∈ Rd×d  which
captures a small amount of prior knowledge about the distribution of inliers Sg. For best performance
W should approximate W ∗ = (Σg)−1/2 since W ∗Xi form an isotropic set of vectors. Of course  to
compute W ∗ exactly would require knowing which points are inliers  but we ﬁnd that relatively naive
approximations sufﬁce. In particular  if a clean dataset Y1  . . .   Ym whose distribution is similar to
the distribution of inliers is available  its empirical covariance can be used to ﬁnd a good whitening
transformation W . In our synthetic data we use W = Id. In our word embeddings experiment  we
obtain W using the empirical covariance of the embedding of another random section of Sherlock
Holmes. In our CIFAR-10 experiment  we obtain W from the empirical covariance of a fresh sample
of ≈ 5000 randomly chosen images from CIFAR-10.

Algorithm 2 QUE-Scoring for Outlier Detection
1: Input: dataset X1  . . .   Xn ∈ Rd  optional whitening transformation W ∈ Rd×d  scalar α > 0.
2: Let X(cid:48)
3: For i ≤ n  let τi = (X(cid:48)
Note on α: in both synthetic and real data we ﬁnd that α = 4 is a good rule-of-thumb choice 
consistently resulting in improved scores over baseline methods.

(cid:80)n
i=1 X(cid:48)
i)/ Tr exp(αΣ/(cid:107)Σ(cid:107)2). Return τ1  . . .   τn.

i = W Xi be whitened data  µ = 1
n

(cid:80)d
i=1(X(cid:48)

i − µ)(X(cid:48)

i − µ)(cid:62).

(cid:62)

i

exp(αΣ/(cid:107)Σ(cid:107)2)X(cid:48)

i and Σ = 1
n

High-dimensional scaling: Implementing Algorithm 2 by explicitly forming the matrix Σ and
performing a singular value decomposition (SVD) to compute exp(αΣ) is feasible on relatively low-
dimensional data (d ≈ 100). See supplementary material for discussion and results of a nearly-linear
time implementation.

8

(a) synthetic

(b) whitened CIFAR-10

(c) whitened word embeddings

(d) synthetic

(e) whitened CIFAR-10

(f) whitened word embeddings

(g) synthetic

(h) whitened CIFAR-10

(i) whitened word embeddings

Figure 1: (a-f): We plot the difference between ROCAUC performance of QUE and naive spectral
(a-c)  (cid:96)2 scoring (d-f) on all three data sets  as α varies. Error bars represent one empirical standard
deviation in 20 trials. Note that in all three cases the mean improvement in ROCAUC score given by
QUE is at least one standard deviation above 0 for a wide range of α. Observe also that in synthetic
data (which most closely parallels theory) the optimal α decreases with increasing number of outlier
directions  in accord with the need to ﬁnd a higher-entropy solution to (1). (g-i) We plot ROCAUC
scores of QUE (with α = 4) and a variety of other methods as the number of outlier directions
increases. Error bars represent one standard deviation over 3 − 4 trials. Number of trials is small due
to large running time requirements of Scikit-learn methods IsolationForest and EllipticEnvelope. The
methods "lof" and "knn" are based on nearest-neighbor distances [5]. All except spectral methods
perform poorly on synthetic data; as k increases the performance gap between QUE and naive
spectral scoring grows. In all plots ε = 0.2. Experiments were generated on a quad-core 2.6Ghz
machine with 16GB RAM and an NVIDIA P100 GPU.

References
[1] Zeyuan Allen-Zhu  Zhenyu Liao  and Lorenzo Orecchia. Spectral sparsiﬁcation and regret
minimization beyond matrix multiplicative updates. In Proceedings of the forty-seventh annual
ACM symposium on Theory of computing  pages 237–245. ACM  2015.

[2] Frank J Anscombe. Rejection of outliers. Technometrics  2(2):123–146  1960.

[3] Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a

meta-algorithm and applications. Theory of Computing  8(1):121–164  2012.

[4] MM Breunig  HP Kriegel  R Ng  and J Sander. Efﬁcient algorithms for mining outliers from
large data sets. Proceedings of the ACM international conference on management of data
(SIGMOD)  pages 93–104  2000.

9

[5] Guilherme O Campos  Arthur Zimek  Jörg Sander  Ricardo JGB Campello  Barbora Micenková 
Erich Schubert  Ira Assent  and Michael E Houle. On the evaluation of unsupervised outlier
detection: measures  datasets  and an empirical study. Data Mining and Knowledge Discovery 
30(4):891–927  2016.

[6] Yu Cheng  Ilias Diakonikolas  and Rong Ge. High-dimensional robust mean estimation in
nearly-linear time. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms  pages 2755–2771. SIAM  2019.

[7] Yu Cheng  Ilias Diakonikolas  Rong Ge  and David Woodruff. Faster algorithms for high-
dimensional robust covariance estimation. In Proceedings of the 32nd Annual Conference on
Learning Theory (COLT 2019).

[8] Ilias Diakonikolas  Gautam Kamath  Daniel Kane  Jerry Li  Ankur Moitra  and Alistair Stewart.
Robust estimators in high-dimensions without the computational intractability. SIAM Journal
on Computing  48(2):742–864  2019.

[9] Ilias Diakonikolas  Gautam Kamath  Daniel M Kane  Jerry Li  Ankur Moitra  and Alistair
In Proceedings of the 34th
Stewart. Being robust (in high dimensions) can be practical.
International Conference on Machine Learning-Volume 70  pages 999–1008. JMLR. org  2017.

[10] Ilias Diakonikolas  Daniel M Kane  and Alistair Stewart. Statistical query lower bounds for
robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th
Annual Symposium on Foundations of Computer Science (FOCS)  pages 73–84. IEEE  2017.

[11] Douglas M Hawkins. Identiﬁcation of outliers  volume 11. Springer  1980.

[12] Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics  pages

492–518. Springer  1992.

[13] Edwin M Knorr and Raymond T Ng. A uniﬁed notion of outliers: Properties and computation.

In KDD  volume 97  pages 219–222  1997.

[14] Edwin M Knox and Raymond T Ng. Algorithms for mining distancebased outliers in large
In Proceedings of the international conference on very large data bases  pages

datasets.
392–403. Citeseer  1998.

[15] Kevin A Lai  Anup B Rao  and Santosh Vempala. Agnostic estimation of mean and covariance.
In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)  pages
665–674. IEEE  2016.

[16] Guillaume Lecué and Jules Depersin. Robust subgaussian estimation of a mean vector in nearly

linear time. arXiv preprint arXiv:1906.03058  2019.

[17] Jerry Zheng Li. Principled approaches to robust machine learning and beyond. PhD thesis 

Massachusetts Institute of Technology  2018.

[18] Fei Tony Liu  Kai Ming Ting  and Zhi-Hua Zhou. Isolation forest. In 2008 Eighth IEEE

International Conference on Data Mining  pages 413–422. IEEE  2008.

[19] Gábor Lugosi  Shahar Mendelson  et al. Sub-gaussian estimators of the mean of a random

vector. The Annals of Statistics  47(2):783–794  2019.

[20] Fabian Pedregosa  Gaël Varoquaux  Alexandre Gramfort  Vincent Michel  Bertrand Thirion 
Olivier Grisel  Mathieu Blondel  Peter Prettenhofer  Ron Weiss  Vincent Dubourg  et al. Scikit-
learn: Machine learning in python. Journal of machine learning research  12(Oct):2825–2830 
2011.

[21] Jeffrey Pennington  Richard Socher  and Christopher D. Manning. Glove: Global vectors for
word representation. In Empirical Methods in Natural Language Processing (EMNLP)  pages
1532–1543  2014.

[22] S Ramaswamy  R Rastogi  and K Shim. Efﬁcient algorithms for mining outliers from large data
sets. Proceedings of the ACM international conference on management of data (SIGMOD) 
pages 427–438  2000.

10

[23] Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance

determinant estimator. Technometrics  41(3):212–223  1999.

[24] Jacob Steinhardt. Robust Learning: Information Theory and Algorithms. PhD thesis  Stanford

University  2018.

[25] John W Tukey. A survey of sampling from contaminated distributions. Contributions to

probability and statistics  pages 448–485  1960.

[26] John W Tukey. Mathematics and the picturing of data. In Proceedings of the International

Congress of Mathematicians  Vancouver  1975  volume 2  pages 523–531  1975.

[27] https://en.wikipedia.org/wiki/Wikipedia:Today’s_featured_article/May_2019.

May 2019. Wikimedia Foundation.

11

,Yihe Dong
Samuel Hopkins
Jerry Li