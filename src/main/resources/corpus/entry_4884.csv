2019,Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data,Continuous-time Bayesian Networks (CTBNs) represent a compact yet powerful framework for understanding multivariate time-series data. Given complete data  parameters and structure can be estimated efficiently in closed-form. However  if data is incomplete  the latent states of the CTBN have to be estimated by laboriously simulating the intractable dynamics of the assumed CTBN. This is a problem  especially for structure learning tasks  where this has to be done for each element of a super-exponentially growing set of possible structures. In order to circumvent this notorious bottleneck  we develop a novel gradient-based approach to structure learning. Instead of sampling and scoring all possible structures individually  we assume the generator of the CTBN to be composed as a mixture of generators stemming from different structures. In this framework  structure learning can be performed via a gradient-based optimization of mixture weights. We combine this approach with a new variational method that allows for a closed-form calculation of this mixture marginal likelihood.
We show the scalability of our method by learning structures of previously inaccessible sizes from synthetic and real-world data.,Scalable Structure Learning of Continuous-Time

Bayesian Networks from Incomplete Data

Dominik Linzner1 Michael Schmidt1 Heinz Koeppl1 2

1Department of Electrical Engineering and Information Technology

{dominik.linzner  michael.schmidt  heinz.koeppl}@bcs.tu-darmstadt.de

2Department of Biology

Technische Universität Darmstadt

Abstract

Continuous-time Bayesian Networks (CTBNs) represent a compact yet powerful
framework for understanding multivariate time-series data. Given complete data 
parameters and structure can be estimated efﬁciently in closed-form. However  if
data is incomplete  the latent states of the CTBN have to be estimated by laboriously
simulating the intractable dynamics of the assumed CTBN. This is a problem 
especially for structure learning tasks  where this has to be done for each element
of a super-exponentially growing set of possible structures. In order to circumvent
this notorious bottleneck  we develop a novel gradient-based approach to structure
learning. Instead of sampling and scoring all possible structures individually  we
assume the generator of the CTBN to be composed as a mixture of generators
stemming from different structures. In this framework  structure learning can be
performed via a gradient-based optimization of mixture weights. We combine this
approach with a new variational method that allows for a closed-form calculation
of this mixture marginal likelihood. We show the scalability of our method by
learning structures of previously inaccessible sizes from synthetic and real-world
data.

1

Introduction

Learning correlative or causative dependencies in multivariate data is a fundamental problem in
science and has application across many disciplines such as natural and social sciences  ﬁnance and
engineering [1  20]. Most statistical approaches consider the case of snapshot or static data  where
one assumes that the data is drawn from an unknown probability distribution. For that case several
methods for learning the directed or undirected dependency structure have been proposed  e.g.  the PC
algorithm [21  13] or the graphical LASSO [8  12]  respectively. Causality for such models can only
partially be recovered up to an equivalence class that relates to the preservation of v-structures [21] in
the graphical model corresponding to the distribution. If longitudinal and especially temporal data
is available  structure learning methods need to exploit the temporal ordering of cause and effect
that is implicit in the data for determining the causal dependency structure. One assumes that the
data are drawn from an unknown stochastic process. Classical approaches such as Granger causality
or transfer entropy methods usually require large sample sizes [23]. Dynamic Bayesian networks
offer an appealing framework to formulate structure learning for temporal data within the graphical
model framework [10]. The fact that the time granularity of the data can often be very different
from the actual granularity of the underlying process motivates the extension to continuous-time
Bayesian networks (CTBN) [14]  where no time granularity of the unknown process has to be
assumed. Learning the structure within the CTBN framework involves a combinatorial search over
structures and is hence generally limited to low-dimensional problems even if one considers variational
approaches [11] and/or greedy hill-climbing strategies in structure space [15  16]. Reminiscent of

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

optimization-based approaches such as graphical LASSO  where structure scoring is circumvented
by performing gradient descent on the edge coefﬁcients of the structure under a sparsity constraint 
we here propose the ﬁrst gradient-based scheme for learning the structure of CTBNs.

2 Background

2.1 Continuous-time Bayesian Networks
We consider continuous-time Markov chains (CTMCs) {X(t)}t≥0 taking values in a countable state-
space S. A time-homogeneous Markov chain evolves according to an intensity matrix R : S×S → R 
whose elements are denoted by R(s  s(cid:48))  where s  s(cid:48) ∈ S. A continuous-time Bayesian network [14]
is deﬁned as an N-component process over a factorized state-space S = X1 × ··· × XN evolving
i ∈ Xi  we will drop the states’ component index i  if
jointly as a CTMC. For local states xi  x(cid:48)
evident by the context and no ambiguity arises. We impose a directed graph structure G = (V  E) 
encoding the relationship among the components V ≡ {V1  . . .   VN}  which we refer to as nodes.
These are connected via an edge set E ⊆ V × V . This quantity is the structure  which we will later
learn. The state of each component is denoted by Xi(t) assuming values in Xi  which depends only
on the states of a subset of nodes  called the parent set parG(i) ≡ {j | (j  i) ∈ E}. Conversely 
we deﬁne the child set chG(i) ≡ {j |
(i  j) ∈ E}. The dynamics of a local state Xi(t) are
described as a Markov process conditioned on the current state of all its parents Un(t) taking values
in Ui ≡ {Xj | j ∈ parG(i)}. They can then be expressed by means of the conditional intensity
matrices (CIMs) Ri : Xi × Xi × Ui → R  where ui ≡ (u1  . . . uL) ∈ Ui denotes the current state of
the parents (L = |parG(i)|). The CIMs are the generators of the dynamics of a CTBN. Speciﬁcally 
we can express the probability of ﬁnding node i in state x(cid:48) after some small time-step h  given that it
was in state x at time t with x  x(cid:48) ∈ Xi as

p(Xi(t + h) = x(cid:48) | Xi(t) = x  Ui(t) = u) = δx x(cid:48) + hRi(x  x(cid:48) | u) + o(h) 

limh→0 o(h)/h = 0. It holds that Ri(x  x | u) = −(cid:80)

where Ri(x  x(cid:48) | u) is the rate the transition x → x(cid:48) given the parents’ state u ∈ Ui and δx x(cid:48)
being the Kronecker-delta. We further make use of the small o(h) notation  which is deﬁned via
x(cid:48)(cid:54)=x Ri(x  x(cid:48) | u). The CIMs are connected

to the joint intensity matrix R of the CTMC via amalgamation – see  for example [14].

2.2 Structure Learning for CTBNs

Complete data. The likelihood of a CTBN can be expressed in terms of its sufﬁcient statis-
tics [15]  Mi(x  x(cid:48) | u)  which denotes the number of transitions of node i from state x to
x(cid:48) and Ti(x | u)  which denotes the amount of time node i spend in state x.
In order to
avoid clutter  we introduce the sets M ≡ {Mi(x  x(cid:48) | u) | i ∈ {1  . . .   N}  x  x(cid:48) ∈ X   u ∈ U} and
T ≡ {Ti(x | u) | i ∈ {1  . . .   N}  x ∈ X   u ∈ U}. The likelihood then takes the form

 (cid:88)

x x(cid:48)(cid:54)=x u

N(cid:89)

i=1

 .

(1)

p(M T | G  R) =

exp

Mi(x  x(cid:48) | u) ln Ri(x  x(cid:48) | u) − Ti(x | u)Ri(x  x(cid:48) | u)

In [15] and similarly in [22] it was shown that a marginal likelihood for the structure can be calculated
in closed form  when assuming a gamma prior over the rates Ri(x  x(cid:48) | u) ∼ Gam(αi(x  x(cid:48) |
u)  βi(x(cid:48) | u)). In this case  the marginal log-likelihood of a structure takes the form

ln p(M T | G  α  β) ∝ N(cid:88)

(cid:88)

(cid:8)ln Γ (¯αi(x  x(cid:48) | u)) − ¯αi(x  x(cid:48) | u) ln ¯βi(x | u)(cid:9)  

(2)

i=1

u x x(cid:48)(cid:54)=x

with ¯αi(x  x(cid:48) | u) ≡ Mi(x  x(cid:48) | u) + αi(x  x(cid:48) | u) and ¯βi(x | u) ≡ Ti(x | u) + βi(x | u). Structure
learning in previous works [16  22  11] is then performed by iterating over possible structures and
scoring them using the marginal likelihood. The best scoring structure is then the maximum-a-
posteriori estimate of the structure.
Incomplete data. In many cases  the sufﬁcient statistics of a CTBN cannot be provided. Instead 
data comes in the form of noisy state observations at some points in time. In the following  we

2

will assume data is provided in form of Ns samples D ≡ (cid:8)(tk  yk) | k ∈ {1  . . .   Ns}(cid:9)  where

yk is some  possibly noisy  measurement of the latent-state generated by some observation model
yk ∼ p(Y = yk | X(tk) = s) at time tk. This data is incomplete  as the sufﬁcient statistics of
the underlying latent process have to be estimated before model identiﬁcation can be performed.
In [16]  an expectation-maximization for structure learning (SEM) was introduced  in which  given a
proposal CTBN  sufﬁcient statistics were ﬁrst estimated by exact inference  the CTBN parameters
were optimized given those expected sufﬁcient-statistics and  subsequently  structures where scored
via (1). Similarly  in [11] expected sufﬁcient-statistics were estimated via variational inference under
marginal (parameter-free) dynamics and structures were then scored via (2).
The problem of structure learning from incomplete data has two distinct bottlenecks  (i) Latent
state estimation (scales exponentially in the number of nodes) (ii) Structure identiﬁcation (scales
super-exponentially in the number of nodes). While bottleneck (i) has been tackled in many ways [4 
5  19  11]  existing approaches [16  11] employ a combinatorial search over structures  thus an
efﬁcient solution for bottleneck (ii) is still outstanding.
Our approach. We will employ a similar strategy as mentioned above in this manuscript. However 
statistics are estimated under a marginal CTBN that no longer depends on rate parameters or a discrete
structure. Instead  statistics are estimated given a mixture of different parent-sets. Thus  instead of
blindly iterating over possible structures in a hill-climbing procedure  we can update our distribution
over structures by a gradient step. This allows us to directly converge into regions of high-probability.
Further  in combination of this gradient-based approach with a high-order variational method  we
can perform estimation of the expected sufﬁcient-statistics in large systems. These two features
combined  enable us to perform structure learning in large systems. An implementation of our method
is available via Git1.

3 Likelihood of CTBNs Under a Mixture of CIMs
Complete data. In the following  we consider a CTBN over some 2over-complete graph G. In
practice  this graph may be derived from data as prior knowledge. In the absence of prior knowledge 
we will choose the full graph. We want to represent its CIMs Ri(x  x(cid:48) | u)  here for node i  as mixture
of CIMs of smaller support and write by using the power-set P(·) (set of all possible subsets)

Ri(x  x(cid:48) | u) =

πi(m)ri(x  x(cid:48) | um) ≡ Eπ

i [ri(x  x(cid:48) | um)] 

m∈P(parG (i))

where um denotes the projection of the full parent-state u on the subset m  i.e. f (um) =(cid:80)

f (u) 
m∈P(parG (i)) πi(m)f (θm). The mixture-weights are given by a
and the expectation Eπ
distribution πi ∈ ∆i with ∆i being the |P(parG(i))|−dimensional probability simplex. Correspond-
ing edge probabilities of the graph can be computed via marginalization. The probability that an edge
eij ∈ E exists is then

u/um

(3)

(cid:88)
i [f (θm)] =(cid:80)

(cid:88)

p(eij = 1) =

m∈P(parG (j))

πj(m)1(i ∈ m) 

(4)

with 1(·) being the indicator function. In order to arrive at a marginal score for the mixture we
insert (3) into (1) and apply Jensen’s inequality Eπ
i [r]). This yields a lower-bound
to the mixture likelihood

i [ln (r)] ≤ ln (Eπ

i [Mi(x x(cid:48)|um) ln ri(x x(cid:48)|um)−Ti(x|um)ri(x x(cid:48)|um)].

eEπ

p(M T | π  r) ≥ N(cid:89)

(cid:89)

i=1

x x(cid:48)(cid:54)=x um

For details on this derivation  we refer to the supplementary material A.1. Note that Jensens inequality 
which only provides a poor approximation in general  improves with increasing concentration
of probability mass and becomes exact for degenerate distributions. For the task of selecting
a CTBN with a speciﬁc parent-set  it is useful to marginalize over the rate parameters r of the
CTBNs. This allows for a direct estimation of the parent-set  without ﬁrst estimating the rates. This

1https://git.rwth-aachen.de/bcs/ssl-ctbn
2An over-complete graph has more edges than the underlying true graph  which generated the data.

3

marginal likelihood can be computed under the assumption of independent gamma prior distributions
ri(x  x(cid:48) | um) ∼ Gam(αi(x  x(cid:48) | um)  βi(x(cid:48) | um)) over the rates. The marginal likelihood lower-
bound can then be computed analytically. Under the assumption of independent Dirichlet priors
πi ∼ Dir(πi | ci)  with concentration parameters ci we arrive at a lower-bound to the marginal
log-posterior of the mixture weights π

Fi[M T   π] + ln Z 

(cid:8)ln Γ (¯αi(x  x(cid:48) | um)) − ¯αi(x  x(cid:48) | um) ln ¯βi(x | um)(cid:9)+ln Dir(πi | ci) 

(5)

ln p(π | M T   α  β) ≥(cid:88)
Fi[M T   π] ≡ (cid:88)

i

m um x x(cid:48)(cid:54)=x

with the updated posterior parameters ¯αi(x  x(cid:48) | um) ≡ πi(m)Mi(x  x(cid:48) | um) + αi(x  x(cid:48) | um) and
¯βi(x | um) ≡ πi(m)Ti(x | um) + βi(x | um). For details  we refer to the supplementary material
A.2. The constant log-partition function ln Z can be ignored in the following analysis. Because (5)
decomposes into a sum of node-wise terms  the maximum-a-posterior estimate of the mixture weights
of node i can be calculated as solution of the following optimization problem:

π∗
i = arg max
πi∈∆i

{Fi[M T   π]} .

(6)

By construction  learning the mixture weights π of the CIMs  corresponds to learning a distribution
over parent-sets for each node. We thus re-expressed the problem of structure learning to an estimation
of π. Further  we note that for any degenerate π  (5) coincides with the exact structure score (2).
Incomplete data. In the case of incomplete noisy data D  the likelihood of the CTBN does no
longer decompose into node-wise terms. Instead  the likelihood is one of the full amalgamated
CTMC [16]. In order to tackle this problem  approximation methods through sampling [7  6  19] 
or variational approaches [4  5] have been investigated. These  however  either fail to treat high-
dimensional spaces because of sample sparsity  are unsatisfactory in terms of accuracy  or provide
only an uncontrolled approximation. Our method is based on a variational approximation  e.g.
weak coupling expansion [11]. Under this approximation  we recover by the same calculation an
approximate likelihood of the same form as (1)  where the sufﬁcient statistics Mi(x  x(cid:48) | u) and
Ti(x | u) are  however  replaced by their expectation Eq [Mi(x  x(cid:48) | u)] and Eq [Ti(x | u)] under a
variational distribution q  – for details we refer to the supplementary B.1. Subsequently  also our
optimization objective Fi[M T   π] becomes dependent on the variational distribution Fi[D  π  q]. In
the following chapter  we will develop an Expectation-Maximization (EM)-algorithm that iteratively
estimates the expected sufﬁcient-statistics given the mixture-weights and subsequently optimizes
those mixture-weights given the expected sufﬁcient-statistics.

4

Incomplete data: Expected Sufﬁcient Statistics Under a Mixture of CIMs

Short review of the foundational method. In [11]  the exact posterior over paths of a CTBN given
incomplete data D  is approximated by a path measure q(X[0 T ]) of a variational time-inhomogeneous
Markov process via a higher order variational inference method. For a CTBN  this path measure is
fully described by its node-wise marginals qi(x(cid:48)  x  u; t) ≡ qi(Xi(t + h) = x(cid:48)  Xi(t) = x  Ui(t) =
u; t). From it  one can compute the marginal probability qi(x; t) of node i to be in state x  the
marginal probability of the parents qi(Ui(t) = u; t) ≡ qu
i (t) and the marginal transition probability
τi(x  x(cid:48)  u; t) ≡ limh→0 qi(x(cid:48)  x  u; t)/h
for x (cid:54)= x(cid:48). The exact form of the expected statistics
were calculated to be

Eq [Ti(x | u)] ≡

i (t)  Eq [Mi(x  x(cid:48) | u)] ≡

dt τi(x  x(cid:48)  u; t).

0

dt qi(x; t)qu

(7)
In the following  we will use the short-hand Eq [M] and Eq [T ] to denote the sets of expected
sufﬁcient-statistics. We note  that the variational distribution q has the support of the full over-
complete parent-set parG(i). Via marginalization of qi(x(cid:48)  x  u; t)  the marginal probability and the
marginal transition probability can be shown to be connected via the relation
[τi(x (cid:48) x  u; t) − τi(x  x(cid:48)  u; t)] .

(cid:88)

qi(x; t) =

(8)

(cid:90) T

0

(cid:90) T

d
dt

x(cid:48)(cid:54)=x u

4

and data D.

repeat

2: repeat
3:
4:
5:
6:

for all i ∈ {1  . . .   N} do
for all (yk  tk) ∈ D do

Update ρi(t) by backward propagation from tk to tk−1 using (10) fulﬁlling the jump
conditons (12).

end for

end for
Update qi(t) by forward propagation using (10) given ρi(t).

7:
8:
9:
10:
11:
12: until Convergence of F[D  π  q]
13: Output: Set of expected sufﬁcient statistics Eq[M] and Eq[T ].

until Convergence
Compute expected sufﬁcient statistics using (7) and (11) from qi(t) and ρi(t).

Application to our setting. As discussed in the last section  the objective function in the incomplete
data case has the same form as (5)

Fi[D  π  q] ≡ (cid:88)

(cid:8)ln Γ (¯αq

m um x x(cid:48)(cid:54)=x

i (x  x(cid:48) | um)) − ¯αq

i (x  x(cid:48) | um) ln ¯βq

i (x | um)(cid:9)+ln Dir(πi | ci) 

(9)

now

and
however 
i (x | um) ≡ πi(m)Eq[Ti(x | um)] + βi(x | um).
¯βq
In order to arrive at approximation to the
expected sufﬁcient statistics in our case  we have to maximize (9) with respect to q  while fulﬁlling
the constraint (8). The corresponding Lagrangian becomes

i (x  x(cid:48) | um) ≡ πi(m)Eq[Mi(x  x(cid:48) | um)] + αi(x  x(cid:48) | um)
¯αq

with

L[D  π  q  λ] =

Fi[D  π  q] − (cid:88)

N(cid:88)

(cid:90) T

i=1

x x(cid:48)(cid:54)=x u

0

dt λi(x; t)

qi(x; t) − [τi(x (cid:48) x  u; t) − τi(x  x(cid:48)  u; t)]

(cid:26) d
(cid:113) 2π

dt

(cid:0) z

(cid:1)z

+ O(cid:0) 1

(cid:27)  
(cid:1)  which becomes exact asymp-

with Lagrange-multipliers λi(x; t). In order to derive Euler-Lagrange equations  we employ Stirlings-
approximation for the gamma function Γ(z) =
totically. In our case  Stirlings-approximation is valid if ¯α (cid:29) 1. We thereby assumed that either
enough data has been recorded  or a sufﬁciently strong prior α. Finally  we recover the approximate
forward- and backward-equations of the mixture CTBNs as the stationary point of the Lagrangian
Euler-Lagrange equations

z

z

e

ρi(t) = ˜Ωπ

i (t)ρi(t) 

d
dt

qi(t) = qi(t)Ωπ

i (t) 

(10)

Algorithm 1 Stationary points of Euler–Lagrange equation
1: Input: Initial trajectories qi(x; t)  boundary conditions q(x; 0) and ρ(x; T )  mixture weights π

d
dt
with effective rate matrices

i (x  x(cid:48); t) ≡ Eu
Ωπ
i (x  x(cid:48); t) ≡ (1 − δx x(cid:48))Eu
˜Ωπ

i (x  x(cid:48) | u)

i

(cid:104) ˜Rπ

(cid:105)
ρi(x; t)
i (x  x(cid:48) | u)

(cid:105) ρi(x(cid:48); t)
(cid:104) ˜Rπ
i [f (u)] =(cid:80)
(cid:21)

i

 

˜Rπ

i (x  x(cid:48) | um)
i (x | um)
¯βq

(cid:20) ¯αq

i (x  x(cid:48) | u) ≡ Eπ
Rπ

i

with ρi(x; t) ≡ exp(−λi(x; t)) and Ψi(x; t) as given in the supplementary material B.2. Further we
have introduced the shorthand Eu
and deﬁned the posterior expected rates

u f (u)qu

i (t)

+ δx x(cid:48) {Eu

i [Rπ

i (x  x(cid:48) | u)] + Ψi(x; t)}  

(cid:18) ¯αq

i (x  x(cid:48) | um)
i (x | um)
¯βq

(cid:19)πi(m)

 

i (x  x(cid:48) | u) ≡(cid:89)

m

5

Algorithm 2 Gradient-based Structure Learning
1: Input: Initial trajectories qi(x; t)  boundary conditions qi(x; 0) and ρi(x; T )  initial mixture

weights π(0)  data D and iterator n = 0

Compute expected sufﬁcient statistics Eq[M] and Eq[T ] given π(n) using Algorithm 1.
for all i ∈ {1  . . .   N} do

2: repeat
3:
4:
5:
6:
7: until Convergence of F[D  π  q]
8: Output: Maximum-a-posteriori mixture weights π(n)

Maximize (6) with respect to πi  set maximizer π(n+1)

end for

i

= π∗

i and n → n + 1.

which take the form of an arithmetic and geometric mean  respectively. For the variational transition-
matrix we ﬁnd the algebraic relationship

τi(x  x(cid:48)  u; t) = qi(x; t)qu

i (t) ˜Rπ

i (x  x(cid:48) | u)

.

(11)

ρi(x(cid:48); t)
ρi(x; t)

Because 
order
an observation model.

p(Y = yk | X(tk) = s) =(cid:81)

the derivation is quite lengthy  we refer to supplementary B.2 for details.

In
to incorporate noisy observations into the CTBN dynamics  we need to specify
the data likelihood factorizes
i | Xi(tk) = x)  allowing us to condition on the data by

In the following we assume that
i pi(Yi = yk

enforcing jump conditions

t→tk− ρi(x; t) = lim
lim
t→tk+

pi(Yi = yk

i | Xi(tk) = x)ρi(x; t).

(12)

The converged solutions of the ODE system can then be used to compute the sufﬁcient statistics
via (7). For a full derivation  we refer to the supplementary material B.2.
We note that in the limiting case of a degenerate mixture distribution π  this set of equations reduces
to the marginal dynamics for CTBNs proposed in [11]. The set of ODEs can be solved iteratively
as a ﬁxed-point procedure in the same manner as in previous works [17  4] (see Algorithm 1) in a
forward-backward procedure.
Exhaustive structure search. As we are now able to calculate expected-sufﬁcient statistics given
mixture weights π  we can design an EM-algorithm for structure learning. For this iteratively optimize
π given the expected sufﬁcient statistics  which we subsequently re-calculate. The EM-algorithm
is summarized in Algorithm 2. In contrast to the exact EM-procedure [16]  we preserve structure
modularity. We can thus optimize the parent-set of each node independently. This already provides a
huge boost in performance  as in our case the search space scales exponentially in the components 
instead of super-exponentially. In the paragraph "Greedy structure search"  we will demonstrate how
to further reduce complexity to a polynomial scaling  while preserving most prediction accuracy.
Restricted exhaustive search. In many cases  especially for applications in molecular biology 
comprehensive 3databases of putative interactions are available and can be used to construct over-
complete yet not fully connected prior networks G0 of reported gene and protein interactions. In this
case we can restrict the search space by excluding possible non-reported parents for every node i 
parG(i) = parG0 (i)  allowing for structure learning of large networks.
Greedy structure search. Although we have derived a gradient-based scheme for exhaustive search 
the number of possible mixture components still equals the number of all possible parent-sets.
However  in many applications  it is reasonable to assume the number of parents to be limited  which
corresponds to a sparsity assumption. For this reason  greedy schemes for structure learning have been
proposed in previous works [16]. Here  candidate parent-sets were limited to have at most K parents 
in which case  the number of candidate graphs only grows polynomially in the number of nodes. In
order to incorporate a similar scheme in our method  we have to perform an additional approximation
to the set of equations (10). The problem lies in the expectation step (Algorithm 1)  as expectation
is performed with respect to the full over-complete graph. In order to calculate expectations of the
i (x  x(cid:48) | u)]  we have to consider the over-complete set of parenting nodes
geometric mean Eu
i (x  x(cid:48) | u)] only
i (t) for each node i. However  for the calculation of the arithmetic mean Eu
qu

i [ ˜Rπ

i [Rπ

3e.g. https://string-db.org/ or https://www.ebi.ac.uk/intact/

6

Figure 1: a) and b) AUROC and AUPR  respectively  for complete observations for different numbers
of trajectories. Learning is performed via the graph-score (2) (blue) and gradient-based optimization
of the marginal mixture likelihood (5) (red-dashed). c) Relative deviation of approximate marginal
mixture likelihood (5) from the exact marginal likelihood  computed via numerical integration  for
mixtures of different entropies given different amounts of trajectories (legend). Conﬁdence intervals
are given by 75% and 25% percentiles.

parent-sets restricted to the considered sub-graphs have to be considered  due to linearity. For this
reason  we approximate the geometric mean by the arithmetic mean ˜Rπ
i   corresponding to the
i [x]) + O(Var[x])  which  as before  becomes more valid for
ﬁrst-order expansion Eπ
more concentrated πi and is exact if πi is degenerate.

i [ln(x)] = ln(Eπ

i ≈ Rπ

5 Experiments

We demonstrate the effectiveness of our method on synthetic and two real-world data sets. For
all experiments  we consider a ﬁxed set of hyper-parameters. We set the Dirichlet concentration
parameter ci = 0.9 for all i ∈ {1  . . .   N}. Further  we assume a prior for the generators  which is
uninformative on the structure αi(x  x(cid:48) | u) = 5 and βi(x | u) = 10  for all x  x(cid:48) ∈ Xi  u ∈ Ui. For
the optimization step in Algorithm 2  we use standard Matlab implementation of the interior-point
method with 100 random restarts. This is feasible  as the Jacobian of (9) can be calculated analytically.

5.1 Synthetic Data

(cid:16)

2 + 1

(cid:17)

2 tanh

γx(cid:80)

In this experiment  we consider synthetic data generated by random graphs with a ﬂat degree
distribution  truncated at degree two  i.e. each nodes has a maximal number of two parents. We
restrict the state-space of each node to be binary X = {−1  1}. The generators of each node are
chosen such that they undergo Glauber-dynamics [9] Ri(x  ¯x | u) = 1
 
j∈parG (i) uj
which is a popular model for benchmarking  also in CTBN literature [4]. The parameter γ denotes the
coupling-strength of node j to i. With increasing γ the dynamics of the network become increasingly
deterministic  converging to a logical-model for γ → ∞. In order to avoid violating the weak-
coupling assumption [11]  underlying our method  we choose γ = 0.6. We generated a varying
number of trajectories with each containing 10 transitions. In order to have a fair evaluation  we
generate data from thirty random graphs among ﬁve nodes  as described above. By computing the
edge probabilities p(eij = 1) via (4)  we can evaluate the performance of our method as an edge-
classiﬁer by computing the receiver-operator characteristic curve (ROC) and the precision-recall curve
(PR) and their area-under-curve (AUROC) and (AUPR). For an unbiased classiﬁer  both quantities
have to approach 1  for increasing amounts of data.
Complete data. In this experiment  we investigate the viability of using the marginal mixture
likelihood lower-bound as in (5) given the complete data in the form of the sufﬁcient statistics M
and T . In Figure 1 we compare the AUROCs a) and AUPRs b) achieved in an edge classiﬁcation task
using exhaustive scoring of the exact marginal likelihood (2) as in [15] (blue) and gradient ascend in
π of the mixture marginal likelihood lower-bound (red-dashed) as in (5). In Figure 1 c) we show via
numerical integration  that the marginal mixture likelihood lower-bound approaches the exact one (2)
for decreasing entropy of π and increasing number of trajectories. Small negative deviations are due

7

00.511.52Entropy()02468101214Relative Deviation of lower bound[%]101102Number of Trajectories00.20.40.60.81AUROCexactgradient101102Number of Trajectories00.20.40.60.81AUPRexactgradientFigure 2: a) AUROCs and b) AUPRs for varying number of trajectories. c) ROC and d) PR curve
for 40 trajectories. In all plots (red) denotes the exhaustive  (blue/dashed) the greedy-algorithm. e)
ROC-curve f) PR-curve for different initial π(0)  where (red) denotes heuristic and (grey/dashed)
random. Conﬁdence intervals are given by 75% and 25% percentiles of the results from 30 random
graphs  generated as explained in the main text.

to the limited accuracy of numerical integration. Additional synthetic experiments investigating the
effect of different concentration parameters c can be found in the supplementary C.1
Incomplete data. Next  we test our method for network inference from incomplete data. Noisy
incomplete observations were generated by measuring the state at Ns = 10 uniformly random time-
points and adding Gaussian noise with zero mean and variance 0.2. Because of the expectation-step
in Algorithm 1  is only approximate [11]  we do not expect a perfect classiﬁer in this experiment. We
compare the exhaustive search  with a K = 4 parents greedy search  such that both methods have the
same search-space. We initialized both methods with π(0)
(m) = 1 if m = parG(i) and 0 else  as
a heuristic. In Figure 2 a) and b)  it can be seen that both methods approach AUROCs and AUPRs
close to one  for increasing amounts of data. However  due to the additional approximation in the
greedy algorithm  it performs slightly worse. In Figure 2 c) and d) we plot the corresponding ROC
and PR curves for 40 trajectories.
Scalablity. We compare the scalability of our
gradient-based greedy structure search with a greedy
hill-climbing implementation of structure seach
(K = 2) with variational inference as in [11] (we
limited this search to one sweep over families). We
ﬁxed all parameters as before and the number of tra-
jectories to 40. Results are displayed in Figure 3.
Dependence on initial values. We investigate the
performance of our method with respect to differ-
ent initial values. For this  we draw the initial val-
ues of mixture components uniformly at random 
and then project them on the probability simplex
via normalization  ˜π(0)
(m) =
˜π(0)
(n). We ﬁxed all parameters as be-
i
fore and the number of trajectories to 40. In Figure 2 
we displayed ROC e) and PR f) for our heuristic
initial and random initial values. We ﬁnd  that the
heuristic performs almost consistently better.

Figure 3: Run-time comparison of hill-
climbing structure search with variational in-
ference as in [11] with our gradient-based
method.

(m)/(cid:80)

i ∼ U (0  1) and π(0)

i

n ˜π(0)

i

i

8

00.51FPR00.20.40.60.81TPR00.51Recall00.20.40.60.81Precision00.51FPR00.20.40.60.81TPR00.51Recall00.20.40.60.81Precisiona)b)c)d)e)f)10203040Number of Trajectories00.20.40.60.81AUROCexhaustivegreedy10203040Number of Trajectories00.20.40.60.81AUPRexhaustivegreedy345678number of nodes01234computational time [a.u.]gradient-based greedy seachhill-climbing structure searchTable 1: AUROC (AUPR) of different methods on IRMA-data (top performers in bold).

method
steady state
DBN

ODE
NDS

GC
CTBN

random

knockout
G1DBN
VBSSM
TNSI
GP4GRN
CSId
CSIc
GCCA
exhaustive
greedy K=2

switch on
0.68 (0.42)
0.78 (0.64)
0.79 (0.70)
0.68 (0.51)
0.73 (0.61)
0.63 (0.46)
0.64 (0.39)
0.71 (0.55)
0.81 (0.86)
0.88 (0.85)
0.65 (0.45)

switch off
0.81 (0.50)
0.61 (0.34)
0.76 (0.60)
0.68 (0.42)
0.76 (0.57)
0.86 (0.72)
0.73 (0.59)
0.74 (0.65)
0.93 (0.92)
0.91 (0.89)
0.65 (0.45)

5.2 Real-world data

British household dataset. We show scalabil-
ity in a realistic setting  we applied our method
to the British Household Panel Survey (ESRC
Research Centre on Micro-social Change  2003).
This dataset has been collected yearly from 1991
to 2002  thus consisting of 11 time-points. Each
of the 1535 participants was questioned about sev-
eral facts of their life. We picked 15 of those  that
we deemed interpretable  some of them  "health
status"  "job status" and "health issues"  having
non-binary state-spaces. Because the participants
had the option of not answering a question and
changes in their lives are unlikely to happen dur-
ing the questionnaire  this dataset is strongly in-
complete. Out of the 1535 trajectories  we picked
600 at random and inferred the network presented in Figure 4. In supplementary C.2 we investigate
the stability of this result. We performed inference with our greedy algorithm (K = 2). This dataset
has been considered in [16]  where a network among 4 variables was inferred. Inferring a large
network at once is important  as latent variables can create spurious edges in the network [2].
IRMA gene-regulatory network. Finally  we investigate performance on realistic data. For this 
we apply it to the In vivo Reverse-engineering and Modeling Assessment (IRMA) network [3]. It
is  to best of our knowledge  the only molecular biological network with a ground-truth. This
gene regulatory network has been implemented on cultures of yeast  as a benchmark for network
reconstruction algorithms. Special care has been taken to isolate this network from crosstalk with
other cellular components. The authors of [3] provide time course data from two perturbation
experiments  referred to as “switch on” and “switch off”  and attempted reconstruction using different
methods. In Table 1  we compare to other methods tested in [18]. For more details on this experiment
and details on other methods  we refer to the supplementary C.3  respectively.

Figure 4: Learned structure using gradient-based
greedy structure learning with maximal K = 2
parents from 600 trajectories.

6 Conclusion

We presented a novel scalable gradient-based approach for structure learning for CTBNs from
complete and incomplete data  and demonstrated its usefulness on synthetic and real-world data. In
the future we plan to apply our algorithm to new bio-molecular datasets. Further  we believe that the
mixture likelihood may also be applicable to tasks different from structure learning.

Acknowledgements

We thank the anonymous reviewers for helpful comments on the previous version of this manuscript.
Dominik Linzner and Michael Schmidt are funded by the European Union’s Horizon 2020 research
and innovation programme (iPC–Pediatric Cure  No. 826121). Heinz Koeppl acknowledges support
by the European Research Council (ERC) within the CONSYN project  No. 773196  and by the
Hessian research priority programme LOEWE within the project CompuGene.

9

disabledcarsmokesmarriedhospitaljobstatuspromotionoptionlookingforworkﬁnancialsituationlivingwithpartnerhealthstatuschildcareReferences
[1] Enzo Acerbi  Teresa Zelante  Vipin Narang  and Fabio Stella. Gene network inference us-
ing continuous time Bayesian networks: a comparative study and application to Th17 cell
differentiation. BMC Bioinformatics  15  2014.

[2] Claudia Battistin  Benjamin Dunn  and Yasser Roudi. Learning with unknowns: Analyzing
biological data in the presence of hidden variables. Current Opinion in Systems Biology 
1:122–128  2017.

[3] Irene Cantone  Lucia Marucci  Francesco Iorio  Maria Aurelia Ricci  Vincenzo Belcastro 
Mukesh Bansal  Stefania Santini  Mario Di Bernardo  Diego di Bernardo  and Maria Pia Cosma.
A Yeast Synthetic Network for In Vivo Assessment of Reverse-Engineering and Modeling
Approaches. Cell  137(1):172–181  apr 2009.

[4] Ido Cohn  Tal El-Hay  Nir Friedman  and Raz Kupferman. Mean ﬁeld variational approximation
for continuous-time Bayesian networks. Journal Of Machine Learning Research  11:2745–2783 
2010.

[5] Tal El-Hay  Ido Cohn  Nir Friedman  and Raz Kupferman. Continuous-Time Belief Propagation.
Proceedings of the 27th International Conference on Machine Learning  pages 343–350  2010.

[6] Tal El-Hay  R Kupferman  and N Friedman. Gibbs sampling in factorized continuous-time
Markov processes. Proceedings of the 22th Conference on Uncertainty in Artiﬁcial Intelligence 
2011.

[7] Yu Fan and CR Shelton. Sampling for approximate inference in continuous time Bayesian

networks. AI and Math  2008.

[8] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Sparse covariance estimation. Bio-

statistics2  9(3):432–441  2008.

[9] Roy J Glauber. Time-Dependent Statistics of the Ising Model. J. Math. Phys.  4(1963):294–307 

1963.

[10] Daphne Koller and Nir Friedman. Probabilistic graphical models principles and techniques.

MIT Press  2010.

[11] Dominik Linzner and Heinz Koeppl. Cluster Variational Approximations for Structure Learning
of Continuous-Time Bayesian Networks from Incomplete Data. Advances in Neural Information
Processing Systems 31  pages 7880–7890  2018.

[12] Nicolai Meinshausen and Peter Bühlmann. High-dimensional graphs and variable selection

with the Lasso. Annals of Statistics  34(3):1436–1462  2006.

[13] Preetam Nandy  Alain Hauser  and Marloes H Maathuis. High-dimensional consistency in

score-based and hybrid structure learning. Annals of Statistics  46(6A):3151–3183  2018.

[14] Uri Nodelman  Christian R Shelton  and Daphne Koller. Continuous Time Bayesian Networks.
Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence  pages 378–387 
1995.

[15] Uri Nodelman  Christian R. Shelton  and Daphne Koller. Learning continuous time Bayesian
networks. Proceedings of the 19th Conference on Uncertainty in Artiﬁcial Intelligence  pages
451–458  2003.

[16] Uri Nodelman  Christian R Shelton  and Daphne Koller. Expectation Maximization and Complex
Duration Distributions for Continuous Time Bayesian Networks. Proc. Twenty-ﬁrst Conference
on Uncertainty in Artiﬁcial Intelligence  pages pages 421–430  2005.

[17] Manfred Opper and Guido Sanguinetti. Variational inference for Markov jump processes.

Advances in Neural Information Processing Systems 20  pages 1105–1112  2008.

[18] Christopher A. Penfold and David L. Wild. How to infer gene networks from expression proﬁles 

revisited. Interface Focus  1(6):857–870  dec 2011.

10

[19] Vinayak Rao and Yee Whye Teh. Fast MCMC sampling for Markov jump processes and

extensions. Journal of Machine Learning Research  14:3295–3320  2012.

[20] Eric E Schadt  John Lamb  Xia Yang  Jun Zhu  Steve Edwards  Debraj Guha Thakurta  Solveig K
Sieberts  Stephanie Monks  Marc Reitman  Chunsheng Zhang  Pek Yee Lum  Amy Leonardson 
Rolf Thieringer  Joseph M Metzger  Liming Yang  John Castle  Haoyuan Zhu  Shera F Kash 
Thomas A Drake  Alan Sachs  and Aldons J Lusis. An integrative genomics approach to infer
causal associations between gene expression and disease. Nature Genetics  37(7):710–717  jul
2005.

[21] Peter. Spirtes  Clark N. Glymour  and Richard. Scheines. Causation  prediction  and search.

MIT Press  2000.

[22] Lukas Studer  Christoph Zechner  Matthias Reumann  Loic Pauleve  Maria Rodriguez Mar-
tinez  and Heinz Koeppl. Marginalized Continuous Time Bayesian Networks for Network
Reconstruction from Incomplete Observations. Proceedings of the 30th Conference on Artiﬁcial
Intelligence (AAAI 2016)  pages 2051–2057  2016.

[23] Cunlu Zou and Jianfeng Feng. Granger causality vs. dynamic Bayesian network inference: a

comparative study. BMC Bioinformatics  10(1):122  dec 2009.

11

,Itay Hubara
Matthieu Courbariaux
Daniel Soudry
Ran El-Yaniv
Yoshua Bengio
Dominik Linzner
Michael Schmidt
Heinz Koeppl