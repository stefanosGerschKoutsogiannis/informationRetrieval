2018,Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes,We consider stochastic gradient descent (SGD) for least-squares regression with potentially several passes over the data. While several passes have been widely reported to perform practically better in terms of predictive performance on unseen data  the existing theoretical analysis of SGD suggests that a single pass is statistically optimal. While this is true for low-dimensional easy problems  we show that for hard problems  multiple passes lead to statistically optimal predictions while single pass does not; we also show that in these hard models  the optimal number of passes over the data increases with sample size. In order to define the notion of hardness and show that our predictive performances are optimal  we consider potentially infinite-dimensional models and notions typically associated to kernel methods  namely  the decay of eigenvalues of the covariance matrix of the features and the complexity of the optimal predictor as measured through the covariance matrix.
We illustrate our results on synthetic experiments with non-linear kernel methods and on a classical benchmark with a linear model.,Statistical Optimality of Stochastic Gradient Descent
on Hard Learning Problems through Multiple Passes

Loucas Pillaud-Vivien

INRIA - Ecole Normale Supérieure

PSL Research University

loucas.pillaud-vivien@inria.fr

Alessandro Rudi

INRIA - Ecole Normale Supérieure

PSL Research University

alessandro.rudi@inria.fr

Francis Bach

INRIA - Ecole Normale Supérieure

PSL Research University
francis.bach@inria.fr

Abstract

We consider stochastic gradient descent (SGD) for least-squares regression with
potentially several passes over the data. While several passes have been widely
reported to perform practically better in terms of predictive performance on unseen
data  the existing theoretical analysis of SGD suggests that a single pass is statisti-
cally optimal. While this is true for low-dimensional easy problems  we show that
for hard problems  multiple passes lead to statistically optimal predictions while
single pass does not; we also show that in these hard models  the optimal number
of passes over the data increases with sample size. In order to deﬁne the notion
of hardness and show that our predictive performances are optimal  we consider
potentially inﬁnite-dimensional models and notions typically associated to kernel
methods  namely  the decay of eigenvalues of the covariance matrix of the features
and the complexity of the optimal predictor as measured through the covariance
matrix. We illustrate our results on synthetic experiments with non-linear kernel
methods and on a classical benchmark with a linear model.

1

Introduction

Stochastic gradient descent (SGD) and its multiple variants—averaged [1]  accelerated [2]  variance-
reduced [3  4  5]—are the workhorses of large-scale machine learning  because (a) these methods
looks at only a few observations before updating the corresponding model  and (b) they are known in
theory and in practice to generalize well to unseen data [6].
Beyond the choice of step-size (often referred to as the learning rate)  the number of passes to make
on the data remains an important practical and theoretical issue. In the context of ﬁnite-dimensional
models (least-squares regression or logistic regression)  the theoretical answer has been known for
many years: a single passes sufﬁces for the optimal statistical performance [1  7]. Worse  most of the
theoretical work only apply to single pass algorithms  with some exceptions leading to analyses of
multiple passes when the step-size is taken smaller than the best known setting [8  9].
However  in practice  multiple passes are always performed as they empirically lead to better
generalization (e.g.  loss on unseen test data) [6]. But no analysis so far has been able to show that 
given the appropriate step-size  multiple pass SGD was theoretically better than single pass SGD.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

The main contribution of this paper is to show that for least-squares regression  while single pass
averaged SGD is optimal for a certain class of “easy” problems  multiple passes are needed to reach
optimal prediction performance on another class of “hard” problems.
In order to deﬁne and characterize these classes of problems  we need to use tools from inﬁnite-
dimensional models which are common in the analysis of kernel methods. De facto  our analysis
will be done in inﬁnite-dimensional feature spaces  and for ﬁnite-dimensional problems where the
dimension far exceeds the number of samples  using these tools are the only way to obtain non-vacuous
dimension-independent bounds. Thus  overall  our analysis applies both to ﬁnite-dimensional models
with explicit features (parametric estimation)  and to kernel methods (non-parametric estimation).
The two important quantities in the analysis are:

(a) The decay of eigenvalues of the covariance matrix ⌃ of the input features  so that the ordered
eigenvalues m decay as O(m↵); the parameter ↵ > 1 characterizes the size of the feature
space  ↵ = 1 corresponding to the largest feature spaces and ↵ = +1 to ﬁnite-dimensional
spaces. The decay will be measured through tr⌃1/↵ =Pm 1/↵
m   which is small when the
decay of eigenvalues is faster than O(m↵).
(b) The complexity of the optimal predictor ✓⇤ as measured through the covariance matrix ⌃ 
that is with coefﬁcients hem ✓ ⇤i in the eigenbasis (em)m of the covariance matrix that
decay so that h✓⇤  ⌃12r✓⇤i is small. The parameter r > 0 characterizes the difﬁculty
of the learning problem: r = 1/2 corresponds to characterizing the complexity of the
predictor through the squared norm k✓⇤k2  and thus r close to zero corresponds to the
hardest problems while r larger  and in particular r > 1/2  corresponds to simpler problems.

Dealing with non-parametric estimation provides a simple way to evaluate the optimality of learning
procedures. Indeed  given problems with parameters r and ↵  the best prediction performance
(averaged square loss on unseen data) is well known [10] and decay as O(n 2r↵
2r↵+1 )  with ↵ = +1
leading to the usual parametric rate O(n1). For easy problems  that is for which r > ↵1
2↵   then it
is known that most iterative algorithms achieve this optimal rate of convergence (but with various
running-time complexities)  such as exact regularized risk minimization [11]  gradient descent on the
empirical risk [12]  or averaged stochastic gradient descent [13].
We show that for hard problems  that is for which r 6 ↵1
2↵ (see Example 1 for a typical hard problem) 
then multiple passes are superior to a single pass. More precisely  under additional assumptions
detailed in Section 2 that will lead to a subset of the hard problems  with ⇥(n(↵12r↵)/(1+2r↵))
passes  we achieve the optimal statistical performance O(n 2r↵
2r↵+1 )  while for all other hard problems 
a single pass only achieves O(n2r). This is illustrated in Figure 1.
We thus get a number of passes that grows with the number of observations n and depends precisely
on the quantities r and ↵. In synthetic experiments with kernel methods where ↵ and r are known 
these scalings are precisely observed. In experiments on parametric models with large dimensions 
we also exhibit an increasing number of required passes when the number of observations increases.

Figure 1 – (Left) easy and hard problems in the (↵  r)-plane. (Right) different regions for which
multiple passes improved known previous bounds (green region) or reaches optimality (red region).

2

2 Least-squares regression in ﬁnite dimension
We consider a joint distribution ⇢ on pairs of input/output (x  y) 2 X⇥ R  where X is any input space 
and we consider a feature map  from the input space X to a feature space H  which we assume
Euclidean in this section  so that all quantities are well-deﬁned. In Section 4  we will extend all the
notions to Hilbert spaces.

2.1 Main assumptions
We are considering predicting y as a linear function f✓(x) = h✓  (x)iH of (x)  that is estimating
2E(y  h✓  (x)iH)2 is as small as possible. Estimators will depend on n
✓ 2 H such that F (✓) = 1
observations  with standard sampling assumptions:

(A1)

The n observations (xi  yi) 2 X ⇥ R  i = 1  . . .   n  are independent and identically
distributed from the distribution ⇢.

Since H is ﬁnite-dimensional  F always has a (potentially non-unique) minimizer in H which we
denote ✓⇤. We make the following standard boundedness assumptions:
(A2)

k(x)k 6 R almost surely  |y  h✓⇤  (x)iH| is almost surely bounded by  and |y| is
almost surely bounded by M.

In order to obtain improved rates with multiple passes  and motivated by the equivalent previously
used condition in reproducing kernel Hilbert spaces presented in Section 4  we make the following
extra assumption (we denote by ⌃= E[(x) ⌦H (x)] the (non-centered) covariance matrix).
(A3)

For µ 2 [0  1]  there exists µ > 0 such that  almost surely  (x)⌦H (x) 4H 2
Note that it can also be written as k⌃µ/21/2(x)kH 6 µRµ.

µR2µ⌃1µ.

Assumption (A3) is always satisﬁed with any µ 2 [0  1]  and has particular values for µ = 1  with
1 = 1  and µ = 0  where 0 has to be larger than the dimension of the space H.
We will also introduce a parameter ↵ that characterizes the decay of eigenvalues of ⌃ through the
quantity tr⌃1/↵  as well as the difﬁculty of the learning problem through k⌃1/2r✓⇤kH  for r 2 [0  1].
In the ﬁnite-dimensional case  these quantities can always be deﬁned and most often ﬁnite  but may
be very large compared to sample size. In the following assumptions the quantities are assumed to be
ﬁnite and small compared to n.

(A4)

There exists ↵> 1 such that tr ⌃1/↵ < 1.

Assumption (A4) is often called the “capacity condition”. First note that this assumption implies
that the decreasing sequence of the eigenvalues of ⌃  (m)m>1  satisﬁes m = o (1/m↵). Note that
tr⌃µ 6 2
µR2µ and thus often we have µ > 1/↵  and in the most favorable cases in Section 4  this
bound will be achieved. We also assume:

(A5)

There exists r > 0  such that k⌃1/2r✓⇤kH < 1.

Assumption (A5) is often called the “source condition”. Note also that for r = 1/2  this simply says
that the optimal predictor has a small norm.
In the subsequent sections  we essentially assume that ↵  µ and r are chosen (by the theoretical
analysis  not by the algorithm) so that all quantities Rµ  k⌃1/2r✓⇤kH and tr⌃1/↵ are ﬁnite and
small. As recalled in the introduction  these parameters are often used in the non-parametric literature
to quantify the hardness of the learning problem (Figure 1).
We will use result with O(·) and ⇥(·) notations  which will all be independent of n and t (number of
observations and number of iterations) but can depend on other ﬁnite constants. Explicit dependence
on all parameters of the problem is given in proofs. More precisely  we will use the usual O(·) and
⇥(·) notations for sequences bnt and ant that can depend on n and t  as ant = O(bnt) if and only if 
there exists M > 0 such that for all n  t  ant 6 M bnt  and ant =⇥( bnt) if and only if  there exist
M  M0 > 0 such that for all n  t  M0bnt 6 ant 6 M bnt.

3

2.2 Related work
Given our assumptions above  several algorithms have been developed for obtaining low values of

the expected excess risk E⇥F (✓)⇤  F (✓⇤).

H  for appropriate values of . It is known that for easy problems where r > ↵1

Regularized empirical risk minimization. Forming the empirical risk ˆF (✓)  it minimizes ˆF (✓) +
2↵   it achieves
k✓k2
the optimal rate of convergence O(n 2r↵
2r↵+1 ) [11]. However  algorithmically  this requires to solve a
linear system of size n times the dimension of H. One could also use fast variance-reduced stochastic
gradient algorithms such as SAG [3]  SVRG [4] or SAGA [5]  with a complexity proportional to the
dimension of H times n + R2/.
Early-stopped gradient descent on the empirical risk. Instead of solving the linear system directly 
one can use gradient descent with early stopping [12  14]. Similarly to the regularized empirical
risk minimization case  a rate of O(n 2r↵
2r↵+1 ) is achieved for the easy problems  where r > ↵1
2↵ .
Different iterative regularization techniques beyond batch gradient descent with early stopping have
been considered  with computational complexities ranging from O(n1+ ↵
4r↵+2 ) times
the dimension of H (or n in the kernel case in Section 4) for optimal predictions [12  15  16  17  14].
Stochastic gradient. The usual stochastic gradient recursion is iterating from i = 1 to n 

2r↵+1 ) to O(n1+ ↵

✓i = ✓i1 + yi  h✓i1  (xi)iH(xi) 
nPn

with the averaged iterate ¯✓n = 1
i=1 ✓i. Starting from ✓0 = 0  [18] shows that the expected excess
performance E[F (¯✓n)]  F (✓⇤) decomposes into a variance term that depends on the noise 2 in
the prediction problem  and a bias term  that depends on the deviation ✓⇤  ✓0 = ✓⇤ between the
+ k✓⇤k2
initialization and the optimal predictor. Their bound is  up to universal constants  2dim(H)
n .
Further  [13] considered the quantities ↵ and r above to get the bound  up to constant factors:

n

H

2tr⌃1/↵(n)1/↵

n

+ k⌃1/2r✓⇤k2

2rn2r

.

We recover the ﬁnite-dimensional bound for ↵ = +1 and r = 1/2. The bounds above are valid for
all ↵ > 1 and all r 2 [0  1]  and the step-size  is such that R2 6 1/4  and thus we see a natural
trade-off appearing for the step-size   between bias and variance.
2↵   then the optimal step-size minimizing the bound above is  / n 2↵ min{r 1}1+↵
When r > ↵1
 
2↵ min{r 1}+1
and the obtained rate is optimal. Thus a single pass is optimal. However  when r 6 ↵1
2↵   the best
step-size does not depend on n  and one can only achieve O(n2r).
Finally  in the same multiple pass set-up as ours  [9] has shown that for easy problems where r > ↵1
2↵
(and single-pass averaged SGD is already optimal) that multiple-pass non-averaged SGD is becoming
optimal after a correct number of passes (while single-pass is not). Our proof principle of comparing
to batch gradient is taken from [9]  but we apply it to harder problems where r 6 ↵1
2↵ . Moreover we
consider the multi-pass averaged-SGD algorithm  instead of non-averaged SGD  and take explicitly
into account the effect of Assumption (A3).

3 Averaged SGD with multiple passes

We consider the following algorithm  which is stochastic gradient descent with sampling with
replacement with multiple passes over the data (we experiment in Section E of the Appendix with
cycling over the data  with or without reshufﬂing between each pass).

• Initialization: ✓0 = ¯✓0 = 0  t = maximal number of iterations   = 1/(4R2) = step-size
• Iteration: for u = 1 to t  sample i(u) uniformly from {1  . . .   n} and make the step
✓u = ✓u1 + yi(u)  h✓t1  (xi(u))iH(xi(u)) and ¯✓u = (1  1
u )¯✓u1 + 1

In this paper  following [18  13]  but as opposed to [19]  we consider unregularized recursions. This
removes a unnecessary regularization parameter (at the expense of harder proofs).

u ✓u.

4

3.1 Convergence rate and optimal number of passes

Our main result is the following (see full proof in Appendix):
Theorem 1. Let n 2 N⇤ and t > n  under Assumptions (A1)  (A2)  (A3)  (A4)  (A5)  (A6)  with
 = 1/(4R2).

• For µ↵ < 2r↵ + 1 <↵   if we take t =⇥( n↵/(2r↵+1))  we obtain the following rate:

EF (¯✓t)  F (✓⇤) = O(n2r↵/(2r↵+1)).

• For µ↵ > 2r↵ + 1  if we take t =⇥( n1/µ (log n)

1

µ )  we obtain the following rate:

EF (¯✓t)  F (✓⇤) 6 O(n2r/µ).

Sketch of proof. The main difﬁculty in extending proofs from the single pass case [18  13] is that
as soon as an observation is processed twice  then statistical dependences are introduced and the
proof does not go through. In a similar context  some authors have considered stability results [8] 
but the large step-sizes that we consider do not allow this technique. Rather  we follow [16  9] and
compare our multi-pass stochastic recursion ✓t to the batch gradient descent iterate ⌘t deﬁned as
i=1yi  h⌘t1  (xi)iH(xi) with its averaged iterate ¯⌘t. We thus need to
nPn
⌘t = ⌘t1 + 
study the predictive performance of ¯⌘t and the deviation ¯✓t  ¯⌘t. It turns out that  given the data  the
deviation ✓t  ⌘t satisﬁes an SGD recursion (with the respect to the randomness of the sampling with
replacement). For a more detailed summary of the proof technique see Section B.
The novelty compared to [16  9] is (a) to use reﬁned results on averaged SGD for least-squares  in
particular convergence in various norms for the deviation ¯✓t  ¯⌘t (see Section A)  that can use our
new Assumption (A3). Moreover  (b) we need to extend the convergence results for the batch gradient
descent recursion from [14]  also to take into account the new assumption (see Section D). These two
results are interesting on their own.

Improved rates with multiple passes. We can draw the following conclusions:

• If 2↵r + 1 > ↵  that is  easy problems  it has been shown by [13] that a single pass with a
smaller step-size than the one we propose here is optimal  and our result does not apply.
• If µ↵ < 2r↵ + 1 <↵   then our proposed number of iterations is t =⇥( n↵/(2↵r+1)) 
which is now greater than n; the convergence rate is then O(n 2r↵
2r↵+1 )  and  as we will see in
Section 4.2  the predictive performance is then optimal when µ 6 2r.
• If µ↵ > 2r↵ + 1  then with a number of iterations is t =⇥( n1/µ)  which is greater than n
(thus several passes)  with a convergence rate equal to O(n2r/µ)  which improves upon
the best known rates of O(n2r). As we will see in Section 4.2  this is not optimal.

Note that these rates are theoretically only bounds on the optimal number of passes over the data 
and one should be cautious when drawing conclusions; however our simulations on synthetic data 
see Figure 2 in Section 5  conﬁrm that our proposed scalings for the number of passes is observed in
practice.

4 Application to kernel methods

In the section above  we have assumed that H was ﬁnite-dimensional  so that the optimal predic-
tor ✓⇤ 2 H was always deﬁned. Note however  that our bounds that depends on ↵  r and µ are
independent of the dimension  and hence  intuitively  following [19]  should apply immediately to
inﬁnite-dimensional spaces.
We now ﬁrst show in Section 4.1 how this intuition can be formalized and how using kernel methods
provides a particularly interesting example. Moreover  this interpretation allows to characterize the
statistical optimality of our results in Section 4.2.

5

4.1 Extension to Hilbert spaces  kernel methods and non-parametric estimation
Our main result in Theorem 1 extends directly to the case where H is an inﬁnite-dimensional Hilbert
space. In particular  given a feature map : X ! H  any vector ✓ 2 H is naturally associated to
a function deﬁned as f✓(x) = h✓  (x)iH. Algorithms can then be run with inﬁnite-dimensional
objects if the kernel K(x0  x) = h(x0)  (x)iH can be computed efﬁciently. This identiﬁcation of
elements ✓ of H with functions f✓ endows the various quantities we have introduced in the previous
sections  with natural interpretations in terms of functions. The stochastic gradient descent described
in Section 3 adapts instantly to this new framework as the iterates (✓u)u6t are linear combinations of
feature vectors (xi)  i = 1  . . .   n  and the algorithms can classically be “kernelized” [20  13]  with
an overall running time complexity of O(nt).
First note that Assumption (A3) is equivalent to  for all x 2 X and ✓ 2 H  |f✓(x)|2 6
L1 6 2
µR2µhf✓  ⌃1µf✓iH  that is  kgk2
H for any g 2 H and also im-
2
Hkgk1µ
plies1 kgkL1 6 µRµkgkµ
L2   which are common assumptions in the context of kernel
methods [22]  essentially controlling in a more reﬁned way the regularity of the whole space of
functions associated to H  with respect to the L1-norm  compared to the too crude inequality
kgkL1 = supx |h (x)  giH | 6 supx k(x)kHkgkH 6 RkgkH.
The natural relation with functions allows to analyze effects that are crucial in the context of learning 
but difﬁcult to grasp in the ﬁnite-dimensional setting. Consider the following prototypical example of
a hard learning problem 
Example 1 (Prototypical hard problem on simple Sobolev space). Let X = [0  1]  with x sampled
uniformly on X and

µR2µk⌃1/2µ/2gk2

y = sign(x  1/2) + ✏  (x) = {|k|1e2ik⇡x}k2Z⇤.

✓k
|k|

This corresponds to the kernel K(x  y) =Pk2Z⇤ |k|2e2ik⇡(xy)  which is well deﬁned (and lead
to the simplest Sobolev space). Note that for any ✓ 2 H  which is here identiﬁed as the space
of square-summable sequences `2(Z)  we have f✓(x) = h✓  (x)i`2(Z) =Pk2Z⇤
e2ik⇡x. This
means that for any estimator ˆ✓ given by the algorithm  fˆ✓ is at least once continuously differentiable 
while the target function sign(· 1/2) is not even continuous. Hence  we are in a situation where ✓⇤ 
the minimizer of the excess risk  does not belong to H. Indeed let represent sign(· 1/2) in H 
for almost all x 2 [0  1]  by its Fourier series sign(x  1/2) =Pk2Z⇤
↵ke2ik⇡x  with |↵k|⇠ 1/k 
an informal reasoning would lead to (✓⇤)k = ↵k|k|⇠ 1  which is not square-summable and thus
✓⇤ /2 H. For more details  see [23  24].
This setting generalizes important properties that are valid for Sobolev spaces  as shown in the
following example  where ↵  r  µ are characterized in terms of the smoothness of the functions in H 
the smoothness of f⇤ and the dimensionality of the input space X.
Example 2 (Sobolev Spaces [25  22  26  10]). Let X ✓ Rd  d 2 N  with ⇢X supported on X 
absolutely continous with the uniform distribution and such that ⇢X(x) > a > 0 almost everywhere 
for a given a. Assume that f⇤(x) = E[y|x] is s-times differentiable  with s > 0. Choose a kernel 
inducing Sobolev spaces of smoothness m with m > d/2  as the Matérn kernel

K(x0  x) = kx0  xkmd/2Kd/2m(kx0  xk) 

where Kd/2m is the modiﬁed Bessel function of the second kind. Then the assumptions are satisﬁed
for any ✏> 0  with ↵ = 2m

r = s

d   µ = d

2m + ✏ 

2m .

In the following subsection we compare the rates obtained in Thm. 1  with known lower bounds
under the same assumptions.

4.2 Minimax lower bounds
In this section we recall known lower bounds on the rates for classes of learning problems satisfying
the conditions in Sect. 2.1. Interestingly  the comparison below shows that our results in Theorem 1

1Indeed  for any g 2 H  k⌃1/2µ/2gkH = k⌃µ/2gkL2 6 k⌃1/2gkµ

L2kgk1µ
we used that for any g 2 H  any bounded operator A  s 2 [0  1]: kAsgkL2 6 kAgks

= kgkµ
L2kgk1s

L2

L2

L2   where

Hkgk1µ
(see [21]).

6

are optimal in the setting 2r > µ. While the optimality of SGD was known for the regime {2r↵ +1 >
↵ \ 2r > µ}  here we extend the optimality to the new regime ↵ > 2r↵ + 1 > µ↵  covering
essentially all the region 2r > µ  as it is possible to observe in Figure 1  where for clarity we plotted
the best possible value for µ that is µ = 1/↵ [10] (which is true for Sobolev spaces).
When r 2 (0  1] is ﬁxed  but there are no assumptions on ↵ or µ  then the optimal minimax rate of
convergence is O(n2r/(2r+1))  attained by regularized empirical risk minimization [11] and other
spectral ﬁlters on the empirical covariance operator [27].
When r 2 (0  1] and ↵ > 1 are ﬁxed (but there are no constraints on µ)  the optimal minimax rate
of convergence O(n 2r↵
2↵   with empirical risk minimization [14] or
stochastic gradient descent [13].
When r > ↵1
2r↵+1 ) is known to be a lower bound on the opti-
mal minimax rate  but the best upper-bound so far is O(n2r) and is achieved by empirical risk
minimization [14] or stochastic gradient descent [13]  and the optimal rate is not known.
When r 2 (0  1]  ↵ > 1 and µ 2 [1/↵  1] are ﬁxed  then the rate of convergence O(n  max{µ 2r}↵
2 max{µ 2r}↵+1 )
is known to be a lower bound on the optimal minimax rate [10]. This is attained by regularized
empirical risk minimization when 2r > µ [10]  and now by SGD with multiple passes  and it is thus
the optimal rate in this situation. When 2r < µ  the only known upper bound is O(n2↵r/(µ↵+1)) 
and the optimal rate is not known.

2↵   the rate of convergence O(n 2r↵

2r↵+1 ) is attained when r > ↵1

5 Experiments

In our experiments  the main goal is to show that with more that one pass over the data  we can
improve the accuracy of SGD when the problem is hard. We also want to highlight our dependence
of the optimal number of passes (that is t/n) with respect to the number of observations n.

Synthetic experiments. Our main experiments are performed on artiﬁcial data following the setting
in [21]. For this purpose  we take kernels K corresponding to splines of order q (see [24]) that fulﬁll
Assumptions (A1) (A2) (A3) (A4) (A5) (A6). Indeed  let us consider the following function

⇤q(x  z) =Xk2Z

e2i⇡k(xz)

|k|q

 

deﬁned almost everywhere on [0  1]  with q 2 R  and for which we have the interesting relationship:
h⇤q(x ·)  ⇤q0(z ·)iL2(d⇢X) =⇤ q+q0(x  z) for any q  q0 2 R. Our setting is the following:

• Input distribution: X = [0  1] and ⇢X is the uniform distribution.
• Kernel: 8(x  z) 2 [0  1]  K(x  z) =⇤ ↵(x  z).
• Target function: 8x 2 [0  1] ✓ ⇤ =⇤ r↵+ 1
• Output distribution : ⇢(y|x) is a Gaussian with variance 2 and mean ✓⇤.

(x  0).

2

For this setting we can show that the learning problem satisﬁes Assumptions (A1) (A2) (A3) (A4)
(A5) (A6) with r  ↵  andµ = 1/↵. We take different values of these parameters to encounter all the
different regimes of the problems shown in Figure 1.
For each n from 100 to 1000  we found the optimal number of steps t⇤(n) that minimizes the test
error F (¯✓t)  F (✓⇤). Note that because of overﬁtting the test error increases for t > t⇤(n). In
Figure 2  we show t⇤(n) with respect to n in log scale. As expected  for the easy problems (where
r > ↵1
2↵   see top left and right plots)  the slope of the plot is 1 as one pass over the data is enough:
t⇤(n) =⇥( n). But we see that for hard problems (where r 6 ↵1
2↵   see bottom left and right plots) 
we need more than one pass to achieve optimality as the optimal number of iterations is very close to

2r↵+1. That matches the theoretical predictions of Theorem 1. We also notice in the

plots that  the bigger
2r↵+1 the harder the problem is and the bigger the number of epochs we have
to take. Note  that to reduce the noise on the estimation of t⇤(n)  plots show an average over 100
replications.

t⇤(n) =⇥ n

↵

↵

7

To conclude  the experiments presented in the section correspond exactly to the theoretical setting
of the article (sampling with replacement)  however we present in Figures 4 and 5 of Section E of
the Appendix results on the same datasets for two different ways of sampling the data: (a)without
replacement: for which we select randomly the data points but never use twice the same point in one
epoch  (b) cycles: for which we pick successively the data points in the same order. The obtained
scalings relating number of iterations or passes to number of observations are the same.

Figure 2 – The four plots represent each a different conﬁguration on the (↵  r) plan represented in Figure 1  for
r = 1/(2↵). Top left (↵ = 1.5) and right (↵ = 2) are two easy problems (Top right is the limiting case where
2↵ ) for which one pass over the data is optimal. Bottom left (↵ = 2.5) and right (↵ = 3) are two hard
r = ↵1
problems for which an increasing number of passes is required. The blue dotted line are the slopes predicted by
the theoretical result in Theorem 1.

Linear model. To illustrate our result with some real data  we show how the optimal number of
passes over the data increases with the number of samples. In Figure 3  we simply performed linear
least-squares regression on the MNIST dataset and plotted the optimal number of passes over the
data that leads to the smallest error on the test set. Evaluating ↵ and r from Assumptions (A4) and
(A5)  we found ↵ = 1.7 and r = 0.18. As r = 0.18 6 ↵1
2↵ ⇠ 0.2  Theorem 1 indicates that this
corresponds to a situation where only one pass on the data is not enough  conﬁrming the behavior of
Figure 3. This suggests that learning MNIST with linear regression is a hard problem.

6 Conclusion

In this paper  we have shown that for least-squares regression  in hard problems where single-pass
SGD is not statistically optimal (r < ↵1
2↵ )  then multiple passes lead to statistical optimality with a
number of passes that somewhat surprisingly needs to grow with sample size  with a convergence
rate which is superior to previous analyses of stochastic gradient. Using a non-parametric estimation 
we show that under certain conditions (2r > µ)  we attain statistical optimality.
Our work could be extended in several ways: (a) our experiments suggest that cycling over the
data and cycling with random reshufﬂing perform similarly to sampling with replacement  it would
be interesting to combine our theoretical analysis with work aiming at analyzing other sampling
schemes [28  29]. (b) Mini-batches could be also considered with a potentially interesting effects
compared to the streaming setting. Also  (c) our analysis focuses on least-squares regression  an
extension to all smooth loss functions would widen its applicability. Moreover  (d) providing optimal

8

Figure 3 – For the MNIST data set  we show the optimal number of passes over the data with respect to the
number of samples in the case of the linear regression.

efﬁcient algorithms for the situation 2r < µ is a clear open problem (for which the optimal rate is
not known  even for non-efﬁcient algorithms). Additionally  (e) in the context of classiﬁcation  we
could combine our analysis with [30] to study the potential discrepancies between training and testing
losses and errors when considering high-dimensional models [31]. More generally  (f) we could
explore the effect of our analysis for methods based on the least squares estimator in the context of
structured prediction [32  33  34] and (non-linear) multitask learning [35]. Finally  (g) to reduce the
computational complexity of the algorithm  while retaining the (optimal) statistical guarantees  we
could combine multi-pass stochastic gradient descent  with approximation techniques like random
features [36]  extending the analysis of [37] to the more general setting considered in this paper.

Acknowledgements

We acknowledge support from the European Research Council (grant SEQUOIA 724063). We also
thank Raphaël Berthier and Yann Labbé for their enlightening advices on this project.

References
[1] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM

Journal on Control and Optimization  30(4):838–855  1992.

[2] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical

Programming  133(1-2):365–397  2012.

[3] Nicolas L. Roux  Mark Schmidt  and Francis Bach. A stochastic gradient method with an
In Advances in Neural Information

exponential convergence rate for ﬁnite training sets.
Processing Systems (NIPS)  2012.

[4] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  2013.

[5] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems  2014.

[6] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. SIAM Review  60(2):223–311  2018.

[7] A. S. Nemirovski and D. B. Yudin. Problem complexity and method efﬁciency in optimization.

John Wiley  1983.

[8] Moritz Hardt  Ben Recht  and Yoram Singer. Train faster  generalize better: Stability of

stochastic gradient descent. In International Conference on Machine Learning  2016.

[9] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods.

Journal of Machine Learning Research  18(97):1–47  2017.

[10] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares

algorithm. Fakultät für Mathematik und Physik  Universität Stuttgart  2017.

9

[11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

[12] Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. On early stopping in gradient descent

learning. Constructive Approximation  26(2):289–315  2007.

[13] Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large

step-sizes. The Annals of Statistics  44(4):1363–1399  2016.

[14] Junhong Lin  Alessandro Rudi  Lorenzo Rosasco  and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over hilbert spaces. Applied and Computational
Harmonic Analysis  2018.

[15] L Lo Gerfo  L Rosasco  F Odone  E De Vito  and A Verri. Spectral algorithms for supervised

learning. Neural Computation  20(7):1873–1897  2008.

[16] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization.

Advances in Neural Information Processing Systems  pages 1630–1638  2015.

In

[17] Gilles Blanchard and Nicole Krämer. Convergence rates of kernel conjugate gradient for random

design regression. Analysis and Applications  14(06):763–794  2016.

[18] Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with
convergence rate O(1/n). In Advances in Neural Information Processing Systems (NIPS)  pages
773–781  2013.

[19] Aymeric Dieuleveut  Nicolas Flammarion  and Francis Bach. Harder  better  faster  stronger con-
vergence rates for least-squares regression. Journal of Machine Learning Research  18(1):3520–
3570  2017.

[20] Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Foundations

of Computational Mathematics  8(5):561–596  Oct 2008.

[21] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random

features. In Advances in Neural Information Processing Systems  pages 3215–3225  2017.

[22] Ingo Steinwart  Don R. Hush  and Clint Scovel. Optimal rates for regularized least squares

regression. In Proc. COLT  2009.

[23] R. A. Adams. Sobolev spaces / Robert A. Adams. Academic Press New York  1975.
[24] G. Wahba. Spline Models for Observational Data. Society for Industrial and Applied Mathe-

matics  1990.

[25] Holger Wendland. Scattered data approximation  volume 17. Cambridge university press  2004.
[26] Francis Bach. On the equivalence between kernel quadrature rules and random feature expan-

sions. Journal of Machine Learning Research  18(21):1–38  2017.

[27] Gilles Blanchard and Nicole Mücke. Optimal rates for regularization of statistical inverse

learning problems. Foundations of Computational Mathematics  pages 1–43  2017.

[28] Ohad Shamir. Without-replacement sampling for stochastic gradient methods. In Advances in

Neural Information Processing Systems 29  pages 46–54  2016.

[29] Mert Gürbüzbalaban  Asu Ozdaglar  and Pablo Parrilo. Why random reshufﬂing beats stochastic

gradient descent. Technical Report 1510.08560  arXiv  2015.

[30] Loucas Pillaud-Vivien  Alessandro Rudi  and Francis Bach. Exponential convergence of testing
error for stochastic gradient methods. In Proceedings of the 31st Conference On Learning
Theory  volume 75  pages 250–296  2018.

[31] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In Proceedings of the International Conference
on Learning Representations (ICLR)  2017.

[32] Carlo Ciliberto  Lorenzo Rosasco  and Alessandro Rudi. A consistent regularization approach
for structured prediction. In Advances in Neural Information Processing Systems  pages 4412–
4420  2016.

[33] Anton Osokin  Francis Bach  and Simon Lacoste-Julien. On structured prediction theory with
calibrated convex surrogate losses. In Advances in Neural Information Processing Systems 
pages 302–313  2017.

[34] Carlo Ciliberto  Francis Bach  and Alessandro Rudi. Localized structured prediction. arXiv

preprint arXiv:1806.02402  2018.

10

[35] Carlo Ciliberto  Alessandro Rudi  Lorenzo Rosasco  and Massimiliano Pontil. Consistent multi-
task learning with nonlinear output relations. In Advances in Neural Information Processing
Systems  pages 1986–1996  2017.

[36] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in Neural Information Processing Systems  pages 1177–1184  2008.

[37] Luigi Carratino  Alessandro Rudi  and Lorenzo Rosasco. Learning with sgd and random features.

In Advances in Neural Information Processing Systems 31  pages 10213–10224  2018.

[38] R. Aguech  E. Moulines  and P. Priouret. On a perturbation approach for the analysis of

stochastic tracking algorithms. SIAM J. Control and Optimization  39(3):872–899  2000.

[39] Alessandro Rudi  Guillermo D Canas  and Lorenzo Rosasco. On the sample complexity of
subspace learning. In Advances in Neural Information Processing Systems  pages 2067–2075 
2013.

[40] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computa-

tional mathematics  12(4):389–434  2012.

[41] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. Falkon: An optimal large scale kernel

method. In Advances in Neural Information Processing Systems  pages 3888–3898  2017.

11

,Loucas Pillaud-Vivien
Alessandro Rudi
Francis Bach
Vivek Veeriah
Matteo Hessel
Zhongwen Xu
Janarthanan Rajendran
Richard Lewis
Junhyuk Oh
Hado van Hasselt
David Silver
Satinder Singh