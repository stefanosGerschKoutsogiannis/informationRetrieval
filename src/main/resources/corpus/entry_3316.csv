2019,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments,We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure  where the experimenter randomizes over which user will receive a recommendation to take an action  and we are interested in the effect of the downstream action. We develop a statistical learning approach to the estimation of heterogeneous effects  reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). The reduction enables the use of all recent algorithmic advances (e.g. neural nets  forests). We show that the estimated effect model is robust to estimation errors in the auxiliary models  by showing that the loss satisfies a Neyman orthogonality criterion. Our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. When these spaces are parametric  then the parameter estimates are asymptotically normal  which enables construction of confidence sets. We applied our method to estimate the effect of membership on downstream webpage engagement for a major travel webpage  using as an instrument an intent-to-treat A/B test among 4 million users  where some users received an easier membership sign-up process. We also validate our method on synthetic data and on public datasets for the effects of schooling on income.,Machine Learning Estimation of Heterogeneous

Treatment Effects with Instruments

Vasilis Syrgkanis
Microsoft Research

vasy@microsoft.com

Victor Lei
TripAdvisor

vlei@tripadvisor.com

Miruna Oprescu
Microsoft Research

moprescu@microsoft.com

Maggie Hei

Microsoft Research

Maggie.Hei@microsoft.com

Greg Lewis

Microsoft Research

glewis@microsoft.com

Keith Battocchi
Microsoft Research

kebatt@microsoft.com

Abstract

We consider the estimation of heterogeneous treatment effects with arbitrary ma-
chine learning methods in the presence of unobserved confounders with the aid of
a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure 
where the experimenter randomizes over which user will receive a recommendation
to take an action  and we are interested in the effect of the downstream action. We
develop a statistical learning approach to the estimation of heterogeneous effects 
reducing the problem to the minimization of an appropriate loss function that
depends on a set of auxiliary models (each corresponding to a separate prediction
task). The reduction enables the use of all recent algorithmic advances (e.g. neural
nets  forests). We show that the estimated effect model is robust to estimation errors
in the auxiliary models  by showing that the loss satisﬁes a Neyman orthogonality
criterion. Our approach can be used to estimate projections of the true effect model
on simpler hypothesis spaces. When these spaces are parametric  then the parame-
ter estimates are asymptotically normal  which enables construction of conﬁdence
sets. We applied our method to estimate the effect of membership on downstream
webpage engagement on TripAdvisor  using as an instrument an intent-to-treat
A/B test among 4 million TripAdvisor users  where some users received an easier
membership sign-up process. We also validate our method on synthetic data and
on public datasets for the effects of schooling on income.1

1

Introduction

A/B testing is the gold standard of causal inference. But even when A/B testing is feasible  estimating
the effect of a treatment on an outcome might not be a straightforward task. One major difﬁculty is
non-compliance: even if we randomize what treatment to recommend to a subject  the subject might
not comply with the recommendation due to unobserved factors and follow the alternate action. The
impact that unobserved factors might have on the measured outcome is a source of endogeneity and
can lead to biased estimates of the effect. This problem arises in large scale data problems in the
digital economy; when optimizing a digital service  we might often want to estimate the effect of
some action taken by our users on downstream metrics. However  the service cannot force users to
comply  but can only ﬁnd means of incentivizing or recommending the action. The unobserved factors
of compliance can lead to biased estimates if we consider the takers and not takers as exogenously

1Prototype code for all the algorithms presented and the synthetic data experimental study can be found at

https://github.com/Microsoft/EconML/tree/master/prototypes/dml_iv.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

assigned and employ machine learning approaches to estimate the potentially heterogeneous effect of
the action on the downstream metric.
The problem can be solved by using the technique of instrumental variable (IV) regression: as long
as the recommendation increases the probability of taking the treatment  then we know that there is at
least some fraction of users that were assigned the treatment “exogeneously”. IV regression parses
out this population of “exogenously treated” users and estimates an effect based solely on them.
Most classical IV approaches estimate a constant average treatment effect. However  to make
personalized policy decisions (an emerging trend in most digital services) one might want to estimate
a heterogeneous effect based on observable characteristics of the user. The latter is a daunting task 
as we seek to estimate a function of observable characteristics as opposed to a single number. Hence 
statistical power is at stake. Even estimating an ATE is non-trivial when effect and compliance are
correlated through observables. The emergence of large data-sets in the digital economy alleviates this
concern; with A/B tests running on millions of users it is possible to estimate complex heterogeneous
effect models  even if compliance levels are relatively weak. Moreover  as we control for more and
more observable features of the user  we also reduce the risk that correlation between effect and
compliance is stemming from unobserved factors.
This leads to the question this work seeks to answer: how can we blend the power of modern machine
learning approaches (e.g. random forests  gradient boosting  penalized regressions  neural networks)
with instrumental variable methods  so as to estimate complex heterogeneous effect rules. Recent
work at the intersection of machine learning and econometrics has proposed powerful methods for
estimating the effect of a treatment on an outcome  while using machine learning methods for learning
nuisance models that help de-bias the ﬁnal effect rule. However  the majority of the work has either
focused on 1) estimating average treatment effects or low dimensional parametric effect models (e.g.
the double machine learning approach of [11])  2) developing new algorithms for estimating fully
non-parametric models of the effect (e.g. the IV forest method of [4]  the DeepIV method of [15])  3)
assuming that the treatment is exogenous once we condition on the observable features and reducing
the problem to an appropriate square loss minimization framework (see e.g. [26  21]).
Nevertheless  a general reduction of IV based machine learning estimation of heterogeneous effects
to a more standard statistical learning problem that can incorporate existing algorithms in a black-box
manner has not been formulated in prior work. In fact  the recent work of [26]  which develops a
statistical learning based approach in the setting with no unboserved confounders  leaves as a major
open question the development of an analogue statistical learning approach for our setting  with
unobserved confounders and access to valid instruments. Such a reduction can help us leverage
the recent algorithmic advances in statistical learning theory so as to work with large data-sets.
Our work proposes the reduction of heterogeneous effects estimation via instruments to a square
loss minimization problem over a hypothesis space. This enables us to learn not only the true
heterogeneous effect model  but also the projections of the true model in simpler hypothesis spaces
for interpretability. Moreover  our work leverages recent advances in statistical learning with nuisance
functions [12  13]  to show that the mean squared error (MSE) of the learned model is robust to
the estimation error of auxiliary models that need to be estimated (as is standard in IV regression).
Thus we achieve MSE rates where the leading term depends only on the sample complexity of the
hypothesis space of the heterogeneous effect model.
Some advantages of reducing our problem to a set of standard regression problems include being
able to use existing algorithms and implementations  as well as recent advances of interpretability
in machine learning. For instance  in our application we deploy the SHAP framework [23  22] to
interpret random forest based models of the heterogeneous effect. Furthermore  when the hypothesis
space is low dimensional and parametric then our approach falls in the setting studied by prior
work of [11] and  hence  not only MSE rates but also conﬁdence interval construction is relatively
straightforward. This enables hypothesis testing on parametric projections of the true effect model.
We apply our approach to an intent-to-treat A/B test among 4 million users on a major travel webpage
so as to estimate the effect of membership on downstream engagement. We identify sources of
heterogeneity that have policy implications on which users the platform should engage more and
potentially how to re-design the recommendation to target users with large effects. We validate the
ﬁndings on a different cohort in a separate experiment among 10 million users on the same platform.
Even though the new experiment was deployed on a much broader and different cohort  we identify
common leading factors of effect heterogeneity  hence conﬁrming our ﬁndings. As a robustness check

2

we create semi-synthetic data with similar features and marginal distributions of variables as the real
data  but where we know the ground truth. We ﬁnd that our method performs well both in terms of
MSE  identifying the relevant factors and coverage of the conﬁdence intervals.
Finally  we apply our method to a more traditional IV application: estimating the effect of schooling
on wages. We use a well studied public data set and observe that our approach automatically identiﬁes
sources of heterogeneity that were previously uncovered using more structural approaches. We also
validate our method in this application on semi-synthetic data that emulate the true data.

2 Estimation of Heterogeneous Treatment Effects with Instruments

We consider estimation of heterogeneous treatment effects with respect to a set of features X  of
an endogenous treatment T on an outcome Y with an instrument Z. For simplicity of exposition 
we will restrict attention to the case where Y  Z and T are scalar variables  but several of our results
extend to the case of multi-dimensional treatments and instruments. Z is an instrumental variable if it
has an effect on the treatment but does not have a direct effect on the outcome other than through the
treatment. More formally  we assume the following moment condition:

E[Y − θ0(X)T − f0(X) | Z  X] = 0

(1)
Equivalently we assume that: Y = θ0(X) T + f0(X) + e  with E[e | Z  X] = 0. We allow for the
presence of confounders  i.e. e could be correlated with T via some unobserved common factor ν.
However  our exclusion restriction on the instrument implies that the residual is mean zero conditional
on the instrument. Together with the fact that the instrument also has an effect on the treatment at
any value of the feature X  i.e.: Var(E[T | Z  X] | X) ≥ λ  allows us to identify the heterogeneous
effect function θ0(X). We focus on the case where the effect is linear in the treatment T   which is
wlog in the binary treatment setting  which is our main application  and since our goal is to focus on
the non-linearity wrt X (this greatly simpliﬁes our problem  see [9  10  25  16]).2
Given n i.i.d. samples from the data generating process  our goal is to estimate a model ˆθ(X) that
achieves small expected mean-squared-error  i.e.: E[(cid:107)ˆθ − θ0(cid:107)2] := E[(ˆθ(X) − θ0(X))2] ≤ Rn.
Since the true θ0 function can be very complex and difﬁcult to estimate in ﬁnite samples  we are
also interested in estimating projections of the true θ0 on simpler hypothesis spaces Θπ. Projections
are also useful for interpretability: one might want to understand what is the best linear projection
of θ0(X) on X  i.e. α0 = arg minα E[((cid:104)α  X(cid:105) − θ0(X))2]. In this case we will denote with θ∗
E[(θ(X) − θ0(X))2] and our goal would be
the projection of θ0 on Θπ  i.e. θ∗ = arg minθ∈Θπ
to achieve small mean squared error with respect to θ∗. When θ∗ is a low dimensional parametric
class (e.g. a linear function on a low-dimensional feature space or a constant function)  we are also
interested in performing inference; i.e. constructing conﬁdence intervals that asymptotically contain
the correct parameter with probability equal to some target conﬁdence level.
Warm-Up: Estimating the Average Treatment Effect (ATE) For estimation of the average treat-
ment effect (ATE)  assuming that either there is no effect heterogeneity with respect to X or there is
no heterogeneous compliance with respect to X  [11] propose a method for estimating the ATE that
solves the empirical analogue of the following moment equation:

E[(Y − E[Y | X] − θ(T − E[T | X])) (Z − E[Z | X])] = 0

(2)
This moment function is orthogonal to all the functions q0(X) = E[Y | X]  p0(X) = E[T | X] and
r0(X) = E[Z | X] that also need to be estimated from data. This moment avoids the estimation of
the expected T conditional on Z  X and satisﬁes an orthogonality condition that enables robustness of
En[(Y −ˆq(X)) (Z−ˆr(X))]
the estimate ˆθ =
En[(T− ˆp(X)) (Z−ˆr(X))]   to errors in the nuisance estimates ˆq  ˆr and ˆp. The estimate
is asymptotically normal with variance equal to the variance of the method if the estimates were the

2Implicitly our moment condition abstracts away the low level conditions that allow one to interpret the
parameter that satisﬁes the moment condition as causal. For instance  in the case of binary instruments and binary
treatments to interpret the solution to the moment condition as the causal effect  one requires a monotonicity
condition on the compliance structure [18]  i.e. if a unit does not take the treatment when recommended  it would
have also not taken the treatment without the recommendation. However  the fact that we also condition on X
and we estimate an effect conditional on X only requires these conditions to hold conditional on X  i.e. the
directionality of the effect of the instrument on the treatment can change for different X’s. So the requirements
are milder. This weakening has also been observed in prior work in econometrics [19].

3

correct ones  assuming that the mean squared error of these estimates decays at least at a rate of n−1/4
(see [11] for more details). This result requires that the nuisance estimates are ﬁtted in a cross-ﬁtting
manner  i.e. we use half of the data to ﬁt a model for each of these functions and then predict the
values of the model on the other half of the samples. We refer to this algorithm as DMLATEIV.3
Inconsistency under Effect and Compliance Heterogeneity The above estimate ˆθ is a consistent
estimate of the average treatment effect as long as there is either no effect heterogeneity with respect
to X or there is no heterogeneous compliance (i.e. the effect of the instrument on the treatment)
with respect to X. Otherwise it is inconsistent. The reason is that  if we let ˜T = T − p0(X)
and ˜Z = Z − r0(X)  then the population quantity: β0(X) = E[ ˜T ˜Z | X] is a function of X. If
we also have effect heterogeneity  then we are solving for a constant ˆθ that in the limit satisﬁes:
E[( ˜Y − ˆθ ˜T ) ˜Z] = 0  where ˜Y = Y − q0(X). On the other hand the true heterogeneous model
satisﬁes the equation: E[( ˜Y − θ0(X) ˜T ) ˜Z] = 0. In the limit  the two quantities are related via the
equation: ˆθ E[ ˜T ˜Z] = E[θ0(X) ˜T ˜Z]. Then the constant effect that we estimate converges to the
quantity: ˆθ =
. If θ0(X) is not independent with β0(X)  then ˆθ is a re-weighted version
of the true average treatment effect E[θ(X)]  re-weighted by the heterogeneous compliance. To
account for this heterogeneous compliance we need to change our moment equation so as to re-weight
based on β0(X)  which is unknown and also needs to be estimated from data. Given that this function
could be arbitrarily complex  we want our ﬁnal estimate to be robust to estimation errors of β0(X).
We can achieve this by considering a doubly robust approach to estimating ˆθ. Suppose that we had
some other method of computing an estimate of the heterogeneous treatment effect θ0(X)  then we
can combine both estimates to get a more robust method for the ATE  e.g.:

E[θ0(X)β0(X)]

E[β0(X)]

ˆθDR = E(cid:104)ˆθ(X) + ( ˜Y −ˆθ(X) ˜T ) ˜Z

(cid:105)

ˆβ(X)

(3)

This approach has been analyzed in [27] in the case of constant treatment effects and an analogue
of this average effect was also used by [5] in a policy learning problem as opposed to an estima-
tion problem. In particular  the quantity ˜Z/β(X) is known as the compliance score [1  3]. Our
methodological contribution in the next two sections is two-fold: i) ﬁrst we propose a model-based
stable approach for estimating a preliminary estimate ˆθ(X)  which does not necessarily require that
β(X) > 0 everywhere (an assumption that is implicit in the latter method)  ii) second we show that
this doubly robust quantity can be used as a regression target and minimizing the square loss with
respect to this target  corresponds to an orthogonal loss  as deﬁned in [12  13].

2.1 Preliminary Estimate of Conditional Average Treatment Effect (CATE)
Let h0(Z  X) = E[T | Z  X] and p0  q0 as in the previous section. Then observe that
we can re-write the moment condition as: E[Y − θ0(X) h0(Z  X) − f0(X) | Z  X] = 0.
Moreover  observe that the functions p0  q0 and f0 are related via: q0(X) = θ0(X) p0(X) +
f0(X). Thus we can further re-write the moment condition in terms of q0  p0 instead of f0:
E[Y − q0(X) − θ0(X) (h0(Z  X) − p0(X)) | Z  X] = 0. Moreover  we can identify θ(X)
with the following subset of conditional moments  where the conditioning of Z is removed:
E[(Y − q0(X) − θ(X) (h0(Z  X) − p0(X))) (h0(Z  X) − p0(X)) | X] = 0. Equivalently  θ(X)
is a minimizer of the square loss:

L1(θ; q0  h0  p0) := E(cid:104)

(Y − q0(X) − θ(X) (h0(Z  X) − p0(X)))2(cid:105)

(4)

since the derivative of this loss with respect to θ(X) is equal to the moment equation and  thus  the
ﬁrst order condition for the loss minimization problem is satisﬁed by the true model θ0. Moreover  if
the loss function satisﬁes a functional analogue of strong convexity  then any minimizer of the loss

3For Double Machine Learning ATE estimation with Instrumental Variables.

4

achieves small mean squared error with respect to θ0. This leads to the following approach:

Algorithm 1: HETEROGENEOUS EFFECTS: DMLIV Partially orthogonal  convex loss.

1 On a half-sample S1: regress i) Y on X  ii) T on X  Z  iii) T on X  to learn estimates ˆq  ˆh and ˆp corr.;
2 Minimize the empirical analogue of the square loss over some hypothesis space Θ on the other half-sample S2:

(cid:80)

ˆθ = arg inf θ∈Θ

2
n

i∈S2

(Yi − ˆq(Xi) − θ(Xi) (ˆh(Zi  Xi) − ˆp(Xi)))2 := L1

n(θ; ˆq  ˆh  ˆp)

(5)

or any learning algorithm that achieves small generalization error w.r.t. loss L1(θ; ˆq  ˆh  ˆp) over Θ.

This method is an extension of the classical two-stage-least-squares (2SLS) approach [2] to allow
for arbitrary machine learning models; ignoring the residualization part (i.e. if for instance q(X) =
p(X) = 0)  then it boils down to: 1) predict the mean treatment from the instrument and X with
an arbritrary regression/classiﬁcation method  2) predict the outcome from the predicted treatment
multiplied by the heterogeneous effect model θ(X). Residualization helps us remove the dependence
of the mean squared error on the complexity of the baseline function f0(X). We achieve this by
showing that this loss is orthogonal with respect to p  q (see [13] for the deﬁnition of an orthogonal
loss). However  orthogonality does not hold with respect to h. This ﬁnding is reasonable since we are
using h(Z  X) as our regressor. Hence  any error in the measurement of the regressor can directly
propagate to an error in θ(X). This is the same reason why in classical IV regression one cannot
ignore the variance from the ﬁrst stage of 2SLS when calculating conﬁdence intervals.
Lemma 1. The loss function L1(θ; q  h  p) is orthogonal to the nuisance functions p  q  but not h.
Strong convexity and overlap. Note that both the empirical loss L1
n and the population loss L1
are convex in the prediction  which typically implies computational tractability. Moreover  the
second order directional derivative of the population loss in any functional direction θ(·) − θ0(·) is:
. To be
able to achieve mean-squared-error rates based on our loss minimization  we need the population
version L1 of the loss function to satisfy a functional analogue of λ-strong convexity:

(ˆh(Z  X) − ˆp(X))2 (θ(X) − θ0(X))2(cid:105)

and let: V (X) := E(cid:104)

(ˆh(Z  X) − ˆp(X))2 | X

E(cid:104)

(cid:105)

4 (cid:107)p− p0(cid:107)2

∀θ ∈ Θ : E[V (X) · (θ(X) − θ0(X))2] ≥ λ E[(θ(X) − θ0(X))2]

(6)
This setting falls under the “single-index” setup of [13]. Using arguments from Lemma 1 of [13]  if:
(7)

where V0(X) := E(cid:2)(h0(Z  X) − p0(X))2 | X(cid:3) = Var(E[T | Z  X] | X)  then λ ≥ λ0 − O((cid:107)h −

∀θ ∈ Θ : E[V0(X) · (θ(X) − θ0(X))2] ≥ λ0 E[(θ(X) − θ0(X))2]

4) = λ0− o(1). A sufﬁcient condition is that V0(X) ≥ λ0 for all X. This is a standard
h0(cid:107)2
"overlap" condition that the instrument is exogenously varying at any X and has a direct effect on the
treatment at any X. DMLIV only requires an "average" overlap condition  tailored particularly to the
hypothesis space Θ  hence it could handle settings where the instrument is weak for some subset of
the population. For instance  if Θ is a linear function class: Θ = {(cid:104)θ  φ(X)(cid:105) : θ ∈ S ⊆ Rd}  then for
the oracle strong convexity to hold it sufﬁces that: E[V (X)φ(X)φ(X)T ] (cid:23) λI. Lemma 1  combined
with the above discussion and the results of [13] yields:4
Corollary 2. Assume all random variables are bounded and consider any algorithm that achieves
n with respect to loss L1(θ; ˆq  ˆh  ˆp). Moreover  suppose that the
expected generalization error R2
nuisance estimates satisfy (cid:107)ˆq − q0(cid:107)4 (cid:107)ˆp − p0(cid:107)4 = o(gn) and (cid:107)ˆh − h0(cid:107)4 = o(hn). Then ˆθ returned
2 ≤ O
by DMLIV satisﬁes: (cid:107)ˆθ − θ0(cid:107)2
. If empirical risk minimization is used in the ﬁnal
stage  then R2
n  where δn is the critical radius of the hypothesis space Θ as deﬁned
n + g4
via the localized Rademacher complexity [20].
Computational considerations. The empirical loss L1

n is not a standard square loss. However  we
i γ(Xi)2( ˜Yi/γ(Xi) − θ(Xi))2. Thus the problem is equivalent to a standard
square loss minimization with label ˜Yi/γ(Xi) and sample weights γ(Xi)2. Thus we can use any
out-of-the-box machine learning method that accepts sample weights  such as stochastic gradient
based regression methods and gradient boosted or random forests. Alternatively  if we assume a linear
representation of the effect function θ(X) = (cid:104)θ  φ(X)(cid:105)  then the problem is equivalent to regressing
˜Y on the scaled features φ(X) γ(X)  and again any method for ﬁtting linear models can be invoked.

can re-write it as(cid:80)

(cid:16) R2

n = δ2

n + h2

(cid:17)

n+g4

n

n+h2
λ0

4This corollary follows by small modiﬁcations of the proofs of Theorem 1 and Theorem 3 of [13] that

accounts for the non-orthogonality w.r.t. h  so we omit its proof.

5

2.2 DRIV: Orthogonal Loss for IV Estimation of CATE and Projections

We now present the main estimation algorithm that combines the doubly robust approach presented for
ATE estimation with the preliminary estimator of the CATE to obtain a fully orthogonal and strongly
convex loss. This method achieves a second order effect from all nuisance estimation errors and
enables oracle rates for the target effect class Θ and asymptotically valid inference for low dimensional
target effect classes. In particular  given access to a ﬁrst stage model of heterogeneous effects θpre
(such as the one produced by DMLIV)  we can estimate a more robust model of heterogeneous effects
via minimizing a square loss that treats the doubly robust quantity used in Equation (3) as the label:

minθ∈Θπ L2(θ; θpre  β  p  q  r) := E

θpre(X) + ( ˜Y −θpre(X) ˜T ) ˜Z

β(X)

− θ(X)

(8)

(cid:17)2(cid:21)

(cid:20)(cid:16)

We allow for a model space Θπ that is not necessarily equal to Θ. The solution in Equation (3) is a
special case of this minimization problem where the space Θπ contains only constant functions. Our
main result shows that this loss is orthogonal to all nuisance functions θpre  ˆβ  ˆq  ˆp  ˆr. Moreover  it is
strongly convex in the prediction θ(X)  since conditional on all the nuisance estimates it is a standard
square loss. Moreover  we show that the loss is orthogonal irrespective of what the model space Θπ 
even if Θπ (cid:54)= Θ  as long as the preliminary estimate θpre is consistent with respect to the true CATE
θ0 (i.e. ﬁt a ﬂexible preliminary CATE and use it to project to a simpler hypothesis space).
Lemma 3. The loss L2 is orthogonal with respect to the nuisance functions θpre  β  p  q and r.
Algorithm 2: DRIV Orthogonal convex loss for CATE and projections of CATE

1 Estimate a preliminary estimate θpre of the CATE θ0(X) using DMLIV on half-sample S1;
2 Using half-sample S1  regress i) Y on X  ii) T on X  iii) Z on X to learn estimates ˆq  ˆp  ˆr correspondingly;
3 Regress T · Z on X using S1 to learn estimate ˆf of function f0(X) = E[T · Z | X];
4 ∀i ∈ S2  let ˜Yi = Yi − ˆq(Xi)  ˜Ti = Ti − ˆp(Xi)  ˜Zi = Zi − ˆr(Xi)  ˆβ(Xi) = ˆf (Xi) − ˆp(Xi) ˆr(Xi);
5 Minimize empirical analogue of square loss L2 over hypothesis space Θπ on the other half-sample S2  i.e.:

(cid:80)

(cid:16)

(cid:17)2

ˆθDR = arg inf θ∈Θπ

2
n

i∈S2

θpre(Xi) + ( ˜Yi− ˆθ(Xi) ˜Ti) ˜Zi

− θ(Xi)

ˆβ(Xi)

:= L2

n(θ; θpre  ˆβ  ˆp  ˆq  ˆr)

or any learning algorithm that has small generalization error w.r.t. loss L2(θ; θpre  ˆβ  ˆp  ˆq  ˆr) on Θπ.

(cid:16)

n + g4
n

n = δ2

n + g4

2 ≤ O(cid:0)R2

(cid:1)  where θ∗ = arg minθ∈Θπ L2(θ; θ0  β0  p0  q0  r0). If empirical risk

If we use DMLIV for θprel  even though DMLIV has a ﬁrst order impact from the error of h  the
second stage estimate has a second order impact  since it has a second order impact from the ﬁrst
stage CATE error. Lemma 3 together with the results of [13] and [12] implies the following corollary:
Corollary 4. Assume all random variables are bounded and consider any algorithm that achieves
n with respect to loss L2(θ; θpre  ˆβ  ˆp  ˆq  ˆr). Moreover  suppose that
expected generalization error R2
each nuisance estimate ˆg ∈ {θpre  ˆβ  ˆp  ˆq  ˆr}  (cid:107)ˆg − g0(cid:107)4 ≤ gn. Then ˆθ returned by DRIV satisﬁes:
(cid:107)ˆθ − θ∗(cid:107)2
minimization is used in the ﬁnal stage  then R2
n  where δn is the critical radius of the
hypothesis space Θ as deﬁned via the localized Rademacher complexity [20]. If Θ is high-dimensional
sparse linear  i.e. θ(X) = (cid:104)ξ  φ(X)(cid:105) with (cid:107)ξ(cid:107)0 ≤ s  φ(X) ∈ Rp and E[φ(X)φ(X)T ] ≥ λ0I 
then if an (cid:96)1-penalized square loss minimization is used in the ﬁnal step of DRIV  it sufﬁces that
(cid:107)ˆg − g0(cid:107)2 ≤ gn to get: (cid:107) ˆξ − ξ∗(cid:107)2
Interpretability through projections. The fact that our loss function can be used with any
target Θπ allows us to perform inference on the projection of θ0 on a simple space Θπ (e.g.
decision trees  linear functions) for interpretability purposes.
the label in the
ﬁnal regression of DRIV  then observe that when the nuisance estimates take their true val-
ues then E[Y DR
has mean zero. Hence:
L2(θ; θ0  β0  p0  q0  r0) = Var(θDR(X)) + E[(θ0(X) − θ(X))2]. The ﬁrst part is independent of
θ and hence minimizing the oracle L2 is equivalent to minimizing E[(θ0(X) − θ(X))2] over θ ∈ Θπ 
which is exactly the projection of θ0 on Θπ. One version of an interpretable model is estimating the
CATE with respect to a subset T of the variables  i.e.: θ(XT ) = E[θ0(X) | XT ] (e.g. how treatment
effect varies with a single feature). This boils down to setting Θπ some space of functions of XT .
If T is a low dimensional set of features and Θπ is a the space of linear functions of XT   i.e.
Θπ = {X → (cid:104)θT   XT(cid:105) : θT ∈ R|T|}  then the ﬁrst order condition of our loss is equal to the

| X] = θ0(X)  since the second part of Y DR

s2 log(p)/n+g4

n

If we let Y DR

i

2 ≤ O

(cid:17)

.

λ0

i

i

6

moment condition E[(Y DR − (cid:104)θT   XT(cid:105))XT ] = 0. Then orthogonality of our loss implies that DRIV
is equivalent to an orthogonal moment estimation method [11]. Thus using the results of [11] we
get that the estimate ˆθT of DRIV is asympotically normal with asymptotic variance equal to the
hypothetical variance of θT as if the nuisance estimates had their true values. Hence  we can use
out-of-the-box packages for calculating CIs of an OLS regression to get p-values on the coefﬁcients.

3 Estimating Effects of Membership at TripAdvisor
We apply our methods to estimate the treatment effect of membership on the number of days a user
visits TripAdvisor. The instrument used was a 14-day intent-to-treat A/B test run during 2018  where
users in group A received a new  easier membership sign-up process  while the users in group B did
not. The treatment is whether a user became a member or not. Becoming a member and logging
into TripAdvisor gives users exclusive access to trip planning tools  special deals and price alerts 
and personalized ideas and travel advice. Our data consists of 4 606 041 total users in a 50:50 A/B
test. For each user  we have a 28-day pre-experiment summary about their browsing and purchasing
activity on TripAdvisor (see Sec. B.2). The instrument signiﬁcantly increased the rate of treatment 
and is assumed to satisfy the exclusion restriction.
We applied two sets of nuisance estimation models with different complexity characteristics: LASSO
regression and logistic regression with an L2 penalty (LM); and gradient boosting regression and
classiﬁcation (GB). The only exception was E[Z|X]  where we used a ﬁxed estimate of 0.5 since the
instrument was a large randomized experiment. See Sec. B.1 for details.5
Method
Nuisance

Nuisance

GB
GB

ATE Est [95% CI]
DMLATEIV 0.127 [-0.031  0.285]
0.125 [-0.061  0.311]

DRIV

LM
LM

Method

ATE Est [95% CI]
DMLATEIV 0.117 [-0.051  0.285]
0.113 [-0.052  0.279]

DRIV

Table 1: ATE Estimates for 2018 Experiment at TripAdvisor

We estimate the ATE using DRIV projected onto a constant (Table 1). Using linear nuisance models
results in very similar ATE estimates between DMLATEIV and DRIV. We compare the X co-variate
associations for both heterogeneity and compliance under DRIV to understand why. If there are
co-variates with signiﬁcant non-zero associations in both heterogeneity and compliance  this could
lead to different estimates between DRIV and DMLATEIV (and vice versa). Replacing the CATE
projection model with a linear regression  we obtain valid inferences for the co-variates associated
with treatment effect heterogeneity (Figure 1). For compliance  we run a linear regression of the
estimated quantity β(X) on X  to assess its association with each of the features (see Sec. B.1 for
details). Comparing treatment and compliance coefﬁcients  os_type_linux and revenue_pre are the
only coefﬁcients substantially different from 0 in both. However  only a very small proportion of
users in the experiment are Linux users  and the distribution of revenue is very positively skewed.
This justiﬁes the minor difference between the DMLATEIV and DRIV estimates. Moreover  we ﬁt
a shallow  heavily regularized random forest and interpret it using Shapley Additive Explanations
(SHAP) [24]. SHAP gave directionally similar impact of each feature on the effect (Figure 1).
However  since we constrained the model to have depth at most one  it essentially gives the features
in order of importance if we were to split the population based on a single feature. This justiﬁes why
the order of importance of features in the forest is not in the same order as the magnitude of the rank
in the linear model  since they have different interpretations. The features picked up by the forest
intuitively make sense since an already highly engaged member of TripAdvisor  or a user who has
recently made a booking  is less likely to further increase their visits to TripAdvisor. Using gradient
boosting nuisance models  we show that many inferences remain similar (Figure 3 in Appendix). The
most notable changes in heterogeneity were for features which have a highly skewed distribution (e.g.
visits to speciﬁc pages on TripAdvisor)  or which appear rarely in the data (e.g. Linux users). The
linear CATE projection model coefﬁcients are largely similar for both residualization models (except
the Linux operating system feature  which appears rarely in the data). Moving to a random forest for
the CATE projection model with SHAP presents greater differences  especially for the highly skewed
features.
Similar instrument from a recent experiment A recent 2019 A/B test of the same membership
sign-up process provided another viable instrument. This 21-day A/B test included a much larger 
more diverse population of users than in 2018 due to fewer restrictions for eligibility (see Sec. B.2 for

5We attempted to use the R implementation of Generalized Random Forests (GRF)[4] to compare with our
results. However  we could not ﬁt due to the size of the data and insufﬁcient memory errors (with 64GB RAM).

7

Figure 1: (From left to right) Linear CATE projection  SHAP summary of random forest CATE projection 
Linear CATE projection coefﬁcients. Using linear nuisance models.

details). We apply DRIV with gradient boosting residualization models and a linear projection of the
CATE. The CATE distribution has generally higher values compared to the 2018 experiment which
reﬂects the different experimental population. In particular  users in the 2018 experiment had much
higher engagement and signiﬁcantly higher revenue in the pre-experiment period. This was largely
because users were only included in the 2018 experiment on their second visit. The higher baseline
naturally makes it more difﬁcult to achieve high treatment effects  explaining the generally lower
CATE distribution in the 2018 experiment. We note that  unlike in 2018  the revenue coefﬁcient is
no longer signiﬁcant. We again attribute this to the much higher revenue baseline in 2018. Despite
the population differences  however  we observe "days_visited_vrs_pre" continues to have a very
signiﬁcant positive association. "days_visited_exp_pre" now also appears to have a signiﬁcantly
positive association  as does the iPhone device (which was not a feature in the 2018 experiment). The
inclusion of iPhone users is another big domain shift in the two experiments.
Policy recommendations for Trip Advisor Our results offer several policy implications for Trip
Advisor. Firstly  encourage iPhone users  and users who frequent vacation rentals pages to sign-up
for membership. These users exhibited high treatment effects from membership. For frequent visitors
to vacation rentals pages  this effect was robust across residualization models  CATE projections 
and even different instruments (e.g. by providing stronger encouragements for sign-up on particular
sub-pages). Secondly  ﬁnd ways to improve the membership offering for users who are already
engaged: e.g. recently made a booking (high revenue_pre)  were already frequent visitors (high
days_visited_free_pre).
Validation on Semi-Synthetic Data In Appendix C  we validate the correctness of ATE and CATE
from DRIV  by creating a semi-synthetic dataset with the same variables and such that the marginal
distribution of each variable looks similar to the TripAdvisor data  but where we know the true effect
model. We ﬁnd that DRIV recovers a good estimate of the ATE. The CATE of DRIV with linear
regression as ﬁnal stage also recovers the true coefﬁcients  and a random forest ﬁnal stage picks
the correct factors of heterogeneity as most important features. Moreover  coverage of DRIV ATE
conﬁdence intervals is almost nominal at 94%  while DMLATEIV can be very biased and have 0
coverage. 6
4 Estimating the Effect of Schooling on Wages
The causal impact of schooling on wages has been studied at length in Economics (see [14]  [6] 
[7]  [17])  and although it is generally agreed that there is a positive impact  it is difﬁcult to obtain
a consistent estimate of the effect due to self-selection into education levels. To account for this
endogeneity  Card ([6]) proposes using proximity to a 4-year college as an IV for schooling. We
analyze Card’s data from the Nat. Long. Survey of Young Men (NLSYM  1966) to estimate the
ATE of education on wages and ﬁnd sources of heterogeneity. We describe the NLSYM data in
depth in Appendix D. At high level  the data contains 3 010 rows with 22 mostly binary covariates
X  log wages (y)  years of schooling (T )  and 4-year college proximity indicator (Z). We apply
DMLATEIV and DRIV with linear (LM) or gradient boosted (GBM) nuisance models to estimate
the ATE (Table 2 and Table 8 in Appx. D). While the DMLATEIV results are consistent with Card’s
(0.134  [0.026  0.242] 95% CI)  this estimate is likely biased in the presence of compliance and effect
heterogeneity (see Sec. 2). The DRIV ATE estimates  albeit lower  still lie within the 95% CI of the

6Results on the coverage experiment can be recovered by running coverage.py followed by post-processing
with post_processing.ipynb at https://github.com/microsoft/EconML/tree/master/prototypes/dml_iv. Single
synthetic instance results on the quality of the recovered estimates and comparisons with benchmark approaches
can be found in TA_DGP_Analysis.ipynb at the same location.

8

3210123Treatment Effect05000001000000150000020000002500000FrequencyATE Estimate: 0.113Linear CATE (Linear Residualization)4202Coefficient Valueos_type_linuxrevenue_predays_visited_free_preinterceptlocale_en_USdays_visited_rs_predays_visited_exp_preos_type_osxdays_visited_hs_predays_visited_fs_predays_visited_vrs_preLinear Model ResidualizationNuisance

LM
LM

Method

DMLATEIV

DRIV

ATE Est
0.137
0.065

† Contains the true ATE (0.609)

Table 2: NLSYM ATE Estimates for Observational and Semi-synthetic Data

Observational Data
95% CI

Semi-Synthetic Data

95% CI

ATE Est
0.654
0.587

[0.027  0.248]
[-0.02  0.151]
‡ Coverage for 95% CI over 100 Monte Carlo simulations

[0.621  0.687]
[0.521  0.652]†

Cover ‡
10%
92%

DML ATE. We study effect heterogeneity with a shallow random forest an the last stage of DRIV.
Fig. 2 depicts the spread of treatment effects  and the important features selected. Most effects (89%)
are positive  with very few very negative outliers. The heterogeneity is driven mainly by parental
education variables. We project the DRIV treatment effect on the mother’s education variable to
study this effect. In ﬁg. 2  we note that treatment effects are highest among children of less educated
mothers. This pattern has also been observed in [6] and [17].

Figure 2: Treatment effect distribution  heterogeneity features  and linear projection on mother’s education.

Semi-synthetic Data Results. We created semi-synthetic data from the NLSYM covariates X and in-
strument Z  with generated treatments and outcomes based on known compliance and treatment func-
tions (see Appx. D for details). In Table 2  we see that DMLATEIV ATE (true ATE=0.609) is upwards
biased and has poor coverage over 100 runs  whereas the DRIV ATE is less biased and has overall good
coverage. With DRIV  we also recover the correct θ(X) coefﬁcients: 0.142 ([0.037  0.245] 95% CI)
vs 0.1  0.049 ([0.015  0.083]) vs 0.05  and −0.147 ([−0.365  0.072]) vs −0.1.

5 Acknowledgements

We thank Jeff Palmucci  Brett Malone  Baskar Mohan  Molly Steinkrauss  Gwyn Fisher and Matthew
Dacey from TripAdvisor for their support and assistance in making this collaboration possible.

References
[1] Alberto Abadie. Semiparametric instrumental variable estimation of treatment response models. Journal

of Econometrics  113(2):231 – 263  2003.

[2] Joshua D Angrist and Jörn-Steffen Pischke. Mostly harmless econometrics: An empiricist’s companion.

Princeton university press  2008.

[3] Peter M Aronow and Allison Carnegie. Beyond late: Estimation of the average treatment effect with an

instrumental variable. Political Analysis  21(4):492–506  2013.

[4] Susan Athey  Julie Tibshirani  Stefan Wager  et al. Generalized random forests. The Annals of Statistics 

47(2):1148–1178  2019.

[5] Susan Athey and Stefan Wager. Efﬁcient policy learning. arXiv preprint arXiv:1702.02896  2017.

[6] David Card. Using geographic variation in college proximity to estimate the return to schooling. Technical

report  National Bureau of Economic Research  1993.

[7] David Card. Estimating the return to schooling: Progress on some persistent econometric problems.

Econometrica  69(5):1127–1160  2001.

9

%70 92039110.9706:03.0.20.00.2SHAP value (impact on model output)fatheduc_nansinmom14reg662southexperexpersqreg666motheduc_nanmotheducfatheducLowHighFeature value202Mother's Education (scaled)0.40.20.00.20.40.60.8Treatment EffectUpper 95% CILower 95% CIPrediction[8] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  KDD ’16  pages
785–794  New York  NY  USA  2016. ACM.

[9] Xiaohong Chen and Zhipeng Liao. Sieve semiparametric two-step gmm under weak dependence. Cowles
Foundation Discussion Papers 2012  Cowles Foundation for Research in Economics  Yale University  2015.

[10] Xiaohong Chen and Demian Pouzo. Estimation of nonparametric conditional moment models with possibly

nonsmooth generalized residuals. Econometrica  80(1):277–321  2012.

[11] Victor Chernozhukov  Denis Chetverikov  Mert Demirer  Esther Duﬂo  Christian Hansen  Whitney Newey 
and James Robins. Double/debiased machine learning for treatment and structural parameters. The
Econometrics Journal  21(1):C1–C68  2018.

[12] Victor Chernozhukov  Denis Nekipelov  Vira Semenova  and Vasilis Syrgkanis. Plug-in regularized estima-
tion of high-dimensional parameters in nonlinear semiparametric models. arXiv preprint arXiv:1806.04823 
2018.

[13] Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036 

2019.

[14] Zvi Griliches. Estimating the returns to schooling: Some econometric problems. Econometrica: Journal

of the Econometric Society  pages 1–22  1977.

[15] Jason Hartford  Greg Lewis  Kevin Leyton-Brown  and Matt Taddy. Deep IV: A ﬂexible approach
for counterfactual prediction. In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th
International Conference on Machine Learning  volume 70 of Proceedings of Machine Learning Research 
pages 1414–1423  International Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[16] Jason Hartford  Greg Lewis  Kevin Leyton-Brown  and Matt Taddy. Deep iv: A ﬂexible approach for

counterfactual prediction. In International Conference on Machine Learning  pages 1414–1423  2017.

[17] John Hudson and John G Sessions. Parental education  labor market experience and earnings: new wine in

an old bottle? Economics Letters  113(2):112–115  2011.

[18] Guido W. Imbens and Joshua D. Angrist. Identiﬁcation and estimation of local average treatment effects.

Econometrica  62(2):467–475  1994.

[19] Tobias J. Klein. Heterogeneous treatment effects: Instrumental variables without monotonicity? Journal

of Econometrics  155(2):99 – 116  2010.

[20] Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems.

Springer  2011.

[21] Sören R Künzel  Jasjeet S Sekhon  Peter J Bickel  and Bin Yu. Meta-learners for estimating heterogeneous

treatment effects using machine learning. arXiv preprint arXiv:1706.03461  2017.

[22] Scott M Lundberg  Gabriel G Erion  and Su-In Lee. Consistent individualized feature attribution for tree

ensembles. arXiv preprint arXiv:1802.03888  2018.

[23] Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In I. Guyon  U. V.
Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural
Information Processing Systems 30  pages 4765–4774. Curran Associates  Inc.  2017.

[24] Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In I. Guyon  U. V.
Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural
Information Processing Systems 30  pages 4765–4774. Curran Associates  Inc.  2017.

[25] Whitney K. Newey and James L. Powell. Instrumental variable estimation of nonparametric models.

Econometrica  71(5):1565–1578  2003.

[26] Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. arXiv preprint

arXiv:1712.04912  2017.

[27] Ryo Okui  Dylan S Small  Zhiqiang Tan  and James M Robins. Doubly robust instrumental variable

regression. Statistica Sinica  pages 173–205  2012.

10

,Vasilis Syrgkanis
Victor Lei
Miruna Oprescu
Maggie Hei
Keith Battocchi
Greg Lewis