2013,Multisensory Encoding  Decoding  and Identification,We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons  and prove that only a projection of the circuit onto input stimuli can be identified. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms.,Multisensory Encoding  Decoding  and Identiﬁcation

Department of Electrical Engineering

Department of Electrical Engineering

Yevgeniy B. Slutskiy∗

Columbia University
New York  NY 10027

ys2146@columbia.edu

Aurel A. Lazar

Columbia University
New York  NY 10027

aurel@ee.columbia.edu

Abstract

We investigate a spiking neuron model of multisensory integration. Multiple stim-
uli from different sensory modalities are encoded by a single neural circuit com-
prised of a multisensory bank of receptive ﬁelds in cascade with a population of
biophysical spike generators. We demonstrate that stimuli of different dimensions
can be faithfully multiplexed and encoded in the spike domain and derive tractable
algorithms for decoding each stimulus from the common pool of spikes. We also
show that the identiﬁcation of multisensory processing in a single neuron is dual
to the recovery of stimuli encoded with a population of multisensory neurons  and
prove that only a projection of the circuit onto input stimuli can be identiﬁed. We
provide an example of multisensory integration using natural audio and video and
discuss the performance of the proposed decoding and identiﬁcation algorithms.

1

Introduction

Most organisms employ a mutlitude of sensory systems to create an internal representation of their
environment. While the advantages of functionally specialized neural circuits are numerous  many
beneﬁts can also be obtained by integrating sensory modalities [1  2  3]. The perceptual advantages
of combining multiple sensory streams that provide distinct measurements of the same physical
event are compelling  as each sensory modality can inform the other in environmentally unfavorable
circumstances [4]. For example  combining visual and auditory stimuli corresponding to a person
talking at a cocktail party can substantially enhance the accuracy of the auditory percept [5].
Interestingly  recent studies demonstrated that multisensory integration takes place in brain areas that
were traditionally considered to be unisensory [2  6  7]. This is in contrast to classical brain models in
which multisensory integration is relegated to anatomically established sensory convergence regions 
after extensive unisensory processing has already taken place [4]. Moreover  multisensory effects
were shown to arise not solely due to feedback from higher cortical areas. Rather  they seem to be
carried by feedforward pathways at the early stages of the processing hierarchy [2  7  8].
The computational principles of multisensory integration are still poorly understood. In part  this is
because most of the experimental data comes from psychophysical and functional imaging experi-
ments which do not provide the resolution necessary to study sensory integration at the cellular level
[2  7  9  10  11]. Moreover  although multisensory neuron responses depend on several concurrently
received stimuli  existing identiﬁcation methods typically require separate experimental trials for
each of the sensory modalities involved [4  12  13  14]. Doing so creates major challenges  espe-
cially when unisensory responses are weak or together do not account for the multisensory response.
Here we present a biophysically-grounded spiking neural circuit and a tractable mathematical
methodology that together allow one to study the problems of multisensory encoding  decoding 
and identiﬁcation within a uniﬁed theoretical framework. Our neural circuit is comprised of a bank

∗The authors’ names are listed in alphabetical order.

1

point neuron model  e.g.  the IAF model  describes the mapping of the current vi(t) =(cid:80)

Figure 1: Multisensory encoding on neuronal level. (a) Each neuron i = 1  ...  N receives multiple stimuli
k)k∈Z. (b) A spiking
nm  m=1  ...  M  of different modalities and encodes them into a single spike train (ti
um
mvim(t) into spikes.

of multisensory receptive ﬁelds in cascade with a population of neurons that implement stimulus
multiplexing in the spike domain. The circuit architecture is quite ﬂexible in that it can incorporate
complex connectivity [15] and a number different spike generation models [16]  [17].
Our approach is grounded in the theory of sampling in Hilbert spaces. Using this theory  we show
that signals of different modalities  having different dimensions and dynamics  can be faithfully
encoded into a single multidimensional spike train by a common population of neurons. Some
beneﬁts of using a common population include (a) built-in redundancy  whereby  by rerouting  a
circuit could take over the function of another faulty circuit (e.g.  after a stroke) (b) capability to
dynamically allocate resources for the encoding of a given signal of interest (e.g.  during attention)
(c) joint processing and storage of multisensory signals/stimuli (e.g.  in associative memory tasks).
First we show that  under appropriate conditions  each of the stimuli processed by a multisensory
circuit can be decoded loss-free from a common  unlabeled set of spikes. These conditions provide
clear lower bounds on the size of the population of multisensory neurons and the total number of
spikes generated by the entire circuit. We then discuss the open problem of identifying multisen-
sory processing using concurrently presented sensory stimuli. We show that the identiﬁcation of
multisensory processing in a single neuron is elegantly related to the recovery of stimuli encoded
with a population of multisensory neurons. Moreover  we prove that only a projection of the circuit
onto the space of input stimuli can be identiﬁed. Finally  we present examples of both decoding and
identiﬁcation algorithms and demonstrate their performance using natural stimuli.

2 Modeling Sensory Stimuli  their Processing and Encoding

n1  ...  uM
nM

Our formal model of multisensory encoding  called the multisensory Time Encoding Machine
(mTEM) is closely related to traditional TEMs [18]. TEMs are real-time asynchronous mecha-
nisms for encoding continuous and discrete signals into a time sequence. They arise as models of
early sensory systems in neuroscience [17  19] as well as nonlinear sampling circuits and analog-
to-discrete (A/D) converters in communication systems [17  18]. However  in contrast to traditional
TEMs that encode one or more stimuli of the same dimension n  a general mTEM receives M input
of different dimensions nm∈N  m = 1  ...  M  and possibly different dynamics
stimuli u1
(Fig. 1a). The mTEM processes and encodes these signals into a multidimensional spike train using
a population of N neurons. For each neuron i = 1  ...  N  the results of this processing are aggregated
into the dendritic current vi ﬂowing into the spike initiation zone  where it is encoded into a time
sequence (ti
Similarly to traditional TEMs  mTEMs can employ a myriad of spiking neuron models. Several
examples include conductance-based models such as Hodgkin-Huxley  Morris-Lecar  Fitzhugh-
Nagumo  Wang-Buzsaki  Hindmarsh-Rose [20] as well as simpler models such as the ideal and
leaky integrate-and-ﬁre (IAF) neurons [15]. For clarity  we will limit our discussion to the ideal IAF
neuron  since other models can be handled as described previously [20  21]. For an ideal IAF neuron
with a bias bi ∈ R+  capacitance C i ∈ R+ and threshold δi ∈ R+ (Fig. 1b)  the mapping of the
current vi into spikes is described by a set of equations formerly known as the t-transform [18]:

k denoting the timing of the kth spike of neuron i.

k)k∈Z  with ti

k+1

(cid:90) ti

vi(s)ds = qi
k 

k ∈ Z 

(1)

where qi
k = C iδi − bi(ti
providing a measurement qi

ti
k
k). Intuitively  at every spike time ti
k+1 − ti
k of the current vi(t) on the time interval [ti

k+1  the ideal IAF neuron is
k  ti

k+1).

2

u1n1(x1 ... xn1)uMnM(x1 ... xnM)(tik)k∈Zvi1(t)viM(t)vi(t)ΣField1ReceptiveNeuroniReceptiveFieldM+voltageresetto0bi1Ci(cid:2)δi(tik)k∈Zvi(t)(a)(b)2.1 Modeling Sensory Inputs

We model input signals as elements of reproducing kernel Hilbert spaces (RKHSs) [22]. Most
real world signals  including natural stimuli can be described by an appropriately chosen RKHS
[23]. For practical and computational reasons we choose to work with the space of trigonometric
polynomials Hnm deﬁned below  where each element of the space is a function in nm variables
(nm ∈ N  m = 1  2  ...  M). However  we note that the results obtained in this paper are not limited
to this particular choice of RKHS (see  e.g.  [24]).
Deﬁnition 1. The space of trigonometric polynomials Hnm is a Hilbert space of complex-valued
functions

um
l1...lnm

L1(cid:88)l1=−L1

um
nm(x1  ...  xnm ) =

Lnm(cid:88)lnm =−Lnm
l1...lnm∈ C and the functions el1...lnm (x1  ...  xnm) =
n=1 jlnΩnxn/Ln(cid:17)/(cid:112)T1 ··· Tnm   with j denoting the imaginary number. Here Ωn is the

over the domain Dnm =(cid:81)nm
exp(cid:16)(cid:80)nm
bandwidth  Ln is the order  and Tn = 2πLn/Ωn is the period in dimension xn. Hnm is endowed
with the inner product (cid:104)· ·(cid:105) : Hnm × Hnm → C  where
um
nm (x1  ...  xnm )wm

nm (x1  ...  xnm )dx1...dxnm .

n=1[0  Tn]  where um

el1...lnm (x1  ...  xnm) 

nm   wm

···

(2)

(cid:104)um

nm(cid:105) =(cid:90)Dnm

Given the inner product in (2)  the set of elements el1...lnm (x1  ...  xnm ) forms an orthonormal basis
in Hnm. Moreover  Hnm is an RKHS with the reproducing kernel (RK)
Knm (x1  ...  xnm ; y1  ...  ynm) =

el1...lnm (x1  ...  xnm)el1...lnm (y1  ...  ynm ).

. . .

L1(cid:88)l1=−L1

Lnm(cid:88)lnm =−Lnm

1 = um

In what follows  we will primarily be concerned with time-varying stimuli  and the

Remark 1.
dimension xnm will denote the temporal dimension t of the stimulus um
  i.e.  xnm = t.
nm
Remark 2. For M concurrently received stimuli  we have Tn1 = Tn2 = ··· = TnM .
Example 1. We model audio stimuli um
1 (t) as elements of the RKHS H1 over the domain
D1 = [0  T1]. For notational convenience  we drop the dimensionality subscript and use T   Ω and L 
to denote the period  bandwidth and order of the space H1. An audio signal um
1 ∈ H1 can be written
1 (t) =(cid:80)L
as um
Example 2. We model video stimuli um
3 (x  y  t) as elements of the RKHS H3 deﬁned on
D3 = [0  T1] × [0  T2] × [0  T3]  where T1 = 2πL1/Ω1  T2 = 2πL2/Ω2  T3 = 2πL3/Ω3 
with (Ω1  L1)  (Ω2  L2) and (Ω3  L3) denoting the (bandwidth  order) pairs in spatial directions
x  y and in time t  respectively. A video signal um
3 (x  y  t) =
(cid:80)L1
l1=−L1(cid:80)L2
l1l2l3 ∈ C and the func-
tions el1l2l3 (x  y  t) = exp (jl1Ω1x/L1 + jl2Ω2y/L2 + jl3Ω3t/L3) /√T1T2T3.

l ∈ C and el(t) = exp (jlΩt/L) /√T .

3 ∈ H3 can be written as um

el1l2l3(x  y  t)  where the coefﬁcients um

l el(t)  where the coefﬁcients um

l2=−L2(cid:80)L3

l=−L um

3 = um

l3=−L3

um

l1l2l3

2.2 Modeling Sensory Processing

Multisensory processing can be described by a nonlinear dynamical system capable of modeling
linear and nonlinear stimulus transformations  including cross-talk between stimuli [25]. For clarity 
here we will consider only the case of linear transformations that can be described by a linear ﬁlter
having an impulse response  or kernel  hm
nm(x1  ...  xnm ). The kernel is assumed to be bounded-
input bounded-output (BIBO)-stable and causal. Without loss of generality  we assume that such
transformations involve convolution in the time domain (temporal dimension xnm) and integration
in dimensions x1  ...  xnm−1. We also assume that the kernel has a ﬁnite support in each direction
xn  n = 1  ...  nm. In other words  the kernel hm
nm

belongs to the space Hnm deﬁned below.

nm) ⊆ Dnm(cid:9).
Deﬁnition 2. The ﬁlter kernel space Hnm =(cid:8)hm
Deﬁnition 3. The projection operator P : Hnm → Hnm is given (by abuse of notation) by
nm)(x1  ...  xnm ) =(cid:10)hm
(Phm
nm )(x1  ...  xnm) =(cid:80)L1
Since Phm

nm ∈ L1(Rnm)(cid:12)(cid:12) supp(hm
nm(·  ... ·)  Knm(·  ... ·; x1  ...  xnm)(cid:11).

nm ∈Hnm  (Phm

...(cid:80)Lnm

lnm =-Lnm

l1...lnm

l1=-L1

hm

(3)
el1...lnm (x1  ...  xnm).

3

3 Multisensory Decoding

Consider an mTEM comprised of a population of N ideal IAF neurons receiving M input signals
of dimensions nm  m = 1  ...  M. Assuming that the multisensory processing is given by
um
nm
kernels him
nm

  m = 1  ...  M  i = 1  ...  N  the t-transform in (1) can be rewritten as

k ∈ Z 

(4)

k

k

k

k+1

ti
k

k [u2

nM ] = qi
k 

n1 ] + T i2

n2 ] + ... + T iM

nm ] =(cid:90) ti

T i1
k [u1
(cid:20)(cid:90)Dnm

[uM
: Hnm → R are linear functionals deﬁned by
him
nm(x1  ...  xnm−1  s)um

nm(x1  ...  xnm−1  t − s)dx1...dxnm−1ds(cid:21)dt.

where T im
[um
T im
We observe that each qi
k in (4) is a real number representing a quantal measurement of all M
stimuli  taken by the neuron i on the interval [ti
k+1). These measurements are produced in an
asynchronous fashion and can be computed directly from spike times (ti
k)k∈Z using (1). We now
k)k∈Z  i = 1  ...  N.
demonstrate that it is possible to reconstruct stimuli um
nm
Theorem 1. (Multisensory Time Decoding Machine (mTDM))
nm ∈ Hnm be encoded by a multisensory TEM comprised of N ideal IAF neurons
Let M signals um
and N × M receptive ﬁelds with full spectral support. Assume that the IAF neurons do not have the
same parameters  and/or the receptive ﬁelds for each modality are linearly independent. Then given
the ﬁlter kernel coefﬁcients him

can be perfectly recovered as

  m = 1  ...  M from (ti

  i = 1  ...  N  all inputs um
nm

k  ti

l1...lnm

um
nm (x1  ...  xnm) =

L1(cid:88)l1=−L1

...

Lnm(cid:88)lnm =−Lnm

um
l1...lnm

el1...lnm (x1  ...  xnm ) 

(5)

 

 

(6)

l1...lnm

lnm = 0

jlnmΩnm

k)(cid:1)

lnm (cid:54)= 0

where um
Φ = [Φ1; Φ2; ... ; ΦN ]  q = [q1; q2; ... ; qN ] and [qi]k = qi
with

are elements of u = Φ+q  and Φ+ denotes the pseudoinverse of Φ. Furthermore 
k. Each matrix Φi = [Φi1  Φi2  ...  Φim] 

k+1) − elnm (ti

him
−l1 −l2 ... −lnm−1 lnm
him
−l1 −l2 ... −lnm−1 lnm

k+1 − ti
(ti
k) 
Lnm(cid:112)Tnm(cid:0)elnm (ti

where the column index l traverses all possible subscript combinations of l1  l2  ...  lnm. A neces-
sary condition for recovery is that the total number of spikes generated by all neurons is larger than
n=1(2Ln +1)+N. If each neuron produces ν spikes in an interval of length Tn1 = Tn2 =

[Φim]kl =
(cid:80)M
m=1(cid:81)nm
··· = TnM   a sufﬁcient condition is N ≥ (cid:108)(cid:80)M
n=1(2Ln + 1)/ min(ν − 1  2Lnm + 1)(cid:109)  
m=1(cid:81)nm
where (cid:100)x(cid:101) denotes the smallest integer greater than x.
nM ] =(cid:10)u1
M k(cid:11) =
Proof: Substituting (5) into (4)  qi
l1...ln1 k + ... + (cid:80)l1
...(cid:80)lnM
(cid:80)l1
...(cid:80)ln1
φiM
l1...lnM k 
where k ∈ Z and the second equality follows from the Riesz representation theorem with
nmk ∈ Hnm  m = 1  ...  M. In matrix form the above equality can be written as qi = Φiu  with
φim
l1...lnm k 
[qi]k = qi
with index l traversing all possible subscript combinations of l1  l2  ...  lnm. To ﬁnd the coefﬁcients
l1...lnm k  we note that φim
nmk(el1...lnm )  m = 1  ...  M  i = 1  ...  N. The column
l1...lnm k = T im
φim
n=1(2Ln + 1) entries corresponding
to coefﬁcients um
. Repeating for all neurons i = 1  ...  N  we obtain q = Φu with
Φ = [Φ1; Φ2; ... ; ΦN ] and q = [q1; q2; ... ; qN ]. This system of linear equations can be solved
n=1(2Ln + 1). A necessary
condition for the latter is that the total number of measurements generated by all N neurons is
n=1(2Ln + 1). Equivalently  the total number of spikes produced by all N
n=1(2Ln + 1) + N. Then u can be uniquely speciﬁed as the
solution to a convex optimization problem  e.g.  u = Φ+q. To ﬁnd the sufﬁcient condition  we note

vector u = [u1; u2; ...; um] with the vector um containing(cid:81)nm
for u  provided that the rank r(Φ) of matrix Φ satisﬁes r(Φ) =(cid:80)m(cid:81)nm
greater or equal to(cid:81)nm
neurons should be greater than(cid:81)nm

k  Φi = [Φi1  Φi2  ...  ΦiM ]  where elements [Φim]kl are given by [Φim]kl = φim

n1  φi1
uM
−l1 −l2 −lnM −1 lnM

1k(cid:11)+...+(cid:10)uM

u1
−l1 −l2 −ln1−1 ln1

n1 ]+...+T iM

k = T i1
φi1

nM   φiM

k [u1

l1l2...lnm

[uM

k

4

(a)

(b)

Figure 2: Multimodal TEM & TDM for audio and video integration (a) Block diagram of the multimodal
TEM. (b) Block diagram of the multimodal TDM.

that the mth component vim of the dendritic current vi has a maximal bandwidth of Ωnm and we
need only 2Lnm + 1 measurements to specify it. Thus each neuron can produce a maximum of only
2P Lnm + 1 informative measurements  or equivalently  2P Lnm + 2 informative spikes on a time
n=1(2Ln + 1)/(2Lnm + 1)
n=1(2Ln + 1)/(ν − 1)(cid:101) neurons if ν < (2Lnm + 2). (cid:3)

interval [0  Tnm]. It follows that for each modality  we require at least(cid:81)nm
neurons if ν ≥ (2Lnm + 2) and at least (cid:100)(cid:81)nm

4 Multisensory Identiﬁcation

We now investigate the following nonlinear neural identiﬁcation problem: given stimuli um
 
nm
m = 1  ...  M  at the input to a multisensory neuron i and spikes at its output  ﬁnd the multisensory
  m = 1  ...  M. We will show that this problem is mathematically dual
receptive ﬁeld kernels him
nm
to the decoding problem discussed above. Speciﬁcally  we will demonstrate that the identiﬁcation
k)k∈Z produced
problem can be converted into a neural encoding problem  where each spike train (ti
during an experimental trial i  i = 1  ...  N  is interpreted to be generated by the ith neuron in a
population of N neurons. We consider identifying kernels for only one multisensory neuron (identi-
ﬁcation for multiple neurons can be performed in a serial fashion) and drop the superscript i in him
nm
throughout this section. Instead  we introduce the natural notion of performing multiple experimen-
tal trials and use the same superscript i to index stimuli uim
nm

on different trials i = 1  ...  N.

hm
nm(s1  ...  snm−1  snm)uim

nm (s1  ...  snm−1  t − snm )ds1...dsnm−1dsnm =

Consider the multisensory neuron depicted in Fig. 1. Since for every trial i  an input signal uim
 
nm
m = 1  ...  M  can be modeled as an element of some space Hnm  we have uim
nm(x1  ...  xnm) =
nm (·  ... ·)  Knm(·  ... ·; x1  ...  xnm)(cid:105) by the reproducing property of the RK Knm. It follows that
(cid:104)uim
(cid:90)Dnm
=(cid:90)Dnm
nm(s1  ...  snm−1  snm)(cid:10)hm
=(cid:90)Dnm
uim
nm(s1  ...  snm−1  snm)(Phm
where (a) follows from the reproducing property and symmetry of Knm and Deﬁnition 2  and (b)
from the deﬁnition of Phm
Li1
k [Ph1

nm(·  ... ·)  Knm (·  ... ·; s1  ...  snm−1  t − snm)(cid:11) ds1...dsnm =

nm)(s1  ...  snm−1  t − snm)ds1...dsnm−1dsnm 

in (3). The t-transform of the mTEM in Fig. 1 can then be written as

n1] + Li2

k [Ph2

n2 ] + ... + LiM

k [PhM

nM ] = qi
k 

(7)

uim

(a)

(b)

nm

5

v1(t)Σv11(t)v12(t)Σv21(t)v22(t)v2(t)ΣvN(t)vN1(t)vN2(t)(t1k)k∈Z(t2k)k∈Z(tNk)k∈Zh223(x y t)u23(x y t)yxttu11(t)h111(t)h211(t)hN11(t)hN23(x y t)h123(x y t)Neuron1Neuron2NeuronNu1Lu1-L++ΣeL1 L2 L3(t)+e-L1 -L2 -L3+Σu2-L1 -L2 -L3u2L1 L2 L3eL(t)e-Lu23(x y t)yxttu11(t)(t2k)k∈Z(tNk)k∈Z(t1k)k∈ZConvexOptimizationProbleme.g. u=Φ+q(a)

(b)

Figure 3: Multimodal CIM for audio and video integration (a) Time encoding interpretation of the multi-
modal CIM. (b) Block diagram of the multimodal CIM.

k

nm

k  ti

k+1

ti
k

where Lim
Lim
k [Phm

: Hnm → R  m = 1  ...  M  k ∈ Z  are linear functionals deﬁned by
nm ] =(cid:90) ti
k of the (weighted) sum of all kernel projections Phm

uim
nm(s1  ...   snm )(Phm

(cid:20)(cid:90)Dm

nm )(s1  ...  t − snm )ds1 ... dsnm(cid:21) dt.

k+1) produced by the IAF neuron is a time

  m = 1  ...  M.
is determined by the corresponding stimuli uim
nm

Remark 3. Intuitively  each inter-spike interval [ti
measurement qi
Remark 4. Each projection Phm
employed during identiﬁcation and can be substantially different from the underlying kernel hm
nm
It follows that we should be able to identify the projections Phm
  m = 1  ...  M  from the measure-
k)k∈Z. Since we are free to choose any of the spaces Hnm  an arbitrarily-close identiﬁcation
ments (qi
of original kernels is possible  provided that the bandwidth of the test signals is sufﬁciently large.
Theorem 2. (Multisensory Channel Identiﬁcation Machine (mCIM))
Let {ui}N
dent stimuli at the input to an mTEM circuit comprised of receptive ﬁelds with kernels hm
m = 1  ...  M  in cascade with an ideal IAF neuron. Given the coefﬁcients uim
uim
nm
tiﬁed as (Phm
hm
l1...lnm
Φ = [Φ1; Φ2; ... ; ΦN ]  q = [q1; q2; ... ; qN ] and [qi]k = qi
with

nm∈Hnm  m = 1  ...  M  be a collection of N linearly indepen-
nm ∈ Hnm 
of stimuli
  m = 1  ...  M  can be perfectly iden-
el1...lnm (x1  ...  xnm )  where
hm
are elements of h = Φ+q  and Φ+ denotes the pseudoinverse of Φ. Furthermore 
k. Each matrix Φi = [Φi1  Φi2  ...  Φim] 

  i = 1  ...  N  m = 1  ...  M  the kernel projections Phm
lnm =−Lnm

nm)(x1  ...  xnm) = (cid:80)L1

...(cid:80)Lnm

i=1  ui = [ui1

n1   ...  uiM

nM ]T   uim

  i = 1  ...  N 

l1=−L1

l1 ... lnm

nm

l1...lnm

nm

nm

.

k+1 − ti
(ti
k) 
Lnm(cid:112)Tnm(cid:0)elnm (ti

jlnmΩnm

k+1) − elnm (ti

lnm = 0

 

lnm (cid:54)= 0

 

(8)

k)(cid:1)

uim
−l1 −l2 ... −lnm−1 lnm
uim
−l1 −l2 ... −lnm−1 lnm

[Φim]kl =
m=1(cid:81)nm
(cid:80)M
tion is that the number of trials N ≥(cid:108)(cid:80)M

where l traverses all subscript combinations of l1  l2  ...  lnm. A necessary condition for identi-
ﬁcation is that the total number of spikes generated in response to all N trials is larger than
n=1(2Ln + 1) + N. If the neuron produces ν spikes on each trial  a sufﬁcient condi-

m=1(cid:81)nm

n=1(2Ln + 1)/ min(ν − 1  2Lnm + 1)(cid:109) .

Proof: The equivalent representation of the t-transform in equations (4) and (7) implies that the
(in Theorem 1) and the identiﬁcation of the ﬁlter projections Phm
decoding of the stimulus um
nm
nm
encountered here are dual problems. Therefore  the receptive ﬁeld identiﬁcation problem is equiva-
lent to a neural encoding problem: the projections Phm
  m = 1  ...  M  are encoded with an mTEM
comprised of N neurons and receptive ﬁelds uim
  i = 1  ...  N  m = 1  ...  M. The algorithm for
nm
ﬁnding the coefﬁcients hm

is analogous to the one for um

in Theorem 1.

nm

l1...lnm

l1...lnm

6

v1(t)Σv11(t)v12(t)Σv21(t)v22(t)v2(t)ΣvN(t)vN1(t)vN2(t)(t1k)k∈Z(t2k)k∈Z(tNk)k∈ZyxtTrial1Trial2TrialN(Ph11)(t)(Ph23)(x y t)u111(t)u211(t)uN13(t)u223(x y t)uN23(x y t)u123(x y t)th1Lh1-L++ΣeL1 L2 L3(t)+e-L1 -L2 -L3+Σh2-L1 -L2 -L3h2L1 L2 L3eL(t)e-L(t2k)k∈Z(tNk)k∈Z(t1k)k∈ZConvexOptimizationProbleme.g. h=Φ+qyxtt(Ph11)(t)(Ph23)(x y t)5 Examples

3(x  y  t) appear as inputs to temporal ﬁlters with kernels hi1

i=1  k ∈ Z  can provide a faithful representation of both signals.
k}N

A simple (mono) audio/video TEM realized using a bank of temporal and spatiotemporal linear
ﬁlters and a population of integrate-and-ﬁre neurons  is shown in Fig. 2. An analog audio signal
1(t) and an analog video signal u2
u1
1 (t)
and spatiotemporal ﬁlters with kernels hi2
3 (x  y  t)  i = 1  ...  N. Each temporal and spatiotemporal
ﬁlter could be realized in a number of ways  e.g.  using gammatone and Gabor ﬁlter banks. For
simplicity  we assume that the number of temporal and spatiotemporal ﬁlters in Fig. 2 is the same.
In practice  the number of components could be different and would be determined by the bandwidth
of input stimuli Ω  or equivalently the order L  and the number of spikes produced (Theorems 1-2).
For each neuron i  i = 1  ...  N  the ﬁlter outputs vi1 and vi2  are summed to form the aggregate
k)k∈Z by the ith integrate-
dendritic current vi  which is encoded into a sequence of spike times (ti
and-ﬁre neuron. Thus each spike train (ti
k)k∈Z carries information about two stimuli of completely
different modalities (audio and video) and  under certain conditions  the entire collection of spike
trains {ti
To demonstrate the performance of the algorithm presented in Theorem 1  we simulated a multisen-
sory TEM with each neuron having a non-separable spatiotemporal receptive ﬁeld for video stimuli
and a temporal receptive ﬁeld for audio stimuli. Spatiotemporal receptive ﬁelds were chosen ran-
domly and had a bandwidth of 4 Hz in temporal direction t and 2 Hz in each spatial direction x and
y. Similarly  temporal receptive ﬁelds were chosen randomly from functions bandlimited to 4 kHz.
Thus  two distinct stimuli having different dimensions (three for video  one for audio) and dynam-
ics (2-4 cycles vs. 4  000 cycles in each direction) were multiplexed at the level of every spiking
neuron and encoded into an unlabeled set of spikes. The mTEM produced a total of 360  000 spikes
in response to a 6-second-long grayscale video and mono audio of Albert Einstein explaining the
mass-energy equivalence formula E = mc2: “... [a] very small amount of mass may be converted
into a very large amount of energy.” A multisensory TDM was then used to reconstruct the video and
audio stimuli from the produced set of spikes. Fig. 4a-b shows the original (top row) and recovered
(middle row) video and audio  respectively  together with the error between them (bottom row).
The neural encoding interpretation of the identiﬁcation problem for the grayscale video/mono audio
TEM is shown in Fig. 3a. The block diagram of the corresponding mCIM appears in Fig. 3b.
Comparing this diagram to the one in Fig. 2  we note that neuron blocks have been replaced by trial
blocks. Furthermore  the stimuli now appear as kernels describing the ﬁlters and the inputs to the
  m = 1  ...  M. In other words  identiﬁcation of a single neuron
circuit are kernel projections Phm
has been converted into a population encoding problem  where the artiﬁcially constructed population
of N neurons is associated with the N spike trains generated in response to N experimental trials.
The performance of the mCIM algorithm is visualized in Fig. 5. Fig. 5a-b shows the original
(top row) and recovered (middle row) spatio-temporal and temporal receptive ﬁelds  respectively 
together with the error between them (bottom row).

nm

6 Conclusion

We presented a spiking neural circuit for multisensory integration that encodes multiple information
streams  e.g.  audio and video  into a single spike train at the level of individual neurons. We derived
conditions for inverting the nonlinear operator describing the multiplexing and encoding in the spike
domain and developed methods for identifying multisensory processing using concurrent stimulus
presentations. We provided explicit algorithms for multisensory decoding and identiﬁcation and
evaluated their performance using natural audio and video stimuli. Our investigations brought to
light a key duality between identiﬁcation of multisensory processing in a single neuron and the re-
covery of stimuli encoded with a population of multisensory neurons. Given the powerful machinery
of employed RKHSs  extensions to neural circuits with noisy neurons are straightforward [15  23].

Acknowledgement

The work presented here was supported in part by AFOSR under grant #FA9550-12-1-0232 and  in
part  by NIH under the grant #R021 DC012440001.

7

3. (middle row) Corresponding three frames of the decoded video projection P3u2

Figure 4: Multisensory decoding. (a) Grayscale Video Recovery. (top row) Three frames of the original
grayscale video u2
3. (bottom
row) Error between three frames of the original and identiﬁed video. Ω1 = 2π · 2 rad/s  L1 = 30  Ω2 =
2π · 36/19 rad/s  L2 = 36  Ω3 = 2π · 4 rad/s  L3 = 4. (b) Mono Audio Recovery. (top row) Original mono
audio signal u1
1. (bottom row) Error between the original and decoded
audio. Ω = 2π · 4  000 rad/s  L = 4  000. Click here to see and hear the decoded video and audio stimuli.

1. (middle row) Decoded projection P1u1

Figure 5: Multisensory identiﬁcation. (a) (top row) Three frames of the original spatiotemporal kernel
3 is a spatial Gabor function rotating clockwise in space as a function of time. (middle row)
3(x  y  t). Here  h2
h2
Corresponding three frames of the identiﬁed kernel Ph2∗
3 (x  y  t). (bottom row) Error between three frames of
the original and identiﬁed kernel. Ω1 = 2π·12 rad/s  L1 = 9  Ω2 = 2π·12 rad/s  L2 = 9  Ω3 = 2π·100 rad/s 
L3 = 5. (b) Identiﬁcation of the temporal RF (top row) Original temporal kernel h1
1(t). (middle row) Identiﬁed
projection Ph1∗

1 . Ω = 2π · 200 rad/s  L = 10.

1 (t). (bottom row) Error between h1

1 and Ph1∗

8

y [px]t=0s(cid:4)(cid:1)(cid:1)(cid:4)(cid:10)(cid:3)t=2st=4s(cid:1)(cid:1)(cid:11)(cid:14)(cid:16)(cid:12)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:1)(cid:3)(cid:2)(cid:8)(cid:3)(cid:3)(cid:2)(cid:8)y [px](cid:4)(cid:1)(cid:1)(cid:4)(cid:10)(cid:3)(cid:1)(cid:1)(cid:11)(cid:14)(cid:16)(cid:12)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:1)(cid:3)(cid:2)(cid:8)(cid:3)(cid:3)(cid:2)(cid:8)(cid:4)(cid:1)(cid:1)(cid:5)(cid:5)(cid:10)(cid:4)(cid:1)(cid:1)(cid:4)(cid:10)(cid:3)x [px]y [px](cid:1)(cid:1)MSE=-21.5dB(cid:4)(cid:1)(cid:1)(cid:5)(cid:5)(cid:10)x [px](cid:1)(cid:1)MSE=-21.0dB(cid:4)(cid:1)(cid:1)(cid:5)(cid:5)(cid:10)x [px](cid:1)(cid:1)MSE=-19.2dB(cid:11)(cid:14)(cid:16)(cid:12)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:1)(cid:3)(cid:2)(cid:8)(cid:3)(cid:3)(cid:2)(cid:8)(cid:1)(cid:3)(cid:2)(cid:8)(cid:3)(cid:3)(cid:2)(cid:8)Amplitude(cid:1)(cid:3)(cid:2)(cid:8)(cid:3)(cid:3)(cid:2)(cid:8)Amplitude(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:1)(cid:3)(cid:2)(cid:8)(cid:3)(cid:3)(cid:2)(cid:8)Time [s]Amplitude(cid:1)(cid:1)MSE=-8.4dBOriginalDecodedErrorVideoAudio(a)(b)yt=15ms(cid:1)(cid:3)(cid:2)(cid:6)(cid:3)(cid:3)(cid:2)(cid:6)t=30mst=45ms(cid:1)(cid:1)(cid:15)(cid:21)(cid:23)(cid:19)(cid:21)(cid:22)(cid:20)(cid:23)(cid:24)(cid:1)(cid:5)(cid:3)(cid:3)(cid:3)(cid:5)(cid:3)(cid:3)y(cid:1)(cid:3)(cid:2)(cid:6)(cid:3)(cid:3)(cid:2)(cid:6)(cid:1)(cid:1)(cid:15)(cid:21)(cid:23)(cid:19)(cid:21)(cid:22)(cid:20)(cid:23)(cid:24)(cid:1)(cid:5)(cid:3)(cid:3)(cid:3)(cid:5)(cid:3)(cid:3)(cid:1)(cid:3)(cid:2)(cid:6)(cid:3)(cid:3)(cid:2)(cid:6)(cid:1)(cid:3)(cid:2)(cid:6)(cid:3)(cid:3)(cid:2)(cid:6)xy(cid:1)(cid:1)(cid:16)(cid:17)(cid:14)(cid:1)(cid:12)(cid:1)(cid:5)(cid:11)(cid:2)(cid:10)(cid:18)(cid:13)(cid:1)(cid:3)(cid:2)(cid:6)(cid:3)(cid:3)(cid:2)(cid:6)x(cid:1)(cid:1)(cid:16)(cid:17)(cid:14)(cid:1)(cid:12)(cid:1)(cid:5)(cid:11)(cid:2)(cid:10)(cid:18)(cid:13)(cid:1)(cid:3)(cid:2)(cid:6)(cid:3)(cid:3)(cid:2)(cid:6)x(cid:1)(cid:1)(cid:16)(cid:17)(cid:14)(cid:1)(cid:12)(cid:1)(cid:5)(cid:10)(cid:2)(cid:10)(cid:18)(cid:13)(cid:15)(cid:21)(cid:23)(cid:19)(cid:21)(cid:22)(cid:20)(cid:23)(cid:24)(cid:1)(cid:5)(cid:3)(cid:3)(cid:3)(cid:5)(cid:3)(cid:3)(cid:1)(cid:5)(cid:3)(cid:3)(cid:5)(cid:3)Amplitude(cid:1)(cid:5)(cid:3)(cid:3)(cid:5)(cid:3)Amplitude(cid:3)(cid:4)(cid:3)(cid:5)(cid:3)(cid:6)(cid:3)(cid:7)(cid:3)(cid:8)(cid:3)(cid:1)(cid:5)(cid:3)(cid:3)(cid:5)(cid:3)Time [ms]Amplitude(cid:1)(cid:1)(cid:16)(cid:17)(cid:14)(cid:1)(cid:12)(cid:1)(cid:9)(cid:5)(cid:2)(cid:9)(cid:18)(cid:13)OriginalIdentiﬁedErrorSpatiotemporalRFTemporalRF(a)(b)References
[1] Barry E. Stein and Terrence R. Stanford. Multisensory integration: Current issues from the perspective of

a single neuron. Nature Reviews Neuroscience  9:255–266  April 2008.

[2] Christoph Kayser  Christopher I. Petkov  and Nikos K. Logothetis. Multisensory interactions in primate

auditory cortex: fmri and electrophysiology. Hearing Research  258:80–88  March 2009.

[3] Stephen J. Huston and Vivek Jayaraman. Studying sensorimotor integration in insects. Current Opinion

in Neurobiology  21:527–534  June 2011.

[4] Barry E. Stein and M. Alex Meredith. The merging of the senses. The MIT Press  1993.
[5] David A. Bulkin and Jennifer M. Groh. Seeing sounds: Visual and auditory interactions in the brain.

Current Opinion in Neurobiology  16:415–419  July 2006.

[6] Jon Driver and Toemme Noesselt. Multisensory interplay reveals crossmodal inﬂuences on ’sensory-

speciﬁc’ brain regions  natural responses  and judgments. Neuron  57:11–23  January 2008.

[7] Christoph Kayser  Nikos K. Logothetis  and Stefano Panzeri. Visual enhancement of the information

representation in auditory cortex. Current Biology  pages 19–24  January 2010.

[8] Asif A. Ghazanfar and Charles E. Schroeder. Is neocortex essentially multisensory? Trends in Cognitive

Sciences  10:278–285  June 2006.

[9] Paul J. Laurienti  Thomas J. Perrault  Terrence R. Stanford  Mark T. Wallace  and Barry E. Stein. On the
use of superadditivity as a metric for characterizing multisensory integration in functional neuroimaging
studies. Experimental Brain Research  166:289–297  2005.

[10] Konrad P. K¨ording and Joshua B. Tenenbaum. Causal inference in sensorimotor integration. Advances in

Neural Information Processing Systems 19  2007.

[11] Ulrik R. Beierholm  Konrad P. K¨ording  Ladan Shams  and Wei Ji Ma. Comparing bayesian models for
multisensory cue combination without mandatory integration. Advances in Neural Information Processing
Systems 20  2008.

[12] Daniel C. Kadunce  J. William Vaughan  Mark T. Wallace  and Barry E. Stein. The inﬂuence of visual and
auditory receptive ﬁeld organization on multisensory integration in the superior colliculus. Experimental
Brain Research  2001.

[13] Wei Ji Ma and Alexandre Pouget. Linking neurons to behavior in multisensory perception: A computa-

tional review. Brain Research  1242:4–12  2008.

[14] Mark A. Frye. Multisensory systems integration for high-performance motor control in ﬂies. Current

Opinion in Neurobiology  20:347–352  2010.

[15] Aurel A. Lazar and Yevgeniy B. Slutskiy. Channel Identiﬁcation Machines. Computational Intelligence

and Neuroscience  2012.

[16] Aurel A. Lazar.

Time encoding with an integrate-and-ﬁre neuron with a refractory period.

Neurocomputing  58-60:53–58  June 2004.

[17] Aurel A. Lazar. Population encoding with Hodgkin-Huxley neurons. IEEE Transactions on Information

Theory  56(2)  February 2010.

[18] Aurel A. Lazar and Laszlo T. T´oth. Perfect recovery and sensitivity analysis of time encoded bandlimited

signals. IEEE Transactions on Circuits and Systems-I: Regular Papers  51(10):2060–2073  2004.

[19] Aurel A. Lazar and Eftychios A. Pnevmatikakis. Faithful representation of stimuli with a population of

integrate-and-ﬁre neurons. Neural Computation  20(11):2715–2744  November 2008.

[20] Aurel A. Lazar and Yevgeniy B. Slutskiy. Functional identiﬁcation of spike-processing neural circuits.

Neural Computation  in press  2013.

[21] Anmo J. Kim and Aurel A. Lazar. Recovery of stimuli encoded with a Hodgkin-Huxley neuron using
conditional PRCs. In N.W. Schultheiss  A.A. Prinz  and R.J. Butera  editors  Phase Response Curves in
Neuroscience. Springer  2011.

[22] Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and

Statistics. Kluwer Academic Publishers  2004.

[23] Aurel A. Lazar  Eftychios A. Pnevmatikakis  and Yiyin Zhou. Encoding natural scenes with neural circuits
with random thresholds. Vision Research  2010. Special Issue on Mathematical Models of Visual Coding.
[24] Aurel A. Lazar and Eftychios A. Pnevmatikakis. Reconstruction of sensory stimuli encoded with
integrate-and-ﬁre neurons with random thresholds. EURASIP Journal on Advances in Signal Processing 
2009  2009.

[25] Yevgeniy B. Slutskiy.

Identiﬁcation of Dendritic Processing in Spiking Neural Circuits. PhD thesis 

Columbia University  2013.

9

,Aurel Lazar
Yevgeniy Slutskiy
Changyou Chen
Nan Ding
Chunyuan Li
Yizhe Zhang
Lawrence Carin
Yixing Xu
Yunhe Wang
Hanting Chen
Kai Han
Chunjing XU
Dacheng Tao
Chang Xu