2013,PAC-Bayes-Empirical-Bernstein Inequality,We present PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on combination of PAC-Bayesian bounding technique with Empirical Bernstein bound. It allows to take advantage of small empirical variance and is especially useful in regression. We show that when the empirical variance is significantly smaller than the empirical loss PAC-Bayes-Empirical-Bernstein inequality is significantly tighter than PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. PAC-Bayes-Empirical-Bernstein inequality is an interesting example of application of PAC-Bayesian bounding technique to self-bounding functions. We provide empirical comparison of PAC-Bayes-Empirical-Bernstein inequality with PAC-Bayes-kl inequality on a synthetic example and several UCI datasets.,PAC-Bayes-Empirical-Bernstein Inequality

Ilya Tolstikhin
Computing Centre

Russian Academy of Sciences

iliya.tolstikhin@gmail.com

yevgeny.seldin@gmail.com

Queensland University of Technology

Yevgeny Seldin

UC Berkeley

Abstract

We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based
on a combination of the PAC-Bayesian bounding technique with an Empirical
Bernstein bound. We show that when the empirical variance is signiﬁcantly
smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is
signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and oth-
erwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a
synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein
inequality is an interesting example of an application of the PAC-Bayesian bound-
ing technique to self-bounding functions.

1

Introduction

PAC-Bayesian analysis is a general and powerful tool for data-dependent analysis in machine learn-
ing. By now it has been applied in such diverse areas as supervised learning [1–4]  unsupervised
learning [4  5]  and reinforcement learning [6]. PAC-Bayesian analysis combines the best aspects
of PAC learning and Bayesian learning: (1) it provides strict generalization guarantees (like VC-
theory)  (2) it is ﬂexible and allows the incorporation of prior knowledge (like Bayesian learning) 
and (3) it provides data-dependent generalization guarantees (akin to Radamacher complexities).
PAC-Bayesian analysis provides concentration inequalities for the divergence between expected and
empirical loss of randomized prediction rules. For a hypothesis space H a randomized prediction
rule associated with a distribution ρ over H operates by picking a hypothesis at random according
to ρ from H each time it has to make a prediction. If ρ is a delta-distribution we recover classical
prediction rules that pick a single hypothesis h ∈ H. Otherwise  the prediction strategy resembles
Bayesian prediction from the posterior distribution  with a distinction that ρ does not have to be the
Bayes posterior. Importantly  many of PAC-Bayesian inequalities hold for all posterior distributions
ρ simultaneously (with high probability over a random draw of a training set). Therefore  PAC-
Bayesian bounds can be used in two ways. Ideally  we prefer to derive new algorithms that ﬁnd the
posterior distribution ρ that minimizes the PAC-Bayesian bound on the expected loss. However  we
can also use PAC-Bayesian bounds in order to estimate the expected loss of posterior distributions ρ
that were found by other algorithms  such as empirical risk minimization  regularized empirical risk
minimization  Bayesian posteriors  and so forth. In such applications PAC-Bayesian bounds can be
used to provide generalization guarantees for other methods and can be applied as a substitute for
cross-validation in paratemer tuning (since the bounds hold for all posterior distributions ρ simul-
taneously  we can apply the bounds to test multiple posterior distributions ρ without suffering from
over-ﬁtting  in contrast with extensive applications of cross-validation).
There are two forms of PAC-Bayesian inequalities that are currently known to be the tightest de-
pending on a situation. One is the PAC-Bayes-kl inequality of Seeger [7] and the other is the PAC-
Bayes-Bernstein inequality of Seldin et. al. [8]. However  the PAC-Bayes-Bernstein inequality is
expressed in terms of the true expected variance  which is rarely accessible in practice. Therefore  in
order to apply the PAC-Bayes-Bernstein inequality we need an upper bound on the expected variance

1

(or  more precisely  on the average of the expected variances of losses of each hypothesis h ∈ H
weighted according to the randomized prediction rule ρ). If the loss is bounded in the [0  1] interval
the expected variance can be upper bounded by the expected loss and this bound can be used to
recover the PAC-Bayes-kl inequality from the PAC-Bayes-Bernstein inequality (with slightly sub-
optimal constants and suboptimal behavior for small sample sizes). In fact  for the binary loss this
result cannot be signiﬁcantly improved (see Section 3). However  when the loss is not binary it may
be possible to obtain a tighter bound on the variance  which will lead to a tighter bound on the loss
than the PAC-Bayes-kl inequality. For example  in Seldin et. al. [6] a deterministic upper bound
on the variance of importance-weighted sampling combined with PAC-Bayes-Bernstein inequality
yielded an order of magnitude improvement relative to application of PAC-Bayes-kl inequality to
the same problem. We note that the bound on the variance used by Seldin et. al. [6] depends on
speciﬁc properties of importance-weighted sampling and does not apply to other problems.
In this work we derive the PAC-Bayes-Empirical-Bernstein bound  in which the expected average
variance of the loss weighted by ρ is replaced by the weighted average of the empirical variance
of the loss. Bounding the expected variance by the empirical variance is generally tighter than
bounding it by the empirical loss. Therefore  the PAC-Bayes-Empirical-Bernstein bound is generally
tighter than the PAC-Bayes-kl bound  although the exact comparison also depends on the divergence
between the posterior and the prior and the sample size.
In Section 5 we provide an empirical
comparison of the two bounds on several synthetic and UCI datasets.
The PAC-Bayes-Empirical-Bernstein bound is derived in two steps. In the ﬁrst step we combine the
PAC-Bayesian bounding technique with the Empirical Bernstein inequality [9] and derive a PAC-
Bayesian bound on the variance. The PAC-Bayesian bound on the variance bounds the divergence
between averages [weighted by ρ] of expected and empirical variances of the losses of hypotheses
in H and holds with high probability for all averaging distributions ρ simultaneously. In the second
step the PAC-Bayesian bound on the variance is substituted into the PAC-Bayes-Bernstein inequality
yielding the PAC-Bayes-Empirical-Bernstein bound.
The remainder of the paper is organized as follows. We start with some formal deﬁnitions and
review the major PAC-Bayesian bounds in Section 2  provide our main results in Section 3 and their
proof sketches in Section 4  and ﬁnish with experiments in Section 5 and conclusions in Section 6.
Detailed proofs are provided in the supplementary material.

2 Problem Setting and Background

We start with providing the problem setting and then give some background on PAC-Bayesian anal-
ysis.

2.1 Notations and Deﬁnitions
We consider supervised learning setting with an input space X   an output space Y  an i.i.d. training
sample S = {(Xi  Yi)}n
i=1 drawn according to an unknown distribution D on the product-space
X × Y  a loss function (cid:96) : Y 2 → [0  1]  and a hypothesis class H. The elements of H are functions
h : X → Y from the input space to the output space. We use (cid:96)h(X  Y ) = (cid:96)(Y  h(X)) to denote the
loss of a hypothesis h on a pair (X  Y ).
For a ﬁxed hypothesis h ∈ H denote its expected loss by L(h) = E(X Y )∼D[(cid:96)h(X  Y )] 
i=1 (cid:96)h(Xi  Yi)  and the variance of the loss V(h) =
the empirical
loss Ln(h) = 1
n
Var(X Y )∼D[(cid:96)h(X  Y )] = E(X Y )∼D
We deﬁne Gibbs regression rule Gρ associated with a distribution ρ over H in the following way:
for each point X Gibbs regression rule draws a hypothesis h according to ρ and applies it to X. The
expected loss of Gibbs regression rule is denoted by L(Gρ) = Eh∼ρ[L(h)] and the empirical loss is
denoted by Ln(Gρ) = Eh∼ρ[Ln(h)]. We use KL(ρ(cid:107)π) = Eh∼ρ
to denote the Kullback-
Leibler divergence between two probability distributions [10]. For two Bernoulli distributions with
biases p and q we use kl(q(cid:107)p) as a shorthand for KL([q  1 − q](cid:107)[p  1 − p]). In the sequel we use
Eρ [·] as a shorthand for Eh∼ρ [·].

(cid:80)n
(cid:104)(cid:0)(cid:96)h(X  Y ) − E(X Y )∼D [(cid:96)h(X  Y )](cid:1)2(cid:105)
(cid:105)

.

(cid:104)

ln ρ(h)
π(h)

2

2.2 PAC-Bayes-kl bound

Before presenting our results we review several existing PAC-Bayesian bounds. The result in The-
orem 1 was presented by Maurer [11  Theorem 5] and is one of the tightest known concentration
bounds on the expected loss of Gibbs regression rule. Theorem 1 generalizes (and slightly tightens)
PAC-Bayes-kl inequality of Seeger [7  Theorem 1] from binary to arbitrary loss functions bounded
in the [0  1] interval.
Theorem 1. For any ﬁxed probability distribution π over H  for any n ≥ 8 and δ > 0  with
probability greater than 1 − δ over a random draw of a sample S  for all distributions ρ over H
simultaneously:

kl(cid:0)Ln(Gρ)(cid:107)L(Gρ)(cid:1) ≤ KL(ρ(cid:107)π) + ln 2

√

n

Since by Pinsker’s inequality |p − q| ≤ (cid:112)kl(q(cid:107)p)/2  Theorem 1 directly implies (up to minor

(1)

n

.

δ

factors) the more explicit PAC-Bayesian bound of McAllester [12]:
√
KL(ρ(cid:107)π) + ln 2

n

L(Gρ) ≤ Ln(Gρ) +

(2)
which holds with probability greater than 1 − δ for all ρ simultaneously. We note that kl is easy
to invert numerically and for small values of Ln(Gρ) (less than 1/4) the implicit bound in (1)
is signiﬁcantly tighter than the explicit bound in (2). This can be seen from another relaxation
2kl(q(cid:107)p) for p < q:

suggested by McAllester [2]  which follows from (1) by the inequality p ≤ q +(cid:112)2qkl(q(cid:107)p) +

2n

 

δ

(cid:115)

(cid:118)(cid:117)(cid:117)(cid:116) 2Ln(Gρ)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

KL(ρ(cid:107)π) + ln 2

√

δ

n

n

2

+

KL(ρ(cid:107)π) + ln 2

√

δ

n

n

.

(3)

L(Gρ) ≤ Ln(Gρ) +

From inequality (3) we clearly see that inequality (1) achieves “fast convergence rate” or  in other
√
words  when L(Gρ) is zero (or small compared to 1/
n) the bound converges at the rate of 1/n
rather than 1/

n as a function of n.

√

2.3 PAC-Bayes-Bernstein Bound

Seldin et. al. [8] introduced a general technique for combining PAC-Bayesian analysis with con-
centration of measure inequalities and derived the PAC-Bayes-Bernstein bound cited below. (The
PAC-Bayes-Bernstein bound of Seldin et. al. holds for martingale sequences  but for simplicity in
this paper we restrict ourselves to i.i.d. variables.)
Theorem 2. For any ﬁxed distribution π over H  for any δ1 > 0  and for any ﬁxed c1 > 1  with
probability greater than 1 − δ1 (over a draw of S) we have

(cid:118)(cid:117)(cid:117)(cid:116) (e − 2)Eρ[V(h)]

(cid:16)

(cid:17)

KL(ρ(cid:107)π) + ln ν1
n

δ1

(4)

L(Gρ) ≤ Ln(Gρ) + (1 + c1)

simultaneously for all distributions ρ over H that satisfy
KL(ρ(cid:107)π) + ln ν1
(e − 2)Eρ[V(h)]

δ1

(cid:115)
(cid:38)

n 

≤ √
(cid:33)(cid:39)

(cid:32)(cid:115)

1

ln c1

ln

(e − 2)n
4 ln(1/δ1)

+ 1 

where

ν1 =

and for all other ρ we have:

L(Gρ) ≤ Ln(Gρ) + 2

KL(ρ(cid:107)π) + ln ν1

δ1

.

n

Furthermore  the result holds if Eρ [V(h)] is replaced by an upper bound ¯V (ρ)  as long as
Eρ [V(h)] ≤ ¯V (ρ) ≤ 1

4 for all ρ.

3

A few comments on Theorem 2 are in place here. First  we note that Seldin et. al. worked with
cumulative losses and variances  whereas we work with normalized losses and variances  which
means that their losses and variances differ by a multiplicative factor of n from our deﬁnitions.
Second  we note that the statement on the possibility of replacing Eρ [V(h)] by an upper bound is
not part of [8  Theorem 8]  but it is mentioned and analyzed explicitly in the text. The requirement
that ¯V (ρ) ≤ 1
4 is not mentioned explicitly  but it follows directly from the necessity to preserve the
relevant range of the trade-off parameter λ in the proof of the theorem. Since 1
4 is a trivial upper
bound on the variance of a random variable bounded in the [0  1] interval  the requirement is not a
limitation. Finally  we note that since we are working with “one-sided” variables (namely  the loss is
bounded in the [0  1] interval rather than “two-sided” [−1  1] interval  which was considered in [8])
the variance is bounded by 1
4 (rather than 1)  which leads to a slight improvement in the value of ν1.
Since in reality we rarely have access to the expected variance Eρ [V(h)] the tightness of Theorem
2 entirely depends on the tightness of the upper bound ¯V (ρ). If we use the trivial upper bound
Eρ [V(h)] ≤ 1
4 the result is roughly equivalent to (2)  which is inferior to Theorem 1. Design of a
tighter upper bound on Eρ [V(h)] that holds for all ρ simultaneously is the subject of the following
section.

3 Main Results
The key result of our paper is a PAC-Bayesian bound on the average expected variance Eρ [V(h)]
given in terms of the average empirical variance Eρ[Vn(h)] = Eh∼ρ[Vn(h)]  where

n(cid:88)
(cid:0)(cid:96)h(Xi  Yi) − Ln(h)(cid:1)2

Vn(h) =

1

n − 1

i=1

is an unbiased estimate of the variance V(h). The bound is given in Theorem 3 and it holds with
high probability for all distributions ρ simultaneously. Substitution of this bound into Theorem 2
yields the PAC-Bayes-Empirical-Bernstein inequality given in Theorem 4. Thus  the PAC-Bayes-
Empirical-Bernstein inequality is based on two subsequent applications of the PAC-Bayesian bound-
ing technique.

3.1 PAC-Bayesian bound on the variance

Theorem 3 is based on an application of the PAC-Bayesian bounding technique to the difference
Eρ [V(h)]− Eρ [Vn(h)]. We note that Vn(h) is a second-order U-statistics [13] and Theorem 3 pro-
vides an interesting example of combining PAC-Bayesian analysis with concentration inequalities
for self-bounding functions.
Theorem 3. For any ﬁxed distribution π over H  any c2 > 1 and δ2 > 0  with probability greater
than 1 − δ2 over a draw of S  for all distributions ρ over H simultaneously:

(5)

(cid:17)

 

(6)

Eρ[V(h)] ≤ Eρ[Vn(h)] + (1 + c2)

where

ν2 =

(cid:16)

(cid:118)(cid:117)(cid:117)(cid:116)Eρ [Vn(h)]
(cid:115)
(cid:32)

1

ln c2

ln

1
2

(cid:38)

(cid:16)

(cid:17)
(cid:33)(cid:39)

n − 1
ln(1/δ2)

+ 1 +

1
2

.

KL(ρ(cid:107)π) + ln ν2
2(n − 1)

δ2

2c2

+

KL(ρ(cid:107)π) + ln ν2

δ2

n − 1

Note that (6) closely resembles the explicit bound on L(Gρ) in (3). If the empirical variance Vn(h)
is close to zero the impact of the second term of the bound (that scales with 1/
n) is relatively small
and we obtain “fast convergence rate” of Eρ [Vn(h)] to Eρ [V(h)]. Finally  we note that the impact
of c2 on ln ν2 is relatively small and so c2 can be taken very close to 1.

√

3.2 PAC-Bayes-Empirical-Bernstein bound
Theorem 3 controls the average variance Eρ[V(h)] for all posterior distributions ρ simultaneously.
By taking δ1 = δ2 = δ
2 we have the claims of Theorems 2 and 3 holding simultaneously with

4

probability greater than 1 − δ. Substitution of the bound on Eρ [V(h)] from Theorem 3 into the
PAC-Bayes-Bernstein inequality in Theorem 2 yields the main result of our paper  the PAC-Bayes-
Empirical-Bernstein inequality  that controls the loss of Gibbs regression rule Eρ [L(h)] for all pos-
terior distributions ρ simultaneously.
Theorem 4. Let Vn(ρ) denote the right hand side of (6) (with δ2 = δ
with probability greater than 1 − δ (over a draw of S) we have:

(cid:1). For any ﬁxed distribution π over H  for any δ > 0  and for any c1  c2 > 1 

min(cid:0)Vn(ρ)  1

2 ) and let ¯Vn(ρ) =

4

(cid:115)

(e − 2) ¯Vn(ρ)(cid:0)KL(ρ(cid:107)π) + ln 2ν1

(cid:1)

δ

(7)

L(Gρ) ≤ Ln(Gρ) + (1 + c1)

simultaneously for all distributions ρ over H that satisfy
KL(ρ(cid:107)π) + ln 2ν1
(e − 2) ¯Vn(ρ)

δ

(cid:115)

n

≤ √

n 

where ν1 was deﬁned in Theorem 2 (with δ1 = δ

2 )  and for all other ρ we have:

L(Gρ) ≤ Ln(Gρ) + 2

KL(ρ(cid:107)π) + ln 2ν1

δ

.

n

Note that all the quantities in Theorem 4 are computable based on the sample.
√
n) term in PAC-Bayes-Empirical-Bernstein
As we can see immediately by comparing the O(1/
inequality (PB-EB for brevity) with the corresponding term in the relaxed version of the PAC-Bayes-
kl inequality (PB-kl for brevity) in equation (3)  the PB-EB inequality can potentially be tighter
when Eρ [Vn(h)] ≤ (1/(2(e − 2)))Ln(Gρ) ≈ 0.7Ln(Gρ). We also note that when the loss is
bounded in the [0 1] interval we have Vn(h) ≤ (n/(n − 1))Ln(h) (since (cid:96)h(X  Y )2 ≤ (cid:96)h(X  Y )).
Therefore  the PB-EB bound is never much worse than the PB-kl bound and if the empirical variance
is small compared to the empirical loss it can be much tighter. We note that for the binary loss
((cid:96)(y  y(cid:48)) ∈ {0  1}) we have V(h) = L(h)(1 − L(h)) and in this case the empirical variance cannot
be signiﬁcantly smaller than the empirical loss and PB-EB does not provide an advantage over
PB-kl. We also note that the unrelaxed version of the PB-kl inequality in equation (1) has better
behavior for very small sample sizes and in such cases PB-kl can be tighter than PB-EB even when
the empirical variance is small. To summarize the discussion  when Eρ [Vn(h)] ≤ 0.7Ln(Gρ) the
PB-EB inequality can be signiﬁcantly tighter than the PB-kl bound and otherwise it is comparable
(except for very small sample sizes). In Section 5 we provide a more detailed numerical comparison
of the two inequalities.

4 Proofs

In this section we present a sketch of a proof of Theorem 3 and a proof of Theorem 4. Full details
of the proof of Theorem 3 are provided in the supplementary material. The proof of Theorem 3 is
based on the following lemma  which is at the base of all PAC-Bayesian theorems. (Since we could
not ﬁnd a reference  where the lemma is stated explicitly its proof is provided in the supplementary
material.)
Lemma 1. For any function fn : H × (X × Y)n → R and for any distribution π over H  such
that π is independent of S  with probability greater than 1 − δ over a random draw of S  for all
distributions ρ over H simultaneously:

Eρ [fn(h  S)] ≤ KL(ρ(cid:107)π) + ln

+ ln Eπ

1
δ

(cid:104)ES(cid:48)∼Dn

(cid:104)
efn(h S(cid:48))(cid:105)(cid:105)

.

(8)

The smart part is to choose fn(h  S) so that we get the quantities of interest on the left hand side
of (8) and at the same time are able to bound the last term on the right hand side of (8). Bounding
of the moment generating function (the last term in (8)) is usually done by involving some known
concentration of measure results. In the proof of Theorem 3 we use the fact that nVn(h) satisﬁes
the self-bounding property [14]. Speciﬁcally  for any λ > 0:
n2
n−1

V(h)(cid:105) ≤ 1

eλ(nV(h)−nVn(h))− λ2

(cid:104)

ES∼Dn

(9)

2

5

(a) n = 1000

(b) n = 4000

Figure 1: The Ratio of the gap between PB-EB and Ln(Gρ) to the gap between PB-kl and Ln(Gρ)
for different values of n  Eρ[Vn(h)]  and Ln(Gρ). PB-EB is tighter below the dashed line with label
1. The axes of the graphs are in log scale.

(see  for example  [9  Theorem 10]). We take fn(h  S) = λ(cid:0)nV(h) − nVn(h)(cid:1) − λ2

V(h)
and substitute fn and the bound on its moment generating function in (9) into (8). To complete the
proof it is left to optimize the bound with respect to λ. Since it is impossible to minimize the bound
simultaneously for all ρ with a single value of λ  we follow the technique suggested by Seldin et. al.
and take a grid of λ-s in a form of a geometric progression and apply a union bound over this grid.
Then  for each ρ we pick a value of λ from the grid  which is the closest to the value of λ that
minimizes the bound for the corresponding ρ. (The approximation of the optimal λ by the closest λ
from the grid is behind the factor c2 in the bound and the ln ν2 factor is the result of the union bound
over the grid of λ-s.) Technical details of the derivation are provided in the supplementary material.

n2
n−1

2

Proof of Theorem 4. By our choice of δ1 = δ2 = δ
2 the upper bounds of Theorems 2 and 3 hold
simultaneously with probability greater than 1 − δ. Therefore  with probability greater than 1 − δ2
we have Eρ [V(h)] ≤ ¯Vn(h) ≤ 1

4 and the result follows by Theorem 2.

5 Experiments

Before presenting the experiments we present a general comparison of the behavior of the PB-EB
and PB-kl bounds as a function of Ln(Gρ)  Eρ [Vn(h)]  and n. In Figure 1.a and 1.b we examine
the ratio of the complexity parts of the two bounds

PB-EB − Ln(Gρ)
PB-kl − Ln(Gρ)

 

where PB-EB is used to denote the value of the PB-EB bound in equation (7) and PB-kl is used
to denote the value of the PB-kl bound in equation (1). The ratio is presented in the Ln(Gρ) by
Eρ [Vn(h)] plane for two values of n. In the illustrative comparison we took KL(ρ(cid:107)π) = 18 and in
all the experiments presented in this section we take c1 = c2 = 1.15 and δ = 0.05. As we wrote
in the discussion of Theorem 4  PB-EB is never much worse than PB-kl and when Eρ [Vn(h)] (cid:28)
Ln(Gρ) it can be signiﬁcantly tighter. In the illustrative comparison in Figure 1  in the worst case
the ratio is slightly above 2.5 and in the best case it is slightly above 0.3. We note that as the sample
size grows the worst case ratio decreases (asymptotically down to 1.2) and the improvement of the
best case ratio is unlimited.
As we already said  the advantage of the PB-EB inequality over the PB-kl inequality is most promi-
nent in regression (for classiﬁcation with zero-one loss it is roughly comparable to PB-kl). Below
we provide regression experiments with L1 loss on synthetic data and three datasets from the UCI
repository [15]. We use the PB-EB and PB-kl bounds to bound the loss of a regularized empirical

6

0.0010.010.10.0010.010.1Average empirical lossAverage sample variance  112211.522.50.0010.010.10.0010.010.1Average empirical lossAverage sample variance  0.50.511120.511.522.5(cid:110)

risk minimization algorithm. In all our experiments the inputs Xi lie in a d-dimensional unit ball
centered at the origin ((cid:107)Xi(cid:107)2 ≤ 1) and the outputs Y take values in [−0.5  0.5]. The hypothesis
class HW is deﬁned as

HW =

hw(X) = (cid:104)w  X(cid:105) : w ∈ Rd (cid:107)w(cid:107)2 ≤ 0.5

This construction ensures that the L1 regression loss (cid:96)(y  y(cid:48)) = |y − y(cid:48)| is bounded in the [0  1]

interval. We use uniform prior distribution over HW deﬁned by π(w) = (cid:0)V (1/2  d)(cid:1)−1  where

V (r  d) is the volume of a d-dimensional ball with radius r. The posterior distribution ρ ˆw is taken to
be a uniform distribution on a d-dimensional ball of radius  centered at the weight vector ˆw  where
ˆw is the solution of the following minimization problem:

(cid:111)

.

n(cid:88)

i=1

ˆw = arg min

w

1
n

|Yi − (cid:104)w  Xi(cid:105)| + λ∗(cid:107)w(cid:107)2
2.

(10)

Note that (10) is a quadratic program and can be solved by various numerical solvers (we used
Matlab quadprog). The role of the regularization parameter λ∗(cid:107)w(cid:107)2
2 is to ensure that the posterior
distribution is supported by HW . We use binary search in order to ﬁnd the minimal (non-negative)
λ∗  such that the posterior ρ ˆw is supported by HW (meaning that the ball of radius  around ˆw is
within the ball of radius 0.5 around the origin). In all the experiments below we used  = 0.05.

5.1 Synthetic data

Our synthetic datasets are produced as follows. We take inputs X1  . . .   Xn uniformly distributed in
a d-dimensional unit ball centered at the origin. Then we deﬁne
Yi = σ0 (50 · (cid:104)w0  Xi(cid:105)) + i

with weight vector w0 ∈ Rd  centred sigmoid function σ0(z) = 1
[−0.5  0.5]  and noise i independent of Xi and uniformly distributed in [−ai  ai] with
for σ0(50 · (cid:104)w0  Xi(cid:105)) ≥ 0;
for σ0(50 · (cid:104)w0  Xi(cid:105)) < 0.

(cid:26)min(cid:0)0.1  0.5 − σ0(50 · (cid:104)w0  Xi(cid:105))(cid:1) 
min(cid:0)0.1  0.5 + σ0(50 · (cid:104)w0  Xi(cid:105))(cid:1) 

ai =

1+e−z − 0.5 which takes values in

This design ensures that Yi ∈ [−0.5  0.5]. The sigmoid function creates a mismatch between the
data generating distribution and the linear hypothesis class. Together with relatively small level of
the noise (i ≤ 0.1) this results in small empirical variance of the loss Vn(h) and medium to high
empirical loss Ln(h). Let us denote the j-th coordinate of a vector u ∈ Rd by uj and the number
of nonzero coordinates of u by (cid:107)u(cid:107)0. We choose the weight vector w0 to have only a few nonzero
coordinates and consider two settings. In the ﬁrst setting d ∈ {2  5}  (cid:107)w0(cid:107)0 = 2  w1
0 = 0.12  and
0 = −0.04 and in the second setting d ∈ {3  6}  (cid:107)w0(cid:107)0 = 3  w1
0 = −0.08  w2
0 = 0.05  and
w2
0 = 0.2.
w3
For each sample size ranging from 300 to 4000 we averaged the bounds over 10 randomly generated
datasets. The results are presented in Figure 2. We see that except for very small sample sizes
(n < 1000) the PB-EB bound outperforms the PB-kl bound. Inferior performance for very small
sample sizes is a result of domination of the O(1/n) term in the PB-EB bound (7). As soon as n
gets large enough this term signiﬁcantly decreases and PB-EB dominates PB-kl.

5.2 UCI datasets

We compare our PAC-Bayes-Empirical-Bernstein inequality (7) with the PAC-Bayes-kl inequal-
ity (1) on three UCI regression datasets: Wine Quality  Parkinsons Telemonitoring  and Concrete
Compressive Strength. For each dataset we centred and normalised both outputs and inputs so that
Yi ∈ [−0.5  0.5] and (cid:107)Xi(cid:107) ≤ 1. The results for 5-fold train-test split of the data together with basic
descriptions of the datasets are presented in Table 1.

6 Conclusions and future work

We derived a new PAC-Bayesian bound that controls the convergence of averages of empirical vari-
ances of losses of hypotheses in H to averages of expected variances of losses of hypothesis in H si-
multaneously for all averaging distributions ρ. This bound is an interesting example of combination

7

(a) d = 2  (cid:107)w0(cid:107)0 = 2

(b) d = 5  (cid:107)w0(cid:107)0 = 2

(c) d = 3  (cid:107)w0(cid:107)0 = 3

(d) d = 6  (cid:107)w0(cid:107)0 = 3

Figure 2: The values of the PAC-Bayes-kl and PAC-Bayes-Empirical-Bernstein bounds together
with the test and train errors on synthetic data. The values are averaged over the 10 random draws
of training and test sets.

Table 1: Results for the UCI datasets

Dataset

winequality
parkinsons
concrete

n

6497
5875
1030

d
11
16
8

Train

0.106 ± 0.0005
0.188 ± 0.0014
0.110 ± 0.0008

Test

0.106 ± 0.0022
0.188 ± 0.0055
0.111 ± 0.0038

PB-kl bound
0.175 ± 0.0006
0.266 ± 0.0013
0.242 ± 0.0010

PB-EB bound
0.162 ± 0.0006
0.250 ± 0.0012
0.264 ± 0.0011

of PAC-Bayesian bounding technique with concentration inequalities for self-bounding functions.
We applied the bound to derive the PAC-Bayes-Empirical-Bernstein inequality which is a powerful
Bernstein-type inequality outperforming the state-of-the-art PAC-Bayes-kl inequality of Seeger [7]
in situations  where the empirical variance is smaller than the empirical loss and otherwise com-
parable to PAC-Bayes-kl. We also demonstrated an empirical advantage of the new PAC-Bayes-
Empirical-Bernstein inequality over the PAC-Bayes-kl inequality on several synthetic and real-life
regression datasets.
Our work opens a number of interesting directions for future research. One of the most important of
them is to derive algorithms that will directly minimize the PAC-Bayes-Empirical-Bernstein bound.
Another interesting direction would be to decrease the last term in the bound in Theorem 3  as it is
done in the PAC-Bayes-kl inequality. This can probably be achieved by deriving a PAC-Bayes-kl
inequality for the variance.

Acknowledgments

The authors are thankful to Anton Osokin for useful discussions and to the anonymous reviewers
for their comments. This research was supported by an Australian Research Council Australian
Laureate Fellowship (FL110100281) and a Russian Foundation for Basic Research grants 13-07-
00677  14-07-00847.

References

[1] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In Advances in Neural Infor-

mation Processing Systems (NIPS)  2002.

[2] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning  51(1)  2003.
[3] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine

Learning Research  2005.

[4] Yevgeny Seldin and Naftali Tishby. PAC-Bayesian analysis of co-clustering and beyond. Jour-

nal of Machine Learning Research  11  2010.

[5] Matthew Higgs and John Shawe-Taylor. A PAC-Bayes bound for tailored density estimation.
In Proceedings of the International Conference on Algorithmic Learning Theory (ALT)  2010.

8

1000200030000.20.250.30.350.4Sample sizeExpected loss  PB−EBPB−klTrain errorTest error1000200030000.20.30.40.5Sample sizeExpected loss  PB−EBPB−klTrain errorTest error1000200030000.20.30.40.5Sample sizeExpected loss  PB−EBPB−klTrain errorTest error1000200030000.20.30.40.5Sample sizeExpected loss  PB−EBPB−klTrain errorTest error[6] Yevgeny Seldin  Peter Auer  Franc¸ois Laviolette  John Shawe-Taylor  and Ronald Ortner. PAC-
Bayesian analysis of contextual bandits. In Advances in Neural Information Processing Sys-
tems (NIPS)  2011.

[7] Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁca-

tion. Journal of Machine Learning Research  2002.

[8] Yevgeny Seldin  Franc¸ois Laviolette  Nicol`o Cesa-Bianchi  John Shawe-Taylor  and Peter
Auer. PAC-Bayesian inequalities for martingales. IEEE Transactions on Information Theory 
58  2012.

[9] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample variance
penalization. In Proceedings of the International Conference on Computational Learning The-
ory (COLT)  2009.

[10] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley & Sons 

1991.

[11] Andreas Maurer. A note on the PAC-Bayesian theorem. www.arxiv.org  2004.
[12] David McAllester. Some PAC-Bayesian theorems. Machine Learning  37  1999.
[13] A.W. Van Der Vaart. Asymptotic statistics. Cambridge University Press  1998.
[14] St´ephane Boucheron  G´abor Lugosi  and Olivier Bousquet. Concentration inequalities.

In
O. Bousquet  U.v. Luxburg  and G. R¨atsch  editors  Advanced Lectures in Machine Learning.
Springer  2004.
[15] A. Asuncion
http://www.ics.uci.edu/∼mlearn/MLRepository.html.

and D.J. Newman.

2007.

UCI machine

learning

repository 

9

,Ilya Tolstikhin
Yevgeny Seldin
Thanh Huy Nguyen
Umut Simsekli
Mert Gurbuzbalaban
Gaël RICHARD