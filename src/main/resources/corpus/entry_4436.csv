2014,Probabilistic ODE Solvers with Runge-Kutta Means,Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs)  and the basis for the state of the art. Like most numerical methods  they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work  we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly  thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction  provide a richer  probabilistic output  have low computational cost  and raise new research questions.,Probabilistic ODE Solvers with Runge-Kutta Means

Michael Schober

MPI for Intelligent Systems

Tübingen  Germany

mschober@tue.mpg.de

David Duvenaud

Philipp Hennig

Department of Engineering

MPI for Intelligent Systems

Cambridge University
dkd23@cam.ac.uk

Tübingen  Germany

phennig@tue.mpg.de

Abstract

Runge-Kutta methods are the classic family of solvers for ordinary differential
equations (ODEs)  and the basis for the state of the art. Like most numerical meth-
ods  they return point estimates. We construct a family of probabilistic numerical
methods that instead return a Gauss-Markov process deﬁning a probability distribu-
tion over the ODE solution. In contrast to prior work  we construct this family such
that posterior means match the outputs of the Runge-Kutta family exactly  thus in-
heriting their proven good properties. Remaining degrees of freedom not identiﬁed
by the match to Runge-Kutta are chosen such that the posterior probability measure
ﬁts the observed structure of the ODE. Our results shed light on the structure of
Runge-Kutta solvers from a new direction  provide a richer  probabilistic output 
have low computational cost  and raise new research questions.

1

Introduction

Differential equations are a basic feature of dynamical systems. Hence  researchers in machine
learning have repeatedly been interested in both the problem of inferring an ODE description from
observed trajectories of a dynamical system [1  2  3  4]  and its dual  inferring a solution (a trajectory)
for an ODE initial value problem (IVP) [5  6  7  8]. Here we address the latter  classic numerical
problem. Runge-Kutta (RK) methods [9  10] are standard tools for this purpose. Over more than a
century  these algorithms have matured into a very well-understood  efﬁcient framework [11].
As recently pointed out by Hennig and Hauberg [6]  since Runge-Kutta methods are linear extrapola-
tion methods  their structure can be emulated by Gaussian process (GP) regression algorithms. Such
an algorithm was envisioned by Skilling in 1991 [5]  and the idea has recently attracted both theoreti-
cal [8] and practical [6  7] interest. By returning a posterior probability measure over the solution
of the ODE problem  instead of a point estimate  Gaussian process solvers extend the functionality
of RK solvers in ways that are particularly interesting for machine learning. Solution candidates
can be drawn from the posterior and marginalized [7]. This can allow probabilistic solvers to stop
earlier  and to deal (approximately) with probabilistically uncertain inputs and problem deﬁnitions
[6]. However  current GP ODE solvers do not share the good theoretical convergence properties of
Runge-Kutta methods. Speciﬁcally  they do not have high polynomial order  explained below.
We construct GP ODE solvers whose posterior mean functions exactly match those of the RK families
of ﬁrst  second and third order. This yields a probabilistic numerical method which combines the
strengths of Runge-Kutta methods with the additional functionality of GP ODE solvers. It also
provides a new interpretation of the classic algorithms  raising new conceptual questions.
While our algorithm could be seen as a “Bayesian” version of the Runge-Kutta framework  a
philosophically less loaded interpretation is that  where Runge-Kutta methods ﬁt a single curve (a
point estimate) to an IVP  our algorithm ﬁts a probability distribution over such potential solutions 
such that the mean of this distribution matches the Runge-Kutta estimate exactly. We ﬁnd a family of
models in the space of Gaussian process linear extrapolation methods with this property  and select a
member of this family (ﬁx the remaining degrees of freedom) through statistical estimation.

1

p= 1

0

0
1

p= 2
(1− 1

0
α

2α

)

0
α

p= 3
v− v(v−u)
6u(u−v)− 2−3u
1− 2−3v
u(2−3u)
6v(v−u)

0
u

0
u
v

0

v(v−u)
u(2−3u)
2−3v
6u(u−v)

0

0
1
2α

2 Background

2−3u
6v(v−u)
Table 1: All consistent Runge-Kutta methods of order p≤ 3 and number of stages s= p (see [11]).
An ODE Initial Value Problem (IVP) is to ﬁnd a function x(t)∶ R→ RN such that the ordinary
differential equation ˙x= f(x  t) (where ˙x= ∂x~∂t) holds for all t∈ T =[t0  tH]  and x(t0)= x0.
contiguous subintervals[tn  tn+ h]⊂ T of length h. Assume for the moment that n= 0. Within
[t0  t0+ h]  an RK method of stage s collects evaluations yi= f(ˆxi  t0+ hci) at s recursively deﬁned
input locations  i= 1  . . .   s  where ˆxi is constructed linearly from the previously-evaluated yj<i as
then returns a single prediction for the solution of the IVP at t0+ h  as ˆx(t0+ h)= x0+ h∑s

We assume that a unique solution exists. To keep notation simple  we will treat x as scalar-valued;
the multivariate extension is straightforward (it involves N separate GP models  explained in supp.).
Runge-Kutta methods1 [9  10] are carefully designed linear extrapolation methods operating on small

i−1Q
j=1

i=1 biyi

i−1Q
j=1

yi= f

ïïx0+ h

wijyj  t0+ hci

(modern variants can also construct non-probabilistic error estimates  e.g. by combining the same
observations into two different RK predictions [12]). In compact form 

ˆx(t0+ h)= x0+ h
sQ
i=1
ˆx(t0+ h) is then taken as the initial value for t1= t0+ h and the process is repeated until tn+ h≥ tH.
A Runge-Kutta method is thus identiﬁed by a lower-triangular matrix W ={wij}  and vectors
c=[c1  . . .   cs]  b=[b1  . . .   bs]  often presented compactly in a Butcher tableau [13]:

i= 1  . . .   s 

ˆxi= x0+ h
ï  

wijyj 

biyi.

(2)

(1)

⋮



0
c1
c2 w21
0
c3 w31 w32

⋮
⋮

cs ws1 ws2  ws s−1
b2  bs−1

b1

0

0
bs

As Hennig and Hauberg [6] recently pointed out  the linear structure of the extrapolation steps in
Runge-Kutta methods means that their algorithmic structure  the Butcher tableau  can be constructed

proper RK methods have structure that is not generally reproduced by an arbitrary Gaussian pro-
cess prior on x: Their distinguishing property is that the approximation ˆx and the Taylor series

naturally from a Gaussian process regression method over x(t)  where the yi are treated as “ob-
servations” of ˙x(t0+ hci) and the ˆxi are subsequent posterior estimates (more below). However 
of the true solution coincide at t0+ h up to the p-th term—their numerical error is bounded by
x(t0+ h)− ˆx(t0+ h)≤ Khp+1 for some constant K (higher orders are better  because h is assumed
p= s. This is only possible for p< 5 [14  15]. There are no methods of order p> s. High order is a
Table 1 lists all consistent methods of order p≤ 3 where s= p. For s= 1  only Euler’s method (linear
extrapolation) is consistent. For s= 2  there exists a family of methods of order p= 2  parametrized

to be small). The method is then said to be of order p [11]. A method is consistent  if it is of order

strong desideratum for ODE solvers  not currently offered by Gaussian process extrapolators.

1In this work  we only address so-called explicit RK methods (shortened to “Runge-Kutta methods” for
simplicity). These are the base case of the extensive theory of RK methods. Many generalizations can be found
in [11]. Extending the probabilistic framework discussed here to the wider Runge-Kutta class is not trivial.

2

by a single parameter α∈(0  1]  where α= 1~2 and α= 1 mark the midpoint rule and Heun’s method 
respectively. For s= 3  third order methods are parameterized by two variables u  v∈(0  1].
We will use the standard notation µ∶ R→ R for the mean function  and k∶ R× R→ R for the
covariance function; kU V for Gram matrices of kernel values k(ui  vj)  and analogous for the mean
function: µT =[µ(t1)  . . .   µ(tN)]. A GP prior p(x)=GP(x; µ  k) and observations(T  Y)=
{(t1  y1)  . . .  (ts  ys)} having likelihoodN(Y ; xT   Λ) give rise to a posteriorGP s(x; µs  ks) with

Gaussian processes (GPs) are well-known in the NIPS community  so we omit an introduction.

= µt+ ktT(kT T+ Λ)−1(Y − µT)
 ; µ
=GPx
µ∂   k
k∂ = ∂k(t  t′)
k∂= ∂k(t  t′)
∂t′

px
µ∂= ∂µ(t)

= kuv− kuT(kT T+ Λ)−1kT v.
k∂ ∂= ∂2k(t  t′)
∂t∂t′



k∂
k∂ ∂

GPs are closed under linear maps. In particular  the joint distribution over x and its derivative is

(3)

(4)

with

ks
uv

and

(5)

µs
t

∂t

∂t

A recursive algorithm analogous to RK methods can be constructed [5  6] by setting the prior mean

to the constant µ(t)= x0  then recursively estimating ˆxi in some form from the current posterior
over x. The choice in [6] is to set ˆxi= µi(t0+ hci). “Observations” yi= f(ˆxi  t0+ hci) are then
incorporated with likelihood p(yi x)=N(yi; ˙x(t0+ hci)  Λ). This recursively gives estimates
ˆx(t0+ hci)= x0+ i−1Q
k∂(t0+ hci  t0+ hc(cid:96))( K∂
i−1Q
j=1
(cid:96)=1
ij= k∂ ∂(t0+ hci  t0+ hcj). The ﬁnal prediction is the posterior mean at this point:
k∂(t0+ h  t0+ hcj)( K∂
ˆx(t0+ h)= x0+ sQ
sQ
i=1
j=1

(cid:96)j yj= x0+ hQ
ji yi= x0+ h
sQ

∂+ Λ)−1
∂+ Λ)−1

with K∂

wijyj 

biyi.

(7)

(6)

∂

j

i

˙x

 

˙x

k∂

 

 

.

3 Results

The described GP ODE estimate shares the algorithmic structure of RK methods (i.e. they both
use weighted sums of the constructed estimates to extrapolate). However  in RK methods  weights
and evaluation positions are found by careful analysis of the Taylor series of f  such that low-order
terms cancel. In GP ODE solvers they arise  perhaps more naturally but also with less structure 
by the choice of the ci and the kernel. In previous work [6  7]  both were chosen ad hoc  with no
guarantee of convergence order. In fact  as is shown in the supplements  the choices in these two
works—square-exponential kernel with ﬁnite length-scale  evaluations at the predictive mean—do not
even give the ﬁrst order convergence of Euler’s method. Below we present three speciﬁc regression
models based on integrated Wiener covariance functions and speciﬁc evaluation points. Each model is
the improper limit of a Gauss-Markov process  such that the posterior distribution after s evaluations

is a proper Gaussian process  and the posterior mean function at t0+ h coincides exactly with the

Runge-Kutta estimate. We will call these methods  which give a probabilistic interpretation to RK
methods and extend them to return probability distributions  Gauss-Markov-Runge-Kutta (GMRK)
methods  because they are based on Gauss-Markov priors and yield Runge-Kutta predictions.

3.1 Design choices and desiderata for a probabilistic ODE solver

Although we are not the ﬁrst to attempt constructing an ODE solver that returns a probability
distribution  open questions still remain about what  exactly  the properties of such a probabilistic
numerical method should be. Chkrebtii et al. [8] previously made the case that Gaussian measures
are uniquely suited because solution spaces of ODEs are Banach spaces  and provided results on
consistency. Above  we added the desideratum for the posterior mean to have high order  i.e. to
reproduce the Runge-Kutta estimate. Below  three additional issues become apparent:

“nodes” ˆx(t0+ hci) at the current posterior mean of the belief. We will ﬁnd that this can be made

Motivation of evaluation points Both Skilling [5] and Hennig and Hauberg [6] propose to put the

3

1st order (Euler)

2nd order (midpoint)

3rd order (u= 1~4  v= 3~4)

x

)
t
(
µ
−
x

0

t0+ h

t0

t

t0+ h

t0

t

t0+ h

t0

Initial value at t0 = 1 (ﬁlled blue).

t

Figure 1: Top: Conceptual sketches. Prior mean in gray.
Gradient evaluations (empty blue circles  lines). Posterior (means) after ﬁrst  second and third
gradient observation in orange  green and red respectively. Samples from the ﬁnal posterior as dashed
lines. Since  for the second and third-order methods  only the ﬁnal prediction is a proper probability
distribution  for intermediate steps only mean functions are shown. True solution to (linear) ODE in
black. Bottom: For better visibility  same data as above  minus ﬁnal posterior mean.

third-order methods will be forced to use a node ˆx(t0+ hci) that  albeit lying along a function w(t)

consistent with the order requirement for the RK methods of ﬁrst and second order. However  our

in the reproducing kernel Hilbert space associated with the posterior GP covariance function  is not
the mean function itself. It will remain open whether the algorithm can be amended to remove this
blemish. However  as the nodes do not enter the GP regression formulation  their choice does not
directly affect the probabilistic interpretation.

convergence order only holds strictly for the ﬁrst extrapolation interval[t0  t0+ h]. From the second

Extension beyond the ﬁrst extrapolation interval

Importantly  the Runge-Kutta argument for

interval onward  the RK step solves an estimated IVP  and begins to accumulate a global estimation
error not bounded by the convergence order (an effect termed “Lady Windermere’s fan” by Wanner
[16]). Should a probabilistic solver aim to faithfully reproduce this imperfect chain of RK solvers  or
rather try to capture the accumulating global error? We investigate both options below.

Calibration of uncertainty A question easily posed but hard to answer is what it means for the
probability distribution returned by a probabilistic method to be well calibrated. For our Gaussian
case  requiring RK order in the posterior mean determines all but one degree of freedom of an answer.
The remaining parameter is the output scale of the kernel  the “error bar” of the estimate. We offer a
relatively simple statistical argument below that ﬁts this parameter based on observed values of f.
We can now proceed to the main results. In the following  we consider extrapolation algorithms
based on Gaussian process priors with vanishing prior mean function  noise-free observation model

(Λ= 0 in Eq. (3)). All covariance functions in question are integrals over the kernel k0(˜t  ˜t′)=
σ2 min(˜t− τ  ˜t′− τ) (parameterized by scale σ2> 0 and off-set τ∈ R; valid on the domain ˜t  ˜t′> τ) 
cost [18]. We will use the shorthands t= ˜t− τ and t′= ˜t′− τ for inputs shifted by τ.
Theorem 1. The once-integrated Wiener process prior p(x)=GP(x; 0  k1) with
′ min2(t  t′)

the covariance of the Wiener process [17]. Such integrated Wiener processes are Gauss-Markov
processes  of increasing order  so inference in these methods can be performed by ﬁltering  at linear

k0(u  v)du dv= σ2 min3(t  t′)

3.2 Gauss-Markov methods matching Euler’s method

′)=Þ ˜t ˜t

+t− t

k1(t  t



(8)

′

τ

3

2

choosing evaluation nodes at the posterior mean gives rise to Euler’s method.

4

Proof. We show that the corresponding Butcher tableau from Table 1 holds. After “observing” the
initial value  the second observation y1  constructed by evaluating f at the posterior mean at t0  is

y1= fµx0

(t0)  t0= f k(t0  t0)
k(t0  t0) x0  t0= f(x0  t0) 
k∂(t0  t0)
(t0+ h)=k(t0+ h  t0) k∂(t0+ h  t0)(cid:6) k(t0  t0)
k∂ ∂(t0  t0)−1x0
k∂(t0  t0)

directly from the deﬁnitions. The posterior mean after incorporating y1 is

µx0 y1

y1

(9)

= x0+ hy1.

(10)

An explicit linear algebraic derivation is available in the supplements.

′

τ

2

12

20

(t+ t

+t− t′

′) min3(t  t

3.3 Gauss-Markov methods matching all Runge-Kutta methods of second order

inﬁnity. Fortunately  this limit still leads to a proper posterior probability distribution.

(The twice-integrated Wiener process is a proper Gauss-Markov process for all ﬁnite values of τ and

Extending to second order is not as straightforward as integrating the Wiener process a second time.

k1(u  v)du dv= σ2 min5(t  t′)

(11)
Choosing evaluation nodes at the posterior mean gives rise to the RK family of second order methods

The theorem below shows that this only works after moving the onset−τ of the process towards
Theorem 2. Consider the twice-integrated Wiener process prior p(x)=GP(x; 0  k2) with
′)− min4(t  t′)
 .
′)=Þ ˜t ˜t
k2(t  t
in the limit of τ→∞.
˜t  ˜t′> 0. In the limit of τ→∞  it turns into an improper prior of inﬁnite local variance.)
as in Eq. (9). Because y2= f(x0+ hαy1  t0+ hα)  we need to show µx0 y1
(t0+ hα)= x0+ hαy1.
Therefore  let α∈(0  1] arbitrary but ﬁxed:
k∂(t0  t0)
(t0+ hα)=k(t0+ h  t0) k∂(t0+ h  t0)(cid:6) k(t0  t0)
k∂ ∂(t0  t0)−1x0

k∂ (t0  t0)
0~20
0~8
−1x0
t5
0~3
0~8
0
06(hα)2+8hαt0+3t2

x0

Proof. The proof is analogous to the previous one. We need to show all equations given by the
Butcher tableau and choice of parameters hold for any choice of α. The constraint for y1 holds trivially

µx0 y1



t4
t3

y1

y1

= t3
0
010(hα)2+15hαt0+6t2
=1− 10(hα)2
hα+ 2(hα)2
τ→∞ x0+ hαy1
———→

3t2
0

120

t0

As t0= ˜t0− τ  the mismatched terms vanish for τ→∞. Finally  extending the vector and matrix with
(t0+ h)= x0+ h(1− 1~2α)y1+
h~2αy2 also holds  analogous to Eq. (10). Omitted details can be found in the supplements. They also
one more entry  a lengthy computation shows that limτ→∞ µx0 y1 y2

include the ﬁnal-step posterior covariance. Its ﬁnite values mean that this posterior indeed deﬁnes a
proper GP.

(12)

y1

24

t4

t2

3.4 A Gauss-Markov method matching Runge-Kutta methods of third order

Moving from second to third order  additionally to the limit towards an improper prior  also requires
a departure from the policy of placing extrapolation nodes at the posterior mean.

Theorem 3. Consider the thrice-integrated Wiener process prior p(x)=GP(x; 0  k3) with
k3(t  t
′) .

′)=Þ ˜t ˜t
k2(u  v)du dv
= σ2 min7(t  t′)

+t− t′ min4(t  t′)

′+ 3 min2(t  t

5 max2(t  t

′)+ 2tt

(13)

′

τ

252

720

5

Evaluating twice at the posterior mean and a third time at a speciﬁc element of the posterior
covariance functions’ RKHS gives rise to the entire family of RK methods of third order  in the limit

of τ→∞.
for the term where the mean does not match the RK weights exactly. This is the case for y3 =
x0+ h[(v− v(v−u)~u(2−3u))y1+ v(v−u)~u(2−3u)y2] (see Table 1). The weights of Y which give the
posterior mean at this point are given by kK−1 (cf. Eq. (3)  which  in the limit  has value (see supp.):

Proof. The proof progresses entirely analogously as in Theorems 1 and 2  with one exception

2u

2u

lim

−1

τ→∞k(t0+ hv  t0) k∂(t0+ hv  t0) k∂(t0+ hv  t0+ hu)(cid:6) K
(cid:6)
=1 h(v− v2
) h v2
=1 hv− v(v−u)
2(3u−2)
u(2−3u)− v(3v−2)
2(3u−2) h v(v−u)
u(2−3u)+ v(3v−2)
u(2−3u)+0 −h v(3v−2)
u(2−3u) h v(v−u)
=1 hv− v(v−u)
2(3u−2)
2(3u−2) h v(3v−2)
3v− 2
ε(v)= v
3u− 2

2

However  it can be produced by adding a correction term w(v)= µ(v)+ ε(v)(y2− y1) where

This means that the ﬁnal RK evaluation node does not lie at the posterior mean of the regressor.

(14)

(15)

is a second-order polynomial in v. Since k is of third or higher order in v (depending on the value
of u)  w can be written as an element of the thrice integrated Wiener process’ RKHS [19  §6.1].
Importantly  the ﬁnal extrapolation weights b under the limit of the Wiener process prior again match
the RK weights exactly  regardless of how y3 is constructed.

We note in passing that Eq. (15) vanishes for v= 2~3. For this choice  the RK observation y2 is
for α for which the posterior variance at t0+ h is minimized.

generated exactly at the posterior mean of the Gaussian process. Intriguingly  this is also the value

3.5 Choosing the output scale

The above theorems have shown that the ﬁrst three families of Runge-Kutta methods can be con-
structed from repeatedly integrated Wiener process priors  giving a strong argument for the use of such
priors in probabilistic numerical methods. However  requiring this match to a speciﬁc Runge-Kutta
family in itself does not yet uniquely identify a particular kernel to be used: The posterior mean
of a Gaussian process arising from noise-free observations is independent of the output scale (in
our notation: σ2) of the covariance function (this can also be seen by inspecting Eq. (3)). Thus  the
parameter σ2 can be chosen independent of the other parts of the algorithm  without breaking the
match to Runge-Kutta. Several algorithms using the observed values of f to choose σ2 without major
cost overhead have been proposed in the regression community before [e.g. 20  21]. For this particular
model an even more basic rule is possible: A simple derivation shows that  in all three families of

methods deﬁned above  the posterior belief over ∂sx~∂ts is a Wiener process  and the posterior mean

σ2. We choose σ2 such that this property is met  by setting σ2=[∂sµs(t)~∂ts]2.

function over the s-th derivative after all s steps is a constant function. The Gaussian model implies
that the expected distance of this function from the (zero) prior mean should be the marginal standard
deviation
Figure 1 shows conceptual sketches highlighting the structure of GMRK methods. Interestingly  in
both the second- and third-order families  our proposed priors are improper  so the solver can not
actually return a probability distribution until after the observation of all s gradients in the RK step.

√

Some observations We close the main results by highlighting some non-obvious aspects. First  it
is intriguing that higher convergence order results from repeated integration of Wiener processes.
This repeated integration simultaneously adds to and weakens certain prior assumptions in the
implicit (improper) Wiener prior: s-times integrated Wiener processes have marginal variance

ks(t  t)∝ t2s+1. Since many ODEs (e.g. linear ones) have solution paths of valuesO(exp(t))  it

is tempting to wonder whether there exists a limit process of “inﬁnitely-often integrated” Wiener
processes giving natural coverage to this domain (the results on a linear ODE in Figure 1 show how
the polynomial posteriors cannot cover the exponentially diverging true solution). In this context 

6

1

0.8

0.6

0.4

0.2

x

Naïve chaining

Smoothing

Probabilistic continuation

⋅10

−2

⋅10

−2

⋅10

−2

4

2

0

)
t
(
f
−
)
t
(
x

3h

4h

2h
t

2h
t

t0+ h

t0+ h
plots use the midpoint method and h= 1. Posterior after two steps (same for all three options) in red
(mean ±2 standard deviations). Extrapolation after 2  3  4 steps (gray vertical lines) in green. Final

Figure 2: Options for the continuation of GMRK methods after the ﬁrst extrapolation step (red). All

t0+ h

probabilistic prediction as green shading. True solution to (linear) ODE in black. Observations of x
and ˙x marked by solid and empty blue circles  respectively. Bottom row shows the same data  plotted
relative to true solution  at higher y-resolution.

3h

4h

3h

4h

2h
t

s′< s  so “highly-integrated” Wiener kernels can be used to match ﬁnite-order Runge-Kutta methods.

it is also noteworthy that s-times integrated Wiener priors incorporate the lower-order results for

Simultaneously  though  sample paths from an s-times integrated Wiener process are almost surely
s-times differentiable. So it seems likely that achieving good performance with a Gauss-Markov-
Runge-Kutta solver requires trading off the good marginal variance coverage of high-order Markov
models (i.e. repeatedly integrated Wiener processes) against modelling non-smooth solution paths
with lower degrees of integration. We leave this very interesting question for future work.

4 Experiments

Since Runge-Kutta methods have been extensively studied for over a century [11]  it is not necessary
to evaluate their estimation performance again. Instead  we focus on an open conceptual question for
the further development of probabilistic Runge-Kutta methods: If we accept high convergence order
as a prerequisite to choose a probabilistic model  how should probabilistic ODE solvers continue
after the ﬁrst s steps? Purely from an inference perspective  it seems unnatural to introduce new

evaluations of x (as opposed to ˙x) at t0+ nh for n= 1  2  . . . . Also  with the exception of the Euler

case  the posterior covariance after s evaluations is of such a form that its renewed use in the next
interval will not give Runge-Kutta estimates. Three options suggest themselves:

Naïve Chaining One could simply re-start the algorithm several times as if the previous step had
created a novel IVP. This amounts to the classic RK setup. However  it does not produce a joint
“global” posterior probability distribution (Figure 2  left column).

Smoothing An ad-hoc remedy is to run the algorithm in the “Naïve chaining” mode above  pro-

ducing N× s gradient observations and N function evaluations  but then compute a joint posterior
3  then using the remaining s(N− 1) gradients and(N− 1) function values as in standard GP

distribution by using the ﬁrst s gradient observations and 1 function evaluation as described in Section

inference. The appeal of this approach is that it produces a GP posterior whose mean goes through
the RK points (Figure 2  center column). But from a probabilistic standpoint it seems contrived. In
particular  it produces a very conﬁdent posterior covariance  which does not capture global error.

7

⋅10

−2

2

1

2nd-order GMRK
GP with SE kernel

)
t
(
f
−
)
t
(
µ

0

−1
t0+
and posterior uncertainty of GMRK (green) and SE kernel (orange). Dashed lines are+2 standard

Figure 3: Comparison of a 2nd order GMRK method and the method from [6]. Shown is error

2h
t

h

3h

4h

deviations. The SE method shown used the best out of several evaluated parameter choices.

Continuing after s evaluations Perhaps most natural from the probabilistic viewpoint is to break
with the RK framework after the ﬁrst RK step  and simply continue to collect gradient observations—
either at RK locations  or anywhere else. The strength of this choice is that it produces a continuously
growing marginal variance (Figure 2  right). One may perceive the departure from the established RK
paradigm as problematic. However  we note again that the core theoretical argument for RK methods
is only strictly valid in the ﬁrst step  the argument for iterative continuation is a lot weaker.

Figure 2 shows exemplary results for these three approaches on the (stiff) linear IVP ˙x(t)=−1~2x(t) 
x(0)= 1. Naïve chaining does not lead to a globally consistent probability distribution. Smoothing

does give this global distribution  but the “observations” of function values create unnatural nodes of
certainty in the posterior. The probabilistically most appealing mode of continuing inference directly
offers a naturally increasing estimate of global error. At least for this simple test case  it also happens
to work better in practice (note good match to ground truth in the plots). We have found similar results
for other test cases  notably also for non-stiff linear differential equations. But of course  probabilistic
continuation breaks with at least the traditional mode of operation for Runge-Kutta methods  so a
closer theoretical evaluation is necessary  which we are planning for a follow-up publication.

Comparison to Square-Exponential kernel Since all theoretical guarantees are given in forms of
upper bounds for the RK methods  the application of different GP models might still be favorable in
practice. We compared the continuation method from Fig. 2 (right column) to the ad-hoc choice of
a square-exponential (SE) kernel model  which was used by Hennig and Hauberg [6] (Fig. 3). For
this test case  the GMRK method surpasses the SE-kernel algorithm both in accuracy and calibration:
its mean is closer to the true solution than the SE method  and its error bar covers the true solution 
while the SE method is over-conﬁdent. This advantage in calibration is likely due to the more natural
choice of the output scale σ2 in the GMRK framework.

5 Conclusions

We derived an interpretation of Runge-Kutta methods in terms of the limit of Gaussian process
regression with integrated Wiener covariance functions  and a structured but nontrivial extrapolation
model. The result is a class of probabilistic numerical methods returning Gaussian process posterior
distributions whose means can match Runge-Kutta estimates exactly.
This class of methods has practical value  particularly to machine learning  where previous work has
shown that the probability distribution returned by GP ODE solvers adds important functionality over
those of point estimators. But these results also raise pressing open questions about probabilistic
ODE solvers. This includes the question of how the GP interpretation of RK methods can be extended
beyond the 3rd order  and how ODE solvers should proceed after the ﬁrst stage of evaluations.

Acknowledgments

The authors are grateful to Simo Särkkä for a helpful discussion.

8

References
[1] T. Graepel. “Solving noisy linear operator equations by Gaussian processes: Application to
ordinary and partial differential equations”. In: International Conference on Machine Learning
(ICML). 2003.

[2] B. Calderhead  M. Girolami  and N. Lawrence. “Accelerating Bayesian inference over non-
linear differential equations with Gaussian processes.” In: Advances in Neural Information
Processing Systems (NIPS). 2008.

[3] F. Dondelinger et al. “ODE parameter inference using adaptive gradient matching with Gaus-

sian processes”. In: Artiﬁcial Intelligence and Statistics (AISTATS). 2013  pp. 216–228.

[4] Y. Wang and D. Barber. “Gaussian Processes for Bayesian Estimation in Ordinary Differential

Equations”. In: International Conference on Machine Learning (ICML). 2014.
J. Skilling. “Bayesian solution of ordinary differential equations”. In: Maximum Entropy and
Bayesian Methods  Seattle (1991).

[5]

[6] P. Hennig and S. Hauberg. “Probabilistic Solutions to Differential Equations and their Applica-
tion to Riemannian Statistics”. In: Proc. of the 17th int. Conf. on Artiﬁcial Intelligence and
Statistics (AISTATS). Vol. 33. JMLR  W&CP  2014.

[7] M. Schober et al. “Probabilistic shortest path tractography in DTI using Gaussian Process
ODE solvers”. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI
2014. Springer  2014.

[8] O. Chkrebtii et al. “Bayesian Uncertainty Quantiﬁcation for Differential Equations”. In: arXiv

prePrint 1306.2365 (2013).

[9] C. Runge. “Über die numerische Auﬂösung von Differentialgleichungen”. In: Mathematische

Annalen 46 (1895)  pp. 167–178.

[10] W. Kutta. “Beitrag zur näherungsweisen Integration totaler Differentialgleichungen”. In:

Zeitschrift für Mathematik und Physik 46 (1901)  pp. 435–453.

[11] E. Hairer  S. Nørsett  and G. Wanner. Solving Ordinary Differential Equations I – Nonstiff

[12]

[13]

Problems. Springer  1987.
J. R. Dormand and P. J. Prince. “A family of embedded Runge-Kutta formulae”. In: Journal of
computational and applied mathematics 6.1 (1980)  pp. 19–26.
J. Butcher. “Coefﬁcients for the study of Runge-Kutta integration processes”. In: Journal of
the Australian Mathematical Society 3.02 (1963)  pp. 185–201.

[14] F. Ceschino and J. Kuntzmann. Problèmes différentiels de conditions initiales (méthodes

numériques). Dunod Paris  1963.

[15] E. B. Shanks. “Solutions of Differential Equations by Evaluations of Functions”. In: Mathe-

matics of Computation 20.93 (1966)  pp. 21–38.

[16] E. Hairer and C. Lubich. “Numerical solution of ordinary differential equations”. In: The

Princeton Companion to Applied Mathematics  ed. by N. Higham. PUP  2012.

[17] N. Wiener. “Extrapolation  interpolation  and smoothing of stationary time series with engi-

neering applications”. In: Bull. Amer. Math. Soc. 56 (1950)  pp. 378–381.

[18] S. Särkkä. Bayesian ﬁltering and smoothing. Cambridge University Press  2013.
[19] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT  2006.
[20] R. Shumway and D. Stoffer. “An approach to time series smoothing and forecasting using the

EM algorithm”. In: Journal of time series analysis 3.4 (1982)  pp. 253–264.

[21] Z. Ghahramani and G. Hinton. Parameter estimation for linear dynamical systems. Tech. rep.

Technical Report CRG-TR-96-2  University of Totronto  Dept. of Computer Science  1996.

9

,Michael Schober
David Duvenaud
Philipp Hennig
Sagie Benaim
Lior Wolf
Nicholas Polson
Veronika Ročková