2014,Decoupled Variational Gaussian Inference,Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. These methods are fast and easy to use  while being reasonably accurate. A difficulty remains in computation of the lower bound when the latent dimensionality $L$ is large. Even though the lower bound is concave for many models  its computation requires optimization over $O(L^2)$ variational parameters. Efficient reparameterization schemes can reduce the number of parameters  but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First  it maximizes a Lagrangian of the lower bound reducing the number of parameters to $O(N)$  where $N$ is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when the bound is not concave. Second  our method maximizes the lower bound using a sequence of convex problems  each of which is parallellizable over data examples and computes gradient efficiently. Overall  our approach avoids all direct computations of the covariance  only requiring its linear projections. Theoretically  our method converges at the same rate as existing methods in the case of concave lower bounds  while remaining convergent at a reasonable rate for the non-concave case.,Decoupled Variational Gaussian Inference

Ecole Polytechnique F´ed´erale de Lausanne (EPFL)  Switzerland

Mohammad Emtiyaz Khan

emtiyaz@gmail.com

Abstract

Variational Gaussian (VG) inference methods that optimize a lower bound to the
marginal likelihood are a popular approach for Bayesian inference. A difﬁculty
remains in computation of the lower bound when the latent dimensionality L is
large. Even though the lower bound is concave for many models  its computation
requires optimization over O(L2) variational parameters. Efﬁcient reparameter-
ization schemes can reduce the number of parameters  but give inaccurate solu-
tions or destroy concavity leading to slow convergence. We propose decoupled
variational inference that brings the best of both worlds together. First  it max-
imizes a Lagrangian of the lower bound reducing the number of parameters to
O(N )  where N is the number of data examples. The reparameterization obtained
is unique and recovers maxima of the lower-bound even when it is not concave.
Second  our method maximizes the lower bound using a sequence of convex prob-
lems  each of which is parallellizable over data examples. Each gradient compu-
tation reduces to prediction in a pseudo linear regression model  thereby avoiding
all direct computations of the covariance and only requiring its linear projections.
Theoretically  our method converges at the same rate as existing methods in the
case of concave lower bounds  while remaining convergent at a reasonable rate for
the non-concave case.

1

Introduction

Large-scale Bayesian inference remains intractable for many models  such as logistic regression 
sparse linear models  or dynamical systems with non-Gaussian observations. Approximate Bayesian
inference requires fast  robust  and reliable algorithms. In this context  algorithms based on varia-
tional Gaussian (VG) approximations are growing in popularity [17  3  13  6] since they strike a
favorable balance between accuracy  generality  speed  and ease of use.
VG inference remains problematic for models with large latent-dimensionality. While some vari-
ants are convex [3]  they require O(L2) variational parameters to be optimized  where L is the
latent-dimensionality. This slows down the optimization. One solution is to restrict the covariance
representations by naive mean-ﬁeld [2] or restricted Cholesky [3]  but this can result in considerable
loss of accuracy when signiﬁcant posterior correlations exist. An alternative is to reparameterize
the covariance to obtain O(N ) number of parameters  where N is the number of data examples
[17]. However  this destroys the convexity and converges slowly [12]. A recent approach called
dual variational inference [10] obtains fast convergence while retaining this parameterization  but is
applicable to only some models such as Poisson regression.
In this paper  we propose an approach called decoupled variational Gaussian inference which ex-
tends the dual variational inference to a large class of models. Our method relies on the theory
of Lagrangian multiplier methods. While remaining widely applicable  our approach reduces the
number of variational parameters similar to [17  10] and converges at similar convergence rates as
convex methods such as [3]. Our method is similar in spirit to parallel expectation-propagation (EP)
but has provable convergence guarantees even when likelihoods are not log-concave.

1

2 The Model

N(cid:89)

In this paper  we apply our method for Bayesian inference on Latent Gaussian Models (LGMs).
This choice is motivated by a large amount of existing work on VG approximations for LGMs
[16  17  3  10  12  11  7  2]  and because LGMs include many popular models  such as Gaussian
processes  Bayesian regression and classiﬁcation  Gaussian Markov random ﬁeld  and probabilistic
PCA. An extensive list of these models is given in Chapter 1 of [9]. We have also included few
examples in the supplementary material.
Given a vector of observations y of length N  LGMs model the dependencies among its components
using a latent Gaussian vector z of length L. The joint distribution is shown below.

pn(yn|ηn)p(z) 

p(z) := N (z|µ  Σ)

n=1

p(y  z) =

η = Wz 

(1)
where W is a known real-valued matrix of size N × L  and is used to deﬁne linear predictors η.
Each ηn is used to model the observation yn using a link function pn(yn|ηn). The exact form of
this function depends on the type of observations  e.g. a Bernoulli-logit distribution can be used for
binary data [14  7]. See the supplementary material for an example. Usually  an exponential family
distribution is used  although there are other choices (such as T-distribution [8  17]). The parameter
set θ includes {W  µ  Σ} and other parameters of the link function and is assumed to be known.
We suppress θ in our notation  for simplicity.
In Bayesian inference  we wish to compute expectations with respect to the posterior distribution
p(z|y) which is shown below. Another important task is the computation of the marginal likelihood
p(y) which can be maximized to estimate parameters θ  for example  using empirical Bayes [18].

(cid:90) N(cid:89)

p(z|y) ∝ N(cid:89)

p(yn|ηn)N (z|µ  Σ)

 

p(y) =

p(yn|ηn)N (z|µ  Σ) dz

(2)

n=1

n=1

For non-Gaussian likelihoods  both of these tasks are intractable. Applications in practice demand
good approximations that scale favorably in N and L.

3 VG Inference by Lower Bound Maximization

In variational Gaussian (VG) inference [17]  we assume the posterior to be a Gaussian q(z) =
N (z|m  V). The posterior mean m and covariance V form the set of variational parameters  and
are chosen to maximize the variational lower bound to the log marginal likelihood shown in Eq. (3).
To get this lower bound  we ﬁrst multiply and divide by q(z) and then apply Jensen’s inequality
using the concavity of log.

(cid:20)

(cid:81)

(cid:21)

(cid:90)

(cid:81)
n p(yn|ηn)p(z)

q(z)

log p(y) = log

q(z)

dz ≥ Eq(z)

log

n p(yn|ηn)p(z)

q(z)

(3)

n=1

fn( ¯mn  ¯σn) 

fn( ¯mn  ¯σn) := EN (ηn| ¯mn ¯σ2

−D[q(z)(cid:107) p(z)] − N(cid:88)

The simpliﬁed lower bound is shown in Eq. (4). The detailed derivation can be found in Eqs. (4)–(7)
in [11] (and in the supplementary material). Below  we provide a summary of its components.
n)[− log p(yn|ηn)]

(4)
max
m V(cid:31)0
The ﬁrst term is the KL-divergence D[q (cid:107) p] = Eq[log q(z) − log p(z)]  which is jointly concave in
(m  V). The second term sums over data examples  where each term denoted by fn is the expec-
tation of − log p(yn|ηn) with respect to ηn. Since ηn = wT
n z  it follows a Gaussian distribution
q(ηn) = N ( ¯mn  ¯σ2
n m and variances ¯σ2
n Vwn. The terms fn are not
n = wT
always available in closed form  but can be computed using quadrature or look-up tables [14]. Note
that unlike many other methods such [2  11  10  7  21]  we do not bound or approximate these terms.
Such approximations lead to loss of accuracy.
We denote the lower bound of Eq. (3) by f and expand it below in Eq. (5):

n) with mean ¯mn = wT

2 [log |V| − Tr(VΣ−1) − (m − µ)T Σ−1(m − µ) + L] − N(cid:88)

f (m  V) := 1

(5)
Here |V| denotes the determinant of V. We now discuss existing methods and their pros and cons.

fn( ¯mn  ¯σn)

n=1

2

3.1 Related Work

A straight-forward approach is to optimize Eq. (5) directly in (m  V) [2  3  14  11]. In practice 
direct methods are slow and memory-intensive because of the very large number L + L(L + 1)/2
of variables. Challis and Barber [3] show that for log-concave likelihoods p(yn|ηn)  the original
problem Eq. (4) is jointly concave in m and the Cholesky factor of V. This fact  however  does
not result in any reduction in the number of parameters  and they propose to use factorizations of a
restricted form  which negatively affects the approximation accuracy.
[17] and [16] note that the optimal V∗ must be of the form V∗ = [Σ−1 +WT diag(λ)W]−1  which
suggests reparameterizing Eq. (5) in terms of L+N parameters (m  λ)  where λ is the new variable.
However  the problem is not concave in this alternative parameterization [12]. Moreover  as shown
in [12] and [10]  convergence can be exceedingly slow. The coordinate-ascent approach of [12] and
dual variational inference [10] both speed-up convergence  but only for a limited class of models.
A range of different deterministic inference approximations exist as well. The local variational
method is convex for log-concave potentials and can be solved at very large scales [23]  but applies
only to models with super-Gaussian likelihoods. The bound it maximizes is provably less tight than
Eq.
(4) [22  3] making it less accurate. Expectation propagation (EP) [15  21] is more general
and can be more accurate than most other approximations mentioned here. However  it is based
on a saddle-point rather than an optimization problem  and the standard EP algorithm does not al-
ways converge and can be numerically unstable. Among these alternatives  the variational Gaussian
approximation stands out as a compromise between accuracy and good algorithmic properties.

4 Decoupled Variational Gaussian Inference using a Lagrangian

We simplify the form of the objective function by decoupling the KL divergence term from the
terms including fn. In other words  we separate the prior distribution from the likelihoods. We do
so by introducing real-valued auxiliary-variables hn and σn > 0  such that the following constraints
hold: hn = ¯mn and σn = ¯σn. This gives us the following (equivalent) optimization problem over
x := {m  V  h  σ} 

(cid:2)log |V| − Tr(VΣ−1) − (m − µ)T Σ−1(m − µ) + L(cid:3) − N(cid:88)

fn(hn  σn)

(6)

max

x

g(x) := 1
2

N(cid:88)

n(x) := hn − wT

n m = 0 and c2

subject to constraints c1
For log-concave likelihoods  the function g(x) is concave in V  unlike the original function f (see
Eq. (5)) which is concave with respect to Cholesky of V. The difﬁculty now lies with the non-
linear constraints c2
n(x). We will now establish that the new problem gives rise to a convenient
parameterization  but does not affect the maximum.
The signiﬁcance of this reformulation lies in its Lagrangian  shown below.

n Vwn) = 0 for all n.

n(x) := 1

2 (σ2

n=1

n − wT

L(x  α  λ) := g(x) +

αn(hn − wT

n m) + 1

2 λn(σ2

n − wT

n Vwn)

(7)

n=1

Here  αn  λn are Lagrangian multipliers for the constraints c1
n(x) and c2(x). We will now show
that the maximum of f of Eq. (5) can be parameterized in terms of these multipliers  and that this
reparameterization is unique. The following theorem states this result along with three other useful
relationships between the maximum of Eq. (5)  (6)  and (7). Proof is in the supplementary material.
Theorem 4.1. The following holds for maxima of Eq. (5)  (6)  and (7):

1. A stationary point x∗ of Eq. (6) will also be a stationary point of Eq. (5). For every such

stationary point x∗  there exist unique α∗ and λ

∗ such that 

V∗ = [Σ−1 + WT diag(λ

∗

)W]−1  m∗ = µ − ΣWT α∗

(8)

2. The α∗

n and λ∗

n depend on the gradient of function fn and satisfy the following conditions 
(9)

n  (cid:53)σn fn(h∗

(cid:53)hn fn(h∗

n) = α∗

n) = σ∗

n  σ∗

n  σ∗

nλ∗

n

3

where h∗
of f (x) with respect to x at x = x∗.

n m∗ and (σ∗

n = wT

n)2 = wT

n V∗wn for all n and (cid:53)xf (x∗) denotes the gradient

3. When {m∗  V∗} is a local maximizer of Eq. (5)  then the set {m∗  V∗  h
∗

a strict maximizer of Eq. (7).

  σ∗  α∗  λ

∗} is

4. When likelihoods p(yn|ηn) are log-concave  there is only one global maximum of f  and

any {m∗  V∗} obtained by maximizing Eq. (7) will be the global maximizer of Eq. (5).

∗

∗

Part 1 establishes the parameterization of (m∗  V∗) by (α∗  λ
) and its uniqueness  while part
2 shows the conditions that (α∗  λ
) satisfy. This form has also been used in [12] for Gaussian
∗. Part 3 shows that such
Processes where a ﬁxed-point iteration was employed to search for λ
parameterization can be obtained at maxima of the Lagrangian rather than minima or saddle-points.
The ﬁnal part considers the case when f is concave and shows that the global maximum can be
obtained by maximizing the Lagrangian. Note that concavity of the lower bound is required for the
last part only and the other three parts are true irrespective of concavity.
Detailed proof of the theorem is given in the supplementary material.
Note that the conditions of Eq. (9) restrict the values that α∗
n and λ∗
n can take. Their values will be
valid only in the range of the gradients of fn. This is unlike the formulation of [17] which does not
constrain these variables  but is similar to the method of [10]. We will see later that our algorithm
makes the problem infeasible for values outside this range. Ranges of these variables vary depending
on the likelihood p(yn|ηn). However  we show below in Eq. (10) that λ∗
n is always strictly positive
for log-concave likelihoods. The ﬁrst equality is obtained using Eq. (9)  while the second equality
is simply change of variables from σn to σ2
n. The third equality is obtained using Eq. (19) from
[17]. The ﬁnal inequality is obtained since fn is convex for all log-concave likelihoods ((cid:53)xxf (x)
denotes the Hessian of f (x)).

λ∗
n = σ∗

n

−1 (cid:53)σn fn(h∗

n  σ∗

n) = 2 (cid:53)σ2

n

fn(h∗

n  σ∗

n) = (cid:53)2

hnhn

fn(h∗

n  σ∗

n) > 0

(10)

5 Optimization Algorithms for Decoupled Variational Gaussian Inference

Theorem 4.1 suggests that the optimal solution can be obtained by maximizing g(x) or the La-
grangian L. The maximization is difﬁcult for two reasons. First  the constraints c2
n(x) are non-linear
and second the function g(x) may not always be concave. Note that it is not easy to apply the aug-
mented Lagrangian method or ﬁrst-order methods (see Chapter 4 of [1]) because their application
would require storage of V. Instead  we use a method based on linearization of the constraints which
will avoid explicit computation and storage of V. First  we will show that when g(x) is concave 
we can maximize it by minimizing a sequence of convex problems. We will then solve each convex
problem using the dual-variational method of [10].

5.1 Linearly Constrained Lagrangian (LCL) Method

We now derive an algorithm based on the linearly constrained Lagrangian (LCL) method [19]. The
LCL approach involves linearization of the non-linear constraints and is an effective method for
large-scale optimization  e.g. in packages such as MINOS [24]. There are variants of this method
that are globally convergent and robust [4]  but we use the variant described in Chapter 17 of [24].
The ﬁnal algorithm: See Algorithm 1. We start with a α  λ and σ. At every iteration k  we
minimize the following dual:

min
α λ∈S

2 log |Σ−1 + WT diag(λ)W| + 1
− 1
Here  (cid:101)Σ = WΣWT and(cid:101)µ = Wµ. The functions f k∗

f k∗
n (αn  λn) := max

−fn(hn  σn) + αnhn + 1

hn σn>0

where λk

n and σk

n were obtained at the previous iteration.

4

2 αT(cid:101)Σα −(cid:101)µT α +

N(cid:88)

n=1

f k∗
n (αn  λn)

(11)

n are obtained as follows:
2 λk
2 λnσk

n(2σn − σk

n) − 1

n(σn − σk
n)2

(12)

Algorithm 1 Linearly constrained Lagrangian (LCL) method for VG approximation

Initialize α  λ ∈ S and σ (cid:31) 0.
for k = 1  2  3  . . . do

λk ← λ and σk ← σ.
repeat

until convergence

end for

For all n  compute predictive mean ˆm∗
For all n  in parallel  compute (h∗
n  σ∗
Find next (α  λ) ∈ S using gradients gα

n and variances ˆv∗
n) that maximizes Eq. (12).
n = h∗
n = 1

n using linear regression (Eq. (13))
nσn − ˆv∗
n].

n − ˆm∗

n)2 + 2σk

n and gλ

2 [−(σk

The constraint set S is a box constraints on αn and λn such that a global minimum of Eq. (12)
exists. We will show some examples later in this section.
Efﬁcient gradient computation: An advantage of this approach is that the gradient at each iteration
can be computed efﬁciently  especially for large N and L. The gradient computation is decoupled
into two terms. The ﬁrst term can be computed by computing f k∗
n in parallel  while the second
term involves prediction in a linear model. The gradients with respect to αn and λn (derived in the
2 [−(σk
supplementary material) are given as gα
n]  where
n  σ∗
(h∗
ˆv∗
n := wT
ˆm∗
n := wT

n (Σ−1 + WT diag(λ)W)−1wn =(cid:101)Σnn −(cid:101)Σn :((cid:101)Σ + diag(λ)−1)−1(cid:101)Σn :
n (µ − ΣWT α) =(cid:101)µn −(cid:101)Σn :α

n) are maximizers of Eq. (12) and ˆv∗

n are computed as follows:

nwn = wT
n = wT

n − ˆm∗
n and ˆm∗

n V∗
n m∗

n := h∗

n)2 + 2σk

n − ˆv∗

n and gλ

n := 1

nσ∗

(13)

n  σ∗

n and ˆm∗

The quantities (h∗
n) can be computed in parallel over all n. Sometimes  this can be done in closed
form (as we shown in the next section)  otherwise we can compute them by numerically optimizing
over two-dimensional functions. Since these problems are only two-dimensional  a Newton method
can be easily implemented to obtain fast convergence.
The other two terms ˆv∗
n can be interpreted as predictive means and variances of a pseudo
linear model  e.g. compare Eq. (13) with Eq. 2.25 and 2.26 of Rasmussen’s book [18]. Hence
every gradient computation can be expressed as Bayesian prediction in a linear model for which
we can use existing implementation. For example  for binary or multi-class GP classiﬁcation  we
can reuse efﬁcient implementation of GP regression. In general  we can use a Bayesian inference
in a conjuate model to compute the gradient of a non-conjugate model. This way the method also
avoids forming V∗ and work only with its linear projections which can be efﬁciently computed
using vector-matrix-vector products.
The “decoupling” nature of our algorithm should now be clear. The non-linear computations de-
pending on the data  are done in parallel to compute h∗
n. These are completely decoupled
from linear computations for ˆmn and ˆvn. This is summarized in Algorithm (1).
Derivation: To derive the algorithm  we ﬁrst linearize the constraints. Given multiplier λk and a
point xk at the k’th iteration  we linearize the constraints c2

n and σ∗

n(x):

¯c2
nk(x) := c2
= 1
= − 1

n(xk) + (cid:53)c2
n)2 − wT
2 [(σk
n)2 − 2σk

2 [(σk

n(xk)T (x − xk)
n Vkwn + 2σk

nσn + wT

n) − (wT

n Vwn − wT

n Vkwn)]

n(σn − σk
n Vwn]

(14)
(15)
(16)

Since we want the linearized constraint ¯c2
penalize the difference between the two.

nk(x) to be close to the original constraint c2

n(x)  we will

n(x) − ¯c2
c2

nk(x) = 1

2{σ2

n − wT

n Vwn − [−(σk

n)2 + 2σk

nσn − wT

n Vwn]} = 1

2 (σn − σk
n)2

(17)

The key point is that this term is independent of V  allowing us to obtain a closed-form solution for
V∗. This will also be crucial for the extension to non-concave case in the next section.

5

The new k’th subproblem is deﬁned with the linearized constraints and the penalization term:

1

n(σn − σk
n)2
2 λk
n mn = 0   − 1

n)2 − 2σk

2 [(σk

nσn + wT

n Vwn] = 0 

(18)

∀n

This is a concave problem with linear constraints and can be optimized using dual variational infer-
ence [10]. Detailed derivation is given in the supplementary material.
Convergence: When LCL algorithm converges  it has quadratic convergence rates [19]. However 
it may not always converge. Globally convergent methods do exist (e.g. [4]) although we do not
explore them in this paper. Below  we present a simple approach that improves the convergence for
non log-concave likelihoods.
Augmented Lagrangian Methods for non log-concave likelihoods: When the likelihood p(yn|ηn)
are not log-concave  the lower bound can contain local minimum  making the optimization difﬁcult
for function f (m  V). In such scenarios  the algorithm may not converge for all starting values.
The convergence of our approach can be improved for such cases. We simply add an augmented
nk(x)]2 to the linearly constrained Lagrangian deﬁned in Eq. (18)  as shown
Lagrangian term [¯c2
i > 0 and i is the i’th iteration of k’th subproblem:
below [24]. Here  δk

gk(x) := g(x) − N(cid:88)

n=1

s.t. hn − wT

max

x

aug(x) := g(x) − N(cid:88)

gk

n(σn − σk

1

2 λk

n)2 + 1

2 δk

i (σn − σk
n)4

(19)

n=1
subject to the same constraints as Eq. (18).
The sequence δk
i can either be set to a constant or be increased slowly to ensure convergence to a
local maximum. More details on setting this sequence and its affect on the convergence can be found
in Chapter 4.2 of [1]. It is in fact possible to know the value of δk
i such that the algorithm always
converge. This value can be set by examining the primal function - a function with respect to the
deviations in constraints. It turns out that it should be set larger than the largest eigenvalues of the
Hessian of the primal function at 0. A good discussion of this can be found in Chapter 4.2 of [1].
The fact that that the linearized constraint ¯c2
addition of this term then only affects computation of f k∗
changing the computation to optimization of the following function:

nk(x) does not depend on V is very useful here since
n . We modify the algorithm by simply

max

hn σn>0

−fn(hn  σn) + αnhn + 1

2 λnσk

n(2σn − σk

n) − 1

n(σn − σk

2 λk

n)2 − δk
i
2

(σn − σk
n)4

(20)

σk
n 

λn + λk
n

n is ﬁnite.

n = − 1
h∗

2 σ∗2

yn + αn + λk
n

n + log(yn + αn)  S = {αn > −yn  λn > 0 ∀n} (21)

It is clear from this that the augmented Lagrangian term is trying to “convexify” the non-convex
function fn  leading to improved convergence.
Computation of f k∗
n (α λn) These functions are obtained by solving the optimization problem
shown in Eq. (12). In some cases  we can compute these functions in closed form. For exam-
ple  as shown in the supplementary material  we can compute h∗ and σ∗ in closed form for Poisson
likelihood as shown below. We also show the range of αn and λn for which f k∗
σ∗
n =
An expression for Laplace likelihood is also derived in the supplementary material.
When we do not have a closed-form expression for f k∗
n   we can use a 2-D Newton method for opti-
mization. To facilitate convergence  we must warm-start the optimization. When fn is concave  this
usually converges in few iterations  and since we can parallelize over n  a signiﬁcant speed-up can
be obtained. A signiﬁcant engineering effort is required for parallelization and for our experiments
in this paper  we have not done so.
An issue that remains open is the evaluation of the range S for which each f k∗
n is ﬁnite. For now 
we have simply set it to the range of gradients of function fn as shown by Eq. 9 (also see the last
paragraph in that section). It is not clear whether this will always assure convergence for the 2-D
optimization.
Prediction: Given α∗ and λ
regression. See details in Rasmussen’s book [18].

∗  we can compute the predictions by using equations similar to GP

6

6 Results

We demonstrate the advantages of our approach on a binary GP classiﬁcation problem. We model
the binary data using Bernoulli-logit likelihoods. Function fn are computed to a reasonable accuracy
using the piecewise bound [14] with 20 pieces.
We apply this model to a subproblem of the USPS digit data [18]. Here  the task is to classify
between 3’s vs. 5’s. There are a total of 1540 data examples with feature dimensionality of 256.
Since we want to compare the convergence  we will show results for different data sizes by sub-
sampling randomly from these examples.
We set µ = 0 and use a squared-exponential kernel  for which the (i  j)th entry of Σ is deﬁned as:
Σij = −σ2 exp[− 1
2||xi − xj||2/s] where xi is i’th feature. We show results for log(σ) = 4 and
log(s) = −1 which corresponds to a difﬁcult case where VG approximations converge slowly (due
to the ill-conditioning of the Kernel) [18]. Our conclusions hold for other parameter settings as well.
We compare our algorithm with the approach of Opper and Archambeau [17] and Challis and Barber
[3]. We refer to them as ‘Opper’ and ‘Cholesky’  respectively. We call our approach ‘Decoupled’.
For all methods  we use L-BFGS method for optimization (implemented in minFunc by Mark
Schmidt)  since a Newton method would be too expensive for large N. All algorithms were stopped
when the subsequent changes in the lower bound value of Eq. 5 were less than 10−4. All methods
were randomly initialized. Our results are not sensitive to initialization. We compare convergence
in terms of the value of lower bound. The prediction errors show very similar trend  therefore we do
not present them.
The results are summarized in Figure 1. Each plot shows the negative of the lower bound vs time in
seconds for increasing data sizes N = 200  500  1000 and 1500. For Opper and Cholesky  we show
markers for every iteration. For decoupled  we show markers after completion of each subproblem.
We can not see the result of ﬁrst subproblem here  and the ﬁrst visible marker is obtained from the
second subproblem onwards.
We see that as the data size increases  Decoupled converges faster than the other methods  showing
a clear advantage over other methods for large dimensionality.

7 Discussion and Future Work

In this paper  we proposed the decoupled VG inference method for approximate Bayesian inference.
We obtain efﬁcient reparameterization using a Lagrangian to the lower bound. We showed that such
a parameterization is unique  even for non log-concave likelihood functions  and the maximum of
the lower bound can be obtained by maximizing the Lagrangian. For concave likelihood function 
our method recovers the global maximum. We proposed a linearly constrained Lagrangian method
to maximize the Lagrangian. The algorithm has the desired property that it reduces each gradi-
ent computation to a linear model computation  while parallelizing non-linear computations over
data examples. Our proposed algorithm is capable of attaining convergence rates similar to convex
methods.
Unlike methods such as mean-ﬁeld approximation  our method preserves all posterior correlations
and can be useful towards generalizing stochastic variational inference (SVI) methods [5] to non-
conjugate models. Existing SVI methods rely on mean-ﬁeld approximations and are widely applied
for conjugate models. Under our method  we can stochastically include only few constraints to
maximize the Lagrangian. This amounts to a low-rank approximation of the covariance matrix and
can be used to construct an unbiased estimate of the gradient.
We have focused only on latent Gaussian models for simplicity. It is easy to extend our approach
to other non-Gaussian latent models  e.g. sparse Bayesian linear model [21] and Bayesian non-
negative matrix factorization [20]. Similar decoupling method can also be applied to general latent
variable models. Note that a choice of proper posterior distribution is required to get an efﬁcient
parameterization of the posterior.
It is also possible to get sparse posterior covariance approximation using our decoupled formulation.
One possible idea is to use Hinge type of loss to approximate the likelihood terms. Using the
dualization similar to what we have shown here would give us a sparse posterior covariance.

7

Figure 1: Convergence results for a GP classiﬁcation on the USPS-3vs5 data set. Each plot shows
the negative of the lower bound vs time in seconds for data sizes N = 200  500  1000 and 1500.
For Opper and Cholesky  we show markers for every iteration. For decoupled  we show markers
after completion of each subproblem. We can not see the result of ﬁrst subproblem here  and the
ﬁrst visible marker is obtained from the second subproblem. As the data size increases  Decoupled
converges faster  showing a clear advantage over other methods for large dimensionality.

A weakness of our paper is a lack of strong experiments showing that the decoupled method indeed
converge at a fast rate. The implementation of decoupled method requires a good engineering effort
for it to scale to big data. In future  we plan to have an efﬁcient implementation of this method and
demonstrate that this enables variational inference to scale to large data.

Acknowledgments

This work was supported by School of Computer Science and Communication at EPFL. I would
speciﬁcally like to thank Matthias Grossglauser  Rudiger Urbanke  and Jame Larus for providing me
support and funding during this work. I would like to personally thank Volkan Cevher  Quoc Tran-
Dinh  and Matthias Seeger from EPFL for early discussions of this work and Marc Desgroseilliers
from EPFL for checkin some proofs.
I would also like to thank the reviewers for their valuable feedback. The experiments in this paper
are less extensive than what I promised them. Due to time and space constraints  I have not been
able to add all of them. More experiments will appear in an arXiv version of this paper.

8

0.20.30.40.51525535545N = 200Time in secondsNegative Lower Bound 10010113601380140014201440146014801500N = 500Time in secondsNegative Lower Bound 2030405010015020027602780280028202840N = 1000Time in secondsNegative Lower Bound 50100150200300400500423042404250426042704280N = 1500Time in secondsNegative Lower Bound CholeskyOpperDecoupledCholeskyOpperDecoupledCholeskyOpperDecoupledCholeskyOpperDecoupledReferences
[1] Dimitri P Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[2] M. Braun and J. McAuliffe. Variational inference for large-scale models of discrete choice. Journal of

the American Statistical Association  105(489):324–335  2010.

[3] E. Challis and D. Barber. Concave Gaussian variational approximations for inference in large-scale

Bayesian linear models. In International conference on Artiﬁcial Intelligence and Statistics  2011.

[4] Michael P Friedlander and Michael A Saunders. A globally convergent linearly constrained lagrangian

method for nonlinear optimization. SIAM Journal on Optimization  15(3):863–897  2005.

[5] M. Hoffman  D. Blei  C. Wang  and J. Paisley. Stochastic variational inference. Journal of Machine

Learning Research  14:1303–1347  2013.

[6] A. Honkela  T. Raiko  M. Kuusela  M. Tornio  and J. Karhunen. Approximate Riemannian conjugate
gradient learning for ﬁxed-form variational Bayes. Journal of Machine Learning Research  11:3235–
3268  2011.

[7] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression problems and their

extensions. In International conference on Artiﬁcial Intelligence and Statistics  1996.

[8] P. Jyl¨anki  J. Vanhatalo  and A. Vehtari. Robust Gaussian process regression with a Student-t likelihood.

The Journal of Machine Learning Research  999888:3227–3257  2011.

[9] Mohammad Emtiyaz Khan. Variational Learning for Latent Gaussian Models of Discrete Data. PhD

thesis  University of British Columbia  2012.

[10] Mohammad Emtiyaz Khan  Aleksandr Y. Aravkin  Michael P. Friedlander  and Matthias Seeger. Fast
dual variational inference for non-conjugate latent gaussian models. In ICML (3)  volume 28 of JMLR
Proceedings  pages 951–959. JMLR.org  2013.

[11] Mohammad Emtiyaz Khan  Shakir Mohamed  Benjamin Marlin  and Kevin Murphy. A stick breaking
In International conference on

likelihood for categorical data analysis with latent Gaussian models.
Artiﬁcial Intelligence and Statistics  2012.

[12] Mohammad Emtiyaz Khan  Shakir Mohamed  and Kevin Murphy. Fast Bayesian inference for non-

conjugate Gaussian process regression. In Advances in Neural Information Processing Systems  2012.

[13] M. L´azaro-Gredilla and M. Titsias. Variational heteroscedastic Gaussian process regression. In Interna-

tional Conference on Machine Learning  2011.

[14] B. Marlin  M. Khan  and K. Murphy. Piecewise bounds for estimating Bernoulli-logistic latent Gaussian

models. In International Conference on Machine Learning  2011.

[15] T. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Conference

on Uncertainty in Artiﬁcial Intelligence  2001.

[16] H. Nickisch and C.E. Rasmussen. Approximations for binary Gaussian process classiﬁcation. Journal of

Machine Learning Research  9(10)  2008.

[17] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Computation 

21(3):786–792  2009.

[18] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT

Press  2006.

[19] Stephen M Robinson. A quadratically-convergent algorithm for general nonlinear programming prob-

lems. Mathematical programming  3(1):145–156  1972.

[20] Mikkel N Schmidt  Ole Winther  and Lars Kai Hansen. Bayesian non-negative matrix factorization. In

Independent Component Analysis and Signal Separation  pages 540–547. Springer  2009.

[21] M. Seeger. Bayesian Inference and Optimal Design in the Sparse Linear Model. Journal of Machine

Learning Research  9:759–813  2008.

[22] M. Seeger. Sparse linear models: Variational approximate inference and Bayesian experimental design.

Journal of Physics: Conference Series  197(012001)  2009.

[23] M. Seeger and H. Nickisch. Large scale Bayesian inference and experimental design for sparse linear

models. SIAM Journal of Imaging Sciences  4(1):166–199  2011.

[24] SJ Wright and J Nocedal. Numerical optimization  volume 2. Springer New York  1999.

9

,Mohammad Emtiyaz Khan
Mauro Scanagatta
Giorgio Corani
Cassio de Campos
Marco Zaffalon