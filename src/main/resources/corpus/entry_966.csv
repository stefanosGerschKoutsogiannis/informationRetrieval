2009,Clustering sequence sets for motif discovery,Most of existing methods for DNA motif discovery consider only a single set of sequences to find an over-represented motif. In contrast  we consider multiple sets of sequences where we group sets associated with the same motif into a cluster  assuming that each set involves a single motif. Clustering sets of sequences yields clusters of coherent motifs  improving signal-to-noise ratio or enabling us to identify multiple motifs. We present a probabilistic model for DNA motif discovery where we identify multiple motifs through searching for patterns which are shared across multiple sets of sequences. Our model infers cluster-indicating latent variables and learns motifs simultaneously  where these two tasks interact with each other. We show that our model can handle various motif discovery problems  depending on how to construct multiple sets of sequences. Experiments on three different problems for discovering DNA motifs emphasize the useful behavior and confirm the substantial gains over existing methods where only single set of sequences is considered.,Clustering Sequence Sets for Motif Discovery

Jong Kyoung Kim and Seungjin Choi

Department of Computer Science

Pohang University of Science and Technology

San 31 Hyoja-dong  Nam-gu

Pohang 790-784  Korea

fblkimjk seungjing@postech.ac.kr

Abstract

Most of existing methods for DNA motif discovery consider only a single set of
sequences to (cid:2)nd an over-represented motif. In contrast  we consider multiple
sets of sequences where we group sets associated with the same motif into a clus-
ter  assuming that each set involves a single motif. Clustering sets of sequences
yields clusters of coherent motifs  improving signal-to-noise ratio or enabling us
to identify multiple motifs. We present a probabilistic model for DNA motif dis-
covery where we identify multiple motifs through searching for patterns which
are shared across multiple sets of sequences. Our model infers cluster-indicating
latent variables and learns motifs simultaneously  where these two tasks interact
with each other. We show that our model can handle various motif discovery prob-
lems  depending on how to construct multiple sets of sequences. Experiments on
three different problems for discovering DNA motifs emphasize the useful behav-
ior and con(cid:2)rm the substantial gains over existing methods where only a single
set of sequences is considered.

1 Introduction

Discovering how DNA-binding proteins called transcription factors (TFs) regulate gene expression
programs in living cells is fundamental to understanding transcriptional regulatory networks con-
trolling development  cancer  and many human diseases. TFs that bind to speci(cid:2)c cis-regulatory
elements in DNA sequences are essential for mediating this transcriptional control. The (cid:2)rst step
toward deciphering this complex network is to identify functional binding sites of TFs referred to as
motifs.
We address the problem of discovering sequence motifs that are enriched in a given target set of
sequences  compared to a background model (or a set of background sequences). There have been
extensive research works on statistical modeling of this problem (see [1] for review)  and recent
works have focused on improving the motif-(cid:2)nding performance by integrating additional informa-
tion into comparative [2] and discriminative motif discovery [3].
Despite the relative long history and the critical roles of motif discovery in bioinformatics  many
issues are still unsolved and controversial. First  the target set of sequences is assumed to have only
one motif  but this assumption is often incorrect. For example  a recent study examining the binding
speci(cid:2)cities of 104 mouse TFs observed that nearly half of the TFs recognize multiple sequence
motifs [4]. Second  it is unclear how to select the target set on which over-represented motifs are
returned. The target set of sequences is often constructed from genome-wide binding location data
(ChIP-chip or ChIP-seq) or gene expression microarray data. However  there is no clear way to
partition the data into target and background sets in general. Third  a uni(cid:2)ed algorithm which is
applicable to diverse motif discovery problems is solely needed to provide a principled framework
for developing more complex models.

1

S

1

S

m

S

M

M

M

 1ms

 m is

  mm Ls

M

M

z

 
m ij

=

T

[0 1]  (cid:76)(cid:73)(cid:3)(cid:36)(cid:42)(cid:36)(cid:42)(cid:42)(cid:42)(cid:42)(cid:55)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:3)(cid:69)(cid:76)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:76)(cid:87)(cid:72)

 m is

(cid:273)(cid:273)(cid:273)(cid:17)(cid:36)(cid:38)(cid:36)(cid:42)(cid:38)(cid:36)(cid:42)(cid:36)(cid:42)(cid:42)(cid:42)(cid:42)(cid:55)(cid:42)(cid:42)(cid:36)(cid:42)(cid:273)(cid:273)(cid:273)

W
s
 
m ij

=

(

s
 
m ij

=

(cid:36) 

s
  (
m i

j

1)
+

=

(cid:42) 

L
 

s
  (
m i

j W
+ -

1)

=

(cid:55))

Figure 1: Notation illustration.

These considerations motivate us to develop a generative probabilistic framework for learning multi-
ple motifs on multiple sets of sequences. One can view our framework as an extension of the classic
sequence models such as the two-component mixture (TCM) [5] and the zero or one occurrence per
sequence (ZOOPS) [6] models in which sequences are partitioned into two clusters  depending on
whether or not they contain a motif. In this paper  we make use of a (cid:2)nite mixture model to partition
the multiple sequence sets into clusters having distinct sequence motifs  which improves the motif-
(cid:2)nding performance over the classic models by enhancing signal-to-noise ratio of input sequences.
We also show how our algorithm can be applied into three different problems by simply changing
the way of constructing multiple sets from input sequences without any algorithmic modi(cid:2)cations.

2 Problem formulation

We are given M sets of DNA sequences S = fS1; : : : ; SM g to be grouped according to the type of
motif involved with  in which each set is associated with only a single motif but multiple binding
sites are present in each sequence. A set of DNA sequences Sm = fsm;1; : : : ; sm;Lmg is a collection
of strings sm;i of length jsm;ij over the alphabet (cid:6) = fA; C; G; T g. To allow for a variable number
of binding sites per sequence  we represent each sequence sm;i as a set of overlapping subsequences
m;ij = (sm;ij ; sm;i(j+1); : : : ; sm;i(j+W (cid:0)1)) of length W starting at position j 2 Im;i  where sm;ij
sW
denotes the letter at position j and Im;i = f1; : : : ; jsm;ij(cid:0)W +1g  as shown in Fig. 1. We introduce
a latent variable matrix zm;i 2 R2(cid:2)jIm;ij in which the j th column vector zm;ij is a 2-dimensional
binary random vector [zm;ij1; zm;ij2]> such that zm;ij = [0; 1]> if a binding site starts at position
j 2 Im;i  otherwise  zm;ij = [1; 0]>. We also introduce K-dimensional binary random vectors

tm 2 RK ( tm;k 2 f0; 1g and Pk tm;k = 1) for m = 1; : : : ; M  which involve partitioning the

sequence sets S into K disjoint clusters  where sets in the same cluster are associated with the same
common motif.
For a motif model  we use a position-frequency matrix whose entries correspond to probability
distributions (over the alphabet (cid:6)) of each position within a binding site. We denote by (cid:2)k 2 RW (cid:2)4
the kth motif model of length W over (cid:6)  where (cid:2)>
k;w represents row w  each entry is non-negative 
l=1 (cid:2)k;wl = 1 for 8w. The background model (cid:18)0  which describes
frequencies over the alphabet within non-binding sites  is de(cid:2)ned by a P th order Markov chain
(represented by a (P + 1)-dimensional conditional probability table).
Our goal is to construct a probabilistic model for DNA motif discovery where we identify multiple
motifs through searching for patterns which are shared across multiple sets of sequences. Our model
infers cluster-indicating latent variables (to (cid:2)nd a good partition of S) and learns motifs (inferring
binding site-indicating latent variables zm;i) simultaneously  where these two tasks interact with
each other.

(cid:2)k;wl (cid:21) 0 for 8w; l  and P4

2

p

0q

z

 m ij

W
 

m ijs

mt

I

|

 
m i

|

mL

M

Q

k

K

v

b

a

Figure 2: Graphical representation of our mixture model for M sequence sets.

3 Mixture model for motif discovery

We assume that the distribution of S is modeled as a mixture of K components  where it is not
known in advance which mixture component underlies a particular set of sequences. We also as-
sume that the conditional distribution of the subsequence sW
m;ij given tm is modeled as a mixture of
two components  each of which corresponds to the motif and the background models  respectively.
Then  the joint distribution of observed sequence sets S and (unobserved) latent variables Z and T
conditioned on parameters (cid:8) is written as:
Lm

M

p(S; Z; T j(cid:8)) =

p(tmj(cid:8))

p(sW

m;ijjzm;ij ; tm; (cid:8))p(zm;ijj(cid:8));

(1)

Ym

Yi=1 Yj2Im;i

where Z = fzm;ijg and T = ftmg. The graphical model associated with (1) is shown in Fig. 2.

The generative process for subsequences sW
weights v = [v1; : : : ; vK]> (involving set clusters) from the Dirichlet distribution:

m;ij is described as follows. We (cid:2)rst draw mixture

p(vj(cid:11)) /

(cid:11)k
K (cid:0)1
k

;

v

(2)

where (cid:11) = [(cid:11)1; : : : ; (cid:11)K]> are the hyperparameters. Given mixture weights  we choose the cluster-
. The chosen

indicator tm for Sm  according to the multinomial distribution p(tmjv) =QK

kth motif model (cid:2)k is drawn from the product of Dirichlet distributions:

k=1 v

tm;k
k

p((cid:2)kj(cid:12)) =

p((cid:2)k;wj(cid:12)) /

(cid:2)(cid:12)l(cid:0)1
k;wl ;

(3)

W

Yw=1

W

4

Yw=1

Yl=1

where (cid:12) = [(cid:12)1; : : : ; (cid:12)4]> are the hyperparameters. The latent variables zm;ij indicating the starting
positions of binding sites are governed by the prior distribution speci(cid:2)ed by:

K

Yk=1

2

Yr=1

p(zm;ijj(cid:25)) =

(cid:25)zm;ijr

r

;

(4)

where the mixture weights (cid:25) = [(cid:25)1; (cid:25)2]> satisfy (cid:25)1; (cid:25)2 (cid:21) 0 and (cid:25)1 + (cid:25)2 = 1. Finally  the
subsequences sW

m;ij are drawn from the following conditional distribution:

p(sW

m;ijjtm; zm;ij ; f(cid:2)kgK

k=1; (cid:18)0) = p(sW

m;ijj(cid:18)0)zm;ij1

(p(sW

m;ijj(cid:2)k)zm;ij2 )tm;k ;

(5)

K

Yk=1

where

p(sW

m;ijj(cid:18)0) =

W

4

Yw=1

Yl=1

(cid:18)

(cid:14)(l;sm;i(j+w(cid:0)1) )
0l

; p(sW

m;ijj(cid:2)k) =

W

4

Yw=1

Yl=1

(cid:2)

(cid:14)(l;sm;i(j+w(cid:0)1) )
k;wl

;

3

where (cid:14)(l; sm;i(j+w(cid:0)1)) is an indicator function which equals 1 if sm;i(j+w(cid:0)1) = l  and otherwise
0. Here  the background model is speci(cid:2)ed by the 0th-order Markov chain for notational simplicity.
Several assumptions simplify this generative model. First  the width W of the motif model and
the number K of set clusters are assumed to be known and (cid:2)xed. Second  the mixture weights (cid:25)
together with the background model (cid:18)0 are treated as parameters to be estimated. We assume the
hyperparameters (cid:11) and (cid:12) are set to (cid:2)xed and known constants. The full set of parameters and hyper-
parameters will be denoted by (cid:8) = f(cid:11); (cid:12); (cid:25); (cid:18)0g. Extension to double stranded DNA sequences is
obvious and omitted here due to the lack of space.
Our model builds upon the existing TCM model proposed by [5] where the EM algorithm is applied
to learn a motif on a single target set. This model actually generates subsequences instead of se-
quences themselves. An alternative model which explicitly generates sequences has been proposed
based on Gibbs sampling [7  8]. Note that our model is reduced to the TCM model if K  the number
of set clusters  is set to one.
Our model shares some similarities with the recent Bayesian hierarchical model in [9] which also
uses a mixture model to cluster discovered motifs. The main difference is that they focus on cluster-
ing motifs already discovered  and in our formulation  we try to cluster sequence sets and discover
motifs simultaneously.

4 Inference by Gibbs sampling

We (cid:2)nd the con(cid:2)gurations of Z and T by maximizing the posterior distribution over latent variables:

Z (cid:3); T (cid:3) = arg max

p(Z; T jS; (cid:8)):

Z;T

(6)

To this end  we use Gibbs sampling to (cid:2)nd the posterior modes by drawing samples repeatedly from
the posterior distribution over Z and T . We will derive a Gibbs sampler for our generative model
in which the set mixture weights v and motif models f(cid:2)kgK
k=1 are integrated out to improve the
convergence rate and the cost per iteration [8].
The critical quantities needed to implement the Gibbs sampler are the full conditional distributions
for Z and T . We (cid:2)rst derive the relevant full conditional distribution over tm conditioned on the
set cluster assignments of all other sets  Tnm  the latent positions Z  and the observed sets S. By
applying Bayes’ rule  Fig. 2 implies that this distribution factorizes as follows:

p(tm;k = 1jTnm; S; Z; (cid:8)) / p(tm;k = 1jTnm; (cid:11))p(Sm; ZmjT ; Snm; Znm; (cid:8));

(7)

where Znm denotes the entries of Z other than Zm = fzm;igLm
i=1  and Snm is similarly de(cid:2)ned. The
(cid:2)rst term represents the predictive distribution of tm given the other set cluster assignments Tnm 
and is given by marginalizing the set mixture weights v:

p(tm;k = 1jTnm; (cid:11)) = Zv
k = Pn6=m (cid:14)(tn;k; 1). Note that N (cid:0)m

k

where N (cid:0)m
counts the number of sets currently assigned to
the kth set cluster excluding the mth set. The model’s Markov structure implies that the second term
of (7) depends on the current assignments Tnm as follows:

N (cid:0)m
k + (cid:11)k
K
M (cid:0) 1 + (cid:11)k

(8)

p(tm;k = 1jv)p(vjTnm; (cid:11))dv =

p(Sm; Zmjtm;k = 1; Tnm; Snm; Znm; (cid:8))

p(Sm; Zmjtm;k = 1; (cid:2)k; (cid:8))p((cid:2)kjfSn; Znjtn;k = 1; n 6= mg; (cid:8))d(cid:2)k

(9)

Lm

= Z(cid:2)k
= 2
Yi=1 Yj2Im;i
4
" W
Yw=1(cid:26) (cid:0)(Pl(N (cid:0)m

m;ijjzm;ij2 = 0; (cid:18)0)3
5

p(sW

p(zm;ijj(cid:25)) Yj2Im;i;zm;ij2=0
(cid:0)(Pl(Nwl + (cid:12)l)) Ql (cid:0)(Nwl + (cid:12)l)
Ql (cid:0)(N (cid:0)m

wl + (cid:12)l))

wl + (cid:12)l)(cid:27)# ;

4

where Nwl = N (cid:0)m

wl + N m

wl and

N (cid:0)m

wl

= Xtn;k=1;n6=m

Lm

Ln

Xi=1 Xj2In;i;zn;ij2=1

(cid:14)(sn;i(j+w(cid:0)1); l)

N m

wl =

Xi=1 Xj2Im;i;zm;ij2=1

(cid:14)(sm;i(j+w(cid:0)1); l):

counts the number of letter l at position w within currently assigned binding sites
wl denotes the number of letter l at position w within

Note that N (cid:0)m
wl
excluding the ones of the mth set. Similarly  N m
bindings sites of the mth set.
We next derive the full conditional distribution of zm;ij given the remainder of the variables. Inte-
grating over the motif model (cid:2)k  we then have the following factorization:

p(zm;ijjZnm;ij ; S; tm;k = 1; Tnm; (cid:8)) /Z(cid:2)k Ytn;k=1
/ 2
4 Ytn;k=1

Yi=1 Yj2In;i;zn;ij2=0

p(zn;ijj(cid:25))

Yj=1

Yi=1

p(sW

In;i

Ln

Ln

n;ijj(cid:18)0)3
5

p(Zn; Snj(cid:2)k)p((cid:2)kj(cid:12))d(cid:2)k

(cid:0)(Pl(Nwl + (cid:12)l))# ;(10)
" W
Yw=1 Ql (cid:0)(Nwl + (cid:12)l)

where Znm;ij denotes the entries of Z other than zm;ij. For the purpose of sampling  the ratio of the
posterior distribution of zm;ij is given by:

p(zm;ij2 = 1jZnm;ij ; S; T ; (cid:8))
p(zm;ij2 = 0jZnm;ij ; S; T ; (cid:8))

=

(cid:25)2
(cid:25)1p(sW
m;ijj(cid:18)0)

W

Yw=1P4

l=1(N (cid:0)m;ij

wl

+ (cid:12))(cid:14)(sm;i(j+w(cid:0)1); l)

;

l=1(N (cid:0)m;ij

wl

+ (cid:12))

P4

wl

= Ptn;k=1PLn

where N (cid:0)m;ij
notes the number of letter l at position w within currently assigned binding sites other than zm;ij.
Combining (7) with (10) is suf(cid:2)cient to de(cid:2)ne the Gibbs sampler for our (cid:2)nite mixture model. To
provide a convergence measure  we derive the following objective function based on the log of the
posterior distribution:

i=1Pj 02In;i;j 06=j;zn;ij02=1 (cid:14)(sn;i(j 0+w(cid:0)1); l). Note that N (cid:0)m;ij

de-

wl

log p(Z; T jS; (cid:8)) / log p(Z; T ; Sj(cid:8))

M

Lm

Im;i

M

Lm

log p(zm;ijj(cid:25)) +

/

+

K

Xj=1
Xi=1
Xm=1
Xw=1(Xl
Xk=1

W

log (cid:0)(N k

where Nk =Pm (cid:14)(tm;k; 1) and N k

5 Results

Xm=1

Xi=1 Xj2Imi;zm;ij2=0

log p(sW

m;ijj(cid:18)0)

wl + (cid:12)l) (cid:0) log (cid:0)(Xl
wl =Ptm;k =1PLm

(N k

wl + (cid:12)l))) +

log (cid:0)(Nk +

(cid:11)k
K

); (11)

K

Xk=1

i=1Pj2Im;i;zm;ij2=1 (cid:14)(sm;i(j+w(cid:0)1); l).

We evaluated our motif-(cid:2)nding algorithm on the three different tasks: (1) (cid:2)ltering out undesirable
noisy sequences  (2) incorporating evolutionary conservation information  and (3) clustering DNA
sequences based on the learned motifs (Fig. 3). In the all experiments  we (cid:2)xed the hyper-parameters
so that (cid:11)k = 1 and (cid:12)l = 0:5.

5.1 Data sets and evaluation criteria

We (cid:2)rst examined the yeast ChIP-chip data published by [10] to investigate the effect of (cid:2)ltering out
noisy sequences from input sequences on identifying true binding sites. We compiled 156 sequence-
sets by choosing TFs having consensus motifs in the literature [11]. For each sequence-set  we
de(cid:2)ned its sequences to be probe sequences that are bound with P -value (cid:20) 0:001.

5

(a) Filtering out noisy se-
quences

(b) Evolutionary conservation

(c) Motif-based clustering

Figure 3: Three different ways of constructing multiple sequence sets. Black rectangles: sequence
sets  Blue bars: sequences  Red dashed rectangles: set clusters  Red and green rectangles: motifs.

To apply our algorithm into the comparative motif discovery problem  we compiled orthologous
sequences for each probe sequence of the yeast ChIP-chip data based on the multiple alignments
of seven species of Saccharomyces (S. cerevisiae  S. paradoxus  S. mikatae  S. kudriavzevii  S.
bayanus  S. castelli  and S. kluyveri) [12]. In the experiments using the ChIP-chip data  the mo-
tif width was set to 8 and a (cid:2)fth-order Markov chain estimated from the whole yeast intergenic
sequences was used to describe the background model. We (cid:2)xed the mixture weights (cid:25) so that
(cid:25)2 = 0:001.
We next constructed the ChIP-seq data for human neuron-restrictive silence factor (NRSF) to deter-
mine whether our algorithm can be applied to partition DNA sequences into biologically meaningful
clusters [13]. The data consist of 200 sequence segments of length 100 from all peak sites with the
top 10% binding intensity ((cid:21) 500 ChIP-seq reads)  where most sequences have canonical NRSF-
binding sites. We also added 13 sequence segments extracted from peak sites ((cid:21) 300 reads) known
to have noncanonical NRSF-binding sites  resulting in 213 sequences. In the experiment using the
ChIP-seq data  the motif width was set to 30 and a zero-order Markov chain estimated from the 213
sequence segments was used to describe the background model. We (cid:2)xed the mixture weights (cid:25) so
that (cid:25)2 = 0:005.
In the experiments using the yeast ChIP-chip data  we used the inter-motif distance to measure the
quality of discovered motifs [10]. Speci(cid:2)cally  an algorithm will be called successful on a sequence
set only if at least one of the position-frequency matrices constructed from the identi(cid:2)ed binding
sites is at a distance less than 0.25 from the literature consensus [14].

5.2 Filtering out noisy sequences

Selecting target sequences from the ChIP-chip measurements is largely left to users and this choice
is often unclear. Our strategy of constructing sequence-sets based on the binding P -value cutoff
would be exposed to danger of including many irrelevant sequences. In practice  the inclusion of
noisy sequences in the target set is a serious obstacle in the success of motif discovery. One possible
solution is to cluster input sequences into two smaller sets of target and noisy sequences based
on sequence similarity  and predict motifs from the clustered target sequences with the improved
signal-to-noise ratio. This two-step approach has been applied to only protein sequences because
DNA sequences do not share much similarity for effective clustering [15].
One alternative approach is to seek a better sequence representation based on motifs. To this end  we
constructed multiple sets by treating each sequence of a particular yeast ChIP-chip sequence-set as
one set (Fig. 3(a)). We examined the ability of our algorithm to (cid:2)nd a correct motif with two different
numbers of clusters: K = 1 (without (cid:2)ltering) and K = 2 (clustering into two subsets of true and
noisy sequences). We ran each experiment (cid:2)ve times with different initializations and reported
means with (cid:6)1 standard error. Figure 4 shows that the (cid:2)ltering approach (K = 2) outperforms the
baseline method (K = 1) in general  with the increasing value of the P -value cutoff. Note that the
ZOOPS or TCM models can also handle noisy sequences by modeling them with only a background
model [5  6]. But we allow noisy sequences to have a decoy motif (randomly occurring sequence

6

Figure 4: Effect of (cid:2)ltering out noisy sequences on the number of successfully identi(cid:2)ed motifs on
the yeast ChIP-chip data. K = 1: without (cid:2)ltering  K = 2: clustering into two subsets.

patterns or repeating elements) which is modeled with a motif model. Because our model can be
reduced to these classic models by setting K = 1  we concluded that noisy sequences were better
represented by our clustering approach than the previous ones using the background model (Fig. 4).
Two additional lines of evidence indicated that our (cid:2)ltering approach enhances the signal-to-noise
ratio of the target set. First  we compared the results of our (cid:2)ltering approach with that of other
baseline methods (AlignAce [16]  MEME [6]  MDScan [17]  and PRIORITY-U [11]) on the same
yeast ChIP-chip data. For AlignAce  MEME and MDScan  we used the results reported by [14];
for PRIORITY-U  we used two different results reported by [14  11] according to different sampling
strategy. We expected that our model would perform better than these four methods because they try
to remove noisy sequences based on the classic models. By comparing the results of Fig. 4 and Table
1  we see that our algorithm still performs better. Second  we also compared our model with DRIM
speci(cid:2)cally designed to dynamically select the target set from the list of sorted sequences according
to the binding P -values of ChIP-chip measurements. For DRIM  we used the result reported by [18].
Because DRIM does not produce any motifs when they are not statistically enriched at the top of
the ranked list  we counted the number of successfully identi(cid:2)ed motifs on the sequence-sets where
DRIM generated signi(cid:2)cant motifs. Our method (number of successes is 16) was slightly better than
DRIM (number of successes is 15).

5.3 Detecting evolutionary conserved motifs

Comparative approach using evolutionary conservation information has been widely used to improve
the performance of motif-(cid:2)nding algorithms because functional TF binding sites are likely to be
conserved in orthologous sequences. To incorporate conservation information into our clustering
framework  orthologous sequences of each sequence of a particular yeast ChIP-chip sequence-set
were considered as one set and the number of clusters was set to 2 (Fig. 3(b)). The constructed sets
contain at most 7 sequences because we only used seven species of Saccharomyces. We used the
single result with the highest objective function value of (11) among (cid:2)ve runs and compared it with
the results of (cid:2)ve conservation-based motif (cid:2)nding algorithms on the same data set: MEME c [10] 
PhyloCon [19]  PhyMe [20]  PhyloGibbs [21]  PRIORITY-C [11]. For the (cid:2)ve methods  we used
the results reported by [11]. We did not compare with discriminative methods which are known
to perform better at this data set because our model does not use negative sequences. Table 1
presents the motif-(cid:2)nding performance in terms of the number of correctly identi(cid:2)ed motifs for
each algorithm. We see that our algorithm greatly outperforms the four alignment-based methods
which rely on multiple or pair-wise alignments of orthologous sequences to search for motifs that are
conserved across the aligned blocks of orthologous sequences. In our opinion  it is because diverged
regions other than the short conserved binding sites may prevent a correct alignment. Moreover  our
algorithm performs somewhat better than PRIORITY-C  which is a recent alignment-free method.
We believe that it is because the signal-to-noise ratio of the input target set is enhanced by clustering.

5.4 Clustering DNA sequences based on motifs

To examine the ability of our algorithm to partition DNA sequences into biologically meaningful
clusters  we applied our algorithm to the NRSF ChIP-seq data which are assumed to have two

7

Table 1: Comparison of the number of successfully identi(cid:2)ed motifs on the yeast ChIP-chip data
for different methods. NC: Non-conservation  EC: Evolutionary conservation  A: Alignment-based 
AF: Alignment-free  C: Clustering.
Description
Method
NC
AlignAce
NC
MEME
MDScan
NC
PRIORITY-U NC
MEME c
PhyloCon
PhyME
PhyloGibbs
PRIORITY-C
This work

EC + A
EC + A
EC + A
EC + A
EC + AF
EC + AF + C

# of successes

46-58

16
35
54

49
19
21
54
69
75

(a) Canonical NRSF motif

(b) Noncanonical NRSF motif

Figure 5: Sequence logo of discovered NRSF motifs.

different NRSF motifs (Fig. 3(c)). In this experiment  we have already known the number of clusters
(K = 2). We ran our algorithm (cid:2)ve times with different initializations and reported the one with
the highest objective function value. Position-frequency matrices of two clusters are shown in Fig.
5. The two motifs correspond directly to the previously known motifs (canonical and non-canonical
NRSF motifs). However  other motif-(cid:2)nding algorithms such as MEME could not return the non-
canonical motif enriched in a very small set of sequences. These observations suggest that our
motif-driven clustering approach is effective at inferring latent clusters of DNA sequences and can
be used to (cid:2)nd unexpected novel motifs.

6 Conclusions

In this paper  we have presented a generative probabilistic framework for DNA motif discovery us-
ing multiple sets of sequences where we cluster DNA sequences and learn motifs interactively. We
have presented a (cid:2)nite mixture model with two different types of latent variables  in which one is
associated with cluster-indicators and the other corresponds to motifs (transcription factor binding
sites). These two types of latent variables are inferred alternatively using multiple sets of sequences.
Our empirical results show that the proposed method can be applied to various motif discovery prob-
lems  depending on how to construct the multiple sets. In the future  we will explore several other
extensions. For example  it would be interesting to examine the possibility of learning the num-
ber of clusters from data based on Dirichlet process mixture models  or to extend our probabilistic
framework for discriminative motif discovery.

Acknowledgments: We thank Raluca Gord(cid:136)an for providing the literature consensus motifs and the script to
compute the inter-motif distance. This work was supported by National Core Research Center for Systems
Bio-Dynamics funded by Korea NRF (Project No. 2009-0091509) and WCU Program (Project No. R31-2008-
000-10100-0). JKK was supported by a Microsoft Research Asia fellowship.

8

References
[1] G. D. Stormo. DNA binding sites: representation and discovery. Bioinformatics  16:16(cid:150)23  2000.
[2] W. W. Wasserman and A. Sandelin. Applied bioinformatics for the identi(cid:2)cation of regulatory elements.

Nature Review Genetics  5:276(cid:150)287  2004.

[3] E. Segal  Y. Barash  I. Simon  N. Friedman  and D. Koller. From promoter sequence to expression: a
probabilistic framework. In Proceedings of the International Conference on Research in Computational
Molecular Biology  pages 263(cid:150)272  2002.

[4] G. Badis  M. F. Berger  A. A. Philippakis  S. Talukder  A. R. Gehrke  S. A. Jaeger  E. T. Chan  G. Met-
zler  A. Vedenko  X. Chen  H. Kuznetsov  C. F. Wang  D. Coburn  D. E. Newburger  Q. Morris  T. R.
Hughes  and M. L. Bulyk. Diversity and complexity in DNA recognition by transcription factors. Sci-
ence  324:1720(cid:150)1723  2009.

[5] T. L. Bailey and C. Elkan. Fitting a mixture model by expectation maximization to discover motifs in
biopolymers. In Proceedings of the International Conference Intelligent Systems for Molecular Biology 
1994.

[6] T. L. Bailey and C. Elkan. The value of prior knowledge in discovering motifs with MEME. In Proceed-

ings of the International Conference Intelligent Systems for Molecular Biology  1995.

[7] C. E. Lawrence  S. F. Altschul  M. S. Boguski  J. S. Liu  A. F. Neuwald  and J. C. Wootton. Detecting
subtle sequence signals: a Gibbs sampling strategy for multiple alignment. Science  262:208(cid:150)214  1993.
[8] J. S. Liu  A. F. Neuwald  and C. E. Lawrence. Bayesian models for multiple local sequence alignment

and Gibbs sampling strategies. Journal of the American Statistical Association  90:1156(cid:150)1170  1995.

[9] S. T. Jensen and J. S. Liu. Bayesian clustering of transcription factor binding motifs. Journal of the

American Statistical Association  103:188(cid:150)200  2008.

[10] C. T. Harbison  D. B. Gordon  T. I. Lee  N. J. Rinaldi  K. D. Macisaac  T. W. Danford  N. M. Hannett 
J. B. Tagne  D. B. Reynolds  J. Yoo  E. G. Jennings  J. Zeitlinger  D. K. Pokholok  M. Kellis  P. A. Rolfe 
K. T. Takusagawa  E. S. Lander  D. K. Gifford  E. Fraenkel  and R. A. Young. Transcriptional regulatory
code of a eukaryotic genome. Nature  431:99(cid:150)104  2004.

[11] R. Gordan  L. Narlikar  and A. J. Hartemink. A fast  alignment-free  conservation-based method for
transcription factor binding site discovery. In Proceedings of the International Conference on Research
in Computational Molecular Biology  pages 98(cid:150)111  2008.

[12] A. Siepel  G. Bejerano  J. S. Pedersen  A. S. Hinrichs  M. Hou  K. Rosenbloom  H. Clawson  J. Spieth 
L. W. Hillier  S. Richards  G. M. Weinstock  R. K. Wilson  R. A. Gibbs  W. J. Kent  W. Miller  and
D. Haussler. Evolutionarily conserved elements in vertebrate  insect  worm  and yeast genomes. Genome
Research  15:1034(cid:150)1050  2005.

[13] D. S. Johnson  A. Mortazavi  R. M. Myers  and B. Wold. Genome-wide mapping of in vivo protein-DNA

interactions. Science  316:1497(cid:150)1502  2007.

[14] L. Narlikar  R. Gordan  and A. J. Hartemink. Nucleosome occupancy information improves de novo
motif discovery. In Proceedings of the International Conference on Research in Computational Molecular
Biology  pages 107(cid:150)121  2007.

[15] S. Kim  Z. Wang  and M. Dalkilic.

clustering and iterative pattern sampling. Proteins  66:671(cid:150)681  2007.

igibbs:

improving gibbs motif sampler for proteins by sequence

[16] F. P. Roth  J. D. Hughes  P. W. Estep  and G. M. Church. Finding DNA regulatory motifs within unaligned
noncoding sequences clustered by whole-genome mRNA quantitation. Nature Biotechnology  16:939(cid:150)
945  1998.

[17] X. S. Liu  D. L. Brutlag  and J. S. Liu. An algorithm for (cid:2)nding protein-DNA binding sites with appli-
cations to chromatin-immunoprecipitation microarray experiments. Nature Biotechnology  20:835(cid:150)839 
2002.

[18] E. Eden  D. Lipson  S. Yogev  and Z. Yakhini. Discovering motifs in ranked lists of DNA sequences.

PLoS Computational Biology  3:e39  2007.

[19] T. Wang and G. D. Stormo. Combining phylogenetic data with co-regulated genes to identify regulatory

motifs. Bioinformatics  19:2369(cid:150)2380  2003.

[20] S. Sinha  M. Blanchette  and M. Tompa. PhyME: a probabilistic algorithm for (cid:2)nding motifs in sets of

orthologous sequences. BMC Bioinformatics  5:170  2004.

[21] R. Siddharthan  E. D. Siggia  and E. van Nimwegen. PhyloGibbs: a gibbs sampling motif (cid:2)nder that

incorporates phylogeny. PLoS Computational Biology  1:e67  2005.

9

,Stéphanie ALLASSONNIERE
Juliette Chevallier
Stephane Oudard