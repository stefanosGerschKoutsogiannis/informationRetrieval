2013,Linear Convergence with Condition Number Independent Access of Full Gradients,For smooth and strongly convex optimization  the optimal iteration complexity of the gradient-based algorithm is $O(\sqrt{\kappa}\log 1/\epsilon)$  where $\kappa$ is the conditional number. In the case that the optimization problem is ill-conditioned  we need to evaluate a larger number of full gradients  which could be computationally expensive. In this paper  we propose to reduce the number of full gradient required by allowing the algorithm to access the stochastic gradients of the objective function. To this end  we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients.  A distinctive step in EMGD is the mixed gradient descent  where we use an combination of the gradient and the stochastic gradient to update the intermediate solutions. By performing a fixed number of mixed gradient descents  we are able to improve the sub-optimality of the solution by a constant factor  and thus achieve a linear convergence rate. Theoretical analysis shows that EMGD is able to find an $\epsilon$-optimal solution by computing $O(\log 1/\epsilon)$ full gradients and $O(\kappa^2\log 1/\epsilon)$ stochastic gradients.,Linear Convergence with Condition Number

Independent Access of Full Gradients

Lijun Zhang Mehrdad Mahdavi Rong Jin
Department of Computer Science and Engineering

Michigan State University  East Lansing  MI 48824  USA

{zhanglij mahdavim rongjin}@msu.edu

Abstract

For smooth and strongly convex optimizations  the optimal iteration complexity of
the gradient-based algorithm is O(√κ log 1/ǫ)  where κ is the condition number.
In the case that the optimization problem is ill-conditioned  we need to evaluate a
large number of full gradients  which could be computationally expensive. In this
paper  we propose to remove the dependence on the condition number by allowing
the algorithm to access stochastic gradients of the objective function. To this end 
we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that
is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed
gradient descent  where we use a combination of the full and stochastic gradients
to update the intermediate solution. Theoretical analysis shows that EMGD is
able to ﬁnd an ǫ-optimal solution by computing O(log 1/ǫ) full gradients and
O(κ2 log 1/ǫ) stochastic gradients.

1

Introduction

Convex optimization has become a tool central to many areas of engineering and applied sciences 
such as signal processing [20] and machine learning [24]. The problem of convex optimization is
typically given as

F (w) 

min
w∈W

where W is a convex domain  and F (·) is a convex function. In most cases  the optimization algo-
rithm for solving the above problem is an iterative process  and the convergence rate is characterized
by the iteration complexity  i.e.  the number of iterations needed to ﬁnd an ǫ-optimal solution [3 17].
In this study  we focus on ﬁrst order methods  where we only have the access to the (stochastic)
gradient of the objective function. For most convex optimization problems  the iteration complexity
of an optimization algorithm depends on the following two factors.

1. The analytical properties of the objective function. For example  is F (·) smooth or strongly

convex?

2. The information that can be elicited about the objective function. For example  do we have

access to the full gradient or the stochastic gradient of F (·)?

The optimal iteration complexities for some popular combinations of the above two factors are sum-
marized in Table 1 and elaborated in the related work section. We observe that when the objective
function is smooth (and strongly convex)  the convergence rate for full gradients is much faster than
that for stochastic gradients. On the other hand  the evaluation of a stochastic gradient is usually
signiﬁcantly more efﬁcient than that of a full gradient. Thus  replacing full gradients with stochastic
gradients essentially trades the number of iterations with a low computational cost per iteration.

1

Table 1: The optimal iteration complexity of convex optimization. L and λ are the moduli of
smoothness and strong convexity  respectively. κ = L/λ is the condition number.

Full Gradient

Stochastic Gradient

Lipschitz continuous

ǫ2(cid:1)
O(cid:0) 1
O(cid:0) 1
ǫ2(cid:1)

Smooth

O(cid:16) L√ǫ(cid:17)
O(cid:0) 1
ǫ2(cid:1)

Smooth & Strongly Convex

ǫ(cid:1)
O(cid:0)√κ log 1
λǫ(cid:1)
O(cid:0) 1

In this work  we consider the case when the objective function is both smooth and strongly convex 
where the optimal iteration complexity is O(√κ log 1
ǫ ) if the optimization method is ﬁrst order
and has access to the full gradients [17]. For the optimization problems that are ill-conditioned  the
condition number κ can be very large  leading to many evaluations of full gradients  an operation that
is computationally expensive for large data sets. To reduce the computational cost  we are interested
in the possibility of making the number of full gradients required independent from κ. Although the
O(√κ log 1
ǫ ) rate is in general not improvable for any ﬁrst order method  we bypass this difﬁculty by
allowing the algorithm to have access to both full and stochastic gradients. Our objective is to reduce
the iteration complexity from O(√κ log 1
ǫ ) by replacing most of the evaluations of full
gradients with the evaluations of stochastic gradients. Under the assumption that stochastic gradients
can be computed efﬁciently  this tradeoff could lead to a signiﬁcant improvement in computational
efﬁciency.

ǫ ) to O(log 1

To this end  we developed a novel optimization algorithm named Epoch Mixed Gradient Descent
(EMGD). It divides the optimization process into a sequence of epochs  an idea that is borrowed
from the epoch gradient descent [9]. At each epoch  the proposed algorithm performs mixed gra-
dient descent by evaluating one full gradient and O(κ2) stochastic gradients. It achieves a constant
reduction in the optimization error for every epoch  leading to a linear convergence rate. Our analy-
sis shows that EMGD is able to ﬁnd an ǫ-optimal solution by computing O(log 1
ǫ ) full gradients and
O(κ2 log 1
ǫ ) stochastic gradients. In other words  with the help of stochastic gradients  the number
of full gradients required is reduced from O(√κ log 1
ǫ )  independent from the condition
number.

ǫ ) to O(log 1

2 Related Work

During the last three decades  there have been signiﬁcant advances in convex optimization [3 15 17].
In this section  we provide a brief review of the ﬁrst order optimization methods.

We ﬁrst discuss deterministic optimization  where the gradient of the objective function is available.
For the general convex and Lipschitz continuous optimization problem  the iteration complexity
of gradient (subgradient) descent is O( 1
ǫ2 )  which is optimal up to constant factors [15]. When
the objective function is convex and smooth  the optimal optimization scheme is the accelerated
gradient descent developed by Nesterov  whose iteration complexity is O( L√ǫ ) [16  18]. With slight
modiﬁcations  the accelerated gradient descent algorithm can also be applied to optimize the smooth
and strongly convex objective function  whose iteration complexity is O(√κ log 1
ǫ ) and is in general
not improvable [17  19]. The objective of our work is to reduce the number of accesses to the full
gradients by exploiting the availability of stochastic gradients.

In stochastic optimization  we have access to the stochastic gradient  which is an unbiased estimate
of the full gradient [14]. Similar to the case in deterministic optimization  if the objective function
is convex and Lipschitz continuous  stochastic gradient (subgradient) descent is the optimal algo-
rithm and the iteration complexity is also O( 1
ǫ2 ) [14  15]. When the objective function is λ-strongly
convex  the algorithms proposed in very recent works [9  10  21  26] achieve the optimal O( 1
λǫ ) it-
eration complexity [1]. Since the convergence rate of stochastic optimization is dominated by the
randomness in the gradient [6  11]  smoothness usually does not lead to a faster convergence rate for
stochastic optimization. A variant of stochastic optimization is the “semi-stochastic” approximation 
which interleave stochastic gradient descent and full gradient descent [12]. In the strongly convex
case  if the stochastic gradients are taken at a decreasing rate  the convergence rate can be improved
to approach O( 1

λ√ǫ ) [13].

2

From the above discussion  we observe that the iteration complexity in stochastic optimization is
polynomial in 1
ǫ   making it difﬁcult to ﬁnd high-precision solutions. However  when the objective
function is strongly convex and can be written as a sum of a ﬁnite number of functions  i.e. 

F (w) =

1
n

nXi=1

fi(w) 

(1)

where each fi(·) is smooth  the iteration complexity of some speciﬁc algorithms may exhibit a loga-
rithmic dependence on 1
ǫ   i.e.  a linear convergence rate. The two very recent works are the stochastic
average gradient (SAG) [22]  whose iteration complexity is O(n log 1
ǫ )  provided n ≥ 8κ  and the
stochastic dual coordinate ascent (SDCA) [23]  whose iteration complexity is O((n + κ) log 1
ǫ ).1
Under approximate conditions  the incremental gradient method [2] and the hybrid method [5] can
also minimize the function in (1) with a linear convergence rate. But those algorithms usually treat
one pass of all fi’s (or the subset of fi’s) as one iteration  and thus have high computational cost per
iteration.

3 Epoch Mixed Gradient Descent

3.1 Preliminaries

In this paper  we assume there exist two oracles.

1. The ﬁrst one is a gradient oracle Og  which for a given input point w ∈ W returns the

gradient ∇F (w)  that is 

Og(w) = ∇F (w).

2. The second one is a function oracle Of   each call of which returns a random function f (·) 

such that

and f (·) is L-smooth  that is 

F (w) = Ef [f (w)]  ∀w ∈ W 

k∇f (w) − ∇f (w′)k ≤ Lkw − w′k  ∀w  w′ ∈ W.

(2)

(3)

(4)

Although we do not deﬁne a stochastic gradient oracle directly  the function oracle Of allows us to
evaluate the stochastic gradient of F (·) at any point w ∈ W.
Notice that the assumption about the function oracle Of implies that the objective function F (·) is
also L-smooth. Since ∇F (w) = Ef∇f (w)  by Jensen’s inequality  we have

k∇F (w) − ∇F (w′)k ≤ Efk∇f (w) − ∇f (w′)k
Besides  we further assume F (·) is λ-strongly convex  that is 

(2)

≤ Lkw − w′k  ∀w  w′ ∈ W.

k∇F (w) − ∇F (w′)k ≥ λkw − w′k  ∀w  w′ ∈ W.

From (3) and (4)  it is obvious that L ≥ λ. The condition number κ is deﬁned as the ratio between
them. i.e.  κ = L/λ ≥ 1.

3.2 The Algorithm

The detailed steps of the proposed Epoch Mixed Gradient Descent (EMGD) are shown in Algo-
rithm 1  where we use the superscript for the index of epoches  and the subscript for the index of
iterations at each epoch. We denote by B(x; r) the ℓ2 ball of radius r around the point x.
Similar to the epoch gradient descent (EGD) [9]  we divided the optimization process into a sequence
of epochs (step 3 to step 10). While the number of accesses to the gradient oracle in EGD increases
exponentially over the epoches  the number of accesses to the two oracles in EMGD is ﬁxed.

1In order to apply SDCA  we need to assume each function fi is λ-strongly convex  so that we can rewrite

fi(w) as gi(w) + λ

2 kwk2  where gi(w) = fi(w) − λ

2 kwk2 is convex.

3

Algorithm 1 Epoch Mixed Gradient Descent (EMGD)
Input: step size η  the initial domain size ∆1  the number of iterations T per epoch  and the number
of epoches m
1: Initialize ¯w1 = 0
2: for k = 1  . . .   m do
3:
4:
5:
6:
7:

Set wk
Call the gradient oracle Og to obtain ∇F ( ¯wk)
for t = 1  . . .   T do

Call the function oracle Of to obtain a random function f k

Compute the mixed gradient as

1 = ¯wk

t (·)

˜gk
t = ∇F ( ¯wk) + ∇f k

t (wk

t ) − ∇f k

t ( ¯wk)

8:

Update the solution by

wk

t+1 =

argmin
w∈W∩B( ¯w

k;∆k)

ηhw − wk

t   ˜gk

t i +

1
2kw − wk

t k2

end for
Set ¯wk+1 = 1

9:
10:
11: end for
Return ¯wm+1

t and ∆k+1 = ∆k/√2

t=1 wk

T +1PT +1

1 to be the average solution ¯wk obtained
At the beginning of each epoch  we initialize the solution wk
from the last epoch  and then call the gradient oracle Og to obtain ∇F ( ¯wk). At each iteration t
of epoch k  we call the function oracle Of to obtain a random function f k
t (·) and deﬁne the mixed
gradient at the current solution wk

t as

˜gk
t = ∇F ( ¯wk) + ∇f k

t (wk

t ) − ∇f k

t ( ¯wk) 

which involves both the full gradient and the stochastic gradient. The mixed gradient can be divided
t ( ¯wk). Due
t (·) and the shrinkage of the domain size  the norm of the stochastic

into two parts: the deterministic part ∇F ( ¯wk) and the stochastic part ∇f k
to the smoothness property of f k
part is well bounded  which is the reason why our algorithm can achieve linear convergence.

t ) − ∇f k

t (wk

Based on the mixed gradient  we update wk
t by a gradient mapping over a shrinking domain (i.e. 
W ∩ B( ¯wk; ∆k)) in step 8. Since the updating is similar to the standard gradient descent except for
the domain constraint  we refer to it as mixed gradient descent for short. At the end of the iteration
for epoch k  we compute the average value of T + 1 solutions  instead of T solutions  and update
the domain size by reducing a factor of √2.

3.3 The Convergence Rate

The following theorem shows the convergence rate of the proposed algorithm.

Theorem 1. Assume

δ ≤ e−1/2  T ≥

1152L2

λ2

ln

1
δ

  and ∆1 ≥ maxr 2

λ

(F (0) − F (w∗)).

(5)

Set η = 1/[L√T ]. Let ¯wm+1 be the solution returned by Algorithm 1 after m epoches that has m
accesses to oracle Og and mT accesses to oracle Of . Then  with a probability at least 1 − mδ  we
have

F ( ¯wm+1) − F (w∗) ≤

λ[∆1]2
2m+1   and k ¯wm+1 − w∗k2 ≤

[∆1]2
2m .

Theorem 1 immediately implies that EMGD is able to achieve an ǫ optimization error by computing
O(log 1

ǫ ) full gradients and O(κ2 log 1

ǫ ) stochastic gradients.

4

Table 2: The computational complexity for minimizing 1

i=1 fi(w)

Nesterov’s algorithm [17]

O(cid:0)√κn log 1
ǫ(cid:1)

EMGD

O(cid:0)(n + κ2) log 1
ǫ(cid:1)

3.4 Comparisons

SAG (n ≥ 8κ) [22]

nPn
O(cid:0)n log 1
ǫ(cid:1)

SDCA [23]

O(cid:0)(n + κ) log 1
ǫ(cid:1)

Compared to the optimization algorithms that only rely on full gradients [17]  the number of full
gradients needed in EMGD is O(log 1
ǫ ). Compared to the optimization
algorithms that only rely on stochastic gradients [9 10 21]  EMGD is more efﬁcient since it achieves
a linear convergence rate.

ǫ ) instead of O(√κ log 1

The proposed EMGD algorithm can also be applied to the special optimization problem considered
in [22  23]  where F (w) = 1
i=1 fi(w). To make quantitative comparisons  let’s assume the
full gradient is n times more expensive to compute than the stochastic gradient. Table 2 lists the
computational complexities of the algorithms that enjoy linear convergence. As can be seen  the
computational complexity of EMGD is lower than Nesterov’s algorithm [17] as long as the condition

nPn

number κ ≤ n2/3  the complexity of SAG [22] is lower than Nesterov’s algorithm if κ ≤ n/8  and
the complexity of SDCA [23] is lower than Nesterov’s algorithm if κ ≤ n2.2 The complexity of
EMGD is on the same order as SAG and SDCA when κ ≤ n1/2  but higher in other cases. Thus  in

terms of computational cost  EMGD may not be the best one  but it has advantages in other aspects.

1. Unlike SAG and SDCA that only work for unconstrained optimization problem  the pro-
posed algorithm works for both constrained and unconstrained optimization problems  pro-
vided that the constrained problem in Step 8 can be solved efﬁciently.

2. Unlike the SAG and SDCA that require an Ω(n) storage space  the proposed algorithm

only requires the storage space of Ω(d)  where d is the dimension of w.

3. The only step in Algorithm 1 that has dependence on n is step 4 for computing the gradient
∇F ( ¯wk). By utilizing distributed computing  the running time of this step can be reduced
to O(n/k)  where k is the number of computers  and the convergence rate remains the
same. For SAG and SDCA   it is unclear whether they can reduce the running time without
affecting the convergence rate.

4. The linear convergence of SAG and SDCA only holds in expectation  whereas the linear

convergence of EMGD holds with a high probability  which is much stronger.

4 The Analysis

In the proof  we frequently use the following property of strongly convex functions [9].

Lemma 1. Let f (x) be a λ-strongly convex function over the domain X   and x∗ =
argminx∈X

f (x). Then  for any x ∈ X   we have
f (x) − f (x∗) ≥

λ
2kx − x∗k2.

(6)

4.1 The Main Idea

The Proof of Theorem 1 is based on induction. From the assumption about ∆1 in (5)  we have

(5)

λ[∆1]2

2

≤

≤ [∆1]2 
2In machine learning  we usually face a regularized optimization problem minw∈W

  and k ¯w1 − w∗k2

F ( ¯w1) − F (w∗)

i w) +
τ
2 kwk2  where ℓ(·; ·) is some loss function. When the norm of the data is bounded  the smoothness parameter
L can be treated as a constant. The strong convexity parameter λ is lower bounded by τ . Thus  as long as
τ > Ω(n−2/3)  which is a reasonable scenario [25]  we have κ < O(n2/3)  indicating our proposed EMGD
can be applied.

i=1 ℓ(yi; x

1

n Pn

⊤

(5)  (6)

5

which means Theorem 1 is true for m = 0. Suppose Theorem 1 is true for m = k. That is  with a
probability at least 1 − kδ  we have

F ( ¯wk+1) − F (w∗) ≤

λ[∆1]2
2k+1   and k ¯wk+1 − w∗k2 ≤

[∆1]2
2k .

Our goal is to show that after running the k+1-th epoch  with a probability at least 1 − (k + 1)δ  we
have

F ( ¯wk+2) − F (w∗) ≤

4.2 The Details

λ[∆1]2
2k+2   and k ¯wk+2 − w∗k2 ≤

[∆1]2
2k+1 .

For the simplicity of presentation  we drop the index k for epoch. Let ¯w be the solution obtained
from the epoch k. Given the condition

F ( ¯w) − F (w∗) ≤

λ
2

∆2  and k ¯w − w∗k2 ≤ ∆2 

(7)

satisﬁes

we will show that after running the T iterations in one epoch  the new solution  denoted by bw 

∆2 

(8)

F (bw) − F (w∗) ≤

∆2  and kbw − w∗k2 ≤

λ
4

1
2

with a probability at least 1 − δ.
Deﬁne

The objective function can be rewritten as

g = ∇F ( ¯w)  bF (w) = F (w) − hw  gi  and gt(w) = ft(w) − hw ∇ft( ¯w)i.

And the mixed gradient can be rewritten as

F (w) = hw  gi + bF (w).
˜gk = g + ∇gt(wt).

(9)

(10)

(11)

(12)

(13)

(14)

Then  the updating rule given in Algorithm 1 becomes

wt+1 = argmin

w∈W∩B( ¯w ∆)

ηhw − wt  g + ∇gt(wt)i +

1
2kw − wtk2.

Notice that the objective function in (11) is 1-strongly convex. Using the fact that w∗ ∈ W ∩
B( ¯w; ∆) and Lemma 1 (with x∗ = wt+1 and x = w∗)  we have

1
2kwt+1 − wtk2
ηhwt+1 − wt  g + ∇gt(wt)i +
1
1
2kw∗ − wtk2 −
2kw∗ − wt+1k2.
≤ηhw∗ − wt  g + ∇gt(wt)i +
For each iteration t in the current epoch  we have

and

F (wt) − F (w∗)
≤h∇F (wt)  wt − w∗i −

(4)

λ
2kwt − w∗k2

(10)

= hg + ∇gt(wt)  wt − w∗i +D∇bF (wt) − ∇gt(wt)  wt − w∗E −
hg + ∇gt(wt)  wt − w∗i
≤ hg + ∇gt(wt)  wt − wt+1i + kwt − w∗k2
− kwt+1 − w∗k2
≤hg  wt − wt+1i + kwt − w∗k2

2η

2η

2η

(12)

+ max

w (cid:18)h∇gt(wt)  wt − wi − kwt − wk2

2η

=hg  wt − wt+1i + kwt − w∗k2

2η

2η

− kwt+1 − w∗k2
(cid:19)
− kwt+1 − w∗k2

2η

+

η
2k∇gt(wt)k2.

λ
2kwt − w∗k2 

− kwt − wt+1k2

2η

6

Combining (13) and (14)  we have

F (wt) − F (w∗)
≤kwt − w∗k2
+ hg  wt − wt+1i +

2η

2η
η

− kwt+1 − w∗k2

λ
2kwt − w∗k2

−

2k∇gt(wt)k2 +D∇bF (wt) − ∇gt(wt)  wt − w∗E .

By adding the inequalities of all iterations  we have

(F (wt) − F (w∗))

2η

TXt=1
≤k ¯w − w∗k2
TXt=1
|

η
2

+

kwt − w∗k2 + hg  ¯w − wT +1i

2η

TXt=1

−

λ
2

− kwT +1 − w∗k2
TXt=1
h∇bF (wt) − ∇gt(wt)  wt − w∗i
|
}

k∇gt(wt)k2
}
{z

{z

 BT

 AT

+

.

Since F (·) is L-smooth  we have

F (wT +1) − F ( ¯w) ≤ h∇F ( ¯w)  wT +1 − ¯wi +

which implies

L
2 k ¯w − wT +1k2 

hg  ¯w − wT +1i ≤ F ( ¯w) − F (wT +1) +
L
2

≤ F (w∗) − F (wT +1) +

∆2 +

λ
2

(7)

L
2

∆2

∆2 ≤ F (w∗) − F (wT +1) + L∆2.

(15)

(16)

From (15) and (16)  we have

(F (wt) − F (w∗)) ≤ ∆2(cid:18) 1

2η

+ L(cid:19) +

η
2

T +1Xt=1

AT + BT .

(17)

Next  we consider how to bound AT and BT . The upper bound of AT is given by

TXt=1

AT =

k∇gt(wt)k2 =

k∇ft(wt) − ∇ft( ¯w)k2

(2)

≤ L2

kwt − ¯wk2 ≤ T L2∆2.

(18)

TXt=1

TXt=1

To bound BT   we need the Hoeffding-Azuma inequality stated below [4].
Lemma 2. Let V1  V2  . . . be a martingale difference sequence with respect to some sequence
X1  X2  . . . such that Vi ∈ [Ai  Ai + ci] for some random variable Ai  measurable with respect

i=1 Vi  then for any t > 0 

to X1  . . .   Xi−1 and a positive constant ci. If Sn =Pn
Pr[Sn > t] ≤ exp(cid:18)−
Pn

Deﬁne

2t2
i=1 c2

i(cid:19) .

Vt = h∇bF (wt) − ∇gt(wt)  wt − w∗i  t = 1  . . .   T.

Recall the deﬁnition of bF (·) and gt(·) in (9). Based on our assumption about the function oracle

Of   it is straightforward to check that V1  . . . is a martingale difference with respect to g1  . . .. The
value of Vt can be bounded by

|Vt|

(cid:13)(cid:13)(cid:13)∇bF (wt) − ∇gt(wt)(cid:13)(cid:13)(cid:13)kwt − w∗k

2∆ (k∇F (wt) − ∇F ( ¯w)k + k∇ft(wt) − ∇ft( ¯w)k)
4L∆kwt − ¯wk ≤ 4L∆2.

≤
≤
≤

(2)  (3)

7

Following Lemma 2  with a probability at least 1 − δ  we have
1
δ

BT ≤ 4L∆2r2T ln

.

(19)

By adding the inequalities in (17)  (18) and (19) together  with a probability at least 1 − δ  we have

By choosing η = 1/[L√T ]  we have

ηT L2

+ L +

(F (wt) − F (w∗)) ≤ ∆2 1

T +1Xt=1
(F (wt) − F (w∗)) ≤ L∆2 √T + 1 + 4r2T ln

2η

2

T +1Xt=1

+ 4Lr2T ln

1

δ! .

1

δ! (5)
≤ 6L∆2r2T ln

1
δ

 

(20)

where in the second inequality we use the condition δ ≤ e−1/2 in (5). By Jensen’s inequality  we

have

F (bw) − F (w∗) ≤
kbw − w∗k2

and therefore

Thus  when

≤ ∆2 6Lp2 ln 1/δ

√T + 1

 

1

T + 1

(6)

≤

2
λ

(20)

(F (wt) − F (w∗))

T +1Xt=1
F (bw) − F (w∗) ≤ ∆2 12Lp2 ln 1/δ

λ√T + 1

1152L2

.

T ≥

λ2

ln

1
δ

 

with a probability at least 1 − δ  we have

F (bw) − F (w∗) ≤

5 Conclusion and Future Work

λ
4

∆2  and kbw − w∗k2 ≤

1
2

∆2.

ǫ ) to O(log 1

In this paper  we consider how to reduce the number of full gradients needed for smooth and strongly
convex optimization problems. Under the assumption that both the gradient and the stochastic gra-
dient are available  a novel algorithm named Epoch Mixed Gradient Descent (EMGD) is proposed.
Theoretical analysis shows that with the help of stochastic gradients  we are able to reduce the num-
ber of gradients needed from O(√κ log 1
ǫ ). In the case that the objective function is in
the form of (1)  i.e.  a sum of n smooth functions  EMGD has lower computational cost than the full
gradient method [17]  if the condition number κ ≤ n2/3.
In practice  a drawback of EMGD is that it requires the condition number κ is known beforehand.
We will interstage how to ﬁnd a good estimation of κ in future. When the objective function is
a sum of some special functions  such as the square loss (i.e.  (yi − x⊤i w)2)  we can estimate

the condition number by sampling. In particular  the Hessian matrix estimated from a subset of
functions  combined with the concentration inequalities for matrix [7]  can be used to bound the
eigenvalues of the true Hessian matrix and consequentially κ. Furthermore  if there exists a strongly
convex regularizer in the objective function  which happens in many machine learning problems [8] 
the knowledge of the regularizer itself allows us to ﬁnd an upper bound of κ.

Acknowledgments

This work is partially supported by ONR Award N000141210431 and NSF (IIS-1251031).

8

References

[1] A. Agarwal  P. L. Bartlett  P. Ravikumar  and M. J. Wainwright.

Information-theoretic lower bounds
on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory 
58(5):3235–3249  2012.

[2] D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM Journal

on Optimization  7(4):913–926  1997.

[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University Press  2006.

[5] M. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting. SIAM Journal

on Scientiﬁc Computing  34(3):A1380–A1405  2012.

[6] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic
composite optimization i: a generic algorithmic framework. SIAM Journal on Optimization  22(4):1469–
1492  2012.

[7] A. Gittens and J. A. Tropp. Tail bounds for all eigenvalues of a sum of random matrices. ArXiv e-prints 

arXiv:1104.4513  2011.

[8] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer Series in

Statistics. Springer New York  2009.

[9] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic
strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory  pages
421–436  2011.

[10] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex func-

tions. Technical report  2010.

[11] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming  133:365–

397  2012.

[12] K. Marti. On solutions of stochastic programming problems by descent procedures with stochastic and

deterministic directions. Methods of Operations Research  33:281–293  1979.

[13] K. Marti and E. Fuchs. Rates of convergence of semi-stochastic approximation procedures for solving

stochastic optimization problems. Optimization  17(2):243–265  1986.

[14] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[15] A. Nemirovski and D. B. Yudin. Problem complexity and method efﬁciency in optimization. John Wiley

& Sons Ltd  1983.

[16] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence

O(1/k2). Doklady AN SSSR (translated as Soviet. Math. Docl.)  269:543–547  1983.

[17] Y. Nesterov. Introductory lectures on convex optimization: a basic course  volume 87 of Applied opti-

mization. Kluwer Academic Publishers  2004.

[18] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming  103(1):127–

152  2005.

[19] Y. Nesterov. Gradient methods for minimizing composite objective function. Core discussion papers 

2007.

[20] D. P. Palomar and Y. C. Eldar  editors. Convex Optimization in Signal Processing and Communications.

2010  Cambridge University Press.

[21] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex stochastic
optimization. In Proceedings of the 29th International Conference on Machine Learning  pages 449–456 
2012.

[22] N. L. Roux  M. Schmidt  and F. Bach. A stochastic gradient method with an exponential convergence
rate for ﬁnite training sets. In Advances in Neural Information Processing Systems 25  pages 2672–2680 
2012.

[23] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss mini-

mization. Journal of Machine Learning Research  14:567–599  2013.

[24] S. Sra  S. Nowozin  and S. J. Wright  editors. Optimization for Machine Learning. The MIT Press  2011.

[25] Q. Wu and D.-X. Zhou. Svm soft margin classiﬁers: Linear programming versus quadratic programming.

Neural Computation  17(5):1160–1187  2005.

[26] L. Zhang  T. Yang  R. Jin  and X. He. O(log T ) projections for stochastic optimization of smooth and
strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning
(ICML)  pages 621–629  2013.

9

,Lijun Zhang
Mehrdad Mahdavi
Rong Jin
Takashi Takenouchi
Takafumi Kanamori