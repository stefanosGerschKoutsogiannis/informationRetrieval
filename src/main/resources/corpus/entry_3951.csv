2019,On the Calibration of Multiclass Classification  with Rejection,We investigate the problem of multiclass classification with rejection  where a classifier can choose not to make a prediction to avoid critical misclassification. First  we consider an approach based on simultaneous training of a classifier and a rejector  which achieves the state-of-the-art performance in the binary case. We analyze this approach for the multiclass case and derive a general condition for calibration to the Bayes-optimal solution  which suggests that calibration is hard to achieve by general loss functions unlike the binary case. Next  we consider another traditional approach based on confidence scores  in which the existing work focuses on a specific class of losses. We propose rejection criteria for more general losses for this approach and guarantee calibration to the Bayes-optimal solution. Finally  we conduct experiments to validate the relevance of our theoretical findings.,On the Calibration of Multiclass Classiﬁcation

with Rejection

Chenri Ni1 Nontawat Charoenphakdee1 2
1 The University of Tokyo  Japan

Junya Honda1 2 Masashi Sugiyama2 1
2 RIKEN Center for Advanced Intelligence Project  Japan

{nichenri  nontawat}@ms.k.u-tokyo.ac.jp

{jhonda  sugi}@k.u-tokyo.ac.jp

Abstract

We investigate the problem of multiclass classiﬁcation with rejection  where a
classiﬁer can choose not to make a prediction to avoid critical misclassiﬁcation.
First  we consider an approach based on simultaneous training of a classiﬁer and a
rejector  which achieves the state-of-the-art performance in the binary case. We
analyze this approach for the multiclass case and derive a general condition for
calibration to the Bayes-optimal solution  which suggests that calibration is hard to
achieve by general loss functions unlike the binary case. Next  we consider another
traditional approach based on conﬁdence scores  in which the existing work focuses
on a speciﬁc class of losses. We propose rejection criteria for more general losses
for this approach and guarantee calibration to the Bayes-optimal solution. Finally 
we conduct experiments to validate the relevance of our theoretical ﬁndings.

1

Introduction

In real-world classiﬁcation tasks  e.g.  medical diagnosis  autonomous driving  and product inspection 
misclassiﬁcation can be costly and even life-threatening. Classiﬁcation with rejection is a framework
aiming to prevent critical misclassiﬁcation by providing an option not to make a prediction at the
expense of the pre-deﬁned rejection cost [6  7]. If the rejection cost is less than the misclassiﬁcation
cost  there is an incentive to reject an instance. In practice  once the reject option is selected  one may
gather more information about the instance or ask experts to give the correct label.
Much research on the theoretical perspective of classiﬁcation with rejection has been devoted to the
binary classiﬁcation scenario [16  1  14  29  8  9]. However  rather less attention has been paid to
the multiclass scenario  which is undoubtedly important for real-world applications and is a more
general framework. To the best of our knowledge  although there exist many methods that rely on
heuristics [11  28  23]  only the work by Ramaswamy et al. [20] provides the theoretical guarantee
for their method. Nevertheless  the work by Ramaswamy et al. [20] only focuses on speciﬁc types of
non-differentiable losses and their method requires re-training of the classiﬁer when the rejection
cost changes.
The key concept to validate the soundness of the method for classiﬁcation with rejection lies in the
notion of calibration  i.e.  inﬁnite-sample consistency [29  8]. Calibration suggests that the minimizer
of a surrogate risk behaves identically to the Bayes-optimal solution almost surely. The existing
methods with calibration guarantees can be divided into two categories  which we detail in the
following.
The ﬁrst category is called the conﬁdence-based approach. The main idea is to use the real-valued
output of the classiﬁer as a conﬁdence score [1  14  26]. Whether to reject the input is then determined
from the classiﬁer’s output and a threshold depending on the rejection cost and the choice of the
surrogate loss function.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The second category is what we call the classiﬁer-rejector approach. Unlike the conﬁdence-based
approach  this approach separates the role of the classiﬁer and the rejector  and trains both functions
simultaneously [8  9]. This problem formulation enables more ﬂexible modeling for the rejector 
which can be more robust to model-misspeciﬁcation. This is a state-of-the-art method in binary
classiﬁcation  and has been further discussed in online learning setting [10]  structured output learning
setting [13]  and also in some real-world applications such as liver disease diagnosis [15].
The goal of this paper is to provide a better understanding of multiclass classiﬁcation with rejection.
We ﬁrst investigate the classiﬁer-rejector approach and derive a calibration condition of this approach
in the multiclass case. Our condition recovers the known result by Cortes et al. [9] in the binary
case as a special case. However  when there are more than two classes  we argue that the condition
is hard to be satisﬁed. We next analyze the conﬁdence-based approach and prove the calibration
results for various classes of smooth losses  which guarantees the use of well-known losses such as
the logistic loss  the squared loss  the exponential loss and the cross-entropy loss. Our experiments
support the above ﬁndings  that is  the failure of the classiﬁer-rejector approach and the success of
the conﬁdence-based approach with smooth loss functions  particularly the cross-entropy loss.

2 Preliminaries

In this section  we formulate the problem of classiﬁcation with rejection and review related work.

2.1 Problem setting
Let X ⊆ Rd be a d-dimensional input space and Y = {1  . . .   K} be an output space representing
i=1 drawn independently from an
K classes. Suppose we are given n training samples {(xi  yi)}n
unknown probability distribution over X × Y with density p(x  y). In classiﬁcation with rejection 
we will learn a pair (r  f ) consisting of a rejector r and a classiﬁer f. The rejector r : X → R rejects
a point x ∈ X if r(x) ≤ 0  and accepts it otherwise. The classiﬁer f : X → Y is assumed to take
the following form:

f (x) = argmax

y∈Y

gy(x) 

where gy : X → R is a score function for multiclass classiﬁcation. By a slight abuse of notation  we
identify the classiﬁer f (x) with g(x)  where g(x) = [g1(x)  . . .   gK(x)](cid:62) and (cid:62) denotes the trans-
pose. Given a loss function L(r  f ; x  y)  we deﬁne its risk R by R(r  f ) = Ep(x y)[L(r  f ; x  y)] 
where Ep(x y)[·] denotes the expectation over the distribution p(x  y). We also deﬁne the pointwise
risk W of the loss L at x by

W(cid:0)r(x)  f (x); η(x)(cid:1) =

(cid:88)

y∈Y

ηy(x)L

(cid:0)r  f ; x  y(cid:1) 

minimizing W(cid:0)r(x)  f (x); η(x)(cid:1) over(cid:0)r(x)  f (x)(cid:1) for all x ∈ X . Thus  it is sufﬁcient to only
where η(x) = [η1(x)  . . .   ηK(x)](cid:62) for ηy(x) = p(y|x) denotes the class probability vector.
Note that minimizing R(r  f ) with respect to (r  f ) over all measurable functions is equivalent to
write  for example  W (r  f ; η) instead of W(cid:0)r(x)  f (x); η(x)(cid:1) for the pointwise risk. We will also

consider the pointwise risk to minimize R(r  f ) [22  25]. For brevity  we omit the notation of x and

drop the notation of r when classiﬁcation without rejection is considered and write  for example 
L(f ; x  y)  R(f ) and W (f ; η).
In multiclass classiﬁcation with rejection  our goal is to minimize the 0-1-c risk deﬁned as

R0-1-c(r  f ) = E

[L0-1-c(r  f ; x  y)] 

p(x y)

where the 0-1-c loss L0-1-c is given by

L0-1-c(r  f ; x  y) = 1[f (x)(cid:54)=y]1[r(x)>0]

+ c1[r(x)≤0]

misclassiﬁcation loss

rejection loss

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(1)

(2)

.

Here  c ∈ [0  1] denotes the rejection cost  and 1[·] denotes the indicator function.

2

It is well known that the Bayes-optimal classiﬁer and rejector [20]  i.e.  the classiﬁer and the rejector
that minimize (1)  are given by

r∗(x) = max

y∈Y ηy(x) − (1 − c).

f∗(x) = argmax

ηy(x) 

y∈Y

In this paper  we assume c < 1/2 since data points with low conﬁdence are accepted otherwise.

2.2 Calibration

In classiﬁcation without rejection  the classiﬁcation risk  i.e.  the expected risk with respect to the
0-1 loss L0-1(f ; x  y) = 1[f (x)(cid:54)=y]  is the standard performance metric. It is known that minimizing
the 0-1 risk is computationally infeasible [4  12]. Therefore  an important question is what kind of
surrogate loss can be used instead of the 0-1 loss [30  24  19]. Intuitively  a surrogate loss should be
optimization-friendly and its minimization should lead to minimization of the 0-1 risk. The notion of
calibration is deﬁned for loss functions as the minimum requirement to assure that the risk-minimizing
classiﬁer becomes the Bayes-optimal classiﬁer (see Zhang [30] for the formal deﬁnition).
In classiﬁcation with rejection  our goal is to minimize the 0-1-c risk. Similarly to the 0-1 risk  the
0-1-c risk is also difﬁcult to directly minimize [1  20]. For the purpose of theoretical analysis  it is
more convenient to directly deﬁne calibration for classiﬁers and rejectors based on whether they are
Bayes-optimal. Thus  we propose to deﬁne the notions of calibration as follows.
Deﬁnition 1 (Calibration of a classiﬁer-rejector pair). We say that (r  f ) : X → R × Y is calibrated
if R0-1-c(r  f ) = R0-1-c(r∗  f∗).
In this paper  we also consider the notions of calibration separately for classiﬁers and rejectors  which
enables better understanding of where the difﬁculty of classiﬁcation with rejection comes from.
Deﬁnition 2 (Rejection calibration of a rejector). We say that r : X → R is rejection-calibrated if
sign[r(x)] = sign[r∗(x)] for all x ∈ X such that r∗(x) (cid:54)= 0.
Deﬁnition 3 (Classiﬁcation calibration of a classiﬁer). We say that f : X → Y is classiﬁcation-
calibrated if f (x) = f∗(x) holds almost everywhere on X .
As we can see from these deﬁnitions and the form of loss function (2)  if r is rejection-calibrated
and f is classiﬁcation-calibrated  then (r  f ) is calibrated. Furthermore  rejection calibration of r is
necessary for calibration of (r  f )  while classiﬁcation calibration of f is not as exempliﬁed in [20].

2.3 Related work

Here  we review some related work for both the conﬁdence-based and classiﬁer-rejector approaches.
Note that we follow the conventional notation where the output domain is Y = {+1 −1} and the
score function f : X → R is regarded as a classiﬁer when discussing binary classiﬁcation.
2.3.1 Conﬁdence-based approach

In the conﬁdence-based approach  we ﬁrst train a classiﬁer based on some surrogate of the 0-1 loss 
where we regard the real-valued output of the classiﬁer as some conﬁdence score. We then construct
a rejector based on the output and a pre-speciﬁed threshold θ  which takes the form

(3)
in the binary case. Bartlett and Wegkamp [1] proposed a loss called the modiﬁed hinge loss and
designed an SVM-like algorithm. Later  Yuan and Wegkamp [29] considered a smooth margin

r(x) = |f (x)| − θ

loss φ(cid:0)yf (x)(cid:1).

Here the smoothness of the loss is quite important in the construction of rejectors  since the threshold θ
is sometimes not uniquely determined if a non-smooth loss is used. In Bartlett and Wegkamp [1]  a
calibration guarantee for the non-smooth loss is shown for a range of θ  but its empirical performance
is heavily affected by the choice of the threshold. In addition  the loss function also contains a
parameter that has to be determined by the rejection cost c  which means that we need to re-train the
classiﬁer once we change the value of c. On the other hand for smooth losses  the value of c does not
affect the parameter of a smooth loss  but only the threshold θ. This suggests that we do not need to
re-train a classiﬁer when the rejection cost c changes [29].

3

Ramaswamy et al. [20] extended the method of Bartlett and Wegkamp [1] to multiclass classiﬁcation 
and designed non-smooth losses with excess risk bounds. However  their method has the drawbacks
of non-unique θ and the dependence of the loss on c  which comes from the use of non-smooth losses.

2.3.2 Classiﬁer-rejector approach

Cortes et al. [8  9] pointed out that it is too restrictive to require the rejector r to be of form (3) when
the true classiﬁer is out of the considered hypothesis set. Based on this observation  they proposed to
separate the roles of the classiﬁer and the rejector  and directly minimize an upper bound of the 0-1-c
risk with respect to (r  f) in the training phase. Plus bound (PB) loss LPB was proposed as an upper
bound of the 0-1-c loss in Cortes et al. [9]:
(4)
where φ and ψ are convex upper bounds of 1[z≤0]. Cortes et al. [9] derived the calibration result
for the exponential loss φ(z) = ψ(z) = exp(−z) with appropriately chosen parameters α  β > 0.
However  to the best of our knowledge  this approach is currently available only for the binary case 
and an extension to the multiclass case is highly nontrivial as we will see later.

LPB(r  f ; x  y) = φ(cid:0)α[yf (x) − r(x)](cid:1) + cψ(cid:0)βr(x)(cid:1) 

3 An analysis of the classiﬁer-rejector approach

In this section  we provide a general result on multiclass classiﬁcation with rejection using the
classiﬁer-rejector approach. In the following  we discuss the achievability of rejection calibration of
r  which is a necessary condition for calibration of (r  f ).
Given a loss L(r  f ; x  y)  we denote by (r†
η) = argminr∈R  g∈RK W (r  f ; η) the minimizer of
the corresponding pointwise risk W over the real space. First we derive the following theorem  which
is the main result of this section.
Theorem 4 (Necessary and sufﬁcient condition for rejection calibration). Assume that L is a convex
function of class C 1 with respect to r  and also assume ∂2W (r f†
> 0. Let (r†  f†) be the
minimizer of the surrogate risk R over all measurable functions. Then  r† is rejection-calibrated if
and only if

(cid:12)(cid:12)(cid:12)(cid:12)r=0

η  f†

η;η)

∂r2

sup

η: maxy ηy≥1−c

∂W (r  f†
∂r

η; η)

≤ 0 ≤

inf

η: maxy ηy≤1−c

∂W (r  f†
∂r

η; η)

.

(5)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)r=0

The proof of this theorem is given in Appendix B.1. The following corollary is a weaker version of
this theorem but gives more insight into the strength of the requirement for rejection calibration.
Corollary 5 (Necessary condition for rejection calibration). Under the same assumption as Theo-
rem 4  r† is rejection-calibrated only if

sup

η: maxy ηy=1−c

∂W (r  f†
∂r

η; η)

= 0 

inf

η: maxy ηy=1−c

∂W (r  f†
∂r

η; η)

= 0.

(6)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)r=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)r=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)r=0

(cid:12)(cid:12)(cid:12)r=0

This corollary is straightforward from the relation
h(η) ≤

η: maxy ηy≤1−c

η: maxy ηy=1−c

h(η) ≤

inf

inf

sup

η: maxy ηy=1−c

h(η) ≤

sup

η: maxy ηy≥1−c

h(η)

∂r

η;η)

for any function h(η). The conditions in (6) require that the supremum and the inﬁmum of the
objective function ∂W (r f†
coincide under the same constraint. Therefore  the objective
function is required to depend only on maxy ηy  but not on the class probabilities of other classes.
Whereas maxy ηy uniquely determines the other probability as 1− maxy ηy in the binary case  it still
allows a degree of freedom in the multiclass case  which results in the situation where two conditions
in (6) do not necessarily hold simultaneously.
The failure of the classiﬁer-rejector approach is intuitively explained as follows. The Bayes-optimal
rejector r∗ must be determined only from maxy ηy. Nevertheless  the classiﬁer-rejector approach

4

(cid:114)

(cid:114) 1 − c

c

ignores this requirement and tries to directly construct a rejector r  which does not satisfy this
requirement in general. This contrasts to the rejector in (10) obtained by the conﬁdence-based
approach  where the requirement is encoded by the inverse link function and the max operator.
Remark 1. An error of the rejector can be classiﬁed into False Reject (FR) and False Accept (FA) 
which correspond to the outcomes when the rejector mistakenly rejects (resp. accepts) the data that
should be accepted (resp. rejected). We can see from close inspection of the proof of Theorem 4 that
the ﬁrst inequality of (5) is the condition for the FR rate to be zero  while the second inequality is the
condition for the FA rate to be zero.

To understand the above difference between the binary and multiclass cases more precisely  let
us consider the following example so that the conditions in (6) are explicitly written. Deﬁne two
surrogate losses given by

(cid:88)
(cid:88)

y(cid:48)(cid:54)=y

y(cid:48)(cid:54)=y

(cid:16)
(cid:16)

α(cid:0)gy(x) − gy(cid:48)(x)(cid:1)(cid:17)
α(cid:0)gy(x) − gy(cid:48)(x) − r(x)(cid:1)(cid:17)

ψ(−αr(x)) + cψ(cid:0)βr(x)(cid:1) 
+ cψ(cid:0)βr(x)(cid:1) 

φ

φ

LMPC(r  f ; x  y) =

LAPC(r  f ; x  y) =

(7)

(8)

which we call the multiplicative pairwise comparison (MPC) loss and the additive pairwise compari-
son (APC) loss  respectively. Here  φ and ψ are convex losses that bound 1[z≤0] from above  and
α and β are positive constants that control the performance of the rejector. Note that the pairwise
comparison loss is often used as a multiclass extension of a binary loss [27]. Also note that the
APC loss reduces to the PB loss [9] in (4) when K = 2. Here the MPC loss and the APC loss
are natural ones at least for the purpose of classiﬁcation in the sense that the classiﬁers induced by
them are classiﬁcation-calibrated (see Appendix B.3 for the proof). Nevertheless  when φ and ψ are
exponential losses  (6) gives the following conditions:

β
α

= (K − 2) + 2

(K − 1)

1 − c
c

 

β
α

= 2

 

(9)

which recover the result proved by Cortes et al. [9] when K = 2 (see Appendix B.4 for details). Here
the RHSs of (9) for K > 2 are not identical and therefore we cannot ﬁnd any α and β satisfying the
above equations  even though we get a classiﬁcation-calibrated classiﬁer. This implies the failure in
rejection calibration. Not only when φ and ψ are exponential losses  we can also prove the failure of
the classiﬁer-rejector approach when φ and ψ are logistic losses using the same proof technique (see
Appendix B.4).
Note that  strictly speaking  it remains an open question whether it is possible to ﬁnd a calibrated
surrogate loss in the classiﬁer-rejector approach. In this paper  our result emphasizes that calibration in
the multiclass scenario is signiﬁcantly more difﬁcult. Intuitively  a necessary condition in Corollary 5
is relatively easy to satisfy for K = 2 but it is not the case when K > 2  as illustrated in our examples.

4 An analysis of the conﬁdence-based approach

This section focuses on the extension of the conﬁdence-based approach to the multiclass case using
smooth losses. When we need some conﬁdence score in the multiclass case  it is convenient to
consider a class of loss functions called strictly proper composite losses [22] deﬁned as follows.
Deﬁnition 6 (Strictly proper composite loss [22]). A loss L is strictly proper composite with link
function Ψ : [0  1]K → RK if the pointwise risk W of L satisﬁes argming W (g; η) = Ψ(η) =
[Ψ1(η)  . . .   ΨK(η)](cid:62).
With this class of losses  the threshold θ used in the rejector derived in Yuan and Wegkamp [29] is
expressed as Ψ1
link function Ψ sometimes does not have a closed form whereas the inverse link function Ψ−1 often
does in multiclass classiﬁcation [22]. Thus  when we design a rejector in the multiclass case  it would

(cid:0)(1 − c  c)(cid:1) in the binary case. However  unlike the binary case  it is known that the
be natural to use the inverse link function to map output(cid:98)g to the estimated class probability vector(cid:98)η

rather than to use the link function itself as in the binary case. Based on this discussion  we consider
the following rejector based on the relationship between the inverse link Ψ−1
and the Bayes-optimal

y

5

Table 1: A list of margin losses and the values of θ  C and s that satisfy (14) and (15) in Theorem 7.

Loss Name

Logistic

Exponential

Squared

Squared Hinge

log(cid:0)1 + exp(−z)(cid:1)

φ(z)

exp(−z)
(1 − z)2
(1 − z)2

+

θ

c

c

1

log 1−c
2 log 1−c
1 − 2c
1 − 2c

C
1
2
1√
2
1
2
1
2

s
2
2

2
2

rejector r∗(x) = maxy∈Y ηy(x) − (1 − c):

r(x) = rf (x) = max

y∈Y Ψ−1

y

(cid:0)g(x)(cid:1)

− (1 − c).

(10)

Recall that we identify the classiﬁer f with g  and we use the notation rf in the sense that r is
determined by f. Below  we focus on two frequently used losses: one-versus-all (OVA) loss LOVA
and cross-entropy (CE) loss LCE:

LOVA(f ; x  y) = φ(cid:0)gy(x)(cid:1) +

LCE(f ; x  y) = −gy(x) + log

y(cid:48)(cid:54)=y

(cid:88)
φ(cid:0)
− gy(cid:48)(x)(cid:1) 
(cid:88)
exp(cid:0)gy(cid:48)(x)(cid:1) 
(cid:80)

Ψ−1
y  CE(g) =

y(cid:48)∈Y

exp(gy)

y(cid:48)∈Y exp(gy(cid:48))

(11)

 

(12)

for which the inverse link functions are given by

Ψ−1
y  OVA(g) =

φ(cid:48)(−gy)

φ(cid:48)(−gy) + φ(cid:48)(gy)

 

respectively. Here  φ denotes a margin loss [3]. Note that unlike the losses proposed in Ramaswamy
et al. [20]  the OVA loss and the CE loss do not contain c. Thus  training a classiﬁer once is sufﬁcient
for various choices of c.
We rely on the notion of excess risk bounds to prove the calibration result of the OVA loss and the
CE loss. Excess risk bounds [30  3  19] are a tool to directly quantify the relationship between the
surrogate risk R and the risk we truly want to minimize. In our problem  the true risk is the 0-1-c risk
in (1) and the excess risk bound to be derived is expressed as

(13)
where ξ : R → R≥0 is called a calibration function [19]  which is increasing  continuous at 0 and
satisﬁes ξ(0) = 0. Here  excess risks ∆R0-1-c(rf   f ) and ∆R(f ) are deﬁned as follows:

≤ ∆R(f ) 

ξ(cid:0)∆R0-1-c(rf   f )(cid:1)

∆R0-1-c(rf   f ) = R0-1-c(rf   f ) − R0-1-c(r∗  f∗) 

∆R(f ) = R(f ) −

inf

f(cid:48):measurable

R(f(cid:48)).

Ineq. (13) ensures that the minimization of a surrogate risk leads to the minimization of the 0-1-c
risk. Therefore  the existence of an excess risk bound guarantees calibration.
Now we give excess risk bounds for the OVA loss and the CE loss in the following theorems.
Theorem 7 (Excess risk bound for OVA loss). Assume that φ is a convex function  and there exists
θ > 0 such that φ(cid:48)(θ) and φ(cid:48)(−θ) both exist  φ(cid:48)(θ) < 0 and
= 1 − c.

φ(cid:48)(−θ)

(14)

φ(cid:48)(−θ) + φ(cid:48)(θ)

In addition  suppose that there exist some constants C > 0 and s ≥ 1 such that

(cid:27)

for all y ∈ Y and probability vector η. Then  for all f and c ∈

WOVA(f ; η) − inf
g(cid:48)∈RK

WOVA(f(cid:48); η)

≥ C−s|ηy − (1 − c)|s
(cid:2)0  1
(2C)−s∆R0-1-c(rf   f )s ≤ ∆ROVA(f ).

(cid:1)  we have

2

(cid:26)

inf

g: gy=θ

(15)

(16)

6

Figure 1: Average 0-1-c risk on the test data as a function of the training data size on synthetic
datasets.

Table 1 summarizes some margin losses with the values of θ  C and s that satisfy the assumptions
(14) and (15). Their derivations are given in Appendix A.2.
Theorem 8 (Excess risk bound for CE loss). For all f and c ∈ (0  1/2)  we have

1
2

∆R0-1-c(rf   f )2 ≤ ∆RCE(f ).

The proofs of Theorems 7 and 8 can be found in Appendices A.1 and A.4  respectively. The derivation
of the bound for the OVA loss is a natural extension of Yuan and Wegkamp [29] for the binary case.
On the other hand  although the CE loss can be regarded as a generalization of the logistic loss
in binary classiﬁcation  the derivation of the excess risk bound for the logistic loss in Yuan and
Wegkamp [29] heavily relies on the binary setting and is not applicable to the multiclass case. In fact 
the CE loss is generally hard to bound even in the setting without rejection as discussed in Pires and
Szepesvári [19]. In this paper  we reduce the analysis of the CE loss into that of the KL divergence
instead of trying to extend the argument of Yuan and Wegkamp [29] or Pires and Szepesvári [19].
This enabled us to derive the bound in a considerably simple way.
The excess risk bounds in Theorems 7 and 8 ensure that the minimization of the expected surrogate
risk leads to the minimization of the 0-1-c risk. On the other hand  we can also derive an estimation
error bound for the above losses  which shows that the minimization of the empirical surrogate
risk leads to the minimization of the expected surrogate risk for a ﬁnite number of samples with a
hypothesis class of our interest. Combining these results completes the scenario to minimize the 0-1-c
risk from ﬁnite number of samples under the considered hypothesis class. Here the derivation of the
estimation error bound using the notion of Rademacher complexity [2] is given in Appendix A.3.

5 Experiments

In this section  we report the results of two experiments based on synthetic and benchmark datasets.
The purpose of the experiment on synthetic datasets is to verify the performance of calibration for the
setting where Bayes-optimal 0-1-c risk is available. On the other hand  we use benchmark datasets to
evaluate the practical performance.
Common setup: For all methods  we used one-hidden-layer neural networks with the rectiﬁed
linear units (ReLU) as activation functions  where the number of hidden units is 3 for synthetic
datasets  and 50 for benchmark datasets. We added weight decay with candidates {10−7  10−4  10−1}.
AMSGRAD [21] was used for optimization. More detailed setups can be found in Appendix C.
Synthetic datasets: Here we report the performance of four methods analyzed in this paper. For the
classiﬁer-rejector approach  we used the MPC loss with the logistic loss in (7)  where we used α = 1
as in Cortes et al. [9]. To see the performance of the rejector  we set two values for β to satisfy either
of (6) denoted by MPC+log+acc and MPC+log+rej  respectively. It is expected that MPC+log+acc
will over-accept the data  and MPC+log+rej will over-reject the data as discussed in Remark 1. For
the conﬁdence-based approach  we used the CE loss (CE) and OVA loss with the logistic loss in (11)
denoted by OVA+log. Synthetic data consist of eight classes. More detailed information on data
generation process can be found in Appendix C.1.
Figure 1 shows the average 0-1-c risk on the test data for various training data size  where the lower
0-1-c risk is the better. CE shows the best performance in terms of convergence to the Bayes-optimal

7

103104105training size0.0200.0250.0300.0350.0400.0450.0500-1-c riskc=0.05MPC+log+accMPC+log+rejOVA+logCEBayes 0-1-c risk103104105training size0.060.080.100.120.140.160.180.200.220-1-c riskc=0.2MPC+log+accMPC+log+rejOVA+logCEBayes 0-1-c riskBayes 0-1 risk103104105training size0.100.150.200.250.300.350.400-1-c riskc=0.4MPC+log+accMPC+log+rejOVA+logCEBayes 0-1-c riskBayes 0-1 riskTable 2: Mean and standard deviation of the ratio (%) of the rejected data over all test data on
synthetic datasets when the training data size is 10 000 per class.

c MPC+log+acc MPC+log+rej

0.05
0.2
0.4

25.4 (8.6)
0.0 (0.0)
0.0 (0.0)

46.4 (7.6)
23.2 (1.6)
28.8 (9.9)

OVA+log
43.9 (1.3)
31.5 (0.3)
23.1 (0.7)

CE

33.9 (0.5)
28.3 (1.5)
17.3 (0.8)

Figure 2: Average and standard error of 0-1-c risk on the test data as a function of rejection cost c on
benchmark datasets for 10 trials. The standard error is plotted in shaded regions.

0-1-c risk for all values of c. In spite of the theoretical guarantees of the conﬁdence-based methods of
OVA losses  they did not show better performance than the others. A possible reason is that the inverse
link function of the OVA loss is not normalized as can be seen from (12)  which resulted in poor
estimation of class probability η(x). It is observed that the classiﬁer-rejector methods (MPC+log+acc
and MPC+log+rej) show unstable performance compared to the other methods. Table 2 shows the
rejection ratio when the training data size is 10 000 per class. We can conﬁrm that MPC+log+acc
tends to over-accept and MPC+log+rej tends to over-reject the data  which agrees with the discussion
in Remark 1.
Benchmark datasets: We compared the empirical performance using benchmark datasets with
rejection cost ranged over c ∈ {0.05  0.1  0.2  0.3  0.4}. In addition to APC+log  MPC+log  OVA+log
and CE  we further implemented the existing method proposed in Ramaswamy et al. [20] (OVA+hin) 
which uses OVA loss with non-smooth hinge loss in (11). We show the results of vehicle  satimage 
yeast  covtype and letter datasets from UCI Machine Learning Repository [17]  which are the same
datasets as those used in Ramaswamy et al. [20]. Table 3 summarizes the speciﬁcation of the
benchmark datasets we used. For the classiﬁer-rejector methods (APC+log  MPC+log)  we have extra
parameters α and β. We set α = 1 as in Cortes et al. [9]. We chose β by cross-validation  where the
choices of β that satisﬁes either of (6) were also included. In the OVA+hin formulation  Ramaswamy
et al. [20] suggested that the threshold parameter τ ∈ (−1  1) in their methods is preferable at 0.
Nevertheless  we observed that the performance is considerably affected by its choice and thus we
decided to choose the best parameter from ﬁve candidates by cross-validation. See Appendix C.2 for
the detailed information on experimental setups. Note that APC+log  MPC+log and OVA+hin must
be re-trained for different rejection costs  while OVA+log and CE do not need re-training. The full
experimental results including the performance of other methods  the full report of the 0-1-c risk  the
accuracy of the non-rejected data  and the rejection ratio can be found in Appendix C.
Figure 2 illustrates the 0-1-c risk as functions of the rejection cost. It can be observed that CE is
either competitive or preferable in all datasets. For OVA+log  despite its calibration guarantees  it is
outperformed by CE for all datasets and it is even outperformed by MPC+log in letter dataset. The
failure of the OVA methods in letter might be due to their weakness for a large number of classes [5]
and poor estimation of η(x). It is also worth noting that the standard deviations of MPC+log and
OVA+hin are considerably large compared to those of OVA+log and CE  which might be caused by
additional hyper-parameters β and τ. Moreover  model ﬁtting for a rejector and the non-convexity
of the MPC loss function also make MPC+log unstable. Table 4 shows the mean and standard
deviation of the accuracy on non-rejected data. As we can see clearly in yeast datasets  unlike the
conﬁdence-based methods  the classiﬁer-rejector methods reject all the test data even when the value

8

0.10.20.30.4c0.0250.0500.0750.1000.1250.1500.1750.2000.2250-1-c riskvehicleMPC+logOVA+logCEOVA+hin0.10.20.30.4c0.040.060.080.100.120-1-c risksatimageMPC+logOVA+logCEOVA+hin0.10.20.30.4c0.0250.0500.0750.1000.1250.1500.1750.2000.2250-1-c riskletterMPC+logOVA+logCEOVA+hinTable 3: Speciﬁcation of benchmark datasets: the number of features  the number of classes  the
number of training data  and the number of test data.

Name
vehicle
satimage

yeast
covtype
letter

18
36
8
54
16

#features #classes #train #test
146
2000
484

4
6
10
7
26

700
4435
1000
15120
15000

565892
5000

Table 4: Mean and standard deviation of the accuracy (%) of the non-rejected data samples for 10
trials. Best and equivalent methods (with 5% t-test) with respect to the 0-1-c risk are shown in bold
face. “–” corresponds to the case where all the test data samples are rejected.

– ( – )

c APC+log MPC+log OVA+log
100 (0.0)
97.9 (0.7)
90.2 (1.6)
98.7 (0.1)
96.2 (0.2)
92.2 (0.3)

96.6 (2.3)
92.4 (3.0)
85.3 (4.2)
97.2 (1.4)
92.6 (1.2)
89.0 (1.1)

dataset

vehicle

satimage

yeast

0.05
0.2 98.4 (1.9)
0.4 89.1 (2.9)
0.05 99.1 (0.2)
0.2 95.0 (1.0)
0.4 91.5 (0.7)
0.05
0.2
0.4

– ( – )
– ( – )
– ( – )

– ( – )
– ( – )
– ( – )

– ( – )
– ( – )

75.0 (3.9)

CE

100 (0.0)
97.4 (0.1)
91.7 (0.9)
98.3 (0.1)
95.7 (0.1)
91.8 (0.2)

– ( – )

80.6 (6.2)
76.6 (1.7)

dataset

covtype

letter

c APC+log MPC+log
0.05 79.5 (2.1)
79.8 (1.7)
73.8 (1.0)
0.2 74.0 (1.8)
0.4 69.8 (1.3)
64.9 (3.4)
0.05 99.8 (0.1)
98.6 (0.2)
96.9 (0.5)
0.2 97.9 (0.3)
0.4 95.2 (0.5)
94.6 (3.8)

OVA+log
82.1 (2.7)
74.9 (1.4)
68.7 (1.1)
99.6 ( 0.2 )
98.3 (0.2)
94.6 (0.2)

CE

82.0 (3.2)
77.1 (0.3)
69.4 (1.8)
99 8 (0.0)
98.4 (0.1)
94.9 (0.3)

of c is large. This implies that if the dataset is hard to learn  then classiﬁer-rejector methods may fail
to learn the rejector.

6 Conclusion

We presented a series of theoretical results on multiclass classiﬁcation with rejection. First  we
provided a necessary condition of rejection calibration for the classiﬁer-rejector approach that
suggested the difﬁculty of calibration for this approach in the multiclass case. Second  we investigated
the conﬁdence-based approach and established the calibration results for the OVA loss and the CE loss
by deriving excess risk bounds. Experimental results suggested that the CE loss is the most preferable
and the classiﬁer-rejector approach can no longer outperform the conﬁdence-based methods unlike
the binary case.

Acknowledgements

We thank Han Bao for fruitful discussions. We also thank anonymous reviewers for providing
insightful comments. NC was supported by MEXT scholarship and JST AIP challenge. JH was
supported by KAKENHI 18K17998. MS was supported by the International Research Center for
Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study.

References
[1] P. L. Bartlett and M. H. Wegkamp. Classiﬁcation with a reject option using a hinge loss. Journal

of Machine Learning Research  9:1823–1840  2008.

[2] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. The Annals of

Statistics  33:1487–1537  2002.

[3] P. L. Bartlett  M. I. Jordan  and J. D. Mcauliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[4] S. Ben-David  N. Eiron  and P. M. Long. On the difﬁculty of approximately maximizing

agreements. Journal of Computer and System Sciences  66(3):496–514  2003.

9

[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  2nd edition  2006.

[6] C. K. Chow. An optimum character recognition system using decision functions. IRE Transac-

tions on Electronic Computers  EC-6(4):247–254  1957.

[7] C. K. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Informa-

tion Theory  16(1):41–46  1970.

[8] C. Cortes  G. DeSalvo  and M. Mohri. Learning with rejection. In Proceedings of International

Conference on Algorithmic Learning Theory  pages 67–82  2016.

[9] C. Cortes  G. DeSalvo  and M. Mohri. Boosting with abstention. In Advances in Neural

Information Processing Systems 29  pages 1660–1668. 2016.

[10] C. Cortes  G. DeSalvo  C. Gentile  M. Mohri  and S. Yang. Online learning with abstention.
In Proceedings of the 35th International Conference on Machine Learning  pages 1059–1067 
2018.

[11] B. Dubuisson and M. Masson. A statistical decision rule with incomplete knowledge about

classes. Pattern Recognition  26:155–165  1993.

[12] V. Feldman  V. Guruswami  P. Raghavendra  and Y. Wu. Agnostic learning of monomials by

halfspaces is hard. SIAM Journal on Computing  41(6):1558–1590  2012.

[13] A. Garcia  S. Essid  C. Clavel  and F. d’Alché Buc. Structured output learning with abstention:
Application to accurate opinion prediction. In Proceedings of the 35th International Conference
on Machine Learning  pages 1695–1703  2018.

[14] Y. Grandvalet  A. Rakotomamonjy  J. Keshet  and S. Canu. Support vector machines with a
reject option. In Advances in Neural Information Processing Systems 21  pages 537–544. 2009.

[15] K. Hamid  A. Asif  W. Abbasi  D. Sabih  and F. A. Minhas. Machine learning with abstention
for automated liver disease diagnosis. In Proceedings of International Conference on Frontiers
of Information Technology  pages 356–361  2017.

[16] R. Herbei and M. H. Wegkamp. Classiﬁcation with reject option. Canadian Journal of Statistics 

34(4):709–721  2006.

[17] M. Lichman et al. UCI machine learning repository  2013. URL http://archive.ics.

uci.edu/ml.

[18] M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of Machine Learning. The MIT

Press  2012.

[19] B. Á. Pires and C. Szepesvári. Multiclass classiﬁcation calibration functions. arXiv preprint

arXiv:1609.06385  2016.

[20] H. G. Ramaswamy  A. Tewari  and S. Agarwal. Consistent algorithms for multiclass classiﬁca-

tion with an abstain option. Electronic Journal of Statistics  12:530–554  2018.

[21] S. J. Reddi  S. Kale  and S. Kumar. On the convergence of Adam and beyond. In Proceedings

of International Conference on Learning Representations  2018.

[22] M. D. Reid and R. C. Williamson. Composite binary losses. Journal of Machine Learning

Research  11:2387–2422  2010.

[23] D. Tax and R. Duin. Growing a multi-class classiﬁer with a reject option. Pattern Recognition

Letters  29(10):1565–1570  2008.

[24] A. Tewari and P. L. Bartlett. On the consistency of multiclass classiﬁcation methods. Journal of

Machine Learning Research  8:1007–1025  2007.

[25] E. Vernet  M. D. Reid  and R. C. Williamson. Composite multiclass losses. In Advances in

Neural Information Processing Systems 24  pages 1224–1232. 2011.

10

[26] M. Wegkamp and M. Yuan. Support vector machines with a reject option. Bernoulli  17(4):

1368–1385  2011.

[27] J. Weston and C. Watkins. Multi-class support vector machines. Technical report  Royal

Holloway  1998.

[28] Q. Wu  C. Jia  and W. Chen. A novel classiﬁcation-rejection sphere SVMs for multi-class classi-
ﬁcation problems. In Proceedings of the 3rd International Conference on Natural Computation 
volume 1  pages 34–38  2007.

[29] M. Yuan and M. H. Wegkamp. Classiﬁcation methods with reject option based on convex risk

minimization. Journal of Machine Learning Research  11:111–130  2010.

[30] T. Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225–1251  2004.

[31] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk

minimization. The Annals of Statistics  32:56–85  2004.

11

,Chenri Ni
Nontawat Charoenphakdee
Junya Honda
Masashi Sugiyama