2008,Improving on Expectation Propagation,We develop as series of corrections to Expectation Propagation (EP)  which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check  indicating when EP yields unrealiable results.,Improving on Expectation Propagation

Manfred Opper

Computer Science  TU Berlin

opperm@cs.tu-berlin.de

Ulrich Paquet

Computer Laboratory  University of Cambridge

ulrich@cantab.net

Informatics and Mathematical Modelling  Technical University of Denmark

Ole Winther

owi@imm.dtu.dk

Abstract

A series of corrections is developed for the ﬁxed points of Expectation Propaga-
tion (EP)  which is one of the most popular methods for approximate probabilistic
inference. These corrections can lead to improvements of the inference approxi-
mation or serve as a sanity check  indicating when EP yields unrealiable results.

1 Introduction

The expectation propagation (EP) message passing algorithm is often considered as the method of
choice for approximate Bayesian inference when both good accuracy and computational efﬁciency
are required [5]. One recent example is a comparison of EP with extensive MCMC simulations for
Gaussian process (GP) classiﬁers [4]  which has shown that not only the predictive distribution  but
also the typically much harder marginal likelihood (the partition function) of the data  are approxi-
mated remarkably well for a variety of data sets. However  while such empirical studies hold great
value  they can not guarantee the same performance on other data sets or when completely different
types of Bayesian models are considered.

In this paper methods are developed to assess the quality of the EP approximation. We compute
explicit expressions for the remainder terms of the approximation. This leads to various corrections
for partition functions and posterior distributions. Under the hypothesis that the EP approximation
works well  we identify quantities which can be assumed to be small and can be used in a series
expansion of the corrections with increasing complexity. The computation of low order corrections
in this expansion is often feasible  typically require only moderate computational efforts  and can
lead to an improvement to the EP approximation or to the indication that the approximation cannot
be trusted.

2 Expectation Propagation in a Nutshell

Since it is the goal of this paper to compute corrections to the EP approximation  we will not dis-
cuss details of EP algorithms but rather characterise the ﬁxed points which are reached when such
algorithms converge.

EP is applied to probabilistic models with an unobserved latent variable x having an intractable
distribution p(x). In applications p(x) is usually the Bayesian posterior distribution conditioned on
a set of observations. Since the dependency on the latter variables is not important for the subsequent
theory  we will skip them in our notation.

1

It is assumed that p(x) factorizes into a product of terms fn such that

p(x) =

fn(x)  

1

Z Yn

(1)

where the normalising partition function Z = R dx Qn fn(x) is also intractable. We then assume

an approximation to p(x) in the form

q(x) =Yn

gn(x)

(2)

where the terms gn(x) belong to a tractable  e.g. exponential family of distributions. To compute
the optimal parameters of the gn term approximation a set of auxiliary tilted distributions is deﬁned
via

qn(x) =

1

Zn (cid:18) q(x)fn(x)

gn(x) (cid:19) .

(3)

Here a single approximating term gn is replaced by an original term fn. Assuming that this re-
placement leaves qn still tractable  the parameters in gn are determined by the condition that q(x)
and all qn(x) should be made as similar as possible. This is usually achieved by requiring that these
distributions share a set of generalised moments (which usually coincide with the sufﬁcient statistics
of the exponential family). Note  that we will not assume that this expectation consistency [8] for
the moments is derived by minimising a Kullback–Leibler divergence  as was done in the original
derivations of EP [5]. Such an assumption would limit the applicability of the approximate inference
and exclude e.g. the approximation of models with binary  Ising variables by a Gaussian model as
in one of the applications in the last section.

The corresponding approximation to the normalising partition function in (1) was given in [8] and
[7] and reads in our present notation1

ZEP =Yn

Zn .

(4)

3 Corrections to EP

An expression for the remainder terms which are neglected by the EP approximation can be obtained
by solving for fn in (3)  and taking the product to get

Yn

fn(x) =Yn (cid:18) Znqn(x)gn(x)

q(x)

q(x) (cid:19) .
(cid:19) = ZEP q(x)Yn (cid:18) qn(x)

Hence Z =R dx Qn fn(x) = ZEP R  with
R =Z dx q(x)Yn (cid:18) qn(x)

q(x) (cid:19) and p(x) =

1
R

q(x) (cid:19) .
q(x)Yn (cid:18) qn(x)

(5)

(6)

This shows that corrections to EP are small when all distributions qn are indeed close to q  justifying
the optimality criterion of EP. For related expansions  see [2  3  9].

Exact probabilistic inference with the corrections described here again leads to intractable computa-
tions. However  we can derive exact perturbation expansions involving a series of corrections with
increasing computational complexity. Assuming that EP already yields a good approximation  the
computation of a small number of these terms maybe sufﬁcient to obtain the most dominant correc-
tions. On the other hand  when the leading corrections come out large or do not sufﬁciently decrease
with order  this may indicate that the EP approximation is inaccurate. Two such perturbation expan-
sions are be presented in this section.

1The deﬁnition of partition functions Zn is slightly different from previous works.

2

3.1 Expansion I: Clusters

The most basic expansion is based on the variables εn(x) = qn(x)
q(x) − 1 which we can assume to be
typically small  when the EP approximation is good. Expanding the products in (6) we obtain the
correction to the partition function

(1 + εn(x))

R =Z dx q(x)Yn
= 1 + Xn1<n2(cid:10)εn1 (x)εn2 (x)(cid:11)q + Xn1<n2<n3(cid:10)εn1 (x)εn2 (x)εn3 (x)(cid:11)q + . . .  

(7)

(8)

which is a ﬁnite series in terms of growing clusters of “interacting” variables εn(x). Here the
brackets h. . .iq denote expectations with respect to the distribution q. Note  that the ﬁrst order term
Pn hεn(x)iq = 0 vanishes by the normalization of qn and q. As we will see later  the computation
of corrections is feasible when qn is just a ﬁnite mixture of K simpler densities from the exponential
family to which q belongs. Then the number of mixture components in the j-th term of the expansion
of R is just of the order O(K j) and an evaluation of low order terms should be tractable.
In a similar way  we get

p(x) =

q(x) (cid:0)1 +Pn εn(x) +Pn1<n2 εn1(x)εn2 (x) + . . .(cid:1)

 

(9)

In order to keep the resulting density normalized to one  we should keep as many terms in the
numerator as in the denominator. As an example  the ﬁrst order correction to q(x) is

1 +Pn1<n2 hεn1 (x)εn2 (x)iq + . . .
p(x) ≈Xn

qn(x) − (N − 1)q(x) .

(10)

3.2 Expansion II: Cumulants

2

One of most important applications of EP is to the case of statistical models with Gaussian process
priors. Here x is a latent variable with Gaussian prior distribution and covariance E[xx⊤] = K
where K is the kernel matrix. In this case we have N + 1 terms f0  f1  . . .   fN in (1) where f0(x) =
g0(x) = exp[− 1
x⊤K−1x]. For n ≥ 1 each fn(x) = tn(xn) is the likelihood term for the nth
observation which depends only on a single component xn of the vector x.
The corresponding approximating terms are chosen to be Gaussian of the form gn(x) ∝
2 λnx2. The 2N parameters γn and λn are determined in such a way that q(x) and the dis-
eγnx− 1
tributions qn(x) have the same ﬁrst and second marginal moments hxni and hx2
ni. In this case  the
computation of corrections (7) would require the computation of multivariate integrals of increasing
dimensionality. Hence  a different type of expansion seems more appropriate. The main idea is to
expand with respect to the higher order cumulants of the distributions qn.
To derive this expansion  we simplify (6) using the fact that q(x) = q(x\n|xn)q(xn) and qn(x) =
q(x\n|xn)qn(xn)  where we have (with a slight abuse of notation) introduced q(xn) and qn(xn) 
the marginals of q(x) and qn(x). Thus p(x) = 1

R q(x)F (x) and R =R dx q(x)F (x)  where

(11)

q(xn) (cid:19) .
F (x) =Yn (cid:18) qn(xn)

Since q(xn) and the qn(xn) have the same ﬁrst two cumulants  corrections can be expressed by the
higher cumulants of the qn(xn) (note  that the higher cumulants of q(xn) vanish). The cumulants
cln of qn(xn) are deﬁned by their characteristic functions χn(k) via

qn(xn) =Z dk

2π

e−ikxn χn(k)

and

ln χn(k) =Xl

(i)l cln
l!

kl .

(12)

Expressing the Gaussian marginals q(xn) by their ﬁrst and second cumulants  the means mn and
the variances Snn and introducing the function

rn(k) =Xl≥3

(i)l cln
l!

kl

3

(13)

which contains the contributions of all higher order cumulants  we get

F (x) =Yn  R dkn exp(cid:2)−ikn(xn − mn) − 1

R dkn exp(cid:2)−ikn(xn − mn) − 1

2 Snnk2

Snnη2
n

=Z dηsYn

Snn
2π

exp"−Xn

2 Snnk2

!
n + rn(kn)(cid:3)
n(cid:3)
rn(cid:18)ηn − i

# exp"Xn

2

(14)

(15)

(xn − mn)

Snn

(cid:19)#

where in the last equality we have introduced a shift of variables ηn = kn + i (xn−mn)
An expansion can be performed with respect to the cumulants in the terms gn which had been ne-
glected in the EP approximation. The basic computations are most easily explained for the correction
R to the partition function.

Snn

.

3.2.1 Correction to the partition function
Since q(x) is a multivariate Gaussian of the form q(x) = N (x; m  S)  the correction R to the
partition Z involves a double Gaussian average over the vector x and the set of ηn. This can be
simpliﬁed by combining them into a single complex zero mean Gaussian random vector deﬁned as
zn = ηn − i xn−mn

such that

Snn

R =*exp"Xn

rn (zn)#+z

(16)

(17)

The most remarkable property of the Gaussian z is its covariance which is easily found to be

hzizjiz = −

Sij

SiiSjj

when i 6= j 

and hz2

i iz = 0 .

The last equation has important consequences for the surviving terms in an expansion of R!
Assuming that the gn are small we perform a power series expansion of ln R

1

1

1

=

clnclm

2 Xm6=n

hrmrniz ± . . .

hrniz(cid:17)2

rn (zn)i+z

ln R = ln*exphXn

=Xn
hrniz +
= Xm6=nXl≥3

rn(cid:17)2Ez
2D(cid:16)Xn
−
SnnSmm(cid:19)l
l! (cid:18) Snm

2(cid:16)Xn
± . . .
Here we have repeatedly used the fact that each factor zn in expectations hzl
mi have to be paired
(by Wick’s theorem) with a factor zm where m 6= n (diagonal terms vanish by (17)). This gives
nonzero contributions only  when l = s and there are l! ways for pairing.2
This expansion gives a hint why EP may work typically well for multivariate models when covari-
ances Sij are small compared to the variances Sii. While we may expect that ln ZEP = O(N )
where N is the number of variables xn  the vanishing of the “self interactions” indicates that correc-
tions may not scale with N .

± . . . (18)

(19)

nzs

3.2.2 Correction to marginal moments

The predictive density of a novel observation can be treated by extending the Gaussian prior to
include a new latent variable x∗ with E[x∗x] = k∗ and E[x2
∗] = k∗  and appears as an average of a
likelihood term over the posterior marginal of x∗.
A correction for the predictive density can also be derived in terms of the cumulant expansion by
averaging the conditional distribution p(x∗|x) = N (x∗; k⊤
K−1k∗.
∗) with σ2
Using the expression (15) we obtain (where we set R = 1 in (6) to lowest order)
Snn (cid:19) + . . .+η x∼N (x;µ Σ)
xn − mn
p(x∗) =Z dx p(x∗|x) p(x) = N (x∗; µx∗   s2

rn(cid:18)ηn − i

∗ = k∗ − k⊤

K−1x  σ2

x∗)*1 +Xn

∗

∗

(20)

2The terms in the expansion might be organised in Feynman graphs  where “self interaction” loops are

absent.

4

Z
g
o
l

−195

−200

−205

−210

−215

−220

−225

−230

−235

1

2

3

4

5

6

Number of components K

Figure 1:
ln Z approximations obtained from
q(x)’s factorization in (2)  for sec. 4.1’s mixture
model  as obtained by: variational Bayes (see [1]
for details) as red squares; α = 1
2 in Minka’s α-
divergence message passing scheme  described in
[6]  as magenta triangles; EP as blue circles; EP
with the 2nd order correction in (8) as green di-
amonds. For 20 runs each  the colour intensities
correspond to the frequency of reaching different
estimates. A Monte Carlo estimate of the true
ln Z  as found by parallel tempering with thermo-
dynamic integration  is shown as a line with two-
standard deviation error bars.

K−1m and variance s2

where µx∗ = k⊤
∗ (K + Λ−1)−1k∗ and Λ = diag(λ) denotes
∗
the parameters in the Gaussian terms gn. The average in (20) is over a Gaussian x with Σ−1 =
(K − k−1
ΣK−1k∗ + m. By simplifying the inner
expectation over the complex Gaussian variables η we obtain

x∗ = k∗ − k⊤
∗ )−1 + Λ−1 and µ = (x∗ − µx∗ )σ−2

k∗k⊤

∗

∗

p(x∗) = N (x∗; µx∗   s2

x∗)
1 +Xn Xl≥3

+ ···
(21)
where hl is the lth Hermite polynomial. The Hermite polynomials are averaged over a Gaussian
density where the only occurrence of x∗ is through (x∗ − µx∗) in µ  so that the expansion ultimately
appears as a polynomial in x∗. A correction to the predictive density follows from averaging t∗(x∗)
over (21).

√Snn(cid:19)l*hl(cid:18) xn − mn

√Snn (cid:19)+x∼N (x;µ Σ)

cln

l! (cid:18) 1

4 Applications

4.1 Mixture of Gaussians

This section illustrates an example where a large ﬁrst nontrivial correction term in (8) reﬂects an
inaccurate EP approximation. We explain this for a K-component Gaussian mixture model.

Consider N observed data points ζn with likelihood terms fn(x) = Pκ πκN (ζn; µκ  Γ−1
κ )  with
n ≥ 1 and with the mixing weights πκ forming a probability vector. The latent variables are then
κ=1. For our prior on x we use a Dirichlet distribution and product of Normal-
x = {πκ  µκ  Γκ}K
Wisharts densities so that f0(x) = D(π)Qκ NW(µκ  Γκ). When we multiply the fn terms we
see that intractability for the mixture model arises because the number of terms in the marginal
likelihood is K N   rather than because integration is intractable. The computation of lower-order
terms in (8) should therefore be immediately feasible. The approximation q(x) and each gn(x) are
chosen to be of the same exponential family form as f0(x)  where we don’t require gn(x) to be
normalizable.

For brevity we omit the details of the EP algorithm for this mixture model  and assume here that an
EP ﬁxed point has been found  possibly using some damping. Fig. 1 shows various approximations
to the log marginal likelihood ln Z for ζn coming from the acidity data set. It is evident that the
“true peak” doesn’t match the peak obtained by approximate inference  and we will wrongly predict
which K maximizes the log marginal likelihood. Without having to resort to Monte Carlo methods 
the second order correction for K = 3 both corrects our prediction and already conﬁrms that the
original approximation might be inadequate.

4.2 Gaussian Process Classiﬁcation

The GP classiﬁcation model arises when we observe N data points ζn with class labels yn ∈
{−1  1}  and model y through a latent function x with the GP prior mentioned in sec. 3.2. The

5

)
σ
(
n
l

 
e
d
u
t
i
n
g
a
m
g
o
l

0
3

.

0
4

.

0
6

.

0
5

.

5

4.5

4

3.5

3

2.5

0

.

3

0

.

4

0

.

5

0

.

6

2

1.5

1

0.5

0

0.3

0.2

0.1

1.5

0.4

0.3

0.2

0.1

0.3

0 .2

0 . 1

1

0 . 0

1

0 . 0

1

0

0 . 0

6
.
0

0.5

0.4

0.5

0.3

0 . 4

0.2

0.1

1

0 . 0
2.5

1

0

0 . 0
3.5

1

0

0

0 . 0

4.5
2
log lengthscale  ln(ℓ)

4

3

)
σ
(
n
l

 
e
d
u
t
i
n
g
a
m
g
o
l

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0 . 2
0.1

1.5

5

0.2

5

0 . 2

0 . 1

1

0 . 0

1

0

0 . 0

1

0

0

0 . 0

0.1

1

0 . 0

1

0

0 . 0

0.4

3
.
0

0.2

0.1

1

0 . 0
2.5

4.5
2
log lengthscale  ln(ℓ)

3.5

3

4

(a) ln R  second order  with l = 3  4.

(b) Monte Carlo ln R

Figure 2: A comparison of a perturbation expansion of (19) against Monte Carlo estimates of the
true correction ln R  using the USPS data set from [4].

likelihood terms for yn are assumed to be tn(xn) = Φ(ynxn)  where Φ(·) denotes the cumulative
Normal density.

Eq. (19) shows how to compute the cumulant expansion by dovetailing the EP ﬁxed point with the
characteristic function of qn(xn): From the EP ﬁxed point we have q(x) = N (x; m  S) and gn ∝
eγnxn− 1
2 λnxn; consequently the marginal density of xn in q(x)/gn(xn) from (3) is N (xn; µ  v2) 
where v−2 = 1/Snn − λn and µ = v−2(mn/Snn − γn). Using (3) again we have

qn(xn) =

1
Zn

Φ(ynxn)N (xn; µ  v2) .

(22)

The characteristic function of qn(xn) is obtained by the inversion of (12) 

ynµ + ikv2
√1 + v2

ynµ
√1 + v2

2 k2v2 Φ(wk)
Φ(w)

 

(23)

and wk =

  with w =

χn(k) =(cid:10)eikxn(cid:11) = eikµ− 1
with expectations h···i being with respect to qn(xn). Raw moments are computed through deriva-
tives of the characteristic function  i.e. hxj
n (0). The cumulants cln are determined
from the derivatives of ln χn(k) evaluated at zero (or equally from raw moments  e.g. c3n =
2hxni3 − 3hxnihx2

ni + hx3
c3n = α3β(cid:2)2β2 + 3wβ + w2 − 1(cid:3)
c4n = −α4β(cid:2)6β3 + 12wβ2 + 7w2β + w3 − 4β − 3w(cid:3)  
where α = v2/√1 + v2 and β = N (w; 0  1)/Φ(w).

ni = i−jχ(j)

ni)  such that

An extensive MCMC evaluation of EP for GP classiﬁcation on various data sets was recently given
by [4]  showing that the log marginal likelihood of the data can be approximated remarkably well.
An even more accurate estimation of the approximation error is given by considering the second
order correction in (19) (computed here up to l = 4). For GPC we generally found that the l = 3
term dominates l = 4  and we do not include any higher cumulants here. Fig. 2 illustrates the ln R
correction on the binary subproblem of the USPS 3’s vs. 5’s digits data set  with N = 767  as was
used by [4]. We used the same kernel k(ζ  ζ′) = σ2 exp(− 1
2kζ − ζ′k2/ℓ2) as [4]  and evaluated
(19) on a similar grid of ln ℓ and ln σ values. For the same grid values we obtained Monte Carlo
estimates of ln Z  and hence ln R. They are plotted in ﬁg. 2(b) for the cases where they estimate ln Z
to sufﬁcient accuracy (up to four decimal places) to obtain a smoothly varying plot of ln R.3 The
correction from (19)  as computed here  is O(N 2)  and compares favourably to O(N 3) complexity
of EP for GPC.
3The Monte Carlo estimates in [4] are accurate enough for showing EP’s close approximation to ln Z  but

(24)
(25)

not enough to make any quantiﬁed statement about ln R.

6

o
i
t
a
r

n
o
i
t
c
e
r
r
o
c
&
a∗
x

f
o

s
t
n
e
i
c
ﬃ
e
o
C

1.2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

 

 

y
 = +1
n
y
 = −1
n
0
coeff of x
*
1
coeff of x
*
2
coeff of x
*
3
coeff of x
*

correction ratio
p
gpc
    p

 = 1) /
(y
*
(y
 = 1)
*

corr

−6

−4

−2

0

2

4

6

Location of ζ∗

Figure 3: The initial coefﬁ-
cients of the polynomial in
x∗  as they ultimately appear
in the ﬁrst nontrivial correc-
tion term in (21). Cumulants
l = 3 and l = 4 were used.
The coefﬁcients are shown for
test points ζ∗ after observ-
ing data points ζn. The ra-
tio between the standard and
(1st order) corrected GP clas-
siﬁcation predictive density is
also illustrated.

In ﬁg. 3 we show the coefﬁcients of the polynomial corrections (21) in powers of x∗ to the predictive
density p(x∗)  using 3rd and 4th cumulants. The small corrections arise as whenever terms ynmn
are positive and large compared to the posterior variance  non-Gaussian terms fn(x) = tn(xn) ≈ 1
for almost all values of xn which have signiﬁcant probability under the Gaussian distribution that
is proportional to q(x)/gn(xn). For these terms qn(x) is therefore almost Gaussian and higher
cumulants are small. A example where this will no longer be the case is a GP model with tn(xn) = 1
for |xn| < a and tn(xn) = 0 for |xn| > a. This is a regression model yn = xn+νn where i.i.d. noise
variables νn have uniform distribution and the observed outputs are all zero  i.e. yn = 0. For this
case  the exact posterior variance does not shrink to zero even if the number of data points goes
to inﬁnity. The EP approximation however has the variance decrease to zero and our corrections
increase with sample size.

4.3 Ising models

Somewhat surprising (and probably less known) is the fact that EP and our corrections apply
well to a fairly limiting case of the GP model where the terms are of the form tn(xn) =
eθnxn (δ(xn + 1) + δ(xn − 1))  where δ(x) is the Dirac distribution. These terms  together with
a “Gaussian” f0(x) = exp[Pi<j Jij xixj] (where we do not assume that the matrix J is negative
deﬁnite)  makes this GP model an Ising model with binary variables xn = ±1. As shown in [8] 
this model can still be treated with the same type of Gaussian term approximations as ordinary GP
models  allowing for surprisingly accurate estimation of the mean and covariance. Here we will
show the effect of our corrections for toy models  where exact inference is possible by enumeration.
The tilted distributions qn(xn) are biased binary distributions with cumulants: c3n = −2mn(1 −
n  etc. We will consider two different scenarios for random θ and J
n)  c4n = −2 + 8m2
m2

n − 6m4

l

i

s
a
n
g
r
a
m
 
e
d
o
n
 
2
 
D
A
M

100

10−2

10−4

10−6

10−1

105

100

10−5

y
g
r
e
n
e

 

e
e
r
F
D
A

 

100
β

101

10−10

10−1

100
β

101

Figure 4: The left plot shows the MAD of the estimated covariance matrix from the exact one for
different values of β for EP (blue)  EP 2nd order l = 4 corrections (blue with triangles)  Bethe or
loopy belief propagation (LBP; dashed green) and Kikuchi or generalized LBP (dash–dotted red).
The Bethe and Kikuchi approximations both give covariance estimates for all variable pairs as the
model is fully connected. The right plot shows the absolute deviation of ln Z from the true value
using second order perturbations with l = 3  4  5 (l = 3 is the smallest change). The remaining line
styles are the same as in the left plot.

7

described in detail in [8]. In the ﬁrst scenario  with N = 10  the Jij’s are generated independently
at random according to Jij = βwij and wij ∼ N (0  1). For varying β  the maximum absolute
deviation (MAD) of the estimated covariance matrices from the exact one maxi j |Σest
ij − Σexact
|
is shown in ﬁg. 4 left. The absolute deviation on the log partition function is shown in ﬁg. 4 right.
In the Wainwright-Jordan set-up N = 16 nodes are either fully connected or connected to nearest
neighbors in a 4–by–4 grid. The external ﬁeld (observation) strengths θi are drawn from a uniform
distribution θi ∼ U[−dobs  dobs] with dobs = 0.25. Three types of coupling strength statistics are
considered: repulsive (anti-ferromagnetic) Jij ∼ U[−2dcoup  0]  mixed Jij ∼ U[−dcoup  +dcoup]
and attractive (ferromagnetic) Jij ∼ U[0  +2dcoup]. Table 1 gives the MAD of marginals averaged
of 100 repetitions. The results for both set-ups give rise to the conclusion that when the EP approx-
imation works well then the correction give an order of magnitude of improvement. In the opposite
situation  the correction might worsen the results.

ij

Table 1: Average MAD of marginals in a Wainwright-Jordan set-up  comparing loopy belief prop-
agation (LBP)  log-determinant relaxation (LD)  EP  EP with l = 5 correction (EP+)  and EP with
only one spanning tree approximating term (EP tree).

Problem type

Method

Graph

Full

Grid

Coupling
Repulsive
Repulsive

Mixed
Mixed

Attractive
Attractive
Repulsive
Repulsive

Mixed
Mixed

Attractive
Attractive

dcoup
0.25
0.50
0.25
0.50
0.06
0.12
1.0
2.0
1.0
2.0
1.0
2.0

LBP
0.037
0.071
0.004
0.055
0.024
0.435
0.294
0.342
0.014
0.095
0.440
0.520

LD
0.020
0.018
0.020
0.021
0.027
0.033
0.047
0.041
0.016
0.038
0.047
0.042

EP
0.003
0.031
0.002
0.022
0.004
0.117
0.153
0.198
0.011
0.082
0.125
0.177

EP+

0.00058487

0.0157

0.00042727

0.0159
0.0023
0.1066
0.1693
0.4244
0.0122
0.0984
0.1759
0.4730

EP tree
0.0017
0.0143
0.0013
0.0151
0.0025
0.0211
0.0031
0.0021
0.0018
0.0068
0.0028
0.0002

5 Outlook

We expect that it will be possible to develop similar corrections to other approximate inference
methods  such as the variational approach or the “power EP” approximations which interpolate
between the variational method and EP. This may help the user to decide which approximation is
more accurate for a given problem. We will also attempt an analysis of the scaling of higher order
terms in these expansions to see if they are asymptotic or have a ﬁnite radius of convergence.

References
[1] H. Attias. A variational Bayesian framework for graphical models. In Advances in Neural Information

Processing Systems 12  2000.

[2] M. Chertkov and V. Y. Chernyak. Loop series for discrete statistical models on graphs. Journal of Statistical

Mechanics: Theory and Experiment  page P06009  2006.

[3] S. Ikeda  T. Tanaka  and S. Amari. Information geometry of turbo and low-density parity-check codes.

IEEE Transactions on Information Theory  50(6):1097  2004.

[4] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁcation.

Journal of Machine Learning Research  6:1679–1704  2005.

[5] T. P. Minka. Expectation propagation for approximate Bayesian inference. In UAI 2001  pages 362–369 

2001.

[6] T. P. Minka. Divergence measures and message passing. Technical Report MSR-TR-2005-173  Microsoft

Research  Cambridge  UK  2005.

[7] T.P. Minka. The EP energy function and minimization schemes. Technical report  2001.
[8] M. Opper and O. Winther. Expectation consistent approximate inference. Journal of Machine Learning

Research  6:2177–2204  2005.

[9] E. Sudderth  M. Wainwright  and A. Willsky. Loop series and Bethe variational bounds in attractive graph-

ical models. In Advances in Neural Information Processing Systems 20  pages 1425–1432. 2008.

8

,Tengyu Ma
Avi Wigderson
Pan Zhou
Xiaotong Yuan
Jiashi Feng