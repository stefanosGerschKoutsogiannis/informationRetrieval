2013,Learning Chordal Markov Networks by Constraint Satisfaction,We investigate the problem of learning the structure of a Markov network from data. It is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data. To achieve efficient encodings  we develop a novel characterization of Markov network structure using a balancing condition on the separators between cliques forming the network. The resulting translations into propositional satisfiability and its extensions such as maximum satisfiability  satisfiability modulo theories  and answer set programming  enable us to prove the optimality of networks which have been previously found by stochastic search.,Learning Chordal Markov Networks by

Constraint Satisfaction

Jukka Corander∗†
University of Helsinki

Finland

Tomi Janhunen∗‡
Aalto University

Finland

Jussi Rintanen∗‡§
Aalto University

Finland

Henrik Nyman¶

˚Abo Akademi University

Finland

Johan Pensar¶

˚Abo Akademi University

Finland

Abstract

We investigate the problem of learning the structure of a Markov network from
data. It is shown that the structure of such networks can be described in terms of
constraints which enables the use of existing solver technology with optimization
capabilities to compute optimal networks starting from initial scores computed
from the data. To achieve efﬁcient encodings  we develop a novel characteriza-
tion of Markov network structure using a balancing condition on the separators
between cliques forming the network. The resulting translations into proposi-
tional satisﬁability and its extensions such as maximum satisﬁability  satisﬁability
modulo theories  and answer set programming  enable us to prove optimal certain
networks which have been previously found by stochastic search.

1

Introduction

Graphical models (GMs) represent the backbone of the generic statistical toolbox for encoding de-
pendence structures in multivariate distributions. Using Markov networks or Bayesian networks
conditional independencies between variables can be readily communicated and used for various
computational purposes. The development of the statistical theory of GMs is largely set by the
seminal works of Darroch et al. [1] and Lauritzen and Wermuth [2]. Although various approaches
have been developed to generalize the theory of graphical models to allow for modeling of more
complex dependence structures  Markov networks and Bayesian networks are still widely used in
applications ranging from genetic mapping of diseases to machine learning and expert systems.
Bayesian learning of undirected GMs  also known as Markov random ﬁelds  from databases has
attained a considerable interest  both in the statistical and computer science literature [3  4  5  6  7 
8  9]. The cardinality and complex topology of GM space pose difﬁculties with respect to both the
computational complexity of the learning task and the reliability of reaching representative model
structures. Solutions to these problems have been proposed in earlier work. Della Pietra et al. [10]
present a greedy local search algorithm for Markov network learning and apply it to discovering
word morphology. Lee et al. [11] reduce the learning problem to a convex optimization problem
that is solved by gradient descent. Related methods have been investigated later [12  13].

∗This work was funded by the Academy of Finland  project 251170.
†Funded by ERC grant 239784.
‡Also afﬁliated with the Helsinki Institute of Information Technology  Finland.
§Also afﬁliated with Grifﬁth University  Brisbane  Australia.
¶This work was funded by the Foundation of ˚Abo Akademi University  as part of the grant for the Center of

Excellence in Optimization and Systems Engineering.

1

Certain types of stochastic search methods  such as Markov Chain Monte Carlo (MCMC) or simu-
lated annealing  can be proven to be consistent with respect to the identiﬁcation of a structure max-
imizing posterior probability [4  5  6  7]. However  convergence of such methods towards the areas
associated with high posterior probabilities may still be slow when the number of nodes increases
[4  6]. In addition  it is challenging to guarantee that the identiﬁed model indeed truly represents
the global optimum since the consistency of MCMC estimates is by deﬁnition a limit result. To the
best of our knowledge  strict constraint-based search methods have not been previously applied in
learning of Markov random ﬁelds. In this article  we formalize the structure of Markov networks
using constraints at a general level. This enables the development of reductions from the structure
learning problem to propositional satisﬁability (SAT) [14] and its generalizations such as maximum
satisﬁability (MAXSAT) [15]  and satisﬁability modulo theories (SMT) [16]  as well as answer-set
programming (ASP) [17]. A main novelty is the recognition of maximum weight spanning trees
of the clique graph by a condition on the cardinalities of occurrences of variables in cliques and
separators  which we call the balancing condition.
The article is structured as follows. We ﬁrst review some details of Markov networks and the re-
spective structure learning problem in Section 2. To enable efﬁcient encodings of Markov network
learning as a constraint satisfaction problem  in Section 3 we establish a new characterization of
the separators of a Markov network based on a balancing condition. In Section 4  we provide a
high-level description how the learning problem can be expressed using constraints and sketch the
actual translations into propositional satisﬁability (SAT) and its generalizations. We have imple-
mented these translations and conducted experiments to study the performance of existing solver
technology on structure learning problems in Section 5 using two widely used datasets [18]. Finally 
some conclusions and possibilities for further research in this area are presented in Section 6.

2 Structure Learning for Markov Networks
An undirected graph G = (cid:104)V  E(cid:105) consists of a set of nodes V which represents a set of random
variables and a set of undirected edges E ⊆ {{n  n(cid:48)} | n  n(cid:48) ∈ V and n (cid:54)= n(cid:48)}. A path in a graph
is a sequence of nodes such that every two consecutive nodes are connected by an edge. Two sets of
nodes A and B are said to be separated by a third set of nodes D if every path between a node in
A and a node in B contains at least one node in D. An undirected graph is chordal if for all paths
n0  . . . nk with k ≥ 4 and n0 = nk there exist two nodes ni  nj in the path connected by an edge
such that j (cid:54)= i ± 1. A clique in a graph is a set of nodes c such that every two nodes in it are
connected by an edge. In addition  there may not exist a set of nodes c(cid:48) such that c ⊂ c(cid:48) and every
two nodes in c(cid:48) are connected by an edge. Given the set of cliques C in a chordal graph  the set of
separators S can be obtained through intersections of the cliques ordered in terms of a junction tree
[19]  this operation is considered thoroughly in Section 3.
A Markov network is deﬁned as a pair consisting of a graph G and a joint distribution PV over
the variables in V . The graph speciﬁes the dependence structure of the variables and PV factorizes
according to G (see below). Given G it is possible to ascertain if two sets of variables A and B are
conditionally independent given another set of variables D  due to the global Markov property

A ⊥⊥ B | D  if D separates A from B.

For a Markov network with a chordal graph G  the probability of a joint outcome x factorizes as

Following this factorization the marginal likelihood of a dataset X given a Markov network with a
chordal graph G can be written

(cid:81)
(cid:81)
(cid:81)
(cid:81)

PV (x) =

ci∈C Pci(xci)
si∈S Psi(xsi)

.

P (X|G) =

ci∈C Pci(Xci)
si∈S Psi(Xsi)

.

By a suitable choice of prior distribution  the terms Pci(Xci ) and Psi (Xsi) can be calculated ana-
lytically. Let a denote an arbitrary clique or separator containing the variables Xa whose outcome
space has the cardinality k. Further  let n(j)
in

a denote the number of occurrences where Xa = x(j)
a

2

the dataset Xa. Now assign the Dirichlet(αa1  . . .   αak ) distribution as prior over the probabilities
Pa(Xa = x(j)

a ) = θj  determining the distribution Pa(Xa). Now Pa(Xa) can be calculated as

where πa(θ) is the density function of the Dirichlet prior distribution. By the standard properties of
the Dirichlet integral  Pa(Xa) can be reduced to the form

where Γ(·) denotes the gamma function and

(cid:90)

k(cid:89)

Θ

j=1

Pa(Xa) =

(θj)n(j)

a

· πa(θ)dθ

Pa(Xa) =

Γ(α)

Γ(na + α)

Γ(n(j)

a + αaj )
Γ(αaj )

k(cid:89)

j=1

k(cid:88)

j=1

n(j)
a .

k(cid:88)

j=1

α =

αaj

and

na =

(cid:88)

ci∈C

log Pci (Xci) − (cid:88)
(cid:80)
P (X|G)P (G)
G∈G P (X|G)P (G)

log Psi(Xsi) =

P (G|X) =

si∈S

.

When dealing with the marginal likelihood of a dataset it is most often necessary to use the logarith-
mic value log P (X|G). Introducing the notations v(ci) = log Pci(Xci) the logarithmic value of the
marginal likelihood can be written

(cid:88)

ci∈C

v(ci) − (cid:88)

si∈S

v(si).

(1)

log P (X|G) =

The learning problem is to ﬁnd a graph G that optimizes the posterior distribution

Here G denotes the set of all graphs under consideration and P (G) is the prior probability assigned
to G. In the case where a uniform prior is used for the graphs the optimization problem reduces to
ﬁnding the graph with the largest marginal likelihood.

3 Fundamental Properties and Characterization Results

In this section  we point out some properties of chordal graphs and clique graphs that can be utilized
in the encodings of the learning problem. In particular  we develop a characterization of maximum
weight spanning trees in terms of a balancing condition on separators.
The separators needed for determining the score (1) of a candidate Markov network are deﬁned as
follows. Given the cliques  we can form the clique graph  in which the nodes are the cliques and
there is an edge between two nodes if the corresponding cliques have a non-empty intersection.
We label each of the edges with this intersection and consider the cardinality of the label as its
weight. The separators are the edge labels of a maximum weight spanning tree of the clique graph.
Maximum weight spanning trees of arbitrary graphs can be found in polynomial time by reducing
the problem to ﬁnding minimum weight spanning trees. This reduction consists of negating all the
edge weights and then using any of the polynomial time algorithms for the latter problem [20]. There
may be several maximum weight spanning trees  but they induce exactly the same separators  and
they only differ in terms of which pairs of cliques induce the separators.
To restrict the search space we can observe that a chordal graph with n nodes has at most n maximal
cliques [19]. This gives an immediate upper bound on the number of cliques chosen to build a
Markov network  which can be encoded as a simple cardinality constraint.

3.1 Characterization of Maximum Weight Spanning Trees

To simplify the encoding of maximum weight spanning trees (and forests) of chordal clique graphs 
we introduce the notion of balanced spanning trees (respectively  forests)  and show that these two
concepts coincide for chordal graphs. Then separators can be identiﬁed more effectively: rather than
encoding an algorithm for ﬁnding maximum-weight spanning trees as constraints  it is sufﬁcient to
select a subset of the edges of the clique graph that is acyclic and satisﬁes the balancing condition
expressible as a cardinality constraint over occurrences of nodes in cliques and separators.

3

Deﬁnition 1 (Balancing) A spanning tree (or forest) of a clique graph is balanced if for every node
n  the number of cliques containing n is one higher than the number of labeled edges containing n.

While in the following we state many results for spanning trees only  they can be straightforwardly
generalized to spanning forests as well (in case the Markov networks are disconnected.)

Lemma 2 For any clique graph  all its balanced spanning trees have the same weight.

Proof: This holds in general because the balancing condition requires exactly the same number of
occurrences of any node in the separator edges for any balanced spanning tree  and the weight is
(cid:3)
deﬁned as the sum of the occurrences of nodes in the edge labels.

Lemma 3 ([21  22]) Any maximum weight spanning tree of the clique graph is a junction tree  and
hence satisﬁes the running intersection property: for every pair of nodes c and c(cid:48)  (c ∩ c(cid:48)) ⊆ c(cid:48)(cid:48) for
all nodes c(cid:48)(cid:48) on the unique path between c and c(cid:48).

Lemma 4 Let T = (cid:104)V  ET(cid:105) be a maximum weight spanning tree of the clique graph (cid:104)V  E(cid:105) of a
connected chordal graph. Then T is balanced.

Proof: We order the tree by choosing an arbitrary clique as the root and by assigning a depth to all
nodes according to their distance from the root node. The rest of the proof proceeds by induction on
the height of subtrees starting from the leaf nodes as the base case. The induction hypothesis says
that all subtrees satisfy the balancing condition. The base cases are trivial: each leaf node (clique)
trivially satisﬁes the balancing condition  as there are no separators to consider.
In the inductive cases  we have a clique c at depth d  connected to one or more subtrees rooted at
neighboring cliques c1  . . .   ck at depth d + 1  with the subtrees satisfying the balancing condition.
We show that the tree consisting of the clique c  the labeled edges connecting c respectively to
cliques c1  . . .   ck  and the subtrees rooted at c1  . . .   ck  satisﬁes the balancing condition.
First note that by Lemma 3  any maximum weight spanning tree of the clique graph is a junction
tree and hence satisﬁes the running intersection property  meaning that for any two cliques c1 and c2
in the tree  every clique on the unique path connecting them includes c1 ∩ c2.
We have to show that the subtree rooted at c is balanced  given that its subtrees are balanced. We
show that the balancing condition is satisﬁed for each node separately. So let n be one of the nodes
in the original graph. Now each of the subtrees rooted at some ci has either 0 occurrences of n  or
ki ≤ 1 occurrences in the cliques and ki− 1 occurrences in the edge labels  because by the induction
hypothesis the balancing condition is satisﬁed. Four cases arise:

1. The node n does not occur in any of the subtrees.

Now the balancing condition is trivially satisﬁed for the subtree rooted at c  because n
either does not occur in c  or it occurs in c but does not occur in the label of any of the
edges to the subtrees.

2. The node n occurs in more than one subtree.

Since any maximum weight spanning tree is a junction tree by Lemma 3  n must occur
also in c and in the labels of the edges between c and the cliques in which the subtrees with
n are rooted. Let s1  . . .   sj be the numbers of occurrences of n in the edge labels in the
subtrees with at least one occurrence of n  and t1  . . .   tj the numbers of occurrences of n
in the cliques in the same subtrees.
By the induction hypothesis  these subtrees are balanced  and hence ti − si = 1 for all
i=1 ti occurrences of n in the nodes
i=1 si occurrences in the edge labels 

i ∈ {1  . . .   j}. The subtree rooted at c now has 1 +(cid:80)k
(once in c itself and then the subtrees) and j +(cid:80)j

where the j occurrences are in the edges between c and the j subtrees.

4

We establish the balancing condition through a sequence of equalities. The ﬁrst and the last
expression are the two sides of the condition.

(1 +(cid:80)j
= 1 − j +(cid:80)j

i=1 ti) − (j +(cid:80)k

= 1 − j + j
= 1

i=1 si)

i=1(ti − si)

reordering the terms
since ti − si = 1 for every subtree

Hence also the subtree rooted at c is balanced.

3. The node n occurs in one subtree and in c.

Let i be the index of the subtree in which n occurs. Since any maximum weight spanning
tree is a junction tree by Lemma 3  n must occur also in the clique ci. Hence n occurs in
the label of the edge from ci to c. Since the subtree is balanced  the new graph obtained by
adding the clique c and the edge with a label containing n is also balanced. Further  adding
all the other subtrees that do not contain n will not affect the balancing of n.

4. The node n occurs in one subtree but not in c.

Since there are n occurrences of n in any of the other subtrees  in c  or in the edge labels
between c and any of the subtrees  the balancing condition holds.

(cid:3)
This completes the induction step and consequently  the whole spanning tree is balanced.
Lemma 5 Assume T = (cid:104)V  EB(cid:105) is a spanning tree of the clique graph GC = (cid:104)V  E(cid:105) of a chordal
graph that satisﬁes the balancing condition. Then T is a maximum weight spanning tree of GC.

Proof: Let TM be one of the spanning trees of GC with the maximum weight w. By Lemma 4  this
maximum weight spanning tree is balanced. By Lemma 2  T has the same weight w as TM . Hence
(cid:3)
also T is a maximum weight spanning tree of GC.

Theorem 6 For any clique graph of a chordal graph  any of its subgraphs is a maximum weight
spanning tree if and only if it is a balanced acyclic subgraph.

4 Representation as Constraints

In this section we ﬁrst show how the structure learning problem of Markov networks is cast as a
constraint satisfaction problem  and then formalize it concretely in the language of propositional
logic  as directly supported by SMT solvers and easily translatable into conjunctive normal form as
used by SAT and MAXSAT solvers. In ASP slightly different rule-based formulations are used.
The learning problem is formalized as follows. The goal is to ﬁnd a balanced spanning tree (cf. Def-
inition 1) for a set C of cliques forming a Markov network and the set S of separators induced by
the tree structure. In addition  C and S are supposed to be optimal in the sense of (1)  i.e.  the overall
s∈S v(s) is maximized. The individual score v(c) for any set of

score v(C  S) =(cid:80)

c∈C v(c) −(cid:80)

nodes c describes how well it reﬂects the interdependencies of the variables in c in the data.
Deﬁnition 7 Let V be a set of nodes representing random variables and v : 2V → R a scoring
function. A solution to the Markov network learning problem is a set of cliques C = {c1  . . .   ck}
satisfying the following requirements viewed as abstract constraints:

2. Cliques in C are maximal  i.e. 

1. Every node is included in at least one of the chosen cliques in C  i.e. (cid:83)k
(b) for every c ⊆ V   if edges(c) ⊆(cid:83)
3. The graph (cid:104)V  E(cid:105) with the set of edges E =(cid:83)

where edges(c) = {{n  n(cid:48)} ⊆ c | n (cid:54)= n(cid:48)} is deﬁned for each c ⊆ V .
c∈C edges(c) is chordal.

(a) for every c  c(cid:48) ∈ C  if c ⊆ c(cid:48)  then c = c(cid:48); and

c(cid:48)∈C edges(c(cid:48))  then c ⊆ c(cid:48) for some c(cid:48) ∈ C

i=1 ci = V .

5

4. The set C has a balanced spanning tree labeled by a set of separators S = {s1  . . .   sl}.

Moreover  the solution is optimal if it maximizes the overall score v(C  S).

The encodings of basic graph properties (conditions 1 and 2 above) are presented Section 4.1. The
more complex properties (3 and 4) are addressed in Sections 4.2 and 4.3.

4.1 Graph Properties

We assume that clique candidates – which are the non-empty subsets of V – are indexed from 1 to
2|V |. We often identify a clique with its index. Each clique candidate c ⊆ V has an associated score
v(c). To encode the search space for Markov networks  we introduce  for every clique candidate
c  a propositional variable xc denoting that c is part of the learned network. We also introduce
propositional variables en m that represent edges {n  m} that are in at least one chosen clique.1
To formalize condition 1 of Deﬁnition 7  for every node n we have the constraint

xc1 ∨ ··· ∨ xck

(2)

where c1  . . .   ck are all cliques c with n ∈ c.
To satisfy the maximality condition 2(a)  we require that if a clique is chosen  then at least one edge
in each of its super-cliques is not chosen. We ﬁrst make the edges of the chosen cliques explicit by
the next constraint for all {n  m} ⊆ V and cliques c1  . . .   ck such that {n  m} ⊆ ci.

en m ↔ (xc1 ∨ ··· ∨ xck )

xc → (¬en1 n ∨ ··· ∨ ¬enk n)

(3)
Then for every clique candidate c = {n1  . . .   nk} and every node n ∈ V \c we have the constraint
(4)
where en1 n  . . .   enk n represent all additional edges that would turn c ∪ {n} into a clique. For
each pair of clique candidates c and c(cid:48) such that c ⊂ c(cid:48)  ¬xc ∨ ¬xc(cid:48) is a logical consequence of the
constraints (4). They are useful for strengthening the inferences made by SAT solvers.
For condition 2(b) we use propositional variables zc which mean that either c or one of its super-
cliques is chosen  and propositional variables wc which mean that all edges of c are chosen. For
2-element cliques c = {n1  n2} we have

wc ↔ en1 n2.

For larger cliques c we have

(6)
where c1  . . .   ck are all subcliques of c with one less node than c. Hence wc is true iff all edges of c
are chosen. If all edges of a clique are chosen  then the clique itself or one of its super-cliques must
be chosen. If c1  . . .   ck are all cliques that extend c by one node  this is encoded as follows.

wc ↔ wc1 ∧ ··· ∧ wck

(5)

(7)
(8)

wc → zc
zc ↔ (xc ∨ zc1 ∨ ··· ∨ zck )

4.2 Chordality

We use a straightforward encoding of the chordality condition (3) of Deﬁnition 7. The idea is to
generate constraints corresponding to every k ≥ 4 element subset S = {n1  . . .   nk} of V . Let
us consider all cycles these nodes could form in the graph (cid:104)V  E(cid:105) of condition 3 in Deﬁnition 7.
A cycle starts from a given node  goes through all other nodes  with (undirected) edges between
two consecutive nodes  and ends in the starting node. The number of constraints can be reduced
by two observations. First  the same cycle could be generated from different starting nodes  e.g. 
cycles n1  n2  n3  n4  n1 and n2  n3  n4  n1  n2 are the same. Second  generating the same cycle
in two opposite directions  as in n1  n2  n3  n4  n1 and n1  n4  n3  n2  n1  is unnecessary. To avoid

1As the edges are undirected  we limit to en m such that the ordering of n and m according to some ﬁxed

ordering is increasing  i.e.  n < m. Under this assumption  em n for n < m denotes en m.

6

redundant cycle constraints  we arbitrarily ﬁx the starting node and require that the index of the
second node in the cycle is lower than the index of the second last node. These restrictions guarantee
that every cycle associated with S is considered exactly once. Now  the chordality constraint says
that if there is an edge between every pair of consecutive nodes in n1  . . .   nk  n1  then there also
has to be an edge between at least one pair of two non-consecutive nodes. In the case k = 4  for
instance  this leads to formulas of the form

en1 n2 ∧ en2 n3 ∧ en3 n4 ∧ en4 n1 → en1 n3 ∨ en2 n4.

(9)
This encoding of chordality constraints is exponential in |V | and therefore not scalable to large
numbers of nodes. However  the datasets considered in Section 5 have only 6 or 8 variables  and in
these cases the exponentiality is not an issue.

4.3 Separators
Separators for pairs c and c(cid:48) of clique candidates can be formalized as propositional variables sc c(cid:48) 
meaning that c ∩ c(cid:48) is a separator and there is an edge in the spanning tree between c and c(cid:48) labeled
by c ∩ c(cid:48). The corresponding constraint is

sc c(cid:48) → xc ∧ xc(cid:48).

(10)
The lack of the converse implication formalizes the choice of the spanning tree  i.e.  sc c(cid:48) can be
false even if xc and xc(cid:48) are true. The remaining constraints on separators fall into two cases.
First  we have cardinality constraints encoding the balancing condition (cf. Section 3.1): each vari-
able occurs in the chosen cliques one more time than it occurs in the separators which label the
spanning tree. Cardinality constraints are natively supported by some constraint solvers  or they can
be reduced to Boolean constraints [23]. Second  the graph formed by the cliques with the separators
as edges must be acyclic. We encode this through an inductive deﬁnition of trees: repeatedly remove
leaf nodes  i.e.  nodes with at most one neighbor  until all nodes have been removed. When applying
this deﬁnition to a cyclic graph  some nodes will remain in the end. We deﬁne the leaf level for each
node. A node is a level 0 leaf iff it has 0 or 1 neighbors in the graph. A node is a level l + 1 leaf iff
all its neighbors except possibly one are level j ≤ l leaves. This deﬁnition is directly expressible as
Boolean constraints. A graph with m nodes is acyclic iff all its nodes are level (cid:98) m

2 (cid:99) leaves.

5 Experimental Evaluation

The constraints described in Section 4 can be alternatively expressed as MAXSAT  SMT  or ASP
problems. We have used respective solvers in computing optimal Markov networks for datasets from
the literature. The test runs were with an Intel Xeon E3-1230 CPU running at 3.20 GHz.

1. For the MAXSAT encodings  we tried out SAT4J (version 2.3.2) [24] and PWBO (version
2.2) [25]. The latter was run in its default conﬁguration as well as in the UB conﬁguration.

2. For SMT  we used the OPTIMATHSAT solver (version 5) [26].
3. For ASP  we used the CLASP (version 2.1.3) [27] and HCLASP (also v. 2.1.3) [28]
solvers. The latter allows declaratively specifying search heuristics. We also tried the
LP2NORMAL tool that reduces cardinality constraints to more basic constraints [29].

We consider two datasets  one containing risk factors in heart diseases and the other variables related
to economical behavior [18]  to be abbreviated by heart and econ in the sequel. For heart  the glob-
ally optimal network has been veriﬁed via (expensive) exhaustive enumeration. For econ  however 
exhaustive enumeration is impractical due to the extremely large search space  and consequently the
optimality of the Markov network found by stochastic search in [4] had been open until now. For
both datasets  we computed the respective score ﬁle that speciﬁes the score of each clique candidate 
i.e.  the log-value of its potential function  and the list of variables involved in that clique. The score
ﬁles were then translated to be run with the different solvers. The MAXSAT and ASP solvers only
support integer scores obtained by multiplying the original scores by 1000 and rounding. The SMT
solver OptiMathSAT used the original ﬂoating point scores. The results are given in Table 1.
The heart data involves 6 variables giving rise to 26 = 64 clique candidates in total and a search
space of 215 undirected networks of which a subset are decomposable. For instance  the ASP solver

7

OPTIMATHSAT
PWBO (default)
PWBO (UB)
SAT4J
LP2NORMAL+CLASP
CLASP
HCLASP

heart
74
158
63
28
111
5.6
1.6

econ
-
-
-
-
-
-
310 × 103

heart
econ
3930 kB
139 MB
3120 kB
130 MB
3120 kB
130 MB
3120 kB
130 MB
8120 kB 1060 MB
4.2 MB
197 kB
203 kB
4.2 MB

Table 1: Summary of results: Runtimes in seconds and sizes of solver input ﬁles

HCLASP traversed a considerably smaller search space that consisted of 26651 (partial) networks.
This illustrates the power of branch-and-bound type algorithms behind the solvers and their ability
to prune the search space. On the other hand  the econ dataset is based on 8 variables giving rise to
a much larger search space 228. We were able to solve this instance optimally with one solver only 
HCLASP  which allows for a more reﬁned control of the search heuristic: we forced HCLASP to try
cliques in an ascending order by size  with greatest cliques ﬁrst. This allowed us to ﬁnd the global
optimum in about 14 hours  after which 3 days is spent on the proof of optimality.

6 Conclusions

Boolean constraint methods appear not to have been earlier applied to learning of undirected Markov
networks. We have introduced a generic approach in which the learning problem is expressed in
terms of constraints on variables that determine the structure of the learned network. The related
problem of structure learning of Bayesian networks has been addressed by general-purpose com-
binatorial search methods  including MAXSAT [30] and a constraint-programming solver with a
linear-programming solver as a subprocedure [31  32]. We introduced explicit translations of the
generic constraints to the languages of MAXSAT  SMT and ASP  and demonstrated their use through
existing solver technology. Our method thus opens up a novel venue of research to further develop
and optimize the use of such technology for network learning. A wide variety of possibilities does
exist also for using these methods in combination with stochastic or heuristic search.

References
[1] J. N. Darroch  Steffen L. Lauritzen  and T. P. Speed. Markov ﬁelds and log-linear interaction

models for contingency tables. The Annals of Statistics  8:522–539  1980.

[2] Steffen L. Lauritzen and Nanny Wermuth. Graphical models for associations between vari-
ables  some of which are qualitative and some quantitative. The Annals of Statistics  17:31–57 
1989.

[3] Jukka Corander. Bayesian graphical model determination using decision theory. Journal of

Multivariate Analysis  85:253–266  2003.

[4] Jukka Corander  Magnus Ekdahl  and Timo Koski. Parallel interacting MCMC for learning of

topologies of graphical models. Data Mining and Knowledge Discovery  17:431–456  2008.

[5] Petros Dellaportas and Jonathan J. Forster. Markov chain Monte Carlo model determination

for hierarchical and graphical log-linear models. Biometrika  86:615–633  1999.

[6] Paolo Giudici and Robert Castello. Improving Markov chain Monte Carlo model search for

data mining. Machine Learning  50:127–158  2003.

[7] Paolo Giudici and Peter J. Green. Decomposable graphical Gaussian model determination.

Biometrika  86:785–801  1999.

[8] Mikko Koivisto and Kismat Sood. Exact Bayesian structure discovery in Bayesian networks.

Journal of Machine Learning Research  5:549–573  2004.

[9] David Madigan and Adrian E. Raftery. Model selection and accounting for model uncertainty
in graphical models using Occam’s window. Journal of the American Statistical Association 
89:1535–1546  1994.

8

[10] Stephen Della Pietra  Vincent Della Pietra  and John Lafferty. Inducing features of random

ﬁelds. IEEE Trans. on Pattern Analysis and Machine Intelligence  19(4):380–393  1997.

[11] Su-In Lee  Varun Ganapathi  and Daphne Koller. Efﬁcient structure learning of Markov net-
In Advances in Neural Information Processing Systems 19 

works using L1-regularization.
pages 817–824. MIT Press  2006.

[12] M. Schmidt  A. Niculescu-Mizil  and K. Murphy. Learning graphical model structure using
L1-regularization paths. In Proceedings of the National Conference on Artiﬁcial Intelligence 
page 1278. AAAI Press / MIT Press  2007.

[13] Holger H¨oﬂing and Robert Tibshirani. Estimation of sparse binary pairwise Markov networks

using pseudo-likelihoods. Journal of Machine Learning Research  10:883–906  2009.

[14] Armin Biere  Marijn J. H. Heule  Hans van Maaren  and Toby Walsh  editors. Handbook of

Satisﬁability. IOS Press  2009.

[15] Chu Min Li and Felip Many`a. MaxSAT  Hard and Soft Constraints  chapter 19  pages 613–631.

In Biere et al. [14]  2009.

[16] Clark Barrett  Roberto Sebastiani  Sanjit A. Seshia  and Cesare Tinelli. Satisﬁability Modulo

Theories  chapter 26  pages 825–885. In Biere et al. [14]  2009.

[17] Gerhard Brewka  Thomas Eiter  and Miroslaw Truszczynski. Answer set programming at a

glance. Commun. ACM  54(12):92–103  2011.

[18] Joe Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley Publishing  1990.
[19] Martin C. Golumbic. Algorithmic Graph Theory and Perfect Graphs. Academic Press  1980.
[20] Ronald L. Graham and Pavol Hell. On the history of the minimum spanning tree problem.

Annals of the History of Computing  7(1):43–57  1985.

[21] Yukio Shibata. On the tree representation of chordal graphs.

12(3):421–428  1988.

Journal of Graph Theory 

[22] Finn V. Jensen and Frank Jensen. Optimal junction trees. In Proceedings of the Tenth Confer-

ence on Uncertainty in Artiﬁcial Intelligence (UAI-94)  pages 360–366  1994.

[23] Carsten Sinz. Towards an optimal CNF encoding of Boolean cardinality constraints. In Prin-
ciples and Practice of Constraint Programming – CP 2005  number 3709 in Lecture Notes in
Computer Science  pages 827–831. Springer-Verlag  2005.

[24] Daniel Le Berre and Anne Parrain. The Sat4j library  release 2.2 system description. Journal

on Satisﬁability  Boolean Modeling and Computation  7:59–64  2010.

[25] Ruben Martins  Vasco Manquinho  and Inˆes Lynce. Parallel search for maximum satisﬁability.

AI Communications  25:75–95  2012.

[26] Roberto Sebastiani and Silvia Tomasi. Optimization in SMT with LA(Q) cost functions. In

Automated Reasoning  volume 7364 of LNCS  pages 484–498. Springer-Verlag  2012.

[27] Martin Gebser  Benjamin Kaufmann  and Torsten Schaub. Conﬂict-driven answer set solving:

From theory to practice. Artif. Intell.  187:52–89  2012.

[28] Martin Gebser  Benjamin Kaufmann  Ram´on Otero  Javier Romero  Torsten. Schaub  and
In Proceedings of

Philipp Wanko. Domain-speciﬁc heuristics in answer set programming.
the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence. AAAI Press  2013.

[29] Tomi Janhunen and Ilkka Niemel¨a. Compact translations of non-disjunctive answer set pro-
grams to propositional clauses. In Gelfond Festschrift  Vol. 6565 of LNCS  pages 111–130.
Springer-Verlag  2011.

[30] James Cussens. Bayesian network learning by compiling to weighted MAX-SAT. In Proceed-

ings of the Conference on Uncertainty in Artiﬁcial Intelligence  pages 105–112  2008.

[31] James Cussens. Bayesian network learning with cutting planes. In Proceedings of the Twenty-
Seventh Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-11) 
pages 153–160. AUAI Press  2011.

[32] Mark Bartlett and James Cussens. Advances in Bayesian network learning using integer pro-
In Proceedings of the 29th Conference on Uncertainty in Artiﬁcial Intelligence

gramming.
(UAI 2013)  pages 182–191. AUAI Press  2013.

9

,Jukka Corander
Tomi Janhunen
Jussi Rintanen
Henrik Nyman
Johan Pensar
Karin Knudson
Jacob Yates
Alexander Huk
Jonathan Pillow