2017,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes,In sequential decision making  it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However  typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper  we present a probabilistic model  Q-LDA  to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm.  We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes  but also obtains state-of-the-art rewards in these games.,Q-LDA: Uncovering Latent Patterns in Text-based

Sequential Decision Processes

Jianshu Chen⇤  Chong Wang†  Lin Xiao⇤  Ji He‡  Lihong Li† and Li Deng‡

⇤Microsoft Research  Redmond  WA  USA

{jianshuc lin.xiao}@microsoft.com

†Google Inc.  Kirkland  WA  USA⇤
{chongw lihong}@google.com
‡Citadel LLC  Seattle/Chicago  USA
{Ji.He Li.Deng}@citadel.com

Abstract

In sequential decision making  it is often important and useful for end users to
understand the underlying patterns or causes that lead to the corresponding deci-
sions. However  typical deep reinforcement learning algorithms seldom provide
such information due to their black-box nature. In this paper  we present a proba-
bilistic model  Q-LDA  to uncover latent patterns in text-based sequential decision
processes. The model can be understood as a variant of latent topic models that
are tailored to maximize total rewards; we further draw an interesting connection
between an approximate maximum-likelihood estimation of Q-LDA and the cel-
ebrated Q-learning algorithm. We demonstrate in the text-game domain that our
proposed method not only provides a viable mechanism to uncover latent patterns
in decision processes  but also obtains state-of-the-art rewards in these games.

1

Introduction

Reinforcement learning [21] plays an important role in solving sequential decision making problems 
and has seen considerable successes in many applications [16  18  20]. With these methods  however 
it is often difﬁcult to understand or examine the underlying patterns or causes that lead to the sequence
of decisions. Being more interpretable to end users can provide more insights to the problem itself
and be potentially useful for downstream applications based on these results [5].
To investigate new approaches to uncovering underlying patterns of a text-based sequential decision
process  we use text games (also known as interactive ﬁctions) [11  19] as the experimental domain.
Speciﬁcally  we focus on choice-based and hypertext-based games studied in the literature [11] 
where both the action space and the state space are characterized in natural languages. At each time
step  the decision maker (i.e.  agent) observes one text document (i.e.  observation text) that describes
the current observation of the game environment  and several text documents (i.e.  action texts) that
characterize different possible actions that can be taken. Based on the history of these observations 
the agent selects one of the provided actions and the game transits to a new state with an immediate
reward. This game continues until the agent reaches a ﬁnal state and receives a terminal reward.
In this paper  we present a probabilistic model called Q-LDA that is tailored to maximize total
rewards in a decision process. Specially  observation texts and action texts are characterized by two
separate topic models  which are variants of latent Dirichlet allocation (LDA) [4]. In each topic
model  topic proportions are chained over time to model the dependencies for actions or states. And

⇤The work was done while Chong Wang  Ji He  Lihong Li and Li Deng were at Microsoft Research.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

these proportions are partially responsible for generating the immediate/terminal rewards. We also
show an interesting connection between the maximum-likelihood parameter estimation of the model
and the Q-learning algorithm [22  18]. We empirically demonstrate that our proposed method not
only provides a viable mechanism to uncover latent patterns in decision processes  but also obtains
state-of-the-art performance in these text games.

Contribution. The main contribution of this paper is to seamlessly integrate topic modeling with
Q-learning to uncover the latent patterns and interpretable causes in text-based sequential decision-
making processes. Contemporary deep reinforcement learning models and algorithms can seldom
provide such information due to their black-box nature. To the best of our knowledge  there is no
prior work that can achieve this and learn the topic model in an end-to-end fashion to maximize the
long-term reward.

Related work. Q-LDA uses variants of LDA to capture observation and action texts in text-based
decision processes. In this model  the dependence of immediate reward on the topic proportions
is similar to supervised topic models [3]  and the chaining of topic proportions over time to model
long-term dependencies on previous actions and observations is similar to dynamic topic models [6].
The novelty in our approach is that the model is estimated in a way that aims to maximize long-term
reward  thus producing near-optimal policies; hence it can also be viewed as a topic-model-based
reinforcement-learning algorithm. Furthermore  we show an interesting connection to the DQN
variant of Q-learning [18]. The text-game setup used in our experiment is most similar to previous
work [11] in that both observations and actions are described by natural languages  leading to
challenges in both representation and learning. The main difference from that previous work is that
those authors treat observation-texts as Markovian states. In contrast  our model is more general 
capturing both partial observability and long-term dependence on observations that are common
in many text-based decision processes such as dialogues. Finally  the choice of reward function in
Q-LDA share similarity with that in Gaussian process temporal difference methods [9].

Organization. Section 2 describes the details of our probabilistic model  and draws a connection
to the Q-learning algorithm. Section 3 presents an end-to-end learning algorithm that is based on
mirror descent back-propagation. Section 4 demonstrates the empirical performance of our model 
and we conclude with discussions and future work in Section 5.

2 A Probabilistic Model for Text-based Sequential Decision Processes

In this section  we ﬁrst describe text games as an example of sequential decision processes. Then  we
describe our probabilistic model  and relate it to a variant of Q-learning.

2.1 Sequential decision making in text games
Text games are an episodic task that proceeds in discrete time steps t 2{ 1  . . .   T}  where the length
T may vary across different episodes. At time step t  the agent receives a text document of N words
n=1.2 We call these words
describing the current observation of the environment: wS
observation text. The agent also receives At text documents  each of which describes a possible
t   {wa
n=1 with a 2{ 1  . . .   At}  where At
action that the agent can take. We denote them by wa
is the number of feasible actions and it could vary over time. We call these texts action texts. After the
agent takes one of the provided actions  the environment transits to time t + 1 with a new state and an
immediate reward rt; both dynamics and reward generation may be stochastic and unknown. The new
state then reveals a new observation text wS
t+1 for a 2{ 1  . . .   At+1}.
The transition continues until the end of the game at step T when the agent receives a terminal reward
rT . The reward rT depends on the ending of the story in the text game: a good ending leads to a large
positive reward  while bad endings negative rewards.
The goal of the agent is to maximize its cumulative reward by acting optimally in the environment.
1:t : 8a}  previous actions
1:t  all action texts wA
At step t  given all observation texts wS
a1:t1 and rewards r1:t1  the agent is to ﬁnd a policy  ⇡(at|wS
1:t  a1:t1  r1:t1)  a conditional

t   {wS
t n}N

t+1 and several action texts wa

t n}N

1:t   {wa
1:t  wA

2For notation simplicity  we assume all texts have the same length N.

2

A

A

↵A
t

…

rt

↵S
t

S

S

N

wa
t n

za
t n

|At|

✓a
t

at

✓S
t

zS
t n

wS
t n

N

A

A

N

wa

t+1 n

za
t+1 n

|At+1|
✓a
t+1

↵A

t+1

rt+1

at+1

↵S

t+1

✓S
t+1

S

S

zS
t+1 n

wS

t+1 n

N

…

Figure 1: Graphical model representation for the studied sequential decision process. The bottom
section shows the observation topic models  which share the same topics in S  but the topic
distributions ✓S
t changes with time t. The top section shows the action topic models  sharing the
t for each a 2 At. The middle
same action topics in A  but with time varying topic distribution ✓a
section shows the dependence of variables between consecutive time steps. There are no plates for
the observation text (bottom part of the ﬁgure) because there is only one observation text document
at each time step. We follow the standard notation for graphical models by using shaded circles as
observables. Since the topic distributions ✓S
t (except
1 ) are not observable  we need to use their MAP estimate to make end-to-end learning
1 and ↵A
↵S
feasible; see Section 3 for details. The ﬁgure characterizes the general case where rewards appear at
each time step  while in our experiments the (non-zero) rewards only appear at the end of the games.

t and the Dirichlet parameters ↵S

t and ↵A

t and ✓a

probability of selecting action at  that maximizes the expected long-term reward E{PT
⌧ =t ⌧tr⌧} 
where  2 (0  1) is a discount factor. In this paper  for simplicity of exposition  we focus on problems
where the reward is nonzero only in the ﬁnal step T . While our algorithm can be generalized to the
general case (with greater complexity)  this special case is an important case of RL (e.g.  [20]). As a
result  the policy is independent of r1:t1 and its form is simpliﬁed to ⇡(at|wS
The problem setup is similar to previous work [11] in that both observations and actions are described
by natural languages. For actions described by natural languages  the action space is inherently
discrete and large due to the exponential complexity with respect to sentence length. This is
different from most reinforcement learning problems where the action spaces are either small
or continuous. Here  we take a probabilistic modeling approach to this challenge: the observed
variables—observation texts  action texts  selected actions  and rewards—are assumed to be generated
from a probabilistic latent variable model. By examining these latent variables  we aim to uncover
the underlying patterns that lead to the sequence of the decisions. We then show how the model is
related to Q-learning  so that estimation of the model leads to reward maximization.

1:t  a1:t1).

1:t  wA

2.2 The Q-LDA model

The graphical representation of our model  Q-LDA  is depicted in Figure 1. It has two instances of
topic models  one for observation texts and the other for action texts. The basic idea is to chain the
topic proportions (✓s in the ﬁgure) in a way such that they can inﬂuence the topic proportions in the
future  thus capturing long-term effects of actions. Details of the generative models are as follows.
For the observation topic model  we use the columns of S ⇠ Dir(S)3 to denote the topics for
the observation texts. For the action topic model  we use the columns of A ⇠ Dir(A) to denote
the topics for the action texts. We assume these topics do not change over time. Given the initial
topic proportion Dirichlet parameters—↵S
1 for observation and action texts respectively—the
Q-LDA proceeds sequentially from t = 1 to T as follows (see Figure 1 for all latent variables).

1 and ↵A

3S is a word-by-topic matrix. Each column is drawn from a Dirichlet distribution with hyper-parameter S 

representing the word-emission probabilities of the corresponding topic. A is similarly deﬁned.

3

1. Draw observation text wS

t as follows 

(a) Draw observation topic proportions ✓S
(b) Draw all words for the observation text wS

t ).
t ⇠ Dir(↵S

denotes the standard LDA generative process given its topic proportion ✓S
S [4]. The latent variable zS

t n indicates the topic for the word wS

t n.

t ⇠ LDA(wS

t |✓S

t   S)  where LDA(·)
t and topics

t as follows 
2. For a = 1  ...  At  draw action text wa
(a) Draw action topic proportions ✓a
t ).
t ⇠ Dir(↵A
(b) Draw all words for the a-th action text using wa
t n indicates the topic for the word wa

variable za

t ⇠ LDA(wa
t n.

t |✓a

t   A)  where the latent

3. Draw the action: at ⇠ ⇡b(at|wS

1:t  a1:t1)  where ⇡b is an exploration policy for data
collection. It could be chosen in different ways  as discussed in the experiment Section 4.
After model learning is ﬁnished  a greedy policy may be used instead (c.f.  Section 3).

4. The immediate reward rt is generated according to a Gaussian distribution with mean

1:t  wA

function µr(✓S

t  ✓ at

t   U ) and variance 2
r:

rt ⇠N µr(✓S

t   U )  2

t  ✓ at
t   U ) and its parameter U to the next section 

(1)

r .

Here  we defer the deﬁnitions of µr(✓S
where we draw a connection between likelihood-based learning and Q-learning.

t  ✓ at

↵S

t + ↵S

t + WSA✓at

t+1 = WSS✓S

5. Compute the topic proportions Dirichlet parameters for the next time step t + 1 as
t +↵A

(2)
where (x)   max{x  ✏} with ✏ being a small positive number (e.g.  106)  at is the action
selected by the agent at time t  and {WSS  WSA  WAS  WAA} are the model parameters to
be learned. Note that  besides ✓S
a=1 that will inﬂuence
t+1 and ↵A
t   i.e.  the one corresponding to the chosen action at. Furthermore  since
↵S
t and ✓at
✓S
t+1
are (implicitly) chained over time via ✓S

t+1 = WAS✓S
t   the only topic proportions from {✓a

t are generated according to Dir(↵S

t ) and Dir(↵A
t and ✓at
t

1  ↵ A

t )  respectively  ↵S

(c.f. Figure 1).

t +WAA✓at

1  

t+1 and ↵A

t+1 is ✓at

t }At

This generative process deﬁnes a joint distribution p(·) among all random variables depicted in
Figure 1. Running this generative process—step 1 to 5 above for T steps until the game ends—
produces one episode of the game. Now suppose we already have M episodes. In this paper  we
choose to directly learn the conditional distribution of the rewards given other observations. By
learning the model in a discriminative manner [2  7  12  15  23]  we hope to make better predictions
of the rewards for different actions  from which the agent could obtain the best policy for taking
actions. This can be obtained by applying Bayes rule to the joint distribution deﬁned by the generative
process. Let ⇥ denote all model parameters: ⇥= {S  A  U  WSS  WSA  WAS  WAA}. We have
the following loss function

⇥ ( ln p(⇥) 

min

MXi=1

ln pr1:Ti|wS

1:Ti  wA

1:Ti  a1:Ti  ⇥)  

(3)

where p(⇥) denotes a prior distribution of the model parameters (e.g.  Dirichlet parameters over
S and A)  and Ti denotes the length of the i-th episode. Let KS and KA denote the number of
topics for the observation texts and action texts  and let VS and VA denote the vocabulary sizes for
the observation texts and action texts  respectively. Then  the total number of learnable parameters
for Q-LDA is: VS ⇥ KS + VA ⇥ KA + KA ⇥ KS + (KS + KA)2.
We note that a good model learned through Eq. (3) may predict the values of rewards well  but might
not imply the best policy for the game. Next  we show by deﬁning the appropriate mean function
for the rewards  µr(✓S
t   U )  we can achieve both. This closely resembles Q-learning [21  22] 
allowing us to effectively learn the policy in an iterative fashion.

t  ✓ at

2.3 From Q-LDA to Q-learning

Before relating Q-LDA to Q-learning  we ﬁrst give a brief introduction to the latter. Q-learning [22 
18] is a reinforcement learning algorithm for ﬁnding an optimal policy in a Markov decision process
(MDP) described by (S A P  r  )  where S is a state space  A is an action space  and  2 (0  1)
is a discount factor. Furthermore  P deﬁnes a transition probability p(s0|s  a) for going to the next

4

state s0 2S from the current state s 2S after taking action a 2A   and r(s  a) is the immediate
reward corresponding to this transition. A policy ⇡(a|s) in an MDP is deﬁned to be the probability
of taking action a at state s. Let st and at be the state and action at time t  and let rt = r(st  at) be
the immediate reward at time t. An optimal policy is the one that maximizes the expected long-term
reward E{P+1t=1 t1rt}. Q-learning seeks to ﬁnd the optimal policy by estimating the Q-function 

Q(s  a)  deﬁned as the expected long-term discounted reward for taking action a at state s and then
following an optimal policy thereafter. It satisﬁes the Bellman equation [21]

Q(s  a) = E{r(s  a) +  · max

Q(s0  b)|s  a}  
and directly gives the optimal action for any state s: arg maxa Q(s  a).
Q-learning solves for Q(s  a) iteratively based on observed state transitions. The basic Q-learning [22]
requires storing and updating the values of Q(s  a) for all state–action pairs in S⇥A   which is
not practical when S and A are large. This is especially true in our text games  where they can be
exponentially large. Hence  Q(s  a) is usually approximated by a parametric function Q✓(s  a) (e.g. 
neural networks [18])  in which case the model parameter ✓ is updated by:

(4)

b

✓ ✓ + ⌘ ·r ✓Q✓ · (dt  Q✓(st  at))  

(5)
where dt   rt +  · maxa0 Q✓0(st+1  a0) if st nonterminal and dt   rt otherwise  and ✓0 denotes
a delayed version of the model parameter updated periodically [18]. The update rule (5) may be
understood as applying stochastic gradient descent (SGD) to a regression loss function J(✓)  
E[dt  Q✓(s  a)]2. Thus  dt is the target  computed from rt and Q✓0  for the prediction Q✓(st  at).
We are now ready to deﬁne the mean reward function µr in Q-LDA. First  we model the Q-function
t   where U is the same parameter as the one in (1).4 This is different from
by Q(✓S
typical deep RL approaches  where black-box models like neural networks are used. In order to
connect our probabilistic model to Q-learning  we deﬁne the mean reward function as follows 

t ) = (✓a

t )T U✓ S

t  ✓ a

t )   · E⇥ max

b

Q(✓S

t  ✓ at

µr(✓S

t  ✓ at

t+1 ✓ b

t and ✓at

t+1)|✓S

t  ✓ at
t   U ) = Q(✓S
t and ✓at
since the second term in the above expression is
Note that µr remains as a function of ✓S
t
a conditional expectation given ✓S
t . The deﬁnition of the mean reward function in Eq. (6)
has a strong relationship with the Bellman equation (4) in Q-learning; it relates the long-term reward
t ) to the mean immediate reward µr in the same manner as the Bellman equation (4). To
Q(✓S
see this  we move the second term on the right-hand side of (6) to the left  and make the identiﬁcation
that µr corresponds to E{r(s  a)} since both of them represent the mean immediate reward. The
resulting equation share a same form as the Bellman equation (4). With the mean function µr deﬁned
above  we show in Appendix B that the loss function (3) can be approximated by the one below using
the maximum a posteriori (MAP) estimate of ✓S

(denoted as ˆ✓S

t   respectively):

t and ˆ✓at

t and ✓at
t

t  ✓ at

(6)

t ⇤

min

⇥ n  ln p(S|S)  ln p(A|A) +

MXi=1

TiXt=1

1
22

r hdt  Q(ˆ✓S

t   ˆ✓at

t )i2o

(7)

t+1  ˆ✓b

where dt = rt +  maxb Q(ˆ✓S
t+1) for t < Ti and dt = rt for t = Ti. Observe that the ﬁrst two
terms in (7) are regularization terms coming from the Dirichlet prior over S and A  and the third
term shares a similar form as the cost J(✓) in Q-learning; it can also be interpreted as a regression
problem for estimating the Q-function  where the target dt is constructed in a similar manner as
Q-learning. Therefore  optimizing the discriminative objective (3) leads to a variant of Q-learning.
After learning is ﬁnished  we can obtain the greedy policy by taking the action that maximizes the
Q-function estimate in any given state.
We also note that we have used the MAP estimates of ✓S
t due to the intractable marginalization
of the latent variables [14]. Other more advanced approximation techniques  such as Markov Chain
Monte Carlo (MCMC) [1] and variational inference [13] can also be used  and we leave these
explorations as future work.

t and ✓at

3 End-to-end Learning by Mirror Descent Back Propagation

4The intuition of choosing Q(· ·) to be this form is that we want ✓S

action (large Q-value)  and to be misaligned with the ✓a
of U allows the number and the meaning of topics for the observations and actions to be different.

t of the correct
t of the wrong actions (small Q-value). The introduction

t to be aligned with ✓a

5

Algorithm 1 The training algorithm by mirror descent back propagation
1: Input: D (number of experience replays)  J (number of SGD updates)  and learning rate.
2: Randomly initialize the model parameters.
3: for m = 1  . . .   D do
4:

Interact with the environment using a behavior policy ⇡m
episodes of data {wS
for j = 1  . . .   J do

b (at|xS
i=1 and add them to D.

1:Ti  a1:Ti  r1:Ti}M

1:Ti  wA

1:t  xA

1:t  a1:t1) to collect M

Randomly sample an episode from D.
For the sampled episode  compute ˆ✓S
1  . . .   Ti according to Algorithm 2.
For the sampled episode  compute the stochastic gradients of (7) with respect to ⇥ using
back propagation through the computational graph deﬁned in Algorithm 2.
Update {U  WSS  WSA  WAS  WAA} by stochastic gradient descent and update {S  A}
using stochastic mirror descent.

t ) with a = 1  . . .   At and t =

t and Q(ˆ✓S

t   ˆ✓a

t   ˆ✓a

5:
6:
7:

8:

9:

end for

10:
11: end for

t : a = 1  . . .   At} and at  for all t = 1  . . .   Ti.
1 = ↵A
1

1   ↵A

t   {xa
1 and ˆ↵A

1   L    xS
1 = ↵S

Algorithm 2 The recursive MAP inference for one episode
1: Input: ↵S
2: Initialization: ˆ↵S
3: for t = 1  . . .   Ti do
4:

t  exp⇣hT
t by repeating ˆ✓S
t / 1  where C is a normalization factor.
t for each a = 1  . . .   At by repeating ˆ✓a

Compute ˆ✓S
ization ˆ✓S
Compute ˆ✓a
for L times with initialization ˆ✓a
t+1 from ˆ✓S
Compute ˆ↵S
Compute the Q-values: Q(ˆ✓S
t   ˆ✓a

t+1 and ˆ↵A

t 1

ˆ✓S

5:

C

S

6:
7:
8: end for

ˆ✓a

t 1

t  exp⇣hT
t / 1  where C is a normalization factor.
t and ˆ✓at
t ) = (ˆ✓a

t according to (11).
t )T U ˆ✓S

t for a = 1  . . .   At.

C

xS
t

S ˆ✓S
t

+ ˆ↵S
t 1
ˆ✓S
t

i⌘ for L times with initial-
i⌘

+ ˆ↵A
t 1
ˆ✓a
t

A ˆ✓a
t

xa
t

A

In this section  we develop an end-to-end learning algorithm for Q-LDA  by minimizing the loss
function given in (7). As shown in the previous section  solving (7) leads to a variant of Q-learning 
thus our algorithm could be viewed as a reinforcement-learning algorithm for the proposed model.
We consider learning our model with experience replay [17]  a widely used technique in recent state-
of-the-art systems [18]. Speciﬁcally  the learning process consists of multiple stages  and at each stage 
1:t  a1:t1) to
the agent interacts with the environment using a ﬁxed exploration policy ⇡b(at|xS
collect M episodes of data {wS
i=1 and saves them into a replay memory D.
(We will discuss the choice of ⇡b in section 4.) Under the assumption of the generative model Q-LDA 
our objective is to update our estimates of the model parameters in ⇥ using D; the updating process
may take several randomized passes over the data in D. A stage of such learning process is called one
replay. Once a replay is done  we let the agent use a new behavior policy ⇡0b to collect more episodes 
add them to D  and continue to update ⇥ from the augmented D. This process repeats for multiple
stages  and the model parameters learned from the previous stage will be used as the initialization
for the next stage. Therefore  we can focus on learning at a single stage  which was formulated in
Section 2 as one of solving the optimization problem (7). Note that the objective (7) is a function of
the MAP estimates of ✓S
t and
then introduce our learning algorithm for ⇥.

t . Therefore  we start with a recursion for computing ˆ✓S

1:Ti  a1:Ti  r1:Ti}M

t and ˆ✓at

t and ✓at

1:Ti  wA

1:t  xA

3.1 Recursive MAP inference by mirror descent

The MAP estimates  ˆ✓S

t and ˆ✓a

t and ✓a

t are deﬁned as

t   for the topic proportions ✓S
t |wS

t ) = arg max
✓S
t  ✓a
t

t  ✓ a

p(✓S

(ˆ✓S

t   ˆ✓a

1:t  wA

1:t  a1:t1)

(8)

6

Solving for the exact solution is  however  intractable. We instead develop an approximate algorithm
that recursively estimate ˆ✓S
t and ˆ✓a
t . To develop the algorithm  we rely on the following result  whose
proof is deferred to Appendix A.
Proposition 1. The MAP estimates in (8) could be approximated by recursively solving the problems:
(9)

ˆ✓S
t = arg max
✓S
t

ˆ✓a
t = arg max
✓a
t

⇥ln p(xS
t   S) + ln p✓S
t⇤
t |ˆ↵S
t |✓S
⇥ln p(xa
t   A) + ln p✓a
t⇤  
t |✓a
t |ˆ↵A

a 2{ 1  . . .   At}  

(10)

t and xa

where xS
wa
values for the next t + 1 time step according to

t are the bag-of-words vectors for the observation text wS
1 and ˆ↵A

t   respectively. To compute ˆ↵S

t   we begin with ˆ↵S

t and ˆ↵A

1 = ↵S

t and the a-th action text
1 and update their
1 = ↵A

ˆ↵S

t+1 = ⇣WSS ˆ✓S

t +WSA ˆ✓at

t +↵S

1⌘  

ˆ↵A

t+1 = ⇣WAS ˆ✓S

t +WAA ˆ✓at

t +↵A

(11)

1⌘

t and ˆ✓a

Note from (9)–(10) that  for given ˆ✓S
t now becomes At+1 decoupled
sub-problems  each of which has the same form as the MAP inference problem of Chen et al. [8].
Therefore  we solve each sub-problem in (9)–(10) using their mirror descent inference algorithm  and
then use (11) to compute the Dirichlet parameters at the next time step. The overall MAP inference
procedure is summarized in Algorithm 2. We further remark that  after obtaining ˆ✓S
t   the
Q-value for the t step is readily estimated by:

t   the solution of ✓S

t and ˆ✓a

t and ✓a

t  ✓ a

E⇥Q(✓S

1:t  wA

t )|wS

(12)
where we approximate the conditional expectation using the MAP estimates. After learning is ﬁnished 
the agent may extract a greedy policy for any state s by taking the action arg maxa Q(ˆ✓S  ˆ✓a). It
is known that if the learned Q-function is closed to the true Q-function  such a greedy policy is
near-optimal [21].

a 2{ 1  . . .   At}  

1:t  a1:t1⇤ ⇡ Q(ˆ✓S

t   ˆ✓a
t ) 

3.2 End-to-end learning by backpropagation

The training loss (7) for each learning stage has the form of a ﬁnite sum over M episodes. Each term
inside the summation depends on ˆ✓S
t   which in turn depend on all the model parameters in ⇥
via the computational graph deﬁned by Algorithm 2 (see Appendix E for a diagram of the graph).
Therefore  we can learn the model parameters in ⇥ by sampling an episode in the data  computing
the corresponding stochastic gradient in (7) by back-propagation on the computational graph given
in Algorithm 2  and updating ⇥ by stochastic gradient/mirror descent. More details are found in
Algorithm 1  and Appendix E.4 gives the gradient formulas.

t and ˆ✓at

4 Experiments

In this section  we use two text games from [11] to evaluate our proposed model and demonstrate the
idea of interpreting the decision making processes: (i) “Saving John” and (ii) “Machine of Death”
(see Appendix C for a brief introduction of the two games).5 The action spaces of both games are
deﬁned by natural languages and the feasible actions change over time  which is a setting that Q-LDA
is designed for. We choose to use the same experiment setup as [11] in order to have a fair comparison
with their results. For example  at each m-th experience-replay learning (see Algorithm 1)  we use the
softmax action selection rule [21  pp.30–31] as the exploration policy to collect data (see Appendix
E.3 for more details). We collect M = 200 episodes of data (about 3K time steps in “Saving John”
and 16K in “Machine of Death”) at each of D = 20 experience replays  which amounts to a total
of 4  000 episodes. At each experience replay  we update the model with 10 epochs before the next
replay. Appendix E provides additional experimental details.
We ﬁrst evaluate the performance of the proposed Q-LDA model by the long-term rewards it receives
when applied to the two text games. Similar to [11]  we repeat our experiments for ﬁve times with
different random initializations. Table 1 summarize the means and standard deviations of the rewards

5The simulators are obtained from https://github.com/jvking/text-games

7

Table 1: The average rewards (higher is better) and standard deviations of different models on the two
tasks. For DRRN and MA-DQN  the number of topics becomes the number of hidden units per layer.

DRRN (1-layer) DRRN (2-layer) MA-DQN (2-layer)

Tasks
Saving
John

Machine
of Death

# topics

20
50
100
20
50
100

Q-LDA
18.8 (0.3)
18.6 (0.6)
19.1 (0.6)
19.9 (0.8)
18.7 (2.1)
17.5 (2.4)

17.1 (0.6)
18.3 (0.2)
18.2 (0.2)
7.2 (1.5)
8.4 (1.3)
8.7 (0.9)

18.4 (0.1)
18.5 (0.3)
18.7 (0.4)
9.2 (2.1)
10.7 (2.7)
11.2 (0.6)

4.9 (3.2)
9.0 (3.2)
7.1 (3.1)
2.8 (0.9)
4.3 (0.9)
5.2 (1.2)

on the two games. We include the results of Deep Reinforcement Relevance Network (DRRN)
proposed in [11] with different hidden layers. In [11]  there are several variants of DQN (deep
Q-networks) baselines  among which MA-DQN (max-action DQN) is the best performing one. We
therefore only include the results of MA-DQN. Table 1 shows that Q-LDA outperforms all other
approaches on both tasks  especially “Machine of Death”  where Q-LDA even beats the DRRN
models by a large margin. The gain of Q-LDA on “Saving John” is smaller  as both Q-LDA and
DRRN are approaching the upper bound of the reward  which is 20. “Machine of Death” was believed
to be a more difﬁcult task due to its stochastic nature and larger state and action spaces [11]  where the
upper bound on the reward is 30. (See Tables 4–5 for the deﬁnition of the rewards for different story
endings.) Therefore  Q-LDA gets much closer to the upper bound than any other method  although
there may still be room for improvement. Finally  our experiments follow the standard online RL
setup: after a model is updated based on the data observed so far  it is tested on newly generated
episodes. Therefore  the numbers reported in Table 1 are not evaluated on the training dataset  so
they truthfully reﬂect the actual average reward of the learned models.
We now proceed to demonstrate the analysis of the latent pattern of the decision making process
using one example episode of “Machine of Death”. In this episode  the game starts with the player
wandering in a shopping mall  after the peak hour ended. The player approaches a machine that
prints a death card after inserting a coin. The death card hints on how the player will die in future. In
one of the story development  the player’s death is related to a man called Bon Jovi. The player is
so scared that he tries to combat with a cardboard standee of Bon Jovi. He reveals his concern to a
friend named Rachel  and with her help he ﬁnally overcomes his fear and maintains his friendship.
This episode reaches a good ending and receives the highest possible reward of 30 in this game.
In Figure 2  we show the evolution of the topic proportions for the four most active topics (shown in
Table 2)6 for both the observation texts and the selected actions’ texts. We note from Figure 2 that the
most dominant observation topic and action topic at beginning of the episode are “wander at mall”
and “action at mall”  respectively  which is not surprising since the episode starts at a mall scenario.
The topics related to “mall” quickly dies off after the player starts the death machine. Afterwards  the
most salient observation topic becomes “meet Bon Jovi” and then “combat” (t = 8). This is because
after the activation of death machine  the story enters a scenario where the player tries to combat with
a cardboard standee. Towards the end of the episode  the observation topic “converse w/rachel” and
the topic “kitchen & chat” corresponding to the selected action reach their peaks and then decay right
before the end of the story  where the action topic “relieve” climbs up to its peak. This is consistent
with the story ending  where the player chooses to overcome his fear after chatting with Rachel. In
Appendix D  we show the observation and the action texts in the above stages of the story.
Finally  another interesting observation is about the matrix U. Since the Q-function value is computed
from [ˆ✓a
t   the (i  j)-th element of the matrix U measures the positive/negative correlation
between the i-th action topic and the j-th observation topic. In Figure 2(c)  we show the value of the
learned matrix U for the four observation topics and the four action topics in Table 2. Interestingly 
the largest value (39.5) of U is the (1  2)-th element  meaning that the action topic “relieve” and the
state topic “converse w/rachel” has strong positive contribution to a high long-term reward  which is
what happens at the end of the story.

t ]T U ˆ✓S

6In practice  we observe that some topics are never or rarely activated during the learning process. This is
especially true when the number of topics becomes large (e.g.  100). Therefore  we only show the most active
topics. This might also explain why the performance improvement is marginal when the number of topics grows.

8

Table 2: The four most active topics for the observation texts and the action texts  respectively.

Observation Topics
1: combat
2: converse w/ rachel
3: meet Bon Jovi
4: wander at mall
Action Topics
1: relieve
2: kitchen & chat
3: operate the machine
4: action at mall

minutes  lights  ﬁrearm  shoulders  whiff  red  suddenly  huge  rendition
rachel  tonight  grabs  bar  towards  happy  believing  said  moonlight
small  jovi  bon  door  next  dog  insists  room  wrapped  standees
ended  catcher  shopping  peak  wrapped  hanging  attention  door

leave  get  gotta  go  hands  away  maybe  stay  ability  turn  easy  rachel
wait  tea  look  brisk  classics  oysters  kitchen  turn  chair  moment
coin  insert  west  cloth  desk  apply  dollars  saying  hands  touch  tell
alarm  machine  east  ignore  take  shot  oysters  win  gaze  bestowed

1

0.8

0.6

0.4

0.2

0

Observation Topic 1
Observation Topic 2
Observation Topic 3
Observation Topic 4

5

10

15

1

0.8

0.6

0.4

0.2

0

Action Topic 1
Action Topic 2
Action Topic 3
Action Topic 4

1.2
22.1
2.5
5.3

264

39.5
12.4
4.8
8.4

20.7
12.2
1.4 0.2
4.1
1.9
4.1
13.3

375

5

10

15

(a) Observation topic ✓S
t

(b) Selected action topic ✓at
t

(c) Learned values of matrix U

Figure 2: The evolution of the most active topics in “Machine of Death.”

5 Conclusion

We proposed a probabilistic model  Q-LDA  to uncover latent patterns in text-based sequential
decision processes. The model can be viewed as a latent topic model  which chains the topic
proportions over time. Interestingly  by modeling the mean function of the immediate reward in a
special way  we showed that discriminative learning of Q-LDA using its likelihood is closely related
to Q-learning. Thus  our approach could also be viewed as a Q-learning variant for sequential topic
models. We evaluate Q-LDA on two text-game tasks  demonstrating state-of-the-art rewards in these
games. Furthermore  we showed our method provides a viable approach to ﬁnding interesting latent
patterns in such decision processes.

Acknowledgments
The authors would like to thank all the anonymous reviewers for their constructive feedback.

References
[1] Christophe Andrieu  Nando De Freitas  Arnaud Doucet  and Michael I Jordan. An introduction

to MCMC for machine learning. Machine learning  50(1):5–43  2003.

[2] C. M. Bishop and J. Lasserre. Generative or discriminative? getting the best of both worlds.

Bayesian Statistics  8:3–24  2007.

[3] D. M. Blei and J. D. Mcauliffe. Supervised topic models. In Proc. NIPS  pages 121–128  2007.

[4] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. JMLR  3:993–1022  2003.

[5] David M Blei. Probabilistic topic models. Communications of the ACM  55(4):77–84  2012.

[6] David M Blei and John D Lafferty. Dynamic topic models.

In Proceedings of the 23rd

international conference on Machine learning  pages 113–120. ACM  2006.

9

[7] G. Bouchard and B. Triggs. The tradeoff between generative and discriminative classiﬁers. In

Proc. COMPSTAT  pages 721–728  2004.

[8] Jianshu Chen  Ji He  Yelong Shen  Lin Xiao  Xiaodong He  Jianfeng Gao  Xinying Song  and
Li Deng. End-to-end learning of lda by mirror-descent back propagation over a deep architecture.
In Proc. NIPS  pages 1765–1773  2015.

[9] Yaakov Engel  Shie Mannor  and Ron Meir. Reinforcement learning with Gaussian processes. In
Proceedings of the Twenty-Second International Conference on Machine Learning (ICML-05) 
pages 201–208  2005.

[10] Matthew Hausknecht and Peter Stone. Deep recurrent Q-learning for partially observable MDPs.

In Proc. AAAI-SDMIA  November 2015.

[11] Ji He  Jianshu Chen  Xiaodong He  Jianfeng Gao  Lihong Li  Li Deng  and Mari Ostendorf.

Deep reinforcement learning with a natural language action space. In Proc. ACL  2016.

[12] A. Holub and P. Perona. A discriminative framework for modelling object classes. In Proc.

IEEE CVPR  volume 1  pages 664–671  2005.

[13] Michael I Jordan  Zoubin Ghahramani  Tommi S Jaakkola  and Lawrence K Saul. An intro-
duction to variational methods for graphical models. In Learning in graphical models  pages
105–161. Springer  1998.

[14] Michael Irwin Jordan. Learning in graphical models  volume 89. Springer Science & Business

Media  1998.

[15] S. Kapadia. Discriminative Training of Hidden Markov Models. PhD thesis  University of

Cambridge  1998.

[16] Sergey Levine  Chelsea Finn  Trevor Darrell  and Pieter Abbeel. End-to-end training of deep

visuomotor policies. Journal of Machine Learning Research  17(1):1334–1373  2016.

[17] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report 

Technical report  DTIC Document  1993.

[18] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G.
Bellemare  Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Pe-
tersen  Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan
Wierstra  Shane Legg  and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature  518:529–533  2015.

[19] Karthik Narasimhan  Tejas Kulkarni  and Regina Barzilay. Language understanding for text-

based games using deep reinforcement learning. In Proc. EMNLP  2015.

[20] David Silver  Aja Huang  Chris J. Maddison  Arthur Guez  Laurent Sifre  George van den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  Sander
Dieleman  Dominik Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy Lillicrap 
Madeleine Leach  Koray Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mastering the
game of Go with deep neural networks and tree search. Nature  529:484–489  2016.

[21] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press

Cambridge  1998.

[22] Christopher Watkins and Peter Dayan. Q-learning. Machine learning  8(3-4):279–292  1992.
[23] Oksana Yakhnenko  Adrian Silvescu  and Vasant Honavar. Discriminatively trained Markov

model for sequence classiﬁcation. In Proc. IEEE ICDM  2005.

10

,Jianshu Chen
Lin Xiao
Ji He
Lihong Li
Li Deng
Yifan Sun
Yaqi Duan
Hao Gong
Mengdi Wang