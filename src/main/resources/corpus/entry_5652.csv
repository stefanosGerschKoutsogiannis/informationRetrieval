2018,Training deep learning based denoisers without ground truth data,Recently developed deep-learning-based denoisers often outperform state-of-the-art conventional denoisers  such as the BM3D. They are typically trained to minimizethe mean squared error (MSE) between the output image of a deep neural networkand a ground truth image.  In deep learning based denoisers  it is important to use high quality noiseless ground truth data for high performance  but it is often challenging or even infeasible to obtain noiseless images in application areas such as hyperspectral remote sensing and medical imaging. In this article  we propose a method based on Stein’s unbiased risk estimator (SURE) for training deep neural network denoisers only based on the use of noisy images. We demonstrate that our SURE-based method  without the use of ground truth data  is able to train deep neural network denoisers to yield performances close to those networks trained with ground truth  and to outperform the state-of-the-art denoiser BM3D. Further improvements were achieved when noisy test images were used for training of denoiser networks using our proposed SURE-based method.,Training deep learning based denoisers

without ground truth data

Shakarim Soltanayev

Se Young Chun

Ulsan National Institute of Science and Technology (UNIST)  Republic of Korea

Department of Electrical Engineering

{shakarim sychun}@unist.ac.kr

Abstract

Recently developed deep-learning-based denoisers often outperform state-of-the-art
conventional denoisers  such as the BM3D. They are typically trained to minimize
the mean squared error (MSE) between the output image of a deep neural network
and a ground truth image. In deep learning based denoisers  it is important to
use high quality noiseless ground truth data for high performance  but it is often
challenging or even infeasible to obtain noiseless images in application areas such
as hyperspectral remote sensing and medical imaging. In this article  we propose a
method based on Stein’s unbiased risk estimator (SURE) for training deep neural
network denoisers only based on the use of noisy images. We demonstrate that our
SURE-based method  without the use of ground truth data  is able to train deep
neural network denoisers to yield performances close to those networks trained
with ground truth  and to outperform the state-of-the-art denoiser BM3D. Further
improvements were achieved when noisy test images were used for training of
denoiser networks using our proposed SURE-based method. Code is available at
https://github.com/Shakarim94/Net-SURE.

1

Introduction

Deep learning has been successful in various high-level computer vision tasks [1]  such as image
classiﬁcation [2  3]  object detection [4  5]  and semantic segmentation [6  7]. Deep learning has also
been investigated for low-level computer vision tasks  such as image denoising [8  9  10  11  12] 
image inpainting [13]  and image restoration [14  15  16].
In particular  image denoising is a
fundamental computer vision task that yields images with reduced noise  and improves the execution
of other tasks  such as image classiﬁcation [8] and image restoration [16].
Deep learning based image denoisers [9  11  12] have yielded performances that are equivalent
to or better than those of conventional state-of-the-art denoising techniques such as BM3D [17].
These deep denoisers typically train their networks by minimizing the mean-squared error (MSE)
between the output of a network and the ground truth (noiseless) image. Thus  it is crucial to have
high quality noiseless images for high performance deep learning denoisers. Thus far  deep neural
network denoisers have been successful since high quality camera sensors and abundant light allow
the acquisition of high quality  almost noiseless 2D images in daily environment tasks. Acquiring
such high quality photographs is quite cheap with the use of smart phones and digital cameras.
However  it is challenging to apply currently developed deep learning based image denoisers with
minimum MSE to some application areas  such as hyperspectral remote sensing and medical imaging 
where the acquisition of noiseless ground truth data is expensive  or sometimes even infeasible. For
example  hyperspectral imaging contains hundreds of spectral information per pixel  often leading to
increased noise in hyperspectral imaging sensors [18]. Long acquisitions may improve image quality 
but it is challenging to perform them with spaceborne or airborne hyperspectral imaging. Similarly 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

in medical imaging  ultra high resolution 3D MRI (sub-millimeter resolution) often requires several
hours of acquisition time for a single  high quality volume  but reducing acquisition time leads to
increased noise. In X-ray CT  image noise can be substantially reduced by increasing the radiation
dose. Recent studies on deep learning based image denoisers [19  20] used CT images generated
with normal doses as the ground truth so that denoising networks would be able to be trained to
yield excellent performance. However  increased radiation dose leads to harmful effects in scanned
subjects  while excessively high doses may saturate the CT detectors (e.g.  in a similar manner to the
acquisition of a photo of the sun without any ﬁlter). Thus  acquiring ground truth data with newly
developed CT scanners seems challenging without compromising the subjects’ safety.
Conventional denoising methods do not usually require noiseless ground truth images to perform
denoising  but often require them for tuning parameters of image ﬁlters to elicit the best possible
results (minimum MSE). In order to identify the optimal parameters of conventional denoisers
without ground truth data  several works have been conducted [21  22] using Stein’s unbiased risk
estimator (SURE) [23]  which is an unbiased MSE estimator. For the popular non-local means (NLM)
ﬁlter [24]  the analytical form of SURE was used to optimize the denoiser performance [25  26  27].
For denoisers whose analytical forms of SURE are not available  Ramani et al. [28] proposed a Monte-
Carlo-based SURE (MC-SURE) method to determine near-optimal denoising parameters based on
the brute-force search of the parameter space. Deledalle et al. [29] investigated the approximation of
a weak gradient of SURE to optimize parameters using the quasi-Newton algorithm. However  since
this method requires the computation of the weak Jacobian  it is not applicable to high dimensional
parameter spaces  such as deep neural networks.
We propose a SURE-based training method for deep neural network denoisers without ground
truth data. In Section 2  we review key results elicited from SURE and MC-SURE. Subsequently 
in Section 3  we describe our proposed method using MC-SURE and a stochastic gradient for
training deep learning based image denoisers. In Section 4  simulation results are presented for (a)
conventional state-of-the-art denoiser (BM3D)  (b) deep learning based denoiser trained with BM3D
as the ground truth  (c) the same deep neural network denoiser with the proposed SURE training
without the ground truth  and (d) the same denoiser network with ground truth data as a reference.
Section 5 concludes this article by discussing several potential issues for further studies.

2 Background

2.1 Stein’s unbiased risk estimator

A signal (or image) with Gaussian noise can be modeled as 

y = x + n

(1)
where x ∈ RK is an unknown signal in accordance with x ∼ p(x)  y ∈ RK is a known measurement 
n ∈ RK is an i.i.d. Gaussian noise such that n ∼ N (0  σ2I)  and where I is an identity matrix. We
denote n ∼ N (0  σ2I) as n ∼ N0 σ2. An estimator of x from y (or denoiser) can be deﬁned as a
function of y such that
(2)
where h  g are functions from RK to RK. Accordingly  the SURE for h(y) can be derived as follows 

h(y) = y + g(y)

K(cid:88)

i=1

∂hi(y)

∂yi

(3)

η(h(y)) = σ2 +

(cid:107)g(y)(cid:107)2

K

+

2σ2
K

∂gi(y)

∂yi

=

(cid:107)y − h(y)(cid:107)2

K

− σ2 +

2σ2
K

K(cid:88)

i=1

where η : RK → R and yi is the ith element of y. For a ﬁxed x  the following theorem holds:
Theorem 1 ([23  30]). The random variable η(h(y)) is an unbiased estimator of

or

(cid:107)x − h(y)(cid:107)2

= En∼N0 σ2 {η(h(y))}

(4)

MSE(h(y)) =

(cid:26)(cid:107)x − h(y)(cid:107)2

1
K

(cid:27)

K

En∼N0 σ2

2

where En∼N0 σ2{·} is the expectation operator in terms of the random vector n. Note that in
Theorem 1  x is treated as a ﬁxed  deterministic vector.
In practice  σ2 can be estimated [28] and (cid:107)y − h(y)(cid:107)2 only requires the output of the estimator (or
denoiser). The last divergence term of (3) can be obtained analytically in some special cases  such as
in linear or NLM ﬁlters [26]. However  it is challenging to calculate this term analytically for more
general denoising methods.

2.2 Monte-Carlo Stein’s unbiased risk estimator

Ramani et al. [28] introduced a fast Monte-Carlo approximation of the divergence term in (3) for
general denoisers. For a ﬁxed unknown true image x  the following theorem is valid:
Theorem 2 ([28]). Let ˜n ∼ N0 1 ∈ RK be independent of n  y. Then 

(cid:26)

(cid:18) h(y + ˜n) − h(y)

(cid:19)(cid:27)

∂hi(y)

∂yi

E˜n

= lim
→0

˜nt



K(cid:88)

i=1

(5)

provided that h(y) admits a well-deﬁned second-order Taylor expansion. If not  this is still valid in
the weak sense provided that h(y) is tempered.
Based on Theorem 2  the divergence term in (3) can be approximated by one realization of ˜n ∼ N0 1
and a ﬁxed small positive value :

∂hi(y)

∂yi

≈ 1
K

˜nt (h(y + ˜n) − h(y))

(6)

K(cid:88)

i=1

1
K

where t is the transpose operator. This expression has been shown to yield accurate unbiased estimates
of MSE for many conventional denoising methods h(y) [28].

3 Method

In this section  we will develop our proposed MC-SURE-based method for training deep learning
based denoisers without noiseless ground truth images by assuming a Gaussian noise model in (1).

3.1 Training denoisers using the stochastic gradient method

A typical risk for image denoisers with the signal generation model (1) is

Ex∼p(x) n∼N0 σ2(cid:107)x − h(y; θ)(cid:107)2

(7)

where h(y; θ) is a deep learning based denoiser parametrized with a large-scale vector θ. It is usually
infeasible to calculate (7) exactly due to expectation operator. Thus  the empirical risk for (7) is used
as a cost function as follows:

(cid:107)h(y(j); θ) − x(j)(cid:107)2

(8)

N(cid:88)

j=1

1
N

where {(x(1)  y(1)) ···   (x(N )  y(N ))} are the N pairs of a training dataset  sampled from the joint
distribution of x(j) ∼ p(x) and n(j) ∼ N0 σ2. Note that (8) is an unbiased estimator of (7).
To train the deep learning network h(y; θ) with respect to θ  a gradient-based optimization algorithm
is used such as the stochastic gradient descent (SGD) [31]  momentum  Nesterov momentum [32]  or
the Adam optimization algorithm [33]. For any gradient-based optimization method  it is essential to
calculate the gradient of (7) with respect to θ as follows 

Ex∼p(x) n∼N0 σ2 2∇θh(y; θ)t (h(y; θ) − x) .

(9)

Therefore  it is sufﬁcient to calculate the gradient of the empirical risk (8) to approximate (9) for any
gradient-based optimization.
In practice  calculating the gradient of (8) for large N is inefﬁcient since a small amount of well-
shufﬂed training data can often approximate the gradient of (8) accurately. Thus  a mini-batch is

3

typically used for efﬁcient deep neural network training by calculating the mini-batch empirical risk
as follows:

(cid:107)h(y(j); θ) − x(j)(cid:107)2

(10)

M(cid:88)

j=1

1
M

where M is the number of one mini-match. Equation (10) is still an unbiased estimator of (7) provided
that the training data is randomly permuted every epoch.

3.2 Proposed training method for deep learning based denoisers

To incorporate MC-SURE into a stochastic gradient-based optimization algorithm for training  such
as the SGD or the Adam optimization algorithms  we modify the risk (7) in accordance with

(cid:104)En∼N0 σ2

(cid:0)(cid:107)x − h(y; θ)(cid:107)2|x(cid:1)(cid:105)

.

Ex∼p(x)

where (11) is equivalent to (7) owing to conditioning.
From Theorem 1  an unbiased estimator for En∼N0 σ2

(cid:0)(cid:107)x − h(y; θ)(cid:107)2|x(cid:1) can be derived as
(cid:0)(cid:107)x − h(y; θ)(cid:107)2|x(cid:1) = En∼N0 σ2(cid:107)x − h(y; θ)(cid:107)2 = KEn∼N0 σ2 η(h(y; θ)).
M(cid:88)

(cid:107)y(j) − h(y(j); θ)(cid:107)2 − Kσ2 + 2σ2

∂hi(y(j); θ)

K(cid:88)

Kη(h(y; θ))

(cid:26)

(cid:27)

1
M

j=1

i=1

∂yi

Thus  using the empirical risk expression in (10)  an unbiased estimator for (7) is

such that for a ﬁxed x 

En∼N0 σ2

(11)

(12)

(13)

noting that no noiseless ground truth data x(j) were used in (13).
Finally  the last divergence term in (13) can be approximated using MC-SURE so that the ﬁnal
unbiased risk estimator for (7) will be

(cid:107)y(j) − h(y(j); θ)(cid:107)2 − Kσ2 +

2σ2


h(y(j) + ˜n(j); θ) − h(y(j); θ)

(14)

(˜n(j))t(cid:16)

(cid:17)(cid:27)

(cid:26)

M(cid:88)

j=1

1
M

where  is a small ﬁxed positive number and ˜n(j) is a single realization from the standard normal
distribution for each training data j. In order to make sure that the estimator (14) is unbiased  the
order of y(j) should be randomly permuted and the new set of ˜n(j) should be generated at every
epoch.
The deep learning based image denoiser with the cost function of (14) can be implemented using
a deep learning development framework  such as TensorFlow [34]  by properly deﬁning the cost
function. Thus  the gradient of (14) can be automatically calculated when the training is performed.
One of the potential advantages of our SURE-based training method is that we can use all the
available data without noiseless ground truth images. In other words  we can train denoising  deep
neural networks with the use of training and testing data. This advantage may further improve the
performance of deep learning based denoisers.
Lastly  almost any deep neural network denoiser can utilize our MC-SURE-based training by modify-
ing the cost function from (10) to (14) as far as it satisﬁes the condition in Theorem 2. Many deep
learning based denoisers with differentiable activation functions (e.g.  sigmoid) can comply with this
condition. Some denoisers with piecewise differentiable activation functions (e.g.  ReLU) can still
utilize Theorem 2 in the weak sense since

(cid:107)h(y; θ)(cid:107) ≤ C0(1 + (cid:107)y(cid:107)n0)

for some n0 > 1 and C0 > 0. Therefore  we expect that our proposed method should work for most
deep learning image denoisers [8  9  10  11  12].

4

4 Simulation results

In this section  denoising simulation results are presented with the MNIST dataset using a simple
stacked denoising autoencoder (SDA) [8]  and a large-scale natural image dataset using a deep
convolutional neural network (CNN) image denoiser (DnCNN) [11].
All of the networks presented in this section (denoted by NET  which can be either SDA or DnCNN)
were trained using one of the following two optimization objectives: (MSE) the minimum MSE
between a denoised image and its ground truth image in (10) and (SURE) our proposed minimum
MC-SURE without ground truth in (14). NET-MSE methods generated noisy training images at every
epochs in accordance with [11]  while our proposed NET-SURE methods used only noisy images
obtained before training. We also propose the SURE-T method which utilized noisy test images with
noisy training images and without ground truth data. Table 1 summarizes all simulated conﬁgurations
including conventional state-of-the-art image denoiser  BM3D [17]  that did not require any training 
or the use of any ground truth data. Code is available at https://github.com/Shakarim94/Net-SURE.

Table 1: Summary of simulated denoising methods. NET can be either SDA or DnCNN.

Method
BM3D
NET-BM3D
NET-SURE
NET-SURE-T
NET-MSE-GT Optimizing MSE with ground truth

Description
Conventional state-of-the-art method
Optimizing MSE with BM3D output as ground truth
Optimizing SURE without ground truth
Optimizing SURE without ground truth  but with noisy test data

4.1 Results: MNIST dataset

We performed denoising simulations with the MNIST dataset. Noisy images were generated based on
model (1) with two noise levels (one with σ = 25 and the other with σ = 50). For the experiments on
the MNIST dataset which comprised 28 × 28 pixels  a simple SDA network was chosen [8]. Decoder
and encoder networks each consisted of two convolutional layers (kernel size 3 × 3) with sigmoid
activation functions  each of which had a stride of two (both conv and conv transposed). Thus  a
training sample with a size of 28 × 28 is downsampled to 7 × 7  and then upsampled to 28 × 28.
SDA was trained to output a denoised image using a set of 55 000 training and 5 000 validation
images. The performance of the model was tested with 100 images chosen randomly from the default
test set of 10 000 images. For all cases  SDA was trained with the Adam optimization algorithm [33]
with the learning rate of 0.001 for 100 epochs. The batch size was set to 200 (bigger batch sizes did
not improve the performance). The  value in (6) was set to 0.0001.
Our proposed methods SDA-SURE  SDA-SURE-T yielded a comparable performance to SDA-MSE-
GT (only 0.01-0.04 dB difference) and outperformed the conventional BM3D for all simulated noise
levels  σ = 25  50  as shown in Table 2.

Table 2: Results of denoisers for MNIST (performance in dB). Means of 10 experiments are reported.

Methods BM3D SDA-REG SDA-SURE

SDA-SURE-T

SDA-MSE-GT

σ = 25
σ = 50

27.53
21.82

25.07
19.85

28.35
25.23

28.39
25.24

28.35
25.24

Figure 1 illustrates the visual quality of the outputs of the simulated denoising methods at high noise
levels (σ = 50). All SDA-based methods clearly outperform the conventional BM3D method based
on visual inspection (BM3D image looks blurry compared to other SDA-based results)  while it is
indistinguishable for the simulation results among all SDA methods with different cost functions and
training sets. These observations were conﬁrmed by the quantitative results shown in Table 2. All
SDA-based methods outperformed BM3D signiﬁcantly  but there were very small differences among
all the SDA methods  even when noisy test data were used.

5

(a) Noisy image

(b) BM3D

(c) SDA-MSE-GT

(d) SDA-SURE

(e) SDA-SURE-T

(f) SDA-REG

Figure 1: Denoising results of SDA with various methods for MNIST dataset at a noise level of σ=50.

4.2 Regularization effect of deep neural network denoisers

Parametrization of deep neural networks with different number of parameters and structures may
introduce a regularization effect in training denoisers. We further investigated this regularization
effect by training the SDA to minimize the MSE between the output of SDA and the input noisy
image (SDA-REG). In the case of a noise level of σ = 50  early stopping rule was applied when the
network started to overﬁt the noisy dataset after the ﬁrst few epochs. The performance of this method
was signiﬁcantly worse than those of all other methods with PSNR values of 25.07 dB (σ = 25) and
19.85 dB (σ = 50)  as shown in Table 2. These values are approximately 2 dB lower than the PSNRs
of BM3D. Noise patterns are visible  as shown in Figure 1. This shows that the good performance of
SDA is not attributed to its structure only  but also depends on the optimization of MSE or SURE.

4.3 Accuracy of MC-SURE approximation

A small value must be assigned to  in (6) for accurate estimation of SURE. Ramani et al. [28]
have observed that  can take a wide range of values and its choice is not critical. According to
our preliminary experiments for the SDA with an MNIST dataset  any choice for  in the range
of [10−2  10−7] worked well so that the SURE approximation matches close to the MSE during
training  as illustrated in Figure 2 (middle). Extremely small values  < 10−8 resulted in numerical
instabilities  as shown in Figure 2 (right). On the contrary  when  > 10−1  the approximation in
(6) becomes substantially inaccurate. Figure 3 illustrates how the performance of SDA-SURE is
affected by the  value. However  note that these values are only for SDA trained with the MNIST
dataset. The admissible range of  depends on hi(y; θ). For example  we observed that a suitable
 value must be carefully selected in other cases  such as DnCNN with  large-scale parameters and
high resolution images for improved performance.

Figure 2: Loss curves for the training of SDA with MSE (blue) and its corresponding MC-SURE
(red) using different  values   = 1 (left)   = 10−5 (middle)  and  = 10−9 (right). MC-SURE
accurately approximates the true MSE for a wide range of .

The accuracy of MC-SURE also depends on the noise level σ. It was observed that the SURE loss
curves become noisier compared to MSE loss curves as σ increases. However  they still followed
similar trends and yielded similar average PSNRs on MNIST dataset as shown in Figure 4. We
observed that after σ = 350  SURE loss curves started to become too noisy and thus deviated from
the trends of their corresponding MSE loss curves. Conversely  noise levels σ > 300 were too high
for both SDA-based denoisers and BM3D  so that they were not able to output recognizable digits.
Therefore  SDA-SURE can be trained effectively on adequately high noise levels so that it can yield a
performance that is comparable to SDA-MSE-GT and can consistently outperform BM3D.

6

Figure 3: Performance of SDA-SURE for dif-
ferent  values at σ = 25.

Figure 4: Performance of denoising methods
at different σ values.

4.4 Results: high resolution natural images dataset

To demonstrate the capabilities of our SURE-based deep learning denoisers  we investigated a deeper
and more powerful denoising network called DnCNN [11] using high resolution images. DnCNN
consisted of 17 layers of CNN with batch normalization and ReLU activation functions. Each
convolutional layer had 64 ﬁlters with sizes of 3 × 3. Similar to [11]  the network was trained with
400 images with matrix sizes of 180 × 180 pixels. In total  1772 × 128 image patches with sizes
of 40 × 40 pixels were extracted randomly from these images. Two test sets were used to evaluate
performance: one set consisted of 12 widely used images (Set12) [17]  and the other was a BSD68
dataset. For DnCNN-SURE-T  additional 808 × 128 image patches were extracted from these noisy
test images  and were then added to the training dataset. For all cases  the network was trained
with 50 epochs using the Adam optimization algorithm with an initial learning rate of 0.001  which
eventually decayed to 0.0001 after 40 epochs. The batch size was set to 128 (note that bigger batch
sizes did not improve performance). Images were corrupted at three noise levels (σ = 25  50  75).
DnCNN used residual learning [11] whereby the network was forced to learn the difference between
noisy and ground truth images. The output residual image was then subtracted from the input noisy
image to yield the estimated image. In other words  our network was trained with SURE as

h(y; θ) = y − CNNθ(y)

(15)
where CNNθ(.) is the DnCNN that is being trained using residual learning. For DnCNN  selecting
an appropriate  value in (6) turned out to be important for a good denoising performance. To achieve
stable training with good performance   had to be tuned for each of the chosen noise levels of σ =
25  50  75. We observed that the optimal value for  was proportional to σ as shown in [29]. All the
experiments were performed with the setting of  = σ × 1.4 × 10−4.
With the use of an NVidia Titan X GPU  the training process took approximately 7 hours for DnCNN-
MSE-GT and approximately 11 hours for DnCNN-SURE. SURE based methods took more training

Table 3: Results of denoising methods on 12 widely used images (Set12) (performance in dB).
IMAGE

STARFISH MONARCH AIRPLANE

LENA BARBARA BOAT MAN

C. MAN HOUSE

PEPPERS

PARROT

COUPLE Average

BM3D
DNCNN-BM3D
DNCNN-SURE
DNCNN-SURE-T
DNCNN-MSE-GT

BM3D
DNCNN-BM3D
DNCNN-SURE
DNCNN-SURE-T
DNCNN-MSE-GT

BM3D
DNCNN-BM3D
DNCNN-SURE
DNCNN-SURE-T
DNCNN-MSE-GT

29.47
29.34
29.80
29.86
30.14

26.00
25.76
26.48
26.47
27.03

24.58
24.11
24.65
24.82
25.46

33.00
31.99
32.70
32.73
33.16

29.51
28.43
29.14
29.20
29.92

27.45
27.02
27.16
27.34
28.04

30.23
30.13
30.58
30.57
30.84

26.58
26.5
26.77
26.78
27.27

24.69
24.48
24.49
24.58
25.22

28.58
28.38
29.08
29.11
29.4

25.01
24.9
25.38
25.39
25.65

23.19
23.09
23.25
23.34
23.62

29.35
29.21
30.11
30.13
30.45

25.78
25.66
26.50
26.53
26.95

23.81
23.73
24.10
24.25
24.81

σ = 25

28.37
28.46
28.94
28.93
29.11

σ = 50

25.15
25.15
25.66
25.65
25.93

σ = 75

23.38
23.40
23.52
23.56
23.97

7

28.89
28.91
29.17
29.26
29.36

25.98
25.82
26.21
26.21
26.43

24.22
24.06
24.13
24.44
24.71

32.06
31.53
32.06
32.08
32.44

28.93
28.36
28.79
28.81
29.31

27.14
27.11
26.92
27.03
27.60

30.64
28.89
29.16
29.44
29.91

27.19
25.3
24.86
25.23
26.17

25.08
23.80
23.02
23.07
23.88

29.78
29.6
29.84
29.86
30.11

26.62
26.5
26.78
26.79
27.12

25.05
24.84
25.09
25.17
25.53

29.60
29.52
29.89
29.91
30.08

26.79
26.6
26.97
26.97
27.22

25.30
25.19
25.37
25.45
25.68

29.70
29.54
29.76
29.78
30.06

26.46
26.17
26.51
26.48
26.94

24.73
24.59
24.70
24.78
25.13

29.97
29.63
30.09
30.14
30.42

26.67
26.26
26.67
26.71
27.16

24.89
24.62
24.70
24.82
25.30

Table 4: Results of denoising methods on BSD68 dataset (performance in dB).

Methods

BM3D

DnCNN-BM3D

DnCNN-SURE

DnCNN-SURE-T

DnCNN-MSE-GT

σ = 25
σ = 50
σ = 75

28.56
25.62
24.20

28.54
25.44
24.09

28.97
25.93
24.31

29.00
25.95
24.37

29.20
26.22
24.66

time than MSE based methods because of the additional divergence calculations executed to optimize
the MC-SURE cost function. For the DnCNN-SURE-T method  it took approximately 15 hours to
complete the training owing to the larger dataset.
Tables 3 and 4 present denoising performance data using (a) the BM3D denoiser [17]  (b) a state-of-
the-art deep CNN (DnCNN) image denoiser trained with MSE [11]  and (c) the same DnCNN image
denoiser trained with SURE without the use of noiseless ground truth images  for different dataset
variations (as shown in Table 1). The MSE-based DnCNN image denoiser with ground truth data 
DnCNN-MSE-GT  yielded the best denoising performance compared to other methods  such as the
BM3D  which is consistent with the results in [11].
As seen in Table 3  for the Set12 dataset  SURE-based denoisers achieved performances comparable
to or better than that for BM3D for noise levels σ = 25 and 50. In contrast  for higher noise levels
(σ = 75)  DnCNN-SURE and DnCNN-SURE-T yielded lower average PSNR values by 0.19 dB
and 0.07 dB than BM3D. DnCNN-SURE-T outperformed DnCNN-SURE in all cases  and had
considerably better performance on some images  such as “Barbara.” BM3D had exceptionally good
denoising performance on the “Barbara” image (up to 2.33 dB better PSNR)  and even outperformed
the DnCNN-MSE-GT method.
In the case of the BSD68 dataset in Table 4  SURE-based methods outperformed BM3D for all the
noise levels. Unlike the case of the Set12 images  we observed that DnCNN-SURE had a signiﬁcantly
better performance than BM3D  and yielded increased average PSNR values by 0.11 - 0.41 dB. It was
also observed that DnCNN-SURE-T beneﬁted from the utilization of noisy test images and improved
the average PSNR of DnCNN-SURE.
Differences among the performances of denoisers in Tables 3 and 4 can be explained by the working
principle of BM3D. Since BM3D looks for similar image patches for denoising  repeated patterns
(as in the “Barbara” image) and ﬂat areas (as in “House” image) can be key factors to generating
improved denoising results. One of the advantages of DnCNN-SURE over BM3D is that it does
not suffer from rare patch effects. If the test image is relatively detailed and does not contain many
similar patterns  BM3D will have poorer performance than the proposed DnCNN-SURE method.
Note that the DnCNN-BM3D method that trains networks by optimizing MSE with BM3D denoised
images as the ground truth yielded slightly worse performance than the BM3D itself (Tables 3  4).
Figure 5 illustrates the denoised results for an image from the BSD68 dataset. Visual quality assess-
ment indicated that BM3D yielded blurrier images and thus yielded worse PSNR compared to the
results generated by deep neural network denoisers. DnCNN-MSE-GT had the best denoised image
with the highest PSNR of 26.85 dB  while both SURE methods yielded very similar performances in
accordance with PSNR and visual quality assessment.

(a) Noisy image / 14.76dB

(b) BM3D / 26.14dB

(c) SURE / 26.46dB

(d) SURE-T / 26.46dB

(e) MSE / 26.85dB

Figure 5: Denoising results of an image from the BSD68 dataset for σ=50

8

5 Discussion

It has been shown that a single deep denoiser network  such as DnCNN  can be trained to deal
with multiple noise levels (e.g. σ = [0  55]) [11]. Thus  our work can be easily generalized to
train denoisers for multiple noise levels by modifying (14) to have σ(j) and (j) for the SURE risk
of the jth image patch. For example   values for image patches can be speciﬁed by our formula
(j) = σ(j) × 1.4 × 10−4 for DnCNN. In fact  SURE based DnCNN networks were trained to handle
a wide range of noise levels (blind denoising) in [35].
SURE-based methods used noisy training images only  but SURE-T methods used both noisy training
and test images. SURE-T methods yielded a slightly better denoising performance than SURE-based
methods (approximately 0.02 - 0.06 dB and 0.04 - 0.12 dB for BSD68 and Set12 datasets  respectively)
with considerably increased the overall inference time. Thus  at this moment  SURE-T methods do
not seem to have considerable beneﬁts compared to SURE-based methods. However  developing
a hybrid method that ﬁrst trains networks using SURE with training data  and then ﬁne-tunes the
network using SURE with testing data  could be potentially useful to reduce the overall inference
time. Finding a connection between SURE-T and “deep image prior” that used noisy test images for
denoising [36] can constitute interesting future work.
Our proposed SURE-based deep learning denoiser can be useful for applications with considerably
large amounts of noisy images  but with few noiseless images  or with expensive noiseless images.
Deep learning based denoising research is still evolving  and it may be even possible for our SURE-
based training method to achieve signiﬁcantly better performances than BM3D  or other conventional
state-of-the-art denoisers  when it is applied to novel deep neural network denoisers. Further
investigation will be needed for high performance denoising networks for synthetic and real noise.
In this work  Gaussian noise with known variance was assumed in all simulations. However  there
are several noise estimation methods that can be used with SURE (see [28] for details). SURE
can incorporate a variety of noise distributions other than Gaussian noise. For example  SURE has
been used for parameter selection of conventional ﬁlters for a Poisson distribution [29]. Generalized
SURE for exponential families has been proposed [37] so that other common noise types in imaging
systems can be potentially considered for SURE-based methods. It should be noted that SURE does
not require any prior knowledge on images. Thus  potentially it can be applied to the measurement
domain for different applications  such as medical imaging. Owing to noise correlation (colored
noise) in the image domain (e.g.  based on the Radon transform in the case of CT or PET)  further
investigations will be necessary to apply our proposed method directly to the image domain.
Note that unlike (7)  the existence of the minimizer for (14) should be considered with care since
it is theoretically possible that (14) becomes negative inﬁnity due to the divergence term in (14).
However  in practice  this issue can be easily addressed by introducing a regularizer (weight decay) 
with a deep neural network structure so that denoisers can impose regularity conditions on function h
(e.g.  bounded norm of ∇h)  either by choosing an adequate  value  or by using proper training data.
Lastly  note that we derived (14)  an unbiased estimator for MSE  assuming a ﬁxed θ. Thus  there is
no guarantee that the resulting estimator (denoiser) that is tuned by SURE will be unbiased [38].

6 Conclusion

We proposed a MC-SURE based training method for general deep learning denoisers. Our proposed
method trained deep neural network denoisers without noiseless ground truth data so that they
could yield comparable denoising performances to those elicited by the same denoisers that were
trained with noiseless ground truth data  and outperform the conventional state-of-the-art BM3D.
Our SURE-based training method worked successfully in the simple SDA [8]  and in the case of the
state-of-the-art DnCNN [11] without the use of ground truth images.

Acknowledgments

This work was supported partly by Basic Science Research Program through the National Research
Foundation of Korea(NRF) funded by the Ministry of Education(NRF-2017R1D1A1B05035810)
and partly by the Technology Innovation Program or Industrial Strategic Technology Development
Program (10077533  Development of robotic manipulation algorithm for grasping/assembling with

9

the machine learning using visual and tactile sensing information) funded by the Ministry of Trade 
Industry & Energy (MOTIE  Korea).

References
[1] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444 

May 2015.

[2] A Krizhevsky  I Sutskever  and G E Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems (NIPS) 25  pages
1097–1105  2012.

[3] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep Residual Learning for Image
Recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages
770–778  2016.

[4] R Girshick  J Donahue  and T Darrell. Rich feature hierarchies for accurate object detection
and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 580–587  2014.

[5] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster R-CNN: Towards real-time
object detection with region proposal networks. In Advances in Neural Information Processing
Systems (NIPS) 28  pages 91–99  2015.

[6] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille.
In

Semantic image segmentation with deep convolutional nets and fully connected crfs.
International Conference on Learning Representation (ICLR)  2015.

[7] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages
3431–3440  2015.

[8] Pascal Vincent  Hugo Larochelle  Isabelle Lajoie  Yoshua Bengio  and Pierre Antoine Manzagol.
Stacked denoising autoencoders: Learning Useful Representations in a Deep Network with a
Local Denoising Criterion. Journal of Machine Learning Research  11:3371–3408  December
2010.

[9] Harold C Burger  Christian J Schuler  and Stefan Harmeling. Image denoising: Can plain
neural networks compete with BM3D? In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 2392–2399  2012.

[10] Yi-Qing Wang and Jean-Michel Morel. Can a Single Image Denoising Neural Network Handle
All Levels of Gaussian Noise? IEEE Signal Processing Letters  21(9):1150–1153  May 2014.
[11] Kai Zhang  Wangmeng Zuo  Yunjin Chen  Deyu Meng  and Lei Zhang. Beyond a Gaussian
Denoiser: Residual Learning of Deep CNN for Image Denoising. IEEE Transactions on Image
Processing  26(7):3142–3155  May 2017.

[12] Stamatios Lefkimmiatis. Non-local Color Image Denoising with Convolutional Neural Net-
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages

works.
5882–5891  2017.

[13] Junyuan Xie  Linli Xu  and Enhong Chen. Image denoising and inpainting with deep neural
networks. In Advances in Neural Information Processing Systems (NIPS) 25  pages 341–349 
2012.

[14] Xiao Jiao Mao  Chunhua Shen  and Yu Bin Yang. Image restoration using very deep convo-
lutional encoder-decoder networks with symmetric skip connections. In Advances in Neural
Information Processing Systems (NIPS) 29  pages 2810–2818  2016.

[15] Ruohan Gao and Kristen Grauman. On-demand learning for deep image restoration. In IEEE

International Conference on Computer Vision (ICCV)  2017.

[16] Kai Zhang  Wangmeng Zuo  Shuhang Gu  and Lei Zhang. Learning Deep CNN Denoiser
Prior for Image Restoration. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 3929–3938  2017.

[17] Kostadin Dabov  Alessandro Foi  Vladimir Katkovnik  and Karen Egiazarian. Image denoising
by sparse 3-D transform-domain collaborative ﬁltering. IEEE Transactions on Image Processing 
16(8):2080–2095  August 2007.

10

[18] Minchao Ye  Yuntao Qian  and Jun Zhou. Multitask Sparse Nonnegative Matrix Factorization
for Joint Spectral–Spatial Hyperspectral Imagery Denoising. IEEE Transactions on Geoscience
and Remote Sensing  53(5):2621–2639  December 2014.

[19] Hu Chen  Yi Zhang  Mannudeep K Kalra  Feng Lin  Yang Chen  Peixi Liao  Jiliu Zhou  and
Ge Wang. Low-Dose CT With a Residual Encoder-Decoder Convolutional Neural Network.
IEEE Transactions on Medical Imaging  36(12):2524–2535  November 2017.

[20] Eunhee Kang  Junhong Min  and Jong Chul Ye. A deep convolutional neural network using
directional wavelets for low-dose X-ray CT reconstruction. Medical Physics  44(10):e360–e375 
October 2017.

[21] David L Donoho and Iain M Johnstone. Adapting to unknown smoothness via wavelet shrinkage.

Journal of the american statistical association  90(432):1200–1224  1995.

[22] Xiao-Ping Zhang and Mita D Desai. Adaptive denoising based on sure risk. IEEE signal

processing letters  5(10):265–267  1998.

[23] C M Stein. Estimation of the mean of a multivariate normal distribution. The Annals of Statistics 

9(6):1135–1151  November 1981.

[24] A Buades  B Coll  and J M Morel. A review of image denoising algorithms  with a new one.

Multiscale Modeling & Simulation  4(2):490–530  January 2005.

[25] J Salmon. On Two Parameters for Denoising With Non-Local Means. IEEE Signal Processing

Letters  17(3):269–272  March 2010.

[26] D Van De Ville and M Kocher. SURE-Based Non-Local Means. IEEE Signal Processing

Letters  16(11):973–976  November 2009.

[27] Minh Phuong Nguyen and Se Young Chun. Bounded Self-Weights Estimation Method for
Non-Local Means Image Denoising Using Minimax Estimators. IEEE Transactions on Image
Processing  26(4):1637–1649  February 2017.

[28] S Ramani  T Blu  and M Unser. Monte-Carlo Sure: A Black-Box Optimization of Regularization
IEEE Transactions on Image Processing 

Parameters for General Denoising Algorithms.
17(9):1540–1554  August 2008.

[29] Charles-Alban Deledalle  Samuel Vaiter  Jalal Fadili  and Gabriel Peyré. Stein unbiased gradient
estimator of the risk (sugar) for multiple parameter selection. SIAM Journal on Imaging
Sciences  7(4):2448–2487  2014.

[30] T Blu and F Luisier. The SURE-LET Approach to Image Denoising. IEEE Transactions on

Image Processing  16(11):2778–2786  October 2007.

[31] Léon Bottou. Online Learning and Stochastic Approximations. In On-line learning in neural

networks  pages 9–42. Cambridge University Press New York  NY  USA  1998.

[32] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o

(1/k2). In Soviet Mathematics Doklady  1983.

[33] Diederik P Kingma and Jimmy Ba. Adam - A Method for Stochastic Optimization.

International Conference on Learning Representation (ICLR)  2015.

In

[34] Martín Abadi et al. Tensorﬂow: A system for large-scale machine learning. In Proceedings
of the 12th USENIX Conference on Operating Systems Design and Implementation  pages
265–283  2016.

[35] Magauiya Zhussip and Se Young Chun. Simultaneous compressive image recovery and deep
denoiser learning from undersampled measurements. arXiv preprint arXiv:1806.00961  2018.
[36] Dmitry Ulyanov  Andrea Vedaldi  and Victor Lempitsky. Deep image prior. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  volume 17  page 18  2018.
[37] Y C Eldar. Generalized SURE for Exponential Families: Applications to Regularization. IEEE

Transactions on Signal Processing  57(2):471–481  January 2009.

[38] Ryan J Tibshirani and Saharon Rosset. Excess optimism: How biased is the apparent error of

an estimator tuned by sure? arXiv preprint arXiv:1612.09415  2016.

11

,Shakarim Soltanayev
Se Young Chun