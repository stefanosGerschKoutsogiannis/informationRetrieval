2014,Conditional Random Field Autoencoders for Unsupervised Structured Prediction,We introduce a framework for unsupervised learning of structured predictors with overlapping  global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated  conditional on the latent structure  using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders  posterior regularization and multi-view learning. Finally  we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment  and show that training our model can be substantially more efficient than comparable feature-rich baselines.,Conditional Random Field Autoencoders
for Unsupervised Structured Prediction

Waleed Ammar

Chris Dyer

Noah A. Smith

School of Computer Science
Carnegie Mellon University
Pittsburgh  PA 15213  USA

{wammar cdyer nasmith}@cs.cmu.edu

Abstract

We introduce a framework for unsupervised learning of structured predictors with
overlapping  global features. Each input’s latent representation is predicted con-
ditional on the observed data using a feature-rich conditional random ﬁeld (CRF).
Then a reconstruction of the input is (re)generated  conditional on the latent struc-
ture  using a generative model which factorizes similarly to the CRF. The autoen-
coder formulation enables efﬁcient exact inference without resorting to unrealistic
independence assumptions or restricting the kinds of features that can be used.
We illustrate connections to traditional autoencoders  posterior regularization  and
multi-view learning. We then show competitive results with instantiations of the
framework for two canonical tasks in natural language processing: part-of-speech
induction and bitext word alignment  and show that training the proposed model
can be substantially more efﬁcient than a comparable feature-rich baseline.

1

Introduction

Conditional random ﬁelds [24] are used to model structure in numerous problem domains  includ-
ing natural language processing (NLP)  computational biology  and computer vision. They enable
efﬁcient inference while incorporating rich features that capture useful domain-speciﬁc insights. De-
spite their ubiquity in supervised settings  CRFs—and  crucially  the insights about effective feature
sets obtained by developing them—play less of a role in unsupervised structure learning  a prob-
lem which traditionally requires jointly modeling observations and the latent structures of interest.
For unsupervised structured prediction problems  less powerful models with stronger independence
assumptions are standard.1 This state of affairs is suboptimal in at least three ways: (i) adhering
to inconvenient independence assumptions when designing features is limiting—we contend that
effective feature engineering is a crucial mechanism for incorporating inductive bias in unsuper-
vised learning problems; (ii) features and their weights have different semantics in joint and condi-
tional models (see §3.1); and (iii) modeling the generation of high-dimensional observable data with
feature-rich models is computationally challenging  requiring expensive marginal inference in the
inner loop of iterative parameter estimation algorithms (see §3.1).

Our approach leverages the power and ﬂexibility of CRFs in unsupervised learning without sacriﬁc-
ing their attractive computational properties or changing the semantics of well-understood feature
sets. Our approach replaces the standard joint model of observed data and latent structure with a two-
layer conditional random ﬁeld autoencoder that ﬁrst generates latent structure with a CRF (condi-
tional on the observed data) and then (re)generates the observations conditional on just the predicted
structure. For the reconstruction model  we use distributions which offer closed-form maximum

1For example  a ﬁrst-order hidden Markov model requires that yi ⊥ xi+1 | yi+1 for a latent sequence

y = hy1  y2  . . .i generating x = hx1  x2  . . .i  while a ﬁrst-order CRF allows yi to directly depend on xi+1.

1

Extension: partial reconstruction.
In our running POS example  the reconstruction model
pθ(ˆxi | yi) deﬁnes a distribution over words given tags. Because word distributions are heavy-
tailed  estimating such a distribution reliably is quite challenging. Our solution is to deﬁne a func-
tion π : X → ˆX such that | ˆX | ≪ |X |  and let ˆxi = π(xi) be a deterministic transformation of the
original structured observation. We can add indirect supervision by deﬁning π such that it represents
observed information relevant to the latent structure of interest. For example  we found reconstruct-
ing Brown clusters [5] of tokens instead of their surface forms to improve POS induction. Other
possible reconstructions include word embeddings  morphological and spelling features of words.

More general graphs. We presented the CRF autoencoder in terms of sequential Markovian as-
sumptions for ease of exposition; however  this framework can be used to model arbitrary hidden
structures. For example  instantiations of this model can be used for unsupervised learning of parse
trees [21]  semantic role labels [42]  and coreference resolution [35] (in NLP)  motif structures [1]
in computational biology  and object recognition [46] in computer vision. The requirements for
applying the CRF autoencoder model are:

• An encoding discriminative model deﬁning pλ(y | x  φ). The encoder may be any model family

where supervised learning from hx  yi pairs is efﬁcient.

• A reconstruction model that deﬁnes pθ(ˆx | y  φ) such that inference over y given hx  ˆxi is

efﬁcient.

• The independencies among y | x  ˆx are not strictly weaker than those among y | x.

2.1 Learning & Inference

Model parameters are selected to maximize the regularized conditional log likelihood of recon-
structed observations ˆx given the structured observation x:

ℓℓ(λ  θ) = R1(λ) + R2(θ) + P(x ˆx)∈T log Py pλ(y | x) × pθ(ˆx | y)

(2)

We apply block coordinate descent  alternating between maximizing with respect to the CRF param-
eters (λ-step) and the reconstruction parameters (θ-step). Each λ-step applies one or two iterations
of a gradient-based convex optimizer.5 The θ-step applies one or two iterations of EM [10]  with a
closed-form solution in the M-step in each EM iteration. The independence assumptions among y
make the marginal inference required in both steps straightforward; we omit details for space.

In the experiments below  we apply a squared L2 regularizer for the CRF parameters λ  and a
symmetric Dirichlet prior for categorical parameters θ.

The asymptotic runtime complexity of each block coordinate descent iteration  assuming the ﬁrst-
order Markov dependencies in Fig. 2 (right)  is:

O (cid:0)|θ| + |λ| + |T | × |x|max × |Y|max × (|Y|max × |Fyi−1 yi | + |Fx yi |)(cid:1)

(3)

where Fyi−1 yi are the active “label bigram” features used in hyi−1  yii factors  Fx yi are the active
emission-like features used in hx  yii factors.
|x|max is the maximum length of an observation
sequence. |Y|max is the maximum cardinality6 of the set of possible assignments of yi.

After learning the λ and θ parameters of the CRF autoencoder  test-time predictions are made us-
ing maximum a posteriori estimation  conditioning on both observations and reconstructions  i.e. 
ˆyMAP = arg maxy pλ θ(y | x  ˆx).

3 Connections To Previous Work

This work relates to several strands of work in unsupervised learning. Two broad types of models
have been explored that support unsupervised learning with ﬂexible feature representations. Both are

5We experimented with AdaGrad [12] and L-BFGS. When using AdaGrad  we accummulate the gradient

vectors across block coordinate ascent iterations.

6In POS induction  |Y| is a constant  the number of syntactic classes which we conﬁgure to 12 in our exper-
iments. In word alignment  |Y| is the size of the source sentence plus one  therefore |Y|max is the maximum
length of a source sentence in the bitext corpus.

4

fully generative models that deﬁne joint distributions over x and y. We discuss these “undirected”
and “directed” alternatives next  then turn to less closely related methods.

3.1 Existing Alternatives for Unsupervised Learning with Features

Undirected models. A Markov random ﬁeld (MRF) encodes the joint distribution through local
potential functions parameterized using features. Such models “normalize globally ” requiring dur-
ing training the calculation of a partition function summing over all possible inputs and outputs. In
our notation:

Z(θ) = X
x∈X ∗

X
y∈Y |x|

exp λ⊤

¯g(x  y)

(4)

where ¯g collects all the local factorization by cliques of the graph  for clarity. The key difﬁculty
is in the summation over all possible observations. Approximations have been proposed  including
contrastive estimation  which sums over subsets of X ∗ [38  43] (applied variously to POS learning
by Haghighi and Klein [18] and word alignment by Dyer et al. [14]) and noise contrastive estimation
[30].

Directed models. The directed alternative avoids the global partition function by factorizing the
joint distribution in terms of locally normalized conditional probabilities  which are parameterized
in terms of features. For unsupervised sequence labeling  the model was called a “feature HMM”
by Berg-Kirkpatrick et al. [3]. The local emission probabilities p(xi | yi) in a ﬁrst-order HMM for
POS tagging are reparameterized as follows (again  using notation close to ours):

pλ(xi | yi) =

exp λ⊤

g(xi  yi)

Px∈X exp λ⊤

g(x  yi)

(5)

The features relating hidden to observed variables must be local within the factors implied by the
directed graph. We show below that this locality restriction excludes features that are useful (§A.1).

Put in these terms  the proposed autoencoding model is a hybrid directed-undirected model.

Asymptotic Runtime Complexity of Inference. The models just described cannot condition on
arbitrary amounts of x without increasing inference costs. Despite the strong independence assump-
tions of those models  the computational complexity of inference required for learning with CRF
autoencoders is better (§2.1).

Consider learning the parameters of an undirected model by maximizing likelihood of the observed
data. Computing the gradient for a training instance x requires time

O (cid:0)|λ| + |T | × |x| × |Y| × (|Y| × |Fyi−1 yi |+|X | × |Fxi yi |)(cid:1)  

where Fxi−yi are the emission-like features used in an arbitrary assignment of xi and yi. When the
multiplicative factor |X | is large  inference is slow compared to CRF autoencoders.

Inference in directed models is faster than in undirected models  but still slower than CRF autoen-
coder models. In directed models [3]  each iteration requires time

O (cid:0)|λ| + |T | × |x| × |Y| × (|Y| × |Fyi−1 yi | + |Fxi yi |)+|θ′| × max(|Fyi−1 yi |  |FX  yi |)(cid:1)  

where Fxi yi are the active emission features used in an arbitrary assignment of xi and yi  FX  yi
is the union of all emission features used with an arbitrary assignment of yi  and θ′ are the local
emission and transition probabilities. When |X | is large  the last term |θ′|×max(|Fyi−1 yi |  |FX  yi |)
can be prohibitively large.

3.2 Other Related Work

The proposed CRF autoencoder is more distantly related to several important ideas in less-than-
supervised learning.

5

Autoencoders and other “predict self” methods. Our framework borrows its general structure 
Fig. 2 (left)  as well as its name  from neural network autoencoders. The goal of neural autoencoders
has been to learn feature representations that improve generalization in otherwise supervised learn-
ing problems [44  8  39]. In contrast  the goal of CRF autoencoders is to learn speciﬁc interpretable
regularities of interest.7 It is not clear how neural autoencoders could be used to learn the latent
structures that CRF autoencoders learn  without providing supervised training examples. Stoyanov
et al. [40] presented a related approach for discriminative graphical model learning  including fea-
tures and latent variables  based on backpropagation  which could be used to instantiate the CRF
autoencoder.

Daum´e III [9] introduced a reduction of an unsupervised problem instance to a series of single-
variable supervised classiﬁcations. The ﬁrst series of these construct a latent structure y given the
entire x  then the second series reconstruct the input. The approach can make use of any supervised
learner; if feature-based probabilistic models were used  a |X | summation (akin to Eq. 5) would
be required. On unsupervised POS induction  this approach performed on par with the undirected
model of Smith and Eisner [38].

Minka [29] proposed cascading a generative model and a discriminative model  where class labels
(to be predicted at test time) are marginalized out in the generative part ﬁrst  and then (re)generated
in the discriminative part. In CRF autoencoders  observations (available at test time) are conditioned
on in the discriminative part ﬁrst  and then (re)generated in the generative part.

Posterior regularization.
Introduced by Ganchev et al. [16]  posterior regularization is an effec-
tive method for specifying constraint on the posterior distributions of the latent variables of interest;
a similar idea was proposed independently by Bellare et al. [2]. For example  in POS induction 
every sentence might be expected to contain at least one verb. This is imposed as a soft constraint 
i.e.  a feature whose expected value under the model’s posterior is constrained. Such expectation
constraints are speciﬁed directly by the domain-aware model designer.8 The approach was applied
to unsupervised POS induction  word alignment  and parsing. Although posterior regularization was
applied to directed feature-less generative models  the idea is orthogonal to the model family and
can be used to add more inductive bias for training CRF autoencoder models.

4 Evaluation

We evaluate the effectiveness of CRF autoencoders for learning from unlabeled examples in POS
induction and word alignment. We defer the detailed experimental setup to Appendix A.

Part-of-Speech Induction Results. Fig. 3 compares predictions of the CRF autoencoder model
in seven languages to those of a featurized ﬁrst-order HMM model [3] and a standard (feature-less)
ﬁrst-order HMM  using V-measure [37] (higher is better). First  note the large gap between both
feature-rich models on the one hand  and the feature-less HMM model on the other hand. Second 
note that CRF autoencoders outperform featurized HMMs in all languages  except Italian  with an
average relative improvement of 12%.

These results provide empirical evidence that feature engineering is an important source of inductive
bias for unsupervised structured prediction problems.
In particular  we found that using Brown
cluster reconstructions and specifying features which span multiple words signiﬁcantly improve the
performance. Refer to Appendix A for more analysis.

Bitext Word Alignment Results. First  we consider an intrinsic evaluation on a Czech-English
dataset of manual alignments  measuring the alignment error rate (AER; [32]). We also perform an

7This is possible in CRF autoencoders due to the interdependencies among variables in the hidden structure
and the manually speciﬁed feature templates which capture the relationship between observations and their
hidden structures.

8In a semi-supervised setting  when some labeled examples of the hidden structure are available  Druck
and McCallum [11] used labeled examples to estimate desirable expected values. We leave semi-supervised
applications of CRF autoencoders to future work; see also Suzuki and Isozaki [41].

6

Standard HMM
Featurized HMM
CRF autoencoder

e
r
u
s
a
e
m
−
V

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

Arabic

Basque

Danish

Greek Hungarian

Italian

Turkish

Average

Figure 3: V-measure [37] of induced parts of speech in seven languages. The CRF autoencoder with
features spanning multiple words and with Brown cluster reconstructions achieves the best results in
all languages but Italian  closely followed by the feature-rich HMM of Berg-Kirkpatrick et al. [3].
The standard multinomial HMM consistently ranks last.

direction
forward
reverse
symmetric

fast align model 4

27.7
25.9
25.2

31.5
24.1
22.2

auto
27.5
21.1
19.5

pair
cs-en
ur-en
zh-en

fast align model 4
15.3±0.1
15.2±0.3
20.1±0.6
20.0±0.6
56.9±1.6
56.7±1.6

auto

15.5±0.1
20.8±0.5
56.1±1.7

Table 1: Left: AER results (%) for Czech-English word alignment. Lower values are better. . Right:
Bleu translation quality scores (%) for Czech-English  Urdu-English and Chinese-English. Higher
values are better. .

extrinsic evaluation of translation quality in three language pairs  using case-insensitive Bleu [33] of
a machine translation system (cdec9 [13]) built using the word alignment predictions of each model.

AER for variants of each model (forward  reverse  and symmetrized) are shown in Table 1 (left).
Our model signiﬁcantly outperforms both baselines. Bleu scores on the three language pairs are
shown in Table 1; alignments obtained with our CRF autoencoder model improve translation quality
of the Czech-English and Urdu-English translation systems  but not of Chinese-English. This is un-
surprising  given that Chinese orthography does not use letters  so that source-language spelling and
morphology features our model incorporates introduce only noise here. Better feature engineering 
or more data  is called for.

We have argued that the feature-rich CRF autoencoder will scale better than its feature-rich alter-
natives. Fig. 5 (in Appendix A.2) shows the average per-sentence inference runtime for the CRF
autoencoder compared to exact inference in an MRF [14] with a similar feature set  as a function of
the number of sentences in the corpus. For CRF autoencoders  the average inference runtime grows
slightly due to the increased number of parameters  while it grows substantially with vocabulary size
in MRF models [14].10

5 Conclusion

We have presented a general and scalable framework to learn from unlabeled examples for structured
prediction. The technique allows features with global scope in observed variables with favorable
asymptotic inference runtime. We achieve this by embedding a CRF as the encoding model in the

9http://www.cdec-decoder.org/
10We only compare runtime  instead of alignment quality  because retraining the MRF model with exact

inference was too expensive.

7

input layer of an autoencoder  and reconstructing a transformation of the input at the output layer
using simple categorical distributions. The key advantages of the proposed model are scalability and
modeling ﬂexibility. We applied the model to POS induction and bitext word alignment  obtaining
results that are competitive with the state of the art on both tasks.

Acknowledgments

We thank Brendan O’Connor  Dani Yogatama  Jeffrey Flanigan  Manaal Faruqui  Nathan Schneider 
Phil Blunsom and the anonymous reviewers for helpful suggestions. We also thank Taylor Berg-
Kirkpatrick for providing his implementation of the POS induction baseline  and Phil Blunsom for
sharing POS induction evaluation scripts. This work was sponsored by the U.S. Army Research
Laboratory and the U.S. Army Research Ofﬁce under contract/grant number W911NF-10-1-0533.
The statements made herein are solely the responsibility of the authors.

References

[1] T. L. Bailey and C. Elkan. Unsupervised learning of multiple motifs in biopolymers using

expectation maximization. Machine learning  1995.

[2] K. Bellare  G. Druck  and A. McCallum. Alternating projections for learning with expectation

constraints. In Proc. of UAI  2009.

[3] T. Berg-Kirkpatrick  A. Bouchard-Cˆot´e  J. DeNero  and D. Klein. Painless unsupervised learn-

ing with features. In Proc. of NAACL  2010.

[4] P. Blunsom and T. Cohn. Discriminative word alignment with conditional random ﬁelds. In

Proc. of Proceedings of ACL  2006.

[5] P. F. Brown  P. V. deSouza  R. L. Mercer  V. J. D. Pietra  and J. C. Lai. Class-based n-gram

models of natural language. Computational Linguistics  1992.

[6] P. F. Brown  V. J. D. Pietra  S. A. D. Pietra  and R. L. Mercer. The mathematics of statistical

machine translation: parameter estimation. In Computational Linguistics  1993.

[7] S. Buchholz and E. Marsi. CoNLL-X shared task on multilingual dependency parsing.

In

CoNLL-X  2006.

[8] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep

neural networks with multitask learning. In Proc. of ICML  2008.

[9] H. Daum´e III. Unsupervised search-based structured prediction. In Proceedings of the 26th

Annual International Conference on Machine Learning  pages 209–216. ACM  2009.

[10] A. P. Dempster  N. M. Laird  D. B. Rubin  et al. Maximum likelihood from incomplete data

via the em algorithm. Journal of the Royal statistical Society  39(1):1–38  1977.

[11] G. Druck and A. McCallum. High-performance semi-supervised learning using discrimina-

tively constrained generative models. In Proc. of ICML  2010.

[12] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. JMLR  2011.

[13] C. Dyer  A. Lopez  J. Ganitkevitch  J. Weese  F. Ture  P. Blunsom  H. Setiawan  V. Eidelman 
and P. Resnik. cdec: A decoder  alignment  and learning framework for ﬁnite-state and context-
free translation models. In Proc. of ACL  2010.

[14] C. Dyer  J. Clark  A. Lavie  and N. A. Smith. Unsupervised word alignment with arbitrary

features. In Proc. of ACL-HLT  2011.

[15] C. Dyer  V. Chahuneau  and N. A. Smith. A simple  fast  and effective reparameterization of

IBM Model 2. In Proc. of NAACL  2013.

[16] K. Ganchev  J. Grac¸a  J. Gillenwater  and B. Taskar. Posterior regularization for structured

latent variable models. Journal of Machine Learning Research  11:2001–2049  2010.

[17] Q. Gao and S. Vogel. Parallel implementations of word alignment tool. In In Proc. of the ACL

workshop  2008.

[18] A. Haghighi and D. Klein. Prototype-driven learning for sequence models. In Proc. of NAACL-

HLT  2006.

[19] F. Jelinek. Statistical Methods for Speech Recognition. MIT Press  1997.

8

[20] M. Johnson. Why doesn’t EM ﬁnd good HMM POS-taggers? In Proc. of EMNLP  2007.

[21] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of depen-

dency and constituency. In Proc. of ACL  2004.

[22] P. Koehn. Statistical Machine Translation. Cambridge  2010.

[23] P. Koehn  F. J. Och  and D. Marcu. Statistical phrase-based translation. In Proc. of NAACL 

2003.

[24] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for

segmenting and labeling sequence data. In Proc. of ICML  2001.

[25] P. Liang. Semi-supervised learning for natural language. In Thesis  MIT  2005.

[26] C.-C. Lin  W. Ammar  C. Dyer  and L. Levin. The cmu submission for the shared task on lan-
guage identiﬁcation in code-switched data. In First Workshop on Computational Approaches
to Code Switching at EMNLP  2014.

[27] A. V. Lukashin and M. Borodovsky. Genemark. hmm: new solutions for gene ﬁnding. Nucleic

acids research  26(4):1107–1115  1998.

[28] B. Merialdo. Tagging english text with a probabilistic model. In Comp. Ling.  1994.

[29] T. Minka. Discriminative models  not discriminative training. Technical report  Technical

Report MSR-TR-2005-144  Microsoft Research  2005.

[30] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural probabilistic language

models. In Proc. of ICML  2012.

[31] J. Nivre  J. Hall  S. Kubler  R. McDonald  J. Nilsson  S. Riedel  and D. Yuret. The CoNLL

2007 shared task on dependency parsing. In Proc. of CoNLL  2007.

[32] F. Och and H. Ney. A systematic comparison of various statistical alignment models. Compu-

tational Linguistics  2003.

[33] K. Papineni  S. Roukos  T. Ward  and W.-J. Zhu. Bleu: a method for automatic evaluation of

machine translation. In Proc. of ACL  2002.

[34] S. Petrov  D. Das  and R. McDonald. A universal part-of-speech tagset. In Proc. of LREC 

May 2012.

[35] H. Poon and P. Domingos. Joint unsupervised coreference resolution with Markov logic. In

Proc. of EMNLP  2008.

[36] S. Reddy and S. Waxmonsky. Substring-based transliteration with conditional random ﬁelds.

In Proc. of the Named Entities Workshop  2009.

[37] A. Rosenberg and J. Hirschberg. V-measure: A conditional entropy-based external cluster

evaluation measure. In EMNLP-CoNLL  2007.

[38] N. A. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled

data. In Proc. of ACL  2005.

[39] R. Socher  C. D. Manning  and A. Y. Ng. Learning continuous phrase representations and

syntactic parsing with recursive neural networks. In NIPS workshop  2010.

[40] V. Stoyanov  A. Ropson  and J. Eisner. Empirical risk minimization of graphical model pa-
rameters given approximate inference  decoding  and model structure. In Proc. of AISTATS 
2011.

[41] J. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-

word scale unlabeled data. In Proc. of ACL  2008.

[42] R. Swier and S. Stevenson. Unsupervised semantic role labelling. In Proc. of EMNLP  2004.

[43] D. Vickrey  C. C. Lin  and D. Koller. Non-local contrastive objectives. In Proc. of ICML  2010.

[44] P. Vincent  H. Larochelle  Y. Bengio  and P.-A. Manzagol. Extracting and composing robust

features with denoising autoencoders. In Proc. of ICML  2008.

[45] S. Vogel  H. Ney  and C. Tillmann. Hmm-based word alignment in statistical translation. In

Proc. of COLING  1996.

[46] M. Weber  M. Welling  and P. Perona. Unsupervised learning of models for recognition. 2000.

[47] J. Yamato  J. Ohya  and K. Ishii. Recognizing human action in time-sequential images using

hidden Markov model. In Proc. of CVPR  pages 379–385. IEEE  1992.

9

,Waleed Ammar
Chris Dyer
Noah Smith
sabyasachi chatterjee
John Duchi
John Lafferty
Yuancheng Zhu
Rasmus Palm
Ulrich Paquet
Ole Winther