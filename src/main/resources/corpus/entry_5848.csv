2018,Variational Bayesian Monte Carlo,Many probabilistic models of interest in scientific computing and machine learning have expensive  black-box likelihoods that prevent the application of standard techniques for Bayesian inference  such as MCMC  which would require access to the gradient or a large number of likelihood evaluations.
We introduce here a novel sample-efficient inference framework  Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based  active-sampling Bayesian quadrature  using the latter to efficiently approximate the intractable integral in the variational objective.
Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence  useful for model selection.
We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to D = 10)  VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations  unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive  black-box likelihoods.,Variational Bayesian Monte Carlo

Luigi Acerbi∗

Department of Basic Neuroscience

University of Geneva

luigi.acerbi@unige.ch

Abstract

Many probabilistic models of interest in scientiﬁc computing and machine learning
have expensive  black-box likelihoods that prevent the application of standard
techniques for Bayesian inference  such as MCMC  which would require access
to the gradient or a large number of likelihood evaluations. We introduce here a
novel sample-efﬁcient inference framework  Variational Bayesian Monte Carlo
(VBMC). VBMC combines variational inference with Gaussian-process based 
active-sampling Bayesian quadrature  using the latter to efﬁciently approximate
the intractable integral in the variational objective. Our method produces both
a nonparametric approximation of the posterior distribution and an approximate
lower bound of the model evidence  useful for model selection. We demonstrate
VBMC both on several synthetic likelihoods and on a neuronal model with data
from real neurons. Across all tested problems and dimensions (up to D = 10) 
VBMC performs consistently well in reconstructing the posterior and the model
evidence with a limited budget of likelihood evaluations  unlike other methods that
work only in very low dimensions. Our framework shows great promise as a novel
tool for posterior and model inference with expensive  black-box likelihoods.

Introduction

1
In many scientiﬁc  engineering  and machine learning domains  such as in computational neuro-
science and big data  complex black-box computational models are routinely used to estimate model
parameters and compare hypotheses instantiated by different models. Bayesian inference allows us
to do so in a principled way that accounts for parameter and model uncertainty by computing the
posterior distribution over parameters and the model evidence  also known as marginal likelihood or
Bayes factor. However  Bayesian inference is generally analytically intractable  and the statistical
tools of approximate inference  such as Markov Chain Monte Carlo (MCMC) or variational inference 
generally require knowledge about the model (e.g.  access to the gradients) and/or a large number of
model evaluations. Both of these requirements cannot be met by black-box probabilistic models with
computationally expensive likelihoods  precluding the application of standard Bayesian techniques of
parameter and model uncertainty quantiﬁcation to domains that would most need them.
Given a dataset D and model parameters x ∈ RD  here we consider the problem of computing both
the posterior p(x|D) and the marginal likelihood (or model evidence) p(D)  deﬁned as  respectively 

p(D) =

p(D|x)p(x)dx 

and

p(D)

(1)
where p(D|x) is the likelihood of the model of interest and p(x) is the prior over parameters.
Crucially  we consider the case in which p(D|x) is a black-box  expensive function for which we
have a limited budget of function evaluations (of the order of few hundreds).
A promising approach to deal with such computational constraints consists of building a probabilistic
model-based approximation of the function of interest  for example via Gaussian processes (GP)

p(x|D) =

p(D|x)p(x)

(cid:90)

∗Website: luigiacerbi.com. Alternative e-mail: luigi.acerbi@gmail.com.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

[1]. This statistical surrogate can be used in lieu of the original  expensive function  allowing faster
computations. Moreover  uncertainty in the surrogate can be used to actively guide sampling of the
original function to obtain a better approximation in regions of interest for the application at hand.
This approach has been extremely successful in Bayesian optimization [2  3  4  5  6] and in Bayesian
quadrature for the computation of intractable integrals [7  8].
In particular  recent works have applied GP-based Bayesian quadrature to the estimation of the
marginal likelihood [8  9  10  11]  and GP surrogates to build approximations of the posterior [12  13].
However  none of the existing approaches deals simultaneously with posterior and model inference.
Moreover  it is unclear how these approximate methods would deal with likelihoods with realistic
properties  such as medium dimensionality (up to D ∼ 10)  mild multi-modality  heavy tails  and
parameters that exhibit strong correlations—all common issues of real-world applications.
In this work  we introduce Variational Bayesian Monte Carlo (VBMC)  a novel approximate inference
framework that combines variational inference and active-sampling Bayesian quadrature via GP
surrogates.1 Our method affords simultaneous approximation of the posterior and of the model
evidence in a sample-efﬁcient manner. We demonstrate the robustness of our approach by testing
VBMC and other inference algorithms on a variety of synthetic likelihoods with realistic  challenging
properties. We also apply our method to a real problem in computational neuroscience  by ﬁtting a
model of neuronal selectivity in visual cortex [14]. Among the tested methods  VBMC is the only one
with consistently good performance across problems  showing promise as a novel tool for posterior
and model inference with expensive likelihoods in scientiﬁc computing and machine learning.

2 Theoretical background
2.1 Variational inference
Variational Bayes is an approximate inference method whereby the posterior p(x|D) is approximated
by a simpler distribution q(x) ≡ qφ(x) that usually belongs to a parametric family [15  16]. The
goal of variational inference is to ﬁnd the variational parameters φ for which the variational posterior
qφ “best” approximates the true posterior. In variational methods  the mismatch between the two
distributions is quantiﬁed by the Kullback-Leibler (KL) divergence 
qφ(x)
p(x|D)

(2)
where we adopted the compact notation Eφ ≡ Eqφ. Inference is then reduced to an optimization
problem  that is ﬁnding the variational parameter vector φ that minimizes Eq. 2. We rewrite Eq. 2 as
(3)

log p(D) = F[qφ] + KL [qφ(x)||p(x|D)]  

KL [qφ(x)||p(x|D)] = Eφ

(cid:21)

(cid:20)

log

 

(cid:20)

(cid:21)

p(D|x)p(x)

where

F [qφ] = Eφ

= Eφ [f (x)] + H[qφ(x)]

log

qφ(x)

(4)
is the negative free energy  or evidence lower bound (ELBO). Here f (x) ≡ log p(D|x)p(x) =
log p(D  x) is the log joint probability and H[q] is the entropy of q. Note that since the KL divergence
is always non-negative  from Eq. 3 we have F[q] ≤ log p(D)  with equality holding if q(x) ≡ p(x|D).
Thus  maximization of the variational objective  Eq. 4  is equivalent to minimization of the KL
divergence  and produces both an approximation of the posterior qφ and a lower bound on the
marginal likelihood  which can be used as a metric for model selection.
Normally  q is chosen to belong to a family (e.g.  a factorized posterior  or mean ﬁeld) such that
the expected log joint in Eq. 4 and the entropy can be computed analytically  possibly providing
closed-form equations for a coordinate ascent algorithm. Here  we assume that f (x)  like many
computational models of interest  is an expensive black-box function  which prevents a direct
computation of Eq. 4 analytically or via simple numerical integration.

2.2 Bayesian quadrature

of the mean and variance of non-analytical integrals of the form (cid:104)f(cid:105) =(cid:82) f (x)π(x)dx  deﬁned on

Bayesian quadrature  also known as Bayesian Monte Carlo  is a means to obtain Bayesian estimates

1Code available at https://github.com/lacerbi/vbmc.

2

a domain X = RD [7  8]. Here  f is a function of interest and π a known probability distribution.
Typically  a Gaussian Process (GP) prior is speciﬁed for f (x).
Gaussian processes GPs are a ﬂexible class of models for specifying prior distributions over
unknown functions f : X ⊆ RD → R [1]. GPs are deﬁned by a mean function m : X → R and a
positive deﬁnite covariance  or kernel function κ : X × X → R. In Bayesian quadrature  a common
choice is the Gaussian kernel κ(x  x(cid:48)) = σ2
]  where
σf is the output length scale and (cid:96) is the vector of input length scales. Conditioned on training
inputs X = {x1  . . .   xn} and associated function values y = f (X)  the GP posterior will have latent
posterior conditional mean f Ξ(x) ≡ f (x; Ξ  ψ) and covariance CΞ(x  x(cid:48)) ≡ C(x  x(cid:48); Ξ  ψ) in
closed form (see [1])  where Ξ = {X  y} is the set of training function data for the GP and ψ is a
hyperparameter vector for the GP mean  covariance  and likelihood.
Bayesian integration Since integration is a linear operator  if f is a GP  the posterior mean and

variance of the integral(cid:82) f (x)π(x)dx are [8]

fN (x; x(cid:48)  Σ(cid:96))  with Σ(cid:96) = diag[(cid:96)(1)2

  . . .   (cid:96)(D)2

Ef|Ξ[(cid:104)f(cid:105)] =

f Ξ(x)π(x)dx 

Vf|Ξ[(cid:104)f(cid:105)] =

CΞ(x  x(cid:48))π(x)dxπ(x(cid:48))dx(cid:48).

(5)

(cid:90) (cid:90)

(cid:90)

Crucially  if f has a Gaussian kernel and π is a Gaussian or mixture of Gaussians (among other
functional forms)  the integrals in Eq. 5 can be computed analytically.
Active sampling For a given budget of samples nmax  a smart choice of the input samples X would
aim to minimize the posterior variance of the ﬁnal integral (Eq. 5) [11]. Interestingly  for a standard
GP and ﬁxed GP hyperparameters ψ  the optimal variance-minimizing design does not depend on the
function values at X  thereby allowing precomputation of the optimal design. However  if the GP
hyperparameters are updated online  or the GP is warped (e.g.  via a log transform [9] or a square-root
transform [10])  the variance of the posterior will depend on the function values obtained so far  and
an active sampling strategy is desirable. The acquisition function a : X → R determines which
point in X should be evaluated next via a proxy optimization xnext = argmaxxa(x). Examples of
acquisition functions for Bayesian quadrature include the expected entropy  which minimizes the
expected entropy of the integral after adding x to the training set [9]  and the much faster to compute
uncertainty sampling strategy  which maximizes the variance of the integrand at x [10].

3 Variational Bayesian Monte Carlo (VBMC)

We introduce here Variational Bayesian Monte Carlo (VBMC)  a sample-efﬁcient inference method
that combines variational Bayes and Bayesian quadrature  particularly useful for models with (moder-
ately) expensive likelihoods. The main steps of VBMC are described in Algorithm 1  and an example
run of VBMC on a nontrivial 2-D target density is shown in Fig. 1.
VBMC in a nutshell
In each iteration t  the algorithm: (1) sequentially samples a batch of
‘promising’ new points that maximize a given acquisition function  and evaluates the (expensive) log
joint f at each of them; (2) trains a GP model of the log joint f  given the training set Ξt = {Xt  yt}
of points evaluated so far; (3) updates the variational posterior approximation  indexed by φt  by
optimizing the ELBO. This loop repeats until the budget of function evaluations is exhausted  or some
other termination criterion is met (e.g.  based on the stability of the found solution). VBMC includes
an initial warm-up stage to avoid spending computations in regions of low posterior probability mass
(see Section 3.5). In the following sections  we describe various features of VBMC.

3.1 Variational posterior

We choose for the variational posterior q(x) a ﬂexible “nonparametric” family  a mixture of K
Gaussians with shared covariances  modulo a scaling factor 

K(cid:88)

wkN(cid:0)x; µk  σ2
kΣ(cid:1)  

q(x) ≡ qφ(x) =

(6)

where wk  µk  and σk are  respectively  the mixture weight  mean  and scale of the k-th component 
and Σ is a covariance matrix common to all elements of the mixture. In the following  we assume

k=1

3

(cid:46) Initial design  Section 3.5

t ← t + 1
if t (cid:44) 1 then

else

for 1 . . . nactive do

Evaluate y0 ← f (x0) and add (x0  y0) to the training set Ξ
for 2 . . . ninit do

Sample a new point xnew ← Uniform[PLB  PUB]
Evaluate ynew ← f (xnew) and add (xnew  ynew) to the training set Ξ

Algorithm 1 Variational Bayesian Monte Carlo
Input: target log joint f  starting point x0  plausible bounds PLB  PUB  additional options
1: Initialization: t ← 0  initialize variational posterior φ0  STOPSAMPLING ← false
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: until fevals > MaxFunEvals or TERMINATIONCRITERIA (cid:46) Stopping criteria  Section 3.7

else
Kt ← Update number of variational components
φt ← Optimize ELBO via stochastic gradient descent
Evaluate whether to STOPSAMPLING and other TERMINATIONCRITERIA

Actively sample a new point xnew ← argmaxxa(x)
Evaluate ynew ← f (xnew) and add (xnew  ynew) to the training set Ξ
for each ψ1  . . .   ψngp  perform rank-1 update of the GP posterior

22: return variational posterior φt  E [ELBO] (cid:112)V [ELBO]

if not STOPSAMPLING then

{ψ1  . . .   ψngp} ← Sample GP hyperparameters
ψ1 ← Optimize GP hyperparameters

(cid:46) GP hyperparameter training  Section 3.4

(cid:46) Active sampling  Section 3.3

(cid:46) Section 3.6
(cid:46) Section 3.2

a diagonal matrix Σ ≡ diag[λ(1)2
]. The variational posterior for a given number of
mixture components K is parameterized by φ ≡ (w1  . . .   wK  µ1  . . .   µK  σ1  . . .   σK  λ)  which
has K(D + 2) + D parameters. The number of components K is set adaptively (see Section 3.6).

  . . .   λ(D)2

3.2 The evidence lower bound

We approximate the ELBO (Eq. 4) in two ways. First  we approximate the log joint probability f with
a GP with a squared exponential (rescaled Gaussian) kernel  a Gaussian likelihood with observation
noise σobs > 0 (for numerical stability [17])  and a negative quadratic mean function  deﬁned as

(cid:17)2

(cid:16)

D(cid:88)

i=1

x(i) − x(i)

m
ω(i)2

m(x) = m0 − 1
2

 

(7)

where m0 is the maximum value of the mean  xm is the location of the maximum  and ω is a vector of
length scales. This mean function  unlike for example a constant mean  ensures that the posterior GP
predictive mean f is a proper log probability distribution (that is  it is integrable when exponentiated).
Crucially  our choice of variational family (Eq. 6) and kernel  likelihood and mean function of the
GP affords an analytical computation of the posterior mean and variance of the expected log joint
Eφ [f ] (using Eq. 5)  and of their gradients (see Supplementary Material for details). Second  we
approximate the entropy of the variational posterior  H [qφ]  via simple Monte Carlo sampling  and
we propagate its gradient through the samples via the reparametrization trick [18  19].2 Armed
with expressions for the mean expected log joint  the entropy  and their gradients  we can efﬁciently
optimize the (negative) mean ELBO via stochastic gradient descent [21].
Evidence lower conﬁdence bound We deﬁne the evidence lower conﬁdence bound (ELCBO) as

ELCBO(φ  f ) = Ef|Ξ [Eφ [f ]] + H[qφ] − βLCB

(8)

(cid:113)Vf|Ξ [Eφ [f ]]

where the ﬁrst two terms are the ELBO (Eq. 4) estimated via Bayesian quadrature  and the last term is
the uncertainty in the computation of the expected log joint multiplied by a risk-sensitivity parameter

2We also tried a deterministic approximation of the entropy proposed in [20]  with mixed results.

4

Figure 1: Example run of VBMC on a 2-D pdf. A. Contour plots of the variational posterior at
different iterations of the algorithm. Red crosses indicate the centers of the variational mixture
components  black dots are the training samples. B. ELBO as a function of iteration. Shaded area is
95% CI of the ELBO in the current iteration as per the Bayesian quadrature approximation (not the
error wrt ground truth). The black line is the true log marginal likelihood (LML). C. True target pdf.

βLCB (we set βLCB = 3 unless speciﬁed otherwise). Eq. 8 establishes a probabilistic lower bound on
the ELBO  used to assess the improvement of the variational solution (see following sections).

3.3 Active sampling

In VBMC  we are performing active sampling to compute a sequence of
integrals
Eφ1 [f ]   Eφ2 [f ]   . . .   EφT [f ]  across iterations 1  . . .   T such that (1) the sequence of variational
parameters φt converges to the variational posterior that minimizes the KL divergence with the true
posterior  and (2) we have minimum variance on our ﬁnal estimate of the ELBO. Note how this differs
from active sampling in simple Bayesian quadrature  for which we only care about minimizing the
variance of a single ﬁxed integral. The ideal acquisition function for VBMC will correctly balance
exploration of uncertain regions and exploitation of regions with high probability mass to ensure a
fast convergence of the variational posterior as closely as possible to the ground truth.
We describe here two acquisition functions for VBMC based on uncertainty sampling. Let VΞ(x) ≡
CΞ(x  x) be the posterior GP variance at x given the current training set Ξ. ‘Vanilla’ uncertainty
sampling for Eφ [f ] is aus(x) = VΞ(x)qφ(x)2  where qφ is the current variational posterior. Since
aus only maximizes the variance of the integrand under the current variational parameters  we expect it
to be lacking in exploration. To promote exploration  we introduce prospective uncertainty sampling 
(9)
where f Ξ is the GP posterior predictive mean. apro aims at reducing uncertainty of the variational
objective both for the current posterior and at prospective locations where the variational posterior
might move to in the future  if not already there (high GP posterior mean). The variational posterior
in apro acts as a regularizer  preventing active sampling from following too eagerly ﬂuctuations of the
GP mean. For numerical stability of the GP  we include in all acquisition functions a regularization
factor to prevent selection of points too close to existing training points (see Supplementary Material).
At the beginning of each iteration after the ﬁrst  VBMC actively samples nactive points (nactive = 5 by
default in this work). We select each point sequentially  by optimizing the chosen acquisition function
via CMA-ES [22]  and apply fast rank-one updates of the GP posterior after each acquisition.

apro(x) = VΞ(x)qφ(x) exp(cid:0)f Ξ(x)(cid:1)  

3.4 Adaptive treatment of GP hyperparameters

The GP model in VBMC has 3D + 3 hyperparameters  ψ = ((cid:96)  σf   σobs  m0  xm  ω). We impose an
empirical Bayes prior on the GP hyperparameters based on the current training set (see Supplementary

5

ACBIteration 1x1x2Iteration 5 (end of warm-up)x1x2Iteration 8x1x2Iteration 11x1x2Iteration 14x1x2Iteration 17x1x2True posteriorx1x2158111417Iterations-4-2.273Model evidenceELBOLMLngp(cid:88)

j=1

ngp(cid:88)

j=1

(cid:104){E [χ|ψj]}ngp

j=1

(cid:105)

 

(10)

Material)  and we sample from the posterior over hyperparameters via slice sampling [23]. In each
iteration  we collect ngp = round(80/
n) samples  where n is the size of the current GP training
set  with the rationale that we require less samples as the posterior over hyperparameters becomes
narrower due to more observations. Given samples {ψ} ≡ {ψ1  . . .   ψngp}  and a random variable χ
that depends on ψ  we compute the expected mean and variance of χ as

√

E [χ|{ψ}] =

1
ngp

E [χ|ψj]   V [χ|{ψ}] =

1
ngp

V [χ|ψj] + Var

where Var[·] is the sample variance. We use Eq. 10 to compute the GP posterior predictive mean and
variances for the acquisition function  and to marginalize the expected log joint over hyperparameters.
The algorithm adaptively switches to a faster maximum-a-posteriori (MAP) estimation of the hyper-
parameters (via gradient-based optimization) when the additional variability of the expected log joint
brought by multiple samples falls below a threshold for several iterations  a signal that sampling is
bringing little advantage to the precision of the computation.

3.5

Initialization and warm-up

The algorithm is initialized by providing a starting point x0 (ideally  in a region of high posterior
probability mass) and vectors of plausible lower/upper bounds PLB  PUB  that identify a region of high
posterior probability mass in parameter space. In the absence of other information  we obtained good
results with plausible bounds containing the peak of prior mass in each coordinate dimension  such
as the top ∼ 0.68 probability region (that is  mean ± 1 SD for a Gaussian prior). The initial design
consists of the provided starting point(s) x0 and additional points generated uniformly at random
inside the plausible box  for a total of ninit = 10 points. The plausible box also sets the reference
scale for each variable  and in future work might inform other aspects of the algorithm [6]. The
VBMC algorithm works in an unconstrained space (x ∈ RD)  but bound constraints to the variables
can be easily handled via a nonlinear remapping of the input space  with an appropriate Jacobian
correction of the log probability density [24] (see Section 4.2 and Supplementary Material).3
Warm-up We initialize the variational posterior with K = 2 components in the vicinity of x0 
and with small values of σ1  σ2  and λ (relative to the width of the plausible box). The algorithm
starts in warm-up mode  during which VBMC tries to quickly improve the ELBO by moving to
regions with higher posterior probability. During warm-up  K is clamped to only two components
with w1 ≡ w2 = 1/2  and we collect a maximum of ngp = 8 hyperparameter samples. Warm-up
ends when the ELCBO (Eq. 8) shows an improvement of less than 1 for three consecutive iterations 
suggesting that the variational solution has started to stabilize. At the end of warm-up  we trim
the training set by removing points whose value of the log joint probability y is more than 10 · D
points lower than the maximum value ymax observed so far. While not necessary in theory  we found
that trimming generally increases the stability of the GP approximation  especially when VBMC
is initialized in a region of very low probability under the true posterior. To allow the variational
posterior to adapt  we do not actively sample new points in the ﬁrst iteration after the end of warm-up.

3.6 Adaptive number of variational mixture components

After warm-up  we add and remove variational components following a simple set of rules.
Adding components We deﬁne the current variational solution as improving if the ELCBO of the
last iteration is higher than the ELCBO in the past few iterations (nrecent = 4). In each iteration  we
increment the number of components K by 1 if the solution is improving and no mixture component
was pruned in the last iteration (see below). To speed up adaptation of the variational solution
to a complex true posterior when the algorithm has nearly converged  we further add two extra
components if the solution is stable (see below) and no component was recently pruned. Each new
component is created by splitting and jittering a randomly chosen existing component. We set a
maximum number of components Kmax = n2/3  where n is the size of the current training set Ξ.
Removing components At the end of each variational optimization  we consider as a candidate for
pruning a random mixture component k with mixture weight wk < wmin. We recompute the ELCBO

3The available code for VBMC currently supports both unbounded variables and bound constraints.

6

without the selected component (normalizing the remaining weights). If the ‘pruned’ ELCBO differs
from the original ELCBO less than ε  we remove the selected component. We iterate the process
through all components with weights below threshold. For VBMC we set wmin = 0.01 and ε = 0.01.

3.7 Termination criteria
At the end of each iteration  we assign a reliability index ρ(t) ≥ 0 to the current variational solution
based on the following features: change in ELBO between the current and the previous iteration;
estimated variance of the ELBO; KL divergence between the current and previous variational posterior
(see Supplementary Material for details). By construction  a ρ(t) (cid:46) 1 is suggestive of a stable solution.
The algorithm terminates when obtaining a stable solution for nstable = 8 iterations (with at most one
non-stable iteration in-between)  or when reaching a maximum number nmax of function evaluations.
The algorithm returns the estimate of the mean and standard deviation of the ELBO (a lower bound
on the marginal likelihood)  and the variational posterior  from which we can cheaply draw samples
for estimating distribution moments  marginals  and other properties of the posterior. If the algorithm
terminates before achieving long-term stability  it warns the user and returns a recent solution with
the best ELCBO  using a conservative βLCB = 5.

4 Experiments

We tested VBMC and other common inference algorithms on several artiﬁcial and real problems con-
sisting of a target likelihood and an associated prior. The goal of inference consists of approximating
the posterior distribution and the log marginal likelihood (LML) with a ﬁxed budget of likelihood
evaluations  assumed to be (moderately) expensive.

Algorithms We tested VBMC with the ‘vanilla’ uncertainty sampling acquisition function aus
(VBMC-U) and with prospective uncertainty sampling  apro (VBMC-P). We also tested simple Monte
Carlo (SMC)  annealed importance sampling (AIS)  the original Bayesian Monte Carlo (BMC) 
doubly-Bayesian quadrature (BBQ [9])4  and warped sequential active Bayesian integration (WSABI 
both in its linearized and moment-matching variants  WSABI-L and WSABI-M [10]). For the basic
setup of these methods  we follow [10]. Most of these algorithms only compute an approximation
of the marginal likelihood based on a set of sampled points  but do not directly compute a posterior
distribution. We obtain a posterior by training a GP model (equal to the one used by VBMC) on
the log joint evaluated at the sampled points  and then drawing 2·104 MCMC samples from the
GP posterior predictive mean via parallel slice sampling [23  25]. We also tested two methods for
posterior estimation via GP surrogates  BAPE [12] and AGP [13]. Since these methods only compute
an approximate posterior  we obtain a crude estimate of the log normalization constant (the LML) as
the average difference between the log of the approximate posterior and the evaluated log joint at the
top 20% points in terms of posterior density. For all algorithms  we use default settings  allowing
only changes based on knowledge of the mean and (diagonal) covariance of the provided prior.
Procedure For each problem  we allow a ﬁxed budget of 50 × (D + 2) likelihood evaluations 
where D is the number of variables. Given the limited number of samples  we judge the quality
of the posterior approximation in terms of its ﬁrst two moments  by computing the “Gaussianized”
symmetrized KL divergence (gsKL) between posterior approximation and ground truth. The gsKL
is deﬁned as the symmetrized KL between two multivariate normal distributions with mean and
covariances equal  respectively  to the moments of the approximate posterior and the moments of
the true posterior. We measure the quality of the approximation of the LML in terms of absolute
error from ground truth  the rationale being that differences of LML are used for model comparison.
Ideally  we want the LML error to be of order 1 of less  since much larger errors could severely affect
the results of a comparison (e.g.  differences of LML of 10 points or more are often presented as
decisive evidence in favor of one model [26]). On the other hand  errors (cid:46) 0.1 can be considered
negligible; higher precision is unnecessary. For each algorithm  we ran at least 20 separate runs per
test problem with different random seeds  and report the median gsKL and LML error and the 95%
CI of the median calculated by bootstrap. For each run  we draw the starting point x0 (if requested
by the algorithm) uniformly from a box within 1 prior standard deviation (SD) from the prior mean.
We use the same box to deﬁne the plausible bounds for VBMC.

4We also tested BBQ* (approximate GP hyperparameter marginalization)  which perfomed similarly to BBQ.

7

4.1 Synthetic likelihoods
Problem set We built a benchmark set of synthetic likelihoods belonging to three families that
represent typical features of target densities (see Supplementary Material for details). Likelihoods
in the lumpy family are built out of a mixture of 12 multivariate normals with component means
drawn randomly in the unit D-hypercube  distinct diagonal covariances with SDs in the [0.2  0.6]
range  and mixture weights drawn from a Dirichlet distribution with unit concentration parameter.
The lumpy distributions are mildly multimodal  in that modes are nearby and connected by regions
with non-neglibile probability mass. In the Student family  the likelihood is a multivariate Student’s
t-distribution with diagonal covariance and degrees of freedom equally spaced in the [2.5  2 + D/2]
range across different coordinate dimensions. These distributions have heavy tails which might be
problematic for some methods. Finally  in the cigar family the likelihood is a multivariate normal
in which one axis is 100 times longer than the others  and the covariance matrix is non-diagonal
after a random rotation. The cigar family tests the ability of an algorithm to explore non axis-aligned
directions. For each family  we generated test functions for D ∈ {2  4  6  8  10}  for a total of 15
synthetic problems. For each problem  we pick as a broad prior a multivariate normal with mean
centered at the expected mean of the family of distributions  and diagonal covariance matrix with SD
equal to 3-4 times the SD in each dimension. For all problems  we compute ground truth values for
the LML and the posterior mean and covariance analytically or via multiple 1-D numerical integrals.
Results We show the results for D ∈ {2  6  10} in Fig. 2 (see Supplementary Material for full
results  in higher resolution). Almost all algorithms perform reasonably well in very low dimension
(D = 2)  and in fact several algorithms converge faster than VBMC to the ground truth (e.g.  WSABI-
L). However  as we increase in dimension  we see that all algorithms start failing  with only VBMC
peforming consistently well across problems. In particular  besides the simple D = 2 case  only
VBMC obtains acceptable results for the LML with non-axis aligned distributions (cigar). Some
algorithms (such as AGP and BAPE) exhibited large numerical instabilities on the cigar family 
despite our best attempts at regularization  such that many runs were unable to complete.

Figure 2: Synthetic likelihoods. A. Median absolute error of the LML estimate with respect to
ground truth  as a function of number of likelihood evaluations  on the lumpy (top)  Student (middle) 
and cigar (bottom) problems  for D ∈ {2  6  10} (columns). B. Median “Gaussianized” symmetrized
KL divergence between the algorithm’s posterior and ground truth. For both metrics  shaded areas
are 95 % CI of the median  and we consider a desirable threshold to be below one (dashed line).

4.2 Real likelihoods of neuronal model
Problem set For a test with real models and data  we consider a computational model of neuronal
orientation selectivity in visual cortex [14]. We ﬁt the neural recordings of one V1 and one V2
cell with the authors’ neuronal model that combines effects of ﬁltering  suppression  and response
nonlinearity [14]. The model is analytical but still computationally expensive due to large datasets
and a cascade of several nonlinear operations. For the purpose of our benchmark  we ﬁx some
parameters of the original model to their MAP values  yielding an inference problem with D = 7 free

8

ALumpyStudentCigarBsmcaisbmcwsabi-Lwsabi-Mbbqagpbapevbmc-Uvbmc-P2D10020010-410-21102104Median LML error6D20040010-410-2110210410D20040060010-410-2110210410020010-410-21102104Median LML error20040010-410-2110210420040060010-410-21102104100200Function evaluations10-410-21102104Median LMLerror200400Function evaluations10-410-21102104200400600Function evaluations10-410-211021042D10020010-410-21102104106Median gsKL6D20040010-410-2110210410610D20040060010-410-2110210410610020010-410-21102104106Median gsKL20040010-410-2110210410620040060010-410-21102104106100200Function evaluations10-410-21102104106Median gsKL200400Function evaluations10-410-21102104106200400600Function evaluations10-410-21102104106parameters of experimental interest. We transform bounded parameters to uncontrained space via a
logit transform [24]  and we place a broad Gaussian prior on each of the transformed variables  based
on estimates from other neurons in the same study [14] (see Supplementary Material for more details
on the setup). For both datasets  we computed the ground truth with 4· 105 samples from the posterior 
obtained via parallel slice sampling after a long burn-in. We calculated the ground truth LML from
posterior MCMC samples via Geyer’s reverse logistic regression [27]  and we independently validated
it with a Laplace approximation  obtained via numerical calculation of the Hessian at the MAP (for
both datasets  Geyer’s and Laplace’s estimates of the LML are within ∼ 1 point).

Figure 3: Neuronal model likelihoods. A. Median absolute error of the LML estimate  as a function
of number of likelihood evaluations  for two distinct neurons (D = 7). B. Median “Gaussianized”
symmetrized KL divergence between the algorithm’s posterior and ground truth. See also Fig. 2.

Results For both datasets  VBMC is able to ﬁnd a reasonable approximation of the LML and
of the posterior  whereas no other algorithm produces a usable solution (Fig. 3). Importantly  the
behavior of VBMC is fairly consistent across runs (see Supplementary Material). We argue that
the superior results of VBMC stem from a better exploration of the posterior landscape  and from
a better approximation of the log joint (used in the ELBO)  related but distinct features. To show
this  we ﬁrst trained GPs (as we did for the other methods) on the samples collected by VBMC (see
Supplementary Material). The posteriors obtained by sampling from the GPs trained on the VBMC
samples scored a better gsKL than the other methods (and occasionally better than VBMC itself).
Second  we estimated the marginal likelihood with WSABI-L using the samples collected by VBMC.
The LML error in this hybrid approach is much lower than the error of WSABI-L alone  but still
higher than the LML error of VBMC. These results combined suggest that VBMC builds better (and
more stable) surrogate models and obtains higher-quality samples than the compared methods.
The performance of VBMC-U and VBMC-P is similar on synthetic functions  but the ‘prospective’
acquisition function converges faster on the real problem set  so we recommend apro as the default.
Besides scoring well on quantitative metrics  VBMC is able to capture nontrivial features of the true
posteriors (see Supplementary Material for examples). Moreover  VBMC achieves these results with
a relatively small computational cost (see Supplementary Material for discussion).

5 Conclusions

In this paper  we have introduced VBMC  a novel Bayesian inference framework that combines
variational inference with active-sampling Bayesian quadrature for models with expensive black-box
likelihoods. Our method affords both posterior estimation and model inference by providing an
approximate posterior and a lower bound to the model evidence. We have shown on both synthetic
and real model-ﬁtting problems that  given a contained budget of likelihood evaluations  VBMC is
able to reliably compute valid  usable approximations in realistic scenarios  unlike previous methods
whose applicability seems to be limited to very low dimension or simple likelihoods. Our method 
thus  represents a novel useful tool for approximate inference in science and engineering.
We believe this is only the starting point to harness the combined power of variational inference and
Bayesian quadrature. Not unlike the related ﬁeld of Bayesian optimization  VBMC paves the way to
a plenitude of both theoretical (e.g.  analysis of convergence  development of principled acquisition
functions) and applied work (e.g.  application to case studies of interest  extension to noisy likelihood
evaluations  algorithmic improvements)  which we plan to pursue as future directions.

9

V1NeuronalmodelV1V2ABV2200400Function evaluations10-21102104Median LML error200400Function evaluations10-21102104200400Function evaluations10-21102104106Median gsKL200400Function evaluations10-21102104106smcaisbmcwsabi-Lwsabi-Mbbqagpbapevbmc-Uvbmc-PAcknowledgments

We thank Robbe Goris for sharing data and code for the neuronal model; Michael Schartner and
Rex Liu for comments on an earlier version of the paper; and three anonymous reviewers for useful
feedback.

References
[1] Rasmussen  C. & Williams  C. K. I. (2006) Gaussian Processes for Machine Learning. (MIT Press).
[2] Jones  D. R.  Schonlau  M.  & Welch  W. J. (1998) Efﬁcient global optimization of expensive black-box

functions. Journal of Global Optimization 13  455–492.

[3] Brochu  E.  Cora  V. M.  & De Freitas  N. (2010) A tutorial on Bayesian optimization of expensive cost
functions  with application to active user modeling and hierarchical reinforcement learning. arXiv preprint
arXiv:1012.2599.

[4] Snoek  J.  Larochelle  H.  & Adams  R. P. (2012) Practical Bayesian optimization of machine learning

algorithms. Advances in Neural Information Processing Systems 25  2951–2959.

[5] Shahriari  B.  Swersky  K.  Wang  Z.  Adams  R. P.  & de Freitas  N. (2016) Taking the human out of the

loop: A review of Bayesian optimization. Proceedings of the IEEE 104  148–175.

[6] Acerbi  L. & Ma  W. J. (2017) Practical Bayesian optimization for model ﬁtting with Bayesian adaptive

direct search. Advances in Neural Information Processing Systems 30  1834–1844.

[7] O’Hagan  A. (1991) Bayes–Hermite quadrature. Journal of Statistical Planning and Inference 29  245–260.
[8] Ghahramani  Z. & Rasmussen  C. E. (2002) Bayesian Monte Carlo. Advances in Neural Information

Processing Systems 15  505–512.

[9] Osborne  M.  Duvenaud  D. K.  Garnett  R.  Rasmussen  C. E.  Roberts  S. J.  & Ghahramani  Z. (2012)
Active learning of model evidence using Bayesian quadrature. Advances in Neural Information Processing
Systems 25  46–54.

[10] Gunter  T.  Osborne  M. A.  Garnett  R.  Hennig  P.  & Roberts  S. J. (2014) Sampling for inference in
probabilistic models with fast Bayesian quadrature. Advances in Neural Information Processing Systems
27  2789–2797.

[11] Briol  F.-X.  Oates  C.  Girolami  M.  & Osborne  M. A.

(2015) Frank-Wolfe Bayesian quadrature:
Probabilistic integration with theoretical guarantees. Advances in Neural Information Processing Systems
28  1162–1170.

[12] Kandasamy  K.  Schneider  J.  & Póczos  B. (2015) Bayesian active learning for posterior estimation.

Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence.

[13] Wang  H. & Li  J. (2018) Adaptive Gaussian process approximation for Bayesian inference with expensive

likelihood functions. Neural Computation pp. 1–23.

[14] Goris  R. L.  Simoncelli  E. P.  & Movshon  J. A. (2015) Origin and function of tuning diversity in macaque

visual cortex. Neuron 88  819–831.

[15] Jordan  M. I.  Ghahramani  Z.  Jaakkola  T. S.  & Saul  L. K. (1999) An introduction to variational methods

for graphical models. Machine Learning 37  183–233.

[16] Bishop  C. M. (2006) Pattern Recognition and Machine Learning. (Springer).
[17] Gramacy  R. B. & Lee  H. K. (2012) Cases for the nugget in modeling computer experiments. Statistics

and Computing 22  713–722.

[18] Kingma  D. P. & Welling  M. (2013) Auto-encoding variational Bayes. Proceedings of the 2nd International

Conference on Learning Representations.
[19] Miller  A. C.  Foti  N.  & Adams  R. P.

(2017) Variational boosting: Iteratively reﬁning posterior
approximations. Proceedings of the 34th International Conference on Machine Learning 70  2420–2429.
[20] Gershman  S.  Hoffman  M.  & Blei  D. (2012) Nonparametric variational inference. Proceedings of the

29th International Coference on Machine Learning.

[21] Kingma  D. P. & Ba  J. (2014) Adam: A method for stochastic optimization. Proceedings of the 3rd

International Conference on Learning Representations.

[22] Hansen  N.  Müller  S. D.  & Koumoutsakos  P. (2003) Reducing the time complexity of the derandomized

evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation 11  1–18.

[23] Neal  R. M. (2003) Slice sampling. Annals of Statistics 31  705–741.

10

[24] Carpenter  B.  Gelman  A.  Hoffman  M. D.  Lee  D.  Goodrich  B.  Betancourt  M.  Brubaker  M.  Guo  J. 
Li  P.  & Riddell  A. (2017) Stan: A probabilistic programming language. Journal of Statistical Software
76.

[25] Gilks  W. R.  Roberts  G. O.  & George  E. I. (1994) Adaptive direction sampling. The Statistician 43 

179–189.

[26] Kass  R. E. & Raftery  A. E. (1995) Bayes factors. Journal of the American Statistical Association 90 

773–795.

[27] Geyer  C. J. (1994) Estimating normalizing constants and reweighting mixtures. (Technical report).

11

,Luigi Acerbi