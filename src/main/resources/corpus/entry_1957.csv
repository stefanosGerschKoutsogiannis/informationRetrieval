2014,Extracting Latent Structure From Multiple Interacting Neural Populations,Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously  as well as the identification of the types of neurons being recorded (e.g.  excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple  labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared)  we propose to extract a smaller number of latent variables from each population and study how the latent variables interact. Specifically  we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables  as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis  gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2  and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.,Extracting Latent Structure From Multiple

Interacting Neural Populations

Jo˜ao D. Semedo1 2 3  Amin Zandvakili4  Adam Kohn4 

∗Christian K. Machens3  ∗Byron M. Yu1 5

1Department of Electrical and Computer Engineering  Carnegie Mellon University
2Department of Electrical and Computer Engineering  Instituto Superior T´ecnico

3Champalimaud Neuroscience Programme  Champalimaud Center for the Unknown
4Dominick Purpura Department of Neuroscience  Albert Einstein College of Medicine

5Department of Biomedical Engineering  Carnegie Mellon University

jsemedo@cmu.edu
christian.machens@neuro.fchampalimaud.org

{amin.zandvakili adam.kohn}@einstein.yu.edu
byronyu@cmu.edu

∗ Denotes equal contribution.

Abstract

Developments in neural recording technology are rapidly enabling the record-
ing of populations of neurons in multiple brain areas simultaneously  as well as
the identiﬁcation of the types of neurons being recorded (e.g.  excitatory vs. in-
hibitory). There is a growing need for statistical methods to study the interaction
among multiple  labeled populations of neurons. Rather than attempting to iden-
tify direct interactions between neurons (where the number of interactions grows
with the number of neurons squared)  we propose to extract a smaller number
of latent variables from each population and study how these latent variables in-
teract. Speciﬁcally  we propose extensions to probabilistic canonical correlation
analysis (pCCA) to capture the temporal structure of the latent variables  as well
as to distinguish within-population dynamics from between-population interac-
tions (termed Group Latent Auto-Regressive Analysis  gLARA). We then applied
these methods to populations of neurons recorded simultaneously in visual areas
V1 and V2  and found that gLARA provides a better description of the recordings
than pCCA. This work provides a foundation for studying how multiple popula-
tions of neurons interact and how this interaction supports brain function.

1

Introduction

In recent years  developments in neural recording technologies have enabled the recording of popu-
lations of neurons from multiple brain areas simultaneously [1–7]. In addition  it is rapidly becom-
ing possible to identify the types of neurons being recorded (e.g.  excitatory versus inhibitory [8]).
Enabled by these experimental advances  a major growing line of scientiﬁc inquiry is to ask how dif-
ferent populations of neurons interact  whether the populations correspond to different brain areas or
different neuron types. To address such questions  we need statistical methods that are well-suited
for assessing how different groups of neurons interact on a population level.
One way to characterize multi-population activity is to have the neurons interact directly [9–11] 
then examine the properties of the interaction strengths. While this may be a reasonable approach
for small populations of neurons  the number of interactions grows with the square of the number
of recorded neurons  which may make it difﬁcult to summarize how larger populations of neurons
interact [12]. Instead  it may be possible to obtain a more succinct account by extracting latent
variables for each population and asking how these latent variables interact.

1

Figure 1: Directed graphical models for multi-population activity. (a) Probabilistic canonical
correlation analysis (pCCA). (b) pCCA with auto-regressive latent dynamics (AR-pCCA). (c) Group
latent auto-regressive analysis (gLARA). For clarity  we show only two populations in each panel
and auto-regressive dynamics of order 1 in panel (c).

Dimensionality reduction methods have been widely used to extract succinct representations of pop-
ulation activity [13–17] (see [18] for a review). Each observed dimension corresponds to the spike
count (or ﬁring rate) of a neuron  and the goal is to extract latent variables that describe how the pop-
ulation activity varies across experimental conditions  experimental trials  and/or across time. These
previous studies use dimensionality reduction methods that do not explicitly account for multiple
populations of neurons. In other words  these methods are invariant to permutations of the ordering
of the neurons (i.e.  the observed dimensions).
This work focuses on latent variable methods designed explicitly for studying the interaction be-
tween labelled populations of neurons. To motivate the need for these methods  consider applying
a standard dimensionality reduction method  such as factor analysis (FA) [19]  to all neurons to-
gether by ignoring the population labels. The extracted latent variables would capture all modes
of covariability across the neurons  without distinguishing between-population interaction (i.e.  the
quantity of interest) from within-population interaction. Alternatively  one might ﬁrst apply a stan-
dard dimensionality reduction method to each population of neurons individually  then examine
how the latent variables extracted from each population interact. However  important features of the
between-population interaction may be eliminated by the dimensionality reduction step  whose sole
objective is to preserve the within-population interaction.
We begin by considering canonical correlations analysis (CCA) and its probabilistic formulation
(pCCA) [20]  which identify a single set of latent variables that explicitly captures the between-
population covariability. To understand how the different neural populations interact on different
timescales  we propose extensions of pCCA that introduce a separate set of latent variables for each
neural population  as well as dynamics on the latent variables to describe their interaction over time.
We then apply the proposed methods to populations of neurons recorded simultaneously in visual
areas V1 and V2 to demonstrate their utility.

2 Methods

We consider the setting where many neurons are recorded simultaneously  and the neurons belong
t ∈ Rqi represent the observed
to distinct populations (either by brain area or by neuron type). Let yi
activity vector of population i ∈ {1  ...  M} at time t ∈ {1  ...  T}  where qi denotes the number of
neurons in population i. Below  we consider three different ways to study the interaction between
the neural populations. To keep the notation simple  we’ll only consider two populations (M = 2);
the extension to more than two populations is straightforward.

2.1 Factor analysis and probabilistic canonical correlation analysis

Consider the following latent variable model  that deﬁnes a linear-Gaussian relationship between the
observed variables  y1

t   and the latent state  xt ∈ Rp:

t and y2

xt ∼ N (0  I)

2

(1)

(a) pCCA(b) AR-pCCA(c) gLARA(cid:20) y1

(cid:21)

| xt ∼ N
where C i ∈ Rqi×p  di ∈ Rqi and:

t
y2
t

(cid:21)(cid:19)

(cid:18)(cid:20) y1

t
y2
t

cov

=

xt +

C 2

(cid:18)(cid:20) C 1
(cid:21)
(cid:20) R11 R12
(cid:20) C 1

(cid:21)(cid:104)

R12T

R22

C 1T

C 2

 

d2

(cid:21)

(cid:20) d1
(cid:21)
C 2T (cid:105)

∈ Sq

++

(cid:20) R11 R12

R12T

R22

(cid:21)(cid:19)

(cid:20) R11 R12

R12T

R22

(cid:21)

+

with q = q1 + q2. According to this model  the covariance of the observed variables is given by:

(2)

(3)

1  ...  r1
q1

)  R22 = diag(r2

Factor analysis (FA) and probabilistic canonical correlation analysis (pCCA) can be seen as two
special cases of the general model presented above. FA assumes the noise covariance to be diagonal 
i.e.  R11 = diag(r1
) and R12 = 0. This noise covariance captures
only the independent variance of each neuron  and not the covariance between neurons. As a result 
the covariance between neurons is explained by the latent state through the observation matrices C 1
and C 2. pCCA  on the other hand  considers a block diagonal noise covariance  i.e.  R12 = 0. This
noise covariance accounts for the covariance observed between neurons in the same population. The
latent state is therefore only used to explain the covariance between neurons in different populations.
The directed graphical model for pCCA is shown in Fig.1a.

1  ...  r2
q2

2.2 Auto-regressive probabilistic canonical correlation analysis (AR-pCCA)

While pCCA offers a succinct picture of the covariance structure between populations of neurons 
it does not capture any temporal structure. There are two main reasons as to why this time structure
may be interesting. First  pCCA is modelling the covariance structure at zero time lag  which may
not capture all of the interactions of interest. If the two populations of neurons correspond to two
different brain areas  there may be important interactions at non-zero time lags due to physical delays
in information transmission. Second  the two populations of neurons may interact at more than one
time delay  for example if multiple pathways exist between the neurons in these populations. To
take the temporal structure into account we will ﬁrst extend pCCA by deﬁning an auto-regressive
linear-Gaussian model on the latent state:

xt ∼ N (0  I) 

xt | xt−1  xt−2  ...  xt−τ ∼ N

(cid:32) τ(cid:88)

if 1 ≤ t ≤ τ

(cid:33)

Akxt−k  Q

 

if t > τ

(4)

(5)

where Ak ∈ Rp×p  ∀k  Q ∈ Sp
++ and τ denotes the order of the autoregressive model. We term this
model AR-pCCA  which is deﬁned by the state model in Eq.(4)-(5) and the observation model in
Eq.(2) with R12 = 0. Although the observation model is the same as that for pCCA  the latent state
here accounts for temporal dynamics  as well as the covariation structure between the populations.
The corresponding directed graphical model is shown in Fig.1b.

k=1

2.3 Group latent auto-regressive analysis (gLARA)

According to AR-pCCA  a single latent state drives the observed activity in both areas. As a result 
it’s not possible to distinguish the within-population dynamics from the between-population interac-
tions. To allow for this  we propose using two separate latent states  one per population  that interact
over time. We refer to the proposed model as group latent auto-regressive analysis (gLARA):

xt ∼ N (0  I) 

if 1 ≤ t ≤ τ

t | xt−1  xt−2  ...  xt−τ ∼ N
xi

Aij

k xj

t−k  Qi

 

if t > τ

(6)

(7)

 2(cid:88)

τ(cid:88)

j=1

k=1

3

(cid:21)

(cid:21)

(cid:21)(cid:19)

| xt ∼ N

(cid:20) y1

(cid:20) d1

(cid:20) R1

(cid:21)(cid:20) x1

(cid:18)(cid:20) C 1

t
y2
t
k ∈ Rpi×pj and Qi ∈ Spi

(cid:21)
0
(8)
t
x2
0 C 2
t
t ∈ Rp1 and x2
t ∈ Rp2  the latent states for each population 
where xt is obtained by stacking x1
++  ∀k and i ∈ {1  2}. Note that the covariance structure
C i ∈ Rqi×pi  Aij
observed on a population level now has to be completely reﬂected by the latent states (there are no
shared latent variables in this model) and is therefore deﬁned by the dynamics matrices Aij
k   allowing
for the separation of the within-population dynamics (A11
k ) and the between-population
k ). Furthermore  the interaction between the populations is asymmetrically
interactions (A12
deﬁned by A12
k   allowing for a more in depth study of the way in each the two areas interact
by comparing these across the various time delays considered. Note that gLARA represents a special
case of the AR-pCCA model.

0
0 R2

k and A22

k and A21

k and A21

d2

+

 

2.4 Parameter estimation for gLARA

(cid:21)

(cid:20) ¯x1

t
¯x2
t

(cid:104)

The parameters of gLARA can be ﬁt to the training data using the expectation-maximization (EM)
algorithm. To do so  we start by deﬁning the augmented latent state ¯xt ∈ Rpτ   with p = p1 + p2:

¯xt =

=

T

x1
t

. . . x1

t−τ

T x2
t

T

. . . x2

t−τ

and the augmented observation vector ¯yt ∈ Rq  with q = q1 + q2:

(cid:104)

T(cid:105)T

(10)
for t ∈ {τ  ...  T}. Using the augmented latent state ¯x  the dynamics equation (Eq.(6) and (7)) can
be rewritten as:

T y2
t

¯yt =

y1
t

T(cid:105)T

if t = τ

¯xt ∼ N (0  I) 

¯xt | ¯xt−1 ∼ N(cid:0) ¯A¯xt−1  ¯Q(cid:1) 
(cid:20) ¯xt
(cid:21)

(cid:18)

¯yt | ¯xt ∼ N

¯C

(cid:19)

  ¯R

for appropriately structured ¯A ∈ Rpτ×pτ and ¯Q ∈ Spτ
rewritten as:

if t > τ

(12)
++. The observation model (Eq.(8)) can be

1
for appropriately structured ¯C ∈ Rq×(pτ +1) and ¯R ∈ Sq
++. Due to space constraints  we will not
explicitly show the structure of the augmented parameters ¯θ = { ¯C  ¯R  ¯A  ¯Q}. It is straightforward
to derive them by inspection of Eq.(9)-(13).
We ﬁt the model parameters using the EM algorithm. In the E-step  because the latent and observed
variables are jointly Gaussian  P (¯xt | ¯y1  ...  ¯yT ) is also Gaussian and can be computed exactly by
applying the forward-backward recursion of the Kalman smoother [21] on the augmented vectors. In
the M-step  we directly estimate the original parameters θ = {C i  di  Ri  Aij
k }  as opposed to esti-
mating the structured form of the augmented parameters ¯θ = { ¯C  ¯R  ¯A} (without loss of generality 
we set Qi = I):

(cid:104) E(xi

t

T )

1

(cid:105)(cid:33)(cid:32) T(cid:88)

(cid:34) E(xi

T ) E(xi
txi
t)
t
E(xi
T )
1

t

t=1

(cid:35)(cid:33)−1

(cid:2) C i di (cid:3) =

(9)

(11)

(13)

(cid:32) T(cid:88)
T(cid:88)

t=1

1
T

t=1

yi
t

Ri =

T − di) − C iE(xi

t)(yi

{(yi

−(yi

t

t − di)(yi
t − di)E(xi
(cid:21)

(cid:32) T(cid:88)

T

t

=

. . . A12
k
. . . A22
k

t − di)T
)C iT}

T

)C iT

+ C iE(xi

txi
t

(cid:1)(cid:33)(cid:32) T(cid:88)

E(cid:0)¯xt ¯xT

t−1

E(cid:0)¯xt−1 ¯xT

t−1

t=2

t=2

(cid:20) A11

1
A21
1

. . . A11
. . . A21

k A12
1
k A22
1

To initialize the EM algorithm  we start by applying FA to each population individually  and use
the estimated observation matrices C 1 and C 2  as well as the mean vectors d1 and d2 and the
observation covariance matrices R11 and R22. The Aij

k matrices are initialized at 0.

4

(14)

(15)

(16)

(cid:1)(cid:33)−1

Figure 2: Comparing the optimal dimensionality for FA and pCCA. Cross-validated log-
likelihood plotted as a function of the dimensionality of the latent state for FA (black) and pCCA
(blue). pCCA was also applied to the same data after randomly shufﬂing the population labels
(green). Note that the maximum possible dimensionality for pCCA is 31  which is the size of the
smaller of the two populations (in this case  V2).

2.5 Neural recordings

The methods described above were applied to multi-electrode recordings performed simultaneously
in visual area 1 (V1) and visual area 2 (V2) of an anaesthetised monkey  while the monkey was
shown a set of oriented gratings with 8 different orientations. Each of the 8 orientations was shown
400 times for a period of 1.28s  providing a total of 3200 trials. We used 1.23s of data in each trial 
from 50ms after stimulus onset until the end of the trial  and proceeded to bin the observed spikes
with a 5ms window. The recordings include a total of 97 units in V1 and 31 units in V2 (single- and
multi-units). For model comparison  we performed 4-fold cross-validation  splitting the data into
four non-overlapping test folds with 250 trials each. We chose to analyze a subset of the trials for
rapid iteration of the analyses  as the cross-validation procedure is computationally expensive for
the full dataset. Given that 1000 trials provides a total of 246 000 timepoints (at 5 ms resolution) 
this provides a reasonable amount of data to ﬁt any of the models with the 128 observed neurons.
In this study  we sought to investigate how trial-to-trial population variability in V1 relates to the
trial-to-trial population variability in V2. For these gratings stimuli (which are relatively simple
compared to naturalistic stimuli [22])  there is likely richer structure in the V1-V2 interaction for
the trial-to-trial variability than for the stimulus drive. To this end  we preprocessed the neural
activity by computing the peristimulus time histogram (PSTH)  representing the trial-averaged ﬁring
rate timecourse  for each neuron and experimental condition (grating orientation). For each spike
train  we then subtracted the appropriate PSTH from the binned spike counts to obtain a single-trial
“residual”. The residuals across all neurons and conditions were considered together in the analyses
shown in Section 3. Note that the methods considered in this study could also be applied to the
PSTHs of sequentially recorded neurons in multiple areas.

3 Results

We started by asking how many dimensions are needed to describe the between-population covari-
ance  relative to the number of dimensions needed to describe the within-population covariance.
This was assessed by applying pCCA to the labeled V1 and V2 populations  as well as FA to the two
populations together (which ignores the V1 and V2 labels). In this analysis  pCCA captures only
the between-population covariance  whereas FA captures both the between-population and within-
population covariance. By comparing cross-validated data likelihoods for different dimensionalities 
we found that pCCA required three latent dimensions  whereas FA required 40 latent dimensions
(Fig.2). This indicates that the zero time lag interaction between V1 and V2 is conﬁned to a small
number of dimensions (three) relative to the number of dimensions (40) needed to describe all co-
variance among the neurons. The difference of these two dimensionalities (37) describes covariance
that is ‘private’ to each population (i.e.  within-population covariance). The FA and pCCA curves
peak at similar cross-validated likelihoods in Fig.2 because the observation model for pCCA Eq.(2)
accounts for the within-population covariance (which is not captured by the pCCA latents).

5

102030405060−4.795−4.77x 105FApCCA shu"edpCCAlatent dimensionalitycross−validated log−likelihoodFigure 3: Model selection for AR-pCCA and gLARA. (a) Comparing AR-pCCA and gLARA as
a function of the latent dimensionality (deﬁned as p1 + p2 for gLARA  where p2 was ﬁxed at 15) 
for τ = 3. (b) gLARA’s cross-validated log-likelihood plotted as a function of the dimensionality
of V1’s latent state  p1 (for p2 = 15)  for different choices of τ. (c) gLARA’s cross-validated log-
likelihood plotted as a function of the dimensionality of V2’s latent state  p2 (for p1 = 50)  for
different choices of τ.

The distinction between within-population covariance and between-population covariance is further
supported by re-applying pCCA  but now randomly shufﬂing the population labels. The cross-
validated log-likelihood curve for these mixed populations now peaks at a larger dimensionality
than three. The reason is that the shufﬂing procedure removes the distinction between the two types
of covariance  such that the pCCA latents now capture both types of covariance (of the original
unmixed populations). The peak for mixed pCCA occurs at a lower dimensionality than for FA for
two reasons: i) because the mixed populations have the same number of neurons as the original
populations (97 and 31)  the maximum number of dimensions that can be identiﬁed by pCCA is 31 
and ii) for the same latent dimensionality  pCCA has a larger number of parameters than FA  which
makes pCCA more prone to overﬁtting.
Together  the analyses in Fig.2 demonstrate two key points. First  if the focus of the analysis lies
in the interaction between populations  then pCCA provides a more parsimonious description  as it
focuses exclusively on the covariance between populations. In contrast  FA is unable to distinguish
within-population covariance from between-population covariance. Second  the neuron groupings
for V1 and V2 are meaningful  as the number of dimensions needed to describe the covariance
between V1 and V2 is small relative to that within each population.
We then analysed the performance of the models with latent dynamics (AR-pCCA and gLARA).
The cross-validated log-likelihood for these models depends jointly on the dimensionality of the
latent state  p  and the order of the auto-regressive model  τ. For gLARA  p is the sum of the di-
mensionalities of each population’s latent state  p1 + p2  and we therefore want to jointly maximize
the cross-validated log-likelihood with respect to both p1 and p2. AR-pCCA required a latent di-
mensionality of p = 70  while gLARA peaked for a joint latent dimensionality of 65 (p1 = 50 and
p2 = 15) (Fig.3a). When computing the performance of AR-pCCA we considered models with
p ∈ {5  10  ...  75} and τ ∈ {1  3  ...  7} (Fig.3a shows the τ = 3 case). To access how gLARA’s
cross-validated log-likelihood varied with the latent dimensionalities and the model order  we plot-
ted it in Fig.3b  for p2 = 15 and p1 ∈ {5  10  ...  50}  for different choices of τ. This showed that
the performance is greater for an order 3 model  and that it saturates by the time p1 reaches 50.
In Fig.3c  we did a similar analysis for the dimensionality of V2’s latent state  where p1 was held
constant at 50 and p2 ∈ {5  10  ...  25}. The cross-validated log-likelihood shows a clear peak at
p2 = 15 regardless of τ. We found that  for both models  the cross-validated log-likelihood peaks
for τ = 3 (see Fig.3b and 3c for gLARA  results not shown for AR-pCCA).
Finally  we asked which model  AR-pCCA or gLARA  better describes the data. Note that gLARA
is a special case of AR-pCCA  where the observation matrix in Eq.(8) is constrained to have a block
diagonal structure (with blocks C 1 and C 2). The key difference between the two models is that
gLARA assigns a non-overlapping set of latent variables to each population. We found that gLARA
outperforms AR-pCCA (Fig.3a). This suggests that the extra ﬂexibility of the AR-pCCA model

6

20406080−4.62−4.52x 105AR−pCCAgLARAlatent dimensionality(a)cross−validated log−likelihood1020304050τ = 1τ = 5τ = 3latent dimensionality p1(b)510152025τ = 1τ = 5τ = 3latent dimensionality p2(c)Figure 4: Leave-one-neuron-out prediction using gLARA. Observed activity (black) and the
leave-one-neuron-out prediction of gLARA (blue) for a representative held-out trial  averaged over
(a) the V1 population and (b) the V2 population. Note that the activity can be negative because we
are analyzing the single-trial residuals (cf. Section 2.5).

leads to overﬁtting and that the data are better explained by considering two separate sets of latent
variables that interact.
The optimal latent dimensionalities found for AR-pCCA and gLARA are substantially higher than
those found for pCCA  as the latent states now also capture non-zero time lag interactions between
the populations  and the dynamics within each population. For gLARA  the between-population
t and
covariance must be accounted for by the interaction between the population-speciﬁc latents  x1
t   because there are no shared latents in this model. Thus  the interaction between V1 and V2 is
x2
summarized by the A12
k matrices. Also  both AR-pCCA and gLARA outperform FA and
pCCA (comparing vertical axes in Fig.2 and 3)  showing that there is meaningful temporal structure
in how V1 and V2 interact that can be captured by these models.
Having performed a systematic  relative comparison between AR-pCCA and gLARA models of dif-
ferent complexities  we asked how well the best gLARA model ﬁt the data in an absolute sense. To
do so  we used 3/4 of the data to ﬁt the model parameters and performed leave-one-neuron-out pre-

k and A21

diction [15] on the remaining 1/4. This is done by estimating the latent states E(cid:0)x1
and E(cid:0)x2

(cid:1)
(cid:1) using all but one neuron. This estimate of the latent state is then used to

1 ... T | y1

1 ... T

1 ... T | y2

1 ... T

k and A21

k and A22

predict the activity of the neuron that was left out (the same procedure was repeated for each neu-
ron). For visualization purposes  we averaged the predicted activity across neurons for a given trial
and compared it to the recorded activity averaged across neurons for the same trial. We found that
they indeed tracked each other  as shown in Fig.4 for a representative trial.
Finally  we asked whether gLARA reveals differences in the time structures of the within-population
dynamics and the between-population interactions. We computed the Frobenius norm of both the
within-population dynamics matrices A11
k (Fig.5a) and the between-population interaction
k (Fig.5b)  for p1 = 50  p2 = 15 and τ = 3 (k ∈ {1  2  3})  which is the model
matrices A12
for which the cross-validated log-likelihood was the highest. The time structure of the within-
population dynamics appears to differ from that of the between-population interaction. In particular 
the latents for each area depend more strongly on its own previous latents as the time delay increases
up to 15 ms (Fig.5a). In contrast  the dependence between areas is stronger at time lags of 5 and
15 ms  compared to 10 ms (Fig.5b). Note that the peak of the cross-validated log-likelihood for
τ = 3 (Fig.3) shows that delays longer than 15ms do not contribute to an increase in the accuracy
of the model and  therefore  the most signiﬁcant interactions between these areas may occur within
this time window. The structure seen in Fig.5 is not present if the same analysis is performed on
data that are shufﬂed across time (results not shown). Because the latent states may have different
scales  it is not informative to compare the magnitude of A12
k and A22
k and A22
also have different dimensions). Thus  we divided the norms for each Aij
k matrix by the respective
maximum across k.

k and A21

k or A11

k (A11

k

7

20040060080010001200−4040time (ms)average activity (spikes/s)observed activitypredicted activityV1(a)20040060080010001200−2020time (ms)observed activitypredicted activityV2(b)Figure 5: Temporal structure of coupling matrices for gLARA. (a) Frobenius norm of the within-
k   for k ∈ {1  2  3}. Each curve was divided by its
population dynamics matrices A11
maximum value. (b) Same as (a) for the between-population interaction matrices A12

k and A22

k and A21
k .

4 Discussion

We started by applying standard methods  FA and pCCA  to neural activity recorded simultaneously
from visual areas V1 and V2. We found that the neuron groupings by brain area are meaningful 
as the covariance of the neurons across areas is lower dimensional than that within each area. We
then proposed an extension to pCCA that takes temporal dynamics into account and allows for
the separation of within-population dynamics from between-population interactions (gLARA). This
method was then shown to provide a better characterization of the two-population neural activity
than FA and pCCA.
In the context of studying the interaction between populations of neurons  capturing the information
ﬂow is key to understanding how information is processed in the brain [3–7 23]. To do so  one must
be able to characterize the directionality of these between-population interactions. Previous studies
have sought to identify the directionality of interactions directly between neurons  using measures
such as Granger causality [10] (and related extensions  such as directed transfer function (DTF)
[24])  and directed information [11]. Here  we proposed to study between-population interaction
on the level of latent variables  rather than of the neurons themselves. The advantage is that this
approach scales better with the number of recorded neurons and provides a more succinct picture
of the structure of these interactions. To detect ﬁne timescale interactions  it may be necessary to
replace the linear-Gaussian model with a point process model on the spike trains [25].

Acknowledgments

This work was supported by NIH-EY016774 and Fundac¸˜ao para a Ciˆencia e a Tecnologia graduate
scholarship SFRH/BD/52069/2012.

References

[1] Xiaoxuan Jia  Seiji Tanabe  and Adam Kohn. Gamma and the coordination of spiking activity

in early visual cortex. Neuron  77(4):762–774  February 2013.

[2] Misha B. Ahrens  Jennifer M. Li  Michael B. Orger  Drew N. Robson  Alexander F. Schier 
Florian Engert  and Ruben Portugues. Brain-wide neuronal dynamics during motor adaptation
in zebraﬁsh. Nature  485(7399):471–477  May 2012.

[3] David A. Crowe  Shikha J. Goodwin  Rachael K. Blackman  Soﬁa Sakellaridi  Scott R. Spon-
heim  Angus W. MacDonald Iii  and Matthew V. Chafee. Prefrontal neurons transmit signals to
parietal neurons that reﬂect executive control of cognition. Nature Neuroscience  16(10):1484–
1491  October 2013.

[4] Georgia G. Gregoriou  Stephen J. Gotts  Huihui Zhou  and Robert Desimone. High-
frequency  long-range coupling between prefrontal and visual cortex during attention. Science 
324(5931):1207–1210  May 2009.

8

510150.41V1  V1V2  V2time delay (ms)Frobenius norm (a.u.)(a)510150.751V2  V1V1  V2time delay (ms)(b)[5] Yuri B. Saalmann  Mark A. Pinsk  Liang Wang  Xin Li  and Sabine Kastner. The pulvinar reg-
ulates information transmission between cortical areas based on attention demands. Science 
337(6095):753–756  August 2012.

[6] R. F. Salazar  N. M. Dotson  S. L. Bressler  and C. M. Gray. Content-speciﬁc fronto-parietal
synchronization during visual working memory. Science  338(6110):1097–1100  November
2012.

[7] Yuriria V´azquez  Emilio Salinas  and Ranulfo Romo. Transformation of the neural code for
tactile detection from thalamus to cortex. Proceedings of the National Academy of Sciences 
110(28):E2635–E2644  July 2013.

[8] Davi D. Bock  Wei-Chung Allen Lee  Aaron M. Kerlin  Mark L. Andermann  Greg Hood 
Arthur W. Wetzel  Sergey Yurgenson  Edward R. Soucy  Hyon Suk Kim  and R. Clay Reid.
Network anatomy and in vivo physiology of visual cortical neurons. Nature  471(7337):177–
182  March 2011.

[9] Jonathan W. Pillow  Jonathon Shlens  Liam Paninski  Alexander Sher  Alan M. Litke  E. J.
Chichilnisky  and Eero P. Simoncelli. Spatio-temporal correlations and visual signalling in a
complete neuronal population. Nature  454(7207):995–999  August 2008.

[10] Sanggyun Kim  David Putrino  Soumya Ghosh  and Emery N. Brown. A granger causality
measure for point process models of ensemble neural spiking activity. PLoS Comput Biol 
7(3):e1001110  March 2011.

[11] Christopher J. Quinn  Todd P. Coleman  Negar Kiyavash  and Nicholas G. Hatsopoulos. Es-
timating the directed information to infer causal relationships in ensemble neural spike train
recordings. Journal of Computational Neuroscience  30(1):17–44  February 2011.

[12] Jakob H. Macke  Lars Buesing  John P. Cunningham  M. Yu Byron  Krishna V. Shenoy  and
Maneesh Sahani. Empirical models of spiking in neural populations. In NIPS  pages 1350–
1358  2011.

[13] Mark Stopfer  Vivek Jayaraman  and Gilles Laurent.

Intensity versus identity coding in an

olfactory system. Neuron  39(6):991–1004  September 2003.

[14] Jayant E. Kulkarni and Liam Paninski. Common-input models for multiple neural spike-train

data. Network: Computation in Neural Systems  18(4):375–407  January 2007.

[15] Byron M. Yu  John P. Cunningham  Gopal Santhanam  Stephen I. Ryu  Krishna V. Shenoy  and
Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of
neural population activity. In NIPS  pages 1881–1888  2008.

[16] Wieland Brendel  Ranulfo Romo  and Christian K. Machens. Demixed principal component

analysis. In NIPS  pages 2654–2662  2011.

[17] Valerio Mante  David Sussillo  Krishna V. Shenoy  and William T. Newsome. Context-
dependent computation by recurrent dynamics in prefrontal cortex. Nature  503(7474):78–84 
November 2013.

[18] John P. Cunningham and Byron M. Yu. Dimensionality reduction for large-scale neural record-

ings. Nature Neuroscience  17(11):1500–1509  November 2014.

[19] B. S. Everitt. An Introduction to Latent Variable Models. Springer Netherlands  Dordrecht 

1984.

[20] Francis R. Bach and Michael I. Jordan. A probabilistic interpretation of canonical correlation

analysis. 2005.

[21] Brian DO Anderson and John B. Moore. Optimal ﬁltering. Courier Dover Publications  2012.
[22] Jeremy Freeman  Corey M. Ziemba  David J. Heeger  Eero P. Simoncelli  and J. Anthony
Movshon. A functional and perceptual signature of the second visual area in primates. Nature
Neuroscience  16(7):974–981  July 2013.

[23] Pascal Fries. A mechanism for cognitive dynamics: neuronal communication through neuronal

coherence. Trends in Cognitive Sciences  9(10):474–480  October 2005.

[24] M. J. Kaminski and K. J. Blinowska. A new method of the description of the information ﬂow

in the brain structures. Biological Cybernetics  65(3):203–210  July 1991.

[25] Anne C. Smith and Emery N. Brown. Estimating a state-space model from point process

observations. Neural Computation  15(5):965–991  May 2003.

9

,Joao Semedo
Amin Zandvakili
Adam Kohn
Christian Machens
Byron Yu
Michalis Titsias RC AUEB
Miguel Lázaro-Gredilla