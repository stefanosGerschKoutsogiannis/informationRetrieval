2013,Distributed Exploration in Multi-Armed Bandits,We study exploration in Multi-Armed Bandits (MAB) in a setting where~$k$ players collaborate in order to identify an $\epsilon$-optimal arm. Our motivation comes from recent employment of MAB algorithms in computationally intensive  large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players  and the amount of communication between them. In particular  our main result shows that by allowing the $k$ players to communicate \emph{only once}  they are able to learn $\sqrt{k}$ times faster than a single player. That is  distributing learning to $k$ players gives rise to a factor~$\sqrt{k}$ parallel speed-up. We complement this result with a lower bound showing this is in general the best possible.  On the other extreme  we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance  with communication only logarithmic in~$1/\epsilon$.,Distributed Exploration in Multi-Armed Bandits

Eshcar Hillel

Yahoo Labs  Haifa

eshcar@yahoo-inc.com

Tomer Koren∗

Technion — Israel Inst. of Technology

tomerk@technion.ac.il

Zohar Karnin

Yahoo Labs  Haifa

zkarnin@yahoo-inc.com

Ronny Lempel
Yahoo Labs  Haifa

rlempel@yahoo-inc.com

Oren Somekh

Yahoo Labs  Haifa

orens@yahoo-inc.com

Abstract

We study exploration in Multi-Armed Bandits in a setting where k players col-
laborate in order to identify an ε-optimal arm. Our motivation comes from recent
employment of bandit algorithms in computationally intensive  large-scale appli-
cations. Our results demonstrate a non-trivial tradeoff between the number of arm
pulls required by each of the players  and the amount of communication between
them. In particular  our main result shows that by allowing the k players to com-
municate only once  they are able to learn
k times faster than a single player.
That is  distributing learning to k players gives rise to a factor
k parallel speed-
up. We complement this result with a lower bound showing this is in general the
best possible. On the other extreme  we present an algorithm that achieves the
ideal factor k speed-up in learning performance  with communication only loga-
rithmic in 1/ε.

√

√

Introduction

1
Over the past years  multi-armed bandit (MAB) algorithms have been employed in an increasing
amount of large-scale applications. MAB algorithms rank results of search engines [23  24]  choose
between stories or ads to showcase on web sites [2  8]  accelerate model selection and stochastic
optimization tasks [21  22]  and more. In many of these applications  the workload is simply too
high to be handled by a single processor. In the web context  for example  the sheer volume of user
requests and the high rate at which they arrive  require websites to use many front-end machines
that run in multiple data centers.
In the case of model selection tasks  a single evaluation of a
certain model or conﬁguration might require considerable computation time  so that distributing
the exploration process across several nodes may result with a signiﬁcant gain in performance. In
this paper  we study such large-scale MAB problems in a distributed environment where learning is
performed by several independent nodes that may take actions and observe rewards in parallel.
Following recent MAB literature [14  3  15  18]  we focus on the problem of identifying a “good”
bandit arm with high conﬁdence. In this problem  we may repeatedly choose one arm (corresponding
to an action) and observe a reward drawn from a probability distribution associated with that arm.
Our goal is to ﬁnd an arm with an (almost) optimal expected reward  with as few arm pulls as
possible (that is  minimize the simple regret [7]). Our objective is thus explorative in nature  and in

∗Most of this work was done while the author was at Yahoo Labs  Haifa.

1

particular we do not mind the incurred costs or the involved regret. This is indeed the natural goal in
many applications  such as in the case of model selection problems mentioned above. In our setup 
a distributed strategy is evaluated by the number of arm pulls per node required for the task  which
correlates with the parallel speed-up obtained by distributing the learning process.
We abstract a distributed MAB system as follows. In our model  there are k players that correspond
to k independent machines in a cluster. The players are presented with a set of arms  with a common
goal of identifying a good arm. Each player receives a stream of queries upon each it chooses an arm
to pull. This stream is usually regulated by some load balancer ensuring the load is roughly divided
evenly across players. To collaborate  the players may communicate with each other. We assume
that the bandwidth of the underlying network is limited  so that players cannot simply share every
piece of information. Also  communicating over the network might incur substantial latencies  so
players should refrain from doing so as much as possible. When measuring communication of a
certain multi-player protocol we consider the number of communication rounds it requires  where
in a round of communication each player broadcasts a single message (of arbitrary size) to all other
players. Round-based models are natural in distributed learning scenarios  where frameworks such
as MapReduce [11] are ubiquitous.
What is the tradeoff between the learning performance of the players  and the communication be-
tween them? At one extreme  if all players broadcast to each other each and every arm reward as
it is observed  they can simply simulate the decisions of a serial  optimal algorithm. However  the
communication load of this strategy is of course prohibitive. At the other extreme  if the players
never communicate  each will suffer the learning curve of a single player  thereby avoiding any pos-
sible speed-up the distributed system may provide. Our goal in this work is to better understand this
tradeoff between inter-player communication and learning performance.
Considering the high cost of communication  perhaps the simplest and most important question that
arises is how well can the players learn while keeping communication to the very minimum. More
speciﬁcally  is there a non-trivial strategy by which the players can identify a “good” arm while
communicating only once  at the end of the process? As we discuss later on  this is a non-trivial
question. On the positive side  we present a k-player algorithm that attains an asymptotic parallel
speed-up of
k factor  as compared to the conventional  serial setting. In fact  our approach demon-
strates how to convert virtually any serial exploration strategy to a distributed algorithm enjoying
such speed-up. Ideally  one could hope for a factor k speed-up in learning performance; however 
we show a lower bound on the required number of pulls in this case  implying that our
k speed-up
is essentially optimal.
At the other end of the trade-off  we investigate how much communication is necessary for obtaining
the ideal factor k parallel speed-up. We present a k-player strategy achieving such speed-up  with
communication only logarithmic in 1/ε. As a corollary  we derive an algorithm that demonstrates an
explicit trade-off between the number of arm pulls and the amount of inter-player communication.

√

√

1.1 Related Work

Recently there has been an increasing interest in distributed and collaborative learning problems.
In the MAB literature  several recent works consider multi-player MAB scenarios in which players
actually compete with each other  either on arm-pulls resources [15] or on the rewards received [19].
In contrast  we study a collaborative multi-player problem and investigate how sharing observations
helps players achieve their common goal. The related work of Kanade et al. [17] in the context
of non-stochastic (i.e. adversarial) experts also deals with a collaborative problem in a similar dis-
tributed setup  and examine the trade-off between communication and the cumulative regret.
Another line of recent work was focused on distributed stochastic optimization [13  1  12] and dis-
tributed PAC models [6  10  9]  investigating the involved communication trade-offs. The techniques
developed there  however  are inherently “batch” learning methods and thus are not directly applica-
ble to our MAB problem which is online in nature. Questions involving network topology [13  12]
and delays [1] are relevant to our setup as well; however  our present work focuses on establishing
the ﬁrst non-trivial guarantees in a distributed collaborative MAB setting.

2

2 Problem Setup and Statement of Results
In our model of the Distributed Multi-Armed Bandit problem  there are k ≥ 1 individual players.
The players are given n arms  enumerated by [n] := {1  2  . . .   n}. Each arm i ∈ [n] is associated
with a reward  which is a [0  1]-valued random variable with expectation pi. For convenience  we
assume that the arms are ordered by their expected rewards  that is p1 ≥ p2 ≥ ··· ≥ pn. At every
time step t = 1  2  . . .   T   each player pulls one arm of his choice and observes an independent
sample of its reward. Each player may choose any of the arms  regardless of the other players and
their actions. At the end of the game  each player must commit to a single arm. In a communication
round  that may take place at any predeﬁned time step  each player may broadcast a message to all
other players. While we do not restrict the size of each message  in a reasonable implementation a
message should not be larger than ˜O(n) bits.
In the best-arm identiﬁcation version of the problem  the goal of a multi-player algorithm given some
target conﬁdence level δ > 0  is that with probability at least 1 − δ all players correctly identify the
best arm (i.e. the arm having the maximal expected reward). For simplicity  we assume in this setting
that the best arm is unique. Similarly  in the (ε  δ)-PAC variant the goal is that each player ﬁnds an
ε-optimal (or “ε-best”) arm  that is an arm i with pi ≥ p1− ε  with high probability. In this paper we
focus on the more general (ε  δ)-PAC setup  which also includes best-arm identiﬁcation for ε = 0.
We use the notation ∆i := p1 − pi to denote the suboptimality gap of arm i  and occasionally
use ∆(cid:63) := ∆2 for denoting the minimal gap. In the best-arm version of the problem  where we
assume that the best arm is unique  we have ∆i > 0 for all i > 1. When dealing with the (ε  δ)-PAC
i := max{∆i  ε}. In the context of MAB problems  we
setup  we also consider the truncated gaps ∆ε
are interested in deriving distribution-dependent bounds  namely  bounds that are stated as a function
of ε  δ and also the distribution-speciﬁc values ∆ := (∆2  . . .   ∆n). The ˜O notation in our bounds
hides polylogarithmic factors in n  k  ε  δ  and also in ∆2  . . .   ∆n. In the case of serial exploration
algorithms (i.e.  when there is only one player)  the lower bounds of Mannor and Tsitsiklis [20] and
Audibert et al. [3] show that in general ˜Ω(Hε) pulls are necessary for identifying an ε-arm  where

n(cid:88)

i=2

Hε :=

1
i )2 .
(∆ε

(1)

Intuitively  the hardness of the task is therefore captured by the quantity Hε  which is roughly the
number of arm pulls needed to ﬁnd an ε-best arm with a reasonable probability; see also [3] for a
discussion. Our goal in this work is therefore to establish bounds in the distributed model that are
expressed as a function of Hε  in the same vein of the bounds known in the classic MAB setup.

2.1 Baseline approaches

We now discuss several baseline approaches for the problem  starting with our main focus—the sin-
gle round setting. The ﬁrst obvious approach  already mentioned earlier  is the no-communication
strategy: just let each player explore the arms in isolation of the other players  following an inde-
pendent instance of some serial strategy; at the end of the executions  all players hold an ε-best arm.
Clearly  this approach performs poorly in terms of learning performance  needing ˜Ω(Hε) pulls per
player in the worst case and not leading to any parallel speed-up.
Another straightforward approach is to employ a majority vote among the players: let each player
independently identify an arm  and choose the arm having most of the votes (alternatively  at least
half of the votes). However  this approach does not lead to any improvement in performance: for
this vote to work  each player has to solve the problem correctly with reasonable probability  which
already require ˜Ω(Hε) pulls of each. Even if we somehow split the arms between players and let
each player explore a share of them  a majority vote would still fail since those players getting the
“good” arms might have to pull arms ˜Ω(Hε) times—a small MAB instance might be as hard as the
full-sized problem (in terms of the complexity measure Hε).
When considering algorithms employing multiple communication rounds  we use an ideal simulated
serial algorithm (i.e.  a full-communication approach) as our baseline. This approach is of course
prohibited in our context  but is able to achieve the optimal parallel speed-up  linear in the number
of players k.

3

2.2 Our results

√

√

√

We now discuss our approach and overview our algorithmic results. These are summarized in Table 1
below  that compares the different algorithms in terms of parallel speed-up and communication.
Our approach for the one-round case is based on the idea of majority vote. For the best-arm identiﬁ-
cation task  our observation is that by letting each player explore a smaller set of n/
k arms chosen
k of the players would come up with the global
at random and choose one of them as “best”  about
best arm. This (partial) consensus on a single arm is a key aspect in our approach  since it allows the
players to identify the correct best arm among the votes of all k players  after sharing information
only once. Our approach leads to a factor
k parallel speed-up which  as we demonstrate in our
lower bound  is the optimal factor in this setting. Although our goal here is pure exploration  in our
algorithms each player follows an explore-exploit strategy. The idea is that a player should sample
his recommended arm as much as his budget permits  even if it was easy to identify in his small-
sized problem. This way we can guarantee that the top arms are sampled to a sufﬁcient precision by
the time each of the players has to choose a single best arm.
The algorithm for the (ε  δ)-PAC setup is similar  but its analysis is more challenging. As mentioned
above  an agreement on a single arm is essential for a vote to work. Here  however  there might
be several ε-best arms  so arriving at a consensus on a single one is more difﬁcult. Nonetheless 
by examining two different regimes  namely when there are “many” ε-best arms and when there
are “few” of them  our analysis shows that a vote can still work and achieve the
k multiplicative
speed-up.
In the case of multiple communication rounds  we present a distributed elimination-based algorithm
that discards arms right after each communication round. Between rounds  we share the work load
between players uniformly. We show that the number of such rounds can be reduced to as low
as O(log(1/ε))  by eliminating all 2−r-suboptimal arms in the r’th round. A similar idea was
employed in [4] for improving the regret bound of UCB with respect to the parameters ∆i. We also
use this technique to develop an algorithm that performs only R communication rounds  for any
given parameter R ≥ 1  that achieves a slightly worse multiplicative ε2/Rk speed-up.

√

SETTING

ONE-ROUND

MULTI-ROUND

ALGORITHM
No-Communication
Majority Vote
Algorithm 1 2
Serial (simulated)
Algorithm 3
Algorithm 3’

SPEED-UP COMMUNICATION

1
√
1
k
k
k

ε2/R · k

none
1 round
1 round
every time step
ε ) rounds
O(log 1
R rounds

Table 1: Summary of baseline approaches and our results. The speed-up results are asymptotic
(logarithmic factors are omitted).

3 One Communication Round

This section considers the most basic variant of the multi-player MAB problem  where each player
is only allowed a single transmission  when ﬁnishing her queries. For the clarity of exposition  we
ﬁrst consider the best-arm identiﬁcation setting in Section 3.1. Section 3.2 deals with the (ε  δ)-PAC
setup. We demonstrate the tightness of our result in Section 3.3 with a lower bound for the required
budget of arm pulls in this setting.
Our algorithms in this section assume the availability of a serial algorithm A(A  ε)  that given a set
of arms A and target accuracy ε  identiﬁes an ε-best arm in A with probability at least 2/3 using no
more than

(2)

(cid:88)

i∈A

cA

|A|
∆ε
i

1
i )2 log
(∆ε

4

arm pulls  for some constant cA > 1. For example  the Successive Elimination algorithm [14] and
the Exp-Gap Elimination algorithm [18] provide a guarantee of this form. Essentially  any explo-
ration strategy whose guarantee is expressed as a function of Hε can be used as the procedure A 
with technical modiﬁcations in our analysis.

3.1 Best-arm Identiﬁcation Algorithm

We now describe our one-round best-arm identiﬁcation algorithm. For simplicity  we present a
version matching δ = 1/3  meaning that the algorithm produces the correct arm with probability at
least 2/3; we later explain how to extend it to deal with arbitrary values of δ.
Our algorithm is akin to a majority vote among the multiple players  in which each player pulls
arms in two stages. In the ﬁrst EXPLORE stage  each player independently solves a “smaller” MAB
instance on a random subset of the arms using the exploration strategy A. In the second EXPLOIT
stage  each player exploits the arm identiﬁed as “best” in the ﬁrst stage  and communicates that arm
and its observed average reward. See Algorithm 1 below for a precise description. An appealing
feature of our algorithm is that it requires each player to transmit a single message of constant
size (up to logarithmic factors).
In Theorem 3.1 we prove that Algorithm 1 in-
deed achieves the promised upper bound.
Theorem 3.1. Algorithm 1 identiﬁes the best
arm correctly with probability at least 2/3 us-
ing no more than

Algorithm 1 ONE-ROUND BEST-ARM
input time horizon T
output an arm
1: for player j = 1 to k do
2:

√

(cid:32)

(cid:33)

· n(cid:88)

O

1√
k

1
∆2
i

log

n
∆i

i=2

arm pulls per player  provided that 6 ≤ √

k ≤
n. The algorithm uses a single communica-
tion round  in which each player communicates
˜O(1) bits.

3:

4:

k arms uni-
choose a subset Aj of 6n/
formly at random
EXPLORE: execute ij ← A(Aj  0) using
at most 1
2 T pulls (and halting the algorithm
early if necessary);
if the algorithm fails to identify any arm or
does not terminate gracefully  let ij be an
arbitrary arm
EXPLOIT: pull arm ij for 1
let ˆqj be its average reward
communicate the numbers ij  ˆqj

2 T times and

5:
6: end for
7: let ki be the number of players j with ij = i 

By repeating the algorithm O(log(1/δ)) times
and taking the majority vote of the independent
runs  we can amplify the success probability to
1 − δ for any given δ > 0. Note that we can
still do that with one communication round (at
the end of all executions)  but each player now
has to communicate O(log(1/δ)) values1.
Theorem 3.2. There exists a k-player al-
gorithm that given ˜O
arm
pulls  identiﬁes the best arm correctly with probability at least 1 − δ. The algorithm uses a sin-
gle communication round  in which each player communicates O(log(1/δ)) numerical values.

8: let ˆpi = (1/ki)(cid:80){j : ij =i} ˆqj for all i

9: return arg maxi∈A ˆpi; if the set A is empty 

and deﬁne A = {i : ki >

output an arbitrary arm.

(cid:16) 1√

(cid:80)n

i=2 1/∆2
i

(cid:17)

k

√

k}

We now prove Theorem 3.1. We show that a budget T of samples (arm pulls) per player  where

· n(cid:88)

i=2

T ≥ 24cA√
k

1
∆2
i

ln

n
∆i

 

(3)

sufﬁces for the players to jointly identify the best arm i(cid:63) with the desired probability. Clearly  this
would imply the bound stated in Theorem 3.1. We note that we did not try to optimize the constants
in the above expression.
We begin by analyzing the EXPLORE phase of the algorithm. Our ﬁrst lemma shows that each player
chooses the global best arm and identiﬁes it as the local best arm with sufﬁciently large probability.

1In fact  by letting each player pick a slightly larger subset of O((cid:112)log(1/δ)· n/

√
k) arms  we can amplify
the success probability to 1 − δ without needing to communicate more than 2 values per player. However  this
approach only works when k = Ω(log(1/δ)).

5

√
Lemma 3.3. When (3) holds  each player identiﬁes the (global) best arm correctly after the EX-
PLORE phase with probability at least 2/

k.

We next address the EXPLOIT phase. The next simple lemma shows that the popular arms (i.e. those
selected by many players) are estimated to a sufﬁcient precision.
2 ∆(cid:63) for all arms i ∈ A with probability
Lemma 3.4. Provided that (3) holds  we have |ˆpi − pi| ≤ 1
at least 5/6.

Due to lack of space  the proofs of the above lemmas are omitted and can be found in [16]. We can
now prove Theorem 3.1.

√

k] ≤ Pr[ki(cid:63) − E[ki(cid:63) ] ≤ −√

Proof (of Theorem 3.1). Let us ﬁrst show that with probability at least 5/6  the best arm i is con-
tained in the set A. To this end  notice that ki(cid:63) is the sum of k i.i.d. Bernoulli random vari-
ables {Ij}j where Ij is the indicator of whether player j chooses arm i(cid:63) after the EXPLORE
phase. By Lemma 3.3 we have that E[Ij] ≥ 2/
Pr[ki(cid:63) ≤ √
k for all j  hence by Hoeffding’s inequality 
k] ≤ exp(−2k/k) ≤ 1/6 which implies that i(cid:63) ∈ A with
probability at least 5/6.
Next  note that with probability at least 5/6 the arm i ∈ A having the highest empirical reward ˆpi
is the one with the highest expected reward pi. Indeed  this follows directly from Lemma 3.4 that
shows that with probability at least 5/6  for all arms i ∈ A the estimate ˆpi is within 1
2 ∆ of the true
bias pi. Hence  via a union bound we conclude that with probability at least 2/3  the best arm is in
A and has the highest empirical reward. In other words  with probability at least 2/3 the algorithm
outputs the best arm i(cid:63).

3.2

(ε  δ)-PAC Algorithm

We now present an algorithm whose purpose is to recover an ε-optimal arm. Here  there might be
more than one ε-best arm  so each “successful” player might come up with a different ε-best arm.
Nevertheless  our analysis below shows that with high probability  a subset of the players can still
agree on a single ε-best arm  which makes it possible to identify it among the votes of all players.
Our algorithm is described in Algorithm 2  and the following theorem states its guarantees.
Theorem 3.5. Algorithm 2 identiﬁes a 2ε-best arm with probability at least 2/3 using no more than

· n(cid:88)
arm pulls per player  provided that 24 ≤ √

1√
k

i=2

O

(cid:32)

round  in which each player communicates ˜O(1) bits.

(cid:33)

1
i )2 log
(∆ε

n
∆ε
i

k ≤ n. The algorithm uses a single communication

n(cid:88)

i=2

Before proving the theorem  we ﬁrst state several key lemmas. In the following  let nε and n2ε
denote the number of ε-best and 2ε-best arms respectively. Our analysis considers two different
regimes: n2ε ≤ 1

k  and shows that in any case 

√

√

k and n2ε > 1
50

50

T ≥ 400cA√
k

1
i )2 ln
(∆ε

24n
∆ε
i

(4)

sufﬁces for identifying a 2ε-best arm with the desired probability. Clearly  this implies the bound
stated in Theorem 3.5.
The ﬁrst lemma shows that at least one of the players is able to ﬁnd an ε-best arm. As we later show 
this is sufﬁcient for the success of the algorithm in case there are many 2ε-best arms.
Lemma 3.6. When (4) holds  at least one player successfully identiﬁes an ε-best arm in the EX-
PLORE phase  with probability at least 5/6.

The next lemma is more reﬁned and states that in case there are few 2ε-best arms  the probability of
each player to successfully identify an ε-best arm grows linearly with nε.
Lemma 3.7. Assume that n2ε ≤ 1
√
EXPLORE phase  with probability at least 2nε/

k. When (4) holds  each player identiﬁes an ε-best arm in the

√

k.

50

6

The last lemma we need analyzes the accuracy
of the estimated rewards of arms in the set A.
Lemma 3.8. With probability at least 5/6  we
have |ˆpi − pi| ≤ ε/2 for all arms i ∈ A.
For the proofs of the above lemmas  refer to
[16]. We now turn to prove Theorem 3.5.

Proof. We shall prove that with probability 5/6
the set A contains at least one ε-best arm. This
would complete the proof  since Lemma 3.8 as-
sures that with probability 5/6  the estimates ˆpi
of all arms i ∈ A are at most ε/2-away from the
true reward pi  and in turn implies (via a union
bound) that with probability 2/3 the arm i ∈ A
having the maximal empirical reward ˆpi must
be a 2ε-best arm.
First  consider the case n2ε > 1
k. Lemma
50
3.6 shows that with probability 5/6 there exists
a player j that identiﬁes an ε-best arm ij. Since
for at least n2ε arms ∆i ≤ 2ε  we have
24n
2ε

· n2ε − 1

√
2 T ≥ 400

√

ln

2

Algorithm 2 ONE-ROUND ε-ARM
input time horizon T   accuracy ε
output an arm
1: for player j = 1 to k do
2:

√

3:

4:

choose a subset Aj of 12n/
k arms uni-
formly at random
EXPLORE: execute ij ← A(Aj  ε) using
at most 1
2 T pulls (and halting the algorithm
early if necessary);
if the algorithm fails to identify any arm or
does not terminate gracefully  let ij be an
arbitrary arm
EXPLOIT: pull arm ij for 1
let ˆqj be the average reward
communicate the numbers ij  ˆqj

2 T times  and

2 kiT and ˆpi = (1/ki)(cid:80){j : ij =i} ˆqj

5:
6: end for
7: let ki be the number of players j with ij = i
8: let ti = 1
9: deﬁne A = {i ∈ [n] : ti ≥ (1/ε2) ln(12n)}
10: return arg maxi∈A ˆpi; if the set A is empty 

for all i

(2ε)2

output an arbitrary arm.

k
ε2 ln(12n)  

tij ≥ 1
≥ 1
that is  ij ∈ A.
Next  consider the case n2ε ≤ 1
k. Let N denote the number of players that identiﬁed some
ε-best arm. The random variable N is a sum of Bernoulli random variables {Ij}j where Ij in-
√
dicates whether player j identiﬁed some ε-best arm. By Lemma 3.7  E[Ij] ≥ 2nε/
√
k and thus
ε) ≤ 1/6 .
k] = Pr[N − E[N ] ≤ −nε
k] ≤ exp(−2n2
√
by Hoeffding’s inequality  Pr[N < nε
That is  with probability 5/6  at least nε
k players found an ε-best arm. A pigeon-hole argument
now shows that in this case there exists an ε-best arm i(cid:63) selected by at least
k players. Hence 
with probability 5/6 the number of samples of this arm collected in the EXPLOIT phase is at least

√

√

√

50

kT /2 > (1/ε2) ln(12n)  which means that i(cid:63) ∈ A.

ti(cid:63) ≥ √

3.3 Lower Bound

√
The following theorem suggests that in general  for identifying the best arm k players achieve a
multiplicative speed-up of at most ˜O(
k) when allowing one transmission per player (at the end of
the game). Clearly  this also implies that a similar lower bound holds in the PAC setup  and proves
that our algorithmic results for the one-round case are essentially tight.
Theorem 3.9. For any k-player strategy that uses a single round of communication  there exist
rewards p1  . . .   pn ∈ [0  1] and integer T such that
√
• each individual player must use at least T /
k arm pulls for them to collectively identify the
• there exist a single-player algorithm that needs at most ˜O(T ) pulls for identifying the best arm

best arm with probability at least 2/3;

with probability at least 2/3.

The proof of the theorem is omitted due to space constraints and can be found in [16].

4 Multiple Communication Rounds

In this section we establish an explicit tradeoff between the performance of a multi-player algorithm
and the number of communication rounds it uses  in terms of the accuracy ε. Our observation is that

7

by allowing O(log(1/ε)) rounds of communication  it is possible to achieve the optimal speedup of
factor k. That is  we do not gain any improvement in learning performance by allowing more than
O(log(1/ε)) rounds.
Our algorithm is given in Algorithm 3. The
idea is to eliminate in each round r (i.e.  right
after the rth communication round) all 2−r-
suboptimal arms. We accomplish this by let-
ting each player sample uniformly all remain-
ing arms and communicate the results to other
players. Then  players are able to eliminate
suboptimal arms with high conﬁdence.
If
each such round is successful  after log2(1/ε)
rounds only ε-best arms survive. Theorem 4.1
below bounds the number of arm pulls used by
this algorithm (a proof can be found in [16]).
Theorem 4.1. With probability at least 1 − δ 
Algorithm 3
• identiﬁes the optimal arm using

Algorithm 3 MULTI-ROUND ε-ARM
input (ε  δ)
output an arm
1: initialize S0 ← [n]  r ← 0  t0 ← 0
2: repeat
3:
4:
5:
6:

set r ← r + 1
let εr ← 2−r  tr ← (2/kε2
for player j = 1 to k do

sample each arm i ∈ Sr−1 for tr − tr−1
times
let ˆpr
all rounds so far of player j)
communicate the numbers ˆpr

j i be the average reward of arm i (in

i = (1/k)(cid:80)k

r) ln(4nr2/δ)

end for
let ˆpr
and let ˆpr
set Sr ← Sr−1\{i ∈ Sr−1 : ˆpr

(cid:63) = maxi∈Sr−1 ˆpr
i

j 1  . . .   ˆpr
j i for all i ∈ Sr−1 
(cid:63)−εr}

(cid:19)(cid:33)

8:
9:
10:

j=1 ˆpr

i < ˆpr

7:

j n

1
i )2 log
(∆ε

log

1
∆ε
i

(cid:18) n

δ

11:
12: until εr ≤ ε/2 or |Sr| = 1
13: return an arm from Sr

(cid:32)

· n(cid:88)

i=2

1
k

O

arm pulls per player;

rounds for ε = 0).

• terminates after at most 1 + (cid:100)log2(1/ε)(cid:101) rounds of communication (or after 1 + (cid:100)log2(1/∆(cid:63))(cid:101)

By properly tuning the elimination thresholds εr of Algorithm 3 in accordance with the target accu-
racy ε  we can establish an explicit trade-off between the number of communication rounds and the
number of arm pulls each player needs. In particular  we can design a multi-player algorithm that
terminates after at most R communication rounds  for any given parameter R > 0. This  however 
comes at the cost of a compromise in learning performance as quantiﬁed in the following corollary.
Corollary 4.2. Given a parameter R > 0  set εr ← εr/R for all r ≥ 1 in Algorithm 3. With proba-
bility at least 1 − δ  the modiﬁed algorithm

• identiﬁes an ε-best arm using ˜O((ε−2/R/k) ·(cid:80)n

i )2) arm pulls per player;

i=2(1/∆ε

• terminates after at most R rounds of communication.

5 Conclusions and Further Research

We have considered a collaborative MAB exploration problem  in which several independent play-
ers explore a set of arms with a common goal  and obtained the ﬁrst non-trivial results in such
setting. Our main results apply for the speciﬁcally interesting regime where each of the players is
allowed a single transmission; this setting ﬁts naturally to common distributed frameworks such as
MapReduce. An interesting open question in this context is whether one can obtain a strictly better
speed-up result (which  in particular  is independent of ε) by allowing more than a single round.
k speed-up can
Even when allowing merely two communication rounds  it is unclear whether the
be improved. Intuitively  the difﬁculty here is that in the second phase of a reasonable strategy each
player should focus on the arms that excelled in the ﬁrst phase; this makes the sub-problems being
faced in the second phase as hard as the entire MAB instance  in terms of the quantity Hε. Nev-
ertheless  we expect our one-round approach to serve as a building-block in the design of future
distributed exploration algorithms  that are applicable in more complex communication models.
An additional interesting problem for future research is how to translate our results to the regret
minimization setting. In particular  it would be nice to see a conversion of algorithms like UCB [5]
to a distributed setting. In this respect  perhaps a more natural distributed model is a one resembling
that of Kanade et al. [17]  that have established a regret vs. communication trade-off in the non-
stochastic setting.

√

8

References
[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization.

873–881  2011.

In NIPS  pages

[2] D. Agarwal  B.-C. Chen  P. Elango  N. Motgi  S.-T. Park  R. Ramakrishnan  S. Roy  and
J. Zachariah. Online models for content optimization. In NIPS  pages 17–24  December 2008.
[3] J.-Y. Audibert  S. Bubeck  and R. Munos. Best arm identiﬁcation in multi-armed bandits. In

COLT  pages 41–53  2010.

[4] P. Auer and R. Ortner. UCB revisited: Improved regret bounds for the stochastic multi-armed

bandit problem. Periodica Mathematica Hungarica  61(1-2):55–65  2010.

[5] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit prob-

lem. Machine learning  47(2):235–256  2002.

[6] M. Balcan  A. Blum  S. Fine  and Y. Mansour. Distributed learning  communication complexity

and privacy. Arxiv preprint arXiv:1204.3514  2012.

[7] S. Bubeck  R. Munos  and G. Stoltz. Pure exploration in multi-armed bandits problems. In

Algorithmic Learning Theory  pages 23–37. Springer  2009.

[8] D. Chakrabarti  R. Kumar  F. Radlinski  and E. Upfal. Mortal multi-armed bandits. In NIPS 

pages 273–280  2008.

[9] H. Daum´e III  J. M. Phillips  A. Saha  and S. Venkatasubramanian. Efﬁcient protocols for

distributed classiﬁcation and optimization. In ALT  2012.

[10] H. Daum´e III  J. M. Phillips  A. Saha  and S. Venkatasubramanian. Protocols for learning

classiﬁers on distributed data. AISTAT  2012.

[11] J. Dean and S. Ghemawat. MapReduce: simpliﬁed data processing on large clusters. Commun.

ACM  51(1):107–113  Jan. 2008.

[12] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online prediction

using mini-batches. Journal of Machine Learning Research  13:165–202  2012.

[13] J. Duchi  A. Agarwal  and M. J. Wainwright. Distributed dual averaging in networks. NIPS 

23  2010.

[14] E. Even-Dar  S. Mannor  and Y. Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning
Research  7:1079–1105  2006.

[15] V. Gabillon  M. Ghavamzadeh  A. Lazaric  and S. Bubeck. Multi-bandit best arm identiﬁcation.

NIPS  2011.

[16] E. Hillel  Z. Karnin  T. Koren  R. Lempel  and O. Somekh. Distributed exploration in multi-

armed bandits. arXiv preprint arXiv:1311.0800  2013.

[17] V. Kanade  Z. Liu  and B. Radunovic. Distributed non-stochastic experts.

Neural Information Processing Systems 25  pages 260–268  2012.

In Advances in

[18] Z. Karnin  T. Koren  and O. Somekh. Almost optimal exploration in multi-armed bandits. In

Proceedings of the 30th International Conference on Machine Learning  2013.

[19] K. Liu and Q. Zhao. Distributed learning in multi-armed bandit with multiple players. IEEE

Transactions on Signal Processing  58(11):5667–5681  Nov. 2010.

[20] S. Mannor and J. Tsitsiklis. The sample complexity of exploration in the multi-armed bandit

problem. The Journal of Machine Learning Research  5:623–648  2004.

[21] O. Maron and A. W. Moore. Hoeffding races: Accelerating model selection search for classi-

ﬁcation and function approximation. In NIPS  1994.

[22] V. Mnih  C. Szepesv´ari  and J.-Y. Audibert. Empirical bernstein stopping. In ICML  pages

672–679. ACM  2008.

[23] F. Radlinski  M. Kurup  and T. Joachims. How does clickthrough data reﬂect retrieval quality?

In CIKM  pages 43–52  October 2008.

[24] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling

bandits problem. In ICML  page 151  June 2009.

9

,Eshcar Hillel
Zohar Karnin
Tomer Koren
Ronny Lempel
Oren Somekh