2016,Learning shape correspondence with anisotropic convolutional neural networks,Convolutional neural networks have achieved extraordinary results in many computer vision and pattern recognition applications; however  their adoption in the computer graphics and geometry processing communities is limited due to the non-Euclidean structure of their data.  In this paper  we propose Anisotropic Convolutional Neural Network (ACNN)  a generalization of classical CNNs to non-Euclidean domains  where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes  a fundamental problem in geometry processing  arising in a wide variety of applications. We tested ACNNs performance in very challenging settings  achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks.,Learning shape correspondence with

anisotropic convolutional neural networks

Davide Boscaini1  Jonathan Masci1  Emanuele Rodol`a1  Michael Bronstein1 2 3

1USI Lugano  Switzerland

2Tel Aviv University  Israel

3Intel  Israel

name.surname@usi.ch

Abstract

Convolutional neural networks have achieved extraordinary results in many com-
puter vision and pattern recognition applications; however  their adoption in the
computer graphics and geometry processing communities is limited due to the
non-Euclidean structure of their data. In this paper  we propose Anisotropic Con-
volutional Neural Network (ACNN)  a generalization of classical CNNs to non-
Euclidean domains  where classical convolutions are replaced by projections over
a set of oriented anisotropic diffusion kernels. We use ACNNs to effectively learn
intrinsic dense correspondences between deformable shapes  a fundamental prob-
lem in geometry processing  arising in a wide variety of applications. We tested
ACNNs performance in challenging settings  achieving state-of-the-art results on
recent correspondence benchmarks.

1

Introduction

In geometry processing  computer graphics  and vision  ﬁnding intrinsic correspondence between
3D shapes affected by different transformations is one of the fundamental problems with a wide
spectrum of applications ranging from texture mapping to animation [25]. Of particular interest is
the setting in which the shapes are allowed to deform non-rigidly. Traditional hand-crafted corre-
spondence approaches are divided into two main categories: point-wise correspondence methods
[17]  which establish the matching between (a subset of) the points on two or more shapes by min-
imizing metric distortion  and soft correspondence methods [23]  which establish a correspondence
among functions deﬁned over the shapes  rather than the vertices themselves. Recently  the emer-
gence of 3D sensing technology has brought the need to deal with acquisition artifacts  such as
missing parts  geometric  and topological noise  as well as matching 3D shapes in different repre-
sentations  such as meshes and point clouds. With new and broader classes of artifacts  comes the
need of learning from data invariance that is otherwise impossible to model axiomatically.
In the past years  we have witnessed the emergence of learning-based approaches for 3D shape
analysis. The ﬁrst attempts were aimed at learning local shape descriptors [15  5  27]  and shape
correspondence [20]. The dramatic success of deep learning (in particular  convolutional neural
networks [8  14]) in computer vision [13] has led to a recent keen interest in the geometry processing
and graphics communities to apply such methodologies to geometric problems [16  24  28  4  26].

Extrinsic deep learning. Many machine learning techniques successfully working on images were
tried “as is” on 3D geometric data  represented for this purpose in some way “digestible” by stan-
dard frameworks. Su et al. [24] used CNNs applied to range images obtained from multiple views
of 3D objects for retrieval and classiﬁcation tasks. Wei et al. [26] used view-based representation
to ﬁnd correspondence between non-rigid shapes. Wu et al. [28] used volumetric CNNs applied to
rasterized volumetric representation of 3D shapes. The main drawback of such approaches is their
treatment of geometric data as Euclidean structures. Such representations are not intrinsic  and vary

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure1:Illustrationofthedif-ferencebetweenextrinsic(left)andintrinsic(right)deeplearningmethodsongeometricdata.In-trinsicmethodsworkonthemani-foldratherthanitsEuclideanreal-izationandareisometry-invariantbyconstruction.astheresultofposeordeformationoftheobject.Forinstance inFigure1 theﬁlterthatrespondstofeaturesonastraightcylinderwouldnotrespondtoabentone.Achievinginvariancetoshapede-formations acommonrequirementinmanyapplications isextremelyhardwiththeaforementionedmethodsandrequirescomplexmodelsandhugetrainingsetsduetothelargenumberofdegreesoffreedominvolvedindescribingnon-rigiddeformations.Intrinsicdeeplearningapproachestrytoapplylearningtechniquestogeometricdatabygeneral-izingthemainingredientssuchasconvolutionstonon-Euclideandomains.Inanintrinsicrepresen-tation theﬁlterisappliedtosomedataonthesurfaceitself thusbeinginvarianttodeformationsbyconstruction(seeFigure1).Theﬁrstintrinsicconvolutionalneuralnetworkarchitecture(GeodesicCNN)waspresentedin[16].Whileproducingimpressiveresultsonseveralshapecorrespondenceandretrievalbenchmarks GCNNhasanumberofsigniﬁcantdrawbacks.First thechartingproce-dureislimitedtomeshes andsecond thereisnoguaranteethatthechartisalwaystopologicallymeaningful.AnotherintrinsicCNNconstruction(LocalizedSpectralCNN)usinganalternativechartingtechniquebasedonthewindowedFouriertransform[22]wasproposedin[4].Thismethodisageneralizationofapreviouswork[6]onspectraldeeplearningongraphs.Oneofthekeyad-vantagesofLSCNNisthatthesameframeworkcanbeappliedtodifferentshaperepresentations inparticular meshesandpointclouds.Adrawbackofthisapproachisitsmemoryandcomputationrequirements aseachwindowneedstobeexplicitlyproduced.Contributions.WepresentAnisotropicConvolutionalNeuralNetworks(ACNN) amethodforintrinsicdeeplearningonnon-Euclideandomains.Thoughitisagenericframeworkthatcanbeusedtohandledifferenttasks wefocushereonlearningcorrespondencebetweenshapes.Ourap-proachisrelatedtotwopreviousmethodsfordeeplearningonmanifolds GCNN[16]andADD[5].Comparedto[5] wherealearnedspectralﬁlterappliedtotheeigenvaluesofanisotropicLaplace-Beltramioperator weuseanisotropicheatkernelsasspatialweightingfunctionsallowingtoextractalocalintrinsicrepresentationofafunctiondeﬁnedonthemanifold.UnlikeADD ourACNNisaconvolutionalneuralnetworkarchitecture.ComparedtoGCNN ourconstructionofthe“patchop-erator”ismuchsimpler doesnotdependontheinjectivityradiusofthemanifold andisnotlimitedtotriangularmeshes.Overall ACNNcombinesallthebestpropertiesofthepreviousapproacheswithoutinheritingtheirdrawbacks.WeshowthattheproposedframeworkoutperformsGCNN ADD andotherstate-of-the-artapproachesonchallengingcorrespondencebenchmarks.2BackgroundWemodela3Dshapeasatwo-dimensionalcompactRiemannianmanifold(surface)X.LetTxXdenotethetangentplaneatx modelingthesurfacelocallyasaEuclideanspace.ARiemannianmetricisaninnerproducth· ·iTxX:TxX⇥TxX!Ronthetangentplane dependingsmoothlyonx.QuantitieswhichareexpressibleentirelyintermsofRiemannianmetric andthereforeinde-pendentonthewaythesurfaceisembedded arecalledintrinsic.Suchquantitiesareinvarianttoisometric(metric-preserving)deformations.Heatdiffusiononmanifoldsisgovernedbytheheatequation whichhasthemostgeneralformft(x t)=divX(D(x)rXf(x t)) (1)withappropriateboundaryconditionsifnecessary.HererXanddivXdenotetheintrinsicgradientanddivergenceoperators andf(x t)isthetemperatureatpointxattimet.D(x)isthethermalconductivitytensor(2⇥2matrix)appliedtotheintrinsicgradientinthetangentplane.Thisformu-lationallowsmodelingheatﬂowthatisposition-anddirection-dependent(anisotropic).Andreuxet2al.[1]consideredanisotropicdiffusiondrivenbythesurfacecurvature.Boscainietal.[5] assumingthatateachpointxthetangentvectorsareexpressedw.r.t.theorthogonalbasisvm vMofprincipalcurvaturedirections usedathermalconductivitytensoroftheformD↵✓(x)=R✓(x)↵1R>✓(x) (2)wherethe2⇥2matrixR✓(x)performsrotationof✓w.r.t.tothemaximumcurvaturedirectionvM(x) and↵>0isaparametercontrollingthedegreeofanisotropy(↵=1correspondstotheclassicalisotropiccase).Werefertotheoperator↵✓f(x)=divX(D↵✓(x)rXf(x))astheanisotropicLaplacian anddenoteby{↵✓i ↵✓i}i0itseigenfunctionsandeigenvalues(computed ifapplicable withtheappropriateboundaryconditions)satisfying↵✓↵✓i(x)=↵✓i↵✓i(x).Givensomeinitialheatdistributionf0(x)=f(x 0) thesolutionofheatequation(1)attimetisobtainedbyapplyingtheanisotropicheatoperatorHt↵✓=et↵✓tof0 f(x t)=Ht↵✓f0(x)=ZXf0(⇠)h↵✓t(x ⇠)d⇠ (3)whereh↵✓t(x ⇠)istheanisotropicheatkernel andtheaboveequationcanbeinterpretedasanon-shift-invariantversionofconvolution.Inthespectraldomain theheatkernelisexpressedash↵✓t(x ⇠)=Xk0et↵✓k↵✓k(x)↵✓k(⇠).(4)Appealingtothesignalprocessingintuition theeigenvaluesplaytheroleof‘frequencies’ etactsasalow-passﬁlter(largertcorrespondingtolongerdiffusionresultsinaﬁlterwithanarrowerpassband).ThisconstructionwasusedinADD[5]togeneralizetheOSDapproach[15]usinganisotropicheatkernels(consideringthediagonalh↵✓t(x x)andlearningasetofoptimaltask-speciﬁcspectralﬁltersreplacingthelow-passﬁlterset↵✓k).↵ijij✓ijkhR✓ˆumR✓ˆuMˆumˆuMˆnˆekjˆekiˆehiˆehjDiscretization.Inthediscretesetting thesurfaceXissampledatnpointsV={x1 ... xn}.ThepointsareconnectedbyedgesEandfacesF formingamanifoldtriangularmesh(V E F).Toeachtriangleijk2F weattachanorthonormalreferenceframeUijk=(ˆuM ˆum ˆn) whereˆnistheunitnormalvec-tortothetriangleandˆuM ˆum2R3arethedirectionsofprincipalcurvature.Thethermalconductivityten-sorforthetriangleijkoperatingontangentvectorsisexpressedw.r.t.Uijkasa3⇥3matrix⇣↵10⌘.ThediscretizationoftheanisotropicLaplaciantakestheformofann⇥nsparsematrixL=S1W.ThemassmatrixSisadiagonalmatrixofareaelementssi=13Pjk:ijk2FAijk whereAijkdenotestheareaoftriangleijk.ThestiffnessmatrixWiscomposedofweightswij=8><>:12⇣hˆekj ˆekiiH✓sin↵ij+hˆehj ˆehiiH✓sinij⌘(i j)2E;Pk6=iwiki=j;0else (5)wherethenotationisaccordingtotheinsetﬁgure andtheshearmatrixH✓=R✓Uijk⇣↵10⌘U>ijkR>✓encodestheanisotropicscalinguptoanorthogonalbasischange.HereR✓denotesthe3⇥3rotationmatrix rotatingthebasisvectorsUijkoneachtrianglearoundthenormalˆnbyangle✓.33

Intrinsic deep learning

This paper deals with the extension of the popular convolutional neural networks (CNN) [14] to
non-Euclidean domains. The key feature of CNNs is the convolutional layer  implementing the idea
of “weight sharing”  wherein a small set of templates (ﬁlters) is applied to different parts of the data.
In image analysis applications  the input into the CNN is a function representing pixel values given
on a Euclidean domain (plane); due to shift-invariance the convolution can be thought of as passing
a template across the plane and recording the correlation of the template with the function at that
location. One of the major problems in applying the same paradigm to non-Euclidean domains is
the lack of shift-invariance  the template now has to be location-dependent.
Among the recent attempts to develop intrinsic CNNs on non-Euclidean domain [6  4  16]  the most
related to our work is GCNN [16]. The latter approach was introduced as a generalization of CNN
to triangular meshes based on geodesic local patches. The core of this method is the construction of
local geodesic polar coordinates using a procedure previously employed for intrinsic shape context
descriptors [12]. The patch operator (D(x)f )(✓  ⇢) in GCNN maps the values of the function
f around vertex x into the local polar coordinates ✓  ⇢  leading to the deﬁnition of the geodesic
convolution

(f ⇤ a)(x) =

✓2[0 2⇡)Z a(✓ + ✓  ⇢)(D(x)f )(✓  ⇢)d⇢d✓ 

max

which follows the idea of multiplication by template  but is deﬁned up to arbitrary rotation ✓ 2
[0  2⇡) due to the ambiguity in the selection of the origin of the angular coordinate. The authors
propose to take the maximum over all possible rotations of the template a(⇢  ✓) to remove this
ambiguity. Here  and in the following  f is some feature vector that is deﬁned on the surface (e.g.
texture  geometric descriptors  etc.)
There are several drawbacks to this construction. First  the charting method relies on a fast marching-
like procedure requiring a triangular mesh. While relatively insensitive to triangulation [12]  it may
fail if the mesh is very irregular. Second  the radius of the geodesic patches must be sufﬁciently
small compared to the injectivity radius of the shape  otherwise the resulting patch is not guaranteed
to be a topological disk. In practice  this limits the size of the patches one can safely use  or requires
an adaptive radius selection mechanism.

4 Anisotropic convolutional neural networks
The key idea of the Anisotropic CNN presented in this paper is the construction of a patch operator
using anisotropic heat kernels. We interpret heat kernels as local weighting functions and construct

(D↵(x)f )(✓  t) = RX h↵✓t(x  ⇠)f (⇠)d⇠
RX h↵✓t(x  ⇠)d⇠

 

(7)

for some anisotropy level ↵> 1. This way  the values of f around point x are mapped to a local
system of coordinates (✓  t) that behaves like a polar system (here t denotes the scale of the heat
kernel and ✓ is its orientation). We deﬁne intrinsic convolution as

(6)

(8)

(f ⇤ a)(x) = Z a(✓  t)(D↵(x)f )(✓  t)dtd✓ 

Note that unlike the arbitrarily oriented geodesic patches in GCNN  necessitating to take a maximum
over all the template rotations (6)  in our construction it is natural to use the principal curvature
direction as the reference ✓ = 0.
Such an approach has a few major advantages compared to previous intrinsic CNN models. First 
being a spectral construction  our patch operator can be applied to any shape representation (like
LSCNN and unlike GCNN). Second  being deﬁned in the spatial domain  the patches and the result-
ing ﬁlters have a clear geometric interpretation (unlike LSCNN). Third  our construction accounts
for local directional patterns (like GCNN and unlike LSCNN). Fourth  the heat kernels are always
well deﬁned independently of the injectivity radius of the manifold (unlike GCNN). We summarize
the comparative advantages in Table 1.
ACNN architecture. Similarly to Euclidean CNNs  our ACNN consists of several layers that are
applied subsequently  i.e. the output of the previous layer is used as the input into the subsequent one.

4

Generalizable

Context Directional

Input

Geometry
Geometry

Repr.
Method
Any
OSD [15]
Any
ADD [5]
RF [20]
Any
GCNN [16] Mesh
Any
SCNN [6]
LSCNN [4]
Any
ACNN
Any
Table 1: Comparison of different intrinsic learning models. Our ACNN model combines all the best
properties of the other models. Note that OSD and ADD are local spectral descriptors operating
with intrinsic geometric information of the shape and cannot be applied to arbitrary input  unlike the
Random Forest (RF) and convolutional models.

Filters
Spectral
Spectral
Spectral
Spatial
Spectral
Spectral
Spatial

No
No
No
Yes
Yes
Yes
Yes

Any

Any
Any
Any
Any

No
Yes
No
Yes
No
No
Yes

Yes
Yes
Yes
Yes
No
Yes
Yes

Task

Descriptors

Correspondence

Any
Any
Any
Any
Any

ACNN  as any convolutional network  is applied in a point-wise manner on a function deﬁned on
the manifolds  producing a point-wise output that is interpreted as soft correspondence  as described
below. Our intrinsic convolutional layer ICQ  with Q output maps  is deﬁned as follows and replaces
the convolutional layer used in classical Euclidean CNNs with the construction (8). The ICQ layer
contains P Q ﬁlters arranged in banks (P ﬁlters in Q banks); each bank corresponds to an output
dimension. The ﬁlters are applied to the input as follows 

f out
q (x) =

(f in
p ⇤ aqp)(x) 

q = 1  . . .   Q 

(9)

PXp=1

where aqp(✓  t) are the learnable coefﬁcients of the pth ﬁlter in the qth ﬁlter bank. A visualization
of such ﬁlters is available in the supplementary material.
Overall  the ACNN architecture combining several layers of different type  acts as a non-linear
parametric mapping of the form f⇥(x) at each point x of the shape  where ⇥ denotes the set of
all learnable parameters of the network. The choice of the parameters is done by an optimization
process  minimizing a task-speciﬁc cost  and can thus be rather general. Here  we focus on learning
shape correspondence.

Learning correspondence Finding correspondence in a collection of shapes can be cast as a la-
belling problem  where one tries to label each vertex of a given query shape X with the index of a
corresponding point on some reference shape Y [20]. Let n and m denote the number of vertices in
X and Y   respectively. For a point x on a query shape  the output of ACNN f⇥(x) is m-dimensional
and is interpreted as a probability distribution (‘soft correspondence’) on Y . The output of the
network at all the points of the query shape represents the probability of x mapped to y.
Let us denote by y⇤(x) the ground-truth correspondence of x on the reference shape. We assume
to be provided with examples of points from shapes across the collection and their ground-truth
correspondence  T = {(x  y⇤(x))}. The optimal parameters of the network are found by minimizing
the multinomial regression loss

`reg(⇥) =  X(x y⇤(x))2T

log f⇥(x  y⇤(x)).

(10)

5 Results
In this section  we evaluate the proposed ACNN method and compare it to state-of-the-art ap-
proaches. Anisotropic Laplacians were computed according to (5). Heat kernels were computed
in the frequency domain using all the eigenpairs. In all experiments  we used L = 16 orientations
and the anisotropy parameter ↵ = 100. Neural networks were implemented in Theano [2]. The
ADAM [11] stochastic optimization algorithm was used with initial learning rate of 103  1 = 0.9 
and 2 = 0.999. As the input to the networks  we used the local SHOT descriptor [21] with 544
dimensions and using default parameters. For all experiments  training was done by minimizing the
loss (10). For shapes with 6.9K vertices  Laplacian computation and eigendecomposition took 1 sec
and 4 seconds per angle  respectively on a desktop workstation with 64Gb of RAM and i7-4820K
CPU. Forward propagation of the trained model takes approximately 0.5 sec to produce the dense
soft correspondence for all the vertices.

5

cm
20

30

40

0

1

10

s
e
c
n
e
d
n
o
p
s
e
r
r
o
c
%

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

0.2

RF
PFM
ACNN
0.15
0.05
% geodesic diameter

0.1

Geodesic error

RF
PFM
ACNN
0.15
0.05
% geodesic diameter

0.1

Geodesic error

0.2

0.2

BIM
LSCNN
RF
ADD
GCNN
ACNN
0.15
% geodesic diameter

0.05

0.1

Geodesic error

Figure 2: Performance of different correspondence methods 
left to right: FAUST meshes 
SHREC’16 Partial cuts and holes. Evaluation of the correspondence was done using the Prince-
ton protocol.
Full mesh correspondence We used the FAUST humans dataset [3]  containing 100 meshes of
10 scanned subjects  each in 10 different poses. The shapes in the collection manifest strong
non-isometric deformations. Vertex-wise groundtruth correspondence is known between all the
shapes. The zeroth FAUST shape containing 6890 vertices was used as reference; for each point
on the query shape  the output of the network represents the soft correspondence as a 6890-
dimensional vector which was then converted to point correspondence with the technique explained
in Section 4. First 80 shapes for training and the remaining 20 for testing  following verba-
tim the settings of [16]. Batch normalization [9] allowed to effectively train larger and deeper
networks. For this experiment  we adopted the following architecture inspired by GCNN [16]:
FC64+IC64+IC128+IC256+FC1024+FC512+Softmax. The soft correspondences produced by the
net were reﬁned using functional map [18]. We refer to the supplementary material for the details.
We compare to Random Forests (RF) [20]  Blended Intrinsic Maps (BIM) [10]  Localized Spectral
CNN (LSCNN) [4]  and Anisotropic Diffusion Descriptors (ADD) [5].
Figure 2 (left) shows the performance of different methods. The performance was evaluated us-
ing the Princeton protocol [10]  plotting the percentage of matches that are at most r-geodesically
distant from the groundtruth correspondence on the reference shape. Two versions of the proto-
col consider intrinsically symmetric matches as correct (symmetric setting  solid curves) or wrong
(asymmetric  more challenging setting  dashed curves). Some methods based on intrinsic structures
(e.g. LSCNN or RF applied on WKS descriptors) are invariant under intrinsic symmetries and thus
cannot distinguish between symmetric points. The proposed ACNN method clearly outperforms
all the compared approaches and also perfectly distinguishes symmetric points. Figure 3 shows the
pointwise geodesic error of different correspondence methods (distance of the correspondence at a
point from the groundtruth). ACNN shows dramatically smaller distortions compared to other meth-
ods. Over 60% of matches are exact (zero geodesic error)  while only a few points have geodesic
error larger than 10% of the geodesic diameter of the shape 1. Please refer to the supplementary
material for an additional visualization of the quality of the correspondences obtained with ACNN
in terms of texture transfer.

Partial correspondence We used the recent very challenging SHREC’16 Partial Correspon-
dence benchmark [7]  consisting of nearly-isometrically deformed shapes from eight classes  with
different parts removed. Two types of partiality in the benchmark are cuts (removal of a few
large parts) and holes (removal of many small parts). In each class  the vertex-wise groundtruth
correspondence between the full shape and its partial versions is given. The dataset was split
into training and testing disjoint sets. For cuts  training was done on 15 shapes per class; for
holes  training was done on 10 shapes per class. We used the following ACNN architecture:
IC32+FC1024+DO(0.5)+FC2048+DO(0.5)+Softmax. The soft correspondences produced by the
net were reﬁned using partial functional correspondence [19]. We refer to the supplementary mate-

1Per subject leave-one-out produces comparable results with mean accuracy of 59.6 ± 3.7%.

6

0.1

0

Blended Intrinsic Maps

Geodesic CNN

Anisotropic CNN

Figure 3: Pointwise geodesic error (in % of geodesic diameter) of different correspondence methods
(top to bottom: Blended Intrinsic Maps  GCNN  ACNN) on the FAUST dataset. Error values are
saturated at 10% of the geodesic diameter. Hot colors correspond to large errors.

rial for the details. The dropout regularization  with ⇡drop = 0.5  was crucial to avoid overﬁtting on
such a small training set. We compared ACNN to RF [20] and Partial Functional Maps (PFM) [19].
For the evaluation  we used the protocol of [7]  which closely follows the Princeton benchmark.
Figure 2 (middle) compares the performance of different partial matching methods on the
SHREC’16 Partial (cuts) dataset. ACNN outperforms other approaches with a signiﬁcant margin.
Figure 4 (top) shows examples of partial correspondence on the horse shape as well as the point-
wise geodesic error. We observe that the proposed approach produces high-quality correspondences
even in such a challenging setting. Figure 2 (right) compares the performance of different partial
matching methods on the SHREC’16 Partial (holes) dataset. In this setting as well  ACNN out-
performs other approaches with a signiﬁcant margin. Figure 4 (bottom) shows examples of partial
correspondence on the dog shape as well as the pointwise geodesic error.

6 Conclusions
We presented Anisotropic CNN  a new framework generalizing convolutional neural networks to
non-Euclidean domains  allowing to perform deep learning on geometric data. Our work follows
the very recent trend in bringing machine learning methods to computer graphics and geometry
processing applications  and is currently the most generic intrinsic CNN model. Our experiments
show that ACNN outperforms previously proposed intrinsic CNN models  as well as additional
state-of-the-art methods in the shape correspondence application in challenging settings. Being a
generic model  ACNN can be used for many other applications. The most promising future work
direction is applying ACNN to learning on graphs.

7

0.1

0

0.1

0

Anisotropic CNN

Random Forest

Anisotropic CNN

Random Forest

Figure 4: Examples of partial correspondence on the SHREC’16 Partial cuts (top) and holes (bottom)
datasets. Rows 1 and 4: correspondence produced by ACNN. Corresponding points are shown in
similar color. Reference shape is shown on the left. Rows 2  5 and 3  6: pointwise geodesic error
(in % of geodesic diameter) of the ACNN and RF correspondence  respectively. Error values are
saturated at 10% of the geodesic diameter. Hot colors correspond to large errors.

8

Acknowledgments

The authors wish to thank Matteo Sala for the textured models. This research was supported by
the ERC Starting Grant No. 307047 (COMET)  a Google Faculty Research Award  and Nvidia
equipment grant.

References
[1] M. Andreux  E. Rodol`a  M. Aubry  and D. Cremers. Anisotropic Laplace-Beltrami operators for shape

analysis. In Proc. NORDIA  2014.

[2] J. Bergstra et al. Theano: a CPU and GPU math expression compiler. In Proc. SciPy  June 2010.
[3] F. Bogo  J. Romero  M. Loper  and M. J. Black. FAUST: Dataset and evaluation for 3D mesh registration.

In Proc. CVPR  2014.

[4] D. Boscaini  J. Masci  S. Melzi  M. M. Bronstein  U. Castellani  and P. Vandergheynst. Learning class-
speciﬁc descriptors for deformable shapes using localized spectral convolutional networks. Computer
Graphics Forum  34(5):13–23  2015.

[5] D. Boscaini  J. Masci  E. Rodol`a  M. M. Bronstein  and D. Cremers. Anisotropic diffusion descriptors.

[6] J. Bruna  W. Zaremba  A. Szlam  and Y. LeCun. Spectral networks and locally connected networks on

Computer Graphics Forum  35(2)  2016.

graphs. In Proc. ICLR  2014.

[7] L. Cosmo  E. Rodol`a  M. M. Bronstein  A. Torsello  D. Cremers  and Y. Sahillio˘glu. Shrec’16: Partial

matching of deformable shapes. In Proc. 3DOR  2016.

[8] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recog-

nition unaffected by shift in position. Biological Cybernetics  36(4):193–202  1980.

[9] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In Proc. ICML  pages 448–456  2015.

[10] V. G. Kim  Y. Lipman  and T. Funkhouser. Blended intrinsic maps. TOG  30(4):79  2011.
[11] D. P. Kingma and J. Ba. ADAM: A method for stochastic optimization. In ICLR  2015.
[12] I. Kokkinos  M. M. Bronstein  R. Litman  and A. M. Bronstein. Intrinsic shape context descriptors for

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural

deformable shapes. In Proc. CVPR  2012.

networks. In Proc. NIPS  2012.

[14] Y. LeCun  B. Boser  J. S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D. Jackel. Backprop-

agation applied to handwritten zip code recognition. Neural computation  1(4):541–551  1989.

[15] R. Litman and A. M. Bronstein. Learning spectral descriptors for deformable shape correspondence.

[16] J. Masci  D. Boscaini  M. M. Bronstein  and P. Vandergheynst. Geodesic convolutional neural networks

[17] F. M´emoli. Gromov-Wasserstein Distances and the Metric Approach to Object Matching. Foundations of

PAMI  36(1):170–180  2014.

on riemannian manifolds. In Proc. 3dRR  2015.

Computational Mathematics  pages 1–71  2011.

[18] M. Ovsjanikov  M. Ben-Chen  J. Solomon  A. Butscher  and L. Guibas. Functional maps: a ﬂexible

representation of maps between shapes. TOG  31(4):1–11  2012.

[19] E. Rodol`a  L. Cosmo  M. M. Bronstein  A. Torsello  and D. Cremers. Partial functional correspondence.

Computer Graphics Forum  2016.

[20] E. Rodol`a  S. Rota Bul`o  T. Windheuser  M. Vestner  and D. Cremers. Dense non-rigid shape correspon-

dence using random forests. In Proc. CVPR  2014.

[21] S. Salti  F. Tombari  and L. Di Stefano. SHOT: unique signatures of histograms for surface and texture

description. CVIU  125:251–264  2014.

[22] D. I Shuman  B. Ricaud  and P. Vandergheynst. Vertex-frequency analysis on graphs. arXiv:1307.5708 

[23] J. Solomon  A. Nguyen  A. Butscher  M. Ben-Chen  and L. Guibas. Soft maps between surfaces. Com-

puter Graphics Forum  31(5):1617–1626  2012.

[24] H. Su  S. Maji  E. Kalogerakis  and E. Learned-Miller. Multi-view convolutional neural networks for 3D

2013.

shape recognition. In Proc. ICCV  2015.

Graphics Forum  20:1–23  2010.

tional networks. In Proc. CVPR  2016.

[25] O. van Kaick  H. Zhang  G. Hamarneh  and D. Cohen-Or. A survey on shape correspondence. Computer

[26] L. Wei  Q. Huang  D. Ceylan  E. Vouga  and H. Li. Dense human body correspondences using convolu-

[27] T. Windheuser  M. Vestner  E. Rodol`a  R. Triebel  and D. Cremers. Optimal intrinsic descriptors for

non-rigid shape analysis. In Proc. BMVC  2014.

[28] Z. Wu  S. Song  A. Khosla  et al. 3D ShapeNets: A deep representation for volumetric shapes. In Proc.

CVPR  2015.

9

,Davide Boscaini
Jonathan Masci
Emanuele Rodolà
Michael Bronstein
Yihan Jiang
Hyeji Kim
Himanshu Asnani
Sreeram Kannan
Sewoong Oh
Pramod Viswanath