2019,Distributional Reward Decomposition for Reinforcement Learning,Many reinforcement learning (RL) tasks have specific properties that can be leveraged to modify existing RL algorithms to adapt to those tasks and further improve performance  and a general class of such properties is the multiple reward channel. In those environments the full reward can be decomposed into sub-rewards obtained from different channels. Existing work on reward decomposition either requires prior knowledge of the environment to decompose the full reward  or decomposes reward without prior knowledge but with degraded performance. In this paper  we propose Distributional Reward Decomposition for Reinforcement Learning (DRDRL)  a novel reward decomposition algorithm which captures the multiple reward channel structure under distributional setting. Empirically  our method captures the multi-channel structure and discovers meaningful reward decomposition  without any requirements on prior knowledge. Consequently  our agent achieves better performance than existing methods on environments with multiple reward channels.,Distributional Reward Decomposition for

Reinforcement Learning

Zichuan Lin∗

Tsinghua University

linzc16@mails.tsinghua.edu.cn

Li Zhao

Microsoft Research

lizo@microsoft.com

Derek Yang
UC San Diego

dyang1206@gmail.com

Tao Qin

Microsoft Research

taoqin@microsoft.com

Guangwen Yang
Tsinghua University

ygw@tsinghua.edu.cn

Abstract

Tie-Yan Liu

Microsoft Research

tyliu@microsoft.com

Many reinforcement learning (RL) tasks have speciﬁc properties that can be lever-
aged to modify existing RL algorithms to adapt to those tasks and further improve
performance  and a general class of such properties is the multiple reward channel.
In those environments the full reward can be decomposed into sub-rewards obtained
from different channels. Existing work on reward decomposition either requires
prior knowledge of the environment to decompose the full reward  or decomposes
reward without prior knowledge but with degraded performance. In this paper 
we propose Distributional Reward Decomposition for Reinforcement Learning
(DRDRL)  a novel reward decomposition algorithm which captures the multiple
reward channel structure under distributional setting. Empirically  our method cap-
tures the multi-channel structure and discovers meaningful reward decomposition 
without any requirements on prior knowledge. Consequently  our agent achieves
better performance than existing methods on environments with multiple reward
channels.

1

Introduction

Reinforcement learning has achieved great success in decision making problems since Deep Q-
learning was proposed by Mnih et al. [2015]. While general RL algorithms have been deeply studied 
here we focus on those RL tasks with speciﬁc properties that can be utilized to modify general RL
algorithms to achieve better performance. Speciﬁcally  we focus on RL environments with multiple
reward channels  but only the full reward is available.
Reward decomposition has been proposed to investigate such properties. For example  in Atari game
Seaquest  rewards of environment can be decomposed into sub-rewards of shooting sharks and those
of rescuing divers. Reward decomposition views the total reward as the sum of sub-rewards that are
usually disentangled and can be obtained independently (Sprague and Ballard [2003]  Russell and
Zimdars [2003]  Van Seijen et al. [2017]  Grimm and Singh [2019])  and aims at decomposing the
total reward into sub-rewards. The sub-rewards may further be leveraged to learn better policies.
Van Seijen et al. [2017] propose to split a state into different sub-states  each with a sub-reward
obtained by training a general value function  and learn multiple value functions with sub-rewards.
The architecture is rather limited due to requiring prior knowledge of how to split into sub-states.
Grimm and Singh [2019] propose a more general method for reward decomposition via maximizing

∗Contributed during internship at Microsoft Research.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

disentanglement between sub-rewards. In their work  an explicit reward decomposition is learned
via maximizing the disentanglement of two sub-rewards estimated with action-value functions.
However  their work requires that the environment can be reset to arbitrary state and can not apply
to general RL settings where states can hardly be revisited. Furthermore  despite the meaningful
reward decomposition they achieved  they fail to utilize the reward decomposition into learning better
policies.
In this paper  we propose Distributional Reward Decomposition for Reinforcement Learning (DR-
DRL)  an RL algorithm that captures the latent multiple-channel structure for reward  under the
setting of distributional RL. Distributional RL differs from value-based RL in that it estimates the
distribution rather than the expectation of returns  and therefore captures richer information than
value-based RL. We propose an RL algorithm that estimates distributions of the sub-returns  and com-
bine the sub-returns to get the distribution of the total returns. In order to avoid naive decomposition
such as 0-1 or half-half  we further propose a disentanglement regularization term to encourage the
sub-returns to be diverged. To better separate reward channels  we also design our network to learn
different state representations for different channels.
We test our algorithm on chosen Atari Games with multiple reward channels. Empirically  our method
has following achievements:

• Discovers meaningful reward decomposition.
• Requires no external information.
• Achieves better performance than existing RL methods.

2 Background

We consider a general reinforcement learning setting  in which the interaction of the agent and the
environment can be viewed as a Markov Decision Process (MDP). Denote the state space by X  
action space by A  the state transition function by P   the action-state dependent reward function by
R and γ the discount factor  we write this MDP as (X   A  R  P  γ).
Given a ﬁxed policy π  reinforcement learning estimates the action-value function of π  deﬁned by
t=0 γtrt(xt  at) where (xt  at) is the state-action pair at time t  x0 = x  a0 = a and
rt is the corresponding reward. The Bellman equation characterizes the action-value function by
temporal equivalence  given by

Qπ(x  a) =(cid:80)∞

Qπ(x  a) = R(x  a) + γ E

x(cid:48) a(cid:48) [Qπ(x(cid:48)  a(cid:48))]

where x(cid:48) ∼ P (·|x  a)  a(cid:48) ∼ π(·|x(cid:48)). To maximize the total return given by E
(cid:105)
common approach is to ﬁnd the ﬁxed point for the Bellman optimality operator
a(cid:48) Q∗(x(cid:48)  a(cid:48))

Q∗(x  a) = T Q∗(x  a) = R(x  a) + γE
x(cid:48)

max

(cid:104)

x0 a0

with the temporal difference (TD) error  given by

(cid:21)2

[Qπ(x0  a0)]  one

(cid:20)

δ2
t =

rt + γ max

a(cid:48)∈A Q (xt+1  a(cid:48)) − Q (xt  at)

over the samples (xt  at  st  xt+1) along the trajectory. Mnih et al. [2015] propose Deep Q-Networks
(DQN) that learns the action-value function with a neural network and achieves human-level perfor-
mance on the Atari-57 benchmark.

2.1 Reward Decomposition

Studies for reward decomposition also leads to state decomposition (Laversanne-Finot et al. [2018] 
Thomas et al. [2017])  where state decomposition is leveraged to learn different policies. Extending
their work  Grimm and Singh [2019] explore the decomposition of the reward function directly  which
is considered to be most related to our work. Denote the i-th (i=1 2 ... N) sub-reward function at
state-action pair (x  a) as ri(x  a)  the complete reward function is given by

r =

ri

N(cid:88)

i=1

2

For each sub-reward function  consider the sub-value function U π

i and corresponding policy πi:

∞(cid:88)

i (x0  a0) = Ext at [
U π

γtri(xt  at)]

πi = arg max

π

t=0

U π
i

  
(cid:35)

(cid:19)

where xt ∼ P (·|π  x0  a0)  at ∼ π(·|xt).
In their work  reward decomposition is considered meaningful if each reward is obtained indepen-
dently (i.e. πi should not obtain rj) and each reward is obtainable.
Two evaluate the two desiderata  the work proposes the following values:

Jindependent (r1  . . .   rn) = Es∼µ

π∗
αi j(s)U
i

j

(s)

(cid:88)
(cid:34) n(cid:88)

i(cid:54)=j

(1)

(2)

Jnontrivial (r1  . . .   rn) = Es∼µ

αi i(s)U π∗

i

(s)

 

i

where αi j is for weight control and for simplicity set to 1 in their work. During training  the network
would maximize Jnontrivial − Jindependent to achieve the desired reward decomposition.

i=1

2.2 Distributional Reinforcement Learning

In most reinforcement learning settings  the environment is not deterministic. Moreover  in general
people train RL models with an -greedy policy to allow exploration  making the agent also stochastic.
To better analyze the randomness under this setting  Bellemare et al. [2017] propose C51 algorithm
and conduct theoretical analysis of distributional RL.
In distributional RL  reward Rt is viewed as a random variable  and the total return is deﬁned by
t=0 γtRt. The expectation of Z is the traditional action-value Q and the distributional

Z = (cid:80)∞

Bellman optimality operator is given by

(cid:18)

T Z(x  a) :D= R(x  a) + γZ

x(cid:48)  arg max
a(cid:48)∈A

EZ (x(cid:48)  a(cid:48))

where if random variable A and B satisﬁes A D= B then A and B follow the same distribution.
Random variable is characterized by a categorical distribution over a ﬁxed set of values in C51  and it
outperforms all previous variants of DQN on Atari domain.

3 Distributional Reward Decomposition for Reinforcement Learning

3.1 Distributional Reward Decomposition

sub-return Zi =(cid:80)∞

In many reinforcement learning environments  there are multiple sources for an agent to receive
reward as shown in Figure 1(b). Our method is mainly designed for environments with such property.
Under distributional setting  we will assume reward and sub-rewards are random variables and
denote them by R and Ri respectively. In our architecture  the categorical distribution of each
t=0 γtRi t is the output of a network  denoted by Fi(x  a). Note that in most
cases  sub-returns are not independent  i.e. P (Zi = v)! = P (Zi = v|Zj). So theoretically we need
Fij(x  a) for each i and j to obtain the distribution of the full return. We call this architecture as non-
factorial model or full-distribution model. The non-factorial model architecture is shown in appendix.
However  experiment shows that using an approximation form of P (Zi = v) ≈ P (Zi = v|Zj) so
that only Fi(x  a) is required performs much better than brutally computing Fij(x  a) for all i  j  we
believe that is due to the increased sample number. In this paper  we approximate the conditional
probability P (Zi = v|Zj) with P (Zi = v).
Consider categorical distribution function Fi and Fj with same number of atoms K  the k-th atom
is denoted by ak with value ak = a0 + kl  1 ≤ k ≤ K where l is a constant. Let random variable

3

(a)

(b)

Figure 1: (a) Distributional reward decomposition network architecture. (b) Examples of multiple
reward channels in Atari games: the top row shows examples of Seaquest in which the submarine
receives rewards from both shooting sharks and rescuing divers; the bottom row shows examples of
Hero where the hero receives rewards from both shooting bats and rescuing people.

Zi ∼ Fi and Zj ∼ Fj  from basic probability theory we know that the distribution function of
Z = Zi + Zj is the convolution of Fi and Fj

F(v) = P (Zi + Zj = v) =

P (Zi = ak)P (Zj = v − ak|Zi = ak)

P (Zi = ak)P (Zj = v − ak) = Fi(v) ∗ Fj(v).

(3)

K(cid:88)
≈ K(cid:88)

k=1

k=1

When we use N sub-returns  the distribution function of the total return is then given by F =
F1 ∗ F2 ∗ ··· ∗ FN where ∗ denotes linear 1D-convolution.
While reward decomposition is not explicitly done in our algorithm  we can derive the decomposed
i=1 Zi follows bellman equation  so

reward with using trained agents. Recall that total return Z =(cid:80)N

naturally we have

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)

i=1

T Z D= T (

Zi) D= R + γZ(cid:48) = (

Ri) + γ(

Z(cid:48)
i)

(4)

where Z(cid:48)
i represents sub-return on the next state-action pair. Note that we only have access to a
sample of the full reward R  the sub-rewards Ri are arbitrary and for better visualization a direct way
of deriving them is given by

(5)
In the next section we will present an example of those sub-rewards by taking their expectation E(Ri).
Note that our reward decomposition is latent and we do not need Ri for our algorithm  Eq. 5 only
provides an approach to visualize our reward decomposition.

Ri = Zi − γZ(cid:48)

i

3.2 Disentangled Sub-returns

To obtain meaningful reward decomposition  we want the sub-rewards to be disentangled. Inspired
by Grimm and Singh [2019]  we compute the disentanglement of distributions of two sub-returns F i
and F j on state x with the following value:

disentang = DKL(Fx arg maxa
J ij

E(Zi)||Fx arg maxa

E(Zj )) 

(6)

where DKL denotes the cross-entropy term of KL divergence.

4

𝑥ConvolutionSoftmaxSoftmax*Softmax···𝐹1𝐹2𝐹N𝐹···𝜓(𝑥)𝜙𝑒1𝜙𝑒2···𝜙(𝑒𝑁)·Intuitively  J ij
disentang estimates the disentanglement of sub-returns Zi and Zj by ﬁrst obtaining
actions that maximize E(Zi) and E(Zj) respectively  and then computes the KL divergence between
the two estimated total returns of the actions. If Zi and Zj are independent  the action maximizing
two sub-returns would be different and such difference would reﬂect in the estimation for total
return. Through maximizing this value  we can expect a meaningful reward decomposition that learns
independent rewards.

3.3 Projected Bellman Update with Regularization

Following the work of C51 (Bellemare et al. [2017])  we use projected Bellman update for our
algorithm. When applied with the Bellman optimality operator  the atoms of T Z is shifted by rt and
shrank by γ. However to compute the loss  usually KL divergence between Z and T Z  it is required
that the two categorical distributions are deﬁned on the same set of atoms  so the target distribution
T Z would need to be projected to the original set of atoms before Bellman update. Consider a sample
transition (x  a  r  x(cid:48))  the projection operator Φ proposed in C51 is given by

1 −

M−1(cid:88)

j=0

(cid:12)(cid:12)(cid:12)[r + γaj]Vmax

Vmin
l

− ai

(cid:12)(cid:12)(cid:12)

1

0

(ΦT Z(x  a))i =

Fx(cid:48) a(cid:48) (aj)  

(7)

where M is the number of atoms in C51 and [·]b
(x  a  r  x(cid:48)) would be given by the cross-entropy term of KL divergence of Z and ΦT Z

a bounds its argument in [a  b]. The sample loss for

Lx a r x(cid:48) = DKL(ΦT Z(x  a)||Z(x  a)).

(8)
Let F θ be a neural network parameterized by θ  we combine distributional TD error and disentan-
glement to jointly update θ. For each sample transition (x  a  r  x(cid:48))  θ is updated by minimizing the
following objective function:

(cid:88)

(cid:88)

Lx a r x(cid:48) − λ

J ij
disentang 

(9)

where λ denotes learning rate.

i

j!=i

3.4 Multi-channel State Representation
One complication of our approach outlined above is that very often the distribution Fi cannot
distinguish itself from other distributions (e.g.  Fj  j (cid:54)= i) during learning since they all depend
on the same state feature input. This brings difﬁculties in maximizing disentanglement by jointly
training as different distribution functions are exchangeable. A naive idea is to split the state feature
ψ(x) into N pieces (e.g.  ψ(x)1  ψ(x)2  ...  ψ(x)N ) so that each distribution depends on different
sub-state-features. However  we empirically found that this method is not enough to help learn good
disentangled sub-returns.
To address this problem  we utilize an idea similar to universal value function approximation
(UVFA) (Schaul et al. [2015]). The key idea is to take one-hot embedding as an additional in-
put to condition the categorical distribution function  and apply the element-wise multiplication
ψ (cid:12) φ  to force interaction between state features and the one-hot embedding feature:

Fi(x  a) = Fθi(ψ(x) (cid:12) φ(ei))a 

(10)
where ei denotes the one-hot embedding where the i-th element is one and φ denotes one-layer
non-linear neural network that is updated by backpropagation during training.
In this way  the agent explicitly learns different distribution functions for different channels. The
complete network architecture is shown in Figure 1(a).

4 Experiment Results

We tested our algorithm on the games from Arcade Learning Environment (ALE; Bellemare et al.
[2013]). We conduct experiments on six Atari games  some with complicated rules and some with

5

Figure 2: Performance comparison with Rainbow. RD(N) represents using N-channel reward
decomposition. Each training curve is averaged by three random seeds.

simple rules. For our study  we implemented our algorithm based on Rainbow (Hessel et al. [2018])
which is an advanced variant of C51 (Bellemare et al. [2017]) and achieved state-of-the-art results
in Atari games domain. We replace the update rule of Rainbow by Eq. 9 and network architecture
of Rainbow by our convoluted architecture as shown in Figure 1(a). In Rainbow  the Q-value is
bounded by [Vmin  Vmax] where Vmax = −Vmin = 10. In our method  we bound the categorical
distribution of each sub-return Zi(i = 1  2  ...  N ) by a range of [ Vmin
N ]. Rainbow uses a
categorical distribution with M = 51 atoms. For fair comparison  we assign K = (cid:98) M
N (cid:99) atoms for the
distribution of each sub-return  which results in the same network capacity as the original network
architecture.
Our code is built upon dopamine framework (Castro et al. [2018]). We use the default well-tuned
hyper-parameter setting in dopamine. For our updating rule in Eq. 9  we set λ = 0.0001. We run our
agents for 100 epochs  each with 0.25 million of training steps and 0.125 million of evaluation steps.
For evaluation  we follow common practice in Van Hasselt et al. [2016]  starting the game with up to
30 no-op actions to provide random starting positions for the agent. All experiments are performed
on NVIDIA Tesla V100 16GB graphics cards.

N   Vmax

4.1 Comparison with Rainbow

To verify that our architecture achieves reward decomposition without degraded performance  we
compare our methods with Rainbow. However we are not able to compare our method with Van Seijen
et al. [2017] and Grimm and Singh [2019] since they require either pre-deﬁned state pre-processing
or speciﬁc-state resettable environments. We test our reward decomposition (RD) with 2 and 3
channels (e.g.  RD(2)  RD(3)). The results are shown in Figure 2. We found that our methods
perform signiﬁcantly better than Rainbow on the environments that we tested. This implies that
our distributional reward decomposition method can help accelerate the learning process. We also
discover that on some environments  RD(3) performs better than RD(2) while in the rest the two have
similar performance. We conjecture that this is due to the intrinsic settings of the environments. For
example  in Seaquest and UpNDown  the rules are relatively complicated  so RD(3) characterizes
such complex reward better. However in simple environments like Gopher and Asterix  RD(2) and
RD(3) obtain similar performance  and sometimes RD(2) even outperforms RD(3).

4.2 Reward Decomposition Analysis

Here we use Seaquest to illustrate our reward decomposition. Figure 3 shows the sub-rewards
obtained by taking the expectation of the LHS of Eq.5 and the original reward along an actual
trajectory. We observe that while r1 = E(R1) and r2 = E(R2) basically add up to the original
reward r  r1 dominates as the submarine is close to the surface  i.e. when it rescues the divers and

6

020406080100Epoch050001000015000200002500030000ReturnSeaquestRAINBOW + RD(3)RAINBOW + RD(2)RAINBOW020406080100Epoch02500500075001000012500150001750020000ReturnAsterixRAINBOW + RD(3)RAINBOW + RD(2)RAINBOW020406080100Epoch0200040006000800010000120001400016000ReturnGopherRAINBOW + RD(3)RAINBOW + RD(2)RAINBOW020406080100Epoch010000200003000040000500006000070000ReturnStarGunnerRAINBOW + RD(3)RAINBOW + RD(2)RAINBOW020406080100Epoch010000200003000040000ReturnHeroRAINBOW + RD(3)RAINBOW + RD(2)RAINBOW020406080100Epoch0100002000030000400005000060000ReturnUpNDownRAINBOW + RD(3)RAINBOW + RD(2)RAINBOWFigure 3: Reward decomposition along the trajectory. While sub-rewards r1 and r2 usually adds
up to the original reward r  we see that the proportion of sub-rewards greatly depends on how the
original reward is obtained.

reﬁlls oxygen. When the submarine scores by shooting sharks  r2 becomes the main source of reward.
We also monitor the distribution of different sub-returns when the agent is playing game. In Figure 4
(a)  the submarine ﬂoats to the surface to rescue the divers and reﬁll oxygen and Z1 has higher values.
While in Figure 4 (b)  as the submarine dives into the sea and shoots sharks  expected values of Z2
(orange) are higher than Z1 (blue). This result implies that the reward decomposition indeed captures
different sources of returns  in this case shooting sharks and rescuing divers/reﬁlling oxygen. We
also provide statistics on actions for quantitative analysis to support the argument. In Figure 6(a)  we
E(Z2) in a single
count the occurrence of each action obtained with arg maxa
trajectory  using the same policy as in Figure 4. We see that while Z1 prefers going up  Z2 prefers
going down with ﬁre.

E(Z1) and arg maxa

4.3 Visualization by saliency maps

To better understand the roles of different sub-rewards  we train a DRDRL agent with two channels
(N=2) and compute saliency maps (Simonyan et al. [2013]). Speciﬁcally  to visualize the salient
part of the images as seen by different sub-policies  we compute the absolute value of the Jacobian
|∇xQi(x  arg maxa(cid:48) Q(x  a(cid:48)))| for each channel. Figure 5 shows that visualization results. We ﬁnd
that channel 1 (red region) focuses on reﬁlling oxygen while channel 2 (green region) pays more
attention to shooting sharks as well as the positions where sharks are more likely to appear.

4.4 Direct Control using Induced Sub-policies

E((cid:80)M

E(Zi). To clarify 
We also provide videos2 of running sub-policies deﬁned by πi = arg maxa
the sub-policies are never rolled out during training or evaluation and are only used to compute
J ij
disentang in Eq. 6. We execute these sub-policies and observe its difference with the main policy
i=1 Zi) to get a better visual effect of the reward decomposition. Take Seaquest
π = arg maxa
in Figure 6(b) as an example  two sub-policies show distinctive preference. As Z1 mainly captures the
reward for surviving and rescuing divers  π1 tends to stay close to the surface. However Z2 represents
the return gained from shooting sharks  so π2 appears much more aggressive than π1. Also  without
π1 we see that π2 dies quickly due to out of oxygen.

2https://sites.google.com/view/drdpaper

7

123465134625r1:0.71 r2:0.10r1:0.58 r2:0.40r1:0.60 r2:0.39r1:0.07 r2:0.97r1:0.06 r2:0.98r1:0.26 r2:0.72Figure 4: An illustration of how the sub-returns discriminates at different stage of the game. In ﬁgure
(a)  the submarine is reﬁlling oxygen while in ﬁgure (b) the submarine is shooting sharks.

Figure 5: Sub-distribution saliency maps on the Atari game Seaquest  for a trained DRDRL of two
channels (N=2). One channel learns to pay attention to the oxygen  while another channel learns to
pay attention to the sharks.

5 Related Work

Our method is closely related to previous work of reward decomposition. Reward function decompo-
sition has been studied among others by Russell and Zimdars [2003] and Sprague and Ballard [2003].
While these earlier works mainly focus on how to achieve optimal policy given the decomposed
reward functions  there have been several recent works attempting to learn latent decomposed re-
wards. Van Seijen et al. [2017] construct an easy-to-learn value function by decomposing the reward
function of the environment into n different reward functions. To ensure the learned decomposition is
non-trivial  Van Seijen et al. [2017] proposed to split a state into different pieces following domain
knowledge and then feed different state pieces into each reward function branch. While such method
can accelerate learning process  it always requires many pre-deﬁned preprocessing techniques. There

8

（a）（b）𝑍2𝑍1𝑍1𝑍2Figure 6: (a) Action statistics in an example trajectory of Seaquest. (b) Direct controlling using two
induced sub-policies π1 = arg maxa E(Z1)  π2 = arg maxa E(Z2): the top picture shows that π1
prefers to stay at the top to keep agent alive; the bottom picture shows that π2 prefers aggressive
action of shooting sharks.

has been other work that explores learn reward decomposition network end-to-end. Grimm and
Singh [2019] investigates how to learn independently-obtainable reward functions. While it learns
interesting reward decomposition  their method requires that the environments be resettable to speciﬁc
states since it needs multiple trajectories from the same starting state to compute their objective
function. Besides  their method aims at learning different optimal policies for each decomposed
reward function. Different from the works above  our method can learn meaningful implicit reward
decomposition without any requirements on prior knowledge. Also  our method can leverage the
decomposed sub-rewards to ﬁnd better behaviour for a single agent.
Our work also relates to Horde (Sutton et al. [2011]). The Horde architecture consists of a large
number of ‘sub-agents’ that learn in parallel via off-policy learning. Each demon trains a separate
general value function (GVF) based on its own policy and pseudo-reward function. A pseudo-reward
can be any feature-based signal that encodes useful information. The Horde architecture is focused on
building up general knowledge about the world  encoded via a large number of GVFs. UVFA (Schaul
et al. [2015]) extends Horde along a different direction that enables value function generalizing across
different goals. Our method focuses on learning implicit reward decomposition in order to more
efﬁciently learn a control policy.

6 Conclusion

In this paper  we propose Distributional Reward Decomposition for Reinforcement Learning (DR-
DRL)  a novel reward decomposition algorithm which captures the multiple reward channel structure
under distributional setting. Our algorithm signiﬁcantly outperforms state-of-the-art RL methods
RAINBOW on Atari games with multiple reward channels. We also provide interesting experimental
analysis to get insight for our algorithm. In the future  we might try to develop reward decomposition
method based on quantile networks (Dabney et al. [2018a b]).

Acknowledgments

This work was supported in part by the National Key Research & Development Plan of China (grant
No. 2016YFA0602200 and 2017YFA0604500)  and by Center for High Performance Computing and
System Simulation  Pilot National Laboratory for Marine Science and Technology (Qingdao).

9

(a)(b)References
Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research  47:
253–279  2013.

Marc G Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 449–458. JMLR. org  2017.

Pablo Samuel Castro  Subhodeep Moitra  Carles Gelada  Saurabh Kumar  and Marc G. Bellemare.
Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http:
//arxiv.org/abs/1812.06110.

Will Dabney  Georg Ostrovski  David Silver  and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In International Conference on Machine Learning  pages
1104–1113  2018a.

Will Dabney  Mark Rowland  Marc G Bellemare  and Rémi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
2018b.

Christopher Grimm and Satinder Singh. Learning independently-obtainable reward functions. arXiv

preprint arXiv:1901.08649  2019.

Matteo Hessel  Joseph Modayil  Hado Van Hasselt  Tom Schaul  Georg Ostrovski  Will Dabney  Dan
Horgan  Bilal Piot  Mohammad Azar  and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

Adrien Laversanne-Finot  Alexandre Péré  and Pierre-Yves Oudeyer. Curiosity driven exploration of

learned disentangled goal spaces. arXiv preprint arXiv:1807.01521  2018.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G Bellemare 
Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al. Human-level control
through deep reinforcement learning. Nature  518(7540):529  2015.

Stuart J Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages 656–
663  2003.

Tom Schaul  Daniel Horgan  Karol Gregor  and David Silver. Universal value function approximators.

In International Conference on Machine Learning  pages 1312–1320  2015.

Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034  2013.

Nathan Sprague and Dana Ballard. Multiple-goal reinforcement learning with modular sarsa (0).

2003.

Richard S Sutton  Joseph Modayil  Michael Delp  Thomas Degris  Patrick M Pilarski  Adam White 
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and
Multiagent Systems-Volume 2  pages 761–768. International Foundation for Autonomous Agents
and Multiagent Systems  2011.

Valentin Thomas  Jules Pondard  Emmanuel Bengio  Marc Sarfati  Philippe Beaudoin  Marie-Jean
Meurs  Joelle Pineau  Doina Precup  and Yoshua Bengio. Independently controllable features.
arXiv preprint arXiv:1708.01289  2017.

Hado Van Hasselt  Arthur Guez  and David Silver. Deep reinforcement learning with double q-

learning. In Thirtieth AAAI Conference on Artiﬁcial Intelligence  2016.

Harm Van Seijen  Mehdi Fatemi  Joshua Romoff  Romain Laroche  Tavian Barnes  and Jeffrey
Tsang. Hybrid reward architecture for reinforcement learning. In Advances in Neural Information
Processing Systems  pages 5392–5402  2017.

10

,Zichuan Lin
Li Zhao
Derek Yang
Tao Qin
Tie-Yan Liu
Guangwen Yang