2019,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds,Personalized interventions in social services  education  and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies  we show that actually auditing their disparate impacts per standard observational metrics  such as true positive rates  is impossible since ground truths are unknown. Whether our data is experimental or observational  an individual's actual outcome under an intervention different than that received can never be known  only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response  which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption via sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.,Assessing Disparate Impact of Personalized
Interventions: Identiﬁability and Bounds

Nathan Kallus
Cornell University

New York  NY

kallus@cornell.edu

Angela Zhou

Cornell University

New York  NY

az434@cornell.edu

Abstract

Personalized interventions in social services  education  and healthcare leverage
individual-level causal effect predictions in order to give the best treatment to each
individual or to prioritize program interventions for the individuals most likely to
beneﬁt. While the sensitivity of these domains compels us to evaluate the fairness
of such policies  we show that actually auditing their disparate impacts per standard
observational metrics  such as true positive rates  is impossible since ground truths
are unknown. Whether our data is experimental or observational  an individual’s
actual outcome under an intervention different than that received can never be
known  only predicted based on features. We prove how we can nonetheless point-
identify these quantities under the additional assumption of monotone treatment
response  which may be reasonable in many applications. We further provide a
sensitivity analysis for this assumption by means of sharp partial-identiﬁcation
bounds under violations of monotonicity of varying strengths. We show how to use
our results to audit personalized interventions using partially-identiﬁed ROC and
xROC curves and demonstrate this in a case study of a French job training dataset.

Introduction

1
The expanding use of predictive algorithms in the public sector for risk assessment has sparked recent
concern and study of fairness considerations [3  9  10]. One critique of the use of predictive risk
assessment argues that the discussion should be reframed to instead focus on the role of positive
interventions in distributing beneﬁcial resources  such as directing pre-trial services to prevent
recidivism  rather than in meting out pre-trial detention based on a risk prediction [8]; or using risk
assessment in child welfare services to provide families with additional childcare resources rather
than to inform the allocation of harmful suspicion [29  62]. However  due to limited resources 
interventions are necessarily targeted. Recent research speciﬁcally investigates the use of models that
predict an intervention’s beneﬁt in order to efﬁciently target their allocation  such as in developing
triage tools to target homeless youth [46  57]. Both ethics and law compel such personalized
interventions to be fair and to avoid disparities in how they impact different groups deﬁned by certain
protected attributes  such as race  age  or gender.
The delivery of interventions to better target those individuals deemed most likely to respond well 
even if a prediction or policy allocation rule does not have access to the protected attribute  might still
result in disparate impact (with regards to social welfare) for the same reasons that these disparities
occur in machine learning classiﬁcation models [21]. (See Appendix C for an expanded discussion
on our use of the term “disparate impact.”) However  in the problem of personalized interventions 
the “fundamental problem of causal inference ” that outcomes are not observed for interventions not
administered  poses a fundamental challenge for evaluating the fairness of any intervention allocation
rule  as the true “labels” of intervention efﬁcacy of any individual are never observed in the dataset.
Metrics commonly assessed in the study of fairness in machine learning  such as group true positive

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

and false positive rates  are therefore conditional on potential outcomes which are not observed in the
data and therefore cannot be computed as in standard classiﬁcation problems.
The problem of personalized policy learning has surfaced in econometrics and computer science
[13  36  37  37  45  51]  gaining renewed attention alongside recent advances in causal inference and
machine learning [4  14  28  63]. In particular  [17] analyze optimal treatment allocations for malaria
bednets with nonparametric plug-in estimates of conditional average treatment effects  accounting for
budget restrictions; [27] use the generalized random forests method of [64] to evaluate heterogeneity
of causal effects in a program matching at-risk youth in Chicago with summer jobs on outcomes
and crime; and [46] use BART [32] to analyze heterogeneity of treatment effect for allocation
of homeless youth to different interventions  remarking that studying fairness considerations for
algorithmically-guided interventions is necessary.
In this paper  we address the challenges of assessing the disparate impact of such personalized
intervention rules in the face of unknown ground truth labels. We show that we can actually obtain
point identiﬁcation of common observational fairness metrics under the assumption of monotone
treatment response. We motivate this assumption and discuss why it might be natural in settings
where interventions only either help or do nothing. Recognizing nonetheless that this assumption is
not actually testable  we show how to conduct sensitivity analyses for fairness metrics. In particular 
we show how to obtain sharp partial identiﬁcation bounds on the metrics of interest as we vary the
strength of violation of the assumption. We then show to use these tools to visualize disparities using
partially identiﬁed ROC and xROC curves. We illustrate all of this in a case study of personalized job
training based on a dataset from a French ﬁeld experiment.

2 Problem Setup

We suppose we have data on individuals (X  A  T  Y ) consisting of:

• Prognostic features X 2X   upon which interventions are personalized;
• Sensitive attribute A 2A   against which disparate impact will be measured;
• Binary treatment indicator T 2{ 0  1}  indicating intervention exposure; and
• Binary response outcome Y 2{ 0  1}  indicating the beneﬁt to the individual.

Our convention is to identify T = 1 with an active intervention  such as job training or a homeless pre-
vention program  and T = 0 with lack thereof. Similarly  we assume that a positive outcome  Y = 1 
is associated with a beneﬁcial event for the individual  e.g.  successful employment or non-recidivation.
Using the Neyman-Rubin potential outcome framework [34]  we let Y (0)  Y (1) 2{ 0  1} denote the
potential outcomes of each treatment. We let the observed outcome be the potential outcome of the
assigned treatment  Y = Y (T )  encapsulating non-interference and consistency assumptions  also
known as SUTVA [60]. Importantly  for any one individual  we never simultaneously observe Y (0)
and Y (1). This is sometimes termed the fundamental problem of causal inference. We assume our
data either came from a randomized controlled trial (the most common case) or an unconfounded
observational study so that the treatment assignment is ignorable  that is  Y (1)  Y (0) ?? T | X  A.
When both treatment and potential outcomes are binary  we can exhaustively enumerate the
four possible realizations of potential outcomes as (Y (0)  Y (1)) 2{ 0  1}2. We call units with
(Y (0)  Y (1)) = (0  1) responders  (Y (0)  Y (1)) = (1  0) anti-responders  and Y (0) = Y (1) non-
responders. Such a decomposition is also common in instrumental variable analysis [2] where the
binary outcome is take-up of treatment with the analogous nomenclature of compliers  never-takers 
always-takers  and deﬁers. In the context of talking about an actual outcome  following [52]  we
replace this nomenclature with the notion of response rather than compliance. We remind the reader
that due to the fundamental problem of causal inference  response type is unobserved.
We denote the conditional probabilities of each response type by

pij = pij(X  A) = P(Y (0) = i  Y (1) = j | X  A).

By exhaustiveness of these types  p00 + p01 + p10 + p11 = 1. (Note pij are random variables.)
We consider evaluating the fairness of a personalized intervention policy Z = Z(X  A) 2{ 0  1} 
which assigns interventions based on observable features X  A (potentially just X). Note that by
deﬁnition  the intervention has zero effect on non-responders  negative effect on anti-responders 
and a positive effect only on responders. Therefore  in seeking to beneﬁt individuals with limited

2

resources  the personalized intervention policy should seek to target only the responders. Naturally 
response type is unobserved and the policy can only mete out interventions based on observables.
In classiﬁcation settings  minimum-error classiﬁers on the efﬁcient frontier of type-I and -II errors
are given by Bayes classiﬁers that threshold the probability of a positive label. In personalized
interventions  policies that are on the efﬁcient frontier of social welfare (fraction of positive outcomes 
P (Y (Z) = 1)) and program cost (fraction intervened on  P (Z = 1)) are given by thresholding
(Z = I [⌧  ✓]) the conditional average treatment effect (CATE):

⌧ = ⌧ (X  A) = E[Y (1)  Y (0) | X  A] = p01  p10

= P(Y = 1 | T = 1  X  A)  P(Y = 1 | T = 0  X  A) 

where the latter equality follows by the assumed ignorable treatment assignment. Estimating ⌧ from
unconfounded data using ﬂexible models has been the subject of much recent work [32  61  64].
We consider observational fairness metrics in analogy to the classiﬁcation setting  where the “true
label” of an individual is their responder status  R = I [Y (1) > Y (0)]. We deﬁne the analogous true
positive rate and true negative rate for the intervention assignment Z  conditional on the (unobserved)
events of an individual being a responder or non-responder  respectively:
TPRa = P(Z = 1 | A = a  Y (1) > Y (0))  TNRa = P(Z = 0 | A = a  Y (1)  Y (0)).
2.1

Interpreting Disparities for Personalized Interventions

(1)

The use of predictive models to deliver interventions can induce disparate impact if responding
(respectively  non-responding) individuals of different groups receive the intervention at dispropor-
tionate rates under the treatment policy. This can occur even with efﬁcient policies that threshold the
true CATE ⌧ and can arise from the disparate predictiveness of X  A of response type (i.e.  how far
pij are from 0 and 1). This is problematic because the choice of features X is usually made by the
intervening agent (e.g.  government agency  etc.).
We discuss one possible interpretation of TPR or TNR disparities in this setting when the intervention
is the bestowal of a beneﬁt  like access to job training or case management. From the point of view of
the intervening agent  there are speciﬁc program goals  such as employment of the target individual
within 6 months. Therefore  false positives are costly due to program cost and false negatives are
missed opportunities. But outcomes also affect the individual’s utility. Discrepancies in TPR across
values of A are of concern since they suggest that the needs of those who could actually beneﬁt
from intervention (responders) in one group are not being met at the same rates as in other groups.
Arguably  for beneﬁt-bestowing interventions  TPR discrepancies are of greater concern. Nonetheless 
from the point of view of the individual  the intervention may always grant some positive resource
(e.g.  from the point of view of well-being)  regardless of responder status  since it corresponds to
access to a good (and the individual can gain other beneﬁts from job training that may not necessarily
align with the intervener’s program goals  such as employment in 1 year or personal enrichment). If
so  then TNR discrepancies across values of A imply a “disparate beneﬁt of the doubt” such that the
policy disparately over-beneﬁts one group over another using the limited public resource without the
cover of advancing the public program’s goal  which may raise fairness and envy concerns  especially
since this “waste” is at the cost of more slots for responders.
Beyond assessing disparities in TPR and TNR for one ﬁxed policy  we will also use our ability to
assess these over varying CATE thresholds in order to compute xAUC metrics [41] in Section 6.
These give the disparity between the probabilities that a non-responder from group a is ranked above
a responder from group b and vice-versa. Thus  they measure the disproportionate access one group
gets relative to another in any allocation of resources that is non-decreasing in CATE.
We emphasize that the identiﬁcation arguments and bounds that we present on fairness metrics are
primarily intended to facilitate the assessment of disparities  which may require further inquiry as
to their morality and legality  not necessarily to promote statistical parity via adjustments such as
group-speciﬁc thresholds  though that is also possible using our tools. We defer a more detailed
discussion to Section 8 and re-emphasize that assessing the distribution of outcome-conditional model
errors are of central importance both in machine learning [10  30  55] and in the economic efﬁciency
of targeting resources [16  18  54].

3

3 Related Work
[50] consider estimating joint treatment effects of race and treatment under a deep latent variable
model to reconstruct unobserved confounding. For evaluating fairness of policies derived from
estimated effects  they consider the gap in population accuracy Acca = P (Z = Z⇤ | A = a)  where
Z⇤ = I[⌧ (X) > 0] is the (identiﬁable) optimal policy. In contrast  we highlight the unfairness
of even optimal policies and focus on outcome-conditional error rates (TPR  TNR)  where the
non-identiﬁability of responder status introduces challenges regarding identiﬁability.
The issue of model evaluation under the censoring problem of selective labels has been discussed in
situations such as pretrial detention  where detention censors outcomes [40  48]. Sensitivity analysis
to account for possible unmeasured confounders is used in [35  39]. The distinction is that we focus
on the targeted delivery of interventions with unknown (but estimated) causal effects  rather than
considering classiﬁcations that induce one-sided censoring but have deﬁnitionally known effects.
Recently  partial identiﬁcation approaches has also been proposed in the case of known outcomes but
missing protected attributes [22  42].
Our emphasis is distinct from other work discussing fairness and causality that uses graphical causal
models to decompose predictive models along causal pathways and assessing the normative validity
of path-speciﬁc effects [44  47]  such as the effect of probabilistic hypothetical interventions on race
variables or other potentially immutable protected attributes. When discussing treatments  we here
consider interventions corresponding to allocation of concrete resources (e.g.  give job training) 
which are in fact physically manipulable by an intervening agent. The correlation of the intervention’s
conditional average treatment effects by  say  race and its implications for downstream resource
allocation are our primary concern.
There is extensive literature on partial identiﬁcation  e.g. [53]  including for individual-level causal
effect [43]. In contrast to previous work that analyzes partial identiﬁcation of average treatment
effects when data is confounded and using monotonicity to improve precision [6  15  53]  we focus
on unconfounded (e.g.  RCT) data and achieve full identiﬁcation by assuming monotonicity and
consider sensitivity analysis bounds for nonlinear functionals of partially identiﬁed sets  namely  true
positive and false positive rates.
4
Since the deﬁnitions of the disparate impact metrics in Eq. (1) are conditioned on an unobserved
event  such as the response event Y (1) > Y (0)  they actually cannot be identiﬁed from the data 
even under ignorable treatment. That is  the values of TPRa  TNRa can vary even when the joint
distribution of (X  A  T  Y ) remains the same  meaning the data we see cannot possibly tell us about
the speciﬁc value of TPRa  TNRa.
Proposition 1. TPRa  TNRa (or discrepancies therein over groups) are generally not identiﬁable.
Essentially  Proposition 1 follows because the data only identiﬁes the marginals p10 + p11  p01 + p11
while TPRa  TNRa depend on the joint via p01  which can vary even while marginals are ﬁxed.
Since this can vary independently across values of A  discrepancies are not identiﬁable either.
4.1 Identiﬁcation under monotonicity
We next show identiﬁability if we impose the additional assumption of monotone treatment response.
Assumption 1 (Monotone treatment response). Y (1)  Y (0). (Equivalently  p10 = 0.)
Assumption 1 says that anti-responders do not exist. In other words  the treatment either does nothing
(e.g.  an individual would have gotten a job or not gotten a job  regardless of receiving job training)
or it beneﬁts the individual (would get a job if and only if receive job training)  but it never harms
the individual. This assumption is reasonable for positive interventions. As [38] points out  policy
learning in this setting is equivalent to the binary classiﬁcation problem of predicting responder status.
Proposition 2. Under Assumption 1 

Identiﬁability of Disparate Impact Metrics

TPRa = E [⌧ | A = a  Z = 1] P (Z = 1 | A = a)
TNRa = E [(1  ⌧ ) | A = a  Z = 0] P (Z = 0 | A = a)

E [⌧ | A = a]
E [(1  ⌧ ) | A = a]

 

(2)

.

4

Since the quantities on the right hand sides in Eq. (2) are in terms of identiﬁed quantities (functions
of the distribution of (X  A  T  Y ))  this proves identiﬁability. Given a sample and an estimate of
⌧  it also provides a simple recipe for estimation by replacing each average or probability by a
sample version  since both A and Z are discrete. More generally  since these averages are average
treatment effects (over subpopulations deﬁned by A  Z values)  these quantities can also alternatively
be estimated by any average treatment effect estimator and plugged in. For example  we can use
doubly robust estimators to ensure speciﬁcation-robustness [58] or double ML estimators to ensure
efﬁciency when X may be high-dimensional [23].
Thus  Proposition 2 provides a novel means of assessing disparate impact of personalized interventions
under monotone response. This is relevant because monotonicity is a defensible assumption in the
case of many interventions that bestow an additional beneﬁt  good  or resource  such as the ones
mentioned in Section 1. Nonetheless  the validity of Assumption 1 is itself not identiﬁable. Therefore 
should it fail even slightly  it is not immediately clear whether these disparity estimates can be relied
upon. We therefore next study a sensitivity analysis by means of constructing partial identiﬁcation
bounds for TPRa  TNRa.
5 Partial Identiﬁcation Bounds for Sensitivity Analysis
We next study the partial identiﬁcation of disparate impact metrics when Assumption 1 fails  i.e. 
p10 6= 0. We ﬁrst state a more general version of Proposition 2. For any ⌘ = ⌘(X  A)  let

⇢TPR
a

⇢TNR
a

(⌘) := E [⌧ + ⌘ | A = a  Z = 1] P (Z = 1 | A = a)
(⌘) := E [1  (⌧ + ⌘) | A = a  Z = 0] P (Z = 0 | A = a)

E [⌧ + ⌘ | A = a]
E [1  (⌧ + ⌘) | A = a]

 

.

a

b

(p10).

a

a

(⌘) ⇢ TNR

a

a

(⌘) ⇢ TNR

a

(p10)  TNRa = ⇢TNR

As long as 8⌘ 2U

(⌘)a2A
: ⌘ 2U}✓ R2⇥|A|.
(⌘) and ⇢(⌘) = (⇢a(⌘))a2A.

Proposition 3. TPRa = ⇢TPR
Since the anti-responder probability p10 is unknown  we cannot use Proposition 3 to identify
TPRa  TNRa. We instead use Proposition 3 to compute bounds on them by restricting p10 to
be in an uncertainty set. Formally  given an uncertainty set U for p10 (i.e.  a set of functions of x  a) 
we deﬁne the simultaneous identiﬁcation region of the TPR and TNR for all groups a 2A as:
⇥= {⇢TPR
For brevity  we will let ⇢a(⌘) =⇢TPR
true pos-
The set ⇥ describes all possible simultaneous values of the group-conditional
we have 0  ⌘(X  A) 
itive and true negative rates.
min (P (Y = 1 | T = 0  X  A)   P (Y = 0 | T = 1  X  A)) (which is identiﬁed from the data) by
Proposition 3 this set is necessarily sharp [53] given only the restriction that p10 2U . (In particular 
this bound on ⌘ can be achieved by just point-wise clipping U with this identiﬁable bound as nec-
essary.) That is  given a joint on (X  A  T  Y )  on the one hand  every ⇢ 2 ⇥ is realized by some
full joint distribution on (X  A  T  Y (0)  Y (1)) with p10 2U   and on the other hand  every such
joint gives rise to a ⇢ 2 ⇥. In other words  ⇥ is an exact characterization of the in-fact possible
simultaneous values of the group-conditional TPRs and TNRs.
Therefore  if  for example  we are interested in the minimal and maximal possible values for the true
(unknown) TPR discrepancy between groups a and b  we should seek to compute inf ⇢2⇥ ⇢TPR
a 
. More generally  for any µ 2 R2⇥|A|  we may wish to compute
⇢TPR
b
(3)
Note that this  for example  covers the above example since for any µ we can also take µ. The
function h⇥ is known as the support function of ⇥ [59]. Not only does the support function provide
the maximal and minimal contrasts in a set  it also exactly characterizes its convex hull. That is 
Conv (⇥) =⇢ : µ>⇢  h⇥(µ) 8µ . So computing h⇥ allows us to compute Conv (⇥).
Our next result gives an explicit program to compute the support function when U has a product form
of within-group uncertainty sets:
(4)
which leads to ⇥= Qa2A ⇥a where ⇥a = {⇢a(⌘a) : ⌘a 2U a}.

U = {⌘ : ⌘( ·   a) 2U a 8a 2A}  

and sup⇢2⇥ ⇢TPR

a  ⇢TPR

h⇥(µ) := sup⇢2⇥ µ>⇢.

5

Proposition 4. Let rz
(4). Then Eq. (3) can be reformulated as:

a := P (Z = z | A = a) and ⌧ z

a := E [⌧ | A = a  Z = z]. Suppose U is as in

h⇥(µ) = Pa2A h⇥a(µa)  where
a ta⌧ 1

h⇥a(µa) =sup!a ta µTPR

a

a

+

r1
µTNR
r0
a
ta  1

a + E [!a(X) | A = a  Z = 1]
(ta 1  ⌧ 0
s.t.! a(·) 2 ta Ua  tar0

a + E [!a(X) | A = a  Z = 0])
a + E [!a | A = a] = 1.

a + r1

a⌧ 1

a⌧ 0

a

For a ﬁxed value of ta  the above program is a linear program  given that Ua is linearly representable.
Therefore a solution may be found by grid search on the univariate ta. Moreover  if µTPR
= 0 or
= 0  the above remains a linear program even with ta as a variable [20]. With this  we are able
µTNR
a
to express group-level disparities through assessing the support function at speciﬁc contrast vectors µ.
5.1 Partial Identiﬁcation under Relaxed Monotone Treatment Response
We next consider the implications of the above for the following relaxation of the monotone treatment
response assumption:
Assumption 2 (B-relaxed monotone treatment response). p10  B.
Note that Assumption 2 with B = 0 recovers Assumption 1 and Assumption 2 with
B = 1 is a vacuous assumption.
In between these two extremes we can consider milder
or stronger violations of monotone response and the partial identiﬁcation bounds they corre-
sponds to. This provides us with a means of sensitivity analysis of the disparities we mea-
sure  recognizing that monotone response may not hold exactly and that disparities may not
be exactly identiﬁable. For the rest of the paper  we focus solely on partial identiﬁcation
under Assumption 2. Note that Assumption 2 corresponds exactly to the uncertainty set
UB = {⌘ : 0  ⌘(X  A)  min (B  P (Y = 1 | T = 0  X  A)   P (Y = 0 | T = 1  X  A))}. We de-
ﬁne ⇥B =Qa2A ⇥B a to be the corresponding identiﬁcation region.
Under Assumption 2  our bounds take on a particularly simple form.
Let Bz
E [min (B  P (Y = 1 | T = 0  X  A)   P (Y = 0 | T = 1  X  A)) | A = a  Z = z] and deﬁne
a(B))r1
a

a(B) =

a(B))r1
a

⇢TNR
a

⇢TPR
a

(B) =

(B) =

⌧ 0
a r0

 

 

(⌧ 1
a + B1
a(B))r1
a
a + B1
a + (⌧ 1
⌧ 1
a r1
a

(⌧ 0
a + B0

a(B))r0

a + ⌧ 1

a r1
a

⇢TPR
a

(B) =

 ⇢

TNR
a

(B) =

(1  ⌧ 0

(1  ⌧ 0
(B)  ⇢TPR

a )r0
(1  ⌧ 0
a
a B 1
a + (1  ⌧ 1
a(B))r0
a B 0
a
a(B))r0
a + (1  ⌧ 1

a )r0
(1  ⌧ 0
a B 0
(B)] and [⇢TNR
respectively.

a

a )r1
a
(B)  ⇢TNR

.

a

a

(B) ⇢ TNR

Proposition 5. Suppose Assumption 2 holds. Then [⇢TPR
are the sharp identiﬁcation intervals for TPRa and TNRa 
(⇢TPR
multaneously achievable.
6 Partial Identiﬁcation of Group Disparities and ROC and xROC Curves
We discuss diagnostics to summarize possible impact disparities across a range of possible policies.

(B)]
a
Moreover 
(B)) 2 ⇥B a  i.e.  the two extremes are si-

(B)) 2 ⇥B a and (⇢TPR

(B)  ⇢TNR

a

a

a

a

TPR and TNR disparity. Discrepancies in model errors (TPR or TNR) are of interest when audit-
ing classiﬁcation performance on different groups with a given  ﬁxed policy Z. Under Assumption 1 
they are identiﬁed by Proposition 2. Under violations of Assumption 1  we can consider their partial
identiﬁcation bounds. If the minimal disparity remains nonzero  that provides strong evidence of dis-
parity. Similarly  if the maximal disparity is large  a responsible decision maker should be concerned
about the possibility of a disparity.
Under Assumption 2  Proposition 5 provides that the sharp identiﬁcation intervals of TPRa  TPRb
and TNRa  TNRb are  respectively  given by

a

[⇢TPR
[⇢TNR

a

b

(B)  ⇢TPR
(B)  ⇢TNR

b

(B)  ⇢TPR
(B)  ⇢TNR

a

a

b

(B)  ⇢TPR
(B)  ⇢TNR

b

(B)] 

(B)].

(5)

Given effect scores ⌧  we can then use this to plot disparity curves by plotting the endpoints of Eq. (5)
for policies Z = I[⌧  ✓] for varying thresholds ✓.

6

Robust ROC Curves We ﬁrst deﬁne the analogous group-conditional ROC curve corresponding
to a CATE function ⌧. These are the parametric curves traced out by the pairs (1  TNRa  TPRa)
of policies that threshold the CATE for varying thresholds. To make explicit that we are now
computing metrics for different policies  we use the notation ⇢(⌘; ⌧  ✓) to refer to the metrics of
the policy Z = I [⌧  ✓]. Under Assumption 1  Proposition 2 provides point identiﬁcation of the
group-conditional ROC curve:

ROCa(⌧ ) := {(1  ⇢TNR

a

(0; ⌧  ✓) ⇢ TPR

a

(0; ⌧  ✓)) : ✓ 2 R}

When Assumption 1 fails  we cannot point identify TPRa  TNRa and correspondingly we cannot
identify ROCa(⌧ ). We instead deﬁne the robust ROC curve as the union of all partially identiﬁed
ROC curves. Speciﬁcally:

⇥ROC

a

(⌧ ) := {(1  ⇢TNR

a

(⌘a; ⌧  ✓) ⇢ TPR

a

(⌘a; ⌧  ✓)) : ✓ 2 R ⌘ a 2U a}.

Plotted  this set provides a visual representation of the region that the true ROC curve can lie in. We
next prove that under Assumption 2  we can easily compute this set as the area between two curves.
Proposition 6. Let U = UB. Then ⇥ROC
(⌧ ) is given as the area between the two parametric
curves ROCa(⌧ ) := {(1  ⇢TNR
(B; ⌧  ✓)) : ✓ 2 R} and ROCa(⌧ ) := {(1 
(B; ⌧  ✓)) : ✓ 2 R}.
⇢TNR
a
This follows because the extremes are simultaneously achievable as noted in Proposition 5. We
highlight  however  that the lower (resp.  upper) ROC curve may not be simultaneously realizable.

(B; ⌧  ✓) ⇢ TPR

(B; ⌧  ✓)  ⇢TPR

a

a

a

a

Robust xROC Curves Comparison of group-conditional ROC curves may not necessarily show
impact disparities as  even in standard classiﬁcation settings ROC curves can overlap despite disparate
impacts [30  41]. At the same time  comparing disparities for ﬁxed policies Z with ﬁxed thresholds
may not accurately capture the impact of using ⌧ for rankings. [41] develop the xAUC metric for
assessing the bipartite ranking quality of risk scores  as well as the analogous notion of a xROC
curve which parametrically plots the TPR of one group vs. the FPR of another group  at any ﬁxed
threshold. This is relevant if effect scores ⌧ are used for downstream decisions by different facilities
with different budget constraints or if the score is intended to be used by a “human-in-the-loop”
exercising additional judgment  e.g.  individual caseworkers as in the encouragement design of [12].
Under Assumption 1  we can point identify TPRa  TNRa  so  following [41]  we can deﬁne the
point-identiﬁed xROC curve as

xROCa b(⌧ ) = {(1  ⇢TNR

b

(0; ⌧  ✓) ⇢ TPR

a

(0; ⌧  ✓)) : ✓ 2 R}.

Without Assumption 1  we analogously deﬁne the robust xROC curve as the union of all partially
identiﬁed xROC curves:

⇥xROC

a b

(⌧ ) = {(1  ⇢TNR

b

(⌘a; ⌧  ✓) ⇢ TPR

a

(⌘a; ⌧  ✓)) : ✓ 2 R ⌘ a 2U a}.

b

b

a

a

a b

(B; ⌧  ✓)  ⇢TPR

(B; ⌧  ✓) ⇢ TPR

(B; ⌧  ✓)) : ✓ 2 R}.

(⌧ ) is given as the area between the two parametric
(B; ⌧  ✓)) : ✓ 2 R} and xROCa b(⌧ ) :=

Proposition 7. Let U = UB. Then ⇥xROC
curves xROCa b(⌧ ) := {(1  ⇢TNR
{(1  ⇢TNR
This follows because UB takes the form of a product set over a 2A .
7 Case Study: Personalized Job Training (Behaghel et al.)
We consider a case study from a three-armed large randomized controlled trial that randomly assigned
job-seekers in France to a control-group  a job training program managed by a public vendor  and an
out-sourced program managed by a private vendor [11]. While the original experiment was interested
in the design of contracts for program service delivery  we consider a task of heterogeneous causal
effect estimation  motivated by interest in personalizing different types of counseling or active labor
market programs that would be beneﬁcial for the individual. Recent work in policy learning has also
considered personalized job training assignment [45  63] and suggested excluding sensitive attributes
from the input to the decision rule for fairness considerations  but without consideration of fairness
in the causal effect estimation itself and how signiﬁcant impact disparities may still remain after
excising sensitive attributes because of it.

7

TPR Disparity

TPR

Disparity in disfavor 

of French

TNR

Disparity in disfavor 

of non-French

TPR

Disparity in 
disfavor of >26
<26

TNR

Disparity in 
disfavor of <26
>26

Disparity in disfavor 

of non-French

Disparity in 
disfavor of <26
>26

Figure 1: TPR and TNR disparity curves and bounds on French job training dataset (Eq. (5))

Figure 2: ROC and xROC for A = nationality  age on French job training dataset

We focus on the public program vs. control arm  which enrolled about 7950 participants in total 
with n1 = 3385 participants in the public program. The treatment arm  T = 1  corresponds to
assignment to the public program. The original analysis suggests a small but statistically signiﬁcant
positive treatment effect of the public program  with an ATE of 0.023. We omit further details on the
data processing to Appendix B. We consider the group indicators: nationality (0  1 denoting French
nationals vs. non-French  respectively)  gender (denoting woman vs. non-woman)  and age (below
the age of 26 vs. above). (Figures for gender appear in Appendix B.)
In Fig. 1  we plot the identiﬁed “disparity curves” of Eq. (5) corresponding to the maximal and
minimal sensitivity bounds on TPR and TNR disparity between groups. Levels of shading correspond
to different values of B  with color legend at right. We learn ⌧ by the Generalized Random Forests
method of [5  64] and use sample splitting  learning ⌧ on half the data and using our methods to assess
bounds on ⇢TPR ⇢ TNR and other quantities with out-of-sample estimates on the other half of the
data. We bootstrap over 50 sampled splits and average disparity curves to reduce sample uncertainty.
In general  the small probability of being a responder leads to increased sensitivity of TPR estimates
(wide identiﬁcation bands). The curves and sensitivity bounds suggest that with respect to nationality
and gender  there is small or no disparity in true positive rates but the true negative rates for nationality 
gender  and age may differ signiﬁcantly across groups  such that non-women would have a higher
chance of being bestowed job-training beneﬁts when they are in fact not responders. However  TPR
disparity by age appears to hold with as much as -0.1 difference  with older actually-responding
individuals being less likely to be given job training than younger individuals. Overall  this suggests
that differences in heterogeneous treatment effects across age categories could lead to signiﬁcant
adverse impact on older individuals.
This is similarly reﬂected in the robust ROC  xROC curves (Fig. 2). Despite possibly small differences
in ROCs  the xROCs indicate strong disparities: the sensitivity analysis suggests that the likelihood of
ranking a non-responding young individual above a responding old individual (xAUC [41]) is clearly
larger than the symmetric error  meaning that older individuals who beneﬁt from the treatment may
be disproportionately shut out of it as seats are instead given to non-responding younger individuals.
8 Discussion and Conclusion
We presented identiﬁcation results and bounds for assessing disparate model errors of causal-effect
maximizing treatment policies  which can lead disparities in access to those who stand to beneﬁt
from treatment across groups. Whether this is “unfair” would naturally rely on one’s normative

8

assumptions. One such is “claims across outcomes ” that individuals have a claim to the public
intervention if they stand to beneﬁt  which can be understood within [1]’s axiomatic justiﬁcation of
fair distribution. There may also be other justice-based considerations  e.g. minimax fairness. We
discuss this more extensively in Appendix C.
With the new ability to assess disparities using our results  a second natural question is whether these
disparities warrant adjustment  which is easy to do given our tools combined with the approach of
[30]. This question again is dependent both on one’s viewpoint and ultimately on the problem context 
and we discuss it further in Appendix C. Regardless of normative viewpoints  auditing allocative
disparities that would arise from the implementation of a personalized rule must be a crucial step of a
responsible and convincing program evaluation. We presented fundamental identiﬁcation limits to
such assessments but provided sensitivity analyses that can support reliable auditing.

Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No.
1846210. This research was funded in part by JPMorgan Chase & Co. Any views or opinions
expressed herein are solely those of the authors listed  and may differ from the views and opinions
expressed by JPMorgan Chase & Co. or its afﬁliates. This material is not a product of the Research
Department of J.P. Morgan Securities LLC. This material should not be construed as an individual
recommendation for any particular client and is not intended as a recommendation of particular
securities  ﬁnancial instruments or strategies for a particular client. This material does not constitute
a solicitation or offer in any jurisdiction.

References
[1] M. Adler. Well-Being and Fair Distribution.
[2] J. D. Angrist  G. W. Imbens  and D. B. Rubin. Identiﬁcation of causal effects using instrumental

variables. Journal of the American statistical Association  1996.

[3] J. Angwin  J. Larson  S. Mattu  and L. Kirchner. Machine bias. Online.  May 2016.
[4] S. Athey. Beyond prediction: Using big data for policy problems. Science  2017.
[5] S. Athey  J. Tibshirani  S. Wager  et al. Generalized random forests. The Annals of Statistics  47

(2):1148–1178  2019.

[6] A. Balke and J. Pearl. Bounds on treatment effects from studies with imperfect compliance.

Journal of the American Statistical Association  92(439):1171–1176  1997.

[7] A. Banerjee  E. Duﬂo  N. Goldberg  D. Karlan  R. Osei  W. Parienté  J. Shapiro  B. Thuysbaert 
and C. Udry. A multifaceted program causes lasting progress for the very poor: Evidence from
six countries. Science  348(6236):1260799  2015.

[8] C. Barabas  K. Dinakar  J. Ito  M. Virza  and J. Zittrain.

Interventions over predictions:
Reframing the ethical debate for actuarial risk assessment. Proceedings of Machine Learning
Research  2017.

[9] S. Barocas and A. Selbst. Big data’s disparate impact. California Law Review  2014.
[10] S. Barocas  M. Hardt  and A. Narayanan. Fairness and Machine Learning. fairmlbook.org 

2018. http://www.fairmlbook.org.

[11] L. Behaghel  B. Crépon  and M. Gurgand. Private and public provision of counseling to job
seekers: Evidence from a large controlled experiment. American Economic Journal: Applied
Economics  2014.

[12] S. Behncke  M. Frölich  and M. Lechner. Targeting labour market programmes: results from a

randomized experiment. Work. Pap. 3085  IZA (Inst. Study Labor)  2007.

[13] A. Bennett and N. Kallus. Policy evaluation with latent confounders via optimal balance. In

Advances in Neural Information Processing Systems  2019.

9

[14] A. Bennett  N. Kallus  and T. Schnabel. Deep generalized method of moments for instrumental

variable analysis. In Advances in Neural Information Processing Systems  2019.

[15] A. Beresteanu  I. Molchanov  and F. Molinari. Partial identiﬁcation using random set theory.

Journal of Econometrics  166(1):17–32  2012.

[16] M. Berger  D. A. Black  and J. A. Smith. Econometric evaluation of labour market policies 
chapter EvaluatingProﬁling as a Means of Allocating Government Service  pages 59–84. 2000.
[17] D. Bhattacharya and P. Dupas. Inferring welfare maximizing treatment assignment under budget

constraints. Journal of Econometrics  2012.

[18] C. Brown  M. Ravallion  and D. van de Walle. A poor means test? econometric targeting in
africa. Policy Research Working Paper 7915: World Bank Group  Development Research Group 
Human Development and Public Services Team  2016.

[19] P. Carneiro  K. T. Hansen  and J. J. Heckman. Removing the veil of ignorance in assessing
the distributional impacts of social policies. Technical report  National Bureau of Economic
Research  2002.

[20] A. Charnes and W. W. Cooper. Programming with linear fractional functionals. Naval research

logistics (NRL)  9(3-4):181–186  1962.

[21] I. Chen  F. Johansson  and D. Sontag. Why is my classiﬁer discriminatory? In Advances in

Neural Information Processing Systems 31  2018.

[22] J. Chen  N. Kallus  X. Mao  G. Svacha  and M. Udell. Fairness under unawareness: Assessing
disparity when protected class is unobserved. In Proceedings of the Conference on Fairness 
Accountability  and Transparency  pages 339–348. ACM  2019.

[23] V. Chernozhukov  D. Chetverikov  M. Demirer  E. Duﬂo  C. Hansen  and W. Newey. Dou-
ble/debiased/neyman machine learning of treatment effects. American Economic Review  107
(5):261–65  2017.

[24] A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction

instruments. In Proceedings of FATML  2016.

[25] S. Corbett-Davies and S. Goel. The measure and mismeasure of fairness: A critical review of

fair machine learning. ArXiv preprint  2018.

[26] B. Crepon and G. J. van den Berg. Active labor market policies. Annual Review of Economics 

Vol. 8:521-546  2016.

[27] J. M. Davis and S. B. Heller. Using causal forests to predict treatment heterogeneity: An
application to summer jobs. American Economic Review: Papers and Proceedings  107(5):
546–550  2017.

[28] M. Dudik  D. Erhan  J. Langford  and L. Li. Doubly robust policy evaluation and optimization.

Statistical Science  2014.

[29] V. Eubanks. Automating inequality: How high-tech tools proﬁle  police  and punish the poor.

St. Martin’s Press  2018.

[30] M. Hardt  E. Price  N. Srebro  et al. Equality of opportunity in supervised learning. In Advances

in Neural Information Processing Systems  pages 3315–3323  2016.

[31] H. Heidari  C. Ferrari  K. Gummadi  and A. Krause. Fairness behind a veil of ignorance: A
welfare analysis for automated decision making. In Advances in Neural Information Processing
Systems  pages 1265–1276  2018.

[32] J. L. Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational

and Graphical Statistics  20(1):217–240  2011.

[33] L. Hu and Y. Chen. Fair classiﬁcation and social welfare. arXiv preprint arXiv:1905.00147 

2019.

10

[34] G. Imbens and D. Rubin. Causal Inference for Statistics  Social  and Biomedical Sciences.

Cambridge University Press  2015.

[35] J. Jung  R. Shroff  A. Feller  and S. Goel. Algorithmic decision making in the presence of

unmeasured confounding. ArXiv  2018.

[36] N. Kallus. Recursive partitioning for personalization using observation data. Proceedings of the

Thirty-fourth International Conference on Machine Learning  2017.

[37] N. Kallus. Balanced policy evaluation and learning.

Processing Systems  pages 8895–8906  2018.

In Advances in Neural Information

[38] N. Kallus. Classifying treatment responders under causal effect monotonicity. Proceedings of

International Conference on Machine Learning  2019.

[39] N. Kallus and A. Zhou. Confounding-robust policy improvement. In Advances in Neural

Information Processing Systems  pages 9269–9279  2018.

[40] N. Kallus and A. Zhou. Residual unfairness in fair machine learning from prejudiced data.

Forthcoming at ICML  2018.

[41] N. Kallus and A. Zhou. The fairness of risk scores beyond classiﬁcation: Bipartite ranking and

the xauc metric. In Advances in Neural Information Processing Systems  2019.

[42] N. Kallus  X. Mao  and A. Zhou. Assessing algorithmic fairness with unobserved protected

class using data combination. arXiv preprint arXiv:1906.00285  2019.

[43] N. Kallus  X. Mao  and A. Zhou. Interval estimation of individual-level causal effects under
unobserved confounding. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics  pages 2281–2290  2019.

[44] N. Kilbertus  M. Rojas-Carulla  G. Parascandolo  M. Hardt  D. Janzing  and B. Schölkopf.
Avoiding discrimination through causal reasoning. Advances in Neural Information Processing
Systems 30  2017  2017.

[45] T. Kitagawa and A. Tetenov. Empirical welfare maximization. 2015.
[46] A. Kube and S. Das. Allocating interventions based on predicted outcomes: A case study on
homelessness services. Proceedings of the AAAI Conference on Artiﬁcial Intelligence  2019.

[47] M. J. Kusner  J. R. Loftus  C. Russell  and R. Silva. Counterfactual fairness. NIPS  2017.
[48] H. Lakkaraju  J. Kleinberg  J. Leskovec  J. Ludwig  and S. Mullainathan. The selective labels
problem: Evaluating algorithmic predictions in the presence of unobservables. Proceedings of
KKD2017  2017.

[49] L. T. Liu  S. Dean  E. Rolf  M. Simchowitz  and M. Hardt. Delayed impact of fair machine
learning. Proceedings of the 35th International Conference on Machine Learning (ICML) 
Stockholm  Sweden  2018.

[50] D. Madras  E. Creager  T. Pitassi  and R. Zemel. Fairness through causal awareness: Learning
latent-variable models for biased data. ACM Conference on Fairness  Accountability  and
Transparency (ACM FAT*) 2019  2019.

[51] C. Manski. Social Choice with Partial Knoweldge of Treatment Response. The Econometric

Institute Lectures  2005.

[52] C. F. Manski. Monotone treatment response. Econometrica: Journal of the Econometric Society 

pages 1311–1334  1997.

[53] C. F. Manski. Partial identiﬁcation of probability distributions. Springer Science & Business

Media  2003.

[54] L. McBride and A. Nichols. Retooling poverty targeting using out-of-sample validation and
machine learning. Policy Research Working Paper 7849 (World Bank Group  Development
Economics Vice Presidency Operations and Strategy Team)  2016.

11

[55] S. Mitchell  E. Potash  and S. Barocas. Prediction-based decisions and fairness: A catalogue of

choices  assumptions  and deﬁnitions. arXiv  2018.

[56] T. Mkandawire. Targeting and universalism in poverty reduction. Social Policy and Development 

2005.

[57] E. Rice. The tay triage tool: A tool to identify homeless transition age youth most in need of

permanent supportive housing. 2013.

[58] J. M. Robins  A. Rotnitzky  and L. P. Zhao. Estimation of regression coefﬁcients when some
regressors are not always observed. Journal of the American statistical Association  89(427):
846–866  1994.

[59] R. T. Rockafellar. Convex analysis. Princeton university press  2015.
[60] D. B. Rubin. Comments on “randomization analysis of experimental data: The ﬁsher ran-
domization test comment”. Journal of the American Statistical Association  75(371):591–593 
1980.

[61] U. Shalit  F. Johansson  and D. Sontag. Estimating individual treatment effect: generalization
bounds and algorithms. Proceedings of the 34th International Conference on Machine Learning 
2017.

[62] R. Shroff. Predictive analytics for city agencies: Lessons from children’s services. Big data  5

(3):189–196  2017.

[63] S. Wager and S. Athey. Efﬁcient policy learning. 2017.
[64] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using

random forests. Journal of the American Statistical Association  (just-accepted)  2017.

12

,Luke O'Connor
Soheil Feizi
Tengyao Wang
Quentin Berthet
Yaniv Plan
Aladin Virmaux
Kevin Scaman
Nathan Kallus
Angela Zhou