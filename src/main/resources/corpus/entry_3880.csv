2018,On Fast Leverage Score Sampling and Optimal Learning,Leverage score sampling provides an appealing way to perform approximate com- putations for large matrices. Indeed  it allows to derive faithful approximations with a complexity adapted to the problem at hand. Yet  performing leverage scores sampling is a challenge in its own right requiring further approximations. In this paper  we study the problem of leverage score sampling for positive definite ma- trices defined by a kernel. Our contribution is twofold. First we provide a novel algorithm for leverage score sampling and second  we exploit the proposed method in statistical learning by deriving a novel solver for kernel ridge regression. Our main technical contribution is showing that the proposed algorithms are currently the most efficient and accurate for these problems.,On Fast Leverage Score Sampling and Optimal

Learning

Alessandro Rudi⇤
INRIA – Sierra team 

ENS  Paris

Daniele Calandriello⇤
LCSL – IIT & MIT 

Genoa  Italy

Luigi Carratino

University of Genoa 

Genoa  Italy

Lorenzo Rosasco
University of Genoa 
LCSL – IIT & MIT

Abstract

Leverage score sampling provides an appealing way to perform approximate com-
putations for large matrices. Indeed  it allows to derive faithful approximations
with a complexity adapted to the problem at hand. Yet  performing leverage scores
sampling is a challenge in its own right requiring further approximations. In this
paper  we study the problem of leverage score sampling for positive deﬁnite ma-
trices deﬁned by a kernel. Our contribution is twofold. First we provide a novel
algorithm for leverage score sampling and second  we exploit the proposed method
in statistical learning by deriving a novel solver for kernel ridge regression. Our
main technical contribution is showing that the proposed algorithms are currently
the most efﬁcient and accurate for these problems.

1

Introduction

A variety of machine learning problems require manipulating and performing computations with
large matrices that often do not ﬁt memory. In practice  randomized techniques are often employed to
reduce the computational burden. Examples include stochastic approximations [1]  columns/rows
subsampling and more general sketching techniques [2  3]. One of the simplest approach is uniform
column sampling [4  5]  that is replacing the original matrix with a subset of columns chosen
uniformly at random. This approach is fast to compute  but the number of columns needed for a
prescribed approximation accuracy does not take advantage of the possible low rank structure of the
matrix at hand. As discussed in [6]  leverage score sampling provides a way to tackle this shortcoming.
Here columns are sampled proportionally to suitable weights  called leverage scores (LS) [7  6]. With
this sampling strategy  the number of columns needed for a prescribed accuracy is governed by the
so called effective dimension which is a natural extension of the notion of rank. Despite these nice
properties  performing leverage score sampling provides a challenge in its own right  since it has
complexity in the same order of an eigendecomposition of the original matrix. Indeed  much effort
has been recently devoted to derive fast and provably accurate algorithms for approximate leverage
score sampling [2  8  6  9  10].
In this paper  we consider these questions in the case of positive semi-deﬁnite matrices  central for
example in Gaussian processes [11] and kernel methods [12]. Sampling approaches in this context
are related to the so called Nyström approximation [13] and Nyström centers selection problem [11] 
and are widely studied both in practice [4] and in theory [5]. Our contribution is twofold. First 
we propose and study BLESS  a novel algorithm for approximate leverage scores sampling. The
ﬁrst solution to this problem is introduced in [6]  but has poor approximation guarantees and high
time complexity. Improved approximations are achieved by algorithms recently proposed in [8] and
[9]. In particular  the approach in [8] can obtain good accuracy and very efﬁcient computations but
only as long as distributed resources are available. Our ﬁrst technical contribution is showing that
our algorithm can achieve state of the art accuracy and computational complexity without requiring

⇤Equal contribution. Respective emails: alessandro.rudi@inria.fr  daniele.calandriello@iit.it

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

distributed resources. The key idea is to follow a coarse to ﬁne strategy  alternating uniform and
leverage scores sampling on sets of increasing size.
Our second  contribution is considering leverage score sampling in statistical learning with least
squares. We extend the approach in [14] for efﬁcient kernel ridge regression based on combining
fast optimization algorithms (preconditioned conjugate gradient) with uniform sampling. Results in

[14] showed that optimal learning bounds can be achieved with a complexity which is eO(npn) in
time and eO(n) space. In this paper  we study the impact of replacing uniform with leverage score
the time and memory is now eO(ndeff)  and eO(deff

sampling. In particular  we prove that the derived method still achieves optimal learning bounds but
2) respectively  where deff is the effective dimension
which and is never larger  and possibly much smaller  than pn. To the best of our knowledge this is
the best currently known computational guarantees for a kernel ridge regression solver.

2 Leverage score sampling with BLESS

After introducing leverage score sampling and previous algorithms  we present our approach and ﬁrst
theoretical results.

2.1 Leverage score sampling

Suppose bK 2 Rn⇥n is symmetric and positive semideﬁnite. A basic question is deriving memory
efﬁcient approximation of bK [4  8] or related quantities  e.g. approximate projections on its range
[9]  or associated estimators  as in kernel ridge regression [15  14]. The eigendecomposition of bK

offers a natural  but computationally demanding solution. Subsampling columns (or rows) is an
appealing alternative. A basic approach is uniform sampling  whereas a more reﬁned approach is
leverage scores sampling. This latter procedure corresponds to sampling columns with probabilities
proportional to the leverage scores

(1)
where [n] = {1  . . .   n}. The advantage of leverage score sampling  is that potentially very few
columns can sufﬁce for the desired approximation. Indeed  letting

`(i  ) =⇣bK(bK + nI)1⌘ii

 

i 2 [n] 

d1() = n max

i=1 ... n

`(i  ) 

deff() =

`(i  ) 

nXi=1

for > 0  it is easy to see that deff()  d1()  1/ for all   and previous results show that
the number of columns required for accurate approximation are d1 for uniform sampling and deff
for leverage score sampling [5  6]. However  it is clear from deﬁnition (1) that an exact leverage
scores computation would require the same order of computations as an eigendecomposition  hence
approximations are needed.The accuracy of approximate leverage scores is typically measured by
t > 0 in multiplicative bounds of the form

Before proposing a new improved solution  we brieﬂy discuss relevant previous works. To provide a
uniﬁed view  some preliminary discussion is useful.

1

1 + t

`(i  ) e`(i  )  (1 + t)`(i  ) 

8i 2 [n].

(2)

2.2 Approximate leverage scores
First  we recall how a subset of columns can be used to compute approximate leverage scores. For
i=1 with ji 2 [n]  and bKJ J 2 RM⇥M with entries (KJ J )lm = Kjl jm. For
M  n  let J = {ji}M
i 2 [n]  let bKJ i = (bKj1 i  . . .   bKjM  i) and consider for > 1/n 
e`J (i  ) = (n)1(bKii  bK>J i(bKJ J + nA)1bKJ i) 
(3)
where A 2 RM⇥M is a matrix to be speciﬁed ⇤ (see later for details). The above deﬁnition is
motivated by the observation that if J = [n]  and A = I  thene`J (i  ) = `(i  )  by the following
⇤Clearly e`J depends on the choice of the matrix A  but we omit this dependence to simplify the notation.

2

identity

Also in the following we will use the notation

In the following  it is also useful to consider a subset of leverage scores computed as in (3). For
M  R  n  let U = {ui}R

bK(bK + nI)1 = (n)1(bK  bK(bK + nI)1bK).
i=1 with ui 2 [n]  and
LJ (U  ) = {e`J (u1  )  . . .  e`J (uR  )}.

(5)
to indicate the leverage score sampling of J0 ⇢ U columns based on the leverage scores LJ (U  ) 
that is the procedure of sampling columns from U according to their leverage scores 1  computed
using J  to obtain a new subset of columns J0.
We end noting that leverage score sampling (5) requires O(M 2) memory to store KJ  and O(M 3 +
RM 2) time to invert KJ  and compute R leverage scores via (3).

LJ (U  ) 7! J0

(4)

2.3 Previous algorithms for leverage scores computations
We discuss relevant previous approaches using the above quantities.

TWO-PASS sampling [6]. This is the ﬁrst approximate leverage score sampling proposed 
and is based on using directly (5) as LJ1(U2  ) 7! J2  with U2 = [n] and J1 a subset taken
uniformly at random. Here we call this method TWO-PASS sampling since it requires two rounds of
sampling on the whole set [n]  one uniform to select J1 and one using leverage scores to select J2.

RECURSIVE-RLS [9]. This is a development of TWO-PASS sampling based on the idea of
recursing the above construction. In our notation  let U1 ⇢ U2 ⇢ U3 = [n]  where U1  U2 are
uniformly sampled and have cardinalities n/4 and n/2  respectively. The idea is to start from
J1 = U1  and consider ﬁrst

but then continue with

LJ1(U2  ) 7! J2 
LJ2(U3  ) 7! J3.

Indeed  the above construction can be made recursive for a family of nested subsets (Uh)H of
cardinalities n/2h  considering J1 = U1 and

LJh(Uh+1  ) 7! Jh+1.

(6)

SQUEAK[8]. This approach follows a different iterative strategy. Consider a partition U1  U2  U3
of [n]  so that Uj = n/3  for j = 1  . . . 3. Then  consider J1 = U1  and

and then continue with

LJ1[U2(J1 [ U2  ) 7! J2 
LJ2[U3(J2 [ U3  ) 7! J3.

Similarly to the other cases  the procedure is iterated considering H subsets (Uh)H
cardinality n/H. Starting from J1 = U1 the iterations is

h=1 each with

LJh[Uh+1(Jh [ Uh+1  ).

(7)

We note that all the above procedures require specifying the number of iteration to be performed  the
weights matrix to compute the leverage scores at each iteration  and a strategy to select the subsets
(Uh)h. In all the above cases the selection of Uh is based on uniform sampling  while the number of
iterations and weight choices arise from theoretical considerations (see [6  8  9] for details).
Note that TWO-PASS SAMPLING uses a set J1 of cardinality roughly 1/ (an upper bound on d1())
and incurs in a computational cost of RM 2 = n/2. In comparison  RECURSIVE-RLS [9] leads
to essentially the same accuracy while improving computations. In particular  the sets Jh are never
larger than deff(). Taking into account that at the last iteration performs leverage score sampling on
Uh = [n]  the total computational complexity is ndeff()2. SQUEAK [8] recovers the same accuracy 
size of Jh  and ndeff()2 time complexity when |Uh|' deff()  but only requires a single pass over
the data. We also note that a distributed version of SQUEAK is discussed in [8]  which allows to
reduce the computational cost to ndeff()2/p  provided p machines are available.

3

i=1  regularization   step q  starting reg. 0  constants q1  q2 controlling the

log q

approximation level.

Algorithm 1 Bottom-up Leverage Scores Sampling (BLESS)
Input: dataset {xi}n
Output: Mh 2 [n] number of selected points  Jh set of indexes  Ah weights.
1: J0 = ;  A0 = []  H = log(0/)
2: for h = 1 . . . H do
3:
4:
5:
6:
7:
8:
9:
10: Ah = RhMh
11: end for

h = h1/q
set constant Rh = q1 min{2/h  n}
sample Uh = {u1  . . .   uRh} i.i.d. ui ⇠ U nif orm([n])
computee`Jh1(xuk   h) for all uk 2 Uh using Eq. 3
set Ph = (ph k)Rh
set constant Mh = q2dh with dh = n
sample Jh = {j1  . . .   jMh} i.i.d. ji ⇠ M ultinomial(Ph  Uh)

k=1 with ph k =e`Jh1(xuk   h)/(Pu2Uhe`Jh1(xu  h))
diag⇣ph j1  . . .   ph jMh⌘

RhPu2Uhe`Jh1(xu  h)  and

n

2.4 Leverage score sampling with BLESS
The procedure we propose  dubbed BLESS  has similarities to the one proposed in [9] (see (6)) 
but also some important differences. The main difference is that  rather than a ﬁxed   we consider
a decreasing sequence of parameters 0 > 1 > ··· > H =  resulting in different algorithmic
choices. For the construction of the subsets Uh we do not use nested subsets  but rather each (Uh)H
h=1
is sampled uniformly and independently  with a size smoothly increasing as 1/h. Similarly  as in [9]
we proceed iteratively  but at each iteration a different decreasing parameter h is used to compute
the leverage scores. Using the notation introduced above  the iteration of BLESS is given by

LJh(Uh+1  h+1) 7! Jh+1 

(8)

where the initial set J1 = U1 is sampled uniformly with size roughly 1/0.
BLESS has two main advantages. The ﬁrst is computational: each of the sets Uh  including the ﬁnal
UH  has cardinality smaller than 1/. Therefore the overall runtime has a cost of only RM 2  M 2/ 
which can be dramatically smaller than the nM 2 cost achieved by the methods in [9]  [8] and is
comparable to the distributed version of SQUEAK using p = /n machines. The second advantage
is that a whole path of leverage scores {`(i  h)}H
h=1 is computed at once  in the sense that at each
iteration accurate approximate leverage scores at scale h are computed. This is extremely useful in
practice  as it can be used when cross-validating h. As a comparison  for all previous method a full
run of the algorithm is needed for each value of h.
In the paper we consider two variations of the above general idea leading to Algorithm 1 and
Algorithm 2. The main difference in the two algorithms lies in the way in which sampling is
performed: with and without replacement  respectively. In particular  considering sampling without
replacement (see 2) it is possible to take the set (Uh)H
h=1 to be nested and also to obtain slightly
improved results  as shown in the next section.
The derivation of BLESS rests on some basic ideas. First  note that  since sampling uniformly a set
U of size d1()  1/ allows a good approximation  then we can replace L[n]([n]  ) 7! J by

(9)
where J can be taken to have cardinality deff(). However  this is still costly  and the idea is to repeat
and couple approximations at multiple scales. Consider 0 >   a set U0 of size d1(0)  1/0
sampled uniformly  and LU0 (U0  0) 7! J0. The basic idea behind BLESS is to replace (9) by

LU(U  ) 7! J 

The key result  see   is that taking ˜J of cardinality

LJ0(U  ) 7! ˜J.

(10)
sufﬁce to achieve the same accuracy as J. Now  if we take 0 sufﬁciently large  it is easy to see that
deff(0) ⇠ d1(0) ⇠ 1/0  so that we can take J0 uniformly at random. However  the factor (0/)
in (10) becomes too big. Taking multiple scales ﬁx this problem and leads to the iteration in (8).

(0/)deff()

4

i=1  regularization   step q  starting reg. 0  constant q2 controlling the approxi-

 

log q

mation level.

Algorithm 2 Bottom-up Leverage Scores Sampling without Replacement (BLESS-R)
Input: dataset {xi}n
Output: Mh 2 [n] number of selected points  Jh set of indexes  Ah weights.
1: J0 = ;  A0 = []  H = log(0/)
2: for h = 1 . . . H do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for

h = h1/q
set constant h = min{q22/(hn)  1}
initialize Uh = ;
for i 2 [n] do
end for
for j 2 Uh do
compute ph j = min{q2e`Jh1(xj  h1)  1}
Jh = {j1  . . .   jMh}  and Ah = diag⇣ph j1  . . .   ph jMh⌘ .

add j to Jh with probability ph j/h

add i to Uh with probability h

end for

2.5 Theoretical guarantees
Our ﬁrst main result establishes in a precise and quantitative way the advantages of BLESS.
Theorem 1. Let n 2 N  > 0 and  2 (0  1]. Given t > 0  q > 1 and H 2 N  (h)H
in Algorithms 1 and 2  when (Jh  ah)H

h=1 are computed

h=1 deﬁned as

1. by Alg. 1 with parameters 0 = 2

2. by Alg. 2 with parameters 0 = 2

min(t 1)  q1  52q2
min(t 1)  q1  542 (2t+1)2

q(1+t)  q2  12q (2t+1)2
 

log 12Hn

t2

t2



(1 + t) log 12Hn



 

lete`Jh(i  h) as in Eq. (3) depending on Jh  Ah  then with probability at least 1  :

1

`(i  h)  e`Jh(i  h)  (1 + min(t  1))`(i  h) 

1 + t
|Jh| q2deff(h) 

8h 2 [H].

(a)

(b)

8i 2 [n]  h 2 [H] 

The above result conﬁrms that the subsets Jh computed by BLESS are accurate in the desired sense 
see (2)  and the size of all Jh is small and proportional to deff(h)  leading to a computational

cost of only Omin 1

   n deff()2 log2 1

 in time and Odeff()2 log2 1

 in space (for additional

properties of Jh see Thm. 4 in appendixes). Table 1 compares the complexity and number of
columns sampled by BLESS with other methods. The crucial point is that in most applications  the
parameter  is chosen as a decreasing function of n  e.g.  = 1/pn  resulting in potentially massive
computational gains. Indeed  since BLESS computes leverage scores for sets of size at most 1/  this
allows to perform leverage scores sampling on matrices with millions of rows/columns  as shown in
the experiments. In the next section  we illustrate the impact of BLESS in the context of supervised
statistical learning.

3 Efﬁcient supervised learning with leverage scores

In this section  we discuss the impact of BLESS in a supervised learning. Unlike most previous
results on leverage scores sampling in this context [6  8  9]  we consider the setting of statistical
learning  where the challenge is that inputs  as well as the outputs  are random. More precisely  given
a probability space (X ⇥ Y  ⇢)  where Y ⇢ R  and considering least squares  the problem is to solve
(11)

(f (x)  y)2d⇢(x  y) 

E(f ) =ZX⇥Y

min

f2HE(f ) 

5

Algorithm
Uniform Sampling [5]
Exact RLS Sampl.
Two-Pass Sampling [6]
Recursive RLS [9]
SQUEAK [8]
This work  Alg. 1 and 2

Runtime


n3
n/2

ndeff()2
ndeff()2

1/ de↵ ()2

|J|
1/
deff()
deff()
deff()
deff()
deff()

i=1 ⇠ ⇢n.

complexity and cardinality of the set J required to satisfy the approximation condition in Eq. 2.

Table 1: The proposed algorithms are compared with the state of the art (in eO notation)  in terms of time
In the above minimization problem  H is a
when ⇢ is known only through (xi  yi)n
reproducing kernel Hilbert space deﬁned by a positive deﬁnite kernel K : X ⇥ X ! R [12].
Recall that the latter is deﬁned as the completion of span{K(x ·) | x 2 X} with the inner product
hK(x ·)  K(x0 ·)iH = K(x  x0). The quality of an empirical approximate solution bf is measured
via probabilistic bounds on the excess risk R(bf ) = E(bf )  minf2H E(f ).

3.1 Learning with FALKON-BLESS
The algorithm we propose  called FALKON-BLESS  combines BLESS with FALKON [14] a state of
the art algorithm to solve the least squares problem presented above. The appeal of FALKON is that
it is currently the most efﬁcient solution to achieve optimal excess risk bounds. As we discuss in the
following  the combination with BLESS leads to further improvements.
We describe the derivation of the considered algorithm starting from kernel ridge regression (KRR)

nXi=1

bf(x) =

K(x  xi)ci 

c = (bK + nI)1bY

(12)

space requirements. FALKON can be seen as an approximate ridge regression solver combining a

where c = (c1  . . .   cn) bY = (y1  . . .   yn) and bK 2 Rn⇥n is the empirical kernel matrix with entries
(bK)ij = K(xi  xj). KRR has optimal statistical properties [16]  but large O(n3) time and O(n2)
number of algorithmic ideas. First  sampling is used to select a subset {ex1  . . .  exM} of the input

data uniformly at random  and to deﬁne an approximate solution

= (K>nM KnM + KMM )1K>nM y 

(13)

bf M (x) =

MXj=1

K(exj  x)↵j ↵

where ↵ = (↵1  . . .  ↵ M )  KnM 2 Rn⇥M  has entries (KnM )ij = K(xi  ˜xj) and KMM 2 RM⇥M
has entries (KMM )jj0 = K(˜xj  ˜xj0)  with i 2 [n]  j  j0 2 [M ]. We note  that the linear system
in (13) can be seen to obtained from the one in (12) by uniform column subsampling of the empirical

kernel matrix. The columns selected corresponds to the inputs {ex1  . . .  exM}. FALKON proposes to

compute a solution of the linear system 13 via a preconditioned iterative solver. The preconditioner is
the core of the algorithm and is deﬁned by a matrix B such that

BB> =⇣ n

M

K2

MM + KMM⌘1

.

(14)

The above choice provides a computationally efﬁcient approximation to the exact preconditioner
of the linear system in (13) corresponding to B such that BB> = (K>nM KnM + KMM )1. The
preconditioner in (14) can then be combined with conjugate gradient to solve the linear system in (13).
The overall algorithm has complexity O(nM t) in time and O(M 2) in space  where t is the number
of conjugate gradient iterations performed.

leverage score sampling using BLESS  see Algorithm 1 or Algorithm 2  so that M = Mh and

In this paper  we analyze a variation of FALKON where the points {ex1  . . .  exM} are selected via
exk = xjk  for Jh = {j1  . . .   jMh} and k 2 [Mh]. Further  the preconditioner in (14) is replaced by

KJh JhA1

(15)

h KJh Jh + hKJh Jh⌘1

.

BhB>h =⇣ n

M

6

BLESS
BLESS-R
SQUEAK
Uniform
RRLS

Time R-ACC 5th/ 95th quant
17
17
52
-
235

0.57 / 2.03
0.73 / 1.50
0.70 / 1.48
0.22 / 3.75
1.00 / 2.70

1.06
1.06
1.06
1.09
1.59

RLS Accuracy

3

2.5

2

C
C
A
R

-

1.5

1

Figure 1: Leverage scores relative accuracy for  = 105  n = 70 000  M = 10 000  10 repetitions.

BLESS

BLESS-R SQUEAK Uniform

RRLS

This solution can lead to huge computational improvements. Indeed  the total cost of FALKON-
BLESS is the sum of computing BLESS and FALKON  corresponding to
O(M 2) 

OnM t + (1/)M 2 log n + M 3

in time and space respectively  where M is the size of the set JH returned by BLESS.

(16)

3.2 Statistical properties of FALKON-BLESS
In this section  we state and discuss our second main result  providing an excess risk bound for
FALKON-BLESS. Here a population version of the effective dimension plays a key role. Let ⇢X
be the marginal measure of ⇢ on X  let C : H!H be the linear operator deﬁned as follows and
deff⇤() be the population version of deff() 

deff⇤() = Tr(C(C + I)1)  with (Cf )(x0) =ZX

K(x0  x)f (x)d⇢X(x) 

8x  x0 2 X 

K(x  x0)  2 

for any f 2H   x 2 X. It is possible to show that deff⇤() is the limit of deff() as n goes to inﬁnity 
see Lemma 1 below taken from [15]. If we assume throughout that 
(17)
then the operator C is symmetric  positive deﬁnite and trace class  and the behavior of deff⇤() can
be characterized in terms of the properties of the eigenvalues (j)j2N of C. Indeed as for deff()  we
have that deff⇤()  2/  moreover if j = O(j↵)  for ↵  1  we have deff⇤() = O(1/↵) .
Then for larger ↵  deff⇤ is smaller than 1/ and faster learning rates are possible  as shown below.
We next discuss the properties of the FALKON-BLESS solution denoted by bf n t.
Theorem 2. Let n 2 N  > 0 and  2 (0  1]. Assume that y 2 [ a
2 ]  almost surely  a > 0  and
denote by fH a minimizer of (11). There exists n0 2 N  such that for any n  n0  if t  log n 
  92
+ ! .

2   a
   then the following holds with probability at least 1  :
R(bf n t) 

In particular  when deff⇤() = O(1/↵)  for ↵  1  by selecting ⇤ = n↵/(↵+1)  we have

H a2 log2 2

+ 32kfHk2

a deff() log 2


n log n



+

n2

n

4a
n

where c is given explicitly in the proof.

R(bf⇤ n t)  cn ↵

↵+1  

We comment on the above result discussing the statistical and computational implications.

Statistics. The above theorem provides statistical guarantees in terms of ﬁnite sample bounds on
the excess risk of FALKON-BLESS  A ﬁrst bound depends of the number of examples n  the
regularization parameter  and the population effective dimension deff⇤(). The second bound is
derived optimizing   and is the same as the one achieved by exact kernel ridge regression which is

7

Figure 2: Runtimes with  = 103 and n increasing Figure 3: C-err at 5 iterations for varying f alkon

known to be optimal [16  17  18]. Note that improvements under further assumptions are possible
and are derived in the supplementary materials  see Thm. 8. Here  we comment on the computational
properties of FALKON-BLESS and compare it to previous solutions.

Computations. To discuss computational implications  we recall a result from [15] show-
ing that the population version of the effective dimension deff⇤() and the effective dimension deff()
associated to the empirical kernel matrix converge up to constants.
Lemma 1. Let > 0 and  2 (0  1]. When   92

   then with probability at least 1   

n log n

(1/3)deff⇤()  deff()  3deff⇤().

Recalling the complexity of FALKON-BLESS (16)  using Thm 2 and Lemma 1  we derive a cost

O✓ndeff⇤() log n +

1


deff⇤()2 log n + deff⇤()3◆

1

deff⇤()  2/ 

in time and O(deff⇤()2) in space  for all n   satisfying the assumptions in Theorem 2. These
expressions can be further simpliﬁed. Indeed  it is easy to see that for all > 0 
(18)
 deff⇤()2. Moreover  if we consider the optimal choice ⇤ = O(n ↵
so that deff⇤()3  2
↵+1 )
given in Theorem 2  and take deff⇤() = O(1/↵)  we have 1
deff⇤(⇤) O (n)  and therefore
⇤
 deff⇤()2 O (ndeff⇤()). In summary  for the parameter choices leading to optimal learning rates 
FALKON-BLESS has complexity eO(ndeff⇤(⇤))  in time and eO(deff⇤(⇤)2) in space  ignoring log
terms. We can compare this to previous results. In [14] uniform sampling is considered leading to
M O (1/) and achieving a complexity of eO(n/) which is always larger than the one achieved by
eO(ndeff()2) time and reducing the time complexity of FALKON to eO(ndeff(⇤)). Clearly in this

FALKON in view of (18). Approximate leverage scores sampling is also considered in [14] requiring

case the complexity of leverage scores sampling dominates  and our results provide BLESS as a ﬁx.

4 Experiments

Leverage scores accuracy. We ﬁrst study the accuracy of the leverage scores generated by BLESS
and BLESS-R  comparing SQUEAK [8] and Recursive-RLS (RRLS) [9]. We begin by uniformly
sampling a subsets of n = 7 ⇥ 104 points from the SUSY dataset [19]  and computing the exact
leverage scores `(i  ) using a Gaussian Kernel with  = 4 and  = 105  which is at the limit of our
computational feasibility. We then run each algorithm to compute the approximate leverage scores
e`JH (i  )  and we measure the accuracy of each method using the ratioe`JH (i  )/`(i  ) (R-ACC).

The ﬁnal results are presented in Figure 1. On the left side for each algorithm we report runtime  mean
R-ACC  and the 5th and 95th quantile  each averaged over the 10 repetitions. On the right side a box-
plot of the R-ACC. As shown in Figure 1 BLESS and BLESS-R achieve the same optimal accuracy

8

01234567Number of Points10410-210-1100101SecondsTime ComparisonBLESSBLESS-RSQUEAKRRLS10-1410-1210-1010-810-610-410-2100falkon0.180.20.220.240.260.280.30.32Classification ErrorTest ErrorFALKON-UNIFALKON-BLESSFigure 4: AUC per iteration of the SUSY dataset

Figure 5: AUC per iteration of the HIGGS dataset

of SQUEAK with just a fraction of time. Note that despite our best efforts  we could not obtain
high-accuracy results for RRLS (maybe a wrong constant in the original implementation). However
note that RRLS is computationally demanding compared to BLESS  being orders of magnitude
slower  as expected from the theory. Finally  although uniform sampling is the fastest approach  it
suffers from much larger variance and can over or under-estimate leverage scores by an order of
magnitude more than the other methods  making it more fragile for downstream applications.
In Fig. 2 we plot the runtime cost of the compared algorithms as the number of points grows from
n = 1000 to 70000  this time for  = 103. We see that while previous algorithms’ runtime grows
near-linearly with n  BLESS and BLESS-R run in a constant 1/ runtime  as predicted by the theory.

BLESS for supervised learning. We study the performance of FALKON-BLESS and compare it
with the original FALKON [14] where an equal number of Nyström centres are sampled uniformly at
random (FALKON-UNI). We take from [14] the two biggest datasets and their best hyper-parameters
for the FALKON algorithm.
We noticed that it is possible to achieve the same accuracy of FALKON-UNI  by using bless for
BLESS and f alkon for FALKON with bless  f alkon  in order to lower the deff and keep
the number of Nyström centres low. For the SUSY dataset we use a Gaussian Kernel with  =
4  f alkon = 106  bless = 104 obtaining MH ' 104 Nyström centres. For the HIGGS dataset
we use a Gaussian Kernel with  = 22  f alkon = 108  bless = 106  obtaining MH ' 3 ⇥ 104
Nyström centres. We then sample a comparable number of centers uniformly for FALKON-UNI.
Looking at the plot of their AUC at each iteration (Fig.4 5) we observe that FALKON-BLESS
converges much faster than FALKON-UNI. For the SUSY dataset (Figure 4) 5 iterations of FALKON-
BLESS (160 seconds) achieve the same accuracy of 20 iterations of FALKON-UNI (610 seconds).
Since running BLESS takes just 12 secs. this corresponds to a ⇠ 4⇥ speedup. For the HIGGS dataset
10 iter. of FALKON-BLESS (with BLESS requiring 1.5 minutes  for a total of 1.4 hours) achieve
better accuracy of 20 iter. of FALKON-UNI (2.7 hours). Additionally we observed that FALKON-
BLESS is more stable than FALKON-UNI w.r.t. f alkon  . In Figure 3 the classiﬁcation error after
5 iterations of FALKON-BLESS and FALKON-UNI over the SUSY dataset (bless = 104). We
notice that FALKON-BLESS has a wider optimal region (95% of the best error) for the regulariazion
parameter ([1.3 ⇥ 103  4.8 ⇥ 108]) w.r.t. FALKON-UNI ([1.3 ⇥ 103  3.8 ⇥ 106]).
5 Conclusions

In this paper we presented two algorithms BLESS and BLESS-R to efﬁciently compute a small set
of columns from a large symmetric positive semideﬁnite matrix K  useful for approximating the
matrix or to compute leverage scores with a given precision. Moreover we applied the proposed
algorithms in the context of statistical learning with least squares  combining BLESS with FALKON
[14]. We analyzed the computational and statistical properties of the resulting algorithm  showing that
it achieves optimal statistical guarantees with a cost that is O(ndeff⇤()) in time  being currently the
fastest. We can extend the proposed work in several ways: (a) combine BLESS with fast stochastic
[20] or online [21] gradient algorithms and other approximation schemes (i.e. random features
[22  23  24])  to further reduce the computational complexity for optimal rates  (b) consider the
impact of BLESS in the context of multi-tasking [25  26] or structured prediction [27  28].

9

5101520Iterations0.50.550.60.650.70.750.80.85AUCTest AccuracyAcknowledgments.
This material is based upon work supported by the Center for Brains  Minds and Machines (CBMM)  funded by
NSF STC award CCF-1231216  and the Italian Institute of Technology. We gratefully acknowledge the support
of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research.
L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007
(European Ofﬁce of Aerospace Research and Development)  and the EU H2020-MSCA-RISE project NoMADS
- DLV-777826. A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063).

References
[1] Raman Arora  Andrew Cotter  Karen Livescu  and Nathan Srebro. Stochastic optimization for
pca and pls. In Communication  Control  and Computing (Allerton)  2012 50th Annual Allerton
Conference on  pages 861–868. IEEE  2012.

[2] David P. Woodruff. Sketching as a tool for numerical linear algebra.

arXiv:1411.4357  2014.

arXiv preprint

[3] Joel A Tropp. User-friendly tools for random matrices: An introduction. Technical report 
CALIFORNIA INST OF TECH PASADENA DIV OF ENGINEERING AND APPLIED
SCIENCE  2012.

[4] Christopher Williams and Matthias Seeger. Using the Nystrom method to speed up kernel

machines. In Neural Information Processing Systems  2001.

[5] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on

Learning Theory  2013.

[6] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical

guarantees. In Neural Information Processing Systems  2015.

[7] Petros Drineas  Malik Magdon-Ismail  Michael W. Mahoney  and David P. Woodruff. Fast
approximation of matrix coherence and statistical leverage. The Journal of Machine Learning
Research  13(1):3475–3506  2012.

[8] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Distributed adaptive sampling for

kernel matrix approximation. In AISTATS  2017.

[9] Cameron Musco and Christopher Musco. Recursive Sampling for the Nyström Method. In

NIPS  2017.

[10] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Second-order kernel online convex
optimization with adaptive sketching. In Doina Precup and Yee Whye Teh  editors  Proceedings
of the 34th International Conference on Machine Learning  volume 70 of Proceedings of Ma-
chine Learning Research  pages 645–653  International Convention Centre  Sydney  Australia 
06–11 Aug 2017. PMLR.

[11] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine
learning. Adaptive computation and machine learning. MIT Press  Cambridge  Mass  2006.
OCLC: ocm61285753.

[12] Bernhard Schölkopf  Alexander J Smola  et al. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2002.

[13] Alex J Smola and Bernhard Schölkopf. Sparse greedy matrix approximation for machine

learning. 2000.

[14] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. Falkon: An optimal large scale kernel

method. In Advances in Neural Information Processing Systems  pages 3891–3901  2017.

[15] Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. Less is more: Nyström computa-
tional regularization. In Advances in Neural Information Processing Systems  pages 1657–1665 
2015.

10

[16] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

[17] Ingo Steinwart  Don R Hush  Clint Scovel  et al. Optimal rates for regularized least squares

regression. In COLT  2009.

[18] Junhong Lin  Alessandro Rudi  Lorenzo Rosasco  and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over hilbert spaces. Applied and Computational
Harmonic Analysis  2018.

[19] Pierre Baldi  Peter Sadowski  and Daniel Whiteson. Searching for exotic particles in high-energy

physics with deep learning. Nature communications  5:4308  2014.

[20] Nicolas L Roux  Mark Schmidt  and Francis R Bach. A stochastic gradient method with
an exponential convergence _rate for ﬁnite training sets. In Advances in neural information
processing systems  pages 2663–2671  2012.

[21] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Efﬁcient second-order online
kernel learning with adaptive embedding. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach 
R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural Information Processing
Systems 30  pages 6140–6150. Curran Associates  Inc.  2017.

[22] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2008.

[23] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random

features. In Advances in Neural Information Processing Systems  pages 3215–3225  2017.

[24] Luigi Carratino  Alessandro Rudi  and Lorenzo Rosasco. Learning with sgd and random features.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors 
Advances in Neural Information Processing Systems 31  pages 10213–10224. Curran Associates 
Inc.  2018.

[25] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  73(3):243–272  2008.

[26] Carlo Ciliberto  Alessandro Rudi  Lorenzo Rosasco  and Massimiliano Pontil. Consistent multi-
task learning with nonlinear output relations. In Advances in Neural Information Processing
Systems  pages 1986–1996  2017.

[27] Carlo Ciliberto  Lorenzo Rosasco  and Alessandro Rudi. A consistent regularization approach
for structured prediction. Advances in Neural Information Processing Systems 29  pages
4412–4420  2016.

[28] Anna Korba  Alexandre Garcia  and Florence d’Alché Buc. A structured prediction approach
for label ranking. In Advances in Neural Information Processing Systems  pages 9008–9018 
2018.

[29] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathemati-

cal society  68(3):337–404  1950.

[30] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business

Media  2008.

11

,Joseph Geumlek
Shuang Song
Kamalika Chaudhuri
Alessandro Rudi
Daniele Calandriello
Luigi Carratino
Lorenzo Rosasco