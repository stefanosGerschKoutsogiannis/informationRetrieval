2019,Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints,We consider the classical stochastic multi-armed bandit problem with a constraint on the total cost incurred by switching between actions. Under the unit switching cost structure  where the constraint limits the total number of switches  we prove matching upper and lower bounds on regret and provide near-optimal algorithms for this problem. Surprisingly  we discover phase transitions and cyclic phenomena of the optimal regret. That is  we show that associated with the multi-armed bandit problem  there are equal-length phases defined by the number of arms and switching costs  where the regret upper and lower bounds in each phase remain the same and drop significantly between phases. The results enable us to fully characterize the trade-off between regret and incurred switching cost in the stochastic multi-armed bandit problem  contributing new insights to this fundamental problem. Under the general switching cost structure  our analysis reveals a surprising connection between the bandit problem and the shortest Hamiltonian path problem.,Phase Transitions and Cyclic Phenomena in Bandits

with Switching Constraints

David Simchi-Levi

Yunzong Xu

Institute for Data  Systems and Society
Massachusetts Institute of Technology

Institute for Data  Systems and Society
Massachusetts Institute of Technology

Cambridge  MA 02139

dslevi@mit.edu

Cambridge  MA 02139

yxu@mit.edu

Abstract

We consider the classical stochastic multi-armed bandit problem with a constraint
on the total cost incurred by switching between actions. Under the unit switching
cost structure  where the constraint limits the total number of switches  we prove
matching upper and lower bounds on regret and provide near-optimal algorithms
for this problem. Surprisingly  we discover phase transitions and cyclic phenomena
of the optimal regret. That is  we show that associated with the multi-armed bandit
problem  there are equal-length phases deﬁned by the number of arms and switching
costs  where the regret upper and lower bounds in each phase remain the same and
drop signiﬁcantly between phases. The results enable us to fully characterize the
trade-off between regret and incurred switching cost in the stochastic multi-armed
bandit problem  contributing new insights to this fundamental problem. Under
the general switching cost structure  our analysis reveals a surprising connection
between the bandit problem and the shortest Hamiltonian path problem.

1

Introduction

The multi-armed bandit (MAB) problem is one of the most fundamental problems in online learning 
with diverse applications ranging from pricing and online advertising to clinical trails. In a traditional
MAB problem  the learner (i.e.  decision-maker) is allowed to switch freely between actions  and
an effective learning policy may incur frequent switching — indeed  the learner’s task is to balance
the exploration-exploitation trade-off  and both exploration (i.e.  acquiring new information) and
exploitation (i.e.  optimizing decisions based on up-to-date information) require switching. However 
in many real-world scenarios  it is costly to switch between different alternatives  and a learning
policy with limited switching behavior is preferred. The learner thus has to consider the cost of
switching in her learning task.
The conventional model: switching cost as a penalty. There is rich literature studying stochastic
MAB with switching costs. Most of the papers model the switching cost as a penalty in the learner’s
objective  i.e.  they measure a policy’s regret and incurred switching cost using the same metric and
the objective is to minimize the sum of these two terms (e.g.  [1  2  7  8]; there are other variations
with discounted rewards [5  4  6]  see [12] for a survey). Though this conventional “switching
penalty” model has attracted signiﬁcant research interest in the past  it has two limitations. First 
under this model  the learner’s total switching cost is a complete output determined by the learning
algorithm. However  in many real-world applications  there are strict limits on the learner’s switching
behavior  which should be modeled as a hard constraint  and hence the learner’s total budget of
√
switching cost should be an input that helps determine the algorithm. In particular  while the algorithm
in [8] developed for the “switching penalty” model can achieve ˜O(
T ) (distribution-free) regret
with O(log log T ) switches  if the learner wants a policy that always incurs ﬁnite switching cost

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

independent of T   then prior literature does not provide an answer. Second  the “switching penalty”
model has fundamental weakness in studying the trade-off between regret and incurred switching
√
cost in stochastic MAB — since the O(log log T ) bound on the incurred switching cost of a policy is
negligible compared with the ˜O(
T ) bound on its optimal regret  when adding the two terms up 
the term associated with incurred switching cost is always dominated by the regret  thus no trade-off
can be identiﬁed. As a result  to the best of our knowledge  prior literature has not characterized the
fundamental trade-off between regret and incurred switching cost in stochastic MAB.
The BwSC model: switching as a limited resource. In this paper  we introduce the Bandits with
Switching Constraints (BwSC) problem. The BwSC model addresses the issues associated with the
“switching penalty” model in several ways. First  it introduces a hard constraint on the total switching
cost  making the switching budget an input to learning policies  enabling us to design good policies
that guarantee limited switching cost. While O(log log T ) switches has proven to be sufﬁcient for a
learning policy to achieve near-optimal regret in MAB  in BwSC  we are mostly interested in the setting
of ﬁnite or o(log log T ) switching budget  which is highly relevant in practice. Second  by focusing
on rewards in the objective function and incurred switching cost in the switching constraint  the BwSC
framework enables the characterization of the fundamental trade-off between regret and maximum
incurred switching cost in MAB. Third  while most prior research assumes speciﬁc structures on
switching costs (e.g.  unit or homogeneous costs)  in reality  switching between different pairs of
actions may incur heterogeneous costs that do not follow any parametric form. The BwSC model
allows general switching costs  which makes it a powerful modeling framework.
Motivating examples. The BwSC framework has numerous applications  including dynamic pricing 
online assortment optimization  online advertising  clinical trails and vehicle routing. A representative
example is the dynamic pricing problem. Dynamic pricing with demand learning has proven its
effectiveness in online retailing. However  it is well known that in practice  sellers often face business
constraints that prevent them from conducting extensive price experimentation and making frequent
price changes. For example  according to [10]  Groupon limits the number of price changes  either
because of implementation constraints  or for fear of confusing customers and receiving negative
customer feedback.
In such scenarios  the seller’s sequential decision-making problem can be
modeled as a BwSC problem  where changing from each price to another price incurs some cost  and
there is a limit on the total cost incurred by price changes.
Main contributions. In this paper  we introduce the BwSC model  a general framework with strong
modeling power. The model overcomes the limitations of the prior “switching penalty” model and
has both practical and theoretical values.
We ﬁrst study the unit-switching-cost BwSC problem in Section 4. We develop an upper bound on
regret by proposing a simple and intuitive policy with carefully-designed switching rules  and prove
an information-theoretic lower bound that matches the above upper bound  indicating that our policy
is rate-optimal up to logarithmic factors. Methodologically  the proof of the lower bound involves
a novel “tracking the cover time” argument that has not appeared in prior literature and may be of
independent interest. With the analysis described above we obtain some surprising and insightful
results  namely  phase transitions and cyclic phenomena of the optimal regret. That is  we show that
associated with the BwSC problem  there are equal-length phases  deﬁned by the number of arms and
switching costs  where the regret upper and lower bounds in each phase remain the same and drop
signiﬁcantly between phases  see the precise deﬁnitions in Section 4.3.
We then study the general-switching-cost BwSC problem in Section 5. We propose an efﬁcient policy
and prove regret upper and lower bounds in the general setting. The results reveal a surprising
connection between the BwSC problem and the shortest Hamiltonian path problem.
For the full version of this paper (containing additional results and missing proofs)  see [14].

2 Notations  Model and Deﬁnitions
Notations. For all n1  n2 ∈ N such that n1 ≤ n2  we use [n1] to denote the set {1  . . .   n1}  and use
[n1 : n2] (resp. (n1 : n2]) to denote the set {n1  n1 + 1  . . .   n2} (resp. {n1 + 1  . . .   n2}). For all
x ≥ 0  we use (cid:98)x(cid:99) to denote the largest integer less than or equal to x. For ease of presentation  we
deﬁne (cid:98)x(cid:99) = 0 for all x < 0. Throughout the paper  we use big O  Ω  Θ notations to hide constant
factors  and use ˜O  ˜Ω  ˜Θ notations to hide constant factors and logarithmic factors.

2

Problem formulation. Consider a k-armed bandit problem where a learner chooses actions from
a ﬁxed set [k] = {1  . . .   k}. There is a total of T rounds. In each round t ∈ [T ]  the learner ﬁrst
chooses an action it ∈ [k]  then observes a reward rt(it) ∈ R. For each action i ∈ [k]  the reward of
action i is i.i.d. drawn from an (unknown) distribution Di with (unknown) expected value µi. We
assume that the distributions Di are standardized sub-Gaussian.Without loss of generality  we assume
supi j∈[k] |µi − µj| ∈ [0  1].
In our problem  the learner incurs a switching cost ci j = cj i ≥ 0 each time she switches between
action i and action j (i  j ∈ [k]). In particular  ci i = 0 for i ∈ [k]. There is a pre-speciﬁed switching
budget S ≥ 0 representing the maximum amount of switching costs that the learner can incur in total.
Once the total switching cost exceeds the switching budget S  the learner cannot switch her actions
any more. The learner’s goal is to maximize the expected total reward over T rounds.
Admissible policies. Let π denote the learner’s (non-anticipating) learning policy  and πt ∈ [k]
denote the action chosen by policy π at round t ∈ [T ]. More formally  πt establishes a probability
kernel acting from the space of historical actions and observations to the space of actions at round
t. Let PπD and EπD be the probability measure and expectation induced by policy π and latent
distributions D = (D1  . . .  Dk). According to the problem formulation  we only need to restrict our
attention to the S-switching-budget policies  which take S  k and T as input and are deﬁned below.
Deﬁnition 1 A policy π is said to be an S-switching-budget policy if for all D 

(cid:34)T−1(cid:88)

t=1

PπD

(cid:35)

cπt πt+1 ≤ S

= 1.

(cid:40)

(cid:35)(cid:41)

(cid:34) T(cid:88)

t=1

Let ΠS denote the set of all S-switching-budget policies  which is also the admissible policy class of
the BwSC problem.
Regret. The performance of a learning policy is measured against a clairvoyant policy that maximizes
the expected total reward given foreknowledge of the environment (i.e.  latent distributions) D. Let
µ∗ = maxi∈[k] µi. We deﬁne the regret of policy π as the worst-case difference between the expected
performance of the optimal clairvoyant policy and the expected performance of policy π:

µπt

.

Rπ(T ) = supD

S(T ) = inf π∈ΠS Rπ(T ).

T µ∗ − EπD
The minimax (optimal) regret of BwSC is deﬁned as R∗
In our paper  when we say a policy is “near-optimal” or “optimal up to logarithmic factors”  we mean
that its regret bound is optimal in T up to logarithmic factors of T   irrespective of whether the bound
is optimal in k  since typically k is much smaller than T (e.g.  k = O(1)).
Remark. There are two notions of regret in the stochastic bandit literature. The Rπ(T ) regret that
we consider is called distribution-free  as it does not depend on D. On the other hand  one can
also deﬁne the distribution-dependent regret RπD(T ) = T µ∗ − EπD
that depends on
D. This second notion of regret is only meaningful when µ1  . . .   µk are well-separated. Unlike the
classical MAB problem where there are policies simultaneously achieving near-optimal bounds under
both regret notions  in the BwSC problem  due to the limited switching budget  ﬁnding a policy that
simultaneously achieves near-optimal bounds under both regret notions is usually impossible. In this
paper  we focus on the distribution-free regret. Extensions to the distribution-dependent regret can be
found in the full version of this paper [14].
Relationship between BwSC and MAB. Obviously  BwSC and MAB share the same deﬁnition of
Rπ(S)  and the only difference between BwSC and MAB is the existence of a switching constraint
π ∈ ΠS  determined by (ci j) ∈ Rk×k
≥0 and S ∈ R≥0 (when S = ∞  BwSC degenerates to MAB).
This makes BwSC a natural framework to study the trade-off between regret and incurred switching
cost in MAB. That is  the trade-off between the optimal regret R∗
S(T ) and switching budget S in
BwSC completely characterizes the trade-off between a policy’s best achievable regret and its worst
possible incurred switching cost in MAB. We are interested in how R∗
S(T ) behaves over a range of
switching budget S  and how it is affected by the structure of switching costs (ci j).

(cid:104)(cid:80)T

t=1 µπt

(cid:105)

3

3 Other Related Work

This paper is not the ﬁrst one to study online learning problems with limited switches. Indeed  a few
authors have realized the practical signiﬁcance of limited switching budget. [10] considers a dynamic
pricing model where the demand function is unknown but belongs to a known ﬁnite set  and a pricing
policy is allowed to make at most m price changes. [9] studies a multi-period stochastic inventory
replenishment and pricing problem with unknown parametric demand and limited price changes. We
note that both [10  9] only focus on speciﬁc decision-making problems  and their results rely on some
strong assumptions about the unknown environment. By contrast  the BwSC model in our paper is
generic and assumes no prior knowledge of the environment. The learning task in BwSC is thus more
challenging than previous models. In the adversarial setting  [3] studies the adversarial MAB with
limited number of switches. Since our problem is stochastic while their problem is adversarial  the
results and methodologies in our paper are fundamentally different from their paper. It is worth noting
that the switching constraint in BwSC is also more general than the number-of-switch constraints in
the above-mentioned models.
The BwSC problem is also related to the batched bandit problem proposed by [13]. The M-batched
bandit problem is deﬁned as follows: given a classical bandit problem  assumes that the learner
must split her learning process into M batches and is only able to observe the realized rewards
from a given batch after the entire batch is completed. [13] studies the problem in the case of two
arms. Very recently  [11] extends the results to k arms. The batched bandit problem and the BwSC
problem are two different problems: the batched bandit problem limits observations and allows
unlimited switching  while the BwSC problem limits switching and allows unlimited observations.
Surprisingly  we discover some non-trivial connections between the batched bandit problem and the
unit-switching-cost BwSC problem  which are presented in the full version of this paper [14].

4 Unit Switching Costs
In this section  we consider the BwSC problem with unit switching costs  where ci j = 1 for all i (cid:54)= j.
In this case  since every switch incurs a unit cost  the switching budget S can be interpreted as the
maximum number of switches that the learner can make in total. Thus  the unit-switching-cost BwSC
problem can be simply interpreted as “MAB with limited number of switches”.

4.1 Upper Bound on Regret

We ﬁrst propose a simple and intuitive policy that provides an upper bound on the regret. Our
policy  called the S-Switch Successive Elimination (SS-SE) policy  is described in Algorithm 1. The
design philosophy behind the SS-SE policy is to divide the entire horizon into several pre-determined
intervals (i.e. batches) and to control the number of switches in each interval. The policy thus has
some similarities with the 2-armed batched policy of [13] and the k-armed batched policy of [11]  
which proves to be near-optimal in the batched bandit problem. However  since we are studying a
different problem  directly applying a batched policy to the BwSC problem does not work. In particular 
in the batched bandit problem  the number of intervals (i.e.  batches) is a given constraint  while in
the BwSC problem  the switching budget is the given constraint. We thus add two key ingredients into
the SS-SE policy: (1) an index m(S) suggesting how many intervals should be used to partition the
entire horizon; (2) a switching rule ensuring that the total number of switches within k actions cannot
exceed the switching budget S. These two ingredients make the SS-SE policy substantially different
from an ordinary batched policy.
Intuition about the policy. The policy divides the T rounds into (cid:98) S−1
k−1(cid:99) + 1 intervals in advance.
The sizes of the intervals are designed to balance the exploration-exploitation trade-off. An active
set of “good” actions Al is maintained for each interval l and at the end of each interval some “bad”
actions are eliminated before the start of the next interval. The policy controls the number of switches
by ensuring that only |Al| − 1 switches happen within each interval l and at most one switch happens
between two consecutive intervals. Finally  in the last interval only the empirical best action is chosen.
We show that the SS-SE policy is indeed an S-switching-budget policy and establish the following
upper bound on its regret.

4

Algorithm 1 S-Switch Successive Elimination (SS-SE)
Input: Number of arms k  Switching budget S  Horizon T
Partition: Calculate m(S) =

(cid:106) S−1

(cid:107)

.

k−1

Divide the entire time horizon 1  . . .   T into m(S) + 1 intervals: (t0 : t1]  (t1 : t2]  . . .   (tm(S) :

tm(S)+1]  where the endpoints are deﬁned by t0 = 0 and

(cid:22)

(cid:23)

1− 2−2−(i−1)

2−2−m(S) T

2−2−(i−1)
2−2−m(S)

ti =

k

  ∀i = 1  . . .   m(S) + 1.

(cid:115)
(cid:115)

if atl−1 ∈ Al then

Initialization: Let the set of all active actions in the l-th interval be Al. Set A1 = [k]. Let a0 be a
random action in [k].
Policy:
1: for l = 1  . . .   m(S) do
2:
3:

Let atl−1+1 = atl−1. Starting from this action  choose each action in Al for tl−tl−1
|Al|
consecutive rounds. Mark the last chosen action as atl.1
Starting from an arbitrary active action in Al  choose each action in Al for tl−tl−1
|Al|
tive rounds. Mark the last chosen action as atl.

else if atl−1 /∈ Al then

consecu-

end if
Statistical test: deactivate all actions i s.t. ∃ action j with UCBtl (i) < LCBtl(j)  where

4:
5:

6:
7:

UCBtl (i) = empirical mean of action i in[1 : tl] +

LCBtl (i) = empirical mean of action i in[1 : tl] −

2 log T

number of plays of action i in[1 : tl]

2 log T

number of plays of action i in[1 : tl]

 

.

8: end for
9: In the last interval  choose the action with the highest empirical mean (up to round tm(S)).

Theorem 1 Let π be the SS-SE policy  then π ∈ ΠS. There exists an absolute constant C ≥ 0 such
that for all k ≥ 1  S ≥ 1 and T ≥ k 

(cid:107)

(cid:106) S−1

k−1

.

where m(S) =

Rπ(T ) ≤ C(log k log T )k

1−

1

2−2−m(S) T

1

2−2−m(S)  

Theorem 1 provides an upper bound on the optimal regret of the unit-switching-cost BwSC problem:

R∗
S(T ) = ˜O(T 1/(2−2−(cid:98)(S−1)/(k−1)(cid:99))).

4.2 Lower Bound on Regret

The SS-SE policy  though achieves sublinear regret  seems to have many limitations that could have
weaken its performance  and on the surface it may suggest that the regret bound is not optimal. We
discuss two points here. (1) The SS-SE policy does not make full use of its switching budget. Consider
the case of 11 actions and 20 switching budget. Since m(20) = (cid:98)(20 − 1)/(11 − 1)(cid:99) = 1 = m(11) 
the SS-SE policy will just run as if it could only make 11 switches  despite the fact that it has 9
additional switching budget (which will never be used). It seems that by tracking and allocating
the switching budget in a more careful way  one can achieve lower regret. (2) The SS-SE policy
learns from data infrequently. Note that the SS-SE policy pre-determines the number  sizes and
locations of its intervals before seeing any data  and executes actions within each interval based on a
pre-determined schedule. Consider again the case of 11 actions and 20 switching budget  the SS-SE
policy will split the entire horizon into two intervals and will only learn from data at the end of the

5

ﬁrst interval  after which it will choose a single action to be applied throughout the entire second
interval. It seems that by learning from data more frequently  one can achieve lower regret.
While the above arguments are based on our ﬁrst instinct and seem very reasonable  surprisingly 
all of them prove to be wrong: no S-switch policy can theoretically do better! In fact  we match
the upper bound provided by SS-SE by showing an information-theoretic lower bound in Theorem
2. This indicates that the SS-SE policy is rate-optimal up to logarithmic factors  and R∗
S(T ) =
˜Θ(T 1/(2−2−(cid:98)(S−1)/(k−1)(cid:99))). Note that the tightness of T is acheived per instance  i.e.  for every k and
every S. That is  our lower bound is substantially stronger than a single lower bound demonstrated for
speciﬁc k and S. The proof of the lower bound involves a novel “tracking the cover time” argument
that (to the best of our knowledge) has not appeared in previous literature and may be of independent
interest. We state the lower bound and give a sketch of the proof below.
Theorem 2 There exists an absolute constant C > 0 such that for all k ≥ 1  S ≥ 1  T ≥ k and for
all policy π ∈ ΠS 
√
k
C

2−2−m(S) (m(S) + 1)−2(cid:17)

Rπ(T ) ≥

2−2−m(S)  

2−

T

1

1

if m(S) ≤ log2 log2(T /k) 
if m(S) > log2 log2(T /k) 

(cid:16)
(cid:40)
(cid:106) S−1

C

− 3

(cid:107)

kT  

.

k−1

where m(S) =
Proof idea. For any k ≥ 1  S ≥ 1 and T ≥ k  for any S-switch policy π ∈ ΠS  we want to ﬁnd an
environment D such that RπD(T ) is larger than the desired lower bound. A key challenge here is that
π is an arbitrary and abstract S-switch policy — we need more information about π to construct D.
With this goal in mind  we ﬁrst design a concrete “primal environment” α. We use this environment
to evaluate policy π  such that we can observe some key patterns revealed by policy π under α. These
patterns are characterized by a series of ordered stopping times τ1 ≤ τ2 ≤ ··· ≤ τm(S)+1  some of
which may be ∞  that are recursively deﬁned as follows:

• τ1 is the ﬁrst time that all the actions in [k] have been chosen in period [1 : τ1] 
• τ2 is the ﬁrst time that all the actions in [k] have been chosen in period [τ1 : τ2] 
• Generally  τi is the ﬁrst time that all the actions in [k] have been chosen in period [τi−1 : τi] 

for i = 2  . . .   m(S) + 1.

We then compare the realization of τ1  . . .   τm(S) with a series of ﬁxed values t1  . . .   tm(S)  which are
the endpoints of the intervals deﬁned in Algorithm 1. Based on the possible outcomes of comparisons 
we deﬁne m(S) + 1 key events:

• E1 = {τ1 > t1} 
• Ej = {τj−1 ≤ tj−1  τj > tj}  for j = 2  . . .   m(S) 
• Em(S)+1 = {τm(S) ≤ tm(S)} 

at least one of which must occur under π and α with probability at least 1/(m(S) + 1). We then
do a case by case analysis as follows. In the ﬁrst case  {τ1 > t1} occurs with certain probability 
indicating that the action chosen in round τ1 was not chosen in [1 : t1] with certain probability; in
the second case  ∃j ∈ [2 : m(S)] such that {τj−1 ≤ tj−1  τj > tj} occurs with certain probability 
indicating that the action chosen in round τj was not chosen in [tj−1 : tj] with certain probability;
in the third case  {τm(S) ≤ tm(S)} with certain probability  indicating that the number of switches
occurs in [tm(S) : T ] is at most k − 1. For each case  we construct an “auxiliary environment” β by
carefully adjusting α based on the aforementioned indication. The environment β ensures two things:
(1) β is “hard for π to distinguish from α”  such that a crucial event E (constructed based on the
indication) that occurs under π and α with certain probability also occurs under π and β with similar
probability; and (2) β is “different enough from α” such that the certain occurrence probability of
the event E under β makes Rπ
β(T ) larger than the desired lower bound. Theorem 2 then follows by
Rπ(T ) ≥ Rπ
Combining Theorem 1 and Theorem 2  we have
Corollary 1 For any ﬁxed k ≥ 1  for any S ≥ 1  R∗

β(T ). For the complete proof of Theorem 2  see the full version of this paper [14].

S(T ) = ˜Θ(T 1/(2−2−(cid:98)(S−1)/(k−1)(cid:99))).

6

Remark. We brieﬂy explain why the upper and lower bounds in Theorem 1 and Theorem 2 match in
T . When m(S) ≤ log2 log2(T /k)  which is the case we are mostly interested in  (m(S) + 1)2 =
√
o(log T )  thus the upper and lower bounds match within o((log T )2). When m(S) > log2 log2(T /k) 
the upper bound is O(
T log T )  thus the upper and lower bounds directly match within O(log T ).

4.3 Phase Transitions and Cyclic Phenomena

S(T ). To illustrate this trade-off  Table 1 depicts the behavior of R∗

Corollary 1 allows us to characterize the trade-off between the switching budget S and the optimal
regret R∗
S(T ) as a function of
S given a ﬁxed k. Note that as discussed in Section 2  the relationship between R∗
S(T ) and S also
characterizes the inherent trade-off between regret and maximum number of switches in the classical
MAB problem.

Table 1: Regret as a Function of Switching Budget

S
R∗
S(T )
R∗
S(T )/R∗

∞(T )

[0  k)
˜Θ(T )

˜Θ(T 1/2)

[k  2k − 1)
˜Θ(T 2/3)
˜Θ(T 1/6)

[2k − 1  3k − 2)

[3k − 2  4k − 3)

[4k − 3  5k − 4)

˜Θ(T 4/7)
˜Θ(T 1/14)

˜Θ(T 8/15)
˜Θ(T 1/30)

˜Θ(T 16/31)
˜Θ(T 1/62)

As we have shown  R∗
S(T ) = ˜Θ(T 1/(2−2−(cid:98)(S−1)/(k−1)(cid:99))). To the best of knowledge  this is the ﬁrst
time that a ﬂoor function naturally arises in the order of T in the optimal regret of an online learning
problem. As a direct consequence of this ﬂoor function  we discover several surprising phenomena
regarding the trade-off between S and R∗

S(T ) for any given k.

Deﬁnition 2 (Phases and Transition Points) For a k-armed unit-switching-cost BwSC  we call the
interval [(j−1)(k−1)+1  j(k−1)+1) the j-th phase  and call j(k−1)+1 the j-th transition point
(j ∈ Z>0).
Fact 1 (Phase Transitions) As S increases from 0 to Θ(log log T )  S will leave the j-th phase and
enter the (j + 1)-th phase at the j-th transition point (j ∈ Z>0). Each time S arrives at a transition
point  R∗
S(T ) will drop signiﬁcantly  and stay at the same level until S arrives the next transition
point.
Fact 2 (Cyclic Phenomena) The length of each phase is always equal to k − 1  independent of S
and T . We call the quantity k − 1 the budget cycle  which is the length of each phase.

Phase transitions are clearly presented in Table 1. This phenomenon seems counter-intuitive  as
it suggests that increasing switching budget would not help to decrease the best achievable regret 
as long as the budget does not reach the next transition point. Note that phase transitions are only
exhibited when S is in the range of 0 to Θ(log log T ). After S exceeds Θ(log log T )  R∗
√
S(T ) will
reamin unchanged at the level of ˜Θ(
T ) — the optimal regret will only vary within logarithmic
factors and there is no signiﬁcant regret drop any more. Therefore  one can also view Θ(log log T ) as
a “ﬁnal transition point” that marks the disappearance of phase transitions.
Cyclic Phenomena indicate that  assuming that the learner’s switching budget is at a transition point 
then the extra switching budget that the learner needs to achieve the next regret drop (i.e.  to arrive at
the next transition point) is always k − 1. Cyclic phenomena also seem counter-intuitive: when the
learner has more switching budget  she can conduct more statistical tests  eliminate more bad actions
(which can be thought of as reducing k) and allocate her switching budget in a more ﬂexible way —
all of these suggest that the budget cycle should be a quantity decreasing with S. However  the cyclic
phenomena tell us that the budget cycle is always a constant and no learning policy in the unit-cost
BwSC (and in MAB) can escape this cycle  no matter how large S is   as long as S = o(log log T ).
On the other hand  as S contains more and more budget cycles  the gap between R∗
√
S(T ) and
R∗
∞(T ) = ˜Θ(
S(T ) decreases doubly exponentially fast
as S contains more budget cycles. From Table 1  we can verify that 3 or 4 budget cycles are already
enough for an S-switching-budget policy to achieve close-to-optimal regret in MAB (compared with
the optimal policy with unlimited switching budget).

T ) does decrease dramatically. In fact  R∗

7

Finally  we give some comments on the scope of our results. Note that phase transitions and cyclic
phenomena are associated with theoretical bounds of the worst-case regret  so if (1) the underlying
distributions are not the worst-case distributions and we are focusing on the “actual incurred regret” 
or (2) T is not large enough to dominate the constants in the bounds  phase transitions and cyclic
phenemona may not be exhibited.

5 General Switching Costs

We now proceed to the general case of BwSC  where ci j (= cj i) can be any non-negative real number
and even ∞. The problem is signiﬁcantly more challenging in this general setting. For this purpose 
we need to enhance the framework of Section 2 to better characterize the structure of switching costs.
We do this by representing switching costs via a weighted graph.
Let G = (V  E) be a (weighted) complete graph  where V = [k] (i.e.  each vertex corresponds to an
action)  and the edge between i and j is assigned a weight ci j (∀i (cid:54)= j). We call the weighted graph
G the switching graph. In this paper  we assume the switching costs satisfy the triangle inequality:
∀i  j  l ∈ [k]  ci j ≤ ci l + cl j.
The results of the unit-switching-cost model suggest that an effective policy that minimizes the
worst-case regret must repeatedly visit all actions  in a manner similar to the SS-SE policy. This
indicates that in the general-switching-cost model  an effective policy should repeatedly visit all
vertices in the switching graph  in a most economical way to stay within budget. Motivated by this
idea  we propose the Hamiltonian-Switching Successive Elimination (HS-SE) policy  and present
its details in Algorithm 2. The HS-SE policy enhances the original SS-SE policy by adding two
additional ingredients: (1) a pre-speciﬁed switching order: within each interval  the HS-SE policy
switches based on an order determined by the shortest Hamiltonian path of the switching graph G;
(2) a reversing policy: the HS-SE policy switches along one direction in the odd intervals  and along
the reverse direction in the even intervals. Note that while the shortest Hamiltonian path problem is
NP-hard  solving this problem is entirely an “ofﬂine” step in the HS-SE policy. That is  for a given
switching graph  the learner only needs to solve this problem once.
Let H denote the total weight of the shortest Hamiltonian path of G. We give an upper bound on the
regret of the HS-SE policy in Theorem 3.
Theorem 3 Let π be the HS-SE policy  then π ∈ ΠS. There exists an absolute constant C ≥ 0 such
that for all G  k = |G|  S ≥ 0  T ≥ k 

Rπ(T ) ≤ C(log k log T )k

(cid:106) S−maxi j∈[k] ci j

(cid:107)

H

.

where mU

G(S) =

1−

1

−mU
G

2−2

(S) T

2−2

1

−mU
G

(S)  

We then give a lower bound that is close to the above upper bound  see Theorem 4. Compared to
the proof of Theorem 2 (see Section 4.2 for a proof sketch)  we would like to highlight an important
new step in the proof of Theorem 4. Recall that in the proof sketch of Theorem 2  we mention a
step of constructing the “primal environment” α. In the proof of Theorem 4  our construction of α
ensures that α has an additional property: (arg maxi∈[k] minj(cid:54)=i ci j) is the optimal action in α. This
property makes (maxi∈[k] minj(cid:54)=i ci j) a lower bound on the cost incurred by switching between a
sub-optimal action and the optimal action in α. Our new proof utilizes this property and makes the
quantity (maxi∈[k] minj(cid:54)=i ci j) appear in the lower bound. For the complete proof of Theorem 4 
see the full version of this paper [14].
Theorem 4 There exists an absolute constant C > 0 such that for all G  k = |G|  S ≥ 0  T ≥ k
and for all policy π ∈ ΠS 

1

−mL
G

2−2

T

(S)  

if mL

if mL

G(S) ≤ log2 log2(T /k) 
G(S) > log2 log2(T /k) 

(cid:18)

k

√

C

C

Rπ(T ) ≥

− 3

2−

1

−mL
G

2−2

(S) (mG(S) + 1)−2

kT  

(cid:106) S−maxi∈[k] minj(cid:54)=i ci j

(cid:107)

.

H

where mL

G(S) =

(cid:19)

8

(cid:107)

.

H

G(S).

G(S) =

G(S) do

Algorithm 2 Hamiltonian-Switching Successive Elimination (HS-SE)
Input: Switching Graph G  Switching budget S  Horizon T
Ofﬂine Step: Find the shortest Hamiltonian path in G: i1 → ··· → ik. Denote the total weight of
the shortest Hamiltonian path as H. Calculate mU
Partition: Run the partition step in the SS-SE policy with m(S) = mU
Initialization: Let the set of all active actions in the l-th interval be Al. Set A1 = [k]  a0 = i1.
Policy:
1: for l = 1  . . .   mU
2:
3:

(cid:106) S−maxi j∈[k] ci j

if atl−1 ∈ Al and l is odd then

Let atl−1+1 = atl−1. Starting from this action  along the direction of i1 → ··· → ik 
choose each action in Al for tl−tl−1
consecutive rounds. Mark the last chosen action as atl.
|Al|
else if atl−1 ∈ Al and l is even then
Let atl−1+1 = atl−1. Starting from this action  along the direction of ik → ··· → i1 
choose each action in Al for tl−tl−1
consecutive rounds. Mark the last chosen action as atl.
|Al|
else if atl−1 /∈ Al and l is odd then
Along the direction of i1 → ··· → ik  ﬁnd the ﬁrst action that still remains in Al. Starting
from this action  along the direction of i1 → ··· → ik  choose each action in Al for tl−tl−1
|Al|
consecutive rounds. Mark the last chosen action as atl.
Along the direction of ik → ··· → i1  ﬁnd the ﬁrst action that still remains in Al. Starting
from this action  along the direction of ik → ··· → i1  choose each action in Al for tl−tl−1
|Al|
consecutive rounds. Mark the last chosen action as atl.

else if atl−1 /∈ Al and l is even then

end if
Statistical test: deactivate all actions i s.t. ∃ action j with UCBtl (i) < LCBtl(j)  where

4:
5:

6:
7:

8:
9:

10:
11:

UCBtl (i) = empirical mean of action i in[1 : tl] +

LCBtl (i) = empirical mean of action i in[1 : tl] −

2 log T

number of plays of action i in[1 : tl]

2 log T

number of plays of action i in[1 : tl]

 

.

12: end for
13: In the last interval  choose the action with the highest empirical mean (up to round tmU

G(S)).

(cid:115)
(cid:115)

Finally  we illustrate how tight the above upper and lower bounds are. When the switching costs
satisfy the condition maxi j∈[k] ci j = maxi∈[k] minj(cid:54)=i ci j  the two bounds directly match. When
this condition is not satisﬁed  for any switching graph G  the above two bounds still match for a wide

range of S:(cid:20)

(cid:19)(cid:91)(cid:40) ∞(cid:91)

(cid:20)

n=1

0  H + max
i∈[k]

min
j(cid:54)=i

ci j

ci j  (n + 1)H + max
i∈[k]

min
j(cid:54)=i

ci j

.

nH + max
i j∈[k]
G(S) ≤ mL

G(S) ≤ mU

Even when S is not in this range  we still have mU
G(S) + 1 for any G and any
S  which means that the difference between the two indices is at most 1 and the regret bounds are
always very close. In fact  it can be shown that as S increases  the gap between the upper and lower
bounds decreases doubly exponentially. Therefore  the HS-SE policy is quite effective for the general
BwSC problem.
For additional theoretical results for the general BwSC problem  see the full version of this paper [14].

(cid:19)(cid:41)

References
[1] R. Agrawal  M. Hedge  and D. Teneketzis. Asymptotically efﬁcient adaptive allocation rules for
the multiarmed bandit problem with switching cost. IEEE Transactions on Automatic Control 
33(10):899–906  1988.

9

[2] R. Agrawal  M. Hegde  and D. Teneketzis. Multi-armed bandit problems with multiple plays

and switching cost. Stochastics and Stochastic Reports  29(4):437–459  1990.

[3] J. Altschuler and K. Talwar. Online learning over a ﬁnite action set with limited switching. In

Conference on Learning Theory  pages 1569–1573  2018.

[4] M. Asawa and D. Teneketzis. Multi-armed bandits with switching penalties. IEEE Transactions

on Automatic Control  41(3):328–348  1996.

[5] J. S. Banks and R. K. Sundaram. Switching costs and the gittins index. Econometrica  62(3):

687–694  1994.

[6] D. Bergemann and J. Välimäki. Stationary multi-choice bandit problems. Journal of Economic

dynamics and Control  25(10):1585–1594  2001.

[7] M. Brezzi and T. L. Lai. Optimal learning and experimentation in bandit problems. Journal of

Economic Dynamics and Control  27(1):87–108  2002.

[8] N. Cesa-Bianchi  O. Dekel  and O. Shamir. Online learning with switching costs and other
adaptive adversaries. In Advances in Neural Information Processing Systems  pages 1160–1168 
2013.

[9] B. Chen and X. Chao. Parametric demand learning with limited price explorations in a backlog

stochastic inventory system. IISE Transactions  pages 1–9  2019.

[10] W. C. Cheung  D. Simchi-Levi  and H. Wang. Dynamic pricing and demand learning with

limited price experimentation. Operations Research  65(6):1722–1731  2017.

[11] Z. Gao  Y. Han  Z. Ren  and Z. Zhou. Batched multi-armed bandits problem. arXiv preprint

arXiv:1904.01763  2019.

[12] T. Jun. A survey on the bandit problem with switching costs. De Economist  152(4):513–541 

2004.

[13] V. Perchet  P. Rigollet  S. Chassang  and E. Snowberg. Batched bandit problems. The Annals of

Statistics  44(2):660–681  2016.

[14] D. Simchi-Levi and Y. Xu. Phase transitions and cyclic phenomena in bandits with switching
constraints. arXiv preprint arXiv:1905.10825  2019. URL https://arxiv.org/abs/1905.
10825.

10

,David Simchi-Levi
Yunzong Xu