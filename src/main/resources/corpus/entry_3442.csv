2008,Mind the Duality Gap: Logarithmic regret algorithms for online optimization,We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in HazanKaKaAg06. We then show that one can interpolate between these two extreme cases. In particular  we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally  we further extend our framework for generalized strongly convex functions.,Mind the Duality Gap:

Logarithmic regret algorithms for online optimization

Toyota Technological Institute at Chicago

Toyota Technological Institute at Chicago

Shai Shalev-Shwartz

shai@tti-c.org

Sham M. Kakade

sham@tti-c.org

Abstract

We describe a primal-dual framework for the design and analysis of online
strongly convex optimization algorithms. Our framework yields the tightest
known logarithmic regret bounds for Follow-The-Leader and for the gradient de-
scent algorithm proposed in Hazan et al. [2006]. We then show that one can inter-
polate between these two extreme cases. In particular  we derive a new algorithm
that shares the computational simplicity of gradient descent but achieves lower
regret in many practical situations. Finally  we further extend our framework for
generalized strongly convex functions.

1

Introduction

In recent years  online regret minimizing algorithms have become widely used and empirically suc-
cessful algorithms for many machine learning problems. Notable examples include efﬁcient learn-
ing algorithms for structured prediction and ranking problems [Collins  2002  Crammer et al.  2006].
Most of these empirically successful algorithms are based on algorithms which are tailored to gen-
eral convex functions  whose regret is O(√T ). Rather recently  there is a growing body of work
providing online algorithms for strongly convex loss functions  with regret guarantees that are only
O(log T ). These algorithms have potential to be highly applicable since many machine learning
optimization problems are in fact strongly convex — either with strongly convex loss functions (e.g.
log loss  square loss) or  indirectly  via strongly convex regularizers (e.g. L2 or KL based regular-
ization). Note that in this later case  the loss function itself may only be just convex but a strongly
convex regularizer effectively makes this a strongly convex optimization problem (e.g. the SVM op-
timization problem uses the hinge loss with L2 regularization). The aim of this paper is to provide
a template for deriving a wider class of regret-minimizing algorithms for online strongly convex
programming.
Online convex optimization takes place in a sequence of consecutive rounds. At each round  the
learner predicts a vector wt ∈ S ⊂ Rn  and the environment responds with a convex loss function 
!t : S → R. The goal of the learner is to minimize the difference between his cumulative loss and
the cumulative loss of the optimal ﬁxed vector !T
t=1 !t(w). This is termed
‘regret’ since it measures how ‘sorry’ the learner is  in retrospect  not to have predicted the optimal
vector.
Roughly speaking  the family of regret minimizing algorithms (for general convex functions) can be
seen as varying on two axes  the ‘style’ and the ‘aggressiveness’ of the update. In addition to online
algorithms’ relative simplicity  the empirical successes are also due to having these two knobs to tune
for the problem at hand (which determine the nature of the regret bound). By style  we mean updates
which favor either rotational invariance (such as gradient descent like update rules) or sparsity (like
the multiplicative updates). Of course there is a much richer family here  including the Lp updates.
By the aggressiveness of the update  we mean how much the algorithm moves its decision to be
consistent with most recent loss functions. For example  the preceptron algorithm makes no update

t=1 !t(wt)−minw∈S!T

1

when there is no error. In contrast  there is a family of algorithms which more aggressively update
the loss when there is a margin mistake. These algorithms are shown to have improved performance
(see for example the experimental study in Shalev-Shwartz and Singer [2007b]).
While historically much of the analysis of these algorithms have been done on a case by case basis 
in retrospect  the proof techniques have become somewhat boilerplate  which has lead to growing
body of work to unify these analyses (see Cesa-Bianchi and Lugosi [2006] for review). Perhaps the
most uniﬁed view of these algorithms is the ‘primal-dual’ framework of Shalev-Shwartz and Singer
[2006]  Shalev-Shwartz [2007]  for which the gamut of these algorithms can be largely viewed as
special cases. Two aspects are central in providing this uniﬁcation. First  the framework works with
a complexity function  which determines the style of algorithm and the nature of the regret guarantee
(If this function is the L2 norm  then one obtains gradient like updates  and if this function is the KL-
distance  then one obtains multiplicative updates). Second  the algorithm maintains both “primal”
and “dual” variables. Here  the the primal objective function is!T
t=1 !t(w) (where !t is the loss
function provided at round t)  and one can construct a dual objective function Dt(·)  which only
depends on the loss functions !1 ! 2  . . .! t−1. The algorithm works by incrementally increasing the
dual objective value (in an online manner)  which can be done since each Dt is only a function of the
previous loss functions. By weak duality  this can be seen as decreasing the duality gap. The level
of aggressiveness is seen to be how fast the algorithm is attempting to increase the dual objective
value.
This paper focuses on extending the duality framework for online convex programming to the case
of strongly convex functions. This analysis provides a more uniﬁed and intuitive view of the extant
algorithms for online strongly convex programming. An important observation we make is that any
σ-strongly convex loss function can be rewritten as !i(w) = f(w) + gi(w)  where f is a ﬁxed σ-
strongly convex function (i.e. f does not depend on i)  and gi is a convex function. Therefore  after t
i=1 !t(w)
σt proposed in the gradient descent
is at least σt. In particular  this explains the learning rate of
algorithm of Hazan et al. [2006]. Indeed  we show that our framework includes the gradient descent
algorithm of Hazan et al. [2006] as an important special case  in which the aggressiveness level
is minimal. At the most aggressive end  our framework yields the Follow-The-Leader algorithm.
Furthermore  the template algorithm serves as a vehicle for deriving new algorithms (which enjoy
logarithmic regret guarantees).
The remainder of the paper is outlined as follows. We ﬁrst provide background on convex duality. As
a warmup  in Section 3  we present an intuitive primal-dual analysis of Follow-The-Leader (FTL) 
when f is the Euclidean norm. This naturally leads to a more general primal-dual algorithm (for
which FTL is a special case)  which we present in Section 4. Next  we further generalize our
algorithmic framework to include strongly convex complexity functions f with respect to arbitrary
norms &·& . We note that the introduction of a complexity function was already provided in Shalev-
Shwartz and Singer [2007a]  but the analysis is rather specialized and does not have a knob which
can tune the aggressiveness of the algorithm. Finally  in Sec. 6 we conclude with a side-by-side
comparison of our algorithmic framework for strongly convex functions and the framework for
(non-strongly) convex functions given in Shalev-Shwartz [2007].

online rounds  the amount of intrinsic strong convexity we have in the primal objective!t

1

2 Mathematical Background

We denote scalars with lower case letters (e.g. w and λ)  and vectors with bold face letters (e.g. w
and λ). The inner product between vectors x and w is denoted by ’x  w(. To simplify our notation 
given a sequence of vectors λ1  . . .   λt or a sequence of scalars σ1  . . .  σ t we use the shorthand

λ1:t =

λi

and

σ1:t =

t"i=1

σi .

t"i=1

Sets are designated by upper case letters (e.g. S). The set of non-negative real numbers is denoted
by R+. For any k ≥ 1  the set of integers {1  . . .   k} is denoted by [k]. A norm of a vector x is
denoted by &x&. The dual norm is deﬁned as &λ&" = sup{’x  λ( : &x& ≤ 1}. For example  the
Euclidean norm  &x&2 = (’x  x()1/2 is dual to itself and the L1 norm  &x&1 =!i |xi|  is dual to
the L∞ norm  &x&∞ = maxi |xi|.

2

FOR t = 1  2  . . .   T :

λt

σ1:(t−1)

Deﬁne wt = − 1
1:(t−1)
Receive a function !t(w) = σt
Update λt+1
(λt+1

  . . .   λt+1
  . . .   λt+1

1

t

t

1

2 "w"2 + gt(w) and suffer loss !t(wt)

s.t. the following holds
) ∈ argmax

λ1 ... λt Dt+1(λ1  . . .   λt)

Figure 1: A primal-dual view of Follow-the-Leader. Here the algorithm’s decision wt is the best decision
with respect to the previous losses. This presentation exposes the implicit role of the dual variables. Slightly
abusing notation  λ1:0 = 0  so that w1 = 0. See text.

We next recall a few deﬁnitions from convex analysis. A function f is σ-strongly convex if

f(αu + (1 − α)v) ≤ αf(u) + (1 − α)f(v) −

σ
2 α (1 − α)&u − v&2
2 .

In Sec. 5 we generalize the above deﬁnition to arbitrary norms. If a function f is σ-strongly convex
then the function g(w) = f(w) − σ
The Fenchel conjugate of a function f : S → R is deﬁned as

2&w&2 is convex.

f "(θ) = sup

w∈S ’w  θ( −f (w) .

If f is closed and convex  then the Fenchel conjugate of f " is f itself (a function is closed if for
all α> 0 the level set {w : f(w) ≤ α} is a closed set). It is straightforward to verify that the
function f(w) is conjugate to itself. The deﬁnition of f " also implies that for c >0 we have
(c f)"(θ) = c f "(θ/c).
A vector λ is a sub-gradient of a function f at w if for all w$ ∈ S  we have that f(w$) − f(w) ≥
’w$ − w  λ(. The differential set of f at w  denoted ∂f (w)  is the set of all sub-gradients of f at
w. If f is differentiable at w  then ∂f (w) consists of a single vector which amounts to the gradient
of f at w and is denoted by ∇f(w).
The Fenchel-Young inequality states that for any w and θ we have that f(w) + f "(θ) ≥ ’w  θ(.
Sub-gradients play an important role in the deﬁnition of the Fenchel conjugate. In particular  the
following lemma  whose proof can be found in Borwein and Lewis [2006]  states that if λ ∈ ∂f (w)
then the Fenchel-Young inequality holds with equality.
for all λ$ ∈ ∂f (w$)  we have f(w$) +f "(λ$) = #λ$  w$$ .
t (λt) ≤ min
g"

Lemma 1 Let f be a closed and convex function and let ∂f (w$) be its differential set at w$. Then 

We make use of the following variant of Fenchel duality (see the appendix for more details):

λ1 ... λT −f "(−
max

λt) −

f(w) +

gt(w) .

(1)

w

T"t=1

T"t=1

T"t=1

3 Warmup: A Primal-Dual View of Follow-The-Leader

In this section  we provide a dual analysis for the FTL algorithm. The dual view of FTL will help
us to derive a family of logarithmic regret algorithms for online convex optimization with strongly
convex functions.
Recall that FTL algorithm is deﬁned as follows:

For each i ∈ [t − 1] deﬁne gi(w) = !i(w) − σi
2 &w&2  where σi is the largest scalar such that gi is
still a convex function. The assumption that !i is σ-strongly convex guarantees that σi ≥ σ. We can

!i(w) .

(2)

wt = argmin

w

t−1"i=1

3

therefore rewrite the objective function on the right-hand side of Eq. (2) as

Pt(w) = σ1:(t−1)

2

&w&2 +

t−1"i=1

gi(w)  

(3)

i=1 σi). The Fenchel dual optimization problem (see Sec. 2) is to maximize

(recall that σ1:(t−1) =!t−1

the following dual objective function

Dt(λ1  . . .   λt−1) = −

1

2 σ1:(t−1)&λ1:(t−1)&2 −

t−1"i=1

i (λi) .
g"

(4)

1  . . .   λt

Let (λt
the optimal primal vector is given by (see again Sec. 2)

t−1) be the maximizer of Dt. The relation between the optimal dual variables and

wt = −

1

σ1:(t−1)

λt
1:(t−1) .

(5)

Throughout this section we assume that strong duality holds (i.e. Eq. (1) holds with equality). See
the appendix for sufﬁcient conditions. In particular  under this assumption  we have that the above
setting for wt is in fact a minimizer of the primal objective  since (λt
t−1) maximizes the dual
objective (see the appendix). The primal-dual view of Follow-the-Leader is presented in Figure 1.
Denote

1  . . .   λt

To analyze the FTL algorithm  we ﬁrst note that (by strong duality)

∆t = Dt+1(λt+1

1

  . . .   λt+1

t

) −D t(λt

1  . . .   λt

t−1) .

(6)

  . . .   λT +1

T

) = min

w PT +1(w) = min

w

!t(w) .

(7)

T"t=1

T"t=1

1

∆t = DT +1(λT +1
  . . .   λt+1
∆t ≥D t+1(λt

1

t

Second  the fact that (λt+1

) is the maximizer of Dt+1 implies that for any λ we have
1  . . .   λt

(8)
The following central lemma shows that there exists λ such that the right-hand side of the above is
sufﬁciently large.
Lemma 2 Let (λ1  . . .   λt−1) be an arbitrary sequence of vectors. Denote w = − 1
let v ∈ ∂!t(w)  and let λ = v − σtw. Then  λ ∈ ∂gt(w) and

t−1  λ) −D t(λt

λ1:(t−1) 

1  . . .   λt

t−1) .

σ1:(t−1)

Dt+1(λ1  . . .   λt−1  λ) −D t(λ1  . . .   λt−1) = !t(w) − &v&2
2 σ1:t

.

Proof We prove the lemma for the case t >1. The case
larly. Since !t(w) = σt
¯∆t = Dt+1(λ1  . . .   λt−1  λ) −D t(λ1  . . .   λt−1). Simple algebraic manipulations yield

t = 1 can be proved simi-
2 &w&2 + gt(w) and v ∈ ∂!t(w) we have that λ ∈ ∂gt(w). Denote
1

1

¯∆t = −

1

σt

2σ1:(t−1)%%λ1:(t−1)%%2 − g"
t (λ)
σ1:t’ + ’w  λ(
σ1:t − &λ&2
σ1:(t−1)
2σ1:t − g"
σ1:t − &λ&2
σ1:(t−1)
t (λ)
2σ1:t − g"
+ σt ’w  λ(
t &w&2
+ &λ&2
2σ1:t

2σ1:t%%λ1:(t−1) + λ%%2 +
& 1
= &λ1:(t−1)&2
σ1:(t−1) −
&1 −
σ1:t’ + ’w  λ(
−& σ2
2σ1:t’
+ ’w  λ( −g "
+
)*
+
(
. We have thus shown that ¯∆t = !t(w) − %σtw+λ%2

2
= σt&w&2
= σt &w&2
(

)*

t (λ)

σ1:t

2σ1:t

2

2

B

A

t (λ)

Since λ ∈ ∂gt(w)  Lemma 1 thus implies that ’w  λ( − g"
Next  we note that B = %σtw+λ%2
the deﬁnition of λ into the above we conclude our proof.

2σ1:t

t (λ) = gt(w). Therefore  A = !t(w).
. Plugging

Combining Lemma 2 with Eq. (7) and Eq. (8) we obtain the following:

4

FOR t = 1  2  . . .   T :

λt

Deﬁne wt = − 1
1:(t−1)
Receive a function !t(w) = σt
Update λt+1

  . . .   λt+1

σ1:(t−1)

t

1

2 "w"2 + gt(w) and suffer loss !t(wt)

s.t. the following holds

∃λt ∈ ∂gt(wt)  s.t. Dt+1(λt+1

1

  . . .   λt+1

t

) ≥D t+1(λt

1  . . .   λt

t−1  λt)

Figure 2: A primal-dual algorithmic framework for online convex optimization. Again  w1 = 0.

Corollary 1 Let !1  . . .  ! T be a sequence of functions such that for all t ∈ [T ]  !t is σt-strongly
convex. Assume that the FTL algorithm runs on this sequence and for each t ∈ [T ]  let vt be in
∂!t(wt). Then 

T"t=1

!t(wt) − min

w

!t(w) ≤

1
2

&vt&2
σ1:t

(9)

T"t=1

T"t=1

2σ (log(T ) + 1).

Furthermore  let L = maxt &vt& and assume that for all t ∈ [T ]  σt ≥ σ. Then  the regret is
bounded by L2
If we are dealing with the square loss !t(w) =&w − µt&2
2 (where nature chooses µt)  then it is
straightforward to see that Eq. (8) holds with equality  and this leads to the previous regret bound
holding with equality. This equality is the underlying reason that the FTL strategy is a minimax
strategy (See Abernethy et al. [2008] for a proof of this claim).

4 A Primal-Dual Algorithm for Online Strongly Convex Optimization

In the previous section  we provided a dual analysis for FTL. Here  we extend the analysis and derive
a more general algorithmic framework for online optimization.
We start by examining the analysis of the FTL algorithm. We ﬁrst make the important observation
that Lemma 2 is not speciﬁc to the FTL algorithm and in fact holds for any conﬁguration of dual
variables. Consider an arbitrary sequence of dual variables: (λ2
)
  . . .   λT +1
and denote ∆t as in Eq. (6). Using weak duality  we can replace the equality in Eq. (7) with the
following inequality that holds for any sequence of dual variables:

2)  . . .   (λT +1

1)  (λ3

1  λ3

T

1

∆t = DT +1(λT +1

1

  . . .   λT +1

T

) ≤ min

w PT +1(w) = min

w

!t(w) .

(10)

T"t=1

T"t=1

A summary of the algorithmic framework is given in Fig. 2.
The following theorem  a direct corollary of the previous equation and Lemma 2  shows that all
instances of the framework achieve logarithmic regret.
Theorem 1 Let !1  . . .  ! T be a sequence of functions such that for all t ∈ [T ]  !t is σt-strongly
convex. Then  any algorithm that can be derived from Fig. 2 satisﬁes

T"t=1

!t(wt) − min

w

!t(w) ≤

1
2

&vt&2
σ1:t

(11)

T"t=1

T"t=1

where vt ∈ ∂!t(wt).
Proof Let ∆t be as deﬁned in Eq. (6). The last condition in the algorithm implies that

t−1) .
The proof follows directly by combining the above with Eq. (10) and Lemma 2.

t−1  vt − σtwt) −D t(λt

∆t ≥D t+1(λt

1  . . .   λt

1  . . .   λt

We conclude this section by deriving several algorithms from the framework.

5

Example 1 (Follow-The-Leader) As we have shown in Sec. 3  the FTL algorithm (Fig. 1) is equiva-
lent to optimizing the dual variables at each online round. This update clearly satisﬁes the condition
in Fig. 2 and is therefore a special case.

Example 2 (Gradient-Descent) Following Hazan et al. [2006]  Bartlett et al. [2007] suggested the
following update rule for differentiable strongly convex function
1
σ1:t∇!t(wt) .

wt+1 = wt −

(12)

Using a simple inductive argument  it is possible to show that the above update rule is equivalent to
the following update rule of the dual variables
) = (λt

(13)
Clearly  this update rule satisﬁes the condition in Fig. 2. Therefore our framework encompasses this
algorithm as a special case.

t−1 ∇!t(wt) − σtwt) .

  . . .   λt+1

1  . . .   λt

(λt+1

1

t

Example 3 (Online Coordinate-Dual-Ascent) The FTL and the Gradient-Descent updates are
two extreme cases of our algorithmic framework. The former makes the largest possible increase of
the dual while the latter makes the smallest possible increase that still satisﬁes the sufﬁcient dual
increase requirement. Intuitively  the FTL method should have smaller regret as it consumes more
of its potential earlier in the optimization process. However  its computational complexity is large
as it requires a full blown optimization procedure at each online round. A possible compromise is
to fully optimize the dual objective but only with respect to a small number of dual variables. In the
extreme case  we optimize only with respect to the last dual variable. Formally  we let

λt+1

i

=  λt

i

argmax

λt Dt+1(λt

1  . . .   λt

t−1  λt)

if i < t
if i = t

Clearly  the above update satisﬁes the requirement in Fig. 2 and therefore enjoys a logarithmic regret
bound. The computational complexity of performing this update is often small as we optimize over
a single dual vector. Occasionally  it is possible to obtain a closed-form solution of the optimization
problem and in these cases the computational complexity of the coordinate-dual-ascent update is
identical to that of the gradient-descent method.

5 Generalized strongly convex functions

In this section  we extend our algorithmic framework to deal with generalized strongly convex func-
tions. We ﬁrst need the following generalized deﬁnition of strong convexity.

Deﬁnition 1 A continuous function f is σ-strongly convex over a convex set S with respect to a
norm &·& if S is contained in the domain of f and for all v  u ∈ S and α ∈ [0  1] we have

q is strongly convex over S = Rn with
2(p−1)&θ&2

p  where p = (1 − 1/q)−1.

For proofs  see for example Shalev-Shwartz [2007].
In the appendix  we list several important
properties of strongly convex functions. In particular  the Fenchel conjugate of a strongly convex
function is differentiable.

6

f(α v + (1 − α) u) ≤ αf (v) + (1 − α) f(u) −
2&w&2

It is straightforward to show that the function f(w) = 1
Euclidean norm. Less trivial examples are given below.

i=1 wi log(wi/ 1

Example 4 The function f(w) =!n
plex  S = {w ∈ Rn
n!n
f "(θ) = log( 1
Example 5 For q ∈ (1  2)  the function f(w) = 1
2(q−1)&w&2
respect to the Lq norm. Its conjugate function is f "(θ) = 1

i=1 exp(θi)).

+ : &w&1 = 1}  with respect to the L1 norm.

σ
2 α (1 − α)&v − u&2 .

(14)

2 is strongly convex with respect to the

n) is strongly convex over the probability sim-
Its conjugate function is

INPUT: A strongly convex function f
FOR t = 1  2  . . .   T :

1) Deﬁne wt = ∇f "„−

λt

1:(t−1)√t «

2) Receive a function !t
3) Suffer loss !t(wt)
4) Update λt+1

  . . .   λt+1
exists λt ∈ ∂lt(wt) with

1

t

s.t. there

1

Dt+1(λt+1
Dt+1(λt

  . . .   λt+1
t
1  . . .   λt

) ≥
t−1  λt)

INPUT: A σ-strongly convex function f
FOR t = 1  2  . . .   T :

1) Deﬁne wt = ∇f "„−

λt

1:(t−1)

σ1:t «

2) Receive a function !t = σf + gt
3) Suffer loss !t(wt)
4) Update λt+1

  . . .   λt+1
exists λt ∈ ∂gt(wt) with

s.t. there

1

t

1

Dt+1(λt+1
Dt+1(λt

  . . .   λt+1
t
1  . . .   λt

) ≥
t−1  λt)

Figure 3: Primal-dual template algorithms for general online convex optimization (left) and online strongly
i=1 ai  and for notational convenient  we implicitly assume that

convex optimization (right). Here a1:t = Pt

a1:0 = 0. See text for description.

Consider the case where for all t  !t can be written as σtf + gt where f is 1-strongly convex with
respect to some norm &·& and gt is a convex function. We also make the simplifying assumption
that σt is known to the forecaster before he deﬁnes wt.
For each round t  we now deﬁne the primal objective to be

The dual objective is (see again Sec. 2)

gi(w) .

Pt(w) = σ1:(t−1)f(w) +

t−1"i=1
σ1:(t−1). −
Dt(λ1  . . .   λt−1) = − σ1:(t−1)f "-− λ1:(t−1)

i (λi) .
g"

t−1"i=1

(15)

(16)

An algorithmic framework for online optimization in the presence of general strongly convex func-
tions is given on the right-hand side of Fig. 3.
The following theorem provides a logarithmic regret bound for the algorithmic framework given on
the right-hand side of Fig. 3.
Theorem 2 Let !1  . . .  ! T be a sequence of functions such that for all t ∈ [T ]  !t = σtf + gt for f
being strongly convex w.r.t. a norm & ·& and gt is a convex function. Then  any algorithm that can
be derived from Fig. 3 (right) satisﬁes

T"t=1

!t(wt) − min

!t(w) ≤
where vt ∈ ∂gt(wt) and &·& " is the norm dual to &·& .
The proof of the theorem is given in Sec. B

w

T"t=1

&vt&2
σ1:t

"

 

1
2

T"t=1

(17)

6 Summary

In this paper  we extended the primal-dual algorithmic framework for general convex functions from
Shalev-Shwartz and Singer [2006]  Shalev-Shwartz [2007] to strongly convex functions. The tem-
plate algorithms are outlined in Fig. 3. The left algorithm is the primal-dual algorithm for general
convex functions from Shalev-Shwartz and Singer [2006]  Shalev-Shwartz [2007]. Here  f is the
complexity function  (λt
t) are the dual variables at time t  and Dt(·) is the dual objective

1  . . .   λt

7

function at time t (which is a lower bound on primal value). The function ∇f " is the gradient of the
conjugate function of f  which can be viewed as a projection of the dual variables back into the pri-
mal space. At the least aggressive extreme  in order to obtain √T regret  it is sufﬁcient to set λi
t (for
all i) to be a subgradient of the loss ∂!t(wt). We then recover an online ‘mirror descent’ algorithm
[Beck and Teboulle  2003  Grove et al.  2001  Kivinen and Warmuth  1997]  which specializes to
gradient descent when f is the squared 2-norm or the exponentiated gradient descent algorithm when
f is the relative entropy. At the most aggressive extreme  where Dt is maximized at each round  we
i=1 !i(w) + √t f(w). Inter-
have ‘Follow the Regularized Leader’  which is wt = arg minw!t−1
mediate algorithms can also be devised such as the ‘passive-aggressive’ algorithms [Crammer et al. 
2006  Shalev-Shwartz  2007].
The right algorithm in Figure 3 is our new contribution for strongly convex functions. Any σ-
strongly convex loss function can be decomposed into !t = σf + gt  where gt is convex. The
algorithm for strongly convex functions is different in two ways. First  the effective learning rate is
rather than 1√t (see Step 1 in both algorithms). Second  more subtly  the condition on the
now 1
σ1:t
dual variables (in Step 4) is only determined by the subgradient of gt  rather than the subgradient of
!t. At the most aggressive end of the spectrum  where Dt is maximized at each round  we have the
‘Follow the Leader’ (FTL) algorithm: wt = arg minw!t−1
i=1 !i(w). At the least aggressive end 
). Fur-
we have the gradient descent algorithm of Hazan et al. [2006] (which uses learning rate 1
σ1:t
thermore  we provide algorithms which lie in between these two extremes — it is these algorithms
which have the potential for most practical impact.
Empirical observations suggest that algorithms which most aggressively close the duality gap tend
to perform most favorably [Crammer et al.  2006  Shalev-Shwartz and Singer  2007b]. However  at
the FTL extreme  this is often computationally prohibitive to implement (as one must solve a full
blown optimization problem at each round). Our template algorithm suggests a natural compromise 
which is to optimize the dual objective but only with respect to a small number of dual variables
(say the most current dual variable) — we coin this algorithm online coordinate-dual-ascent. In
fact  it is sometimes possible to obtain a closed-form solution of this optimization problem  so that
the computational complexity of the coordinate-dual-ascent update is identical to that of a vanilla
gradient-descent method. This variant update still enjoys a logarithmic regret bound.

References

J. Abernethy  P. Bartlett  A. Rakhlin  and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the Nineteenth Annual

Conference on Computational Learning Theory  2008.

P. L. Bartlett  E. Hazan  and A. Rakhlin. Adaptive online gradient descent. In Advances in Neural Information Processing Systems 21  2007.
A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters  31:167–175  2003.
J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization. Springer  2006.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
M. Collins. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Conference on Empirical Methods in

Natural Language Processing  2002.

K. Crammer  O. Dekel  J. Keshet  S. Shalev-Shwartz  and Y. Singer. Online passive aggressive algorithms. Journal of Machine Learning Research  7:551–585  Mar

2006.

A. J. Grove  N. Littlestone  and D. Schuurmans. General convergence results for linear discriminant updates. Machine Learning  43(3):173–210  2001.
E. Hazan  A. Kalai  S. Kale  and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In Proceedings of the Nineteenth Annual Conference on

Computational Learning Theory  2006.

J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation  132(1):1–64  January 1997.
S. Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis  The Hebrew University  2007.
S. Shalev-Shwartz and Y. Singer. Convex repeated games and Fenchel duality. In Advances in Neural Information Processing Systems 20  2006.
S. Shalev-Shwartz and Y. Singer. Logarithmic regret algorithms for strictly convex repeated games. Technical report  The Hebrew University  2007a. Available at

http://www.cs.huji.ac.il/∼shais.

S. Shalev-Shwartz and Y. Singer. A uniﬁed algorithmic approach for efﬁcient online label ranking. In aistat07  2007b.

8

,Maja Rudolph
Susan Athey
David Blei