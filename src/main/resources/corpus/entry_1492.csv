2015,Differentially private subspace clustering,Subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple ``clusters'' so that data points in a single cluster lie approximately on a low-dimensional linear subspace. It is originally motivated by 3D motion segmentation in computer vision  but has recently been generically applied to a wide range of statistical machine learning problems  which often involves sensitive datasets about human subjects. This raises a dire concern for data privacy. In this work  we build on the framework of ``differential privacy'' and present two provably private subspace clustering algorithms. We demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees; the other one asymptotically preserves differential privacy while having good performance in practice. Along the course of the proof  we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests.,Differentially Private Subspace Clustering

Machine Learning Department  Carnegie Mellon Universty  Pittsburgh  USA

Yining Wang  Yu-Xiang Wang and Aarti Singh
{yiningwa yuxiangw aarti}@cs.cmu.edu

Abstract

Subspace clustering is an unsupervised learning problem that aims at grouping
data points into multiple “clusters” so that data points in a single cluster lie ap-
proximately on a low-dimensional linear subspace. It is originally motivated by
3D motion segmentation in computer vision  but has recently been generically
applied to a wide range of statistical machine learning problems  which often in-
volves sensitive datasets about human subjects. This raises a dire concern for
In this work  we build on the framework of differential privacy
data privacy.
and present two provably private subspace clustering algorithms. We demonstrate
via both theory and experiments that one of the presented methods enjoys formal
privacy and utility guarantees; the other one asymptotically preserves differential
privacy while having good performance in practice. Along the course of the proof 
we also obtain two new provable guarantees for the agnostic subspace clustering
and the graph connectivity problem which might be of independent interests.

1

Introduction

Subspace clustering was originally proposed to solve very speciﬁc computer vision problems having
a union-of-subspace structure in the data  e.g.  motion segmentation under an afﬁne camera model
[11] or face clustering under Lambertian illumination models [15]. As it gains increasing attention
in the statistics and machine learning community  people start to use it as an agnostic learning tool in
social network [5]  movie recommendation [33] and biological datasets [19]. The growing applica-
bility of subspace clustering in these new domains inevitably raises the concern of data privacy  as
many such applications involve dealing with sensitive information. For example  [19] applies sub-
space clustering to identify diseases from personalized medical data and [33] in fact uses subspace
clustering as a effective tool to conduct linkage attacks on individuals in movie rating datasets. Nev-
ertheless  privacy issues in subspace clustering have been less explored in the past literature  with
the only exception of a brief analysis and discussion in [29]. However  the algorithms and analysis
presented in [29] have several notable deﬁciencies. For example  data points are assumed to be inco-
herent and it only protects the differential privacy of any feature of a user rather than the entire user
proﬁle in the database. The latter means it is possible for an attacker to infer with high conﬁdence
whether a particular user is in the database  given sufﬁcient side information.
It is perhaps reasonable why there is little work focusing on private subspace clustering  which
is by all means a challenging task. For example  a negative result in [29] shows that if utility is
measured in terms of exact clustering  then no private subspace clustering algorithm exists when
neighboring databases are allowed to differ on an entire user proﬁle. In addition  state-of-the-art
subspace clustering methods like Sparse Subspace Clustering (SSC  [11]) lack a complete analysis of
its clustering output  thanks to the notorious “graph connectivity” problem [21]. Finally  clustering
could have high global sensitivity even if only cluster centers are released  as depicted in Figure 1.
As a result  general private data releasing schemes like output perturbation [7  8  2] do not apply.
In this work  we present a systematic and principled treatment of differentially private subspace
clustering. To circumvent the negative result in [29]  we use the perturbation of recovered low-

1

dimensional subspace from the ground truth as the utility measure. Our contributions are two-fold.
First  we analyze two efﬁcient algorithms based on the sample-aggregate framework [22] and estab-
lished formal privacy and utility guarantees when data are generated from some stochastic model or
satisfy certain deterministic separation conditions. New results on (non-private) subspace clustering
are obtained along our analysis  including a fully agnostic subspace clustering on well-separated
datasets using stability arguments and exact clustering guarantee for thresholding-based subspace
clustering (TSC  [14]) in the noisy setting. In addition  we employ the exponential mechanism [18]
and propose a novel Gibbs sampler for sampling from this distribution  which involves a novel tweak
in sampling from a matrix Bingham distribution. The method works well in practice and we show it
is closely related to the well-known mixtures of probabilistic PCA model [27].

Related work Subspace clustering can be thought as a generalization of PCA and k-means clus-
tering. The former aims at ﬁnding a single low-dimensional subspace and the latter uses zero-
dimensional subspaces as cluster centers. There has been extensive research on private PCA
[2  4  10] and k-means [2  22  26]. Perhaps the most similar work to ours is [22  4]. [22] applies the
sample-aggregate framework to k-means clustering and [4] employs the exponential mechanism to
recover private principal vectors. In this paper we give non-trivial generalization of both work to the
private subspace clustering setting.
2 Preliminaries
2.1 Notations

For a vector x ∈ Rd  its p-norm is deﬁned as (cid:107)x(cid:107)p = ((cid:80)
is  (cid:107)A(cid:107)2 = σ1(A) and (cid:107)A(cid:107)F = (cid:112)(cid:80)n

i )1/p. If p is not explicitly speciﬁed
then the 2-norm is used. For a matrix A ∈ Rn×m  we use σ1(A) ≥ ··· ≥ σn(A) ≥ 0 to
denote its singular values (assuming without loss of generality that n ≤ m). We use (cid:107) · (cid:107)ξ to
denote matrix norms  with ξ = 2 the matrix spectral norm and ξ = F the Frobenious norm. That
i=1 σi(A)2. For a q-dimensional subspace S ⊆ Rd  we
associate with a basis U ∈ Rd×q  where the q columns in U are orthonormal and S = range(U).
We use Sd
Given x ∈ Rd and S ⊆ Rd  the distance d(x S) is deﬁned as d(x S) = inf y∈S (cid:107)x − y(cid:107)2. If S is
a subspace associated with a basis U  then we have d(x S) = (cid:107)x − PS (x)(cid:107)2 = (cid:107)x − UU(cid:62)x(cid:107)2 
where PS (·) denotes the projection operator onto subspace S. For two subspaces S S(cid:48) of dimension
q  the distance d(S S(cid:48)) is deﬁned as the Frobenious norm of the sin matrix of principal angles; i.e. 

q to denote the set of all q-dimensional subspaces in Rd.

i xp

d(S S(cid:48)) = (cid:107) sin Θ(S S(cid:48))(cid:107)F = (cid:107)UU(cid:62) − U(cid:48)U(cid:48)(cid:62)(cid:107)F  
where U  U(cid:48) are orthonormal basis associated with S and S(cid:48)  respectively.

(1)

2.2 Subspace clustering
Given n data points x1 ···   xn ∈ Rd  the task of subspace clustering is to cluster the data points
into k clusters so that data points within a subspace lie approximately on a low-dimensional sub-
space. Without loss of generality  we assume (cid:107)xi(cid:107)2 ≤ 1 for all i = 1 ···   n. We also use
X = {x1 ···   xn} to denote the dataset and X ∈ Rd×n to denote the data matrix by stacking
all data points in columnwise order. Subspace clustering seeks to ﬁnd k q-dimensional subspaces
ˆC = { ˆS1 ···   ˆSk} so as to minimize the Wasserstein’s distance or distance squared deﬁned as

k(cid:88)

i=1

2

d2( ˆSi S∗

W ( ˆC C∗) = min
d2
π:[k]→[k]

(2)
where π are taken over all permutations on [k] and S∗ are the optimal/ground-truth subspaces. In a
model based approach  C∗ is ﬁxed and data points {xi}n
i=1 are generated either deterministically or
stochastically from one of the ground-truth subspaces in C∗ with noise corruption; for a completely
agnostic setting  C∗ is deﬁned as the minimizer of the k-means subspace clustering objective:
d2(xi Sj).

cost(C;X ) = argminC={S1 ···  Sk}⊆Sd

C∗ := argminC={S1 ···  Sk}⊆Sd

n(cid:88)

π(i)) 

(3)

q

1
n

q

min

j

i=1

To simplify notations  we use ∆k(X ) = cost(C∗;X ) to denote cost of the optimal solution.

i=1 ⊆ Rd  number of subsets m  privacy parameters ε  δ; f  dM.

m  α = ε/(5(cid:112)2 ln(2/δ))  β = ε/(4(D + ln(2/δ))).

Algorithm 1 The sample-aggregate framework [22]
1: Input: X = {xi}n
√
2: Initialize: s =
3: Subsampling: Select m random subsets of size n/m of X independently and uniformly at
√
random without replacement. Repeat this step until no single data point appears in more than
m of the sets. Mark the subsampled subsets XS1  ···  XSm.

i=1 ⊆ RD  where si = f (XSi).

4: Separate queries: Compute B = {si}m
5: Aggregation: Compute g(B) = si∗ where i∗ = argminm
2 + 1). Here
6: Noise calibration: Compute S(B) = 2 maxk(ρ(t0 + (k + 1)s) · e−βk)  where ρ(t) is the mean
7: Output: A(X ) = g(B) + S(B)

ri(t0) denotes the distance dM(· ·) between si and the t0-th nearest neighbor to si in B.
of the top (cid:98)s/β(cid:99) values in {r1(t) ···   rm(t)}.

α u  where u is a standard Gaussian random vector.

i=1ri(t0) with t0 = ( m+s

2.3 Differential privacy
Deﬁnition 2.1 (Differential privacy  [7  8]). A randomized algorithm A is (ε  δ)-differentially private
if for all X  Y satisfying d(X  Y) = 1 and all sets S of possible outputs the following holds:

Pr[A(X ) ∈ S] ≤ eε Pr[A(Y) ∈ S] + δ.

(4)

In addition  if δ = 0 then the algorithm A is ε-differentially private.
In our setting  the distance d(· ·) between two datasets X and Y is deﬁned as the number of different
columns in X and Y. Differential privacy ensures the output distribution is obfuscated to the point
that every user has a plausible deniability about being in the dataset  and in addition any inferences
about individual user will have nearly the same conﬁdence before and after the private release.

3 Sample-aggregation based private subspace clustering

In this section we ﬁrst summarize the sample-aggregate framework introduced in [22] and argue
why it should be preferred to conventional output perturbation mechanisms [7  8] for subspace clus-
tering. We then analyze two efﬁcient algorithms based on the sample-aggregate framework and
prove formal privacy and utility guarantees. We also prove new results in our analysis regarding
the stability of k-means subspace clustering (Lem. 3.3) and graph connectivity (i.e.  consistency) of
noisy threshold-based subspace clustering (TSC  [14]) under a stochastic model (Lem. 3.5).

3.1 Smooth local sensitivity and the sample-aggregate framework

Most existing privacy frameworks [7  8] are
based on the idea of global sensitivity  which
is deﬁned as the maximum output perturbation
(cid:107)f (X1) − f (X2)(cid:107)ξ  where maximum is over
all neighboring databases X1 X2 and ξ = 1 or
2. Unfortunately  global sensitivity of cluster-
ing problems is usually high even if only clus-
ter centers are released. For example  Figure
1 shows that the global sensitivity of k-means
subspace clustering could be as high as O(1) 
which ruins the algorithm utility.
the above-mentioned chal-
To circumvent
lenges  Nissim et al.
introduces the
sample-aggregate framework based on the con-
cept of a smooth version of local sensitivity.
Unlike global sensitivity  local sensitivity measures the maximum perturbation (cid:107)f (X ) − f (X (cid:48))(cid:107)ξ
over all databases X (cid:48) neighboring to the input database X . The proposed sample-aggregate frame-
work (pseudocode in Alg. 1) enjoys local sensitivity and comes with the following guarantee:
Theorem 3.1 ([22]  Theorem 4.2). Let f : D → RD be an efﬁciently computable function where
D is the collection of all databases and D is the output dimension. Let dM(· ·) be a semimetric on

Figure 1:
Illustration of instability of k-means
subspace clustering solutions (d = 2  k = 2  q =
1). Blue dots represent evenly spaced data points
on the unit circle; blue crosses indicate an addi-
tional data point. Red lines are optimal solutions.

[22]

3

√
the outer space of f. 1 Set ε > 2D/
m and m = ω(log2 n). The sample-aggregate algorithm
A in Algorithm 1 is an efﬁcient (ε  δ)-differentially private algorithm. Furthermore  if f and m are
chosen such that the (cid:96)1 norm of the output of f is bounded by Λ and
PrXS⊆X [dM(f (XS)  c) ≤ r] ≥ 3

(5)
for some c ∈ RD and r > 0  then the standard deviation of Gaussian noise added is upper bounded
In addition  when m satisﬁes m = ω(D2 log2(r/Λ)/ε2)  with high
by O(r/ε) + Λ
probability each coordinate of A(X )− ¯c is upper bounded by O(r/ε)  where ¯c depending on A(X )
satisﬁes dM(c  ¯c) = O(r).

ε e−Ω( ε

√
D ).

4

m

Let f be any subspace clustering solver that outputs k estimated low-dimensional subspaces and
dM be the Wasserstein’s distance as deﬁned in Eq. (2). Theorem 3.1 provides privacy guarantee
for an efﬁcient meta-algorithm with any f. In addition  utility guarantee holds with some more
assumptions on input dataset X . In following sections we establish utility guarantees. The main
idea is to prove stability results as outlined in Eq. (5) for particular subspace clustering solvers and
then apply Theorem 3.1.

3.2 The agnostic setting
We ﬁrst consider the setting when data points {xi}n
i=1 are arbitrarily placed. Under such agnostic
setting the optimal solution C∗ is deﬁned as the one that minimizes the k-means cost as in Eq. (3).
The solver f is taken to be any (1 + )-approximation2 of optimal k-means subspace clustering; that
is  f always outputs subspaces ˆC satisfying cost( ˆC;X ) ≤ (1 + )cost(C∗;X ). Efﬁcient core-set
based approximation algorithms exist  for example  in [12]. The key task of this section it to identify
assumptions under which the stability condition in Eq. (5) holds with respect to an approximate
solver f. The example given in Figure 1 also suggests that identiﬁability issue arises when the input
data X itself cannot be well clustered. For example  no two straight lines could well approximate
data uniformly distributed on a circle. To circumvent the above-mentioned difﬁculty  we impose the
following well-separation condition on the input data X :
Deﬁnition 3.2 (Well-separation condition for k-means subspace clustering). A dataset X is
(φ  η  ψ)-well separated if there exist constants φ  η and ψ  all between 0 and 1  such that

k(X ) ≤ min(cid:8)φ2∆2

∆2

k−1(X )  ∆2

k −(X ) − ψ  ∆2

k +(X ) + η(cid:9)  

(6)
k −(X ) =

where ∆k−1  ∆k − and ∆k + are deﬁned as ∆2
minS1∈Sd

cost({Si};X ); and ∆2

q−1 S2:k∈Sd

q

k−1(X ) = minS1:k−1∈Sd
q+1 S2:k∈Sd

k +(X ) = minS1∈Sd

q

q

cost({Si};X ); ∆2
cost({Si};X ).

k(X ) ≤ ∆2

k(X ) ≤ φ2∆2

k−1(X )  constrains that the input dataset X cannot
The ﬁrst condition in Eq. (6)  ∆2
be well clustered using k − 1 instead of k clusters. It was introduced in [23] to analyze stability of
k-means solutions. For subspace clustering  we need another two conditions regarding the intrinsic
k −(X ) − ψ asserts that replacing a q-dimensional
dimension of each subspace. The ∆2
subspace with a (q − 1)-dimensional one is not sufﬁcient  while ∆2
k +(X ) + η means an
additional subspace dimension does not help much with clustering X .
The following lemma is our main stability result for subspace clustering on well-separated datasets.
It states that when a candidate clustering ˆC is close to the optimal clustering C∗ in terms of clustering
cost  they are also close in terms of the Wasserstein distance deﬁned in Eq. (2).
Lemma 3.3 (Stability of agnostic k-means subspace clustering). Assume X is (φ  η  ψ)-well sepa-
rated with φ2 < 1/1602  ψ > η. Suppose a candidate clustering ˆC = { ˆS1 ···   ˆSk} ⊆ Sd
q satisﬁes
cost( ˆC;X ) ≤ a · cost(C∗;X ) for some a < 1−802φ2
600

√
. Then the following holds:

k(X ) ≤ ∆2

2φ2

800φ2

√

k

dW ( ˆC C∗) ≤

(1 − 150φ2)(ψ − η)

.

(7)

The following theorem is then a simple corollary  with a complete proof in Appendix B.

1dM(· ·) satisﬁes dM(x  y) ≥ 0  dM(x  x) = 0 and dM(x  y) ≤ dM(x  z) + dM(y  z) for all x  y  z.
2Here  is an approximation constant and is not related to the privacy parameter ε.

4

i=1 ⊆ Rd  number of clusters k and number of neighbors s.

Algorithm 2 Threshold-based subspace clustering (TSC)  a simpliﬁed version
1: Input: X = {xi}n
2: Thresholding: construct G ∈ {0  1}n×n by connecting xi to the other s data points in X with
3: Clustering: Let X (1) ···  X ((cid:96)) be the connected components in G. Construct ¯X ((cid:96)) by sam-
4: Output: subspaces ˆC = { ˆS((cid:96))}k
(cid:96)=1; ˆS((cid:96)) is the subspace spanned by q arbitrary points in ¯X ((cid:96)).

the largest absolute inner products |(cid:104)xi  x(cid:48)(cid:105)|. Complete G so that it is undirected.
pling q points from X ((cid:96)) uniformly at random without replacement.

Theorem 3.4. Fix a (φ  η  ψ)-well separated dataset X with n data points and φ2 < 1/1602 
ψ > η. Suppose XS ⊆ X is a subset of X with size m  sampled uniformly at random without
replacement. Let ˆC = { ˆS1 ···   ˆS2} be an (1 + )-approximation of optimal k-means subspace
k(X ))
clustering computed on XS. If m = Ω( kqd log(qd/γ(cid:48)∆2
800φ2 − 2(1 + )  then we
k(X )
√
have:

) with γ(cid:48) < 1−802φ2
√

γ(cid:48)2∆4

(cid:35)

(cid:34)

1  ···  S∗

(1 − 150φ2)(ψ − η)

PrXS
k} is the optimal clustering on X ; that is  cost(C∗;X ) = ∆2

where C∗ = {S∗
Consequently  applying Theorem 3.4 together with the sample-aggregate framework we obtain a
weak polynomial-time ε-differentially private algorithm for agnostic k-means subspace clustering 
with additional amount of per-coordinate Gaussian noise upper bounded by O( φ2
ε(ψ−η) ). Our bound
is comparable to the one obtained in [22] for private k-means clustering  except for the (ψ − η) term
which characterizes the well-separatedness under the subspace clustering scenario.

k(X ).

√

k

 

(8)

dW ( ˆC C∗) ≤

600

2φ2

k

≥ 3
4

i

i

1  ···  S∗

i = y((cid:96))

i + ε((cid:96))

(cid:96)   a data point x((cid:96))
(cid:96) : (cid:107)y(cid:107)2 = 1} and εi ∼ N (0  σ2/d · Id) for some noise parameter σ.

3.3 The stochastic setting
We further consider the case when data points are stochastically generated from some underlying
“true” subspace set C∗ = {S∗
k}. Such settings were extensively investigated in previous
development of subspace clustering algorithms [24  25  14]. Below we give precise deﬁnition of the
considered stochastic subspace clustering model:
The stochastic model For every cluster (cid:96) associated with subspace S∗
belonging to cluster (cid:96) can be written as x((cid:96))
  where y((cid:96))
random from {y ∈ S∗
Under the stochastic setting we consider the solver f to be the Threshold-based Subspace Clustering
(TSC  [14]) algorithm. A simpliﬁed version of TSC is presented in Alg. 2. An alternative idea is to
apply results in the previous section since the stochastic model implies well-separated dataset when
noise level σ is small. However  the running time of TSC is O(n2d)  which is much more efﬁcient
than core-set based methods. TSC is provably correct in that the similarity graph G has no false
connections and is connected per cluster  as shown in the following lemma:
Lemma 3.5 (Connectivity of TSC). Fix γ > 1 and assume max 0.04n(cid:96) ≤ s ≤ min n(cid:96)/6. If for
every (cid:96) ∈ {1 ···   k}  the number of data points n(cid:96) and the noise level σ satisfy
1 − min
(cid:96)(cid:54)=(cid:96)(cid:48)

i ∈ Rd
is sampled uniformly at

(cid:18) γ
/(γ log n(cid:96)) − 12/n −(cid:80)

(cid:34)
√
5σ + σ2. Then with probability at least 1 − n2e−√

(cid:32)(cid:18) 0.01(q/2 − 1)(q − 1)
d − n(cid:80)

 
(cid:96) e−n(cid:96)/400 −
(cid:96) n(cid:96)e−c(n(cid:96)−1)  the connected components in G correspond ex-

d2(S∗
(cid:96)  S∗
(cid:96)(cid:48))
q−1(cid:33)(cid:35)
(cid:19) 1
q

0.01(q/2 − 1)(q − 1)
√

(cid:80)
(cid:96) n1−γ

q−1(cid:33)
(cid:19) 1

√
σ(1 + σ)
log n

2q(12π)q−1

where ¯σ = 2

24 log n
√

√
q√
d

n(cid:96)

(cid:115)

2πq log n(cid:96)

− cos

√

γπ

cos

12π

(cid:115)

>

d

≤

1

−

15 log n

;

n(cid:96)

(cid:32)

log n(cid:96)

¯σ <

π

;

(cid:96)

actly to the k subspaces.

Conditions in Lemma 3.5 characterize the interaction between sample complexity n(cid:96)  noise level
σ and “signal” level min(cid:96)(cid:54)=(cid:96)(cid:48) d(S∗
(cid:96)(cid:48)). Theorem 3.6 is then a simple corollary of Lemma 3.5.
Complete proofs are deferred to Appendix C.

(cid:96)  S∗

5

Theorem 3.6 (Stability of TSC on stochastic data). Assume conditions in Lemma 3.5 hold with
respect to n(cid:48) = n/m for ω(log2 n) ≤ m ≤ o(n). Assume in addition that limn→∞ n(cid:96) = ∞ for all
(cid:96) = 1 ···   L and the failure probability does not exceed 1/8. Then for every  > 0 we have

(cid:104)

(cid:105)

lim
n→∞ PrXS

dW ( ˆC C∗) > 

= 0.

(9)

Compared to Theorem 3.4 for the agnostic model  Theorem 3.6 shows that one can achieve consis-
tent estimation of underlying subspaces under a stochastic model. It is an interesting question to
derive ﬁnite sample bounds for the differentially private TSC algorithm.

3.4 Discussion
It is worth noting that the sample-aggregate framework is an (ε  δ)-differentially private mechanism
for any computational subroutine f. However  the utility claim (i.e.  the O(r/ε) bound on each
coordinate of A(X ) − c) requires the stability of the particular subroutine f  as outlined in Eq.
(5). It is unfortunately hard to theoretically argue for stability of state-of-the-art subspace clustering
methods such as sparse subspace cluster (SSC  [11]) due to the “graph connectivity” issue [21]3.
Nevertheless  we observe satisfactory performance of SSC based algorithms in simulations (see
Sec. 5). It remains an open question to derive utility guarantee for (user) differentially private SSC.

4 Private subspace clustering via the exponential mechanism
In Section 3 we analyzed two algorithms with provable privacy and utility guarantees for sub-
space clustering based on the sample-aggregate framework. However  empirical evidence shows
that sample-aggregate based private clustering suffers from poor utility in practice [26]. In this sec-
tion  we propose a practical private subspace clustering algorithm based on the exponential mecha-
nism [18]. In particular  given the dataset X with n data points  we propose to samples parameters
θ = ({S(cid:96)}k

d  zj ∈ {1 ···   k} from the following distribution:

i=1) where S(cid:96) ∈ Sq

(cid:96)=1 {zi}n

p(θ;X ) ∝ exp

 

(10)

(cid:32)

· n(cid:88)

i=1

− ε
2

(cid:33)
d2(xi Szi)

where ε > 0 is the privacy parameter. The following proposition shows that exact sampling from
the distribution in Eq. (10) results in a provable differentially private algorithm. Its proof is trivial
and is deferred to Appendix D.1. Note that unlike sample-aggregate based methods  the exponential
mechanism can privately release clustering assignment z. This does not violate the lower bound in
[29] because the released clustering assignment z is not guaranteed to be exactly correct.
Proposition 4.1. The random algorithm A : X (cid:55)→ θ that outputs one sample from the distribution
deﬁned in Eq. (10) is ε-differential private.

p(zi|{S(cid:96)}k

4.1 A Gibbs sampling implementation
It is hard in general to sample parameters from distributions as complicated as in Eq. (10). We
present a Gibbs sampler that iteratively samples subspaces {Si} and cluster assignments {zj} from
their conditional distributions.
Update of zi: When {S(cid:96)} and z−i are ﬁxed  the conditional distribution of zi is

(cid:96)=1  z−i;X ) ∝ exp(−ε/2 · d2(xi Szi)).

(11)
Since d(xi Szi) can be efﬁciently computed (given an orthonormal basis of Szi)  update of zi can
Update of S(cid:96): Let (cid:101)X ((cid:96)) = {xi ∈ X : zi = (cid:96)} denote data points that are assigned to cluster (cid:96) and
be easily done by sampling zj from a categorical distribution.
˜n(cid:96) = |(cid:101)X ((cid:96))|. Denote (cid:101)X((cid:96)) ∈ Rd×˜n(cid:96) as the matrix with columns corresponding to all data points in
(cid:101)X ((cid:96)). The distribution over S(cid:96) conditioned on z can then be written as
where A(cid:96) = (cid:101)X((cid:96))(cid:101)X((cid:96))(cid:62)

(12)
is the unnormalized sample covariance matrix. Distribution of the form in
Eq. (12) is a special case of the matrix Bingham distribution  which admits a Gibbs sampler [16]. We
give implementation details in Appendix D.2 with modiﬁcations so that the resulting Gibbs sampler
is empirically more efﬁcient for a wide range of parameter settings.

p(S(cid:96) = range(U(cid:96))|z;X ) ∝ exp(ε/2 · tr(U(cid:62)

(cid:96) A(cid:96)U(cid:96))); U(cid:96) ∈ Rd×q  U(cid:62)

(cid:96) U(cid:96) = Iq×q 

3Recently [28] established full clustering guarantee for SSC  however  under strong assumptions.

6

(cid:96)

parameters σ(cid:96) in MPPCA to(cid:112)1/ε. The only difference is that yi are sampled uniformly at random

4.2 Discussion
The proposed Gibbs sampler resembles the k-plane algorithm for subspace clustering [3].
It is
in fact a “probabilistic” version of k-plane since sampling is performed at each iteration rather
than deterministic updates. Furthermore  the proposed Gibbs sampler could be viewed as posterior
sampling for the following generative model: ﬁrst sample U(cid:96) uniformly at random from Sd
q for
each subspace S(cid:96); afterwards  cluster assignments {zi}n
i=1 are sampled such that Pr[zi = j] = 1/k
and xi is set as xi = U(cid:96)yi + PU⊥
wi  where yi is sampled uniformly at random from the q-
dimensional unit ball and wi ∼ N (0  Id/ε). Connection between the above-mentioned generative
model and Gibbs sampler is formally justiﬁed in Appendix D.3. The generative model is strikingly
similar to the well-known mixtures of probabilistic PCA (MPPCA  [27]) model by setting variance
from a unit ball 4 and noise wi is constrained to U⊥
(cid:96)   the complement space of U(cid:96). Note that this is
closely related to earlier observation that “posterior sampling is private” [20  6  31]  but different in
that we constructed a model from a private procedure rather than the other way round.
As the privacy parameter ε → ∞ (i.e.  no privacy guarantee)  we arrive immediately at the exact
k-plane algorithm and the posterior distribution concentrates around the optimal k-means solution
(C∗  z∗). This behavior is similar to what a small-variance asymptotic analysis on MPPCA models
reveals [30]. On the other hand  the proposed Gibbs sampler is signiﬁcantly different from previous
Bayesian probabilisitic PCA formulation [34  30] in that the subspaces are sampled from a matrix
Bingham distribution. Finally  we remark that the proposed Gibbs sampler is only asymptotically
private because Proposition 4.1 requires exact (or nearly exact [31]) sampling from Eq. (10).

5 Numerical results
We provide numerical results of both the sample-aggregate and Gibbs sampling algorithms on syn-
thetic and real-world datasets. We also compare with a baseline method implemented based on the
k-plane algorithm [3] with perturbed sample covariance matrix via the SuLQ framework [2] (de-
tails presented in Appendix E). Three solvers are considered for the sample-aggregate framework:
threshold-based subspace clustering (TSC  [14])  which has provable utility guarantee with sample-
aggregation on stochastic models  along with sparse subspace clustering (SSC  [11]) and low-rank
representation (LRR  [17])  the two state-of-the-art methods for subspace clustering. For Gibbs
sampling  we use non-private SSC and LRR solutions as initialization for the Gibbs sampler. All
methods are implemented using Matlab.
For synthetic datasets  we ﬁrst generate k random q-dimensional linear subspaces. Each subspace is
generated by ﬁrst sampling a d × q random Gaussian matrix and then recording its column space. n
data points are then assigned to one of the k subspaces (clusters) uniformly at random. To generate
a data point xi assigned with subspace S(cid:96)  we ﬁrst sample yi ∈ Rq with (cid:107)yi(cid:107)2 = 1 uniformly
at random from the q-dimensional unit sphere. Afterwards  xi is set as xi = U(cid:96)yi + wi  where
U(cid:96) ∈ Rd×q is an orthonormal basis associated with S(cid:96) and wi ∼ N (0  σ2Id) is a noise vector.
Figure 2 compares the utility (measured in terms of k-means objective cost( ˆC;X ) and the Wasser-
stein’s distance dW ( ˆC C∗)) of sample aggregation  Gibbs sampling and SuLQ subspace clustering.
As shown in the plots  sample-aggregation algorithms have poor utility unless the privacy parameter
ε is truly large (which means very little privacy protection). On the other hand  both Gibbs sampling
and SuLQ subspace clustering give reasonably good performance. Figure 2 also shows that SuLQ
scales poorly with the ambient dimension d. This is because SuLQ subspace clustering requires
calibrating noise to a d × d sample covariance matrix  which induces much error when d is large.
Gibbs sampling seems to be robust to various d settings.
We also experiment on real-world datasets. The right two plots in Figure 2 report utility on a sub-
set of the extended Yale Face Dataset B [13] for face clustering. 5 random individuals are picked 
forming a subset of the original dataset with n = 320 data points (images). The dataset is prepro-
cessed by projecting each individual onto a 9D afﬁne subspace via PCA. Such preprocessing step
was adopted in [32  29] and was theoretically justiﬁed in [1]. Afterwards  ambient dimension of
the entire dataset is reduced to d = 50 by random Gaussian projection. The plots show that Gibbs
sampling signiﬁcantly outperforms the other algorithms.

4In MPPCA latent variables yi are sampled from a normal distribution N (0  ρ2Iq).

7

Figure 2: Utility under ﬁxed privacy budget ε. Top row shows k-means cost and bottom row shows
the Wasserstein’s distance dW ( ˆC C∗). From left to right: synthetic dataset  n = 5000  d = 5  k =
3  q = 3  σ = 0.01; n = 1000  d = 10  k = 3  q = 3  σ = 0.1; extended Yale Face Dataset B
(a subset). n = 320  d = 50  k = 5  q = 9  σ = 0.01. δ is set to 1/(n ln n) for (ε  δ)-privacy
algorithms. “s.a.” stands for smooth sensitivity and “exp.” stands for exponential mechanism.
“SuLQ-10” and “SuLQ-50” stand for the SuLQ framework performing 10 and 50 iterations. Gibbs
sampling is run for 10000 iterations and the mean of the last 100 samples is reported.

Figure 3: Test statistics  k-means cost and dW ( ˆC C∗) of 8 trials of the Gibbs sampler under different
privacy settings. Synthetic dataset setting: n = 1000  d = 10  k = 3  q = 3  σ = 0.1.

((cid:80)k
(cid:96)=1 (cid:107)1/T ·(cid:80)T

√
In Figure 3 we investigate the mixing behavior of proposed Gibbs sampler. We plot for multiple
kq ·
trials of Gibbs sampling the k-means objective  Wasserstein’s distance and a test statistic 1/
is a basis sample of S(cid:96) at the tth iteration. The test
statistic has mean zero under distribution in Eq. (10) and a similar statistic was used in [4] as a
diagnostic of the mixing behavior of another Gibbs sampler. Figure 3 shows that under various
privacy parameter settings  the proposed Gibbs sampler mixes quite well after 10000 iterations.

(cid:96) (cid:107)2
t=1 U(t)

F )1/2  where U(t)

(cid:96)

6 Conclusion
In this paper we consider subspace clustering subject to formal differential privacy constraints. We
analyzed two sample-aggregate based algorithms with provable utility guarantees under agnostic and
stochastic data models. We also propose a Gibbs sampling subspace clustering algorithm based on
the exponential mechanism that works well in practice. Some interesting future directions include
utility bounds for state-of-the-art subspace clustering algorithms like SSC or LRR.

Acknowledgement This research is supported in part by grant NSF CAREER IIS-1252412  NSF
Award BCS-0941518  and a grant by Singapore National Research Foundation under its Interna-
tional Research Centre @ Singapore Funding Initiative administered by the IDM Programme Ofﬁce.

8

−1−0.500.511.522.5300.050.10.150.20.250.3Log10εK−means cost s.a.  SSCs.a.  TSCs.a.  LRRexp.  SSCexp. LRRSuLQ−10SuLQ−50−1−0.500.511.522.5300.10.20.30.40.50.60.7Log10εK−means cost s.a.  SSCs.a.  TSCs.a.  LRRexp.  SSCexp. LRRSuLQ−10SuLQ−50−1−0.500.511.522.5300.10.20.30.40.50.60.70.80.9Log10εK−means cost s.a.  SSCs.a.  TSCs.a.  LRRexp.  SSCexp. LRRSuLQ−10SuLQ−50−1−0.500.511.522.53−0.500.511.522.53Log10εWasserstein distance s.a.  SSCs.a.  TSCs.a.  LRRexp.  SSCexp. LRRSuLQ−10SuLQ−50−1−0.500.511.522.5300.511.522.533.54Log10εWasserstein distance s.a.  SSCs.a.  TSCs.a.  LRRexp.  SSCexp. LRRSuLQ−10SuLQ−50−1−0.500.511.522.5323456789Log10εWasserstein distance s.a.  SSCs.a.  TSCs.a.  LRRexp.  SSCexp. LRRSuLQ−10SuLQ−5002040608010000.20.40.60.81× 100 iterationsTest statistic ε=0.1ε=1ε=10ε=10002040608010000.10.20.30.40.50.60.70.8× 100 iterationsK−means cost02040608010000.511.522.533.54× 100 iterationsWasserstein distanceReferences
[1] R. Basri and D. Jacobs. Lambertian reﬂectance and linear subspaces.

Analysis and Machine Intelligence  25(2):218–233  2003.

IEEE Transactions on Pattern

[2] A. Blum  C. Dwork  F. McSherry  and K. Nissim. Practical privacy: the SULQ framework. In PODS 

2015.

[3] P. S. Bradley and O. L. Mangasarian. k-plane clustering. Journal of Global Optimization  16(1)  2000.
[4] K. Chaudhuri  A. Sarwate  and K. Sinha. Near-optimal algorithms for differentially private principal

components. In NIPS  2012.

[5] Y. Chen  A. Jalali  S. Sanghavi  and H. Xu. Clustering partially observed graphs via convex optimization.

The Journal of Machine Learning Research  15(1):2213–2238  2014.

[6] C. Dimitrakakis  B. Nelson  A. Mitrokotsa  and B. I. Rubinstein. Robust and private bayesian inference.

In Algorithmic Learning Theory  pages 291–305. Springer  2014.

[7] C. Dwork  K. Kenthapadi  F. McSherry  I. Mironov  and M. Naor. Our data  ourselves: Privacy via

distributed noise generation. In EUROCRYPT  2006.

[8] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private data analysis.

In TCC  2006.

[9] C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in

Theoretical Computer Science  9(3–4):211–407  2014.

[10] C. Dwork  K. Talwar  A. Thakurta  and L. Zhang. Analyze Gauss: Optimal bounds for privacy-preserving

principal component analysis. In STOC  2014.

[11] E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm  theory and applications. IEEE Trans-

actions on Pattern Analysis and Machine Intelligence  35(11):2765–2781  2013.

[12] D. Feldman  M. Schmidt  and C. Sohler. Turning big data into tiny data: Constant-size coresets for

k-means  pca and projective clustering. In SODA  2013.

[13] A. Georghiades  P. Belhumeur  and D. Kriegman. From few to many: Illumination cone models for
face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine
Intelligence  23(6):643–660  2001.

[14] R. Heckel and H. B¨olcskei. Robust subspace clustering via thresholding. arXiv:1307.4891  2013.
[15] J. Ho  M.-H. Yang  J. Lim  K.-C. Lee  and D. Kriegman. Clustering appearances of objects under varying

illumination conditions. In CVPR  2003.

[16] P. Hoff. Simulation of the matrix bingham-conmises-ﬁsher distribution  with applications to multivariate

and relational data. Journal of Computational and Graphical Statistics  18(2):438–456  2009.

[17] G. Liu  Z. Lin  S. Yan  J. Sun  Y. Ma  and Y. Yu. Robust recovery of subspace structures by low-rank
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence  35(1):171–184  2012.

[18] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS  2007.
[19] B. McWilliams and G. Montana. Subspace clustering of high-dimensional data: a predictive approach.

Data Mining and Knowledge Discovery  28(3):736–772  2014.

[20] D. J. Mir. Differential privacy: an exploration of the privacy-utility landscape. PhD thesis  Rutgers

[21] B. Nasihatkon and R. Hartley. Graph connectivity in sparse subspace clustering. In CVPR  2011.
[22] K. Nissim  S. Raskhodnikova  and A. Smith. Smooth sensitivity and sampling in private data analysis. In

University  2013.

STOC  2007.

[23] R. Ostrovksy  Y. Rabani  L. Schulman  and C. Swamy. The effectiveness of Lloyd-type methods for the

[24] M. Soltanolkotabi  E. J. Candes  et al. A geometric analysis of subspace clustering with outliers. The

k-means problem. In FOCS  2006.

Annals of Statistics  40(4):2195–2238  2012.

[25] M. Soltanolkotabi  E. Elhamifa  and E. Candes. Robust subspace clustering. The Annals of Statistics 

42(2):669–699  2014.

11(2):443–482  1999.

[26] D. Su  J. Cao  N. Li  E. Bertino  and H. Jin. Differentially private k-means clustering. arXiv  2015.
[27] M. Tipping and C. Bishop. Mixtures of probabilistic principle component anlyzers. Neural computation 

[28] Y. Wang  Y.-X. Wang  and A. Singh. Clustering consistent sparse subspace clustering. arXiv  2015.
[29] Y. Wang  Y.-X. Wang  and A. Singh. A deterministic analysis of noisy sparse subspace clustering for

dimensionality-reduced data. In ICML  2015.

[30] Y. Wang and J. Zhu. DP-space: Bayesian nonparametric subspace clustering with small-variance asymp-

totic analysis. In ICML  2015.

monte carlo. In ICML  2015.

[31] Y.-X. Wang  S. Fienberg  and A. Smola. Privacy for free: Posterior sampling and stochastic gradient

[32] Y.-X. Wang and H. Xu. Noisy sparse subspace clustering. In ICML  pages 89–97  2013.
[33] A. Zhang  N. Fawaz  S. Ioannidis  and A. Montanari. Guess who rated this movie: Identifying users

through subspace clustering. arXiv  2012.

[34] Z. Zhang  K. L. Chan  J. Kwok  and D.-Y. Yeung. Bayesian inference on principal component analysis

using reversible jump markov chain monte carlo. In AAAI  2004.

9

,Yining Wang
Yu-Xiang Wang
Aarti Singh