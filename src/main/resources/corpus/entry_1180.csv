2013,Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses,The design of convex  calibrated surrogate losses  whose minimization entails consistency with respect to a desired target loss  is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems  with target losses including the precision@q  expected rank utility  mean average precision  and pairwise disagreement.,Convex Calibrated Surrogates for Low-Rank Loss
Matrices with Applications to Subset Ranking Losses

Harish G. Ramaswamy

Shivani Agarwal

Computer Science & Automation

Computer Science & Automation

Indian Institute of Science

harish gurup@csa.iisc.ernet.in

Indian Institute of Science
shivani@csa.iisc.ernet.in

Ambuj Tewari

Statistics and EECS

University of Michigan

tewaria@umich.edu

Abstract

The design of convex  calibrated surrogate losses  whose minimization entails
consistency with respect to a desired target loss  is an important concept to have
emerged in the theory of machine learning in recent years. We give an explicit
construction of a convex least-squares type surrogate loss that can be designed to
be calibrated for any multiclass learning problem for which the target loss matrix
has a low-rank structure; the surrogate loss operates on a surrogate target space
of dimension at most the rank of the target loss. We use this result to design
convex calibrated surrogates for a variety of subset ranking problems  with target
losses including the precision@q  expected rank utility  mean average precision 
and pairwise disagreement.

Introduction

1
There has been much interest in recent years in understanding consistency properties of learning
algorithms – particularly algorithms that minimize a surrogate loss – for a variety of ﬁnite-output
learning problems  including binary classiﬁcation  multiclass classiﬁcation  multi-label classiﬁca-
tion  subset ranking  and others [1–17]. For algorithms minimizing a surrogate loss  the question
of consistency reduces to the question of calibration of the surrogate loss with respect to the target
loss of interest [5–7  16]; in general  one is interested in convex surrogates that can be minimized
efﬁciently. In particular  the existence (and lack thereof) of convex calibrated surrogates for various
subset ranking problems  with target losses including for example the discounted cumulative gain
(DCG)  mean average precision (MAP)  mean reciprocal rank (MRR)  and pairwise disagreement
(PD)  has received signiﬁcant attention recently [9  11–13  15–17].
In this paper  we develop a general result which allows us to give an explicit convex  calibrated
surrogate deﬁned on a low-dimensional surrogate space for any ﬁnite-output learning problem for
which the loss matrix has low rank. Recently  Ramaswamy and Agarwal [16] showed the existence
of such surrogates  but their result involved an unwieldy surrogate space  and moreover did not give
an explicit  usable construction for the mapping needed to transform predictions in the surrogate
space back to the original prediction space. Working in the same general setting as theirs  we give
an explicit construction that leads to a simple least-squares type surrogate. We then apply this
result to obtain several new results related to subset ranking. Speciﬁcally  we ﬁrst obtain calibrated 
score-based surrogates for the Precision@q loss  which includes the winner-take-all (WTA) loss as
a special case  and the expected rank utility (ERU) loss; to the best of our knowledge  consistency
with respect to these losses has not been studied previously in the literature. When there are r
documents to be ranked for each query  the score-based surrogates operate on an r-dimensional
surrogate space. We then turn to the MAP and PD losses  which are both widely used in practice  and
for which it has been shown that no convex score-based surrogate can be calibrated for all probability
distributions [11  15  16]. For the PD loss  Duchi et al. [11] gave certain low-noise conditions on the
probability distribution under which a convex  calibrated score-based surrogate could be designed;

1

we are unaware of such a result for the MAP loss. A straightforward application of our low-rank
result to these losses yields convex calibrated surrogates deﬁned on O(r2)-dimensional surrogate
spaces  but in both cases  the mapping needed to transform back to predictions in the original space
involves solving a computationally hard problem.
Inspired by these surrogates  we then give a
convex score-based surrogate with an efﬁcient mapping that is calibrated with respect to MAP under
certain conditions on the probability distribution; this is the ﬁrst such result for the MAP loss that
we are aware of. We also give a family of convex score-based surrogates calibrated with the PD
loss under certain noise conditions  generalizing the surrogate and conditions of Duchi et al. [11].
Finally  we give an efﬁcient mapping for the O(r2)-dimensional surrogate for the PD loss  and show
that this leads to a convex surrogate calibrated with the PD loss under a more general condition  i.e.
over a larger set of probability distributions  than those associated with the score-based surrogates.
Paper outline. We start with some preliminaries and background in Section 2. Section 3 gives our
primary result  namely an explicit convex surrogate calibrated for low-rank loss matrices  deﬁned on
a surrogate space of dimension at most the rank of the matrix. Sections 4–7 then give applications
of this result to the Precision@q  ERU  MAP  and PD losses  respectively. All proofs not included
in the main text can be found in the appendix.

P

−→ er� ∗D .1

D[hm]

2 Preliminaries and Background
Setup. We work in the same general setting as that of Ramaswamy and Agarwal [16]. There is an
instance space X   a ﬁnite set of class labels Y = [n] = {1  . . .   n}  and a ﬁnite set of target labels
(possible predictions) T = [k] = {1  . . .   k}. Given training examples (X1  Y1)  . . .   (Xm  Ym)
drawn i.i.d. from a distribution D on X ×Y  the goal is to learn a prediction model h : X→T . Often 
T = Y  but this is not always the case (for example  in the subset ranking problems we consider 
the labels in Y are typically relevance vectors or preference graphs over a set of r documents  while
the target labels in T are permutations over the r documents). The performance of a prediction
model h : X→T is measured via a loss function � : Y × T →R+ (where R+ = [0 ∞)); here �(y  t)
denotes the loss incurred on predicting t ∈ T when the label is y ∈ Y. Speciﬁcally  the goal is
to learn a model h with low expected loss or �-error er�
D[h] = E(X Y )∼D[�(Y  h(X))]; ideally  one
wants the �-error of the learned model to be close to the optimal �-error er� ∗D = inf h:X→T er�
D[h].
An algorithm which when given a random training sample as above produces a (random) model
hm : X→T is said to be consistent w.r.t. � if the �-error of the learned model hm converges in
probability to the optimal: er�
Typically  minimizing the discrete �-error directly is computationally difﬁcult; therefore one uses
instead a surrogate loss function ψ : Y × Rd→ ¯R+ (where ¯R+ = [0 ∞])  deﬁned on the continuous
surrogate target space Rd for some d ∈ Z+ instead of the discrete target space T   and learns a
model f : X→Rd by minimizing (approximately  based on the training sample) the ψ-error erψ
D[f ] =
E(X Y )∼D[ψ(Y  f (X))]. Predictions on new instances x ∈ X are then made by applying the learned
model f and mapping back to predictions in the target space T via some mapping pred : Rd→T  
giving h(x) = pred(f (x)). Under suitable conditions  algorithms that approximately minimize the
ψ-error based on a training sample are known to be consistent with respect to ψ  i.e. to converge in
probability to the optimal ψ-error erψ ∗D = inf f :X→Rd erψ
D[f ]. A desirable property of ψ is that it be
calibrated w.r.t. �  in which case consistency w.r.t. ψ also guarantees consistency w.r.t. �; we give a
formal deﬁnition of calibration and statement of this result below.
+ :�i pi = 1}.
In what follows  we will denote by Δn the probability simplex in Rn: Δn = {p ∈ Rn
For z ∈ R  let (z)+ = max(z  0). We will ﬁnd it convenient to view the loss function � : Y×T →R+
as an n × k matrix with elements �yt = �(y  t) for y ∈ [n]  t ∈ [k]  and column vectors �t =
+ for t ∈ [k]. We will also represent the surrogate loss ψ : Y × Rd→ ¯R+
(�1t  . . .   �nt)� ∈ Rn
as a vector function ψ : Rd→ ¯Rn
+ with ψy(u) = ψ(y  u) for y ∈ [n]  u ∈ Rd  and ψ(u) =
(ψ1(u)  . . .   ψn(u))� ∈ ¯Rn
Deﬁnition 1 (Calibration). Let � : Y ×T →R+ and let P ⊆ Δn. A surrogate loss ψ : Y × Rd→ ¯R+
is said to be calibrated w.r.t. � over P if there exists a function pred : Rd→T such that

+ for u ∈ Rd.

∀p ∈ P :

p�ψ(u) > inf
u∈Rd

p�ψ(u) .

inf

u∈Rd:pred(u) /∈argmintp��t

1Here P

−→ denotes convergence in probability: Xm

P

−→ a if ∀� > 0  P(|Xm − a| ≥ �)→ 0 as m→∞.

2

In this case we also say (ψ  pred) is (� P)-calibrated  or if P = Δn  simply �-calibrated.
Theorem 2 ( [6  7  16]). Let � : Y × T →R+ and ψ : Y × Rd→ ¯R+. Then ψ is calibrated w.r.t. �
over Δn iff ∃ a function pred : Rd→T such that for all distributions D on X × Y and all sequences
of random (vector) functions fm : X→Rd (depending on (X1  Y1)  . . .   (Xm  Ym)) 

D[fm] P−→ erψ ∗D
erψ

implies

D[pred ◦ fm] P−→ er� ∗D .
er�

For any instance x ∈ X   let p(x) ∈ Δn denote the conditional label probability vector at x  given by
p(x) = (p1(x)  . . .   pn(x))� where py(x) = P(Y = y | X = x). Then one can extend the above
result to show that for P ⊂ Δn  ψ is calibrated w.r.t. � over P iff ∃ a function pred : Rd→T such
that the above implication holds for all distributions D on X × Y for which p(x) ∈ P ∀x ∈ X .
Subset ranking. Subset ranking problems arise frequently in information retrieval applications.
In a subset ranking problem  each instance in X consists of a query together with a set of say
r documents to be ranked. The label space Y varies from problem to problem:
in some cases 
labels consist of binary or multi-level relevance judgements for the r documents  in which case
Y = {0  1}r or Y = {0  1  . . .   s}r for some appropriate s ∈ Z+; in other cases  labels consist
of pairwise preference graphs over the r documents  represented as (possibly weighted) directed
acyclic graphs (DAGs) over r nodes. Given examples of such instance-label pairs  the goal is to
learn a model to rank documents for new queries/instances; in most cases  the desired ranking takes
the form of a permutation over the r documents  so that T = Sr (where Sr denotes the group of
permutations on r objects). As noted earlier  various loss functions are used in practice  and there
has been much interest in understanding questions of consistency and calibration for these losses in
recent years [9–15  17]. The focus so far has mostly been on designing r-dimensional surrogates 
which operate on a surrogate target space of dimension d = r; these are also termed ‘score-based’
surrogates since the resulting algorithms can be viewed as learning one real-valued score function
for each of the r documents  and in this case the pred mapping usually consists of simply sorting
the documents according to these scores. Below we will apply our result on calibrated surrogates
for low-rank loss matrices to obtain new calibrated surrogates – both r-dimensional  score-based
surrogates and  in some cases  higher-dimensional surrogates – for several subset ranking losses.

3 Calibrated Surrogates for Low Rank Loss Matrices
The following is the primary result of our paper. The result gives an explicit construction for a
convex  calibrated  least-squares type surrogate loss deﬁned on a low-dimensional surrogate space
for any target loss matrix that has a low-rank structure.
Theorem 3. Let � : Y × T →R+ be a loss function such that there exist d ∈ Z+  vectors
α1  . . .   αn ∈ Rd  β1  . . .   βk ∈ Rd and c ∈ R such that

d�i=1
d�i=1

αyiβti + c .

(ui − αyi)2

pred∗� (u) ∈ argmint∈[k]u�βt .

�(y  t) =

Let ψ∗� : Y × Rd→ ¯R+ be deﬁned as

ψ∗� (y  u) =

and let pred∗� : Rd→T be deﬁned as
Then�ψ∗�   pred∗�� is �-calibrated.
Proof. Let p ∈ Δn. Deﬁne up ∈ Rd as up
p�ψ∗� (u) =

Minimizing this over u ∈ Rd yields that up is the unique minimizer of p�ψ∗� (u). Also  for any
t ∈ [k]  we have

y=1 pyαyi ∀i ∈ [d]. Now for any u ∈ Rd  we have

py(ui − αyi)2 .

i =�n
d�i=1
n�y=1

3

Now  for each t ∈ [k]  deﬁne

regret�

p��t =

n�y=1

py� d�i=1
p(t) �= p��t − min
t�∈[k]

αyiβti + c� = (up)�βt + c .
p��t� = (up)�βt − min
t�∈[k]

(up)�βt� .

Clearly  by deﬁnition of pred∗�   we have regret�
p(t) = 0 for all
t ∈ [k]  then trivially pred∗� (u) ∈ argmintp��t ∀u ∈ Rd (and there is nothing to prove in this case).
Therefore assume ∃t ∈ [k] : regret�

p(pred∗� (up)) = 0. Also  if regret�

p(t) > 0  and let
� =

min
t∈[k]:regret�

p(t)>0

regret�

p(t) .

Then we have

inf

u∈Rd:pred∗� (u) /∈argmintp��t

p�ψ∗� (u) =

=

u∈Rd:regret�

inf
p(pred∗� (u))≥�
inf

p�ψ∗� (u)

p(pred∗� (u))≥regret�

u∈Rd:regret�
p(pred∗� (u)) is continuous at u = up. To see this 

p(pred∗� (up))+�

p�ψ∗� (u) .

Now  we claim that the mapping u �→ regret�
suppose the sequence {um} converges to up. Then we have
p(pred∗� (um)) = (up)�βpred∗� (um) − min
t�∈[k]

regret�

(up)�βt�

= (up − um)�βpred∗� (um) + u�mβpred∗� (um) − min
t�∈[k]
= (up − um)�βpred∗� (um) + min
u�mβt� − min
t�∈[k]
t�∈[k]

(up)�βt�

(up)�βt�

The last equality holds by deﬁnition of pred∗� . It is easy to see the term on the right goes to zero
p(pred∗� (up)) = 0  yielding
as um converges to up. Thus regret�
continuity at up. In particular  this implies ∃δ > 0 such that

p(pred∗� (um)) converges to regret�

�u − up� < δ =⇒ regret�

p(pred∗� (u)) − regret�

p(pred∗� (up)) < � .

This gives

u∈Rd:regret�

p(pred∗� (u))≥regret�

p(pred∗� (up))+�

inf

p�ψ∗� (u) ≥

p�ψ∗� (u)

inf

u∈Rd:�u−up�≥δ
p�ψ∗� (u)  

> inf
u∈Rd

where the last inequality holds since p�ψ∗� (u) is a strictly convex function of u and up is its unique
minimizer. The above sequence of inequalities give us that

inf

u∈Rd:pred∗� (u) /∈argmintp��t

p�ψ∗� (u) > inf
u∈Rd

p�ψ∗� (u) .

Since this holds for all p ∈ Δn  we have that (ψ∗�   pred∗� ) is �-calibrated.
We note that Ramaswamy and Agarwal [16] showed a similar least-squares type surrogate calibrated
for any loss � : Y × T →R+; indeed our proof technique above draws inspiration from the proof
technique there. However  the surrogate they gave was deﬁned on a surrogate space of dimension
n− 1  where n is the number of class labels in Y. For many practical problems  this is an intractably
large number. For example  as noted above  in the subset ranking problems we consider  the number
of class labels is typically exponential in r  the number of documents associated with each query.
On the other hand  as we will see below  many subset ranking losses have a low-rank structure 
with rank linear or quadratic in r  allowing us to use the above result to design convex calibrated
surrogates on an O(r) or O(r2)-dimensional space. Ramaswamy and Agarwal also gave another
result in which they showed that any loss matrix of rank d has a d-dimensional convex calibrated
surrogate; however the surrogate there was deﬁned such that it took values < ∞ on an awkward
space in Rd (not the full space Rd) that would be difﬁcult to construct in practice  and moreover 
their result did not yield an explicit construction for the pred mapping required to use a calibrated
surrogate in practice. Our result above combines the beneﬁts of both these previous results  allowing
explicit construction of low-dimensional least-squares type surrogates for any low-rank loss matrix.
The following sections will illustrate several applications of this result.

4

4 Calibrated Surrogates for Precision@q
The Precision@q is a popular performance measure for subset ranking problems in information
retrieval. As noted above  in a subset ranking problem  each instance in X consists of a query
together with a set of r documents to be ranked. Consider a setting with binary relevance judgement
labels  so that Y = {0  1}r with n = 2r. The prediction space is T = Sr (group of permutations on
r objects) with k = r!. For y ∈ {0  1}r and σ ∈ Sr  where σ(i) denotes the position of document i
under σ  the Precision@q loss for any integer q ∈ [r] can be written as follows:

�P@q(y  σ) = 1 −

= 1 −

1
q

1
q

yσ−1(i)

yi · 1(σ(i) ≤ q) .

q�i=1
r�i=1

Therefore  by Theorem 3  for the r-dimensional surrogate ψ∗P@q : {0  1}r × Rr→ ¯R+ and pred∗P@q :
Rr→Sr deﬁned as

ψ∗P@q(y  u) =

r�i=1

(ui − yi)2
r�i=1

pred∗P@q(u) ∈ argmaxσ∈Sr

ui · 1(σ(i) ≤ q)  

we have that (ψ∗P@q  pred∗P@q) is �P@q-calibrated. It can easily be seen that for any u ∈ Rr  any
permutation σ which places the top q documents sorted in decreasing order of scores ui in the top
q positions achieves the maximum in pred∗P@q(u); thus pred∗P@q(u) can be implemented efﬁciently
using a standard sorting or selection algorithm. Note that the popular winner-take-all (WTA) loss 
which assigns a loss of 0 if the top-ranked item is relevant (i.e. if yσ−1(1) = 1) and 1 otherwise 
is simply a special case of the above loss with q = 1; therefore the above construction also yields
a calibrated surrogate for the WTA loss. To our knowledge  this is the ﬁrst example of convex 
calibrated surrogates for the Precision@q and WTA losses.

5 Calibrated Surrogates for Expected Rank Utility
The expected rank utility (ERU) is a popular subset ranking performance measure used in recom-
mender systems displaying short ranked lists [18].
In this case the labels consist of multi-level
relevance judgements (such as 0 to 5 stars)  so that Y = {0  1  . . .   s}r for some appropriate s ∈ Z+
with n = (s + 1)r. The prediction space again is T = Sr with k = r!. For y ∈ {0  1  . . .   s}r and
σ ∈ Sr  where σ(i) denotes the position of document i under σ  the ERU loss is deﬁned as

�ERU(y  σ) = z −

max(yi − v  0) · 2

1−σ(i)
w−1

 

where z is a constant to ensure the positivity of the loss  v ∈ [s] is a constant that indicates a
neutral score  and w ∈ R is a constant indicating the viewing half-life. Thus  by Theorem 3  for the
r-dimensional surrogate ψ∗ERU : {0  1  . . .   s}r × Rr→ ¯R+ and pred∗ERU : Rr→Sr deﬁned as

r�i=1

r�i=1

ψ∗ERU(y  u) =

(ui − max(yi − v  0))2

pred∗ERU(u) ∈ argmaxσ∈Sr

1−σ(i)
w−1

 

ui · 2

r�i=1

we have that (ψ∗ERU  pred∗ERU) is �ERU-calibrated. It can easily be seen that for any u ∈ Rr  any
permutation σ satisfying the condition

achieves the maximum in pred∗ERU(u)  and therefore pred∗ERU(u) can be implemented efﬁciently
by simply sorting the r documents in decreasing order of scores ui. As for Precision@q  to our
knowledge  this is the ﬁrst example of a convex  calibrated surrogate for the ERU loss.

ui > uj =⇒ σ(i) < σ(j)

5

6 Calibrated Surrogates for Mean Average Precision
The mean average precision (MAP) is a widely used ranking performance measure in information
retrieval and related applications [15  19]. As with the Precision@q loss  Y = {0  1}r and T = Sr.
For y ∈ {0  1}r and σ ∈ Sr  where σ(i) denotes the position of document i under σ  the MAP loss
is deﬁned as follows:

�MAP(y  σ) = 1 −

1

|{γ : yγ = 1}| �i:yi=1

1

σ(i)

σ(i)�j=1

yσ−1(j) .

It was recently shown that there cannot exist any r-dimensional convex  calibrated surrogates for
the MAP loss [15]. We now re-write the MAP loss above in a manner that allows us to show the
existence of an O(r2)-dimensional convex  calibrated surrogate. In particular  we can write

2

i

yiyj

�r

r�i=1

i�j=1

�r

. = 1 −

1
γ=1 yγ

1
γ=1 yγ

yσ−1(i)yσ−1(j)

�MAP(y  σ) = 1 −

r�i=1
i�j=1
max(σ(i)  σ(j))
Thus  by Theorem 3  for the r(r+1)
-dimensional surrogate ψ∗MAP : {0  1}r × Rr(r+1)/2→ ¯R+ and
pred∗MAP : Rr(r+1)/2→Sr deﬁned as
i�j=1�uij −
γ=1 yγ�2
r�i=1
yiyj�r
ψ∗MAP(y  u) =
r�i=1
i�j=1

pred∗MAP(u) ∈ argmaxσ∈Sr
we have that (ψ∗MAP  pred∗MAP) is �MAP-calibrated.
Note however that the optimization problem associated with computing pred∗MAP(u) above can be
written as a quadratic assignment problem (QAP)  and most QAPs are known to be NP-hard. We
conjecture that the QAP associated with the mapping pred∗MAP above is also NP-hard. Therefore 
while the surrogate loss ψ∗MAP is calibrated for �MAP and can be minimized efﬁciently over a training
sample to learn a model f : X→Rr(r+1)/2  for large r  evaluating the mapping required to transform
predictions in Rr(r+1)/2 back to predictions in Sr is likely to be computationally infeasible. Below
we describe an alternate mapping in place of pred∗MAP which can be computed efﬁciently  and show
that under certain conditions on the probability distribution  the surrogate ψ∗MAP together with this
mapping is still calibrated for �MAP.
Speciﬁcally  deﬁne predMAP : Rr(r+1)/2→Sr as follows:

max(σ(i)  σ(j))

uij ·

1

 

predMAP(u) ∈ �σ ∈ Sr : uii > ujj =⇒ σ(i) < σ(j)� .

∀i  j ∈ [r] : i ≥ j .

Clearly  predMAP(u) can be implemented efﬁciently by simply sorting the ‘diagonal’ elements uii
for i ∈ [r]. Also  let ΔY denote the probability simplex over Y  and for each p ∈ ΔY  deﬁne
up ∈ Rr(r+1)/2 as follows:

up

ij = EY ∼p� YiYj�r
Now deﬁne Preinforce ⊂ ΔY as follows:
Preinforce = �p ∈ ΔY : up

γ=1 Yγ� = �y∈Y

ii ≥ up

jj =⇒ up

γ=1 yγ�
py� yiyj�r
jj + �γ∈[r]\{i j}
ii ≥ up

ij = up

ji for i < j. Then we have the following result:

where we set up
Theorem 4. (ψ∗MAP  predMAP) is (�MAP Preinforce)-calibrated.
The ideal predictor pred∗MAP uses the entire u matrix  but the predictor predMAP  uses only the diag-
onal elements. The noise conditions Preinforce can be viewed as basically enforcing that the diagonal
elements dominate and enforce a clear ordering themselves.
In fact  since the mapping predMAP depends on only the diagonal elements of u  we can equivalently
deﬁne an r-dimensional surrogate that is calibrated w.r.t. �MAP over Preinforce. Speciﬁcally  we have
the following immediate corollary:

(up
jγ − up

iγ)+�  

6

Corollary 5. Let �ψMAP : {0  1}r × Rr→ ¯R+ and �predMAP : Rr→Sr be deﬁned as
�ψMAP(y �u) =
�predMAP(�u) ∈ �σ ∈ Sr :�ui >�uj =⇒ σ(i) < σ(j)� .

r�i=1��ui −
γ=1 yγ�2
yi�r
Then (�ψMAP �predMAP) is (�MAP Preinforce)-calibrated.
Looking at the form of �ψMAP and �predMAP  we can see that the function s : Y→Rr deﬁned as
si(y) = yi/(�r
γ=1 yr) is a ‘standardization function’ for the MAP loss over Preinforce  and therefore
it follows that any ‘order-preserving surrogate’ with this standardization function is also calibrated
with the MAP loss over Preinforce [13]. To our knowledge  this is the ﬁrst example of conditions on
the probability distribution under which a convex calibrated (and moreover  score-based) surrogate
can be designed for the MAP loss.

7 Calibrated Surrogates for Pairwise Disagreement
The pairwise disagreement (PD) loss is a natural and widely used loss in subset ranking [11  17].
The label space Y here consists of a ﬁnite number of (possibly weighted) directed acyclic graphs
(DAGs) over r nodes; we can represent each such label as a vector y ∈ Rr(r−1)
where at least one
of yij or yji is 0 for each i �= j  with yij > 0 indicating a preference for document i over document
j and yij denoting the weight of the preference. The prediction space as usual is T = Sr with
k = r!. For y ∈ Y and σ ∈ Sr  where σ(i) denotes the position of document i under σ  the PD loss
is deﬁned as follows:

+

�PD(y  σ) =

r�i=1�j�=i

yij 1�σ(i) > σ(j)� .

It was recently shown that there cannot exist any r-dimensional convex  calibrated surrogates for the
PD loss [15  16]. By Theorem 3  for the r(r − 1)-dimensional surrogate ψ∗PD : Y × Rr(r−1)→ ¯R+
and pred∗PD : Rr(r−1)→Sr deﬁned as
ψ∗PD(y  u) =

(1)

r�i=1�j�=i
pred∗PD(u) ∈ argminσ∈Sr

(uij − yij)2
r�i=1�j�=i

uij · 1�σ(i) > σ(j)�

2

2

  allowing for an r(r−1)

we immediately have that (ψ∗PD  pred∗PD) is �PD-calibrated (in fact the loss matrix �PD has rank at most
r(r−1)
-dimensional surrogate; we use r(r−1) dimensions for convenience).
In this case  the optimization problem associated with computing pred∗PD(u) above is a minimum
weighted feedback arc set (MWFAS) problem  which is known to be NP-hard. Therefore  as with the
MAP loss  while the surrogate loss ψ∗PD is calibrated for �PD and can be minimized efﬁciently over
a training sample to learn a model f : X→Rr(r−1)  for large r  evaluating the mapping required to
transform predictions in Rr(r−1) back to predictions in Sr is likely to be computationally infeasible.
Below we give two sets of results. In Section 7.1  we give a family of score-based (r-dimensional)
surrogates that are calibrated with the PD loss under different conditions on the probability distri-
bution; these surrogates and conditions generalize those of Duchi et al. [11]. In Section 7.2  we
give a different condition on the probability distribution under which we can actually avoid ‘difﬁ-
cult’ graphs being passed to pred∗PD. This condition is more general (i.e. encompasses a larger set
of probability distributions) than those associated with the score-based surrogates; this gives a new
(non-score-based  r(r−1)-dimensional) surrogate with an efﬁciently computable pred mapping that
is calibrated with the PD loss over a larger set of probability distributions than previous surrogates
for this loss.

7.1 Family of r-Dimensional Surrogates Calibrated with �PD Under Noise Conditions
The following gives a family of score-based surrogates  parameterized by functions f : Y→Rr  that
are calibrated with the PD loss under different conditions on the probability distribution:

7

ψf (y  u) =

Theorem 6. Let f : Y→Rr be any function that maps DAGs y ∈ Y to score vectors f (y) ∈ Rr. Let
ψf : Y × Rr→ ¯R+  pred : Rr→Sr and Pf ⊂ ΔY be deﬁned as
r�i=1�ui − fi(y)�2

pred(u) ∈ �σ ∈ Sr : ui > uj =⇒ σ(i) < σ(j)�

Pf = �p ∈ ΔY : EY ∼p[Yij] > EY ∼p[Yji] =⇒ EY ∼p[fi(Y )] > EY ∼p[fj(Y )]� .

Then (ψf   pred) is (�PD Pf )-calibrated.
The noise conditions Pf state that the expected value of function f must decide the ‘right’ ordering.
We note that the surrogate given by Duchi et al. [11] can be written in our notation as

ψDMJ(y  u) =

yij(uj − ui) + ν

λ(ui)  

r�i=1

r�i=1�j�=i
fi(y) = �j�=i

(yij − yji) .

where λ is a strictly convex and 1-coercive function and ν > 0. Taking λ(z) = z2 and ν = 1
a special case of the family of score-based surrogates in Theorem 6 above obtained by taking f as

2 gives

Indeed  the set of noise conditions under which the surrogate ψDMJ is shown to be calibrated with
the PD loss in Duchi et al. [11] is exactly the set Pf above with this choice of f. We also note that f
can be viewed as a ‘standardization function’ [13] for the PD loss over Pf .
7.2 An O(r2)-dimensional Surrogate Calibrated with �PD Under More General Conditions
Consider now the r(r − 1)-dimensional surrogate ψ∗PD : Y × Rr(r−1) deﬁned in Eq. (1). We noted
the corresponding mapping pred∗PD involved an NP-hard optimization problem. Here we give an
alternate mapping predPD : Rr(r−1)→Sr that can be computed efﬁciently  and show that under
certain conditions on the probability distribution   the surrogate ψ∗PD together with this mapping
predPD is calibrated for �PD. The mapping predPD is described by Algorithm 1 below:

Algorithm 1 predPD

(Input: u ∈ Rr(r−1); Output: Permutation σ ∈ Sr)

Construct a directed graph over [r] with edge (i  j) having weight (uij − uji)+. If this graph is
acyclic  return any topological sorted order. If the graph has cycles  sort the edges in ascending
order by weight and delete them one by one (smallest weight ﬁrst) until the graph becomes acyclic;
return any topological sorted order of the resulting acyclic graph.

For each p ∈ ΔY  deﬁne Ep = {(i  j) ∈ [r] × [r] : EY ∼p[Yij] > EY ∼p[Yji]}  and deﬁne

PDAG = �p ∈ ΔY :�[r]  Ep� is a DAG� .

Then we have the following result:
Theorem 7. (ψ∗PD  predPD) is (�PD PDAG)-calibrated.
It is easy to see that PDAG � Pf ∀f (where Pf is as deﬁned in Theorem 6)  so that the above result
yields a low-dimensional  convex surrogate with an efﬁciently computable pred mapping that is
calibrated for the PD loss under a broader set of conditions than the previous surrogates.

8 Conclusion
Calibration of surrogate losses is an important property in designing consistent learning algorithms.
We have given an explicit method for constructing calibrated surrogates for any learning problem
with a low-rank loss structure  and have used this to obtain several new results for subset ranking 
including new calibrated surrogates for the Precision@q  ERU  MAP and PD losses.

Acknowledgments

The authors thank the anonymous reviewers  Aadirupa Saha and Shiv Ganesh for their comments. HGR ac-
knowledges a Tata Consultancy Services (TCS) PhD fellowship and the Indo-US Virtual Institute for Math-
ematical and Statistical Sciences (VIMSS). SA thanks the Department of Science & Technology (DST) and
Indo-US Science & Technology Forum (IUSSTF) for their support. AT gratefully acknowledges the support of
NSF under grant IIS-1319810.

8

References
[1] G´abor Lugosi and Nicolas Vayatis. On the Bayes-risk consistency of regularized boosting

methods. Annals of Statistics  32(1):30–55  2004.

[2] Wenxin Jiang. Process consistency for AdaBoost. Annals of Statistics  32(1):13–29  2004.
[3] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. Annals of Statistics  32(1):56–134  2004.

[4] Ingo Steinwart. Consistency of support vector machines and other regularized kernel classi-

ﬁers. IEEE Transactions on Information Theory  51(1):128–142  2005.

[5] Peter L. Bartlett  Michael Jordan  and Jon McAuliffe. Convexity  classiﬁcation and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[6] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225–1251  2004.

[7] Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research  8:1007–1025  2007.

[8] Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approx-

imation  26:225–287  2007.

[9] David Cossock and Tong Zhang. Statistical analysis of bayes optimal subset ranking. IEEE

Transactions on Information Theory  54(11):5140–5154  2008.

[10] Fen Xia  Tie-Yan Liu  Jue Wang  Wensheng Zhang  and Hang Li. Listwise approach to learning

to rank: Theory and algorithm. In International Conference on Machine Learning  2008.

[11] John Duchi  Lester Mackey  and Michael Jordan. On the consistency of ranking algorithms. In

International Conference on Machine Learning  2010.

[12] Pradeep Ravikumar  Ambuj Tewari  and Eunho Yang. On NDCG consistency of listwise rank-

ing methods. In International Conference on Artiﬁcial Intelligence and Statistics  2011.

[13] David Buffoni  Cl´ement Calauz`enes  Patrick Gallinari  and Nicolas Usunier. Learning scoring
functions with order-preserving losses and standardized supervision. In International Confer-
ence on Machine Learning  2011.

[14] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In Conference on

Learning Theory  2011.

[15] Cl´ement Calauz`enes  Nicolas Usunier  and Patrick Gallinari. On the (non-)existence of convex 
calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems
25  pages 197–205. 2012.

[16] Harish G. Ramaswamy and Shivani Agarwal. Classiﬁcation calibration dimension for general
In Advances in Neural Information Processing Systems 25  pages 2087–

multiclass losses.
2095. 2012.

[17] Yanyan Lan  Jiafeng Guo  Xueqi Cheng  and Tie-Yan Liu. Statistical consistency of ranking
methods in a rank-differentiable probability space. In Advances in Neural Information Pro-
cessing Systems 25  pages 1241–1249. 2012.

[18] Quoc V. Le and Alex Smola. Direct optimization of ranking measures  arXiv:0704.3359  2007.
[19] Yisong Yue  Thomas Finley  Filip Radlinski  and Thorsten Joachims. A support vector method
for optimizing average precision. In Proceedings of the 30th ACM SIGIR International Con-
ference on Research and Development in Information Retrieval  2007.

9

,Harish Ramaswamy
Shivani Agarwal
Ambuj Tewari
Mehryar Mohri
Scott Yang
Pratik Kumar Jawanpuria
Maksim Lapin
Matthias Hein
Bernt Schiele