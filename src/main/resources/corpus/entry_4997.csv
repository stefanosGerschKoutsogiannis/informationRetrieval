2010,Shadow Dirichlet for Restricted Probability Modeling,Although the Dirichlet distribution is widely used  the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems  such as regularized pmfs  monotonic pmfs  and pmfs with bounded variation. We describe some properties of this new class of distributions  provide maximum entropy constructions  give an expectation-maximization method for estimating the mean parameter  and illustrate with real data.,Shadow Dirichlet for Restricted Probability Modeling

Bela A. Frigyik  Maya R. Gupta  and Yihua Chen

Department of Electrical Engineering

frigyik@gmail.com  gupta@ee.washington.edu  yihuachn@gmail.com

University of Washington

Seattle  WA 98195

Abstract

Although the Dirichlet distribution is widely used  the independence structure of
its components limits its accuracy as a model. The proposed shadow Dirichlet
distribution manipulates the support in order to model probability mass functions
(pmfs) with dependencies or constraints that often arise in real world problems 
such as regularized pmfs  monotonic pmfs  and pmfs with bounded variation. We
describe some properties of this new class of distributions  provide maximum en-
tropy constructions  give an expectation-maximization method for estimating the
mean parameter  and illustrate with real data.

1 Modeling Probabilities for Machine Learning

Modeling probability mass functions (pmfs) as random is useful in solving many real-world prob-
lems. A common random model for pmfs is the Dirichlet distribution [1]. The Dirichlet is conjugate
to the multinomial and hence mathematically convenient for Bayesian inference  and the number of
parameters is conveniently linear in the size of the sample space. However  the Dirichlet is a distri-
bution over the entire probability simplex  and for many problems this is simply the wrong domain
if there is application-speciﬁc prior knowledge that the pmfs come from a restricted subset of the
simplex.
For example  in natural language modeling  it is common to regularize a pmf over n-grams by some
generic language model distribution q0  that is  the pmf to be modeled is assumed to have the form
θ = λq + (1 − λ)q0 for some q in the simplex  λ ∈ (0  1) and a ﬁxed generic model q0 [2]. But
once q0 and λ are ﬁxed  the pmf θ can only come from a subset of the simplex. Another natural
language processing example is modeling the probability of keywords in a dictionary where some
words are related  such as espresso and latte  and evidence for the one is to some extent
evidence for the other. This relationship can be captured with a bounded variation model that would
constrain the modeled probability of espresso to be within some  of the modeled probability
of latte. We show that such bounds on the variation between pmf components also restrict the
domain of the pmf to a subset of the simplex. As a third example of restricting the domain  the
similarity discriminant analysis classiﬁer estimates class-conditional pmfs that are constrained to be
monotonically increasing over an ordered sample space of discrete similarity values [3].
In this paper we propose a simple variant of the Dirichlet whose support is a subset of the simplex 
explore its properties  and show how to learn the model from data. We ﬁrst discuss the alternative
solution of renormalizing the Dirichlet over the desired subset of the simplex  and other related work.
Then we propose the shadow Dirichlet distribution; explain how to construct a shadow Dirichlet
for three types of restricted domains: the regularized pmf case  bounded variation between pmf
components  and monotonic pmfs; and discuss the most general case. We show how to use the
expectation-maximization (EM) algorithm to estimate the shadow Dirichlet parameter α  and present
simulation results for the estimation.

1

Dirichlet

Shadow Dirichlet

Renormalized Dirichlet

Figure 1: Dirichlet  shadow Dirichlet  and renormalized Dirichlet for α = [3.94 2.25 2.81].

2 Related Work

One solution to modeling pmfs on only a subset of the simplex is to simply restrict the support of
the Dirichlet to the desired support ˜S  and renormalize the Dirichlet over ˜S (see Fig. 1 for an ex-
ample). This renormalized Dirichlet has the advantage that it is still a conjugate distribution for the
multinomial. Nallapati et al.considered the renormalized Dirichlet for language modeling  but found
it difﬁcult to use because the density requires numerical integration to compute the normalizer [4]
. In addition  there is no closed form solution for the mean  covariance  or peak of the renormal-
ized Dirichlet  making it difﬁcult to work with. Table 1 summarizes these properties. Additionally 
generating samples from the renormalized Dirichlet is inefﬁcient: one draws samples from the stan-
dard Dirichlet  then rejects realizations that are outside ˜S. For high-dimensional sample spaces  this
could greatly increase the time to generate samples.
Although the Dirichlet is a classic and popular distribution on the simplex  Aitchison warns it “is to-
tally inadequate for the description of the variability of compositional data ” because of its “implied
independence structure and so the Dirichlet class is unlikely to be of any great use for describ-
ing compositions whose components have even weak forms of dependence” [5]. Aitchison instead
championed a logistic normal distribution with more parameters to control covariance between com-
ponents.
A number of variants of the Dirichlet that can capture more dependence have been proposed and
analyzed. For example  the scaled Dirichlet enables a more ﬂexible shape for the distribution [5] 
j Yj
where Yj ∼ Γ(αj  β)  whereas the scaled Dirichlet is derived from Yj ∼ Γ(αj  βj)  resulting in
+ are parameters  and γ is the normalizer.

but does not change the support. The original Dirichlet(α1  α2  . . . αd) can be derived as Yj/(cid:80)
density p(θ) = γ(cid:81)

αj
j θ

j

Another variant is the generalized Dirichlet [6] which also has parameters β  α ∈ Rd
+  and allows
greater control of the covariance structure  again without changing the support. As perhaps ﬁrst
noted by Karl Pearson [7] and expounded upon by Aitchison [5]  correlations of proportional data
can be very misleading. Many Dirichlet variants have been generalizations of the Connor-Mossiman
variant  Dirichlet process variants  other compound Dirichlet models  and hierarchical Dirichlet
models. Ongaro et al. [8] propose the ﬂexible Dirichlet distribution by forming a re-parameterized
mixture of Dirichlet distributions. Rayens and Srinivasan [9] considered the dependence structure
for the general Dirichlet family called the generalized Liouville distributions. In contrast to prior
efforts  the shadow Dirichlet manipulates the support to achieve various kinds of dependence that
arise frequently in machine learning problems.

αj−1

((cid:80)
i βiθi)α1+···+αd   where β  α ∈ Rd

β

j

3 Shadow Dirichlet Distribution
We introduce a new distribution that we call the shadow Dirichlet distribution. Let S be the prob-
ability (d − 1)-simplex  and let ˜Θ ∈ S be a random pmf drawn from a Dirichlet distribution with
density pD and unnormalized parameter α ∈ Rd
+. Then we say the random pmf Θ ∈ S is distributed
according to a shadow Dirichlet distribution if Θ = M ˜Θ for some ﬁxed d × d left-stochastic (that
is  each column of M sums to 1) full-rank (and hence invertible) matrix M  and we call ˜Θ the gen-

2

erating Dirichlet of Θ  or Θ’s Dirichlet shadow. Because M is a left-stochastic linear map between
ﬁnite-dimensional spaces  it is a continuous map from the convex and compact S to a convex and
compact subset of S that we denote SM .
The shadow Dirichlet has two parameters: the generating Dirichlet’s parameter α ∈ Rd
+  and the
d × d matrix M. Both α and M can be estimated from data. However  as we show in the following
subsections  the matrix M can be proﬁtably used as a design parameter that is chosen based on
application-speciﬁc knowledge or side-information to specify the restricted domain SM   and in that
way impose dependency between the components of the random pmfs.
The shadow Dirichlet density p(θ) is the normalized pushforward of the Dirichlet density  that is  it
is the composition of the Dirichlet density and M−1 with the Jacobian:
(M−1θ)αj−1

(cid:89)

p(θ) =

1

(1)

B(α)|det(M)|

is the standard Dirichlet normalizer  and α0 = (cid:80)d

 

j

j

j Γ(αj )
Γ(α0)

where B(α) (cid:44)
j=1 αj is the standard
Dirichlet precision factor. Table 1 summarizes the basic properties of the shadow Dirichlet. Fig. 1
shows an example shadow Dirichlet distribution.
Generating samples from the shadow Dirichlet is trivial: generate samples from its generating
Dirichlet (for example  using stick-breaking or urn-drawing) and multiply each sample by M to
create the corresponding shadow Dirichlet sample.

(cid:81)

Table 1: Table compares and summarizes the Dirichlet  renormalized Dirichlet  and shadow Dirich-
let distributions.

Density p(θ)

1

B(α)

Dirichlet(α)

(cid:81)d
j=1 θαj−1

j

Shadow

Dirichlet (α  M)

(cid:81)d
j=1(M−1θ)αj−1

j

1

B(α)|det(M )|

(cid:82)

˜S

(cid:82)

Renormalized
Dirichlet (α  ˜S)

(cid:81)d
j=1 θαj−1

j

(cid:81)d

1
j=1 q

αj−1

dq

j

(cid:82)

˜S θp(θ)dθ

˜S (θ − ¯θ)(θ − ¯θ)T p(θ)dθ

p(θ)

max
θ∈ ˜S

Mean

Covariance

Mode (if α > 1)

How to Sample

α
α0

Cov(Θ)

αj−1
α0−d

M

α
α0

M Cov(Θ)M T

M αj−1
α0−d

stick-breaking 
urn-drawing

draw from Dirichlet(α) 

multiply by M

draw from Dirichlet(α) 

reject if not in ˜S

ML Estimate

iterative

(simple functions)

iterative

(simple functions)

ML Compound
Estimate

iterative

(simple functions)

iterative

(numerical integration)

unknown
complexity

unknown
complexity

{θ(cid:12)(cid:12) θ = λ˜θ + (1 − λ)˘θ  ˜θ ∈ S}  for speciﬁc values of λ and ˘θ. In general  for a given λ and ˘θ ∈ S 

3.1 Example: Regularized Pmfs
The shadow Dirichlet can be designed to specify a distribution over a set of regularized pmfs SM =
the following d × d matrix M will change the support to the desired subset SM by mapping the
extreme points of S to the extreme points of SM :

(2)
where I is the d × d identity matrix. In Section 4 we show that the M given in (2) is optimal in a
maximum entropy sense.

M = (1 − λ)˘θ1T + λI 

3

3.2 Example: Bounded Variation Pmfs

We describe how to use the shadow Dirichlet to model a random pmf that has bounded variation
such that |θk − θl| ≤ k l for any k  (cid:96) ∈ {1  2  . . .   d} and k l > 0. To construct speciﬁed bounds
on the variation  we ﬁrst analyze the variation for a given M. For any d × d left stochastic matrix
M  θ = M ˜θ =
  so the difference between any two entries is

(cid:104)(cid:80)d

j=1 M1j

˜θj

j=1 Mdj

˜θj

. . . (cid:80)d
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)

j

(cid:105)T
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:88)

j

|θk − θl| =

(Mkj − Mlj)˜θj

|Mkj − Mlj| ˜θj.

(3)

Thus  to obtain a distribution over pmfs with bounded |θk − θ(cid:96)| ≤ k l for any k  (cid:96) components  it is
sufﬁcient to choose components of the matrix M such that |Mkj − Mlj| ≤ k l for all j = 1  . . .   d
because ˜θ in (3) sums to 1.
One way to create such an M is using the regularization strategy described in Section 3.1. For this
= λ˜θj + (1− λ)˘θj  and thus the variation between the
M ˜θ
case  the jth component of θ is θj =
ith and jth component of any pmf in SM is:

(cid:12)(cid:12)(cid:12)λ˜θi + (1 − λ)˘θi − λ˜θj − (1 − λ)˘θj

j

(cid:12)(cid:12)(cid:12) ≤ λ

(cid:12)(cid:12)(cid:12)˜θi − ˜θj

(cid:12)(cid:12)(cid:12) + (1 − λ)

(cid:12)(cid:12)(cid:12)˘θi − ˘θj

(cid:12)(cid:12)(cid:12)

|θi − θj| =

(cid:16)
(cid:17)
(cid:12)(cid:12)(cid:12)˘θi − ˘θj

(cid:12)(cid:12)(cid:12) .

≤ λ + (1 − λ) max

i j

(4)

Thus by choosing an appropriate λ and regularizing pmf ˘θ  one can impose the bounded variation
given by (4). For example  set ˘θ to be the uniform pmf  and choose any λ ∈ (0  1)  then the matrix
M given by (2) will guarantee that the difference between any two entries of any pmf drawn from
the shadow Dirichlet (M  α) will be less than or equal to λ.

3.3 Example: Monotonic Pmfs

For pmfs over ordered components  it may be desirable to restrict the support of the random pmf
distribution to only monotonically increasing pmfs (or to only monotonically decreasing pmfs).
A d × d left-stochastic matrix M that will result in a shadow Dirichlet that generates only mono-
tonically increasing d × 1 pmfs has kth column [0 . . . 0 1/(d − k + 1) . . . 1/(d − k + 1)]T   we
call this the monotonic M. It is easy to see that with this M only monotonic θ’s can be produced 
˜θ1 + 1
˜θ2 and so on. In Section 4 we show
because θ1 = 1
d−1
d
that the monotonic M is optimal in a maximum entropy sense.
Note that to provide support over both monotonically increasing and decreasing pmfs with one
distribution is not achievable with a shadow Dirichlet  but could be achieved by a mixture of two
shadow Dirichlets.

˜θ1 which is less than or equal to θ2 = 1

d

3.4 What Restricted Subsets are Possible?

Above we have described solutions to construct M for three kinds of dependence that arise in
machine learning applications. Here we consider the more general question: What subsets of the
simplex can be the support of the shadow Dirichlet  and how to design a shadow Dirichlet for a par-
ticular support? For any matrix M  by the Krein-Milman theorem [10]  SM = MS is the convex
hull of its extreme points. If M is injective  the extreme points of SM are easy to specify  as a d × d
matrix M will have d extreme points that occur for the d choices of θ that have only one nonzero
component  as the rest of the θ will create a non-trivial convex combination of the columns of M 
and therefore cannot result in extreme points of SM by deﬁnition. That is  the extreme points of SM
are the d columns of M  and one can design any SM with d extreme points by setting the columns
of M to be those extreme pmfs.
However  if one wants the new support to be a polytope in the probability (d − 1)-simplex with
m > d extreme points  then one must use a fat M with d× m entries. Let S m denote the probability

4

(m − 1)-simplex  then the domain of the shadow Dirichlet will be MS m  which is the convex hull
of the m columns of M and forms a convex polytope in S with at most m vertices. In this case
M cannot be injective  and hence it is not bijective between S m and MS m. However  a density on
MS m can be deﬁned as:

(cid:90)

(cid:12)(cid:12) M ˜θ=θ}

(cid:89)

j

p(θ) =

1

B(α)

{˜θ

˜θαj−1

j

d˜θ.

(5)

On the other hand  if one wants the support to be a low-dimensional polytope subset of a higher-
dimensional probability simplex  then a thin d × m matrix M  where m < d  can be used to
implement this. If M is injective  then it has a left inverse M∗ that is a matrix of dimension m × d 
and the normalized pushforward of the original density can be used as a density on the image MS m:

p(θ) =

1

B(α)|det(M T M)|1/2

(M∗θ)αj−1

 

j

(cid:89)

j

If M is not injective then one way to determine a density is to use (5).

4

Information-theoretic Properties

In this section we note two information-theoretic properties of the shadow Dirichlet. Let Θ be drawn
from shadow Dirichlet density pM   and let its generating Dirichlet ˜Θ be drawn from pD. Then the
differential entropy of the shadow Dirichlet is h(pM ) = log |det(M)| + h(pD)  where h(pD) is
the differential entropy of its generating Dirichlet. In fact  the shadow Dirichlet always has less
entropy than its Dirichlet shadow because log |det(M)| ≤ 0  which can be shown as a corollary to
the following lemma (proof not included due to lack of space):
Lemma 4.1. Let {x1  . . .   xn} and {y1  . . .   yn} be column vectors in Rn. If each yj is a convex
i=1 γji = 1  γjk ≥ 0  ∀j  k ∈ {1  . . .   n} then
|det[y1  . . .   yn]| ≤ |det[x1  . . .   xn]|.

combination of the xi’s  i.e. yj = (cid:80)n

i=1 γjixi (cid:80)n

It follows from Lemma 4.1 that the constructive solutions for M given in (2) and the monotonic M
are optimal in the sense of maximizing entropy:
Corollary 4.1. Let Mreg be the set of left-stochastic matrices M that parameterize shadow Dirichlet

distributions with support in {θ (cid:12)(cid:12) θ = λ˜θ + (1 − λ)˘θ  ˜θ ∈ S}  for a speciﬁc choice of λ and ˘θ.

Then the M given in (2) results in the shadow Dirichlet with maximum entropy  that is  (2) solves
arg maxM∈Mreg h(pM ).
Corollary 4.2. Let Mmono be the set of left-stochastic matrices M that parameterize shadow
Dirichlet distributions that generate only monotonic pmfs. Then the monotonic M given in Sec-
tion 3.3 results in the shadow Dirichlet with maximum entropy  that is  the monotonic M solves
arg maxM∈Mmono h(pM ).

5 Estimating the Distribution from Data

In this section  we discuss the estimation of α for the shadow Dirichlet and compound shadow
Dirichlet  and the estimation of M.

5.1 Estimating α for the Shadow Dirichlet

Let matrix M be speciﬁed (for example  as described in the subsections of Section 3)  and let q be a
d × N matrix where the ith column qi is the ith sample pmf for i = 1 . . . N  and let (qi)j be the jth
component of the ith sample pmf for j = 1  . . .   d. Then ﬁnding the maximum likelihood estimate

5

of α for the shadow Dirichlet is straightforward:

N(cid:89)

i=1

arg max
α∈Rk
+

log

(cid:20)
 1

(cid:21)N
(cid:89)
(˜qi)αj−1

+ log

j

(cid:89)
  

i

(cid:89)
(M−1qi)αj−1

j

j



(6)

(cid:89)

1

B(α)|det(M)|

B(α)N

i

j

p(qi|α) ≡ arg max
α∈Rk
+

log

≡ arg max
α∈Rk
+

log

where ˜q = M−1q. Note (6) is the maximum likelihood estimation problem for the Dirichlet dis-
tribution given the matrix ˜q  and can be solved using the standard methods for that problem (see
e.g. [11  12]).

5.2 Estimating α for the Compound Shadow Dirichlet

For many machine learning applications the given data are modeled as samples from realizations
of a random pmf  and given these samples one must estimate the random pmf model’s parameters.
We refer to this case as the compound shadow Dirichlet  analogous to the compound Dirichlet (also
called the multivariate P´olya distribution). Assuming one has already speciﬁed M  we ﬁrst discuss
method of moments estimation  and then describe an expectation-maximization (EM) method for
computing the maximum likelihood estimate ˘α.
One can form an estimate of α by the method of moments. For the standard compound Dirichlet 
one treats the samples of the realizations as normalized empirical histograms  sets the normalized
α parameter equal to the empirical mean of the normalized histograms  and uses the empirical
variances to determine the precision α0. By deﬁnition  this estimate will be less likely than the
maximum likelihood estimate  but may be a practical short-cut in some cases. For the compound
shadow Dirichlet  we believe the method of moments estimator will be a poorer estimate in general.
The problem is that if one draws samples from a pmf θ from a restricted subset SM of the simplex 
then the normalized empirical histogram ˘θ of those samples may not be in SM . For example given
a monotonic pmf  the histogram of ﬁve samples drawn from it may not be monotonic. Then the
empirical mean of such normalized empirical histograms may not be in SM   and so setting the
shadow Dirichlet mean M α equal to the empirical mean may lead to an infeasible estimate (one that
is outside SM ). A heuristic solution is to project the empirical mean into SM ﬁrst  for example  by
ﬁnding the nearest pmf in SM in squared error or relative entropy. As with the compound Dirichlet 
this may still be a useful approach in practice for some problems.
Next we state an EM method to ﬁnd the maximum likelihood estimate ˘α. Let s be a d × N matrix
of sample histograms from different experiments  such that the ith column si is the ith histogram
for i = 1  . . .   N  and (si)j is the number of times we have observed the jth event from the ith
pmf vi. Then the maximum log-likelihood estimate of α solves arg max log p(s|α) for α ∈ Rk
+.
If the random pmfs are drawn from a Dirichlet distribution  then ﬁnding this maximum likelihood
estimate requires an iterative procedure  and can be done in several ways including a gradient descent
(ascent) approach. However  if the random pmfs are drawn from a shadow Dirichlet distribution 
then a direct gradient descent approach is highly inconvenient as it requires taking derivatives of
numerical integrals. However  it is practical to apply the expectation-maximization (EM) algorithm
[13][14]  as we describe in the rest of this section. Code to perform the EM estimation of α can be
downloaded from idl.ee.washington.edu/publications.php.

We assume that the experiments are independent and therefore p(s|α) = p({si}|α) =(cid:81)

i p(si|α)

+

and hence arg maxα∈Rk
To apply the EM method  we consider the complete data to be the sample histograms s and the
pmfs that generated them (s  v1  v2  . . .   vN )  whose expected log-likelihood will be maximized.
Speciﬁcally  because of the assumed independence of the {vi}  the EM method requires one to
repeatedly maximize the Q-function such that the estimate of α at the (m + 1)th iteration is:

+

(cid:80)
i log p(si|α).

log p(s|α) = arg maxα∈Rk

α(m+1) = arg max
α∈Rk
+

Evi|si α(m) [log p(vi|α)] .

(7)

N(cid:88)

i=1

6

(cid:0)M−1vi

(cid:1)   where pD α is the Dirichlet distribution with parameter α 

Like the compound Dirichlet likelihood  the compound shadow Dirichlet likelihood is not neces-
sarily concave. However  note that the Q-function given in (7) is concave  because log p(vi|α) =
− log |det(M)| + log pD α
and by a theorem of Ronning [11]  log pD α is a concave function  and adding a constant does not
change the concavity. The Q-function is a ﬁnite integration of such concave functions and hence
also concave [15].
We simplify (7) without destroying the concavity to yield the equivalent problem α(m+1) =
arg max g(α) for α ∈ Rk
j=1 βjαj  and
βj = 1
N

  where tij and zi are integrals we compute with Monte Carlo integration:

j=1 log Γ(αj) + (cid:80)d

(cid:80)N

i=1

tij
zi

+  where g(α) = log Γ(α0) − (cid:80)d
(cid:90)
(cid:90)

log(M−1vi)jγi

d(cid:89)

(vi)(si)k

SM

k=1

k

(vi)jk(si)kpM (vi |α(m))dvi 

pM (vi |α(m))dvi

tij =

zi =

d(cid:89)

k=1

γi

SM

where γi is the normalization constant for the multinomial with histogram si.
We apply the Newton method [16] to maximize g(α)  where the gradient ∇g(α) has kth component
ψ0(α0) − ψ0(α1) + β1  where ψ0 denotes the digamma function. Let ψ1 denote the trigamma
function  then the Hessian matrix of g(α) is: H = ψ1(α0)11T − diag (ψ1(α1)  . . .   ψ1(αd)) .
Note that because H has a very simple structure  the inversion of H required by the Newton
step is greatly simpliﬁed by using the Woodbury identity [17]: H−1 = − diag(ξ1  . . .   ξd) −
ξ0−(cid:80)d

[ξiξj]d×d  where ξ0 = 1

ψ1(αj )  j = 1  . . .   d.

ψ1(α0) and ξj = 1

1
j=1 ξj

5.3 Estimating M for the Shadow Dirichlet

Thus far we have discussed how to construct M to achieve certain desired properties and how to
interpret a given M’s effect on the support. In some cases it may be useful to estimate M directly
from data  for example  ﬁnding the maximum likelihood M. In general  this is a non-convex problem
because the set of rank d − 1 matrices is not convex. However  we offer two approximations. First 
note that as in estimating the support of a uniform distribution  the maximum likelihood M will
correspond to a support that is no larger than needed to contain the convex hull of sample pmfs.
Second  the mean of the empirical pmfs will be in the support  and thus a heuristic is to set the kth
column of M (which corresponds to the kth vertex of the support) to be a convex combination of the
kth vertex of the standard probability simplex and the empirical mean pmf. We provide code that
ﬁnds the d optimal such convex combinations such that a speciﬁced percentage of the sample pmfs
are within the support  which reduces the non-convex problem of ﬁnding the maximum likelihood
d × d matrix M to a d-dimensional convex relaxation.

6 Demonstrations

It is reasonable to believe that if the shadow Dirichlet better matches the problem’s statistics  it will
perform better in practice  but an open question is how much better? To motivate the reader to
investigate this question further in applications  we provide two small demonstrations.

6.1 Verifying the EM Estimation

We used a broad suite of simulations to test and verify the EM estimation. Here we include a simple
visual conﬁrmation that the EM estimation works: we drew 100 i.i.d. pmfs from a shadow Dirichlet
with monotonic M for d = 3 and α = [3.94 2.25 2.81] (used in [18]). From each of the 100 pmfs 
we drew 100 i.i.d. samples. Then we applied the EM algorithm to ﬁnd the α for both the standard
compound Dirichlet  and the compound shadow Dirichlet with the correct M. Fig. 2 shows the true
distribution and the two estimated distributions.

7

True Distribution
(Shadow Dirichlet)

Estimated Shadow Dirichlet

Estimated Dirichlet

Figure 2: Samples were drawn from the true distribution and the given EM method was applied to
form the estimated distributions.

6.2 Estimating Proportions from Sales

Manufacturers often have constrained manufacturing resources  such as equipment  inventory of raw
materials  and employee time  with which to produce multiple products. The manufacturer must
decide how to proportionally allocate such constrained resources across their product line based on
their estimate of proportional sales. Manufacturer Artifact Puzzles gave us their past retail sales
data for the 20 puzzles they sold during July 2009 through Dec 2009  which we used to predict
the proportion of sales expected for each puzzle. These estimates were then tested on the next ﬁve
months of sales data  for January 2010 through April 2010. The company also provided a similarity
between puzzles S  where S(A  B) is the proportion of times an order during the six training months
included both puzzle A and B if it included puzzle A. We compared treating each of the six training
months of sales data as a sample from a compound Dirichlet versus or a compound shadow Dirichlet.
For the shadow Dirichlet  we normalized each column of the similarity matrix S to sum to one so
that it was left-stochastic  and used that as the M matrix; this forces puzzles that are often bought
together to have closer estimated proportions. We estimated each α parameter by EM to maximize
the likelihood of the past sales data  and then estimated the future sales proportions to be the mean
of the estimated Dirichlet or shadow Dirichlet distribution. We also compared with treating all
six months of sales data as coming from one multinomial which we estimated as the maximum
likelihood multinomial  and to taking the mean of the six empirical pmfs.

Table 2: Squared errors between estimates and actual proportional sales.

Multinomial Mean Pmf Dirichlet

Jan.
Feb.
Mar.
Apr.

7 Summary

.0129
.0185
.0231
.0240

.0106
.0206
.0222
.0260

.0109
.0172
.0227
.0235

Shadow Dirichlet
.0093
.0164
.0197
.0222

In this paper we have proposed a variant of the Dirichlet distribution that naturally captures some of
the dependent structure that arises often in machine learning applications. We have discussed some
of its theoretical properties  and shown how to specify the distribution for regularized pmfs  bounded
variation pmfs  monotonic pmfs  and for any desired convex polytopal domain. We have derived the
EM method and made available code to estimate both the shadow Dirichlet and compound shadow
Dirichlet from data. Experimental results demonstrate that the EM method can estimate the shadow
Dirichlet effectively  and that the shadow Dirichlet may provide worthwhile advantages in practice.

8

References
[1] B. Frigyik  A. Kapila  and M. R. Gupta  “Introduction to the Dirichlet distribution and related

processes ” Tech. Rep.  University of Washington  2010.

[2] C. Zhai and J. Lafferty  “A study of smoothing methods for language models applied to infor-

mation retrieval ” ACM Trans. on Information Systems  vol. 22  no. 2  pp. 179–214  2004.

[3] Y. Chen  E. K. Garcia  M. R. Gupta  A. Rahimi  and L. Cazzanti  “Similarity-based classiﬁca-
tion: Concepts and algorithms ” Journal of Machine Learning Research  vol. 10  pp. 747–776 
March 2009.

[4] R. Nallapati  T. Minka  and S. Robertson  “The smoothed-Dirichlet distribution: a building

block for generative topic models ” Tech. Rep.  Microsoft Research  Cambridge  2007.
[5] Aitchison  Statistical Analysis of Compositional Data  Chapman Hall  New York  1986.
[6] R. J. Connor and J. E. Mosiman  “Concepts of independence for proportions with a general-
ization of the Dirichlet distibution ” Journal of the American Statistical Association  vol. 64 
pp. 194–206  1969.

[7] K. Pearson  “Mathematical contributions to the theory of evolution–on a form of spurious
correlation which may arise when indices are used in the measurement of organs ” Proc. Royal
Society of London  vol. 60  pp. 489–498  1897.

[8] A. Ongaro  S. Migliorati  and G. S. Monti  “A new distribution on the simplex containing the

Dirichlet family ” Proc. 3rd Compositional Data Analysis Workshop  2008.

[9] W. S. Rayens and C. Srinivasan  “Dependence properties of generalized Liouville distributions
on the simplex ” Journal of the American Statistical Association  vol. 89  no. 428  pp. 1465–
1470  1994.

[10] Walter Rudin  Functional Analysis  McGraw-Hill  New York  1991.
[11] G. Ronning  “Maximum likelihood estimation of Dirichlet distributions ” Journal of Statistical

Computation and Simulation  vol. 34  no. 4  pp. 215221  1989.

[12] T. Minka  “Estimating a Dirichlet distribution ” Tech. Rep.  Microsoft Research  Cambridge 

2009.

[13] A. P. Dempster  N. M. Laird  and D. B. Rubin  “Maximum likelihood from incomplete data
via the EM algorithm ” Journal of the Royal Statistical Society: Series B (Methodological) 
vol. 39  no. 1  pp. 1–38  1977.

[14] M. R. Gupta and Y. Chen  Theory and Use of the EM Method  Foundations and Trends in

Signal Processing  Hanover  MA  2010.

[15] R. T. Rockafellar  Convex Analysis  Princeton University Press  Princeton  NJ  1970.
[16] S. Boyd and L. Vandenberghe  Convex Optimization  Cambridge University Press  Cambridge 

2004.

[17] K. B. Petersen and M. S. Pedersen  Matrix Cookbook  2009  Available at matrixcookbook.com.
[18] R. E. Madsen  D. Kauchak  and C. Elkan  “Modeling word burstiness using the Dirichlet

distribution ” in Proc. Intl. Conf. Machine Learning  2005.

9

,Tomer Koren
Kfir Levy