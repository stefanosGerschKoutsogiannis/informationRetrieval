2019,Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning,We study two time-scale linear stochastic approximation algorithms  which can be used to model well-known reinforcement learning algorithms such as GTD  GTD2  and TDC. We present finite-time performance bounds for the case where the learning rate is fixed. The key idea in obtaining these bounds is to use a Lyapunov function motivated by singular perturbation theory for linear differential equations. We use the bound to design an adaptive learning rate scheme which significantly improves the convergence rate over the known optimal polynomial decay rule in our experiments  and can be used to potentially improve the performance of any other schedule where the learning rate is changed at pre-determined time instants.,Finite-Time Performance Bounds and Adaptive

Learning Rate Selection for Two Time-Scale

Reinforcement Learning

Harsh Gupta
ECE and CSL

R. Srikant
ECE and CSL

University of Illinois at Urbana-Champaign

University of Illinois at Urbana-Champaign

hgupta10@illinois.edu

rsrikant@illinois.edu

Lei Ying
EECS

University of Michigan  Ann Arbor

leiying@umich.edu

Abstract

We study two time-scale linear stochastic approximation algorithms  which can
be used to model well-known reinforcement learning algorithms such as GTD 
GTD2  and TDC. We present ﬁnite-time performance bounds for the case where the
learning rate is ﬁxed. The key idea in obtaining these bounds is to use a Lyapunov
function motivated by singular perturbation theory for linear differential equations.
We use the bound to design an adaptive learning rate scheme which signiﬁcantly
improves the convergence rate over the known optimal polynomial decay rule in
our experiments  and can be used to potentially improve the performance of any
other schedule where the learning rate is changed at pre-determined time instants.

1

Introduction

A key component of reinforcement learning algorithms is to learn or approximate value functions
under a given policy [Sutton  1988]  [Bertsekas and Tsitsiklis  1996]  [Szepesvári  2010]  [Bertsekas 
2011]  [Bhatnagar et al.  2012]  [Sutton and Barto  2018]. Many existing algorithms for learning
value functions are variants of the temporal-difference (TD) learning algorithms [Sutton  1988] 
[Tsitsiklis and Van Roy  1997]  and can be viewed as stochastic approximation algorithms for
minimizing the Bellman error (or objectives related to the Bellman error). Characterizing the
convergence of these algorithms  such as TD(0)  TD(λ)  GTD   nonlinear GTD has been an important
objective of reinforcement learning [Szepesvári  2010]  [Bhatnagar et al.  2009]  and [Sutton et al. 
2016]. The asymptotic convergence of these algorithms with diminishing steps has been established
using stochastic approximation theory in many prior works (comprehensive surveys on stochastic
approximations can be found in [Benveniste et al.  2012]  [Kushner and Yin  2003]  and [Borkar 
2009]).
The conditions required for theoretically establishing asymptotic convergence in an algorithm with
diminishing step sizes imply that the learning rate becomes very small very quickly. As a result 
the algorithm will require a very large number of samples to converge. Reinforcement learning
algorithms used in practice follow a pre-determined learning rate (step-size) schedule which  in
most cases  uses decaying step sizes ﬁrst and then a ﬁxed step size. This gap between theory and
practice has prompted a sequence of works on ﬁnite-time performance of temporal difference learning
algorithms with either time-varying step sizes or constant step sizes [Dalal et al.  2017a b  Liu et al. 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2018  Lakshminarayanan and Szepesvari  2018  Bhandari et al.  2018  Srikant and Ying  2019]. Most
of these results are for single time-scale TD algorithms  except [Dalal et al.  2017b] which considers
two time-scale algorithms with decaying step sizes. Two time-scale TD algorithms are an important
class of reinforcement learning algorithms because they can improve the convergence rate of TD
learning or remedy the instability of single time-scale TD in some cases. This paper focuses on two
time-scale linear stochastic approximation algorithms with constant step sizes. The model includes
TDC  GTD and GTD2 as special cases (see [Sutton et al.  2008]  [Sutton et al.  2009] and [Szepesvári 
2010] for more details). We note that  in contemporaneous work  [Xu et al.  2019] have carried out a
two-time-scale analysis of linear stochastic approximation with diminishing step sizes.
Besides the theoretical analysis of ﬁnite-time performance of two time-scale reinforcement learning
algorithms  another important aspect of reinforcement learning algorithms  which is imperative in
practice but has been largely overlooked  is the design of learning rate schedule  i.e.  how to choose
proper step-sizes to improve the learning accuracy and reduce the learning time. This paper addresses
this important question by developing principled heuristics based on the ﬁnite-time performance
bounds.
The main contributions of this paper are summarized below.
• Finite Time Performance Bounds: We study two time-scale linear stochastic approximation
algorithms  driven by Markovian samples. We establish ﬁnite time bounds on the mean-square
error with respect to the ﬁxed point of the corresponding ordinary differential equations (ODEs).
The performance bound consists of two parts: a steady-state error and a transient error  where the
steady-state error is determined by the step sizes but independent of the number of samples (or
number of iterations)  and the transient error depends on both step sizes and the number of samples.
The transient error decays geometrically as the number of samples increases. The key differences
between this paper and [Dalal et al.  2017b] include (i) we do not require a sparse projection step
in the algorithm; and (ii) we assume constant step-sizes which allows us to develop the adaptive
step-size selection heuristic mentioned next.

• Adaptive Learning Rate Selection: Based on the ﬁnite-time performance bounds  in particular 
the steady-state error and the transient error terms in the bounds  we propose an adaptive learning
rate selection scheme. The intuition is to use a constant learning rate until the transient error is
dominated by the steady-state error; after that  running the algorithm further with the same learning
rate is not very useful and therefore  we reduce the learning rate at this time. To apply adaptive
learning rate selection in a model-free fashion  we develop data-driven heuristics to determine the
time at which the transient error is close to the steady-state error. A useful property of our adaptive
rate selection scheme is that it can be used with any learning rate schedule which already exists in
many machine learning software platforms: one can start with the initial learning rate suggested by
such schedules and get improved performance by using our adaptive scheme. Our experiments on
Mountain Car and Inverted Pendulum show that our adaptive learning rate selection signiﬁcantly
improves the convergence rates as compared to optimal polynomial decay learning rate strategies
(see [Dalal et al.  2017b] and [Konda et al.  2004] for more details on polynomial decay step-size
rules).

2 Model  Notation and Assumptions

We consider the following two time-scale linear stochastic approximation algorithm:

Uk+1 = Uk + α (Auu(Xk)Uk + Auv(Xk)Vk + bu(Xk))
Vk+1 = Vk + β (Avu(Xk)Uk + Avv(Xk)Vk + bv(Xk))  

(1)

the change in V is O(1) while the change in U is O(cid:0)α−β(cid:1) . Therefore  V is updated at a faster time

where {Xk} are the samples from a Markov process. We assume β < α so that  over −β iterations 

scale than U.
In the context of reinforcement learning  when combined with linear function approximation of
the value function  GTD  GTD2  and and TDC can be viewed as two time-scale linear stochastic
approximation algorithms  and can be described in the same form as (1). For example  GTD2 with

2

linear function approximation is as follows:

Uk+1 =Uk + α (φ(Xk) − ζφ(Xk+1)) φ(cid:62)(Xk)Vk

Vk+1 =Vk + β(cid:0)δk − φ(cid:62)(Xk)Vk

(cid:1) φ(Xk) 

where ζ is the discount factor  φ(x) is the feature vector of state x  Uk is the weight vec-
tor such that φ(cid:62)(x)Uk is the approximation of value function of state x at iteration k  δk =
c(Xk) + ζφ(cid:62)(Xk+1)Uk − φ(cid:62)(Xk)Uk is the TD error  and Vk is the weight vector that estimates
E[φ(Xk)φ(Xk)T ]−1E[δkφ(Xk)].
We now summarize the notation we use throughout the paper and the assumptions we make.
• Assumption 1: {Xk} is a Markov chain with state space S. We assume that the following two

limits exist:

(cid:18) ¯Auu
(cid:0)¯bu

¯Avu

(cid:19)
(cid:18)E [Auu(Xk)] E [Auv(Xk)]
(cid:19)
(cid:1) = lim
k−→∞
E [Avu(Xk)] E [Avv(Xk)]
= lim
k−→∞ (E[bu(Xk)] E[bv(Xk)]) = 0.

¯Auv
¯Avv
¯bv

Note that without the loss of generality  we assume ¯b = 0 which allows for the ﬁxed point of the
associated ODEs to be 0. This can be guaranteed by appropriate centering. We deﬁne

B(Xk) =Auu(Xk) − Auv(Xk) ¯A−1

vv

¯Avu

˜B(Xk) =Avu(Xk) − Avv(Xk) ¯A−1

vv

¯Avu

¯B = ¯Auu − ¯Auv ¯A−1

vv Avu

¯˜B = ¯Avu − ¯Avv ¯A−1

vv

¯Avu.

• Assumption 2: We assume that max{(cid:107)bu(x)(cid:107) (cid:107)bv(x)(cid:107)} ≤ bmax < ∞ for any x ∈ S. We
also assume that max{(cid:107)B(x)(cid:107) (cid:107) ˜B(x)(cid:107) (cid:107)Auu(x)(cid:107) (cid:107)Avu(x)(cid:107) (cid:107)Auv(x)(cid:107) (cid:107)Avv(x)(cid:107)} ≤ 1 for
any x ∈ S. Note that these assumptions imply that the steady-state limits of the random matri-
ces/vectors will also satisfy the same inequalities.
• Assumption 3: We assume ¯Avv and ¯B are Hurwitz and ¯Avv is invertible. Let Pu and Pv be the

solutions to the following Lyapunov equations:

−I = ¯B(cid:62)Pu + Pu ¯B
−I = ¯A(cid:62)

vvPv + Pv ¯Avv.

Since both ¯Avv and ¯B are Hurwitz  Pu and Pv are real positive deﬁnite matrices.

• Assumption 4: Deﬁne τ∆ ≥ 1 to be the mixing time of the Markov chain {Xk}. We assume

(cid:107)E[bk|X0 = i](cid:107) ≤ ∆ ∀i ∀k ≥ τ∆
(cid:107) ¯B − E[B(Xk)|X0 = i](cid:107) ≤ ∆ ∀i ∀k ≥ τ∆
(cid:107) ¯˜B − E[ ˜B(Xk)|X0 = i](cid:107) ≤ ∆ ∀i ∀k ≥ τ∆
(cid:107) ¯Auv − E[Auv(Xk)|X0 = i](cid:107) ≤ ∆ ∀i ∀k ≥ τ∆
(cid:107) ¯Avv − E[Avv(Xk)|X0 = i](cid:107) ≤ ∆ ∀i ∀k ≥ τ∆.

∆ = 2α(cid:0)1 + (cid:107) ¯A−1

¯Avu(cid:107) + β−α(cid:1)

4   where ˜ = ∆ = 2α(cid:0)1 + (cid:107) ¯A−1

vv

vv

¯Avu(cid:107) + β−α(cid:1) .
(cid:33)

• Assumption 5: As in [Srikant and Ying  2019]  we assume that there exists K ≥ 1 such that

τ∆ ≤ K log( 1

∆ ). For convenience  we choose

and drop the subscript from τ∆  i.e.  τ∆ = τ. Also  for convenience  we assume that  is small
enough such that ˜τ ≤ 1

We further deﬁne the following notation:
• Deﬁne matrix

(cid:32) ξv
where ξu = 2(cid:107)Pu ¯Auv(cid:107) and ξv = 2(cid:13)(cid:13)Pv ¯A−1

P =

vv

ξu+ξv
0

0
ξu

ξu+ξv

Pu

¯Avu ¯B(cid:13)(cid:13) .

 

Pv

(2)

• Let γmax and γmin denote the largest and smallest eigenvalues of Pu and Pv  respectively. So γmax

and γmin are also upper and lower bounds on the eigenvalues of P.

3

3 Finite-Time Performance Bounds

To establish the ﬁnite-time performance guarantees of the two time-scale linear stochastic approxima-
tion algorithm (1)  we deﬁne

Zk = Vk + ¯A−1

vv

¯AvuUk

and Θk =

(cid:18)Uk

(cid:19)

Zk

.

Then we consider the following Lyapunov function:
W (Θk) = Θ(cid:62)

k P Θk 

(3)
where P is a symmetric positive deﬁnite matrix deﬁned in (2) (P is positive deﬁnite because both
Pu and Pv are positive deﬁnite matrices). The reason to introduce Zk will become clear when we
introduce the key idea of our analysis based on singular perturbation theory.
The following lemma bounds the expected change in the Lyapunov function in one time step.
Lemma 1. For any k ≥ τ and   α  and β such that η1˜τ + 2 ˜2
holds:

2   the following inequality

α γmax ≤ κ1

(cid:16) κ1

− κ2α−β(cid:17) E[W (Θk)] + 2βτ η2 

where ˜ = 2α(cid:0)1 + (cid:107) ¯A−1

E[W (Θk+1) − W (Θk)] ≤ − α
γmax

¯Avu(cid:107) + β−α(cid:1)   and η1  η2 κ1  and κ2 are constants independent of .

2

vv

The proof of Lemma 1 is somewhat involved  and is provided in the supplementary material. The
deﬁnitions of η1  η2  κ1 and κ2 can be found in the supplementary material as well. Here  we
provide some intuition behind the result by studying a related ordinary differential equation (ODE).
In particular  consider the expected change in the stochastic system divided by the slow time-scale
step size α:

E[Uk+1 − Uk|Uk−τ = u  Vk−τ = v  Xk−τ = x]
=E [ (Auu(Xk)Uk + Auv(Xk)Vk + bu)| Uk−τ = u  Vk−τ = v  Xk−τ = x]
α−β
=E [ (Avu(Xk)Uk + Avv(Xk)Vk + bv(Xk))| Uk−τ = u  Vk−τ = v  Xk−τ = x]  

E[Vk+1 − Vk|Uk−τ = u  Vk−τ = v  Xk−τ = x]

α

α

(4)

where the expectation is conditioned sufﬁciently in the past in terms of the underlying Markov chain
(i.e. conditioned on the state at time k − τ instead of k) so the expectation is approximately in
steady-state.
Approximating the left-hand side by derivatives and the right-hand side using steady-state expecta-
tions  we get the following ODEs:

˙u = ¯Auuu + ¯Auvv
α−β ˙v = ¯Avuu + ¯Avvv.

(5)
(6)
Note that  in the limit as  → 0  the second of the above two ODEs becomes an algebraic equation 
instead of a differential equation. In the control theory literature  such systems are called singularly-
perturbed differential equations  see for example [Kokotovic et al.  1999]. In [Khalil  2002  Chapter
11]  the following Lyapunov equation has been suggested to study the stability of such singularly
perturbed ODEs:

W (u  v) = du(cid:62)Puu + (1 − d)(cid:0)v + ¯A−1

¯Avuu(cid:1)(cid:62)

(7)
for d ∈ [0  1]. The function W mentioned earlier in (3) is the same as above for a carefully chosen d.
The rationale behind the use of the Lyapunov function (7) is presented in the appendix.
The intuition behind the result in Lemma 1 can be understood by studying the dynamics of the above
Lyapunov function in the ODE setting. To simplify the notation  we deﬁne z = v + ¯A−1
¯Avuu  so
the Lyapunov function can also be written as

Pv

vv

vv

vv

(cid:0)v + ¯A−1

¯Avuu(cid:1)  

W (u  z) = du(cid:62)Puu + (1 − d)z(cid:62)Pvz 

(8)

4

and adapting the manipulations for nonlinear ODEs in [Khalil  2002  Chapter 11] to our linear model 
we get

˙W =2duT Pu ˙u + 2(1 − d)z(cid:62)Pv ˙z

≤ − ((cid:107)u(cid:107)

(cid:107)z(cid:107)) ˜Ψ

(cid:19)

(cid:18)(cid:107)u(cid:107)
(cid:0) 1−d
−dγmax − (1 − d)γmaxσmin
2α−β − (1 − d)γmaxσmin

(cid:107)z(cid:107)

 

(cid:1)(cid:19)

≥ (dγmax + (1 − d)γmaxσmin)2  

where

˜Ψ =

d

−dγmax − (1 − d)γmaxσmin

(cid:18)
(cid:18) 1 − d
2α−β − (1 − d)γmaxσmin

(cid:19)

Note that ˜Ψ is positive deﬁnite when

d

i.e.  when

.

(9)

(10)

(11)

(12)

(13)

(14)

α−β ≤

2d(1 − d)γmaxσmin + (dγmax + (1 − d)γmaxσmin)2 .

d(1 − d)

Let ˜λmin denote the smallest eigenvalue of ˜Ψ. We have

˙W ≤ −˜λmin

(cid:0)(cid:107)u(cid:107)2 + (cid:107)z(cid:107)2(cid:1) ≤ − ˜λmin

W.

γmax

In particular  recall that we obtained the ODEs by dividing by the step-size α. Therefore  for the
discrete equations  we would expect

E[W (Θk+1) − W (Θk)] ≈≤ −α

˜λmin
γmax

E [W (Θk)]  

(15)

which resembles the transient term of the upper bound in Lemma 1. The exact expression in the
discrete  stochastic case is of course different and additionally includes a steady-state term  which is
not captured by the ODE analysis above.
Now  we are ready to the state the main theorem.
Theorem 1. For any k ≥ τ    α and β such that η1˜τ + 2 ˜2

α γmax ≤ κ1

2   we have

(cid:18)

− κ2α−β(cid:17)(cid:19)k−τ

(cid:16) κ1
(cid:0) κ1
2 − κ2α−β(cid:1) .

2
η2τ

1 − α
E[(cid:107)Θk(cid:107)2] ≤ γmax
γmin
γmax
+ 2β−α γmax
γmin

(1.5(cid:107)Θ0(cid:107) + 0.5bmax)2

Proof. Applying Lemma 1 recursively  we obtain

E[W (Θk)] ≤ uk−τE[W (Θτ )] + v

2 − κ2α−β(cid:1) and v = η2τ 2β. Also  we have that
(cid:0) κ1

≤ uk−τE[W (Θk)] + v

1 − uk−τ
1 − u

1

1 − u

where u = 1 − α

γmax

E[(cid:107)Θk(cid:107)2] ≤ 1
γmin

E[W (Θk)] ≤ 1
γmin

uk−τE[W (Θτ )] + v

1

γmin(1 − u)

.

Furthermore 

E[W (Θτ )] ≤ γmaxE[(cid:107)Θτ(cid:107)2] ≤ γmaxE[((cid:107)Θτ − Θ0(cid:107) + (cid:107)Θ0(cid:107))2]

≤ γmax ((1 + 2˜τ )(cid:107)Θ0(cid:107) + 2˜τ bmax)2 .

The theorem then holds using the fact that ˜τ ≤ 1
4.

(16)

(17)

(18)

Theorem 1 essentially states that the expected error for a two-time scale linear stochastic approx-
imation algorithm comprises two terms: a transient error term which decays geometrically with
time and a steady-state error term which is directly proportional to 2β−α and the mixing time. This
characterization of the ﬁnite-time error is useful in understanding the impact of different algorithmic
and problem parameters on the rate of convergence  allowing the design of efﬁcient techniques such
as the adaptive learning rate rule which we will present in the next section.

5

4 Adaptive Selection of Learning Rates

Equipped with the theoretical results from the previous section  one interesting question that arises
is the following: given a time-scale ratio λ = α
β   can we use the ﬁnite-time performance bound to
design a rule for adapting the learning rate to optimize performance?
In order to simplify the discussion  let β = µ and α = µλ. Therefore  Theorem 1 can be simpliﬁed
and written as

(cid:18)

(cid:18) κ1

E[(cid:107)Θk(cid:107)2] ≤K1

1 − µλ

− κ2
γmax

µλ−1

2γmax

(cid:19)(cid:19)k

+ µ2−λ

2 − κ2µλ−1(cid:1)
(cid:0) κ1

K2

(19)

(20)

where K1 and K2 are problem-dependent positive constants. Since we want the system to be stable 
µλ−1 = c > 0. Plugging this condition
we will assume that µ is small enough such that
in (19)  we get

− κ2

γmax

κ1

2γmax

(cid:0)1 − cµλ(cid:1)k

K2µ2−λ
γmaxc

+

E[(cid:107)Θk(cid:107)2] ≤K1

In order to optimize performance for a given number of samples  we would like to choose the learning
rate µ as a function of the time step. In principle  one can assume time-varying learning rates  derive
more general mean-squared error expressions (similar to Theorem 1)  and then try to optimize over
the learning rates to minimize the error for a given number of samples. However  this optimization
problem is computationally intractable. We note that even if we assume that we are only going to
change the learning rate a ﬁnite number of times  the resulting optimization problem of ﬁnding the
times at which such changes are performed and ﬁnding the learning rate at these change points is an
equally intractable optimization problem. Therefore  we have to devise simpler adaptive learning rate
rules.
To motivate our learning rate rule  we ﬁrst con-
sider a time T such that errors due to the tran-
sient and steady-state parts in (20) are equal  i.e. 

K1(1 − cµλ)T =

K2µ2−λ
γmaxc

(21)

Figure 1: The evolution of (cid:107)Θk − Θ0(cid:107).

From this time onwards  running the two time-
scale stochastic approximation algorithm any
further with µ as the learning rate is not going
to signiﬁcantly improve the mean-squared error.
In particular  the mean-squared error beyond
this time is upper bounded by twice the steady-
state error K2µ2−λ
γmaxc . Thus  at time T  it makes
sense to reset µ as µ ← µ/ξ  where ξ > 1 is
a hyperparameter. Roughly speaking  T is the
time at which one is close to steady-state for a given learning rate  and therefore  it is the time to
reduce the learning rate to get to a new "steady-state" with a smaller error.
The key difﬁculty in implementing the above idea is that it is difﬁcult to determine T . For ease of
exposition  we considered a system centered around 0 in our analysis (i.e.  Θ∗ = 0). More generally 
the results presented in Theorem 1 and (19) - (20) will have Θk replaced by Θk − Θ∗. In any practical
application  Θ∗ will be unknown. Thus  we cannot determine (cid:107)Θk − Θ∗(cid:107) as a function of k and
hence  it is difﬁcult to use this approach.
Our idea to overcome this difﬁculty is to estimate whether the algorithm is close to its steady-state by
observing (cid:107)Θk − Θ0(cid:107) where Θ0 is our initial guess for the unknown parameter vector and is thus
known to us. Note that (cid:107)Θk − Θ0(cid:107) is zero at k = 0 and will increase (with some ﬂuctuations due
to randomness) to (cid:107)Θ∗ − Θ0(cid:107) in steady-state (see Figure 1 for an illustration). Roughly speaking 
we approximate the curve in this ﬁgure by a sequence of straight lines  i.e.  perform a piecewise
linear approximation  and conclude that the system has reached steady-state when the lines become
approximately horizontal. We provide the details next.
To derive a test to estimate whether (cid:107)Θk − Θ0(cid:107) has reached steady-state  we ﬁrst note the following
inequality for k ≥ T (i.e.  after the steady-state time deﬁned in (21)):

6

(cid:115)

E[(cid:107)Θ0 − Θ∗(cid:107)] − E[(cid:107)Θk − Θ∗(cid:107)] ≤E[(cid:107)Θk − Θ0(cid:107)] ≤ E[(cid:107)Θk − Θ∗(cid:107)] + E[(cid:107)Θ0 − Θ∗(cid:107)]

⇒ d −

2K2µ2−λ
γmaxc

≤E[(cid:107)Θk − Θ0(cid:107)] ≤ d +

2K2µ2−λ
γmaxc

(22)

(cid:115)

where the ﬁrst pair of inequalities follow from the triangle inequality and the second pair of inequalities
follow from (20) - (21)  Jensen’s inequality and letting d = E[(cid:107)Θ0 − Θ∗(cid:107)]. Now  for k ≥ T   consider
the following N points: {Xi = i  Yi = (cid:107)Θk+i − Θ0(cid:107)}N
i=1. Since these points are all obtained after
“steady-state" is reached  if we draw the best-ﬁt line through these points  its slope should be small.
More precisely  let ψN denote the slope of the best-ﬁt line passing through these N points. Using
(22) along with formulas for the slope in linear regression  and after some algebraic manipulations
(see Appendix ?? for detailed calculations)  one can show that:

(cid:33)

(cid:32)

2

µ1− λ
N

|E[ψN ]| = O

  Var(ψN ) = O

(cid:18) 1

(cid:19)

N 2

(cid:18)

(23)

(cid:19)

(cid:19)

(cid:18)

µ1− λ
N

µ

λ
2

  then the slope of the best-ﬁt line connecting {Xi  Yi} will be O

Therefore  if N ≥ χ
with high probability (for a sufﬁciently large constant χ > 0). On the other hand  when the algorithm
is in the transient state  the difference between (cid:107)Θk+m − Θ0(cid:107) and (cid:107)Θk − Θ0(cid:107) will be O(mµ) since
Θk changes by O(µ) from one time slot to the next (see Lemma 3 in Appendix ?? for more details).
Using this fact  the slope of the best-ﬁt line through N consecutive points in the transient state can
be shown to be O (µ)  similar to (23). Since we choose N ≥ χ
  the slope of the best-ﬁt line in

µ1− λ
N

2

λ
2

µ

2

steady state  i.e.  O
will be lower than the slope of the best-ﬁt line in the transient phase 
i.e.  O (µ) (for a sufﬁciently large χ). We use this fact as a diagnostic test to determine whether or
not the algorithm has entered steady-state. If the diagnostic test returns true  we update the learning
rate (see Algorithm 1).

2   Θ0  Θini = Θ0.

Do two time-scale algorithm update.

Algorithm 1 Adaptive Learning Rate Rule

Hyperparameters: ρ  σ  ξ  N
Initialize µ = ρ  ψN = 2σµ1− λ
for i = 1  2  ... do

Compute ψN = Slope(cid:0){k (cid:107)Θi−k − Θini(cid:107)}N−1

We note that our adaptive learning
rate rule will also work for single
time-scale reinforcement learning al-
gorithms such as TD(λ) since our ex-
pressions for the mean-square error 
when specialized to the case of a sin-
gle time-scale  will recover the re-
sult in [Srikant and Ying  2019] (see
[Gupta et al.  2019] for more details).
Therefore  an interesting question that
arises from (19) is whether one can op-
timize the rate of convergence with re-
spect to the time-scale ratio λ? Since
the RHS in (19) depends on a variety
of problem-dependent parameters  it
is difﬁcult to optimize it over λ. An in-
teresting direction of further research
is to investigate if practical adaptive strategies for λ can be developed in order to improve the rate of
convergence further.

if ψN < σµ1− λ

µ = µ
ξ .
Θini = Θi.

end if
end for

(cid:1).

2

then

k=0

N

5 Experiments

We implemented our adaptive learning rate schedule on two popular classic control problems in
reinforcement learning - Mountain Car and Inverted Pendulum  and compared its performance with
the optimal polynomial decay learning rate rule suggested in [Dalal et al.  2017b] (described in the
next subsection). See Appendix ?? for more details on the Mountain Car and Inverted Pendulum
problems. We evaluated the following policies using the two time-scale TDC algorithm (see [Sutton
et al.  2009] for more details regarding TDC):

7

(a) Mountain Car

(b) Inverted Pendulum

Figure 2: Performance of different learning rate rules in classic control problems.

• Mountain Car - At each time step  choose a random action ∈ {0  2}  i.e.  accelerate randomly to
• Inverted Pendulum - At each time step  choose a random action in the entire action space  i.e. 

the left or right.
apply a random torque ∈ [−2.0  2.0] at the pivot point.

Since the true value of Θ∗ is not known in both the problems we consider  to quantify the performance
of the TDC algorithm  we used the error metric known as the norm of the expected TD update (NEU 
see [Sutton et al.  2009] for more details). For both problems  we used a O(3) Fourier basis (see
[Konidaris et al.  2011] for more details) to approximate the value function and used 0.95 as the
discount factor.

5.1 Learning Rate Rules and Tuning
1. The optimal polynomial decay rule suggested in [Dalal et al.  2017b] is the following: at time
3. For our experiments 
step k  choose α
β = 1.5. Since the problems we considered
we chose α = 0.99 and β = 0.66. This implies λ = α
require smaller initial step-sizes for convergence  we let α
(k+1)β and did a
grid search to determine the best ρ0  i.e.  the best initial learning rate. The following values for ρ0
were found to be the best: Mountain Car - ρ0 = 0.2  Inverted Pendulum - ρ0 = 0.2.

(k+1)β   where α → 1 and β → 2

(k+1)α and β

k = 1

k = 1

k = ρ0

(k+1)α and β

k = ρ0

2. For our proposed adaptive learning rate rule  we ﬁxed ξ = 1.2  N = 200 in both problems since
we did not want the decay in the learning rate to be too aggressive and the resource consumption
for slope computation to be high. We also set λ = 1.5 as in the polynomial decay case to have a
fair comparison. We then ﬁxed ρ and conducted a grid search to ﬁnd the best σ. Subsequently 
we conducted a grid search over ρ. Interestingly  the adaptive learning rate rule was reasonably
robust to the value of ρ. We used ρ = 0.05 in Inverted Pendulum and ρ = 0.1 in Mountain Car.
Effectively  the only hyperparameter that affected the rule’s performance signiﬁcantly was σ. The
following values for σ were found to be the best: Mountain Car - σ = 0.001  Inverted Pendulum -
σ = 0.01.

5.2 Results

For each experiment  one run involved the following: 10  000 episodes with the number of iterations
in each episode being 50 and 200 for Inverted Pendulum and Mountain Car respectively. After every
1  000 episodes  training/learning was paused and the NEU was computed by averaging over 1  000
test episodes. We initialized Θ0 = 0. For Mountain Car  50 such runs were conducted and the results
were computed by averaging over these runs. For Inverted Pendulum  100 runs were conducted and
the results were computed by averaging over these runs. Note that the learning rate for each adaptive
strategy was adapted at the episodic level due to the episodic nature of the problems. The results are
reported in Figures 2a and 2b. As is clear from the ﬁgures  our proposed adaptive learning rate rule
signiﬁcantly outperforms the optimal polynomial decay rule.

8

12345678910Number of Episodes (in Thousands)01234567NEUPolynomial DecayAdaptive Rule6 Conclusion

We have presented ﬁnite-time bounds quantifying the performance of two time-scale linear stochastic
approximation algorithms. The bounds give insight into how the different time-scale and learning rate
parameters affect the rate of convergence. We utilized these insights and designed an adaptive learning
rate selection rule. We implemented our rule on popular classical control problems in reinforcement
learning and showed that the proposed rule signiﬁcantly outperforms the optimal polynomial decay
strategy suggested in literature.

Acknowledgements

Research supported by ONR Grant N00014-19-1-2566  NSF Grants CPS ECCS 1739189  NeTS
1718203  CMMI 1562276  ECCS 16-09370  and NSF/USDA Grant AG 2018-67007-28379. Lei
Ying’s work supported by NSF grants CNS 1618768  ECCS 1609202  IIS 1715385  ECCS 1739344 
CNS 1824393 and CNS 1813392.

References
A. Benveniste  M. Métivier  and P. Priouret. Adaptive algorithms and stochastic approximations 

volume 22. Springer Science & Business Media  2012.

D. P. Bertsekas. Dynamic programming and optimal control 3rd edition  volume II. Belmont  MA:

Athena Scientiﬁc  2011.

D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena  1996.

J. Bhandari  D. Russo  and R. Singal. A ﬁnite time analysis of temporal difference learning with

linear function approximation. arXiv preprint arXiv:1806.02450  2018.

S. Bhatnagar  H. L. Prasad  and L. A. Prashanth. Stochastic recursive algorithms for optimization:

simultaneous perturbation methods  volume 434. Springer  2012.

Shalabh Bhatnagar  Doina Precup  David Silver  Richard S Sutton  Hamid R Maei  and Csaba
Szepesvári. Convergent temporal-difference learning with arbitrary smooth function approximation.
In Advances in Neural Information Processing Systems  pages 1204–1212  2009.

V. S. Borkar. Stochastic approximation: a dynamical systems viewpoint. Springer  2009.

G. Dalal  B. Szörényi  G. Thoppe  and S. Mannor. Finite sample analyses for TD(0) with function

approximation. arXiv preprint arXiv:1704.01161  2017a. Also appeared in AAAI 2018.

G. Dalal  B. Szorenyi  G. Thoppe  and S. Mannor. Finite sample analysis of two-timescale stochastic
approximation with applications to reinforcement learning. arXiv preprint arXiv:1703.05376 
2017b. Also appeared in COLT 2018.

Harsh Gupta  R Srikant  and Lei Ying. Adaptive learning rate selection for temporal difference

learning. Real-world Sequential Decision Making Workshop  ICML  2019.

H. K. Khalil. Nonlinear Systems  volume 3. Prentice hall Upper Saddle River  NJ  2002.

P. Kokotovic  H. K. Khalil  and J. O’Reilly. Singular perturbation methods in control: analysis and

design  volume 25. SIAM  1999.

Vijay R Konda  John N Tsitsiklis  et al. Convergence rate of linear two-time-scale stochastic

approximation. The Annals of Applied Probability  14(2):796–819  2004.

G. Konidaris  S. Osentoski  and P. Thomas. Value function approximation in reinforcement learning

using the fourier basis. In Twenty-ﬁfth AAAI conference on artiﬁcial intelligence  2011.

H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications 

volume 35. Springer Science & Business Media  2003.

9

C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant
step-size and iterate averaging go? In International Conference on Artiﬁcial Intelligence and
Statistics  pages 1347–1355  2018.

Bo Liu  Ian Gemp  Mohammad Ghavamzadeh  Ji Liu  Sridhar Mahadevan  and Marek Petrik.
Proximal gradient temporal difference learning: Stable reinforcement learning with polynomial
sample complexity. Journal of Artiﬁcial Intelligence Research  63:461–494  2018.

R. Srikant and L. Ying. Finite-time error bounds for linear stochastic approximation and TD learning.

Conference on Learning Theorey (COLT)  2019. ArXiv preprint arXiv:1902.00923.

R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning  3(1):

9–44  1988.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press  2018.

R. S. Sutton  H. R. Maei  D. Precup  S. Bhatnagar  D. Silver  C. Szepesvári  and E. Wiewiora. Fast
gradient-descent methods for temporal-difference learning with linear function approximation. In
Proceedings of the 26th Annual International Conference on Machine Learning  pages 993–1000.
ACM  2009.

Richard S Sutton  Csaba Szepesvári  and Hamid Reza Maei. A convergent O(n) algorithm for
off-policy temporal-difference learning with linear function approximation. Advances in neural
information processing systems  21(21):1609–1616  2008.

Richard S Sutton  A Rupam Mahmood  and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. The Journal of Machine Learning Research  17(1):
2603–2631  2016.

C. Szepesvári. Algorithms for reinforcement learning. Synthesis lectures on Artiﬁcial Intelligence

and Machine Learning  4(1):1–103  2010.

J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approxi-

mation. IEEE Transactions on Automatic Control  42(5)  1997.

Tengyu Xu  Shaofeng Zou  and Yingbin Liang. Two time-scale off-policy td learning: Non-asymptotic
analysis over markovian samples. In Advances in Neural Information Processing Systems  pages
10633–10643  2019.

10

,Harsh Gupta
R. Srikant
Lei Ying