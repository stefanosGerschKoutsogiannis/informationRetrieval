2019,Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model,This paper focuses on the problem of computing an $\epsilon$-optimal policy in a discounted Markov Decision Process (MDP) provided that we can access the reward and transition function through a generative model. We propose an algorithm that is initially agnostic to the MDP but that can leverage the specific MDP structure  expressed in terms of variances of the rewards and next-state value function  and gaps in the optimal action-value function to reduce the sample complexity needed to find a good policy  precisely highlighting the contribution of each state-action pair to the final sample complexity. A key feature of our analysis is that it removes all horizon dependencies in the sample complexity of suboptimal actions except for the intrinsic scaling of the value function and a constant additive term.,Almost Horizon-Free Structure-Aware

Best Policy Identiﬁcation with a Generative Model

Institute for Computational and Mathematical Engineering 

Andrea Zanette

Stanford University  CA
zanette@stanford.edu

Mykel J. Kochenderfer

Emma Brunskill

Department of Aeronautics and Astronautics 

Department of Computer Science 

Stanford University  CA
mykel@stanford.edu

Stanford University  CA

ebrun@cs.stanford.edu

Abstract

This paper focuses on the problem of computing an ǫ-optimal policy in a discounted
Markov Decision Process (MDP) provided that we can access the reward and
transition function through a generative model. We propose an algorithm that is
initially agnostic to the MDP but that can leverage the speciﬁc MDP structure 
expressed in terms of variances of the rewards and next-state value function  and
gaps in the optimal action-value function to reduce the sample complexity needed
to ﬁnd a good policy  precisely highlighting the contribution of each state-action
pair to the ﬁnal sample complexity. A key feature of our analysis is that it removes
all horizon dependencies in the sample complexity of suboptimal actions except
for the intrinsic scaling of the value function and a constant additive term.

1

Introduction

A key goal is to design reinforcement learning (RL) agents that can leverage problem structure to
efﬁciently learn a good policy  especially in problems with very long time horizons. Ideally the RL
algorithm should be able to adjust without apriori information about the problem structure. Formal
analyses that characterize the performance of such algorithms yielding instance-dependent bound
help to advance our core understanding of the characteristics that govern the hardness of learning to
make good decisions under uncertainty.

Though there is relatively limited work in reinforcement learning  strong problem-dependent guar-
antees are available for multi-armed bandits. In particular  well known bounds for online learning
scale as a function of the gap between the expected reward of a particular action and the optimal
action [ABF02] and also on the variance of the rewards [AMS09]. In the pure exploration setting
in bandits  which is related to the setting we consider in this paper  there exist multiple algorithms
with problem-dependent bounds [EMM06; MM94; MSA08; Jam+14; BMS09; ABM10; GGL12;
KKS13] of this form. Ideally the complexity of learning to make good decisions in reinforcement
learning tasks would scale with previously identiﬁed quantities of gap and variance over the next
value function. As a step towards this  in this paper we introduce an algorithm for an RL agent
operating in a discrete state and action space that has access to a generative model and can leverage
problem-dependent structure to have strong instance-dependent PAC sample complexity bounds
that are a function of the variance of the rewards and next state value functions  as well as the gaps
between the optimal and suboptimal state-action values. While the sequential setting brings additional
difﬁculties due to possibly long horizon  our bounds also show that in the dominant terms  our

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

approach avoids suffering any horizon dependence for suboptimal actions beyond the scaling of the
value function. This signiﬁcantly improves in statistical efﬁciency over prior worst-case bounds for
the generative model case [GAMK13; Sid+18] and matches existing worst-case bounds in worst-case
settings.

To do so we introduce a novel algorithm structure that acquires samples of state-action pairs in
iterative rounds. A slight variant of the well known simulation lemma (see e.g. [KMN02]) suggests
that in order to improve our estimate of the optimal value function and policy  it is sufﬁcient to ensure
that after each round of sampling  the conﬁdence intervals shrink over the MDP parameter estimates
of both the state–action pairs visited by the optimal policy and the state–action pairs visited by the
empirically-optimal policy. While of course both are unknown  we show that we can implicitly
maintain a set of candidate policies that are ǫ-accurate  and by ensuring that we shrink the conﬁdence
sets of all state–action pairs likely to be visited by any such policy  we are also guaranteed (with
high probability) to shrink the conﬁdence intervals of the optimal policy. Interestingly we can show
that by focusing on such state–action pairs  we can avoid the horizon dependence on suboptimal
actions. The key idea is to take into account the MDP learned dynamics to enforce a constraint on
the suboptimality of the candidate policies. The sampling strategy is derived by solving a minimax
problem that minimizes the number of samples to guarantee that every policy in the set of candidate
policies is accurately estimated. Importantly  this minimax problem can be reformulated as a convex
minimization problem that can be solved with any standard solver for convex optimization.

Our algorithmic approach is quite different from many prior approaches  both in the generative
model setting and the online setting. When a generative model is available  the available worst-case
optimal algorithms [AMK12; Sid+18] allocate samples uniformly to all state and action pairs. We
show our approach can be substantialy more effective for general case of MDPs with heterogeneous
structure  and even for the pathologically hard instances because of the reduced horizon dependence
on suboptimal actions. Note too that our approach is quite different from online RL algorithms that
often (implicitly) allocate exploration budget to state-action pairs encountered by the policy with
the most optimistic upper bound [JOA10; AOM17; OVR13; DB15; DLB17; SLL09; LH14]  since
here we explicitly reason about the reduction in the conﬁdence intervals across a large set of policies
whose value is near the empirical optimal value at this round.

2 Notation and Preliminaries

We consider discounted inﬁnite horizon MDPs [SB18]  which are deﬁned by a tuple M =
hS  A  p  r  γi  where S and A are the state and action spaces with cardinality S and A  respec-
tively. We denote by p(s′ | s  a) the probability of transitioning to state s′ after taking action a in
state s while r(s  a) ∈ [0  1] is the average instantaneous reward collected and R(s  a) ∈ [0  1] the
corresponding random variable. The vector value function of policy π is denoted with V π. If ρ is the

initial starting distribution then V (ρ) =Ps ρsV (s). The value function of the optimal policy π⋆ is

denoted with V ⋆ = V π⋆
. We call Var R(s  a) and Varp(s a) V ⋆ the variance of R(s  a) and of V ⋆(s′)
where s′ ∼ p(s  a). The agent interacts with the MDP via a generative model that takes as input a
(s  a) pair and returns a random sample of the reward R(s  a) and a random next state s+ according
to the transition model s+ ∼ p(s  a). The reinforcement learning agent maintains an empirical MDP

subscript indicates what iteration/episode they refer to. We denote with wπ ρ
t=0 γt Pr(s  a  t  ρ)
the discounted sum of visit probabilities Pr(s  a  t  ρ) to the (s  a) pair in timestep t if the starting

cMk = hS  A bpk brk  γi for every iteration/episode k  and the maximum likelihood transitionbpk(s  a)
k is the empirical estimate using MDP cMk
sa samples. The bV ⋆
and rewardsbrk(s  a) have received nk
k. Variables with a hat refer to the empirical MDP cMk and the
of the empirical optimal policybπ⋆
is its analogous on cMk. We use the ˜O(·) notation to
state is drawn uniformly from ρ and bw

indicate a quantity that depends on (·) up to a polylog expression of a quantity at most polynomial
1
in S  A 
δ   where δ is the “failure probability”. Before proceeding  we ﬁrst recall the following
lemma from [GAMK13]:

sa =P∞

1

1−γ

π k ρ
sa

2

Lemma 2 (Simulation Lemma for Optimal Value Function Estimate [GAMK13]). With probability
at least 1 − δ  outside the failure event for any starting distribution ρ it holds that:

(V ⋆ −bV ⋆
(V ⋆ −bV ⋆

k )(ρ) ≤ X(s a)bw
k )(ρ) ≥ X(s a)bw

π⋆ k ρ
sa

k k ρ

bπ⋆
sa

(cid:0)(r −brk)(s  a) + γ(p −bpk)(s  a)⊤V ⋆(cid:1) ≤ X(s a)bw
(cid:0)(r −brk)(s  a) + γ(p −bpk)(s  a)⊤V ⋆(cid:1) ≥ −X(s a)bw

π⋆ k ρ
sa

CIsa(nk

sa)

k k ρ

bπ⋆
sa

CIsa(nk

sa)

The CIsa(nk
sa) are Bernstein’s conﬁdence intervals (deﬁned in more details in appendix A) after nk
sa
samples over the rewards and transitions and are function of the unknown rewards and transition
variances. The proof (see appendix) is a slight variation of lemma 3 in [GAMK13].

3 Sampling Strategy Given an Empirical MDP

We ﬁrst describe how our approach will allocate samples to state–action pairs given a current empirical
MDP  before presenting in the next section our full algorithm.

k (optimal on cMk). Since π⋆ andbπ⋆

and the empirical optimal policybπ⋆
bπ⋆
small for all state–action pairs leading to a small |(V ⋆ −bV ⋆
that the empirical optimal policybπ⋆
near-optimal). Therefore  in the main text we mostly focus on showing that |(V ⋆ −bV ⋆

Lemma 1 suggests that to estimate the optimal value function it sufﬁces to accurately estimate the
(s  a) pairs in the trajectories identiﬁed by two policies  namely the optimal policy π⋆ (optimal on M)
k are unknown (in particular 
k is a random variable prior to sampling)  a common strategy is to allocate an identical number of
samples uniformly [GAMK13; Sid+18] so that the conﬁdence intervals CIsa(nk
sa) are sufﬁciently
k )(ρ)|; from here it is possible to show
k is
k )(ρ)| is small 
and defer additional details to the appendix. The proposed approach is to proceed in iterations or
episodes. In each episode our algorithm implicitly maintains a set of candidate policies  which are
near-optimal  and allocates more samples to the (s  a) pairs visited by these policies to reﬁne their
estimated value. On the next episode those policies that are too suboptimal relative to their estimation
accuracy are implicitly discarded. In particular  the samples are placed in a way that is related to the
visit probabilities of the near-optimal empirical policies in addition to the variances of the reward and
transitions of state–action pairs encountered in potentially good policies.

k )(ρ)| is also small (sobπ⋆

k can be extracted and that |(V ⋆ − V bπ⋆

3.1 Oracle Minimax Program

Suppose we have already allocated some samples and have computed the maximum likelihood
k and know that the optimal value function estimate is at
k k∞ ≤ ǫk. How should we allocate further sampling resources to
improve the accuracy in the optimal value function estimate? The idea is given by the simulation
lemma (lemma 2): in order to see an improvement after sampling (i.e.  in the next episode k + 1)
sa ) in the

MDP cMk with empirical optimal policybπ⋆
least ǫk-accurate  i.e.  kV ⋆ −bV ⋆
the maximum likelihood MDP cMk+1 must have smaller conﬁdence intervals CIsa(nk+1
k+1 on cMk+1. Both are of course
(s  a) pairs visited by π⋆ and the empirical optimal policy bπ⋆
unknown. However  we introduce the constraint (bV ⋆
k − bV π
Cǫk-optimal policies (and starting distributions) on cMk. Here  C is a numerical constant that will
ensure that π⋆ andbπ⋆
ensureP(s a)bw
Lemma 2 ensures |(V ⋆ −bV ⋆

k+1 satisfy this condition and are therefore allocated enough samples. Given C
and ǫk  the idea is that we should choose a sampling strategy {nsa}sa with high enough samples to
k )(ρ) ≤ Cǫk so that

sa ) ≤ ǫk+1 for all policies that satisfy (bV ⋆
k −bV π
k )(ρ) ≤ Cǫk X(s a)

k+1)(ρ)| ≤ ǫk+1 = ǫk/2. This is equivalent to solving the following1:

k )(ρ) ≤ Cǫk that restricts sampling to

Deﬁnition 1 (Oracle Minimax Problem).

π k+1 ρ
sa

CIsa(nk+1

sa ) 

nsa ≤ nmax.

(1)

min

n

π k+1 ρ

CIsa(nk+1

max

π ρ X(s a)bw

s.t.

(bV ⋆
k −bV π

1For space  we omit the constraint ρs ≥ 0 and kρk1 = 1 on the starting distribution.

3

π k+1 ρ

Here the vector of the discounted sum of visit probabilitiesbw
system (I −γ(bP π
uses the next-episode empirical visit probabilitiesbw

is computable from the linear
= ρ and nmax is a guess on the number of samples needed to ensure
that the objective function is ≤ ǫk/2. We call this problem the oracle minimax problem because it
which are not known. In addition  it uses
the true variance of the next state value function (embedded in the deﬁnition of conﬁdence intervals
CIsa(nk

sa)). As these quantities are unknown in episode k  the program cannot be solved.

k+1)⊤)bw

π k+1 ρ

π k+1 ρ

3.2 Algorithm Minimax Program

This section shows how to construct a minimax program that is ‘close’ enough to the Oracle minimax

π k ρ

π k ρ

π k+1 ρ

π k+1 ρ

π k ρ

k
π k+1 ρ

π k+1 ρ
sa

CIsa(nk+1

− bw

and instead use the currently-

accurately leads to a high sample complexity; fortunately we are able to claim that the product
and the conﬁdence interval vector CI k+1
on the rewards and transitions is already small if policy π has received enough samples along its
trajectories before the current episode. Let us rewrite the objective function of equation 1 as a function

problem (Equation 1) but is function of only empirical quantities computable from cMk. The idea
is 1) to avoid using the next-episode empirical distributionbw
and 2) use the empirical variance of the next state value function Varbpk(s a)bV ⋆
computablebw
instead of the real  unknown variance Varp(s a) V ⋆. Estimating the visit distribution bw
between the visit distribution shift bw
of the visit distribution on cMk plus a term that takes into account the shift in the distribution from
cMk to cMk+1:
X(s a)bw
for both2 π = π⋆ andbπ⋆
(s  a) pair3  independent of the desired accuracy ǫk+1. This way we can ensure that we can usebw
instead ofbw

Lemma 9 in appendix allows us to claim that the rightmost summation above is less than 2Cp(nmin)ǫk
k+1. Here Cp(nmin) is deﬁned in appendix A and can be made (see lemma
16) for example < 1/100 by allocating a small constant number of samples ˜O(S/(1 − γ)2) to each
π k ρ

sa ) = X(s a) bw
sa| {z }

Now the only quantities that are not known by the algorithm are the variance of the transitions and
rewards that appear in the conﬁdence intervals CIsa(nk+1
sa ). Precisely  to estimate the variance of the
transitions Varp(s a) V ⋆ in the (s  a) pair  we need to known both the transition probability p(s  a)
and the true value function V ⋆  both of which are unknown. Fortunately it is possible to use the
k plus a fast-shrinking (as a function
of the number of samples) correction term. Since this analysis was similarly performed in prior
papers for this setting [GAMK13; Sid+18]  we defer its discussion to Lemma 11 in the appendix.
With these corrections (Bksa  deﬁned in appendix A  is the variance correction and 2ǫk/625 accounts
for the distribution shift) we can write the following minimax problem:

empirical transitionsbpk(s  a) and the empirical value functionbV ⋆

sa ) +X(s a)

plus a small correction term ≪ ǫk.

−bw
{z

CIsa(nk+1
sa )

Shift of Empirical Distributions

CIsa(nk+1

(bw
|

π k+1 ρ
sa

Computable

π k ρ
sa

)

}

π k+1 ρ

Deﬁnition 2 (Algorithm Minimax Problem).

sa ) + Bksa) + 2ǫk/625

min

n

max

π k ρ
sa

π ρ X(s a)bw
(bV ⋆
k −bV π

s.t.:

(cCI sa(nk+1
k )(ρ) ≤ Cǫk; X(s a)

nsa ≤ nmax;

π k ρ

= ρ.

(2)

(I − γ(bP π

k )⊤)bw

sa ) are the conﬁdence intervals evaluated with the empirical variances deﬁned in

Here cCI sa(nk+1
Appendix A. This program is fully expressed in terms of empirical quantities that depends on cMk.

As long as a solution to the above minimax program is ≤ ǫk/2 the oracle objective function will also

2Lemma 9 bounds this term as 2Cp(nmin)ǫπ

k for π = π⋆  π = bπ⋆

k+1  respectively; ǫπ

k is deﬁned in appendix
k ≤ ǫk we need an inductive argument

A and represents the “accuracy” of policy π in episode k. To ensure ǫπ
which is sketched out in the main theorem (Theorem 1).

3As we will shortly see  this will contribute only a constant term to the ﬁnal sample complexity.

4

be ≤ ǫk/2 at the solution of the program (for more details see Lemma 6 in the Appendix). In other
words  by solving the minimax program (def 2) we put enough samples to satisfy the oracle program
1  which ensures accuracy in the value function estimate through Lemma 2.

4 Algorithm

We now take the sampling approach
described in the previous section and
use it to construct an iterative al-
gorithm for quickly learning a near-
optimal or optimal policy given access
to a generative model. Speciﬁcally
we present BESt POlicy identiﬁcation
with no Knowledge of the Environ-
ment (BESPOKE) in Algorithm 1. The
algorithm proceeds in episodes. Each
episode starts with an empirical MDP

Algorithm 1 BESPOKE

Input: Failure probability δ > 0  accuracy ǫInput > 0
Set ǫ1 = 1
for k = 1  2  . . .

1−γ and allocate nmin samples to each (s  a) pair

for nmax = 20  21  22  . . .

Solve the optimization program of deﬁnition 7 (appendix)
if the optimal value of the program of deﬁnition 7 is ≤ ǫk
2

Break and return sampling strategy {nk+1
sa   ∀(s  a)

Query the generative model up to nk+1
k+1 and bV ⋆
Compute  bπ⋆
Set ǫk+1 = ǫk
2
if ǫk+1 ≤ ǫInput

k+1

sa }sa

Break and return the policy bπ⋆

cMk whose optimal value functionbV ⋆
is ǫk accurate kV ⋆ −bV ⋆
2 to halve the accuracy in the value function estimate  i.e.  kV ⋆ −bV ⋆

k
k k∞ ≤ ǫk un-
der an inductive assumption. The sam-
ples are allocated at each episode k by solving an optimization program equivalent to that in deﬁnition
k+1k∞ ≤ ǫk+1 = ǫk/2. In the
innermost loop of the algorithm the required number of samples for the next episode is guessed
nmax = 1  2  4  8  . . .   until nmax is sufﬁcient to ensure that the objective function of the minimax
problem of deﬁnition 2 will be ≤ ǫk/2; the purpose of the inner loop is to avoid putting more samples
than needed and allows us to obtain the sample complexity result of Theorem 2. In Appendix G we
reformulate the optimization program 2 (described more precisely in Deﬁnition 5 in the appendix)
obtaining a convex minimization program that avoids optimizing over the policy and instead works

k+1

π k ρ

convex optimization [BV04].

; this can be efﬁciently solved with standard techniques from

Theorem 1 (BESPOKE Works as Intended). With probability at least 1 − δ  in every episode k
k and its optimal

directly with the distribution bw
BESPOKE maintains an empirical MDP cMk such that its optimal value function bV ⋆
policybπ⋆

kV ⋆ − V bπ⋆

k k∞ ≤ ǫk 

k k∞ ≤ 2ǫk

k satisfy:

2   ∀k. In particular  when BESPOKE terminates in episode kF inal it holds that

kV ⋆ −bV ⋆

where ǫk+1
ǫInput

def
= ǫk

2 ≤ ǫkF inal ≤ ǫInput.

The proof is reported in the appendix  and shows by induction that for every episode k  π⋆ and
k −
)(ρ) ≤ Cǫk for all ρ and are therefore allocated enough samples;

k+1 are in the set of ‘candidate’ policies because they are near-optimal on cMk  satisfying (bV ⋆
bπ⋆
k −bV
k )(ρ) ≤ Cǫk and (bV ⋆
bV π⋆
this guarantees accuracy in bV ⋆

bπ⋆
k
k+1 by Lemma 2.

k+1

5 Sample Complexity Analysis

To analyze the sample complexity of BESPOKE we derive another optimization program function

of only problem dependent quantities. We 1) shift from the empirical visit distribution bw
cMk to the “real” visit distribution wπ ρ on M; 2) move from empirical conﬁdence intervals to those

evaluated with the true variances; and ﬁnally 3) relax the near-optimality constraint on the policies by
using Lemma 7 in the appendix (for an appropriate numerical constant C ⋆ > C) in order to be able
to use the value functions on M:

on

π k ρ

k )(ρ) ≤ Cǫk → (V ⋆ − V π)(ρ) ≤ C ⋆ǫk 

∀ρ.

(3)

(bV ⋆
k −bV π

5

In the end  we have enlarged the feasible set of the algorithm minimax problem while upper bounding
its objective function obtaining:4
Deﬁnition 3 (⋆-Minimax Program).

min

n

sa (CIsa(nk+1

sa ) + 2Bksa) + ǫk/25

(4)

(5)

subject to the constraints (r ∈ RSA is the reward vector):

nsa ≤ nmax;

(I − γ(P π)⊤)wπ ρ = ρ.

max

wπ ρ

wπ ρ X(s a)
≤ C ⋆ǫk; X(s a)

(V ⋆ − V π)(ρ)

V ⋆(ρ)−(wπ ρ)⊤r

|

{z

}

This is made rigorous in Lemma 6  but essentially we have obtained a minimax program whose
solution can be studied in terms of problem dependent quantities; in particular  its solution in terms
of number of samples nsa upper bounds the sample complexity of the algorithm in every episode.

Problem Dependent Analysis Due to space constraints  here we sketch the sample complexity
def
= V ⋆(s) − Q⋆(s  a) appear while simultane-
analysis of suboptimal actions to make the gaps ∆sa
ously eliminating the horizon dependence. We recall the following (e.g.  Lemma 5.2.1 in [Kak+03];
see also our appendix):

Lemma 1 (Sum of Losses). It holds that:

(V ⋆ − V π)(ρ) = X(s a)

|

def
= ∆sa

{z

) = X(s a)
}

wπ ρ

sa (Q⋆(s  π⋆(s)) − Q⋆(s  a)

wπ ρ

sa ∆sa

(20)

Lemma 1 expresses the value of a suboptimal policy as a sum of per-step losses ∆sa weighted by the
discounted sum of probabilities of being in that (s  a) pair. The key step that enables us to obtain
strong problem dependent bounds and to remove the horizon dependence for suboptimal actions is
sa Bksa + 3ǫk/625).
Lemma 1 (Gap-Conﬁdence Interval Lemma). If (π  ρ) satisﬁes (V ⋆ −V π)(ρ) ≤ C ⋆ǫk then a sample
complexity:

synthesized in the following short lemma  where we ignore the term (P(s a) 2wπ ρ
  

γ2 Varp(s a) V ⋆

(1 − γ)∆sa

Var R(s  a)

Transition Estimation

Reward Estimation

∆2
sa

∆2
sa

∆sa

+

nsa = ˜O
|

1

+

+

sufﬁces to ensure

{z

|

}

γ

∀(s  a)

(6)

(7)

wπ ρ

sa CIsa(nk+1

sa ) ≤

ǫk
2

.

{z

}
wπ ρ X(s a)

max

Proof. A direct computation shows that if nk+1

sa

satisﬁes equation 6 with appropriate constants5 then:

This justiﬁes the ﬁrst inequality below:

CIsa(nk+1

sa ) ≤

∆sa
2C ⋆ .

wπ ρ

sa CIsa(nk+1

sa ) ≤

X(s a)

1

2C ⋆ X(s a)

wπ ρ

sa ∆sa =

1
2C ⋆ (V ⋆ − V π)(ρ) ≤

1
2

ǫk.

(8)

(9)

The equality follows from lemma 1 and the last inequality from the constraint on the optimality of
π.

4The relaxed optimization program is over the distribution induced by the policy. Here  P π is the transition

matrix identiﬁed by the policy π on M.

5Note that  in particular  C ⋆ is a constant.

6

They key idea is that by having conﬁdence intervals of the same size as the gaps is sufﬁcient to
estimate the policy as accurately as its suboptimality gap (V ⋆ −V π)(ρ)  regardless of the horizon. By
augmenting this argument with the law of total variance [GAMK13]  splitting into further subcases 
and by taking into account the correction terms we obtain:

Theorem 2 (Sample Complexity of the Algorithm BESPOKE). With probability at least 1 − δ  the

total sample complexity of BESPOKE up to episode k is upper bounded byP(s a) nsa where nsa is

the total number of samples allocated to the (s  a) pair:

nsa = ˜O(cid:16) minn

1

Var R(s  a) + γ2 Varp(s a) V ⋆

(1 − γ)3(ǫk)2  
Var R(s  a) + γ2 Varp(s a) V ⋆

(1 − γ)2(ǫk)2

+

1

(1 − γ)2(ǫk)

 

(166)

+

1

(1 − γ)∆s a o +

γS

(1 − γ)2 (cid:17) .

(167)

∆2

s a

Notice that the BESPOKE would suffer a worst-case sample complexity similar to [GAMK13; Sid+18]
only in the initial phases of learning  i.e.  whenever ǫk is much larger than the gaps.

6 Signiﬁcance of the Bound

We motivate the importance of theorem 2 by specializing the result in two noteworthy cases.

Sample Complexity to Identify the Best Policy and the Worst-Case Lower Bound If the opti-
mal policy is unique  deﬁne the minimum gap ∆min = mins a a6=π⋆(a) ∆sa. To identify the optimal
policy we must set ǫInput ≤ ∆min and the sample complexity of BESPOKE at termination becomes:

Var R(s  π⋆(s)) + γ2 Varp(s π⋆(s)) V ⋆

(1 − γ)2∆2

min

 

1

min

(1 − γ)3∆2

˜O Xs
min(
{z
|
+ X(s a)|a6=π⋆(s)(cid:18) Var R(s  a) + γ2 Varp(s a) V ⋆
|

RULING-OUT SUBOPTIMAL ACTIONS

{z

∆2
sa

ESTIMATING π⋆

+

1

(1 − γ)∆sa(cid:19)
}

1

(1 − γ)2∆min)
}

γS2A
(1 − γ)2

!

+

+

(10)

|

CONSTANT

{z

}

N

S

(1−γ)3∆2

min

in ˜O(cid:16)
˜O(cid:16)

One of our core contributions is that we suffer a dependence on the horizon 1/(1 − γ) only in
estimating the optimal (s  a) pairs (ﬁrst summation over the state space). The summation over
suboptimal (s  a) is independent of the horizon  although of the horizon implicitly affects the scaling
of the variance Varp(s a) V ⋆ and explicitly the maximum value function range (term 1/(1 − γ)∆sa).
It is important to compare the above result with the established lower bound [GAMK13]
which is Ω(
(1−γ)3ǫ2 ) to obtain an ǫ-accurate policy  where N is the number of state-action
pairs. Since ∆sa = ∆min  ∀(s  a)  a 6= π⋆(s) in the lower bound construction and the
variance is maximum Varp(s a) V ⋆ ≤ 1/(1 − γ)2  we are able to identify the optimal policy

+ S2A

+ S(A−1)
(1−γ)2∆2

(1−γ)2(cid:17) samples which improves6 on the worst case bound
(1−γ)2(cid:17) of [GAMK13; Sid+18] by a full horizon factor for suboptimal ac-

tions. While our result can be surprising at ﬁrst  it does not contradict the lower bound:
the
lower bound makes no attempt to distinguish between optimal and suboptimal actions as it is only
expressed in terms of total (s  a) pairs N   and the construction uses a number of (s  a) pairs that
)
is a constant multiple of the state space cardinality  i.e.  one could only deduce Ω(
min
as a lower bound. Our result  therefore  does not violate the lower bound  but rather it shows
that while we must suffer an unavoidable worst-case 1/(1 − γ)3 factor on the state space corre-
sponding to the optimal (s  a) pairs  the dependence on the planning horizon is absent for sub-
optimal (s  a) except for the scaling of the value function implicit in the variance. Surprisingly 
excluding the constant term S2A
(1−γ)2   suboptimal (s  a) pairs get a combined number of samples

(1−γ)3∆2

+ S2A

(1−γ)3∆2

SA

min

min

S

6The paper [Sid+18] has the same bound as [GAMK13] but avoids the constant term S2A

(1−γ)2 .

7

˜OX(s a)(cid:18) Var R(s  a)

max{ǫ2

k  ∆2

sa}

+

1

max{ǫk  ∆sa}(cid:19) ≤ ˜OX(s a)

1
max{ǫ2
k  ∆2

sa}

(11)

˜O(cid:16)P(s a)|a6=π⋆(s)(cid:16) Var R(s a)+γ2 Varp(s a) V ⋆

∆2

sa

+

1

(1−γ)∆sa(cid:17)(cid:17) which is the sample complexity (ignor-

ing log and constant factors) that a variance-aware bandit algorithm for best arm identiﬁcation would
need (see e.g.  [GGL12]  appendix B) to ‘reject’ these suboptimal arms provided that it can obtain
samples7 of the random variable R(s  a) + γV ⋆(s′)  s′ ∼ p(s  a). In this case  however  the V ⋆
vector would need to be known to the bandit algorithm. In other words  the sample complexity of
BESPOKE at termination consists of two main terms: a leading order term with a dependence on the
state space with an unavoidable (due to the lower bound) dependence on the horizon 1
1−γ   and an
horizon-free bandit-like sample complexity to rule out suboptimal actions as if the optimal value
function V ⋆ was known.

BESPOKE applied to Bandits Finally  if γ = 0 we are in the bandit setting  and the sample
complexity of BESPOKE at step k becomes exactly (since Var R(s  a) ≤ 1):

This matches the best-known sample complexity bound for best arm identiﬁcation for tabular bandit
with gaps and variances [ABM10; GGL12] except for constants and log terms. This is encouraging
as it suggests it may be possible to have algorithms with a smooth transition in sample complexity as
a function of the discount factor when moving from a bandit to an RL setting.

7 Related Literature and Conclusion

Related Literature
In the more challenging setting of online exploration (i.e.  without a generative
model) the PAC literature [DB15; DLB17; LH14; SLL09] directly provides algorithms to identify an
ǫ-optimal policy with high probability in the worst-case. Gap-aware analyses exists  see for example
[BK97; TB08; OPT18] for asymptotic regret bounds on ergodic MDPs with matching upper and
lower bounds and with an emphasis on the minimum gap; since these analyses look at the asymptotic
regret they are not comparable to the proposal here. Very recently [SJ19] presents a gap-based
non-asymptotic regret bound for episodic MDPs but not yet free of the horizon and dependencies on
∆min. Gaps in MDPS have also been used to justify the observed relation between the value function
accuracy and the resulting policy performance [FSM10]. In addition  [EMM06; Bru10] also propose
an algorithm and PAC bounds that depend the minimum gap  but the results do not leverage recent
advances in tighter sample complexity analysis. [JOA10] presents a regret bound based on the same
quantity. The maximum variance of the next-state optimal value function is discussed in [MMM14;
ZB19].

The closest related work in the PAC setting similarly assumes access to a generative model  and
provides near-matching worst-case sample complexity upper and lower bounds [AMK12] for tabular
MDPs even in terms of computational complexity [Sid+18]. However  this work focuses on near-
optimal worst-case performance: as these algorithms allocate samples uniformly they do not adapt to
the problem structure. Finally  [Aga+19] show how to improve on the constant sample complexity
term for model based approaches like the one we use here; it is possible that their techniques can be
applied to our setting.

Conclusion This work leverages domain structure  notably the action-value function gaps  to
eliminate the impact of the horizon when ruling out suboptimal actions to identify a near-optimal
policy for discounted-reward Markov decision processes using a generative model  except for a
constant term and the inherent value function scaling. This is achieved through a tractable algorithm.
In doing so  our ﬁnite time sample complexity analysis quantiﬁes the sample complexity contribution
of each state-action pair as a function of the action-value function gaps and variances of the rewards
and next-state value function  and recovers the best-known bounds (excepts for logs and constants)
when deployed to bandit instances using these quantities.

Our work provides at least two important analytical tools: 1) the way we relate the suboptimality
of the policies with the gaps to reduce the dependence on the horizon is new  and could be used in

7Here  Var R(s  a) + γ2 Varp(s a) V ⋆ is the variance of the random variable R(s  a) + γV ⋆(s′) with

s′ ∼ p(s  a). Note the scaling of this random variable  which has range

1
1−γ .

8

other settings to make the gap appear while simultaneously reducing the horizon dependence 2) the
way we analyze the visit distribution shift induced by the policies  weighted by the local reward and
transition conﬁdence intervals  and show it is small  is another analytical contribution of our work
which can be extended to the settings where one is interested in obtaining a good policy from a given
starting distribution ρ as opposed to all starting states.

Acknowledgment

This work is partially supported by a Total Innovation Fellowship program  an NSF CAREER award
and an Ofﬁce of Naval Research Young Investigator Award. The authors are grateful to the reviewers
for the high-quality reviews and suggestions.

References

[ABF02]

[ABM10]

[Aga+19]

[AMK12]

[AMS09]

[AOM17]

[BK97]

[BMS09]

[Bru10]

[BV04]

[DB15]

[DLB17]

[EMM06]

[FSM10]

Peter Auer  Nicolo Cesa Bianchi  and Paul Fischer. “Finite-time Analysis of the Multiarmed
Bandit Problem”. In: Machine Learning (2002).
Jean-Yves Audibert  Sebastien Bubeck  and Remi Munos. “Best Arm Identiﬁcation in Multi-
Armed Bandits”. In: Conference on Learning Theory (COLT). 2010.
Alekh Agarwal et al. “Optimality and approximation with policy gradient methods in markov
decision processes”. In: arXiv preprint arXiv:1908.00261 (2019).
Mohammad Gheshlaghi Azar  Remi Munos  and Hilbert J. Kappen. “On the Sample Complexity
of Reinforcement Learning with a Generative Model”. In: International Conference on Machine
Learning (ICML). 2012.
Jean Yves Audibert  Remi Munos  and Csaba Szepesvari. “Exploration-exploitation trade-off
using variance estimates in multi-armed bandits”. In: Theoretical Computer Science (2009).
Mohammad Gheshlaghi Azar  Ian Osband  and Remi Munos. “Minimax Regret Bounds for
Reinforcement Learning”. In: International Conference on Machine Learning (ICML). 2017.
Apostolos N Burnetas and Michael N Katehakis. “Optimal adaptive policies for Markov decision
processes”. In: Mathematics of Operations Research 22.1 (1997)  pp. 222–255.
Sébastien Bubeck  Rémi Munos  and Gilles Stoltz. “Pure exploration in multi-armed bandits
problems”. In: International Conference on Algorithmic Learning Theory. 2009.
Emma Brunskill. “When Policies Can Be Trusted: Analyzing a Criteria to Identify Optimal
Policies in MDPs with Unknown Model Parameters.” In: International Conference on Automated
Planning and Scheduling (ICAPS). 2010  pp. 218–221.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press 
2004.
Christoph Dann and Emma Brunskill. “Sample Complexity of Episodic Fixed-Horizon Rein-
forcement Learning”. In: Advances in Neural Information Processing Systems (NIPS). 2015.
Christoph Dann  Tor Lattimore  and Emma Brunskill. “Unifying PAC and Regret: Uniform PAC
Bounds for Episodic Reinforcement Learning”. In: Advances in Neural Information Processing
Systems (NIPS). 2017.
Eyal Even-Dar  Shie Mannor  and Yishay Mansour. “Action Elimination and Stopping Con-
ditions for the Multi-Armed Bandit and Reinforcement Learning Problems”. In: Journal of
Machine Learning Research (2006).
Amir-massoud Farahmand  Csaba Szepesvári  and Rémi Munos. “Error propagation for approxi-
mate policy and value iteration”. In: Advances in Neural Information Processing Systems (NIPS).
2010.

[GAMK13] Mohammad Gheshlaghi Azar  Rémi Munos  and Hilbert J. Kappen. “Minimax PAC bounds
on the sample complexity of reinforcement learning with a generative model”. In: Machine
Learning 91.3 (June 2013)  pp. 325–349. DOI: 10.1007/s10994-013-5368-1. URL: https:
//doi.org/10.1007/s10994-013-5368-1.
Victor Gabillon  Mohammad Ghavamzadeh  and Alessandro Lazaric. “Best arm identiﬁcation:
A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence”. In: Advances in Neural Information
Processing Systems (NIPS). 2012  pp. 3212–3220.
Kevin Jamieson et al. “lil’ucb: An optimal exploration algorithm for multi-armed bandits”. In:
Conference on Learning Theory (COLT). 2014  pp. 423–439.
Thomas Jaksch  Ronald Ortner  and Peter Auer. “Near-optimal Regret Bounds for Reinforcement
Learning”. In: Journal of Machine Learning Research (2010).

[Jam+14]

[GGL12]

[JOA10]

9

[Kak+03]

[KKS13]

[KMN02]

[LH14]

[MM94]

[MMM14]

[MP09]

[MSA08]

[OPT18]

[OVR13]

[SB18]

[Sid+18]

[SJ19]

[SLL09]

[TB08]

[WBS07]

[Wei+03]

[ZB19]

Sham Machandranath Kakade et al. “On the sample complexity of reinforcement learning”.
PhD thesis. University of London London  England  2003.
Zohar Karnin  Tomer Koren  and Oren Somekh. “Almost optimal exploration in multi-armed
bandits”. In: International Conference on Machine Learning (ICML). 2013.
Michael Kearns  Yishay Mansour  and Andrew Y Ng. “A sparse sampling algorithm for near-
optimal planning in large Markov decision processes”. In: Machine Learning 49.2-3 (2002) 
pp. 193–208.
Tor Lattimore and Marcus Hutter. “Near-optimal PAC bounds for discounted MDPs”. In: Theo-
retical Computer Science 558 (2014)  pp. 125–143.
Oded Maron and Andrew W Moore. “Hoeffding races: Accelerating model selection search
for classiﬁcation and function approximation”. In: Advances in Neural Information Processing
Systems (NIPS). 1994  pp. 59–66.
Odalric-Ambrym Maillard  Timothy A. Mann  and Shie Mannor. ““How hard is my MDP?” The
distribution-norm to the rescue”. In: Advances in Neural Information Processing Systems (NIPS).
2014.
Andreas Maurer and Massimiliano Pontil. “Empirical Bernstein Bounds and Sample Variance
Penalization”. In: Conference on Learning Theory (COLT). 2009.
Volodymyr Mnih  Csaba Szepesvári  and Jean-Yves Audibert. “Empirical bernstein stopping”.
In: International Conference on Machine Learning (ICML). ACM. 2008  pp. 672–679.
Jungseul Ok  Alexandre Proutiere  and Damianos Tranos. “Exploration in Structured Rein-
forcement Learning”. In: Advances in Neural Information Processing Systems. 2018  pp. 8874–
8882.
Ian Osband  Benjamin Van Roy  and Daniel Russo. “(More) Efﬁcient Reinforcement Learning
via Posterior Sampling”. In: Advances in Neural Information Processing Systems (NIPS). 2013.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press 
2018.
Aaron Sidford et al. “Near-Optimal Time and Sample Complexities for for Solving Discounted
Markov Decision Process with a Generative Model”. In: Advances in Neural Information
Processing Systems (NIPS). 2018.
Max Simchowitz and Kevin Jamieson. “Non-Asymptotic Gap-Dependent Regret Bounds for
Tabular MDPs”. In: arXiv preprint arXiv:1905.03814 (2019).
Alexander L Strehl  Lihong Li  and Michael L Littman. “Reinforcement learning in ﬁnite MDPs:
PAC analysis”. In: Journal of Machine Learning Research 10.Nov (2009)  pp. 2413–2444.
Ambuj Tewari and Peter L Bartlett. “Optimistic linear programming gives logarithmic regret for
irreducible MDPs”. In: Advances in Neural Information Processing Systems. 2008  pp. 1505–
1512.
Tao Wang  Michael Bowling  and Dale Schuurmans. “Dual representations for dynamic program-
ming and reinforcement learning”. In: 2007 IEEE International Symposium on Approximate
Dynamic Programming and Reinforcement Learning. IEEE. 2007  pp. 44–51.
Tsachy Weissman et al. Inequalities for the l1 deviation of the empirical distribution. Tech. rep.
Hewlett-Packard Labs  2003.
Andrea Zanette and Emma Brunskill. “Tighter Problem-Dependent Regret Bounds in Reinforce-
ment Learning without Domain Knowledge using Value Function Bounds”. In: International
Conference on Machine Learning (ICML). 2019. URL: http://proceedings.mlr.press/
v97/zanette19a.html.

10

,Andrea Zanette
Mykel Kochenderfer
Emma Brunskill