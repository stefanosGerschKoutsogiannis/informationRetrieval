2009,Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering,Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a fixed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper  we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a non-parametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of fixed metric because its local distance metric is implicitly derived from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We present an efficient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning  and (ii) is computationally efficient for high dimensional data.,Learning Bregman Distance Functions and Its
Application for Semi-Supervised Clustering

Lei Wu†(cid:93)  Rong Jin‡  Steven C.H. Hoi†  Jianke Zhu(cid:92)  and Nenghai Yu(cid:93)
†School of Computer Engineering  Nanyang Technological University  Singapore
‡Department of Computer Science & Engineering  Michigan State University

(cid:92)Computer Vision Lab  ETH Zurich  Swiss

(cid:93)Univeristy of Science and Technology of China  P.R. China

Abstract

Learning distance functions with side information plays a key role in many ma-
chine learning and data mining applications. Conventional approaches often as-
sume a Mahalanobis distance function. These approaches are limited in two as-
pects: (i) they are computationally expensive (even infeasible) for high dimen-
sional data because the size of the metric is in the square of dimensionality; (ii)
they assume a ﬁxed metric for the entire input space and therefore are unable
to handle heterogeneous data.
In this paper  we propose a novel scheme that
learns nonlinear Bregman distance functions from side information using a non-
parametric approach that is similar to support vector machines. The proposed
scheme avoids the assumption of ﬁxed metric by implicitly deriving a local dis-
tance from the Hessian matrix of a convex function that is used to generate the
Bregman distance function. We also present an efﬁcient learning algorithm for
the proposed scheme for distance function learning. The extensive experiments
with semi-supervised clustering show the proposed technique (i) outperforms the
state-of-the-art approaches for distance function learning  and (ii) is computation-
ally efﬁcient for high dimensional data.

1 Introduction
An effective distance function plays an important role in many machine learning and data mining
techniques. For instance  many clustering algorithms depend on distance functions for the pairwise
distance measurements; most information retrieval techniques rely on distance functions to identify
the data points that are most similar to a given query; k-nearest-neighbor classiﬁer depends on dis-
tance functions to identify the nearest neighbors for data classiﬁcation. In general  learning effective
distance functions is a fundamental problem in both data mining and machine learning.
Recently  learning distance functions from data has been actively studied in machine learning. In-
stead of using a predeﬁned distance function (e.g.  Euclidean distance)  researchers have attempted
to learn distance functions from side information that is often provided in the form of pairwise con-
straints  i.e.  must-link constraints for pairs of similar data points and cannot-link constraints for
pairs of dissimilar data points. Example algorithms include [16  2  8  11  7  15].
Most distance learning methods assume a Mahalanobis distance. Given two data points x and x(cid:48) 
the distance between x and x(cid:48) is calculated by d(x  x(cid:48)) = (x − x(cid:48))(cid:62)A(x − x(cid:48))  where A is the dis-
tance metric that needs to be learned from the side information. [16] learns a global distance metric
(GDM) by minimizing the distance between similar data points while keeping dissimilar data points
far apart. It requires solving a Semi-Deﬁnite Programming (SDP) problem  which is computation-
ally expensive when the dimensionality is high. BarHillel et al [2] proposed the Relevant Compo-
nents Analysis (RCA)  which is computationally efﬁcient and achieves comparable results as GDM.
The main drawback with RCA is that it is unable to handle the cannot-link constraints. This problem
was addressed by Discriminative Component Analysis (DCA) in [8]  which learns a distance metric
by minimizing the distance between similar data points and in the meantime maximizing the distance

1

between dissimilar data points. The authors in [4] proposed an information-theoretic based metric
learning approach (ITML) that learns the Mahalanobis distance by minimizing the differential rel-
ative entropy between two multivariate Gaussians. Neighborhood Component Analysis (NCA) [5]
learns a distance metric by extending the nearest neighbor classiﬁer. The maximum-margin nearest
neighbor (LMNN) classiﬁer [14] extends NCA through a maximum margin framework. Yang et
al. [17] propose a Local Distance Metric (LDM) that addresses multimodal data distributions. Hoi
et al. [7] propose a semi-supervised distance metric learning approach that explores the unlabeled
data for metric learning. In addition to learning a distance metric  several studies [12  6] are devoted
to learning a distance function  mostly non-metric  from the side information.
Despite the success  the existing approaches for distance metric learning are limited in two aspects.
First  most existing methods assume a ﬁxed distance metric for the entire input space  which make
it difﬁcult for them to handle the heterogeneous data. This issue was already demonstrated in [17]
when learning distance metrics from multi-modal data distributions. Second  the existing methods
aim to learn a full matrix for the target distance metric that is in the square of the dimensionality 
making it computationally unattractive for high dimensional data. Although the computation can be
reduced signiﬁcantly by assuming certain forms of the distance metric (e.g.  diagonal matrix)  these
simpliﬁcations often lead to suboptimal solutions. To address these two limitations  we propose a
novel scheme that learns Bregman distance functions from the given side information. Bregman
distance or Bregman divergence [3] has several salient properties for distance measure. Bregman
distance generalizes the class of Mahalanobis distance by deriving a distance function from a given
convex function φ(x). Since the local distance metric can be derived from the local Hessian matrix of
ϕ(x)  Bregman distance function avoids the assumption of ﬁxed distance metric. Recent studies [1]
also reveal the connection between Bregman distances and exponential families of distributions. For
example  Kullback-Leibler divergence is a special Bregman distance when choosing the negative
entropy function for the convex function ϕ(x).
The objective of this work is to design an efﬁcient and effective algorithm that learns a Bregman
distance function from pairwise constraints. Although Bregman distance or Bregman divergence has
been explored in [1]  all these studies assume a predeﬁned Bregman distance function. To the best of
our knowledge  this is the ﬁrst work that addresses the problem of learning Bregman distances from
the pairwise constraints. We present a non-parametric framework for Bregman distance learning 
together with an efﬁcient learning algorithm. Our empirical study with semi-supervised clustering
show that the proposed approach (i) outperforms the state-of-the-art algorithms for distance metric
learning  and (ii) is computationally efﬁcient for high dimensional data.
The rest of the paper is organized as follows. Section 2 presents the proposed framework of learning
Bregman distance functions from the pairwise constraints  together with an efﬁcient learning algo-
rithm. Section 3 presents the experimental results with semi-supervised clustering by comparing
the proposed algorithms with a number of state-of-the-art algorithms for distance metric learning.
Section 5 concludes this work.
2 Learning Bregman Distance Functions
2.1 Bregman Distance Function
Bregman distance function is deﬁned based on a given convex function. Let ϕ(x) : Rd (cid:55)→ R be a
strictly convex function that is twice differentiable. Given ϕ(x)  the Bregman distance function is
deﬁned as

d(x1  x2) = ϕ(x1) − ϕ(x2) − (x1 − x2)(cid:62)∇ϕ(x2)

For the convenience of discussion  we consider a symmetrized version of the Bregman distance
function that is deﬁned as follows

d(x1  x2) = (∇ϕ(x1) − ∇ϕ(x2))(cid:62)(x1 − x2)

(1)

The following proposition shows the properties of d(x1  x2).
Proposition 1. The distance function deﬁned in (1) satisﬁes the following properties if ϕ(x) is a
strictly convex function: (a) d(x1  x2) = d(x2  x1)  (b) d(x1  x2) ≥ 0  (c) d(x1  x2) = 0 ↔ x1 = x2

Remark To better understand the Bregman distance function  we can rewrite d(x1  x2) in (1) as

d(x1  x2) = (x1 − x2)(cid:62)∇2ϕ(˜x)(x1 − x2)

2

where ˜x is a point on the line segment between x1 and x2. As indicated by the above expression  the
Bregman distance function can be viewed as a general Mahalanobis distance that introduces a local
distance metric A = ∇2ϕ(˜x). Unlike the conventional Mahalanobis distance where metric A is a
constant matrix throughout the entire space  the local distance metric A = ∇2ϕ(˜x) is introduced via
the Hessian matrix of convex function ϕ(x) and therefore depends on the location of x1 and x2.
Although the Bregman distance function deﬁned in (1) does not satisfy the triangle inequality  the
following proposition shows the degree of violation could be bounded if the Hessian matrix of ϕ(x)
is bounded.
Proposition 2. Let Ω be the closed domain for x. If ∃m  M ∈ R  M > m > 0 and

where I is the identity matrix  we have the following inequality

mI (cid:185) min
x∈Ω

(cid:112)

(cid:112)

∇2ϕ(x) (cid:185) M I
∇2ϕ(x) (cid:185) max
x∈Ω
√
M − √

d(xc  xb) + (

d(xa  xb) ≤

d(xa  xc) +

(cid:112)

m)[d(xa  xc)d(xc  xb)]1/4

(2)

The proof of this proposition can be found in Appendix A. As indicated by Proposition 2  the de-
m. Given a smooth
gree of violation of the triangle inequality is essentially controlled by
convex function with almost constant Hessian matrix  we would expect that to a large degree  Breg-
man distance will satisfy the triangle inequality. In the extreme case when ϕ(x) = x(cid:62)Ax/2 and
∇2ϕ(x) = A  we have a constant Hessian matrix  leading to a complete satisfaction of the triangle
inequality.

√
M − √

2.2 Problem Formulation

To a learn a Bregman distance function  the key is to ﬁnd the appropriate convex function ϕ(x) that
is consistent with the given pairwise constraints. In order to learn the convex function ϕ(x)  we take
a non-parametric approach by assuming that ϕ(·) belongs to a Reproducing Kernel Hilbert Space
Hκ. Given a kernel function κ(x  x(cid:48)) : Rd × Rd (cid:55)→ R  our goal is to search for a convex function
ϕ(x) ∈ Hκ such that the induced Bregman distance function  denoted by dϕ(x  x(cid:48))  minimizes the
overall training error with respect to the given pairwise constraints.
We denote by D = {(x1
i   x2
Each pairwise constraint consists of a pair of instances x1
i are similar and −1 if x1
x2
the input patterns of all training instances in D.
Following the maximum margin framework for classiﬁcation  we cast the problem of learning a
Bregman distance function from pairwise constraints into the following optimization problem  i.e. 

i   yi)  i = 1  . . .   n} the collection of pairwise constraints for training.
i and
i are dissimilar. We also introduce X = (x1  . . .   xN ) to include
i and x2

i   and a label yi that is +1 if x1

i and x2

i ) − b])

|ϕ|2Hκ + C

1
2

min

i   x2

(cid:96)(yi[d(x1

ϕ∈Ω(Hκ) b∈R+

(3)
where Ω(H) = {f ∈ H : f is convex} refers to the subspace of functional space H that only
includes convex functions  (cid:96)(z) = max(0  1 − z) is a hinge loss  and C is a penalty cost parameter.
The main challenge with solving the variational problem in (3) is that it is difﬁcult to derive a
representer theorem for ϕ(x) because it is ∇ϕ(x) used in the deﬁnition of distance function  not
ϕ(x). Note that although it seems to be convenient to regularize ∇ϕ(x)  it will be difﬁcult to restrict
ϕ(x) to be convex. To resolve this problem  we consider a special family of kernel functions κ(x  x(cid:48))
1 x2) where h : R (cid:55)→ R is a strictly convex function. Examples
that has the form κ(x1  x2) = h(x(cid:62)
of h(z) that guarantees κ(· ·) to be positive semi-deﬁnite are h(z) = |z|d (d ≥ 1)  h(z) = |z + 1|d
(d ≥ 1)  and h(z) = exp(z). For the convenience of discussion  we assume h(0) = 0 throughout
this paper.
First  since ϕ(x) ∈ Hκ  we have
ϕ(x) =

dyh(x(cid:62)y)q(y)

dyκ(x  y)q(y) =

(cid:90)

(cid:90)

n(cid:88)

i=1

where q(y) is a weighting function. Given the training instances x1  . . .   xN   we divide the space
Rd into A and ¯A that are deﬁned as

A = span{x1  . . .   xN}  ¯A = Null(x1  . . .   xN )

(4)

(5)

3

We deﬁne H(cid:107) and H⊥ as follows

H(cid:107) = span{κ(x ·) ∀x ∈ A}  H⊥ = span{κ(x ·) ∀x ∈ ¯A}

(6)

The following proposition summarizes an important property of reproducing kernel Hilbert space
Hκ when kernel function κ(· ·) is restricted to the form in Eq. (2.2).
Proposition 3. If the kernel function κ(· ·) is written in the form of Equation (2.2) with h(0) = 0 
we have H(cid:107) and H⊥ form a complete partition of Hκ  i.e.  Hκ = H(cid:107) ∪ H⊥  and H(cid:107)⊥H⊥.
We therefore have the following representer theorem for ϕ(x) that minimizes (3)
Theorem 1. The function ϕ(x) that minimizes (3) admits the following expression

(cid:90)

ϕ(x) ∈ H(cid:107) =

dyq(y)h(x(cid:62)y) =

duq(u)h(x(cid:62)Xu)

(7)

y∈A

where u ∈ RN and X = (x1  . . .   xN ).
The proof of the above theorem can be found in Appendix B.

(cid:90)

N(cid:88)

(8)

(9)

(10)

(11)

(12)

(13)

2.3 Algorithm

(cid:80)N
To further derive a concrete expression for ϕ(x)  we restrict q(y) in (7) to the special form: q(y) =
i=1 αiδ(y − xi) where αi ≥ 0  i = 1  . . .   N are non-negative combination weights. This results

in ϕ(x) =

i=1 αih(x(cid:62)

i x)  and consequently d(xa  xb) as follows

(cid:80)N

d(xa  xb) =

αi(h(cid:48)(x(cid:62)

a xi) − h(cid:48)(x(cid:62)

b xi))x(cid:62)

i (xa − xb)

i=1

By deﬁning h(xa) = (h(cid:48)(x(cid:62)

a x1)  . . .   h(cid:48)(x(cid:62)

a xN ))(cid:62)  we can express d(xa  xb) as follows

d(xa  xb) = (xa − xb)(cid:62)X(α ◦ [h(xa) − h(xb)])

Notice that when h(z) = z2/2  we have d(xa  xb) expressed as

d(xa  xb) = (xa − xb)(cid:62)Xdiag(α)X(cid:62)(xa − xb).

(cid:80)N

This is a Mahanalobis distance with metric A = Xdiag(α)X(cid:62) =
i . When h(z) =
exp(z)  we have h(x) = (exp(x(cid:62)x1)  . . .   exp(x(cid:62)xN ))  and the resulting distance function is no
longer stationary due to the non-linear function exp(z).

i=1 αixix(cid:62)

Given the assumption that q(y) =

(cid:80)N
n(cid:88)
i=1 αiδ(y − xi)  we have (3) simpliﬁed as
(cid:161)
1
2 α(cid:62)Kα + C
i )(cid:62)X(α ◦ [h(x1
i − x2
(x1

i )]) − b
yi
εi ≥ 0  i = 1  . . .   n  αk ≥ 0  k = 1  . . .   N

i ) − h(x2

i=1

εi

(cid:162) ≥ 1 − εi 
(cid:80)N

min
α∈RN  b

s. t.

Note that the constraint αk ≥ 0 is introduced to ensure ϕ(x) =
function. By deﬁning

k=1 αkh(x(cid:62)xk) is a convex

zi = [h(x1

i ) − h(x2

i )] 

i )] ◦ [X(cid:62)(x1
i − x2
n(cid:88)

1
2 α(cid:62)Kα + C

i=1

(cid:96)(yi[z(cid:62)

i α − b])

we simplify the problem in (11) as follows

L =

min
α∈RN
+  b

where (cid:96)(z) = max(0  1 − z).

4

We solve the above problem by a simple subgradient descent approach. In particular  at iteration t 
given the current solution αt and bt  we compute the gradients as
i αt − bt])yizi  ∇bL = −C

∇αL = Kαt + C

i αt − bt])yi

∂(cid:96)(yi[z(cid:62)

∂(cid:96)(yi[z(cid:62)

n(cid:88)

n(cid:88)

i=1

where ∂(cid:96)(z) stands for the subgradient of (cid:96)(z). Let S +
which (αt  bt) suffers a non-zeros loss  i.e. 
S +
t = {(zi  yi) ∈ D : yi(z(cid:62)

We can then express the sub-gradients of L at αt and bt as follows:

(cid:88)

∇αL = Kα − C

yizi  ∇bL = C

i=1

(14)
t ∈ D denotes the set of training instances for
(cid:88)
i αt − bt) < 1}

(15)

(16)

(zi yi)∈S+

t

(cid:161)

(cid:162)

yi

(zi yi)∈S+

t

The new solution  denoted by αt+1 and bt+1  is computed as follows:

k = π[0 +∞]
αt+1

(17)
is the k-th element of vector αt+1  πG(x) projects x into the domain G  and γt is the
t by following the Pegasos algorithm [10] for solving SVMs. The

where αt+1
step size that is set to be γt = C
pseudo-code of the proposed algorithm is summarized in Algorithm 1.

k

k − γt[∇αL]k
αt

  bt+1 = bt − γt∇bL

Algorithm 1 Algorithm of Learning Bregman Distance Functions
INPUT:• data matrix: X ∈ RN×d
• pair-wise constraint {(x1
• kernel function: κ(x1  x2) = h(x(cid:62)
• penalty cost parameter C

i   yi)  i = 1  . . .   n}

i   x2

1 x2)

i xj)]N×N

+   b ∈ R

i ) − h(x2

OUTPUT:• Bregman coefﬁcients α ∈ RN
PROCEDURE
1: initialize Bregman coefﬁcients: α = α0  b = b0
2: calculate kernel matrix: K = [h(x(cid:62)
3: calculate vectors zi: zi = [h(x1
4: set iteration step t = 1;
5: repeat
6:
7:
8:
9:
10:
11:
12: until convergence

(1) update the learning rate: γ = C/t  t = t + 1
(2) update subset of training instances: S +
(3) compute the gradients w.r.t α and b:

(cid:80)

t

(cid:80)

∇αL = Kα − C
b ← b − γ∇bL  αk ← π[0 +∞] (αk − γ[∇αL]k)   k = 1  . . .   N

(4) update Bregman coefﬁcients α = (α1  . . .   αn) and threshold b:

yizi  ∇bL = C

zi∈S+

zi∈S+

yi

t

i )] ◦ [X(cid:62)(x1

i − x2
i )]

t = {(zi  yi) ∈ D : yi(z(cid:62)

i α − b) < 1}

Computational complexity One of the major computational costs for Algorithm 1 is the prepa-
ration of kernel matrix K and vector {zi}n
i=1  which fortunately can be pre-computed. Each step of
the subgradient descent algorithm has a linear complexity  i.e.  O(max(N  n))  which makes it rea-
sonable even for large data sets with high dimensionality. The number of iterations for convergence
is O(1/2) where  is the target accuracy. It thus works ﬁne if we are not critical about the accuracy
of the solution.
3 Experiments
We evaluate the proposed distance learning technique by semi-supervised clustering. In particular 
we ﬁrst learn a distance function from the given pairwise constraints and then apply the learned
distance function to data clustering. We verify the efﬁcacy and efﬁciency of the proposed technique
by comparing it with a number of state-of-the-art algorithms for distance metric learning.

3.1 Experimental Testbed and Settings
We adopt six well-known datasets from UCI machine learning repository  and six popular text bench-
mark datasets1 in our experiments. These datasets are chosen for clustering because they vary signif-

1The Reuter dataset is available at: http://renatocorrea.googlepages.com/textcategorizationdatasets

5

icantly in properties such as the number of clusters/classes  the number of features  and the number
of instances. The diversity of datasets allows us to examine the effectiveness of the proposed learn-
ing technique more comprehensively. The details of the datasets are shown in Table 1.

#samples #feature #classes

#samples #feature #classes

dataset
breast-cancer
diabetes
ionosphere
liver-disorders
sonar
a1a

683
768
251
345
208
1 605

10
8
34
6
60
123

2 477
3 470
17 188
4 291
7 149
10 789
Table 1: The details of our experimental testbed

dataset
w1a
w2a
w6a
WebKB
newsgroup
Reuter

2
2
2
2
2
2

300
300
300

19 687
47 411
5 189

2
2
2
6
11
79

Similar to previous work [16]  the pairwise constraints are created by random sampling. More
speciﬁcally  we randomly sample a subset of pairs from the pool of all possible pairs (every two
instances forms a pair). Two instances form a must-link constraint (i.e.  yi = +1) if they share the
same class label  and form a cannot-link constraint (i.e.  yi = −1) if they are assigned to different
classes. To calculate the Bregman function  in this experiment  we adopt the non-linear function
h(x) = (exp(x(cid:62)x1)  . . .   exp(x(cid:62)xN )).
To perform data clustering  we run the k-means algorithm using the distance function learned from
500 randomly sampled positive constraints 500 random negative constraints. The number of clusters
is simply set to the number of classes in the ground truth. The initial cluster centroids are randomly
chosen from the dataset. To enable fair comparisons  all comparing algorithms start with the same
set of initial centroids. We repeat each clustering experiment for 20 times  and report the ﬁnal results
by averaging over the 20 runs.
We compare the proposed Bregman distance learning method using the k-means algorithm for semi-
supervised clustering  termed Bk-means  with the following approaches: (1) a standard k-means 
(2) the constrained k-means [13] (Ck-means)  (3) Ck-means with distance learned by RCA [2]  (4)
Ck-means with distance learned by DCA [8]  (5) Ck-means with distance learned by the Xing’s
algorithm [16] (Xing)  (6) Ck-means with information-theoretic metric learning (ITML) [4]  and (7)
Ck-means with a distance function learned by a boosting algorithm (DistBoost) [12].
To evaluate the clustering performance  we use the some standard performance metrics 
in-
cluding pairwise Precision  pairwise Recall  and pairwise F1 measures [9]  which are evalu-
ated base on the pairwise results. Speciﬁcally  pairwise precision is the ratio of the number
of correct pairs placed in the same cluster over the total number of pairs placed in the same
cluster  pairwise recall is the ratio of the number of correct pairs placed in the same cluster
over the total number of pairs actually placed in the same cluster  and pairwise F1 equals to
2 × precision × recall/(precision + recall).

3.2 Performance Evaluation on Low-dimensional Datasets
The ﬁrst set of experiments evaluates the clustering performance on six UCI datasets. Table 2 shows
the average precision  recall  and F1 measurements of all the competing algorithms given a set of
1  000 random constraints. The top two highest average F1 scores on each dataset were highlighted
in bold font. From the results in Table 2  we observe that the proposed Bregman distance based
k-means clustering approach (Bk-means) is either the best or the second best for almost all datasets 
indicating that the proposed algorithm is in general more effective than the other algorithms for
distance metric learning.

3.3 Performance Evaluation on High-dimensional Text Data
We evaluate the clustering performance on six text datasets. Since some of the methods are infeasible
for text clustering due to the high dimensionality  we only include the results for the methods which
are feasible for this experiment (i.e.  OOM indicates the method takes more than 10 hours  and
OOT indicates the method needs more than 16G REM). Table 3 summarizes the F1 performance
of all feasible methods for datasets w1a  w2a  w6a  WebKB  20newsgroup and reuter. Since cosine
similarity is commonly used in textual domain  we use k-means  Ck-means in both Euclidian space
and Cosine similarity space as baselines. The best F1 scores are marked in bold in Table 3. The
results show that the learned Bregman distance function is applicable for high dimensional data  and
it outperforms the other commonly used text clustering methods for four out of six datasets.

6

F1

72.73±3.42
85.31±1.48
91.94±2.15
88.11±0.22
90.18±2.94
93.88±0.22
94.29±0.29
98.37±0.19

F1

57.28±6.20
61.46±1.36
72.62±1.24
63.52±0.39
66.99±0.45
66.68±0.00
72.72±1.03
73.28±1.93

precision
52.47±8.93
60.06±1.13
73.93±1.28
58.11±0.48
59.86±2.99
61.23±2.05
64.45±1.02
99.42±0.40

precision
63.92±8.60
62.90±8.43
93.53±3.28
95.42±2.85
59.56±18.95
70.18±4.27
51.60±1.43
96.89±4.11

diabetes
recall
57.17±3.68
55.98±0.64
70.11±0.41
58.31±0.16
62.70±2.18
64.88±0.56
68.33±0.98
64.68±0.63
liver-disorders
50.50±0.40
50.35±1.68
55.57±0.10
49.65±0.08
52.15±1.68
50.41±0.07
52.88±1.31
50.29±2.09

recall

F1

56.41±4.53
57.57±0.85
71.55±0.81
58.21±0.31
61.22±2.59
63.00±0.75
66.33±1.00
77.43±0.92

F1

55.67±5.96
55.13±1.63
68.73±1.40
65.31±1.10
54.92±5.76
58.67±1.63
52.23±1.37
66.86±3.10

F1

51.87±1.47
55.32±1.37
70.46±2.35
79.83±2.70
79.83±5.85
73.11±0.57
75.54±0.62
82.52±1.44

precision
55.81±1.01
69.91±0.08
99.99±0.98
57.70±1.32
76.64±0.08
57.15±1.32
99.98±0.21

n/a

a1a
recall

69.99±0.91
80.34±0.18
70.30±0.54
70.89±1.01
66.96±0.35
71.76±1.87
77.72±0.17

n/a

F1

62.10±0.99
77.01±0.12
81.76±0.76
63.62±1.21
69.96±0.18
63.63±1.55
86.32±0.19

n/a

method
baseline
Ck-means
ITML
Xing
RCA
DCA
DistBoost
Bk-means

method
baseline
Ck-means
ITML
Xing
RCA
DCA
DistBoost
Bk-means

precision
72.85±3.77
98.10±2.20
97.05±2.77
93.61±0.14
85.40±0.14
94.53±0.34
94.76±0.24
99.04±0.10

precision
62.35±6.30
57.05±1.24
97.10±2.70
63.46±0.11
100.00±6.19
66.36±3.01
75.91±1.11
97.64±1.93

method
baseline
Ck-means
ITML
Xing
RCA
DCA
DistBoost
Bk-means

precision
52.98±2.05
60.44±4.53
98.68±2.46
96.99±4.53
100.00±13.69
100.00±0.64
76.64±0.57
99.20±1.62

breast
recall
72.52±2.30
81.01±0.10
88.96±0.30
84.19±0.83
94.16±0.29
93.23±0.29
93.83±0.31
98.33±0.24
ionosphere
recall
53.39±2.74
51.28±1.58
59.99±0.31
64.10±0.03
50.36±1.44
67.01±2.12
69.34±0.91
62.71±1.94
sonar
recall
50.84±1.69
51.71±1.17
56.31±2.28
69.81±0.05
69.81±1.33
59.75±0.30
74.48±0.69
74.24±1.23

Table 2: Evaluation of clustering performance (average precision  recall  and F1) on six UCI
datasets. The top two F1 scores are highlighted in bold font for each dataset.

w1a

w2a

w6a

methods
k-means(EU)
k-means(Cos)
Ck-means(EU)
Ck-means(Cos)
RCA
DCA
ITML
64.51±0.95
Bk-means
Table 3: Evaluation of clustering F1 performance on the high dimensional text data. Only applicable
methods are shown. OOM indicates “out of memory”  and OOT indicates “out of time”.

76.52±0.97
77.16±1.27
76.52±1.01
75.32±0.91
93.51±1.13
87.44±1.99
96.95 ±0.13
98.64±0.24

72.59±0.77
73.47±1.35
97.23±1.21
97.14±2.12
96.45±1.17
94.30±2.56
94.12±0.92
96.92±1.02

76.68±0.25
76.87±5.61
87.04±1.15
87.14±2.14
91.00±1.02
92.13±1.04
92.31±0.84
93.43±1.07

WebKB
35.78±0.17
35.18±3.41
70.84±2.29
75.84±1.08

newsgroup
16.54±0.05
18.87±0.14
19.12±0.54
20.08±0.49

43.88±0.23
45.42±0.73
56.00±0.42
58.24±0.82

73.94±1.25

25.17±1.27

OOM
OOM
OOM

OOM
OOM
OOT

OOT
OOT
OOT

Reuter

3.4 Computational Complexity
Here  we evaluate the running time of semi-supervised clustering. For a conventional clustering
algorithm such as k-means  its computational complexity is determined by both the calculation of
distance and the clustering scheme. For a semi-supervised clustering algorithm based on distance
learning  the overall computational time include both the time for training an appropriate distance
function and the time for clustering data points. The average running times of semi-supervised
clustering over the six UCI datasets are listed in Table 4. It is clear that the Bregman distance based
clustering has comparable efﬁciency with simple methods like RCA and DCA on low dimensional
data  and runs much faster than Xing  ITML  and DistBoost. On the high dimensional text data  it is
much faster than other applicable DML methods.

Algorithm

UCI data(Sec.)
Text data(Min.)

k-means Ck-means

0.51
0.78

0.72
4.56

ITML Xing
8.56
7.59
71.55
n/a

RCA
0.88
68.90

DCA DistBoost Bk-means
0.90
69.34

13.09
n/a

1.70
3.84

Table 4: Comparison of average running time over the six UCI datasets and subsets of six text
datasets (10% sampling from the datasets in Table 1).

7

4 Conclusions
In this paper  we propose to learn a Bregman distance function for clustering algorithms using a
non-parametric approach. The proposed scheme explicitly address two shortcomings of the existing
approaches for distance fuction/metric learning  i.e.  assuming a ﬁxed distance metric for the entire
input space and high computational cost for high dimensional data. We incorporate the Bregman
distance function into the k-means clustering algorithm for semi-supervised data clustering. Exper-
iments of semi-supervised clustering with six UCI datasets and six high dimensional text datasets
have shown that the Bregman distance function outperforms other distance metric learning algo-
rithms in F1 measure. It also veriﬁes that the proposed distance learning algorithm is computation-
ally efﬁcient  and is capable of handling high dimensional data.
Acknowledgements
This work was done when Mr. Lei Wu was an RA at Nanyang Technological University  Sin-
gapore. This work was supported in part by MOE tier-1 Grant (RG67/07)  NRF IDM Grant
(NRF2008IDM-IDM-004-018)  National Science Foundation (IIS-0643494)  and US Navy Re-
search Ofﬁce (N00014-09-1-0663).

APPENDIX A: Proof of Proposition 2
Proof. First  let us denote by f as follows:

f = (

m)[d(xa  xc)d(xc  xb)]1/4

The square of the right side of Eq. (2) is

d(xa  xc) +

d(xc  xb) + f 1/4)2 = d(xa  xb) − η(xa  xb  xc) + δ(xa  xb  xc)

(cid:112)

(
where

(cid:112)

(cid:112)

(cid:112)

√
M − √
(cid:112)

δ(xa  xb  xc) = f 2 + 2f
η(xa  xb  xc) = (∇ϕ(xa) − ∇ϕ(xc))(xc − xb) + (∇ϕ(xc) − ∇ϕ(xb))(xa − xc).

d(xa  xc)d(xc  xb)

d(xa  xc) + 2f

d(xc  xb) + 2

δ(xa  xb  xc) − η(xa  xb  xc)

From this above equation  the proposition holds if and only if δ(xa  xb  xc) − η(xa  xb  xc) ≥ 0.
From the fact that
M − √
√
m and the distance function d(·) ≥ 0  we get δ(xa  xb  xc)− η(xa  xb  xc) ≥ 0.

√
M − √

√
M >

1
4 + d(xc  xb)

d(xa  xc)d(xc  xb)

3
4 d(xa  xc)

3
4 d(xc  xb)

m)2 + 2(

d(xa  xc)

(cid:112)

√
(

(cid:180)

1
4

=

since

(cid:179)

m)

+ 2d(xa  xc)d(xc  xb)

(cid:90)

y∈A

APPENDIX B: Proof of Theorem 1
Proof. We write ϕ(x) = ϕ(cid:107)(x) + ϕ⊥(x) where

ϕ(cid:107)(x) ∈ H(cid:107) =

dyq(y)h(x(cid:62)y)  ϕ⊥(x) ∈ H⊥ =

Thus  the distance function deﬁned in (1) is then expressed as

d(xa  xb) = (xa − xb)(cid:62)(cid:161)∇ϕ(cid:107)(xa) − ∇ϕ(cid:107)(xb)

(cid:162)

(cid:90)

q(y)(h(cid:48)(x(cid:62)

a y) − h(cid:48)(x(cid:62)

b y))y(cid:62) (xa − xb) +

(cid:90)

y∈ ¯A

dyq(y)h(x(cid:62)y)

+ (xa − xb)(cid:62) (∇ϕ⊥(xa) − ∇ϕ⊥(xb))
a y) − h(cid:48)(x(cid:62)

q(y)(h(cid:48)(x(cid:62)

b y))y(cid:62) (xa − xb) = (xa − xb)(cid:62)(cid:161)∇ϕ(cid:107)(xa) − ∇ϕ(cid:107)(xb)

y∈ ¯A

(cid:162)

b y))y(cid:62) (xa − xb)

q(y)(h(cid:48)(x(cid:62)

a y) − h(cid:48)(x(cid:62)

(cid:90)
(cid:90)

y∈A

=

=

y∈A
= |ϕ(cid:107)(x)|2Hκ

Since |ϕ(x)|2Hκ
= 0. Since
|ϕ⊥(x)| = (cid:104)ϕ⊥(·)  κ(x ·)(cid:105)Hκ ≤ |κ(x ·)|Hκ|ϕ⊥|Hκ = 0   we have ϕ⊥(x) = 0 for any x. We thus
have ϕ(x) = ϕ(cid:107)(x)  which leads to the result in the theorem.

  the minimizer of (1) should have |ϕ⊥(x)|2Hκ

+|ϕ⊥(x)|2Hκ

8

References
[1] A. Banerjee  S. Merugu  I. Dhillon  and J. Ghosh. Clustering with bregman divergences. In

Journal of Machine Learning Research  pages 234–245  2004.

[2] A. Bar-Hillel  T. Hertz  N. Shental  and D. Weinshall. Learning a mahalanobis metric from

equivalence constraints. JMLR  6:937–965  2005.

[3] L. Bregman. The relaxation method of ﬁnding the common points of convex sets and its appli-
cation to the solution of problems in convex programming. USSR Computational Mathematics
and Mathematical Physics  7:200–217  1967.

[4] J. V. Davis  B. Kulis  P. Jain  S. Sra  and I. S. Dhillon. Information-theoretic metric learning.

In ICML’07  pages 209–216  Corvalis  Oregon  2007.

[5] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighborhood component analy-

sis. In NIPS.

[6] T. Hertz  A. B. Hillel  and D. Weinshall. Learning a kernel function for classiﬁcation with
small training samples. In ICML ’06: Proceedings of the 23rd international conference on
Machine learning  pages 401–408. ACM  2006.

[7] S. C. H. Hoi  W. Liu  and S.-F. Chang. Semi-supervised distance metric learning for collab-
orative image retrieval. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR2008)  June 2008.

[8] S. C. H. Hoi  W. Liu  M. R. Lyu  and W.-Y. Ma. Learning distance metrics with contextual

constraints for image retrieval. In Proc. CVPR2006  New York  US  June 17–22 2006.

[9] Y. Liu  R. Jin  and A. K. Jain. Boostcluster: boosting clustering by pairwise constraints. In

KDD’07  pages 450–459  San Jose  California  USA  2007.

[10] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for svm. In ICML ’07: Proceedings of the 24th international conference on Machine learning 
pages 807–814  New York  NY  USA  2007. ACM.

[11] L. Si  R. Jin  S. C. H. Hoi  and M. R. Lyu. Collaborative image retrieval via regularized metric

learning. ACM Multimedia Systems Journal  12(1):34–44  2006.

[12] T. H. Tomboy  A. Bar-hillel  and D. Weinshall. Boosting margin based distance functions
In In Proceedings of the Twenty-First International Conference on Machine

for clustering.
Learning  pages 393–400  2004.

[13] K. Wagstaff  C. Cardie  S. Rogers  and S. Schr¨odl. Constrained k-means clustering with back-
In ICML’01  pages 577–584  San Francisco  CA  USA  2001. Morgan

ground knowledge.
Kaufmann Publishers Inc.

[14] K. Weinberger  J. Blitzer  and L. Saul. Distance metric learning for large margin nearest neigh-

bor classiﬁcation. In NIPS 18  pages 1473–1480  2006.

[15] L. Wu  S. C. H. Hoi  J. Zhu  R. Jin  and N. Yu. Distance metric learning from uncertain side
information with application to automated photo tagging. In Proceedings of ACM International
Conference on Multimedia (MM2009)  Beijing  China  Oct. 19–24 2009.

[16] E. P. Xing  A. Y. Ng  M. I. Jordan  and S. Russell. Distance metric learning with application to

clustering with side-information. In NIPS2002  2002.

[17] L. Yang  R. Jin  R. Sukthankar  and Y. Liu. An efﬁcient algorithm for local distance metric
learning. In Proceedings of the Twenty-Second Conference on Artiﬁcial Intelligence (AAAI) 
2006.

9

,Yuanyuan Mi
C. C. Alan Fung
K. Y. Michael Wong
Si Wu
Richard Nock
Zac Cranko
Aditya Menon
Lizhen Qu
Robert Williamson