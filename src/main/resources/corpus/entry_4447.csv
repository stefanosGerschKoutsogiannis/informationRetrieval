2018,Processing of missing data by neural networks,We propose a general  theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover  in contrast to recent approaches  it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.,Processing of missing data by neural networks

Marek ´Smieja

marek.smieja@uj.edu.pl

Łukasz Struski

lukasz.struski@uj.edu.pl

Jacek Tabor

jacek.tabor@uj.edu.pl

Bartosz Zieli´nski

bartosz.zielinski@uj.edu.pl

Przemysław Spurek

przemyslaw.spurek@uj.edu.pl

Faculty of Mathematics and Computer Science

Jagiellonian University

Łojasiewicza 6  30-348 Kraków  Poland

Abstract

We propose a general  theoretically justiﬁed mechanism for processing missing
data by neural networks. Our idea is to replace typical neuron’s response in the
ﬁrst hidden layer by its expected value. This approach can be applied for various
types of networks at minimal cost in their modiﬁcation. Moreover  in contrast to
recent approaches  it does not require complete data for training. Experimental
results performed on different types of architectures show that our method gives
better results than typical imputation strategies and other methods dedicated for
incomplete data.

1

Introduction

Learning from incomplete data has been recognized as one of the fundamental challenges in machine
learning [1]. Due to the great interest in deep learning in the last decade  it is especially important to
establish uniﬁed tools for practitioners to process missing data with arbitrary neural networks.
In this paper  we introduce a general  theoretically justiﬁed methodology for feeding neural networks
with missing data. Our idea is to model the uncertainty on missing attributes by probability density
functions  which eliminates the need of direct completion (imputation) by single values. In conse-
quence  every missing data point is identiﬁed with parametric density  e.g. GMM  which is trained
together with remaining network parameters. To process this probabilistic representation by neural
network  we generalize the neuron’s response at the ﬁrst hidden layer by taking its expected value
(Section 3). This strategy can be understand as calculating the average neuron’s activation over the
imputations drawn from missing data density (see Figure 1 for the illustration).
The main advantage of the proposed approach is the ability to train neural network on data sets
containing only incomplete samples (without a single fully observable data). This distinguishes our
approach from recent models like context encoder [2  3]  denoising autoencoder [4] or modiﬁed
generative adversarial network [5]  which require complete data as an output of the network in training.
Moreover  our approach can be applied to various types of neural networks what requires only minimal
modiﬁcation in their architectures. Our main theoretical result shows that this generalization does not
lead to loss of information when processing the input (Section 4). Experimental results performed on
several types of networks demonstrate practical usefulness of the method (see Section 5 and Figure 2
for sample results) .

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

INPUT

OUTPUT

GMM params: (pi  mi  Σi)i

(cid:82) φ(wT x + b)FS(x)dx

w1

w2

w3

w4

w5

w6

w7

x1

(cid:63)

x3

(cid:63)

x5

x6

x7

Figure 1: Missing data point (x  J)  where x ∈ RD and J ⊂ {1  . . .   D} denotes absent attributes  is
represented as a conditional density FS (data density restricted to the afﬁne subspace S = Aﬀ[x  J]
identiﬁed with (x  J)). Instead of calculating the activation function φ on a single data point (as for
complete data points)  the ﬁrst hidden layer computes the expected activation of neurons. Parameters
of missing data density (pi  µi  Σi)i are tuned jointly with remaining network parameters.

2 Related work

Typical strategy for using machine learning methods with incomplete inputs relies on ﬁlling absent
attributes based on observable ones [6]  e.g. mean or k-NN imputation. One can also train separate
models  e.g. neural networks [7]  extreme learning machines (ELM) [8]  k-nearest neighbors [9]  etc. 
for predicting the unobserved features. Iterative ﬁlling of missing attributes is one of the most popular
technique in this class [10  11]. Recently  a modiﬁed generative adversarial net (GAN) was adapted to
ﬁll in absent attributes with realistic values [12]. A supervised imputation  which learns a replacement
value for each missing attribute jointly with remaining network parameters  was proposed in [13].
Instead of generating candidates for ﬁlling missing attributes  one can build a probabilistic model of
incomplete data (under certain assumptions on missing mechanism) [14  15]  which is subsequently
fed into particular learning model [16  17  18  19  20  21  22  23]. Decision function can also be
learned based on the visible inputs alone [24  25]  see [26  27] for SVM and random forest cases.
Pelckmans et. al. [28] modeled the expected risk under the uncertainty of the predicted outputs. The
authors of [29] designed an algorithm for kernel classiﬁcation under low-rank assumption  while
Goldberg et. al. [30] used matrix completion strategy to solve missing data problem.
The paper [31] used recurrent neural networks with feedback into the input units  which ﬁlls absent
attributes for the sole purpose of minimizing a learning criterion. By applying the rough set theory 
the authors of [32] presented a feedforward neural network which gives an imprecise answer as
the result of input data imperfection. Goodfellow et. al. [33] introduced the multi-prediction deep
Boltzmann machine  which is capable of solving different inference problems  including classiﬁcation
with missing inputs.
Alternatively  missing data can be processed using the popular context encoder (CE) [2  3] or modiﬁed
GAN [5]  which were proposed for ﬁlling missing regions in natural images. The other possibility
would be to use denoising autoencoder [4]  which was used e.g. for removing complex patterns like
superimposed text from an image. Both approaches  however  require complete data as an output of
the network in training phase  which is in contradiction with many real data sets (such us medical
ones).

2

3 Layer for processing missing data

In this section  we present our methodology for feeding neural networks with missing data. We show
how to represent incomplete data by probability density functions and how to generalize neuron’s
activation function to process them.
Missing data representation. A missing data point is denoted by (x  J)  where x ∈ RD and
J ⊂ {1  . . .   D} is a set of attributes with missing values. With each missing point (x  J) we
associate the afﬁne subspace consisting of all points which coincide with x on known coordinates
J(cid:48) = {1  . . .   N} \ J:

S = Aﬀ[x  J] = x + span(eJ ) 

where eJ = [ej]j∈J and ej is j-th canonical vector in RD.
Let us assume that the values at missing attributes come from the unknown D-dimensional probability
distribution F . Then we can model the unobserved values of (x  J) by restricting F to the afﬁne
subspace S = Aﬀ[x  J].
In consequence  possible values of incomplete data point (x  J) are
described by a conditional density1 FS : S → R given by (see Figure 1):

(cid:40)

1

(cid:82)
S F (s)ds F (x)  for x ∈ S 
0  otherwise.

FS(x) =

(1)

Notice that FS is a degenerate density deﬁned on the whole RD space2  which allows to interpret it
as a probabilistic representation of missing data point (x  J).
In our approach  we use the mixture of Gaussians (GMM) with diagonal covariance matrices as a
missing data density F . The choice of diagonal covariance reduces the number of model parameters 
which is crucial in high dimensional problems. Clearly  a conditional density for the mixture of
Gaussians is a (degenerate) mixture of Gaussians with a support in the subspace. Moreover  we apply
an additional regularization in the calculation of conditional density (1) to avoid some artifacts when
Gaussian densities are used3. This regularization allows to move from typical conditional density
given by (1) to marginal density in the limiting case. Precise formulas for a regularized density for
GMM with detailed explanations are presented in Supplementary Materials (section 1).
Generalized neuron’s response. To process probability density functions (representing missing data
points) by neural networks  we generalize the neuron’s activation function. For a probability density
function FS  we deﬁne the generalized response (activation) of a neuron n : RD → R on FS as the
mean output:

(cid:90)

n(FS) = E[n(x)|x ∼ FS] =

n(x)FS(x)dx.

Observe that it is sufﬁcient to generalize neuron’s response at the ﬁrst layer only  while the rest of
network architecture can remain unchanged. Basic requirement is the ability of computing expected
value with respect to FS. We demonstrate that the generalized response of ReLU and RBF neurons
with respect to the mixture of diagonal Gaussians can be calculated efﬁciently.
Let us recall that the ReLU neuron is given by

where w ∈ RD and b ∈ R is the bias. Given 1-dimensional Gaussian density N (m  σ2)  we ﬁrst
evaluate ReLU[N (m  σ2)]  where ReLU = max(0  x). If we deﬁne an auxiliary function:

ReLUw b(x) = max(wT x + b  0) 

then the generalized response equals:

NR(w) = ReLU[N (w  1)] 

ReLU[N (m  σ2)] = σNR(

m
σ

).

1More precisely  FS equals a density F conditioned on the observed attributes.
2An example of degenerate density is a degenerate Gaussian N (m  Σ)  for which Σ is not invertible. A
degenerate Gaussian is deﬁned on afﬁne subspace (given by image of Σ)  see [34] for details. For simplicity we
use the same notation N (m  Σ) to denote both standard and degenerate Gaussians.

3One can show that the conditional density of a missing point (x  J) sufﬁciently distant from the data reduces

to only one Gaussian  which center is nearest in the Mahalanobis distance to Aﬀ[x  J]

3

Elementary calculation gives:

pi

NR(w) =

exp(− w2
2

) +

w
2

(1 + erf(

w√
2

)) 

(2)

1√
(cid:82) z
2π
0 exp(−t2)dt.
Theorem 3.1. Let F =(cid:80)

where erf(z) = 2√
We proceed with a general case  where an input data point x is generated from the mixture of
(degenerate) Gaussians. The following observation shows how to calculate the generalized response
of ReLUw b(x)  where w ∈ RD  b ∈ R are neuron weights.
weights w = (w1  . . .   wD) ∈ RD  b ∈ R  we have:

i piN (mi  Σi) be the mixture of (possibly degenerate) Gaussians. Given

(cid:88)

i

piNR(cid:0) wT mi + b

(cid:112)

wT Σiw

(cid:1).

ReLUw b(F ) =

Proof. If x ∼ N (m  Σ) then wT x + b ∼ N (wT x + b  wT Σw). Consequently 

if x ∼

i piN (wT mi + b  wT Σiw).

Making use of (2)  we get:

(cid:80)
i piN (mi  Σi)  then wT x + b ∼(cid:80)
(cid:88)
(cid:90) ∞

ReLUw b(F ) =

ReLU(x)

(cid:90)

R

i

(cid:88)

=

pi

i

0

piN (wT mi + b  wT Σiw)(x)dx

xN (wT mi + b  wT Σiw)(x)dx =

(cid:88)

i

piNR(cid:0) wT mi + b

(cid:112)

wT Σiw

(cid:1).

k(cid:88)

i=1

k(cid:88)

i=1

pi

(cid:90)

RD

Theorem 3.2. Let F =(cid:80)

We show the formula for a generalized RBF neuron’s activation. Let us recall that RBF function is
given by RBFc Γ(x) = N (c  Γ)(x).

i piN (mi  Σi) be the mixture of (possibly degenerate) Gaussians and let

RBF unit be parametrized by N (c  Γ). We have:

RBFc Γ(F ) =

piN (mi − c  Γ + Σi)(0).

Proof. We have:

RBFc Γ(F ) =

(cid:90)

RD

RBFc Γ(x)F (x)dx =

N (c  Γ)(x)N (mi  Σi)(x)dx

k(cid:88)

=

k(cid:88)

pi(cid:104)N (c  Γ)  N (mi  Σi)(cid:105) =

piN (mi − c  Γ + Σi)(0).

(3)

i=1

i=1

Network architecture. Adaptation of a given neural network to incomplete data relies on the
following steps:

1. Estimation of missing data density with the use of mixture of diagonal Gaussians. If data
satisfy missing at random assumption (MAR)  then we can adapt EM algorithm to estimate
incomplete data density with the use of GMM. In more general case  we can let the network
to learn optimal parameters of GMM with respect to its cost function4. The later case was
examined in the experiment.

4If huge amount of complete data is available during training  one should use variants of EM algorithm to
estimate data density. It could be either used directly as a missing data density or tuned by neural networks with
small amount of missing data.

4

2. Generalization of neuron’s response. A missing data point (x  J) is interpreted as the mixture
of degenerate Gaussians FS on S = Aﬀ[x  J]. Thus we need to generalize the activation
functions of all neurons in the ﬁrst hidden layer of the network to process probability
measures. In consequence  the response of n(·) on (x  J) is given by n(FS).

The rest of the architecture does not change  i.e. the modiﬁcation is only required on the ﬁrst hidden
layer.
Observe that our generalized network can also process classical points  which do not contain any
missing values. In this case  generalized neurons reduce to classical ones  because missing data
density F is only used to estimate possible values at absent attributes. If all attributes are complete
then this density is simply not used. In consequence  if we want to use missing data in testing stage 
we need to feed the network with incomplete data in training to ﬁt accurate density model.

4 Theoretical analysis

There appears a natural question: how much information we lose using generalized neuron’s activation
at the ﬁrst layer? Our main theoretical result shows that our approach does not lead to the lose of
information  which justiﬁes our reasoning from a theoretical perspective. For a transparency  we will
work with general probability measures instead of density functions. The generalized response of
neuron n : RD → R evaluated on a probability measure µ is given by:

(cid:90)

n(µ) :=

n(x)dµ(x).

The following theorem shows that a neural network with generalized ReLU units is able to identify
any two probability measures. The proof is a natural modiﬁcation of the respective standard proofs of
Universal Approximation Property (UAP)  and therefore we present only its sketch. Observe that all
generalized ReLU return ﬁnite values iff a probability measure µ satisﬁes the condition

(cid:90)

(cid:107)x(cid:107)dµ(x) < ∞.

That is the reason why we reduce to such measures in the following theorem.
Theorem 4.1. Let µ  ν be probabilistic measures satisfying condition (4). If
ReLUw b(µ) = ReLUw b(ν) for w ∈ RD  b ∈ R 

then ν = µ.
Proof. Let us ﬁx an arbitrary w ∈ RD and deﬁne the set

Fw =(cid:8)p : R → R :

(cid:90)

p(wT x)dµ(x) =

(cid:90)

p(wT x)dν(x)(cid:9).

(4)

(5)

Our main step in the proof lies in showing that Fw contains all continuous bounded functions.
Let ri ∈ R such that −∞ = r0 < r1 < . . . < rl−1 < rl = ∞ and qi ∈ R such that q0 = q1 = 0 =
ql−1 = ql  be given. Let Q : R → R be a piecewise linear continuous function which is afﬁne linear
on intervals [ri  ri+1] and such that Q(ri) = qi. We show that Q ∈ Fw. Since

Q =

qi · Tri−1 ri ri+1 

where the tent-like piecewise linear function T is deﬁned by
0 for r ≤ p0 
r−p0
p1−p0
p2−r
p2−p1
0 for r ≥ p2 

Tp0 p1 p2(r) =

for r ∈ [p0  p1] 
for r ∈ [p1  p2] 

l−1(cid:88)

i=1



5

original mask

k-nn mean

dropout

our CE

Figure 2: Reconstructions of partially incomplete images using the autoencoder. From left: (1) origi-
nal image  (2) image with missing pixels passed to autoencooder; the output produced by autoencoder
when unknown pixels were initially ﬁlled by (3) k-nn imputation and (4) mean imputation; (5) the
results obtained by autoencoder with dropout  (6) our method and (7) context encoder. All columns
except the last one were obtained with loss function computed based on pixels from outside the mask
(no fully observable data available in training phase). It can be noticed that our method gives much
sharper images than the competitive methods.

it is sufﬁcient to prove that T ∈ Fw. Let Mp(r) = max(0  r − p). Clearly

Tp0 p1 p2 =

1

p1 − p0

· (Mp0 − Mp1) −

1

p2 − p1

· (Mp2 − Mp1).

However  directly from (5) we see that Mp ∈ Fw for every p  and consequently T and Q are also in
Fw.
Now let us ﬁx an arbitrary bounded continuous function G. We show that G ∈ Fw. To observe
this  take an arbitrary uniformly bounded sequence of piecewise linear functions described before
which is convergent pointwise to G. By the Lebesgue dominated convergence theorem we obtain that
G ∈ Fw.
Therefore cos(·)  sin(·) ∈ Fw holds consequently also for the function eir = cos r + i sin r we have
the equality

(cid:90)

(cid:90)

exp(iwT x)dµ(x) =

exp(iwT x)dν(x).

Since w ∈ RD was chosen arbitrarily  this means that the characteristic functions of two measures
coincide  and therefore µ = ν.

It is possible to obtain an analogical result for RBF activation function. Moreover  we can also get
more general result under stronger assumptions on considered probability measures. More precisely 
if a given family of neurons satisﬁes UAP  then their generalization is also capable of identifying
any probability measure with compact support. Complete analysis of both cases is presented in
Supplementary Material (section 2).

5 Experiments

We evaluated our model on three types of architectures. First  as a proof of concept  we veriﬁed the
proposed approach in the context of autoencoder (AE). Next we applied multilayer perceptron (MLP)
to multiclass classiﬁcation problem and ﬁnally we used shallow radial basis function network (RBFN)
in binary classiﬁcation. For a comparison we only considered methods with publicly available codes
and thus many methods described in the related work section have not been taken into account.
The code implementing the proposed method is available at https://github.com/lstruski/
Processing-of-missing-data-by-neural-networks.
Autoencoder. Autoencoder (AE) is usually used for generating compressed representation of data.
However  in this experiment  we were interested in restoring corrupted images  where part of data
was hidden.

6

Table 1: Mean square error of reconstruction on MNIST incomplete images (we report the errors
calculated over the whole area  inside and outside the mask). Described errors are obtained for images
with intensities scaled to [0  1].

Total error
Error inside the mask
Error outside the mask

k-nn

0.01189
0.00722
0.00468

only missing data
mean
dropout

our

0.01727
0.00898
0.00829

0.01379
0.00882
0.00498

0.01056
0.00810
0.00246

complete data

CE

0.01326
0.00710
0.00617

As a data set  we used grayscale handwritten digits retrieved from MNIST database. For each image
of the size 28 × 28 = 784 pixels  we removed a square patch of the size5 13 × 13. The location of
the patch was uniformly sampled for each image. AE used in the experiments consists of 5 hidden
layers with 256  128  64  128  256 neurons in subsequent layers. The ﬁrst layer was parametrized by
ReLU activation functions  while the remaining units used sigmoids6.
As describe in Section 1  our model assumes that there is no complete data in training phase.
Therefore  the loss function was computed based only on pixels from outside the mask.
As a baseline  we considered combination of analogical architecture with popular imputation tech-
niques:
k-nn: Missing features were replaced with mean values of those features computed from the K
nearest training samples (we used K = 5). Neighborhood was measured using Euclidean distance in
the subspace of observed features.
mean: Missing features were replaced with mean values of those features computed for all (incom-
plete) training samples.
dropout: Input neurons with missing values were dropped7.
Additionally  we used a type of context encoder (CE)  where missing features were replaced with
mean values  however in contrast to mean imputation  the complete data were used as an output of
the network in training phase. This model was expected to perform better  because it used complete
data in computing the network loss function.
Incomplete inputs and their reconstructions obtained with various approaches are presented in Figure
2 (more examples are included in Supplementary Material  section 3). It can be observed that our
method gives sharper images then the competitive methods. In order to support the qualitative results 
we calculated mean square error of reconstruction (see Table 1). Quantitative results conﬁrm that
our method has lower error than imputation methods  both inside and outside the mask. Moreover  it
overcomes CE in case of the whole area and the area outside the mask. In case of the area inside the
mask  CE error is only slightly better than ours  however CE requires complete data in training.
Multilayer perceptron. In this experiment  we considered a typical MLP architecture with 3 ReLU
hidden layers. It was applied to multiclass classiﬁcation problem on Epileptic Seizure Recognition
data set (ESR) taken from [35]. Each 178-dimensional vector (out of 11500 samples) is EEG
recording of a given person for 1 second  categorized into one of 5 classes. To generate missing
attributes  we randomly removed 25%  50%  75% and 90% of values.
In addition to the imputation methods described in the previous experiment  we also used iterative
ﬁlling of missing attributes using Multiple Imputation by Chained Equation (mice)  where several
imputations are drawing from the conditional distribution of data by Markov chain Monte Carlo
techniques [10  11]. Moreover  we considered the mixture of Gaussians (gmm)  where missing

5In the case when the removed patch size was smaller  all considered methods performed very well and

cannot be visibly distinguished.

obtained were less plausible.

6We also experimented with ReLU in remaining layers (except the last one)  however the results we have
7Values of the remaining neurons were divided by 1 − dropout rate

7

Table 2: Classiﬁcation results on ESR data obtained using MLP (the results of CE are not bolded 
because it had access to complete examples).

only missing data

complete data

% of missing

k-nn

mice mean

gmm dropout

our

25%
50%
75%
90%

0.773
0.773
0.628
0.615

0.823
0.816
0.786
0.670

0.799
0.703
0.624
0.596

0.823
0.801
0.748
0.697

0.796
0.780
0.755
0.749

0.815
0.817
0.787
0.760

CE

0.812
0.813
0.792
0.771

Table 3: Summary of data sets with internally absent attributes.

Data set
bands
kidney disease
hepatitis
horse
mammographics
pima
winconsin

#Instances
539
400
155
368
961
768
699

#Attributes
19
24
19
22
5
8
9

#Missing
5.38%
10.54%
5.67%
23.80%
3.37%
12.24%
0.25%

features were replaced with values sampled from GMM estimated from incomplete data using EM
algorithm8.
We applied double 5-fold cross-validation procedure to report classiﬁcation results and we tuned
required hyper-parameters. The number of the mixture components for our method was selected in
the inner cross-validation from the possible values {2  3  5}. Initial mixture of Gaussians was selected
using classical GMM with diagonal matrices. The results were assessed using classical accuracy
measure.
The results presented in Table 2 show the advantage of our model over classical imputation methods 
which give reasonable results only for low number of missing values. It is also slightly better than
dropout  which is more robust to the number of absent attributes than typical imputations. It can be
seen that our method gives comparable scores to CE  even though CE had access to complete training
data. We also ran MLP on complete ESR data (with no missing attributes)  which gave 0.836 of
accuracy.
Radial basis function network. RBFN can be considered as a minimal architecture implementing
our model  which contains only one hidden layer. We used cross-entropy function applied on a
softmax in the output layer. This network suits well for small low-dimensional data.
For the evaluation  we considered two-class data sets retrieved from UCI repository [36] with
internally missing attributes  see Table 3 (more data sets are included in Supplementary Materials 
section 4). Since the classiﬁcation is binary  we extended baseline with two additional SVM kernel
models which work directly with incomplete data without performing any imputations:
geom: Its objective function is based on the geometric interpretation of the margin and aims to
maximize the margin of each sample in its own relevant subspace [26].
karma: This algorithm iteratively tunes kernel classiﬁer under low-rank assumptions [29].
The above SVM methods were combined with RBF kernel function.
We applied analogical cross-validation procedure as before. The number of RBF units was selected in
the inner cross-validation from the range {25  50  75  100}. Initial centers of RBFNs were randomly
selected from training data while variances were samples from N (0  1). For SVM methods  the
margin parameter C and kernel radius γ were selected from {2k : k = −5 −3  . . .   9} for both
parameters. For karma  additional parameter γkarma was selected from the set {1  2}.

8Due to the high-dimensionality of MNIST data  mice was not able to construct imputations in previous
experiment. Analogically  EM algorithm was not able to ﬁt GMM because of singularity of covariance matrices.

8

Table 4: Classiﬁcation results obtained using RBFN (the results of CE are not bolded  because it had
access to complete examples).

data
bands
kidney
hepatitis
horse
mammogr.
pima
winconsin

karma

geom k-nn

only missing data
mice mean

gmm dropout

our

0.580
0.995
0.665
0.826
0.773
0.768
0.958

0.571
0.986
0.817
0.822
0.815
0.766
0.958

0.520
0.992
0.825
0.807
0.822
0.767
0.967

0.544
0.992
0.792
0.820
0.825
0.769
0.970

0.545
0.985
0.825
0.793
0.819
0.760
0.965

0.577
0.980
0.820
0.818
0.803
0.742
0.957

0.616
0.983
0.780
0.823
0.814
0.754
0.964

0.598
0.993
0.846
0.864
0.831
0.747
0.970

complete data

CE

0.621
0.996
0.843
0.858
0.822
0.743
0.968

The results  presented in Table 4  indicate that our model outperformed imputation techniques in
almost all cases. It partially conﬁrms that the use of raw incomplete data in neural networks is usually
better approach than ﬁlling missing attributes before learning process. Moreover  it obtained more
accurate results than modiﬁed kernel methods  which directly work on incomplete data.

6 Conclusion

In this paper  we proposed a general approach for adapting neural networks to process incomplete
data  which is able to train on data set containing only incomplete samples. Our strategy introduces
input layer for processing missing data  which can be used for a wide range of networks and does
not require their extensive modiﬁcations. Thanks to representing incomplete data with probability
density function  it is possible to determine more generalized and accurate response (activation)
of the neuron. We showed that this generalization is justiﬁed from a theoretical perspective. The
experiments conﬁrm its practical usefulness in various tasks and for diverse network architectures. In
particular  it gives comparable results to the methods  which require complete data in training.

Acknowledgement

This work was partially supported by National Science Centre  Poland (grants no.
2016/21/D/ST6/00980  2015/19/B/ST6/01819  2015/19/D/ST6/01215  2015/19/D/ST6/01472). We
would like to thank the anonymous reviewers for their valuable comments on our paper.

References
[1] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep learning. MIT press  2016.

[2] Deepak Pathak  Philipp Krahenbuhl  Jeff Donahue  Trevor Darrell  and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 2536–2544  2016.

[3] Chao Yang  Xin Lu  Zhe Lin  Eli Shechtman  Oliver Wang  and Hao Li. High-resolution image
inpainting using multi-scale neural patch synthesis. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  volume 1  page 3  2017.

[4] Junyuan Xie  Linli Xu  and Enhong Chen. Image denoising and inpainting with deep neural

networks. In Advances in neural information processing systems  pages 341–349  2012.

[5] Raymond A Yeh  Chen Chen  Teck Yian Lim  Alexander G Schwing  Mark Hasegawa-Johnson 
and Minh N Do. Semantic image inpainting with deep generative models. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 5485–5493  2017.

[6] Patrick E McKnight  Katherine M McKnight  Souraya Sidani  and Aurelio Jose Figueredo.

Missing data: A gentle introduction. Guilford Press  2007.

[7] Peter K Sharpe and RJ Solly. Dealing with missing values in neural network-based diagnostic

systems. Neural Computing & Applications  3(2):73–77  1995.

9

[8] Dušan Sovilj  Emil Eirola  Yoan Miche  Kaj-Mikael Björk  Rui Nian  Anton Akusok  and
Amaury Lendasse. Extreme learning machine for missing data using multiple imputations.
Neurocomputing  174:220–231  2016.

[9] Gustavo EAPA Batista  Maria Carolina Monard  et al. A study of k-nearest neighbour as an

imputation method. HIS  87(251-260):48  2002.

[10] Stef Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained

equations in r. Journal of statistical software  45(3)  2011.

[11] Melissa J Azur  Elizabeth A Stuart  Constantine Frangakis  and Philip J Leaf. Multiple imputa-
tion by chained equations: what is it and how does it work? International journal of methods in
psychiatric research  20(1):40–49  2011.

[12] Jinsung Yoon  James Jordon  and Mihaela van der Schaar. Gain: Missing data imputation using

generative adversarial nets. pages 5689–5698  2018.

[13] Maya Gupta  Andrew Cotter  Jan Pfeifer  Konstantin Voevodski  Kevin Canini  Alexander
Mangylov  Wojciech Moczydlowski  and Alexander Van Esbroeck. Monotonic calibrated
interpolated look-up tables. The Journal of Machine Learning Research  17(1):3790–3836 
2016.

[14] Zoubin Ghahramani and Michael I Jordan. Supervised learning from incomplete data via an
EM approach. In Advances in Neural Information Processing Systems  pages 120–127. Citeseer 
1994.

[15] Volker Tresp  Subutai Ahmad  and Ralph Neuneier. Training neural networks with deﬁcient

data. In Advances in neural information processing systems  pages 128–135  1994.

[16] Marek ´Smieja  Łukasz Struski  and Jacek Tabor. Generalized rbf kernel for incomplete data.

arXiv preprint arXiv:1612.01480  2016.

[17] David Williams  Xuejun Liao  Ya Xue  and Lawrence Carin. Incomplete-data classiﬁcation
using logistic regression. In Proceedings of the International Conference on Machine Learning 
pages 972–979. ACM  2005.

[18] Alexander J Smola  SVN Vishwanathan  and Thomas Hofmann. Kernel methods for missing
variables. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics.
Citeseer  2005.

[19] David Williams and Lawrence Carin. Analytical kernel matrix completion with incomplete
multi-view data. In Proceedings of the ICML Workshop on Learning With Multiple Views  2005.

[20] Pannagadatta K Shivaswamy  Chiranjib Bhattacharyya  and Alexander J Smola. Second order
cone programming approaches for handling missing and uncertain data. Journal of Machine
Learning Research  7:1283–1314  2006.

[21] Diego PP Mesquita  João PP Gomes  and Leonardo R Rodrigues. Extreme learning machines
for datasets with missing values using the unscented transform. In Intelligent Systems (BRACIS) 
2016 5th Brazilian Conference on  pages 85–90. IEEE  2016.

[22] Xuejun Liao  Hui Li  and Lawrence Carin. Quadratically gated mixture of experts for incomplete
data classiﬁcation. In Proceedings of the International Conference on Machine Learning  pages
553–560. ACM  2007.

[23] Uwe Dick  Peter Haider  and Tobias Scheffer. Learning from incomplete data with inﬁnite
imputations. In Proceedings of the International Conference on Machine Learning  pages
232–239. ACM  2008.

[24] Ofer Dekel  Ohad Shamir  and Lin Xiao. Learning to classify with missing and corrupted

features. Machine Learning  81(2):149–178  2010.

[25] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion.
In Proceedings of the International Conference on Machine Learning  pages 353–360. ACM 
2006.

10

[26] Gal Chechik  Geremy Heitz  Gal Elidan  Pieter Abbeel  and Daphne Koller. Max-margin
classiﬁcation of data with absent features. Journal of Machine Learning Research  9:1–21 
2008.

[27] Jing Xia  Shengyu Zhang  Guolong Cai  Li Li  Qing Pan  Jing Yan  and Gangmin Ning. Adjusted
weight voting algorithm for random forests in handling missing values. Pattern Recognition 
69:52–60  2017.

[28] Kristiaan Pelckmans  Jos De Brabanter  Johan AK Suykens  and Bart De Moor. Handling
missing values in support vector machine classiﬁers. Neural Networks  18(5):684–692  2005.

[29] Elad Hazan  Roi Livni  and Yishay Mansour. Classiﬁcation with low rank and missing data. In
Proceedings of The 32nd International Conference on Machine Learning  pages 257–266  2015.

[30] Andrew Goldberg  Ben Recht  Junming Xu  Robert Nowak  and Xiaojin Zhu. Transduction with
matrix completion: Three birds with one stone. In Advances in neural information processing
systems  pages 757–765  2010.

[31] Yoshua Bengio and Francois Gingras. Recurrent neural networks for missing or asynchronous

data. In Advances in neural information processing systems  pages 395–401  1996.

[32] Robert K Nowicki  Rafal Scherer  and Leszek Rutkowski. Novel rough neural network for
classiﬁcation with missing data. In Methods and Models in Automation and Robotics (MMAR) 
2016 21st International Conference on  pages 820–825. IEEE  2016.

[33] Ian Goodfellow  Mehdi Mirza  Aaron Courville  and Yoshua Bengio. Multi-prediction deep
boltzmann machines. In Advances in Neural Information Processing Systems  pages 548–556 
2013.

[34] Calyampudi Radhakrishna Rao  Calyampudi Radhakrishna Rao  Mathematischer Statistiker 
Calyampudi Radhakrishna Rao  and Calyampudi Radhakrishna Rao. Linear statistical inference
and its applications  volume 2. Wiley New York  1973.

[35] Ralph G Andrzejak  Klaus Lehnertz  Florian Mormann  Christoph Rieke  Peter David  and
Christian E Elger. Indications of nonlinear deterministic and ﬁnite-dimensional structures in
time series of brain electrical activity: Dependence on recording region and brain state. Physical
Review E  64(6):061907  2001.

[36] Arthur Asuncion and David J. Newman. UCI Machine Learning Repository  2007.

11

,Ming Yu
Mladen Kolar
Varun Gupta
Marek Śmieja
Łukasz Struski
Jacek Tabor
Bartosz Zieliński
Przemysław Spurek
Xiuyuan Lu
Benjamin Van Roy