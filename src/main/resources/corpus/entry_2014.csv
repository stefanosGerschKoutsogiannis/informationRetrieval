2009,Fast Graph Laplacian Regularized Kernel Learning via Semidefinite–Quadratic–Linear Programming,Kernel learning is a powerful framework for nonlinear data modeling. Using the kernel trick  a number of problems have been formulated as semidefinite programs (SDPs). These include Maximum Variance Unfolding (MVU) (Weinberger et al.  2004) in nonlinear dimensionality reduction  and Pairwise Constraint Propagation (PCP) (Li et al.  2008) in constrained clustering. Although in theory SDPs can be efficiently solved  the high computational complexity incurred in numerically processing the huge linear matrix inequality constraints has rendered the SDP approach unscalable. In this paper  we show that a large class of kernel learning problems can be reformulated as semidefinite-quadratic-linear programs (SQLPs)  which only contain a simple positive semidefinite constraint  a second-order cone constraint and a number of linear constraints. These constraints are much easier to process numerically  and the gain in speedup over previous approaches is at least of the order $m^{2.5}$  where m is the matrix dimension. Experimental results are also presented to show the superb computational efficiency of our approach.,Fast Graph Laplacian Regularized Kernel Learning
via Semideﬁnite–Quadratic–Linear Programming

Xiao-Ming Wu

Dept. of IE

The Chinese University of Hong Kong

Anthony Man-Cho So

Dept. of SE&EM

The Chinese University of Hong Kong

wxm007@ie.cuhk.edu.hk

manchoso@se.cuhk.edu.hk

Zhenguo Li
Dept. of IE

The Chinese University of Hong Kong

Shuo-Yen Robert Li

Dept. of IE

The Chinese University of Hong Kong

zgli@ie.cuhk.edu.hk

bobli@ie.cuhk.edu.hk

Abstract

Kernel learning is a powerful framework for nonlinear data modeling. Using the
kernel trick  a number of problems have been formulated as semideﬁnite programs
(SDPs). These include Maximum Variance Unfolding (MVU) (Weinberger et al. 
2004) in nonlinear dimensionality reduction  and Pairwise Constraint Propagation
(PCP) (Li et al.  2008) in constrained clustering. Although in theory SDPs can
be efﬁciently solved  the high computational complexity incurred in numerically
processing the huge linear matrix inequality constraints has rendered the SDP
approach unscalable. In this paper  we show that a large class of kernel learning
problems can be reformulated as semideﬁnite-quadratic-linear programs (SQLPs) 
which only contain a simple positive semideﬁnite constraint  a second-order cone
constraint and a number of linear constraints. These constraints are much easier to
process numerically  and the gain in speedup over previous approaches is at least
of the order m2.5  where m is the matrix dimension. Experimental results are also
presented to show the superb computational efﬁciency of our approach.

1 Introduction

Kernel methods provide a principled framework for nonlinear data modeling  where the inference
in the input space can be transferred intactly to any feature space by simply treating the associ-
ated kernel as inner products  or more generally  as nonlinear mappings on the data (Sch¨olkopf &
Smola  2002). Some well-known kernel methods include support vector machines (SVMs) (Vap-
nik  2000)  kernel principal component analysis (kernel PCA) (Sch¨olkopf et al.  1998)  and kernel
k-means (Shawe-Taylor & Cristianini  2004). Naturally  an important issue in kernel methods is
kernel design. Indeed  the performance of a kernel method depends crucially on the kernel used 
where different choices of kernels often lead to quite different results. Therefore  substantial efforts
have been made to design appropriate kernels for the problems at hand. For instance  in (Chapelle
& Vapnik  2000)  parametric kernel functions are proposed  where the focus is on model selection
(Chapelle & Vapnik  2000). The modeling capability of parametric kernels  however  is limited. A
more natural idea is to learn specialized nonparametric kernels for speciﬁc problems. For instance 
in cases where only inner products of the input data are involved  kernel learning is equivalent to the
learning of a kernel matrix. This is the main focus of recent kernel methods.
Currently  many different kernel learning frameworks have been proposed. These include spectral
kernel learning (Li & Liu  2009)  multiple kernel learning (Lanckriet et al.  2004)  and the Breg-

1

man divergence-based kernel learning (Kulis et al.  2009). Typically  a kernel learning framework
consists of two main components: the problem formulation in terms of the kernel matrix  and an
optimization procedure for ﬁnding the kernel matrix that has certain desirable properties. As seen
in  e.g.  the Maximum Variance Unfolding (MVU) method (Weinberger et al.  2004) for nonlinear
dimensionality reduction (see (So  2007) for related discussion) and Pairwise Constraint Propaga-
tion (PCP) (Li et al.  2008) for constrained clustering  a nice feature of such a framework is that
the problem formulation often becomes straightforward. Thus  the major challenge in optimization-
based kernel learning lies in the second component  where the key is to ﬁnd an efﬁcient procedure
to obtain a positive semideﬁnite kernel matrix that satisﬁes certain properties.
Using the kernel trick  most kernel learning problems (Graepel  2002; Weinberger et al.  2004;
Globerson & Roweis  2007; Song et al.  2008; Li et al.  2008) can naturally be formulated as
semideﬁnite programs (SDPs). Although in theory SDPs can be efﬁciently solved  the high computa-
tional complexity has rendered the SDP approach unscalable. An effective and widely used heuristic
for speedup is to perform low-rank kernel approximation and matrix factorization (Weinberger et al. 
2005; Weinberger et al.  2007; Li et al.  2009). In this paper  we investigate the possibility of further
speedup by studying a class of convex quadratic semideﬁnite programs (QSDPs). These QSDPs
arise in many contexts  such as graph Laplacian regularized low-rank kernel learning in nonlinear
dimensionality reduction (Sha & Saul  2005; Weinberger et al.  2007; Globerson & Roweis  2007;
Song et al.  2008; Singer  2008) and constrained clustering (Li et al.  2009). In the aforementioned
papers  a QSDP is reformulated as an SDP with O(m2) variables and a linear matrix inequality of
size O(m2) × O(m2). Such a reformulation is highly inefﬁcient and unscalable  as it has an order
of m9 time complexity (Ben-Tal & Nemirovski  2001  Lecture 6). In this paper  we propose a novel
reformulation that exploits the structure of the QSDP and leads to a semideﬁnite-quadratic-linear
program (SQLP) that can be solved by the standard software SDPT3 (T¨ut¨unc¨u et al.  2003). Such a
reformulation has the advantage that it only has one positive semideﬁnite constraint on a matrix of
size m × m  one second-order cone constraint of size O(m2) and a number of linear constraints on
O(m2) variables. As a result  our reformulation is much easier to process numerically. Moreover 
a simple complexity analysis shows that the gain in speedup over previous approaches is at least
of the order m2.5. Experimental results show that our formulation is indeed far more efﬁcient than
previous ones.
The rest of the paper is organized as follows. We review related kernel learning problems in Section
2 and present our formulation in Section 3. Experiment results are reported in Section 4. Section 5
concludes the paper.

2 The Problems

In this section  we brieﬂy review some kernel learning problems that arise in dimensionality re-
duction and constrained clustering. They include MVU (Weinberger et al.  2004)  Colored MVU
(Song et al.  2008)  (Singer  2008)  Pairwise Semideﬁnite Embedding (PSDE) (Globerson & Roweis 
2007)  and PCP (Li et al.  2008). MVU maximizes the variance of the embedding while preserving
local distances of the input data. Colored MVU generalizes MVU with side information such as
class labels. PSDE derives an embedding that strictly respects known similarities  in the sense that
objects known to be similar are always closer in the embedding than those known to be dissimilar.
PCP is designed for constrained clustering  which embeds the data on the unit hypersphere such that
two objects that are known to be from the same cluster are embedded to the same point  while two
objects that are known to be from different clusters are embedded orthogonally. In particular  PCP
seeks the smoothest mapping for such an embedding  thereby propagating pairwise constraints.
Initially  each of the above problems is formulated as an SDP  whose kernel matrix K is of size n×n 
where n denotes the number of objects. Since such an SDP is computationally expensive  one can
try to reduce the problem size by using graph Laplacian regularization. In other words  one takes
K = QY QT   where Q ∈ Rn×m consists of the smoothest m eigenvectors of the graph Laplacian
(m (cid:191) n)  and Y is of size m × m (Sha & Saul  2005; Weinberger et al.  2007; Song et al.  2008;
Globerson & Roweis  2007; Singer  2008; Li et al.  2009). The learning of K is then reduced to
the learning of Y   leading to a quadratic semideﬁnite program (QSDP) that is similar to a standard
quadratic program (QP)  except that the feasible set of a QSDP resides in the positive semideﬁnite
cone as well. The intuition behind this low-rank kernel approximation is that a kernel matrix of the

2

form K = QY QT actually  to some degree  preserves the proximity of objects in the feature space.
Detailed justiﬁcation can be found in the related work mentioned above.
Next  we use MVU and PCP as representatives to demonstrate how the SDP formulations emerge
from nonlinear dimensionality reduction and constrained clustering.

2.1 MVU

The SDP of MVU (Weinberger et al.  2004) is as follows:

tr(K) = I • K

n(cid:88)

max
K

s.t.

kij = 0 

(1)

(2)

i j=1

ij  ∀(i  j) ∈ N  

kii + kjj − 2kij = d2
K (cid:186) 0 

(3)
(4)
where K = (kij) denotes the kernel matrix to be learned  I denotes the identity matrix  tr(·) denotes
the trace of a square matrix  • denotes the element-wise dot product between matrices  dij denotes
the Euclidean distance between the i-th and j-th objects  and N denotes the set of neighbor pairs 
whose distances are to be preserved in the embedding.
The constraint in (2) centers the embedding at the origin  thus removing the translation freedom.
The constraints in (3) preserve local distances. The constraint K (cid:186) 0 in (4) speciﬁes that K must
(cid:80)
be symmetric and positive semideﬁnite  which is necessary since K is taken as the inner product
matrix of the embedding. Note that given the constraint in (2)  the variance of the embedding
is characterized by V(K) = 1
i j(kii + kjj − 2kij) = tr(K) (Weinberger et al.  2004) (see
related discussion in (So  2007)  Chapter 4). Thus  the SDP in (1-4) maximizes the variance of the
embedding while keeping local distances unchanged. After K is obtained  kernel PCA is applied to
K to compute the low-dimensional embedding.

2n

2.2 PCP

The SDP of PCP (Li et al.  2008) is:

¯L • K

(5)

min
K
s.t. kii = 1  i = 1  2  . . .   n 
kij = 1  ∀(i  j) ∈ M 
kij = 0  ∀(i  j) ∈ C 
K (cid:186) 0 

(6)
(7)
(8)
(9)
where ¯L denotes the normalized graph Laplacian  M denotes the set of object pairs that are known
to be from the same cluster  and C denotes those that are known to be from different clusters.
The constraints in (6) map the objects to the unit hypersphere. The constraints in (7) map two objects
that are known to be from the same cluster to the same vector. The constraints in (8) map two objects
that are known to be from different clusters to vectors that are orthogonal. Let X = {xi}n
i=1 be the
data set  F be the feature space  and φ : X → F be the associated feature map of K. Then  the
degree of smoothness of φ on the data graph can be captured by (Zhou et al.  2004):

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176) φ(xi)√
− φ(xj)(cid:112)
(cid:80)n
j=1 wij  and (cid:107) · (cid:107)F is the distance metric in F.
where wij is the similarity of xi and xj  dii =
The smaller the value S(φ)  the smoother is the feature map φ. Thus  the SDP in (5-9) seeks the
smoothest feature map that embeds the data on the unit hypersphere and at the same time respects the
pairwise constraints. After K is solved  kernel k-means is then applied to K to obtain the clusters.

= ¯L • K 

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2

S(φ) =

djj

F

(10)

n(cid:88)

i j=1

1
2

wij

dii

3

2.3 Low-Rank Approximation: from SDP to QSDP

The SDPs in MVU and PCP are difﬁcult to solve efﬁciently because their computational complexity
scales at least cubically in the size of the matrix variable and the number of constraints (Borchers 
1999). A useful heuristic is to use low-rank kernel approximation  which is motivated by the obser-
vation that the degree of freedom in the data is often much smaller than the number of parameters
in a fully nonparametric kernel matrix K. For instance  it may be equal to or slightly larger than
the intrinsic dimension of the data manifold (for dimensionality reduction) or the number of clusters
(for clustering). Another more speciﬁc observation is that it is often desirable to have nearby objects
mapped to nearby points  as is done in MVU or PCP.
Based on these observations  instead of learning a fully nonparametric K  one learns a K of the
form K = QY QT   where Q and Y are of sizes n× m and m× m  respectively  where m (cid:191) n. The
matrix Q should be smooth in the sense that nearby objects in the input space are mapped to nearby
points (the i-th row of Q is taken as a new representation of xi). Q is computed prior to the learning
of K. In this way  the learning of a kernel matrix K is reduced to the learning of a much smaller
Y   subject to the constraint that Y (cid:186) 0. This idea is used in (Weinberger et al.  2007) and (Li et al. 
2009) to speed up MVU and PCP  respectively  and is also adopted in Colored MVU (Song et al. 
2008) and PSDE (Globerson & Roweis  2007) for efﬁcient computation.
The choice of Q can be different for MVU and PCP. In (Weinberger et al.  2007)  Q =
(v2  . . .   vm+1)  where {vi} are the eigenvectors of the graph Laplacian.
In (Li et al.  2009) 
Q = (u1  . . .   um)  where {ui} are the eigenvectors of the normalized graph Laplacian. Since
such Q’s are obtained from graph Laplacians  we call the learning of K of the form K = QY QT
the Graph Laplacian Regularized Kernel Learning problem  and denote the methods in (Weinberger
et al.  2007) and (Li et al.  2009) by RegMVU and RegPCP  respectively.
With K = QY QT   RegMVU and RegPCP become:

((QY QT )ii − 2(QY QT )ij + (QY QT )jj − d2

ij)2 

(cid:88)

(i j)∈N
((QY QT )ij − tij)2 

RegMVU : max
Y (cid:186)0

RegPCP : min
Y (cid:186)0

tr(Y ) − ν

(cid:88)

(i j tij )∈S

(11)

(12)

(13)

where ν > 0 is a regularization parameter and S = {(i  j  tij) | (i  j) ∈ M ∪ C  or i = j  tij =
1 if (i  j) ∈ M or i = j  tij = 0 if (i  j) ∈ C}. Both RegMVU and RegPCP can be succinctly
rewritten in the uniﬁed form:

yT Ay + bT y

min
y
s.t. Y (cid:186) 0 

(14)
where y = vec(Y ) ∈ Rm2 denotes the vector obtained by concatenating all the columns of Y   and
A (cid:186) 0 (Weinberger et al.  2007; Li et al.  2009). Note that this problem is convex since both the
objective function and the feasible set are convex.
Problem (13-14) is an instance of the so-called convex quadratic semideﬁnite program (QSDP) 
where the objective is a quadratic function in the matrix variable Y . Note that similar QSDPs arise
in Colored MVU  PSDE  Conformal Eigenmaps (Sha & Saul  2005)  Locally Rigid Embedding
(Singer  2008)  and Kernel Matrix Completion (Graepel  2002). Before we present our approach for
tackling the QSDP (13-14)  let us brieﬂy review existing approaches in the literature.

2.4 Previous Approach: from QSDP to SDP

Currently  a typical approach for tackling a QSDP is to use the Schur complement (Boyd & Vanden-
berghe  2004) to rewrite it as an SDP (Sha & Saul  2005; Weinberger et al.  2007; Li et al.  2009;
Song et al.  2008; Globerson & Roweis  2007; Singer  2008; Graepel  2002)  and then solve it using
an SDP solver such as CSDP1 (Borchers  1999) or SDPT32 (Toh et al.  2006). In this paper  we call

1https://projects.coin-or.org/Csdp/
2http://www.math.nus.edu.sg/˜mattohkc/sdpt3.html

4

this approach the Schur Complement Based SDP (SCSDP) formulation. For the QSDP in (13-14) 
the equivalent SDP takes the form:

(cid:183)

min
y ν

ν + bT y

s.t. Y (cid:186) 0 and

Im2
1
2 y)T

(A

1
2 y
A
ν

(cid:184)

(cid:186) 0 

(15)

(16)

1

1

1

2 y)T (A

2 y) ≤ ν by the Schur complement.

2 is the matrix square root of A  Im2 is the identity matrix of size m2 × m2  and ν is a slack
where A
variable serving as an upper bound of yT Ay. The second semideﬁnite cone constraint is equivalent
to (A
Although the SDP in (15-16) has only m(m + 1)/2 + 1 variables  it has two semideﬁnite cone
constraints  of sizes m×m and (m2+1)×(m2+1)  respectively. Such an SDP not only scales poorly 
but is also difﬁcult to process numerically. Indeed  by considering Problem (15-16) as an SDP in
the standard dual form  the number of iterations required by standard interior-point algorithms is of
the order m  and the total number of arithmetic operations required is of the order m9 (Ben-Tal &
Nemirovski  2001  Lecture 6). In practice  it takes only a few seconds to solve the aforementioned
SDP when m = 10  but can take more than 1 day when m = 40 (see Section 4 for details). Thus 
it is not surprising that m is set to a very small value in the related methods—for example  m = 10
in (Weinberger et al.  2007) and m = 15 in (Li et al.  2009). However  as noted by the authors in
(Weinberger et al.  2007)  a larger m does lead to better performance. In (Li et al.  2009)  the authors
suggest that m should be larger than the number of clusters.
Is this formulation from QSDP to SDP the best we can have? The answer is no. In the next section 
we present a novel formulation that leads to a semideﬁnite-quadratic-linear program (SQLP)  which
is much more efﬁcient and scalable than the one above. For instance  it takes about 15 seconds when
m = 30  2 minutes when m = 40  and 1 hour when m = 100  as reported in Section 4.

3 Our Formulation: from QSDP to SQLP

In this section  we formulate the QSDP in (13-14) as an SQLP. Though our focus here is on the
QSDP in (13-14)  we should point out that our method applies to any convex QSDP.
Recall that the size of A is m2 × m2. Let r be the rank of A. With Cholesky factorization  we can
obtain an r× m2 matrix B such that A = BT B  as A is symmetric positive semideﬁnite and of rank
r (Golub & Loan  1996). Now  let z = By. Then  the QSDP in (13-14) is equivalent to:

min
y z µ

µ + bT y

(17)

(18)
(19)
(20)
Next  we show that the constraint in (19) is equivalent to a second-order cone constraint. Let Kn be
the second-order cone of dimension n  i.e. 

s.t. z = By 
zT z ≤ µ 
Y (cid:186) 0.

Kn = {(x0; x) ∈ Rn : x0 ≥ (cid:107)x(cid:107)} 
where (cid:107) · (cid:107) denotes the standard Euclidean norm. Let u = ( 1+µ
holds.
Theorem 3.1. zT z ≤ µ if and only if u ∈ Kr+2.
Proof. Note that u ∈ Rr+2  since z ∈ Rr. Also  note that µ = ( 1+µ
then ( 1+µ
u ∈ Kr+2. Conversely  if u ∈ Kr+2  then ( 1+µ

2 )2 = µ ≥ zT z  which means that 1+µ
2 )2 ≥ ( 1−µ

2 )2 − ( 1−µ

2   1−µ

2 )2 − ( 1−µ

2 )2. If zT z ≤ µ 
2   zT )T(cid:107). In particular  we have

2 ≥ (cid:107)( 1−µ
2 )2 + zT z  thus implying zT z ≤ µ.

2   zT )T . Then  the following

Let ei (where i = 1  2  . . .   r + 2) be the i-th basis vector  and let C = (0r×2  Ir×r). Then  we have
(e1 − e2)T u = µ  (e1 + e2)T u = 1  and z = Cu. Hence  by Theorem 3.1  the problem in (17-20)

5

(a)

(b)

Figure 1: Swiss Roll. (a) The true manifold. (b) A set of 2000 points sampled from the manifold.

is equivalent to:

(e1 − e2)T u + bT y

min
y u
s.t. (e1 + e2)T u = 1 

By − Cu = 0 
u ∈ Kr+2 
Y (cid:186) 0 

(21)

(22)
(23)
(24)
(25)

which is an instance of the SQLP problem (T¨ut¨unc¨u et al.  2003). Note that in this formulation 
we have traded the semideﬁnite cone constraint of size (m2 + 1) × (m2 + 1) in (16) with one
second-order cone constraint of size r + 2 and r + 1 linear constraints. As it turns out  such a
formulation is much easier to process numerically and can be solved much more efﬁciently. Indeed 
m (Ben-
using standard interior-point algorithms  the number of iterations required is of the order
Tal & Nemirovski  2001  Lecture 6)  and the total number of arithmetic operations required is of the
order m6.5 (T¨ut¨unc¨u et al.  2003). This compares very favorably with the m9 arithmetic complexity
of the SCSDP approach  and our experimental results indicate that the speedup in computation is
quite substantial. Moreover  in contrast with the SCSDP formulation  which does not take advantage
of the low rank structure of A  our formulation does take advantage of such a structure.

√

4 Experimental Results

In this section  we perform several experiments to demonstrate the viability of our SQLP formulation
and its superior computational performance. Since both the SQLP formulation and the previous
SCSDP formulation can be solved by standard softwares to a satisfying gap tolerance  the focus in
this comparison is not on the accuracy aspect but on the computational efﬁciency aspect.
We set the relative gap tolerance for both algorithms to be 1e-08. We use SDPT3 (Toh et al.  2006;
T¨ut¨unc¨u et al.  2003) to solve the SQLP. We follow (Weinberger et al.  2007; Li et al.  2009) and
use CSDP 6.0.1 (Borchers  1999) to solve the SCSDP. All experiments are conducted in Matlab
7.6.0(R2008a) on a PC with 2.5GHz CPU and 4GB RAM.
Two benchmark databases  Swiss Roll3 and USPS4 are used in our experiments. Swiss Roll (Fig-
ure 1(a)) is a standard manifold model used for manifold learning and nonlinear dimensionality
reduction. In the experiments  we use the data set shown in Figure 1(b)  which consists of 2000
points sampled from the Swiss Roll manifold. USPS is a handwritten digits database widely used
for clustering and classiﬁcation. It contains images of handwritten digits from 0 to 9 of size 16× 16 
and has 7291 training examples and 2007 test examples. In the experiments  we use a subset of
USPS with 2000 images  containing the ﬁrst 200 examples of each digit from 0-9 in the training
data. The feature to represent each image is a vector formed by concatenating all the columns of the
image intensities. In the sequel  we shall refer to the two subsets used in the experiments simply as
Swiss Roll and USPS.

3http://www.cs.toronto.edu/˜roweis/lle/code.html
4http://www-stat.stanford.edu/˜tibs/ElemStatLearn/

6

swiss rollsample (n=2000)Table 1: Computational Results on Swiss Roll (Time: second; # Iter: number of iterations)

Table 2: Computational Results on USPS (Time: second; # Iter: number of iterations)

m
10
15
20
25
30
35
40
50
60
80
100

Time
3.84
60.36
557.79
2821.76
13039.30
38559.50
> 1 day

—
—
—
—

0.13
2.01
17.43
82.99
352.41
1168.50

SCSDP
# Iter Time per Iter
29
30
32
34
37
33
—
—
—
—
—

—
—
—
—
—

m
10
15
20
25
30
35
40
50
60
80
100

Time
2.84
42.96
461.38
2572.72
10576.01
35173.60
> 1 day

—
—
—
—

0.14
1.95
17.09
82.99
352.53
1172.50

SCSDP
# Iter Time per Iter
21
22
27
31
30
30
—
—
—
—
—

—
—
—
—
—

Time
0.96
1.75
4.48
7.84
13.39
29.74
74.01
213.26
467.90
1729.65
3988.31

Time
0.47
1.26
3.35
5.97
15.72
44.53
133.58
362.24
936.58
1784.12
2900.44

SQLP

# Iter Time per Iter
32
31
35
37
35
35
35
35
35
39
36

0.03
0.06
0.13
0.21
0.38
0.85
2.12
6.09
13.37
44.35
110.79

SQLP

# Iter Time per Iter
16
17
17
14
19
17
20
16
19
17
17

0.03
0.07
0.20
0.43
0.83
2.62
6.68
22.64
49.29
104.95
170.61

rank(A)

64
153
264
403
537
670
852
1152
1451
2062
2623

rank(A)

100
225
400
625
900
1225
1600
2379
2938
3112
3111

The Swiss Roll (resp. USPS) is used to derive the QSDP in RegMVU (resp. RegPCP). For RegMVU 
the 4NN graph is used  i.e.  wij = 1 if xi is within the 4NN of xj or vice versa  and wij = 0
otherwise. We veriﬁed that the 4NN graph derived from our Swiss Roll data is connected. For
RegPCP  we construct the graph following the approach suggested in (Li et al.  2009). Speciﬁcally 
we have wij = exp(−d2
ij/(2σ2)) if xi is within 20NN of xj or vice versa  and wij = 0 otherwise.
Here  σ is the averaged distance from each object to its 20-th nearest neighbor. For the pairwise
constraints used in RegPCP  we randomly generate 20 similarity constraints for each class  and 20
dissimilarity constraints for every two classes  yielding a total of 1100 constraints. For each data set 
m ranges over {10  15  20  25  30  35  40  50  60  80  100}. In summary  for each data set  11 QSDPs
are formed. Each QSDP gives rise to one SQLP and one SCSDP. Therefore  for each data set  11
SQLPs and 11 SCSDPs are derived.

4.1 The Results

The computational results of the programs are shown in Tables 1 and 2. For each program  we
report the total computation time  the number of iterations needed to achieve the required tolerance 
and the average time per iteration in solving the program. A dash (—) in the box indicates that the
corresponding program takes too much time to solve. We choose to stop the program if it fails to
converge within 1 day. This happens for the SCSDP with m = 40 on both data sets.
¿From the tables  we see that solving an SQLP is consistently much more faster than solving an
SCSDP. To see the scalability  we plot the solution time (Time) against the problem size (m) in
Figure 2. It can be seen that the solution time of the SCSDP grows much faster than that of the
SQLP. This demonstrates the superiority of our proposed approach.

7

(a)

(b)

Figure 2: Curves on computational cost: Solution Time vs. Problem Scale.

We also note that the per-iteration computational costs of SCSDP and SQLP are drastically different.
Indeed  for the same problem size m  it takes much less time per iteration for the SQLP than that for
the SCSDP. This is not very surprising  as the SQLP formulation takes advantage of the low rank
structure of the data matrix A.

5 Conclusions

We have studied a class of convex optimization programs called convex Quadratic Semideﬁnite
Programs (QSDPs)  which arise naturally from graph Laplacian regularized kernel learning (Sha &
Saul  2005; Weinberger et al.  2007; Li et al.  2009; Song et al.  2008; Globerson & Roweis  2007;
Singer  2008). A QSDP is similar to a QP  except that it is subject to a semideﬁnite cone constraint
as well. To tackle the QSDP  one typically uses the Schur complement to rewrite it as an SDP
(SCSDP)  thus resulting in a large linear matrix inequality constraint. In this paper  we argue that
this formulation is not computationally optimal and have proposed a novel formulation that leads to
a semideﬁnite-quadratic-linear program (SQLP). Our formulation introduces one positive semidef-
inite constraint  one second-order cone constraint and a set of linear constraints. This should be
contrasted with the two large semideﬁnite cone constraints in the SCSDP. Our complexity analysis
and experimental results have shown that the proposed SQLP formulation scales far better than the
SCSDP formulation.

Acknowledgements

The authors would like to thank Professor Kim-Chuan Toh for his valuable comments. This re-
search work was supported in part by GRF grants CUHK 2150603  CUHK 414307 and CRF grant
CUHK2/06C from the Research Grants Council of the Hong Kong SAR  China  as well as the
NSFC-RGC joint research grant N CUHK411/07.

References
Ben-Tal  A.  & Nemirovski  A. (2001). Lectures on Modern Convex Optimization: Analysis  Algorithms  and
Engineering Applications. MPS–SIAM Series on Optimization. Philadelphia  Pennsylvania: Society for
Industrial and Applied Mathematics.

Borchers  B. (1999). CSDP  a C Library for Semideﬁnite Programming. Optimization Methods and Software 

11/12  613–623.

Boyd  S.  & Vandenberghe  L. (2004). Convex Optimization. Cambridge: Cambridge University Press. Avail-

able online at http://www.stanford.edu/˜boyd/cvxbook/.

Chapelle  O.  & Vapnik  V. (2000). Model Selection for Support Vector Machines. In S. A. Solla  T. K. Leen
and K.-R. M¨uller (Eds.)  Advances in Neural Information Processing Systems 12: Proceedings of the 1999
Conference  230–236. Cambridge  Massachusetts: The MIT Press.

8

101520253035405060801000.511.522.533.5x 104mTime (second)Swiss Roll  SCSDPSQLP101520253035405060801000.511.522.533.5x 104mTime (second)USPS  SCSDPSQLPGloberson  A.  & Roweis  S. (2007). Visualizing Pairwise Similarity via Semideﬁnite Programming. Proceed-

ings of the 11th International Conference on Artiﬁcial Intelligence and Statistics (pp. 139–146).

Golub  G. H.  & Loan  C. F. V. (1996). Matrix Computations. Baltimore  Maryland: The Johns Hopkins

University Press. Third edition.

Graepel  T. (2002). Kernel Matrix Completion by Semideﬁnite Programming. Proceedings of the 12th Inter-

national Conference on Artiﬁcial Neural Networks (pp. 694–699). Springer–Verlag.

Kulis  B.  Sustik  M. A.  & Dhillon  I. S. (2009). Low–Rank Kernel Learning with Bregman Matrix Diver-

gences. The Journal of Machine Learning Research  10  341–376.

Lanckriet  G. R. G.  Cristianini  N.  Bartlett  P.  El Ghaoui  L.  & Jordan  M. I. (2004). Learning the Kernel

Matrix with Semideﬁnite Programming. The Journal of Machine Learning Research  5  27–72.

Li  Z.  & Liu  J. (2009). Constrained Clustering by Spectral Kernel Learning. To appear in the Proceedings of

the 12th IEEE International Conference on Computer Vision.

Li  Z.  Liu  J.  & Tang  X. (2008). Pairwise Constraint Propagation by Semideﬁnite Programming for Semi–
Supervised Classiﬁcation. Proceedings of the 25th International Conference on Machine Learning (pp.
576–583).

Li  Z.  Liu  J.  & Tang  X. (2009). Constrained Clustering via Spectral Regularization. Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition 2009 (pp. 421–428).

Sch¨olkopf  B.  & Smola  A. J. (2002). Learning with Kernels: Support Vector Machines  Regularization 

Optimization  and Beyond. Cambridge  Massachusetts: The MIT Press.

Sch¨olkopf  B.  Smola  A. J.  & M¨uller  K.-R. (1998). Nonlinear Component Analysis as a Kernel Eigenvalue

Problem. Neural Computation  10  1299–1319.

Sha  F.  & Saul  L. K. (2005). Analysis and Extension of Spectral Methods for Nonlinear Dimensionality

Reduction. Proceedings of the 22nd International Conference on Machine Learning (pp. 784–791).

Shawe-Taylor  J.  & Cristianini  N. (2004). Kernel Methods for Pattern Analysis. Cambridge: Cambridge

University Press.

Singer  A. (2008). A Remark on Global Positioning from Local Distances. Proceedings of the National

Academy of Sciences  105  9507–9511.

So  A. M.-C. (2007). A Semideﬁnite Programming Approach to the Graph Realization Problem: Theory 

Applications and Extensions. Doctoral dissertation  Stanford University.

Song  L.  Smola  A.  Borgwardt  K.  & Gretton  A. (2008). Colored Maximum Variance Unfolding. In J. C.
Platt  D. Koller  Y. Singer and S. Roweis (Eds.)  Advances in Neural Information Processing Systems 20:
Proceedings of the 2007 Conference  1385–1392. Cambridge  Massachusetts: The MIT Press.

Toh  K. C.  T¨ut¨unc¨u  R. H.  & Todd  M. J. (2006). On the Implementation and Usage of SDPT3 — A MATLAB

Software Package for Semideﬁnite–Quadratic–Linear Programming  Version 4.0. User’s Guide.

T¨ut¨unc¨u  R. H.  Toh  K. C.  & Todd  M. J. (2003). Solving Semideﬁnite–Quadratic–Linear Programs using

SDPT3. Mathematical Programming  95  189–217.

Vapnik  V. N. (2000). The Nature of Statistical Learning Theory. Statistics for Engineering and Information

Science. New York: Springer–Verlag. Second edition.

Weinberger  K. Q.  Packer  B. D.  & Saul  L. K. (2005). Nonlinear Dimensionality Reduction by Semideﬁnite
Programming and Kernel Matrix Factorization. Proceedings of the 10th International Workshop on Artiﬁcial
Intelligence and Statistics (pp. 381–388).

Weinberger  K. Q.  Sha  F.  & Saul  L. K. (2004). Learning a Kernel Matrix for Nonlinear Dimensionality

Reduction. Proceedings of the 21st International Conference on Machine Learning (pp. 85–92).

Weinberger  K. Q.  Sha  F.  Zhu  Q.  & Saul  L. K. (2007). Graph Laplacian Regularization for Large–Scale
Semideﬁnite Programming. Advances in Neural Information Processing Systems 19: Proceedings of the
2006 Conference (pp. 1489–1496). Cambridge  Massachusetts: The MIT Press.

Zhou  D.  Bousquet  O.  Lal  T. N.  Weston  J.  & Sch¨olkopf  B. (2004). Learning with Local and Global
Consistency. Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference
(pp. 595–602). Cambridge  Massachusetts: The MIT Press.

9

,Sebastian Stober
Jin-Hwa Kim
Jaehyun Jun
Byoung-Tak Zhang