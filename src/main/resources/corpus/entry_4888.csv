2019,Scalable inference of topic evolution via models for latent geometric structures,We develop new models and algorithms for learning the temporal dynamics of the topic polytopes and related geometric objects that arise in topic model based inference. Our model is nonparametric Bayesian and the corresponding inference algorithm is able to discover new topics as the time progresses. By exploiting the connection between the modeling of topic polytope evolution  Beta-Bernoulli process and the Hungarian matching algorithm  our method is shown to be several orders of magnitude faster than existing topic modeling approaches  as demonstrated by experiments working with several million documents in under two dozens of minutes.,Scalable inference of topic evolution via models for

latent geometric structures

Mikhail Yurochkin

IBM Research

mikhail.yurochkin@ibm.com

Zhiwei Fan

University of Wisconsin-Madison

zhiwei@cs.wisc.edu

Aritra Guha

University of Michigan
aritra@umich.edu

Paraschos Koutris

University of Wisconsin-Madison

paris@cs.wisc.edu

XuanLong Nguyen
University of Michigan

xuanlong@umich.edu

Abstract

We develop new models and algorithms for learning the temporal dynamics of
the topic polytopes and related geometric objects that arise in topic model based
inference. Our model is nonparametric Bayesian and the corresponding inference
algorithm is able to discover new topics as the time progresses. By exploiting
the connection between the modeling of topic polytope evolution  Beta-Bernoulli
process and the Hungarian matching algorithm  our method is shown to be several
orders of magnitude faster than existing topic modeling approaches  as demon-
strated by experiments working with several million documents in under two dozens
of minutes.1

1

Introduction

The topic or population polytope is a fundamental geometric object that underlies the presence of latent
topic variables in topic and admixture models [4  19  21]. The geometry of topic models provides the
theoretical basis for posterior contraction analysis of latent topics  in addition to helping to develop
fast and quite accurate inference algorithms in parametric and nonparametric settings [18  28  29  32].
When data and the associated topics are indexed by time dimension  it is of interest to study the
temporal dynamics of such latent geometric structures. In this paper  we will study the modeling
and algorithms for learning temporal dynamics of topic polytope that arises in the analysis of text
corpora.
Several authors have extended the basic topic modeling framework to analyze how topics evolve
over time. The Dynamic Topic Models (DTM) [3] demonstrated the importance of accounting for
non-exchangeability between document groups  particularly when time index is provided. Another
approach is to keep topics ﬁxed and consider only evolving topic popularity [26]. Hong et al. [13]
extended such an approach to multiple corpora. Ahmed and Xing [1] proposed a nonparametric
construction extending DTM where topics can appear or eventually die out. Although the evolution
of the latent geometric structure (i.e.  the topic polytope) is implicitly present in these works  it was
not explicitly addressed nor is the geometry exploited. A related limitation shared by these modeling

1Code: https://github.com/moonfolk/SDDM

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

frameworks is the lack of scalability  due to inefﬁcient joint modeling and learning of topics at each
time point and topic evolution over time. To improve scalability  a natural solution is decoupling the
two phases of inference.
To this end  we seek to develop a series of topic meta-models  i.e. models for temporal dynamics of
topic polytopes  assuming that the topic estimates from each time point have already been obtained via
some efﬁcient static topic inference technique. The focus on inference of topic evolution offers novel
opportunities and challenges. To start  what is the suitable ambient space in which the topic polytope
is represented? As topics evolve  so are the number of topics that may become active or dormant 
raising distinct modeling choices. Interesting issues arise in the inference  too. For instance  what is
the principled way of matching vertices of a collection of polytopes to their next reincarnations? Such
question arises because we consider modeling of topics learned independently across timestamps and
text corpora  which entails the need for preserving the topic structure’s permutation invariance of the
vertex labels.
We consider an isometric embedding of the unit sphere in the word simplex  so that the evolution
of topic polytopes may be represented by a collection of (random) trajectories of points residing on
the unit sphere. Instead of attempting to mix-match vertices in an ad hoc fashion  we appeal to a
Bayesian nonparametric modeling framework that allows the number of topic vertices to be random
and vary across time. The mix-matching between topics shall be guided by the assumption on the
smoothness of the collection of global trajectories on the sphere using von Mises-Fisher dynamics
[15]. The selection of active topics at each time point will be enabled by a nonparametric prior on the
random binary matrices via the (hierarchical) Beta-Bernoulli process [24].
Our contribution includes a sequence of Bayesian nonparametric models in increasing levels of
complexity: the simpler model describes a topic polytope evolving over time  while the full model
describes the temporal dynamics of a collection of topic polytopes as they arise from multiple corpora.
The semantics of topics can be summarized as follows: there is a collection of latent global topics of
unknown cardinality evolving over time (e.g. topics in science or social topics in Twitter). Each year
(or day) a subset of the global topics is elucidated by the community (some topics may be dormant
at a given time point). The nature of each global topic may change smoothly (via varying word
frequencies). Additionally  different subsets of global topics are associated with different groups (e.g.
journals or Twitter location stamps)  some becoming active/inactive over time.
Another key contribution includes a suite of scalable approximate inference algorithms suitable
for online and distributed settings. In particular  we focus mainly on MAP updates rather than a
full Bayesian integration. This is appropriate in an online learning setting  moreover such updates
of the latent topic polytope can be viewed as solving an optimal matching problem for which
a fast Hungarian matching algorithm can be applied. Our approach is able to perform dynamic
nonparametric topic inference on 3 million documents in 20 minutes  which is signiﬁcantly faster
than prior static online and/or distributed topic modeling algorithms [16  12  25  6  5].
The remainder of the paper is organized as follows. In Section 2 we deﬁne a Markov process over
the space of topic polytopes (simplices). In Section 3 we present a series of models for polytope
dynamics and describe our algorithms for online dynamic and/or distributed inference. Section 4
demonstrates experimental results. We conclude with a discussion in Section 5.

2 Temporal dynamics of a topic polytope

The fundamental object of inference in this work is the topic polytope arising in topic modeling
which we shall now deﬁne [4  18]. Given a vocabulary of V words  a topic is deﬁned as a probability
distribution on the vocabulary. Thus a topic is taken to be a point in the vocabulary simplex  namely 
∆V −1  and a topic polytope for a corpus of documents is deﬁned as a convex hull of topics associated
with the documents. Geometrically  the topics correspond to the vertices (extreme points) of the
(latent) topic polytope to be inferred from data.
In order to infer about the temporal dynamics of a topic polytope  one might consider the evo-
lution of each topic variable  say θ(t)  which represents a vertex of the polytope at time t. A
standard approach is due to Blei and Lafferty [3]  who proposed to use a Gaussian Markov chain
θ(t)|θ(t−1) ∼ N (θ(t−1)  σI) in RV for modeling temporal dynamics and a logistic normal transfor-

2

Figure 1: Invertible transformation between unit sphere and a standard simplex; dynamics example

(cid:80)

i

i

)

  which sends elements of RV into ∆V −1. In our meta-modeling
mation π(θ(t))i := exp(θ(t)
)
i exp(θ(t)
approach  we consider topics  i.e. points in ∆V −1  learned independently across time and corpora.
Logistic normal map is many-to-one  hence it is undesirably ambiguous in mapping a collection of
topic polytopes to RV .
We propose to represent each topic variable as a point in a unit sphere SV −2  which possesses a
natural isometric embedding (i.e. one-to-one) in the vocabulary simplex ∆V −1  so that the temporal
dynamics of a topic variable can be identiﬁed as a (random) trajectory on SV −2. This trajectory shall
be modeled as a Markovian process on SV −2: θ(t)|θ(t−1) ∼ vMF(θ(t−1)  τ0). Von Mises-Fisher
(vMF) distribution is commonly used in the ﬁeld of directional statistics [15] to model points on a
unit sphere and was previously utilized for text modeling [2  20].

Isometric embedding of SV −2 into the vocabulary simplex We start with the directional repre-
sentation of topic polytope [29]: let B = {β1  . . .   βK} be a collection of vertices of a topic polytope.
˜βk  where C ∈ Conv(B) is a reference point in a convex
Each vertex is represented as βk := C + Rk
hull of B  ˜βk ∈ RV is a topic direction and Rk ∈ R+. Moreover  Rk ∈ [0  1] is determined so that
the tip of direction vector ˜βk resides on the boundary of ∆V −1. Since the effective dimensionality of
˜βk is V − 2  we can now deﬁne an one-to-one and isometric map sending ˜βk onto SV −2 as follows:
map of the vocabulary simplex ∆V −1 ∈ RV where it is ﬁrst translated so that C becomes the origin
and then rotated into RV −1  where resulting topics  say θ1  . . .   θK ∈ SV −2  are normalized to the
unit length. Observe that this geometric map is an isometry and hence invertible. It preserves angles
between vectors  therefore we can evaluate vMF density without performing the map explicitly  by
simply setting θk := βk−C
Lemma 1. Γ : {β = (β1  . . .   βV ) ∈ ∆V −1 : βi = 0 for some i} → {θ ∈ SV −1 : 1T
a homeomorphism  where Γ(β) = (β − C) /(cid:107)β − C(cid:107)2  and Γ−1(θ) = −
C = (c1  . . .   cV ) ∈ ∆V −1.

(cid:107)βk−C(cid:107). The following lemma formalizes this idea.

V θ = 0} is
+ C  for any

maxi θi/ci

θ

Proofs of this Lemma and subsequent technical results are given in the Supplement. The intuition
behind the construction is provided via Figure 1 which gives a geometric illustration for V = 3 
vocabulary simplex ∆V −1 shown as red triangle. Two topics on the boundary (face) of the vocabulary
simplex are β1 = C + ˜β1 and β2 = C + ˜β2. Green dot C is the reference point and α = ∠( ˜β1  ˜β2).
In Fig. 1 (left) we move C by translation to the origin and rotate ∆V −1 from xyz to xy plane.
In Fig. 1 (center left) we show the resulting image of ∆V −1 and add a unit sphere (blue) in R2.
Corresponding to β1  β2 topics are the points θ1  θ2 on the sphere with ∠(θ1  θ2) = α. Now  apply
the inverse translation and rotation to both ∆V −1 and SV −2  the result is shown in Fig. 1 (center
right) — we are back to R3 and ∠(θ1  θ2) = ∠( ˜β1  ˜β2) = α  where θk = βk−C
. In Fig. 1 (right)
(cid:107)βk−C(cid:107)2
we give a geometric illustration of the temporal dynamics.
As described above  each topic evolves in a random trajectory residing in a unit sphere  so the
evolution of a collection of topics can be modeled by a collection of corresponding trajectories on the
sphere. Note that the number of "active" topics may be unknown and vary over time. Moreover  a
topic may be activated  become dormant  and then resurface after some time. New modeling elements
are introduced in the next section to account for these phenomena.

3

�̃ 1�̃ 2������̃ 1��1���̃ 2�2��̃ 1�̃ 2������2�1�����(�)2�(�+1)2�(�+2)2�(�)1�(�+1)1�(�+2)13 Hierarchical Bayesian modeling for single or multiple topic polytopes

We shall present a sequence of models with increasing levels of complexity: we start by introducing a
hierarchical model for online learning of the temporal dynamics of a single topic polytope  allowing
for varying number of vertices over time. Next  a static model for multiple topic polytopes learned on
different corpora drawing on a common pool of global topics. Finally  we present a "full" model for
modeling evolution of global topic trajectories over time and across groups of corpora.

3.1 Dynamic model for single topic polytope

At a high level  our model maintains a collection of global trajectories taking values on a unit sphere.
Each trajectory shall be endowed with a von Mises-Fisher dynamic described in the previous section.
At each time point  a random topic polytope is constructed by selecting a (random) subset of points
on the trajectory evaluated at time t. The random selection is guided by a Beta-Bernoulli process
prior [24]. This construction is motivated by a modeling technique of Nguyen [17]  who studied
a Bayesian hierarchical model for inference of smooth trajectories on an Euclidean domain using
Dirichlet process priors. Our generative model  using Beta-Bernoulli process as a building block 
is more appropriate for the purpose of topic discovery. Due to the isometric embedding of SV −2 in
∆V −1 described in the previous section  from here on we shall refer to topics as points on SV −2.
First  generate a collection of global topic trajectories using Beta Process prior (cf. Thibaux and
Jordan [24]) 2 with a base measure H on the space of trajectories on SV −2 and mass parameter γ0:
(1)
i=1 follows a stick-breaking construction [23]: µi ∼
j=1 µj  and each θi ∼ H is a sequence of T random elements on the unit sphere

It follows that Q = (cid:80)
Beta(γ0  1)  qi =(cid:81)i

Q|γ0  H ∼ BP(γ0  H).

i qiδθi  where {qi}∞

θi := {θ(t)

i }T

t=1  which are generated as follows:

i

∼ vMF(θ(t−1)

|θ(t−1)
∼ vMF(·  0) – uniform on SV −2.

i

  τ0) for t = 1  . . .   T 

θ(t)
i
θ(0)
i

(2)

tive at t via the Bernoulli process T (t)|Qt ∼ BeP(Qt). Then T (t) :=(cid:80)

At any given time t = 1  . . .   T   the process Q induces a marginal measure Qt  whose support is given
by the atoms of Q as they are evaluated at time t. Now  select a subset of the global topics that are ac-
|qi ∼
Bern(qi)  ∀i. T (t) are supported by atoms {θ(t)
i = 1  i = 1  2  . . .} representing topics active
at time t. Finally  assume that noisy measurements of each of these topic variables are generated via:

  where b(t)
i

i=1b(t)

i δθ(t)

: b(t)

i

i

k |T (t) ∼ vMF(T (t)
v(t)
K (t) := card(T (t));T (t)

k

k   τ1)  k = 1  . . .   K (t)  where

is k-th atom of T (t).

(3)

Noisy estimates for the topics at any particular time point may come from either the global topics
observed until the previous time point or a topic yet unexplored. We emphasize that topics {v(t)
k }K(t)
k=1
for t = 1  . . .   T are the quantities we aim to model  hence we refer to our approach as the meta-model.
These topics may be learned  for each time point independently  by any stationary topic modeling
algorithms  and then transformed to sphere by applying Lemma 1.
Let B(t) denote the binary matrix representing the assignment of observed topic estimates to global
topics at time point t  i.e  B(t)
. In words  these
random variables “link up” the noisy estimates at any time point to the global topics observed thus
far. By conditional independence  the joint posterior of the hidden θ(t) given observed noisy v(t) is:

ik = 1 if the vector v(t)

is a noisy estimate for θ(t)

k

i

θ(0) {θ(t)  B(t)}T

t=1|{v(t)}T

t=1

P(θ(t)  B(t)|θ(t−1) {B(a)}t−1

a=1)P(v(t)|θ(t)  B(t)).

2Thibaux and Jordan [24] write BP(c  H)  H(Ω) = γ0; we set c = 1  H = H/γ0 and write BP(γ0  H).

t=1

4

P(cid:16)

(cid:17) ∝ P(θ(0))

T(cid:89)

At t  P(θ(t)  B(t)|θ(t−1) {B(a)}t−1

a=1)P(v(t)|θ(t)  B(t)) ∝ P(θ(t)  B(t)|θ(t−1)  v(t) {B(a)}t−1

a=1) ∝

(cid:32)(cid:16)
Lt−1(cid:89)
· exp(− γ0

i=1

k=1 B(t)

ik

(cid:17)(cid:80)K(t)
(cid:80)Lt

exp(τ1

i=1

)

i

i

/(t − m(t−1)
m(t−1)
t )(γ0/t)Lt−Lt−1
(Lt − Lt−1)!

(cid:33)

i

exp(τ0(cid:104)θ(t−1)
(cid:80)K(t)

k=1 B(t)

ik (cid:104)θ(t)

i

i (cid:105))
  θ(t)

k (cid:105)).
  v(t)

(4)

The equation above represents a product of four quantities: (1) probability of B(t)s  where m(t)
i
denotes the number of occurrences of topic i up to time point t (cf. popularity of a dish in the
Indian Buffet Process (IBP) metaphor [9])  (2) vMF conditional of θ(t)
(cf. Eq. (2)) 
(3) number of new global topics at time t  Lt − Lt−1 ∼ Pois(γ0/t)  and (4) emission probability
P(v(t)|θ(t)  B(t)) (cf. Eq. (3)). Derivation details are given in the Supplement.

i given θ(t−1)

i

Streaming Dynamic Matching (SDM) To perform MAP estimation in the streaming setting  we
highlight the connection of the maximization of the posterior (4) to the objective of an optimal
matching problem: given a cost matrix  workers should be assigned to tasks  at most one worker per
task and one task per worker. The solution of this problem is obtained by employing the well-known
Hungarian algorithm [14]. In the context of dynamic topic modeling  our goal is to match topics
learned on the new timestamp to the trajectories of topics learned over the previous timestamps 
where the cost is governed by our model. This connection is formalized by the following.

(cid:107)τ1v(t)
k + τ0θ(t−1)
(cid:80)

τ1 + log γ0
i k B(t)

i

(cid:107)2 − τ0 + log m(t−1)
t−m(t−1)

i

  i ≤ Lt−1

t − log(i − Lt−1)  Lt−1 < i ≤ Lt−1 + K (t)
ik C (t)

i

ik subject to the constraints that (a) for each
consider the optimization problem maxB(t)
ﬁxed i  at most one of B(t)
ik is 1
and the rest are 0. Then  the MAP estimate for Eq. (4) can be obtained by the Hungarian algorithm 
which solves for ((B(t)

ik is 1 and the rest are 0  and (b) for each ﬁxed k  exactly one of B(t)

as

Proposition 1. Given the cost C (t)

ik =

i

ik )) to obtain θ(t)
k +τ0θ(t−1)
k +τ0θ(t−1)
(cid:107)2

 

i

i



τ1v(t)
(cid:107)τ1v(t)
v(t)
k  
θ(t−1)

i

if ∃ k s.t. B(t)
if ∃ k s.t. B(t)
otherwise (topic is dormant at t).

ik = 1 and i ≤ Lt−1
ik = 1 and i > Lt−1 (new topic)

(5)

m ∈ NV }Mt

We defer proof to the Supplement. To complete description of the inference we shall discuss how
noisy estimates are obtained from the bag-of-words representation of the documents observed at
time point t. We choose to use CoSAC [29] algorithm to obtain topics {β(t)
k=1 from
{x(t)
m=1  collection of Mt documents at time point t. CoSAC is a stationary topic modeling
algorithm which can infer number of topics from the data and is computationally efﬁcient for
moderately sized corpora. We note that other topic modeling algorithms  e.g.  variational inference
[4] or Gibbs sampling [11  22]  can be used in place of CoSAC. Estimated topics are then transformed
to {v(t)
a=1 Ma 
where N (a)
m is the number of words in the corresponding document. Our reference point is simply an
average (computed dynamically) of the normalized documents observed thus far. Finally we update
MAP estimates of global topics dynamics based on Proposition 1. Streaming Dynamic Matching
(SDM) is summarized in Algorithm 1.

k=1 using Lemma 1 and reference point Ct = (cid:80)t

k ∈ ∆V −1}K(t)

k ∈ SV −2}K(t)

/(cid:80)t

(cid:80)Ma

x(a)
m
N (a)
m

m=1

a=1

Additional related literature
utilizing similar technical building blocks in different contexts. Fox
et al. [8] utilized Beta-Bernoulli process in time series modeling to capture switching regimes of an
autoregressive process  where the corresponding Indian Buffet Process was used to select subsets of
the latent states of the Hidden Markov Model. Williamson et al. [27] used Indian Buffet Process in
topic models to sparsify document topic proportions. Campbell et al. [7] utilized Hungarian algorithm
for streaming mean-ﬁeld variational inference of the Dirichlet Process mixture model.

5

Algorithm 1 Streaming Dynamic Matching (SDM)
1: for t = 1  . . .   T do
Observe documents {x(t)
2:
Estimate topics {β(t)
k }K(t)
3:
4: Map topics to sphere {v(t)
5:
6:
7:

m }Mt
k=1 = CoSAC({x(t)
k }K(t)
k=1 (Lemma 1)
i=1 and {v(t)
k }K(t)

i=1 as in eq. (5)

m=1

Given {θ(t−1)}Lt−1
Using Hungarian algorithm solve the corresponding matching problem to obtain B(t)
Compute {θ(t)}Lt

k=1 compute cost matrix as in Proposition 1

m }Mt

m=1)

3.2 Beta-Bernoulli Process for multiple topic polytopes

Tj|Q ∼ BeP(Q)  then Tj :=(cid:80)

We now consider meta-modeling in the presence of multiple corpora  each of which maintains its
own topic polytope. Large text corpora often can be partitioned based on some grouping criteria  e.g.
scientiﬁc papers by journals  news by different media agencies or tweets by location stamps. In this
subsection we model the collection of topic polytopes observed at a single time point by employing
the Beta-Bernoulli Process prior [24]. The modeling of a collection of polytopes evolving over time
will be described in the following subsection.
First  generate global topic measure Q as in Eq. (1). Here  we are interested only in a single time
point  the base measure H is simply a vMF(·  0)  the uniform distribution over SV −2. Next  for each
group j = 1  . . .   J  select a subset of the global topics:

i=1bjiδθi  where bji|qi ∼ Bern(qi)  ∀i.

(6)
Notice that each group Tj := {θi : bji = 1  i = 1  2  . . .} selects only a subset from the collection of
global topics  which is consistent with the idea of partitioning by journals: some topics of ICML are
not represented in SIGGRAPH and vice versa. The next step is analogous to Eq. (3):
vjk|Tj ∼ vMF(Tjk  τ1) for k = 1  . . .   Kj  where Kj := card(Tj).

(7)
We again use B to denote the binary matrix representing the assignment of global topics to the noisy
topic estimates  i.e.  Bjik = 1 if the kth topic estimate for group j arises as a noisy estimate of global
topic θi. However  the matching problem is now different from before: we don’t have any information
about the global topics as there is no history  instead we should match a collection of topic polytopes
to a global topic polytope. The matrix of topic assignments is distributed a priori by an Indian Buffet
Process (IBP) with parameter γ0. The conditional probability for global topics θi and assignment
matrix B given topic estimates vjk has the following form:

(cid:80)
j i kBjik(cid:104)θi  vjk(cid:105))IBP({mi})  where mi =(cid:80)

P(B  θ|v) ∝ exp(τ1

j kBjik

(8)

and IBP is the prior (see Eq. (15) in [10]) with mi denoting the popularity of global topic i.

(cid:40)

Distributed Matching (DM) Similar to Section 3.1  we look for point estimates for the topic
directions θ and for the topic assignment matrix B. Direct computation of the global MAP estimate
for Eq. (8) is not straight-forward. The problem of matching across groups and topics is not amenable
to a closed form Hungarian algorithm. However we show that for a ﬁxed group the assignment
optimization reduces to a case of the Hungarian algorithm. This motivates the use of Hungarian
algorithm iteratively  which guarantees convergence to a local optimum.
Proposition 2. Given the cost

τ1(cid:107)vjk +(cid:80)−j i k B−jikv−jk(cid:107)2 − τ1(cid:107)(cid:80)−j i k B−jikv−jk(cid:107)2 + log m−ji

J − log(i − L−j)  if L−j < i ≤ L−j + Kj 

for each group j  (((Bjik))) which maximizes(cid:80)

Cjik =
where −j denotes groups excluding group j and L−j is the number of global topics before group j
(due to exchangeability of the IBP  group j can always be considered last). Then  a locally optimum
MAP estimate for Eq. (8) can be obtained by iteratively employing the Hungarian algorithm to solve:
j i k BjikCjik  subject to constraints: (a) for each
ﬁxed i and j  at most one of Bjik is 1  rest are 0 and (b) for each ﬁxed k and j  exactly one of Bjik is
1  rest are 0. After solving for (((Bjik)))  θi is obtained as θi =

(cid:80)
(cid:107)(cid:80)
j k Bjikvjk
j k Bjikvjk(cid:107)2

  if i ≤ L−j

τ1 + log γ0

J−m−ji

.

6

The noisy topics for each of the groups can be obtained by applying CoSAC to corresponding
documents  which is trivially parallel. Distributed Matching algorithm and proof of the Proposition 2
are given in the Supplement.

3.3 Dynamic Hierarchical Beta Process

Our “full” model  the Dynamic Hierarchical Beta Process model (dHBP)  builds on the constructions
described in subsections 3.1 and 3.2 to enable the inference of temporal dynamics of collections
of topic polytopes. We start by specifying the upper level Beta Process given by Eq. (1) and base
measure H given by Eq. (2). Next  for each group j = 1  . . .   J  we introduce an additional level of
hierarchy to model group speciﬁc distributions over topics
Qj|Q ∼ BP(γj  Q)  then Qj :=

(cid:88)

(9)

pjiδθi 

j

where pjis vary around corresponding qi. The distributional properties of pji are described in [24].
At any given time t  each group j selects a subset from the common pool of global topics:
ji |pji ∼ Bern(pji)  ∀i.

T (t)
Let T (t)
time t in group j. Noisy measurements of these topics are generated by:
  where K (t)

(10)
ji = 1  i = 1  2  . . .} be the corresponding collection of atoms – topics active at
∼ vMF(T (t)

|Qjt ∼ BeP(Qjt)  then T (t)
:= {θ(t)

:=(cid:80)

:= card(T (t)

jk   τ1) for k = 1  . . .   K (t)

The conditional distribution of global topics at t given the state of the global topics at t − 1 is

  where b(t)

i=1b(t)

ji δθ(t)

: b(t)

(11)

).

j

j

j

j

j

i

i

j

jk |T (t)
v(t)
(cid:80)

(cid:16)

P(θ(t)  B(t)|θ(t−1)  v(t) {B(a)}t−1

(cid:17)

a=1) ∝
F ({m(t−1)

i

(cid:16)(cid:80)

jk (cid:105)(cid:17)

i

i

ji

ji

τ0

(cid:105))

exp

} {m(t)

j i kτ1B(t)

ji }) · exp

i=1(cid:104)θ(t)
} {m(t)

  θ(t−1)
where F ({m(t−1)
ji }) is the prior term dependent on the popularity counts history from current
and previous time points. Analogous to the Chinese Restaurant Franchise [22]  one can think of an
Indian Buffet Franchise in the case of HBP. A headquarter buffet provides some dishes each day
and the local branches serve a subset of those dishes. Although this analogy seems intuitive  we
are not aware of a corresponding Gibbs sampler and it remains to be a question of future studies.
Therefore  unfortunately  we are unable to handle this prior term directly and instead propose a
heuristic replacement — stripping away popularity of topics across groups and only considering
group speciﬁc topic popularity (groups still remain dependent through the atom locations).

jik(cid:104)θ(t)

  v(t)

 

i

(12)

(cid:107)τ1v(t)

Streaming Dynamic Distributed Matching (SDDM) We combine our results to perform approx-
imate inference of the model in Section 3.3. Using Hungarian algorithm  iterating over groups at time
t obtain estimates for (((B(t)

jik))) based on the following cost C (t)

jik =

(cid:80)−j i k B(t)−jikv(t)−jk + τ0θ(t−1)

i

(cid:107)2 − (cid:107)(cid:80)−j i k B(t)−jikv(t)−jk + τ0θ(t−1)

i

J − log(i − L(t)−j)  if L(t)−j < i ≤ L(t)−j + K (t)

jk + τ1
τ1 + log γ0
where ﬁrst case is if i ≤ L(t)−j; m(t)
ji denotes the popularity of topic i in group j up to time t (plus
one is used to indicate that global topic i exists even when m(t)
ji = 0). Then compute global topic
estimates θ(t)
. At time point t  the noisy topics for each of the groups
can be obtained by applying CoSAC to corresponding documents in parallel. SDDM algorithm and
cost derivations are presented in the Supplement.

jk +τ0θ(t−1)
jk +τ0θ(t−1)
(cid:107)2

j k B(t)
j k B(t)

jikv(t)
jikv(t)

(cid:80)
(cid:80)

τ1
(cid:107)τ1

i =

 

j

i

i

(cid:107)2 + log

1+m(t)
ji
t−m(t)

ji

 

4 Experiments

We study ability of our models to learn the latent temporal dynamics and discover new topics that
change over time. Next we show that our models scale well by utilizing temporal and group inherent
data structures. We also study hyperparameters choices. We analyze two datasets: the Early Journal
Content (http://www.jstor.org/dfr/about/sample-datasets)  and a collection of
Wikipedia articles partitioned by categories and in time according to their popularity.

7

(a) SDM Epidemics: evolution of top 15 words

(b) DM Law: matched topics from journals

Figure 2: Qualitative examples of topics learned by SDM and DM algorithms on the EJC data

4.1 Temporal Dynamics and Topic Discovery

Early Journal Content. The Early Journal Content dataset spans years from 1665 up to 1922.
Years before 1882 contain very few articles  and we aggregated them into a single timepoint. After
preprocessing  dataset has 400k scientiﬁc articles from over 400 unique journals. The vocabulary was
truncated to 4516 words. We set all articles from the last available year (1922) aside for the testing
purposes.

Case study: epidemics. The beginning of the 20th century is known to have a vast history of
disease epidemics of various kinds  such as smallpox  typhoid  yellow fever to name a few. Vaccines
or effective treatments for the majority of them were developed shortly after. One of the journals
represented in the EJC dataset is the "Public Health Report"; however  publications from it are only
available starting 1896. Primary objective of the journal was to reﬂect epidemic disease infections.
As one of the goals of our modeling approach is topic discovery  we verify that the model can discover
an epidemics-related topic around 1896. Figure 2a shows that SDM correctly discovered a new topic
is 1896 semantically related to epidemics. We plot the evolution of probabilities of the top 15 words
in this topic across time. We observe that word "typhoid" increases in probability towards 1910 in the
"epidemics" topic  which aligns with historical events such as Typhoid Mary in 1907 and chlorination
of public drinking water in the US in 1908 for controlling the typhoid fever. The probability of
"tuberculosis" also increases  aligning with foundation of the National Association for the Study and
Prevention of Tuberculosis in 1904.

Case study: law. Some of the EJC journals are related to the topic of law. Our DM algorithm
identiﬁed a global topic semantically similar to law by matching similar topics present in 32 out of the
417 journals. In Figure 2b we present the learned global topic and 4 examples of the matched local
topics with the corresponding journal names. Our algorithm correctly identiﬁed that these journals
have a shared law topic.

4.2 Scalability

Wiki Corpus. We collected articles from
Wikipedia and their page view counts for the 12
months of 2017 and category information (e.g. 
Arts  History). We used categories as groups
Figure 3: Comparison on Wiki Data (20 cores)
and partitioned the data across time according
to the page view counts. Dataset construction details are given in the Supplement. The total number
of documents is about 3 million  and we reduced vocabulary to 7359 words similarly to [12]. For
testing we set aside documents from category Art from December 2017.

Modeling Grouping.
In Fig. 3 we present comparisons on Wiki data: CoSAC [29] v.s DM under
the static distributed setting and SDM v.s SDDM under the dynamic streaming setting. Fig. 3 (left)
shows that for data accessible in groups  DM outperforms CoSAC by ∼ 25X  as DM runs CoSAC
on different data groups in parallel and then matches the outputs. Matching time adds only a small

8

18851890189519001905191019151920year0.000.020.040.060.080.100.12word probabilityTopic firstappearancefevertotalnumbertuberculosipopuldiphtheriamonthscarletentermeaslendtyphoidratesmallpoxweekTopic 9 in The Virginia Law Register court case law state ani supremcircuit jurisdictopinion said judgorder right appeal time unit judgment action countiquestionTopic 54 in The North American Reviewcourt law ani judgcase justictime trial onlistate juribefornew unit supremgenerpublic constitutpower maniGlobal Topic 61 court law case state ani unit time act new onlisupremjusticgenerpar question power beforright jurisdictshallTopic 16 inThe Yale Law Journalcourt case law state jurisdictsupremunit decisani act question defend rule constitutpower new right journal beforactionTopic 18 in Columbia Law Reviewcourt case law state jurisdictrule decisani right new question act statutreview onliunit federconstitutgenerpowerCoSACDMModels02500500075001000012500150001750020000Time(s)CoSACMatchingSDMSDDMModels020004000600080001000012000Time(s)CoSACInterleaveMatchingTable 1: Modeling topics of EJC

|| Modeling Wikipedia articles

Perplexity
1179
1361
1241
1194
1840
1191

Time Topics Cores
1
22min
20
5min
2.3min
20
56hours
1
20
3hours
51min
1

SDM
DM
SDDM
DTM
SVB
CoSAC
overhead compared to the runtime of CoSAC. Similarly  in Fig. 3 (right)  SDDM is ∼ 6X faster than
SDM  since SDDM can process documents of different groups in parallel and interleaves CoSAC
with matching: while matching is being performed on data groups with timestamp t  CoSAC can
process the data that arrives with timestamp t + 1 in parallel.

125
125
103
100
100
132

Perplexity
2.4hours
1254
15min
1260
1201
20min
NA >72hours
29.5hours
1219
1227
4.4hours

Time Topics Cores
1
20
20
1
20
1

182
182
238
100
100
173

Modeling temporality also beneﬁts scalability. We compare our methods with other topic models
on both Wiki and EJC datasets: Streaming Variational Bayes (SVB) [5] and Dynamic Topic Models
(DTM) [3] trained with 100 topics. Perplexity scores on the held out data  training times  computing
resources and number of topics are reported in Table 1. On the wiki dataset  SDDM took only 20min
to process approximately 3 million documents  which is much faster than the other approaches.
Regarding perplexity scores  SDDM generally outperforms DM  which suggests that modeling time
is beneﬁcial. For the EJC dataset  SDM outperforms SDDM. Modeling groups might negatively
affect perplexity because the majority of the EJC journals (groups) have very few articles (i.e. less
than 100 – a setup challenging for many topic modeling algorithms). On the Wiki corpus each
category (group) has sufﬁcient amount of training documents and time-group partitioning considered
by SDDM achieves the best perplexity score.

4.3 Parameter choices

The rate of topic dynamics of the SDM and SDDM is effectively con-
trolled by τ0  where smaller values imply higher dynamics rate. Parameter
τ1 controls variance of local topics around corresponding global topics
in all of our models. This variance dictates how likely a local topic to
be matched to an existing global topic. When this variance is small 
the model will tend to identify local topics as new global topics more
often. Lastly  γ0 affects the probability of new topic discovery  which
scales with time and number of groups. In the preceding experiments
we set τ0 = 2  τ1 = 1  γ0 = 1 for SDM; τ1 = 2  γ0 = 1 for DM;
τ0 = 4  τ1 = 2  γ0 = 2 for SDDM. In Figure 4 we show heatpmaps for
perplexity and number of learned topics  ﬁxing γ0 = 1 and varying τ0
and τ1. We see that for large τ1  SDM identiﬁes more topics to ﬁt the
smaller variability constraint imposed by the parameter.

(a) EJC perplexity

5 Discussion and Conclusion

(b) # of topics in EJC

Fig. 4: SDM parameters

Our work suggests the naturalness of incorporating sophisticated Bayesian nonparametric techniques
in the inference of rich latent geometric structures of interest. We demonstrated the feasibility of
approximate nonparametric learning at scale  by utilizing suitable geometric representations and
devising fast algorithms for obtaining reasonable point estimates for such representations. Further
directions include incorporating more meaningful geometric features into the models (e.g.  via more
elaborated base measure modeling for the Beta Process) and developing efﬁcient algorithms for full
Bayesian inference. For instance  the latent geometric structure of the problem is solely encoded in
the base measure. We want to explore choices of base measures for other geometric structures such
as collections of k-means centroids  principal components  etc. Once an appropriate base measure
is constructed  our Beta process based models can be utilized to enable a new class of Bayesian
nonparametric models amenable to scalable inference and suitable for analysis of large datasets. In
our concurrent work we have utilized model construction similar to one from Section 3.2 to perform
Federated Learning of neural networks trained on heterogeneous data [31] and proposed a general
framework for model fusion [30].

9

124816321(=1)124816320117511771177117811781181117911761174117711731176119011781173117111691167121011881176116111561154124012111181116011551151126312411195116611571152116011801200122012401260124816321(=1)124816320125125127128127127125129133136137137128134146169192202126135172229259264128133188259270274125134194264274278150180210240270Acknowledgments

This research is supported in part by grants NSF CAREER DMS-1351362  NSF CNS-1409303  a
research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award to XN.

References
[1] Ahmed  A. and Xing  E. P. (2012). Timeline: A dynamic hierarchical Dirichlet process model
for recovering birth/death and evolution of topics in text stream. arXiv preprint arXiv:1203.3463.
[2] Banerjee  A.  Dhillon  I. S.  Ghosh  J.  and Sra  S. (2005). Clustering on the unit hypersphere

using von mises-ﬁsher distributions. Journal of Machine Learning Research  6  1345–1382.

[3] Blei  D. M. and Lafferty  J. D. (2006). Dynamic topic models. In Proceedings of the 23rd

International Conference on Machine Learning  pages 113–120.

[4] Blei  D. M.  Ng  A. Y.  and Jordan  M. I. (2003). Latent Dirichlet Allocation. Journal of Machine

Learning Research  3  993–1022.

[5] Broderick  T.  Boyd  N.  Wibisono  A.  Wilson  A. C.  and Jordan  M. I. (2013). Streaming

variational Bayes. In Advances in Neural Information Processing Systems  pages 1727–1735.

[6] Bryant  M. and Sudderth  E. B. (2012). Truly nonparametric online variational inference for
hierarchical Dirichlet processes. In Advances in Neural Information Processing Systems  pages
2699–2707.

[7] Campbell  T.  Straub  J.  Fisher III  J. W.  and How  J. P. (2015). Streaming  distributed variational
inference for bayesian nonparametrics. In Advances in Neural Information Processing Systems 
pages 280–288.

[8] Fox  E.  Jordan  M. I.  Sudderth  E. B.  and Willsky  A. S. (2009). Sharing features among
dynamical systems with Beta processes. In Advances in Neural Information Processing Systems 
pages 549–557.

[9] Ghahramani  Z. and Grifﬁths  T. L. (2005). Inﬁnite latent feature models and the Indian buffet

process. In Advances in Neural Information Processing Systems  pages 475–482.

[10] Grifﬁths  T. L. and Ghahramani  Z. (2011). The Indian buffet process: An introduction and

review. Journal of Machine Learning Research  12  1185–1224.

[11] Grifﬁths  T. L. and Steyvers  M. (2004). Finding scientiﬁc topics. PNAS  101(suppl. 1) 

5228–5235.

[12] Hoffman  M.  Bach  F. R.  and Blei  D. M. (2010). Online learning for Latent Dirichlet

Allocation. In Advances in Neural Information Processing Systems  pages 856–864.

[13] Hong  L.  Dom  B.  Gurumurthy  S.  and Tsioutsiouliklis  K. (2011). A time-dependent topic
model for multiple text streams. In Proceedings of the 17th ACM SIGKDD international conference
on Knowledge discovery and data mining  pages 832–840. ACM.

[14] Kuhn  H. W. (1955). The Hungarian method for the assignment problem. Naval Research

Logistics (NRL)  2(1-2)  83–97.

[15] Mardia  K. V. and Jupp  P. E. (2009). Directional statistics  volume 494. John Wiley & Sons.
[16] Newman  D.  Smyth  P.  Welling  M.  and Asuncion  A. U. (2008). Distributed inference
for Latent Dirichlet Allocation. In Advances in Neural Information Processing Systems  pages
1081–1088.

[17] Nguyen  X. (2010). Inference of global clusters from locally distributed data. Bayesian Analysis 

5(4)  817–845.

[18] Nguyen  X. (2015). Posterior contraction of the population polytope in ﬁnite admixture models.

Bernoulli  21(1)  618–646.

[19] Pritchard  J. K.  Stephens  M.  and Donnelly  P. (2000). Inference of population structure using

multilocus genotype data. Genetics  155(2)  945–959.

[20] Reisinger  J.  Waters  A.  Silverthorn  B.  and Mooney  R. J. (2010). Spherical topic models. In

Proceedings of the 27th International Conference on Machine Learning  pages 903–910.

10

[21] Tang  J.  Meng  Z.  Nguyen  X.  Mei  Q.  and Zhang  M. (2014). Understanding the limiting
factors of topic modeling via posterior contraction analysis. In Proceedings of the 31st International
Conference on Machine Learning  pages 190–198.

[22] Teh  Y. W.  Jordan  M. I.  Beal  M. J.  and Blei  D. M. (2006). Hierarchical Dirichlet processes.

Journal of the American Statistical Association  101(476).

[23] Teh  Y. W.  Grür  D.  and Ghahramani  Z. (2007). Stick-breaking construction for the Indian

buffet process. In Artiﬁcial Intelligence and Statistics  pages 556–563.

[24] Thibaux  R. and Jordan  M. I. (2007). Hierarchical Beta processes and the Indian buffet process.

In Artiﬁcial Intelligence and Statistics  pages 564–571.

[25] Wang  C.  Paisley  J.  and Blei  D. (2011). Online variational inference for the hierarchical
Dirichlet process. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence
and Statistics  pages 752–760.

[26] Wang  X. and McCallum  A. (2006). Topics over time: a non-Markov continuous-time model of
topical trends. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 424–433. ACM.

[27] Williamson  S.  Wang  C.  Heller  K. A.  and Blei  D. M. (2010). The IBP compound Dirichlet
process and its application to focused topic modeling. In Proceedings of the 27th International
Conference on Machine Learning  pages 1151–1158.

[28] Yurochkin  M. and Nguyen  X. (2016). Geometric Dirichlet Means Algorithm for topic inference.

In Advances in Neural Information Processing Systems  pages 2505–2513.

[29] Yurochkin  M.  Guha  A.  and Nguyen  X. (2017). Conic Scan-and-Cover algorithms for
nonparametric topic modeling. In Advances in Neural Information Processing Systems  pages
3881–3890.

[30] Yurochkin  M.  Agarwal  M.  Ghosh  S.  Greenewald  K.  and Hoang  N. (2019a). Statistical
model aggregation via parameter matching. In Advances in Neural Information Processing Systems 
pages 10954–10964.

[31] Yurochkin  M.  Agarwal  M.  Ghosh  S.  Greenewald  K.  Hoang  N.  and Khazaeni  Y. (2019b).
Bayesian nonparametric federated learning of neural networks. In International Conference on
Machine Learning  pages 7252–7261.

[32] Yurochkin  M.  Guha  A.  Sun  Y.  and Nguyen  X. (2019c). Dirichlet simplex nest and geometric

inference. In International Conference on Machine Learning  pages 7262–7271.

11

,Mikhail Yurochkin
Zhiwei Fan
Aritra Guha
Paraschos Koutris
XuanLong Nguyen