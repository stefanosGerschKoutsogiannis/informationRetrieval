2018,DropMax: Adaptive Variational Softmax,We propose DropMax  a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically  we overlay binary masking variables over class output probabilities  which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover  the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification  on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.,DropMax: Adaptive Variational Softmax

Hae Beom Lee1 2  Juho Lee3 2  Saehoon Kim2  Eunho Yang1 2  Sung Ju Hwang1 2

KAIST1  AItrics2  South Korea 

University of Oxford3  United Kingdom 

{haebeom.lee  eunhoy  sjhwang82}@kaist.ac.kr
juho.lee@stats.ox.ac.uk  shkim@aitrics.com

Abstract

We propose DropMax  a stochastic version of softmax classiﬁer which at each
iteration drops non-target classes according to dropout probabilities adaptively
decided for each instance. Speciﬁcally  we overlay binary masking variables
over class output probabilities  which are input-adaptively learned via variational
inference. This stochastic regularization has an effect of building an ensemble
classiﬁer out of exponentially many classiﬁers with different decision boundaries.
Moreover  the learning of dropout rates for non-target classes on each instance
allows the classiﬁer to focus more on classiﬁcation against the most confusing
classes. We validate our model on multiple public datasets for classiﬁcation  on
which it obtains signiﬁcantly improved accuracy over the regular softmax classiﬁer
and other baselines. Further analysis of the learned dropout probabilities shows
that our model indeed selects confusing classes more often when it performs
classiﬁcation.

1

Introduction

Deep learning models have shown impressive performances on classiﬁcation tasks [17  10  11].
However  most of the efforts thus far have been made on improving the network architecture  while
the predominant choice of the ﬁnal classiﬁcation function remained to be the basic softmax regression.
Relatively less research has been done here  except for few works that propose variants of softmax 
such as Sampled Softmax [12]  Spherical Softmax [5]  and Sparsemax [22]. However  they either do
not target accuracy improvement or obtain improved accuracy only on certain limited settings.
In this paper  we propose a novel variant of softmax classiﬁer that achieves improved accuracy over
the regular softmax function by leveraging the popular dropout regularization  which we refer to as
DropMax. At each stochastic gradient descent step in network training  DropMax classiﬁer applies
dropout to the exponentiations in the softmax function  such that we consider the true class and a
random subset of other classes to learn the classiﬁer. At each training step  this allows the classiﬁer to
be learned to solve a distinct subproblem of the given multi-class classiﬁcation problem  enabling it
to focus on discriminative properties of the target class relative to the sampled classes. Finally  when
training is over  we can obtain an ensemble of exponentially many 1 classiﬁers with different decision
boundaries.
Moreover  when doing so  we further exploit the intuition that some classes could be more important
than others in correct classiﬁcation of each instance  as they may be confused more with the given
instance. For example in Figure 1  the instance of the class cat on the left is likely to be more confused
with class lion because of the lion mane wig it is wearing. The cat instance on the right  on the other
hand  resembles Jaguar due to its spots. Thus  we extend our classiﬁer to learn the probability of
dropping non-target classes for each input instance  such that the stochastic classiﬁer can consider

1to number of classes

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

classiﬁcation against confusing classes more of-
ten than others adaptively for each input. This
helps to better classify such difﬁcult instances 
which in turn will results in improving the over-
all classiﬁcation performance.
The proposed adaptive class dropout can be also
viewed as stochastic attention mechanism  that
selects a subset of classes each instance should
attend to in order for it to be well discrimi-
nated from any of the false classes. It also in
some sense has similar effect as boosting  since
learning a classiﬁer at each iteration with ran-
domly selected non-target classes can be seen
as learning a weak classiﬁer  which is combined
into a ﬁnal strong classiﬁer that solves the com-
plete multi-class classiﬁcation problem with the
weights provided by the class retain probabili-
ties learned for each input. Our regularization is
generic and can be applied even to networks on
which the regular dropout is ineffective  such as ResNet  to obtain improved performance.
We validate our model on ﬁve public datasets for classiﬁcation  on which it consistently obtains
signiﬁcant accuracy improvements over the base softmax  with noticeable improvements on ﬁne-
grained datasets (3.38%p on AWA and 7.77%p on CUB dataset.)
Our contribution is threefold:

Figure 1: Concepts.
For a given instance 
classes are randomly sampled with the probabili-
ties learned adaptively for each instance. Then the
sampled subset participates in the classiﬁcation.

• We propose a novel stochastic softmax function  DropMax  that randomly drops non-target

classes when computing the class probability for each input instance.

• We propose a variational inference framework to adaptively learn the dropout probability of
non-target classes for each input  s.t. our stochastic classiﬁer considers non-target classes
confused with the true class of each instance more often than others.

• We propose a novel approach to incorporate label information into our conditional variational

inference framework  which yields more accurate posterior estimation.

2 Related Work

Subset sampling with softmax classiﬁer Several existing work propose to consider only a partial
susbset of classes to compute the softmax  as done in our work. The main motivation is on improving
the efﬁciency of the computation  as matrix multiplication for computing class logits is expensive
when there are too many classes to consider. For example  the number of classes (or words) often
exceeds millions in language translation task. The common practice to tackle this challenge is to
use a shortlist of 30K to 80K the most frequent target words to reduce the inherent scale of the
classiﬁcation problem [3  20]. Further  to leverage the full vocabulary  [12] propose to calculate the
importance of each word with a deterministic function and select top-K among them. On the other
hand  [22] suggest a new softmax variant that can generate sparse class probabilities  which has a
similar effect to aforementioned models. Our model also works with subset of classes  but the main
difference is that our model aims to improve the accuracy of the classiﬁer  rather than improving its
computational efﬁciency.

Dropout variational inference Dropout [25] is one of the most popular and succesful regularizers
for deep neural networks. Dropout randomly drops out each neuron with a predeﬁned probability
at each iteration of a stochastic gradient descent  to achieve the effect of ensemble learning by
combining exponentially many networks learned during training. Dropout can be also understood
as a noise injection process [4]  which makes the model to be robust to a small perturbation of
inputs. Noise injection is also closely related to probabilistic modeling  and [8] has shown that a
network trained with dropout can be seen as an approximation to deep Gaussian process. Such
Bayesian understanding of dropout allows us to view model training as posterior inference  where

2

Lion5JaguarCatWhale0.80.6Input1Input2CatJaguarLionWhaleCatJaguarCatLionsampled subsetsampled subsetsampled subsettarget……predictive distribution is sampled by dropout at test time [13]. The same process can be applied to
convolutional [6] and recurrent networks [7].

Learning dropout probability
In regular dropout regularization  dropout rate is a tunable parame-
ter that can be found via cross-validation. However  some recently proposed models allow to learn
the dropout probability in the training process. Variational dropout [14] assumes that each individ-
ual weight has independent Gaussian distribution with mean and variance  which are trained with
reparameterization trick. Due to the central limit theorem  such Gaussian dropout is identical to the
binary dropout  with much faster convergence [25  27]. [23] show that variational dropout that allows
inﬁnite variance results in sparsity  whose effect is similar to automatic relevance determination
(ARD). All the aforementioned work deals with the usual posterior distribution not dependent on
input at test time. On the other hand  adaptive dropout [2] learns input dependent posterior at test
time by overlaying binary belief network on hidden layers. Whereas approximate posterior is usually
assumed to be decomposed into independent components  adaptive dropout allows us to overcome
it by learning correlations between network components in the mean of input dependent posterior.
Recently  [9] proposed to train dropout probability for each layer for accurate estimation of model
uncertainty  by reparameterizing Bernoulli distribution with continuous relaxation [21].

3 Approach
We ﬁrst introduce the general problem setup. Suppose a dataset D = {(xi  yi)}N
i=1  xi ∈ Rd  and
one-hot categorical label yi ∈ {0  1}K  with K the number of classes. We will omit the index i
when dealing with a single datapoint. Further suppose h = NN(x; ω)  which is the last feature
vector generated from an arbitrary neural network NN(·) parameterized by ω. Note that ω is globally
optimized w.r.t. the other network components to be introduced later  and we will omit the details for
brevity. We then deﬁne K dimensional class logits (or scores):

o(x; ψ) = W(cid:62)h + b  ψ = {W  b}

The original form of the softmax classiﬁer can then be written as:

(cid:80)

exp(ot(x; ψ))
k exp(ok(x; ψ))

p(y|x; ψ) =

3.1 DropMax

  where t is the target class of x.

(1)

(2)

(3)

As mentioned in the introduction  we propose to randomly drop out classes at training phase  with
the motivation of learning an ensemble of exponentially many classiﬁers in a single training. In
(2)  one can see that class k is completely excluded from the classiﬁcation when exp(ok) = 0 
and the gradients are not back-propagated from it. From this observation  we randomly drop
exp(o1)  . . .   exp(oK) based on Bernoulli trials  by introducing a dropout binary mask vector zk
with retain probability ρk  which is one minus the dropout probability for each class k:

zk ∼ Ber(zk; ρk) 

p(y|x  z; ψ) =

(zt + ε) exp(ot(x; ψ))
k(zk + ε) exp(ok(x; ψ))

(cid:80)

where sufﬁciently small ε > 0 (e.g. 10−20) prevents the whole denominator from vanishing.
However  if we drop the classes based on purely random Bernoulli trials  we may exclude the classes
that are important for classiﬁcation. Obviously  the target class t of a given instance should not
be dropped  but we cannot manually set the retain probabilities ρt = 1 since the target classes
differ for each instance  and more importantly  we do not know them at test time. We also want the
retain probabilities ρ1  . . .   ρK to encode meaningful correlations between classes  so that the highly
correlated classes may be dropped or retained together to limit the hypothesis space to a meaningful
subspace.
To resolve these issues  we adopt the idea of Adaptive Dropout [2]  and model ρ ∈ [0  1]K as an
output of a neural network which takes the last feature vector h as an input:
θ = {Wθ  bθ}.

ρ(x; θ) = sgm(W(cid:62)

(4)

θ h + bθ) 

3

(cid:80)

By learning θ  we expect these retain probabilities to be high for the target class of given inputs  and
consider correlations between classes. Based on this retain probability network  DropMax is deﬁned
as follows.

zk|x ∼ Ber(zk; ρk(x; θ)) 

p(y|x  z; ψ  θ) =

(zt + ε) exp(ot(x; ψ))
k(zk + ε) exp(ok(x; ψ))

(5)

The main difference of our model from [2] is that  unlike in the adaptive dropout where the neurons
of intermediate layers are dropped  we drop classes. As we stated earlier  this is a critical difference 
because by dropping classes we let the model to learn on different (sub)-problems at each iteration 
while in the adaptive dropout we train different models at each iteration. Of course  our model can be
extended to let it learn the dropout probabilities for the intermediate layers  but it is not our primary
concern at this point. Note that DropMax can be easily applied to any type of neural networks  such
as convolutional neural nets or recurrent neural nets  provided that they have the softmax output for
the last layer. This generality is another beneﬁt of our approach compared to the (adaptive) dropout
that are reported to degrade the performance when used in the intermediate layers of convolutional or
recurrent neural networks without careful conﬁguration.
A limitation of [2] is the use of heuristics to learn the dropout probabilities that may possibly
result in high variance in gradients during training. To overcome this weakness  we use concrete
distribution [21]  which is a continuous relaxation of discrete random variables that allows to back-
propagate through the (relaxed) bernoulli random variables zk to compute the gradients of θ [9]:

zk = sgm(cid:8)τ−1 (log ρk(x; θ) − log(1 − ρk(x; θ)) + log u − log(1 − u))(cid:9)

(6)
with u ∼ Unif(0  1). The temperature τ is usually set to 0.1  which determines the degree of
probability mass concentration towards 0 and 1.

4 Approximate Inference for DropMax

In this section  we describe the learning framework for DropMax. For notational simplicity  we deﬁne
X  Y  Z as the concatenations of xi  yi  and zi over all training instances (i = 1  . . .   N).

4.1

Intractable true posterior

We ﬁrst check the form of the true posterior distribution p(Z|X  Y) =(cid:81)N

i=1 p(zi|xi  yi). If it is
tractable  then we can use exact inference algorithms such as EM to directly maximize the log-
likelihood of our observation Y|X. For each instance  the posterior distribution can be written as

p(y  z|x)
p(y|x)

(cid:80)
p(y|x  z)p(z|x)
z(cid:48) p(y|x  z(cid:48))p(z(cid:48)|x)

=

where we let p(z|x) =(cid:81)K

p(z|x  y) =
(7)
k=1 p(zk|x) for simplicity. However  the graphical representation of (5)
indicates the dependencies among z1  . . .   zK when y is observed. It means that unlike p(z|x) 
the true posterior p(z|x  y) is not decomposable into the product of each element. Further  the
denominator is the summation w.r.t. the exponentially many combinations of z  which makes the
form of the true posterior even more complicated.
Thus  we suggest to use stochastic gradient variational Bayes (SGVB)  which is a general framework
for approximating intractable posterior of latent variables in neural network [15  24]. In standard
variational inference  we maximize the evidence lower bound (ELBO):

log p(Y|X; ψ  θ) ≥ N(cid:88)

(cid:26)

(cid:104)

Eq(zi|xi yi;φ)

log p(yi|zi  xi; ψ)

(cid:105) − KL
(cid:104)

q(zi|xi  yi; φ)(cid:13)(cid:13)p(zi|xi; θ)

(cid:105)(cid:27)

i=1

where q(zi|xi  yi; φ) is our approximate posterior with a set of variational parameters φ.

(8)

4.2 Structural form of the approximate posterior

The probabilistic interpretation of each term in (8) is straightforward. However  it does not tell us how
to encode them into the network components. Especially  in modeling q(z|x  y; φ)  how to utilize

4

(a) DropMax(q = p) (train)

(c) DropMax
and
DropMax(q = p) (test)
Figure 2: Illustration of model architectures. (a) DropMax (q = p) model at training time that lets q(z|x  y) =
p(z|x  y; θ)  except that it ﬁxes the target mask as 1. (b) DropMax model that utilizes the label information at
training time. (c) The test-time architecture for both models.

(b) DropMax (train)

the label y is not a straightforward matter. [24] suggests a simple concatenation [h(x); y] as an
input to q(z|x  y; φ)  while generating a pseudo-label from p(z|x; θ) to make the pipeline of training
and testing network to be identical. However  it signiﬁcantly increases the number of parameters of
the whole network. On the other hand  [24] also proposes another solution where the approximate
posterior simply ignores y and share the parameters with p(z|x; θ) (Figure 2(a)). This method is
known to be stable due to the consistency between training and testing pipeline [24  28  13]. However 
we empirically found that this approach produces suboptimal results for DropMax since it yields
inaccurate approximated posterior.
Our novel approach to solve the problem starts from the observation that the relationship between
z and y is relatively simple in DropMax (5)  unlike the case where latent variables are assumed at
lower layers. In this case  even though a closed form of true posterior is not available  we can capture
a few important property of it and encode them into the approximate posterior.
The ﬁrst step is to encode the structural form of the true posterior (7)  which is decomposable into
two factors: 1) the factor dependent only on x  and 2) the factor dependent on both x and y.

p(z|x  y) = p(z|x)

× p(y|z  x)/p(y|x)

.

(9)

(cid:124) (cid:123)(cid:122) (cid:125)A

(cid:124)

(cid:123)(cid:122)

B

(cid:125)

The key idea is that the factor B can be interpreted as the rescaling factor from the unlabeled posterior
p(z|x)  which takes x and y as inputs. In doing so  we model the approximate posterior q(z|x  y)
with two pipelines. Each of them corresponds to: A without label  which we already have deﬁned as
p(z|x; θ) = Ber(z; sgm(W(cid:62)
B is able to scale up or down A  but at the same time should bound the resultant posterior p(z|x  y) in
the range of [0  1]K. To model B with network components  we simply add to the logit of A  a vector
r ∈ RK taking x as an input (we will defer the description on how to use y to the next subsection).
Then we squash it again in the range of [0  1] (Note that addition in the logit level is multiplicative):

θ h + bθ)) in (4)  and B with label  which we discuss below.

g(x; φ) = sgm(W(cid:62)

θ h + bθ + r(x; φ)) 

(10)
where g(x; φ) is the main ingredient of our approximate posterior in (12)  and φ = {Wφ  bφ} is
variational parameter. Wθ and bθ denote that stop-gradients are applied to them  to make sure g(x; φ)
is only parameterized by the variational parameter φ  which is important for properly deﬁniting the
variational distribution. Next we discuss how to encode y into r(x; φ) and g(x; φ)  to ﬁnialize the
approximate posterior q(z|x  y; φ).

φ h + bφ.

r(x; φ) = W(cid:62)

4.3 Encoding the label information

Our modeling choice for encoding y is based on the following observations.
Observation 1. If we are given (x  y) and consider z1  . . .   zK one by one  then zt is positively
correlated with the DropMax likelihood p(y|x  z) in (5)  while zk(cid:54)=t is negatively correlated with it.
Observation 2. The true posterior of the target retain probability p(zt = 1|x  y) is 1  if we exclude
the case z1 = z2 = ··· = zK = 0  i.e. the retain probability for every class is 0.
One can easily verify the observation 1: the likelihood will increase if we attend the target more  and
vice versa. We encode this observation as follows. Noting that the likelihood p(y|x  z) is in general
maximized over the training instances  the factor B in (9) involves p(y|x  z) and should behave

5

11Laux(φ) = − N(cid:88)

K(cid:88)

(cid:110)

consistently (as in observation 1). Toward this  each rt(x; φ) and rk(cid:54)=t(x; φ) should be maximized
and minimized respectively. We achieve this by minimizing the cross-entropy for sgm(r(x; φ))
across the training instances:

yi k log sgm(rk(xi; φ)) + (1 − yi k) log(1 − sgm(rk(xi; φ)))

(11)
The observation 2 says that z\t (cid:54)= 0 → zt = 1 given y. Thus  simply ignoring the case z\t = 0 and
ﬁxing q(zt|x  y; φ) = Ber(zt; 1) is a close approximation of p(zt|x  y)  especially under mean-ﬁeld
assumption (see the Appendix A for justiﬁcation). Hence  our ﬁnal approximate posterior is given as:

i=1

k=1

(cid:111)

q(z|x  y; φ) = Ber(zt; 1)

Ber (zk; gk(x; φ)) .

(12)

(cid:89)

k(cid:54)=t

See Figure 2(b) and 2(c) for the illustration of the model architecture.

4.4 Regularized variational inference
One last critical issue in optimizing the ELBO (8) is that p(z|x; θ) collapses into q(z|x  y; φ) too
easily  as p(z|x; θ) is parameteric with input x. Preventing it is crucial for z to generalize well on
a test instance x∗  because z is sampled from p(z|x∗; θ) at test time (Figure 2(c)). We empirically
found that imposing some prior (e.g. zero-mean gaussian or laplace prior) to θ = {Wθ  bθ} was not
effective in preventing this behavior. (The situation is different from VAE [15] where the prior of
latent code p(z) is commonly set to gaussian with no trainable parameters (i.e. N (0  λI)).)
We propose to remove weight decay for θ and apply an entropy regularizer directly to p(z|x; θ). We
empirically found that the method works well without any scaling hyperparameters.

H(p(z|x; θ)) =

ρk log ρk + (1 − ρk) log(1 − ρk)

(13)

(cid:88)

k

We are now equipped with all the essential components. The KL divergence and the ﬁnal minimization
objective are given as:

KL[q(z|x  y; φ)||p(z|x; θ)] =

I{k=t} log

+ I{k(cid:54)=t}

1
ρk

gk log

+ (1 − gk) log

gk
ρk

(cid:18)

(cid:19)(cid:41)

1 − gk
1 − ρk

(cid:21)

(cid:40)

(cid:88)

k

(cid:20)

N(cid:88)

i=1

S(cid:88)

s=1

− 1
S

L(ψ  θ  φ) =

log p(yi|xi  z(s)

i

; ψ) + KL[q(zi|xi  yi; φ)||p(zi|xi; θ)] − H

+ Laux

i ∼ q(zi|xi  yi; φ) and S = 1 as usual. Figure 2(b) and (c) illustrate the model architectures

where z(s)
for training and testing respectively.
When testing  we can perform Monte-Carlo sampling:

p(y∗|x∗) = Ez[p(y∗|x∗  z)] ≈ 1
S

p(y∗|x∗  z(s)) 

z(s) ∼ p(z|x∗; θ).

Alternatively  we can approximate the expectation as

p(y∗|x∗) = Ez[p(y∗|x∗  z)] ≈ p(y∗|x∗  E[z|x∗]) = p(y∗|x∗  ρ(x∗; θ)) 
which is a common practice for many practitioners. We report test error based on (15).

5 Experiments

(14)

(15)

S(cid:88)

s=1

Baselines and our models We ﬁrst introduce relevant baselines and our models.
1) Base Softmax. The baseline CNN network with softmax  that only uses the hidden unit dropout at
fully connected layers  or no dropout regularization at all.
2) Sparsemax. Base network with Sparsemax loss proposed by [22]  which produces sparse class
probabilities.

6

Table 1: Classiﬁcation performance in test error (%). The reported numbers are mean and standard
errors with 95% conﬁdence interval over 5 runs.

Models

Base Softmax
Sparsemax [22]

DropMax

Sampled Softmax [12]

Random DropMax

Deterministic Attention
Deterministic DropMax

DropMax (q = p)

M-1K

7.09±0.46
6.57±0.17
7.36±0.22
7.19±0.57
6.91±0.46
6.30±0.64
7.52±0.26
5.32±0.09

M-5K

2.13±0.21
2.05±0.18
2.31±0.14
2.23±0.19
2.03±0.11
1.89±0.04
2.05±0.07
1.64±0.08

M-55K
0.65±0.04
0.75±0.06
0.66±0.04
0.68±0.07
0.69±0.05
0.64±0.05
0.63±0.02
0.59±0.04

C10

7.90±0.21
7.90±0.28
7.98±0.24
8.21±0.08
7.87±0.24
7.84±0.14
7.80±0.22
7.67±0.11

C100

30.60±0.12
31.41±0.16
30.87±0.19
30.78±0.28
30.60±0.21
30.55±0.51
29.98±0.35
29.87±0.36

AWA

30.29±0.80
36.06±0.64
29.81±0.45
31.11±0.54
30.98±0.66
26.22±0.76
29.27±1.19
26.91±0.54

CUB

48.84±0.85
64.41±1.12
49.90±0.56
48.87±0.79
49.97±0.32
47.35±0.42
42.08±0.94
41.07±0.57

3) Sampled Softmax. Base network with sampled softmax [12]. Sampling function Q(y|x) is uni-
formly distributed during training. We tune the number of sampled classes among {20%  40%  60%}
of total classes  while the target class is always selected. Test is done with (2).
4) Random DropMax. A baseline that randomly drops out non-target classes with a predeﬁned
retain probability ρ ∈ {0.2  0.4  0.6} at training time. For learning stability  the target class is not
dropped out during training. Test is done with the softmax function (2)  without sampling the dropout
masks.
5) Deterministic Attention. Softmax with deterministic sigmoid attentions multiplied at the expo-
nentiations. The attention probabilities are generated from the last feature vector h with additional
weights and biases  similarly to (4).
6) Deterministic DropMax. This model is the same with Deterministic Attention  except that it is
trained in a supervised manner with true class labels. With such consideration of labels when training
the attention generating network  the model can be viewed as a deterministic version of DropMax.
7) DropMax (q = p). A variant of DropMax where we let q(z|x  y) = p(z|x; θ) except that we
ﬁx q(zt|x  y) = Ber(zt; 1) as in (12) for learning stability. The corresponding KL[q(cid:107)p] can be
easily computed. The auxiliary loss term Laux is removed and the entropy term H is scaled with a
hyperparameter γ ∈ {1  0.1  0.01} (See Figure 2(a)).
8) DropMax. Our adaptive stochastic softmax  where each class is dropped out with input dependent
probabilities trained from the data. No hyperparameters are needed for scaling each term.
We implemented DropMax using Tensorﬂow [1] framework. The source codes are available at
https://github.com/haebeom-lee/dropmax.

Datasets and base networks We validate our method on multiple public datasets for classiﬁcation 
with different network architecture for each dataset.
1) MNIST. This dataset [19] consists of 60  000 images that describe hand-written digits from 0 to 9.
We experiment with varying number of training instances: 1K  5K  and 55K. The validation and
test set has 5K and 10K instances  respectively. As for the base network  we use the CNN provided
in the Tensorﬂow Tutorial  which has a similar structure to LeNet.
2) CIFAR-10. This dataset [16] consists of 10 generic object classes  which for each class has 5000
images for training and 1000 images for test. We use ResNet-34 [10] as the base network.
3) CIFAR-100. This dataset consists of 100 object classes. It has 500 images for training and 100
images are for test for each class. We use ResNet-34 as the base network.
4) AWA. This is a dataset for classifying different animal species [18]  that contains 30  475 images
from 50 animal classes such as cow  fox  and humpback whale. For each class  we used 50 images for
test  while rest of the images are used as training set. We use ResNet-18 as the base network.
5) CUB-200-2011. This dataset [26] consists of 200 bird classes such as Black footed albatross 
Rusty blackbird  and Eastern towhee. It has 5994 training images and 5794 test images  which is
quite small compared to the number of classes. We only use the class label for the classiﬁcation. We
use ResNet-18 as the base network.
As AWA and CUB datasets are subsets of ImageNet-1K dataset  for those datasets we do not use a
pretained model but train from scratch. The experimental setup is available in the Appendix C.

7

(a) ρ (easy)

(b) ρ (hard)

(c) q (hard)

(d) z(s) (hard)

(e) ¯ρ (hard)

(f) p(y|x)

Figure 4: Visualization of class dropout probabilities for example test instances from MNIST-1K dataset. (a)
and (b) shows estimated class retain probability for easy and difﬁcult test instances respectively. The green o’s
denote the ground truths  while the red x’s denote the base model predictoins. (c) shows approximate posterior
q(z|x  y; φ). (d) shows generated retain masks from (b). (e) shows the average retain probability per class for
hard instances. (f) shows sampled predictive distributions of easy and difﬁcult instance respectively.

5.1 Quantitative Evaluation

Multi-class classiﬁcation. We report the classiﬁcation performances of our models and the base-
lines in Table 1. The results show that the variants of softmax function such as Sparsemax and
Sampled Softmax perform similarly to the original softmax function (or worse). Random DropMax
also performs worse due to the inconsistency between train and test time handling of the dropout
probabilities for target class. Deterministic Attention also performs similarly to all the previous
baselines. Interestingly  Deterministic DropMax with supervised learning of attention mechanism
improves the performance over the base soft classiﬁer  which suggests that such combination of a
multi-class and multi-label classiﬁer could be somewhat beneﬁcial. However  the improvements are
marginal except on the AWA dataset  because the gating function also lacks proper regularization add
thus yields very sharp attention probabilities. DropMax (q = p) has an entropy regularizer to address
this issue  but the model obtains suboptimal performance due to the inaccurate posterior estimation.
On the other hand  the gating function of DropMax is optimally regularized to make a crude selection
of candidate classes via the proposed variational inference framework  and shows consistent and
signiﬁcant improvements over the baselines across all datasets. DropMax also obtains noticeably
higher accuracy gains on AWA and CUB dataset that are ﬁne-grained  with 3.38%p and 7.77%p
improvements  as these ﬁne-grained datasets contain many ambiguous instances that can be effectively
handled by DropMax with its focus on the most confusing classes. On MNIST dataset  we also
observe that the DropMax is more effective when the number of training instances is small. We
attribute this to the effect of stochastic regularization that effectively prevents overﬁtting. This is also
a general advantage of Bayesian learning as well.

Convergence rate. We examine the con-
vergence rate of our model against the
base network with regular softmax func-
tion. Figure 3 shows the plots of cross en-
tropy loss computed at each training step
on MNIST-55K and CIFAR-100. To re-
duce the variance of z  we plot with ρ in-
stead (training is done with z). DropMax
shows slightly lower convergence rate  but
the test loss is signiﬁcantly improved  effec-
tively preventing overﬁtting. Moreover  the
learning curve of DropMax is more stable
than that of regular softmax (see Appendix B for more discussion).

(a) MNIST-55K

Figure 3: Convergence plots

(b) CIFAR-100

5.2 Qualitative Analysis

We further perform qualitative analysis of our model to see how exactly it works and where the
accuracy improvements come from.

8

012345678901234567890.20.30.4010K20K30K40K50K60Knumber of steps103102101100cross entropyBase trainBase testDropMax trainDropMax test010K20K30K40K50K60Knumber of steps0.51.02.05.0cross entropyBase trainBase testDropMax trainDropMax testFigure 5: Examples from CIFAR-100 dataset with top-4 and bottom-2 retain probabilities. Blue and red color
denotes the ground truths and base model predictions respectively.

Figure 4(a) shows the retain probabilities estimated for easy examples  in which case the model set
the retain probability to be high for the true class  and evenly low for non-target classes. Thus  when
the examples are easy  the dropout probability estimator works like a second classiﬁer. However  for
difﬁcult examples in Figure 4(b) that is missclassiﬁed by the base softmax function  we observe that
the retain probability is set high for the target class and few other candidates  as this helps the model
focus on the classiﬁcation between them. For example  in Figure 4(b)  the instance from class 3 sets
high retain probability for class 5  since its handwritten character looks somewhat similar to number
5. However  the retain probability could be set differently even across the instances from the same
class  which makes sense since even within the same class  different instances may get confused with
different classes. For example  for the ﬁrst instance of 4  the class with high retain probability is 0 
which looks like 0 in its contour. However  for the second instance of 4  the network sets class 9 with
high retain probability as this instance looks like 9.
Similar behaviors can be observed on CIFAR-100 dataset (Figure 5) as well. As an example  for
instances that belong to class girl  DropMax sets the retain probability high on class woman and boy 
which shows that it attends to most confusing classes to focus more on such difﬁcult problems.
We further examine the class-average dropout probabilities for each class in MNIST dataset in
Figure 4(e). We observe the patterns by which classes are easily confused with the others. For
example  class 3 is often confused with 5  and class 4 with 9. It suggests that retain probability
implicitly learns correlations between classes  since it is modeled as an input dependent distribution.
Also  since DropMax is a Bayesian inference framework  we can easily obtain predictive uncertainty
from MC sampling in Figure 4(f)  even when probabilistic modeling on intermediate layers is difﬁcult.

6 Conclusion and Future Work
We proposed a stochastic version of a softmax function  DropMax  that randomly drops non-target
classes at each iteration of the training step. DropMax enables to build an ensemble over exponentially
many classiﬁers that provide different decision boundaries. We further proposed to learn the class
dropout probabilities based on the input  such that it can consider the discrimination of each instance
against more confusing classes. We cast this as a Bayesian learning problem and present how
to optimize the parameters through variational inference  while proposing a novel regularizer to
more exactly estimate the true posterior. We validate our model on multiple public datasets for
classiﬁcation  on which our model consistently obtains signiﬁcant performance improvements over
the base softmax classiﬁer and its variants  achieving especially high accuracy on datasets for ﬁne-
grained classiﬁcation. For future work  we plan to further investigate the source of generalization
improvements with DropMax  besides increased stability of gradients (Appendix B).

Acknowledgement
This research was supported by the Engineering Research Center Program through the Na-
tional Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-
2018R1A5A1059921)  Samsung Research Funding Center of Samsung Electronics (SRFC-
IT150203)  Machine Learning and Statistical Inference Framework for Explainable Artiﬁcial Intel-
ligence (No.2017-0-01779)  and Basic Science Research Program through the National Research
Foundation of Korea (NRF) funded by the Ministry of Education (2015R1D1A1A01061019). Juho
Lee’s research leading to these results has received funding from the European Research Council
under the European Union’s Seventh Framework Programme (FP7/2007-2013) ERC grant agreement
no. 617071.

9

girlwomanbabyboyrabbitmanmaple treewillow treepine treeskyscraperpalm treebridgewardrobechaircouchbedtroutbicycletroutflatfishraycrabcrocodileotteroak treepine treemaple treewillow treedinosaursunflowerraywhaledolphinoak treebearotteroaktreemaple treedinosaurlionstreetcarwhalegirlwomanboywardrobebearmanReferences
[1] M. Abadi  A. Agarwal  P. Barham  E. Brevdo  Z. Chen  C. Citro  G. S. Corrado  A. Davis 
J. Dean  M. Devin  et al. Tensorﬂow: Large-scale Machine Learning on Heterogeneous
Distributed Systems. arXiv:1603.04467  2016.

[2] J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In NIPS  2013.

[3] D. Bahdanau  K. Cho  and Y. Bengio. Neural Machine Translation by Jointly Learning to Align

and Translate. In ICLR  2015.

[4] X. Bouthillier  K. Konda  P. Vincent  and R. Memisevic. Dropout as data augmentation. ArXiv

e-prints  June 2015.

[5] A. de Brébisson and P. Vincent. An exploration of softmax alternatives belonging to the

spherical loss family. In ICLR  2016.

[6] Y. Gal and Z. Ghahramani. Bayesian Convolutional Neural Networks with Bernoulli Approxi-

mate Variational Inference. ArXiv e-prints  June 2015.

[7] Y. Gal and Z. Ghahramani. A Theoretically Grounded Application of Dropout in Recurrent

Neural Networks. In NIPS  2016.

[8] Y. Gal and Z. Ghahramani. Dropout as a Bayesian Approximation: Representing Model

Uncertainty in Deep Learning. In ICML  2016.

[9] Y. Gal  J. Hron  and A. Kendall. Concrete Dropout. In NIPS  2017.

[10] K. He  X. Zhang  S. Ren  and J. Sun. Deep Residual Learning for Image Recognition. In CVPR 

2016.

[11] G. Huang  Z. Liu  L. van der Maaten  and K. Q. Weinberger. Densely connected convolutional

networks. In CVPR  2017.

[12] S. Jean  K. Cho  R. Memisevic  and Y. Bengio. On Using Very Large Target Vocabulary for

Neural Machine Translation. In ACL  2015.

[13] A. Kendall and Y. Gal. What Uncertainties Do We Need in Bayesian Deep Learning for

Computer Vision? In NIPS  2017.

[14] D. P. Kingma  T. Salimans  and M. Welling. Variational Dropout and the Local Reparameteriza-

tion Trick. In NIPS  2015.

[15] D. P. Kingma and M. Welling. Auto encoding variational bayes. In ICLR  2014.

[16] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.

[17] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet Classiﬁcation with Deep Convolutional

Neural Networks. In NIPS  2012.

[18] C. Lampert  H. Nickisch  and S. Harmeling. Learning to Detect Unseen Object Classes by

Between-Class Attribute Transfer. In CVPR  2009.

[19] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[20] M.-T. Luong  I. Sutskever  Q. V. Le  O. Vinyals  and W. Zaremba. Addressing the Rare Word

Problem in Neural Machine Translation. In ACL  2015.

[21] C. J. Maddison  A. Mnih  and Y. Whye Teh. The Concrete Distribution: A Continuous

Relaxation of Discrete Random Variables. In ICLR  2017.

[22] A. F. T. Martins and R. Fernandez Astudillo. From Softmax to Sparsemax: A Sparse Model of

Attention and Multi-Label Classiﬁcation. In ICML  2016.

10

[23] D. Molchanov  A. Ashukha  and D. Vetrov. Variational Dropout Sparsiﬁes Deep Neural

Networks. In ICML  2017.

[24] K. Sohn  H. Lee  and X. Yan. Learning structured output representation using deep conditional

generative models. In NIPS  2015.

[25] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: A
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research 
15:1929–1958  2014.

[26] C. Wah  S. Branson  P. Welinder  P. Perona  and S. Belongie. The Caltech-UCSD Birds-200-2011

Dataset. Technical report  2011.

[27] S. Wang and C. Manning. Fast dropout training. In ICML  2013.

[28] K. Xu  J. Ba  R. Kiros  K. Cho  A. Courville  R. Salakhutdinov  R. Zemel  and Y. Bengio. Show 

Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML  2015.

11

,William Uther
Hae Beom Lee
Juho Lee
Saehoon Kim
Eunho Yang
Sung Ju Hwang