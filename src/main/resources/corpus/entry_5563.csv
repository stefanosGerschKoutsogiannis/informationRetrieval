2017,Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation,The sparse matrix estimation problem consists of estimating the distribution of an $n\times n$  matrix $Y$  from a sparsely observed single instance of this matrix where the entries of $Y$ are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems  graphon estimation  and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems  we propose a novel iterative  collaborative filtering-style algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to $0$ at the rate of $O(d^2 (pn)^{-2/5})$ as long as $\omega(d^5 n)$ random entries from a total of $n^2$ entries of $Y$ are observed (uniformly sampled)  $\E[Y]$ has rank $d$  and the entries of $Y$ have bounded support. The maximum squared error across all entries converges to $0$ with high probability as long as we observe a little more  $\Omega(d^5 n \ln^5(n))$ entries. Our results are the best known sample complexity results in this generality.,Thy Friend is My Friend: Iterative Collaborative

Filtering for Sparse Matrix Estimation

Christian Borgs
borgs@microsoft.com

Jennifer Chayes

jchayes@microsoft.com
Microsoft Research New England

One Memorial Drive  Cambridge MA  02142

Christina E. Lee
celee@mit.edu

Devavrat Shah

devavrat@mit.edu

Massachusetts Institute of Technology

77 Massachusetts Ave  Cambridge  MA 02139

Abstract

The sparse matrix estimation problem consists of estimating the distribution of
an n × n matrix Y   from a sparsely observed single instance of this matrix where
the entries of Y are independent random variables. This captures a wide array
of problems; special instances include matrix completion in the context of rec-
ommendation systems  graphon estimation  and community detection in (mixed
membership) stochastic block models. Inspired by classical collaborative ﬁltering
for recommendation systems  we propose a novel iterative  collaborative ﬁltering-
style algorithm for matrix estimation in this generic setting. We show that the mean
squared error (MSE) of our estimator converges to 0 at the rate of O(d2(pn)−2/5)
as long as ω(d5n) random entries from a total of n2 entries of Y are observed
(uniformly sampled)  E[Y ] has rank d  and the entries of Y have bounded support.
The maximum squared error across all entries converges to 0 with high probability
as long as we observe a little more  Ω(d5n ln5(n)) entries. Our results are the best
known sample complexity results in this generality.

1

Introduction

In this work  we propose and analyze an iterative similarity-based collaborative ﬁltering algorithm
for the sparse matrix completion problem with noisily observed entries. As a prototype for such a
problem  consider a noisy observation of a social network where observed interactions are signals
of true underlying connections. We might want to predict the probability that two users would
choose to connect if recommended by the platform  e.g. LinkedIn. As a second example  consider
a recommendation system where we observe movie ratings provided by users  and we may want
to predict the probability distribution over ratings for speciﬁc movie-user pairs. The classical
collaborative ﬁltering approach is to compute similarities between pairs of users by comparing their
commonly rated movies. For a social network  similarities between users would be computed by
comparing their sets of friends. We will be particularly interested in the very sparse case where most
pairs of users have no common friends  or most pairs of users have no commonly rated movies; thus
there is insufﬁcient data to compute the traditional similarity metrics.
To overcome this limitation  we propose a novel algorithm which computes similarities iteratively 
incorporating information within a larger radius neighborhood. Whereas traditional collaborative
ﬁltering learns the preferences of a user through the ratings of her/his “friends”  i.e. users who share
similar ratings on commonly rated movies  our algorithm learns about a user through the ratings of

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the friends of her/his friends  i.e. users who may be connected through an indirect path in the data.
For a social network  this intuition translates to computing similarities of two users by comparing
the boundary of larger radius neighborhoods of their connections in the network. While an actual
implementation of our algorithm will beneﬁt from modiﬁcations to make it practical  we believe
that our approach is very practical; indeed  we plan to implement it in a corporate setting. Like all
such nearest-neighbor style algorithms  our algorithm can be accelerated and scaled to large datasets
in practice by using a parallel implementation via an approximate nearest neighbor data structure.
In this paper  however  our goal is to describe the basic setting and concept of the algorithm  and
provide clear mathematical foundation and analysis. The theoretical results indicate that this method
achieves consistency (i.e. guaranteed convergence to the correct solution) for very sparse datasets for
a reasonably general Latent Variable Model with bounded entries.
The problems discussed above can be mathematically formulated as a matrix estimation problem 
where we observe a sparse subset of entries in an m × n random matrix Y   and we wish to complete
or de-noise the matrix by estimating the probability distribution of Yij for all (i  j). Suppose that Yij
is categorical  taking values in [k] according to some unknown distribution. The task of estimating the
distribution of Yij can be reduced to k − 1 smaller tasks of estimating the expectation of a binary data
ij] = P(Yij = t). If the matrix that we would like to
matrix  e.g. Y t where Y t
learn is asymmetric  we can transform it to an equivalent symmetric model by deﬁning a new data

(cid:3). Therefore  for the remainder of the paper  we will assume a n × n symmetric

ij = I(Yij = t) and E[Y t

matrix Y (cid:48) =(cid:2) 0 Y

Y T 0

matrix which takes values in [0  1] (real-valued or binary)  but as argued above  our results apply
more broadly to categorical-valued asymmetric matrices. We assume that the data is generated from
a Latent Variable Model in which latent variables θ1  . . .   θn are sampled independently from U [0  1] 
and the distribution of Yij is such that E[Yij|θi  θj] = f (θi  θj) ≡ Fij for some latent function f. Our
goal is to estimate the matrix F . It is worth remarking that the Latent Variable Model is a canonical
representation for exchangeable arrays as shown by Aldous and Hoover [5  25  7].
We present a novel algorithm for estimating F = [Fij] from a sparsely sampled dataset {Yij}(i j)∈E
where E ⊂ [n] × [n] is generated by assuming each entry is observed independently with probability
p. We require that the latent function f when regarded as an integral operator has ﬁnite spectrum with
rank d. We prove that the mean squared error (MSE) of our estimates converges to zero at a rate of
O(d2(pn)−2/5) as long as the sparsity p = ω(d5n−1) (i.e. ω(d5n) total observations). In addition 
with high probability  the maximum squared error converges to zero at a rate of O(d2(pn)−2/5) as
long as the sparsity p = Ω(d5n−1 ln5(n)). Our analysis applies to a generic noise setting as long as
Yij has bounded support.
Our work takes inspiration from [1  2  3]  which estimates clusters of the stochastic block model by
computing distances from local neighborhoods around vertices. We improve upon their analysis to
provide MSE bounds for the general latent variable model with ﬁnite spectrum  which includes a
larger class of generative models such as mixed membership stochastic block models  while they
consider the stochastic block model with non-overlapping communities. We show that our results
hold even when the rank d increases with n  as long as d = o((pn)1/5). As compared to spectral
methods such as [28  39  20  19  18]  our analysis handles the general bounded noise model and holds
for sparser regimes  only requiring p = ω(n−1).

Related work. The matrix estimation problem introduced above includes as speciﬁc cases problems
from different areas of literature: matrix completion popularized in the context of recommendation
systems  graphon estimation arising from the asymptotic theory of graphs  and community detection
using the stochastic block model or its generalization known as the mixed membership stochastic
block model. The key representative results for each of these are mentioned in Table 1. We discuss
the scaling of the sample complexity with respect to d (model complexity  usually rank) and n
for polynomial time algorithms  including results for both mean squared error convergence  exact
recovery in the noiseless setting  and convergence with high probability in the noisy setting. As
can be seen from Table 1  our result provides the best sample complexity with respect to n for the
general matrix estimation problem with bounded entries noise model and rank d  as the other models
either require extra log(n) factors  or impose additional requirements on the noise model or the
expected matrix. Similarly  ours is the best known sample complexity for high probability max-error
convergence to 0 for the general rank d bounded entries setting  as other results either assume block
constant or noiseless.

2

Table 1: Sample Complexity of Related Literature grouped in sections according to the following
areas —matrix completion  1-bit matrix completion  stochastic block model  mixed membership
stochastic block model  graphon estimation  and our results

Sample Complexity
ω(dn)
Ω(dn max(log n  d))  ω(dn)
ω(dn log n)
Ω(n max(d  log2 n))
ω(dn log6 n)
Ω(n3/2)
Ω(dn log2 n max(d  log4 n))
Ω(dn max(d  log n))
Ω(dn log2 n)
Ω(n max(d log n  log2 n  d2))
Ω(n max(d  log n))  ω(dn)
ω(n)∗
Ω(n log n)∗
Ω(n log n)∗
Ω(d2n polylog n)
Ω(d2n)
Ω(n2)
Ω(n2)
ω(n)
ω(d5n)

Paper
[27]
[28]
[37]
[19]
[18]
[32]
[17]
[27]
[39]
[19]
[20]
[1  3]
[1]
[43]
[6]
[40]
[4]
[44]
[10]
this
work Ω(d5n log5 n)

Data/Noise
noiseless
iid Gaussian
iid Gaussian
iid Gaussian
indep bounded
iid bounded
noiseless
noiseless
noiseless
binary entries
binary entries
binary entries
binary entries
binary entries
binary entries
binary entries
binary entries
binary entries
binary entries
indep bounded
indep bounded

*result does not indicate dependence on d.

Guarantee
Expected matrix
MSE→ 0
rank d
MSE→ 0
rank d
MSE→ 0
rank d
MSE→ 0
rank d
MSE→ 0
rank d
MSE→ 0
Lipschitz
exact recovery
rank d
exact recovery
rank d
exact recovery
rank d
MSE→ 0
rank d
MSE→ 0
rank d
partial recovery
d blocks
exact recovery
d blocks (SBM)
MSE→ 0
rank d
whp error → 0
rank d
detection
rank d
monotone row sum MSE→ 0
piecewise Lipschitz MSE→ 0
monotone row sum MSE→ 0
MSE→ 0
rank d  Lipschitz
whp error → 0
rank d  Lipschitz

It is worth comparing our results with the known lower bounds on the sample complexity. For the
special case of matrix completion with an additive noise model  i.e. Yij = E[Yij] + ηij and ηij are
i.i.d. zero mean  [16  20] showed that ω(dn) samples are needed for a consistent estimator  i.e. MSE
convergence to 0  and [17] showed that dn log n samples are needed for exact recovery. There is a
conjectured computational lower bound for the mixed membership stochastic block model of d2n
even for detection  which is weaker than MSE going to 0. Recently  [40] showed a partial result that
this computational lower bound holds for algorithms that rely on ﬁtting low-degree polynomials to
the observed data. Given that these lower bounds apply to special cases of our setting  it seems that
our result is optimal in terms of its dependence on n for MSE convergence as well as high probability
(near) exact recovery.
Next we provide a brief overview of prior works reported in the Tables 1. In the context of matrix
completion  there has been much progress under the low-rank assumption. Most theoretically founded
methods are based on spectral decompositions or minimizing a loss function with respect to spectral
constraints [27  28  15  17  39  37  20  19  18]. A work that is closely related to ours is by [32]. It
proves that a similarity based collaborative ﬁltering-style algorithm provides a consistent estimator
for matrix completion under the generic model when the latent function is Lipschitz  not just low
rank; however  it requires ˜O(n3/2) samples. In a sense  ours can be viewed as an algorithmic
generalization of [32] that handles the sparse sampling regime and a generic noise model. Most of
the results in matrix completion require additive noise models  which do not extend to setting when
the observations are binary or quantized. The USVT estimator is able to handle general bounded
noise  although it requires a few log factors more in its sample complexity [18]. Our work removes
the extra log factors while still allowing for general bounded noise.
There is also a signiﬁcant amount of literature which looks at the estimation problem when the data
matrix is binary  also known as 1-bit matrix completion  stochastic block model (SBM) parameter
estimation  or graphon estimation. The latter two terms are found within the context of community

3

detection and network analysis  as the binary data matrix can alternatively be interpreted as the
adjacency matrix of a graph – which are symmetric  by deﬁnition. Under the SBM  each vertex is
associated to one of d community types  and the probability of an edge is a function of the community
types of both endpoints. Estimating the n × n parameter matrix becomes an instance of matrix
estimation. In SBM  the expected matrix is at most rank d due to its block structure. Precise thresholds
for cluster detection (better than random) and estimation have been established by [1  2  3]. Our
work  both algorithmically and technically  draws insight from this sequence of works  extending
the analysis to a broader class of generative models through the design of an iterative algorithm  and
improving the technical results with precise MSE bounds.
The mixed membership stochastic block model (MMSBM) allows each vertex to be associated to
a length d vector  which represents its weighted membership in each of the d communities. The
probability of an edge is a function of the weighted community memberships vectors of both endpoints 
resulting in an expected matrix with rank at most d. Recent work by [40] provides an algorithm for
weak detection for MMSBM with sample complexity d2n  when the community membership vectors
are sparse and evenly weighted. They provide partial results to support a conjecture that d2n is a
computational lower bound  separated by a gap of d from the information theoretic lower bound of
dn. This gap was ﬁrst shown in the simpler context of the stochastic block model [21]. [43] proposed
a spectral clustering method for inferring the edge label distribution for a network sampled from a
generalized stochastic block model. When the expected function has a ﬁnite spectrum decomposition 
i.e. low rank  then they provide a consistent estimator for the sparse data regime  with Ω(n log n)
samples.
Graphon estimation extends SBM and MMSBM to the generic Latent Variable Model where the
probability of an edge can be any measurable function f of real-valued types (or latent variables)
associated to each endpoint. Graphons were ﬁrst deﬁned as the limiting object of a sequence of large
dense graphs [14  22  34]  with recent work extending the theory to sparse graphs [12  13  11  41].
In the graphon estimation problem  we would like to estimate the function f given an instance of
a graph generated from the graphon associated to f. [23  29] provide minimax optimal rates for
graphon estimation; however a majority of the proposed estimators are not computable in polynomial
time  since they require optimizing over an exponentially large space (e.g. least squares or maximum
likelihood) [42  10  9  23  29]. [10] provided a polynomial time method based on degree sorting in
the special case when the expected degree function is monotonic. To our knowledge  existing positive
results for sparse graphon estimation require either strong monotonicity assumptions [10]  or rank
constraints as assumed in the SBM  the 1-bit matrix completion  and in this work.
We call special attention to the similarity based methods which are able to bypass the rank constraints 
relying instead on smoothness properties of the latent function f (e.g. Lipschitz) [44  32]. They
hinge upon computing similarities between rows or columns by comparing commonly observed
entries. Similarity based methods  also known in the literature as collaborative ﬁltering  have been
successfully employed across many large scale industry applications (Netﬂix  Amazon  Youtube) due
to its simplicity and scalability [24  33  30  38]; however the theoretical results have been relatively
sparse. These recent results suggest that the practical success of these methods across a variety of
applications may be due to its ability to capture local structure. A key limitation of this approach is
that it requires a dense dataset with sufﬁcient entries in order to compute similarity metrics  requiring
that each pair of rows or columns has a growing number of overlapped observed entries  which does
not hold when p = o(n−1/2). This work overcomes this limitation in an intuitive and simple way;
rather than only considering directly overlapped entries  we consider longer “paths” of data associated
to each row  expanding the set of associated datapoints until there is sufﬁcient overlap. Although we
may initially be concerned that this would introduce bias and variance due to the sparse sampling 
our analysis shows that in fact the estimate does converge to the true solution.
The idea of comparing vertices by looking at larger radius neighborhoods was introduced in [1]  and
has connections to belief propagation [21  3] and the non-backtracking operator [31  26  36  35  8].
The non-backtracking operator was introduced to overcome the issue of sparsity. For sparse graphs 
vertices with high-degree dominate the spectrum  such that the informative components of the
spectrum get hidden behind the high degree vertices. The non-backtracking operator avoids paths
that immediately return to the previously visited vertex in a similar manner as belief propagation 
and its spectrum has been shown to be more well-behaved  perhaps adjusting for the high degree
vertices  which get visited very often by paths in the graph. In our algorithm  the neighborhood paths
are deﬁned by ﬁrst selecting a rooted tree at each vertex  thus enforcing that each vertex along a path

4

in the tree is unique. This is important in our analysis  as it guarantees that the distribution of vertices
at the boundary of each subsequent depth of the neighborhood is unbiased  since the sampled vertices
are freshly visited.

2 Model

We shall use graph and matrix notations in an interchangeable manner. For each pair of vertices (i.e.
row or column indices) u  v ∈ [n]  let Yuv ∈ [0  1] denote its random realization. Let E denote the
edges. If (u  v) ∈ E  Yuv is observed; otherwise it is unknown.
• Each vertex u ∈ [n] is associated to a latent variable θu ∼ U [0  1] sampled i.i.d.
• For each (u  v) ∈ [n] × [n]  Yuv = Yvu ∈ [0  1] is a bounded random variable. Conditioned on

{θi}i∈[n]  the random variables {Yuv}1≤u<v≤n are independent.

(cid:3) = f (θu  θv) ∈ [0  1] for a symmetric L-Lipschitz function f.
f (θu  θv) =(cid:80)d

k=1 λkqk(θu)qk(θv) 

• The function f  when regarded as an integral operator  has ﬁnite spectrum with rank d. That is 

• Fuv := E(cid:2)Yuv | {θw}w∈[n]

where qk are orthonormal L2-integrable basis functions. We assume that there exists some B such
that |qk(y)| ≤ B for all k and y ∈ [0  1].
• For every (unordered) index pair (u  v)  the entry is observed independently with probability p  i.e.
(u  v) ∈ E and Muv = Mvu = Yuv. If (u  v) /∈ E  then Muv = 0.

The data (E  M ) can be viewed as a weighted undirected graph over n vertices with each (u  v) ∈ E
having weights Muv. The goal is to estimate the matrix F = [Fuv]u v∈[n]. Let Λ denote the d × d
diagonal matrix with {λk}k∈[d] as the diagonal entries. Let the eigenvalues be sorted in such a way
that |λ1| ≥ |λ2| ≥ ··· ≥ |λd| > 0. Let Q denote the d × n matrix where Q(k  u) = qk(θu). Since
Q is a random matrix depending on the sampled θ  it is not guaranteed to be an orthonormal matrix
(even though qk are orthonormal functions). By deﬁnition  it follows that F = QT ΛQ. Let d(cid:48) be the
number of distinct valued eigenvalues. Let ˜Λ denote be the d × d(cid:48) matrix where ˜Λ(a  b) = λa−1

.

b

Discussing Assumptions. The latent variable model imposes a natural and mild assumption  as
Aldous and Hoover proved that if the network is exchangeable  i.e. the distribution over edges is
invariant under permutations of vertex labels  then the network can be equivalently represented by a
latent variable model [5  25  7]. Exchangeability is reasonable for anonymized datasets for which
the identity of entities can be easily renamed. Our model additionally requires that the function is
L-Lipschitz and has ﬁnite spectrum when regarded as an integral operator  i.e. F is low rank; this
includes interesting scenarios such as the mixed membership stochastic block model and ﬁnite degree
polynomials. We can also relax the condition to piecewise Lipschitz  as we only need to ensure that
for every vertex u there are sufﬁciently many vertices v which are similar in function value to u. We
assume observations are sampled independently with probability p; however  we discuss a possible
solution for dealing with non-uniform sampling in Section 5.

3 Algorithm

The algorithm that we propose uses the concept of local approximation  ﬁrst determining which
datapoints are similar in value  and then computing neighborhood averages for the ﬁnal estimate. All
similarity-based collaborative ﬁltering methods have the following basic format:

where Euv := {(a  b) ∈ E s.t. dist(u  a) < ηn  dist(v  b) < ηn}.

5

1. Compute distances between pairs of vertices  e.g. 

dist(u  a) ≈(cid:82) 1

2. Form estimate by averaging over “nearby” datapoints 

0 (f (θu  t) − f (θa  t))2dt.
(cid:80)

(a b)∈Euv

Mab 

ˆFuv = 1|Euv|

(1)

(2)

The choice of ηn = Θ(d(c1pn)−2/5) will be small enough to drive the bias to zero  ensuring the
included datapoints are close in value  yet large enough to reduce the variance  ensuring |Euv|
diverges.

(cid:80)

Inutition. Various similarity-based algorithms differ in the distance computation (Step 1). For
dense datasets  i.e. p = ω(n−1/2)  previous works have proposed and analyzed algorithms which
approximate the L2 distance of (1) by using variants of the ﬁnite sample approximation 

(Fuy − Fay)2 

y∈Xua

dist(u  a) = 1|Xua|

(3)
where y ∈ Xua iff (u  y) ∈ E and (a  y) ∈ E [4  44  32]. For sparse datasets  with high probability 
Xua = ∅ for almost all pairs (u  a)  such that this distance cannot be computed.
In this paper we are interested in the sparse setting when p is signiﬁcantly smaller than n−1/2  down
to the lowest threshold of p = ω(n−1). If we visualize the data via a graph with edge set E  then (3)
corresponds to comparing common neighbors of vertices u and a. A natural extension when u and
a have no common neighbors  is to instead compare the r-hop neighbors of u and a  i.e. vertices y
which are at distance exactly r from both u and a. We compare the product of weights along edges in
the path from u to y and a to y respectively  which in expectation approximates

[0 1]r−1 f (θu  t1)((cid:81)r−2

s=1 f (ts  ts+1))f (tr−1  θy)d(cid:126)t =(cid:80)

kqk(θu)qk(θy) = eT

u QT ΛrQey.

k λr

(cid:82)

(4)

We choose a large enough r such that there are sufﬁciently many “common” vertices y which have
paths to both u and a  guaranteeing that our distance can be computed from a sparse dataset.

Algorithm Details. We present and discuss details of each step of the algorithm  which primarily
involves computing pairwise distances (or similarities) between vertices.

Step 1: Sample Splitting. We partition the datapoints into disjoint sets  which are used in different
steps of the computation to minimize correlation across steps for the analysis. Each edge in E is
independently placed into E1  E2  or E3  with probabilities c1  c2  and 1 − c1 − c2 respectively.
Matrices M1  M2  and M3 contain information from the subset of the data in M associated to E1 E2 
and E3 respectively. M1 is used to deﬁne local neighborhoods of each vertex  M2 is used to compute
similarities of these neighborhoods  and M3 is used to average over datapoints for the ﬁnal estimate
in (2).

Step 2: Expanding the Neighborhood. We ﬁrst expand local neighborhoods of radius r around each
vertex. Let Su s denote the set of vertices which are at distance s from vertex u in the graph deﬁned
by edge set E1. Speciﬁcally  i ∈ Su s if the shortest path in G1 = ([n] E1) from u to i has a length
of s. Let Tu denote a breadth-ﬁrst tree in G1 rooted at vertex u. The breadth-ﬁrst property ensures
that the length of the path from u to i within Tu is equal to the length of the shortest path from u
to i in G1. If there is more than one valid breadth-ﬁrst tree rooted at u  choose one uniformly at
random. Let Nu r ∈ [0  1]n denote the following vector with support on the boundary of the r-radius
neighborhood of vertex u (we also call Nu r the neighborhood boundary):
if i ∈ Su r 
if i /∈ Su r 

(cid:40)(cid:81)

(u i) M1(a  b)

(a b)∈pathTu

Nu r(i) =

0

(u  i) denotes the set of edges along the path from u to i in the tree Tu. The sparsity of
where pathTu
Nu r(i) is equal to Su r  and the value of the coordinate Nu r(i) is equal to the product of weights
along the path from u to i. Let ˜Nu r denote the normalized neighborhood boundary such that
˜Nu r = Nu r/|Su r|. We will choose radius r to be r = 6 ln(1/p)
8 ln(c1pn).

Step 3: Computing the distances. For each vertex  we present two variants for estimating the distance.

1. For each pair (u  v)  compute dist1(u  v) according to

(cid:0) 1−c1p

c2p

(cid:1)(cid:0) ˜Nu r − ˜Nv r

(cid:1)T

(cid:0) ˜Nu r+1 − ˜Nv r+1

(cid:1).

M2

6

2. For each pair (u  v)  compute distance according to

dist2(u  v) =(cid:80)
(cid:1)(cid:0) ˜Nu r − ˜Nv r
(cid:1)T

(cid:0) 1−c1p

c2p

i∈[d(cid:48)] zi∆uv(r  i) 

(cid:0) ˜Nu r+i − ˜Nv r+i

(cid:1) 

M2

where ∆uv(r  i) is deﬁned as

and z ∈ Rd(cid:48)
because ˜ΛT is a Vandermonde matrix  and Λ−2r1 lies within the span of its columns.

is a vector that satisﬁes Λ2r+2 ˜ΛT z = Λ21. z always exists and is unique

Computing dist1 does not require knowledge of the spectrum of f. In our analysis we prove that
the expected squared error of the estimate computed in (2) using dist1 converges to zero with n
for p = ω(n−1+) for some  > 0 and constant rank d  i.e. p must be polynomially larger than
n−1. Although computing dist2 requires knowledge of the spectrum of f to determine the vector
z  the expected squared error of the estimate computed in (2) using dist2 conveges to zero for
p = ω(n−1) and constant rank d  which includes the sparser settings when p is only larger than n−1
by polylogarithmic factors. We also will show the dependence on d allowing for it to grow slowly
with pn. It seems plausible that the technique employed by [2] could be used to design a modiﬁed
algorithm which does not need to have prior knowledge of the spectrium. They achieve this for
the stochastic block model case by bootstrapping the algorithm with a method which estimates the
spectrum ﬁrst and then computes pairwise distances with the estimated eigenvalues.

Step 4: Averaging datapoints to produce ﬁnal estimate. The estimate ˆF (u  v) is computed by
averaging over nearby points deﬁned by the distance estimates dist1 (or dist2). Recall that B ≥ 1
was assumed in the model deﬁnition to upper bound supy∈[0 1] |qk(y)|.
Let Euv1 denote the set of undirected edges (a  b) such that (a  b) ∈ E3 and both dist1(u  a) and
dist1(v  b) are less than η1(n) = 33Bd|λ1|2r+1(c1pn)−2/5. The ﬁnal estimate ˆF (u  v) produced
by using dist1 is computed by averaging over the undirected edge set Euv1 

ˆF (u  v) =

1

|Euv1|

(a b)∈Euv1

M3(a  b).

(5)

Let Euv2 denote the set of undirected edges (a  b) such that (a  b) ∈ E3  and both dist2(u  a) and
dist2(v  b) are less than ξ2(n) = 33Bd|λ1|(c1pn)−2/5. The ﬁnal estimate ˆF (u  v) produced by
using dist2 is computed by averaging over the undirected edge set Euv2 

(cid:88)

(cid:88)

ˆF (u  v) =

1

|Euv2|

4 Main Results

(a b)∈Euv2

M3(a  b).

(6)

We prove bounds on the estimation error of our algorithm in terms of the mean squared error (MSE) 

MSE := E(cid:104)
(cid:82) 1
0 (f (θu  y) − f (θv  y))2dy =(cid:80)d

1

n(n−1)

u(cid:54)=v( ˆFuv − Fuv)2(cid:105)
(cid:80)

 

which averages the squared error over all edges. It follows from the model that

k=1 λ2

k(qk(θu) − qk(θv))2 = (cid:107)ΛQ(eu − ev)(cid:107)2
2.

The key part of the analysis is to show that the computed distances are in fact good estimates of
(cid:107)ΛQ(eu − ev)(cid:107)2
2. The analysis essentially relies on showing that the neighborhood growth around a
vertex behaves according to its expectation  according to some properly deﬁned notion. The radius
r must be small enough to guarantee that the growth of the size of the neighborhood boundary
is exponential  increasing at a factor of approximately c1pn. However  if the radius is too small 
then the boundaries of the respective neighborhoods of the two chosen vertices would have a small
intersection  so that estimating the similarities based on the small intersection of datapoints would

7

(cid:16) ln(1/p)

(cid:17)

(cid:16) ln(1/c1p)

(cid:17)

result in high variance. Therefore  the choice of r is critical to the algorithm and analysis. We are
able to prove bounds on the squared error when r is chosen to satisfy the following conditions:

≥

1
2

r + d(cid:48) ≤ 7 ln(1/c1p)

6 ln(1/p)

.

 

r +

ln(c1pn)

ln(c1pn)

8 ln(9c1pn/8) = Θ

8 ln(7|λd|2c1pn/8|λ1|) = Θ

(7)
The parameter d(cid:48) denotes the number of distinct valued eigenvalues in the spectrum of f  (λ1 . . . λd) 
and determines the number of different radius “measurements” involved in computing dist2(u  v).
Computing dist1(u  v) only involves a single measurement  thus the left hand side of (7) can be
reduced to r + 1 instead of r + d(cid:48). When p is above a threshold  we choose c1 to decrease with n to
ensure (7) can be satisﬁed  sparsifying the edge set E1 used for expanding the neighborhood around
a vertex . When the sample probability is polynomially larger than n−1  i.e. p = n−1+ for some
 > 0  these constraints imply that r is a constant with respect to n. However  if p = ˜O(n−1)  we
will need r to grow with n according to a rate of 6 ln(1/p)/8 ln(c1pn).
Theorem 4.1. If p = n−1+ for some  ∈ (0  1
Θ
p = ω(n−1d5) and |λd| = ω((c1pn)− 1
eter r achieves

6 )  with a choice of c1 such that c1pn =
If
4 )  then the estimate computed using dist1 with param-

  there exists a constant r (with respect to n) which satisﬁes (7).

max(pn  (p6n7) 1

(cid:16)

(cid:17)

19 )

(cid:32)(cid:18)|λ1|

|λd|

MSE = O

(cid:33)
(cid:19)2r B3d2|λ1|
(cid:16)
(cid:1)(cid:17)
d exp(cid:0) − (c1pn)1/5
(cid:19)1/2(cid:33)
(cid:32)(cid:18)|λ1|
(cid:19)r(cid:18) B3d2|λ1|

(c1pn)2/5

9B2d

.

|λd|

(c1pn)2/5

.

If p = ω(n−1d5 ln5(n))  with probability greater than 1 − O
satisﬁes

(cid:107) ˆF − F(cid:107)max := max

i j

| ˆFij − Fij| = O

  the estimate

Theorem 4.1 proves that the mean squared error (MSE) of the estimate computed with dist1
is bounded by O((|λ1|/|λd|)2rd2(c1pn)−2/5). Therefore  our algorithm with dist1 provides a
consistent estimate when r is constant with respect to n  which occurs for p = n−1+ for some
 > 0. In fact  the reason why the error blows up with a factor of (|λ1|/|λd|)−2r is because we
compute the distance by summing product of weights over paths of length 2r. From (4)  we see
that in expectation  when we take the product of edge weights over a path of length r from u to y 
instead of computing f (θu  θy) = eT
u QΛrQey  which
contains extra factors of Λr−1. Therefore  by computing over a radius r  the calculation in dist1 will
approximate (cid:107)Λr+1Q(eu − ev)(cid:107)2
2  thus leading to an error
factor of (|λ1|/|λd|)2r. It turns out that dist2 adjusts for this bias  as the multiple measurements
∆uv(r  i) with different length paths allows us to separate out ekΛQ(eu − ev) for all k with distinct
values of λk.
Theorem 4.2. If p = o(n−5/6)  with a choice of c1 such that c1pn = Θ
there exists a value for r which satisﬁes (7). If p = ω(n−1d5)  |λd| = ω((c1pn)− 1
then the estimate computed using dist2 with parameter r achieves

2 rather than our intended (cid:107)ΛQ(eu − ev)(cid:107)2
(cid:16)

u QΛQey  the expression concentrates around eT

 
4 )  and d = o(r) 

max(pn  (p6n7)

(8d(cid:48)+11) )

(cid:17)

1

MSE = O

If p = ω(n−1d5 ln5(n))  with probability 1 − O

(cid:107) ˆF − F(cid:107)max := max

i j

| ˆFij − Fij| = O

.

(cid:19)

(c1pn)2/5

(cid:18) B3d2|λ1|
(cid:1)(cid:17)
(cid:16)
d exp(cid:0) − (c1pn)1/5
(cid:32)(cid:18) B3d2|λ1|

9B2d

(c1pn)2/5

  the estimate satisﬁes

(cid:19)1/2(cid:33)

.

Theorem 4.2 proves that the mean squared error (MSE) of the estimate computed using dist2 is
bounded by O(d2(c1pn)−2/5); and thus the estimate is consistent in the ultra sparse sampling regime
of p = ω(d5n−1).

8

5 Discussion

In this work we presented a similarity based collaborative ﬁltering algorithm which is provably
consistent in sparse sampling regimes  as long as the sample probability p = ω(n−1). The algorithm
computes similarity between two users by comparing their local neighborhoods. Our model assumes
that the data matrix is generated according to a latent variable model  in which the weight on an
observed edge (u  v) is equal in expectation to a function f over associated latent variables θu and θv.
We presented two variants for computing similarities (or distances) between vertices. Computing
dist1 does not require knowledge of the spectrum of f  but the estimate requires p to be polynomially
larger than n in order to guarantee the expected squared error converges to zero. Computing dist2
uses the knowledge of the spectrum of f  but it provides an estimate that is provably consistent
for a signiﬁcantly sparse regime  only requiring that p = ω(n−1). The mean squared error of both
algorithms is bounded by O((pn)−1/5). Since the computation is based on of comparing local
neighborhoods within the graph  the algorithm can be easily implemented for large scale datasets
where the data may be stored in a distributed fashion optimized for local graph computations.

Practical implementation. In practice  we do not know the model parameters  and we would use
cross validation to tune the radius r and threshold ηn. If r is either too small or too large  then the
vector Nu r will be too sparse. The threshold ηn trades off between bias and variance of the ﬁnal
estimate. Since we do not know the spectrum  dist1 may be easier to compute  and still enjoys good
properties as long as r is not too large. When the sampled observations are not uniform across entries 
the algorithm may require more modiﬁcations to properly normalize for high degree hub vertices  as
the optimal choice of r may differ depending on the local sparsity. The key computational step of
our algorithm involves comparing the expanded local neighborhoods of pairs of vertices to ﬁnd the
“nearest neighbors”. The local neighborhoods can be computed in parallel  as they are independent
computations. Furthermore  the local neighborhood computations are suitable for systems in which
the data is distributed across different machines in a way that optimizes local neighborhood queries.
The most expensive part of our algorithm involves computing similarities for all pairs of vertices in
order to determine the set of nearest neighbors. However  it would be possible to use approximate
nearest neighbor techniques to greatly reduce the computation such that approximate nearest neighbor
sets could be computed with signiﬁcantly fewer than n2 pairwise comparisons.

Non-uniform sampling. In reality  the probability that entries are observed is not be uniform across
all pairs (i  j). However  we believe that an extension of our result can also handle variations in
the sample probability as long as the sample probability is a function of the latent variables and
scales in the same way with respect to n across all entries. Suppose that the probability of observing
(i  j) is given by pg(θi  θj)  where p is the scaling factor (contains the dependence upon n)  and g
allows for constant factor variations in the sample probability across entries as a function of the latent
variables. If we let matrix X indicate the presence of an observation or not  then we can apply our
algorithm twice  ﬁrst on matrix X to estimate function g  and then on data matrix M to estimate f
times g. We can simply divide by the estimate for g to obtain the estimate for f. The limitation is that
if g(θi  θj) is very small  then the error in estimating the corresponding f (θi  θj) will have higher
variance. However  it is expected that error increases for edge types with fewer samples.

Acknowledgments

This work is supported in parts by NSF under grants CMMI-1462158 and CMMI-1634259  by
DARPA under grant W911NF-16-1-0551  and additionally by a NSF Graduate Fellowship and
Claude E. Shannon Research Assistantship.

References
[1] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
Fundamental limits and efﬁcient algorithms for recovery. In Foundations of Computer Science
(FOCS)  2015 IEEE 56th Annual Symposium on  pages 670–688. IEEE  2015.

[2] Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic block
model without knowing the parameters. In Advances in neural information processing systems 
2015.

9

[3] Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple
clusters: proof of the achievability conjectures  acyclic bp  and the information-computation
gap. Advances in neural information processing systems  2016.

[4] Edo M Airoldi  Thiago B Costa  and Stanley H Chan. Stochastic blockmodel approximation of
a graphon: Theory and consistent estimation. In Advances in Neural Information Processing
Systems  pages 692–700  2013.

[5] D.J. Aldous. Representations for partially exchangeable arrays of random variables. J. Multi-

variate Anal.  11:581 – 598  1981.

[6] Animashree Anandkumar  Rong Ge  Daniel Hsu  and Sham Kakade. A tensor spectral approach
to learning mixed membership community models. In Conference on Learning Theory  pages
867–881  2013.

[7] Tim Austin. Exchangeable random arrays. Technical Report  Notes for IAS workshop.  2012.

[8] Charles Bordenave  Marc Lelarge  and Laurent Massoulié. Non-backtracking spectrum of
random graphs: community detection and non-regular ramanujan graphs. In Foundations of
Computer Science (FOCS)  2015 IEEE 56th Annual Symposium on  pages 1347–1357. IEEE 
2015.

[9] Christian Borgs  Jennifer Chayes  and Adam Smith. Private graphon estimation for sparse

graphs. In Advances in Neural Information Processing Systems  pages 1369–1377  2015.

[10] Christian Borgs  Jennifer T Chayes  Henry Cohn  and Shirshendu Ganguly. Consistent nonpara-

metric estimation for heavy-tailed sparse graphs. arXiv preprint arXiv:1508.06675  2015.

[11] Christian Borgs  Jennifer T Chayes  Henry Cohn  and Nina Holden. Sparse exchangeable graphs

and their limits via graphon processes. arXiv preprint arXiv:1601.07134  2016.

[12] Christian Borgs  Jennifer T Chayes  Henry Cohn  and Yufei Zhao. An Lp theory of sparse graph
convergence I: limits  sparse random graph models  and power law distributions. arXiv preprint
arXiv:1401.2906  2014.

[13] Christian Borgs  Jennifer T Chayes  Henry Cohn  and Yufei Zhao. An Lp theory of sparse
graph convergence II: Ld convergence  quotients  and right convergence. arXiv preprint
arXiv:1408.0744  2014.

[14] Christian Borgs  Jennifer T Chayes  László Lovász  Vera T Sós  and Katalin Vesztergombi.
Convergent sequences of dense graphs I: Subgraph frequencies  metric properties and testing.
Advances in Mathematics  219(6):1801–1851  2008.

[15] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.

Communications of the ACM  55(6):111–119  2009.

[16] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE 

98(6):925–936  2010.

[17] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix

completion. IEEE Transactions on Information Theory  56(5):2053–2080  2010.

[18] Sourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of

Statistics  43(1):177–214  2015.

[19] Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:

General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025  2015.

[20] Mark A Davenport  Yaniv Plan  Ewout van den Berg  and Mary Wootters. 1-bit matrix

completion. Information and Inference  3(3):189–223  2014.

[21] Aurelien Decelle  Florent Krzakala  Cristopher Moore  and Lenka Zdeborová. Asymptotic
analysis of the stochastic block model for modular networks and its algorithmic applications.
Phys. Rev. E  84:066106  Dec 2011.

10

[22] Persi Diaconis and Svante Janson. Graph limits and exchangeable random graphs. Rendiconti

di Matematica  VII(28):33–61  2008.

[23] Chao Gao  Yu Lu  and Harrison H Zhou. Rate-optimal graphon estimation. The Annals of

Statistics  43(6):2624–2652  2015.

[24] David Goldberg  David Nichols  Brian M. Oki  and Douglas Terry. Using collaborative ﬁltering

to weave an information tapestry. Commun. ACM  1992.

[25] D.N. Hoover. Row-column exchangeability and a generalized model for probability.

Exchangeability in Probability and Statistics (Rome  1981)  pages 281 – 291  1981.

In

[26] Brian Karrer  M. E. J. Newman  and Lenka Zdeborová. Percolation on sparse networks. Phys.

Rev. Lett.  113:208702  Nov 2014.

[27] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a

few entries. IEEE Transactions on Information Theory  56(6):2980–2998  2010.

[28] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from noisy

entries. Journal of Machine Learning Research  11(Jul):2057–2078  2010.

[29] Olga Klopp  Alexandre B Tsybakov  and Nicolas Verzelen. Oracle inequalities for network

models and sparse graphon estimation. To appear in Annals of Statistics  2015.

[30] Yehuda Koren and Robert Bell. Advances in collaborative ﬁltering. In Recommender Systems

Handbook  pages 145–186. Springer US  2011.

[31] Florent Krzakala  Cristopher Moore  Elchanan Mossel  Joe Neeman  Allan Sly  Lenka Zde-
borová  and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the
National Academy of Sciences  110(52):20935–20940  2013.

[32] Christina E. Lee  Yihua Li  Devavrat Shah  and Dogyoon Song. Blind regression: Nonpara-
metric regression for latent variable models via collaborative ﬁltering. In Advances in Neural
Information Processing Systems 29  pages 2155–2163  2016.

[33] Greg Linden  Brent Smith  and Jeremy York. Amazon.com recommendations: Item-to-item

collaborative ﬁltering. IEEE Internet Computing  7(1):76–80  2003.

[34] László Lovász. Large networks and graph limits  volume 60. American Mathematical Society

Providence  2012.

[35] Laurent Massoulié. Community detection thresholds and the weak ramanujan property. In
Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing  STOC ’14 
pages 694–703  New York  NY  USA  2014. ACM.

[36] Elchanan Mossel  Joe Neeman  and Allan Sly. A proof of the block model threshold conjecture.

Combinatorica  Aug 2017.

[37] Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise

and high-dimensional scaling. The Annals of Statistics  pages 1069–1097  2011.

[38] Xia Ning  Christian Desrosiers  and George Karypis. Recommender Systems Handbook  chapter
A Comprehensive Survey of Neighborhood-Based Recommendation Methods  pages 37–76.
Springer US  2015.

[39] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning

Research  12(Dec):3413–3430  2011.

[40] David Steurer and Sam Hopkins. Bayesian estimation from few samples: community detection

and related problems. https://arxiv.org/abs/1710.00264  2017.

[41] Victor Veitch and Daniel M Roy. The class of random graphs arising from exchangeable random

measures. arXiv preprint arXiv:1512.03099  2015.

11

[42] Patrick J Wolfe and Soﬁa C Olhede. Nonparametric graphon estimation. arXiv preprint

arXiv:1309.5936  2013.

[43] Jiaming Xu  Laurent Massoulié  and Marc Lelarge. Edge label inference in generalized
In Conference on

stochastic block models: from spectral theory to impossibility results.
Learning Theory  pages 903–920  2014.

[44] Yuan Zhang  Elizaveta Levina  and Ji Zhu. Estimating network edge probabilities by neighbor-

hood smoothing. arXiv preprint arXiv:1509.08588  2015.

12

,Christian Borgs
Jennifer Chayes
Christina Lee
Devavrat Shah