2009,Convex Relaxation of Mixture Regression with Efficient Algorithms,We develop a convex relaxation of maximum a posteriori estimation of a mixture of regression models. Although our relaxation involves a semidefinite matrix variable  we reformulate the problem to eliminate the need for general semidefinite programming. In particular  we provide two reformulations that admit fast algorithms. The first is a max-min spectral reformulation exploiting quasi-Newton descent. The second is a min-min reformulation consisting of fast alternating steps of closed-form updates. We evaluate the methods against Expectation-Maximization in a real problem of motion segmentation from video data.,Convex Relaxation of Mixture Regression with

Efﬁcient Algorithms

Novi Quadrianto  Tib´erio S. Caetano  John Lim

NICTA - Australian National University
{ﬁrstname.lastname}@nicta.com.au

Canberra  Australia

Dale Schuurmans
University of Alberta
Edmonton  Canada
dale@cs.ualberta.ca

Abstract

We develop a convex relaxation of maximum a posteriori estimation of a mixture
of regression models. Although our relaxation involves a semideﬁnite matrix vari-
able  we reformulate the problem to eliminate the need for general semideﬁnite
programming. In particular  we provide two reformulations that admit fast algo-
rithms. The ﬁrst is a max-min spectral reformulation exploiting quasi-Newton de-
scent. The second is a min-min reformulation consisting of fast alternating steps of
closed-form updates. We evaluate the methods against Expectation-Maximization
in a real problem of motion segmentation from video data.

Introduction

1
Regression is a foundational problem in machine learning and statistics. In practice  however  data
is often better modeled by a mixture of regressors  as demonstrated by the prominence of mixture
regression in a number of application areas. Gaffney and Smyth [1]  for example  use mixture regres-
sion to cluster trajectories  i.e. sets of short sequences of data such as cyclone or object movements
in video sequences as a function of time. Each trajectory is believed to have been generated from one
of a number of components  where each component is associated with a regression model. Finney et
al. [2] have employed an identical mixture regression model in the context of planning: regression
functions are strategies for a given planning problem. Elsewhere  the mixture of regressors model
has been shown to be useful in addressing covariate shift  i.e. the situation where the distribution of
the training set used for modeling does not match the distribution of the test set in which the model
will be used. Storkey and Sugiyama [3] model the covariate shift process in a mixture regression
setting by assuming a shift in the mixing proportions of the components.
In each of these problems  one must estimate k distinct latent regression functions; that is  estimate
functions whose values correspond to the mean of response variables  under the assumption that
the response variable is generated by a mixture of k components. This estimation problem can
be easily tackled if it is known to which component each response variable belongs (yielding k
independent regression problems). However in general the component of a given observation is not
known and is modeled as a latent variable. A commonly adopted approach for maximum-likelihood
estimation with latent variables (in this case  component membership for each response variable) is
Expectation-Maximization (EM) [4]. Essentially  EM iterates inference over the hidden variables
and parameter estimation of the resulting decoupled models until a local optimum is reached. We
are not aware of any approach to maximum likelihood estimation of a mixture of regression models
that is not based on the non-convex marginal likelihood objective of EM.
In this paper we present a convex relaxation of maximum a posteriori estimation of a mixture of re-
gression models. Recently  convex relaxations have gained considerable attention in machine learn-
ing (c.f. [5  6]). By exploiting convex duality  we reformulate a relaxation of mixture regression as
a semideﬁnite program. To achieve a scalable approach  however  we propose two reformulations
that admit fast algorithms. The ﬁrst is a max-min optimization problem which can be solved by iter-
ations of quasi-Newton steps and eigenvector computations. The second is a min-min optimization
problem solvable by iterations of closed-form solutions. We present experimental results comparing
our methods against EM  both in synthetic problems and real computer vision problems  and show
some beneﬁts of a convex approach over a local solution method.

1

Related work Goldfeld and Quandt [7] introduced a mixture regression model with two components
called switching regressions. The problem is re-cast into a single composite regression equation by
introducing a switching variable. A consistent estimator is then produced by a continuous relaxation
of this switching variable. An EM algorithm for switching regressions was ﬁrst presented by Hos-
mer [8]. Sp¨ath [9] introduced a problem called clusterwise linear regression  consisting of ﬁnding a
k-partition of the data such that a least squares regression criterion within those partitions becomes
a minimum. A non-probabilistic algorithm similar to k-means was proposed. Subsequently  the
general k-partition case employing EM was developed (c.f. [10  11  1]) and extended to various
situations including the use of variable length trajectory data and to non-parametric regression mod-
els. In the extreme  each individual could have its speciﬁc regression model but coupled at higher
level with a mixture on regression parameters [12]. An EM algorithm is again employed to handle
hidden data  in this case group membership of parameters. The Hierarchical Mixtures of Experts
[13] model also shares some similarity to mixture regression in that gating networks which contain
mixtures of generalized linear models are deﬁned. In principle  our algorithmic advances can be
applied to many of these formulations.

2 The Model
Notation In the following we use the uppercase letters (X  Π  Ψ) to denote matrices and the low-
ercase letters (x  y  w  π  ψ  c) to denote vectors. We use t to denote the sample size  n to denote
the dimensionality of the data and k to denote the number of mixture components. Λ(a) denotes a
diagonal matrix whose diagonal is equal to vector a  and diag(A) is a vector equal to the diagonal
of matrix A. Finally  we let 1 denote the vector of all ones  use (cid:12) to denote Hadamard (component-
wise) matrix product  and use ⊗ to denote Kronecker product.
We are given a matrix of regressors X ∈ Rt×n and a vector of regressands y ∈ Rt×1 where the re-
sponse variable y is generated by a mixture of k components  but we do not know which component
of the mixture generates each response yi. We therefore use the matrix Π ∈ {0  1}t×k  Π1 = 1 
to denote the hidden assignment of mixture labels to each observation: Πij = 1 iff observation i
has mixture label j. We use xi to denote the ith row of X (i.e. observation i as a row vector)  πi to
denote the ith row of Π and yi to denote the ith element of y. We assume a linear generative model
for yi on a feature representation ψi = πi ⊗ xi  under i.i.d. sampling
i ∼ N(0  σ2) 

(1)
where w ∈ R(n×k)×1 is the vector of stacked parameter vectors of the components. We therefore
have the likelihood

yi|xi  πi = ψiw + i 

p(yi|xi  πi; w) =

1√
2πσ2

exp

(cid:20)
− 1
2σ2 (ψiw − yi)2

(cid:21)

(2)

for a single observation i (recalling that ψi depends on both xi and πi). We further impose a Gaussian
prior on w for capacity control. Also  one may want to constrain the size of the largest mixture
component. For that purpose one could constrain the solutions Π such that max(diag(ΠT Π)) ≤ γt 
where γt is an upper bound on the size of the largest component (γ is an upper bound on the
proportion of the largest component). Combining these assumptions and adopting matrix notation
we obtain the optimization problem: minimize the negative log-posterior of the entire sample

min
Π w

A(ψi  w) − 1

σ2 yT Ψw +

1
2σ2 yT y + α
log(2πσ2).

2 wT w

1
2σ2 wT ψT

1
2

A(ψi  w) =

(4)
Here Ψ is the matrix whose rows are the vectors ψi = πi ⊗ xi. Since X is observed  note that the
optimization only runs over Π in Ψ. The constraint max(diag(ΠT Π)) ≤ γt may also be added.
Eliminating constant terms  our ﬁnal task will be to solve

i ψiw +

  where

(3)

(cid:34)(cid:88)

i

σ2 yT Ψw + α

2 wT w

.

(5)

Although marginally convex on w  this objective is not jointly convex on w and Π (and involves
non-convex constraints on Π owing to its discreteness). The lack of joint convexity makes the opti-
mization difﬁcult. The typical approach in such situations is to use an alternating descent strategy 
such as EM. Instead  in the following we develop a convex relaxation for problem (5).

2

(cid:35)

(cid:21)

(cid:20) 1
2σ2 wT ΨT Ψw − 1

min
Π w

3 Semideﬁnite Relaxation
To obtain a convex relaxation we proceed in three steps. First  we dualize the ﬁrst term in (5).
2σ2 wT ΨT Ψw. Then the Fenchel dual of A(Ψw) is A∗(c) = 1

Lemma 1 Deﬁne A(Ψw) := 1
and therefore A(Ψw) = maxc cT Ψw − 1
Proof From the deﬁnition of Fenchel dual we have A∗(u) := maxw uT w − 1
ferentiating with respect to w and equating to zero we obtain u = 1
realizable if there exists a c such that u = ΨT c. Solving for A∗(c) we obtain A∗(c) = 1
therefore by deﬁnition of Fenchel duality A(Ψw) = maxc cT Ψw − 1
A second Lemma is required to further establish the relaxation:

2σ2 wT ΨT Ψw. Dif-
σ2 ΨT Ψw. Therefore u is only
2 σ2cT c  and

2 σ2cT c.

2 σ2cT c 

2 σ2cT c.

Lemma 2 The following set inclusion holds

{ΠΠT : Π ∈ {0  1}t×k  Π1 = 1  max(diag(ΠT Π)) ≤ γt}

⊆ {M : M ∈ Rt×t  tr M = t  γtI (cid:60) M (cid:60) 0}.

(6)
(7)
Proof Let ΠΠT be an element of the ﬁrst set. First notice that [ΠΠT ]ij ∈ {0  1} since Π ∈ {0  1}t×k
and Π1 = 1 together imply that Π has a single 1 per row (and the rest are zeros). In particular
[ΠΠT ]ii = 1 for all i  i.e. tr M = t. Finally  note that (ΠΠT )Π = Π(ΠT Π) where ΠT Π is a
diagonal matrix and therefore its diagonal elements are the eigenvalues of ΠΠT and in particular
max(diag(ΠT Π)) ≤ γt means that the largest possible eigenvalue of ΠΠT is γt  which implies
γtI (cid:60) ΠΠT . Since ΠΠT is by construction positive semideﬁnite  we have γtI (cid:60) ΠΠT (cid:60) 0.
Therefore ΠΠT is also a member of the second set.
The above two lemmas allow us to state our ﬁrst main result below.

Theorem 3 The following convex optimization problem

min

M :tr M =t γtI(cid:60)M(cid:60)0

max

c

is a relaxation of (5) only in the sense that domain (6) is replaced by domain (7).

Proof We ﬁrst use Lemma 1 in order to rewrite the objective (5) and obtain

(cid:20)
2 σ2cT c − 1
−1
(cid:19)

2α

cT Ψw − 1
(cid:20)
cT Ψw − 1

(cid:20)(cid:18)

min
Π w

max

c

min
Π

max

c

min
w

(cid:17)T
(cid:16) y
σ2 − c

(cid:17)(cid:21)
M (cid:12) XX T(cid:16) y
σ2 − c
(cid:21)
(cid:21)

2 wT w

.

2 σ2cT c

− 1
σ2 yT Ψw + α

Second  using the distributivity of the (max  +) semi-ring  the maxc can be pulled out and we then
use Sion’s minimax theorem [14]  which allows us to interchange maxc with minw

(8)

(9)

(10)

(11)

(12)

and we can solve for w ﬁrst  obtaining

Substituting (11) in the objective of (10) results in

.

 

2 wT w

2 σ2cT c − 1
σ2 yT Ψw + α
ΨT(cid:16) y
(cid:17)
σ2 − c
(cid:16) y
(cid:17)T
σ2 − c
(cid:17)T

ΨΨT(cid:16) y
σ2 − c
ΠΠT (cid:12) XX T(cid:16) y
σ2 − c

(cid:17)(cid:21)

.

w =

1
α

(cid:20)
2 σ2cT c − 1
−1
(cid:16) y
σ2 − c

(cid:20)
2 σ2cT c − 1
−1

2α

2α

(cid:17)(cid:21)

min
Π

max

c

min
ΠΠT

max

c

We now note the critical fact that Ψ only shows up in the expression ΨΨT which  from the deﬁnition
ψi = πi⊗xi  is seen to be equivalent to ΠΠT (cid:12)XX T . Therefore the minimization over Π effectively
takes place over ΠΠT (since X is observed)  and we have that (12) can be rewritten as

.

(13)

So far no relaxation has taken place. By ﬁnally replacing the constraint (6) with constraint (7) from
Lemma 2  we obtain the claimed semideﬁnite relaxation.

3

4 Max-Min Reformulation
By upper bounding the inner maximization in (8) and applying a Schur complement  problem (8)
can be re-expressed as a semideﬁnite program. Unfortunately  such a formulation is computationally
expensive to solve  requiring O(t6) for typical interior-point methods. Instead  we can reformulate
problem (8) to allow for a fast algorithmic approach  without the introduction of any additional
relaxation. The basis of our development is the following classical result.
Theorem 4 ([15]) Let V ∈ Rt×t  V = V T have eigenvalues λ1 ≥ λ2 ≥ ··· ≥ λt. Let P be the
matrix whose columns are the normalized eigenvectors of V  i.e. P T V P = Λ((λ1  . . .   λt)). Let
q ∈ {1  . . .   t} and Pq be the matrix comprised by the top q eigenvectors of P . Then

q(cid:88)

max

M :tr(M )=q I(cid:60)M(cid:60)0

argmax

M :tr(M )=q I(cid:60)M(cid:60)0

tr M V T =
λi
tr M V T (cid:51) PqP T
q .

i=1

and

(14)

(15)

Proof See [15] for a proof of a slightly more general result (Theorem 3.4).
We will now show how the optimization on M for problem (8) can be cast in the terms of Theorem
4. This will turn out to be critical for the efﬁciency of the optimization procedure  since Theorem
4 describes a purely spectral optimization routine  which is far more efﬁcient (O(t3)) than standard
interior-point methods used for semideﬁnite programming (O(t6)).

(cid:20)
Proposition 5 Deﬁne ¯y := y
2 σ2cT c − 1
−1

max

c

2α

is equivalent to optimization problem (8).

σ2 . The following optimization problem

max

M :tr M =t γtI(cid:60)M(cid:60)0

tr(M(XX T (cid:12) (¯y − c)(¯y − c)T ))

Proof By Sion’s minimax theorem [14]  minM and maxc in (8) can be interchanged
(¯y − c)T M (cid:12) XX T (¯y − c)

max

min

2 σ2cT c − 1
−1

2α

M :tr M =t γtI(cid:60)M(cid:60)0

c

(cid:20)

which  by distributivity of the (min  +) semi-ring  is equivalent to

(cid:21)

(16)

(17)

(cid:21)
(cid:21)

(cid:20)
−1
2 σ2cT c +

max

c

1
2α

min

M :tr M =t γtI(cid:60)M(cid:60)0

− (¯y − c)T M (cid:12) XX T (¯y − c)

.

(18)

Now  deﬁne K := XX T . The objective of the minimization in (18) can then be written as

− (¯y − c)T (M (cid:12) K)(¯y − c) = − tr(cid:0)(M (cid:12) K)(cid:2)(¯y − c)(¯y − c)T(cid:3)(cid:1)
= −(cid:88)

(MijKij)(cid:2)(¯y − c)(¯y − c)T(cid:3)

ij = −(cid:88)

(cid:2)(¯y − c)(¯y − c)T(cid:3)

(cid:16)

Mij

Kij

(cid:17)

ij

ij

ij

= − tr(M(K (cid:12) (¯y − c)(¯y − c)T )) = − tr(M(XX T (cid:12) (¯y − c)(¯y − c)T )).

(19)

(20)

(21)

Finally  by writing minM −f(M) as − maxM f(M)  we obtain the claim.
We can now exploit the result in Theorem 4 for the purpose of our optimization problem.
Proposition 6 Let q = {u : u = max{1  . . .   t}  u ≤ γ−1}. The following optimization problem

(cid:21)

max

¯M :tr ¯M =q I(cid:60) ¯M(cid:60)0

tr( ¯M(XX T (cid:12) (¯y − c)(¯y − c)T ))

(22)

(cid:20)
−1
2 σ2cT c − t
2αq

max

c

is equivalent to optimization problem (16).

4

Algorithm 1
1: Input: γ  σ  α  XX T
2: Output: (c∗  M∗)
3: Initialize c = 0
4: repeat
Solve for maximum value in inner maximization of (22) using (14)
5:
Solve outer maximization in (22) using nonsmooth BFGS [16]  obtain new c
6:
7: until c has converged (c = c∗)
8: At c∗  solve for the maximizer(s) Pq in the inner maximization of (22) using (15)
9: if Pq is unique then
10:
11: else
12:
13:
14:
15: end if

Assemble top l eigenvectors in Pl
Solve (24)
return M∗ = PlΛ(λ∗)P T

return M∗ = PqP T

q break

l

Proof The only differences between (16) and (22) are (i) the factor t/q in the second term of (22) and
(ii) the constraints {M : tr M = t  γtI (cid:60) M (cid:60) 0} in (16) versus {M : tr M = q  I (cid:60) M (cid:60) 0} in
(22). These differences are simply the result of a proper rescaling of M. If we deﬁne ¯M := (q/t)M 
then I (cid:60) ¯M (cid:60) 0 since q ≤ γ−1. We then have tr ¯M = q. The result follows.
And ﬁnally we have the second main result
Theorem 7 Optimization problem (22) is equivalent to optimization problem (8).
Proof The equivalence follows directly from Propositions 5 and 6.
Note that  crucially  the objective in (22) is concave in c. Our strategy is now clear. Instead of solving
(8)  which demands O(t6) operations  we instead solve (22)  which has as inner optimization a max
eigenvalue problem  demanding only O(t3) operations. In the next section we describe an algorithm
to jointly optimize for M and c in (22)  which will essentially consist of alternating the efﬁcient
spectral solution over M with a subgradient optimization over c.

4.1 Max-Min Algorithm
Algorithm 1 describes how we solve optimization problem (22). The idea of the algorithm is the
following. First  having noted that (22) is concave in c  we can simply initialize c arbitrarily and
pursue a fast subgradient ascent algorithm (e.g. such as nonsmooth BFGS [16]). So at each step
we solve the eigenvalue problem and recompute a subgradient  until convergence to c∗. We then
need to recover M∗ such that (c∗  M∗) is a saddle point (note that problem (22) is concave in c
and convex in M). For that purpose we use (15). If M∗ = PqP T
q is such that Pq is unique  then
we are done and the labeling solution of mixture membership is M∗ (subject to roundoff). If Pq
is not unique  then we have multiplicity of eigenvalues and we need to proceed as follows. Deﬁne
Pl = [p1 . . . pq . . . pl]  l > q  where each of the additional l − q eigenvectors has an associated
eigenvalue which is equal to the eigenvalue of some of the previous q eigenvectors. We then have
that at the saddle point there must exist a diagonal matrix Λ such that M∗ = PlΛP T
  subject to
Λ (cid:60) 0 and tr Λ = q (if this were not the case there would be an ascent direction in c∗  contradicting
the hypothesis that c∗ is optimal). To ﬁnd such a Λ and therefore recover the correct M  we need to
enforce that we are at the optimal c (c∗)  i.e. we must have

l

(cid:13)(cid:13)(cid:13)(cid:13) d

dc

(cid:20)
−1
2 σ2cT c − q
2αt

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

2

tr(M(XX T (cid:12) (¯y − c)(¯y − c)T ))

= 0

(23)

max

M :tr M =q I(cid:60)M(cid:60)0

(cid:13)(cid:13)(cid:13)σ2c∗ + q

(cid:0)PlΛ(λ)P T

(cid:13)(cid:13)(cid:13)2
l (cid:12) XX T(cid:1) (c∗ − ¯y)

Such condition can be pursued by minimizing the above norm  which gives a quadratic program

min

λ≥0 λT 1=q

(24)
We can then recover the ﬁnal solution (subject to roundoff) by M∗ = PlΛ(λ∗)P T
  where λ∗ is the
optimizer of (24). The optimal value of (24) should be very close to zero (since it’s the norm of the
derivative at point c∗). The pseudocode for the algorithm appears in Algorithm 1.

αt

2

l

5

Algorithm 2
1: Input: γ  σ  α  XX T
2: Output: (c∗  M∗)
3: Initialize M = Λ((1/(γt)  . . .   1/(γt)))
4: repeat
5:
6:
7: until M has converged (M = M∗)
8: Recover c∗ = −1

σ2 diag(X(A∗)T )

Solve for minimum value in inner minimization of (25)  obtain A
Solve outer minimization in (25) given SVD of A using Theorem 4.1 of [18]  obtain new M

5 Min-Min Reformulation
Although the max-min formulation appears satisfactory  the recent literature on multitask learn-
ing [17  18] has developed an alternate strategy for bypassing general semideﬁnite programming.
Speciﬁcally  work in this area lead to convex optimization problems expressed jointly over two ma-
trix variables where each step is an alternating min-min descent that can be executed in closed-form
or by a very fast algorithm. Although it is not immediately apparent that this algorithmic strategy
is applicable to the problem at hand  with some further reformulation of (8) we discover that in fact
the same min-min algorithmic approach can be applied to our mixture of regression problem.

Theorem 8 The following optimization problem
σ2 yT diag(XAT ) +

{M :I(cid:23)M(cid:23)0 tr M =1/γ}

min
A

min

(cid:20) 1

1
2σ2 diag(XAT )T diag(XAT ) + α
2γt

tr(AT M−1A)

(cid:21)

(25)

(26)

(27)

is equivalent to optimization problem (8).

Proof

min

{M :I(cid:23)M(cid:23)0 tr M =1/γ}

max

c

=

min

{M :I(cid:23)M(cid:23)0 tr M =1/γ}

{c C:C=Λ(c−¯y)X}

tr(C T M C)

(c − ¯y)T (M (cid:12) XX T )(c − ¯y)

− σ2
2 cT c − γt
2α
− σ2
2 cT c − γt
2α

max

=

min

{M :I(cid:23)M(cid:23)0 tr M =1/γ}

− σ2
2 cT c − γt
2α
We can then solve for c and C  obtaining c = − 1
those two variables into (28) proves the claim.

max
c C

min
A

tr(C T M C) + tr(AT C) − tr(AT Λ(c − ¯y)X)
(28)
γt M−1A. Substituting

σ2 diag(XAT ) and C = α

5.1 Min-Min Algorithm

The problem (25) is jointly convex in A and M [14] and Algorithm 2 describes how to solve it.
It is important to note that although each iteration in Algorithm 2 is efﬁcient  many iterations are
required to reach a desired tolerance  since it is only ﬁrst-order convergent. It is observed in our
experiments that the concave-convex max-min approach in Algorithm 1 is more efﬁcient simply
because it has the same iteration cost but exploits a quasi-Newton descent in the outer optimization 
which converges faster.

Remark 9 In practice  similarly to [17]  a regularizer on M is added to avoid singularity  resulting
in the following regularized objective function 

min

{M :I(cid:23)M(cid:23)0 tr M =1/γ}

1
σ2 yT diag(XAT ) +
tr(AT M−1A) +  tr(M−1).

1
2σ2 diag(XAT )T diag(XAT )

min
A
+ α
2γt

(29)

The problem is still jointly convex in M and A.

6

6 Experiments

Our primary objective in formulating this convex approach to mixture regression is to tackle a dif-
ﬁcult problem in video analysis (see below). However  to initially evaluate the different approaches
we conducted some synthetic experiments. We generated 30 synthetic data points according to
yi = (πi ⊗ xi)w + i  with xi ∈ R  i ∼ N(0  1) and w ∈ U(0  1). The response variable yi is
assumed to be generated from a mixture of 5 components. We compared the quality of the relax-
ation in (22) to EM. Max-min algorithm is used in this experiment. For EM  100 random restarts
was used to help avoid poor local optima. The experiment is repeated 10 times. The error rates are
0.347 ± 0.086 and 0.280 ± 0.063 for EM and convex relaxation  respectively. The visualization
of the recovered membership for one of the runs is given in Figure 1. This demonstrates that the
relaxation can retain much of the structure of the problem.
6.1 Vision Experiment
In a dynamic scene  various static and moving objects are viewed by a possibly moving observer.
For example  consider a moving  hand-held camera ﬁlming a scene of several cars driving down
the road. Each car has a separate motion  and even the static objects  such as trees  appear to move
in the video due to the self-motion of the camera. The task of segmenting each object according
to its motion  estimating the parameters of each motion  and recovering the structure of the scene
is known as the multibody structure and motion problem. This is a missing variable problem. If
the motions have been segmented correctly  it is easy to estimate the parameters of each motion.
Naturally  models employing EM have been proposed to tackle such problems (c.f. [19  20]).
From epipolar geometry  given a pair of corresponding points pi and qi from two images (pi  qi ∈
R3×1)  we have the epipolar equation qT
i F pi = 0. The fundamental matrix F encapsulates infor-
mation about the translation and rotation relative to the scene points between the positions of the
camera where the two images were captured  as well as the camera calibration parameters such as
its focal length. In a static scene  where only the camera is moving  there is only one fundamental
matrix  which arises from the camera self-motion. However  if some of the scene points are moving
independently under multiple different motions  there are several fundamental matrices. If there
are k motion groups  the epipolar equation can be expressed in term of the multibody fundamental
i Fjpi) = 0. An algebraic method was proposed to recover this matrix via
Generalized PCA [21]. An alternative approach  which we follow here  is by Li [22]  who casts the
j=1 πijFj)pi = 0 where the membership
problem as a mixture of fundamental matrices  i.e. qT
variable πij = 1 when image point i belongs to motion group j  and zero otherwise. Furthermore 
i wj = 0  with the column
since qT
vectors xi = [qx
j ). Thus  we will end up with the
i pπ
i wj = 0. The weight vector wj for motion group j can be

following linear equation: (cid:80)k

i F pi = 0 is bilinear in the image points  we can rewrite it to be xT

matrix [21]  i.e.(cid:81)k

i ]T and w = vec(F T

i ((cid:80)k

recovered easily if the indicator variable πij is known.
We are interested in assessing the effectiveness of EM-based and convex relaxation-based methods
for this multibody structure and motion problem. We used the Hopkins 155 dataset [23]. The exper-
imental results are summarized in Table 1. All hyperparameters (EM: α and σ; Convex relaxation:
α  σ  and γ) were tuned and the best performances for each learning algorithm are reported. The
EM algorithm was run with 100 random restarts to help avoid poor local optima. In terms of com-
putation time  the max-min runs comparably to the EM algorithm  while min-min runs in the order
of 3 to 4 times slower. As an illustration  on a Pentium 4 3.6 GHz machine  the elapsed time (in
seconds) for two cranes dataset is 16.880  23.536  and 60.003 for EM  max-min and min-min 
respectively. Rounding for the convex versions was done by k-means  which introduces some dif-
ferences in the ﬁnal results for both algorithms. Noticeably  both max-min and min-min outperform
the EM algorithm. Visualizations of the motion segmentation on two cranes  three cars 
and cars2 07 datasets are given in Figure 2 (for kanatani2 and articulated please refer
to Appendix).
7 Conclusion
The mixture regression problem is pervasive in many applications and known approaches for param-
eter estimation rely on variants of EM  which naturally have issues with local minima. In this paper
we introduced a semideﬁnite relaxation for the mixture regression problem  thus obtaining a con-
vex formulation which does not suffer from local minima. In addition we showed how to avoid the

j=1(qT

i px

i qx

i py

i qx

i pπ
.... qπ
i
j=1 πijxT

7

use of expensive interior-point methods typically needed to solve semideﬁnite programs. This was
achieved by introducing two reformulations amenable to the use of faster algorithms. Experimental
results with synthetic data as well as with real computer vision data suggest the proposed methods
can substantially improve on EM while one of the methods in addition has comparable runtimes.

Table 1: Error rate on several datasets from the Hopkins 155

Data set
m
173
three cars
63
kanatani2
212
cars2 07
94
two cranes
articulated 150

EM Max-Min Convex Min-Min Convex
0.0347
0.0000
0.2594
0.0106
0.0000

0.0289
0.0000
0.2642
0.0213
0.0000

0.0532
0.0000
0.3396
0.0532
0.0000

(a) Ground Truth

(b) EM

(c) Convex Relaxation

Figure 1: Recovered membership on synthetic data with EM and convex relaxation. 30 data points
are generated according to yi = (πi ⊗ xi)w + i  with xi ∈ R  i ∼ N(0  1) and w ∈ U(0  1).

(a) Ground Truth

(b) EM

(c) Max-Min Convex (d) Min-Min Convex

(e) Ground Truth

(f) EM

(g) Max-Min Convex (h) Min-Min Convex

(i) Ground Truth

(j) EM

(k) Max-Min Convex (l) Min-Min Convex

Figure 2: Resulting motion segmentations produced by the various techniques on the Hopkins 155
dataset. 2(a)-2(d): two cranes  2(e)-2(h): three cars  and 2(i)-2(l): cars2 07. In two
cranes (ﬁrst row)  EM produces more segmentation errors at the left crane. In three cars
(second row)  the max-min method gives the least segmentation error (at the front side of the middle
car) and EM produces more segmentation errors at the front side of the left car. The contrast of EM
and convex methods is apparent for cars2 07 (third row): the convex methods segment correctly
the static grass ﬁeld object  while EM makes mistakes. Further  the min-min method can almost
perfectly segment the car in the middle of the scene from the static tree background.

8

References
[1] S. Gaffney and P. Smyth. Trajectory clustering with mixtures of regression models. In ACM

SIGKDD  volume 62  pages 63–72  1999.

[2] S. Finney  L. Kaelbling  and T. Lozano-Perez. Predicting partial paths from planning problem
parameters. In Proceedings of Robotics: Science and Systems  Atlanta  GA  USA  June 2007.
[3] A. J. Storkey and M. Sugiyama. Mixture regression for covariate shift. In Sch¨olkopf  editor 

Advances in Neural Information Processing Systems 19  pages 1337–1344  2007.

[4] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 
39(1):1–38  1977.

[5] T. De Bie  N. Cristianini  P. Bennett  and E. Parrado-hern¨andez. Fast sdp relaxations of graph

cut clustering  transduction  and other combinatorial problems. JMLR  7:1409–1436  2006.

[6] Y. Guo and D. Schuurmans. Convex relaxations for latent variable training. In Platt et al. 

editor  Advances in Neural Information Processing Systems 20  pages 601–608  2008.

[7] S. M. Goldfeld and R.E. Quandt. Nonlinear methods in econometrics. Amsterdam: North-

Holland Publishing Co.  1972.

[8] D. W. Hosmer. Maximum likelihood estimates of the parameters of a mixture of two regression

lines. Communications in Statistics  3(10):995–1006  1974.

[9] H. Sp¨ath. Algorithm 39: clusterwise linear regression. Computing  22:367–373  1979.
[10] W.S. DeSarbo and W.L. Cron. A maximum likelihood methodology for clusterwise linear

regression. Journal of Classiﬁcation  5(1):249–282  1988.

[11] P.N. Jones and G.J. McLachlan. Fitting ﬁnite mixtures models in a regression context. Austral.

J. Statistics  34(2):233–240  1992.

[12] S. Gaffney and P. Smyth. Curve clustering with random effects regression mixtures. In AIS-

TATS  2003.

[13] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural

computation  6:181–214  1994.

[14] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[15] M. Overton and R. Womersley. Optimality conditions and duality theory for minimizing sums
of the largest eigenvalues of symmetric matrices. Mathematical Programming  62:321–357 
1993.

[16] J. Yu  S.V.N. Vishwanathan  S. G¨unter  and N. Schraudolph. A quasi-Newton approach to

nonsmooth convex optimization. In ICML  2008.

[17] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learn-

ing  73:243–272  2008.

[18] J. Chen  L. Tang  J. Liu  and J. Ye. A convex formulation for learning shared structures from

multiple tasks. In ICML  2009.

[19] N.Vasconcelos and A. Lippman. Empirical bayesian em-based motion segmentation. In CVPR 

1997.

[20] P. Torr. Geometric motion segmentation and model selection. Philosophical Trans. of the

Royal Society of London  356(1740):1321–1340  1998.

[21] R. Vidal  Y. Ma  S. Soatto  and S. Sastry. Two-view multibody structure from motion. IJCV 

68(1):7–25  2006.

[22] H. Li. Two-view motion segmentation from linear programming relaxation. In CVPR  2007.
[23] http://www.vision.jhu.edu/data/hopkins155/.

9

,Christian Borgs
Jennifer Chayes
Adam Smith
Matthias Bauer
Mark van der Wilk
Carl Edward Rasmussen