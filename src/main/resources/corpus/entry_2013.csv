2018,Fast Estimation of Causal Interactions using Wold Processes,We here focus on the task of learning Granger causality matrices for multivariate point processes. In order to accomplish this task  our work is the first to explore the use of Wold processes. By doing so  we are able to develop asymptotically fast MCMC learning algorithms. With $N$ being the total number of events and $K$ the number of processes  our learning algorithm has a $O(N(\ \log(N)\ +\ \log(K)))$ cost per iteration. This is much faster than the $O(N^3\ K^2)$ or $O(K^3)$ for the state of the art. Our approach  called GrangerBusca  is validated on nine datasets. This is an advance in relation to most prior efforts which focus mostly on subsets of the Memetracker data. Regarding accuracy  GrangerBusca is three times more accurate (in Precision@10) than the state of the art for the commonly explored subsets Memetracker. Due to GrangerBusca's much lower training complexity  our approach is the only one able to train models for larger  full  sets of data.,Fast Estimation of Causal Interactions

using Wold Processes

Flavio Figueiredo Guilherme Borges

Pedro O. S. Vaz de Melo Renato Assunc¸ ˜ao

Universidade Federal de Minas Gerais (UFMG)

{flaviovdf  guilherme.borges  olmo  assuncao}@dcc.ufmg.br

Reproducibility: http://github.com/flaviovdf/granger-busca

Abstract

We here focus on the task of learning Granger causality matrices for multivariate
point processes. In order to accomplish this task  our work is the ﬁrst to explore
the use of Wold processes. By doing so  we are able to develop asymptotically fast
MCMC learning algorithms. With N being the total number of events and K the
number of processes  our learning algorithm has a O(N ( log(N ) + log(K))) cost
per iteration. This is much faster than the O(N 3 K 2) or O(K 3) for the state of the
art. Our approach  called GRANGER-BUSCA  is validated on nine datasets. This is
an advance in relation to most prior efforts which focus mostly on subsets of the
Memetracker data. Regarding accuracy  GRANGER-BUSCA is three times more
accurate (in Precision@10) than the state of the art for the commonly explored
subsets Memetracker. Due to GRANGER-BUSCA’s much lower training complexity 
our approach is the only one able to train models for larger  full  sets of data.

1

Introduction

In order to understand complex systems we need to know how their components (or entities) interact
with each other. Networks (or graphs) offer a common language to model such systems  where their
entities are represented by nodes and their interactions by edges [6]. The networked point process
is a stochastic model for these systems  when data take the form of a time series of random events
observed in each node. That is  in each node of a network we have a temporal point process  which
is a random process whose realizations consist of the times P = {tj  j ∈ N} of isolated events.
Networked point processes are probabilistic models designed to analyze the inﬂuence that events
occurring in a node may have on the events occurring on other nodes of the network.

Recently  several ﬁelds used networked point processes to understand complex systems such as
spiking biological neurons [36]  social networks [8  42] geo-sensor networks [22]  ﬁnancial agents
in markets [37]  television records [48] and patient visits [11]. One of the main objectives in these
analyses is to uncover the causal relationships among the entities of the system  or the interaction
structure among the nodes  which is also called the latent network structure. Typically  this is
represented by a graph where edges connect nodes that affect each other and edge weights represent
the intensity of this pairwise interaction. To the best of our knowledge  all methods that tackle
this problem are based on Hawkes point process [25  24] with a Granger causality framework [20]
imposed to retrieve the causal graph from data [1  48  53  35  34]. A point process Pb is said to
Granger cause another point process Pa when the past information of Pb can provide statistically
signiﬁcant information about the future occurrences of Pa. We can thus encode causal relationships as
a matrix [15  16]. In a multivariate point process  this notion of causality can be reduced to measuring
if the conditional intensity function of Pb is affected by the previous timestamps of Pa [48].

In contrast with the popular choice of using Hawkes process to model interacting processes  Wold
processes [47] have been neglected as a possible model. Wold processes are a class of point processes

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

deﬁned in terms of the joint distribution of inter-event times. Let δi = ti − ti−1 be the waiting
time for i-th event since the occurrence of the (i − 1)-th event. The main characteristic of Wold
processes is that the collection of inter-event times {δi  i ∈ N} follows a Markov chain of ﬁnite
memory. That is  different from Hawkes processes  whose intensity function depends on the whole
history of previous events  the probability distribution of the i-th inter-event time δi depends only
on the previous inter-event time δi−1. When Wold processes are able to mimic the dynamics of
complex systems [46  45]  this Markovian property can potentially boost the performance of learning
algorithms as in our setting. Wold processes were proposed over sixty years ago  however  they
remain scarcely explored in machine learning  which is unfortunate. As we will demonstrate in this
paper  Wold processes can be both fast and accurate for some learning tasks.

We here present the ﬁrst approach to tackle the discovery of latent network structures in point process
data using Wold processes. Similar to recent efforts [1  48]  our goal in this work is to extract Granger
causality [20] from multivariate point process data only. The main reason to consider the Wold
process as an alternative to the Hawkes process is its Markovian structure. By adding Dirichlet
priors over the mutual inﬂuences among the processes  we exploit the Markov property to develop
learning algorithms that are asymptotically fast. We call our approach GRANGER-BUSCA. With N
being the number of observations in all processes and K the number of processes  the state of the art
approaches learn at a cost of O(M N 3 K 2) [48] (M being deﬁned by hyper-parameters) or O(K 3) [1]
per iteration. GRANGER-BUSCA  in contrast  learns at a cost of O(N (log(N ) + log(K))).

Equally important  our results show signiﬁcant improvements over the state of the art methods. For
instance  when we measure the Precision@10 score between our Granger causal estimates and the
ground-truth number of mentions of the commonly explored Memetracker data [29]  our results are
at least three times more accurate than the most recent and most accurate method [1].

In summary  our main contributions are: (1) We present the ﬁrst approach to extract Granger causality
matrices via Wold processes; (2) We develop asymptotically fast algorithms to learn such matrices;
(3) We show how GRANGER-BUSCA is much faster and more accurate than state of the art methods 
opening up the potential of Wold processes for the machine learning community.

2 Background and Related Work

A temporal point process P is a probability model for a collection of times {0 ≤ t0 < t1 < t2 < . . .}
indexing the occurrences of random isolated events. Our context considers several point processes
Pa  Pb  Pc  . . . observed simultaneously  where each event is associated to a single point process:

Pa = {0 ≤ ta0 < ta1 < . . .}. Let P = Sa Pa be the union of all timestamps from all point
processes with N = |P| being the total number of events. We denote by Na(t) = P|Pa|

✶tai ≤t the
random cumulative count of the number of events up to (and including) time t in process Pa. The
collection Na = {Na(t) | t ∈ [0  T ]} is an equivalent representation of the point process Pa.

i=1

The conditional intensity rate function is the fundamental tool for modeling and making inferences
on point processes. Let Ha(t) be the random history of the process Pa up to  but not including 
time t  called the ﬁltration. Similarly  H(t) is deﬁned as the ﬁltration considering the collection
P of all point processes. In the limit  as dt → 0  the conditional intensity rate function is given
by λa(t|H(t))dt = Pr [Na(t + dt) − Na(t) > 0 | H(t)] = E (Na(t + dt) − Na(t) | H(t)). The
interpretation of this function is that  for a small time interval dt  the value of λa(t|H(t)) dt is
approximately equal to the expected number of events in (t  t + dt]. It can also be interpreted as the
probability that process Pa has at least one event in the interval. The notation emphasizes that the
conditional intensity at time t depends on the random events that occurred previously.

The commonly explored Hawkes process P is deﬁned by its set of conditional intensity functions:

b=0 R t
λa(t|H(t)) = µa +PK−1

0 φba(t − t′)dNb(t′) = µa +PK−1

b=0 Ptbi <t φba(t − tbi )

(1)

We can consider processes as being enumerated from {a  b  · · · } ∈ [0  K). µa captures the exogenous
(Poissonian) background arrival rate of process Pa. φba captures the inﬂuence of the point process
Pb on Pa. φba(t) has to be integrable  non-negative  and with φba(t) = 0 when t < 0. Usual forms
of φba(t) are the exponential and power-law functions [4]. With λa(t) from Eq (1)  there is evidence
that Pb Granger causes Pa when there exists a time t where φba(t) 6= 0 [48].

2

In contrast to the Hawkes process  a Wold process [14  47] is deﬁned in terms of a Markovian
transition on the inter-event times (increments). Let Da = {δai = tai − tai−1   . . .} be the stochastic
process of time increments associated with point process Pa. The Markovian transition between
increments is established by the transition probability density Pr[x = δai−1 | y = δai ] which
measures the likelihood of δai given the value of the previous increment δai−1 [47  14].
It is important to point out that  for most forms of Pr[x | y]  the model is analytically intractable [21].
However  when Pr[x | y] has an exponential form  such as Pr[x | y] = f (x)e−f (x)y  the model has
several interesting properties [12  13]. In this particular form  the next increment is exponentially
distributed with rate λ = f (x) where x is the previous increment  i.e.: δi+1 ∼ Exponential(λ =
f (δi)). For the particular case of f (x) = β + αx  the work of Cox [12] and Daley [13  14] derive
the stationary distribution of increments showing that it is of the form: Pr[x] ∝ (β + αx)−1e−αx.

GRANGER-BUSCA is motivated by recent efforts [2  45  46] that employ variations of the exponential
Wold process (deﬁned above). Instead of deﬁning the Wold process in terms of its interval exponential
rate  such efforts deﬁned the process in terms of the conditional mean µ(x) = Ea(δai|δai−1 = x) =
β + αx of an exponentially distributed random variable. This class of point processes is called self-
feeding processes (SFP). For the particular case of α = 1 and β = median(δai )/e  with e ≈ 2.718
being the Euler constant  [45  46] showed that the stationary behavior can be very well approximated
by the more tractable Log-Logistic distribution. This new form of specifying a Wold process is
interesting because it is able to capture both exponential and power-law behavior  disparate behaviors
simultaneously observed in real data [2  45  46]. Realizations of this process tend to generate bursts
of intense activity followed by long periods of silence.

Busca [2] is another point process model based on Wold processes and it is GRANGER-BUSCA’s
starting point. Starting from a SFP model with α = 1  the authors accommodate the less frequent long
periods of waiting times observed in some real datasets  the process adds a background Poissonian
rate (µa). The conditional intensity can thus be derived given by:

λa(t | Ha(t)) = µa +

1

β + δap

 

(2)

where δap = tap − tap−1 with p being deﬁned by arg max{p : tai < t}. That is  p the index
associated to the closest previous event to t. δap is thus the previously observed increment.

Over the years  several Hawkes methods have been created for different applications with varying
asymptotic costs [5  8  9  10  33  41]. We now discuss the methods most related to GRANGER-BUSCA.
Achab et at. [1] presents a O(K 3) approach. Xu et al. [48] learns kernels at cost of O(M K 2 N 3).
Here  M represents the number of basis functions used to approximate the kernel no-parametrically.
Similarly Yang et al. [49] discusses a non-parametric Hawkes that does not explore the inﬁnite
memory. The authors show that after W events  the kernel is adequately estimated. However  the
proposed method still scales in the order of O(M K 3 W )1  M represents again a pre-deﬁned number
of functions used to approximate the kernel  W is the maximum number of previous events to be
considered. A similar proposal is presented by Etesami et al. [18]. In contrast  GRANGER-BUSCA has
a computation complexity of O(N (log(N ) + log(K))) per iteration. We achieve this low cost by
employing a Bayesian inference where at each step the algorithm learns  for each event in P  which
other process  if any  caused that event. Past efforts have employed similar sampling approaches on
Hawkes based models [17  34  35] which again do not scale.

It is important to point out that Isham [27] was one of the ﬁrst and few to discuss multivariate Wold
processes. However  the author was mostly focused on analytical properties of the Multivariate Wold
Process on a very speciﬁc setting (an inﬁnite process deﬁned on the unit circle).

3 Model

We deﬁne GRANGER-BUSCA using Figure 1  which exempliﬁes the behavior of the model with two
processes. In (a)  we show the events of each process as a horizontal line of symbols  triangles in

1The authors discuss a O(K 2) cost for a parallel algorithm with hyper-parameters being O(1). While this
is correct asymptotically  we compare  for all methods  the non-parallel cost with hyper-parameters. In practice 
hyper-parameters have a multiplicative effect on learning time. Moreover  for every parallel method (GRANGER-
BUSCA included)  the reduction factor of K (K 2 from K 3) is only possible via unbounded parallelization (one
process per CPU)  being unfeasible for large data: K >> #CP U s.

3

(a) GRANGER-BUSCA’s clustering behavior

Cross-excitment

Cross-excitement

Burst

Idleness

Burst

Idleness

(b) GRANGER-BUSCA’s usage of increments

(c) Rate and Number of Events

Figure 1: GRANGER-BUSCA at work. Plot (a) shows the events of process Pa (circles) and process
Pb (triangles). The arrows show the excitement component of the model. Plot (b) illustrates how
∆aa(t) and ∆ba(t) are calculated. Plot (c) shows the cumulative random processes Na(t) and Nb(t)
in the top  while the bottom plot shows the random conditional intensity functions λa(t) and λb(t).

the upper row for process Pb  and circles in the bottom row for process Pa. The unﬁlled symbols
represent events that are caused in an exogenous way  not triggered by any other event. We shall detail
how to label events in Section 4. The ﬁlled symbols are events that appear as a causal inﬂuence from
some other previous event. We say that these events have been excited by the previous occurrence
of other events. The directed edge connects the origin event to the resulting event. Edges crossing
the two parallel lines of events represent the cross-excitement component. In this case  one event
in a given process stimulates the occurrence of events in another process. Another situation is due
to self-excitement  when events in a given process stimulate further events in the same process.
These are represented by the horizontal arrows in Figure 1(a). Figure 1(c) shows the behavior of the
random Na(t) and the random conditional intensity function λa(t). Notice that λa(t) behaves like
a step function. That is  the rate of arrivals is ﬁxed until the next arrival. The ﬁgure also shows the
burst-idleness cycle observed with GRANGER-BUSCA  as well as the cross-excitation.

To formalize GRANGER-BUSCA  let us ﬁrst redeﬁne δap as a function ∆a(t). Recall that p is the
index of the previously observed event in Pa that is closest to t. Also recall that δap is the last
observed increment. The function notation will simplify our extension to a multivariate Wold process.
See Figure 1(b) for an illustration. Let us now deﬁne deﬁne q as the event in Pb that is closest to tap  
that is arg max{q : tbq < tap < t}. With such an index  we can denote by ∆ba(t) the difference
between tap and tbq   i.e.: ∆ba(t) = tap − tbq . When the expected values of ∆ba(t) are small  events
in Pb usually precede Pa. In this sense  one intuition on how GRANGER-BUSCA works is that larger
observed values of ∆ba(t) lead to weaker evidence for the inﬂuence of process Pb on process Pa.

The above behavior motivates GRANGER-BUSCA’s multivariate conditional intensity function:

λa(t) =

+

Exogenous Poisson Rate

µa

|{z}

Endogenous Wold Rate

αba

βb + ∆ba(t)

(3)

K−1

Xb=0
|

{z

}

The random intensity function λa(t) is the sum of two components. The ﬁrst one is µa and it
represents the exogenous events in process Pa that are generated at a Poissonian constant rate µa by
unit of time. The other component is a sum over all processes  including the same a process. The
terms in this sum represent the increment on the baseline rate µa contributed by other previous events
from Pa itself (self-excitement) or from other processes (cross-excitement). Based on a very sparse
representation  the entire history is concentrated only on the most recent time gaps between events.
Hence  the process Pb inﬂuence on process Pa is represented by the ratio αba/(βb + ∆ba(t)). The
numerator is a scale factor measuring the amount of cross-excitement: when is equal to zero  there
is no inﬂuence from Pb on Pa. The denominator models how this cross-excitement takes place. At
time t  we add the time gap ∆ba(t) to the ﬂat value βb. A large gap ∆ba(t) makes the contribution of

4

Self-Exc.Cross-Exc.ExogenousTimeExogenousSelf-Exc.0246810TimePaPb∆a(t)=∆aa(t)∆ba(t)t∆a(t)=∆aa(t)∆ba(t)t020406080100Time020Na(t)020406080100Time0.00.51.0λa(t)the ratio αba/(βb + ∆ba(t)) small relative to the baseline rate µa. Otherwise  a small gap raises this
contribution up to its maximum possible contribution rate of αba/βb.

G = [αba] is the Granger matrix of the model. We require that αba ≥ 0 and that PK−1

Model parameters and deﬁnitions: Θ = {G  β  µ} contains the full set of model parameters.
a=0 αba = 1.
Hence  the value αba in the G[b  a] is proportional to the fraction of events from process Pb that
triggered events on Pa. By deﬁnition  G is row stochastic [26]. This property leads to several
interesting characteristics of the model  that we further develop throughout the rest of this section.
The vector β = [βb] captures the the overall inﬂuence strength for each process Pb. That is  when a
process inﬂuences another  it does so by exponentially distributed inter-event times with a mean of
βb + ∆ba(t). As we now show  to guarantee stationary conditions  it is necessary that 1 ≤ βb < ∞.
Finally  we consider that each event tai has a latent label indicating either that it is exogenous or
which process caused it (edges in Figure 1(a)). Estimation of Θ would be trivial if these labels were
known. Thus  our learning algorithm (see Section 4) focuses on estimating on such labels from data.

αba

As pointed out  GRANGER-BUSCA’s stationarity (some authors also call this property stability [14 
34]) depends on 1 ≤ βb < ∞. Let B(t) be the diagonal matrix where each diagonal cell is equal to
1/(βb + ∆ba(t)). Moreover  let Φ(t) = B(t)G = [
βb+∆ba(t) = λba(t)]. Each value in Φ(t) is the
cross-intensity for a pair of processes. If Pb are represented in the rows and Pa in the columns  the
expected number of events at an inﬁnitesimal region for Pa is equal to the row sum of this matrix.
This value is simply GRANGER-BUSCA’s cross-feeding intensity without the exogenous factor. Now 
let ||X|| be the induced l∞-norm (maximum row sum).
Deﬁnition 1 (Stationarity): Notice that  ||Φ(t)|| < 1 [26]. The proof for this property is straight-
forward and has interesting implications. As G is row-stochastic  we have ||G|| = 1. Also 
βb + ∆ba(t) > 1 since βb ≥ 1 and ∆ba(t) > 0. The matrix B(t) will either scale down this norm
or keep it unchanged. The spectral radius is ρ(Φ(t)) < 1. Also  limk→∞ Φ(t)k = 0.

Due to the above deﬁnitions  the model is stationary/stable as it will never generate inﬁnite offspring.
k=0 Φ(t)k =

In fact  the total number of offspring at any given time is determined by the sum P∞

(I −Φ(t))−1. Next  we explore the deﬁnition of Granger causality for point processes [16  48].

Deﬁnition 2 (Granger Causality): A process Pa is said to be independent of any other process Pb
when: λa(t|Ha(t)) = λa(t|HP (t)) for any t ∈ [0  T ]. In contrast  Pb is deﬁned to Granger cause Pa
when: λa(t|Ha(t)) 6= λa(t|HQ(t))  where Q = Pa ∪ Pb . As a consequence  given two processes
Pa and Pb whose intensity functions follow Eq (3)  Granger causality arises when αba 6= 0 2

4 Learning GRANGER-BUSCA

Recall from Figure 1 that GRANGER-BUSCA exhibits a cluster like behavior. That is  exogenous
observations arrive at a ﬁxed rate  with each observation being able to trigger new observations
leading to a burst/idleness cycle. Based on this remark  we developed our Markov Chain Monte Carlo
(MCMC) sampling algorithm to learn GRANGER-BUSCA from data. Our algorithm will work by
sampling  for each observation tai   the hidden process (or label)  that likely caused this observation.
In other words  we sample a latent variable  zai   which takes a value of b ∈ [0  K − 1] when process
Pb inﬂuences tai . When the stamp is exogenous  we set this value to a constant K. Such a number
merely represents a label (exogenous) and does not affect our sampling.

To simplify our learning strategy  we decided to ﬁx β = 1. We notice that such a value shown to be
sufﬁcient for GRANGER-BUSCA to outperform state of the art baselines (see Section 5). Later in
this section  we shall discuss that our learning algorithm is easily adaptable for general forms of the
intensity function. G is estimated based on the number of events that Pb caused on Pa  whereas µ
is estimated as the maximum likelihood rate for a Poisson process based on the exogenous events
for Pa. We can thus learn GRANGER-BUSCA with an Expectation Maximization approach. Hidden
labels are estimated in the Expectation step. The matrix G can also be readily updated in such a step.
With the labels  µ estimated in the maximization step. Next  we discuss our ﬁtting strategy.

Initially  we explored the Markovian nature of the process to evaluate ∆ba(t) at a O(log(N )) cost.
Given some labels’ assignments for the events  we obtain ∆ba(t) with a binary search over Pa and Pb.

2When interpreting results from GRANGER-BUSCA  we relax the above condition of strict independence to

αba ≈ 0. In such cases  we can state that we have no statistical evidence for Granger causality.

5

We explain now how we update the zai labels. Given any event at tai in process Pa  the event either
exogenous or induced by some other previous event on Pa or from some other process Pb. By the
superposition theorem [14  34  35  17]  we obtain the conditional probability that an individual event
at tai is exogenous or was caused by process Pb (where b can be equal to a for SFP like behavior) as:

Pr[tai ∈ EXOG.] =

 

Pr[tai ← Pb] =

µa
b′=0 λb′a(tai )

µa +PK−1

λba(tai )
b′=0 λb′a(tai )

µa +PK−1

 

(4)

where the ← operator indicates causality. Notice that  zai = b is equivalent to tai ← Pb. Eq (4) is
carried out conditionally on the history Ha(tai ).

We can accelerate substantially our evaluations by using a binary modiﬁed Fenwick Tree [19] (the
F+Tree [51]) data structure if we break the zai label assignment into two steps. First  we decide if it
is exogenous. Given it is not  we select the inducing process based on the conditional probability:

Pr[tai ← Pb | tai 6∈ EXOG.] =

.

(5)

λba(tai )
b′=0 λb′a(tai )

PK−1

The evaluation of the probability that an event is not exogenous has an O(1) cost because we estimate
Pr[tai 6∈ EXOG.] ˆ= 1 − e−µa(tai −tµa ) where tµa is the last event before tai that arrived from an
exogenous factor3. This probability is the complement of the probability that zero Poissonian events
happened between tµa and tai . As G is row stochastic  we ﬁrst add a Dirichlet prior over each row
of this matrix (αp). We ﬁnally sample the hidden labels zai as follows:

1. For each process Pa

(a) Sample row a from G as ∼ Dirichlet(αp)

2. For each process Pa

(a) For each observation tai ∈ Pa

i. Sample p ∼ U nif orm(0  1)
A. When p < e−µa(tai −tµa )

zai ← exogeneous

B. Otherwise

Sample zai ∼ M ultinomial(Eq 5)

Model parameters are estimated through a MCMC sampler. Starting at an arbitrary random state
(i.e.  labels’ assignment)  let nba be the number of times Pb inﬂuenced Pa. Similarly  nb captures
the number of times Pb inﬂuenced any process  including itself. The conditional probability of
Pr[tai 6∈ EXOG.] ×
Pr[tai ← Pb | tai 6∈ EXOG.]. By factoring G  labels are sampled based on the collapsed Gibbs
sampler [43]. Thus  we re-write Eq (5) below. In this next equation  we point out the Proposal and
Target Distributions used on the Metropolis Based Sampler (discussed next).

hidden labels for every observation  z  is given by: Pr[z | Θ] = QK−1

a=0 Q|Pa|−1

i=0

Target Distribution

Pr[tai ← Pb | tai 6∈ EXOG.] ∝

1

{

βb + ∆ba(t)

 

(6)

z
|

−tai
n
ba + αp
−tai
+ αpK
b

n

Proposal Distribution

{z

}|
}

−tai
ba = (n

−tai
ba + αp)/(n

−tai
b

+ αpK) being the current estimate of αba  with n

where α
the count for the pair nba excluding the current assignment for tai and n
being similarly deﬁned.
Thus  the full algorithm follows an EM approach. After labels are assigned in the Expectation step 
we can compute µa for every process by simply estimating the maximum likelihood Poissonian rate.
Given that it takes O(log(N )) time to compute ∆ba(t) and O(K) time to compute Eq 5  the total
sampling complexity per learning iteration for the Gibbs sampler will be of O(N log(N ) K).

−tai
b

being

−tai
ba

Speeding Up with a Metropolis Based Sampler: The K factor in the traditional Gibbs sampler may
be reduced by exploiting speciﬁc data structures  such as the AliasTable [52  32] or the F+Tree [51].
In order to speed-up GRANGER-BUSCA  we shall employ the latter (F+Tree). The trade-offs between

3The ˆ= operator means equality by deﬁnition.

6

the two are discussed in [51]. Our choice is motivated by the fact that the F+Tree does not make use
of stale samples. Thus  we can perform multinomial sampling with a O(log(K)) cost. Given that the
AliasTable cannot be updated quickly  it is usually only suitable at later iterations [52  32].

We exploit the F+Tree by changing our sampler to a Metropolis Hasting (MH) approach. Using the
common notation for an MH  let Q(b) be the proposal probability density function as detailed in Eq 6.
Here  b is a candidate process which may replace the current assignment c = zai . When the target
distribution function is simply Eq 6  i.e.  P (c) = Eq 6  we can sample the assignment zai using the
acceptance probability min{1  (P (c)Q(b))/(P (b)Q(c))}. In other words  at each iteration we either
keep the previous zai or replace with b according to the acceptance probability. As discussed  with
the F+Tree  we can sample from Q(b) in O(log(K)) time. We can also update the tree with the new
probabilities after each step with the same cost. Given that F+Tree has a O(K log(K)) cost to build 
we build the tree once per process. Finally  as required for the Metropolis Hasting algorithm  it is
trivial to see that the proposal distribution is proportional to the target distribution [23].

Parallel Sampler: With the F+Tree  candidates are sampled at a O(log(K)) cost per event. More-
over  ﬁnding previous stamp costs O(log(N )). By adding these two costs  the algorithmic complexity
of GRANGER-BUSCA per iteration is O(N ( log(N ) + log(K))). Finally  notice that the sampler
is easy to parallelize. That is  by iterating over events on a per-process basis  we parallelize the
algorithm by scheduling different processes to different CPU cores. Overall  only vector of variables
is shared across processes (nb in Eq (6)). In our implementation  each core will read nb for each
process before an iteration. After the iteration  the value is thus updated globally. Our sampler falls
in the case of being Asynchronous with Shared Memory as discussed in [44].

Learning different Kernels Consider the equivalent rewrite of λa(t) = µa + PK−1

b=0 αbaωba(t) 
where  ωba(t) = 1/(βb +∆ba(t)) for GRANGER-BUSCA in particular. With this new form  the model
acts as a mixture of intensities (ωba(t)). Each pairwise intensity is weighted by the causal parameters
αba. Now  notice that using this form our EM algorithm is easily extensible for different functions
−tai
ωba(t). The E-step is able to estimate the causal graph (considering that Eq (6) ˆ= α
ba ωba(t)).
The M-Step provides maximum likelihood estimates for the speciﬁc parameters appearing in ωba(t).
In fact  even unsupervised forms may be learned. As we discuss in the next section  we keep the
aforementioned intensity given that it is simpler and outperforms baselines in our datasets.

5 Results and Experiments

We now present our main results. GRANGER-BUSCA is compared with three recently proposed
baselines methods: ADM4 [53]  Hawkes-Granger [48] and Hawkes-Cumulants [1]. The code for
each method is publicly available via the library tick4. Experiments were performed on a dedicated
Azure Standard DS15 v2 instance with 20 Intel Xeon CPU E5-2673 v3 cores and 140GB of memory.
We point out that we perform comparisons are performed with Hawkes methods given that it is the
most prominent framework. There is no Wold method for our task. We compare with approaches that
are: parametric [53] and non-parametric [48  1]  and explore ﬁnite [1] and inﬁnite [53  48] memory.

Hyper Parameters: ADM4 enforces an exponential kernel and with a ﬁxed rate. We employ a
Tree-structured Parzen Estimator to learn such a rate [7]  optimizing for the best model in terms of
log-likelihood. For Hawkes-Granger  we ﬁt the model with M = 10 basis functions as suggested
in [1]. Finally  Hawkes-Cumulants [1] also has a single hyper parameter called the half-width  which
was also estimated using [7]. Training is performed until convergence or until 300 iterations is
reached. Our MCMC sampler executes for 300 iterations with αp = 1

K and β = 1.

Datasets: We evaluate GRANGER-BUSCA and the aforementioned three baselines on 9 different
datasets  all of which were gathered from the Snap Network Repository5. Out of the nine datasets 
we pay particular attention to the Memetracker data  which is the only one used to evaluate all past
efforts. The Memetracker dataset consists of web-domains linking to one another. We pre-process
the Memetracker dataset using the code made available by [1]. The other eight datasets consist of
source nodes (e.g.  students or blogs) sending events to destination nodes (e.g.  messages or citations).
Each datasets can be represented as triples D = {(source  destination  timestamp)}. The ground
truth network is deﬁned as the graph Gt = {Vt  Et  Wt}  where the vertices Vt are both the sources

4https://github.com/X-DataInitiative/tick. Results produced with version 0.4.0.0.
5https://snap.stanford.edu/data/

7

Table 1: Datasets used for Experiments and Precision Scores for Full Datasets. Due to their sizes 
only GRANGER-BUSCA is able to execute in all datasets. To allow comparisons  we execute baselines
methods with only the Top-100 destination nodes. Other results are presented in Table 2 and Figure 2.

# Proc (K)

# Obs. (N)

N (Top-100)

Span

%NZ

P@5

P@10

P@20

TT(s)

bitcoinalpha [28]
bitcoinotc [28]
college-msg [39]
email [31  50]
sx-askubuntu [40]
sx-mathoverﬂow [40]
sx-superuser [40]
wikitalk [30  40]
memetracker-100 [29]
memetracker-500 [29]

3 257
4 791
1 313
803
88 549
16 936
114 623
251 154
100
500

23 399
33 766
58 486
327 677
879 121
488 984
1 360 974
7 833 140
24 665 418
39 318 989

2 279
2 328
10 869
92 924
58 142
59 602
64 866
211 344
-
-

5Y
5Y
193D
803D
7Y
7Y
7Y
6Y
9M
9M

0.2%
0.1%
1.1%
3.74%
0.006%
0.07%
0.006%
0.003%
9.85%
4.44%

0.26
0.25
0.36
0.23
0.25
0.28
0.26
0.25
0.30
0.30

0.14
0.14
0.30
0.28
0.13
0.16
0.14
0.14
0.29
0.30

0.07
0.07
0.19
0.32
0.06
0.09
0.07
0.07
0.22
0.23

3
7
1
4
2774
98
4614
27540
114
274

Table 2: Comparing GRANGER-BUSCA (GB) with Hawkes-Cumulants (HC) Memetracker.

Precision@5
HC
GB

Precision@10
HC
GB

Precision@20
HC
GB

Kendall

HC

GB

Rel. Error
HC
GB

top-100
top-500

0.06
0.01

0.30
0.30

0.09
0.01

0.29
0.30

0.01
0.02

0.22
0.23

0.05
0.08

0.26
0.20

1.0
1.8

0.44
0.06

TT(s)

HC

87
715

GB

114
274

and the destinations. Edges  e = (b  a) ∈ Et and {b  a} ⊆ Vt  represent the relationship between
two entities. The weighted adjacency matrix of this graph  Gt  is our ground-truth. It is deﬁned as:
Gt[b  a] = (# (b  a) ∈ D)/(# b is a source). To compose each process from these datasets  each
destination node represents a process. In compliance to our notation  we call such processes Pa.
Notice that we do not consider source nodes  Pb  as processes. Thus  the models will aim to extract
causal graphs based on the likelihood that reception of messages at a Pa destination will trigger
incoming messages. If this is the case  we have evidence that Pa is also be a source node. Finally  we
pre-process the data to only consider destinations that have also sent messages.

Metrics: We evaluate GRANGER-BUSCA and its competitors using the Precision@n  Kendall
Correlation and Relative Error Scores. Each score is measured per node (or row of G)  and is
summarized for the entire dataset as the average over every node. Both the Kendall Correlation  as
well as the Relative Errors  were proposed as evaluation metrics for networked point processes in [1].
Precision@n captures the average fraction of edges in G out of the top n edges ordered by weight
which are also present in Gt. As we shall discuss  there are several limitations with the Kendall and
Relative Error scores due to graph sparsity. We argue that Precision@n measured at different cut-offs
(n) is a more reliable evaluation metric for large and sparse graphs  as the ones we explore here.

Results: Table 1 describes the datasets used in this work presenting their number of nodes and of
observations. Most baselines do not execute on large datasets in under 24h of training time (TT)  in
contrast with GRANGER-BUSCA. Given the asymptotic complexity  we estimate that some models
may take months to execute. Hence  to allow comparisons with GRANGER-BUSCA  we considered
subsamples containing only the events involving the Top-100 destinations. We pay particular attention
to Top-100 and 500 nodes for Memetracker  given that it was explored in prior efforts [53  48  1].

Table 1 also presents the Precision@n scores for the GRANGER-BUSCA. Recall that ours is the
only approach able to execute on the full sets of data. Firstly  notice that the Kendall and Relative
Error scores are absent from Table 1. Given that datasets are sparse – as shown by the fraction of
non-zeros in the ground truth  or %NZ  in Table 1 – the Kendall Correlations and Relative Errors are
unreliable metrics for large networks. It is well known that complex networks as ours have long-tailed
distributions for the edge-weights [6]  leading to the high sparsity levels (%NZ) seen in Table 1. With
most cells being zeros  Kendall Scores also tend to zero as most sources connect to few destinations.
Similarly  the relative errors will likely increase. In order to avoid divisions by zero  previous efforts
impose a constant penalization  of one  when zero edges exist between two nodes. Similar to the
Kendall Correlation  this penalization will also dominate the score due to the sparsity.

Because of the above limitations of prior efforts and metrics  we are unable to present a fair comparison
with competitors on the full datasets. To achieve this goal  in Table 2  we present the overall scores for
GRANGER-BUSCA and the Hawkes-Cumulants (HC) [1] approach  focusing only on the Memetracker
data. In this setting  Hawkes-Cumulants has already been shown to be more accurate and faster than

8

Figure 2: Precision Scores for the Top-100 datasets.

ADM4 [53] and Granger-Hawkes [48] (GH). Observe that GRANGER-BUSCA is more accurate than
the competing method in every score. Indeed  Precision@n scores are at least three times greater
depending on the cut-off (Precision@5  10 and 20). Kendall Scores also show substantial gain (250%) 
with GRANGER-BUSCA achieving 0.20 and HC achieving 0.08 correlations on average. Relative
errors for GRANGER-BUSCA are also 56% lower than HC (1.0 versus 0.44). Finally  notice how
GRANGER-BUSCA is slightly slower than HC when fewer nodes are present (100)  but signiﬁcantly
surpasses HC in speed as K increases. This comes from the O(K 3) cubic cost incurred by HC.

To present an overall view of GRANGER-BUSCA against all three competing methods (ADM4  HC
and HG)  in Figure 2 we show Precision@5  10 and 20 scores for each approach on every Top-100
dataset. A total of 26 (out of 27 possible) points are plotted in the ﬁgure. One single setting  HC with
Memetracker  is absent since the model was unable o train sufﬁcient time. The x-axis of the ﬁgure
presents the Precision@n score for the baseline. Similarly  the y-axis shows the Precision@n score
for GRANGER-BUSCA. We also show three regions where either GRANGER-BUSCA or competitors
perform worse than a Null model. This model was created by performing uniformly random rankings.
Notice that for Precision@5 and Precision@10  GRANGER-BUSCA outperforms most baselines on
a large fraction of the datasets. In fact  for Precision@5  there is only one setting where ADM4
outperforms GRANGER-BUSCA. As the precision cut-off increases  so does the efﬁcacy of the Null
model (i.e.  it easier for a random ranking to recover top edges). For Precision@20  there does exists
some cases where GRANGER-BUSCA is outperformed by baseline methods. However  the majority
of these cases exist in the region where both models are below the efﬁcacy of a Null model.

Why does the model work? Recall that a Wold process is an adequate model when there is a strong
dependency between consecutive inter-event times δt and δt+1. To explain our results  we measured
the correlation between δt and δt+1 for pairs of interacting processes from the ground-truth data. Out
of nine datasets  the worst case had the median Pearson correlation per pair equal to 0.3  a moderate
value. However  in the remaining eight datasets this median was above 0.70  indicating the adequacy
of a Wold model. The high linear dependency implies that δt+1 ≈ αδt + β → E[δt+1] ≈ α E[δt] + β.
Thus  E[δt+1] is a linear function  f   of the previous inter-event  justifying the intensity: δt+1 ∼
Exponential(µ = f (δt)) (see Section 4 for a discussion on how to learn other forms).

It is also important to understand the limitations of non-parametric methods such as HC. Recall that
HC relies on the statistical moments (e.g.  mean/variance) of the inter-event times [1]. Given that web
datasets exhibit long tails (with theoretical distributions exhibiting high  or even inﬁnite  variance) 
such moments will not accurately capture the underlying behavior of the dataset (see Section 2).

6 Conclusions and Future Work

In this work  we present the ﬁrst method to extract Granger causality matrices via Wold Processes.
Though it was proposed over sixty years ago  this framework of point processes remain largely unex-
plored by the machine learning community. Our approach  called GRANGER-BUSCA  outperforms
recent baseline methods [1  48  53] both in training time and in overall accuracy. GRANGER-BUSCA
works particularly well when extracting the top connections of a node (Precision@5  10  20).

Given the efﬁcacy of GRANGER-BUSCA  our hope is that current results may open up a new class of
models  Wold processes  to be explored by the machine learning community. Finally  GRANGER-
BUSCA can be used to explore real world behavior in complex large scale datasets.

9

0.000.050.100.150.200.250.300.350.40Competitor0.000.050.100.150.200.250.300.350.40Granger-BuscaPrecision@50.000.050.100.150.200.250.300.350.40Competitor0.000.050.100.150.200.250.300.350.40Granger-BuscaPrecision@100.000.050.100.150.200.250.300.350.40Competitor0.000.050.100.150.200.250.300.350.40Granger-Busca (39% of cases) (61% of cases)Precision@20HCADM4HGGB Ourperforms Competitors (96% of cases)Competitors Outperform GB (4% of cases) (73% of cases) (27% of cases)All are randomCompetitors are random GB is randomAcknowledgements

We thank Fabricio Murai and the anonymous reviewers for providing comments. We also thank
Gabriel Coutinho for discussions on the mathematical properties of GRANGER-BUSCA  as well as
Alexandre Souza for providing pointers to prior studies. This work has been partially supported by
the project ATMOSPHERE (atmosphere-eubrazil.eu)  funded by the Brazilian Ministry of Science 
Technology and Innovation (Project 51119 - MCTI/RNP 4th Coordinated Call) and by the European
Commission under the Cooperation Programme  Horizon 2020 grant agreement no 777154. Funding
was also provided by the authors’ individual grants from CNPq  CAPES and Fapemig. Computational
resources were provided by the Microsoft Azure for Data Science Research Award (CRM:0740801).

References

[1] M. Achab  E. Bacry  S. Ga¨ıffas  I. Mastromatteo  and J.-F. Muzy. Uncovering causality from

multivariate hawkes integrated cumulants. In ICML  2017.

[2] R. Alves  R. Assunc¸ ˜ao  and P. O. S. Vaz de Melo. Burstiness scale: A parsimonious model for

characterizing random series of events. In KDD  2016.

[3] R. H. Arpaci-Dusseau and A. C. Arpaci-Dusseau. Operating Systems: Three Easy Pieces.

Arpaci-Dusseau Books  0.91 edition  2015.

[4] E. Bacry  I. Mastromatteo  and J.-F. Muzy. Hawkes processes in ﬁnance. Market Microstructure

and Liquidity  1(01)  2015.

[5] Y. Bao  Z. Kuang  P. Peissig  D. Page  and R. Willett. Hawkes process modeling of adverse

drug reactions with longitudinal observational data. In ML for HC  2017.

[6] A.-L. Barab´asi and M. P´osfai. Network science. Cambridge University Press  Cambridge  2016.

[7] J. S. Bergstra  R. Bardenet  Y. Bengio  and B. K´egl. Algorithms for hyper-parameter optimiza-

tion. In NIPS  2011.

[8] C. Blundell  J. Beck  and K. A. Heller. Modeling reciprocating relationships with hawkes

processes. In NIPS  2012.

[9] S. Chen  A. Shojaie  E. Shea-Brown  and D. Witten. The multivariate hawkes process in high

dimensions: Beyond mutual excitation. arXiv preprint arXiv:1707.04928  2017.

[10] J. Chevallier  M. J. C´aceres  M. Doumic  and P. Reynaud-Bouret. Microscopic approach of a
time elapsed neural model. Mathematical Models and Methods in Applied Sciences  25(14) 
2015.

[11] E. Choi  N. Du  R. Chen  L. Song  and J. Sun. Constructing disease network and temporal

progression model via context-sensitive hawkes process. In ICDM  2015.

[12] D. R. Cox. Some statistical methods connected with series of events. Journal of the Royal

Statistical Society. Series B (Methodological)  1955.

[13] D. Daley. Stationary point processes by markov-dependent intervals and inﬁnite intensity.

Journal of Applied Probability  19(A)  1982.

[14] D. J. Daley and D. Vere-Jones. An introduction to the theory of point processes  vol. 1. Springer 

New York  2003.

[15] M. Dhamala  G. Rangarajan  and M. Ding. Estimating granger causality from fourier and

wavelet transforms of time series data. Physical review letters  100(1)  2008.

[16] V. Didelez. Graphical models for marked point processes based on local independence. Journal

of the Royal Statistical Society: Series B (Statistical Methodology)  70(1)  2008.

[17] N. Du  M. Farajtabar  A. Ahmed  A. J. Smola  and L. Song. Dirichlet-hawkes processes with

applications to clustering continuous-time document streams. In KDD  2015.

[18] J. Etesami  N. Kiyavash  K. Zhang  and K. Singhal. Learning network of multivariate hawkes

processes: A time series approach. In UAI  2016.

[19] P. M. Fenwick. A new data structure for cumulative frequency tables. Software: Practice and

Experience  24(3)  1994.

10

[20] C. W. Granger. Investigating causal relations by econometric models and cross-spectral methods.

Econometrica: Journal of the Econometric Society  1969.

[21] P. Guttorp and T. L. Thorarinsdottir. What happened to discrete chaos  the quenouille process 
and the sharp markov property? some history of stochastic point processes. International
Statistical Review  80(2)  2012.

[22] D. Hallac  Y. Park  S. Boyd  and J. Leskovec. Network inference via the time-varying graphical

lasso. In KDD  2017.

[23] W. K. Hastings. Monte carlo sampling methods using markov chains and their applications.

Biometrika  57(1):97–109  1970.

[24] A. G. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal

Statistical Society. Series B (Methodological)  1971.

[25] A. G. Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika 

58(1)  1971.

[26] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press  1990.

[27] V. Isham. A markov construction for a multidimensional point process. Journal of Applied

Probability  14(3)  1977.

[28] S. Kumar  F. Spezzano  V. Subrahmanian  and C. Faloutsos. Edge weight prediction in weighted

signed networks. In ICDM  2016.

[29] J. Leskovec  L. Backstrom  and J. Kleinberg. Meme-tracking and the dynamics of the news

cycle. In KDD  2009.

[30] J. Leskovec  D. P. Huttenlocher  and J. M. Kleinberg. Governance in social media: A case study

of the wikipedia promotion process. In ICWSM  2010.

[31] J. Leskovec  J. Kleinberg  and C. Faloutsos. Graph evolution: Densiﬁcation and shrinking

diameters. ACM Transactions on Knowledge Discovery from Data (TKDD)  1(1)  2007.

[32] A. Q. Li  A. Ahmed  S. Ravi  and A. J. Smola. Reducing the sampling complexity of topic

models. In KDD  2014.

[33] S. Li  X. Gao  W. Bao  and G. Chen. Fm-hawkes: A hawkes process based approach for

modeling online activity correlations. In CIKM  2017.

[34] S. Linderman and R. Adams. Discovering latent network structure in point process data. In

ICML  2014.

[35] S. Linderman and R. Adams. Scalable bayesian inference for excitatory point process networks.

arXiv preprint arXiv:1507.03228  2015.

[36] R. P. Monti  P. Hellyer  D. Sharp  R. Leech  C. Anagnostopoulos  and G. Montana. Estimating
time-varying brain connectivity networks from functional mri time series. NeuroImage  103 
2014.

[37] A. Namaki  A. Shirazi  R. Raei  and G. Jafari. Network analysis of a ﬁnancial market based on
genuine correlation and threshold method. Physica A: Statistical Mechanics and its Applications 
390(21)  2011.

[38] Y. Ogata. On lewis’ simulation method for point processes. IEEE Transactions on Information

Theory  27(1):23–31  1981.

[39] P. Panzarasa  T. Opsahl  and K. M. Carley. Patterns and dynamics of users’ behavior and inter-
action: Network analysis of an online community. Journal of the Association for Information
Science and Technology  60(5)  2009.

[40] A. Paranjape  A. R. Benson  and J. Leskovec. Motifs in temporal networks. In WSDM  2017.

[41] M.-A. Rizoiu  Y. Lee  S. Mishra  and L. Xie. Hawkes processes for events in social media. In

S.-F. Chang  editor  Frontiers of Multimedia Research. ACM  2018.

[42] J. Silva and R. Willett. Hypergraph-based anomaly detection of high-dimensional co-

occurrences. IEEE transactions on pattern analysis and machine intelligence  31(3)  2009.

[43] M. Steyvers and T. Grifﬁths. Probabilistic topic models. Handbook of latent semantic analysis 

427(7)  2007.

11

[44] A. Terenin and E. P. Xing. Techniques for proving asynchronous convergence results for markov

chain monte carlo methods. In NIPS  2017.

[45] P. O. S. Vaz de Melo  C. Faloutsos  R. Assunc¸ ˜ao  R. Alves  and A. A. Loureiro. Universal
and distinct properties of communication dynamics: how to generate realistic inter-event times.
ACM Transactions on Knowledge Discovery from Data (TKDD)  9(3)  2015.

[46] P. O. S. Vaz de Melo  C. Faloutsos  R. Assunc¸ ˜ao  and A. Loureiro. The self-feeding process: A

unifying model for communication dynamics in the web. In WWW  2013.

[47] H. Wold. On stationary point processes and markov chains. Scandinavian Actuarial Journal 

1948(1-2)  1948.

[48] H. Xu  M. Farajtabar  and H. Zha. Learning granger causality for hawkes processes. In ICML 

2016.

[49] Y. Yang  J. Etesami  N. He  and N. Kiyavash. Online learning for multivariate hawkes processes.

In NIPS  2017.

[50] H. Yin  A. R. Benson  J. Leskovec  and D. F. Gleich. Local higher-order graph clustering. In

KDD  2017.

[51] H.-F. Yu  C.-J. Hsieh  H. Yun  S. Vishwanathan  and I. S. Dhillon. A scalable asynchronous

distributed algorithm for topic modeling. In WWW  2015.

[52] J. Yuan  F. Gao  Q. Ho  W. Dai  J. Wei  X. Zheng  E. P. Xing  T.-Y. Liu  and W.-Y. Ma. Lightlda:

Big topic models on modest computer clusters. In WWW  2015.

[53] K. Zhou  H. Zha  and L. Song. Learning social infectivity in sparse low-rank networks using

multi-dimensional hawkes processes. In AISTATS  2013.

A Simulating GRANGER-BUSCA

In Algorithm 1 we show how Ogata’s Modiﬁed Thinning algorithm [38] is adapted for GRANGER-
BUSCA. We initially point out that some care has to be taken for the initial simulated timestamps.
Given that tai (the previous observation) does not exist  the algorithm will need to either start with a
synthetic initial increment of fall back to the Poisson rate. In the algorithm  the rate of each individual
process is computed. Then  a new observation is generated based on the sum of such rates. Given
that each process will behave like a Poisson process while a new event does not surface (Figure 1) 
the sum of these processes is also a Poisson process. Lastly  we employ a multinomial sampling to
determine the process from which the observation belongs to.

B Log Likelihood

We now derive the log likelihood of GRANGER-BUSCA for parameters Θ = {G  β  µ}. For a point
process with intensity λ(t)  the likelihood can be computed as [14]:

L(Θ) =

N

Yi=i

λ(ti) exp(−Z t

0

λ(t)dt).

Considering the intensity of each process separately  we can write the log likelihood as:

(7)

(8)

LLa(Θ) = Xtai ∈Pa
= Xtai ∈Pa

(cid:16)log(cid:0) λa(tai(cid:1)(cid:17) −Z t
(cid:16)log(cid:0)µa +

Xb=0

K−1

0

αba

βb + ∆ba(tai )(cid:1)(cid:17) − Taµa − Xtai ∈P
Here  Ta is the last event from Pa. The expansion of the integral R Ta
the form Pti∈P PK−1

0 λa(t)dt comes from the
stepwise nature of λa(t) (see Figure 1). For simplicity  let us initially assume that there is only one
process. As discussed in the paper  computing ∆ba(t) has a log(N ) cost. Due to summations of
b=0   the cost to evaluate LLa(Θ) is O(K N log(N )). N log(N ) is the cost to

K−1

Xb=0

αba(tai − tai−1 )
βb + ∆ba(tai−1 )

evaluate ∆ba(t) for every observation.

λa(t)dt

12

,Flavio Figueiredo
Guilherme Resende Borges
Pedro O.S. Vaz de Melo
Renato Assunção
Baekjin Kim
Ambuj Tewari