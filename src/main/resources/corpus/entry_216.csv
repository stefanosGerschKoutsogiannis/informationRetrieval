2013,A Stability-based Validation Procedure for Differentially Private Machine Learning,Differential privacy is a cryptographically motivated definition of privacy which has gained considerable attention in the algorithms  machine-learning and data-mining communities. While there has been an explosion of work on differentially private machine learning algorithms  a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning  or  determining the parameter value  such as a bin size in a histogram  or a regularization parameter  that is suitable for a particular application.   In this paper  we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning -- training a regularized linear classifier and building a histogram density estimator that result in end-to-end differentially private solutions for these problems.,A Stability-based Validation Procedure for
Differentially Private Machine Learning

Kamalika Chaudhuri

Department of Computer Science and Engineering

UC San Diego  La Jolla CA 92093

kamalika@cs.ucsd.edu

Staal Vinterbo

Division of Biomedical Informatics
UC San Diego  La Jolla CA 92093

sav@ucsd.edu

Abstract

Differential privacy is a cryptographically motivated deﬁnition of privacy which
has gained considerable attention in the algorithms  machine-learning and data-
mining communities. While there has been an explosion of work on differentially
private machine learning algorithms  a major barrier to achieving end-to-end dif-
ferential privacy in practical machine learning applications is the lack of an ef-
fective procedure for differentially private parameter tuning  or  determining the
parameter value  such as a bin size in a histogram  or a regularization parameter 
that is suitable for a particular application.
In this paper  we introduce a generic validation procedure for differentially private
machine learning algorithms that apply when a certain stability condition holds on
the training algorithm and the validation performance metric. The training data
size and the privacy budget used for training in our procedure is independent of
the number of parameter values searched over. We apply our generic procedure to
two fundamental tasks in statistics and machine-learning – training a regularized
linear classiﬁer and building a histogram density estimator that result in end-to-
end differentially private solutions for these problems.

1

Introduction

Privacy-preserving machine learning algorithms are increasingly essential for settings where sensi-
tive and personal data are mined. The emerging standard for privacy-preserving computation for
the past few years is differential privacy [7]. Differential privacy is a cryptographically motivated
deﬁnition  which guarantees privacy by ensuring that the log-likelihood of any outcome does not
change by more than α due to the participation of a single individual; an adversary will thus have
difﬁculty inferring the private value of a single individual when α is small. This is achieved by
adding random noise to the data or to the result of a function computed on the data. The value α is
called the privacy budget  and measures the level of privacy risk allowed. As more noise is needed
to achieve lower α the price of higher privacy is reduced utility or accuracy. The past few years
have seen an explosion in the literature on differentially private algorithms  and there currently exist
differentially private algorithms for many statistical and machine-learning tasks such as classiﬁca-
tion [4  15  23  10]  regression [18]  PCA [2  5  17  12]  clustering [2]  density estimation [28  19] 
among others.
Many statistics and machine learning algorithms involve one or more parameters  for example  the
regularization parameter λ in Support Vector Machines and the number of clusters in k-means.
Accurately setting these parameters is critical to performance. However there is no good apriori way
to set these parameters  and common practice is to run the algorithm for a few different plausible
parameter values on a dataset  and then select the output that yields the best performance on held-out
validation data. This process is often called parameter-tuning  and is an essential component of any
practical machine-learning system.

1

A major barrier to achieving end-to-end differential privacy in practical machine-learning appli-
cations is the absence of an effective procedure for differentially private parameter-tuning. Most
previous experimental works either assume that a good parameter value is known apriori [15  5] or
use a heuristic to determine a suitable parameter value [19  28]. Currently  parameter-tuning with
differential privacy is done in two ways. The ﬁrst is to run the training algorithm on the same data
multiple times. However re-using the data leads to a degradation in the privacy guarantees  and thus
to maintain the privacy budget α  for each training  we need to use a privacy budget that shrinks
polynomially with the number of parameter values. The second procedure  used by [4]  is to divide
the training data into disjoint sets and train for each parameter value using a different set. Both so-
lutions are highly sub-optimal  particularly  if a large number of parameter values are involved – the
ﬁrst due to the lower privacy budget  and the second due to less data. Thus the challenge is to design
a differentially private validation procedure that uses the data and the privacy budget effectively  but
can still do parameter-tuning. This is an important problem  and has been mentioned as an open
question by [28] and [4].
In this paper  we show that it is indeed possible to do effective parameter-tuning with differential
privacy in a fairly general setting  provided the training algorithm and the performance measure
used to evaluate its output on the validation data together obey a certain stability condition. We
characterize this stability condition by introducing a notion of (β1  β2  δ)-stability; loosely speaking 
stability holds if the validation performance measure does not change very much when one person’s
private value in the training set changes  when exactly the same random bits are used in the training
algorithm in both cases or  when one person’s private value in the validation set changes. The second
condition is fairly standard  and our key insight is in characterizing the ﬁrst condition and showing
that it can help in differentially private parameter tuning.
We next design a generic differentially private training and validation procedure that provides end-
to-end privacy provided this stability condition holds. The training set size and the privacy budget
used by our training algorithms are independent of k  the number of parameter values  and the
accuracy of our validation procedure degrades only logarithmically with k.
We apply our generic procedure to two fundamental tasks in machine-learning and statistics – train-
ing a linear classiﬁer using regularized convex optimization  and building a histogram density esti-
mator. We prove that existing differentially private algorithms for these problems obey our notion
of stability with respect to standard validation performance measures  and we show how to combine
them to provide end-to-end differentially private solutions for these tasks. In particular  our appli-
cation to linear classiﬁcation is based on existing differentially private procedures for regularized
convex optimization due to [4]  and our application to histogram density estimation is based on the
algorithm variant due to [19].
Finally we provide an experimental evaluation of our procedure for training a logistic regression
classiﬁer on real data.
In our experiments  even for a moderate value of k  our procedure out-
performed existing differentially private solutions for parameter tuning  and achieved performance
only slightly worse than knowing the best parameter to use ahead of time. We also observed that
our procedure  in contrast to the other procedures we tested  improved the correspondence between
predicted probabilities and observed outcomes  often referred to as model calibration.
Related Work. Differential privacy  proposed by [7]  has gained considerable attention in the algo-
rithms  data-mining and machine-learning communities over the past few years as there has been a
large explosion of theoretical and experimental work on differentially private algorithms for statis-
tical and machine-learning tasks [10  2  15  19  27  28  3] – see [24] for a recent survey of machine
learning methods with a focus on continuous data. In particular  our case study on linear classi-
ﬁcation is based on existing differentially private procedures for regularized convex optimization 
which were proposed by [4]  and extended by [23  18  15]. There has also been a large body of
work on differentially private histogram construction in the statistics  algorithms and database liter-
ature [7  19  27  28  20  29  14]. We use the algorithm variant due to [19].
While the problem of differentially private parameter tuning has been mentioned in several works 
to the best of our knowledge  an efﬁcient systematic solution has been elusive. Most previous
experimental works either assume that a good parameter value is known apriori [15  5] or use a
heuristic to determine a suitable parameter value [19  28]. [4] use a parameter-tuning procedure
where they divide the training data into disjoint sets  and train for a parameter value on each set. [28]

2

mentions ﬁnding a good bin size for a histogram using differentially private validation procedure as
an open problem.
Finally  our analysis uses ideas similar to the analysis of the Multiplicative Weights Method for
answering a set of linear queries [13].

2 Preliminaries

Privacy Deﬁnition and Composition Properties. We adopt differential privacy as our notion of
privacy.
Deﬁnition 1 A (randomized) algorithm A whose output lies in a domain S is said to be (α  δ)-
differentially private if for all measurable S ⊆ S  for all datasets D and D(cid:48) that differ in the value
of a single individual  it is the case that: Pr(A(D) ∈ S) ≤ eα Pr(A(D(cid:48)) ∈ S) + δ. An algorithm is
said to be α-differentially private if δ = 0.

Here α and δ are privacy parameters where lower α and δ imply higher privacy. Differential privacy
has been shown to have many desirable properties  such as robustness to side information [7] and
resistance to composition attacks [11].
An important property of differential privacy is that the privacy guarantees degrade gracefully if
the same sensitive data is used in multiple private computations. In particular  if we apply an α-
differentially private procedure k times on the same data  the result is kα-differential private as

well as (α(cid:48)  δ)-differentially private for α(cid:48) = kα(eα − 1) +(cid:112)2k log(1/δ)α [7  8]. These privacy

composition results are the basis of existing differentially private parameter tuning procedures.
Training Procedure and Validation Score. Typical (non-private) machine learning algorithms
have one or more undetermined parameters  and standard practice is to run the machine learning
algorithm for a number of different parameter values on a training set  and evaluate the outputs on a
separate held-out validation dataset. The ﬁnal output is the one which performs best on the validation
data. For example  in linear classiﬁcation  we train logistic regression or SVM classiﬁers with
several different values of the regularization parameter λ  and then select the classiﬁer which has
the best performance on held-out validation data. Our goal in this paper is to design a differentially
private version of this procedure which uses the privacy budget efﬁciently.
The full validation process thus has two components – a training procedure  and a validation score
which evaluates how good the training procedure is.
We assume that training and validation data are drawn from a domain X   and the result of the
differentially private training algorithm lies in a domain C. For example  for linear classiﬁcation  X
is the set of all labelled examples (x  y) where x ∈ Rd and y ∈ {−1  1}  and C is the set of linear
classiﬁers in d dimensions. We use n to denote the size of a training set  m to denote the size of a
held-out validation set  and Θ to denote a set of parameters.
A differentially private training procedure is a randomized algorithm  which takes as input a (sensi-
tive) training dataset  a parameter (of the training procedure)  and a privacy parameter α and outputs
an element of C; the procedure is expected to be α-differentially private. For ease of exposition and
proof  we represent a differentially private training procedure T as a tuple T = (G  F )  where G is
a density over sequences of real numbers  and F is a function  which takes as input a training set  a
parameter in the parameter set Θ  a privacy parameter α  and a random sequence drawn from G  and
outputs an element of C. F is thus a deterministic function  and the randomization in the training
procedure is isolated in the draw from G.
Observe that any differentially private algorithm can be represented as such a tuple. For example 
given x1  . . .   xn ∈ [0  1]  an α-differentially private approximation to the sample mean ¯x is ¯x +
αn Z where Z is drawn from the standard Laplace distribution. We can represent this procedure
as a tuple T = (G  F ) as follows: G is the standard Laplace density over reals  and for any θ 
F ({x1  . . .   xn}  θ  α  r) = ¯x + r
αn. In general  more complicated procedures will require more
involved functions F .
A validation score is a function q : C × X m → R which takes an object h in C and a validation
dataset V   and outputs a score which reﬂects the quality of h with respect to V . For example  a

1

3

common validation score used in linear classiﬁcation is classiﬁcation accuracy.
In (non-private)
validation  if hi is obtained by running the machine learning algorithm with parameter θi  then the
goal is to output the i (or equivalently the hi) which maximizes q(hi  V ); our goal is to output
an i that approximately maximizes q(hi  V ) while still preserving the privacy of V as well as the
sensitive training data used in constructing the his.

3 Stability and Generic Validation Procedure

We now introduce and discuss our notion of stability  and provide a generic validation procedure
that uses the privacy budget efﬁciently when this notion of stability holds.

Deﬁnition 2 ((β1  β2  δ)-Stability) A validation score q is said to be (β1  β2  δ)-stable with respect
to a training procedure T = (G  F )  a privacy parameter α  and a parameter set Θ if the following
holds. There exists a set Σ such that PrR∼G(R ∈ Σ) ≥ 1 − δ  and whenever R ∈ Σ  the following
two conditions hold:

1. Training Stability: For all θ ∈ Θ  V   and all training sets T and T (cid:48) that differ in a single

entry  |q(F (T  θ  α  R)  V ) − q(F (T (cid:48)  θ  α  R)  V )| ≤ β1
n .

2. Validation Stability: For all T   θ ∈ Θ  and for all V and V (cid:48) that differ in a single entry 

|q(F (T  θ  α  R)  V ) − q(F (T  θ  α  R)  V (cid:48))| ≤ β2
m .

Condition (1)  the training stability condition  bounds the change in the validation score q  when one
person’s private data in the training set T changes  and the validation set V as well as the value of the
random variable R remains the same. Our validation procedure critically relies on this condition 
and our main contribution in this paper is to identify and exploit it to provide a validation procedure
that uses the privacy budget efﬁciently.
As F (T  θ  α  R) is a deterministic function  Condition (2)  the validation stability condition  bounds
the change in q when one person’s private data in the validation set V changes  and the output of the
training procedure remains the same. We observe that (some version of) Condition (2) is a standard
requirement in existing differentially private algorithms that preserve the privacy of the validation
dataset while selecting a h ∈ C that approximately maximizes q(h  V )  even if it is not required to
maintain privacy with respect to the training data.
Several remarks are in order. First  observe that Condition (1) is a property of the differentially
private training algorithm (in addition to q and the non-private quantity being approximated). Even
if all else remains the same  different differentially private approximations to the same non-private
quantity will have different values of β1.
Second  Condition (1) does not always hold for small β1 as an immediate consequence of differential
privacy of the training procedure. Differential privacy ensures that the probability of any outcome is
almost the same when the inputs differ in the value of a single individual; Condition (1) requires that
even when the same randomness is used  the validation score evaluated on the actual output of the
algorithm does not change very much when the inputs differ by a single individual’s private value.
In Section 6.1  we present an example of a problem and two α-differentially private training algo-
rithms which approximately optimize the same function; the ﬁrst algorithm is based on exponential
mechanism  and the second on a maximum of Laplace random variables mechanism. We show
that while both provide α-differential privacy guarantees  the ﬁrst algorithm does not satisfy train-
ing stability for β1 = o(n) and small enough δ while the second one ensures training stability for
β1 = 1 and δ = 0. In Section 4  we present two case studies of commonly used differentially private
algorithms where Conditions (1) and (2) hold for constant β1 and β2.
When the (β1  β2  δ)-stability condition holds  we can design an end-to-end differentially private
parameter tuning algorithm  which is shown in Algorithm 2. The algorithm ﬁrst uses a validation
procedure to determine which parameter out of the given set Θ is (approximately) optimal based
on the held-out data (see Algorithm 1). In the next step  the training data is re-used along with the
parameter output by Algorithm 1 and fresh randomness to generate the ﬁnal output. Note that we
use Exp(γ) to denote the exponential distribution with expectation γ.

4

Algorithm 1 Validate(Θ  T   T   V   β1  β2  α1  α2)
1: Inputs: Parameter list Θ = {θ1  . . .   θk}  training procedure T = (G  F )  validation score q 
training set T   validation set V   stability parameters β1 and β2  training privacy parameter α1 
validation privacy parameter α2.

).

Draw Ri ∼ G. Compute hi = F (T  θi  α1  Ri).
Let β = max( β1
Let ti = q(hi  V ) + 2βZi  where Zi ∼ Exp( 1

2: for i = 1  . . .   k do
3:
4:
5:
6: end for
7: Output i∗ = argmaxiti.

n   β2

m ).

α2

Algorithm 1 takes as input a training procedure T   a parameter list Θ  a validation score q  training
and validation datasets T and V   and privacy parameters α1 and α2. It runs the training procedure
T on the same training set T with privacy budget α1 for each parameter in Θ to generate outputs
h1  h2  . . .  and then uses an α2-differentially private procedure to select the index i∗ such that
the validation score q(hi∗   V ) is (approximately) maximum. For simplicity  we use a maximum of
Exponential random variables procedure  inspired by [1]  to ﬁnd the approximate maximum; an
exponential mechanism [21] may also be used instead. Algorithm 2 then re-uses the training data
set T to train with parameter θi∗ to get the ﬁnal output.

Algorithm 2 End-to-end Differentially Private Training and Validation Procedure
1: Inputs: Parameter list Θ = {θ1  . . .   θk}  training procedure T = (G  F )  validation score q 
training set T   validation set V   stability parameters β1 and β2  training privacy parameter α1 
validation privacy parameter α2.

2: i∗ = Validate(Θ T   T  V  β1  β2  α1  α2).
3: Draw R ∼ G. Output h = F (T  θi∗   α1  R).

3.1 Performance Guarantees

Theorem 1 shows that Algorithm 1 is (α2  δ)-differentially private  and Theorem 2 shows privacy
guarantees on Algorithm 2. Detailed proofs of both theorems are provided in the Supplementary
Material. We observe that Conditions (1) and (2) are critical to the proof of Theorem 1.

is
k )-stable with respect to the training procedure T   the privacy parameter α1 and the

Theorem 1 (Privacy Guarantees for Validation Procedure) If
(β1  β2  δ
parameter set Θ  then  Algorithm 1 guarantees (α2  δ)-differential privacy.
Theorem 2 (End-to-end Privacy Guarantees) If the conditions in Theorem 1 hold  and if T is
α1-differentially private  then Algorithm 2 is (α1 + α2  δ)-differentially private.
Theorem 3 shows guarantees on the utility of the validation procedure – that it selects an index i∗
which is not too suboptimal.

validation

score

the

q

Theorem 3 (Utility Guarantees) Let h1  . . .   hk be the output of the differentially private train-
ing procedure in Step (3) of Algorithm 1. Then  with probability ≥ 1 − δ0  q(hi∗   V ) ≥
max1≤i≤k q(hi  V ) − 2β log(k/δ0)

.

α2

4 Case Studies

We next show that Algorithm 2 may be applied to design end-to-end differentially private training
and validation procedures for two fundamental statistical and machine-learning tasks – training a lin-
ear classiﬁer  and building a histogram density estimator. In each case  we use existing differentially
private algorithms and validation scores for these tasks. We show that the validation score satisﬁes
the (β1  β2  δ)-stability property with respect to the training procedure for small values of β1 and

5

β2  and thus we can apply in Algorithm 2 with a small value of β to obtain end-to-end differential
privacy.
Details of the case study for regularized linear classiﬁcation is shown in Section 4.1  and those for
histogram density estimation is presented in the Supplementary Material.

4.1 Linear Classiﬁcation based on Logistic Regression and SVM
Given a set of labelled examples (x1  y1)  . . .   (xn  yn) where xi ∈ Rd  (cid:107)xi(cid:107) ≤ 1 for all i  and
yi ∈ {−1  1}  the goal in linear classiﬁcation is to train a linear classiﬁer that largely separates
examples from the two classes. A popular solution in machine learning is to ﬁnd a classiﬁer w∗ by
solving a regulared convex optimization problem:

w∗ = argminw∈Rd

(cid:107)w(cid:107)2 +

λ
2

1
n

(cid:96)(w  xi  yi)

(1)

n(cid:88)

i=1

Here λ is a regularization parameter  and (cid:96) is a convex loss function. When (cid:96) is the logistic loss
function (cid:96)(w  x  y) = log(1 + e−yiw(cid:62)xi )  then we have logistic regression. When (cid:96) is the hinge loss
(cid:96)(w  x  y) = max(0  1 − yiw(cid:62)xi)  then we have Support Vector Machines. The optimal value of λ
is data-dependent  and there is no good pre-deﬁned way to select λ apriori. In practice  the optimal
λ is determined by training a small number of classiﬁers with different λ values  and picking the one
that has the best performance on a held-out validation dataset.
[4] present two algorithms for computing differentially private approximations to these regularized
convex optimization problems for ﬁxed λ: output perturbation and objective perturbation. We restate
output perturbation as Algorithm 4 (in the Supplementary Material) and objective perturbation as
Algorithm 3. It was shown by [4] that provided certain conditions hold on (cid:96) and the data  Algorithm 4
is α-differentially private; moreover  with some additional conditions on (cid:96)  Algorithm 3 is α +

(cid:1)-differentially private  where c is a constant that depends on the loss function (cid:96)  and

2 log(cid:0)1 + c

λ is the regularization parameter.

λn

Algorithm 3 Objective Perturbation for Differentially Private Linear Classiﬁcation
1: Inputs: Regularization parameter λ  training set T = {(xi  yi)  i = 1  . . .   n}  privacy parame-
2: Let G be the following density over Rd: ρG(r) ∝ e−(cid:107)r(cid:107). Draw R ∼ G.
3: Solve the convex optimization problem:

ter α.

w∗ = argminw∈Rd

(cid:107)w(cid:107)2 +

λ
2

1
n

4: Output w∗.

(cid:96)(w  xi  yi) +

R(cid:62)w

2
αn

(2)

n(cid:88)

i=1

In the sequel  we use the notation X to denote the set {x ∈ Rd : (cid:107)x(cid:107) ≤ 1}.
Deﬁnition 3 A function g : Rd ×X ×{−1  1} → R is said to be L-Lipschitz if for all w  w(cid:48) ∈ Rd 
for all x ∈ X   and for all y  |g(w  x  y) − g(w(cid:48)  x  y)| ≤ L · (cid:107)w − w(cid:48)(cid:107).
Let V = {(¯xi  ¯yi)  i = 1  . . .   m} be the validation dataset. For our validation score  we choose a
function of the form:

g(w  ¯xi  ¯yi)

(3)

q(w  V ) = − 1
m

m(cid:88)

i=1

where g is an L-Lipschitz loss function. In particular  the logistic loss and the hinge loss are 1-
Lipschitz  whereas the 0/1 loss is not L-Lipschitz for any L. Other examples of 1-Lipschitz but
non-convex losses include the ramp loss: g(w  x  y) = min(1  max(0  1 − yw(cid:62)x)).
The following theorem shows that any non-negative and L-Lipschitz validation score is stable with
respect to Algorithms 3 and 4 and a set of regularization parameters Λ; a detailed proof is provided
in the Supplementary Material. Thus we can use Algorithm 2 along with this training procedure

6

and any L-Lipschitz validation score to get an end-to-end differentially private algorithm for linear
classiﬁcation.
Theorem 4 (Stability of differentially private linear classiﬁers) Let Λ = {λ1  . . .   λk} be a set
i=1 λi  and let g∗ = max(x y)∈X  w∈Rd g(w  x  y). If
of regularization parameters  let λmin = mink
(cid:96) is convex and 1-Lipschitz  and if g is L-Lipschitz and non-negative  then  the validation score q in
Equation 3 is (β1  β2  δ

k )-stable with respect to Algorithms 3 and 4  α and Λ for:

β1 =

2L
λmin

 

β2 = min

g∗ 

L

λmin

1 +

d log(dk/δ)

αn

(cid:18)

(cid:18)

(cid:19)(cid:19)

(cid:16)

(cid:17)

Example. For example 

1

1 + d log(dk/δ)

if g is chosen to be the hinge loss 

and β2 =
. This follows from the fact that the hinge loss is 1-Lipschitz  but may be

then β1 = 2
λmin

αn

λmin
unbounded for w of unbounded norm.
  and β2 = 1 (assuming that λmin ≤ 1). This
If g is chosen to be the ramp loss  then β1 = 2
follows from the fact that the ramp loss is 1-Lipschitz  but bounded at 1 for any w and (x  y) ∈ X .
λmin

5 Experiments

In order to evaluate Algorithm 2 empirically  we compare the regularizer parameter values and per-
formance of regularized logistic regression classiﬁers the algorithm produces with those produced
by four alternative methods. We used datasets from two domains  and used 10 times 10-fold cross-
validation (CV) to reduce variability in the computed performance averages.

The Methods Each method takes input (α  Θ  T  V )  where α denotes the allowed differential
privacy  T is a training set  V is a validation set  and Θ = {θ1  . . .   θk} a list of k regularizer values.
Also  let oplr (α  λ  T ) denote the application of the objective perturbation training procedure given
in Algorithm 3 such that it yields α-differential privacy.
The ﬁrst of the ﬁve methods we compare is Stability  the application of Algorithm 2 with oplr used
for learning classiﬁers  δ chosen in an ad-hoc manner to be 0.01  average negative ramp loss used as
validation score q  and with α1 = α2 = α/2.
The four other methods work by performing the following 4 steps: (1) for each θi ∈ Θ  train a
differentially private classiﬁer fi = oplr (αi  θi  Ti)  (2) determine the number of errors ei each fi
makes on validation set V   (3) randomly choose i∗ from {1  2  . . .   k} with probability P (i∗ = i|pi) 
and (4) output (θi∗   fi∗ ).
What differentiates the four alternative methods is how αi  Ti  and pi are determined. For
alphaSplit: αi = α/k  Ti = T   pi ∝ e−αei/2  dataSplit: αi = α  partition T into k equally
sized sets Ti  pi ∝ e−αei/2 (used in [4])  Random: αi = α  Ti = T   pi ∝ 1  and Control: αi = α 
Ti = T   pi ∝ 1(i = arg maxj q(fj  V )). Note that for alphaSplit  α/k > α(cid:48) where α(cid:48) is the
α = 0.3  then α/k > α(cid:48) − 0.0003. The method Control is not private  and serves to provide an
approximate upper bound on the performance of Stability. The three other alternative methods are
differentially private which we state in the following theorem.

solution of α = k(eα(cid:48) − 1)α(cid:48) +(cid:112)2k log(1/δ)α(cid:48) for all of our experimental settings  except when

Theorem 5 (Privacy of alternative methods) If T and V are disjoint  both alphaSplit and
dataSplit are α-differentially private. Random is α differentially private even if T and V are
not disjoint  in which case alphaSplit and dataSplit are 2α-differentially private.

Procedures and Data We performed 10 10-fold CV as follows. For round i in each of the CV
experiments  fold i was used as a test set W on which the produced classiﬁers were evaluated  fold
(i mod 10)+1 was used as V   and the remaining 8 folds were used as T . Furthermore k = 10 with
Θ = {0.001  0.112  0.223  0.334  0.445  0.556  0.667  0.778  0.889  1}. Note that the order of Θ is
chosen such that i < j implies θi < θj. By Theorems 2 and 5  all methods except Control produce

7

a (α  δ)-differentially private classiﬁer. Classiﬁer performance was evaluated using the area under
the receiver operator curve [25] (AUC) as well as mean squared error (MSE). All computations
were done using the R environment [22]  and data sets were scaled such that covariate vectors were
constrained to the unit ball. We used the following data available from the UCI Machine Learning
Repository [9]:
Adult – 98 predictors (14 original including categorical variables that needed to be recoded). The
data set describes measurements on cases taken from the 1994 Census data base. The classiﬁcation is
whether or not a person has an annual income exceeding 50000 USD  which has a prevalence of 0.22.
Each experiment involves computing more than 24000 classiﬁers. In order to reduce computation
time  we selected 52 predictors using the step procedure for a model computed by glm with family
binomial and logit link function.
Magic – 10 predictors on 19020 cases. The data set describes simulated high energy gamma par-
ticles registered by a ground-based atmospheric Cherenkov gamma telescope. The classiﬁcation is
whether particles are primary gammas (signal) or from hadronic showers initiated by cosmic rays in
the upper atmosphere (background). The prevalence of primary gammas is 0.35.

(a) Averages of AUC for the two data sets.

(b) Averages of MSE for the two data sets.

Figure 1: A summary of 10 times 10-fold cross-validation experiments for different privacy levels
α. Each point in the ﬁgure represents a summary of 100 data points. The error bars indiciate a
boot-strap sample estimate of the 95% conﬁdence interval of the mean. A small amount of jitter was
added to positions on the x-axes to avoid over-plotting.

Results Figure 1 summarizes classiﬁer performances and regularizer choices for the different val-
ues of the privacy parameter α  aggregated over all cross-validation runs. Figure 1a shows average
performance in terms of AUC  and Figure 1b shows average performance in terms of MSE.
Looking at AUC in our experiments  Stability signiﬁcantly outperformed alphaSplit and dataSplit.
However  Stability only outperformed Random for α > 1 in the Magic data set  and was in fact out-
performed by Random in the Adult data set. In the Adult data set  regularizer choice did not seem
to matter as Random performed equally well to Control. For MSE on the other hand  Stability
outperformed the differentially private alternatives in all experiments. We suggest the following
intuition regarding these results. The calibration of a logistic regression model instance  i.e.  the
difference between predicted probabilities and a 0/1 encoding of the corresponding labels  is not
captured well by AUC (or 0/1 error rate) as AUC is insensitive to all strictly monotonically increas-
ing transformations of the probabilities. MSE is often used as a measure of probabilistic model
calibration and can be decomposed into two terms: reliability (a calibration term)  and reﬁnement
(a discrimination measure) which is related to the AUC. In the Adult data set  the minor change
in AUC of Control and Random for α > 0.5  together with the apparent insensitivity of AUC
to regularizer value  suggests that any improvement in Stability performance can only come from
(the observed) improved calibration. Unlike in the Adult data set  there is a AUC performance gap
between Control and Random in the Magic data set. This means that regularizer choice matters for
discrimination  and we observe improvement for Stability in both discrimination and calibration.
Acknowledgements This work was supported by NIH grants R01 LM07273 and U54
HL108460  the Hellman Foundation  and NSF IIS 1253942.

8

llllllllllllAdultMagic0.60.70.80.30.51.02.03.05.00.30.51.02.03.05.0alphallllllllllllAdultMagic0.180.200.220.240.30.51.02.03.05.00.30.51.02.03.05.0alphalStabilityalphaSplitdataSplitRandomControlReferences
[1] R Bhaskar  S Laxman  A Smith  and A Thakurta. Discovering frequent patterns in sensitive

data. In KDD  2010.

[2] A. Blum  C. Dwork  F. McSherry  and K. Nissim. Practical privacy: the SuLQ framework. In

PODS  2005.

[3] K. Chaudhuri and D. Hsu. Convergence rates for differentially private statistical estimation. In

ICML  2012.

[4] K. Chaudhuri  C. Monteleoni  and A. D. Sarwate. Differentially private empirical risk mini-

mization. Journal of Machine Learning Research  12:1069–1109  March 2011.

[5] K. Chaudhuri  A.D. Sarwate  and K. Sinha. Near-optimal algorithms for differentially-private

principal components. Journal of Machine Learning Research  2013 (to appear).

[6] L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer  2001.
[7] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private

data analysis. In Theory of Cryptography  Berlin  Heidelberg  2006.

[8] C. Dwork  G. Rothblum  and S. Vadhan. Boosting and differential privacy. In FOCS  2010.
[9] A. Frank and A. Asuncion. UCI machine learning repository  2013.
[10] A. Friedman and A. Schuster. Data mining with differential privacy. In KDD  2010.
[11] S. R. Ganta  S. P. Kasiviswanathan  and A. Smith. Composition attacks and auxiliary informa-

tion in data privacy. In KDD  2008.

[12] M. Hardt and A. Roth. Beyond worst-case analysis in private singular vector computation. In

STOC  2013.

[13] M. Hardt and G. Rothblum. A multiplicative weights mechanism for privacy-preserving data

analysis. In FOCS  pages 61–70  2010.

[14] M. Hay  V. Rastogi  G. Miklau  and D. Suciu. Boosting the accuracy of differentially private

histograms through consistency. PVLDB  3(1):1021–1032  2010.

[15] P. Jain  P. Kothari  and A. Thakurta. Differentially private online learning. In COLT  2012.
[16] M C Jones  J S Marron  and S J Sheather. A brief survey of bandwidth selection for density

estimation. JASA  91(433):401–407  1996.

[17] M. Kapralov and K. Talwar. On differentially private low rank approximation. In SODA  2013.
[18] D. Kifer  A. Smith  and A. Thakurta. Private convex optimization for empirical risk minimiza-

tion with applications to high-dimensional regression. In COLT  2012.

[19] J. Lei. Differentially private M-estimators. In NIPS 24  2011.
[20] A. Machanavajjhala  D. Kifer  J. M. Abowd  J. Gehrke  and L. Vilhuber. Privacy: Theory

meets practice on the map. In ICDE  2008.

[21] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS  2007.
[22] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation.
[23] B. Rubinstein  P. Bartlett  L. Huang  and N. Taft. Learning in a large function space: Privacy-

preserving mechanisms for svm learning. Journal of Privacy and Conﬁdentiality  2012.

[24] A.D. Sarwate and K. Chaudhuri. Signal processing and machine learning with differential

privacy: Algorithms and challenges for continuous data. IEEE Signal Process. Mag.  2013.

[25] J. A. Swets and R. M. Pickett. Evaluation of Diagnostic Systems. Methods from Signal Detec-

tion Theory. Academic Press  New York  1982.

[26] Berwin A Turlach. Bandwidth selection in kernel density estimation: A review. In CORE and

Institut de Statistique. Citeseer  1993.

[27] S. Vinterbo. Differentially private projected histograms: Construction and use for prediction.

In ECML  2012.

[28] L. Wasserman and S. Zhou. A statistical framework for differential privacy.

105(489):375–389  2010.

JASA 

[29] J. Xu  Z. Zhang  X. Xiao  Y. Yang  and G. Yu. Differentially private histogram publication. In

ICDE  2012.

9

6 Appendix

6.1 An Example to Show Training Stability is not a Direct Consequence of Differential

Privacy

We now present an example to illustrate that training stability is a property of the training algorithm
and not a direct consequence of differential privacy. We present a problem and two α-differentially
private training algorithms which approximately optimize the same function; the ﬁrst algorithm
is based on exponential mechanism  and the second on a maximum of Laplace random variables
mechanism. We show that while both provide α-differential privacy guarantees  the ﬁrst algorithm
does not satisfy training stability while the second one does.
Let i ∈ {1  . . .   l}  and let f : X n × R → [0  1] be a function such that for all i and all datasets D
and D(cid:48) of size n that differ in the value of a single individual  |f (D  i) − f (D(cid:48)  i)| ≤ 1
n.
Consider the following training and validation problem. Given a sensitive dataset D  the private
training procedure A outputs a tuple (i∗  t1  . . .   tl)  where i∗ is the output of the α/2-differentially
private exponential mechanism [21] run to approximately maximize f (D  i)  and each ti is equal to
f (D  i) plus an independent Laplace random variable with standard deviation 2l
αn. For any validation
dataset V   the validation score q((i∗  t1  . . .   tl)  V ) = ti∗.
It follows from standard results that A is α-differentially private. Moreover  A can be represented
by a tuple TA = (GA  FA)  where GA is the following density over sequences of real numbers of
length l + 1:

GA(r0  r1  . . .   rl) = 10≤r0≤1 · 1

2l e−(|r1|+|r2|+...+|rl|)

Thus GA is the product of the uniform density on [0  1] and l standard Laplace densities. Consider
the following map E0. For r ∈ [0  1]  let

(cid:80)
(cid:80)

(cid:80)
(cid:80)

E0(r) = i 

if

j<i enαf (D j)/4
j enαf (D j)/4

≤ r ≤

j≤i enαf (D j)/4
j enαf (D j)/4

In other words  E0(r) is the map that converts a random number r drawn from the uniform distribu-
tion on [0  1] to the α/2-differentially private exponential mechanism distribution that approximately
maximizes f (D  i). Given a l + 1-tuple R = (R0  R1  . . .   Rl)  FA is now the following map:

FA(D  α  R) =

E(R0)  f (D  1) +

2lR1
αn

  f (D  2) +

2lR2
αn

  . . .   f (D  l) +

2lRl
αn

(cid:18)

(cid:19)

enα/8

e(n+2)α/8

2 + 1

n  f (D(cid:48)  2) = 1

2 and f (D(cid:48)  1) = 1 − 1

Let l = 2 and D and D(cid:48) be two datasets that differ in the value of a single individual. Suppose it
is the case that f (D  1) = 1  f (D  2) = 1
n. Observe
enα/4
that for D  the exponential mechanism picks 1 with probability
enα/4+enα/8   and 2 with probability
e(n−1)α/4
enα/4+enα/8   where as for D(cid:48)  it picks 1 with probability
e(n−1)α/4+e(n+2)α/8 and 2 with proba-
e(n−1)α/4
bility
e(n−1)α/4+e(n+2)α/8 . Thus  if R0 lies in the interval [
enα/4+enα/8 ]  then 
e(n−1)α/4+e(n+2)α/8  
FA(D  α  R) = t1 whereas FA(D(cid:48)  α  R) = t2. When n is large enough  with high probabil-
ity  |t1 − t2| ≥ 1
3; thus  the training stability condition does not hold for A for β1 = o(n) and
δ <
Consider a different algorithm A(cid:48) which computes t1  . . .   tl ﬁrst  and then outputs the index i∗ that
maximizes ti∗. Then A(cid:48) can be represented by a tuple TA(cid:48) = (GA(cid:48)  FA(cid:48))  where GA(cid:48) is a density
over sequences of real numbers of length l as follows:

enα/8(eα/2−1)

(enα/8+1)(enα/8+eα/2).

enα/4

GA(r1  . . .   rl) =

1

2l e−(|r1|+...+|rl|)

and FA(cid:48) is the map:

FA(cid:48)(D  α  R) =

(cid:18)

argmaxi(f (D  i) +

lRi
αn

)  f (D  1) +

lR1
αn

  f (D  2) +

lR2
αn

  . . .   f (D  l) +

10

(cid:19)

lRl
αn

For the same value of R1  . . .   Rl  if i∗ = i on input dataset D and if i∗ = i(cid:48) on input dataset D(cid:48) 
then  |f (D  i) − f (D  i(cid:48))| ≤ 1

n; this implies that

|q(FA(cid:48)(D  α  R)  V ) − q(FA(cid:48)(D(cid:48)  α  R)  V )| = |ti − ti(cid:48)| = |f (D  i) − f (D(cid:48)  i(cid:48))| ≤ 1
n
with probability 1 over GA(cid:48). Thus the training stability condition holds for β1 = 1 and δ = 0.

6.2 Output Perturbation Algorithm

We present the output perturbation algorithm for regularized linear classiﬁcation.

Algorithm 4 Output Perturbation for Differentially Private Linear Classiﬁcation
1: Inputs: Regularization parameter λ  training set T = {(xi  yi)  i = 1  . . .   n}  privacy parame-
2: Let G be the following density over Rd: ρG(r) ∝ e−(cid:107)r(cid:107). Draw R ∼ G.
3: Solve the convex optimization problem:

ter α.

(cid:96)(w  xi  yi)

(4)

w∗ = argminw∈Rd

λ(cid:107)w(cid:107)2 +

1
2

1
n

4: Output w∗ + 2

λαn R.

6.3 Case Study: Histogram Density Estimation

n(cid:88)

i=1

(cid:90)

m(cid:88)

i=1

Our second case study is developing an end-to-end differentially private solution for histogram-
based density estimation. In density estimation  we are given n samples x1  . . .   xn drawn from
an unknown density f  and our goal is to build an approximation ˆf to f. In a histogram density
estimator  we divide the range of the data into equal-sized bins of width h; if ni out of n of the input

samples lie in bin i  then ˆf is the density function: ˆf (x) =(cid:80)1/h

hn · 1(x ∈ Bin i).

i=1

ni

A critical parameter while constructing the histogram density estimator is the bin size h. There is
much theoretical literature on how to choose h – see [16  26] for surveys. However  the choice
of h is usually data-dependent  and in practice  the optimal h is often determined by building a
histogram density estimator for a few different values of h  and selecting the one which has the best
performance on held-out validation data.
The most popular measure to evaluate the quality of a density estimator is the L2-distance or the
Integrated Square Error (ISE) between the density estimate and the true density:

(cid:107) ˆf − f(cid:107)2 =

( ˆf (x) − f (x))2dx =

f 2(x)dx +

ˆf 2(x)dx − 2

f (x) ˆf (x)dx

(5)

x

x

x

x

f is typically unknown  so the ISE cannot be computed exactly. Fortunately it is still possible to
compare multiple density estimates based on this distance. The ﬁrst term in the right hand side of
Equation 5 depends only on f  and is equal for all ˆf. The second term is a function of ˆf only and can
thus be computed. The third term is 2Ex∼f [ ˆf (x)]  and even though it cannot be computed exactly
without knowledge of f  we can estimate it based on a held out validation dataset. Thus  given a
density estimator ˆf and a validation dataset V = {z1  . . .   zm}  we will use the following function
to evaluate the quality of ˆf on V :

(cid:90)

(cid:90)

(cid:90)

(cid:90)

q( ˆf   V ) = −

ˆf 2(x)dx +

2
m

x

ˆf (zi)

(6)

A higher value of q indicates a smaller distance (cid:107) ˆf − f(cid:107)2  and thus a higher quality density estimate.
For other measures  see [6].
In the sequel  we assume that the data lies in the interval [0  1] and that this interval is known in
advance. For ease of notation  we also assume without loss of generality that 1
h is an integer. For

11

ease of exposition  we conﬁne ourselves to one-dimensional data  although the general techniques
can be easily extended to higher dimensions. Given n samples and a bin size h  several works 
including [7  19  27  28  20  29  14] have shown different ways of constructing and sampling from
differentially private histograms. The most basic approach is to construct a non-private histogram
and then add Laplace noise to each cell  followed by some post-processing. Algorithm 5 presents a
variant of a differentially private histogram density estimator due to [19] in our framework.

h do

Algorithm 5 Differentially Private Histogram Density Estimator
1: Inputs: Bin size h (such that 1/h is an integer)  data T = {x1  . . .   xn}  privacy parameter α.
2: for i = 1  . . .   1
3:
4:
5: end for

(cid:104) i−1
i ˜ni. Return the density estimator: ˆf (x) =(cid:80)1/h

j=1 1(xj ∈ Ii)  and let ˜ni = max(cid:0)0  ni + 2Ri

Draw Ri independently from the standard Laplace density: ρG(r) = 1
Let Ii =

. Deﬁne: ni =(cid:80)n

6: Let ˜n =(cid:80)

h˜n · 1(x ∈ Ii)

2 e−|r|.

(cid:1).

h   i

(cid:17)

˜ni

i=1

h

α

The following theorem shows stability guarantees on the differentially private histogram density
estimator described in Algorithm 5.
Theorem 6 (Stability of Private Histogram Density Estimator) Let H = {h1  . . .   hk} be a set
of bin sizes  and let hmin = mini hi. For any ﬁxed δ  if the sample size n ≥ 1 + 2 ln(4k/δ)
  then 
the validation score q in Equation 6 is (β1  β2  δ
k )-Stable with respect to Algorithm 5 and H for:
√
β1 =

√
α

  where: ν = 2 ln(4k/δ)
hmin

β2 = 2
hmin

(1−ν)hmin

hmin

nα

.

6

 

6.4 Proofs of Theorems 1  2 and 3

We now present the proofs of Theorems 1  2 and 3. Our proofs involve ideas similar to those in
the analysis of the multiplicative weights update method for answering a set of linear queries in a
differentially private manner [13].
Let A(D) denote the output of Algorithm 1 when the input is a sensitive dataset D = (T  V )  where
T is the training part and V is the validation part. Let D(cid:48) = (T (cid:48)  V ) where T and T (cid:48) differ in the
value of a single individual  and let D(cid:48)(cid:48) = (T  V (cid:48)) where V and V (cid:48) differ in the value of a single
individual. The proof of Theorem 1 is a consequence of the following two lemmas.
Lemma 1 Suppose that the conditions in Theorem 1 hold. Then  for all D = (T  V )  all D(cid:48) =
(T (cid:48)  V )  such that T and T (cid:48) differ in the value of a single individual  and for any set of outcomes S:
(7)

Pr(A(D) ∈ S) ≤ eα2 Pr(A(D(cid:48)) ∈ S) + δ

Lemma 2 Suppose that the conditions in Theorem 1 hold. Then  for all D = (T  V )  all D(cid:48)(cid:48) =
(T  V (cid:48)) such that V and V (cid:48) differ in the value of a single individual  and for any set of outcomes S 
(8)

Pr(A(D) ∈ S) ≤ eα2 Pr(A(D(cid:48)(cid:48)) ∈ S) + δ

PROOF: (Of Lemma 1) Let S = (I  C)  where I ⊆ [k] is a set of indices and C ⊆ C. Let E be the
event that all of R1  . . .   Rk lie in the set Σ. We will ﬁrst show that conditioned on E  for all i  it
holds that:
(9)
Since Pr(E) ≥ 1 − δ  from the conditions in Theorem 1  for any subset I of indices  we can write:

Pr(i∗ = i|D  E) ≤ eα2 Pr(i∗ = i|D(cid:48)  E)

Pr(i∗ ∈ I|D) ≤ Pr(i∗ ∈ I|D  E) Pr(E) + (1 − Pr(E))

≤ eα2 Pr(i∗ ∈ I|D(cid:48)  E) Pr(E) + δ
≤ eα2 Pr(i∗ ∈ I  E|D(cid:48)) + δ
≤ eα2 Pr(i∗ ∈ I|D(cid:48)) + δ

12

(10)

We will now prove Equation 9. For this purpose  we adopt the following notation. We use the
notation Z\i to denote the random variables Z1  . . .   Zi−1  Zi+1  . . .   Zk and z\i to denote the set of
values z1  . . .   zi−1  zi+1  . . .   zk. We also use the notation h(·) to represent the density induced on
the random variables Z1  . . .   Zk by Algorithm 1. In addition  we use the notation R to denote the
vector (R1  . . .   Rk). We ﬁrst ﬁx a value z\i for Z\i  and a value of R such that R1  . . .   Rk all lie
in Σ  and consider the ratio of probabilities:

Pr(i∗ = i|Z\i = z\i  D  R)
Pr(i∗ = i|Z\i = z\i  D(cid:48)  R)

Observe that this ratio of probabilities is equal to:

Pr(Zi + q(F (T  θi  α1  Ri)  V ) ≥ supj(cid:54)=i zj + q(F (T  θj  α1  Rj)  V ))
Pr(Zi + q(F (T (cid:48)  θi  α1  Ri)  V ) ≥ supj(cid:54)=i zj + q(F (T (cid:48)  θj  α1  Rj)  V ))

which is in turn equal to:

Pr(Zi ≥ supj(cid:54)=i zj + q(F (T  θj  α1  Rj)  V ) − q(F (T  θi  α1  Ri)  V ))
Pr(Zi ≥ supj(cid:54)=i zj + q(F (T (cid:48)  θj  α1  Rj)  V ) − q(F (T (cid:48)  θi  α1  Ri)  V ))

Observe that from the stability condition 

|(q(F (T  θj  α1  Rj)  V ) − q(F (T  θi  α1  Ri)  V )) − (q(F (T (cid:48)  θj  α1  Rj)  V ) − q(F (T (cid:48)  θi  α1  Ri)  V ))|
≤ |q(F (T  θj  α1  Rj)  V ) − q(F (T (cid:48)  θj  α1  Rj)  V (cid:48))| + |q(F (T  θi  α1  Ri)  V ) − q(F (T (cid:48)  θi  α1  Ri)  V )|
≤ 2β1
n

≤ 2β

Thus  the ratio of the probabilities is at most the ratio Pr(Zi ≥ γ)/ Pr(Zi ≥ γ + 2β) where
γ = supj(cid:54)=i zj +q(F (T  θj  α1  Rj)  V )−q(F (T  θi  α1  Ri)  V )  which is at most eα2 by properties
of the exponential distribution. Thus  we have established that for all z\i  for all R in Σk 

Pr(i∗ = i|Z\i = z\i  D  R) ≤ eα2 · Pr(i∗ = i|Z\i = z\i  D(cid:48)  R)

Equation 9 follows by integrating over z\i and R. The lemma follows. (cid:3)
PROOF:(Of Lemma 2) Let S = (I  C)  where I ⊆ [k] is a set of indices and C ⊆ C. Let E be the
event that all of R1  . . .   Rk lie in Σ. We will ﬁrst show that conditioned on E  for all i  it holds that:
(11)
Since Pr(E) ≥ 1 − δ  from the conditions in Theorem 1  for any subset I of indices  we can write:

Pr(i∗ = i|D  E) ≤ eα2 Pr(i∗ = i|D(cid:48)(cid:48)  E)

Pr(i∗ ∈ I|D) ≤ Pr(i∗ ∈ I|D  E) Pr(E) + (1 − Pr(E))

≤ eα2 Pr(i∗ ∈ I|D(cid:48)(cid:48)  E) Pr(E) + δ
≤ eα2 Pr(i∗ ∈ I  E|D(cid:48)(cid:48)) + δ
≤ eα2 Pr(i∗ ∈ I|D(cid:48)(cid:48)) + δ

(12)

We will now focus on showing Equation 11. We ﬁrst consider the case when event E holds  that is 
Rj ∈ R  for j = 1  . . .   k. In this case  the stability deﬁnition and the conditions of the theorem
imply that for all θj ∈ Θ 

|q(F (T  θj  α1  Rj)  V ) − q(F (T  θj  α1  Rj)  V (cid:48))| ≤ β2
m

≤ β

(13)

In what follows  we use the notation Z\i to denote the random variables Z1  . . .   Zi−1  Zi+1  . . .   Zk
and z\i to denote the set of values z1  . . .   zi−1  zi+1  . . .   zk. We also use the notation h(·) to
represent the density induced on the random variables Z1  . . .   Zk by Algorithm 1. In addition  we
use the notation R to denote the vector (R1  . . .   Rk). We ﬁrst ﬁx a value z\i for Z\i  and a value of
R such that E holds  and consider the ratio of probabilities:

Pr(i∗ = i|Z\i = z\i  D  R)
Pr(i∗ = i|Z\i = z\i  D(cid:48)(cid:48)  R)

13

Observe that this ratio of probabilities is equal to:

Pr(Zi + q(F (T  θi  α1  Ri)  V ) ≥ supj(cid:54)=i zj + q(F (T  θj  α1  Rj)  V ))
Pr(Zi + q(F (T  θi  α1  Ri)  V (cid:48)) ≥ supj(cid:54)=i zj + q(F (T  θj  α1  Rj)  V (cid:48)))

which is in turn equal to:

Pr(Zi ≥ supj(cid:54)=i zj + q(F (T  θj  α1  Rj)  V ) − q(F (T  θi  α1  Ri)  V ))
Pr(Zi ≥ supj(cid:54)=i zj + q(F (T  θj  α1  Rj)  V (cid:48)) − q(F (T  θi  α1  Ri)  V (cid:48)))

Observe that from Equation 13 
|(q(F (T  θj  α1  Rj)  V )−q(F (T  θi  α1  Ri)  V ))−(q(F (T  θj  α1  Rj)  V (cid:48))−q(F (T  θi  α1  Ri)  V (cid:48)))| ≤ 2β2
m
Thus  the ratio of the probabilities is at most the ratio Pr(Zi ≥ γ)/ Pr(Zi ≥ γ + 2β) for γ =
supj(cid:54)=i zj + q(F (T  θj  α1  rj)  V ) − q(F (T  θi  α1  ri)  V )  which is at most eα2 by properties of
the exponential distribution. Thus  we have established that when R ∈ Σk  for all j 

≤ 2β

Pr(i∗ = i|Z\i = z\i  D  R)
Pr(i∗ = i|Z\i = z\i  D(cid:48)(cid:48)  R)

≤ eα2

Thus for any such R  we can write:

Pr(i∗ = i|D  R)
Pr(i∗ = i|D(cid:48)(cid:48)  R)

=

(cid:82)
(cid:82)

z\i

z\i

Equation 11 now follows by integrating R over E. (cid:3)

Pr(i∗ = i|Z\i = z\i  D  R)h(z\i)dz\i
Pr(i∗ = i|Z\i = z\i  D(cid:48)(cid:48)  R)h(z\i)dz\i

≤ eα2

PROOF:(Of Theorem 1) The proof of Theorem 1 follows from a combination of Lemmas 1 and 2.
(cid:3)

PROOF:(Of Theorem 2) The proof of Theorem 2 follows from privacy composition; Theorem 1
ensures that Step (2) of Algorithm 2 is (α2  δ)-differentially private; moreover the training procedure
T is α1-differentially private. The theorem follows by composing these two results. (cid:3)

PROOF:(Of Theorem 3) Observe that:

(cid:18)

(cid:19)

(cid:18)

(cid:19)

Pr

q(hi∗   V ) < max
1≤i≤k

q(hi  V ) − 2β log(k/δ0)

α2

≤ Pr

By properties of the exponential distribution  for any ﬁxed j  Pr(Zj ≥ log(k/δ0)
theorem follows by an Union Bound. (cid:3)

α2

∃j s.t. Zj ≥ log(k/δ0)
α2
) ≤ δ0

k . Thus the

6.5 Proof of Theorem 4
PROOF: (Of Theorem 4 for Output Perturbation) Let T and T (cid:48) be two training sets which differ in
a single labelled example ((xn  yn) vs. (x(cid:48)
n))  and let w∗(T ) and w∗(T (cid:48)) be the solutions to the
regularized convex optimization problem in Equation 1 when the inputs are T and T (cid:48) respectively.
We observe that for ﬁxed λ  α and R 

n  y(cid:48)

F (T  λ  α  R) − F (T (cid:48)  λ  α  R) = w∗(T ) − w∗(T (cid:48))

When the training sets are T and T (cid:48)  the objective functions in the regularized convex optimization
problems are both λ-strongly convex  and they differ by 1
n)). Combining
this fact with Lemma 1 of [4]  and using the fact that (cid:96) is 1-Lipschitz  we have that for all λ and R 

n ((cid:96)(w  xn  yn)−(cid:96)(w  x(cid:48)

n  y(cid:48)

(cid:107)F (T  λ  α  R) − F (T (cid:48)  λ  α  R)(cid:107) ≤ 2
λn

Since g is L-Lipschitz  this implies that for any ﬁxed validation set V   and for all λ  α and R 

|q(F (T  λ  α  R)  V ) − q(F (T (cid:48)  λ  α  R)  V )| ≤ 2L
λn

(14)

14

Now let V and V (cid:48) be two validation sets that differ in the value of a single labelled example
(¯xm  ¯ym). Since g ≥ 0 for all inputs  for any such V and V (cid:48)  and for a ﬁxed Λ  α and R 
|q(F (T  λ  α  R)  V ) − q(F (T  λ  α  R)  V (cid:48))| ≤ gmax

m   where

gmax = sup

(x y)∈X

g(F (T  λ  α  R)  x  y)

By deﬁnition  gmax ≤ g∗. Moreover  as g is L-Lipschitz 

gmax ≤ L · (cid:107)F (T  λ  α  R)(cid:107)

Now  let E be the event that (cid:107)R(cid:107) ≤ d log(dk/δ). From Lemma 4 of [4]  Pr(E) ≥ 1 − δ/k. Thus 
provided E holds  we have that:
(cid:107)F (T  λ  α  R)(cid:107) ≤ (cid:107)w∗(cid:107) +

d log(dk/δ)

d log(dk/δ)

d log(dk/δ)

(cid:19)

(cid:18)

1 +

≤ 1
λ

+

λαn

λαn

=

1
λ

where the bound on (cid:107)w∗(cid:107) follows from an application of Lemma 1 of [4] on the functions 1
and 1
for all λ 

2 λ(cid:107)w(cid:107)2
i=1 (cid:96)(w  xi  yi). This implies that provided E holds  for all training sets T   and

2 λ(cid:107)w(cid:107)2 + 1

n

(cid:80)n

|q(F (T  λ  α  R)  V ) − q(F (T  λ  α  R)  V (cid:48))| ≤ L
λm

1 +

d log(dk/δ)

nα

(15)

The theorem now follows from a combination of Equations 14 and 15  and the deﬁnition of g∗. (cid:3)
PROOF: (Of Theorem 4 for Objective Perturbation) Let T and T (cid:48) be two training sets which differ in
a single labelled example (xn  yn). We observe that for a ﬁxed R and λ  the objective of the regular-
n)).
ized convex optimization problem in Equation 2 differs in the term 1
Combining this with Lemma 1 of [4]  and using the fact that (cid:96) is 1-Lipschitz  we have that for all λ 
α  R 

n ((cid:96)(w  xn  yn) − (cid:96)(w  x(cid:48)

n  y(cid:48)

(cid:107)F (T  λ  α  R) − F (T (cid:48)  λ  α  R)(cid:107) ≤ 2
λn

Since g is L-Lipschitz  this implies that for any ﬁxed validation set V   and for all λ and r 

|q(F (T  λ  α  R)  V ) − q(F (T (cid:48)  λ  α  R)  V )| ≤ 2L
λn

(16)

Now let V and V (cid:48) be two validation sets that differ in the value of a single labelled example
(¯xm  ¯ym). Since g ≥ 0  for any such V and V (cid:48)  |q(F (T  λ  α  R)  V ) − q(F (T  λ  α  R)  V (cid:48))| ≤
gmax
m   where

nα

(cid:19)

(cid:18)

By deﬁnition gmax ≤ g∗. Moreover  as g is L-Lipschitz 

gmax = sup

(x y)∈X

g(F (T  λ  α  R)  x  y)

gmax ≤ L · (cid:107)F (T  λ  α  R)(cid:107)

Let E be the event that (cid:107)R(cid:107) ≤ d log(dk/δ). From Lemma 4 of [4]  Pr(E) ≥ 1 − δ/k. Thus 
provided E holds  we have that:

(cid:107)F (T  λ  α  R)(cid:107) ≤ 1 + (cid:107)R(cid:107)/(αn)

λ

≤ 1
λ

1 +

d log(dk/δ)

nα

(cid:19)

(cid:18)

(cid:18)

This implies that provided E holds  for all training sets T   and for all λ 

|q(F (T  λ  α  R)  V ) − q(F (T  λ  α  R)  V (cid:48))| ≤ L
λm

1 +

d log(dk/δ)

nα

The theorem now follows from a combination of Equations 16 and 17  and the deﬁnition of g∗. (cid:3)

15

(cid:19)

(17)

6.6 Proof of Theorem 6
Lemma 3 (Concentration of Sum of Laplace Random Variables) Let Z1  . . .   Zs be s ≥ 2 iid
standard Laplace random variables  and let Z = Z1 + . . . + Zs. Then  for any θ 

(cid:18)

(cid:19)−s

Pr(Z ≥ θ) ≤

1 − 1
s

√
e−θ/

√
s ≤ 4e−θ/

s

PROOF: The proof follows from using the method of generating functions. The generating function
1−t2   for |t| ≤ 1. As Z1  . . .   Zs are
for the standard Laplace distribution is: ψ(X) = E[etX ] = 1
independently distributed  the generating function for Z is E[etZ] = (1− t2)−s. Now  we can write:

Pr(Z ≥ θ) = Pr(etZ ≥ etθ)

≤ E[etZ]
etθ = e−tθ · (1 − t2)−s
(cid:18)

(cid:19)−s
√
e−θ/
s )s ≥ 1
4. (cid:3)

Plugging in t = 1√

s  we get that:

s

Pr(Z ≥ θ) ≤

1 − 1
s
The lemma follows by observing that for s ≥ 2  (1 − 1
PROOF: (Of Theorem 6) Let V = {z1  . . .   zm} be a validation dataset  and let V (cid:48) be a valida-
tion dataset that differs from V in a single sample (zm vs z(cid:48)
m). We use the notation R to denote
the sequence of values R = (R1  R2  . . .   R1/h). Given an input sample T   a bin size h  a pri-
vacy parameter α  and a sequence R  we use the notation ˆfT h α R to denote the density estimator
F (T  h  α  R). For all such T   all h  all α and all R  we can write:

|q(F (T  h  α  R)  V ) − q(F (T  h  α  R)  V (cid:48))| =

2
m
≤ 2
m

( ˆfT h α R(zm) − ˆfT h α R(z(cid:48)
· maxi ˜ni

≤ 2
mh

h˜n

m))

(18)

For a ﬁxed value of h  we deﬁne the following event E:

Ri ≥ − ln(4k/δ)√

h

1/h(cid:88)

i=1

1/h(cid:88)

i=1

Using the symmetry of Laplace random variables and Lemma 3  we get that Pr(E) ≥ 1 − δ/k. We
observe that provided the event E holds 

˜n ≥ n −

Ri ≥ n − 2 ln(4k/δ)

√

α

h

≥ n(1 − ν)

(19)

Let T and T (cid:48) be two input datasets that differ in a single sample (xn vs x(cid:48)
value of α  and a sequence R  and for these ﬁxed values  we use the notation ˜ni and ˜n(cid:48)

value of ˜ni in Algorithm 5 when the inputs are T and T (cid:48) respectively. Similarly  we use ˜n =(cid:80)
and ˜n(cid:48) =(cid:80)

n). We ﬁx a bin size h  a
i to denote the
i ˜ni

i ˜n(cid:48)
i.

For any V   we can write:

q(F (T  h  α  R)  V ) − q(F (T (cid:48)  h  α  R)  V ) =

−

m(cid:88)
1/h(cid:88)

2
m

j=1

h ·

i=1

( ˆfT h α R(zj) − ˆfT (cid:48) h α R(zj))
(cid:18) ˜n2
h2 ˜n2 − ˜n(cid:48)2
h2 ˜n(cid:48)2

(cid:19)

i

i

(20)

We now look at bounding the right hand side of Equation 20 term by term. Suppose T (cid:48) is obtained
rom T by moving a single sample xn from bin a to bin b in the histogram. Then  depending on the
relative values of ˜na and ˜nb  there are four cases:

16

1. ˜n(cid:48)
2. ˜n(cid:48)
3. ˜n(cid:48)
4. ˜n(cid:48)

a = ˜na − 1  ˜n(cid:48)
a = ˜na = 0  ˜n(cid:48)
a = ˜na − 1  ˜n(cid:48)
a = ˜na = 0  ˜n(cid:48)

b = ˜nb + 1. Thus ˜n(cid:48) = ˜n.
b = ˜nb + 1. Thus ˜n(cid:48) = ˜n + 1.
b = ˜nb = 0. Thus ˜n(cid:48) = ˜n − 1.
b = ˜nb = 0. Thus ˜n(cid:48) = ˜n.

In the fourth case  ˆfT h α R = ˆfT (cid:48) h α R  and thus the right hand side of Equation 20 is 0. Moreover 
the second and the third cases are symmetric. We thus focus on the ﬁrst two cases.
In the ﬁrst case  the ﬁrst term in the right hand side of Equation 20 can be written as:

(cid:18) ˜ni

h˜n

(cid:19)(cid:12)(cid:12)(cid:12) =

− ˜n(cid:48)
h˜n(cid:48)

i

(cid:12)(cid:12)(cid:12) 2

· m(cid:88)

1/h(cid:88)

j=1

i=1

1(zj ∈ Ii) · ˜ni − ˜n(cid:48)

i

h˜n

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12) 2

m

· m(cid:88)

1/h(cid:88)

j=1

i=1

1(zj ∈ Ii) ·

≤ 2
h˜n
The second term on the right hand side of Equation 20 can be written as:

· m · 1
h˜n

m
≤ 2
m

(cid:12)(cid:12)(cid:12) 1/h(cid:88)

i=1

(cid:18) ˜n2
h˜n2 − ˜n(cid:48)2
h˜n(cid:48)2

i

i

(cid:19)(cid:12)(cid:12)(cid:12) =

b − (˜na − 1)2 − (˜nb + 1)2

˜n2
a + ˜n2

(cid:12)(cid:12)(cid:12) 2˜na − 2˜nb − 2

h˜n2

h˜n2

(cid:12)(cid:12)(cid:12) ≤ 2

h˜n

=

where the last step follows from the fact that ˜n(cid:48)
hand side of Equation 20 is at most 4
h˜n.
We now consider the second case. The ﬁrst term on the right hand side of Equation 20 can be written
as:

b = ˜nb + 1 ≤ ˜n. Thus  for the ﬁrst case  the right

(cid:18) ˜ni
(cid:18) ˜ni

h˜n

(cid:19)(cid:12)(cid:12)(cid:12)
(cid:19)(cid:12)(cid:12)(cid:12)

i

i

− ˜n(cid:48)
h˜n(cid:48)
− ˜n(cid:48)

˜n

˜n + 1

1(zj ∈ Ii) ·

1(zj ∈ Ii) ·

(cid:12)(cid:12)(cid:12) 2
(cid:12)(cid:12)(cid:12) 2

m

mh

1/h(cid:88)
· m(cid:88)
1/h(cid:88)
· m(cid:88)

j=1

i=1

j=1

i=1

=

≤

· m ·

1

2
hm
·

≤ 2
h

˜n(˜n + 1)

(cid:12)(cid:12)(cid:12) 1/h(cid:88)

i=1

(cid:18) ˜n2
h˜n2 − ˜n(cid:48)2
h˜n(cid:48)2

i

i

(cid:19)(cid:12)(cid:12)(cid:12) =

1

˜n(˜n + 1)

· max(|˜ni(˜n + 1) − ˜ni ˜n| |˜ni(˜n + 1) − ˜n(˜ni + 1)|)

· max(|˜ni| |˜n − ˜ni|) ≤

2

h(˜n + 1)

(cid:18) ˜n2
(cid:88)
h˜n2 −
h˜n2(˜n + 1)2 ·(cid:88)

2˜n + 1

i(cid:54)=b

i

˜n2
i

h(˜n + 1)2

(cid:19)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) ˜n2
h˜n2 − (˜nb + 1)2
(cid:12)(cid:12)(cid:12) (˜nb − ˜n)(2˜nb ˜n + ˜n + ˜nb)

h(˜n + 1)2

h˜n2(˜n + 1)2

+

b

(cid:12)(cid:12)(cid:12)

˜n2

i +

i(cid:54)=b
˜n · 2˜n(˜n + 1)
h˜n2(˜n + 1)2 ≤

4

2˜n + 1
h(˜n + 1)2 +

=

≤

where the last step follows from the fact that max(|˜ni| |˜n − ˜ni|) ≤ ˜n. The second term on the right
hand side of Equation 20 can be written as:

h(˜n + 1)
Thus  in the second case  the right hand side of Equation 20 is at most
h(˜n+1). We observe that the
third case is symmetric to the second case  and thus we can carry out very similar calculations in
h˜n. Thus  we have that for any T and T (cid:48) 
the third case to show that the right hand side is at most 6
provided the event E holds 

6

|q(F (T  h  α  R)  V ) − q(F (T (cid:48)  h  α  R)  V )| ≤ 6
h˜n
The theorem now follows by combining Equation 21 with Equation 19. (cid:3)

(21)

17

6.7 Proof of Theorem 5
Lemma 4 (Parallel construction) Let A = {A1 A2  . . .  Ak} be a list of k independently random-
ized functions  and let Ai be αi-differentially private. Let {D1  D2  . . .   Dk} be k subsets of a set
D such that i (cid:54)= j =⇒ Di ∩ Dj = ∅. Algorithm B(D  A) = (A1(D1) A2(D2)  . . .  Ak(Dk)) is
max1≤i≤k αi-differentially private.
PROOF: Let D  D(cid:48) be two datasets such that their symmetric difference contains one element. We
have that
P (B(D  A) ∈ S)
P (A1(D1) ∈ S1)··· P (Ak(Dk) ∈ Sk)
P (A1(D(cid:48)
P (B(D(cid:48)  A) ∈ S)
k) ∈ Sk)
(22)
by independence of randomness in the Ai. Since i (cid:54)= j =⇒ Di ∩ Dj = ∅  there exists at most one
j. If j does not exist  (22) reduces to e0 ≤ emax1≤i≤k αi. Let j exist  then
index j such that Dj (cid:54)= D(cid:48)

P (B(D  A) ∈ S1 × ··· × Sk)
P (B(D(cid:48)  A) ∈ S1 × ··· × Sk)

=

=

1) ∈ S1)··· P (Ak(D(cid:48)

P (B(D  A) ∈ S)
P (B(D(cid:48)  A) ∈ S)

=

P (Aj(Dj) ∈ Sj)
j) ∈ Sj)
P (Aj(D(cid:48)

≤ eαj ≤ emax1≤i≤k αi 

which concludes the proof. (cid:3)

PROOF: (Theorem 5) We begin by separating task (a) of producing the fi in step 1. from the task
(b) of computing ei in step 2. and selecting i∗ in step 3.
From the parallel construction Lemma 4 it follows that (a) in dataSplit is α-differentially private.
From standard composition of privacy it follows that (a) in alphaSplit is α-differentially private.
Task (b) is for both alphaSplit and dataSplit an application of the exponential mechanism [21] 
which for choosing with a probability proportional to (−ei) yields 2∆-differential privacy  where
∆ is the sensitivity of ei. Since a single change in V can change the number of errors any ﬁxed
classiﬁer can make by at most 1 = ∆  we get that task (b) is α-differentially private for  = α/2.
If T and V are disjoint  we get by parallel construction that both alphaSplit and dataSplit yield
α-differential privacy. If T and V are not disjoint  by standard composition of privacy we get that
both alphaSplit and dataSplit yield 2α-differential privacy.
In Random  the results of step 2. in task (b) are never used in step 3. Step 3 is done without looking
at the input data and does not incur loss of differential privacy. We can therefore simulate Random
by ﬁrst choosing i∗ uniformly at random  and then computing fi at α-differential privacy  which by
standard privacy composition is α-differentially private. (cid:3)

6.8 Experimental selection of regularizer index

18

Figure 2: A summary of 10 times 10-fold cross-validation selection of regularizer index i into Θ
for different privacy levels α. Each point in the ﬁgure represents a summary of 100 data points.
The error bars indiciate a boot-strap sample estimate of the 95% conﬁdence interval of the mean. A
small amount of jitter was added to positions on the x-axes to avoid over-plotting.

19

llllllllllllAdultMagic2460.30.51.02.03.05.00.30.51.02.03.05.0alphalStabilityalphaSplitdataSplitRandomControl,Kamalika Chaudhuri
Staal Vinterbo
Alberto Maria Metelli
Matteo Pirotta
Marcello Restelli
Osbert Bastani
Yewen Pu
Armando Solar-Lezama
Su Young Lee
Choi Sungik
Sae-Young Chung