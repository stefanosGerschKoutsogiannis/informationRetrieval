2019,Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations,In this paper  we propose one novel model for point cloud semantic segmentation which exploits both the local and global structures within the point cloud based onthe contextual point representations. Specifically  we enrich each point represen-tation by performing one novel gated fusion on the point itself and its contextualpoints. Afterwards  based on the enriched representation  we propose one novelgraph pointnet module  relying on the graph attention block to dynamically com-pose and update each point representation within the local point cloud structure.Finally  we resort to the spatial-wise and channel-wise attention strategies to exploitthe point cloud global structure and thereby yield the resulting semantic label foreach point.  Extensive results on the public point cloud databases  namely theS3DIS and ScanNet datasets  demonstrate the effectiveness of our proposed model outperforming the state-of-the-art approaches. Our code for this paper is available at https://github.com/fly519/ELGS.,Exploiting Local and Global Structure for Point

Cloud Semantic Segmentation with Contextual Point

Representations

Xu Wang

Jingming He

College of Computer Science

and Software Engineering

College of Computer Science

and Software Engineering

Shenzhen University

Shenzhen  China

Shenzhen University

Shenzhen  China

wangxu@szu.edu.cn

hejingming519@gmail.com

Lin Ma∗

Tencent AI Lab
Shenzhen  China

forest.linma@gmail.com

Abstract

In this paper  we propose one novel model for point cloud semantic segmentation 
which exploits both the local and global structures within the point cloud based on
the contextual point representations. Speciﬁcally  we enrich each point represen-
tation by performing one novel gated fusion on the point itself and its contextual
points. Afterwards  based on the enriched representation  we propose one novel
graph pointnet module  relying on the graph attention block to dynamically com-
pose and update each point representation within the local point cloud structure.
Finally  we resort to the spatial-wise and channel-wise attention strategies to exploit
the point cloud global structure and thereby yield the resulting semantic label for
each point. Extensive results on the public point cloud databases  namely the
S3DIS and ScanNet datasets  demonstrate the effectiveness of our proposed model 
outperforming the state-of-the-art approaches. Our code for this paper is available
at https://github.com/fly519/ELGS.

1

Introduction

The point cloud captured by 3D scanners has attracted more and more research interests  especially for
the point cloud understanding tasks  including the 3D object classiﬁcation [13  14  10  11]  3D object
detection [21  27]  and 3D semantic segmentation [25  13  14  23  10]. 3D semantic segmentation 
aiming at providing class labels for each point in the 3D space  is a prevalent challenging problem.
First  the points captured by the 3D scanners are usually sparse  which hinders the design of one
effective and efﬁcient deep model for semantic segmentation. Second  the points always appear
unstructured and unordered. As such  the relationship between the points is hard to be captured and
modeled.
As points are not in a regular format  some existing approaches ﬁrst transform the point clouds into
regular 3D voxel grids or collections of images  and then feed them into traditional convolutional
neural network (CNN) to yield the resulting semantic segmentation [25  5  22]. Such a transformation
process can somehow capture the structure information of the points and thereby exploit their
relationships. However  such approaches  especially in the format of 3D volumetric data  require
high memory and computation cost. Recently  another thread of deep learning architectures on point
clouds  namely PointNet [13] and PointNet++ [14]  is proposed to handle the points in an efﬁcient
and effective way. Speciﬁcally  PointNet learns a spatial encoding of each point and then aggregates
all individual point features as one global representation. However  PointNet does not consider the

∗Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

local structures. In order to further exploit the local structures  PointNet++ processes a set of points
in a hierarchical manner. Speciﬁcally  the points are partitioned into overlapping local regions to
capture the ﬁne geometric structures. And the obtained local features are further aggregated into
larger units to generate higher level features until the global representation is obtained. Although
promising results have been achieved on the public datasets  there still remains some opening issues.
First  each point is characterized by its own coordinate information and extra attribute values  i.e.
color  normal  reﬂectance  etc. Such representation only expresses the physical meaning of the point
itself  which does not consider its neighbouring and contextual ones. Second  we argue that the local
structures within point cloud are complicated  while the simple partitioning process in PointNet++
cannot effectively capture such complicated relationships. Third  each point labeling not only depends
on its own representation  but also relates to the other points. Although the global representation is
obtained in PointNet and PointNet++  the complicated global relationships within the point cloud
have not been explicitly exploited and characterized.
In this paper  we propose one novel model for point cloud semantic segmentation. First  for each
point  we construct one contextual representation by considering its neighboring points to enrich its
semantic meaning by one novel gated fusion strategy. Based on the enriched semantic representations 
we propose one novel graph pointnet module (GPM)  which relies on one graph attention block
(GAB) to compose and update the feature representation of each point within the local structure.
Multiple GPMs can be stacked together to generate the compact representation of the point cloud.
Finally  the global point cloud structure is exploited by the spatial-wise and channel-wise attention
strategies to generate the semantic label for each point.

2 Related Work

Recently  deep models have demonstrated the feature learning abilities on computer vision tasks
with regular data structure. However  due to the limitation of data representation method  there are
still many challenges for 3D point cloud task  which is of irregular data structures. According to
the 3D data representation methods  existing approaches can be roughly categorized as 3D voxel-
based [5  25  22  7  9]  multiview-based [18  12]  and set-based approaches [13  14].
3D Voxel-based Approach. The 3D voxel-based methods ﬁrst transform the point clouds into regular
3D voxel grids  and then the 3D CNN can be directly applied similarly as the image or video. Wu et
al. [25] propose the full-voxels based 3D ShapeNets network to store and process 3D data. Due to
the constraints of representation resolution  information loss are inevitable during the discretization
process. Meanwhile  the memory and computational consumption are increases dramatically with
respect to the resolution of voxel. Recently  Oct-Net [16]  Kd-Net [7]  and O-CNN [22] have been
proposed to reduce the computational cost by skipping the operations on empty voxels.
Multiview-based Approach. The multiview-based methods need to render multiple images from the
target point cloud based on different view angle settings. Afterwards  each image can be processed by
the traditional 2D CNN operations [18]. Recently  the multiview image CNN [12] has been applied
to 3D shape segmentation  and has obtained satisfactory results. The multiview-based approaches
help reducing the computational cost and running memory. However  converting the 3D point cloud
into images also introduce information loss. And how to determine the number of views and how to
allocate the view to better represent the 3D shape still remains as an intractable problem.
Set-based Approach. PointNet [13] is the ﬁrst set-based method  which learns the representation
directly on the unordered and unstructured point clouds. PointNet++ [14] relies on the hierarchical
learning strategy to extend PointNet for capturing local structures information. PointCNN [10] is
further proposed to exploit the canonical order of points for local context information extraction.
Recently  there have been several attempts in the literature to model the point cloud as structured
graphs. For example  Qi et al. [15] propose to build a k-nearest neighbor directed graph on top of point
cloud to boost the performance on the semantic segmentation task. SPGraph [8] is proposed to deal
with large scale point clouds. The points are adaptively partitioned into geometrically homogeneous
elements to build a superpoint graph  which is then fed into a graph convolutional network (GCN) for
predicting the semantic labels. DGCNN [24] relies on the edge convolution operation to dynamically
capture the local shapes. RS-CNN [11] extends regular grid CNN to irregular conﬁguration  which
encodes the geometric relation of points to achieve contextual shape-aware learning of point cloud.

2

Figure 1: Our proposed model for the point cloud semantic segmentation  consisting of three fully-
coupled components. The point enrichment not only considers the point itself but also its contextual
points to enrich the corresponding semantic representation. The feature representation relies on
conventional encoder-decoder architecture with lateral connections to learn the feature representation
for each point. Speciﬁcally  the GPM is proposed to dynamically compose and update each point
representation via a GAB module. For the prediction  we resort to both channel-wise and spatial-wise
attentions to exploit the global structure for the ﬁnal semantic label prediction for each point.

These approaches mainly focus on the point local relationship exploitation  and neglect the global
relationship.
Unlike previous set-based methods that only consider the raw coordinate and attribute information of
each single point  we pay more attentions on the spatial context information within neighbor points.
Our proposed context representation is able to express more ﬁne-grained structural information. We
also rely on one novel graph pointnet module to compose and update each point representation within
the local point cloud structure. Moreover  the point cloud global structure information is considered
with the spatial-wise and channel-wise attention strategies.

3 Approach

The point cloud semantic segmentation aims to take the 3D point cloud as input and assign one
semantic class label for each point. We propose one novel model for handling this point cloud
semantic segmentation  as shown in Fig. 1. Speciﬁcally  our proposed network consists of three
components  namely  the point enrichment  the feature representation  and the prediction. These three
components fully couple together  ensuring an end-to-end training manner.
Point Enrichment. To make accurate class prediction for each point within the complicated point
cloud structure  we need to not only consider the information of each point itself but also its
neighboring or contextual points. Different from the existing approaches  relying on the information
of each point itself  such as the geometry  color  etc.  we propose one novel point enrichment layer to
enrich each point representation by taking its neighboring or contextual points into consideration.
With the incorporated contextual information  each point is able to sense the complicated point cloud
structure information. As will be demonstrated in Sec. 4.4  the contextual information  enriching the
semantic information of each point  can help boosting the ﬁnal segmentation performance.
Feature Representation. With the enriched point representation  we resort to the conventional
encoder-decoder architecture with lateral connections to learn the feature representation for each
point. To further exploit local structure information of the point cloud  the GPM is employed in the
encoder  which relies on the GAB to dynamically compose and update the feature representation of
each point within its local regions. The decoder with lateral connections works on the compacted
representation obtained from the encoder  to generate the semantic feature representation for each
point.
Prediction. Based on the obtained semantic representations  we resort to both the channel-wise
and spatial-wise attentions to further exploit the global structure of the point cloud. Afterwards  the
semantic label is predicted for each point.

3

3.1 Point Enrichment

The raw representation of each point is usually its 3D position and associated attributes  such as color 
reﬂectance  surface normal  etc. Existing approaches usually directly take such representation as
input  neglecting its neighboring or contextual information  which is believed to play an essential
role [17] for characterizing the point cloud structure  especially from the local perspective. In this
paper  besides the point itself  we incorporate its neighboring points as its contextual information to
enrich the point semantic representation. With such incorporated contextual information  each point
is aware of the complicated point cloud structure information.
As illustrated in Fig. 2  a point cloud con-
sists of N points  which can be represented as
{P1  P2  ...  PN}  with Pi ∈ RCf denoting the
attribute values of the i-th point  such as posi-
tion coordinate  color  normal  etc. To charac-
terize the contextual information for each point 
k-nearest neighbor set Ni within the local re-
gion centered on i-th point are selected and con-
catenated together  where the contextual repre-
sentation Ri ∈ RkCf of the given point i is as
follows:

Ri = (cid:107)
j∈Ni

Pj.

(1)

Figure 2: The point enrichment process relies on our
proposed gated fusion strategy to enrich the point repre-
sentation by considering the neighbouring and contex-
tual points of each point.

For each point  we have two different representa-
tions  speciﬁcally the Pi and Ri. However  these
two representations are of different dimensions and different characteristics. How to effectively fuse
them together to produce one more representative feature for each point remains an open issue. In
this paper  we propose a novel gated fusion strategy. We ﬁrst feed Pi into one fully-connected (FC)
layer to obtain a new feature vector ˜Pi ∈ RkCf . Afterwards  the gated fusion operation is performed:

ˆPi = gi (cid:12) ˜Pi 
i (cid:12) Ri 
ˆRi = gR

(2)

gi = σ(wiRi + bi) 
gR
i = σ(wR
i ) 
i
i ∈ RkCf×kCf and bi  bR

˜Pi + bR

i ∈ RkCf are the learnable parameters. σ is the non-linear
where wi  wR
sigmoid function. (cid:12) is the element-wise multiplication. The gated fusion aims to mutually absorb
useful and meaningful information of Pi and Ri. And the interactions between Pi and Ri are updated 
yielding ˆPi and ˆRi. As such  the i-th point representation is then enriched by concatenating them
together as ˆPi (cid:107) ˆRi. For easing the following introduction  we will re-use Pi to denote the enriched
representation of the i-th point.

3.2 Feature Representation

Based on the enriched point representation  we rely on one traditional encoder-decoder architecture
with lateral connections to learn the feature representation of each point.

3.2.1 Encoder

Although the enriched point representation has somewhat considered the local structure information 
the complicated relationships within points  especially from the local perspective need to be further
exploited. In order to tackle this challenge  we propose one novel GPM in the encoder  which aims to
learn the composition ability between points and thereby more effectively capture the local structural
information within the point cloud.

Graph Pointnet Module. Same as [14]  we ﬁrst use the sampling and grouping layers to divide
the point set into several local groups. Within each group  the GPM is used to exploit the local
relationships between points  and thereby update the point representation by aggregating the point
information within the local structure.

4

the proposed
As illustrated in Fig. 3 
GPM consists of one multi-layer percep-
tron (MLP) and GAB. The MLP in conven-
tional PointNet [13] and PointNet++ [14]
independently performs on each point to
mine the information within the point it-
self  while neglects the correlations and
relationships among the points. In order
to more comprehensively exploit the point
relationship  we rely on the GAB to aggre-
gate the neighboring point representations
and thereby updated the point representation.
For each obtained local structure obtained by the sampling and grouping layers  GAB [20] ﬁrst
deﬁnes one fully connected undirected graph to measure the similarities between any two points with
such local structures. Given the output feature map G ∈ RCe×Ne of the MLP layer in the GPM
module  we ﬁrst linearly project each point to one common space through a FC layer to obtain new
feature map ˆG ∈ RCe×Ne. The similarity αij between point i and point j is measured as follows:
(3)

Figure 3: The architecture of our proposed GPM  which
stacks MLP and GAB to exploit the point relationships within
the local structure.

αij = ˆGi · ˆGj.

Afterwards  we calculate the inﬂuence factor of point j on point i:
βij = softmaxj(LeakyReLU(αij)) 

(4)
where βij is regarded as the normalized attentive weight  representing how point j relates to point i.
The representation of each point is updated by attentively aggregating the point representations with
reference to βij:

Ne(cid:88)

˜Gi =

βij ˆGj.

(5)

j=1

It can be observed that the GAB dynamically updates the local feature representation by referring to
the similarities between points and captures their relationships. Moreover  in order to preserve the
original information  the point feature after MLP is concatenated with the updated one via one skip
connection through a gated fusion operation  as shown in Fig. 3.
Please note that we can stack multiple GPMs  as shown in Fig. 3  to further exploit the complicated
non-linear relationships within each local structure. Afterwards  one max pooling layer is used to
aggregate the feature map into a one-dimensional feature vector  which not only lowers the dimen-
sionality of the representation  thus making it possible to quickly generate compact representation of
the point cloud  but also help ﬁltering out the unreliable noises.

3.2.2 Decoder

For decoder  we use the same architecture as [14]. Speciﬁcally  we progressively upsample the
compact feature obtained from the encoder until the original resolution. Please note that for preserving
the information generated in the encoder as much as possible  lateral connections are also used.

3.3 Prediction

After performing the feature representation  rich semantic representation for each point is obtained.
Note that our previous operations  including contextual representation and feature representation  only
mine the point local relationships. However  the global information is also important  which needs to
be considered when determining the label for each individual point. For the semantic segmentation
task  two points departing greatly in space may belong to the same semantic category  which can be
jointly considered to mutually enhance their feature representations. Moreover  for high-dimensional
feature representations  the inter-dependencies between feature channels also exist. As such  in order
to capture the global context information for each point  we introduce two attention modules  namely
spatial-wise and channel-wise attentions [4] for modeling the global relationships between points.
Spatial-wise Attention. To model rich global contextual relationships among points  the spatial-wise
attention module is employed to adaptively aggregate spatial contexts of local features. Given the

5

feature map F ∈ RCd×Nd from the decoder  we ﬁrst feed it into two FC layers to obtain two new
feature maps A and B  respectively  where {A  B} ∈ RCd×Nd. Nd is the number of points and Cd is
number of feature channel. The normalized spatial-wise attentive weight vij measures the inﬂuence
factor of point j on point i as follows:

(6)
Afterwards  the feature map F is fed into another FC layer to generate a new feature map D ∈
RCd×Nd. The output feature map ˆF ∈ RCd×Nd after spatial-wise attention is obtained:

vij = softmaxj(Ai · Bj) 

Nd(cid:88)

ˆFi =

(vijDj) + Fi.

(7)

j=1

As such  the global spatial structure information is attentively aggregated with each point representa-
tion.
Channel-wise Attention. The channel-wise attention performs similarly with the spatial-wise
attention  with the channel attention map explicitly modeling the interdependencies between channels
and thereby boosting the feature discriminability. Similar as the spatial-wise attention module  the
output feature map ˜F ∈ RCd×Nd is obtained by aggregating the global channel structure information
with each channel representation.
After summing the feature maps ˆF and ˜F  the semantic label for each point can be obtained with
one additional FC layer. With such attention processes from the global perspective  the feature
representation of each point is updated. As such  the complicated relationships between the points
can be comprehensively exploited  yielding more accurate segmentation results.

4 Experiment

4.1 Experiment Setting

Dataset. To evaluate the performance of proposed model and compare with state-of-the-art  we
conduct experiments on two public available datasets  the Stanford 3D Indoor Semantics (S3DIS)
Dataset [1] and ScanNet Dataset[2]. The S3DIS dataset comes from real scan of the indoor envi-
ronment  including 3D scans of Matterport scanners from 6 areas. There are 271 rooms divided by
room. ScanNet is a point cloud dataset with scanned indoor scenes. It has 22 categories of semantic
tags  with 1513 scenes. ScanNet contains a wide variety of spaces. Each point is annotated with an
instance-level semantic category label.
Implementation Details. The number of neighbor-
ing points k in contextual representation is set as 3 
where the farthest distance for neighboring point is
ﬁxed to 0.06. For feature extraction  a four-layer en-
coder is used  where the spatial scale of each layer is
set as 1024  256  64  and 16  respectively. The GPM
is enabled in the ﬁrst two layers of the encoder  to
exploit the local relationships between points. The
maximum training epochs for S3DIS and ScanNet
are set as 120 and 500  respectively.
Evaluation Metric.
Two widely used metrics 
namely overall accuracy (OA) and mean intersection
of union (mIoU)  are used to measure the semantic
segmentation performance. OA is the prediction accu-
racy of all points. IoU measures the ratio of the area
of overlap to the area of union between the ground
truth and segmentation result. mIoU is the average of
IoU over all categories.
Competitor Methods. For S3DIS dataset  we com-
pare our method with PointNet [13]  PointNet++ [14] 

Table 1: Results of S3DIS dataset on “Area 5”
and over 6 fold in terms of OA and mIoU. † and
‡ indicate that the PointNet performances are
directly copied from [8] and [3]  respectively.
∗ indicates that the PointNet++ performances
are produced with the publicly available code.

PointNet† [13]
SEGCloud [19]
RSNet [6]
PointNet++∗ [14]
SPGraph [8]
Ours
PointNet‡ [13]
SGPN [23]
Engelmann et al. [3]
A-SCN [26]
SPGraph [8]
DGCNN [24]
Ours

mIoU

41.09
48.92
51.93
54.98
58.04
60.06

47.6
50.4
49.7
52.7
62.1
56.1
66.3

Test Area Method

OA

-
-
-

86.43
86.38
88.43

78.5
80.8
81.1
81.6
85.5
84.3
87.6

Area5

6 fold

6

Figure 4: Qualitative results from the S3DIS dataset. All the walls are removed for better visualiza-
tion. From top to bottom are the result of the Point Cloud  PointNet++  Ours  and Ground Truth 
respectively. The segmentation results of our proposed model is closer to the ground truth than that
of PointNet++.

SEGCloud [19]  RSNet [6]  SPGraph [8]  SGPN [23]  Engelmann et al. [3]  A-SCN [26] and
DGCNN [24]. For ScanNet dataset  we compare with 3DCNN [2]  PointNet [13]  PointNet++ [14] 
RSNet [6] and PointCNN [10].

Table 2: The segmentation results of S3DIS dataset in terms of IoU for each category.

Test Area Method

Area5

6fold

PointNet [13]in [8]
SEGCloud [19]
RSNet [6]
PointNet++ [14]
SPGraph [8]
Ours
PointNet [13] in [3]
Engelmann et al. [3]
SPGraph [8]
Ours

ceiling
88.80
90.06
93.34
91.41
89.35
92.80
88.0
90.3
89.9
93.7

ﬂoor
97.33
96.05
98.36
97.92
96.87
98.48
88.7
92.1
95.1
95.6

wall
69.80
69.86
79.18
69.45
78.12
72.65
69.3
67.9
76.4
76.9

4.2 S3DIS Semantic Segmentation

beam column window door
0.05
10.76
23.12
0.00
50.10
0.00
14.48
0.00
61.58
0.00
0.01
28.79
51.6
42.4
51.2
44.7
62.8
68.4
69.0
42.6

3.92
18.37
15.75
16.27
42.81
32.42
23.1
24.2
47.1
46.7

46.26
38.35
45.37
66.13
48.93
68.12
47.5
52.3
55.3
63.9

table
52.61
75.89
65.52
72.32
84.66
74.91
42.0
47.4
73.5
70.1

chair
58.93
70.40
67.87
81.10
75.41
85.12
54.1
58.1
69.2
76.0

sofa
40.28
58.42
22.45
35.12
69.84
55.89
38.2
39.0
63.2
52.8

bookcase

5.85
40.88
52.45
59.67
52.60
64.93
9.6
6.9
45.9
57.2

board
26.38
12.96
41.02
59.45
2.10
47.74
29.4
30.0
8.7
54.8

clutter
33.22
41.60
43.64
51.42
52.22
58.22
35.2
41.9
52.9
62.5

We perform semantic segmentation experiments on the S3DIS dataset to evaluate our performance
in indoor real-world scene scans and perform ablation experiments on this dataset. Same as the
experimental setup in PointNet [13]  we divide each room evenly into several 1m3 cube  with each
uniformly sampleing 4096 points.
Same as [13  3  8]  we perform 6-fold cross validation with micro-averaging. In order to compare
with more methods  we also report the performance on the ﬁfth fold only (Area 5). The OA and
mIoU results are summarized in Table 1. From the results we can see that our algorithm performs
better than other competitor methods in terms of both OA and mIoU metrics.
Besides  the IoU values of each category are summarized in Table 2  it can be observed that our
proposed method achieves the best performance for several categories. For simple shapes such
as “ﬂoor” and “ceiling”  each model performs well  with our approach performing better. This
is mainly due to that the prediction layer of our propose method incorporates the global structure
information between points  which enhances the point representation in the ﬂat area. For categories
with complex local structure  such as “chair” and “bookcase”  our model shows the best performance 

7

Figure 5: Qualitative results from the S3DIS dataset. From top to bottom are the result of the Point
Cloud  PointNet++  Ours  and Ground Truth  respectively. The segmentation results of our proposed
model is closer to the ground truth than that of PointNet++.

since we consider the contextual representation to enhance the relationship between each point and its
neighbors  and use the GPM module to exploit the local structure information. However  the “window”
and “board” categories are more difﬁcult to distinguish from the “wall”  as they are close to the “wall”
in position and appear similarly. The key to distinguishing them is to ﬁnd subtle shape differences
and detect the edges. It can be observed that our model performs well on the “window” and “board”
categories. In order to further demonstrate the effectiveness of our model  some qualitative examples
from S3DIS dataset are provided in Fig. 4 and Fig. 5  demonstrating that our model can yield more
accurate segmentation results.

4.3 ScanNet Semantic Segmentation

For the ScanNet dataset  the number of scenes trained and
tested is 1201 and 312  same as [14  10]. We only use its XYZ
coordinate information. The results are illustrated in Table 3.
Compared with other competitive methods  our proposed model
achieves better performance in terms of both the OA and mIoU
metrics.

4.4 Ablation Study

Table 3: The segmentation
results of ScanNet dataset in
terms of both OA and mIoU.

Method

OA

mIoU

3DCNN [2]
PointNet [13]
PointNet++ [14]
RSNet [6]
PointCNN [10]

Ours

73.0
73.9
84.5

-

85.1
85.3

-
-

-

38.28
39.35

40.6

To validate the contribution of each module in our framework 
we conduct ablation studies to demonstrate their effectiveness.
Detailed experimental results are provided in Table 4.
Contextual Representation Module. After remov-
ing the contextual representation module in the input
layer (denoted as w/o CR)  we can see that the mIoU
value dropped from 60.06 to 56.15  as shown in Ta-
ble 4. Based on the results of each category in Table 5 
some categories have signiﬁcant drops in IoU  such
as “column”  “sofa”  and “door”. The contextual
representation can enhance the point feature of the
categories with complex local structures. We also
replace the gating operation in the contextual repre-
sentation with a simple concatenation operation. Due to the inequality of the two kinds of information 

Table 4: Ablation studies in terms of OA and
mIoU.

Ours(w/o CR)
Ours(w/o GPM)
Ours(w/o AM)
Ours(CR with concatenation)

56.15
57.84
58.67
59.14
60.06

mean IoU

OA

87.91
87.74
87.90
88.21
88.43

Method

Ours

8

Table 5: Ablation studies and analysis in terms of IoU for each category.

Method

Ours(w/o CR)
Ours(w/o AM)
Ours(w/o GPM)

Ours

ceiling
92.62
92.30
92.17
92.80

ﬂoor
98.69
97.91
98.75
98.48

wall
69.65
70.98
72.29
72.65

beam column window door
21.92
0.00
31.58
0.00
19.70
0.00
0.01
28.79

66.02
65.43
72.30
68.12

7.81
21.40
14.89
32.42

table
74.64
75.16
75.78
74.91

chair
84.38
83.26
84.61
85.12

sofa
29.94
48.80
36.48
55.89

bookcase

62.53
62.68
62.73
64.93

board
66.52
56.84
68.01
47.74

clutter
55.19
56.45
54.25
58.22

the OA and mIoU decreases. Thus  the proposed gating operation is useful for fusing the information
of the point itself and its neighborhood.
Graph Pointnet Module. The segmentation performance of our model without GPM module
(denoted as w/o GPM) also signiﬁcantly drops  which indicates that both the proposed GPM and CR
are important for performance improvement. Speciﬁcally  without GPM  the mIoU of the categories 
such as “column” and “sofa” drops signiﬁcantly.
Attention Module. Removing the attention module (denoted as w/o AM) decreases both OA and
mIoU. Moreover  the performances on categories with large ﬂat area  such as “ceiling”  “ﬂoor” 
“wall”  and “window”  signiﬁcantly drop. As aforementioned  the attention module aims to mine
the global relationship between points. Two points within the same category may with large spatial
distance. With the attention module  the features of these points are mutually aggregated.
We further incorporate the proposed CR  AM  and GPM together
with DGCNN [24] for point cloud semantic segmentation  with
the performances illustrated in Table 6. It can be observed that
CR  AM  and GPM can help improving the performances  demon-
strating the effectiveness of each module.
Model Complexity. Table 7 illustrates the model complexity
comparisons. The sample sizes for all the models are ﬁxed as
4096. It can be observed that the inference time of our model
(28ms) is less than the other competitor models  except for Point-
Net (5.3ms) and PointNet++ (24ms). And the model size seems to be identical with other models
except PointCNN  which presents the largest model.
Robustness under Noise. We further demonstrate the robustness
of our proposed model with respect to PointNet++. As for scaling 
when the scaling ratio are 50%  the OA of our proposed model
and PointNet++ on segmentation task decreases by 3.0% and
4.5%  respectively. As for rotation  when the rotation angle is π
10 
the OA of our proposed model and PointNet++ on segmentation
task decreases by 1.7% and 1.0%  respectively. As such  our
model is more robust to scaling while less robust to rotation.

DGCNN
DGCNN+CR
DGCNN+GPM
DGCNN+AM
DGCNN+CR+GPM+AM

Table 7: Model complexity
Size (M)
Model

Table 6: Performances of DGCNN with
our proposed module in terms of OA.

PointNet
DGCNN
PointNet++
RSNet
PointCNN
Ours

OA

84.31
85.35
84.90
85.17
86.07

Model

Time (ms)

5.3
42.0
24.0
60.4
34.4
28.0

1.17
0.99
0.97
6.92
11.51
1.04

5 Conclusion

In this paper  we proposed one novel network for point cloud semantic segmentation. Different
with existing approaches  we enrich each point representation by incorporating its neighboring and
contextual points. Moreover  we proposed one novel graph pointnet module to exploit the point
cloud local structure  and rely on the spatial-wise and channel-wise attention strategies to exploit the
point cloud global structure. Extensive experiments on two public point cloud semantic segmentation
datasets demonstrating the superiority of our proposed model.

Acknowledgments

This work was supported in part by the National Natural Science Foundation of China (Grant
61871270 and Grant 61672443)  in part by the Natural Science Foundation of SZU (grant no.
827000144) and in part by the National Engineering Laboratory for Big Data System Computing
Technology of China.

9

References
[1] Iro Armeni  Ozan Sener  Amir R Zamir  Helen Jiang  Ioannis Brilakis  Martin Fischer  and
Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 1534–1543  2016.

[2] Angela Dai  Angel X Chang  Manolis Savva  Maciej Halber  Thomas Funkhouser  and Matthias
Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition  pages 5828–5839  2017.

[3] Francis Engelmann  Theodora Kontogianni  Alexander Hermans  and Bastian Leibe. Exploring
spatial context for 3D semantic segmentation of point clouds. In Proceedings of the IEEE
International Conference on Computer Vision  pages 716–724  2017.

[4] Jun Fu  Jing Liu  Haijie Tian  Zhiwei Fang  and Hanqing Lu. Dual attention network for
scene segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 3146–3154  2019.

[5] Benjamin Graham  Martin Engelcke  and Laurens van der Maaten. 3D semantic segmentation
with submanifold sparse convolutional networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition  pages 3577–3586  2018.

[6] Qiangui Huang  Weiyue Wang  and Ulrich Neumann. Recurrent slice networks for 3D segmen-
tation of point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 2626–2635  2018.

[7] Roman Klokov and Victor Lempitsky. Escape from cells: Deep Kd-networks for the recognition
of 3D point cloud models. In Proceedings of the IEEE International Conference on Computer
Vision  pages 863–872  2017.

[8] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with
superpoint graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 4558–4567  2018.

[9] Truc Le and Ye Duan. Pointgrid: A deep network for 3D shape understandings. In Proceedings
of the IEEE conference on computer vision and pattern recognition  pages 9204–9214  2018.

[10] Yangyan Li  Rui Bu  Mingchao Sun  Wei Wu  Xinhan Di  and Baoquan Chen. PointCNN:
Convolution on X-transformed points. In Advances in Neural Information Processing Systems 
pages 820–830  2018.

[11] Yongcheng Liu  Bin Fan  Shiming Xiang  and Chunhong Pan. Relation-shape convolutional
neural network for point cloud analysis. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 8895–8904  2019.

[12] Guan Pang and Ulrich Neumann. 3D point cloud object detection with multi-view convolutional
neural network. In 2016 23rd International Conference on Pattern Recognition (ICPR)  pages
585–590. IEEE  2016.

[13] Charles R Qi  Hao Su  Kaichun Mo  and Leonidas J Guibas. PointNet: Deep learning on
point sets for 3D classiﬁcation and segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 652–660  2017.

[14] Charles Ruizhongtai Qi  Li Yi  Hao Su  and Leonidas J Guibas. PointNet++: Deep hierarchical
feature learning on point sets in a metric space. In Advances in Neural Information Processing
Systems  pages 5099–5108  2017.

[15] Xiaojuan Qi  Renjie Liao  Jiaya Jia  Sanja Fidler  and Raquel Urtasun. 3D graph neural networks
for RGBD semantic segmentation. In International Conference on Computer Vision (ICCV) 
pages 5199–5208  2017.

[16] Gernot Riegler  Ali Osman Ulusoy  and Andreas Geiger. Octnet: Learning deep 3D representa-
tions at high resolutions. In Proceedings of the IEEE conference on computer vision and pattern
recognition  pages 3577–3586  2017.

10

[17] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear

embedding. SCIENCE  290:2323–2326  2000.

[18] Baoguang Shi  Song Bai  Zhichao Zhou  and Xiang Bai. DeepPano: Deep panoramic rep-
resentation for 3-D shape recognition. IEEE Signal Processing Letters  22(12):2339–2343 
2015.

[19] Lyne Tchapmi  Christopher Choy  Iro Armeni  JunYoung Gwak  and Silvio Savarese. SEGCloud:
Semantic segmentation of 3D point clouds. In 2017 International Conference on 3D Vision
(3DV)  pages 537–547. IEEE  2017.

[20] Petar Veliˇckovi´c  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Lio  and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations 
2018.

[21] Dominic Zeng Wang and Ingmar Posner. Voting for voting in online point cloud object detection.

In Proceedings of Robotics: Science and Systems  Rome  Italy  July 2015.

[22] Peng-Shuai Wang  Yang Liu  Yu-Xiao Guo  Chun-Yu Sun  and Xin Tong. O-CNN: Octree-based
convolutional neural networks for 3D shape analysis. ACM Transactions on Graphics (TOG) 
36(4):72  2017.

[23] Weiyue Wang  Ronald Yu  Qiangui Huang  and Ulrich Neumann. SGPN: Similarity group
In Proceedings of the IEEE

proposal network for 3D point cloud instance segmentation.
Conference on Computer Vision and Pattern Recognition  pages 2569–2578  2018.

[24] Yue Wang  Yongbin Sun  Ziwei Liu  Sanjay E. Sarma  Michael M. Bronstein  and Justin M.
Solomon. Dynamic graph CNN for learning on point clouds. ACM Transactions on Graphics
(TOG)  2019.

[25] Zhirong Wu  Shuran Song  Aditya Khosla  Fisher Yu  Linguang Zhang  Xiaoou Tang  and
Jianxiong Xiao. 3D shapenets: A deep representation for volumetric shapes. In Proceedings of
the IEEE conference on computer vision and pattern recognition  pages 1912–1920  2015.

[26] Saining Xie  Sainan Liu  Zeyu Chen  and Zhuowen Tu. Attentional shapecontextnet for point
cloud recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 4606–4615  2018.

[27] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-end learning for point cloud based 3D object
detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 4490–4499  2018.

11

,Xu Wang
Jingming He
Lin Ma