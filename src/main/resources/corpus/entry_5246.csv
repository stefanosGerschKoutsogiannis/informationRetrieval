2018,Stochastic Nested Variance Reduction for Nonconvex Optimization,We study finite-sum nonconvex optimization problems  where the objective function is an average of $n$ nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration  our algorithm uses $K+1$ nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions  the proposed algorithm converges to an $\epsilon$-approximate first-order stationary point (i.e.  $\|\nabla F(\mathbf{x})\|_2\leq \epsilon$) within $\tilde O(n\land \epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$\footnote{$\tilde O(\cdot)$ hides the logarithmic factors  and $a\land b$ means $\min(a b)$.} number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG $O(n+n^{2/3}\epsilon^{-2})$ and that of SCSG $O(n\land \epsilon^{-2}+\epsilon^{-10/3}\land n^{2/3}\epsilon^{-2})$. For gradient dominated functions  our algorithm also achieves better gradient complexity than the state-of-the-art algorithms. Thorough experimental results on different nonconvex optimization problems back up our theory.,Stochastic Nested Variance Reduction for

Nonconvex Optimization

Dongruo Zhou

Pan Xu

Department of Computer Science

University of California  Los Angeles

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095
drzhou@cs.ucla.edu

Los Angeles  CA 90095
panxu@cs.ucla.edu

Quanquan Gu

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095

qgu@cs.ucla.edu

Abstract

We study ﬁnite-sum nonconvex optimization problems  where the objective func-
tion is an average of n nonconvex functions. We propose a new stochastic gradient
descent algorithm based on nested variance reduction. Compared with conventional
stochastic variance reduced gradient (SVRG) algorithm that uses two reference
points to construct a semi-stochastic gradient with diminishing variance in each it-
eration  our algorithm uses K + 1 nested reference points to build a semi-stochastic
gradient to further reduce its variance in each iteration. For smooth nonconvex
functions  the proposed algorithm converges to an ✏-approximate ﬁrst-order station-

ary point (i.e.  krF (x)k2  ✏) within eO(n ^ ✏2 + ✏3 ^ n1/2✏2)1 number of
stochastic gradient evaluations. This improves the best known gradient complexity
of SVRG O(n + n2/3✏2) and that of SCSG O(n^ ✏2 + ✏10/3 ^ n2/3✏2). For
gradient dominated functions  our algorithm also achieves better gradient complex-
ity than the state-of-the-art algorithms. Thorough experimental results on different
nonconvex optimization problems back up our theory.

1

Introduction

We study the following nonconvex ﬁnite-sum problem

F (x) :=

min
x2Rd

1
n

nXi=1

fi(x) 

(1.1)

where each component function fi : Rd ! R has L-Lipschitz continuous gradient but may be
nonconvex. A lot of machine learning problems fall into (1.1) such as empirical risk minimization
(ERM) with nonconvex loss. Since ﬁnding the global minimum of (1.1) is general NP-hard [17]  we
instead aim at ﬁnding an ✏-approximate stationary point x  which satisﬁes krF (x)k2  ✏  where
rF (x) is the gradient of F (x) at x  and ✏> 0 is the accuracy parameter.
In this work  we mainly focus on ﬁrst-order algorithms  which only need the function value and
gradient evaluations. We use gradient complexity  the number of stochastic gradient evaluations 

1eO(·) hides the logarithmic factors  and a ^ b means min(a  b).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

to measure the convergence of different ﬁrst-order algorithms.2 For nonconvex optimization  it is
well-known that Gradient Descent (GD) can converge to an ✏-approximate stationary point with
O(n · ✏2) [32] number of stochastic gradient evaluations. It can be seen that GD needs to calculate
the full gradient at each iteration  which is a heavy load when n  1. Stochastic Gradient Descent
(SGD) has O(✏4) gradient complexity to an ✏-approximate stationary point under the assumption
that the stochastic gradient has a bounded variance [15]. While SGD only needs to calculate a mini-
batch of stochastic gradients in each iteration  due to the noise brought by stochastic gradients  its
gradient complexity has a worse dependency on ✏. In order to improve the dependence of the gradient
complexity of SGD on n and ✏ for nonconvex optimization  variance reduction technique was ﬁrstly
proposed in [41  19  46  10  30  6  45  11  16] for convex ﬁnite-sum optimization. Representative
algorithms include Stochastic Average Gradient (SAG) [41]  Stochastic Variance Reduced Gradient
(SVRG) [19]  SAGA [10]  Stochastic Dual Coordinate Ascent (SDCA) [45]  Finito [11] and Batching
SVRG [16]  to mention a few. The key idea behind variance reduction is that the gradient complexity
can be saved if the algorithm use history information as reference. For instance  one representative
variance reduction method is SVRG  which is based on a semi-stochastic gradient that is deﬁned
by two reference points. Since the the variance of this semi-stochastic gradient will diminish
when the iterate gets closer to the minimizer  it therefore accelerates the convergence of stochastic
gradient method. Later on  Harikandeh et al. [16] proposed Batching SVRG which also enjoys fast
convergence property of SVRG without computing the full gradient. The convergence of SVRG under
nonconvex setting was ﬁrst analyzed in [13  44]  where F is still convex but each component function
fi can be nonconvex. The analysis for the general nonconvex function F was done by [38  5]  which
shows that SVRG can converge to an ✏-approximate stationary point with O(n2/3 · ✏2) number
of stochastic gradient evaluations. This result is strictly better than that of GD. Recently  Lei et al.
[26] proposed a Stochastically Controlled Stochastic Gradient (SCSG) based on variance reduction 
which further reduces the gradient complexity of SVRG to O(n ^ ✏2 + ✏10/3 ^ (n2/3✏2)). This
result outperforms both GD and SGD strictly. To the best of our knowledge  this is the state-of-art
gradient complexity under the smoothness (i.e.  gradient lipschitz) and bounded stochastic gradient
variance assumptions. A natural and long standing question is:

Is there still room for improvement in nonconvex ﬁnite-sum optimization without making additional

assumptions beyond smoothness and bounded stochastic gradient variance?

In this paper  we provide an afﬁrmative answer to the above question  by showing that the dependence
on n in the gradient complexity of SVRG [38  5] and SCSG [26] can be further reduced. We propose
a novel algorithm namely Stochastic Nested Variance-Reduced Gradient descent (SNVRG). Similar
to SVRG and SCSG  our proposed algorithm works in a multi-epoch way. Nevertheless  the technique
we developed is highly nontrivial. At the core of our algorithm is the multiple reference points-based
variance reduction technique in each iteration. In detail  inspired by SVRG and SCSG  which uses
two reference points to construct a semi-stochastic gradient with diminishing variance  our algorithm
uses K + 1 reference points to construct a semi-stochastic gradient  whose variance decays faster
than that of the semi-stochastic gradient used in SVRG and SCSG.

1.1 Our Contributions

Our major contributions are summarized as follows:
• We propose a stochastic nested variance reduction technique for stochastic gradient method  which

reduces the dependence of the gradient complexity on n compared with SVRG and SCSG.

ﬁrst-order algorithms such as GD  SGD  SVRG and SCSG.

• We show that our proposed algorithm is able to achieve an ✏-approximate stationary point with
eO(n ^ ✏2 + ✏3 ^ n1/2✏2) stochastic gradient evaluations  which outperforms all existing
• As a by-product  when F is a ⌧-gradient dominated function  a variant of our algorithm can achieve
an ✏-approximate global minimizer (i.e.  F (x)  miny F (y)  ✏) within eOn ^ ⌧ ✏1 + ⌧ (n ^
⌧ ✏1)1/2 stochastic gradient evaluations  which also outperforms the state-of-the-art.

2While we use gradient complexity as in [26] to present our result  it is basically the same if we use

incremental ﬁrst-order oracle (IFO) complexity used by [38]. In other words  these are directly comparable.

2

1.2 Additional Related Work

✏2

Since it is hardly possible to review the huge body of literature on convex and nonconvex optimization
due to space limit  here we review some additional most related work on accelerating nonconvex
(ﬁnite-sum) optimization.
Acceleration by high-order smoothness assumption With
only Lipschitz continuous gradient assumption  Carmon et al.
[9] showed that the lower bound for both deterministic and
stochastic algorithms to achieve an ✏-approximate stationary
point is ⌦(✏2). With high-order smoothness assumptions 
i.e.  Hessian Lipschitzness  Hessian smoothness etc.  a series
of work have shown the existence of acceleration. For in-
stance  Agarwal et al. [1] gave an algorithm based on Fast-PCA
which can achieve an ✏-approximate stationary point with gra-

SVRG
SCSG
SNVRG
n

Gradient
Complexity

n1/2
✏2

n2/3
✏2

✏10/3

✏3

✏2

1

n2/3
✏2

Figure 1: Comparison of gradient
complexities.

showed two algorithms based on ﬁnding exact or inexact neg-
ative curvature which can achieve an ✏-approximate stationary

dient complexity eO(n✏3/2 + n3/4✏7/4) Carmon et al. [7  8]
point with gradient complexity eO(n✏7/4). In this work  we

only consider gradient Lipschitz without assuming Hessian Lipschitz or Hessian smooth. Therefore 
our result is not directly comparable to the methods in this category.
Acceleration by momentum The fact that using momentum is able to accelerate algorithms has been
shown both in theory and practice in convex optimization [35  31  18  23  14  32  29  2]. However 
there is no evidence that such acceleration exists in nonconvex optimization with only Lipschitz
continuous gradient assumption [15  27  34  28  24].
If F satisﬁes -strongly nonconvex  i.e. 
r2F ⌫ I  Allen-Zhu [3] proved that Natasha 1  an algorithm based on nonconvex momentum  is
able to ﬁnd an ✏-approximate stationary point in eO(n2/3L2/31/3✏2). Later  Allen-Zhu [3] further
showed that Natasha 2  an online version of Natasha 1  is able to achieve an ✏-approximate stationary
point within eO(✏3.25) stochastic gradient evaluations3.

After our paper was submitted to NIPS and released on arXiv  a paper [12] was released on arXiv after
our work  which independently proposes a different algorithm and achieves the same convergence
rate for ﬁnding an ✏-approximate stationary point.
To give a thorough comparison of our proposed algorithm with existing ﬁrst-order algorithms for
nonconvex ﬁnite-sum optimization  we summarize the gradient complexity of the most relevant
algorithms in Table 1. We also plot the gradient complexities of different algorithms in Figure 1
for nonconvex smooth functions. Note that GD and SGD are always worse than SVRG and SCSG
according to Table 1. In addition  GNC-AGD and Natasha2 needs additional Hessian Lipschitz
condition. Therefore  we only plot the gradient complexity of SVRG  SCSG and our proposed
SNVRG in Figure 1.
Notation: Let A = [Aij] 2 Rd⇥d be a matrix and x = (x1  ...  xd)> 2 Rd be a vector. I denotes an
identity matrix. We use kvk2 to denote the 2-norm of vector v 2 Rd. We use h· ·i to represent the
inner product of two vectors. Given two sequences {an} and {bn}  we write an = O(bn) if there
exists a constant 0 < C < +1 such that an  C bn. We write an =⌦( bn) if there exists a constant
0 < C < +1  such that an  C bn. We use notation eO(·) to hide logarithmic factors. We also make
use of the notation fn . gn (fn & gn) if fn is less than (larger than) gn up to a constant. We use
productive symbolQb
i=a ci to denote caca+1 . . . cb. Moreover  if a > b  we take the product as 1.
We use b·c as the ﬂoor function. We use log(x) to represent the logarithm of x to base 2. a ^ b is a
shorthand notation for min(a  b).

2 Preliminaries

In this section  we present some deﬁnitions that will be used throughout our analysis.

3In fact  Natasha 2 is guaranteed to converge to an (✏ p✏)-approximate second-order stationary point with

eO(✏3.25) gradient complexity  which implies the convergence to an ✏-approximate stationary point.

3

Table 1: Comparisons on gradient complexity of different algorithms. The second column shows the
gradient complexity for a nonconvex and smooth function to achieve an ✏-approximate stationary
point (i.e.  krF (x)k2  ✏). The third column presents the gradient complexity for a gradient
dominant function to achieve an ✏-approximate global minimizer (i.e.  F (x)  minx F (x)  ✏). The
last column presents the space complexity of all algorithms.

gradient dominant

Hessian Lipschitz

Algorithm

GD
SGD

SVRG [38]
SCSG [26]

nonconvex

O n
✏2
O 1
✏4
✏2 
O n2/3
O 1
✏2 
✏10/3 ^ n2/3
eO n
✏1.75
✏3.25
eO 1
eO 1
✏2 
✏3 ^ n1/2

GNC-AGD [8]
Natasha 2 [3]

✏2/3⌘
✏1/2⌘
Deﬁnition 2.1. A function f is L-smooth  if for any x  y 2 Rd  we have

eO⇣n ^ ⌧
eO⇣n ^ ⌧

SNVRG (this paper)

eO(⌧n )
O 1
✏4
eO(n + ⌧n 2/3)
✏ + ⌧n ^ ⌧
✏ + ⌧n ^ ⌧

N/A
N/A

No
No
No
No

Needed
Needed

No

krf (x)  rf (y)k2  Lkx  yk2.

Deﬁnition 2.1 implies that if f is L-smooth  we have for any x  h 2 Rd
L
2 khk2
2.

f (x + h)  f (x) + hrf (x)  hi +

Deﬁnition 2.2. A function f is -strongly convex  if for any x  y 2 Rd  we have

f (x + h)  f (x) + hrf (x)  hi +


2khk2
2.

(2.1)

(2.2)

(2.3)

Deﬁnition 2.3. A function F with ﬁnite-sum structure in (1.1) is said to have stochastic gradients
with bounded variance 2  if for any x 2 Rd  we have

Eikrfi(x)  rF (x)k2

2  2 

(2.4)

where i a random index uniformly chosen from [n] and Ei denotes the expectation over such i.

2 is called the upper bound on the variance of stochastic gradients [26].
Deﬁnition 2.4. A function F with ﬁnite-sum structure in (1.1) is said to have averaged L-Lipschitz
gradient  if for any x  y 2 Rd  we have

Eikrfi(x)  rfi(y)k2

2  L2kx  yk2
2 

(2.5)

where i is a random index uniformly chosen from [n] and Ei denotes the expectation over the choice.
Deﬁnition 2.5. We say a function f is lower-bounded by f⇤ if for any x 2 Rd  f (x)  f⇤.
We also consider a class of functions namely gradient dominated functions [36]  which is formally
deﬁned as follows:
Deﬁnition 2.6. We say function f is ⌧-gradient dominated if for any x 2 Rd  we have

f (x)  f (x⇤)  ⌧ · krf (x)k2
2 

(2.6)

where x⇤ 2 Rd is the global minimum of f.
Note that gradient dominated condition is also known as the Polyak-Lojasiewicz (P-L) condition [36] 
and is not necessarily convex. It is weaker than strong convexity as well as other popular conditions
that appear in the optimization literature [20].

4

Algorithm 1 One-epoch-SNVRG(x0  F  K  M {Tl} {Bl}  B)
1: Input: initial point x0  function F   loop number K  step size parameter M  loop parameters

l=0 g(l)

t

l=1 Tl

l=0 g(l)

0

l=j+1 Tl)  0  j  K}

t1}  xt  r)  0  l  K.

6: x1 = x0  1/(10M ) · v0
8:
9:
10:
11:
12:
13: end for

Tl  l 2 [K]  batch parameters Bl  l 2 [K]  base batch size B > 0.
2: x(l)
0 x0  g(l)
0 0  0  l  K
3: Uniformly generate index set I ⇢ [n] without replacement  |I| = B
0 1/BPi2I rfi(x0)
4: g(0)
5: v0 PK
7: for t = 1  ... QK
l=1 Tl  1 do
r = min{j : 0 = (t mod QK
t } Update_reference_points({x(l)
{x(l)
{g(l)
t } Update_reference_gradients({g(l)
vt PK
xt+1 xt  1/(10M ) · vt
14: xout uniformly random choice from {xt}  where 0  t <QK
15: T =QK
16: Output: [xout  xT ]
17: Function: Update_reference_points({x(l)
old  0  l  r  1; x(l)
new x(l)
18: x(l)
19: return {x(l)
new}
20: Function: Update_reference_gradients({g(l)
new g(l)
21: g(l)
22: for r  l  K do
Uniformly generate index set I ⇢ [n] without replacement  |I| = Bl
23:
new 1/BlPi2I⇥rfi(x(l)
g(l)
24:
25: end for
26: return {g(l)
3 The Proposed Algorithm

new )⇤
new)  rfi(x(l1)

old}  x  r)
new x  r  l  K

old} {x(l)

new}  r)

old  0  l < r

new}.

t1} {x(l)

t }  r)  0  l  K.

l=1 Tl

In this section  we present our nested stochastic variance reduction algorithm  namely  SNVRG.
One-epoch-SNVRG: We ﬁrst present the key component of our main algorithm  One-epoch-
SNVRG  which is displayed in Algorithm 1. The most innovative part of Algorithm 1 attributes to the
K + 1 reference points and K + 1 reference gradients. Note that when K = 1  Algorithm 1 reduces
to one epoch of SVRG algorithm [19  38  5]. To better understand our One-epoch SNVRG algorithm 
it would be helpful to revisit the original SVRG which is a special case of our algorithm. For the
ﬁnite-sum optimization problem in (1.1)  the original SVRG takes the following updating formula

xt+1 = xt  ⌘vt = xt  ⌘rF (ex) + rfit(xt)  rfit(ex) 

where ⌘> 0 is the step size  it is a random index uniformly chosen from [n] andex is a snapshot for
xt after every T1 iterations. There are two reference points in the update formula at xt: x(0)
t =ex
and x(1)
t = xt. Note thatex is updated every T1 iterations  namely ex is set to be xt only when (t
mod T1) = 0. Moreover  in the semi-stochastic gradient vt  there are also two reference gradients
and we denote them by g(0)
t )rfit(x(0)
t ).
Back to our One-epoch-SNVRG  we can deﬁne similar reference points and reference gradients as
l=1 Tl  1  each point xt has K + 1

t = rfit(xt)rfit(ex) = rfit(x(1)

that in the special case of SVRG. Speciﬁcally  for t = 0  . . .  QK

t = rF (ex) and g(1)

5

(3.1)

1

l=0 g(l)

t

t

Tk.

g(0)
t =

rfi(x0) 

t when x(l)

1

BXi2I

t = x0 and x(K)

reference points {x(l)

t = xtl with index tl deﬁned as

t }  l = 0  . . .   K  which is set to be x(l)
KYk=l+1

tl =
k=l+1 Tk⌫ ·
QK
t = xt for all t = 0  . . .  QK
Specially  note that we have x(0)
l=1 Tl  1. Similarly  xt
t }  which can be deﬁned based on the reference points {x(l)
also has K + 1 reference gradients {g(l)
t }:
BlXi2Il⇥rfi(x(l)
)⇤  l = 1  . . .   K 
t )  rfi(x(l1)
g(l)
t =
(3.2)

x(0)
Reference point
t
g(0)
Reference gradient
t
For t1 = 1  . . .   T1
x(1)
Reference point
t
g(1)
Reference gradient
t

where I  Il are random index sets with |I| = B |Il| = Bl and are uniformly generated from
[n] without replacement. Based on the reference points and reference gradients  we then update
xt+1 = xt  1/(10M )· vt  where vt =PK
and M is the step size parameter. The illustration

at each iteration. Fortunately  due
t = g(l)
t1
t has been updated as is suggested by Line 24 in Algorithm 1.

of reference points and gradients of SNVRG is displayed in Figure 2.
We remark that it would be a huge waste for us to re-evaluate g(l)
t
to the fact that each reference point is only updated after a long period  we can maintain g(l)
and only need to update g(l)
SNVRG: Using One-epoch-SNVRG (Algorithm 1) as a build-
ing block  we now present our main algorithm: Algorithm 2
for nonconvex ﬁnite-sum optimization to ﬁnd an ✏-approximate
stationary point. At each iteration of Algorithm 2  it executes
One-epoch-SNVRG (Algorithm 1) which takes zs1 as its input
and outputs [ys  zs]. We choose yout as the output of Algorithm
2 uniformly from {ys}  for s = 1  . . .   S.
SNVRG-PL: In addition  when function F in (1.1) is gradient
dominated as deﬁned in Deﬁnition 2.6 (P-L condition)  it has
been proved that the global minimum can be found by SGD
[20]  SVRG [38] and SCSG [26] very efﬁciently. Following
a similar trick used in [38]  we present Algorithm 3 on top of
Algorithm 2  to ﬁnd the global minimum in this setting. We call
Algorithm 3 SNVRG-PL  because gradient dominated condition
is also known as Polyak-Lojasiewicz (PL) condition [36].
Space complexity: We brieﬂy compare the space complexity
between our algorithms and other variance reduction based algo-
rithms. SVRG and SCSG needs O(d) space complexity to store
one reference gradient  SAGA [10] needs to store reference gra-
dients for each component functions  and its space complexity is
O(nd) without using any trick. For our algorithm SNVRG  we
need to store K reference gradients  thus its space complexity
is O(Kd). In our theory  we will show that K = O(log log n). Therefore  the space complexity of

......
For tK1 = 1  . . .   TK1
Reference point
g(K1)
Reference gradient
t
For tK = 1  . . .   TK
x(K)
Reference point
t
g(K)
Reference gradient
t
update
xt+1 = xt  ⌘

Figure 2: Illustration of reference
points and gradients.

x(K1)
t

g(l)
t

KXl=0

t

……

our algorithm is actually eO(d)  which is almost comparable to that of SVRG and SCSG.
{Tl}  {Bl}  batch B  S.

Algorithm 2 SNVRG
1: Input: initial point z0  function F   K  M 

Algorithm 3 SNVRG-PL
1: Input: initial point z0  function F   K  M 

{Tl}  {Bl}  batch B  S  U.

2: for s = 1  . . .   S do
denote P = (F  K  M {Tl} {Bl}  B)
3:
[ys  zs] One-epoch-SNVRG(zs1 P)
4:
5: end for
6: Output: Uniformly choose yout from {ys}.
4 Main Theory
In this section  we provide the convergence analysis of SNVRG.

2: for u = 1  . . .   U do
3:
4:
5: end for
6: Output: zout = zU.

denote Q = (F  K  M {Tl} {Bl}  B  S)
zu = SNVRG(zu1 Q)

6

4.1 Convergence of SNVRG

We ﬁrst analyze One-epoch-SNVRG (Algorithm 1) and provide a particular choice of parameters.
Lemma 4.1. Suppose that F has averaged L-Lipschitz gradient  in Algorithm 1  suppose B  2 and
let the number of nested loops be K = log log B. Choose the step size parameter as M = 6L. For
the loop and batch parameters  let T1 = 2  B1 = 6K · B and

 

Tl = 22l2

Bl = 6Kl+1 · B/22l1
for all 2  l  K. Then the output of Algorithm 1 [xout  xT ] satisﬁes
B1/2 · E⇥F (x0)  F (xT )⇤ +
2

2  C✓ L

EkrF (xout)k2

 

B · 1(B < n)◆

(4.1)

l=1 Tl  C = 600 is a constant

within 1_ (7B log3 B) stochastic gradient computations  where T =QK
and 1(·) is the indicator function.
The following theorem shows the gradient complexity for Algorithm 2 to ﬁnd an ✏-approximate
stationary point with a constant base batch size B.
Theorem 4.2. Suppose that F has averaged L-Lipschitz gradient and stochastic gradients with
bounded variance 2. In Algorithm 2  let B = n ^ (2C 2/✏2)   S = 1 _ (2CLF /(B1/2✏2)) and
C = 600. The rest parameters (K  M {Bl} {Tl}) are chosen the same as in Lemma 4.1. Then the
output yout of Algorithm 2 satisﬁes EkrF (yout)k2
✏2 ^ n◆ 2

✏2  2
stochastic gradient computations  where F = F (z0)  F ⇤.
Remark 4.3. If we treat 2  L and F as constants  and assume ✏ ⌧ 1  then (4.2) can be simpliﬁed
to eO(✏3 ^ n1/2✏2). This gradient complexity is strictly better than O(✏10/3 ^ n2/3✏2)  which is
achieved by SCSG [26]. Speciﬁcally  when n . 1/✏2  our proposed SNVRG is faster than SCSG
by a factor of n1/6; when n & 1/✏2  SNVRG is faster than SCSG by a factor of ✏1/3. Moreover 
SNVRG also outperforms Natasha 2 [3] which attains eO(✏3.25) gradient complexity and needs the

✏2 ^ n1/2◆

O✓ log3✓ 2

additional Hessian Lipschitz condition.

2  ✏2 with less than

✏2 ^ n +

(4.2)

LF

4.2 Convergence of SNVRG-PL

We now consider the case when F is a ⌧-gradient dominated function. In general  we are able to ﬁnd
an ✏-approximate global minimizer of F instead of only an ✏-approximate stationary point. Algorithm
3 uses Algorithm 2 as a component.
Theorem 4.4. Suppose that F has averaged L-Lipschitz gradient and stochastic gradients with
bounded variance 2  F is a ⌧-gradient dominated function. In Algorithm 3  let the base batch size
B = n ^ (4C1⌧ 2/✏)  the number of epochs for SNVRG S = 1 _ (2C1⌧ L/B1/2) and the number
of epochs U = log(2F /✏). The rest parameters (K  M {Bl} {Tl}) are chosen as the same in
Lemma 4.1. Then the output zout of Algorithm 3 satisﬁes E⇥F (zout)  F ⇤⇤  ✏ within
✏ 1/2◆

O✓ log3✓n ^

+ ⌧Ln ^

✏ ◆ log

✏ n ^

⌧ 2
✏

(4.3)

⌧ 2

⌧ 2

F

stochastic gradient computations  where F = F (z0)  F ⇤
Remark 4.5. If we treat 2  L and F as constants  then the gradient complexity in (4.3) turns
into eO(n ^ ⌧ ✏1 + ⌧ (n ^ ⌧ ✏1)1/2). Compared with nonconvex SVRG [39] which achieves
eO(n + ⌧n 2/3) gradient complexity  our SNVRG-PL is strictly better than SVRG in terms of the
eOn ^ ⌧ ✏1 + ⌧ (n ^ ⌧ ✏1)2/3 gradient complexity  SNVRG-PL also outperforms it by a factor of
(n ^ ⌧ ✏1)1/6.

ﬁrst summand and is faster than SVRG at least by a factor of n1/6 in terms of the second summand.
Compared with a more general variant of SVRG  namely  the SCSG algorithm [26]  which attains

7

If we further assume that F is -strongly convex  then it is easy to verify that F is also 1/(2)-gradient
dominated. As a direct consequence  we have the following corollary:
Corollary 4.6. Under the same conditions and parameter choices as Theorem 4.4. If we additionally
assume that F is -strongly convex  then Algorithm 3 will outputs an ✏-approximate global minimizer
within

2
✏

+  ·n ^

2

✏ 1/2◆

(4.4)

eO✓n ^

stochastic gradient computations  where  = L/ is the condition number of F .
Remark 4.7. Corollary 4.6 suggests that when we regard  and 2 as constants and set ✏ ⌧ 1 
Algorithm 3 is able to ﬁnd an ✏-approximate global minimizer within eO(n + n1/2) stochastic
gradient computations  which matches SVRG-lep in Katyusha X [4]. Using catalyst techniques [29]
or Katyusha momentum [2]  it can be further accelerated to eO(n + n3/4p)  which matches the

best-known convergence rate [43  4].

5 Experiments

In this section  we compare our algorithm SNVRG with other baseline algorithms on training a
convolutional neural network for image classiﬁcation. We compare the performance of the following
algorithms: SGD; SGD with momentum [37] (denoted by SGD-momentum); ADAM[21]; SCSG [26].
It is worth noting that SCSG is a special case of SNVRG when the number of nested loops K = 1.
Due to the memory cost  we did not compare GD and SVRG which need to calculate the full gradient.
Although our theoretical analysis holds for general K nested loops  it sufﬁces to choose K = 2 in
SNVRG to illustrate the effectiveness of the nested structure for the simpliﬁcation of implementation.
In this case  we have 3 reference points and gradients. All experiments are conducted on Amazon
AWS p2.xlarge servers which comes with Intel Xeon E5 CPU and NVIDIA Tesla K80 GPU (12G
GPU RAM). All algorithm are implemented in Pytorch platform version 0.4.0 within Python 3.6.4.
Datasets We use three image datasets: (1) The MNIST dataset [42] consists of handwritten digits
and has 50  000 training examples and 10  000 test examples. The digits have been size-normalized
to ﬁt the network  and each image is 28 pixels by 28 pixels. (2) CIFAR10 dataset [22] consists of
images in 10 classes and has 50  000 training examples and 10  000 test examples. The digits have
been size-normalized to ﬁt the network  and each image is 32 pixels by 32 pixels. (3) SVHN dataset
[33] consists of images of digits and has 531  131 training examples and 26  032 test examples. The
digits have been size-normalized to ﬁt the network  and each image is 32 pixels by 32 pixels.
CNN Architecture We use the standard LeNet [25]  which has two convolutional layers with 6 and
16 ﬁlters of size 5 respectively  followed by three fully-connected layers with output size 120  84 and
10. We apply max pooling after each convolutional layer.
Implementation Details & Parameter Tuning We did not use the random data augmentation which
is set as default by Pytorch  because it will apply random transformation (e.g.  clip and rotation) at
the beginning of each epoch on the original image dataset  which will ruin the ﬁnite-sum structure
of the loss function. We set our grid search rules for all three datasets as follows. For SGD  we
search the batch size from {256  512  1024  2048} and the initial step sizes from {1  0.1  0.01}.
For SGD-momentum  we set the momentum parameter as 0.9. We search its batch size from
{256  512  1024  2048} and the initial learning rate from {1  0.1  0.01}. For ADAM  we search the
batch size from {256  512  1024  2048} and the initial learning rate from {0.01  0.001  0.0001}. For
SCSG and SNVRG  we choose loop parameters {Tl} which satisfy Bl ·Ql
j=1 Tj = B automatically.
In addition  for SCSG  we set the batch sizes (B  B1) = (B  B/b)  where b is the batch size
ratio parameter. We search B from {256  512  1024  2048} and we search b from {2  4  8}. We
search its initial learning rate from {1  0.1  0.01}. For our proposed SNVRG  we set the batch
sizes (B  B1  B2) = (B  B/b  B/b2)  where b is the batch size ratio parameter. We search B from
{256  512  1024  2048} and b from {2  4  8}. We search its initial learning rate from {1  0.1  0.01}.
Following the convention of deep learning practice  we apply learning rate decay schedule to each
algorithm with the learning rate decayed by 0.1 every 20 epochs. We also conducted experiments
based on plain implementation of different algorithms without learning rate decay  which is deferred
to the appendix.

8

(a) training loss (MNIST)

(b) training loss (CIFAR10)

(c) training loss (SVHN)

(d) test error (MNIST)

(e) test error (CIFAR10)

(f) test error (SVHN)

Figure 3: Experiment results on different datasets with learning rate decay. (a) and (d) depict the
training loss and test error (top-1 error) v.s. data epochs for training LeNet on MNIST dataset. (b)
and (e) depict the training loss and test error v.s. data epochs for training LeNet on CIFAR10 dataset.
(c) and (f) depict the training loss and test error v.s. data epochs for training LeNet on SVHN dataset.

We plotted the training loss and test error for different algorithms on each dataset in Figure 3. The
results on MNIST are presented in Figures 3(a) and 3(d); the results on CIFAR10 are in Figures 3(b)
and 3(e); and the results on SVHN dataset are shown in Figures 3(c) and 3(f). It can be seen that
with learning rate decay schedule  our algorithm SNVRG outperforms all baseline algorithms  which
conﬁrms that the use of nested reference points and gradients can accelerate the nonconvex ﬁnite-sum
optimization.
We would like to emphasize that  while this experiment is on training convolutional neural networks 
the major goal of this experiment is to illustrate the advantage of our algorithm and corroborate our
theory  rather than claiming a state-of-the-art algorithm for training deep neural networks.

6 Conclusions and Future Work

In this paper  we proposed a stochastic nested variance reduced gradient method for ﬁnite-sum
nonconvex optimization. It achieves substantially better gradient complexity than existing ﬁrst-order
algorithms. This partially resolves a long standing question that whether the dependence of gradient
complexity on n for nonconvex SVRG and SCSG can be further improved. There is still an open

question: whether eO(n^ ✏2 + ✏3 ^ n1/2✏2) is the optimal gradient complexity for ﬁnite-sum and

stochastic nonconvex optimization problem? For ﬁnite-sum nonconvex optimization problem  the
lower bound has been proved in Fang et al. [12]  which suggests that our algorithm is near optimal up
to a logarithmic factor. However  for general stochastic problem  the lower bound is still unknown.
We plan to derive such lower bound in our future work. On the other hand  our algorithm can also be
extended to deal with nonconvex nonsmooth ﬁnite-sum optimization using proximal gradient [40].

Acknowledgement

We would like to thank the anonymous reviewers for their helpful comments. This research was
sponsored in part by the National Science Foundation IIS-1652539 and BIGDATA IIS-1855099.
We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA
award. The views and conclusions contained in this paper are those of the authors and should not be
interpreted as representing any funding agencies.

9

References
[1] Naman Agarwal  Zeyuan Allenzhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Finding

approximate local minima for nonconvex optimization in linear time. 2017.

[2] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
1200–1205. ACM  2017.

[3] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint

arXiv:1708.08694  2017.

[4] Zeyuan Allen-Zhu. Katyusha x: Practical momentum method for stochastic sum-of-nonconvex

optimization. arXiv preprint arXiv:1802.03866  2018.

[5] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[6] Alberto Bietti and Julien Mairal. Stochastic optimization with variance reduction for inﬁnite
datasets with ﬁnite sum structure. In Advances in Neural Information Processing Systems  pages
1622–1632  2017.

[7] Yair Carmon  John C. Duchi  Oliver Hinder  and Aaron Sidford. Accelerated methods for

non-convex optimization. 2016.

[8] Yair Carmon  John C Duchi  Oliver Hinder  and Aaron Sidford. “convex until proven guilty":
Dimension-free acceleration of gradient descent on non-convex functions. In International
Conference on Machine Learning  pages 654–663  2017.

[9] Yair Carmon  John C Duchi  Oliver Hinder  and Aaron Sidford. Lower bounds for ﬁnding

stationary points of non-convex  smooth high-dimensional functions. 2017.

[10] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems  pages 1646–1654  2014.

[11] Aaron Defazio  Justin Domke  et al. Finito: A faster  permutable incremental gradient method
for big data problems. In International Conference on Machine Learning  pages 1125–1133 
2014.

[12] Cong Fang  Chris Junchi Li  Zhouchen Lin  and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. In Advances in Neural
Information Processing Systems  pages 686–696  2018.

[13] Dan Garber and Elad Hazan. Fast and simple pca via convex optimization. arXiv preprint

arXiv:1509.05647  2015.

[14] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly
convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal
on Optimization  22(4):1469–1492  2012.

[15] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and

stochastic programming. Mathematical Programming  156(1-2):59–99  2016.

[16] Reza Harikandeh  Mohamed Osama Ahmed  Alim Virani  Mark Schmidt  Jakub Koneˇcn`y  and
Scott Sallinen. Stopwasting my gradients: Practical svrg. In Advances in Neural Information
Processing Systems  pages 2251–2259  2015.

[17] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM

(JACM)  60(6):45  2013.

[18] Chonghai Hu  Weike Pan  and James T Kwok. Accelerated gradient methods for stochastic
optimization and online learning. In Advances in Neural Information Processing Systems  pages
781–789  2009.

10

[19] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[20] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases  pages 795–811. Springer  2016.

[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[22] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
[23] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical

Programming  133(1):365–397  2012.

[24] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. Mathematical

programming  pages 1–49  2017.

[25] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[26] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via scsg methods. In Advances in Neural Information Processing Systems  pages 2345–2355 
2017.

[27] Huan Li and Zhouchen Lin. Accelerated proximal gradient methods for nonconvex program-

ming. In Advances in neural information processing systems  pages 379–387  2015.

[28] Qunwei Li  Yi Zhou  Yingbin Liang  and Pramod K Varshney. Convergence analysis of proximal
gradient with momentum for nonconvex optimization. arXiv preprint arXiv:1705.04925  2017.
[29] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimiza-

tion. In Advances in Neural Information Processing Systems  pages 3384–3392  2015.

[30] Julien Mairal. Incremental majorization-minimization optimization with application to large-

scale machine learning. SIAM Journal on Optimization  25(2):829–855  2015.

[31] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming  103

(1):127–152  2005.

[32] Yurii Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers 

2014.

[33] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y Ng.

Reading digits in natural images with unsupervised feature learning.

[34] Courtney Paquette  Hongzhou Lin  Dmitriy Drusvyatskiy  Julien Mairal  and Zaid Har-
chaoui. Catalyst acceleration for gradient-based non-convex optimization. arXiv preprint
arXiv:1703.10993  2017.

[35] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics  4(5):1–17  1964.

[36] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychisli-

tel’noi Matematiki i Matematicheskoi Fiziki  3(4):643–653  1963.

[37] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks 

12(1):145–151  1999.

[38] Sashank J. Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic

variance reduction for nonconvex optimization. pages 314–323  2016.

[39] Sashank J Reddi  Suvrit Sra  Barnabás Póczos  and Alex Smola. Fast incremental method for
smooth nonconvex optimization. In Decision and Control (CDC)  2016 IEEE 55th Conference
on  pages 1971–1977. IEEE  2016.

11

[40] Sashank J Reddi  Suvrit Sra  Barnabas Poczos  and Alexander J Smola. Proximal stochastic
methods for nonsmooth nonconvex ﬁnite-sum optimization. In Advances in Neural Information
Processing Systems  pages 1145–1153  2016.

[41] Nicolas L Roux  Mark Schmidt  and Francis R Bach. A stochastic gradient method with an
In Advances in Neural Information

exponential convergence _rate for ﬁnite training sets.
Processing Systems  pages 2663–2671  2012.

[42] Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2002.

[43] Shai Shalev-Shwartz. Sdca without duality. arXiv preprint arXiv:1502.06177  2015.
[44] Shai Shalev-Shwartz. Sdca without duality  regularization  and individual convexity.

International Conference on Machine Learning  pages 747–754  2016.

In

[45] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  14(Feb):567–599  2013.

[46] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

12

,Nan Du
Yichen Wang
Niao He
Jimeng Sun
Le Song
Ashkan Panahi
Babak Hassibi
Dongruo Zhou
Pan Xu
Quanquan Gu