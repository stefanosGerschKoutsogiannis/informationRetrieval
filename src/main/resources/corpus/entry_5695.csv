2010,Two-Layer Generalization Analysis for Ranking Using Rademacher Average,This paper is concerned with the generalization analysis on learning to rank for information retrieval (IR). In IR  data are hierarchically organized  i.e.  consisting of queries and documents per query. Previous generalization analysis for ranking  however  has not fully considered this structure  and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of algorithms. In this paper  we propose performing generalization analysis under the assumption of two-layer sampling  i.e.  the i.i.d. sampling of queries and the conditional i.i.d sampling of documents per query. Such a sampling can better describe the generation mechanism of real data  and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms. However  it is challenging to perform such analysis  because the documents associated with different queries are not identically distributed  and the documents associated with the same query become no longer independent if represented by features extracted from the matching between document and query. To tackle the challenge  we decompose the generalization error according to the two layers  and make use of the new concept of two-layer Rademacher average. The generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performance of ranking algorithms.,Two-layer Generalization Analysis for Ranking Using

Rademacher Average

Wei Chen∗

Chinese Academy of Sciences

Tie-Yan Liu

Microsoft Research Asia

Zhiming Ma

Chinese Academy of Sciences

chenwei@amss.ac.cn

tyliu@micorsoft.com

mazm@amt.ac.cn

Abstract

This paper is concerned with the generalization analysis on learning to rank for
information retrieval (IR). In IR  data are hierarchically organized  i.e.  consisting
of queries and documents. Previous generalization analysis for ranking  however 
has not fully considered this structure  and cannot explain how the simultaneous
change of query number and document number in the training data will affect the
performance of the learned ranking model. In this paper  we propose performing
generalization analysis under the assumption of two-layer sampling  i.e.  the i.i.d.
sampling of queries and the conditional i.i.d sampling of documents per query.
Such a sampling can better describe the generation mechanism of real data  and
the corresponding generalization analysis can better explain the real behaviors of
learning to rank algorithms. However  it is challenging to perform such analy-
sis  because the documents associated with different queries are not identically
distributed  and the documents associated with the same query become no longer
independent after represented by features extracted from query-document match-
ing. To tackle the challenge  we decompose the expected risk according to the two
layers  and make use of the new concept of two-layer Rademacher average. The
generalization bounds we obtained are quite intuitive and are in accordance with
previous empirical studies on the performances of ranking algorithms.

1

Introduction

Learning to rank has recently gained much attention in machine learning  due to its wide applica-
tions in real problems such as information retrieval (IR). When applied to IR  learning to rank is a
process as follows [16]. First  a set of queries  their associated documents  and the corresponding
relevance judgments are given. Each document is represented by a set of features  measuring the
matching between document and query. Widely-used features include the frequency of query terms
in the document and the query likelihood given by the language model of the document. A rank-
ing function  which combines the features to predict the relevance of a document to the query  is
learned by minimizing a loss function deﬁned on the training data. Then for a new query  the rank-
ing function is used to rank its associated documents according to their predicted relevance. Many
learning to rank algorithms have been proposed  among which the pairwise ranking algorithms such
as Ranking SVMs [12  13]  RankBoost [11]  and RankNet [5] have been widely applied.

To understand existing ranking algorithms  and to guide the development of new ones  people have
studied the learning theory for ranking  in particular  the generalization ability of ranking methods.
Generalization ability is usually represented by a bound of the deviation between the expected and
empirical risks for an arbitrary ranking function in the hypothesis space. People have investigated the
generalization bounds under different assumptions. First  with the assumption that documents are
i.i.d.  the generalization bounds of RankBoost [11]  stable pairwise ranking algorithms like Ranking

∗The work was performed when the ﬁrst author was an intern at Microsoft Research Asia.

1

SVMs [2]  and algorithms minimizing pairwise 0-1 loss [1  9] were studied. We call these general-
ization bounds “document-level generalization bounds”  which converge to zero when the number of
documents in the training set approaches inﬁnity. Second  with the assumption that queries are i.i.d. 
the generalization bounds of stable pairwise ranking algorithms like Ranking SVMs and IR-SVM
[6] and listwise algorithms were obtained in [15] and [14]. We call these generalization bounds
“query-level generalization bounds”. When analyzing the query-level generalization bounds  the
documents associated with each query are usually regarded as a deterministic set [10  14]  and no
random sampling of documents is assumed. As a result  query-level generalization bounds converge
to zero only when the number of queries approaches inﬁnity  no matter how many documents are
associated with them.

While the existing generalization bounds can explain the behaviors of some ranking algorithms  they
also have their limitations. (1) The assumption that documents are i.i.d. makes the document-level
generalization bounds not directly applicable to ranking in IR. This is because it has been widely ac-
cepted that the documents associated with different queries do not follow the same distribution [17]
and the documents with the same query are no longer independent after represented by document-
query matching features. (2) It is not reasonable for query-level generalization bounds to assume
that one can obtain the document set associated with each query in a deterministic manner. Usually
there are many random factors that affect the collection of documents. For example  in the labeling
process of TREC  the ranking results submitted by all TREC participants were put together and then
a proportion of them were selected and presented to human annotators for labeling. In this process 
the number of participants  the ranking result given by each participant  the overlap between dif-
ferent ranking results  the labeling budget  and the selection methodology can all inﬂuence which
documents and how many documents are labeled for each query. As a result  it is more reasonable
to assume a random sampling process for the generation of labeled documents per query.

To address the limitations of previous work  we propose a novel theoretical framework for ranking 
in which a two-layer sampling strategy is assumed. In the ﬁrst layer  queries are i.i.d. sampled
from the query space according to a ﬁxed but unknown probability distribution. In the second layer 
for each query  documents are i.i.d. sampled from the document space according to a ﬁxed but
unknown conditional probability distribution determined by the query (i.e.  documents associated
with different queries do not have the identical distribution). Then  a set of features are extracted
for each document with respect to the query. Note that the feature representations of the documents
with the same query  as random variables  are not independent any longer. But they are conditionally
independent if the query is given. As can be seen  this new sampling strategy removes improper
assumptions in previous work  and can more accurately describe the data generation process in IR.

Based on the framework  we have performed two-layer generalization analysis for pairwise rank-
ing algorithms. However  the task is non-trivial mainly because the two-layer sampling does not
correspond to a typical empirical process: the documents for different queries are not identically
distributed while the documents for the same query are not independent. Thus  the empirical pro-
cess techniques  widely used in previous work on generalization analysis  are not sufﬁcient. To
tackle the challenge  we carefully decompose the expected risk according to the query and docu-
ment layers  and employ a new concept called two-layer Rademacher average. The new concept
accurately describes the complexity in the two-layer setting  and its reduced versions can be used to
derive meaningful bounds for query layer error and document layer error respectively.

According to the generalization bounds we obtained  we have the following ﬁndings: (i) Both more
queries and more documents per query can enhance the generalization ability of ranking methods;
(ii) Only if both the number of training queries and that of documents per query simultaneously
approach inﬁnity  can the generalization bound converge to zero; (iii) Given a ﬁxed size of training
data  there exists an optimal tradeoff between the number of queries and the number of documents
per query. These ﬁndings are quite intuitive and can well explain empirical observations [19].

2 Related Work

2.1 Pairwise Learning to Rank

Pairwise ranking is one of the major approaches to learning to rank  and has been widely adopted in
real applications [5  11  12  13]. The process of pairwise ranking can be described as follows.

2

1 ···   di

mi} and their judgments {yi

Assume there are n queries {q1  q2 ···   qn} in the training data. Each query qi is associated with mi
j ∈ Y. Each document di
j is
documents {di
represented by a set of features xi
j  qi) ∈ X   measuring the matching between document di
j
and query qi. Widely-used features include the frequency of query terms in the document and the
query likelihood given by the language model of the document. For ease of reference  we use z =
(x  y) ∈ X × Y = Z to denote document d since it encodes all the information of d in the learning
process. Then the training set can be denoted as S = {S1 ···   Sn} where Si   {zi
j ∈ Z}j=1 ···  mi
is the document sample for query qi. For a ranking function f : X → R  the pairwise 0-1 loss l0−1
and pairwise surrogate loss lφ are deﬁned as below:

mi}  where yi

1 ···   yi

j = ψ(di

l0−1(z  z0; f ) = I{(y−y0)(f (x)−f (x0))<0} 

lφ(z  z0; f ) = φ(cid:0) − sgn(y − y0) · (f (x) − f (x0))(cid:1) 

(1)
where I{·} is the indicator function and z  z0 are two documents associated with the same query.
When function φ takes different forms  we will get the surrogate loss functions for different al-
gorithms. For example  for Ranking SVMs  RankBoost  and RankNet  function φ is the hinge 
exponential  and logistic functions respectively.

2.2 Document-level Generalization Analysis

In document-level generalization analysis  it is assumed that the documents are i.i.d. sampled from
the document space Z according to P (z).Then the expected risk of pairwise ranking algorithms can
be deﬁned as below 

Rl

Q(f ) ≤

m(m − 1) Xj6=k
where (δ F  n) → 0 as query number n → ∞.

Xi=1

1
n

1

3

Rl

D(f ) =ZZ2

l(z  z0; f )dP 2(z  z0) 

where P 2(z  z0) is the product probability of P (z) on the product space Z 2.
The document-level generalization bound usually takes the following form: with probability at least
1 − δ 

l(zj  zk; f ) + (δ  F   m)  ∀f ∈ F  

Rl

D(f ) ≤

1

m(m − 1) Xj6=k

where (δ F  m) → 0 when document number m → ∞.
As representative work  the generalization bounds for the pairwise 0-1 loss were derived in [1  9]
and the generalization bounds for RankBoost and Ranking SVMs were obtained in [2] and [11].

As aforementioned  the assumption that documents are i.i.d. makes the document-level generaliza-
tion bounds not directly applicable to ranking in IR. Even if the assumption holds  the document-
level generalization bounds still cannot be used to explain existing pairwise ranking algorithms.
Actually  according to the document-level generalization bound  what we can obtain is: with proba-
bility at least 1− δ  Rl
average of the pairwise losses on all the document pairs. This is clearly not the real empirical risk
of ranking in IR  where documents associated with different queries cannot be compared with each
other  and pairs are constructed only by documents associated with the same query.

+ (δ  F  P mi)  ∀f ∈ F . The empirical risk is the

j  zi0
P mi(P mi−1)

P(i j)6=(i0  k) l(zi

D(f ) ≤

k ;f )

2.3 Query-level Generalization Analysis

In existing query-level generalization analysis [14]  it is assumed that each query qi  represented by
a deterministic document set Si with the same number of documents (i.e. mi ≡ m)  is i.i.d. sampled
from the space Z m. Then the expected risk can be deﬁned as follows 

Rl

Q(f ) =ZZm

1

m(m − 1) Xj6=k

l(zj  zk; f )dP (z1  · · ·   zm).

The query-level generalization bound usually takes the following form: with probability at least
1 − δ 

n

l(zi

j  zi

k; f ) + (δ  F   n)  ∀f ∈ F  

As representative work  the query-level generalization bounds for stable pairwise ranking algorithms
such as Ranking SVMs and IR-SVM and listwise ranking algorithms were derived in [15] 1 and [14].
As mentioned in the introduction  the assumption that each query is associated with a deterministic
set of documents is not reasonable. The fact is that many random factors can inﬂuence what kinds of
documents and how many documents are labeled for each query. Due to this inappropriate assump-
tion  the query-level generalization bounds are sometimes not intuitive. For example  when more
labeled documents are added to the training set  the generalization bounds of stable pairwise rank-
ing algorithms derived in [15] do not change and the generalization bounds of some of the listwise
ranking algorithms derived in [14] get even looser.

3 Two-Layer Generalization Analysis

In this section  we introduce the concepts of two-layer data sampling and two-layer generalization
ability for ranking. These concepts can help describe the data generation process and explain the
behaviors of learning to rank algorithms more accurately than previous work.

3.1 Two-Layer Sampling in IR

1  yi

mi   yi

1) ···   (di

When applying learning to rank techniques to IR  a training set is needed. The creation of such a
training set is usually as follows. First  queries are randomly sampled from query logs of search
engines. Then for each query  documents that are potentially relevant to the query are sampled (e.g. 
using the strategy in TREC [8]) from the entire document repository and presented to human anno-
tators. Human annotators make relevance judgment to these documents  according to the matching
between them and the query. Mathematically  we can represent the above process in the following
manner. First  queries Q = {q1 ···   qn} are i.i.d. sampled from the query space Q according to
distribution P (q). Second  for each query qi  its associated documents and their relevant judgments
mi )} are i.i.d. sampled from the document space D according to a conditional
{(di
j is then rep-
distribution P (d|qi) where mi is the number of sampled documents. Each document di
resented by a set of matching features  i.e.  xi
j  qi)  where ψ is a feature extractor. Following
j = ψ(di
the notation rules in Section 2.1  we use zi
j) to represent document di
j and its label  and
j = (xi
j  yi
j}j=1 ···  mi are
j}j=1 ···  mi. Note that although {di
denote the training data for query qi as Si = {zi
j}j=1 ···  mi are no longer independent because they share the
i.i.d. samples  random variables {zi
same query qi. Only if qi is given  we can regard them as independent of each other.
We call the above data generation process two-layer sampling  and denote the training data generated
in this way as (Q  S)  where Q is the query sample and S = {Si}i=1 ···  n is the document sample.
The two-layer sampling process can be illustrated using Figure 1.



P (·|q) 




2  y1
2)
2  y2
2)
...
2   yn
2 )

1  y1
1)
1  y2
1)
...
1   yn
1 )

(dn

mn   yn

mn )

(d2

m2   y2

m2 )




(d1

m1   y1
. . .

m1 )

q1
q2
...
qn

. . .
. . .

...

. . .

...

. . .

. . .

...

(d1
(d2

(dn

(Q  P )

Figure 1: Two-layer sampling

Note that two-layer sampling has signiﬁcant difference from the sampling strategies used in pre-
vious generalization analysis. (i) As compared to the sampling in document-level generalization
analysis  two-layer sampling introduces the sampling of queries  and documents associated with

1In [15]  although a similar sampling strategy to the two-layer sampling is mentioned  the generalization
analysis  however  does not consider the independent sampling at the document layer. As a result  the general-
ization bound they obtained is a query-level generalization bound  but not a two-layer generalization bound.

4

(d1
(d2

(dn

z1
1
z2
1
...
zn
1

z1
2
z2
2
...
zn
2

z = (φ(q  d)  y) 


. . .
. . .

...

z1
m1
. . .

...

. . .

. . .

z2
m2

. . .

...
zn
mn




(P  P ) →

P1
P2
...
Pn

→
→

...

→

z1
1
z2
1
...
zn
1

z1
2
z2
2
...
zn
2

. . .
. . .

...

. . .




z1
m
z2
m
...
zn
m




Figure 2: (n  m)-sampling

different queries are sampled according to different conditional distributions. (ii) As compared to
the sampling in query-level generalization analysis  two-layer sampling considers the sampling of
documents for each query.

To some extent  the aforementioned two-layer sampling has relationship with directly sampling from
the product space of query and document  and the (n  m)-sampling proposed in [4]. However  as
shown below  they also have signiﬁcant differences. Firstly  it is clear that directly sampling from
the product space of query and document does not describe the real data generation process. Fur-
thermore  even if we sample a large number of documents in this way  it is not guaranteed that we
can have sufﬁcient number of documents for each single query. Secondly  comparing Figure 1 with
Figure 2 (which illustrate (n  m)-sampling)  we can easily ﬁnd: (i) in (n  m)-sampling  tasks (cor-
responding to queries) have the same number (i.e.  m) of elements (corresponding to documents) 
however  in two-layer sampling  queries can be associated with different numbers of documents;
(ii) in (n  m)-sampling  all the elements are i.i.d.  however  in two-layer sampling documents (if
represented by matching features) associated with the same query are not independent of each other.

3.2 Two-Layer Generalization Ability

With the probabilistic assumption of two-layer sampling  we deﬁne the expected risk for pairwise
ranking as follows 

Rl(f ) =ZQZZ2

l(z  z0; f )dP (z  z0|q)dP (q) 

(2)

where P (z  z0|q) is the product probability of P (z|q) on the product space Z 2.
Deﬁnition 1. We say that an ERM learning process with loss l in hypothesis space F has two-layer
generalization ability  if with probability at least 1 − δ 

where ˆRl
0 iff query number n and document number per query mi simultaneously approach inﬁnity.

k; f )  and (δ F  n  m1 ···   mn) →

mi(mi−1) Pj6=k l(zi

n;m1 ···  mn (f ; S) + (δ  F   n  m1    · · ·   mn)  ∀f ∈ F  
n Pn

Rl(f ) ≤ ˆRl
n;m1 ···  mn (f ; S) = 1

j  zi

i=1

1

In the next section  we will show our theoretical results on the two-layer generalization abilities of
typical pairwise ranking algorithms.

4 Main Theoretical Result

In this section  we show our results on the two-layer generalization ability of ERM learning with
pairwise ranking losses (either pairwise 0-1 loss or pairwise surrogate losses). As prerequisites  we
recall the concept of conventional Rademacher averages (RA)[3].
Deﬁnition 2. For sample {x1  . . .   xm}  the RA of l ◦ F is deﬁned as follows  Rm(l ◦ F) =
Eσ hsupf∈F (cid:12)(cid:12)(cid:12)
i   where σ1  . . .   σm are independent Rademacher random vari-

j=1 σjl(xj; f )(cid:12)(cid:12)(cid:12)

ables independent of data sample.

m Pm

2

With the above deﬁnitions  we have the following theorem  which describes when and how the
two-layer generalization bounds of pairwise ranking algorithms converge to zero.
Theorem 1. Suppose l is the loss function for pairwise ranking. Assume 1) l ◦ F is bounded by M 
2) E [Rm(l ◦ F)] ≤ D(l ◦ F  m)  then with probability at least 1 − δ  for ∀f ∈ F
Rl(f ) ≤ ˆRl

n;m1 ···  mn (f ) + D(l ◦ F   n) +s 2M 2 log( 4

2M 2 log 4
δ

D(l ◦ F   b

mi
2

min2

1
n

δ )

+

n

.

n

Xi=1

c) +vuut

n

Xi=1

5

Remark: The condition of the existence of upper bounds for E [Rm(l ◦ F)] can be satisﬁed in
many situations. For example  for ranking function class F that satisﬁes V C( ˜F ) = V   where
˜F = {f (x  x0) = f (x) − f (x0); f ∈ F}  V C(·) denotes the VC dimension  and |f (x)| ≤ B  it
has been proved that D(l0−1 ◦ F  m) = c1pV /m and D(lφ ◦ F  m) = c2Bφ0(B)pV /m in [3  9] 
where c1 and c2 are both constants.

4.1 Proof of Theorem 1

n(f ) − ˆRl

n(f ) + ˆRl

n(f ) = 1

n;m1 ···  mn (f ) 

Note that the proof of Theorem 1 is non-trivial because documents generated by two-layer sampling
are neither independent nor identically distributed  as aforementioned. As a result  the two-layer
sampling does not correspond to an empirical process and classical proving techniques in statisti-
cal learning are not sufﬁcient for the proof. To tackle the challenge  we decompose the two-layer
expected risk as follows:
Rl(f ) = ˆRl
n Pn

n;m1 ···  mn (f ) + Rl(f ) − ˆRl
i=1 RZ 2 l(z  z0; f )dP (z  z0|qi). We call Rl(f ) − ˆRl

n(f ) query-layer error and
n;m1 ···  mn(f ) document-layer error. Then  inspired by conventional RA [3]  we propose

where ˆRl
ˆRl
n(f )− ˆRl
a concept called two-layer RA to describe the complexity of sample (Q  S).
Deﬁnition 3. For two-layer sample (Q  S)  the two-layer RA of l ◦ F is deﬁned as follows 

  

Rn;m1 ···  mn (l ◦ F (Q  S)) = Eσ
f ∈F(cid:12)(cid:12)(cid:12)
sup

j} are independent Rademacher random variables independent of data sample.

where {σi
If
(Q  S) = {qi; zi  z0i}i=1 ···  n  we call its expected two-layer RA  i.e.  EQ S [Rn;2 ···  2(l ◦ F(Q  S))] 
document-layer reduced two-layer RA. If (q  S) = {q; z1 ···   zm}  we call its conditional expected
two-layer RA  i.e.  ES|q [R1;m(l ◦ F(q  S))]  query-layer reduced two-layer RA.
Based on the concept of two-layer RA  we can derive meaningful bounds for the two-layer expected
risk. In Section 4.1.1  we prove the query-layer error bound by using document-layer reduced two-
layer RA; and in Section 4.1.2  we prove the document-layer error bound by using query-layer
reduced two-layer RA. Combining the two bounds  we can prove Theorem 1 in Section 4.1.3.

bmi/2c+j; f )(cid:12)(cid:12)(cid:12)

Xi=1

Xj=1

σi
jl(zi

bmi/2c

j  zi

bmi/2c

2
n

1

n

4.1.1 Query-Layer Error Bounds

As for the query-layer error bound  we have the following theorem.
Theorem 2. Assume l ◦ F is bounded by M  then with probability at least 1 − δ 

Rl(f ) − ˆRl

n(f ) ≤ EQ S [Rn;2 ···  2(l ◦ F (Q  S))] +r 2M 2 log(2/δ)

n

  ∀f ∈ F .

n

δ )

Proof. We deﬁne a function Lf as follows: Lf (q) = RZ 2 l(z  z0; f )dP 2(z|q). Since q1 ···   qn
are i.i.d. sampled  Lf (q1) ···   Lf (qn) are also i.i.d.. Denote G1(Q) = supf∈F (cid:12)(cid:12)(cid:12)
n(f )(cid:12)(cid:12)(cid:12)
.
Since l ◦ F is bounded by M  by the McDiarmid’s inequality  we have G1(Q) ≤ E [G1(Q)] +
q 2M 2 log( 2
. By introducing a ghost query sample ˜Q = { ˜q1 ···   ˜qn}  we have
Lf ( ˜qi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f ∈F(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
E [G1(Q)] = EQ"sup
Xi=1
Further assuming that there are virtual document samples {zi  z0i}i=1 ···  n and { ˜zi  ˜z0i}i=1 ···  n for
[l( ˜zi  ˜z0i; f )].
query samples Q and ˜Q  we have Lf (qi) = Ezi z0
Substitute Lf (qi) and Lf ( ˜qi) into inequality 3  we obtain the following result:

Lf (qi) −Z Lf (q)dP (q)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i|qi[l(zi  z0i; f ); Lf ( ˜qi)] = E

f ∈F(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Q  ˜Q"sup

Rl(f ) − ˆRl

# ≤ E

Xi=1

Xi=1

Lf (qi) −

˜zi  ˜z0

i| ˜qi

1
n

1
n

1
n

n

n

n

# (3)

n

1
n

f ∈F(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
E[G1(Q)] = EQ Q0"sup
Xi=1
f ∈F(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
i|qi  ˜qi"sup

≤ Eqi  ˜qi

i  ˜zi  ˜z0

zi z0

E

1
n

E

zi z0

i  ˜zi  ˜z0

i|qi  ˜qi(cid:16)l(zi  z0

i; f ) − l( ˜zi  ˜z0

n

Xi=1(cid:16)l(zi  z0

i; f ) − l( ˜zi  ˜z0

According to the deﬁnition of document-layer reduced two-layer RA  Theorem 2 is proved.

#

i; f )(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f ∈F(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
# = EQ S Eσ"sup

i; f )(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

σil(zi  z0

2
n

n

Xi=1

#

i; f )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

6

4.1.2 Document-layer Error Bound

In order to obtain the bound for document-layer error  we consider the fact that documents are
independent if the query sample is given. Then for any given query sample  we can obtain the
following theorem by concentration inequality and symmetrization.
Theorem 3. Denote G(S)   supf∈F ( ˆRn(f ) − ˆRn;m1 ···  mn(f )) and assume l ◦ F is bounded by
M  then we have:

PnG(S) ≤

1
n

n

Xi=1

ESi|qi [R1;mi (l ◦ F (qi  Si))] +vuut

n

Xi=1

2M 2 log (2/δ)

min2

(cid:12)(cid:12)(cid:12)

Qo ≤ 1 − δ.

(4)

Proof. First  we prove the bounded difference property for G(S).2 Given query sample Q  all the
documents in the document sample will become independent. Denote S0 as the document sample
obtained by replacing document zi0
j0

in S with a new document ˜zi0
j0

. It is clear that

sup

S S0(cid:12)(cid:12)G(S) − G(S0)(cid:12)(cid:12) ≤ sup
f ∈F Pk6=j0(cid:12)(cid:12)(l(zi0

≤ sup
S S0

sup

S S0

sup

f ∈F(cid:12)(cid:12)

ˆRn;m1 ···  mn (f ; S) − ˆRn;m1 ···  mn (f ; S0)(cid:12)(cid:12)

  zi0

  zi0

k ; f ) − l(˜zi0
j0

j0
nmi0 (mi0 − 1)

k ; f )(cid:12)(cid:12)

≤

2M
mi0 n

.

Then by the McDiarmid’s inequality  with probability at least 1 − δ  we have

G(S) ≤ ES|Q[G(S)] +vuut

n

Xi=1

2M 2 log (2/δ)

min2

.

(5)

Second  inspired by [9] we introduce permutations to convert the non-sum-of-i.i.d. pairwise loss to
a sum-of-i.i.d. form. Assume Smi is the symmetric group of degree mi and πi ∈ Smi(i = 1 ···   n)
which permutes the mi documents associates with qi. Since documents associated with the same
query follow the identical distribution  we have 

n

1
n

Xi=1
where p

1

mi(mi − 1) Xj6=k

l(zi

j  zi

k; f )

p
=

1
n

n

Xi=1

1

mi! Xπi

1

bmi/2c

bmi/2c

Xj=1

= means identity in distribution. Deﬁne a function ˜G(Si) on each Si as follows:

˜G(Si) = sup

f ∈F(cid:12)(cid:12)(cid:12)

1

bmi/2c

bmi/2c

Xj=1

l(zi

j  zi
b

mi

2 c+j; f ) − Ez z0|qi(cid:2)l(z  z0; f )(cid:3)(cid:12)(cid:12)(cid:12)

.

We can see that ˜G(Si) does not contain any document pairs that share a common document. By
using Eqn.(6)  we can decompose ES|Q[G(S)] into the sum of ESi|qi[ ˜G(Si)] as below:

l(zi

πi(j)  zi

πi(bmi/2c+j); f ) 

(6)

ES|Q[G(S)] ≤

1
n

−

1
b mi
2 c

bmi/2c

Xj=1

n

Xi=1

1

mi! Xπi

l(Z i

πi(j)  Z i

πi(b

f ∈F(cid:12)(cid:12)(cid:12)ZZ2
ESi|qih sup
2 c+j); f )(cid:12)(cid:12)(cid:12)i =

mi

1
n

l(z  z0; f )dP (z  z0|qi)

n

Xi=1

ESi|qih ˜G(Si)i .

(7)

bmi/2c

Third  we give a bound for ESi|qi h ˜G(Si)i by use of symmetrization. We introduce a ghost doc-
ument sample ˜Si = {˜zi
j}j=1 ···  mi that is independent of Si and identically distributed. Assume
are independent Rademacher random variables  independent of Si and ˜Si. Then 
1 ···   σi
σi
Si  ˜Si|qi
f ∈F(cid:12)(cid:12)(cid:12)
sup
Xj=1


2 c+j; f )(cid:1)(cid:12)(cid:12)(cid:12)


 = ESi|qi [R1;mi (l ◦ F (qi  Si))] .

ESi|qih ˜G(Si)i ≤ E
Si σi|qi
f ∈F(cid:12)(cid:12)(cid:12)
sup

Xj=1 (cid:0)l(zi
bmi/2c+j; f )(cid:12)(cid:12)(cid:12)

Jointly considering (5)  (7)  and (8)  we can prove the theorem.

2 c+j; f ) − l(˜zi

σi
jl(zi

bmi/2c

bmi/2c

j  zi
b

j  ˜zi
b

j  zi

bmi/2c

bmi/2c

= E

(8)

mi

mi

2

1

2We say a function has bounded difference  if the value of the function can only have bounded change when

only one variable is changed.

7

4.1.3 Combining the Bounds

Considering Theorem 3 and taking expectation on query sample Q  we can obtain that with proba-
bility at least 1 − δ 

ˆRl

n(f ) − ˆRl

n;m1 ···  mn (f ) ≤

1
n

n

Xi=1

ESi|qi [R1;mi (l ◦ F (qi  Si))] +vuut

2M 2 log 2
δ

min2

n

Xi=1

  ∀f ∈ F .

Furthermore  if conventional RA has an upper bound D(l ◦ F ·)  for arbitrary sample distribution 
document-layer reduced two-layer RA can be upper bounded by D(l◦F  n) and query-layer reduced
two-layer RA can be bounded by D(l ◦ F bmi/2c).
Combining the document-layer error bound and the query-layer error bound presented in the previ-
ous subsections  and considering the above discussions  we can eventually prove Theorem 1.

4.2 Discussions

According to Theorem 1  we can have the following discussions.

(1) The increasing number of either queries or documents per query in the training data will enhance
the two-layer generalization ability. This conclusion seems more intuitive and reasonable than that
obtained in [15].
(2) Only if n → ∞ and mi → ∞ simultaneously does the two-layer generalization bound uniformly
converge. That is  if the number of documents for some query is ﬁnite  there will always exist
document-layer error no matter how many queries have been used for training; if the number of
queries is ﬁnite  then there will always exist query-layer error  no matter how many documents per
query have been used for training.
(3) If we only have a limited budget to label C documents in total  according to Theorem 1  there is
an optimal trade off between the number of training queries and that of training documents per query.
This is consistent with previous empirical ﬁndings in [19]. Actually one can attain the optimal trade
off by solving the following optimization problem:

D(l ◦ F   n) +vuut

n

Xi=1

2M 2 log 2
δ

min2 +

n

Xi=1

D(l ◦ F   bmi/2c)

min

n m1 ···  mn

s.t.

n

Xi=1

mi = C

This optimum problem is easy to solve. For example  if ranking function class F satisﬁes V C( ˜F) =
n∗ where c1 is a constant.
V   for the pairwise 0-1 loss  we have n∗ =
From this result we have the following discussions. (i) n∗ decreases with the increasing capacity of
the function class. That is  we should label fewer queries and more documents per query when the
hypothesis space is larger. (ii) For ﬁxed hypothesis space  n∗ increases with the conﬁdence level δ.
That is  we should label more query if we want the bound to hold with a larger probability.

√C  m∗i ≡ C

c1√V +√2 log(4/δ)

c1√2V

The above ﬁndings can be used to explain the behavior of existing pairwise ranking algorithms  and
can be used to guide the construction of training set for learning to rank.

5 Conclusions and Discussions

In this paper  we have proposed conducting two-layer generalization analysis for ranking  and proved
a two-layer generalization bound for ERM learning with pairwise losses. The theoretical results we
have obtained can better explain experimental observations in learning to rank than previous results 
and can provide general guidelines to trade off between deep labeling and shallow labeling in the
construction of training data.

For future work  we plan to i) extend our analysis to listwise loss functions in ranking  such as
ListNet [7] and listMLE [18]; ii) and introduce noise condition in order to obtain faster convergency.

8

References

[1] S. Agarwal  T. Graepel  R. Herbrich  S.Har-Peled  and D. Roth. Generalization bounds for the

area under the roc curve. Journal of Machine Learning Research  6:393–425  2005.

[2] S. Agarwal and P. Niyogi. Generalization bounds for ranking algorithms via algorithmic sta-

bility. Journal of Machine Learning Research  10:441–474  2009.

[3] P. L. Bartlett  S. Mendelson  and M. Long. Rademacher and gaussian complexities: risk bounds

and structural results. Journal of Machine Learning Research  3:463–482  2002.

[4] J. Baxter. Learning internal representations. In Proceedings of the Eighth International Con-

ference on Computational Learning Theory  pages 311–320. ACM Press  1995.

[5] C. Burges  T. Shaked  E. Renshaw  A. Lazier  M. Deeds  N. Hamilton  and G. Hullender.
Learning to rank using gradient descent. In ICML ’05: Proceedings of the 22nd International
Conference on Machine learning  pages 89–96  2005.

[6] Y. Cao  J. Xu  T. Y. Liu  H. Li  Y. Huang  and H. W. Hon. Adapting ranking svm to document
retrieval. In SIGIR ’06: Proceedings of the 29th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval  pages 186–193. ACM Press  2006.

[7] Z. Cao  T. Qin  T. Y. Liu  M. F. Tsai  and H. Li. Learning to rank: from pairwise approach to
listwise approach. In ICML ’07: Proceedings of the 24th International Conference on Machine
learning  pages 129–136  2007.

[8] C. L. Clarke  N. Craswell  and I. Soboroff. Overview of the trec 2009 web track. Technical

report  no date.

[9] S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and scoring using empirical risk minimiza-
tion. In COLT ’05: Proceedings of the 18th Annual Conference on Learning Theory  pages
1–15  2005.

[10] D. Cossock and T. Zhang. Subset ranking using regression. In COLT ’06: Proceedings of the

19th Annual Conference on Learning Theory  pages 605–619  2006.

[11] Y. Freund  R. Iyer  R. E. Schapire  and Y. Singer. An efﬁcient boosting algorithm for combining

preferences. Journal of Machine Learning Research  4:933–969  2003.

[12] R. Herbrich  T. Graepel  and K. Obermayer. Large margin rank boundaries for ordinal re-
gression. In Advances in Large Margin Classiﬁers  pages 115–132  Cambridge  MA  1999.
MIT.

[13] T. Joachims. Optimizing search engines using clickthrough data. In KDD ’02: Proceedings
of the 8th ACM SIGKDD international conference on Knowledge discovery and data mining 
pages 133–142  2002.

[14] Y. Y. Lan  T. Y. Liu  Z. M. Ma  and H. Li. Generalization analysis of listwise learning-to-
rank algorithms. In ICML ’09: Proceedings of the 26th International Conference on Machine
Learning  pages 577–584  2009.

[15] Y. Y. Lan  T. Y. Liu  T. Qin  Z. M. Ma  and H. Li. Query-level stability and generalization in
learning to rank. In ICML ’08: Proceedings of the 25th International Conference on Machine
Learning  pages 512–519  2008.

[16] T. Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information

Retrieval  3:225–331  2009.

[17] J. R. Wen  J. Y. Nie  and H. J. Zhang. Clustering user queries of a search engine. In WWW ’01:
Proceedings of the 10th international conference on World Wide Web  pages 162–168  New
York  NY  USA  2001. ACM.

[18] F. Xia  T.-Y. Liu  J. Wang  W. Zhang  and H. Li. Listwise approach to learning to rank - theory
and algorithm. In ICML ’08: Proceedings of the 25th International Conference on Machine
learning  pages 1192–1199. Omnipress  2008.

[19] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In SIGIR
’09: Proceedings of the 32th annual international ACM SIGIR conference on Research and
development in information retrieval  pages 662–663  2009.

9

,Guillaume Dehaene
Simon Barthelmé