2015,Space-Time Local Embeddings,Space-time is a profound concept in physics. This concept was shown to be useful for dimensionality reduction. We present basic definitions with interesting counter-intuitions. We give theoretical propositions to show that space-time is a more powerful representation than Euclidean space. We apply this concept to manifold learning for preserving local information. Empirical results on non-metric datasets show that more information can be preserved in space-time.,Space-Time Local Embeddings

Jun Wang2

Alexandros Kalousis3 1

St´ephane Marchand-Maillet1

Ke Sun1∗

1 Viper Group  Computer Vision and Multimedia Laboratory  University of Geneva

sunk.edu@gmail.com  Stephane.Marchand-Maillet@unige.ch  and 2 Expedia 
Switzerland  jwang1@expedia.com  and 3 Business Informatics Department  University

of Applied Sciences  Western Switzerland  Alexandros.Kalousis@hesge.ch

Abstract

Space-time is a profound concept in physics. This concept was shown to be
useful for dimensionality reduction. We present basic deﬁnitions with interest-
ing counter-intuitions. We give theoretical propositions to show that space-time
is a more powerful representation than Euclidean space. We apply this concept
to manifold learning for preserving local information. Empirical results on non-
metric datasets show that more information can be preserved in space-time.

1

Introduction

As a simple and intuitive representation  the Euclidean space (cid:60)d has been widely used in various
learning tasks. In dimensionality reduction  n given high-dimensional points in (cid:60)D  or their pair-
wise (dis-)similarities  are usually represented as a corresponding set of points in (cid:60)d (d < D).
The representation power of (cid:60)d is limited. Some of its limitations are listed next.  The maximum
number of points which can share a common nearest neighbor is limited (2 for (cid:60); 5 for (cid:60)2) [1  2] 
while such centralized structures do exist in real data.  (cid:60)d can at most embed (d + 1) points
with uniform pair-wise similarities. It is hard to model pair-wise relationships with less variance. 
Even if d is large enough  (cid:60)d as a metric space must satisfy the triangle inequality  and therefore
must admit transitive similarities [2]  meaning that a neighbor’s neighbor should also be nearby.
Such relationships can be violated on real data  e.g. social networks.  The Gram matrix of n
real vectors must be positive semi-deﬁnite (p. s. d.). Therefore (cid:60)d cannot faithfully represent the
negative eigen-spectrum of input similarities  which was discovered to be meaningful [3].
To tackle the above limitations of Euclidean embeddings  a commonly-used method is to impose a
statistical mixture model. Each embedding point is a random point on several candidate locations
w. r. t. some mixture weights. These candidate locations can be in the same (cid:60)d [4]. This allows an
embedding point to jump across a long distance through a “statistical worm-hole”. Or  they can be
in m independent (cid:60)d’s [2  5]  resulting in m different views of the input data.
Another approach beyond Euclidean embeddings is to change the embedding destination to a curved
space Md. This Md can be a Riemannian manifold [6] with a positive deﬁnite metric  or equiva-
lently  a curved surface embedded in a Euclidean space [7  8]. To learn such an embedding requires
a closed-form expression of the distance measure. This Md can also be semi-Riemannian [9] with
an indeﬁnite metric. This semi-Riemannian representation  under the names “pseudo-Euclidean
space”  “Minkowski space”  or more conveniently  “space-time”  was shown [3  7  10–12] to be a
powerful representation for non-metric datasets. In these works  an embedding is obtained through
a spectral decomposition of a “pseudo-Gram” matrix  which is computed based on some input data.
On the other hand  manifold learning methods [4  13  14] are capable of learning a p. s. d. ker-
nel Gram matrix  that encapsulates useful information into a narrow band of its eigen-spectrum.

∗Corresponding author

1

Usually  local neighborhood information is more strongly preserved as compared to non-local in-
formation [4  15]  so that the input information is unfolded in a non-linear manner to achieve the
desired compactness.
The present work advocates the space-time representation. Section 2 introduces the basic concepts.
Section 3 gives several simple propositions that describe the representation power of space-time. As
novel contributions  section 4 applies the space-time representation to manifold learning. Section 5
shows that using the same number of parameters  more information can be preserved by such em-
beddings as compared to Euclidean embeddings. This leads to new data visualization techniques.
Section 6 concludes and discusses possible extensions.

2 Space-time

The fundamental measurements in geometry are established by the concept of a metric [6]. Intu-
itively  it is a locally- or globally-deﬁned inner product. The metric of a Euclidean space (cid:60)d is
everywhere identity. The inner product between any two vectors y1 and y2 is (cid:104)y1  y2(cid:105) = yT
1 Idy2 
where Id is the d × d identity matrix. A space-time (cid:60)ds dt is a (ds + dt)-dimensional real vector
space  where ds ≥ 0  dt ≥ 0  and the metric is
M =

(cid:20)Ids

(cid:21)

(1)

.

0
0 −Idt

This metric is not trivial. It is semi-Riemannian with a background in physics [9]. A point in (cid:60)ds dt
is called an event  denoted by y = (y1  . . .   yds   yds+1  . . .   yds+dt)T . The ﬁrst ds dimensions
are space-like  where the measurements are exactly the same as in a Euclidean space. The last dt
dimensions are time-like  which cause counter-intuitions. In accordance to the metric M in eq. (1) 

∀y1  y2 ∈ (cid:60)ds dt 

(cid:104)y1  y2(cid:105) =

yl
1yl

2 −

1yl
yl
2.

(2)

In analogy to using inner products to deﬁne distances  the following deﬁnition gives a dissimilarity
measure between two events in (cid:60)ds dt.
Deﬁnition 1. The space-time interval  or shortly interval  between any two events y1 and y2 is

ds(cid:88)

l=1

ds+dt(cid:88)

l=ds+1

ds(cid:88)

l=1

ds+dt(cid:88)

l=ds+1

c(y1  y2) = (cid:104)y1  y1(cid:105) + (cid:104)y2  y2(cid:105) − 2(cid:104)y1  y2(cid:105) =

(yl
1 − yl

2)2 −

(yl
1 − yl

2)2.

(3)

The space-time interval c(y1  y2) can be positive  zero or negative. With respect to a reference point
y0 ∈ (cid:60)ds dt  the set {y : c(y  y0) = 0} is called a light cone. Figure 1a shows a light cone in
(cid:60)2 1. Within the light cone  c(y  y0) < 0  i. e.  negative interval occurs; outside the light cone 
c(y  y0) > 0. The following counter-intuitions help to establish the concept of space-time.
A low-dimensional (cid:60)ds dt can accommodate an arbitrarily large number of events sharing a com-
mon nearest neighbor.
In (cid:60)2 1  let A = (0  0  1)  and put {B1  B2  . . .  } evenly on the circle
{(y1  y2  0) : (y1)2 + (y2)2 = 1} at time 0. Then  A is the unique nearest neighbor of B1  B2  . . . .
A low-dimensional (cid:60)ds dt can represent uniform pair-wise similarities between an arbitrarily large
number of points. In (cid:60)1 1  the similarities within {Ai : Ai = (i  i)}n
In (cid:60)ds dt  the triangle inequality is not necessarily satisﬁed. In (cid:60)2 1  let A = (−1  0  0)  B =
(0  0  1)  C = (1  0  0). Then c(A  C) > c(A  B) + c(B  C). The trick is that  as B’s absolute
time value increases  its intervals with all events at time 0 are shrinking. Correspondingly  similarity
measures in (cid:60)ds dt can be non-transitive. The fact that B is similar to A and C independently does
not necessarily mean that A and C are similar.
A neighborhood of y0 ∈ (cid:60)2 1 is {(y1  y2  y3) : (y1−y1
0)2 ≤ }  where  ∈
(cid:60). This hyperboloid has inﬁnite “volume”  no matter how small  is. Comparatively  a neighborhood
in (cid:60)d is much narrower  with an exponentially shrinking volume as its radius decreases.

0)2 +(y2−y2

0)2−(y3−y3

i=1 are uniform.

2

e
m
i
t

e

c

a

p

s

space

y0

lightcone

e
m
i
t

0

=

c

− 1
5
0 .
−
c =
c = 0 . 5
c = 1

space

(a)

(b)

∆n

ˆp3 0

g(K 3 0
n )

p(cid:63)

ˆp2 1

g(K 2 1
n )
(c)

Figure 1: (a) A space-time; (b) A space-time “compass” in (cid:60)1 1. The colored lines show equal-
interval contours with respect to the origin; (c) All possible embeddings in (cid:60)2 1 (resp. (cid:60)3) are
mapped to a sub-manifold of ∆n  as shown by the red (resp. blue) line. Dimensionality reduction
projects the input p(cid:63) onto these sub-manifolds  e. g. by minimizing the KL divergence.

3 The representation capability of space-time

(4)

1
2

(In −

1
n

eeT )C(In −

1
n

∀i (cid:80)n

(cid:54)= j  Cij = Cji} and Kn = {Kn×n :
j=1 Kij = 0; ∀i (cid:54)= j  Kij = Kji} are two families of real symmetric matrices. dim(Cn) =

Proof. ∀C(cid:63) ∈ Cn  K(cid:63) = K(C(cid:63)) has the eigen-decomposition K(cid:63) = (cid:80)rank(K(cid:63))
l } are orthonormal. For each l = 1 ···   rank(K(cid:63)) (cid:112)

This section formally discusses some basic properties of (cid:60)ds dt in relation to dimensionality reduc-
tion. We ﬁrst build a tool to shift between two different representations of an embedding: a matrix
of c(yi  yj) and a matrix of (cid:104)yi  yj(cid:105). From straightforward derivations  we have
Lemma 1. Cn = {Cn×n : ∀i  Cii = 0; ∀i
dim(Kn) = n(n − 1)/2. A linear mapping from Cn to Kn and its inverse are given by
eeT )  C(K) = diag(K)eT + ediag(K)T − 2K 
K(C) = −
where e = (1  ···   1)T   and diag(K) means the diagonal entries of K as a column vector.
Cn and Kn are the sets of interval matrices and “pseudo-Gram” matrices  respectively [3  12]. In
particular  a p. s. d. K ∈ Kn means a Gram matrix  and the corresponding C(K) means a square
distance matrix. The double centering mapping K(C) is widely used to generate a (pseudo-)Gram
matrix from a dissimilarity matrix.
Proposition 2. ∀C(cid:63) ∈ Cn  ∃ n events in (cid:60)ds dt  s. t. ds + dt ≤ n − 1 and their intervals are C(cid:63).
l )T
l (v(cid:63)
where rank(K(cid:63)) ≤ n − 1 and {v(cid:63)
l |v(cid:63)
|λ(cid:63)
l
gives the coordinates in one dimension  which is space-like if λ(cid:63)
Remark 2.1. (cid:60)ds dt (ds + dt ≤ n − 1) can represent any interval matrix C(cid:63) ∈ Cn  or equivalently 
any K(cid:63) ∈ Kn. Comparatively  (cid:60)d (d ≤ n − 1) can only represent {K ∈ Kn : K (cid:23) 0}.
A pair-wise distance matrix in (cid:60)d is invariant to rotations. In other words  the direction information
of a point cloud is completely discarded. In (cid:60)ds dt  some direction information is kept to distinguish
between space-like and time-like dimensions. As shown in ﬁg. 1b  one can tell the direction in (cid:60)1 1
(cid:110)
by moving a point along the curve {(y1)2 + (y2)2 = 1} and measuring its interval w. r. t. the origin.
p = (pij) : 1 ≤ i ≤ n; 1 ≤ j ≤ n; i < j; ∀i  ∀j  pij > 0;(cid:80)
Local embedding techniques often use similarity measures in a statistical simplex ∆n =
. This ∆n has one
less dimension than Cn and Kn so that dim(∆n) = n(n − 1)/2 − 1. A mapping from Kn (Cn) to
∆n is given by
(5)
where f (·) is a positive-valued strictly monotonically decreasing function  so that a large probability
mass is assigned to a pair of events with a small interval. Proposition 2 trivially extends to
Proposition 3. ∀p(cid:63) ∈ ∆n  ∃ n events in (cid:60)ds dt  s. t. ds + dt ≤ n − 1 and their similarities are p(cid:63).
Remark 3.1. (cid:60)ds dt (ds + dt ≤ n − 1) can represent any n × n symmetric positive similarities.

l > 0 or time-like if λ(cid:63)

l < 0.

pij ∝ f (Cij(K)) 

i j:i<j pij = 1

l=1

l v(cid:63)
λ(cid:63)

(cid:111)

3

Typically in eq. (5) we have f (x) = exp (−x). The pre-image in Cn of any given p(cid:63) ∈ ∆n is
a uniform increment on the off-diagonal entries of C(cid:63). By eq. (4)  the corresponding curve in

(cid:1) means
(cid:9)  where K(cid:63)(0) = K(cid:63) = K(C(cid:63)). Because
n eeT(cid:1) shares with K(cid:63) a common eigenvector e with zero eigenvalue  and the rest eigen-
n eeT(cid:1) =(cid:80)n−1
l }n−1
l=1 and real numbers {λ(cid:63)
l=1 v(cid:63)

the curve(cid:8)C(cid:63) + 2δ(cid:0)eeT − In
(cid:1) : ∀i (cid:54)= j  C(cid:63)
n eeT(cid:1) : δ ∈ (cid:60)
Kn is (cid:8)K(cid:63)(δ) = K(cid:63) + δ(cid:0)In − 1
(cid:0)In − 1
l )T   and(cid:0)In − 1
K(cid:63) =(cid:80)rank(K(cid:63))
rank(K(cid:63))(cid:88)

(cid:9)  where 2δ(cid:0)eeT − In

values are all 1  there exist orthonormal vectors {v(cid:63)

l }rank(K(cid:63))

ij = − ln p(cid:63)

l )T . Therefore

l (v(cid:63)

n−1(cid:88)

ij; δ ∈ (cid:60)

λ(cid:63)
l v(cid:63)

l (v(cid:63)

  s. t.

l=1

l=1

K(cid:63)(δ) =

(λ(cid:63)

l + δ)v(cid:63)

l (v(cid:63)

l )T +

δv(cid:63)

l (v(cid:63)

l )T .

(6)

l=1

l=rank(K(cid:63))+1

Depending on δ  K(cid:63)(δ) can be negative deﬁnite  positive deﬁnite  or somewhere in between. This
is summarized in the following theorem.
Theorem 4. If f (x) = exp(−x) in eq. (5)  the pre-image in Kn of ∀p(cid:63) ∈ ∆n is a continuous curve
{K(cid:63)(δ) : δ ∈ (cid:60)}. ∃δ0  δ1 ∈ (cid:60)  s. t. ∀δ < δ0  K(cid:63)(δ) ≺ 0  ∀δ > δ1  K(cid:63)(δ) (cid:31) 0  and the number
of positive eigenvalues of K(cid:63)(δ) increases monotonically with δ.
With enough dimensions  any p(cid:63) ∈ ∆n can be perfectly represented in a space-only  or time-
only  or space-time-mixed (cid:60)ds dt. There is no particular reason to favor a space-only model 
because the objective of dimensionality reduction is to get a compact model with a small num-
ber of dimensions  regardless of whether they are space-like or time-like. Formally  K ds dt
=
{K+ − K− : rank(K+) ≤ ds; rank(K−) ≤ dt; K+ (cid:23) 0; K−
(cid:23) 0} is a low-rank subset of
In the domain Kn  dimensionality reduction based on the input p(cid:63) ﬁnds some ˆKds dt ∈
Kn.
K ds dt
  which is close to the curve K(cid:63)(δ).
n
under some mapping g : Kn → ∆n is
In the probability domain ∆n  the image of K ds dt
g(K ds dt
)  so
that ˆpds dt is the closest point to p(cid:63) w. r. t. some information theoretic measure. The proximity
of p(cid:63) to ˆpds dt  i. e. its proximity to g(K ds dt
)  measures the quality of the model (cid:60)ds dt as the
embedding target space  when the model scale or the number of dimensions is given.
We will investigate the latter approach  which depends on the choice of ds  dt  the mapping g  and
some proximity measure on ∆n. We will show that  with the same number of dimensions ds + dt 
the region g(K ds dt

). As shown in ﬁg. 1c  dimensionality reduction ﬁnds some ˆpds dt ∈ g(K ds dt

) with space-time-mixed dimensions is naturally close to certain input p(cid:63).

n

n

n

n

n

n

4 Space-time local embeddings

We project a given similarity matrix p(cid:63) ∈ ∆n to some ˆK ∈ K ds dt
  or equivalently  to a set of
i=1 ⊂ (cid:60)ds dt  so that ∀i  ∀j  (cid:104)yi  yj(cid:105) = ˆKij as in eq. (2)  and the similarities
events Y = {yi}n
among these events resemble p(cid:63). As discussed in section 3  a mapping g : Kn → ∆n helps transfer
K ds dt
into a sub-manifold of ∆n  so that the projection can be done inside ∆n. This mapping
expressed in the event coordinates is given by

n

n

exp(cid:0)

pij(Y ) ∝

(cid:107)yt
1 + (cid:107)ys

i − yt
i − ys

j(cid:107)2(cid:1)
j(cid:107)2  

(7)

where ys = (y1  . . .   yds)T   yt = (yds+1  . . .   yds+dt)T   and (cid:107) · (cid:107) denotes the 2-norm. For any pair
of events yi and yj  pij(Y ) increases when their space coordinates move close  and/or when their
time coordinates move away. This agrees with the basic intuitions of space-time. For time-like di-
mensions  the heat kernel is used to make pij(Y ) sensitive to time variations. This helps to suppress
events with large absolute time values  which make the embedding less interpretable. For space-like
dimensions  the Student-t kernel  as suggested by t-SNE [13]  is used  so that there could be more
“volume” to accommodate the often high-dimensional input data. Based on our experience  this
hybrid parametrization of pij(Y ) can better model real data as compared to alternative parametriza-
tions. Similar to SNE [4] and t-SNE [13]  an optimal embedding can be obtained by minimizing the
Kullback-Leibler (KL) divergence from the input p(cid:63) to the output p(Y )  given by

KL(Y ) =

p(cid:63)
ij ln

p(cid:63)
ij

pij(Y )

.

(8)

(cid:88)

i j:i<j

4

According to some straightforward derivations  its gradients are

(cid:88)
(cid:88)

j:j(cid:54)=i

j:j(cid:54)=i

(cid:1)  

(cid:0)p(cid:63)
ij − pij(Y )(cid:1)(cid:0)yt
(cid:0)p(cid:63)
ij − pij(Y )(cid:1)(cid:0)ys

i − yt

1
i − ys
1 + (cid:107)ys

j(cid:107)2

j

i − ys

j

∂KL
∂yt
i

∂KL
∂ys
i

= −2

= 2

(cid:1)  

(9)

(10)

ij = p(cid:63)

i and ys

j are attracting  and yt

ji and pij(Y ) = pji(Y ). As an intuitive interpretation of a gradient descent
ij  i. e. yi and yj are put too far
j are repelling  so that their space-time
ij  then yi and yj are repelling in space and attracting in

where ∀i  ∀j  p(cid:63)
process w. r. t. eqs. (9) and (10)  we have that if pij(Y ) < p(cid:63)
from each other  then ys
interval becomes shorter; if pij(Y ) > p(cid:63)
time.
i } are updated by the delta-bar-delta scheme as used in t-SNE [13] 
During gradient descent  {ys
where each scalar parameter has its own adaptive learning rate initialized to γs > 0; {yt
i} are
updated based on one global adaptive learning rate initialized to γt > 0. The learning of time
should be more cautious  because pij(Y ) is more sensitive to time variations by eq. (7). Therefore 
the ratio γt/γs should be very small  e.g. 1/100.

i and yt

5 Empirical results

CAij(1/(cid:80)

j CAij + 1/(cid:80)

Aiming at potential applications in data visualization and social network analysis  we compare
SNE [4]  t-SNE [13]  and the method proposed in section 4 denoted as SNEST . They are based
on the same optimizer but correspond to different sub-manifolds of ∆n  as presented by the curves
in ﬁg. 1c. Given different embeddings of the same dataset using the same number of dimensions 
we perform model selection based on the KL divergence as explained in the end of section 3.
We generated a toy dataset SCHOOL  representing a school with two classes. Each class has 20
students standing evenly on a circle  where each student is communicating with his (her) 4 nearest
neighbours  and one teacher  who is communicating with all the students in the same class and the
teacher in the other class. The input p(cid:63) is distributed evenly on the pairs (i  j) who are socially
connected.
NIPS22 contains a 4197 × 3624 author-document matrix from NIPS 1988 to 2009 [2]. After
discarding the authors who have only one NIPS paper  we get 1418 authors who co-authored
2121 papers. The co-authorship matrix is CA1418×1418  where CAij denotes the number of pa-
pers that author i co-authored with author j. The input similarity p(cid:63) is computed so that p(cid:63)
ij ∝
i CAij)  where the number of co-authored papers is normalized by each
author’s total number of papers. NIPS17 is built in the same way using only the ﬁrst 17 volumes.
GrQc is an arXiv co-authorship graph [16] with 5242 nodes and 14496 edges. After removing
one isolated node  a matrix CA5241×5241 gives the numbers of co-authored papers between any two
authors who submitted to the general relativity and quantum cosmology category from January 1993
to April 2003. The input similarity p(cid:63) satisﬁes p(cid:63)
W5000 is the semantic similarities among 5000 English words in WS5000×5000 [2  17]. Each WSij
is an asymmetric non-negative similarity from word i to word j. The input is normalized into a
probability vector p(cid:63) so that p(cid:63)
i WSji. W1000 is built in the same way
using a subset of 1000 words.
Table 1 shows the KL divergence in eq. (8). In most cases  SNEST for a ﬁxed number of free param-
eters has the lowest KL. On NIPS22  GrQc  W1000 and W5000  the embedding by SNEST in (cid:60)2 1
is even better than SNE and t-SNE in (cid:60)4  meaning that the embedding by SNEST is both compact
and faithful. This is in contrast to the mixture approach for visualization [2]  which multiplies the
number of parameters to get a faithful representation.
Fixing the free parameters to two dimensions  t-SNE in (cid:60)2 has the best overall performance  and
SNEST in (cid:60)1 1 is worse. We also discovered that  using d dimensions  (cid:60)d−1 1 usually performs
better than alternative choices such as (cid:60)d−2 2  which are not shown due to space limitation. A time-
like dimension allows adaptation to non-metric data. The investigated similarities  however  are

ij ∝ CAij(1/(cid:80)
j WSij + WSji/(cid:80)

ij ∝ WSij/(cid:80)

j CAij + 1/(cid:80)

i CAij).

5

Table 1: KL divergence of different embeddings. After repeated runs on different conﬁgurations for
each embedding  the minimal KL that we have achieved within 5000 epochs is shown. The bold
numbers show the winners among SNE  t-SNE and SNEST using the same number of parameters.

SCHOOL NIPS17 NIPS22 GrQc
3.19
0.52
1.82
0.36
0.19
1.03
1.24
0.61
1.14
0.58
1.11
0.58
0.43
2.34
0.31
1.00
0.88
0.29

1.88
0.85
0.35
0.88
0.85
0.84
0.91
0.60
0.54

2.98
1.79
1.01
1.29
1.23
1.22
1.62
0.97
0.93

e
m
i
t

SNE → (cid:60)2
SNE → (cid:60)3
SNE → (cid:60)4
t-SNE → (cid:60)2
t-SNE → (cid:60)3
t-SNE → (cid:60)4
SNEST → (cid:60)1 1
SNEST → (cid:60)2 1
SNEST → (cid:60)3 1

teachers

(a)

W1000 W5000
3.67
3.20
2.76
2.15
2.00
1.96
2.59
1.92
1.79

4.93
4.42
3.93
3.00
2.79
2.74
3.64
2.57
2.39

(b)

teachers). The paper coordinates (resp. color) mean the space (resp.

Figure 2: (a) The embedding of SCHOOL by SNEST in (cid:60)2 1. The black (resp. colored) dots denote
time)
the students (resp.
coordinates. The links mean social connections. (b) The contour of exp((cid:107)yt
in eq. (7) as a
1+(cid:107)ys
function of (cid:107)ys

j(cid:107) (y-axis). The unit of the displayed levels is 10−3.

j(cid:107) (x-axis) and (cid:107)yt

i − ys

i − yt

i−yt
i −ys

j(cid:107)2)
j(cid:107)2

mainly space-like  in the sense that a random pair of people or words are more likely to be dissimilar
(space-like) rather than similar (time-like). According to our experience  on such datasets  good
performance is often achieved with mainly space-like dimensions mixed with a small number of
time-dimensions  e.g. (cid:60)2 1 or (cid:60)3 1 as suggested by table 1.
To interpret the embeddings  ﬁg. 2a presents the embedding of SCHOOL in (cid:60)2 1  where the space
and time are represented by paper coordinates and three colors levels  respectively. Each class is
embedded as a circle. The center of each class  the teacher  is lifted to a different time  so as to be
near to all students in the same class. One teacher being blue  while the other being red  creates a
“hyper-link” between the teachers  because their large time difference makes them nearby in (cid:60)2 1.
Figures 3 and 4 show the embeddings of NIPS22 and W5000 in (cid:60)2 1. Similar to the (t-)SNE
visualizations [2  4  13]  it is easy to ﬁnd close authors or words embedded nearby. The learned
p(Y )  however  is not equivalent to the visual proximity  because of the counter-intuitive time di-
mension. How much does the visual proximity reﬂect the underlying p(Y )? From the histogram
of the time coordinates  we see that the time values are in the narrow range [−1.5  1.5]  while the
range of the space coordinates is at least 100 times larger. Figure 2b shows the similarity function
on the right-hand-side of eq. (7) over an interesting range of (cid:107)ys
j(cid:107). In this range 
large similarity values are very sensitive to space variations  and their red level curves are almost
vertical  meaning that the similarity information is largely carried by space coordinates. Therefore 
the visualization of neighborhoods is relatively accurate: visually nearby points are indeed similar;
proximity in a neighborhood is informative regarding p(Y ). On the other hand  small similarity val-
ues are less sensitive to space variations  and their blue level curves span a large distance in space 
meaning that the visual distance between dissimilar points is less informative regarding p(Y ). For

j(cid:107) and (cid:107)yt

i − ys

i − yt

6

50100150200kysi−ysjk012kyti−ytjk0.1110100exp(kyti−ytjk2)/(1+kysi−ysjk2)050100Figure 3: An embedding of NIPS22 in (cid:60)2 1. “Major authors” with at least 10 NIPS papers or with
a time value in the range (−∞ −1] ∪ [1 ∞) are shown by their names. Other authors are shown
by small dots. The paper coordinates are in space-like dimensions. The positions of the displayed
names are adjusted up to a tiny radius to avoid text overlap. The color of each name represents the
time dimension. The font size is proportional to the absolute time value.

example  a visual distance of 165 with a time difference of 1 has roughly the same similarity as a
visual distance of 100 with no time difference. This is a matter of embedding dissimilar samples far
or very far and does not affect much the visual perception  which naturally requires less accuracy on
such samples. However  perception errors could still occur in these plots  although they are increas-
ingly unlikely as the observation radius turns small. In viewing such visualizations  one must count
in the time represented by the colors and font sizes  and remember that a point with a large absolute
time value should be weighted higher in similarity judgment.
Consider the learning of yi by eq. (9)  if the input p(cid:63)
ij is larger than what can be faithfully modeled
in a space-only model  then j will push i to a different time. Therefore  the absolute value of time
is a signiﬁcance measurement. By ﬁg. 2a  the connection hubs  and points with remote connections 
are more likely to be at a different time. Emphasizing the embedding points with large absolute time
values helps the user to focus on important points. One can easily identify well-known authors and
popular words in ﬁgs. 3 and 4. This type of information is not discovered by traditional embeddings.

6 Conclusions and Discussions

We advocate the use of space-time representation for non-metric data. While previous works on
such embeddings [3  12] compute an indeﬁnite kernel by simple transformations of the input data 
we learn a low-rank indeﬁnite kernel by manifold learning  trying to better preserve the neigh-

7

−250−1500150250−250−1500150250AchanAmariAtiyaAtkesonAttiasBachBaldiBallardBarberBartlettBartoBeckBengioBengioBialekBishopBlackBlairBleiBowerBradleyBuhmannCaruanaCauwenberghsChapelleCohnCottrellCourvilleCowanCrammerCristianiniDarrellDasDayanDeFreitasDeWeeseDenkerDoyaFrasconiFreemanFreyFukumizuGerstnerGhahramaniGilesGoldsteinGordonGraepelGrayGrettonGriffithsGrimesGuptaHaslerHastieHerbrichHintonHochreiterHofmannHornJaakkolaJinJohnsonJordanKakadeKawatoKearnsKochKollerLaffertyLeCunLeeLeeLeeLeenLewickiLiLippmannLiuMaassMalikMarchandMeirMelMinchMitchellMjolsnessMohriMontagueMoodyMooreMorganMovellanMozerMullerMurrayNgNowlanObermayerOpperPearlmutterPillowPlattPoggioPougetRahimiRaoRasmussenRatschRiesenhuberRosenfeldRothRoweisRumelhartRuppinSaadSahaniSaulScholkopfSchraudolphSchuurmansScottSeegerSejnowskiSeungShawe-TaylorSimardSimoncelliSingerSinghSmithSmolaSmythSollichStevensSunSuttonTehTenenbaumTesauroThrunTishbyTouretzkyTrespVapnikViolaWaibelWainwrightWangWangWarmuthWeinshallWeissWellingWestonWilliamsWilliamsonWillskyWintherXingYuYuilleZadorZemelZhangZhangSminchisescuGraumanGarriguesKimKulis−1.50.01.550100150200250histogram of time coordinates<-1.0-0.500.5>1.0---time-->Figure 4: An embedding of W5000 in (cid:60)2 1. Only a subset is shown for a clear visualization. The
position of each word represents its space coordinates up to tiny adjustments to avoid overlap. The
color of each word shows its time value. The font size represents the absolute time value.

bours [4]. We discovered that  using the same number of dimensions  certain input information is
better preserved in space-time than Euclidean space. We built a space-time visualizer of non-metric
data  which automatically discovers important points.
To enhance the proposed visualization  an interactive interface can allow the user select one ref-
erence point  and show the true similarity values  e.g.  by aligning other points so that the visual
distances correspond to the similarities. Proper constraints or regularization could be proposed  so
that the time values are discrete or sparse  and the resulting embedding can be more easily inter-
preted.
The proposed learning is on a sub-manifold K ds dt

n

Another interesting sub-manifold of Kn could be(cid:8)K − ttT : K (cid:31) 0; t ∈ (cid:60)n(cid:9)  which extends the

⊂ Kn  or a corresponding sub-manifold of ∆n.
p. s. d. cone to any matrix in Kn with a compact negative eigen-spectrum. It is possible to construct
a sub-manifold of Kn so that the embedder can learn whether a dimension is space-like or time-like.
As another axis of future investigation  given the large family of manifold learners  there can be many
ways to project the input information onto these sub-manifolds. The proposed method SNEST is
based on the KL divergence in ∆n. Some immediate extensions can be based on other dissimilarity
measures in Kn or ∆n. This could also be useful for faithful representations of graph datasets with
indeﬁnite weights.

Acknowledgments

This work has been supported be the Department of Computer Science  University of Geneva  in
collaboration with Swiss National Science Foundation Project MAAYA (Grant number 144238).

8

−150−100−50050100150−150−100−50050100150FIELDCOMPUTERBODYCONDEMNDISOWNRANGEINTENSITYATTENTIONBECHEERLEADERCHICKENCONFUSIONCRISISCULTUREGRACEHANGHOBBYPARSLEYRESISTANCEANIMALBEARCLEANINGDECENCYDRUGSEXERCISEHIDDENIMPATIENCEMADEPLANPOEMRESTORESALESMANSPLITBLOCKCLEANEREGOEVERYDAYGRADUATIONLACKMAINMANAGEMENTMEDICINEMOVENERVESPROFESSIONALRABBITRAREREASONRENOUNCERETREATRUNNERSUPERSTITIONTHERAPYTRAUMAATTRACTCLAIMSCLOTHESDISBELIEVEFORTFRAYFREEMOLENORMOUTLINEPROTEINRAPEREBELRESPECTSALESSCARSHEDSPYSTROKETRAITORUNIONWOODWORTHLESSBARRELCARCHISELCONGRESSCONSEQUENCECOVEREDDARINGDECORATEDIFFERENCEDUEELABORATEEMPIREEXCELEXTRAVAGANTFAIRFAMILYFLAPFOGFUZZHIGHLIGHTHONORIMPORTANTIMPRESSIONITALIANKEEPERMUSICNATURALPARADEPASSAGEPERSONALITYPLUGPOLICEMANPOTENTIALPROCESSSAUSAGESCIENTIFICSEALSPACESUPPORTSUSPENSETHEORYTOURISTTRAVELTUBEANNOYINGASSOCIATEAWARDBUSYCAPTURECLAYCOMFORTCOMMUNISTCOMPULSIONCONFUSECRIMECRUNCHDETERIORATEDIRECTIONDOMINATEDOWNTOWNELIMINATIONENGINEEREUROPEEVALUATEFACTORYFISHFREEDOMFRONTIERGHOSTGROWHOLEISSUEKIDSLACEMAFIAMASTERMINTNERVEOATMEALPERFORMANCEPERISHPRESENTATIONPROVEPUBERTYRACKRIGHTEOUSNESSROADSNEAKSTAINSTICKSWAMPTABOOTENDTOPPINGVIOLENTWARNWORRYBIRDBLOWBONDBUMPSCAPACITYCOMMONCONTROLSCOVERCREATIVITYCROOKEDDANCERDELAYDEPLETIONDICEDISASTERDISCIPLINEDISTINCTDOORDRAGONEMERGENCYFAITHFULFOOTBALLGETGODGRINDGROWTHHOROSCOPEINVENTORIRONJEWISHLABELLOBSTERMEASUREOPINIONPAINTERPINKPLASTICPLUSHPOTATOPRECIOUSPROJECTPROOFPROTECTIONRANKRECEIPTREDUCERETURNRIBSSCUMSENSITIVESPIKESPITSTAFFSTRIPESTUBBORNSTYLESUGGESTTILLTROPICALUNSUREWORMWRESTLINGABSENCEBEERBISCUITBLAMEBOWLCOASTCOMPOUNDCORNERCRITICISMDANGEROUSDILIGENCEELECTRICIANELEGANTELFEVENTEXTREMEFORBIDGRAVEHELPFULHORMONESINTERESTKITCHENLEADERLEANLEOLIMPLUXURYMAIDENMARBLEMONKEYMORALMUSCLENEGOTIATIONORDERPANICPANTSPARENTSPARTYPASTRYPERCENTPIGPINCHPLACEPOPULARPROTECTRECKLESSREGRETREPLACERESPONSIBILITYSCENERYSILVERWARESOAPSTOLENSWINGTHINKTHRESHOLDTRADEUNEVENUSEWINEABUNDANCEATTENDATTICBALLOONBATTERYBIRDSBOARDBUFFALOBUMCARDCHALLENGECLAMPCOLESLAWCOOKEDCREWCUEDECISIONDISMAYECONOMICENVIRONMENTFAVORFITFLOWERGENERALGLIDEHARDYHEALTHHIKERHISTORYJAPANLEVELLIFTLIMITLIZARDMAKE UPMISCHIEFMISSILEMIXEDNEUTRALNOTOBNOXIOUSOUTDOORSOVERFLOWPIEPOISEPOSSIBLERATEREACTIONREVIVALSECRETARYSEWSKILLSMEARSOUTHERNSPEAKERSPELLSQUEEZESTIMULUSSTRAWBERRYSYMBOLTIPTREETWELVEUNDERSTANDINGUNLOADVASELINEVIOLATIONVOTEWASTEDWELCOMEACCIDENTACCUMULATEAFTERNOONANARCHYBASEBEYONDBLACKMAILBLOODBREASTBREEZEWAYBROWNBUILDINGBUTTERFLYCASTCHARITYCHUCKCLEARCODECOURSECRUSHDATEDISGUSTDISPERSEDODRIFTDRUGECSTASYEGGENDLESSENTERTAINESSENCEEVICTEXPLORERFATTENINGFLOWERSFORBIDDENFOREIGNFUSSGHETTOGIVE UPGONEHANDLEINTAKEINTIMATELANDSCAPELOVERSMILDMIXTUREMOTORCYCLENONSENSEORANGE JUICEOUTRAGEOUSPEACEFULPILEPLAINPREDATORREPENTANCERIVERROCKSRUBBERSERIOUSSHAKESHARKSINGERSINKERSNEAKYSPECIFICSPRAYSQUASHSTRANGERTENTONEUNIFORMVOIDWOLFACCOMPLISHEDADAFFAIRALONEARTSBABYBACTERIABITEBRIEFCASECAPTIONCHANCECLAMCOLDDAMPDELIVERDOCTORDRAINDRILLDRUNKDUCKSELEPHANTESCAPEEXPERIENCEEYEBALLFAKEFIGUREFLUTEFLYFOLDFOULFUSEGARLICGLOVESGREEKHAIRHAIRCUTHANDKERCHIEFINFLATIONLEARNMATHMEANINGMICROSCOPEMONEYMOUTHNECKOPPONENTORIENTOUTSTANDINGPATPLEASERATRITUALSTICKERSWIMMERTEAPOTTELEVISIONTOGETHERTRAINTREATWASTEWRITERADMITAPARTMENTAURAAUTHORITYAWAREAXBEGBROKECHARTCOMMANDERCOSTUMECRACKERCROSSCUTEDAMNDAREDEERDEFENSEDELIGHTDIAMETERDOLPHINEFFORTENGAGEEXTRAFEELINGFILLFRYGIVINGGOOGULLYGUNHAYHIKINGHITHYPNOTIZEIMITATEINDEPENDENTINTESTINELEGALLEMONADELIVERMARINESMEETMILKNOMADOATHOFTENPANTYHOSEPERFECTPLANETSPOURPROFESSIONRAINRECENTRELIEFREPEATROBESENSESHADOWSLIVERSLURPSPONTANEOUSSTAIRSSTEAMSTIFFSTINGYSUPERMANTEMPERTHESISTURTLEVALVEVEERWAKEWATERWELFAREWRAPANOTHERAPPLEBAGBATBLENDERBLOCKADEBUSCAMPINGCLUMSYCOATCONSOLECOUNTERCOURTCURTAINSDIRECTDIVISIONGOODSHELPERHOSTESSIDENTITYINDIANINTEGRITYKEEPLUNCHMARINEMUSTYOILOZONEPADDYPENGUINPERSUADEROACHROYALRUNNINGSERIESSHEEPSUNDAYSUNSHINETAILTARTTELEPHONETELESCOPETRUCKVALUEVODKAWANDERAFFECTANKLEBOATCARTCHEEKDISCOVERDUNKDUSTEGYPTERAEXCISEEXPRESSFUMESHANDHAULHEATHEDGEHORSEHOTELIMAGELOVERMENTHOLMESSAGEMOLASSESMOTIONPOSSESSIONQUALITYSCREENSCRIBBLESHYSIGNALSISTERSNAPSOMETARTASTYUNICORNAHOYBICYCLEBOILBOUNDBRITTLECHANGECHINESECONTEMPORARYCONTEXTCOWGIRLCUPBOARDEXPLODEFIREPLACEFRAILFRUSTRATEHELICOPTERHUNGERIDOLINNOCENCEINSTANCELAKELICKLOFTMEMORYMINERNOTHINGNUNPROVERBPROVISIONQUARTERRADIATORSALUTESINCESLUGWIDEACCOUNTANGELBASSBOXERCATTLECHAMPIONCHASECORNDESCENTDRAFTEINSTEINFAVORITEFEATHERFEVERFIREMANFLEETGRASSHOPPERHOT DOGSJUNIORLEADLIGHTNINGMAROONMAXIMUMORIGINATEPERSONPIANOPIZZAPOUNDREDRESTRICTIONSHOPSHUTTERSITTINGSNOWSPOILSQUIDSTALLSUNSETTALETERMINALTIREDTRAILERTURKEYWATERFALLZITAARDVARKBEARDBIRTHBOOTBOOTSBREATHBUZZCYLINDERDOWNSTAIRSFORHANDBAGHEADACHEHOCKEYKEYSLONGMAJORITYOPENINGPRIMEPRONOUNRECLINERSHOTSMELLSPADESTABLESUBMARINETARGETTHIRSTYTOOTHPASTEWEEKANNIHILATEBACKBORROWCENTSCOCA-COLACOMPONENTSCOOLELDERSHANDICAPPALERAMRIDERSCREAMSPIDERSUPERMARKETADDCORALCRANECUBEEAGLEGROOMHOOPLAVALEMONNEPHEWSAUCERVALENTINEWHOANISETTEBETBOYBRAKECRATERMONARCHPARENTWASPDEFEATDRYERGOINGHARBORMANPARROTSMALLSTRAYADDITIONEMERALDHERSABERSWOONADORESALOONTHIRSTSWABSNESTPROFITDILL−1.50.01.55001000histogram of time coordinates<-0.8-0.400.4>0.8---time-->References
[1] K. Zeger and A. Gersho. How many points in Euclidean space can have a common nearest

neighbor? In International Symposium on Information Theory  page 109  1994.

[2] L. van der Maaten and G. E. Hinton. Visualizing non-metric similarities in multiple maps.

Machine Learning  87(1):33–55  2012.

[3] J. Laub and K. R. M¨uller. Feature discovery in non-metric pairwise data. JMLR  5(Jul):801–

818  2004.

[4] G. E. Hinton and S. T. Roweis. Stochastic neighbor embedding. In NIPS 15  pages 833–840.

MIT Press  2003.

[5] J. Cook  I. Sutskever  A. Mnih  and G. E. Hinton. Visualizing similarity data with a mixture of

maps. In AISTATS’07  pages 67–74  2007.

[6] J. Jost. Riemannian Geometry and Geometric Analysis. Universitext. Springer  6th edition 

2011.

[7] R. C. Wilson  E. R. Hancock  E. Pekalska  and R. P. W. Duin. Spherical embeddings for

non-Euclidean dissimilarities. In CVPR’10  pages 1903–1910  2010.

[8] D. Lunga and O. Ersoy. Spherical stochastic neighbor embedding of hyperspectral data. Geo-

science and Remote Sensing  IEEE Transactions on  51(2):857–871  2013.

[9] B. O’Neill. Semi-Riemannian Geometry With Applications to Relativity. Number 103 in Series:

Pure and Applied Mathematics. Academic Press  1983.

[10] L. Goldfarb. A uniﬁed approach to pattern recognition. Pattern Recognition  17(5):575–582 

1984.

[11] E. Pekalska and R. P. W. Duin. The Dissimilarity Representation for Pattern Recognition:

Foundations and Applications. World Scientiﬁc  2005.

[12] J. Laub  J. Macke  K. R. M¨uller  and F. A. Wichmann. Inducing metric violations in human

similarity judgements. In NIPS 19  pages 777–784. MIT Press  2007.

[13] L. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. JMLR  9(Nov):2579–2605 

2008.

[14] N. D. Lawrence. Spectral dimensionality reduction via maximum entropy. In AISTATS’11 

JMLR W&CP 15  pages 51–59  2011.

[15] K. Q. Weinberger  F. Sha  and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality

reduction. In ICML’04  pages 839–846  2004.

[16] J. Leskovec  J. Kleinberg  and C. Faloutsos. Graph evolution: Densiﬁcation and shrinking

diameters. ACM Transactions on Knowledge Discovery from Data  1(1)  2007.

[17] D. L. Nelson  C. L. McEvoy  and T. A Schreiber.

The university of South Florida
word association  rhyme  and word fragment norms. 1998. http://www.usf.edu/
FreeAssociation.

9

,Ke Sun
Jun Wang
Stephane Marchand-Maillet
Jian Zhao
Lin Xiong
Panasonic Karlekar Jayashree
Jianshu Li
Fang Zhao
Zhecan Wang
Panasonic Sugiri Pranata
Panasonic Shengmei Shen
Shuicheng Yan
Jiashi Feng