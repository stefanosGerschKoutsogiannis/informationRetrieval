2009,Efficient and Accurate Lp-Norm Multiple Kernel Learning,Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations and hence support interpretability. Unfortunately  L1-norm MKL is hardly observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures  we generalize MKL to arbitrary Lp-norms. We devise new insights on the connection between  several existing MKL formulations and develop two efficient interleaved optimization strategies for arbitrary p>1. Empirically  we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches. Finally  we apply Lp-norm MKL to real-world problems from computational biology  showing that non-sparse MKL achieves accuracies that go beyond the state-of-the-art.,Efﬁcient and Accurate (cid:96)p-Norm

Multiple Kernel Learning

Marius Kloft

University of California

Berkeley  USA

Ulf Brefeld

Yahoo! Research
Barcelona  Spain

S¨oren Sonnenburg

Technische Universit¨at Berlin

Berlin  Germany

Pavel Laskov

Universit¨at T¨ubingen
T¨ubingen  Germany

Klaus-Robert M¨uller

Technische Universit¨at Berlin

Berlin  Germany

Alexander Zien

LIFE Biosystems GmbH

Heidelberg  Germany

Abstract

Learning linear combinations of multiple kernels is an appealing strategy when
the right choice of features is unknown. Previous approaches to multiple kernel
learning (MKL) promote sparse kernel combinations to support interpretability.
Unfortunately  (cid:96)1-norm MKL is hardly observed to outperform trivial baselines in
practical applications. To allow for robust kernel mixtures  we generalize MKL
to arbitrary (cid:96)p-norms. We devise new insights on the connection between several
existing MKL formulations and develop two efﬁcient interleaved optimization
strategies for arbitrary p > 1. Empirically  we demonstrate that the interleaved
optimization strategies are much faster compared to the traditionally used wrap-
per approaches. Finally  we apply (cid:96)p-norm MKL to real-world problems from
computational biology  showing that non-sparse MKL achieves accuracies that go
beyond the state-of-the-art.

1

Introduction

kernel matrices K1  . . .   KM   and the task is to learn the optimal mixture K =(cid:80)

Sparseness is being regarded as one of the key features in machine learning [15] and biology [16].
Sparse models are appealing since they provide an intuitive interpretation of a task at hand by sin-
gling out relevant pieces of information. Such automatic complexity reduction facilitates efﬁcient
training algorithms  and the resulting models are distinguished by small capacity. The interpretabil-
ity is one of the main reasons for the popularity of sparse methods in complex domains such as
computational biology  and consequently building sparse models from data has received a signiﬁ-
cant amount of recent attention.
Unfortunately  sparse models do not always perform well in practice [7  15]. This holds particularly
for learning sparse linear combinations of data sources [15]  an abstraction of which is known as
multiple kernel learning (MKL) [10]. The data sources give rise to a set of (possibly correlated)
m θmKm for the
problem at hand. Previous MKL research aims at ﬁnding sparse mixtures to effectively simplify
the underlying data representation. For instance  [10] study semi-deﬁnite matrices K (cid:23) 0 inducing
sparseness by bounding the trace tr(K) ≤ c; unfortunately  the resulting semi-deﬁnite optimization
problems are computationally too expensive for large-scale deployment.
Recent approaches to MKL promote sparse solutions either by Tikhonov regularization over the
mixing coefﬁcients [25] or by incorporating an additional constraint (cid:107)θ(cid:107) ≤ 1 [18  27] requiring
solutions on the standard simplex  known as Ivanov regularization. Based on the one or the other 
efﬁcient optimization strategies have been proposed for solving (cid:96)1-norm MKL using semi-inﬁnite
linear programming [21]  second order approaches [6]  gradient-based optimization [19]  and level-
set methods [26]. Other variants of (cid:96)1-norm MKL have been proposed in subsequent work address-
ing practical algorithms for multi-class [18  27] and multi-label [9] problems.

1

unweighted-sum kernels K =(cid:80)

Previous approaches to MKL successfully identify sparse kernel mixtures  however  the solutions
found  frequently suffer from poor generalization performances. Often  trivial baselines using
m Km are observed to outperform the sparse mixture [7]. One rea-
son for the collapse of (cid:96)1-norm MKL is that kernels deployed in real-world tasks are usually highly
sophisticated and effectively capture relevant aspects of the data. In contrast  sparse approaches to
MKL rely on the assumption that some kernels are irrelevant for solving the problem. Enforcing
sparse mixtures in these situations may lead to degenerate models. As a remedy  we propose to
sacriﬁce sparseness in these situations and deploy non-sparse mixtures instead. After submission of
this paper  we learned about a related approach  in which the sum of an (cid:96)1- and an (cid:96)2-regularizer are
used [12]. Although non-sparse solutions are not as easy to interpret  they account for (even small)
contributions of all available kernels to live up to practical applications.
In this paper  we ﬁrst show the equivalence of the most common approaches to (cid:96)1-norm MKL
[18  25  27]. Our theorem allows for a generalized view of recent strands of multiple kernel learn-
ing research. Based on the detached view  we extend the MKL framework to arbitrary (cid:96)p-norm
MKL with p ≥ 1. Our approach can either be motivated by additionally regularizing over the mix-
ing coefﬁcients (cid:107)θ(cid:107)p
p ≤ 1. We propose two
alternative optimization strategies based on Newton descent and cutting planes  respectively. Em-
pirically  we demonstrate the efﬁciency and accuracy of none-sparse MKL. Large-scale experiments
on gene start detection show a signiﬁcant improvement of predictive accuracy compared to (cid:96)1- and
(cid:96)∞-norm MKL.
The rest of the paper is structured as follows. We present our main contributions in Section 2 
the theoretical analysis of existing approaches to MKL  our (cid:96)p-norm MKL generalization with two
highly efﬁcient optimization strategies  and relations to (cid:96)1-norm MKL. We report on our empirical
results in Section 3 and Section 4 concludes.

p  or equivalently by incorporating the constraint (cid:107)θ(cid:107)p

2 Generalized Multiple Kernel Learning

2.1 Preliminaries
In the standard supervised learning setup  a labeled sample D = {(xi  yi)}i=1... n is given  where
the x lie in some input space X and y ∈ Y ⊂ R. The goal is to ﬁnd a hypothesis f ∈ H 
that generalizes well on new and unseen data. Applying regularized risk minimization returns the
minimizer f∗ 

f∗ = argminf Remp(f) + λΩ(f) 

where Remp(f) = 1
i=1 V (f(xi)  yi) is the empirical risk of hypothesis f w.r.t. to the loss V :
R × Y → R  regularizer Ω : H → R  and trade-off parameter λ > 0. In this paper  we focus on
n
Ω(f) = 1

2 and on linear models of the form

2(cid:107) ˜w(cid:107)2

(1)
together with a (possibly non-linear) mapping ψ : X → H to a Hilbert space H [20]. We will later
make use of kernel functions K(x  x(cid:48)) = (cid:104)ψ(x)  ψ(x(cid:48))(cid:105)H to compute inner products in H.

f ˜w b(x) = ˜w(cid:62)ψ(x) + b 

(cid:80)n

2.2 Learning with Multiple Kernels
When learning with multiple kernels  we are given M different feature mappings ψm : X →
Hm  m = 1  . . . M  each giving rise to a reproducing kernel Km of Hm. Approaches to multi-

ple kernel learning consider linear kernel mixtures Kθ =(cid:80) θmKm  θm ≥ 0. Compared to Eq. (1) 

the primal model for learning with multiple kernels is extended to

f ˜w b θ(x) = ˜w(cid:62)ψθ(xi) + b =

θm ˜w(cid:62)

mψm(x) + b 

(2)

1   . . .   ˜w(cid:62)

M )(cid:62) and ψθ =

where the weight vector ˜w and the composite feature map ψθ have a block structure ˜w =
( ˜w(cid:62)
The idea in learning with multiple kernels is to minimize the loss on the training data w.r.t. to

optimal kernel mixture(cid:80) θmKm in addition to regularizing θ to avoid overﬁtting. Hence  in terms

θ1ψ1 × . . . × √

θM ψM   respectively.

√

m=1

2

M(cid:88)

(cid:112)

of regularized risk minimization  the optimization problem becomes

n(cid:88)

i=1

M(cid:88)

m=1

inf

˜w b θ≥0

1
n

V (fw b θ(xi)  yi) + λ
2

(cid:107) ˜wm(cid:107)2

2 + ˜µ˜Ω[θ].

(3)

(cid:32) M(cid:88)

n(cid:88)

i=1

(cid:33)

M(cid:88)

(cid:107)wm(cid:107)2

2

θm

Previous approaches to multiple kernel learning employ regularizers of the form ˜Ω(θ) = ||θ||1 to
promote sparse kernel mixtures. By contrast  we propose to use smooth convex regularizers of the
p  1 < p < ∞  allowing for non-sparse solutions. The non-convexity of the
form ˜Ω(θ) = ||θ||p
resulting optimization problem is not inherent and can be resolved by substituting wm ← √
θm ˜wm.
Furthermore  regularization parameter and sample size can be decoupled by introducing ˜C = 1
(and adjusting µ ← ˜µ
λ ) which has favorable scaling properties in practice. We obtain the following
convex optimization problem [5] that has also been considered by [25] for hinge loss and p = 1 

nλ

+

1
2

V

˜C

inf

w b θ≥0

mψm(xi) + b  yi

w(cid:62)
(4)
m=1
0 = 0 if t = 0 and ∞ otherwise. An alternative approach has
where we use the convention that t
been studied by [18  27] (again using hinge loss and p = 1). They upper bound the value of the
regularizer (cid:107)θ(cid:107)1 ≤ 1 and incorporate the latter as an additional constraint into the optimization
problem. For C > 0  they arrive at
w(cid:62)

(cid:32) M(cid:88)

+ µ||θ||p
p 

||wm||2

n(cid:88)

M(cid:88)

(cid:33)

mψm(xi) + b  yi

||θ||p

p ≤ 1.

s.t.

C

V

(5)

m=1

inf

w b θ≥0

+

1
2

m=1

2

θm

i=1

m=1

Our ﬁrst contribution shows that both  the Tikhonov regularization in Eq. (4) and the Ivanov regu-
larization in Eq. (5)  are equivalent.
Theorem 1 Let be p ≥ 1. For each pair ( ˜C  µ) there exists C > 0 such that for each optimal
∗) is also an optimal solution
solution (w∗  b∗  θ
of Eq. (5) using C  and vice versa  where κ > 0 is some multiplicative constant.

∗) of Eq. (4) using ( ˜C  µ)  we have that (w∗  b∗  κ θ

Proof. The proof is shown in the supplementary material for lack of space. Sketch of the proof:
We incorporate the regularizer of (4) into the constraints and show that the resulting upper bound is
2
tight. A variable substitution completes the proof.
Zien and Ong [27] showed that the MKL optimization problems by Bach et al. [3]  Sonnenburg et
al. [21]  and their own formulation are equivalent. As a main implication of Theorem 1 and by using
the result of Zien and Ong it follows that the optimization problem of Varma and Ray [25] and the
ones from [3  18  21  27] all are equivalent.
In addition  our result shows the coupling between trade-off parameter C and the regularization pa-
rameter µ in Eq. (4): tweaking one also changes the other and vice versa. Moreover  Theorem 1
implies that optimizing C in Eq. (5) implicitly searches the regularization path for the parameter µ
of Eq. (4). In the remainder  we will therefore focus on the formulation in Eq. (5)  as a single param-
eter is preferable in terms of model selection. Furthermore  we will focus on binary classiﬁcation
problems with Y = {−1  +1}  equipped with the hinge loss V (f(x)  y) = max{0  1 − yf(x)}.
However note  that all our results can easily be transferred to regression and multi-class settings
using appropriate convex loss functions and joint kernel extensions.

2.3 Non-Sparse Multiple Kernel Learning

We now extend the existing MKL framework to allow for non-sparse kernel mixtures θ  see also
[13]. Let us begin with rewriting Eq. (5) by expanding the hinge loss into the slack variables as
follows

M(cid:88)

m=1

||wm||2

2

θm

(cid:32) M(cid:88)

min
θ w b ξ

1
2

+ C(cid:107)ξ(cid:107)1

s.t. ∀i : yi

w(cid:48)

mψm(xi) + b

m=1

3

(cid:33)

≥ 1 − ξi ;

ξ ≥ 0 ;

θ ≥ 0 ;

(cid:107)θ(cid:107)p

p ≤ 1.

(6)

(cid:32) M(cid:88)

m=1

(cid:18)1
2 α(cid:62)Qmα

δ =

p−1(cid:33) p−1
(cid:19) p

p

.

(8)

Applying Lagrange’s theorem incorporates the constraints into the objective by introducing non-
negative Lagrangian multipliers α  β ∈ Rn  γ ∈ RM   δ ∈ R (including a pre-factor of 1
p for the
δ-Term). Resubstitution of optimality conditions w.r.t. to w  b  ξ  and θ removes the dependency
of the Lagrangian on the primal variables. After some additional algebra (e.g.  the terms associated
with γ cancel)  the Lagrangian can be written as

(cid:32) M(cid:88)

m=1

(cid:18)1
2 α(cid:62)Qmα

p−1(cid:33)
(cid:19) p

 

(7)

L = 1(cid:62)α − 1
p

δ − p − 1

p

− 1
p−1

δ

where Qm = diag(y)Kmdiag(y). Eq. (7) now has to be maximized w.r.t. to the dual variables α  δ 
subject to α(cid:62)y = 0  0 ≤ αi ≤ C for 1 ≤ i ≤ n  and δ ≥ 0. Let us ignore for a moment the
non-negativity δ ≥ 0 and solve ∂L/∂δ = 0 for the unbounded δ. Setting the partial derivative to
zero yields

(cid:32) M(cid:88)

(cid:0)α(cid:62)Qmα(cid:1) p

(cid:33) p−1

p

Interestingly  at optimality  we always have δ ≥ 0 because the quadratic term in α is non-negative.
Plugging the optimal δ into Eq. (7)  we arrive at the following optimization problem which solely
depends on α.

α

max

1(cid:62)α − 1
2

In the limit p → ∞  the above problem reduces to the SVM dual (with Q =(cid:80)

m Qm)  while p → 1
gives rise to a QCQP (cid:96)1-MKL variant. However  optimizing the dual efﬁciently is difﬁcult and will
cause numerical problems in the limits p → 1 and p → ∞.

s.t. 0 ≤ α ≤ C1; α(cid:62)y = 0.

(9)

m=1

p−1

2.4 Two Efﬁcient Second-Order Optimization Strategies

Many recent MKL solvers (e.g.  [19  24  26]) are based on wrapping linear programs around SVMs.
From an optimization standpoint  our work is most closely related to the SILP approach [21] and
the simpleMKL method [19  24]. Both of these methods also aim at efﬁcient large-scale MKL
algorithms. The two alternative approaches proposed for (cid:96)p-norm MKL proposed in this paper are
largely inspired by these methods and extend them in two aspects: customization to arbitrary norms
and a tight coupling with minor iterations of an SVM solver  respectively.
Our ﬁrst strategy interleaves maximizing the Lagrangian of (6) w.r.t. α with minor precision and
Newton descent on θ. For the second strategy  we devise a semi-inﬁnite convex program  which we
solve by column generation with nested sequential quadratically constrained linear programming
(SQCLP). In both cases  the maximization step w.r.t. α is performed by chunking optimization with
minor iterations. The Newton approach can be applied without a common purpose QCQP solver 
however  convergence can only be guaranteed for the SQCLP [8].

2.4.1 Newton Descent

For a Newton descent on the mixing coefﬁcients  we ﬁrst compute the partial derivatives
+ (p − 1)δθp−2

+ δθp−1

and

∂L
∂θm

(cid:124)
= −1
2

w(cid:62)
mwm
θ2
m
=:∇θm

(cid:123)(cid:122)

m

(cid:125)

∂2L
∂2θm

= w(cid:62)
(cid:124)

mwm
θ3
m

(cid:123)(cid:122)

=:hm

m

(cid:125)

of the original Lagrangian. Fortunately  the Hessian H is diagonal  i.e. given by H = diag(h). The
m-th element sm of the corresponding Netwon step  deﬁned as s := −H−1∇θ  is thus computed
by

sm =

1

2 θm||wm||2 − δθp+2
||wm||2 + (p − 1)δθp+1

m

m

 

4

where δ is deﬁned in Eq. (8). However  a Newton step θt+1 = θt + s might lead to non-positive θ.
To avoid this awkward situation  we take the Newton steps in the space of log(θ) by adjusting the
derivatives according to the chain rule. We obtain
m) −

m ) = log(θt

log(θt+1

(10)

/(θt

m)2  

θm

∇t
/θt
m
m)2 − ∇t
(cid:33)

θm

m/(θt
ht

(cid:32) ∇t

θm

θt
m
− ht

m

∇t

θm

which corresponds to multiplicative update of θ:

θt+1
m

= θt

m · exp

.

(11)

Furthermore we additionally enhance the Newton step by a line search.

2.4.2 Cutting Planes

In order to obtain an alternative optimization strategy  we ﬁx θ and build the partial Lagrangian
w.r.t. all other primal variables w  b  ξ. The derivation is analogous to [18  27] and we omit details
for lack of space. The resulting dual problem is a min-max problem of the form

2 α(cid:62) M(cid:88)

min
θ

1(cid:62)α − 1

α

max
θmQmα
s.t. 0 ≤ α ≤ C1; y(cid:62)α = 0;

m=1

θ ≥ 0;

(cid:107)θ(cid:107)p

p ≤ 1.

The above optimization problem is a saddle point problem and can be solved by alternating α and
θ optimization step. While the former can simply be carried out by a support vector machine for a
ﬁxed mixture θ  the latter has been optimized for p = 1 by reduced gradients [18].
We take a different approach and translate the min-max problem into an equivalent semi-inﬁnite
program (SIP) as follows. Denote the value of the target function by t(α  θ) and suppose α∗ is
optimal. Then  according to the max-min inequality [5]  we have t(α∗  θ) ≥ t(α  θ) for all α and
θ. Hence  we can equivalently minimize an upper bound η on the optimal value and arrive at

min
η θ

η

s.t.

η ≥ 1(cid:62)α − 1

θmQmα

(12)

p ≤ 1 and θ ≥ 0.

for all α ∈ Rn with 0 ≤ α ≤ C1  and y(cid:62)α = 0 as well as (cid:107)θ(cid:107)p
[21] optimize the above SIP for p ≥ 1 with interleaving cutting plane algorithms. The solution of
a quadratic program (here the regular SVM) generates the most strongly violated constraint for the
actual mixture θ. The optimal (θ
  η) is then identiﬁed by solving a linear program with respect to
the set of active constraints. The optimal mixture is then used for computing a new constraint and
so on.
Unfortunately  for p > 1  a non-linearity is introduced by requiring (cid:107)θ(cid:107)p
p ≤ 1 and such constraint is
unlikely to be found in standard optimization toolboxes that often handle only linear and quadratic
constraints. As a remedy  we propose to approximate (cid:107)θ(cid:107)p
p ≤ 1 by sequential second-order Taylor
expansion of the form

∗

2 α(cid:62) M(cid:88)

m=1

||θ||p

p ≈ 1 + p(p − 3)

2

p(p − 2)(˜θm)p−1 θm + p(p − 1)

2

− M(cid:88)

m=1

M(cid:88)

m=1

˜θp−2
m θ2
m 

M ). The sequence (θ0  θ1 ··· ) is initial-
where θp is deﬁned element-wise  that is θp := (θp
ized with a uniform mixture satisfying (cid:107)θ0(cid:107)p
p = 1 as a starting point. Successively θt+1 is computed
using ˜θ = θt. Note that the quadratic term in the approximation is diagonal wherefore the subse-
quent quadratically constrained problem can be solved efﬁciently. Finally note  that this approach
can be further sped-up by an additional projection onto the level-sets in the θ-optimization phase
similar to [26]. In our case  the level-set projection is a convex quadratic problem with (cid:96)p-norm
constraints and can again be approximated by successive second-order Taylor expansions.

1  ...  θp

5

Figure 1: Execution times of SVM Training  (cid:96)p-norm MKL based on interleaved optimization via the Newton 
the cutting plane algorithm (CPA)  and the SimpleMKL wrapper. (left) Training using ﬁxed number of 50
kernels varying training set size. (right) For 500 examples and varying numbers of kernels. Our proposed
Newton and CPA obtain speedups of over an order of magnitude. Notice the tiny error bars.

3 Computational Experiments

In this section we study non-sparse MKL in terms of efﬁciency and accuracy.1 We apply the method
of [21] for (cid:96)1-norm results as it is contained as a special case of our cutting plane strategy. We write

(cid:96)∞-norm MKL for a regular SVM with the unweighted-sum kernel K =(cid:80)

m Km.

3.1 Execution Time

We demonstrate the efﬁciency of our implementations of non-sparse MKL. We experiment on the
MNIST data set where the task is to separate odd vs. even digits. We compare our (cid:96)p-norm MKL
with two methods for (cid:96)1-norm MKL  simpleMKL [19] and SILP-based chunking [21]  and to SVMs
using the unweighted-sum kernel ((cid:96)∞-norm MKL) as additional baseline. We optimize all methods
up to a precision of 10−3 for the outer SVM-ε and 10−5 for the “inner” SIP precision and computed
relative duality gaps. To provide a fair stopping criterion to simpleMKL  we set the stopping criterion
of simpleMKL to the relative duality gap of its (cid:96)1-norm counterpart. This way  the deviations of
relative objective values of (cid:96)1-norm MKL variants are guaranteed to be smaller than 10−4. SVM
trade-off parameters are set to C = 1 for all methods.
Figure 1 (left) displays the results for varying sample sizes and 50 precomputed Gaussian kernels
with different bandwidths. Error bars indicate standard error over 5 repetitions. Unsurprisingly 
the SVM with the unweighted-sum kernel is the fastest method. Non-sparse MKL scales similarly
as (cid:96)1-norm chunking; the Newton strategy (Section 2.4.1) is slightly faster than the cutting plane
variant (Section 2.4.2) that needs additional Taylor expansions within each θ-step. SimpleMKL
suffers from training an SVM to full precision for each gradient evaluation and performs worst.2
Figure 1 (right) shows the results for varying the number of precomputed RBF kernels for a ﬁxed
sample size of 500. The SVM with the unweighted-sum kernel is hardly affected by this setup and
performs constantly. The (cid:96)1-norm MKL by [21] handles the increasing number of kernels best and is
the fastest MKL method. Non-sparse approaches to MKL show reasonable run-times  the Newton-
based (cid:96)p-norm MKL being again slightly faster than its peer. Simple MKL performs again worst.
Overall  our proposed Newton and cutting plane based optimization strategies achieve a speedup of
often more than one order of magnitude.

3.2 Protein Subcellular Localization

The prediction of the subcellular localization of proteins is one of the rare empirical success stories
of (cid:96)1-norm-regularized MKL [17  27]: after deﬁning 69 kernels that capture diverse aspects of

1Available at http://www.shogun-toolbox.org/
2SimpleMKL could not be evaluated for 2000 instances (ran out of memory on a 4GB machine).

6

10210310−210−1100101102sample sizetime in seconds10110210310−1100101102number of kernelstime in seconds  Table 1: Results for Protein Subcellular Localization

(cid:96)p-norm
1 - MCC [%]

1

9.13

32/31
9.12

16/15
9.64

8/7
9.84

4/3
9.56

2

10.18

4

10.08

∞
10.41

protein sequences  (cid:96)1-norm-MKL could raise the predictive accuracy signiﬁcantly above that of
the unweighted sum of kernels (thereby also improving on established prediction systems for this
problem). Here we investigate the performance of non-sparse MKL.
We download the kernel matrices of the dataset plant3 and follow the experimental setup of [17]
with the following changes:
instead of a genuine multiclass SVM  we use the 1-vs-rest decom-
position; instead of performing cross-validation for model selection  we report results for the best
models  as we are only interested in the relative performance of the MKL regularizers. Speciﬁcally 
for each C ∈ {1/32  1/8  1/2  1  2  4  8  32  128}  we compute the average Mathews correlation co-
efﬁcient (MCC) on the test data. For each norm  the best average MCC is recorded. Table 1 shows
the averages over several splits of the data.
The results indicate that  indeed  with proper choice of a non-sparse regularizer  the accuracy of
(cid:96)1-norm can be recovered. This is remarkable  as this dataset is particular in that it fullﬁlls the rare
condition that (cid:96)1-norm MKL performs better than (cid:96)∞-norm MKL. In other words  selecting these
data may imply a bias towards (cid:96)1-norm. Nevertheless our novel non-sparse MKL can keep up with
this  essentially by approximating (cid:96)1-norm.

3.3 Gene Start Recognition

This experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes
in genomic DNA sequences. Accurate detection of the transcription start site is crucial to identify
genes and their promoter regions and can be regarded as a ﬁrst step in deciphering the key regulatory
elements in the promoter region that determine transcription. For our experiments we use the dataset
from [22] which contains a curated set of 8 508 TSS annotated genes built from dbTSS version 4
[23] and refseq genes. These are translated into positive training instances by extracting windows of
size [−1000  +1000] around the TSS. Similar to [4]  85 042 negative instances are generated from
the interior of the gene using the same window size.
Following [22]  we employ ﬁve different kernels representing the TSS signal (weighted degree with
shift)  the promoter (spectrum)  the 1st exon (spectrum)  angles (linear)  and energies (linear). Opti-
mal kernel parameters are determined by model selection in [22]. Every kernel is normalized such
that all points have unit length in feature space. We reserve 13 000 and 20 000 randomly drawn
instances for holdout and test sets  respectively  and use the remaining 60 000 as the training pool.
Figure 2 shows test errors for varying training set sizes drawn from the pool; training sets of the
same size are disjoint. Error bars indicate standard errors of repetitions for small training set sizes.
Regardless of the sample size  (cid:96)1-MKL is signiﬁcantly outperformed by the sum-kernel. On the
contrary  non-sparse MKL signiﬁcantly achieves higher AUC values than the (cid:96)∞-MKL for sample
sizes up to 20k. The scenario is well suited for (cid:96)2-norm MKL which performs best. Finally  for 60k
training instances  all methods but (cid:96)1-norm MKL yield the same performance. Again  the superior
performance of non-sparse MKL is remarkable  and of signiﬁcance for the application domain: the
method using the unweighted sum of kernels [22] has recently been conﬁrmed to be the leading in
a comparison of 19 state-of-the-art promoter prediction programs [1]  and our experiments suggest
that its accuracy can be further elevated by non-sparse MKL.

4 Conclusion and Discussion

We presented an efﬁcient and accurate approach to non-sparse multiple kernel learning and showed
that our (cid:96)p-norm MKL can be motivated as Tikhonov and Ivanov regularization of the mixing coef-
ﬁcients  respectively. Applied to previous MKL research  our result allows for a uniﬁed view as so
far seemingly different approaches turned out to be equivalent. Furthermore  we devised two efﬁ-
cient approaches to non-sparse multiple kernel learning for arbitrary (cid:96)p-norms  p > 1. The resulting

3from http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc/

7

Figure 2: Left: Area under ROC curve (AUC) on test data for TSS recognition as a function of the training
set size. Notice the tiny bars indicating standard errors w.r.t. repetitions on disjoint training sets. Right: Corre-
sponding kernel mixtures. For p = 1 consistent sparse solutios are obtained while the optimal p = 2 distributes
wheights on the weighted degree and the 2 spectrum kernels in good agreement to [22].

√

optimization strategies are based on semi-inﬁnite programming and Newton descent  both inter-
leaved with chunking-based SVM training. Execution times moreover revealed that our interleaved
optimization vastly outperforms commonly used wrapper approaches.
We would like to note that there is a certain preference/obsession for sparse models in the scientiﬁc
community due to various reasons. The present paper  however  shows clearly that sparsity by itself
is not the ultimate virtue to be strived for. Rather on the contrary: non-sparse model may improve
quite impressively over sparse ones. The reason for this is less obvious and its theoretical explo-
ration goes well beyond the scope of its submissions. We remark nevertheless that some interesting
asymptotic results exist that show model selection consistency of sparse MKL (or the closely related
group lasso) [2  14]  in other words in the limit n → ∞ MKL is guaranteed to ﬁnd the correct subset
of kernels. However  also the rate of convergence to the true estimator needs to be considered  thus
we conjecture that the rate slower than
n which is common to sparse estimators [11] may be one
of the reasons for ﬁnding excellent (nonasymptotic) results in non-sparse MKL. In addition to the
convergence rate the variance properties of MKL estimators may play an important role to elucidate
the performance seen in our various simulation experiments.
Intuitively speaking  we observe clearly that in some cases all features even though they may contain
redundant information are to be kept  since putting their contributions to zero does not improve
prediction. I.e. all of them are informative to our MKL models. Note however that this result is
also class speciﬁc  i.e. for some classes we may sparsify. Cross-validation based model building that
includes the choice of p will however inevitably tell us which classes should be treated sparse and
which non-sparse.
Large-scale experiments on TSS recognition even raised the bar for (cid:96)1-norm MKL: non-sparse MKL
proved consistently better than its sparse counterparts which were outperformed by an unweighted-
sum kernel. This exempliﬁes how the unprecedented combination of accuracy and scalability of our
MKL approach and methods paves the way for progress in other real world applications of machine
learning.

Authors’ Contributions

The authors contributed in the following way: MK and UB had the initial idea. MK  UB  SS  and AZ each
contributed substantially to both mathematical modelling  design and implementation of algorithms  conception
and execution of experiments  and writing of the manuscript. PL had some shares in the initial phase and KRM
contributed to the text. Most of the work was done at previous afﬁliations of several authors: Fraunhofer
Institute FIRST (Berlin)  Technical University Berlin  and the Friedrich Miescher Laboratory (T¨ubingen).

Acknowledgments

This work was supported in part by the German BMBF grant REMIND (FKZ 01-IS07007A) and by the Euro-
pean Community under the PASCAL2 Network of Excellence (ICT-216886).

8

010K20K30K40K50K60K0.880.890.90.910.920.93sample sizeAUC  1−norm MKL4/3−norm MKL2−norm MKL4−norm MKLSVM1−normn=5k4/3−norm2−norm4−normunw.−sumn=20kn=60kReferences
[1] T. Abeel  Y. V. de Peer  and Y. Saeys. Towards a gold standard for promoter prediction evaluation.

Bioinformatics  2009.

[2] F. R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res.  9:1179–

1225  2008.

[3] F. R. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the smo

algorithm. In Proc. 21st ICML. ACM  2004.

[4] V. B. Bajic  S. L. Tan  Y. Suzuki  and S. Sugano. Promoter prediction analysis on the whole human

genome. Nature Biotechnology  22(11):1467–1473  2004.

[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambrigde University Press  Cambridge  UK  2004.
[6] O. Chapelle and A. Rakotomamonjy. Second order optimization of kernel parameters. In Proc. of the

NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels  2008.

[7] C. Cortes  A. Gretton  G. Lanckriet  M. Mohri  and A. Rostamizadeh. Proceedings of the NIPS Workshop

on Kernel Learning: Automatic Selection of Optimal Kernels  2008.

[8] R. Hettich and K. O. Kortanek. Semi-inﬁnite programming: theory  methods  and applications. SIAM

Rev.  35(3):380–429  1993.

[9] S. Ji  L. Sun  R. Jin  and J. Ye. Multi-label multiple kernel learning. In Advances in Neural Information

Processing Systems  2009.

[10] G. Lanckriet  N. Cristianini  L. E. Ghaoui  P. Bartlett  and M. I. Jordan. Learning the kernel matrix with

semi-deﬁnite programming. JMLR  5:27–72  2004.

[11] H. Leeb and B. M. P¨otscher. Sparse estimators and the oracle property  or the return of hodges’ estimator.

Journal of Econometrics  142:201–211  2008.

[12] C. Longworth and M. J. F. Gales. Combining derivative and parametric kernels for speaker veriﬁcation.

IEEE Transactions in Audio  Speech and Language Processing  17(4):748–757  2009.

[13] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine

Learning Research  6:1099–1125  2005.

[14] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for linear models.

Electron. J. Statist.  2:605–633  2008.

[15] S. Olhede  M. Pontil  and J. Shawe-Taylor. Proceedings of the PASCAL2 Workshop on Sparsity in

Machine Learning and Statistics  2009.

[16] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381:607–609  1996.

[17] C. S. Ong and A. Zien. An Automated Combination of Kernels for Predicting Protein Subcellular Local-

ization. In Proc. of the 8th Workshop on Algorithms in Bioinformatics  2008.

[18] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet. More efﬁciency in multiple kernel learning. In

ICML  pages 775–782  2007.

[19] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet. SimpleMKL. Journal of Machine Learning

Research  9:2491–2521  2008.

[20] B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[21] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large Scale Multiple Kernel Learning. Journal

of Machine Learning Research  7:1531–1565  July 2006.

[22] S. Sonnenburg  A. Zien  and G. R¨atsch. ARTS: Accurate Recognition of Transcription Starts in Human.

Bioinformatics  22(14):e472–e480  2006.

[23] Y. Suzuki  R. Yamashita  K. Nakai  and S. Sugano. dbTSS: Database of human transcriptional start sites

and full-length cDNAs. Nucleic Acids Research  30(1):328–331  2002.

[24] M. Szafranski  Y. Grandvalet  and A. Rakotomamonjy. Composite kernel learning. In Proceedings of the

International Conference on Machine Learning  2008.

[25] M. Varma and D. Ray. Learning the discriminative power-invariance trade-off. In IEEE 11th International

Conference on Computer Vision (ICCV)  pages 1–8  2007.

[26] Z. Xu  R. Jin  I. King  and M. Lyu. An extended level method for efﬁcient multiple kernel learning. In
D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information Processing
Systems 21  pages 1825–1832. 2009.

[27] A. Zien and C. S. Ong. Multiclass multiple kernel learning. In Proceedings of the 24th international

conference on Machine learning (ICML)  pages 1191–1198. ACM  2007.

9

,Mateusz Malinowski
Mario Fritz
Simyung Chang
John Yang
Jaeseok Choi
Nojun Kwak