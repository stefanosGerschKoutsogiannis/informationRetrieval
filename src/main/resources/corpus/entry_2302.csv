2016,Geometric Dirichlet Means Algorithm for topic inference,We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function  which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections  which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference  while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.,Geometric Dirichlet Means algorithm

for topic inference

Mikhail Yurochkin
Department of Statistics
University of Michigan

moonfolk@umich.edu

XuanLong Nguyen

Department of Statistics
University of Michigan

xuanlong@umich.edu

Abstract

We propose a geometric algorithm for topic learning and inference that is built on
the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA)
model and its nonparametric extensions. To this end we study the optimization of a
geometric loss function  which is a surrogate to the LDA’s likelihood. Our method
involves a fast optimization based weighted clustering procedure augmented with
geometric corrections  which overcomes the computational and statistical inefﬁ-
ciencies encountered by other techniques based on Gibbs sampling and variational
inference  while achieving the accuracy comparable to that of a Gibbs sampler. The
topic estimates produced by our method are shown to be statistically consistent
under some conditions. The algorithm is evaluated with extensive experiments on
simulated and real data.

1

Introduction

Most learning and inference algorithms in the probabilistic topic modeling literature can be delineated
along two major lines: the variational approximation popularized in the seminal paper of Blei et al.
(2003)  and the sampling based approach studied by Pritchard et al. (2000) and other authors. Both
classes of inference algorithms  their virtues notwithstanding  are known to exhibit certain deﬁciencies 
which can be traced back to the need for approximating or sampling from the posterior distributions
of the latent variables representing the topic labels. Since these latent variables are not geometrically
intrinsic — any permutation of the labels yields the same likelihood — the manipulation of these
redundant quantities tend to slow down the computation  and compromise with the learning accuracy.
In this paper we take a convex geometric perspective of the Latent Dirichlet Allocation  which may
be obtained by integrating out the latent topic label variables. As a result  topic learning and inference
may be formulated as a convex geometric problem: the observed documents correspond to points
randomly drawn from a topic polytope  a convex set whose vertices represent the topics to be inferred.
The original paper of Blei et al. (2003) (see also Hofmann (1999)) contains early hints about a convex
geometric viewpoint  which is left unexplored. This viewpoint had laid dormant for quite some time 
until studied in depth in the work of Nguyen and co-workers  who investigated posterior contraction
behaviors for the LDA both theoretically and practically (Nguyen  2015; Tang et al.  2014).
Another fruitful perspective on topic modeling can be obtained by partially stripping away the
distributional properties of the probabilistic model and turning the estimation problem into a form
of matrix factorization (Deerwester et al.  1990; Xu et al.  2003; Anandkumar et al.  2012; Arora
et al.  2012). We call this the linear subspace viewpoint. For instance  the Latent Semantic Analysis
approach (Deerwester et al.  1990)  which can be viewed as a precursor of the LDA model  looks
to ﬁnd a latent subspace via singular-value decomposition  but has no topic structure. Notably  the
RecoverKL by Arora et al. (2012) is one of the recent fast algorithms with provable guarantees
coming from the linear subspace perspective.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

The geometric perspective continues to be the main force driving this work. We develop and analyze a
new class of algorithms for topic inference  which exploits both the convex geometry of topic models
and the distributional properties they carry. The main contributions in this work are the following: (i)
we investigate a geometric loss function to be optimized  which can be viewed as a surrogate to the
LDA’s likelihood; this leads to a novel estimation and inference algorithm — the Geometric Dirichlet
Means algorithm  which builds upon a weighted k-means clustering procedure and is augmented with
a geometric correction for obtaining polytope estimates; (ii) we prove that the GDM algorithm is
consistent  under conditions on the Dirichlet distribution and the geometry of the topic polytope; (iii)
we propose a nonparametric extension of GDM and discuss geometric treatments for some of the
LDA extensions; (v) ﬁnally we provide a thorough evaluation of our method against a Gibbs sampler 
a variational algorithm  and the RecoverKL algorithm. Our method is shown to be comparable to a
Gibbs sampler in terms of estimation accuracy  but much more efﬁcient in runtime. It outperforms
RecoverKL algorithm in terms of accuracy  in some realistic settings of simulations and in real data.
The paper proceeds as follows. Section 2 provides a brief background of the LDA and its convex
geometric formulation. Section 3 carries out the contributions outlined above. Section 4 presents
experiments results. We conclude with a discussion in Section 5.

2 Background on topic models

In this section we give an overview of the well-known Latent Dirichlet Allocation model for topic
modeling (Blei et al.  2003)  and the geometry it entails. Let α ∈ RK
+ be hyperparameters 
where V denotes the number of words in a vocabulary  and K the number of topics. The K topics are
represented as distributions on words: βk|η ∼ DirV (η)  for k = 1  . . .   K. Each of the M documents
can be generated as follows. First  draw the document topic proportions: θm|α ∼ DirK(α)  for
m = 1  . . .   M. Next  for each of the Nm words in document m  pick a topic label z and then sample
a word d from the chosen topic:

+ and η ∈ RV

znm|θm ∼ Categorical(θm); dnm|znm  β1...K ∼ Categorical(βznm

(1)
Each of the resulting documents is a vector of length Nm with entries dnm ∈ {1  . . .   V }  where
nm = 1  . . .   Nm. Because these words are exchangeable by the modeling  they are equivalently
represented as a vector of word counts wm ∈ NV . In practice  the Dirichlet distributions are often
simpliﬁed to be symmetric Dirichlet  in which case hyperparameters α  η ∈ R+ and we will proceed
with this setting. Two most common approaches for inference with the LDA are Gibbs sampling
(Grifﬁths & Steyvers  2004)  based on the Multinomial-Dirichlet conjugacy  and mean-ﬁeld inference
(Blei et al.  2003). The former approach produces more accurate estimates but is less computationally
efﬁcient than the latter. The inefﬁciency of both techniques can be traced to the need for sampling or
estimating the (redundant) topic labels. These labels are not intrinsic — any permutation of the topic
labels yield the same likelihood function.

).

Convex geometry of topics. By integrating out the latent variables that represent the topic labels 
we obtain a geometric formulation of the LDA. Indeed  integrating z’s out yields that  for m =
1  . . .   M 

wm|θm  β1...K  Nm ∼ Multinomial(pm1  . . .   pmV   Nm) 

where pmi denotes probability of observing the i-th word from the vocabulary in the m-th document 
and is given by

K(cid:88)

k=1

pmi =

θmkβki for i = 1  . . .   V ; m = 1  . . .   M.

(2)
The model’s geometry becomes clear. Each topic is represented by a point βk lying in the V − 1
dimensional probability simplex ∆V −1. Let B := Conv(β1  . . .   βK) be the convex hull of the
K topics βk  then each document corresponds to a point pm := (pm1  . . .   pmV ) lying inside the
polytope B. This point of view has been proposed before (Hofmann  1999)  although topic proportions
θ were not given any geometric meaning. The following treatment of θ lets us relate to the LDA’s
Dirichlet prior assumption and complete the geometric perspective of the problem. The Dirichlet
distribution generates probability vectors θm  which can be viewed as the (random) barycentric
k θmkβk is a vector of
cartesian coordinates of the m-th document’s multinomial probabilities. Given pm  document m is

coordinates of the document m with respect to the polytope B. Each pm =(cid:80)

2

generated by taking wm ∼ Multinomial(pm  Nm). In Section 4 we will show how this interpretation
of topic proportions can be utilized by other topic modeling approaches  including for example the
RecoverKL algorithm of Arora et al. (2012). In the following the model geometry is exploited to
derive fast and effective geometric algorithm for inference and parameter estimation.

3 Geometric inference of topics

We shall introduce a geometric loss function that can be viewed as a surrogate to the LDA’s likelihood.
To begin  let β denote the K × V topic matrix with rows βk  θ be a M × K document topic
proportions matrix with rows θm  and W be M × V normalized word counts matrix with rows
¯wm = wm/Nm.

3.1 Geometric surrogate loss to the likelihood

Unlike the original LDA formulation  here the Dirichlet distribution on θ can be viewed as a prior on
parameters θ. The log-likelihood of the observed corpora of M documents is

(cid:32) K(cid:88)
where the parameters β and θ are subject to constraints(cid:80)
(cid:80)

M(cid:88)

V(cid:88)

L(θ  β) =

wmi log

m=1

k=1

i=1

(cid:33)

θmkβki

 

i βki = 1 for each k = 1  . . .   K  and
k θmk = 1 for each m = 1  . . .   M. Partially relaxing these constraints and keeping only the one
that the sum of all entries for each row of the matrix product θβ is 1  yields the upper bound that
L(θ  β) ≤ L(W )  where function L(W ) is given by

(cid:88)

(cid:88)

m

i

L(W ) =

wmi log ¯wmi.

We can establish a tighter bound  which will prove useful (the proof of this and other technical results
are in the Supplement):
Proposition 1. Given a ﬁxed topic polytope B and θ. Let Um be the set of words present in document
m  and assume that pmi > 0 ∀ i ∈ Um  then

M(cid:88)

m=1

L(W ) − 1
2

Nm

i∈Um

(cid:88)

( ¯wmi − pmi)2 ≥ L(θ  β) ≥ L(W ) − M(cid:88)

(cid:88)

i∈Um

Nm

m=1

( ¯wmi − pmi)2.

1
pmi

Since L(W ) is constant  the proposition above shows that maximizing the likelihood has the effect of
minimizing the following quantity with respect to both θ and β:
( ¯wmi − pmi)2.

(cid:88)

(cid:88)

Nm

m

i

G(B)

(cid:88)

(cid:88)

For each ﬁxed β (and thus B)  minimizing ﬁrst with respect to θ leads to the following

M(cid:88)
where the second equality in the above display is due pm = (cid:80)

Nm min
x:x∈B
k θmkβk ∈ B. The proposition
suggests a strategy for parameter estimation: β (and B) can be estimated by minimizing the geometric
loss function G:

( ¯wmi − pmi)2 =

(cid:107)x − ¯wm(cid:107)2
2 

:= min

Nm

(3)

m=1

m

θ

i

B

min

G(B) = min

(4)
In words  we aim to ﬁnd a convex polytope B ∈ ∆V −1  which is closest to the normalized word
counts ¯wm of the observed documents. It is interesting to note the presence of document length Nm 
which provides the weight for the squared (cid:96)2 error for each document. Thus  our loss function adapts
to the varying length of documents in the collection. Without the weights  our objective is similar to
the sum of squared errors of the Nonnegative Matrix Factorization(NMF). Ding et al. (2006) studied

Nm min
x:x∈B

m=1

B

(cid:107)x − ¯wm(cid:107)2
2.

M(cid:88)

3

the relation between the likelihood function of interest and NMF  but with a different objective of
the NMF problem and without geometric considerations. Once ˆB is solved  ˆθ can be obtained as
the barycentric coordinates of the projection of ¯wm onto ˆB for each document m = 1  . . .   M (cf.
Eq (3)). We note that if K ≤ V   then B is a simplex and β1  . . .   βk in general positions are the
extreme points of B  and the barycentric coordinates are unique. (If K > V   the uniqueness no
ˆβ gives the cartesian coordinates of a point in B that minimizes
longer holds). Finally  ˆpm = ˆθT
(cid:107)x − ¯wm(cid:107)2. This projection
m
Euclidean distance to the maximum likelihood estimate: ˆpm = argmin
is not available in the closed form  but a fast algorithm is available (Golubitsky et al.  2012)  which
can easily be extended to ﬁnd the corresponding distance and to evaluate our geometric objective.

x∈B

3.2 Geometric Dirichlet Means algorithm

We proceed to devise a procedure for approximately solving the topic polytope B via Eq. (4): ﬁrst 
obtain an estimate of the underlying subspace based on weighted k-means clustering and then 
estimate the vertices of the polytope that lie on the subspace just obtained via a geometric correction
technique. Please refer to the Supplement for a clariﬁcation of the concrete connection between our
geometric loss function and other objectives which arise in subspace learning and weighted k-means
clustering literature  the connection that motivates the ﬁrst step of our algorithm.

Geometric Dirichlet Means (GDM) algorithm estimates a topic polytope B based on the training
documents (see Algorithm 1). The algorithm is conceptually simple  and consists of two main steps:
First  we perform a (weighted) k-means clustering on the M points ¯w1  . . .   ¯wM to obtain the K
centroids µ1  . . .   µK  and second  construct a ray emanating from a (weighted) center of the polytope
and extending through each of the centroids µk until it intersects with a sphere of radius Rk or
with the simplex ∆V −1 (whichever comes ﬁrst). The intersection point will be our estimate for
vertices βk  k = 1  . . .   K of the polytope B. The center C of the sphere is given in step 1 of the
(cid:107)C − ¯wm(cid:107)2  where the maximum is taken over those documents m
algorithm  while Rk = max
1≤m≤M
that are clustered with label k. To see the intuition behind the algorithm  let us consider a simple

Algorithm 1 Geometric Dirichlet Means (GDM)
Input: documents w1  . . .   wM   K 

extension scalar parameters m1  . . .   mK

(cid:80)

m ¯wm {ﬁnd center of the data}

Output: topics β1  . . .   βK
1: C = 1
M
2: µ1  . . .   µK = weighted k-means( ¯w1  . . .   ¯wM   K) {ﬁnd centers of K clusters}.
3: for all k = 1  . . .   K do
4:
5:
6:
7:
8:
end if
9:
10: end for
11: β1  . . .   βK.

βk = C + mk (µk − C).
if any βki < 0 then {threshold topic if it is outside vocabulary simplex ∆V −1}
for all i = 1  . . .   V do
.

(cid:80)

βki = βik1βki>0
i βki1βki>0

end for

simulation experiment. We use the LDA data generative model with α = 0.1  η = 0.1  V = 5 
K = 4  M = 5000  Nm = 100. Multidimensional scaling is used for visualization (Fig. 1). We
observe that the k-means centroids (pink) do not represent the topics very well  but our geometric
modiﬁcation ﬁnds extreme points of the tetrahedron: red and yellow spheres overlap  meaning we
found the true topics. In this example  we have used a very small vocabulary size  but in practice V is
much higher and the cluster centroids are often on the boundary of the vocabulary simplex  therefore
we have to threshold the betas at 0. Extending length until Rk is our default choice for the extension
parameters:

mk =

Rk

(cid:107)C − µk(cid:107)2

for k = 1  . . .   K 

(5)

4

Figure 1: Visualization of GDM: Black  green  red and blue are cluster assignments; purple is the
center  pink are cluster centroids  dark red are estimated topics and yellow are the true topics.

but we will see in our experiments that a careful tuning of the extension parameters based on
optimizing the geometric objective (4) over a small range of mk helps to improve the performance
considerably. We call this tGDM algorithm (tuning details are presented in the Supplement). The
connection between extension parameters and the thresholding is the following: if the cluster centroid
assigns probability to a word smaller than the whole data does on average  this word will be excluded
from topic k with large enough mk. Therefore  the extension parameters can as well be used to
control for the sparsity of the inferred topics.

3.3 Consistency of Geometric Dirichlet Means

We shall present a theorem which provides a theoretical justiﬁcation for the Geometric Dirichlet
Means algorithm. In particular  we will show that the algorithm can achieve consistent estimates
of the topic polytope  under conditions on the parameters of the Dirichlet distribution of the topic
proportion vector θm  along with conditions on the geometry of the convex polytope B. The problem
of estimating vertices of a convex polytope given data drawn from the interior of the polytope has
long been a subject of convex geometry — the usual setting in this literature is to assume the uniform
distribution for the data sample. Our setting is somewhat more general — the distribution of the points
iid∼ DirK(α).
inside the polytope will be driven by a symmetric Dirichlet distribution setting  i.e.  θm
(If α = 1 this results in the uniform distribution on B.) Let n = K − 1. Assume that the document
multinomial parameters p1  . . .   pM (given in Eq. (2)) are the actual data. Now we formulate a
geometric problem linking the population version of k-means and polytope estimation:
Problem 1. Given a convex polytope A ∈ Rn  a continuous probability density function f (x)
supported by A  ﬁnd a K-partition A =

Ak that minimizes:

K(cid:70)
(cid:90)

k=1

K(cid:88)

where µk is the center of mass of Ak: µk :=

Ak

k

(cid:107)µk − x(cid:107)2
(cid:82)

1
f (x) dx

Ak

(cid:82)

2f (x) dx 

xf (x) dx.

Ak

This problem is closely related to the Centroidal Voronoi Tessellations (Du et al.  1999). This
connection can be exploited to show that
Lemma 1. Problem 1 has a unique global minimizer.

In the following lemma  a median of a simplex is a line segment joining a vertex of a simplex with
the centroid of the opposite face.
Lemma 2. If A ∈ Rn is an equilateral simplex with symmetric Dirichlet density f parameterized by
α  then the optimal centers of mass of the Problem 1 lie on the corresponding medians of A.

5

Based upon these two lemmas  consistency is established under two distinct asymptotic regimes.
Theorem 1. Let B = Conv(β1  . . .   βK) be the true convex polytope from which the M-sample
p1  . . .   pM ∈ ∆V −1 are drawn via Eq. (2)  where θm

iid∼ DirK(α) for m = 1  . . .   M.

(a) If B is also an equilateral simplex  then topic estimates obtained by the GDM algorithm
using the extension parameters given in Eq. (5) converge to the vertices of B in probability 
as α is ﬁxed and M → ∞.

(b) If M is ﬁxed  while α → 0 then the topic estimates obtained by the GDM also converge to

the vertices of B in probability.

3.4 nGDM: nonparametric geometric inference of topics

In practice  the number of topics K may be unknown  necessitating a nonparametric probabilistic
approach such as the well-known Hierarchical Dirichlet Process (HDP) (Teh et al.  2006). Our
geometric approach can be easily extended to this situation. The objective (4) is now given by

M(cid:88)

m=1

min

B

G(B) = min

B

Nm min
x∈B

(cid:107)x − ¯wm(cid:107)2

2 + λ|B| 

(6)

where |B| denotes the number of extreme points of convex polytope B = Conv(β1  . . .   βK).
Accordingly  our nGDM algorithm now consists of two steps: (i) solve a penalized and weighted
k-means clustering to obtain the cluster centroids (e.g. using DP-means (Kulis & Jordan  2012));
(ii) apply geometric correction for recovering the extreme points  which proceeds as before. Our
theoretical analysis can be also extended to this nonparametric framework. We note that the penalty
term is reminiscent of the DP-means algorithm of Kulis & Jordan (2012)  which was derived under a
small-variance asymptotics regime. For the HDP this corresponds to α → 0 — the regime in part
(b) of Theorem 1. This is an unrealistic assumption in practice. Our geometric correction arguably
enables the accounting of the non-vanishing variance in data. We perform a simulation experiment
for varying values of α and show that nGDM outperforms the KL version of DP-means (Jiang et al. 
2012) in terms of perplexity. This result is reported in the Supplement.

4 Performance evaluation

Simulation experiments We use the LDA model to simulate data and focus our attention on the
perplexity of held-out data and minimum-matching Euclidean distance between the true and estimated
topics (Tang et al.  2014). We explore settings with varying document lengths (Nm increasing from
10 to 1400 - Fig. 2(a) and Fig. 3(a))  different number of documents (M increasing from 100 to 7000
- Fig. 2(b) and Fig. 3(b)) and when lengths of documents are small  while number of documents
is large (Nm = 50  M ranging from 1000 to 15000 - Fig. 2(c) and Fig. 3(c)). This last setting is
of particular interest  since it is the most challenging for our algorithm  which in theory works well
given long documents  but this is not always the case in practice. We compare two versions of the
Geometric Dirichlet Means algorithm: with tuned extension parameters (tGDM) and the default one
(GDM) (cf. Eq. 5) against the variational EM (VEM) algorithm (Blei et al.  2003) (with tuned
hyperparameters)  collapsed Gibbs sampling (Grifﬁths & Steyvers  2004) (with true data generating
hyperparameters)  and RecoverKL (Arora et al.  2012) and verify the theoretical upper bounds for
topic polytope estimation (i.e. either (log M/M )0.5 or (log Nm/Nm)0.5) - cf. Tang et al. (2014)
and Nguyen (2015). We are also interested in estimating each document’s topic proportion via
the projection technique. RecoverKL produced only a topic matrix  which is combined with our
projection based estimates to compute the perplexity (Fig. 3). Unless otherwise speciﬁed  we set
η = 0.1  α = 0.1  V = 1200  M = 1000  K = 5; Nm = 1000 for each m; the number of held-out
documents is 100; results are averaged over 5 repetitions. Since ﬁnding exact solution to the k-means
objective is NP hard  we use the algorithm of Hartigan & Wong (1979) with 10 restarts and the
k-means++ initialization. Our results show that (i) Gibbs sampling and tGDM have the best and
almost identical performance in terms of statistical estimation; (ii) RecoverKL and GDM are the
fastest while sharing comparable statistical accuracy; (iii) VEM is the worst in most scenarios due
to its instability (i.e. often producing poor topic estimates); (iv) short document lengths (Fig. 2(c)
and Fig. 3(c)) do not degrade performance of GDM  (this appears to be an effect of the law of large

6

numbers  as the algorithm relies on the cluster means  which are obtained by averaging over a large
number of documents); (v) our procedure for estimating document topic proportions results in a
good quality perplexity of the RecoverKL algorithm in all scenarios (Fig. 3) and could be potentially
utilized by other algorithms. Additional simulation experiments are presented in the Supplement 
which considers settings with varying Nm  α and the nonparametric extension.

Figure 2: Minimum-matching Euclidean distance: increasing Nm  M = 1000 (a); increasing M 
Nm = 1000 (b); increasing M  Nm = 50 (c); increasing η  Nm = 50  M = 5000 (d).

Figure 3: Perplexity of the held-out data: increasing Nm  M = 1000 (a); increasing M  Nm = 1000
(b); increasing M  Nm = 50 (c); increasing η  Nm = 50  M = 5000 (d).

Comparison to RecoverKL Both tGDM and RecoverKL exploit the geometry of the model  but
they rely on very different assumptions: RecoverKL requires the presence of anchor words in the
topics and exploits this in a crucial way (Arora et al.  2012); our method relies on long documents in
theory  even though the violation of this does not appear to degrade its performance in practice  as we
have shown earlier. The comparisons are performed by varying the document length Nm  and varying
the Dirichlet parameter η (recall that βk|η ∼ DirV (η)). In terms of perplexity  RecoverKL  GDM
and tGDM perform similarly (see Fig.4(c d))  with a slight edge to tGDM. Pronounced differences
come in the quality of topic’s word distribution estimates. To give RecoverKL the advantage  we
considered manually inserting anchor words for each topic generated  while keeping the document
length short  Nm = 50 (Fig. 4(a c)). We found that tGDM outperforms RecoverKL when η ≤ 0.3 
an arguably more common setting  while RecoverKL is more accurate when η ≥ 0.5. However  if the
presence of anchor words is not explicitly enforced  tGDM always outperforms RecoverKL in terms
of topic distribution estimation accuracy for all η (Fig. 2(d)). The superiority of tGDM persists even
as Nm varies from 50 to 10000 (Fig. 4(b))  while GDM is comparable to RecoverKL in this setting.

NIPS corpora analysis We proceed with the analysis of the NIPS corpus.1 After preprocessing 
there are 1738 documents and 4188 unique words. Length of documents ranges from 39 to 1403 with
mean of 272. We consider K = 5  10  15  20  α = 5
K   η = 0.1. For each value of K we set aside
300 documents chosen at random to compute the perplexity and average results over 3 repetitions.
Our results are compared against Gibbs sampling  Variational EM and RecoverKL (Table 1). For
K = 10  GDM with 1500 k-means iterations and 5 restarts in R took 50sec; Gibbs sampling with
5000 iterations took 10.5min; VEM with 750 variational  1500 EM iterations and 3 restarts took
25.2min; RecoverKL coded in Python took 1.1min. We note that with recent developments (e.g. 

1https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

7

llllllllllllllll0.0000.0250.0500.07505001000Document length NmMM distancellGDMtGDMGibbs samplingVEM0.1(log(Nm)Nm)0.5RecoverKLllllllllllllllll0.0000.0050.0100.0150.0200200040006000Number of documents M with Nm=1000llGDMtGDMGibbs samplingVEM0.1(log(M)M)0.5RecoverKLllllllllllllllllllllllllllllll0.0000.0250.0500.0754000800012000Number of documents M with Nm=50llGDMtGDMGibbs samplingVEM0.1(log(M)M)0.5RecoverKL0.010.020.030.00.30.60.9hGDMtGDMRecoverKLllllllll25027530032535037505001000Document length NmPerplexitylGDMtGDMGibbs samplingVEMRecoverKLllllllll2602702802903000200040006000Number of documents M with Nm=1000lGDMtGDMGibbs samplingVEMRecoverKLlllllllllllllll3004005006004000800012000Number of documents M with Nm=50lGDMtGDMGibbs samplingVEMRecoverKL02505007500.00.30.60.9hGDMtGDMRecoverKLFigure 4: MM distance and Perplexity for varying η  Nm = 50 with anchors (a c); varying Nm (b d).

(Hoffman et al.  2013)) VEM could be made faster  but its statistical accuracy remains poor. Although
RecoverKL is as fast as GDM  its perplexity performance is poor and is getting worse with more
topics  which we believe could be due to lack of anchor words in the data. We present topics found
by Gibbs sampling  GDM and RecoverKL for K = 10 in the Supplement.

Table 1: Perplexities of the 4 topic modeling algorithms trained on the NIPS dataset.

GDM RecoverKL VEM Gibbs sampling
1269
1061
957
763

1980
1953
1545
1352

1378
1235
1409
1586

1168
924
802
704

K = 5
K = 10
K = 15
K = 20

5 Discussion

We wish to highlight a conceptual aspect of GDM distinguishing it from moment-based methods
such as RecoverKL. GDM operates on the document-to-document distance/similarity matrix  as
opposed to the second-order word-to-word matrix. So  from an optimization viewpoint  our method
can be viewed as the dual to RecoverKL method  which must require anchor-word assumption to
be computationally feasible and theoretically justiﬁable. While the computational complexity of
RecoverKL grows with the vocabulary size and not the corpora size  our convex geometric approach
continues to be computationally feasible when number of documents is large: since only documents
near the polytope boundary are relevant in the inference of the extreme points  we can discard most
documents residing near the polytope’s center.
We discuss some potential improvements and extensions next. The tGDM algorithm showed a superior
performance when the extension parameters are optimized. This procedure  while computationally
effective relative to methods such as Gibbs sampler  may still be not scalable to massive datasets. It
seems possible to reformulate the geometric objective as a function of extension parameters  whose
optimization can be performed more efﬁciently. In terms of theory  we would like to establish the
error bounds by exploiting the connection of topic inference to the geometric problem of Centroidal
Voronoi Tessellation of a convex polytope.
The geometric approach to topic modeling and inference may lend itself naturally to other LDA
extensions  as we have demonstrated with nGDM algorithm for the HDP (Teh et al.  2006). Correlated
topic models of Blei & Lafferty (2006a) also ﬁt naturally into the geometric framework — we would
need to adjust geometric modiﬁcation to capture logistic normal distribution of topic proportions
inside the topic polytope. Another interesting direction is to consider dynamic (Blei & Lafferty 
2006b) (extreme points of topic polytope evolving over time) and supervised (McAuliffe & Blei 
2008) settings. Such settings appear relatively more challenging  but they are worth pursuing further.

Acknowledgments

This research is supported in part by grants NSF CAREER DMS-1351362 and NSF CNS-1409303.

8

0.010.020.030.00.30.60.9hMM distanceGDMtGDMRecoverKL0.0050.010025005000750010000Document length NmGDMtGDMRecoverKL02505007500.00.30.60.9hPerplexityGDMtGDMRecoverKL260265270275280025005000750010000Document length NmGDMtGDMRecoverKLReferences
Anandkumar  A.  Foster  D. P.  Hsu  D.  Kakade  S. M.  and Liu  Y. A spectral algorithm for Latent Dirichlet

Allocation. Advances in Neural Information Processing Systems  2012.

Arora  S.  Ge  R.  Halpern  Y.  Mimno  D.  Moitra  A.  Sontag  D.  Wu  Y.  and Zhu  M. A practical algorithm for

topic modeling with provable guarantees. arXiv preprint arXiv:1212.4777  2012.

Blei  D. M. and Lafferty  J. D. Correlated topic models. Advances in Neural Information Processing Systems 

2006a.

Blei  D. M. and Lafferty  J. D. Dynamic topic models. In Proceedings of the 23rd international conference on

Machine learning  pp. 113–120. ACM  2006b.

Blei  D. M.  Ng  A. Y.  and Jordan  M. I. Latent Dirichlet Allocation. J. Mach. Learn. Res.  3:993–1022  March

2003.

Deerwester  S.  Dumais  S. T.  Furnas  G. W.  Landauer  T. K.  and Harshman  R. Indexing by latent semantic

analysis. Journal of the American Society for Information Science  41(6):391  Sep 01 1990.

Ding  C.  Li  T.  and Peng  W. Nonnegative matrix factorization and probabilistic latent semantic indexing:
Equivalence chi-square statistic  and a hybrid method. In Proceedings of the National Conference on Artiﬁcial
Intelligence  volume 21  pp. 342. AAAI Press; MIT Press  2006.

Du  Q.  Faber  V.  and Gunzburger  M. Centroidal Voronoi Tessellations: applications and algorithms. SIAM

Review  41(4):637–676  1999.

Golubitsky  O.  Mazalov  V.  and Watt  S. M. An algorithm to compute the distance from a point to a simplex.

ACM Commun. Comput. Algebra  46:57–57  2012.

Grifﬁths  T. L. and Steyvers  M. Finding scientiﬁc topics. PNAS  101(suppl. 1):5228–5235  2004.

Hartigan  J. A. and Wong  M. A. Algorithm as 136: A K-means clustering algorithm. Journal of the Royal

Statistical Society. Series C (Applied Statistics)  28(1):100–108  1979.

Hoffman  M. D.  Blei  D. M.  Wang  C.  and Paisley  J. Stochastic variational inference. J. Mach. Learn. Res.  14

(1):1303–1347  May 2013.

Hofmann  T. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval  SIGIR ’99  pp. 50–57. ACM 
1999.

Jiang  K.  Kulis  B.  and Jordan  M. I. Small-variance asymptotics for exponential family Dirichlet process

mixture models. In Advances in Neural Information Processing Systems  pp. 3158–3166  2012.

Kulis  B. and Jordan  M. I. Revisiting k-means: new algorithms via Bayesian nonparametrics. In Proceedings of

the 29th International Conference on Machine Learning. ACM  2012.

McAuliffe  J. D. and Blei  D. M. Supervised topic models. In Advances in Neural Information Processing

Systems  pp. 121–128  2008.

Nguyen  X. Posterior contraction of the population polytope in ﬁnite admixture models. Bernoulli  21(1):

618–646  02 2015.

Pritchard  J. K.  Stephens  M.  and Donnelly  P. Inference of population structure using multilocus genotype data.

Genetics  155(2):945–959  2000.

Tang  J.  Meng  Z.  Nguyen  X.  Mei  Q.  and Zhang  M. Understanding the limiting factors of topic modeling
via posterior contraction analysis. In Proceedings of the 31st International Conference on Machine Learning 
pp. 190–198. ACM  2014.

Teh  Y. W.  Jordan  M. I.  Beal  M. J.  and Blei  D. M. Hierarchical Dirichlet processes. Journal of the American

Statistical Association  101(476)  2006.

Xu  W.  Liu  X.  and Gong  Y. Document clustering based on non-negative matrix factorization. In Proceedings
of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion
Retrieval  SIGIR ’03  pp. 267–273. ACM  2003.

9

,Mikhail Yurochkin
XuanLong Nguyen
Aviv Rosenberg
Yishay Mansour