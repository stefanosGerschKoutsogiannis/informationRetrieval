2011,Composite Multiclass Losses,We consider loss functions for multiclass prediction problems. We   show when a  multiclass loss can be expressed as a ``proper   composite loss''  which is the composition of a proper loss and a link   function. We extend existing results for binary losses to   multiclass losses.  We determine the stationarity condition    Bregman representation  order-sensitivity  existence and uniqueness   of the composite representation for multiclass losses.  We also   show that the integral representation  for binary proper losses can   not be extended to  multiclass losses. We subsume existing results   on ``classification calibration'' by relating it to properness.  We   draw conclusions concerning the design of multiclass losses.,Composite Multiclass Losses

Elodie Vernet
ENS Cachan

Robert C. Williamson

ANU and NICTA

Mark D. Reid
ANU and NICTA

evernet@ens-cachan.fr

Bob.Williamson@anu.edu.au

Mark.Reid@anu.edu.au

Abstract

We consider loss functions for multiclass prediction problems. We show when
a multiclass loss can be expressed as a “proper composite loss”  which is the
composition of a proper loss and a link function. We extend existing results for
binary losses to multiclass losses. We determine the stationarity condition  Breg-
man representation  order-sensitivity  existence and uniqueness of the composite
representation for multiclass losses. We subsume existing results on “classiﬁca-
tion calibration” by relating it to properness and show that the simple integral
representation for binary proper losses can not be extended to multiclass losses.

1

Introduction

+. The partial losses (cid:96)i are the components of (cid:96)(q) = ((cid:96)1(q)  . . .   (cid:96)n(q))(cid:48).

The motivation of this paper is to understand the intrinsic structure and properties of suitable loss
functions for the problem of multiclass prediction  which includes multiclass probability estimation.
Suppose we are given a data sample S := (xi yi)i∈[m] where xi ∈ X is an observation and yi ∈
{1  .. n} =: [n] is its corresponding class. We assume the sample S is drawn iid according to some
distribution P = PX  Y on X × [n]. Given a new observation x we want to predict the probability
pi := P(Y = i|X = x) of x belonging to class i  for i ∈ [n]. Multiclass classiﬁcation requires the
learner to predict the most likely class of x; that is to ﬁnd ˆy = argmaxi∈[n] pi.
A loss measures the quality of prediction. Let ∆n :={(p1  . . .   pn): ∑i∈[n] pi = 1 and 0≤ pi ≤ 1  ∀i∈
[n]} denote the n-simplex. For multiclass probability estimation  (cid:96): ∆n → Rn
+. For classiﬁcation  the
loss (cid:96): [n] → Rn
Proper losses are particularly suitable for probability estimation. They have been studied in detail
when n = 2 (the “binary case”) where there is a nice integral representation [1  2  3]  and charac-
terization [4] when differentiable. Classiﬁcation calibrated losses are an analog of proper losses for
the problem of classiﬁcation [5]. The relationship between classiﬁcation calibration and properness
was determined in [4] for n = 2. Most of these results have had no multiclass analogue until now.
The design of losses for multiclass prediction has received recent attention [6  7  8  9  10  11  12]
although none of these papers developed the connection to proper losses  and most restrict consid-
eration to margin losses (which imply certain symmetry conditions). Glasmachers [13] has shown
that certain learning algorithms can still behave well when the losses do not satisfy the conditions in
these earlier papers because the requirements are actually stronger than needed.
Our contributions are: We relate properness  classiﬁcation calibration  and the notion used in [8]
which we rename “prediction calibrated” §3; we provide a novel characterization of multiclass
properness §4; we study composite proper losses (the composition of a proper loss with an invertible
link) presenting new uniqueness and existence results §5; we show how the above results can aid in
the design of proper losses §6; and we present a (somewhat surprising) negative result concerning
the integral representation of proper multiclass losses §7. Many of our results are characterisations.
Full proofs are provided in the extended version [14].

1

2 Formal Setup
Suppose X is some set and Y = {1  . . .  n} = [n] is a set of labels. We suppose we are given
data (xi yi)i∈[m] such that Yi ∈ Y is the label corresponding to xi ∈ X . These data follow a joint
distribution PX  Y . We denote by EX  Y and EY |X respectively  the expectation and the conditional
expectation with respect to PX  Y .
The conditional risk L associated with a loss (cid:96) is the function

L: ∆n × ∆n (cid:51) (p q) (cid:55)→ L(p q) = EY∼p(cid:96)Y(q) = p(cid:48) · (cid:96)(q) = ∑
i∈[n]

pi(cid:96)i(q) ∈ R+ 

where Y ∼ p means Y is drawn according to a multinomial distribution with parameter p. In a typical
learning problem one will make an estimate q: X → ∆n. The full risk is L(q) = EX EY |X (cid:96)Y(q(X)).
Minimizing L(q) over q: X → ∆n is equivalent to minimizing L(p(x) q(x)) over q(x) ∈ ∆n for all
x ∈ X where p(x) = (p1(x)  . . .   pn(x))(cid:48)  p(cid:48) is the transpose of p  and pi(x) = P(Y = i|X = x). Thus
it sufﬁces to only consider the conditional risk; confer [3].
A loss (cid:96): ∆n → Rn
+ is proper if L(p  p) ≤ L(p q)  ∀p q ∈ ∆n. It is strictly proper if the inequality is
strict when p (cid:54)= q. The conditional Bayes risk L: ∆n (cid:51) p (cid:55)→ infq∈∆n L(p q). This function is always
concave [2]. If (cid:96) is proper  then L(p) = L(p  p) = p(cid:48) · (cid:96)(p). Strictly proper losses induce Fisher
consistent estimators of probabilities: if (cid:96) is strictly proper  p = argminq L(p q).
In order to differentiate the losses we project the n-simplex into a subset of Rn−1. We de-
note by Π∆ : ∆n (cid:51) p = (p1  . . .   pn)(cid:48) (cid:55)→ ˜p = (p1  . . .   pn−1)(cid:48) ∈ ˜∆n := {(p1  . . .   pn−1)(cid:48) : pi ≥ 0  ∀i ∈
[n]  ∑n−1
∆ : ˜∆n (cid:51) ˜p = ( ˜p1  . . .   ˜pn−1) (cid:55)→ p =
( ˜p1  . . .   ˜pn−1 1− ∑n−1
The losses above are deﬁned on the simplex ∆n since the argument (an estimator) represents
a probability vector. However it is sometimes desirable to use another set V of predictions.
One can consider losses (cid:96): V → Rn
+. Suppose there exists an invertible function ψ : ∆n → V .
Then (cid:96) can be written as a composition of a loss λ deﬁned on the simplex with ψ−1. That is 
(cid:96)(v) = λ ψ (v) := λ (ψ−1(v)). Such a function λ ψ is a composite loss. If λ is proper  we say (cid:96) is a
proper composite loss  with associated proper loss λ and link ψ.
We use the following notation. The kth unit vector ek is the n vector with all components zero except
the kth which is 1. The n-vector 1n := (1  . . .  1)(cid:48). The derivative of a function f is denoted D f and
its Hessian H f . Let ˚∆n := {(p1  . . .   pn): ∑i∈[n] pi = 1 and 0 < pi < 1  ∀i ∈ [n]} and ∂∆n := ∆n\ ˚∆n.

i=1 pi ≤ 1}  the projection of the n-simplex ∆n  and Π−1

i=1 ˜pi)(cid:48) ∈ ∆n its inverse.

3 Relating Properness to Classiﬁcation Calibration

Properness is an attractive property of a loss for the task of class probability estimation. However if
one is merely interested in classifying (predicting ˆy ∈ [n] given x ∈ X ) then one requires less. We
relate classiﬁcation calibration (the analog of properness for classiﬁcation problems) to properness.
Suppose c ∈ ˚∆n. We cover ∆n with n subsets each representing one class:

Ti(c) := {p ∈ ∆n : ∀ j (cid:54)= i pic j ≥ p jci}.

Observe that for i (cid:54)= j  the sets {p ∈ R: pic j = p jc j} are subsets of dimension n− 2 through c and
all ek such that k (cid:54)= i and k (cid:54)= j. These subsets partition ∆n into two parts  the subspace Ti is the
intersection of the subspaces delimited by the precedent (n− 2)-subspace and in the same side as ei.
We will make use of the following properties of Ti(c).
Lemma 1 Suppose c ∈ ˚∆n  i ∈ [n]. Then the following hold:

1. For all p ∈ ∆n  there exists i such that p ∈ Ti(c).
2. Suppose p ∈ ∆n. Ti(c)∩ T j(c) ⊆ {p ∈ ∆n : pic j = p jci}  a subspace of dimension n− 2.

3. Suppose p ∈ ∆n. If p ∈(cid:84)n

i=1 Ti(c) then p = c.

4. For all p q ∈ ∆n  p (cid:54)= q  there exists c ∈ ˚∆n  and i ∈ [n] such that p ∈ Ti(c) and q /∈ Ti(c).

2

Classiﬁcation calibrated losses have been developed and studied under some different deﬁnitions
and names [6  5]. Below we generalise the notion of c-calibration which was proposed for n = 2 in
[4] as a generalisation of the notion of classiﬁcation calibration in [5].
Deﬁnition 2 Suppose (cid:96): ∆n → Rn
+ is a loss and c ∈ ˚∆n. We say (cid:96) is c-calibrated at p ∈ ∆n if for all
i ∈ [n] such that p /∈ Ti(c) then ∀q ∈ Ti(c)  L(p) < L(p q). We say that (cid:96) is c-calibrated if ∀p ∈ ∆n 
(cid:96) is c-calibrated at p.

n   . . .   1

Deﬁnition 2 means that if the probability vector q one predicts doesn’t belong to the same subset
(i.e. doesn’t predict the same class) as the real probability vector p  then the loss might be larger.
Classiﬁcation calibration in the sense used in [5] corresponds to 1
2-calibrated losses when n = 2. If
n )(cid:48)  cmid-calibration induces Fisher-consistent estimates in the case of classiﬁcation.
cmid := ( 1
Furthermore “(cid:96) is cmid-calibrated and for all i ∈ [n]  and (cid:96)i is continuous and bounded below” is
equivalent to “(cid:96) is inﬁnite sample consistent as deﬁned by [6]”. This is because if (cid:96) is continuous
and Ti(c) is closed  then ∀q ∈ Ti(c)  L(p) < L(p q) if and only if L(p) < infq∈Ti(c) L(p q).
The following result generalises the correspondence between binary classiﬁcation calibration and
properness [4  Theorem 16] to multiclass losses (n > 2).
Proposition 3 A continuous loss (cid:96): ∆n → Rn
all c ∈ ˚∆n.
In particular  a continuous strictly proper loss is cmid-calibrated. Thus for any estimator ˆqn of the
conditional probability vector one constructs by minimizing the empirical average of a continuous
strictly proper loss  one can build an estimator of the label (corresponding to the largest probability
of ˆqn) which is Fisher consistent for the problem of classiﬁcation.
In the binary case  (cid:96) is classiﬁcation calibrated if and only if the following implication holds [5]:

+ is strictly proper if and only if it is c-calibrated for

(cid:18)

(cid:19)

(cid:18)

(cid:19)

L( fn) → min

L(g)

⇒

PX  Y (Y (cid:54)= fn(X)) → min

PX  Y (Y (cid:54)= g(X))

.

(1)

g

g

Tewari and Bartlett [8] have characterised when (1) holds in the multiclass case. Since there is no
reason to assume the equivalence between classiﬁcation calibration and (1) still holds for n > 2  we
give different names for these two notions. We keep the name of classiﬁcation calibration for the
notion linked to Fisher consistency (as deﬁned before) and call prediction calibrated the notion of
Tewari and Bartlett (equivalent to (1)).
Deﬁnition 4 Suppose (cid:96): V → Rn
+ is a loss. Let C(cid:96) = co({(cid:96)(v): v ∈ V })  the convex hull of the
image of V . (cid:96) is said to be prediction calibrated if there exists a prediction function pred: Rn → [n]
such that

∀p ∈ ∆n :

inf

z∈C(cid:96) ppred(z)<maxi pi

p(cid:48) · z > inf
z∈C(cid:96)

p(cid:48) · z = L(p).

Observe that the class is predicted from (cid:96)(p) and not directly from p (which is equivalent if the
+ is such that (cid:96) is prediction calibrated and pred((cid:96)(p)) ∈
loss is invertible). Suppose that (cid:96): ∆n → Rn
argmaxi pi. Then (cid:96) is cmid-calibrated almost everywhere.
By introducing a reference “link” ¯ψ (which corresponds to the actual link if (cid:96) is a proper composite
loss) we now show how the pred function can be canonically expressed in terms of argmaxi pi.
Proposition 5 Suppose (cid:96): V → Rn
λ is proper. If (cid:96) is prediction calibrated then pred(λ (p)) ∈ argmaxi pi.

+ is a loss. Let ¯ψ(p) ∈ argminv∈V L(p v) and λ = (cid:96)◦ ¯ψ. Then

4 Characterizing Properness
We ﬁrst present some simple (but new) consequences of properness. We say f : C ⊂ Rn → Rn is
monotone on C when for all x and y in C  ( f (x)− f (y))(cid:48) · (x− y) ≥ 0; confer [15].
Proposition 6 Suppose (cid:96): ∆n → Rn
+ is a loss. If (cid:96) is proper  then −(cid:96) is monotone.

3

Proposition 7 If (cid:96) is strictly proper then it is invertible.

A theme of the present paper is the extensibility of results concerning binary losses to multiclass
losses. The following proposition shows how the characterisation of properness in the general (not
necessarily differentiable) multiclass case can be reduced to the binary case. In the binary case 
the two classes are often denoted −1 and 1 and the loss is denoted (cid:96) = ((cid:96)1  (cid:96)−1)(cid:48). We project the
2-simplex ∆2 into [0 1]: η ∈ [0 1] is the projection of (η 1− η) ∈ ∆2.
Proposition 8 Suppose (cid:96): ∆n → Rn
˜(cid:96)p q : [0 1] (cid:51) η (cid:55)→

p(cid:48) · (cid:96)(cid:0)p + η(q− p)(cid:1) (cid:19)
(cid:18) q(cid:48) · (cid:96)(cid:0)p + η(q− p)(cid:1)

(cid:18) ˜(cid:96)p q

+ is a loss. Deﬁne

(cid:19)

=

.

1 (η)
˜(cid:96)p q−1 (η)

−(cid:96)(cid:48)
1(η)
1−η =

+ is proper if and only if ∀η ∈ [0 1] 

+ is a loss. Then (cid:96) is (strictly) proper if and only if ∀p q ∈ ∆n 

Then (cid:96) is (strictly) proper if and only if ˜(cid:96)p q is (strictly) proper ∀p q ∈ ∂∆n.
This proposition shows that in order to check if a loss is proper one needs only to check the proper-
ness in each line. One could use the easy characterization of properness for differentiable binary
(cid:96)(cid:48)−1(η)
losses ((cid:96): [0 1] → R2
η ≥ 0  [4]). However this
needs to be checked for all lines deﬁned by p q ∈ ∂∆n. We now extend some characterisations of
properness to the multiclass case by using Proposition 8.
Lambert [16] proved that in the binary case  properness is equivalent to the fact that the further your
prediction is from reality  the larger the loss (“order sensitivity”). The result relied upon on the total
order of R. In the multiclass case  there does not exist such a total order. Yet  one can compare
two predictions if they are in the same line as the true real class probability. The next result is a
generalization of the binary case equivalence of properness and order sensitivity.
Proposition 9 Suppose (cid:96): ∆n → Rn
∀0 ≤ h1 ≤ h2  L(p  p + h1(q− p)) ≤ L(p  p + h2(q− p)) (the inequality is strict if h1 (cid:54)= h2).
“Order sensitivity” tells us more about properness: the true class probability minimizes the risk
and if the prediction moves away from the true class probability in a line then the risk increases.
This property appears convenient for optimization purposes: if one reaches a local minimum in the
second argument of the risk and the loss is strictly proper then it is a global minimum. If the loss is
proper  such a local minimum is a global minimum or a constant in an open set. But observe that
typically one is minimising the full risk L(q(·)) over functions q: X → ∆n. Order sensitivity of (cid:96)
does not imply this optimisation problem is well behaved; one needs convexity of q (cid:55)→ L(p q) for
all p ∈ ∆n to ensure convexity of the functional optimisation problem.
The order sensitivity along a line leads to a new characterisation of differentiable proper losses. As
in the binary case  one condition comes from the fact that the derivative is zero at a minimum and
the other ensures that it is really a minimum.
Corollary 10 Suppose (cid:96): ∆n → Rn
D ˜(cid:96)(Π∆(p))· DΠ∆(p). Then (cid:96) is proper if and only if
p(cid:48) · M(p) = 0
(q− r)(cid:48) · M(p)· (q− r) ≤ 0

+ is a loss such that ˜(cid:96) = (cid:96)◦ Π−1

∆ is differentiable. Let M(p) =

∀q r ∈ ∆n  ∀p ∈ ˚∆n.

(cid:27)

(2)

We know that for any loss  its Bayes risk L(p) = infq∈∆n L(p q) = infq∈∆n p(cid:48) · (cid:96)(q) is concave. If (cid:96) is
proper  L(p) = p(cid:48) · (cid:96)(p). Rather than working with the loss (cid:96): V → Rn
+ we will now work with the
simpler associated conditional Bayes risk L: V → R+.
We need two deﬁnitions from [15]. Suppose f : Rn → R is concave. Then limt↓0
exists 
and is called the directional derivative of f at x in the direction d and is denoted D f (x d). By
analogy with the usual deﬁnition of subdifferential  the superdifferential ∂ f (x) of f at x is

∂ f (x) :=(cid:8)s ∈ Rn : s(cid:48) · y ≥ D f (x y)  ∀y ∈ Rn(cid:9) =(cid:8)s ∈ Rn : f (y) ≤ f (x) + s(cid:48) · (y− x)  ∀y ∈ Rn(cid:9) .

A vector s ∈ ∂ f (x) is called a supergradient of f at x.
The next proposition is a restatement of the well known Bregman representation of proper losses;
see [17] for the differentiable case  and [2  Theorem 3.2] for the general case.

f (x+td)− f (x)

t

4

Proposition 11 Suppose (cid:96): ∆n → Rn
function f and ∀q ∈ ∆n  there exists a supergradient A(q) ∈ ∂ f (q) such that
∀p q ∈ ∆n  p(cid:48) · (cid:96)(q) = L(p q) = f (q) + (p− q)(cid:48) · A(q).

+ is a loss. Then (cid:96) is proper if and only if there exists a concave

Then f is unique and f (p) = L(p  p) = L(p).

∆ is differentiable at ˜q ∈ ˜∆n  A(q) = (D ˜f (Π∆(q)) 0)(cid:48) +α 1(cid:48)

The fact that f is deﬁned on a simplex is not a problem. Indeed  the superdifferential becomes
∂ f (x) = {s ∈ Rn : s(cid:48) · d ≥ D f (x d) ∀d ∈ ∆n} = {s ∈ Rn : f (y) ≤ f (x) + s(cid:48) · (y− x)  ∀y ∈ ∆n}.
If
˜f = f ◦Π−1
n  α ∈ R. Then (p−q)(cid:48)·A(q) =
D ˜f (Π∆(q)) · (Π∆(p) − Π∆(q)). Hence for any concave differentiable function f   there exists an
unique proper loss whose Bayes risk is equal to f (we say that f is differentiable when ˜f is differ-
entiable).
The last property gives us the form of the proper losses associated with a Bayes risk. Suppose
L: ∆n → R+ is concave. The proper losses whose Bayes risk is equal to L are

L(q) + (ei − q)(cid:48) · A(q)

∈ Rn

+  ∀A(q) ∈ ∂ L(q).

(3)

(cid:96): ∆n (cid:51) q (cid:55)→(cid:16)

(cid:17)n

i=1

This result suggests that some information is lost by representing a proper loss via its Bayes risk
(when the last is not differentiable). The next proposition elucidates this by showing that proper
losses which have the same Bayes risk are equal almost everywhere.

Proposition 12 Two proper losses (cid:96)1 and (cid:96)2 have the same conditional Bayes risk function L if and
only if (cid:96)1 = (cid:96)2 almost everywhere. If L is differentiable  (cid:96)1 = (cid:96)2 everywhere.
We say that L is differentiable at p if ˜L = L◦ Π−1
Proposition 13 Suppose (cid:96): ∆n → Rn
differentiable on ˚∆n; (cid:96) is continuous at p ∈ ˚∆n if and only if  L is differentiable at p ∈ ˚∆n.

+ is a proper loss. Then (cid:96) is continuous in ˚∆n if and only if L is

∆ is differentiable at ˜p = Π∆(p).

5 The Proper Composite Representation: Uniqueness and Existence

It is sometimes helpful to deﬁne a loss on some set V rather than ∆n; confer [4]. Composite losses
(see the deﬁnition in §2) are a way of constructing such losses: given a proper loss λ : ∆n → Rn
+ and
an invertible link ψ : ∆n → V   one deﬁnes λ ψ : V → Rn
+ using λ ψ = λ ◦ψ−1. We now consider the
question: given a loss (cid:96): V → Rn
+  when does (cid:96) have a proper composite representation (whereby (cid:96)
can be written as (cid:96) = λ ◦ ψ−1)  and is this representation unique? We ﬁrst consider the binary case
and study the uniqueness of the representation of a loss as a proper composite loss.
Proposition 14 Suppose (cid:96) = λ ◦ψ−1 : V → R2
+ is a proper composite loss and that the proper loss
λ is differentiable and the link function ψ is differentiable and invertible. Then the proper loss λ
is unique. Furthermore ψ is unique if ∀v1 v2 ∈ R  ∃v ∈ [v1 v2]  (cid:96)(cid:48)
−1(v) (cid:54)= 0. If there
−1(v) = 0 ∀v ∈ [ ¯v1  ¯v2]  one can choose any ψ|[ ¯v1  ¯v2] such that
exists ¯v1  ¯v2 ∈ R such that (cid:96)(cid:48)
ψ is differentiable  invertible and continuous in [ ¯v1  ¯v2] and obtain (cid:96) = λ ◦ ψ−1  and ψ is uniquely
deﬁned where (cid:96) is invertible.
−1(v) (cid:54)= 0
Proposition 15 Suppose (cid:96): V → R2
1(v) (cid:54)= 0. Then (cid:96) can be expressed as a proper composite loss if and only if the following
or (cid:96)(cid:48)
three conditions hold: 1) (cid:96)1 is decreasing (increasing); 2) (cid:96)−1 is increasing (decreasing); and 3)
f : V (cid:51) v (cid:55)→ (cid:96)(cid:48)

+ is a differentiable binary loss such that ∀v ∈ V   (cid:96)(cid:48)

1(v)
(cid:96)(cid:48)−1(v) is strictly increasing (decreasing) and continuous.

1(v) (cid:54)= 0 or (cid:96)(cid:48)

1(v) = (cid:96)(cid:48)

Observe that the last condition is alway satisﬁed if both (cid:96)1 and (cid:96)−1 are convex.
(cid:48)
Suppose ϕ : R → R+ is a function. The loss deﬁned via (cid:96)ϕ : V (cid:51) v (cid:55)→ ((cid:96)−1(v)  (cid:96)1(v))
(ϕ(−v) ϕ(v))
ﬁcation problems. We will now show how the above proposition applies to them.

=
+ is called a binary margin loss. Binary margin losses are often used for classi-

(cid:48) ∈ R2

5

Corollary 16 Suppose ϕ : R → R+ is differentiable and ∀v ∈ R  ϕ(cid:48)(v) (cid:54)= 0 or ϕ(cid:48)(−v) (cid:54)= 0. Then (cid:96)ϕ
can be expressed as a proper composite loss if and only if f : R (cid:51) v (cid:55)→ − ϕ(cid:48)(v)
ϕ(cid:48)(−v) is strictly monotonic
continuous and ϕ is monotonic.

2x2+4

π arctan(x− 1). Then f (v) = ϕ(cid:48)(−v)

If ϕ is convex or concave then f deﬁned above is monotonic. However not all binary margin losses
are composite proper losses. One can even build a smooth margin loss which cannot be expressed as
a proper composite loss. Consider ϕ(x) = 1− 1
ϕ(cid:48)(−v)+ϕ(cid:48)(v) = x2−2x+2
which is not invertible.
We now generalize the above results to the multiclass case.
Proposition 17 Suppose (cid:96) has two proper composite representations (cid:96) = λ ◦ ψ−1 = µ ◦ φ−1 where
λ and µ are proper losses and ψ and φ are continuous invertible. Then λ = m almost everywhere.
If (cid:96) is continuous and has a composite representation  then the proper loss (in the decomposition) is
unique (λ = µ everywhere).
If (cid:96) is invertible and has a composite representation  then the representation is unique.
Given a loss (cid:96): V → Rn
+  we denote by S(cid:96) = (cid:96)(V ) +
[0 ∞)n = {λ : ∃v ∈ V   ∀i ∈ [n]  λi ≥ (cid:96)i(v)} the super-
prediction set of (cid:96) (confer e.g. [18]). We introduce a
set of hyperplanes for p ∈ ∆n and β ∈ R  hβ
p = {x ∈
Rn : x(cid:48) · p = β}. A hyperplane hβ
p supports a set A at
x ∈ A when x ∈ hβ
p and for all a ∈ A   a(cid:48) · p ≥ β or
for all a ∈ A   a(cid:48) · p ≤ β . We say that S(cid:96) is strictly
convex in its inner part when for all p ∈ ∆n  there ex-
ists an unique x ∈ (cid:96)(V ) such that there exists a hyper-
plane hβ
p supporting S(cid:96) at x. S(cid:96) is said to be smooth
when for all x ∈ (cid:96)(V )  there exists an unique hyper-
plane supporting S(cid:96) at x.
If (cid:96) is invertible  we can
express these two deﬁnitions in terms of v ∈ V rather
than x ∈ (cid:96)(V ). If (cid:96): V → Rn
+ is strictly convex  then
S(cid:96) will be strictly convex in its inner part.
Proposition 18 Suppose (cid:96): V → Rn
composite representation if and only if S(cid:96) is convex  smooth and strictly convex in its inner part.
Proposition 19 Suppose (cid:96): V → Rn
+ is a continuous loss. If (cid:96) has a proper composite represen-
tation  then S(cid:96) is convex and smooth. If (cid:96) is also invertible  then S(cid:96) is strictly convex in its inner
part.

+ is a continuous invertible loss. Then (cid:96) has a strictly proper

x = (cid:96)(v)

(cid:96)1(v)

)
v
(
2
(cid:96)

{
x
:
x·

q

(cid:96)(V )

S(cid:96)

L

(

v
)}

hL
(

q

v
)

=

q

=

6 Designing Proper Losses

Suppose we are given n(n−1)

We now build a family of conditional Bayes risks.
concave
functions {Li1 i2 : ∆2 → R}1≤i1<i2≤n on ∆2  and we want to build a concave function L on ∆n
which is equal to one of the given functions on each edge of the simplex (∀1 ≤ i1 < i2 ≤ n 
L(0  . 0  pi1 0  . 0  pi2 0  . 0) = Li1 i2(pi1  pi2 )). This is equivalent to choosing a binary loss function 
knowing that the observation is in the class i1 or i2. The result below gives one possible construction.
(There exists an inﬁnity of solutions — one can simply add any concave function equal to zero in
each edge).
(cid:19)
Lemma 20 Suppose we have a family of concave functions {Li1 i2 : ∆2 → R}1≤i1<i2≤n  then

2

(cid:18) pi1

is concave and ∀1 ≤ i1 < i2 ≤ n  L(0  . 0  pi1 0  . 0  pi2 0  . 0) = Li1 i2(pi1  pi2).

L: ∆n (cid:51) p (cid:55)→ L(p1  . . .   pn) = ∑

(pi1 + pi2 )Li1 i2

1≤i1<i2≤n

pi2

 

pi1 + pi2

pi1 + pi2

6

Using this family of Bayes risks  one can build a family of proper losses.
Lemma 21 Suppose we have a family of binary proper losses (cid:96)i1 i2 : ∆2 → R2. Then

(cid:32) j−1

(cid:96)i  j−1

∑

i=1

(cid:18) pi

pi + p j

(cid:96): ∆n (cid:51) p (cid:55)→ (cid:96)(p) =

is a proper n-class loss such that

(cid:96)i  j
1

i= j+1

n

+

∑

(cid:19)
 (cid:96)i1 i2

(pi1)
1
(cid:96)i1 i2−1 (pi1)
0

∈ Rn

+

(cid:18) p j

(cid:19)(cid:33)n

pi + p j

j=1

i = i1
i = i2
otherwise

.

(cid:96)i((0  . 0  pi1 0  . 0  pi2 0  . 0)) =

Observe that it is much easier to work at ﬁrst with the Bayes risk and then using the correspondence
between Bayes risks and proper losses.

7

Integral Representations of Proper Losses

(cid:96)c

Unlike the natural generalisation of the results from proper binary to proper multiclass losses above 
there is one result that does not carry over: the integral representation of proper losses [1]. In the
binary case there exists a family of “extremal” loss functions (cost-weighted generalisations of the

0-1 loss) each parametrised by c ∈ [0 1] and deﬁned for all η ∈ [0 1] by (cid:96)c−1(η) := c(cid:74)η ≥ c(cid:75) and
1 := (1− c)(cid:74)η < c(cid:75). As shown in [1  3]  given these extremal functions  any proper binary loss (cid:96)
can be expressed as the weighted integral (cid:96) =(cid:82) 1
0 (cid:96)c w(c)dc + constant with w(c) = −L(cid:48)(cid:48)(c). This
representation is a special case of a representation from Choquet theory [19] which characterises
when every point in some set can be expressed as a weighted combination of the “extremal points”
of the set. Although there is such a representation when n > 2  the difﬁculty is that the set of extremal
points is much larger and this rules out the existence of a nice small set of “primitive” proper losses
when n > 2. The rest of this section makes this statement precise.
A convex cone K is a set of points closed under linear combinations of positive coefﬁcients. That
2 (g + h) for g h ∈ K
is  K = αK + β K for any α β ≥ 0. A point f ∈ K is extremal if f = 1
implies ∃α ∈ R+ such that g = α f . That is  f cannot be represented as a non-trivial combination of
other points in K . The set of extremal points for K will be denoted ex K . Suppose U is a bounded
closed convex set in Rd  and Kb(U) is the set of convex functions on U bounded by 1  then Kb(U)
is compact with respect to the topology of uniform convergence. Theorem 2.2 of [20] shows that the
extremal points of the convex cone K (U) ={α f +β g : f  g ∈ Kb(U) α β ≥ 0} are dense (w.r.t. the
topology of uniform convergence) in K (U) when d > 1. This means for any function f ∈ K (U)
there is a sequence of functions (gi)i such that for all i gi ∈ ex K (U) and limi→∞(cid:107) f − gi(cid:107)∞ = 0 
where (cid:107) f(cid:107)∞ := supu∈U | f (u)|. We use this result to show that the set of extremal Bayes risks is
dense in the set of Bayes risks when n > 2.
In order to simplify our analysis  we restrict attention to fair proper losses. A loss is fair if each
partial loss is zero on its corresponding vertex of the simplex ((cid:96)i(ei) = 0  ∀i ∈ [n]). A proper loss is
fair if and only if its Bayes risk is zero at each vertex of the simplex (in this case the Bayes risk is
also called fair). One does not lose generality by studying fair proper losses since any proper loss is
a sum of a fair proper loss and a constant vector.
The set of fair proper losses deﬁned on ∆n form a closed convex cone  denoted Ln. The set of
concave functions which are zero on all the vertices of the simplex ∆n is denoted Fn and is also a
closed convex cone.
Proposition 22 Suppose n > 2. Then for any fair proper loss (cid:96) ∈ Ln there exists a sequence ((cid:96)i)i
of extremal fair proper losses ((cid:96)i ∈ ex Ln) which converges almost everywhere to (cid:96).
The proof of Proposition 22 requires the following lemma which relies upon the correspondence
between a proper loss and its Bayes risk (Proposition 11) and the fact that two continuous functions
equal almost everywhere are equal everywhere.
Lemma 23 If (cid:96) ∈ ex Ln then its corresponding Bayes risk L is extremal in Fn. Conversely  if
L ∈ ex Fn then all the proper losses (cid:96) with Bayes risk equal to L are extremal in Ln.

7

We also need a correspondence between the uniform convergence of a sequence of Bayes risk func-
tions and the convergence of their associated proper losses.
Lemma 24 Suppose L Li ∈ Fn for i ∈ N and suppose (cid:96) and (cid:96)i  i ∈ N are associated proper losses.
Then (Li)i converges uniformly to L if and only if ((cid:96)i)i converges almost everywhere to (cid:96).

Bronshtein [20]
and Johansen [21]
showed how to construct a set of ex-
tremal convex functions which is dense
in K (U). With a trivial change of sign
this leads to a family of extremal proper
fair Bayes risks that is dense in the set
of Bayes risks in the topology of uniform
convergence. This means that it is not
possible to have a small set of extremal
(“primitive”) losses from which one can
construct any proper fair loss by linear
combinations when n > 2.
A convex polytope is a compact convex
intersection of a ﬁnite set of half-spaces
and is therefore the convex hull of its
vertices. Let {ai}i be a ﬁnite family
of afﬁne functions deﬁned on ∆n. Now
deﬁne the convex polyhedral function f
by f (x) := maxi ai(x). The set K :=
{Pi = {x ∈ ∆n : f (x) = ai(x)}} is a cover-
ing of ∆n by polytopes. Theorem 2.1 of [20] shows that for f   Pi and K so deﬁned  f is extremal
if the following two conditions are satisﬁed: 1) for all polytopes Pi in K and for every face F of Pi 
F ∩ ∆n (cid:54)= ∅ implies F has a vertex in ∆n; 2) every vertex of Pi in ∆n belongs to n distinct polytopes
of K. The set of all such f is dense in K (U).
Using this result it is straightforward to exhibit some sets of extremal fair Bayes risks {Lc(p): c ∈
∆n}. Two examples are when Lc(p) =

Figure 1: Complexity of extremal concave functions in two
dimensions (corresponds to n = 3). Graph of an extremal con-
cave function in two dimensions. Lines are where the slope
changes. The pattern of these lines can be arbitrarily complex.

(cid:94)

≤ p j

n

1−pi
1−ci

.

j(cid:54)=i(cid:74) pi
ci ∏
pi

ci

∑

i=1

c j(cid:75) or Lc(p) =

i∈[n]

8 Conclusion

We considered loss functions for multiclass prediction problems and made four main contributions:
• We extended existing results for binary losses to multiclass prediction problems includ-
ing several characterisations of proper losses and the relationship between properness and
classiﬁcation calibration;

• We related the notion of prediction calibration to classiﬁcation calibration;
• We developed some new existence and uniqueness results for proper composite losses
(which are new even in the binary case) which characterise when a loss has a proper com-
posite representation in terms of the geometry of the associated superprediction set; and
• We showed that the attractive (simply parametrised) integral representation for binary

proper losses can not be extended to the multiclass case.

Our results suggest that in order to design losses for multiclass prediction problems it is helpful to
use the composite representation  and design the proper part via the Bayes risk as suggested for the
binary case in [1]. The proper composite representation is used in [22].

Acknowledgements

The work was performed whilst Elodie Vernet was visiting ANU and NICTA  and was supported by
the Australian Research Council and NICTA  through backing Australia’s ability.

8

References
[1] Andreas Buja  Werner Stuetzle and Yi Shen. Loss functions for binary class probability estima-
tion and classiﬁcation: Structure and applications. Technical report  University of Pennsylva-
nia  November 2005. http://www-stat.wharton.upenn.edu/˜buja/PAPERS/
paper-proper-scoring.pdf.

[2] Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules  prediction  and estima-

tion. Journal of the American Statistical Association  102(477):359-378  March 2007.

[3] Mark D. Reid and Robert C. Williamson. Information  divergence and risk for binary experi-

ments. Journal of Machine Learning Research  12:731-817  March 2011.

[4] Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine

Learning Research  11:2387-2422  2010.

[5] Peter L. Bartlett  Michael I. Jordan and Jon D. McAuliffe. Convexity  classiﬁcation  and risk

bounds. Journal of the American Statistical Association  101(473):138-156  March 2006.

[6] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225-1251  2004.

[7] Simon I. Hill and Arnaud Doucet. A framework for kernel-based multi-category classiﬁcation.

Journal of Artiﬁcial Intelligence Research  30:525-564  2007.

[8] Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research  8:1007-1025  2007.

[9] Yufeng Liu. Fisher consistency of multicategory support vector machines. Proceedings of the
Eleventh International Conference on Artiﬁcial Intelligence and Statistics  side 289-296  2007.
[10] Ra´ul Santos-Rodr´ıguez  Alicia Guerrero-Curieses  Roc´ıo Alaiz-Rodriguez and Jes´us Cid-
Sueiro. Cost-sensitive learning based on Bregman divergences. Machine Learning  76:271-
285  2009. http://dx.doi.org/10.1007/s10994-009-5132-8.

[11] Hui Zou  Ji Zhu and Trevor Hastie. New multicategory boosting algorithms based on multi-

category Fisher-consistent losses. The Annals of Applied Statistics  2(4):1290-1306  2008.

[12] Zhihua Zhang  Michael I. Jordan  Wu-Jun Li and Dit-Yan Yeung. Coherence functions for
multicategory margin-based classiﬁcation methods. Proceedings of the Twelfth Conference on
Artiﬁcial Intelligence and Statistics (AISTATS)  2009.

[13] Tobias Glasmachers. Universal consistency of multi-class support vector classication. Ad-

vances in Neural Information Processing Systems (NIPS)  2010.

[14] Elodie Vernet  Robert C. Williamson and Mark D. Reid. Composite multiclass losses. (with
proofs). To appear in NIPS 2011  October 2011. http://users.cecs.anu.edu.au/
˜williams/papers/P188.pdf.

[15] Jean-Baptiste Hiriart-Urruty and Claude Lemar´echal. Fundamentals of Convex Analysis.

Springer  Berlin  2001.

[16] Nicolas S. Lambert. Elicitation and evaluation of statistical forecasts. Technical report  Stan-
ford University  March 2010. http://www.stanford.edu/˜nlambert/lambert_
elicitation.pdf.

[17] Jes´us Cid-Sueiro and An´ıbal R. Figueiras-Vidal. On the structure of strict sense Bayesian cost
functions and its applications. IEEE Transactions on Neural Networks  12(3):445-455  May
2001.

[18] Yuri Kalnishkan and Michael V. Vyugin. The weak aggregating algorithm and weak mixability.

Journal of Computer and System Sciences  74:1228-1244  2008.

[19] Robert R. Phelps. Lectures on Choquet’s Theorem  volume 1757 of Lecture Notes in Mathe-

matics. Springer  2nd edition  2001.

[20] Eﬁm Mikhailovich Bronshtein. Extremal convex functions. Siberian Mathematical Journal 

19:6-12  1978.

[21] Søren Johansen. The extremal convex functions. Mathematica Scandinavica  34:61-68  1974.
[22] Tim van Erven  Mark D. Reid and Robert C. Williamson. Mixability is Bayes risk curvature
relative to log loss. Proceedings of the 24th Annual Conference on Learning Theory  2011. To
appear. http://users.cecs.anu.edu.au/˜williams/papers/P186.pdf.

[23] Rolf Schneider. Convex Bodies: The Brunn-Minkowski Theory. Cambridge University Press 

1993.

9

,Emily Denton
Wojciech Zaremba
Joan Bruna
Yann LeCun
Rob Fergus
Alexis Bellot
Mihaela van der Schaar
Shuang Wu
Guanrui Wang
Pei Tang
Feng Chen
Luping Shi