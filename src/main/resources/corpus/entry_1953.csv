2015,Extending Gossip Algorithms to Distributed Estimation of U-statistics,Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention  computation of U-statistics  relying on more expensive averaging over pairs of observations  is a less investigated area. Yet  such data functionals are essential to describe global properties of a statistical population  with important examples including Area Under the Curve  empirical variance  Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U-statistic of interest. We establish convergence rate bounds of O(1 / t) and O(log t / t) for the synchronous and asynchronous cases respectively  where t is the number of iterations  with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis  numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.,Extending Gossip Algorithms to

Distributed Estimation of U-Statistics

Igor Colin  Joseph Salmon  St´ephan Cl´emenc¸on

LTCI  CNRS  T´el´ecom ParisTech

Universit´e Paris-Saclay

75013 Paris  France

first.last@telecom-paristech.fr

Aur´elien Bellet
Magnet Team

INRIA Lille - Nord Europe

59650 Villeneuve d’Ascq  France
aurelien.bellet@inria.fr

Abstract

Efﬁcient and robust algorithms for decentralized estimation in networks are es-
sential to many distributed systems. Whereas distributed estimation of sample
mean statistics has been the subject of a good deal of attention  computation of U-
statistics  relying on more expensive averaging over pairs of observations  is a less
investigated area. Yet  such data functionals are essential to describe global prop-
erties of a statistical population  with important examples including Area Under
the Curve  empirical variance  Gini mean difference and within-cluster point scat-
ter. This paper proposes new synchronous and asynchronous randomized gossip
algorithms which simultaneously propagate data across the network and main-
tain local estimates of the U-statistic of interest. We establish convergence rate
bounds of O(1/t) and O(log t/t) for the synchronous and asynchronous cases
respectively  where t is the number of iterations  with explicit data and network
dependent terms. Beyond favorable comparisons in terms of rate analysis  numer-
ical experiments provide empirical evidence the proposed algorithms surpasses
the previously introduced approach.

1

Introduction

Decentralized computation and estimation have many applications in sensor and peer-to-peer net-
works as well as for extracting knowledge from massive information graphs such as interlinked Web
documents and on-line social media. Algorithms running on such networks must often operate under
tight constraints: the nodes forming the network cannot rely on a centralized entity for communica-
tion and synchronization  without being aware of the global network topology and/or have limited
resources (computational power  memory  energy). Gossip algorithms [19  18  5]  where each node
exchanges information with at most one of its neighbors at a time  have emerged as a simple yet pow-
erful technique for distributed computation in such settings. Given a data observation on each node 
gossip algorithms can be used to compute averages or sums of functions of the data that are separa-
ble across observations (see for example [10  2  15  11  9] and references therein). Unfortunately 
these algorithms cannot be used to efﬁciently compute quantities that take the form of an average
over pairs of observations  also known as U-statistics [12]. Among classical U-statistics used in
machine learning and data mining  one can mention  among others: the sample variance  the Area
Under the Curve (AUC) of a classiﬁer on distributed data  the Gini mean difference  the Kendall
tau rank correlation coefﬁcient  the within-cluster point scatter and several statistical hypothesis test
statistics such as Wilcoxon Mann-Whitney [14].
In this paper  we propose randomized synchronous and asynchronous gossip algorithms to efﬁciently
compute a U-statistic  in which each node maintains a local estimate of the quantity of interest
throughout the execution of the algorithm. Our methods rely on two types of iterative information
exchange in the network: propagation of local observations across the network  and averaging of lo-

1

cal estimates. We show that the local estimates generated by our approach converge in expectation to
the value of the U-statistic at rates of O(1/t) and O(log t/t) for the synchronous and asynchronous
versions respectively  where t is the number of iterations. These convergence bounds feature data-
dependent terms that reﬂect the hardness of the estimation problem  and network-dependent terms
related to the spectral gap of the network graph [3]  showing that our algorithms are faster on well-
connected networks. The proofs rely on an original reformulation of the problem using “phantom
nodes”  i.e.  on additional nodes that account for data propagation in the network. Our results largely
improve upon those presented in [17]: in particular  we achieve faster convergence together with
lower memory and communication costs. Experiments conducted on AUC and within-cluster point
scatter estimation using real data conﬁrm the superiority of our approach.
The rest of this paper is organized as follows. Section 2 introduces the problem of interest as well as
relevant notation. Section 3 provides a brief review of the related work in gossip algorithms. We then
describe our approach along with the convergence analysis in Section 4  both in the synchronous and
asynchronous settings. Section 5 presents our numerical results.

2 Background

2.1 Deﬁnitions and Notations
For any integer p > 0  we denote by [p] the set {1  . . .   p} and by |F| the cardinality of any ﬁnite set
F . We represent a network of size n > 0 as an undirected graph G = (V  E)  where V = [n] is the
set of vertices and E ⊆ V × V the set of edges. We denote by A(G) the adjacency matrix related
to the graph G  that is for all (i  j) ∈ V 2  [A(G)]ij = 1 if and only if (i  j) ∈ E. For any node
i ∈ V   we denote its degree by di = |{j : (i  j) ∈ E}|. We denote by L(G) the graph Laplacian of
G  deﬁned by L(G) = D(G) − A(G) where D(G) = diag(d1  . . .   dn) is the matrix of degrees. A
graph G = (V  E) is said to be connected if for all (i  j) ∈ V 2 there exists a path connecting i and j;
it is bipartite if there exist S  T ⊂ V such that S ∪ T = V   S ∩ T = ∅ and E ⊆ (S × T ) ∪ (T × S).
A matrix M ∈ Rn×n is nonnegative (resp. positive) if and only if for all (i  j) ∈ [n]2  [M ]ij ≥ 0 
[M ]ij > 0). We write M ≥ 0 (resp. M > 0) when this holds. The transpose of M is
(resp.
denoted by M(cid:62). A matrix P ∈ Rn×n is stochastic if and only if P ≥ 0 and P 1n = 1n  where
1n = (1  . . .   1)(cid:62) ∈ Rn. The matrix P ∈ Rn×n is bi-stochastic if and only if P and P (cid:62) are
stochastic. We denote by In the identity matrix in Rn×n  (e1  . . .   en) the standard basis in Rn  I{E}
the indicator function of an event E and (cid:107) · (cid:107) the usual (cid:96)2 norm.

2.2 Problem Statement
Let X be an input space and (X1  . . .   Xn) ∈ X n a sample of n ≥ 2 points in that space. We assume
X ⊆ Rd for some d > 0 throughout the paper  but our results straightforwardly extend to the more
general setting. We denote as X = (X1  . . .   Xn)(cid:62) the design matrix. Let H : X × X → R be
a measurable function  symmetric in its two arguments and with H(X  X) = 0  ∀X ∈ X . We
consider the problem of estimating the following quantity  known as a degree two U-statistic [12]:1

H(Xi  Xj).

(1)

n(cid:88)

i j=1

ˆUn(H) =

1
n2

In this paper  we illustrate the interest of U-statistics on two applications  among many others. The
ﬁrst one is the within-cluster point scatter [4]  which measures the clustering quality of a partition
P of X as the average distance between points in each cell C ∈ P. It is of the form (1) with

HP (X  X(cid:48)) = (cid:107)X − X(cid:48)(cid:107) ·(cid:88)

(2)
We also study the AUC measure [8]. For a given sample (X1  (cid:96)1)  . . .   (Xn  (cid:96)n) on X × {−1  +1} 
the AUC measure of a linear classiﬁer θ ∈ Rd−1 is given by:

C∈P

I{(X X(cid:48))∈C2}.

(cid:80)
(cid:16)(cid:80)
(cid:17) .
1≤i j≤n(1 − (cid:96)i(cid:96)j)I{(cid:96)i(θ(cid:62)Xi)>−(cid:96)j (θ(cid:62)Xj )}
I{(cid:96)i=−1}

(cid:17)(cid:16)(cid:80)

I{(cid:96)i=1}

1≤i≤n

4

1≤i≤n

AUC(θ) =

(3)

1We point out that the usual deﬁnition of U-statistic differs slightly from (1) by a factor of n/(n − 1).

2

for p = 1  . . .   n do

Algorithm 1 GoSta-sync: a synchronous gossip algorithm for computing a U-statistic
Require: Each node k holds observation Xk
1: Each node k initializes its auxiliary observation Yk = Xk and its estimate Zk = 0
2: for t = 1  2  . . . do
3:
Set Zp ← t−1
4:
5:
6:
7:
8:
9: end for

end for
Draw (i  j) uniformly at random from E
Set Zi  Zj ← 1
Swap auxiliary observations of nodes i and j: Yi ↔ Yj

2 (Zi + Zj)

t Zp + 1

t H(Xp  Yp)

This score is the probability for a classiﬁer to rank a positive observation higher than a negative one.
We focus here on the decentralized setting  where the data sample is partitioned across a set of nodes
in a network. For simplicity  we assume V = [n] and each node i ∈ V only has access to a single
data observation Xi.2 We are interested in estimating (1) efﬁciently using a gossip algorithm.

3 Related Work

Gossip algorithms have been extensively studied in the context of decentralized averaging in net-
works  where the goal is to compute the average of n real numbers (X = R):

n(cid:88)

i=1

¯Xn =

1
n

Xi =

X(cid:62)1n.

1
n

(4)

as maxima and minima  or sums of the form(cid:80)n

One of the earliest work on this canonical problem is due to [19]  but more efﬁcient algorithms have
recently been proposed  see for instance [10  2]. Of particular interest to us is the work of [2]  which
introduces a randomized gossip algorithm for computing the empirical mean (4) in a context where
nodes wake up asynchronously and simply average their local estimate with that of a randomly
chosen neighbor. The communication probabilities are given by a stochastic matrix P   where pij
is the probability that a node i selects neighbor j at a given iteration. As long as the network
graph is connected and non-bipartite  the local estimates converge to (4) at a rate O(e−ct) where
the constant c can be tied to the spectral gap of the network graph [3]  showing faster convergence
for well-connected networks.3 Such algorithms can be extended to compute other functions such
i=1 f (Xi) for some function f : X → R (as done
for instance in [15]). Some work has also gone into developing faster gossip algorithms for poorly
connected networks  assuming that nodes know their (partial) geographic location [6  13]. For a
detailed account of the literature on gossip algorithms  we refer the reader to [18  5].
However  existing gossip algorithms cannot be used to efﬁciently compute (1) as it depends on pairs
of observations. To the best of our knowledge  this problem has only been investigated in [17].
Their algorithm  coined U2-gossip  achieves O(1/t) convergence rate but has several drawbacks.
First  each node must store two auxiliary observations  and two pairs of nodes must exchange an
observation at each iteration. For high-dimensional problems (large d)  this leads to a signiﬁcant
memory and communication load. Second  the algorithm is not asynchronous as every node must
update its estimate at each iteration. Consequently  nodes must have access to a global clock  which
is often unrealistic in practice. In the next section  we introduce new synchronous and asynchronous
algorithms with faster convergence as well as smaller memory and communication cost per iteration.

4 GoSta Algorithms

observation that ˆUn(H) = 1/n(cid:80)n

i=1 hi  with hi = 1/n(cid:80)n

In this section  we introduce gossip algorithms for computing (1). Our approach is based on the
j=1 H(Xi  Xj)  and we write h =
(h1  . . .   hn)(cid:62). The goal is thus similar to the usual distributed averaging problem (4)  with the

2Our results generalize to the case where each node holds a subset of the observations (see Section 4).
3For the sake of completeness  we provide an analysis of this algorithm in the supplementary material.

3

(a) Original graph G.

(b) New graph ˜G.

Figure 1: Comparison of original network and “phantom network”.

key difference that each local value hi is itself an average depending on the entire data sample.
Consequently  our algorithms will combine two steps at each iteration: a data propagation step to
allow each node i to estimate hi  and an averaging step to ensure convergence to the desired value
ˆUn(H). We ﬁrst present the algorithm and its analysis for the (simpler) synchronous setting in
Section 4.1  before introducing an asynchronous version (Section 4.2).

4.1 Synchronous Setting

In the synchronous setting  we assume that the nodes have access to a global clock so that they can
all update their estimate at each time instance. We stress that the nodes need not to be aware of the
global network topology as they will only interact with their direct neighbors in the graph.
Let us denote by Zk(t) the (local) estimate of ˆUn(H) by node k at iteration t. In order to propagate
data across the network  each node k maintains an auxiliary observation Yk  initialized to Xk. Our
algorithm  coined GoSta  goes as follows. At each iteration  each node k updates its local estimate
by taking the running average of Zk(t) and H(Xk  Yk). Then  an edge of the network is drawn uni-
formly at random  and the corresponding pair of nodes average their local estimates and swap their
auxiliary observations. The observations are thus each performing a random walk (albeit coupled)
on the network graph. The full procedure is described in Algorithm 1.
In order to prove the convergence of Algorithm 1  we consider an equivalent reformulation of the
problem which allows us to model the data propagation and the averaging steps separately. Specif-
ically  for each k ∈ V   we deﬁne a phantom Gk = (Vk  Ek) of the original network G  with
Vk = {vk
j ); (i  j) ∈ E}. We then create a new graph ˜G = ( ˜V   ˜E)
where each node k ∈ V is connected to its counterpart vk

i ; 1 ≤ i ≤ n} and Ek = {(vk

i   vk

k ∈ Vk:

(cid:26) ˜V = V ∪ (∪n

˜E = E ∪ (∪n

k=1Vk)
k=1Ek) ∪ {(k  vk

k ); k ∈ V }

The construction of ˜G is illustrated in Figure 1. In this new graph  the nodes V from the original
network will hold the estimates Z1(t)  . . .   Zn(t) as described above. The role of each Gk is to
simulate the data propagation in the original graph G. For i ∈ [n]  vk
i ∈ V k initially holds the value
i and vk
H(Xk  Xi). At each iteration  we draw a random edge (i  j) of G and nodes vk
j swap their
value for all k ∈ [n]. To update its estimate  each node k will use the current value at vk
k.
We can now represent the system state at iteration t by a vector S(t) = (S1(t)(cid:62)  S2(t)(cid:62))(cid:62) ∈
Rn+n2. The ﬁrst n coefﬁcients  S1(t)  are associated with nodes in V and correspond to the estimate
vector Z(t) = [Z1(t)  . . .   Zn(t)](cid:62). The last n2 coefﬁcients  S2(t)  are associated with nodes in
(Vk)1≤k≤n and represent the data propagation in the network. Their initial value is set to S2(0) =
(e(cid:62)
1 H  . . .   e(cid:62)
Remark 1. The “phantom network” ˜G is of size O(n2)  but we stress the fact that it is used solely
as a tool for the convergence analysis: Algorithm 1 operates on the original graph G.

n H) so that for any (k  l) ∈ [n]2  node vk

l initially stores the value H(Xk  Xl).

The transition matrix of this system accounts for three events: the averaging step (the action of G
on itself)  the data propagation (the action of Gk on itself for all k ∈ V ) and the estimate update

4

(the action of Gk on node k for all k ∈ V ). At a given step t > 0  we are interested in characterizing
the transition matrix M (t) such that E[S(t + 1)] = M (t)E[S(t)]. For the sake of clarity  we write
M (t) as an upper block-triangular (n + n2) × (n + n2) matrix:

(5)
with M1(t) ∈ Rn×n  M2(t) ∈ Rn×n2 and M3(t) ∈ Rn2×n2. The bottom left part is necessarily
0  because G does not inﬂuence any Gk. The upper left M1(t) block corresponds to the averaging
step; therefore  for any t > 0  we have:

M (t) =

0

 

M3(t)

(cid:18)M1(t) M2(t)
(cid:19)
(ei − ej)(ei − ej)(cid:62)(cid:19)
(ei − ej)(ei − ej)(cid:62)(cid:19)

(cid:124)

and M3(t) =

W1 (G)

...

0

0

(cid:18)

(cid:88)
(cid:18)

(i j)∈E

(cid:88)

M1(t) =

t − 1
t

· 1
|E|

In − 1
2

where for any α > 1  Wα(G) is deﬁned by:
In − 1
α

Wα(G) =

1
|E|

(i j)∈E

Furthermore  M2(t) and M3(t) are deﬁned as follows:

M2(t) =

1
t



(cid:124)

e(cid:62)

1

0

...

0



(cid:125)

···

...
0

0

...

0
e(cid:62)

n

0
...

···

(cid:123)(cid:122)

B

t − 1
t

=

W2 (G)  

= In − 2

α|E| L(G).

(6)


(cid:125)

 

···

0
...

···

(cid:123)(cid:122)

C

0

...
...

...
0 W1 (G)

where M2(t) is a block diagonal matrix corresponding to the observations being propagated  and
M3(t) represents the estimate update for each node k. Note that M3(t) = W1 (G) ⊗ In where ⊗ is
the Kronecker product.
We can now describe the expected state evolution. At iteration t = 0  one has:

E[S(1)] = M (1)E[S(0)] = M (1)S(0) =

(cid:19)

(cid:19)(cid:18) 0

(cid:18)BS2(0)
(cid:19)

(cid:18)0 B
(cid:18) 1
(cid:19)
(cid:80)t
CS2(0)
s=1 W2 (G)t−s BC s−1S2(0)

S2(0)

0 C

=

.

t

.

(7)

(8)

Using recursion  we can write:

E[S(t)] = M (t)M (t − 1) . . . M (1)S(0) =

in order

(cid:80)t
to prove the convergence of Algorithm 1  one needs to show that
Therefore 
s=1 W2 (G)t−s BC s−1S2(0) = ˆUn(H)1n. We state this precisely in the next theo-
limt→+∞ 1
t
rem.
Theorem 1. Let G be a connected and non-bipartite graph with n nodes  X ∈ Rn×d a design
matrix and (Z(t)) the sequence of estimates generated by Algorithm 1. For all k ∈ [n]  we have:

C tS2(0)

Moreover  for any t > 0 

E[Zk(t)] =

lim
t→+∞

(cid:13)(cid:13)(cid:13)E[Z(t)] − ˆUn(H)1n

(cid:13)(cid:13)(cid:13) ≤ 1

ct

H(Xi  Xj) = ˆUn(H).

(cid:13)(cid:13)(cid:13) +

(cid:18) 2

ct

(cid:19)(cid:13)(cid:13)H − h1(cid:62)

n

(9)

(cid:13)(cid:13)  

+ e−ct

where c = c(G) := 1 − λ2(2) and λ2(2) is the second largest eigenvalue of W2 (G).

1
n2

(cid:88)
(cid:13)(cid:13)(cid:13)h − ˆUn(H)1n

1≤i j≤n

Proof. See supplementary material.

Theorem 1 shows that the local estimates generated by Algorithm 1 converge to ˆUn(H) at a rate
O(1/t). Furthermore  the constants reveal the rate dependency on the particular problem instance.
Indeed  the two norm terms are data-dependent and quantify the difﬁculty of the estimation problem
itself through a dispersion measure. In contrast  c(G) is a network-dependent term since 1−λ2(2) =
βn−1/|E|  where βn−1 is the second smallest eigenvalue of the graph Laplacian L(G) (see Lemma 1
in the supplementary material). The value βn−1 is also known as the spectral gap of G and graphs
with a larger spectral gap typically have better connectivity [3]. This will be illustrated in Section 5.

5

Algorithm 2 GoSta-async: an asynchronous gossip algorithm for computing a U-statistic
Require: Each node k holds observation Xk and pk = 2dk/|E|
1: Each node k initializes Yk = Xk  Zk = 0 and mk = 0
2: for t = 1  2  . . . do
3:
4:
5:
6:
7:
8:
9: end for

Draw (i  j) uniformly at random from E
Set mi ← mi + 1/pi and mj ← mj + 1/pj
Set Zi  Zj ← 1
Set Zi ← (1 − 1
Set Zj ← (1 − 1
Swap auxiliary observations of nodes i and j: Yi ↔ Yj

2 (Zi + Zj)
pimi

H(Xi  Yi)
H(Xj  Yj)

)Zi + 1
pimi
)Zj + 1

pj mj

pj mj

  Y (2)

Comparison to U2-gossip. To estimate ˆUn(H)  U2-gossip [17] does not use averaging. Instead 
each node k requires two auxiliary observations Y (1)
k which are both initialized to Xk.
At each iteration  each node k updates its local estimate by taking the running average of Zk and
H(Y (1)
). Then  two random edges are selected: the nodes connected by the ﬁrst (resp. sec-
ond) edge swap their ﬁrst (resp. second) auxiliary observations. A precise statement of the algorithm
is provided in the supplementary material. U2-gossip has several drawbacks compared to GoSta: it
requires initiating communication between two pairs of nodes at each iteration  and the amount of
communication and memory required is higher (especially when data is high-dimensional). Further-
more  applying our convergence analysis to U2-gossip  we obtain the following reﬁned rate:4

and Y (2)

k

k

k

(cid:13)(cid:13)(cid:13)E[Z(t)] − ˆUn(H)1n

(cid:13)(cid:13)(cid:13) ≤

(cid:18)

√

n
t

(cid:13)(cid:13)(cid:13)h − ˆUn(H)1n

(cid:13)(cid:13)(cid:13) +

2

1 − λ2(1)

(cid:13)(cid:13)H − h1(cid:62)

n

(cid:13)(cid:13)(cid:19)

 

1

1 − λ2(1)2

(10)
where 1 − λ2(1) = 2(1 − λ2(2)) = 2c(G) and λ2(1) is the second largest eigenvalue of W1(G).
The advantage of propagating two observations in U2-gossip is seen in the 1/(1 − λ2(1)2) term 
n factor. Intuitively  this is because nodes do
however the absence of averaging leads to an overall
not beneﬁt from each other’s estimates. In practice  λ2(2) and λ2(1) are close to 1 for reasonably-
sized networks (for instance  λ2(2) = 1 − 1/n for the complete graph)  so the square term does
n factor dominates in (10). We thus expect U2-gossip to converge
not provide much gain and the
slower than GoSta  which is conﬁrmed by the numerical results presented in Section 5.

√

√

4.2 Asynchronous Setting

In practical settings  nodes may not have access to a global clock to synchronize the updates. In this
section  we remove the global clock assumption and propose a fully asynchronous algorithm where
each node has a local clock  ticking at a rate 1 Poisson process. Yet  local clocks are i.i.d. so one
can use an equivalent model with a global clock ticking at a rate n Poisson process and a random
edge draw at each iteration  as in synchronous setting (one may refer to [2] for more details on clock
modeling). However  at a given iteration  the estimate update step now only involves the selected
pair of nodes. Therefore  the nodes need to maintain an estimate of the current iteration number to
ensure convergence to an unbiased estimate of ˆUn(H). Hence for all k ∈ [n]  let pk ∈ [0  1] denote
the probability of node k being picked at any iteration. With our assumption that nodes activate with
a uniform distribution over E  pk = 2dk/|E|. Moreover  the number of times a node k has been
selected at a given iteration t > 0 follows a binomial distribution with parameters t and pk. Let us
deﬁne mk(t) such that mk(0) = 0 and for t > 0:

pk

if k is picked at iteration t 
otherwise.

mk(t) =

mk(t − 1)

(11)
For any k ∈ [n] and any t > 0  one has E[mk(t)] = t × pk × 1/pk = t. Therefore  given that
every node knows its degree and the total number of edges in the network  the iteration estimates are
unbiased. We can now give an asynchronous version of GoSta  as stated in Algorithm 2.
To show that local estimates converge to ˆUn(H)  we use a similar model as in the synchronous
setting. The time dependency of the transition matrix is more complex ; so is the upper bound.

(cid:26) mk(t − 1) + 1

4The proof can be found in the supplementary material.

6

Dataset

Wine Quality (n = 1599)
SVMguide3 (n = 1260)

Complete graph Watts-Strogatz
2.72 · 10−5
6.26 · 10−4
7.94 · 10−4
5.49 · 10−5

2d-grid graph
3.66 · 10−6
6.03 · 10−6

Table 1: Value of 1 − λ2(2) for each network.

Theorem 2. Let G be a connected and non bipartite graph with n nodes  X ∈ Rn×d a design
matrix and (Z(t)) the sequence of estimates generated by Algorithm 2. For all k ∈ [n]  we have:

lim
t→+∞

1
n2

H(Xi  Xj) = ˆUn(H).

Moreover  there exists a constant c(cid:48)(G) > 0 such that  for any t > 1 

E[Zk(t)] =

(cid:88)
(cid:13)(cid:13)(cid:13) ≤ c(cid:48)(G) · log t
(cid:13)(cid:13)(cid:13)E[Z(t)] − ˆUn(H)1n

1≤i j≤n

(cid:107)H(cid:107).

t

(12)

(13)

Proof. See supplementary material.
Remark 2. Our methods can be extended to the situation where nodes contain multiple observa-
tions: when drawn  a node will pick a random auxiliary observation to swap. Similar convergence
results are achieved by splitting each node into a set of nodes  each containing only one observation
and new edges weighted judiciously.

5 Experiments

n) is quite high  especially in comparison to usual scale-free networks.

In this section  we present two applications on real datasets: the decentralized estimation of the Area
Under the ROC Curve (AUC) and of the within-cluster point scatter. We compare the performance of
our algorithms to that of U2-gossip [17] — see supplementary material for additional comparisons
to some baseline methods. We perform our simulations on the three types of network described
below (corresponding values of 1 − λ2(2) are shown in Table 1).
• Complete graph: This is the case where all nodes are connected to each other. It is the ideal
situation in our framework  since any pair of nodes can communicate directly. For a complete graph
G of size n > 0  1 − λ2(2) = 1/n  see [1  Ch.9] or [3  Ch.1] for details.
• Two-dimensional grid: Here  nodes are located on a 2D grid  and each node is connected to its
√
four neighbors on the grid. This network offers a regular graph with isotropic communication  but
its diameter (
• Watts-Strogatz: This random network generation technique is introduced in [20] and allows us to
create networks with various communication properties. It relies on two parameters: the average
degree of the network k and a rewiring probability p. In expectation  the higher the rewiring prob-
ability  the better the connectivity of the network. Here  we use k = 5 and p = 0.3 to achieve a
connectivity compromise between the complete graph and the two-dimensional grid.
AUC measure. We ﬁrst focus on the AUC measure of a linear classiﬁer θ as deﬁned in (3). We use
the SMVguide3 binary classiﬁcation dataset which contains n = 1260 points in d = 23 dimensions.5
We set θ to the difference between the class means. For each generated network  we perform 50 runs
of GoSta-sync (Algorithm 1) and U2-gossip. The top row of Figure 2 shows the evolution over time
of the average relative error and the associated standard deviation across nodes for both algorithms
on each type of network. On average  GoSta-sync outperforms U2-gossip on every network. The
variance of the estimates across nodes is also lower due to the averaging step. Interestingly  the
performance gap between the two algorithms is greatly increasing early on  presumably because the
exponential term in the convergence bound of GoSta-sync is signiﬁcant in the ﬁrst steps.
Within-cluster point scatter. We then turn to the within-cluster point scatter deﬁned in (2). We use
the Wine Quality dataset which contains n = 1599 points in d = 12 dimensions  with a total of K =
11 classes.6 We focus on the partition P associated to class centroids and run the aforementioned
5This dataset is available at http://mldata.org/repository/data/viewslug/svmguide3/
6This dataset is available at https://archive.ics.uci.edu/ml/datasets/Wine

7

Figure 2: Evolution of the average relative error (solid line) and its standard deviation (ﬁlled area)
with the number of iterations for U2-gossip (red) and Algorithm 1 (blue) on the SVMguide3 dataset
(top row) and the Wine Quality dataset (bottom row).

(a) 20% error reaching time.

(b) Average relative error.

Figure 3: Panel (a) shows the average number of iterations needed to reach an relative error below
0.2  for several network sizes n ∈ [50  1599]. Panel (b) compares the relative error (solid line) and
its standard deviation (ﬁlled area) of synchronous (blue) and asynchronous (red) versions of GoSta.

methods 50 times. The results are shown in the bottom row of Figure 2. As in the case of AUC 
GoSta-sync achieves better perfomance on all types of networks  both in terms of average error and
variance. In Figure 3a  we show the average time needed to reach a 0.2 relative error on a complete
graph ranging from n = 50 to n = 1599. As predicted by our analysis  the performance gap
widens in favor of GoSta as the size of the graph increases. Finally  we compare the performance
of GoSta-sync and GoSta-async (Algorithm 2) in Figure 3b. Despite the slightly worse theoretical
convergence rate for GoSta-async  both algorithms have comparable performance in practice.

6 Conclusion

We have introduced new synchronous and asynchronous randomized gossip algorithms to compute
statistics that depend on pairs of observations (U-statistics). We have proved the convergence rate in
both settings  and numerical experiments conﬁrm the practical interest of the proposed algorithms.
In future work  we plan to investigate whether adaptive communication schemes (such as those of
[6  13]) can be used to speed-up our algorithms. Our contribution could also be used as a building
block for decentralized optimization of U-statistics  extending for instance the approaches of [7  16].

Acknowledgements This work was supported by the chair Machine Learning for Big Data of
T´el´ecom ParisTech  and was conducted when A. Bellet was afﬁliated with T´el´ecom ParisTech.

8

References
[1] B´ela Bollob´as. Modern Graph Theory  volume 184. Springer  1998.
[2] Stephen P. Boyd  Arpita Ghosh  Balaji Prabhakar  and Devavrat Shah. Randomized gossip

algorithms. IEEE Transactions on Information Theory  52(6):2508–2530  2006.

[3] Fan R. K. Chung. Spectral Graph Theory  volume 92. American Mathematical Society  1997.
In Advances in Neural
[4] St´ephan Cl´emenc¸on. On U-processes and clustering performance.

Information Processing Systems 24  pages 37–45  2011.

[5] Alexandros G. Dimakis  Soummya Kar  Jos´e M. F. Moura  Michael G. Rabbat  and Anna
Scaglione. Gossip Algorithms for Distributed Signal Processing. Proceedings of the IEEE 
98(11):1847–1864  2010.

[6] Alexandros G. Dimakis  Anand D. Sarwate  and Martin J. Wainwright. Geographic Gossip: Ef-
ﬁcient Averaging for Sensor Networks. IEEE Transactions on Signal Processing  56(3):1205–
1216  2008.

[7] John C. Duchi  Alekh Agarwal  and Martin J. Wainwright. Dual Averaging for Distributed
Optimization: Convergence Analysis and Network Scaling. IEEE Transactions on Automatic
Control  57(3):592–606  2012.

[8] James A. Hanley and Barbara J. McNeil. The meaning and use of the area under a receiver

operating characteristic (ROC) curve. Radiology  143(1):29–36  1982.

[9] Richard Karp  Christian Schindelhauer  Scott Shenker  and Berthold Vocking. Randomized
rumor spreading. In Symposium on Foundations of Computer Science  pages 565–574. IEEE 
2000.

[10] David Kempe  Alin Dobra  and Johannes Gehrke. Gossip-Based Computation of Aggregate
Information. In Symposium on Foundations of Computer Science  pages 482–491. IEEE  2003.
[11] Wojtek Kowalczyk and Nikos A. Vlassis. Newscast EM. In Advances in Neural Information

Processing Systems  pages 713–720  2004.

[12] Alan J. Lee. U-Statistics: Theory and Practice. Marcel Dekker  New York  1990.
[13] Wenjun Li  Huaiyu Dai  and Yanbing Zhang. Location-Aided Fast Distributed Consensus in

Wireless Networks. IEEE Transactions on Information Theory  56(12):6208–6227  2010.

[14] Henry B. Mann and Donald R. Whitney. On a Test of Whether one of Two Random Variables
is Stochastically Larger than the Other. Annals of Mathematical Statistics  18(1):50–60  1947.
[15] Damon Mosk-Aoyama and Devavrat Shah. Fast distributed algorithms for computing separable

functions. IEEE Transactions on Information Theory  54(7):2997–3007  2008.

[16] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent opti-

mization. IEEE Transactions on Automatic Control  54(1):48–61  2009.

[17] Kristiaan Pelckmans and Johan Suykens. Gossip Algorithms for Computing U-Statistics. In

IFAC Workshop on Estimation and Control of Networked Systems  pages 48–53  2009.

[18] Devavrat Shah. Gossip Algorithms. Foundations and Trends in Networking  3(1):1–125  2009.
[19] John N. Tsitsiklis. Problems in decentralized decision making and computation. PhD thesis 

Massachusetts Institute of Technology  1984.

[20] Duncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’networks. Nature 

393(6684):440–442  1998.

9

,Igor Colin
Aurélien Bellet
Joseph Salmon
Stéphan Clémençon
Yunhe Wang
Chang Xu
Dacheng Tao
Chao Xu