2018,Implicit Reparameterization Gradients,By providing a simple and efficient way of computing low-variance gradients of continuous random variables  the reparameterization trick has become the technique of choice for training a variety of latent variable models. However  it is not applicable to a number of important continuous distributions.  We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma  Beta  Dirichlet  and von Mises distributions  which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.,Implicit Reparameterization Gradients

Michael Figurnov

Shakir Mohamed Andriy Mnih

DeepMind  London  UK

{mfigurnov shakir amnih}@google.com

Abstract

By providing a simple and efﬁcient way of computing low-variance gradients of
continuous random variables  the reparameterization trick has become the technique
of choice for training a variety of latent variable models. However  it is not
applicable to a number of important continuous distributions. We introduce an
alternative approach to computing reparameterization gradients based on implicit
differentiation and demonstrate its broader applicability by applying it to Gamma 
Beta  Dirichlet  and von Mises distributions  which cannot be used with the classic
reparameterization trick. Our experiments show that the proposed approach is faster
and more accurate than the existing gradient estimators for these distributions.

1

Introduction

Pathwise gradient estimators are a core tool for stochastic estimation in machine learning and
statistics [12  15  26  42  51]. In machine learning  we now commonly introduce these estimators
using the “reparameterization trick”  in which we replace a probability distribution with an equivalent
parameterization of it  using a deterministic and differentiable transformation of some ﬁxed base
distribution. This reparameterization is a powerful tool for learning because it makes backpropagation
possible in computation graphs with certain types of continuous random variables  e.g. with Normal 
Logistic  or Concrete distributions [23  30]. Many of the recent advances in machine learning were
made possible by this ability to backpropagate through stochastic nodes. They include variational
autoenecoders (VAEs)  automatic variational inference [26  28  42]  Bayesian learning in neural
networks [7  14]  and principled regularization in deep networks [13  34].
The reparameterization trick is easily used with distributions that have location-scale parameteriza-
tions or tractable inverse cumulative distribution functions (CDFs)  or are expressible as deterministic
transformations of such distributions. These seemingly modest requirements are still fairly restrictive
as they preclude a number of standard distributions  such as truncated  mixture  Gamma  Beta 
Dirichlet  or von Mises  from being used with reparameterization gradients. This paper provides a
general tool for reparameterization in these important cases.
The limited applicability of reparameterization has often been addressed by using a different class
of gradient estimators  the score-function estimators [12  16  53]. While being more general  they
typically result in high-variance gradients which require problem-speciﬁc variance reduction tech-
niques to be practical. Generalized reparameterizations involve combining the reparameterization
and score-function estimators [36  44]. Another approach is to approximate the intractable derivative
of the inverse CDF [27].
Following Graves [17]  we use implicit differentiation to differentiate the CDF rather than its inverse.
While the method of Graves [17] is only practical for distributions with analytically tractable CDFs
and has been used solely with mixture distributions  we leverage automatic differentiation to handle
distributions with numerically tractable CDFs  such as Gamma and von Mises. We review the
standard reparameterization trick in Section 2 and then make the following contributions:

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

• We develop implicit reparameterization gradients that provide unbiased estimators for continuous
distributions with numerically tractable CDFs. This allows many other important distributions to
be used as easily as the Normal distribution in stochastic computation graphs.
• We show that the proposed gradients are both faster and more accurate than alternative approaches.
• We demonstrate that our method can outperform existing stochastic variational methods at training
• We use implicit reparameterization gradients to train VAEs with Gamma  Beta  and von Mises
latent variables instead of the usual Normal variables  leading to latent spaces with interesting
alternative topologies.

the Latent Dirichlet Allocation topic model in a black-box fashion using amortized inference.

2 Background

2.1 Explicit reparameterization gradients

We start with a review of the original formulation of reparameterization gradients [26  42  51]  which
we will refer to as explicit reparameterization. Suppose we would like to optimize an expectation
Eq(z) [f (z)] of some continuously differentiable function f (z) w.r.t. the parameters  of the
distribution. We assume that we can ﬁnd a standardization function S(z) that when applied to a
sample from q(z) removes its dependence on the parameters of the distribution. The standardization
function should be continuously differentiable w.r.t. its argument and parameters  and invertible:

S(z) = " ⇠ q(")

z = S1

 (").

(1)

For example  for a Gaussian distribution N (µ  ) we can use Sµ (z) = (z  µ)/ ⇠N (0  1). We
can then express the objective as an expectation w.r.t. "  transferring the dependence on  into f:

(2)

(3)

This allows us to compute the gradient of the expectation as the expectation of the gradients:

Eq(z) [f (z)] = Eq(")hf (S1

 ("))i .
 ("))i = Eq(")hrzf (S1

r Eq(z) [f (z)] = Eq(")hrf (S1

 ("))rS1

 (")i .

A standardization function S(z) satisfying the requirements exists for a wide range of continuous
distributions  but it is not always practical to take advantage of this. For instance  the CDF F (z|)
of a univariate distribution provides such a function  mapping samples from it to samples from the
uniform distribution over [0  1]. However  inverting the CDF is often complicated and expensive  and
computing its derivative is even harder.

2.2 Stochastic variational inference

Stochastic variational inference [19] for latent variable models is perhaps the most popular use

case for reparameterization gradients. Consider a model p✓(x) = R p✓(x|z)p(z)dz  where x is
an observation  z 2 RD is a vector-valued latent variable  p✓(x|z) is the likelihood function with
parameters ✓  and p(z) is the prior distribution. Except for a few special cases  maximum likelihood
learning in such models is intractable because of the difﬁculty of the integrals involved. Variational
inference [22] provides a tractable alternative by introducing a variational posterior distribution
q(z|x) and maximizing a lower bound on the marginal log-likelihood:

L(x  ✓  ) = Eq(z|x) [log p✓(x|z)]  KL(q(z|x)kp(z))  log p✓(x).

(4)

Training models with modern stochastic variational inference [26  39] involves gradient-based
optimization of the bound w.r.t. the model parameters ✓ and the variational posterior parameters .
While the KL-divergence term and its gradients can often be computed analytically  the remaining
term and its gradients are typically intractable and are approximated using samples from the variational
posterior. The most general form of this approach involves score-function gradient estimators [33  39 
41] that handle both discrete and continuous latent variables but have relatively high variance. The
reparameterization trick usually provides a lower variance gradient estimator and is easier to use  but
due to the limitations discussed above  is not applicable to many important continuous distributions.

2

3

Implicit reparameterization gradients

r Eq(z) [f (z)] = Eq(z) [rzf (z)rz] ;

We propose an alternative way of computing the reparameterization gradient that avoids the inversion
of the standardization function. We start from Eqn. (3) and perform a change of variable z = S1
 ("):
(5)
Our key insight is that we can compute rz by implicit differentiation. We apply the total gradient
 to the equality S(z) = ". Then  we use the chain rule to expand the total gradient in terms of
rTD
the partial gradients. The standardization function S(z) depends on the parameters  directly via
the subscript parameters and indirectly via the argument z  while the noise " is independent of  by
the deﬁnition of a standardization function. Thus  we have rzS(z)rz + rS(z) = 0  where
all the gradients are partial. Solving this equation for rz yields
rz = (rzS(z))1rS(z)

rz = rS1

 (")|"=S(z).

(6)

This expression for the gradient only requires differentiating the standardization function and not
inverting it. Note that its value does not change under any invertible transformation T (") of the
standardization function  since the corresponding Jacobian r"T (") cancels out with the inverse.
Example: univariate Normal distribution N (µ  2). We illustrate that explicit and implicit repa-
rameterizations give identical results. A standardization function is given by Sµ (z) = (z  µ)/ =
" ⇠N (0  1). Explicit reparameterization inverts this function: z = S1
@µ =
1  @z

@ = ". The implicit reparameterization  Eqn. (6)  gives:

µ (") = µ + "  @z

@z
@µ

= 

@µ

@Sµ (z)
@Sµ (z)

@z

=  1


1


= 1 

@z
@

= 

@

@Sµ (z)
@Sµ (z)

@z

=  zµ

2
1


=

z  µ


.

(7)

The expressions are equivalent  but the implicit version avoids inverting Sµ (z).
Universal standardization function. For univariate distributions  a standardization function is given
by the CDF: S(z) = F (z|) ⇠ Uniform(0  1). Assuming that the CDF is strictly monotonic and
continuously differentiable w.r.t. z and   it satisﬁes the requirements for a standardization function.
Plugging this function into (6)  we have

rz = rF (z|)

q(z)

.

(8)

Therefore  computing the implicit gradient requires only differentiating the CDF. In the multivariate
case  we can perform the multivariate distributional transform [45]:

(9)
S(z) = (F (z1|)  F (z2|z1  )  . . .   F (zD|z1  . . .   zD1  )) = " 
where q(") =QD
d=1 Uniform("d|0  1). Eqn. (6) requires computing the gradient of the (conditional)
CDFs and solving a linear system with matrix rzS(z). If the distribution is factorized  the matrix
is diagonal and the system can be solved in O(D). Otherwise  the matrix is triangular because each
CDF depends only on the preceding elements  and the system is solvable in O(D2).
Algorithm. We present the comparison between the standard explicit and the proposed implicit
reparameterization in Table 1. Samples of z in implicit reparameterization can be obtained with
any suitable method  such as rejection sampling [10]. The required gradients of the standardization
function can be computed either analytically or using automatic differentiation.

4 Applications of implicit reparameterization gradients

We now demonstrate how implicit reparameterization can be applied to a variety of distributions.
Our strategy is to provide a computation method for a standardization function  such as CDF or
multivariate distributional transform  and its gradients.
Truncated univariate distribution. A truncated distribution is obtained by restricting a distribution’s
domain to some range [a  b]. Its CDF can be computed from the CDF of the original distribution:

3

Table 1: Comparison of the two reparameterization types. While they provide the same result  the
implicit version is easier to implement for distributions such as Gamma because it does not require
inverting the standardization function S(z).

Forward pass

Backward pass

Explicit reparameterization
Sample " ⇠ q(")
Set z S 1
 (")
Set rz rS1
Set rf (z) rzf (z)rz

 (")

Implicit reparameterization (proposed)
Sample z ⇠ q(z)

Set rz (rzS(z))1rS(z)
Set rf (z) rzf (z)rz

i=1 wd

F (b|)F (a|)   z 2 [a  b]. Assuming that the gradient rF (z|) is available  we

ˆF (z|  a  b) = F (z|)F (a|)
can easily compute the implicit gradient for the truncated distribution.
Mixture distribution q(z) = PK
i=1 wiqi(z)  where  = (1  . . .   K  w1  . . .   wK). In the
univariate case  the CDF of the mixture is simplyPK
i=1 wiF (z|i). In the multivariate case  the dis-
tributional transform is given by F (zd|z1  . . .   zd1  ) =PK
i F (zd|z1  . . .   zd1  i)  where
wiqi (z1 ... zd1)
j=1 wj qj (z1 ... zd1) is the posterior weight for the mixture component after observing the
wd
i =
PK
ﬁrst d  1 dimensions of the sample. The required gradient can be obtained via automatic differentia-
tion. When the mixture components are fully factorized  we obtain the same result as [17]  but in a
simpler form  due to automatic differentiation and the explicitly speciﬁed linear system.
Gamma distribution Gamma(↵  ) with shape ↵> 0 and rate > 0. The rate can be standardized
using the scaling property: if z ⇠ Gamma(↵  1)  then z/ ⇠ Gamma(↵  ). For the shape
parameter  the CDF of the Gamma distribution with shape ↵ and unit rate is the regularized incomplete
Gamma function (z  ↵) that does not have an analytic expression. Following Moore [35]  we propose
to apply forward-mode automatic differentiation [2] to a numerical method [3] that computes its
value. This provides the derivative @(z ↵)
Student’s t-distribution samples can be derived from samples of Gamma.
2 )  then z ⇠N (0  2) is t-distributed with ⌫ degrees of freedom.
Gamma( ⌫
Beta and Dirichlet distribution samples can also be obtained from samples of Gamma.
z1 ⇠ Gamma(↵  1) and z2 ⇠ Gamma(  1)  then
j=1 zj◆ ⇠ Dirichlet(↵1  . . .  ↵ D).
zDPD

Indeed  if  ⇠
If
z1+z2 ⇠ Beta(↵  ). Similarly  if zi ⇠

Gamma(↵i  1)  then✓ z1PD

Von Mises distribution [31  32] is a maximum entropy distribution on a circle with the density
function vonMises(z|µ  ) = exp( cos(zµ))
  where µ is the location parameter  > 0 is the
concentration  and I0() is the modiﬁed Bessel function of the ﬁrst kind. The location parameter µ
can be standardized by noting that if z ⇠ vonMises(0  )  then z + µ ⇠ vonMises(µ  ). For the
concentration parameter   we propose to use implicit reparameterization by performing forward-
mode automatic differentiation of an efﬁcient numerical method [18] for computation of the CDF.

2⇡I0()

  . . .  

j=1 zj

for roughly twice the cost of computing the CDF.

@↵

2   ⌫

z1

4.1 Accuracy and speed of reparameterization gradient estimators

Implicit reparameterization requires differentiating the CDF w.r.t. its parameters. When this operation
is analytically intractable  e.g. for Gamma and von Mises distributions  we estimate it via forward-
mode differentiation of the code that numerically evaluates the CDF. We implement this approach by
manually performing the required modiﬁcations of the C++ code (see Appendix B). An alternative is
to use a central ﬁnite difference approximation of the derivative: @F (z|)
@ ⇡ F (z|(1+))F (z|(1))
 
where 0 << 1 is the relative step size that we choose via grid search. For the Gamma distribution 
we also compare with two alternatives: (1) the estimator of Knowles [27] that performs explicit
reparameterization by approximately computing the derivative of the inverse CDF; (2) the concurrently
developed method of Jankowiak and Obermeyer [25] that computes implicit reparameterization using
a closed-form approximation of the CDF derivative. We use the reference PyTorch Paszke et al. [40]
implementation of the method of Jankowiak and Obermeyer [25]. The ground truth value of the CDF

2

4

Table 2: Average error and time (measured in seconds per element) of the reparameterization gradient
computation methods. Automatic differentiation achieves the lowest error and the highest speed.

Method
Automatic differentiation
Finite difference
Jankowiak and Obermeyer [25]
Automatic differentiation
Finite difference
Knowles [27]

Gamma

float32

Precision Mean abs. error
2.3 ⇥ 106
1.9 ⇥ 103
4.1 ⇥ 105
5.4 ⇥ 1013
3.2 ⇥ 109
6.5 ⇥ 103

float64

Time (s)
1.9 ⇥ 108
3.8 ⇥ 108
9.0 ⇥ 108
3.2 ⇥ 108
7.1 ⇥ 108
1.2 ⇥ 106

Von Mises

Mean abs. error
1.9 ⇥ 107
9.6 ⇥ 105
1.3 ⇥ 1013
1.1 ⇥ 1010

–

–

Time (s)
3.1 ⇥ 108
3.8 ⇥ 108
3.7 ⇥ 108
5.9 ⇥ 108

–

–

derivative is computed in a computationally expensive but accurate way (see Appendix C). The results
in Table 2 suggest that the automatic differentiation approach provides the highest accuracy and speed.
The ﬁnite difference method can be easier to implement if a CDF computation method is available 
but requires computation in float64 to obtain the float32 precision. This can be problematic for
devices such as GPUs and other accelerators that do not support fast high-precision computation. The
approach of Knowles is slower and signiﬁcantly less accurate due to the approximations of the inverse
CDF derivative computation method. The method of Jankowiak and Obermeyer is 4.5⇥ slower
and 3⇥ less accurate than the automatic differentiation approach  which reﬂects the complexity of
obtaining fast and accurate closed-form approximations to the CDF derivative. In the remaining
experiments we use automatic differentiation and float32 precision.

5 Related work

Surrogate distributions. When explicit reparameterization is not feasible  it is often possible to
modify the model to use alternative distributions that are reparameterizable. This is a popular approach
due to is simplicity. Kucukelbir et al. [28] approximate posterior distributions by a deterministic
transformation of Normal samples; Nalisnick et al. [37] and Nalisnick and Smyth [38] replace Beta
distributions with Kumaraswamy distributions in the Dirichlet Process stick-breaking construction;
Zhang et al. [54] substitute the Gamma distribution for a Weibull distribution; Srivastava and Sutton
[47  48] replace the Dirichlet distribution with a Logistic Normal. Surrogate distributions however do
not always have all the desirable properties of the distributions they replace. For example  as noted
by Ruiz et al. [44]  such surrogate distributions struggle to capture sparsity  which is achievable with
Gamma and Dirichlet distributions.
Integrating out the nuisance variables. In some cases it is possible to trade computation for sim-
plicity of reparameterization. Roeder et al. [43] consider a mixture of reparameterizable distributions
and analytically sum out the discrete mixture component id variable. For a mixture with K compo-
nents  this results in a K-fold increase of computation  compared to direct reparameterization of the
mixture. This approach becomes prohibitively expensive for a chain of mixture distributions  where
the amount of computation grows exponentially with the length of the chain. On the other hand  we
can always estimate the gradients with just one sample by directly reparameterizing the mixture.
Implicit reparameterization gradients. Reparameterization gradients have been known in the
operations research community since the late 1980s under the name of pathwise  or stochastic 
gradients [12  49]. There the “explicit” and “implicit” versions were usually introduced side-by-
side  but they were applied only to univariate distributions and simple computational graphs that do
not require backpropagation. In the machine learning community  the implicit reparameterization
gradients for univariate distributions were introduced by Salimans and Knowles [46]. That work 
as well as Hoffman and Blei [21]  used the implicit gradients to perform backpropagation through
the Gamma distribution using a ﬁnite difference approximation of the CDF derivative. Graves [17]
independently introduced the implicit reparameterization gradients for multivariate distributions
with analytically tractable CDFs  such as mixtures. We add to this rich literature by generalizing
the technique to handle arbitrary standardization functions  deriving a simpler expression than that
of Graves [17] for the multivariate case  showing the connection to explicit reparameterization
gradients  and providing an efﬁcient automatic differentiation method to compute the intractable CDF
derivatives.

5

Reparameterization gradients as differential equation solutions. The concurrent works [24  25]
provide a complementary view of the reparameterization gradients as solutions of a differential
equation called the transport equation. For univariate distributions  the unique solution is Eqn. (8).
However  for the non-factorial multivariate distributions  there are multiple solutions. By choosing an
appropriate one  the variance of the gradient estimator may be reduced. Unfortunately  there does not
seem to be a general way to obtain these solutions  so distribution-speciﬁc derivations are required.
We hypothesize that the transport equation solutions correspond to the implicit reparameterization
gradients for different standardization functions.
Generalized reparameterizations. The limitations of standard reparameterization was recently
tackled by several other works. Ruiz et al. [44] introduced generalized reparameterization gradients
(GRG) that expand the applicability of the reparameterization trick by using a standardization
function that allows the underlying base distribution to depend weakly on the parameter vector
(e.g. only through the higher moments). The resulting gradient estimator  which in addition to the
the reparameterized gradients term includes a score-function gradient term that takes into account
the dependence of the base distribution on the parameter vector  was applied to the Gamma  Beta 
and log-Normal distributions. The challenge of using this approach lies in ﬁnding an effective
approximate standardization function  which is nontrivial yet essential for obtaining low-variance
gradients.
Rejection sampling variational inference (RSVI) [36] is a closely-related approach that combines
the reparameterization gradients from the proposal distribution of a rejection sampler with a score-
function gradient term that takes into account the effect of the accept/reject step. When applied to the
gamma distribution the RSVI gradients can have lower variance gradients than those computed using
GRG [36]. Davidson et al. [9] have recently demonstrated the use of RSVI with the von Mises-Fisher
distribution.

6 Experiments

We apply implicit reparameterization for two distributions with analytically intractable CDFs (Gamma
and von Mises) to three problems: a toy setting of stochastic cross-entropy estimation  training a
Latent Dirichlet Allocation [6] (LDA) topic model  and training VAEs [26  42] with non-Normal
latent distributions. We use the RSVI gradient estimator [36] as our main baseline. For Gamma
distributions  RSVI provides a shape augmentation parameter B that decreases the magnitude of
the score-function correction term by using additional B samples from a uniform distribution. As
B ! 1  the term vanishes and the RSVI gradient becomes equivalent to ours  but with a higher
computational cost. Von Mises distribution does not have such an augmentation parameter. For
LDA  we also compare with a surrogate distribution approach [47] and a classic stochastic variational
inference method [19]. The experimental details are given in Appendix D. We use TensorFlow [1] for
our experiments. Implicit reparameterization for Gamma  Student’s t  Beta  Dirichlet and von Mises
distributions is available in TensorFlow Probability [11]. This library also contains an implementation
of the LDA model from section 6.2.

6.1 Gradient of the cross-entropy

We compare the variance of the implicit and RSVI gradient estimators on a toy problem of stochastic
estimation of the cross-entropy gradient  @
@ Eq(z)[ log p(z)]. It was introduced by Naesseth
et al. [36] as minimization of the KL-divergence; however  since they analytically compute the
entropy  the only source of variance is the cross-entropy term. We use their setting for the Dirichlet
distribution: p(z) = Dirichlet(z|↵1 ↵ 2  . . .  ↵ 100)  q(z) = Dirichlet(z|  ↵2  . . .  ↵ 100)  where
↵ are the posterior parameters for a Dirichlet with a uniform prior after observing 100 samples
from a Categorical distribution. The Dirichlet samples are obtained by transforming samples from
Gamma. Additionally  we construct a similar problem with the von Mises distribution: p(z) =

Q10
d=1 vonMises(zd|0  2) and q(z) = vonMises(z1|0  )Q10

The results presented on Fig. 1 show that the implicit gradient is faster and has lower variance than
RSVI. For the Dirichlet distribution  increasing the shape augmentation parameter B allows RSVI to
asymptotically approach the variance of the implicit gradient. However  this comes at an additional

d=2 vonMises(zd|0  2).

6

(a) Dirichlet distribution

(b) Von Mises distribution

Method
Implicit

RSVI

Dirichlet
5.8 ⇥ 108
1.4 ⇥ 107
1.6 ⇥ 107
1.8 ⇥ 107
2.0 ⇥ 107

Von Mises
2.0 ⇥ 107
3.0 ⇥ 107

B = 0
B = 1
B = 5
B = 10

(c) Computation time (in seconds per
sample of Gamma/von Mises)

Figure 1: Variance of the gradient and computation time for the cross-entropy optimization problem.
The vertical line denotes the optimal value for the parameter. Implicit gradient is faster and has lower
variance than RSVI [36].

computational cost and requires tuning this parameter. Furthermore  such a parameter is not available
for other distributions  including von Mises.

6.2 Latent Dirichlet Allocation

LDA [6] is a popular topic model that represents each document as a bag-of-words and ﬁnds a set
of topics so that each document is well-described by a few topics. It has been extended in various
ways  e.g. [4  5]  and often serves as a testbed for approximate inference methods [19  20  50]. LDA
is a latent variable model with a likelihood p(w|z) =QK
i=1 Categorical(wi|z)  and the prior
p↵(z) = Dirichlet(z|↵)  where w is the observed document represented as a vector of word counts 
z is a distribution of topics   2 R#words⇥#topics is a matrix that speciﬁes the categorical distribution of
words in each topic  and ↵ parameterizes the prior distribution over the topics. We perform amortized
variational inference by using a neural network to parameterize the Dirichlet variational posterior
over the topics z as a function of the observation.
We use the 20 Newsgroups (11 200 documents  2 000-word vocabulary) and RCV1 [29] (800 000
documents  10 000-word vocabulary) datasets with the same preprocessing as in [47]. We report the

n=1

1
Ln

log p(wn)⌘  where Ln is the number of words in

test perplexity of the models  exp⇣ 1

NPN

the document and the marginal log-likelihood is approximated with a single-sample estimate of the
evidence lower bound. Following [52]  we optimize the prior parameters ↵ during training.
We compare amortized variational inference in LDA using implicit reparameterization to several
alternatives: (i) training the LDA model with the RSVI gradients; (ii) stochastic variational inference
(SVI) [19] training method for LDA; (iii) the method of Srivastava and Sutton [47]  which we refer to
as LN-LDA  that uses a Logistic Normal approximation in place of the Dirichlet prior and performs
amortized variational inference using a Logistic Normal variational posterior.
The results in Table 3 and Fig. 3(a-b) show that RSVI matches the implicit gradient results only at
B = 20  as opposed to B = 10 for the previous problem. Lower gradient variance leads to faster
training objective convergence. Interestingly  amortized inference can achieve better perplexity than
SVI. Finally  we see that LDA trained with implicit gradients performs as well or better than LN-LDA.
The learned topics and the prior weights shown on Fig. 2 demonstrate that LDA automatically
determines the number of topics in the corpus by setting some of the prior weights to 0; this does
not occur in LN-LDA model. Additionally  LN-LDA is prone to representing the same topic several
times  perhaps due to a non-sparse variational posterior distribution.
The obtained results suggest that the advantage of implicit gradients compared to RSVI increases
with the complexity of the problem. When the original distributions are replaced by surrogates  some
desirable properties of the solution  such as sparsity  might be lost.

6.3 Variational Autoencoders

VAE [26  42] is a generative latent variable model trained using amortized variational inference. Both
the variational posterior and the generative distributions (also known as the encoder and decoder) are
parameterized using neural networks. VAEs typically use the standard Normal distribution as the
prior and a factorized Normal as the variational posterior. The form of the likelihood depends on

7

Table 3: Test perplexity (lower is better) for the topic modeling task. Mean ± standard deviation over
5 runs. LN-LDA uses Logistic Normal distributions instead of Dirichlet.

Model

LDA [6]

Training method
Implicit reparameterization
RSVI B = 1
RSVI B = 5
RSVI B = 10
RSVI B = 20
SVI

LN-LDA [47] Explicit reparameterization

20 Newsgroups

876 ± 7
1066 ± 7
968 ± 18
887 ± 10
865 ± 11
964 ± 4
875 ± 6

RCV1
896 ± 6
1505 ± 33
1075 ± 15
953 ± 16
907 ± 13
1330 ± 4
951 ± 10

↵ = 1.15 write article get think go
↵ = 1.07 write get think like article
↵ = 1.07 write article get think like
↵ = 1.07 write article get like know
↵ = 1.06 write article think get like
↵ = 1.04 write article get know think
↵ = 1.04 write article get know like
↵ = 1.02 write article think get like

↵ = 0.47 write article get like one
↵ = 0.31 write one people say think
↵ = 0.25 please thanks post send know
↵ = 0.11 use drive card problem system
↵ = 0.10 go say people know get
↵ = 0.08 use ﬁle key program system
↵ = 0.08 gun government law state use
↵ = 0.08 god christian jesus say people

(a) LN-LDA topics

(b) LDA topics (implicit)

(c) 20 Newsgroups weights

(d) RCV1 weights

Figure 2: Left: topics with the highest weight for the 20 Newsgroups dataset; Right: prior topic
weights ↵. LDA learns sparse prior weights  while LN-LDA does not.

the data  with factorized Bernoulli or Normal distributions being popular choices for images. In this
section  we experiment with using distributions other than Normal for the prior and the variational
posterior. The use of alternative distributions allows incorporating different prior assumptions about
the latent factors of the data  such as bounded support or periodicity.
We use fully factorized priors and variational posteriors. For the variational posterior we explore
Gamma  Beta  and von Mises distributions. For Gamma  we use a sparse Gamma(0.3  0.3) prior and
a bell-shaped prior Gamma(10  10). For Beta and von Mises  instead of a sparse prior we choose a
uniform prior over the corresponding domain.
We train the models on the dynamically binarized MNIST dataset [8] using the fully-connected
encoder and decoder architectures from [9]  so our results are comparable. The results in Table 4 show
that a uniform prior and cyclic latent space of von Mises is advantageous for low-dimensional latent
spaces  consistent with the ﬁndings of [9]. For a uniform prior  the factorized von Mises distribution
outperforms the multivariate von Mises-Fisher distribution in low dimensions  perhaps due to the
more ﬂexible concentration parameterization (von Mises-Fisher uses shared concentration across
dimensions). The results obtained with bell-shaped priors are similar to the Normal prior/posterior

(a) LDA on 20 Newsgroups

(b) LDA on RCV1

(c) VAE with von Mises posterior

Figure 3: The training objective (top) and the variance of the gradient (bottom) during training. The
sharp drop in perplexity on RCV1 dataset occurs at the end of the ↵ burn-in period.

8

D = 2

D = 5

Table 4: Test negative log-likelihood (lower is better) for VAE on MNIST. Mean ± standard deviation
over 5 runs. The von Mises-Fisher results are from [9].
Variational posterior
Prior
N (µ  2)
N (0  1)
Gamma(0.3  0.3) Gamma(↵  )
Gamma(↵  )
Gamma(10  10)
Beta(↵  )
Uniform(0  1)
Beta(↵  )
Beta(10  10)
Uniform(⇡  ⇡ )
vonMises(µ  )
vonMises(0  10)
vonMises(µ  )
Uniform(SD)

D = 10
92.5 ± 0.2
94.0 ± 0.3
92.3 ± 0.2
94.1 ± 0.1
92.1 ± 0.2
94.4 ± 0.5
92.3 ± 0.2
93.2 ± 0.1

D = 20
88.1 ± 0.2
90.3 ± 0.2
88.3 ± 0.2
88.9 ± 0.1
87.8 ± 0.1
90.9 ± 0.1
87.8 ± 0.2
89.0 ± 0.3

D = 40
88.1 ± 0.0
90.6 ± 0.2
88.3 ± 0.1
88.6 ± 0.1
87.7 ± 0.1
91.5 ± 0.4
87.9 ± 0.3
90.9 ± 0.3

131.1 ± 0.6
132.4 ± 0.3
135.0 ± 0.2
128.3 ± 0.2
131.1 ± 0.4
127.6 ± 0.4
130.7 ± 0.8
132.5 ± 0.7

107.9 ± 0.4
108.0 ± 0.3
107.0 ± 0.2
107.4 ± 0.2
106.7 ± 0.1
107.5 ± 0.4
107.5 ± 0.5
108.4 ± 0.1

vonMisesFisher(µ  )

(a) Normal posterior and prior 

(b) Beta  uniform prior 

(c) Von Mises  uniform prior 

[3  3] ⇥ [3  3]

[0  1] ⇥ [0  1]

[⇡  ⇡ ] ⇥ [⇡  ⇡ ]

Figure 4: 2D latent spaces learned by a VAE on the MNIST dataset. Normal distribution exhibits a
strong pull to the center  while Beta and Von Mises latents are tiling the entire available space.

pair  as expected. The latent spaces learned by models with 2 latents shown on Fig. 4 demonstrate the
differences in topology.
We provide a detailed comparison between implicit gradients and RSVI in Table 7 of the supplemen-
tary material. For Gamma and Beta distributions  RSVI with B = 20 performs similarly to implicit
gradients. However  for the von Mises distribution implicit gradients usually perform better than
RSVI. For example  for a uniform prior and D = 40  implicit gradients yield a 1.3 nat advantage in
the test log-likelihood due to lower gradient variance (Fig. 3c).

7 Conclusion

Reparameterization gradients have become established as a central tool underlying many of the
recent advances in machine learning. In this paper  we strengthened this tool by extending its
applicability to distributions  such as truncated  Gamma  and von Mises  that are often encountered
in probabilistic modelling. The proposed implicit reparameterization gradients offer a simple and
practical approach to stochastic gradient estimation which has the properties we expect from such
a new type of estimator: it is faster than the existing methods and simultaneously provides lower
gradient variance. These new estimators allow us to move away from making model choices for
reasons of computational convenience. Applying these estimators requires a numerically tractable
CDF or some other standardization function. When one is not available  it should be possible to use
an approximate standardization function to augment implicit reparameterization with a score function
correction term  along the lines of generalized reparameterization. We intend to explore this direction
in future work.

Acknowledgments
We would like to thank Chris Maddison  Hyunjik Kim  Jörg Bornschein  Alex Graves  Hussein Fawzi 
Chris Burgess  Matt Hoffman  and Charles Sutton for helpful discussions. We also thank Akash
Srivastava for providing the preprocessed document datasets.

9

References

[1] M. Abadi  P. Barham  J. Chen  Z. Chen  A. Davis  J. Dean  M. Devin  S. Ghemawat  G. Irving 
M. Isard  et al. “TensorFlow: A System for Large-Scale Machine Learning.” In: USENIX
Symposium on Operating Systems Design and Implementation. Vol. 16. 2016  pp. 265–283.
[2] A. G. Baydin  B. A. Pearlmutter  A. A. Radul  and J. M. Siskind. “Automatic differentiation in

machine learning: a survey”. In: arXiv preprint arXiv:1502.05767 (2015).

[3] G. P. Bhattacharjee. “Algorithm AS 32: The Incomplete Gamma Integral”. In: Journal of
the Royal Statistical Society. Series C (Applied Statistics) 19.3 (1970)  pp. 285–287. ISSN:
00359254  14679876.

[4] D. M. Blei and J. D. Lafferty. “Correlated topic models”. In: Advances in Neural Information

Processing Systems (2005)  pp. 147–154.

[5] D. M. Blei and J. D. Lafferty. “Dynamic topic models”. In: International Conference on

Machine Learning (2006)  pp. 113–120.

[6] D. M. Blei  A. Y. Ng  and M. I. Jordan. “Latent dirichlet allocation”. In: Journal of Machine

Learning Research 3.Jan (2003)  pp. 993–1022.

[7] C. Blundell  J. Cornebise  K. Kavukcuoglu  and D. Wierstra. “Weight uncertainty in neural

networks”. In: International Conference on Machine Learning (2015).

[8] Y. Burda  R. Grosse  and R. Salakhutdinov. “Importance weighted autoencoders”. In: Interna-

tional Conference on Learning Representations (2016).

[9] T. R. Davidson  L. Falorsi  N. De Cao  T. Kipf  and J. M. Tomczak. “Hyperspherical Variational

Auto-Encoders”. In: Conference on Uncertainty in Artiﬁcial Intelligence (2018).

[10] L. Devroye. Non-Uniform Random Variate Generation. Springer  1986.
[11]

J. V. Dillon  I. Langmore  D. Tran  E. Brevdo  S. Vasudevan  D. Moore  B. Patton  A. Alemi 
M. Hoffman  and R. A. Saurous. “TensorFlow Distributions”. In: arXiv (2017).

[12] M. C. Fu. “Gradient estimation”. In: Handbooks in operations research and management

science 13 (2006)  pp. 575–616.

[13] Y. Gal and Z. Ghahramani. “A theoretically grounded application of dropout in recurrent neural

networks”. In: Advances in Neural Information Processing Systems. 2016  pp. 1019–1027.

[14] Y. Gal and Z. Ghahramani. “Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning”. In: International Conference on Machine Learning (2016) 
pp. 1050–1059.

[15] P. Glasserman. Monte Carlo methods in ﬁnancial engineering. Vol. 53. Springer Science &

Business Media  2013.

[16] P. W. Glynn. “Likelihood ratio gradient estimation for stochastic systems”. In: Communications

of the ACM 33.10 (1990)  pp. 75–84.

[17] A. Graves. “Stochastic backpropagation through mixture density distributions”. In: arXiv

preprint arXiv:1607.05690 (2016).

[18] G. W. Hill. “Algorithm 518: Incomplete Bessel Function I0. The Von Mises Distribution”. In:

ACM Transactions on Mathematical Software (TOMS) 3.3 (1977)  pp. 279–284.

[19] M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley. “Stochastic variational inference”. In:

Journal of Machine Learning Research 14.1 (2013)  pp. 1303–1347.

[20] M. Hoffman  F. R. Bach  and D. M. Blei. “Online learning for latent dirichlet allocation”. In:

Advances in Neural Information Processing Systems (2010)  pp. 856–864.

[21] M. Hoffman and D. Blei. “Stochastic structured variational inference”. In: International

Conference on Artiﬁcial Intelligence and Statistics (2015)  pp. 361–369.

[22] T. S. Jaakkola and M. I. Jordan. “Bayesian parameter estimation via variational methods”. In:

Statistics and Computing 10.1 (2000)  pp. 25–37.

[23] E. Jang  S. Gu  and B. Poole. “Categorical reparameterization with gumbel-softmax”. In:

International Conference on Learning Representations (2017).

[24] M. Jankowiak and T. Karaletsos. “Pathwise Derivatives for Multivariate Distributions”. In:

arXiv preprint arXiv:1806.01856 (2018).

[25] M. Jankowiak and F. Obermeyer. “Pathwise Derivatives Beyond the Reparameterization Trick”.

In: International Conference on Machine Learning (2018).

10

[26] D. P. Kingma and M. Welling. “Auto-encoding variational bayes”. In: International Conference

on Learning Representations (2014).

[27] D. A. Knowles. “Stochastic gradient variational Bayes for Gamma approximating distribu-

tions”. In: arXiv preprint arXiv:1509.01631 (2015).

[28] A. Kucukelbir  D. Tran  R. Ranganath  A. Gelman  and D. M. Blei. “Automatic differentiation
variational inference”. In: Journal of Machine Learning Research 18.1 (2017)  pp. 430–474.
[29] D. D. Lewis  Y. Yang  T. G. Rose  and F. Li. “Rcv1: A new benchmark collection for text
categorization research”. In: Journal of Machine Learning Research 5.Apr (2004)  pp. 361–
397.

[30] C. J. Maddison  A. Mnih  and Y. W. Teh. “The concrete distribution: A continuous relaxation of
discrete random variables”. In: International Conference on Learning Representations (2017).

[31] K. V. Mardia and P. E. Jupp. Directional statistics. John Wiley & Sons  2009  p. 494.
[32] R. von Mises. “Über die “Ganzzahligkeit” der Atomgewicht und verwandte Fragen.” In:

Physikalische Z. 19 (1918)  pp. 490–500.

[33] A. Mnih and K. Gregor. “Neural variational inference and learning in belief networks”. In:

International Conference on Machine Learning (2014).

[34] D. Molchanov  A. Ashukha  and D. Vetrov. “Variational dropout sparsiﬁes deep neural net-

works”. In: International Conference on Machine Learning (2017).

[35] R. Moore. “Algorithm AS 187: Derivatives of the incomplete gamma integral”. In: Journal of

the Royal Statistical Society. Series C (Applied Statistics) 31.3 (1982)  pp. 330–335.

[36] C. Naesseth  F. Ruiz  S. Linderman  and D. Blei. “Reparameterization gradients through
acceptance-rejection sampling algorithms”. In: International Conference on Artiﬁcial Intelli-
gence and Statistics (2017)  pp. 489–498.

[37] E. Nalisnick  L. Hertel  and P. Smyth. “Approximate inference for deep latent gaussian
mixtures”. In: Advances in Neural Information Processing Systems Workshop on Bayesian
Deep Learning. Vol. 2. 2016.

[38] E. Nalisnick and P. Smyth. “Stick-breaking variational autoencoders”. In: International Con-

ference on Learning Representations (2017).
J. Paisley  D. Blei  and M. Jordan. “Variational Bayesian inference with stochastic search”. In:
International Conference on Machine Learning (2012).

[39]

[40] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison 
L. Antiga  and A. Lerer. “Automatic differentiation in PyTorch”. In: Advances in Neural
Information Processing Systems Workshop (2017).

[41] R. Ranganath  S. Gerrish  and D. Blei. “Black box variational inference”. In: International

Conference on Artiﬁcial Intelligence and Statistics (2014)  pp. 814–822.

[42] D. J. Rezende  S. Mohamed  and D. Wierstra. “Stochastic backpropagation and approximate
inference in deep generative models”. In: International Conference on Machine Learning
(2014).

[43] G. Roeder  Y. Wu  and D. Duvenaud. “Sticking the landing: An asymptotically zero-variance
gradient estimator for variational inference”. In: Advances in Neural Information Processing
Systems (2017).

[44] F. R. Ruiz  M. Titsias  and D. Blei. “The Generalized Reparameterization Gradient”. In:

Advances in Neural Information Processing Systems (2016).

[45] L. Rüschendorf. “Copulas  Sklar’s theorem  and distributional transform”. In: Mathematical

Risk Analysis. Springer  2013  pp. 3–34.

[46] T. Salimans and D. A. Knowles. “Fixed-form variational posterior approximation through

stochastic linear regression”. In: Bayesian Analysis 8.4 (2013)  pp. 837–882.

[47] A. Srivastava and C. Sutton. “Autoencoding variational inference for topic models”. In:

International Conference on Learning Representations (2017).

[48] A. Srivastava and C. Sutton. “Variational Inference In Pachinko Allocation Machines”. In:

arXiv preprint arXiv:1804.07944 (2018).

[49] R. Suri and M. A. Zazanis. “Perturbation analysis gives strongly consistent sensitivity estimates

for the M/G/1 queue”. In: Management Science 34.1 (1988)  pp. 39–64.

[50] Y. W. Teh  D. Newman  and M. Welling. “A collapsed variational Bayesian inference algorithm
for latent Dirichlet allocation”. In: Advances in Neural Information Processing Systems (2007) 
pp. 1353–1360.

11

[51] M. Titsias and M. Lázaro-Gredilla. “Doubly stochastic variational Bayes for non-conjugate

inference”. In: International Conference on Machine Learning. 2014  pp. 1971–1979.

[52] H. M. Wallach  D. M. Mimno  and A. McCallum. “Rethinking LDA: Why priors matter”. In:

Advances in Neural Information Processing Systems. 2009  pp. 1973–1981.

[53] R. J. Williams. “Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning”. In: Reinforcement Learning (1992)  pp. 5–32.

[54] H. Zhang  B. Chen  D. Guo  and M. Zhou. “WHAI: Weibull Hybrid Autoencoding Inference
for Deep Topic Modeling”. In: International Conference on Learning Representations (2018).

12

,Mikhail Figurnov
Shakir Mohamed
Andriy Mnih