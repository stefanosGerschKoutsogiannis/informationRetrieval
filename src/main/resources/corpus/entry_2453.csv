2012,Hierarchical spike coding of sound,We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation  while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data  the model encodes acoustic features such as harmonic stacks  sweeps  and frequency modulations  that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task.,To appear in: Neural Information Processing Systems (NIPS) 

Lake Tahoe  Nevada. December 3-6  2012.

Hierarchical spike coding of sound

Yan Karklin∗

Chaitanya Ekanadham∗

Howard Hughes Medical Institute 

Courant Institute of Mathematical Sciences

Center for Neural Science

New York University

yan.karklin@nyu.edu

New York University

chaitu@math.nyu.edu

Eero P. Simoncelli

Howard Hughes Medical Institute  Center for Neural Science 

and Courant Institute of Mathematical Sciences

New York University

eero.simoncelli@nyu.edu

Abstract

Natural sounds exhibit complex statistical regularities at multiple scales. Acous-
tic events underlying speech  for example  are characterized by precise temporal
and frequency relationships  but they can also vary substantially according to the
pitch  duration  and other high-level properties of speech production. Learning
this structure from data while capturing the inherent variability is an important
ﬁrst step in building auditory processing systems  as well as understanding the
mechanisms of auditory perception. Here we develop Hierarchical Spike Coding 
a two-layer probabilistic generative model for complex acoustic structure. The
ﬁrst layer consists of a sparse spiking representation that encodes the sound us-
ing kernels positioned precisely in time and frequency. Patterns in the positions
of ﬁrst layer spikes are learned from the data: on a coarse scale  statistical reg-
ularities are encoded by a second-layer spiking representation  while ﬁne-scale
structure is captured by recurrent interactions within the ﬁrst layer. When ﬁt to
speech data  the second layer acoustic features include harmonic stacks  sweeps 
frequency modulations  and precise temporal onsets  which can be composed to
represent complex acoustic events. Unlike spectrogram-based methods  the model
gives a probability distribution over sound pressure waveforms. This allows us to
use the second-layer representation to synthesize sounds directly  and to perform
model-based denoising  on which we demonstrate a signiﬁcant improvement over
standard methods.

1

Introduction

Natural sounds  such as speech and animal vocalizations  consist of complex acoustic events oc-
curring at multiple scales. Precise timing and frequency relationships among these events convey
important information about the sound  while intrinsic variability confounds simple approaches to
sound processing and understanding. Speech  for example  can be described as a sequence of words 
which are composed of precisely interrelated phones  but each utterance may have its own prosody 
with variable duration  loudness  and/or pitch. An auditory representation that captures the corre-
sponding structure while remaining invariant to this variability would provide a useful ﬁrst step for
many applications in auditory processing.

∗Contributed equally

1

Many recent efforts to learn auditory representations in an unsupervised setting have focused on
sparse decompositions chosen to capture structure inherent in sound ensembles. The dictionaries
can be chosen by hand [1  2] or learned from data. For example  Klein et al [3] adapted a set of
time-frequency kernels to represent spectrograms of speech signals and showed that the resulting
kernels were localized and bore resemblance to auditory receptive ﬁelds. Lee et al [4] trained a
two-layer deep belief network on spectrogram patches and used it for several auditory classiﬁcation
tasks.

These approaches have several limitations. First  they operate on spectrograms (rather than the origi-
nal sound waveforms)  which impose limitations on both time and frequency resolution. In addition 
most models built on spectrograms rely on block-based partitioning of time  and thus are susceptible
to artifacts – precisely-timed acoustic events can appear across multiple blocks and events can ap-
pear at different temporal offsets relative to the block  making their identiﬁcation and representation
difﬁcult [5]. The features learned by these models are tied to speciﬁc frequencies  and must be repli-
cated at different frequency offsets to accommodate pitch shifts that occur in natural sounds. Finally 
the linear generative models underlying most methods are unsuitable for constructing hierarchical
models  since the composition of multiple linear stages is again linear.

To address these limitations  we propose a two-layer hierarchical model that encodes complex acous-
tic events using a representation that is shiftable in both time and frequency. The ﬁrst layer is a
“spikegram” representation of the sound pressure waveform  as developed in [6  5]. The prior prob-
abilities for coefﬁcients in the ﬁrst layer are modulated by the output of the second layer  combined
with a recurrent component that operates within the ﬁrst layer. When trained on speech  the kernels
learned at the second layer encode complex acoustic events which  when positioned at speciﬁc times
and frequencies  compactly represent the ﬁrst-layer spikegram  which is itself a compact description
of the sound pressure waveform. Despite its very sparse activation  the second-layer representation
retains much of the acoustic information: sounds sampled according to the generative model approx-
imate well the original sound. Finally  we demonstrate that the model performs well on a denoising
task  particularly when the noise is structured  suggesting that the higher-order representation pro-
vides a useful statistical description of speech.

2 Hierarchical spike coding

In the “spikegram” representation [5]  a sound is encoded using a linear combination of sparse 
time-shifted kernels φf (t):

xt =Xτ f

Sτ f φf (t − τ ) + ǫt

(1)

where ǫt denotes Gaussian white noise and the coefﬁcients Sτ f are mostly zero. As in [5]  the φf (t)
are gammatone functions with varying center frequencies  indexed by f. In order to encode the sig-
nal  a sparse set of “spikes” (i.e.  nonzero coefﬁcients at speciﬁc times and frequencies) is estimated
using an approximate inference method  such as matching pursuit [7]. The resulting spikegram 
shown in Fig. 1b  offers an efﬁcient representation of sounds [8] that avoids the blocking artifacts
and time-frequency trade-offs associated with more traditional spectrogram representations.

We aim to model the statistical regularities present in the spikegram representations. Spikegrams ex-
hibit clear statistical structure  both at coarse (Fig. 1b c) and at ﬁne temporal scales (Fig. 1e f). Spikes
placed at precise locations in time and frequency reveal acoustic features  harmonic structures  as
well as slow modulations in the sound envelope. The coarse scale non-stationarity is likely caused
by higher-order acoustic events  such as phoneme utterances that span a much larger time-frequency
range than the individual gammatone kernels. On the other hand  the ﬁne-scale correlations are
due to some combination of the correlations inherent in the gammatone ﬁlterbank and the precise
temporal structure present in speech.

We introduce the hierarchical spike coding (HSC) model  illustrated in Fig. 2  to capture the struc-
ture in the spikegrams (S(1)) on both coarse and ﬁne scales. We add a second layer of unobserved
spikes (S(2))  assumed to arise from a Poisson process with constant rate λ. These spikes are con-
volved with a set of time-frequency “rate kernels” (K r) to yield the logarithm of the ﬁring rate of
the ﬁrst-layer spikes on a coarse scale. On a ﬁne scale  the logarithm of the ﬁring rate of ﬁrst-
layer spikes is modulated using recurrent interactions  by convolving the local spike history with

2

a

speech waveform

0

d

3

0.81

0.82

0.83

0.84

time (sec)

b

)
z
H

(
 
q
e
r
f
 
r
e

t

n
e
c

e

)
z
H

(
 
q
e
r
f
 
r
e

t

n
e
c

104

103

102
104

0

103

102

0.81

spikegram representation

1

0

c

)
z
H
g
o

l
 

∆
(

−1
−0.5

3

f

time/freq cross−correlation

0

(∆ sec)

0.5

0.82

0.83

0.84

time (sec)

0

0.01

inter spike interval (sec)

0.02

Figure 1: Coarse (top row) and ﬁne (bottom row) scale structure in spikegram encodings of speech.
a. The sound pressure waveform of a spoken sentence and b. the corresponding spikegram. Each
spike (dot) has an associated time (abscissa) and center frequency (ordinate) as well as an amplitude
(dot size). c. Cross-correlation function for a spikegram ensemble reveals correlations across large
time/frequency scales. d. Magniﬁcation of a portion of (a)  with two gammatone kernels (red and
blue)  corresponding to the red and blue spikes in (e). e. Magniﬁcation of corresponding portion of
(b)   revealing that spike timing exhibits strong regularities at a ﬁne scale. f. Histograms of inter-
spike-intervals for two frequency channels corresponding to the colored spikes in (e) reveal strong
temporal dependencies.

a set of “coupling kernels” (K c). The amplitudes of the ﬁrst-layer spikes are also speciﬁed hi-
erarchically: the logarithm of the amplitudes is assumed to be normally distributed  with a mean
speciﬁed by the convolution of second-layer spikes with “amplitude kernels”  (K a not shown) with-
out any recurrent contribution  and the variance ﬁxed at σ2. The model parameters are denoted by

Θ = ³K r  K a  K c ~br ~ba´ where ~br ~ba are the bias vectors corresponding to the log-rate and log-

amplitude of the ﬁrst-layer coefﬁcients  respectively. The model speciﬁes a conditional probability
density over ﬁrst-layer coefﬁcients 

P (S(1)

t f |S(2); Θ) = (1 − p) δ(S(1)

t f ) + pN ³log S(1)

where p = ∆t∆f eRt f

and

Rt f = br

At f = ba

f + (K c ∗ 1
f +Xi h(K a

S(1))t f +Xi h(K r
i ∗ S(2)

i

)t fi

for S(1)

t f ≥ 0  ∀t  f

t f ; At f   σ2´
N ¡x; µ  σ2¢ =
i ∗ S(2)

i

(x−µ)2

2σ2

e−
√2πσ2
)t fi

(2)

(3)

(4)

(5)

In Eq. (2)  δ(.) is the Dirac delta function. In Eq. (3)  ∆t and ∆f are the time and frequency bin
sizes. In Eqs. (4-5)  ∗ denotes convolution and 1x is 1 if x 6= 0  and 0 otherwise.
3 Learning

The joint log-probability of the ﬁrst and second layer can be expressed as a function of the model
parameters Θ and the (unobserved) second-layer spikes S(2):

L(Θ  S(2)) = log P (S(1)  S(2); Θ  λ) = log P (S(1)|S(2); Θ) + log P (S(2); λ)
eRt f ∆t∆f

1

= X(t f )∈S(1)µRt f −
2σ2 ³log S(1)
− log (λ∆t∆f )kS(2)k0 + const

t f − At f´2¶ −Xt f

(6)

(7)

3

Figure 2: Illustration of the hierarchical spike coding model. Second-layer spikes S(2) associ-
ated with 3 features (indicated by color) are sampled in time and frequency according to a Poisson
process  with exponentially-distributed amplitudes (indicated by dot size). These are convolved
with corresponding rate kernels K r (outlined in colored rectangles)  summed together  and passed
through an exponential nonlinearity to drive the instantaneous rate of the ﬁrst-layer spikes on a
coarse scale. The ﬁrst-layer spike rate is also modulated on a ﬁne scale by a recurrent component
that convolves previous spikes with coupling kernels K c. At a given time step (vertical line)  spikes
S(1) are generated according to a Poisson process whose rate depends on the top-down and the
recurrent terms.

where the equality in Eq. (7) holds in the limit ∆t∆f → 0. Maximizing the data likelihood re-
quires integrating L over all possible second-layer representations S(2)  which is computationally
intractable. Instead  we choose to approximate the optimal Θ by maximizing L jointly over Θ and
S(2). If S(2) is known  then the model falls within the well-known class of generalized linear models
(GLMs) [9]  and Eq. (6) is convex in Θ. Conversely  if Θ is known then Eq. (6) is convex in S(2)
except for the L0 penalty term corresponding to the prior on S(2). Motivated by these facts  we
adopt a coordinate-descent approach by alternating between the following steps:

S(2) ← arg max

S(2) L(Θ  S(2))
Θ ← Θ + η∇ΘL(Θ  S(2))

(9)
where η is a ﬁxed learning rate. Section 4 describes a method for approximate inference of the
second-layer spikes (solving Eq. (8)). The gradients used in Eq. (9) are straightforward to compute
and are given by

(8)

(10)

(11)

(12)

(13)

∂L
∂br
f
∂L
∂ba
f
∂L
∂K r

τ ζ i

∂L
∂K c

τ f f ′

1

=

= (# 1′ spikes in channel f ) −Xt
σ2 Xt ³log S(1)
= X(t f )∈S(1)
= Xt∈S(1)

t f − At f´
(t − τ  f − ζ) − Xt f

t−τ f ′ − Xt

eRt f 1

S(2)

i

S(1)

t−τ f ′

1

S(1)

f

4

eRt f ∆t∆f

eRt f S(2)

t−τ f −ζ i∆t∆f

∆t∆f

3.84

)
s
e
v
a
t
c
o
(
 

q
e
r
f

0

0

time (sec)

0.4

center freq = 111Hz

1.34

)
s
e
v
a
t
c
o
(
 

q
e
r
f

0

−1.34

0

time (sec)

0.02

center freq = 246Hz

center freq = 546Hz

center freq = 1214Hz

1.5

0

−1.5

Figure 3: Example model kernels learned on the TIMIT data set. Top: rate kernels (colormaps
individually rescaled). Bottom: Four representative coupling kernels (scaling indicated by colorbar).

4

Inference

Inference of the second-layer spikes S(2) (Eq. (8)) involves maximizing the trade-off between the
GLM likelihood term  which we denote by ˜L(Θ  S(2)) and the last term which penalizes the number
of spikes (kS(2)k0). Solving Eq. (8) exactly is NP-hard. We adopt a variant of the well-known
matching pursuit algorithm [7] to approximate the solution. First  S(2) is initialized to ~0. Then the
following two steps are repeated:

1. Select the coefﬁcient that maximizes a second-order Taylor approximation of ˜L(Θ ·) about

the current solution S(2):

(τ ∗  ζ ∗  i∗) = arg max

τ ζ i −Ã ∂ ˜L

∂S(2)

τ ζ i!2

/

∂2 ˜L
∂S(2)

τ ζ i

2

(14)

2. Perform a line search to determine the step size for this coefﬁcient that maximizes ˜L(Θ ·).
If the maximal improvement does not outweigh the cost − log(λ∆t∆f ) of adding a spike 
terminate. Otherwise update S(2) using this step and repeat Step 1.

5 Results

Model parameters learned from speech

We applied the model to the TIMIT speech corpus [10]. First  we obtained spikegrams by encoding
sounds to 20dB precision using a set of 200 gammatone ﬁlters with center frequencies spaced evenly
on a logarithmic scale (see [5] for details). For each audio sample  this gave us a spikegram with
ﬁne time and frequency resolution (6.25×10−5s and 3.8×10−2 octaves  respectively). We trained
a model with 20 rate and 20 amplitude kernels  with frequency resolution equivalent to that of the
spikegram and time resolution of 20ms. These kernels extended over 400ms×3.8 octaves (spanning
20 time and 100 frequency bins). Coupling kernels were deﬁned independently for each frequency
channel; they extended over 20ms and 2.7 octaves around the channel center frequency with the
same time/frequency resolution as the spikegram. All parameters were initialized randomly  and
were learned according to Eq. (8-9).

Fig. 3 displays the learned rate kernels (top) and coupling kernels (bottom). Among the patterns
learned by the rate kernels are harmonic stacks of different durations and pitch shifts (e.g.  kernels
4  9  11  18)  ramps in frequency (kernels 1  7  15  16)  sharp temporal onsets and offsets (kernels

5

S(2)

aa + r

S(2)

ao + l

+

+

+

≈

+

+

≈

+

+

+

+

≈

+

+

≈

+

+

+

≈

+

+

≈

+

+

+

+

≈

+

+

≈

5

q
e
r
f

0

0

time

0.4

Figure 4: Model representation of phone pairs aa+r (left) and ao+l (right)  as uttered by four speak-
ers (rows: two male  two female). Each row shows inferred second-layer spikes  the rate kernels
most correlated with the utterance of each phone pair  shifted to their corresponding spikes’ fre-
quencies (colored on left)  and the encoded log ﬁring rate centered on the phone pair utterance.

7  13  19)  and acoustic features localized in time and frequency (kernels 5  10  12  20) (example
sounds synthesized by turning on single features are available in supplementary materials). The
corresponding amplitude kernels (not shown) contain patterns highly correlated with the rate kernels 
suggesting a strong dependence in the spikegram between spike rate and magnitude. For most
frequency channels  the coupling kernels are strongly negative at times immediately following the
spike and at adjacent frequencies  representing “refractory periods” observed in the spikegrams.
Positive peaks in the coupling kernels encode precise alignment of spikes across time and frequency.
Second-layer representation

The learned kernels combine in various ways to represent complex acoustic events. For example 
Fig. 4 illustrates how features can combine to represent two different phone pairs. Vowel phones
are approximated by a harmonic stack (outlined in yellow) together with a ramp in frequency (out-
lined in orange and dark blue). Because the rate kernels add to specify the logarithm of the ﬁring
rate  their superposition results in a multiplicative modulation of the intensities at each level of the
harmonic stack. In addition  the ‘r‘ consonant in the ﬁrst example is characterized by a high concen-
tration of energy at the high frequencies and is largely accounted for by the kernel outlined in red.
The ‘l‘ consonant following ‘ao‘ contains a frequency modulation captured by the v-shaped feature
(outlined in cyan).

Translating the kernels in log-frequency allows the same set of fundamental features to participate
in a range of acoustic events: the same vocalizations at different pitch are often represented by the
same set of features. In Fig. 4  the same set of kernels is used in a similar conﬁguration across dif-
ferent speakers and genders. It should be noted that the second-layer representation does not discard
precise time and frequency information (this information is carried in the times and frequencies of
the second-layer spikes). However  the identities of the features that are active remain invariant to
pitch and frequency modulations.
Synthesis

One can further understand the acoustic information that is captured by second-layer spikes by
sampling a spikegram according to the generative model. We took the second-layer encoding of a
single sentence from the TIMIT speech corpus [10] (Fig. 5 middle) and sampled two spikegrams:
one with only the hierarchical component (left)  and one that included both hierarchical and coupling
components (right). At a coarse scale the two samples closely resemble the spikegram of the original
sound. However  at the ﬁne time scale  only the spikegram sampled with coupling contains the
regularities observed in speech data (Fig. 5 bottom row). Sounds were also generated from these
spikegram samples by superimposing gammatone kernels as in [5]. Despite the fact that the second-

6

Second layer (176 spikes)

4
10

3
10

 

)
z
H
g
o
l
(
 

q
e
r
f

2
10

0

3

Hierarchical (2741 spikes)

Data (2544 spikes)

Coupling + Hierarchical (2358 spikes)

3

0.9

 

)
z
H
g
o
l
(
 

q
e
r
f

 

)
z
H
g
o
l
(
 
q
e
r
f

4
10

3
10

2
10

0

4
10

3
10

2
10

0.8

time

Figure 5: Synthesis from inferred second-layer spikes. Middle bottom: spikegram representation
of the sentence in Fig. 1; Middle top: Inferred second-layer representation; Left: ﬁrst-layer spikes
generated using only the hierarchical model component; Right: ﬁrst-layer spikes generated using
hierarchical and coupling kernels. Synthesized waveforms are included in the supplementary mate-
rials.

white noise

noise level Wiener wav thr

-10dB
-5dB
0dB
5dB
10dB

-7.00
0.00
5.49
7.84
10.31

2.41
4.93
7.94
11.15
14.64

MP
2.26
4.79
7.71
11.01
14.49

HSC
2.50
5.01
7.99
11.33
14.83

-10dB
-5dB
0dB
5dB
10dB

sparse temporally modulated noise
Wiener
HSC
-4.37
-8.68
-0.38
-3.09
3.30
1.90
7.40
6.37
11.88
9.68

wav thr
-8.73
-3.63
1.23
6.06
11.28

MP
-5.12
-0.96
2.97
7.11
11.58

Table 1: Denoising accuracy (dB SNR) for speech corrupted with white noise (left) or with sparse 
temporally modulated noise (right).

layer representation contains over 15 times fewer spikes as the ﬁrst-layer spikegrams  the synthesized
sounds are intelligible and the addition of the coupling ﬁlters provides a noticeable improvement
(audio examples in supplementary materials).
Denoising

Although the model parameters have been adapted to the data ensemble  obtaining an estimate of the
likelihood of the data ensemble under the model is difﬁcult  as it requires integrating over unobserved
variables (S(2)). Instead  we can use performance on unsupervised signal processing tasks  such
as denoising  to validate the model and compare it to other methods that explicitly or implicitly
represent data density. In the noiseless case  a spikegram is obtained by running matching pursuit
until the decrease in the residual falls below a threshold; in the presence of noise  this encoding
process can be formulated as a denoising operation  terminated when the improvement in the log-
likelihood (variance of the residual divided by the variance of the noise) is less than the cost of
adding a spike (the negative log-probability of spiking). We incorporate the HSC model directly
into this denoising algorithm by replacing the ﬁxed probability of spiking at the ﬁrst layer with the

7

rate speciﬁed by the second layer. Since neither the ﬁrst- nor second-layer spike code for the noisy
signal is known  we ﬁrst infer the ﬁrst and then the second layer using MAP estimation  and then
recompute the ﬁrst layer given both the data and second layer. The denoised waveform is obtained
by reconstructing from the resulting ﬁrst-layer spikes.

To the extent that the parameters learned by HSC reﬂect statistical properties of the signal  incorpo-
rating the more sophisticated spikegram prior into a denoising algorithm should allow us to better
distinguish signal from noise. We tested this by denoising speech waveforms (held out during model
training) that have been corrupted by additive white Gaussian noise. We compared the model’s per-
formance to that of the matching pursuit encoding (sparse signal representation without a hierarchi-
cal model)  as well as to two standard denoising methods  Wiener ﬁltering and wavelet-threshold
denoising (implemented with MATLAB’s wden function  using symlets  SURE estimator for soft
threshold selection; other parameters optimized for performance on the training data set) [11].

HSC-based denoising is able to outperform standard methods  as well as matching pursuit denoising
(Table 1 left). Although the performance gains are modest  the fact that the HSC model  which is not
optimized for the task or trained on noisy data  can match the performance of adaptive algorithms
like wavelet ﬁltering denoising suggests that it has learned a representation that successfully exploits
the statistical regularities present in the data.

To test more rigorously the beneﬁt of a structured prior  we evaluated denoising performance on
signals corrupted with non-stationary noise whose power is correlated over time. This is a more
challenging task  but it is also more relevant to real-world applications  where sources of noise are
often non-stationary. Algorithms that incorporate speciﬁc (but often incorrect) noise models (e.g. 
Wiener ﬁltering) tend to perform poorly in this setting. We generated sparse temporally modulated
noise by scaling white Gaussian noise with a temporally smooth envelope (given as a convolution of
a Gaussian function with st. dev. of 0.02s with a Poisson process with rate 16s−1). All methods fare
worse on this task. Again  the hierarchical model outperforms other methods (Table 1 right)  but
here the improvement in performance is larger  especially at high noise levels where the model prior
plays a greater role. The reconstruction SNR does not fully convey the manner in which different
algorithms handle noise: perceptually  we ﬁnd that the sounds denoised by the hierarchical model
sound more similar to the original (audio examples in supplementary materials).

6 Discussion

We developed a hierarchical spike code model that captures complex structure in sounds. Our
work builds on the spikegram representation of [5]  thus avoiding the limitations arising from
spectrogram-based methods  and makes a number of novel contributions. Unlike previous work
[3  4]  the learned kernels are shiftable in both time and log-frequency  which enables the model to
learn time- and frequency-relative patterns and use a small number of kernels efﬁciently to represent
a wide variety of sound features. In addition  the model describes acoustic structure on multiple
scales (via a hierarchical component and a recurrent component)  which capture fundamentally dif-
ferent kinds of statistical regularities.

Technical contributions of ths work include methods for learning and performing approximate in-
ference in a generalized linear model in which some of the inputs are unobserved and sparse (in
this case the second-layer spikes). The computational framework developed here is general  and
may have other applications in modeling sparse data with partially observed variables. Because the
model is nonlinear  multi-layer cascades could lead to substantially more powerful models.

Applying the model to complex natural sounds (speech)  we demonstrated that it can learn non-
trivial features  and we have shown how these features can be composed to form basic acoustic
units. We also showed a simple application to denoising  demonstrating improved performance to
wavelet thresholding. The framework provides a general methodology for learning higher-order
features of sounds  and we expect that it will prove useful in representing other structured sounds
such as music  animal vocalizations  or ambient natural sounds.

6.1 Acknowledgments

We thank Richard Turner and Josh McDermott for helpful discussions.

8

References
[1] C. Fevotte  B. Torresani  L. Daudet  and S. Godsill  “Sparse linear regression with structured priors and
application to denoising of musical audio ” Audio  Speech  and Language Processing  IEEE Transactions
on  vol. 16  pp. 174 –185  jan. 2008.

[2] M. Plumbley  T. Blumensath  L. Daudet  R. Gribonval  and M. Davies  “Sparse representations in audio
and music: From coding to source separation ” Proceedings of the IEEE  vol. 98  pp. 995 –1005  june
2010.

[3] D. J. Klein  P. K¨onig  and K. P. K¨ording  “Sparse spectrotemporal coding of sounds ” EURASIP J. Appl.

Signal Process.  vol. 2003  pp. 659–667  Jan. 2003.

[4] H. Lee  Y. Largman  P. Pham  and A. Y. Ng  “Unsupervised feature learning for audio classiﬁcation using
convolutional deep belief networks ” in Advances in Neural Information Processing Systems  pp. 1096–
1104  The MIT Press  2009.

[5] E. Smith and M. S. Lewicki  “Efﬁcient coding of time-relative structure using spikes ” Neural Computa-

tion  vol. 17  no. 1  pp. 19–45  2005.

[6] M. Lewicki and T. Sejnowski  “Coding time-varying signals using sparse  shift-invariant representations ”

in Advances in Neural Information Processing Systems  pp. 730–736  The MIT Press  1999.

[7] S. Mallat and Z. Zhang  “Matching pursuits with time-frequency dictionaries ” IEEE Trans Sig Proc 

vol. 41  pp. 3397–3415  December 1993.

[8] E. Smith and M. S. Lewicki  “Efﬁcient auditory coding ” Nature  vol. 439  no. 7079  2006.
[9] P. McCullagh and J. A. Nelder  Generalized linear models (Second edition). London: Chapman & Hall 

1989.

[10] J. S. Garofolo  L. F. Lamel  W. M. Fisher  J. G. Fiscus  D. S. Pallett  and N. L. Dahlgren  “Darpa timit

acoustic phonetic continuous speech corpus cdrom ” 1993.

[11] S. Mallat  A Wavelet Tour of Signal Processing  Third Edition: The Sparse Way. Academic Press  3rd ed. 

2008.

9

,Qianli Liao
Joel Leibo
Tomaso Poggio
Alan Kuhnle