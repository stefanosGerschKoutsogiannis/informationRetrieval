2019,A Game Theoretic Approach to Class-wise Selective Rationalization,Selection of input features such as relevant pieces of text has become a common technique of highlighting how complex neural predictors operate. The selection can be optimized post-hoc for trained models or incorporated directly into the method itself (self-explaining). However  an overall selection does not properly capture the multi-faceted nature of useful rationales such as pros and cons for decisions. To this end  we propose a new game theoretic approach to class-dependent rationalization  where the method is specifically trained to highlight evidence supporting alternative conclusions. Each class involves three players set up competitively to find evidence for factual and counterfactual scenarios. We show theoretically in a simplified scenario how the game drives the solution towards meaningful class-dependent rationales. We evaluate the method in single- and multi-aspect sentiment classification tasks and demonstrate that the proposed method is able to identify both factual (justifying the ground truth label) and counterfactual (countering the ground truth label) rationales consistent with human rationalization.  The code for our method is publicly available.,A Game Theoretic Approach to Class-wise Selective

Rationalization

Shiyu Chang1 2∗

Yang Zhang1 2∗

1MIT-IBM Watson AI Lab

{shiyu.chang yang.zhang2}@ibm.com yum@us.ibm.com

Mo Yu2∗
2IBM Research

Tommi S. Jaakkola3
3CSAIL MIT
tommi@csail.mit.edu

Abstract

Selection of input features such as relevant pieces of text has become a common
technique of highlighting how complex neural predictors operate. The selection can
be optimized post-hoc for trained models or incorporated directly into the method
itself (self-explaining). However  an overall selection does not properly capture the
multi-faceted nature of useful rationales such as pros and cons for decisions. To this
end  we propose a new game theoretic approach to class-dependent rationalization 
where the method is speciﬁcally trained to highlight evidence supporting alternative
conclusions. Each class involves three players set up competitively to ﬁnd evidence
for factual and counterfactual scenarios. We show theoretically in a simpliﬁed
scenario how the game drives the solution towards meaningful class-dependent
rationales. We evaluate the method in single- and multi-aspect sentiment classiﬁca-
tion tasks and demonstrate that the proposed method is able to identify both factual
(justifying the ground truth label) and counterfactual (countering the ground truth
label) rationales consistent with human rationalization. The code for our method is
publicly available2.

1

Introduction

Interpretability is rapidly rising alongside performance as a key operational characteristics across
NLP and other applications. Perhaps the most straightforward means of highlighting how a complex
method works is by selecting input features relevant for the prediction (e.g.  [19]). If the selected
subset is short and concise (for text)  it can potentially be understood and veriﬁed against domain
knowledge. The selection of features can be optimized to explain already trained models [24] 
incorporated directly into the method itself as in self-explaining models [19  12]  or optimized to
mimic available human rationales [8].
One of the key questions motivating our work is extending how rationales are deﬁned and estimated.
The common paradigm to date is to make an overall selection of a feature subset that maximally
explains the target output/decision. For example  maximum mutual information criterion [12  19]
chooses an overall subset of features such that the mutual information between the feature subset and
the target output decision is maximized  or  equivalently  the entropy of the target output decision
conditional on this subset is minimized. Rationales can be multi-faceted  however  involving support
for different outcomes  just with different degrees. For example  we could understand the overall
sentiment associated with a product in terms of weighing associated pros and cons contained in the
review. Existing rationalization techniques strive for a single overall selection  therefore lumping
together the facets supporting different outcomes.
We propose the notion of class-wise rationales  which is deﬁned as multiple sets of rationales
that respectively explain support for different output classes (or decisions). Unlike conventional

∗Authors contributed equally to this paper.
2https://github.com/code-terminator/classwise_rationale

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

rationalization schemes  class-wise rationalization takes a candidate outcome as input  which can
be different from the ground-truth class labels  and uncovers rationales speciﬁcally for the given
class. To ﬁnd such rationales  we introduce a game theoretic algorithm  called Class-wise Adversarial
Rationalization (CAR). CAR consists of three types of players: factual rationale generators  which
generate rationales that are consistent with the actual label  counterfactual rationale generators  which
generate rationales that counter the actual label  and discriminators  which discriminate between
factual and counterfactual rationales. Both factual and counterfactual rationale generators try to
competitively “convince” the discriminator that they are factual  resulting in an adversarial game
between the counterfactual generators and the other two types of players.
We will show in a simpliﬁed scenario how CAR game drives towards meaningful class-wise rational-
ization  under an information-theoretic metric  which is a class-wise generalization of the maximum
mutual information criterion. Moreover  empirical evaluation on both single- and multi-aspect senti-
ment classiﬁcation show that CAR can successfully ﬁnd class-wise rationales that align well with
human understanding. The data and code will become publicly available.

2 Related Work

There are two lines of research on generating interpretable features of neural network. The ﬁrst is
to directly incorporate the interpretations into the models  a.k.a self-explaining models [3  4  5  15].
The other line is to generate interpretations in a post-hoc manner. There are several ways to perform
post-hoc interpretations. The ﬁrst class of method is to explicitly introduce a generator that learns
to select important subsets of inputs as explanations [12  19  21  30  31]  which often comes with
some information-theoretic properties. The second class is to evaluate the importance of each input
feature via backpropagation of the prediction. Many of these methods utilize gradient information [6 
20  25  26  27  28]  while techniques like local perturbations [11  13  16  22] and Parzen window [7]
have also been used to loose the requirement of differentiability. Finally  the third class is locally
ﬁtting a deep network with interpretable models  such as linear models [2  24]. There are also some
recent works trying to improve the ﬁdelity and/or stability of post hoc explanations by including the
explanation mechanism in the training procedure [17  18].
Although none of the aforementioned approaches can perform class-wise rationalization  gradient-
based methods can be intuitively adapted for this purpose  which produces explanations toward a
certain class by probing the importance with respect to the corresponding class logit. However  as
noted in [24]  when the input feature is far away from the corresponding class  the local gradient or
perturbation probe can be very inaccurate. Evaluation of such methods will be provided in section 5.

3 Class-wise Rationalization

In this section  we will introduce our adversarial approach to class-wise rationalization. For notations 
upper-cased letters  e.g. X or X  denote random variables or random vectors respectively; lower-cased
letters  e.g. x or x  denote deterministic scalars or vectors respectively; script letters  e.g. X   denote
sets. pX|Y (x|y) denotes the probability of X = x conditional on Y = y. E[X] denotes expectation.

3.1 Problem Formulation

Consider a text classiﬁcation problem  where X is a random vector representing a string of text  and
Y ∈ Y represents the class that X is in. The class-wise rationalization problem can be formulated as
follows. For any input X  our goal is to derive a class-wise rationale Z(t) for any t ∈ Y such that
Z(t) provides evidence supporting class t. Each rationale can be understood as a masked version
X  i.e. X with a subset of its words masked away by a special value (e.g. 0). Note that class-wise
rationales are deﬁned for every class t ∈ Y. For t = Y (the correct class) the corresponding rationale
is called factual; for t (cid:54)= Y we call them counterfactual rationales. For simplicity  we will focus on
two-class classiﬁcation problems (Y = {0  1}) for the remainder of this section. Generalization to
multiple classes will be discussed in appendix A.4.
As a clariﬁcation  notice that during inference  the class t that is provided to the system does not
need to be the ground truth. No matter what t is provided  factual or counterfactual  the algorithm is
supposed to try its best to ﬁnd evidence in support of t. Therefore  the inference does not need to

2

(a)

(b)

Figure 1: CAR training and inference procedures of the class-0 case. (a) The training procedure. (b) During
inference  there is no ground truth label. In this case  we will always trigger the factual generators.

access the ground truth label. However  the training of the algorithm requires the ground truth label
Y   because it needs to learn the phrases and sentences that are informative of each class.

3.2 The CAR Framework

CAR uncovers class-wise rationales using adversarial learning  inspired by outlining pros and cons for
t (X)  t ∈ {0  1}  which generate
decisions. Speciﬁcally  there are two factual rationale generators  gf
rationales that justify class t when the actual label agrees with t  and two counterfactual rationale
t (X)  t ∈ {0  1}  which generate rationales for the label other than the ground truth.
generators  gc
Finally  we introduce two discriminators dt(Z)  t ∈ {0  1}  which aim to discriminate between factual
and counterfactual rationales  i.e.  between gf
t (X). We thus have six players  divided into
two groups. The ﬁrst group pertains to t = 0 and involves gf
0(X) and d0(Z) as players. Both
groups play a similar adversarial game  so we focus the discussion on the ﬁrst group.
Discriminator: In our adversarial game  d0(·) takes a rationale Z generated by either gf
as input  and outputs the probability that Z is generated by the factual generator gf
target for d0(·) is similar to the generative adversarial network (GAN) [14]:
0 (X))|Y = 0] − pY (1)E[log(1 − d(gc

0(·)
0 (·). The training

−pY (0)E[log d(gf

0(X)))|Y = 1].

d0(·) = argmin

t (X) and gc

0 (·) or gc

0 (X)  gc

(1)

d(·)

Generators: The factual generator gf
The counterfactual generator gc
convince the discriminator that they are factual generators for Y = 0.
0(·) = argmax

E[h0(d0(g(X)))|Y = 0] 

0 (·) = argmax
gf

and gc

0 (·) is trained to generate rationales from text labeled Y = 0.
0(·)  in contrast  learns from text labeled Y = 1. Both generators try to

E[h1(d0(g(X)))|Y = 1] 

(2)

g(·)

g(·)

s.t. gf

0 (X)) and gc

0(X) satisfy some sparsity and continuity constraints.

The constraints stipulate that the words selected as rationales should be a relatively small subset of
the entire text (sparse) and they should constitute consecutive segments (continuous). We will keep
the constraints abstract for generality for now. The actual form of the constraints will be speciﬁed
in section 4. h0(·) and h1(·) are both monotonically-increasing functions that satisfy the following
properties:

(cid:18) x

(cid:19)

xh0

x + a

(cid:18) a

(cid:19)

x + a

is convex in x 

and xh1

is concave in x 

∀x  a ∈ [0  1].

(3)

One valid choice is h0(x) = log(x) and h1(x) = − log(1 − x)  which reduces the problem to the more
canonical GAN-style problem. In practice  we ﬁnd that other functional forms have more stable
training behavior. As shown later  this generalization is closely related to f-divergence.
0(·) plays an
Figure 1(a) summarizes the training procedure of these three players. As can be seen  gc
0 (·)  because it tries to trick d0(·) into misclassifying its output
adversarial game with both d0(·) and gf
1 (·) 
as factual  whereas gf
1(·) and d1(·)  play a similar game. The only difference is that now the factual generator operates on
gc
text with label Y = 1  and the counterfactual generator on text with label Y = 0.

0 (·) helps d0(·) make the correct decision. The other group of players  gf

3

𝒙~𝑝𝒙𝑦=0)𝒙′~𝑝𝒙𝑦=1)𝒈+ (⋅)𝑑+(⋅ ⋅)factual / counter𝒈+1(⋅)𝒛+ 𝒛+1𝒛"(⋅)𝒙𝒈"((⋅)𝒛)(⋅)𝒙𝒈)((⋅)3.3 How Does It Work?

0 and Z c

0 are also multivariate binary vectors. Z f

Consider a simple bag-of-word scenario  where the input text is regarded as a collection of words
drawn from a vocabulary of size N. In this case  X can be formulated as an N-dimensional binary
vector. Xi = 1  if the i-th word is present  and Xi = 0 otherwise. pX|Y (x|y) represents the probability
distribution of X in natural text conditional on different classes Y = y.
The rationales Z f
0 i = 1 if the i-th word is selected
0 |Y (z|0) denotes the induced distribution
as part of the factual rationale  and Z f
of the factual rationales  which is only well-deﬁned in the factual case (Y = 0). This distribution is
0 (·) generates the rationales across examples. In the optimization problem  we
determined by how gf
will primarily make use of the induced distribution  and similarly for the counterfactual rationales.
To simplify our discussion  we assume that the dimensions of X are independent conditional on Y .
Furthermore  we assume that the rationale selection scheme selects each word independently  so
the induced distributions over Z f
0 are also independent across dimensions  conditional on Y .
Formally  ∀x  z ∈ {0  1}N  ∀y ∈ {0  1} 

0 i = 0 otherwise. pZf

0 and Z c

0|Y (z|y) =

0 |Y (z|y) =

pXi|Y (xi|y)  pZf

0 i|Y (zi|y)  pZc
pZf

pX|Y (x|y) =
(4)
Figure 2(left) plots pXi|Y (1|0) and pXi|Y (1|1) as functions of i (the horizontal axis corresponds to
sorted word identities). These two curves represent the occurrence of each word in the two classes.
In the ﬁgure  the words to the left satisfy pXi|Y (1|0) > pXi|Y (1|1)  i.e. they occur more often in
class 0 than in class 1. These words are most indicative of class 0  which we will call class-0 words.
Similarly  the words to the right are called class-1 words.
0 i|Y (1|1) curves (solid  shaded curves) 
Figure 2(left) also plots an example of pZf
which represents the occurrence of each word in the factual and counterfactual rationales respectively.
Note that these two curves must satisfy the following constraints:

0 i|Y (1|0) and pZc

0 i|Y (zi|y).

pZc

N(cid:89)

i=1

N(cid:89)

i=1

N(cid:89)

i=1

0 i|Y (1|0) ≤ pXi|Y (1|0) 
pZf

and pZc

0 i|Y (1|1) ≤ pXi|Y (1|1).

(5)

0 i|Y (1|0) and pZc

This is because a word can be chosen as a rationale only if it appears in a text  and this strict
relation translates into an inequality constraint in terms of the induced distributions. As shown in
0 i|Y (1|1) curves are always below the pXi|Y (1|0) and pXi|Y (1|1)
ﬁgure 2(left)  the pZf
curves respectively. For the remainder of this section  we will refer to pXi|Y (1|0) as the factual
upper-bound  and pXi|Y (1|1) as the counterfactual upper-bound. What we intend to show is that the
optimal strategy for both rationale generators in this adversarial game is to choose the class-0 words.
The optimal strategy for the counterfactual generator: We will ﬁrst ﬁnd out what is the optimal
0 i|Y (1|1) curve  given an
strategy for the counterfactual generator  or  equivalently  the optimal pZc
0 i|Y (1|1) curve. The goal of the counterfactual generator is to fool the discriminator.
arbitrary pZf
Therefore  its optimal strategy is to match the the counterfactual rationale distribution with the factual
0 i|Y (1|1) (blue) curve tries to overlay
rationale distribution. As shown in ﬁgure 2(middle)  the pZc
0 i|Y (1|1) (green) curve  within the limits of the counterfactual upper bound constraint.
with the pZf
The optimal strategy for the factual generator: The goal of the factual generator is to help the
discriminator. Therefore  its optimal strategy  given the optimized counterfactual generator  is to
“steer” the factual rationale distribution away from the counterfactual rationale distribution. Recall
that the counterfactual rationale distribution always tries to match the factual rationale distribution 
unless its upper-bound is binding. The factual generator will therefore choose the words whose factual
upper-bound is much higher than the counterfactual upper-bound. These words are  by deﬁnition 
most indicative of class 0. The counterfactual generator will also favor the same set of words  due to
its incentive to match the distributions. Figure 2(right) illustrates the optimal strategy for the factual
rationale under sparsity constraint

N(cid:88)

N(cid:88)

E[Z f

0 i] =

0 i|Y (1|1) ≤ α.
pZf

(6)

i=1

i=1

The left-hand side in equation (6) represents the expected factual rationale length (in number of
0 i|Y (1|1) curve (the green shaded areas in ﬁgure 2).
words). It also represents the area under the pZf

4

Figure 2: An illustration of how CAR works in the bag-of-word scenario with independence assumption
(equation (4)). Left: example probability of occurrence of each word in the rationales from each class (solid
lines)  upper bounded by the probability of occurrence of each word in the natural text from each class (dashed
lines). Middle: the optimal strategy for the counterfactual rationale is to match the factual rationale distribution 
unless prohibited by the upper-bound. Right: the optimal strategy for the factual rationale is to steer away from
the counterfactual rationale distribution  leveraging the upper-bound difference.

3.4

Information-theoretic Analysis

Now we are ready to embark on a more formal analysis of the effectiveness of the CAR framework 
as stated in the following theorem.
Theorem 1. In the bag-of-word scenario with the independence assumption as in equation (4):
(1) Given the optimal d0(·) and an arbitrary gf
the counterfactual rationales that follow the following distribution:

0(·) to equation (2) (left) will generate

0 (·)  the optimal gc
(cid:110)

0 i|Y (1|1) = min

pZc

0 i|Y (1|0)  pXi|Y (1|1)
pZf

(cid:111)

.

(7)

(2) Under some additional assumptions (see appendix A.1)  given the optimal d0(·) and the optimal
0(·)  the optimal gf
0 (·) to equation (2) (right) subject to the sparsity constraint as in equation (6) is
gc
given by Z f

0 i = XI∗  where
EX∼pX|Y (·|0)

(cid:20)

(cid:18) pXI|Y (XI|0)

(cid:19)(cid:21)

I

h

I∗

= argmax

s.t.
where XI denotes a subvector of X containing Xi  ∀i ∈ I.
The proof will be given in the appendix. To better understand equation (8)  it is useful to ﬁrst write
down the mutual information between XI and Y   a similar quantity to which has been applied to the
maximum mutual information criterion [12  19].

pXI (XI)

pXi|Y (1|0) > pXi|Y (1|1) ∀i ∈ I 

(8)

 

I(Y ; XI) = EX Y ∼pX Y (· ·)

log

=

pY (y)EX∼pX|Y (·|y)

log

(9)
As can be seen  there is a correspondence between equations (8) and (9). First  the log(·) function
in equation (9) is generalized a wider selection of functional forms  h(·). As will be shown in
the appendix A.2  equation (8) applies the f-divergence [1]  which is a generalization to the KL-
divergence as applied in equation (9). Second  notice that equation (9) is decomposed into two
class-dependent terms  while equation (8) is for class-0 generators only. It can be easily shown that
the class-1 generators come with a similar theoretical guarantee that corresponds to the term with
y = 1. Therefore  the target function in equation (8) can be considered as the component in the mutual
information that is speciﬁcally related to class 0. Hence we call it class-wise mutual information.

(cid:20)

(cid:18) pXI|Y (XI|Y )

(cid:19)(cid:21)

pXI (XI)

1(cid:88)

y=0

(cid:20)

(cid:18) pXI|Y (XI|y)

(cid:19)(cid:21)

pXI (XI)

.

3.5 Coping with Degeneration

It has been pointed out in [32] that the existing generator-predictor framework in [12] and [19]
can suffer from the problem of degeneration. Since the generator-predictor framework aims to
maximize the predictive accuracy of the predictor  the generator and predictor can collude by
selecting uninformative symbols to encode the class information  instead of selecting words and
phrases that truly explain the class. For example  consider the following punctuation communication
scheme: when Y = 0  the rationale would select only one comma “ ”; when Y = 1  the rationale
would select only one period “.”. This rationalization scheme guarantees a high predictive accuracy.
However  this is apparently not what we expect. Such cases are called degeneration.

5

!Prob. Class 0 wordsClass 1 wordsProb. !Prob. !!"|$1|0!"|$1|1!'()|$1|0!'(*|$1|1From section 3.3  we can conclude that CAR will not suffer from degeneration. This is because if
the factual rationale generators attempt to select uninformative words or symbols like punctuation
(i.e. words in the middle of the x-axis in ﬁgure 2)  then the factual rationale distribution can be easily
matched by the counterfactual rationale distribution. Therefore  this strategy is not optimal for the
factual generators  whose goal is to avoid being matched by the counterfactual generators.

4 Architecture Design and Implementation

1(·) and gf

0(·) and gf

0 (·) = g0(·  0)  and gc

Architecture with parameter sharing: In our actual implementation  we impose parameter sharing
among the players. This is motivated by our observation in sections 3.3 and 3.4 that both the
factual and counterfactual generators adopt the same rationalization strategy upon reaching the
equilibrium. Therefore  instead of having two separate networks for the two generators  we introduce
one uniﬁed generator network for each class  a class-0 generator and a class-1 generator  with the
ground truth label Y as an additional input to identify between factual and counterfactual modes.
0 (·) now share the same parameters in a single generator network g0(·  Y ) 
Speciﬁcally  gc
0(·) = g0(·  1). Please note that after the parameter sharing  g0(·  0) and
where gf
g0(·  1) are still considered as two distinct players  in the sense that they are still trained to optimize
different target functions (equation (2))  and they still play the same adversarial game with each other.
1 (·) share the same parameters in a single generator network g1(·  Y ). We also
Similarly  gc
impose parameter sharing between the two discriminators  d0(·) and d1(·)  by introducing a uniﬁed
discriminator  d(·  t)  with an additional input t to identify between the class-0 and class-1 cases. The
trainable parameters are signiﬁcantly reduced with parameter sharing.
Both the generators and the discriminators consist of a word embedding layer  a bi-direction LSTM
layer followed by a linear projection layer. The generators generate the rationales by the independent
selection process as proposed in [19]. At each word position k  the convolutional layer outputs a
quantized binary mask Sk  which equals to 1 if the k-th word is selected and 0 otherwise. The binary
masks are multiplied with the corresponding words to produce the rationales. For the discriminators 
the outputs of all the times are max-pooled to produce the factual/counterfactual decision.
For parameter sharing  we append the input class as a one-hot vector to each word embedding vector
in both the generators and the discriminator. For the generators  the groundtruth class label Y of each
instance is appended; while for the discriminator  the class of generator t used for generating the
input rationale is appended.
Training: The training objectives are essentially equations (1) and (2). The only difference is that
we instantiate the constraints in equation (2) transform it into a multiplier form. Speciﬁcally  the
multiplier terms (or the regularization terms) are

|Sk − Sk−1|

 

(10)

(cid:12)(cid:12)(cid:12)(cid:12) 1

K

λ1

E[(cid:107)S(cid:107)1] − α

(cid:12)(cid:12)(cid:12)(cid:12) + λ2E

(cid:20) K(cid:88)

t=2

(cid:21)

where K denotes the number of words in the input text. The ﬁrst term constrains on the sparsity
of the rationale. It encourages that the percentage of the words being selected as rationales is
close to a preset level α. The second term constrains on the continuity of the rationale. λ1  λ2 and
α are hyperparameters. The constraint is slightly different from the one in [19] in order have a
more precise control of the sparsity level. The h0(·) and h1(·) functions in equation (2) are set to
h0(x) = h1(x) = x  which empirically shows good convergence performance  and which can be
shown to satisfy equation (3). To resolve the non-differentiable quantization operation that produces
St  we apply the straight-through gradient computation technique [9]. The training scheme involves
the following alternate stochastic gradient descent. First  the class-0 generator and the discriminator
are updated jointly by passing one batch of data into the class-0 generator  and the resulting rationales 
which contain both factual and counterfatual rationales depending on the actual class  are fed into the
discriminator with t = 0. Then  the class-1 generator and the discriminator are updated jointly in a
similar fashion with t = 1.
Inference: During the inference  the ground truth label is unavailable for fair comparisons with the
baselines  therefore we have no oracle knowledge of which class is factual and which is counterfactual.
In this case  we always trigger the factual generators  no matter what the ground truth is  as shown
in ﬁgure 1(b). This is again justiﬁed by our observation in sections 3.3 and 3.4 that both the factual
and counterfactual modes adopt the same rationalization strategy upon reaching the equilibrium. The

6

only reason why we favor the factual mode to the counterfactual mode is that the former has more
exposure to the words it is supposed to select during training.

5 Experiments

5.1 Datasets

To evaluate both factual and counterfactual rationale generation  we consider the following three
binary classiﬁcation datasets. The ﬁrst one is the single-aspect Amazon reviews [10] (book and
electronic domains)  where the input texts often contain evidence for both positive and negative
sentiments. We use predeﬁned rules to parse reviews containing comments on both the pros and cons
of a product  which is further used for automatic evaluations. We also evaluate algorithms on the multi-
aspect beer [23] and hotel reviews [29] that are commonly used in the ﬁeld of rationalization [8  19].
The labels of the beer review dataset are binarized  resulting in a harder rationalization task than in
[19]. The multi-aspect review is considered as a more challenging task  where each review contains
comments on different aspects. However  unlike the Amazon dataset  both beer and hotel datasets
only contain factual annotations. The construction of evaluation tasks is detailed in appendix B.1.

5.2 Baselines

RNP: A generator-predictor framework proposed by Lei et al. [19] for rationalizing neural prediction
(RNP). The generator selects text spans as rationales which are then fed to the predictor for label
classiﬁcation. The selection maximizes the predictive accuracy of the target output and is constrained
to be sparse and continuous. RNP is only able to generate factual rationales.
POST-EXP: The post-explanation method generates rationales of both positive and negative classes
based on a pre-trained predictor. Given the predictor trained on full-text inputs  we train two separate
generators g0(X) and g1(X) on the data to be explained. g0(X) always generate rationales for the
negative class and g1(X) always generate rationales for the positive class. The two generators are
trained to maximize the respective logits of the ﬁxed predictor subject to sparsity and continuity
regularizations  which is closely related to gradient-based explanations [20].
To seek fair comparisons  the predictors of both RNP and POST-EXP and the discriminator of CAR
are of the same architecture; the rationale generators in all three methods are of the same architecture.
The hidden state size of all LSTMs is set to 100. In addition  the sparsity and continuity constraints
are also in the same form as our method. It is important pointing out that CAR does not use any
ground truth label for generating rationales  which follows the procedures discussed in section 4.

5.3 Experiment Settings

Objective evaluation: We compare the generated rationales with the human annotations and report
the precision  recall and F1 score. To be consistent with previous studies [19]  we evaluate different
algorithms conditioned on a similar actual sparsity level in factual rationales. Speciﬁcally  the target
factual sparsity level is set to around (±2%) 20% for the Amazon dataset and 10% for both beer and
hotel review. The reported performances are based on the best performance of a set of hyperparameter
values. For details of the setting  please refer to appendix B.2.
Subjective evaluation: We also conduct subjective evaluations via Amazon Mechanical Turk. Specif-
ically  we reserve 100 randomly balanced examples from each dev set for the subjective evaluations.
For the single-aspect dataset  the subject is presented with either the factual rationale or the counter-
factual rationale of a text generated by one of the three methods (unselected words blocked). For the
factual rationales  a success is credited when the subject correctly guess the ground-truth sentiment;
for the counterfactual rationales  a success is credited when the subject is convinced to choose the
opposite sentiment to the ground-truth. For the multi-aspect datasets  we introduce a much harder
test. In addition to guessing the sentiment  the subject is also asked to guess what aspect the rationale
is about. A success is credited only when both the intended sentiment and the correct aspect are
chosen. Under this criterion  a generator that picks the sentiment words only will score poorly. We
then compute the success rate as the performance metric. The test cases are randomly shufﬂed.
The subjects have to meet certain English proﬁciency and are reminded that some of the generated

7

Table 1: Objective performances of selected rationales of the Amazon review dataset. The numbers in each
column represent the sparsity level  precision  recall  and F1 score  respectively. Each domain is trained
independently. All results are calculated in a “micro” perspective.

Amazon

RNP [19]
POST-EXP
CAR

Book

Factual

Counterfactual

Factual

Electronic

Counterfactual

18.6/55.1/20.1/29.5
20.2/64.5/28.8/39.8 27.9/70.2/35.8/47.4
20.9/68.7/31.9/43.6 15.2/72.2/20.2/31.5

-

20.7/49.7/22.8/31.3
18.6/64.1/27.8/38.8 15.3/72.6/19.5/30.7
21.2/70.0/34.7/46.4
10.2/76.4/13.6/23.1

-

Table 2: Objective performances of selected factual rationales for both (a) beer and (b) hotel review datasets.
Each aspect is trained independently. S  P  R  and F1 indicate the sparsity level  precision  recall  and F1 score.

(a)

(b)

Beer

RNP [19]
POST-EXP
CAR

Hotel

RNP [19]
POST-EXP
CAR

S
11.9
11.9
11.9

S
10.9
8.9
10.6

Appearance
R
P
46.1
72.0
64.2
41.4
49.3
76.2

Location
R
P
43.3
55.5
31.8
30.4
46.6
58.1

F1
56.2
50.4
59.9

F1
48.6
31.1
51.7

S
10.7
10.3
10.3

S
11.0
10.0
11.7

Aroma
P
R
48.3
70.5
50.0
33.1
33.3
50.3

Service
P
R
38.2
40.0
28.3
32.5
40.7
41.4

F1
57.3
39.8
40.1

F1
39.1
30.3
41.1

S
10.0
10.0
10.2

S
10.6
9.2
9.9

Palate

P
53.1
33.0
56.6

R
42.8
26.5
46.2

Cleanliness
R
P
36.0
30.5
23.7
23.0
32.3
35.7

F1
47.5
29.4
50.9

F1
33.0
23.3
33.9

rationales are intended to trick them via word selections and masking (e.g. masking the negation
words). Appendix B.2 contains a screenshot and the details of the online evaluation setups.

5.4 Results

Table 1 shows the objection evaluation results for both factual and counterfactual rationales on
Amazon reviews. Constrained to highlighting 20% of the inputs  CAR consistently surpasses the other
two baselines in the factual case for both domains. Compared to the POST-EXP  our method generates
the counterfactual rationales with higher precision. However  since the sparsity constraint regularizes
both factual and counterfactual generations and the model selection is conducted on factual sparsity
only  we cannot control counterfactual sparsity among different algorithms. POST-EXP tends to
highlight much more text  resulting in higher recall and F1 score. However  as will be seen later  the
human evaluators still favor the counterfactual rationales generated by our algorithm.
Since the beer and hotel datasets contain factual annotations only  we report objective evaluation
results for the factual rationales in table 2. CAR achieves the best performances in ﬁve out of the six
cases in the multi-aspect setting. Speciﬁcally  for the hotel review  CAR achieves the best performance
almost in all three aspects. Similarly  CAR delivers the best performance for the appearance and
palate aspects of the beer review dataset  but fails on the aroma aspect. One possible reason for the
failure is that compared to the other aspects  the aroma reviews often have annotated ground truth
containing mixed sentiments. Therefore  CAR has low recalls of these annotated ground truth even
when it successfully selects all the correct class-wise rationales. Also to fulﬁll the sparsity constraint 
sometimes CAR has to select irrelevant aspect words with the desired sentiment  which decreases the
precision. Illustrative examples of the described case can be found in appendix B.3. Please note that
the RNP is not directly comparable to the results in [19]  because the labels are binarized under our
experiment setting.
We visualize the generated rationales on the appearance aspect of beer reviews in ﬁgure 3. More
examples of other datasets can be found in appendix B.3. We observe that the CAR model is able to
produce meaningful justiﬁcations for both factual and counterfactual labels. The factual generator
picks “two inches of frothy light brown head with excellent retention” while the counterfactual one
picks “really light body like water”. By reading these selected texts alone  humans will easily predict
a positive sentiment for the ﬁrst case and be tricked for the counterfactual case.
At last  we present the subjective evaluations in ﬁgure 4. Similar to the observations in the objective
studies  CAR achieves the best performances in almost all cases with two exceptions. The ﬁrst one is
the aroma aspect of the beer reviews  of which we have discussed the potential causes already. The

8

Beer - Appearance
Label - Positive
poured into pint glass . a : used motor oil color . two inches of frothy light brown head with excellent
retention and quite a bit of lacing . nice cascade going for a while . s : oatmeal is the biggest component
of the aroma . not any hops content . a bit fusely and a bit of alcohol . t : tastes like slightly sour nothing .
i do n’t know what the hell made this dark because their is no crystal malt or roasted barley component in
the taste . this sucks . m : light body   really light body like water . carbonation is ﬁne   but that ’s about
it . d : this is slightly sour water . how does anybody like this ?

Figure 3: Examples of CAR generated rationales on the appearance aspect of the beer reviews. All selected
words are bold and underlined. Factual generation uses blue highlight while the counterfactual uses red one.

(a)

(b)

Figure 4: Subjective performances of generated rationales for both (a) factual and (b) counterfactual cases.
For the Amazon reviews  subjects are asked to guess the sentiment based on the generated rationales  which
random guess will have 50% accuracy. For multi-aspect beer and hotel reviews  subjects need to guess both the
sentiment and what aspect the rationale is about  which makes random guess only 16.67%.

second one is the counterfactual performance on the cleanliness aspect of the hotel reviews  where
both POST-EXP and CAR fail to trick human. One potential reason is that the reviews on cleanliness is
often very short and the valence is very clear without a mix of sentiments. Thus  it is very challenging
to generate counterfactual rationales to trick a human. This can be veriﬁed by the analysis in appendix
B.3. Speciﬁcally  according to ﬁgure 4  69% of the time CAR is able to trick people to guess the
counterfactual sentiment  but often with the rationales extracted from the other aspects.

6 Conclusion

In this paper  we propose a game theoretic approach to class-wise rationalization  where the method
is trained to generate supporting evidence for any given label. The framework consists of three
types of players  which competitively select text spans for both factual and counterfactual scenarios.
We theoretically demonstrate the proposed game theoretic framework drives the solution towards
meaningful rationalizations in a simpliﬁed case. Extensive objective and subjective evaluations
on both single- and multi-aspect sentiment classiﬁcation datasets demonstrate that CAR performs
favorably against existing algorithms in terms of both factual and counterfactual rationale generations.

Acknowledgment

We would like to thank Yujia Bao  Yujie Qian  and Jiang Guo from the MIT NLP group for their
insightful discussions. We also want to thank Prof. Regina Barzilay for her support and help.

9

BookElectronicAmazon Domains0.650.700.750.800.850.900.95Joint accuarcy0.810.790.840.800.910.88AppearanceAromaPalateBeer aspects0.00.20.40.60.80.540.570.500.460.440.350.590.420.68Factual performanceLocationServiceCleanlinessHotel aspects0.20.40.60.80.660.710.400.550.760.240.740.780.52RNPPost-expOursBookElectronicAmazon Domains0.500.550.600.650.700.75Joint accuarcy0.590.570.680.63AppearanceAromaPalateBeer aspects0.00.20.40.60.430.390.490.580.170.51Counterfactual performanceLocationServiceCleanlinessHotel aspects0.00.10.20.30.40.340.350.060.410.360.08References
[1] Syed Mumtaz Ali and Samuel D Silvey. A general class of coefﬁcients of divergence of one distribution
from another. Journal of the Royal Statistical Society: Series B (Methodological)  28(1):131–142  1966.

[2] David Alvarez-Melis and Tommi Jaakkola. A causal framework for explaining the predictions of black-box
sequence-to-sequence models. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing  pages 412–421  2017.

[3] David Alvarez-Melis and Tommi S Jaakkola. Towards robust interpretability with self-explaining neural

networks. arXiv preprint arXiv:1806.07538  2018.

[4] Jacob Andreas  Marcus Rohrbach  Trevor Darrell  and Dan Klein. Learning to compose neural networks

for question answering. arXiv preprint arXiv:1601.01705  2016.

[5] Jacob Andreas  Marcus Rohrbach  Trevor Darrell  and Dan Klein. Neural module networks. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  pages 39–48  2016.

[6] Sebastian Bach  Alexander Binder  Grégoire Montavon  Frederick Klauschen  Klaus-Robert Müller  and
Wojciech Samek. On pixel-wise explanations for non-linear classiﬁer decisions by layer-wise relevance
propagation. PloS one  10(7):e0130140  2015.

[7] David Baehrens  Timon Schroeter  Stefan Harmeling  Motoaki Kawanabe  Katja Hansen  and Klaus-Robert
MÃžller. How to explain individual classiﬁcation decisions. Journal of Machine Learning Research 
11(Jun):1803–1831  2010.

[8] Yujia Bao  Shiyu Chang  Mo Yu  and Regina Barzilay. Deriving machine attention from human rationales.

arXiv preprint arXiv:1808.09367  2018.

[9] Yoshua Bengio  Nicholas Léonard  and Aaron Courville. Estimating or propagating gradients through

stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432  2013.

[10] John Blitzer  Mark Dredze  and Fernando Pereira. Biographies  bollywood  boom-boxes and blenders:
Domain adaptation for sentiment classiﬁcation. In Proceedings of the 45th annual meeting of the association
of computational linguistics  pages 440–447  2007.

[11] Jianbo Chen  Le Song  Martin J Wainwright  and Michael I Jordan. L-shapley and c-shapley: Efﬁcient

model interpretation for structured data. arXiv preprint arXiv:1808.02610  2018.

[12] Jianbo Chen  Le Song  Martin J Wainwright  and Michael I Jordan. Learning to explain: An information-

theoretic perspective on model interpretation. arXiv preprint arXiv:1802.07814  2018.

[13] Anupam Datta  Shayak Sen  and Yair Zick. Algorithmic transparency via quantitative input inﬂuence:
Theory and experiments with learning systems. In 2016 IEEE symposium on security and privacy (SP) 
pages 598–617. IEEE  2016.

[14] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680  2014.

[15] Justin Johnson  Bharath Hariharan  Laurens van der Maaten  Judy Hoffman  Li Fei-Fei  C Lawrence Zitnick 
and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE
International Conference on Computer Vision  pages 2989–2998  2017.

[16] Igor Kononenko et al. An efﬁcient explanation of individual classiﬁcations using game theory. Journal of

Machine Learning Research  11(Jan):1–18  2010.

[17] Guang-He Lee  David Alvarez-Melis  and Tommi S Jaakkola. Towards robust  locally linear deep networks.

arXiv preprint arXiv:1907.03207  2019.

[18] Guang-He Lee  Wengong Jin  David Alvarez-Melis  and Tommi S Jaakkola. Functional transparency for

structured data: a game-theoretic approach. arXiv preprint arXiv:1902.09737  2019.

[19] Tao Lei  Regina Barzilay  and Tommi Jaakkola. Rationalizing neural predictions. arXiv preprint

arXiv:1606.04155  2016.

[20] Jiwei Li  Xinlei Chen  Eduard Hovy  and Dan Jurafsky. Visualizing and understanding neural models
in NLP. In Proceedings of the 2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies  pages 681–691  2016.

10

[21] Jiwei Li  Will Monroe  and Dan Jurafsky. Understanding neural networks through representation erasure.

arXiv preprint arXiv:1612.08220  2016.

[22] Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In Advances in

Neural Information Processing Systems  pages 4765–4774  2017.

[23] Julian McAuley  Jure Leskovec  and Dan Jurafsky. Learning attitudes and attributes from multi-aspect

reviews. In 2012 IEEE 12th International Conference on Data Mining  pages 1020–1025. IEEE  2012.

[24] Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. Why should I trust you?: Explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference on
knowledge discovery and data mining  pages 1135–1144. ACM  2016.

[25] Avanti Shrikumar  Peyton Greenside  and Anshul Kundaje. Learning important features through propa-
gating activation differences. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70  pages 3145–3153. JMLR. org  2017.

[26] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks: Visualising

image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034  2013.

[27] Jost Tobias Springenberg  Alexey Dosovitskiy  Thomas Brox  and Martin Riedmiller. Striving for simplicity:

The all convolutional net. arXiv preprint arXiv:1412.6806  2014.

[28] Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70  pages 3319–3328. JMLR. org 
2017.

[29] Hongning Wang  Yue Lu  and Chengxiang Zhai. Latent aspect rating analysis on review text data: a rating
regression approach. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 783–792. ACm  2010.

[30] Adam Yala  Constance Lehman  Tal Schuster  Tally Portnoi  and Regina Barzilay. A deep learning
mammography-based model for improved breast cancer risk prediction. Radiology  page 182716  2019.

[31] Mo Yu  Shiyu Chang  and Tommi S Jaakkola. Learning corresponded rationales for text matching. 2018.

[32] Mo Yu  Shiyu Chang  Yang Zhang  and Tommi S Jaakkola. Rethinking cooperative rationalization:
Introspective extraction and complement control. In Empirical Methods in Natural Language Processing 
2019.

11

,Shiyu Chang
Yang Zhang
Mo Yu
Tommi Jaakkola