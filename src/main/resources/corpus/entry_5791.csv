2019,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video,Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However  the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly  due to lack of proper constraints  networks output scale-inconsistent results over different samples  i.e.  the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works  our framework is much simpler and more efficient. Comprehensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the KITTI dataset. Moreover  we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences  and the resulting visual odometry accuracy is competitive with the recent model that is trained using stereo videos. To the best of our knowledge  this is the first work to show that deep networks trained using unlabelled monocular videos can predict globally scale-consistent camera trajectories over a long video sequence.,Unsupervised Scale-consistent Depth and Ego-motion

Learning from Monocular Video

Jia-Wang Bian1 2  Zhichao Li3  Naiyan Wang3  Huangying Zhan1 2

Chunhua Shen1 2  Ming-Ming Cheng4  Ian Reid1 2

1University of Adelaide  Australia

2Australian Centre for Robotic Vision  Australia

3TuSimple  China

4Nankai University  China

Abstract

Recent work has shown that CNN-based depth and ego-motion estimators can
be learned using unlabelled monocular videos. However  the performance is
limited by unidentiﬁed moving objects that violate the underlying static scene
assumption in geometric image reconstruction. More signiﬁcantly  due to lack
of proper constraints  networks output scale-inconsistent results over different
samples  i.e.  the ego-motion network cannot provide full camera trajectories over
a long video sequence because of the per-frame scale ambiguity. This paper tackles
these challenges by proposing a geometry consistency loss for scale-consistent
predictions and an induced self-discovered mask for handling moving objects and
occlusions. Since we do not leverage multi-task learning like recent works  our
framework is much simpler and more efﬁcient. Comprehensive evaluation results
demonstrate that our depth estimator achieves the state-of-the-art performance on
the KITTI dataset. Moreover  we show that our ego-motion network is able to
predict a globally scale-consistent camera trajectory for long video sequences  and
the resulting visual odometry accuracy is competitive with the recent model that is
trained using stereo videos. To the best of our knowledge  this is the ﬁrst work to
show that deep networks trained using unlabelled monocular videos can predict
globally scale-consistent camera trajectories over a long video sequence.

1

Introduction

Depth and ego-motion estimation is crucial for various applications in robotics and computer vision.
Traditional methods are usually hand-crafted stage-wise systems  which rely on correspondence
search [1  2] and multi-view geometry [3  4] for estimation. Recently  deep learning based methods [5 
6] show that the depth can be inferred from a single image by using Convolutional Neural Network
(CNN). Especially  unsupervised methods [7–11] show that CNN-based depth and ego-motion
networks can be solely trained on monocular video sequences without using ground-truth depth or
stereo image pairs (pose supervision). The principle is that one can warp the image in one frame to
another frame using the predicted depth and ego-motion  and then employ the image reconstruction
loss as the supervision signal [7] to train the network. However  the performance limitation arises
due to the moving objects that violate the underlying static scene assumption in geometric image
reconstruction. More signiﬁcantly  due to lack of proper constraints the network predicts scale-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

inconsistent results over different samples  i.e.  the ego-motion network cannot provide a full camera
trajectory over a long video sequence because of the per-frame scale ambiguity1.
To the best of our knowledge  no previous work (unsupervised learning from monocular videos)
addresses the scale-inconsistency issue mentioned above. To this end  we propose a geometry
consistency loss for tackling the challenge. Speciﬁcally  for any two consecutive frames sampled
from a video  we convert the predicted depth map in one frame to 3D space  then project it to the
other frame using the estimated ego-motion  and ﬁnally minimize the inconsistency of the projected
and the estimated depth maps. This explicitly enforces the depth network to predict geometry-
consistent (of course scale-consistent) results over consecutive frames. With iterative sampling and
training from videos  depth predictions on each consecutive image pair would be scale-consistent 
and the frame-to-frame consistency can eventually propagate to the entire video sequence. As the
scale of ego-motions is tightly linked to the scale of depths  the proposed ego-motion network can
predict scale-consistent relative camera poses over consecutive snippets. We show that just simply
accumulating pose predictions can result in globally scale-consistent camera trajectories over a long
video sequence (Fig. 3).
Regarding the challenge of moving objects  recent work addresses it by introducing an additional
optical ﬂow [9–11  13] or semantic segmentation network [14]. Although this improves performance
signiﬁcantly  it also brings about huge computational cost during training. Here we show that we
could automatically discover a mask from the proposed geometry consistency term for solving the
problem without introducing new networks. Speciﬁcally  we can easily locate pixels that belong to
dynamic objects/occluded regions or difﬁcult regions (e.g.  textureless regions) using the proposed
term. By assigning lower weights to those pixels  we can avoid their impact to the fragile image
reconstruction loss (see Fig. 2 for mask visualization). Compared with these recent approaches [9–11]
that leverage multi-task learning  the proposed method is much simpler and more efﬁcient.
We conduct detailed ablation studies that clearly demonstrate the efﬁcacy of the proposed approach.
Furthermore  comprehensive evaluation results on the KITTI [15] dataset show that our depth
network outperforms state-of-the-art models that are trained in more complicated multi-task learning
frameworks [9–11  16]. Meanwhile  our ego-motion network is able to predict scale-consistent
camera trajectories over long video sequences  and the accuracy of trajectory is competitive with the
state-of-the-art model that is trained using stereo videos [17].
To summarize  our main contributions are three-fold:

ego-motion networks  leading to a globally scale-consistent ego-motion estimator.

• We propose a geometry consistency constraint to enforce the scale-consistency of depth and
• We propose a self-discovered mask for dynamic scenes and occlusions by the aforementioned
geometry consistency constraint. Compared with other approaches  our proposed approach
does not require additional optical ﬂow or semantic segmentation networks  which makes
the learning framework simpler and more efﬁcient.
• The proposed depth estimator achieves state-of-the-art performance on the KITTI dataset 
and the proposed ego-motion predictor shows competitive visual odometry results compared
with the state-of-the-art model that is trained using stereo videos.

2 Related work

Traditional methods rely on the disparity between multiple views of a scene to recover the 3D scene
geometry  where at least two images are required [3]. With the rapid development of deep learning 
Eigen et al. [5] show that the depth can be predicted from a single image using Convolution Neural
Network (CNN). Speciﬁcally  they design a coarse-to-ﬁne network to predict the single-view depth
and use the ground truth depths acquired by range sensors as the supervision signal to train the
network. However  although these supervised methods [5  6  18–21] show high-quality ﬂow and
depth estimation results  it is expensive to acquire ground truth in real-world scenes.

1Monocular systems such as ORB-SLAM [12] suffer from the scale ambiguity issue  but their predictions
are globally scale-consistent. However  recently learned models using monocular videos not only suffer from the
scale ambiguity  but also predict scale-inconsistent results over different snippets.

2

Without requiring the ground truth depth  Garg et al. [22] show that a single-view depth network can
be trained using stereo image pairs. Instead of using depth supervision  they leverage the established
epipolar geometry [3]. The color inconsistency between a left image and a synthesized left image
warped from the right image is used as the supervision signal. Following this idea  Godard et al. [23]
propose to constrain the left-right consistency for regularization  and Zhan et al. [17] extend the
method to stereo videos. However  though stereo pairs based methods do not require the ground truth
depth  accurately rectifying stereo cameras is also non-trivial in real-world scenarios.
To that end  Zhou et al. [7] propose a fully unsupervised framework  in which the depth network
can be learned solely from monocular videos. The principle is that they introduce an additional
ego-motion network to predict the relative camera pose between consecutive frames. With the
estimated depth and relative pose  image reconstruction as in [22] is applied and the photometric loss
is used as the supervision signal. However  the performance is limited due to dynamic objects that
violate the underlying static scene assumption in geometric image reconstruction. More importantly 
Zhou et al. [7]’s method suffers from the per-frame scale ambiguity  in that a single and consistent
scaling of the camera translations is missing and only direction is known. As a result  the ego-motion
network cannot predict a full camera trajectory over a long video sequence.
For handling moving objects  recent work [9  10] proposes to introduce an additional optical ﬂow
network. Even more recently [11] introduces an extra motion segmentation network. Although they
show signiﬁcant performance improvement  there is a huge additional computational cost added into
the basic framework  yet they still suffer from the scale-inconsistency issue. Besides  Liu et al. [24]
use depth projection loss for supervision density  similar to the proposed consistency loss  but their
method relies on the pre-computed 3D reconstruction for supervision.
To the best of our knowledge  this paper is the ﬁrst one to show that the ego-motion network trained
in monocular videos can predict a globally scale-consistent camera trajectory over a long video
sequence. This shows signiﬁcant potentials to leverage deep learning methods in Visual SLAM [12]
for robotics and autonomous driving.

3 Unsupervised Learning of Scale-consistent Depth and Ego-motion

3.1 Method Overview

Our goal is to train depth and ego-motion networks using monocular videos  and constrain them to
predict scale-consistent results. Given two consecutive frames (Ia  Ib) sampled from an unlabeled
video  we ﬁrst estimate their depth maps (Da  Db) using the depth network  and then predict the
relative 6D camera pose Pab between them using the pose network.
With the predicted depth and relative camera pose  we can synthesize the reference image I(cid:48)
a by
interpolating the source image Ib [25  7]. Then  the network can be supervised by the photometric
loss between the real image Ia and the synthesized one I(cid:48)
a. However  due to dynamic scenes that
violate the geometric assumption in image reconstruction  the performance of this basic framework
is limited. To this end  we propose a geometry consistency loss LGC for scale-consistency and a
self-discovered mask M for handling the moving objects and occlusions. Fig. 1 shows an illustration
of the proposed loss and mask.
Our overall objective function can be formulated as follows:

L = αLM

p + βLs + γLGC 

(1)
where LM
p stands for the weighted photometric loss (Lp) by the proposed mask M  and Ls stands for
the smoothness loss. We train the network in both forward and backward directions to maximize the
data usage  and for simplicity we only derive the loss for the forward direction.
In the following sections  we ﬁrst introduce the widely used photometric loss and smoothness loss in
Sec. 3.2  and then describe the proposed geometric consistency loss in Sec. 3.3 and the self-discovered
mask in Sec. 3.4.

3.2 Photometric loss and smoothness loss

Photometric loss. Leveraging the brightness constancy and spatial smoothness priors used in
classical dense correspondence algorithms [26]  previous works [7  9–11] have used the photometric

3

Figure 1: Illustration of the proposed geometry consistency loss and self-discover mask. Given two
consecutive frames (Ia  Ib)  we ﬁrst estimate their depth maps (Da  Db) and relative pose (Pab) using
the network  then we get the warped (Da
b ) by converting Da to 3D space and projecting to the image
plane of Ib using Pab  and ﬁnally we use the inconsistency between Da
b interpolated
from Db as the geometric consistency loss LGC (Eqn. 6) to supervise the network training. Here  we
interpolate Db because the projection ﬂow does not lie on the pixel grid of Ib. Besides  we discover a
mask M (Eqn. 7) from the inconsistency map for handling dynamic scenes and ill-estimated regions
(Fig. 2). For clarity  the photometric loss and smoothness loss are not shown in this ﬁgure.

b and the D(cid:48)

error between the warped frame and the reference frame as an unsupervised loss function for training
the network.
With the predicted depth map Da and the relative camera pose Pab  we synthesize I(cid:48)
Ib  where differentiable bilinear interpolation [25] is used as in [7]. With the synthesized I(cid:48)
reference image Ia  we formulate the objective function as
(cid:107)Ia(p) − I(cid:48)

a by warping
a and the

(cid:88)

Lp =

a(p)(cid:107)1 

(2)

1
|V |

p∈V

where V stands for valid points that are successfully projected from Ia to the image plane of Ib  and
|V | deﬁnes the number of points in V . We choose L1 loss due to its robustness to outliers. However  it
is still not invariant to illumination changes in real-world scenarios. Here we add an additional image
dissimilarity loss SSIM [27] for better handling complex illumination changes  since it normalizes
the pixel illumination. We modify the photometric loss term Eqn. 2 as:

Lp =

1
|V |

(λi(cid:107)Ia(p) − I(cid:48)

a(p)(cid:107)1 + λs

2
where SSIMaa(cid:48) stands for the element-wise similarity between Ia and I(cid:48)
Following [23  9  11]  we use λi = 0.15 and λs = 0.85 in our framework.

p∈V

1 − SSIMaa(cid:48)(p)

) 

(3)

a by the SSIM function [27].

(cid:88)

(cid:88)

Smoothness loss. As the photometric loss is not informative in low-texture nor homogeneous
region of the scene  existing work incorporates a smoothness prior to regularize the estimated depth
map. We adopt the edge-aware smoothness loss used in [11]  which is formulated as:

Ls =

(e−∇Ia(p) · ∇Da(p))2 

(4)

where ∇ is the ﬁrst derivative along spatial directions. It ensures that smoothness is guided by the
edge of images.

p

3.3 Geometry consistency loss

As mentioned before  we enforce the geometry consistency on the predicted results. Speciﬁcally  we
require that Da and Db (related by Pab) conform the same 3D scene structure  and minimize their
differences. The optimization not only encourages the geometry consistency between samples in a
batch but also transfers the consistency to the entire sequence. e.g.  depths of I1 agree with depths
of I2 in a batch; depths of I2 agree with depths of I3 in another training batch. Eventually  depths

4

ConcatDepthNetDepthNetPoseNet𝑷𝑷𝒂𝒂𝒂𝒂𝐃𝐃𝐛𝐛𝐚𝐚𝐃𝐃𝐛𝐛′𝑫𝑫𝒂𝒂𝑫𝑫𝒂𝒂𝑰𝑰𝒂𝒂𝑰𝑰𝒂𝒂𝑳𝑳𝑮𝑮𝑮𝑮𝑴𝑴WarpedInterpolatedProjectionFlow(a)

(c)

(b)

(d)

Figure 2: Visual results. Top to bottom: sample image  estimated depth  self-discovered mask. The
proposed mask can effectively identify occlusions and moving objects.

of Ii of a sequence should all agree with each other. As the pose network is naturally coupled with
the depth network during training  our method yields scale-consistent predictions over the entire
sequence.
With this constraint  we compute the depth inconsistency map Ddiff. For each p ∈ V   it is deﬁned as:

Ddiff(p) =

b (p) − D(cid:48)
|Da
b (p) + D(cid:48)
Da

b(p)|
b(p)

b is the computed depth map of Ib by warping Da using Pab  and D(cid:48)

where Da
b is the interpolated
depth map from the estimated depth map Db (Note that we cannot directly use Db because the
warping ﬂow does not lie on the pixel grid ). Here we normalize their difference by their sum. This
is more intuitive than the absolute distance as it treats points at different absolute depths equally in
optimization. Besides  the function is symmetric and the outputs are naturally ranging from 0 to 1 
which contributes to numerical stability in training.
With the inconsistency map  we simply deﬁne the proposed geometry consistency loss as:

Ddiff(p) 

(6)

(cid:88)

p∈V

LGC =

1
|V |

(5)

(7)

which minimizes the geometric distance of predicted depths between each consecutive pair and
enforces their scale-consistency. With training  the consistency can propagate to the entire video
sequence. Due to the tight link between ego-motion and depth predictions  the ego-motion network
can eventually predict globally scale-consistent trajectories (Fig. 3).

3.4 Self-discovered mask

To handle moving objects and occlusions that may impair the network training  recent work propose to
introduce an additional optical ﬂow [9–11] or semantic segmentation network [14]. This is effective 
however it also introduces extra computational cost and training burden. Here  we show that these
regions can be effectively located by the proposed inconsistency map Ddiff in Eqn. 5.
There are several scenarios that result in inconsistent scene structure observed from different views 
including (1) dynamic objects  (2) occlusions  and (3) inaccurate predictions for difﬁcult regions.
Without separating them explicitly  we observe each of these will result in Ddiff increasing from its
ideal value of zero.
Based on this simple observation  we propose a weight mask M as Ddiff is in [0  1]:

M = 1 − Ddiff 

5

which assigns low/high weights for inconsistent/consistent pixels. It can be used to re-weight the
photometric loss. Speciﬁcally  we modify the photometric loss in Eqn. 3 as

(cid:88)

p∈V

LM

p =

1
|V |

(M (p) · Lp(p)).

(8)

By using the mask  we mitigate the adverse impact from moving objects and occlusions. Further 
the gradients computed on inaccurately predicted regions carry less weight during back-propagation.
Fig. 2 shows visual results for the proposed mas  which coincides with our anticipation stated above.

4 Experiment

4.1

Implementation details

Network architecture. For the depth network  we experiment with DispNet [7] and DispRes-
Net [11]  which takes a single RGB image as input and outputs a depth map. For the ego-motion
network  PoseNet without the mask prediction branch [7] is used. The network estimates a 6D relative
camera pose from a concatenated RGB image pair. Instead of computing the loss on multiple-scale
outputs of the depth network (4 scales in [7] or 6 scales in [11])  we empirically ﬁnd that using
single-scale supervision (i.e.  only compute the loss on the ﬁnest output) is better (Tab. 4). Our
single-scale supervision not only improves the performance but also contributes a more concise
training pipeline. We hypothesize the reason of this phenomenon is that the photometric loss is not
accurate in low-resolution images  where the pixel color is over-smoothed.

Single-view depth estimation. The proposed learning framework is implemented using PyTorch
Library [28]. For depth network  we train and test models on KITTI raw dataset [15] using Eigen [5]’s
split that is the same with related works [10  9  11  7]. Following [7]  we use a snippet of three
sequential video frames as a training sample  where we set the second image as reference frame to
compute loss with other two images and then inverse their roles to compute loss again for maximizing
the data usage. The data is also augmented with random scaling  cropping and horizontal ﬂips
during training  and we experiment with two input resolutions (416 × 128 and 832 × 256). We use
ADAM [29] optimizer  and set the batch size to 4 and the learning rate to 10−4. During training 
we adopt α = 1.0  β = 0.1  and γ = 0.5 in Eqn. 1. We train the network in 200 epochs with 1000
randomly sampled batches in one epoch  and validate the model at per epoch. Also  we pre-train
the network on CityScapes [30] and ﬁnetune on KITTI [15]  each for 200 epochs. Here we follow
Eigen et al. [5]’s evaluation metrics for depth evaluation.

Visual odometry prediction. For pose network  following Zhan et al. [17]  we evaluate visual
odometry results on KITTI odometry dataset [15]  where sequence 00-08/09-10 are used for train-
ing/testing. We use the standard evaluation metrics by the dataset for trajectory evaluation rather than
Zhou et al. [7]’s 5-frame pose evaluation  since they are more widely used and more meaningful.

4.2 Comparisons with the state-of-the-art

Depth results on KITTI raw dataset. Tab. 1 shows the results on KITTI raw dataset [15]  where
our method achieves the state-of-the-art performance when compared with models trained on monoc-
ular video sequences. Note that recent work [9–11  31] all jointly learn multiple tasks  while our
approach does not. This effectively reduces the training and inference overhead. Moreover  our
method competes quite favorably with other methods using stronger supervision signals such as
calibrated stereo image pairs (i.e.  pose supervision) or even ground-truth depth annotation.

Visual odometry results on KITTI odometry dataset. We compare with SfMLearner [7] and
the methods trained with stereo videos [17]. We also report the results of ORB-SLAM [12] system
(without loop closing) as a reference  though emphasize that this results in a comparison note between
a simple frame-to-frame pose estimation framework with a Visual SLAM system  in which the
latter has a strong back-end optimization system (i.e.  bundle adjustment [32]) for improving the
performance. Here  we ignore the frames (First 9 and 30 respectively) from the sequences (09 and
10) for which ORB-SLAM [12] fails to output camera poses because of unsuccessful initialization.

6

Table 1: Single-view depth estimation results on test split of KITTI raw dataset [15]. The methods
trained on KITTI raw dataset [15] are denoted by K. Models with pre-training on CityScapes [30]
are denoted by CS+K. (D) denotes depth supervision  (B) denotes binocular/stereo input pairs  (M)
denotes monocular video clips. (J) denotes joint learning of multiple tasks. The best performance in
each block is highlighted as bold.

K (B)

K (B+D)

CS+K (B)

Dataset
K (D)
K (D)
K (B)

Methods
Eigen et al. [5]
Liu et al. [6]
Garg et al. [22]
Kuznietsov et al. [18]
Godard et al. [23]
Godard et al. [23]
Zhan et al. [17]
Zhou et al. [7]
Yang et al. [31] (J)
Mahjourian et al. [8]
Wang et al. [16]
Geonet-VGG [9] (J)
Geonet-Resnet [9] (J)
DF-Net [10] (J)
CC [11] (J)
Ours
CS+K (M)
Zhou et al. [7]
CS+K (M)
Yang et al. [31] (J)
CS+K (M)
Mahjourian et al. [8]
Wang et al. [16]
CS+K (M)
Geonet-Resnet [9] (J) CS+K (M)
CS+K (M)
DF-Net [10] (J)
CS+K (M)
CC [11] (J)
Ours
CS+K (M)

K (B)
K (M)
K (M)
K (M)
K (M)
K (M)
K (M)
K (M)
K (M)
K (M)

AbsRel
0.203
0.202
0.152
0.113
0.148
0.124
0.144
0.208
0.182
0.163
0.151
0.164
0.155
0.150
0.140
0.137
0.198
0.165
0.159
0.148
0.153
0.146
0.139
0.128

Error ↓

Accuracy ↑

SqRel RMS RMSlog < 1.25 < 1.252 < 1.253
0.958
1.548
0.965
1.614
0.967
1.226
0.741
0.986
0.964
1.344
0.973
1.076
0.969
1.391
0.957
1.768
0.963
1.481
1.240
0.968
0.974
1.257
0.968
1.303
0.973
1.296
0.973
1.124
1.070
0.975
0.975
1.089
0.960
1.836
0.969
1.360
0.970
1.231
0.975
1.187
1.328
0.972
0.978
1.182
1.032
0.977
1.047
0.976

6.307
6.523
5.849
4.621
5.927
5.311
5.869
6.856
6.501
6.220
5.583
6.090
5.857
5.507
5.326
5.439
6.565
6.641
5.912
5.496
5.737
5.215
5.199
5.234

0.282
0.275
0.246
0.189
0.247
0.219
0.241
0.283
0.267
0.250
0.228
0.247
0.233
0.223
0.217
0.217
0.275
0.248
0.243
0.226
0.232
0.213
0.213
0.208

0.702
0.678
0.784
0.862
0.803
0.847
0.803
0.678
0.725
0.762
0.810
0.765
0.793
0.806
0.826
0.830
0.718
0.750
0.784
0.812
0.802
0.818
0.827
0.846

0.890
0.895
0.921
0.960
0.922
0.942
0.928
0.885
0.906
0.916
0.936
0.919
0.931
0.933
0.941
0.942
0.901
0.914
0.923
0.938
0.934
0.943
0.943
0.947

Table 2: Visual odometry results on KITTI odometry dataset [15]. We report the performance of
ORB-SLAM [12] as a reference and compare with recent deep methods. K denotes the model trained
on KITTI  and CS+K denotes the model with pre-training on Cityscapes [30].

Methods

ORB-SLAM [12]
Zhou et al. [7]
Zhan et al. [17]
Ours (K)
Ours (CS+K)

terr (%)
15.30
17.84
11.93
11.2
8.24

Seq. 09
rerr (◦/100m)

0.26
6.78
3.91
3.35
2.19

terr (%)

3.68
37.91
12.45
10.1
10.7

Seq. 10
rerr (◦/100m)

0.48
17.78
3.46
4.96
4.58

(a) sequence 09

(b) sequence 10

Figure 3: Qualitative results on the testing sequences of KITTI odometry dataset [15].

7

Tab. 2 shows the average translation and rotation errors for the testing sequence 09 and 10  and Fig. 3
shows qualitative results. Note that the comparison is highly disadvantageous to the proposed method:
i) we align per-frame scale to the ground truth scale for [7] due to its scale-inconsistency  while we
only align one global scale for our method; ii) [17] requires stereo videos for training  while we only
use monocular videos. Although it is unfair to the proposed method  the results show that our method
achieves competitive results with [17]. Even when compared with the ORB-SLAM [12] system 
our method shows a lower translational error and a better visual result on sequence 09. This is a
remarkable progress that deep models trained on unlabelled monocular videos can predict a globally
scale-consistent visual odometry.

4.3 Ablation study

In this section  we ﬁrst validate the efﬁcacy of the proposed geometry-consistency loss LGC and
the self-discovered weight mask M. Then we experiment with different scale numbers  network
architectures  and image resolutions.

Validating proposed LGC and M. We conduct ablation studies using DispNet [7] and images of
416×128 resolution. Tab. 3 shows the depth results for both single-scale and multi-scale supervisions.
The results clearly demonstrate the contribution of our proposed terms to the overall performance.
Besides  Fig. 4 shows the validation error during training  which indicates that the proposed LGC can
effectively prevent the model from overﬁtting.

Table 3: Ablation studies on LGC and M. Brackets show results of multi-scale (4) supervisions.

AbsRel

Methods
0.161 (0.185)
Basic
0.160 (0.163)
Basic+SSIM
Basic+SSIM+GC
0.158 (0.161)
Basic+SSIM+GC+M 0.151 (0.158)

Error ↓
SqRel RMS RMSlog < 1.25 < 1.252 < 1.253
0.972
1.225
0.969
1.230
0.971
1.247
1.154
0.972

0.237
0.243
0.235
0.232

5.765
5.950
5.827
5.716

Accuracy ↑

0.780
0.775
0.786
0.798

0.927
0.923
0.927
0.930

Figure 4: Validation error. Both Basic and Basic+SSIM overﬁt after about 50 epochs  while others do
not due to proposed LGC. Besides  models with the single-scale supervision in training outperforms
those with multi-scale (4) supervisions.

Proposed single-scale vs multi-scale supervisions. As mentioned in Sec. 4.1  we empirically ﬁnd
that using single-scale supervision leads to better performance than using the widely-used multi-scale
solution. Tab. 4 shows the depth results. We hypothesis the reason is that the photometric loss
is not accurate in low-resolution images  where the pixel color is over-smoothed. Besides  as the
displacement between two consecutive views is small  the multi-scale solution is unnecessary.

Network architectures and image resolutions. Tab. 5 shows the results of different network
architectures on different resolution images  where DispNet and DispResNet are both borrowed from

8

050100150200Epochs0.160.180.20.220.240.260.280.30.320.340.36AbsRelsingle-scaleBasicBasic+SSIMBasic+SSIM+GCBasic+SSIM+GC+M050100150200Epochs0.160.180.20.220.240.260.280.30.320.340.36AbsRelmulti-scaleTable 4: Ablation studies on scale numbers of supervision.

Error ↓

Accuracy ↑

#Scales AbsRel
0.151
1
0.152
2
3
0.159
0.158
4

SqRel RMS RMSlog < 1.25 < 1.252 < 1.253
0.972
1.154
0.971
1.192
1.226
0.969
0.971
1.214

5.716
5.900
5.987
5.898

0.232
0.235
0.240
0.239

0.798
0.795
0.780
0.782

0.930
0.927
0.921
0.925

CC [11]  and DispNet is also used in SfMLearner [7]. It shows that higher resolution images and
deeper networks can results in better performance.

Table 5: Ablation studies on different network architectures and image resolutions.

Methods
DispNet
DispResNet
DispNet
DispResNet

Resolutions AbsRel
416 × 128
0.151
0.149
832 × 256
0.146
0.137

4.4 Timing and memory analysis

Error ↓

Accuracy ↑

SqRel RMS RMSlog < 1.25 < 1.252 < 1.253
0.972
1.154
0.973
1.137
0.975
1.197
1.089
0.975

5.716
5.771
5.578
5.439

0.930
0.932
0.940
0.942

0.232
0.230
0.223
0.217

0.798
0.799
0.814
0.830

Training time and parameter numbers. We compare with CC [11]  and both methods are trained
on a single 16GB Tesla V100 GPU. We measure the time taken for each training iteration consisting
of forward and backward pass using a batch size of 4. The image resolution is 832 × 256. CC [11]
needs train 3 parts  including (Depth  Pose)  Flow  and Mask. In contrast our method only trains
(Depth  Pose). In total  CC takes about 7 days for training as reported by authors  while our method
takes about 32 hours. Tab. 6 shows the per-iteration time and model parameters of each network.

Table 6: Training time per iteration and model parameters for each network.

Network
Time
Parameter Numbers

(Depth  Pose)

0.96s

(80.88M  2.18M)

CC [11]

Flow
1.32s
39.28M 5.22M (80.88M  1.59M)

(Depth  Pose)

Mask
0.48s

Ours

0.55s

Inference time. We test models on a single RTX 2080 GPU. The batch size is 1  and the time is
averaged over 100 iterations. Tab. 7 shows the results. The DispNet and DispResNet architectures
are same with SfMLearner [7] and CC [11]  respectively  so their speeds are theoretically same.

Table 7: Inference time on per image or image pair.

128 × 416
256 × 832

DispNet DispResNet
4.9 ms
9.2 ms

9.6 ms
15.5 ms

PoseNet
0.6 ms
1.0 ms

5 Conclusion

This paper presents an unsupervised learning framework for scale-consistent depth and ego-motion
estimation. The core of the proposed approach is a geometry consistency loss for scale-consistency
and a self-discovered mask for handling dynamic scenes. With the proposed learning framework  our
depth model achieves the state-of-the-art performance on the KITTI [15] dataset  and our ego-motion
network can show competitive visual odometry results with the model that is trained using stereo
videos. To the best of our knowledge  this is the ﬁrst work to show that deep models training on
unlabelled monocular videos can predict a globally scale-consistent camera trajectory over a long
sequence. In future work  we will focus on improving the visual odometry accuracy by incorporating
drift correcting solutions into the current framework.

9

Acknowledgments

The work was supported by the Australian Centre for Robotic Vision  the Major Project for New
Generation of AI (No. 2018AAA0100400)  and NSFC (NO. 61922046). Jiawang would also like to
thank TuSimple  where he started research in this ﬁeld.

References
[1] David G Lowe. Distinctive image features from scale-invariant keypoints. International Journal

on Computer Vision (IJCV)  60(2)  2004.

[2] Jia-Wang Bian  Wen-Yan Lin  Yun Liu  Le Zhang  Sai-Kit Yeung  Ming-Ming Cheng  and
Ian Reid. GMS: Grid-based motion statistics for fast  ultra-robust feature correspondence.
International Journal on Computer Vision (IJCV)  2019.

[3] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge

university press  2003.

[4] Jia-Wang Bian  Yu-Huan Wu  Ji Zhao  Yun Liu  Le Zhang  Ming-Ming Cheng  and Ian Reid.
An evaluation of feature matchers for fundamental matrix estimation. In British Machine Vision
Conference (BMVC)  2019.

[5] David Eigen  Christian Puhrsch  and Rob Fergus. Depth map prediction from a single image

using a multi-scale deep network. In Neural Information Processing Systems (NIPS)  2014.

[6] Fayao Liu  Chunhua Shen  Guosheng Lin  and Ian Reid. Learning depth from single monocular
images using deep convolutional neural ﬁelds. IEEE Transactions on Pattern Recognition and
Machine Intelligence (PAMI)  38(10)  2016.

[7] Tinghui Zhou  Matthew Brown  Noah Snavely  and David G Lowe. Unsupervised learning
of depth and ego-motion from video. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2017.

[8] Reza Mahjourian  Martin Wicke  and Anelia Angelova. Unsupervised learning of depth and
ego-motion from monocular video using 3d geometric constraints. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  2018.

[9] Zhichao Yin and Jianping Shi. GeoNet: Unsupervised learning of dense depth  optical ﬂow and
camera pose. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[10] Yuliang Zou  Zelun Luo  and Jia-Bin Huang. DF-Net: Unsupervised joint learning of depth and
ﬂow using cross-task consistency. In European Conference on Computer Vision (ECCV)  2018.

[11] Anurag Ranjan  Varun Jampani  Kihwan Kim  Deqing Sun  Jonas Wulff  and Michael J Black.
Competitive Collaboration: Joint unsupervised learning of depth  camera motion  optical ﬂow
and motion segmentation. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2019.

[12] Raul Mur-Artal  Jose Maria Martinez Montiel  and Juan D Tardos. ORB-SLAM: a versatile and

accurate monocular slam system. IEEE Transactions on Robotics (TRO)  31(5)  2015.

[13] Yang Wang  Zhenheng Yang  Peng Wang  Yi Yang  Chenxu Luo  and Wei Xu. Joint unsupervised
learning of optical ﬂow and depth by watching stereo videos. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2019.

[14] Jianbo Jiao  Ying Cao  Yibing Song  and Rynson Lau. Look deeper into depth: Monocular
depth estimation with semantic booster and attention-driven loss. In European Conference on
Computer Vision (ECCV)  2018.

[15] Andreas Geiger  Philip Lenz  Christoph Stiller  and Raquel Urtasun. Vision meets Robotics:

The kitti dataset. International Journal of Robotics Research (IJRR)  2013.

10

[16] Chaoyang Wang  José Miguel Buenaposada  Rui Zhu  and Simon Lucey. Learning depth from
monocular videos using direct methods. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  June 2018.

[17] Huangying Zhan  Ravi Garg  Chamara Saroj Weerasekera  Kejie Li  Harsh Agarwal  and Ian
Reid. Unsupervised learning of monocular depth estimation and visual odometry with deep
In IEEE Conference on Computer Vision and Pattern Recognition
feature reconstruction.
(CVPR)  2018.

[18] Yevhen Kuznietsov  Jorg Stuckler  and Bastian Leibe. Semi-supervised deep learning for monoc-
ular depth map prediction. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2017.

[19] Chengzhou Tang and Ping Tan. BA-Net: Dense bundle adjustment network. In International

Conference on Learning Representations (ICLR)  2019.

[20] Zhichao Yin  Trevor Darrell  and Fisher Yu. Hierarchical discrete distribution decomposition
for match density estimation. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2019.

[21] Wei Yin  Yifan Liu  Chunhua Shen  and Youliang Yan. Enforcing geometric constraints of
virtual normal for depth prediction. In IEEE International Conference on Computer Vision
(ICCV)  2019.

[22] Ravi Garg  Vijay Kumar BG  Gustavo Carneiro  and Ian Reid. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In European Conference on Computer Vision
(ECCV). Springer  2016.

[23] Clément Godard  Oisin Mac Aodha  and Gabriel J Brostow. Unsupervised monocular depth
estimation with left-right consistency. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2017.

[24] Xingtong Liu  Ayushi Sinha  Mathias Unberath  Masaru Ishii  Gregory D Hager  Russell H
Taylor  and Austin Reiter. Self-supervised learning for dense depth estimation in monocular en-
doscopy. In OR 2.0 Context-Aware Operating Theaters  Computer Assisted Robotic Endoscopy 
Clinical Image-Based Procedures  and Skin Image Analysis  pages 128–138. Springer  2018.
[25] Max Jaderberg  Karen Simonyan  Andrew Zisserman  et al. Spatial transformer networks. In

Neural Information Processing Systems (NIPS)  2015.

[26] Simon Baker and Iain Matthews. Lucas-kanade 20 years on: A unifying framework. Interna-

tional Journal on Computer Vision (IJCV)  56(3)  2004.

[27] Zhou Wang  Alan C Bovik  Hamid R Sheikh  Eero P Simoncelli  et al. Image Quality Assess-
ment: from error visibility to structural similarity. IEEE Transactions on Image Processing
(TIP)  13(4)  2004.

[28] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W  2017.

[29] Diederik P Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980  2014.

[30] Marius Cordts  Mohamed Omran  Sebastian Ramos  Timo Rehfeld  Markus Enzweiler  Rodrigo
Benenson  Uwe Franke  Stefan Roth  and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2016.

[31] Zhenheng Yang  Peng Wang  Wei Xu  Liang Zhao  and Ramakant Nevatia. Unsupervised learn-
ing of geometry with edge-aware depth-normal consistency. In Association for the Advancement
of Artiﬁcial Intelligence (AAAI)  2018.

[32] Bill Triggs  Philip F McLauchlan  Richard I Hartley  and Andrew W Fitzgibbon. Bundle
adjustment—a modern synthesis. In International workshop on vision algorithms. Springer 
1999.

11

,Jiawang Bian
Zhichao Li
Naiyan Wang
Huangying Zhan
Chunhua Shen
Ming-Ming Cheng
Ian Reid