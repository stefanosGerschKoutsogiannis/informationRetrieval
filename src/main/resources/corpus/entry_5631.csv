2015,Tractable Bayesian Network Structure Learning with Bounded Vertex Cover Number,Both learning and inference tasks on Bayesian networks are NP-hard in general. Bounded tree-width Bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however  while inference on bounded tree-width networks is tractable  the learning problem remains NP-hard even for tree-width~2. In this paper  we propose bounded vertex cover number Bayesian networks as an alternative to bounded tree-width networks. In particular  we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound $k$  in contrast to the general and bounded tree-width cases; on the other hand  we also show that learning problem is W[1]-hard in parameter $k$. Furthermore  we give an alternative way to learn bounded vertex cover number Bayesian networks using integer linear programming (ILP)   and show this is feasible in practice.,Tractable Bayesian Network Structure Learning with

Bounded Vertex Cover Number

Janne H. Korhonen

Helsinki Institute for Information Technology HIIT

Department of Computer Science

University of Helsinki

janne.h.korhonen@helsinki.fi

Pekka Parviainen

Helsinki Institute for Information Technology HIIT

Department of Computer Science

Aalto University

pekka.parviainen@aalto.fi

Abstract

Both learning and inference tasks on Bayesian networks are NP-hard in general.
Bounded tree-width Bayesian networks have recently received a lot of attention as
a way to circumvent this complexity issue; however  while inference on bounded
tree-width networks is tractable  the learning problem remains NP-hard even for
tree-width 2. In this paper  we propose bounded vertex cover number Bayesian
networks as an alternative to bounded tree-width networks. In particular  we show
that both inference and learning can be done in polynomial time for any ﬁxed
vertex cover number bound k  in contrast to the general and bounded tree-width
cases; on the other hand  we also show that learning problem is W[1]-hard in
parameter k. Furthermore  we give an alternative way to learn bounded vertex
cover number Bayesian networks using integer linear programming (ILP)  and
show this is feasible in practice.

1

Introduction

Bayesian networks are probabilistic graphical models representing joint probability distributions
of random variables. They can be used as a model in a variety of prediction tasks  as they enable
computing the conditional probabilities of a set of random variables given another set of random
variables; this is called the inference task. However  to use a Bayesian network as a model for
inference  one must ﬁrst obtain the network. Typically  this is done by estimating the network based
on observed data; this is called the learning task.
Both the inference and learning tasks are NP-hard in general [3  4  6]. One approach to deal with
this issue has been to investigate special cases where these problems would be tractable. That is 
the basic idea is to select models from a restricted class of Bayesian networks that have structural
properties enabling fast learning or inference; this way  the computational complexity will not be
an issue  though possibly at the cost of accuracy if the true distribution is far from the model family.
Most notably  it is known that the inference task can be solved in polynomial time if the network
has bounded tree-width  or more precisely  the inference task is ﬁxed-parameter tractable in the
tree-width of the network. Moreover  this is in a sense optimal  as bounded tree-width is necessary
for polynomial-time inference unless the exponential time hypothesis (ETH) fails [17].

1

The possibility of tractable inference has motivated several recent studies also on learning bounded
tree-width Bayesian networks [2  12  16  19  22]. However  unlike in the case of inference  learning a
Bayesian network of bounded tree-width is NP-hard for any ﬁxed tree-width bound at least 2 [16].
Furthermore  it is known that learning many relatively simple classes such as paths [18] and polytrees
[9] is also NP-hard. Indeed  so far the only class of Bayesian networks for which a polynomial
time learning algorithm is known are trees  i.e.  graphs with tree-width 1 [5] – it appears that our
knowledge about structure classes allowing tractable learning is quite limited.

1.1 Structure Learning with Bounded Vertex Cover Number

In this work  we propose bounded vertex cover number Bayesian networks as an alternative to
the tree-width paradigm. Roughly speaking  we consider Bayesian networks where all pairwise
dependencies – i.e.  edges in the moralised graph – are covered by having at least one node from the
vertex cover incident to each of them; see Section 2 for technical details. Like bounded tree-width
Bayesian networks  this is a parameterised class  allowing a trade-off between the complexity of
models and the size of the space of possible models by varying the parameter k.

Results: complexity of learning bounded vertex cover networks. Crucially  we show that learn-
ing an optimal Bayesian network structure with vertex cover number at most k can be done in
polynomial time for any ﬁxed k. Moreover  vertex cover number provides an upper bound for
tree-width  implying that inference is also tractable; thus  we identify a rare example of a class of
Bayesian networks where both learning and inference are tractable.
Speciﬁcally  our main theoretical result shows that an optimal Bayesian network structure with
vertex cover number at most k can be found in time 4kn2k+O(1) (Theorem 5). However  while the
running time of our algorithm is polynomial with respect to the number of nodes  the degree of the
polynomial depends on k. We show that this is in a sense best we can hope for; that is  we show that
there is no ﬁxed-parameter algorithm with running time f (k) poly(n) for any function f even when
the maximum allowed parent set size is restricted to 2  unless the commonly accepted complexity
assumption FPT (cid:54)= W[1] fails (Theorem 6).
Results: ILP formulation and learning in practice. While we prove that the learning bounded
vertex cover Bayesian network structures can be done in polynomial time  the unavoidable dependence
on k in the degree the polynomial makes the algorithm of our main theorem infeasible for practical
usage when the vertex cover number k increases. Therefore  we investigate using an integer linear
programming (ILP) formulation as an alternative way to ﬁnd optimal bounded vertex cover Bayesian
networks in practice (Section 4). Although the running time of an ILP is exponential in the worst
case  the actual running time in many practical scenarios is signiﬁcantly lower; indeed  most of the
state-of-the-art algorithms for exact learning of Bayesian networks in general [1  8] and with bounded
tree-width [19  22] are based on ILPs. Our experiments show that bounded vertex cover number
Bayesian networks can  indeed  be learned fast in practice using ILP (Section 5).

2 Preliminaries

Directed graphs. A directed graph D = (N  A) consists of a node set N and arc set A ⊆ N × N;
for a ﬁxed node set  we usually identify a directed graph with its arc set A. A directed graph is called
a directed acyclic graph or DAG if it contains no directed cycles. We write n = |N| and uv for arc
(u  v) ∈ A. For u  v ∈ N with uv ∈ A  we say that u is a parent of v and v is a child of u. We write
Av for the parent set of v  that is  Av = {u ∈ N : uv ∈ A}.
Bayesian network structure learning. We consider the Bayesian network structure learning using
the score-based approach [7  14]  where the input consists of the node set N and the local scores
fv(S) for each node v ∈ N and S ⊆ N \ {v}. The task is to ﬁnd a DAG A – the network structure –
We assume that the scores fv are computed beforehand  and that we can access each entry fv(S) in
constant time. We generally consider a setting where only parent sets belonging to speciﬁed sets
Fv ⊆ 2N are permitted. Typically  Fv consists of parent sets of size at most k  in which case we

that maximises the score f (A) =(cid:80)
assume that the scores fv(S) are given only for |S| ≤ k; that is  the size of the input is O(cid:0)n(cid:0)n

v∈N fv(Av).

(cid:1)(cid:1).

k

2

Moralised graphs. For a DAG A  the moralised graph of A is an undirected graph MA = (N  EA) 
where EA is obtained by adding (1) an undirected edge {u  v} to EA for each arc uv ∈ A  and (2) by
adding an undirected edge {u  v} to EA if u and v have a common child  that is  {uw  vw} ⊆ A in
A for some w ∈ A. The edges added to EA due to rule (2) are called moral edges.
Tree-width and vertex cover number. A tree-decomposition of a graph G = (V  E) is a pair
(X   T )  where T is a tree with node set {1  2  . . .   m} and X = {X1  X2  . . .   Xm} is a collection of

subsets of V with(cid:83)m

i=1 Xi = V such that

(a) for each {u  v} ∈ E there is i with u  v ∈ Xi  and
(b) for each v ∈ V the graph T [{i : v ∈ Xi}] is connected.

The width of a tree-decomposition (T X ) is maxi |Xi| − 1. The tree-width tw(G) of a graph G is
the minimum width of a tree-decomposition of G. For a DAG A  we deﬁne the tree-width tw(A) as
the tree-width of the moralised graph MA [12].
For a graph G = (V  E)  a set C ⊆ V is a vertex cover if each edge is incident to at least one vertex
in C. The vertex cover number of a graph τ (G) is the size of the smallest vertex cover in G. As with
tree-width  we deﬁne the vertex cover number τ (A) of a DAG A as τ (MA).
Lemma 1. For a DAG A  we have tw(A) ≤ τ (A).
Proof. By deﬁnition  the moralised graph MA has a vertex cover C of size τ (A). We can construct
a star-shaped tree-decomposition for MA with a central node i with Xi = C and a leaf j with
Xj = C ∪ v for every v ∈ N \ C. Clearly  this tree-decomposition has width τ (A); thus  we have
tw(A) = tw(MA) ≤ τ (A).
Structure learning with parameters. Finally  we give a formal deﬁnition for the bounded tree-
width and bounded vertex cover number Bayesian network structure learning problems. That is  let
p ∈ {τ  tw}; in the bounded-p Bayesian network structure learning  we are given a node set N  local
v∈N fv(Av)
subject to p(A) ≤ k. For both tree-width and vertex cover number  the parameter k also bounds the
maximum parent set size  so we will assume that the local scores fv(S) are given only if |S| ≤ k.
3 Complexity Results

scores fv(S) and an integer k  and the task is to ﬁnd a DAG A maximising score(cid:80)

3.1 Polynomial-time Algorithm

We start by making a few simple observations about the structure of bounded vertex cover number
Bayesian networks. In the following  we slightly abuse the terminology and say that N1 ⊆ N is a
vertex cover for a DAG A if N1 is a vertex cover of MA.
Lemma 2. Let N1 ⊆ N be a set of size k  and let A be a DAG on N. Set N1 is a vertex cover for A
if and only if

(a) for each node v /∈ N1  we have Av ⊆ N1  and
(b) each node v ∈ N1 has at most one parent outside N1.

Proof. (⇒) For (a)  we have that if there were nodes u  v /∈ N1 such that u is the child of v  the
moralised graph MA would have edge {u  v} that is not covered by N1. Likewise for (b)  we have
that if a node u ∈ N1 had parents v  w /∈ N1  then MA would have edge {v  w} not covered by N1.
Thus  both (a) and (b) have to hold if A has vertex cover N1.
(⇐) Since (a) holds  all directed edges in A have one endpoint in N1  and thus the corresponding
undirected edges in MA are covered by N1. Moreover  by (a) and (b)  no node has two parents
outside N1  so all moral edges in MA also have at least one endpoint in N1.

Lemma 2 allows us to partition a DAG with vertex cover number k into a core that covers at most 2k
nodes that are either in a ﬁxed vertex cover or are parents of those nodes (core nodes)  and a periphery

3

Figure 1: (a) Example of a DAG with vertex cover number 4  with sets N1 and N2 as in Lemma 3.
(b) Reduction used in Theorem 6; each edge in the original graph is replaced by a possible v-structure.

containing arcs going into nodes that have no children and all parents in the vertex cover (peripheral
nodes). This is illustrated in Figure 1(a)  and the following lemma formalises the observation.
Lemma 3. Let A be a DAG on N with vertex cover N1 of size k. Then there is a set N2 ⊆ N \ N1
of size at most k and arc sets B and C such that A = B ∪ C and

(a) B is a DAG on N1 ∪ N2 with vertex cover N1  and
(b) C contains only arcs uv with u ∈ N1 and v /∈ N1 ∪ N2.

Proof. First  let N2 =(cid:0)(cid:83)

(cid:1). By Lemma 2  each v ∈ N1 can have at most one parent

v∈N1 Av \ N1
outside N1  so we have |N2| ≤ |N1| ≤ k.
Now let B = {uv ∈ A : u  v ∈ N1 ∪ N2} and C = A \ B. To see that (a) holds for this choice of B 
we observe that the edge set of the moralised graph MB is a subset of the edges in MA  and thus N1
covers all edges of MB. For (b)  the choice of N2 and Lemma 2 ensure that nodes in N \ (N1 ∪ N2)
have no children  and again by Lemma 2 their parents are all in N1.

Dually  if we ﬁx the core and peripheral node sets  we can construct a DAG with bounded vertex cover
number by the selecting the core independently from the parents of the peripheral nodes. Formally:
Lemma 4. Let N1  N2 ⊆ N be disjoint. Let B be a DAG on N1 ∪ N2 with vertex cover N1  and let
C be a DAG on N such that C only contains arcs uv with u ∈ N1 and v /∈ N1 ∪ N2. Then

(b) the score of A is f (A) =(cid:80)

(a) A = B ∪ C is a DAG on N with vertex cover N1  and

v∈N1∪N2 fv(Bv) +(cid:80)

v /∈N1∪N2 fv(Cv).

Proof. To see that (a) holds  we observe that B is acyclic by assumption  and addition of arcs from
C cannot create cycles as there are no outgoing arcs from nodes in N \ (N1 ∪ N2). Moreover  for
v ∈ N1 ∪ N2  there are no arcs ending at v in C  and likewise for v /∈ N1 ∪ N2  there are no arcs
ending at v in B. Thus  we have Av = Bv if v ∈ N1 ∪ N2 and Av = Cv otherwise. This implies that
since conditions of Lemma 2 hold for both B and C  they also hold for A  and thus N1 is a vertex
cover for A. Finally  the preceding observation implies also that fv(Av) = fv(Bv) for v ∈ N1 ∪ N2
and fv(Av) = fv(Cv) otherwise  which implies (b).

vertex cover number at most k. That is  we iterate over all possible(cid:0)n

(cid:1)(cid:0)n−k

Lemmas 3 and 4 give the basis of our strategy for ﬁnding an optimal Bayesian network structure with

(cid:1) = O(n2k) choices for

k

k

sets N1 and N2; for each choice  we construct the optimal core and periphery as follows  keeping
track of the best found DAG A∗:
Step 1. To ﬁnd the optimal core B  we construct a Bayesian network structure learning instance on
N1 ∪ N2 by removing nodes outside N1 ∪ N2 and restricting the possible choices of parent
sets so that Fv = 2N1 for all v ∈ N2  and Fv = {S ⊆ N1∪N2 : |S ∩ N2| ≤ 1} for v ∈ N1.
By Lemma 2  any solution for this instance is a DAG with vertex cover N1. Moreover  this
instance has 2k nodes  so it can be solved in time O(k222k) using the dynamic programming
algorithm of Silander and Myllymäki [23].

4

(b)(a)euveuvN1N2Step 2. To construct the periphery C  we compute the value ˆfv(N1) = maxS⊆N1 fv(S) and select
corresponding best parent set choice Cv for each v /∈ N1 ∪ N2; this can be done in time in
O(nk2k) time using the dynamic programming algorithm of Ott and Miyano [21].

Step 3. We check if f (B ∪ C) > f (A∗)  and replace A∗ with B ∪ C if this holds.
By Lemma 4(a)  all DAGs considered by the algorithm are valid solutions for Bayesian network
structure learning with bounded vertex cover number  and by Lemma 4(b)  we can ﬁnd the optimal
solution for ﬁxed N1 and N2 by optimising the choice of the core and the periphery separately.
Moreover  by Lemma 3 each bounded vertex cover DAG is included in the search space  so we are
guaranteed to ﬁnd the optimal one. Thus  we have proven our main theorem:
Theorem 5. Bounded vertex cover number Bayesian network structure learning can be solved in
time 4kn2k+O(1).

3.2 Lower Bound

k

Although the algorithm presented in the previous section runs in polynomial time in n  the degree of
the polynomial depends on the size of vertex cover k  which poses a serious barrier to practical use
when k grows. Moreover  the algorithm is essentially optimal in the general case  as the input has

(cid:1)(cid:1) when parent sets of size at most k are allowed. However  in practice one often assumes

size Ω(cid:0)n(cid:0)n

that a node can have at most  say  2 or 3 parents. Thus  it makes sense to consider settings where
the input is restricted  by e.g. considering instances where parent set size is bounded from above by
some constant w while allowing vertex cover number k to be higher. In this case  we might hope to
do better  as the input size is not a restricting factor.
Unfortunately  we show that it is not possible to obtain a algorithm where the degree of the polynomial
does not depend on k even when the maximum parent set size is limited to 2  that is  there is no
algorithm with running time g(k) poly(n) for any function g  unless the widely believed complexity
assumption FPT (cid:54)= W[1] fails. Speciﬁcally  we show that Bayesian network structure learning
with bounded vertex cover number is W[1]-hard when restricted to instances with parent set size 2 
implying the above claim. For full technical details on complexity classes FPT and W[1] and the
related theory  we refer the reader to standard texts on the topic [11  13  20]; for our result  it sufﬁces
to note that the assumption FPT (cid:54)= W[1] implies that ﬁnding a k-clique from a graph cannot be done
in time g(k) poly(n) for any function g.
Theorem 6. Bayesian network structure learning with bounded vertex cover number is W[1]-hard in
parameter k  even when restricted to instances with maximum parent set size 2.

Proof. We prove the result by a parameter-preserving reduction from clique  which is known to
be W[1]-hard [10]. We use the same reduction strategy as Korhonen and Parviainen [16] use in
proving that the bounded tree-width version of the problem is NP-hard. That is  given an instance
(G = (V  E)  k) of clique  we construct a new instance of bounded vertex cover number Bayesian
network structure learning as follows. The node set of the instance is N = V ∪ E. The parent scores
are deﬁned by setting fe({u  v}) = 1 for each e = {u  v} ∈ E  and fv(S) = 0 for all other v and S;
see Figure 1(b). Finally  the vertex cover size is required to be at most k. Clearly  the new instance
can be constructed in polynomial time.
It now sufﬁces to show that the original graph G has a clique of size k if and only if the optimal DAG

N with vertex cover number at most k has score(cid:0)k
are now clearly covered by C. Furthermore  since C is a clique in G  there are(cid:0)k
non-empty parent set  giving f (A) =(cid:0)k
There must be at least(cid:0)k
C ⊆ V and there are at least(cid:0)k

(cid:1) nodes with a
(⇒) Assume G has a k-clique C ⊆ V . Let A be a DAG on N obtained by setting Ae = {u  v} for
each e = {u  v} ⊆ C  and Av = ∅ for all other nodes v ∈ N. All edges in the moralised graph MA
(cid:0)k
(cid:1).
(cid:1) nodes e = {u  v} ∈ E such that Ae = {u  v}  as these are the only nodes
(⇐) Assume now that there is a DAG A on N with vertex cover number k and a score f (A) ≥
that can contribute to a positive score. Each of these triangles Te = {e  u  v} for e = {u  v} must
contain at least two nodes from a minimum vertex cover C; without loss of generality  we may
assume that these nodes are u and v  as e cannot cover any other edges. However  this means that

(cid:1) edges e ⊆ C  implying that C must be a k-clique in G.

(cid:1):

2

(cid:1).

2

2

2

5

2

2

4

Integer Linear Programming

To complement the combinatorial algorithm of Section 3.1  we will formulate the bounded vertex
cover number Bayesian network structure learning problem as an integer linear program (ILP).
Without loss of generality  we may assume that nodes are labeled with integers [n].
As a basis for the formulation  let zSv be a binary variable that takes value 1 when S is the parent set
of v and 0 otherwise. The objective function for the ILP is

(cid:88)

(cid:88)

v∈N

S∈Fv

max

fv(S)zSv .

To ensure that the variables zSv encode a valid DAG  we use the standard constraints introduced by

Jaakkola et al. [15] and Cussens [8]:(cid:88)
(cid:88)

(cid:88)

S∈Fv

v∈W

S∈Fv
S∩W =∅

zSv = 1

∀v ∈ N

zSv ≥ 1

∀W ⊆ N : |W| ≥ 1

zSv ∈ {0  1} ∀v ∈ N  S ∈ Fv.

(1)

(2)

(3)

Now it remains to bound the vertex cover number of the moralised graph. We introduce two sets
of binary variables. The variable yuv takes value 1 if there is an edge between nodes u and v in
the moralised graph and 0 otherwise. The variable cu takes value 1 if the node u is a part of the
vertex cover and 0 otherwise. By combining a construction of the moralised graph and a well-known
formulation for vertex cover  we get the following:

(cid:88)

(cid:88)

zSv +

S∈Fv : u∈S

T∈Fu : v∈T

zT u − yuv ≤ 0
zSv − yuw ≤ 0
yuv − cu − cv ≤ 0
cu ≤ k

(cid:88)

∀u  v ∈ N : u < v
∀v ∈ N  S ∈ Fv : u  w ∈ S  u < w
∀u  v ∈ N : u < v

u∈N
yuv  cu ∈ {0  1} ∀u  v ∈ N.

(4)

(5)
(6)
(7)

(8)

The constraints (4) and (5) guarantee that y-variables encode the moral graph. The constraint (6)
guarantees that if there is an edge between u and v in the moral graph then either u or v is included
in the vertex cover. Finally  the constraint (7) bounds the size of the vertex cover.

5 Experiments

We implemented both the combinatorial algorithm of Section 3.1 and the ILP formulation of Section 4
to benchmark the practical performance of the algorithms and test how good approximations bounded
vertex cover DAGs provide. The combinatorial algorithm was implemented in Matlab and is available
online1. The ILPs were implemented using CPLEX Python API and solved using CPLEX 12. The
implementation is available as a part of TWILP software2.

Combinatorial algorithm. As the worst- and best-case running time of the combinatorial algorithm
are the same  we tested it with synthetic data sets varying the number of nodes n and the vertex cover
bound k  limiting each run to at most 24 hours. The results are shown in Figure 2. With reasonable
vertex cover number bounds the polynomial-time algorithm scales only up to about 15 nodes; this is
mainly due to the fact that  while the running time is polynomial in n  the degree of the polynomial
depends on k and when k grows  the algorithm becomes quickly infeasible.

1http://research.cs.aalto.ﬁ/pml/software/VCDP/
2http://bitbucket.org/twilp/twilp

6

Figure 2: Running times of the polynomial time algorithm. Number of nodes varies from 13 to 16
and the vertex cover number from 1 to 5. For n = 15 and n = 16 with k = 5  the algorithm did not
ﬁnish in 24 hours.

Integer linear program. We ran our experiments using a union of the data sets used by Berg et
al. [2] and those provided at GOBNILP homepage3. We benchmarked the results against other
ILP-based algorithms  namely GOBNILP [8] for learning Bayesian networks without any restrictions
to the structure and TWILP [22] for learning bounded tree-width Bayesian networks. In our tests 
each algorithm was given 4 hours of CPU time. Figure 3 shows results for selected data sets. Due to
space reasons  full results are reported in the supplement.
The results show that optimal DAGs with moderate vertex cover number (7 for ﬂag  6 for carpo10000)
tend to have higher scores than optimal trees. This suggests that often one can trade speed for
accuracy by moving from trees to bounded vertex cover number DAGs. We also note that bounded
vertex cover number DAGs are usually learned quickly  typically at least two orders-of-magnitude
faster than bounded tree-width DAGs. However  bounded tree-width DAGs are a less constrained
class  and thus in multiple cases the best found bounded tree-width DAG has better score than the
corresponding bounded vertex cover number DAG even when the bounded tree-width DAG is not
proven to be optimal. This seems to be the case also if we have mismatching bound  say  5 for
tree-width and 10 for vertex cover number.
Finally  we notice that ILP solves easily problem instances with  say  60 nodes and vertex cover bound
8; see the results for carpo10000 data set. Thus  in practice ILP scales up to signiﬁcantly larger data
sets and vertex cover number bounds than the combinatorial algorithm of Section 3.1. Presumably 
this is due to the fact that ILP solvers tend to use heuristics that can quickly prune out provably
non-optimal parts of choices for the vertex cover  while the combinatorial algorithm considers them
all.

6 Discussion

We have shown that bounded vertex cover number Bayesian networks both allow tractable inference
and can be learned in polynomial time. The obvious point of comparison is the class of trees  which
has the same properties. Structurally these two classes are quite different. In particular  neither is a
subclass of the other – DAGs with vertex cover number k > 1 can contain dense substructures  while
a path of n nodes (which is also a tree) has a vertex cover number (cid:98)n/2(cid:99) = Ω(n).
In contrast with trees  bounded vertex cover number Bayesian networks have a densely connected
“core”   and each node outside the core is either connected to the core or it has no connections. Thus 
we would expect them to perform better than trees when the “real” network has a few dense areas
and only few connections between nodes outside these areas. On the other hand  bounding the vertex
cover number bounds the total size of the core area  which can be problematic especially in large
networks when some parts of the network are not represented in the minimum vertex cover.

3http://www.cs.york.ac.uk/aig/sw/gobnilp/

7

12345k100101102103104105time(s)n=16n=15n=14n=13Figure 3: Results for selected data sets. We report the score for the optimal DAG without structure
constraints  and for the optimal DAGs with bounded tree-width and bounded vertex cover when the
bound k changes  as well as the running time required for ﬁnding the optimal DAG in each case. If
the computations were not ﬁnished at the time limit of 4 hours  we show the score of the best DAG
found so far; the shaded area represents the unexplored part of the search space  that is  the upper
bound of the shaded area is the best score upper bound proven by the ILP solver.

We also note that bounded vertex cover Bayesian networks have a close connection to naive Bayes
classiﬁers. That is  variables outside a vertex cover are conditionally independent of each other
given the vertex cover. Thus  we can replace the vertex cover by a single variable whose states are a
Cartesian product of the states of the vertex cover variables; this star-shaped network can then be
viewed as a naive Bayes classiﬁer.
Finally  we note some open question related to our current work. From a theoretical perspective 
we would like to classify different graph parameters in terms of complexity of learning. Ideally  we
would want to have a graph parameter that has a ﬁxed-parameter learning algorithm when we bound
the maximum parent set size  circumventing the barrier of Theorem 6. From a practical perspective 
there is clearly room for improvement in efﬁciency of our ILP-based learning algorithm; for instance 
GOBNILP uses various optimisations beyond the basic ILP encoding to speed up the search.

Acknowledgments

We thank James Cussens for fruitful discussions. This research was partially funded by the Academy
of Finland (Finnish Centre of Excellence in Computational Inference Research COIN  251170).
The experiments were performed using computing resources within the Aalto University School of
Science “Science-IT” project.

8

12345678910k−16400−16200−16000−15800−15600−15400−15200scoreabalone(n=9) scores12345678910k0100101102103104time(s)abalone(n=9) runningtimes12345678910k−3100−3050−3000−2950−2900−2850−2800−2750−2700scoreﬂag(n=29) scores12345678910k0100101102103104time(s)ﬂag(n=29) runningtimes12345678910k−230000−220000−210000−200000−190000−180000−170000−160000−150000scorecarpo10000(n=60) scores12345678910k0100101102103104time(s)carpo10000(n=60) runningtimesNostructureconstraintsBoundedtree-widthBoundedvertexcoverReferences
[1] Mark Bartlett and James Cussens. Advances in Bayesian network learning using integer programming. In

29th Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2013.

[2] Jeremias Berg  Matti Järvisalo  and Brandon Malone. Learning Optimal Bounded Treewidth Bayesian
Networks via Maximum Satisﬁability. In 17th International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS)  2014.

[3] David M. Chickering. Learning Bayesian networks is NP-Complete. In Learning from Data: Artiﬁcial

Intelligence and Statistics V  pages 121–130. Springer-Verlag  1996.

[4] David M. Chickering  David Heckerman  and Chris Meek. Large-sample learning of Bayesian networks is

NP-Hard. Journal of Machine Learning Research  5:1287–1330  2004.

[5] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE

Transactions on Information Theory  14(3):462–467  1968.

[6] Gregory. F. Cooper. The computational complexity of probabilistic inference using Bayesian belief

networks. Artiﬁcial Intelligence  42:393–405  1990.

[7] Gregory F. Cooper and Edward Herskovits. A Bayesian method for the induction of probabilistic networks

from data. Machine Learning  9:309–347  1992.

[8] James Cussens. Bayesian network learning with cutting planes. In 27th Conference on Uncertainty in

Artiﬁcial Intelligence (UAI)  2011.

[9] Sanjoy Dasgupta. Learning polytrees. In 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) 

1999.

[10] Rodney G. Downey and Michael R. Fellows. Parameterized computational feasibility.

Mathematics II  pages 219–244. Birkhauser  1994.

In Feasible

[11] Rodney G. Downey and Michael R. Fellows. Parameterized complexity. Springer-Verlag  1999.
[12] Gal Elidan and Stephen Gould. Learning bounded treewidth Bayesian networks. Journal of Machine

Learning Research  9:2699–2731  2008.

[13] Jörg Flum and Martin Grohe. Parameterized complexity theory. Springer-Verlag  2006.
[14] David Heckerman  Dan Geiger  and David M. Chickering. Learning Bayesian networks: The combination

of knowledge and statistical data. Machine Learning  20(3):197–243  1995.

[15] Tommi Jaakkola  David Sontag  Amir Globerson  and Marina Meila. Learning bayesian network structure
using LP relaxations. In 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
2010.

[16] Janne H. Korhonen and Pekka Parviainen. Learning bounded tree-width Bayesian networks. In 16th

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2013.

[17] Johan H. P. Kwisthout  Hans L. Bodlaender  and L. C. van der Gaag. The necessity of bounded treewidth
for efﬁcient inference in Bayesian networks. In 19th European Conference on Artiﬁcial Intelligence (ECAI) 
2010.

[18] Chris Meek. Finding a path is harder than ﬁnding a tree. Journal of Artiﬁcial Intelligence Research  15:

383–389  2001.

[19] Siqi Nie  Denis Deratani Maua  Cassio Polpo de Campos  and Qiang Ji. Advances in Learning Bayesian
Networks of Bounded Treewidth. In Advances in Neural Information Processing Systems 27 (NIPS)  2014.

[20] Rolf Niedermeier. Invitation to ﬁxed-parameter algorithms. Oxford University Press  2006.
[21] Sascha Ott and Satoru Miyano. Finding optimal gene networks using biological constraints. Genome

Informatics  14:124–133  2003.

[22] Pekka Parviainen  Hossein Shahrabi Farahani  and Jens Lagergren. Learning Bounded Tree-width Bayesian
Networks using Integer Linear Programming. In 17th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS)  2014.

[23] Tomi Silander and Petri Myllymäki. A simple approach for ﬁnding the globally optimal Bayesian network

structure. In 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2006.

9

,Kareem Amin
Afshin Rostamizadeh
Umar Syed
Janne Korhonen
Pekka Parviainen
Ashish Kumar
Saurabh Gupta
David Fouhey
Sergey Levine
Jitendra Malik