2019,A New Distribution on the Simplex with Auto-Encoding Applications,We construct a new distribution for the simplex using the Kumaraswamy distribution and an ordered stick-breaking process. We explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry (exchangeability) under the same conditions as the well-known Dirichlet. Like the Dirichlet  the new distribution is adept at capturing sparsity but  unlike the Dirichlet  has an exact and closed form reparameterization--making it well suited for deep variational Bayesian modeling. We demonstrate the distribution's utility in a variety of semi-supervised auto-encoding tasks. In all cases  the resulting models achieve competitive performance commensurate with their simplicity  use of explicit probability models  and abstinence from adversarial training.,A New Distribution on the Simplex
with Auto-Encoding Applications

Andrew Stirn∗  Tony Jebara†  David A Knowles‡

Department of Computer Science

Columbia University
New York  NY 10027

{andrew.stirn jebara daknowles}@cs.columbia.edu

Abstract

We construct a new distribution for the simplex using the Kumaraswamy dis-
tribution and an ordered stick-breaking process. We explore and develop the
theoretical properties of this new distribution and prove that it exhibits symmetry
(exchangeability) under the same conditions as the well-known Dirichlet. Like
the Dirichlet  the new distribution is adept at capturing sparsity but  unlike the
Dirichlet  has an exact and closed form reparameterization–making it well suited
for deep variational Bayesian modeling. We demonstrate the distribution’s utility
in a variety of semi-supervised auto-encoding tasks. In all cases  the resulting
models achieve competitive performance commensurate with their simplicity  use
of explicit probability models  and abstinence from adversarial training.

1

Introduction

The Variational Auto-Encoder (VAE) [12] is a computationally efﬁcient approach for performing
variational inference [11  27] since it avoids per-data-point variational parameters through the use of
an inference network with shared global parameters. For models where stochastic gradient variational
Bayes requires Monte Carlo estimates in lieu of closed-form expectations  [23  12] note that low-
variance estimators can be calculated from gradients of samples with respect to the variational
parameters that describe their generating densities. In the case of the normal distribution  such
gradients are straightforward to obtain via an explicit  tractable reparameterization  which is often
referred to as the “reparameterization trick”. Unfortunately  most distributions do not admit such
a convenient reparameterization. Computing low-bias and low-variance stochastic gradients is an
active area of research with a detailed breakdown of current methods presented in [4]. Of particular
interest in Bayesian modeling is the well-known Dirichlet distribution that often serves as a conjugate
prior for latent categorical variables. Perhaps the most desirable property of a Dirichlet prior is its
ability to induce sparsity by concentrating mass towards the corners of the simplex. In this work  we
develop a surrogate distribution for the Dirichlet that offers explicit  tractable reparameterization  the
ability to capture sparsity  and has barycentric symmetry (exchangeability) properties equivalent to
the Dirichlet.
Generative processes can be used to infer missing class labels in semi-supervised learning. The
ﬁrst VAE-based method that used deep generative models for semi-supervised learning derived two
variational objectives for the same the generative process–one for when labels are observed and one
for when labels are latent–that are jointly optimized [13]. As they note  however  the variational
distribution over class labels appears only in the objective for unlabeled data. Its absence from

∗jointly afﬁliated with New York Genome Center
†jointly afﬁliated with Spotify Technology S.A.
‡jointly afﬁliated with Columbia University’s Data Science Institute and the New York Genome Center

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the labeled-data objective  as they point out  results from their lack of a (Dirichlet) prior on the
(latent) labels. We suspect they neglected to specify this prior  because  at the time  it would have
rendered inference intractable. They ameliorate this shortcoming by introducing a discriminative third
objective  the cross-entropy of the variational distribution over class labels  which they compute over
the labeled data. They then jointly optimize the two variational objectives after adding a scaled version
of the cross-entropy term. Our work builds on [13]  while offering some key improvements. First 
we remove the need for adding an additional discriminative loss through our use of a Dirichlet prior.
We overcome intractability using our proposed distribution as an approximation for the Dirichlet
posterior. Naturally  our generative process is slightly different  but it allows us to consider only
unmodiﬁed variational objectives. Second  we do not stack models together. Kingma et al.’s best
results utilized a standard VAE (M1) to learn a latent space upon which their semi-supervised VAE
(M2) was ﬁt. For SVHN data  they perform dimensionality reduction with PCA prior to ﬁtting M1.
We abandon the stacked-model approach in favor of training a single model with more expressive
recognition and generative networks. Also  we use minimal preprocessing (rescaling pixel intensities
to [0  1]).
Use of the Kumaraswamy distribution [14] by the machine learning community has only occurred
in the last few years. It has been used to ﬁt Gaussian Mixture Models  for which a Dirichlet prior
is part of the generative process  with VAEs [19]. To sample mixture weights from the variational
posterior  they recognize they can decompose a Dirichlet into its stick-breaking Beta distributions
and approximate them with the Kumaraswamy distribution. We too employ the same stick-breaking
decomposition coupled with Kumaraswamy approximations. However  we improve on this technique
by expounding and resolving the order-dependence their approximation incurs. As we detail in
section 2  using the Kumaraswamy for stick-breaking is not order agnostic (exchangeable); the
generated variable has a density that depends on ordering. We leverage the observation that one
can permute a Dirichlet’s parameters  perform the stick-breaking sampling procedure with Beta
distributions  and undo the permutation on the sampled variable without affecting its density. Those
same authors also use this Beta-Kumaraswamy stick-breaking approximation to ﬁt a Bayesian non-
parametric model with a VAE [20]. Here too  they do not account for the impact ordering has on their
approximation. Their latent space  being non-parametric  grows in dimensions when it insufﬁciently
represents the data. As we demonstrate in section 2.2 and ﬁg. 1  approximating sparse Dirichlet
samples with the Kumaraswamy stick-breaking decomposition without accounting for the ordering
dependence produces a large bias in the samples’ last dimension. We conjecture that their Bayesian
non-parametric model would utilize fewer dimensions with our proposed distribution and would be
an interesting follow up to our work.

Figure 1: Sampling bias for a 5-dimensional sparsity-inducing Dirichlet approximation using α =
5 (1  1  1  1  1). We maintain histograms for each sample dimension for three methods: Dirichlet 
1
Kumaraswamy stick-breaks with a ﬁxed order  Kumaraswamy stick-breaks with a random ordering.
Note the bias on the last dimension when using a ﬁxed order. Randomizing order eliminates this bias.

2 A New Distribution on the Simplex

The stick-breaking process is a sampling procedure used to generate a K dimensional random variable
in the K − 1 simplex. The process requires sampling from K − 1 (often out of K) distributions each
with support over [0  1]. Let pi(v; ai  bi) be some distribution for v ∈ [0  1] parameterized by ai and
bi. Let o be some ordering (permutation) of {1  . . .   K}. Then  algorithm 1 captures a generalized
stick-breaking process. The necessity for incorporating ordering will become clear in section 2.1.

2

Algorithm 1 A Generalized Stick-Breaking Process
Require: K ≥ 2  base distributions pi(v; ai  bi) ∀ i ∈ {1  . . .   K}  and some ordering o

Sample: vo1 ∼ po1 (v; ao1   bo1)
Assign: xo1 ← vo1   i ← 2
(cid:16)
1 −(cid:80)i−1
while i < K do
Assign: xoK ← 1 −(cid:80)K−1

Sample: voi ∼ poi (v; aoi  boi)
Assign: xoi ← voi

end while

j=1 xoj

return x

(cid:17)

  i ← i + 1

j=1 xoj

From a probabilistic perspective  algorithm 1 recursively creates a joint distribution p(xo1   . . .   xoK−1)
from its chain-rule factors p(xo1)p(xo2|xo1)p(xo3|xo2  xo1) . . . p(xoK−1|xoK−2  . . . xo1). Note  how-
ever  that xoK does not appear in the distribution. Its absence occurs because it is deterministic
given xo1  . . .   xoK−1 (the K − 1 degrees of freedom for the K − 1 simplex). Each iteration of the
while loop generates p(xoi|xoi−1  . . .   xo1) by sampling poi(v; aoi  boi) and a change-of-variables
transform Ti : [0  1]i → [0  1]i to the samples collected thus far. This transform and its inverse are

(cid:32)
(cid:32)

Ti(xo1  . . .   xoi−1  voi) =

xo1  . . .   xoi−1   voi

T −1

i

(xo1  . . .   xoi−1  xoi) =

xo1   . . .   xoi−1   xoi

(cid:17)(cid:33)
(cid:17)−1(cid:33)

.

xoj

xoj

(cid:16)
1 − i−1(cid:88)
(cid:16)
1 − i−1(cid:88)

j=1

j=1

(1)

(2)

Applying the change-of-variables formula to the conditional distribution generated by a while loop
iteration  allows us to formulate the conditional as an expression involving just pi(v; ai  bi)  which
we assume access to  and det(JT

is the Jacobian of eq. (2).

)  where JT

−1
i

−1
i

) = poi(v; aoi  boi) ·(cid:16)

1 − i−1(cid:88)

(cid:17)−1

xoj

j=1

p(xoi|xoi−1  . . .   xo1) = p(voi|xoi−1   . . .   xo1 ) · det(JT

−1
i

pi(v; ai  bi) ≡ Beta(x; αi (cid:80)K

A common application of the stick-breaking process is to construct a Dirichlet sample from
Beta samples. If we wish to sample from Dirichlet(x; α)  with α ∈ RK
++  it sufﬁces to assign
j=i+1 αj). With this assignment  algorithm 1 will return a Dirichlet

distributed x with density

(cid:16)(cid:80)K
(cid:81)K

Γ

i=1 αoi

(cid:17)

K(cid:89)

αoi−1
oi

.

x

i=1 Γ(αoi)

p(xo1   . . .   xoK ; α) =

This form requires substituting for algorithm 1’s ﬁnal assignment xoK ≡ 1 −(cid:80)K−1

i=1 xoK . Upon
inspection  the Dirichlet distribution is order agnostic (exchangeable). In other words  given any
ordering o  the random variable returned from algorithm 1 can be permuted to (x1  . . .   xK) (along
with the parameters) without modifying its probability density. This convenience arises from the Beta
distribution’s form.

Theorem 1 For K ≥ 2 and pi(v; ai  bi) ≡ Beta(x; αi (cid:80)K

j=i+1 αj)  algorithm 1 returns a random

i=1

variable whose density is captured via the Dirichlet distribution.

A proof of theorem 1 appears in section 7.1 (appendix). A variation of this proof also appears in [5].

2.1 The Kumaraswamy distribution

The Kumaraswamy(a  b) [14]  a Beta-like distribution  has two parameters a  b > 0 and support
for x ∈ [0  1] with PDF f (x; a  b) = abxa−1(1 − xa)b−1 and CDF F (x; a  b) = 1 − (1 − xa)b.
With this analytically invertible CDF  one can reparameterize a sample u from the continuous

3

−1
i

a Dirichlet sample from Beta samples  we let pi(v; ai  bi) ≡ Kumaraswamy(x; αi (cid:80)K

Uniform(0  1) via the transform T (u) = (1− (1− u)1/b)1/a such that T (u) ∼ Kumaraswamy(a  b).
Unfortunately  this convenient reparameterization comes at a cost when we derive p(xo1  . . .   xoK ; α) 
which captures the density of the variable returned by algorithm 1. If  in a manner similar to generating
j=i+1 αj)  then
the resulting variable’s density is no longer order agnostic (exchangeable). The exponential in the
Kumaraswamy’s (1 − xa) term that admits analytic inverse-CDF sampling  can no longer cancel out
) terms as the (1 − x) term in the Beta analog could. In the simplest case  the 1-simplex
det(JT
(K = 2)  the possible orderings for algorithm 1 are o ∈ O = {{1  2} {2  1}}. Indeed  algorithm 1
returns two distinct densities according to their respective orderings:
1 − xα1
1 − x1
1 − xα2
1 − x2

(cid:33)α2−1
(cid:33)α1−1

f12(x; a  b) = α1α2xα1−1

(cid:32)
(cid:32)

xα2−1

2

f21(x; a  b) = α1α2xα1−1

1

xα2−1

2

1

2

(3)

(4)

1

.

In section 7.2 of the appendix  we derive f12 and f21 as well as the distribution for the 2-simplex 
which has orderings o ∈ O = {{1  2  3} {1  3  2} {2  1  3} {2  3  1} {3  1  2} {3  2  1}}. For
K > 3  the algebraic book-keeping gets rather involved. We thus rely on algorithm 1 to succinctly
represent the complicated densities over the simplex that describe the random variables generated by
a stick-breaking process using the Kumaraswamy distribution as the base (stick-breaking) distribution.
Our code repository § contains a symbolic implementation of algorithm 1 with the Kumaraswamy
that programmatically keeps track of the algebra.

2.2 The multivariate Kumaraswamy

We posit that a good surrogate for the Dirichlet will exhibit symmetry (exchangeability) properties
identical to the Dirichlet it is approximating. If our stick-breaking distribution  pi(v; ai  bi)  cannot
achieve symmetry for all values ai = bi > 0  then it is possible that the samples will exhibit bias
(ﬁg. 1). If x ∼ Beta(a  b)  then (1−x) ∼ Beta(b  a). It follows then that when a = b  p(x) = p(1−x).
Unfortunately  Kumaraswamy(a  b) does not admit such symmetry for all a = b > 0. However  hope
is not lost. From [6  8]  we have lemma 1.

Lemma 1 Given a function f of n variables  one can induce symmetry by taking the sum of f over
all n! possible permutations of the variables.

Kumaraswamy(x; αi (cid:80)K

If we deﬁne fo(xo1   . . .   xoK ; αo1  . . .   αoK ) to be the joint density of the K-dimensional ran-
dom variable returned from algorithm 1 with stick-breaking base distribution as pi(v; ai  bi) ≡
j=i+1 αj) and some ordering o  then our proposed distribution for the
(K − 1)-simplex is

MV-Kumaraswamy(x; α) =

E

o∼Uniform(O)

[fo(xo1   . . .   xoK ; αo1  . . .   αoK )] 

(5)

where MV-Kumaraswamy stands for Multivariate Kumaraswamy. Here  O is the set of all possible
orderings (permutations) of {1  . . .   K}. In the context of [8]  we create a U-statistic over the
variables x  α. The expectation in eq. (5) is a summation since we are uniformly sampling o from a
discrete set. We therefore can apply lemma 1 to eq. (5) to prove corollary 1.
Corollary 1 Let S ⊆ {1  . . .   K} be the set of indices i where for i (cid:54)= j we have αi = αj. Deﬁne
A = {1  . . .   K} \ S. Then  Eo∼Uniform(O)[fo(xo1   . . .   xoK ; αo1  . . .   αoK )] is symmetric across
barycentric axes xa ∀ a ∈ A.
While the factorial growth (|O| = K!) for full symmetry is undesirable  we expect approximate
symmetry should arise  in expectation  after O(K) samples. Since the problematic bias occurs
during the last stick break  each label ideally experiences an ordering where it is not last; this occurs
with probability K−1
K−1 draws from Uniform(O).

K . Thus  a label is not last  in expectation  after K

§https://github.com/astirn/MV-Kumaraswamy

4

Therefore  to satisfy this condition for all labels  one needs K2
K−1 = O(K) samples  in expectation. An
alternative  which we discuss and demonstrate below in ﬁg. 4  would be to use the K cyclic orderings
(e.g. {{1  2  3} {2  3  1} {3  1  2}} for K = 3) to achieve approximate symmetry (exchangeability).
In ﬁg. 2  we provide 1-simplex examples for varying α that demonstrate the effect ordering has on the
Kumaraswamy distributions f12(x; α) and f21(x; α) (respectively in eqs. (3) and (4)). In each exam-
ple  we plot the symmetrized versions arising from our proposed distribution Eo[fo(x; α)] (eq. (5)).
For reference  we plot the corresponding Dirichlet(x; α)  which is equivalent to Beta(x1; α1  α2)
for the 1-simplex. Qualitatively  we observe how effectively our proposed distribution resolves the
differences between f12 and f21 and yields a E[fo(x; α)] ≈ Dirichlet(x; α).

Figure 2: Kumaraswamy asymmetry and symmetrization examples on the 1-simplex.

In ﬁg. 3  we employ Beta distributed stick breaks to generate a Dirichlet random variable. In this
example  we pick an α such that the resulting density should be symmetric only about the barycentric
x1 axis. Furthermore  because the resulting density is a Dirichlet  the densities arising from all
possible orderings should be identical with identical barycentric symmetry properties. The ﬁrst row
contains densities. The subsequent rows measure asymmetry across the speciﬁed barycentric axis
by computing the absolute difference of the PDF folded along that axis. The ﬁrst column is for
expectation over all possible orderings. The second column is for the expectation over the cyclic
orderings. Each column thereafter represents a different stick-breaking order. Indeed  we ﬁnd that the
Dirichlet has an order agnostic density with symmetry only about the barycentric x1 axis.

Figure 3: 2-simplex with Beta sticks

Figure 4: 2-simplex with Kumaraswamy sticks

In ﬁg. 4  we employ the same methodology with the same α as in ﬁg. 3  but this time we use
Kumaraswamy distributed stick breaks. Note the signiﬁcant variations among the densities resulting
from the different orderings. It follows that symmetry/asymmetry too vary with respect to ordering.
We only see the desired symmetry about the barycentric x1 axis when we take the expectation over
all orderings. This example qualitatively illustrates corollary 1. However  we do achieve approximate
symmetry when we average over the K cyclic orderings–suggesting we can  in practice  get away
with linearly scaling complexity.

3 Gradient Variance

We compare our method’s gradient variance to other non-explicit gradient reparameterization methods:
Implicit Reparameterization Gradients (IRG) [4]  RSVI [18]  and Generalized Reparameterization

5

Gradient (GRG) [22]. These works all seek gradient methods with low variance. In ﬁg. 5  we compare
MV-Kumaraswamy’s (MVK) gradient variance to these other methods by leveraging techniques and
code from [18]. Speciﬁcally  we consider their test that ﬁts a variational Dirichlet posterior to
Categorical data with a Dirichlet prior. In this conjugate setting  true analytic gradients can be
computed. Their reported ‘gradient variance’ is actually the mean square error with respect to the true
gradient. In our test  however  we are ﬁtting a MV-Kumaraswamy variational posterior. Therefore 
we compute gradient variance  for all methods  according to variance’s more common deﬁnition. Our
tests show that IRG and RSVI (B = 10) offer similar variance; this result matches ﬁndings in [4].

Figure 5: Variance of the ELBO’s gradient’s ﬁrst dimension for GRG [22]  RSVI [18]  IRG [4] 
and MVK (ours) when ﬁtting a variational posterior to Categorical data with 100 dimensions and
a Dirichlet prior. They ﬁt a Dirichlet. We ﬁt a MV-Kumaraswamy using K = 100 samples from
Uniform(O) to Monte-Carlo approximate the full expectation; this corresponds to linear complexity.

4 A single generative model for semi-supervised learning

We demonstrate the utility of the MV-Kumaraswamy in the context of a parsimonious generative
model for semi-supervised learning  with observed data x  partially observable classes/labels y with
prior π and latent variable z  all of which are local to each data point. We specify 
z ∼ N (z; 0  I) 
x|y  z ∼ p(x|fθ(y  z)) 

π ∼ Dirichlet(π; α) 
y|π ∼ Discrete(y; π) 

where fθ(y  z) is a neural network  with parameters θ  operating on the latent variables. For observable
y  the evidence lower bound (ELBO) for a mean-ﬁeld posterior approximation q(π  z) = q(π)q(z) is

ln p(x  y) ≥ E

[ln p(x|fθ(y  z)) + ln πy] − DKL(q(π) || p(π)) − DKL(q(z) || p(z))

(6)
For latent y  we can derive an alternative ELBO that corresponds to the same generative process
of eq. (6)  by reintroducing y via marginalization. We derive eqs. (6) and (7) in section 7.3 of the
appendix.

ln p(x) ≥ E

p(x|fθ(y  z))πy

(cid:105) − DKL(q(π) || p(π)) − DKL(q(z) || p(z))
(cid:88)

Lu(x  φ  θ)

q(π z)

≡Ll(x  y  φ  θ).

(cid:104)

(cid:88)

q(π z)

ln
≡Lu(x  φ  θ)

y

(cid:88)
(cid:88)

(x y)∈L

L =

1
|L|
≈ 1
B

(7)
Let L be our set of labeled data and U be our unlabeled set. We then consider a combined objective

Ll(x  y  φ  θ) +

1
|U|

x∈U
Ll(xi  yi  φ  θ) +

1
B

(cid:88)

(xi yi)∼L ∀ i∈[B]

xi∼U ∀ i∈[B]

Lu(xi  φ  θ)

(8)

(9)

6

(cid:34) K−1(cid:88)

E
p(o)

(cid:16)

(cid:16)

K(cid:88)

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Beta

(cid:16)

K(cid:88)

α(oj )(cid:17)(cid:17)(cid:35)

that balances the two ELBOs evenly. Of concern is when |U| (cid:29) |L|. Here  the optimizer could
effectively ignore Ll(x  y  φ  θ). This possibility motivates our rebalancing in eq. (8). During
optimization we employ batch updates of size B to maximize eq. (9)  which similarly balances
the contribution between U and L. We deﬁne an epoch to be the set of batches (sampled without
replacement) that constitute U. Therefore  when |U| (cid:29) |L|  the optimizer will observe samples from
L many more times than samples from U. Intuitively  the data with observable labels in conjunction
with eq. (6) breaks symmetry and encourages the correct assignment of classes to labels.
Following [12  13]  we use an inference network with parameters φ and deﬁne our variational
distribution q(z) = N (z; µφ(x)  Σφ(x))  where µφ(x) and Σφ(x) are outputs of a neural network
operating on the observable data. We restrict Σφ(x) to output a diagonal covariance and use a softplus 
ln(exp(x) + 1)  output layer to constrain it to the positive reals. Since µφ(x) ∈ Rdim(z)  we use an
afﬁne output layer. We let q(π) = MV-Kumaraswamy(π; αφ(x))  where αφ(x) is also an output of
our inference network. We similarly restrict αφ(x) to the positive reals via the softplus activation.
We evaluate the expectations in eqs. (6) and (7) using Monte-Carlo integration. For q(z)  we sample
from N (0  I) and utilize the reparameterization trick. Since q(π) contains an expectation over
orderings  we ﬁrst sample o ∼ Uniform(O) and then employ algorithm 1 with pi(v; ai  bi) ≡
j=i+1 αj)  for which we use inverse-CDF sampling. In both cases  gradients
are well deﬁned with respect to the variational parameters.
We can decompose DKL(MV-Kumaraswamy(αφ(x)) || Dirichlet(α)) into a sum over the corre-
sponding Kumaraswamy and Beta stick-breaking distributions as in [20]. Let α(j)
φ (x) be the jth con-
centration parameter of the inference network  and α(j) be jth parameter of the Dirichlet prior. If  as
above  we let p(o) = Uniform(O) for the set of all orderings O  then DKL(q(π; αφ(x)) || p(π; α)) =

Kumaraswamy(x; αi (cid:80)K

DKL

Kumaraswamy

α(oi)

φ (x) 

α(oj )
φ

(x)

i=1

j=i+1

α(oi) 

j=i+1

We compute DKL(Kumaraswamy(a  b) || Beta(a(cid:48)  b(cid:48))) analytically as in [20] with a Taylor approxi-
mation order of 5. We too approximate this expectation with far fewer than K! samples from p(o).
Please see section 7.4 of the appendix for a reproduction of this KL-Divergence’s mathematical form.

5 Experiments

We consider a variety of baselines for our semi-supervised model. Since our work expounds and
resolves the order dependence of the original Kumaraswamy stick-breaking construction [20] that
uses ﬁxed and constant ordering  we employ their construction (Kumar-SB) as a baseline  for which
we force our implementation to use a ﬁxed and constant order during the stick-breaking procedure.
As noted in section 1  our model is similar to the M2 model [13]. We too consider it an important
baseline for our semi-supervised experiments. Additionally  we use the Softmax-Dirichlet sampling
approximation [25]. This approximation forces logits sampled from a Normal variational posterior
onto the simplex via the softmax function. In this case  the Dirichlet prior is approximated with
a prior for the Gaussian logits [25]. However  this softmax approximation struggles to capture
sparsity because the Gaussian prior cannot achieve the multi-modality available to the Dirichlet
[22]. Lastly  we include a comparison to Implicit Reparameterization Gradients (IRG) [4]. Here 
we set q(π; αφ(x)) = Dirichlet(π; αφ(x)) in our semi-supervised model with the same architecture.
IRG uses independent Gamma samples to construct Beta and Dirichlet samples. IRG’s principle
contribution for gradient reparameterization is that it side-steps the need to invert the standardization
function (i.e. the CDF). However  IRG still requires Gamma CDF gradients w.r.t. the variational
parameters. These gradients do not have a known analytic form  mandating their application of
forward-mode automatic differentiation to a numerical method. In our IRG baseline  both the prior
and variational posterior are Dirichlet distributions yielding an analytic KL-Divergence. We mention
but do not test [9]  which similarly constructs Dirichlet samples from normalized Gamma samples.
They too employ implicit differentiation to avoid differentiating the inverse CDF  but necessarily fall
back to numerically differentiating the Gamma CDF.
Our source code can be found at https://github.com/astirn/MV-Kumaraswamy. For our latest
experimental results  please refer to https://arxiv.org/abs/1905.12052. In our generative

7

process and eqs. (6) and (7)  we referred generally to our data likelihood as p(x|fθ(y  z)). In all of
our experiments  we assume p(x|fθ(y  z)) = N (x  µθ(y  z)  Σθ(y  z))  where µθ(y  z) and Σθ(y  z)
are outputs of a neural network with parameters θ operating on the latent variables. We use diagonal
covariance for Σθ(y  z). Across all of our experiments  we maintain consistent recognition and
generative network architectures  which we detail in section 7.5 of the appendix.
We do not use any explicit regularization. Our models are implemented in TensorFlow and were
trained using ADAM with a batch size B = 250 and 5 Monte-Carlo samples for each training
example. We use learning rates 1 × 10−3 and 1 × 10−4 respectively for MNIST and SVHN. Other
optimizer parameters were kept at TensorFlow defaults. We utilized GPU acceleration and found that
cards with ∼8 GB of memory were sufﬁcient. We utilize the TensorFlow Datasets API  from which
we source our data. For all experiments  we split our data into 4 subsets: unlabeled training (U)
data  labeled training (L) data  validation data  and test data. For MNIST: |U| = 49  400  |L| = 600 
|validation| = |test| = 10  0000. For SVHN: |U| = 62  257  |L| = 1000  |validation| = 10  000 
|test| = 26  032. When constructing L  we enforce label balancing. We allow all trials to train for a
maximum of 750 epochs  but use validation set performance to enable early stopping whenever the
loss (eq. (8)) and classiﬁcation error have not improved in the previous 15 epochs. All reported metrics
were collected from the test set during the validation set’s best epoch–we do this independently for
classiﬁcation error and log likelihood. For each trial  all models utilize the same random data split
except where noted†. We translate the uint8 encoded pixel intensities to [0  1] by dividing by 255  but
perform no other preprocessing.
Table 1: Held-out test set classiﬁcation errors and log likelihoods. A “−−” for a p-value indicates it
was unavailable either because it was with respect to itself or the corresponding data and/or number
of trials were missing. Since [13] did not report log likelihoods  we did not collect them with our
implementation.

Experiment
MNIST
10 trials
600 labels
dim(z) = 0
MNIST
10 trials
600 labels
dim(z) = 2

Method
MV-Kum.
IRG[4]
Kumar-SB[20]
Softmax
MV-Kum.
IRG[4]
M2 (ours)
Kumar-SB[20]
Softmax
MV-Kum.
IRG[4]
M2 (ours)

MNIST
10 trials
600 labels
dim(z) = 50 Kumar-SB[20]

SVHN
4 trials
1000 labels
dim(z) = 50 Kumar-SB[20]

Softmax
M2†[13]
M1 + M2†[13]
MV-Kum.
IRG[4]
M2 (ours)

Softmax
M1 + M2†[13]

Error

0.099 ± 0.011
0.097 ± 0.008
0.248 ± 0.009
0.093 ± 0.009
0.043 ± 0.005
0.044 ± 0.006
0.098 ± 0.014
0.138 ± 0.015
0.042 ± 0.003
0.018 ± 0.004
0.018 ± 0.004
0.020 ± 0.003
0.071 ± 0.008
0.018 ± 0.003
0.049 ± 0.001
0.026 ± 0.005
0.296 ± 0.014
0.288 ± 0.008
0.406 ± 0.027
0.702 ± 0.011
0.300 ± 0.007
0.360 ± 0.001

p-value
−−
0.72

1.05 × 10−17

0.24
−−
0.89

5.37 × 10−10
1.65 × 10−13

0.40
−−
0.98
0.32

2.58 × 10−13

0.87
−−
−−
−−
0.38

3.64 × 10−04
7.42 × 10−09

0.61
−−

Log Likelihood
−6.4 ± 6.3
−7.8 ± 7.1
−6.5 ± 6.3
−6.5 ± 6.2
45.06 ± 0.92
45.69 ± 0.38
Not collected
44.33 ± 1.65
45.14 ± 0.73
116.58 ± 0.68
116.57 ± 0.43
Not collected
116.22 ± 0.33
116.24 ± 0.45
Not reported
Not reported
669.37 ± 0.57
669.84 ± 0.84
Not collected
669.44 ± 0.77
669.51 ± 0.72
Not reported

p-value
−−
0.64
0.95
0.95
−−
0.06
−−
0.24
0.82
−−
0.97
−−
0.15
0.21
−−
−−
−−
0.39
−−
0.89
0.78
−−

For the semi-supervised learning task  we present classiﬁcation and reconstruction performances in
table 1 using our algorithm as well as the baselines discussed previously. We organize our results
by experiment group. All reported p-values are with respect to our MV-Kumaraswamy model’s
performance for corresponding dim(z). We say  “M2 (ours) ” whenever we use the generative
process of [13] with our neural network architecture. For a subset of experiments  we present results
from [13]–without knowing how many trials they ran we cannot compute the corresponding p-value.
We recognize that there are numerous works [21  1  26  15  10  7  24  2  16  17] that offer superior

8

performance on these tasks  however  we abstain from reporting these performances whenever those
models are not variational Bayesian  use adversarial training  lack explicit generative processes  use
architectures vastly larger in size than ours  or use a different number of labeled examples ((cid:54)= 600 for
MNIST and (cid:54)= 1000 for SVHN).
In ﬁg. 6  we plot the latent space representation for our MV-Kumaraswamy model for MNIST when
dim(z) = 2. Each digit’s manifold is over (−1.5 −1.5) × (1.5  1.5)  which corresponds to ±1.5
standard deviations from the prior. The only difference in latent encoding between corresponding
manifold positions is the label provided to the generative network. Interestingly  the model learns to
use z in a qualitatively similar way to represent character transformations across classes.

Figure 6: Latent space for MV-Kumaraswamy model with dim(z) = 2.

6 Discussion

The statistically signiﬁcant classiﬁcation performance gains of MV-Kumaraswamy (approximate
integration over all orderings) against Kumar-SB [20] (ﬁxed and constant ordering) validates the
impact of our contribution. Kumar-SB’s worse performance is likely due to the over allocation of
probability mass to the ﬁnal stick during sampling (ﬁg. 1). When the class-assignment posterior has
high entropy  the ﬁxed order sampling will bias the last label dimension. Further  MV-Kumaraswamy
beats [13] for both classiﬁcation tasks despite our single model approach and minimal preprocessing.
Interestingly  our implementation of M2 seemingly requires a larger dim(z) to match the classiﬁcation
performance of MV-Kumaraswamy. Lastly  IRG’s classiﬁcation performance is not statistically
distinguishable from ours. Deep learning frameworks’ (e.g. TensorFlow  PyTorch  Theano  CNTK 
MXNET  Chainer) distinct advantage is NOT requiring user-computed gradients. We argue that
methods requiring numerical gradients [4  9] do not admit a straightforward implementation for
the common practitioner as they require additional (often non-trivial) code to supply the gradient
estimates to the framework’s optimizer. Conversely  our method has analytic gradients  enabling easy
integration into ANY deep learning framework. To the best of our knowledge  IRG for the Gamma 
Beta  and Dirichlet distributions only exists in TensorFlow (IRG was developed at Deep Mind).
VAEs offer scalable and efﬁcient learning for a subset of Bayesian models. Applied Bayesian
modeling  however  makes heavy use of distributions outside this subset. In particular  the Dirichlet 
without some form of accommodation or approximation  will render a VAE intractable since gradients
with respect to variational parameters are challenging to compute. Efﬁcient approximation of such
gradients is an active area of research. However  explicit reparameterization is advantageous in terms
of simplicity and efﬁciency. In this article  we present and develop theory for a computationally
efﬁcient and explicitly reparameterizable Dirichlet surrogate that has similar sparsity-inducing
capabilities and identical exchangeability properties to the Dirichlet it is approximating. We conﬁrm
its surrogate candidacy through a range of semi-supervised auto-encoding tasks. We look forward to
utilizing our new distribution to scale inference in more structured probabilistic models such as topic
models. We hope others will use our distribution not only as a surrogate for a Dirichlet posterior but
also as a prior. The latter might yield a more exact divergence between the variational posterior and
its prior.

Acknowledgments

This work was supported in part by NSF grant III-1526914.

9

References
[1] David Berthelot  Nicholas Carlini  Ian Goodfellow  Nicolas Papernot  Avital Oliver  and Colin
Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv:1905.02249  2019.

[2] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural
Information Processing Systems 29  pages 2172–2180. Curran Associates  Inc.  2016.

[3] Djork-Arne’ Clevert  Thomas Unterthiner  and Sepp Hochreiter. Fast and accurate deep
network learning by exponential linear units (elus). International Conference on Learning
Representations (ICLR)  2016.

[4] Mikhail Figurnov  Shakir Mohamed  and Andriy Mnih. Implicit reparameterization gradients.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors 
Advances in Neural Information Processing Systems 31  pages 441–452. Curran Associates 
Inc.  2018.

[5] Bela A. Frigyik  Amol Kapila  and Maya R. Gupta. Introduction to the dirichlet distribution
and related processes. Technical report  University of Washington  Department of Electrical
Engineering  2010.

[6] Michiel Hazewinkel  editor. Encyclopaedia of Mathematics  volume 6. Springer Netherlands  1

edition  1990.

[7] Tobias Hinz and Stefan Wermter. Inferencing based on unsupervised learning of disentangled

representations. CoRR  abs/1803.02627  2018.

[8] Wassily Hoeffding. A class of statistics with asymptotically normal distributions. Annals of

Statistics  19(3):293–325  1948.

[9] Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization
In Jennifer Dy and Andreas Krause  editors  Proceedings of the 35th International
trick.
Conference on Machine Learning  volume 80 of Proceedings of Machine Learning Research 
pages 2235–2244  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.

[10] Xu Ji  João F. Henriques  and Andrea Vedaldi. Invariant information distillation for unsupervised

image segmentation and clustering. CoRR  abs/1807.06653  2018.

[11] Michael I. Jordan  Zoubin Ghahramani  Tommi S. Jaakkola  and Lawrence K. Saul. An
introduction to variational methods for graphical models. Machine Learning  37(2):183–233 
1999.

[12] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Confer-

ence on Learning Representations (ICLR)  2014.

[13] Durk P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems
27  pages 3581–3589. Curran Associates  Inc.  2014.

[14] Ponnambalam Kumaraswamy. A generalized probability density function for double-bounded

random processes. Journal of Hydrology  1980.

[15] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. CoRR 

abs/1610.02242  2016.

[16] Alireza Makhzani and Brendan J Frey. Pixelgan autoencoders. In I. Guyon  U. V. Luxburg 
S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural
Information Processing Systems 30  pages 1975–1985. Curran Associates  Inc.  2017.

[17] Alireza Makhzani  Jonathon Shlens  Navdeep Jaitly  and Ian Goodfellow. Adversarial autoen-

coders. In International Conference on Learning Representations (ICLR)  2016.

10

[18] Christian A. Naesseth  Francisco J. R. Ruiz  Scott W. Linderman  and David M. Blei. Reparam-
eterization gradients through acceptance-rejection sampling algorithms. In Proceedings of the
20th International Conference on Artiﬁcial Intelligence and Statistics  2017.

[19] Eric Nalisnick  Lars Hertel  and Padhraic Smyth. Approximate inference fordeep latent gaussian

mixtures. Workshop on Bayesian Deep Learning  NIPS  2016.

[20] Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. International

Conference on Learning Representations (ICLR)  Apr 2017.

[21] Antti Rasmus  Harri Valpola  Mikko Honkala  Mathias Berglund  and Tapani Raiko. Semi-

supervised learning with ladder network. CoRR  abs/1507.02672  2015.

[22] Francisco R Ruiz  Michalis Titsias RC AUEB  and David Blei. The generalized reparameteriza-
tion gradient. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors 
Advances in Neural Information Processing Systems 29  pages 460–468. Curran Associates 
Inc.  2016.

[23] Tim Salimans and David A. Knowles. Fixed-form variational posterior approximation through

stochastic linear regression. Bayesian Analysis  8  2013.

[24] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical gen-
erative adversarial networks. International Conference on Learning Representations (ICLR) 
2016.

[25] Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. In

International Conference on Learning Representations (ICLR)  2017.

[26] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged
consistency targets improve semi-supervised deep learning results. In Advances in Neural
Information Processing Systems 30  pages 1195–1204. Curran Associates  Inc.  2017.

[27] Martin J. Wainwright and Michael I. Jordan. Graphical models  exponential families  and

variational inference. Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

11

,Andrew Stirn
Tony Jebara
David Knowles