2019,Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction,Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) algorithms have received increasing attention in both theory and practice. In this paper  we  propose a Stochastic Recursive Variance-Reduced gradient HMC (SRVR-HMC) algorithm. It makes use of a semi-stochastic gradient estimator that recursively accumulates the gradient information to reduce the variance of the stochastic gradient. We provide a convergence analysis of SRVR-HMC for sampling from a class of non-log-concave distributions and show that SRVR-HMC converges faster than all existing HMC-type algorithms based on underdamped Langevin dynamics. Thorough experiments on synthetic and real-world datasets validate our theory and demonstrate the superiority of SRVR-HMC.,Stochastic Gradient Hamiltonian Monte Carlo
Methods with Recursive Variance Reduction

Difan Zou

Pan Xu

Department of Computer Science

University of California  Los Angeles

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095
knowzou@cs.ucla.edu

Los Angeles  CA 90095
panxu@cs.ucla.edu

Quanquan Gu

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095

qgu@cs.ucla.edu

Abstract

Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) algorithms have received
increasing attention in both theory and practice.
In this paper  we propose a
Stochastic Recursive Variance-Reduced gradient HMC (SRVR-HMC) algorithm.
It makes use of a semi-stochastic gradient estimator that recursively accumulates
the gradient information to reduce the variance of the stochastic gradient. We
provide a convergence analysis of SRVR-HMC for sampling from a class of
non-log-concave distributions and show that SRVR-HMC converges faster than
all existing HMC-type algorithms based on underdamped Langevin dynamics.
Thorough experiments on synthetic and real-world datasets validate our theory and
demonstrate the superiority of SRVR-HMC.

1

Introduction

Monte Carlo Markov Chain (MCMC) has been widely used in Bayesian learning [1] as a powerful
tool for posterior sampling  inference and decision making. More recently  Hamiltonian MCMC
approaches based on the Hamiltonian Langevin dynamics [24  43] have received extensive attention
in both theory and practice [16  5  40  14  6  18  55  28] due to their widespread empirical successes.
Hamiltonian Langevin dynamics (a.k.a.  underdamped Langevin dynamics) [19] is described by the
following stochastic differential equation:

dVt = Vtdt  urf (Xt)dt +p2udBt 

dXt = Vtdt 

where > 0 is called the friction parameter  u > 0 is the inverse mass  Xt  Vt 2 Rd are the position
and velocity variables of the continuous-time dynamics respectively  and Bt 2 Rd is the standard
Brownian motion. Under mild assumptions on the function f (x)  the Markov process (Xt  Vt) has a
unique stationary distribution which is proportional to exp{f (x)  kvk2
2/(2u)} and the marginal
distribution of Xt converges to a stationary distribution ⇡ / exp{f (x)}. Hence  we can apply
numerical integrators to discretize the continuous-time dynamics (1.1) in order to sample from the
target distribution ⇡. Direct Euler-Maruyama discretization [34] of (1.1) gives rise to

vk+1 = vk  ⌘vk  ⌘urf (xk) +p2u⌘✏ k 

xk+1 = xk + ⌘vk 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(1.1)

(1.2)

which is known as underdamped Langevin MCMC (UL-MCMC) and can also be viewed as a type of
Hamiltonian Monte Carlo (HMC) methods [43  6]. Cheng et al. [18] studied a modiﬁed version of
UL-MCMC in (1.2) and proved its convergence rate to the stationary distribution in 2-Wasserstein
distance for sampling from strongly log-concave densities. When the target distribution is non-log-
concave but admits certain good properties  the convergence guarantees of UL-MCMC in Wasserstein
metric have also been established in [27  17  8  30].
In practice  f (x) in (1.2) can be chosen as the negative log-likelihood function on the training data:

f (x) = n1Pn

i=1 fi(x) 

(1.3)
where n is the size of training data and fi(x) : Rd ! R is the negative log-likelihood function on the
i-th data point. For a large dataset  it can be extremely inefﬁcient to compute the full gradient rf (x)
which consists of gradients rfi(x)’s for all data points. To alleviate this computational burden 
stochastic gradient Hamiltonian Monte Carlo (SGHMC) methods [16  40] and stochastic gradient
UL-MCMC (SG-UL-MCMC) [18] were proposed  which replace the full gradient in (1.2) with a
mini-batch stochastic gradient. While SGHMC is much more efﬁcient than HMC methods  it comes
at the cost of a slower mixing rate due to the large variance caused by stochastic gradients [5  6  23].
To resolve this dilemma  Zou et al. [55]  Li et al. [37] proposed stochastic variance-reduced gradient
HMC methods using variance reduction techniques [33  36] and proved that variance reduction can
accelerate the convergence of both HMC and SGHMC for sampling and Bayesian inference. For
sampling from a class of non-log-concave densities  Gao et al. [30] showed that SGHMC converges
to the stationary distribution of (1.1) up to an ✏-error in 2-Wasserstein distance with eO(✏8µ5
⇤ )1
gradient complexity2  where µ⇤ is a lower bound of the spectral gap of the Markov process generated
by (1.1) and is in the order of exp(eO(d)) in the worst case [27]. This gradient complexity of
SGHMC is very high even for a moderate sampling error ✏.
In this paper  we aim to reduce the gradient complexity of SGHMC for sampling from non-log-
concave densities. The fundamental challenge in speeding up HMC-type methods lies in the control
of the discretization error between the Hamiltonian Langevin dynamics (1.1) and discrete algo-
rithms. We propose a novel algorithm  namely stochastic recursive variance-reduced gradient HMC
(SRVR-HMC)  which employs a recursively updated semi-stochastic gradient estimator to reduce
the variance of stochastic gradient and improve the discretization error. Note that such a recursively
updated semi-stochastic gradient estimator was originally proposed in [44  29] for ﬁnding stationary
points in stochastic nonconvex optimization. Nevertheless  our analysis is fundamentally different
from that in [44  29] since their goal is just to ﬁnd a stationary point of f (x)  while we aim to sample
from the target distribution ⇡ / exp(f (x)) that concentrates on the global minimizer of f (x) 
which is substantially more challenging.

1.1 Our contributions

We summarize our major contributions as follows.
• We propose a new HMC algorithm called SRVR-HMC for approximate sampling  which is
built on a recursively updated semi-stochastic gradient estimator that signiﬁcantly decreases the
discretization error and speeds up the sampling process.

• We establish the convergence guarantee of SRVR-HMC for sampling from non-log-concave
densities satisfying certain dissipativeness condition. Speciﬁcally  we show that its gradient
complexity for achieving ✏-error in 2-Wasserstein distance is eO((n + ✏2n1/2µ3/2
⇤ ).
) ^ ✏4µ2
Remarkably  the convergence guarantee of SRVR-HMC is better than the eO(✏4µ3
⇤ n) gradient
complexity of HMC [30] by a factor of at least eO(✏2µ3/2
n1/2)  and better than the eO(✏8µ5
⇤ )
gradient complexity of SGHMC [30] by a factor of at least eO(✏4µ3
⇤ ).
• With a proper choice of parameters  our algorithm can reduce to UL-MCMC [18] and SG-UL-
MCMC [18]  which are originally proposed for sampling from strongly-log-concave distributions.
1eO(·) hides constant and logarithm factors.
2Gradient complexity is the total number of stochastic gradients rfi(x) an algorithm needs to compute in

order to achieve ✏-error in terms of certain measurement.

⇤

⇤

2

Our theoretical analysis shows that these two algorithms can be used for sampling from non-log-
concave distributions as well  and they enjoy lower gradient complexities than HMC and SGHMC
[30]  which is of independent interest.

• We compare our algorithm with many state-of-the-art baselines through experiments on sampling
from Gaussian mixture distributions  independent component analysis (ICA) and Bayesian logistic
regression  which further validates the superiority of our algorithm.

dXt = rf (Xt)dt +p2dBt 

1.2 Additional related work
There is also a vast literature of MCMC methods based on the overdamped Langevin dynamics [35]:
(1.4)
where > 0 is the temperature parameter and Bt is Brownian motion. The convergence analysis of
Langevin based algorithms dates back to [46]. Mattingly et al. [41] established convergence rates for
a class of discrete approximation of Langevin dynamics. When the target distribution is smooth and
strongly log-concave  the convergence of Langevin Monte Carlo (LMC) based on the discretization of
(1.4) has been widely studied in terms of both total variation (TV) distance [21  26] and 2-Wasserstein
distance [22  20]. Welling and Teh [50] proposed the stochastic gradient Langevin dynamics (SGLD)
algorithm to avoid full gradient computation. Teh et al. [47] proposed to apply decreasing step size
with SGLD and proved its convergence in terms of mean square error (MSE). Vollmer et al. [48]
characterized the bias of SGLD and further proposed a modiﬁed SGLD algorithm that removes the
bias. [10] establish a link between LMC  SGLD  SGLDFP (a variant of SGLD) and SGD  which
shows that the stationary distribution of LMC and SGLDFP can be closer to the target density ⇡ as the
sample size increases  while the dynamics of SGLD is more similar to that of SGD. Barkhagen et al.
[4]  Chau et al. [13] studied the convergence of SGLD when the training data in (1.3) are dependent.
In order to reduce the variance of SGLD  SVRG-LD and SAGA-LD have been proposed by Dubey
et al. [25] and their convergence have been studied in terms of MSE [25  15] and 2-Wasserstein
distance [56  12]. Baker et al. [2] proposed to use control variate in SGLD which can also reduce the
variance and improve the convergence rate. Mou et al. [42] studied the generalization performance of
SGLD from both stability and PAC-Bayesian perspectives. For nonconvex optimization  Raginsky
et al. [45] proved the non-asymptotic convergence rate of SGLD and Zhang et al. [52] analyzed the
hitting time of SGLD to local minima. Xu et al. [51] further studied the global convergence of a class
of Langevin dynamics based algorithms.

Table 1: Gradient complexity of different methods to achieve ✏-error in 2-Wasserstein distance for
sampling from non-log-concave densities.

Methods
LMC
SGLD
SVRG-LD
HMC
UL-MCMC
SGHMC
SG-UL-MCMC
SRVR-HMC

eO✏45
⇤ n
eO✏89
⇤ 
eOn + ✏24
eO✏4µ3
⇤ n
eO✏2µ3/2
n
eO✏8µ5
⇤ 

eO✏6µ5/2
eO(n + ✏2n1/2µ3/2

⇤

⇤

⇤

Gradient Complexity

⇤ n3/4 + ✏44

⇤ n1/2

⇤ 
) ^ ✏4µ2

[45]
[45]
[57]
[30]
. Corollary 3.9
[30]
. Corollary 3.9
. Corollary 3.5

In Table 1  we compare the gradient complexity of different methods to achieve ✏-error in 2-
Wasserstein distance for sampling from non-log-concave densities3. LMC  SGLD and SVRG-LD are
based on overdamped Langevin dynamics (1.4) and HMC  UL-MCMC  SGHMC  SG-UL-MCMC
and SRVR-HMC are based on underdamped Langevin dynamics (1.1). The HMC/SGHMC algo-
rithm studied in [30] and the UL-MCMC/SG-UL-MCMC algorithm [18] analyzed in this paper are
3The original results for LMC/SGLD in [45] and for HMC/SG-HMC in [30] are about the global convergence
in nonconvex optimization. Yet their results can be adapted to sampling from non-log-concave distributions  and
the corresponding gradient complexities can be spelled out from their convergence rates.

3

overdamped Langevin dynamics (1.4)  which is also in the order of exp(eO(d)) [9  45] in the worst

slightly different since they rely on different discretization methods to the Hamiltonian Langevin
dynamics (1.1). In addition  note that ⇤ denotes the spectral gap of the Markov process generated by
case.
From Table 1  we can see that the proposed SRVR-HMC algorithm strictly outperforms HMC  UL-
MCMC  SGHMC and SG-UL-MCMC  and also outperforms LMC  SGLD and SVRG-LD in terms
of the dependency on target accuracy ✏ and training sample size n. We remark that for a general
non-log-concave target density  ⇤ and µ⇤ are not directly comparable  though both of them are
exponential in dimension d. However  it is shown that for a class of target densities  µ⇤ can be in the
order of O(1/2
) [27  30]  which suggests that SRVR-HMC is also strictly better than LMC  SGLD
and SVRG-LD for sampling from such densities.
Notation. We denote discrete update by lower case bold symbol xk and continuous-time dynamics
by upper case italicized bold symbol Xt. For a vector x 2 Rd  we denote by kxk2 the Euclidean
norm. For random vectors xk  Xt 2 Rd  we denote their probability distribution functions by P(xk)
and P(Xt) respectively. For a probability measure µ  we denote by Eµ[X] the expectation of X
under probability measure u. The 2-Wasserstein distance between two probability measures u and v
is

⇤

W2(u  v) =s inf

⇣2(u v)ZRd⇥Rd kXu  Xvk2

2d⇣(Xu  Xv) 

where the inﬁmum is taken over all joint distributions ⇣ with u and v being its marginal distributions.
1(·) denotes the indicator function. We denote index set [n] = {1  2  . . .   n} for an integer n. We
use an = O(bn) to denote that an  Cbn for some constant C > 0 independent of n  and use
an = eO(bn) to hide the logarithmic factors in bn. The Vinogradov notation an . bn is also used
synonymously with an = O(bn). We denote min{a  b} and max{a  b} by a^ b and a_ b respectively.
The ceiling function dxe outputs the least integer greater than or equal to x.
2 The proposed algorithm

In this section  we present our algorithm  SRVR-HMC  for sampling from a target distribution in
the form of ⇡ / exp{f (x)}. Our algorithm is shown in Algorithm 1  which has a multi-epoch
structure. In detail  there are dK/Le epochs  where K is the number of total iterations and L denotes
the epoch length  i.e.  the number of iterations within each inner loop.
Recall that the update rule of HMC in (1.2) requires the computation of full gradient rf (xk) at each
iteration  which is the average of n stochastic gradients. This causes a high per-iteration complexity
when n is large. Therefore  we propose to leverage the stochastic gradient to offset the computational

training data (uniformly sampled from [n] without replacement) as shown in Line 4 of Algorithm

burden. At the beginning of the j-th epoch  we compute a stochastic gradientegj based on a batch of
1  where the batch is denoted by eBj with batch size |eBj| = B0. In each epoch  we make use of

the stochastic path-integrated differential estimator [29] to compute the following semi-stochastic
gradient

gk = 1/BPi2Bk⇥rfi(xk)  rfi(xk1)⇤ + gk1 

(2.1)
where Bk is another uniformly sampled (without replacement) mini-batch from [n] with mini-batch
size |Bk| = B. Unlike the unbiased stochastic gradient estimators in SGHMC [16] and SVR-HMC
[55]  gk is a biased estimator of the full gradient rf (xk) conditioned on xk. However  we can show
that while being biased  the variance of gk is substantially smaller than that of unbiased ones. This
is the key reason why our algorithm can achieve a faster convergence rate than existing HMC-type
algorithms. Based on the semi-stochastic gradient in (2.1)  we update the position and velocity
variables as follows

vk+1 = vke⌘  u1(1  e⌘)gk + ✏v
k 
xk+1 = xk + 1(1  e⌘)vk + u2(⌘ + e⌘  1)gk + ✏x
k 

(2.2)

where ⌘ is the step size and u   are the inverse mass and friction parameter deﬁned in (1.1)  which
are usually treated as tunable hyper parameters in practice. Moreover  ✏v
k 2 Rd are zero mean

k  ✏x

4

Algorithm 1 Stochastic Recursive Variance-Reduced gradient HMC (SRVR-HMC)

k = jL + l
if l = 0 then

else

iterations K; epoch length L

1: input: Initial pointsex0 = x0 = x0  v0; step size ⌘; batch sizes B0 and B; total number of
Uniformly sample a subset of index eBj ⇢ [n] with |eBj| = B0
Computeegj = 1/B0Pi2eBj rfi(exj)
for l = 0  . . .   L  1 do
gk =egj
Uniformly sample a subset of index Bk ⇢ [n] with |Bk| = B
Compute gk = 1/BPi2Bk
(rfi(xk)  rfi(xk1)) + gk1
end if
xk+1 = xk + (1  e⌘)vk + u2(⌘ + e⌘  1)gk + ✏x
vk+1 = vke⌘  u1(1  e⌘)gk + ✏v
exj+1 = x(j+1)L

2: for j = 0  . . .  dK/Le do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for
18: output: xK

end for

k

k

Gaussian random vectors with covariance matrices satisfying

E[✏v
E[✏x
E[✏v

k(✏v
k(✏x
k(✏x

k)>] = u(1  e2⌘) · I 
k)>] = u2(2⌘ + 4e⌘  e2⌘  3) · I 
k)>] = u1(1  2e⌘ + e2⌘) · I 

(2.3)

where I 2 Rd⇥d is the identity matrix. The covariance of the Gaussian noises in (2.3) is obtained
by integrating the Hamiltonian Langevin dynamics (1.1) over a time period of length ⌘. It is worth
noting our update rule in (2.2) and the construction of the Gaussian noises in (2.3) follow Cheng
et al. [18]  Zou et al. [55]  Cheng et al. [17]  except that we use a different semi-stochastic gradient
estimator as shown in (2.1). In contrast  Cheng et al. [18] uses full gradient and noisy gradient  and
Zou et al. [55] uses an unbiased semi-stochastic gradient based on SVRG [33].
We remark here that the semi-stochastic gradient estimator in (2.1) was originally proposed in ﬁnding
stationary points in ﬁnite-sum optimization [44  29] and further extended in [49  32]. In addition 
another semi-stochastic gradient estimator called SNVRG [54  53] has also been demonstrated to
achieve similar convergence rate in ﬁnite-sum optimization. Despite using the same semi-stochastic
gradient estimator  our work differs from [44  29] in at least two aspects: (1) the sampling problem
studied in this paper is different from the optimization problem studied in [44  29]  where our goal
is to sample from a target distribution concentrating on the global minimizer of f (x) such that the
sample distribution is close to the target distribution in 2-Wasserstein distance. In contrast  Nguyen
et al. [44]  Fang et al. [29] aim at ﬁnding a stationary point of f (x) with small gradient; and (2)
the algorithms in [44  29] only have one update variable  while our SRVR-HMC algorithm has an
additional Hamiltonian momentum term and therefore has two update variables (i.e.  velocity and
position variables). The Hamiltonian momentum is essential for underdamped Langevin Monte Carlo
methods to achieve a smaller discretization error than overdamped methods such as SGLD [50] and
SVRG-LD [25]. At the same time  this also introduces a great technical challenge in our theoretical
analysis and requires nontrivial efforts.

3 Main theory

In this section  we provide the convergence guarantee for Algorithm 1. In particular  we characterize
the 2-Wasserstein distance between the distribution of the output of Algorithm 1 and the target
distribution ⇡ / ef (x). We focus on sampling from non-log-concave densities that satisfy the
smoothness and dissipativeness conditions  which are formally deﬁned as follows.

5

W2P(xK) ⇡  1✓✓1 +

L

B◆K⌘ 3 +

K⌘

2B0 · 1(B0 < n)◆1/4

+ 0eµ⇤K⌘  

where B0  B are the batch and minibatch sizes  L is the epoch length and µ⇤ = exp(eO(d)) is
a lower bound of the spectral gap of the Markov process generated by (1.1). 0 = eO(µ1
⇤ ) and

1 = 2D1(M 23uD2)1/4 are problem-dependent parameters with constants D1  D2 deﬁned as

D1 =

D2 =

8

r um(f (0)  f (x⇤)) + 2M u(4d + 2b + mkx⇤k2
8um(f (0)  f (x⇤)) + 8M u20(d + b) + mkx⇤k2
2

2m

m

22) + (12um + 32)

 

krfi(0)k2

2

M 2

 

+ max
i2[n]

Assumption 3.1 (Smoothness). Each fi in (1.3) is M-smooth  i.e.  there exists a positive constant
M > 0  such that the following holds

krfi(x)  rfi(y)k2  Mkx  yk2 

for any x  y 2 Rd.
Note that Assumption 3.1 directly implies that function f (x) is also M-smooth.
Assumption 3.2 (Dissipativeness). There exist constants m  b > 0  such that the following holds

hrf (x)  xi  mkxk2

2  b 

for any x 2 Rd.

Different from the smoothness assumption  Assumption 3.2 is only required for f (x) rather than
fi(x). The dissipativeness assumption is standard in the analysis for sampling from non-log-concave
densities and is essential to guarantee the convergence of underdamped Langevin dynamics [46  41].

3.1 Convergence analysis of the proposed algorithm
Now we state our main theorem that establishes the convergence rate of Algorithm 1.
Theorem 3.3. Suppose Assumptions 3.1 and 3.2 hold and the initial points are x0 = v0 = 0. If set
  2pM u and the step size ⌘  O(mM3 ^ m1/2M3/2L1/2)  the output xK of Algorithm 1

satisﬁes

and x⇤ = argminx2Rd f (x) is the global minimizer of f.
Theorem 3.3 states that the 2-Wasserstein distance between the output of SRVR-HMC and the target
distribution is upper bounded by two terms: the ﬁrst term is the discretization error between the
discrete-time Algorithm 1 and the continuous-time dynamics (1.1)  which goes to zero when the step
size ⌘ goes to zero; the second term represents the ergodicity of the Markov process generated by
(1.1) which converges to zero exponentially fast.
Remark 3.4. The result in Theorem 3.3 encloses a term µ⇤ with an exponential dependence on
the dimension d  which is a lower bound of the spectral of the Markov process generated by (1.1).
When f is nonconvex  the exponential dependence of µ⇤ on dimension is unavoidable under the
dissipativeness assumption [9]. However  this exponential dependency on d can be weakened by
imposing stronger assumptions on f (x). For instance  Eberle et al. [27]  Gao et al. [30] showed that
for a symmetric double-well potential f (x)  µ⇤ is in the order of ⌦(1/a)  where a is the distance
between these two wells  and is typically polynomial in the dimension d. Another example is shown
by Cheng et al. [17]: when f (x) is strongly convex outside a `2 ball centered at the origin with radius
R  µ⇤ is in the order of exp(O(M R2)) where M is the smoothness parameter.
From Theorem 3.3  we can obtain the gradient complexity of SRVR-HMC by optimizing the choice
of minibatch size B and batch size B0 in the following corollary.

0

µ1/2

⇤ ^n)  B . B1/2
Corollary 3.5. Under the same assumptions in Theorem 3.3  if set B0 = eO(✏4µ1
 
⇤ B)  then Algorithm 1 requires eO((n + ✏2n1/2µ3/2
L = O(B0/B)  and ⌘ = eO(✏2B1/2
) ^
⇤ ) stochastic gradient evaluations to achieve ✏-error in 2-Wasserstein distance.
✏4µ2
Remark 3.6. Recall the gradient complexities of HMC and SGHMC in Table 1  it is evident that the
gradient complexity of Algorithm 1 is lower than that of HMC [30] by a factor of eO(✏2n1/2µ3/2
⇤ _
nµ⇤) and is lower than that of SGHMC [30] by a factor of eO(✏6n1/2µ7/2

⇤ ).
_ ✏4µ3

⇤

⇤

0

6

Remark 3.7. As shown in Table 1  the gradient complexities of overdamped Langevin dynamics
based algorithms  including LMC  SGLD and SVRG-LD  depend on the spectral gap ⇤ of the
Markov chain generated by (1.4). Although the magnitudes of µ⇤ and ⇤ are not directly comparable 
they are generally in the same order in the worst case [9  45  27]. Thus we treat them the same in the
following comparison. In speciﬁc  the gradient complexity of SRVR-HMC is better than those of

LMC [45] SGLD [45] and SVRG-LD [57] by factors of eO(✏2n1/2 _ n)  eO(✏6n1/2 _ ✏4) and
eO(✏2 _ n1/2) respectively.

3.2
Recall the proposed SRVR-HMC algorithm in Algorithm 1  if we set the epoch length to be L = 1 
Algorithm 1 degenerates to SG-UL-MCMC [18]  with the following update formulation:

Implication for UL-MCMC and SG-UL-MCMC

k 

vk+1 = vke⌘  u1(1  e⌘)egk + ✏v
xk+1 = xk + 1(1  e⌘vk) + u2(⌘ + e⌘  1)egk + ✏x

whereegk = |eBk|1Pn
addition  if we replaceegk with the full gradient rf (xk)  SG-UL-MCMC in (3.1) further reduces

i=1 rfi(xk) denotes the stochastic gradient computed in the k-th iteration. In
to UL-MCMC [18]. Although these two algorithms were originally proposed for sampling from
strongly-log-concave densities [18]  in this subsection  we show that our analysis of SRVR-HMC
can be easily adapted to derive the gradient complexity of UL-MCMC/SG-UL-MCMC for sampling
from non-log-concave densities. We ﬁrst state the convergence of SG-UL-MCMC in the following
theorem.
Theorem 3.8. Under the same assumptions in Theorem 3.3  the output xK of the SG-UL-MCMC
algorithm in (3.1) satisﬁes

(3.1)

k 

W2P(xK) ⇡  1⇥2K⌘ 3 + K⌘/ (2B0) · 1(B0 < n)⇤1/4 + 0eµ⇤K⌘  

where B0 denotes the mini-batch size  µ⇤  0 and 1 are deﬁned in Theorem 3.3.
Similar to the results in Theorem 3.3  the sampling error of SG-UL-MCMC in 2-Wasserstein distance
is also controlled by the discretization error of the discrete algorithm (3.1) and the ergodicity rate of
Hamiltonian Langevin dynamics (1.1). In particular  the main difference in the convergence results
of SG-UL-MCMC and SRVR-HMC lies in the discretization error term  which leads to a different
gradient complexity for SG-UL-MCMC.

⇤

) and B0 =
) stochastic gradient evaluations to achieve
n) stochastic gradient

Corollary 3.9. Under the same assumptions in Theorem 3.3  if we set ⌘ = eO(✏2µ1/2
eO(✏4µ1
✏-error in 2-Wasserstein distance. Moreover  UL-MCMC requires eO(✏2µ3/2

⇤ )  SG-UL-MCMC in (3.1) requires eO(✏6µ5/2

evaluations to achieve ✏-error in 2-Wasserstein distance.
Remark 3.10. Our theoretical analysis suggests that the gradient complexity of UL-MCMC is better
than that of HMC [30] by a factor of O(✏2µ3/2
) and the gradient complexity of SG-UL-MCMC
is better than that of SGHMC [30] by a factor of O(✏2µ5/2
). We note that Cheng et al. [17]
proved O(1/✏) convergence rate of UL-MCMC for sampling from a smaller class of non-log-concave
densities in 1-Wasserstein distance. Their result is not directly comparable to our result since 1-
Wasserstein distance is strictly smaller than 2-Wasserstein distance and more importantly  their results
rely on a stronger assumption than the dissipativeness assumption used in our paper as we commented
in Remark 3.4.

⇤

⇤

⇤

⇤

4 Experiments

In this section  we evaluate the empirical performance of SRVR-HMC on both synthetic and real
datasets. We compare our proposed algorithm with existing overdamped and underdamped Langevin
based stochastic gradient algorithms including SGLD [50]  SVRG-LD [25]  SGHMC [16]  SG-UL-
MCMC [18] and SVR-HMC [55].

4.1 Sampling from Gaussian mixture distributions

7

We ﬁrst demonstrate the performance of SRVR-HMC for ﬁt-
ting a Gaussian mixture model on synthetic data . In this case 
the density on each data point is deﬁned as

efi(x) = 2ekxaik2

2/2 + ekx+aik2

2/2 

0.05

0
-6
6

4

-4

-2

0

2

4

6

SRVR-HMC
True

6

4

0

2

4

6

-4

-2

0

-6

-2

-2

-4

-6

-4

-6

2

0

2

0

0.05

i=1 fi(x).

Figure 1: Kernel density estimation
for Gaussian mixture distribution.

which is proportional to the probability density function (PDF)
of two-component Gaussian mixture density with weights 1/3
and 2/3. By simple calculation  it can be veriﬁed that when
kaik2  1  fi(x) is nonconvex but satisﬁes Assumption 3.2 
and so does f (x) = 1/nPn
We generated n = 500 vectors {ai}i=1 ... n 2 R2 to construct
the target density functions. We ﬁrst show that the proposed
algorithm can well approximate the target distribution. Specif-
ically  we run SRVR-HMC for 104 data passes  and use the
last 105 iterates to visualize the estimated distribution  where the batch size  minibatch size and
epoch length are set to be B0 = n  B = 1 and L = n respectively. As a reference  we run MCMC
with Metropolis-Hasting (MH) correction to represent the underlying distribution. Following [3]  we
display the kernel densities of random samples generated by SRVR-HMC in Figures 4.1  which shows
that the random samples generated by SRVR-HMC well approximate Gaussian mixture distribution.
In Figure 2(a)  we compare the perfor-
mance of SRVR-HMC with baseline
algorithms for sampling from Gaus-
sian mixture distribution. Since di-
rectly computing the 2-Wasserstein
distance is expensive  we resort to the
2] 
where ¯x = E⇡[x] is obtained via run-
ning MCMC with MH correction and
s=1001 xs/(k  1000) is the
sample path average  where xs de-
notes the s-th position iterate of the al-
gorithms and we discard the ﬁrst 1000
iterates as burn-in. We report the MSE
results of all algorithms in Figure 2(a)
by repeating each algorithms for 20
times. It can be seen that SRVR-HMC
converges faster than all baseline algo-
rithms  which is well aligned with our theory. In addition  it can be seen SG-UL-MCMC outperforms
SGHMC  which is consistent with our results in Table 1. We also compare the convergence perfor-
mance of SRVR-HMC with different batch sizes in Figure 2(b). It can be observed that SRVR-HMC
works well for all small batch sizes (B < 20) but becomes signiﬁcantly worse when B is large
(B = 50). This observation is consistent with Corollary 3.5 where we prove that when B . B1/2
the gradient complexity maintains the same.

Figure 2: Experiment results for sampling from Gaussian
mixture distribution  where X-axis represents the number of
data passes and Y-axis represents MSE: (a) Comparison with
baseline algorithms. (b) Convergence of SRVR-HMC with
varying batch size B.

mean square error (MSE) E[kbx¯xk2
bx = Pk

500 1000 1500 2000 2500 3000 3500 4000 4500 5000

500 1000 1500 2000 2500 3000 3500 4000 4500 5000

(b)

(a)

0

0

0

0

3

2

3

2

1

1.5

1

0.5

0

2.5

2.5

1.5

0.5

Independent components analysis

4.2
In
We further run the sampling algorithms for independent components analysis (ICA) tasks.
the ICA model  the input are examples {xi}n
i=1  and the likelihood function can be written as
p(x|W) = |det(W)|Ql
j=1 p(w>j x)  where W 2 Rd⇥l is the model matrix  d is the problem dimen-
sion  l denotes the number of independent components and wj denotes the j-th column of W. Fol-
lowing [50  25] we set p(w>j x) = 1/(4 cosh2(w>j x/2)) with a Gaussian prior p(W) ⇠N (0  1I).
Then the negative log-posterior can be written as f (W) = 1/nPn

i=1 fi(W)  where

F /2.

We compare the performance of SRVR-HMC with all the baseline algorithms on MEG dataset4 
which consists of 17730 time-points in 122 channels. In order to explore the performance of our

fi(W) = n log(|det(W)|)  2nPl

j=1 log cosh(w>j xi/2) + kWk2

4http://research.ics.aalto.fi/ica/eegmeg/MEG_data.html

8

8

6

4

2

0

-2

0

2

4

6

8

10

12

14

16

18

20

8

6

4

2

0

-2

-4

0

2

4

6

8

10

12

14

16

18

20

8

6

4

2

0

-2

0

2

4

6

8

10

12

14

16

18

20

8

6

4

2

0

-2

-4

0

1

2

3

4

5

6

7

8

9

10

(a) n=500  B0=100

(b) n=5000  B0=1000

(c) n=500  B0=100

(d) n=5000  B0=1000

Figure 3: Experiment results for ICA  where X-axis represents the number of data passes  and Y-axis
represents the negative log likelihood on the test dataset: (a)-(b) Comparison with different baselines
(c)-(d) Convergence of SRVR-HMC with varying batch size B.

algorithm for different sample size  we extract two subset with sizes n = 500 and n = 5000 from the
original dataset for training  and regard the rest 12730 examples as test dataset. For inference  we
compute the sample path average while discarding the ﬁrst 100 iterates as burn-in. We ﬁrst compare
the convergence performance of SRVR-HMC with baseline algorithms and report the negative log
likelihood on test dataset in Figures 3(a)-3(b)  where the batch size  minibatch size and epoch length
are set to be B0 = n/5  B = 10 and L = B0/B  and the rest hyper parameters are tuned to achieve
the best performance. It is worth noting that we do not perform the normalization when evaluating
the test likelihood  thus the negative log likelihood results may be smaller than 0. From Figures
3(a)-3(b) it can be clearly seen that SRVR-HMC outperforms all baseline algorithms  which validates
its superior theoretical properties. Again  we can see that SG-UL-MCMC can decrease the negative
log likelihood much faster than SGHMC  which is well aligned with our theory. Furthermore  we
evaluate the convergence for different minibatch size  which are displayed in Figures 3(c)-3(d)  where
the batch size B0 is ﬁxed as n/5 for both scenarios. It can be seen that SRVR-HMC attains similar
convergence performance for all small minibatch sizes (B  10 when B0 = 100 and B  20 when
B0 = 1000)  which again corroborates our theory that when B . B1/2
the gradient complexity
maintains the same.
We also evaluate our proposed algorithm SRVR-HMC on Bayesian logistic regression. We defer the
additional experimental results to Appendix E due to space limit.

0

5 Conclusions

We propose a novel algorithm SRVR-HMC based on Hamiltonian Langevin dynamics for sampling
from a class of non-log-concave target densities. We show that SRVR-HMC achieves a lower gradient
complexity in 2-Wasserstein distance than all existing HMC-type algorithms. In addition  we show
that our algorithm reduces to UL-MCMC and SG-UL-MCMC with properly chosen parameters. Our
analysis of SRVR-HMC directly applies to these two algorithms and suggests that UL-MCMC/SG-
UL-MCMC are faster than HMC/SGHMC for sampling from non-log-concave densities.

Acknowledgement

We would like to thank the anonymous reviewers for their helpful comments. This research was
sponsored in part by the National Science Foundation BIGDATA IIS-1855099 and CAREER Award
IIS-1906169. The views and conclusions contained in this paper are those of the authors and should
not be interpreted as representing any funding agencies.

References
[1] Christophe Andrieu  Nando De Freitas  Arnaud Doucet  and Michael I Jordan. An introduction

to mcmc for machine learning. Machine learning  50(1-2):5–43  2003.

[2] Jack Baker  Paul Fearnhead  Emily B Fox  and Christopher Nemeth. Control variates for
stochastic gradient MCMC. Statistics and Computing  2018. ISSN 1573-1375. doi: 10.1007/
s11222-018-9826-2.

9

[3] Rémi Bardenet  Arnaud Doucet  and Chris Holmes. On markov chain monte carlo methods for

tall data. The Journal of Machine Learning Research  18(1):1515–1557  2017.

[4] M Barkhagen  NH Chau  É Moulines  M Rásonyi  S Sabanis  and Y Zhang. On stochastic
gradient langevin dynamics with dependent data streams in the logconcave case. arXiv preprint
arXiv:1812.02709  2018.

[5] Michael Betancourt. The fundamental incompatibility of scalable Hamiltonian monte carlo and
naive data subsampling. In International Conference on Machine Learning  pages 533–540 
2015.

[6] Michael Betancourt  Simon Byrne  Sam Livingstone  Mark Girolami  et al. The geometric

foundations of Hamiltonian monte carlo. Bernoulli  23(4A):2257–2298  2017.

[7] Francois Bolley and Cedric Villani. Weighted csiszár-kullback-pinsker inequalities and applica-
tions to transportation inequalities. Annales de la Faculté des Sciences de Toulouse. Série VI.
Mathématiques  14  01 2005. doi: 10.5802/afst.1095.

[8] Nawaf Bou-Rabee  Andreas Eberle  and Raphael Zimmer. Coupling and convergence for

Hamiltonian monte carlo. arXiv preprint arXiv:1805.00452  2018.

[9] Anton Bovier  Michael Eckhoff  Véronique Gayrard  and Markus Klein. Metastability in
reversible diffusion processes i: Sharp asymptotics for capacities and exit times. Journal of the
European Mathematical Society  6(4):399–424  2004.

[10] Nicolas Brosse  Alain Durmus  and Eric Moulines. The promises and pitfalls of stochastic
gradient langevin dynamics. In Advances in Neural Information Processing Systems  pages
8268–8278  2018.

[11] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM

transactions on intelligent systems and technology (TIST)  2(3):27  2011.

[12] Niladri S Chatterji  Nicolas Flammarion  Yi-An Ma  Peter L Bartlett  and Michael I Jor-
dan. On the theory of variance reduction for stochastic gradient monte carlo. arXiv preprint
arXiv:1802.05431  2018.

[13] Ngoc Huy Chau  Éric Moulines  Miklos Rásonyi  Sotirios Sabanis  and Ying Zhang. On
stochastic gradient langevin dynamics with dependent data streams: the fully non-convex case.
arXiv preprint arXiv:1905.13142  2019.

[14] Changyou Chen  Nan Ding  and Lawrence Carin. On the convergence of stochastic gradient
mcmc algorithms with high-order integrators. In Advances in Neural Information Processing
Systems  pages 2278–2286  2015.

[15] Changyou Chen  Wenlin Wang  Yizhe Zhang  Qinliang Su  and Lawrence Carin. A convergence
analysis for a class of practical variance-reduction stochastic gradient mcmc. arXiv preprint
arXiv:1709.01180  2017.

[16] Tianqi Chen  Emily Fox  and Carlos Guestrin. Stochastic gradient Hamiltonian monte carlo. In

International Conference on Machine Learning  pages 1683–1691  2014.

[17] Xiang Cheng  Niladri S Chatterji  Yasin Abbasi-Yadkori  Peter L Bartlett  and Michael I Jordan.
Sharp convergence rates for Langevin dynamics in the nonconvex setting. arXiv preprint
arXiv:1805.01648  2018.

[18] Xiang Cheng  Niladri S. Chatterji  Peter L. Bartlett  and Michael I. Jordan. Underdamped
In Proceedings of the 31st Conference On

Langevin mcmc: A non-asymptotic analysis.
Learning Theory  volume 75  pages 300–323  2018.

[19] William Coffey and Yu P Kalmykov. The Langevin equation: with applications to stochastic
problems in physics  chemistry and electrical engineering  volume 27. World Scientiﬁc  2012.
[20] Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin
Monte Carlo and gradient descent. In Conference on Learning Theory  pages 678–689  2017.

10

[21] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-
concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
79(3):651–676  2017.

[22] Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the Langevin monte

carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095  2017.

[23] Khue-Dung Dang  Matias Quiroz  Robert Kohn  Minh-Ngoc Tran  and Mattias Villani. Hamil-
tonian monte carlo with energy conserving subsampling. Journal of machine learning research 
20(100):1–31  2019.

[24] Simon Duane  Anthony D Kennedy  Brian J Pendleton  and Duncan Roweth. Hybrid monte

carlo. Physics letters B  195(2):216–222  1987.

[25] Kumar Avinava Dubey  Sashank J Reddi  Sinead A Williamson  Barnabas Poczos  Alexander J
Smola  and Eric P Xing. Variance reduction in stochastic gradient Langevin dynamics. In
Advances in Neural Information Processing Systems  pages 1154–1162  2016.

[26] Alain Durmus  Eric Moulines  et al. Nonasymptotic convergence analysis for the unadjusted

Langevin algorithm. The Annals of Applied Probability  27(3):1551–1587  2017.

[27] Andreas Eberle  Arnaud Guillin  and Raphael Zimmer. Couplings and quantitative contraction

rates for Langevin dynamics. arXiv preprint arXiv:1703.01617  2017.

[28] Murat A Erdogdu  Lester Mackey  and Ohad Shamir. Global non-convex optimization with
discretized diffusions. In Advances in Neural Information Processing Systems  pages 9671–9680 
2018.

[29] Cong Fang  Chris Junchi Li  Zhouchen Lin  and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. In Advances in Neural
Information Processing Systems  pages 686–696  2018.

[30] Xuefeng Gao  Mert Gürbüzbalaban  and Lingjiong Zhu. Global convergence of stochastic
gradient Hamiltonian monte carlo for non-convex stochastic optimization: Non-asymptotic
performance bounds and momentum-based acceleration. arXiv preprint arXiv:1809.04618 
2018.

[31] István Gyöngy. Mimicking the one-dimensional marginal distributions of processes having an

itô differential. Probability theory and related ﬁelds  71(4):501–516  1986.

[32] Kaiyi Ji  Zhe Wang  Yi Zhou  and Yingbin Liang. Improved zeroth-order variance reduced
algorithms and analysis for nonconvex optimization. In International Conference on Machine
Learning  pages 3100–3109  2019.

[33] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  pages 315–323  2013.

[34] Peter E Kloeden and Eckhard Platen. Higher-order implicit strong numerical schemes for

stochastic differential equations. Journal of statistical physics  66(1):283–314  1992.

[35] Paul Langevin. On the theory of brownian motion. CR Acad. Sci. Paris  146:530–533  1908.

[36] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via scsg methods. In Advances in Neural Information Processing Systems  pages 2345–2355 
2017.

[37] Zhize Li  Tianyi Zhang  and Jian Li. Stochastic gradient Hamiltonian monte carlo with variance

reduction for bayesian inference. arXiv preprint arXiv:1803.11159  2018.

[38] M. Lichman. UCI machine learning repository  2013. URL http://archive.ics.uci.edu/

ml.

[39] Robert S Liptser and Albert N Shiryaev. Statistics of random processes: I. General theory 

volume 5. Springer Science & Business Media  2013.

11

[40] Yi-An Ma  Tianqi Chen  and Emily Fox. A complete recipe for stochastic gradient MCMC. In

Advances in Neural Information Processing Systems  pages 2917–2925  2015.

[41] Jonathan C Mattingly  Andrew M Stuart  and Desmond J Higham. Ergodicity for sdes and
approximations: locally lipschitz vector ﬁelds and degenerate noise. Stochastic processes and
their applications  101(2):185–232  2002.

[42] Wenlong Mou  Liwei Wang  Xiyu Zhai  and Kai Zheng. Generalization bounds of sgld for
non-convex learning: Two theoretical viewpoints. In Conference on Learning Theory  pages
605–638  2018.

[43] Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte

Carlo  2:113–162  2011.

[44] Lam M Nguyen  Jie Liu  Katya Scheinberg  and Martin Takáˇc. Sarah: A novel method for
machine learning problems using stochastic recursive gradient. arXiv preprint arXiv:1703.00102 
2017.

[45] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via stochastic
gradient Langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory 
pages 1674–1703  2017.

[46] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions

and their discrete approximations. Bernoulli  pages 341–363  1996.

[47] Yee Whye Teh  Alexandre H Thiery  and Sebastian J Vollmer. Consistency and ﬂuctuations
for stochastic gradient Langevin dynamics. The Journal of Machine Learning Research  17(1):
193–225  2016.

[48] Sebastian J Vollmer  Konstantinos C Zygalakis  and Yee Whye Teh. Exploration of the (non-)
asymptotic bias and variance of stochastic gradient Langevin dynamics. The Journal of Machine
Learning Research  17(1):5504–5548  2016.

[49] Zhe Wang  Kaiyi Ji  Yi Zhou  Yingbin Liang  and Vahid Tarokh. Spiderboost: A class of faster
variance-reduced algorithms for nonconvex optimization. arXiv preprint arXiv:1810.10690 
2018.

[50] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics.
In Proceedings of the 28th International Conference on Machine Learning  pages 681–688 
2011.

[51] Pan Xu  Jinghui Chen  Difan Zou  and Quanquan Gu. Global convergence of Langevin dynamics
based algorithms for nonconvex optimization. In Advances in Neural Information Processing
Systems  pages 3126–3137  2018.

[52] Yuchen Zhang  Percy Liang  and Moses Charikar. A hitting time analysis of stochastic gradient

Langevin dynamics. In Conference on Learning Theory  pages 1980–2022  2017.

[53] Dongruo Zhou  Pan Xu  and Quanquan Gu. Finding local minima via stochastic nested variance

reduction. arXiv preprint arXiv:1806.08782  2018.

[54] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic nested variance reduced gradient descent
for nonconvex optimization. In Advances in Neural Information Processing Systems  pages
3925–3936  2018.

[55] Difan Zou  Pan Xu  and Quanquan Gu. Stochastic variance-reduced Hamilton Monte Carlo
methods. In Proceedings of the 35th International Conference on Machine Learning  pages
6028–6037  2018.

[56] Difan Zou  Pan Xu  and Quanquan Gu. Subsampled stochastic variance-reduced gradient
Langevin dynamics. In Proceedings of International Conference on Uncertainty in Artiﬁcial
Intelligence  2018.

[57] Difan Zou  Pan Xu  and Quanquan Gu. Sampling from non-log-concave distributions via
variance-reduced gradient Langevin dynamics. In Artiﬁcial Intelligence and Statistics  vol-
ume 89 of Proceedings of Machine Learning Research  pages 2936–2945. PMLR  2019.

12

,Zhijie Deng
Hao Zhang
Xiaodan Liang
Luona Yang
Shizhen Xu
Jun Zhu
Eric Xing
Difan Zou
Pan Xu
Quanquan Gu