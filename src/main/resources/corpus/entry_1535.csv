2014,Discrete Graph Hashing,Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular  learning based hashing has received considerable attention due to its appealing storage and search efficiency. However  the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints  yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods  especially for longer codes.,Discrete Graph Hashing

Wei Liu† Cun Mu‡

†IBM T. J. Watson Research Center

Sanjiv Kumar(cid:2)

‡Columbia University

Shih-Fu Chang‡

(cid:2)Google Research

weiliu@us.ibm.com

cm3052@columbia.edu

sfchang@ee.columbia.edu

sanjivk@google.com

Abstract

Hashing has emerged as a popular technique for fast nearest neighbor search in gi-
gantic databases. In particular  learning based hashing has received considerable
attention due to its appealing storage and search efﬁciency. However  the perfor-
mance of most unsupervised learning based hashing methods deteriorates rapidly
as the hash code length increases. We argue that the degraded performance is due
to inferior optimization procedures used to achieve discrete binary codes. This
paper presents a graph-based unsupervised hashing model to preserve the neigh-
borhood structure of massive data in a discrete code space. We cast the graph
hashing problem into a discrete optimization framework which directly learns the
binary codes. A tractable alternating maximization algorithm is then proposed to
explicitly deal with the discrete constraints  yielding high-quality codes to well
capture the local neighborhoods. Extensive experiments performed on four large
datasets with up to one million samples show that our discrete optimization based
graph hashing method obtains superior search accuracy over state-of-the-art un-
supervised hashing methods  especially for longer codes.

Introduction

1
During the past few years  hashing has become a popular tool for tackling a variety of large-scale
computer vision and machine learning problems including object detection [6]  object recognition
[35]  image retrieval [22]  linear classiﬁer training [19]  active learning [24]  kernel matrix approx-
imation [34]  multi-task learning [36]  etc. In these problems  hashing is exploited to map similar
data points to adjacent binary hash codes  thereby accelerating similarity search via highly efﬁcient
Hamming distances in the code space. In practice  hashing with short codes  say about one hundred
bits per sample  can lead to signiﬁcant gains in both storage and computation. This scenario is called
Compact Hashing in the literature  which is the focus of this paper.
Early endeavors in hashing concentrated on using random permutations or projections to construct
randomized hash functions. The well-known representatives include Min-wise Hashing (MinHash)
[3] and Locality-Sensitive Hashing (LSH) [2]. MinHash estimates the Jaccard set similarity and is
improved by b-bit MinHash [18]. LSH can accommodate a variety of distance or similarity metrics
such as (cid:2)p distances for p ∈ (0  2]  cosine similarity [4]  and kernel similarity [17]. Due to random-
ized hashing  one needs more bits per hash table to achieve high precision. This typically reduces
recall  and multiple hash tables are thus required to achieve satisfactory accuracy of retrieved nearest
neighbors. The overall number of hash bits used in an application can easily run into thousands.
Beyond the data-independent randomized hashing schemes  a recent trend in machine learning is to
develop data-dependent hashing techniques that learn a set of compact hash codes using a training
set. Binary codes have been popular in this scenario for their simplicity and efﬁciency in compu-
tation. The compact hashing scheme can accomplish almost constant-time nearest neighbor search 
after encoding the whole dataset to short binary codes and then aggregating them into a hash table.
Additionally  compact hashing is particularly beneﬁcial to storing massive-scale data. For exam-
ple  saving one hundred million samples each with 100 binary bits costs less than 1.5 GB  which

1

can easily ﬁt in memory. To create effective compact codes  several methods have been proposed.
These include the unsupervised methods  e.g.  Iterative Quantization [9]  Isotropic Hashing [14] 
Spectral Hashing [38  37]  and Anchor Graph Hashing [23]  the semi-supervised methods  e.g. 
Weakly-Supervised Hashing [25]  and the supervised methods  e.g.  Semantic Hashing [30]  Binary
Reconstruction Embeddings [16]  Minimal Loss Hashing [27]  Kernel-based Supervised Hashing
[22]  Hamming Distance Metric Learning [28]  and Column Generation Hashing [20].
This paper focuses on the problem of unsupervised learning of compact hash codes. Here we argue
that most unsupervised hashing methods suffer from inadequate search performance  particularly
low recall  when applied to learn relatively longer codes (say around 100 bits) in order to achieve
higher precision. The main reason is that the discrete (binary) constraints which should be imposed
on the codes during learning itself have not been treated adequately. Most existing methods either
neglect the discrete constraints like PCA Hashing and Isotropic Hashing  or discard the constraints
to solve the relaxed optimizations and afterwards round the continuous solutions to obtain the bi-
nary codes like Spectral Hashing and Anchor Graph Hashing. Crucially  we ﬁnd that the hashing
performance of the codes obtained by such relaxation + rounding schemes deteriorates rapidly when
the code length increases (see Fig. 2). Till now  very few approaches work directly in the discrete
code space. Parameter-Sensitive Hashing [31] and Binary Reconstruction Embeddings (BRE) learn
the parameters of predeﬁned hash functions by progressively tuning the codes generated by such
functions; Iterative Quantization (ITQ) iteratively learns the codes by explicitly imposing the binary
constraints. While ITQ and BRE work in the discrete space to generate the hash codes  they do not
capture the local neighborhoods of raw data in the code space well. ITQ targets at minimizing the
quantization error between the codes and the PCA-reduced data. BRE trains the Hamming distances
to mimic the (cid:2)2 distances among a limited number of sampled data points  but could not incorporate
the entire dataset into training due to its expensive optimization procedure.
In this paper  we leverage the concept of Anchor Graphs [21] to capture the neighborhood structure
inherent in a given massive dataset  and then formulate a graph-based hashing model over the whole
dataset. This model hinges on a novel discrete optimization procedure to achieve nearly balanced
and uncorrelated hash bits  where the binary constraints are explicitly imposed and handled. To
tackle the discrete optimization in a computationally tractable manner  we propose an alternating
maximization algorithm which consists of solving two interesting subproblems. For brevity  we call
the proposed discrete optimization based graph hashing method as Discrete Graph Hashing (DGH).
Through extensive experiments carried out on four benchmark datasets with size up to one million 
we show that DGH consistently obtains higher search accuracy than state-of-the-art unsupervised
hashing methods  especially when relatively longer codes are learned.

2 Discrete Graph Hashing

First we deﬁne a few main notations used throughout this paper: sgn(x) denotes the sign function
which returns 1 for x > 0 and −1 otherwise; In denotes the n×n identity matrix; 1 denotes a vector
with all 1 elements; 0 denotes a vector or matrix of all 0 elements; diag(c) represents a diagonal
matrix with elements of vector c being its diagonal entries; tr(·)  (cid:3) · (cid:3)F  (cid:3) · (cid:3)1  and (cid:4)· ·(cid:5) express
matrix trace norm  matrix Frobenius norm  (cid:2)1 norm  and inner-product operator  respectively.
Anchor Graphs. In the discrete graph hashing model  we need to choose a neighborhood graph that
can easily scale to massive data points. For simplicity and efﬁciency  we choose Anchor Graphs [21] 
which involve no special indexing scheme but still have linear construction time in the number of
data points. An anchor graph uses a small set of m points (called anchors)  U = {uj ∈ R
j=1  to
approximate the neighborhood structure underlying the input dataset X = {xi ∈ R
i=1. Afﬁnities
(or similarities) of all n data points are computed with respect to these m anchors in linear time
O(dmn) where m (cid:6) n. The true afﬁnity matrix Ao ∈ R
n×n is then approximated by using these
afﬁnities.
(cid:2)
m) z(x) =
Speciﬁcally  an anchor graph leverages a nonlinear data-to-anchor mapping (R
δ1 exp(−D2(x u1)
/M  where δj ∈ {1  0} and δj = 1 if and only
if anchor uj is one of s (cid:6) m closest anchors of x in U according to some distance function
(cid:4)m
j=1 δj exp(−D2(x uj )
D() (e.g.  (cid:2)2 distance)  t > 0 is the bandwidth parameter  and M =
)
leading to (cid:3)z(x)(cid:3)1 = 1. Then  the anchor graph builds a data-to-anchor afﬁnity matrix Z =

) ···   δm exp(−D2(x um)

d (cid:7)→ R

(cid:3)(cid:2)

)

t

t

d}m

d}n

t

2

(cid:8)

(cid:9)

 

n(cid:7)

i j=1

1
2

B

(cid:6)(cid:2) ∈ R

n×n where Λ = diag(Z(cid:2)1) ∈ R

(cid:5)
z(x1) ···   z(xn)
n×m that is highly sparse. Finally  the anchor graph gives a data-to-data
afﬁnity matrix as A = ZΛ−1Z(cid:2) ∈ R
m×m. Such an afﬁnity
matrix empirically approximates the true afﬁnity matrix Ao  and has two nice characteristics: 1)
A is a low-rank positive semideﬁnite (PSD) matrix with rank at most m  so the anchor graph does
not need to compute it explicitly but instead keeps its low-rank form and only saves Z and Λ in
memory; 2) A has unit row and column sums  so the resulting graph Laplacian is L = In − A. The
two characteristics permit convenient and efﬁcient matrix manipulations upon A  as shown later on.
We also deﬁne an anchor graph afﬁnity function as A(x  x(cid:3)
) in which (x  x(cid:3)
)
is any pair of points in R
Learning Model. The purpose of unsupervised hashing is to learn to map each data point xi to an
r-bit binary hash code b(xi) ∈ {1 −1}r given a training dataset X = {xi}n
i=1. For simplicity  let
us denote b(xi) as bi  and the corresponding code matrix as B = [b1 ···   bn]
(cid:2) ∈ {1 −1}n×r. The
standard graph-based hashing framework  proposed by [38]  aims to learn the hash codes such that
the neighbors in the input space have small Hamming distances in the code space. This is formulated
as:

(x)Λ−1z(x(cid:3)

) = z(cid:2)

d.

(cid:8)

tr

(cid:9)

 

max

B

ij = tr

B(cid:2)AB

B(cid:2)LoB

(cid:3)bi − bj(cid:3)2Ao

s.t. B ∈ {±1}n×r  1(cid:2)B = 0  B(cid:2)B = nIr  (1)
min
where Lo is the graph Laplacian based on the true afﬁnity matrix Ao1. The constraint 1(cid:2)B = 0
is imposed to maximize the information from each hash bit  which occurs when each bit leads to
a balanced partitioning of the dataset X . Another constraint B(cid:2)B = nIr makes r bits mutually
uncorrelated to minimize the redundancy among these bits. Problem (1) is NP-hard  and Weiss et al.
[38] therefore solved a relaxed problem by dropping the discrete (binary) constraint B ∈ {±1}n×r
and making a simplifying assumption of data being distributed uniformly.
We leverage the anchor graph to replace Lo by the anchor graph Laplacian L = In − A. Hence  the
objective in Eq. (1) can be rewritten as a maximization problem:

s.t. B ∈ {1 −1}n×r  1(cid:2)B = 0  B(cid:2)B = nIr.

(2)
In [23]  the solution to this problem is obtained via spectral relaxation [33] in which B is relaxed
to be a matrix of reals followed by a thresholding step (threshold is 0) that brings the ﬁnal discrete
B. Unfortunately  this procedure may result in poor codes due to ampliﬁcation of the error caused
by the relaxation as the code length r increases. To this end  we propose to directly solve the binary
codes B without resorting to such error-prone relaxations.
n×r|1(cid:2)Y = 0  Y(cid:2)Y = nIr}. Then we formulate a more general
Let us deﬁne a set Ω =
(cid:9) − ρ
graph hashing framework which softens the last two hard constraints in Eq. (2) as:
s.t. B ∈ {1 −1}n×r 

(3)
where dist(B  Ω) = minY∈Ω (cid:3)B − Y(cid:3)F measures the distance from any matrix B to the set Ω  and
ρ ≥ 0 is a tuning parameter. If problem (2) is feasible  we can enforce dist(B  Ω) = 0 in Eq. (3) by
(cid:8)
imposing a very large ρ  thereby turning problem (3) into problem (2). However  in Eq. (3) we allow
a certain discrepancy between B and Ω (controlled by ρ)  which makes problem (3) more ﬂexible.
Y(cid:2)Y) = nr  problem (3) can be equivalently transformed to the following
Since tr
problem:
Q(B  Y) := tr

Y ∈ R
(cid:8)

B(cid:2)B) = tr

dist2(B  Ω) 

B(cid:2)AB

B(cid:2)Y

max

B

(cid:10)

(cid:8)

(cid:8)

(cid:9)

(cid:8)

(cid:9)

tr

2

B(cid:2)AB
max
B Y
s.t. B ∈ {1 −1}n×r  Y ∈ R

 

+ ρtr
n×r  1(cid:2)Y = 0  Y(cid:2)Y = nIr.

(4)
We call the code learning model formulated in Eq. (4) as Discrete Graph Hashing (DGH). Because
concurrently imposing B ∈ {±1}n×r and B ∈ Ω will make graph hashing computationally in-
tractable  DGH does not pursue the latter constraint but penalizes the distance from the target code
matrix B to Ω. Different from the previous graph hashing methods which discard the discrete con-
straint B ∈ {±1}n×r to obtain continuously relaxed B  our DGH model enforces this constraint to
directly achieve discrete B. As a result  DGH yields nearly balanced and uncorrelated binary bits. In
Section 3  we will propose a computationally tractable optimization algorithm to solve this discrete
programming problem in Eq. (4).

1The spectral hashing method in [38] did not compute the true afﬁnity matrix Ao because of the scalability

issue  but instead used a complete graph built over 1D PCA embeddings.

3

Algorithm 1 Signed Gradient Method (SGM) for B-Subproblem

Input: B(0) ∈ {1  −1}n×r and Y ∈ Ω.
j := 0; repeat B(j+1) := sgn
Output: B = B(j).

(cid:8)C(cid:8)

(cid:9)(cid:9)

2AB(j) + ρY  B(j)

  j := j + 1  until B(j) converges.

Out-of-Sample Hashing. Since a hashing scheme should be able to generate the hash code for any
data point q ∈ R
d beyond the points in the training set X   here we address the out-of-sample ex-
tension of the DGH model. Similar to the objective in Eq. (1)  we minimize the Hamming distances
between a novel data point q and its neighbors (revealed by the afﬁnity function A) in X as
(cid:13)
b(q) ∈ arg min
b(q)∈{±1}r
where B∗
= [b∗
(cid:2)ZΛ−1 ∈ R
(B∗
for any novel data point q very efﬁciently.

(cid:11)(cid:11)b(q) − b∗
 
(cid:9)
(cid:2) is the solution of problem (4). After pre-computing a matrix W =
Wz(q)

1
2
1 ···   b∗
n]
r×m in the training phase  one can compute the hash code b∗

A(q  xi) = arg max

(cid:2)ZΛ−1z(q)

b(q)  (B∗

b(q)∈{±1}r

(q) = sgn

n(cid:7)

(cid:11)(cid:11)2

(cid:12)

(cid:8)

i=1

)

)

i

3 Alternating Maximization
The graph hashing problem in Eq. (4) is essentially a nonlinear mixed-integer program involving
both discrete variables in B and continuous variables in Y. It turns out that problem (4) is generally
NP-hard and also difﬁcult to approximate. In speciﬁc  since the Max-Cut problem is a special case
of problem (4) when ρ = 0 and r = 1  there exists no polynomial-time algorithm which can achieve
the global optimum  or even an approximate solution with its objective value beyond 16/17 of the
global maximum unless P = NP [11]. To this end  we propose a tractable alternating maximization
algorithm to optimize problem (4)  leading to good hash codes which are demonstrated to exhibit
superior search performance through extensive experiments conducted in Section 5.
(cid:9)
The proposed algorithm proceeds by alternately solving the B-subproblem

(cid:9)

(cid:8)

(cid:8)

f (B) := tr

B(cid:2)AB

Y(cid:2)B

+ ρtr

(5)

and the Y-subproblem

max

B∈{±1}n×r
(cid:8)

max
Y∈Rn×r

tr

(cid:9)

 

s.t. 1(cid:2)Y = 0  Y(cid:2)Y = nIr.

(6)

B(cid:2)Y

(cid:9)

In what follows  we propose an iterative ascent procedure called Signed Gradient Method for sub-
problem (5) and derive a closed-form optimal solution to subproblem (6). As we can show  our
alternating algorithm is provably convergent. Schemes for choosing good initializations are also
discussed. Due to the space limit  all the proofs of lemmas  theorems and propositions presented in
this section are placed in the supplemental material.
3.1 B-Subproblem
We tackle subproblem (5) with a simple iterative ascent procedure described in Algorithm 1. In the
j-th iteration  we deﬁne a local function ˆfj(B) that linearizes f (B) at the point B(j)  and employ
ˆfj(B) as a surrogate of f (B) for discrete optimization. Given B(j)  the next discrete point is
derived as B(j+1) ∈ arg maxB∈{±1}n×r ˆfj(B) := f
. Note that
since ∇f
may include zero entries  multiple solutions for B(j+1) could exist. To avoid this
ambiguity  we introduce the function C(x  y) =

B(j)
(cid:14)
x  x (cid:11)= 0
(cid:15)
(cid:9)(cid:16)
y  x = 0 to specify the following update:

  B − B(j)

(7)
in which C is applied in an element-wise manner  and no update is carried out to the entries where
∇f
Due to the PSD property of the matrix A  f is a convex function and thus f (B) ≥ ˆfj(B) for any B.
(cid:17)
  Lemma 1 ensures
Taking advantage of the fact f
that both the sequence of cost values

(cid:8)
and the sequence of iterates

(cid:8)
(cid:17)
f (B(j))

(cid:9) ≥ ˆfj

(cid:9) ≥ ˆfj

(cid:9) ≡ f

2AB(j) + ρY  B(j)

C(cid:8)∇f

B(j+1) := sgn

(cid:12)∇f

converge.

vanishes.

C(cid:8)

B(j+1)

B(j+1)

  B(j)

(cid:9)(cid:16)

= sgn

B(j)

B(j)

B(j)

B(j)

(cid:9)

(cid:10)

(cid:8)

(cid:9)

B(j)

B(j)

B(j)

(cid:9)

+

(cid:15)

(cid:10)

(cid:8)

(cid:9)

(cid:8)

(cid:9)

(cid:8)

(cid:13)

 

(cid:8)

(cid:8)

(cid:8)

4

Algorithm 2 Discrete Graph Hashing (DGH)

(cid:9)

B(j)

B(j)

(cid:10)

= Yk.

(cid:8)

B(j+1)

(cid:17)

(cid:10)

(cid:17)

f (B(j))

and

B(j)

converge.

= Bk  Y∗
(cid:10)

(cid:17)

holds for any integer j ≥ 0  and both

is the sequence of iterates produced by Algorithm 1  then f

Input: B0 ∈ {1 −1}n×r and Y0 ∈ Ω.
k := 0;
repeat Bk+1 := SGM(Bk  Yk)  Yk+1 ∈ Φ(JBk+1)  k := k + 1  until Q(Bk  Yk) converges.
Output: B∗
(cid:9) ≥

(cid:8)
Lemma 1. If
f
Our idea of optimizing a proxy function ˆfj(B) can be considered as a special case of majorization
methodology exploited in the ﬁeld of optimization. The majorization method typically deals with a
generic constrained optimization problem: min g(x)  s.t. x ∈ F  where g : R
n (cid:7)→ R is a contin-
uous function and F ⊆ R
n is a compact set. The majorization method starts with a feasible point
x0 ∈ F  and then proceeds by setting xj+1 as a minimizer of ˆgj(x) over F  where ˆgj satisfying
ˆgj(xj) = g(xj) and ˆgj(x) ≥ g(x) ∀x ∈ F is called a majorization function of g at xj. In speciﬁc 
in our scenario  problem (5) is equivalent to minB∈{±1}n×r −f (B)  and the linear surrogate − ˆfj
is a majorization function of −f at point B(j). The majorization method was ﬁrst systematically
introduced by [5] to deal with multidimensional scaling problems  although the EM algorithm [7] 
proposed at the same time  also falls into the framework of majorization methodology. Since then 
the majorization method has played an important role in various statistics problems such as multi-
dimensional data analysis [12]  hyperparameter learning [8]  conditional random ﬁelds and latent
likelihoods [13]  and so on.
3.2 Y-Subproblem
(cid:4)r(cid:2)
An analytical solution to subproblem (6) can be obtained with the aid of a centering matrix J = In−
. Write the singular value decomposition (SVD) of JB as JB = UΣV(cid:2)
n 11(cid:2)
k=1 σkukv(cid:2)
1
k  
(cid:3) ≤ r is the rank of JB  σ1 ···   σr(cid:2) are the positive singular values  and U = [u1 ···   ur(cid:2) ]
where r
and V = [v1 ···   vr(cid:2) ] contain the left- and right-singular vectors  respectively. Then  by employing
a Gram-Schmidt process  one can easily construct matrices ¯U ∈ R
r×(r−r(cid:2))
such that ¯U(cid:2) ¯U = Ir−r(cid:2)  [U 1]
(cid:2) ¯U = 0  and ¯V(cid:2) ¯V = Ir−r(cid:2)  V(cid:2) ¯V = 02. Now we are ready to
characterize a closed-form solution of the Y-subproblem by Lemma 2.
Lemma 2. Y(cid:3) =
(cid:2)
For notational convenience  we deﬁne the set of all matrices in the form of
as Φ(JB). Lemma 2 reveals that any matrix in Φ(JB) is an optimal solution to subproblem (6).
In practice  to compute such an optimal Y(cid:3)  we perform the eigendecomposition over the small
r × r matrix B(cid:2)JB to have B(cid:2)JB = [V ¯V]
(cid:2)  which gives V  ¯V  Σ  and
immediately leads to U = JBVΣ−1. The matrix ¯U is initially set to a random matrix followed
by the aforementioned Gram-Schmidt orthogonalization. It can be seen that Y(cid:3) is uniquely optimal
when r
3.3 DGH Algorithm
The proposed alternating maximization algorithm  also referred to as Discrete Graph Hashing
(DGH)  for solving the raw problem in Eq. (4) is summarized in Algorithm 2  in which we introduce
SGM(· ·) to represent the functionality of Algorithm 1. The convergence of Algorithm 2 is guar-
anteed by Theorem 1  whose proof is based on the nature of the proposed alternating maximization
procedure that always generates a monotonically non-decreasing and bounded sequence.
is the sequence generated by Algorithm 2  then Q(Bk+1  Yk+1) ≥
Theorem 1. If
Q(Bk  Yk) holds for any integer k ≥ 0  and
converges starting with any feasible
initial point (B0  Y0).
Initialization. Since the DGH algorithm deals with discrete and non-convex optimization  a good
choice of an initial point (B0  Y0) is vital. Here we suggest two different initial points which are
both feasible to problem (4).

is an optimal solution to the Y-subproblem in Eq. (6).

(cid:10)Q(Bk  Yk)

(cid:17)

n×(r−r(cid:2)) and ¯V ∈ R

= r (i.e.  JB is full column rank).

n[U ¯U][V ¯V]

(cid:2)

Σ2 0
0
0

[V ¯V]

=

√

n[U ¯U][V ¯V]

(cid:10)

(Bk  Yk)

(cid:17)

(cid:18)

(cid:19)

√

(cid:3)

2Note that when r(cid:2) = r  ¯U and ¯V are nothing but 0.

5

(cid:4)m
Let us perform the eigendecomposition over A to obtain A = PΘP(cid:2)
k=1 θkpkp(cid:2)
k   where
θ1 ···   θm are the eigenvalues arranged in a non-increasing order  and p1 ···   pm are the corre-
(cid:9)
sponding normalized eigenvectors. We write Θ = diag(θ1 ···   θm) and P = [p1 ···   pm]. Note
√
that θ1 = 1 and p1 = 1/
  where
H = [p2 ···   pr+1] ∈ R
Alternatively  Y0 can be allowed to consist of orthonormal columns within the column space of H 
r×r. We can obtain R along with B0
i.e.  Y0 =
by solving a new discrete optimization problem:
s.t. R ∈ R
= Ir  B0 ∈ {1 −1}n×r 

√
n. The ﬁrst initialization used is
n×r. The initial codes B0 were used as the ﬁnal codes by [23].

nHR subject to some orthogonal matrix R ∈ R
r×r  RR(cid:2)

R(cid:2)H(cid:2)AB0

nH  B0 = sgn(H)

Y0 =

√

(cid:8)

(cid:9)

(cid:8)

tr

=

 

(8)

max
R B0

(cid:8)

(cid:8)

.

(cid:9)

.

(cid:8)

(cid:8)

√

Y0 =

R(cid:2)H(cid:2)AB

r
B(cid:2)AB

B(cid:2)AB
(cid:8)

(cid:9) ≥ 1
(cid:9)

nHR  B0 = sgn(H ˆΘR)

j for j = 0  1  2 ···   where ˜Uj  ˜Vj ∈ R

r×r and any binary matrix B ∈ {1 −1}n×r  we

which is motivated by the proposition below.
Proposition 1. For any orthogonal matrix R ∈ R
tr2
have tr
Proposition 1 implies that the optimization in Eq. (8) can be interpreted as to maximize a lower
which is the ﬁrst term of the objective Q(B  Y) in the original problem
bound of tr
(cid:8)
(4). We still exploit an alternating maximization procedure to solve problem (8). Noticing AH =
(cid:9)
H ˆΘ where ˆΘ = diag(θ2 ···   θr+1)  the objective in Eq. (8) is equal to tr
R(cid:2) ˆΘH(cid:2)B0). The
alternating procedure starts with R0 = Ir  and then makes the simple updates Bj
H ˆΘRj
 
0 := sgn
r×r stem from the full SVD ˜Uj ˜Σj ˜V(cid:2)
Rj+1 := ˜Uj ˜V(cid:2)
j of
(cid:9)
the matrix ˆΘH(cid:2)Bj
0. When convergence is reached  we obtain the optimized rotation R that yields
the second initialization
Empirically  we ﬁnd that the second initialization typically gives a better objective value Q(B0  Y0)
at the start than the ﬁrst one  as it aims to maximize the lower bound of the ﬁrst term in the objective
Q. We also observe that the second initialization often results in a higher objective value Q(B∗
  Y∗
)
at convergence (Figs. 1-2 in the supplemental material show convergence curves of Q starting from
the two initial points). We call DGH using the ﬁrst and second initializations as DGH-I and DGH-R 
respectively. Regarding the convergence property  we would like to point out that since the DGH al-
gorithm (Algorithm 2) works on a mixed-integer objective  it is hard to quantify the convergence to a
local optimum of the objective function Q. Nevertheless  this does not affect the performance of our
algorithm in practice. In our experiments in Section 5  we consistently ﬁnd a convergent sequence
{(Bk  Yk)} arriving at a good objective value when started with the suggested initializations.
(cid:8)
4 Discussions
Here we analyze space and time complexities of DGH-I/DGH-R. The space complexity is O
(d +
in the training stage and O(rn) for storing hash codes in the test stage for DGH-I/DGH-R.
s + r)n
Let TB and TG be the budget iteration numbers of optimizing the B-subproblem and the whole DGH
(cid:9)
dmn + m2n + (mTB +
problem  respectively. Then  the training time complexity of DGH-I is O
dmn + m2n + (mTB + sTB +
sTB + r)rTGn
r)rTGn + r2TRn
  where TR is the budget iteration number for seeking the initial point via Eq. (8).
Note that the time for ﬁnding anchors and building the anchor graph is O(dmn) which is included
in the above training time. Their test time (referring to encoding a query to an r-bit code) is both
O(dm + sr). In our experiments  we ﬁx m  s  TB  TG  TR to constants independent of the dataset
size n  and make r ≤ 128. Thus  DGH-I/DGH-R enjoy linear training time and constant test time.
It is worth mentioning again that the low-rank PSD property of the anchor graph afﬁnity matrix A is
advantageous for training DGH  permitting efﬁcient matrix computations in O(n) time  such as the
eigendecomposition of A (encountered in initializations) and multiplying A with B (encountered
in solving the B-subproblem with Algorithm 1).
It is interesting to point out that DGH falls into the asymmetric hashing category [26] in the sense
that hash codes are generated differently for samples within the dataset and queries outside the
dataset. Unlike most existing hashing techniques  DGH directly solves the hash codes B∗ of the
training samples via the proposed discrete optimization in Eq. (4) without relying on any explicit or
predeﬁned hash functions. On the other hand  the hash code for any query q is induced from the
solved codes B∗  leading to a hash function b∗
parameterized by the matrix

(cid:9)
  and the training time complexity of DGH-R is O

(q) = sgn

Wz(q)

(cid:8)

(cid:9)

(cid:8)

(cid:8)

(cid:9)

6

(a) Hash lookup success rate @ CIFAR−10

(b) Hash lookup success rate @ SUN397

(c) Hash lookup success rate @ YouTube Faces

(d) Hash lookup success rate @ Tiny−1M

48
# bits

64

96

48
# bits

64

96

64
# bits

96

128

1216 24 32

48

64
# bits

Figure 1: Hash lookup success rates for different hashing techniques. DGH tends to achieve nearly
100% success rates even for longer code lengths.

(a) Hash lookup F−measure @ CIFAR−10

(b) Hash lookup F−measure @ SUN397

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

LSH
KLSH
ITQ
IsoH
SH
IMH
1−AGH
2−AGH
BRE
DGH−I
DGH−R

96

128

(c) Hash lookup F−measure @ YouTube Faces

(d) Hash lookup F−measure @ Tiny−1M

0.25

0.2

0.15

0.1

0.05

LSH
KLSH
ITQ
IsoH
SH
IMH
1−AGH
2−AGH
BRE
DGH−I
DGH−R
24

8 1216

32

e

t

a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

2

i

 
s
u
d
a
r
 

i

g
n
m
m
a
H
n
h

 

i

t
i

 

w
e
r
u
s
a
e
m
−
F

0.3

0.25

0.2

0.15

0.1

0.05

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

LSH
KLSH
ITQ
IsoH
SH
IMH
1−AGH
2−AGH
BRE
DGH−I
DGH−R
24

8 1216

32

LSH
KLSH
ITQ
IsoH
SH
IMH
1−AGH
2−AGH
BRE
DGH−I
DGH−R

64

96

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

LSH
KLSH
ITQ
IsoH
SH
IMH
1−AGH
2−AGH
BRE
DGH−I
DGH−R
1216 24 32

48

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

96

0

128

1216 24 32

48

64
# bits

96

128

8 1216

24

32

48
# bits

8 1216

24

32

64

96

1216 24 32

48

48
# bits

64
# bits

Figure 2: Mean F-measures of hash lookup within Hamming radius 2 for different techniques. DGH
tends to retain good recall even for longer codes  leading to much higher F-measures than the others.
W which was computed using B∗. While the hashing mechanisms for producing B∗ and b∗
(q) are
distinct  they are tightly coupled and prone to be adaptive to speciﬁc datasets. The ﬂexibility of the
asymmetric hashing nature of DGH is validated through the experiments shown in the next section.
5 Experiments
We conduct large-scale similarity search experiments on four benchmark datasets: CIFAR-10 [15] 
SUN397 [40]  YouTube Faces [39]  and Tiny-1M. CIFAR-10 is a labeled subset of the 80 Million
Tiny Images dataset [35]  which consists of 60K images from ten object categories with each image
represented by a 512-dimensional GIST feature vector [29]. SUN397 contains about 108K images
from 397 scene categories  where each image is represented by a 1 600-dimensional feature vector
extracted by PCA from 12 288-dimensional Deep Convolutional Activation Features [10]. The raw
YouTube Faces dataset contains 1 595 different people  from which we choose 340 people such that
each one has at least 500 images to form a subset of 370 319 face images  and represent each face
image as a 1 770-dimensional LBP feature vector [1]. Tiny-1M is one million subset of the 80M
tiny images  where each image is represented by a 384-dimensional GIST vector. In CIFAR-10  100
images are sampled uniformly randomly from each object category to form a separate test (query)
set of 1K images; in SUN397  100 images are sampled uniformly randomly from each of the 18
largest scene categories to form a test set of 1.8K images; in YouTube Faces  the test set includes
3.8K face images which are evenly sampled from the 38 people each containing more than 2K faces;
in Tiny-1M  a separate subset of 5K images randomly sampled from the 80M images is used as the
test set. In the ﬁrst three datasets  groundtruth neighbors are deﬁned based on whether two samples
share the same class label; in Tiny-1M which does not have full annotations  we deﬁne groundtruth
neighbors for a given query as the samples among the top 2% (cid:2)2 distances from the query in the 1M
training set  so each query has 20K groundtruth neighbors.
We evaluate twelve unsupervised hashing methods including: two randomized methods LSH [2] and
Kernelized LSH (KLSH) [17]  two linear projection based methods Iterative Quantization (ITQ) [9]
and Isotropic Hashing (IsoH) [14]  two spectral methods Spectral Hashing (SH) [38] and its weight-
ed version MDSH [37]  one manifold based method Inductive Manifold Hashing (IMH) [32]  two
existing graph-based methods One-Layer Anchor Graph Hashing (1-AGH) and Two-Layer Anchor
Graph Hashing (2-AGH) [23]  one distance preservation method Binary Reconstruction Embed-
dings (BRE) [16] (unsupervised version)  and our proposed discrete optimization based methods
DGH-I and DGH-R. We use the publicly available codes of the competing methods  and follow the
conventional parameter settings therein. In particular  we use the Gaussian kernel and 300 randomly
sampled exemplars (anchors) to run KLSH; IMH  1-AGH  2-AGH  DGH-I and DGH-R also use
m = 300 anchors (obtained by K-means clustering with 5 iterations) for fair comparison. This
choice of m gives a good trade-off between hashing speed and performance. For 1-AGH  2-AGH 
DGH-I and DGH-R that all use anchor graphs  we adopt the same construction parameters s  t on
each dataset (s = 3 and t is tuned following AGH)  and (cid:2)2 distance as D(·). For BRE  we uniformly

7

Table 1: Hamming ranking performance on YouTube Faces and Tiny-1M. r denotes the number of
hash bits used in the hashing methods. All training and test times are in seconds.
Method
Tiny-1M

YouTube Faces

Mean Precision / Top-2K

r = 48

r = 128

TrainTime
r = 128

TestTime
r = 128

Mean Precision / Top-20K

r = 48

r = 96

r = 128

TrainTime
r = 128

TestTime
r = 128

r = 96
0.7591
0.1005
0.5210
0.7493
0.6962
0.6655
0.6752
0.3641
0.7571
0.7377
0.6238
0.7644
0.7672

–

–

(cid:4)2 Scan

1

0.1324
0.4105
0.4726
0.4816
0.1923
0.3878
0.2497
0.4117
0.4099
0.4836
0.4865
0.5006

LSH
KLSH
ITQ
IsoH
SH

0.0830
0.3982
0.7017
0.6093
0.5897
0.6110
0.3150
0.7138
0.6727
0.5564
0.7086
0.7245

0.1061
0.5871
0.7562
0.7058
0.6736
0.6795
0.3889
0.7646
0.7521
0.6483
0.7750
0.7805

0.1155
0.3054
0.3925
0.3896
0.1857
0.3312
0.2257
0.4061
0.3925
0.3943
0.4045
0.4208

6.1
20.7
297.3
13.5
61.4
193.6
139.3
141.4
272.5
8419.0
1769.4
2793.4

0.1766
0.4705
0.5052
0.5161
0.2079
0.3955
0.2557
0.4107
0.4152
0.5218
0.5178
0.5358

6.4
16.1
169.0
73.6
108.9
118.8
92.1
84.1
94.7
10372.0
402.6
408.9

1.8×10−5
4.8×10−5
1.8×10−5
1.8×10−5
2.0×10−4
4.9×10−5
2.3×10−5
2.1×10−5
3.5×10−5
9.0×10−5
2.1×10−5
2.1×10−5

1.0×10−5
4.6×10−5
1.0×10−5
1.0×10−5
1.6×10−4
2.8×10−5
MDSH
2.7×10−5
IMH
3.4×10−5
1-AGH
4.7×10−5
2-AGH
8.8×10−5
BRE
3.3×10−5
DGH-I
3.3×10−5
DGH-R
randomly sample 1K  and 2K training samples to train the distance preservations on CIFAR-10 &
SUN397  and YouTube Faces & Tiny-1M  respectively. For DGH-I and DGH-R  we set the penalty
parameter ρ to the same value in [0.1  5] on each dataset  and ﬁx TR = 100  TB = 300  TG = 20.
We employ two widely used search procedures hash lookup and Hamming ranking with 8 to 128
hash bits for evaluations. The Hamming ranking procedure ranks the dataset samples according to
their Hamming distances to a given query  while the hash lookup procedure ﬁnds all the points within
a certain Hamming radius away from the query. Since hash lookup can be achieved in constant time
by using a single hash table  it is the main focus of this work. We carry out hash lookup within a
Hamming ball of radius 2 centered on each query  and report the search recall and F-measure which
are averaged over all queries for each dataset. Note that if table lookup fails to ﬁnd any neighbors
within a given radius for a query  we call it a failed query and assign it zero recall and F-measure. To
quantify the failed queries  we report the hash lookup success rate which gives the proportion of the
queries for which at least one neighbor is retrieved. For Hamming ranking  mean average precision
(MAP) and mean precision of top-retrieved samples are computed.
The hash lookup results are shown in Figs. 1-2. DGH-I/DGH-R achieve the highest (close to 100%)
hash lookup success rates  and DGH-I is slightly better than DGH-R. The reason is that the asym-
metric hashing scheme exploited by DGH-I/DGH-R poses a tight linkage to connect queries and
database samples  providing a more adaptive out-of-sample extension than the traditional symmet-
ric hashing schemes used by the competing methods. Also  DGH-R achieves the highest F-measure
except on CIFAR-10  where DGH-I is highest while DGH-R is the second. The F-measures of
KLSH  IsoH  SH and BRE deteriorate quickly and are with very poor values (< 0.05) when r ≥ 48
due to poor recall3. Although IMH achieves nice hash lookup succuss rates  its F-measures are
much lower than DGH-I/DGH-R due to lower precision. MDSH produces the same hash bits as SH 
so is not included in the hash lookup experiments. DGH-I/DGH-R employ the proposed discrete
optimization to yield high-quality codes that preserve the local neighborhood of each data point
within a small Hamming ball  so obtain much higher search accuracy in F-measure and recall than
SH  1-AGH and 2-AGH which rely on relaxed optimizations and degrade drastically when r ≥ 48.
Finally  we report the Hamming ranking results in Table 1 and the table in the sup-material  which
clearly show the superiority of DGH-R over the competing methods in MAP and mean precision;
on the ﬁrst three datasets  DGH-R even outperforms exhaustive (cid:2)2 scan. The training time of DGH-
I/DGH-R is acceptable and faster than BRE  and their test time (i.e.  coding time since hash lookup
time is small enough to be ignored) is comparable with 1-AGH.
6 Conclusion
This paper investigated a pervasive problem of not enforcing the discrete constraints in optimiza-
tion pertaining to most existing hashing methods. Instead of resorting to error-prone continuous
relaxations  we introduced a novel discrete optimization technique that learns the binary hash codes
directly. To achieve this  we proposed a tractable alternating maximization algorithm which solves
two interesting subproblems and provably converges. When working with a neighborhood graph 
the proposed method yields high-quality codes to well preserve the neighborhood structure inherent
in the data. Extensive experimental results on four large datasets up to one million showed that our
discrete optimization based graph hashing technique is highly competitive.

3The recall results are shown in Fig. 3 of the supplemental material  which indicate that DGH-I achieves the

highest recall except on YouTube Faces  where DGH-R is highest while DGH-I is the second.

8

References
[1] T. Ahonen  A. Hadid  and M. Pietikainen. Face description with local binary patterns: Application to face recognition. TPAMI 

28(12):2037–2041  2006.

[2] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the

ACM  51(1):117–122  2008.

[3] A. Z. Broder  M. Charikar  A. M. Frieze  and M. Mitzenmacher. Min-wise independent permutations. In Proc. STOC  1998.
[4] M. Charikar. Similarity estimation techniques from rounding algorithms. In Proc. STOC  2002.
[5] J. de Leeuw. Applications of convex analysis to multidimensinal scaling. Recent Developments in Statistics  pages 133–146  1977.
[6] T. Dean  M. A. Ruzon  M. Segal  J. Shlens  S. Vijayanarasimhan  and J. Yagnik. Fast  accurate detection of 100 000 object classes on a

single machine. In Proc. CVPR  2013.

[7] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal

Statistical Society  Series B  39(1):1–38  1977.

[8] C.-S. Foo  C. B. Do  and A. Y. Ng. A majorization-minimization algorithm for (multiple) hyperparameter learning. In Proc. ICML  2009.
[9] Y. Gong  S. Lazebnik  A. Gordo  and F. Perronnin. Iterative quantization: A procrustean approach to learning binary codes for large-scale

image retrieval. TPAMI  35(12):2916–2929  2013.

[10] Y. Gong  L. Wang  R. Guo  and S. Lazebnik. Multi-scale orderless pooling of deep convolutional activation features. In Proc. ECCV 

2014.

[11] J. Hastad. Some optimal inapproximability results. Journal of the ACM  48(4):798–859  2001.
[12] W. J. Heiser. Convergent computation by iterative majorization: theory and applications in multidimensional data analysis. Recent

advances in descriptive multivariate analysis  pages 157–189  1995.

[13] T. Jebara and A. Choromanska. Majorization for crfs and latent likelihoods. In NIPS 25  2012.
[14] W. Kong and W.-J. Li. Isotropic hashing. In NIPS 25  2012.
[15] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report  2009.
[16] B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. In NIPS 22  2009.
[17] B. Kulis and K. Grauman. Kernelized locality-sensitive hashing. TPAMI  34(6):1092–1104  2012.
[18] P. Li and A. C. Konig. Theory and applications of b-bit minwise hashing. Communications of the ACM  54(8):101–109  2011.
[19] P. Li  A. Shrivastava  J. Moore  and A. C. Konig. Hashing algorithms for large-scale learning. In NIPS 24  2011.
[20] X. Li  G. Lin  C. Shen  A. van den Hengel  and A. R. Dick. Learning hash functions using column generation. In Proc. ICML  2013.
[21] W. Liu  J. He  and S.-F. Chang. Large graph construction for scalable semi-supervised learning. In Proc. ICML  2010.
[22] W. Liu  J. Wang  R. Ji  Y.-G. Jiang  and S.-F. Chang. Supervised hashing with kernels. In Proc. CVPR  2012.
[23] W. Liu  J. Wang  S. Kumar  and S.-F. Chang. Hashing with graphs. In Proc. ICML  2011.
[24] W. Liu  J. Wang  Y. Mu  S. Kumar  and S.-F. Chang. Compact hyperplane hashing with bilinear functions. In Proc. ICML  2012.
[25] Y. Mu  J. Shen  and S. Yan. Weakly-supervised hashing in kernel space. In Proc. CVPR  2010.
[26] B. Neyshabur  P. Yadollahpour  Y. Makarychev  R. Salakhutdinov  and N. Srebro. The power of asymmetry in binary hashing. In NIPS

26  2013.

[27] M. Norouzi and D. J. Fleet. Minimal loss hashing for compact binary codes. In Proc. ICML  2011.
[28] M. Norouzi  D. J. Fleet  and R. Salakhudinov. Hamming distance metric learning. In NIPS 25  2012.
[29] A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope. IJCV  42(3):145–175  2001.
[30] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning  50(7):969–978  2009.
[31] G. Shakhnarovich  P. Viola  and T. Darrell. Fast pose estimation with parameter-sensitive hashing. In Proc. ICCV  2003.
[32] F. Shen  C. Shen  Q. Shi  A. van den Hengel  and Z. Tang. Inductive hashing on manifolds. In Proc. CVPR  2013.
[33] J. Shi and J. Malik. Normalized cuts and image segmentation. TPAMI  22(8):888–905  2000.
[34] Q. Shi  J. Petterson  G. Dror  J. Langford  A. Smola  and S. V. N. Vishwanathan. Hash kernels for structured data. JMLR  10:2615–2637 

2009.

[35] A. Torralba  R. Fergus  and W. T. Freeman. 80 million tiny images: a large dataset for non-parametric object and scene recognition.

TPAMI  30(11):1958–1970  2008.

[36] K. Q. Weinberger  A. Dasgupta  J. Langford  A. J. Smola  and J. Attenberg. Feature hashing for large scale multitask learning. In Proc.

ICML  2009.

[37] Y. Weiss  R. Fergus  and A. Torralba. Multidimensional spectral hashing. In Proc. ECCV  2012.
[38] Y. Weiss  A. Torralba  and R. Fergus. Spectral hashing. In NIPS 21  2008.
[39] L. Wolf  T. Hassner  and I. Maoz. Face recognition in unconstrained videos with matched background similarity. In Proc. CVPR  2011.
[40] J. Xiao  J. Hays  K. A. Ehinger  A. Oliva  and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Proc.

CVPR  2010.

9

,Wei Liu
Cun Mu
Sanjiv Kumar
Shih-Fu Chang
Yi Ren
Yangjun Ruan
Xu Tan
Tao Qin
Sheng Zhao
Zhou Zhao
Tie-Yan Liu