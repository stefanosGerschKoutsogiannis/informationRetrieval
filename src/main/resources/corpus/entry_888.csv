2010,CUR from a Sparse Optimization Viewpoint,The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard  it appears to be similar to many sparse PCA methods. However  CUR takes a randomized algorithmic approach whereas most sparse PCA methods are framed as convex optimization problems. In this paper  we try to understand CUR from a sparse optimization viewpoint. In particular  we show that CUR is implicitly optimizing a sparse regression objective and  furthermore  cannot be directly cast as a sparse PCA method. We observe that the sparsity attained by CUR possesses an interesting structure  which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.,CUR from a Sparse Optimization Viewpoint

Jacob Bien∗

Ya Xu∗

Department of Statistics

Department of Statistics

Stanford University
Stanford  CA 94305

Stanford University
Stanford  CA 94305

Michael W. Mahoney

Department of Mathematics

Stanford University
Stanford  CA 94305

jbien@stanford.edu

yax.stanford@gmail.com

mmahoney@cs.stanford.edu

Abstract

The CUR decomposition provides an approximation of a matrix X that has low
reconstruction error and that is sparse in the sense that the resulting approximation
lies in the span of only a few columns of X. In this regard  it appears to be simi-
lar to many sparse PCA methods. However  CUR takes a randomized algorithmic
approach  whereas most sparse PCA methods are framed as convex optimization
problems. In this paper  we try to understand CUR from a sparse optimization
viewpoint. We show that CUR is implicitly optimizing a sparse regression objec-
tive and  furthermore  cannot be directly cast as a sparse PCA method. We also
observe that the sparsity attained by CUR possesses an interesting structure  which
leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.

1 Introduction
CUR decompositions are a recently-popular class of randomized algorithms that approximate a data
matrix X ∈ Rn×p by using only a small number of actual columns of X [12  4]. CUR decomposi-
tions are often described as SVD-like low-rank decompositions that have the additional advantage of
being easily interpretable to domain scientists. The motivation to produce a more interpretable low-
rank decomposition is also shared by sparse PCA (SPCA) methods  which are optimization-based
procedures that have been of interest recently in statistics and machine learning.

Although CUR and SPCA methods start with similar motivations  they proceed very differently. For
example  most CUR methods have been randomized  and they take a purely algorithmic approach.
By contrast  most SPCA methods start with a combinatorial optimization problem  and they then
solve a relaxation of this problem. Thus far  it has not been clear to researchers how the CUR and
SPCA approaches are related. It is the purpose of this paper to understand CUR decompositions
from a sparse optimization viewpoint  thereby elucidating the connection between CUR decompo-
sitions and the SPCA class of sparse optimization methods.

To do so  we begin by putting forth a combinatorial optimization problem (see (6) below) which
CUR is implicitly approximately optimizing. This formulation will highlight two interesting features
of CUR: ﬁrst  CUR attains a distinctive pattern of sparsity  which has practical implications from
the SPCA viewpoint; and second  CUR is implicitly optimizing a regression-type objective. These
two observations then lead to the three main contributions of this paper: (a) ﬁrst  we formulate a
non-randomized optimization-based version of CUR (see Problem 1: GL-REG in Section 3) that is
based on a convex relaxation of the CUR combinatorial optimization problem; (b) second  we show
that  in contrast to the original PCA-based motivation for CUR  CUR’s implicit objective cannot
be directly expressed in terms of a PCA-type objective (see Theorem 3 in Section 4); and (c) third 
we propose an SPCA approach (see Problem 2: GL-SPCA in Section 5) that achieves the sparsity
structure of CUR within the PCA framework. We also provide a brief empirical evaluation of our
two proposed objectives. While our proposed GL-REG and GL-SPCA methods are promising in
and of themselves  our purpose in this paper is not to explore them as alternatives to CUR; instead 
our goal is to use them to help clarify the connection between CUR and SPCA methods.

∗Jacob Bien and Ya Xu contributed equally.

1

We conclude this introduction with some remarks on notation. Given a matrix A  we use A(i) to
denote its ith row (as a row-vector) and A(i) its ith column. Similarly  given a set of indices I 
AI and AI denote the submatrices of A containing only these I rows and columns  respectively.
Finally  we let Lcol(A) denote the column space of A.
2 Background

In this section  we provide a brief background on CUR and SPCA methods  with a particular em-
phasis on topics to which we will return in subsequent sections. Before doing so  recall that  given
an input matrix X  Principal Component Analysis (PCA) seeks the k-dimensional hyperplane with
the lowest reconstruction error. That is  it computes a p × k orthogonal matrix W that minimizes

ERR(W) = ||X − XWWT ||F .

(1)
Writing the SVD of X as UΣVT   the minimizer of (1) is given by Vk  the ﬁrst k columns of V. In
the data analysis setting  each column of V provides a particular linear combination of the columns
of X. These linear combinations are often thought of as latent factors. In many applications  in-
terpreting such factors is made much easier if they are comprised of only a small number of actual
columns of X  which is equivalent to Vk only having a small number of nonzero elements.

2.1 CUR matrix decompositions

CUR decompositions were proposed by Drineas and Mahoney [12  4] to provide a low-rank approx-
imation to a data matrix X by using only a small number of actual columns and/or rows of X. Fast
randomized variants [3]  deterministic variants [5]  Nystr¨om-based variants [1  11]  and heuristic
variants [17] have also been considered. Observing that the best rank-k approximation to the SVD
provides the best set of k linear combinations of all the columns  one can ask for the best set of k
actual columns. Most formalizations of “best” lead to intractable combinatorial optimization prob-
lems [12]  but one can take advantage of oversampling (choosing slightly more than k columns) and
randomness as computational resources to obtain strong quality-of-approximation guarantees.
Theorem 1 (Relative-error CUR [12]). Given an arbitrary matrix X ∈ Rn×p and an integer k 
there exists a randomized algorithm that chooses a random subset I ⊂ {1  . . .   p} of size c =
O(k log k log(1/δ)/ǫ2) such that XI  the n × c submatrix containing those c columns of X  satisﬁes
(2)

||X − XI B||F ≤ (1 + ǫ)||X − Xk||F  

||X − XI XI+X||F = min
B∈Rc×p

with probability at least 1 − δ  where Xk is the best rank k approximation to X.

The algorithm referred to by Theorem 1 is very simple:

1) Compute the normalized statistical leverage scores  deﬁned below in (3).
2) Form I by randomly sampling c columns of X  using these normalized statistical leverage scores

as an importance sampling distribution.

3) Return the n × c matrix XI consisting of these selected columns.
The key issue here is the choice of the importance sampling distribution. Let the p × k matrix Vk
be the top-k right singular vectors of X. Then the normalized statistical leverage scores are

πi =

||Vk(i)||2
2 

(3)

1
k

for all i = 1  . . .   p  where Vk(i) denotes the i-th row of Vk. These scores  proportional to the
Euclidean norms of the rows of the top-k right singular vectors  deﬁne the relevant nonuniformity
structure to be used to identify good (in the sense of Theorem 1) columns. In addition  these scores
are proportional to the diagonal elements of the projection matrix onto the top-k right singular
subspace. Thus  they generalize the so-called hat matrix [8]  and they have a natural interpretation
as capturing the “statistical leverage” or “inﬂuence” of a given column on the best low-rank ﬁt of
the data matrix [8  12].

2.2 Regularized sparse PCA methods

SPCA methods attempt to make PCA easier to interpret for domain experts by ﬁnding sparse approx-
imations to the columns of V.1 There are several variants of SPCA. For example  Jolliffe et al. [10]
1For SPCA  we only consider sparsity in the right singular vectors V and not in the left singular vectors U.

This is similar to considering only the choice of columns and not of both columns and rows in CUR.

2

and Witten et al. [19] use the maximum variance interpretation of PCA and provide an optimization
problem which explicitly encourages sparsity in V based on a Lasso constraint [18]. d’Aspremont
et al. [2] take a similar approach  but instead formulate the problem as an SDP.

Zou et al. [21] use the minimum reconstruction error interpretation of PCA to suggest a different
approach to the SPCA problem; this formulation will be most relevant to our present purpose. They
begin by formulating PCA as the solution to a regression-type problem.
Theorem 2 (Zou et al. [21]). Given an arbitrary matrix X ∈ Rn×p and an integer k  let A and W
be p × k matrices. Then  for any λ > 0  let

(A∗  V∗

k) = argminA W∈Rp×k ||X − XWAT ||2

F + λ||W||2
F

s.t. AT A = Ik.

(4)

Then  the minimizing matrices A∗ and V∗
si = 1 or −1.

k satisfy A∗(i) = siV(i) and V∗(i)

k = si

Σ2
ii
ii+λ

Σ2

V(i)  where

That is  up to signs  A∗ consists of the top-k right singular vectors of X  and V∗
k consists of
those same vectors “shrunk” by a factor depending on the corresponding singular value. Given this
regression-type characterization of PCA  Zou et al. [21] then “sparsify” the formulation by adding
an L1 penalty on W:
(A∗  V∗
s.t. AT A = Ik  (5)
where ||W||1 = Pij |Wij|. This regularization tends to sparsify W element-wise  so that the
solution V∗

k) = argminA W∈Rp×k ||X − XWAT ||2

k gives a sparse approximation of Vk.

F + λ||W||2

F + λ1||W||1

3 Expressing CUR as an optimization problem

In this section  we present an optimization formulation of CUR. Recall  from Section 2.1  that CUR
takes a purely algorithmic approach to the problem of approximating a matrix in terms of a small
number of its columns. That is  it achieves sparsity indirectly by randomly selecting c columns  and
it does so in such a way that the reconstruction error is small with high probability (Theorem 1). By
contrast  SPCA methods are generally formulated as the exact solution to an optimization problem.
From Theorem 1  it is clear that CUR seeks a subset I of size c for which minB∈Rc×p ||X− XI B||F
is small. In this sense  CUR can be viewed as a randomized algorithm for approximately solving the
following combinatorial optimization problem:

min

I⊂{1 ... p}

min

B∈Rc×p

||X − XI B||F

s.t. |I| ≤ c.

(6)

In words  this objective asks for the subset of c columns of X which best describes the entire matrix
X. Notice that relaxing |I| = c to |I| ≤ c does not affect the optimum. This optimization problem
is analogous to all-subsets multivariate regression [7]  which is known to be NP-hard.

However  by using ideas from the optimization literature we can approximate this combinatorial
problem as a regularized regression problem that is convex. First  notice that (6) is equivalent to

min

B∈Rp×p

||X − XB||F

s.t.

pX

i=1

1{||B(i)||26=0} ≤ c 

(7)

where we now optimize over a p × p matrix B. To see the equivalence between (6) and (7)  note that
the constraint in (7) is the same as ﬁnding some subset I with |I| ≤ c such that BI c = 0.
The formulation in (7) provides a natural entry point to proposing a convex optimization approach
corresponding to CUR. First notice that (7) uses an L0 norm on the rows of B  which is not convex.
However  we can approximate the L0 constraint by a group lasso penalty  which uses a well-known
convex heuristic proposed by Yuan et al. [20] that encourages prespeciﬁed groups of parameters
to be simultaneously sparse. Thus  the combinatorial problem in (6) can be approximated by the
following convex (and thus tractable) problem:
Problem 1 (Group lasso regression: GL-REG). Given an arbitrary matrix X ∈ Rn×p  let B ∈
Rp×p and t > 0. The GL-REG problem is to solve

B∗ = argminB||X − XB||F

s.t.

pX

i=1

||B(i)||2 ≤ t 

(8)

where t is chosen to get c nonzero rows in B∗.

3

Since the rows of B are grouped together in the penalty Pp
i=1 ||B(i)||2  the row vector B(i) will tend
to be either dense or entirely zero. Note also that the algorithm to solve Problem 1 is a special case
of Algorithm 1 (see below)  which solves the GL-SPCA problem  to be introduced later. (Finally 
as a side remark  note that our proposed GL-REG is strikingly similar to a recently proposed method
for sparse inverse covariance estimation [6  15].)

4 Distinguishing CUR from SPCA

Our original intention in casting CUR in the optimization framework was to understand better
whether CUR could be seen as an SPCA-type method. So far  we have established CUR’s con-
nection to regression by showing that CUR can be thought of as an approximation algorithm for the
sparse regression problem (7). In this section  we discuss the relationship between regression and
PCA  and we show that CUR cannot be directly cast as an SPCA method.

To do this  recall that regression  in particular “self” regression  ﬁnds a B ∈ Rp×p that minimizes

On the other hand  PCA-type methods ﬁnd a set of directions W that minimize

ERR(W) := ||X − XWW+||F .

||X − XB||F .

(9)

(10)

Here  unlike in (1)  we do not assume that W is orthogonal  since the minimizer produced from
SPCA methods is often not required to be orthogonal (recall Section 2.2).

Clearly  with no constraints on B or W  we can trivially achieve zero reconstruction error in both
cases by taking B = Ip and W any p × p full-rank matrix. However  with additional constraints 
these two problems can be very different. It is common to consider sparsity and/or rank constraints.
We have seen in Section 3 that CUR effectively requires B to be row-sparse; in the standard PCA
setting  W is taken to be rank k (with k < p)  in which case (10) is minimized by Vk and obtains
the optimal value ERR(Vk) = ||X − Xk||F ; ﬁnally  for SPCA  W is further required to be sparse.
To illustrate the difference between the reconstruction errors (9) and (10) when extra constraints
are imposed  consider the 2-dimensional toy example in Figure 1. In this example  we compare
regression with a row-sparsity constraint to PCA with both rank and sparsity constraints. With
X ∈ Rn×2  we plot X(2) against X(1) as the solid points in both plots of Figure 1. Constraining
B(2) = 0 (giving row-sparsity  as with CUR methods)  (9) becomes minB12 ||X(2) − X(1)B12||2 
which is a simple linear regression  represented by the black thick line and minimizing the sum
of squared vertical errors as shown. The red line (left plot) shows the ﬁrst principal component
direction  which minimizes ERR(W) among all rank-one matrices W. Here  ERR(W) is the sum
of squared projection distances (red dotted lines). Finally  if W is further required to be sparse in
the X(2) direction (as with SPCA methods)  we get the rank-one  sparse projection represented by
the green line in Figure 1 (right). The two sets of dotted lines in each plot clearly differ  indicating
that their corresponding reconstruction errors are different as well. Since we have shown that CUR
is minimizing a regression-based objective  this toy example suggests that CUR may not in fact be
optimizing a PCA-type objective such as (10). Next  we will make this intuition more precise.
The ﬁrst step to showing that CUR is an SPCA method would be to produce a matrix VCUR for
which XI XI+X = XVCUR V+
CUR  i.e. to express CUR’s approximation in the form of an SPCA
approximation. However  this equality implies Lcol(XVCUR V+
CUR) ⊆ Lcol(XI )  meaning that
(VCUR)I c = 0. If such a VCUR existed  then clearly ERR(VCUR) = ||X − XI XI+X||F   and so
CUR could be regarded as implicitly performing sparse PCA in the sense that (a) VCUR is sparse;
and (b) by Theorem 1 (with high probability)  ERR(VCUR) ≤ (1 + ǫ)ERR(Vk). Thus  the existence
of such a VCUR would cast CUR directly as a randomized approximation algorithm for SPCA. How-
ever  the following theorem states that unless an unrealistic constraint on X holds  there does not
exist a matrix VCUR for which ERR(VCUR) = ||X − XI XI+X||F . The larger implication of this
theorem is that CUR cannot be directly viewed as an SPCA-type method.
Theorem 3. Let I ⊂ {1  . . .   p} be an index set and suppose W ∈ Rp×p satisﬁes WI c = 0. Then 

||X − XWW+||F > ||X − XI XI+X||F  

unless Lcol(XI ) ⊥ Lcol(XI c

)  in which case “≥” holds.

4

Regression
error (9)
PCA
error (10)

)
2
(
X

Regression
error (9)
SPCA
error (10)

)
2
(
X

X(1)

X(1)

Figure 1: Example of the difference in reconstruction errors (9) and (10)  when additional constraints
imposed. Left: regression with row-sparsity constraint (black) compared with PCA with low rank
constraint (red). Right: regression with row-sparsity constraint (black) compared with PCA with
low rank and sparsity constraint (green). In both plots  the corresponding errors are represented by
the dotted lines.

Proof.

||X − XWW+||2

F = ||X − XIWIW+||2
F = ||X − XIWI(WT
WI)−1WT ||2
I
F
F ≥ ||XI c
F + ||XI c
= ||XI − XIWIW+
||2
I ||2
||2
F
= ||XI c
− XI XI+XI c
F + ||XI XI+XI c
||2
||2
F
= ||X − XI XI+X||2
The last inequality is strict unless XI XI+XI c

F + ||XI XI+XI c
= 0.

F ≥ ||X − XI XI+X||2
||2
F .

5 CUR-type sparsity and the group lasso SPCA
Although CUR cannot be directly cast as an SPCA-type method  in this section we propose a sparse
PCA approach (which we call the group lasso SPCA or GL-SPCA) that accomplishes something
very close to CUR. Our proposal produces a V∗ that has rows that are entirely zero  and it is mo-
tivated by the following two observations about CUR. First  following from the deﬁnition of the
leverage scores (3)  CUR chooses columns of X based on the norm of their corresponding rows of
Vk. Thus  it essentially “zeros-out” the rows of Vk with small norms (in a probabilistic sense).
Second  as we have noted in Section 4  if CUR could be expressed as a PCA method  its principal
directions matrix “VCUR” would have p − c rows that are entirely zero  corresponding to removing
those columns of X.
Recall that Zou et al. [21] obtain a sparse V∗ by including in (5) an additional L1 penalty from
the optimization problem (4). Since the L1 penalty is on the entire matrix viewed as a vector 
it encourages only unstructured sparsity. To achieve the CUR-type row sparsity  we propose the
following modiﬁcation of (4):
Problem 2 (Group lasso SPCA: GL-SPCA). Given an arbitrary matrix X ∈ Rn×p and an integer
k  let A and W be p × k matrices  and let λ  λ1 > 0. The GL-SPCA problem is to solve

(A∗  V∗) = argminA W||X − XWAT ||2

F + λ||W||2

F + λ1

pX

i=1

||W(i)||2 s.t. AT A = Ik. (11)

the lasso penalty λ1||W||1 in (5)

is replaced in (11) by a group lasso penalty
i=1 ||W(i)||2  where rows of W are grouped together so that each row of V∗ will tend to

Thus 
λ1 Pp
be either dense or entirely zero.

Importantly  the GL-SPCA problem is not convex in W and A together; it is  however  convex in
W  and it is easy to solve in A. Thus  analogous to the treatment in Zou et al. [21]  we propose
an iterative alternate-minimization algorithm to solve GL-SPCA. This is described in Algorithm 1;
and the justiﬁcation of this algorithm is given in Section 7. Note that if we ﬁx A to be I throughout 
then Algorithm 1 can be used to solve the GL-REG problem discussed in Section 3.

5

Algorithm 1: Iterative algorithm for solving the GL-SPCA (and GL-REG) problems.

(For the GL-REG problem  ﬁx A = I throughout this algorithm.)

Input: Data matrix X and initial estimates for A and W
Output: Final estimates for A and W
repeat

Compute SVD of XT XW as UDVT and then A ← UVT ;
S ← {i : ||W(i)||2 6= 0};
for i ∈ S do

Compute bi = Pj6=i (cid:0)X(j)T X(i)(cid:1) WT
(j);
if ||AT XT X(i) − bi||2 ≤ λ1/2 then

WT

(i) ← 0;

else

WT

(i) ←

until convergence;

2+λ+λ1/||W(i)||2 (cid:0)AT XT X(i) − bi(cid:1);

2

2||X(i)||2

1

2

3

4

We remark that such row-sparsity in V∗ can have either advantages or disadvantages. Consider  for
example  when there are a small number of informative columns in X and the rest are not important
for the task at hand [12  14]. In such a case  we would expect that enforcing entire rows to be zero
would lead to better identiﬁcation of the signal columns; and this has been empirically observed in
the application of CUR to DNA SNP analysis [14]. The unstructured V∗  by contrast  would not
be able to “borrow strength” across all columns of V∗ to differentiate the signal columns from the
noise columns. On the other hand  requiring such structured sparsity is more restrictive and may
not be desirable. For example  in microarray analysis in which we have measured p genes on n
patients  our goal may be to ﬁnd several underlying factors. Biologists have identiﬁed “pathways”
of interconnected genes [16]  and it would be desirable if each sparse factor could be identiﬁed with
a different pathway (that is  a different set of genes). Requiring all factors of V∗ to exclude the same
p − c genes does not allow a different sparse subset of genes to be active in each factor.
We ﬁnish this section by pointing out that while most SPCA methods only enforce unstructured
zeros in V∗  the idea of having a structured sparsity in the PCA context has very recently been
explored [9]. Our GL-SPCA problem falls within the broad framework of this idea.

6 Empirical Comparisons
In this section  we evaluate the performance of the four methods discussed above on both syn-
thetic and real data. In particular  we compare the randomized CUR algorithm of Mahoney and
Drineas [12  4] to our GL-REG (of Problem 1)  and we compare the SPCA algorithm proposed
by Zou et al. [21] to our GL-SPCA (of Problem 2). We have also compared against the SPCA
algorithm of Witten et al. [19]  and we found the results to be very similar to those of Zou et al.
6.1 Simulations
We ﬁrst consider synthetic examples of the form X = bX + E  where bX is the underlying signal
matrix and E is a matrix of noise. In all our simulations  E has i.i.d. N (0  1) entries  while the
signal bX has one of the following forms:
Case I) bX = [0n×(p−c); bX∗] where the n × c matrix bX∗ is the nonzero part of bX. In other words 

bX has c nonzero columns and does not necessarily have a low-rank structure.
being low-rank  V has entire rows equal to zero (i.e. it is row-sparse).

Case II) bX = UVT where U and V each consist of k < p orthogonal columns. In addition to
Case III) bX = UVT where U and V each consist of k < p orthogonal columns. Here V is

low-rank and sparse  but the sparsity is not structured (i.e. it is scattered-sparse).

A successful method attains low reconstruction error of the true signal bX and has high precision in
identifying correctly the zeros in the underlying model. As previously discussed  the four methods

6

optimize for different types of reconstruction error. Thus  in comparing CUR and GL-REG  we
use the regression-type reconstruction error ERRreg(I) = ||bX − XI XI+X||F   whereas for the
comparison of SPCA and GL-SPCA  we use the PCA-type error ERR(V) = ||bX − XVV+||F .
Table 1 presents the simulation results from the three cases. All comparisons use n = 100 and
p = 1000. In Case II and III  the signal matrix has rank k = 10. The underlying sparsity level is
20%  i.e. 80% of the entries of bX (Case I) and V (Case II&III) are zeros. Note that all methods
except for GL-REG require the rank k as an input  and we always take it to be 10 even in Case I. For
easy comparison  we have tuned each method to have the correct total number of zeros. The results
are averaged over 5 trials.

ERRreg(I)

ERR(V)

Case I
Methods
316.29 (0.835)
CUR
316.29 (0.989)
GL-REG
SPCA
177.92 (0.809)
GL-SPCA 141.85 (0.998)

Case II
315.28 (0.797)
315.28 (0.750)
44.388 (0.799)
37.310 (0.767)

Case III
315.64 (0.166)
315.64 (0.107)
44.995 (0.792)
45.500 (0.804)

Table 1: Simulation results: The reconstruction errors and the percentages of correctly identiﬁed
zeros (in parentheses).

We notice in Table 1 that the two regression-type methods CUR and GL-REG have very similar
performance. As we would expect  since CUR only uses information in the top k singular vectors  it
does slightly worse than GL-REG in terms of precision when the underlying signal is not low-rank
(Case I). In addition  both methods perform poorly if the sparsity is not structured as in Case III. The
two PCA-type methods perform similarly as well. Again  the group lasso method seems to work
better in Case I. We note that the precisions reported here are based on element-wise sparsity—if we
were measuring row-sparsity  methods like SPCA would perform poorly since they do not encourage
entire rows to be zero.
6.2 Microarray example
We next consider a microarray dataset of soft tissue tumors studied by Nielsen et al. [13]. Ma-
honey and Drineas [12] apply CUR to this dataset of n = 31 tissue samples and p = 5520 genes.
As with the simulation results  we use two sets of comparisons: we compare CUR with GL-REG 
and we compare SPCA with GL-SPCA. Since we do not observe the underlying truth bX  we take
ERRreg(I) = ||X − XI XI+X||F and ERR(V) = ||X − XVV+||F . Also  since we do not observe
the true sparsity  we cannot measure the precision as we do in Table 1. The left plot in Figure 2
shows ERRreg(I) as a function of |I|. We see that CUR and GL-REG perform similarly. (However 
since CUR is a randomized algorithm  on every run it gives a different result. From a practical
standpoint  this feature of CUR can be disconcerting to biologists wanting to report a single set of
important genes. In this light  GL-REG may be thought of as an attractive non-randomized alterna-
tive to CUR.) The right plot of Figure 2 compares GL-SPCA to SPCA (speciﬁcally  Zou et al. [21]).
Since SPCA does not explicitly enforce row-sparsity  for a gene to be not used in the model requires
all of the (k = 4) columns of V∗ to exclude it. This likely explains the advantage of GL-SPCA over
SPCA seen in the ﬁgure.
7 Justiﬁcation of Algorithm 1
The algorithm alternates between minimizing with respect to A and B until convergence.
Solving for A given B:
If B is ﬁxed  then the regularization penalty in (11) can be ignored  in
which case the optimization problem becomes minA ||X − XBAT ||2
F subject to AT A = I. This
problem was considered by Zou et al. [21]  who showed that the solution is obtained by computing
the SVD of (XT X)B as (XT X)B = UDVT and then setting bA = UVT . This explains step 1 in
Algorithm 1.
Solving for B given A: If A is ﬁxed  then (11) becomes an unconstrained convex optimization
problem in B. The subgradient equations (using that AT A = Ik) are

2BT XT X(i) − 2AT XT X(i) + 2λBT

(i) + λ1si = 0;

i = 1  . . .   p 

(12)

7

Microarray Dataset

Microarray Dataset

)
I
(
g
e
r
R
R
E

0
0
4

0
0
3

0
0
2

0
0
1

0

GL-REG
CUR

)

V

(
R
R
E

0
6
4

0
4
4

0
2
4

0
0
4

0
8
3

0
6
3

GL-SPCA
SPCA

0

100

50
150
Number of genes used

200

1000

2000

3000

4000

5000

Number of genes used

Figure 2: Left: Comparison of CUR  multiple runs  with GL-REG; Right: Comparison of GL-
SPCA with SPCA (speciﬁcally  Zou et al. [21]).

where the subgradient vectors si = BT
deﬁne bi = Pj6=i(X(j)T X(i))BT
can be written as

(i)/||B(i)||2 if B(i) 6= 0  or ||si||2 ≤ 1 if B(i) = 0. Let us
(i)  so that the subgradient equations

(j) = BT XT X(i)−||X(i)||2
2

BT

bi + (||X(i)||2

2 + λ)BT

(i) − AT XT X(i) + (λ1/2)si = 0.

(13)

The following claim explains Step 3 in Algorithm 1.
Claim 1. B(i) = 0 if and only if ||AT XT X(i) − bi||2 ≤ λ1/2.

Proof. First  if B(i) = 0  the subgradient equations (13) become bi − AT XT X(i) + (λ1/2)si = 0.
Since ||si||2 ≤ 1 if B(i) = 0  we have ||AT XT X(i) − bi||2 ≤ λ1/2. To prove the other
(i)/||B(i)||2. Substituting this expression into
direction  recall that B(i)
(13)  rearranging terms  and taking the norm on both sides  we get 2||AT XT X(i) − bi||2 =
(cid:0)2||X(i)||2

2 + 2λ + λ1/||B(i)||2(cid:1) ||B(i)||2 > λ1.

6= 0 implies si = BT

By Claim 1  ||AT XT X(i) − bi||2 > λ1/2 implies that B(i) 6= 0 which further implies si =
BT

(i)/||B(i)||2. Substituting into (13) gives Step 4 in Algorithm 1.

8 Conclusion
In this paper  we have elucidated several connections between two recently-popular matrix decom-
position methods that adopt very different perspectives on obtaining interpretable low-rank matrix
decompositions. In doing so  we have suggested two optimization problems  GL-REG and GL-
SPCA  that highlight similarities and differences between the two methods.
In general  SPCA
methods obtain interpretability by modifying an existing intractable objective with a convex regu-
larization term that encourages sparsity  and then exactly optimizing that modiﬁed objective. On
the other hand  CUR methods operate by using randomness and approximation as computational re-
sources to optimize approximately an intractable objective  thereby implicitly incorporating a form
of regularization into the steps of the approximation algorithm. Understanding this concept of im-
plicit regularization via approximate computation is clearly of interest more generally  in particular
for applications where the size scale of the data is expected to increase.

Acknowledgments

We would like to thank Art Owen and Robert Tibshirani for encouragement and helpful suggestions.
Jacob Bien was supported by the Urbanek Family Stanford Graduate Fellowship  and Ya Xu was
supported by the Melvin and Joan Lane Stanford Graduate Fellowship. In addition  support from
the NSF and AFOSR is gratefully acknowledged.

8

References
[1] M.-A. Belabbas and P.J. Wolfe. Fast low-rank approximation for covariance matrices.

In
Second IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing  pages 293–296  2007.

[2] A. d’Aspremont  L. El Ghaoui  M. I. Jordan  and G. R. G. Lanckriet. A direct formulation for

sparse PCA using semideﬁnite programming. SIAM Review  49(3):434–448  2007.

[3] P. Drineas  R. Kannan  and M.W. Mahoney. Fast Monte Carlo algorithms for matrices III:
Computing a compressed approximate matrix decomposition. SIAM Journal on Computing 
36:184–206  2006.

[4] P. Drineas  M.W. Mahoney  and S. Muthukrishnan. Relative-error CUR matrix decompositions.

SIAM Journal on Matrix Analysis and Applications  30:844–881  2008.

[5] S.A. Goreinov and E.E. Tyrtyshnikov. The maximum-volume concept in approximation by

low-rank matrices. Contemporary Mathematics  280:47–51  2001.

[6] T. Hastie  R. Tibshirani  and J. Friedman. Applications of the lasso and grouped lasso to the

estimation of sparse graphical models. Manuscript. Submitted. 2010.

[7] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer-

Verlag  New York  2003.

[8] D.C. Hoaglin and R.E. Welsch. The hat matrix in regression and ANOVA. The American

Statistician  32(1):17–22  1978.

[9] R. Jenatton  G. Obozinski  and F. Bach. Structured sparse principal component analysis. Tech-

nical report. Preprint: arXiv:0909.1440 (2009).

[10] I. T. Jolliffe  N. T. Trendaﬁlov  and M. Uddin. A modiﬁed principal component technique based

on the LASSO. Journal of Computational and Graphical Statistics  12(3):531–547  2003.

[11] S. Kumar  M. Mohri  and A. Talwalkar. Ensemble Nystr¨om method. In Annual Advances in

Neural Information Processing Systems 22: Proceedings of the 2009 Conference  2009.

[12] M.W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proc.

Natl. Acad. Sci. USA  106:697–702  2009.

[13] T. Nielsen  R.B. West  S.C. Linn  O. Alter  M.A. Knowling  J. O’Connell  S. Zhu  M. Fero 
G. Sherlock  J.R. Pollack  P.O. Brown  D. Botstein  and M. van de Rijn. Molecular characteri-
sation of soft tissue tumours: a gene expression study. Lancet  359(9314):1301–1307  2002.

[14] P. Paschou  E. Ziv  E.G. Burchard  S. Choudhry  W. Rodriguez-Cintron  M.W. Mahoney  and
P. Drineas. PCA-correlated SNPs for structure identiﬁcation in worldwide human populations.
PLoS Genetics  3:1672–1686  2007.

[15] J. Peng  P. Wang  N. Zhou  and J. Zhu. Partial correlation estimation by joint sparse regression

models. Journal of the American Statistical Association  104:735–746  2009.

[16] A. Subramanian  P. Tamayo  V. K. Mootha  S. Mukherjee  B. L. Ebert  M. A. Gillette 
A. Paulovich  S. L. Pomeroy  T. R. Golub  E. S. Lander  and J. P. Mesirov. Gene set enrich-
ment analysis: A knowledge-based approach for interpreting genome-wide expression proﬁles.
Proc. Natl. Acad. Sci. USA  102(43):15545–15550  2005.

[17] J. Sun  Y. Xie  H. Zhang  and C. Faloutsos. Less is more: Compact matrix decomposition for
large sparse graphs. In Proceedings of the 7th SIAM International Conference on Data Mining 
2007.

[18] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B  58(1):267–288  1996.

[19] D. M. Witten  R. Tibshirani  and T. Hastie. A penalized matrix decomposition  with ap-
plications to sparse principal components and canonical correlation analysis. Biostatistics 
10(3):515–534  2009.

[20] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society: Series B  68(1):49–67  2006.

[21] H. Zou  T. Hastie  and R. Tibshirani. Sparse principal component analysis. Journal of Compu-

tational and Graphical Statistics  15(2):262–286  2006.

9

,Alfredo Kalaitzis
Ricardo Silva
Scott Yang
Mehryar Mohri
Matan Atzmon
Niv Haim
Lior Yariv
Ofer Israelov
Haggai Maron
Yaron Lipman