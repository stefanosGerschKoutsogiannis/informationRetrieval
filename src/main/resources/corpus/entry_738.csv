2009,Bayesian Nonparametric Models on Decomposable Graphs,Over recent years Dirichlet processes and the associated Chinese restaurant process (CRP) have found many applications in clustering while the Indian buffet process (IBP) is increasingly used to describe latent feature models. In the clustering case  we associate to each data point a latent allocation variable. These latent variables can share the same value and this induces a partition of the data set. The CRP is a prior distribution on such partitions.  In latent feature models  we associate to each data point a potentially infinite number of binary latent variables indicating the possession of some features and the IBP is a prior distribution on the associated infinite binary matrix. These prior distributions are attractive because they ensure exchangeability (over samples). We propose here extensions of these models to decomposable graphs. These models have appealing properties and can be easily learned using Monte Carlo techniques.,Bayesian Nonparametric Models on Decomposable

Graphs

Franc¸ois Caron

INRIA Bordeaux Sud–Ouest

Institut de Math´ematiques de Bordeaux

University of Bordeaux  France
francois.caron@inria.fr

Arnaud Doucet

Departments of Computer Science & Statistics

University of British Columbia  Vancouver  Canada

and The Institute of Statistical Mathematics

Tokyo  Japan

arnaud@cs.ubc.ca

Abstract

Over recent years Dirichlet processes and the associated Chinese restaurant pro-
cess (CRP) have found many applications in clustering while the Indian buffet
process (IBP) is increasingly used to describe latent feature models. These mod-
els are attractive because they ensure exchangeability (over samples). We propose
here extensions of these models where the dependency between samples is given
by a known decomposable graph. These models have appealing properties and
can be easily learned using Monte Carlo techniques.

1 Motivation

The CRP and IBP have found numerous applications in machine learning over recent years [5 
10]. We consider here the case where the data we are interested in are ‘locally’ dependent; these
dependencies being represented by a known graph G where each data point/object is associated
to a vertex. These local dependencies can correspond to any conceptual or real (e.g. space  time)
metric. For example  in the context of clustering  we might want to propose a prior distribution on
partitions enforcing that data which are ‘close’ in the graph are more likely to be in the same cluster.
Similarly  in the context of latent feature models  we might be interested in a prior distribution on
features enforcing that data which are ‘close’ in the graph are more likely to possess similar features.
The ‘standard’ CRP and IBP correspond to the case where the graph G is complete; that is it is fully
connected. In this paper  we generalize the CRP and IBP to decomposable graphs. The resulting
generalized versions of the CRP and IBP enjoy attractive properties. Each clique of the graph follows
marginally a CRP or an IBP process and explicit expressions for the joint prior distribution on the
graph is available. It makes it easy to learn those models using straightforward generalizations of
Markov chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) algorithms proposed to
perform inference for the CRP and IBP [5  10  14].
The rest of the paper is organized as follows. In Section 2  we review the popular Dirichlet multi-
nomial allocation model and the Dirichlet Process (DP) partition distribution. We propose an exten-
sion of these two models to decomposable graphical models. In Section 3 we discuss nonparametric
latent feature models  reviewing brieﬂy the construction in [5] and extending it to decomposable
graphs. We demonstrate these models in Section 4 on two applications: an alternative to the hierar-
chical DP model [12] and a time-varying matrix factorization problem.

2 Prior distributions for partitions on decomposable graphs

Assume we have n observations. When performing clustering  we associate to each of this observa-
tion an allocation variable zi ∈ [K] = {1  . . .   K}. Let Πn be the partition of [n] = {1  . . .   n} de-
ﬁned by the equivalence relation i ↔ j ⇔ zi = zj. The resulting partition Πn = {A1  . . .   An(Πn)}

1

is an unordered collection of disjoint non-empty subsets Aj of [n]  j = 1  . . .   n(Πn)  where
∪jAj = [n] and n(Πn) is the number of subsets for partition Πn. We also denote by Pn be the
set of all partitions of [n] and let nj  j = 1  . . .   n(Πn)  be the size of the subset Aj.
Each allocation variable zi is associated to a vertex/site of an undirected graph G  which is assumed
to be known. In the standard case where the graph G is complete  we ﬁrst review brieﬂy here two
popular prior distributions on z1:n  equivalently on Πn. We then extend these models to undirected
decomposable graphs; see [2  8] for an introduction to decomposable graphs. Finally we brieﬂy
discuss the directed case. Note that the models proposed here are completely different from the
hyper multinomial-Dirichlet in [2] and its recent DP extension [6].

2.1 Dirichlet multinomial allocation model and DP partition distribution

Assume for the time being that K is ﬁnite. When the graph is complete  a popular choice for the
allocation variables is to consider a Dirichlet multinomial allocation model [11]

(1)
where D is the standard Dirichlet distribution and θ > 0. Integrating out π  we obtain the following
Dirichlet multinomial prior distribution

  . . .  

π ∼ D( θ
K

)  zi|π ∼ π

θ
K

(cid:81)K

and then  using the straightforward equality Pr(Πn) =
PK where PK = {Πn ∈ Pn|n(Πn) ≤ K}  we obtain
Γ(θ)

K!

Pr(Πn) =

(K − n(Πn))!

Pr(z1:n) =

Γ(θ)

K )
j=1 Γ(nj + θ
K )K

Γ(θ + n)Γ( θ

(2)
(K−n(Πn))! Pr(z1:n) valid for for all Πn ∈
(cid:81)n(Πn)

K!

.

(3)

j=1 Γ(nj + θ
K )
K )n(Πn)

Γ(θ + n)Γ( θ

θn(Πn)(cid:81)n(Πn)
(cid:81)n
j=1 Γ(nj)
i=1(θ + i − 1)

DP may be seen as a generalization of the Dirichlet multinomial model when the number of com-
ponents K → ∞; see for example [10]. In this case the distribution over the partition Πn of [n] is
given by [11]

Pr(Πn) =

(4)
Let Π−k = {A1 −k  . . .   An(Π−k) −k} be the partition induced by removing item k to Πn and nj −k
be the size of cluster j for j = 1  . . .   n(Π−k).
It follows from (4) that an item k is assigned
to an existing cluster j  j = 1  . . .   n(Π−k)  with probability proportional to nj −k/ (n − 1 + θ)
and forms a new cluster with probability θ/ (n − 1 + θ). This property is the basis of the CRP.
We now extend the Dirichlet multinomial allocation and the DP partition distribution models to
decomposable graphs.

.

2.2 Markov combination of Dirichlet multinomial and DP partition distributions
Let G be a decomposable undirected graph  C = {C1  . . .   Cp} a perfect ordering of the cliques
and S = {S2  . . .   Cp} the associated separators.
It can be easily checked that if the marginal
distribution of zC for each clique C ∈ C is deﬁned by (2) then these distributions are consistent as
they yield the same distribution (2) over the separators. Therefore  the unique Markov distribution
over G with Dirichlet multinomial distribution over the cliques is deﬁned by [8]

(5)
where for each complete set B ⊆ G  we have Pr(zB) given by (2). It follows that we have for any
Πn ∈ PK

Pr(z1:n) =

Pr(Πn) =

K!

(K − n(Πn))!

(6)

(cid:81)
(cid:81)
(cid:81)
(cid:81)

2

C∈C Pr(zC)
S∈S Pr(zS)
(cid:81)K
(cid:81)K

C∈C

Γ(θ)

Γ(θ)

S∈S

j=1 Γ(nj C + θ

K )

Γ(θ+nC )Γ( θ

K )K

j=1 Γ(nj S + θ

K )

Γ(θ+nS )Γ( θ

K )K

where for each complete set B ⊆ G  nj B is the number of items associated to cluster j  j =
1  . . .   K in B and nB is the total number of items in B. Within each complete set B  the allocation
variables deﬁne a partition distributed according to the Dirichlet-multinomial distribution.
We now extend this approach to DP partition distributions; that is we derive a joint distribution over
Πn such that the distribution of ΠB over each complete set B of the graph is given by (4) with
θ > 0. Such a distribution satisﬁes the consistency condition over the separators as the restriction of
any partition distributed according to (4) still follows (4) [7].
Proposition. Let PG
n be the set of partitions Πn ∈ Pn such that for each decomposition A  B  and
any (i  j) ∈ A × B  i ↔ j ⇒ ∃k ∈ A ∩ B such that k ↔ i ↔ j. As K → ∞  the prior distribution
over partitions (6) is given for each Πn ∈ PG

n by

where n(ΠB) is the number of clusters in the complete set B.
Proof. From (6)  we have

C∈C

S∈S

Pr(Πn) = θn(Πn)

Γ(nj C )
i=1(θ+i−1)
j=1 Γ(nj S )
i=1(θ+i−1)

(cid:81)n(ΠC )
(cid:81)nC
(cid:81)n(ΠS )
(cid:81)nS
(cid:81)
θn(ΠC )(cid:81)n(ΠC )
(cid:81)nC
(cid:81)
(cid:80)
C∈C n(ΠC )−(cid:80)
θn(ΠS )(cid:81)n(ΠS )
Pr(Πn) = K(K − 1) . . . (K − n(Πn) + 1)
i=1(θ+i−1)
(cid:81)nS
(cid:80)
C∈C n(ΠC) −(cid:80)
j=1 Γ(nj S + θ
C∈C n(ΠC) −(cid:80)
i=1(θ+i−1)
(cid:80)
C∈C n(ΠC) −(cid:80)
S∈S n(ΠS) and 0 otherwise.
S∈S n(ΠS) for any Πn ∈ Pn and the subset of Pn verifying

S∈S n(ΠS) corresponds to the set PG

Γ(nj C + θ

K )

S∈S n(ΠS )

C∈C

S∈S

n .(cid:165)

(7)

j=1

K )

Thus when K → ∞  we obtain (7) if n(Πn) =

K

We have n(Πn) ≤(cid:80)

n(Πn) =

(cid:81)
(cid:81)

j=1

Example. Let the notation i ∼ j (resp. i (cid:28) j) indicates an edge (resp. no edge) between two sites.
Let n = 3 and G be the decomposable graph deﬁned by the relations 1 ∼ 2  2 ∼ 3 and 1 (cid:28) 3.
3 is then equal to {{{1  2  3}};{{1  2} {3}};{{1} {2  3}};{{1} {2} {3}}}. Note that
The set PG
the partition {{1  3} {2}} does not belong to PG
3 . Indeed  as there is no edge between 1 and 3  they
cannot be in the same cluster if 2 is in another cluster. The cliques are C1 = {1  2} and C2 = {2  3}
and the separator is S2 = {2}. The distribution is given by Pr(Π3) = Pr(ΠC1 ) Pr(ΠC2 )
hence we can
check that we obtain Pr({1  2  3}) = (θ + 1)−2  Pr({1  2} {3}) = Pr({1  2} {3}) = θ(θ + 1)−2
and Pr({1} {2} {3}) = θ2(θ + 1)−2.(cid:165)
Let now deﬁne the full conditional distributions. Based on (7) the conditional assignment of an item
k is proportional to the conditional over the cliques divided by the conditional over the separators.
Let denote G−k the undirected graph obtained by removing vertex k from G. Suppose that Πn ∈ PG
n .
If Π−k /∈ PG−k
n−1  then do not change the value of item k. Otherwise  item k is assigned to cluster j
where j = 1  . . .   n(Π−k) with probability proportional to

Pr(ΠS2 )

(cid:81)
(cid:81)

{C∈C|n−k j C >0} n−k j C
{S∈S|n−k j S >0} n−k j S

(8)

and to a new cluster with probability proportional to θ  where n−k j C is the number of items in the
set C \{k} belonging to cluster j. The updating process is illustrated by the Chinese wedding party
process1 in Fig. 1. The results of this section can be extended to the Pitman-Yor process  and more
generally to species sampling models.
Given Π−2 = {A1 = {1}  A2 = {3}}  we have
Example (continuing).
Pr(item 2 assigned to A1 = {1}| Π−2) = Pr(item 2 assigned to A2 = {3}| Π−2) = (θ + 2)−1
and Pr(item 2 assigned to new cluster A3| Π−2) = θ (θ + 2)−1. Given Π−2 = {A1 = {1  3}} 
item 2 is assigned to A1 with probability 1.(cid:165)

1Note that this representation describes the full conditionals while the CRP represents the sequential updat-

ing.

3

(a)

(b)

(c)

(d)

(e)

Figure 1: Chinese wedding party. Consider a group of n guests attending a wedding party. Each
of the n guests may belong to one or several cliques  i.e. maximal groups of people such that
everybody knows everybody. The belonging of each guest to the different cliques is represented by
color patches on the ﬁgures  and the graphical representation of the relationship between the guests
is represented by the graphical model (e). (a) Suppose that the guests are already seated such that
two guests cannot be together at the same table is they are not part of the same clique  or if there
does not exist a group of other guests such that they are related (“Any friend of yours is a friend of
mine”). (b) The guest number k leaves his table and either (c) joins a table where there are guests
from the same clique as him  with probability proportional to the product of the number of guests
from each clique over the product of the number of guests belonging to several cliques on that table
or (d) he joins a new table with probability proportional to θ.

2.3 Monte Carlo inference

2.3.1 MCMC algorithm

(cid:81)
(cid:81)

Using the full conditionals  a single site Gibbs sampler can easily be designed to approximate the
posterior distribution Pr(Πn|z1:n). Given a partition Πn  an item k is taken out of the partition. If
Π−k /∈ PG−k
n−1  item k keeps the same value. Otherwise  the item will be assigned to a cluster j 
j = 1  . . .   n(Π−k)  with probability proportional to

×

p(z{k}∪Aj −k)

{C∈C|n−k j C >0} n−k j C
{S∈S|n−k j S >0} n−k j S

p(zAj −k)

(9)
and the item will be assigned to a new cluster with probability proportional to p(z{k})× θ. Similarly
to [3]  we can also deﬁne a procedure to sample from p(θ|n(Πn) = k)). We assume that θ ∼ G(a  b)
and use p auxiliary variables x1  . . .   xp. The procedure is as follows.
• For j = 1  . . .   p  sample xj|k  θ ∼ Beta(θ + nSj   nCj − nSj )

• Sample θ|k  x1:p ∼ G(a + k  b −(cid:80)

j log xj)

2.3.2 Sequential Monte Carlo
We have so far only treated the case of an undirected decomposable graph G. We can formu-
late a sequential updating rule for the corresponding perfect directed version D of G. Indeed  let
(a1  . . . a|V |) be a perfect ordering and pa(ak) be the set of parents of ak which is by deﬁnition com-
plete. Let Πk−1 = {A1 k−1  . . .   An(Πk−1) k−1} denote the partition of the ﬁrst k−1 vertices a1:k−1
and let nj pa(ak) be the number of elements with value j in the set pa(ak)  j = 1  . . .   n(Πk−1).
Then the vertex ak joins the set j with probability nj pa(ak)/
and creates a new
cluster with probability θ/

q nq pa(ak)

(cid:80)

(cid:179)

(cid:180)

(cid:180)

(cid:179)

θ +

q nq pa(ak)

.

One can then design a particle ﬁlter/SMC method in a similar fashion as [4]. Consider a set of
N particles Π(i)
k−1 = 1) that approximate
the posterior distribution Pr(Πk−1|z1:k−1). For each particle i  there are n(Π(i)
k−1) + 1 possible

k−1 with weights w(i)

k−1 ∝ Pr(Π(i)

k−1  z1:k−1) (

i=1 w(i)

θ +

(cid:80)
(cid:80)N

4

allocations for component ak. We denote(cid:101)Π(i j)
to cluster j. The weight associated to(cid:101)Π(i j)


p(z{ak}∪Aj k−1)

k−1 = w(i)
k−1

(cid:101)w(i j)
Then we can perform a deterministic resampling step by keeping the N particles(cid:101)Π(i j)
weights (cid:101)w(i j)

if j = 1  . . .   n(Π(i)
if j = n(Π(i)
k−1) + 1

k be the resampled particles and w(i)

k−1 . Let Π(i)

k
is given by

p(zAj k−1)

(cid:80)
(cid:80)

q nq pa(ak)

q nq pa(ak)

nj pa(ak)

×

θ+

θ+

k

θ

k−1)

k the associated normalized weights.

the partition obtained by associating component ak

k with highest

(10)

3 Prior distributions for inﬁnite binary matrices on decomposable graphs
Assume we have n objects; each of these objects being associated to the vertex of a graph G. To
each object is associated a K-dimensional binary vector zn = (zn 1  . . .   zn K) ∈ {0  1}K where
zn i = 1 if object n possesses feature i and zn i = 0 otherwise. These vectors zt form a binary
n × K matrix denoted Z1:n. We denote by ξ1:n the associated equivalence class of left-ordered
matrices and let EK be the set of left-ordered matrices with at most K features.
In the standard case where the graph G is complete  we review brieﬂy here two popular prior distribu-
tions on Z1:n  equivalently on ξ1:n: the Beta-Bernoulli model and the IBP [5]. We then extend these
models to undirected decomposable graphs. This can be used for example to deﬁne a time-varying
IBP as illustrated in Section 4.

3.1 Beta-Bernoulli and IBP distributions

The Beta-Bernoulli distribution over the allocation Z1:n is

K )
Γ(n + 1 + α
where nj is the number of objects having feature j. It follows that

j=1

Pr(Z1:n) =

α

K Γ(nj + α

K )Γ(n − nj + 1)

K(cid:89)

K(cid:89)

K!(cid:81)2n−1

h=0 Kh!

j=1

Pr(ξ1:n) =

α

K Γ(nj + α

K )Γ(n − nj + 1)

K )
Γ(n + 1 + α

(11)

(12)

(13)

where Kh is the number of features possessing the history h (see [5] for details). The nonparametric
model is obtained by taking the limit when K → ∞

Pr(ξ1:n) =

αK+(cid:81)2n−1

h=1 Kh!

K+(cid:89)

(n − nj)!(nj − 1)!

j=1

n!

exp(−αHn)

(cid:80)n

where K + is the total number of features and Hn =

1

k . The IBP follows from (13).

k=1

3.2 Markov combination of Beta-Bernoulli and IBP distributions
Let G be a decomposable undirected graph  C = {C1  . . .   Cp} a perfect ordering of the cliques and
S = {S2  . . .   Cp} the associated separators. As in the Dirichlet-multinomial case  it is easily seen
that if for each clique C ∈ C  the marginal distribution is deﬁned by (11)  then these distributions
are consistent as they yield the same distribution (11) over the separators. Therefore  the unique
Markov distribution over G with Beta-Bernoulli distribution over the cliques is deﬁned by [8]

(14)
where Pr(ZB) given by (11) for each complete set B ⊆ G. The prior over ξ1:n is thus given  for
ξ1:n ∈ EK  by

Pr(Z1:n) =

C∈C Pr(ZC)
S∈S Pr(ZS)

(cid:81)
(cid:81)

C∈C

S∈S

K!(cid:81)2n−1

h=0 Kh!

Pr(ξ1:n) =

α

K Γ(nj C + α

α

K Γ(nj S + α

Γ(nC +1+ α

K )Γ(nC−nj C +1)
K )Γ(nS−nj S +1)

K )

Γ(nS +1+ α

K )

(15)

(cid:81)
(cid:81)
(cid:81)K
(cid:81)K

j=1

j=1

5

where for each complete set B ⊆ G  nj B is the number of items having feature j  j = 1  . . .   K in
the set B and nB is the whole set of objects in set B. Taking the limit when K → ∞  we obtain
after a few calculations
Pr(ξ1:n) = αK+

C HnC −(cid:80)
(cid:80)
(cid:81)2n−1

(nC−nj C )!(nj C−1)!

[n] exp [−α (

(cid:81)K+
(cid:81)K+

(cid:81)
(cid:81)

S HnS )]

(nS−nj S )!(nj S−1)!

C∈C

C
j=1

×

h=1 Kh!

S∈S

S
j=1

nC !

nS !

[n] =

S K +

C K +

S and 0 otherwise  where K +

if K +
possessed by objects in B.
Let EG
n be the subset of En such that for each decomposition A  B and any (u  v) ∈ A × B: {u and
v possess feature j} ⇒ ∃k ∈ A ∩ B such that {k possesses feature j}. Let ξ−k be the left-ordered
matrix obtained by removing object k from ξn and K +−k be the total number of different features in
ξ−k. For each feature j = 1  . . .   K +−k  if ξ−k ∈ EG−k

B is the number of different features

n−1 then we have

(cid:80)

C −(cid:80)

 b

b

(cid:81)
(cid:81)
(cid:81)
C∈C nj C
(cid:81)
S∈C nj S
C∈C(nC−nj C )
S∈C(nS−nj S )

if i = 1
if i = 0

Pr(ξk j = i) =

(cid:179)

(cid:81)
(cid:81)

(16)

(cid:180)

{S∈S|k∈S} nS
where b is the appropriate normalizing constant then the customer k tries Poisson
{C∈C|k∈C} nC
new dishes. We can easily generalize this construction to a directed version D of G using arguments
similar to those presented in Section 2; see Section 4 for an application to time-varying matrix
factorization.

α

4 Applications

4.1 Sharing clusters among relative groups: An alternative to HDP

Consider that we are given d groups with nj data yi j in each group  i = 1  . . .   nj  j = 1  . . .   d. We
consider latent cluster variables zi j that deﬁne the partition of the data. We will use alternatively the
notation θi j = Uzi j in the following. Hierarchical Dirichlet Process [12] (HDP) is a very popular
model for sharing clusters among related groups. It is based on a hierarchy of DPs

G0 ∼ DP (γ  H) 
Gj|G0 ∼ DP (α  G0) j = 1  . . . d
θi j|Gj ∼ Gj  yi j|θi j ∼ f (θi j) i = 1  . . .   nj.

Under conjugacy assumptions  G0  Gj and U can be integrated out and we can approximate the
marginal posterior of (zi j) given y = (yi j) with Gibbs sampling using the Chinese restaurant
franchise to sample from the full conditional p(zi j|z−{i j}  y).
Using the graph formulation deﬁned in Section 2  we propose an alternative to HDP. Let
θ0 1  . . .   θ0 N be N auxiliary variables belonging to what we call group 0. We deﬁne each clique Cj
(j = 1  . . .   d) to be composed of elements from group j and elements from group 0. This deﬁnes a
decomposable graphical model whose separator is given by the elements of group 0. We can rewrite
the model in a way quite similar to HDP

G0 ∼ DP (α  H) 
θ0 i|G0 ∼ G0
Gj|θ0 1  . . .   θ0 N ∼ DP (α + N  α
α+N H + α
θi j|Gj ∼ Gj  yi j|θi j ∼ f(θi j) i = 1  . . .   nj

i = 1  ...  N

α+N

(cid:80)N

i=1 δθ0 i)

j = 1  . . . d 

For any subset A and j (cid:54)= k ∈ {1  . . .   p} we have corr(Gj(A)  Gk(A)) = N
α+N . Again  under
conjugacy conditions  we can integrate out G0  Gj and U and approximate the marginal posterior
distribution over the partition using the Chinese wedding party process deﬁned in Section 2. Note
that for latent variables zi j  j = 1  . . .   d  associated to data  this is the usual CRP update. As in
HDP  multiple layers can be added to the model. Figures 2 (a) and (b) resp. give the graphical DP
alternative to HDP and 2-layer HDP.

6

root

z0

docs

z1

z2

z3

root

z0

corpora

z1

z2

docs

z1 1 z1 2 z2 1 z2 2

z2 3

(a) Graphical DP alter-
native to HDP

(b) Graphical DP alternative to 2-layer
HDP

Figure 2: Hierarchical Graphs of dependency with (a) one layer and (b) two layers of hierarchy.

If N = 0  then Gj ∼ DP (α  H) for all j and this is equivalent to setting γ → ∞ in HDP. If N → ∞
then Gj = G0 for all j  G0 ∼ DP (α  H). This is equivalent to setting α → ∞ in the HDP. One
interesting feature of the model is that  contrary to HDP  the marginal distribution of Gj at any layer
of the tree is DP (α  H). As a consequence  the total number of clusters scales logarithmically (as in
the usual DP) with the size of each group  whereas it scales doubly logarithmically in HDP. Contrary
to HDP  there are at most N clusters shared between different groups. Our model is in that sense
reminiscent of [9] where only a limited number of clusters can be shared. Note however that contrary
to [9] we have a simple CRP-like process. The proposed methodology can be straightforwardly
extended to the inﬁnite HMM [12].
The main issue of the proposed model is the setting of the number N of auxiliary parameters.
Another issue is that to achieve high correlation  we need a large number of auxiliary variables.
Nonetheless  the computational time used to sample from auxiliary variables is negligible compared
to the time used for latent variables associated to data. Moreover  it can be easily parallelized. The
model proposed offers a far richer framework and ensures that at each level of the tree  the marginal
distribution of the partition is given by a DP partition model.

is a binary matrix whereas Y is a matrix of latent features. By assuming that Y ∼ N(cid:161)

4.2 Time-varying matrix factorization
Let X1:n be an observed matrix of dimension n× D. We want to ﬁnd a representation of this matrix
in terms of two latent matrices Z1:n of dimension n × K and Y of dimension K × D. Here Z1:n
Y IK×D
and

0  σ2

(cid:162)

X1:n = Z1:nY + σX εn where εn ∼ N(cid:161)
(cid:175)(cid:175)(cid:175)Z+T
(cid:189)
(cid:179)

(cid:175)(cid:175)(cid:175)−D/2
(cid:180)−1

1:n + σ2
(n−K+
X

Y IK+
σK+
n D

X /σ2
n )D

1:nZ+

exp

σ

Y

n

(cid:162)

(cid:161)

0  σ2

X In×D

 

− 1
2σ2
X

tr

1:nΣ−1
XT

n X1:n

(cid:162)(cid:190)

(17)

we obtain

p(X1:n|Z1:n) ∝

n

1:n

Y IK+

X /σ2

1:n + σ2

Z+T
1:n  K +

n = I −Z+

Z+T
1:n is the ﬁrst K +

where Σ−1
n the number of non-zero columns of
1:nZ+
Z1:n and Z+
n columns of Z1:n. To avoid having to set K  [5  14] assume that Z1:n
follows an IBP. The resulting posterior distribution p(Z1:n|X1:n) can be estimated through MCMC
[5] or SMC [14].
We consider here a different model where the object Xt is assumed to arrive at time index t and we
want a prior distribution on Z1:n ensuring that objects close in time are more likely to possess similar
features. To achieve this  we consider the simple directed graphical model D of Fig. 3 where the site
numbering corresponds to a time index in that case and a perfect numbering of D is (1  2  . . .). The
set of parents pa(t) is composed of the r preceding sites {{t − r}  . . .  {t − 1}}. The time-varying
IBP to sample from p(Z1:n) associated to this directed graph follows from (16) and proceeds as
follows.
At time t = 1
• Sample K new
At times t = 2  . . .   r
• For k = 1  . . . K +

1 ∼Poisson(α)  set z1 i = 1 for i = 1  ...  K new

t   sample zt k ∼ Ber( n1:t−1 k

∼Poisson( α
t ).

1 = Knew.

) and K new

and set K +

1

t

t

7







t−r+1

t−r

-

-

. . .

-

-










t−1

-

?

?

-

t+1

t



6



6

Figure 3: Directed graph.

t

r+1

r+1).

) and K new

∼Poisson( α

t   sample zt k ∼ Ber( nt−r:t−1 k

At times t = r + 1  . . .   n
• For k = 1  . . . K +
is the total number of features appearing from time max(1  t − r) to t − 1 and nt−r:t−1 k
Here K +
t
the restriction of n1:t−1 to the r last customers. Using (17) and the prior distribution of Z1:n which
can be sampled using the time-varying IBP described above  we can easily design an SMC method
to sample from p(Z1:n|X1:n). We do not detail it here. Note that contrary to [14]  our algorithm
does not require inverting a matrix whose dimension grows linearly with the size of the data but only
a matrix of dimension r× r. In order to illustrate the model and SMC algorithm  we create 200 6×6
images using a ground truth Y consisting of 4 different 6 × 6 latent images. The 200 × 4 binary
.5 0
matrix was generated from Pr(zt k = 1) = πt k  where πt = ( .6
0 ) if t = 1  . . .   30 
πt = ( .4
.6
.6 ) if t = 51  . . .   200. The
order of the model is set to r = 50. The feature occurences Z1:n and true features Y and their
estimates are represented in Figure 4. Two spurious features are detected by the model (features 2
and 5 on Fig. 3(c)) but quickly discarded (Fig. 4(d)). The algorithm is able to correctly estimate the
varying prior occurences of the features over time.

.4 0 ) if t = 31  . . .   50 and πt = ( 0

.8

.3

(a)
Figure 4: (a) True features  (b) True features occurences  (c) MAP estimate ZM AP and (d) associated
E[Y|ZM AP ]

(b)

(d)

(c)

Figure 5: (a) E[Xt|πt  Y] and (b) E[Xt|X1:t−1] at t = 20  50  100  200.

(a)

(b)

5 Related work and Discussion
The ﬁxed-lag version of the time-varying DP of Caron et al. [1] is a special case of the proposed
model when G is given by Fig. 3. The bivariate DP of Walker and Muliere [13] is also a special
case when G has only two cliques. In this paper  we have assumed that the structure of the graph
was known beforehand and we have shown that many ﬂexible models arise from this framework. It
would be interesting in the future to investigate the case where the graphical structure is unknown
and must be estimated from the data.

Acknowledgment

The authors thank the reviewers for their comments that helped to improve the writing of the paper.

8

Feature1Feature2Feature3Feature4FeatureTime123420406080100120140160180200Feature1Feature2Feature3Feature4Feature5Feature6FeatureTime12345620406080100120140160180200t=20t=50t=100t=200t=20t=50t=100t=200References
[1] F. Caron  M. Davy  and A. Doucet. Generalized Polya urn for time-varying Dirichlet process

mixtures. In Uncertainty in Artiﬁcial Intelligence  2007.

[2] A.P. Dawid and S.L. Lauritzen. Hyper Markov laws in the statistical analysis of decomposable

graphical models. The Annals of Statistics  21:1272–1317  1993.

[3] M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal

of the American Statistical Association  90:577–588  1995.

[4] P. Fearnhead. Particle ﬁlters for mixture models with an unknown number of components.

Statistics and Computing  14:11–21  2004.

[5] T.L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In Advances in Neural Information Processing Systems  2006.

[6] D. Heinz. Building hyper dirichlet processes for graphical models. Electonic Journal of Statis-

tics  3:290–315  2009.

[7] J.F.C. Kingman. Random partitions in population genetics. Proceedings of the Royal Society

of London  361:1–20  1978.

[8] S.L. Lauritzen. Graphical Models. Oxford University Press  1996.
[9] P. M¨uller  F. Quintana  and G. Rosner. A method for combining inference across related non-

parametric Bayesian models. Journal of the Royal Statistical Society B  66:735–749  2004.

[10] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of

Computational and Graphical Statistics  9:249–265  2000.

[11] J. Pitman. Exchangeable and partially exchangeable random partitions. Probability theory and

related ﬁelds  102:145–158  1995.

[12] Y.W. Teh  M.I. Jordan  M.J. Beal  and D.M. Blei. Hierarchical Dirichlet processes. Journal of

the American Statistical Association  101:1566–1581  2006.

[13] S. Walker and P. Muliere. A bivariate Dirichlet process. Statistics and Probability Letters 

64:1–7  2003.

[14] F. Wood and T.L. Grifﬁths. Particle ﬁltering for nonparametric Bayesian matrix factorization.

In Advances in Neural Information Processing Systems  2007.

9

,Tengyang Xie
Bo Liu
Yangyang Xu
Mohammad Ghavamzadeh
Yinlam Chow
Daoming Lyu
Daesub Yoon