2016,Mixed Linear Regression with Multiple Components,In this paper  we study the mixed linear regression (MLR) problem  where the goal is to recover multiple underlying linear models from their unlabeled linear measurements. We propose a non-convex objective function which we show is {\em locally strongly convex} in the neighborhood of the ground truth. We use a tensor method for initialization so that the initial models are in the local strong convexity region. We then employ general convex optimization algorithms to minimize the objective function. To the best of our knowledge  our approach provides first exact recovery guarantees for the MLR problem with $K \geq 2$ components. Moreover   our method has near-optimal computational complexity $\tilde O (Nd)$ as well as near-optimal sample complexity $\tilde O (d)$ for {\em constant} $K$. Furthermore  we show that our non-convex formulation can be extended to solving the {\em subspace clustering} problem as well. In particular  when initialized within a small constant distance to the true subspaces  our method converges to the global optima (and recovers true subspaces) in time {\em linear} in the number of points. Furthermore  our empirical results indicate that even with random initialization  our approach converges to the global optima in linear time  providing speed-up of up to two orders of magnitude.,Mixed Linear Regression with Multiple Components

Kai Zhong 1

Prateek Jain 2

Inderjit S. Dhillon 3

1 3 University of Texas at Austin
1 zhongkai@ices.utexas.edu 

2 Microsoft Research India
2 prajain@microsoft.com

3 inderjit@cs.utexas.edu

Abstract

In this paper  we study the mixed linear regression (MLR) problem  where the
goal is to recover multiple underlying linear models from their unlabeled linear
measurements. We propose a non-convex objective function which we show is
locally strongly convex in the neighborhood of the ground truth. We use a tensor
method for initialization so that the initial models are in the local strong convexity
region. We then employ general convex optimization algorithms to minimize the
objective function. To the best of our knowledge  our approach provides ﬁrst exact
recovery guarantees for the MLR problem with K  2 components. Moreover  our
method has near-optimal computational complexity eO(N d) as well as near-optimal
sample complexity eO(d) for constant K. Furthermore  we show that our non-

convex formulation can be extended to solving the subspace clustering problem
as well. In particular  when initialized within a small constant distance to the true
subspaces  our method converges to the global optima (and recovers true subspaces)
in time linear in the number of points. Furthermore  our empirical results indicate
that even with random initialization  our approach converges to the global optima
in linear time  providing speed-up of up to two orders of magnitude.

1

Introduction

The mixed linear regression (MLR) [7  9  29] models each observation as being generated from one
of the K unknown linear models; the identity of the generating model for each data point is also
unknown. MLR is a popular technique for capturing non-linear measurements while still keeping the
models simple and computationally efﬁcient. Several widely-used variants of linear regression  such
as piecewise linear regression [14  28] and locally linear regression [8]  can be viewed as special
cases of MLR. MLR has also been applied in time-series analysis [6]  trajectory clustering [15] 
health care analysis [11] and phase retrieval [4]. See [27] for more applications.
In general  MLR is NP-hard [29] with the hardness arising due to lack of information about the model
labels (model from which a point is generated) as well as the model parameters. However  under
certain statistical assumptions  several recent works have provided poly-time algorithms for solving
MLR [2  4  9  29]. But most of the existing recovery gurantees are restricted either to mixtures with
K = 2 components [4  9  29] or require poly(1/✏) samples/time to achieve ✏-approximate solution
[7  24] (analysis of [29] for two components can obtain ✏ approximate solution in log(1/✏) samples).
Hence  solving the MLR problem with K  2 mixtures while using near-optimal number of samples
and computational time is still an open question.
In this paper  we resolve the above question under standard statistical assumptions for constant
many mixture components K. To this end  we propose the following smooth objective function as a

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

surrogate to solve MLR:

f (w1  w2 ···   wK) :=

nXi=1

⇧K
k=1(yi  xT

i wk)2 

(1)

where {(xi  yi) 2 Rd+1}i=1 2 ···  N are the data points and {wk}k=1 2 ···  K are the model parameters.
The intuition for this objective is that the objective value is zero when {wk}k=1 2 ···  K is the global
optima and y’s do not contain any noise. Furthermore  the objective function is smooth and hence
less prone to getting stuck in arbitrary saddle points or oscillating between two points. The standard
EM [29] algorithm instead makes a “sharp” selection of mixture component and hence the algorithm
is more likely to oscillate or get stuck. This intuition is reﬂected in Figure 1 (d) which shows that
with random initialization  EM algorithm routinely gets stuck at poor solutions  while our proposed
method based on the above objective still converges to the global optima.
Unfortunately  the above objective function is non-convex and is in general prone to poor saddle
points  local minima. However under certain standard assumptions  we show that the objective is
locally strongly convex (Theorem 1) in a small basin of attraction near the optimal solution. Moreover 
the objective function is smooth. Hence  we can use gradient descent method to achieve linear rate
of convergence to the global optima. But  we will need to initialize the optimization algorithm with
an iterate which lies in a small ball around the optima. To this end  we modify the tensor method
in [2  7] to obtain a “good” initialization point. Typically  tensor methods require computation of
third and higher order moments which leads to signiﬁcantly worse sample complexity in terms of
data dimensionality d. However  for the special case of MLR  we provide a small modiﬁcation of the
standard tensor method that achieves nearly optimal sample and time complexity bounds for constant
K (see Theorem 3) . More concretely  our approach requires ˜O(d(K log d)K) many samples and
requires ˜O(N d) computational time; note the exponential dependence on K. Also for constant K 
the method has nearly optimal sample and time complexity.
Subspace clustering: MLR can be viewed as a special case of subspace clustering (SC)  since each
regressor-response pair lies in the subspace determined by this pair’s model parameters. However 
solving MLR using SC approaches is intractable because the dimension of each subspace is only one
less than the ambient dimension  which will easily violate the conditions for the recovery guarantees
of most methods (see e.g. Table 1 in [23] for the conditions of different methods). Nonetheless 
our objective for MLR easily extends to the subspace clustering problem. That is  given data points
{zi 2 Rd}i=1 2 ···  N  the goal is to minimize the following objective w.r.t. K subspaces (each of
dimension at most r):

min

Uk2Od⇥r k=1 2 ···  K

f (U1  U2 ···   UK) =

NXi=1

⇧K

k=1⌦Id  UkU T

k   zizT

i ↵.

(2)

Uk denotes the basis spanned by k-th estimated subspace and Od⇥r ⇢ Rd⇥r denotes the set of
orthonormal matrices  i.e.  U T U = I if U 2 Od⇥r. We propose a power-method style algorithm
to alternately optimize (2) w.r.t {Uk}k=1 2 ···  K  which takes only O(rdN ) time compared with
O(dN 2) for the state-of-the-art methods  e.g. [13  22  23].
Although EM with power method [4] shares the same computational complexity as ours  there
is no convergence guarantee for EM to the best of our knowledge. In contrast  we provide local
convergence guarantee for our method. That is  if N = ˜O(rKK) and if data satisﬁes certain standard
assumptions  then starting from an initial point {Uk}k=1 ···  K that lies in a small ball of constant
radius around the globally optimal solution  our method converges super-linearly to the globally
optimal solution. Unfortunately  our existing analyses do not provide global convergence guarantee
and we leave it as a topic for future work. Interestingly  our empirical results indicated that even with
randomly initialized {Uk}k=1 ···  K  our method is able to recover the true subspace exactly using
nearly O(rK) samples.
We summarize our contributions below:
(1) MLR: We propose a non-convex continuous objective function for solving the mixed linear
regression problem. To the best of our knowledge  our algorithm is the ﬁrst work that can handle K 
2 components with global convergence guarantee in the noiseless case (Theorem 4). Our algorithm has
near-optimal linear (in d) sample complexity and near-optimal computational complexity; however 
our sample complexity dependence on K is exponential.

2

(2) Subspace Clustering: We extend our objective function to subspace clustering  which can be
optimized efﬁciently in O(rdN ) time compared with O(dN 2) for state-of-the-art methods. We also
provide a small basin of attraction in which our iterates converge to the global optima at super-linear
rate (Theorem 5).

2 Related Work

Mixed Linear Regression:
EM algorithm without careful initialization is only guaranteed to have local convergence [4  21  29].
[29] proposed a grid search method for initialization. However  it is limited to the two-component
case and seems non-trivial to extend to multiple components. It is known that exact minimization
for each step of EM is not scalable due to the O(d2N + d3) complexity. Alternatively  we can
use EM with gradient update  whose local convergence is also guaranteed by [4] but only in the
two-symmetric-component case  i.e.  when w2 = w1.
Tensor Methods for MLR were studied by [7  24]. [24] approximated the third-order moment directly
from samples with Gaussian distribution  while [7] learned the third-order moment from a low-rank
linear regression problem. Tensor methods can obtain the model parameters to any precision ✏ but
requires 1/✏2 time/samples. Also  tensor methods can handle multiple components but suffer from
high sample complexity and high computational complexity. For example  the sample complexity
required by [7] and [24] is O(d6) and O(d3) respectively. On the other hand  the computational
burden mainly comes from the operation on tensor  which costs at least O(d3) for a very simple
tensor evaluation. [7] also suffers from the slow nuclear norm minimization when estimating the
second and third order moments. In contrast  we use tensor method only for initialization  i.e.  we
require ✏ to be a certain constant. Moreover  with a simple trick  we can ensure that the sample and
time complexity of our initialization step is only linear in d and N.
Convex Formulation. Another approach to guarantee the recovery of the parameters is to relax
the non-convex problem to convex problem. [9] proposed a convex formulation of MLR with two
components. The authors provide upper bounds on the recovery errors in the noisy case and show their
algorithm is information-theoretically optimal. However  the convex formulation needs to solve a
nuclear norm function under linear constraints  which leads to high computational cost. The extension
from two components to multiple components for this formulation is also not straightforward.
Subspace Clustering:
Subspace clustering [13  17  22  23] is an important data clustering problem arising in many research
areas. The most popular subspace clustering algorithms  such as [13  17  23]  are based on a two-stage
algorithm – ﬁrst ﬁnding a neighborhood for each data point and then clustering the points given the
neighborhood. The ﬁrst stage usually takes at least O(dN 2) time  which is prohibitive when N is
large. On the other hand  several methods such as K-subspaces clustering [18]  K-SVD [1] and online
subspace clustering [25] do have linear time complexity O(rdN ) per iteration  however  there are no
global or local convergence guarantees. In contrast  we show locally superlinear convergence result
for an algorithm with computational complexity O(rdN ). Our empirical results indicate that random
initialization is also sufﬁcient to get to the global optima; we leave further investigation of such an
algorithm for future work.

3 Mixed Linear Regression with Multiple Components
In this paper  we assume the dataset {(xi  yi) 2 Rd+1}i=1 2 ···  N is generated by 

yi = xT

i w⇤zi 

zi ⇠ multinomial(p)  xi ⇠N (0  Id) 

(3)
where p is the proportion of different components satisfying pT 1 = 1  {w⇤k 2 Rd}k=1 2 ···  K are
the ground truth parameters. The goal is to recover {w⇤k}k=1 2 ···  K from the dataset. Our analysis is
based on noiseless cases but we illustrate the empirical performance of our algorithm for the noisy
cases  where yi = xT
Notation. We use [N ] to denote the set {1  2 ···   N} and Sk ⇢ [N ] to denote the index set of the
samples that come from k-th component. Deﬁne pmin := mink2[K]{pk}  pmax := maxk2[K]{pk}.
Deﬁne wj := wj  w⇤j and w⇤kj := w⇤k  w⇤j . Deﬁne min := minj6=k{kw⇤jkk} and

i w⇤zi + ei for some noise ei (see Figure 1).

3

max

max := maxj6=k{kw⇤jkk}. We assume min
is independent of the dimension d. Deﬁne w :=
[w1; w2;··· ; wK] 2 RKd. We denote w(t) as the parameters at t-th iteration and w(0) as the
initial parameters. For simplicity  we assume there are pkN samples from the k-th model in any

random subset of N samples. We use EJXK to denote the expectation of a random variable X. Let
T 2 Rd⇥d⇥d be a tensor and Tijk be the i  j  k-th entry of T . We say a tensor is supersymmetric
if Tijk is invariant under any permutation of i  j  k. We also use the same notation T to denote
the multi-array map from three matrices  A  B  C 2 Rd⇥r  to a new tensor: [T (A  B  C)]i j k =
Pp q l TpqlApiBqjClk. We say a tensor T is rank-one if T = a ⌦ b ⌦ c  where Tijk = aibjck.
We use kAk denote the spectral norm of the matrix A and i(A) to denote the i-th largest singular
value of A. For tensors  we use kTkop to denote the operator norm for a supersymmetric tensor T  
kTkop := maxkak=1 |T (a  a  a)|. We use T(1) 2 Rd⇥d2 to denote the matricizing of T in the ﬁrst
order  i.e.  [T(1)]i (j1)d+k = Tijk. Throughout the paper  we use eO(d) to denote O(d⇥ polylog(d)).
We assume K is a constant in general. However  if some numbers depend on KK  we will explicitly
present it in the big O notation. For simplicity  we just include higher-order terms of K and ignore
lower-order terms  e.g.  O((2K)2K) may be replaced by O(KK).

1
8

3.1 Local Strong Convexity
In this section  we analyze the Hessian of objective (1).
Theorem 1 (Local Positive Deﬁniteness). Let {xi  yi}i=1 2 ···  N be sampled from the MLR model
(3). Let {wk}k=1 2 ···  K be independent of the samples and lie in the neighborhood of the optimal
solution  i.e. 
(4)
where cm = O(pmin(3K)K(min/max)2K2)  min = minj6=k{kw⇤j  w⇤kk} and max =
maxj6=k{kw⇤j  w⇤kk}. Let P  1 be a constant. Then if N  O((P K)Kd logK+2(d))  w.p.
1  O(KdP )  we have 

kwkk := kwk  w⇤kk  cmmin 8k 2 [K] 

pminN 2K2

min I  r2f (w + w)  10N (3K)K2K2
max I 

(5)
for any w := [w1; w2;··· ; wK] satisfying kwkk  cf min  where cf =
O(pmin(3K)KdK+1(min/max)2K2).
The above theorem shows the Hessians of a small neighborhood around a ﬁxed {wk}k=1 2 ···  K 
which is close enough to the optimum  are positive deﬁniteness (PD). The conditions on
{wk}k=1 ···  K and {wk}k=1 ···  K are different. {wk}k=1 ···  K are required to be independent
of samples and in a ball of radius cmmin centered at the optimal solution. On the other hand 
{wk}k=1 2 ···  K can be dependent on the samples but are required to be in a smaller ball of radius
cf min. The conditions are natural as if min is very small then distinguishing between w⇤k and w⇤k0
is not possible and hence Hessians will not be PD w.r.t both the components.
To prove the theorem  we decompose the Hessian of Eq. (1) into multiple blocks  (rf )jl = @2f
@wj @wl 2
Rd⇥d. When wk ! w⇤k for all k 2 [K]  the diagonal blocks of the Hessian will be strictly positive
deﬁnite. At the same time  the off-diagonal blocks will be close to zeros. The blocks are approximated
by the samples using matrix Bernstein inequality. The detailed proof can be found in Appendix A.2.
Traditional analysis of optimization methods on strongly convex functions  such as gradient descent 
requires the Hessians of all the parameters are PD. Theorem 1 implies that when wk = w⇤k for all
k = 1  2 ···   K  a small basin of attraction around the optimum is strongly convex as formally stated
in the following corollary.
Corollary 1 (Strong Convexity near the Optimum). Let {xi  yi}i=1 2 ···  N be sampled from the MLR
model (3). Let {wk}k=1 2 ···  K lie in the neighborhood of the optimal solution  i.e. 
(6)
where cf = O(pmin(3K)KdK+1(min/max)2K2). Then  for any constant P  1  if N 
O((P K)Kd logK+2(d))  w.p. 1  O(KdP )  the objective function f (w1  w2 ···   wK) in Eq. (1)
is strongly convex. In particular  w.p. 1  O(KdP )  for all w satisfying Eq. (6) 

kwk  w⇤kk  cf min 8k 2 [K] 

1
8

pminN 2K2

min I  r2f (w)  10N (3K)K2K2
max I.

(7)

4

The strong convexity of Corollary 1 only holds in the basin of attraction near the optimum that has
diameter in the order of O(dK+1)  which is too small to be achieved by our initialization method
(in Sec. 3.2) using ˜O(d) samples. Next  we show by a simple construction  the linear convergence of
gradient descent (GD) with resampling is still guaranteed when the solution is initialized in a much
larger neighborhood.
Theorem 2 (Convergence of Gradient Descent). Let {xi  yi}i=1 2 ···  N be sampled from the MLR
model (3). Let {wk}k=1 2 ···  K be independent of the samples and lie in the neighborhood of
the optimal solution  deﬁned in Eq. (4). One iteration of gradient descent can be described as 
max ). Then  if N  O(KKd logK+2(d))  w.p.
w+ = w  ⌘rf (w)  where ⌘ = 1/(10N (3K)K2K2
1  O(Kd2) 

kw+  w⇤k2  (1 

)kw  w⇤k2

(8)

pmin2K2

min

80(3K)K2K2

max

Remark. The linear convergence Eq. (8) requires the resampling of the data points for each iteration.
In Sec. 3.3  we combine Corollary 1  which doesn’t require resampling when the iterate is sufﬁciently
close to the optimum  to show that there exists an algorithm using a ﬁnite number of samples to
achieve any solution precision.
To prove Theorem 2  we prove the PD properties on a line between a current iterate and the optimum
by constructing a set of anchor points and then apply traditional analysis for the linear convergence
of gradient descent. The detailed proof can be found in Appendix A.3.

3.2

Initialization via Tensor method

In this section  we propose a tensor method to initialize the parameters. We deﬁne the second-order

moment M2 := Eqy2(x ⌦ x  I)y and the third-order moments  M3 := Eqy3x ⌦ x ⌦ xy 
Eqy3(ej ⌦ x ⌦ ej + ej ⌦ ej ⌦ x + x ⌦ ej ⌦ ej)y. According to Lemma 6 in [24]  M2 =
Pj2[d]
Pk=[K] 2pkw⇤k ⌦ w⇤k and M3 =Pk=[K] 6pkw⇤k ⌦ w⇤k ⌦ w⇤k. Therefore by calculating the eigen-

decomposition of the estimated moments  we are able to recover the parameters to any precision
provided enough samples. Theorem 8 of [24] needs O(d3) sample complexity to obtain the model
parameters with certain precision. Such high sample complexity comes from the tensor concentration
bound. However  we ﬁnd the problem of tensor eigendecomposition in MLR can be reduced to
RK⇥K⇥K space such that the sample complexity and computational complexity are O(poly(K)).
Our method is similar to the whitening process in [7  19]. However  [7] needs O(d6) sample complex-

ity due to the nuclear-norm minimization problem  while ours requires only eO(d). For this sample
complexity  we need assume the following 
op   Pk2[K] pkkw⇤kk2 and
Assumption 1. The following quantities  K(M2)  kM2k  kM3k2/3
(Pk2[K] pkkw⇤kk3)2/3  have the same order of d  i.e.  the ratios between any two of them are
independent of d.
The above assumption holds when {w⇤k}k=1 2 ···  K are orthonormal to each other.
We formally present the tensor method in Algorithm 1 and its theoretical guarantee in Theorem 3.
Theorem 3. Under Assumption 1  if |⌦| O(d log2(d)+log4(d))  then w.p. 1O(d2)  Algorithm 1
will output {w(0)

k }K

k=1 that satisﬁes 
kw(0)

k  w⇤kk  cmmin 8k 2 [K]

which falls in the locally PD region  Eq. (4)  in Theorem 1.

forming ˆM2. In particular  we alternately compute ˆY (t+1) = Pi2⌦M2

The proof can be found in Appendix B.2. Forming ˆM2 explicitly will cost O(N d2) time  which
is expensive when d is large. We can compute each step of the power method without explicitly
i Y (t))  Y (t))
and let Y (t+1) = QR( ˆY (t+1)). Now each power method iteration only needs O(KN d) time.
Furthermore  the number of iterations needed will be a constant  since power method has linear
convergence rate and we don’t need very accurate solution. For the proof of this claim  we refer

y2
i (xi(xT

5

k=1

2

Algorithm 1 Initialization for MLR via Tensor Method
Input: {xi  yi}i2⌦
Output: {w(0)
k }K
1: Partition the dataset ⌦ into ⌦=⌦ M2[⌦2[⌦3 with |⌦M2| = O(d log2(d))  |⌦2| = O(d log2(d))
and |⌦3| = O(log4(d))
2: Compute the approximate top-K eigenvectors  Y 2 Rd⇥K  of the second-order moment  ˆM2 :=
|⌦M2|Pi2⌦M2
i (xi ⌦ xi  I)  by the power method.
y2
2|⌦2|Pi2⌦2
3: Compute ˆR2 = 1
i (Y T xi ⌦ Y T xi  I).
y2
4: Compute the whitening matrix ˆW 2 RK⇥K of ˆR2  i.e.  ˆW = ˆU2 ˆ⇤1/2
6|⌦3|Pi2⌦3
i (ri ⌦ ri ⌦ ri Pj2[K] ej ⌦ ri ⌦ ej Pj2[K] ej ⌦ ej ⌦ ri 
5: Compute ˆR3 = 1
Pj2[K] ri ⌦ ej ⌦ ej)  where ri = Y T xi for all i 2 ⌦3.
6: Compute the eigenvalues {ˆak}K
k=1 and the eigenvectors {ˆvk}K
ˆR3( ˆW   ˆW   ˆW ) 2 RK⇥K⇥K by using the robust tensor power method [2].
7: Return the estimation of the models  w(0)

2 is the eigendecomposition of ˆR2.

k=1 of the whitened tensor

ˆU T
2   where ˆR2 =

k = Y ( ˆW T )†(ˆak ˆvk)

1

ˆU2 ˆ⇤2 ˆU T

y3

to the proof of Lemma 10 in Appendix B. Next we compute ˆR2 using O(KN d) and compute ˆW
in O(K3) time. Computing ˆR3 takes O(KN d + K3N ) time. The robust tensor power method
takes O(poly(K)polylog(d)) time. In summary  the computational complexity for the initialization

is O(KdN + K3N + poly(K)polylog(d)) = eO(dN ).

3.3 Global Convergence Algorithm
We are now ready to show the complete algorithm  Algorithm 2  that has global convergence guarantee.
We use f⌦(w) to denote the objective function Eq. (1) generated from a subset of the dataset ⌦ 

i.e. f⌦(w) =Pi2⌦ ⇧K
Theorem 4 (Global Convergence Guarantee). Let {xi  yi}i=1 2 ···  N be sampled from the MLR
model (3) with N  O(d(K log(d))2K+3). Let the step size ⌘ be smaller than a positive constant.
Then given any precision ✏> 0  after T = O(log(d/✏)) iterations  w.p. 1  O(Kd2 log(d))  the
output of Algorithm 2 satisﬁes

k=1(yi  xT

i wk)2.

kw(T )  w⇤k  ✏min.

The detailed proof is in Appendix B.3. The computational complexity required by our algorithm
is near-optimal: (a) tensor method (Algorithm 1) is carefully employed such that only O(dN )
computation is needed; (b) gradient descent with resampling is conducted in log(d) iterations to
push the iterate to the next phase; (c) gradient descent without resampling is ﬁnally executed to
achieve any precision with log(1/✏) iterations. Therefore the total computational complexity is
O(dN log(d/✏)). As shown in the theorem  our algorithm can achieve any precision ✏> 0 without
any sample complexity dependency on ✏. This follows from Corollary 1 that shows local strong
convexity of objective (1) with a ﬁxed set of samples. By contrast  tensor method [7  24] requires
O(1/✏2) samples and EM algorithm requires O(log(1/✏)) samples[4  29].

4 Subspace Clustering (SC)

The mixed linear regression problem can be viewed as clustering N (d + 1)-dimensional data points 
zi = [xi  yi]T   into one of the K subspaces  {z : [w⇤k 1]T z = 0} for k 2 [K]. Assume we have
data points {zi}i=1 2 ···  N sampled from the following model 
(9)

ai ⇠ multinomial(p)  si ⇠N (0  Ir)  zi = U⇤aisi 

where p is the proportion of samples from different subspaces and satisﬁes pT 1 = 1 and
{U⇤k}k=1 2 ···  K are the bases of the ground truth subspaces. We can solve Eq. (2) by alternately
minimizing over Uk when ﬁxing the others  which is equivalent to ﬁnding the top-r eigenvectors

6

j   zizT

ofPN

i=1 ↵k

i zizT

i   where ↵k

i =⇧ j6=k⌦Id  UjU T

i ↵. When the dimension is high  it is very

expensive to compute the exact top-r eigenvectors. A more efﬁcient way is to use one iteration of the
power method (aka subspace iteration)  which only takes O(KdN ) computational time per iteration.
We present our algorithm in Algorithm 3.
We show Algorithm 3 will converge to the ground truth when the initial subspaces are sufﬁciently close
to the underlying subspaces. Deﬁne D( ˆU   ˆV ) := 1p2kU U TV V TkF for some ˆU   ˆV 2 Rd⇥r  where
U  V are orthogonal bases of Span( ˆU )  Span( ˆV ) respectively. Deﬁne Dmax := maxj6=q D(U⇤q   U⇤j ) 
Dmin := minj6=q D(U⇤q   U⇤j ).
Theorem 5. Let {zi}i=1 2 ···  N be sampled from subspace clustering model (9).
O(r(K log(r))2K+2) and the initial parameters {U 0
k {D(U⇤k   U 0
max

(10)
where cs = O(pmin/pmax(3K)K(Dmin/Dmax)2K3)  then w.p. 1  O(Kr2)  the sequence
K}t=1 2 ··· generated by Algorithm 3 converges to the ground truth superlinearly. In
{U t
particular  for t := maxk{D(U⇤k   U t

k}k2[K] satisfy
k )} csDmin 

2 ···   U t

If N 

k)} 

1  U t

t+1  2

t /(2csDmin) 

1
2

t.

We refer to Appendix C.2 for the proof. Compared to other methods  our sample complexity only
depends on the dimension of each subspace linearly. We refer to Table 1 in [23] for a comparison
of conditions for different methods. Note that if Dmin/Dmax is independent of r or d  then the
initialization radius cs is a constant. However  initialization within the required distance to the optima
is still an open question; tensor methods do not apply in this case. Interestingly  our experiments
seem to suggest that our proposed method converges to the global optima (in the setting considered
in the above theorem).

Algorithm 2 Gradient Descent for MLR
Input: {xi  yi}i=1 2 ···  N  step size ⌘.
Output: w
1: Partition

dataset

the
{⌦(t)}t=0 1 ···  T0+1

2: Initialize w(0) by Algorithm 1 with ⌦(0)
3: for t = 1  2 ···   T0 do
w(t) = w(t1)  ⌘rf⌦(t)(w(t1))
4:
5: for t = T0 + 1  T0 + 2 ···   T0 + T1 do
w(t) = w(t1) ⌘rf⌦(T0+1)(w(t1))
6:

into

5 Numerical Experiments

Algorithm 3 Power Method for SC
Input: data points {zi}i=1 2 ···  N
Output: {Uk}k2[K]
1: Some initialization  {U 0
k}k2[K].
2: Partition the data into {⌦(t)}t=0 1 2 ···  T .
3: for t = 0  1  2 ···   T do
i ↵  i 2 ⌦(t)
j=1⌦Id  U t
j U tT
↵i =⇧ K
4:
j
for k = 1  2 ···   K do
5:
i = ↵i/⌦Id  U t
i ↵
6:
k QR(Pi2⌦(t) ↵k
7:

k   zizT
i zizT

↵k
U t+1

i U t
k)

  zizT

kU tT

5.1 Mixed Linear Regression
In this section  we use synthetic data to show the properties of our algorithm that minimizes Eq. (1) 
which we call LOSCO (LOcally Strongly Convex Objective). We generate data points and parameters
from standard normal distribution. We set K = 3 and pk = 1
3 for all k 2 [K]. The error is deﬁned
as ✏(t) = min⇡2Perm([K]){maxk2[K] kw(t)
⇡(k)  w⇤kk/kw⇤kk}  where Perm([K]) is the set of all the
permutation functions on the set [K]. The errors reported in the paper are averaged over 10 trials. In
our experiments  we ﬁnd there is no difference whether doing resampling or not. Hence  for simplicity 
we use the original dataset for all the processes. We set both of two parameters in the robust tensor
power method (denoted as N and L in Algorithm 1 in [2]) to be 100. The experiments are conducted
in Matlab. After the initialization  we use alternating minimization (i.e.  block coordinate descent) to
exactly minimize the objective over wk for k = 1  2 ···   K cyclicly.
Fig. 1(a) shows the recovery rate for different dimensions and different samples. We call the result of
a trial is a successful recovery if ✏(t) < 106 for some t < 100. The recovery rate is the proportion of

7

10000

8000

6000

4000

2000

N

N=6000
N=60000
N=600000

0

-5

-10

)
ϵ
(
g
o

l

0

0

200

400

d

600

800

1000

(a) Sample complexity

-15

-10

-5

log(σ)

0
(b) Noisy case

0

)
r
r
e
(
g
o

l

-10

-20

-30

0

0

)
r
r
e
(
g
o

l

-10

-20

1.5

-30

0

LOSCO-ALT-tensor
EM-tensor
LOSCO-ALT-random
EM-random
1

0.5

time(s)

LOSCO-ALT-tensor
EM-tensor
LOSCO-ALT-random
EM-random

100

200
time(s)

300

400

(c) d = 100  N = 6k

(d) d = 1k  N = 60k

Figure 1: (a) (b): Empirical performance of our method. (c)  (d): performance of our methods vs
EM method. Our method with random initialization is signﬁcantly better than EM with random
initialization. Performance of the two methods is comparable when initialized with tensor method.

10 trials with successful recovery. As shown in the ﬁgure  the sample complexity for exact recovery
is nearly linear to d. Fig. 1(b) shows the behavior of our algorithm in the noisy case. The noise is
drawn from ei 2N (0  2)  i.i.d.  and d is ﬁxed as 100. As we can see from the ﬁgure  the solution
error is almost proportional to the noise deviation. Comparing among different N’s  the solution error
decreases when N increases  so it seems consistent in presence of unbiased noise. We also illustrate
the performance of our tensor initialization method in Fig. 2(a) in Appendix D.
We next compare with EM algorithm [29]  where we alternately assign labels to points and exactly
solve each model parameter according to the labels. EM has been shown to be very sensitive to the
initialization [29]. The grid search initialization method proposed in [29] is not feasible here  because
it only handles two components with a same magnitude. Therefore  we use random initialization
and tensor initialization for EM. We compare our method with EM on convergence speed under
different dimensions and different initialization methods. We use exact alternating minimization
(LOSCO-ALT) to optimize our objective (1)  which has similar computational complexity as EM.
Fig. 1(c)(d) shows our method is competitive with EM on computational time  when it converges to
the optima. In the case of (d)  EM with random initialization doesn’t converge to the optima  while
our method still converges. In Appendix D  we will show some more experimental results.

Table 1: Time (sec.) comparison for different subspace clustering methods

N/K
200
400
600
800
1000

SSC
22.08
152.61
442.29
918.94
1738.82

SSC-OMP

31.83
60.74
99.63
159.91
258.39

LRR
4.01
11.18
33.36
79.06
154.89

TSC
2.76
8.45
30.09
75.69
151.64

NSN+spectral NSN+GSR PSC
0.41
0.32
0.60
0.73
0.76

3.28
11.51
36.04
85.92
166.70

5.90
15.90
33.26
54.46
83.96

5.2 Subspace Clustering
In this section  we compare our subspace clustering method  which we call P SC (Power method
for Subspace Clustering)  with state-of-the-art methods  SSC [13]  SSC-OMP [12]  LRR [22]  TSC
[17]  NSN+spectral [23] and NSN+GSR [23] on computational time. We ﬁx K = 5  r = 30 and
d = 50. The ground truth U⇤k is generated from Gaussian matrices. Each data point is a normalized
Gaussian vector in their own subspace. Set pk = 1/K. The initial subspace estimation is generated
by orthonormlizing Gaussian matrices. The stopping criterion for our algorithm is that every point is
clustered correctly  i.e.  the clustering error (CE) (deﬁned in [23]) is zero. We use publicly available
codes for all the other methods (see [23] for the links).
As we shown from Table 1  our method is much faster than all other methods especially when N
is large. Almost all CE’s corresponding to the results in Table 1 are very small  which are listed in
Appendix D. We also illustrate CE’s of our method for different N  d and r when ﬁxing K = 5 in
Fig. 6 of Appendix D  from which we see whatever the ambient dimension d is  the clusters will be
exactly recovered when N is proportional to r.
Acknowledgement: This research was supported by NSF grants CCF-1320746  IIS-1546459 and
CCF-1564000.

8

References
[1] Amir Adler  Michael Elad  and Yacov Hel-Or. Linear-time subspace clustering via bipartite graph modeling.

IEEE transactions on neural networks and learning systems  26(10):2234–2246  2015.

[2] Animashree Anandkumar  Rong Ge  Daniel Hsu  Sham M Kakade  and Matus Telgarsky. Tensor decompo-

sitions for learning latent variable models. JMLR  15:2773–2832  2014.

[3] Peter Arbenz  Daniel Kressner  and D-MATH ETH Zürich. Lecture notes on solving large scale eigenvalue

problems. http://people.inf.ethz.ch/arbenz/ewp/Lnotes/lsevp2010.pdf.

[4] Sivaraman Balakrishnan  Martin J Wainwright  and Bin Yu. Statistical guarantees for the em algorithm:

From population to sample-based analysis. Annals of Statistics  2015.

[5] Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics  9(6):717–772  December 2009.

[6] Alexandre X Carvalho and Martin A Tanner. Mixtures-of-experts of autoregressive time series: asymptotic

normality and model speciﬁcation. Neural Networks  16(1):39–56  2005.

[7] Arun T. Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions. In

ICML  pages 1040–1048  2013.

[8] Xiujuan Chai  Shiguang Shan  Xilin Chen  and Wen Gao. Locally linear regression for pose-invariant face

recognition. Image Processing  16(7):1716–1725  2007.

[9] Yudong Chen  Xinyang Yi  and Constantine Caramanis. A convex formulation for mixed regression with

two components: Minimax optimal rates. COLT  2014.

[10] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM

Journal on Numerical Analysis  7(1):1–46  1970.

[11] P. Deb and A. M. Holmes. Estimates of use and costs of behavioural health care: a comparison of standard

and ﬁnite mixture models. Health economics  9(6):475–489  2000.

[12] Eva L Dyer  Aswin C Sankaranarayanan  and Richard G Baraniuk. Greedy feature selection for subspace

clustering. The Journal of Machine Learning Research  14(1):2487–2517  2013.

[13] Ehsan Elhamifar and René Vidal. Sparse subspace clustering. In CVPR  pages 2790–2797  2009.
[14] Giancarlo Ferrari-Trecate and Marco Muselli. A new learning method for piecewise linear regression. In

Artiﬁcial Neural Networks—ICANN 2002  pages 444–449. Springer  2002.

[15] Scott Gaffney and Padhraic Smyth. Trajectory clustering with mixtures of regression models. In KDD.

ACM  1999.

[16] Jihun Hamm and Daniel D Lee. Grassmann discriminant analysis: a unifying view on subspace-based

learning. In ICML  pages 376–383. ACM  2008.

[17] Reinhard Heckel and Helmut Bolcskei. Subspace clustering via thresholding and spectral clustering. In

Acoustics  Speech and Signal Processing  pages 3263–3267. IEEE  2013.

[18] Jeffrey Ho  Ming-Husang Yang  Jongwoo Lim  Kuang-Chih Lee  and David Kriegman. Clustering
appearances of objects under varying illumination conditions. In CVPR  volume 1  pages I–11. IEEE 
2003.

[19] Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral

decompositions. In ITCS  pages 11–20. ACM  2013.

[20] Daniel Hsu  Sham M Kakade  and Tong Zhang. A tail inequality for quadratic forms of subgaussian

random vectors. Electronic Communications in Probability  17(52):1–6  2012.

[21] Abbas Khalili and Jiahua Chen. Variable selection in ﬁnite mixture of regression models. Journal of the

american Statistical association  2012.

[22] Guangcan Liu  Zhouchen Lin  and Yong Yu. Robust subspace segmentation by low-rank representation. In

ICML  pages 663–670  2010.

2753–2761  2014.

[23] Dohyung Park  Constantine Caramanis  and Sujay Sanghavi. Greedy subspace clustering. In NIPS  pages

[24] Hanie Sedghi and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized

linear models. arXiv preprint arXiv:1412.3046  2014.

[25] Jie Shen  Ping Li  and Huan Xu. Online low-rank subspace clustering by basis dictionary pursuit. In ICML 

[26] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

[27] Kert Viele and Barbara Tong. Modeling with mixtures of linear regressions. Statistics and Computing 

[28] Elisabeth Vieth. Fitting piecewise linear regression functions to biological responses. Journal of applied

[29] Xinyang Yi  Constantine Caramanis  and Sujay Sanghavi. Alternating minimization for mixed linear

regression. In ICML  pages 613–621  2014.

2016.

Mathematics  12(4):389–434  2012.

12(4):315–330  2002.

physiology  67(1):390–396  1989.

9

,Kai Zhong
Prateek Jain
Inderjit Dhillon