2015,Enforcing balance allows local supervised learning in spiking recurrent networks,To predict sensory inputs or control motor trajectories  the brain must constantly learn temporal dynamics based on error feedback. However  it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules  such as temporal back-propagation  are not local and thus not biologically plausible. Furthermore  reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach  we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly  the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron  in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules  using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme  with minimal dimensions and activity.,Enforcing balance allows local supervised learning in

spiking recurrent networks

Ralph Bourdoukan

Group For Neural Theory  ENS Paris

Rue dUlm  29  Paris  France

ralph.bourdoukan@gmail.com

Sophie Deneve

Group For Neural Theory  ENS Paris

Rue dUlm  29  Paris  France
sophie.deneve@ens.fr

Abstract

To predict sensory inputs or control motor trajectories  the brain must con-
stantly learn temporal dynamics based on error feedback. However  it remains
unclear how such supervised learning is implemented in biological neural net-
works. Learning in recurrent spiking networks is notoriously difﬁcult because lo-
cal changes in connectivity may have an unpredictable effect on the global dynam-
ics. The most commonly used learning rules  such as temporal back-propagation 
are not local and thus not biologically plausible. Furthermore  reproducing the
Poisson-like statistics of neural responses requires the use of networks with bal-
anced excitation and inhibition. Such balance is easily destroyed during learning.
Using a top-down approach  we show how networks of integrate-and-ﬁre neu-
rons can learn arbitrary linear dynamical systems by feeding back their error as
a feed-forward input. The network uses two types of recurrent connections: fast
and slow. The fast connections learn to balance excitation and inhibition using a
voltage-based plasticity rule. The slow connections are trained to minimize the
error feedback using a current-based Hebbian learning rule. Importantly  the bal-
ance maintained by fast connections is crucial to ensure that global error signals
are available locally in each neuron  in turn resulting in a local learning rule for
the slow connections. This demonstrates that spiking networks can learn complex
dynamics using purely local learning rules  using E/I balance as the key rather
than an additional constraint. The resulting network implements a given function
within the predictive coding scheme  with minimal dimensions and activity.

The brain constantly predicts relevant sensory inputs or motor trajectories. For example  there is
evidence that neural circuits mimic the dynamics of motor effectors using internal models [1]. If the
dynamics of the predicted sensory and motor variables change in time  these models may become
false [2] and therefore need to be readjusted through learning based on error feedback.
From a modeling perspective  supervised learning in recurrent networks faces many challenges.
Earlier models have succeeded in learning useful functions at the cost of non local learning rules
that are biologically implausible [3  4]. More recent models based on reservoir computing [5–7]
transfer the learning from the recurrent network (with now “random”  ﬁxed weights) to the readout
weights. Using this simple scheme  the network can learn to generate complex patterns. However 
the majority of these models use abstract rate units and are yet to be translated into more realistic
spiking networks. Moreover  to provide a sufﬁciently large reservoir  the recurrent network needs
to be large  balanced and have a rich and high dimensional dynamics. This typically generates far
more activity than strictly required  a redundancy that can be seen as inefﬁcient.
On the other hand  supervised learning models involving spiking neurons have essentially concen-
trated on the learning of precise spike sequences [8–10]. With some exceptions [10 11] these models
use feed-forward architectures [12]. In a balanced recurrent network with asynchronous  irregular
and highly variable spike trains  such as those found in cortex  the activity has been shown to be

1

chaotic [13  14]. This leads to spike timing being intrinsically unreliable  rendering a representation
of the trajectory by precise spike sequences problematic. Moreover  many conﬁgurations of spike
times may achieve the same goal [15].
Here we derive two local learning rules that drive a network of leaky integrate-and-ﬁre (LIF) neu-
rons into implementing a desired linear dynamical system. The network is trained to minimize the
objective (cid:107)x(t) − ˆx(t)(cid:107)2 + H(r)  Where ˆx(t) is the output of the network decoded from the spikes 
x(t) is the desired output  and H(r) is a cost associated with ﬁring (penalizing unnecessary activ-
ity  and thus enforcing efﬁciency). The dynamical system is linear  ˙x = Ax + c  with A being
a constant matrix and c a time varying command signal. We ﬁrst study the learning of an autoen-
coder  i.e.  a network where the desired output is fed to the network as a feedforward input. The
autoencoder learns to represent its inputs as precisely as possible in an unsupervised fashion. After
learning  each unit represents the encoding error made by the entire network. We then show that
the network can learn more complex computations if slower recurrent connections are added to the
autoencoder. Thus  it receives the command c along with an error signal and learns to generate the
output ˆx with the desired temporal dynamics. Despite the spike-based nature of the representation
and of the plasticity rules  the learning does not enforce precise spike timing trajectories but  on the
contrary  enforces irregular and highly variable spike trains.

1 Learning a balance : global becomes local

Using a predictive coding strategy [15–17]  we build a network that learns to accurately represent its
inputs while expending the least amount of spikes. To introduce the learning rules and explain how
they work  we start by describing the optimized network (after learning).
Let us ﬁrst consider a set of unconnected integrate-and-ﬁre neurons receiving shared input signals
x = (xi) through feedforward connections F = (Fji). We assume that the network performs predic-
tive coding  i.e. it subtracts from each of these input signals an estimate ˆx obtained by decoding the

output spike trains (ﬁg 1A). Speciﬁcally  ˆxi =(cid:80) Dijrj  where D = (Dij) are the decoding weights
and r = (rj) are the ﬁltered spike trains which obey ˙rj = −λrj + oj with oj(t) =(cid:80)

k δ(t − tk
j )
being the spike train of neuron j and tk
j are the times of its spikes. Note that such an autoencoder
automatically maintains an accurate representation  because it responds to any encoding error larger
than the ﬁring threshold by increasing its response and in turn decreasing the error. It is also efﬁ-
cient  because neurons respond only when input and decoded signals differ. The autoencoder can be
equivalently implemented by lateral connections  rather than feedback targeting the inputs (Fig 1A).
These lateral connections combine the feedforward connections and the decoding weights and they
subtract from the feedforward inputs received by each neuron. The membrane potential dynamics
in this recurrent network are described by:

˙V = −λV + Fs + Wo

(1)
where V is the vector of the membrane potentials of the population  s = ˙x + λx is the effective
input to the population  W = −FD is the connectivity matrix  and o is the population vector of the
spikes. Neuron i has threshold Ti = (cid:107)Fi(cid:107)2/2 [15]. When input channels are independent and the
feed-forward weights are distributed uniformly on a sphere then the optimal decoding weights D are
equal to the encoding weights F and hence the optimal recurrent connectivity W = −FFT [17].
In the following we assume that this is always the case and we choose the feedforward weights
accordingly.
In this auto-encoding scheme having a precise representation of the inputs is equivalent to main-
taining a precise balance between excitation and inhibition. In fact  the membrane potential of a
neuron is the projection of the global error of the network on the neurons’s feedforward weight
(Vi = Fi(x − ˆx) [15]). If the output of the network matches the input  the recurrent term in the
membrane potential  Fi ˆx  should precisely cancel the feedforward term Fix. Therefore  in order to
learn the connectivity matrix W  we tackle the problem through balance  which is its physiological
characterization. The learning rule that we derive achieves efﬁcient coding by enforcing a precise
balance at a single neuron level. It makes the network converge to a state where each presynaptic
spike cancels the recent charge that was accumulated by the postsynaptic neuron (Fig 1B). This
accumulation of charge is naturally represented by the postsynaptic membrane potential Vi  which
jumps upon the arrival of a presynaptic spike by a magnitude given by the recurrent weight Wij due

2

Figure 1: A: a network preforming predictive coding. Top panel: a set of unconnected leaky
integrate-and-ﬁre neurons receiving the error between a signal and their own decoded spike trains.
Bottom panel: the previous architecture is equivalent to the recurrent network with lateral connec-
tions equal to the product of the encoding and the decoding weights. B: illustration of the learning
of an inhibitory weight. The trace of the membrane potential of a postsynaptic neuron is shown in
blue and red. The blue lines correspond to changes due to the integration of the feedforward input 
and the red to changes caused by the integration of spikes from neurons in the population. The black
line represents the resting potential of the neuron. In the left panel the presynaptic spike perfectly
cancels the accumulated feedforward current during a cycle and therefore there is no learning. In the
right panel the inhibitory weight is too strong and thus creates imbalance in the membrane potential.
Therefore  it is depressed by learning. C: learning in a 20-neuron network. Top panels: the two
dimensions of the input (blue lines) and the output (red lines) before (left) and after (right) learning.
Bottom panels: raster plots of the spikes in the population. D  left panel: after learning each neuron
receives a local estimate of the output of the network through lateral connections (red arrows). Right
panel: scatter plot of the output of the network projected on the feedforward weights of the neurons
versus the recurrent input they receive. E: the evolution of the mean error between the recurrent
weights of the network and the optimal recurrent weights −FFT using the rule deﬁned by equation
2 (black line) and the rule in [16] (gray line). Note that our rule is different from [16] because it
operates on a ﬁner time-scale and reaches the optimal balanced state with more than one order of
magnitude faster. This speed-up is important because  as we will see below  some computations
require a very fast restoration of this balance.

to the instantaneous nature of recurrent synapses. Because the two charges should cancel each other 
the greedy learning rule is proportional to the sum of both quantities:

µ(cid:80)

δWij ∝ −(Vi + βWij)

(2)
where Vi is the membrane potential of the postsynaptic neuron  Wij is the recurrent weight from
neuron j to neuron i  and the factor β controls the overall magnitude of lateral weights. More
importantly  β regularizes the cost penalizing the total spike count in the population (i.e. H(r) =
i ri where µ is the effective linear cost [15]). The example of an inhibitory synapse Wij < 0
is illustrated in ﬁgure 1B. If neuron i is too hyperpolarized upon the arrival of a presynaptic spike
from neuron j  i.e.  if the inhibitory weight Wij is smaller than −Vi/β  the absolute weight of

3

W=0W>0+ - ˆxxWf=FDˆxFDFD-20020-20020100020003000400010201000200030004000100020003000400010201000200030004000Neuronindexx1 ˆx1x2 ˆx2BeforeAfterFiˆxWirWir=FiˆxFixBalancedUnbalancedABCDE10110210310-5100EWTime(s)Vpost200msxthe synapse (the amplitude of the IPSP) is decreased. The opposite occurs if the membrane is too
depolarized. The synaptic weights thus converge when the two quantities balance each other on
average Wij = −(cid:104)Vi(cid:105)tj /β  where tj are the spike times of the presynaptic neuron j.
Fig 1C shows the learning in a 20-neuron network receiving random input signals. For illustration
purposes the weights are initialized with very small values. Before learning  the lack of lateral
connectivity causes neurons to ﬁre synchronously and regularly. After learning  spike trains are
sparse  irregular and asynchronous  despite the quasi absence of noise in the network. Even though
the ﬁring rates decrease globally  the quality of the input representation drastically improves over
the course of learning. Moreover  the convergence of recurrent weights to their optimal values is
typically quick and monotonic (Fig 1E).
By enforcing balance  the learning rule establishes an efﬁcient and reliable communication between
neurons. Because V = Fx − FFT r = F(x − ˆx)  every neuron has access - through its recurrent
input - to the network’s global coding error projected on its feedforward weight (Fig 1D). This local
representation of the network’s global performance is crucial in the supervised learning scheme we
describe in the following sections.

2 Generating temporal dynamics within the network

While in the previous section we presented a novel rule that drives a spiking network into efﬁciently
representing its inputs  we are generally interested in networks that perform more complex compu-
tations. It has been shown already that a network having two synaptic time scales can implement an
arbitrary linear dynamical system [15]. We brieﬂy summarize this approach in this section.

Figure 2: The construction of a recurrent network that implements a linear dynamical system.

In the autoencoder presented above  the effective input to the network is s = ˙x + λx (Fig 2A). We
assume that x follows linear dynamics ˙x = Ax + c  where A is a constant matrix and c(t) is a time
varying command. Thus  the input can be expanded to s = Ax + c + λx = (A + λI)x + c (Fig
2B). Because the output of the network ˆx approximates x very precisely  they can be interchanged.
According to this self-consistency argument  the external input term (A + λI)x is replaced by
(A + λI)ˆx which only depends on the activity of the network (Fig 2C). This replacement amounts
to including a global loop that adds the term (A + λI)ˆx to the source input (Fig 2D). As in the
autoencoder  this can be achieved using recurrent connections in the form of F(A + λI)FT (Fig
2E). Note that this recurrent input is the ﬁltered spike train r  not the raw spikes o. As a result  these
new connections have slower dynamics than the connections presented in the ﬁrst section. This
motivates us to characterize connections as fast and slow depending on their underlying dynamics.
The dynamics of the membrane potentials are now described by:

˙V = −λV V + Fc + Wsr + Wf o

(3)

4

˙x+x(A+I)ˆxˆxˆxˆxAx+xAˆx+ˆx+c+cccˆxFFTFFTWfWsDECBAwhere λV is the leak in the membrane potential  which is different from the leak in the decoder λ. It
is clear from the previous construction that the slow connectivity Ws = F(A + λI)FT   is involved
in generating the temporal dynamics of x. Owing to the slow connections  the network is able to
generate autonomously the temporal dynamics of the output and thus  only needs the command
c as an external input. For example  if A = 0 (i.e.
the network implements a pure integrator) 
Ws = λFFT compensates for the leak in the decoder by generating a positive feedback term that
prevents the activity form decaying. On the other hand  the fast connectivity matrix Wf = −FFT  
trained with the unsupervised  voltage-based rule presented previously  plays the same role as in the
autoencoder; It insures that the global output and the global coding error of the network are available
locally to each neuron.

3 Teaching the network to implement a desired dynamical system

Our aim is to develop a supervised learning scheme where a network learns to generate a desired
output with an error feedback as well as a local learning rule. The learning rule targets the slow
recurrent connections responsible for the generation of the temporal dynamics in the output  as seen
in the previous section. Instead of deriving directly the learning rule for the recurrent connections 
we ﬁrst derive a learning rule for the matrix A of the linear dynamical system using simple results
from control theory  and then we translate the learning to the recurrent network.

3.1

learning a linear dynamical system online

Consider the linear dynamical system ˙ˆx = Mˆx + c where M is a matrix. We derive an online
learning rule for the coefﬁcients of the matrix M  such that the output ˆx becomes after learning
equal to the desired output x. The latter undergoes the dynamics ˙x = Ax + c. Therefore  we deﬁne
e = x − ˆx as the error vector between the actual and the desired output. This error is fed to the
mistuned system in order to correct and “guide” its behavior (Fig 3A). Thus  the dynamics of the
system with this feedback are ˙ˆx = Mˆx + c + K(x − ˆx)  where K is a scalar implementing the gain
of the loop. The previous equation can be rewritten in the following form:

˙ˆx = (M − KI)ˆx + c + Kx

(4)
where I is the identity matrix. If we assume that the spectra of the signals are bounded  it is straight-
forward to show  via a Laplace transform  that ˆx → x when K → +∞. The larger the gain of the
feedback  the smaller the error. Intuitively  if K is large  very small errors are immediately detected
and therefore  corrected by the system. Nevertheless our aim is not to correct the dynamical system
forever  but to teach it to generate the desired output itself without the error feedback. Thus  the
matrix M needs to be modiﬁed over time. To derive the learning rule for the matrix M  we operate
a gradient descent on the loss function L = eT e = (cid:107)x − ˆx(cid:107)2 with respect to the components of the
matrix. The component Mij is updated proportionally to the gradient of L 

δMij ∝ −

∂L
∂Mij ∝ (

∂ˆx
∂Mij

)T e

(5)

To evaluate the term ∂ˆx/∂Mij  we solve the equation 4 for the simple case were inputs c are con-
stant. If we assume that K is much larger than the eigenvalues of M  the gradient ∂ˆx/∂Mij is
approximated by Eij ˆx  where Eij is a matrix of zeros except for component ij which is one. This
leads to the very simple learning rule δMij ≈ ˆxjei  which we can write in matrix form as:

(6)
The learning rule is simply the outer product of the output and the error. To derive the learning rule
we assume constant or slowly varying input. In practice  however  learning can be achieved also
using fast varying inputs (Fig 3).

δM ∝ eˆxT

3.2 Learning rule for the slow connections

In the previous section we derived a simple learning rule for the state matrix M of a linear dynamical
system. We translate this learning scheme into the recurrent network described in section 2. To this
end  we need to determine two things. First  we have to deﬁne the form of the error feedback in the

5

recurrent network case. Second  we need to adapt the learning rule of the matrix of the underlying
dynamical system to the slow weights of the recurrent neural network.
In the previous learning scheme the error is fed into the dynamical system as an additional input.
Since the input weight vector of a neuron Fi deﬁnes the direction that is relevant for its “action”
space  the neuron should only receive the errors in that direction. Thus  the error vector is projected
on the feedforward weights vector of a neuron before being fed to it. Accordingly  equation 3
becomes:

˙V = −λV V + Fc + Wsr + Wf o + KFe

(7)
In the autoencoder  the membrane potential of a neuron represents the error between the input and
the output of the entire network along the neuron’s feedforward weight. With the addition of the
dynamic error feedback and the slow connections  the membrane potentials now represent the error
between the actual and the desired network output trajectories.
To translate the learning rule of the dynamical system into a rule for the recurrent network  we as-
sume that any modiﬁcation of the recurrent weights directly reﬂects a modiﬁcation in the underlying
dynamical system. This is achieved if the updates δWs of the slow connectivity matrix are in the
form of F(δM)FT . This ensures that the network always implements a linear dynamical system and
guarantees that the analysis is consistent. The learning rule of the slow connections Ws is obtained
by replacing δM by its expression according to equation 6 in F(δM)FT :

δWs ∝ (Fe)(Fˆx)T

(8)
According to this learning rule  the weight update between two neurons  δW s
ij  is proportional to
the error feedback Fie received as a current by the postsynaptic neuron i and to Fj ˆx  the output of
the network projected on the feedforward weight of the presynaptic neuron j. The latter quantity is
available to the presynaptic neuron through its inward fast recurrent connections  as shown for the
autoencoder in Fig 1D.
One might object that the previous learning rule is not biologically plausible because it involves
currents present separately in the pre- and post-synaptic neurons. Indeed  the presynaptic term may
not be available to the synapse. However  as shown in the supplementary information of [15]  the
ﬁltered spike train rj of the presynaptic neuron is approximately proportional to (cid:98)Fj ˆx(cid:99)+  a rectiﬁed
version of the presynaptic term in the previous learning rule. By replacing Fj ˆx by rj in the equation
8 we obtain the following biologically plausible learning rule:

δW s

ij = Eirj

(9)

Where Ei = Fie is the total error current received by the postsynaptic neuron.

3.3 Learning the underlying dynamical system while maintaining balance

For the previous analysis to hold  the fast connectivity Wf should be learned simultaneously with
the slow connections using the learning rule deﬁned by equation 2. As shown in the ﬁrst section 
the learning of the fast connections establishes a detailed balance on the level of the neuron and
guarantees that the output of the network is available to each neuron through the term Fj ˆx. The
latter is the presynaptic term in the learning rule of equation 8. Despite not being involved in the
dynamics per se  these fast connections are crucial in order to learn any temporal dynamics. In other
words  learning a detailed balance is a pre-requirement to learn dynamics with local plasticity rules
in a spiking network. The plasticity of the fast connections remediate very quickly any perturbation
to the balance caused by the learning of the slow connections.

3.4 Simulation

As a toy example  we simulated a 20-neuron network learning a 2D-damped oscillator using a feed-
back gain K = 100. The network is initialized with weak fast connections and weak slow con-
nections. The learning is driven by smoothed gaussian noise as the command c. Note that in the
initial state there are no fast recurrent connection and the output of the network does not depend
linearly on the input because membrane potentials are too hyperpolarized (Fig 3B). The network’s
output is quickly linearized through the learning of the fast connections (equation 2) by enforcing a
balance on the membrane potential (Fig 3B): initial membrane potentials exhibit large ﬂuctuations

6

Figure 3: Learning temporal dynamics in a recurrent network. A  top panel: the linear dynamical
system characterized by the state matrix M receives feedback signaling the difference between its
actual output and a desired output. Bottom panel: a recurrent network displaying slow and fast
connections is equivalent to the top architecture if the error feedback is fed into the network through
the feedforward matrix F. B: a 20 neuron network learns using equations 9 and 2. Left panel: the
evolution of the error between the desired and the actual output during learning. The black and
grey arrows represent instances where the time course of the membrane potential is shown in the
next plot. Right panel: the time course of the membrane potential of one neuron at two different
instances during learning. The gray line corresponds to the initial state while the black line is a few
iterations after. C: scatter plots of the learned versus the predicted weights at the end of learning for
fast (top panel) and slow (bottom panel) connections. D  top panels: the output of the network (red)
and the desired output (blue)  before (left) and after (right) learning. The black solid line on the top
shows the impulse command that drives the network. Bottom panels: raster plots before and after
learning. In the left raster plot there is no spiking activity after the ﬁrst 50 ms.

that wane drastically after a few iterations (Fig 3B). On a slower time scale the slow connections
learn to minimize the prediction error using the learning rule of equation 9. The error between the
output of the network and the desired output decreases drastically (Fig 3B). To compute this error 
different instances of the connectivity matrices were sampled during learning. The network was
then re-simulated using the same instances while ﬁxing K=0 to mesure the performance in the ab-
sence of feedback. At the end of learning  both slow and fast connections converge to their predicted
values Ws = F(A + λI)FT and Wf = −FFT (Fig 3C). The presence of feedback is no longer
required for the network to have the right dynamics (i.e. if we set K = 0 we still obtain the desired
output  see Figs. 3D and 3B). The output of the network is very accurate (representing the state x
with a precision of the order of the contribution of a single spike)  parsimonious (no unnecessary
spikes are emitted to represent the dynamical state at this level of accuracy) and the spike trains are
asynchronous and irregular. Note that because the slow connections are weak at the initial state 
spiking activity decays quickly once the command impulse is turned off  due to the absence of slow
recurrent excitation (Fig 3D).

Simulation parameters Figure 1 : λ = 0.05  β = 0.51  learning rate: 0.01. Figure 3 : λ = 50 
λV = 1  β = 0.52  K = 100  learning rate of the fast connections: 0.03  learning rate of the slow
connections: 0.15.

7

McKe+ + - + cˆxxKe+ - FFFTACDM.P.100102104048-8-40150ms500010000150001102050001000015000Neuronindexx1 ˆx1x2 ˆx2-101-101-1020102-1020102300msPredictedLearnedLearnedWfWs100102104048-8-40BErrorTime(s)ˆxx4 Discussion

Using a top-down approach we derived a pair of spike-based and current-based plasticity rules that
enable precise supervised learning in a recurrent network of LIF neurons. The essence of this ap-
proach is that every neuron is a precise computational unit that represents the network error in a
subspace of dimension 1 in the output space. The precise and distributed nature of this code allows
the derivation of local learning rules from global objectives.
To compute collectively  the neurons need to communicate to each other about their contributions to
the output of the network. The fast connections are trained in an unsupervised fashion using a spike-
based rule to optimize this communication. It establishes this efﬁcient communication by enforcing
a detailed balance between excitation and inhibition. The slow connections however are trained to
minimize the error between the actual output of the network and a target dynamical system. They
produce currents with long temporal correlations implementing the temporal dynamics of the under-
lying linear dynamical system. The plasticity rule for the slow connections is simply proportional
to an error feedback injected as a current in the postsynaptic neuron  and to a quantity akin to the
ﬁring rate of the presynaptic neuron. To guide the behavior of the network during learning  the error
feedback must be strong and speciﬁc. Such strength and specialization is in agreement with data
on climbing ﬁbers in the cerebellum [18–20]  which are believed to bring information about errors
during motor learning [21]. However  in this model  the speciﬁcity of the error signals are deﬁned
by a weight matrix through which the errors are fed to the neurons. Learning these weights is still
under investigation. We believe that they could be learned using a covariance-based rule.
Our approach is substantially different from usual supervised learning paradigms in spiking net-
works since it does not target the spike times explicitly. However  observing spike times may be
misleading since there are many combinations that can produce the same output [15  16]. Thus  in
this framework  variability in spiking is not a lack of precision  but is the consequence of the redun-
dancy in the representation. Neurons having similar decoding weights may have their spike times
interchanged while the global representation is conserved. What is important  is the cooperation
between the neurons and the precise spike timing relative to the population. For example  using in-
dependent Poisson neurons with instantaneous ﬁring rates identical to the predictive coding network
drastically degrades the quality of the representaion [15].
Our approach is also different from liquid computing in the sense that the network is small  struc-
tured  and ﬁres only when needed.
In addition  in these studies the feedback error used in the
learning rule has no clear physiological correlate  while here it is concretely injected as a current in
the neurons. This current is used simultaneously to drive the learning rule and to guide the dynamics
of the neuron in the short term. However  it is still unclear what the mechanisms are that could
implement such a current dependent learning rule in biological neurons.
An obvious limitation of our framework is that it is currently restricted to linear dynamical systems.
One possibility to overcome this limitation would be to introduce non-linearities in the decoder 
which would translate into speciﬁc non-linearities and structures in the dendrites. A similar strategy
has been employed recently to combine the approach of predictive coding and FORCE learning [7]
using two compartment LIF neurons [22]. We are currently exploring less constraining forms of
synaptic non-linearities  with the ultimate goal of being able to learn arbitrary dynamics in spiking
networks using purely local plasticity rules.

Acknowledgments

This work was supported by ANR-10-LABX-0087 IEC  ANR-10-IDEX-0001-02 PSL  ERC grant
FP7-PREDISPIKE and the James McDonnell Foundation Award - Human Cognition.

Refrences

[1] Kawato  M. (1999). Internal models for motor control and trajectory planning. Current opinion

in neurobiology  9(6)  718-727.

[2] Lackner  J. R.  & Dizio  P. (1998). Gravitoinertial force background level affects adaptation to
coriolis force perturbations of reaching movements. Journal of neurophysiology  80(2)  546-
553.

8

[3] Rumelhart  D. E.  Hinton  G. E.  & Williams  R. J. (1988). Learning representations by back-

propagating errors. Cognitive modeling  5.

[4] Williams  R. J.  & Zipser  D. (1989). A learning algorithm for continually running fully recur-

rent neural networks. Neural computation  1(2)  270-280.

[5] Jaeger  H. (2001). The echo state approach to analysing and training recurrent neural networks-
with an erratum note. Bonn  Germany: German National Research Center for Information
Technology GMD Technical Report  148  34.

[6] Maass  W.  Natschlger  T.  & Markram  H. (2002). Real-time computing without stable states:
A new framework for neural computation based on perturbations. Neural computation  14(11) 
2531-2560.

[7] Sussillo  D.  & Abbott  L. F. (2009). Generating coherent patterns of activity from chaotic

neural networks. Neuron  63(4)  544-557.

[8] Legenstein  R.  Naeger  C.  & Maass  W. (2005). What can a neuron learn with spike-timing-

dependent plasticity?. Neural computation  17(11)  2337-2382.

[9] Pﬁster  J.  Toyoizumi  T.  Barber  D.  & Gerstner  W. (2006). Optimal spike-timing-dependent
plasticity for precise action potential ﬁring in supervised learning. Neural computation  18(6) 
1318-1348.

[10] Ponulak  F.  & Kasinski  A. (2010). Supervised learning in spiking neural networks with Re-
SuMe: sequence learning  classiﬁcation  and spike shifting. Neural Computation  22(2)  467-
510.

[11] Memmesheimer  R. M.  Rubin  R.  lveczky  B. P.  & Sompolinsky  H. (2014). Learning pre-

cisely timed spikes. Neuron  82(4)  925-938.

[12] G¨utig  R.  & Sompolinsky  H. (2006). The tempotron: a neuron that learns spike timingbased

decisions. Nature neuroscience  9(3)  420-428.

[13] van Vreeswijk  C.  & Sompolinsky  H. (1996). Chaos in neuronal networks with balanced

excitatory and inhibitory activity. Science  274(5293)  1724-1726.

[14] Brunel  N. (2000). Dynamics of networks of randomly connected excitatory and inhibitory

spiking neurons. Journal of Physiology-Paris  94(5)  445-463.

[15] Boerlin  M.  Machens  C. K.  & Den`eve  S. (2013). Predictive coding of dynamical variables

in balanced spiking networks. PLoS computational biology  9(11)  e1003258.

[16] Bourdoukan  R.  Barrett  D.  Machens  C. K & Den`eve  S. (2012). Learning optimal spike-
based representations. In Advances in neural information processing systems (pp. 2285-2293).
[17] Vertechi  P.  Brendel  W.  & Machens  C. K. (2014). Unsupervised learning of an efﬁcient
short-term memory network. In Advances in Neural Information Processing Systems (pp. 3653-
3661).

[18] Watanabe  M.  & Kano  M. (2011). Climbing ﬁber synapse elimination in cerebellar Purkinje

cells. European Journal of Neuroscience  34(10)  1697-1710.

[19] Chen  C.  Kano  M.  Abeliovich  A.  Chen  L.  Bao  S.  Kim  J. J.  ... & Tonegawa  S. (1995).
Impaired motor coordination correlates with persistent multiple climbing ﬁber innervation in
PKC mutant mice. Cell  83(7)  1233-1242.

[20] Eccles  J. C.  Llinas  R.  & Sasaki  K. (1966). The excitatory synaptic action of climbing ﬁbres

on the Purkinje cells of the cerebellum. The Journal of Physiology  182(2)  268-296.

[21] Knudsen  E. I. (1994). Supervised Learning in the Brain. The Journal of Neuroscience 14(7) 

39853997.

[22] Thalmeier  D.  Uhlmann  M.  Kappen  H.J.  & Memmesheimer  R. (2015) Learning universal

computations with spikes. arXiv preprint arXiv:1505.07866.

9

,Nagarajan Natarajan
Inderjit Dhillon
Pradeep Ravikumar
Ambuj Tewari
Ralph Bourdoukan
Sophie Denève
Sepehr Assadi
Eric Balkanski
Renato Leme