2019,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels,While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees  their practical performances are limited by their expressive power  as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels  graph neural networks (GNNs) usually achieve better practical performance  as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However  due to the large number of hyper-parameters and the non-convex nature of the training procedure  GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore  the expressive power of GNNs scales with the number of parameters  and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels  Graph Neural Tangent Kernels (GNTKs)  which correspond to \emph{infinitely wide} multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically  we show GNTKs provably learn a class of smooth functions on graphs. Empirically  we test GNTKs on graph classification datasets and show they achieve strong performance.,Graph Neural Tangent Kernel:

Fusing Graph Neural Networks with Graph Kernels

Simon S. Du

Institute for Advanced Study

ssdu@ias.edu

Kangcheng Hou
Zhejiang University

kangchenghou@gmail.com

Barnabás Póczos

Carnegie Mellon University
bapoczos@cs.cmu.edu

Ruslan Salakhutdinov

Carnegie Mellon University
rsalakhu@cs.cmu.edu

Ruosong Wang

Carnegie Mellon University.
ruosongw@andrew.cmu.edu

Keyulu Xu

Massachusetts Institute of Technology

keyulu@mit.edu

Abstract

While graph kernels (GKs) are easy to train and enjoy provable theoretical guar-
antees  their practical performances are limited by their expressive power  as the
kernel function often depends on hand-crafted combinatorial features of graphs.
Compared to graph kernels  graph neural networks (GNNs) usually achieve better
practical performance  as GNNs use multi-layer architectures and non-linear acti-
vation functions to extract high-order information of graphs as features. However 
due to the large number of hyper-parameters and the non-convex nature of the
training procedure  GNNs are harder to train. Theoretical guarantees of GNNs are
also not well-understood. Furthermore  the expressive power of GNNs scales with
the number of parameters  and thus it is hard to exploit the full power of GNNs
when computing resources are limited. The current paper presents a new class
of graph kernels  Graph Neural Tangent Kernels (GNTKs)  which correspond to
inﬁnitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the
full expressive power of GNNs and inherit advantages of GKs. Theoretically  we
show GNTKs provably learn a class of smooth functions on graphs. Empirically 
we test GNTKs on graph classiﬁcation datasets and show they achieve strong per-
formance.

1

Introduction

Learning on graph-structured data such as social networks and biological networks requires one to
design methods that effectively exploit the structure of graphs. Graph Kernels (GKs) and Graph Neu-
ral Networks (GNNs) are two major classes of methods for learning on graph-structured data. GKs 
explicitly or implicitly  build feature vectors based on combinatorial properties of input graphs. Pop-
ular choices of GKs include Weisfeiler-Lehman subtree kernel [Shervashidze et al.  2011]  graphlet
kernel [Shervashidze et al.  2009] and random walk kernel [Vishwanathan et al.  2010  Gärtner et al. 
2003]. GKs inherit all beneﬁts of kernel methods. GKs are easy to train  since the correspond-
ing optimization problem is convex. Moreover  the kernel function often has explicit expressions 
and thus we can analyze their theoretical guarantees using tools in learning theory. The downside
of GKs  however  is that hand-crafted features may not be powerful enough to capture high-order

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

information that involves complex interaction between nodes  which could lead to worse practical
performance than GNNs.
GNNs  on the other hand  do not require explicitly hand-crafted feature maps. Similar to convo-
lutional neural networks (CNNs) which are widely applied in computer vision  GNNs use multi-
layer structures and convolutional operations to aggregate local information of nodes  together with
non-linear activation functions to extract features from graphs. Various architectures have been pro-
posed [Xu et al.  2019a  2018]. GNNs extract higher-order information of graphs  which lead to
more powerful features compared to hand-crafted combinatorial features used by GKs. As a result 
GNNs have achieved state-of-the-art performance on a large number of tasks on graph-structured
data. Nevertheless  there are also disadvantages of using GNNs. The objective function of GNNs
is highly non-convex  and thus it requires careful hyper-parameter tuning to stabilize the training
procedure. Meanwhile  due to the non-convex nature of the training procedure  it is also hard to
analyze the learned GNNs directly. For example  one may ask whether GNNs can provably learn
certain class of functions. This question seems hard to answer given our limited theoretical under-
standing of GNNs. Another disadvantage of GNNs is that the expressive power of GNNs scales
with the number of parameters. Thus  it is hard to learn a powerful GNN when computing resources
are limited. Can we build a model that enjoys the best of both worlds  i.e.  a model that extracts
powerful features as GNNs and is easy to train and analyze like GKs?
In this paper  we give an afﬁrmative answer to this question. Inspired by recent connections between
kernel methods and over-parameterized neural networks [Arora et al.  2019b a  Du et al.  2019  2018 
Jacot et al.  2018  Yang  2019]  we propose a class of new graph kernels  Graph Neural Tangent
Kernels (GNTKs). GNTKs are equivalent to inﬁnitely wide GNNs trained by gradient descent  where
the word “tangent” corresponds to the training algorithm — gradient descent. While GNTKs are
induced by inﬁnitely wide GNNs  the prediction of GNTKs depends only on pairwise kernel values
between graphs  for which we give an analytic formula to calculate efﬁciently. Therefore  GNTKs
enjoy the full expressive power of GNNs  while inheriting beneﬁts of GKs.

Our Contributions. First  inspired by recent connections between over-parameterized neural net-
works and kernel methods Jacot et al. [2018]  Arora et al. [2019a]  Yang [2019]  we present a gen-
eral recipe which translates a GNN architecture to its corresponding GNTK. This recipe works for
a wide range of GNNs  including graph isomorphism network (GIN) [Xu et al.  2019a]  graph con-
volutional network (GCN) [Kipf and Welling  2016]  and GNN with jumping knowledge [Xu et al. 
2018]. Second  we conduct a theoretical analysis of GNTKs. Using the technique developed in
Arora et al. [2019b]  we show for a broad range of smooth functions over graphs  a certain GNTK
can learn them with polynomial number of samples. To our knowledge  this is the ﬁrst sample com-
plexity analysis in the GK and GNN literature. Finally  we validate the performance of GNTKs on 7
standard benchmark graph classiﬁcation datasets. On four of them  we ﬁnd GNTK outperforms all
baseline methods and achieves state-of-the-art performance. In particular  GNKs achieve 83.6% ac-
curacy on COLLAB dataset and 67.9% accuracy on PTC dataset  compared to the best of baselines 
81.0% and 64.6% respectively. Moreover  in our experiments  we also observe that GNTK is more
computationally efﬁcient than its GNN counterpart.
This paper is organized as follow. In Section 2  we provide necessary background and review op-
erations in GNNs that we will use to derive GNTKs. In Section 3  we present our general recipe
that translates a GNN to its corresponding GNTK. In Section 4  we give our theoretical analysis
of GNTKs. In Section 5  we compare GNTK with state-of-the-art methods on graph classiﬁcation
datasets. We defer technical proofs to the supplementary material.

2 Preliminaries

We begin by summarizing the most common models for learning with graphs and  along the way 
introducing our notation. Let G = (V  E) be a graph with node features hv ∈ Rd for each v ∈ V .
We denote the neighborhood of node v by N (v). In this paper  we consider the graph classiﬁcation
task  where  given a set of graphs {G1  ...  Gn} ⊆ G and their labels {y1  ...  yn} ⊆ Y  our goal is to
learn to predict labels of unseen graphs.
Graph Neural Network. GNN is a powerful framework for graph representation learning. Modern
GNNs generally follow a neighborhood aggregation scheme Xu et al. [2019a]  Gilmer et al. [2017] 

2

Xu et al. [2018]  where the representation h(ℓ)
v of each node v (in layer ℓ) is recursively updated by
aggregating and transforming the representations of its neighbors. After iterations of aggregation 
the representation of an entire graph is then obtained through pooling  e.g.  by summing the rep-
resentations of all nodes in the graph. Many GNNs  with different aggregation and graph readout
functions  have been proposed under the neighborhood aggregation framework Xu et al. [2019a b 
2018]  Scarselli et al. [2009]  Li et al. [2016]  Kearnes et al. [2016]  Ying et al. [2018]  Velickovic
et al. [2018]  Hamilton et al. [2017]  Duvenaud et al. [2015]  Kipf and Welling [2016]  Defferrard
et al. [2016]  Santoro et al. [2018  2017]  Battaglia et al. [2016].
Next  we formalize the GNN framework. We refer to the neighbor aggregation process as a BLOCK
operation  and to graph-level pooling to as a READOUT operation.
BLOCK Operation. A BLOCK operation aggregates features over a neighborhood N (u) ∪ {u}
via  e.g.  summation  and transforms the aggregated features with non-linearity  e.g. multi-layer
perceptron (MLP) or a fully-connected layer followed by ReLU. We denote the number of fully-
connected layers in each BLOCK operation  i.e.  the number of hidden layers of an MLP  by R.
When R = 1  the BLOCK operation can be formulated as

r

· σ

cσ
m

0@Wℓ · cu

X

h(ℓ−1)

v

v∈N (u)∪{u}

1A .

BLOCK(ℓ)(u) =

Here  Wℓ are learnable weights  initialized as Gaussian random variables. σ is an activation function
like ReLU. m is the output dimension of Wℓ. We set the scaling factor cσ to 2  following the
initialization scheme in He et al. [2015]. cu is a scaling factor for neighbor aggregation. Different
GNNs often have different choices for cu. In Graph Convolution Network (GCN) [Kipf and Welling 
2016]  cu =
|N (u)|+1  and in Graph Isomorphism Network (GIN) [Xu et al.  2019a]  cu = 1  which
correspond to averaging and summing over neighbor features  respectively.
When the number of fully-connected layers R = 2  the BLOCK operation can be written as

1

r

0@Wℓ 2

r

cσ
m

σ

· σ

cσ
m

0@Wℓ 1 · cu

X

h(ℓ−1)

v

v∈N (u)∪{u}

1A1A  

BLOCK(ℓ)(u) =

where Wℓ 1 and Wℓ 2 are learnable weights. Notice that here we ﬁrst aggregate features over neigh-
borhood N (u) ∪ {u} and then transforms the aggregated features with an MLP with R = 2 hidden
layers. BLOCK operations can be deﬁned similarly for R > 2. Notice that the BLOCK operation
we deﬁned above is also known as the graph (spatial) convolutional layer in the GNN literature.

READOUT Operation. To get the representation of an entire graph hG after L steps of aggrega-
tion  we take the summation over all node features  i.e. 

hG = READOUT

u   u ∈ V
h(L)

(cid:16)n

o(cid:17)

X

h(L)
u .

=

u∈V

There are more sophisticated READOUT operations than a simple summation Xu et al. [2018] 
Zhang et al. [2018a]  Ying et al. [2018]. Jumping Knowledge Network (JK-Net) Xu et al. [2018]
considers graph structures of different granularity  and aggregates graph features across all layers as

i

(cid:16)n

o(cid:17)
u   u ∈ V  ℓ ∈ [L]
h(ℓ)

X

h

=

u∈V

hG = READOUTJK

h(0)
u ; . . . ; h(L)

u

.

o(cid:17)
Building GNNs using BLOCK and READOUT. Most modern GNNs are constructed using the
BLOCK operation and the READOUT operation Xu et al. [2019a]. We denote the number of
BLOCK operations (aggregation steps) in a GNN by L. For each ℓ ∈ [L] and u ∈ V   we deﬁne
h(ℓ)
u = BLOCK(ℓ)(u). The graph-level feature is then hG = READOUT
or

u   u ∈ V
h(L)

(cid:16)n

(cid:16)n

o(cid:17)

hG = READOUTJK
is applied or not.

u   u ∈ V  ℓ ∈ [L]
h(ℓ)

  depending on whether jumping knowledge (JK)

3

3 GNTK Formulas

In this section we present our general recipe which translates a GNN architecture to its corresponding
GNTK. We ﬁrst provide some intuitions on neural tangent kernels (NTKs). We refer readers to Jacot
et al. [2018]  Arora et al. [2019a] for more comprehensive descriptions.

Intuition of the Formulas

3.1
Consider a general neural network f (θ  x) ∈ R where θ ∈ Rm is all the parameters in the network
and x is the input. Given a training dataset {(xi  yi)n
}  consider training the neural network by
minimizing the squared loss over training data

i=1

nX

i=1

ℓ(θ) =

1
2

(f (θ  xi) − yi)2.

Suppose we minimize the squared loss ℓ(θ) by gradient descent with inﬁnitesimally small learning
rate  i.e.  dθ(t)
i=1 be the network outputs. u(t) follows the
evolution

dt = −∇ℓ(θ(t)). Let u(t) = (f (θ(t)  xi))n

where

(cid:28)

H(t)ij =

du
dt

= −H(t)(u(t) − y) 
(cid:29)

 

∂θ

∂θ

∂f (θ(t)  xi)

∂f (θ(t)  xj)

for (i  j) ∈ [n] × [n].

Recent advances in optimization of neural networks have shown  for sufﬁciently over-parameterized
neural networks  the matrix H(t) keeps almost unchanged during the training process Arora et al.
[2019b a]  Du et al. [2019  2018]  Jacot et al. [2018]  in which case the training dynamics is identical
to that of kernel regression. Moreover  under a random initialization of parameters  the random
matrix H(0) converges in probability to a certain deterministic kernel matrix  which is called Neural
Tangent Kernel (NTK) Jacot et al. [2018] and corresponds to inﬁnitely wide neural networks. See
Figure 4 in the supplementary material for an illustration.
Explicit formulas for NTKs of fully-connected neural networks have been given in Jacot et al. [2018].
Recently  explicit formulas for NTKs of convolutional neural networks are given in Arora et al.
[2019a]. The goal of this section is to give an explicit formula for NTKs that correspond to GNNs
deﬁned in Section 2. Our general strategy is inspired by Arora et al. [2019a]. Let f (θ  G) ∈ R be
the output of the corresponding GNN under parameters θ and input graph G  for two given graphs
′  to calculate the corresponding GNTK value  we need to calculate the expected value of
G and G

(cid:28)

(cid:29)

∂f (θ  G)

∂θ

 

∂f (θ  G

′

)

∂θ

in the limit that m → ∞ and θ are all Gaussian random variables  which can be viewed as a Gaussian
process. For each layer in the GNN  we use Σ to denote the covariance matrix of outputs of that
layer  and ˙Σ to denote the covariance matrix corresponds to the derivative of that layer. Due to the
multi-layer structure of GNNs  these covariance matrices can be naturally calculated via dynamic
programming.

3.2 Formulas for Calculating GNTKs
′

′

′

= (V

  E

) with |V | = n  |V

′ and a GNN with L BLOCK
Given two graphs G = (V  E)  G
operations and R fully-connected layers with ReLU activation in each BLOCK operation. We give
the GNTK formula of pairwise kernel value Θ(G  G
We ﬁrst deﬁne the covariance matrix between input features of two input graphs G  G
use Σ(0)(G  G
deﬁned to be h

) ∈ R induced by this GNN.
(cid:2)
′ ∈ V
) ∈ Rn×n
′ 
′
Σ(0)(G  G
′ ∈ V
u hu′  where hu and hu′ are the input features of u ∈ V and u
⊤

to denote. For two nodes u ∈ V and u

′  which we
uu′ is

′| = n

(cid:3)

′

)

′.

′

′

4

′

′

′

′

(R)

(G  G

BLOCK Operation. A BLOCK operation in GNTK calculates a covariance matrix
Σ(ℓ)
  and calculates intermediate kernel values
Θ(ℓ)

) ∈ Rn×n
′
(R)(G  G
) ∈ Rn×n
′
h
(r)(G  G
h

using Σ(ℓ−1)
  which will be later used to compute the ﬁnal output.
i
i

h
More speciﬁcally  we ﬁrst perform a neighborhood aggregation operation
h
Σ(ℓ−1)
Θ(ℓ−1)

) ∈ Rn×n
X
X

X
X

uu′ =cucu′

v′∈N (u′)∪{u′}

v∈N (u)∪{u}

(0)(G  G

i
i

vv′  

(G  G

(G  G

Θ(ℓ)

Σ(ℓ)

(R)

)

)

)

′

′

′

′

(0)(G  G

uu′ =cucu′

)

vv′ .

v∈N (u)∪{u}

v′∈N (u′)∪{u′}

(R)

′

′

′

(R)(G  G

(R)(G  G

) and Θ(0)

) as Σ(0)(G  G

Here we deﬁne Σ(0)
)  for notational convenience. Next we
perform R transformations that correspond to the R fully-connected layers with ReLU activation.
Here σ(z) = max{0  z} is the ReLU activation function. We denote ˙σ(z) = [z ≥ 0] to be the
derivative of the ReLU activation function.
For each r ∈ [R]  we deﬁne
• For u ∈ V  u
i
h

′ ∈ V

′ 

′

1A ∈ R2×2.

i
i

)

′

uu′

)

u′u′

h
0@
h
i
i

• For u ∈ V  u

A(ℓ)

(r) (G  G

′

)

uu′ =

Σ(ℓ)

(r−1)(G  G)
(r−1)(G

  G)

′

Σ(ℓ)

u u

uu′

′ h
′ ∈ V
h

′

′

)

)

h

Σ(ℓ)

(r)(G  G

˙Σ(ℓ)
(r) (G  G
′ 
′

i

uu′ =

uu′ =cσE
uu′ =cσE

(a b)∼N

0 

(a b)∼N

0 

Θ(ℓ)

(r−1)(G  G

′

)

uu′

h

• For u ∈ V  u
Θ(ℓ)

′ ∈ V

(r)(G  G

)

i
i

(
(
h

h
h

Σ(ℓ)

Σ(ℓ)

′

  G

(r−1)(G  G
(r−1)(G
)
)

uu′

[
[

]
(r)(G G′)
]
A(ℓ)
(r)(G G′)
A(ℓ)
i

uu′

′

h

[σ (a) σ (b)]  

[ ˙σ(a) ˙σ(b)] .

(1)

(2)

i

′

i

i

′

i

uu′

Note in the above we have shown how to calculate Θ(ℓ)
intermediate outputs will be used to calculate the ﬁnal output of the corresponding GNTK.

(R)(G  G

˙Σ(ℓ)

(r) (G  G
′

Σ(ℓ)

uu′ +

(r) (G  G

)
) for each ℓ ∈ {0  1  . . .   L}. These

uu′ .

)

READOUT Operation. Given these intermediate outputs  we can now calculate the ﬁnal output
of GNTK using the following formula.

Θ(G  G

′

) =

u∈V u′∈V ′
u∈V u′∈V ′

)

Θ(ℓ)
(R) (G  G
ℓ=0 Θ(ℓ)
L

uu′
(R)(G  G

′

)

without jumping knowledge

with jumping knowledge

.

8<:

P
P

h
hP

To better illustrate our general recipe  in Figure 1 we give a concrete example in which we translate
a GNN with L = 2 BLOCK operations  R = 1 fully-connection layer in each BLOCK operation 
and jumping knowledge  to its corresponding GNTK.

4 Theoretical Analysis of GNTK

In this section  we analyze the generalization ability of a GNTK that corresponds to a simple GNN.
We consider the standard supervised learning setup. We are given n training data {(Gi  yi)}n
drawn i.i.d. from the underlying distribution D  where Gi is the i-th input graph and yi is its label.
i=1
Consider a GNN with a single BLOCK operation  followed by the READOUT operation (without
. We use Θ ∈ Rn×n to denote
jumping knowledge). Here we set cu =
) is the kernel function that corresponds
the kernel matrix  where Θij = Θ(Gi  Gj). Here Θ(G  G

(cid:16)(cid:13)(cid:13)(cid:13)P

v∈N (u)∪{u} hv

(cid:17)−1

(cid:13)(cid:13)(cid:13)

′

2

5

h

h

i

′

)

Figure 1: Illustration of our recipe that translates a GNN to a GNTK. For a GNN with
L = 2 BLOCK operations  R = 1 fully-connected layer in each BLOCK operation  and jump-
′ 
ing knowledge  the corresponding GNTK is calculated as follow. For two graphs G and G
⊤
u hu′. We
we ﬁrst calculate
uu′ = h
uu′ =
  Θ(ℓ−1)
follow the kernel formulas in Section 3 to calculate Σ(ℓ)
(0)  Θ(ℓ)
(Aggre-
(r) using Σ(ℓ)
gation) and calculate Σ(ℓ)
(r−1)  Θ(ℓ)
(r−1) (Nonlinearity). The ﬁnal output is
uu′.
u∈V u′∈V ′
(R)(G  G

(cid:3)
(cid:2)
Σ(0)(G  G
(0) using Σ(ℓ−1)

(r)  Θ(ℓ)
ℓ=0 Θ(ℓ)

hP

(r)  ˙Σ(ℓ)

(1)(G  G

(1)(G  G

uu′ =

P

Θ(G  G

) =

Θ(0)

Σ(0)

i

′

)

(R)

(R)

i

′

)

′

L

′

)

to the simple GNN. See Section 3 for the formulas for calculating Θ(G  G
sion  we assume that the kernel matrix Θ ∈ Rn×n is invertible.
For a testing point Gte  the prediction of kernel regression using GNTK on this testing point is

). Throughout the discus-

′

fker(Gte) = [Θ(Gte  G1)  Θ(Gte  G1)  . . .   Θ(Gte  Gn)]

⊤

−1y.

Θ

The following result is a standard result for kernel regression proved using Rademacher complexity.
For a proof  see Bartlett and Mendelson [2002].
Theorem 4.1 (Bartlett and Mendelson [2002]). Given n training data {(Gi  yi)}n
i=1 drawn i.i.d.
from the underlying distribution D. Consider any loss function ℓ : R×R → [0  1] that is 1-Lipschitz
in the ﬁrst argument such that ℓ(y  y) = 0. With probability at least 1 − δ  the population loss of the
GNTK predictor can be upper bounded by

LD (fker) = E(G y)∼D [ℓ(fker(G)  y)] = O

 p

r

!

y⊤Θ−1y · tr (Θ)

n

+

log(1/δ)

n

.

Note that this theorem presents a data-dependent generalization bound which is related to the kernel
matrix Θ ∈ Rn×n and the labels {yi}n
−1y and
tr (Θ)  then we can obtain a concrete sample complexity bound. We instantiate this idea to study
the class of graph labeling functions that can be efﬁciently learned by GNTKs.
The following two theorems guarantee that if labels are generated as described in (3)  then the GNTK
that corresponds to the simple GNN described above can learn this function with polynomial number
of samples. We ﬁrst give an upper bound on y

i=1. Using this theorem  if we can bound y

−1y.

Θ

Θ

⊤

⊤

6

(cid:17)
∞X
Theorem 4.2. For each i ∈ [n]  if the labels {yi}n

X

(cid:16)

i=1 satisfy

X

(cid:16)

(cid:17)2l

 

where hu = cu
we have

⊤
u β1

h

+

u∈V

yi = α1

P
(3)
q
v∈N (u)∪{u} hv  α1  α2  α4  . . . ∈ R  β1  β2  β4  . . . ∈ Rd  and Gi = (V  E)  then
y⊤Θ

−1y ≤ 2|α1| · ∥β1∥2 +

2π(2l − 1)|α2l| · ∥β2l∥2l
2 .

∞X

u∈V

√

α2l

l=1

⊤
u β2l

h

l=1

2

The following theorem gives an upper bound on tr (Θ).
Theorem 4.3. If for all graphs Gi = (Vi  Ei) in the training set  |Vi| is upper bounded by V   then
tr(Θ) ≤ O(nV
Combining Theorem 4.2 and Theorem 4.3 with Theorem 4.1  we know if
2π(2l − 1)|α2l| · ∥β2l∥2l

). Here n is the number of training samples.

2|α1| · ∥β1∥2 +

∞X

√

2

is bounded  and |Vi| is bounded for all graphs Gi = (Vi  Ei) in the training set  then the GNTK that
corresponds to the simple GNN described above can learn functions of forms in (3)  with polynomial
number of samples. To our knowledge  this is the ﬁrst sample complexity analysis in the GK and
GNN literature.

l=1

5 Experiments

In this section  we demonstrate the effectiveness of GNTKs using experiments on graph classiﬁca-
tion tasks. For ablation study  we investigate how the performance varies with the architecture of
the corresponding GNN. Following common practices of evaluating performance of graph classiﬁ-
cation models Yanardag and Vishwanathan [2015]  we perform 10-fold cross validation and report
the mean and standard deviation of validation accuracies. More details about the experiment setup
can be found in Section B of the supplementary material.

Datasets. The benchmark datasets include four bioinformatics datasets MUTAG  PTC  NCI1 
PROTEINS and three social network datasets COLLAB  IMDB-BINARY  IMDB-MULTI. For each
graph  we transform the categorical input features to one-hot encoding representations. For datasets
where the graphs have no node features  i.e. only graph structure matters  we use degrees as input
node features.

5.1 Results

We compare GNTK with various state-of-the-art graph classiﬁcation algorithms: (1) the WL subtree
kernel Shervashidze et al. [2011]; (2) state-of-the-art deep learning architectures  including Graph
Convolutional Network (GCN) Kipf and Welling [2016]  GraphSAGE Hamilton et al. [2017]  Graph
Isomorphism Network(GIN) Xu et al. [2019a]  PATCHY-SANNiepert et al. [2016] and Deep Graph
CNN (DGCNN) Zhang et al. [2018a]; (3) Graph kernels based on random walks  i.e.  Anonymous
Walk Embeddings Ivanov and Burnaev [2018] and RetGK Zhang et al. [2018b]. For deep learning
methods and random walk graph kernels  we report the accuracies reported in the original papers.
The experiment setup is deferred to Section B.
The graph classiﬁcation results are shown in Table 1. The best results are highlighted as bold. Our
proposed GNTKs are powerful and achieve state-of-the-art classiﬁcation accuracy on most datasets.
In four of them  we ﬁnd GNTKs outperform all baseline methods. In particular  GNTKs achieve
83.6% accuracy on COLLAB dataset and 67.9% accuracy on PTC dataset  compared to the best of
baselines  81.0% and 64.6% respectively. Notably  GNTKs give the best performance on all social
network datasets. Moreover  In our experiments  we also observe that with the same architecture 
GNTK is more computational efﬁcient that its GNN counterpart. On IMDB-B dataset  running GIN
with the default setup (ofﬁcial implementation of Xu et al. [2019a]) takes 19 minutes on a TITAN X
GPU and running GNTK only takes 2 minutes.

7

Method

GCN
GraphSAGE
PatchySAN
DGCNN
GIN
WL subtree
AWL
RetGK
GNTK

N
N
G

K
G

–

73.7

COLLAB
79.0 ± 1.8
72.6 ± 2.2
80.2 ± 1.9
78.9 ± 1.9
73.9 ± 1.9
81.0 ± 0.3
83.6 ± 1.0

70.0

IMDB-B
74.0 ± 3.4
72.3 ± 5.3
71.0 ± 2.2
75.1 ± 5.1
73.8 ± 3.9
74.5 ± 5.9
71.9 ± 1.0
76.9 ± 3.6

47.8

IMDB-M
51.9 ± 3.8
50.9 ± 2.2
45.2 ± 2.8
52.3 ± 2.8
50.9 ± 3.8
51.5 ± 3.6
47.7 ± 0.3
52.8 ± 4.6

PTC

58.6

64.2 ± 4.3
63.9 ± 7.7
60.0 ± 4.8
64.6 ± 7.0
59.9 ± 4.3
62.5 ± 1.6
67.9 ± 6.9

–

74.4

NCI1
80.2 ± 2.0
77.7 ± 1.5
78.6 ± 1.9
82.7 ± 1.7
86.0 ± 1.8
84.5 ± 0.2
84.2 ± 1.5

–

85.8

MUTAG PROTEINS
76.0 ± 3.2
85.6 ± 5.8
75.9 ± 3.2
85.1 ± 7.6
75.9 ± 2.8
92.6 ± 4.2
76.2 ± 2.8
89.4 ± 5.6
90.4 ± 5.7
75.0 ± 3.1
87.9 ± 9.8
75.8 ± 0.6
90.3 ± 1.1
90.0 ± 8.5
75.6 ± 4.2

75.5

–

Table 1: Classiﬁcation results (in %) for graph classiﬁcation datasets. GNN: graph neural net-
work based methods. GK: graph kernel based methods. GNTK: fusion of GNN and GK.

(a) IMDB-BINARY

(b) NCI1

Figure 2: Effects of number of BLOCK operations and the scaling factor cu on the performance
of GNTK. Each dot represents the performance of a particular GNTK architecture. We divide
different architectures into different groups by number of BLOCK operations. We color these GNTK
architecture points by the scaling factor cu. We observe the test accuracy is correlated with the
dataset and the architecture.

5.2 Relation between GNTK Performance and the Corresponding GNN

We conduct ablation study to investigate how the performance of GNTK varies as we change the
architecture of the corresponding GNN. We select two representative datasets  one social network
dataset IMDBBINARY  and another bioinformatics dataset NCI1. For IMDBBINARY  we vary
the number of BLOCK operations in {2  3  4  5  6}. For NCI1  we vary the number of BLOCK
operations in {8  10  12  14  16}. For both datasets  we vary the number of MLP layers in {1  2  3}.

Effects of Number of BLOCK Operations and the Scaling Factor cu. We investigate how the
performance of GNTKs is correlated with number of BLOCK operations and the scaling factor cu.
First  on the bioinformatics dataset (NCI)  we observe that GNTKs with more layers perform better.
This is perhaps because  for molecules and bio graphs  more global structural information is helpful 
as they provide important information about the chemical/bio entity. On such graphs  GNTKs are
particularly effective because GNTKs can easily scale to many layers  whereas the number of layers
in GNNs may be restricted by computing resources.
Moreover  the performance of GNTK is correlated with that of the corresponding GNN. For example 
in social networks  GNTKs with sum aggregation cu = 1 work better than average aggregation
|N (u)|+1. The similar pattern holds in GNNs  because sum aggregation learns more graph
cu =
structure information than average aggregation Xu et al. [2019a]. This suggests GNTK can indeed
inherit the properties and advantages of the corresponding GNN  while also gaining the beneﬁts of
graph kernels.

1

8

23456Number of BLOCK operations0.720.730.740.750.760.770.78Testing Accuracycu=1cu=1|(u)|+18910111213141516Number of BLOCK operations0.790.800.810.820.830.840.85Testing Accuracycu=1cu=1|(u)|+1(a) IMDBBINARY

(b) NCI1

Figure 3: Effects of jumping knowledge and number of MLP layers on the performance of
GNTK. Each dot represents the test performance of a GNTK architecture. We divide all GNTK
architectures into different groups  according to whether jumping knowledge is applied  or number
of MLP layers.

Effects of Jumping Knowledge and Number of MLP Layers
In the GNN literature  jumping
knowledge network (JK) is expected to improve performance Xu et al. [2018]  Fey [2019].
In
Figure 3  we observe that a similar trend holds for GNTK. The performance of GNTK is improved
on both NCI and IMDB datasets when jumping knowledge is applied. Moreover  increasing the
number of MLP layers can increase the performance by ∼ 0.8%. These empirical ﬁndings further
conﬁrm that GNTKs can inherit the beneﬁts of GNNs  since improvements on GNN architectures
are reﬂected in the improvements GNTKs.
We conclude that GNTKs are attractive for graph representation learning because they can combine
the advantages of both GNNs and GKs.

Acknowledgments

S. S. Du and B. Póczos acknowledge support from AFRL grant FA8750-17-2-0212 and DARPA
D17AP0000. R. Salakhutdinov and R. Wang are supported in part by NSF IIS-1763562  Ofﬁce
of Naval Research grant N000141812861  and Nvidia NVAIL award. K. Xu is supported by NSF
CAREER award 1553284 and a Chevron-MIT Energy Fellowship. This work was performed while
S. S. Du was a Ph.D. student at Carnegie Mellon University and K. Hou was visiting Carnegie
Mellon University.

9

WithoutWithJumping Knowledge0.720.730.740.750.760.770.78Testing Accuracy123Number of MLP layersWithoutWithJumping Knowledge0.790.800.810.820.830.840.85Testing Accuracy123Number of MLP layersReferences
Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  Ruslan Salakhutdinov  and Ruosong Wang. On

exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955  2019a.

Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584  2019b.

Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

Peter Battaglia  Razvan Pascanu  Matthew Lai  Danilo Jimenez Rezende  et al. Interaction networks
for learning about objects  relations and physics. In Advances in Neural Information Processing
Systems  pages 4502–4510  2016.

Michaël Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing
Systems  pages 3844–3852  2016.

Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds global

minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018.

Simon S. Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations 
2019.

David K Duvenaud  Dougal Maclaurin  Jorge Iparraguirre  Rafael Bombarell  Timothy Hirzel  Alán
Aspuru-Guzik  and Ryan P Adams. Convolutional networks on graphs for learning molecular
ﬁngerprints. In Advances in neural information processing systems  pages 2224–2232  2015.

Matthias Fey. Just jump: Dynamic neighborhood aggregation in graph neural networks. arXiv

preprint arXiv:1904.04849  2019.

Thomas Gärtner  Peter Flach  and Stefan Wrobel. On graph kernels: Hardness results and efﬁcient

alternatives. In Learning theory and kernel machines  pages 129–143. Springer  2003.

Justin Gilmer  Samuel S Schoenholz  Patrick F Riley  Oriol Vinyals  and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning  pages
1273–1272  2017.

Will Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on large graphs.

In Advances in Neural Information Processing Systems  pages 1024–1034  2017.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision  pages 1026–1034  2015.

Sergey Ivanov and Evgeniy Burnaev. Anonymous walk embeddings. In ICML  2018.

Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and gener-

alization in neural networks. arXiv preprint arXiv:1806.07572  2018.

Steven Kearnes  Kevin McCloskey  Marc Berndl  Vijay Pande  and Patrick Riley. Molecular graph
convolutions: moving beyond ﬁngerprints. Journal of computer-aided molecular design  30(8):
595–608  2016.

Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907  2016.

Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard Zemel. Gated graph sequence neural

networks. In International Conference on Learning Representations  2016.

Mathias Niepert  Mohamed Ahmed  and Konstantin Kutzkov. Learning convolutional neural net-

works for graphs. In International conference on machine learning  pages 2014–2023  2016.

10

Adam Santoro  David Raposo  David G Barrett  Mateusz Malinowski  Razvan Pascanu  Peter
Battaglia  and Timothy Lillicrap. A simple neural network module for relational reasoning. In
Advances in neural information processing systems  pages 4967–4976  2017.

Adam Santoro  Felix Hill  David Barrett  Ari Morcos  and Timothy Lillicrap. Measuring abstract
reasoning in neural networks. In International Conference on Machine Learning  pages 4477–
4486  2018.

Franco Scarselli  Marco Gori  Ah Chung Tsoi  Markus Hagenbuchner  and Gabriele Monfardini.

The graph neural network model. IEEE Transactions on Neural Networks  20(1):61–80  2009.

Nino Shervashidze  SVN Vishwanathan  Tobias Petri  Kurt Mehlhorn  and Karsten Borgwardt. Efﬁ-
cient graphlet kernels for large graph comparison. In Artiﬁcial Intelligence and Statistics  pages
488–495  2009.

Nino Shervashidze  Pascal Schweitzer  Erik Jan van Leeuwen  Kurt Mehlhorn  and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research  12(Sep):2539–
2561  2011.

Petar Velickovic  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Lio  and Yoshua
In International Conference on Learning Representations 

Bengio. Graph attention networks.
2018.

S Vichy N Vishwanathan  Nicol N Schraudolph  Risi Kondor  and Karsten M Borgwardt. Graph

kernels. Journal of Machine Learning Research  11(Apr):1201–1242  2010.

Keyulu Xu  Chengtao Li  Yonglong Tian  Tomohiro Sonobe  Ken-ichi Kawarabayashi  and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning  pages 5449–5458  2018.

Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural

networks? In International Conference on Learning Representations  2019a.

Keyulu Xu  Jingling Li  Mozhi Zhang  Simon S Du  Ken-ichi Kawarabayashi  and Stefanie Jegelka.

What can neural networks reason about? arXiv preprint arXiv:1905.13211  2019b.

Pinar Yanardag and SVN Vishwanathan. Deep graph kernels.

In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 1365–1374.
ACM  2015.

Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior 
gradient independence  and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760 
2019.

Rex Ying  Jiaxuan You  Christopher Morris  Xiang Ren  William L Hamilton  and Jure Leskovec.
In Advances in Neural

Hierarchical graph representation learning with differentiable pooling.
Information Processing Systems  2018.

Muhan Zhang  Zhicheng Cui  Marion Neumann  and Yixin Chen. An end-to-end deep learning
architecture for graph classiﬁcation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
2018a.

Zhen Zhang  Mianzhi Wang  Yijian Xiang  Yan Huang  and Arye Nehorai. RetGK: Graph kernels

based on return probabilities of random walks. In NeurIPS  2018b.

11

,Simon Du
Kangcheng Hou
Russ Salakhutdinov
Barnabas Poczos
Ruosong Wang
Keyulu Xu