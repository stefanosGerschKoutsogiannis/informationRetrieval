2017,Gaussian Quadrature for Kernel Features,Kernel methods have recently attracted resurgent interest  showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines  but employing the randomized feature map means that $O(\epsilon^{-2})$ samples are required to achieve an approximation error of at most $\epsilon$. We investigate some alternative schemes for constructing feature maps that are deterministic  rather than random  by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed  for any $\gamma > 0$  to achieve error $\epsilon$ with $O(e^{e^\gamma} + \epsilon^{-1/\gamma})$ samples as $\epsilon$ goes to 0. Our method works particularly well with sparse ANOVA kernels  which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains  such as MNIST and TIMIT  showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.,Gaussian Quadrature for Kernel Features

Department of Computer Science

Department of Computer Science

Christopher De Sa

Cornell University
Ithaca  NY 14853

cdesa@cs.cornell.edu

Tri Dao

Stanford University
Stanford  CA 94305
trid@stanford.edu

Christopher Ré

Department of Computer Science

Stanford University
Stanford  CA 94305

chrismre@cs.stanford.edu

Abstract

Kernel methods have recently attracted resurgent interest  showing performance
competitive with deep neural networks in tasks such as speech recognition. The
random Fourier features map is a technique commonly used to scale up kernel
machines  but employing the randomized feature map means that O(−2) samples
are required to achieve an approximation error of at most . We investigate some
alternative schemes for constructing feature maps that are deterministic  rather
than random  by approximating the kernel in the frequency domain using Gaussian
quadrature. We show that deterministic feature maps can be constructed  for any
+ −1/γ) samples as  goes to 0. Our method
γ > 0  to achieve error  with O(eeγ
works particularly well with sparse ANOVA kernels  which are inspired by the
convolutional layer of CNNs. We validate our methods on datasets in different
domains  such as MNIST and TIMIT  showing that deterministic features are faster
to generate and achieve accuracy comparable to the state-of-the-art kernel methods
based on random Fourier features.

1

Introduction

Kernel machines are frequently used to solve a wide variety of problems in machine learning [26].
They have gained resurgent interest and have recently been shown [13  18  21  19  22] to be competi-
tive with deep neural networks in some tasks such as speech recognition on large datasets. A kernel
machine is one that handles input x1  . . .   xn  represented as vectors in Rd  only in terms of some
kernel function k : Rd × Rd → R of pairs of data points k(xi  xj). This representation is attractive
for classiﬁcation problems because one can learn non-linear decision boundaries directly on the input
without having to extract features before training a linear classiﬁer.
One well-known downside of kernel machines is the fact that they scale poorly to large datasets.
Naive kernel methods  which operate on the Gram matrix Gi j = k(xi  xj) of the data  can take a very
long time to run because the Gram matrix itself requires O(n2) space and many operations on it (e.g. 
the singular value decomposition) take up to O(n3) time. Rahimi and Recht [23] proposed a solution
to this problem: approximating the kernel with an inner product in a higher-dimensional space.
Speciﬁcally  they suggest constructing a feature map z : Rd → RD such that k(x  y) ≈ (cid:104)z(x)  z(y)(cid:105).
This approximation enables kernel machines to use scalable linear methods for solving classiﬁcation
problems and to avoid the pitfalls of naive kernel methods by not materializing the Gram matrix.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In the case of shift-invariant kernels  one technique that was proposed for constructing the function z
is random Fourier features [23]. This data-independent method approximates the Fourier transform
integral (1) of the kernel by averaging Monte-Carlo samples  which allows for arbitrarily-good
estimates of the kernel function k. Rahimi and Recht [23] proved that if the feature map has

(cid:1) then  with constant probability  the approximation (cid:104)z(x)  z(y)(cid:105) is uniformly

dimension D = ˜Ω(cid:0) d

2

-close to the true kernel on a bounded set. While the random Fourier features method has proven
to be effective in solving practical problems  it comes with some caveats. Most importantly  the
accuracy guarantees are only probabilistic and there is no way to easily compute  for a particular
random sample  whether the desired accuracy is achieved.
Our aim is to understand to what extent randomness is necessary to approximate a kernel. We
thus propose a fundamentally different scheme for constructing the feature map z. While still
approximating the kernel’s Fourier transform integral (1) with a discrete sum  we select the sample
points and weights deterministically. This gets around the issue of probabilistic-only guarantees
by removing the randomness from the algorithm. For small dimension  deterministic maps yield
signiﬁcantly lower error. As the dimension increases  some random sampling may become necessary 
and our theoretical insights provide a new approach to sampling. Moreover  for a particular class
of kernels called sparse ANOVA kernels (also known as convolutional kernels as they are similar
to the convolutional layer in CNNs) which have shown state-of-the-art performance in speech
recognition [22]  deterministic maps require fewer samples than random Fourier features  both in
terms of the desired error and the kernel size. We make the following contributions:

using only O(d) samples  whereas random Fourier features requires O(d3) samples.

• In Section 3  we describe how to deterministically construct a feature map z for the class of
subgaussian kernels (which can approximate any kernel well) that has exponentially small
(in D) approximation error.
• In Section 4  for sparse ANOVA kernels  we show that our method produces good estimates
• In Section 5  we validate our results experimentally. We demonstrate that  for real clas-
siﬁcation problems on MNIST and TIMIT datasets  our method combined with random
sampling yields up to 3 times lower kernel approximation error. With sparse ANOVA ker-
nels  our method slightly improves classiﬁcation accuracy compared to the state-of-the-art
kernel methods based on random Fourier features (which are already shown to match the
performance of deep neural networks)  all while speeding up the feature generation process.

2 Related Work

(cid:16)

Much work has been done on extracting features for kernel methods. The random Fourier features
method has been analyzed in the context of several learning algorithms  and its generalization error
has been characterized and compared to that of other kernel-based algorithms [24]. It has also been
compared to the Nyström method [35]  which is data-dependent and thus can sometimes outperform
random Fourier features. Other recent work has analyzed the generalization performance of the
random Fourier features algorithm [17]  and improved the bounds on its maximum error [29  31].
While we focus here on deterministic approximations to the Fourier transform integral and compare
them to Monte Carlo estimates  these are not the only two methods available to us. A possible
middle-ground method is quasi-Monte Carlo estimation  in which low-discrepancy sequences  rather
than the fully-random samples of Monte Carlo estimation  are used to approximate the integral.
This approach was analyzed in Yang et al. [34] and shown to achieves an asymptotic error of
. While this is asymptotically better than the random Fourier features
 = O
method  the complexity of the quasi-Monte Carlo method coupled with its larger constant factors
prevents it from being strictly better than its predecessor. Our method still requires asymptotically
fewer samples as  goes to 0.
Our deterministic approach here takes advantage of a long line of work on numerical quadrature
for estimating integrals. Bach [1] analyzed in detail the connection between quadrature and random
feature expansions  thus deriving bounds for the number of samples required to achieve a given
average approximation error (though they did not present complexity results regarding maximum error
nor suggested new feature maps). This connection allows us to leverage longstanding deterministic
numerical integration methods such as Gaussian quadrature [6  33] and sparse grids [2].

D−1 (log(D))d(cid:17)

2

Unlike many other kernels used in machine learning  such as the Gaussian kernel  the sparse ANOVA
kernel allows us to encode prior information about the relationships among the input variables into
the kernel itself. Sparse ANOVA kernels have been shown [30] to work well for many classiﬁcation
tasks  especially in structural modeling problems that beneﬁt from both the good generalization of a
kernel machine and the representational advantage of a sparse model [9].

3 Kernels and Quadrature
We start with a brief overview of kernels. A kernel function k : Rd × Rd → R encodes the similarity
between pairs of examples. In this paper  we focus on shift invariant kernels (those which satisfy
k(x  y) = k(x − y)  where we overload the deﬁnition of k to also refer to a function k : Rd → R)
that are positive deﬁnite and properly scaled. A kernel is positive deﬁnite if its Gram matrix is always
positive deﬁnite for all non-trivial inputs  and it is properly-scaled if k(x  x) = 1 for all x. In this
setting  our results make use of a theorem [25] that also provides the “key insight” behind the random
Fourier features method.
Theorem 1 (Bochner’s theorem). A continuous shift-invariant properly-scaled kernel k : Rd×Rd →
R is positive deﬁnite if and only if k is the Fourier transform of a proper probability distribution.

(cid:90)

Rd

We can then write k in terms of its Fourier transform Λ (which is a proper probability distribution):

k(x − y) =

Λ(ω) exp(jω(cid:62)(x − y)) dω.

(1)

For ω distributed according to Λ  this is equivalent to writing

k(x − y) = E(cid:2)exp(jω(cid:62)(x − y))(cid:3) = E(cid:2)(cid:104)exp(jω(cid:62)x)  exp(jω(cid:62)y)(cid:105)(cid:3)  

where we use the usual Hermitian inner product (cid:104)x  y(cid:105) =(cid:80)

i xiyi. The random Fourier features
method proceeds by estimating this expected value using Monte Carlo sampling averaged across D
random selections of ω. Equivalently  we can think of this as approximating (1) with a discrete sum
at randomly selected sample points.
Our objective is to choose some points ωi and weights ai to uniformly approximate the integral (1)
j (x − y)). To obtain a feature map z : Rd → CD where

i=1 ai exp(jω(cid:62)

with ˜k(x − y) = (cid:80)D
˜k(x − y) =(cid:80)D

i=1 aizi(x)zi(y)  we can deﬁne
a1 exp(jω(cid:62)

z(x) =(cid:2)√
(cid:12)(cid:12)(cid:12)k(x − y) − ˜k(x − y)

1 x)

. . .

(cid:12)(cid:12)(cid:12) = sup

(cid:107)u(cid:107)≤M

√

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:90)

Rd

 = sup

(x y)∈M

aD exp(jω(cid:62)

Dx)(cid:3)(cid:62)
Λ(ω)ejω(cid:62)u dω − D(cid:88)

.

i=1

We aim to bound the maximum error for x  y in a region M with diameter M = supx y∈M (cid:107)x − y(cid:107):

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

aiejω(cid:62)

i u

(2)

A quadrature rule is a choice of ωi and ai to minimize this maximum error. To evaluate a quadrature
rule  we are concerned with the sample complexity (for a ﬁxed diameter M).
Deﬁnition 1. For any  > 0  a quadrature rule has sample complexity DSC() = D  where D is the
smallest value such that the rule  when instantiated with D samples  has maximum error at most .

We will now examine ways to construct deterministic quadrature rules and their sample complexities.

3.1 Gaussian Quadrature

The main idea is to approximate integrals of the form(cid:82) Λ(ω)f (ω) dω ≈(cid:80)D

Gaussian quadrature is one of the most popular techniques in one-dimensional numerical integration.
i=1 aif (ωi) such that
the approximation is exact for all polynomials below a certain degree; D points are sufﬁcient for
polynomials of degree up to 2D − 1. While the points and weights used by Gaussian quadrature
depend both on the distribution Λ and the parameter D  they can be computed efﬁciently using
orthogonal polynomials [10  32]. Gaussian quadrature produces accurate results when integrating
functions that are well-approximated by polynomials  which include all subgaussian densities.

3

r
o
r
r
e
x
a
m
d
e
t
a
m

i
t
s
E

0.1

0.12

0.08

Polynomially-exact
Random Fourier

Error of Polynomially-Exact Quadrature vs RFFs
0.14
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2

0.02

0.04

0.06

0

Region diameter (M)
(a) Polynomially-exact

0

Error of Sparse Grid Quadrature vs RFFs

Sparse grid
Random Fourier

Error of Subsampled Dense Grid vs RFFs

Subsampled Dense Grid
Random Fourier

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0.2 0.4 0.6 0.8

1

1.2 1.4 1.6 1.8

Region diameter (M)

2

0

0

0.5

1

1.5

2

Region diameter (M)

2.5

3

(b) Sparse grid

(c) Subsampled dense grid

Figure 1: Error comparison (empirical maximum over 106 uniformly-distributed samples) of different
quadrature schemes and the random Fourier features method.

Deﬁnition 2 (Subgaussian Distribution). We say that a distribution Λ : Rd → R is subgaussian with
parameter b if for X ∼ Λ and for all t ∈ Rd  E [exp((cid:104)t  X(cid:105))] ≤ exp

.

(cid:16) 1
2 b2 (cid:107)t(cid:107)2(cid:17)

We subsequently assume that the distribution Λ is subgaussian  which is a technical restriction
compared to random Fourier features. Many of the kernels encountered in practice have subgaussian
spectra  including the ubiquitous Gaussian kernel. More importantly  we can approximate any kernel
by convolving it with the Gaussian kernel  resulting in a subgaussian kernel. The approximation error
can be made much smaller than the inherent noise in the data generation process.

3.2 Polynomially-Exact Rules

Since Gaussian quadrature is so successful in one dimension  as commonly done in the numerical
analysis literature [14]  we might consider using quadrature rules that are multidimensional analogues
of Gaussian quadrature — rules that are accurate for all polynomials up to a certain degree R. In
higher dimensions  this is equivalent to saying that our quadrature rule satisﬁes

for all r ∈ Nd such that (cid:88)

rl ≤ R 

(3)

l

l=1

l=1

2 exp

i=1
where el are the standard basis vectors.
To test the accuracy of polynomially-exact quadrature  we constructed a feature map for a Gaussian
kernel  Λ(ω) = (2π)− d
  in d = 25 dimensions with D = 1000 and accurate for
all polynomials up to degree R = 2. In Figure 1a  we compared this to a random Fourier features
rule with the same number of samples  over a range of region diameters M that captures most
of the data points in practice (as the kernel is properly scaled). For small regions in particular  a
polynomially-exact scheme can have a signiﬁcantly lower error than a random Fourier feature map.
This experiment motivates us to investigate theoretical bounds on the behavior of this method. For
subgaussian kernels  it is straightforward to bound the maximum error of a polynomially-exact feature
map using the Taylor series approximation of the exponential function in (2).
Theorem 2. Let k be a kernel with b-subgaussian spectrum  and let ˜k be its estimation under some
quadrature rule with non-negative weights that is exact up to some even degree R. Let M ⊂ Rd
be some region of diameter M. Then  for all x  y ∈ M  the error of the quadrature features

approximation is bounded by(cid:12)(cid:12)(cid:12)k(x − y) − ˜k(x − y)
quadrature samples we will need to satisfy the conditions of Theorem 2. There are(cid:0)d+R

All the proofs are found in the Appendix.
To bound the sample complexity of polynomially-exact quadrature  we need to determine how many

(cid:1) constraints

(cid:18) eb2M 2

(cid:12)(cid:12)(cid:12) ≤ 3

(cid:19) R

in (3)  so a series of polynomially-exact quadrature rules that use only about this many sample points
can yield a bound on the sample complexity of this quadrature rule.

R

d

.

2

(cid:90)

d(cid:89)

Λ(ω)

Rd

(e(cid:62)

l ω)rl dω =

(e(cid:62)

l ωi)rl

d(cid:89)

ai

D(cid:88)
(cid:16)− 1
2 (cid:107)ω(cid:107)2(cid:17)

4

and that all have a number of samples D ≤ β(cid:0)d+R

(cid:1) for some ﬁxed constant β. Then  for any γ > 0 

Corollary 1. Assume that we are given a class of feature maps that satisfy the conditions of Theorem 2 

the sample complexity of features maps in this class can be bounded by

d

(cid:32)
exp(cid:0)e2γ+1b2M 2(cid:1)  

γ(cid:33)
(cid:19) 1

(cid:18) 3



D() ≤ β2d max

.

(cid:16)

− 1

γ



(cid:17)

.

In particular  for a ﬁxed dimension d  this means that for any γ  D() = O

The result of this corollary implies that  in terms of the desired error   the sample complexity
increases asymptotically slower than any negative power of . Compared to the result for random
Fourier features which had D() = O(−2)  this has a much weaker dependence on . While this
weaker dependence does come at the cost of an additional factor of 2d  it is a constant cost of operating
in dimension d  and is not dependent on the error .
The more pressing issue  when comparing polynomially-exact features to random Fourier features  is
the fact that we have no way of efﬁciently constructing quadrature rules that satisfy the conditions
of Theorem 2. One possible construction involves selecting random sample points ωi  and then
solving (3) for the values of ai using a non-negative least squares (NNLS) algorithm. While this
construction works in low dimensions — it is the method we used for the experiment in Figure 1a —
it rapidly becomes infeasible to solve for higher values of d and R.
We will now show how to overcome this issue by introducing quadrature rules that can be rapidly
constructed using grid-based quadrature rules. These rules are constructed directly from products of a
one-dimensional quadrature rule  such as Gaussian quadrature  and so avoid the construction-difﬁculty
problems encountered in this section. Although grid-based quadrature rules can be constructed for any
kernel function [2]  they are easier to conceptualize when the kernel k factors along the dimensions 

i=1 ki(ui). For simplicity we will focus on this factorizable case.

as k(u) =(cid:81)d

3.3 Dense Grid Quadrature

(cid:17)

(cid:81)d

(cid:16)(cid:82) ∞

i=1

i u) dω

−∞ Λi(ω) exp(jωe(cid:62)

The simplest way to do this is with a dense grid (also known as tensor product) con-
struction.
A dense grid construction starts by factoring the integral (1) into k(u) =
  where ei are the standard basis vectors. Since each of the
factors is an integral over a single dimension  we can approximate them all with a one-dimensional
quadrature rule. In this paper  we focus on Gaussian quadrature  although we could also use other
methods such as Clenshaw-Curtis [3]. Taking tensor products of the points and weights results in the
dense grid quadrature. The detailed construction is given in Appendix A.
The individual Gaussian quadrature rules are exact for all polynomials up to degree 2L − 1  so the
dense grid is also accurate for all such polynomials. Theorem 2 then yields a bound on its sample
complexity.
Corollary 2. Let k be a kernel with a spectrum that is subgaussian with parameter b. Then  for any
γ > 0  the sample complexity of dense grid features can be bounded by

(cid:32)

(cid:18)

D() ≤ max

exp

deγd eb2M 2

2

γ(cid:33)
(cid:19) 1

.

(cid:19)

(cid:18) 3



 

(cid:16)

− 1

γ



(cid:17)

.

In particular  as was the case with polynomially-exact features  for a ﬁxed d  D() = O

Unfortunately  this scheme suffers heavily from the curse of dimensionality  since the sample
complexity is doubly-exponential in d. This means that  even though they are easy to compute  the
dense grid method does not represent a useful solution to the issue posed in Section 3.2.

3.4 Sparse Grid Quadrature

The curse of dimensionality for quadrature in high dimensions has been studied in the numerical
integration setting for decades. One of the more popular existing techniques for getting around

5

the curse is called sparse grid or Smolyak quadrature [28]  originally developed to solve partial
differential equations. Instead of taking the tensor product of the one-dimensional quadrature rule 
we only include points up to some ﬁxed total level A  thus constructing a linear combination of dense
grid quadrature rules that achieves a similar error with exponentially fewer points than a single larger
quadrature rule. The detailed construction is given in Appendix B. Compared to polynomially-exact
rules  sparse grid quadrature can be computed quickly and easily (see Algorithm 4.1 from Holtz [12]).
To measure the performance of sparse grid quadrature  we constructed a feature map for the same
Gaussian kernel analyzed in the previous section  with d = 25 dimensions and up to level A = 2. We
compared this to a random Fourier features rule with the same number of samples  D = 1351  and
plot the results in Figure 1b. As was the case with polynomially-exact quadrature  this sparse grid
scheme has tiny error for small-diameter regions  but this error unfortunately increases to be even
larger than that of random Fourier features as the region diameter increases.

The sparse grid construction yields a bound on the sample count: D ≤ 3A(cid:0)d+A

(cid:1)  where A is the

bound on the total level. By extending known bounds on the error of Gaussian quadrature  we can
similarly bound the error of the sparse grid feature method.
Theorem 3. Let k be a kernel with a spectrum that is subgaussian with parameter b  and let ˜k be
its estimation under the sparse grid quadrature rule up to level A. Let M ⊂ Rd be some region of
diameter M  and assume that A ≥ 24eb2M 2. Then  for all x  y ∈ M  the error of the quadrature
features approximation is bounded by

A

(cid:12)(cid:12)(cid:12)k(x − y) − ˜k(x − y)

(cid:12)(cid:12)(cid:12) ≤ 2d

(cid:18) 12eb2M 2

(cid:19)A

.

A

This  along with our above upper bound on the sample count  yields a bound on the sample complexity.
Corollary 3. Let k be a kernel with a spectrum that is subgaussian with parameter b. Then  for any
γ > 0  the sample complexity of sparse grid features can be bounded by
− 1

exp(cid:0)24e2γ+1b2M 2(cid:1)   2

D() ≤ 2d max

(cid:16)

(cid:17)

d
γ 

.

γ

As was the case with all our previous deterministic features maps  for a ﬁxed d  D() = O

(cid:16)

− 1

γ



(cid:17)

.

Subsampled grids One of the downsides of the dense/sparse grids analyzed above is the difﬁculty
of tuning the number of samples extracted in the feature map. As the only parameter we can typically
set is the degree of polynomial exactness  even a small change in this (e.g.  from 2 to 4) can produce
a signiﬁcant increase in the number of features. However  we can always subsample the grid points
according to the distribution determined by their weights to both tame the curse of dimensionality and
to have ﬁne-grained control over the number of samples. For simplicity  we focus on subsampling
the dense grid. In Figure 1c  we compare the empirical errors of subsampled dense grid and random
Fourier features  noting that they are essentially the same across all diameters.

3.5 Reweighted Grid Quadrature

Both random Fourier features and dense/sparse grid quadratures are data-independent. We now
describe a data-adaptive method to choose a quadrature for a pre-speciﬁed number of samples:
reweighting the grid points to minimize the difference between the approximate and the exact kernel
on a small subset of data. Adjusting the grid to the data distribution yields better kernel approximation.
We approximate the kernel k(x − y) with

˜k(x − y) =

ai exp(jω(cid:62)

i (x − y)) =

ai cos(ω(cid:62)

i (x − y)) 

where ai ≥ 0  as k is real-valued. We ﬁrst choose the set of potential grid points ω1  . . .   ωD by
sampling from a dense grid of Gaussian quadrature points. To solve for the weights a1  . . .   aD  we
independently sample n pairs (x1  y1)  . . .   (xn  yn) from the dataset  then minimize the empirical
mean squared error (with variable a1  . . .   aD):

D(cid:88)

i=1

D(cid:88)

i=1

minimize
subject to ai ≥ 0  for i = 1  . . .   D.

l=1

k(xl − yl) − ˜k(xl − yl)

1
n

(cid:17)2

n(cid:88)

(cid:16)

6

For appropriately deﬁned matrix M and vector b  this is an NNLS problem of minimizing
n (cid:107)M a − b(cid:107)2 subject to a ≥ 0  with variable a ∈ RD. The solution is often sparse  due to
1
the active elementwise constraints a ≥ 0. Hence we can pick a larger set of potential grid points
ω1  . . .   ωD(cid:48) (with D(cid:48) > D) and solve the above problem to obtain a smaller set of grid points (those
with aj > 0). To get even sparser solution  we add an (cid:96)1-penalty term with parameter λ ≥ 0:

minimize
subject to ai ≥ 0  for i = 1  . . .   D(cid:48).

n (cid:107)M a − b(cid:107)2 + λ 1(cid:62) a

1

Bisecting on λ yields the desired number of grid points.
As this is a data-dependent quadrature  we empirically evaluate its performance on the TIMIT dataset 
which we will describe in more details in Section 5. In Figure 2b  we compare the estimated root
mean squared error on the dev set of different feature generation schemes against the number of
features D (mean and standard deviation over 10 runs). Random Fourier features  Quasi-Monte Carlo
(QMC) with Halton sequence  and subsampled dense grid have very similar approximation error 
while reweighted quadrature has much lower approximation error. Reweighted quadrature achieves
2–3 times lower error for the same number of features and requiring 3–5 times fewer features for a
ﬁxed threshold of approximation error compared to random Fourier features. Moreover  reweighted
features have extremely low variance  even though the weights are adjusted based only on a very
small fraction of the dataset (500 samples out of 1 million data points).

Faster feature generation Not only does grid-based quadrature yield better statistical performance
to random Fourier features  it also has some notable systems beneﬁts. Generating quadrature features
requires a much smaller number of multiplies  as the grid points only take on a ﬁnite set of values for
all dimensions (assuming an isotropic kernel). For example  a Gaussian quadrature that is exact up to
polynomials of degree 21 only requires 11 grid points for each dimension. To generate the features  we
multiply the input with these 11 numbers before adding the results to form the deterministic features.
The save in multiples may be particularly signiﬁcant in architectures such as application-speciﬁc
integrated circuits (ASICs). In our experiment on the TIMIT dataset in Section 5  this specialized
matrix multiplication procedure (on CPU) reduces the feature generation time in half.

4 Sparse ANOVA Kernels

One type of kernel that is commonly used in machine learning  for example in structural modeling  is
the sparse ANOVA kernels [11  8]. They are also called convolutional kernels  as they operate similarly
to the convolutional layer in CNNs. These kernels have achieved state-of-the-art performance on
large real-world datasets [18  22]  as we will see in Section 5. A kernel of this type can be written as

where S is a set of subsets of the variables in {1  . . .   d}  and k1 is a one-dimensional kernel.
(Straightforward extensions  which we will not discuss here  include using different one-dimensional
kernels for each element of the products  and weighting the sum.) Sparse ANOVA kernels are
used to encode sparse dependencies among the variables: two variables are related if they appear
together in some S ∈ S. These sparse dependencies are typically problem-speciﬁc: each S could
correspond to a factor in the graph if we are analyzing a distribution modeled with a factor graph.
Equivalently  we can think of the set S as a hypergraph  where each S ∈ S corresponds to a hyperedge.
Using this notion  we deﬁne the rank of an ANOVA kernel to be r = maxS∈S |S|  the degree as
∆ = maxi∈{1 ... d} |{S ∈ S|i ∈ S}|  and the size of the kernel to be the number of hyperedges
m = |S|. For sparse models  it is common for both the rank and the degree to be small  even as the
number of dimensions d becomes large  so m = O(d). This is the case we focus on in this section.
It is straightforward to apply the random Fourier features method to construct feature maps for
ANOVA kernels: construct feature maps for each of the (at most r-dimensional) sub-kernels kS(x −
i∈S k1(xi − yi) individually  and then combine the results. To achieve overall error   it
sufﬁces for each of the sub-kernel feature maps to have error /m; this can be achieved by random

y) = (cid:81)
Fourier features using DS = ˜Ω(cid:0)r(m−1)−2(cid:1) = ˜Ω(cid:0)rm2−2(cid:1) samples each  where the notation

˜Ω hides the log 1/ factor. Summed across all the m sub-kernels  this means that the random

7

(cid:88)

(cid:89)

S∈S

i∈S

k(x  y) =

k1(xi − yi) 

D() = ˜Ω(cid:0)rm3−2(cid:1) samples. While it is nice to be able to tackle this problem using random

Fourier features map can achieve error  with constant probability with a sample complexity of

features  the cubic dependence on m in this expression is undesirable: it is signiﬁcantly larger than
the D = ˜Ω(d−2) we get in the non-ANOVA case.
Can we construct a deterministic feature map that has a better error bound? It turns out that we can.
Theorem 4. Assume that we use polynomially-exact quadrature to construct features for each of
the sub-kernels kS  under the conditions of Theorem 2  and then combine the resulting feature maps
to produce a feature map for the full ANOVA kernel. For any γ > 0  the sample complexity of this
method is

(cid:16)

exp(cid:0)e2γ+1b2M 2(cid:1)   (3∆)

D() ≤ βm2r max

(cid:17)

.

− 1

γ

1

γ 

Compared to the random Fourier features  this rate depends only linearly on m. For ﬁxed parameters
β  b  M  ∆  r  and for any γ > 0  we can bound the sample complexity D() = O(m
γ )  which is
better than random Fourier features both in terms of the kernel size m and the desired error .

− 1

5 Experiments

To evaluate the performance of deterministic feature maps  we analyzed the accuracy of a sparse
ANOVA kernel on the MNIST digit classiﬁcation task [16] and the TIMIT speech recognition task [5].

Digit classiﬁcation on MNIST This task consists of 70  000 examples (60  000 in the training
dataset and 10  000 in the test dataset) of hand-written digits which need to be classiﬁed. Each
example is a 28 × 28 gray-scale image. Clever kernel-based SVM techniques are known to achieve
very low error rates (e.g.  0.79%) on this problem [20]. We do not attempt to compare ourselves with
these rates; rather  we compare random Fourier features and subsampled dense grid features that
both approximate the same ANOVA kernel. The ANOVA kernel we construct is designed to have
a similar structure to the ﬁrst layer of a convolutional neural network [27]. Just as a ﬁlter is run on
each 5 × 5 square of the image  for our ANOVA kernel  each of the sub-kernels is chosen to run on a
5 × 5 square of the original image (note that there are many  (28 − 5 + 1)2 = 576  such squares).
We choose the simple Gaussian kernel as our one-dimensional kernel.
Figure 2a compares the dense grid subsampling method to random Fourier features across a range of
feature counts. The deterministic feature map with subsampling performs better than the random
Fourier feature map across most large feature counts  although its performance degrades for very
small feature counts. The deterministic feature map is also somewhat faster to compute  taking—for
the 28800-features—320 seconds vs. 384 seconds for the random Fourier features  a savings of 17%.

Speech recognition on TIMIT This task requires producing accurate transcripts from raw audio
recordings of conversations in English  involving 630 speakers  for a total of 5.4 hours of speech.
We use the kernel features in the acoustic modeling step of speech recognition. Each data point
corresponds to a frame (10ms) of audio data  preprocessed using the standard feature space Maximum
Likelihood Linear Regression (fMMLR) [4]. The input x has dimension 40. After generating kernel
features z(x) from this input  we model the corresponding phonemes y by a multinomial logistic
regression model. Again  we use a sparse ANOVA kernel  which is a sum of 50 sub-kernels of the
form exp(−γ (cid:107)xS − yS(cid:107)2)  each acting on a subset S of 5 indices. These subsets are randomly
chosen a priori. To reweight the quadrature features  we sample 500 data points out of 1 million.
We plot the phone error rates (PER) of a speech recognizer trained based on different feature
generation schemes against the number of features D in Figure 2c (mean and standard deviation
over 10 runs). Again  subsampled dense grid performs similarly to random Fourier features  QMC
yields slightly higher error  while reweighted features achieve slightly lower phone error rates. All
four methods have relatively high variability in their phone error rates due to the stochastic nature of
the training and decoding steps in the speech recognition pipeline. The quadrature-based features
(subsampled dense grids and reweighted quadrature) are about twice as fast to generate  compared to
random Fourier features  due to the small number of multiplies required. We use the same setup as
May et al. [22]  and the performance here matches both that of random Fourier features and deep
neural networks in May et al. [22].

8

y
c
a
r
u
c
c
a

t
s
e
T

0.985
0.98
0.975
0.97
0.965
0.96
0.955
0.95
0.945

Test Accuracy on MNIST

Random Fourier
Subsampled dense grid

r
o
r
r
e

n
o
i
t
a
m
i
x
o
r
p
p
a
S
M
R

0

5000 10000 15000 20000 25000 30000

Number of features

Kernel RMS approximation error on TIMIT

Random Fourier
Quasi-Monte Carlo
Subsampled dense grid
Reweighted quadrature

2.5

2

1.5

1

0.5

0

10000

20000

30000

40000

50000

Number of features

e
t
a
r

r
o
r
r
e

e
n
o
h
P

19

18.9

18.8

18.7

18.6

18.5

18.4

18.3

10000

Phone error rate on TIMIT
Random Fourier
Quasi-Monte Carlo
Subsampled dense grid
Reweighted quadrature

15000

20000

25000

30000

Number of features

(a) Test accuracy on MNIST

(b) Kernel approx. error on TIMIT

(c) Phone error rate on TIMIT

Figure 2: Performance of different feature generation schemes on MNIST and TIMIT.

6 Conclusion

We presented deterministic feature maps for kernel machines. We showed that we can achieve better
scaling in the desired accuracy  compared to the state-of-the-art method  random Fourier features.
We described several ways to construct these feature maps  including polynomially-exact quadrature 
dense grid construction  sparse grid construction  and reweighted grid construction. Our results apply
well to the case of sparse ANOVA kernels  achieving signiﬁcant improvements (in the dependency on
the dimension d) over random Fourier features. Finally  we evaluated our results experimentally  and
showed that ANOVA kernels with deterministic feature maps can produce comparable accuracy to
the state-of-the-art methods based on random Fourier features on real datasets.
ANOVA kernels are an example of how structure can be used to deﬁne better kernels. Resembling
the convolutional layers of convolutional neural networks  they induce the necessary inductive bias in
the learning process. Given CNNs’ recent success in other domains beside images  such as sentence
classiﬁcation [15] and machine translation [7]  we hope that our work on deterministic feature maps
will enable kernel methods such as ANOVA kernels to ﬁnd new areas of application.

Acknowledgments

This material is based on research sponsored by Defense Advanced Research Projects Agency
(DARPA) under agreement number FA8750-17-2-0095. We gratefully acknowledge the support of
the DARPA SIMPLEX program under No. N66001-15-C-4043  DARPA FA8750-12-2-0335 and
FA8750-13-2-0039  DOE 108845  National Institute of Health (NIH) U54EB020405  the National
Science Foundation (NSF) under award No. CCF-1563078  the Ofﬁce of Naval Research (ONR)
under awards No. N000141210041 and No. N000141310129  the Moore Foundation  the Okawa
Research Grant  American Family Insurance  Accenture  Toshiba  and Intel. This research was
supported in part by afﬁliate members and other supporters of the Stanford DAWN project: Intel 
Microsoft  Teradata  and VMware. The U.S. Government is authorized to reproduce and distribute
reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and
conclusions contained herein are those of the authors and should not be interpreted as necessarily
representing the ofﬁcial policies or endorsements  either expressed or implied  of DARPA or the U.S.
Government. Any opinions  ﬁndings  and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views of DARPA  AFRL  NSF  NIH  ONR 
or the U.S. government.

9

References
[1] Francis Bach. On the equivalence between quadrature rules and random features. arXiv preprint

arXiv:1502.06800  2015.

[2] Hans-Joachim Bungartz and Michael Griebel. Sparse grids. Acta numerica  13:147–269  2004.

[3] Charles W Clenshaw and Alan R Curtis. A method for numerical integration on an automatic

computer. Numerische Mathematik  2(1):197–205  1960.

[4] Mark JF Gales. Maximum likelihood linear transformations for HMM-based speech recognition.

Computer speech & language  12(2):75–98  1998.

[5] J. S. Garofolo  L. F. Lamel  W. M. Fisher  J. G. Fiscus  D. S. Pallett  and N. L. Dahlgren.
DARPA TIMIT acoustic phonetic continuous speech corpus CDROM  1993. URL http:
//www.ldc.upenn.edu/Catalog/LDC93S1.html.

[6] Carl Friedrich Gauss. Methodus nova integralium valores per approximationem inveniendi.

apvd Henricvm Dieterich  1815.

[7] Jonas Gehring  Michael Auli  David Grangier  Denis Yarats  and Yann N Dauphin. Convolutional

sequence to sequence learning. arXiv preprint arXiv:1705.03122  2017.

[8] S. R. Gunn and J. S. Kandola. Structural Modelling with Sparse Kernels. Machine Learning 
48(1-3):137–163  July 2002. ISSN 0885-6125  1573-0565. doi: 10.1023/A:1013903804720.
URL https://link.springer.com/article/10.1023/A:1013903804720.

[9] Steve R. Gunn and Jaz S. Kandola. Structural modelling with sparse kernels. Machine learning 

48(1-3):137–163  2002.

[10] Nicholas Hale and Alex Townsend. Fast and accurate computation of Gauss–Legendre and
Gauss–Jacobi quadrature nodes and weights. SIAM Journal on Scientiﬁc Computing  35(2):
A652–A674  2013.

[11] Thomas Hofmann  Bernhard Schölkopf  and Alexander J Smola. Kernel methods in machine

learning. The annals of statistics  pages 1171–1220  2008.

[12] Markus Holtz. Sparse grid quadrature in high dimensions with applications in ﬁnance and

insurance  volume 77. Springer Science & Business Media  2010.

[13] Po-Sen Huang  Haim Avron  Tara N Sainath  Vikas Sindhwani  and Bhuvana Ramabhadran.
Kernel methods match deep neural networks on TIMIT. In Acoustics  Speech and Signal
Processing (ICASSP)  2014 IEEE International Conference on  pages 205–209. IEEE  2014.

[14] Eugene Isaacson and Herbert Bishop Keller. Analysis of numerical methods. Courier Corpora-

tion  1994.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)  pages
1746–1751.

[16] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[17] Ming Lin  Shifeng Weng  and Changshui Zhang. On the sample complexity of random Fourier
features for online learning: How many random Fourier features do we need? ACM Trans.
Knowl. Discov. Data  2014.

[18] Zhiyun Lu  Avner May  Kuan Liu  Alireza Bagheri Garakani  Dong Guo  Aurélien Bellet  Linxi
Fan  Michael Collins  Brian Kingsbury  Michael Picheny  and Fei Sha. How to scale up kernel
methods to be as good as deep neural nets. arXiv:1411.4000 [cs  stat]  November 2014. URL
http://arxiv.org/abs/1411.4000. arXiv: 1411.4000.

10

[19] Zhiyun Lu  Dong Quo  Alireza Bagheri Garakani  Kuan Liu  Avner May  Aurélien Bellet  Linxi
Fan  Michael Collins  Brian Kingsbury  Michael Picheny  et al. A comparison between deep
neural nets and kernel acoustic models for speech recognition. In Acoustics  Speech and Signal
Processing (ICASSP)  2016 IEEE International Conference on  pages 5070–5074. IEEE  2016.

[20] Subhransu Maji and Jitendra Malik. Fast and accurate digit classiﬁcation. EECS Department 

University of California  Berkeley  Tech. Rep. UCB/EECS-2009-159  2009.

[21] Avner May  Michael Collins  Daniel Hsu  and Brian Kingsbury. Compact kernel models for
acoustic modeling via random feature selection. In Acoustics  Speech and Signal Processing
(ICASSP)  2016 IEEE International Conference on  pages 2424–2428. IEEE  2016.

[22] Avner May  Alireza Bagheri Garakani  Zhiyun Lu  Dong Guo  Kuan Liu  Aurélien Bellet  Linxi
Fan  Michael Collins  Daniel Hsu  Brian Kingsbury  et al. Kernel approximation methods for
speech recognition. arXiv preprint arXiv:1701.03577  2017.

[23] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2007.

[24] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimiza-
tion with randomization in learning. In Advances in neural information processing systems 
pages 1313–1320  2009.

[25] Walter Rudin. Fourier analysis on groups. Number 12. John Wiley & Sons  1990.

[26] Bernhard Schölkopf and Alexander J Smola. Learning with kernels: Support vector machines 

regularization  optimization  and beyond. MIT press  2002.

[27] Patrice Y Simard  Dave Steinkraus  and John C Platt. Best practices for convolutional neural

networks applied to visual document analysis. In ICDAR  page 958. IEEE  2003.

[28] S. A. Smolyak. Quadrature and interpolation formulas for tensor products of certain class
of functions. Dokl. Akad. Nauk SSSR  148(5):1042–1053  1963. Transl.: Soviet Math. Dokl.
4:240-243  1963.

[29] Bharath Sriperumbudur and Zoltan Szabo. Optimal rates for random Fourier features. In
C. Cortes  N.D. Lawrence  D.D. Lee  M. Sugiyama  and R. Garnett  editors  Advances in Neural
Information Processing Systems 28  pages 1144–1152. Curran Associates  Inc.  2015.

[30] M Stitson  Alex Gammerman  Vladimir Vapnik  Volodya Vovk  Chris Watkins  and Jason
Weston. Support vector regression with anova decomposition kernels. Advances in kernel
methods—Support vector learning  pages 285–292  1999.

[31] Dougal J. Sutherland and Jeff Schneider. On the error of random Fourier features. In Proceedings
of the 31th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-15). AUAI Press 
2015.

[32] Alex Townsend  Thomas Trogdon  and Sheehan Olver. Fast computation of Gauss quadrature
nodes and weights on the whole real line. IMA Journal of Numerical Analysis  page drv002 
2015.

[33] Lloyd N Trefethen. Is Gauss quadrature better than Clenshaw–Curtis? SIAM review  50(1):

67–87  2008.

[34] Jiyan Yang  Vikas Sindhwani  Haim Avron  and Michael Mahoney. Quasi-Monte Carlo feature
In Proceedings of The 31st International Conference on

maps for shift-invariant kernels.
Machine Learning (ICML-14)  pages 485–493  2014.

[35] Tianbao Yang  Yu-Feng Li  Mehrdad Mahdavi  Rong Jin  and Zhi-Hua Zhou. Nyström method
vs random Fourier features: A theoretical and empirical comparison. In Advances in neural
information processing systems  pages 476–484  2012.

11

,Tri Dao
Christopher De Sa
Christopher Ré